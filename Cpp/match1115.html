<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for recurrent_layer.hpp & reduction_layer.hpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for recurrent_layer.hpp & reduction_layer.hpp
      </h3>
      <h1 align="center">
        50.0%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>recurrent_layer.hpp (38.938053%)<TH>reduction_layer.hpp (69.84127%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match1115-0.html#0',2,'match1115-1.html#0',3)" NAME="0">(25-34)<TD><A HREF="javascript:ZweiFrames('match1115-0.html#0',2,'match1115-1.html#0',3)" NAME="0">(19-29)</A><TD ALIGN=center><FONT COLOR="#ff0000">17</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match1115-0.html#1',2,'match1115-1.html#1',3)" NAME="1">(44-47)<TD><A HREF="javascript:ZweiFrames('match1115-0.html#1',2,'match1115-1.html#1',3)" NAME="1">(29-31)</A><TD ALIGN=center><FONT COLOR="#d20000">14</FONT>
<TR><TD BGCOLOR="#980517"><FONT COLOR="#980517">-</FONT><TD><A HREF="javascript:ZweiFrames('match1115-0.html#2',2,'match1115-1.html#2',3)" NAME="2">(143-148)<TD><A HREF="javascript:ZweiFrames('match1115-0.html#2',2,'match1115-1.html#2',3)" NAME="2">(34-39)</A><TD ALIGN=center><FONT COLOR="#c30000">13</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>recurrent_layer.hpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#ifndef CAFFE_RECURRENT_LAYER_HPP_
#define CAFFE_RECURRENT_LAYER_HPP_

#include &lt;string&gt;
#include &lt;utility&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/layer.hpp&quot;
#include &quot;caffe/net.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;
#include &quot;caffe/util/format.hpp&quot;

namespace caffe {

template &lt;typename Dtype&gt; class RecurrentLayer;

/**
 * @brief An abstract class for implementing recurrent behavior inside of an
 *        unrolled network.  This Layer type cannot be instantiated -- instead,
<A NAME="0"></A> *        you should use one of its implementations which defines the recurrent
 *        architecture, such as RNNLayer or LSTMLayer.
 */
<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match1115-1.html#0',3,'match1115-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>template &lt;typename Dtype&gt;
class RecurrentLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit RecurrentLayer(const LayerParameter&amp; param)
      : Layer&lt;Dtype&gt;(param) {}
  virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Reset();</B></FONT>

  virtual inline const char* type() const { return &quot;Recurrent&quot;; }
  virtual inline int MinBottomBlobs() const {
    int min_bottoms = 2;
    if (this-&gt;layer_param_.recurrent_param().expose_hidden()) {
      vector&lt;string&gt; inputs;
<A NAME="1"></A>      this-&gt;RecurrentInputBlobNames(&amp;inputs);
      min_bottoms += inputs.size();
    }
<FONT color="#f63526"><A HREF="javascript:ZweiFrames('match1115-1.html#1',3,'match1115-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>    return min_bottoms;
  }
  virtual inline int MaxBottomBlobs() const { return MinBottomBlobs() + 1; }
  virtual inline int ExactNumTopBlobs() const {</B></FONT>
    int num_tops = 1;
    if (this-&gt;layer_param_.recurrent_param().expose_hidden()) {
      vector&lt;string&gt; outputs;
      this-&gt;RecurrentOutputBlobNames(&amp;outputs);
      num_tops += outputs.size();
    }
    return num_tops;
  }

  virtual inline bool AllowForceBackward(const int bottom_index) const {
    // Can't propagate to sequence continuation indicators.
    return bottom_index != 1;
  }

 protected:
  /**
   * @brief Fills net_param with the recurrent network architecture.  Subclasses
   *        should define this -- see RNNLayer and LSTMLayer for examples.
   */
  virtual void FillUnrolledNet(NetParameter* net_param) const = 0;

  /**
   * @brief Fills names with the names of the 0th timestep recurrent input
   *        Blob&amp;s.  Subclasses should define this -- see RNNLayer and LSTMLayer
   *        for examples.
   */
  virtual void RecurrentInputBlobNames(vector&lt;string&gt;* names) const = 0;

  /**
   * @brief Fills shapes with the shapes of the recurrent input Blob&amp;s.
   *        Subclasses should define this -- see RNNLayer and LSTMLayer
   *        for examples.
   */
  virtual void RecurrentInputShapes(vector&lt;BlobShape&gt;* shapes) const = 0;

  /**
   * @brief Fills names with the names of the Tth timestep recurrent output
   *        Blob&amp;s.  Subclasses should define this -- see RNNLayer and LSTMLayer
   *        for examples.
   */
  virtual void RecurrentOutputBlobNames(vector&lt;string&gt;* names) const = 0;

  /**
   * @brief Fills names with the names of the output blobs, concatenated across
   *        all timesteps.  Should return a name for each top Blob.
   *        Subclasses should define this -- see RNNLayer and LSTMLayer for
   *        examples.
   */
  virtual void OutputBlobNames(vector&lt;string&gt;* names) const = 0;

  /**
   * @param bottom input Blob vector (length 2-3)
   *
   *   -# @f$ (T \times N \times ...) @f$
   *      the time-varying input @f$ x @f$.  After the first two axes, whose
   *      dimensions must correspond to the number of timesteps @f$ T @f$ and
   *      the number of independent streams @f$ N @f$, respectively, its
   *      dimensions may be arbitrary.  Note that the ordering of dimensions --
   *      @f$ (T \times N \times ...) @f$, rather than
   *      @f$ (N \times T \times ...) @f$ -- means that the @f$ N @f$
   *      independent input streams must be &quot;interleaved&quot;.
   *
   *   -# @f$ (T \times N) @f$
   *      the sequence continuation indicators @f$ \delta @f$.
   *      These inputs should be binary (0 or 1) indicators, where
   *      @f$ \delta_{t,n} = 0 @f$ means that timestep @f$ t @f$ of stream
   *      @f$ n @f$ is the beginning of a new sequence, and hence the previous
   *      hidden state @f$ h_{t-1} @f$ is multiplied by @f$ \delta_t = 0 @f$
   *      and has no effect on the cell's output at timestep @f$ t @f$, and
   *      a value of @f$ \delta_{t,n} = 1 @f$ means that timestep @f$ t @f$ of
   *      stream @f$ n @f$ is a continuation from the previous timestep
   *      @f$ t-1 @f$, and the previous hidden state @f$ h_{t-1} @f$ affects the
   *      updated hidden state and output.
   *
   *   -# @f$ (N \times ...) @f$ (optional)
   *      the static (non-time-varying) input @f$ x_{static} @f$.
   *      After the first axis, whose dimension must be the number of
   *      independent streams, its dimensions may be arbitrary.
   *      This is mathematically equivalent to using a time-varying input of
   *      @f$ x'_t = [x_t; x_{static}] @f$ -- i.e., tiling the static input
   *      across the @f$ T @f$ timesteps and concatenating with the time-varying
   *      input.  Note that if this input is used, all timesteps in a single
   *      batch within a particular one of the @f$ N @f$ streams must share the
   *      same static input, even if the sequence continuation indicators
   *      suggest that difference sequences are ending and beginning within a
   *      single batch.  This may require padding and/or truncation for uniform
   *      length.
   *
   * @param top output Blob vector (length 1)
   *   -# @f$ (T \times N \times D) @f$
   *      the time-varying output @f$ y @f$, where @f$ D @f$ is
   *      &lt;code&gt;recurrent_param.num_output()&lt;/code&gt;.
<A NAME="2"></A>   *      Refer to documentation for particular RecurrentLayer implementations
   *      (such as RNNLayer and LSTMLayer) for the definition of @f$ y @f$.
   */
<FONT color="#980517"><A HREF="javascript:ZweiFrames('match1115-1.html#2',3,'match1115-top.html#2',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);</B></FONT>

  /// @brief A Net to implement the Recurrent functionality.
  shared_ptr&lt;Net&lt;Dtype&gt; &gt; unrolled_net_;

  /// @brief The number of independent streams to process simultaneously.
  int N_;

  /**
   * @brief The number of timesteps in the layer's input, and the number of
   *        timesteps over which to backpropagate through time.
   */
  int T_;

  /// @brief Whether the layer has a &quot;static&quot; input copied across all timesteps.
  bool static_input_;

  /**
   * @brief The last layer to run in the network. (Any later layers are losses
   *        added to force the recurrent net to do backprop.)
   */
  int last_layer_index_;

  /**
   * @brief Whether the layer's hidden state at the first and last timesteps
   *        are layer inputs and outputs, respectively.
   */
  bool expose_hidden_;

  vector&lt;Blob&lt;Dtype&gt;* &gt; recur_input_blobs_;
  vector&lt;Blob&lt;Dtype&gt;* &gt; recur_output_blobs_;
  vector&lt;Blob&lt;Dtype&gt;* &gt; output_blobs_;
  Blob&lt;Dtype&gt;* x_input_blob_;
  Blob&lt;Dtype&gt;* x_static_input_blob_;
  Blob&lt;Dtype&gt;* cont_input_blob_;
};

}  // namespace caffe

#endif  // CAFFE_RECURRENT_LAYER_HPP_
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>reduction_layer.hpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#ifndef CAFFE_REDUCTION_LAYER_HPP_
#define CAFFE_REDUCTION_LAYER_HPP_

#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/layer.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;

namespace caffe {

/**
 * @brief Compute &quot;reductions&quot; -- operations that return a scalar output Blob
 *        for an input Blob of arbitrary size, such as the sum, absolute sum,
 *        and sum of squares.
<A NAME="0"></A> *
 * TODO(dox): thorough documentation for Forward, Backward, and proto params.
 */
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match1115-0.html#0',2,'match1115-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>template &lt;typename Dtype&gt;
class ReductionLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit ReductionLayer(const LayerParameter&amp; param)
      : Layer&lt;Dtype&gt;(param) {}
  virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
<A NAME="1"></A>  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline const char* type() const { return &quot;Reduction&quot;; }</B></FONT><FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match1115-0.html#1',2,'match1115-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>
  virtual inline int ExactNumBottomBlobs() const { return 1; }
<A NAME="2"></A>  virtual inline int ExactNumTopBlobs() const { return 1; }</B></FONT>

 protected:
<FONT color="#980517"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match1115-0.html#2',2,'match1115-top.html#2',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);</B></FONT>
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);

  /// @brief the reduction operation performed by the layer
  ReductionParameter_ReductionOp op_;
  /// @brief a scalar coefficient applied to all outputs
  Dtype coeff_;
  /// @brief the index of the first input axis to reduce
  int axis_;
  /// @brief the number of reductions performed
  int num_;
  /// @brief the input size of each reduction
  int dim_;
  /// @brief a helper Blob used for summation (op_ == SUM)
  Blob&lt;Dtype&gt; sum_multiplier_;
};

}  // namespace caffe

#endif  // CAFFE_REDUCTION_LAYER_HPP_
</PRE>
</div>
  </div>
</body>
</html>
