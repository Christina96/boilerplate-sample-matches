
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 6.4482029598308666%, Tokens: 9</h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-caffe.cpp</h3>
            <pre><code>1  #ifdef WITH_PYTHON_LAYER
2  #include "boost/python.hpp"
3  namespace bp = boost::python;
4  #endif
5  #include <gflags/gflags.h>
6  #include <glog/logging.h>
7  #include <cstring>
8  #include <map>
9  #include <string>
10  #include <utility>
11  #include <vector>
12  #include "boost/algorithm/string.hpp"
13  #include "boost/make_shared.hpp"
14  #include "caffe/caffe.hpp"
15  #include "caffe/training_utils.hpp"
16  #include "caffe/util/performance.hpp"
17  #include "caffe/util/signal_handler.h"
18  #include "caffe/util/bbox_util.hpp"
19  #ifdef USE_MLSL
20  #include "caffe/multinode/mlsl.hpp"
21  #include "caffe/multinode/multi_sync.hpp"
22  #include "caffe/multinode/async_param_server.hpp"
23  #endif &bsol;* USE_MLSL */
24  using caffe::Blob;
25  using caffe::Caffe;
26  using caffe::Net;
27  using caffe::Layer;
28  using caffe::Solver;
29  using caffe::shared_ptr;
30  using caffe::string;
31  using caffe::Timer;
32  using caffe::vector;
33  using std::ostringstream;
34  DEFINE_string(gpu, "",
35      "Optional; run in GPU mode on given device IDs separated by ','."
36      "Use '-gpu all' to run on all available GPUs. The effective training "
37      "batch size is multiplied by the number of devices.");
38  DEFINE_string(solver, "",
39      "The solver definition protocol buffer text file.");
40  DEFINE_string(model, "",
41      "The model definition protocol buffer text file.");
42  DEFINE_string(phase, "",
43      "Optional; network phase (TRAIN or TEST). Only used for 'time'.");
44  DEFINE_int32(level, 0,
45      "Optional; network level.");
46  DEFINE_string(stage, "",
47      "Optional; network stages (not to be confused with phase), "
48      "separated by ','.");
49  DEFINE_string(snapshot, "",
50      "Optional; the snapshot solver state to resume training.");
51  DEFINE_string(weights, "",
52      "Optional; the pretrained weights to initialize finetuning, "
53      "separated by ','. Cannot be set simultaneously with snapshot.");
54  DEFINE_int32(iterations, 50,
55      "The number of iterations to run.");
56  DEFINE_string(sigint_effect, "stop",
57               "Optional; action to take when a SIGINT signal is received: "
58                "snapshot, stop or none.");
59  DEFINE_string(sighup_effect, "snapshot",
60               "Optional; action to take when a SIGHUP signal is received: "
61               "snapshot, stop or none.");
62  DEFINE_bool(forward_only, false,
63      "Optional; Execute only forward pass");
64  DEFINE_string(engine, "",
65      "Optional; Engine sequence in format: engine:subengine_1,subengine_2,...");
66  DEFINE_string(collect_dir, "collect_out",
67      "Optional; Directory with reference binary files");
68  DEFINE_string(compare_output_dir, "compare_out",
69      "Optional; Directory with output files");
70  DEFINE_double(epsilon, 1e-3, "Optional; Layer output comparison error");
71  DEFINE_bool(detection, false,
72      "Optional; Enables detection for testing. "
73      "By default it is false and classification is on.");
74  DEFINE_bool(fast_compare, false,
75      "Optional; Break layer comparison after fast_compare_max errors found");
76  DEFINE_bool(sampling, false,
77      "Optional; Caffe test with sampling mode");
78  DEFINE_int32(fast_compare_max, 50,
79      "Optional; Max errors for fast_compare");
80  DEFINE_double(buffer_filler, std::nanf(""), "Buffer filler for compare tool");
81  DEFINE_int32(n_group, 1, "Optional; if given, it specifies how many trees"
82               " we want in the async forest");
83  DEFINE_int32(n_server, 0, "Optional; if given, it specifies how many parts"
84               "The model is splited to. I.e. how many process you have for param server");
85  typedef int (*BrewFunction)();
86  typedef std::map<caffe::string, BrewFunction> BrewMap;
87  BrewMap g_brew_map;
88  #define RegisterBrewFunction(func) \
89  namespace { \
90  class __Registerer_##func { \
91   public: &bsol;* NOLINT */ \
92    __Registerer_##func() { \
93      g_brew_map[#func] = &func; \
94    } \
95  }; \
96  __Registerer_##func g_registerer_##func; \
97  }
98  static BrewFunction GetBrewFunction(const caffe::string& name) {
99    if (g_brew_map.count(name)) {
100      return g_brew_map[name];
101    } else {
102      LOG(ERROR) << "Available caffe actions:";
103      for (BrewMap::iterator it = g_brew_map.begin();
104           it != g_brew_map.end(); ++it) {
105        LOG(ERROR) << "\t" << it->first;
106      }
107      LOG(FATAL) << "Unknown action: " << name;
108      return NULL;  
109    }
110  }
111  static void get_gpus(vector<int>* gpus) {
112    if (FLAGS_gpu == "all") {
113      int count = 0;
114  #ifndef CPU_ONLY
115      CUDA_CHECK(cudaGetDeviceCount(&count));
116  #else
117      NO_GPU;
118  #endif
119      for (int i = 0; i < count; ++i) {
120        gpus->push_back(i);
121      }
122    } else if (FLAGS_gpu.size()) {
123      vector<string> strings;
124      boost::split(strings, FLAGS_gpu, boost::is_any_of(","));
125      for (int i = 0; i < strings.size(); ++i) {
126        gpus->push_back(boost::lexical_cast<int>(strings[i]));
127      }
128    } else {
129      CHECK_EQ(gpus->size(), 0);
130    }
131  }
132  caffe::Phase get_phase_from_flags(caffe::Phase default_value) {
133    if (FLAGS_phase == "")
134      return default_value;
135    if (FLAGS_phase == "TRAIN")
136      return caffe::TRAIN;
137    if (FLAGS_phase == "TEST")
138      return caffe::TEST;
139    LOG(FATAL) << "phase must be \"TRAIN\" or \"TEST\"";
140    return caffe::TRAIN;  
141  }
142  int device_query() {
143    LOG(INFO) << "Querying GPUs " << FLAGS_gpu;
144    vector<int> gpus;
145    get_gpus(&gpus);
146    for (int i = 0; i < gpus.size(); ++i) {
147      caffe::Caffe::SetDevice(gpus[i]);
148      caffe::Caffe::DeviceQuery();
149    }
150    return 0;
151  }
152  RegisterBrewFunction(device_query);
153  void CopyLayers(caffe::Solver<float>* solver, const std::string& model_list) {
154    std::vector<std::string> model_names;
155    boost::split(model_names, model_list, boost::is_any_of(",") );
156    for (int i = 0; i < model_names.size(); ++i) {
157      LOG(INFO) << "Finetuning from " << model_names[i];
158      solver->net()->CopyTrainedLayersFrom(model_names[i]);
159      for (int j = 0; j < solver->test_nets().size(); ++j) {
160        solver->test_nets()[j]->CopyTrainedLayersFrom(model_names[i]);
161      }
162    }
163  }
164  caffe::SolverAction::Enum GetRequestedAction(
165      const std::string& flag_value) {
166    if (flag_value == "stop") {
167      return caffe::SolverAction::STOP;
168    }
169    if (flag_value == "snapshot") {
170      return caffe::SolverAction::SNAPSHOT;
171    }
172    if (flag_value == "none") {
173      return caffe::SolverAction::NONE;
174    }
175    LOG(FATAL) << "Invalid signal effect \""<< flag_value << "\" was specified";
176    return caffe::SolverAction::UNKNOWN;
177  }
178  int train() {
179    CHECK_GT(FLAGS_solver.size(), 0) << "Need a solver definition to train.";
180    CHECK(!FLAGS_snapshot.size() || !FLAGS_weights.size())
181        << "Give a snapshot to resume training or weights to finetune "
182        "but not both.";
183    caffe::SolverParameter solver_param;
184    if (!caffe::ReadProtoFromTextFile(FLAGS_solver, &solver_param)) {
185      caffe::MultiPhaseSolverParameter multi_solver_params;
186      CHECK(caffe::ReadProtoFromTextFile(FLAGS_solver, &multi_solver_params))
187        << "Failed to parse SolverParameter file: "  <<  FLAGS_solver;
188      return multiphase_train(
189        &multi_solver_params,
190        FLAGS_solver,
191        FLAGS_engine,
192        FLAGS_level,
193        FLAGS_stage);
194    }
195    use_flags(
196      &solver_param,
197      FLAGS_solver,
198      FLAGS_engine,
199      FLAGS_level,
200      FLAGS_stage);
201    if (FLAGS_gpu.size() == 0
202        && solver_param.solver_mode() == caffe::SolverParameter_SolverMode_GPU) {
203        if (solver_param.has_device_id()) {
204            FLAGS_gpu = "" +
205                boost::lexical_cast<string>(solver_param.device_id());
206        } else {  
207            FLAGS_gpu = "" + boost::lexical_cast<string>(0);
208        }
209    }
210    vector<int> gpus;
211    get_gpus(&gpus);
212    if (gpus.size() == 0) {
213      LOG(INFO) << "Use CPU.";
214      Caffe::set_mode(Caffe::CPU);
215    } else {
216      ostringstream s;
217      for (int i = 0; i < gpus.size(); ++i) {
218        s << (i ? ", " : "") << gpus[i];
219      }
220      LOG(INFO) << "Using GPUs " << s.str();
221  #ifndef CPU_ONLY
222      cudaDeviceProp device_prop;
223      for (int i = 0; i < gpus.size(); ++i) {
224        cudaGetDeviceProperties(&device_prop, gpus[i]);
225        LOG(INFO) << "GPU " << gpus[i] << ": " << device_prop.name;
226      }
227  #endif
228      solver_param.set_device_id(gpus[0]);
229      Caffe::SetDevice(gpus[0]);
230      Caffe::set_mode(Caffe::GPU);
231      Caffe::set_solver_count(gpus.size());
232    }
233    caffe::SignalHandler signal_handler(
234          GetRequestedAction(FLAGS_sigint_effect),
235          GetRequestedAction(FLAGS_sighup_effect));
236    shared_ptr<caffe::Solver<float> >
237        solver(caffe::SolverRegistry<float>::CreateSolver(solver_param));
238    solver->SetActionFunction(signal_handler.GetActionFunction());
239    if (FLAGS_snapshot.size()) {
240      LOG(INFO) << "Resuming from " << FLAGS_snapshot;
241      solver->Restore(FLAGS_snapshot.c_str());
242    } else if (FLAGS_weights.size()) {
243      CopyLayers(solver.get(), FLAGS_weights);
244    }
245  #ifdef USE_MLSL
246    if (caffe::mn::is_multinode()) {
247      caffe::mn::barrier();
248      LOG(INFO) << "Configuring multinode setup";
249      if (!caffe::mn::is_param_server()) {
250        caffe::MultiSync<float> sync(solver);
251        LOG(INFO) << "Starting Multi-node Optimization in MLSL environment";
252        sync.run();
253      } else {
254        caffe::mn::AsyncParamServer<float> aps(solver);
255        LOG(INFO) << "Starting Parameter Server";
256        aps.Run();
257      }
258    } else
259  #endif &bsol;* USE_MLSL */
260    if (gpus.size() > 1) {
261      caffe::P2PSync<float> sync(solver, NULL, solver->param());
262      sync.Run(gpus);
263    } else {
264      LOG(INFO) << "Starting Optimization";
265      solver->Solve();
266    }
267    LOG(INFO) << "Optimization Done.";
268    return 0;
269  }
270  RegisterBrewFunction(train);
271  int test_detection(Net<float>& caffe_net) {
272    std::map<int, std::map<int,
273      std::vector<std::pair<float, int> > > > all_true_pos;
274    std::map<int, std::map<int,
275      std::vector<std::pair<float, int> > > > all_false_pos;
276    std::map<int, std::map<int, int> > all_num_pos;
277    PERFORMANCE_INIT_MONITOR();
278    for (int i = 0; i < FLAGS_iterations; ++i) {
279  #ifdef DEBUG
280      LOG(INFO) << "Iteration: " << i;
281  #endif
282      float iter_loss;
283      const vector<Blob<float>*>& result = caffe_net.Forward(&iter_loss);
284      for (int j = 0; j < result.size(); ++j) {
285        const float* result_vec = result[j]->cpu_data();
286        int num_det = result[j]->height();
287        for (int k = 0; k < num_det; ++k) {
288          int item_id = static_cast<int>(result_vec[k * 5]);
289          int label = static_cast<int>(result_vec[k * 5 + 1]);
290          if (item_id == -1) {
291            if (all_num_pos[j].find(label) == all_num_pos[j].end()) {
292              all_num_pos[j][label] = static_cast<int>(result_vec[k * 5 + 2]);
293            } else {
294              all_num_pos[j][label] += static_cast<int>(result_vec[k * 5 + 2]);
295            }
296          } else {
297            float score = result_vec[k * 5 + 2];
298            int tp = static_cast<int>(result_vec[k * 5 + 3]);
299            int fp = static_cast<int>(result_vec[k * 5 + 4]);
300            if (tp == 0 && fp == 0) {
301              continue;
302            }
303            all_true_pos[j][label].push_back(std::make_pair(score, tp));
304            all_false_pos[j][label].push_back(std::make_pair(score, fp));
305          }
306        }
307      }
308    }
309    for (int i = 0; i < all_true_pos.size(); ++i) {
310      if (all_true_pos.find(i) == all_true_pos.end()) {
311        LOG(FATAL) << "Missing output_blob true_pos: " << i;
312      }
313      const std::map<int, std::vector<std::pair<float, int> > >& true_pos =
314          all_true_pos.find(i)->second;
315      if (all_false_pos.find(i) == all_false_pos.end()) {
316        LOG(FATAL) << "Missing output_blob false_pos: " << i;
317      }
318      const std::map<int, std::vector<std::pair<float, int> > >& false_pos =
319          all_false_pos.find(i)->second;
320      if (all_num_pos.find(i) == all_num_pos.end()) {
321        LOG(FATAL) << "Missing output_blob num_pos: " << i;
322      }
323      const std::map<int, int>& num_pos = all_num_pos.find(i)->second;
324      std::map<int, float> APs;
325      float mAP = 0.;
326      for (std::map<int, int>::const_iterator it = num_pos.begin();
327           it != num_pos.end(); ++it) {
328        int label = it->first;
329        int label_num_pos = it->second;
330        if (true_pos.find(label) == true_pos.end()) {
331          LOG(WARNING) << "Missing true_pos for label: " << label;
332          continue;
333        }
334        const std::vector<std::pair<float, int> >& label_true_pos =
335            true_pos.find(label)->second;
336        if (false_pos.find(label) == false_pos.end()) {
337          LOG(WARNING) << "Missing false_pos for label: " << label;
338          continue;
339        }
340        const std::vector<std::pair<float, int> >& label_false_pos =
341            false_pos.find(label)->second;
342        std::vector<float> prec, rec;
343        caffe::ComputeAP(label_true_pos, label_num_pos, label_false_pos,
344                  "11point", &prec, &rec, &(APs[label]));
345        mAP += APs[label];
346      }
347      mAP /= num_pos.size();
348      const int output_blob_index = caffe_net.output_blob_indices()[i];
349      const string& output_name = caffe_net.blob_names()[output_blob_index];
350      LOG(INFO) << "    Test net output #" << i << ": " << output_name << " = "
351                << mAP;
352    }
353    return 0;
354  }
355  int test() {
356    CHECK_GT(FLAGS_model.size(), 0) << "Need a model definition to score.";
357    CHECK_GT(FLAGS_weights.size(), 0) << "Need model weights to score.";
358    vector<string> stages = get_stages_from_flags(FLAGS_stage);
359    vector<int> gpus;
360    get_gpus(&gpus);
361    if (gpus.size() != 0) {
362      LOG(INFO) << "Use GPU with device ID " << gpus[0];
363  #ifndef CPU_ONLY
364      cudaDeviceProp device_prop;
365      cudaGetDeviceProperties(&device_prop, gpus[0]);
366      LOG(INFO) << "GPU device name: " << device_prop.name;
367  #endif
368      Caffe::SetDevice(gpus[0]);
369      Caffe::set_mode(Caffe::GPU);
370    } else {
371      LOG(INFO) << "Use CPU.";
372      Caffe::set_mode(Caffe::CPU);
373    }
374    Net<float> caffe_net(FLAGS_model, caffe::TEST, FLAGS_level, &stages, NULL,
375                         FLAGS_engine);
376    caffe_net.CopyTrainedLayersFrom(FLAGS_weights);
377    if (FLAGS_sampling)
378        return 0;
379    LOG(INFO) << "Running for " << FLAGS_iterations << " iterations.";
380    if (FLAGS_detection) {
381      test_detection(caffe_net);
382      return 0;
383    }
384    vector<int> test_score_output_id;
385    vector<float> test_score;
386    float loss = 0;
387    for (int i = 0; i < FLAGS_iterations; ++i) {
388      float iter_loss;
389      const vector<Blob<float>*>& result =
390          caffe_net.Forward(&iter_loss);
391      loss += iter_loss;
392      int idx = 0;
393      for (int j = 0; j < result.size(); ++j) {
394        const float* result_vec = result[j]->cpu_data();
395        for (int k = 0; k < result[j]->count(); ++k, ++idx) {
396          const float score = result_vec[k];
397          if (i == 0) {
398            test_score.push_back(score);
399            test_score_output_id.push_back(j);
400          } else {
401            test_score[idx] += score;
402          }
403           const std::string& output_name = caffe_net.blob_names()[
404              caffe_net.output_blob_indices()[j]];
405          LOG(INFO) << "Batch " << i << ", " << output_name << " = " << score;
406        }
407      }
408    }
409    loss /= FLAGS_iterations;
410    LOG(INFO) << "Loss: " << loss;
411    for (int i = 0; i < test_score.size(); ++i) {
412      const std::string& output_name = caffe_net.blob_names()[
413          caffe_net.output_blob_indices()[test_score_output_id[i]]];
414      const float loss_weight = caffe_net.blob_loss_weights()[
415          caffe_net.output_blob_indices()[test_score_output_id[i]]];
416      std::ostringstream loss_msg_stream;
417      const float mean_score = test_score[i] / FLAGS_iterations;
418      if (loss_weight) {
419        loss_msg_stream << " (* " << loss_weight
420                        << " = " << loss_weight * mean_score << " loss)";
421      }
422      LOG(INFO) << output_name << " = " << mean_score << loss_msg_stream.str();
423    }
424    return 0;
425  }
426  RegisterBrewFunction(test);
427  int time() {
428    CHECK_GT(FLAGS_model.size(), 0) << "Need a model definition to time.";
429    caffe::Phase phase = get_phase_from_flags(caffe::TRAIN);
430    vector<string> stages = get_stages_from_flags(FLAGS_stage);
431    vector<int> gpus;
432    get_gpus(&gpus);
433    if (gpus.size() != 0) {
434      LOG(INFO) << "Use GPU with device ID " << gpus[0];
435      Caffe::SetDevice(gpus[0]);
436      Caffe::set_mode(Caffe::GPU);
437    } else {
438      LOG(INFO) << "Use CPU.";
439      Caffe::set_mode(Caffe::CPU);
440    }
441    Net<float> caffe_net(FLAGS_model, phase, FLAGS_level, &stages, NULL,
442                         FLAGS_engine);
443    PERFORMANCE_INIT_MONITOR();
444    LOG(INFO) << "Performing Forward";
445    float initial_loss;
446    caffe_net.Forward(&initial_loss);
447    LOG(INFO) << "Initial loss: " << initial_loss;
448    if (!FLAGS_forward_only) {
449      LOG(INFO) << "Performing Backward";
450      caffe_net.Backward();
451    }
452    const vector<shared_ptr<Layer<float> > >& layers = caffe_net.layers();
453    const vector<vector<Blob<float>*> >& bottom_vecs = caffe_net.bottom_vecs();
454    const vector<vector<Blob<float>*> >& top_vecs = caffe_net.top_vecs();
455    const vector<vector<bool> >& bottom_need_backward =
456        caffe_net.bottom_need_backward();
457    int warmup_iterations = 5;
458    for (int j = 0; j < warmup_iterations; ++j) {
459      if (j == warmup_iterations - 1)
460        PERFORMANCE_START_RESETTING_MONITOR();
461      for (int i = 0; i < layers.size(); ++i) {
462        layers[i]->Forward(bottom_vecs[i], top_vecs[i]);
463      }
464      if (!FLAGS_forward_only) {
465        for (int i = layers.size() - 1; i >= 0; --i) {
466          layers[i]->Backward(top_vecs[i], bottom_need_backward[i],
467                              bottom_vecs[i]);
468        }
469      }
470    }
471    PERFORMANCE_STOP_RESETTING_MONITOR();
472    LOG(INFO) << "*** Benchmark begins ***";
473    LOG(INFO) << "Testing for " << FLAGS_iterations << " iterations.";
474    Timer total_timer;
475    total_timer.Start();
476    Timer forward_timer;
477    Timer backward_timer;
478    Timer timer;
479    std::vector<double> forward_time_per_layer(layers.size(), 0.0);
480    std::vector<double> backward_time_per_layer(layers.size(), 0.0);
481    double forward_time = 0.0;
482    std::vector<double> forward_time_iteration(FLAGS_iterations, 0.0);
483    double backward_time = 0.0;
484    for (int j = 0; j < FLAGS_iterations; ++j) {
485      Timer iter_timer;
486      iter_timer.Start();
487      forward_timer.Start();
488      for (int i = 0; i < layers.size(); ++i) {
489        timer.Start();
490        layers[i]->Forward(bottom_vecs[i], top_vecs[i]);
491        forward_time_per_layer[i] += timer.MicroSeconds();
492      }
493      forward_time_iteration[j] = forward_timer.MicroSeconds();
<span onclick='openModal()' class='match'>494      forward_time += forward_timer.MicroSeconds();
495      if (!FLAGS_forward_only) {
496        backward_timer.Start();
497        for (int i = layers.size() - 1; i >= 0; --i) {
498          timer.Start();
499          layers[i]->Backward(top_vecs[i], bottom_need_backward[i],
500                              bottom_vecs[i]);
501          backward_time_per_layer[i] += timer.MicroSeconds();
</span>502        }
503        backward_time += backward_timer.MicroSeconds();
504        LOG(INFO) << "Iteration: " << j + 1 << " forward-backward time: "
505          << iter_timer.MilliSeconds() << " ms.";
506      } else {
507        LOG(INFO) << "Iteration: " << j + 1 << " forward time: "
508          << iter_timer.MilliSeconds() << " ms.";
509      }
510    }
511    LOG(INFO) << "Average time per layer: ";
512    for (int i = 0; i < layers.size(); ++i) {
513      const caffe::string& layername = layers[i]->layer_param().name();
514      LOG(INFO) << std::setfill(' ') << std::setw(10) << layername <<
515        "\tforward: " << forward_time_per_layer[i] / 1000 /
516        FLAGS_iterations << " ms.";
517      if (!FLAGS_forward_only) {
518        LOG(INFO) << std::setfill(' ') << std::setw(10) << layername  <<
519          "\tbackward: " << backward_time_per_layer[i] / 1000 /
520          FLAGS_iterations << " ms.";
521      }
522    }
523    total_timer.Stop();
524    for (int j = 0; j < FLAGS_iterations; ++j)
525      LOG(INFO) << "###" << j << ":" << forward_time_iteration[j] / 1000 << " ms.";
526    LOG(INFO) << "Average Forward pass: " << forward_time / 1000 /
527      FLAGS_iterations << " ms.";
528    if (!FLAGS_forward_only) {
529      LOG(INFO) << "Average Backward pass: " << backward_time / 1000 /
530        FLAGS_iterations << " ms.";
531      LOG(INFO) << "Average Forward-Backward: " << total_timer.MilliSeconds() /
532        FLAGS_iterations << " ms.";
533    }
534    LOG(INFO) << "Total Time: " << total_timer.MilliSeconds() << " ms.";
535    LOG(INFO) << "*** Benchmark ends ***";
536    return 0;
537  }
538  RegisterBrewFunction(time);
539  #include <stdio.h>
540  #include "caffe/util/compareToolUtilities.h"
541  int collect() {
542    #ifndef DETERMINISTIC
543      LOG(ERROR) << "Recompile caffe with DETERMINISTIC to run collect tool";
544      return 1;
545    #endif
546    CHECK_GT(FLAGS_model.size(), 0) << "Need a model definition!";
547    vector<int> gpus;
548    get_gpus(&gpus);
549    bool use_gpu = (gpus.size() != 0);
550    if (use_gpu) {
551      LOG(INFO) << "Use GPU with device ID " << gpus[0];
552      Caffe::SetDevice(gpus[0]);
553      Caffe::set_mode(Caffe::GPU);
554    } else {
555      LOG(INFO) << "Use CPU.";
556      Caffe::set_mode(Caffe::CPU);
557    }
558    boost::filesystem::path dir(FLAGS_collect_dir);
559    if (!boost::filesystem::exists(dir)) {
560        if (!boost::filesystem::create_directory(dir)) {
561            LOG(ERROR) << "Could not create directory for output files";
562        }
563    }
564    return collectAndCheckLayerData(true, use_gpu, FLAGS_collect_dir.c_str());
565  }
566  RegisterBrewFunction(collect);
567  int compare() {
568    #ifndef DETERMINISTIC
569      LOG(ERROR) << "Recompile caffe with DETERMINISTIC to run compare tool";
570      return 1;
571    #endif
572    CHECK_GT(FLAGS_model.size(), 0) << "Need a model definition!";
573    vector<int> gpus;
574    get_gpus(&gpus);
575    bool use_gpu = (gpus.size() != 0);
576    if (use_gpu) {
577      LOG(INFO) << "Use GPU with device ID " << gpus[0];
578      Caffe::SetDevice(gpus[0]);
579      Caffe::set_mode(Caffe::GPU);
580    } else {
581      LOG(INFO) << "Use CPU.";
582      Caffe::set_mode(Caffe::CPU);
583    }
584    boost::filesystem::path dir(FLAGS_compare_output_dir);
585    if (!boost::filesystem::exists(dir)) {
586        if (!boost::filesystem::create_directory(dir)) {
587            LOG(ERROR) << "Could not create directory for output files";
588        }
589    }
590    return collectAndCheckLayerData(false,
591      use_gpu, FLAGS_compare_output_dir.c_str());
592  }
593  RegisterBrewFunction(compare);
594  int main(int argc, char** argv) {
595    FLAGS_alsologtostderr = 1;
596    gflags::SetVersionString(AS_STRING(CAFFE_VERSION));
597    gflags::SetUsageMessage("command line brew\n"
598        "usage: caffe <command> <args>\n\n"
599        "commands:\n"
600        "  train           train or finetune a model\n"
601        "  test            score a model\n"
602        "  device_query    show GPU diagnostic information\n"
603        "  time            benchmark model execution time\n"
604        "  collect         collects layer data on specified device\n"
605        "  compare         collects layer data using inputs from other device");
606    caffe::GlobalInit(&argc, &argv);
607  #ifdef USE_MLSL
608    caffe::mn::nGroup = FLAGS_n_group;
609    if (caffe::mn::nGroup <= 0) {
610        LOG(ERROR) << "Invalid number of group: " << caffe::mn::nGroup;
611        return 1;
612    }
613    caffe::mn::nServer = FLAGS_n_server;
614    caffe::mn::init(&argc, &argv);
615    if (caffe::mn::get_group_size() <= 0) {
616        LOG(ERROR) << "Invalid group size: " << caffe::mn::get_group_size();
617        return 1;
618    }
619    CHECK_EQ(caffe::mn::get_world_size(),
620             caffe::mn::nGroup * caffe::mn::get_group_size() + caffe::mn::nServer);
621    if (caffe::mn::nGroup > 1) {
622      CHECK_GE(caffe::mn::nServer, 1)
623        << "Expect there exists parameter server to support multiple groups";
624    }
625    if (caffe::mn::get_node_rank() == 0) {
626      LOG(INFO) << "Number of groups: " << caffe::mn::nGroup
627                << ", group size: " << caffe::mn::get_group_size()
628                << ", number of parameter servers: " << caffe::mn::nServer;
629    }
630  #endif
631    if (argc == 2) {
632  #ifdef WITH_PYTHON_LAYER
633      try {
634  #endif
635        int ret = GetBrewFunction(caffe::string(argv[1]))();
636        return ret;
637  #ifdef WITH_PYTHON_LAYER
638      } catch (bp::error_already_set) {
639        PyErr_Print();
640        return 1;
641      }
642  #endif
643    } else {
644      gflags::ShowUsageWithFlagsRestrict(argv[0], "tools/caffe");
645    }
646    return 0;
647  }
</code></pre>
        </div>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-flx.cpp</h3>
            <pre><code>1  const int TCs::MxMask=0x0FFFFFFF;
2  TCs TCs::GetCsFromBf(char* Bf, const int& BfL){
3    TCs Cs;
4    for (int BfC=0; BfC<BfL; BfC++){Cs+=Bf[BfC];}
5    return Cs;
6  }
7  TStr TSBase::GetSNm() const {
8    return TStr(SNm.CStr());
9  }
10  TSIn::TSIn(const TStr& Str):
11    TSBase(Str.CStr()), FastMode(false){}
12  void TSIn::LoadCs(){
13    TCs CurCs=Cs; TCs TestCs;
14    Cs+=GetBf(&TestCs, sizeof(TestCs));
15    EAssertR(CurCs==TestCs, "Invalid checksum reading '"+GetSNm()+"'.");
16  }
17  void TSIn::Load(char*& CStr){
18    char Ch; Load(Ch);
19    int CStrLen=int(Ch);
20    EAssertR(CStrLen>=0, "Error reading stream '"+GetSNm()+"'.");
21    CStr=new char[CStrLen+1];
22    if (CStrLen>0){Cs+=GetBf(CStr, CStrLen);}
23    CStr[CStrLen]=TCh::NullCh;
24  }
25  const PSIn TSIn::StdIn=PSIn(new TStdIn());
26  TSOut::TSOut(const TStr& Str):
27    TSBase(Str.CStr()), MxLnLen(-1), LnLen(0){}
28  int TSOut::UpdateLnLen(const int& StrLen, const bool& ForceInLn){
29    int Cs=0;
30    if (MxLnLen!=-1){
31      if ((!ForceInLn)&&(LnLen+StrLen>MxLnLen)){Cs+=PutLn();}
32      LnLen+=StrLen;
33    }
34    return Cs;
35  }
36  int TSOut::PutMem(const TMem& Mem){
37    return PutBf(Mem(), Mem.Len());
38  }
39  int TSOut::PutCh(const char& Ch, const int& Chs){
40    int Cs=0;
41    for (int ChN=0; ChN<Chs; ChN++){Cs+=PutCh(Ch);}
42    return Cs;
43  }
44  int TSOut::PutBool(const bool& Bool){
45    return PutStr(TBool::GetStr(Bool));
46  }
47  int TSOut::PutInt(const int& Int){
48    return PutStr(TInt::GetStr(Int));
49  }
50  int TSOut::PutInt(const int& Int, char* FmtStr){
51    return PutStr(TInt::GetStr(Int, FmtStr));
52  }
53  int TSOut::PutUInt(const uint& UInt){
54    return PutStr(TUInt::GetStr(UInt));
55  }
56  int TSOut::PutUInt(const uint& UInt, char* FmtStr){
57    return PutStr(TUInt::GetStr(UInt, FmtStr));
58  }
59  int TSOut::PutFlt(const double& Flt){
60    return PutStr(TFlt::GetStr(Flt));
61  }
62  int TSOut::PutFlt(const double& Flt, char* FmtStr){
63    return PutStr(TFlt::GetStr(Flt, FmtStr));
64  }
65  int TSOut::PutStr(const char* CStr){
66    int Cs=UpdateLnLen(int(strlen(CStr)));
67    return Cs+PutBf(CStr, int(strlen(CStr)));
68  }
69  int TSOut::PutStr(const TChA& ChA){
70    int Cs=UpdateLnLen(ChA.Len());
71    return Cs+PutBf(ChA.CStr(), ChA.Len());
72  }
73  int TSOut::PutStr(const TStr& Str, char* FmtStr){
74    return PutStr(TStr::GetStr(Str, FmtStr));
75  }
76  int TSOut::PutStr(const TStr& Str, const bool& ForceInLn){
77    int Cs=UpdateLnLen(Str.Len(), ForceInLn);
78    return Cs+PutBf(Str.CStr(), Str.Len());
79  }
80  int TSOut::PutIndent(const int& IndentLev){
81    return PutCh(' ', IndentLev*2);
82  }
83  int TSOut::PutLn(const int& Lns){
84    LnLen=0; int Cs=0;
85    for (int LnN=0; LnN<Lns; LnN++){Cs+=PutCh('\n');}
86    return Cs;
87  }
88  int TSOut::PutDosLn(const int& Lns){
89    LnLen=0; int Cs=0;
90    for (int LnN=0; LnN<Lns; LnN++){Cs+=PutCh(TCh::CrCh)+PutCh(TCh::LfCh);}
91    return Cs;
92  }
93  int TSOut::PutSep(const int& NextStrLen){
94    int Cs=0;
95    if (MxLnLen==-1){
96      Cs+=PutCh(' ');
97    } else {
98      if (LnLen>0){
99        if (LnLen+1+NextStrLen>MxLnLen){Cs+=PutLn();} else {Cs+=PutCh(' ');}
100      }
101    }
102    return Cs;
103  }
104  int TSOut::PutSepLn(const int& Lns){
105    int Cs=0;
106    if (LnLen>0){Cs+=PutLn();}
107    Cs+=PutLn(Lns);
108    return Cs;
109  }
110  void TSOut::Save(const char* CStr){
111    int CStrLen=int(strlen(CStr));
112    EAssertR(CStrLen<=127, "Error writting stream '"+GetSNm()+"'.");
113    Save(char(CStrLen));
114    if (CStrLen>0){Cs+=PutBf(CStr, CStrLen);}
115  }
116  void TSOut::Save(TSIn& SIn, const int& BfL){
117    if (BfL==-1){
118      while (!SIn.Eof()){Save(SIn.GetCh());}
119    } else {
120      for (int BfC=0; BfC<BfL; BfC++){Save(SIn.GetCh());}
121    }
122  }
123  TSOut& TSOut::operator<<(TSIn& SIn) {
124    while (!SIn.Eof())
125      operator<<((char)SIn.GetCh());
126    return *this;
127  }
128  const PSOut TSOut::StdOut=PSOut(new TStdOut());
129  int TStdIn::GetBf(const void* LBf, const int& LBfL){
130    int LBfS=0;
131    for (int LBfC=0; LBfC<LBfL; LBfC++){
132      LBfS+=(((char*)LBf)[LBfC]=GetCh());}
133    return LBfS;
134  }
135  int TStdOut::PutBf(const void* LBf, const int& LBfL){
136    int LBfS=0;
137    for (int LBfC=0; LBfC<LBfL; LBfC++){
138      LBfS+=PutCh(((char*)LBf)[LBfC]);}
139    return LBfS;
140  }
141  const int TFIn::MxBfL=16*1024;
142  void TFIn::SetFPos(const int& FPos) const {
143    EAssertR(
144     fseek(FileId, FPos, SEEK_SET)==0,
145     "Error seeking into file '"+GetSNm()+"'.");
146  }
147  int TFIn::GetFPos() const {
148    const int FPos=ftell(FileId);
149    EAssertR(FPos!=-1, "Error seeking into file '"+GetSNm()+"'.");
150    return FPos;
151  }
152  int TFIn::GetFLen() const {
153    const int FPos=GetFPos();
154    EAssertR(
155     fseek(FileId, 0, SEEK_END)==0,
156     "Error seeking into file '"+GetSNm()+"'.");
157    const int FLen=GetFPos(); SetFPos(FPos);
158    return FLen;
159  }
160  void TFIn::FillBf(){
161    EAssertR(
162     (BfC==BfL)&&((BfL==-1)||(BfL==MxBfL)),
163     "Error reading file '"+GetSNm()+"'.");
164    BfL=int(fread(Bf, 1, MxBfL, FileId));
165    EAssertR((BfC!=0)||(BfL!=0), "Error reading file '"+GetSNm()+"'.");
166    BfC=0;
167  }
168  TFIn::TFIn(const TStr& FNm):
169    TSIn(FNm), FileId(NULL), Bf(NULL), BfC(0), BfL(0){
170    EAssertR(!FNm.Empty(), "Empty file-name.");
171    FileId=fopen(FNm.CStr(), "rb");
172    EAssertR(FileId!=NULL, "Can not open file '"+FNm+"'.");
173    Bf=new char[MxBfL]; BfC=BfL=-1; FillBf();
174  }
175  TFIn::TFIn(const TStr& FNm, bool& OpenedP):
176    TSIn(FNm), FileId(NULL), Bf(NULL), BfC(0), BfL(0){
177    EAssertR(!FNm.Empty(), "Empty file-name.");
178    FileId=fopen(FNm.CStr(), "rb");
179    OpenedP=(FileId!=NULL);
180    if (OpenedP){
181      Bf=new char[MxBfL]; BfC=BfL=-1; FillBf();}
182  }
183  PSIn TFIn::New(const TStr& FNm){
184    return PSIn(new TFIn(FNm));
185  }
186  PSIn TFIn::New(const TStr& FNm, bool& OpenedP){
187    return PSIn(new TFIn(FNm, OpenedP));
188  }
189  TFIn::~TFIn(){
190    if (FileId!=NULL){
191      EAssertR(fclose(FileId)==0, "Can not close file '"+GetSNm()+"'.");}
192    if (Bf!=NULL){delete[] Bf;}
193  }
194  int TFIn::GetBf(const void* LBf, const int& LBfL){
195    int LBfS=0;
196    if (BfC+LBfL>BfL){
197      for (int LBfC=0; LBfC<LBfL; LBfC++){
198        if (BfC==BfL){FillBf();}
199        LBfS+=((char*)LBf)[LBfC]=Bf[BfC++];}
200    } else {
201      for (int LBfC=0; LBfC<LBfL; LBfC++){
202        LBfS+=(((char*)LBf)[LBfC]=Bf[BfC++]);}
203    }
204    return LBfS;
205  }
206  const int TFOut::MxBfL=16*1024;;
207  void TFOut::FlushBf(){
208    EAssertR(
209     (int)fwrite(Bf, 1, BfL, FileId)==BfL,
210     "Error writting to the file '"+GetSNm()+"'.");
211    BfL=0;
212  }
213  TFOut::TFOut(const TStr& FNm, const bool& Append):
214    TSOut(FNm), FileId(NULL), Bf(NULL), BfL(0){
215    if (FNm.GetUc()=="CON"){
216      FileId=stdout;
217    } else {
218      if (Append){FileId=fopen(FNm.CStr(), "a+b");}
219      else {FileId=fopen(FNm.CStr(), "w+b");}
220      EAssertR(FileId!=NULL, "Can not open file '"+FNm+"'.");
221      Bf=new char[MxBfL]; BfL=0;
222    }
223  }
224  TFOut::TFOut(const TStr& FNm, const bool& Append, bool& OpenedP):
225    TSOut(FNm), FileId(NULL), Bf(NULL), BfL(0){
226    if (FNm.GetUc()=="CON"){
227      FileId=stdout;
228    } else {
229      if (Append){FileId=fopen(FNm.CStr(), "a+b");}
230      else {FileId=fopen(FNm.CStr(), "w+b");}
231      OpenedP=(FileId!=NULL);
232      if (OpenedP){
233        Bf=new char[MxBfL]; BfL=0;}
234    }
235  }
236  PSOut TFOut::New(const TStr& FNm, const bool& Append){
237    return PSOut(new TFOut(FNm, Append));
238  }
239  PSOut TFOut::New(const TStr& FNm, const bool& Append, bool& OpenedP){
240    PSOut SOut=PSOut(new TFOut(FNm, Append, OpenedP));
241    if (OpenedP){return SOut;} else {return NULL;}
242  }
243  TFOut::~TFOut(){
244    if (FileId!=NULL){FlushBf();}
245    if (Bf!=NULL){delete[] Bf;}
246    if (FileId!=NULL){
247      EAssertR(fclose(FileId)==0, "Can not close file '"+GetSNm()+"'.");}
248  }
249  int TFOut::PutCh(const char& Ch){
250    if (BfL==MxBfL){FlushBf();}
251    return Bf[BfL++]=Ch;
252  }
253  int TFOut::PutBf(const void* LBf, const int& LBfL){
<span onclick='openModal()' class='match'>254    int LBfS=0;
255    if (BfL+LBfL>MxBfL){
256      for (int LBfC=0; LBfC<LBfL; LBfC++){
257        LBfS+=PutCh(((char*)LBf)[LBfC]);}
</span>258    } else {
259      for (int LBfC=0; LBfC<LBfL; LBfC++){
260        LBfS+=(Bf[BfL++]=((char*)LBf)[LBfC]);}
261    }
262    return LBfS;
263  }
264  void TFOut::Flush(){
265    FlushBf();
266    EAssertR(fflush(FileId)==0, "Can not flush file '"+GetSNm()+"'.");
267  }
268  TFInOut::TFInOut(const TStr& FNm, const TFAccess& FAccess, const bool& CreateIfNo) :
269   TSBase(TSStr(FNm.CStr())), FileId(NULL) {
270    switch (FAccess){
271      case faCreate: FileId=fopen(FNm.CStr(), "w+b"); break;
272      case faUpdate: FileId=fopen(FNm.CStr(), "r+b"); break;
273      case faAppend: FileId=fopen(FNm.CStr(), "r+b");
274        if (FileId!=NULL){fseek(FileId, SEEK_END, 0);} break;
275      case faRdOnly: FileId=fopen(FNm.CStr(), "rb"); break;
276      default: Fail;
277    }
278    if ((FileId==NULL)&&(CreateIfNo)){FileId=fopen(FNm.CStr(), "w+b");}
279    IAssert(FileId!=NULL);
280  }
281  PSInOut TFInOut::New(const TStr& FNm, const TFAccess& FAccess, const bool& CreateIfNo) {
282    return PSInOut(new TFInOut(FNm, FAccess, CreateIfNo));
283  }
284  int TFInOut::GetSize() const {
285    const int FPos = GetPos();
286    IAssert(fseek(FileId, 0, SEEK_END) == 0);
287    const int FLen = GetPos();
288    IAssert(fseek(FileId, FPos, SEEK_SET) == 0);
289    return FLen;
290  }
291  int TFInOut::PutBf(const void* LBf, const int& LBfL) {
292    int LBfS = 0;
293    for (int i = 0; i < LBfL; i++) {
294      LBfS += ((char *)LBf)[i];
295    }
296    IAssert(fwrite(LBf, sizeof(char), LBfL, FileId) == (size_t) LBfL);
297    return LBfS;;
298  }
299  int TFInOut::GetBf(const void* LBf, const int& LBfL) {
300    IAssert(fread((void *)LBf, sizeof(char), LBfL, FileId) == (size_t) LBfL);
301    int LBfS = 0;
302    for (int i = 0; i < LBfL; i++) {
303      LBfS += ((char *)LBf)[i];
304    }
305    return LBfS;
306  }
307  TStr TFInOut::GetFNm() const {
308    return GetSNm();
309  }
310  TMIn::TMIn(const void* _Bf, const int& _BfL, const bool& TakeBf, const bool& _Free):
311    TSIn("Input-Memory"), Bf(NULL), BfC(0), BfL(_BfL), Free(_Free){
312    if (TakeBf){
313      Bf=(char*)_Bf;
314    } else {
315      Bf=new char[BfL]; memmove(Bf, _Bf, BfL);
316    }
317  }
318  TMIn::TMIn(TSIn& SIn):
319    TSIn("Input-Memory"), Bf(NULL), BfC(0), BfL(0){
320    BfL=SIn.Len(); Bf=new char[BfL];
321    for (int BfC=0; BfC<BfL; BfC++){Bf[BfC]=SIn.GetCh();}
322  }
323  TMIn::TMIn(const char* CStr):
324    TSIn("Input-Memory"), Bf(NULL), BfC(0), BfL(0){
325    BfL=int(strlen(CStr)); Bf=new char[BfL+1]; strcpy(Bf, CStr);
326  }
327  TMIn::TMIn(const TStr& Str):
328    TSIn("Input-Memory"), Bf(NULL), BfC(0), BfL(0){
329    BfL=Str.Len(); Bf=new char[BfL]; strncpy(Bf, Str.CStr(), BfL);
330  }
331  TMIn::TMIn(const TChA& ChA):
332    TSIn("Input-Memory"), Bf(NULL), BfC(0), BfL(0){
333    BfL=ChA.Len(); Bf=new char[BfL]; strncpy(Bf, ChA.CStr(), BfL);
334  }
335  PSIn TMIn::New(const char* CStr){
336    return PSIn(new TMIn(CStr));
337  }
338  PSIn TMIn::New(const TStr& Str){
339    return PSIn(new TMIn(Str));
340  }
341  PSIn TMIn::New(const TChA& ChA){
342    return PSIn(new TMIn(ChA));
343  }
344  char TMIn::GetCh(){
345    EAssertR(BfC<BfL, "Reading beyond the end of stream.");
346    return Bf[BfC++];
347  }
348  char TMIn::PeekCh(){
349    EAssertR(BfC<BfL, "Reading beyond the end of stream.");
350    return Bf[BfC];
351  }
352  int TMIn::GetBf(const void* LBf, const int& LBfL){
353    EAssertR(BfC+LBfL<=BfL, "Reading beyond the end of stream.");
354    int LBfS=0;
355    for (int LBfC=0; LBfC<LBfL; LBfC++){
356      LBfS+=(((char*)LBf)[LBfC]=Bf[BfC++]);}
357    return LBfS;
358  }
359  void TMOut::Resize(){
360    IAssert(OwnBf&&(BfL==MxBfL));
361    if (Bf==NULL){
362      IAssert(MxBfL==0); Bf=new char[MxBfL=1024];
363    } else {
364      MxBfL*=2; char* NewBf=new char[MxBfL];
365      memmove(NewBf, Bf, BfL); delete[] Bf; Bf=NewBf;
366    }
367  }
368  TMOut::TMOut(const int& _MxBfL):
369    TSOut("Output-Memory"),
370    Bf(NULL), BfL(0), MxBfL(0), OwnBf(true){
371    MxBfL=_MxBfL>0?_MxBfL:1024;
372    Bf=new char[MxBfL];
373  }
374  TMOut::TMOut(char* _Bf, const int& _MxBfL):
375    TSOut("Output-Memory"),
376    Bf(_Bf), BfL(0), MxBfL(_MxBfL), OwnBf(false){}
377  int TMOut::PutBf(const void* LBf, const int& LBfL){
378    int LBfS=0;
379    if (BfL+LBfL>MxBfL){
380      for (int LBfC=0; LBfC<LBfL; LBfC++){
381        LBfS+=PutCh(((char*)LBf)[LBfC]);}
382    } else {
383      for (int LBfC=0; LBfC<LBfL; LBfC++){
384        LBfS+=(Bf[BfL++]=((char*)LBf)[LBfC]);}
385    }
386    return LBfS;
387  }
388  TStr TMOut::GetAsStr() const {
389    TChA ChA(BfL);
390    for (int BfC=0; BfC<BfL; BfC++){ChA+=Bf[BfC];}
391    return ChA;
392  }
393  void TMOut::CutBf(const int& CutBfL){
394    IAssert((0<=CutBfL)&&(CutBfL<=BfL));
395    if (CutBfL==BfL){BfL=0;}
396    else {memmove(Bf, Bf+CutBfL, BfL-CutBfL); BfL=BfL-CutBfL;}
397  }
398  PSIn TMOut::GetSIn(const bool& IsCut, const int& CutBfL){
399    IAssert((CutBfL==-1)||((0<=CutBfL)));
400    int SInBfL= (CutBfL==-1) ? BfL : TInt::GetMn(BfL, CutBfL);
401    PSIn SIn;
402    if (OwnBf&&IsCut&&(SInBfL==BfL)){
403      SIn=PSIn(new TMIn(Bf, SInBfL, true));
404      Bf=NULL; BfL=MxBfL=0; OwnBf=true;
405    } else {
406      SIn=PSIn(new TMIn(Bf, SInBfL, false));
407      if (IsCut){CutBf(SInBfL);}
408    }
409    return SIn;
410  }
411  bool TMOut::IsCrLfLn() const {
412    for (int BfC=0; BfC<BfL; BfC++){
413      if ((Bf[BfC]==TCh::CrCh)&&((BfC+1<BfL)&&(Bf[BfC+1]==TCh::LfCh))){return true;}}
414    return false;
415  }
416  TStr TMOut::GetCrLfLn(){
417    IAssert(IsCrLfLn());
418    TChA Ln;
419    for (int BfC=0; BfC<BfL; BfC++){
420      char Ch=Bf[BfC];
421      if ((Ch==TCh::CrCh)&&((BfC+1<BfL)&&(Bf[BfC+1]==TCh::LfCh))){
422        Ln+=TCh::CrCh; Ln+=TCh::LfCh; CutBf(BfC+1+1); break;
423      } else {
424        Ln+=Ch;
425      }
426    }
427    return Ln;
428  }
429  bool TMOut::IsEolnLn() const {
430    for (int BfC=0; BfC<BfL; BfC++){
431      if ((Bf[BfC]==TCh::CrCh)||(Bf[BfC]==TCh::LfCh)){return true;}
432    }
433    return false;
434  }
435  TStr TMOut::GetEolnLn(const bool& DoAddEoln, const bool& DoCutBf){
436    IAssert(IsEolnLn());
437    int LnChs=0; TChA Ln;
438    for (int BfC=0; BfC<BfL; BfC++){
439      char Ch=Bf[BfC];
440      if ((Ch==TCh::CrCh)||(Ch==TCh::LfCh)){
441        LnChs++; if (DoAddEoln){Ln+=Ch;}
442        if (BfC+1<BfL){
443          char NextCh=Bf[BfC+1];
444          if (((Ch==TCh::CrCh)&&(NextCh==TCh::LfCh))||
445           ((Ch==TCh::LfCh)&&(NextCh==TCh::CrCh))){
446            LnChs++; if (DoAddEoln){Ln+=NextCh;}
447          }
448        }
449        break;
450      } else {
451        LnChs++; Ln+=Ch;
452      }
453    }
454    if (DoCutBf){
455      CutBf(LnChs);
456    }
457    return Ln;
458  }
459  void TMOut::MkEolnLn(){
460    if (!IsEolnLn()){
461      PutCh(TCh::CrCh); PutCh(TCh::LfCh);}
462  }
463  #ifndef SEEK_SET
464  #define SEEK_CUR    1
465  #define SEEK_END    2
466  #define SEEK_SET    0
467  #endif
468  void TFRnd::RefreshFPos(){
469    EAssertR(
470     fseek(FileId, 0, SEEK_CUR)==0,
471     "Error seeking into file '"+FNm+"'.");
472  }
473  TFRnd::TFRnd(const TStr& _FNm, const TFAccess& FAccess,
474   const bool& CreateIfNo, const int& _HdLen, const int& _RecLen):
475    FileId(NULL), FNm(_FNm.CStr()),
476    RecAct(false), HdLen(_HdLen), RecLen(_RecLen){
477    RecAct=(HdLen>=0)&&(RecLen>0);
478    switch (FAccess){
479      case faCreate: FileId=fopen(FNm.CStr(), "w+b"); break;
480      case faUpdate: FileId=fopen(FNm.CStr(), "r+b"); break;
481      case faAppend: FileId=fopen(FNm.CStr(), "r+b");
482        if (FileId!=NULL){fseek(FileId, SEEK_END, 0);} break;
483      case faRdOnly: FileId=fopen(FNm.CStr(), "rb"); break;
484      default: Fail;
485    }
486    if ((FileId==NULL)&&(CreateIfNo)){
487      FileId=fopen(FNm.CStr(), "w+b");}
488    EAssertR(FileId!=NULL, "Can not open file '"+_FNm+"'.");
489  }
490  TFRnd::~TFRnd(){
491    EAssertR(fclose(FileId)==0, "Can not close file '"+FNm+"'.");
492  }
493  TStr TFRnd::GetFNm() const {
494    return FNm.CStr();
495  }
496  void TFRnd::SetFPos(const int& FPos){
497    EAssertR(
498     fseek(FileId, FPos, SEEK_SET)==0,
499     "Error seeking into file '"+FNm+"'.");
500  }
501  void TFRnd::MoveFPos(const int& DFPos){
502    EAssertR(
503     fseek(FileId, DFPos, SEEK_CUR)==0,
504     "Error seeking into file '"+FNm+"'.");
505  }
506  int TFRnd::GetFPos(){
507    int FPos=ftell(FileId);
508    EAssertR(FPos!=-1, "Error seeking into file '"+FNm+"'.");
509    return FPos;
510  }
511  int TFRnd::GetFLen(){
512    int FPos=GetFPos();
513    EAssertR(
514     fseek(FileId, 0, SEEK_END)==0,
515     "Error seeking into file '"+FNm+"'.");
516    int FLen=GetFPos(); SetFPos(FPos); return FLen;
517  }
518  void TFRnd::SetRecN(const int& RecN){
519    IAssert(RecAct);
520    SetFPos(HdLen+RecN*RecLen);
521  }
522  int TFRnd::GetRecN(){
523    IAssert(RecAct);
524    int FPos=GetFPos()-HdLen;
525    EAssertR(FPos%RecLen==0, "Invalid position in file'"+FNm+"'.");
526    return FPos/RecLen;
527  }
528  int TFRnd::GetRecs(){
529    IAssert(RecAct);
530    int FLen=GetFLen()-HdLen;
531    EAssertR(FLen%RecLen==0, "Invalid length of file'"+FNm+"'.");
532    return FLen/RecLen;
533  }
534  void TFRnd::GetBf(void* Bf, const int& BfL){
535    RefreshFPos();
536    EAssertR(
537     int(fread(Bf, 1, BfL, FileId))==BfL,
538     "Error reading file '"+FNm+"'.");
539  }
540  void TFRnd::PutBf(const void* Bf, const int& BfL){
541    RefreshFPos();
542    EAssertR(
543     int(fwrite(Bf, 1, BfL, FileId))==BfL,
544     "Error writting to the file '"+FNm+"'.");
545  }
546  void TFRnd::Flush(){
547    EAssertR(fflush(FileId)==0, "Can not flush file '"+FNm+"'.");
548  }
549  void TFRnd::PutCh(const char& Ch, const int& Chs){
550    if (Chs>0){
551      char* CStr=new char[Chs];
552      for (int ChN=0; ChN<Chs; ChN++){CStr[ChN]=Ch;}
553      PutBf(CStr, Chs);
554      delete[] CStr;
555    }
556  }
557  void TFRnd::PutStr(const TStr& Str){
558    PutBf(Str.CStr(), Str.Len()+1);
559  }
560  TStr TFRnd::GetStr(const int& StrLen, bool& IsOk){
561    IsOk=false; TStr Str;
562    if (GetFPos()+StrLen+1<=GetFLen()){
563      char* CStr=new char[StrLen+1];
564      GetBf(CStr, StrLen+1);
565      if (CStr[StrLen+1-1]==TCh::NullCh){IsOk=true; Str=CStr;}
566      delete[] CStr;
567    }
568    return Str;
569  }
570  TStr TFRnd::GetStr(const int& StrLen){
571    TStr Str;
572    char* CStr=new char[StrLen+1];
573    GetBf(CStr, StrLen+1);
574    EAssertR(CStr[StrLen+1-1]==TCh::NullCh, "Error reading file '"+FNm+"'.");
575    Str=CStr;
576    delete[] CStr;
577    return Str;
578  }
579  void TFRnd::PutSIn(const PSIn& SIn, TCs& Cs){
580    int BfL=SIn->Len();
581    char* Bf=new char[BfL];
582    SIn->GetBf(Bf, BfL);
583    Cs=TCs::GetCsFromBf(Bf, BfL);
584    PutBf(Bf, BfL);
585    delete[] Bf;
586  }
587  PSIn TFRnd::GetSIn(const int& BfL, TCs& Cs){
588    char* Bf=new char[BfL];
589    GetBf(Bf, BfL);
590    Cs=TCs::GetCsFromBf(Bf, BfL);
591    PSIn SIn=PSIn(new TMIn(Bf, BfL, true));
592    return SIn;
593  }
594  TStr TFRnd::GetStrFromFAccess(const TFAccess& FAccess){
595    switch (FAccess){
596      case faCreate: return "Create";
597      case faUpdate: return "Update";
598      case faAppend: return "Append";
599      case faRdOnly: return "ReadOnly";
600      case faRestore: return "Restore";
601      default: Fail; return TStr();
602    }
603  }
604  TFAccess TFRnd::GetFAccessFromStr(const TStr& Str){
605    TStr UcStr=Str.GetUc();
606    if (UcStr=="CREATE"){return faCreate;}
607    if (UcStr=="UPDATE"){return faUpdate;}
608    if (UcStr=="APPEND"){return faAppend;}
609    if (UcStr=="READONLY"){return faRdOnly;}
610    if (UcStr=="RESTORE"){return faRestore;}
611    if (UcStr=="NEW"){return faCreate;}
612    if (UcStr=="CONT"){return faUpdate;}
613    if (UcStr=="CONTINUE"){return faUpdate;}
614    if (UcStr=="REST"){return faRestore;}
615    if (UcStr=="RESTORE"){return faRestore;}
616    return faUndef;
617  }
618  const TStr TFile::TxtFExt=".Txt";
619  const TStr TFile::HtmlFExt=".Html";
620  const TStr TFile::HtmFExt=".Htm";
621  const TStr TFile::GifFExt=".Gif";
622  const TStr TFile::JarFExt=".Jar";
623  bool TFile::Exists(const TStr& FNm){
624    bool DoExists;
625    TFIn FIn(FNm, DoExists);
626    return DoExists;
627  }
628  void TFile::Del(const TStr& FNm, const bool& ThrowExceptP){
629    if (ThrowExceptP){
630      EAssertR(
631       remove(FNm.CStr())==0,
632       "Error removing file '"+FNm+"'.");
633    } else {
634      remove(FNm.CStr());
635    }
636  }
637  void TFile::DelWc(const TStr& WcStr, const bool& RecurseDirP){
638    TStrV FNmV;
639    TFFile FFile(WcStr, RecurseDirP); TStr FNm;
640    while (FFile.Next(FNm)){
641      FNmV.Add(FNm);}
642    for (int FNmN=0; FNmN<FNmV.Len(); FNmN++){
643      Del(FNmV[FNmN], false);}
644  }
645  void TFile::Rename(const TStr& SrcFNm, const TStr& DstFNm){
646    EAssertR(
647     rename(SrcFNm.CStr(), DstFNm.CStr())==0,
648     "Error renaming file '"+SrcFNm+"' to "+DstFNm+"'.");
649  }
650  TStr TFile::GetUniqueFNm(const TStr& FNm){
651    int Cnt=1; int ch;
652    TStr NewFNm; TStr TmpFNm=FNm;
653    if (FNm.SearchCh('#') == -1) {
654      for (ch = FNm.Len()-1; ch >= 0; ch--) if (FNm[ch] == '.') break;
655      if (ch != -1) TmpFNm.InsStr(ch, ".#");
656      else TmpFNm += ".#";
657    }
658    forever{
659      NewFNm=TmpFNm;
660      NewFNm.ChangeStr("#", TStr::Fmt("%03d", Cnt)); Cnt++;
661      if (!TFile::Exists(NewFNm)){break;}
662    }
663    return NewFNm;
664  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-caffe.cpp</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-flx.cpp</div>
                <div class="column column_space"><pre><code>494      forward_time += forward_timer.MicroSeconds();
495      if (!FLAGS_forward_only) {
496        backward_timer.Start();
497        for (int i = layers.size() - 1; i >= 0; --i) {
498          timer.Start();
499          layers[i]->Backward(top_vecs[i], bottom_need_backward[i],
500                              bottom_vecs[i]);
501          backward_time_per_layer[i] += timer.MicroSeconds();
</pre></code></div>
                <div class="column column_space"><pre><code>254    int LBfS=0;
255    if (BfL+LBfL>MxBfL){
256      for (int LBfC=0; LBfC<LBfL; LBfC++){
257        LBfS+=PutCh(((char*)LBf)[LBfC]);}
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    