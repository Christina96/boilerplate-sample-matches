
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 223, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-solver.cpp</h3>
            <pre><code>1  #include &lt;algorithm&gt;
2  #include &lt;map&gt;
3  #include &lt;cstdio&gt;
4  #include &lt;functional&gt;
5  #include &lt;string&gt;
6  #include &lt;utility&gt;
7  #include &lt;vector&gt;
8  #include &lt;numeric&gt;
9  #include &quot;boost/bind.hpp&quot;
10  #include &quot;caffe/solver.hpp&quot;
11  #include &quot;caffe/util/bbox_util.hpp&quot;
12  #include &quot;caffe/util/format.hpp&quot;
13  #include &quot;caffe/util/hdf5.hpp&quot;
14  #include &quot;caffe/util/io.hpp&quot;
15  #include &quot;caffe/util/performance.hpp&quot;
16  #include &quot;caffe/util/upgrade_proto.hpp&quot;
17  #ifdef USE_MLSL
18  #include &quot;caffe/multinode/mlsl.hpp&quot;
19  #endif
20  namespace caffe {
21  template&lt;typename Dtype&gt;
22  void Solver&lt;Dtype&gt;::SetActionFunction(ActionCallback func) {
23    action_request_function_ = func;
24  }
25  template&lt;typename Dtype&gt;
26  SolverAction::Enum Solver&lt;Dtype&gt;::GetRequestedAction() {
27    if (action_request_function_) {
28      return action_request_function_();
29    }
30    return SolverAction::NONE;
31  }
32  template &lt;typename Dtype&gt;
33  Solver&lt;Dtype&gt;::Solver(const SolverParameter&amp; param, const Solver* root_solver)
34      : net_(), callbacks_(), root_solver_(root_solver),
35        requested_early_exit_(false),
36        forward_backward_(boost::bind(&amp;Solver&lt;Dtype&gt;::ForwardBackward, this)) {
37    Init(param);
38    Caffe::set_iter_size(param_.iter_size());
39  }
40  template &lt;typename Dtype&gt;
41  Solver&lt;Dtype&gt;::Solver(const string&amp; param_file, const Solver* root_solver)
42      : net_(), callbacks_(), root_solver_(root_solver),
43        requested_early_exit_(false),
44        forward_backward_(boost::bind(&amp;Solver&lt;Dtype&gt;::ForwardBackward, this)) {
45    SolverParameter param;
46    ReadSolverParamsFromTextFileOrDie(param_file, &amp;param);
47    Init(param);
48    Caffe::set_iter_size(param_.iter_size());
49  }
50  template &lt;typename Dtype&gt;
51  void Solver&lt;Dtype&gt;::Init(const SolverParameter&amp; param) {
52    CHECK(Caffe::root_solver() || root_solver_)
53        &lt;&lt; &quot;root_solver_ needs to be set for all non-root solvers&quot;;
54    param_ = param;
55  #ifdef USE_MLSL
56    ReplaceMultinodeSolverParams(&amp;param_);
57  #endif
58    LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Initializing solver from parameters: &quot;
59      &lt;&lt; std::endl &lt;&lt; param_.DebugString();
60    CHECK_GE(param_.average_loss(), 1) &lt;&lt; &quot;average_loss should be non-negative.&quot;;
61  #ifndef USE_MLSL
62    CheckSnapshotWritePermissions();
63  #endif
64    if (Caffe::root_solver() &amp;&amp; param_.random_seed() &gt;= 0) {
65      Caffe::set_random_seed(param_.random_seed());
66    }
67    InitTrainNet();
68    if (Caffe::root_solver()) {
69      InitTestNets();
70      LOG(INFO) &lt;&lt; &quot;Solver scaffolding done.&quot;;
71    }
72    iter_ = 0;
73    current_step_ = 0;
74  }
75  template &lt;typename Dtype&gt;
76  void Solver&lt;Dtype&gt;::InitTrainNet() {
77    const int num_train_nets = param_.has_net() + param_.has_net_param() +
78        param_.has_train_net() + param_.has_train_net_param();
79    const string&amp; field_names = &quot;net, net_param, train_net, train_net_param&quot;;
80    CHECK_GE(num_train_nets, 1) &lt;&lt; &quot;SolverParameter must specify a train net &quot;
81        &lt;&lt; &quot;using one of these fields: &quot; &lt;&lt; field_names;
82    CHECK_LE(num_train_nets, 1) &lt;&lt; &quot;SolverParameter must not contain more than &quot;
83        &lt;&lt; &quot;one of these fields specifying a train_net: &quot; &lt;&lt; field_names;
84    NetParameter net_param;
85    if (param_.has_train_net_param()) {
86      LOG_IF(INFO, Caffe::root_solver())
87          &lt;&lt; &quot;Creating training net specified in train_net_param.&quot;;
88      net_param.CopyFrom(param_.train_net_param());
89    } else if (param_.has_train_net()) {
90      LOG_IF(INFO, Caffe::root_solver())
91          &lt;&lt; &quot;Creating training net from train_net file: &quot; &lt;&lt; param_.train_net();
92      ReadNetParamsFromTextFileOrDie(param_.train_net(), &amp;net_param);
93    }
94    if (param_.has_net_param()) {
95      LOG_IF(INFO, Caffe::root_solver())
96          &lt;&lt; &quot;Creating training net specified in net_param.&quot;;
97      net_param.CopyFrom(param_.net_param());
98    }
99    if (param_.has_net()) {
100      LOG_IF(INFO, Caffe::root_solver())
101          &lt;&lt; &quot;Creating training net from net file: &quot; &lt;&lt; param_.net();
102      ReadNetParamsFromTextFileOrDie(param_.net(), &amp;net_param);
103    }
104    if (param_.engine() != &quot;&quot;)
105      net_param.set_engine(param_.engine());
106    NetState net_state;
107    net_state.set_phase(TRAIN);
108    net_state.MergeFrom(net_param.state());
109    net_state.MergeFrom(param_.train_state());
110    net_param.mutable_state()-&gt;CopyFrom(net_state);
111    if (Caffe::root_solver()) {
112      net_.reset(new Net&lt;Dtype&gt;(net_param));
113    } else {
114      net_.reset(new Net&lt;Dtype&gt;(net_param, root_solver_-&gt;net_.get()));
115    }
116  }
117  template &lt;typename Dtype&gt;
118  void Solver&lt;Dtype&gt;::InitTestNets() {
119    CHECK(Caffe::root_solver());
120    const bool has_net_param = param_.has_net_param();
121    const bool has_net_file = param_.has_net();
122    const int num_generic_nets = has_net_param + has_net_file;
123    CHECK_LE(num_generic_nets, 1)
124        &lt;&lt; &quot;Both net_param and net_file may not be specified.&quot;;
125    const int num_test_net_params = param_.test_net_param_size();
126    const int num_test_net_files = param_.test_net_size();
127    const int num_test_nets = num_test_net_params + num_test_net_files;
128    if (num_generic_nets) {
129        CHECK_GE(param_.test_iter_size(), num_test_nets)
130            &lt;&lt; &quot;test_iter must be specified for each test network.&quot;;
131    } else {
132        CHECK_EQ(param_.test_iter_size(), num_test_nets)
133            &lt;&lt; &quot;test_iter must be specified for each test network.&quot;;
134    }
135    const int num_generic_net_instances = param_.test_iter_size() - num_test_nets;
136    const int num_test_net_instances = num_test_nets + num_generic_net_instances;
137    if (param_.test_state_size()) {
138      CHECK_EQ(param_.test_state_size(), num_test_net_instances)
139          &lt;&lt; &quot;test_state must be unspecified or specified once per test net.&quot;;
140    }
141    if (num_test_net_instances) {
142      CHECK_GT(param_.test_interval(), 0);
143    }
144    int test_net_id = 0;
145    vector&lt;string&gt; sources(num_test_net_instances);
146    vector&lt;NetParameter&gt; net_params(num_test_net_instances);
147    for (int i = 0; i &lt; num_test_net_params; ++i, ++test_net_id) {
148        sources[test_net_id] = &quot;test_net_param&quot;;
149        net_params[test_net_id].CopyFrom(param_.test_net_param(i));
150    }
151    for (int i = 0; i &lt; num_test_net_files; ++i, ++test_net_id) {
152        sources[test_net_id] = &quot;test_net file: &quot; + param_.test_net(i);
153        ReadNetParamsFromTextFileOrDie(param_.test_net(i),
154            &amp;net_params[test_net_id]);
155    }
156    const int remaining_test_nets = param_.test_iter_size() - test_net_id;
157    if (has_net_param) {
158      for (int i = 0; i &lt; remaining_test_nets; ++i, ++test_net_id) {
159        sources[test_net_id] = &quot;net_param&quot;;
160        net_params[test_net_id].CopyFrom(param_.net_param());
161      }
162    }
163    if (has_net_file) {
164      for (int i = 0; i &lt; remaining_test_nets; ++i, ++test_net_id) {
165        sources[test_net_id] = &quot;net file: &quot; + param_.net();
166        ReadNetParamsFromTextFileOrDie(param_.net(), &amp;net_params[test_net_id]);
167      }
168    }
169    test_nets_.resize(num_test_net_instances);
170    for (int i = 0; i &lt; num_test_net_instances; ++i) {
171      NetState net_state;
172      net_state.set_phase(TEST);
173      net_state.MergeFrom(net_params[i].state());
174      if (param_.test_state_size()) {
175        net_state.MergeFrom(param_.test_state(i));
176      }
177      net_params[i].mutable_state()-&gt;CopyFrom(net_state);
178      if (param_.engine() != &quot;&quot;)
179        net_params[i].set_engine(param_.engine());
180      LOG(INFO)
181          &lt;&lt; &quot;Creating test net (#&quot; &lt;&lt; i &lt;&lt; &quot;) specified by &quot; &lt;&lt; sources[i];
182      if (Caffe::root_solver()) {
183        test_nets_[i].reset(new Net&lt;Dtype&gt;(net_params[i]));
184      } else {
185        test_nets_[i].reset(new Net&lt;Dtype&gt;(net_params[i],
186            root_solver_-&gt;test_nets_[i].get()));
187      }
188      test_nets_[i]-&gt;set_debug_info(param_.debug_info());
189    }
190  }
191  template &lt;typename Dtype&gt;
192  Dtype Solver&lt;Dtype&gt;::ForwardBackward() {
193    net_-&gt;ClearParamDiffs();
194    Dtype loss = Dtype();
195    vector&lt;Blob&lt;Dtype&gt;*&gt; bottom_vec;
196    for (int i = 0; i &lt; param_.iter_size(); ++i) {
197      loss += net_-&gt;ForwardBackward();
198    }
199    return loss / param_.iter_size();
200  }
201  template &lt;typename Dtype&gt;
202  void Solver&lt;Dtype&gt;::Step(int iters) {
203    const int start_iter = iter_;
204    const int stop_iter = iter_ + iters;
205    int average_loss = this-&gt;param_.average_loss();
206    losses_.clear();
207    smoothed_loss_ = 0;
208    while (iter_ &lt; stop_iter) {
209      if (param_.test_interval() &amp;&amp; (iter_ - param_.test_offset()) % param_.test_interval() == 0
210          &amp;&amp; (iter_ &gt; 0 || param_.test_initialization())
211          &amp;&amp; Caffe::root_solver()) {
212        TestAll();
213        if (requested_early_exit_) {
214          break;
215        }
216      }
217      for (int i = 0; i &lt; callbacks_.size(); ++i) {
218        callbacks_[i]-&gt;on_start();
219      }
220      const bool display = param_.display() &amp;&amp; iter_ % param_.display() == 0;
221      net_-&gt;set_debug_info(display &amp;&amp; param_.debug_info());
222      Timer iter_timer;
223      double iter_time = 0.0;
224      iter_timer.Start();
225      Dtype loss = forward_backward_();
226      iter_time += iter_timer.MilliSeconds();
227      UpdateSmoothedLoss(loss, start_iter, average_loss);
228      if (display) {
229        LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Iteration &quot; &lt;&lt; iter_
230            &lt;&lt; &quot;, loss = &quot; &lt;&lt; smoothed_loss_;
231        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; result = net_-&gt;output_blobs();
232        int score_index = 0;
233        for (int j = 0; j &lt; result.size(); ++j) {
234          const Dtype* result_vec = result[j]-&gt;cpu_data();
235          const string&amp; output_name =
236              net_-&gt;blob_names()[net_-&gt;output_blob_indices()[j]];
237          const Dtype loss_weight =
238              net_-&gt;blob_loss_weights()[net_-&gt;output_blob_indices()[j]]
239  #ifdef USE_MLSL
240              * mn::get_distrib()-&gt;get_data_parts()
241  #endif
242                ;
243          for (int k = 0; k &lt; result[j]-&gt;count(); ++k) {
244            ostringstream loss_msg_stream;
245            if (loss_weight) {
246              loss_msg_stream &lt;&lt; &quot; (* &quot; &lt;&lt; loss_weight
247                              &lt;&lt; &quot; = &quot; &lt;&lt; loss_weight * result_vec[k] &lt;&lt; &quot; loss)&quot;;
248            }
249            LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;    Train net output #&quot;
250                &lt;&lt; score_index++ &lt;&lt; &quot;: &quot; &lt;&lt; output_name &lt;&lt; &quot; = &quot;
251                &lt;&lt; result_vec[k] &lt;&lt; loss_msg_stream.str();
252          }
253        }
254      }
255      iter_timer.Start();
256      for (int i = 0; i &lt; callbacks_.size(); ++i) {
257        callbacks_[i]-&gt;on_gradients_ready();
258      }
259      if (!param().disabled_update()) {
260        PERFORMANCE_MEASUREMENT_BEGIN();
261        ApplyUpdate();
262        PERFORMANCE_MEASUREMENT_END_STATIC(&quot;weights_update&quot;);
263      }else{
264        PrintLearningRate();
265      }
266      iter_time += iter_timer.MilliSeconds();
267  #ifdef CAFFE_PER_LAYER_TIMINGS
268      net_-&gt;SaveTimeline();
269      net_-&gt;PrintTimers(false);
270      net_-&gt;ResetTimers();
271  #ifdef USE_MLSL
272      if (mn::is_root())
273  #endif
274      LOG(INFO) &lt;&lt; &quot;iter &quot; &lt;&lt; iter_ &lt;&lt; &quot;, forward_backward_update_time: &quot;
275              &lt;&lt; iter_time &lt;&lt; &quot; ms&quot;;
276  #endif
277      ++iter_;
278      SolverAction::Enum request = GetRequestedAction();
279      if ((param_.snapshot()
280           &amp;&amp; iter_ % param_.snapshot() == 0
281           &amp;&amp; Caffe::root_solver()) ||
282           (request == SolverAction::SNAPSHOT)) {
283        Snapshot();
284      }
285      if (SolverAction::STOP == request) {
286        requested_early_exit_ = true;
287        break;
288      }
289    }
290  #ifdef CAFFE_PER_LAYER_TIMINGS
291    net_-&gt;ResetTimers();
292    net_-&gt;PrintTimers(true);
293    net_-&gt;PrintPayloadSize();
294  #endif
295  }
296  template &lt;typename Dtype&gt;
297  void Solver&lt;Dtype&gt;::Solve(const char* resume_file) {
298    CHECK(Caffe::root_solver());
299    LOG(INFO) &lt;&lt; &quot;Solving &quot; &lt;&lt; net_-&gt;name();
300    LOG(INFO) &lt;&lt; &quot;Learning Rate Policy: &quot; &lt;&lt; param_.lr_policy();
301    PERFORMANCE_INIT_MONITOR();
302    requested_early_exit_ = false;
303    if (resume_file) {
304      LOG(INFO) &lt;&lt; &quot;Restoring previous solver status from &quot; &lt;&lt; resume_file;
305      Restore(resume_file);
306    }
307    int start_iter = iter_;
308    Step(param_.max_iter() - iter_);
309    if (param_.snapshot_after_train()
310        &amp;&amp; (!param_.snapshot() || iter_ % param_.snapshot() != 0)) {
311      Snapshot();
312    }
313    if (requested_early_exit_) {
314      LOG(INFO) &lt;&lt; &quot;Optimization stopped early.&quot;;
315      return;
316    }
317    if (param_.display() &amp;&amp; iter_ % param_.display() == 0) {
318      int average_loss = this-&gt;param_.average_loss();
319      Dtype loss;
320      net_-&gt;Forward(&amp;loss);
321      UpdateSmoothedLoss(loss, start_iter, average_loss);
322      LOG(INFO) &lt;&lt; &quot;Iteration &quot; &lt;&lt; iter_ &lt;&lt; &quot;, loss = &quot; &lt;&lt; smoothed_loss_;
323    }
324    if (param_.test_interval() &amp;&amp; (iter_ - param_.test_offset()) % param_.test_interval() == 0)
325      TestAll();
326    LOG(INFO) &lt;&lt; &quot;Optimization Done.&quot;;
327  }
328  template &lt;typename Dtype&gt;
329  void Solver&lt;Dtype&gt;::TestAll() {
330  #ifdef USE_MLSL
331    for (int i = 0; i &lt; callbacks_.size(); ++i) {
332      callbacks_[i]-&gt;on_before_test();
333    }
334  #endif
335    for (int test_net_id = 0;
336         test_net_id &lt; test_nets_.size() &amp;&amp; !requested_early_exit_;
337         ++test_net_id) {
338      if (param_.eval_type() == &quot;classification&quot;) {
339        TestClassification(test_net_id);
340      } else if (param_.eval_type() == &quot;detection&quot;) {
341        TestDetection(test_net_id);
342      } else {
343        LOG(FATAL) &lt;&lt; &quot;Unknown evaluation type: &quot; &lt;&lt; param_.eval_type();
344      }
345    }
346  #ifdef USE_MLSL
347    for (int i = 0; i &lt; callbacks_.size(); ++i) {
348      callbacks_[i]-&gt;on_after_test();
349    }
350  #endif
351  }
352  template &lt;typename Dtype&gt;
353  void Solver&lt;Dtype&gt;::TestClassification(const int test_net_id) {
354    CHECK(Caffe::root_solver());
355    LOG(INFO) &lt;&lt; &quot;Iteration &quot; &lt;&lt; iter_
356              &lt;&lt; &quot;, Testing net (#&quot; &lt;&lt; test_net_id &lt;&lt; &quot;)&quot;;
357    CHECK_NOTNULL(test_nets_[test_net_id].get())-&gt;
358        ShareTrainedLayersWith(net_.get());
359    vector&lt;Dtype&gt; test_score;
360    vector&lt;int&gt; test_score_output_id;
361    const shared_ptr&lt;Net&lt;Dtype&gt; &gt;&amp; test_net = test_nets_[test_net_id];
362    Dtype loss = 0;
363    int global_test_iter = param_.test_iter(test_net_id);
364  #ifdef USE_MLSL
365    int local_test_iter = global_test_iter / mn::get_nodes_count(); 
366    int left_test_iter = global_test_iter % mn::get_nodes_count();
367    if (mn::get_node_id() &lt; left_test_iter)
368      local_test_iter += 1;
369  #else
370    int local_test_iter = global_test_iter;
371  #endif
372    for (int i = 0; i &lt; local_test_iter; ++i) {
373      SolverAction::Enum request = GetRequestedAction();
374      while (request != SolverAction::NONE) {
375          if (SolverAction::SNAPSHOT == request) {
376            Snapshot();
377          } else if (SolverAction::STOP == request) {
378            requested_early_exit_ = true;
379          }
380          request = GetRequestedAction();
381      }
382      if (requested_early_exit_) {
383        break;
384      }
385      Dtype iter_loss;
386      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; result =
387          test_net-&gt;Forward(&amp;iter_loss);
388      if (param_.test_compute_loss()) {
389        loss += iter_loss;
390      }
391      if (i == 0) {
392        for (int j = 0; j &lt; result.size(); ++j) {
393          const Dtype* result_vec = result[j]-&gt;cpu_data();
394          for (int k = 0; k &lt; result[j]-&gt;count(); ++k) {
395            test_score.push_back(result_vec[k]);
396            test_score_output_id.push_back(j);
397          }
398        }
399      } else {
400        int idx = 0;
401        for (int j = 0; j &lt; result.size(); ++j) {
402          const Dtype* result_vec = result[j]-&gt;cpu_data();
403          for (int k = 0; k &lt; result[j]-&gt;count(); ++k) {
404            test_score[idx++] += result_vec[k];
405          }
406        }
407      }
408    }
409    if (requested_early_exit_) {
410      LOG(INFO)     &lt;&lt; &quot;Test interrupted.&quot;;
411      return;
412    }
413    if (param_.test_compute_loss()) {
414  #ifdef USE_MLSL
415      mn::allreduce(&amp;loss, 1);
416      if (mn::is_root()) {
417  #endif &amp;bsol;* USE_MLSL */
418      loss /= global_test_iter;
419      LOG(INFO) &lt;&lt; &quot;Test loss: &quot; &lt;&lt; loss;
420  #ifdef USE_MLSL
421      }
422  #endif &amp;bsol;* USE_MLSL */
423    }
424  #ifdef USE_MLSL
425    mn::allreduce(test_score.data(), test_score.size());
426    if (mn::is_root())
427  #endif &amp;bsol;* USE_MLSL */
428    for (int i = 0; i &lt; test_score.size(); ++i) {
429      const int output_blob_index =
430          test_net-&gt;output_blob_indices()[test_score_output_id[i]];
431      const string&amp; output_name = test_net-&gt;blob_names()[output_blob_index];
432      const Dtype loss_weight = test_net-&gt;blob_loss_weights()[output_blob_index];
433      ostringstream loss_msg_stream;
434      const Dtype mean_score = test_score[i] / global_test_iter;
435      if (loss_weight) {
436        loss_msg_stream &lt;&lt; &quot; (* &quot; &lt;&lt; loss_weight
437                        &lt;&lt; &quot; = &quot; &lt;&lt; loss_weight * mean_score &lt;&lt; &quot; loss)&quot;;
438      }
439      LOG(INFO) &lt;&lt; &quot;    Test net output #&quot; &lt;&lt; i &lt;&lt; &quot;: &quot; &lt;&lt; output_name &lt;&lt; &quot; = &quot;
440                &lt;&lt; mean_score &lt;&lt; loss_msg_stream.str();
441    }
442  }
443  template &lt;typename Dtype&gt;
444  void Solver&lt;Dtype&gt;::TestDetection(const int test_net_id) {
445    CHECK(Caffe::root_solver());
446    LOG(INFO) &lt;&lt; &quot;Iteration &quot; &lt;&lt; iter_
447              &lt;&lt; &quot;, Testing net (#&quot; &lt;&lt; test_net_id &lt;&lt; &quot;)&quot;;
448    CHECK_NOTNULL(test_nets_[test_net_id].get())-&gt;
449        ShareTrainedLayersWith(net_.get());
450    map&lt;int, map&lt;int, vector&lt;pair&lt;float, int&gt; &gt; &gt; &gt; all_true_pos;
451    map&lt;int, map&lt;int, vector&lt;pair&lt;float, int&gt; &gt; &gt; &gt; all_false_pos;
452    map&lt;int, map&lt;int, int&gt; &gt; all_num_pos;
453    const shared_ptr&lt;Net&lt;Dtype&gt; &gt;&amp; test_net = test_nets_[test_net_id];
454    Dtype loss = 0;
455    for (int i = 0; i &lt; param_.test_iter(test_net_id); ++i) {
456      SolverAction::Enum request = GetRequestedAction();
457      while (request != SolverAction::NONE) {
458          if (SolverAction::SNAPSHOT == request) {
459            Snapshot();
460          } else if (SolverAction::STOP == request) {
461            requested_early_exit_ = true;
462          }
463          request = GetRequestedAction();
464      }
465      if (requested_early_exit_) {
466        break;
467      }
468      Dtype iter_loss;
469      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; result = test_net-&gt;Forward(&amp;iter_loss);
470      if (param_.test_compute_loss()) {
471        loss += iter_loss;
472      }
473      for (int j = 0; j &lt; result.size(); ++j) {
474        CHECK_EQ(result[j]-&gt;width(), 5);
475        const Dtype* result_vec = result[j]-&gt;cpu_data();
476        int num_det = result[j]-&gt;height();
477        for (int k = 0; k &lt; num_det; ++k) {
<span onclick='openModal()' class='match'>478          int item_id = static_cast&lt;int&gt;(result_vec[k * 5]);
479          int label = static_cast&lt;int&gt;(result_vec[k * 5 + 1]);
480          if (item_id == -1) {
481            if (all_num_pos[j].find(label) == all_num_pos[j].end()) {
482              all_num_pos[j][label] = static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
483            } else {
484              all_num_pos[j][label] += static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
485            }
486          } else {
487            float score = result_vec[k * 5 + 2];
488            int tp = static_cast&lt;int&gt;(result_vec[k * 5 + 3]);
489            int fp = static_cast&lt;int&gt;(result_vec[k * 5 + 4]);
490            if (tp == 0 &amp;&amp; fp == 0) {
491              continue;
492            }
493            all_true_pos[j][label].push_back(std::make_pair(score, tp));
494            all_false_pos[j][label].push_back(std::make_pair(score, fp));
495          }
496        }
497      }
498    }
499    if (requested_early_exit_) {
</span>500      LOG(INFO)     &lt;&lt; &quot;Test interrupted.&quot;;
501      return;
502    }
503    if (param_.test_compute_loss()) {
504  #ifdef USE_MLSL
505      mn::allreduce(&amp;loss, 1);
506      loss /= (param_.test_iter(test_net_id) * mn::get_group_size());
507      if (mn::is_root() == true)
508        LOG(INFO) &lt;&lt; &quot;Test loss: &quot; &lt;&lt; loss;
509  #else &amp;bsol;* !USE_MLSL */
510      loss /= param_.test_iter(test_net_id);
511      LOG(INFO) &lt;&lt; &quot;Test loss: &quot; &lt;&lt; loss;
512  #endif &amp;bsol;* USE_MLSL */
513    }
514    for (int i = 0; i &lt; all_true_pos.size(); ++i) {
515      if (all_true_pos.find(i) == all_true_pos.end()) {
516        LOG(FATAL) &lt;&lt; &quot;Missing output_blob true_pos: &quot; &lt;&lt; i;
517      }
518      const map&lt;int, vector&lt;pair&lt;float, int&gt; &gt; &gt;&amp; true_pos =
519          all_true_pos.find(i)-&gt;second;
520      if (all_false_pos.find(i) == all_false_pos.end()) {
521        LOG(FATAL) &lt;&lt; &quot;Missing output_blob false_pos: &quot; &lt;&lt; i;
522      }
523      const map&lt;int, vector&lt;pair&lt;float, int&gt; &gt; &gt;&amp; false_pos =
524          all_false_pos.find(i)-&gt;second;
525      if (all_num_pos.find(i) == all_num_pos.end()) {
526        LOG(FATAL) &lt;&lt; &quot;Missing output_blob num_pos: &quot; &lt;&lt; i;
527      }
528      const map&lt;int, int&gt;&amp; num_pos = all_num_pos.find(i)-&gt;second;
529      map&lt;int, float&gt; APs;
530      float mAP = 0.;
531      for (map&lt;int, int&gt;::const_iterator it = num_pos.begin();
532           it != num_pos.end(); ++it) {
533        int label = it-&gt;first;
534        int label_num_pos = it-&gt;second;
535        if (true_pos.find(label) == true_pos.end()) {
536          LOG(WARNING) &lt;&lt; &quot;Missing true_pos for label: &quot; &lt;&lt; label;
537          continue;
538        }
539        const vector&lt;pair&lt;float, int&gt; &gt;&amp; label_true_pos =
540            true_pos.find(label)-&gt;second;
541        if (false_pos.find(label) == false_pos.end()) {
542          LOG(WARNING) &lt;&lt; &quot;Missing false_pos for label: &quot; &lt;&lt; label;
543          continue;
544        }
545        const vector&lt;pair&lt;float, int&gt; &gt;&amp; label_false_pos =
546            false_pos.find(label)-&gt;second;
547        vector&lt;float&gt; prec, rec;
548        ComputeAP(label_true_pos, label_num_pos, label_false_pos,
549                  param_.ap_version(), &amp;prec, &amp;rec, &amp;(APs[label]));
550        mAP += APs[label];
551      }
552  #ifdef USE_MLSL
553      Dtype allnodes_mAP = static_cast&lt;Dtype&gt;(mAP);
554      mn::allreduce(&amp;allnodes_mAP, 1);
555      mAP = allnodes_mAP;
556      Dtype allnodes_num_pos = static_cast&lt;Dtype&gt;(num_pos.size());
557      mn::allreduce(&amp;allnodes_num_pos, 1);
558      if (mn::is_root() == true)
559        mAP /= allnodes_num_pos;
560  #else &amp;bsol;* USE_MLSL */
561      mAP /= num_pos.size();
562  #endif
563      const int output_blob_index = test_net-&gt;output_blob_indices()[i];
564      const string&amp; output_name = test_net-&gt;blob_names()[output_blob_index];
565  #ifdef USE_MLSL
566      if (mn::is_root() == true)
567  #endif
568      LOG(INFO) &lt;&lt; &quot;    Test net output #&quot; &lt;&lt; i &lt;&lt; &quot;: &quot; &lt;&lt; output_name &lt;&lt; &quot; = &quot;
569                &lt;&lt; mAP;
570    }
571  }
572  template &lt;typename Dtype&gt;
573  void Solver&lt;Dtype&gt;::Snapshot() {
574    LOG(INFO)&lt;&lt;&quot;Snapshot begin&quot;;
575    CHECK(Caffe::root_solver());
576  #ifdef USE_MLSL
577    for (int i = 0; i &lt; callbacks_.size(); ++i) {
578      callbacks_[i]-&gt;on_before_snapshot();
579    }
580  #endif &amp;bsol;* USE_MLSL */
581    string model_filename;
582    switch (param_.snapshot_format()) {
583    case caffe::SolverParameter_SnapshotFormat_BINARYPROTO:
584      model_filename = SnapshotToBinaryProto();
585      break;
586    case caffe::SolverParameter_SnapshotFormat_HDF5:
587      model_filename = SnapshotToHDF5();
588      break;
589    default:
590      LOG(FATAL) &lt;&lt; &quot;Unsupported snapshot format.&quot;;
591    }
592    SnapshotSolverState(model_filename);
593  #ifdef USE_MLSL
594    for (int i = 0; i &lt; callbacks_.size(); ++i) {
595      callbacks_[i]-&gt;on_after_snapshot();
596    }
597  #endif
598    LOG(INFO)&lt;&lt;&quot;Snapshot end&quot;;
599  }
600  template &lt;typename Dtype&gt;
601  void Solver&lt;Dtype&gt;::CheckSnapshotWritePermissions() {
602    if (Caffe::root_solver() &amp;&amp; param_.snapshot()) {
603      CHECK(param_.has_snapshot_prefix())
604          &lt;&lt; &quot;In solver params, snapshot is specified but snapshot_prefix is not&quot;;
605      string probe_filename = SnapshotFilename(&quot;.tempfile&quot;);
606      std::ofstream probe_ofs(probe_filename.c_str());
607      if (probe_ofs.good()) {
608        probe_ofs.close();
609        std::remove(probe_filename.c_str());
610      } else {
611        LOG(FATAL) &lt;&lt; &quot;Cannot write to snapshot prefix &#x27;&quot;
612            &lt;&lt; param_.snapshot_prefix() &lt;&lt; &quot;&#x27;.  Make sure &quot;
613            &lt;&lt; &quot;that the directory exists and is writeable.&quot;;
614      }
615    }
616  }
617  template &lt;typename Dtype&gt;
618  string Solver&lt;Dtype&gt;::SnapshotFilename(const string extension) {
619    return param_.snapshot_prefix() + &quot;_iter_&quot; + caffe::format_int(iter_)
620      + extension;
621  }
622  template &lt;typename Dtype&gt;
623  string Solver&lt;Dtype&gt;::SnapshotToBinaryProto() {
624    string model_filename = SnapshotFilename(&quot;.caffemodel&quot;);
625    NetParameter net_param;
626    net_-&gt;ToProto(&amp;net_param, param_.snapshot_diff());
627  #ifdef USE_MLSL
628    if (mn::is_root()) {
629  #endif
630    LOG(INFO) &lt;&lt; &quot;Snapshotting to binary proto file &quot; &lt;&lt; model_filename;
631    WriteProtoToBinaryFile(net_param, model_filename);
632  #ifdef USE_MLSL
633    }
634  #endif
635    return model_filename;
636  }
637  template &lt;typename Dtype&gt;
638  string Solver&lt;Dtype&gt;::SnapshotToHDF5() {
639    string model_filename = SnapshotFilename(&quot;.caffemodel.h5&quot;);
640  #ifdef USE_MLSL
641    if (mn::is_root()) {
642  #endif
643    LOG(INFO) &lt;&lt; &quot;Snapshotting to HDF5 file &quot; &lt;&lt; model_filename;
644  #ifdef USE_MLSL
645    }
646  #endif
647    net_-&gt;ToHDF5(model_filename, param_.snapshot_diff());
648    return model_filename;
649  }
650  template &lt;typename Dtype&gt;
651  void Solver&lt;Dtype&gt;::Restore(const char* state_file) {
652    CHECK(Caffe::root_solver());
653    string state_filename(state_file);
654    if (state_filename.size() &gt;= 3 &amp;&amp;
655        state_filename.compare(state_filename.size() - 3, 3, &quot;.h5&quot;) == 0) {
656      RestoreSolverStateFromHDF5(state_filename);
657    } else {
658      RestoreSolverStateFromBinaryProto(state_filename);
659    }
660  }
661  template &lt;typename Dtype&gt;
662  void Solver&lt;Dtype&gt;::UpdateSmoothedLoss(Dtype loss, int start_iter,
663      int average_loss) {
664  #ifdef USE_MLSL
665    loss *= mn::get_distrib()-&gt;get_data_parts();
666  #endif
667    if (losses_.size() &lt; average_loss) {
668      losses_.push_back(loss);
669      int size = losses_.size();
670      smoothed_loss_ = (smoothed_loss_ * (size - 1) + loss) / size;
671    } else {
672      int idx = (iter_ - start_iter) % average_loss;
673      smoothed_loss_ += (loss - losses_[idx]) / average_loss;
674      losses_[idx] = loss;
675    }
676  }
677  INSTANTIATE_CLASS(Solver);
678  }  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-caffe.cpp</h3>
            <pre><code>1  #ifdef WITH_PYTHON_LAYER
2  #include &quot;boost/python.hpp&quot;
3  namespace bp = boost::python;
4  #endif
5  #include &lt;gflags/gflags.h&gt;
6  #include &lt;glog/logging.h&gt;
7  #include &lt;cstring&gt;
8  #include &lt;map&gt;
9  #include &lt;string&gt;
10  #include &lt;utility&gt;
11  #include &lt;vector&gt;
12  #include &quot;boost/algorithm/string.hpp&quot;
13  #include &quot;boost/make_shared.hpp&quot;
14  #include &quot;caffe/caffe.hpp&quot;
15  #include &quot;caffe/training_utils.hpp&quot;
16  #include &quot;caffe/util/performance.hpp&quot;
17  #include &quot;caffe/util/signal_handler.h&quot;
18  #include &quot;caffe/util/bbox_util.hpp&quot;
19  #ifdef USE_MLSL
20  #include &quot;caffe/multinode/mlsl.hpp&quot;
21  #include &quot;caffe/multinode/multi_sync.hpp&quot;
22  #include &quot;caffe/multinode/async_param_server.hpp&quot;
23  #endif &amp;bsol;* USE_MLSL */
24  using caffe::Blob;
25  using caffe::Caffe;
26  using caffe::Net;
27  using caffe::Layer;
28  using caffe::Solver;
29  using caffe::shared_ptr;
30  using caffe::string;
31  using caffe::Timer;
32  using caffe::vector;
33  using std::ostringstream;
34  DEFINE_string(gpu, &quot;&quot;,
35      &quot;Optional; run in GPU mode on given device IDs separated by &#x27;,&#x27;.&quot;
36      &quot;Use &#x27;-gpu all&#x27; to run on all available GPUs. The effective training &quot;
37      &quot;batch size is multiplied by the number of devices.&quot;);
38  DEFINE_string(solver, &quot;&quot;,
39      &quot;The solver definition protocol buffer text file.&quot;);
40  DEFINE_string(model, &quot;&quot;,
41      &quot;The model definition protocol buffer text file.&quot;);
42  DEFINE_string(phase, &quot;&quot;,
43      &quot;Optional; network phase (TRAIN or TEST). Only used for &#x27;time&#x27;.&quot;);
44  DEFINE_int32(level, 0,
45      &quot;Optional; network level.&quot;);
46  DEFINE_string(stage, &quot;&quot;,
47      &quot;Optional; network stages (not to be confused with phase), &quot;
48      &quot;separated by &#x27;,&#x27;.&quot;);
49  DEFINE_string(snapshot, &quot;&quot;,
50      &quot;Optional; the snapshot solver state to resume training.&quot;);
51  DEFINE_string(weights, &quot;&quot;,
52      &quot;Optional; the pretrained weights to initialize finetuning, &quot;
53      &quot;separated by &#x27;,&#x27;. Cannot be set simultaneously with snapshot.&quot;);
54  DEFINE_int32(iterations, 50,
55      &quot;The number of iterations to run.&quot;);
56  DEFINE_string(sigint_effect, &quot;stop&quot;,
57               &quot;Optional; action to take when a SIGINT signal is received: &quot;
58                &quot;snapshot, stop or none.&quot;);
59  DEFINE_string(sighup_effect, &quot;snapshot&quot;,
60               &quot;Optional; action to take when a SIGHUP signal is received: &quot;
61               &quot;snapshot, stop or none.&quot;);
62  DEFINE_bool(forward_only, false,
63      &quot;Optional; Execute only forward pass&quot;);
64  DEFINE_string(engine, &quot;&quot;,
65      &quot;Optional; Engine sequence in format: engine:subengine_1,subengine_2,...&quot;);
66  DEFINE_string(collect_dir, &quot;collect_out&quot;,
67      &quot;Optional; Directory with reference binary files&quot;);
68  DEFINE_string(compare_output_dir, &quot;compare_out&quot;,
69      &quot;Optional; Directory with output files&quot;);
70  DEFINE_double(epsilon, 1e-3, &quot;Optional; Layer output comparison error&quot;);
71  DEFINE_bool(detection, false,
72      &quot;Optional; Enables detection for testing. &quot;
73      &quot;By default it is false and classification is on.&quot;);
74  DEFINE_bool(fast_compare, false,
75      &quot;Optional; Break layer comparison after fast_compare_max errors found&quot;);
76  DEFINE_bool(sampling, false,
77      &quot;Optional; Caffe test with sampling mode&quot;);
78  DEFINE_int32(fast_compare_max, 50,
79      &quot;Optional; Max errors for fast_compare&quot;);
80  DEFINE_double(buffer_filler, std::nanf(&quot;&quot;), &quot;Buffer filler for compare tool&quot;);
81  DEFINE_int32(n_group, 1, &quot;Optional; if given, it specifies how many trees&quot;
82               &quot; we want in the async forest&quot;);
83  DEFINE_int32(n_server, 0, &quot;Optional; if given, it specifies how many parts&quot;
84               &quot;The model is splited to. I.e. how many process you have for param server&quot;);
85  typedef int (*BrewFunction)();
86  typedef std::map&lt;caffe::string, BrewFunction&gt; BrewMap;
87  BrewMap g_brew_map;
88  #define RegisterBrewFunction(func) \
89  namespace { \
90  class __Registerer_##func { \
91   public: &amp;bsol;* NOLINT */ \
92    __Registerer_##func() { \
93      g_brew_map[#func] = &amp;func; \
94    } \
95  }; \
96  __Registerer_##func g_registerer_##func; \
97  }
98  static BrewFunction GetBrewFunction(const caffe::string&amp; name) {
99    if (g_brew_map.count(name)) {
100      return g_brew_map[name];
101    } else {
102      LOG(ERROR) &lt;&lt; &quot;Available caffe actions:&quot;;
103      for (BrewMap::iterator it = g_brew_map.begin();
104           it != g_brew_map.end(); ++it) {
105        LOG(ERROR) &lt;&lt; &quot;\t&quot; &lt;&lt; it-&gt;first;
106      }
107      LOG(FATAL) &lt;&lt; &quot;Unknown action: &quot; &lt;&lt; name;
108      return NULL;  
109    }
110  }
111  static void get_gpus(vector&lt;int&gt;* gpus) {
112    if (FLAGS_gpu == &quot;all&quot;) {
113      int count = 0;
114  #ifndef CPU_ONLY
115      CUDA_CHECK(cudaGetDeviceCount(&amp;count));
116  #else
117      NO_GPU;
118  #endif
119      for (int i = 0; i &lt; count; ++i) {
120        gpus-&gt;push_back(i);
121      }
122    } else if (FLAGS_gpu.size()) {
123      vector&lt;string&gt; strings;
124      boost::split(strings, FLAGS_gpu, boost::is_any_of(&quot;,&quot;));
125      for (int i = 0; i &lt; strings.size(); ++i) {
126        gpus-&gt;push_back(boost::lexical_cast&lt;int&gt;(strings[i]));
127      }
128    } else {
129      CHECK_EQ(gpus-&gt;size(), 0);
130    }
131  }
132  caffe::Phase get_phase_from_flags(caffe::Phase default_value) {
133    if (FLAGS_phase == &quot;&quot;)
134      return default_value;
135    if (FLAGS_phase == &quot;TRAIN&quot;)
136      return caffe::TRAIN;
137    if (FLAGS_phase == &quot;TEST&quot;)
138      return caffe::TEST;
139    LOG(FATAL) &lt;&lt; &quot;phase must be \&quot;TRAIN\&quot; or \&quot;TEST\&quot;&quot;;
140    return caffe::TRAIN;  
141  }
142  int device_query() {
143    LOG(INFO) &lt;&lt; &quot;Querying GPUs &quot; &lt;&lt; FLAGS_gpu;
144    vector&lt;int&gt; gpus;
145    get_gpus(&amp;gpus);
146    for (int i = 0; i &lt; gpus.size(); ++i) {
147      caffe::Caffe::SetDevice(gpus[i]);
148      caffe::Caffe::DeviceQuery();
149    }
150    return 0;
151  }
152  RegisterBrewFunction(device_query);
153  void CopyLayers(caffe::Solver&lt;float&gt;* solver, const std::string&amp; model_list) {
154    std::vector&lt;std::string&gt; model_names;
155    boost::split(model_names, model_list, boost::is_any_of(&quot;,&quot;) );
156    for (int i = 0; i &lt; model_names.size(); ++i) {
157      LOG(INFO) &lt;&lt; &quot;Finetuning from &quot; &lt;&lt; model_names[i];
158      solver-&gt;net()-&gt;CopyTrainedLayersFrom(model_names[i]);
159      for (int j = 0; j &lt; solver-&gt;test_nets().size(); ++j) {
160        solver-&gt;test_nets()[j]-&gt;CopyTrainedLayersFrom(model_names[i]);
161      }
162    }
163  }
164  caffe::SolverAction::Enum GetRequestedAction(
165      const std::string&amp; flag_value) {
166    if (flag_value == &quot;stop&quot;) {
167      return caffe::SolverAction::STOP;
168    }
169    if (flag_value == &quot;snapshot&quot;) {
170      return caffe::SolverAction::SNAPSHOT;
171    }
172    if (flag_value == &quot;none&quot;) {
173      return caffe::SolverAction::NONE;
174    }
175    LOG(FATAL) &lt;&lt; &quot;Invalid signal effect \&quot;&quot;&lt;&lt; flag_value &lt;&lt; &quot;\&quot; was specified&quot;;
176    return caffe::SolverAction::UNKNOWN;
177  }
178  int train() {
179    CHECK_GT(FLAGS_solver.size(), 0) &lt;&lt; &quot;Need a solver definition to train.&quot;;
180    CHECK(!FLAGS_snapshot.size() || !FLAGS_weights.size())
181        &lt;&lt; &quot;Give a snapshot to resume training or weights to finetune &quot;
182        &quot;but not both.&quot;;
183    caffe::SolverParameter solver_param;
184    if (!caffe::ReadProtoFromTextFile(FLAGS_solver, &amp;solver_param)) {
185      caffe::MultiPhaseSolverParameter multi_solver_params;
186      CHECK(caffe::ReadProtoFromTextFile(FLAGS_solver, &amp;multi_solver_params))
187        &lt;&lt; &quot;Failed to parse SolverParameter file: &quot;  &lt;&lt;  FLAGS_solver;
188      return multiphase_train(
189        &amp;multi_solver_params,
190        FLAGS_solver,
191        FLAGS_engine,
192        FLAGS_level,
193        FLAGS_stage);
194    }
195    use_flags(
196      &amp;solver_param,
197      FLAGS_solver,
198      FLAGS_engine,
199      FLAGS_level,
200      FLAGS_stage);
201    if (FLAGS_gpu.size() == 0
202        &amp;&amp; solver_param.solver_mode() == caffe::SolverParameter_SolverMode_GPU) {
203        if (solver_param.has_device_id()) {
204            FLAGS_gpu = &quot;&quot; +
205                boost::lexical_cast&lt;string&gt;(solver_param.device_id());
206        } else {  
207            FLAGS_gpu = &quot;&quot; + boost::lexical_cast&lt;string&gt;(0);
208        }
209    }
210    vector&lt;int&gt; gpus;
211    get_gpus(&amp;gpus);
212    if (gpus.size() == 0) {
213      LOG(INFO) &lt;&lt; &quot;Use CPU.&quot;;
214      Caffe::set_mode(Caffe::CPU);
215    } else {
216      ostringstream s;
217      for (int i = 0; i &lt; gpus.size(); ++i) {
218        s &lt;&lt; (i ? &quot;, &quot; : &quot;&quot;) &lt;&lt; gpus[i];
219      }
220      LOG(INFO) &lt;&lt; &quot;Using GPUs &quot; &lt;&lt; s.str();
221  #ifndef CPU_ONLY
222      cudaDeviceProp device_prop;
223      for (int i = 0; i &lt; gpus.size(); ++i) {
224        cudaGetDeviceProperties(&amp;device_prop, gpus[i]);
225        LOG(INFO) &lt;&lt; &quot;GPU &quot; &lt;&lt; gpus[i] &lt;&lt; &quot;: &quot; &lt;&lt; device_prop.name;
226      }
227  #endif
228      solver_param.set_device_id(gpus[0]);
229      Caffe::SetDevice(gpus[0]);
230      Caffe::set_mode(Caffe::GPU);
231      Caffe::set_solver_count(gpus.size());
232    }
233    caffe::SignalHandler signal_handler(
234          GetRequestedAction(FLAGS_sigint_effect),
235          GetRequestedAction(FLAGS_sighup_effect));
236    shared_ptr&lt;caffe::Solver&lt;float&gt; &gt;
237        solver(caffe::SolverRegistry&lt;float&gt;::CreateSolver(solver_param));
238    solver-&gt;SetActionFunction(signal_handler.GetActionFunction());
239    if (FLAGS_snapshot.size()) {
240      LOG(INFO) &lt;&lt; &quot;Resuming from &quot; &lt;&lt; FLAGS_snapshot;
241      solver-&gt;Restore(FLAGS_snapshot.c_str());
242    } else if (FLAGS_weights.size()) {
243      CopyLayers(solver.get(), FLAGS_weights);
244    }
245  #ifdef USE_MLSL
246    if (caffe::mn::is_multinode()) {
247      caffe::mn::barrier();
248      LOG(INFO) &lt;&lt; &quot;Configuring multinode setup&quot;;
249      if (!caffe::mn::is_param_server()) {
250        caffe::MultiSync&lt;float&gt; sync(solver);
251        LOG(INFO) &lt;&lt; &quot;Starting Multi-node Optimization in MLSL environment&quot;;
252        sync.run();
253      } else {
254        caffe::mn::AsyncParamServer&lt;float&gt; aps(solver);
255        LOG(INFO) &lt;&lt; &quot;Starting Parameter Server&quot;;
256        aps.Run();
257      }
258    } else
259  #endif &amp;bsol;* USE_MLSL */
260    if (gpus.size() &gt; 1) {
261      caffe::P2PSync&lt;float&gt; sync(solver, NULL, solver-&gt;param());
262      sync.Run(gpus);
263    } else {
264      LOG(INFO) &lt;&lt; &quot;Starting Optimization&quot;;
265      solver-&gt;Solve();
266    }
267    LOG(INFO) &lt;&lt; &quot;Optimization Done.&quot;;
268    return 0;
269  }
270  RegisterBrewFunction(train);
271  int test_detection(Net&lt;float&gt;&amp; caffe_net) {
272    std::map&lt;int, std::map&lt;int,
273      std::vector&lt;std::pair&lt;float, int&gt; &gt; &gt; &gt; all_true_pos;
274    std::map&lt;int, std::map&lt;int,
275      std::vector&lt;std::pair&lt;float, int&gt; &gt; &gt; &gt; all_false_pos;
276    std::map&lt;int, std::map&lt;int, int&gt; &gt; all_num_pos;
277    PERFORMANCE_INIT_MONITOR();
278    for (int i = 0; i &lt; FLAGS_iterations; ++i) {
279  #ifdef DEBUG
280      LOG(INFO) &lt;&lt; &quot;Iteration: &quot; &lt;&lt; i;
281  #endif
282      float iter_loss;
283      const vector&lt;Blob&lt;float&gt;*&gt;&amp; result = caffe_net.Forward(&amp;iter_loss);
284      for (int j = 0; j &lt; result.size(); ++j) {
285        const float* result_vec = result[j]-&gt;cpu_data();
286        int num_det = result[j]-&gt;height();
287        for (int k = 0; k &lt; num_det; ++k) {
<span onclick='openModal()' class='match'>288          int item_id = static_cast&lt;int&gt;(result_vec[k * 5]);
289          int label = static_cast&lt;int&gt;(result_vec[k * 5 + 1]);
290          if (item_id == -1) {
291            if (all_num_pos[j].find(label) == all_num_pos[j].end()) {
292              all_num_pos[j][label] = static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
293            } else {
294              all_num_pos[j][label] += static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
295            }
296          } else {
297            float score = result_vec[k * 5 + 2];
298            int tp = static_cast&lt;int&gt;(result_vec[k * 5 + 3]);
299            int fp = static_cast&lt;int&gt;(result_vec[k * 5 + 4]);
300            if (tp == 0 &amp;&amp; fp == 0) {
301              continue;
302            }
303            all_true_pos[j][label].push_back(std::make_pair(score, tp));
304            all_false_pos[j][label].push_back(std::make_pair(score, fp));
305          }
306        }
307      }
308    }
309    for (int i = 0; i &lt; all_true_pos.size(); ++i) {
</span>310      if (all_true_pos.find(i) == all_true_pos.end()) {
311        LOG(FATAL) &lt;&lt; &quot;Missing output_blob true_pos: &quot; &lt;&lt; i;
312      }
313      const std::map&lt;int, std::vector&lt;std::pair&lt;float, int&gt; &gt; &gt;&amp; true_pos =
314          all_true_pos.find(i)-&gt;second;
315      if (all_false_pos.find(i) == all_false_pos.end()) {
316        LOG(FATAL) &lt;&lt; &quot;Missing output_blob false_pos: &quot; &lt;&lt; i;
317      }
318      const std::map&lt;int, std::vector&lt;std::pair&lt;float, int&gt; &gt; &gt;&amp; false_pos =
319          all_false_pos.find(i)-&gt;second;
320      if (all_num_pos.find(i) == all_num_pos.end()) {
321        LOG(FATAL) &lt;&lt; &quot;Missing output_blob num_pos: &quot; &lt;&lt; i;
322      }
323      const std::map&lt;int, int&gt;&amp; num_pos = all_num_pos.find(i)-&gt;second;
324      std::map&lt;int, float&gt; APs;
325      float mAP = 0.;
326      for (std::map&lt;int, int&gt;::const_iterator it = num_pos.begin();
327           it != num_pos.end(); ++it) {
328        int label = it-&gt;first;
329        int label_num_pos = it-&gt;second;
330        if (true_pos.find(label) == true_pos.end()) {
331          LOG(WARNING) &lt;&lt; &quot;Missing true_pos for label: &quot; &lt;&lt; label;
332          continue;
333        }
334        const std::vector&lt;std::pair&lt;float, int&gt; &gt;&amp; label_true_pos =
335            true_pos.find(label)-&gt;second;
336        if (false_pos.find(label) == false_pos.end()) {
337          LOG(WARNING) &lt;&lt; &quot;Missing false_pos for label: &quot; &lt;&lt; label;
338          continue;
339        }
340        const std::vector&lt;std::pair&lt;float, int&gt; &gt;&amp; label_false_pos =
341            false_pos.find(label)-&gt;second;
342        std::vector&lt;float&gt; prec, rec;
343        caffe::ComputeAP(label_true_pos, label_num_pos, label_false_pos,
344                  &quot;11point&quot;, &amp;prec, &amp;rec, &amp;(APs[label]));
345        mAP += APs[label];
346      }
347      mAP /= num_pos.size();
348      const int output_blob_index = caffe_net.output_blob_indices()[i];
349      const string&amp; output_name = caffe_net.blob_names()[output_blob_index];
350      LOG(INFO) &lt;&lt; &quot;    Test net output #&quot; &lt;&lt; i &lt;&lt; &quot;: &quot; &lt;&lt; output_name &lt;&lt; &quot; = &quot;
351                &lt;&lt; mAP;
352    }
353    return 0;
354  }
355  int test() {
356    CHECK_GT(FLAGS_model.size(), 0) &lt;&lt; &quot;Need a model definition to score.&quot;;
357    CHECK_GT(FLAGS_weights.size(), 0) &lt;&lt; &quot;Need model weights to score.&quot;;
358    vector&lt;string&gt; stages = get_stages_from_flags(FLAGS_stage);
359    vector&lt;int&gt; gpus;
360    get_gpus(&amp;gpus);
361    if (gpus.size() != 0) {
362      LOG(INFO) &lt;&lt; &quot;Use GPU with device ID &quot; &lt;&lt; gpus[0];
363  #ifndef CPU_ONLY
364      cudaDeviceProp device_prop;
365      cudaGetDeviceProperties(&amp;device_prop, gpus[0]);
366      LOG(INFO) &lt;&lt; &quot;GPU device name: &quot; &lt;&lt; device_prop.name;
367  #endif
368      Caffe::SetDevice(gpus[0]);
369      Caffe::set_mode(Caffe::GPU);
370    } else {
371      LOG(INFO) &lt;&lt; &quot;Use CPU.&quot;;
372      Caffe::set_mode(Caffe::CPU);
373    }
374    Net&lt;float&gt; caffe_net(FLAGS_model, caffe::TEST, FLAGS_level, &amp;stages, NULL,
375                         FLAGS_engine);
376    caffe_net.CopyTrainedLayersFrom(FLAGS_weights);
377    if (FLAGS_sampling)
378        return 0;
379    LOG(INFO) &lt;&lt; &quot;Running for &quot; &lt;&lt; FLAGS_iterations &lt;&lt; &quot; iterations.&quot;;
380    if (FLAGS_detection) {
381      test_detection(caffe_net);
382      return 0;
383    }
384    vector&lt;int&gt; test_score_output_id;
385    vector&lt;float&gt; test_score;
386    float loss = 0;
387    for (int i = 0; i &lt; FLAGS_iterations; ++i) {
388      float iter_loss;
389      const vector&lt;Blob&lt;float&gt;*&gt;&amp; result =
390          caffe_net.Forward(&amp;iter_loss);
391      loss += iter_loss;
392      int idx = 0;
393      for (int j = 0; j &lt; result.size(); ++j) {
394        const float* result_vec = result[j]-&gt;cpu_data();
395        for (int k = 0; k &lt; result[j]-&gt;count(); ++k, ++idx) {
396          const float score = result_vec[k];
397          if (i == 0) {
398            test_score.push_back(score);
399            test_score_output_id.push_back(j);
400          } else {
401            test_score[idx] += score;
402          }
403           const std::string&amp; output_name = caffe_net.blob_names()[
404              caffe_net.output_blob_indices()[j]];
405          LOG(INFO) &lt;&lt; &quot;Batch &quot; &lt;&lt; i &lt;&lt; &quot;, &quot; &lt;&lt; output_name &lt;&lt; &quot; = &quot; &lt;&lt; score;
406        }
407      }
408    }
409    loss /= FLAGS_iterations;
410    LOG(INFO) &lt;&lt; &quot;Loss: &quot; &lt;&lt; loss;
411    for (int i = 0; i &lt; test_score.size(); ++i) {
412      const std::string&amp; output_name = caffe_net.blob_names()[
413          caffe_net.output_blob_indices()[test_score_output_id[i]]];
414      const float loss_weight = caffe_net.blob_loss_weights()[
415          caffe_net.output_blob_indices()[test_score_output_id[i]]];
416      std::ostringstream loss_msg_stream;
417      const float mean_score = test_score[i] / FLAGS_iterations;
418      if (loss_weight) {
419        loss_msg_stream &lt;&lt; &quot; (* &quot; &lt;&lt; loss_weight
420                        &lt;&lt; &quot; = &quot; &lt;&lt; loss_weight * mean_score &lt;&lt; &quot; loss)&quot;;
421      }
422      LOG(INFO) &lt;&lt; output_name &lt;&lt; &quot; = &quot; &lt;&lt; mean_score &lt;&lt; loss_msg_stream.str();
423    }
424    return 0;
425  }
426  RegisterBrewFunction(test);
427  int time() {
428    CHECK_GT(FLAGS_model.size(), 0) &lt;&lt; &quot;Need a model definition to time.&quot;;
429    caffe::Phase phase = get_phase_from_flags(caffe::TRAIN);
430    vector&lt;string&gt; stages = get_stages_from_flags(FLAGS_stage);
431    vector&lt;int&gt; gpus;
432    get_gpus(&amp;gpus);
433    if (gpus.size() != 0) {
434      LOG(INFO) &lt;&lt; &quot;Use GPU with device ID &quot; &lt;&lt; gpus[0];
435      Caffe::SetDevice(gpus[0]);
436      Caffe::set_mode(Caffe::GPU);
437    } else {
438      LOG(INFO) &lt;&lt; &quot;Use CPU.&quot;;
439      Caffe::set_mode(Caffe::CPU);
440    }
441    Net&lt;float&gt; caffe_net(FLAGS_model, phase, FLAGS_level, &amp;stages, NULL,
442                         FLAGS_engine);
443    PERFORMANCE_INIT_MONITOR();
444    LOG(INFO) &lt;&lt; &quot;Performing Forward&quot;;
445    float initial_loss;
446    caffe_net.Forward(&amp;initial_loss);
447    LOG(INFO) &lt;&lt; &quot;Initial loss: &quot; &lt;&lt; initial_loss;
448    if (!FLAGS_forward_only) {
449      LOG(INFO) &lt;&lt; &quot;Performing Backward&quot;;
450      caffe_net.Backward();
451    }
452    const vector&lt;shared_ptr&lt;Layer&lt;float&gt; &gt; &gt;&amp; layers = caffe_net.layers();
453    const vector&lt;vector&lt;Blob&lt;float&gt;*&gt; &gt;&amp; bottom_vecs = caffe_net.bottom_vecs();
454    const vector&lt;vector&lt;Blob&lt;float&gt;*&gt; &gt;&amp; top_vecs = caffe_net.top_vecs();
455    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
456        caffe_net.bottom_need_backward();
457    int warmup_iterations = 5;
458    for (int j = 0; j &lt; warmup_iterations; ++j) {
459      if (j == warmup_iterations - 1)
460        PERFORMANCE_START_RESETTING_MONITOR();
461      for (int i = 0; i &lt; layers.size(); ++i) {
462        layers[i]-&gt;Forward(bottom_vecs[i], top_vecs[i]);
463      }
464      if (!FLAGS_forward_only) {
465        for (int i = layers.size() - 1; i &gt;= 0; --i) {
466          layers[i]-&gt;Backward(top_vecs[i], bottom_need_backward[i],
467                              bottom_vecs[i]);
468        }
469      }
470    }
471    PERFORMANCE_STOP_RESETTING_MONITOR();
472    LOG(INFO) &lt;&lt; &quot;*** Benchmark begins ***&quot;;
473    LOG(INFO) &lt;&lt; &quot;Testing for &quot; &lt;&lt; FLAGS_iterations &lt;&lt; &quot; iterations.&quot;;
474    Timer total_timer;
475    total_timer.Start();
476    Timer forward_timer;
477    Timer backward_timer;
478    Timer timer;
479    std::vector&lt;double&gt; forward_time_per_layer(layers.size(), 0.0);
480    std::vector&lt;double&gt; backward_time_per_layer(layers.size(), 0.0);
481    double forward_time = 0.0;
482    std::vector&lt;double&gt; forward_time_iteration(FLAGS_iterations, 0.0);
483    double backward_time = 0.0;
484    for (int j = 0; j &lt; FLAGS_iterations; ++j) {
485      Timer iter_timer;
486      iter_timer.Start();
487      forward_timer.Start();
488      for (int i = 0; i &lt; layers.size(); ++i) {
489        timer.Start();
490        layers[i]-&gt;Forward(bottom_vecs[i], top_vecs[i]);
491        forward_time_per_layer[i] += timer.MicroSeconds();
492      }
493      forward_time_iteration[j] = forward_timer.MicroSeconds();
494      forward_time += forward_timer.MicroSeconds();
495      if (!FLAGS_forward_only) {
496        backward_timer.Start();
497        for (int i = layers.size() - 1; i &gt;= 0; --i) {
498          timer.Start();
499          layers[i]-&gt;Backward(top_vecs[i], bottom_need_backward[i],
500                              bottom_vecs[i]);
501          backward_time_per_layer[i] += timer.MicroSeconds();
502        }
503        backward_time += backward_timer.MicroSeconds();
504        LOG(INFO) &lt;&lt; &quot;Iteration: &quot; &lt;&lt; j + 1 &lt;&lt; &quot; forward-backward time: &quot;
505          &lt;&lt; iter_timer.MilliSeconds() &lt;&lt; &quot; ms.&quot;;
506      } else {
507        LOG(INFO) &lt;&lt; &quot;Iteration: &quot; &lt;&lt; j + 1 &lt;&lt; &quot; forward time: &quot;
508          &lt;&lt; iter_timer.MilliSeconds() &lt;&lt; &quot; ms.&quot;;
509      }
510    }
511    LOG(INFO) &lt;&lt; &quot;Average time per layer: &quot;;
512    for (int i = 0; i &lt; layers.size(); ++i) {
513      const caffe::string&amp; layername = layers[i]-&gt;layer_param().name();
514      LOG(INFO) &lt;&lt; std::setfill(&#x27; &#x27;) &lt;&lt; std::setw(10) &lt;&lt; layername &lt;&lt;
515        &quot;\tforward: &quot; &lt;&lt; forward_time_per_layer[i] / 1000 /
516        FLAGS_iterations &lt;&lt; &quot; ms.&quot;;
517      if (!FLAGS_forward_only) {
518        LOG(INFO) &lt;&lt; std::setfill(&#x27; &#x27;) &lt;&lt; std::setw(10) &lt;&lt; layername  &lt;&lt;
519          &quot;\tbackward: &quot; &lt;&lt; backward_time_per_layer[i] / 1000 /
520          FLAGS_iterations &lt;&lt; &quot; ms.&quot;;
521      }
522    }
523    total_timer.Stop();
524    for (int j = 0; j &lt; FLAGS_iterations; ++j)
525      LOG(INFO) &lt;&lt; &quot;###&quot; &lt;&lt; j &lt;&lt; &quot;:&quot; &lt;&lt; forward_time_iteration[j] / 1000 &lt;&lt; &quot; ms.&quot;;
526    LOG(INFO) &lt;&lt; &quot;Average Forward pass: &quot; &lt;&lt; forward_time / 1000 /
527      FLAGS_iterations &lt;&lt; &quot; ms.&quot;;
528    if (!FLAGS_forward_only) {
529      LOG(INFO) &lt;&lt; &quot;Average Backward pass: &quot; &lt;&lt; backward_time / 1000 /
530        FLAGS_iterations &lt;&lt; &quot; ms.&quot;;
531      LOG(INFO) &lt;&lt; &quot;Average Forward-Backward: &quot; &lt;&lt; total_timer.MilliSeconds() /
532        FLAGS_iterations &lt;&lt; &quot; ms.&quot;;
533    }
534    LOG(INFO) &lt;&lt; &quot;Total Time: &quot; &lt;&lt; total_timer.MilliSeconds() &lt;&lt; &quot; ms.&quot;;
535    LOG(INFO) &lt;&lt; &quot;*** Benchmark ends ***&quot;;
536    return 0;
537  }
538  RegisterBrewFunction(time);
539  #include &lt;stdio.h&gt;
540  #include &quot;caffe/util/compareToolUtilities.h&quot;
541  int collect() {
542    #ifndef DETERMINISTIC
543      LOG(ERROR) &lt;&lt; &quot;Recompile caffe with DETERMINISTIC to run collect tool&quot;;
544      return 1;
545    #endif
546    CHECK_GT(FLAGS_model.size(), 0) &lt;&lt; &quot;Need a model definition!&quot;;
547    vector&lt;int&gt; gpus;
548    get_gpus(&amp;gpus);
549    bool use_gpu = (gpus.size() != 0);
550    if (use_gpu) {
551      LOG(INFO) &lt;&lt; &quot;Use GPU with device ID &quot; &lt;&lt; gpus[0];
552      Caffe::SetDevice(gpus[0]);
553      Caffe::set_mode(Caffe::GPU);
554    } else {
555      LOG(INFO) &lt;&lt; &quot;Use CPU.&quot;;
556      Caffe::set_mode(Caffe::CPU);
557    }
558    boost::filesystem::path dir(FLAGS_collect_dir);
559    if (!boost::filesystem::exists(dir)) {
560        if (!boost::filesystem::create_directory(dir)) {
561            LOG(ERROR) &lt;&lt; &quot;Could not create directory for output files&quot;;
562        }
563    }
564    return collectAndCheckLayerData(true, use_gpu, FLAGS_collect_dir.c_str());
565  }
566  RegisterBrewFunction(collect);
567  int compare() {
568    #ifndef DETERMINISTIC
569      LOG(ERROR) &lt;&lt; &quot;Recompile caffe with DETERMINISTIC to run compare tool&quot;;
570      return 1;
571    #endif
572    CHECK_GT(FLAGS_model.size(), 0) &lt;&lt; &quot;Need a model definition!&quot;;
573    vector&lt;int&gt; gpus;
574    get_gpus(&amp;gpus);
575    bool use_gpu = (gpus.size() != 0);
576    if (use_gpu) {
577      LOG(INFO) &lt;&lt; &quot;Use GPU with device ID &quot; &lt;&lt; gpus[0];
578      Caffe::SetDevice(gpus[0]);
579      Caffe::set_mode(Caffe::GPU);
580    } else {
581      LOG(INFO) &lt;&lt; &quot;Use CPU.&quot;;
582      Caffe::set_mode(Caffe::CPU);
583    }
584    boost::filesystem::path dir(FLAGS_compare_output_dir);
585    if (!boost::filesystem::exists(dir)) {
586        if (!boost::filesystem::create_directory(dir)) {
587            LOG(ERROR) &lt;&lt; &quot;Could not create directory for output files&quot;;
588        }
589    }
590    return collectAndCheckLayerData(false,
591      use_gpu, FLAGS_compare_output_dir.c_str());
592  }
593  RegisterBrewFunction(compare);
594  int main(int argc, char** argv) {
595    FLAGS_alsologtostderr = 1;
596    gflags::SetVersionString(AS_STRING(CAFFE_VERSION));
597    gflags::SetUsageMessage(&quot;command line brew\n&quot;
598        &quot;usage: caffe &lt;command&gt; &lt;args&gt;\n\n&quot;
599        &quot;commands:\n&quot;
600        &quot;  train           train or finetune a model\n&quot;
601        &quot;  test            score a model\n&quot;
602        &quot;  device_query    show GPU diagnostic information\n&quot;
603        &quot;  time            benchmark model execution time\n&quot;
604        &quot;  collect         collects layer data on specified device\n&quot;
605        &quot;  compare         collects layer data using inputs from other device&quot;);
606    caffe::GlobalInit(&amp;argc, &amp;argv);
607  #ifdef USE_MLSL
608    caffe::mn::nGroup = FLAGS_n_group;
609    if (caffe::mn::nGroup &lt;= 0) {
610        LOG(ERROR) &lt;&lt; &quot;Invalid number of group: &quot; &lt;&lt; caffe::mn::nGroup;
611        return 1;
612    }
613    caffe::mn::nServer = FLAGS_n_server;
614    caffe::mn::init(&amp;argc, &amp;argv);
615    if (caffe::mn::get_group_size() &lt;= 0) {
616        LOG(ERROR) &lt;&lt; &quot;Invalid group size: &quot; &lt;&lt; caffe::mn::get_group_size();
617        return 1;
618    }
619    CHECK_EQ(caffe::mn::get_world_size(),
620             caffe::mn::nGroup * caffe::mn::get_group_size() + caffe::mn::nServer);
621    if (caffe::mn::nGroup &gt; 1) {
622      CHECK_GE(caffe::mn::nServer, 1)
623        &lt;&lt; &quot;Expect there exists parameter server to support multiple groups&quot;;
624    }
625    if (caffe::mn::get_node_rank() == 0) {
626      LOG(INFO) &lt;&lt; &quot;Number of groups: &quot; &lt;&lt; caffe::mn::nGroup
627                &lt;&lt; &quot;, group size: &quot; &lt;&lt; caffe::mn::get_group_size()
628                &lt;&lt; &quot;, number of parameter servers: &quot; &lt;&lt; caffe::mn::nServer;
629    }
630  #endif
631    if (argc == 2) {
632  #ifdef WITH_PYTHON_LAYER
633      try {
634  #endif
635        int ret = GetBrewFunction(caffe::string(argv[1]))();
636        return ret;
637  #ifdef WITH_PYTHON_LAYER
638      } catch (bp::error_already_set) {
639        PyErr_Print();
640        return 1;
641      }
642  #endif
643    } else {
644      gflags::ShowUsageWithFlagsRestrict(argv[0], &quot;tools/caffe&quot;);
645    }
646    return 0;
647  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-solver.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-caffe.cpp</div>
                </div>
                <div class="column column_space"><pre><code>478          int item_id = static_cast&lt;int&gt;(result_vec[k * 5]);
479          int label = static_cast&lt;int&gt;(result_vec[k * 5 + 1]);
480          if (item_id == -1) {
481            if (all_num_pos[j].find(label) == all_num_pos[j].end()) {
482              all_num_pos[j][label] = static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
483            } else {
484              all_num_pos[j][label] += static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
485            }
486          } else {
487            float score = result_vec[k * 5 + 2];
488            int tp = static_cast&lt;int&gt;(result_vec[k * 5 + 3]);
489            int fp = static_cast&lt;int&gt;(result_vec[k * 5 + 4]);
490            if (tp == 0 &amp;&amp; fp == 0) {
491              continue;
492            }
493            all_true_pos[j][label].push_back(std::make_pair(score, tp));
494            all_false_pos[j][label].push_back(std::make_pair(score, fp));
495          }
496        }
497      }
498    }
499    if (requested_early_exit_) {
</pre></code></div>
                <div class="column column_space"><pre><code>288          int item_id = static_cast&lt;int&gt;(result_vec[k * 5]);
289          int label = static_cast&lt;int&gt;(result_vec[k * 5 + 1]);
290          if (item_id == -1) {
291            if (all_num_pos[j].find(label) == all_num_pos[j].end()) {
292              all_num_pos[j][label] = static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
293            } else {
294              all_num_pos[j][label] += static_cast&lt;int&gt;(result_vec[k * 5 + 2]);
295            }
296          } else {
297            float score = result_vec[k * 5 + 2];
298            int tp = static_cast&lt;int&gt;(result_vec[k * 5 + 3]);
299            int fp = static_cast&lt;int&gt;(result_vec[k * 5 + 4]);
300            if (tp == 0 &amp;&amp; fp == 0) {
301              continue;
302            }
303            all_true_pos[j][label].push_back(std::make_pair(score, tp));
304            all_false_pos[j][label].push_back(std::make_pair(score, fp));
305          }
306        }
307      }
308    }
309    for (int i = 0; i &lt; all_true_pos.size(); ++i) {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    