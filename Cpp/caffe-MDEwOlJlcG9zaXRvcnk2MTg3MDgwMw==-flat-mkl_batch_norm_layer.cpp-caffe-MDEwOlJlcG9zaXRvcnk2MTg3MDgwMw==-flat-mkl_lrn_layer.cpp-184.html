
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 15, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_batch_norm_layer.cpp</h3>
            <pre><code>1  #if defined(MKL2017_SUPPORTED)
2  #include &lt;vector&gt;
3  #include &quot;caffe/filler.hpp&quot;
4  #include &quot;caffe/layer.hpp&quot;
5  #include &quot;caffe/layers/mkl_layers.hpp&quot;
6  #include &quot;caffe/util/math_functions.hpp&quot;
7  #include &quot;caffe/util/performance.hpp&quot;
8  namespace caffe {
9  template &lt;typename Dtype&gt;
10  MKLBatchNormLayer&lt;Dtype&gt;::~MKLBatchNormLayer() {
11    dnnDelete&lt;Dtype&gt;(batchNormFwd);
12    dnnDelete&lt;Dtype&gt;(batchNormFwdInference);
13    dnnDelete&lt;Dtype&gt;(batchNormBwd);
14    dnnLayoutDelete&lt;Dtype&gt;(layout_usr_);
15    for (int i = 0; i &lt; mean_buffers_.size(); i++) {
16      dnnReleaseBuffer&lt;Dtype&gt;(mean_buffers_[i]);
17    }
18    for (int i = 0; i &lt; variance_buffers_.size(); i++) {
19      dnnReleaseBuffer&lt;Dtype&gt;(variance_buffers_[i]);
20    }
21    dnnReleaseBuffer&lt;Dtype&gt;(scaleShift_buffer_);
22    dnnReleaseBuffer&lt;Dtype&gt;(diffScaleShift_buffer_);
23  }
24  template &lt;typename Dtype&gt;
25  void MKLBatchNormLayer&lt;Dtype&gt;::Init(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
26        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
27    moving_average_fraction_ =
28                  this-&gt;layer_param_.batch_norm_param().moving_average_fraction();
29    eps_ = this-&gt;layer_param_.batch_norm_param().eps();
30    use_weight_bias_ = this-&gt;layer_param_.batch_norm_param().use_weight_bias();
31    bias_term_ = this-&gt;layer_param_.batch_norm_param().bias_term();
32    use_global_stats_ = this-&gt;phase_ == TEST;
33    if (this-&gt;layer_param_.batch_norm_param().has_use_global_stats())
34      use_global_stats_ = this-&gt;layer_param_.batch_norm_param().use_global_stats();
35    num_stats_batches_ = 1;
36    stats_batch_size_ = bottom[0]-&gt;shape(0);
37    BatchNormParameter param = this-&gt;layer_param_.batch_norm_param();
38    if (!use_global_stats_ &amp;&amp; param.stats_batch_size() &gt; 0) {
39      CHECK_EQ(bottom[0]-&gt;shape(0) % param.stats_batch_size(), 0);
40      num_stats_batches_ = bottom[0]-&gt;shape(0) / param.stats_batch_size();
41      stats_batch_size_ = param.stats_batch_size();
42    }
43    CHECK(use_weight_bias_) &lt;&lt; &quot;BatchNorm without scaling have not supported yet&quot;;
44    size_t dim = 4, sizes[4], strides[4];
45    channels_ = bottom[0]-&gt;channels();
46    height_   = bottom[0]-&gt;height();
47    width_    = bottom[0]-&gt;width();
48    num_      = bottom[0]-&gt;num();
49    sizes[0] = width_;
50    sizes[1] = height_;
51    sizes[2] = channels_;
52    sizes[3] = num_;
53    strides[0] = 1;
54    strides[1] = sizes[0];
55    strides[2] = sizes[0]*sizes[1];
56    strides[3] = sizes[0]*sizes[1]*sizes[2];
57    fwd_bottom_data-&gt;name = &quot;fwd_bottom_data   @ &quot; + this-&gt;layer_param_.name();
58    fwd_top_data-&gt;name =    &quot;fwd_top_data      @ &quot; + this-&gt;layer_param_.name();
59    bwd_bottom_diff-&gt;name = &quot;bwd_bottom_diff   @ &quot; + this-&gt;layer_param_.name();
60    bwd_top_diff-&gt;name =    &quot;bwd_top_diff      @ &quot; + this-&gt;layer_param_.name();
61    fwd_bottom_data-&gt;create_user_layout(dim, sizes, strides, false);
62    fwd_top_data   -&gt;create_user_layout(dim, sizes, strides, false);
63    bwd_bottom_diff-&gt;create_user_layout(dim, sizes, strides, false);
64    bwd_top_diff   -&gt;create_user_layout(dim, sizes, strides, false);
65    sizes[3] /= num_stats_batches_;
66    dnnError_t e;
67    dnnLayoutDelete&lt;Dtype&gt;(layout_usr_);
68    e = dnnLayoutCreate&lt;Dtype&gt;(&amp;layout_usr_, dim, sizes, strides);
69    CHECK_EQ(e, E_SUCCESS);
70    for (int i = 0; i &lt; mean_buffers_.size(); i++) {
71      dnnReleaseBuffer&lt;Dtype&gt;(mean_buffers_[i]);
72    }
73    for (int i = 0; i &lt; variance_buffers_.size(); i++) {
74      dnnReleaseBuffer&lt;Dtype&gt;(variance_buffers_[i]);
75    }
76    mean_buffers_.resize(num_stats_batches_, NULL);
77    variance_buffers_.resize(num_stats_batches_, NULL);
78    dnnReleaseBuffer&lt;Dtype&gt;(scaleShift_buffer_);
79    dnnReleaseBuffer&lt;Dtype&gt;(diffScaleShift_buffer_);
80    dnnDelete&lt;Dtype&gt;(batchNormFwd);
81    dnnDelete&lt;Dtype&gt;(batchNormFwdInference);
82    dnnDelete&lt;Dtype&gt;(batchNormBwd);
83    this-&gt;blobs_.resize(3);
84    if (use_weight_bias_) {
85      if ( bias_term_ ) {
86          this-&gt;blobs_.resize(5);
87      } else {
88          this-&gt;blobs_.resize(4);
89      }
90      vector&lt;int&gt; scaleshift_shape(1);
91      scaleshift_shape[0] = channels_;
92      this-&gt;blobs_[3].reset(new Blob&lt;Dtype&gt;(scaleshift_shape));
93      FillerParameter filler_param(
94        this-&gt;layer_param_.batch_norm_param().filler());
95      if (!this-&gt;layer_param_.batch_norm_param().has_filler()) {
96        filler_param.set_type(&quot;constant&quot;);
97        filler_param.set_value(1);
98      }
99      shared_ptr&lt;Filler&lt;Dtype&gt; &gt; filler(GetFiller&lt;Dtype&gt;(filler_param));
100      filler-&gt;Fill(this-&gt;blobs_[3].get());
101      if ( bias_term_ ) {
102        this-&gt;blobs_[4].reset(new Blob&lt;Dtype&gt;(scaleshift_shape));
103        FillerParameter bias_filler_param(
104          this-&gt;layer_param_.batch_norm_param().bias_filler());
105        if (!this-&gt;layer_param_.batch_norm_param().has_bias_filler()) {
106          bias_filler_param.set_type(&quot;constant&quot;);
107          bias_filler_param.set_value(0);
108        }
109        shared_ptr&lt;Filler&lt;Dtype&gt; &gt; bias_filler(
110          GetFiller&lt;Dtype&gt;(bias_filler_param));
111        bias_filler-&gt;Fill(this-&gt;blobs_[4].get());
112      }
113    }
114    vector&lt;int&gt; sz;
115    sz.push_back(channels_);
116    this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(sz));
117    this-&gt;blobs_[1].reset(new Blob&lt;Dtype&gt;(sz));
118    sz[0]=1;
119    this-&gt;blobs_[2].reset(new Blob&lt;Dtype&gt;(sz));
120    for (int i = 0; i &lt; 3; ++i) {
121      caffe_set(this-&gt;blobs_[i]-&gt;count(), Dtype(0),
122                this-&gt;blobs_[i]-&gt;mutable_cpu_data());
123    }
124    for (int i = 0; i &lt; 3; ++i) {
125      if (this-&gt;layer_param_.param_size() == i) {
126        ParamSpec* fixed_param_spec = this-&gt;layer_param_.add_param();
127        fixed_param_spec-&gt;set_lr_mult(0.f);
128      } else {
129        CHECK_EQ(this-&gt;layer_param_.param(i).lr_mult(), 0.f)
130            &lt;&lt; &quot;Cannot configure batch normalization statistics as layer &quot;
131            &lt;&lt; &quot;parameters.&quot;;
132      }
133    }
134  }
135  template &lt;typename Dtype&gt;
136  void MKLBatchNormLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
137        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
138    Init(bottom, top);
139  }
140  template &lt;typename Dtype&gt;
141  void MKLBatchNormLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
142        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
143    bool re_init = true;
144    if (channels_ == bottom[0]-&gt;channels() &amp;&amp;
145        height_ == bottom[0]-&gt;height() &amp;&amp;
146        width_ == bottom[0]-&gt;width()) {
147      re_init = false;
148    }
149    if (bottom[0] == top[0]) {  
150      temp_.ReshapeLike(*bottom[0]);
151    } else {
152      channels_ = bottom[0]-&gt;channels();
153      height_ = bottom[0]-&gt;height();
154      width_ = bottom[0]-&gt;width();
155      num_ = bottom[0]-&gt;num();
156      top[0]-&gt;Reshape(num_, channels_, height_, width_);
157    }
158    if (re_init == true) {
159      Init(bottom, top);
160    } else if (num_ != bottom[0]-&gt;num()) { 
161      size_t dim = 4, sizes[4], strides[4];
162      sizes[0] = width_;
163      sizes[1] = height_;
164      sizes[2] = channels_;
165      sizes[3] = num_;
166      strides[0] = 1;
167      strides[1] = sizes[0];
168      strides[2] = sizes[0]*sizes[1];
169      strides[3] = sizes[0]*sizes[1]*sizes[2];
170      fwd_bottom_data-&gt;create_user_layout(dim, sizes, strides, false);
171      fwd_top_data   -&gt;create_user_layout(dim, sizes, strides, false);
172      bwd_bottom_diff-&gt;create_user_layout(dim, sizes, strides, false);
173      bwd_top_diff   -&gt;create_user_layout(dim, sizes, strides, false);
174      sizes[3] /= num_stats_batches_;
175      dnnError_t e;
176      dnnLayoutDelete&lt;Dtype&gt;(layout_usr_);
177      e = dnnLayoutCreate&lt;Dtype&gt;(&amp;layout_usr_, dim, sizes, strides);
178      CHECK_EQ(e, E_SUCCESS);
179    }
180  }
181  template &lt;typename Dtype&gt;
182  void MKLBatchNormLayer&lt;Dtype&gt;::ForwardStatsBatch_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
183      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, int stats_batch_idx) {
184    long data_offset = stats_batch_idx * stats_batch_size_ * bottom[0]-&gt;count(1);
185    void* bottom_data =
186      reinterpret_cast&lt;void *&gt;(const_cast&lt;Dtype*&gt;(bottom[0]-&gt;prv_data()));
187    int is_first_pass = 0;
188    long amount_to_copy =0;
189    if (NULL != bottom_data &amp;&amp; num_stats_batches_ == 1) {
190      amount_to_copy = bottom[0]-&gt;prv_data_count();
191      if (batchNormFwd == NULL) {
192        is_first_pass = 1;
193        CHECK((bottom[0]-&gt;get_prv_data_descriptor())-&gt;get_descr_type() ==
194          PrvMemDescr::PRV_DESCR_MKL2017);
195        shared_ptr&lt;MKLData&lt;Dtype&gt; &gt; mem_descr
196          =  boost::static_pointer_cast&lt;MKLData&lt;Dtype&gt; &gt;(
197             bottom[0]-&gt;get_prv_data_descriptor());
198        CHECK(mem_descr != NULL);
199        DLOG(INFO) &lt;&lt; &quot;Using layout of &quot; &lt;&lt; mem_descr-&gt;name
200                &lt;&lt; &quot; as input layout for &quot; &lt;&lt; this-&gt;layer_param_.name();
201        fwd_bottom_data = mem_descr;
202        dnnError_t e;
203        e = dnnBatchNormalizationCreateForward&lt;Dtype&gt;(
204          &amp;batchNormFwd, NULL, mem_descr-&gt;layout_int, eps_, dnnUseScaleShift);
205        CHECK_EQ(e, E_SUCCESS);
206        e = dnnBatchNormalizationCreateForward&lt;Dtype&gt;(
207          &amp;batchNormFwdInference, NULL, mem_descr-&gt;layout_int, eps_,
208                                      dnnUseScaleShift | dnnUseInputMeanVariance);
209        CHECK_EQ(e, E_SUCCESS);
210        fwd_top_data   -&gt;create_internal_layout(batchNormFwd, dnnResourceDst);
211        bwd_top_diff   -&gt;create_internal_layout(batchNormFwd, dnnResourceDst);
212        bwd_bottom_diff-&gt;create_internal_layout(batchNormFwd, dnnResourceSrc);
213         if (!use_global_stats_) {
214           e = dnnBatchNormalizationCreateBackward&lt;Dtype&gt;(
215              &amp;batchNormBwd, NULL, mem_descr-&gt;layout_int, eps_, dnnUseScaleShift);
216           CHECK_EQ(e, E_SUCCESS);
217         } else {
218           e = dnnBatchNormalizationCreateBackward&lt;Dtype&gt;(
219              &amp;batchNormBwd, NULL, mem_descr-&gt;layout_int, eps_, dnnUseScaleShift | dnnUseInputMeanVariance);
220           CHECK_EQ(e, E_SUCCESS);
221         }
222      }
223    } else {
224      DLOG(INFO) &lt;&lt; &quot;Using cpu_data in MKLBatchNormLayer.&quot;;
225      if (batchNormFwd == NULL) {
226        is_first_pass = 1;
227        dnnError_t e;
228        e = dnnBatchNormalizationCreateForward&lt;Dtype&gt;(
229          &amp;batchNormFwd, NULL, layout_usr_, eps_, dnnUseScaleShift);
230        CHECK_EQ(e, E_SUCCESS);
231        e = dnnBatchNormalizationCreateForward&lt;Dtype&gt;(
232          &amp;batchNormFwdInference, NULL, layout_usr_, eps_,
233                                      dnnUseScaleShift | dnnUseInputMeanVariance);
234        CHECK_EQ(e, E_SUCCESS);
235        if (!use_global_stats_) {
236          e = dnnBatchNormalizationCreateBackward&lt;Dtype&gt;(
237            &amp;batchNormBwd, NULL, layout_usr_, eps_, dnnUseScaleShift);
238          CHECK_EQ(e, E_SUCCESS);
239        } else {
240          e = dnnBatchNormalizationCreateBackward&lt;Dtype&gt;(
241            &amp;batchNormBwd, NULL, layout_usr_, eps_, dnnUseScaleShift | dnnUseInputMeanVariance);
242          CHECK_EQ(e, E_SUCCESS);
243        }
244      }
245      bottom_data =
246        reinterpret_cast&lt;void *&gt;(const_cast&lt;Dtype*&gt;(bottom[0]-&gt;cpu_data()));
247      amount_to_copy = bottom[0]-&gt;count() / num_stats_batches_;
248    }
249    if (is_first_pass == 1) {
250        dnnError_t e;
251        dnnLayout_t mean_buffer_l = NULL;
<span onclick='openModal()' class='match'>252        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
253          &amp;mean_buffer_l, batchNormFwd, dnnResourceMean);
254        CHECK_EQ(e, E_SUCCESS);
</span>255        for (int i = 0; i &lt; num_stats_batches_; i++) {
256          e = dnnAllocateBuffer&lt;Dtype&gt;(
257            reinterpret_cast&lt;void**&gt;(&amp;mean_buffers_[i]), mean_buffer_l);
258          CHECK_EQ(e, E_SUCCESS);
259        }
260        dnnLayoutDelete&lt;Dtype&gt;(mean_buffer_l);
261        dnnLayout_t variance_buffer_l = NULL;
262        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
263          &amp;variance_buffer_l, batchNormFwd, dnnResourceVariance);
264        CHECK_EQ(e, E_SUCCESS);
265        for (int i = 0; i &lt; num_stats_batches_; i++) {
266          e = dnnAllocateBuffer&lt;Dtype&gt;(
267            reinterpret_cast&lt;void**&gt;(&amp;variance_buffers_[i]), variance_buffer_l);
268          CHECK_EQ(e, E_SUCCESS);
269        }
270        dnnLayoutDelete&lt;Dtype&gt;(variance_buffer_l);
271         dnnLayout_t diffScaleShift_buffer_l = NULL;
272        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
273          &amp;diffScaleShift_buffer_l, batchNormBwd, dnnResourceDiffScaleShift);
274        CHECK_EQ(e, E_SUCCESS);
275        e = dnnAllocateBuffer&lt;Dtype&gt;(
276          reinterpret_cast&lt;void**&gt;(&amp;diffScaleShift_buffer_), diffScaleShift_buffer_l);
277        CHECK_EQ(e, E_SUCCESS);
278        dnnLayoutDelete&lt;Dtype&gt;(diffScaleShift_buffer_l);
279        dnnLayout_t scaleShift_buffer_l = NULL;
280        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
281          &amp;scaleShift_buffer_l, batchNormFwd, dnnResourceScaleShift);
282        CHECK_EQ(e, E_SUCCESS);
283        e = dnnAllocateBuffer&lt;Dtype&gt;(
284          reinterpret_cast&lt;void**&gt;(&amp;scaleShift_buffer_), scaleShift_buffer_l);
285        CHECK_EQ(e, E_SUCCESS);
286        dnnLayoutDelete&lt;Dtype&gt;(scaleShift_buffer_l);
287        if (!use_weight_bias_) {
288           for (int i = 0; i &lt; channels_; i++) {
289              scaleShift_buffer_[i] = 1.0;
290              scaleShift_buffer_[channels_ + i] = 0;
291           }
292        }
293    }
294    if (use_weight_bias_) {
295      for (int i = 0; i &lt; channels_; i++) {
296        scaleShift_buffer_[i] = this-&gt;blobs_[3]-&gt;cpu_data()[i];
297        scaleShift_buffer_[channels_ + i] = 0;
298        if (bias_term_) {
299           scaleShift_buffer_[channels_ + i] = this-&gt;blobs_[4]-&gt;cpu_data()[i];
300        }
301      }
302    }
303    if (bottom[0] == top[0] &amp;&amp; this-&gt;phase_ == TRAIN) {
304      caffe_copy(amount_to_copy, static_cast&lt;Dtype*&gt;(bottom_data) + data_offset,
305                 temp_.mutable_cpu_data() + data_offset);
306    }
307    if (use_global_stats_) {
308      const Dtype scale_factor = this-&gt;blobs_[2]-&gt;cpu_data()[0] == 0 ?
309                                 0 : 1 / this-&gt;blobs_[2]-&gt;cpu_data()[0];
310      caffe_cpu_scale(this-&gt;blobs_[0]-&gt;count(), scale_factor,
311                      this-&gt;blobs_[0]-&gt;cpu_data(), mean_buffers_[stats_batch_idx]);
312      caffe_cpu_scale(this-&gt;blobs_[1]-&gt;count(), scale_factor,
313                      this-&gt;blobs_[1]-&gt;cpu_data(), variance_buffers_[stats_batch_idx]);
314    }
315    dnnError_t e;
316    void* BatchNorm_res[dnnResourceNumber];
317    BatchNorm_res[dnnResourceMean] = mean_buffers_[stats_batch_idx];
318    BatchNorm_res[dnnResourceVariance] = variance_buffers_[stats_batch_idx];
319    BatchNorm_res[dnnResourceSrc] = (Dtype*)bottom_data + data_offset;
320    BatchNorm_res[dnnResourceScaleShift] = scaleShift_buffer_;
321    if (fwd_top_data-&gt;conversion_needed()) {
322      top[0]-&gt;set_prv_data_descriptor(fwd_top_data);
323      data_offset = stats_batch_idx * (top[0]-&gt;prv_data_count() / num_stats_batches_);
324      BatchNorm_res[dnnResourceDst] =
325              reinterpret_cast&lt;void *&gt;(top[0]-&gt;mutable_prv_data() + data_offset);
326    } else {
327      BatchNorm_res[dnnResourceDst] =
328              reinterpret_cast&lt;void *&gt;(top[0]-&gt;mutable_cpu_data() + data_offset);
329      DLOG(INFO) &lt;&lt; &quot;Using cpu_data for top in DnnBatchNorm.&quot;;
330    }
331    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME(&quot;FW&quot;));
332    PERFORMANCE_MEASUREMENT_BEGIN();
333    e = dnnExecute&lt;Dtype&gt;(use_global_stats_? batchNormFwdInference : batchNormFwd,
334                                                                   BatchNorm_res);
335    PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
336    CHECK_EQ(e, E_SUCCESS);
337    if (!use_global_stats_) {
338      this-&gt;blobs_[2]-&gt;mutable_cpu_data()[0] *= moving_average_fraction_;
339      this-&gt;blobs_[2]-&gt;mutable_cpu_data()[0] += 1;
340      caffe_cpu_axpby(this-&gt;blobs_[0]-&gt;count(), Dtype(1), mean_buffers_[stats_batch_idx],
341          moving_average_fraction_, this-&gt;blobs_[0]-&gt;mutable_cpu_data());
342      int m = bottom[0]-&gt;count()/num_stats_batches_/channels_;
343      Dtype bias_correction_factor = m &gt; 1 ? Dtype(m)/(m-1) : 1;
344      caffe_cpu_axpby(this-&gt;blobs_[1]-&gt;count(), bias_correction_factor,
345          variance_buffers_[stats_batch_idx], moving_average_fraction_,
346          this-&gt;blobs_[1]-&gt;mutable_cpu_data());
347    }
348  }
349  template &lt;typename Dtype&gt;
350  void MKLBatchNormLayer&lt;Dtype&gt;::BackwardStatsBatch_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
351      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
352      int stats_batch_idx) {
353    long data_offset = stats_batch_idx * stats_batch_size_ * bottom[0]-&gt;count(1);
354    void *bottom_data = NULL;
355    if (bottom[0] == top[0]) {
356      bottom_data = reinterpret_cast&lt;void *&gt;(
357                          const_cast&lt;Dtype*&gt;(temp_.cpu_data()));
358    } else {
359      bottom_data =
360              reinterpret_cast&lt;void *&gt;(
361                          const_cast&lt;Dtype*&gt;(bottom[0]-&gt;prv_data()));
362      if (NULL == bottom_data || num_stats_batches_ &gt; 1)
363        bottom_data =
364              reinterpret_cast&lt;void *&gt;(
365                          const_cast&lt;Dtype*&gt;(bottom[0]-&gt;cpu_data()));
366    }
367    dnnError_t e;
368    void* BatchNorm_res[dnnResourceNumber];
369    BatchNorm_res[dnnResourceMean] = mean_buffers_[stats_batch_idx];
370    BatchNorm_res[dnnResourceVariance] = variance_buffers_[stats_batch_idx];
371    BatchNorm_res[dnnResourceSrc] = (Dtype*)bottom_data + data_offset;
372    BatchNorm_res[dnnResourceScaleShift] = scaleShift_buffer_;
373    BatchNorm_res[dnnResourceDiffScaleShift] = diffScaleShift_buffer_;
374    BatchNorm_res[dnnResourceDiffDst] =
375      bwd_top_diff-&gt;get_converted_prv(top[0], true) + data_offset;
376    if (bwd_bottom_diff-&gt;conversion_needed()) {
377      bottom[0]-&gt;set_prv_diff_descriptor(bwd_bottom_diff);
378      data_offset = stats_batch_idx * (bottom[0]-&gt;prv_diff_count() / num_stats_batches_);
379      BatchNorm_res[dnnResourceDiffSrc] = bottom[0]-&gt;mutable_prv_diff() + data_offset;
380    } else {
381      BatchNorm_res[dnnResourceDiffSrc] = bottom[0]-&gt;mutable_cpu_diff() + data_offset;
382    }
383    PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKL_NAME(&quot;BW&quot;));
384    PERFORMANCE_MEASUREMENT_BEGIN();
385    e = dnnExecute&lt;Dtype&gt;(batchNormBwd, BatchNorm_res);
386    PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
387    CHECK_EQ(e, E_SUCCESS);
388    if (use_weight_bias_) {
389      caffe_cpu_axpby(this-&gt;blobs_[3]-&gt;count(), (Dtype)1.,
390                      diffScaleShift_buffer_, (Dtype)1., this-&gt;blobs_[3]-&gt;mutable_cpu_diff());
391      if (bias_term_)
392        caffe_cpu_axpby(this-&gt;blobs_[4]-&gt;count(), (Dtype)1.,
393                        diffScaleShift_buffer_ + channels_,
394                        (Dtype)1., this-&gt;blobs_[4]-&gt;mutable_cpu_diff());
395      else
396        caffe_set(this-&gt;blobs_[4]-&gt;count(),
397                      static_cast&lt;Dtype&gt;(0), this-&gt;blobs_[4]-&gt;mutable_cpu_diff());
398    }
399  }
400  template &lt;typename Dtype&gt;
401  void MKLBatchNormLayer&lt;Dtype&gt;::Forward_cpu(
402      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
403    for (int i = 0; i &lt; num_stats_batches_; i++) {
404      ForwardStatsBatch_cpu(bottom, top, i);
405    }
406  }
407  template &lt;typename Dtype&gt;
408  void MKLBatchNormLayer&lt;Dtype&gt;::Backward_cpu(
409      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
410      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
411    for (int i = 0; i &lt; num_stats_batches_; i++) {
412      BackwardStatsBatch_cpu(top, propagate_down, bottom, i);
413    }
414  }
415  #ifdef CPU_ONLY
416  STUB_GPU(MKLBatchNormLayer);
417  #else
418  template &lt;typename Dtype&gt;
419  void MKLBatchNormLayer&lt;Dtype&gt;::Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
420      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {NOT_IMPLEMENTED;}
421  template &lt;typename Dtype&gt;
422  void MKLBatchNormLayer&lt;Dtype&gt;::Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
423      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)
424    {NOT_IMPLEMENTED;}
425  #endif
426  INSTANTIATE_CLASS(MKLBatchNormLayer);
427  }  
428  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_lrn_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include &lt;vector&gt;
3  #include &quot;caffe/layer.hpp&quot;
4  #include &quot;caffe/layers/mkl_layers.hpp&quot;
5  #include &quot;caffe/util/math_functions.hpp&quot;
6  #include &quot;caffe/util/performance.hpp&quot;
7  namespace caffe {
8  template &lt;typename Dtype&gt;
9  MKLLRNLayer&lt;Dtype&gt;::~MKLLRNLayer() {
10    dnnDelete&lt;Dtype&gt;(lrnFwd);
11    dnnDelete&lt;Dtype&gt;(lrnBwd);
12    dnnReleaseBuffer&lt;Dtype&gt;(lrn_buffer_);
13  }
14  template &lt;typename Dtype&gt;
15  void MKLLRNLayer&lt;Dtype&gt;::Init(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
16        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
17    size_ = this-&gt;layer_param_.lrn_param().local_size();
18    CHECK_EQ(size_ % 2, 1) &lt;&lt; &quot;LRN only supports odd values for local_size&quot;;
19    alpha_ = this-&gt;layer_param_.lrn_param().alpha();
20    beta_ = this-&gt;layer_param_.lrn_param().beta();
21    k_ = this-&gt;layer_param_.lrn_param().k();
22    size_t dim = 4, sizes[4], strides[4];
23    channels_ = bottom[0]-&gt;channels();
24    height_   = bottom[0]-&gt;height();
25    width_    = bottom[0]-&gt;width();
26    num_      = bottom[0]-&gt;num();
27    sizes[0] = width_;
28    sizes[1] = height_;
29    sizes[2] = channels_;
30    sizes[3] = num_;
31    strides[0] = 1;
32    strides[1] = sizes[0];
33    strides[2] = sizes[0]*sizes[1];
34    strides[3] = sizes[0]*sizes[1]*sizes[2];
35    fwd_bottom_data-&gt;name = &quot;fwd_bottom_data   @ &quot; + this-&gt;layer_param_.name();
36    fwd_top_data-&gt;name =    &quot;fwd_top_data      @ &quot; + this-&gt;layer_param_.name();
37    bwd_top_diff-&gt;name =    &quot;bwd_top_diff      @ &quot; + this-&gt;layer_param_.name();
38    bwd_bottom_diff-&gt;name = &quot;bwd_bottom_diff   @ &quot; + this-&gt;layer_param_.name();
39    fwd_bottom_data-&gt;create_user_layout(dim, sizes, strides, false);
40    fwd_top_data   -&gt;create_user_layout(dim, sizes, strides, false);
41    bwd_bottom_diff-&gt;create_user_layout(dim, sizes, strides, false);
42    bwd_top_diff   -&gt;create_user_layout(dim, sizes, strides, false);
43    dnnDelete&lt;Dtype&gt;(lrnFwd);
44    dnnDelete&lt;Dtype&gt;(lrnBwd);
45    dnnReleaseBuffer&lt;Dtype&gt;(lrn_buffer_);
46    lrn_buffer_ = NULL;
47  }
48  template &lt;typename Dtype&gt;
49  void MKLLRNLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
50        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
51    Init(bottom, top);
52  }
53  template &lt;typename Dtype&gt;
54  void MKLLRNLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
55        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
56    CHECK_EQ(4, bottom[0]-&gt;num_axes()) &lt;&lt; &quot;Input must have 4 axes, &quot;
57        &lt;&lt; &quot;corresponding to (num, channels, height, width)&quot;;
58    bool reshaping = true;
59    if ((num_ == bottom[0]-&gt;num()) &amp;&amp;
60        channels_ == bottom[0]-&gt;channels() &amp;&amp;
61        height_ == bottom[0]-&gt;height() &amp;&amp;
62        width_ == bottom[0]-&gt;width()) {
63      reshaping = false;
64    }
65    channels_ = bottom[0]-&gt;channels();
66    height_ = bottom[0]-&gt;height();
67    width_ = bottom[0]-&gt;width();
68    num_ = bottom[0]-&gt;num();
69    switch (this-&gt;layer_param_.lrn_param().norm_region()) {
70    case LRNParameter_NormRegion_ACROSS_CHANNELS:
71      top[0]-&gt;Reshape(num_, channels_, height_, width_);
72      break;
73    case LRNParameter_NormRegion_WITHIN_CHANNEL:
74      NOT_IMPLEMENTED;
75      break;
76    default:
77      LOG(FATAL) &lt;&lt; &quot;Unknown normalization region.&quot;;
78    }
79    if (reshaping == true) {
80      Init(bottom, top);
81    }
82  }
83  template &lt;typename Dtype&gt;
84  void MKLLRNLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
85      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
86    switch (this-&gt;layer_param_.lrn_param().norm_region()) {
87    case LRNParameter_NormRegion_ACROSS_CHANNELS:
88      CrossChannelForward_cpu(bottom, top);
89      break;
90    case LRNParameter_NormRegion_WITHIN_CHANNEL:
91      NOT_IMPLEMENTED;
92      break;
93    default:
94      LOG(FATAL) &lt;&lt; &quot;Unknown normalization region.&quot;;
95    }
96  }
97  template &lt;typename Dtype&gt;
98  void MKLLRNLayer&lt;Dtype&gt;::CrossChannelForward_cpu(
99      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
100    const void* bottom_data =
101      reinterpret_cast&lt;const void*&gt;(bottom[0]-&gt;prv_data());
102    if (NULL != bottom_data) {
103      if (lrnFwd == NULL) {
104        CHECK_EQ((bottom[0]-&gt;get_prv_data_descriptor())-&gt;get_descr_type(),
105                PrvMemDescr::PRV_DESCR_MKL2017);
106        shared_ptr&lt;MKLData&lt;Dtype&gt; &gt; mem_descr
107          =  boost::static_pointer_cast&lt;MKLData&lt;Dtype&gt; &gt;
108                (bottom[0]-&gt;get_prv_data_descriptor());
109        CHECK(mem_descr != NULL);
110        fwd_bottom_data = mem_descr;
111        dnnError_t e;
112        dnnLayout_t lrn_buffer_l = NULL;
113        e = dnnLRNCreateForward&lt;Dtype&gt;(&amp;lrnFwd, NULL, fwd_bottom_data-&gt;layout_int,
114                size_, alpha_, beta_, k_);
115        CHECK_EQ(e, E_SUCCESS);
116        fwd_top_data-&gt;create_internal_layout(lrnFwd, dnnResourceDst);
117        e = dnnLRNCreateBackward&lt;Dtype&gt;(&amp;lrnBwd, NULL,
118                fwd_bottom_data-&gt;layout_int, fwd_bottom_data-&gt;layout_int,
119                size_, alpha_, beta_, k_);
120        CHECK_EQ(e, E_SUCCESS);
121        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
122                &amp;lrn_buffer_l, lrnFwd, dnnResourceWorkspace);
123        CHECK_EQ(e, E_SUCCESS);
124        e = dnnAllocateBuffer&lt;Dtype&gt;(
125                reinterpret_cast&lt;void **&gt;(&amp;lrn_buffer_), lrn_buffer_l);
126        CHECK_EQ(e, E_SUCCESS);
127        dnnLayoutDelete&lt;Dtype&gt;(lrn_buffer_l);
128        bwd_top_diff-&gt;create_internal_layout(lrnBwd, dnnResourceDiffDst);
129        bwd_bottom_diff-&gt;create_internal_layout(lrnBwd, dnnResourceDiffSrc);
130      }
131    } else {
132      DLOG(INFO) &lt;&lt; &quot;Using cpu_data in MKLLRNLayer.&quot;;
133      if (lrnFwd == NULL) {
134        dnnError_t e;
135        dnnLayout_t lrn_buffer_l = NULL;
136        e = dnnLRNCreateForward&lt;Dtype&gt;(&amp;lrnFwd, NULL, fwd_bottom_data-&gt;layout_usr,
137                size_, alpha_, beta_, k_);
138        CHECK_EQ(e, E_SUCCESS);
<span onclick='openModal()' class='match'>139        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
140                &amp;lrn_buffer_l, lrnFwd, dnnResourceWorkspace);
141        CHECK_EQ(e, E_SUCCESS);
</span>142        e = dnnAllocateBuffer&lt;Dtype&gt;(
143                reinterpret_cast&lt;void **&gt;(&amp;lrn_buffer_), lrn_buffer_l);
144        CHECK_EQ(e, E_SUCCESS);
145        dnnLayoutDelete&lt;Dtype&gt;(lrn_buffer_l);
146        e = dnnLRNCreateBackward&lt;Dtype&gt;(&amp;lrnBwd, NULL,
147                fwd_bottom_data-&gt;layout_usr, fwd_bottom_data-&gt;layout_usr,
148                size_, alpha_, beta_, k_);
149        CHECK_EQ(e, E_SUCCESS);
150      }
151      bottom_data = reinterpret_cast&lt;const void*&gt;(bottom[0]-&gt;cpu_data());
152    }
153    dnnError_t e;
154    void* lrn_res[dnnResourceNumber];
155    lrn_res[dnnResourceSrc] = const_cast&lt;void*&gt;(bottom_data);
156    if (fwd_top_data-&gt;conversion_needed()) {
157      top[0]-&gt;set_prv_data_descriptor(fwd_top_data);
158      lrn_res[dnnResourceDst] =
159              reinterpret_cast&lt;void *&gt;(top[0]-&gt;mutable_prv_data());
160    } else {
161      lrn_res[dnnResourceDst] =
162              reinterpret_cast&lt;void *&gt;(top[0]-&gt;mutable_cpu_data());
163      DLOG(INFO) &lt;&lt; &quot;Using cpu_data for top in DnnLRN.&quot;;
164    }
165    lrn_res[dnnResourceWorkspace] = lrn_buffer_;
166    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME(&quot;FW&quot;));
167    PERFORMANCE_MEASUREMENT_BEGIN();
168    e = dnnExecute&lt;Dtype&gt;(lrnFwd, lrn_res);
169    PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
170    CHECK_EQ(e, E_SUCCESS);
171  }
172  template &lt;typename Dtype&gt;
173  void MKLLRNLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
174      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
175    switch (this-&gt;layer_param_.lrn_param().norm_region()) {
176    case LRNParameter_NormRegion_ACROSS_CHANNELS:
177      CrossChannelBackward_cpu(top, propagate_down, bottom);
178      break;
179    case LRNParameter_NormRegion_WITHIN_CHANNEL:
180      NOT_IMPLEMENTED;
181      break;
182    default:
183      LOG(FATAL) &lt;&lt; &quot;Unknown normalization region.&quot;;
184    }
185  }
186  template &lt;typename Dtype&gt;
187  void MKLLRNLayer&lt;Dtype&gt;::CrossChannelBackward_cpu(
188      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
189      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
190    dnnError_t e;
191    void* lrn_res[dnnResourceNumber];
192    lrn_res[dnnResourceDiffDst] =
193            bwd_top_diff-&gt;get_converted_prv(top[0], true);
194    lrn_res[dnnResourceWorkspace] = lrn_buffer_;
195    lrn_res[dnnResourceSrc] =
196            fwd_bottom_data-&gt;get_converted_prv(bottom[0], false);
197    if (bwd_bottom_diff-&gt;conversion_needed()) {
198      bottom[0]-&gt;set_prv_diff_descriptor(bwd_bottom_diff);
199      lrn_res[dnnResourceDiffSrc] = bottom[0]-&gt;mutable_prv_diff();
200    } else {
201      lrn_res[dnnResourceDiffSrc] = bottom[0]-&gt;mutable_cpu_diff();
202    }
203    PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKL_NAME(&quot;BW&quot;));
204    PERFORMANCE_MEASUREMENT_BEGIN();
205    e = dnnExecute&lt;Dtype&gt;(lrnBwd, lrn_res);
206    PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
207    CHECK_EQ(e, E_SUCCESS);
208  }
209  #ifdef CPU_ONLY
210  STUB_GPU(MKLLRNLayer);
211  STUB_GPU_FORWARD(MKLLRNLayer, CrossChannelForward);
212  STUB_GPU_BACKWARD(MKLLRNLayer, CrossChannelBackward);
213  #else
214  template &lt;typename Dtype&gt;
215  void MKLLRNLayer&lt;Dtype&gt;::Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
216      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {NOT_IMPLEMENTED;}
217  template &lt;typename Dtype&gt;
218  void MKLLRNLayer&lt;Dtype&gt;::Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
219      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom)
220    {NOT_IMPLEMENTED;}
221  template &lt;typename Dtype&gt;
222  void MKLLRNLayer&lt;Dtype&gt;::CrossChannelForward_gpu(
223      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top)
224    {NOT_IMPLEMENTED;}
225  template &lt;typename Dtype&gt;
226  void MKLLRNLayer&lt;Dtype&gt;::CrossChannelBackward_gpu(
227      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
228      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {NOT_IMPLEMENTED;}
229  #endif
230  INSTANTIATE_CLASS(MKLLRNLayer);
231  }  
232  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_batch_norm_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_lrn_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>252        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
253          &amp;mean_buffer_l, batchNormFwd, dnnResourceMean);
254        CHECK_EQ(e, E_SUCCESS);
</pre></code></div>
                <div class="column column_space"><pre><code>139        e = dnnLayoutCreateFromPrimitive&lt;Dtype&gt;(
140                &amp;lrn_buffer_l, lrnFwd, dnnResourceWorkspace);
141        CHECK_EQ(e, E_SUCCESS);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    