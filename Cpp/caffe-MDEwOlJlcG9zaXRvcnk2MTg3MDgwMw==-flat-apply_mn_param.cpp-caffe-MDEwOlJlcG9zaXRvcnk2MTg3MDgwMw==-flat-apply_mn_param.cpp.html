
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 33, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-apply_mn_param.cpp</h3>
            <pre><code>1  #ifdef USE_MLSL
2  #include <string>
3  #include <map>
4  #include <set>
5  #include "caffe/common.hpp"
6  #include "caffe/blob.hpp"
7  #include "caffe/util/math_functions.hpp"
8  #include "caffe/multinode/mlsl.hpp"
9  #include "caffe/multinode/apply_mn_param.hpp"
10  namespace caffe {
11  template <typename Dtype>
12  void ApplyMultinodeParams(const NetParameter& param,
13      NetParameter* param_with_mn) {
14    map<string, MnModelParallelParameter> net_layer_params;
15    map<string, MnActivationParameter> blob_param_map;
16    MultinodeParameter mn_param = param.multinode();
17    const MnParamGradCompressLayerTypeList &compress_layer_list = mn_param.compress_layer_type_list();
18    for (int param_id = 0; param_id < mn_param.model_parallel_size(); param_id++) {
19      MnModelParallelParameter model_parallel_param = mn_param.model_parallel(param_id);
20      string layer_from = model_parallel_param.layer_from();
21      string layer_to = model_parallel_param.layer_to();
22      set<string> marked_blobs;
23      for (int i = 0; i < param.layer_size(); i++) {
24        const LayerParameter& layer_param = param.layer(i);
25        bool layer_covered_by_global = false;
26        if (layer_param.name() == layer_from ||
27          layer_param.name() == layer_to) {
28          layer_covered_by_global = true;
29        } else {
30          for (int j = 0; j < layer_param.bottom_size(); j++) {
31            if (marked_blobs.find(layer_param.bottom(j)) !=
32              marked_blobs.end()) {
33              layer_covered_by_global = true;
34              break;
35            }
36          }
37        }
38        if (layer_covered_by_global) {
39          for (int j = 0; j < layer_param.top_size(); j++) {
40            marked_blobs.insert(layer_param.top(j));
41          }
42          net_layer_params[layer_param.name()] = model_parallel_param;
43          if (layer_param.type() == "LRN" &&
44              layer_param.lrn_param().norm_region() ==
45              LRNParameter_NormRegion_ACROSS_CHANNELS) {
46            net_layer_params[layer_param.name()].set_model_parts(1);
47          }
48        }
49        if (layer_param.name() == layer_to ||
50            layer_param.top_size() == 0) {
51          break;
52        }
53      }
54    }
55    map<string, MnActivationParameter> blob_mdg_map;
56    for (int i = 0; i < param.layer_size(); i++) {
57      const LayerParameter& layer_param = param.layer(i);
58      string layer_name = layer_param.name();
59      string layer_type = layer_param.type();
60      const MultinodeLayerParameter& mn_layer_param = layer_param.multinode();
61      int num_nodes = mn_layer_param.num_nodes();
62      int model_parts = mn_layer_param.model_parts();
63      if (net_layer_params.find(layer_name) != net_layer_params.end()) {
64        MnModelParallelParameter model_parallel_param =
65          net_layer_params[layer_name];
66        num_nodes = model_parallel_param.num_nodes();
67        model_parts = model_parallel_param.model_parts();
68      }
69      for (int j = 0; j < layer_param.bottom_size(); j++) {
70        string bottom_name = layer_param.bottom(j);
71        if (blob_mdg_map.find(bottom_name) != blob_mdg_map.end()) {
72          MnActivationParameter mdg = blob_mdg_map[bottom_name];
73          mdg.set_num_nodes_out(num_nodes);
74          mdg.set_model_parts_out(model_parts);
75          int num_nodes_in = mdg.num_nodes_in();
76          int num_nodes_out = mdg.num_nodes_out();
77          int model_parts_in = mdg.model_parts_in();
78          int model_parts_out = mdg.model_parts_out();
79          mn::GetCanonicalMnParam(num_nodes_in, model_parts_in);
80          mn::GetCanonicalMnParam(num_nodes_out, model_parts_out);
81          if ((model_parts_out > 1 &&
82               (layer_type == "Convolution" || layer_type == "InnerProduct" ||
83                layer_type == "Accuracy" || layer_type == "SoftmaxWithLoss")) ||
84              num_nodes_in != num_nodes_out ||
85              model_parts_in != model_parts_out) {
86            string layer_blob_name = layer_name + "/" + layer_param.bottom(j);
87            if (layer_type == "Accuracy" || layer_type == "SoftmaxWithLoss") {
88              mdg.set_need_reduce(false);
89            }
90            blob_param_map[layer_blob_name] = mdg;
91          }
92          blob_mdg_map.erase(bottom_name);
93        }
94      }
95      for (int j = 0;  j < layer_param.top_size(); j++) {
96        MnActivationParameter mdg;
97        mdg.set_num_nodes_in(num_nodes);
98        mdg.set_model_parts_in(model_parts);
99        blob_mdg_map[layer_param.top(j)] = mdg;
100      }
101    }
102    param_with_mn->CopyFrom(param);
103    param_with_mn->clear_layer();
104    if (mn::is_param_server()) {
105      blob_param_map.clear();
106    }
107    for (int i = 0; i < param.layer_size(); i++) {
108      const LayerParameter& orig_layer_param = param.layer(i);
109      map<int, string> updated_blob_idx_to_name;
110      for (int j = 0; j < orig_layer_param.bottom_size(); j++) {
111        const string& bottom_blob_name = orig_layer_param.bottom(j);
112        string layer_blob_name = orig_layer_param.name() + "/" + bottom_blob_name;
113        if (blob_param_map.find(layer_blob_name) != blob_param_map.end()) {
114          LayerParameter* mn_activation_layer_param =
115            param_with_mn->add_layer();
116          string new_name = "mn_activation/" + layer_blob_name;
117          mn_activation_layer_param->Clear();
118          mn_activation_layer_param->set_name(new_name);
119          mn_activation_layer_param->set_type("MnActivation");
120          mn_activation_layer_param->add_bottom(bottom_blob_name);
121          mn_activation_layer_param->add_top(new_name);
122          MnActivationParameter *mn_activation_param =
123            mn_activation_layer_param->mutable_mn_activation_param();
124          *mn_activation_param = blob_param_map[layer_blob_name];
125          updated_blob_idx_to_name[j] = new_name;
126        }
127      }
128      LayerParameter* layer_param = param_with_mn->add_layer();
129      layer_param->CopyFrom(orig_layer_param);
130      if (net_layer_params.find(layer_param->name()) != net_layer_params.end()) {
131        MultinodeLayerParameter *mn_layer_param = layer_param->mutable_multinode();
132        const MnModelParallelParameter &mn_param = net_layer_params[layer_param->name()];
133        mn_layer_param->set_num_nodes(mn_param.num_nodes());
134        mn_layer_param->set_model_parts(mn_param.model_parts());
135      }
136      const MultinodeLayerParameter &mn_layer_param = layer_param->multinode();
137      int num_nodes = mn_layer_param.num_nodes();
138      int model_parts = mn_layer_param.model_parts();
139      mn::GetCanonicalMnParam(num_nodes, model_parts);
140      if (model_parts > 1 && !mn::is_param_server()) {
141        if (layer_param->type() == "Convolution") {
142          ConvolutionParameter *conv_param = layer_param->mutable_convolution_param();
143          int new_num_output = conv_param->num_output() / model_parts;
<span onclick='openModal()' class='match'>144          CHECK_EQ(conv_param->num_output(), model_parts * new_num_output)
145            << "Convolution layer " << layer_param->name()
146            << ": Undividible num_output " << conv_param->num_output()
147            << " by model_parts " << model_parts;
148          conv_param->set_num_output(new_num_output);
</span>149        } else if (layer_param->type() == "InnerProduct") {
150          InnerProductParameter *ip_param = layer_param->mutable_inner_product_param();
151          int new_num_output = ip_param->num_output() / model_parts;
152          CHECK_EQ(ip_param->num_output(), model_parts * new_num_output)
153            << "InnerProduct layer " << layer_param->name()
154            << ": Undividible num_output " << ip_param->num_output()
155            << " by model_parts " << model_parts;
156          ip_param->set_num_output(ip_param->num_output() / model_parts);
157          CHECK(!ip_param->transpose()) << "Model parallelism does not support transpose!";
158        }
159        for (int j = 0; j < layer_param->blobs_size(); j++) {
160          Blob<Dtype> blob;
161          Blob<Dtype> new_blob;
162          const BlobProto &proto = layer_param->blobs(j);
163          blob.FromProto(proto);
164          vector<int> shape = blob.shape();
165          new_blob.Reshape(shape);
166          if (shape.size() > 0) {
167            if (proto.has_num() || proto.has_channels() ||
168                proto.has_height() || proto.has_width()) {
169              if (layer_param->type() == "InnerProduct") {
170                CHECK_EQ(shape.size(), 4);
171                CHECK_EQ(shape[0], 1);
172                CHECK_EQ(shape[1], 1);
173                if (shape[2] == 1) {
174                  shape.resize(1);
175                  shape[0] = blob.shape(3);
176                } else {
177                  shape.resize(2);
178                  shape[0] = blob.shape(2);
179                  shape[1] = blob.shape(3);
180                }
181                new_blob.Reshape(shape);
182              }
183            }
184            int count = blob.count() / model_parts;
185            int offset = count * (mn::get_node_id() % model_parts);
186            shape[0] /= model_parts;
187            new_blob.Reshape(shape);
188            caffe_copy(count, blob.cpu_data() + offset, new_blob.mutable_cpu_data());
189            caffe_copy(count, blob.cpu_diff() + offset, new_blob.mutable_cpu_diff());
190            BlobProto *updated_blob_proto = layer_param->mutable_blobs(j);
191            updated_blob_proto->Clear();
192            new_blob.ToProto(updated_blob_proto, true);
193          }
194        }
195      }
196      for (int j = 0; j < orig_layer_param.bottom_size(); j++) {
197        if (updated_blob_idx_to_name.find(j) != updated_blob_idx_to_name.end()) {
198          layer_param->set_bottom(j, updated_blob_idx_to_name[j]);
199        }
200      }
201    }
202    for (int i = 0; i < param_with_mn->layer_size(); i++) {
203      LayerParameter *layer_param = param_with_mn->mutable_layer(i);
204      string layer_type = layer_param->type();
205      for (int j = 0; j < compress_layer_list.layer_type_size(); j++) {
206        if (compress_layer_list.layer_type(j) == layer_type) {
207          MnParamGradCompressParameter *grad_comp_param = layer_param->mutable_mn_grad_compress_param();
208          if (grad_comp_param->param_grad_compress_enable_size() == 0) {
209            grad_comp_param->add_param_grad_compress_enable(true);
210          }
211          break;
212        }
213      }
214    }
215  }
216  template <typename Dtype>
217  void CopyMultinodeParamsFromNet(const Net<Dtype> *net, NetParameter *param) {
218    for (int i = 0; i < param->layer_size(); i++) {
219      LayerParameter* source_layer = param->mutable_layer(i);
220      const string& source_layer_name = source_layer->name();
221      int target_layer_id = 0;
222      while (target_layer_id != net->layer_names().size() &&
223             net->layer_names()[target_layer_id] != source_layer_name) {
224        ++target_layer_id;
225      }
226      if (target_layer_id == net->layer_names().size()) continue;
227      *source_layer->mutable_multinode() =
228        net->layers()[target_layer_id]->layer_param().multinode();
229    }
230  }
231  template <typename Dtype>
232  void RevertMultinodeParams(NetParameter* param, bool write_diff) {
233    NetParameter orig_param;
234    orig_param.CopyFrom(*param);
235    param->clear_layer();
236    for (int i = 0; i < orig_param.layer_size(); i++) {
237      const LayerParameter& orig_layer_param = orig_param.layer(i);
238      if (orig_layer_param.type() == "MnActivation") continue;
239      LayerParameter* layer_param = param->add_layer();
240      layer_param->CopyFrom(orig_layer_param);
241      layer_param->clear_bottom();
242      for (int j = 0; j < orig_layer_param.bottom_size(); j++) {
243        string bottom_name = orig_layer_param.bottom(j);
244        string prefix = "mn_activation/" + orig_layer_param.name() + "/";
245        if (bottom_name.find(prefix) == 0) {
246          bottom_name = bottom_name.substr(prefix.size());
247        }
248        layer_param->add_bottom(bottom_name);
249      }
250      const MultinodeLayerParameter &mn_layer_param = orig_layer_param.multinode();
251      int num_nodes = mn_layer_param.num_nodes();
252      int model_parts = mn_layer_param.model_parts();
253      mn::GetCanonicalMnParam(num_nodes, model_parts);
254      if (model_parts > 1) {
255        if (layer_param->type() == "Convolution") {
256          ConvolutionParameter *conv_param = layer_param->mutable_convolution_param();
257          conv_param->set_num_output(conv_param->num_output() * model_parts);
258        } else if (layer_param->type() == "InnerProduct") {
259          InnerProductParameter *ip_param = layer_param->mutable_inner_product_param();
260          ip_param->set_num_output(ip_param->num_output() * model_parts);
261          CHECK(!ip_param->transpose()) << "Model parallelism does not support transpose!";
262        }
263        layer_param->clear_blobs();
264        for (int j = 0; j < orig_layer_param.blobs_size(); j++) {
265          BlobProto *blob_proto = layer_param->add_blobs();
266          Blob<Dtype> orig_blob;
267          orig_blob.FromProto(orig_layer_param.blobs(j));
268          vector<int> shape = orig_blob.shape();
269          Blob<Dtype> new_blob;
270          if (shape.size() > 0) {
271            mn::Distribution *distrib = mn::get_distrib(num_nodes/model_parts, model_parts);
272            int count = orig_blob.count();
273            shape[0] *= model_parts;
274            new_blob.Reshape(shape);
275            distrib->allgather<Dtype,MLSL::GT_MODEL>(
276              orig_blob.mutable_cpu_data(), count, new_blob.mutable_cpu_data());
277            if (write_diff) {
278              distrib->allgather<Dtype,MLSL::GT_MODEL>(
279                orig_blob.mutable_cpu_diff(), count, new_blob.mutable_cpu_diff());
280            }
281          }
282          new_blob.ToProto(blob_proto, write_diff);
283        }
284      }
285      layer_param->mutable_multinode()->Clear();
286    }
287  }
288  template void ApplyMultinodeParams<float>(const NetParameter& param,
289      NetParameter* param_with_mn);
290  template void ApplyMultinodeParams<double>(const NetParameter& param,
291      NetParameter* param_with_mn);
292  template void CopyMultinodeParamsFromNet<float>(const Net<float> *net, NetParameter *param);
293  template void CopyMultinodeParamsFromNet<double>(const Net<double> *net, NetParameter *param);
294  template void RevertMultinodeParams<float>(NetParameter* param, bool write_diff);
295  template void RevertMultinodeParams<double>(NetParameter* param, bool write_diff);
296  } 
297  #endif 
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-apply_mn_param.cpp</h3>
            <pre><code>1  #ifdef USE_MLSL
2  #include <string>
3  #include <map>
4  #include <set>
5  #include "caffe/common.hpp"
6  #include "caffe/blob.hpp"
7  #include "caffe/util/math_functions.hpp"
8  #include "caffe/multinode/mlsl.hpp"
9  #include "caffe/multinode/apply_mn_param.hpp"
10  namespace caffe {
11  template <typename Dtype>
12  void ApplyMultinodeParams(const NetParameter& param,
13      NetParameter* param_with_mn) {
14    map<string, MnModelParallelParameter> net_layer_params;
15    map<string, MnActivationParameter> blob_param_map;
16    MultinodeParameter mn_param = param.multinode();
17    const MnParamGradCompressLayerTypeList &compress_layer_list = mn_param.compress_layer_type_list();
18    for (int param_id = 0; param_id < mn_param.model_parallel_size(); param_id++) {
19      MnModelParallelParameter model_parallel_param = mn_param.model_parallel(param_id);
20      string layer_from = model_parallel_param.layer_from();
21      string layer_to = model_parallel_param.layer_to();
22      set<string> marked_blobs;
23      for (int i = 0; i < param.layer_size(); i++) {
24        const LayerParameter& layer_param = param.layer(i);
25        bool layer_covered_by_global = false;
26        if (layer_param.name() == layer_from ||
27          layer_param.name() == layer_to) {
28          layer_covered_by_global = true;
29        } else {
30          for (int j = 0; j < layer_param.bottom_size(); j++) {
31            if (marked_blobs.find(layer_param.bottom(j)) !=
32              marked_blobs.end()) {
33              layer_covered_by_global = true;
34              break;
35            }
36          }
37        }
38        if (layer_covered_by_global) {
39          for (int j = 0; j < layer_param.top_size(); j++) {
40            marked_blobs.insert(layer_param.top(j));
41          }
42          net_layer_params[layer_param.name()] = model_parallel_param;
43          if (layer_param.type() == "LRN" &&
44              layer_param.lrn_param().norm_region() ==
45              LRNParameter_NormRegion_ACROSS_CHANNELS) {
46            net_layer_params[layer_param.name()].set_model_parts(1);
47          }
48        }
49        if (layer_param.name() == layer_to ||
50            layer_param.top_size() == 0) {
51          break;
52        }
53      }
54    }
55    map<string, MnActivationParameter> blob_mdg_map;
56    for (int i = 0; i < param.layer_size(); i++) {
57      const LayerParameter& layer_param = param.layer(i);
58      string layer_name = layer_param.name();
59      string layer_type = layer_param.type();
60      const MultinodeLayerParameter& mn_layer_param = layer_param.multinode();
61      int num_nodes = mn_layer_param.num_nodes();
62      int model_parts = mn_layer_param.model_parts();
63      if (net_layer_params.find(layer_name) != net_layer_params.end()) {
64        MnModelParallelParameter model_parallel_param =
65          net_layer_params[layer_name];
66        num_nodes = model_parallel_param.num_nodes();
67        model_parts = model_parallel_param.model_parts();
68      }
69      for (int j = 0; j < layer_param.bottom_size(); j++) {
70        string bottom_name = layer_param.bottom(j);
71        if (blob_mdg_map.find(bottom_name) != blob_mdg_map.end()) {
72          MnActivationParameter mdg = blob_mdg_map[bottom_name];
73          mdg.set_num_nodes_out(num_nodes);
74          mdg.set_model_parts_out(model_parts);
75          int num_nodes_in = mdg.num_nodes_in();
76          int num_nodes_out = mdg.num_nodes_out();
77          int model_parts_in = mdg.model_parts_in();
78          int model_parts_out = mdg.model_parts_out();
79          mn::GetCanonicalMnParam(num_nodes_in, model_parts_in);
80          mn::GetCanonicalMnParam(num_nodes_out, model_parts_out);
81          if ((model_parts_out > 1 &&
82               (layer_type == "Convolution" || layer_type == "InnerProduct" ||
83                layer_type == "Accuracy" || layer_type == "SoftmaxWithLoss")) ||
84              num_nodes_in != num_nodes_out ||
85              model_parts_in != model_parts_out) {
86            string layer_blob_name = layer_name + "/" + layer_param.bottom(j);
87            if (layer_type == "Accuracy" || layer_type == "SoftmaxWithLoss") {
88              mdg.set_need_reduce(false);
89            }
90            blob_param_map[layer_blob_name] = mdg;
91          }
92          blob_mdg_map.erase(bottom_name);
93        }
94      }
95      for (int j = 0;  j < layer_param.top_size(); j++) {
96        MnActivationParameter mdg;
97        mdg.set_num_nodes_in(num_nodes);
98        mdg.set_model_parts_in(model_parts);
99        blob_mdg_map[layer_param.top(j)] = mdg;
100      }
101    }
102    param_with_mn->CopyFrom(param);
103    param_with_mn->clear_layer();
104    if (mn::is_param_server()) {
105      blob_param_map.clear();
106    }
107    for (int i = 0; i < param.layer_size(); i++) {
108      const LayerParameter& orig_layer_param = param.layer(i);
109      map<int, string> updated_blob_idx_to_name;
110      for (int j = 0; j < orig_layer_param.bottom_size(); j++) {
111        const string& bottom_blob_name = orig_layer_param.bottom(j);
112        string layer_blob_name = orig_layer_param.name() + "/" + bottom_blob_name;
113        if (blob_param_map.find(layer_blob_name) != blob_param_map.end()) {
114          LayerParameter* mn_activation_layer_param =
115            param_with_mn->add_layer();
116          string new_name = "mn_activation/" + layer_blob_name;
117          mn_activation_layer_param->Clear();
118          mn_activation_layer_param->set_name(new_name);
119          mn_activation_layer_param->set_type("MnActivation");
120          mn_activation_layer_param->add_bottom(bottom_blob_name);
121          mn_activation_layer_param->add_top(new_name);
122          MnActivationParameter *mn_activation_param =
123            mn_activation_layer_param->mutable_mn_activation_param();
124          *mn_activation_param = blob_param_map[layer_blob_name];
125          updated_blob_idx_to_name[j] = new_name;
126        }
127      }
128      LayerParameter* layer_param = param_with_mn->add_layer();
129      layer_param->CopyFrom(orig_layer_param);
130      if (net_layer_params.find(layer_param->name()) != net_layer_params.end()) {
131        MultinodeLayerParameter *mn_layer_param = layer_param->mutable_multinode();
132        const MnModelParallelParameter &mn_param = net_layer_params[layer_param->name()];
133        mn_layer_param->set_num_nodes(mn_param.num_nodes());
134        mn_layer_param->set_model_parts(mn_param.model_parts());
135      }
136      const MultinodeLayerParameter &mn_layer_param = layer_param->multinode();
137      int num_nodes = mn_layer_param.num_nodes();
138      int model_parts = mn_layer_param.model_parts();
139      mn::GetCanonicalMnParam(num_nodes, model_parts);
140      if (model_parts > 1 && !mn::is_param_server()) {
141        if (layer_param->type() == "Convolution") {
142          ConvolutionParameter *conv_param = layer_param->mutable_convolution_param();
143          int new_num_output = conv_param->num_output() / model_parts;
<span onclick='openModal()' class='match'>144          CHECK_EQ(conv_param->num_output(), model_parts * new_num_output)
145            << "Convolution layer " << layer_param->name()
146            << ": Undividible num_output " << conv_param->num_output()
147            << " by model_parts " << model_parts;
148          conv_param->set_num_output(new_num_output);
</span>149        } else if (layer_param->type() == "InnerProduct") {
150          InnerProductParameter *ip_param = layer_param->mutable_inner_product_param();
151          int new_num_output = ip_param->num_output() / model_parts;
152          CHECK_EQ(ip_param->num_output(), model_parts * new_num_output)
153            << "InnerProduct layer " << layer_param->name()
154            << ": Undividible num_output " << ip_param->num_output()
155            << " by model_parts " << model_parts;
156          ip_param->set_num_output(ip_param->num_output() / model_parts);
157          CHECK(!ip_param->transpose()) << "Model parallelism does not support transpose!";
158        }
159        for (int j = 0; j < layer_param->blobs_size(); j++) {
160          Blob<Dtype> blob;
161          Blob<Dtype> new_blob;
162          const BlobProto &proto = layer_param->blobs(j);
163          blob.FromProto(proto);
164          vector<int> shape = blob.shape();
165          new_blob.Reshape(shape);
166          if (shape.size() > 0) {
167            if (proto.has_num() || proto.has_channels() ||
168                proto.has_height() || proto.has_width()) {
169              if (layer_param->type() == "InnerProduct") {
170                CHECK_EQ(shape.size(), 4);
171                CHECK_EQ(shape[0], 1);
172                CHECK_EQ(shape[1], 1);
173                if (shape[2] == 1) {
174                  shape.resize(1);
175                  shape[0] = blob.shape(3);
176                } else {
177                  shape.resize(2);
178                  shape[0] = blob.shape(2);
179                  shape[1] = blob.shape(3);
180                }
181                new_blob.Reshape(shape);
182              }
183            }
184            int count = blob.count() / model_parts;
185            int offset = count * (mn::get_node_id() % model_parts);
186            shape[0] /= model_parts;
187            new_blob.Reshape(shape);
188            caffe_copy(count, blob.cpu_data() + offset, new_blob.mutable_cpu_data());
189            caffe_copy(count, blob.cpu_diff() + offset, new_blob.mutable_cpu_diff());
190            BlobProto *updated_blob_proto = layer_param->mutable_blobs(j);
191            updated_blob_proto->Clear();
192            new_blob.ToProto(updated_blob_proto, true);
193          }
194        }
195      }
196      for (int j = 0; j < orig_layer_param.bottom_size(); j++) {
197        if (updated_blob_idx_to_name.find(j) != updated_blob_idx_to_name.end()) {
198          layer_param->set_bottom(j, updated_blob_idx_to_name[j]);
199        }
200      }
201    }
202    for (int i = 0; i < param_with_mn->layer_size(); i++) {
203      LayerParameter *layer_param = param_with_mn->mutable_layer(i);
204      string layer_type = layer_param->type();
205      for (int j = 0; j < compress_layer_list.layer_type_size(); j++) {
206        if (compress_layer_list.layer_type(j) == layer_type) {
207          MnParamGradCompressParameter *grad_comp_param = layer_param->mutable_mn_grad_compress_param();
208          if (grad_comp_param->param_grad_compress_enable_size() == 0) {
209            grad_comp_param->add_param_grad_compress_enable(true);
210          }
211          break;
212        }
213      }
214    }
215  }
216  template <typename Dtype>
217  void CopyMultinodeParamsFromNet(const Net<Dtype> *net, NetParameter *param) {
218    for (int i = 0; i < param->layer_size(); i++) {
219      LayerParameter* source_layer = param->mutable_layer(i);
220      const string& source_layer_name = source_layer->name();
221      int target_layer_id = 0;
222      while (target_layer_id != net->layer_names().size() &&
223             net->layer_names()[target_layer_id] != source_layer_name) {
224        ++target_layer_id;
225      }
226      if (target_layer_id == net->layer_names().size()) continue;
227      *source_layer->mutable_multinode() =
228        net->layers()[target_layer_id]->layer_param().multinode();
229    }
230  }
231  template <typename Dtype>
232  void RevertMultinodeParams(NetParameter* param, bool write_diff) {
233    NetParameter orig_param;
234    orig_param.CopyFrom(*param);
235    param->clear_layer();
236    for (int i = 0; i < orig_param.layer_size(); i++) {
237      const LayerParameter& orig_layer_param = orig_param.layer(i);
238      if (orig_layer_param.type() == "MnActivation") continue;
239      LayerParameter* layer_param = param->add_layer();
240      layer_param->CopyFrom(orig_layer_param);
241      layer_param->clear_bottom();
242      for (int j = 0; j < orig_layer_param.bottom_size(); j++) {
243        string bottom_name = orig_layer_param.bottom(j);
244        string prefix = "mn_activation/" + orig_layer_param.name() + "/";
245        if (bottom_name.find(prefix) == 0) {
246          bottom_name = bottom_name.substr(prefix.size());
247        }
248        layer_param->add_bottom(bottom_name);
249      }
250      const MultinodeLayerParameter &mn_layer_param = orig_layer_param.multinode();
251      int num_nodes = mn_layer_param.num_nodes();
252      int model_parts = mn_layer_param.model_parts();
253      mn::GetCanonicalMnParam(num_nodes, model_parts);
254      if (model_parts > 1) {
255        if (layer_param->type() == "Convolution") {
256          ConvolutionParameter *conv_param = layer_param->mutable_convolution_param();
257          conv_param->set_num_output(conv_param->num_output() * model_parts);
258        } else if (layer_param->type() == "InnerProduct") {
259          InnerProductParameter *ip_param = layer_param->mutable_inner_product_param();
260          ip_param->set_num_output(ip_param->num_output() * model_parts);
261          CHECK(!ip_param->transpose()) << "Model parallelism does not support transpose!";
262        }
263        layer_param->clear_blobs();
264        for (int j = 0; j < orig_layer_param.blobs_size(); j++) {
265          BlobProto *blob_proto = layer_param->add_blobs();
266          Blob<Dtype> orig_blob;
267          orig_blob.FromProto(orig_layer_param.blobs(j));
268          vector<int> shape = orig_blob.shape();
269          Blob<Dtype> new_blob;
270          if (shape.size() > 0) {
271            mn::Distribution *distrib = mn::get_distrib(num_nodes/model_parts, model_parts);
272            int count = orig_blob.count();
273            shape[0] *= model_parts;
274            new_blob.Reshape(shape);
275            distrib->allgather<Dtype,MLSL::GT_MODEL>(
276              orig_blob.mutable_cpu_data(), count, new_blob.mutable_cpu_data());
277            if (write_diff) {
278              distrib->allgather<Dtype,MLSL::GT_MODEL>(
279                orig_blob.mutable_cpu_diff(), count, new_blob.mutable_cpu_diff());
280            }
281          }
282          new_blob.ToProto(blob_proto, write_diff);
283        }
284      }
285      layer_param->mutable_multinode()->Clear();
286    }
287  }
288  template void ApplyMultinodeParams<float>(const NetParameter& param,
289      NetParameter* param_with_mn);
290  template void ApplyMultinodeParams<double>(const NetParameter& param,
291      NetParameter* param_with_mn);
292  template void CopyMultinodeParamsFromNet<float>(const Net<float> *net, NetParameter *param);
293  template void CopyMultinodeParamsFromNet<double>(const Net<double> *net, NetParameter *param);
294  template void RevertMultinodeParams<float>(NetParameter* param, bool write_diff);
295  template void RevertMultinodeParams<double>(NetParameter* param, bool write_diff);
296  } 
297  #endif 
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-apply_mn_param.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-apply_mn_param.cpp</div>
                </div>
                <div class="column column_space"><pre><code>144          CHECK_EQ(conv_param->num_output(), model_parts * new_num_output)
145            << "Convolution layer " << layer_param->name()
146            << ": Undividible num_output " << conv_param->num_output()
147            << " by model_parts " << model_parts;
148          conv_param->set_num_output(new_num_output);
</pre></code></div>
                <div class="column column_space"><pre><code>144          CHECK_EQ(conv_param->num_output(), model_parts * new_num_output)
145            << "Convolution layer " << layer_param->name()
146            << ": Undividible num_output " << conv_param->num_output()
147            << " by model_parts " << model_parts;
148          conv_param->set_num_output(new_num_output);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    