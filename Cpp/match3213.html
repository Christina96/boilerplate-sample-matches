<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for dummy_data_layer.cpp &amp; blob.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for dummy_data_layer.cpp &amp; blob.cpp
      </h3>
<h1 align="center">
        5.9%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>dummy_data_layer.cpp (22.64151%)<th>blob.cpp (3.4188035%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(95-101)<td><a href="#" name="0">(51-58)</a><td align="center"><font color="#ff0000">12</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(62-75)<td><a href="#" name="1">(477-483)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>dummy_data_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;vector&gt;
2 #include "caffe/filler.hpp"
3 #include "caffe/layers/dummy_data_layer.hpp"
4 namespace caffe {
5 template &lt;typename Dtype&gt;
6 void DummyDataLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
7       const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
8   const int num_top = top.size();
9   const DummyDataParameter&amp; param = this-&gt;layer_param_.dummy_data_param();
10   const int num_data_filler = param.data_filler_size();
11   CHECK(num_data_filler == 0 || num_data_filler == 1 ||
12         num_data_filler == num_top)
13       &lt;&lt; "Number of data fillers must be 0, 1 or equal to the number of tops: "
14       &lt;&lt; num_top &lt;&lt; "; you specified " &lt;&lt; num_data_filler &lt;&lt; " data fillers.";
15   const bool legacy_dims = param.num_size() || param.channels_size() ||
16                            param.height_size() || param.width_size();
17   if (legacy_dims) {
18     CHECK_EQ(0, param.shape_size())
19         &lt;&lt; "Both shape and legacy fields were specified";
20     CHECK(param.num_size() == 1 || param.num_size() == num_top)
21         &lt;&lt; "Must specify 'num' once, or once per top blob "
22         &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.num_size() &lt;&lt; ".";
23     CHECK(param.channels_size() == 1 || param.channels_size() == num_top)
24         &lt;&lt; "Must specify 'channels' once, or once per top blob "
25         &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.channels_size() &lt;&lt; ".";
26     CHECK(param.height_size() == 1 || param.height_size() == num_top)
27         &lt;&lt; "Must specify 'height' once, or once per top blob "
28         &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.height_size() &lt;&lt; ".";
29     CHECK(param.width_size() == 1 || param.width_size() == num_top)
30         &lt;&lt; "Must specify 'width' once, or once per top blob "
31         &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.width_size() &lt;&lt; ".";
32   } else {
33     CHECK(param.shape_size() == 1 || param.shape_size() == num_top)
34         &lt;&lt; "Must specify 'shape' once, or once per top blob "
35         &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.shape_size() &lt;&lt; ".";
36   }
37   refill_.clear();
38   fillers_.clear();
39   if (num_data_filler &lt;= 1) {
40     FillerParameter filler_param;
41     if (num_data_filler == 0) {
42       filler_param.set_type("constant");
43       filler_param.set_value(0);
44     } else {
45       filler_param.CopyFrom(param.data_filler(0));
46     }
47 <a name="1"></a>        refill_.resize(1);
48 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>    refill_[0] = (strcmp(filler_param.type().c_str(), "constant") == 0);
49     fillers_.resize(1);
50     fillers_[0].reset(GetFiller&lt;Dtype&gt;(filler_param));
51   } else {
52     refill_.resize(num_top);
53     fillers_.resize(num_top);
54     for (int i = 0; i &lt; num_top; ++i) {
55       fillers_[i].reset(GetFiller&lt;Dtype&gt;(param.data_filler(i)));
56       refill_[i] =
57           (strcmp(param.data_filler(i).type().c_str(), "constant") == 0);
58     }
59   }</b></font>
60   for (int i = 0; i &lt; num_top; ++i) {
61     if (legacy_dims) {
62       const int num = (param.num_size() == 1) ? param.num(0) : param.num(i);
63       const int channels =
64           (param.channels_size() == 1) ? param.channels(0) : param.channels(i);
65       const int height =
66           (param.height_size() == 1) ? param.height(0) : param.height(i);
67       const int width =
68           (param.width_size() == 1) ? param.width(0) : param.width(i);
69       top[i]-&gt;Reshape(num, channels, height, width);
70     } else {
71       const int shape_index = (param.shape_size() == 1) ? 0 : i;
72       top[i]-&gt;Reshape(param.shape(shape_index));
73     }
74   }
75 <a name="0"></a>  this-&gt;Forward(bottom, top);
76 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  for (int i = 0; i &lt; refill_.size(); ++i) {
77     refill_[i] = !refill_[i];
78   }
79 }
80 template &lt;typename Dtype&gt;
81 void DummyDataLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</b></font>
82       const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
83   for (int i = 0; i &lt; top.size(); ++i) {
84     const int filler_id = (fillers_.size() &gt; 1) ? i : 0;
85     if (refill_[filler_id]) {
86       fillers_[filler_id]-&gt;Fill(top[i]);
87     }
88   }
89 }
90 INSTANTIATE_CLASS(DummyDataLayer);
91 REGISTER_LAYER_CLASS(DummyData);
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>blob.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;climits&gt;
2 #include &lt;vector&gt;
3 #include "caffe/blob.hpp"
4 #include "caffe/common.hpp"
5 #include "caffe/syncedmem.hpp"
6 #include "caffe/util/math_functions.hpp"
7 namespace caffe {
8 template &lt;typename Dtype&gt;
9 void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
10     const int width) {
11   vector&lt;int&gt; shape(4);
12   shape[0] = num;
13   shape[1] = channels;
14   shape[2] = height;
15   shape[3] = width;
16   Reshape(shape);
17 }
18 template &lt;typename Dtype&gt;
19 void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
20   CHECK_LE(shape.size(), kMaxBlobAxes);
21   count_ = 1;
22   shape_.resize(shape.size());
23   if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
24     shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
25   }
26   int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
27   for (int i = 0; i &lt; shape.size(); ++i) {
28     CHECK_GE(shape[i], 0);
29     if (count_ != 0) {
30       CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; "blob size exceeds INT_MAX";
31     }
32     count_ *= shape[i];
33     shape_[i] = shape[i];
34     shape_data[i] = shape[i];
35   }
36   if (count_ &gt; capacity_) {
37     capacity_ = count_;
38     data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
39     diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
40   }
41 }
42 template &lt;typename Dtype&gt;
43 <a name="0"></a>void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) {
44   CHECK_LE(shape.dim_size(), kMaxBlobAxes);
45   vector&lt;int&gt; shape_vec(shape.dim_size());
46 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>  for (int i = 0; i &lt; shape.dim_size(); ++i) {
47     shape_vec[i] = shape.dim(i);
48   }
49   Reshape(shape_vec);
50 }
51 template &lt;typename Dtype&gt;
52 void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other) {</b></font>
53   Reshape(other.shape());
54 }
55 template &lt;typename Dtype&gt;
56 Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
57     const int width)
58   : capacity_(0) {
59   Reshape(num, channels, height, width);
60 }
61 template &lt;typename Dtype&gt;
62 Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
63   : capacity_(0) {
64   Reshape(shape);
65 }
66 template &lt;typename Dtype&gt;
67 const int* Blob&lt;Dtype&gt;::gpu_shape() const {
68   CHECK(shape_data_);
69   return (const int*)shape_data_-&gt;gpu_data();
70 }
71 template &lt;typename Dtype&gt;
72 const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
73   CHECK(data_);
74   return (const Dtype*)data_-&gt;cpu_data();
75 }
76 template &lt;typename Dtype&gt;
77 void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
78   CHECK(data);
79   size_t size = count_ * sizeof(Dtype);
80   if (data_-&gt;size() != size) {
81     data_.reset(new SyncedMemory(size));
82     diff_.reset(new SyncedMemory(size));
83   }
84   data_-&gt;set_cpu_data(data);
85 }
86 template &lt;typename Dtype&gt;
87 const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
88   CHECK(data_);
89   return (const Dtype*)data_-&gt;gpu_data();
90 }
91 template &lt;typename Dtype&gt;
92 void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
93   CHECK(data);
94   size_t size = count_ * sizeof(Dtype);
95   if (data_-&gt;size() != size) {
96     data_.reset(new SyncedMemory(size));
97     diff_.reset(new SyncedMemory(size));
98   }
99   data_-&gt;set_gpu_data(data);
100 }
101 template &lt;typename Dtype&gt;
102 const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
103   CHECK(diff_);
104   return (const Dtype*)diff_-&gt;cpu_data();
105 }
106 template &lt;typename Dtype&gt;
107 const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
108   CHECK(diff_);
109   return (const Dtype*)diff_-&gt;gpu_data();
110 }
111 template &lt;typename Dtype&gt;
112 Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
113   CHECK(data_);
114   return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
115 }
116 template &lt;typename Dtype&gt;
117 Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
118   CHECK(data_);
119   return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
120 }
121 template &lt;typename Dtype&gt;
122 Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
123   CHECK(diff_);
124   return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
125 }
126 template &lt;typename Dtype&gt;
127 Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
128   CHECK(diff_);
129   return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
130 }
131 template &lt;typename Dtype&gt;
132 void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
133   CHECK_EQ(count_, other.count());
134   data_ = other.data();
135 }
136 template &lt;typename Dtype&gt;
137 void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
138   CHECK_EQ(count_, other.count());
139   diff_ = other.diff();
140 }
141 template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
142 template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }
143 template &lt;typename Dtype&gt;
144 void Blob&lt;Dtype&gt;::Update() {
145   switch (data_-&gt;head()) {
146   case SyncedMemory::HEAD_AT_CPU:
147     caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
148         static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
149         static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
150     break;
151   case SyncedMemory::HEAD_AT_GPU:
152   case SyncedMemory::SYNCED:
153 #ifndef CPU_ONLY
154     caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
155         static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
156         static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
157 #else
158     NO_GPU;
159 #endif
160     break;
161   default:
162     LOG(FATAL) &lt;&lt; "Syncedmem not initialized.";
163   }
164 }
165 template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
166   NOT_IMPLEMENTED;
167   return 0;
168 }
169 template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
170   NOT_IMPLEMENTED;
171   return 0;
172 }
173 template &lt;typename Dtype&gt;
174 Dtype Blob&lt;Dtype&gt;::asum_data() const {
175   if (!data_) { return 0; }
176   switch (data_-&gt;head()) {
177   case SyncedMemory::HEAD_AT_CPU:
178     return caffe_cpu_asum(count_, cpu_data());
179   case SyncedMemory::HEAD_AT_GPU:
180   case SyncedMemory::SYNCED:
181 #ifndef CPU_ONLY
182   {
183     Dtype asum;
184     caffe_gpu_asum(count_, gpu_data(), &amp;asum);
185     return asum;
186   }
187 #else
188     NO_GPU;
189 #endif
190   case SyncedMemory::UNINITIALIZED:
191     return 0;
192   default:
193     LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
194   }
195   return 0;
196 }
197 template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
198   NOT_IMPLEMENTED;
199   return 0;
200 }
201 template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
202   NOT_IMPLEMENTED;
203   return 0;
204 }
205 template &lt;typename Dtype&gt;
206 Dtype Blob&lt;Dtype&gt;::asum_diff() const {
207   if (!diff_) { return 0; }
208   switch (diff_-&gt;head()) {
209   case SyncedMemory::HEAD_AT_CPU:
210     return caffe_cpu_asum(count_, cpu_diff());
211   case SyncedMemory::HEAD_AT_GPU:
212   case SyncedMemory::SYNCED:
213 #ifndef CPU_ONLY
214   {
215     Dtype asum;
216     caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
217     return asum;
218   }
219 #else
220     NO_GPU;
221 #endif
222   case SyncedMemory::UNINITIALIZED:
223     return 0;
224   default:
225     LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; diff_-&gt;head();
226   }
227   return 0;
228 }
229 template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
230   NOT_IMPLEMENTED;
231   return 0;
232 }
233 template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
234   NOT_IMPLEMENTED;
235   return 0;
236 }
237 template &lt;typename Dtype&gt;
238 Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
239   Dtype sumsq;
240   const Dtype* data;
241   if (!data_) { return 0; }
242   switch (data_-&gt;head()) {
243   case SyncedMemory::HEAD_AT_CPU:
244     data = cpu_data();
245     sumsq = caffe_cpu_dot(count_, data, data);
246     break;
247   case SyncedMemory::HEAD_AT_GPU:
248   case SyncedMemory::SYNCED:
249 #ifndef CPU_ONLY
250     data = gpu_data();
251     caffe_gpu_dot(count_, data, data, &amp;sumsq);
252 #else
253     NO_GPU;
254 #endif
255     break;
256   case SyncedMemory::UNINITIALIZED:
257     return 0;
258   default:
259     LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
260   }
261   return sumsq;
262 }
263 template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
264   NOT_IMPLEMENTED;
265   return 0;
266 }
267 template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
268   NOT_IMPLEMENTED;
269   return 0;
270 }
271 template &lt;typename Dtype&gt;
272 Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
273   Dtype sumsq;
274   const Dtype* diff;
275   if (!diff_) { return 0; }
276   switch (diff_-&gt;head()) {
277   case SyncedMemory::HEAD_AT_CPU:
278     diff = cpu_diff();
279     sumsq = caffe_cpu_dot(count_, diff, diff);
280     break;
281   case SyncedMemory::HEAD_AT_GPU:
282   case SyncedMemory::SYNCED:
283 #ifndef CPU_ONLY
284     diff = gpu_diff();
285     caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
286     break;
287 #else
288     NO_GPU;
289 #endif
290   case SyncedMemory::UNINITIALIZED:
291     return 0;
292   default:
293     LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
294   }
295   return sumsq;
296 }
297 template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
298   NOT_IMPLEMENTED;
299 }
300 template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
301   NOT_IMPLEMENTED;
302 }
303 template &lt;typename Dtype&gt;
304 void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
305   Dtype* data;
306   if (!data_) { return; }
307   switch (data_-&gt;head()) {
308   case SyncedMemory::HEAD_AT_CPU:
309     data = mutable_cpu_data();
310     caffe_scal(count_, scale_factor, data);
311     return;
312   case SyncedMemory::HEAD_AT_GPU:
313   case SyncedMemory::SYNCED:
314 #ifndef CPU_ONLY
315     data = mutable_gpu_data();
316     caffe_gpu_scal(count_, scale_factor, data);
317     return;
318 #else
319     NO_GPU;
320 #endif
321   case SyncedMemory::UNINITIALIZED:
322     return;
323   default:
324     LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
325   }
326 }
327 template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
328   NOT_IMPLEMENTED;
329 }
330 template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
331   NOT_IMPLEMENTED;
332 }
333 template &lt;typename Dtype&gt;
334 void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
335   Dtype* diff;
336   if (!diff_) { return; }
337   switch (diff_-&gt;head()) {
338   case SyncedMemory::HEAD_AT_CPU:
339     diff = mutable_cpu_diff();
340     caffe_scal(count_, scale_factor, diff);
341     return;
342   case SyncedMemory::HEAD_AT_GPU:
343   case SyncedMemory::SYNCED:
344 #ifndef CPU_ONLY
345     diff = mutable_gpu_diff();
346     caffe_gpu_scal(count_, scale_factor, diff);
347     return;
348 #else
349     NO_GPU;
350 #endif
351   case SyncedMemory::UNINITIALIZED:
352     return;
353   default:
354     LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; diff_-&gt;head();
355   }
356 }
357 template &lt;typename Dtype&gt;
358 bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
359   if (other.has_num() || other.has_channels() ||
360       other.has_height() || other.has_width()) {
361     return shape_.size() &lt;= 4 &amp;&amp;
362            LegacyShape(-4) == other.num() &amp;&amp;
363            LegacyShape(-3) == other.channels() &amp;&amp;
364            LegacyShape(-2) == other.height() &amp;&amp;
365            LegacyShape(-1) == other.width();
366   }
367   vector&lt;int&gt; other_shape(other.shape().dim_size());
368   for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
369     other_shape[i] = other.shape().dim(i);
370   }
371   return shape_ == other_shape;
372 }
373 template &lt;typename Dtype&gt;
374 void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
375   if (source.count() != count_ || source.shape() != shape_) {
376     if (reshape) {
377       ReshapeLike(source);
378     } else {
379       LOG(FATAL) &lt;&lt; "Trying to copy blobs of different sizes.";
380     }
381   }
382   switch (Caffe::mode()) {
383   case Caffe::GPU:
384     if (copy_diff) {
385       caffe_copy(count_, source.gpu_diff(),
386           static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
387     } else {
388       caffe_copy(count_, source.gpu_data(),
389           static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
390     }
391     break;
392   case Caffe::CPU:
393     if (copy_diff) {
394       caffe_copy(count_, source.cpu_diff(),
395           static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
396     } else {
397       caffe_copy(count_, source.cpu_data(),
398           static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
399     }
400     break;
401   default:
402     LOG(FATAL) &lt;&lt; "Unknown caffe mode.";
403   }
404 }
405 template &lt;typename Dtype&gt;
406 void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
407   if (reshape) {
408     vector&lt;int&gt; shape;
409     if (proto.has_num() || proto.has_channels() ||
410         proto.has_height() || proto.has_width()) {
411       shape.resize(4);
412 <a name="1"></a>      shape[0] = proto.num();
413       shape[1] = proto.channels();
414       shape[2] = proto.height();
415 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      shape[3] = proto.width();
416     } else {
417       shape.resize(proto.shape().dim_size());
418       for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
419         shape[i] = proto.shape().dim(i);
420       }
421     }</b></font>
422     Reshape(shape);
423   } else {
424     CHECK(ShapeEquals(proto)) &lt;&lt; "shape mismatch (reshape not set)";
425   }
426   Dtype* data_vec = mutable_cpu_data();
427   if (proto.double_data_size() &gt; 0) {
428     CHECK_EQ(count_, proto.double_data_size());
429     for (int i = 0; i &lt; count_; ++i) {
430       data_vec[i] = proto.double_data(i);
431     }
432   } else {
433     CHECK_EQ(count_, proto.data_size());
434     for (int i = 0; i &lt; count_; ++i) {
435       data_vec[i] = proto.data(i);
436     }
437   }
438   if (proto.double_diff_size() &gt; 0) {
439     CHECK_EQ(count_, proto.double_diff_size());
440     Dtype* diff_vec = mutable_cpu_diff();
441     for (int i = 0; i &lt; count_; ++i) {
442       diff_vec[i] = proto.double_diff(i);
443     }
444   } else if (proto.diff_size() &gt; 0) {
445     CHECK_EQ(count_, proto.diff_size());
446     Dtype* diff_vec = mutable_cpu_diff();
447     for (int i = 0; i &lt; count_; ++i) {
448       diff_vec[i] = proto.diff(i);
449     }
450   }
451 }
452 template &lt;&gt;
453 void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
454   proto-&gt;clear_shape();
455   for (int i = 0; i &lt; shape_.size(); ++i) {
456     proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
457   }
458   proto-&gt;clear_double_data();
459   proto-&gt;clear_double_diff();
460   const double* data_vec = cpu_data();
461   for (int i = 0; i &lt; count_; ++i) {
462     proto-&gt;add_double_data(data_vec[i]);
463   }
464   if (write_diff) {
465     const double* diff_vec = cpu_diff();
466     for (int i = 0; i &lt; count_; ++i) {
467       proto-&gt;add_double_diff(diff_vec[i]);
468     }
469   }
470 }
471 template &lt;&gt;
472 void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
473   proto-&gt;clear_shape();
474   for (int i = 0; i &lt; shape_.size(); ++i) {
475     proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
476   }
477   proto-&gt;clear_data();
478   proto-&gt;clear_diff();
479   const float* data_vec = cpu_data();
480   for (int i = 0; i &lt; count_; ++i) {
481     proto-&gt;add_data(data_vec[i]);
482   }
483   if (write_diff) {
484     const float* diff_vec = cpu_diff();
485     for (int i = 0; i &lt; count_; ++i) {
486       proto-&gt;add_diff(diff_vec[i]);
487     }
488   }
489 }
490 INSTANTIATE_CLASS(Blob);
491 template class Blob&lt;int&gt;;
492 template class Blob&lt;unsigned int&gt;;
493 }  
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
