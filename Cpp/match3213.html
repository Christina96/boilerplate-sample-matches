<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for dummy_data_layer.cpp & blob.cpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for dummy_data_layer.cpp & blob.cpp
      </h3>
      <h1 align="center">
        5.9%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>dummy_data_layer.cpp (22.64151%)<TH>blob.cpp (3.4188035%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match3213-0.html#0',2,'match3213-1.html#0',3)" NAME="0">(95-101)<TD><A HREF="javascript:ZweiFrames('match3213-0.html#0',2,'match3213-1.html#0',3)" NAME="0">(51-58)</A><TD ALIGN=center><FONT COLOR="#ff0000">12</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match3213-0.html#1',2,'match3213-1.html#1',3)" NAME="1">(62-75)<TD><A HREF="javascript:ZweiFrames('match3213-0.html#1',2,'match3213-1.html#1',3)" NAME="1">(477-483)</A><TD ALIGN=center><FONT COLOR="#ff0000">12</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>dummy_data_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;vector&gt;

#include &quot;caffe/filler.hpp&quot;
#include &quot;caffe/layers/dummy_data_layer.hpp&quot;

namespace caffe {

template &lt;typename Dtype&gt;
void DummyDataLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const int num_top = top.size();
  const DummyDataParameter&amp; param = this-&gt;layer_param_.dummy_data_param();
  const int num_data_filler = param.data_filler_size();
  CHECK(num_data_filler == 0 || num_data_filler == 1 ||
        num_data_filler == num_top)
      &lt;&lt; &quot;Number of data fillers must be 0, 1 or equal to the number of tops: &quot;
      &lt;&lt; num_top &lt;&lt; &quot;; you specified &quot; &lt;&lt; num_data_filler &lt;&lt; &quot; data fillers.&quot;;

  const bool legacy_dims = param.num_size() || param.channels_size() ||
                           param.height_size() || param.width_size();
  if (legacy_dims) {
    CHECK_EQ(0, param.shape_size())
        &lt;&lt; &quot;Both shape and legacy fields were specified&quot;;
    // Using deprecated 4D output dim specifiers.
    CHECK(param.num_size() == 1 || param.num_size() == num_top)
        &lt;&lt; &quot;Must specify 'num' once, or once per top blob &quot;
        &lt;&lt; &quot;(&quot; &lt;&lt; num_top &lt;&lt; &quot;); specified &quot; &lt;&lt; param.num_size() &lt;&lt; &quot;.&quot;;
    CHECK(param.channels_size() == 1 || param.channels_size() == num_top)
        &lt;&lt; &quot;Must specify 'channels' once, or once per top blob &quot;
        &lt;&lt; &quot;(&quot; &lt;&lt; num_top &lt;&lt; &quot;); specified &quot; &lt;&lt; param.channels_size() &lt;&lt; &quot;.&quot;;
    CHECK(param.height_size() == 1 || param.height_size() == num_top)
        &lt;&lt; &quot;Must specify 'height' once, or once per top blob &quot;
        &lt;&lt; &quot;(&quot; &lt;&lt; num_top &lt;&lt; &quot;); specified &quot; &lt;&lt; param.height_size() &lt;&lt; &quot;.&quot;;
    CHECK(param.width_size() == 1 || param.width_size() == num_top)
        &lt;&lt; &quot;Must specify 'width' once, or once per top blob &quot;
        &lt;&lt; &quot;(&quot; &lt;&lt; num_top &lt;&lt; &quot;); specified &quot; &lt;&lt; param.width_size() &lt;&lt; &quot;.&quot;;
  } else {
    CHECK(param.shape_size() == 1 || param.shape_size() == num_top)
        &lt;&lt; &quot;Must specify 'shape' once, or once per top blob &quot;
        &lt;&lt; &quot;(&quot; &lt;&lt; num_top &lt;&lt; &quot;); specified &quot; &lt;&lt; param.shape_size() &lt;&lt; &quot;.&quot;;
  }
  // refill_[i] tells Forward i whether or not to actually refill top Blob i.
  // If refill_[i] is false, Forward does nothing for Blob i. We use this to
  // avoid wastefully refilling &quot;constant&quot; Blobs in every forward pass.
  // We first fill refill_ in with the INVERSE of its final values.
  // The first time we run Forward from the LayerSetUp method, we'll fill only
  // Blobs for which refill_ is normally false.  These Blobs will never be
  // filled again.
  refill_.clear();
  fillers_.clear();
  if (num_data_filler &lt;= 1) {
    FillerParameter filler_param;
    if (num_data_filler == 0) {
      filler_param.set_type(&quot;constant&quot;);
      filler_param.set_value(0);
    } else {
      filler_param.CopyFrom(param.data_filler(0));
    }
<A NAME="1"></A>    // Refill on each iteration iff not using a constant filler,
    // but use the inverse of this rule for the first run.
    refill_.resize(1);
<FONT color="#f63526"><A HREF="javascript:ZweiFrames('match3213-1.html#1',3,'match3213-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>    refill_[0] = (strcmp(filler_param.type().c_str(), &quot;constant&quot;) == 0);
    fillers_.resize(1);
    fillers_[0].reset(GetFiller&lt;Dtype&gt;(filler_param));
  } else {
    refill_.resize(num_top);
    fillers_.resize(num_top);
    for (int i = 0; i &lt; num_top; ++i) {
      fillers_[i].reset(GetFiller&lt;Dtype&gt;(param.data_filler(i)));
      // Refill on each iteration iff not using a constant filler,
      // but use the inverse of this rule for the first run.
      refill_[i] =
          (strcmp(param.data_filler(i).type().c_str(), &quot;constant&quot;) == 0);
    }
  }</B></FONT>
  for (int i = 0; i &lt; num_top; ++i) {
    if (legacy_dims) {
      const int num = (param.num_size() == 1) ? param.num(0) : param.num(i);
      const int channels =
          (param.channels_size() == 1) ? param.channels(0) : param.channels(i);
      const int height =
          (param.height_size() == 1) ? param.height(0) : param.height(i);
      const int width =
          (param.width_size() == 1) ? param.width(0) : param.width(i);
      top[i]-&gt;Reshape(num, channels, height, width);
    } else {
      const int shape_index = (param.shape_size() == 1) ? 0 : i;
      top[i]-&gt;Reshape(param.shape(shape_index));
    }
  }
  // Run Forward once, with refill_ inverted, to fill the constant Blobs.
<A NAME="0"></A>  this-&gt;Forward(bottom, top);
  // Invert the inverted refill_ values to refill the desired (non-constant)
  // Blobs in every usual forward pass.
<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match3213-1.html#0',3,'match3213-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>  for (int i = 0; i &lt; refill_.size(); ++i) {
    refill_[i] = !refill_[i];
  }
}

template &lt;typename Dtype&gt;
void DummyDataLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</B></FONT>
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  for (int i = 0; i &lt; top.size(); ++i) {
    const int filler_id = (fillers_.size() &gt; 1) ? i : 0;
    if (refill_[filler_id]) {
      fillers_[filler_id]-&gt;Fill(top[i]);
    }
  }
}

INSTANTIATE_CLASS(DummyDataLayer);
REGISTER_LAYER_CLASS(DummyData);

}  // namespace caffe
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>blob.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;climits&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/syncedmem.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes);
  count_ = 1;
  shape_.resize(shape.size());
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);
    if (count_ != 0) {
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; &quot;blob size exceeds INT_MAX&quot;;
    }
    count_ *= shape[i];
    shape_[i] = shape[i];
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {
    capacity_ = count_;
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}

template &lt;typename Dtype&gt;
<A NAME="0"></A>void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) {
  CHECK_LE(shape.dim_size(), kMaxBlobAxes);
  vector&lt;int&gt; shape_vec(shape.dim_size());
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match3213-0.html#0',2,'match3213-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>  for (int i = 0; i &lt; shape.dim_size(); ++i) {
    shape_vec[i] = shape.dim(i);
  }
  Reshape(shape_vec);
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other) {</B></FONT>
  Reshape(other.shape());
}

template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
    const int width)
  // capacity_ must be initialized before calling Reshape
  : capacity_(0) {
  Reshape(num, channels, height, width);
}

template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
  // capacity_ must be initialized before calling Reshape
  : capacity_(0) {
  Reshape(shape);
}

template &lt;typename Dtype&gt;
const int* Blob&lt;Dtype&gt;::gpu_shape() const {
  CHECK(shape_data_);
  return (const int*)shape_data_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;cpu_data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_cpu_data(data);
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
  CHECK(data);
  // Make sure CPU and GPU sizes remain equal
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_gpu_data(data);
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;cpu_data();
}

template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;gpu_data();
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
}

template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  data_ = other.data();
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  diff_ = other.diff();
}

// The &quot;update&quot; method is used for parameter blobs in a Net, which are stored
// as Blob&lt;float&gt; or Blob&lt;double&gt; -- hence we do not define it for
// Blob&lt;int&gt; or Blob&lt;unsigned int&gt;.
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Update() {
  // We will perform update based on where the data is located.
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    // perform computation on CPU
    caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    // perform computation on GPU
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
#else
    NO_GPU;
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;
  }
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_data() const {
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_data());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return 0;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_diff() const {
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_diff());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
  return 0;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
  Dtype sumsq;
  const Dtype* data;
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = cpu_data();
    sumsq = caffe_cpu_dot(count_, data, data);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = gpu_data();
    caffe_gpu_dot(count_, data, data, &amp;sumsq);
#else
    NO_GPU;
#endif
    break;
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}

template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}

template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
  Dtype sumsq;
  const Dtype* diff;
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = cpu_diff();
    sumsq = caffe_cpu_dot(count_, diff, diff);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = gpu_diff();
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
    break;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
  Dtype* data;
  if (!data_) { return; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = mutable_cpu_data();
    caffe_scal(count_, scale_factor, data);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = mutable_gpu_data();
    caffe_gpu_scal(count_, scale_factor, data);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; data_-&gt;head();
  }
}

template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
  NOT_IMPLEMENTED;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
  Dtype* diff;
  if (!diff_) { return; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = mutable_cpu_diff();
    caffe_scal(count_, scale_factor, diff);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = mutable_gpu_diff();
    caffe_gpu_scal(count_, scale_factor, diff);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown SyncedMemory head state: &quot; &lt;&lt; diff_-&gt;head();
  }
}

template &lt;typename Dtype&gt;
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
  if (other.has_num() || other.has_channels() ||
      other.has_height() || other.has_width()) {
    // Using deprecated 4D Blob dimensions --
    // shape is (num, channels, height, width).
    // Note: we do not use the normal Blob::num(), Blob::channels(), etc.
    // methods as these index from the beginning of the blob shape, where legacy
    // parameter blobs were indexed from the end of the blob shape (e.g., bias
    // Blob shape (1 x 1 x 1 x N), IP layer weight Blob shape (1 x 1 x M x N)).
    return shape_.size() &lt;= 4 &amp;&amp;
           LegacyShape(-4) == other.num() &amp;&amp;
           LegacyShape(-3) == other.channels() &amp;&amp;
           LegacyShape(-2) == other.height() &amp;&amp;
           LegacyShape(-1) == other.width();
  }
  vector&lt;int&gt; other_shape(other.shape().dim_size());
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
    other_shape[i] = other.shape().dim(i);
  }
  return shape_ == other_shape;
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
  if (source.count() != count_ || source.shape() != shape_) {
    if (reshape) {
      ReshapeLike(source);
    } else {
      LOG(FATAL) &lt;&lt; &quot;Trying to copy blobs of different sizes.&quot;;
    }
  }
  switch (Caffe::mode()) {
  case Caffe::GPU:
    if (copy_diff) {
      caffe_copy(count_, source.gpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
    } else {
      caffe_copy(count_, source.gpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
    }
    break;
  case Caffe::CPU:
    if (copy_diff) {
      caffe_copy(count_, source.cpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
    } else {
      caffe_copy(count_, source.cpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    }
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown caffe mode.&quot;;
  }
}

template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
  if (reshape) {
    vector&lt;int&gt; shape;
    if (proto.has_num() || proto.has_channels() ||
        proto.has_height() || proto.has_width()) {
      // Using deprecated 4D Blob dimensions --
      // shape is (num, channels, height, width).
      shape.resize(4);
<A NAME="1"></A>      shape[0] = proto.num();
      shape[1] = proto.channels();
      shape[2] = proto.height();
<FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match3213-0.html#1',2,'match3213-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>      shape[3] = proto.width();
    } else {
      shape.resize(proto.shape().dim_size());
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
        shape[i] = proto.shape().dim(i);
      }
    }</B></FONT>
    Reshape(shape);
  } else {
    CHECK(ShapeEquals(proto)) &lt;&lt; &quot;shape mismatch (reshape not set)&quot;;
  }
  // copy data
  Dtype* data_vec = mutable_cpu_data();
  if (proto.double_data_size() &gt; 0) {
    CHECK_EQ(count_, proto.double_data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.double_data(i);
    }
  } else {
    CHECK_EQ(count_, proto.data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.data(i);
    }
  }
  if (proto.double_diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.double_diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.double_diff(i);
    }
  } else if (proto.diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.diff(i);
    }
  }
}

template &lt;&gt;
void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_double_data();
  proto-&gt;clear_double_diff();
  const double* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_double_data(data_vec[i]);
  }
  if (write_diff) {
    const double* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_double_diff(diff_vec[i]);
    }
  }
}

template &lt;&gt;
void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_data();
  proto-&gt;clear_diff();
  const float* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_data(data_vec[i]);
  }
  if (write_diff) {
    const float* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_diff(diff_vec[i]);
    }
  }
}

INSTANTIATE_CLASS(Blob);
template class Blob&lt;int&gt;;
template class Blob&lt;unsigned int&gt;;

}  // namespace caffe

</PRE>
</div>
  </div>
</body>
</html>
