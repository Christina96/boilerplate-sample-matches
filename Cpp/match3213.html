<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for dummy_data_layer.cpp &amp; blob.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for dummy_data_layer.cpp &amp; blob.cpp
      </h3>
<h1 align="center">
        5.9%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>dummy_data_layer.cpp (22.64151%)<th>blob.cpp (3.4188035%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(95-101)<td><a href="#" name="0">(51-58)</a><td align="center"><font color="#ff0000">12</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(62-75)<td><a href="#" name="1">(477-483)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>dummy_data_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;vector&gt;
#include "caffe/filler.hpp"
#include "caffe/layers/dummy_data_layer.hpp"
namespace caffe {
template &lt;typename Dtype&gt;
void DummyDataLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const int num_top = top.size();
  const DummyDataParameter&amp; param = this-&gt;layer_param_.dummy_data_param();
  const int num_data_filler = param.data_filler_size();
  CHECK(num_data_filler == 0 || num_data_filler == 1 ||
        num_data_filler == num_top)
      &lt;&lt; "Number of data fillers must be 0, 1 or equal to the number of tops: "
      &lt;&lt; num_top &lt;&lt; "; you specified " &lt;&lt; num_data_filler &lt;&lt; " data fillers.";
  const bool legacy_dims = param.num_size() || param.channels_size() ||
                           param.height_size() || param.width_size();
  if (legacy_dims) {
    CHECK_EQ(0, param.shape_size())
        &lt;&lt; "Both shape and legacy fields were specified";
    CHECK(param.num_size() == 1 || param.num_size() == num_top)
        &lt;&lt; "Must specify 'num' once, or once per top blob "
        &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.num_size() &lt;&lt; ".";
    CHECK(param.channels_size() == 1 || param.channels_size() == num_top)
        &lt;&lt; "Must specify 'channels' once, or once per top blob "
        &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.channels_size() &lt;&lt; ".";
    CHECK(param.height_size() == 1 || param.height_size() == num_top)
        &lt;&lt; "Must specify 'height' once, or once per top blob "
        &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.height_size() &lt;&lt; ".";
    CHECK(param.width_size() == 1 || param.width_size() == num_top)
        &lt;&lt; "Must specify 'width' once, or once per top blob "
        &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.width_size() &lt;&lt; ".";
  } else {
    CHECK(param.shape_size() == 1 || param.shape_size() == num_top)
        &lt;&lt; "Must specify 'shape' once, or once per top blob "
        &lt;&lt; "(" &lt;&lt; num_top &lt;&lt; "); specified " &lt;&lt; param.shape_size() &lt;&lt; ".";
  }
  refill_.clear();
  fillers_.clear();
  if (num_data_filler &lt;= 1) {
    FillerParameter filler_param;
    if (num_data_filler == 0) {
      filler_param.set_type("constant");
      filler_param.set_value(0);
    } else {
      filler_param.CopyFrom(param.data_filler(0));
    }
<a name="1"></a>        refill_.resize(1);
<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>    refill_[0] = (strcmp(filler_param.type().c_str(), "constant") == 0);
    fillers_.resize(1);
    fillers_[0].reset(GetFiller&lt;Dtype&gt;(filler_param));
  } else {
    refill_.resize(num_top);
    fillers_.resize(num_top);
    for (int i = 0; i &lt; num_top; ++i) {
      fillers_[i].reset(GetFiller&lt;Dtype&gt;(param.data_filler(i)));
      refill_[i] =
          (strcmp(param.data_filler(i).type().c_str(), "constant") == 0);
    }
  }</b></font>
  for (int i = 0; i &lt; num_top; ++i) {
    if (legacy_dims) {
      const int num = (param.num_size() == 1) ? param.num(0) : param.num(i);
      const int channels =
          (param.channels_size() == 1) ? param.channels(0) : param.channels(i);
      const int height =
          (param.height_size() == 1) ? param.height(0) : param.height(i);
      const int width =
          (param.width_size() == 1) ? param.width(0) : param.width(i);
      top[i]-&gt;Reshape(num, channels, height, width);
    } else {
      const int shape_index = (param.shape_size() == 1) ? 0 : i;
      top[i]-&gt;Reshape(param.shape(shape_index));
    }
  }
<a name="0"></a>  this-&gt;Forward(bottom, top);
<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  for (int i = 0; i &lt; refill_.size(); ++i) {
    refill_[i] = !refill_[i];
  }
}
template &lt;typename Dtype&gt;
void DummyDataLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</b></font>
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  for (int i = 0; i &lt; top.size(); ++i) {
    const int filler_id = (fillers_.size() &gt; 1) ? i : 0;
    if (refill_[filler_id]) {
      fillers_[filler_id]-&gt;Fill(top[i]);
    }
  }
}
INSTANTIATE_CLASS(DummyDataLayer);
REGISTER_LAYER_CLASS(DummyData);
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>blob.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;climits&gt;
#include &lt;vector&gt;
#include "caffe/blob.hpp"
#include "caffe/common.hpp"
#include "caffe/syncedmem.hpp"
#include "caffe/util/math_functions.hpp"
namespace caffe {
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const int num, const int channels, const int height,
    const int width) {
  vector&lt;int&gt; shape(4);
  shape[0] = num;
  shape[1] = channels;
  shape[2] = height;
  shape[3] = width;
  Reshape(shape);
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Reshape(const vector&lt;int&gt;&amp; shape) {
  CHECK_LE(shape.size(), kMaxBlobAxes);
  count_ = 1;
  shape_.resize(shape.size());
  if (!shape_data_ || shape_data_-&gt;size() &lt; shape.size() * sizeof(int)) {
    shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
  }
  int* shape_data = static_cast&lt;int*&gt;(shape_data_-&gt;mutable_cpu_data());
  for (int i = 0; i &lt; shape.size(); ++i) {
    CHECK_GE(shape[i], 0);
    if (count_ != 0) {
      CHECK_LE(shape[i], INT_MAX / count_) &lt;&lt; "blob size exceeds INT_MAX";
    }
    count_ *= shape[i];
    shape_[i] = shape[i];
    shape_data[i] = shape[i];
  }
  if (count_ &gt; capacity_) {
    capacity_ = count_;
    data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
    diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
  }
}
template &lt;typename Dtype&gt;
<a name="0"></a>void Blob&lt;Dtype&gt;::Reshape(const BlobShape&amp; shape) {
  CHECK_LE(shape.dim_size(), kMaxBlobAxes);
  vector&lt;int&gt; shape_vec(shape.dim_size());
<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>  for (int i = 0; i &lt; shape.dim_size(); ++i) {
    shape_vec[i] = shape.dim(i);
  }
  Reshape(shape_vec);
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ReshapeLike(const Blob&lt;Dtype&gt;&amp; other) {</b></font>
  Reshape(other.shape());
}
template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height,
    const int width)
  : capacity_(0) {
  Reshape(num, channels, height, width);
}
template &lt;typename Dtype&gt;
Blob&lt;Dtype&gt;::Blob(const vector&lt;int&gt;&amp; shape)
  : capacity_(0) {
  Reshape(shape);
}
template &lt;typename Dtype&gt;
const int* Blob&lt;Dtype&gt;::gpu_shape() const {
  CHECK(shape_data_);
  return (const int*)shape_data_-&gt;gpu_data();
}
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;cpu_data();
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_cpu_data(Dtype* data) {
  CHECK(data);
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_cpu_data(data);
}
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_data() const {
  CHECK(data_);
  return (const Dtype*)data_-&gt;gpu_data();
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::set_gpu_data(Dtype* data) {
  CHECK(data);
  size_t size = count_ * sizeof(Dtype);
  if (data_-&gt;size() != size) {
    data_.reset(new SyncedMemory(size));
    diff_.reset(new SyncedMemory(size));
  }
  data_-&gt;set_gpu_data(data);
}
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;cpu_data();
}
template &lt;typename Dtype&gt;
const Dtype* Blob&lt;Dtype&gt;::gpu_diff() const {
  CHECK(diff_);
  return (const Dtype*)diff_-&gt;gpu_data();
}
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data());
}
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_data() {
  CHECK(data_);
  return static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data());
}
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_cpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data());
}
template &lt;typename Dtype&gt;
Dtype* Blob&lt;Dtype&gt;::mutable_gpu_diff() {
  CHECK(diff_);
  return static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data());
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareData(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  data_ = other.data();
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::ShareDiff(const Blob&amp; other) {
  CHECK_EQ(count_, other.count());
  diff_ = other.diff();
}
template &lt;&gt; void Blob&lt;unsigned int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;&gt; void Blob&lt;int&gt;::Update() { NOT_IMPLEMENTED; }
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::Update() {
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),
        static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),
        static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
#else
    NO_GPU;
#endif
    break;
  default:
    LOG(FATAL) &lt;&lt; "Syncedmem not initialized.";
  }
}
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::asum_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_data() const {
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_data());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_data(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
  }
  return 0;
}
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::asum_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::asum_diff() const {
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    return caffe_cpu_asum(count_, cpu_diff());
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
  {
    Dtype asum;
    caffe_gpu_asum(count_, gpu_diff(), &amp;asum);
    return asum;
  }
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; diff_-&gt;head();
  }
  return 0;
}
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_data() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_data() const {
  Dtype sumsq;
  const Dtype* data;
  if (!data_) { return 0; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = cpu_data();
    sumsq = caffe_cpu_dot(count_, data, data);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = gpu_data();
    caffe_gpu_dot(count_, data, data, &amp;sumsq);
#else
    NO_GPU;
#endif
    break;
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
template &lt;&gt; unsigned int Blob&lt;unsigned int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;&gt; int Blob&lt;int&gt;::sumsq_diff() const {
  NOT_IMPLEMENTED;
  return 0;
}
template &lt;typename Dtype&gt;
Dtype Blob&lt;Dtype&gt;::sumsq_diff() const {
  Dtype sumsq;
  const Dtype* diff;
  if (!diff_) { return 0; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = cpu_diff();
    sumsq = caffe_cpu_dot(count_, diff, diff);
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = gpu_diff();
    caffe_gpu_dot(count_, diff, diff, &amp;sumsq);
    break;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return 0;
  default:
    LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
  }
  return sumsq;
}
template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_data(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}
template &lt;&gt; void Blob&lt;int&gt;::scale_data(int scale_factor) {
  NOT_IMPLEMENTED;
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_data(Dtype scale_factor) {
  Dtype* data;
  if (!data_) { return; }
  switch (data_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    data = mutable_cpu_data();
    caffe_scal(count_, scale_factor, data);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    data = mutable_gpu_data();
    caffe_gpu_scal(count_, scale_factor, data);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; data_-&gt;head();
  }
}
template &lt;&gt; void Blob&lt;unsigned int&gt;::scale_diff(unsigned int scale_factor) {
  NOT_IMPLEMENTED;
}
template &lt;&gt; void Blob&lt;int&gt;::scale_diff(int scale_factor) {
  NOT_IMPLEMENTED;
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::scale_diff(Dtype scale_factor) {
  Dtype* diff;
  if (!diff_) { return; }
  switch (diff_-&gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    diff = mutable_cpu_diff();
    caffe_scal(count_, scale_factor, diff);
    return;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    diff = mutable_gpu_diff();
    caffe_gpu_scal(count_, scale_factor, diff);
    return;
#else
    NO_GPU;
#endif
  case SyncedMemory::UNINITIALIZED:
    return;
  default:
    LOG(FATAL) &lt;&lt; "Unknown SyncedMemory head state: " &lt;&lt; diff_-&gt;head();
  }
}
template &lt;typename Dtype&gt;
bool Blob&lt;Dtype&gt;::ShapeEquals(const BlobProto&amp; other) {
  if (other.has_num() || other.has_channels() ||
      other.has_height() || other.has_width()) {
    return shape_.size() &lt;= 4 &amp;&amp;
           LegacyShape(-4) == other.num() &amp;&amp;
           LegacyShape(-3) == other.channels() &amp;&amp;
           LegacyShape(-2) == other.height() &amp;&amp;
           LegacyShape(-1) == other.width();
  }
  vector&lt;int&gt; other_shape(other.shape().dim_size());
  for (int i = 0; i &lt; other.shape().dim_size(); ++i) {
    other_shape[i] = other.shape().dim(i);
  }
  return shape_ == other_shape;
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::CopyFrom(const Blob&amp; source, bool copy_diff, bool reshape) {
  if (source.count() != count_ || source.shape() != shape_) {
    if (reshape) {
      ReshapeLike(source);
    } else {
      LOG(FATAL) &lt;&lt; "Trying to copy blobs of different sizes.";
    }
  }
  switch (Caffe::mode()) {
  case Caffe::GPU:
    if (copy_diff) {
      caffe_copy(count_, source.gpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_gpu_data()));
    } else {
      caffe_copy(count_, source.gpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));
    }
    break;
  case Caffe::CPU:
    if (copy_diff) {
      caffe_copy(count_, source.cpu_diff(),
          static_cast&lt;Dtype*&gt;(diff_-&gt;mutable_cpu_data()));
    } else {
      caffe_copy(count_, source.cpu_data(),
          static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));
    }
    break;
  default:
    LOG(FATAL) &lt;&lt; "Unknown caffe mode.";
  }
}
template &lt;typename Dtype&gt;
void Blob&lt;Dtype&gt;::FromProto(const BlobProto&amp; proto, bool reshape) {
  if (reshape) {
    vector&lt;int&gt; shape;
    if (proto.has_num() || proto.has_channels() ||
        proto.has_height() || proto.has_width()) {
      shape.resize(4);
<a name="1"></a>      shape[0] = proto.num();
      shape[1] = proto.channels();
      shape[2] = proto.height();
<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      shape[3] = proto.width();
    } else {
      shape.resize(proto.shape().dim_size());
      for (int i = 0; i &lt; proto.shape().dim_size(); ++i) {
        shape[i] = proto.shape().dim(i);
      }
    }</b></font>
    Reshape(shape);
  } else {
    CHECK(ShapeEquals(proto)) &lt;&lt; "shape mismatch (reshape not set)";
  }
  Dtype* data_vec = mutable_cpu_data();
  if (proto.double_data_size() &gt; 0) {
    CHECK_EQ(count_, proto.double_data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.double_data(i);
    }
  } else {
    CHECK_EQ(count_, proto.data_size());
    for (int i = 0; i &lt; count_; ++i) {
      data_vec[i] = proto.data(i);
    }
  }
  if (proto.double_diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.double_diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.double_diff(i);
    }
  } else if (proto.diff_size() &gt; 0) {
    CHECK_EQ(count_, proto.diff_size());
    Dtype* diff_vec = mutable_cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      diff_vec[i] = proto.diff(i);
    }
  }
}
template &lt;&gt;
void Blob&lt;double&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_double_data();
  proto-&gt;clear_double_diff();
  const double* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_double_data(data_vec[i]);
  }
  if (write_diff) {
    const double* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_double_diff(diff_vec[i]);
    }
  }
}
template &lt;&gt;
void Blob&lt;float&gt;::ToProto(BlobProto* proto, bool write_diff) const {
  proto-&gt;clear_shape();
  for (int i = 0; i &lt; shape_.size(); ++i) {
    proto-&gt;mutable_shape()-&gt;add_dim(shape_[i]);
  }
  proto-&gt;clear_data();
  proto-&gt;clear_diff();
  const float* data_vec = cpu_data();
  for (int i = 0; i &lt; count_; ++i) {
    proto-&gt;add_data(data_vec[i]);
  }
  if (write_diff) {
    const float* diff_vec = cpu_diff();
    for (int i = 0; i &lt; count_; ++i) {
      proto-&gt;add_diff(diff_vec[i]);
    }
  }
}
INSTANTIATE_CLASS(Blob);
template class Blob&lt;int&gt;;
template class Blob&lt;unsigned int&gt;;
}  
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
