<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html><head><title>Matches for test_convolution_layer.cpp &amp; test_data_layer.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_convolution_layer.cpp &amp; test_data_layer.cpp
      </h3>
<h1 align="center">
        3.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_convolution_layer.cpp (2.5688074%)<th>test_data_layer.cpp (6.6193852%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(79-81)<td><a href="#" name="0">(186-188)</a><td align="center"><font color="#ff0000">15</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(70-74)<td><a href="#" name="1">(121-126)</a><td align="center"><font color="#dd0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_convolution_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;vector&gt;

#include "gtest/gtest.h"

#include "caffe/blob.hpp"
#include "caffe/common.hpp"
#include "caffe/filler.hpp"
#include "caffe/layers/conv_layer.hpp"

#ifdef USE_CUDNN
#include "caffe/layers/cudnn_conv_layer.hpp"
#endif

#include "caffe/test/test_caffe_main.hpp"
#include "caffe/test/test_gradient_check_util.hpp"

namespace caffe {

// Reference convolution for checking results:
// accumulate through explicit loops over input, output, and filters.
template &lt;typename Dtype&gt;
void caffe_conv(const Blob&lt;Dtype&gt;* in, ConvolutionParameter* conv_param,
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; weights,
    Blob&lt;Dtype&gt;* out) {
  const bool has_depth = (out-&gt;num_axes() == 5);
  if (!has_depth) { CHECK_EQ(4, out-&gt;num_axes()); }
  // Kernel size, stride, and pad
  int kernel_h, kernel_w;
  if (conv_param-&gt;has_kernel_h() || conv_param-&gt;has_kernel_w()) {
    kernel_h = conv_param-&gt;kernel_h();
    kernel_w = conv_param-&gt;kernel_w();
  } else {
    kernel_h = kernel_w = conv_param-&gt;kernel_size(0);
  }
  int pad_h, pad_w;
  if (conv_param-&gt;has_pad_h() || conv_param-&gt;has_pad_w()) {
    pad_h = conv_param-&gt;pad_h();
    pad_w = conv_param-&gt;pad_w();
  } else {
    pad_h = pad_w = conv_param-&gt;pad_size() ? conv_param-&gt;pad(0) : 0;
  }
  int stride_h, stride_w;
  if (conv_param-&gt;has_stride_h() || conv_param-&gt;has_stride_w()) {
    stride_h = conv_param-&gt;stride_h();
    stride_w = conv_param-&gt;stride_w();
  } else {
    stride_h = stride_w = conv_param-&gt;stride_size() ? conv_param-&gt;stride(0) : 1;
  }
  int dilation_h, dilation_w;
  dilation_h = dilation_w = conv_param-&gt;dilation_size() ?
                            conv_param-&gt;dilation(0) : 1;
  int kernel_d, pad_d, stride_d, dilation_d;
  if (has_depth) {
    kernel_d = kernel_h;
    stride_d = stride_h;
    pad_d = pad_h;
    dilation_d = dilation_h;
  } else {
    kernel_d = stride_d = dilation_d = 1;
    pad_d = 0;
  }
  // Groups
  int groups = conv_param-&gt;group();
  int o_g = out-&gt;shape(1) / groups;
  int k_g = in-&gt;shape(1) / groups;
  int o_head, k_head;
<a name="1"></a>  // Convolution
  vector&lt;int&gt; weight_offset(4 + has_depth);
  vector&lt;int&gt; in_offset(4 + has_depth);
<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  vector&lt;int&gt; out_offset(4 + has_depth);
  Dtype* out_data = out-&gt;mutable_cpu_data();
  for (int n = 0; n &lt; out-&gt;shape(0); n++) {
    for (int g = 0; g &lt; groups; g++) {
      o_head = o_g * g;</b></font>
      k_head = k_g * g;
<a name="0"></a>      for (int o = 0; o &lt; o_g; o++) {
        for (int k = 0; k &lt; k_g; k++) {
          for (int z = 0; z &lt; (has_depth ? out-&gt;shape(2) : 1); z++) {
<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>            for (int y = 0; y &lt; out-&gt;shape(2 + has_depth); y++) {
              for (int x = 0; x &lt; out-&gt;shape(3 + has_depth); x++) {
                for (int r = 0; r &lt; kernel_d; r++) {</b></font>
                  for (int p = 0; p &lt; kernel_h; p++) {
                    for (int q = 0; q &lt; kernel_w; q++) {
                      int in_z = z * stride_d - pad_d + r * dilation_d;
                      int in_y = y * stride_h - pad_h + p * dilation_h;
                      int in_x = x * stride_w - pad_w + q * dilation_w;
                      if (in_z &gt;= 0 &amp;&amp; in_z &lt; (has_depth ? in-&gt;shape(2) : 1)
                          &amp;&amp; in_y &gt;= 0 &amp;&amp; in_y &lt; in-&gt;shape(2 + has_depth)
                          &amp;&amp; in_x &gt;= 0 &amp;&amp; in_x &lt; in-&gt;shape(3 + has_depth)) {
                        weight_offset[0] = o + o_head;
                        weight_offset[1] = k;
                        if (has_depth) { weight_offset[2] = r; }
                        weight_offset[2 + has_depth] = p;
                        weight_offset[3 + has_depth] = q;
                        in_offset[0] = n;
                        in_offset[1] = k + k_head;
                        if (has_depth) { in_offset[2] = in_z; }
                        in_offset[2 + has_depth] = in_y;
                        in_offset[3 + has_depth] = in_x;
                        out_offset[0] = n;
                        out_offset[1] = o + o_head;
                        if (has_depth) { out_offset[2] = z; }
                        out_offset[2 + has_depth] = y;
                        out_offset[3 + has_depth] = x;
                        out_data[out-&gt;offset(out_offset)] +=
                            in-&gt;data_at(in_offset)
                            * weights[0]-&gt;data_at(weight_offset);
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  // Bias
  if (conv_param-&gt;bias_term()) {
    const Dtype* bias_data = weights[1]-&gt;cpu_data();
    for (int n = 0; n &lt; out-&gt;shape(0); n++) {
      for (int o = 0; o &lt; out-&gt;shape(1); o++) {
        for (int z = 0; z &lt; (has_depth ? out-&gt;shape(2) : 1); z++) {
          for (int y = 0; y &lt; out-&gt;shape(2 + has_depth); y++) {
            for (int x = 0; x &lt; out-&gt;shape(3 + has_depth); x++) {
              out_offset[0] = n;
              out_offset[1] = o;
              if (has_depth) { out_offset[2] = z; }
              out_offset[2 + has_depth] = y;
              out_offset[3 + has_depth] = x;
              out_data[out-&gt;offset(out_offset)] += bias_data[o];
            }
          }
        }
      }
    }
  }
}

template void caffe_conv(const Blob&lt;float&gt;* in,
    ConvolutionParameter* conv_param,
    const vector&lt;shared_ptr&lt;Blob&lt;float&gt; &gt; &gt;&amp; weights,
    Blob&lt;float&gt;* out);
template void caffe_conv(const Blob&lt;double&gt;* in,
    ConvolutionParameter* conv_param,
    const vector&lt;shared_ptr&lt;Blob&lt;double&gt; &gt; &gt;&amp; weights,
    Blob&lt;double&gt;* out);

template &lt;typename TypeParam&gt;
class ConvolutionLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;

 protected:
  ConvolutionLayerTest()
      : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
        blob_bottom_2_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
        blob_top_(new Blob&lt;Dtype&gt;()),
        blob_top_2_(new Blob&lt;Dtype&gt;()) {}
  virtual void SetUp() {
    // fill the values
    FillerParameter filler_param;
    filler_param.set_value(1.);
    GaussianFiller&lt;Dtype&gt; filler(filler_param);
    filler.Fill(this-&gt;blob_bottom_);
    filler.Fill(this-&gt;blob_bottom_2_);
    blob_bottom_vec_.push_back(blob_bottom_);
    blob_top_vec_.push_back(blob_top_);
  }

  virtual ~ConvolutionLayerTest() {
    delete blob_bottom_;
    delete blob_bottom_2_;
    delete blob_top_;
    delete blob_top_2_;
  }

  virtual Blob&lt;Dtype&gt;* MakeReferenceTop(Blob&lt;Dtype&gt;* top) {
    this-&gt;ref_blob_top_.reset(new Blob&lt;Dtype&gt;());
    this-&gt;ref_blob_top_-&gt;ReshapeLike(*top);
    return this-&gt;ref_blob_top_.get();
  }

  Blob&lt;Dtype&gt;* const blob_bottom_;
  Blob&lt;Dtype&gt;* const blob_bottom_2_;
  Blob&lt;Dtype&gt;* const blob_top_;
  Blob&lt;Dtype&gt;* const blob_top_2_;
  shared_ptr&lt;Blob&lt;Dtype&gt; &gt; ref_blob_top_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
};

TYPED_TEST_CASE(ConvolutionLayerTest, TestDtypesAndDevices);

TYPED_TEST(ConvolutionLayerTest, TestSetup) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(4);
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 4);
  EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 1);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;channels(), 4);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;width(), 1);
  // setting group should not change the shape
  convolution_param-&gt;set_num_output(3);
  convolution_param-&gt;set_group(3);
  layer.reset(new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 3);
  EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 1);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;channels(), 3);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;width(), 1);
}

TYPED_TEST(ConvolutionLayerTest, TestSimpleConvolution) {
  typedef typename TypeParam::Dtype Dtype;
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(4);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("constant");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const Dtype* top_data;
  const Dtype* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
  caffe_conv(this-&gt;blob_bottom_2_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_2_));
  top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, TestDilatedConvolution) {
  typedef typename TypeParam::Dtype Dtype;
  vector&lt;int&gt; bottom_shape;
  bottom_shape.push_back(2);
  bottom_shape.push_back(3);
  bottom_shape.push_back(8);
  bottom_shape.push_back(7);
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
    this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
  }
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_dilation(2);
  convolution_param-&gt;set_num_output(4);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("constant");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const Dtype* top_data;
  const Dtype* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
             this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
  caffe_conv(this-&gt;blob_bottom_2_, convolution_param, layer-&gt;blobs(),
             this-&gt;MakeReferenceTop(this-&gt;blob_top_2_));
  top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, Test0DConvolution) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  const int kNumOutput = 3;
  convolution_param-&gt;set_num_output(kNumOutput);
  convolution_param-&gt;set_axis(3);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  vector&lt;int&gt; top_shape = this-&gt;blob_bottom_-&gt;shape();
  top_shape[3] = kNumOutput;
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  EXPECT_EQ(top_shape, this-&gt;blob_top_-&gt;shape());
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  vector&lt;int&gt; weight_offset(2);
  const Blob&lt;Dtype&gt;* weight = layer-&gt;blobs()[0].get();
  const Blob&lt;Dtype&gt;* bias = layer-&gt;blobs()[1].get();
  const int num = this-&gt;blob_top_-&gt;count(3);
  const int dim = this-&gt;blob_top_-&gt;shape(3);
  const int bottom_dim = this-&gt;blob_bottom_-&gt;shape(3);
  for (int n = 0; n &lt; num; ++n) {
    for (int d = 0; d &lt; dim; ++d) {
      weight_offset[0] = d;
      Dtype value = bias-&gt;cpu_data()[d];
      for (int bottom_d = 0; bottom_d &lt; bottom_dim; ++bottom_d) {
        weight_offset[1] = bottom_d;
        value += weight-&gt;data_at(weight_offset) *
                 this-&gt;blob_bottom_-&gt;cpu_data()[n * bottom_dim + bottom_d];
      }
      EXPECT_NEAR(value, this-&gt;blob_top_-&gt;cpu_data()[n * dim + d], 1e-4);
    }
  }
}

TYPED_TEST(ConvolutionLayerTest, TestSimple3DConvolution) {
  typedef typename TypeParam::Dtype Dtype;
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  vector&lt;int&gt; bottom_shape(5);
  bottom_shape[0] = this-&gt;blob_bottom_vec_[0]-&gt;shape(0);
  bottom_shape[1] = this-&gt;blob_bottom_vec_[0]-&gt;shape(1);
  bottom_shape[2] = 5;
  bottom_shape[3] = this-&gt;blob_bottom_vec_[0]-&gt;shape(2);
  bottom_shape[4] = this-&gt;blob_bottom_vec_[0]-&gt;shape(3);
  FillerParameter filler_param;
  GaussianFiller&lt;Dtype&gt; filler(filler_param);
  for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
    this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
    filler.Fill(this-&gt;blob_bottom_vec_[i]);
  }
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(4);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const Dtype* top_data;
  const Dtype* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
  caffe_conv(this-&gt;blob_bottom_2_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_2_));
  top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, TestDilated3DConvolution) {
  typedef typename TypeParam::Dtype Dtype;
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  vector&lt;int&gt; bottom_shape(5);
  bottom_shape[0] = this-&gt;blob_bottom_vec_[0]-&gt;shape(0);
  bottom_shape[1] = this-&gt;blob_bottom_vec_[0]-&gt;shape(1);
  bottom_shape[2] = 6;
  bottom_shape[3] = 7;
  bottom_shape[4] = 8;
  FillerParameter filler_param;
  GaussianFiller&lt;Dtype&gt; filler(filler_param);
  for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
    this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
    filler.Fill(this-&gt;blob_bottom_vec_[i]);
  }
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_dilation(2);
  convolution_param-&gt;set_num_output(4);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const Dtype* top_data;
  const Dtype* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
             this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
  caffe_conv(this-&gt;blob_bottom_2_, convolution_param, layer-&gt;blobs(),
             this-&gt;MakeReferenceTop(this-&gt;blob_top_2_));
  top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, Test1x1Convolution) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(1);
  convolution_param-&gt;add_stride(1);
  convolution_param-&gt;set_num_output(4);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("constant");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const Dtype* top_data;
  const Dtype* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, TestSimpleConvolutionGroup) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(3);
  convolution_param-&gt;set_group(3);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("constant");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const Dtype* top_data;
  const Dtype* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, TestSobelConvolution) {
  // Test separable convolution by computing the Sobel operator
  // as a single filter then comparing the result
  // as the convolution of two rectangular filters.
  typedef typename TypeParam::Dtype Dtype;
  // Fill bottoms with identical Gaussian noise.
  shared_ptr&lt;GaussianFiller&lt;Dtype&gt; &gt; filler;
  FillerParameter filler_param;
  filler_param.set_value(1.);
  filler.reset(new GaussianFiller&lt;Dtype&gt;(filler_param));
  filler-&gt;Fill(this-&gt;blob_bottom_);
  this-&gt;blob_bottom_2_-&gt;CopyFrom(*this-&gt;blob_bottom_);
  // Compute Sobel G_x operator as 3 x 3 convolution.
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(1);
  convolution_param-&gt;set_bias_term(false);
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
      new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;blobs().resize(1);
  layer-&gt;blobs()[0].reset(new Blob&lt;Dtype&gt;(1, 3, 3, 3));
  Dtype* weights = layer-&gt;blobs()[0]-&gt;mutable_cpu_data();
  for (int c = 0; c &lt; 3; ++c) {
    int i = c * 9;  // 3 x 3 filter
    weights[i +  0] = -1;
    weights[i +  1] =  0;
    weights[i +  2] =  1;
    weights[i +  3] = -2;
    weights[i +  4] =  0;
    weights[i +  5] =  2;
    weights[i +  6] = -1;
    weights[i +  7] =  0;
    weights[i +  8] =  1;
  }
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Compute Sobel G_x operator as separable 3 x 1 and 1 x 3 convolutions.
  // (1) the [1 2 1] column filter
  vector&lt;Blob&lt;Dtype&gt;*&gt; sep_blob_bottom_vec;
  vector&lt;Blob&lt;Dtype&gt;*&gt; sep_blob_top_vec;
  shared_ptr&lt;Blob&lt;Dtype&gt; &gt; blob_sep(new Blob&lt;Dtype&gt;());
  sep_blob_bottom_vec.push_back(this-&gt;blob_bottom_2_);
  sep_blob_top_vec.push_back(this-&gt;blob_top_2_);
  convolution_param-&gt;clear_kernel_size();
  convolution_param-&gt;clear_stride();
  convolution_param-&gt;set_kernel_h(3);
  convolution_param-&gt;set_kernel_w(1);
  convolution_param-&gt;set_stride_h(2);
  convolution_param-&gt;set_stride_w(1);
  convolution_param-&gt;set_num_output(1);
  convolution_param-&gt;set_bias_term(false);
  layer.reset(new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;blobs().resize(1);
  layer-&gt;blobs()[0].reset(new Blob&lt;Dtype&gt;(1, 3, 3, 1));
  Dtype* weights_1 = layer-&gt;blobs()[0]-&gt;mutable_cpu_data();
  for (int c = 0; c &lt; 3; ++c) {
    int i = c * 3;  // 3 x 1 filter
    weights_1[i +  0] = 1;
    weights_1[i +  1] = 2;
    weights_1[i +  2] = 1;
  }
  layer-&gt;SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
  layer-&gt;Forward(sep_blob_bottom_vec, sep_blob_top_vec);
  // (2) the [-1 0 1] row filter
  blob_sep-&gt;CopyFrom(*this-&gt;blob_top_2_, false, true);
  sep_blob_bottom_vec.clear();
  sep_blob_bottom_vec.push_back(blob_sep.get());
  convolution_param-&gt;set_kernel_h(1);
  convolution_param-&gt;set_kernel_w(3);
  convolution_param-&gt;set_stride_h(1);
  convolution_param-&gt;set_stride_w(2);
  convolution_param-&gt;set_num_output(1);
  convolution_param-&gt;set_bias_term(false);
  layer.reset(new ConvolutionLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;blobs().resize(1);
  layer-&gt;blobs()[0].reset(new Blob&lt;Dtype&gt;(1, 1, 1, 3));
  Dtype* weights_2 = layer-&gt;blobs()[0]-&gt;mutable_cpu_data();
  weights_2[0] = -1;
  weights_2[1] =  0;
  weights_2[2] =  1;
  layer-&gt;SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
  layer-&gt;Forward(sep_blob_bottom_vec, sep_blob_top_vec);
  // Test equivalence of full and separable filters.
  const Dtype* top_data = this-&gt;blob_top_-&gt;cpu_data();
  const Dtype* sep_top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], sep_top_data[i], 1e-4);
  }
}

TYPED_TEST(ConvolutionLayerTest, TestNDAgainst2D) {
  typedef typename TypeParam::Dtype Dtype;
  const int kernel_h = 11;
  const int kernel_w = 13;
  vector&lt;int&gt; bottom_shape(4);
  bottom_shape[0] = 15;
  bottom_shape[1] = 18;
  bottom_shape[2] = kernel_h * 2;
  bottom_shape[3] = kernel_w * 2;
  FillerParameter filler_param;
  GaussianFiller&lt;Dtype&gt; filler(filler_param);
  for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
    this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
    filler.Fill(this-&gt;blob_bottom_vec_[i]);
  }
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;set_num_output(12);
  convolution_param-&gt;set_bias_term(false);
  convolution_param-&gt;set_group(6);
  convolution_param-&gt;set_kernel_h(kernel_h);
  convolution_param-&gt;set_kernel_w(kernel_w);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  Blob&lt;Dtype&gt; weights;
  Blob&lt;Dtype&gt; top_diff;
  // Shape and fill weights and top_diff.
  bool copy_diff;
  bool reshape;
  {
    ConvolutionLayer&lt;Dtype&gt; layer(layer_param);
    layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    top_diff.ReshapeLike(*this-&gt;blob_top_);
    filler.Fill(&amp;top_diff);
    ASSERT_EQ(1, layer.blobs().size());
    copy_diff = false; reshape = true;
    weights.CopyFrom(*layer.blobs()[0], copy_diff, reshape);
  }
  vector&lt;bool&gt; propagate_down(1, true);
  Blob&lt;Dtype&gt; result_2d;
  Blob&lt;Dtype&gt; backward_result_2d;
  Blob&lt;Dtype&gt; backward_weight_result_2d;
  // Test with 2D im2col
  {
    caffe_set(this-&gt;blob_top_-&gt;count(), Dtype(0),
              this-&gt;blob_top_-&gt;mutable_cpu_data());
    caffe_set(this-&gt;blob_bottom_-&gt;count(), Dtype(0),
              this-&gt;blob_bottom_-&gt;mutable_cpu_diff());
    caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
    // Do SetUp and Forward; save Forward result in result_2d.
    convolution_param-&gt;set_force_nd_im2col(false);
    ConvolutionLayer&lt;Dtype&gt; layer_2d(layer_param);
    layer_2d.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    ASSERT_EQ(1, layer_2d.blobs().size());
    copy_diff = false; reshape = false;
    layer_2d.blobs()[0]-&gt;CopyFrom(weights, copy_diff, reshape);
    layer_2d.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    copy_diff = false; reshape = true;
    result_2d.CopyFrom(*this-&gt;blob_top_, copy_diff, reshape);
    // Copy pre-generated top diff into actual top diff;
    // do Backward and save result in backward_result_2d.
    ASSERT_EQ(this-&gt;blob_top_-&gt;shape(), top_diff.shape());
    caffe_copy(top_diff.count(), top_diff.cpu_data(),
               this-&gt;blob_top_-&gt;mutable_cpu_diff());
    layer_2d.Backward(this-&gt;blob_top_vec_, propagate_down,
                      this-&gt;blob_bottom_vec_);
    copy_diff = true; reshape = true;
    backward_result_2d.CopyFrom(*this-&gt;blob_bottom_, copy_diff, reshape);
    backward_weight_result_2d.CopyFrom(weights, copy_diff, reshape);
  }
  Blob&lt;Dtype&gt; result_nd;
  Blob&lt;Dtype&gt; backward_result_nd;
  Blob&lt;Dtype&gt; backward_weight_result_nd;
  // Test with ND im2col
  {
    caffe_set(this-&gt;blob_top_-&gt;count(), Dtype(0),
              this-&gt;blob_top_-&gt;mutable_cpu_data());
    caffe_set(this-&gt;blob_bottom_-&gt;count(), Dtype(0),
              this-&gt;blob_bottom_-&gt;mutable_cpu_diff());
    caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
    // Do SetUp and Forward; save Forward result in result_nd.
    convolution_param-&gt;set_force_nd_im2col(true);
    ConvolutionLayer&lt;Dtype&gt; layer_nd(layer_param);
    layer_nd.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    ASSERT_EQ(1, layer_nd.blobs().size());
    copy_diff = false; reshape = false;
    layer_nd.blobs()[0]-&gt;CopyFrom(weights, copy_diff, reshape);
    layer_nd.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    copy_diff = false; reshape = true;
    result_nd.CopyFrom(*this-&gt;blob_top_, copy_diff, reshape);
    // Copy pre-generated top diff into actual top diff;
    // do Backward and save result in backward_result_nd.
    ASSERT_EQ(this-&gt;blob_top_-&gt;shape(), top_diff.shape());
    caffe_copy(top_diff.count(), top_diff.cpu_data(),
               this-&gt;blob_top_-&gt;mutable_cpu_diff());
    layer_nd.Backward(this-&gt;blob_top_vec_, propagate_down,
                      this-&gt;blob_bottom_vec_);
    copy_diff = true; reshape = true;
    backward_result_nd.CopyFrom(*this-&gt;blob_bottom_, copy_diff, reshape);
    backward_weight_result_nd.CopyFrom(weights, copy_diff, reshape);
  }
  ASSERT_EQ(result_nd.count(), result_2d.count());
  for (int i = 0; i &lt; result_2d.count(); ++i)  {
    EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
  }
  ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
  for (int i = 0; i &lt; backward_result_2d.count(); ++i) {
    EXPECT_FLOAT_EQ(backward_result_2d.cpu_diff()[i],
              backward_result_nd.cpu_diff()[i]);
  }
  ASSERT_EQ(backward_weight_result_nd.count(),
            backward_weight_result_2d.count());
  for (int i = 0; i &lt; backward_weight_result_2d.count(); ++i) {
    EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
              backward_weight_result_nd.cpu_diff()[i]);
  }
}

TYPED_TEST(ConvolutionLayerTest, TestGradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(2);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  ConvolutionLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

TYPED_TEST(ConvolutionLayerTest, TestDilatedGradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  vector&lt;int&gt; bottom_shape;
  bottom_shape.push_back(2);
  bottom_shape.push_back(3);
  bottom_shape.push_back(5);
  bottom_shape.push_back(6);
  for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
    this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
  }
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_dilation(2);
  convolution_param-&gt;set_num_output(2);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  ConvolutionLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
                                  this-&gt;blob_top_vec_);
}

TYPED_TEST(ConvolutionLayerTest, TestGradient3D) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  vector&lt;int&gt; bottom_shape(5);
  bottom_shape[0] = this-&gt;blob_bottom_vec_[0]-&gt;shape(0);
  bottom_shape[1] = this-&gt;blob_bottom_vec_[0]-&gt;shape(1);
  bottom_shape[2] = 5;
  bottom_shape[3] = this-&gt;blob_bottom_vec_[0]-&gt;shape(2);
  bottom_shape[4] = this-&gt;blob_bottom_vec_[0]-&gt;shape(3);
  FillerParameter filler_param;
  GaussianFiller&lt;Dtype&gt; filler(filler_param);
  for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
    this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
    filler.Fill(this-&gt;blob_bottom_vec_[i]);
  }
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(2);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  ConvolutionLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

TYPED_TEST(ConvolutionLayerTest, Test1x1Gradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  convolution_param-&gt;add_kernel_size(1);
  convolution_param-&gt;add_stride(1);
  convolution_param-&gt;set_num_output(2);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  ConvolutionLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

TYPED_TEST(ConvolutionLayerTest, TestGradientGroup) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(3);
  convolution_param-&gt;set_group(3);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  ConvolutionLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

#ifdef USE_CUDNN

template &lt;typename Dtype&gt;
class CuDNNConvolutionLayerTest : public GPUDeviceTest&lt;Dtype&gt; {
 protected:
  CuDNNConvolutionLayerTest()
      : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
        blob_bottom_2_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
        blob_top_(new Blob&lt;Dtype&gt;()),
        blob_top_2_(new Blob&lt;Dtype&gt;()) {}
  virtual void SetUp() {
    // fill the values
    FillerParameter filler_param;
    filler_param.set_value(1.);
    GaussianFiller&lt;Dtype&gt; filler(filler_param);
    filler.Fill(this-&gt;blob_bottom_);
    filler.Fill(this-&gt;blob_bottom_2_);
    blob_bottom_vec_.push_back(blob_bottom_);
    blob_top_vec_.push_back(blob_top_);
  }

  virtual ~CuDNNConvolutionLayerTest() {
    delete blob_bottom_;
    delete blob_bottom_2_;
    delete blob_top_;
    delete blob_top_2_;
  }

  virtual Blob&lt;Dtype&gt;* MakeReferenceTop(Blob&lt;Dtype&gt;* top) {
    this-&gt;ref_blob_top_.reset(new Blob&lt;Dtype&gt;());
    this-&gt;ref_blob_top_-&gt;ReshapeLike(*top);
    return this-&gt;ref_blob_top_.get();
  }

  Blob&lt;Dtype&gt;* const blob_bottom_;
  Blob&lt;Dtype&gt;* const blob_bottom_2_;
  Blob&lt;Dtype&gt;* const blob_top_;
  Blob&lt;Dtype&gt;* const blob_top_2_;
  shared_ptr&lt;Blob&lt;Dtype&gt; &gt; ref_blob_top_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
};

TYPED_TEST_CASE(CuDNNConvolutionLayerTest, TestDtypes);

TYPED_TEST(CuDNNConvolutionLayerTest, TestSetupCuDNN) {
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(4);
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  shared_ptr&lt;Layer&lt;TypeParam&gt; &gt; layer(
      new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 4);
  EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 1);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;channels(), 4);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;width(), 1);
  // setting group should not change the shape
  convolution_param-&gt;set_num_output(3);
  convolution_param-&gt;set_group(3);
  layer.reset(new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 3);
  EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 1);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;channels(), 3);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;height(), 2);
  EXPECT_EQ(this-&gt;blob_top_2_-&gt;width(), 1);
}

TYPED_TEST(CuDNNConvolutionLayerTest, TestSimpleConvolutionCuDNN) {
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(4);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("constant");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
  shared_ptr&lt;Layer&lt;TypeParam&gt; &gt; layer(
      new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const TypeParam* top_data;
  const TypeParam* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
  caffe_conv(this-&gt;blob_bottom_2_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_2_));
  top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(CuDNNConvolutionLayerTest, TestSimpleConvolutionGroupCuDNN) {
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(3);
  convolution_param-&gt;set_group(3);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("constant");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
  shared_ptr&lt;Layer&lt;TypeParam&gt; &gt; layer(
      new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Check against reference convolution.
  const TypeParam* top_data;
  const TypeParam* ref_top_data;
  caffe_conv(this-&gt;blob_bottom_, convolution_param, layer-&gt;blobs(),
      this-&gt;MakeReferenceTop(this-&gt;blob_top_));
  top_data = this-&gt;blob_top_-&gt;cpu_data();
  ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
  }
}

TYPED_TEST(CuDNNConvolutionLayerTest, TestSobelConvolutionCuDNN) {
  // Test separable convolution by computing the Sobel operator
  // as a single filter then comparing the result
  // as the convolution of two rectangular filters.

  // Fill bottoms with identical Gaussian noise.
  shared_ptr&lt;GaussianFiller&lt;TypeParam&gt; &gt; filler;
  FillerParameter filler_param;
  filler_param.set_value(1.);
  filler.reset(new GaussianFiller&lt;TypeParam&gt;(filler_param));
  filler-&gt;Fill(this-&gt;blob_bottom_);
  this-&gt;blob_bottom_2_-&gt;CopyFrom(*this-&gt;blob_bottom_);
  // Compute Sobel G_x operator as 3 x 3 convolution.
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(1);
  convolution_param-&gt;set_bias_term(false);
  shared_ptr&lt;Layer&lt;TypeParam&gt; &gt; layer(
      new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;blobs().resize(1);
  layer-&gt;blobs()[0].reset(new Blob&lt;TypeParam&gt;(1, 3, 3, 3));
  TypeParam* weights = layer-&gt;blobs()[0]-&gt;mutable_cpu_data();
  for (int c = 0; c &lt; 3; ++c) {
    int i = c * 9;  // 3 x 3 filter
    weights[i +  0] = -1;
    weights[i +  1] =  0;
    weights[i +  2] =  1;
    weights[i +  3] = -2;
    weights[i +  4] =  0;
    weights[i +  5] =  2;
    weights[i +  6] = -1;
    weights[i +  7] =  0;
    weights[i +  8] =  1;
  }
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Compute Sobel G_x operator as separable 3 x 1 and 1 x 3 convolutions.
  // (1) the [1 2 1] column filter
  vector&lt;Blob&lt;TypeParam&gt;*&gt; sep_blob_bottom_vec;
  vector&lt;Blob&lt;TypeParam&gt;*&gt; sep_blob_top_vec;
  shared_ptr&lt;Blob&lt;TypeParam&gt; &gt; blob_sep(new Blob&lt;TypeParam&gt;());
  sep_blob_bottom_vec.push_back(this-&gt;blob_bottom_2_);
  sep_blob_top_vec.push_back(this-&gt;blob_top_2_);
  convolution_param-&gt;clear_kernel_size();
  convolution_param-&gt;clear_stride();
  convolution_param-&gt;set_kernel_h(3);
  convolution_param-&gt;set_kernel_w(1);
  convolution_param-&gt;set_stride_h(2);
  convolution_param-&gt;set_stride_w(1);
  convolution_param-&gt;set_num_output(1);
  convolution_param-&gt;set_bias_term(false);
  layer.reset(new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;blobs().resize(1);
  layer-&gt;blobs()[0].reset(new Blob&lt;TypeParam&gt;(1, 3, 3, 1));
  TypeParam* weights_1 = layer-&gt;blobs()[0]-&gt;mutable_cpu_data();
  for (int c = 0; c &lt; 3; ++c) {
    int i = c * 3;  // 3 x 1 filter
    weights_1[i +  0] = 1;
    weights_1[i +  1] = 2;
    weights_1[i +  2] = 1;
  }
  layer-&gt;SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
  layer-&gt;Forward(sep_blob_bottom_vec, sep_blob_top_vec);
  // (2) the [-1 0 1] row filter
  blob_sep-&gt;CopyFrom(*this-&gt;blob_top_2_, false, true);
  sep_blob_bottom_vec.clear();
  sep_blob_bottom_vec.push_back(blob_sep.get());
  convolution_param-&gt;set_kernel_h(1);
  convolution_param-&gt;set_kernel_w(3);
  convolution_param-&gt;set_stride_h(1);
  convolution_param-&gt;set_stride_w(2);
  convolution_param-&gt;set_num_output(1);
  convolution_param-&gt;set_bias_term(false);
  layer.reset(new CuDNNConvolutionLayer&lt;TypeParam&gt;(layer_param));
  layer-&gt;blobs().resize(1);
  layer-&gt;blobs()[0].reset(new Blob&lt;TypeParam&gt;(1, 1, 1, 3));
  TypeParam* weights_2 = layer-&gt;blobs()[0]-&gt;mutable_cpu_data();
  weights_2[0] = -1;
  weights_2[1] =  0;
  weights_2[2] =  1;
  layer-&gt;SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
  layer-&gt;Forward(sep_blob_bottom_vec, sep_blob_top_vec);
  // Test equivalence of full and separable filters.
  const TypeParam* top_data = this-&gt;blob_top_-&gt;cpu_data();
  const TypeParam* sep_top_data = this-&gt;blob_top_2_-&gt;cpu_data();
  for (int i = 0; i &lt; this-&gt;blob_top_-&gt;count(); ++i) {
    EXPECT_NEAR(top_data[i], sep_top_data[i], 1e-4);
  }
}

TYPED_TEST(CuDNNConvolutionLayerTest, TestGradientCuDNN) {
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
  this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(2);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  CuDNNConvolutionLayer&lt;TypeParam&gt; layer(layer_param);
  GradientChecker&lt;TypeParam&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

TYPED_TEST(CuDNNConvolutionLayerTest, TestGradientGroupCuDNN) {
  LayerParameter layer_param;
  ConvolutionParameter* convolution_param =
      layer_param.mutable_convolution_param();
  convolution_param-&gt;add_kernel_size(3);
  convolution_param-&gt;add_stride(2);
  convolution_param-&gt;set_num_output(3);
  convolution_param-&gt;set_group(3);
  convolution_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
  convolution_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
  CuDNNConvolutionLayer&lt;TypeParam&gt; layer(layer_param);
  GradientChecker&lt;TypeParam&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

#endif

}  // namespace caffe
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_data_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#ifdef USE_OPENCV
#include &lt;string&gt;
#include &lt;vector&gt;

#include "boost/scoped_ptr.hpp"
#include "gtest/gtest.h"

#include "caffe/blob.hpp"
#include "caffe/common.hpp"
#include "caffe/filler.hpp"
#include "caffe/layers/data_layer.hpp"
#include "caffe/proto/caffe.pb.h"
#include "caffe/util/db.hpp"
#include "caffe/util/io.hpp"

#include "caffe/test/test_caffe_main.hpp"

namespace caffe {

using boost::scoped_ptr;

template &lt;typename TypeParam&gt;
class DataLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;

 protected:
  DataLayerTest()
      : backend_(DataParameter_DB_LEVELDB),
        blob_top_data_(new Blob&lt;Dtype&gt;()),
        blob_top_label_(new Blob&lt;Dtype&gt;()),
        seed_(1701) {}
  virtual void SetUp() {
    filename_.reset(new string());
    MakeTempDir(filename_.get());
    *filename_ += "/db";
    blob_top_vec_.push_back(blob_top_data_);
    blob_top_vec_.push_back(blob_top_label_);
  }

  // Fill the DB with data: if unique_pixels, each pixel is unique but
  // all images are the same; else each image is unique but all pixels within
  // an image are the same.
  void Fill(const bool unique_pixels, DataParameter_DB backend) {
    backend_ = backend;
    LOG(INFO) &lt;&lt; "Using temporary dataset " &lt;&lt; *filename_;
    scoped_ptr&lt;db::DB&gt; db(db::GetDB(backend));
    db-&gt;Open(*filename_, db::NEW);
    scoped_ptr&lt;db::Transaction&gt; txn(db-&gt;NewTransaction());
    for (int i = 0; i &lt; 5; ++i) {
      Datum datum;
      datum.set_label(i);
      datum.set_channels(2);
      datum.set_height(3);
      datum.set_width(4);
      std::string* data = datum.mutable_data();
      for (int j = 0; j &lt; 24; ++j) {
        int datum = unique_pixels ? j : i;
        data-&gt;push_back(static_cast&lt;uint8_t&gt;(datum));
      }
      stringstream ss;
      ss &lt;&lt; i;
      string out;
      CHECK(datum.SerializeToString(&amp;out));
      txn-&gt;Put(ss.str(), out);
    }
    txn-&gt;Commit();
    db-&gt;Close();
  }

  void TestRead() {
    const Dtype scale = 3;
    LayerParameter param;
    param.set_phase(TRAIN);
    DataParameter* data_param = param.mutable_data_param();
    data_param-&gt;set_batch_size(5);
    data_param-&gt;set_source(filename_-&gt;c_str());
    data_param-&gt;set_backend(backend_);

    TransformationParameter* transform_param =
        param.mutable_transform_param();
    transform_param-&gt;set_scale(scale);

    DataLayer&lt;Dtype&gt; layer(param);
    layer.SetUp(blob_bottom_vec_, blob_top_vec_);
    EXPECT_EQ(blob_top_data_-&gt;num(), 5);
    EXPECT_EQ(blob_top_data_-&gt;channels(), 2);
    EXPECT_EQ(blob_top_data_-&gt;height(), 3);
    EXPECT_EQ(blob_top_data_-&gt;width(), 4);
    EXPECT_EQ(blob_top_label_-&gt;num(), 5);
    EXPECT_EQ(blob_top_label_-&gt;channels(), 1);
    EXPECT_EQ(blob_top_label_-&gt;height(), 1);
    EXPECT_EQ(blob_top_label_-&gt;width(), 1);

    for (int iter = 0; iter &lt; 100; ++iter) {
      layer.Forward(blob_bottom_vec_, blob_top_vec_);
      for (int i = 0; i &lt; 5; ++i) {
        EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
      }
      for (int i = 0; i &lt; 5; ++i) {
        for (int j = 0; j &lt; 24; ++j) {
          EXPECT_EQ(scale * i, blob_top_data_-&gt;cpu_data()[i * 24 + j])
              &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " i " &lt;&lt; i &lt;&lt; " j " &lt;&lt; j;
        }
      }
    }
  }

  void TestSkip() {
    LayerParameter param;
    param.set_phase(TRAIN);
    DataParameter* data_param = param.mutable_data_param();
    int batch_size = 5;
    data_param-&gt;set_batch_size(batch_size);
    data_param-&gt;set_source(filename_-&gt;c_str());
    data_param-&gt;set_backend(backend_);
    Caffe::set_solver_count(8);
    for (int dev = 0; dev &lt; Caffe::solver_count(); ++dev) {
<a name="1"></a>      Caffe::set_solver_rank(dev);
      DataLayer&lt;Dtype&gt; layer(param);
      layer.SetUp(blob_bottom_vec_, blob_top_vec_);
<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      int label = dev;
      for (int iter = 0; iter &lt; 10; ++iter) {
        layer.Forward(blob_bottom_vec_, blob_top_vec_);
        for (int i = 0; i &lt; batch_size; ++i) {
          EXPECT_EQ(label % batch_size, blob_top_label_-&gt;cpu_data()[i]);
          label += Caffe::solver_count();</b></font>
        }
      }
    }
    Caffe::set_solver_count(1);
    Caffe::set_solver_rank(0);
  }

  void TestReshape(DataParameter_DB backend) {
    const int num_inputs = 5;
    // Save data of varying shapes.
    LOG(INFO) &lt;&lt; "Using temporary dataset " &lt;&lt; *filename_;
    scoped_ptr&lt;db::DB&gt; db(db::GetDB(backend));
    db-&gt;Open(*filename_, db::NEW);
    scoped_ptr&lt;db::Transaction&gt; txn(db-&gt;NewTransaction());
    for (int i = 0; i &lt; num_inputs; ++i) {
      Datum datum;
      datum.set_label(i);
      datum.set_channels(2);
      datum.set_height(i % 2 + 1);
      datum.set_width(i % 4 + 1);
      std::string* data = datum.mutable_data();
      const int data_size = datum.channels() * datum.height() * datum.width();
      for (int j = 0; j &lt; data_size; ++j) {
        data-&gt;push_back(static_cast&lt;uint8_t&gt;(j));
      }
      stringstream ss;
      ss &lt;&lt; i;
      string out;
      CHECK(datum.SerializeToString(&amp;out));
      txn-&gt;Put(ss.str(), out);
    }
    txn-&gt;Commit();
    db-&gt;Close();

    // Load and check data of various shapes.
    LayerParameter param;
    param.set_phase(TEST);
    DataParameter* data_param = param.mutable_data_param();
    data_param-&gt;set_batch_size(1);
    data_param-&gt;set_source(filename_-&gt;c_str());
    data_param-&gt;set_backend(backend);

    DataLayer&lt;Dtype&gt; layer(param);
    layer.SetUp(blob_bottom_vec_, blob_top_vec_);
    EXPECT_EQ(blob_top_data_-&gt;num(), 1);
    EXPECT_EQ(blob_top_data_-&gt;channels(), 2);
    EXPECT_EQ(blob_top_label_-&gt;num(), 1);
    EXPECT_EQ(blob_top_label_-&gt;channels(), 1);
    EXPECT_EQ(blob_top_label_-&gt;height(), 1);
    EXPECT_EQ(blob_top_label_-&gt;width(), 1);

    for (int iter = 0; iter &lt; num_inputs; ++iter) {
      layer.Forward(blob_bottom_vec_, blob_top_vec_);
      EXPECT_EQ(blob_top_data_-&gt;height(), iter % 2 + 1);
      EXPECT_EQ(blob_top_data_-&gt;width(), iter % 4 + 1);
      EXPECT_EQ(iter, blob_top_label_-&gt;cpu_data()[0]);
<a name="0"></a>      const int channels = blob_top_data_-&gt;channels();
      const int height = blob_top_data_-&gt;height();
      const int width = blob_top_data_-&gt;width();
<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      for (int c = 0; c &lt; channels; ++c) {
        for (int h = 0; h &lt; height; ++h) {
          for (int w = 0; w &lt; width; ++w) {</b></font>
            const int idx = (c * height + h) * width + w;
            EXPECT_EQ(idx, static_cast&lt;int&gt;(blob_top_data_-&gt;cpu_data()[idx]))
                &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " c " &lt;&lt; c
                &lt;&lt; " h " &lt;&lt; h &lt;&lt; " w " &lt;&lt; w;
          }
        }
      }
    }
  }

  void TestReadCrop(Phase phase) {
    const Dtype scale = 3;
    LayerParameter param;
    param.set_phase(phase);
    Caffe::set_random_seed(1701);

    DataParameter* data_param = param.mutable_data_param();
    data_param-&gt;set_batch_size(5);
    data_param-&gt;set_source(filename_-&gt;c_str());
    data_param-&gt;set_backend(backend_);

    TransformationParameter* transform_param =
        param.mutable_transform_param();
    transform_param-&gt;set_scale(scale);
    transform_param-&gt;set_crop_size(1);

    DataLayer&lt;Dtype&gt; layer(param);
    layer.SetUp(blob_bottom_vec_, blob_top_vec_);
    EXPECT_EQ(blob_top_data_-&gt;num(), 5);
    EXPECT_EQ(blob_top_data_-&gt;channels(), 2);
    EXPECT_EQ(blob_top_data_-&gt;height(), 1);
    EXPECT_EQ(blob_top_data_-&gt;width(), 1);
    EXPECT_EQ(blob_top_label_-&gt;num(), 5);
    EXPECT_EQ(blob_top_label_-&gt;channels(), 1);
    EXPECT_EQ(blob_top_label_-&gt;height(), 1);
    EXPECT_EQ(blob_top_label_-&gt;width(), 1);

    for (int iter = 0; iter &lt; 2; ++iter) {
      layer.Forward(blob_bottom_vec_, blob_top_vec_);
      for (int i = 0; i &lt; 5; ++i) {
        EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
      }
      int num_with_center_value = 0;
      for (int i = 0; i &lt; 5; ++i) {
        for (int j = 0; j &lt; 2; ++j) {
          const Dtype center_value = scale * (j ? 17 : 5);
          num_with_center_value +=
              (center_value == blob_top_data_-&gt;cpu_data()[i * 2 + j]);
          // At TEST time, check that we always get center value.
          if (phase == caffe::TEST) {
            EXPECT_EQ(center_value, this-&gt;blob_top_data_-&gt;cpu_data()[i * 2 + j])
                &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " i " &lt;&lt; i &lt;&lt; " j " &lt;&lt; j;
          }
        }
      }
      // At TRAIN time, check that we did not get the center crop all 10 times.
      // (This check fails with probability 1-1/12^10 in a correct
      // implementation, so we call set_random_seed.)
      if (phase == caffe::TRAIN) {
        EXPECT_LT(num_with_center_value, 10);
      }
    }
  }

  void TestReadCropTrainSequenceSeeded() {
    LayerParameter param;
    param.set_phase(TRAIN);
    DataParameter* data_param = param.mutable_data_param();
    data_param-&gt;set_batch_size(5);
    data_param-&gt;set_source(filename_-&gt;c_str());
    data_param-&gt;set_backend(backend_);

    TransformationParameter* transform_param =
        param.mutable_transform_param();
    transform_param-&gt;set_crop_size(1);
    transform_param-&gt;set_mirror(true);

    // Get crop sequence with Caffe seed 1701.
    Caffe::set_random_seed(seed_);
    vector&lt;vector&lt;Dtype&gt; &gt; crop_sequence;
    {
      DataLayer&lt;Dtype&gt; layer1(param);
      layer1.SetUp(blob_bottom_vec_, blob_top_vec_);
      for (int iter = 0; iter &lt; 2; ++iter) {
        layer1.Forward(blob_bottom_vec_, blob_top_vec_);
        for (int i = 0; i &lt; 5; ++i) {
          EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
        }
        vector&lt;Dtype&gt; iter_crop_sequence;
        for (int i = 0; i &lt; 5; ++i) {
          for (int j = 0; j &lt; 2; ++j) {
            iter_crop_sequence.push_back(
                blob_top_data_-&gt;cpu_data()[i * 2 + j]);
          }
        }
        crop_sequence.push_back(iter_crop_sequence);
      }
    }  // destroy 1st data layer and unlock the db

    // Get crop sequence after reseeding Caffe with 1701.
    // Check that the sequence is the same as the original.
    Caffe::set_random_seed(seed_);
    DataLayer&lt;Dtype&gt; layer2(param);
    layer2.SetUp(blob_bottom_vec_, blob_top_vec_);
    for (int iter = 0; iter &lt; 2; ++iter) {
      layer2.Forward(blob_bottom_vec_, blob_top_vec_);
      for (int i = 0; i &lt; 5; ++i) {
        EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
      }
      for (int i = 0; i &lt; 5; ++i) {
        for (int j = 0; j &lt; 2; ++j) {
          EXPECT_EQ(crop_sequence[iter][i * 2 + j],
                    blob_top_data_-&gt;cpu_data()[i * 2 + j])
              &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " i " &lt;&lt; i &lt;&lt; " j " &lt;&lt; j;
        }
      }
    }
  }

  void TestReadCropTrainSequenceUnseeded() {
    LayerParameter param;
    param.set_phase(TRAIN);
    DataParameter* data_param = param.mutable_data_param();
    data_param-&gt;set_batch_size(5);
    data_param-&gt;set_source(filename_-&gt;c_str());
    data_param-&gt;set_backend(backend_);

    TransformationParameter* transform_param =
        param.mutable_transform_param();
    transform_param-&gt;set_crop_size(1);
    transform_param-&gt;set_mirror(true);

    // Get crop sequence with Caffe seed 1701, srand seed 1701.
    Caffe::set_random_seed(seed_);
    srand(seed_);
    vector&lt;vector&lt;Dtype&gt; &gt; crop_sequence;
    {
      DataLayer&lt;Dtype&gt; layer1(param);
      layer1.SetUp(blob_bottom_vec_, blob_top_vec_);
      for (int iter = 0; iter &lt; 2; ++iter) {
        layer1.Forward(blob_bottom_vec_, blob_top_vec_);
        for (int i = 0; i &lt; 5; ++i) {
          EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
        }
        vector&lt;Dtype&gt; iter_crop_sequence;
        for (int i = 0; i &lt; 5; ++i) {
          for (int j = 0; j &lt; 2; ++j) {
            iter_crop_sequence.push_back(
                blob_top_data_-&gt;cpu_data()[i * 2 + j]);
          }
        }
        crop_sequence.push_back(iter_crop_sequence);
      }
    }  // destroy 1st data layer and unlock the db

    // Get crop sequence continuing from previous Caffe RNG state; reseed
    // srand with 1701. Check that the sequence differs from the original.
    srand(seed_);
    DataLayer&lt;Dtype&gt; layer2(param);
    layer2.SetUp(blob_bottom_vec_, blob_top_vec_);
    for (int iter = 0; iter &lt; 2; ++iter) {
      layer2.Forward(blob_bottom_vec_, blob_top_vec_);
      for (int i = 0; i &lt; 5; ++i) {
        EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
      }
      int num_sequence_matches = 0;
      for (int i = 0; i &lt; 5; ++i) {
        for (int j = 0; j &lt; 2; ++j) {
          num_sequence_matches += (crop_sequence[iter][i * 2 + j] ==
                                   blob_top_data_-&gt;cpu_data()[i * 2 + j]);
        }
      }
      EXPECT_LT(num_sequence_matches, 10);
    }
  }

  virtual ~DataLayerTest() { delete blob_top_data_; delete blob_top_label_; }

  DataParameter_DB backend_;
  shared_ptr&lt;string&gt; filename_;
  Blob&lt;Dtype&gt;* const blob_top_data_;
  Blob&lt;Dtype&gt;* const blob_top_label_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
  int seed_;
};

TYPED_TEST_CASE(DataLayerTest, TestDtypesAndDevices);

#ifdef USE_LEVELDB
TYPED_TEST(DataLayerTest, TestReadLevelDB) {
  const bool unique_pixels = false;  // all pixels the same; images different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
  this-&gt;TestRead();
}

TYPED_TEST(DataLayerTest, TestSkipLevelDB) {
  this-&gt;Fill(false, DataParameter_DB_LEVELDB);
  this-&gt;TestSkip();
}

TYPED_TEST(DataLayerTest, TestReshapeLevelDB) {
  this-&gt;TestReshape(DataParameter_DB_LEVELDB);
}

TYPED_TEST(DataLayerTest, TestReadCropTrainLevelDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
  this-&gt;TestReadCrop(TRAIN);
}

// Test that the sequence of random crops is consistent when using
// Caffe::set_random_seed.
TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceSeededLevelDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
  this-&gt;TestReadCropTrainSequenceSeeded();
}

// Test that the sequence of random crops differs across iterations when
// Caffe::set_random_seed isn't called (and seeds from srand are ignored).
TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceUnseededLevelDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
  this-&gt;TestReadCropTrainSequenceUnseeded();
}

TYPED_TEST(DataLayerTest, TestReadCropTestLevelDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
  this-&gt;TestReadCrop(TEST);
}
#endif  // USE_LEVELDB

#ifdef USE_LMDB
TYPED_TEST(DataLayerTest, TestReadLMDB) {
  const bool unique_pixels = false;  // all pixels the same; images different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
  this-&gt;TestRead();
}

TYPED_TEST(DataLayerTest, TestSkipLMDB) {
  this-&gt;Fill(false, DataParameter_DB_LMDB);
  this-&gt;TestSkip();
}

TYPED_TEST(DataLayerTest, TestReshapeLMDB) {
  this-&gt;TestReshape(DataParameter_DB_LMDB);
}

TYPED_TEST(DataLayerTest, TestReadCropTrainLMDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
  this-&gt;TestReadCrop(TRAIN);
}

// Test that the sequence of random crops is consistent when using
// Caffe::set_random_seed.
TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceSeededLMDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
  this-&gt;TestReadCropTrainSequenceSeeded();
}

// Test that the sequence of random crops differs across iterations when
// Caffe::set_random_seed isn't called (and seeds from srand are ignored).
TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceUnseededLMDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
  this-&gt;TestReadCropTrainSequenceUnseeded();
}

TYPED_TEST(DataLayerTest, TestReadCropTestLMDB) {
  const bool unique_pixels = true;  // all images the same; pixels different
  this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
  this-&gt;TestReadCrop(TEST);
}

#endif  // USE_LMDB
}  // namespace caffe
#endif  // USE_OPENCV
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerHTML.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
