
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 3.5578144853875475%, Tokens: 19, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkl_deconvolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include <vector>
3  #include "gtest/gtest.h"
4  #include "caffe/blob.hpp"
5  #include "caffe/common.hpp"
6  #include "caffe/filler.hpp"
7  #include "caffe/layers/mkl_layers.hpp"
8  #include "caffe/layers/deconv_layer.hpp"
9  #include "caffe/test/test_caffe_main.hpp"
10  #include "caffe/test/test_gradient_check_util.hpp"
11  namespace caffe {
12  template <typename TypeParam>
13  class MKLDeconvolutionLayerTest : public MultiDeviceTest<TypeParam> {
14    typedef typename TypeParam::Dtype Dtype;
15   protected:
16    MKLDeconvolutionLayerTest()
17        : blob_bottom_(new Blob<Dtype>(2, 3, 6, 4)),
18          ref_blob_bottom_(new Blob<Dtype>(2, 3, 6, 4)),
19          blob_bottom_2_(new Blob<Dtype>(2, 3, 6, 4)),
20          blob_top_(new Blob<Dtype>()),
21          ref_blob_top_(new Blob<Dtype>()),
22          blob_top_2_(new Blob<Dtype>()) {}
23    virtual void SetUp() {
24      FillerParameter filler_param;
25      filler_param.set_value(1.);
26      GaussianFiller<Dtype> filler(filler_param);
27      filler.Fill(this->blob_bottom_);
28      filler.Fill(this->ref_blob_bottom_);
29      filler.Fill(this->blob_bottom_2_);
30      blob_bottom_vec_.push_back(blob_bottom_);
31      blob_top_vec_.push_back(blob_top_);
32      ref_blob_bottom_vec_.push_back(ref_blob_bottom_);
33      ref_blob_top_vec_.push_back(ref_blob_top_);
34    }
35    virtual ~MKLDeconvolutionLayerTest() {
36      delete blob_bottom_;
37      delete ref_blob_bottom_;
38      delete blob_bottom_2_;
39      delete blob_top_;
40      delete ref_blob_top_;
41      delete blob_top_2_;
42    }
43    Blob<Dtype>* const blob_bottom_;
44    Blob<Dtype>* const ref_blob_bottom_;
45    Blob<Dtype>* const blob_bottom_2_;
46    Blob<Dtype>* const blob_top_;
47    Blob<Dtype>* const ref_blob_top_;
48    Blob<Dtype>* const blob_top_2_;
49    vector<Blob<Dtype>*> blob_bottom_vec_;
50    vector<Blob<Dtype>*> blob_top_vec_;
51    vector<Blob<Dtype>*> ref_blob_bottom_vec_;
52    vector<Blob<Dtype>*> ref_blob_top_vec_;
53  };
54  TYPED_TEST_CASE(MKLDeconvolutionLayerTest, TestDtypesAndDevices);
55  TYPED_TEST(MKLDeconvolutionLayerTest, TestSetup) {
56    typedef typename TypeParam::Dtype Dtype;
57    LayerParameter layer_param;
58    ConvolutionParameter* convolution_param =
59        layer_param.mutable_convolution_param();
60    convolution_param->add_kernel_size(3);
61    convolution_param->add_stride(2);
62    convolution_param->set_num_output(4);
63    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
64    this->blob_top_vec_.push_back(this->blob_top_2_);
65    shared_ptr<Layer<Dtype> > layer(
66        new MKLDeconvolutionLayer<Dtype>(layer_param));
67    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
68    EXPECT_EQ(this->blob_top_->num(), 2);
69    EXPECT_EQ(this->blob_top_->channels(), 4);
70    EXPECT_EQ(this->blob_top_->height(), 13);
71    EXPECT_EQ(this->blob_top_->width(), 9);
72    EXPECT_EQ(this->blob_top_2_->num(), 2);
73    EXPECT_EQ(this->blob_top_2_->channels(), 4);
74    EXPECT_EQ(this->blob_top_2_->height(), 13);
75    EXPECT_EQ(this->blob_top_2_->width(), 9);
76    convolution_param->set_num_output(3);
77    convolution_param->set_group(3);
78    layer.reset(new MKLDeconvolutionLayer<Dtype>(layer_param));
79    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
80    EXPECT_EQ(this->blob_top_->num(), 2);
81    EXPECT_EQ(this->blob_top_->channels(), 3);
82    EXPECT_EQ(this->blob_top_->height(), 13);
83    EXPECT_EQ(this->blob_top_->width(), 9);
84    EXPECT_EQ(this->blob_top_2_->num(), 2);
85    EXPECT_EQ(this->blob_top_2_->channels(), 3);
86    EXPECT_EQ(this->blob_top_2_->height(), 13);
87    EXPECT_EQ(this->blob_top_2_->width(), 9);
88  }
89  TYPED_TEST(MKLDeconvolutionLayerTest, TestSimpleMKLDeconvolution) {
90    typedef typename TypeParam::Dtype Dtype;
91    LayerParameter layer_param;
92    ConvolutionParameter* convolution_param =
93        layer_param.mutable_convolution_param();
94    convolution_param->add_kernel_size(3);
95    convolution_param->add_stride(2);
96    convolution_param->set_num_output(4);
97    convolution_param->mutable_weight_filler()->set_type("constant");
98    convolution_param->mutable_weight_filler()->set_value(1);
99    convolution_param->mutable_bias_filler()->set_type("constant");
100    convolution_param->mutable_bias_filler()->set_value(0.1);
101    shared_ptr<Layer<Dtype> > layer(
102        new MKLDeconvolutionLayer<Dtype>(layer_param));
103    shared_ptr<Layer<Dtype> > ref_layer(
104        new DeconvolutionLayer<Dtype>(layer_param));
105    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
106    ref_layer->SetUp(this->ref_blob_bottom_vec_, this->ref_blob_top_vec_);
107    FillerParameter filler_param;
108    filler_param.set_value(1.);
109    ConstantFiller<Dtype> filler(filler_param);
110    filler.Fill(this->blob_bottom_);
111    filler.Fill(this->ref_blob_bottom_);
112    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
113    ref_layer->Forward(this->ref_blob_bottom_vec_, this->ref_blob_top_vec_);
114    const Dtype* top_data = this->blob_top_->cpu_data();
115    const Dtype* ref_top_data = this->ref_blob_top_->cpu_data();
116    for (int n = 0; n < this->blob_top_->num(); ++n) {
117      for (int c = 0; c < this->blob_top_->channels(); ++c) {
118        for (int h = 0; h < this->blob_top_->height(); ++h) {
119          for (int w = 0; w < this->blob_top_->width(); ++w) {
120            Dtype expected = 3.1;
121            bool h_overlap = h % 2 == 0 && h > 0
122              && h < this->blob_top_->height() - 1;
123            bool w_overlap = w % 2 == 0 && w > 0
124              && w < this->blob_top_->width() - 1;
125            if (h_overlap && w_overlap) {
126              expected += 9;
127            } else if (h_overlap || w_overlap) {
128              expected += 3;
129            }
130            EXPECT_NEAR(top_data[this->blob_top_->offset(n, c, h, w)],
131                expected, 1e-4);
132            EXPECT_NEAR(ref_top_data[this->blob_top_->offset(n, c, h, w)],
133                expected, 1e-4);
134          }
135        }
136      }
137    }
138    Dtype* top_diff = this->blob_top_->mutable_cpu_diff();
139    Dtype* ref_top_diff = this->ref_blob_top_->mutable_cpu_diff();
140    for( int n = 0; n < this->blob_top_->num(); ++n) {
141        for( int c = 0; c < this->blob_top_->channels(); ++c) {
142            for( int h=0; h < this->blob_top_->height(); ++h) {
143                for(int w = 0; w < this->blob_top_->width(); ++w) {
144                  top_diff[this->blob_top_->offset(n, c, h, w)] = ref_top_data[this->blob_top_->offset(n, c, h, w)];
145                  ref_top_diff[this->blob_top_->offset(n, c, h, w)] = ref_top_data[this->blob_top_->offset(n, c, h, w)];
146                }
147            }
148        }
149    }
150    vector<bool> need_backward({true});
151    layer->Backward(this->blob_top_vec_, need_backward, this->blob_bottom_vec_);
152    ref_layer->Backward(this->ref_blob_top_vec_, need_backward, this->ref_blob_bottom_vec_);
153    const Dtype* bottom_diff = this->blob_bottom_->cpu_diff();
154    const Dtype* ref_bottom_diff = this->ref_blob_bottom_->cpu_diff();
155    for( int n = 0; n < this->blob_bottom_->num(); ++n) {
156        for( int c = 0; c < this->blob_bottom_->channels(); ++c) {
157            for( int h=0; h < this->blob_bottom_->height(); ++h) {
158                for(int w = 0; w < this->blob_bottom_->width(); ++w) {
159                  EXPECT_NEAR(bottom_diff[this->blob_bottom_->offset(n, c, h, w)],
160                          ref_bottom_diff[this->blob_bottom_->offset(n, c, h, w)],
161                          1e-4);
162                }
163            }
164        }
165    }
166    for (int i = 0; i < layer->blobs().size(); ++i) {
167        Blob<Dtype>* blob = layer->blobs()[i].get();
168        Blob<Dtype>* ref_blob = ref_layer->blobs()[i].get();
169        const Dtype* weights_diff = blob->cpu_diff();
170        const Dtype* ref_weights_diff = ref_blob->cpu_diff();
171        for( int n = 0; n < blob->num(); ++n) {
172            for( int c = 0; c <blob->channels(); ++c) {
173                for( int h = 0; h < blob->height(); ++h) {
174                    for( int w =0; w < blob->width(); ++w) {
175                      EXPECT_NEAR(weights_diff[blob->offset(n, c, h, w)],
176                              ref_weights_diff[ref_blob->offset(n, c, h, w)],
177                              1e-3);
178                    }
179                }
180            }
181        }
182    }
183  }
184  TYPED_TEST(MKLDeconvolutionLayerTest, TestGradient) {
185    typedef typename TypeParam::Dtype Dtype;
186    LayerParameter layer_param;
187    ConvolutionParameter* convolution_param =
188        layer_param.mutable_convolution_param();
189    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
190    this->blob_top_vec_.push_back(this->blob_top_2_);
191    convolution_param->add_kernel_size(2);
192    convolution_param->add_stride(1);
193    convolution_param->set_num_output(1);
194    convolution_param->mutable_weight_filler()->set_type("gaussian");
195    convolution_param->mutable_bias_filler()->set_type("gaussian");
196    MKLDeconvolutionLayer<Dtype> layer(layer_param);
197    GradientChecker<Dtype> checker(1e-2, 1e-3);
198    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
199        this->blob_top_vec_);
200  }
201  TYPED_TEST(MKLDeconvolutionLayerTest, TestNDAgainst2D) {
202    typedef typename TypeParam::Dtype Dtype;
203    const int kernel_h = 11;
204    const int kernel_w = 13;
205    vector<int> bottom_shape(4);
206    bottom_shape[0] = 15;
207    bottom_shape[1] = 12;
208    bottom_shape[2] = kernel_h * 2;
209    bottom_shape[3] = kernel_w * 2;
210    FillerParameter filler_param;
211    GaussianFiller<Dtype> filler(filler_param);
212    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
213      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
214      filler.Fill(this->blob_bottom_vec_[i]);
215    }
216    LayerParameter layer_param;
217    ConvolutionParameter* convolution_param =
218        layer_param.mutable_convolution_param();
219    convolution_param->set_num_output(18);
220    convolution_param->set_bias_term(false);
221    convolution_param->set_group(6);
222    convolution_param->set_kernel_h(kernel_h);
223    convolution_param->set_kernel_w(kernel_w);
224    convolution_param->mutable_weight_filler()->set_type("gaussian");
225    Blob<Dtype> weights;
226    Blob<Dtype> top_diff;
227    bool copy_diff;
228    bool reshape;
229    {
230      MKLDeconvolutionLayer<Dtype> layer(layer_param);
231      layer.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
232      top_diff.ReshapeLike(*this->blob_top_);
233      filler.Fill(&top_diff);
234      ASSERT_EQ(1, layer.blobs().size());
235      copy_diff = false; reshape = true;
236      weights.CopyFrom(*layer.blobs()[0], copy_diff, reshape);
237    }
238    vector<bool> propagate_down(1, true);
239    Blob<Dtype> result_2d;
240    Blob<Dtype> backward_result_2d;
241    Blob<Dtype> backward_weight_result_2d;
242    {
243      caffe_set(this->blob_top_->count(), Dtype(0),
244                this->blob_top_->mutable_cpu_data());
245      caffe_set(this->blob_bottom_->count(), Dtype(0),
246                this->blob_bottom_->mutable_cpu_diff());
247      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
248      convolution_param->set_force_nd_im2col(false);
249      MKLDeconvolutionLayer<Dtype> layer_2d(layer_param);
250      layer_2d.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
251      ASSERT_EQ(1, layer_2d.blobs().size());
252      copy_diff = false; reshape = false;
253      layer_2d.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
254      layer_2d.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
255      copy_diff = false; reshape = true;
256      result_2d.CopyFrom(*this->blob_top_, copy_diff, reshape);
257      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
258      caffe_copy(top_diff.count(), top_diff.cpu_data(),
259                 this->blob_top_->mutable_cpu_diff());
260      layer_2d.Backward(this->blob_top_vec_, propagate_down,
261                        this->blob_bottom_vec_);
262      copy_diff = true; reshape = true;
263      backward_result_2d.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
264      backward_weight_result_2d.CopyFrom(weights, copy_diff, reshape);
265    }
266    Blob<Dtype> result_nd;
267    Blob<Dtype> backward_result_nd;
268    Blob<Dtype> backward_weight_result_nd;
269    {
270      caffe_set(this->blob_top_->count(), Dtype(0),
271                this->blob_top_->mutable_cpu_data());
272      caffe_set(this->blob_bottom_->count(), Dtype(0),
273                this->blob_bottom_->mutable_cpu_diff());
274      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
275      convolution_param->set_force_nd_im2col(true);
276      MKLDeconvolutionLayer<Dtype> layer_nd(layer_param);
277      layer_nd.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
278      ASSERT_EQ(1, layer_nd.blobs().size());
279      copy_diff = false; reshape = false;
280      layer_nd.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
281      layer_nd.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
282      copy_diff = false; reshape = true;
283      result_nd.CopyFrom(*this->blob_top_, copy_diff, reshape);
284      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
285      caffe_copy(top_diff.count(), top_diff.cpu_data(),
286                 this->blob_top_->mutable_cpu_diff());
287      layer_nd.Backward(this->blob_top_vec_, propagate_down,
288                        this->blob_bottom_vec_);
289      copy_diff = true; reshape = true;
290      backward_result_nd.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
291      backward_weight_result_nd.CopyFrom(weights, copy_diff, reshape);
<span onclick='openModal()' class='match'>292    }
293    ASSERT_EQ(result_nd.count(), result_2d.count());
294    for (int i = 0; i < result_2d.count(); ++i)  {
295      EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
296    }
297    ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
298    for (int i = 0; i < backward_result_2d.count(); ++i) {
299      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
300                backward_result_nd.cpu_diff()[i]);
301    }
302    ASSERT_EQ(backward_weight_result_nd.count(),
303              backward_weight_result_2d.count());
304    for (int i = 0; i < backward_weight_result_2d.count(); ++i) {
305      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
306                backward_weight_result_nd.cpu_diff()[i]);
307    }
</span>308  }
309  #if 0
310  TYPED_TEST(MKLDeconvolutionLayerTest, TestGradient3D) {
311    typedef typename TypeParam::Dtype Dtype;
312    vector<int> bottom_shape(5);
313    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
314    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
315    bottom_shape[2] = 2;
316    bottom_shape[3] = 3;
317    bottom_shape[4] = 2;
318    FillerParameter filler_param;
319    GaussianFiller<Dtype> filler(filler_param);
320    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
321      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
322      filler.Fill(this->blob_bottom_vec_[i]);
323    }
324    LayerParameter layer_param;
325    ConvolutionParameter* convolution_param =
326        layer_param.mutable_convolution_param();
327    convolution_param->add_kernel_size(2);
328    convolution_param->add_stride(2);
329    convolution_param->add_pad(1);
330    convolution_param->set_num_output(2);
331    convolution_param->mutable_weight_filler()->set_type("gaussian");
332    convolution_param->mutable_bias_filler()->set_type("gaussian");
333    MKLDeconvolutionLayer<Dtype> layer(layer_param);
334    GradientChecker<Dtype> checker(1e-2, 1e-3);
335    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
336        this->blob_top_vec_);
337  }
338  #endif
339  }  
340  #endif
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-upgrade_proto.cpp</h3>
            <pre><code>1  #include <google/protobuf/io/coded_stream.h>
2  #include <google/protobuf/io/zero_copy_stream_impl.h>
3  #include <google/protobuf/text_format.h>
4  #include <boost/algorithm/string/replace.hpp>
5  #include <map>
6  #include <string>
7  #include "caffe/common.hpp"
8  #include "caffe/proto/caffe.pb.h"
9  #include "caffe/util/io.hpp"
10  #include "caffe/util/upgrade_proto.hpp"
11  #ifdef USE_MLSL
12  #include "caffe/multinode/mlsl.hpp"
13  #endif &bsol;* USE_MLSL */
14  namespace caffe {
15  bool NetNeedsUpgrade(const NetParameter& net_param) {
16    return NetNeedsV0ToV1Upgrade(net_param) || NetNeedsV1ToV2Upgrade(net_param)
17        || NetNeedsDataUpgrade(net_param) || NetNeedsInputUpgrade(net_param);
18  }
19  bool UpgradeNetAsNeeded(const string& param_file, NetParameter* param) {
20    bool success = true;
21    if (NetNeedsV0ToV1Upgrade(*param)) {
22      LOG(INFO) << "Attempting to upgrade input file specified using deprecated "
23                << "V0LayerParameter: " << param_file;
24      NetParameter original_param(*param);
25      if (!UpgradeV0Net(original_param, param)) {
26        success = false;
27        LOG(ERROR) << "Warning: had one or more problems upgrading "
28            << "V0NetParameter to NetParameter (see above); continuing anyway.";
29      } else {
30        LOG(INFO) << "Successfully upgraded file specified using deprecated "
31                  << "V0LayerParameter";
32      }
33      LOG(WARNING) << "Note that future Caffe releases will not support "
34          << "V0NetParameter; use ./build/tools/upgrade_net_proto_text for "
35          << "prototxt and ./build/tools/upgrade_net_proto_binary for model "
36          << "weights upgrade this and any other net protos to the new format.";
37    }
38    if (NetNeedsDataUpgrade(*param)) {
39      LOG(INFO) << "Attempting to upgrade input file specified using deprecated "
40                << "transformation parameters: " << param_file;
41      UpgradeNetDataTransformation(param);
42      LOG(INFO) << "Successfully upgraded file specified using deprecated "
43                << "data transformation parameters.";
44      LOG(WARNING) << "Note that future Caffe releases will only support "
45                   << "transform_param messages for transformation fields.";
46    }
47    if (NetNeedsV1ToV2Upgrade(*param)) {
48      LOG(INFO) << "Attempting to upgrade input file specified using deprecated "
49                << "V1LayerParameter: " << param_file;
50      NetParameter original_param(*param);
51      if (!UpgradeV1Net(original_param, param)) {
52        success = false;
53        LOG(ERROR) << "Warning: had one or more problems upgrading "
54                   << "V1LayerParameter (see above); continuing anyway.";
55      } else {
56        LOG(INFO) << "Successfully upgraded file specified using deprecated "
57                  << "V1LayerParameter";
58      }
59    }
60    if (NetNeedsInputUpgrade(*param)) {
61      LOG(INFO) << "Attempting to upgrade input file specified using deprecated "
62                << "input fields: " << param_file;
63      UpgradeNetInput(param);
64      LOG(INFO) << "Successfully upgraded file specified using deprecated "
65                << "input fields.";
66      LOG(WARNING) << "Note that future Caffe releases will only support "
67                   << "input layers and not input fields.";
68    }
69    return success;
70  }
71  void ReadNetParamsFromTextFileOrDie(const string& param_file,
72                                      NetParameter* param) {
73    CHECK(ReadProtoFromTextFile(param_file, param))
74        << "Failed to parse NetParameter file: " << param_file;
75  #ifdef USE_MLSL
76    ReplaceMultinodeNetParams(param);
77  #endif
78    UpgradeNetAsNeeded(param_file, param);
79  }
80  void ReadNetParamsFromBinaryFileOrDie(const string& param_file,
81                                        NetParameter* param) {
82    CHECK(ReadProtoFromBinaryFile(param_file, param))
83        << "Failed to parse NetParameter file: " << param_file;
84    UpgradeNetAsNeeded(param_file, param);
85  }
86  bool NetNeedsV0ToV1Upgrade(const NetParameter& net_param) {
87    for (int i = 0; i < net_param.layers_size(); ++i) {
88      if (net_param.layers(i).has_layer()) {
89        return true;
90      }
91    }
92    return false;
93  }
94  bool NetNeedsV1ToV2Upgrade(const NetParameter& net_param) {
95    return net_param.layers_size() > 0;
96  }
97  bool UpgradeV0Net(const NetParameter& v0_net_param_padding_layers,
98                    NetParameter* net_param) {
99    NetParameter v0_net_param;
100    UpgradeV0PaddingLayers(v0_net_param_padding_layers, &v0_net_param);
101    bool is_fully_compatible = true;
102    net_param->Clear();
103    if (v0_net_param.has_name()) {
104      net_param->set_name(v0_net_param.name());
105    }
106    for (int i = 0; i < v0_net_param.layers_size(); ++i) {
107      is_fully_compatible &= UpgradeV0LayerParameter(v0_net_param.layers(i),
108                                                     net_param->add_layers());
109    }
110    for (int i = 0; i < v0_net_param.input_size(); ++i) {
111      net_param->add_input(v0_net_param.input(i));
112    }
113    for (int i = 0; i < v0_net_param.input_dim_size(); ++i) {
114      net_param->add_input_dim(v0_net_param.input_dim(i));
115    }
116    if (v0_net_param.has_force_backward()) {
117      net_param->set_force_backward(v0_net_param.force_backward());
118    }
119    return is_fully_compatible;
120  }
121  void UpgradeV0PaddingLayers(const NetParameter& param,
122                              NetParameter* param_upgraded_pad) {
123    param_upgraded_pad->Clear();
124    param_upgraded_pad->CopyFrom(param);
125    param_upgraded_pad->clear_layers();
126    map<string, int> blob_name_to_last_top_idx;
127    for (int i = 0; i < param.input_size(); ++i) {
128      const string& blob_name = param.input(i);
129      blob_name_to_last_top_idx[blob_name] = -1;
130    }
131    for (int i = 0; i < param.layers_size(); ++i) {
132      const V1LayerParameter& layer_connection = param.layers(i);
133      const V0LayerParameter& layer_param = layer_connection.layer();
134      if (layer_param.type() != "padding") {
135        param_upgraded_pad->add_layers()->CopyFrom(layer_connection);
136      }
137      for (int j = 0; j < layer_connection.bottom_size(); ++j) {
138        const string& blob_name = layer_connection.bottom(j);
139        if (blob_name_to_last_top_idx.find(blob_name) ==
140            blob_name_to_last_top_idx.end()) {
141          LOG(FATAL) << "Unknown blob input " << blob_name << " to layer " << j;
142        }
143        const int top_idx = blob_name_to_last_top_idx[blob_name];
144        if (top_idx == -1) {
145          continue;
146        }
147        const V1LayerParameter& source_layer = param.layers(top_idx);
148        if (source_layer.layer().type() == "padding") {
149          CHECK((layer_param.type() == "conv") || (layer_param.type() == "pool"))
150              << "Padding layer input to "
151              "non-convolutional / non-pooling layer type "
152              << layer_param.type();
153          CHECK_EQ(layer_connection.bottom_size(), 1)
154              << "Conv Layer takes a single blob as input.";
155          CHECK_EQ(source_layer.bottom_size(), 1)
156              << "Padding Layer takes a single blob as input.";
157          CHECK_EQ(source_layer.top_size(), 1)
158              << "Padding Layer produces a single blob as output.";
159          int layer_index = param_upgraded_pad->layers_size() - 1;
160          param_upgraded_pad->mutable_layers(layer_index)->mutable_layer()
161              ->set_pad(source_layer.layer().pad());
162          param_upgraded_pad->mutable_layers(layer_index)
163              ->set_bottom(j, source_layer.bottom(0));
164        }
165      }
166      for (int j = 0; j < layer_connection.top_size(); ++j) {
167        const string& blob_name = layer_connection.top(j);
168        blob_name_to_last_top_idx[blob_name] = i;
169      }
170    }
171  }
172  bool UpgradeV0LayerParameter(const V1LayerParameter& v0_layer_connection,
173                               V1LayerParameter* layer_param) {
174    bool is_fully_compatible = true;
175    layer_param->Clear();
176    for (int i = 0; i < v0_layer_connection.bottom_size(); ++i) {
177      layer_param->add_bottom(v0_layer_connection.bottom(i));
178    }
179    for (int i = 0; i < v0_layer_connection.top_size(); ++i) {
180      layer_param->add_top(v0_layer_connection.top(i));
181    }
182    if (v0_layer_connection.has_layer()) {
183      const V0LayerParameter& v0_layer_param = v0_layer_connection.layer();
184      if (v0_layer_param.has_name()) {
185        layer_param->set_name(v0_layer_param.name());
186      }
187      const string& type = v0_layer_param.type();
188      if (v0_layer_param.has_type()) {
189        layer_param->set_type(UpgradeV0LayerType(type));
<span onclick='openModal()' class='match'>190      }
191      for (int i = 0; i < v0_layer_param.blobs_size(); ++i) {
192        layer_param->add_blobs()->CopyFrom(v0_layer_param.blobs(i));
193      }
194      for (int i = 0; i < v0_layer_param.blobs_lr_size(); ++i) {
195        layer_param->add_blobs_lr(v0_layer_param.blobs_lr(i));
196      }
197      for (int i = 0; i < v0_layer_param.weight_decay_size(); ++i) {
198        layer_param->add_weight_decay(v0_layer_param.weight_decay(i));
199      }
</span>200      if (v0_layer_param.has_num_output()) {
201        if (type == "conv") {
202          layer_param->mutable_convolution_param()->set_num_output(
203              v0_layer_param.num_output());
204        } else if (type == "innerproduct") {
205          layer_param->mutable_inner_product_param()->set_num_output(
206              v0_layer_param.num_output());
207        } else {
208          LOG(ERROR) << "Unknown parameter num_output for layer type " << type;
209          is_fully_compatible = false;
210        }
211      }
212      if (v0_layer_param.has_biasterm()) {
213        if (type == "conv") {
214          layer_param->mutable_convolution_param()->set_bias_term(
215              v0_layer_param.biasterm());
216        } else if (type == "innerproduct") {
217          layer_param->mutable_inner_product_param()->set_bias_term(
218              v0_layer_param.biasterm());
219        } else {
220          LOG(ERROR) << "Unknown parameter biasterm for layer type " << type;
221          is_fully_compatible = false;
222        }
223      }
224      if (v0_layer_param.has_weight_filler()) {
225        if (type == "conv") {
226          layer_param->mutable_convolution_param()->
227              mutable_weight_filler()->CopyFrom(v0_layer_param.weight_filler());
228        } else if (type == "innerproduct") {
229          layer_param->mutable_inner_product_param()->
230              mutable_weight_filler()->CopyFrom(v0_layer_param.weight_filler());
231        } else {
232          LOG(ERROR) << "Unknown parameter weight_filler for layer type " << type;
233          is_fully_compatible = false;
234        }
235      }
236      if (v0_layer_param.has_bias_filler()) {
237        if (type == "conv") {
238          layer_param->mutable_convolution_param()->
239              mutable_bias_filler()->CopyFrom(v0_layer_param.bias_filler());
240        } else if (type == "innerproduct") {
241          layer_param->mutable_inner_product_param()->
242              mutable_bias_filler()->CopyFrom(v0_layer_param.bias_filler());
243        } else {
244          LOG(ERROR) << "Unknown parameter bias_filler for layer type " << type;
245          is_fully_compatible = false;
246        }
247      }
248      if (v0_layer_param.has_pad()) {
249        if (type == "conv") {
250          layer_param->mutable_convolution_param()->add_pad(v0_layer_param.pad());
251        } else if (type == "pool") {
252          layer_param->mutable_pooling_param()->set_pad(v0_layer_param.pad());
253        } else {
254          LOG(ERROR) << "Unknown parameter pad for layer type " << type;
255          is_fully_compatible = false;
256        }
257      }
258      if (v0_layer_param.has_kernelsize()) {
259        if (type == "conv") {
260          layer_param->mutable_convolution_param()->add_kernel_size(
261              v0_layer_param.kernelsize());
262        } else if (type == "pool") {
263          layer_param->mutable_pooling_param()->set_kernel_size(
264              v0_layer_param.kernelsize());
265        } else {
266          LOG(ERROR) << "Unknown parameter kernelsize for layer type " << type;
267          is_fully_compatible = false;
268        }
269      }
270      if (v0_layer_param.has_group()) {
271        if (type == "conv") {
272          layer_param->mutable_convolution_param()->set_group(
273              v0_layer_param.group());
274        } else {
275          LOG(ERROR) << "Unknown parameter group for layer type " << type;
276          is_fully_compatible = false;
277        }
278      }
279      if (v0_layer_param.has_stride()) {
280        if (type == "conv") {
281          layer_param->mutable_convolution_param()->add_stride(
282              v0_layer_param.stride());
283        } else if (type == "pool") {
284          layer_param->mutable_pooling_param()->set_stride(
285              v0_layer_param.stride());
286        } else {
287          LOG(ERROR) << "Unknown parameter stride for layer type " << type;
288          is_fully_compatible = false;
289        }
290      }
291      if (v0_layer_param.has_pool()) {
292        if (type == "pool") {
293          V0LayerParameter_PoolMethod pool = v0_layer_param.pool();
294          switch (pool) {
295          case V0LayerParameter_PoolMethod_MAX:
296            layer_param->mutable_pooling_param()->set_pool(
297                PoolingParameter_PoolMethod_MAX);
298            break;
299          case V0LayerParameter_PoolMethod_AVE:
300            layer_param->mutable_pooling_param()->set_pool(
301                PoolingParameter_PoolMethod_AVE);
302            break;
303          case V0LayerParameter_PoolMethod_STOCHASTIC:
304            layer_param->mutable_pooling_param()->set_pool(
305                PoolingParameter_PoolMethod_STOCHASTIC);
306            break;
307          default:
308            LOG(ERROR) << "Unknown pool method " << pool;
309            is_fully_compatible = false;
310          }
311        } else {
312          LOG(ERROR) << "Unknown parameter pool for layer type " << type;
313          is_fully_compatible = false;
314        }
315      }
316      if (v0_layer_param.has_dropout_ratio()) {
317        if (type == "dropout") {
318          layer_param->mutable_dropout_param()->set_dropout_ratio(
319              v0_layer_param.dropout_ratio());
320        } else {
321          LOG(ERROR) << "Unknown parameter dropout_ratio for layer type " << type;
322          is_fully_compatible = false;
323        }
324      }
325      if (v0_layer_param.has_local_size()) {
326        if (type == "lrn") {
327          layer_param->mutable_lrn_param()->set_local_size(
328              v0_layer_param.local_size());
329        } else {
330          LOG(ERROR) << "Unknown parameter local_size for layer type " << type;
331          is_fully_compatible = false;
332        }
333      }
334      if (v0_layer_param.has_alpha()) {
335        if (type == "lrn") {
336          layer_param->mutable_lrn_param()->set_alpha(v0_layer_param.alpha());
337        } else {
338          LOG(ERROR) << "Unknown parameter alpha for layer type " << type;
339          is_fully_compatible = false;
340        }
341      }
342      if (v0_layer_param.has_beta()) {
343        if (type == "lrn") {
344          layer_param->mutable_lrn_param()->set_beta(v0_layer_param.beta());
345        } else {
346          LOG(ERROR) << "Unknown parameter beta for layer type " << type;
347          is_fully_compatible = false;
348        }
349      }
350      if (v0_layer_param.has_k()) {
351        if (type == "lrn") {
352          layer_param->mutable_lrn_param()->set_k(v0_layer_param.k());
353        } else {
354          LOG(ERROR) << "Unknown parameter k for layer type " << type;
355          is_fully_compatible = false;
356        }
357      }
358      if (v0_layer_param.has_source()) {
359        if (type == "data") {
360          layer_param->mutable_data_param()->set_source(v0_layer_param.source());
361        } else if (type == "hdf5_data") {
362          layer_param->mutable_hdf5_data_param()->set_source(
363              v0_layer_param.source());
364        } else if (type == "images") {
365          layer_param->mutable_image_data_param()->set_source(
366              v0_layer_param.source());
367        } else if (type == "window_data") {
368          layer_param->mutable_window_data_param()->set_source(
369              v0_layer_param.source());
370        } else if (type == "infogain_loss") {
371          layer_param->mutable_infogain_loss_param()->set_source(
372              v0_layer_param.source());
373        } else {
374          LOG(ERROR) << "Unknown parameter source for layer type " << type;
375          is_fully_compatible = false;
376        }
377      }
378      if (v0_layer_param.has_scale()) {
379        layer_param->mutable_transform_param()->
380            set_scale(v0_layer_param.scale());
381      }
382      if (v0_layer_param.has_meanfile()) {
383        layer_param->mutable_transform_param()->
384            set_mean_file(v0_layer_param.meanfile());
385      }
386      if (v0_layer_param.has_batchsize()) {
387        if (type == "data") {
388          layer_param->mutable_data_param()->set_batch_size(
389              v0_layer_param.batchsize());
390        } else if (type == "hdf5_data") {
391          layer_param->mutable_hdf5_data_param()->set_batch_size(
392              v0_layer_param.batchsize());
393        } else if (type == "images") {
394          layer_param->mutable_image_data_param()->set_batch_size(
395              v0_layer_param.batchsize());
396        } else if (type == "window_data") {
397          layer_param->mutable_window_data_param()->set_batch_size(
398              v0_layer_param.batchsize());
399        } else {
400          LOG(ERROR) << "Unknown parameter batchsize for layer type " << type;
401          is_fully_compatible = false;
402        }
403      }
404      if (v0_layer_param.has_cropsize()) {
405        layer_param->mutable_transform_param()->
406            set_crop_size(v0_layer_param.cropsize());
407      }
408      if (v0_layer_param.has_mirror()) {
409        layer_param->mutable_transform_param()->
410            set_mirror(v0_layer_param.mirror());
411      }
412      if (v0_layer_param.has_rand_skip()) {
413        if (type == "data") {
414          layer_param->mutable_data_param()->set_rand_skip(
415              v0_layer_param.rand_skip());
416        } else if (type == "images") {
417          layer_param->mutable_image_data_param()->set_rand_skip(
418              v0_layer_param.rand_skip());
419        } else {
420          LOG(ERROR) << "Unknown parameter rand_skip for layer type " << type;
421          is_fully_compatible = false;
422        }
423      }
424      if (v0_layer_param.has_shuffle_images()) {
425        if (type == "images") {
426          layer_param->mutable_image_data_param()->set_shuffle(
427              v0_layer_param.shuffle_images());
428        } else {
429          LOG(ERROR) << "Unknown parameter shuffle for layer type " << type;
430          is_fully_compatible = false;
431        }
432      }
433      if (v0_layer_param.has_new_height()) {
434        if (type == "images") {
435          layer_param->mutable_image_data_param()->set_new_height(
436              v0_layer_param.new_height());
437        } else {
438          LOG(ERROR) << "Unknown parameter new_height for layer type " << type;
439          is_fully_compatible = false;
440        }
441      }
442      if (v0_layer_param.has_new_width()) {
443        if (type == "images") {
444          layer_param->mutable_image_data_param()->set_new_width(
445              v0_layer_param.new_width());
446        } else {
447          LOG(ERROR) << "Unknown parameter new_width for layer type " << type;
448          is_fully_compatible = false;
449        }
450      }
451      if (v0_layer_param.has_concat_dim()) {
452        if (type == "concat") {
453          layer_param->mutable_concat_param()->set_concat_dim(
454              v0_layer_param.concat_dim());
455        } else {
456          LOG(ERROR) << "Unknown parameter concat_dim for layer type " << type;
457          is_fully_compatible = false;
458        }
459      }
460      if (v0_layer_param.has_det_fg_threshold()) {
461        if (type == "window_data") {
462          layer_param->mutable_window_data_param()->set_fg_threshold(
463              v0_layer_param.det_fg_threshold());
464        } else {
465          LOG(ERROR) << "Unknown parameter det_fg_threshold for layer type "
466                     << type;
467          is_fully_compatible = false;
468        }
469      }
470      if (v0_layer_param.has_det_bg_threshold()) {
471        if (type == "window_data") {
472          layer_param->mutable_window_data_param()->set_bg_threshold(
473              v0_layer_param.det_bg_threshold());
474        } else {
475          LOG(ERROR) << "Unknown parameter det_bg_threshold for layer type "
476                     << type;
477          is_fully_compatible = false;
478        }
479      }
480      if (v0_layer_param.has_det_fg_fraction()) {
481        if (type == "window_data") {
482          layer_param->mutable_window_data_param()->set_fg_fraction(
483              v0_layer_param.det_fg_fraction());
484        } else {
485          LOG(ERROR) << "Unknown parameter det_fg_fraction for layer type "
486                     << type;
487          is_fully_compatible = false;
488        }
489      }
490      if (v0_layer_param.has_det_context_pad()) {
491        if (type == "window_data") {
492          layer_param->mutable_window_data_param()->set_context_pad(
493              v0_layer_param.det_context_pad());
494        } else {
495          LOG(ERROR) << "Unknown parameter det_context_pad for layer type "
496                     << type;
497          is_fully_compatible = false;
498        }
499      }
500      if (v0_layer_param.has_det_crop_mode()) {
501        if (type == "window_data") {
502          layer_param->mutable_window_data_param()->set_crop_mode(
503              v0_layer_param.det_crop_mode());
504        } else {
505          LOG(ERROR) << "Unknown parameter det_crop_mode for layer type "
506                     << type;
507          is_fully_compatible = false;
508        }
509      }
510      if (v0_layer_param.has_hdf5_output_param()) {
511        if (type == "hdf5_output") {
512          layer_param->mutable_hdf5_output_param()->CopyFrom(
513              v0_layer_param.hdf5_output_param());
514        } else {
515          LOG(ERROR) << "Unknown parameter hdf5_output_param for layer type "
516                     << type;
517          is_fully_compatible = false;
518        }
519      }
520    }
521    return is_fully_compatible;
522  }
523  V1LayerParameter_LayerType UpgradeV0LayerType(const string& type) {
524    if (type == "accuracy") {
525      return V1LayerParameter_LayerType_ACCURACY;
526    } else if (type == "bnll") {
527      return V1LayerParameter_LayerType_BNLL;
528    } else if (type == "concat") {
529      return V1LayerParameter_LayerType_CONCAT;
530    } else if (type == "conv") {
531      return V1LayerParameter_LayerType_CONVOLUTION;
532    } else if (type == "data") {
533      return V1LayerParameter_LayerType_DATA;
534    } else if (type == "dropout") {
535      return V1LayerParameter_LayerType_DROPOUT;
536    } else if (type == "euclidean_loss") {
537      return V1LayerParameter_LayerType_EUCLIDEAN_LOSS;
538    } else if (type == "flatten") {
539      return V1LayerParameter_LayerType_FLATTEN;
540    } else if (type == "hdf5_data") {
541      return V1LayerParameter_LayerType_HDF5_DATA;
542    } else if (type == "hdf5_output") {
543      return V1LayerParameter_LayerType_HDF5_OUTPUT;
544    } else if (type == "im2col") {
545      return V1LayerParameter_LayerType_IM2COL;
546    } else if (type == "images") {
547      return V1LayerParameter_LayerType_IMAGE_DATA;
548    } else if (type == "infogain_loss") {
549      return V1LayerParameter_LayerType_INFOGAIN_LOSS;
550    } else if (type == "innerproduct") {
551      return V1LayerParameter_LayerType_INNER_PRODUCT;
552    } else if (type == "lrn") {
553      return V1LayerParameter_LayerType_LRN;
554    } else if (type == "multinomial_logistic_loss") {
555      return V1LayerParameter_LayerType_MULTINOMIAL_LOGISTIC_LOSS;
556    } else if (type == "pool") {
557      return V1LayerParameter_LayerType_POOLING;
558    } else if (type == "relu") {
559      return V1LayerParameter_LayerType_RELU;
560    } else if (type == "sigmoid") {
561      return V1LayerParameter_LayerType_SIGMOID;
562    } else if (type == "softmax") {
563      return V1LayerParameter_LayerType_SOFTMAX;
564    } else if (type == "softmax_loss") {
565      return V1LayerParameter_LayerType_SOFTMAX_LOSS;
566    } else if (type == "split") {
567      return V1LayerParameter_LayerType_SPLIT;
568    } else if (type == "tanh") {
569      return V1LayerParameter_LayerType_TANH;
570    } else if (type == "window_data") {
571      return V1LayerParameter_LayerType_WINDOW_DATA;
572    } else {
573      LOG(FATAL) << "Unknown layer name: " << type;
574      return V1LayerParameter_LayerType_NONE;
575    }
576  }
577  bool NetNeedsDataUpgrade(const NetParameter& net_param) {
578    for (int i = 0; i < net_param.layers_size(); ++i) {
579      if (net_param.layers(i).type() == V1LayerParameter_LayerType_DATA) {
580        DataParameter layer_param = net_param.layers(i).data_param();
581        if (layer_param.has_scale()) { return true; }
582        if (layer_param.has_mean_file()) { return true; }
583        if (layer_param.has_crop_size()) { return true; }
584        if (layer_param.has_mirror()) { return true; }
585      }
586      if (net_param.layers(i).type() == V1LayerParameter_LayerType_IMAGE_DATA) {
587        ImageDataParameter layer_param = net_param.layers(i).image_data_param();
588        if (layer_param.has_scale()) { return true; }
589        if (layer_param.has_mean_file()) { return true; }
590        if (layer_param.has_crop_size()) { return true; }
591        if (layer_param.has_mirror()) { return true; }
592      }
593      if (net_param.layers(i).type() == V1LayerParameter_LayerType_WINDOW_DATA) {
594        WindowDataParameter layer_param = net_param.layers(i).window_data_param();
595        if (layer_param.has_scale()) { return true; }
596        if (layer_param.has_mean_file()) { return true; }
597        if (layer_param.has_crop_size()) { return true; }
598        if (layer_param.has_mirror()) { return true; }
599      }
600    }
601    return false;
602  }
603  #define CONVERT_LAYER_TRANSFORM_PARAM(TYPE, Name, param_name) \
604    do { \
605      if (net_param->layers(i).type() == V1LayerParameter_LayerType_##TYPE) { \
606        Name##Parameter* layer_param = \
607            net_param->mutable_layers(i)->mutable_##param_name##_param(); \
608        TransformationParameter* transform_param = \
609            net_param->mutable_layers(i)->mutable_transform_param(); \
610        if (layer_param->has_scale()) { \
611          transform_param->set_scale(layer_param->scale()); \
612          layer_param->clear_scale(); \
613        } \
614        if (layer_param->has_mean_file()) { \
615          transform_param->set_mean_file(layer_param->mean_file()); \
616          layer_param->clear_mean_file(); \
617        } \
618        if (layer_param->has_crop_size()) { \
619          transform_param->set_crop_size(layer_param->crop_size()); \
620          layer_param->clear_crop_size(); \
621        } \
622        if (layer_param->has_mirror()) { \
623          transform_param->set_mirror(layer_param->mirror()); \
624          layer_param->clear_mirror(); \
625        } \
626      } \
627    } while (0)
628  void UpgradeNetDataTransformation(NetParameter* net_param) {
629    for (int i = 0; i < net_param->layers_size(); ++i) {
630      CONVERT_LAYER_TRANSFORM_PARAM(DATA, Data, data);
631      CONVERT_LAYER_TRANSFORM_PARAM(IMAGE_DATA, ImageData, image_data);
632      CONVERT_LAYER_TRANSFORM_PARAM(WINDOW_DATA, WindowData, window_data);
633    }
634  }
635  bool UpgradeV1Net(const NetParameter& v1_net_param, NetParameter* net_param) {
636    if (v1_net_param.layer_size() > 0) {
637      LOG(FATAL) << "Refusing to upgrade inconsistent NetParameter input; "
638          << "the definition includes both 'layer' and 'layers' fields. "
639          << "The current format defines 'layer' fields with string type like "
640          << "layer { type: 'Layer' ... } and not layers { type: LAYER ... }. "
641          << "Manually switch the definition to 'layer' format to continue.";
642    }
643    bool is_fully_compatible = true;
644    net_param->CopyFrom(v1_net_param);
645    net_param->clear_layers();
646    net_param->clear_layer();
647    for (int i = 0; i < v1_net_param.layers_size(); ++i) {
648      if (!UpgradeV1LayerParameter(v1_net_param.layers(i),
649                                   net_param->add_layer())) {
650        LOG(ERROR) << "Upgrade of input layer " << i << " failed.";
651        is_fully_compatible = false;
652      }
653    }
654    return is_fully_compatible;
655  }
656  bool UpgradeV1LayerParameter(const V1LayerParameter& v1_layer_param,
657                               LayerParameter* layer_param) {
658    layer_param->Clear();
659    bool is_fully_compatible = true;
660    for (int i = 0; i < v1_layer_param.bottom_size(); ++i) {
661      layer_param->add_bottom(v1_layer_param.bottom(i));
662    }
663    for (int i = 0; i < v1_layer_param.top_size(); ++i) {
664      layer_param->add_top(v1_layer_param.top(i));
665    }
666    if (v1_layer_param.has_name()) {
667      layer_param->set_name(v1_layer_param.name());
668    }
669    for (int i = 0; i < v1_layer_param.include_size(); ++i) {
670      layer_param->add_include()->CopyFrom(v1_layer_param.include(i));
671    }
672    for (int i = 0; i < v1_layer_param.exclude_size(); ++i) {
673      layer_param->add_exclude()->CopyFrom(v1_layer_param.exclude(i));
674    }
675    if (v1_layer_param.has_type()) {
676      layer_param->set_type(UpgradeV1LayerType(v1_layer_param.type()));
677    }
678    for (int i = 0; i < v1_layer_param.blobs_size(); ++i) {
679      layer_param->add_blobs()->CopyFrom(v1_layer_param.blobs(i));
680    }
681    for (int i = 0; i < v1_layer_param.param_size(); ++i) {
682      while (layer_param->param_size() <= i) { layer_param->add_param(); }
683      layer_param->mutable_param(i)->set_name(v1_layer_param.param(i));
684    }
685    ParamSpec_DimCheckMode mode;
686    for (int i = 0; i < v1_layer_param.blob_share_mode_size(); ++i) {
687      while (layer_param->param_size() <= i) { layer_param->add_param(); }
688      switch (v1_layer_param.blob_share_mode(i)) {
689      case V1LayerParameter_DimCheckMode_STRICT:
690        mode = ParamSpec_DimCheckMode_STRICT;
691        break;
692      case V1LayerParameter_DimCheckMode_PERMISSIVE:
693        mode = ParamSpec_DimCheckMode_PERMISSIVE;
694        break;
695      default:
696        LOG(FATAL) << "Unknown blob_share_mode: "
697                   << v1_layer_param.blob_share_mode(i);
698        break;
699      }
700      layer_param->mutable_param(i)->set_share_mode(mode);
701    }
702    for (int i = 0; i < v1_layer_param.blobs_lr_size(); ++i) {
703      while (layer_param->param_size() <= i) { layer_param->add_param(); }
704      layer_param->mutable_param(i)->set_lr_mult(v1_layer_param.blobs_lr(i));
705    }
706    for (int i = 0; i < v1_layer_param.weight_decay_size(); ++i) {
707      while (layer_param->param_size() <= i) { layer_param->add_param(); }
708      layer_param->mutable_param(i)->set_decay_mult(
709          v1_layer_param.weight_decay(i));
710    }
711    for (int i = 0; i < v1_layer_param.loss_weight_size(); ++i) {
712      layer_param->add_loss_weight(v1_layer_param.loss_weight(i));
713    }
714    if (v1_layer_param.has_accuracy_param()) {
715      layer_param->mutable_accuracy_param()->CopyFrom(
716          v1_layer_param.accuracy_param());
717    }
718    if (v1_layer_param.has_argmax_param()) {
719      layer_param->mutable_argmax_param()->CopyFrom(
720          v1_layer_param.argmax_param());
721    }
722    if (v1_layer_param.has_concat_param()) {
723      layer_param->mutable_concat_param()->CopyFrom(
724          v1_layer_param.concat_param());
725    }
726    if (v1_layer_param.has_contrastive_loss_param()) {
727      layer_param->mutable_contrastive_loss_param()->CopyFrom(
728          v1_layer_param.contrastive_loss_param());
729    }
730    if (v1_layer_param.has_convolution_param()) {
731      layer_param->mutable_convolution_param()->CopyFrom(
732          v1_layer_param.convolution_param());
733    }
734    if (v1_layer_param.has_data_param()) {
735      layer_param->mutable_data_param()->CopyFrom(
736          v1_layer_param.data_param());
737    }
738    if (v1_layer_param.has_dropout_param()) {
739      layer_param->mutable_dropout_param()->CopyFrom(
740          v1_layer_param.dropout_param());
741    }
742    if (v1_layer_param.has_dummy_data_param()) {
743      layer_param->mutable_dummy_data_param()->CopyFrom(
744          v1_layer_param.dummy_data_param());
745    }
746    if (v1_layer_param.has_eltwise_param()) {
747      layer_param->mutable_eltwise_param()->CopyFrom(
748          v1_layer_param.eltwise_param());
749    }
750    if (v1_layer_param.has_exp_param()) {
751      layer_param->mutable_exp_param()->CopyFrom(
752          v1_layer_param.exp_param());
753    }
754    if (v1_layer_param.has_hdf5_data_param()) {
755      layer_param->mutable_hdf5_data_param()->CopyFrom(
756          v1_layer_param.hdf5_data_param());
757    }
758    if (v1_layer_param.has_hdf5_output_param()) {
759      layer_param->mutable_hdf5_output_param()->CopyFrom(
760          v1_layer_param.hdf5_output_param());
761    }
762    if (v1_layer_param.has_hinge_loss_param()) {
763      layer_param->mutable_hinge_loss_param()->CopyFrom(
764          v1_layer_param.hinge_loss_param());
765    }
766    if (v1_layer_param.has_image_data_param()) {
767      layer_param->mutable_image_data_param()->CopyFrom(
768          v1_layer_param.image_data_param());
769    }
770    if (v1_layer_param.has_infogain_loss_param()) {
771      layer_param->mutable_infogain_loss_param()->CopyFrom(
772          v1_layer_param.infogain_loss_param());
773    }
774    if (v1_layer_param.has_inner_product_param()) {
775      layer_param->mutable_inner_product_param()->CopyFrom(
776          v1_layer_param.inner_product_param());
777    }
778    if (v1_layer_param.has_lrn_param()) {
779      layer_param->mutable_lrn_param()->CopyFrom(
780          v1_layer_param.lrn_param());
781    }
782    if (v1_layer_param.has_memory_data_param()) {
783      layer_param->mutable_memory_data_param()->CopyFrom(
784          v1_layer_param.memory_data_param());
785    }
786    if (v1_layer_param.has_mvn_param()) {
787      layer_param->mutable_mvn_param()->CopyFrom(
788          v1_layer_param.mvn_param());
789    }
790    if (v1_layer_param.has_pooling_param()) {
791      layer_param->mutable_pooling_param()->CopyFrom(
792          v1_layer_param.pooling_param());
793    }
794    if (v1_layer_param.has_power_param()) {
795      layer_param->mutable_power_param()->CopyFrom(
796          v1_layer_param.power_param());
797    }
798    if (v1_layer_param.has_relu_param()) {
799      layer_param->mutable_relu_param()->CopyFrom(
800          v1_layer_param.relu_param());
801    }
802    if (v1_layer_param.has_sigmoid_param()) {
803      layer_param->mutable_sigmoid_param()->CopyFrom(
804          v1_layer_param.sigmoid_param());
805    }
806    if (v1_layer_param.has_softmax_param()) {
807      layer_param->mutable_softmax_param()->CopyFrom(
808          v1_layer_param.softmax_param());
809    }
810    if (v1_layer_param.has_slice_param()) {
811      layer_param->mutable_slice_param()->CopyFrom(
812          v1_layer_param.slice_param());
813    }
814    if (v1_layer_param.has_tanh_param()) {
815      layer_param->mutable_tanh_param()->CopyFrom(
816          v1_layer_param.tanh_param());
817    }
818    if (v1_layer_param.has_threshold_param()) {
819      layer_param->mutable_threshold_param()->CopyFrom(
820          v1_layer_param.threshold_param());
821    }
822    if (v1_layer_param.has_window_data_param()) {
823      layer_param->mutable_window_data_param()->CopyFrom(
824          v1_layer_param.window_data_param());
825    }
826    if (v1_layer_param.has_transform_param()) {
827      layer_param->mutable_transform_param()->CopyFrom(
828          v1_layer_param.transform_param());
829    }
830    if (v1_layer_param.has_loss_param()) {
831      layer_param->mutable_loss_param()->CopyFrom(
832          v1_layer_param.loss_param());
833    }
834    if (v1_layer_param.has_layer()) {
835      LOG(ERROR) << "Input NetParameter has V0 layer -- ignoring.";
836      is_fully_compatible = false;
837    }
838    return is_fully_compatible;
839  }
840  const char* UpgradeV1LayerType(const V1LayerParameter_LayerType type) {
841    switch (type) {
842    case V1LayerParameter_LayerType_NONE:
843      return "";
844    case V1LayerParameter_LayerType_ABSVAL:
845      return "AbsVal";
846    case V1LayerParameter_LayerType_ACCURACY:
847      return "Accuracy";
848    case V1LayerParameter_LayerType_ARGMAX:
849      return "ArgMax";
850    case V1LayerParameter_LayerType_BNLL:
851      return "BNLL";
852    case V1LayerParameter_LayerType_CONCAT:
853      return "Concat";
854    case V1LayerParameter_LayerType_CONTRASTIVE_LOSS:
855      return "ContrastiveLoss";
856    case V1LayerParameter_LayerType_CONVOLUTION:
857      return "Convolution";
858    case V1LayerParameter_LayerType_DECONVOLUTION:
859      return "Deconvolution";
860    case V1LayerParameter_LayerType_DATA:
861      return "Data";
862    case V1LayerParameter_LayerType_DROPOUT:
863      return "Dropout";
864    case V1LayerParameter_LayerType_DUMMY_DATA:
865      return "DummyData";
866    case V1LayerParameter_LayerType_EUCLIDEAN_LOSS:
867      return "EuclideanLoss";
868    case V1LayerParameter_LayerType_ELTWISE:
869      return "Eltwise";
870    case V1LayerParameter_LayerType_EXP:
871      return "Exp";
872    case V1LayerParameter_LayerType_FLATTEN:
873      return "Flatten";
874    case V1LayerParameter_LayerType_HDF5_DATA:
875      return "HDF5Data";
876    case V1LayerParameter_LayerType_HDF5_OUTPUT:
877      return "HDF5Output";
878    case V1LayerParameter_LayerType_HINGE_LOSS:
879      return "HingeLoss";
880    case V1LayerParameter_LayerType_IM2COL:
881      return "Im2col";
882    case V1LayerParameter_LayerType_IMAGE_DATA:
883      return "ImageData";
884    case V1LayerParameter_LayerType_INFOGAIN_LOSS:
885      return "InfogainLoss";
886    case V1LayerParameter_LayerType_INNER_PRODUCT:
887      return "InnerProduct";
888    case V1LayerParameter_LayerType_LRN:
889      return "LRN";
890    case V1LayerParameter_LayerType_MEMORY_DATA:
891      return "MemoryData";
892    case V1LayerParameter_LayerType_MULTINOMIAL_LOGISTIC_LOSS:
893      return "MultinomialLogisticLoss";
894    case V1LayerParameter_LayerType_MVN:
895      return "MVN";
896    case V1LayerParameter_LayerType_POOLING:
897      return "Pooling";
898    case V1LayerParameter_LayerType_POWER:
899      return "Power";
900    case V1LayerParameter_LayerType_RELU:
901      return "ReLU";
902    case V1LayerParameter_LayerType_SIGMOID:
903      return "Sigmoid";
904    case V1LayerParameter_LayerType_SIGMOID_CROSS_ENTROPY_LOSS:
905      return "SigmoidCrossEntropyLoss";
906    case V1LayerParameter_LayerType_SILENCE:
907      return "Silence";
908    case V1LayerParameter_LayerType_SOFTMAX:
909      return "Softmax";
910    case V1LayerParameter_LayerType_SOFTMAX_LOSS:
911      return "SoftmaxWithLoss";
912    case V1LayerParameter_LayerType_SPLIT:
913      return "Split";
914    case V1LayerParameter_LayerType_SLICE:
915      return "Slice";
916    case V1LayerParameter_LayerType_TANH:
917      return "TanH";
918    case V1LayerParameter_LayerType_WINDOW_DATA:
919      return "WindowData";
920    case V1LayerParameter_LayerType_THRESHOLD:
921      return "Threshold";
922    default:
923      LOG(FATAL) << "Unknown V1LayerParameter layer type: " << type;
924      return "";
925    }
926  }
927  bool NetNeedsInputUpgrade(const NetParameter& net_param) {
928    return net_param.input_size() > 0;
929  }
930  void UpgradeNetInput(NetParameter* net_param) {
931    bool has_shape = net_param->input_shape_size() > 0;
932    bool has_dim = net_param->input_dim_size() > 0;
933    if (has_shape || has_dim) {
934      LayerParameter* layer_param = net_param->add_layer();
935      layer_param->set_name("input");
936      layer_param->set_type("Input");
937      InputParameter* input_param = layer_param->mutable_input_param();
938      for (int i = 0; i < net_param->input_size(); ++i) {
939        layer_param->add_top(net_param->input(i));
940        if (has_shape) {
941          input_param->add_shape()->CopyFrom(net_param->input_shape(i));
942        } else {
943          BlobShape* shape = input_param->add_shape();
944          int first_dim = i*4;
945          int last_dim = first_dim + 4;
946          for (int j = first_dim; j < last_dim; j++) {
947            shape->add_dim(net_param->input_dim(j));
948          }
949        }
950      }
951      for (int i = net_param->layer_size() - 1; i > 0; --i) {
952        net_param->mutable_layer(i-1)->Swap(net_param->mutable_layer(i));
953      }
954    }
955    net_param->clear_input();
956    net_param->clear_input_shape();
957    net_param->clear_input_dim();
958  }
959  bool SolverNeedsTypeUpgrade(const SolverParameter& solver_param) {
960    if (solver_param.has_solver_type()) {
961      return true;
962    }
963    return false;
964  }
965  bool UpgradeSolverType(SolverParameter* solver_param) {
966    CHECK(!solver_param->has_solver_type() || !solver_param->has_type())
967        << "Failed to upgrade solver: old solver_type field (enum) and new type "
968        << "field (string) cannot be both specified in solver proto text.";
969    if (solver_param->has_solver_type()) {
970      string type;
971      switch (solver_param->solver_type()) {
972      case SolverParameter_SolverType_SGD:
973        type = "SGD";
974        break;
975      case SolverParameter_SolverType_NESTEROV:
976        type = "Nesterov";
977        break;
978      case SolverParameter_SolverType_ADAGRAD:
979        type = "AdaGrad";
980        break;
981      case SolverParameter_SolverType_RMSPROP:
982        type = "RMSProp";
983        break;
984      case SolverParameter_SolverType_ADADELTA:
985        type = "AdaDelta";
986        break;
987      case SolverParameter_SolverType_ADAM:
988        type = "Adam";
989        break;
990      default:
991        LOG(FATAL) << "Unknown SolverParameter solver_type: " << type;
992      }
993      solver_param->set_type(type);
994      solver_param->clear_solver_type();
995    } else {
996      LOG(ERROR) << "Warning: solver type already up to date. ";
997      return false;
998    }
999    return true;
1000  }
1001  bool UpgradeSolverAsNeeded(const string& param_file, SolverParameter* param) {
1002    bool success = true;
1003    if (SolverNeedsTypeUpgrade(*param)) {
1004      LOG(INFO) << "Attempting to upgrade input file specified using deprecated "
1005                << "'solver_type' field (enum)': " << param_file;
1006      if (!UpgradeSolverType(param)) {
1007        success = false;
1008        LOG(ERROR) << "Warning: had one or more problems upgrading "
1009                   << "SolverType (see above).";
1010      } else {
1011        LOG(INFO) << "Successfully upgraded file specified using deprecated "
1012                  << "'solver_type' field (enum) to 'type' field (string).";
1013        LOG(WARNING) << "Note that future Caffe releases will only support "
1014                     << "'type' field (string) for a solver's type.";
1015      }
1016    }
1017    return success;
1018  }
1019  void ReadSolverParamsFromTextFileOrDie(const string& param_file,
1020                                         SolverParameter* param) {
1021    CHECK(ReadProtoFromTextFile(param_file, param))
1022        << "Failed to parse SolverParameter file: " << param_file;
1023    UpgradeSolverAsNeeded(param_file, param);
1024  }
1025  #ifdef USE_MLSL
1026  static std::string getNodeId() {
1027    return std::to_string(mn::get_node_id());
1028  }
1029  static std::string getNumNodes() {
1030    return std::to_string(mn::get_nodes_count());
1031  }
1032  void ReplaceMultinodeSolverParams(SolverParameter* param) {
1033    std::string node_id = getNodeId();
1034    std::string num_nodes = getNumNodes();
1035    if (param->has_train_net()) {
1036      std::string* train_net = param->mutable_train_net();
1037      if (train_net) {
1038          boost::replace_all(*train_net, "%#", node_id);
1039          boost::replace_all(*train_net, "%*", num_nodes);
1040      }
1041    }
1042    if (param->has_snapshot_prefix()) {
1043      std::string* prefix = param->mutable_snapshot_prefix();
1044      if (prefix) {
1045          boost::replace_all(*prefix, "%#", node_id);
1046          boost::replace_all(*prefix, "%*", num_nodes);
1047      }
1048    }
1049  }
1050  void ReplaceMultinodeNetParams(NetParameter* param) {
1051    for (int i = 0; i < param->layer_size(); ++i) {
1052      std::string* source = nullptr;
1053      if (param->layer(i).has_data_param()) {
1054        source = param->mutable_layer(i)->mutable_data_param()->
1055                mutable_source();
1056      } else if (param->layer(i).has_image_data_param()) {
1057        source = param->mutable_layer(i)->mutable_image_data_param()->
1058                mutable_source();
1059      }
1060      if (source) {
1061          boost::replace_all(*source, "%#", getNodeId());
1062          boost::replace_all(*source, "%*", getNumNodes());
1063      }
1064    }
1065  }
1066  #endif
1067  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkl_deconvolution_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-upgrade_proto.cpp</div>
                </div>
                <div class="column column_space"><pre><code>292    }
293    ASSERT_EQ(result_nd.count(), result_2d.count());
294    for (int i = 0; i < result_2d.count(); ++i)  {
295      EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
296    }
297    ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
298    for (int i = 0; i < backward_result_2d.count(); ++i) {
299      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
300                backward_result_nd.cpu_diff()[i]);
301    }
302    ASSERT_EQ(backward_weight_result_nd.count(),
303              backward_weight_result_2d.count());
304    for (int i = 0; i < backward_weight_result_2d.count(); ++i) {
305      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
306                backward_weight_result_nd.cpu_diff()[i]);
307    }
</pre></code></div>
                <div class="column column_space"><pre><code>190      }
191      for (int i = 0; i < v0_layer_param.blobs_size(); ++i) {
192        layer_param->add_blobs()->CopyFrom(v0_layer_param.blobs(i));
193      }
194      for (int i = 0; i < v0_layer_param.blobs_lr_size(); ++i) {
195        layer_param->add_blobs_lr(v0_layer_param.blobs_lr(i));
196      }
197      for (int i = 0; i < v0_layer_param.weight_decay_size(); ++i) {
198        layer_param->add_weight_decay(v0_layer_param.weight_decay(i));
199      }
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    