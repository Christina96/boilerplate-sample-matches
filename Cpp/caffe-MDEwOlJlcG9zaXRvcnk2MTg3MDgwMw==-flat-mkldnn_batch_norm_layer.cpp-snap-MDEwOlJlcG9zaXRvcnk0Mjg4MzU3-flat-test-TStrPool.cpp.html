
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 16, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_batch_norm_layer.cpp</h3>
            <pre><code>1  #ifdef MKLDNN_SUPPORTED
2  #include <algorithm>
3  #include <vector>
4  #include "caffe/filler.hpp"
5  #include "caffe/layers/mkldnn_layers.hpp"
6  namespace caffe {
7  template <typename Dtype>
8  void MKLDNNBatchNormLayer<Dtype>::InitStatsBatchVars(int batch_size) {
9      num_stats_batches_ = 1;
10      stats_batch_size_ = batch_size;
11      BatchNormParameter param = this->layer_param_.batch_norm_param();
12      if (!use_global_stats_ && param.stats_batch_size() > 0) {
13        CHECK_EQ(batch_size % param.stats_batch_size(), 0);
14        num_stats_batches_ = batch_size / param.stats_batch_size();
15        stats_batch_size_ = param.stats_batch_size();
16      }
17  }
18  template <typename Dtype>
19  void MKLDNNBatchNormLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom
20                                          ,const vector<Blob<Dtype>*>& top)
21  {
22      VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::LayerSetUp: " << this->layer_param_.name();
23      Layer<Dtype>::LayerSetUp(bottom, top);
24      shape_ = bottom[0]->shape();
25      const int channels = shape_[1];
26      eps_ = this->layer_param_.batch_norm_param().eps();
27      use_weight_bias_ = this->layer_param_.batch_norm_param().use_weight_bias();
28      bias_term_ = this->layer_param_.batch_norm_param().bias_term();
29      moving_average_fraction_ = this->layer_param_.batch_norm_param().moving_average_fraction();
30      use_global_stats_ = this->phase_ == TEST;
31      if (this->layer_param_.batch_norm_param().has_use_global_stats())
32        use_global_stats_ = this->layer_param_.batch_norm_param().use_global_stats();
33      InitStatsBatchVars(shape_[0]);
34      this->blobs_.resize(3 + (use_weight_bias_ ? 1:0) + (use_weight_bias_ && bias_term_ ? 1:0));
35      vector<int> sz;
36      sz.push_back(channels);
37      this->blobs_[0].reset(new Blob<Dtype>(sz));
38      this->blobs_[1].reset(new Blob<Dtype>(sz));
39      sz[0]=1;
40      this->blobs_[2].reset(new Blob<Dtype>(sz));
41      for (int i = 0; i < 3; ++i) {
42          caffe_set(this->blobs_[i]->count(), Dtype(0),
43              this->blobs_[i]->mutable_cpu_data());
44      }
45      vector<int> scaleshift_blob_shape(1);
46      scaleshift_blob_shape[0] = 2*channels;
47      scaleshift_blob_.reset(new Blob<Dtype>(scaleshift_blob_shape));
48      caffe_set(scaleshift_blob_shape[0], static_cast<Dtype>(0),
49                scaleshift_blob_->mutable_cpu_data());
50      shared_ptr<Blob<Dtype> > scaleshift_diff_blob = scaleshift_blob_;
51      scaleshift_acc_ = scaleshift_blob_;
52      if (num_stats_batches_ > 1) {
53        this->scaleshift_acc_.reset(new Blob<Dtype>(scaleshift_blob_shape));
54        scaleshift_diff_blob = scaleshift_acc_;
55      }
56      if (use_weight_bias_) {
57          vector<int> scaleshift_shape(1);
58          scaleshift_shape[0] = channels;
59          VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::LayerSetUp: channels_  = " << channels;
60          this->blobs_[3].reset(new Blob<Dtype>(scaleshift_shape));
61          this->blobs_[3]->set_cpu_data(scaleshift_blob_->mutable_cpu_data());
62          this->blobs_[3]->set_cpu_diff(scaleshift_diff_blob->mutable_cpu_diff());
63          FillerParameter filler_param(this->layer_param_.batch_norm_param().filler());
64          if (!this->layer_param_.batch_norm_param().has_filler()) {
65              filler_param.set_type("constant");
66              filler_param.set_value(1);
67          }
68          shared_ptr<Filler<Dtype> > filler(GetFiller<Dtype>(filler_param));
69          VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::LayerSetUp: scaleshift " << __LINE__ << ":" << this->layer_param_.name();
70          filler->Fill(this->blobs_[3].get());
71          if (bias_term_) {
72              this->blobs_[4].reset(new Blob<Dtype>(scaleshift_shape));
73              this->blobs_[4]->set_cpu_data(scaleshift_blob_->mutable_cpu_data() + scaleshift_blob_->offset(channels));
74              this->blobs_[4]->set_cpu_diff(scaleshift_diff_blob->mutable_cpu_diff() + scaleshift_blob_->offset(channels));
75              FillerParameter bias_filler_param(this->layer_param_.batch_norm_param().bias_filler());
76              if (!this->layer_param_.batch_norm_param().has_bias_filler()) {
77                  bias_filler_param.set_type("constant");
78                  bias_filler_param.set_value(0);
79              }
80              shared_ptr<Filler<Dtype> > bias_filler(GetFiller<Dtype>(bias_filler_param));
81              VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::LayerSetUp: bias " << __LINE__ << ":" << this->layer_param_.name();
82              bias_filler->Fill(this->blobs_[4].get());
83          }
84      }
85      for (int i = 0; i < 3; ++i) {
86        if (this->layer_param_.param_size() == i) {
87          ParamSpec* fixed_param_spec = this->layer_param_.add_param();
88          fixed_param_spec->set_lr_mult(0.f);
89        } else {
90          CHECK_EQ(this->layer_param_.param(i).lr_mult(), 0.f)
91              << "Cannot configure batch normalization statistics as layer "
92              << "parameters.";
93        }
94      }
95  }
96  template <typename Dtype>
97  void MKLDNNBatchNormLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom
98                                      ,const vector<Blob<Dtype>*>& top)
99  {
100      VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::Reshape: " << this->layer_param_.name();
101      this->reshape = (this->shape_ == bottom[0]->shape()) ? false : true;
102      this->shape_ = bottom[0]->shape();
103      InitStatsBatchVars(this->shape_[0]);
104  #ifdef DEBUG
105      LOG(INFO) << "size of bottom blob: " << bottom[0]->shape().size();
106  #endif
107      top[0]->ReshapeLike(*bottom[0]);
108      if(bottom[0] == top[0] && this->phase_ == TRAIN)
109          inplace_buffer.ReshapeLike(*bottom[0]);
110  }
111  template <typename Dtype>
112  void MKLDNNBatchNormLayer<Dtype>::InitBatchNorm(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
113  {
114      if (std::is_same<Dtype, double>::value) NOT_IMPLEMENTED;
115      auto propagation = this->phase_ == TEST ? prop_kind::forward_scoring : prop_kind::forward_training;
116      unsigned flags = 0;
117      if (use_weight_bias_) flags |= use_scale_shift;
118      if (use_global_stats_) flags |= use_global_stats;
119      memory::format src_mfmt;
120      auto tensor_size = this->shape_.size();
121      memory::dims dim = this->shape_;
122      if(tensor_size == 5) {
123          src_mfmt = memory::format::ncdhw;
124      } else {
125          CHECK_LE(tensor_size, 4)
126              << "mkldnn batch normalization layer only supports dim size <= 5!";
127          if (tensor_size < 4) dim.resize(4, 1); 
128          src_mfmt = memory::format::nchw;
129      }
130      const int channels = this->shape_[1];
131      bool bottom_data_is_prv = (const_cast<Dtype*>(bottom[0]->prv_data()) != NULL);
132      bool inplace = (bottom[0] == top[0]);
133      engine cpu_engine = CpuEngine::Instance().get_engine();
134      memory::data_type mpcsn = memory::data_type::f32;
135      shared_ptr<memory::desc> input_md, input_stats_md, output_md, scaleshift_md;
136      shared_ptr<memory::primitive_desc> usr_mpd, prv_mpd;
137      shared_ptr<memory::primitive_desc> scaleshift_mpd;
138      if (bottom_data_is_prv) {
139          shared_ptr<MKLDNNMemoryDescriptor<Dtype, false> > mem_descr
140              = get_mkldnn_prv_descriptor<Dtype, false>(bottom[0]);
141          input_md.reset(new memory::desc(mem_descr->prv_memory_pd()->desc()));
142          usr_mpd = mem_descr->usr_memory_pd();
143          prv_mpd = mem_descr->prv_memory_pd();
144      } else {
145          input_md.reset(new memory::desc({dim}, mpcsn, src_mfmt));
146          usr_mpd.reset(new memory::primitive_desc(*input_md, cpu_engine));
147      }
148      output_md = input_md;
149      input_stats_md.reset(new memory::desc(*input_md));
150      CHECK(input_stats_md->data.ndims > 0 &&
151            input_stats_md->data.dims[0] == this->shape_[0]);
152      input_stats_md->data.dims[0] = stats_batch_size_;
153      batch_normalization_forward::desc BatchNormFwd_desc(propagation, *input_stats_md, eps_, flags);
154      std::string subengines = this->layer_param_.engine();
155      if (subengines.find("MKLDNN") == std::string::npos || subengines == "MKLDNN")
156        subengines = "MKLDNN:CPU";
157      EngineParser ep(subengines);
158      unsigned subEngineIndex = 0;
159      BatchNormFwd_pd = NULL;
160      bool relu = this->layer_param_.batch_norm_param().relu();
161      mkldnn::primitive_attr attr;
162      mkldnn::post_ops ops;
163      if (relu) {
164          ops.append_eltwise(1.f, eltwise_relu, 0.f, 0.f);
165          attr.set_post_ops(ops);
166      }
167      for(; subEngineIndex < ep.getNumberOfSubEngines(); subEngineIndex++) {
168        try {
169          if (relu)
170              BatchNormFwd_pd.reset(new batch_normalization_forward::primitive_desc(BatchNormFwd_desc, attr,
171                  ep.getMKLDNNSubEngine(subEngineIndex)));
172          else
173              BatchNormFwd_pd.reset(new batch_normalization_forward::primitive_desc(BatchNormFwd_desc,
174                  ep.getMKLDNNSubEngine(subEngineIndex)));
175        }
176        catch(...) {
177          continue;
178        }
179        break;
180      }
181      CHECK(BatchNormFwd_pd);
182      if (use_weight_bias_) {
183          if((this->blobs_[3]->mutable_cpu_data() + this->blobs_[3]->offset(channels)) == this->blobs_[4]->mutable_cpu_data()){
184              scaleshift_memory.reset(new memory(BatchNormFwd_pd->weights_primitive_desc(), this->blobs_[3]->mutable_cpu_data()));
185          }else {
186              scaleshift_memory.reset(new memory(BatchNormFwd_pd->weights_primitive_desc(), this->scaleshift_blob_->mutable_cpu_data()));
187          }
188      }
189      fwd_bottom_data.reset(new MKLDNNData<Dtype>(usr_mpd, prv_mpd, bottom[0], this));
190      input_primitive = fwd_bottom_data->create_input(false);
191      if(inplace && this->phase_ == TRAIN) {
192          fwd_top_data.reset(new MKLDNNData<Dtype>(usr_mpd, prv_mpd, &inplace_buffer, this));
193      } else {
194          fwd_top_data.reset(new MKLDNNData<Dtype>(usr_mpd, prv_mpd, top[0], this));
195      }
196      output_memory = fwd_top_data->create_output_memory();
197      mean_memory.resize(num_stats_batches_);
198      variance_memory.resize(num_stats_batches_);
199      input_stats.resize(num_stats_batches_);
200      output_stats.resize(num_stats_batches_);
201      BatchNormFwd.resize(num_stats_batches_);
202      for (int i = 0; i < num_stats_batches_; i++) {
203        InitBatchNormFwdPrimitive(i);
204      }
205      MKLDNNPrimitive<Dtype> fwd_bottom_data_primitive_transfer(input_primitive);
206      fwd_bottom_data->set_mkldnn_primitive(fwd_bottom_data_primitive_transfer);
207      MKLDNNPrimitive<Dtype> fwd_top_data_memory_transfer(output_memory);
208      fwd_top_data->set_mkldnn_primitive(fwd_top_data_memory_transfer);
209      bool has_spatial = (bottom[0]->shape().size() != 2);
210  #ifdef DEBUG
211      LOG(INFO) << "has_spatial flag value: " << has_spatial;
212  #endif
213      if (has_spatial == false)
214      {
215  #ifdef DEBUG
216          LOG(INFO) << "size of bottom blob: " << bottom[0]->shape().size();
217          LOG(INFO) << "MKLDNN batch norm only support 4D memory descriptor! Use 4D for calculation and reshape to 2D for output!";
218  #endif
219          vector<int> top_shape;
220          top_shape.push_back(bottom[0]->shape(0));
221          top_shape.push_back(bottom[0]->shape(1));
222          top[0]->Reshape(top_shape);
223      }
224  }
225  template <typename Dtype>
226  template <bool diff>
227  shared_ptr<memory> MKLDNNBatchNormLayer<Dtype>::GetStatsBatchMemory(
228    shared_ptr<MKLDNNMemoryDescriptor<Dtype, diff> > mkldnn_mem, int idx) {
229      int length = this->shape_[1];
230      for(int i=2;i<this->shape_.size();i++)
231          length *= this->shape_[i];
232      long data_offset =  idx * stats_batch_size_ * length;
233      engine cpu_engine = CpuEngine::Instance().get_engine();
234      shared_ptr<memory::desc> stats_md = mkldnn_mem->get_memory_desc();
235      CHECK(stats_md->data.ndims > 0 &&
236            stats_md->data.dims[0] == this->shape_[0]);
237      stats_md->data.dims[0] = stats_batch_size_;
238      shared_ptr<memory::primitive_desc> stats_mpd(
239        new memory::primitive_desc(*stats_md, cpu_engine));
240      shared_ptr<memory> stats(
241        new memory(*stats_mpd, mkldnn_mem->get_memory_ptr(data_offset)));
242      return stats;
243  }
244  template <typename Dtype>
245  void MKLDNNBatchNormLayer<Dtype>::InitBatchNormFwdPrimitive(int idx) {
246      input_stats[idx] = GetStatsBatchMemory<false>(fwd_bottom_data, idx);
247      output_stats[idx] = GetStatsBatchMemory<false>(fwd_top_data, idx);
248      const int channels = this->shape_[1];
249      if (this->phase_ == TEST && !use_global_stats_) {
250          if (use_weight_bias_) {
251              BatchNormFwd[idx].reset(new batch_normalization_forward(*BatchNormFwd_pd,
252                      *input_stats[idx], *scaleshift_memory,
253                      *output_stats[idx]));
254          } else {
255              BatchNormFwd[idx].reset(new batch_normalization_forward(*BatchNormFwd_pd,
256                      *input_stats[idx], *output_stats[idx]));
257          }
258      } else {
259          mean_memory[idx].reset(new memory(BatchNormFwd_pd->mean_primitive_desc()));
260          variance_memory[idx].reset(new memory(BatchNormFwd_pd->variance_primitive_desc()));
261          if (use_global_stats_) {
262              caffe_copy<Dtype>(channels, this->blobs_[0]->cpu_data(),
263                  static_cast<Dtype *>(mean_memory[idx]->get_data_handle()));
264              caffe_copy<Dtype>(channels, this->blobs_[1]->cpu_data(),
265                 static_cast<Dtype *>(variance_memory[idx]->get_data_handle()));
266              if (use_weight_bias_) {
267                  BatchNormFwd[idx].reset(new batch_normalization_forward(*BatchNormFwd_pd,
268                          *input_stats[idx], (const primitive::at)*mean_memory[idx],
269                          (const primitive::at)*variance_memory[idx], *scaleshift_memory,
270                          *output_stats[idx]));
271              } else {
272                  BatchNormFwd[idx].reset(new batch_normalization_forward(*BatchNormFwd_pd,
273                          *input_stats[idx], (const primitive::at)*mean_memory[idx],
274                          (const primitive::at)*variance_memory[idx], *output_stats[idx]));
275              }
276          } else {
277              if (use_weight_bias_) {
278                  BatchNormFwd[idx].reset(new batch_normalization_forward(*BatchNormFwd_pd,
279                          *input_stats[idx], *scaleshift_memory, *output_stats[idx],
280                          *mean_memory[idx], *variance_memory[idx]));
281              } else {
282                  BatchNormFwd[idx].reset(new batch_normalization_forward(*BatchNormFwd_pd,
283                          *input_stats[idx], *output_stats[idx], *mean_memory[idx], *variance_memory[idx]));
284              }
285          }
286      }
287  }
288  template <typename Dtype>
289  void MKLDNNBatchNormLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom
290                                          ,const vector<Blob<Dtype>*>& top)
291  {
292      VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::Forward_cpu: " << this->layer_param_.name();
293  #ifdef DEBUG
294      LOG(INFO) << "MKLDNNBatchNormLayer<Dtype>::Forward_cpu: " << this->layer_param_.name();
295  #endif
296      if(BatchNormFwd_pd == NULL || this->reshape)
297          InitBatchNorm(bottom, top);
298      bool inplace = (bottom[0] == top[0]);
299      fwd_bottom_data->sync_before_read();
300      fwd_top_data->sync_before_write();
301      const int channels = this->shape_[1];
302      if((this->blobs_[3]->mutable_cpu_data() + this->blobs_[3]->offset(channels)) != this->blobs_[4]->mutable_cpu_data()){
303          caffe_copy(channels, this->blobs_[3]->cpu_data(), this->scaleshift_blob_->mutable_cpu_data());
304          caffe_copy(channels, this->blobs_[4]->cpu_data(), this->scaleshift_blob_->mutable_cpu_data() + scaleshift_blob_->offset(channels));
305      }
306      for (int stats_batch_idx = 0; stats_batch_idx < num_stats_batches_; stats_batch_idx++) {
307        if (use_global_stats_) {
308          const Dtype scale_factor = this->blobs_[2]->cpu_data()[0] == 0 ?
309              0 : 1 / this->blobs_[2]->cpu_data()[0];
310          Dtype *mean_buffer_ = (Dtype *)(mean_memory[stats_batch_idx]->get_data_handle());
311          Dtype *variance_buffer_ = (Dtype *)(variance_memory[stats_batch_idx]->get_data_handle());
312          caffe_cpu_scale(this->blobs_[0]->count(), scale_factor,
313                      this->blobs_[0]->cpu_data(), mean_buffer_);
314          caffe_cpu_scale(this->blobs_[1]->count(), scale_factor,
315                      this->blobs_[1]->cpu_data(), variance_buffer_);
316        }
317        PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKLDNN_NAME("FW"));
318        PERFORMANCE_MEASUREMENT_BEGIN();
319        BatchNormFwd[stats_batch_idx].submit();
320        PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
321        if (this->phase_ == TRAIN && !use_global_stats_) {
322          Dtype *mean_buffer_ = (Dtype *)(mean_memory[stats_batch_idx]->get_data_handle());
323          Dtype *variance_buffer_ = (Dtype *)(variance_memory[stats_batch_idx]->get_data_handle());
324          this->blobs_[2]->mutable_cpu_data()[0] *= moving_average_fraction_;
325          this->blobs_[2]->mutable_cpu_data()[0] += 1;
326          caffe_cpu_axpby<Dtype>(channels, Dtype(1), mean_buffer_,
327              moving_average_fraction_, this->blobs_[0]->mutable_cpu_data());
328          int m = bottom[0]->count()/num_stats_batches_/channels;
329          Dtype bias_correction_factor = m > 1 ? Dtype(m)/(m-1) : 1;
330          caffe_cpu_axpby<Dtype>(channels, bias_correction_factor,
331              variance_buffer_, moving_average_fraction_,
332              this->blobs_[1]->mutable_cpu_data());
333        }
334      }
335      if(inplace && this->phase_ == TRAIN)
336          bottom[0]->data()->swap((inplace_buffer.data()));
337  }
338  template <typename Dtype>
339  void MKLDNNBatchNormLayer<Dtype>::InitBatchNormBwd(
340          const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down,
341          const vector<Blob<Dtype>*>& bottom)
342  {
343      if (std::is_same<Dtype, double>::value) NOT_IMPLEMENTED;
344      memory::format src_mfmt;
345      auto tensor_size = this->shape_.size();
346      memory::dims dim = this->shape_;
347      if(tensor_size == 5) {
348          src_mfmt = memory::format::ncdhw;
349      } else {
350          CHECK_LE(tensor_size, 4)
351              << "mkldnn batch normalization layer only supports dim size <= 5!";
352          if (tensor_size < 4) dim.resize(4, 1); 
353          src_mfmt = memory::format::nchw;
354      }
355      unsigned flags = 0;
356      if (use_weight_bias_) flags |= use_scale_shift;
357      if (use_global_stats_) flags |= use_global_stats;
358      bool top_diff_is_prv = (const_cast<Dtype*>(top[0]->prv_diff()) != NULL);
359      bool inplace = (bottom[0] == top[0]);
360      engine cpu_engine = CpuEngine::Instance().get_engine();
361      memory::data_type mpcsn = memory::data_type::f32;
362      shared_ptr<memory::desc> top_diff_md, top_diff_stats_md, top_data_md, output_stats_md;
363      shared_ptr<memory::primitive_desc> usr_diff_mpd(NULL), prv_diff_mpd(NULL);
364      if (top_diff_is_prv) {
365          shared_ptr<MKLDNNMemoryDescriptor<Dtype, true> > mem_descr
366              = get_mkldnn_prv_descriptor<Dtype, true>(top[0]);
367          top_diff_md.reset(new memory::desc(mem_descr->prv_memory_pd()->desc()));
368          usr_diff_mpd = mem_descr->usr_memory_pd();
369          prv_diff_mpd = mem_descr->prv_memory_pd();
370      } else {
371          top_diff_md.reset(new memory::desc({dim}, mpcsn, src_mfmt));   
372          usr_diff_mpd.reset(new memory::primitive_desc(*top_diff_md, cpu_engine));
373      }
374      top_diff_stats_md.reset(new memory::desc(*top_diff_md));
375      CHECK(top_diff_stats_md->data.ndims > 0 &&
376            top_diff_stats_md->data.dims[0] == this->shape_[0]);
377      top_diff_stats_md->data.dims[0] = stats_batch_size_;
378      output_stats_md.reset(new memory::desc(output_memory->get_primitive_desc().desc()));
379      CHECK(output_stats_md->data.ndims > 0 &&
380            output_stats_md->data.dims[0] == this->shape_[0]);
381      output_stats_md->data.dims[0] = stats_batch_size_;
382      batch_normalization_backward::desc BatchNormBwd_desc(prop_kind::backward,
383              *top_diff_stats_md, *output_stats_md, eps_,
384              flags);
385      std::string subengines = this->layer_param_.engine();
386      if (subengines.find("MKLDNN") == std::string::npos || subengines == "MKLDNN")
387        subengines = "MKLDNN:CPU";
388      EngineParser ep(subengines);
389      unsigned subEngineIndex = 0;
390      BatchNormBwd_pd = NULL;
391      for(; subEngineIndex < ep.getNumberOfSubEngines(); subEngineIndex++) {
392        try {
393          BatchNormBwd_pd.reset(new batch_normalization_backward::primitive_desc(
394                      BatchNormBwd_desc, ep.getMKLDNNSubEngine(subEngineIndex),
395                      *BatchNormFwd_pd));
396        }
397        catch(...) {
398          continue;
399        }
400        break;
401      }
402      CHECK(BatchNormBwd_pd);
403      if (use_weight_bias_) {
404          bwd_scaleshift_diff_memory.reset(new memory(
405                      BatchNormFwd_pd->weights_primitive_desc(), this->scaleshift_blob_->mutable_cpu_diff()));
406      }
407      bwd_top_diff.reset(new MKLDNNDiff<Dtype>(usr_diff_mpd, prv_diff_mpd, top[0], this));
408      bwd_top_diff->name = "bwd_top_diff_data   @ " + this->layer_param_.name();
409      bwd_top_diff_primitive = bwd_top_diff->create_input(false);
410      bwd_bottom_diff.reset(new MKLDNNDiff<Dtype>(usr_diff_mpd, prv_diff_mpd, bottom[0], this));
411      bwd_bottom_diff->name = "bwd_bottom_diff_data   @ " + this->layer_param_.name();
<span onclick='openModal()' class='match'>412      bwd_bottom_diff_memory = bwd_bottom_diff->create_output_memory(inplace);
413      top_diff_stats.resize(num_stats_batches_);
414      bottom_diff_stats.resize(num_stats_batches_);
</span>415      BatchNormBwd.resize(num_stats_batches_);
416      for (int i = 0; i < num_stats_batches_; i++) {
417        InitBatchNormBwdPrimitive(i);
418      }
419      MKLDNNPrimitive<Dtype> bwd_top_diff_primitive_transfer(bwd_top_diff_primitive);
420      bwd_top_diff->set_mkldnn_primitive(bwd_top_diff_primitive_transfer);
421      MKLDNNPrimitive<Dtype> bwd_bottom_diff_memory_transfer(bwd_bottom_diff_memory);
422      bwd_bottom_diff->set_mkldnn_primitive(bwd_bottom_diff_memory_transfer);
423  }
424  template <typename Dtype>
425  void MKLDNNBatchNormLayer<Dtype>::InitBatchNormBwdPrimitive(int idx) {
426      top_diff_stats[idx] = GetStatsBatchMemory<true>(bwd_top_diff, idx);
427      bottom_diff_stats[idx] = GetStatsBatchMemory<true>(bwd_bottom_diff, idx);
428      if (use_weight_bias_) {
429          BatchNormBwd[idx].reset(new batch_normalization_backward(*BatchNormBwd_pd,
430                      *input_stats[idx], *mean_memory[idx], *variance_memory[idx],
431                      *top_diff_stats[idx], *scaleshift_memory,
432                      *bottom_diff_stats[idx], *bwd_scaleshift_diff_memory));
433      } else {
434          BatchNormBwd[idx].reset(new batch_normalization_backward(*BatchNormBwd_pd,
435                      *input_stats[idx], *mean_memory[idx], *variance_memory[idx],
436                      *top_diff_stats[idx], *bottom_diff_stats[idx]));
437      }
438  }
439  template <typename Dtype>
440  void MKLDNNBatchNormLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
441          const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom)
442  {
443      VLOG(1) << "MKLDNNBatchNormLayer<Dtype>::Backward_cpu: " << this->layer_param_.name();
444  #ifdef DEBUG
445      LOG(INFO) << "MKLDNNBatchNormLayer<Dtype>::Backward_cpu: " << this->layer_param_.name();
446  #endif
447      if (BatchNormBwd_pd == NULL || this->reshape)
448          InitBatchNormBwd(top, propagate_down, bottom);
449      bwd_top_diff->sync_before_read();
450      bwd_bottom_diff->sync_before_write();
451      for (int stats_batch_idx = 0; stats_batch_idx < num_stats_batches_; stats_batch_idx++) {
452        PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKLDNN_NAME("BW"));
453        PERFORMANCE_MEASUREMENT_BEGIN();
454  #ifdef DEBUG
455        if (bottom[0]->prv_data() != NULL)
456        {
457          LOG(INFO) << "Debug: Bottom prv data: " << *bottom[0]->prv_data();
458        }
459        else
460        {
461          LOG(INFO) << "Debug: Bottom prv data is NULL!";
462        }
463        if (top[0]->prv_diff() != NULL)
464        {
465          LOG(INFO) << "Debug: Top prv diff: " << *top[0]->prv_diff();
466        }
467        else
468        {
469          LOG(INFO) << "Debug: Top prv diff is NULL!";
470          LOG(INFO) << "Debug: Top cpu diff: " << *top[0]->cpu_diff();
471        }
472  #endif
473        BatchNormBwd[stats_batch_idx].submit();
474  #ifdef DEBUG
475        if (bottom[0]->prv_diff() != NULL)
476        {
477          LOG(INFO) << "Debug: Bottom prv diff: " << *bottom[0]->prv_diff();
478        }
479        else
480        {
481          LOG(INFO) << "Debug: Bottom prv diff is NULL!";
482          LOG(INFO) << "Debug: Bottom cpu diff: " << *bottom[0]->cpu_diff();
483        }
484  #endif
485        PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
486        if (num_stats_batches_ > 1) {
487          CHECK(scaleshift_blob_ != scaleshift_acc_);
488          CHECK(scaleshift_blob_->count() == scaleshift_acc_->count());
489          caffe_cpu_axpby(scaleshift_acc_->count(), Dtype(1),
490                          scaleshift_blob_->mutable_cpu_diff(),
491                          Dtype(1), scaleshift_acc_->mutable_cpu_diff());
492        }
493      }
494  }
495  #ifdef CPU_ONLY
496  STUB_GPU(MKLDNNBatchNormLayer);
497  #else
498  template <typename Dtype>
499  void MKLDNNBatchNormLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom
500                                          ,const vector<Blob<Dtype>*>& top)
501  { NOT_IMPLEMENTED; }
502  template <typename Dtype>
503  void MKLDNNBatchNormLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top
504                                              ,const vector<bool>& propagate_down
505                                              ,const vector<Blob<Dtype>*>& bottom)
506  { NOT_IMPLEMENTED; }
507  #endif
508  INSTANTIATE_CLASS(MKLDNNBatchNormLayer);
509  }  
510  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-test-TStrPool.cpp</h3>
            <pre><code>1  #include <gtest/gtest.h>
2  #include "Snap.h"
3  TEST(TStrPool, DefaultConstructor) {
4    PStrPool Pool;
5    Pool = TStrPool::New();
6    EXPECT_EQ(1,Pool->Len());
7    EXPECT_EQ(0,Pool->Empty());
8    Pool->Clr();
9    EXPECT_EQ(0,Pool->Len());
10    EXPECT_EQ(1,Pool->Empty());
11    Pool->AddStr("");
12    EXPECT_EQ(1,Pool->Len());
13    EXPECT_EQ(0,Pool->Empty());
14  }
15  TEST(TStrPool, PoolOps) {
16    int NStr = 1000000;
17    const char *FName = "test.pool.dat";
18    int i;
19    char Str[32];
20    PStrPool Pool;
21    PStrPool Pool1;
22    PStrPool Pool2;
23    TIntV PosV;
24    uint Pos;
25    Pool = TStrPool::New();
26    for (i = 0; i < NStr; i++) {
27      sprintf(Str, "%d", i);
<span onclick='openModal()' class='match'>28      Pos = Pool->AddStr(Str);
29      PosV.Add(Pos);
30    }
</span>31    for (i = 0; i < NStr; i++) {
32      Pos = PosV[i];
33      sprintf(Str, "%d", i);
34      EXPECT_EQ(0,Pool->Cmp(Pos, Str));
35    }
36    {
37      TFOut FOut(FName);
38      Pool->Save(FOut);
39      FOut.Flush();
40    }
41    {
42      TFIn FIn(FName);
43      Pool1 = TStrPool::Load(FIn);
44    }
45    for (i = 0; i < NStr; i++) {
46      Pos = PosV[i];
47      sprintf(Str, "%d", i);
48      EXPECT_EQ(0,Pool1->Cmp(Pos, Str));
49    }
50    for (i = 0; i < NStr; i++) {
51      Pos = PosV[i];
52      EXPECT_EQ(0,Pool1->Cmp(Pos, Pool->GetStr(Pos).CStr()));
53    }
54    Pool2 = TStrPool::New();
55    *Pool2 = *Pool;
56    for (i = 0; i < NStr; i++) {
57      Pos = PosV[i];
58      sprintf(Str, "%d", i);
59      EXPECT_EQ(0,Pool2->Cmp(Pos, Str));
60    }
61    for (i = 0; i < NStr; i++) {
62      Pos = PosV[i];
63      EXPECT_EQ(0,Pool2->Cmp(Pos, Pool->GetStr(Pos).CStr()));
64    }
65    Pool->Clr();
66    EXPECT_EQ(0,Pool->Len());
67    EXPECT_EQ(1,Pool->Empty());
68    Pool->AddStr("");
69    EXPECT_EQ(1,Pool->Len());
70    EXPECT_EQ(0,Pool->Empty());
71  }
72  #if 0
73  void TestDestructor();
74  TEST(TStrPool, PoolAsserts) {
75    int i;
76    char Str[32];
77    PStrPool Pool;
78    TStrPool Tool;
79    uint Pos;
80    Pool = TStrPool::New();
81    for (i = 0; i < 10000; i++) {
82      sprintf(Str, "%d", i);
83      Pos = Pool->AddStr(Str);
84    }
85  #if 0
86    for (i = 0; ; i++) {
87      sprintf(Str, "%d", i);
88      Pos = Pool->AddStr(Str);
89    }
90  #endif
91  #if 0
92    TestDestructor();
93  #endif
94  #if 0
95    PStrPool Pool7 = PStrPool(new TStrPool(*Pool));
96  #endif
97  #if 0
98    Pool = TStrPool::New(1000000);
99  #endif
100  }
101  #if 0
102  #endif
103  void TestDestructor() {
104    int i;
105    char Str[32];
106    PStrPool Pool;
107    uint Pos;
108    Pool = TStrPool::New();
109    for (i = 0; i < 10000; i++) {
110      sprintf(Str, "%d", i);
111      Pos = Pool->AddStr(Str);
112    }
113  }
114  #endif
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_batch_norm_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-test-TStrPool.cpp</div>
                </div>
                <div class="column column_space"><pre><code>412      bwd_bottom_diff_memory = bwd_bottom_diff->create_output_memory(inplace);
413      top_diff_stats.resize(num_stats_batches_);
414      bottom_diff_stats.resize(num_stats_batches_);
</pre></code></div>
                <div class="column column_space"><pre><code>28      Pos = Pool->AddStr(Str);
29      PosV.Add(Pos);
30    }
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    