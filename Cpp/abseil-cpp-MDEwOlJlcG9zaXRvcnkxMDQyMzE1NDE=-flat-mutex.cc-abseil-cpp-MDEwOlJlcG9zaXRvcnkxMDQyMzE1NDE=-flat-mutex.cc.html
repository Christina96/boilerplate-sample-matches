
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 22, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>abseil-cpp-MDEwOlJlcG9zaXRvcnkxMDQyMzE1NDE=-flat-mutex.cc</h3>
            <pre><code>1  #include "absl/synchronization/mutex.h"
2  #ifdef _WIN32
3  #include <windows.h>
4  #ifdef ERROR
5  #undef ERROR
6  #endif
7  #else
8  #include <fcntl.h>
9  #include <pthread.h>
10  #include <sched.h>
11  #include <sys/time.h>
12  #endif
13  #include <assert.h>
14  #include <errno.h>
15  #include <stdio.h>
16  #include <stdlib.h>
17  #include <string.h>
18  #include <time.h>
19  #include <algorithm>
20  #include <atomic>
21  #include <cstddef>
22  #include <cstdlib>
23  #include <cstring>
24  #include <thread>  
25  #include "absl/base/attributes.h"
26  #include "absl/base/call_once.h"
27  #include "absl/base/config.h"
28  #include "absl/base/dynamic_annotations.h"
29  #include "absl/base/internal/atomic_hook.h"
30  #include "absl/base/internal/cycleclock.h"
31  #include "absl/base/internal/hide_ptr.h"
32  #include "absl/base/internal/low_level_alloc.h"
33  #include "absl/base/internal/raw_logging.h"
34  #include "absl/base/internal/spinlock.h"
35  #include "absl/base/internal/sysinfo.h"
36  #include "absl/base/internal/thread_identity.h"
37  #include "absl/base/internal/tsan_mutex_interface.h"
38  #include "absl/base/optimization.h"
39  #include "absl/debugging/stacktrace.h"
40  #include "absl/debugging/symbolize.h"
41  #include "absl/synchronization/internal/graphcycles.h"
42  #include "absl/synchronization/internal/per_thread_sem.h"
43  #include "absl/time/time.h"
44  using absl::base_internal::CurrentThreadIdentityIfPresent;
45  using absl::base_internal::CycleClock;
46  using absl::base_internal::PerThreadSynch;
47  using absl::base_internal::SchedulingGuard;
48  using absl::base_internal::ThreadIdentity;
49  using absl::synchronization_internal::GetOrCreateCurrentThreadIdentity;
50  using absl::synchronization_internal::GraphCycles;
51  using absl::synchronization_internal::GraphId;
52  using absl::synchronization_internal::InvalidGraphId;
53  using absl::synchronization_internal::KernelTimeout;
54  using absl::synchronization_internal::PerThreadSem;
55  extern "C" {
56  ABSL_ATTRIBUTE_WEAK void ABSL_INTERNAL_C_SYMBOL(AbslInternalMutexYield)() {
57    std::this_thread::yield();
58  }
59  }  
60  namespace absl {
61  ABSL_NAMESPACE_BEGIN
62  namespace {
63  #if defined(ABSL_HAVE_THREAD_SANITIZER)
64  constexpr OnDeadlockCycle kDeadlockDetectionDefault = OnDeadlockCycle::kIgnore;
65  #else
66  constexpr OnDeadlockCycle kDeadlockDetectionDefault = OnDeadlockCycle::kAbort;
67  #endif
68  ABSL_CONST_INIT std::atomic<OnDeadlockCycle> synch_deadlock_detection(
69      kDeadlockDetectionDefault);
70  ABSL_CONST_INIT std::atomic<bool> synch_check_invariants(false);
71  ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES
72  absl::base_internal::AtomicHook<void (*)(int64_t wait_cycles)>
73      submit_profile_data;
74  ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES absl::base_internal::AtomicHook<void (*)(
75      const char* msg, const void* obj, int64_t wait_cycles)>
76      mutex_tracer;
77  ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES
78  absl::base_internal::AtomicHook<void (*)(const char* msg, const void* cv)>
79      cond_var_tracer;
80  }  
81  static inline bool EvalConditionAnnotated(const Condition* cond, Mutex* mu,
82                                            bool locking, bool trylock,
83                                            bool read_lock);
84  void RegisterMutexProfiler(void (*fn)(int64_t wait_cycles)) {
85    submit_profile_data.Store(fn);
86  }
87  void RegisterMutexTracer(void (*fn)(const char* msg, const void* obj,
88                                      int64_t wait_cycles)) {
89    mutex_tracer.Store(fn);
90  }
91  void RegisterCondVarTracer(void (*fn)(const char* msg, const void* cv)) {
92    cond_var_tracer.Store(fn);
93  }
94  namespace {
95  enum DelayMode { AGGRESSIVE, GENTLE };
96  struct ABSL_CACHELINE_ALIGNED MutexGlobals {
97    absl::once_flag once;
98    int spinloop_iterations = 0;
99    int32_t mutex_sleep_spins[2] = {};
100    absl::Duration mutex_sleep_time;
101  };
102  absl::Duration MeasureTimeToYield() {
103    absl::Time before = absl::Now();
104    ABSL_INTERNAL_C_SYMBOL(AbslInternalMutexYield)();
105    return absl::Now() - before;
106  }
107  const MutexGlobals& GetMutexGlobals() {
108    ABSL_CONST_INIT static MutexGlobals data;
109    absl::base_internal::LowLevelCallOnce(&data.once, [&]() {
110      if (absl::base_internal::NumCPUs() > 1) {
111        data.spinloop_iterations = 1500;
112        data.mutex_sleep_spins[AGGRESSIVE] = 5000;
113        data.mutex_sleep_spins[GENTLE] = 250;
114        data.mutex_sleep_time = absl::Microseconds(10);
115      } else {
116        data.spinloop_iterations = 0;
117        data.mutex_sleep_spins[AGGRESSIVE] = 0;
118        data.mutex_sleep_spins[GENTLE] = 0;
119        data.mutex_sleep_time = MeasureTimeToYield() * 5;
120        data.mutex_sleep_time =
121            std::min(data.mutex_sleep_time, absl::Milliseconds(1));
122        data.mutex_sleep_time =
123            std::max(data.mutex_sleep_time, absl::Microseconds(10));
124      }
125    });
126    return data;
127  }
128  }  
129  namespace synchronization_internal {
130  int MutexDelay(int32_t c, int mode) {
131    const int32_t limit = GetMutexGlobals().mutex_sleep_spins[mode];
132    const absl::Duration sleep_time = GetMutexGlobals().mutex_sleep_time;
133    if (c < limit) {
134      c++;
135    } else {
136      SchedulingGuard::ScopedEnable enable_rescheduling;
137      ABSL_TSAN_MUTEX_PRE_DIVERT(nullptr, 0);
138      if (c == limit) {
139        ABSL_INTERNAL_C_SYMBOL(AbslInternalMutexYield)();
140        c++;
141      } else {
142        absl::SleepFor(sleep_time);
143        c = 0;
144      }
145      ABSL_TSAN_MUTEX_POST_DIVERT(nullptr, 0);
146    }
147    return c;
148  }
149  }  
150  static void AtomicSetBits(std::atomic<intptr_t>* pv, intptr_t bits,
151                            intptr_t wait_until_clear) {
152    intptr_t v;
153    do {
154      v = pv->load(std::memory_order_relaxed);
155    } while ((v & bits) != bits &&
156             ((v & wait_until_clear) != 0 ||
157              !pv->compare_exchange_weak(v, v | bits, std::memory_order_release,
158                                         std::memory_order_relaxed)));
159  }
160  static void AtomicClearBits(std::atomic<intptr_t>* pv, intptr_t bits,
161                              intptr_t wait_until_clear) {
162    intptr_t v;
163    do {
164      v = pv->load(std::memory_order_relaxed);
165    } while ((v & bits) != 0 &&
166             ((v & wait_until_clear) != 0 ||
167              !pv->compare_exchange_weak(v, v & ~bits, std::memory_order_release,
168                                         std::memory_order_relaxed)));
169  }
170  ABSL_CONST_INIT static absl::base_internal::SpinLock deadlock_graph_mu(
171      absl::kConstInit, base_internal::SCHEDULE_KERNEL_ONLY);
172  ABSL_CONST_INIT static GraphCycles* deadlock_graph
173      ABSL_GUARDED_BY(deadlock_graph_mu) ABSL_PT_GUARDED_BY(deadlock_graph_mu);
174  namespace {  
175  enum {       
176    SYNCH_EV_TRYLOCK_SUCCESS,
177    SYNCH_EV_TRYLOCK_FAILED,
178    SYNCH_EV_READERTRYLOCK_SUCCESS,
179    SYNCH_EV_READERTRYLOCK_FAILED,
180    SYNCH_EV_LOCK,
181    SYNCH_EV_LOCK_RETURNING,
182    SYNCH_EV_READERLOCK,
183    SYNCH_EV_READERLOCK_RETURNING,
184    SYNCH_EV_UNLOCK,
185    SYNCH_EV_READERUNLOCK,
186    SYNCH_EV_WAIT,
187    SYNCH_EV_WAIT_RETURNING,
188    SYNCH_EV_SIGNAL,
189    SYNCH_EV_SIGNALALL,
190  };
191  enum {                    
192    SYNCH_F_R = 0x01,       
193    SYNCH_F_LCK = 0x02,     
194    SYNCH_F_TRY = 0x04,     
195    SYNCH_F_UNLOCK = 0x08,  
196    SYNCH_F_LCK_W = SYNCH_F_LCK,
197    SYNCH_F_LCK_R = SYNCH_F_LCK | SYNCH_F_R,
198  };
199  }  
200  static const struct {
201    int flags;
202    const char* msg;
203  } event_properties[] = {
204      {SYNCH_F_LCK_W | SYNCH_F_TRY, "TryLock succeeded "},
205      {0, "TryLock failed "},
206      {SYNCH_F_LCK_R | SYNCH_F_TRY, "ReaderTryLock succeeded "},
207      {0, "ReaderTryLock failed "},
208      {0, "Lock blocking "},
209      {SYNCH_F_LCK_W, "Lock returning "},
210      {0, "ReaderLock blocking "},
211      {SYNCH_F_LCK_R, "ReaderLock returning "},
212      {SYNCH_F_LCK_W | SYNCH_F_UNLOCK, "Unlock "},
213      {SYNCH_F_LCK_R | SYNCH_F_UNLOCK, "ReaderUnlock "},
214      {0, "Wait on "},
215      {0, "Wait unblocked "},
216      {0, "Signal on "},
217      {0, "SignalAll on "},
218  };
219  ABSL_CONST_INIT static absl::base_internal::SpinLock synch_event_mu(
220      absl::kConstInit, base_internal::SCHEDULE_KERNEL_ONLY);
221  static constexpr uint32_t kNSynchEvent = 1031;
222  static struct SynchEvent {  
223    int refcount ABSL_GUARDED_BY(synch_event_mu);
224    SynchEvent* next ABSL_GUARDED_BY(synch_event_mu);
225    uintptr_t masked_addr;  
226    void (*invariant)(void* arg);  
227    void* arg;                     
228    bool log;                      
229    char name[1];  
230  }* synch_event[kNSynchEvent] ABSL_GUARDED_BY(synch_event_mu);
231  static SynchEvent* EnsureSynchEvent(std::atomic<intptr_t>* addr,
232                                      const char* name, intptr_t bits,
233                                      intptr_t lockbit) {
234    uint32_t h = reinterpret_cast<uintptr_t>(addr) % kNSynchEvent;
235    SynchEvent* e;
236    synch_event_mu.Lock();
237    for (e = synch_event[h];
238         e != nullptr && e->masked_addr != base_internal::HidePtr(addr);
239         e = e->next) {
240    }
241    if (e == nullptr) {  
242      if (name == nullptr) {
243        name = "";
244      }
245      size_t l = strlen(name);
246      e = reinterpret_cast<SynchEvent*>(
247          base_internal::LowLevelAlloc::Alloc(sizeof(*e) + l));
248      e->refcount = 2;  
249      e->masked_addr = base_internal::HidePtr(addr);
250      e->invariant = nullptr;
251      e->arg = nullptr;
252      e->log = false;
253      strcpy(e->name, name);  
254      e->next = synch_event[h];
255      AtomicSetBits(addr, bits, lockbit);
256      synch_event[h] = e;
257    } else {
258      e->refcount++;  
259    }
260    synch_event_mu.Unlock();
261    return e;
262  }
263  static void DeleteSynchEvent(SynchEvent* e) {
264    base_internal::LowLevelAlloc::Free(e);
265  }
266  static void UnrefSynchEvent(SynchEvent* e) {
267    if (e != nullptr) {
268      synch_event_mu.Lock();
269      bool del = (--(e->refcount) == 0);
270      synch_event_mu.Unlock();
271      if (del) {
272        DeleteSynchEvent(e);
273      }
274    }
275  }
276  static void ForgetSynchEvent(std::atomic<intptr_t>* addr, intptr_t bits,
277                               intptr_t lockbit) {
278    uint32_t h = reinterpret_cast<uintptr_t>(addr) % kNSynchEvent;
279    SynchEvent** pe;
280    SynchEvent* e;
281    synch_event_mu.Lock();
282    for (pe = &synch_event[h];
283         (e = *pe) != nullptr && e->masked_addr != base_internal::HidePtr(addr);
284         pe = &e->next) {
285    }
286    bool del = false;
287    if (e != nullptr) {
288      *pe = e->next;
289      del = (--(e->refcount) == 0);
290    }
291    AtomicClearBits(addr, bits, lockbit);
292    synch_event_mu.Unlock();
293    if (del) {
294      DeleteSynchEvent(e);
295    }
296  }
297  static SynchEvent* GetSynchEvent(const void* addr) {
298    uint32_t h = reinterpret_cast<uintptr_t>(addr) % kNSynchEvent;
299    SynchEvent* e;
300    synch_event_mu.Lock();
301    for (e = synch_event[h];
302         e != nullptr && e->masked_addr != base_internal::HidePtr(addr);
303         e = e->next) {
304    }
305    if (e != nullptr) {
306      e->refcount++;
307    }
308    synch_event_mu.Unlock();
309    return e;
310  }
311  static void PostSynchEvent(void* obj, int ev) {
312    SynchEvent* e = GetSynchEvent(obj);
313    if (e == nullptr || e->log) {
314      void* pcs[40];
315      int n = absl::GetStackTrace(pcs, ABSL_ARRAYSIZE(pcs), 1);
316      char buffer[ABSL_ARRAYSIZE(pcs) * 24];
317      int pos = snprintf(buffer, sizeof(buffer), " @");
318      for (int i = 0; i != n; i++) {
319        int b = snprintf(&buffer[pos], sizeof(buffer) - static_cast<size_t>(pos),
320                         " %p", pcs[i]);
321        if (b < 0 ||
322            static_cast<size_t>(b) >= sizeof(buffer) - static_cast<size_t>(pos)) {
323          break;
324        }
325        pos += b;
326      }
327      ABSL_RAW_LOG(INFO, "%s%p %s %s", event_properties[ev].msg, obj,
328                   (e == nullptr ? "" : e->name), buffer);
329    }
330    const int flags = event_properties[ev].flags;
331    if ((flags & SYNCH_F_LCK) != 0 && e != nullptr && e->invariant != nullptr) {
332      struct local {
333        static bool pred(SynchEvent* ev) {
334          (*ev->invariant)(ev->arg);
335          return false;
336        }
337      };
338      Condition cond(&local::pred, e);
339      Mutex* mu = static_cast<Mutex*>(obj);
340      const bool locking = (flags & SYNCH_F_UNLOCK) == 0;
341      const bool trylock = (flags & SYNCH_F_TRY) != 0;
342      const bool read_lock = (flags & SYNCH_F_R) != 0;
343      EvalConditionAnnotated(&cond, mu, locking, trylock, read_lock);
344    }
345    UnrefSynchEvent(e);
346  }
347  struct SynchWaitParams {
348    SynchWaitParams(Mutex::MuHow how_arg, const Condition* cond_arg,
349                    KernelTimeout timeout_arg, Mutex* cvmu_arg,
350                    PerThreadSynch* thread_arg,
351                    std::atomic<intptr_t>* cv_word_arg)
352        : how(how_arg),
353          cond(cond_arg),
354          timeout(timeout_arg),
355          cvmu(cvmu_arg),
356          thread(thread_arg),
357          cv_word(cv_word_arg),
358          contention_start_cycles(CycleClock::Now()),
359          should_submit_contention_data(false) {}
360    const Mutex::MuHow how;  
361    const Condition* cond;   
362    KernelTimeout timeout;  
363    Mutex* const cvmu;      
364    PerThreadSynch* const thread;  
365    std::atomic<intptr_t>* cv_word;
366    int64_t contention_start_cycles;  
367    bool should_submit_contention_data;
368  };
369  struct SynchLocksHeld {
370    int n;          
371    bool overflow;  
372    struct {
373      Mutex* mu;      
374      int32_t count;  
375      GraphId id;     
376    } locks[40];
377  };
378  static PerThreadSynch* const kPerThreadSynchNull =
379      reinterpret_cast<PerThreadSynch*>(1);
380  static SynchLocksHeld* LocksHeldAlloc() {
381    SynchLocksHeld* ret = reinterpret_cast<SynchLocksHeld*>(
382        base_internal::LowLevelAlloc::Alloc(sizeof(SynchLocksHeld)));
383    ret->n = 0;
384    ret->overflow = false;
385    return ret;
386  }
387  static PerThreadSynch* Synch_GetPerThread() {
388    ThreadIdentity* identity = GetOrCreateCurrentThreadIdentity();
389    return &identity->per_thread_synch;
390  }
391  static PerThreadSynch* Synch_GetPerThreadAnnotated(Mutex* mu) {
392    if (mu) {
393      ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
394    }
395    PerThreadSynch* w = Synch_GetPerThread();
396    if (mu) {
397      ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
398    }
399    return w;
400  }
401  static SynchLocksHeld* Synch_GetAllLocks() {
402    PerThreadSynch* s = Synch_GetPerThread();
403    if (s->all_locks == nullptr) {
404      s->all_locks = LocksHeldAlloc();  
405    }
406    return s->all_locks;
407  }
408  void Mutex::IncrementSynchSem(Mutex* mu, PerThreadSynch* w) {
409    if (mu) {
410      ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
411      ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_BEGIN();
412      PerThreadSem::Post(w->thread_identity());
413      ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_END();
414      ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
415    } else {
416      PerThreadSem::Post(w->thread_identity());
417    }
418  }
419  bool Mutex::DecrementSynchSem(Mutex* mu, PerThreadSynch* w, KernelTimeout t) {
420    if (mu) {
421      ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
422    }
423    assert(w == Synch_GetPerThread());
424    static_cast<void>(w);
425    bool res = PerThreadSem::Wait(t);
426    if (mu) {
427      ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
428    }
429    return res;
430  }
431  void Mutex::InternalAttemptToUseMutexInFatalSignalHandler() {
432    ThreadIdentity* identity = CurrentThreadIdentityIfPresent();
433    if (identity != nullptr) {
434      identity->per_thread_synch.suppress_fatal_errors = true;
435    }
436    synch_deadlock_detection.store(OnDeadlockCycle::kIgnore,
437                                   std::memory_order_release);
438  }
439  static const intptr_t kMuReader = 0x0001L;  
440  static const intptr_t kMuDesig = 0x0002L;
441  static const intptr_t kMuWait = 0x0004L;    
442  static const intptr_t kMuWriter = 0x0008L;  
443  static const intptr_t kMuEvent = 0x0010L;   
444  static const intptr_t kMuWrWait = 0x0020L;
445  static const intptr_t kMuSpin = 0x0040L;  
446  static const intptr_t kMuLow = 0x00ffL;   
447  static const intptr_t kMuHigh = ~kMuLow;  
448  enum {
449    kGdbMuSpin = kMuSpin,
450    kGdbMuEvent = kMuEvent,
451    kGdbMuWait = kMuWait,
452    kGdbMuWriter = kMuWriter,
453    kGdbMuDesig = kMuDesig,
454    kGdbMuWrWait = kMuWrWait,
455    kGdbMuReader = kMuReader,
456    kGdbMuLow = kMuLow,
457  };
458  static const intptr_t kMuOne = 0x0100;  
459  static const int kMuHasBlocked = 0x01;  
460  static const int kMuIsCond = 0x02;      
461  static_assert(PerThreadSynch::kAlignment > kMuLow,
462                "PerThreadSynch::kAlignment must be greater than kMuLow");
463  struct MuHowS {
464    intptr_t fast_need_zero;
465    intptr_t fast_or;
466    intptr_t fast_add;
467    intptr_t slow_need_zero;  
468    intptr_t slow_inc_need_zero;  
469  };
470  static const MuHowS kSharedS = {
471      kMuWriter | kMuWait | kMuEvent,   
472      kMuReader,                        
473      kMuOne,                           
474      kMuWriter | kMuWait,              
475      kMuSpin | kMuWriter | kMuWrWait,  
476  };
477  static const MuHowS kExclusiveS = {
478      kMuWriter | kMuReader | kMuEvent,  
479      kMuWriter,                         
480      0,                                 
481      kMuWriter | kMuReader,             
482      ~static_cast<intptr_t>(0),         
483  };
484  static const Mutex::MuHow kShared = &kSharedS;        
485  static const Mutex::MuHow kExclusive = &kExclusiveS;  
486  #ifdef NDEBUG
487  static constexpr bool kDebugMode = false;
488  #else
489  static constexpr bool kDebugMode = true;
490  #endif
491  #ifdef ABSL_INTERNAL_HAVE_TSAN_INTERFACE
492  static unsigned TsanFlags(Mutex::MuHow how) {
493    return how == kShared ? __tsan_mutex_read_lock : 0;
494  }
495  #endif
496  static bool DebugOnlyIsExiting() {
497    return false;
498  }
499  Mutex::~Mutex() {
500    intptr_t v = mu_.load(std::memory_order_relaxed);
501    if ((v & kMuEvent) != 0 && !DebugOnlyIsExiting()) {
502      ForgetSynchEvent(&this->mu_, kMuEvent, kMuSpin);
503    }
504    if (kDebugMode) {
505      this->ForgetDeadlockInfo();
506    }
507    ABSL_TSAN_MUTEX_DESTROY(this, __tsan_mutex_not_static);
508  }
509  void Mutex::EnableDebugLog(const char* name) {
510    SynchEvent* e = EnsureSynchEvent(&this->mu_, name, kMuEvent, kMuSpin);
511    e->log = true;
512    UnrefSynchEvent(e);
513  }
514  void EnableMutexInvariantDebugging(bool enabled) {
515    synch_check_invariants.store(enabled, std::memory_order_release);
516  }
517  void Mutex::EnableInvariantDebugging(void (*invariant)(void*), void* arg) {
518    if (synch_check_invariants.load(std::memory_order_acquire) &&
519        invariant != nullptr) {
520      SynchEvent* e = EnsureSynchEvent(&this->mu_, nullptr, kMuEvent, kMuSpin);
521      e->invariant = invariant;
522      e->arg = arg;
523      UnrefSynchEvent(e);
524    }
525  }
526  void SetMutexDeadlockDetectionMode(OnDeadlockCycle mode) {
527    synch_deadlock_detection.store(mode, std::memory_order_release);
528  }
529  static bool MuEquivalentWaiter(PerThreadSynch* x, PerThreadSynch* y) {
530    return x->waitp->how == y->waitp->how && x->priority == y->priority &&
531           Condition::GuaranteedEqual(x->waitp->cond, y->waitp->cond);
532  }
533  static inline PerThreadSynch* GetPerThreadSynch(intptr_t v) {
534    return reinterpret_cast<PerThreadSynch*>(v & kMuHigh);
535  }
536  static PerThreadSynch* Skip(PerThreadSynch* x) {
537    PerThreadSynch* x0 = nullptr;
538    PerThreadSynch* x1 = x;
539    PerThreadSynch* x2 = x->skip;
540    if (x2 != nullptr) {
541      while ((x0 = x1, x1 = x2, x2 = x2->skip) != nullptr) {
542        x0->skip = x2;  
543      }
544      x->skip = x1;  
545    }
546    return x1;
547  }
548  static void FixSkip(PerThreadSynch* ancestor, PerThreadSynch* to_be_removed) {
549    if (ancestor->skip == to_be_removed) {  
550      if (to_be_removed->skip != nullptr) {
551        ancestor->skip = to_be_removed->skip;  
552      } else if (ancestor->next != to_be_removed) {  
553        ancestor->skip = ancestor->next;             
554      } else {
555        ancestor->skip = nullptr;  
556      }
557    }
558  }
559  static void CondVarEnqueue(SynchWaitParams* waitp);
560  static PerThreadSynch* Enqueue(PerThreadSynch* head, SynchWaitParams* waitp,
561                                 intptr_t mu, int flags) {
562    if (waitp->cv_word != nullptr) {
563      CondVarEnqueue(waitp);
564      return head;
565    }
566    PerThreadSynch* s = waitp->thread;
567    ABSL_RAW_CHECK(
568        s->waitp == nullptr ||    
569            s->waitp == waitp ||  
570            s->suppress_fatal_errors,
571        "detected illegal recursion into Mutex code");
572    s->waitp = waitp;
573    s->skip = nullptr;   
574    s->may_skip = true;  
575    s->wake = false;     
576    s->cond_waiter = ((flags & kMuIsCond) != 0);
577  #ifdef ABSL_HAVE_PTHREAD_GETSCHEDPARAM
578    int64_t now_cycles = CycleClock::Now();
579    if (s->next_priority_read_cycles < now_cycles) {
580      int policy;
581      struct sched_param param;
582      const int err = pthread_getschedparam(pthread_self(), &policy, &param);
583      if (err != 0) {
584        ABSL_RAW_LOG(ERROR, "pthread_getschedparam failed: %d", err);
585      } else {
586        s->priority = param.sched_priority;
587        s->next_priority_read_cycles =
588            now_cycles + static_cast<int64_t>(CycleClock::Frequency());
589      }
590    }
591  #endif
592    if (head == nullptr) {         
593      s->next = s;                 
594      s->readers = mu;             
595      s->maybe_unlocking = false;  
596      head = s;                    
597    } else {
598      PerThreadSynch* enqueue_after = nullptr;  
599  #ifdef ABSL_HAVE_PTHREAD_GETSCHEDPARAM
600      if (s->priority > head->priority) {  
601        if (!head->maybe_unlocking) {
602          PerThreadSynch* advance_to = head;  
603          do {
604            enqueue_after = advance_to;
605            advance_to = Skip(enqueue_after->next);
606          } while (s->priority <= advance_to->priority);
607        } else if (waitp->how == kExclusive &&
608                   Condition::GuaranteedEqual(waitp->cond, nullptr)) {
609          enqueue_after = head;  
610        }
611      }
612  #endif
613      if (enqueue_after != nullptr) {
614        s->next = enqueue_after->next;
615        enqueue_after->next = s;
616        ABSL_RAW_CHECK(enqueue_after->skip == nullptr ||
617                           MuEquivalentWaiter(enqueue_after, s),
618                       "Mutex Enqueue failure");
619        if (enqueue_after != head && enqueue_after->may_skip &&
620            MuEquivalentWaiter(enqueue_after, enqueue_after->next)) {
621          enqueue_after->skip = enqueue_after->next;
622        }
623        if (MuEquivalentWaiter(s, s->next)) {  
624          s->skip = s->next;                   
625        }
626      } else {  
627        s->next = head->next;  
628        head->next = s;
629        s->readers = head->readers;  
630        s->maybe_unlocking = head->maybe_unlocking;  
631        if (head->may_skip && MuEquivalentWaiter(head, s)) {
632          head->skip = s;
633        }
634        head = s;  
635      }
636    }
637    s->state.store(PerThreadSynch::kQueued, std::memory_order_relaxed);
638    return head;
639  }
640  static PerThreadSynch* Dequeue(PerThreadSynch* head, PerThreadSynch* pw) {
641    PerThreadSynch* w = pw->next;
642    pw->next = w->next;                 
643    if (head == w) {                    
644      head = (pw == w) ? nullptr : pw;  
645    } else if (pw != head && MuEquivalentWaiter(pw, pw->next)) {
646      if (pw->next->skip !=
647          nullptr) {  
648        pw->skip = pw->next->skip;
649      } else {  
650        pw->skip = pw->next;
651      }
652    }
653    return head;
654  }
655  static PerThreadSynch* DequeueAllWakeable(PerThreadSynch* head,
656                                            PerThreadSynch* pw,
657                                            PerThreadSynch** wake_tail) {
658    PerThreadSynch* orig_h = head;
659    PerThreadSynch* w = pw->next;
660    bool skipped = false;
661    do {
662      if (w->wake) {  
663        ABSL_RAW_CHECK(pw->skip == nullptr, "bad skip in DequeueAllWakeable");
664        head = Dequeue(head, pw);
665        w->next = *wake_tail;               
666        *wake_tail = w;                     
667        wake_tail = &w->next;               
668        if (w->waitp->how == kExclusive) {  
669          break;
670        }
671      } else {         
672        pw = Skip(w);  
673        skipped = true;
674      }
675      w = pw->next;
676    } while (orig_h == head && (pw != head || !skipped));
677    return head;
678  }
679  void Mutex::TryRemove(PerThreadSynch* s) {
680    SchedulingGuard::ScopedDisable disable_rescheduling;
681    intptr_t v = mu_.load(std::memory_order_relaxed);
682    if ((v & (kMuWait | kMuSpin | kMuWriter | kMuReader)) == kMuWait &&
683        mu_.compare_exchange_strong(v, v | kMuSpin | kMuWriter,
684                                    std::memory_order_acquire,
685                                    std::memory_order_relaxed)) {
686      PerThreadSynch* h = GetPerThreadSynch(v);
687      if (h != nullptr) {
688        PerThreadSynch* pw = h;  
689        PerThreadSynch* w;
690        if ((w = pw->next) != s) {  
691          do {                      
692            if (!MuEquivalentWaiter(s, w)) {
693              pw = Skip(w);  
694            } else {          
695              FixSkip(w, s);  
696              pw = w;
697            }
698          } while ((w = pw->next) != s && pw != h);
699        }
700        if (w == s) {  
701          h = Dequeue(h, pw);
702          s->next = nullptr;
703          s->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
704        }
705      }
706      intptr_t nv;
707      do {  
708        v = mu_.load(std::memory_order_relaxed);
709        nv = v & (kMuDesig | kMuEvent);
710        if (h != nullptr) {
711          nv |= kMuWait | reinterpret_cast<intptr_t>(h);
712          h->readers = 0;              
713          h->maybe_unlocking = false;  
714        }
715      } while (!mu_.compare_exchange_weak(v, nv, std::memory_order_release,
716                                          std::memory_order_relaxed));
717    }
718  }
719  void Mutex::Block(PerThreadSynch* s) {
720    while (s->state.load(std::memory_order_acquire) == PerThreadSynch::kQueued) {
721      if (!DecrementSynchSem(this, s, s->waitp->timeout)) {
722        this->TryRemove(s);
723        int c = 0;
724        while (s->next != nullptr) {
725          c = synchronization_internal::MutexDelay(c, GENTLE);
726          this->TryRemove(s);
727        }
728        if (kDebugMode) {
729          this->TryRemove(s);
730        }
731        s->waitp->timeout = KernelTimeout::Never();  
732        s->waitp->cond = nullptr;  
733      }
734    }
735    ABSL_RAW_CHECK(s->waitp != nullptr || s->suppress_fatal_errors,
736                   "detected illegal recursion in Mutex code");
737    s->waitp = nullptr;
738  }
739  PerThreadSynch* Mutex::Wakeup(PerThreadSynch* w) {
740    PerThreadSynch* next = w->next;
741    w->next = nullptr;
742    w->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
743    IncrementSynchSem(this, w);
744    return next;
745  }
746  static GraphId GetGraphIdLocked(Mutex* mu)
747      ABSL_EXCLUSIVE_LOCKS_REQUIRED(deadlock_graph_mu) {
748    if (!deadlock_graph) {  
749      deadlock_graph =
750          new (base_internal::LowLevelAlloc::Alloc(sizeof(*deadlock_graph)))
751              GraphCycles;
752    }
753    return deadlock_graph->GetId(mu);
754  }
755  static GraphId GetGraphId(Mutex* mu) ABSL_LOCKS_EXCLUDED(deadlock_graph_mu) {
756    deadlock_graph_mu.Lock();
757    GraphId id = GetGraphIdLocked(mu);
758    deadlock_graph_mu.Unlock();
759    return id;
760  }
761  static void LockEnter(Mutex* mu, GraphId id, SynchLocksHeld* held_locks) {
762    int n = held_locks->n;
763    int i = 0;
764    while (i != n && held_locks->locks[i].id != id) {
765      i++;
766    }
767    if (i == n) {
768      if (n == ABSL_ARRAYSIZE(held_locks->locks)) {
769        held_locks->overflow = true;  
770      } else {                        
771        held_locks->locks[i].mu = mu;
772        held_locks->locks[i].count = 1;
773        held_locks->locks[i].id = id;
774        held_locks->n = n + 1;
775      }
776    } else {
777      held_locks->locks[i].count++;
778    }
779  }
780  static void LockLeave(Mutex* mu, GraphId id, SynchLocksHeld* held_locks) {
781    int n = held_locks->n;
782    int i = 0;
783    while (i != n && held_locks->locks[i].id != id) {
784      i++;
785    }
786    if (i == n) {
787      if (!held_locks->overflow) {
788        i = 0;
789        while (i != n && held_locks->locks[i].mu != mu) {
790          i++;
791        }
792        if (i == n) {  
793          SynchEvent* mu_events = GetSynchEvent(mu);
794          ABSL_RAW_LOG(FATAL,
795                       "thread releasing lock it does not hold: %p %s; "
796                       ,
797                       static_cast<void*>(mu),
798                       mu_events == nullptr ? "" : mu_events->name);
799        }
800      }
801    } else if (held_locks->locks[i].count == 1) {
802      held_locks->n = n - 1;
803      held_locks->locks[i] = held_locks->locks[n - 1];
804      held_locks->locks[n - 1].id = InvalidGraphId();
805      held_locks->locks[n - 1].mu =
806          nullptr;  
807    } else {
808      assert(held_locks->locks[i].count > 0);
809      held_locks->locks[i].count--;
810    }
811  }
812  static inline void DebugOnlyLockEnter(Mutex* mu) {
813    if (kDebugMode) {
814      if (synch_deadlock_detection.load(std::memory_order_acquire) !=
815          OnDeadlockCycle::kIgnore) {
816        LockEnter(mu, GetGraphId(mu), Synch_GetAllLocks());
817      }
818    }
819  }
820  static inline void DebugOnlyLockEnter(Mutex* mu, GraphId id) {
821    if (kDebugMode) {
822      if (synch_deadlock_detection.load(std::memory_order_acquire) !=
823          OnDeadlockCycle::kIgnore) {
824        LockEnter(mu, id, Synch_GetAllLocks());
825      }
826    }
827  }
828  static inline void DebugOnlyLockLeave(Mutex* mu) {
829    if (kDebugMode) {
830      if (synch_deadlock_detection.load(std::memory_order_acquire) !=
831          OnDeadlockCycle::kIgnore) {
832        LockLeave(mu, GetGraphId(mu), Synch_GetAllLocks());
833      }
834    }
835  }
836  static char* StackString(void** pcs, int n, char* buf, int maxlen,
837                           bool symbolize) {
838    static constexpr int kSymLen = 200;
839    char sym[kSymLen];
840    int len = 0;
841    for (int i = 0; i != n; i++) {
842      if (len >= maxlen)
843        return buf;
844      size_t count = static_cast<size_t>(maxlen - len);
845      if (symbolize) {
846        if (!absl::Symbolize(pcs[i], sym, kSymLen)) {
847          sym[0] = '\0';
848        }
849        snprintf(buf + len, count, "%s\t@ %p %s\n", (i == 0 ? "\n" : ""), pcs[i],
850                 sym);
851      } else {
852        snprintf(buf + len, count, " %p", pcs[i]);
853      }
854      len += strlen(&buf[len]);
855    }
856    return buf;
857  }
858  static char* CurrentStackString(char* buf, int maxlen, bool symbolize) {
859    void* pcs[40];
860    return StackString(pcs, absl::GetStackTrace(pcs, ABSL_ARRAYSIZE(pcs), 2), buf,
861                       maxlen, symbolize);
862  }
863  namespace {
864  enum {
865    kMaxDeadlockPathLen = 10
866  };  
867  struct DeadlockReportBuffers {
868    char buf[6100];
869    GraphId path[kMaxDeadlockPathLen];
870  };
871  struct ScopedDeadlockReportBuffers {
872    ScopedDeadlockReportBuffers() {
873      b = reinterpret_cast<DeadlockReportBuffers*>(
874          base_internal::LowLevelAlloc::Alloc(sizeof(*b)));
875    }
876    ~ScopedDeadlockReportBuffers() { base_internal::LowLevelAlloc::Free(b); }
877    DeadlockReportBuffers* b;
878  };
879  int GetStack(void** stack, int max_depth) {
880    return absl::GetStackTrace(stack, max_depth, 3);
881  }
882  }  
883  static GraphId DeadlockCheck(Mutex* mu) {
884    if (synch_deadlock_detection.load(std::memory_order_acquire) ==
885        OnDeadlockCycle::kIgnore) {
886      return InvalidGraphId();
887    }
888    SynchLocksHeld* all_locks = Synch_GetAllLocks();
889    absl::base_internal::SpinLockHolder lock(&deadlock_graph_mu);
890    const GraphId mu_id = GetGraphIdLocked(mu);
891    if (all_locks->n == 0) {
892      return mu_id;
893    }
894    deadlock_graph->UpdateStackTrace(mu_id, all_locks->n + 1, GetStack);
895    for (int i = 0; i != all_locks->n; i++) {
896      const GraphId other_node_id = all_locks->locks[i].id;
897      const Mutex* other =
898          static_cast<const Mutex*>(deadlock_graph->Ptr(other_node_id));
899      if (other == nullptr) {
900        continue;
901      }
902      if (!deadlock_graph->InsertEdge(other_node_id, mu_id)) {
903        ScopedDeadlockReportBuffers scoped_buffers;
904        DeadlockReportBuffers* b = scoped_buffers.b;
905        static int number_of_reported_deadlocks = 0;
906        number_of_reported_deadlocks++;
907        bool symbolize = number_of_reported_deadlocks <= 2;
908        ABSL_RAW_LOG(ERROR, "Potential Mutex deadlock: %s",
909                     CurrentStackString(b->buf, sizeof (b->buf), symbolize));
910        size_t len = 0;
911        for (int j = 0; j != all_locks->n; j++) {
912          void* pr = deadlock_graph->Ptr(all_locks->locks[j].id);
913          if (pr != nullptr) {
914            snprintf(b->buf + len, sizeof(b->buf) - len, " %p", pr);
915            len += strlen(&b->buf[len]);
916          }
917        }
918        ABSL_RAW_LOG(ERROR,
919                     "Acquiring absl::Mutex %p while holding %s; a cycle in the "
920                     "historical lock ordering graph has been observed",
921                     static_cast<void*>(mu), b->buf);
922        ABSL_RAW_LOG(ERROR, "Cycle: ");
923        int path_len = deadlock_graph->FindPath(mu_id, other_node_id,
924                                                ABSL_ARRAYSIZE(b->path), b->path);
925        for (int j = 0; j != path_len && j != ABSL_ARRAYSIZE(b->path); j++) {
926          GraphId id = b->path[j];
927          Mutex* path_mu = static_cast<Mutex*>(deadlock_graph->Ptr(id));
928          if (path_mu == nullptr) continue;
929          void** stack;
930          int depth = deadlock_graph->GetStackTrace(id, &stack);
931          snprintf(b->buf, sizeof(b->buf),
932                   "mutex@%p stack: ", static_cast<void*>(path_mu));
933          StackString(stack, depth, b->buf + strlen(b->buf),
934                      static_cast<int>(sizeof(b->buf) - strlen(b->buf)),
935                      symbolize);
936          ABSL_RAW_LOG(ERROR, "%s", b->buf);
937        }
938        if (path_len > static_cast<int>(ABSL_ARRAYSIZE(b->path))) {
939          ABSL_RAW_LOG(ERROR, "(long cycle; list truncated)");
940        }
941        if (synch_deadlock_detection.load(std::memory_order_acquire) ==
942            OnDeadlockCycle::kAbort) {
943          deadlock_graph_mu.Unlock();  
944          ABSL_RAW_LOG(FATAL, "dying due to potential deadlock");
945          return mu_id;
946        }
947        break;  
948      }
949    }
950    return mu_id;
951  }
952  static inline GraphId DebugOnlyDeadlockCheck(Mutex* mu) {
953    if (kDebugMode && synch_deadlock_detection.load(std::memory_order_acquire) !=
954                          OnDeadlockCycle::kIgnore) {
955      return DeadlockCheck(mu);
956    } else {
957      return InvalidGraphId();
958    }
959  }
960  void Mutex::ForgetDeadlockInfo() {
961    if (kDebugMode && synch_deadlock_detection.load(std::memory_order_acquire) !=
962                          OnDeadlockCycle::kIgnore) {
963      deadlock_graph_mu.Lock();
964      if (deadlock_graph != nullptr) {
965        deadlock_graph->RemoveNode(this);
966      }
967      deadlock_graph_mu.Unlock();
968    }
969  }
970  void Mutex::AssertNotHeld() const {
971    if (kDebugMode &&
972        (mu_.load(std::memory_order_relaxed) & (kMuWriter | kMuReader)) != 0 &&
973        synch_deadlock_detection.load(std::memory_order_acquire) !=
974            OnDeadlockCycle::kIgnore) {
975      GraphId id = GetGraphId(const_cast<Mutex*>(this));
976      SynchLocksHeld* locks = Synch_GetAllLocks();
977      for (int i = 0; i != locks->n; i++) {
978        if (locks->locks[i].id == id) {
979          SynchEvent* mu_events = GetSynchEvent(this);
980          ABSL_RAW_LOG(FATAL, "thread should not hold mutex %p %s",
981                       static_cast<const void*>(this),
982                       (mu_events == nullptr ? "" : mu_events->name));
983        }
984      }
985    }
986  }
987  static bool TryAcquireWithSpinning(std::atomic<intptr_t>* mu) {
988    int c = GetMutexGlobals().spinloop_iterations;
989    do {  
990      intptr_t v = mu->load(std::memory_order_relaxed);
991      if ((v & (kMuReader | kMuEvent)) != 0) {
992        return false;                       
993      } else if (((v & kMuWriter) == 0) &&  
994                 mu->compare_exchange_strong(v, kMuWriter | v,
995                                             std::memory_order_acquire,
996                                             std::memory_order_relaxed)) {
997        return true;
998      }
999    } while (--c > 0);
1000    return false;
1001  }
1002  void Mutex::Lock() {
1003    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1004    GraphId id = DebugOnlyDeadlockCheck(this);
1005    intptr_t v = mu_.load(std::memory_order_relaxed);
1006    if ((v & (kMuWriter | kMuReader | kMuEvent)) != 0 ||
1007        !mu_.compare_exchange_strong(v, kMuWriter | v, std::memory_order_acquire,
1008                                     std::memory_order_relaxed)) {
1009      if (!TryAcquireWithSpinning(&this->mu_)) {
1010        this->LockSlow(kExclusive, nullptr, 0);
1011      }
1012    }
1013    DebugOnlyLockEnter(this, id);
1014    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1015  }
1016  void Mutex::ReaderLock() {
1017    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1018    GraphId id = DebugOnlyDeadlockCheck(this);
1019    intptr_t v = mu_.load(std::memory_order_relaxed);
1020    if ((v & (kMuWriter | kMuWait | kMuEvent)) != 0 ||
1021        !mu_.compare_exchange_strong(v, (kMuReader | v) + kMuOne,
1022                                     std::memory_order_acquire,
1023                                     std::memory_order_relaxed)) {
1024      this->LockSlow(kShared, nullptr, 0);
1025    }
1026    DebugOnlyLockEnter(this, id);
1027    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1028  }
1029  void Mutex::LockWhen(const Condition& cond) {
1030    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1031    GraphId id = DebugOnlyDeadlockCheck(this);
1032    this->LockSlow(kExclusive, &cond, 0);
1033    DebugOnlyLockEnter(this, id);
1034    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1035  }
1036  bool Mutex::LockWhenWithTimeout(const Condition& cond, absl::Duration timeout) {
1037    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1038    GraphId id = DebugOnlyDeadlockCheck(this);
1039    bool res = LockSlowWithDeadline(kExclusive, &cond, KernelTimeout(timeout), 0);
1040    DebugOnlyLockEnter(this, id);
1041    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1042    return res;
1043  }
1044  bool Mutex::LockWhenWithDeadline(const Condition& cond, absl::Time deadline) {
1045    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1046    GraphId id = DebugOnlyDeadlockCheck(this);
1047    bool res =
1048        LockSlowWithDeadline(kExclusive, &cond, KernelTimeout(deadline), 0);
1049    DebugOnlyLockEnter(this, id);
1050    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1051    return res;
1052  }
1053  void Mutex::ReaderLockWhen(const Condition& cond) {
1054    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1055    GraphId id = DebugOnlyDeadlockCheck(this);
1056    this->LockSlow(kShared, &cond, 0);
1057    DebugOnlyLockEnter(this, id);
1058    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1059  }
1060  bool Mutex::ReaderLockWhenWithTimeout(const Condition& cond,
1061                                        absl::Duration timeout) {
1062    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1063    GraphId id = DebugOnlyDeadlockCheck(this);
1064    bool res = LockSlowWithDeadline(kShared, &cond, KernelTimeout(timeout), 0);
1065    DebugOnlyLockEnter(this, id);
1066    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1067    return res;
1068  }
1069  bool Mutex::ReaderLockWhenWithDeadline(const Condition& cond,
1070                                         absl::Time deadline) {
1071    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1072    GraphId id = DebugOnlyDeadlockCheck(this);
1073    bool res = LockSlowWithDeadline(kShared, &cond, KernelTimeout(deadline), 0);
1074    DebugOnlyLockEnter(this, id);
1075    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1076    return res;
1077  }
1078  void Mutex::Await(const Condition& cond) {
1079    if (cond.Eval()) {  
1080      if (kDebugMode) {
1081        this->AssertReaderHeld();
1082      }
1083    } else {  
1084      ABSL_RAW_CHECK(this->AwaitCommon(cond, KernelTimeout::Never()),
1085                     "condition untrue on return from Await");
1086    }
1087  }
1088  bool Mutex::AwaitWithTimeout(const Condition& cond, absl::Duration timeout) {
1089    if (cond.Eval()) {  
1090      if (kDebugMode) {
1091        this->AssertReaderHeld();
1092      }
1093      return true;
1094    }
1095    KernelTimeout t{timeout};
1096    bool res = this->AwaitCommon(cond, t);
1097    ABSL_RAW_CHECK(res || t.has_timeout(),
1098                   "condition untrue on return from Await");
1099    return res;
1100  }
1101  bool Mutex::AwaitWithDeadline(const Condition& cond, absl::Time deadline) {
1102    if (cond.Eval()) {  
1103      if (kDebugMode) {
1104        this->AssertReaderHeld();
1105      }
1106      return true;
1107    }
1108    KernelTimeout t{deadline};
1109    bool res = this->AwaitCommon(cond, t);
1110    ABSL_RAW_CHECK(res || t.has_timeout(),
1111                   "condition untrue on return from Await");
1112    return res;
1113  }
1114  bool Mutex::AwaitCommon(const Condition& cond, KernelTimeout t) {
1115    this->AssertReaderHeld();
1116    MuHow how =
1117        (mu_.load(std::memory_order_relaxed) & kMuWriter) ? kExclusive : kShared;
1118    ABSL_TSAN_MUTEX_PRE_UNLOCK(this, TsanFlags(how));
1119    SynchWaitParams waitp(how, &cond, t, nullptr &bsol;*no cvmu*/,
1120                          Synch_GetPerThreadAnnotated(this),
1121                          nullptr &bsol;*no cv_word*/);
1122    int flags = kMuHasBlocked;
1123    if (!Condition::GuaranteedEqual(&cond, nullptr)) {
1124      flags |= kMuIsCond;
1125    }
1126    this->UnlockSlow(&waitp);
1127    this->Block(waitp.thread);
1128    ABSL_TSAN_MUTEX_POST_UNLOCK(this, TsanFlags(how));
1129    ABSL_TSAN_MUTEX_PRE_LOCK(this, TsanFlags(how));
1130    this->LockSlowLoop(&waitp, flags);
1131    bool res = waitp.cond != nullptr ||  
1132               EvalConditionAnnotated(&cond, this, true, false, how == kShared);
1133    ABSL_TSAN_MUTEX_POST_LOCK(this, TsanFlags(how), 0);
1134    return res;
1135  }
1136  bool Mutex::TryLock() {
1137    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_try_lock);
1138    intptr_t v = mu_.load(std::memory_order_relaxed);
1139    if ((v & (kMuWriter | kMuReader | kMuEvent)) == 0 &&  
1140        mu_.compare_exchange_strong(v, kMuWriter | v, std::memory_order_acquire,
1141                                    std::memory_order_relaxed)) {
1142      DebugOnlyLockEnter(this);
1143      ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_try_lock, 0);
1144      return true;
1145    }
1146    if ((v & kMuEvent) != 0) {                      
1147      if ((v & kExclusive->slow_need_zero) == 0 &&  
1148          mu_.compare_exchange_strong(
1149              v, (kExclusive->fast_or | v) + kExclusive->fast_add,
1150              std::memory_order_acquire, std::memory_order_relaxed)) {
1151        DebugOnlyLockEnter(this);
1152        PostSynchEvent(this, SYNCH_EV_TRYLOCK_SUCCESS);
1153        ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_try_lock, 0);
1154        return true;
1155      } else {
1156        PostSynchEvent(this, SYNCH_EV_TRYLOCK_FAILED);
1157      }
1158    }
1159    ABSL_TSAN_MUTEX_POST_LOCK(
1160        this, __tsan_mutex_try_lock | __tsan_mutex_try_lock_failed, 0);
1161    return false;
1162  }
1163  bool Mutex::ReaderTryLock() {
1164    ABSL_TSAN_MUTEX_PRE_LOCK(this,
1165                             __tsan_mutex_read_lock | __tsan_mutex_try_lock);
1166    intptr_t v = mu_.load(std::memory_order_relaxed);
1167    int loop_limit = 5;
1168    while ((v & (kMuWriter | kMuWait | kMuEvent)) == 0 && loop_limit != 0) {
1169      if (mu_.compare_exchange_strong(v, (kMuReader | v) + kMuOne,
1170                                      std::memory_order_acquire,
1171                                      std::memory_order_relaxed)) {
1172        DebugOnlyLockEnter(this);
1173        ABSL_TSAN_MUTEX_POST_LOCK(
1174            this, __tsan_mutex_read_lock | __tsan_mutex_try_lock, 0);
1175        return true;
1176      }
1177      loop_limit--;
1178      v = mu_.load(std::memory_order_relaxed);
1179    }
1180    if ((v & kMuEvent) != 0) {  
1181      loop_limit = 5;
1182      while ((v & kShared->slow_need_zero) == 0 && loop_limit != 0) {
1183        if (mu_.compare_exchange_strong(v, (kMuReader | v) + kMuOne,
1184                                        std::memory_order_acquire,
1185                                        std::memory_order_relaxed)) {
1186          DebugOnlyLockEnter(this);
1187          PostSynchEvent(this, SYNCH_EV_READERTRYLOCK_SUCCESS);
1188          ABSL_TSAN_MUTEX_POST_LOCK(
1189              this, __tsan_mutex_read_lock | __tsan_mutex_try_lock, 0);
1190          return true;
1191        }
1192        loop_limit--;
1193        v = mu_.load(std::memory_order_relaxed);
1194      }
1195      if ((v & kMuEvent) != 0) {
1196        PostSynchEvent(this, SYNCH_EV_READERTRYLOCK_FAILED);
1197      }
1198    }
1199    ABSL_TSAN_MUTEX_POST_LOCK(this,
1200                              __tsan_mutex_read_lock | __tsan_mutex_try_lock |
1201                                  __tsan_mutex_try_lock_failed,
1202                              0);
1203    return false;
1204  }
1205  void Mutex::Unlock() {
1206    ABSL_TSAN_MUTEX_PRE_UNLOCK(this, 0);
1207    DebugOnlyLockLeave(this);
1208    intptr_t v = mu_.load(std::memory_order_relaxed);
1209    if (kDebugMode && ((v & (kMuWriter | kMuReader)) != kMuWriter)) {
1210      ABSL_RAW_LOG(FATAL, "Mutex unlocked when destroyed or not locked: v=0x%x",
1211                   static_cast<unsigned>(v));
1212    }
1213    bool should_try_cas = ((v & (kMuEvent | kMuWriter)) == kMuWriter &&
1214                           (v & (kMuWait | kMuDesig)) != kMuWait);
1215    intptr_t x = (v ^ (kMuWriter | kMuWait)) & (kMuWriter | kMuEvent);
1216    intptr_t y = (v ^ (kMuWriter | kMuWait)) & (kMuWait | kMuDesig);
1217    if (kDebugMode && should_try_cas != (x < y)) {
1218      ABSL_RAW_LOG(FATAL, "internal logic error %llx %llx %llx\n",
1219                   static_cast<long long>(v), static_cast<long long>(x),
1220                   static_cast<long long>(y));
1221    }
1222    if (x < y && mu_.compare_exchange_strong(v, v & ~(kMuWrWait | kMuWriter),
1223                                             std::memory_order_release,
1224                                             std::memory_order_relaxed)) {
1225    } else {
1226      this->UnlockSlow(nullptr &bsol;*no waitp*/);  
1227    }
1228    ABSL_TSAN_MUTEX_POST_UNLOCK(this, 0);
1229  }
1230  static bool ExactlyOneReader(intptr_t v) {
1231    assert((v & (kMuWriter | kMuReader)) == kMuReader);
1232    assert((v & kMuHigh) != 0);
1233    constexpr intptr_t kMuMultipleWaitersMask = kMuHigh ^ kMuOne;
1234    return (v & kMuMultipleWaitersMask) == 0;
1235  }
1236  void Mutex::ReaderUnlock() {
1237    ABSL_TSAN_MUTEX_PRE_UNLOCK(this, __tsan_mutex_read_lock);
1238    DebugOnlyLockLeave(this);
1239    intptr_t v = mu_.load(std::memory_order_relaxed);
1240    assert((v & (kMuWriter | kMuReader)) == kMuReader);
1241    if ((v & (kMuReader | kMuWait | kMuEvent)) == kMuReader) {
1242      intptr_t clear = ExactlyOneReader(v) ? kMuReader | kMuOne : kMuOne;
1243      if (mu_.compare_exchange_strong(v, v - clear, std::memory_order_release,
1244                                      std::memory_order_relaxed)) {
1245        ABSL_TSAN_MUTEX_POST_UNLOCK(this, __tsan_mutex_read_lock);
1246        return;
1247      }
1248    }
1249    this->UnlockSlow(nullptr &bsol;*no waitp*/);  
1250    ABSL_TSAN_MUTEX_POST_UNLOCK(this, __tsan_mutex_read_lock);
1251  }
1252  static intptr_t ClearDesignatedWakerMask(int flag) {
1253    assert(flag >= 0);
1254    assert(flag <= 1);
1255    switch (flag) {
1256      case 0:  
1257        return ~static_cast<intptr_t>(0);
1258      case 1:  
1259        return ~static_cast<intptr_t>(kMuDesig);
1260    }
1261    ABSL_UNREACHABLE();
1262  }
1263  static intptr_t IgnoreWaitingWritersMask(int flag) {
1264    assert(flag >= 0);
1265    assert(flag <= 1);
1266    switch (flag) {
1267      case 0:  
1268        return ~static_cast<intptr_t>(0);
1269      case 1:  
1270        return ~static_cast<intptr_t>(kMuWrWait);
1271    }
1272    ABSL_UNREACHABLE();
1273  }
1274  ABSL_ATTRIBUTE_NOINLINE void Mutex::LockSlow(MuHow how, const Condition* cond,
1275                                               int flags) {
1276    ABSL_RAW_CHECK(
1277        this->LockSlowWithDeadline(how, cond, KernelTimeout::Never(), flags),
1278        "condition untrue on return from LockSlow");
1279  }
1280  static inline bool EvalConditionAnnotated(const Condition* cond, Mutex* mu,
1281                                            bool locking, bool trylock,
1282                                            bool read_lock) {
1283    bool res = false;
1284  #ifdef ABSL_INTERNAL_HAVE_TSAN_INTERFACE
1285    const uint32_t flags = read_lock ? __tsan_mutex_read_lock : 0;
1286    const uint32_t tryflags = flags | (trylock ? __tsan_mutex_try_lock : 0);
1287  #endif
1288    if (locking) {
1289      ABSL_TSAN_MUTEX_POST_LOCK(mu, tryflags, 0);
1290      res = cond->Eval();
1291      ABSL_TSAN_MUTEX_PRE_UNLOCK(mu, flags);
1292      ABSL_TSAN_MUTEX_POST_UNLOCK(mu, flags);
1293      ABSL_TSAN_MUTEX_PRE_LOCK(mu, tryflags);
1294    } else {
1295      ABSL_TSAN_MUTEX_POST_UNLOCK(mu, flags);
1296      ABSL_TSAN_MUTEX_PRE_LOCK(mu, flags);
1297      ABSL_TSAN_MUTEX_POST_LOCK(mu, flags, 0);
1298      res = cond->Eval();
1299      ABSL_TSAN_MUTEX_PRE_UNLOCK(mu, flags);
1300    }
1301    static_cast<void>(mu);
1302    static_cast<void>(trylock);
1303    static_cast<void>(read_lock);
1304    return res;
1305  }
1306  static inline bool EvalConditionIgnored(Mutex* mu, const Condition* cond) {
1307    ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
1308    ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_BEGIN();
1309    bool res = cond->Eval();
1310    ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_END();
1311    ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
1312    static_cast<void>(mu);  
1313    return res;
1314  }
1315  bool Mutex::LockSlowWithDeadline(MuHow how, const Condition* cond,
1316                                   KernelTimeout t, int flags) {
1317    intptr_t v = mu_.load(std::memory_order_relaxed);
1318    bool unlock = false;
1319    if ((v & how->fast_need_zero) == 0 &&  
1320        mu_.compare_exchange_strong(
1321            v,
1322            (how->fast_or |
1323             (v & ClearDesignatedWakerMask(flags & kMuHasBlocked))) +
1324                how->fast_add,
1325            std::memory_order_acquire, std::memory_order_relaxed)) {
1326      if (cond == nullptr ||
1327          EvalConditionAnnotated(cond, this, true, false, how == kShared)) {
1328        return true;
1329      }
1330      unlock = true;
1331    }
1332    SynchWaitParams waitp(how, cond, t, nullptr &bsol;*no cvmu*/,
1333                          Synch_GetPerThreadAnnotated(this),
1334                          nullptr &bsol;*no cv_word*/);
1335    if (!Condition::GuaranteedEqual(cond, nullptr)) {
1336      flags |= kMuIsCond;
1337    }
1338    if (unlock) {
1339      this->UnlockSlow(&waitp);
1340      this->Block(waitp.thread);
1341      flags |= kMuHasBlocked;
1342    }
1343    this->LockSlowLoop(&waitp, flags);
1344    return waitp.cond != nullptr ||  
1345           cond == nullptr ||
1346           EvalConditionAnnotated(cond, this, true, false, how == kShared);
1347  }
1348  #define RAW_CHECK_FMT(cond, ...)                                   \
1349    do {                                                             \
1350      if (ABSL_PREDICT_FALSE(!(cond))) {                             \
1351        ABSL_RAW_LOG(FATAL, "Check " #cond " failed: " __VA_ARGS__); \
1352      }                                                              \
1353    } while (0)
1354  static void CheckForMutexCorruption(intptr_t v, const char* label) {
1355    const uintptr_t w = static_cast<uintptr_t>(v ^ kMuWait);
1356    static_assert(kMuReader << 3 == kMuWriter, "must match");
1357    static_assert(kMuWait << 3 == kMuWrWait, "must match");
1358    if (ABSL_PREDICT_TRUE((w & (w << 3) & (kMuWriter | kMuWrWait)) == 0)) return;
1359    RAW_CHECK_FMT((v & (kMuWriter | kMuReader)) != (kMuWriter | kMuReader),
1360                  "%s: Mutex corrupt: both reader and writer lock held: %p",
1361                  label, reinterpret_cast<void*>(v));
1362    RAW_CHECK_FMT((v & (kMuWait | kMuWrWait)) != kMuWrWait,
1363                  "%s: Mutex corrupt: waiting writer with no waiters: %p", label,
1364                  reinterpret_cast<void*>(v));
1365    assert(false);
1366  }
1367  void Mutex::LockSlowLoop(SynchWaitParams* waitp, int flags) {
1368    SchedulingGuard::ScopedDisable disable_rescheduling;
1369    int c = 0;
1370    intptr_t v = mu_.load(std::memory_order_relaxed);
1371    if ((v & kMuEvent) != 0) {
1372      PostSynchEvent(
1373          this, waitp->how == kExclusive ? SYNCH_EV_LOCK : SYNCH_EV_READERLOCK);
1374    }
1375    ABSL_RAW_CHECK(
1376        waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors,
1377        "detected illegal recursion into Mutex code");
1378    for (;;) {
1379      v = mu_.load(std::memory_order_relaxed);
1380      CheckForMutexCorruption(v, "Lock");
1381      if ((v & waitp->how->slow_need_zero) == 0) {
1382        if (mu_.compare_exchange_strong(
1383                v,
1384                (waitp->how->fast_or |
1385                 (v & ClearDesignatedWakerMask(flags & kMuHasBlocked))) +
1386                    waitp->how->fast_add,
1387                std::memory_order_acquire, std::memory_order_relaxed)) {
1388          if (waitp->cond == nullptr ||
1389              EvalConditionAnnotated(waitp->cond, this, true, false,
1390                                     waitp->how == kShared)) {
1391            break;  
1392          }
1393          this->UnlockSlow(waitp);  
1394          this->Block(waitp->thread);
1395          flags |= kMuHasBlocked;
1396          c = 0;
1397        }
1398      } else {  
1399        bool dowait = false;
1400        if ((v & (kMuSpin | kMuWait)) == 0) {  
1401          PerThreadSynch* new_h = Enqueue(nullptr, waitp, v, flags);
1402          intptr_t nv =
1403              (v & ClearDesignatedWakerMask(flags & kMuHasBlocked) & kMuLow) |
1404              kMuWait;
1405          ABSL_RAW_CHECK(new_h != nullptr, "Enqueue to empty list failed");
1406          if (waitp->how == kExclusive && (v & kMuReader) != 0) {
1407            nv |= kMuWrWait;
1408          }
1409          if (mu_.compare_exchange_strong(
1410                  v, reinterpret_cast<intptr_t>(new_h) | nv,
1411                  std::memory_order_release, std::memory_order_relaxed)) {
1412            dowait = true;
1413          } else {  
1414            waitp->thread->waitp = nullptr;
1415          }
1416        } else if ((v & waitp->how->slow_inc_need_zero &
1417                    IgnoreWaitingWritersMask(flags & kMuHasBlocked)) == 0) {
1418          if (mu_.compare_exchange_strong(
1419                  v,
1420                  (v & ClearDesignatedWakerMask(flags & kMuHasBlocked)) |
1421                      kMuSpin | kMuReader,
1422                  std::memory_order_acquire, std::memory_order_relaxed)) {
1423            PerThreadSynch* h = GetPerThreadSynch(v);
1424            h->readers += kMuOne;  
1425            do {                   
1426              v = mu_.load(std::memory_order_relaxed);
1427            } while (!mu_.compare_exchange_weak(v, (v & ~kMuSpin) | kMuReader,
1428                                                std::memory_order_release,
1429                                                std::memory_order_relaxed));
1430            if (waitp->cond == nullptr ||
1431                EvalConditionAnnotated(waitp->cond, this, true, false,
1432                                       waitp->how == kShared)) {
1433              break;  
1434            }
1435            this->UnlockSlow(waitp);  
1436            this->Block(waitp->thread);
1437            flags |= kMuHasBlocked;
1438            c = 0;
1439          }
1440        } else if ((v & kMuSpin) == 0 &&  
1441                   mu_.compare_exchange_strong(
1442                       v,
1443                       (v & ClearDesignatedWakerMask(flags & kMuHasBlocked)) |
1444                           kMuSpin | kMuWait,
1445                       std::memory_order_acquire, std::memory_order_relaxed)) {
1446          PerThreadSynch* h = GetPerThreadSynch(v);
1447          PerThreadSynch* new_h = Enqueue(h, waitp, v, flags);
1448          intptr_t wr_wait = 0;
1449          ABSL_RAW_CHECK(new_h != nullptr, "Enqueue to list failed");
1450          if (waitp->how == kExclusive && (v & kMuReader) != 0) {
1451            wr_wait = kMuWrWait;  
1452          }
1453          do {  
1454            v = mu_.load(std::memory_order_relaxed);
1455          } while (!mu_.compare_exchange_weak(
1456              v,
1457              (v & (kMuLow & ~kMuSpin)) | kMuWait | wr_wait |
1458                  reinterpret_cast<intptr_t>(new_h),
1459              std::memory_order_release, std::memory_order_relaxed));
1460          dowait = true;
1461        }
1462        if (dowait) {
1463          this->Block(waitp->thread);  
1464          flags |= kMuHasBlocked;
1465          c = 0;
1466        }
1467      }
1468      ABSL_RAW_CHECK(
1469          waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors,
1470          "detected illegal recursion into Mutex code");
1471      c = synchronization_internal::MutexDelay(c, GENTLE);
1472    }
1473    ABSL_RAW_CHECK(
1474        waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors,
1475        "detected illegal recursion into Mutex code");
1476    if ((v & kMuEvent) != 0) {
1477      PostSynchEvent(this, waitp->how == kExclusive
1478                               ? SYNCH_EV_LOCK_RETURNING
1479                               : SYNCH_EV_READERLOCK_RETURNING);
1480    }
1481  }
1482  ABSL_ATTRIBUTE_NOINLINE void Mutex::UnlockSlow(SynchWaitParams* waitp) {
1483    SchedulingGuard::ScopedDisable disable_rescheduling;
1484    intptr_t v = mu_.load(std::memory_order_relaxed);
1485    this->AssertReaderHeld();
1486    CheckForMutexCorruption(v, "Unlock");
1487    if ((v & kMuEvent) != 0) {
1488      PostSynchEvent(
1489          this, (v & kMuWriter) != 0 ? SYNCH_EV_UNLOCK : SYNCH_EV_READERUNLOCK);
1490    }
1491    int c = 0;
1492    PerThreadSynch* w = nullptr;
1493    PerThreadSynch* pw = nullptr;
1494    PerThreadSynch* old_h = nullptr;
1495    const Condition* known_false = nullptr;
1496    PerThreadSynch* wake_list = kPerThreadSynchNull;  
1497    intptr_t wr_wait = 0;  
1498    ABSL_RAW_CHECK(waitp == nullptr || waitp->thread->waitp == nullptr ||
1499                       waitp->thread->suppress_fatal_errors,
1500                   "detected illegal recursion into Mutex code");
1501    for (;;) {
1502      v = mu_.load(std::memory_order_relaxed);
1503      if ((v & kMuWriter) != 0 && (v & (kMuWait | kMuDesig)) != kMuWait &&
1504          waitp == nullptr) {
1505        if (mu_.compare_exchange_strong(v, v & ~(kMuWrWait | kMuWriter),
1506                                        std::memory_order_release,
1507                                        std::memory_order_relaxed)) {
1508          return;
1509        }
1510      } else if ((v & (kMuReader | kMuWait)) == kMuReader && waitp == nullptr) {
1511        intptr_t clear = ExactlyOneReader(v) ? kMuReader | kMuOne : kMuOne;
1512        if (mu_.compare_exchange_strong(v, v - clear, std::memory_order_release,
1513                                        std::memory_order_relaxed)) {
1514          return;
1515        }
1516      } else if ((v & kMuSpin) == 0 &&  
1517                 mu_.compare_exchange_strong(v, v | kMuSpin,
1518                                             std::memory_order_acquire,
1519                                             std::memory_order_relaxed)) {
1520        if ((v & kMuWait) == 0) {  
1521          intptr_t nv;
1522          bool do_enqueue = true;  
1523          ABSL_RAW_CHECK(waitp != nullptr,
1524                         "UnlockSlow is confused");  
1525          do {  
1526            v = mu_.load(std::memory_order_relaxed);
1527            intptr_t new_readers = (v >= kMuOne) ? v - kMuOne : v;
1528            PerThreadSynch* new_h = nullptr;
1529            if (do_enqueue) {
1530              do_enqueue = (waitp->cv_word == nullptr);
1531              new_h = Enqueue(nullptr, waitp, new_readers, kMuIsCond);
1532            }
1533            intptr_t clear = kMuWrWait | kMuWriter;  
1534            if ((v & kMuWriter) == 0 && ExactlyOneReader(v)) {  
1535              clear = kMuWrWait | kMuReader;                    
1536            }
1537            nv = (v & kMuLow & ~clear & ~kMuSpin);
1538            if (new_h != nullptr) {
1539              nv |= kMuWait | reinterpret_cast<intptr_t>(new_h);
1540            } else {  
1541              nv |= new_readers & kMuHigh;
1542            }
1543          } while (!mu_.compare_exchange_weak(v, nv, std::memory_order_release,
1544                                              std::memory_order_relaxed));
1545          break;
1546        }
1547        PerThreadSynch* h = GetPerThreadSynch(v);
1548        if ((v & kMuReader) != 0 && (h->readers & kMuHigh) > kMuOne) {
1549          h->readers -= kMuOne;    
1550          intptr_t nv = v;         
<span onclick='openModal()' class='match'>1551          if (waitp != nullptr) {  
1552            PerThreadSynch* new_h = Enqueue(h, waitp, v, kMuIsCond);
1553            ABSL_RAW_CHECK(new_h != nullptr,
</span>1554                           "waiters disappeared during Enqueue()!");
1555            nv &= kMuLow;
1556            nv |= kMuWait | reinterpret_cast<intptr_t>(new_h);
1557          }
1558          mu_.store(nv, std::memory_order_release);  
1559          break;
1560        }
1561        ABSL_RAW_CHECK(old_h == nullptr || h->maybe_unlocking,
1562                       "Mutex queue changed beneath us");
1563        if (old_h != nullptr &&
1564            !old_h->may_skip) {    
1565          old_h->may_skip = true;  
1566          ABSL_RAW_CHECK(old_h->skip == nullptr, "illegal skip from head");
1567          if (h != old_h && MuEquivalentWaiter(old_h, old_h->next)) {
1568            old_h->skip = old_h->next;  
1569          }
1570        }
1571        if (h->next->waitp->how == kExclusive &&
1572            Condition::GuaranteedEqual(h->next->waitp->cond, nullptr)) {
1573          pw = h;  
1574          w = h->next;
1575          w->wake = true;
1576          wr_wait = kMuWrWait;
1577        } else if (w != nullptr && (w->waitp->how == kExclusive || h == old_h)) {
1578          if (pw == nullptr) {  
1579            pw = h;
1580          }
1581        } else {
1582          if (old_h == h) {  
1583            intptr_t nv = (v & ~(kMuReader | kMuWriter | kMuWrWait));
1584            h->readers = 0;
1585            h->maybe_unlocking = false;  
1586            if (waitp != nullptr) {      
1587              PerThreadSynch* new_h = Enqueue(h, waitp, v, kMuIsCond);
1588              nv &= kMuLow;
1589              if (new_h != nullptr) {
1590                nv |= kMuWait | reinterpret_cast<intptr_t>(new_h);
1591              }  
1592            }
1593            mu_.store(nv, std::memory_order_release);
1594            break;
1595          }
1596          PerThreadSynch* w_walk;   
1597          PerThreadSynch* pw_walk;  
1598          if (old_h != nullptr) {  
1599            pw_walk = old_h;
1600            w_walk = old_h->next;
1601          } else {  
1602            pw_walk =
1603                nullptr;  
1604            w_walk = h->next;
1605          }
1606          h->may_skip = false;  
1607          ABSL_RAW_CHECK(h->skip == nullptr, "illegal skip from head");
1608          h->maybe_unlocking = true;  
1609          mu_.store(v, std::memory_order_release);  
1610          old_h = h;  
1611          while (pw_walk != h) {
1612            w_walk->wake = false;
1613            if (w_walk->waitp->cond ==
1614                    nullptr ||  
1615                (w_walk->waitp->cond != known_false &&
1616                 EvalConditionIgnored(this, w_walk->waitp->cond))) {
1617              if (w == nullptr) {
1618                w_walk->wake = true;  
1619                w = w_walk;
1620                pw = pw_walk;
1621                if (w_walk->waitp->how == kExclusive) {
1622                  wr_wait = kMuWrWait;
1623                  break;  
1624                }
1625              } else if (w_walk->waitp->how == kShared) {  
1626                w_walk->wake = true;
1627              } else {  
1628                wr_wait = kMuWrWait;
1629              }
1630            } else {                              
1631              known_false = w_walk->waitp->cond;  
1632            }
1633            if (w_walk->wake) {  
1634              pw_walk = w_walk;  
1635            } else {             
1636              pw_walk = Skip(w_walk);
1637            }
1638            if (pw_walk != h) {
1639              w_walk = pw_walk->next;
1640            }
1641          }
1642          continue;  
1643        }
1644        ABSL_RAW_CHECK(pw->next == w, "pw not w's predecessor");
1645        h = DequeueAllWakeable(h, pw, &wake_list);
1646        intptr_t nv = (v & kMuEvent) | kMuDesig;
1647        if (waitp != nullptr) {  
1648          h = Enqueue(h, waitp, v, kMuIsCond);
1649        }
1650        ABSL_RAW_CHECK(wake_list != kPerThreadSynchNull,
1651                       "unexpected empty wake list");
1652        if (h != nullptr) {  
1653          h->readers = 0;
1654          h->maybe_unlocking = false;  
1655          nv |= wr_wait | kMuWait | reinterpret_cast<intptr_t>(h);
1656        }
1657        mu_.store(nv, std::memory_order_release);
1658        break;  
1659      }
1660      c = synchronization_internal::MutexDelay(c, AGGRESSIVE);
1661    }  
1662    if (wake_list != kPerThreadSynchNull) {
1663      int64_t total_wait_cycles = 0;
1664      int64_t max_wait_cycles = 0;
1665      int64_t now = CycleClock::Now();
1666      do {
1667        if (!wake_list->cond_waiter) {
1668          int64_t cycles_waited =
1669              (now - wake_list->waitp->contention_start_cycles);
1670          total_wait_cycles += cycles_waited;
1671          if (max_wait_cycles == 0) max_wait_cycles = cycles_waited;
1672          wake_list->waitp->contention_start_cycles = now;
1673          wake_list->waitp->should_submit_contention_data = true;
1674        }
1675        wake_list = Wakeup(wake_list);  
1676      } while (wake_list != kPerThreadSynchNull);
1677      if (total_wait_cycles > 0) {
1678        mutex_tracer("slow release", this, total_wait_cycles);
1679        ABSL_TSAN_MUTEX_PRE_DIVERT(this, 0);
1680        submit_profile_data(total_wait_cycles);
1681        ABSL_TSAN_MUTEX_POST_DIVERT(this, 0);
1682      }
1683    }
1684  }
1685  void Mutex::Trans(MuHow how) {
1686    this->LockSlow(how, nullptr, kMuHasBlocked | kMuIsCond);
1687  }
1688  void Mutex::Fer(PerThreadSynch* w) {
1689    SchedulingGuard::ScopedDisable disable_rescheduling;
1690    int c = 0;
1691    ABSL_RAW_CHECK(w->waitp->cond == nullptr,
1692                   "Mutex::Fer while waiting on Condition");
1693    ABSL_RAW_CHECK(!w->waitp->timeout.has_timeout(),
1694                   "Mutex::Fer while in timed wait");
1695    ABSL_RAW_CHECK(w->waitp->cv_word == nullptr,
1696                   "Mutex::Fer with pending CondVar queueing");
1697    for (;;) {
1698      intptr_t v = mu_.load(std::memory_order_relaxed);
1699      const intptr_t conflicting =
1700          kMuWriter | (w->waitp->how == kShared ? 0 : kMuReader);
1701      if ((v & conflicting) == 0) {
1702        w->next = nullptr;
1703        w->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
1704        IncrementSynchSem(this, w);
1705        return;
1706      } else {
1707        if ((v & (kMuSpin | kMuWait)) == 0) {  
1708          PerThreadSynch* new_h = Enqueue(nullptr, w->waitp, v, kMuIsCond);
1709          ABSL_RAW_CHECK(new_h != nullptr,
1710                         "Enqueue failed");  
1711          if (mu_.compare_exchange_strong(
1712                  v, reinterpret_cast<intptr_t>(new_h) | (v & kMuLow) | kMuWait,
1713                  std::memory_order_release, std::memory_order_relaxed)) {
1714            return;
1715          }
1716        } else if ((v & kMuSpin) == 0 &&
1717                   mu_.compare_exchange_strong(v, v | kMuSpin | kMuWait)) {
1718          PerThreadSynch* h = GetPerThreadSynch(v);
1719          PerThreadSynch* new_h = Enqueue(h, w->waitp, v, kMuIsCond);
1720          ABSL_RAW_CHECK(new_h != nullptr,
1721                         "Enqueue failed");  
1722          do {
1723            v = mu_.load(std::memory_order_relaxed);
1724          } while (!mu_.compare_exchange_weak(
1725              v,
1726              (v & kMuLow & ~kMuSpin) | kMuWait |
1727                  reinterpret_cast<intptr_t>(new_h),
1728              std::memory_order_release, std::memory_order_relaxed));
1729          return;
1730        }
1731      }
1732      c = synchronization_internal::MutexDelay(c, GENTLE);
1733    }
1734  }
1735  void Mutex::AssertHeld() const {
1736    if ((mu_.load(std::memory_order_relaxed) & kMuWriter) == 0) {
1737      SynchEvent* e = GetSynchEvent(this);
1738      ABSL_RAW_LOG(FATAL, "thread should hold write lock on Mutex %p %s",
1739                   static_cast<const void*>(this), (e == nullptr ? "" : e->name));
1740    }
1741  }
1742  void Mutex::AssertReaderHeld() const {
1743    if ((mu_.load(std::memory_order_relaxed) & (kMuReader | kMuWriter)) == 0) {
1744      SynchEvent* e = GetSynchEvent(this);
1745      ABSL_RAW_LOG(FATAL,
1746                   "thread should hold at least a read lock on Mutex %p %s",
1747                   static_cast<const void*>(this), (e == nullptr ? "" : e->name));
1748    }
1749  }
1750  static const intptr_t kCvSpin = 0x0001L;   
1751  static const intptr_t kCvEvent = 0x0002L;  
1752  static const intptr_t kCvLow = 0x0003L;  
1753  enum {
1754    kGdbCvSpin = kCvSpin,
1755    kGdbCvEvent = kCvEvent,
1756    kGdbCvLow = kCvLow,
1757  };
1758  static_assert(PerThreadSynch::kAlignment > kCvLow,
1759                "PerThreadSynch::kAlignment must be greater than kCvLow");
1760  void CondVar::EnableDebugLog(const char* name) {
1761    SynchEvent* e = EnsureSynchEvent(&this->cv_, name, kCvEvent, kCvSpin);
1762    e->log = true;
1763    UnrefSynchEvent(e);
1764  }
1765  CondVar::~CondVar() {
1766    if ((cv_.load(std::memory_order_relaxed) & kCvEvent) != 0) {
1767      ForgetSynchEvent(&this->cv_, kCvEvent, kCvSpin);
1768    }
1769  }
1770  void CondVar::Remove(PerThreadSynch* s) {
1771    SchedulingGuard::ScopedDisable disable_rescheduling;
1772    intptr_t v;
1773    int c = 0;
1774    for (v = cv_.load(std::memory_order_relaxed);;
1775         v = cv_.load(std::memory_order_relaxed)) {
1776      if ((v & kCvSpin) == 0 &&  
1777          cv_.compare_exchange_strong(v, v | kCvSpin, std::memory_order_acquire,
1778                                      std::memory_order_relaxed)) {
1779        PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1780        if (h != nullptr) {
1781          PerThreadSynch* w = h;
1782          while (w->next != s && w->next != h) {  
1783            w = w->next;
1784          }
1785          if (w->next == s) {  
1786            w->next = s->next;
1787            if (h == s) {
1788              h = (w == s) ? nullptr : w;
1789            }
1790            s->next = nullptr;
1791            s->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
1792          }
1793        }
1794        cv_.store((v & kCvEvent) | reinterpret_cast<intptr_t>(h),
1795                  std::memory_order_release);
1796        return;
1797      } else {
1798        c = synchronization_internal::MutexDelay(c, GENTLE);
1799      }
1800    }
1801  }
1802  static void CondVarEnqueue(SynchWaitParams* waitp) {
1803    std::atomic<intptr_t>* cv_word = waitp->cv_word;
1804    waitp->cv_word = nullptr;
1805    intptr_t v = cv_word->load(std::memory_order_relaxed);
1806    int c = 0;
1807    while ((v & kCvSpin) != 0 ||  
1808           !cv_word->compare_exchange_weak(v, v | kCvSpin,
1809                                           std::memory_order_acquire,
1810                                           std::memory_order_relaxed)) {
1811      c = synchronization_internal::MutexDelay(c, GENTLE);
1812      v = cv_word->load(std::memory_order_relaxed);
1813    }
1814    ABSL_RAW_CHECK(waitp->thread->waitp == nullptr, "waiting when shouldn't be");
1815    waitp->thread->waitp = waitp;  
1816    PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1817    if (h == nullptr) {  
1818      waitp->thread->next = waitp->thread;
1819    } else {
1820      waitp->thread->next = h->next;
1821      h->next = waitp->thread;
1822    }
1823    waitp->thread->state.store(PerThreadSynch::kQueued,
1824                               std::memory_order_relaxed);
1825    cv_word->store((v & kCvEvent) | reinterpret_cast<intptr_t>(waitp->thread),
1826                   std::memory_order_release);
1827  }
1828  bool CondVar::WaitCommon(Mutex* mutex, KernelTimeout t) {
1829    bool rc = false;  
1830    intptr_t mutex_v = mutex->mu_.load(std::memory_order_relaxed);
1831    Mutex::MuHow mutex_how = ((mutex_v & kMuWriter) != 0) ? kExclusive : kShared;
1832    ABSL_TSAN_MUTEX_PRE_UNLOCK(mutex, TsanFlags(mutex_how));
1833    intptr_t v = cv_.load(std::memory_order_relaxed);
1834    cond_var_tracer("Wait", this);
1835    if ((v & kCvEvent) != 0) {
1836      PostSynchEvent(this, SYNCH_EV_WAIT);
1837    }
1838    SynchWaitParams waitp(mutex_how, nullptr, t, mutex,
1839                          Synch_GetPerThreadAnnotated(mutex), &cv_);
1840    mutex->UnlockSlow(&waitp);
1841    while (waitp.thread->state.load(std::memory_order_acquire) ==
1842           PerThreadSynch::kQueued) {
1843      if (!Mutex::DecrementSynchSem(mutex, waitp.thread, t)) {
1844        t = KernelTimeout::Never();
1845        this->Remove(waitp.thread);
1846        rc = true;
1847      }
1848    }
1849    ABSL_RAW_CHECK(waitp.thread->waitp != nullptr, "not waiting when should be");
1850    waitp.thread->waitp = nullptr;  
1851    cond_var_tracer("Unwait", this);
1852    if ((v & kCvEvent) != 0) {
1853      PostSynchEvent(this, SYNCH_EV_WAIT_RETURNING);
1854    }
1855    ABSL_TSAN_MUTEX_POST_UNLOCK(mutex, TsanFlags(mutex_how));
1856    ABSL_TSAN_MUTEX_PRE_LOCK(mutex, TsanFlags(mutex_how));
1857    mutex->Trans(mutex_how);  
1858    ABSL_TSAN_MUTEX_POST_LOCK(mutex, TsanFlags(mutex_how), 0);
1859    return rc;
1860  }
1861  bool CondVar::WaitWithTimeout(Mutex* mu, absl::Duration timeout) {
1862    return WaitCommon(mu, KernelTimeout(timeout));
1863  }
1864  bool CondVar::WaitWithDeadline(Mutex* mu, absl::Time deadline) {
1865    return WaitCommon(mu, KernelTimeout(deadline));
1866  }
1867  void CondVar::Wait(Mutex* mu) { WaitCommon(mu, KernelTimeout::Never()); }
1868  void CondVar::Wakeup(PerThreadSynch* w) {
1869    if (w->waitp->timeout.has_timeout() || w->waitp->cvmu == nullptr) {
1870      Mutex* mu = w->waitp->cvmu;
1871      w->next = nullptr;
1872      w->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
1873      Mutex::IncrementSynchSem(mu, w);
1874    } else {
1875      w->waitp->cvmu->Fer(w);
1876    }
1877  }
1878  void CondVar::Signal() {
1879    SchedulingGuard::ScopedDisable disable_rescheduling;
1880    ABSL_TSAN_MUTEX_PRE_SIGNAL(nullptr, 0);
1881    intptr_t v;
1882    int c = 0;
1883    for (v = cv_.load(std::memory_order_relaxed); v != 0;
1884         v = cv_.load(std::memory_order_relaxed)) {
1885      if ((v & kCvSpin) == 0 &&  
1886          cv_.compare_exchange_strong(v, v | kCvSpin, std::memory_order_acquire,
1887                                      std::memory_order_relaxed)) {
1888        PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1889        PerThreadSynch* w = nullptr;
1890        if (h != nullptr) {  
1891          w = h->next;
1892          if (w == h) {
1893            h = nullptr;
1894          } else {
1895            h->next = w->next;
1896          }
1897        }
1898        cv_.store((v & kCvEvent) | reinterpret_cast<intptr_t>(h),
1899                  std::memory_order_release);
1900        if (w != nullptr) {
1901          CondVar::Wakeup(w);  
1902          cond_var_tracer("Signal wakeup", this);
1903        }
1904        if ((v & kCvEvent) != 0) {
1905          PostSynchEvent(this, SYNCH_EV_SIGNAL);
1906        }
1907        ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1908        return;
1909      } else {
1910        c = synchronization_internal::MutexDelay(c, GENTLE);
1911      }
1912    }
1913    ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1914  }
1915  void CondVar::SignalAll() {
1916    ABSL_TSAN_MUTEX_PRE_SIGNAL(nullptr, 0);
1917    intptr_t v;
1918    int c = 0;
1919    for (v = cv_.load(std::memory_order_relaxed); v != 0;
1920         v = cv_.load(std::memory_order_relaxed)) {
1921      if ((v & kCvSpin) == 0 &&
1922          cv_.compare_exchange_strong(v, v & kCvEvent, std::memory_order_acquire,
1923                                      std::memory_order_relaxed)) {
1924        PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1925        if (h != nullptr) {
1926          PerThreadSynch* w;
1927          PerThreadSynch* n = h->next;
1928          do {  
1929            w = n;
1930            n = n->next;
1931            CondVar::Wakeup(w);
1932          } while (w != h);
1933          cond_var_tracer("SignalAll wakeup", this);
1934        }
1935        if ((v & kCvEvent) != 0) {
1936          PostSynchEvent(this, SYNCH_EV_SIGNALALL);
1937        }
1938        ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1939        return;
1940      } else {
1941        c = synchronization_internal::MutexDelay(c, GENTLE);
1942      }
1943    }
1944    ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1945  }
1946  void ReleasableMutexLock::Release() {
1947    ABSL_RAW_CHECK(this->mu_ != nullptr,
1948                   "ReleasableMutexLock::Release may only be called once");
1949    this->mu_->Unlock();
1950    this->mu_ = nullptr;
1951  }
1952  #ifdef ABSL_HAVE_THREAD_SANITIZER
1953  extern "C" void __tsan_read1(void* addr);
1954  #else
1955  #define __tsan_read1(addr)  
1956  #endif
1957  static bool Dereference(void* arg) {
1958    __tsan_read1(arg);
1959    return *(static_cast<bool*>(arg));
1960  }
1961  ABSL_CONST_INIT const Condition Condition::kTrue;
1962  Condition::Condition(bool (*func)(void*), void* arg)
1963      : eval_(&CallVoidPtrFunction), arg_(arg) {
1964    static_assert(sizeof(&func) <= sizeof(callback_),
1965                  "An overlarge function pointer passed to Condition.");
1966    StoreCallback(func);
1967  }
1968  bool Condition::CallVoidPtrFunction(const Condition* c) {
1969    using FunctionPointer = bool (*)(void*);
1970    FunctionPointer function_pointer;
1971    std::memcpy(&function_pointer, c->callback_, sizeof(function_pointer));
1972    return (*function_pointer)(c->arg_);
1973  }
1974  Condition::Condition(const bool* cond)
1975      : eval_(CallVoidPtrFunction),
1976        arg_(const_cast<bool*>(cond)) {
1977    using FunctionPointer = bool (*)(void*);
1978    const FunctionPointer dereference = Dereference;
1979    StoreCallback(dereference);
1980  }
1981  bool Condition::Eval() const {
1982    return (this->eval_ == nullptr) || (*this->eval_)(this);
1983  }
1984  bool Condition::GuaranteedEqual(const Condition* a, const Condition* b) {
1985    if (a == nullptr || a->eval_ == nullptr) {
1986      return b == nullptr || b->eval_ == nullptr;
1987    } else if (b == nullptr || b->eval_ == nullptr) {
1988      return false;
1989    }
1990    return a->eval_ == b->eval_ && a->arg_ == b->arg_ &&
1991           !memcmp(a->callback_, b->callback_, sizeof(a->callback_));
1992  }
1993  ABSL_NAMESPACE_END
1994  }  
</code></pre>
        </div>
        <div class="column">
            <h3>abseil-cpp-MDEwOlJlcG9zaXRvcnkxMDQyMzE1NDE=-flat-mutex.cc</h3>
            <pre><code>1  #include "absl/synchronization/mutex.h"
2  #ifdef _WIN32
3  #include <windows.h>
4  #ifdef ERROR
5  #undef ERROR
6  #endif
7  #else
8  #include <fcntl.h>
9  #include <pthread.h>
10  #include <sched.h>
11  #include <sys/time.h>
12  #endif
13  #include <assert.h>
14  #include <errno.h>
15  #include <stdio.h>
16  #include <stdlib.h>
17  #include <string.h>
18  #include <time.h>
19  #include <algorithm>
20  #include <atomic>
21  #include <cstddef>
22  #include <cstdlib>
23  #include <cstring>
24  #include <thread>  
25  #include "absl/base/attributes.h"
26  #include "absl/base/call_once.h"
27  #include "absl/base/config.h"
28  #include "absl/base/dynamic_annotations.h"
29  #include "absl/base/internal/atomic_hook.h"
30  #include "absl/base/internal/cycleclock.h"
31  #include "absl/base/internal/hide_ptr.h"
32  #include "absl/base/internal/low_level_alloc.h"
33  #include "absl/base/internal/raw_logging.h"
34  #include "absl/base/internal/spinlock.h"
35  #include "absl/base/internal/sysinfo.h"
36  #include "absl/base/internal/thread_identity.h"
37  #include "absl/base/internal/tsan_mutex_interface.h"
38  #include "absl/base/optimization.h"
39  #include "absl/debugging/stacktrace.h"
40  #include "absl/debugging/symbolize.h"
41  #include "absl/synchronization/internal/graphcycles.h"
42  #include "absl/synchronization/internal/per_thread_sem.h"
43  #include "absl/time/time.h"
44  using absl::base_internal::CurrentThreadIdentityIfPresent;
45  using absl::base_internal::CycleClock;
46  using absl::base_internal::PerThreadSynch;
47  using absl::base_internal::SchedulingGuard;
48  using absl::base_internal::ThreadIdentity;
49  using absl::synchronization_internal::GetOrCreateCurrentThreadIdentity;
50  using absl::synchronization_internal::GraphCycles;
51  using absl::synchronization_internal::GraphId;
52  using absl::synchronization_internal::InvalidGraphId;
53  using absl::synchronization_internal::KernelTimeout;
54  using absl::synchronization_internal::PerThreadSem;
55  extern "C" {
56  ABSL_ATTRIBUTE_WEAK void ABSL_INTERNAL_C_SYMBOL(AbslInternalMutexYield)() {
57    std::this_thread::yield();
58  }
59  }  
60  namespace absl {
61  ABSL_NAMESPACE_BEGIN
62  namespace {
63  #if defined(ABSL_HAVE_THREAD_SANITIZER)
64  constexpr OnDeadlockCycle kDeadlockDetectionDefault = OnDeadlockCycle::kIgnore;
65  #else
66  constexpr OnDeadlockCycle kDeadlockDetectionDefault = OnDeadlockCycle::kAbort;
67  #endif
68  ABSL_CONST_INIT std::atomic<OnDeadlockCycle> synch_deadlock_detection(
69      kDeadlockDetectionDefault);
70  ABSL_CONST_INIT std::atomic<bool> synch_check_invariants(false);
71  ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES
72  absl::base_internal::AtomicHook<void (*)(int64_t wait_cycles)>
73      submit_profile_data;
74  ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES absl::base_internal::AtomicHook<void (*)(
75      const char* msg, const void* obj, int64_t wait_cycles)>
76      mutex_tracer;
77  ABSL_INTERNAL_ATOMIC_HOOK_ATTRIBUTES
78  absl::base_internal::AtomicHook<void (*)(const char* msg, const void* cv)>
79      cond_var_tracer;
80  }  
81  static inline bool EvalConditionAnnotated(const Condition* cond, Mutex* mu,
82                                            bool locking, bool trylock,
83                                            bool read_lock);
84  void RegisterMutexProfiler(void (*fn)(int64_t wait_cycles)) {
85    submit_profile_data.Store(fn);
86  }
87  void RegisterMutexTracer(void (*fn)(const char* msg, const void* obj,
88                                      int64_t wait_cycles)) {
89    mutex_tracer.Store(fn);
90  }
91  void RegisterCondVarTracer(void (*fn)(const char* msg, const void* cv)) {
92    cond_var_tracer.Store(fn);
93  }
94  namespace {
95  enum DelayMode { AGGRESSIVE, GENTLE };
96  struct ABSL_CACHELINE_ALIGNED MutexGlobals {
97    absl::once_flag once;
98    int spinloop_iterations = 0;
99    int32_t mutex_sleep_spins[2] = {};
100    absl::Duration mutex_sleep_time;
101  };
102  absl::Duration MeasureTimeToYield() {
103    absl::Time before = absl::Now();
104    ABSL_INTERNAL_C_SYMBOL(AbslInternalMutexYield)();
105    return absl::Now() - before;
106  }
107  const MutexGlobals& GetMutexGlobals() {
108    ABSL_CONST_INIT static MutexGlobals data;
109    absl::base_internal::LowLevelCallOnce(&data.once, [&]() {
110      if (absl::base_internal::NumCPUs() > 1) {
111        data.spinloop_iterations = 1500;
112        data.mutex_sleep_spins[AGGRESSIVE] = 5000;
113        data.mutex_sleep_spins[GENTLE] = 250;
114        data.mutex_sleep_time = absl::Microseconds(10);
115      } else {
116        data.spinloop_iterations = 0;
117        data.mutex_sleep_spins[AGGRESSIVE] = 0;
118        data.mutex_sleep_spins[GENTLE] = 0;
119        data.mutex_sleep_time = MeasureTimeToYield() * 5;
120        data.mutex_sleep_time =
121            std::min(data.mutex_sleep_time, absl::Milliseconds(1));
122        data.mutex_sleep_time =
123            std::max(data.mutex_sleep_time, absl::Microseconds(10));
124      }
125    });
126    return data;
127  }
128  }  
129  namespace synchronization_internal {
130  int MutexDelay(int32_t c, int mode) {
131    const int32_t limit = GetMutexGlobals().mutex_sleep_spins[mode];
132    const absl::Duration sleep_time = GetMutexGlobals().mutex_sleep_time;
133    if (c < limit) {
134      c++;
135    } else {
136      SchedulingGuard::ScopedEnable enable_rescheduling;
137      ABSL_TSAN_MUTEX_PRE_DIVERT(nullptr, 0);
138      if (c == limit) {
139        ABSL_INTERNAL_C_SYMBOL(AbslInternalMutexYield)();
140        c++;
141      } else {
142        absl::SleepFor(sleep_time);
143        c = 0;
144      }
145      ABSL_TSAN_MUTEX_POST_DIVERT(nullptr, 0);
146    }
147    return c;
148  }
149  }  
150  static void AtomicSetBits(std::atomic<intptr_t>* pv, intptr_t bits,
151                            intptr_t wait_until_clear) {
152    intptr_t v;
153    do {
154      v = pv->load(std::memory_order_relaxed);
155    } while ((v & bits) != bits &&
156             ((v & wait_until_clear) != 0 ||
157              !pv->compare_exchange_weak(v, v | bits, std::memory_order_release,
158                                         std::memory_order_relaxed)));
159  }
160  static void AtomicClearBits(std::atomic<intptr_t>* pv, intptr_t bits,
161                              intptr_t wait_until_clear) {
162    intptr_t v;
163    do {
164      v = pv->load(std::memory_order_relaxed);
165    } while ((v & bits) != 0 &&
166             ((v & wait_until_clear) != 0 ||
167              !pv->compare_exchange_weak(v, v & ~bits, std::memory_order_release,
168                                         std::memory_order_relaxed)));
169  }
170  ABSL_CONST_INIT static absl::base_internal::SpinLock deadlock_graph_mu(
171      absl::kConstInit, base_internal::SCHEDULE_KERNEL_ONLY);
172  ABSL_CONST_INIT static GraphCycles* deadlock_graph
173      ABSL_GUARDED_BY(deadlock_graph_mu) ABSL_PT_GUARDED_BY(deadlock_graph_mu);
174  namespace {  
175  enum {       
176    SYNCH_EV_TRYLOCK_SUCCESS,
177    SYNCH_EV_TRYLOCK_FAILED,
178    SYNCH_EV_READERTRYLOCK_SUCCESS,
179    SYNCH_EV_READERTRYLOCK_FAILED,
180    SYNCH_EV_LOCK,
181    SYNCH_EV_LOCK_RETURNING,
182    SYNCH_EV_READERLOCK,
183    SYNCH_EV_READERLOCK_RETURNING,
184    SYNCH_EV_UNLOCK,
185    SYNCH_EV_READERUNLOCK,
186    SYNCH_EV_WAIT,
187    SYNCH_EV_WAIT_RETURNING,
188    SYNCH_EV_SIGNAL,
189    SYNCH_EV_SIGNALALL,
190  };
191  enum {                    
192    SYNCH_F_R = 0x01,       
193    SYNCH_F_LCK = 0x02,     
194    SYNCH_F_TRY = 0x04,     
195    SYNCH_F_UNLOCK = 0x08,  
196    SYNCH_F_LCK_W = SYNCH_F_LCK,
197    SYNCH_F_LCK_R = SYNCH_F_LCK | SYNCH_F_R,
198  };
199  }  
200  static const struct {
201    int flags;
202    const char* msg;
203  } event_properties[] = {
204      {SYNCH_F_LCK_W | SYNCH_F_TRY, "TryLock succeeded "},
205      {0, "TryLock failed "},
206      {SYNCH_F_LCK_R | SYNCH_F_TRY, "ReaderTryLock succeeded "},
207      {0, "ReaderTryLock failed "},
208      {0, "Lock blocking "},
209      {SYNCH_F_LCK_W, "Lock returning "},
210      {0, "ReaderLock blocking "},
211      {SYNCH_F_LCK_R, "ReaderLock returning "},
212      {SYNCH_F_LCK_W | SYNCH_F_UNLOCK, "Unlock "},
213      {SYNCH_F_LCK_R | SYNCH_F_UNLOCK, "ReaderUnlock "},
214      {0, "Wait on "},
215      {0, "Wait unblocked "},
216      {0, "Signal on "},
217      {0, "SignalAll on "},
218  };
219  ABSL_CONST_INIT static absl::base_internal::SpinLock synch_event_mu(
220      absl::kConstInit, base_internal::SCHEDULE_KERNEL_ONLY);
221  static constexpr uint32_t kNSynchEvent = 1031;
222  static struct SynchEvent {  
223    int refcount ABSL_GUARDED_BY(synch_event_mu);
224    SynchEvent* next ABSL_GUARDED_BY(synch_event_mu);
225    uintptr_t masked_addr;  
226    void (*invariant)(void* arg);  
227    void* arg;                     
228    bool log;                      
229    char name[1];  
230  }* synch_event[kNSynchEvent] ABSL_GUARDED_BY(synch_event_mu);
231  static SynchEvent* EnsureSynchEvent(std::atomic<intptr_t>* addr,
232                                      const char* name, intptr_t bits,
233                                      intptr_t lockbit) {
234    uint32_t h = reinterpret_cast<uintptr_t>(addr) % kNSynchEvent;
235    SynchEvent* e;
236    synch_event_mu.Lock();
237    for (e = synch_event[h];
238         e != nullptr && e->masked_addr != base_internal::HidePtr(addr);
239         e = e->next) {
240    }
241    if (e == nullptr) {  
242      if (name == nullptr) {
243        name = "";
244      }
245      size_t l = strlen(name);
246      e = reinterpret_cast<SynchEvent*>(
247          base_internal::LowLevelAlloc::Alloc(sizeof(*e) + l));
248      e->refcount = 2;  
249      e->masked_addr = base_internal::HidePtr(addr);
250      e->invariant = nullptr;
251      e->arg = nullptr;
252      e->log = false;
253      strcpy(e->name, name);  
254      e->next = synch_event[h];
255      AtomicSetBits(addr, bits, lockbit);
256      synch_event[h] = e;
257    } else {
258      e->refcount++;  
259    }
260    synch_event_mu.Unlock();
261    return e;
262  }
263  static void DeleteSynchEvent(SynchEvent* e) {
264    base_internal::LowLevelAlloc::Free(e);
265  }
266  static void UnrefSynchEvent(SynchEvent* e) {
267    if (e != nullptr) {
268      synch_event_mu.Lock();
269      bool del = (--(e->refcount) == 0);
270      synch_event_mu.Unlock();
271      if (del) {
272        DeleteSynchEvent(e);
273      }
274    }
275  }
276  static void ForgetSynchEvent(std::atomic<intptr_t>* addr, intptr_t bits,
277                               intptr_t lockbit) {
278    uint32_t h = reinterpret_cast<uintptr_t>(addr) % kNSynchEvent;
279    SynchEvent** pe;
280    SynchEvent* e;
281    synch_event_mu.Lock();
282    for (pe = &synch_event[h];
283         (e = *pe) != nullptr && e->masked_addr != base_internal::HidePtr(addr);
284         pe = &e->next) {
285    }
286    bool del = false;
287    if (e != nullptr) {
288      *pe = e->next;
289      del = (--(e->refcount) == 0);
290    }
291    AtomicClearBits(addr, bits, lockbit);
292    synch_event_mu.Unlock();
293    if (del) {
294      DeleteSynchEvent(e);
295    }
296  }
297  static SynchEvent* GetSynchEvent(const void* addr) {
298    uint32_t h = reinterpret_cast<uintptr_t>(addr) % kNSynchEvent;
299    SynchEvent* e;
300    synch_event_mu.Lock();
301    for (e = synch_event[h];
302         e != nullptr && e->masked_addr != base_internal::HidePtr(addr);
303         e = e->next) {
304    }
305    if (e != nullptr) {
306      e->refcount++;
307    }
308    synch_event_mu.Unlock();
309    return e;
310  }
311  static void PostSynchEvent(void* obj, int ev) {
312    SynchEvent* e = GetSynchEvent(obj);
313    if (e == nullptr || e->log) {
314      void* pcs[40];
315      int n = absl::GetStackTrace(pcs, ABSL_ARRAYSIZE(pcs), 1);
316      char buffer[ABSL_ARRAYSIZE(pcs) * 24];
317      int pos = snprintf(buffer, sizeof(buffer), " @");
318      for (int i = 0; i != n; i++) {
319        int b = snprintf(&buffer[pos], sizeof(buffer) - static_cast<size_t>(pos),
320                         " %p", pcs[i]);
321        if (b < 0 ||
322            static_cast<size_t>(b) >= sizeof(buffer) - static_cast<size_t>(pos)) {
323          break;
324        }
325        pos += b;
326      }
327      ABSL_RAW_LOG(INFO, "%s%p %s %s", event_properties[ev].msg, obj,
328                   (e == nullptr ? "" : e->name), buffer);
329    }
330    const int flags = event_properties[ev].flags;
331    if ((flags & SYNCH_F_LCK) != 0 && e != nullptr && e->invariant != nullptr) {
332      struct local {
333        static bool pred(SynchEvent* ev) {
334          (*ev->invariant)(ev->arg);
335          return false;
336        }
337      };
338      Condition cond(&local::pred, e);
339      Mutex* mu = static_cast<Mutex*>(obj);
340      const bool locking = (flags & SYNCH_F_UNLOCK) == 0;
341      const bool trylock = (flags & SYNCH_F_TRY) != 0;
342      const bool read_lock = (flags & SYNCH_F_R) != 0;
343      EvalConditionAnnotated(&cond, mu, locking, trylock, read_lock);
344    }
345    UnrefSynchEvent(e);
346  }
347  struct SynchWaitParams {
348    SynchWaitParams(Mutex::MuHow how_arg, const Condition* cond_arg,
349                    KernelTimeout timeout_arg, Mutex* cvmu_arg,
350                    PerThreadSynch* thread_arg,
351                    std::atomic<intptr_t>* cv_word_arg)
352        : how(how_arg),
353          cond(cond_arg),
354          timeout(timeout_arg),
355          cvmu(cvmu_arg),
356          thread(thread_arg),
357          cv_word(cv_word_arg),
358          contention_start_cycles(CycleClock::Now()),
359          should_submit_contention_data(false) {}
360    const Mutex::MuHow how;  
361    const Condition* cond;   
362    KernelTimeout timeout;  
363    Mutex* const cvmu;      
364    PerThreadSynch* const thread;  
365    std::atomic<intptr_t>* cv_word;
366    int64_t contention_start_cycles;  
367    bool should_submit_contention_data;
368  };
369  struct SynchLocksHeld {
370    int n;          
371    bool overflow;  
372    struct {
373      Mutex* mu;      
374      int32_t count;  
375      GraphId id;     
376    } locks[40];
377  };
378  static PerThreadSynch* const kPerThreadSynchNull =
379      reinterpret_cast<PerThreadSynch*>(1);
380  static SynchLocksHeld* LocksHeldAlloc() {
381    SynchLocksHeld* ret = reinterpret_cast<SynchLocksHeld*>(
382        base_internal::LowLevelAlloc::Alloc(sizeof(SynchLocksHeld)));
383    ret->n = 0;
384    ret->overflow = false;
385    return ret;
386  }
387  static PerThreadSynch* Synch_GetPerThread() {
388    ThreadIdentity* identity = GetOrCreateCurrentThreadIdentity();
389    return &identity->per_thread_synch;
390  }
391  static PerThreadSynch* Synch_GetPerThreadAnnotated(Mutex* mu) {
392    if (mu) {
393      ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
394    }
395    PerThreadSynch* w = Synch_GetPerThread();
396    if (mu) {
397      ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
398    }
399    return w;
400  }
401  static SynchLocksHeld* Synch_GetAllLocks() {
402    PerThreadSynch* s = Synch_GetPerThread();
403    if (s->all_locks == nullptr) {
404      s->all_locks = LocksHeldAlloc();  
405    }
406    return s->all_locks;
407  }
408  void Mutex::IncrementSynchSem(Mutex* mu, PerThreadSynch* w) {
409    if (mu) {
410      ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
411      ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_BEGIN();
412      PerThreadSem::Post(w->thread_identity());
413      ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_END();
414      ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
415    } else {
416      PerThreadSem::Post(w->thread_identity());
417    }
418  }
419  bool Mutex::DecrementSynchSem(Mutex* mu, PerThreadSynch* w, KernelTimeout t) {
420    if (mu) {
421      ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
422    }
423    assert(w == Synch_GetPerThread());
424    static_cast<void>(w);
425    bool res = PerThreadSem::Wait(t);
426    if (mu) {
427      ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
428    }
429    return res;
430  }
431  void Mutex::InternalAttemptToUseMutexInFatalSignalHandler() {
432    ThreadIdentity* identity = CurrentThreadIdentityIfPresent();
433    if (identity != nullptr) {
434      identity->per_thread_synch.suppress_fatal_errors = true;
435    }
436    synch_deadlock_detection.store(OnDeadlockCycle::kIgnore,
437                                   std::memory_order_release);
438  }
439  static const intptr_t kMuReader = 0x0001L;  
440  static const intptr_t kMuDesig = 0x0002L;
441  static const intptr_t kMuWait = 0x0004L;    
442  static const intptr_t kMuWriter = 0x0008L;  
443  static const intptr_t kMuEvent = 0x0010L;   
444  static const intptr_t kMuWrWait = 0x0020L;
445  static const intptr_t kMuSpin = 0x0040L;  
446  static const intptr_t kMuLow = 0x00ffL;   
447  static const intptr_t kMuHigh = ~kMuLow;  
448  enum {
449    kGdbMuSpin = kMuSpin,
450    kGdbMuEvent = kMuEvent,
451    kGdbMuWait = kMuWait,
452    kGdbMuWriter = kMuWriter,
453    kGdbMuDesig = kMuDesig,
454    kGdbMuWrWait = kMuWrWait,
455    kGdbMuReader = kMuReader,
456    kGdbMuLow = kMuLow,
457  };
458  static const intptr_t kMuOne = 0x0100;  
459  static const int kMuHasBlocked = 0x01;  
460  static const int kMuIsCond = 0x02;      
461  static_assert(PerThreadSynch::kAlignment > kMuLow,
462                "PerThreadSynch::kAlignment must be greater than kMuLow");
463  struct MuHowS {
464    intptr_t fast_need_zero;
465    intptr_t fast_or;
466    intptr_t fast_add;
467    intptr_t slow_need_zero;  
468    intptr_t slow_inc_need_zero;  
469  };
470  static const MuHowS kSharedS = {
471      kMuWriter | kMuWait | kMuEvent,   
472      kMuReader,                        
473      kMuOne,                           
474      kMuWriter | kMuWait,              
475      kMuSpin | kMuWriter | kMuWrWait,  
476  };
477  static const MuHowS kExclusiveS = {
478      kMuWriter | kMuReader | kMuEvent,  
479      kMuWriter,                         
480      0,                                 
481      kMuWriter | kMuReader,             
482      ~static_cast<intptr_t>(0),         
483  };
484  static const Mutex::MuHow kShared = &kSharedS;        
485  static const Mutex::MuHow kExclusive = &kExclusiveS;  
486  #ifdef NDEBUG
487  static constexpr bool kDebugMode = false;
488  #else
489  static constexpr bool kDebugMode = true;
490  #endif
491  #ifdef ABSL_INTERNAL_HAVE_TSAN_INTERFACE
492  static unsigned TsanFlags(Mutex::MuHow how) {
493    return how == kShared ? __tsan_mutex_read_lock : 0;
494  }
495  #endif
496  static bool DebugOnlyIsExiting() {
497    return false;
498  }
499  Mutex::~Mutex() {
500    intptr_t v = mu_.load(std::memory_order_relaxed);
501    if ((v & kMuEvent) != 0 && !DebugOnlyIsExiting()) {
502      ForgetSynchEvent(&this->mu_, kMuEvent, kMuSpin);
503    }
504    if (kDebugMode) {
505      this->ForgetDeadlockInfo();
506    }
507    ABSL_TSAN_MUTEX_DESTROY(this, __tsan_mutex_not_static);
508  }
509  void Mutex::EnableDebugLog(const char* name) {
510    SynchEvent* e = EnsureSynchEvent(&this->mu_, name, kMuEvent, kMuSpin);
511    e->log = true;
512    UnrefSynchEvent(e);
513  }
514  void EnableMutexInvariantDebugging(bool enabled) {
515    synch_check_invariants.store(enabled, std::memory_order_release);
516  }
517  void Mutex::EnableInvariantDebugging(void (*invariant)(void*), void* arg) {
518    if (synch_check_invariants.load(std::memory_order_acquire) &&
519        invariant != nullptr) {
520      SynchEvent* e = EnsureSynchEvent(&this->mu_, nullptr, kMuEvent, kMuSpin);
521      e->invariant = invariant;
522      e->arg = arg;
523      UnrefSynchEvent(e);
524    }
525  }
526  void SetMutexDeadlockDetectionMode(OnDeadlockCycle mode) {
527    synch_deadlock_detection.store(mode, std::memory_order_release);
528  }
529  static bool MuEquivalentWaiter(PerThreadSynch* x, PerThreadSynch* y) {
530    return x->waitp->how == y->waitp->how && x->priority == y->priority &&
531           Condition::GuaranteedEqual(x->waitp->cond, y->waitp->cond);
532  }
533  static inline PerThreadSynch* GetPerThreadSynch(intptr_t v) {
534    return reinterpret_cast<PerThreadSynch*>(v & kMuHigh);
535  }
536  static PerThreadSynch* Skip(PerThreadSynch* x) {
537    PerThreadSynch* x0 = nullptr;
538    PerThreadSynch* x1 = x;
539    PerThreadSynch* x2 = x->skip;
540    if (x2 != nullptr) {
541      while ((x0 = x1, x1 = x2, x2 = x2->skip) != nullptr) {
542        x0->skip = x2;  
543      }
544      x->skip = x1;  
545    }
546    return x1;
547  }
548  static void FixSkip(PerThreadSynch* ancestor, PerThreadSynch* to_be_removed) {
549    if (ancestor->skip == to_be_removed) {  
550      if (to_be_removed->skip != nullptr) {
551        ancestor->skip = to_be_removed->skip;  
552      } else if (ancestor->next != to_be_removed) {  
553        ancestor->skip = ancestor->next;             
554      } else {
555        ancestor->skip = nullptr;  
556      }
557    }
558  }
559  static void CondVarEnqueue(SynchWaitParams* waitp);
560  static PerThreadSynch* Enqueue(PerThreadSynch* head, SynchWaitParams* waitp,
561                                 intptr_t mu, int flags) {
562    if (waitp->cv_word != nullptr) {
563      CondVarEnqueue(waitp);
564      return head;
565    }
566    PerThreadSynch* s = waitp->thread;
567    ABSL_RAW_CHECK(
568        s->waitp == nullptr ||    
569            s->waitp == waitp ||  
570            s->suppress_fatal_errors,
571        "detected illegal recursion into Mutex code");
572    s->waitp = waitp;
573    s->skip = nullptr;   
574    s->may_skip = true;  
575    s->wake = false;     
576    s->cond_waiter = ((flags & kMuIsCond) != 0);
577  #ifdef ABSL_HAVE_PTHREAD_GETSCHEDPARAM
578    int64_t now_cycles = CycleClock::Now();
579    if (s->next_priority_read_cycles < now_cycles) {
580      int policy;
581      struct sched_param param;
582      const int err = pthread_getschedparam(pthread_self(), &policy, &param);
583      if (err != 0) {
584        ABSL_RAW_LOG(ERROR, "pthread_getschedparam failed: %d", err);
585      } else {
586        s->priority = param.sched_priority;
587        s->next_priority_read_cycles =
588            now_cycles + static_cast<int64_t>(CycleClock::Frequency());
589      }
590    }
591  #endif
592    if (head == nullptr) {         
593      s->next = s;                 
594      s->readers = mu;             
595      s->maybe_unlocking = false;  
596      head = s;                    
597    } else {
598      PerThreadSynch* enqueue_after = nullptr;  
599  #ifdef ABSL_HAVE_PTHREAD_GETSCHEDPARAM
600      if (s->priority > head->priority) {  
601        if (!head->maybe_unlocking) {
602          PerThreadSynch* advance_to = head;  
603          do {
604            enqueue_after = advance_to;
605            advance_to = Skip(enqueue_after->next);
606          } while (s->priority <= advance_to->priority);
607        } else if (waitp->how == kExclusive &&
608                   Condition::GuaranteedEqual(waitp->cond, nullptr)) {
609          enqueue_after = head;  
610        }
611      }
612  #endif
613      if (enqueue_after != nullptr) {
614        s->next = enqueue_after->next;
615        enqueue_after->next = s;
616        ABSL_RAW_CHECK(enqueue_after->skip == nullptr ||
617                           MuEquivalentWaiter(enqueue_after, s),
618                       "Mutex Enqueue failure");
619        if (enqueue_after != head && enqueue_after->may_skip &&
620            MuEquivalentWaiter(enqueue_after, enqueue_after->next)) {
621          enqueue_after->skip = enqueue_after->next;
622        }
623        if (MuEquivalentWaiter(s, s->next)) {  
624          s->skip = s->next;                   
625        }
626      } else {  
627        s->next = head->next;  
628        head->next = s;
629        s->readers = head->readers;  
630        s->maybe_unlocking = head->maybe_unlocking;  
631        if (head->may_skip && MuEquivalentWaiter(head, s)) {
632          head->skip = s;
633        }
634        head = s;  
635      }
636    }
637    s->state.store(PerThreadSynch::kQueued, std::memory_order_relaxed);
638    return head;
639  }
640  static PerThreadSynch* Dequeue(PerThreadSynch* head, PerThreadSynch* pw) {
641    PerThreadSynch* w = pw->next;
642    pw->next = w->next;                 
643    if (head == w) {                    
644      head = (pw == w) ? nullptr : pw;  
645    } else if (pw != head && MuEquivalentWaiter(pw, pw->next)) {
646      if (pw->next->skip !=
647          nullptr) {  
648        pw->skip = pw->next->skip;
649      } else {  
650        pw->skip = pw->next;
651      }
652    }
653    return head;
654  }
655  static PerThreadSynch* DequeueAllWakeable(PerThreadSynch* head,
656                                            PerThreadSynch* pw,
657                                            PerThreadSynch** wake_tail) {
658    PerThreadSynch* orig_h = head;
659    PerThreadSynch* w = pw->next;
660    bool skipped = false;
661    do {
662      if (w->wake) {  
663        ABSL_RAW_CHECK(pw->skip == nullptr, "bad skip in DequeueAllWakeable");
664        head = Dequeue(head, pw);
665        w->next = *wake_tail;               
666        *wake_tail = w;                     
667        wake_tail = &w->next;               
668        if (w->waitp->how == kExclusive) {  
669          break;
670        }
671      } else {         
672        pw = Skip(w);  
673        skipped = true;
674      }
675      w = pw->next;
676    } while (orig_h == head && (pw != head || !skipped));
677    return head;
678  }
679  void Mutex::TryRemove(PerThreadSynch* s) {
680    SchedulingGuard::ScopedDisable disable_rescheduling;
681    intptr_t v = mu_.load(std::memory_order_relaxed);
682    if ((v & (kMuWait | kMuSpin | kMuWriter | kMuReader)) == kMuWait &&
683        mu_.compare_exchange_strong(v, v | kMuSpin | kMuWriter,
684                                    std::memory_order_acquire,
685                                    std::memory_order_relaxed)) {
686      PerThreadSynch* h = GetPerThreadSynch(v);
687      if (h != nullptr) {
688        PerThreadSynch* pw = h;  
689        PerThreadSynch* w;
690        if ((w = pw->next) != s) {  
691          do {                      
692            if (!MuEquivalentWaiter(s, w)) {
693              pw = Skip(w);  
694            } else {          
695              FixSkip(w, s);  
696              pw = w;
697            }
698          } while ((w = pw->next) != s && pw != h);
699        }
700        if (w == s) {  
701          h = Dequeue(h, pw);
702          s->next = nullptr;
703          s->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
704        }
705      }
706      intptr_t nv;
707      do {  
708        v = mu_.load(std::memory_order_relaxed);
709        nv = v & (kMuDesig | kMuEvent);
710        if (h != nullptr) {
711          nv |= kMuWait | reinterpret_cast<intptr_t>(h);
712          h->readers = 0;              
713          h->maybe_unlocking = false;  
714        }
715      } while (!mu_.compare_exchange_weak(v, nv, std::memory_order_release,
716                                          std::memory_order_relaxed));
717    }
718  }
719  void Mutex::Block(PerThreadSynch* s) {
720    while (s->state.load(std::memory_order_acquire) == PerThreadSynch::kQueued) {
721      if (!DecrementSynchSem(this, s, s->waitp->timeout)) {
722        this->TryRemove(s);
723        int c = 0;
724        while (s->next != nullptr) {
725          c = synchronization_internal::MutexDelay(c, GENTLE);
726          this->TryRemove(s);
727        }
728        if (kDebugMode) {
729          this->TryRemove(s);
730        }
731        s->waitp->timeout = KernelTimeout::Never();  
732        s->waitp->cond = nullptr;  
733      }
734    }
735    ABSL_RAW_CHECK(s->waitp != nullptr || s->suppress_fatal_errors,
736                   "detected illegal recursion in Mutex code");
737    s->waitp = nullptr;
738  }
739  PerThreadSynch* Mutex::Wakeup(PerThreadSynch* w) {
740    PerThreadSynch* next = w->next;
741    w->next = nullptr;
742    w->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
743    IncrementSynchSem(this, w);
744    return next;
745  }
746  static GraphId GetGraphIdLocked(Mutex* mu)
747      ABSL_EXCLUSIVE_LOCKS_REQUIRED(deadlock_graph_mu) {
748    if (!deadlock_graph) {  
749      deadlock_graph =
750          new (base_internal::LowLevelAlloc::Alloc(sizeof(*deadlock_graph)))
751              GraphCycles;
752    }
753    return deadlock_graph->GetId(mu);
754  }
755  static GraphId GetGraphId(Mutex* mu) ABSL_LOCKS_EXCLUDED(deadlock_graph_mu) {
756    deadlock_graph_mu.Lock();
757    GraphId id = GetGraphIdLocked(mu);
758    deadlock_graph_mu.Unlock();
759    return id;
760  }
761  static void LockEnter(Mutex* mu, GraphId id, SynchLocksHeld* held_locks) {
762    int n = held_locks->n;
763    int i = 0;
764    while (i != n && held_locks->locks[i].id != id) {
765      i++;
766    }
767    if (i == n) {
768      if (n == ABSL_ARRAYSIZE(held_locks->locks)) {
769        held_locks->overflow = true;  
770      } else {                        
771        held_locks->locks[i].mu = mu;
772        held_locks->locks[i].count = 1;
773        held_locks->locks[i].id = id;
774        held_locks->n = n + 1;
775      }
776    } else {
777      held_locks->locks[i].count++;
778    }
779  }
780  static void LockLeave(Mutex* mu, GraphId id, SynchLocksHeld* held_locks) {
781    int n = held_locks->n;
782    int i = 0;
783    while (i != n && held_locks->locks[i].id != id) {
784      i++;
785    }
786    if (i == n) {
787      if (!held_locks->overflow) {
788        i = 0;
789        while (i != n && held_locks->locks[i].mu != mu) {
790          i++;
791        }
792        if (i == n) {  
793          SynchEvent* mu_events = GetSynchEvent(mu);
794          ABSL_RAW_LOG(FATAL,
795                       "thread releasing lock it does not hold: %p %s; "
796                       ,
797                       static_cast<void*>(mu),
798                       mu_events == nullptr ? "" : mu_events->name);
799        }
800      }
801    } else if (held_locks->locks[i].count == 1) {
802      held_locks->n = n - 1;
803      held_locks->locks[i] = held_locks->locks[n - 1];
804      held_locks->locks[n - 1].id = InvalidGraphId();
805      held_locks->locks[n - 1].mu =
806          nullptr;  
807    } else {
808      assert(held_locks->locks[i].count > 0);
809      held_locks->locks[i].count--;
810    }
811  }
812  static inline void DebugOnlyLockEnter(Mutex* mu) {
813    if (kDebugMode) {
814      if (synch_deadlock_detection.load(std::memory_order_acquire) !=
815          OnDeadlockCycle::kIgnore) {
816        LockEnter(mu, GetGraphId(mu), Synch_GetAllLocks());
817      }
818    }
819  }
820  static inline void DebugOnlyLockEnter(Mutex* mu, GraphId id) {
821    if (kDebugMode) {
822      if (synch_deadlock_detection.load(std::memory_order_acquire) !=
823          OnDeadlockCycle::kIgnore) {
824        LockEnter(mu, id, Synch_GetAllLocks());
825      }
826    }
827  }
828  static inline void DebugOnlyLockLeave(Mutex* mu) {
829    if (kDebugMode) {
830      if (synch_deadlock_detection.load(std::memory_order_acquire) !=
831          OnDeadlockCycle::kIgnore) {
832        LockLeave(mu, GetGraphId(mu), Synch_GetAllLocks());
833      }
834    }
835  }
836  static char* StackString(void** pcs, int n, char* buf, int maxlen,
837                           bool symbolize) {
838    static constexpr int kSymLen = 200;
839    char sym[kSymLen];
840    int len = 0;
841    for (int i = 0; i != n; i++) {
842      if (len >= maxlen)
843        return buf;
844      size_t count = static_cast<size_t>(maxlen - len);
845      if (symbolize) {
846        if (!absl::Symbolize(pcs[i], sym, kSymLen)) {
847          sym[0] = '\0';
848        }
849        snprintf(buf + len, count, "%s\t@ %p %s\n", (i == 0 ? "\n" : ""), pcs[i],
850                 sym);
851      } else {
852        snprintf(buf + len, count, " %p", pcs[i]);
853      }
854      len += strlen(&buf[len]);
855    }
856    return buf;
857  }
858  static char* CurrentStackString(char* buf, int maxlen, bool symbolize) {
859    void* pcs[40];
860    return StackString(pcs, absl::GetStackTrace(pcs, ABSL_ARRAYSIZE(pcs), 2), buf,
861                       maxlen, symbolize);
862  }
863  namespace {
864  enum {
865    kMaxDeadlockPathLen = 10
866  };  
867  struct DeadlockReportBuffers {
868    char buf[6100];
869    GraphId path[kMaxDeadlockPathLen];
870  };
871  struct ScopedDeadlockReportBuffers {
872    ScopedDeadlockReportBuffers() {
873      b = reinterpret_cast<DeadlockReportBuffers*>(
874          base_internal::LowLevelAlloc::Alloc(sizeof(*b)));
875    }
876    ~ScopedDeadlockReportBuffers() { base_internal::LowLevelAlloc::Free(b); }
877    DeadlockReportBuffers* b;
878  };
879  int GetStack(void** stack, int max_depth) {
880    return absl::GetStackTrace(stack, max_depth, 3);
881  }
882  }  
883  static GraphId DeadlockCheck(Mutex* mu) {
884    if (synch_deadlock_detection.load(std::memory_order_acquire) ==
885        OnDeadlockCycle::kIgnore) {
886      return InvalidGraphId();
887    }
888    SynchLocksHeld* all_locks = Synch_GetAllLocks();
889    absl::base_internal::SpinLockHolder lock(&deadlock_graph_mu);
890    const GraphId mu_id = GetGraphIdLocked(mu);
891    if (all_locks->n == 0) {
892      return mu_id;
893    }
894    deadlock_graph->UpdateStackTrace(mu_id, all_locks->n + 1, GetStack);
895    for (int i = 0; i != all_locks->n; i++) {
896      const GraphId other_node_id = all_locks->locks[i].id;
897      const Mutex* other =
898          static_cast<const Mutex*>(deadlock_graph->Ptr(other_node_id));
899      if (other == nullptr) {
900        continue;
901      }
902      if (!deadlock_graph->InsertEdge(other_node_id, mu_id)) {
903        ScopedDeadlockReportBuffers scoped_buffers;
904        DeadlockReportBuffers* b = scoped_buffers.b;
905        static int number_of_reported_deadlocks = 0;
906        number_of_reported_deadlocks++;
907        bool symbolize = number_of_reported_deadlocks <= 2;
908        ABSL_RAW_LOG(ERROR, "Potential Mutex deadlock: %s",
909                     CurrentStackString(b->buf, sizeof (b->buf), symbolize));
910        size_t len = 0;
911        for (int j = 0; j != all_locks->n; j++) {
912          void* pr = deadlock_graph->Ptr(all_locks->locks[j].id);
913          if (pr != nullptr) {
914            snprintf(b->buf + len, sizeof(b->buf) - len, " %p", pr);
915            len += strlen(&b->buf[len]);
916          }
917        }
918        ABSL_RAW_LOG(ERROR,
919                     "Acquiring absl::Mutex %p while holding %s; a cycle in the "
920                     "historical lock ordering graph has been observed",
921                     static_cast<void*>(mu), b->buf);
922        ABSL_RAW_LOG(ERROR, "Cycle: ");
923        int path_len = deadlock_graph->FindPath(mu_id, other_node_id,
924                                                ABSL_ARRAYSIZE(b->path), b->path);
925        for (int j = 0; j != path_len && j != ABSL_ARRAYSIZE(b->path); j++) {
926          GraphId id = b->path[j];
927          Mutex* path_mu = static_cast<Mutex*>(deadlock_graph->Ptr(id));
928          if (path_mu == nullptr) continue;
929          void** stack;
930          int depth = deadlock_graph->GetStackTrace(id, &stack);
931          snprintf(b->buf, sizeof(b->buf),
932                   "mutex@%p stack: ", static_cast<void*>(path_mu));
933          StackString(stack, depth, b->buf + strlen(b->buf),
934                      static_cast<int>(sizeof(b->buf) - strlen(b->buf)),
935                      symbolize);
936          ABSL_RAW_LOG(ERROR, "%s", b->buf);
937        }
938        if (path_len > static_cast<int>(ABSL_ARRAYSIZE(b->path))) {
939          ABSL_RAW_LOG(ERROR, "(long cycle; list truncated)");
940        }
941        if (synch_deadlock_detection.load(std::memory_order_acquire) ==
942            OnDeadlockCycle::kAbort) {
943          deadlock_graph_mu.Unlock();  
944          ABSL_RAW_LOG(FATAL, "dying due to potential deadlock");
945          return mu_id;
946        }
947        break;  
948      }
949    }
950    return mu_id;
951  }
952  static inline GraphId DebugOnlyDeadlockCheck(Mutex* mu) {
953    if (kDebugMode && synch_deadlock_detection.load(std::memory_order_acquire) !=
954                          OnDeadlockCycle::kIgnore) {
955      return DeadlockCheck(mu);
956    } else {
957      return InvalidGraphId();
958    }
959  }
960  void Mutex::ForgetDeadlockInfo() {
961    if (kDebugMode && synch_deadlock_detection.load(std::memory_order_acquire) !=
962                          OnDeadlockCycle::kIgnore) {
963      deadlock_graph_mu.Lock();
964      if (deadlock_graph != nullptr) {
965        deadlock_graph->RemoveNode(this);
966      }
967      deadlock_graph_mu.Unlock();
968    }
969  }
970  void Mutex::AssertNotHeld() const {
971    if (kDebugMode &&
972        (mu_.load(std::memory_order_relaxed) & (kMuWriter | kMuReader)) != 0 &&
973        synch_deadlock_detection.load(std::memory_order_acquire) !=
974            OnDeadlockCycle::kIgnore) {
975      GraphId id = GetGraphId(const_cast<Mutex*>(this));
976      SynchLocksHeld* locks = Synch_GetAllLocks();
977      for (int i = 0; i != locks->n; i++) {
978        if (locks->locks[i].id == id) {
979          SynchEvent* mu_events = GetSynchEvent(this);
980          ABSL_RAW_LOG(FATAL, "thread should not hold mutex %p %s",
981                       static_cast<const void*>(this),
982                       (mu_events == nullptr ? "" : mu_events->name));
983        }
984      }
985    }
986  }
987  static bool TryAcquireWithSpinning(std::atomic<intptr_t>* mu) {
988    int c = GetMutexGlobals().spinloop_iterations;
989    do {  
990      intptr_t v = mu->load(std::memory_order_relaxed);
991      if ((v & (kMuReader | kMuEvent)) != 0) {
992        return false;                       
993      } else if (((v & kMuWriter) == 0) &&  
994                 mu->compare_exchange_strong(v, kMuWriter | v,
995                                             std::memory_order_acquire,
996                                             std::memory_order_relaxed)) {
997        return true;
998      }
999    } while (--c > 0);
1000    return false;
1001  }
1002  void Mutex::Lock() {
1003    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1004    GraphId id = DebugOnlyDeadlockCheck(this);
1005    intptr_t v = mu_.load(std::memory_order_relaxed);
1006    if ((v & (kMuWriter | kMuReader | kMuEvent)) != 0 ||
1007        !mu_.compare_exchange_strong(v, kMuWriter | v, std::memory_order_acquire,
1008                                     std::memory_order_relaxed)) {
1009      if (!TryAcquireWithSpinning(&this->mu_)) {
1010        this->LockSlow(kExclusive, nullptr, 0);
1011      }
1012    }
1013    DebugOnlyLockEnter(this, id);
1014    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1015  }
1016  void Mutex::ReaderLock() {
1017    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1018    GraphId id = DebugOnlyDeadlockCheck(this);
1019    intptr_t v = mu_.load(std::memory_order_relaxed);
1020    if ((v & (kMuWriter | kMuWait | kMuEvent)) != 0 ||
1021        !mu_.compare_exchange_strong(v, (kMuReader | v) + kMuOne,
1022                                     std::memory_order_acquire,
1023                                     std::memory_order_relaxed)) {
1024      this->LockSlow(kShared, nullptr, 0);
1025    }
1026    DebugOnlyLockEnter(this, id);
1027    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1028  }
1029  void Mutex::LockWhen(const Condition& cond) {
1030    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1031    GraphId id = DebugOnlyDeadlockCheck(this);
1032    this->LockSlow(kExclusive, &cond, 0);
1033    DebugOnlyLockEnter(this, id);
1034    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1035  }
1036  bool Mutex::LockWhenWithTimeout(const Condition& cond, absl::Duration timeout) {
1037    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1038    GraphId id = DebugOnlyDeadlockCheck(this);
1039    bool res = LockSlowWithDeadline(kExclusive, &cond, KernelTimeout(timeout), 0);
1040    DebugOnlyLockEnter(this, id);
1041    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1042    return res;
1043  }
1044  bool Mutex::LockWhenWithDeadline(const Condition& cond, absl::Time deadline) {
1045    ABSL_TSAN_MUTEX_PRE_LOCK(this, 0);
1046    GraphId id = DebugOnlyDeadlockCheck(this);
1047    bool res =
1048        LockSlowWithDeadline(kExclusive, &cond, KernelTimeout(deadline), 0);
1049    DebugOnlyLockEnter(this, id);
1050    ABSL_TSAN_MUTEX_POST_LOCK(this, 0, 0);
1051    return res;
1052  }
1053  void Mutex::ReaderLockWhen(const Condition& cond) {
1054    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1055    GraphId id = DebugOnlyDeadlockCheck(this);
1056    this->LockSlow(kShared, &cond, 0);
1057    DebugOnlyLockEnter(this, id);
1058    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1059  }
1060  bool Mutex::ReaderLockWhenWithTimeout(const Condition& cond,
1061                                        absl::Duration timeout) {
1062    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1063    GraphId id = DebugOnlyDeadlockCheck(this);
1064    bool res = LockSlowWithDeadline(kShared, &cond, KernelTimeout(timeout), 0);
1065    DebugOnlyLockEnter(this, id);
1066    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1067    return res;
1068  }
1069  bool Mutex::ReaderLockWhenWithDeadline(const Condition& cond,
1070                                         absl::Time deadline) {
1071    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_read_lock);
1072    GraphId id = DebugOnlyDeadlockCheck(this);
1073    bool res = LockSlowWithDeadline(kShared, &cond, KernelTimeout(deadline), 0);
1074    DebugOnlyLockEnter(this, id);
1075    ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_read_lock, 0);
1076    return res;
1077  }
1078  void Mutex::Await(const Condition& cond) {
1079    if (cond.Eval()) {  
1080      if (kDebugMode) {
1081        this->AssertReaderHeld();
1082      }
1083    } else {  
1084      ABSL_RAW_CHECK(this->AwaitCommon(cond, KernelTimeout::Never()),
1085                     "condition untrue on return from Await");
1086    }
1087  }
1088  bool Mutex::AwaitWithTimeout(const Condition& cond, absl::Duration timeout) {
1089    if (cond.Eval()) {  
1090      if (kDebugMode) {
1091        this->AssertReaderHeld();
1092      }
1093      return true;
1094    }
1095    KernelTimeout t{timeout};
1096    bool res = this->AwaitCommon(cond, t);
1097    ABSL_RAW_CHECK(res || t.has_timeout(),
1098                   "condition untrue on return from Await");
1099    return res;
1100  }
1101  bool Mutex::AwaitWithDeadline(const Condition& cond, absl::Time deadline) {
1102    if (cond.Eval()) {  
1103      if (kDebugMode) {
1104        this->AssertReaderHeld();
1105      }
1106      return true;
1107    }
1108    KernelTimeout t{deadline};
1109    bool res = this->AwaitCommon(cond, t);
1110    ABSL_RAW_CHECK(res || t.has_timeout(),
1111                   "condition untrue on return from Await");
1112    return res;
1113  }
1114  bool Mutex::AwaitCommon(const Condition& cond, KernelTimeout t) {
1115    this->AssertReaderHeld();
1116    MuHow how =
1117        (mu_.load(std::memory_order_relaxed) & kMuWriter) ? kExclusive : kShared;
1118    ABSL_TSAN_MUTEX_PRE_UNLOCK(this, TsanFlags(how));
1119    SynchWaitParams waitp(how, &cond, t, nullptr &bsol;*no cvmu*/,
1120                          Synch_GetPerThreadAnnotated(this),
1121                          nullptr &bsol;*no cv_word*/);
1122    int flags = kMuHasBlocked;
1123    if (!Condition::GuaranteedEqual(&cond, nullptr)) {
1124      flags |= kMuIsCond;
1125    }
1126    this->UnlockSlow(&waitp);
1127    this->Block(waitp.thread);
1128    ABSL_TSAN_MUTEX_POST_UNLOCK(this, TsanFlags(how));
1129    ABSL_TSAN_MUTEX_PRE_LOCK(this, TsanFlags(how));
1130    this->LockSlowLoop(&waitp, flags);
1131    bool res = waitp.cond != nullptr ||  
1132               EvalConditionAnnotated(&cond, this, true, false, how == kShared);
1133    ABSL_TSAN_MUTEX_POST_LOCK(this, TsanFlags(how), 0);
1134    return res;
1135  }
1136  bool Mutex::TryLock() {
1137    ABSL_TSAN_MUTEX_PRE_LOCK(this, __tsan_mutex_try_lock);
1138    intptr_t v = mu_.load(std::memory_order_relaxed);
1139    if ((v & (kMuWriter | kMuReader | kMuEvent)) == 0 &&  
1140        mu_.compare_exchange_strong(v, kMuWriter | v, std::memory_order_acquire,
1141                                    std::memory_order_relaxed)) {
1142      DebugOnlyLockEnter(this);
1143      ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_try_lock, 0);
1144      return true;
1145    }
1146    if ((v & kMuEvent) != 0) {                      
1147      if ((v & kExclusive->slow_need_zero) == 0 &&  
1148          mu_.compare_exchange_strong(
1149              v, (kExclusive->fast_or | v) + kExclusive->fast_add,
1150              std::memory_order_acquire, std::memory_order_relaxed)) {
1151        DebugOnlyLockEnter(this);
1152        PostSynchEvent(this, SYNCH_EV_TRYLOCK_SUCCESS);
1153        ABSL_TSAN_MUTEX_POST_LOCK(this, __tsan_mutex_try_lock, 0);
1154        return true;
1155      } else {
1156        PostSynchEvent(this, SYNCH_EV_TRYLOCK_FAILED);
1157      }
1158    }
1159    ABSL_TSAN_MUTEX_POST_LOCK(
1160        this, __tsan_mutex_try_lock | __tsan_mutex_try_lock_failed, 0);
1161    return false;
1162  }
1163  bool Mutex::ReaderTryLock() {
1164    ABSL_TSAN_MUTEX_PRE_LOCK(this,
1165                             __tsan_mutex_read_lock | __tsan_mutex_try_lock);
1166    intptr_t v = mu_.load(std::memory_order_relaxed);
1167    int loop_limit = 5;
1168    while ((v & (kMuWriter | kMuWait | kMuEvent)) == 0 && loop_limit != 0) {
1169      if (mu_.compare_exchange_strong(v, (kMuReader | v) + kMuOne,
1170                                      std::memory_order_acquire,
1171                                      std::memory_order_relaxed)) {
1172        DebugOnlyLockEnter(this);
1173        ABSL_TSAN_MUTEX_POST_LOCK(
1174            this, __tsan_mutex_read_lock | __tsan_mutex_try_lock, 0);
1175        return true;
1176      }
1177      loop_limit--;
1178      v = mu_.load(std::memory_order_relaxed);
1179    }
1180    if ((v & kMuEvent) != 0) {  
1181      loop_limit = 5;
1182      while ((v & kShared->slow_need_zero) == 0 && loop_limit != 0) {
1183        if (mu_.compare_exchange_strong(v, (kMuReader | v) + kMuOne,
1184                                        std::memory_order_acquire,
1185                                        std::memory_order_relaxed)) {
1186          DebugOnlyLockEnter(this);
1187          PostSynchEvent(this, SYNCH_EV_READERTRYLOCK_SUCCESS);
1188          ABSL_TSAN_MUTEX_POST_LOCK(
1189              this, __tsan_mutex_read_lock | __tsan_mutex_try_lock, 0);
1190          return true;
1191        }
1192        loop_limit--;
1193        v = mu_.load(std::memory_order_relaxed);
1194      }
1195      if ((v & kMuEvent) != 0) {
1196        PostSynchEvent(this, SYNCH_EV_READERTRYLOCK_FAILED);
1197      }
1198    }
1199    ABSL_TSAN_MUTEX_POST_LOCK(this,
1200                              __tsan_mutex_read_lock | __tsan_mutex_try_lock |
1201                                  __tsan_mutex_try_lock_failed,
1202                              0);
1203    return false;
1204  }
1205  void Mutex::Unlock() {
1206    ABSL_TSAN_MUTEX_PRE_UNLOCK(this, 0);
1207    DebugOnlyLockLeave(this);
1208    intptr_t v = mu_.load(std::memory_order_relaxed);
1209    if (kDebugMode && ((v & (kMuWriter | kMuReader)) != kMuWriter)) {
1210      ABSL_RAW_LOG(FATAL, "Mutex unlocked when destroyed or not locked: v=0x%x",
1211                   static_cast<unsigned>(v));
1212    }
1213    bool should_try_cas = ((v & (kMuEvent | kMuWriter)) == kMuWriter &&
1214                           (v & (kMuWait | kMuDesig)) != kMuWait);
1215    intptr_t x = (v ^ (kMuWriter | kMuWait)) & (kMuWriter | kMuEvent);
1216    intptr_t y = (v ^ (kMuWriter | kMuWait)) & (kMuWait | kMuDesig);
1217    if (kDebugMode && should_try_cas != (x < y)) {
1218      ABSL_RAW_LOG(FATAL, "internal logic error %llx %llx %llx\n",
1219                   static_cast<long long>(v), static_cast<long long>(x),
1220                   static_cast<long long>(y));
1221    }
1222    if (x < y && mu_.compare_exchange_strong(v, v & ~(kMuWrWait | kMuWriter),
1223                                             std::memory_order_release,
1224                                             std::memory_order_relaxed)) {
1225    } else {
1226      this->UnlockSlow(nullptr &bsol;*no waitp*/);  
1227    }
1228    ABSL_TSAN_MUTEX_POST_UNLOCK(this, 0);
1229  }
1230  static bool ExactlyOneReader(intptr_t v) {
1231    assert((v & (kMuWriter | kMuReader)) == kMuReader);
1232    assert((v & kMuHigh) != 0);
1233    constexpr intptr_t kMuMultipleWaitersMask = kMuHigh ^ kMuOne;
1234    return (v & kMuMultipleWaitersMask) == 0;
1235  }
1236  void Mutex::ReaderUnlock() {
1237    ABSL_TSAN_MUTEX_PRE_UNLOCK(this, __tsan_mutex_read_lock);
1238    DebugOnlyLockLeave(this);
1239    intptr_t v = mu_.load(std::memory_order_relaxed);
1240    assert((v & (kMuWriter | kMuReader)) == kMuReader);
1241    if ((v & (kMuReader | kMuWait | kMuEvent)) == kMuReader) {
1242      intptr_t clear = ExactlyOneReader(v) ? kMuReader | kMuOne : kMuOne;
1243      if (mu_.compare_exchange_strong(v, v - clear, std::memory_order_release,
1244                                      std::memory_order_relaxed)) {
1245        ABSL_TSAN_MUTEX_POST_UNLOCK(this, __tsan_mutex_read_lock);
1246        return;
1247      }
1248    }
1249    this->UnlockSlow(nullptr &bsol;*no waitp*/);  
1250    ABSL_TSAN_MUTEX_POST_UNLOCK(this, __tsan_mutex_read_lock);
1251  }
1252  static intptr_t ClearDesignatedWakerMask(int flag) {
1253    assert(flag >= 0);
1254    assert(flag <= 1);
1255    switch (flag) {
1256      case 0:  
1257        return ~static_cast<intptr_t>(0);
1258      case 1:  
1259        return ~static_cast<intptr_t>(kMuDesig);
1260    }
1261    ABSL_UNREACHABLE();
1262  }
1263  static intptr_t IgnoreWaitingWritersMask(int flag) {
1264    assert(flag >= 0);
1265    assert(flag <= 1);
1266    switch (flag) {
1267      case 0:  
1268        return ~static_cast<intptr_t>(0);
1269      case 1:  
1270        return ~static_cast<intptr_t>(kMuWrWait);
1271    }
1272    ABSL_UNREACHABLE();
1273  }
1274  ABSL_ATTRIBUTE_NOINLINE void Mutex::LockSlow(MuHow how, const Condition* cond,
1275                                               int flags) {
1276    ABSL_RAW_CHECK(
1277        this->LockSlowWithDeadline(how, cond, KernelTimeout::Never(), flags),
1278        "condition untrue on return from LockSlow");
1279  }
1280  static inline bool EvalConditionAnnotated(const Condition* cond, Mutex* mu,
1281                                            bool locking, bool trylock,
1282                                            bool read_lock) {
1283    bool res = false;
1284  #ifdef ABSL_INTERNAL_HAVE_TSAN_INTERFACE
1285    const uint32_t flags = read_lock ? __tsan_mutex_read_lock : 0;
1286    const uint32_t tryflags = flags | (trylock ? __tsan_mutex_try_lock : 0);
1287  #endif
1288    if (locking) {
1289      ABSL_TSAN_MUTEX_POST_LOCK(mu, tryflags, 0);
1290      res = cond->Eval();
1291      ABSL_TSAN_MUTEX_PRE_UNLOCK(mu, flags);
1292      ABSL_TSAN_MUTEX_POST_UNLOCK(mu, flags);
1293      ABSL_TSAN_MUTEX_PRE_LOCK(mu, tryflags);
1294    } else {
1295      ABSL_TSAN_MUTEX_POST_UNLOCK(mu, flags);
1296      ABSL_TSAN_MUTEX_PRE_LOCK(mu, flags);
1297      ABSL_TSAN_MUTEX_POST_LOCK(mu, flags, 0);
1298      res = cond->Eval();
1299      ABSL_TSAN_MUTEX_PRE_UNLOCK(mu, flags);
1300    }
1301    static_cast<void>(mu);
1302    static_cast<void>(trylock);
1303    static_cast<void>(read_lock);
1304    return res;
1305  }
1306  static inline bool EvalConditionIgnored(Mutex* mu, const Condition* cond) {
1307    ABSL_TSAN_MUTEX_PRE_DIVERT(mu, 0);
1308    ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_BEGIN();
1309    bool res = cond->Eval();
1310    ABSL_ANNOTATE_IGNORE_READS_AND_WRITES_END();
1311    ABSL_TSAN_MUTEX_POST_DIVERT(mu, 0);
1312    static_cast<void>(mu);  
1313    return res;
1314  }
1315  bool Mutex::LockSlowWithDeadline(MuHow how, const Condition* cond,
1316                                   KernelTimeout t, int flags) {
1317    intptr_t v = mu_.load(std::memory_order_relaxed);
1318    bool unlock = false;
1319    if ((v & how->fast_need_zero) == 0 &&  
1320        mu_.compare_exchange_strong(
1321            v,
1322            (how->fast_or |
1323             (v & ClearDesignatedWakerMask(flags & kMuHasBlocked))) +
1324                how->fast_add,
1325            std::memory_order_acquire, std::memory_order_relaxed)) {
1326      if (cond == nullptr ||
1327          EvalConditionAnnotated(cond, this, true, false, how == kShared)) {
1328        return true;
1329      }
1330      unlock = true;
1331    }
1332    SynchWaitParams waitp(how, cond, t, nullptr &bsol;*no cvmu*/,
1333                          Synch_GetPerThreadAnnotated(this),
1334                          nullptr &bsol;*no cv_word*/);
1335    if (!Condition::GuaranteedEqual(cond, nullptr)) {
1336      flags |= kMuIsCond;
1337    }
1338    if (unlock) {
1339      this->UnlockSlow(&waitp);
1340      this->Block(waitp.thread);
1341      flags |= kMuHasBlocked;
1342    }
1343    this->LockSlowLoop(&waitp, flags);
1344    return waitp.cond != nullptr ||  
1345           cond == nullptr ||
1346           EvalConditionAnnotated(cond, this, true, false, how == kShared);
1347  }
1348  #define RAW_CHECK_FMT(cond, ...)                                   \
1349    do {                                                             \
1350      if (ABSL_PREDICT_FALSE(!(cond))) {                             \
1351        ABSL_RAW_LOG(FATAL, "Check " #cond " failed: " __VA_ARGS__); \
1352      }                                                              \
1353    } while (0)
1354  static void CheckForMutexCorruption(intptr_t v, const char* label) {
1355    const uintptr_t w = static_cast<uintptr_t>(v ^ kMuWait);
1356    static_assert(kMuReader << 3 == kMuWriter, "must match");
1357    static_assert(kMuWait << 3 == kMuWrWait, "must match");
1358    if (ABSL_PREDICT_TRUE((w & (w << 3) & (kMuWriter | kMuWrWait)) == 0)) return;
1359    RAW_CHECK_FMT((v & (kMuWriter | kMuReader)) != (kMuWriter | kMuReader),
1360                  "%s: Mutex corrupt: both reader and writer lock held: %p",
1361                  label, reinterpret_cast<void*>(v));
1362    RAW_CHECK_FMT((v & (kMuWait | kMuWrWait)) != kMuWrWait,
1363                  "%s: Mutex corrupt: waiting writer with no waiters: %p", label,
1364                  reinterpret_cast<void*>(v));
1365    assert(false);
1366  }
1367  void Mutex::LockSlowLoop(SynchWaitParams* waitp, int flags) {
1368    SchedulingGuard::ScopedDisable disable_rescheduling;
1369    int c = 0;
1370    intptr_t v = mu_.load(std::memory_order_relaxed);
1371    if ((v & kMuEvent) != 0) {
1372      PostSynchEvent(
1373          this, waitp->how == kExclusive ? SYNCH_EV_LOCK : SYNCH_EV_READERLOCK);
1374    }
1375    ABSL_RAW_CHECK(
1376        waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors,
1377        "detected illegal recursion into Mutex code");
1378    for (;;) {
1379      v = mu_.load(std::memory_order_relaxed);
1380      CheckForMutexCorruption(v, "Lock");
1381      if ((v & waitp->how->slow_need_zero) == 0) {
1382        if (mu_.compare_exchange_strong(
1383                v,
1384                (waitp->how->fast_or |
1385                 (v & ClearDesignatedWakerMask(flags & kMuHasBlocked))) +
1386                    waitp->how->fast_add,
1387                std::memory_order_acquire, std::memory_order_relaxed)) {
1388          if (waitp->cond == nullptr ||
1389              EvalConditionAnnotated(waitp->cond, this, true, false,
1390                                     waitp->how == kShared)) {
1391            break;  
1392          }
1393          this->UnlockSlow(waitp);  
1394          this->Block(waitp->thread);
1395          flags |= kMuHasBlocked;
1396          c = 0;
1397        }
1398      } else {  
1399        bool dowait = false;
1400        if ((v & (kMuSpin | kMuWait)) == 0) {  
1401          PerThreadSynch* new_h = Enqueue(nullptr, waitp, v, flags);
1402          intptr_t nv =
1403              (v & ClearDesignatedWakerMask(flags & kMuHasBlocked) & kMuLow) |
1404              kMuWait;
1405          ABSL_RAW_CHECK(new_h != nullptr, "Enqueue to empty list failed");
1406          if (waitp->how == kExclusive && (v & kMuReader) != 0) {
1407            nv |= kMuWrWait;
1408          }
1409          if (mu_.compare_exchange_strong(
1410                  v, reinterpret_cast<intptr_t>(new_h) | nv,
1411                  std::memory_order_release, std::memory_order_relaxed)) {
1412            dowait = true;
1413          } else {  
1414            waitp->thread->waitp = nullptr;
1415          }
1416        } else if ((v & waitp->how->slow_inc_need_zero &
1417                    IgnoreWaitingWritersMask(flags & kMuHasBlocked)) == 0) {
1418          if (mu_.compare_exchange_strong(
1419                  v,
1420                  (v & ClearDesignatedWakerMask(flags & kMuHasBlocked)) |
1421                      kMuSpin | kMuReader,
1422                  std::memory_order_acquire, std::memory_order_relaxed)) {
1423            PerThreadSynch* h = GetPerThreadSynch(v);
1424            h->readers += kMuOne;  
1425            do {                   
1426              v = mu_.load(std::memory_order_relaxed);
1427            } while (!mu_.compare_exchange_weak(v, (v & ~kMuSpin) | kMuReader,
1428                                                std::memory_order_release,
1429                                                std::memory_order_relaxed));
1430            if (waitp->cond == nullptr ||
1431                EvalConditionAnnotated(waitp->cond, this, true, false,
1432                                       waitp->how == kShared)) {
1433              break;  
1434            }
1435            this->UnlockSlow(waitp);  
1436            this->Block(waitp->thread);
1437            flags |= kMuHasBlocked;
1438            c = 0;
1439          }
1440        } else if ((v & kMuSpin) == 0 &&  
1441                   mu_.compare_exchange_strong(
1442                       v,
1443                       (v & ClearDesignatedWakerMask(flags & kMuHasBlocked)) |
1444                           kMuSpin | kMuWait,
1445                       std::memory_order_acquire, std::memory_order_relaxed)) {
1446          PerThreadSynch* h = GetPerThreadSynch(v);
1447          PerThreadSynch* new_h = Enqueue(h, waitp, v, flags);
1448          intptr_t wr_wait = 0;
1449          ABSL_RAW_CHECK(new_h != nullptr, "Enqueue to list failed");
1450          if (waitp->how == kExclusive && (v & kMuReader) != 0) {
1451            wr_wait = kMuWrWait;  
1452          }
1453          do {  
1454            v = mu_.load(std::memory_order_relaxed);
1455          } while (!mu_.compare_exchange_weak(
1456              v,
1457              (v & (kMuLow & ~kMuSpin)) | kMuWait | wr_wait |
1458                  reinterpret_cast<intptr_t>(new_h),
1459              std::memory_order_release, std::memory_order_relaxed));
1460          dowait = true;
1461        }
1462        if (dowait) {
1463          this->Block(waitp->thread);  
1464          flags |= kMuHasBlocked;
1465          c = 0;
1466        }
1467      }
1468      ABSL_RAW_CHECK(
1469          waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors,
1470          "detected illegal recursion into Mutex code");
1471      c = synchronization_internal::MutexDelay(c, GENTLE);
1472    }
1473    ABSL_RAW_CHECK(
1474        waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors,
1475        "detected illegal recursion into Mutex code");
1476    if ((v & kMuEvent) != 0) {
1477      PostSynchEvent(this, waitp->how == kExclusive
1478                               ? SYNCH_EV_LOCK_RETURNING
1479                               : SYNCH_EV_READERLOCK_RETURNING);
1480    }
1481  }
1482  ABSL_ATTRIBUTE_NOINLINE void Mutex::UnlockSlow(SynchWaitParams* waitp) {
1483    SchedulingGuard::ScopedDisable disable_rescheduling;
1484    intptr_t v = mu_.load(std::memory_order_relaxed);
1485    this->AssertReaderHeld();
1486    CheckForMutexCorruption(v, "Unlock");
1487    if ((v & kMuEvent) != 0) {
1488      PostSynchEvent(
1489          this, (v & kMuWriter) != 0 ? SYNCH_EV_UNLOCK : SYNCH_EV_READERUNLOCK);
1490    }
1491    int c = 0;
1492    PerThreadSynch* w = nullptr;
1493    PerThreadSynch* pw = nullptr;
1494    PerThreadSynch* old_h = nullptr;
1495    const Condition* known_false = nullptr;
1496    PerThreadSynch* wake_list = kPerThreadSynchNull;  
1497    intptr_t wr_wait = 0;  
1498    ABSL_RAW_CHECK(waitp == nullptr || waitp->thread->waitp == nullptr ||
1499                       waitp->thread->suppress_fatal_errors,
1500                   "detected illegal recursion into Mutex code");
1501    for (;;) {
1502      v = mu_.load(std::memory_order_relaxed);
1503      if ((v & kMuWriter) != 0 && (v & (kMuWait | kMuDesig)) != kMuWait &&
1504          waitp == nullptr) {
1505        if (mu_.compare_exchange_strong(v, v & ~(kMuWrWait | kMuWriter),
1506                                        std::memory_order_release,
1507                                        std::memory_order_relaxed)) {
1508          return;
1509        }
1510      } else if ((v & (kMuReader | kMuWait)) == kMuReader && waitp == nullptr) {
1511        intptr_t clear = ExactlyOneReader(v) ? kMuReader | kMuOne : kMuOne;
1512        if (mu_.compare_exchange_strong(v, v - clear, std::memory_order_release,
1513                                        std::memory_order_relaxed)) {
1514          return;
1515        }
1516      } else if ((v & kMuSpin) == 0 &&  
1517                 mu_.compare_exchange_strong(v, v | kMuSpin,
1518                                             std::memory_order_acquire,
1519                                             std::memory_order_relaxed)) {
1520        if ((v & kMuWait) == 0) {  
1521          intptr_t nv;
1522          bool do_enqueue = true;  
1523          ABSL_RAW_CHECK(waitp != nullptr,
1524                         "UnlockSlow is confused");  
1525          do {  
1526            v = mu_.load(std::memory_order_relaxed);
1527            intptr_t new_readers = (v >= kMuOne) ? v - kMuOne : v;
1528            PerThreadSynch* new_h = nullptr;
1529            if (do_enqueue) {
1530              do_enqueue = (waitp->cv_word == nullptr);
1531              new_h = Enqueue(nullptr, waitp, new_readers, kMuIsCond);
1532            }
1533            intptr_t clear = kMuWrWait | kMuWriter;  
1534            if ((v & kMuWriter) == 0 && ExactlyOneReader(v)) {  
1535              clear = kMuWrWait | kMuReader;                    
1536            }
1537            nv = (v & kMuLow & ~clear & ~kMuSpin);
1538            if (new_h != nullptr) {
1539              nv |= kMuWait | reinterpret_cast<intptr_t>(new_h);
1540            } else {  
1541              nv |= new_readers & kMuHigh;
1542            }
1543          } while (!mu_.compare_exchange_weak(v, nv, std::memory_order_release,
1544                                              std::memory_order_relaxed));
1545          break;
1546        }
1547        PerThreadSynch* h = GetPerThreadSynch(v);
1548        if ((v & kMuReader) != 0 && (h->readers & kMuHigh) > kMuOne) {
1549          h->readers -= kMuOne;    
1550          intptr_t nv = v;         
1551          if (waitp != nullptr) {  
1552            PerThreadSynch* new_h = Enqueue(h, waitp, v, kMuIsCond);
1553            ABSL_RAW_CHECK(new_h != nullptr,
1554                           "waiters disappeared during Enqueue()!");
1555            nv &= kMuLow;
1556            nv |= kMuWait | reinterpret_cast<intptr_t>(new_h);
1557          }
1558          mu_.store(nv, std::memory_order_release);  
1559          break;
1560        }
1561        ABSL_RAW_CHECK(old_h == nullptr || h->maybe_unlocking,
1562                       "Mutex queue changed beneath us");
1563        if (old_h != nullptr &&
1564            !old_h->may_skip) {    
1565          old_h->may_skip = true;  
1566          ABSL_RAW_CHECK(old_h->skip == nullptr, "illegal skip from head");
1567          if (h != old_h && MuEquivalentWaiter(old_h, old_h->next)) {
1568            old_h->skip = old_h->next;  
1569          }
1570        }
1571        if (h->next->waitp->how == kExclusive &&
1572            Condition::GuaranteedEqual(h->next->waitp->cond, nullptr)) {
1573          pw = h;  
1574          w = h->next;
1575          w->wake = true;
1576          wr_wait = kMuWrWait;
1577        } else if (w != nullptr && (w->waitp->how == kExclusive || h == old_h)) {
1578          if (pw == nullptr) {  
1579            pw = h;
1580          }
1581        } else {
1582          if (old_h == h) {  
1583            intptr_t nv = (v & ~(kMuReader | kMuWriter | kMuWrWait));
1584            h->readers = 0;
1585            h->maybe_unlocking = false;  
<span onclick='openModal()' class='match'>1586            if (waitp != nullptr) {      
1587              PerThreadSynch* new_h = Enqueue(h, waitp, v, kMuIsCond);
1588              nv &= kMuLow;
</span>1589              if (new_h != nullptr) {
1590                nv |= kMuWait | reinterpret_cast<intptr_t>(new_h);
1591              }  
1592            }
1593            mu_.store(nv, std::memory_order_release);
1594            break;
1595          }
1596          PerThreadSynch* w_walk;   
1597          PerThreadSynch* pw_walk;  
1598          if (old_h != nullptr) {  
1599            pw_walk = old_h;
1600            w_walk = old_h->next;
1601          } else {  
1602            pw_walk =
1603                nullptr;  
1604            w_walk = h->next;
1605          }
1606          h->may_skip = false;  
1607          ABSL_RAW_CHECK(h->skip == nullptr, "illegal skip from head");
1608          h->maybe_unlocking = true;  
1609          mu_.store(v, std::memory_order_release);  
1610          old_h = h;  
1611          while (pw_walk != h) {
1612            w_walk->wake = false;
1613            if (w_walk->waitp->cond ==
1614                    nullptr ||  
1615                (w_walk->waitp->cond != known_false &&
1616                 EvalConditionIgnored(this, w_walk->waitp->cond))) {
1617              if (w == nullptr) {
1618                w_walk->wake = true;  
1619                w = w_walk;
1620                pw = pw_walk;
1621                if (w_walk->waitp->how == kExclusive) {
1622                  wr_wait = kMuWrWait;
1623                  break;  
1624                }
1625              } else if (w_walk->waitp->how == kShared) {  
1626                w_walk->wake = true;
1627              } else {  
1628                wr_wait = kMuWrWait;
1629              }
1630            } else {                              
1631              known_false = w_walk->waitp->cond;  
1632            }
1633            if (w_walk->wake) {  
1634              pw_walk = w_walk;  
1635            } else {             
1636              pw_walk = Skip(w_walk);
1637            }
1638            if (pw_walk != h) {
1639              w_walk = pw_walk->next;
1640            }
1641          }
1642          continue;  
1643        }
1644        ABSL_RAW_CHECK(pw->next == w, "pw not w's predecessor");
1645        h = DequeueAllWakeable(h, pw, &wake_list);
1646        intptr_t nv = (v & kMuEvent) | kMuDesig;
1647        if (waitp != nullptr) {  
1648          h = Enqueue(h, waitp, v, kMuIsCond);
1649        }
1650        ABSL_RAW_CHECK(wake_list != kPerThreadSynchNull,
1651                       "unexpected empty wake list");
1652        if (h != nullptr) {  
1653          h->readers = 0;
1654          h->maybe_unlocking = false;  
1655          nv |= wr_wait | kMuWait | reinterpret_cast<intptr_t>(h);
1656        }
1657        mu_.store(nv, std::memory_order_release);
1658        break;  
1659      }
1660      c = synchronization_internal::MutexDelay(c, AGGRESSIVE);
1661    }  
1662    if (wake_list != kPerThreadSynchNull) {
1663      int64_t total_wait_cycles = 0;
1664      int64_t max_wait_cycles = 0;
1665      int64_t now = CycleClock::Now();
1666      do {
1667        if (!wake_list->cond_waiter) {
1668          int64_t cycles_waited =
1669              (now - wake_list->waitp->contention_start_cycles);
1670          total_wait_cycles += cycles_waited;
1671          if (max_wait_cycles == 0) max_wait_cycles = cycles_waited;
1672          wake_list->waitp->contention_start_cycles = now;
1673          wake_list->waitp->should_submit_contention_data = true;
1674        }
1675        wake_list = Wakeup(wake_list);  
1676      } while (wake_list != kPerThreadSynchNull);
1677      if (total_wait_cycles > 0) {
1678        mutex_tracer("slow release", this, total_wait_cycles);
1679        ABSL_TSAN_MUTEX_PRE_DIVERT(this, 0);
1680        submit_profile_data(total_wait_cycles);
1681        ABSL_TSAN_MUTEX_POST_DIVERT(this, 0);
1682      }
1683    }
1684  }
1685  void Mutex::Trans(MuHow how) {
1686    this->LockSlow(how, nullptr, kMuHasBlocked | kMuIsCond);
1687  }
1688  void Mutex::Fer(PerThreadSynch* w) {
1689    SchedulingGuard::ScopedDisable disable_rescheduling;
1690    int c = 0;
1691    ABSL_RAW_CHECK(w->waitp->cond == nullptr,
1692                   "Mutex::Fer while waiting on Condition");
1693    ABSL_RAW_CHECK(!w->waitp->timeout.has_timeout(),
1694                   "Mutex::Fer while in timed wait");
1695    ABSL_RAW_CHECK(w->waitp->cv_word == nullptr,
1696                   "Mutex::Fer with pending CondVar queueing");
1697    for (;;) {
1698      intptr_t v = mu_.load(std::memory_order_relaxed);
1699      const intptr_t conflicting =
1700          kMuWriter | (w->waitp->how == kShared ? 0 : kMuReader);
1701      if ((v & conflicting) == 0) {
1702        w->next = nullptr;
1703        w->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
1704        IncrementSynchSem(this, w);
1705        return;
1706      } else {
1707        if ((v & (kMuSpin | kMuWait)) == 0) {  
1708          PerThreadSynch* new_h = Enqueue(nullptr, w->waitp, v, kMuIsCond);
1709          ABSL_RAW_CHECK(new_h != nullptr,
1710                         "Enqueue failed");  
1711          if (mu_.compare_exchange_strong(
1712                  v, reinterpret_cast<intptr_t>(new_h) | (v & kMuLow) | kMuWait,
1713                  std::memory_order_release, std::memory_order_relaxed)) {
1714            return;
1715          }
1716        } else if ((v & kMuSpin) == 0 &&
1717                   mu_.compare_exchange_strong(v, v | kMuSpin | kMuWait)) {
1718          PerThreadSynch* h = GetPerThreadSynch(v);
1719          PerThreadSynch* new_h = Enqueue(h, w->waitp, v, kMuIsCond);
1720          ABSL_RAW_CHECK(new_h != nullptr,
1721                         "Enqueue failed");  
1722          do {
1723            v = mu_.load(std::memory_order_relaxed);
1724          } while (!mu_.compare_exchange_weak(
1725              v,
1726              (v & kMuLow & ~kMuSpin) | kMuWait |
1727                  reinterpret_cast<intptr_t>(new_h),
1728              std::memory_order_release, std::memory_order_relaxed));
1729          return;
1730        }
1731      }
1732      c = synchronization_internal::MutexDelay(c, GENTLE);
1733    }
1734  }
1735  void Mutex::AssertHeld() const {
1736    if ((mu_.load(std::memory_order_relaxed) & kMuWriter) == 0) {
1737      SynchEvent* e = GetSynchEvent(this);
1738      ABSL_RAW_LOG(FATAL, "thread should hold write lock on Mutex %p %s",
1739                   static_cast<const void*>(this), (e == nullptr ? "" : e->name));
1740    }
1741  }
1742  void Mutex::AssertReaderHeld() const {
1743    if ((mu_.load(std::memory_order_relaxed) & (kMuReader | kMuWriter)) == 0) {
1744      SynchEvent* e = GetSynchEvent(this);
1745      ABSL_RAW_LOG(FATAL,
1746                   "thread should hold at least a read lock on Mutex %p %s",
1747                   static_cast<const void*>(this), (e == nullptr ? "" : e->name));
1748    }
1749  }
1750  static const intptr_t kCvSpin = 0x0001L;   
1751  static const intptr_t kCvEvent = 0x0002L;  
1752  static const intptr_t kCvLow = 0x0003L;  
1753  enum {
1754    kGdbCvSpin = kCvSpin,
1755    kGdbCvEvent = kCvEvent,
1756    kGdbCvLow = kCvLow,
1757  };
1758  static_assert(PerThreadSynch::kAlignment > kCvLow,
1759                "PerThreadSynch::kAlignment must be greater than kCvLow");
1760  void CondVar::EnableDebugLog(const char* name) {
1761    SynchEvent* e = EnsureSynchEvent(&this->cv_, name, kCvEvent, kCvSpin);
1762    e->log = true;
1763    UnrefSynchEvent(e);
1764  }
1765  CondVar::~CondVar() {
1766    if ((cv_.load(std::memory_order_relaxed) & kCvEvent) != 0) {
1767      ForgetSynchEvent(&this->cv_, kCvEvent, kCvSpin);
1768    }
1769  }
1770  void CondVar::Remove(PerThreadSynch* s) {
1771    SchedulingGuard::ScopedDisable disable_rescheduling;
1772    intptr_t v;
1773    int c = 0;
1774    for (v = cv_.load(std::memory_order_relaxed);;
1775         v = cv_.load(std::memory_order_relaxed)) {
1776      if ((v & kCvSpin) == 0 &&  
1777          cv_.compare_exchange_strong(v, v | kCvSpin, std::memory_order_acquire,
1778                                      std::memory_order_relaxed)) {
1779        PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1780        if (h != nullptr) {
1781          PerThreadSynch* w = h;
1782          while (w->next != s && w->next != h) {  
1783            w = w->next;
1784          }
1785          if (w->next == s) {  
1786            w->next = s->next;
1787            if (h == s) {
1788              h = (w == s) ? nullptr : w;
1789            }
1790            s->next = nullptr;
1791            s->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
1792          }
1793        }
1794        cv_.store((v & kCvEvent) | reinterpret_cast<intptr_t>(h),
1795                  std::memory_order_release);
1796        return;
1797      } else {
1798        c = synchronization_internal::MutexDelay(c, GENTLE);
1799      }
1800    }
1801  }
1802  static void CondVarEnqueue(SynchWaitParams* waitp) {
1803    std::atomic<intptr_t>* cv_word = waitp->cv_word;
1804    waitp->cv_word = nullptr;
1805    intptr_t v = cv_word->load(std::memory_order_relaxed);
1806    int c = 0;
1807    while ((v & kCvSpin) != 0 ||  
1808           !cv_word->compare_exchange_weak(v, v | kCvSpin,
1809                                           std::memory_order_acquire,
1810                                           std::memory_order_relaxed)) {
1811      c = synchronization_internal::MutexDelay(c, GENTLE);
1812      v = cv_word->load(std::memory_order_relaxed);
1813    }
1814    ABSL_RAW_CHECK(waitp->thread->waitp == nullptr, "waiting when shouldn't be");
1815    waitp->thread->waitp = waitp;  
1816    PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1817    if (h == nullptr) {  
1818      waitp->thread->next = waitp->thread;
1819    } else {
1820      waitp->thread->next = h->next;
1821      h->next = waitp->thread;
1822    }
1823    waitp->thread->state.store(PerThreadSynch::kQueued,
1824                               std::memory_order_relaxed);
1825    cv_word->store((v & kCvEvent) | reinterpret_cast<intptr_t>(waitp->thread),
1826                   std::memory_order_release);
1827  }
1828  bool CondVar::WaitCommon(Mutex* mutex, KernelTimeout t) {
1829    bool rc = false;  
1830    intptr_t mutex_v = mutex->mu_.load(std::memory_order_relaxed);
1831    Mutex::MuHow mutex_how = ((mutex_v & kMuWriter) != 0) ? kExclusive : kShared;
1832    ABSL_TSAN_MUTEX_PRE_UNLOCK(mutex, TsanFlags(mutex_how));
1833    intptr_t v = cv_.load(std::memory_order_relaxed);
1834    cond_var_tracer("Wait", this);
1835    if ((v & kCvEvent) != 0) {
1836      PostSynchEvent(this, SYNCH_EV_WAIT);
1837    }
1838    SynchWaitParams waitp(mutex_how, nullptr, t, mutex,
1839                          Synch_GetPerThreadAnnotated(mutex), &cv_);
1840    mutex->UnlockSlow(&waitp);
1841    while (waitp.thread->state.load(std::memory_order_acquire) ==
1842           PerThreadSynch::kQueued) {
1843      if (!Mutex::DecrementSynchSem(mutex, waitp.thread, t)) {
1844        t = KernelTimeout::Never();
1845        this->Remove(waitp.thread);
1846        rc = true;
1847      }
1848    }
1849    ABSL_RAW_CHECK(waitp.thread->waitp != nullptr, "not waiting when should be");
1850    waitp.thread->waitp = nullptr;  
1851    cond_var_tracer("Unwait", this);
1852    if ((v & kCvEvent) != 0) {
1853      PostSynchEvent(this, SYNCH_EV_WAIT_RETURNING);
1854    }
1855    ABSL_TSAN_MUTEX_POST_UNLOCK(mutex, TsanFlags(mutex_how));
1856    ABSL_TSAN_MUTEX_PRE_LOCK(mutex, TsanFlags(mutex_how));
1857    mutex->Trans(mutex_how);  
1858    ABSL_TSAN_MUTEX_POST_LOCK(mutex, TsanFlags(mutex_how), 0);
1859    return rc;
1860  }
1861  bool CondVar::WaitWithTimeout(Mutex* mu, absl::Duration timeout) {
1862    return WaitCommon(mu, KernelTimeout(timeout));
1863  }
1864  bool CondVar::WaitWithDeadline(Mutex* mu, absl::Time deadline) {
1865    return WaitCommon(mu, KernelTimeout(deadline));
1866  }
1867  void CondVar::Wait(Mutex* mu) { WaitCommon(mu, KernelTimeout::Never()); }
1868  void CondVar::Wakeup(PerThreadSynch* w) {
1869    if (w->waitp->timeout.has_timeout() || w->waitp->cvmu == nullptr) {
1870      Mutex* mu = w->waitp->cvmu;
1871      w->next = nullptr;
1872      w->state.store(PerThreadSynch::kAvailable, std::memory_order_release);
1873      Mutex::IncrementSynchSem(mu, w);
1874    } else {
1875      w->waitp->cvmu->Fer(w);
1876    }
1877  }
1878  void CondVar::Signal() {
1879    SchedulingGuard::ScopedDisable disable_rescheduling;
1880    ABSL_TSAN_MUTEX_PRE_SIGNAL(nullptr, 0);
1881    intptr_t v;
1882    int c = 0;
1883    for (v = cv_.load(std::memory_order_relaxed); v != 0;
1884         v = cv_.load(std::memory_order_relaxed)) {
1885      if ((v & kCvSpin) == 0 &&  
1886          cv_.compare_exchange_strong(v, v | kCvSpin, std::memory_order_acquire,
1887                                      std::memory_order_relaxed)) {
1888        PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1889        PerThreadSynch* w = nullptr;
1890        if (h != nullptr) {  
1891          w = h->next;
1892          if (w == h) {
1893            h = nullptr;
1894          } else {
1895            h->next = w->next;
1896          }
1897        }
1898        cv_.store((v & kCvEvent) | reinterpret_cast<intptr_t>(h),
1899                  std::memory_order_release);
1900        if (w != nullptr) {
1901          CondVar::Wakeup(w);  
1902          cond_var_tracer("Signal wakeup", this);
1903        }
1904        if ((v & kCvEvent) != 0) {
1905          PostSynchEvent(this, SYNCH_EV_SIGNAL);
1906        }
1907        ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1908        return;
1909      } else {
1910        c = synchronization_internal::MutexDelay(c, GENTLE);
1911      }
1912    }
1913    ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1914  }
1915  void CondVar::SignalAll() {
1916    ABSL_TSAN_MUTEX_PRE_SIGNAL(nullptr, 0);
1917    intptr_t v;
1918    int c = 0;
1919    for (v = cv_.load(std::memory_order_relaxed); v != 0;
1920         v = cv_.load(std::memory_order_relaxed)) {
1921      if ((v & kCvSpin) == 0 &&
1922          cv_.compare_exchange_strong(v, v & kCvEvent, std::memory_order_acquire,
1923                                      std::memory_order_relaxed)) {
1924        PerThreadSynch* h = reinterpret_cast<PerThreadSynch*>(v & ~kCvLow);
1925        if (h != nullptr) {
1926          PerThreadSynch* w;
1927          PerThreadSynch* n = h->next;
1928          do {  
1929            w = n;
1930            n = n->next;
1931            CondVar::Wakeup(w);
1932          } while (w != h);
1933          cond_var_tracer("SignalAll wakeup", this);
1934        }
1935        if ((v & kCvEvent) != 0) {
1936          PostSynchEvent(this, SYNCH_EV_SIGNALALL);
1937        }
1938        ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1939        return;
1940      } else {
1941        c = synchronization_internal::MutexDelay(c, GENTLE);
1942      }
1943    }
1944    ABSL_TSAN_MUTEX_POST_SIGNAL(nullptr, 0);
1945  }
1946  void ReleasableMutexLock::Release() {
1947    ABSL_RAW_CHECK(this->mu_ != nullptr,
1948                   "ReleasableMutexLock::Release may only be called once");
1949    this->mu_->Unlock();
1950    this->mu_ = nullptr;
1951  }
1952  #ifdef ABSL_HAVE_THREAD_SANITIZER
1953  extern "C" void __tsan_read1(void* addr);
1954  #else
1955  #define __tsan_read1(addr)  
1956  #endif
1957  static bool Dereference(void* arg) {
1958    __tsan_read1(arg);
1959    return *(static_cast<bool*>(arg));
1960  }
1961  ABSL_CONST_INIT const Condition Condition::kTrue;
1962  Condition::Condition(bool (*func)(void*), void* arg)
1963      : eval_(&CallVoidPtrFunction), arg_(arg) {
1964    static_assert(sizeof(&func) <= sizeof(callback_),
1965                  "An overlarge function pointer passed to Condition.");
1966    StoreCallback(func);
1967  }
1968  bool Condition::CallVoidPtrFunction(const Condition* c) {
1969    using FunctionPointer = bool (*)(void*);
1970    FunctionPointer function_pointer;
1971    std::memcpy(&function_pointer, c->callback_, sizeof(function_pointer));
1972    return (*function_pointer)(c->arg_);
1973  }
1974  Condition::Condition(const bool* cond)
1975      : eval_(CallVoidPtrFunction),
1976        arg_(const_cast<bool*>(cond)) {
1977    using FunctionPointer = bool (*)(void*);
1978    const FunctionPointer dereference = Dereference;
1979    StoreCallback(dereference);
1980  }
1981  bool Condition::Eval() const {
1982    return (this->eval_ == nullptr) || (*this->eval_)(this);
1983  }
1984  bool Condition::GuaranteedEqual(const Condition* a, const Condition* b) {
1985    if (a == nullptr || a->eval_ == nullptr) {
1986      return b == nullptr || b->eval_ == nullptr;
1987    } else if (b == nullptr || b->eval_ == nullptr) {
1988      return false;
1989    }
1990    return a->eval_ == b->eval_ && a->arg_ == b->arg_ &&
1991           !memcmp(a->callback_, b->callback_, sizeof(a->callback_));
1992  }
1993  ABSL_NAMESPACE_END
1994  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from abseil-cpp-MDEwOlJlcG9zaXRvcnkxMDQyMzE1NDE=-flat-mutex.cc</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from abseil-cpp-MDEwOlJlcG9zaXRvcnkxMDQyMzE1NDE=-flat-mutex.cc</div>
                </div>
                <div class="column column_space"><pre><code>1551          if (waitp != nullptr) {  
1552            PerThreadSynch* new_h = Enqueue(h, waitp, v, kMuIsCond);
1553            ABSL_RAW_CHECK(new_h != nullptr,
</pre></code></div>
                <div class="column column_space"><pre><code>1586            if (waitp != nullptr) {      
1587              PerThreadSynch* new_h = Enqueue(h, waitp, v, kMuIsCond);
1588              nv &= kMuLow;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    