
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 14.130434782608695%, Tokens: 11, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-circles.h</h3>
            <pre><code>1  #pragma once
2  #include "stdafx.h"
3  class TGraphAttributes {
4  public:
5    TGraphAttributes(PUNGraph G, const char* nodeFeaturePath, const char* groundtruthPath);
6    ~TGraphAttributes() {
7    }
8    PUNGraph G;
9    TInt NFeatures;
10    THash<TInt, TIntIntH> NodeFeatures;
11    THash<TIntPr, TIntIntH> EdgeFeatures;
12    TVec<TInt> NodeIDs;
13    TCRef CRef;
14    TVec<TIntSet> GroundTruth; 
15  };
16  typedef TPt<TGraphAttributes> PGraphAttributes;
17  class TCluster {
18  public:
19    TCluster(PGraphAttributes GraphAttributes, TInt K, TFlt Lambda) :
20      GraphAttributes(GraphAttributes), K(K), Lambda(Lambda) {
21      Theta = new TFlt[K * GraphAttributes->NFeatures];
22      Derivative = new TFlt[K * GraphAttributes->NFeatures];
23      for (int k = 0; k < K; k++) {
24        for (int f = 0; f < GraphAttributes->NFeatures; f++) {
25          Theta[k * GraphAttributes->NFeatures + f] = 0;
26          Derivative[k * GraphAttributes->NFeatures + f] = 0;
27        }
28        CHat.Add(TIntSet());
29      }
30    }
31    ~TCluster() {
32      delete[] Theta;
33      delete[] Derivative;
34    }
35    void Train(TInt OuterReps, TInt GradientReps, TInt MCMCReps);
36    TVec<TIntSet> GetCircles(void) {
37      return CHat;
38    }
39    TCRef CRef;
40  private:
41    TFlt* Theta; 
42    TFlt* Derivative; 
43    TVec<TIntSet> CHat; 
44    PGraphAttributes GraphAttributes; 
45    TInt K;
46    TFlt Lambda;
47    TFlt LogLikelihood();
48    TIntSet MCMC(TInt k, TInt MCMCReps);
49    void Gradient();
50  };
51  typedef TPt<TCluster> PCluster;
52  enum lossType
53  {
54    zeroOne = 0,
55    balancedError = 1,
56    fScore = 2
57  };
58  TFlt Loss(TIntSet& l, TIntSet lHat, int N, int Which)
59  {
60    if (l.Len() == 0) {
61      if (lHat.Len() == 0) {
62        return 0;
63      }
64      return 1.0;
65    }
66    if (lHat.Len() == 0) {
67      if (l.Len() == 0) {
68        return 0;
69      }
70      return 1.0;
71    }
72    TInt TruePositives = 0;
73    TInt FalsePositives = 0;
74    TInt FalseNegatives = 0;
75    TFlt LabelLoss = 0;
76    for (THashSetKeyI<TInt> it = l.BegI(); it != l.EndI(); it ++) {
77      int c = it.GetKey();
78      if (!lHat.IsKey(c)) {
79        FalseNegatives ++;
80        if (Which == zeroOne) {
81          LabelLoss += 1.0/N;
82        }
83        else if (Which == balancedError) {
84          LabelLoss += 0.5/l.Len();
85        }
86      }
87    }
88    for (THashSetKeyI<TInt> it = lHat.BegI(); it != lHat.EndI(); it ++) {
89      int c = it.GetKey();
90      if (!l.IsKey(c)) {
91        FalsePositives ++;
92        if (Which == zeroOne) {
93          LabelLoss += 1.0/N;
94        }
95        else if (Which == balancedError) {
96          LabelLoss += 0.5/(N - l.Len());
97        }
98      }
99      else {
100        TruePositives ++;
101      }
102    }
103    if ((lHat.Len() == 0 || TruePositives == 0) && Which == fScore) {
104      return 1.0;
105    }
106    TFlt precision = (1.0*TruePositives)/lHat.Len();
107    TFlt recall = (1.0*TruePositives)/l.Len();
108    if (Which == fScore) {
109      return 1 - 2 * (precision*recall) / (precision + recall);
110    }
111    return LabelLoss;
112  }
113  TGraphAttributes::TGraphAttributes(PUNGraph G, const char* NodeFeaturePath,
114                                     const char* GroundTruthPath) :
115    G(G) {
116    FILE* f = fopen(NodeFeaturePath, "r");
117    int NNodes;
118    int nF;
119    fscanf(f, "%d %d", &NNodes, &nF);
120    NFeatures = nF;
121    for (int i = 0; i < NNodes; i++) {
122      int nid;
123      fscanf(f, "%d", &nid);
124      if (!G->IsNode(nid)) {
125        printf("Warning: %d is not a node in G.\n", nid);
126      }
127      TInt kv = NodeFeatures.AddKey(nid);
128      for (int x = 0; x < nF; x++) {
129        int z = 0;
130        fscanf(f, "%d", &z);
131        if (z) {
132          NodeFeatures[kv].AddDat(x) = z;
133        }
134      }
135      if (G->IsNode(nid)) {
136        NodeIDs.Add(nid);
137      }
138    }
139    fclose(f);
140    f = fopen(GroundTruthPath, "r");
141    if (f == NULL) {
142      printf("Groundtruth file %s not found.\n", GroundTruthPath);
143    }
144    else {
145      char* CircleName = new char [1000];
146      while (fscanf(f, "%s", CircleName) == 1)
147      {
148        TIntSet Circle;
149        while (true) {
150          int nid;
151          fscanf(f, "%d", &nid);
152          Circle.AddKey(nid);
153          char c;
154          while (true) {          
155            c = fgetc(f);
156            if (c == '\n') break;
157            if (c >= '0' && c <= '9') {
158              fseek(f, -1, SEEK_CUR);
159              break;
160            }
161          }
162          if (c == '\n') break;
163        }
164        GroundTruth.Add(Circle);
165      }
166      delete [] CircleName;
167    }
168    fclose(f);
169    for (int i = 0; i < NodeIDs.Len(); i++) {
170      TInt ni = NodeIDs[i];
171      for (int j = i + 1; j < NodeIDs.Len(); j++) {
172        TInt nj = NodeIDs[j];
173        TInt kv = EdgeFeatures.AddKey(TIntPr(ni, nj));
174        for (THashKeyDatI<TInt, TInt> it = NodeFeatures.GetDat(ni).BegI();
175             !it.IsEnd(); it++) {
176          TInt k = it.GetKey();
177          TInt diff = 0;
178          if (NodeFeatures.GetDat(nj).IsKey(k)) {
179            diff = abs(it.GetDat() - NodeFeatures.GetDat(nj).GetDat(k));
180          } else {
181            diff = abs(it.GetDat());
182          }
183          if (diff) {
184            EdgeFeatures[kv].AddDat(k) = diff;
185          }
186        }
187        for (THashKeyDatI<TInt, TInt> it = NodeFeatures.GetDat(nj).BegI();
188             !it.IsEnd(); it++) {
189          TInt k = it.GetKey();
190          TInt diff = 0;
191          if (NodeFeatures.GetDat(ni).IsKey(k)) {
192            diff = abs(it.GetDat() - NodeFeatures.GetDat(ni).GetDat(k));
193          } else {
194            diff = abs(it.GetDat());
195          }
196          if (diff) {
197            EdgeFeatures[kv].AddDat(k) = diff;
198          }
199        }
200      }
201    }
202  }
203  void TCluster::Train(TInt OuterReps, TInt GradientReps, TInt MCMCReps) {
204    TFlt Increment = 1.0 / (1.0 * GraphAttributes->NodeIDs.Len() * GraphAttributes->NodeIDs.Len());
205    TRnd t;
206    for (int OuterRep = 0; OuterRep < OuterReps; OuterRep++) {
207      for (int k = 0; k < K; k++) {
208        if (OuterRep == 0 || CHat[k].Empty() || CHat[k].Len()
209            == GraphAttributes->NodeIDs.Len()) {
210          CHat[k].Clr();
211          for (int i = 0; i < GraphAttributes->NodeIDs.Len(); i++) {
212            if (t.GetUniDevInt(2) == 0) {
213              CHat[k].AddKey(GraphAttributes->NodeIDs[i]);
214            }
215          }
216          for (int i = 0; i < GraphAttributes->NFeatures; i++) {
217            Theta[k * GraphAttributes->NFeatures + i] = 0;
218          }
219          Theta[k * GraphAttributes->NFeatures + t.GetUniDevInt(GraphAttributes->NFeatures)] = 1.0;
220          Theta[k * GraphAttributes->NFeatures] = 1;
221        }
222      }
223      for (int k = 0; k < K; k++) {
224        CHat[k] = MCMC(k, MCMCReps);
225      }
226      TFlt llPrevious = LogLikelihood();
227      TFlt ll = 0;
228      for (int gradientRep = 0; gradientRep < GradientReps; gradientRep++) {
229        Gradient();
230        for (int i = 0; i < K * GraphAttributes->NFeatures; i++) {
231          Theta[i] += Increment * Derivative[i];
232        }
233        printf(".");
234        fflush( stdout);
235        ll = LogLikelihood();
236        if (ll < llPrevious) {
237          for (int i = 0; i < K * GraphAttributes->NFeatures; i++) {
238            Theta[i] -= Increment * Derivative[i];
239          }
240          ll = llPrevious;
241          break;
242        }
243        llPrevious = ll;
244      }
245      printf("\nIteration %d, ll = %f\n", OuterRep + 1, (double) ll);
246    }
247  }
248  TFlt Inner(TIntIntH& Feature, TFlt* Parameter) {
249    TFlt res = 0;
250    for (THashKeyDatI<TInt, TInt> it = Feature.BegI(); !it.IsEnd(); it++) {
251      res += it.GetDat() * Parameter[it.GetKey()];
252    }
253    return res;
254  }
255  TIntSet TCluster::MCMC(TInt k, TInt MCMCReps) {
256    TRnd t;
257    THash<TInt, TFlt> CostNotIncludeHash;
258    THash<TInt, TFlt> CostIncludeHash;
259    TVec<TInt> NewLabel;
260    int csize = 0;
261    for (int i = 0; i < GraphAttributes->NodeIDs.Len(); i++) {
262      if (CHat[k].IsKey(GraphAttributes->NodeIDs[i])) {
263        NewLabel.Add(0);
264      } else {
265        NewLabel.Add(1);
266      }
267      if (CHat[k].IsKey(GraphAttributes->NodeIDs[i])) {
268        csize++;
269      }
270    }
271    for (THashKeyDatI<TIntPr, TIntIntH> it = GraphAttributes->EdgeFeatures.BegI();
272         !it.IsEnd(); it++) {
273      TIntPr e = it.GetKey();
274      TInt kv = GraphAttributes->EdgeFeatures.GetKeyId(e);
275      TInt Src = e.Val1;
276      TInt Dst = e.Val2;
277      TBool Exists = GraphAttributes->G->IsEdge(Src, Dst);
278      TFlt InnerProduct = Inner(it.GetDat(), Theta + k * GraphAttributes->NFeatures);
<span onclick='openModal()' class='match'>279      TFlt Other = 0;
280      for (int l = 0; l < K; l++) {
281        if (l == k) {
282          continue;
283        }
284        TFlt d = (CHat[l].IsKey(Src) && CHat[l].IsKey(Dst)) ? 1 : -1;
</span>285        Other += d * Inner(it.GetDat(), Theta + l * GraphAttributes->NFeatures);
286      }
287      TFlt CostNotInclude;
288      TFlt CostInclude;
289      if (Exists) {
290        CostNotInclude = -Other + InnerProduct + log(1 + exp(Other - InnerProduct));
291        CostInclude = -Other - InnerProduct + log(1 + exp(Other + InnerProduct));
292      } else {
293        CostNotInclude = log(1 + exp(Other - InnerProduct));
294        CostInclude = log(1 + exp(Other + InnerProduct));
295      }
296      CostNotIncludeHash.AddDat(kv) = -CostNotInclude;
297      CostIncludeHash.AddDat(kv) = -CostInclude;
298    }
299    TFlt InitialTemperature = 1.0; 
300    for (int r = 2; r < MCMCReps + 2; r++) {
301      TFlt Temperature = InitialTemperature / log((double) r);
302      for (int n = 0; n < GraphAttributes->NodeIDs.Len(); n++) {
303        TFlt l0 = 0;
304        TFlt l1 = 0;
305        for (int np = 0; np < GraphAttributes->NodeIDs.Len(); np++) {
306          if (n == np) {
307            continue;
308          }
309          TIntPr ed(GraphAttributes->NodeIDs[n], GraphAttributes->NodeIDs[np]);
310          if (ed.Val1 > ed.Val2) {
311            ed = TIntPr(ed.Val2, ed.Val1);
312          }
313          TInt kv = GraphAttributes->EdgeFeatures.GetKeyId(ed);
314          TFlt m0 = CostNotIncludeHash.GetDat(kv);
315          if (NewLabel[np] == 0) {
316            l0 += m0;
317            l1 += m0;
318          } else {
319            l0 += m0;
320            l1 += CostIncludeHash.GetDat(kv);
321          }
322        }
323        TFlt LogLikelihoodDiff = exp(l1 - l0);
324        TFlt AcceptProb = pow(LogLikelihoodDiff, 1.0 / Temperature);
325        if (t.GetUniDev() < AcceptProb) {
326          NewLabel[n] = 1;
327        } else {
328          NewLabel[n] = 0;
329        }
330      }
331    }
332    TIntSet Result;
333    for (int i = 0; i < GraphAttributes->NodeIDs.Len(); i++) {
334      if (NewLabel[i]) {
335        Result.AddKey(GraphAttributes->NodeIDs[i]);
336      }
337    }
338    return Result;
339  }
340  void TCluster::Gradient(void) {
341    for (int i = 0; i < K * GraphAttributes->NFeatures; i++) {
342      if (Theta[i] > 0) {
343        Derivative[i] = -Lambda * Theta[i];
344      } else {
345        Derivative[i] = Lambda * Theta[i];
346      }
347    }
348    for (THashKeyDatI<TIntPr, TIntIntH> it = GraphAttributes->EdgeFeatures.BegI();
349         !it.IsEnd(); it++) {
350      TFlt InnerProduct = 0;
351      TIntPr Edge = it.GetKey();
352      TInt Src = Edge.Val1;
353      TInt Dst = Edge.Val2;
354      TBool Exists = GraphAttributes->G->IsEdge(Src, Dst);
355      for (int k = 0; k < K; k++) {
356        TFlt d = CHat[k].IsKey(Src) && CHat[k].IsKey(Dst) ? 1 : -1;
357        InnerProduct += d * Inner(it.GetDat(), Theta + k * GraphAttributes->NFeatures);
358      }
359      TFlt expinp = exp(InnerProduct);
360      TFlt q = expinp / (1 + expinp);
361      if (q != q) {
362        q = 1; 
363      }
364      for (int k = 0; k < K; k++) {
365        TBool d_ = CHat[k].IsKey(Src) && CHat[k].IsKey(Dst);
366        TFlt d = d_ ? 1 : -1;
367        for (THashKeyDatI<TInt, TInt> itf = it.GetDat().BegI();
368             !itf.IsEnd(); itf++) {
369          TInt i = itf.GetKey();
370          TInt f = itf.GetDat();
371          if (Exists) {
372            Derivative[k * GraphAttributes->NFeatures + i] += d * f;
373          }
374          Derivative[k * GraphAttributes->NFeatures + i] += -d * f * q;
375        }
376      }
377    }
378  }
379  TFlt TCluster::LogLikelihood(void) {
380    TFlt ll = 0;
381    for (THashKeyDatI<TIntPr, TIntIntH> it = GraphAttributes->EdgeFeatures.BegI();
382         !it.IsEnd(); it++) {
383      TFlt InnerProduct = 0;
384      TIntPr Edge = it.GetKey();
385      TInt Src = Edge.Val1;
386      TInt Dst = Edge.Val2;
387      TBool Exists = GraphAttributes->G->IsEdge(Src, Dst);
388      for (int k = 0; k < K; k++) {
389        TFlt d = CHat[k].IsKey(Src) && CHat[k].IsKey(Dst) ? 1 : -1;
390        InnerProduct += d * Inner(it.GetDat(), Theta + k * GraphAttributes->NFeatures);
391      }
392      if (Exists) {
393        ll += InnerProduct;
394      }
395      TFlt ll_ = log(1 + exp(InnerProduct));
396      ll += -ll_;
397    }
398    if (ll != ll) {
399      printf("ll isnan\n");
400      exit(1);
401    }
402    return ll;
403  }
</code></pre>
        </div>
        <div class="column">
            <h3>tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-lstm.cpp</h3>
            <pre><code>1  #ifdef HAVE_CONFIG_H
2  #  include "config_auto.h"
3  #endif
4  #include "lstm.h"
5  #ifdef _OPENMP
6  #  include <omp.h>
7  #endif
8  #include <cstdio>
9  #include <cstdlib>
10  #include <sstream> 
11  #if defined(_MSC_VER) && !defined(__clang__)
12  #  include <intrin.h> 
13  #endif
14  #include "fullyconnected.h"
15  #include "functions.h"
16  #include "networkscratch.h"
17  #include "tprintf.h"
18  #ifdef _OPENMP
19  #  define PARALLEL_IF_OPENMP(__num_threads)                                  \
20      PRAGMA(omp parallel if (__num_threads > 1) num_threads(__num_threads)) { \
21        PRAGMA(omp sections nowait) {                                          \
22          PRAGMA(omp section) {
23  #  define SECTION_IF_OPENMP \
24      }                       \
25      PRAGMA(omp section) {
26  #  define END_PARALLEL_IF_OPENMP \
27      }                            \
28      } &bsol;* end of sections */      \
29      } &bsol;* end of parallel section */
30  #  ifdef _MSC_VER 
31  #    define PRAGMA(x) __pragma(x)
32  #  else
33  #    define PRAGMA(x) _Pragma(#    x)
34  #  endif 
35  #else 
36  #  define PARALLEL_IF_OPENMP(__num_threads)
37  #  define SECTION_IF_OPENMP
38  #  define END_PARALLEL_IF_OPENMP
39  #endif 
40  namespace tesseract {
41  const TFloat kStateClip = 100.0;
42  const TFloat kErrClip = 1.0f;
43  static inline uint32_t ceil_log2(uint32_t n) {
44  #if defined(__GNUC__)
45    uint32_t l2 = 31 - __builtin_clz(n);
46  #elif defined(_MSC_VER)
47    unsigned long l2 = 0;
48    _BitScanReverse(&l2, n);
49  #else
50    if (n == 0)
51      return UINT_MAX;
52    if (n == 1)
53      return 0;
54    uint32_t val = n;
55    uint32_t l2 = 0;
56    while (val > 1) {
57      val >>= 1;
58      l2++;
59    }
60  #endif
61    return (n == (1u << l2)) ? l2 : l2 + 1;
62  }
63  LSTM::LSTM(const std::string &name, int ni, int ns, int no, bool two_dimensional, NetworkType type)
64      : Network(type, name, ni, no)
65      , na_(ni + ns)
66      , ns_(ns)
67      , nf_(0)
68      , is_2d_(two_dimensional)
69      , softmax_(nullptr)
70      , input_width_(0) {
71    if (two_dimensional) {
72      na_ += ns_;
73    }
74    if (type_ == NT_LSTM || type_ == NT_LSTM_SUMMARY) {
75      nf_ = 0;
76      ASSERT_HOST(no == ns);
77    } else if (type_ == NT_LSTM_SOFTMAX || type_ == NT_LSTM_SOFTMAX_ENCODED) {
78      nf_ = type_ == NT_LSTM_SOFTMAX ? no_ : ceil_log2(no_);
79      softmax_ = new FullyConnected("LSTM Softmax", ns_, no_, NT_SOFTMAX);
80    } else {
81      tprintf("%d is invalid type of LSTM!\n", type);
82      ASSERT_HOST(false);
83    }
84    na_ += nf_;
85  }
86  LSTM::~LSTM() {
87    delete softmax_;
88  }
89  StaticShape LSTM::OutputShape(const StaticShape &input_shape) const {
90    StaticShape result = input_shape;
91    result.set_depth(no_);
92    if (type_ == NT_LSTM_SUMMARY) {
93      result.set_width(1);
94    }
95    if (softmax_ != nullptr) {
96      return softmax_->OutputShape(result);
97    }
98    return result;
99  }
100  void LSTM::SetEnableTraining(TrainingState state) {
101    if (state == TS_RE_ENABLE) {
102      if (training_ == TS_TEMP_DISABLE) {
103        training_ = TS_ENABLED;
104      }
105    } else if (state == TS_TEMP_DISABLE) {
106      if (training_ == TS_ENABLED) {
107        training_ = state;
108      }
109    } else {
110      if (state == TS_ENABLED && training_ != TS_ENABLED) {
111        for (int w = 0; w < WT_COUNT; ++w) {
112          if (w == GFS && !Is2D()) {
113            continue;
114          }
115          gate_weights_[w].InitBackward();
116        }
117      }
118      training_ = state;
119    }
120    if (softmax_ != nullptr) {
121      softmax_->SetEnableTraining(state);
122    }
123  }
124  int LSTM::InitWeights(float range, TRand *randomizer) {
125    Network::SetRandomizer(randomizer);
<span onclick='openModal()' class='match'>126    num_weights_ = 0;
127    for (int w = 0; w < WT_COUNT; ++w) {
128      if (w == GFS && !Is2D()) {
129        continue;
130      }
131      num_weights_ +=
</span>132          gate_weights_[w].InitWeightsFloat(ns_, na_ + 1, TestFlag(NF_ADAM), range, randomizer);
133    }
134    if (softmax_ != nullptr) {
135      num_weights_ += softmax_->InitWeights(range, randomizer);
136    }
137    return num_weights_;
138  }
139  int LSTM::RemapOutputs(int old_no, const std::vector<int> &code_map) {
140    if (softmax_ != nullptr) {
141      num_weights_ -= softmax_->num_weights();
142      num_weights_ += softmax_->RemapOutputs(old_no, code_map);
143    }
144    return num_weights_;
145  }
146  void LSTM::ConvertToInt() {
147    for (int w = 0; w < WT_COUNT; ++w) {
148      if (w == GFS && !Is2D()) {
149        continue;
150      }
151      gate_weights_[w].ConvertToInt();
152    }
153    if (softmax_ != nullptr) {
154      softmax_->ConvertToInt();
155    }
156  }
157  void LSTM::DebugWeights() {
158    for (int w = 0; w < WT_COUNT; ++w) {
159      if (w == GFS && !Is2D()) {
160        continue;
161      }
162      std::ostringstream msg;
163      msg << name_ << " Gate weights " << w;
164      gate_weights_[w].Debug2D(msg.str().c_str());
165    }
166    if (softmax_ != nullptr) {
167      softmax_->DebugWeights();
168    }
169  }
170  bool LSTM::Serialize(TFile *fp) const {
171    if (!Network::Serialize(fp)) {
172      return false;
173    }
174    if (!fp->Serialize(&na_)) {
175      return false;
176    }
177    for (int w = 0; w < WT_COUNT; ++w) {
178      if (w == GFS && !Is2D()) {
179        continue;
180      }
181      if (!gate_weights_[w].Serialize(IsTraining(), fp)) {
182        return false;
183      }
184    }
185    if (softmax_ != nullptr && !softmax_->Serialize(fp)) {
186      return false;
187    }
188    return true;
189  }
190  bool LSTM::DeSerialize(TFile *fp) {
191    if (!fp->DeSerialize(&na_)) {
192      return false;
193    }
194    if (type_ == NT_LSTM_SOFTMAX) {
195      nf_ = no_;
196    } else if (type_ == NT_LSTM_SOFTMAX_ENCODED) {
197      nf_ = ceil_log2(no_);
198    } else {
199      nf_ = 0;
200    }
201    is_2d_ = false;
202    for (int w = 0; w < WT_COUNT; ++w) {
203      if (w == GFS && !Is2D()) {
204        continue;
205      }
206      if (!gate_weights_[w].DeSerialize(IsTraining(), fp)) {
207        return false;
208      }
209      if (w == CI) {
210        ns_ = gate_weights_[CI].NumOutputs();
211        is_2d_ = na_ - nf_ == ni_ + 2 * ns_;
212      }
213    }
214    delete softmax_;
215    if (type_ == NT_LSTM_SOFTMAX || type_ == NT_LSTM_SOFTMAX_ENCODED) {
216      softmax_ = static_cast<FullyConnected *>(Network::CreateFromFile(fp));
217      if (softmax_ == nullptr) {
218        return false;
219      }
220    } else {
221      softmax_ = nullptr;
222    }
223    return true;
224  }
225  void LSTM::Forward(bool debug, const NetworkIO &input, const TransposedArray *input_transpose,
226                     NetworkScratch *scratch, NetworkIO *output) {
227    input_map_ = input.stride_map();
228    input_width_ = input.Width();
229    if (softmax_ != nullptr) {
230      output->ResizeFloat(input, no_);
231    } else if (type_ == NT_LSTM_SUMMARY) {
232      output->ResizeXTo1(input, no_);
233    } else {
234      output->Resize(input, no_);
235    }
236    ResizeForward(input);
237    NetworkScratch::FloatVec temp_lines[WT_COUNT];
238    int ro = ns_;
239    if (source_.int_mode() && IntSimdMatrix::intSimdMatrix) {
240      ro = IntSimdMatrix::intSimdMatrix->RoundOutputs(ro);
241    }
242    for (auto &temp_line : temp_lines) {
243      temp_line.Init(ns_, ro, scratch);
244    }
245    NetworkScratch::FloatVec curr_state, curr_output;
246    curr_state.Init(ns_, scratch);
247    ZeroVector<TFloat>(ns_, curr_state);
248    curr_output.Init(ns_, scratch);
249    ZeroVector<TFloat>(ns_, curr_output);
250    int buf_width = Is2D() ? input_map_.Size(FD_WIDTH) : 1;
251    std::vector<NetworkScratch::FloatVec> states, outputs;
252    if (Is2D()) {
253      states.resize(buf_width);
254      outputs.resize(buf_width);
255      for (int i = 0; i < buf_width; ++i) {
256        states[i].Init(ns_, scratch);
257        ZeroVector<TFloat>(ns_, states[i]);
258        outputs[i].Init(ns_, scratch);
259        ZeroVector<TFloat>(ns_, outputs[i]);
260      }
261    }
262    NetworkScratch::FloatVec softmax_output;
263    NetworkScratch::IO int_output;
264    if (softmax_ != nullptr) {
265      softmax_output.Init(no_, scratch);
266      ZeroVector<TFloat>(no_, softmax_output);
267      int rounded_softmax_inputs = gate_weights_[CI].RoundInputs(ns_);
268      if (input.int_mode()) {
269        int_output.Resize2d(true, 1, rounded_softmax_inputs, scratch);
270      }
271      softmax_->SetupForward(input, nullptr);
272    }
273    NetworkScratch::FloatVec curr_input;
274    curr_input.Init(na_, scratch);
275    StrideMap::Index src_index(input_map_);
276    StrideMap::Index dest_index(output->stride_map());
277    do {
278      int t = src_index.t();
279      bool valid_2d = Is2D();
280      if (valid_2d) {
281        StrideMap::Index dim_index(src_index);
282        if (!dim_index.AddOffset(-1, FD_HEIGHT)) {
283          valid_2d = false;
284        }
285      }
286      int mod_t = Modulo(t, buf_width); 
287      source_.CopyTimeStepGeneral(t, 0, ni_, input, t, 0);
288      if (softmax_ != nullptr) {
289        source_.WriteTimeStepPart(t, ni_, nf_, softmax_output);
290      }
291      source_.WriteTimeStepPart(t, ni_ + nf_, ns_, curr_output);
292      if (Is2D()) {
293        source_.WriteTimeStepPart(t, ni_ + nf_ + ns_, ns_, outputs[mod_t]);
294      }
295      if (!source_.int_mode()) {
296        source_.ReadTimeStep(t, curr_input);
297      }
298      PARALLEL_IF_OPENMP(GFS)
299      if (source_.int_mode()) {
300        gate_weights_[CI].MatrixDotVector(source_.i(t), temp_lines[CI]);
301      } else {
302        gate_weights_[CI].MatrixDotVector(curr_input, temp_lines[CI]);
303      }
304      FuncInplace<GFunc>(ns_, temp_lines[CI]);
305      SECTION_IF_OPENMP
306      if (source_.int_mode()) {
307        gate_weights_[GI].MatrixDotVector(source_.i(t), temp_lines[GI]);
308      } else {
309        gate_weights_[GI].MatrixDotVector(curr_input, temp_lines[GI]);
310      }
311      FuncInplace<FFunc>(ns_, temp_lines[GI]);
312      SECTION_IF_OPENMP
313      if (source_.int_mode()) {
314        gate_weights_[GF1].MatrixDotVector(source_.i(t), temp_lines[GF1]);
315      } else {
316        gate_weights_[GF1].MatrixDotVector(curr_input, temp_lines[GF1]);
317      }
318      FuncInplace<FFunc>(ns_, temp_lines[GF1]);
319      if (Is2D()) {
320        if (source_.int_mode()) {
321          gate_weights_[GFS].MatrixDotVector(source_.i(t), temp_lines[GFS]);
322        } else {
323          gate_weights_[GFS].MatrixDotVector(curr_input, temp_lines[GFS]);
324        }
325        FuncInplace<FFunc>(ns_, temp_lines[GFS]);
326      }
327      SECTION_IF_OPENMP
328      if (source_.int_mode()) {
329        gate_weights_[GO].MatrixDotVector(source_.i(t), temp_lines[GO]);
330      } else {
331        gate_weights_[GO].MatrixDotVector(curr_input, temp_lines[GO]);
332      }
333      FuncInplace<FFunc>(ns_, temp_lines[GO]);
334      END_PARALLEL_IF_OPENMP
335      MultiplyVectorsInPlace(ns_, temp_lines[GF1], curr_state);
336      if (Is2D()) {
337        int8_t *which_fg_col = which_fg_[t];
338        memset(which_fg_col, 1, ns_ * sizeof(which_fg_col[0]));
339        if (valid_2d) {
340          const TFloat *stepped_state = states[mod_t];
341          for (int i = 0; i < ns_; ++i) {
342            if (temp_lines[GF1][i] < temp_lines[GFS][i]) {
343              curr_state[i] = temp_lines[GFS][i] * stepped_state[i];
344              which_fg_col[i] = 2;
345            }
346          }
347        }
348      }
349      MultiplyAccumulate(ns_, temp_lines[CI], temp_lines[GI], curr_state);
350      ClipVector<TFloat>(ns_, -kStateClip, kStateClip, curr_state);
351      if (IsTraining()) {
352        node_values_[CI].WriteTimeStep(t, temp_lines[CI]);
353        node_values_[GI].WriteTimeStep(t, temp_lines[GI]);
354        node_values_[GF1].WriteTimeStep(t, temp_lines[GF1]);
355        node_values_[GO].WriteTimeStep(t, temp_lines[GO]);
356        if (Is2D()) {
357          node_values_[GFS].WriteTimeStep(t, temp_lines[GFS]);
358        }
359      }
360      FuncMultiply<HFunc>(curr_state, temp_lines[GO], ns_, curr_output);
361      if (IsTraining()) {
362        state_.WriteTimeStep(t, curr_state);
363      }
364      if (softmax_ != nullptr) {
365        if (input.int_mode()) {
366          int_output->WriteTimeStepPart(0, 0, ns_, curr_output);
367          softmax_->ForwardTimeStep(int_output->i(0), t, softmax_output);
368        } else {
369          softmax_->ForwardTimeStep(curr_output, t, softmax_output);
370        }
371        output->WriteTimeStep(t, softmax_output);
372        if (type_ == NT_LSTM_SOFTMAX_ENCODED) {
373          CodeInBinary(no_, nf_, softmax_output);
374        }
375      } else if (type_ == NT_LSTM_SUMMARY) {
376        if (src_index.IsLast(FD_WIDTH)) {
377          output->WriteTimeStep(dest_index.t(), curr_output);
378          dest_index.Increment();
379        }
380      } else {
381        output->WriteTimeStep(t, curr_output);
382      }
383      if (Is2D()) {
384        CopyVector(ns_, curr_state, states[mod_t]);
385        CopyVector(ns_, curr_output, outputs[mod_t]);
386      }
387      if (src_index.IsLast(FD_WIDTH)) {
388        ZeroVector<TFloat>(ns_, curr_state);
389        ZeroVector<TFloat>(ns_, curr_output);
390      }
391    } while (src_index.Increment());
392  #if DEBUG_DETAIL > 0
393    tprintf("Source:%s\n", name_.c_str());
394    source_.Print(10);
395    tprintf("State:%s\n", name_.c_str());
396    state_.Print(10);
397    tprintf("Output:%s\n", name_.c_str());
398    output->Print(10);
399  #endif
400  #ifndef GRAPHICS_DISABLED
401    if (debug) {
402      DisplayForward(*output);
403    }
404  #endif
405  }
406  bool LSTM::Backward(bool debug, const NetworkIO &fwd_deltas, NetworkScratch *scratch,
407                      NetworkIO *back_deltas) {
408  #ifndef GRAPHICS_DISABLED
409    if (debug) {
410      DisplayBackward(fwd_deltas);
411    }
412  #endif
413    back_deltas->ResizeToMap(fwd_deltas.int_mode(), input_map_, ni_);
414    NetworkScratch::FloatVec outputerr;
415    outputerr.Init(ns_, scratch);
416    NetworkScratch::FloatVec curr_stateerr, curr_sourceerr;
417    curr_stateerr.Init(ns_, scratch);
418    curr_sourceerr.Init(na_, scratch);
419    ZeroVector<TFloat>(ns_, curr_stateerr);
420    ZeroVector<TFloat>(na_, curr_sourceerr);
421    NetworkScratch::FloatVec gate_errors[WT_COUNT];
422    for (auto &gate_error : gate_errors) {
423      gate_error.Init(ns_, scratch);
424    }
425    int buf_width = Is2D() ? input_map_.Size(FD_WIDTH) : 1;
426    std::vector<NetworkScratch::FloatVec> stateerr, sourceerr;
427    if (Is2D()) {
428      stateerr.resize(buf_width);
429      sourceerr.resize(buf_width);
430      for (int t = 0; t < buf_width; ++t) {
431        stateerr[t].Init(ns_, scratch);
432        sourceerr[t].Init(na_, scratch);
433        ZeroVector<TFloat>(ns_, stateerr[t]);
434        ZeroVector<TFloat>(na_, sourceerr[t]);
435      }
436    }
437    NetworkScratch::FloatVec sourceerr_temps[WT_COUNT];
438    for (auto &sourceerr_temp : sourceerr_temps) {
439      sourceerr_temp.Init(na_, scratch);
440    }
441    int width = input_width_;
442    NetworkScratch::GradientStore gate_errors_t[WT_COUNT];
443    for (auto &w : gate_errors_t) {
444      w.Init(ns_, width, scratch);
445    }
446    NetworkScratch::FloatVec softmax_errors;
447    NetworkScratch::GradientStore softmax_errors_t;
448    if (softmax_ != nullptr) {
449      softmax_errors.Init(no_, scratch);
450      softmax_errors_t.Init(no_, width, scratch);
451    }
452    TFloat state_clip = Is2D() ? 9.0 : 4.0;
453  #if DEBUG_DETAIL > 1
454    tprintf("fwd_deltas:%s\n", name_.c_str());
455    fwd_deltas.Print(10);
456  #endif
457    StrideMap::Index dest_index(input_map_);
458    dest_index.InitToLast();
459    StrideMap::Index src_index(fwd_deltas.stride_map());
460    src_index.InitToLast();
461    do {
462      int t = dest_index.t();
463      bool at_last_x = dest_index.IsLast(FD_WIDTH);
464      int up_pos = -1;
465      int down_pos = -1;
466      if (Is2D()) {
467        if (dest_index.index(FD_HEIGHT) > 0) {
468          StrideMap::Index up_index(dest_index);
469          if (up_index.AddOffset(-1, FD_HEIGHT)) {
470            up_pos = up_index.t();
471          }
472        }
473        if (!dest_index.IsLast(FD_HEIGHT)) {
474          StrideMap::Index down_index(dest_index);
475          if (down_index.AddOffset(1, FD_HEIGHT)) {
476            down_pos = down_index.t();
477          }
478        }
479      }
480      int mod_t = Modulo(t, buf_width); 
481      if (at_last_x) {
482        ZeroVector<TFloat>(na_, curr_sourceerr);
483        ZeroVector<TFloat>(ns_, curr_stateerr);
484      }
485      if (type_ == NT_LSTM_SUMMARY) {
486        if (dest_index.IsLast(FD_WIDTH)) {
487          fwd_deltas.ReadTimeStep(src_index.t(), outputerr);
488          src_index.Decrement();
489        } else {
490          ZeroVector<TFloat>(ns_, outputerr);
491        }
492      } else if (softmax_ == nullptr) {
493        fwd_deltas.ReadTimeStep(t, outputerr);
494      } else {
495        softmax_->BackwardTimeStep(fwd_deltas, t, softmax_errors, softmax_errors_t.get(), outputerr);
496      }
497      if (!at_last_x) {
498        AccumulateVector(ns_, curr_sourceerr + ni_ + nf_, outputerr);
499      }
500      if (down_pos >= 0) {
501        AccumulateVector(ns_, sourceerr[mod_t] + ni_ + nf_ + ns_, outputerr);
502      }
503      if (!at_last_x) {
504        const float *next_node_gf1 = node_values_[GF1].f(t + 1);
505        for (int i = 0; i < ns_; ++i) {
506          curr_stateerr[i] *= next_node_gf1[i];
507        }
508      }
509      if (Is2D() && t + 1 < width) {
510        for (int i = 0; i < ns_; ++i) {
511          if (which_fg_[t + 1][i] != 1) {
512            curr_stateerr[i] = 0.0;
513          }
514        }
515        if (down_pos >= 0) {
516          const float *right_node_gfs = node_values_[GFS].f(down_pos);
517          const TFloat *right_stateerr = stateerr[mod_t];
518          for (int i = 0; i < ns_; ++i) {
519            if (which_fg_[down_pos][i] == 2) {
520              curr_stateerr[i] += right_stateerr[i] * right_node_gfs[i];
521            }
522          }
523        }
524      }
525      state_.FuncMultiply3Add<HPrime>(node_values_[GO], t, outputerr, curr_stateerr);
526      ClipVector<TFloat>(ns_, -state_clip, state_clip, curr_stateerr);
527  #if DEBUG_DETAIL > 1
528      if (t + 10 > width) {
529        tprintf("t=%d, stateerr=", t);
530        for (int i = 0; i < ns_; ++i)
531          tprintf(" %g,%g,%g", curr_stateerr[i], outputerr[i], curr_sourceerr[ni_ + nf_ + i]);
532        tprintf("\n");
533      }
534  #endif
535      PARALLEL_IF_OPENMP(GFS)
536      node_values_[CI].FuncMultiply3<GPrime>(t, node_values_[GI], t, curr_stateerr, gate_errors[CI]);
537      ClipVector(ns_, -kErrClip, kErrClip, gate_errors[CI].get());
538      gate_weights_[CI].VectorDotMatrix(gate_errors[CI], sourceerr_temps[CI]);
539      gate_errors_t[CI].get()->WriteStrided(t, gate_errors[CI]);
540      SECTION_IF_OPENMP
541      node_values_[GI].FuncMultiply3<FPrime>(t, node_values_[CI], t, curr_stateerr, gate_errors[GI]);
542      ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GI].get());
543      gate_weights_[GI].VectorDotMatrix(gate_errors[GI], sourceerr_temps[GI]);
544      gate_errors_t[GI].get()->WriteStrided(t, gate_errors[GI]);
545      SECTION_IF_OPENMP
546      if (t > 0) {
547        node_values_[GF1].FuncMultiply3<FPrime>(t, state_, t - 1, curr_stateerr, gate_errors[GF1]);
548        ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GF1].get());
549        gate_weights_[GF1].VectorDotMatrix(gate_errors[GF1], sourceerr_temps[GF1]);
550      } else {
551        memset(gate_errors[GF1], 0, ns_ * sizeof(gate_errors[GF1][0]));
552        memset(sourceerr_temps[GF1], 0, na_ * sizeof(*sourceerr_temps[GF1]));
553      }
554      gate_errors_t[GF1].get()->WriteStrided(t, gate_errors[GF1]);
555      if (up_pos >= 0) {
556        node_values_[GFS].FuncMultiply3<FPrime>(t, state_, up_pos, curr_stateerr, gate_errors[GFS]);
557        ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GFS].get());
558        gate_weights_[GFS].VectorDotMatrix(gate_errors[GFS], sourceerr_temps[GFS]);
559      } else {
560        memset(gate_errors[GFS], 0, ns_ * sizeof(gate_errors[GFS][0]));
561        memset(sourceerr_temps[GFS], 0, na_ * sizeof(*sourceerr_temps[GFS]));
562      }
563      if (Is2D()) {
564        gate_errors_t[GFS].get()->WriteStrided(t, gate_errors[GFS]);
565      }
566      SECTION_IF_OPENMP
567      state_.Func2Multiply3<HFunc, FPrime>(node_values_[GO], t, outputerr, gate_errors[GO]);
568      ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GO].get());
569      gate_weights_[GO].VectorDotMatrix(gate_errors[GO], sourceerr_temps[GO]);
570      gate_errors_t[GO].get()->WriteStrided(t, gate_errors[GO]);
571      END_PARALLEL_IF_OPENMP
572      SumVectors(na_, sourceerr_temps[CI], sourceerr_temps[GI], sourceerr_temps[GF1],
573                 sourceerr_temps[GO], sourceerr_temps[GFS], curr_sourceerr);
574      back_deltas->WriteTimeStep(t, curr_sourceerr);
575      if (Is2D()) {
576        CopyVector(ns_, curr_stateerr, stateerr[mod_t]);
577        CopyVector(na_, curr_sourceerr, sourceerr[mod_t]);
578      }
579    } while (dest_index.Decrement());
580  #if DEBUG_DETAIL > 2
581    for (int w = 0; w < WT_COUNT; ++w) {
582      tprintf("%s gate errors[%d]\n", name_.c_str(), w);
583      gate_errors_t[w].get()->PrintUnTransposed(10);
584    }
585  #endif
586    NetworkScratch::GradientStore source_t, state_t;
587    source_t.Init(na_, width, scratch);
588    source_.Transpose(source_t.get());
589    state_t.Init(ns_, width, scratch);
590    state_.Transpose(state_t.get());
591  #ifdef _OPENMP
592  #  pragma omp parallel for num_threads(GFS) if (!Is2D())
593  #endif
594    for (int w = 0; w < WT_COUNT; ++w) {
595      if (w == GFS && !Is2D()) {
596        continue;
597      }
598      gate_weights_[w].SumOuterTransposed(*gate_errors_t[w], *source_t, false);
599    }
600    if (softmax_ != nullptr) {
601      softmax_->FinishBackward(*softmax_errors_t);
602    }
603    return needs_to_backprop_;
604  }
605  void LSTM::Update(float learning_rate, float momentum, float adam_beta, int num_samples) {
606  #if DEBUG_DETAIL > 3
607    PrintW();
608  #endif
609    for (int w = 0; w < WT_COUNT; ++w) {
610      if (w == GFS && !Is2D()) {
611        continue;
612      }
613      gate_weights_[w].Update(learning_rate, momentum, adam_beta, num_samples);
614    }
615    if (softmax_ != nullptr) {
616      softmax_->Update(learning_rate, momentum, adam_beta, num_samples);
617    }
618  #if DEBUG_DETAIL > 3
619    PrintDW();
620  #endif
621  }
622  void LSTM::CountAlternators(const Network &other, TFloat *same, TFloat *changed) const {
623    ASSERT_HOST(other.type() == type_);
624    const LSTM *lstm = static_cast<const LSTM *>(&other);
625    for (int w = 0; w < WT_COUNT; ++w) {
626      if (w == GFS && !Is2D()) {
627        continue;
628      }
629      gate_weights_[w].CountAlternators(lstm->gate_weights_[w], same, changed);
630    }
631    if (softmax_ != nullptr) {
632      softmax_->CountAlternators(*lstm->softmax_, same, changed);
633    }
634  }
635  #if DEBUG_DETAIL > 3
636  void LSTM::PrintW() {
637    tprintf("Weight state:%s\n", name_.c_str());
638    for (int w = 0; w < WT_COUNT; ++w) {
639      if (w == GFS && !Is2D()) {
640        continue;
641      }
642      tprintf("Gate %d, inputs\n", w);
643      for (int i = 0; i < ni_; ++i) {
644        tprintf("Row %d:", i);
645        for (int s = 0; s < ns_; ++s) {
646          tprintf(" %g", gate_weights_[w].GetWeights(s)[i]);
647        }
648        tprintf("\n");
649      }
650      tprintf("Gate %d, outputs\n", w);
651      for (int i = ni_; i < ni_ + ns_; ++i) {
652        tprintf("Row %d:", i - ni_);
653        for (int s = 0; s < ns_; ++s) {
654          tprintf(" %g", gate_weights_[w].GetWeights(s)[i]);
655        }
656        tprintf("\n");
657      }
658      tprintf("Gate %d, bias\n", w);
659      for (int s = 0; s < ns_; ++s) {
660        tprintf(" %g", gate_weights_[w].GetWeights(s)[na_]);
661      }
662      tprintf("\n");
663    }
664  }
665  void LSTM::PrintDW() {
666    tprintf("Delta state:%s\n", name_.c_str());
667    for (int w = 0; w < WT_COUNT; ++w) {
668      if (w == GFS && !Is2D()) {
669        continue;
670      }
671      tprintf("Gate %d, inputs\n", w);
672      for (int i = 0; i < ni_; ++i) {
673        tprintf("Row %d:", i);
674        for (int s = 0; s < ns_; ++s) {
675          tprintf(" %g", gate_weights_[w].GetDW(s, i));
676        }
677        tprintf("\n");
678      }
679      tprintf("Gate %d, outputs\n", w);
680      for (int i = ni_; i < ni_ + ns_; ++i) {
681        tprintf("Row %d:", i - ni_);
682        for (int s = 0; s < ns_; ++s) {
683          tprintf(" %g", gate_weights_[w].GetDW(s, i));
684        }
685        tprintf("\n");
686      }
687      tprintf("Gate %d, bias\n", w);
688      for (int s = 0; s < ns_; ++s) {
689        tprintf(" %g", gate_weights_[w].GetDW(s, na_));
690      }
691      tprintf("\n");
692    }
693  }
694  #endif
695  void LSTM::ResizeForward(const NetworkIO &input) {
696    int rounded_inputs = gate_weights_[CI].RoundInputs(na_);
697    source_.Resize(input, rounded_inputs);
698    which_fg_.ResizeNoInit(input.Width(), ns_);
699    if (IsTraining()) {
700      state_.ResizeFloat(input, ns_);
701      for (int w = 0; w < WT_COUNT; ++w) {
702        if (w == GFS && !Is2D()) {
703          continue;
704        }
705        node_values_[w].ResizeFloat(input, ns_);
706      }
707    }
708  }
709  } 
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-circles.h</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-lstm.cpp</div>
                </div>
                <div class="column column_space"><pre><code>279      TFlt Other = 0;
280      for (int l = 0; l < K; l++) {
281        if (l == k) {
282          continue;
283        }
284        TFlt d = (CHat[l].IsKey(Src) && CHat[l].IsKey(Dst)) ? 1 : -1;
</pre></code></div>
                <div class="column column_space"><pre><code>126    num_weights_ = 0;
127    for (int w = 0; w < WT_COUNT; ++w) {
128      if (w == GFS && !Is2D()) {
129        continue;
130      }
131      num_weights_ +=
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    