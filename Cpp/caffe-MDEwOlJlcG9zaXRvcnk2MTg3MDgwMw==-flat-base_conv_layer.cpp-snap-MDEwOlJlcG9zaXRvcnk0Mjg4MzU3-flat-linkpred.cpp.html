
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 8.680351906158359%, Tokens: 9</h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-base_conv_layer.cpp</h3>
            <pre><code>1  #include <algorithm>
2  #include <vector>
3  #include "caffe/filler.hpp"
4  #include "caffe/layers/base_conv_layer.hpp"
5  #include "caffe/util/im2col.hpp"
6  #include "caffe/util/math_functions.hpp"
7  #ifdef _OPENMP
8  #include <omp.h>
9  #endif
10  namespace caffe {
11  template <typename Dtype>
12  void BaseConvolutionLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
13        const vector<Blob<Dtype>*>& top) {
14    ConvolutionParameter conv_param = this->layer_param_.convolution_param();
15    force_nd_im2col_ = conv_param.force_nd_im2col();
16    channel_axis_ = bottom[0]->CanonicalAxisIndex(conv_param.axis());
17    const int first_spatial_axis = channel_axis_ + 1;
18    const int num_axes = bottom[0]->num_axes();
19    num_spatial_axes_ = num_axes - first_spatial_axis;
20    CHECK_GE(num_spatial_axes_, 0);
21    vector<int> bottom_dim_blob_shape(1, num_spatial_axes_ + 1);
22    vector<int> spatial_dim_blob_shape(1, std::max(num_spatial_axes_, 1));
23    kernel_shape_.Reshape(spatial_dim_blob_shape);
24    int* kernel_shape_data = kernel_shape_.mutable_cpu_data();
25    if (conv_param.has_kernel_h() || conv_param.has_kernel_w()) {
26      CHECK_EQ(num_spatial_axes_, 2)
27          << "kernel_h & kernel_w can only be used for 2D convolution.";
28      CHECK_EQ(0, conv_param.kernel_size_size())
29          << "Either kernel_size or kernel_h/w should be specified; not both.";
30      kernel_shape_data[0] = conv_param.kernel_h();
31      kernel_shape_data[1] = conv_param.kernel_w();
<span onclick='openModal()' class='match'>32    } else {
33      const int num_kernel_dims = conv_param.kernel_size_size();
34      CHECK(num_kernel_dims == 1 || num_kernel_dims == num_spatial_axes_)
35          << "kernel_size must be specified once, or once per spatial dimension "
36          << "(kernel_size specified " << num_kernel_dims << " times; "
37          << num_spatial_axes_ << " spatial dims).";
38        for (int i = 0; i < num_spatial_axes_; ++i) {
</span>39          kernel_shape_data[i] =
40              conv_param.kernel_size((num_kernel_dims == 1) ? 0 : i);
41        }
42    }
43    for (int i = 0; i < num_spatial_axes_; ++i) {
44      CHECK_GT(kernel_shape_data[i], 0) << "Filter dimensions must be nonzero.";
45    }
46    stride_.Reshape(spatial_dim_blob_shape);
47    int* stride_data = stride_.mutable_cpu_data();
48    if (conv_param.has_stride_h() || conv_param.has_stride_w()) {
49      CHECK_EQ(num_spatial_axes_, 2)
50          << "stride_h & stride_w can only be used for 2D convolution.";
51      CHECK_EQ(0, conv_param.stride_size())
52          << "Either stride or stride_h/w should be specified; not both.";
53      stride_data[0] = conv_param.stride_h();
54      stride_data[1] = conv_param.stride_w();
55    } else {
56      const int num_stride_dims = conv_param.stride_size();
57      CHECK(num_stride_dims == 0 || num_stride_dims == 1 ||
58            num_stride_dims == num_spatial_axes_)
59          << "stride must be specified once, or once per spatial dimension "
60          << "(stride specified " << num_stride_dims << " times; "
61          << num_spatial_axes_ << " spatial dims).";
62      const int kDefaultStride = 1;
63      for (int i = 0; i < num_spatial_axes_; ++i) {
64        stride_data[i] = (num_stride_dims == 0) ? kDefaultStride :
65            conv_param.stride((num_stride_dims == 1) ? 0 : i);
66        CHECK_GT(stride_data[i], 0) << "Stride dimensions must be nonzero.";
67      }
68    }
69    pad_.Reshape(spatial_dim_blob_shape);
70    int* pad_data = pad_.mutable_cpu_data();
71    if (conv_param.has_pad_h() || conv_param.has_pad_w()) {
72      CHECK_EQ(num_spatial_axes_, 2)
73          << "pad_h & pad_w can only be used for 2D convolution.";
74      CHECK_EQ(0, conv_param.pad_size())
75          << "Either pad or pad_h/w should be specified; not both.";
76      pad_data[0] = conv_param.pad_h();
77      pad_data[1] = conv_param.pad_w();
78    } else {
79      const int num_pad_dims = conv_param.pad_size();
80      CHECK(num_pad_dims == 0 || num_pad_dims == 1 ||
81            num_pad_dims == num_spatial_axes_)
82          << "pad must be specified once, or once per spatial dimension "
83          << "(pad specified " << num_pad_dims << " times; "
84          << num_spatial_axes_ << " spatial dims).";
85      const int kDefaultPad = 0;
86      for (int i = 0; i < num_spatial_axes_; ++i) {
87        pad_data[i] = (num_pad_dims == 0) ? kDefaultPad :
88            conv_param.pad((num_pad_dims == 1) ? 0 : i);
89      }
90    }
91    dilation_.Reshape(spatial_dim_blob_shape);
92    int* dilation_data = dilation_.mutable_cpu_data();
93    const int num_dilation_dims = conv_param.dilation_size();
94    CHECK(num_dilation_dims == 0 || num_dilation_dims == 1 ||
95          num_dilation_dims == num_spatial_axes_)
96        << "dilation must be specified once, or once per spatial dimension "
97        << "(dilation specified " << num_dilation_dims << " times; "
98        << num_spatial_axes_ << " spatial dims).";
99    const int kDefaultDilation = 1;
100    for (int i = 0; i < num_spatial_axes_; ++i) {
101      dilation_data[i] = (num_dilation_dims == 0) ? kDefaultDilation :
102                         conv_param.dilation((num_dilation_dims == 1) ? 0 : i);
103    }
104    is_1x1_ = true;
105    for (int i = 0; i < num_spatial_axes_; ++i) {
106      is_1x1_ &=
107          kernel_shape_data[i] == 1 && stride_data[i] == 1 && pad_data[i] == 0;
108      if (!is_1x1_) { break; }
109    }
110    channels_ = bottom[0]->shape(channel_axis_);
111    num_output_ = this->layer_param_.convolution_param().num_output();
112    CHECK_GT(num_output_, 0);
113    group_ = this->layer_param_.convolution_param().group();
114    CHECK_EQ(channels_ % group_, 0);
115    CHECK_EQ(num_output_ % group_, 0)
116        << "Number of output should be multiples of group.";
117    if (reverse_dimensions()) {
118      conv_out_channels_ = channels_;
119      conv_in_channels_ = num_output_;
120    } else {
121      conv_out_channels_ = num_output_;
122      conv_in_channels_ = channels_;
123    }
124    vector<int> weight_shape(2);
125    weight_shape[0] = conv_out_channels_;
126    weight_shape[1] = conv_in_channels_ / group_;
127    for (int i = 0; i < num_spatial_axes_; ++i) {
128      weight_shape.push_back(kernel_shape_data[i]);
129    }
130    bias_term_ = this->layer_param_.convolution_param().bias_term();
131    vector<int> bias_shape(bias_term_, num_output_);
132    if (this->blobs_.size() > 0) {
133      CHECK_EQ(1 + bias_term_, this->blobs_.size())
134          << "Incorrect number of weight blobs.";
135      if (weight_shape != this->blobs_[0]->shape()) {
136        Blob<Dtype> weight_shaped_blob(weight_shape);
137        LOG(FATAL) << "Incorrect weight shape: expected shape "
138            << weight_shaped_blob.shape_string() << "; instead, shape was "
139            << this->blobs_[0]->shape_string();
140      }
141      if (bias_term_ && bias_shape != this->blobs_[1]->shape()) {
142        Blob<Dtype> bias_shaped_blob(bias_shape);
143        LOG(FATAL) << "Incorrect bias shape: expected shape "
144            << bias_shaped_blob.shape_string() << "; instead, shape was "
145            << this->blobs_[1]->shape_string();
146      }
147      LOG(INFO) << "Skipping parameter initialization";
148    } else {
149      if (bias_term_) {
150        this->blobs_.resize(2);
151      } else {
152        this->blobs_.resize(1);
153      }
154      this->blobs_[0].reset(new Blob<Dtype>(weight_shape));
155      shared_ptr<Filler<Dtype> > weight_filler(GetFiller<Dtype>(
156          this->layer_param_.convolution_param().weight_filler()));
157      weight_filler->Fill(this->blobs_[0].get());
158      if (bias_term_) {
159        this->blobs_[1].reset(new Blob<Dtype>(bias_shape));
160        shared_ptr<Filler<Dtype> > bias_filler(GetFiller<Dtype>(
161            this->layer_param_.convolution_param().bias_filler()));
162        bias_filler->Fill(this->blobs_[1].get());
163      }
164    }
165    kernel_dim_ = this->blobs_[0]->count(1);
166    weight_offset_ = conv_out_channels_ * kernel_dim_ / group_;
167    this->param_propagate_down_.resize(this->blobs_.size(), true);
168  }
169  template <typename Dtype>
170  void BaseConvolutionLayer<Dtype>::DoReshape(const vector<Blob<Dtype>*>& bottom,
171        const vector<Blob<Dtype>*>& top) {
172    const int first_spatial_axis = channel_axis_ + 1;
173    CHECK_EQ(bottom[0]->num_axes(), first_spatial_axis + num_spatial_axes_)
174        << "bottom num_axes may not change.";
175    num_ = bottom[0]->count(0, channel_axis_);
176    CHECK_EQ(bottom[0]->shape(channel_axis_), channels_)
177        << "Input size incompatible with convolution kernel.";
178  #ifdef DISABLE_CONV_SUM_FUSION
179    for (int bottom_id = 1; bottom_id < bottom.size(); ++bottom_id) {
180      CHECK(bottom[0]->shape() == bottom[bottom_id]->shape())
181          << "All inputs must have the same shape.";
182    }
183  #endif
184    bottom_shape_ = &bottom[0]->shape();
185    compute_output_shape();
186    vector<int> top_shape(bottom[0]->shape().begin(),
187        bottom[0]->shape().begin() + channel_axis_);
188    top_shape.push_back(num_output_);
189    for (int i = 0; i < num_spatial_axes_; ++i) {
190      top_shape.push_back(output_shape_[i]);
191    }
192    for (int top_id = 0; top_id < top.size(); ++top_id) {
193      top[top_id]->Reshape(top_shape);
194    }
195    if (reverse_dimensions()) {
196      conv_out_spatial_dim_ = bottom[0]->count(first_spatial_axis);
197    } else {
198      conv_out_spatial_dim_ = top[0]->count(first_spatial_axis);
199    }
200    col_offset_ = kernel_dim_ * conv_out_spatial_dim_;
201    output_offset_ = conv_out_channels_ * conv_out_spatial_dim_ / group_;
202    vector<int> bottom_dim_blob_shape(1, num_spatial_axes_ + 1);
203    conv_input_shape_.Reshape(bottom_dim_blob_shape);
204    int* conv_input_shape_data = conv_input_shape_.mutable_cpu_data();
205    for (int i = 0; i < num_spatial_axes_ + 1; ++i) {
206      if (reverse_dimensions()) {
207        conv_input_shape_data[i] = top[0]->shape(channel_axis_ + i);
208      } else {
209        conv_input_shape_data[i] = bottom[0]->shape(channel_axis_ + i);
210      }
211    }
212    col_buffer_shape_.clear();
213    col_buffer_shape_.push_back(kernel_dim_ * group_);
214    for (int i = 0; i < num_spatial_axes_; ++i) {
215      if (reverse_dimensions()) {
216        col_buffer_shape_.push_back(input_shape(i + 1));
217      } else {
218        col_buffer_shape_.push_back(output_shape_[i]);
219      }
220    }
221    col_buffer_.Reshape(col_buffer_shape_);
222    bottom_dim_ = bottom[0]->count(channel_axis_);
223    top_dim_ = top[0]->count(channel_axis_);
224    num_kernels_im2col_ = conv_in_channels_ * conv_out_spatial_dim_;
225    num_kernels_col2im_ = reverse_dimensions() ? top_dim_ : bottom_dim_;
226    out_spatial_dim_ = top[0]->count(first_spatial_axis);
227    if (bias_term_) {
228      vector<int> bias_multiplier_shape(1, out_spatial_dim_);
229      bias_multiplier_.Reshape(bias_multiplier_shape);
230      caffe_set(bias_multiplier_.count(), Dtype(1),
231          bias_multiplier_.mutable_cpu_data());
232    }
233  }
234  template <typename Dtype>
235  void BaseConvolutionLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
236        const vector<Blob<Dtype>*>& top) {
237    DoReshape(bottom, top);
238    num_of_threads_ = 1;
239  #ifdef _OPENMP
240    num_of_threads_ = omp_get_max_threads() < bottom[0]->shape(0) ?
241                      omp_get_max_threads() : bottom[0]->shape(0);
242    if (num_of_threads_ < 1) {
243       LOG(WARNING) << "Base Conv layer: omp_get_max_threads() ="
244                    << num_of_threads_;
245       num_of_threads_ = 1;
246    }
247  #endif
248    col_buffer_mt_size = num_of_threads_ * static_cast<size_t>(col_buffer_.count());
249    weight_diff_mt_size = num_of_threads_ * static_cast<size_t>(this->blobs_[0]->count());
250    col_buffer_mt_.resize(col_buffer_mt_size);
251    weight_diff_mt_.resize(weight_diff_mt_size);
252  }
253  template <typename Dtype>
254  void BaseConvolutionLayer<Dtype>::ReshapeForMKL(const vector<Blob<Dtype>*>& bottom,
255        const vector<Blob<Dtype>*>& top) {
256    DoReshape(bottom, top);
257  }
258  template <typename Dtype>
259  void BaseConvolutionLayer<Dtype>::forward_cpu_gemm(const Dtype* input,
260      const Dtype* weights, Dtype* output, bool skip_im2col) {
261    int tid = 0;
262  #ifdef _OPENMP
263    tid = omp_get_thread_num();
264    if (tid >= num_of_threads_) {
265      LOG(FATAL) << "ConvLayer::Forward_cpu: omp_thread_num() =" << tid
266                 << " > OMP_num_THREADS = " << num_of_threads_;
267    }
268    tid = tid % num_of_threads_;  
269  #endif
270    size_t col_data_buffer_size = col_buffer_mt_.size()/num_of_threads_;
271    Dtype* col_buff = const_cast<Dtype*>(input);
272    if (!is_1x1_) {
273      col_buff = & col_buffer_mt_[ tid* col_data_buffer_size];
274      if (!skip_im2col) {
275        conv_im2col_cpu(input, col_buff);
276      }
277    }
278    for (int g = 0; g < group_; ++g) {
279      caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, conv_out_channels_ /
280          group_, conv_out_spatial_dim_, kernel_dim_,
281          (Dtype)1., weights + weight_offset_ * g, col_buff + col_offset_ * g,
282          (Dtype)0., output + output_offset_ * g);
283    }
284  }
285  template <typename Dtype>
286  void BaseConvolutionLayer<Dtype>::forward_cpu_bias(Dtype* output,
287      const Dtype* bias) {
288    caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num_output_,
289        out_spatial_dim_, 1, (Dtype)1., bias, bias_multiplier_.cpu_data(),
290        (Dtype)1., output);
291  }
292  template <typename Dtype>
293  void BaseConvolutionLayer<Dtype>::backward_cpu_gemm(const Dtype* output,
294      const Dtype* weights, Dtype* input) {
295    int tid = 0;
296  #ifdef _OPENMP
297    tid = omp_get_thread_num();
298    if (tid >= num_of_threads_) {
299      LOG(FATAL) << "ConvLayer::backward_cpu_gemm: omp_thread_num() =" << tid
300                 << " > OMP_num_THREADS = " << num_of_threads_;
301    }
302    tid = tid % num_of_threads_;  
303  #endif
304    size_t col_data_buffer_size = col_buffer_mt_.size()/num_of_threads_;
305    Dtype* col_buff = & col_buffer_mt_[ tid* col_data_buffer_size];
306    if (is_1x1_) {
307      col_buff = input;
308    }
309    for (int g = 0; g < group_; ++g) {
310      caffe_cpu_gemm<Dtype>(CblasTrans, CblasNoTrans, kernel_dim_,
311          conv_out_spatial_dim_, conv_out_channels_ / group_,
312          (Dtype)1., weights + weight_offset_ * g, output + output_offset_ * g,
313          (Dtype)0., col_buff + col_offset_ * g);
314    }
315    if (!is_1x1_) {
316      conv_col2im_cpu(col_buff, input);
317    }
318  }
319  template <typename Dtype>
320  void BaseConvolutionLayer<Dtype>::clear_weight_mt(void) {
321    size_t weight_diff_size = weight_diff_mt_.size() / num_of_threads_;
322    caffe_memset(num_of_threads_*weight_diff_size*sizeof(Dtype),
323                 0.,
324                 &weight_diff_mt_[0]);
325  }
326  template <typename Dtype>
327  void BaseConvolutionLayer<Dtype>::sum_weight_mt(Dtype* weight_diff) {
328    size_t weight_diff_size =  weight_diff_mt_.size() / num_of_threads_;
329    size_t col_per_thread = weight_diff_size/num_of_threads_;
330    int tid = 0;
331  #ifdef _OPENMP
332      if (omp_in_parallel()) {
333          tid = omp_get_thread_num();
334      }
335  #endif
336      for (size_t j = 0; j < col_per_thread; ++j) {
337        for (size_t t = 0; t < num_of_threads_ ; ++t) {
338            weight_diff[tid*col_per_thread + j] +=
339              weight_diff_mt_[t*weight_diff_size + tid*col_per_thread + j];
340        }
341      }
342      size_t j = col_per_thread*num_of_threads_ + tid;
343      if (j < weight_diff_size) {
344        for (size_t t = 0; t < num_of_threads_ ; ++t) {
345          weight_diff[j] += weight_diff_mt_[t * weight_diff_size + j];
346        }
347      }
348  }
349  template <typename Dtype>
350  void BaseConvolutionLayer<Dtype>::weight_cpu_gemm(const Dtype* input,
351      const Dtype* output, Dtype* weights) {
352    int tid = 0;
353  #ifdef _OPENMP
354    Dtype* weight_diff_data = NULL;
355    if (num_of_threads_ > 1) {
356      tid = omp_get_thread_num();
357      if (tid >= num_of_threads_) {
358        LOG(FATAL) << "ConvLayer::weights_cpu_gemm: omp_thread_num() =" << tid
359                   << " > OMP_num_THREADS = " << num_of_threads_;
360      }
361      tid = tid % num_of_threads_;  
362      weight_diff_data = &weight_diff_mt_[tid * (weight_diff_mt_.size() / num_of_threads_)];
363    } else {
364      weight_diff_data = weights;
365    }
366  #else
367    Dtype* weight_diff_data = weights;
368  #endif
369    Dtype* col_buff = const_cast<Dtype*>(input);
370    if (!is_1x1_) {
371      size_t col_data_buffer_size = col_buffer_mt_.size() / num_of_threads_;
372      col_buff = &col_buffer_mt_[tid * col_data_buffer_size];
373      conv_im2col_cpu(input, col_buff);
374    }
375    for (int g = 0; g < group_; ++g) {
376      caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasTrans, conv_out_channels_ / group_,
377          kernel_dim_, conv_out_spatial_dim_,
378          (Dtype)1., output + output_offset_ * g, col_buff + col_offset_ * g,
379          (Dtype)1., weight_diff_data + weight_offset_ * g);
380    }
381  }
382  template <typename Dtype>
383  void BaseConvolutionLayer<Dtype>::backward_cpu_bias(Dtype* bias,
384      const Dtype* input) {
385    caffe_cpu_gemv<Dtype>(CblasNoTrans, num_output_, out_spatial_dim_, 1.,
386        input, bias_multiplier_.cpu_data(), 1., bias);
387  }
388  #ifndef CPU_ONLY
389  template <typename Dtype>
390  void BaseConvolutionLayer<Dtype>::forward_gpu_gemm(const Dtype* input,
391      const Dtype* weights, Dtype* output, bool skip_im2col) {
392    const Dtype* col_buff = input;
393    if (!is_1x1_) {
394      if (!skip_im2col) {
395        conv_im2col_gpu(input, col_buffer_.mutable_gpu_data());
396      }
397      col_buff = col_buffer_.gpu_data();
398    }
399    for (int g = 0; g < group_; ++g) {
400      caffe_gpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, conv_out_channels_ /
401          group_, conv_out_spatial_dim_, kernel_dim_,
402          (Dtype)1., weights + weight_offset_ * g, col_buff + col_offset_ * g,
403          (Dtype)0., output + output_offset_ * g);
404    }
405  }
406  template <typename Dtype>
407  void BaseConvolutionLayer<Dtype>::forward_gpu_bias(Dtype* output,
408      const Dtype* bias) {
409    caffe_gpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num_output_,
410        out_spatial_dim_, 1, (Dtype)1., bias, bias_multiplier_.gpu_data(),
411        (Dtype)1., output);
412  }
413  template <typename Dtype>
414  void BaseConvolutionLayer<Dtype>::backward_gpu_gemm(const Dtype* output,
415      const Dtype* weights, Dtype* input) {
416    Dtype* col_buff = col_buffer_.mutable_gpu_data();
417    if (is_1x1_) {
418      col_buff = input;
419    }
420    for (int g = 0; g < group_; ++g) {
421      caffe_gpu_gemm<Dtype>(CblasTrans, CblasNoTrans, kernel_dim_,
422          conv_out_spatial_dim_, conv_out_channels_ / group_,
423          (Dtype)1., weights + weight_offset_ * g, output + output_offset_ * g,
424          (Dtype)0., col_buff + col_offset_ * g);
425    }
426    if (!is_1x1_) {
427      conv_col2im_gpu(col_buff, input);
428    }
429  }
430  template <typename Dtype>
431  void BaseConvolutionLayer<Dtype>::weight_gpu_gemm(const Dtype* input,
432      const Dtype* output, Dtype* weights) {
433    const Dtype* col_buff = input;
434    if (!is_1x1_) {
435      conv_im2col_gpu(input, col_buffer_.mutable_gpu_data());
436      col_buff = col_buffer_.gpu_data();
437    }
438    for (int g = 0; g < group_; ++g) {
439      caffe_gpu_gemm<Dtype>(CblasNoTrans, CblasTrans, conv_out_channels_ / group_,
440          kernel_dim_, conv_out_spatial_dim_,
441          (Dtype)1., output + output_offset_ * g, col_buff + col_offset_ * g,
442          (Dtype)1., weights + weight_offset_ * g);
443    }
444  }
445  template <typename Dtype>
446  void BaseConvolutionLayer<Dtype>::backward_gpu_bias(Dtype* bias,
447      const Dtype* input) {
448    caffe_gpu_gemv<Dtype>(CblasNoTrans, num_output_, out_spatial_dim_, 1.,
449        input, bias_multiplier_.gpu_data(), 1., bias);
450  }
451  #endif  
452  INSTANTIATE_CLASS(BaseConvolutionLayer);
453  }  
</code></pre>
        </div>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-linkpred.cpp</h3>
            <pre><code>1  #include "stdafx.h"
2  #include "linkpred.h"
3  template <class PGraph>
4  void GetRndWalkRestart(const PGraph& Graph, const double& JumpProb, const int& JumpNId, THash<TInt, TFlt>& RwrNIdH) {
5    const double DefVal = 1.0/Graph->GetNodes();
6    RwrNIdH.Clr(false);
7    TIntH NIdOutDegH;
8    for (typename PGraph::TObj::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++) {
9      RwrNIdH.AddDat(NI.GetId(), DefVal);
10      NIdOutDegH.AddDat(NI.GetId(), NI.GetOutDeg());
11    }
12    THash<TInt, TFlt> RwrNIdH2(Graph->GetNodes());
13    for (int iter = 0; iter < 10; iter++) {
14      double Sum = 0;
15      for (typename PGraph::TObj::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++) {
16        double Val = 0;
17        for (int i = 0; i < NI.GetInDeg(); i++) {
18          const int InId = NI.GetInNId(i);
19          Val += (1.0-JumpProb) * 1.0/(NIdOutDegH.GetDat(InId)) * RwrNIdH.GetDat(InId);
20        }
21        if (NI.GetId() == JumpNId) { Val+= JumpProb; }
22        RwrNIdH.AddDat(NI.GetId(), Val);
23        Sum += Val;
24      }
25      for (int i = 0; i < RwrNIdH.Len(); i++) {
26        RwrNIdH[i] /= Sum; 
27      }
28    }
29  }
30  template <class PGraph>
31  double GetAdamicAdar(const PGraph& Graph, const int& SrcNId, const int& DstNId) {
32    TIntV CmnV;
33    TSnap::GetCmnNbrs(Graph, SrcNId, DstNId, CmnV);
34    double Aa = 0;
35    for (int c=0; c < CmnV.Len(); c++) {
36      Aa += 1.0/log(Graph->GetNI(CmnV[c]).GetDeg());
37    }
38    return Aa;
39  }
40  template <class PGraph>
41  void GetAdamicAdar(const PGraph& Graph, const int& SrcNId, THash<TInt, TFlt>& AaNIdH) {
42    TIntV NIdV, CmnV;
43    TSnap::GetNodesAtHop(Graph, SrcNId, 2, NIdV, false);
44    AaNIdH.Clr(false);
45    for (int i = 0; i < NIdV.Len(); i++) {
46      const int DstNId = NIdV[i];
47      TSnap::GetCmnNbrs(Graph, SrcNId, DstNId, CmnV);
48      double Aa=0;
49      for (int c=0; c < CmnV.Len(); c++) {
50        Aa += 1.0/log((double)Graph->GetNI(CmnV[c]).GetDeg());
51      }
52      AaNIdH.AddDat(DstNId, Aa);
53    }
54  }
55  void TLpExample::GetNetAttrV(TVec<TFltV>& AttrV) const { 
56    THash<TInt, TFlt> RwrNIdH, AdamicAdar;
57    GetRndWalkRestart(PNet((TLpExample*)this), 0.15, SrcNId, RwrNIdH);
58    GetAdamicAdar(PNet((TLpExample*)this), SrcNId, AdamicAdar);
59    AttrV.Gen(DstNIdV.Len()+NolNIdV.Len(), 0);
60    const TNodeI SrcNI = GetNI(SrcNId);
61    for (int i = 0; i < DstNIdV.Len(); i++) {
62      const int Dst = DstNIdV[i];
63      const int Cmn = TSnap::GetCmnNbrs(PNet((TLpExample*)this), SrcNId, Dst);
64      AttrV.Add();  TFltV& A = AttrV.Last();
65      A.Add(DstNIdV[i]()); A.Add(1);
66      A.Add(SrcNI.GetDeg());   A.Add(GetNI(Dst).GetDeg());
67      A.Add(Cmn);  A.Add(RwrNIdH.GetDat(Dst));  A.Add(AdamicAdar.GetDat(Dst)); 
68    }
69    for (int i = 0; i < NolNIdV.Len(); i++) {
70      const int Dst = NolNIdV[i];
71      const int Cmn = TSnap::GetCmnNbrs(PNet((TLpExample*)this), SrcNId, Dst);
72      AttrV.Add();  TFltV& A = AttrV.Last();
73      A.Add(Dst); A.Add(0);
74      A.Add(SrcNI.GetDeg());   A.Add(GetNI(Dst).GetDeg());
75      A.Add(Cmn);  
76      A.Add(RwrNIdH.GetDat(Dst)); 
77      if (AdamicAdar.IsKey(Dst)) {
78        A.Add(AdamicAdar.GetDat(Dst)); }
79      else { A.Add(0); }
80    }
81  }
82  void TLpExample::GetNodeAttrV(TVec<TFltV>& AttrV) const { 
83    THash<TInt, TPair<TInt, TFltV> > NIdAttrH;
84    for (TEdgeI EI=BegEI(); EI<EndEI(); EI++) {
85      const TFltV& EA = EI().AttrV;
86      if (! NIdAttrH.IsKey(EI.GetSrcNId())) {
87        NIdAttrH.AddDat(EI.GetSrcNId(), TPair<TInt, TFltV>(1, EI().AttrV)); }
88      else {
89        TPair<TInt, TFltV>& A = NIdAttrH.GetDat(EI.GetSrcNId());
90        for (int e = 0; e < EA.Len(); e++) { A.Val2[e]+=EA[e]; } A.Val1++;
91      }
92    }
93    for (int i = 0; i <NIdAttrH.Len(); i++) {
94      TPair<TInt, TFltV>& A = NIdAttrH[i];
95      for (int i =0; i < A.Val2.Len(); i++) { 
96        A.Val2[i] /= double(A.Val1()); }
97    } /&bsol;*/
98    AttrV.Gen(DstNIdV.Len()+NolNIdV.Len(), 0);
99    const TFltV& SrcAV = NIdAttrH.GetDat(SrcNId).Val2;
100    for (int i = 0; i < DstNIdV.Len(); i++) {
101      AttrV.Add();  TFltV& A = AttrV.Last();
102      A.Add(DstNIdV[i]()); A.Add(1);
103      A.AddV(SrcAV); A.AddV(NIdAttrH.GetDat(DstNIdV[i]).Val2);
104    }
105    for (int i = 0; i < NolNIdV.Len(); i++) {
106      AttrV.Add();  TFltV& A = AttrV.Last();
107      A.Add(NolNIdV[i]()); A.Add(0);
108      A.AddV(SrcAV); A.AddV(NIdAttrH.GetDat(NolNIdV[i]).Val2);
109    }
110  }
111  void TLpExample::GetLen2PathAttrV(TVec<TFltV>& AttrV) const { 
112    TIntV CmnV;
113    AttrV.Gen(DstNIdV.Len()+NolNIdV.Len(), 0);
114    TIntV NIdV(DstNIdV);  NIdV.AddV(NolNIdV);
115    for (int i = 0; i < NIdV.Len(); i++) {
116      TFltV AttrV1(GetNAttr()), AttrV2(GetNAttr());
117      int a1=0, a2=0;
118      const int DstNId = NIdV[i];
119      TSnap::GetCmnNbrs(PNet((TLpExample*)this), SrcNId, DstNId, CmnV);
120      for (int c = 0; c < CmnV.Len(); c++) {
121        const int C = CmnV[c];
122        if (IsEdge(SrcNId, C)) {
123          const TFltV& A = GetEDat(SrcNId, C).AttrV;
124          for (int a=0;a<A.Len();a++) { AttrV1[a]+=A[a]; } a1++; }
125        else {Fail; }
126        if (IsEdge(DstNId, C)) {
127          const TFltV& A = GetEDat(DstNId, C).AttrV;
128          for (int a=0;a<A.Len();a++) { AttrV2[a]+=A[a]; } a2++; }
129        else { Fail; }
130      }
131      for (int a=0;a<AttrV1.Len();a++) { AttrV1[a]/=double(a1); }
132      for (int a=0;a<AttrV2.Len();a++) { AttrV2[a]/=double(a2); }
133      AttrV.Add();
134      AttrV.Last().Add(DstNId);
135      if (i < DstNIdV.Len()) { AttrV.Last().Add(1); }
136      else { AttrV.Last().Add(0); }
137      AttrV.Last().AddV(AttrV1);
138      AttrV.Last().AddV(AttrV2);
139    }
140  }
141  void TLpExample::SaveAttrV(FILE *F, const bool& NetA, const bool& NodeA, const bool& PathA) const {
142    TVec<TFltV> NetAV, NodeAV, PathAV;
143    if (NetA) { GetNetAttrV(NetAV); }
144    if (NodeA) { GetNodeAttrV(NodeAV); }
145    if (PathA) { GetLen2PathAttrV(PathAV); }
146    const TVec<TFltV>& NonE = NetAV.Len()>0?NetAV:(NodeAV.Len()>0?NodeAV:PathAV);
147    for (int e = 0; e < NonE.Len(); e++) {
148      fprintf(F, "%d\t%d\t%d", SrcNId.Val, (int)NonE[e][0](), e<DstNIdV.Len()?1:0); 
149      if (NodeAV.Len()) { for (int a=2; a<NodeAV[e].Len(); a++) { fprintf(F, "\t%f", NodeAV[e][a].Val); } }
150      if (NetAV.Len())  { for (int a=2; a<NetAV[e].Len(); a++) { fprintf(F, "\t%f", NetAV[e][a].Val); } }
151      if (PathAV.Len()) { for (int a=2; a<PathAV[e].Len(); a++) { fprintf(F, "\t%f", PathAV[e][a].Val); } }
152      fprintf(F, "\n");
153    }
154  }
155  void TLpExample::InitQGraph() {
156    const int NAttr = GetNAttr();
157    if (DummyNId == -1) {
158      DummyNId = AddNode();
159      const TEdgeGrad DummyEG(NAttr);
160      for (TNodeI NI = BegNI(); NI < DummyNI(); NI++) { 
161        AddEdge(NI.GetId(), DummyNId, DummyEG); }
162      AddEdge(DummyNId, SrcNId, DummyEG); 
163    }
164    for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) {
165      TEdgeGrad& EG = EI(); 
166      EG.Val = 0;
167      EG.GradV.PutAll(0);
168    }
169    PRankH.Gen(GetNodes());
170    PrGradH.Gen(GetNodes());
171    TFltV GradV(NAttr);
172    GradV.PutAll(0.0);
173    for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
174      PRankH.AddDat(NI.GetId(), 1.0/(double)GetNodes());
175      PrGradH.AddDat(NI.GetId(), GradV);
176    }
177  }
178  void TLpExample::UpdateQGraph(const TLpPredictor& Predictor) {
179    TFltV SumGradV(GetNAttr());
180    for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
181      double Z = 0.0;
182      SumGradV.PutAll(0.0);
183      const int I = NI.GetId();
184      for (int i = 0; i < NI.GetOutDeg(); i++) {
185        const int J = NI.GetOutNId(i);
186        TEdgeGrad& EG = NI.GetOutEDat(i);
187        if (I == DummyNId) { 
188          EG.Val = 1; 
189        } else if (J == DummyNId) { 
190          EG.Val = Predictor.Alpha; 
191        } else { 
192          EG.Val = Predictor.GetFVal(EG.AttrV);
193          Predictor.GetFGrad(EG.AttrV, EG.GradV);
194          Z += EG.Val;
195          for (int k = 0; k < SumGradV.Len(); k++) { 
196            SumGradV[k] += EG.GradV[k]; }
197        }
198      }
199      for (int j = 0; j < NI.GetOutDeg(); j++) {
200        const int J = NI.GetOutNId(j);
201        if (I != DummyNId && J != DummyNId) { 
202          TEdgeGrad& EG = NI.GetOutEDat(j);
203          for (int k = 0; k < SumGradV.Len(); k++) { 
204            EG.GradV[k] = (1.0-Predictor.Alpha) * (EG.GradV[k]*Z - EG.Val*SumGradV[k]) / TMath::Sqr(Z); }
205          EG.Val = (1.0-Predictor.Alpha) * EG.Val / Z;
206        }
207      }
208    }
209  }
210  void TLpExample::CalcPageRank() {
211    const double InitVal = 1.0/double(GetNodes());
212    const int NAttr = GetNAttr();
213    for (int i = 0; i < PRankH.Len(); i++) {
214      PRankH[i] = InitVal;
215      PrGradH[i].PutAll(0);
216    }
217    THash<TInt, TFlt> NewPrH(PRankH);
218    THash<TInt, TFltV> NewGrH(PrGradH);
219    for (int i = 0; i < NewPrH.Len(); i++) { 
220      NewPrH[i] = 0; 
221      NewGrH[i].PutAll(0);
222    }
223    double s=0, sumDelta=1, sumDelta2=1;
224    for (int T = 0; T < 100 && sumDelta>1e-2; T++) {
225      for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
226        double sum = 0;
227        const int I = NI.GetId();
228        const double SrcPr = PRankH.GetDat(I);
229        for (int e = 0; e < NI.GetOutDeg(); e++) {
230          const int J = NI.GetOutNId(e);
231          NewPrH.GetDat(J) += SrcPr * NI.GetOutEDat(e).Val; }
232      }
233      s=0; sumDelta=0;
234      printf("\n***ITER %d\n", T+1);
235      for (int i = 0; i < NewPrH.Len(); i++) { 
236        sumDelta += fabs(PRankH[i]-NewPrH[i]);
237        s += NewPrH[i];
238        PRankH[i] = NewPrH[i];  NewPrH[i]=0; 
239      }
240      printf("\t%f (Delta: %f)\n", s, sumDelta);
241      for (TNodeI NI = BegNI(); NI < DummyNI(); NI++) {
242        const int I = NI.GetId();
243        const double SrcPR = PRankH.GetDat(I);
244        const TFltV& SrcGR = PrGradH.GetDat(I);
245        for (int e = 0; e < NI.GetOutDeg()-1; e++) {
246          const int J = NI.GetOutNId(e);
247          const TEdgeGrad& EG = NI.GetOutEDat(e);
248          TFltV& NewDstGr = NewGrH.GetDat(J);
249          for (int k = 0; k < NAttr; k++) {
250            NewDstGr[k] +=  SrcPR * EG.GradV[k] + SrcGR[k] * EG.Val; }
251        }
252      }
253      sumDelta2=0;
254      for (int i = 0; i < NewGrH.Len(); i++) { 
255        const TFltV& GrV = NewGrH[i];
256        const TFltV& OldV = PrGradH[i];
257        for (int k = 0; k < NAttr; k++) {
258          sumDelta2 += fabs(GrV[k]-OldV[k]);
259        }
260        PrGradH[i].Swap(NewGrH[i]);  NewGrH[i].PutAll(0); 
261      }
262      printf("\t%Delta: %f\n", sumDelta2);
263    }
264  }
265  void TLpExample::GenSmallExample1() {
266    for (int i = 0; i < 3; i++) { AddNode(i); }
267    AddEdge(0,1, TEdgeGrad(TFltV::GetV(0,1,1)));
268    AddEdge(1,0, TEdgeGrad(TFltV::GetV(0,1,1)));
269    AddEdge(0,2, TEdgeGrad(TFltV::GetV(1,0,1)));
270    AddEdge(2,0, TEdgeGrad(TFltV::GetV(1,0,1)));
271    SrcNId = 0;
272    DstNIdV = TIntV::GetV(1);
273    NolNIdV = TIntV::GetV(2);
274  }
275  void TLpExample::GenSmallExample2() {
276    for (int i = 0; i < 7; i++) { AddNode(i); }
277    AddEdge(0,1, TEdgeGrad(TFltV::GetV(1,1)));
278    AddEdge(0,2, TEdgeGrad(TFltV::GetV(1,1)));
279    AddEdge(2,3, TEdgeGrad(TFltV::GetV(1,1)));
280    AddEdge(1,3, TEdgeGrad(TFltV::GetV(1,1)));
281    AddEdge(0,4, TEdgeGrad(TFltV::GetV(0,0)));
282    AddEdge(4,5, TEdgeGrad(TFltV::GetV(1,1)));
283    AddEdge(4,6, TEdgeGrad(TFltV::GetV(0,1)));
284    SrcNId = 0;
285    DstNIdV = TIntV::GetV(6);
286    NolNIdV = TIntV::GetV(3, 5);
287  }
288  void TLpExample::SaveTxt(FILE *F) const {
289    fprintf(F, "%d\n", SrcNId.Val);
290    fprintf(F, "%d", DstNIdV[0].Val);
291    for (int i = 1 ; i < DstNIdV.Len(); i++) { fprintf(F, "\t%d", DstNIdV[i].Val); }
292    fprintf(F, "\n%d", NolNIdV[0].Val);
293    for (int i = 1 ; i < NolNIdV.Len(); i++) { fprintf(F, "\t%d", NolNIdV[i].Val); }
294    for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) {
295      if (EI.GetSrcNId()==DummyNId || EI.GetDstNId()==DummyNId) { continue; }
296      fprintf(F, "\n%d\t%d", EI.GetSrcNId(), EI.GetDstNId());
297      const TFltV& AttrV = EI().AttrV;
298      for (int a = 0; a < AttrV.Len(); a++) {
299        fprintf(F, "\t%f", AttrV[a].Val); }
300    }
301    fprintf(F, "\n\n");
302  }
303  void TLpExample::Dump(const bool& DumpPRank, const bool& DumpGrad, const bool& DumpNet) const {
304    if (DumpPRank) {
305      printf("PageRank:\n");
306      for (int i = 0; i < PRankH.Len(); i++) {
307        printf("  %d\t%f\n", PRankH.GetKey(i).Val, PRankH[i].Val); }
308      if (DumpGrad) { printf("\n"); }
309    }
310    if (DumpGrad) {
311      printf("PageRank Gradient:\n");
312      for (int i = 0; i < PrGradH.Len(); i++) {
313        printf("  %d", PrGradH.GetKey(i).Val);
314        for (int j = 0; j < PrGradH[i].Len(); j++) {
315          printf("\t%f", PrGradH[i][j].Val); }
316        printf("\n");
317      }
318      if (DumpNet) { printf("\n"); }
319    }
320    if (DumpNet) {
321      printf("Q-NET:\n");
322      for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
323        printf("%d:", NI.GetId());
324        for (int i = 0; i < NI.GetOutDeg(); i++) {
325          printf("  %d:%f", NI.GetOutNId(i), NI.GetOutEDat(i).Val()); }
326        printf("\n");
327      }
328      for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) {
329        printf("%d -- %d  G:", EI.GetSrcNId(), EI.GetDstNId());
330        for (int i = 0; i < EI().GradV.Len(); i++) {
331          printf("\t%f", EI().GradV[i].Val); }
332        printf("\n");
333      }
334    }
335  }
336  void TLpExample::LoadTxt(const TStr& FNm, TVec<TLpExample>& ExV) {
337    TSsParser SS(FNm, ssfTabSep);
338    printf("load:");
339    while (SS.Next()) {
340      ExV.Add();
341      TLpExample& Ex = ExV.Last();
342      Ex.SrcNId = SS.GetInt(0); IAssert(SS.Len()==1);
343      SS.Next();
344      for (int s = 0; s < SS.Len(); s++) {
345        Ex.DstNIdV.Add(SS.GetInt(s)); }
346      SS.Next();
347      for (int s = 0; s < SS.Len(); s++) {
348        Ex.NolNIdV.Add(SS.GetInt(s)); }
349      SS.Next();
350      do {
351        TFltV AttrV;
352        for (int i = 2; i < SS.Len(); i++) { AttrV.Add(SS.GetFlt(i)); }
353        const int I=SS.GetInt(0), J=SS.GetInt(1);
354        if (! Ex.IsNode(I)) { Ex.AddNode(I); }
355        if (! Ex.IsNode(J)) { Ex.AddNode(J); }
356        Ex.AddEdge(I, J, AttrV);
357        AttrV.Swap(0,1); Ex.AddEdge(J, I, AttrV); 
358        if (! SS.Next()) { break;}
359      } while (SS.Len() > 1);
360      printf("  %d", Ex.SrcNId.Val);
361    }
362    printf("\nitems: %d\n", ExV.Len());
363  }
364  double TLpPredictor::GetFVal(const TFltV& AttrV) const {
365    Assert(WgtV.Len() == AttrV.Len());
366    double Val = 0;
367    for (int i = 0; i < WgtV.Len(); i++) {
368      Val += WgtV[i]*AttrV[i];
369    }
370    return exp(-Val);
371  }
372  void TLpPredictor::GetFGrad(const TFltV& AttrV, TFltV& GradV) const {
373    const double Val = GetFVal(AttrV);
374    if (GradV.Len() != WgtV.Len()) {
375      GradV.Gen(WgtV.Len()); }
376    for (int i = 0; i < AttrV.Len(); i++) {
377      GradV[i] = - Val * AttrV[i];
378    }
379  }
380  double TLpPredictor::GetLoss(const double& Val) const {
381    if (Val < 0) { return 0; }
382    else if (Val < Z) { return Val*Val/(2.0*Z); }
383    else { return Val-Z/2.0; }
384  }
385  double TLpPredictor::GetLossGrad(const double& Val) const {
386    if (Val < 0) { return 0; }
387    else if (Val < Z) { return Val/Z; }
388    else { return 1; }
389  }
390  namespace __OLD__ {
391  void TLpExample::GenSmallExample1() {
392    Graph = TNGraph::New();
393    for (int i = 0; i < 3; i++) { Graph->AddNode(i); }
394    Graph->AddEdge(0,1);                          
395    Graph->AddEdge(1,0);
396    Graph->AddEdge(0,2);
397    Graph->AddEdge(2,0);
398    EAttrV.AddDat(TIntPr(0,1), TFltV::GetV(0,1,1)); 
399    EAttrV.AddDat(TIntPr(1,0), TFltV::GetV(0,1,1));
400    EAttrV.AddDat(TIntPr(0,2), TFltV::GetV(1,0,1));
401    EAttrV.AddDat(TIntPr(2,0), TFltV::GetV(1,0,1));
402    SrcNId = 0;
403    DstNIdV = TIntV::GetV(1);
404    NolNIdV = TIntV::GetV(2);
405  }
406  void TLpExample::GenSmallExample2() {
407    Graph = TNGraph::New();
408    for (int i = 0; i < 7; i++) { Graph->AddNode(i); }
409    Graph->AddEdge(0,1);
410    Graph->AddEdge(0,2);
411    Graph->AddEdge(2,3);
412    Graph->AddEdge(1,3);
413    Graph->AddEdge(0,4);
414    Graph->AddEdge(4,5);
415    Graph->AddEdge(4,6);
416    EAttrV.AddDat(TIntPr(0,1), TFltV::GetV(1,1));
417    EAttrV.AddDat(TIntPr(0,2), TFltV::GetV(1,1));
418    EAttrV.AddDat(TIntPr(2,3), TFltV::GetV(1,1));
419    EAttrV.AddDat(TIntPr(1,3), TFltV::GetV(1,1));
420    EAttrV.AddDat(TIntPr(0,4), TFltV::GetV(0,0));
421    EAttrV.AddDat(TIntPr(4,5), TFltV::GetV(1,1));
422    EAttrV.AddDat(TIntPr(4,6), TFltV::GetV(0, 1));
423    SrcNId = 0;
424    DstNIdV = TIntV::GetV(6);
425    NolNIdV = TIntV::GetV(3, 5);
426  }
427  double TLpPredictor::GetFVal(const TFltV& WgtV, const TFltV& AttrV) const {
428    Assert(WgtV.Len() == AttrV.Len());
429    double Val = 0;
430    for (int i = 0; i < WgtV.Len(); i++) {
431      Val += WgtV[i]*AttrV[i];
432    }
433    return exp(-Val);
434  }
435  void TLpPredictor::GetFGrad(const TFltV& WgtV, const TFltV& AttrV, TFltV& GradV) const {
436    const double Val = GetFVal(WgtV, AttrV);
437    if (GradV.Len() != WgtV.Len()) {
438      GradV.Gen(WgtV.Len()); }
439    for (int i = 0; i < AttrV.Len(); i++) {
440      GradV[i] = - Val * AttrV[i];
441    }
442  }
443  double TLpPredictor::GetLoss(const double& Val) const {
444    if (Val < 0) { return 0; }
445    else if (Val < Z) { return Val*Val/(2.0*Z); }
446    else { return Val-Z/2.0; }
447  }
448  double TLpPredictor::GetLossGrad(const double& Val) const {
449    if (Val < 0) { return 0; }
450    else if (Val < Z) { return Val/Z; }
451    else { return 1; }
452  }
453  void TQGraph::UpdateNormConst() {
454    for (int i = 0; i < EdgeGradH.Len(); i++) {
455      TEdgeGrad& E = NormConstH.AddDat(EdgeGradH.GetKey(i).Val1);
456      E.Val1 = 0;
457      if (! E.Val2.Empty()) { E.Val2.PutAll(0.0); }
458      else { E.Val2.Gen(EdgeGradH[i].Val2.Len()); }
459    }
460    for (int i = 0; i < EdgeGradH.Len(); i++) {
461      TEdgeGrad& E = NormConstH.AddDat(EdgeGradH.GetKey(i).Val1);
462      E.Val1 += EdgeGradH[i].Val1;
463      for (int e = 0; e < E.Val2.Len(); e++) {
464        E.Val2[e] += EdgeGradH[i].Val2[e];
465      }
466    }
467  }
468  void TQGraph::UpdateGrad(const double& Alpha, const TIntPr& Edge, TEdgeGrad& Grad) {
469    IAssert(GetNDat(Edge.Val1)!=lntDummy && GetNDat(Edge.Val2)!=lntDummy);
470    const TEdgeGrad& GradE = EdgeGradH.GetDat(Edge);
471    const TEdgeGrad& NormConst = NormConstH.GetDat(Edge.Val1);
472    Grad.Val1 = Alpha * GradE.Val1 / NormConst.Val1; 
473    TFltV& GradV = Grad.Val2;
474    printf("%d -- %d\n", Edge.Val1(), Edge.Val2());
475    for (int i = 0; i < GradV.Len(); i++) {
476      GradV[i] = Alpha * (GradE.Val2[i]*NormConst.Val1 - GradE.Val1*NormConst.Val2[i]) / TMath::Sqr(NormConst.Val1);
477    }
478  }
479  void TQGraph::MakeGraph(const TLpExample& Example) {
480    TEdgeGrad GradInit(0, TFltV(Example.Attrs()));
481    Clr(false);
482    PNGraph G = Example.Graph;
483    for (TNGraph::TNodeI NI = G->BegNI(); NI < G->EndNI(); NI++) {
484      AddNode(NI.GetId(), lntUndef);
485    }
486    DummyNId = AddNode(-1, lntDummy); 
487    for (TNGraph::TEdgeI EI = G->BegEI(); EI < G->EndEI(); EI++) {
488      AddEdge(EI.GetSrcNId(), EI.GetDstNId(), GradInit);
489    }
490    for (TNGraph::TNodeI NI = G->BegNI(); NI < G->EndNI(); NI++) { 
491      AddEdge(NI.GetId(), DummyNId, GradInit);
492    }
493    AddEdge(DummyNId, Example.SrcNId, GradInit); 
494    GetNDat(Example.SrcNId) = lntSrc;
495    for (int i = 0; i < Example.DstNIdV.Len(); i++) {
496      GetNDat(Example.DstNIdV[i]) = lntDst; }
497    for (int i = 0; i < Example.NolNIdV.Len(); i++) {
498      GetNDat(Example.NolNIdV[i]) = lntNoLink; }
499  }
500  void TQGraph::UpdateGraph(const TLpExample& Example, const TLpPredictor& Pred, const TFltV& WgtV) {
501    const double Alpha = 0.15;
502    if (Empty()) { MakeGraph(Example); }
503    PNGraph G = Example.Graph;
504    TFltV GradV;
505    for (TNGraph::TEdgeI EI = G->BegEI(); EI < G->EndEI(); EI++) { 
506      const TIntPr Edge(EI.GetSrcNId(), EI.GetDstNId());
507      const double FVal = Pred.GetFVal(WgtV, Example.GetEAttrV(Edge));
508      Pred.GetFGrad(WgtV, Example.GetEAttrV(Edge), GradV);
509      EdgeGradH.AddDat(Edge, TEdgeGrad(FVal, GradV));
510    }
511    UpdateNormConst();
512    for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) { EI().Val1=-1; } 
513    for (TNodeI NI = BegNI(); NI < GetNI(DummyNId); NI++) {
514      for (int i = 0; i < NI.GetOutDeg()-1; i++) {
515        UpdateGrad(Alpha, TIntPr(NI.GetId(), NI.GetOutNId(i)), NI.GetOutEDat(i)); }
516      NI.GetOutEDat(NI.GetOutDeg()-1).Val1 = (1.0-Alpha);
517      NI.GetOutEDat(NI.GetOutDeg()-1).Val2.PutAll(0.0);
518    }
519    GetEDat(DummyNId, Example.SrcNId).Val1 = 1.0;
520    GetEDat(DummyNId, Example.SrcNId).Val2.PutAll(0.0);
521    for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) { IAssert(EI().Val1!=-1.0); } 
522  }
523  void TQGraph::CalcPRankGrad() {
524    const int NAttr = NormConstH[0].Val2.Len();
525    if (PRankH.Empty()) { 
526      for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
527        PRankH.AddDat(NI.GetId(), 0);
528        PrGradH.AddDat(NI.GetId(), TFltV(NAttr));
529      }
530    }
531    const double InitVal = 1.0/double(GetNodes());
532    for (int i = 0; i < PRankH.Len(); i++) {
533      PRankH[i] = InitVal;
534      PrGradH[i].PutAll(0);
535    }
536    THash<TInt, TFlt> NewPrH(PRankH);
537    THash<TInt, TFltV> NewGrH(PrGradH);
538    for (int T = 0; T < 2; T++) {
539      for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
540        double sum = 0;
541        for (int e = 0; e < NI.GetInDeg(); e++) {
542          sum += PRankH.GetDat(NI.GetInNId(e)) * NI.GetInEDat(e).Val1; }
543        NewPrH.AddDat(NI.GetId(), sum);
544      }
545      for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
546        TFltV& RankV = NewGrH.GetDat(NI.GetId());
547        RankV.PutAll(0);
548        for (int e = 0; e < NI.GetInDeg(); e++) {
549          const double Qne = NI.GetInEDat(e).Val1;
550          const TFltV& QGne = NI.GetInEDat(e).Val2;
551          const double PRe = PRankH.GetDat(NI.GetInNId(e));
552          const TFltV& GRe = PrGradH.GetDat(NI.GetInNId(e));
553          for (int i = 0; i < NAttr; i++) {
554            RankV[i] +=  Qne * GRe[i] + PRe * QGne[i];
555          }
556        }
557      }
558      PRankH = NewPrH;
559      PrGradH = NewGrH;
560      printf("\nITER %d\n", T+1);  Dump();
561    }
562  }
563  double TQGraph::GetGradient(const TLpExample& Example, const TLpPredictor& Prd, TFltV& GradV) const {
564    const double B = 0.1; 
565    const int NAttr = NormConstH[0].Val2.Len();
566    GradV.PutAll(0);
567    double Loss=0;
568    for (int l = 0; l < Example.NolNIdV.Len(); l++) {
569      const TFltV& LGrad = PrGradH.GetDat(Example.NolNIdV[l]);
570      const double LRank = PRankH.GetDat(Example.NolNIdV[l]);
571      for (int d = 0; d < Example.DstNIdV.Len(); d++) {
572        const TFltV& DGrad = PrGradH.GetDat(Example.DstNIdV[d]);
573        const double DRank = PRankH.GetDat(Example.DstNIdV[d]);
574        for (int i = 0; i < NAttr; i++) {
575          GradV[i] += Prd.GetLossGrad(LGrad[i]+B-DGrad[i])*(LGrad[i]-DGrad[i]);
576        }
577        Loss += Prd.GetLoss(LRank+B-DRank);
578      }
579    }
580    return Loss;
581  }
582  void TQGraph::Dump(const bool& OnlyPRank) const {
583    printf("PageRank:\n");
584    double S = 0;
585    for (int i = 0; i < PRankH.Len(); i++) {
586      TStr Str = "";
587      switch(TLpNodeTy(GetNDat(PRankH.GetKey(i)).Val)) {
588        case lntDst : Str="dst"; break;
589        case lntNoLink : Str="nol"; break;
590      }
591      printf("%d\t%f\t%s\n", PRankH.GetKey(i).Val, PRankH[i](), Str.CStr());
592      S+=PRankH[i];
593    }
594    printf("\t%f\n", S);
595    if (! OnlyPRank) {
596      printf("Gradient:\n");
597      for (int i = 0; i < PrGradH.Len(); i++) {
598        printf("%d", PrGradH.GetKey(i).Val);
599        for (int j = 0; j < PrGradH[i].Len(); j++) {
600          printf("\t%f", PrGradH[i][j].Val); }
601        printf("\n");
602      }
603    }
604    printf("Q-NET:\n");
605    for (TNodeI NI = BegNI(); NI < EndNI(); NI++) {
606      printf("%d:", NI.GetId());
607      for (int i = 0; i < NI.GetOutDeg(); i++) {
608        printf("  %d:%f", NI.GetOutNId(i), NI.GetOutEDat(i).Val1());
609      }
610      printf("\n");
611    }
612    for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) {
613      printf("%d -- %d  G:", EI.GetSrcNId(), EI.GetDstNId());
614      for (int i = 0; i < EI().Val2.Len(); i++) {
615        printf("\t%f", EI().Val2[i].Val); }
616      printf("\n");
617    }
618  }
<span onclick='openModal()' class='match'>619  void TLpBase::TestGrad() {
620    const double Lambda = 0.001;
621    LpData.GenSmallExample1();
622    WgtV.Gen(LpData.Attrs());
623    WgtV.PutAll(0);
624    TFltV GradV(LpData.Attrs());
625    for (int i = 0; i < 1; i++) {
</span>626      printf("\n----STEP %d\n", i+1);
627      QGraph.UpdateGraph(LpData, LpPred, WgtV);
628      QGraph.CalcPRankGrad();
629      return;
630      double Loss = QGraph.GetGradient(LpData, LpPred, GradV);
631      for (int i = 0; i < WgtV.Len(); i++) {
632        GradV[i] += Lambda*2*WgtV[i];
633        Loss += Lambda*TMath::Sqr(WgtV[i]);
634      }
635      printf("LOSS: %f\n", Loss);
636      printf("Grad:");
637      for (int i = 0; i < GradV.Len(); i++) { printf("\t%f", GradV[i].Val); } printf("\n");
638      printf("Wgt:");
639      for (int i = 0; i < GradV.Len(); i++) {
640        WgtV[i] += -2 * GradV[i];
641        printf("\t%f", WgtV[i].Val); } printf("\n");
642    }
643  }
644  }; 
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-base_conv_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-linkpred.cpp</div>
                </div>
                <div class="column column_space"><pre><code>32    } else {
33      const int num_kernel_dims = conv_param.kernel_size_size();
34      CHECK(num_kernel_dims == 1 || num_kernel_dims == num_spatial_axes_)
35          << "kernel_size must be specified once, or once per spatial dimension "
36          << "(kernel_size specified " << num_kernel_dims << " times; "
37          << num_spatial_axes_ << " spatial dims).";
38        for (int i = 0; i < num_spatial_axes_; ++i) {
</pre></code></div>
                <div class="column column_space"><pre><code>619  void TLpBase::TestGrad() {
620    const double Lambda = 0.001;
621    LpData.GenSmallExample1();
622    WgtV.Gen(LpData.Attrs());
623    WgtV.PutAll(0);
624    TFltV GradV(LpData.Attrs());
625    for (int i = 0; i < 1; i++) {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    