
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 20, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-sgd_solver.cpp</h3>
            <pre><code>1  #include <string>
2  #include <vector>
3  #include "caffe/sgd_solvers.hpp"
4  #include "caffe/util/hdf5.hpp"
5  #include "caffe/util/io.hpp"
6  #include "caffe/util/upgrade_proto.hpp"
7  namespace caffe {
8  #ifdef CAFFE_PER_LAYER_TIMINGS
9  #define LAYER_UPDATE_TIMING_START(index) do { \
10    if (this->net()->phase() == TRAIN) { \
11      this->net()->update_start_time_per_layer[index] = this->net()->timer.Duration(); \
12    } \
13  }while(0)
14  #define LAYER_UPDATE_TIMING_STOP(index) do { \
15    if (this->net()->phase() == TRAIN) { \
16      this->net()->update_stop_time_per_layer[index] = this->net()->timer.Duration(); \
17      this->net()->update_time_per_layer[index] += (this->net()->update_stop_time_per_layer[index] - this->net()->update_start_time_per_layer[index]); \
18    } \
19  }while(0)
20  #else
21  #define LAYER_UPDATE_TIMING_START(index)
22  #define LAYER_UPDATE_TIMING_STOP(index)
23  #endif
24  template <typename Dtype>
25  Dtype SGDSolver<Dtype>::GetWarmUpLR(int cur_iter, int warmup_iter, Dtype warmup_start_lr) {
26    if (cur_iter < 0) {
27      cur_iter = 0;
28    }
29    return (cur_iter * this->param_.base_lr() +
30            (warmup_iter - cur_iter) * warmup_start_lr) / warmup_iter;
31  }
32  template <typename Dtype>
33  Dtype SGDSolver<Dtype>::GetLearningRate() {
34    Dtype rate;
35    const string& lr_policy = this->param_.lr_policy();
36    if (this->param_.warmup_iter() > 0 &&
37        this->iter_ < this->param_.warmup_iter()) {
38      rate = GetWarmUpLR(this->iter_, this->param_.warmup_iter(),
39                         this->param_.warmup_start_lr());
40    } else if (lr_policy == "fixed") {
41      rate = this->param_.base_lr();
42    } else if (lr_policy == "step") {
43      this->current_step_ = this->iter_ / this->param_.stepsize();
44      rate = this->param_.base_lr() *
45          pow(this->param_.gamma(), this->current_step_);
46    } else if (lr_policy == "exp") {
47      rate = this->param_.base_lr() * pow(this->param_.gamma(), this->iter_);
48    } else if (lr_policy == "inv") {
49      rate = this->param_.base_lr() *
50          pow(Dtype(1) + this->param_.gamma() * this->iter_,
51              - this->param_.power());
52    } else if (lr_policy == "multistep") {
53      if (this->current_step_ < this->param_.stepvalue_size() &&
54            this->iter_ >= this->param_.stepvalue(this->current_step_)) {
55        this->current_step_++;
56        LOG(INFO) << "MultiStep Status: Iteration " <<
57        this->iter_ << ", step = " << this->current_step_;
58      }
59      rate = this->param_.base_lr() *
60          pow(this->param_.gamma(), this->current_step_);
61    } else if (lr_policy == "poly") {
62      rate = this->param_.base_lr() * pow(Dtype(1.) -
63          (Dtype(this->iter_) / Dtype(this->param_.max_iter())),
64          this->param_.power());
65    } else if (lr_policy == "sigmoid") {
66      rate = this->param_.base_lr() * (Dtype(1.) /
67          (Dtype(1.) + exp(-this->param_.gamma() * (Dtype(this->iter_) -
68            Dtype(this->param_.stepsize())))));
69    } else if (lr_policy == "plateau") {
70      if (this->smoothed_loss_ < this->minimum_loss_) {
71        this->minimum_loss_ = this->smoothed_loss_;
72        this->iter_last_event_ = this->iter_;
73      }
74      if (this->current_step_ < this->param_.plateau_winsize_size()) {
75        int iter_next_update = this->iter_last_event_
76              + this->param_.plateau_winsize(this->current_step_);
77        if (this->iter_ >= iter_next_update) {
78          this->current_step_++;
79          this->iter_last_event_ = this->iter_;
80          LOG(INFO) << "Plateau Status: Iteration " << this->iter_
81                    << ", step = " << this->current_step_;
82        }
83      }
84      if (this->param_.display() && this->iter_ % this->param_.display() == 0
85          && this->iter_last_event_ > (this->iter_ - this->param_.display())) {
86        LOG(INFO) << "Plateau Status: Iteration " << this->iter_
87                  << ", current minimum_loss = " << this->minimum_loss_;
88      }
89      rate = this->param_.base_lr() *
90          pow(this->param_.gamma(), this->current_step_);
91    } else if (lr_policy == "multifixed") {
92        CHECK_EQ(this->param_.stageiter_size(), this->param_.stagelr_size());
93        int num_stages = this->param_.stagelr_size();
94        int stage = 0;
95        for (; stage < num_stages; ++stage) {
96            if (this->iter_ <= this->param_.stageiter(stage))
97                break;
98        }
99        stage = (stage == num_stages) ? stage - 1 : stage;
100        rate = this->param_.stagelr(stage);
101    } else {
102      LOG(FATAL) << "Unknown learning rate policy: " << lr_policy;
103    }
104    return rate;
105  }
106  template <typename Dtype>
107  void SGDSolver<Dtype>::PreSolve() {
108    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
109    history_.clear();
110    update_.clear();
111    temp_.clear();
112    for (int i = 0; i < net_params.size(); ++i) {
113      const vector<int>& shape = net_params[i]->shape();
114      history_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
115      update_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
116      temp_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
117    }
118    this->minimum_loss_ = std::numeric_limits<float>::max();
119  }
120  template <typename Dtype>
121  void SGDSolver<Dtype>::ClipGradients() {
122    const Dtype clip_gradients = this->param_.clip_gradients();
123    if (clip_gradients < 0) { return; }
124    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
125    Dtype sumsq_diff = 0;
126    for (int i = 0; i < net_params.size(); ++i) {
127      sumsq_diff += net_params[i]->sumsq_diff();
128    }
129    const Dtype l2norm_diff = std::sqrt(sumsq_diff);
130    if (l2norm_diff > clip_gradients) {
131      Dtype scale_factor = clip_gradients / l2norm_diff;
132      LOG(INFO) << "Gradient clipping: scaling down gradients (L2 norm "
133          << l2norm_diff << " > " << clip_gradients << ") "
134          << "by scale factor " << scale_factor;
135      for (int i = 0; i < net_params.size(); ++i) {
136        net_params[i]->scale_diff(scale_factor);
137      }
138    }
139  }
140  template <typename Dtype>
141  void SGDSolver<Dtype>::PrintLearningRate() {
142    CHECK(Caffe::root_solver());
143    Dtype rate = GetLearningRate();
144    if (this->param_.display() && this->iter_ % this->param_.display() == 0) {
145      LOG(INFO) << "Iteration " << this->iter_ << ", lr = " << rate;
146    }
147  }
148  template <typename Dtype>
149  void SGDSolver<Dtype>::ApplyUpdate() {
150    PrintLearningRate();
151    ClipGradients();
152  #ifdef CAFFE_PER_LAYER_TIMINGS
153  #ifdef USE_MLSL
154    CHECK(mn::is_multinode() == false);
155  #endif
156    for (int i=0; i<this->net_->layers().size(); i++) {
157      const std::vector<int> param_ids = this->net_->get_layer_learnable_param_ids(i);
158      LAYER_UPDATE_TIMING_START(i);
159      for (int param_id = 0; param_id < param_ids.size(); ++param_id) {
160        ApplyUpdate(param_ids[param_id]);
161      }
162      LAYER_UPDATE_TIMING_STOP(i);
163    }
164  #else
165    for (int param_id = 0; param_id < this->net_->learnable_params().size();
166         ++param_id) {
167      ApplyUpdate(param_id);
168    }
169  #endif
170  }
171  template <typename Dtype>
172  void SGDSolver<Dtype>::ApplyUpdate(int param_id) {
173    CHECK(Caffe::root_solver());
174    Dtype rate = GetLearningRate();
175    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: raw delwt:");
176    if (this->net_->params_lr()[param_id] == 0) {
177      return;
178    }
179  #ifdef ENABLE_SGD_FUSION
180    if ((Caffe::mode() == Caffe::CPU) && (this->type() == string("SGD")))
181    {
182      SGDFusion(param_id, rate);
183      return;
184    }
185  #endif &bsol;* ENABLE_SGD_FUSION */
186    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
187    bool need_sync_data_back_to_prv = false;
188    bool need_sync_diff_back_to_prv = false;
189    if (net_params[param_id]->prv_data()
190        && (net_params[param_id]->prv_data_count()
191            != net_params[param_id]->count())) {
192      need_sync_data_back_to_prv = true;
193    }
194    if (net_params[param_id]->prv_diff()
195        && (net_params[param_id]->prv_diff_count()
196            != net_params[param_id]->count())) {
197      need_sync_diff_back_to_prv = true;
198    }
199    Normalize(param_id);
200    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Normalize:");
201    if (strcmp(this->type(), "SGD")) {
202      Regularize(param_id);
<span onclick='openModal()' class='match'>203      LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
204    }
</span>205    ComputeUpdateValue(param_id, rate);
206    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: wtinc:");
207    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight before update:");
208    this->net_->learnable_params()[param_id]->Update();
209    if (need_sync_diff_back_to_prv) {
210      net_params[param_id]->mutable_prv_diff();
211    }
212    if (need_sync_data_back_to_prv) {
213      net_params[param_id]->mutable_prv_data();
214    }
215    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight after update:");
216  }
217  #ifdef ENABLE_SGD_FUSION
218  template <typename Dtype>
219  void axpy_axpby_copy(size_t count, const Dtype decay, const Dtype* net_params_data, Dtype *net_params_diff,
220                       const Dtype rate, const Dtype momentum, Dtype* history_data);
221  template <>
222  void axpy_axpby_copy<float>(size_t count, const float decay, const float* net_params_data, float *net_params_diff,
223                              const float rate, const float momentum, float* history_data)
224  {
225  #ifdef _OPENMP
226  #pragma omp parallel for simd schedule(static)
227  #endif  
228    for (size_t i = 0; i < count; ++i) {
229      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
230      net_params_diff[i] = history_data[i];
231    }
232  }
233  template <>
234  void axpy_axpby_copy<double>(size_t count, const double decay, const double* net_params_data, double *net_params_diff,
235                               const double rate, const double momentum, double* history_data)
236  {
237  #ifdef _OPENMP
238  #pragma omp parallel for simd schedule(static)
239  #endif  
240    for (size_t i = 0; i < count; ++i) {
241      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
242      net_params_diff[i] = history_data[i];
243    }
244  }
245  template <typename Dtype>
246  void axpy_axpby_copy_axpy(size_t count, const Dtype decay, Dtype* net_params_data, Dtype *net_params_diff,
247                       const Dtype rate, const Dtype momentum, Dtype* history_data, const Dtype update_param);
248  template <>
249  void axpy_axpby_copy_axpy<float>(size_t count, const float decay, float* net_params_data, float *net_params_diff,
250                              const float rate, const float momentum, float* history_data, const float update_param)
251  {
252  #ifdef _OPENMP
253  #pragma omp parallel for simd schedule(static)
254  #endif  
255    for (size_t i = 0; i < count; ++i) {
256      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
257      net_params_data[i] = update_param * history_data[i] + net_params_data[i];
258    }
259  }
260  template <>
261  void axpy_axpby_copy_axpy<double>(size_t count, const double decay, double* net_params_data, double *net_params_diff,
262                               const double rate, const double momentum, double* history_data, const double update_param)
263  {
264  #ifdef _OPENMP
265  #pragma omp parallel for simd schedule(static)
266  #endif  
267    for (size_t i = 0; i < count; ++i) {
268      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
269      net_params_data[i] = update_param * history_data[i] + net_params_data[i];
270    }
271  }
272  template <typename Dtype>
273  void SGDSolver<Dtype>::SGDFusion(int param_id, Dtype rate) {
274    bool skip_Normalize_stage_flag = false;
275    if (this->param_.iter_size() == 1) { skip_Normalize_stage_flag = true; }
276    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
277    const vector<float>& net_params_weight_decay =
278      this->net_->params_weight_decay();
279    Dtype weight_decay = this->param_.weight_decay();
280    string regularization_type = this->param_.regularization_type();
281    Dtype local_decay = weight_decay * net_params_weight_decay[param_id];
282    Dtype momentum = this->param_.momentum();
283    bool prv_diff_condition_flag = false;
284    bool need_sync_data_back_to_prv = false;
285    bool need_sync_diff_back_to_prv = false;
286    if (net_params[param_id]->prv_diff()
287      && (net_params[param_id]->prv_diff_count()
288      == net_params[param_id]->count())) {
289        prv_diff_condition_flag = true;
290    }
291    if (net_params[param_id]->prv_diff()
292      && (net_params[param_id]->prv_diff_count()
293      != net_params[param_id]->count())) {
294        need_sync_diff_back_to_prv = true;
295    }
296    if (net_params[param_id]->prv_data()
297      && (net_params[param_id]->prv_data_count()
298      != net_params[param_id]->count())) {
299        need_sync_data_back_to_prv = true;
300    }
301    if (skip_Normalize_stage_flag == false)
302    {
303      const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();
304      if (prv_diff_condition_flag) {
305        caffe_scal(net_params[param_id]->prv_diff_count(), accum_normalization,
306          net_params[param_id]->mutable_prv_diff());
307      }
308      else {
309        caffe_scal(net_params[param_id]->count(), accum_normalization,
310          net_params[param_id]->mutable_cpu_diff());
311      }
312    }
313    Dtype local_rate = rate * GetLocalRate(param_id);
314    bool is_separate_ComputeUpdateValue_Update = true;
315    if (local_decay) {
316      if (regularization_type == "L2") {
317        if (net_params[param_id]->prv_data() && net_params[param_id]->prv_diff()
318          && (net_params[param_id]->prv_data_count()
319          == net_params[param_id]->count()) &&
320              net_params[param_id]->get_prv_data_descriptor()->layout_compare(
321              net_params[param_id]->get_prv_diff_descriptor())) {
322            if (prv_diff_condition_flag) {
323              axpy_axpby_copy_axpy(net_params[param_id]->prv_data_count(), local_decay,
324                                  net_params[param_id]->mutable_prv_data(), net_params[param_id]->mutable_prv_diff(),
325                                  local_rate, momentum, history_[param_id]->mutable_cpu_data(), Dtype(-1));
326              is_separate_ComputeUpdateValue_Update = false;
327            }
328        } else {
329          if (!prv_diff_condition_flag)
330          {
331            axpy_axpby_copy_axpy(net_params[param_id]->count(), local_decay,
332                                  net_params[param_id]->mutable_cpu_data(), net_params[param_id]->mutable_cpu_diff(),
333                                  local_rate, momentum, history_[param_id]->mutable_cpu_data(), Dtype(-1));
334            is_separate_ComputeUpdateValue_Update = false;
335          }
336        }
337      } else if (regularization_type == "L1") {
338        caffe_cpu_sign(net_params[param_id]->count(),
339                        net_params[param_id]->cpu_data(),
340                        temp_[param_id]->mutable_cpu_data());
341        axpy_axpby_copy(net_params[param_id]->count(), local_decay,
342                                  temp_[param_id]->cpu_data(), net_params[param_id]->mutable_cpu_diff(),
343                                  local_rate, momentum, history_[param_id]->mutable_cpu_data());
344        is_separate_ComputeUpdateValue_Update = false;
345        net_params[param_id]->Update();
346      } else {
347        LOG(FATAL) << "Unknown regularization type: " << regularization_type;
348      }
349    }
350    if (is_separate_ComputeUpdateValue_Update == true)
351    {
352      if (prv_diff_condition_flag) {
353        caffe_cpu_axpby(net_params[param_id]->prv_diff_count(), local_rate,
354                        net_params[param_id]->prv_diff(), momentum,
355                        history_[param_id]->mutable_cpu_data());
356        caffe_copy(net_params[param_id]->count(),
357                    history_[param_id]->cpu_data(),
358                    net_params[param_id]->mutable_prv_diff());
359      } else {
360        caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
361                        net_params[param_id]->cpu_diff(), momentum,
362                        history_[param_id]->mutable_cpu_data());
363        caffe_copy(net_params[param_id]->count(),
364                    history_[param_id]->cpu_data(),
365                    net_params[param_id]->mutable_cpu_diff());
366      }
367      net_params[param_id]->Update();
368    }
369    if (need_sync_data_back_to_prv) {
370      net_params[param_id]->mutable_prv_data();
371    }
372    if (need_sync_diff_back_to_prv) {
373      net_params[param_id]->mutable_prv_diff();
374    }
375  }
376  #endif &bsol;* ENABLE_SGD_FUSION */
377  template <typename Dtype>
378  void SGDSolver<Dtype>::Normalize(int param_id) {
379    if (this->param_.iter_size() == 1) { 
380      return;
381    }
382    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
383    const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();
384    switch (Caffe::mode()) {
385    case Caffe::CPU: {
386      if (net_params[param_id]->prv_diff()
387          && (net_params[param_id]->prv_diff_count()
388              == net_params[param_id]->count())) {
389          caffe_scal(net_params[param_id]->prv_diff_count(), accum_normalization,
390              net_params[param_id]->mutable_prv_diff());
391      }
392      else {
393          caffe_scal(net_params[param_id]->count(), accum_normalization,
394              net_params[param_id]->mutable_cpu_diff());
395      }
396      break;
397    }
398    case Caffe::GPU: {
399  #ifndef CPU_ONLY
400      caffe_gpu_scal(net_params[param_id]->count(), accum_normalization,
401          net_params[param_id]->mutable_gpu_diff());
402  #else
403      NO_GPU;
404  #endif
405      break;
406    }
407    default:
408      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
409    }
410  }
411  template <typename Dtype>
412  void SGDSolver<Dtype>::Regularize(int param_id) {
413    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
414    const vector<float>& net_params_weight_decay =
415        this->net_->params_weight_decay();
416    Dtype weight_decay = this->param_.weight_decay();
417    string regularization_type = this->param_.regularization_type();
418    Dtype local_decay = weight_decay * net_params_weight_decay[param_id];
419    switch (Caffe::mode()) {
420    case Caffe::CPU: {
421      if (local_decay) {
422        if (regularization_type == "L2") {
423          if (net_params[param_id]->prv_data() && net_params[param_id]->prv_diff()
424               && (net_params[param_id]->prv_data_count()
425                   == net_params[param_id]->count()) &&
426              net_params[param_id]->get_prv_data_descriptor()->layout_compare(
427              net_params[param_id]->get_prv_diff_descriptor())) {
428            caffe_axpy(net_params[param_id]->prv_data_count(),
429                       local_decay,
430                       net_params[param_id]->prv_data(),
431                       net_params[param_id]->mutable_prv_diff());
432          } else {
433            caffe_axpy(net_params[param_id]->count(),
434                local_decay,
435                net_params[param_id]->cpu_data(),
436                net_params[param_id]->mutable_cpu_diff());
437          }
438        } else if (regularization_type == "L1") {
439          caffe_cpu_sign(net_params[param_id]->count(),
440              net_params[param_id]->cpu_data(),
441              temp_[param_id]->mutable_cpu_data());
442          caffe_axpy(net_params[param_id]->count(),
443              local_decay,
444              temp_[param_id]->cpu_data(),
445              net_params[param_id]->mutable_cpu_diff());
446        } else {
447          LOG(FATAL) << "Unknown regularization type: " << regularization_type;
448        }
449      }
450      break;
451    }
452    case Caffe::GPU: {
453  #ifndef CPU_ONLY
454      if (local_decay) {
455        if (regularization_type == "L2") {
456          caffe_gpu_axpy(net_params[param_id]->count(),
457              local_decay,
458              net_params[param_id]->gpu_data(),
459              net_params[param_id]->mutable_gpu_diff());
460        } else if (regularization_type == "L1") {
461          caffe_gpu_sign(net_params[param_id]->count(),
462              net_params[param_id]->gpu_data(),
463              temp_[param_id]->mutable_gpu_data());
464          caffe_gpu_axpy(net_params[param_id]->count(),
465              local_decay,
466              temp_[param_id]->gpu_data(),
467              net_params[param_id]->mutable_gpu_diff());
468        } else {
469          LOG(FATAL) << "Unknown regularization type: " << regularization_type;
470        }
471      }
472  #else
473      NO_GPU;
474  #endif
475      break;
476    }
477    default:
478      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
479    }
480  }
481  #ifndef CPU_ONLY
482  template <typename Dtype>
483  void sgd_update_gpu(int N, Dtype* g, Dtype* h, Dtype momentum,
484      Dtype local_rate);
485  #endif
486  template <typename Dtype>
487  void SGDSolver<Dtype>::ComputeUpdateValue(int param_id, Dtype rate) {
488    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
489    Dtype momentum = this->param_.momentum();
490    Dtype local_rate = rate * GetLocalRate(param_id);
491    Regularize(param_id);
492    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
493    if (this->param_.warmup_iter() > 0 &&
494        this->iter_ < this->param_.warmup_iter()) {
495      Dtype prev_rate = GetWarmUpLR(this->iter_ - 1, this->param_.warmup_iter(),
496                                    this->param_.warmup_start_lr());
497      momentum = momentum * (rate / prev_rate);
498    }
499    switch (Caffe::mode()) {
500    case Caffe::CPU: {
501      if (net_params[param_id]->prv_diff()
502          && (net_params[param_id]->prv_diff_count()
503              == net_params[param_id]->count())) {
504        caffe_cpu_axpby(net_params[param_id]->prv_diff_count(), local_rate,
505                        net_params[param_id]->prv_diff(), momentum,
506                        history_[param_id]->mutable_cpu_data());
507        caffe_copy(net_params[param_id]->count(),
508                   history_[param_id]->cpu_data(),
509                   net_params[param_id]->mutable_prv_diff());
510      } else {
511        caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
512                       net_params[param_id]->cpu_diff(), momentum,
513                       history_[param_id]->mutable_cpu_data());
514        caffe_copy(net_params[param_id]->count(),
515                   history_[param_id]->cpu_data(),
516                   net_params[param_id]->mutable_cpu_diff());
517      }
518      break;
519    }
520    case Caffe::GPU: {
521  #ifndef CPU_ONLY
522      sgd_update_gpu(net_params[param_id]->count(),
523          net_params[param_id]->mutable_gpu_diff(),
524          history_[param_id]->mutable_gpu_data(),
525          momentum, local_rate);
526  #else
527      NO_GPU;
528  #endif
529      break;
530    }
531    default:
532      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
533    }
534  }
535  template <typename Dtype>
536  Dtype SGDSolver<Dtype>::GetLocalRate(int param_id) const {
537    const vector<float>& net_params_lr = this->net_->params_lr();
538    float local_lr = net_params_lr[param_id];
539    if (this->param_.local_lr_auto()) {
540      Blob<Dtype>* param = this->net_->learnable_params()[param_id];
541      const float w_norm = std::sqrt(param->sumsq_data());
542      const float wgrad_norm = std::sqrt(param->sumsq_diff());
543      const float gw_ratio = this->param_.local_gw_ratio();
544      float rate = 1.F;
545      float weight_decay = this->param_.weight_decay();
546      if (w_norm > 0.F && wgrad_norm > 0.F) {
547        rate = gw_ratio * w_norm / (wgrad_norm + weight_decay * w_norm);
548      }
549      if (local_lr > 0.F) {
550        local_lr = rate;
551      }
552  #ifdef DEBUG
553      if (Caffe::root_solver()
554          && this->param_.display()
555          && (this->iter_ % this->param_.display() == 0)) {
556        const int layer_id = this->net_->param_layer_indices(param_id).first;
557        const string& layer_name = this->net_->layer_names()[layer_id];
558        const int blob_id = this->net_->param_layer_indices(param_id).second;
559        LOG(INFO) << layer_name << "." << blob_id << " lr=" << local_lr
560          << ".\t  w=" << w_norm << "\t  dw=" << wgrad_norm;
561      }
562  #endif
563    }
564    return local_lr;
565  }
566  template <typename Dtype>
567  void SGDSolver<Dtype>::SnapshotSolverState(const string& model_filename) {
568    switch (this->param_.snapshot_format()) {
569      case caffe::SolverParameter_SnapshotFormat_BINARYPROTO:
570        SnapshotSolverStateToBinaryProto(model_filename);
571        break;
572      case caffe::SolverParameter_SnapshotFormat_HDF5:
573        SnapshotSolverStateToHDF5(model_filename);
574        break;
575      default:
576        LOG(FATAL) << "Unsupported snapshot format.";
577    }
578  }
579  template <typename Dtype>
580  void SGDSolver<Dtype>::SnapshotSolverStateToBinaryProto(
581      const string& model_filename) {
582  #ifdef USE_MLSL
583    if (mn::is_root()) {
584  #endif
585    SolverState state;
586    state.set_iter(this->iter_);
587    state.set_learned_net(model_filename);
588    state.set_current_step(this->current_step_);
589    state.set_iter_last_event(this->iter_last_event_);
590    state.set_minimum_loss(this->minimum_loss_);
591    state.clear_history();
592    for (int i = 0; i < history_.size(); ++i) {
593      BlobProto* history_blob = state.add_history();
594      history_[i]->ToProto(history_blob);
595    }
596    string snapshot_filename = Solver<Dtype>::SnapshotFilename(".solverstate");
597    LOG(INFO)
598      << "Snapshotting solver state to binary proto file " << snapshot_filename;
599    WriteProtoToBinaryFile(state, snapshot_filename.c_str());
600  #ifdef USE_MLSL
601    }
602  #endif
603  }
604  template <typename Dtype>
605  void SGDSolver<Dtype>::SnapshotSolverStateToHDF5(
606      const string& model_filename) {
607  #ifdef USE_MLSL
608    if (mn::is_root()) {
609  #endif
610    string snapshot_filename =
611        Solver<Dtype>::SnapshotFilename(".solverstate.h5");
612    LOG(INFO) << "Snapshotting solver state to HDF5 file " << snapshot_filename;
613    hid_t file_hid = H5Fcreate(snapshot_filename.c_str(), H5F_ACC_TRUNC,
614        H5P_DEFAULT, H5P_DEFAULT);
615    CHECK_GE(file_hid, 0)
616        << "Couldn't open " << snapshot_filename << " to save solver state.";
617    hdf5_save_int(file_hid, "iter", this->iter_);
618    hdf5_save_string(file_hid, "learned_net", model_filename);
619    hdf5_save_int(file_hid, "current_step", this->current_step_);
620    hdf5_save_int(file_hid, "iter_last_event", this->iter_last_event_);
621    hdf5_save_float<Dtype>(file_hid, "minimum_loss", this->minimum_loss_);
622    hid_t history_hid = H5Gcreate2(file_hid, "history", H5P_DEFAULT, H5P_DEFAULT,
623        H5P_DEFAULT);
624    CHECK_GE(history_hid, 0)
625        << "Error saving solver state to " << snapshot_filename << ".";
626    for (int i = 0; i < history_.size(); ++i) {
627      ostringstream oss;
628      oss << i;
629      hdf5_save_nd_dataset<Dtype>(history_hid, oss.str(), *history_[i]);
630    }
631    H5Gclose(history_hid);
632    H5Fclose(file_hid);
633  #ifdef USE_MLSL
634    }
635  #endif
636  }
637  template <typename Dtype>
638  void SGDSolver<Dtype>::RestoreSolverStateFromBinaryProto(
639      const string& state_file) {
640    SolverState state;
641    ReadProtoFromBinaryFile(state_file, &state);
642    this->iter_ = state.iter();
643    if (state.has_learned_net()) {
644      NetParameter net_param;
645      ReadNetParamsFromBinaryFileOrDie(state.learned_net().c_str(), &net_param);
646      this->net_->CopyTrainedLayersFrom(net_param);
647    }
648    this->current_step_ = state.current_step();
649    this->iter_last_event_ = state.iter_last_event();
650    this->minimum_loss_ = state.minimum_loss();
651    CHECK_EQ(state.history_size(), history_.size())
652        << "Incorrect length of history blobs.";
653    LOG(INFO) << "SGDSolver: restoring history";
654    for (int i = 0; i < history_.size(); ++i) {
655      history_[i]->FromProto(state.history(i));
656    }
657  }
658  template <typename Dtype>
659  void SGDSolver<Dtype>::RestoreSolverStateFromHDF5(const string& state_file) {
660    hid_t file_hid = H5Fopen(state_file.c_str(), H5F_ACC_RDONLY, H5P_DEFAULT);
661    CHECK_GE(file_hid, 0) << "Couldn't open solver state file " << state_file;
662    this->iter_ = hdf5_load_int(file_hid, "iter");
663    if (H5LTfind_dataset(file_hid, "learned_net")) {
664      string learned_net = hdf5_load_string(file_hid, "learned_net");
665      this->net_->CopyTrainedLayersFrom(learned_net);
666    }
667    this->current_step_ = hdf5_load_int(file_hid, "current_step");
668    this->iter_last_event_ = hdf5_load_int(file_hid, "iter_last_event");
669    this->minimum_loss_ = hdf5_load_float<Dtype>(file_hid, "minimum_loss");
670    hid_t history_hid = H5Gopen2(file_hid, "history", H5P_DEFAULT);
671    CHECK_GE(history_hid, 0) << "Error reading history from " << state_file;
672    int state_history_size = hdf5_get_num_links(history_hid);
673    CHECK_EQ(state_history_size, history_.size())
674        << "Incorrect length of history blobs.";
675    for (int i = 0; i < history_.size(); ++i) {
676      ostringstream oss;
677      oss << i;
678      hdf5_load_nd_dataset<Dtype>(history_hid, oss.str().c_str(), 0,
679                                  kMaxBlobAxes, history_[i].get());
680    }
681    H5Gclose(history_hid);
682    H5Fclose(file_hid);
683  }
684  INSTANTIATE_CLASS(SGDSolver);
685  REGISTER_SOLVER_CLASS(SGD);
686  }  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-sgd_solver.cpp</h3>
            <pre><code>1  #include <string>
2  #include <vector>
3  #include "caffe/sgd_solvers.hpp"
4  #include "caffe/util/hdf5.hpp"
5  #include "caffe/util/io.hpp"
6  #include "caffe/util/upgrade_proto.hpp"
7  namespace caffe {
8  #ifdef CAFFE_PER_LAYER_TIMINGS
9  #define LAYER_UPDATE_TIMING_START(index) do { \
10    if (this->net()->phase() == TRAIN) { \
11      this->net()->update_start_time_per_layer[index] = this->net()->timer.Duration(); \
12    } \
13  }while(0)
14  #define LAYER_UPDATE_TIMING_STOP(index) do { \
15    if (this->net()->phase() == TRAIN) { \
16      this->net()->update_stop_time_per_layer[index] = this->net()->timer.Duration(); \
17      this->net()->update_time_per_layer[index] += (this->net()->update_stop_time_per_layer[index] - this->net()->update_start_time_per_layer[index]); \
18    } \
19  }while(0)
20  #else
21  #define LAYER_UPDATE_TIMING_START(index)
22  #define LAYER_UPDATE_TIMING_STOP(index)
23  #endif
24  template <typename Dtype>
25  Dtype SGDSolver<Dtype>::GetWarmUpLR(int cur_iter, int warmup_iter, Dtype warmup_start_lr) {
26    if (cur_iter < 0) {
27      cur_iter = 0;
28    }
29    return (cur_iter * this->param_.base_lr() +
30            (warmup_iter - cur_iter) * warmup_start_lr) / warmup_iter;
31  }
32  template <typename Dtype>
33  Dtype SGDSolver<Dtype>::GetLearningRate() {
34    Dtype rate;
35    const string& lr_policy = this->param_.lr_policy();
36    if (this->param_.warmup_iter() > 0 &&
37        this->iter_ < this->param_.warmup_iter()) {
38      rate = GetWarmUpLR(this->iter_, this->param_.warmup_iter(),
39                         this->param_.warmup_start_lr());
40    } else if (lr_policy == "fixed") {
41      rate = this->param_.base_lr();
42    } else if (lr_policy == "step") {
43      this->current_step_ = this->iter_ / this->param_.stepsize();
44      rate = this->param_.base_lr() *
45          pow(this->param_.gamma(), this->current_step_);
46    } else if (lr_policy == "exp") {
47      rate = this->param_.base_lr() * pow(this->param_.gamma(), this->iter_);
48    } else if (lr_policy == "inv") {
49      rate = this->param_.base_lr() *
50          pow(Dtype(1) + this->param_.gamma() * this->iter_,
51              - this->param_.power());
52    } else if (lr_policy == "multistep") {
53      if (this->current_step_ < this->param_.stepvalue_size() &&
54            this->iter_ >= this->param_.stepvalue(this->current_step_)) {
55        this->current_step_++;
56        LOG(INFO) << "MultiStep Status: Iteration " <<
57        this->iter_ << ", step = " << this->current_step_;
58      }
59      rate = this->param_.base_lr() *
60          pow(this->param_.gamma(), this->current_step_);
61    } else if (lr_policy == "poly") {
62      rate = this->param_.base_lr() * pow(Dtype(1.) -
63          (Dtype(this->iter_) / Dtype(this->param_.max_iter())),
64          this->param_.power());
65    } else if (lr_policy == "sigmoid") {
66      rate = this->param_.base_lr() * (Dtype(1.) /
67          (Dtype(1.) + exp(-this->param_.gamma() * (Dtype(this->iter_) -
68            Dtype(this->param_.stepsize())))));
69    } else if (lr_policy == "plateau") {
70      if (this->smoothed_loss_ < this->minimum_loss_) {
71        this->minimum_loss_ = this->smoothed_loss_;
72        this->iter_last_event_ = this->iter_;
73      }
74      if (this->current_step_ < this->param_.plateau_winsize_size()) {
75        int iter_next_update = this->iter_last_event_
76              + this->param_.plateau_winsize(this->current_step_);
77        if (this->iter_ >= iter_next_update) {
78          this->current_step_++;
79          this->iter_last_event_ = this->iter_;
80          LOG(INFO) << "Plateau Status: Iteration " << this->iter_
81                    << ", step = " << this->current_step_;
82        }
83      }
84      if (this->param_.display() && this->iter_ % this->param_.display() == 0
85          && this->iter_last_event_ > (this->iter_ - this->param_.display())) {
86        LOG(INFO) << "Plateau Status: Iteration " << this->iter_
87                  << ", current minimum_loss = " << this->minimum_loss_;
88      }
89      rate = this->param_.base_lr() *
90          pow(this->param_.gamma(), this->current_step_);
91    } else if (lr_policy == "multifixed") {
92        CHECK_EQ(this->param_.stageiter_size(), this->param_.stagelr_size());
93        int num_stages = this->param_.stagelr_size();
94        int stage = 0;
95        for (; stage < num_stages; ++stage) {
96            if (this->iter_ <= this->param_.stageiter(stage))
97                break;
98        }
99        stage = (stage == num_stages) ? stage - 1 : stage;
100        rate = this->param_.stagelr(stage);
101    } else {
102      LOG(FATAL) << "Unknown learning rate policy: " << lr_policy;
103    }
104    return rate;
105  }
106  template <typename Dtype>
107  void SGDSolver<Dtype>::PreSolve() {
108    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
109    history_.clear();
110    update_.clear();
111    temp_.clear();
112    for (int i = 0; i < net_params.size(); ++i) {
113      const vector<int>& shape = net_params[i]->shape();
114      history_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
115      update_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
116      temp_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
117    }
118    this->minimum_loss_ = std::numeric_limits<float>::max();
119  }
120  template <typename Dtype>
121  void SGDSolver<Dtype>::ClipGradients() {
122    const Dtype clip_gradients = this->param_.clip_gradients();
123    if (clip_gradients < 0) { return; }
124    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
125    Dtype sumsq_diff = 0;
126    for (int i = 0; i < net_params.size(); ++i) {
127      sumsq_diff += net_params[i]->sumsq_diff();
128    }
129    const Dtype l2norm_diff = std::sqrt(sumsq_diff);
130    if (l2norm_diff > clip_gradients) {
131      Dtype scale_factor = clip_gradients / l2norm_diff;
132      LOG(INFO) << "Gradient clipping: scaling down gradients (L2 norm "
133          << l2norm_diff << " > " << clip_gradients << ") "
134          << "by scale factor " << scale_factor;
135      for (int i = 0; i < net_params.size(); ++i) {
136        net_params[i]->scale_diff(scale_factor);
137      }
138    }
139  }
140  template <typename Dtype>
141  void SGDSolver<Dtype>::PrintLearningRate() {
142    CHECK(Caffe::root_solver());
143    Dtype rate = GetLearningRate();
144    if (this->param_.display() && this->iter_ % this->param_.display() == 0) {
145      LOG(INFO) << "Iteration " << this->iter_ << ", lr = " << rate;
146    }
147  }
148  template <typename Dtype>
149  void SGDSolver<Dtype>::ApplyUpdate() {
150    PrintLearningRate();
151    ClipGradients();
152  #ifdef CAFFE_PER_LAYER_TIMINGS
153  #ifdef USE_MLSL
154    CHECK(mn::is_multinode() == false);
155  #endif
156    for (int i=0; i<this->net_->layers().size(); i++) {
157      const std::vector<int> param_ids = this->net_->get_layer_learnable_param_ids(i);
158      LAYER_UPDATE_TIMING_START(i);
159      for (int param_id = 0; param_id < param_ids.size(); ++param_id) {
160        ApplyUpdate(param_ids[param_id]);
161      }
162      LAYER_UPDATE_TIMING_STOP(i);
163    }
164  #else
165    for (int param_id = 0; param_id < this->net_->learnable_params().size();
166         ++param_id) {
167      ApplyUpdate(param_id);
168    }
169  #endif
170  }
171  template <typename Dtype>
172  void SGDSolver<Dtype>::ApplyUpdate(int param_id) {
173    CHECK(Caffe::root_solver());
174    Dtype rate = GetLearningRate();
175    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: raw delwt:");
176    if (this->net_->params_lr()[param_id] == 0) {
177      return;
178    }
179  #ifdef ENABLE_SGD_FUSION
180    if ((Caffe::mode() == Caffe::CPU) && (this->type() == string("SGD")))
181    {
182      SGDFusion(param_id, rate);
183      return;
184    }
185  #endif &bsol;* ENABLE_SGD_FUSION */
186    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
187    bool need_sync_data_back_to_prv = false;
188    bool need_sync_diff_back_to_prv = false;
189    if (net_params[param_id]->prv_data()
190        && (net_params[param_id]->prv_data_count()
191            != net_params[param_id]->count())) {
192      need_sync_data_back_to_prv = true;
193    }
194    if (net_params[param_id]->prv_diff()
195        && (net_params[param_id]->prv_diff_count()
196            != net_params[param_id]->count())) {
197      need_sync_diff_back_to_prv = true;
198    }
199    Normalize(param_id);
200    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Normalize:");
201    if (strcmp(this->type(), "SGD")) {
202      Regularize(param_id);
203      LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
204    }
205    ComputeUpdateValue(param_id, rate);
<span onclick='openModal()' class='match'>206    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: wtinc:");
207    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight before update:");
</span>208    this->net_->learnable_params()[param_id]->Update();
209    if (need_sync_diff_back_to_prv) {
210      net_params[param_id]->mutable_prv_diff();
211    }
212    if (need_sync_data_back_to_prv) {
213      net_params[param_id]->mutable_prv_data();
214    }
215    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight after update:");
216  }
217  #ifdef ENABLE_SGD_FUSION
218  template <typename Dtype>
219  void axpy_axpby_copy(size_t count, const Dtype decay, const Dtype* net_params_data, Dtype *net_params_diff,
220                       const Dtype rate, const Dtype momentum, Dtype* history_data);
221  template <>
222  void axpy_axpby_copy<float>(size_t count, const float decay, const float* net_params_data, float *net_params_diff,
223                              const float rate, const float momentum, float* history_data)
224  {
225  #ifdef _OPENMP
226  #pragma omp parallel for simd schedule(static)
227  #endif  
228    for (size_t i = 0; i < count; ++i) {
229      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
230      net_params_diff[i] = history_data[i];
231    }
232  }
233  template <>
234  void axpy_axpby_copy<double>(size_t count, const double decay, const double* net_params_data, double *net_params_diff,
235                               const double rate, const double momentum, double* history_data)
236  {
237  #ifdef _OPENMP
238  #pragma omp parallel for simd schedule(static)
239  #endif  
240    for (size_t i = 0; i < count; ++i) {
241      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
242      net_params_diff[i] = history_data[i];
243    }
244  }
245  template <typename Dtype>
246  void axpy_axpby_copy_axpy(size_t count, const Dtype decay, Dtype* net_params_data, Dtype *net_params_diff,
247                       const Dtype rate, const Dtype momentum, Dtype* history_data, const Dtype update_param);
248  template <>
249  void axpy_axpby_copy_axpy<float>(size_t count, const float decay, float* net_params_data, float *net_params_diff,
250                              const float rate, const float momentum, float* history_data, const float update_param)
251  {
252  #ifdef _OPENMP
253  #pragma omp parallel for simd schedule(static)
254  #endif  
255    for (size_t i = 0; i < count; ++i) {
256      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
257      net_params_data[i] = update_param * history_data[i] + net_params_data[i];
258    }
259  }
260  template <>
261  void axpy_axpby_copy_axpy<double>(size_t count, const double decay, double* net_params_data, double *net_params_diff,
262                               const double rate, const double momentum, double* history_data, const double update_param)
263  {
264  #ifdef _OPENMP
265  #pragma omp parallel for simd schedule(static)
266  #endif  
267    for (size_t i = 0; i < count; ++i) {
268      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
269      net_params_data[i] = update_param * history_data[i] + net_params_data[i];
270    }
271  }
272  template <typename Dtype>
273  void SGDSolver<Dtype>::SGDFusion(int param_id, Dtype rate) {
274    bool skip_Normalize_stage_flag = false;
275    if (this->param_.iter_size() == 1) { skip_Normalize_stage_flag = true; }
276    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
277    const vector<float>& net_params_weight_decay =
278      this->net_->params_weight_decay();
279    Dtype weight_decay = this->param_.weight_decay();
280    string regularization_type = this->param_.regularization_type();
281    Dtype local_decay = weight_decay * net_params_weight_decay[param_id];
282    Dtype momentum = this->param_.momentum();
283    bool prv_diff_condition_flag = false;
284    bool need_sync_data_back_to_prv = false;
285    bool need_sync_diff_back_to_prv = false;
286    if (net_params[param_id]->prv_diff()
287      && (net_params[param_id]->prv_diff_count()
288      == net_params[param_id]->count())) {
289        prv_diff_condition_flag = true;
290    }
291    if (net_params[param_id]->prv_diff()
292      && (net_params[param_id]->prv_diff_count()
293      != net_params[param_id]->count())) {
294        need_sync_diff_back_to_prv = true;
295    }
296    if (net_params[param_id]->prv_data()
297      && (net_params[param_id]->prv_data_count()
298      != net_params[param_id]->count())) {
299        need_sync_data_back_to_prv = true;
300    }
301    if (skip_Normalize_stage_flag == false)
302    {
303      const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();
304      if (prv_diff_condition_flag) {
305        caffe_scal(net_params[param_id]->prv_diff_count(), accum_normalization,
306          net_params[param_id]->mutable_prv_diff());
307      }
308      else {
309        caffe_scal(net_params[param_id]->count(), accum_normalization,
310          net_params[param_id]->mutable_cpu_diff());
311      }
312    }
313    Dtype local_rate = rate * GetLocalRate(param_id);
314    bool is_separate_ComputeUpdateValue_Update = true;
315    if (local_decay) {
316      if (regularization_type == "L2") {
317        if (net_params[param_id]->prv_data() && net_params[param_id]->prv_diff()
318          && (net_params[param_id]->prv_data_count()
319          == net_params[param_id]->count()) &&
320              net_params[param_id]->get_prv_data_descriptor()->layout_compare(
321              net_params[param_id]->get_prv_diff_descriptor())) {
322            if (prv_diff_condition_flag) {
323              axpy_axpby_copy_axpy(net_params[param_id]->prv_data_count(), local_decay,
324                                  net_params[param_id]->mutable_prv_data(), net_params[param_id]->mutable_prv_diff(),
325                                  local_rate, momentum, history_[param_id]->mutable_cpu_data(), Dtype(-1));
326              is_separate_ComputeUpdateValue_Update = false;
327            }
328        } else {
329          if (!prv_diff_condition_flag)
330          {
331            axpy_axpby_copy_axpy(net_params[param_id]->count(), local_decay,
332                                  net_params[param_id]->mutable_cpu_data(), net_params[param_id]->mutable_cpu_diff(),
333                                  local_rate, momentum, history_[param_id]->mutable_cpu_data(), Dtype(-1));
334            is_separate_ComputeUpdateValue_Update = false;
335          }
336        }
337      } else if (regularization_type == "L1") {
338        caffe_cpu_sign(net_params[param_id]->count(),
339                        net_params[param_id]->cpu_data(),
340                        temp_[param_id]->mutable_cpu_data());
341        axpy_axpby_copy(net_params[param_id]->count(), local_decay,
342                                  temp_[param_id]->cpu_data(), net_params[param_id]->mutable_cpu_diff(),
343                                  local_rate, momentum, history_[param_id]->mutable_cpu_data());
344        is_separate_ComputeUpdateValue_Update = false;
345        net_params[param_id]->Update();
346      } else {
347        LOG(FATAL) << "Unknown regularization type: " << regularization_type;
348      }
349    }
350    if (is_separate_ComputeUpdateValue_Update == true)
351    {
352      if (prv_diff_condition_flag) {
353        caffe_cpu_axpby(net_params[param_id]->prv_diff_count(), local_rate,
354                        net_params[param_id]->prv_diff(), momentum,
355                        history_[param_id]->mutable_cpu_data());
356        caffe_copy(net_params[param_id]->count(),
357                    history_[param_id]->cpu_data(),
358                    net_params[param_id]->mutable_prv_diff());
359      } else {
360        caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
361                        net_params[param_id]->cpu_diff(), momentum,
362                        history_[param_id]->mutable_cpu_data());
363        caffe_copy(net_params[param_id]->count(),
364                    history_[param_id]->cpu_data(),
365                    net_params[param_id]->mutable_cpu_diff());
366      }
367      net_params[param_id]->Update();
368    }
369    if (need_sync_data_back_to_prv) {
370      net_params[param_id]->mutable_prv_data();
371    }
372    if (need_sync_diff_back_to_prv) {
373      net_params[param_id]->mutable_prv_diff();
374    }
375  }
376  #endif &bsol;* ENABLE_SGD_FUSION */
377  template <typename Dtype>
378  void SGDSolver<Dtype>::Normalize(int param_id) {
379    if (this->param_.iter_size() == 1) { 
380      return;
381    }
382    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
383    const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();
384    switch (Caffe::mode()) {
385    case Caffe::CPU: {
386      if (net_params[param_id]->prv_diff()
387          && (net_params[param_id]->prv_diff_count()
388              == net_params[param_id]->count())) {
389          caffe_scal(net_params[param_id]->prv_diff_count(), accum_normalization,
390              net_params[param_id]->mutable_prv_diff());
391      }
392      else {
393          caffe_scal(net_params[param_id]->count(), accum_normalization,
394              net_params[param_id]->mutable_cpu_diff());
395      }
396      break;
397    }
398    case Caffe::GPU: {
399  #ifndef CPU_ONLY
400      caffe_gpu_scal(net_params[param_id]->count(), accum_normalization,
401          net_params[param_id]->mutable_gpu_diff());
402  #else
403      NO_GPU;
404  #endif
405      break;
406    }
407    default:
408      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
409    }
410  }
411  template <typename Dtype>
412  void SGDSolver<Dtype>::Regularize(int param_id) {
413    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
414    const vector<float>& net_params_weight_decay =
415        this->net_->params_weight_decay();
416    Dtype weight_decay = this->param_.weight_decay();
417    string regularization_type = this->param_.regularization_type();
418    Dtype local_decay = weight_decay * net_params_weight_decay[param_id];
419    switch (Caffe::mode()) {
420    case Caffe::CPU: {
421      if (local_decay) {
422        if (regularization_type == "L2") {
423          if (net_params[param_id]->prv_data() && net_params[param_id]->prv_diff()
424               && (net_params[param_id]->prv_data_count()
425                   == net_params[param_id]->count()) &&
426              net_params[param_id]->get_prv_data_descriptor()->layout_compare(
427              net_params[param_id]->get_prv_diff_descriptor())) {
428            caffe_axpy(net_params[param_id]->prv_data_count(),
429                       local_decay,
430                       net_params[param_id]->prv_data(),
431                       net_params[param_id]->mutable_prv_diff());
432          } else {
433            caffe_axpy(net_params[param_id]->count(),
434                local_decay,
435                net_params[param_id]->cpu_data(),
436                net_params[param_id]->mutable_cpu_diff());
437          }
438        } else if (regularization_type == "L1") {
439          caffe_cpu_sign(net_params[param_id]->count(),
440              net_params[param_id]->cpu_data(),
441              temp_[param_id]->mutable_cpu_data());
442          caffe_axpy(net_params[param_id]->count(),
443              local_decay,
444              temp_[param_id]->cpu_data(),
445              net_params[param_id]->mutable_cpu_diff());
446        } else {
447          LOG(FATAL) << "Unknown regularization type: " << regularization_type;
448        }
449      }
450      break;
451    }
452    case Caffe::GPU: {
453  #ifndef CPU_ONLY
454      if (local_decay) {
455        if (regularization_type == "L2") {
456          caffe_gpu_axpy(net_params[param_id]->count(),
457              local_decay,
458              net_params[param_id]->gpu_data(),
459              net_params[param_id]->mutable_gpu_diff());
460        } else if (regularization_type == "L1") {
461          caffe_gpu_sign(net_params[param_id]->count(),
462              net_params[param_id]->gpu_data(),
463              temp_[param_id]->mutable_gpu_data());
464          caffe_gpu_axpy(net_params[param_id]->count(),
465              local_decay,
466              temp_[param_id]->gpu_data(),
467              net_params[param_id]->mutable_gpu_diff());
468        } else {
469          LOG(FATAL) << "Unknown regularization type: " << regularization_type;
470        }
471      }
472  #else
473      NO_GPU;
474  #endif
475      break;
476    }
477    default:
478      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
479    }
480  }
481  #ifndef CPU_ONLY
482  template <typename Dtype>
483  void sgd_update_gpu(int N, Dtype* g, Dtype* h, Dtype momentum,
484      Dtype local_rate);
485  #endif
486  template <typename Dtype>
487  void SGDSolver<Dtype>::ComputeUpdateValue(int param_id, Dtype rate) {
488    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
489    Dtype momentum = this->param_.momentum();
490    Dtype local_rate = rate * GetLocalRate(param_id);
491    Regularize(param_id);
492    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
493    if (this->param_.warmup_iter() > 0 &&
494        this->iter_ < this->param_.warmup_iter()) {
495      Dtype prev_rate = GetWarmUpLR(this->iter_ - 1, this->param_.warmup_iter(),
496                                    this->param_.warmup_start_lr());
497      momentum = momentum * (rate / prev_rate);
498    }
499    switch (Caffe::mode()) {
500    case Caffe::CPU: {
501      if (net_params[param_id]->prv_diff()
502          && (net_params[param_id]->prv_diff_count()
503              == net_params[param_id]->count())) {
504        caffe_cpu_axpby(net_params[param_id]->prv_diff_count(), local_rate,
505                        net_params[param_id]->prv_diff(), momentum,
506                        history_[param_id]->mutable_cpu_data());
507        caffe_copy(net_params[param_id]->count(),
508                   history_[param_id]->cpu_data(),
509                   net_params[param_id]->mutable_prv_diff());
510      } else {
511        caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
512                       net_params[param_id]->cpu_diff(), momentum,
513                       history_[param_id]->mutable_cpu_data());
514        caffe_copy(net_params[param_id]->count(),
515                   history_[param_id]->cpu_data(),
516                   net_params[param_id]->mutable_cpu_diff());
517      }
518      break;
519    }
520    case Caffe::GPU: {
521  #ifndef CPU_ONLY
522      sgd_update_gpu(net_params[param_id]->count(),
523          net_params[param_id]->mutable_gpu_diff(),
524          history_[param_id]->mutable_gpu_data(),
525          momentum, local_rate);
526  #else
527      NO_GPU;
528  #endif
529      break;
530    }
531    default:
532      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
533    }
534  }
535  template <typename Dtype>
536  Dtype SGDSolver<Dtype>::GetLocalRate(int param_id) const {
537    const vector<float>& net_params_lr = this->net_->params_lr();
538    float local_lr = net_params_lr[param_id];
539    if (this->param_.local_lr_auto()) {
540      Blob<Dtype>* param = this->net_->learnable_params()[param_id];
541      const float w_norm = std::sqrt(param->sumsq_data());
542      const float wgrad_norm = std::sqrt(param->sumsq_diff());
543      const float gw_ratio = this->param_.local_gw_ratio();
544      float rate = 1.F;
545      float weight_decay = this->param_.weight_decay();
546      if (w_norm > 0.F && wgrad_norm > 0.F) {
547        rate = gw_ratio * w_norm / (wgrad_norm + weight_decay * w_norm);
548      }
549      if (local_lr > 0.F) {
550        local_lr = rate;
551      }
552  #ifdef DEBUG
553      if (Caffe::root_solver()
554          && this->param_.display()
555          && (this->iter_ % this->param_.display() == 0)) {
556        const int layer_id = this->net_->param_layer_indices(param_id).first;
557        const string& layer_name = this->net_->layer_names()[layer_id];
558        const int blob_id = this->net_->param_layer_indices(param_id).second;
559        LOG(INFO) << layer_name << "." << blob_id << " lr=" << local_lr
560          << ".\t  w=" << w_norm << "\t  dw=" << wgrad_norm;
561      }
562  #endif
563    }
564    return local_lr;
565  }
566  template <typename Dtype>
567  void SGDSolver<Dtype>::SnapshotSolverState(const string& model_filename) {
568    switch (this->param_.snapshot_format()) {
569      case caffe::SolverParameter_SnapshotFormat_BINARYPROTO:
570        SnapshotSolverStateToBinaryProto(model_filename);
571        break;
572      case caffe::SolverParameter_SnapshotFormat_HDF5:
573        SnapshotSolverStateToHDF5(model_filename);
574        break;
575      default:
576        LOG(FATAL) << "Unsupported snapshot format.";
577    }
578  }
579  template <typename Dtype>
580  void SGDSolver<Dtype>::SnapshotSolverStateToBinaryProto(
581      const string& model_filename) {
582  #ifdef USE_MLSL
583    if (mn::is_root()) {
584  #endif
585    SolverState state;
586    state.set_iter(this->iter_);
587    state.set_learned_net(model_filename);
588    state.set_current_step(this->current_step_);
589    state.set_iter_last_event(this->iter_last_event_);
590    state.set_minimum_loss(this->minimum_loss_);
591    state.clear_history();
592    for (int i = 0; i < history_.size(); ++i) {
593      BlobProto* history_blob = state.add_history();
594      history_[i]->ToProto(history_blob);
595    }
596    string snapshot_filename = Solver<Dtype>::SnapshotFilename(".solverstate");
597    LOG(INFO)
598      << "Snapshotting solver state to binary proto file " << snapshot_filename;
599    WriteProtoToBinaryFile(state, snapshot_filename.c_str());
600  #ifdef USE_MLSL
601    }
602  #endif
603  }
604  template <typename Dtype>
605  void SGDSolver<Dtype>::SnapshotSolverStateToHDF5(
606      const string& model_filename) {
607  #ifdef USE_MLSL
608    if (mn::is_root()) {
609  #endif
610    string snapshot_filename =
611        Solver<Dtype>::SnapshotFilename(".solverstate.h5");
612    LOG(INFO) << "Snapshotting solver state to HDF5 file " << snapshot_filename;
613    hid_t file_hid = H5Fcreate(snapshot_filename.c_str(), H5F_ACC_TRUNC,
614        H5P_DEFAULT, H5P_DEFAULT);
615    CHECK_GE(file_hid, 0)
616        << "Couldn't open " << snapshot_filename << " to save solver state.";
617    hdf5_save_int(file_hid, "iter", this->iter_);
618    hdf5_save_string(file_hid, "learned_net", model_filename);
619    hdf5_save_int(file_hid, "current_step", this->current_step_);
620    hdf5_save_int(file_hid, "iter_last_event", this->iter_last_event_);
621    hdf5_save_float<Dtype>(file_hid, "minimum_loss", this->minimum_loss_);
622    hid_t history_hid = H5Gcreate2(file_hid, "history", H5P_DEFAULT, H5P_DEFAULT,
623        H5P_DEFAULT);
624    CHECK_GE(history_hid, 0)
625        << "Error saving solver state to " << snapshot_filename << ".";
626    for (int i = 0; i < history_.size(); ++i) {
627      ostringstream oss;
628      oss << i;
629      hdf5_save_nd_dataset<Dtype>(history_hid, oss.str(), *history_[i]);
630    }
631    H5Gclose(history_hid);
632    H5Fclose(file_hid);
633  #ifdef USE_MLSL
634    }
635  #endif
636  }
637  template <typename Dtype>
638  void SGDSolver<Dtype>::RestoreSolverStateFromBinaryProto(
639      const string& state_file) {
640    SolverState state;
641    ReadProtoFromBinaryFile(state_file, &state);
642    this->iter_ = state.iter();
643    if (state.has_learned_net()) {
644      NetParameter net_param;
645      ReadNetParamsFromBinaryFileOrDie(state.learned_net().c_str(), &net_param);
646      this->net_->CopyTrainedLayersFrom(net_param);
647    }
648    this->current_step_ = state.current_step();
649    this->iter_last_event_ = state.iter_last_event();
650    this->minimum_loss_ = state.minimum_loss();
651    CHECK_EQ(state.history_size(), history_.size())
652        << "Incorrect length of history blobs.";
653    LOG(INFO) << "SGDSolver: restoring history";
654    for (int i = 0; i < history_.size(); ++i) {
655      history_[i]->FromProto(state.history(i));
656    }
657  }
658  template <typename Dtype>
659  void SGDSolver<Dtype>::RestoreSolverStateFromHDF5(const string& state_file) {
660    hid_t file_hid = H5Fopen(state_file.c_str(), H5F_ACC_RDONLY, H5P_DEFAULT);
661    CHECK_GE(file_hid, 0) << "Couldn't open solver state file " << state_file;
662    this->iter_ = hdf5_load_int(file_hid, "iter");
663    if (H5LTfind_dataset(file_hid, "learned_net")) {
664      string learned_net = hdf5_load_string(file_hid, "learned_net");
665      this->net_->CopyTrainedLayersFrom(learned_net);
666    }
667    this->current_step_ = hdf5_load_int(file_hid, "current_step");
668    this->iter_last_event_ = hdf5_load_int(file_hid, "iter_last_event");
669    this->minimum_loss_ = hdf5_load_float<Dtype>(file_hid, "minimum_loss");
670    hid_t history_hid = H5Gopen2(file_hid, "history", H5P_DEFAULT);
671    CHECK_GE(history_hid, 0) << "Error reading history from " << state_file;
672    int state_history_size = hdf5_get_num_links(history_hid);
673    CHECK_EQ(state_history_size, history_.size())
674        << "Incorrect length of history blobs.";
675    for (int i = 0; i < history_.size(); ++i) {
676      ostringstream oss;
677      oss << i;
678      hdf5_load_nd_dataset<Dtype>(history_hid, oss.str().c_str(), 0,
679                                  kMaxBlobAxes, history_[i].get());
680    }
681    H5Gclose(history_hid);
682    H5Fclose(file_hid);
683  }
684  INSTANTIATE_CLASS(SGDSolver);
685  REGISTER_SOLVER_CLASS(SGD);
686  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-sgd_solver.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-sgd_solver.cpp</div>
                </div>
                <div class="column column_space"><pre><code>203      LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
204    }
</pre></code></div>
                <div class="column column_space"><pre><code>206    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: wtinc:");
207    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight before update:");
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    