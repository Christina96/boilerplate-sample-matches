<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for softmax_loss_layer.cpp &amp; test_data_layer.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for softmax_loss_layer.cpp &amp; test_data_layer.cpp
      </h3>
<h1 align="center">
        9.0%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>softmax_loss_layer.cpp (15.254237%)<th>test_data_layer.cpp (6.382979%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(98-100)<td><a href="#" name="0">(187-190)</a><td align="center"><font color="#ff0000">14</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(130-133)<td><a href="#" name="1">(231-234)</a><td align="center"><font color="#ec0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>softmax_loss_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;algorithm&gt;
2 #include &lt;cfloat&gt;
3 #include &lt;vector&gt;
4 #include "caffe/layers/softmax_loss_layer.hpp"
5 #include "caffe/util/math_functions.hpp"
6 namespace caffe {
7 template &lt;typename Dtype&gt;
8 void SoftmaxWithLossLayer&lt;Dtype&gt;::LayerSetUp(
9     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
10   LossLayer&lt;Dtype&gt;::LayerSetUp(bottom, top);
11   LayerParameter softmax_param(this-&gt;layer_param_);
12   softmax_param.set_type("Softmax");
13   softmax_layer_ = LayerRegistry&lt;Dtype&gt;::CreateLayer(softmax_param);
14   softmax_bottom_vec_.clear();
15   softmax_bottom_vec_.push_back(bottom[0]);
16   softmax_top_vec_.clear();
17   softmax_top_vec_.push_back(&amp;prob_);
18   softmax_layer_-&gt;SetUp(softmax_bottom_vec_, softmax_top_vec_);
19   has_ignore_label_ =
20     this-&gt;layer_param_.loss_param().has_ignore_label();
21   if (has_ignore_label_) {
22     ignore_label_ = this-&gt;layer_param_.loss_param().ignore_label();
23   }
24   if (!this-&gt;layer_param_.loss_param().has_normalization() &amp;&amp;
25       this-&gt;layer_param_.loss_param().has_normalize()) {
26     normalization_ = this-&gt;layer_param_.loss_param().normalize() ?
27                      LossParameter_NormalizationMode_VALID :
28                      LossParameter_NormalizationMode_BATCH_SIZE;
29   } else {
30     normalization_ = this-&gt;layer_param_.loss_param().normalization();
31   }
32 }
33 template &lt;typename Dtype&gt;
34 void SoftmaxWithLossLayer&lt;Dtype&gt;::Reshape(
35     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
36   LossLayer&lt;Dtype&gt;::Reshape(bottom, top);
37   softmax_layer_-&gt;Reshape(softmax_bottom_vec_, softmax_top_vec_);
38   softmax_axis_ =
39       bottom[0]-&gt;CanonicalAxisIndex(this-&gt;layer_param_.softmax_param().axis());
40   outer_num_ = bottom[0]-&gt;count(0, softmax_axis_);
41   inner_num_ = bottom[0]-&gt;count(softmax_axis_ + 1);
42   CHECK_EQ(outer_num_ * inner_num_, bottom[1]-&gt;count())
43       &lt;&lt; "Number of labels must match number of predictions; "
44       &lt;&lt; "e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
45       &lt;&lt; "label count (number of labels) must be N*H*W, "
46       &lt;&lt; "with integer values in {0, 1, ..., C-1}.";
47   if (top.size() &gt;= 2) {
48     top[1]-&gt;ReshapeLike(*bottom[0]);
49   }
50 }
51 template &lt;typename Dtype&gt;
52 Dtype SoftmaxWithLossLayer&lt;Dtype&gt;::get_normalizer(
53     LossParameter_NormalizationMode normalization_mode, int valid_count) {
54   Dtype normalizer;
55   switch (normalization_mode) {
56     case LossParameter_NormalizationMode_FULL:
57       normalizer = Dtype(outer_num_ * inner_num_);
58       break;
59     case LossParameter_NormalizationMode_VALID:
60       if (valid_count == -1) {
61         normalizer = Dtype(outer_num_ * inner_num_);
62       } else {
63         normalizer = Dtype(valid_count);
64       }
65       break;
66     case LossParameter_NormalizationMode_BATCH_SIZE:
67       normalizer = Dtype(outer_num_);
68       break;
69     case LossParameter_NormalizationMode_NONE:
70       normalizer = Dtype(1);
71       break;
72     default:
73       LOG(FATAL) &lt;&lt; "Unknown normalization mode: "
74           &lt;&lt; LossParameter_NormalizationMode_Name(normalization_mode);
75   }
76   return std::max(Dtype(1.0), normalizer);
77 }
78 template &lt;typename Dtype&gt;
79 void SoftmaxWithLossLayer&lt;Dtype&gt;::Forward_cpu(
80     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
81   softmax_layer_-&gt;Forward(softmax_bottom_vec_, softmax_top_vec_);
82   const Dtype* prob_data = prob_.cpu_data();
83   const Dtype* label = bottom[1]-&gt;cpu_data();
84 <a name="0"></a>  int dim = prob_.count() / outer_num_;
85   int count = 0;
86   Dtype loss = 0;
87 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  for (int i = 0; i &lt; outer_num_; ++i) {
88     for (int j = 0; j &lt; inner_num_; j++) {
89       const int label_value = static_cast&lt;int&gt;(label[i * inner_num_ + j]);</b></font>
90       if (has_ignore_label_ &amp;&amp; label_value == ignore_label_) {
91         continue;
92       }
93       DCHECK_GE(label_value, 0);
94       DCHECK_LT(label_value, prob_.shape(softmax_axis_));
95       loss -= log(std::max(prob_data[i * dim + label_value * inner_num_ + j],
96                            Dtype(FLT_MIN)));
97       ++count;
98     }
99   }
100   top[0]-&gt;mutable_cpu_data()[0] = loss / get_normalizer(normalization_, count);
101   if (top.size() == 2) {
102     top[1]-&gt;ShareData(prob_);
103   }
104 }
105 template &lt;typename Dtype&gt;
106 void SoftmaxWithLossLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
107     const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
108   if (propagate_down[1]) {
109     LOG(FATAL) &lt;&lt; this-&gt;type()
110                &lt;&lt; " Layer cannot backpropagate to label inputs.";
111   }
112   if (propagate_down[0]) {
113     Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
114     const Dtype* prob_data = prob_.cpu_data();
115 <a name="1"></a>    caffe_copy(prob_.count(), prob_data, bottom_diff);
116     const Dtype* label = bottom[1]-&gt;cpu_data();
117     int dim = prob_.count() / outer_num_;
118 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>    int count = 0;
119     for (int i = 0; i &lt; outer_num_; ++i) {
120       for (int j = 0; j &lt; inner_num_; ++j) {
121         const int label_value = static_cast&lt;int&gt;(label[i * inner_num_ + j]);</b></font>
122         if (has_ignore_label_ &amp;&amp; label_value == ignore_label_) {
123           for (int c = 0; c &lt; bottom[0]-&gt;shape(softmax_axis_); ++c) {
124             bottom_diff[i * dim + c * inner_num_ + j] = 0;
125           }
126         } else {
127           bottom_diff[i * dim + label_value * inner_num_ + j] -= 1;
128           ++count;
129         }
130       }
131     }
132     Dtype loss_weight = top[0]-&gt;cpu_diff()[0] /
133                         get_normalizer(normalization_, count);
134     caffe_scal(prob_.count(), loss_weight, bottom_diff);
135   }
136 }
137 #ifdef CPU_ONLY
138 STUB_GPU(SoftmaxWithLossLayer);
139 #endif
140 INSTANTIATE_CLASS(SoftmaxWithLossLayer);
141 REGISTER_LAYER_CLASS(SoftmaxWithLoss);
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_data_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #ifdef USE_OPENCV
2 #include &lt;string&gt;
3 #include &lt;vector&gt;
4 #include "boost/scoped_ptr.hpp"
5 #include "gtest/gtest.h"
6 #include "caffe/blob.hpp"
7 #include "caffe/common.hpp"
8 #include "caffe/filler.hpp"
9 #include "caffe/layers/data_layer.hpp"
10 #include "caffe/proto/caffe.pb.h"
11 #include "caffe/util/db.hpp"
12 #include "caffe/util/io.hpp"
13 #include "caffe/test/test_caffe_main.hpp"
14 namespace caffe {
15 using boost::scoped_ptr;
16 template &lt;typename TypeParam&gt;
17 class DataLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
18   typedef typename TypeParam::Dtype Dtype;
19  protected:
20   DataLayerTest()
21       : backend_(DataParameter_DB_LEVELDB),
22         blob_top_data_(new Blob&lt;Dtype&gt;()),
23         blob_top_label_(new Blob&lt;Dtype&gt;()),
24         seed_(1701) {}
25   virtual void SetUp() {
26     filename_.reset(new string());
27     MakeTempDir(filename_.get());
28     *filename_ += "/db";
29     blob_top_vec_.push_back(blob_top_data_);
30     blob_top_vec_.push_back(blob_top_label_);
31   }
32   void Fill(const bool unique_pixels, DataParameter_DB backend) {
33     backend_ = backend;
34     LOG(INFO) &lt;&lt; "Using temporary dataset " &lt;&lt; *filename_;
35     scoped_ptr&lt;db::DB&gt; db(db::GetDB(backend));
36     db-&gt;Open(*filename_, db::NEW);
37     scoped_ptr&lt;db::Transaction&gt; txn(db-&gt;NewTransaction());
38     for (int i = 0; i &lt; 5; ++i) {
39       Datum datum;
40       datum.set_label(i);
41       datum.set_channels(2);
42       datum.set_height(3);
43       datum.set_width(4);
44       std::string* data = datum.mutable_data();
45       for (int j = 0; j &lt; 24; ++j) {
46         int datum = unique_pixels ? j : i;
47         data-&gt;push_back(static_cast&lt;uint8_t&gt;(datum));
48       }
49       stringstream ss;
50       ss &lt;&lt; i;
51       string out;
52       CHECK(datum.SerializeToString(&amp;out));
53       txn-&gt;Put(ss.str(), out);
54     }
55     txn-&gt;Commit();
56     db-&gt;Close();
57   }
58   void TestRead() {
59     const Dtype scale = 3;
60     LayerParameter param;
61     param.set_phase(TRAIN);
62     DataParameter* data_param = param.mutable_data_param();
63     data_param-&gt;set_batch_size(5);
64     data_param-&gt;set_source(filename_-&gt;c_str());
65     data_param-&gt;set_backend(backend_);
66     TransformationParameter* transform_param =
67         param.mutable_transform_param();
68     transform_param-&gt;set_scale(scale);
69     DataLayer&lt;Dtype&gt; layer(param);
70     layer.SetUp(blob_bottom_vec_, blob_top_vec_);
71     EXPECT_EQ(blob_top_data_-&gt;num(), 5);
72     EXPECT_EQ(blob_top_data_-&gt;channels(), 2);
73     EXPECT_EQ(blob_top_data_-&gt;height(), 3);
74     EXPECT_EQ(blob_top_data_-&gt;width(), 4);
75     EXPECT_EQ(blob_top_label_-&gt;num(), 5);
76     EXPECT_EQ(blob_top_label_-&gt;channels(), 1);
77     EXPECT_EQ(blob_top_label_-&gt;height(), 1);
78     EXPECT_EQ(blob_top_label_-&gt;width(), 1);
79     for (int iter = 0; iter &lt; 100; ++iter) {
80       layer.Forward(blob_bottom_vec_, blob_top_vec_);
81       for (int i = 0; i &lt; 5; ++i) {
82         EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
83       }
84       for (int i = 0; i &lt; 5; ++i) {
85         for (int j = 0; j &lt; 24; ++j) {
86           EXPECT_EQ(scale * i, blob_top_data_-&gt;cpu_data()[i * 24 + j])
87               &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " i " &lt;&lt; i &lt;&lt; " j " &lt;&lt; j;
88         }
89       }
90     }
91   }
92   void TestSkip() {
93     LayerParameter param;
94     param.set_phase(TRAIN);
95     DataParameter* data_param = param.mutable_data_param();
96     int batch_size = 5;
97     data_param-&gt;set_batch_size(batch_size);
98     data_param-&gt;set_source(filename_-&gt;c_str());
99     data_param-&gt;set_backend(backend_);
100     Caffe::set_solver_count(8);
101     for (int dev = 0; dev &lt; Caffe::solver_count(); ++dev) {
102       Caffe::set_solver_rank(dev);
103       DataLayer&lt;Dtype&gt; layer(param);
104       layer.SetUp(blob_bottom_vec_, blob_top_vec_);
105       int label = dev;
106       for (int iter = 0; iter &lt; 10; ++iter) {
107         layer.Forward(blob_bottom_vec_, blob_top_vec_);
108         for (int i = 0; i &lt; batch_size; ++i) {
109           EXPECT_EQ(label % batch_size, blob_top_label_-&gt;cpu_data()[i]);
110           label += Caffe::solver_count();
111         }
112       }
113     }
114     Caffe::set_solver_count(1);
115     Caffe::set_solver_rank(0);
116   }
117   void TestReshape(DataParameter_DB backend) {
118     const int num_inputs = 5;
119     LOG(INFO) &lt;&lt; "Using temporary dataset " &lt;&lt; *filename_;
120     scoped_ptr&lt;db::DB&gt; db(db::GetDB(backend));
121     db-&gt;Open(*filename_, db::NEW);
122     scoped_ptr&lt;db::Transaction&gt; txn(db-&gt;NewTransaction());
123     for (int i = 0; i &lt; num_inputs; ++i) {
124       Datum datum;
125       datum.set_label(i);
126       datum.set_channels(2);
127       datum.set_height(i % 2 + 1);
128       datum.set_width(i % 4 + 1);
129       std::string* data = datum.mutable_data();
130       const int data_size = datum.channels() * datum.height() * datum.width();
131       for (int j = 0; j &lt; data_size; ++j) {
132         data-&gt;push_back(static_cast&lt;uint8_t&gt;(j));
133       }
134       stringstream ss;
135       ss &lt;&lt; i;
136       string out;
137       CHECK(datum.SerializeToString(&amp;out));
138       txn-&gt;Put(ss.str(), out);
139     }
140     txn-&gt;Commit();
141     db-&gt;Close();
142     LayerParameter param;
143     param.set_phase(TEST);
144     DataParameter* data_param = param.mutable_data_param();
145     data_param-&gt;set_batch_size(1);
146     data_param-&gt;set_source(filename_-&gt;c_str());
147     data_param-&gt;set_backend(backend);
148     DataLayer&lt;Dtype&gt; layer(param);
149     layer.SetUp(blob_bottom_vec_, blob_top_vec_);
150     EXPECT_EQ(blob_top_data_-&gt;num(), 1);
151     EXPECT_EQ(blob_top_data_-&gt;channels(), 2);
152     EXPECT_EQ(blob_top_label_-&gt;num(), 1);
153     EXPECT_EQ(blob_top_label_-&gt;channels(), 1);
154     EXPECT_EQ(blob_top_label_-&gt;height(), 1);
155     EXPECT_EQ(blob_top_label_-&gt;width(), 1);
156     for (int iter = 0; iter &lt; num_inputs; ++iter) {
157       layer.Forward(blob_bottom_vec_, blob_top_vec_);
158       EXPECT_EQ(blob_top_data_-&gt;height(), iter % 2 + 1);
159       EXPECT_EQ(blob_top_data_-&gt;width(), iter % 4 + 1);
160       EXPECT_EQ(iter, blob_top_label_-&gt;cpu_data()[0]);
161       const int channels = blob_top_data_-&gt;channels();
162 <a name="0"></a>      const int height = blob_top_data_-&gt;height();
163       const int width = blob_top_data_-&gt;width();
164       for (int c = 0; c &lt; channels; ++c) {
165 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>        for (int h = 0; h &lt; height; ++h) {
166           for (int w = 0; w &lt; width; ++w) {
167             const int idx = (c * height + h) * width + w;
168             EXPECT_EQ(idx, static_cast&lt;int&gt;(blob_top_data_-&gt;cpu_data()[idx]))</b></font>
169                 &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " c " &lt;&lt; c
170                 &lt;&lt; " h " &lt;&lt; h &lt;&lt; " w " &lt;&lt; w;
171           }
172         }
173       }
174     }
175   }
176   void TestReadCrop(Phase phase) {
177     const Dtype scale = 3;
178     LayerParameter param;
179     param.set_phase(phase);
180     Caffe::set_random_seed(1701);
181     DataParameter* data_param = param.mutable_data_param();
182     data_param-&gt;set_batch_size(5);
183     data_param-&gt;set_source(filename_-&gt;c_str());
184     data_param-&gt;set_backend(backend_);
185     TransformationParameter* transform_param =
186         param.mutable_transform_param();
187     transform_param-&gt;set_scale(scale);
188     transform_param-&gt;set_crop_size(1);
189     DataLayer&lt;Dtype&gt; layer(param);
190     layer.SetUp(blob_bottom_vec_, blob_top_vec_);
191     EXPECT_EQ(blob_top_data_-&gt;num(), 5);
192     EXPECT_EQ(blob_top_data_-&gt;channels(), 2);
193     EXPECT_EQ(blob_top_data_-&gt;height(), 1);
194     EXPECT_EQ(blob_top_data_-&gt;width(), 1);
195     EXPECT_EQ(blob_top_label_-&gt;num(), 5);
196     EXPECT_EQ(blob_top_label_-&gt;channels(), 1);
197     EXPECT_EQ(blob_top_label_-&gt;height(), 1);
198     EXPECT_EQ(blob_top_label_-&gt;width(), 1);
199     for (int iter = 0; iter &lt; 2; ++iter) {
200       layer.Forward(blob_bottom_vec_, blob_top_vec_);
201 <a name="1"></a>      for (int i = 0; i &lt; 5; ++i) {
202         EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
203       }
204 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      int num_with_center_value = 0;
205       for (int i = 0; i &lt; 5; ++i) {
206         for (int j = 0; j &lt; 2; ++j) {
207           const Dtype center_value = scale * (j ? 17 : 5);</b></font>
208           num_with_center_value +=
209               (center_value == blob_top_data_-&gt;cpu_data()[i * 2 + j]);
210           if (phase == caffe::TEST) {
211             EXPECT_EQ(center_value, this-&gt;blob_top_data_-&gt;cpu_data()[i * 2 + j])
212                 &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " i " &lt;&lt; i &lt;&lt; " j " &lt;&lt; j;
213           }
214         }
215       }
216       if (phase == caffe::TRAIN) {
217         EXPECT_LT(num_with_center_value, 10);
218       }
219     }
220   }
221   void TestReadCropTrainSequenceSeeded() {
222     LayerParameter param;
223     param.set_phase(TRAIN);
224     DataParameter* data_param = param.mutable_data_param();
225     data_param-&gt;set_batch_size(5);
226     data_param-&gt;set_source(filename_-&gt;c_str());
227     data_param-&gt;set_backend(backend_);
228     TransformationParameter* transform_param =
229         param.mutable_transform_param();
230     transform_param-&gt;set_crop_size(1);
231     transform_param-&gt;set_mirror(true);
232     Caffe::set_random_seed(seed_);
233     vector&lt;vector&lt;Dtype&gt; &gt; crop_sequence;
234     {
235       DataLayer&lt;Dtype&gt; layer1(param);
236       layer1.SetUp(blob_bottom_vec_, blob_top_vec_);
237       for (int iter = 0; iter &lt; 2; ++iter) {
238         layer1.Forward(blob_bottom_vec_, blob_top_vec_);
239         for (int i = 0; i &lt; 5; ++i) {
240           EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
241         }
242         vector&lt;Dtype&gt; iter_crop_sequence;
243         for (int i = 0; i &lt; 5; ++i) {
244           for (int j = 0; j &lt; 2; ++j) {
245             iter_crop_sequence.push_back(
246                 blob_top_data_-&gt;cpu_data()[i * 2 + j]);
247           }
248         }
249         crop_sequence.push_back(iter_crop_sequence);
250       }
251     }  
252     Caffe::set_random_seed(seed_);
253     DataLayer&lt;Dtype&gt; layer2(param);
254     layer2.SetUp(blob_bottom_vec_, blob_top_vec_);
255     for (int iter = 0; iter &lt; 2; ++iter) {
256       layer2.Forward(blob_bottom_vec_, blob_top_vec_);
257       for (int i = 0; i &lt; 5; ++i) {
258         EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
259       }
260       for (int i = 0; i &lt; 5; ++i) {
261         for (int j = 0; j &lt; 2; ++j) {
262           EXPECT_EQ(crop_sequence[iter][i * 2 + j],
263                     blob_top_data_-&gt;cpu_data()[i * 2 + j])
264               &lt;&lt; "debug: iter " &lt;&lt; iter &lt;&lt; " i " &lt;&lt; i &lt;&lt; " j " &lt;&lt; j;
265         }
266       }
267     }
268   }
269   void TestReadCropTrainSequenceUnseeded() {
270     LayerParameter param;
271     param.set_phase(TRAIN);
272     DataParameter* data_param = param.mutable_data_param();
273     data_param-&gt;set_batch_size(5);
274     data_param-&gt;set_source(filename_-&gt;c_str());
275     data_param-&gt;set_backend(backend_);
276     TransformationParameter* transform_param =
277         param.mutable_transform_param();
278     transform_param-&gt;set_crop_size(1);
279     transform_param-&gt;set_mirror(true);
280     Caffe::set_random_seed(seed_);
281     srand(seed_);
282     vector&lt;vector&lt;Dtype&gt; &gt; crop_sequence;
283     {
284       DataLayer&lt;Dtype&gt; layer1(param);
285       layer1.SetUp(blob_bottom_vec_, blob_top_vec_);
286       for (int iter = 0; iter &lt; 2; ++iter) {
287         layer1.Forward(blob_bottom_vec_, blob_top_vec_);
288         for (int i = 0; i &lt; 5; ++i) {
289           EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
290         }
291         vector&lt;Dtype&gt; iter_crop_sequence;
292         for (int i = 0; i &lt; 5; ++i) {
293           for (int j = 0; j &lt; 2; ++j) {
294             iter_crop_sequence.push_back(
295                 blob_top_data_-&gt;cpu_data()[i * 2 + j]);
296           }
297         }
298         crop_sequence.push_back(iter_crop_sequence);
299       }
300     }  
301     srand(seed_);
302     DataLayer&lt;Dtype&gt; layer2(param);
303     layer2.SetUp(blob_bottom_vec_, blob_top_vec_);
304     for (int iter = 0; iter &lt; 2; ++iter) {
305       layer2.Forward(blob_bottom_vec_, blob_top_vec_);
306       for (int i = 0; i &lt; 5; ++i) {
307         EXPECT_EQ(i, blob_top_label_-&gt;cpu_data()[i]);
308       }
309       int num_sequence_matches = 0;
310       for (int i = 0; i &lt; 5; ++i) {
311         for (int j = 0; j &lt; 2; ++j) {
312           num_sequence_matches += (crop_sequence[iter][i * 2 + j] ==
313                                    blob_top_data_-&gt;cpu_data()[i * 2 + j]);
314         }
315       }
316       EXPECT_LT(num_sequence_matches, 10);
317     }
318   }
319   virtual ~DataLayerTest() { delete blob_top_data_; delete blob_top_label_; }
320   DataParameter_DB backend_;
321   shared_ptr&lt;string&gt; filename_;
322   Blob&lt;Dtype&gt;* const blob_top_data_;
323   Blob&lt;Dtype&gt;* const blob_top_label_;
324   vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
325   vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
326   int seed_;
327 };
328 TYPED_TEST_CASE(DataLayerTest, TestDtypesAndDevices);
329 #ifdef USE_LEVELDB
330 TYPED_TEST(DataLayerTest, TestReadLevelDB) {
331   const bool unique_pixels = false;    this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
332   this-&gt;TestRead();
333 }
334 TYPED_TEST(DataLayerTest, TestSkipLevelDB) {
335   this-&gt;Fill(false, DataParameter_DB_LEVELDB);
336   this-&gt;TestSkip();
337 }
338 TYPED_TEST(DataLayerTest, TestReshapeLevelDB) {
339   this-&gt;TestReshape(DataParameter_DB_LEVELDB);
340 }
341 TYPED_TEST(DataLayerTest, TestReadCropTrainLevelDB) {
342   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
343   this-&gt;TestReadCrop(TRAIN);
344 }
345 TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceSeededLevelDB) {
346   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
347   this-&gt;TestReadCropTrainSequenceSeeded();
348 }
349 TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceUnseededLevelDB) {
350   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
351   this-&gt;TestReadCropTrainSequenceUnseeded();
352 }
353 TYPED_TEST(DataLayerTest, TestReadCropTestLevelDB) {
354   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LEVELDB);
355   this-&gt;TestReadCrop(TEST);
356 }
357 #endif  
358 #ifdef USE_LMDB
359 TYPED_TEST(DataLayerTest, TestReadLMDB) {
360   const bool unique_pixels = false;    this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
361   this-&gt;TestRead();
362 }
363 TYPED_TEST(DataLayerTest, TestSkipLMDB) {
364   this-&gt;Fill(false, DataParameter_DB_LMDB);
365   this-&gt;TestSkip();
366 }
367 TYPED_TEST(DataLayerTest, TestReshapeLMDB) {
368   this-&gt;TestReshape(DataParameter_DB_LMDB);
369 }
370 TYPED_TEST(DataLayerTest, TestReadCropTrainLMDB) {
371   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
372   this-&gt;TestReadCrop(TRAIN);
373 }
374 TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceSeededLMDB) {
375   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
376   this-&gt;TestReadCropTrainSequenceSeeded();
377 }
378 TYPED_TEST(DataLayerTest, TestReadCropTrainSequenceUnseededLMDB) {
379   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
380   this-&gt;TestReadCropTrainSequenceUnseeded();
381 }
382 TYPED_TEST(DataLayerTest, TestReadCropTestLMDB) {
383   const bool unique_pixels = true;    this-&gt;Fill(unique_pixels, DataParameter_DB_LMDB);
384   this-&gt;TestReadCrop(TEST);
385 }
#endif  }  #endif  </pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
