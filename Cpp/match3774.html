<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_gradient_based_solver.cpp &amp; recurrent_layer.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_gradient_based_solver.cpp &amp; recurrent_layer.cpp
      </h3>
<h1 align="center">
        1.2%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_gradient_based_solver.cpp (0.7038123%)<th>recurrent_layer.cpp (4.8192773%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(559-568)<td><a href="#" name="0">(157-162)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_gradient_based_solver.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;algorithm&gt;
#include &lt;string&gt;
#include &lt;utility&gt;
#include &lt;vector&gt;
#include "google/protobuf/text_format.h"
#include "gtest/gtest.h"
#include "caffe/common.hpp"
#include "caffe/parallel.hpp"
#include "caffe/proto/caffe.pb.h"
#include "caffe/sgd_solvers.hpp"
#include "caffe/util/io.hpp"
#include "caffe/test/test_caffe_main.hpp"
using std::ostringstream;
namespace caffe {
template &lt;typename TypeParam&gt;
class GradientBasedSolverTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  GradientBasedSolverTest() :
      seed_(1701), num_(4), channels_(3), height_(10), width_(10),
      share_(false) {
        input_file_ = new string(
        ABS_TEST_DATA_DIR "/solver_data_list.txt");
      }
  ~GradientBasedSolverTest() {
    delete input_file_;
  }
  string snapshot_prefix_;
  shared_ptr&lt;SGDSolver&lt;Dtype&gt; &gt; solver_;
#ifdef USE_NCCL
  shared_ptr&lt;NCCL&lt;Dtype&gt; &gt; nccl_;
#endif
  int seed_;
  int num_, channels_, height_, width_;
  bool share_;
  Dtype delta_;  
  string* input_file_;
  virtual void InitSolver(const SolverParameter&amp; param) = 0;
  virtual void InitSolverFromProtoString(const string&amp; proto) {
    SolverParameter param;
    CHECK(google::protobuf::TextFormat::ParseFromString(proto, &amp;param));
    switch (Caffe::mode()) {
      case Caffe::CPU:
        param.set_solver_mode(SolverParameter_SolverMode_CPU);
        break;
      case Caffe::GPU:
        param.set_solver_mode(SolverParameter_SolverMode_GPU);
        break;
      default:
        LOG(FATAL) &lt;&lt; "Unknown Caffe mode: " &lt;&lt; Caffe::mode();
    }
    InitSolver(param);
    delta_ = param.delta();
  }
  string RunLeastSquaresSolver(const Dtype learning_rate,
      const Dtype weight_decay, const Dtype momentum, const int num_iters,
      const int iter_size = 1, const int devices = 1,
      const bool snapshot = false, const char* from_snapshot = NULL) {
    ostringstream proto;
    int device_id = 0;
#ifndef CPU_ONLY
    if (Caffe::mode() == Caffe::GPU) {
      CUDA_CHECK(cudaGetDevice(&amp;device_id));
    }
#endif
    proto &lt;&lt;
       "snapshot_after_train: " &lt;&lt; snapshot &lt;&lt; " "
       "max_iter: " &lt;&lt; num_iters &lt;&lt; " "
       "base_lr: " &lt;&lt; learning_rate &lt;&lt; " "
       "lr_policy: 'fixed' "
       "iter_size: " &lt;&lt; iter_size &lt;&lt; " "
       "device_id: " &lt;&lt; device_id &lt;&lt; " "
       "layer_wise_reduce: " &lt;&lt; (!share_) &lt;&lt; " "
       "net_param { "
       "  name: 'TestNetwork' "
       "  layer { "
       "    name: 'data' "
       "    type: 'HDF5Data' "
       "    hdf5_data_param { "
       "      source: '" &lt;&lt; *(this-&gt;input_file_) &lt;&lt; "' "
       "      batch_size: " &lt;&lt; num_ / iter_size &lt;&lt; " "
       "    } "
       "    top: 'data' "
       "    top: 'targets' "
       "  } ";
    if (share_) {
      proto &lt;&lt;
         "  layer { "
         "    name: 'slice' "
         "    type: 'Slice' "
         "    bottom: 'data' "
         "    top: 'data1' "
         "    top: 'data2' "
         "    slice_param { "
         "      axis: 0 "
         "    } "
         "  } ";
    }
    proto &lt;&lt;
       "  layer { "
       "    name: 'innerprod' "
       "    type: 'InnerProduct' "
       "    param { name: 'weights' } "
       "    param { name: 'bias' } "
       "    inner_product_param { "
       "      num_output: 1 "
       "      weight_filler { "
       "        type: 'gaussian' "
       "        std: 1.0 "
       "      } "
       "      bias_filler { "
       "        type: 'gaussian' "
       "        std: 1.0 "
       "      } "
       "    } "
       "    bottom: '" &lt;&lt; string(share_ ? "data1": "data") &lt;&lt; "' "
       "    top: '" &lt;&lt; string(share_ ? "innerprod1": "innerprod") &lt;&lt; "' "
       "  } ";
    if (share_) {
      proto &lt;&lt;
         "  layer { "
         "    name: 'innerprod2' "
         "    type: 'InnerProduct' "
         "    param { name: 'weights' } "
         "    param { name: 'bias' } "
         "    inner_product_param { "
         "      num_output: 1 "
         "      weight_filler { "
         "        type: 'gaussian' "
         "        std: 1.0 "
         "      } "
         "      bias_filler { "
         "        type: 'gaussian' "
         "        std: 1.0 "
         "      } "
         "    } "
         "    bottom: 'data2' "
         "    top: 'innerprod2' "
         "  } "
         "  layer { "
         "    name: 'concat' "
         "    type: 'Concat' "
         "    bottom: 'innerprod1' "
         "    bottom: 'innerprod2' "
         "    top: 'innerprod' "
         "    concat_param { "
         "      axis: 0 "
         "    } "
         "  } ";
    }
    proto &lt;&lt;
       "  layer { "
       "    name: 'loss' "
       "    type: 'EuclideanLoss' "
       "    bottom: 'innerprod' "
       "    bottom: 'targets' "
       "  } "
       "} ";
    if (weight_decay != 0) {
      proto &lt;&lt; "weight_decay: " &lt;&lt; weight_decay &lt;&lt; " ";
    }
    if (momentum != 0) {
      proto &lt;&lt; "momentum: " &lt;&lt; momentum &lt;&lt; " ";
    }
    MakeTempDir(&amp;snapshot_prefix_);
    proto &lt;&lt; "snapshot_prefix: '" &lt;&lt; snapshot_prefix_ &lt;&lt; "/' ";
    if (snapshot) {
      proto &lt;&lt; "snapshot: " &lt;&lt; num_iters &lt;&lt; " ";
    }
    Caffe::set_random_seed(this-&gt;seed_);
    this-&gt;InitSolverFromProtoString(proto.str());
    if (from_snapshot) {
      this-&gt;solver_-&gt;Restore(from_snapshot);
      for (int i = 0; i &lt; this-&gt;solver_-&gt;iter(); ++i) {
        this-&gt;solver_-&gt;net()-&gt;Forward();
      }
    }
    if (devices == 1) {
      this-&gt;solver_-&gt;Solve();
    } else {
      LOG(INFO) &lt;&lt; "Multi-GPU test on " &lt;&lt; devices &lt;&lt; " devices";
      vector&lt;int&gt; gpus;
      int device_id = solver_-&gt;param().device_id();
      gpus.push_back(device_id);
      for (int i = 0; gpus.size() &lt; devices; ++i) {
        if (i != device_id)
          gpus.push_back(i);
      }
      Caffe::set_solver_count(gpus.size());
#ifdef USE_NCCL
      this-&gt;nccl_.reset(new NCCL&lt;Dtype&gt;(this-&gt;solver_));
      this-&gt;nccl_-&gt;Run(gpus, from_snapshot);
#endif
      Caffe::set_solver_count(1);
    }
    if (snapshot) {
      ostringstream resume_file;
      resume_file &lt;&lt; snapshot_prefix_ &lt;&lt; "/_iter_" &lt;&lt; num_iters
                  &lt;&lt; ".solverstate";
      string resume_filename = resume_file.str();
      return resume_filename;
    }
    return string();
  }
  void ComputeLeastSquaresUpdate(const Dtype learning_rate,
      const Dtype weight_decay, const Dtype momentum, const int num_iters,
      vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;* updated_params) {
    const int N = num_;
    const int D = channels_ * height_ * width_;
    Net&lt;Dtype&gt;&amp; net = *this-&gt;solver_-&gt;net();
    net.Forward();
    ASSERT_TRUE(net.has_blob("data"));
    const Blob&lt;Dtype&gt;&amp; data = *net.blob_by_name("data");
    ASSERT_TRUE(net.has_blob("targets"));
    const Blob&lt;Dtype&gt;&amp; targets = *net.blob_by_name("targets");
    ASSERT_TRUE(net.has_layer("innerprod"));
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; param_blobs =
        net.layer_by_name("innerprod")-&gt;blobs();
    const int num_param_blobs = 2;
    ASSERT_EQ(num_param_blobs, param_blobs.size());
    const Blob&lt;Dtype&gt;&amp; weights = *param_blobs[0];
    const Blob&lt;Dtype&gt;&amp; bias = *param_blobs[1];
    ASSERT_EQ(D * N, data.count());
    ASSERT_EQ(N, targets.count());
    ASSERT_EQ(D, weights.count());
    ASSERT_EQ(1, bias.count());
    updated_params-&gt;clear();
    updated_params-&gt;resize(num_param_blobs);
    for (int i = 0; i &lt; num_param_blobs; ++i) {
      (*updated_params)[i].reset(new Blob&lt;Dtype&gt;());
    }
    Blob&lt;Dtype&gt;&amp; updated_weights = *(*updated_params)[0];
    updated_weights.ReshapeLike(weights);
    Blob&lt;Dtype&gt;&amp; updated_bias = *(*updated_params)[1];
    updated_bias.ReshapeLike(bias);
    for (int i = 0; i &lt;= D; ++i) {
      Dtype grad = 0;
      for (int j = 0; j &lt;= D; ++j) {
        Dtype element = 0;
        for (int k = 0; k &lt; N; ++k) {
          const Dtype element_i = (i == D) ? 1 : data.cpu_data()[k * D + i];
          const Dtype element_j = (j == D) ? 1 : data.cpu_data()[k * D + j];
          element += element_i * element_j;
        }
        if (j == D) {
          grad += element * bias.cpu_data()[0];
        } else {
          grad += element * weights.cpu_data()[j];
        }
      }
      for (int k = 0; k &lt; N; ++k) {
        const Dtype element_i = (i == D) ? 1 : data.cpu_data()[k * D + i];
        grad -= element_i * targets.cpu_data()[k];
      }
      grad /= N;
      grad += weight_decay *
          ((i == D) ? bias.cpu_data()[0] : weights.cpu_data()[i]);
      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; history = solver_-&gt;history();
      if (solver_-&gt;type() != string("AdaDelta")
          &amp;&amp; solver_-&gt;type() != string("Adam")) {
        ASSERT_EQ(2, history.size());        } else {
        ASSERT_EQ(4, history.size());        }
      Dtype update_value = learning_rate * grad;
      const Dtype history_value = (i == D) ?
            history[1]-&gt;cpu_data()[0] : history[0]-&gt;cpu_data()[i];
      const Dtype temp = momentum * history_value;
      if (solver_-&gt;type() == string("SGD")) {
        update_value += temp;
      } else if (solver_-&gt;type() == string("Nesterov")) {
        update_value += temp;
        update_value = (1 + momentum) * update_value - temp;
      } else if (solver_-&gt;type() == string("AdaGrad")) {
        update_value /= std::sqrt(history_value + grad * grad) + delta_;
      } else if (solver_-&gt;type() == string("RMSProp")) {
        const Dtype rms_decay = 0.95;
        update_value /= std::sqrt(rms_decay*history_value
            + grad * grad * (1 - rms_decay)) + delta_;
      } else if (solver_-&gt;type() == string("AdaDelta")) {
        const Dtype update_history_value = (i == D) ?
            history[1 + num_param_blobs]-&gt;cpu_data()[0] :
            history[0 + num_param_blobs]-&gt;cpu_data()[i];
        const Dtype weighted_gradient_average =
            momentum * history_value + (1 - momentum) * (grad * grad);
        update_value = grad * std::sqrt((update_history_value + delta_) /
            (weighted_gradient_average + delta_)) * learning_rate;
      } else if (solver_-&gt;type() == string("Adam")) {
        const Dtype momentum2 = 0.999;
        const Dtype m = history_value;
        const Dtype v = (i == D) ?
            history[1 + num_param_blobs]-&gt;cpu_data()[0] :
            history[0 + num_param_blobs]-&gt;cpu_data()[i];
        const Dtype val_m = (1 - momentum) * grad + momentum * m;
        const Dtype val_v = (1 - momentum2) * grad * grad + momentum2 * v;
        Dtype alpha_t = learning_rate *
            std::sqrt(Dtype(1) - pow(momentum2, num_iters)) /
            (Dtype(1.) - pow(momentum, num_iters));
        update_value = alpha_t * val_m / (std::sqrt(val_v) + delta_);
      } else {
        LOG(FATAL) &lt;&lt; "Unknown solver type: " &lt;&lt; solver_-&gt;type();
      }
      if (i == D) {
        updated_bias.mutable_cpu_diff()[0] = update_value;
        updated_bias.mutable_cpu_data()[0] = bias.cpu_data()[0] - update_value;
      } else {
        updated_weights.mutable_cpu_diff()[i] = update_value;
        updated_weights.mutable_cpu_data()[i] =
            weights.cpu_data()[i] - update_value;
      }
    }
  }
  void CheckLeastSquaresUpdate(
      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; updated_params) {
    const int D = channels_ * height_ * width_;
    const Blob&lt;Dtype&gt;&amp; updated_weights = *updated_params[0];
    const Blob&lt;Dtype&gt;&amp; updated_bias = *updated_params[1];
    Net&lt;Dtype&gt;&amp; net = *this-&gt;solver_-&gt;net();
    ASSERT_TRUE(net.has_layer("innerprod"));
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; param_blobs =
        net.layer_by_name("innerprod")-&gt;blobs();
    ASSERT_EQ(2, param_blobs.size());
    const Blob&lt;Dtype&gt;&amp; solver_updated_weights = *param_blobs[0];
    ASSERT_EQ(D, solver_updated_weights.count());
    const double kPrecision = 1e-2;
    const double kMinPrecision = 1e-7;
    for (int i = 0; i &lt; D; ++i) {
      const Dtype expected_updated_weight = updated_weights.cpu_data()[i];
      const Dtype solver_updated_weight = solver_updated_weights.cpu_data()[i];
      const Dtype error_margin = std::max(kMinPrecision, kPrecision *
          std::min(fabs(expected_updated_weight), fabs(solver_updated_weight)));
      EXPECT_NEAR(expected_updated_weight, solver_updated_weight, error_margin);
    }
    const Blob&lt;Dtype&gt;&amp; solver_updated_bias_blob = *param_blobs[1];
    ASSERT_EQ(1, solver_updated_bias_blob.count());
    const Dtype expected_updated_bias = updated_bias.cpu_data()[0];
    const Dtype solver_updated_bias = solver_updated_bias_blob.cpu_data()[0];
    const Dtype error_margin = std::max(kMinPrecision, kPrecision *
          std::min(fabs(expected_updated_bias), fabs(solver_updated_bias)));
    EXPECT_NEAR(expected_updated_bias, solver_updated_bias, error_margin);
    if (solver_-&gt;type() == string("SGD")) {
      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; history = solver_-&gt;history();
      ASSERT_EQ(2, history.size());
      for (int i = 0; i &lt; D; ++i) {
        const Dtype expected_history = updated_weights.cpu_diff()[i];
        const Dtype solver_history = history[0]-&gt;cpu_data()[i];
        const Dtype error_margin_hist = std::max(kMinPrecision, kPrecision *
            std::min(fabs(expected_history), fabs(solver_history)));
        EXPECT_NEAR(expected_history, solver_history, error_margin_hist);
      }
      const Dtype expected_history = updated_bias.cpu_diff()[0];
      const Dtype solver_history = history[1]-&gt;cpu_data()[0];
      const Dtype error_margin_hist = std::max(kMinPrecision, kPrecision *
          std::min(fabs(expected_history), fabs(solver_history)));
      EXPECT_NEAR(expected_history, solver_history, error_margin_hist);
    }
  }
  void CheckAccumulation(const Dtype kLearningRate, const Dtype kWeightDecay,
      const Dtype kMomentum, const int kNumIters, const int kIterSize) {
    const double kPrecision = 1e-2;
    const double kMinPrecision = 1e-7;
    this-&gt;RunLeastSquaresSolver(kLearningRate, kWeightDecay, kMomentum,
        kNumIters);
    Net&lt;Dtype&gt;&amp; net = *this-&gt;solver_-&gt;net();
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; param_blobs =
        net.layer_by_name("innerprod")-&gt;blobs();
    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; noaccum_params(param_blobs.size());
    for (int i = 0; i &lt; param_blobs.size(); ++i) {
      noaccum_params[i].reset(new Blob&lt;Dtype&gt;());
      noaccum_params[i]-&gt;CopyFrom(*param_blobs[i], false, true);
    }
    this-&gt;RunLeastSquaresSolver(kLearningRate, kWeightDecay, kMomentum,
        kNumIters, kIterSize);
    Net&lt;Dtype&gt;&amp; net_accum = *this-&gt;solver_-&gt;net();
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; accum_params =
        net_accum.layer_by_name("innerprod")-&gt;blobs();
    const int D = this-&gt;channels_ * this-&gt;height_ * this-&gt;width_;
    for (int i = 0; i &lt; D; ++i) {
      const Dtype expected_param = noaccum_params[0]-&gt;cpu_data()[i];
      const Dtype accum_param = accum_params[0]-&gt;cpu_data()[i];
      const Dtype error_margin = std::max(kMinPrecision, kPrecision *
          std::min(fabs(expected_param), fabs(accum_param)));
      EXPECT_NEAR(expected_param, accum_param, error_margin);
    }
    ASSERT_EQ(1, accum_params[1]-&gt;count());
    const Dtype expected_bias = noaccum_params[1]-&gt;cpu_data()[0];
    const Dtype accum_bias = accum_params[1]-&gt;cpu_data()[0];
    const Dtype error_margin = std::max(kMinPrecision, kPrecision *
        std::min(fabs(expected_bias), fabs(accum_bias)));
    EXPECT_NEAR(expected_bias, accum_bias, error_margin);
  }
  void TestLeastSquaresUpdate(const Dtype learning_rate = 1.0,
      const Dtype weight_decay = 0.0, const Dtype momentum = 0.0,
      const int iter_to_check = 0) {
    const int kNum = num_;
    const int kIterSize = 1;
    int available_devices = 1;
#ifdef USE_NCCL
    if (Caffe::mode() == Caffe::GPU) {
      CUDA_CHECK(cudaGetDeviceCount(&amp;available_devices));
    }
#endif
    vector&lt;int&gt; sizes;
    sizes.push_back(1);
    if (available_devices &gt;= 2) {
      sizes.push_back(2);
    }
    if (available_devices &gt;= 3) {
      sizes.push_back(3);
    }
    if (available_devices &gt;= 8) {
      sizes.push_back(8);
    }
    if (available_devices &gt;= 16) {
      sizes.push_back(16);
    }
    for (int i = 0; i &lt; sizes.size(); ++i) {
      int devices = sizes[i];
      num_ = kNum * devices;
      RunLeastSquaresSolver(learning_rate, weight_decay, momentum,
                            iter_to_check, kIterSize, 1);
      vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; updated_params;
      ComputeLeastSquaresUpdate(learning_rate, weight_decay, momentum,
          iter_to_check + 1, &amp;updated_params);
      num_ = kNum;
      RunLeastSquaresSolver(learning_rate, weight_decay, momentum,
          iter_to_check + 1, kIterSize, devices);
      CheckLeastSquaresUpdate(updated_params);
    }
  }
  void TestSnapshot(const Dtype learning_rate = 1.0,
      const Dtype weight_decay = 0.0, const Dtype momentum = 0.0,
      const int num_iters = 1) {
    const int total_num_iters = num_iters * 2;
    bool snapshot = false;
    const int kIterSize = 1;
    const int kDevices = 1;
    RunLeastSquaresSolver(learning_rate, weight_decay, momentum,
        total_num_iters, kIterSize, kDevices, snapshot);
    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_copies;
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; orig_params =
        solver_-&gt;net()-&gt;learnable_params();
    param_copies.resize(orig_params.size());
    for (int i = 0; i &lt; orig_params.size(); ++i) {
      param_copies[i].reset(new Blob&lt;Dtype&gt;());
      const bool kReshape = true;
      for (int copy_diff = false; copy_diff &lt;= true; ++copy_diff) {
        param_copies[i]-&gt;CopyFrom(*orig_params[i], copy_diff, kReshape);
      }
    }
    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; history_copies;
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; orig_history = solver_-&gt;history();
    history_copies.resize(orig_history.size());
    for (int i = 0; i &lt; orig_history.size(); ++i) {
      history_copies[i].reset(new Blob&lt;Dtype&gt;());
      const bool kReshape = true;
      for (int copy_diff = false; copy_diff &lt;= true; ++copy_diff) {
        history_copies[i]-&gt;CopyFrom(*orig_history[i], copy_diff, kReshape);
      }
    }
    snapshot = true;
    string snapshot_name = RunLeastSquaresSolver(learning_rate, weight_decay,
        momentum, num_iters, kIterSize, kDevices, snapshot);
    snapshot = false;
    RunLeastSquaresSolver(learning_rate, weight_decay, momentum,
        total_num_iters, kIterSize, kDevices,
        snapshot, snapshot_name.c_str());
<a name="0"></a>
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; params = solver_-&gt;net()-&gt;learnable_params();
<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>    for (int i = 0; i &lt; params.size(); ++i) {
      for (int j = 0; j &lt; params[i]-&gt;count(); ++j) {
        EXPECT_FLOAT_EQ(param_copies[i]-&gt;cpu_data()[j],
            params[i]-&gt;cpu_data()[j])
            &lt;&lt; "param " &lt;&lt; i &lt;&lt; " data differed at dim " &lt;&lt; j;
        EXPECT_FLOAT_EQ(param_copies[i]-&gt;cpu_diff()[j],
            params[i]-&gt;cpu_diff()[j])
            &lt;&lt; "param " &lt;&lt; i &lt;&lt; " diff differed at dim " &lt;&lt; j;
      }
    }</b></font>
    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; history = solver_-&gt;history();
    for (int i = 0; i &lt; history.size(); ++i) {
      for (int j = 0; j &lt; history[i]-&gt;count(); ++j) {
        EXPECT_FLOAT_EQ(history_copies[i]-&gt;cpu_data()[j],
            history[i]-&gt;cpu_data()[j])
            &lt;&lt; "history blob " &lt;&lt; i &lt;&lt; " data differed at dim " &lt;&lt; j;
        EXPECT_FLOAT_EQ(history_copies[i]-&gt;cpu_diff()[j],
            history[i]-&gt;cpu_diff()[j])
            &lt;&lt; "history blob " &lt;&lt; i &lt;&lt; " diff differed at dim " &lt;&lt; j;
      }
    }
  }
};
template &lt;typename TypeParam&gt;
class SGDSolverTest : public GradientBasedSolverTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  virtual void InitSolver(const SolverParameter&amp; param) {
    this-&gt;solver_.reset(new SGDSolver&lt;Dtype&gt;(param));
  }
};
TYPED_TEST_CASE(SGDSolverTest, TestDtypesAndDevices);
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdate) {
  this-&gt;TestLeastSquaresUpdate();
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateLROneHundredth) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  this-&gt;TestLeastSquaresUpdate(kLearningRate);
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithWeightDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 1;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithWeightDecayMultiIter) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithMomentum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 1;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithMomentumMultiIter) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithEverything) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithEverythingShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithEverythingAccum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(SGDSolverTest, TestLeastSquaresUpdateWithEverythingAccumShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;share_ = true;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(SGDSolverTest, TestSnapshot) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(SGDSolverTest, TestSnapshotShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
template &lt;typename TypeParam&gt;
class AdaGradSolverTest : public GradientBasedSolverTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  virtual void InitSolver(const SolverParameter&amp; param) {
    this-&gt;solver_.reset(new AdaGradSolver&lt;Dtype&gt;(param));
  }
};
TYPED_TEST_CASE(AdaGradSolverTest, TestDtypesAndDevices);
TYPED_TEST(AdaGradSolverTest, TestAdaGradLeastSquaresUpdate) {
  this-&gt;TestLeastSquaresUpdate();
}
TYPED_TEST(AdaGradSolverTest, TestAdaGradLeastSquaresUpdateLROneHundredth) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  this-&gt;TestLeastSquaresUpdate(kLearningRate);
}
TYPED_TEST(AdaGradSolverTest, TestAdaGradLeastSquaresUpdateWithWeightDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay);
}
TYPED_TEST(AdaGradSolverTest, TestAdaGradLeastSquaresUpdateWithEverything) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaGradSolverTest,
      TestAdaGradLeastSquaresUpdateWithEverythingShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaGradSolverTest, TestLeastSquaresUpdateWithEverythingAccum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(AdaGradSolverTest, TestLeastSquaresUpdateWithEverythingAccumShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;share_ = true;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(AdaGradSolverTest, TestSnapshot) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaGradSolverTest, TestSnapshotShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
template &lt;typename TypeParam&gt;
class NesterovSolverTest : public GradientBasedSolverTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  virtual void InitSolver(const SolverParameter&amp; param) {
    this-&gt;solver_.reset(new NesterovSolver&lt;Dtype&gt;(param));
  }
};
TYPED_TEST_CASE(NesterovSolverTest, TestDtypesAndDevices);
TYPED_TEST(NesterovSolverTest, TestNesterovLeastSquaresUpdate) {
  this-&gt;TestLeastSquaresUpdate();
}
TYPED_TEST(NesterovSolverTest, TestNesterovLeastSquaresUpdateLROneHundredth) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  this-&gt;TestLeastSquaresUpdate(kLearningRate);
}
TYPED_TEST(NesterovSolverTest, TestNesterovLeastSquaresUpdateWithWeightDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay);
}
TYPED_TEST(NesterovSolverTest,
           TestNesterovLeastSquaresUpdateWithWeightDecayMultiIter) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(NesterovSolverTest, TestNesterovLeastSquaresUpdateWithMomentum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 1;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(NesterovSolverTest, TestLeastSquaresUpdateWithMomentumMultiIter) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(NesterovSolverTest, TestNesterovLeastSquaresUpdateWithEverything) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(NesterovSolverTest,
           TestNesterovLeastSquaresUpdateWithEverythingShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(NesterovSolverTest, TestLeastSquaresUpdateWithEverythingAccum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(NesterovSolverTest, TestLeastSquaresUpdateWithEverythingAccumShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;share_ = true;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(NesterovSolverTest, TestSnapshot) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(NesterovSolverTest, TestSnapshotShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
template &lt;typename TypeParam&gt;
class AdaDeltaSolverTest : public GradientBasedSolverTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  virtual void InitSolver(const SolverParameter&amp; param) {
    this-&gt;solver_.reset(new AdaDeltaSolver&lt;Dtype&gt;(param));
  }
};
TYPED_TEST_CASE(AdaDeltaSolverTest, TestDtypesAndDevices);
TYPED_TEST(AdaDeltaSolverTest, TestAdaDeltaLeastSquaresUpdate) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  this-&gt;TestLeastSquaresUpdate(kLearningRate);
}
TYPED_TEST(AdaDeltaSolverTest, TestAdaDeltaLeastSquaresUpdateWithWeightDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.95;
  this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum);
}
TYPED_TEST(AdaDeltaSolverTest, TestAdaDeltaLeastSquaresUpdateWithHalfMomentum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.0;
  const Dtype kMomentum = 0.5;
  const int kNumIters = 1;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum);
  }
}
TYPED_TEST(AdaDeltaSolverTest, TestAdaDeltaLeastSquaresUpdateWithMomentum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.0;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 1;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum);
  }
}
TYPED_TEST(AdaDeltaSolverTest, TestLeastSquaresUpdateWithMomentumMultiIter) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.0;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaDeltaSolverTest, TestAdaDeltaLeastSquaresUpdateWithEverything) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.1;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaDeltaSolverTest,
           TestAdaDeltaLeastSquaresUpdateWithEverythingShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.1;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaDeltaSolverTest, TestLeastSquaresUpdateWithEverythingAccum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.1;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(AdaDeltaSolverTest, TestLeastSquaresUpdateWithEverythingAccumShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.1;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;share_ = true;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(AdaDeltaSolverTest, TestSnapshot) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.1;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdaDeltaSolverTest, TestSnapshotShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.1;
  const Dtype kWeightDecay = 0.1;
  const Dtype kMomentum = 0.95;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
template &lt;typename TypeParam&gt;
class AdamSolverTest : public GradientBasedSolverTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  virtual void InitSolver(const SolverParameter&amp; param) {
    SolverParameter new_param = param;
    const Dtype momentum = 0.9;
    new_param.set_momentum(momentum);
    const Dtype momentum2 = 0.999;
    new_param.set_momentum2(momentum2);
    this-&gt;solver_.reset(new AdamSolver&lt;Dtype&gt;(new_param));
  }
};
TYPED_TEST_CASE(AdamSolverTest, TestDtypesAndDevices);
TYPED_TEST(AdamSolverTest, TestAdamLeastSquaresUpdate) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0;
  const Dtype kMomentum = 0.9;
  this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum);
}
TYPED_TEST(AdamSolverTest, TestAdamLeastSquaresUpdateWithWeightDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum);
}
TYPED_TEST(AdamSolverTest, TestAdamLeastSquaresUpdateWithEverything) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdamSolverTest, TestAdamLeastSquaresUpdateWithEverythingShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdamSolverTest, TestLeastSquaresUpdateWithEverythingAccum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(AdamSolverTest, TestLeastSquaresUpdateWithEverythingAccumShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;share_ = true;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(AdamSolverTest, TestSnapshot) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(AdamSolverTest, TestSnapshotShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.9;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
template &lt;typename TypeParam&gt;
class RMSPropSolverTest : public GradientBasedSolverTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  virtual void InitSolver(const SolverParameter&amp; param) {
    const Dtype rms_decay = 0.95;
    SolverParameter new_param = param;
    new_param.set_rms_decay(rms_decay);
    this-&gt;solver_.reset(new RMSPropSolver&lt;Dtype&gt;(new_param));
  }
};
TYPED_TEST_CASE(RMSPropSolverTest, TestDtypesAndDevices);
TYPED_TEST(RMSPropSolverTest, TestRMSPropLeastSquaresUpdateWithWeightDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 1.0;
  const Dtype kWeightDecay = 0.5;
  this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay);
}
TYPED_TEST(RMSPropSolverTest, TestRMSPropLeastSquaresUpdateWithRmsDecay) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.0;
  const Dtype kMomentum = 0.0;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(RMSPropSolverTest, TestRMSPropLeastSquaresUpdateWithEverything) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.0;
  const int kNumIters = 4;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(RMSPropSolverTest,
      TestRMSPropLeastSquaresUpdateWithEverythingShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.0;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 0; i &lt;= kNumIters; ++i) {
    this-&gt;TestLeastSquaresUpdate(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(RMSPropSolverTest, TestLeastSquaresUpdateWithEverythingAccum) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.0;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(RMSPropSolverTest, TestLeastSquaresUpdateWithEverythingAccumShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0.0;
  const int kNumIters = 4;
  const int kIterSize = 2;
  this-&gt;share_ = true;
  this-&gt;CheckAccumulation(kLearningRate, kWeightDecay, kMomentum, kNumIters,
      kIterSize);
}
TYPED_TEST(RMSPropSolverTest, TestSnapshot) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
TYPED_TEST(RMSPropSolverTest, TestSnapshotShare) {
  typedef typename TypeParam::Dtype Dtype;
  const Dtype kLearningRate = 0.01;
  const Dtype kWeightDecay = 0.5;
  const Dtype kMomentum = 0;
  const int kNumIters = 4;
  this-&gt;share_ = true;
  for (int i = 1; i &lt;= kNumIters; ++i) {
    this-&gt;TestSnapshot(kLearningRate, kWeightDecay, kMomentum, i);
  }
}
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>recurrent_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;string&gt;
#include &lt;vector&gt;
#include "caffe/blob.hpp"
#include "caffe/common.hpp"
#include "caffe/filler.hpp"
#include "caffe/layer.hpp"
#include "caffe/layers/recurrent_layer.hpp"
#include "caffe/util/math_functions.hpp"
namespace caffe {
template &lt;typename Dtype&gt;
void RecurrentLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  CHECK_GE(bottom[0]-&gt;num_axes(), 2)
      &lt;&lt; "bottom[0] must have at least 2 axes -- (#timesteps, #streams, ...)";
  T_ = bottom[0]-&gt;shape(0);
  N_ = bottom[0]-&gt;shape(1);
  LOG(INFO) &lt;&lt; "Initializing recurrent layer: assuming input batch contains "
            &lt;&lt; T_ &lt;&lt; " timesteps of " &lt;&lt; N_ &lt;&lt; " independent streams.";
  CHECK_EQ(bottom[1]-&gt;num_axes(), 2)
      &lt;&lt; "bottom[1] must have exactly 2 axes -- (#timesteps, #streams)";
  CHECK_EQ(T_, bottom[1]-&gt;shape(0));
  CHECK_EQ(N_, bottom[1]-&gt;shape(1));
  expose_hidden_ = this-&gt;layer_param_.recurrent_param().expose_hidden();
  vector&lt;string&gt; output_names;
  OutputBlobNames(&amp;output_names);
  vector&lt;string&gt; recur_input_names;
  RecurrentInputBlobNames(&amp;recur_input_names);
  vector&lt;string&gt; recur_output_names;
  RecurrentOutputBlobNames(&amp;recur_output_names);
  const int num_recur_blobs = recur_input_names.size();
  CHECK_EQ(num_recur_blobs, recur_output_names.size());
  const int num_hidden_exposed = expose_hidden_ * num_recur_blobs;
  static_input_ = (bottom.size() &gt; 2 + num_hidden_exposed);
  if (static_input_) {
    CHECK_GE(bottom[2]-&gt;num_axes(), 1);
    CHECK_EQ(N_, bottom[2]-&gt;shape(0));
  }
  NetParameter net_param;
  LayerParameter* input_layer_param = net_param.add_layer();
  input_layer_param-&gt;set_type("Input");
  InputParameter* input_param = input_layer_param-&gt;mutable_input_param();
  input_layer_param-&gt;add_top("x");
  BlobShape input_shape;
  for (int i = 0; i &lt; bottom[0]-&gt;num_axes(); ++i) {
    input_shape.add_dim(bottom[0]-&gt;shape(i));
  }
  input_param-&gt;add_shape()-&gt;CopyFrom(input_shape);
  input_shape.Clear();
  for (int i = 0; i &lt; bottom[1]-&gt;num_axes(); ++i) {
    input_shape.add_dim(bottom[1]-&gt;shape(i));
  }
  input_layer_param-&gt;add_top("cont");
  input_param-&gt;add_shape()-&gt;CopyFrom(input_shape);
  if (static_input_) {
    input_shape.Clear();
    for (int i = 0; i &lt; bottom[2]-&gt;num_axes(); ++i) {
      input_shape.add_dim(bottom[2]-&gt;shape(i));
    }
    input_layer_param-&gt;add_top("x_static");
    input_param-&gt;add_shape()-&gt;CopyFrom(input_shape);
  }
  this-&gt;FillUnrolledNet(&amp;net_param);
  const string&amp; layer_name = this-&gt;layer_param_.name();
  if (layer_name.size()) {
    for (int i = 0; i &lt; net_param.layer_size(); ++i) {
      LayerParameter* layer = net_param.mutable_layer(i);
      layer-&gt;set_name(layer_name + "_" + layer-&gt;name());
    }
  }
  vector&lt;string&gt; pseudo_losses(output_names.size());
  for (int i = 0; i &lt; output_names.size(); ++i) {
    LayerParameter* layer = net_param.add_layer();
    pseudo_losses[i] = output_names[i] + "_pseudoloss";
    layer-&gt;set_name(pseudo_losses[i]);
    layer-&gt;set_type("Reduction");
    layer-&gt;add_bottom(output_names[i]);
    layer-&gt;add_top(pseudo_losses[i]);
    layer-&gt;add_loss_weight(1);
  }
  unrolled_net_.reset(new Net&lt;Dtype&gt;(net_param));
  unrolled_net_-&gt;set_debug_info(
      this-&gt;layer_param_.recurrent_param().debug_info());
  x_input_blob_ = CHECK_NOTNULL(unrolled_net_-&gt;blob_by_name("x").get());
  cont_input_blob_ = CHECK_NOTNULL(unrolled_net_-&gt;blob_by_name("cont").get());
  if (static_input_) {
    x_static_input_blob_ =
        CHECK_NOTNULL(unrolled_net_-&gt;blob_by_name("x_static").get());
  }
  recur_input_blobs_.resize(num_recur_blobs);
  recur_output_blobs_.resize(num_recur_blobs);
  for (int i = 0; i &lt; recur_input_names.size(); ++i) {
    recur_input_blobs_[i] =
        CHECK_NOTNULL(unrolled_net_-&gt;blob_by_name(recur_input_names[i]).get());
    recur_output_blobs_[i] =
        CHECK_NOTNULL(unrolled_net_-&gt;blob_by_name(recur_output_names[i]).get());
  }
  CHECK_EQ(top.size() - num_hidden_exposed, output_names.size())
      &lt;&lt; "OutputBlobNames must provide an output blob name for each top.";
  output_blobs_.resize(output_names.size());
  for (int i = 0; i &lt; output_names.size(); ++i) {
    output_blobs_[i] =
        CHECK_NOTNULL(unrolled_net_-&gt;blob_by_name(output_names[i]).get());
  }
  CHECK_EQ(2 + num_recur_blobs + static_input_,
           unrolled_net_-&gt;input_blobs().size());
  this-&gt;blobs_.clear();
  for (int i = 0; i &lt; unrolled_net_-&gt;params().size(); ++i) {
    if (unrolled_net_-&gt;param_owners()[i] == -1) {
      LOG(INFO) &lt;&lt; "Adding parameter " &lt;&lt; i &lt;&lt; ": "
                &lt;&lt; unrolled_net_-&gt;param_display_names()[i];
      this-&gt;blobs_.push_back(unrolled_net_-&gt;params()[i]);
    }
<a name="0"></a>  }
<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>  for (int i = 0; i &lt; unrolled_net_-&gt;layers().size(); ++i) {
    for (int j = 0; j &lt; unrolled_net_-&gt;layers()[i]-&gt;blobs().size(); ++j) {
      CHECK(unrolled_net_-&gt;layers()[i]-&gt;param_propagate_down(j))
          &lt;&lt; "param_propagate_down not set for layer " &lt;&lt; i &lt;&lt; ", param " &lt;&lt; j;
    }
  }</b></font>
  this-&gt;param_propagate_down_.clear();
  this-&gt;param_propagate_down_.resize(this-&gt;blobs_.size(), true);
  for (int i = 0; i &lt; recur_output_blobs_.size(); ++i) {
    caffe_set(recur_output_blobs_[i]-&gt;count(), Dtype(0),
              recur_output_blobs_[i]-&gt;mutable_cpu_diff());
  }
  const vector&lt;string&gt;&amp; layer_names = unrolled_net_-&gt;layer_names();
  last_layer_index_ = layer_names.size() - 1 - pseudo_losses.size();
  for (int i = last_layer_index_ + 1, j = 0; i &lt; layer_names.size(); ++i, ++j) {
    CHECK_EQ(layer_names[i], pseudo_losses[j]);
  }
}
template &lt;typename Dtype&gt;
void RecurrentLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  CHECK_GE(bottom[0]-&gt;num_axes(), 2)
      &lt;&lt; "bottom[0] must have at least 2 axes -- (#timesteps, #streams, ...)";
  CHECK_EQ(T_, bottom[0]-&gt;shape(0)) &lt;&lt; "input number of timesteps changed";
  N_ = bottom[0]-&gt;shape(1);
  CHECK_EQ(bottom[1]-&gt;num_axes(), 2)
      &lt;&lt; "bottom[1] must have exactly 2 axes -- (#timesteps, #streams)";
  CHECK_EQ(T_, bottom[1]-&gt;shape(0));
  CHECK_EQ(N_, bottom[1]-&gt;shape(1));
  x_input_blob_-&gt;ReshapeLike(*bottom[0]);
  vector&lt;int&gt; cont_shape = bottom[1]-&gt;shape();
  cont_input_blob_-&gt;Reshape(cont_shape);
  if (static_input_) {
    x_static_input_blob_-&gt;ReshapeLike(*bottom[2]);
  }
  vector&lt;BlobShape&gt; recur_input_shapes;
  RecurrentInputShapes(&amp;recur_input_shapes);
  CHECK_EQ(recur_input_shapes.size(), recur_input_blobs_.size());
  for (int i = 0; i &lt; recur_input_shapes.size(); ++i) {
    recur_input_blobs_[i]-&gt;Reshape(recur_input_shapes[i]);
  }
  unrolled_net_-&gt;Reshape();
  x_input_blob_-&gt;ShareData(*bottom[0]);
  x_input_blob_-&gt;ShareDiff(*bottom[0]);
  cont_input_blob_-&gt;ShareData(*bottom[1]);
  if (static_input_) {
    x_static_input_blob_-&gt;ShareData(*bottom[2]);
    x_static_input_blob_-&gt;ShareDiff(*bottom[2]);
  }
  if (expose_hidden_) {
    const int bottom_offset = 2 + static_input_;
    for (int i = bottom_offset, j = 0; i &lt; bottom.size(); ++i, ++j) {
      CHECK(recur_input_blobs_[j]-&gt;shape() == bottom[i]-&gt;shape())
          &lt;&lt; "shape mismatch - recur_input_blobs_[" &lt;&lt; j &lt;&lt; "]: "
          &lt;&lt; recur_input_blobs_[j]-&gt;shape_string()
          &lt;&lt; " vs. bottom[" &lt;&lt; i &lt;&lt; "]: " &lt;&lt; bottom[i]-&gt;shape_string();
      recur_input_blobs_[j]-&gt;ShareData(*bottom[i]);
    }
  }
  for (int i = 0; i &lt; output_blobs_.size(); ++i) {
    top[i]-&gt;ReshapeLike(*output_blobs_[i]);
    top[i]-&gt;ShareData(*output_blobs_[i]);
    top[i]-&gt;ShareDiff(*output_blobs_[i]);
  }
  if (expose_hidden_) {
    const int top_offset = output_blobs_.size();
    for (int i = top_offset, j = 0; i &lt; top.size(); ++i, ++j) {
      top[i]-&gt;ReshapeLike(*recur_output_blobs_[j]);
    }
  }
}
template &lt;typename Dtype&gt;
void RecurrentLayer&lt;Dtype&gt;::Reset() {
  for (int i = 0; i &lt; recur_output_blobs_.size(); ++i) {
    caffe_set(recur_output_blobs_[i]-&gt;count(), Dtype(0),
              recur_output_blobs_[i]-&gt;mutable_cpu_data());
  }
}
template &lt;typename Dtype&gt;
void RecurrentLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  if (this-&gt;phase_ == TEST) {
    unrolled_net_-&gt;ShareWeights();
  }
  DCHECK_EQ(recur_input_blobs_.size(), recur_output_blobs_.size());
  if (!expose_hidden_) {
    for (int i = 0; i &lt; recur_input_blobs_.size(); ++i) {
      const int count = recur_input_blobs_[i]-&gt;count();
      DCHECK_EQ(count, recur_output_blobs_[i]-&gt;count());
      const Dtype* timestep_T_data = recur_output_blobs_[i]-&gt;cpu_data();
      Dtype* timestep_0_data = recur_input_blobs_[i]-&gt;mutable_cpu_data();
      caffe_copy(count, timestep_T_data, timestep_0_data);
    }
  }
  unrolled_net_-&gt;ForwardTo(last_layer_index_);
  if (expose_hidden_) {
    const int top_offset = output_blobs_.size();
    for (int i = top_offset, j = 0; i &lt; top.size(); ++i, ++j) {
      top[i]-&gt;ShareData(*recur_output_blobs_[j]);
    }
  }
}
template &lt;typename Dtype&gt;
void RecurrentLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  CHECK(!propagate_down[1]) &lt;&lt; "Cannot backpropagate to sequence indicators.";
  unrolled_net_-&gt;BackwardFrom(last_layer_index_);
}
#ifdef CPU_ONLY
STUB_GPU_FORWARD(RecurrentLayer, Forward);
#endif
INSTANTIATE_CLASS(RecurrentLayer);
}  </pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
