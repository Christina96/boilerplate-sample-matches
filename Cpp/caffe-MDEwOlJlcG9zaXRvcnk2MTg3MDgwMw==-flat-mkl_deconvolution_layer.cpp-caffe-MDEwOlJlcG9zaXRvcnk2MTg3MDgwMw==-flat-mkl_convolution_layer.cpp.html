
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 209, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_deconvolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include <algorithm>
3  #include <cstdlib>
4  #include <vector>
5  #include "caffe/filler.hpp"
6  #include "caffe/layer.hpp"
7  #include "caffe/layers/mkl_layers.hpp"
8  #include "caffe/util/performance.hpp"
9  #include "mkl_service.h"
10  #ifdef _OPENMP
11  #include <omp.h>
12  #endif
13  static int getMKLBuildDate() {
14    static int build = 0;
15    if (build == 0) {
16      MKLVersion v;
17      mkl_get_version(&v);
18      build = atoi(v.Build);
19    }
20    return build;
21  }
22  namespace caffe {
23  template <typename Dtype>
24  MKLDeconvolutionLayer<Dtype>::MKLDeconvolutionLayer(
25    const LayerParameter& param)
26        : DeconvolutionLayer<Dtype>(param),
27          fwd_bottom_data(new MKLData<Dtype>()),
28          fwd_top_data(new MKLData<Dtype>()),
29          fwd_filter_data(new MKLData<Dtype>()),
30          fwd_bias_data(new MKLData<Dtype>()),
<span onclick='openModal()' class='match'>31          convolutionFwd(NULL),
32          bwdd_top_diff(new MKLDiff<Dtype>()),
33          bwdd_bottom_diff(new MKLDiff<Dtype>()),
34          bwdd_filter_data(new MKLData<Dtype>()),
35          convolutionBwdData(static_cast<dnnPrimitive_t>(NULL)),
36          bwdf_top_diff(new MKLDiff<Dtype>()),
37          bwdf_filter_diff(new MKLDiff<Dtype>()),
38          bwdf2fwd_filter_diff(new MKLDiff<Dtype>()),
39          bwdf_bottom_data(new MKLData<Dtype>()),
40          convolutionBwdFilter(static_cast<dnnPrimitive_t>(NULL)),
41          bwdb_top_diff(new MKLDiff<Dtype>()),
42          bwdb_bias_diff(new MKLDiff<Dtype>()),
43          convolutionBwdBias(static_cast<dnnPrimitive_t>(NULL)),
44          bwdf_filter_diff_iter(new MKLDiff<Dtype>()),
45          bwdb_bias_diff_iter(new MKLDiff<Dtype>()) {
46            PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
47            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
48            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_prop_);
49            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_diff_);
50            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_bias_);
51          }
52  template <typename Dtype>
53  void MKLDeconvolutionLayer<Dtype>::compute_output_shape() {
54    DeconvolutionLayer<Dtype>::compute_output_shape();
55    this->height_out_ = this->stride_h_ * (this->height_ - 1)
</span>56        + this->kernel_h_ - 2 * this->pad_h_ ;
57    this->width_out_ = this->stride_w_ * (this->width_ - 1)
58        + this->kernel_w_ - 2 * this->pad_w_ ;
59  }
60  template <typename Dtype>
61  MKLDeconvolutionLayer<Dtype>::~MKLDeconvolutionLayer() {
62      dnnDelete<Dtype>(convolutionFwd);
63      dnnDelete<Dtype>(convolutionBwdData);
64      dnnDelete<Dtype>(convolutionBwdFilter);
65      if (this->bias_term_)
66          dnnDelete<Dtype>(convolutionBwdBias);
67  }
68  template <typename Dtype>
69  void MKLDeconvolutionLayer<Dtype>::Init(
70        const vector<Blob<Dtype>*>& bottom,
71        const vector<Blob<Dtype>*>& top) {
72  #ifdef _OPENMP
73    this->num_of_threads_ = omp_get_max_threads() < bottom[0]->shape(0) ?
74                      omp_get_max_threads() : bottom[0]->shape(0);
75    if (this->num_of_threads_ < 1) {
76       LOG(WARNING) << "DeConv layer: omp_get_max_threads() ="
77                    << this->num_of_threads_;
78       this->num_of_threads_ = 1;
79    }
80  #endif
81    this->width_ = bottom[0]->width();
82    this->height_ = bottom[0]->height();
83    this->num_ = bottom[0]->num();
84    kernel_w_ = this->kernel_shape_.cpu_data()[1];
85    kernel_h_ = this->kernel_shape_.cpu_data()[0];
86    stride_w_ = this->stride_.cpu_data()[1];
87    stride_h_ = this->stride_.cpu_data()[0];
88    pad_w_ = this->pad_.cpu_data()[1];
89    pad_h_ = this->pad_.cpu_data()[0];
90    this->bottom_shape_ = &bottom[0]->shape();
91    compute_output_shape();
92    int status;
93    size_t n, g;
94    size_t iw, ih, ic;
95    size_t ow, oh, oc;
96    size_t kw, kh; &bsol;* filter */
97    size_t dimension = 4;
98    g  = std::max(this->group_, 1);
99    n  = this->num_;
100    iw = this->width_;
101    ih = this->height_;
102    ic = this->channels_;
103    ow = this->width_out_;
104    oh = this->height_out_;
105    oc = this->num_output_;
106    kw = this->kernel_w_;
107    kh = this->kernel_h_;
108    size_t bdata_sizes[4] = {iw, ih, ic, n};
109    size_t bdata_strides[4] = {1, iw, iw*ih, iw*ih*ic};
110    size_t g_mkl2017 = g;
111    size_t f_dimension = dimension + (g != 1);
112    if (getMKLBuildDate() < 20160701) {
113        g_mkl2017 = 1;
114        f_dimension = dimension;
115    }
116    size_t fdata_sizes[5] = {kw, kh, oc/g, ic/g_mkl2017, g_mkl2017};
117    size_t fdata_strides[5]  = {1, kw, kw*kh, kw*kh*oc/g, kw*kh*ic/g*oc/g};
118    size_t bias_sizes[1] = {oc};
119    size_t bias_strides[1] = {1};
120    size_t tdata_sizes[4] = {ow, oh, oc, n};
121    size_t tdata_strides[4]  = {1, ow, ow*oh, ow*oh*oc};
122    size_t convolutionStrides[2] = {this->stride_w_, this->stride_h_};
123    int    inputOffset[2] = {-this->pad_w_, -this->pad_h_};
124    fwd_bottom_data ->name = "fwd_bottom_data   @ " + this->layer_param_.name();
125    fwd_top_data    ->name = "fwd_top_data      @ " + this->layer_param_.name();
126    fwd_filter_data ->name = "fwd_filter_data   @ " + this->layer_param_.name();
127    fwd_bias_data   ->name = "fwd_bias_data     @ " + this->layer_param_.name();
128    bwdd_top_diff   ->name = "bwdd_top_diff     @ " + this->layer_param_.name();
129    bwdd_bottom_diff->name = "bwdd_bottom_diff  @ " + this->layer_param_.name();
130    bwdd_filter_data->name = "bwdd_filter_data  @ " + this->layer_param_.name();
131    bwdf_top_diff   ->name = "bwdf_top_diff     @ " + this->layer_param_.name();
132    bwdf_bottom_data->name = "bwdf_bottom_data  @ " + this->layer_param_.name();
133    bwdf_filter_diff->name = "bwdf_filter_diff  @ " + this->layer_param_.name();
134    bwdf2fwd_filter_diff->name =
135                         "bwdf2fwd_filter_diff  @ " + this->layer_param_.name();
136    bwdb_top_diff   ->name = "bwdb_top_diff     @ " + this->layer_param_.name();
137    bwdb_bias_diff  ->name = "bwdb_bias_diff    @ " + this->layer_param_.name();
138    dnnDelete<Dtype>(convolutionBwdData);
139    status = dnnGroupsConvolutionCreateBackwardData<Dtype>(
140      &convolutionBwdData,
141      NULL,
142      dnnAlgorithmConvolutionDirect,
143      g,
144      dimension,
145      tdata_sizes,
146      bdata_sizes,
147      fdata_sizes,
148      convolutionStrides,
149      inputOffset,
150      dnnBorderZeros);
151    CHECK_EQ(status, 0)
152            << "Failed dnnConvolutionCreateBackwardData with status "
153            << status << "\n";
154    fwd_bottom_data->create_layouts(convolutionBwdData, dnnResourceDiffDst, dimension,
155                                    bdata_sizes, bdata_strides);
156    fwd_top_data   ->create_layouts(convolutionBwdData, dnnResourceDiffSrc, dimension,
157                                    tdata_sizes, tdata_strides);
158    fwd_filter_data->create_layouts(convolutionBwdData, dnnResourceFilter,
159                                    f_dimension, fdata_sizes, fdata_strides);
160    dnnDelete<Dtype>(convolutionFwd);
161    status = dnnGroupsConvolutionCreateForward<Dtype>(
162            &convolutionFwd,
163            NULL,
164            dnnAlgorithmConvolutionDirect,
165            g,
166            dimension,
167            tdata_sizes,
168            bdata_sizes,
169            fdata_sizes,
170            convolutionStrides,
171            inputOffset,
172            dnnBorderZeros);
173    CHECK_EQ(status, 0)
174            << "Failed dnnCreateConvolution<Dtype>(dnnForward) with status "
175            << status << "\n";
176    bwdd_bottom_diff->create_layouts(convolutionFwd, dnnResourceDst,
177                                     dimension, bdata_sizes, bdata_strides);
178    bwdd_top_diff   ->create_layouts(convolutionFwd, dnnResourceSrc,
179                                     dimension, tdata_sizes, tdata_strides);
180    bwdd_filter_data->create_layouts(convolutionFwd, dnnResourceFilter,
181                                     f_dimension, fdata_sizes, fdata_strides);
182    dnnDelete<Dtype>(convolutionBwdFilter);
183    status = dnnGroupsConvolutionCreateBackwardFilter<Dtype>(
184      &convolutionBwdFilter,
185      NULL,
186      dnnAlgorithmConvolutionDirect,
187      g,
188      dimension,
189      tdata_sizes,
190      bdata_sizes,
191      fdata_sizes,
192      convolutionStrides,
193      inputOffset,
194      dnnBorderZeros);
195    CHECK_EQ(status, 0)
196            << "Failed dnnConvolutionCreateBackwardFilter with status "
197            << status << "\n";
198    bwdf_bottom_data->create_layouts(convolutionBwdFilter, dnnResourceDiffDst,
199                                     dimension, bdata_sizes, bdata_strides);
200    bwdf_top_diff   ->create_layouts(convolutionBwdFilter, dnnResourceSrc,
201                                     dimension, tdata_sizes, tdata_strides);
202    bwdf_filter_diff->create_layouts(convolutionBwdData, dnnResourceFilter,
203                                     f_dimension, fdata_sizes, fdata_strides);
204    bwdf_filter_diff_iter->create_layouts(convolutionFwd, dnnResourceFilter,
205                                     f_dimension, fdata_sizes, fdata_strides);
206    if (getMKLBuildDate() > 20160701) {
207      bwdf2fwd_filter_diff->create_internal_layout(convolutionBwdFilter,
208          dnnResourceDiffFilter);
209      bwdf2fwd_filter_diff->remove_user_layout();
210      status = dnnLayoutCreateFromPrimitive<Dtype>(
211          &bwdf2fwd_filter_diff->layout_usr, convolutionBwdData, dnnResourceFilter);
212      CHECK_EQ(status, 0) << "Failed dnnLayoutCreateFromPrimitive with status "
213              << status << "\n";
214      bwdf2fwd_filter_diff->create_conversions();
215    }
216    if (this->bias_term_) {
217      dnnDelete<Dtype>(convolutionBwdBias);
218      status = dnnGroupsConvolutionCreateBackwardBias<Dtype>(
219        &convolutionBwdBias,
220        NULL,
221        dnnAlgorithmConvolutionDirect,
222        g,
223        dimension,
224        tdata_sizes);
225      CHECK_EQ(status, 0)
226              << "Failed dnnConvolutionCreateBackwardBias with status "
227              << status << "\n";
228      bwdb_top_diff->create_layouts(convolutionBwdBias, dnnResourceDiffDst,
229                                    dimension, tdata_sizes, tdata_strides);
230      bwdb_bias_diff->create_layouts(convolutionBwdBias, dnnResourceDiffBias,
231                                     1, bias_sizes, bias_strides);
232      bwdb_bias_diff_iter->create_layouts(convolutionBwdBias, dnnResourceDiffBias,
233                                          1, bias_sizes, bias_strides);
234    }
235  }
236  template <typename Dtype>
237  void MKLDeconvolutionLayer<Dtype>::LayerSetUp(
238        const vector<Blob<Dtype>*>& bottom,
239        const vector<Blob<Dtype>*>& top) {
240    DeconvolutionLayer<Dtype>::LayerSetUp(bottom, top);
241    Init(bottom, top);
242  }
243  template <typename Dtype>
244  void MKLDeconvolutionLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
245        const vector<Blob<Dtype>*>& top) {
246    bool reinitialize = (this->width_ == bottom[0]->width() &&
247                         this->height_ == bottom[0]->height() &&
248                         this->channels_ == bottom[0]->channels() &&
249                         this->num_ == bottom[0]->num()) ? false : true;
250    BaseConvolutionLayer<Dtype>::ReshapeForMKL(bottom, top);
251    if (reinitialize == true) {
252      Init(bottom, top);
253    }
254  }
255  template <typename Dtype>
256  void MKLDeconvolutionLayer<Dtype>::Forward_cpu(
257    const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
258    int status;
259    size_t n, g;
260    size_t iw, ih, ic;
261    size_t ow, oh, oc;
262    g  = this->group_;
263    n  = this->num_;
264    iw = this->width_;
265    ih = this->height_;
266    ic = this->channels_/g;
267    CHECK(bottom[0]->width()    == iw &&
268          bottom[0]->height()   == ih &&
269          bottom[0]->channels() == ic*g &&
270          bottom[0]->num()      == n)
271            << "Inclompatible shape of bottom with layer";
272    ow = this->width_out_;
273    oh = this->height_out_;
274    oc = this->num_output_/g;
275    CHECK(top[0]->width()    == ow &&
276          top[0]->height()   == oh &&
277          top[0]->channels() == oc*g &&
278          top[0]->num()      == n) << "Inclompatible shape of bottom with layer";
279    void *res_convolutionBwdData[dnnResourceNumber];
280    res_convolutionBwdData[dnnResourceDiffDst] =
281        fwd_bottom_data->get_converted_prv(bottom[0], false);
282    res_convolutionBwdData[dnnResourceFilter]  =
283        fwd_filter_data->get_converted_prv(this->blobs_[0].get(), true);
284    if (fwd_top_data->conversion_needed()) {
285        top[0]->set_prv_data_descriptor(fwd_top_data);
286        res_convolutionBwdData[dnnResourceDiffSrc] =
287            reinterpret_cast<void *>(top[0]->mutable_prv_data());
288    } else {
289        res_convolutionBwdData[dnnResourceDiffSrc] =
290            top[0]->mutable_cpu_data();
291    }
292    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME("FW"));
293    PERFORMANCE_MEASUREMENT_BEGIN();
294    status = dnnExecute<Dtype>(convolutionBwdData, res_convolutionBwdData);
295    PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
296    CHECK_EQ(status, 0) << "Forward deconvolution failed with status " << status;
297    if (this->bias_term_) {
298        const Dtype* bias = this->blobs_[1]->cpu_data();
299        Dtype* top_data = top[0]->mutable_cpu_data();
300  #ifdef _OPENMP
301  #   pragma omp parallel for num_threads(this->num_of_threads_)
302  #endif
303        for (int n = 0; n < this->num_; ++n) {
304            this->forward_cpu_bias(top_data + n * this->top_dim_, bias);
305        }
306    }
307  }
308  template <typename Dtype>
309  void MKLDeconvolutionLayer<Dtype>::Backward_cpu(
310    const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down,
311    const vector<Blob<Dtype>*>& bottom) {
312    int status;
313    size_t n, g;
314    size_t iw, ih, ic;
315    size_t ow, oh, oc;
316    g  = this->group_;
317    n  = this->num_;
318    iw = this->width_;
319    ih = this->height_;
320    ic = this->channels_/g;
321    CHECK(bottom[0]->width()    == iw &&
322          bottom[0]->height()   == ih &&
323          bottom[0]->channels() == ic*g &&
324          bottom[0]->num()      == n)
325            << "Incompatible shape of bottom with layer";
326    ow = this->width_out_;
327    oh = this->height_out_;
328    oc = this->num_output_/g;
329    CHECK(top[0]->width()    == ow &&
330          top[0]->height()   == oh &&
331          top[0]->channels() == oc*g &&
332          top[0]->num()      == n) << "Incompatible shape of top with layer";
333    if (propagate_down[0]) {
334        void *res_convolutionFwd[dnnResourceNumber];
335        res_convolutionFwd[dnnResourceSrc] =
336            bwdd_top_diff->get_converted_prv(top[0], true);
337        res_convolutionFwd[dnnResourceFilter] =
338            bwdd_filter_data->get_converted_prv(this->blobs_[0].get(), false);
339      if (bwdd_bottom_diff->conversion_needed()) {
340        bottom[0]->set_prv_diff_descriptor(bwdd_bottom_diff);
341        res_convolutionFwd[dnnResourceDst] =
342            bottom[0]->mutable_prv_diff();
343      } else {
344        res_convolutionFwd[dnnResourceDst] =
345            bottom[0]->mutable_cpu_diff();
346      }
347      PERFORMANCE_EVENT_ID_INIT(perf_id_bw_prop_,
348          PERFORMANCE_MKL_NAME_DETAILED("BW", "_prop"));
349      PERFORMANCE_MEASUREMENT_BEGIN();
350      status = dnnExecute<Dtype>(convolutionFwd, res_convolutionFwd);
351      PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_prop_);
352      CHECK_EQ(status, 0) << "Backward Data deconv failed with status " << status;
353    }
354    if (this->param_propagate_down(0)) {
355      void *res_convolutionBwdFilter[dnnResourceNumber];
356      res_convolutionBwdFilter[dnnResourceDiffDst] =
357          bwdf_bottom_data->get_converted_prv(bottom[0], false);
358      res_convolutionBwdFilter[dnnResourceSrc] =
359              bwdf_top_diff->get_converted_prv(top[0], false);
360      if (bwdf_filter_diff->conversion_needed()) {
361        this->blobs_[0]->set_prv_diff_descriptor(bwdf_filter_diff);
362      }
363      if (bwdf2fwd_filter_diff->conversion_needed()) {
364        res_convolutionBwdFilter[dnnResourceDiffFilter] =
365                reinterpret_cast<void *>(bwdf2fwd_filter_diff->prv_ptr());
366      } else {
367        if (Caffe::iter_size() > 1) {
368          res_convolutionBwdFilter[dnnResourceDiffFilter] =
369                bwdf_filter_diff_iter->prv_ptr();
370        } else {
371          if (bwdf_filter_diff->conversion_needed()) {
372            res_convolutionBwdFilter[dnnResourceDiffFilter] =
373                  this->blobs_[0]->mutable_prv_diff();
374          } else {
375          res_convolutionBwdFilter[dnnResourceDiffFilter] =
376                this->blobs_[0]->mutable_cpu_diff();
377          }
378        }
379      }
380      PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKL_NAME("BW"));
381      PERFORMANCE_MEASUREMENT_BEGIN();
382      status = dnnExecute<Dtype>(convolutionBwdFilter, res_convolutionBwdFilter);
383      PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
384      CHECK_EQ(status, 0) << "Backward Filter conv failed with status " << status;
385      if (bwdf2fwd_filter_diff->conversion_needed()) {
386        void *convert_resources[dnnResourceNumber];
387        convert_resources[dnnResourceFrom] = bwdf2fwd_filter_diff->prv_ptr();
388        if (Caffe::iter_size() > 1) {
389          convert_resources[dnnResourceTo] =
390                bwdf_filter_diff_iter->prv_ptr();
391          if (bwdf_filter_diff->conversion_needed())
392            DLOG(INFO) << "convert priv => priv  " << bwdf2fwd_filter_diff->name
393                       << " => " << bwdf_filter_diff->name;
394          else
395            DLOG(INFO) << "convert priv =>       " << bwdf2fwd_filter_diff->name
396                       << " =>";
397        } else {
398          if (bwdf_filter_diff->conversion_needed()) {
399            convert_resources[dnnResourceTo] =
400                  this->blobs_[0]->mutable_prv_diff();
401            DLOG(INFO) << "convert priv => priv  " << bwdf2fwd_filter_diff->name
402                       << " => " << bwdf_filter_diff->name;
403          } else {
404            convert_resources[dnnResourceTo] =
405                  this->blobs_[0]->mutable_cpu_diff();
406            DLOG(INFO) << "convert priv =>       " << bwdf2fwd_filter_diff->name
407                       << " =>";
408          }
409        }
410        PERFORMANCE_EVENT_ID_INIT(perf_id_bw_diff_,
411            PERFORMANCE_MKL_NAME_DETAILED("BW", "_diff"));
412        PERFORMANCE_MEASUREMENT_BEGIN();
413        status = dnnExecute<Dtype>(bwdf2fwd_filter_diff->convert_from_int,
414                convert_resources);
415        PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_diff_);
416        CHECK_EQ(status, 0) << "Conversion failed with status " << status;
417      }
418      if (Caffe::iter_size() > 1) {
419        if (bwdf_filter_diff->conversion_needed()) {
420          caffe_axpy<Dtype>((const int)this->blobs_[0]->prv_diff_count(), 1,
421                reinterpret_cast<Dtype*>(bwdf_filter_diff_iter->prv_ptr()),
422                this->blobs_[0]->mutable_prv_diff());
423        } else {
424          caffe_axpy<Dtype>((const int)this->blobs_[0]->count(), 1,
425                reinterpret_cast<Dtype*>(bwdf_filter_diff_iter->prv_ptr()),
426                this->blobs_[0]->mutable_cpu_diff());
427        }
428      }
429    }
430    if (this->param_propagate_down(1)) {
431      void *res_convolutionBwdBias[dnnResourceNumber];
432      res_convolutionBwdBias[dnnResourceDiffDst] =
433              bwdb_top_diff->get_converted_prv(top[0], true);
434      if (Caffe::iter_size() > 1) {
435        res_convolutionBwdBias[dnnResourceDiffBias] =
436              bwdb_bias_diff_iter->prv_ptr();
437      } else {
438        if (bwdb_bias_diff->conversion_needed()) {
439          this->blobs_[1]->set_prv_diff_descriptor(bwdb_bias_diff);
440            res_convolutionBwdBias[dnnResourceDiffBias] =
441                reinterpret_cast<void *>(this->blobs_[1]->mutable_prv_diff());
442        } else {
443          res_convolutionBwdBias[dnnResourceDiffBias] =
444              reinterpret_cast<void *>(this->blobs_[1]->mutable_cpu_diff());
445        }
446      }
447      PERFORMANCE_EVENT_ID_INIT(perf_id_bw_bias_,
448          PERFORMANCE_MKL_NAME_DETAILED("BW", "_bias"));
449      PERFORMANCE_MEASUREMENT_BEGIN();
450      status = dnnExecute<Dtype>(convolutionBwdBias, res_convolutionBwdBias);
451      PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_bias_);
452      CHECK_EQ(status, 0) << "Backward Bias failed with status " << status;
453      if (Caffe::iter_size() > 1) {
454        if (bwdb_bias_diff->conversion_needed()) {
455          caffe_axpy<Dtype>((const int)this->blobs_[1]->prv_diff_count(), 1,
456                reinterpret_cast<Dtype*>(bwdb_bias_diff_iter->prv_ptr()),
457                this->blobs_[1]->mutable_prv_diff());
458        } else {
459          caffe_axpy<Dtype>((const int)this->blobs_[1]->count(), 1,
460                reinterpret_cast<Dtype*>(bwdb_bias_diff_iter->prv_ptr()),
461                this->blobs_[1]->mutable_cpu_diff());
462        }
463      }
464    }
465  }
466  #ifdef CPU_ONLY
467  STUB_GPU(MKLDeconvolutionLayer);
468  #else
469  template <typename Dtype>
470  void MKLDeconvolutionLayer<Dtype>::Forward_gpu(
471      const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
472    {NOT_IMPLEMENTED;}
473  template <typename Dtype>
474  void MKLDeconvolutionLayer<Dtype>::Backward_gpu(
475      const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down,
476      const vector<Blob<Dtype>*>& bottom)
477    {NOT_IMPLEMENTED;}
478  #endif
479  INSTANTIATE_CLASS(MKLDeconvolutionLayer);
480  }  
481  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_convolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include <algorithm>
3  #include <cstdlib>
4  #include <vector>
5  #include "caffe/filler.hpp"
6  #include "caffe/layer.hpp"
7  #include "caffe/layers/mkl_layers.hpp"
8  #include "caffe/util/performance.hpp"
9  #include "mkl_service.h"
10  static int getMKLBuildDate() {
11    static int build = 0;
12    if (build == 0) {
13      MKLVersion v;
14      mkl_get_version(&v);
15      build = atoi(v.Build);
16    }
17    return build;
18  }
19  namespace caffe {
20  template <typename Dtype>
21  MKLConvolutionLayer<Dtype>::MKLConvolutionLayer(
22    const LayerParameter& param)
23        : ConvolutionLayer<Dtype>(param),
24          fwd_bottom_data(new MKLData<Dtype>()),
25          fwd_top_data(new MKLData<Dtype>()),
26          fwd_filter_data(new MKLData<Dtype>()),
27          fwd_bias_data(new MKLData<Dtype>()),
<span onclick='openModal()' class='match'>28          convolutionFwd(NULL),
29          bwdd_top_diff(new MKLDiff<Dtype>()),
30          bwdd_bottom_diff(new MKLDiff<Dtype>()),
31          bwdd_filter_data(new MKLData<Dtype>()),
32          convolutionBwdData(static_cast<dnnPrimitive_t>(NULL)),
33          bwdf_top_diff(new MKLDiff<Dtype>()),
34          bwdf_filter_diff(new MKLDiff<Dtype>()),
35          bwdf2fwd_filter_diff(new MKLDiff<Dtype>()),
36          bwdf_bottom_data(new MKLData<Dtype>()),
37          convolutionBwdFilter(static_cast<dnnPrimitive_t>(NULL)),
38          bwdb_top_diff(new MKLDiff<Dtype>()),
39          bwdb_bias_diff(new MKLDiff<Dtype>()),
40          convolutionBwdBias(static_cast<dnnPrimitive_t>(NULL)),
41          bwdf_filter_diff_iter(new MKLDiff<Dtype>()),
42          bwdb_bias_diff_iter(new MKLDiff<Dtype>()) {
43            PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
44            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
45            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_prop_);
46            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_diff_);
47            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_bias_);
48          }
49  template <typename Dtype>
50  void MKLConvolutionLayer<Dtype>::compute_output_shape() {
51    ConvolutionLayer<Dtype>::compute_output_shape();
52    this->height_out_ = (this->height_ + 2 * this->pad_h_ - this->kernel_h_)
</span>53        / this->stride_h_ + 1;
54    this->width_out_ = (this->width_ + 2 * this->pad_w_ - this->kernel_w_)
55        / this->stride_w_ + 1;
56  }
57  template <typename Dtype>
58  void MKLConvolutionLayer<Dtype>::CreateFwdPrimitive() {
59    int status;
60    size_t g = std::max(this->group_, 1);
61    size_t dimension = 4;
62    dnnDelete<Dtype>(convolutionFwd);
63    if (this->bias_term_) {
64      status = dnnGroupsConvolutionCreateForwardBias<Dtype>(
65        &convolutionFwd,
66        NULL,
67        dnnAlgorithmConvolutionDirect,
68        g,
69        dimension,
70        bdata_sizes,
71        tdata_sizes,
72        fdata_sizes,
73        convolutionStrides,
74        inputOffset,
75        dnnBorderZeros);
76    } else {
77      status = dnnGroupsConvolutionCreateForward<Dtype>(
78        &convolutionFwd,
79        NULL,
80        dnnAlgorithmConvolutionDirect,
81        g,
82        dimension,
83        bdata_sizes,
84        tdata_sizes,
85        fdata_sizes,
86        convolutionStrides,
87        inputOffset,
88        dnnBorderZeros);
89    }
90    CHECK_EQ(status, 0)
91            << "Failed dnnCreateConvolution<Dtype>(dnnForward) with status "
92            << status << "\n";
93    fwd_bottom_data->create_layouts(convolutionFwd, dnnResourceSrc, dimension,
94                                    bdata_sizes, bdata_strides);
95    fwd_top_data   ->create_layouts(convolutionFwd, dnnResourceDst, dimension,
96                                    tdata_sizes, tdata_strides);
97    fwd_filter_data->create_layouts(convolutionFwd, dnnResourceFilter,
98                                    f_dimension, fdata_sizes, fdata_strides);
99    if (this->bias_term_)
100      fwd_bias_data->create_layouts(convolutionFwd, dnnResourceBias, 1,
101                                    bias_sizes, bias_strides);
102  }
103  template <typename Dtype>
104  void MKLConvolutionLayer<Dtype>::CreateBwdDataPrimitive() {
105    int status;
106    size_t g = std::max(this->group_, 1);
107    size_t dimension = 4;
108    dnnDelete<Dtype>(convolutionBwdData);
109    status = dnnGroupsConvolutionCreateBackwardData<Dtype>(
110      &convolutionBwdData,
111      NULL,
112      dnnAlgorithmConvolutionDirect,
113      g,
114      dimension,
115      bdata_sizes,
116      tdata_sizes,
117      fdata_sizes,
118      convolutionStrides,
119      inputOffset,
120      dnnBorderZeros);
121    CHECK_EQ(status, 0)
122            << "Failed dnnConvolutionCreateBackwardData with status "
123            << status << "\n";
124    bwdd_bottom_diff->create_layouts(convolutionBwdData, dnnResourceDiffSrc,
125                                     dimension, bdata_sizes, bdata_strides);
126    bwdd_top_diff   ->create_layouts(convolutionBwdData, dnnResourceDiffDst,
127                                     dimension, tdata_sizes, tdata_strides);
128    bwdd_filter_data->create_layouts(convolutionBwdData, dnnResourceFilter,
129                                     f_dimension, fdata_sizes, fdata_strides);
130  }
131  template <typename Dtype>
132  void MKLConvolutionLayer<Dtype>::CreateBwdFilterPrimitive() {
133    int status;
134    size_t g = std::max(this->group_, 1);
135    size_t dimension = 4;
136    dnnDelete<Dtype>(convolutionBwdFilter);
137    status = dnnGroupsConvolutionCreateBackwardFilter<Dtype>(
138      &convolutionBwdFilter,
139      NULL,
140      dnnAlgorithmConvolutionDirect,
141      g,
142      dimension,
143      bdata_sizes,
144      tdata_sizes,
145      fdata_sizes,
146      convolutionStrides,
147      inputOffset,
148      dnnBorderZeros);
149    CHECK_EQ(status, 0)
150            << "Failed dnnConvolutionCreateBackwardFilter with status "
151            << status << "\n";
152    bwdf_bottom_data->create_layouts(convolutionBwdFilter, dnnResourceSrc,
153                                     dimension, bdata_sizes, bdata_strides);
154    bwdf_top_diff   ->create_layouts(convolutionBwdFilter, dnnResourceDiffDst,
155                                     dimension, tdata_sizes, tdata_strides);
156    bwdf_filter_diff->create_layouts(convolutionBwdFilter, dnnResourceDiffFilter,
157                                     f_dimension, fdata_sizes, fdata_strides);
158    bwdf_filter_diff_iter->create_layouts(convolutionBwdFilter, dnnResourceDiffFilter,
159                                     f_dimension, fdata_sizes, fdata_strides);
160    if (getMKLBuildDate() > 20160701) {
161      bwdf2fwd_filter_diff->create_internal_layout(convolutionBwdFilter,
162          dnnResourceDiffFilter);
163      bwdf2fwd_filter_diff->remove_user_layout();
164      status = dnnLayoutCreateFromPrimitive<Dtype>(
165          &bwdf2fwd_filter_diff->layout_usr, convolutionBwdFilter, dnnResourceDiffFilter);
166      CHECK_EQ(status, 0) << "Failed dnnLayoutCreateFromPrimitive with status "
167              << status << "\n";
168      bwdf2fwd_filter_diff->create_conversions();
169    }
170  }
171  template <typename Dtype>
172  void MKLConvolutionLayer<Dtype>::CreateBwdBiasPrimitive() {
173    int status;
174    size_t g = std::max(this->group_, 1);
175    size_t dimension = 4;
176    if (this->bias_term_) {
177      dnnDelete<Dtype>(convolutionBwdBias);
178      status = dnnGroupsConvolutionCreateBackwardBias<Dtype>(
179        &convolutionBwdBias,
180        NULL,
181        dnnAlgorithmConvolutionDirect,
182        g,
183        dimension,
184        tdata_sizes);
185      CHECK_EQ(status, 0)
186              << "Failed dnnConvolutionCreateBackwardBias with status "
187              << status << "\n";
188      bwdb_top_diff->create_layouts(convolutionBwdBias, dnnResourceDiffDst,
189                                    dimension, tdata_sizes, tdata_strides);
190      bwdb_bias_diff->create_layouts(convolutionBwdBias, dnnResourceDiffBias,
191                                     1, bias_sizes, bias_strides);
192      bwdb_bias_diff_iter->create_layouts(convolutionBwdBias, dnnResourceDiffBias,
193                                          1, bias_sizes, bias_strides);
194    }
195  }
196  template <typename Dtype>
197  MKLConvolutionLayer<Dtype>::~MKLConvolutionLayer() {
198      dnnDelete<Dtype>(convolutionFwd);
199      dnnDelete<Dtype>(convolutionBwdData);
200      dnnDelete<Dtype>(convolutionBwdFilter);
201      if (this->bias_term_)
202          dnnDelete<Dtype>(convolutionBwdBias);
203  }
204  template <typename Dtype>
205  void MKLConvolutionLayer<Dtype>::Init(
206        const vector<Blob<Dtype>*>& bottom,
207        const vector<Blob<Dtype>*>& top) {
208    this->width_ = bottom[0]->width();
209    this->height_ = bottom[0]->height();
210    this->num_ = bottom[0]->num();
211    kernel_w_ = this->kernel_shape_.cpu_data()[1];
212    kernel_h_ = this->kernel_shape_.cpu_data()[0];
213    stride_w_ = this->stride_.cpu_data()[1];
214    stride_h_ = this->stride_.cpu_data()[0];
215    pad_w_ = this->pad_.cpu_data()[1];
216    pad_h_ = this->pad_.cpu_data()[0];
217    this->bottom_shape_ = &bottom[0]->shape();
218    compute_output_shape();
219    size_t n, g;
220    size_t iw, ih, ic;
221    size_t ow, oh, oc;
222    size_t kw, kh; &bsol;* filter */
223    size_t dimension = 4;
224    g  = std::max(this->group_, 1);
225    n  = this->num_;
226    iw = this->width_;
227    ih = this->height_;
228    ic = this->channels_;
229    ow = this->width_out_;
230    oh = this->height_out_;
231    oc = this->num_output_;
232    kw = this->kernel_w_;
233    kh = this->kernel_h_;
234    this->bdata_sizes[0] = iw;
235    this->bdata_sizes[1] = ih;
236    this->bdata_sizes[2] = ic;
237    this->bdata_sizes[3] = n;
238    this->bdata_strides[0] = 1;
239    this->bdata_strides[1] = iw;
240    this->bdata_strides[2] = iw*ih;
241    this->bdata_strides[3] = iw*ih*ic;
242    size_t g_mkl2017 = g;
243    f_dimension = dimension + (g != 1);
244    if (getMKLBuildDate() < 20160701) {
245        g_mkl2017 = 1;
246        f_dimension = dimension;
247    }
248    this->fdata_sizes[0] = kw;
249    this->fdata_sizes[1] = kh;
250    this->fdata_sizes[2] = ic/g;
251    this->fdata_sizes[3] = oc/g_mkl2017;
252    this->fdata_sizes[4] = g_mkl2017;
253    this->fdata_strides[0] = 1;
254    this->fdata_strides[1] = kw;
255    this->fdata_strides[2] = kw*kh;
256    this->fdata_strides[3] = kw*kh*ic/g;
257    this->fdata_strides[4] = kw*kh*ic/g*oc/g;
258    this->bias_sizes[0] = oc;
259    this->bias_strides[0] = 1;
260    this->tdata_sizes[0] = ow;
261    this->tdata_sizes[1] = oh;
262    this->tdata_sizes[2] = oc;
263    this->tdata_sizes[3] = n;
264    this->tdata_strides[0]  = 1;
265    this->tdata_strides[1]  = ow;
266    this->tdata_strides[2]  = ow*oh;
267    this->tdata_strides[3]  = ow*oh*oc;
268    this->convolutionStrides[0] = this->stride_w_;
269    this->convolutionStrides[1] = this->stride_h_;
270    this->inputOffset[0] = -this->pad_w_;
271    this->inputOffset[1] = -this->pad_h_;
272    fwd_bottom_data ->name = "fwd_bottom_data   @ " + this->layer_param_.name();
273    fwd_top_data    ->name = "fwd_top_data      @ " + this->layer_param_.name();
274    fwd_filter_data ->name = "fwd_filter_data   @ " + this->layer_param_.name();
275    fwd_bias_data   ->name = "fwd_bias_data     @ " + this->layer_param_.name();
276    bwdd_top_diff   ->name = "bwdd_top_diff     @ " + this->layer_param_.name();
277    bwdd_bottom_diff->name = "bwdd_bottom_diff  @ " + this->layer_param_.name();
278    bwdd_filter_data->name = "bwdd_filter_data  @ " + this->layer_param_.name();
279    bwdf_top_diff   ->name = "bwdf_top_diff     @ " + this->layer_param_.name();
280    bwdf_bottom_data->name = "bwdf_bottom_data  @ " + this->layer_param_.name();
281    bwdf_filter_diff->name = "bwdf_filter_diff  @ " + this->layer_param_.name();
282    bwdf2fwd_filter_diff->name =
283                         "bwdf2fwd_filter_diff  @ " + this->layer_param_.name();
284    bwdb_top_diff   ->name = "bwdb_top_diff     @ " + this->layer_param_.name();
285    bwdb_bias_diff  ->name = "bwdb_bias_diff    @ " + this->layer_param_.name();
286    CreateFwdPrimitive();
287  }
288  template <typename Dtype>
289  void MKLConvolutionLayer<Dtype>::LayerSetUp(
290        const vector<Blob<Dtype>*>& bottom,
291        const vector<Blob<Dtype>*>& top) {
292    ConvolutionLayer<Dtype>::LayerSetUp(bottom, top);
293    Init(bottom, top);
294  }
295  template <typename Dtype>
296  void MKLConvolutionLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
297        const vector<Blob<Dtype>*>& top) {
298    this->reshape = (this->width_ == bottom[0]->width() &&
299                     this->height_ == bottom[0]->height() &&
300                     this->channels_ == bottom[0]->channels() &&
301                     this->num_ == bottom[0]->num()) ? false : true;
302    BaseConvolutionLayer<Dtype>::ReshapeForMKL(bottom, top);
303    if (this->reshape == true) {
304      this->blobs_[0]->mutable_cpu_data();
305      this->blobs_[0]->mutable_cpu_diff();
306      if (this->bias_term_) {
307        this->blobs_[1]->mutable_cpu_data();
308        this->blobs_[1]->mutable_cpu_diff();
309      }
310      Init(bottom, top);
311    }
312  }
313  template <typename Dtype>
314  void MKLConvolutionLayer<Dtype>::Forward_cpu(
315    const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
316    int status;
317    size_t n, g;
318    size_t iw, ih, ic;
319    size_t ow, oh, oc;
320    g  = this->group_;
321    n  = this->num_;
322    iw = this->width_;
323    ih = this->height_;
324    ic = this->channels_/g;
325    CHECK(bottom[0]->width()    == iw &&
326          bottom[0]->height()   == ih &&
327          bottom[0]->channels() == ic*g &&
328          bottom[0]->num()      == n)
329            << "Inclompatible shape of bottom with layer";
330    ow = this->width_out_;
331    oh = this->height_out_;
332    oc = this->num_output_/g;
333    CHECK(top[0]->width()    == ow &&
334          top[0]->height()   == oh &&
335          top[0]->channels() == oc*g &&
336          top[0]->num()      == n) << "Inclompatible shape of bottom with layer";
337    void *res_convolutionFwd[dnnResourceNumber];
338    res_convolutionFwd[dnnResourceSrc] =
339      fwd_bottom_data->get_converted_prv(bottom[0], false);
340    res_convolutionFwd[dnnResourceFilter] =
341      fwd_filter_data->get_converted_prv(this->blobs_[0].get(), true);
342    if (this->bias_term_) {
343      res_convolutionFwd[dnnResourceBias] =
344        fwd_bias_data  ->get_converted_prv(this->blobs_[1].get(), true);
345    }
346    if (fwd_top_data->conversion_needed()) {
347      top[0]->set_prv_data_descriptor(fwd_top_data);
348      res_convolutionFwd[dnnResourceDst] =
349              reinterpret_cast<void *>(top[0]->mutable_prv_data());
350    } else {
351      res_convolutionFwd[dnnResourceDst] = top[0]->mutable_cpu_data();
352    }
353    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME("FW"));
354    PERFORMANCE_MEASUREMENT_BEGIN();
355    status = dnnExecute<Dtype>(convolutionFwd, res_convolutionFwd);
356    PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
357    CHECK_EQ(status, 0) << "Forward convolution failed with status " << status;
358  }
359  template <typename Dtype>
360  void MKLConvolutionLayer<Dtype>::Backward_cpu(
361    const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down,
362    const vector<Blob<Dtype>*>& bottom) {
363    int status;
364    size_t n, g;
365    size_t iw, ih, ic;
366    size_t ow, oh, oc;
367    g  = this->group_;
368    n  = this->num_;
369    iw = this->width_;
370    ih = this->height_;
371    ic = this->channels_/g;
372    CHECK(bottom[0]->width()    == iw &&
373          bottom[0]->height()   == ih &&
374          bottom[0]->channels() == ic*g &&
375          bottom[0]->num()      == n)
376            << "Incompatible shape of bottom with layer";
377    ow = this->width_out_;
378    oh = this->height_out_;
379    oc = this->num_output_/g;
380    CHECK(top[0]->width()    == ow &&
381          top[0]->height()   == oh &&
382          top[0]->channels() == oc*g &&
383          top[0]->num()      == n) << "Incompatible shape of bottom with layer";
384    if (propagate_down[0]) {
385      void *res_convolutionBwdData[dnnResourceNumber];
386      if ((convolutionBwdData == NULL) || this->reshape)
387        CreateBwdDataPrimitive();
388      res_convolutionBwdData[dnnResourceDiffDst] =
389        bwdd_top_diff->get_converted_prv(top[0], true);
390      res_convolutionBwdData[dnnResourceFilter]  =
391        bwdd_filter_data->get_converted_prv(this->blobs_[0].get(), false);
392      if (bwdd_bottom_diff->conversion_needed()) {
393        bottom[0]->set_prv_diff_descriptor(bwdd_bottom_diff);
394        res_convolutionBwdData[dnnResourceDiffSrc] =
395                bottom[0]->mutable_prv_diff();
396      } else {
397        res_convolutionBwdData[dnnResourceDiffSrc] =
398                bottom[0]->mutable_cpu_diff();
399      }
400      PERFORMANCE_EVENT_ID_INIT(perf_id_bw_prop_,
401          PERFORMANCE_MKL_NAME_DETAILED("BW", "_prop"));
402      PERFORMANCE_MEASUREMENT_BEGIN();
403      status = dnnExecute<Dtype>(convolutionBwdData, res_convolutionBwdData);
404      PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_prop_);
405      CHECK_EQ(status, 0) << "Backward Data conv failed with status " << status;
406    }
407    if (this->param_propagate_down(0)) {
408      void *res_convolutionBwdFilter[dnnResourceNumber];
409      if ((convolutionBwdFilter == NULL) || this->reshape)
410        CreateBwdFilterPrimitive();
411      res_convolutionBwdFilter[dnnResourceDiffDst] =
412              bwdf_top_diff->get_converted_prv(top[0], true);
413      res_convolutionBwdFilter[dnnResourceSrc] =
414              bwdf_bottom_data->get_converted_prv(bottom[0], false,
415              fwd_bottom_data.get());
416      if (bwdf_filter_diff->conversion_needed()) {
417        this->blobs_[0]->set_prv_diff_descriptor(bwdf_filter_diff);
418      }
419      if (bwdf2fwd_filter_diff->conversion_needed()) {
420        res_convolutionBwdFilter[dnnResourceDiffFilter] =
421                reinterpret_cast<void *>(bwdf2fwd_filter_diff->prv_ptr());
422      } else {
423        if (Caffe::iter_size() > 1) {
424          res_convolutionBwdFilter[dnnResourceDiffFilter] =
425                bwdf_filter_diff_iter->prv_ptr();
426        } else {
427          if (bwdf_filter_diff->conversion_needed()) {
428            res_convolutionBwdFilter[dnnResourceDiffFilter] =
429                  this->blobs_[0]->mutable_prv_diff();
430          } else {
431          res_convolutionBwdFilter[dnnResourceDiffFilter] =
432                this->blobs_[0]->mutable_cpu_diff();
433          }
434        }
435      }
436      PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKL_NAME("BW"));
437      PERFORMANCE_MEASUREMENT_BEGIN();
438      status = dnnExecute<Dtype>(convolutionBwdFilter, res_convolutionBwdFilter);
439      PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
440      CHECK_EQ(status, 0) << "Backward Filter conv failed with status " << status;
441      if (bwdf2fwd_filter_diff->conversion_needed()) {
442        void *convert_resources[dnnResourceNumber];
443        convert_resources[dnnResourceFrom] = bwdf2fwd_filter_diff->prv_ptr();
444        if (Caffe::iter_size() > 1) {
445          convert_resources[dnnResourceTo] =
446                bwdf_filter_diff_iter->prv_ptr();
447          if (bwdf_filter_diff->conversion_needed())
448            DLOG(INFO) << "convert priv => priv  " << bwdf2fwd_filter_diff->name
449                       << " => " << bwdf_filter_diff->name;
450          else
451            DLOG(INFO) << "convert priv =>       " << bwdf2fwd_filter_diff->name
452                       << " =>";
453        } else {
454          if (bwdf_filter_diff->conversion_needed()) {
455            convert_resources[dnnResourceTo] =
456                  this->blobs_[0]->mutable_prv_diff();
457            DLOG(INFO) << "convert priv => priv  " << bwdf2fwd_filter_diff->name
458                       << " => " << bwdf_filter_diff->name;
459          } else {
460            convert_resources[dnnResourceTo] =
461                  this->blobs_[0]->mutable_cpu_diff();
462            DLOG(INFO) << "convert priv =>       " << bwdf2fwd_filter_diff->name
463                       << " =>";
464          }
465        }
466        PERFORMANCE_EVENT_ID_INIT(perf_id_bw_diff_,
467            PERFORMANCE_MKL_NAME_DETAILED("BW", "_diff"));
468        PERFORMANCE_MEASUREMENT_BEGIN();
469        status = dnnExecute<Dtype>(bwdf2fwd_filter_diff->convert_from_int,
470                convert_resources);
471        PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_diff_);
472        CHECK_EQ(status, 0) << "Conversion failed with status " << status;
473      }
474      if (Caffe::iter_size() > 1) {
475        if (bwdf_filter_diff->conversion_needed()) {
476          caffe_axpy<Dtype>((const int)this->blobs_[0]->prv_diff_count(), 1,
477                reinterpret_cast<Dtype*>(bwdf_filter_diff_iter->prv_ptr()),
478                this->blobs_[0]->mutable_prv_diff());
479        } else {
480          caffe_axpy<Dtype>((const int)this->blobs_[0]->count(), 1,
481                reinterpret_cast<Dtype*>(bwdf_filter_diff_iter->prv_ptr()),
482                this->blobs_[0]->mutable_cpu_diff());
483        }
484      }
485    }
486    if (this->param_propagate_down(1)) {
487      void *res_convolutionBwdBias[dnnResourceNumber];
488      if ((convolutionBwdBias == NULL) || this->reshape)
489        CreateBwdBiasPrimitive();
490      res_convolutionBwdBias[dnnResourceDiffDst] =
491              bwdb_top_diff->get_converted_prv(top[0], true);
492      if (Caffe::iter_size() > 1) {
493        res_convolutionBwdBias[dnnResourceDiffBias] =
494              bwdb_bias_diff_iter->prv_ptr();
495      } else {
496        if (bwdb_bias_diff->conversion_needed()) {
497          this->blobs_[1]->set_prv_diff_descriptor(bwdb_bias_diff);
498            res_convolutionBwdBias[dnnResourceDiffBias] =
499                reinterpret_cast<void *>(this->blobs_[1]->mutable_prv_diff());
500        } else {
501          res_convolutionBwdBias[dnnResourceDiffBias] =
502              reinterpret_cast<void *>(this->blobs_[1]->mutable_cpu_diff());
503        }
504      }
505      PERFORMANCE_EVENT_ID_INIT(perf_id_bw_bias_,
506          PERFORMANCE_MKL_NAME_DETAILED("BW", "_bias"));
507      PERFORMANCE_MEASUREMENT_BEGIN();
508      status = dnnExecute<Dtype>(convolutionBwdBias, res_convolutionBwdBias);
509      PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_bias_);
510      CHECK_EQ(status, 0) << "Backward Bias failed with status " << status;
511      if (Caffe::iter_size() > 1) {
512        if (bwdb_bias_diff->conversion_needed()) {
513          caffe_axpy<Dtype>((const int)this->blobs_[1]->prv_diff_count(), 1,
514                reinterpret_cast<Dtype*>(bwdb_bias_diff_iter->prv_ptr()),
515                this->blobs_[1]->mutable_prv_diff());
516        } else {
517          caffe_axpy<Dtype>((const int)this->blobs_[1]->count(), 1,
518                reinterpret_cast<Dtype*>(bwdb_bias_diff_iter->prv_ptr()),
519                this->blobs_[1]->mutable_cpu_diff());
520        }
521      }
522    }
523  }
524  #ifdef CPU_ONLY
525  STUB_GPU(MKLConvolutionLayer);
526  #else
527  template <typename Dtype>
528  void MKLConvolutionLayer<Dtype>::Forward_gpu(
529      const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
530    {NOT_IMPLEMENTED;}
531  template <typename Dtype>
532  void MKLConvolutionLayer<Dtype>::Backward_gpu(
533      const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down,
534      const vector<Blob<Dtype>*>& bottom)
535    {NOT_IMPLEMENTED;}
536  #endif
537  INSTANTIATE_CLASS(MKLConvolutionLayer);
538  }  
539  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_deconvolution_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_convolution_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>31          convolutionFwd(NULL),
32          bwdd_top_diff(new MKLDiff<Dtype>()),
33          bwdd_bottom_diff(new MKLDiff<Dtype>()),
34          bwdd_filter_data(new MKLData<Dtype>()),
35          convolutionBwdData(static_cast<dnnPrimitive_t>(NULL)),
36          bwdf_top_diff(new MKLDiff<Dtype>()),
37          bwdf_filter_diff(new MKLDiff<Dtype>()),
38          bwdf2fwd_filter_diff(new MKLDiff<Dtype>()),
39          bwdf_bottom_data(new MKLData<Dtype>()),
40          convolutionBwdFilter(static_cast<dnnPrimitive_t>(NULL)),
41          bwdb_top_diff(new MKLDiff<Dtype>()),
42          bwdb_bias_diff(new MKLDiff<Dtype>()),
43          convolutionBwdBias(static_cast<dnnPrimitive_t>(NULL)),
44          bwdf_filter_diff_iter(new MKLDiff<Dtype>()),
45          bwdb_bias_diff_iter(new MKLDiff<Dtype>()) {
46            PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
47            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
48            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_prop_);
49            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_diff_);
50            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_bias_);
51          }
52  template <typename Dtype>
53  void MKLDeconvolutionLayer<Dtype>::compute_output_shape() {
54    DeconvolutionLayer<Dtype>::compute_output_shape();
55    this->height_out_ = this->stride_h_ * (this->height_ - 1)
</pre></code></div>
                <div class="column column_space"><pre><code>28          convolutionFwd(NULL),
29          bwdd_top_diff(new MKLDiff<Dtype>()),
30          bwdd_bottom_diff(new MKLDiff<Dtype>()),
31          bwdd_filter_data(new MKLData<Dtype>()),
32          convolutionBwdData(static_cast<dnnPrimitive_t>(NULL)),
33          bwdf_top_diff(new MKLDiff<Dtype>()),
34          bwdf_filter_diff(new MKLDiff<Dtype>()),
35          bwdf2fwd_filter_diff(new MKLDiff<Dtype>()),
36          bwdf_bottom_data(new MKLData<Dtype>()),
37          convolutionBwdFilter(static_cast<dnnPrimitive_t>(NULL)),
38          bwdb_top_diff(new MKLDiff<Dtype>()),
39          bwdb_bias_diff(new MKLDiff<Dtype>()),
40          convolutionBwdBias(static_cast<dnnPrimitive_t>(NULL)),
41          bwdf_filter_diff_iter(new MKLDiff<Dtype>()),
42          bwdb_bias_diff_iter(new MKLDiff<Dtype>()) {
43            PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
44            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
45            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_prop_);
46            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_diff_);
47            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_bias_);
48          }
49  template <typename Dtype>
50  void MKLConvolutionLayer<Dtype>::compute_output_shape() {
51    ConvolutionLayer<Dtype>::compute_output_shape();
52    this->height_out_ = (this->height_ + 2 * this->pad_h_ - this->kernel_h_)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    