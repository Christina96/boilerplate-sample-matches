<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for tanh_layer.cpp & pooling_layer.cpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for tanh_layer.cpp & pooling_layer.cpp
      </h3>
      <h1 align="center">
        4.9%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>tanh_layer.cpp (24.489796%)<TH>pooling_layer.cpp (2.7522936%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match3321-0.html#0',2,'match3321-1.html#0',3)" NAME="0">(10-15)<TD><A HREF="javascript:ZweiFrames('match3321-0.html#0',2,'match3321-1.html#0',3)" NAME="0">(140-145)</A><TD ALIGN=center><FONT COLOR="#ff0000">12</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>tanh_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
// TanH neuron activation function layer.
// Adapted from ReLU layer code written by Yangqing Jia

#include &lt;vector&gt;

#include &quot;caffe/layers/tanh_layer.hpp&quot;
<A NAME="0"></A>
namespace caffe {

<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match3321-1.html#0',3,'match3321-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>template &lt;typename Dtype&gt;
void TanHLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  const int count = bottom[0]-&gt;count();</B></FONT>
  for (int i = 0; i &lt; count; ++i) {
    top_data[i] = tanh(bottom_data[i]);
  }
}

template &lt;typename Dtype&gt;
void TanHLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (propagate_down[0]) {
    const Dtype* top_data = top[0]-&gt;cpu_data();
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    const int count = bottom[0]-&gt;count();
    Dtype tanhx;
    for (int i = 0; i &lt; count; ++i) {
      tanhx = top_data[i];
      bottom_diff[i] = top_diff[i] * (1 - tanhx * tanhx);
    }
  }
}

#ifdef CPU_ONLY
STUB_GPU(TanHLayer);
#endif

INSTANTIATE_CLASS(TanHLayer);

}  // namespace caffe
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>pooling_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;algorithm&gt;
#include &lt;cfloat&gt;
#include &lt;vector&gt;

#include &quot;caffe/layers/pooling_layer.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {

using std::min;
using std::max;

template &lt;typename Dtype&gt;
void PoolingLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  PoolingParameter pool_param = this-&gt;layer_param_.pooling_param();
  if (pool_param.global_pooling()) {
    CHECK(!(pool_param.has_kernel_size() ||
      pool_param.has_kernel_h() || pool_param.has_kernel_w()))
      &lt;&lt; &quot;With Global_pooling: true Filter size cannot specified&quot;;
  } else {
    CHECK(!pool_param.has_kernel_size() !=
      !(pool_param.has_kernel_h() &amp;&amp; pool_param.has_kernel_w()))
      &lt;&lt; &quot;Filter size is kernel_size OR kernel_h and kernel_w; not both&quot;;
    CHECK(pool_param.has_kernel_size() ||
      (pool_param.has_kernel_h() &amp;&amp; pool_param.has_kernel_w()))
      &lt;&lt; &quot;For non-square filters both kernel_h and kernel_w are required.&quot;;
  }
  CHECK((!pool_param.has_pad() &amp;&amp; pool_param.has_pad_h()
      &amp;&amp; pool_param.has_pad_w())
      || (!pool_param.has_pad_h() &amp;&amp; !pool_param.has_pad_w()))
      &lt;&lt; &quot;pad is pad OR pad_h and pad_w are required.&quot;;
  CHECK((!pool_param.has_stride() &amp;&amp; pool_param.has_stride_h()
      &amp;&amp; pool_param.has_stride_w())
      || (!pool_param.has_stride_h() &amp;&amp; !pool_param.has_stride_w()))
      &lt;&lt; &quot;Stride is stride OR stride_h and stride_w are required.&quot;;
  global_pooling_ = pool_param.global_pooling();
  round_mode_ = pool_param.round_mode();
  if (global_pooling_) {
    kernel_h_ = bottom[0]-&gt;height();
    kernel_w_ = bottom[0]-&gt;width();
  } else {
    if (pool_param.has_kernel_size()) {
      kernel_h_ = kernel_w_ = pool_param.kernel_size();
    } else {
      kernel_h_ = pool_param.kernel_h();
      kernel_w_ = pool_param.kernel_w();
    }
  }
  CHECK_GT(kernel_h_, 0) &lt;&lt; &quot;Filter dimensions cannot be zero.&quot;;
  CHECK_GT(kernel_w_, 0) &lt;&lt; &quot;Filter dimensions cannot be zero.&quot;;
  if (!pool_param.has_pad_h()) {
    pad_h_ = pad_w_ = pool_param.pad();
  } else {
    pad_h_ = pool_param.pad_h();
    pad_w_ = pool_param.pad_w();
  }
  if (!pool_param.has_stride_h()) {
    stride_h_ = stride_w_ = pool_param.stride();
  } else {
    stride_h_ = pool_param.stride_h();
    stride_w_ = pool_param.stride_w();
  }
  if (global_pooling_) {
    CHECK(pad_h_ == 0 &amp;&amp; pad_w_ == 0 &amp;&amp; stride_h_ == 1 &amp;&amp; stride_w_ == 1)
      &lt;&lt; &quot;With Global_pooling: true; only pad = 0 and stride = 1&quot;;
  }
  if (pad_h_ != 0 || pad_w_ != 0) {
    CHECK(this-&gt;layer_param_.pooling_param().pool()
        == PoolingParameter_PoolMethod_AVE
        || this-&gt;layer_param_.pooling_param().pool()
        == PoolingParameter_PoolMethod_MAX)
        &lt;&lt; &quot;Padding implemented only for average and max pooling.&quot;;
    CHECK_LT(pad_h_, kernel_h_);
    CHECK_LT(pad_w_, kernel_w_);
  }
}

template &lt;typename Dtype&gt;
void PoolingLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  CHECK_EQ(4, bottom[0]-&gt;num_axes()) &lt;&lt; &quot;Input must have 4 axes, &quot;
      &lt;&lt; &quot;corresponding to (num, channels, height, width)&quot;;
  channels_ = bottom[0]-&gt;channels();
  height_ = bottom[0]-&gt;height();
  width_ = bottom[0]-&gt;width();
  if (global_pooling_) {
    kernel_h_ = bottom[0]-&gt;height();
    kernel_w_ = bottom[0]-&gt;width();
  }
  switch (round_mode_) {
  case PoolingParameter_RoundMode_CEIL:
    pooled_height_ = static_cast&lt;int&gt;(ceil(static_cast&lt;float&gt;(
        height_ + 2 * pad_h_ - kernel_h_) / stride_h_)) + 1;
    pooled_width_ = static_cast&lt;int&gt;(ceil(static_cast&lt;float&gt;(
        width_ + 2 * pad_w_ - kernel_w_) / stride_w_)) + 1;
    break;
  case PoolingParameter_RoundMode_FLOOR:
    pooled_height_ = static_cast&lt;int&gt;(floor(static_cast&lt;float&gt;(
        height_ + 2 * pad_h_ - kernel_h_) / stride_h_)) + 1;
    pooled_width_ = static_cast&lt;int&gt;(floor(static_cast&lt;float&gt;(
        width_ + 2 * pad_w_ - kernel_w_) / stride_w_)) + 1;
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown rounding mode.&quot;;
  }
  if (pad_h_ || pad_w_) {
    // If we have padding, ensure that the last pooling starts strictly
    // inside the image (instead of at the padding); otherwise clip the last.
    if ((pooled_height_ - 1) * stride_h_ &gt;= height_ + pad_h_) {
      --pooled_height_;
    }
    if ((pooled_width_ - 1) * stride_w_ &gt;= width_ + pad_w_) {
      --pooled_width_;
    }
    CHECK_LT((pooled_height_ - 1) * stride_h_, height_ + pad_h_);
    CHECK_LT((pooled_width_ - 1) * stride_w_, width_ + pad_w_);
  }
  top[0]-&gt;Reshape(bottom[0]-&gt;num(), channels_, pooled_height_,
      pooled_width_);
  if (top.size() &gt; 1) {
    top[1]-&gt;ReshapeLike(*top[0]);
  }
  // If max pooling, we will initialize the vector index part.
  if (this-&gt;layer_param_.pooling_param().pool() ==
      PoolingParameter_PoolMethod_MAX &amp;&amp; top.size() == 1) {
    max_idx_.Reshape(bottom[0]-&gt;num(), channels_, pooled_height_,
        pooled_width_);
  }
  // If stochastic pooling, we will initialize the random index part.
  if (this-&gt;layer_param_.pooling_param().pool() ==
      PoolingParameter_PoolMethod_STOCHASTIC) {
    rand_idx_.Reshape(bottom[0]-&gt;num(), channels_, pooled_height_,
      pooled_width_);
  }
}
<A NAME="0"></A>
// TODO(Yangqing): Is there a faster way to do pooling in the channel-first
// case?
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match3321-0.html#0',2,'match3321-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>template &lt;typename Dtype&gt;
void PoolingLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  const int top_count = top[0]-&gt;count();</B></FONT>
  // We'll output the mask to top[1] if it's of size &gt;1.
  const bool use_top_mask = top.size() &gt; 1;
  int* mask = NULL;  // suppress warnings about uninitialized variables
  Dtype* top_mask = NULL;
  // Different pooling methods. We explicitly do the switch outside the for
  // loop to save time, although this results in more code.
  switch (this-&gt;layer_param_.pooling_param().pool()) {
  case PoolingParameter_PoolMethod_MAX:
    // Initialize
    if (use_top_mask) {
      top_mask = top[1]-&gt;mutable_cpu_data();
      caffe_set(top_count, Dtype(-1), top_mask);
    } else {
      mask = max_idx_.mutable_cpu_data();
      caffe_set(top_count, -1, mask);
    }
    caffe_set(top_count, Dtype(-FLT_MAX), top_data);
    // The main loop
    for (int n = 0; n &lt; bottom[0]-&gt;num(); ++n) {
      for (int c = 0; c &lt; channels_; ++c) {
        for (int ph = 0; ph &lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &lt; pooled_width_; ++pw) {
            int hstart = ph * stride_h_ - pad_h_;
            int wstart = pw * stride_w_ - pad_w_;
            int hend = min(hstart + kernel_h_, height_);
            int wend = min(wstart + kernel_w_, width_);
            hstart = max(hstart, 0);
            wstart = max(wstart, 0);
            const int pool_index = ph * pooled_width_ + pw;
            for (int h = hstart; h &lt; hend; ++h) {
              for (int w = wstart; w &lt; wend; ++w) {
                const int index = h * width_ + w;
                if (bottom_data[index] &gt; top_data[pool_index]) {
                  top_data[pool_index] = bottom_data[index];
                  if (use_top_mask) {
                    top_mask[pool_index] = static_cast&lt;Dtype&gt;(index);
                  } else {
                    mask[pool_index] = index;
                  }
                }
              }
            }
          }
        }
        // compute offset
        bottom_data += bottom[0]-&gt;offset(0, 1);
        top_data += top[0]-&gt;offset(0, 1);
        if (use_top_mask) {
          top_mask += top[0]-&gt;offset(0, 1);
        } else {
          mask += top[0]-&gt;offset(0, 1);
        }
      }
    }
    break;
  case PoolingParameter_PoolMethod_AVE:
    for (int i = 0; i &lt; top_count; ++i) {
      top_data[i] = 0;
    }
    // The main loop
    for (int n = 0; n &lt; bottom[0]-&gt;num(); ++n) {
      for (int c = 0; c &lt; channels_; ++c) {
        for (int ph = 0; ph &lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &lt; pooled_width_; ++pw) {
            int hstart = ph * stride_h_ - pad_h_;
            int wstart = pw * stride_w_ - pad_w_;
            int hend = min(hstart + kernel_h_, height_ + pad_h_);
            int wend = min(wstart + kernel_w_, width_ + pad_w_);
            int pool_size = (hend - hstart) * (wend - wstart);
            hstart = max(hstart, 0);
            wstart = max(wstart, 0);
            hend = min(hend, height_);
            wend = min(wend, width_);
            for (int h = hstart; h &lt; hend; ++h) {
              for (int w = wstart; w &lt; wend; ++w) {
                top_data[ph * pooled_width_ + pw] +=
                    bottom_data[h * width_ + w];
              }
            }
            top_data[ph * pooled_width_ + pw] /= pool_size;
          }
        }
        // compute offset
        bottom_data += bottom[0]-&gt;offset(0, 1);
        top_data += top[0]-&gt;offset(0, 1);
      }
    }
    break;
  case PoolingParameter_PoolMethod_STOCHASTIC:
    NOT_IMPLEMENTED;
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown pooling method.&quot;;
  }
}

template &lt;typename Dtype&gt;
void PoolingLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (!propagate_down[0]) {
    return;
  }
  const Dtype* top_diff = top[0]-&gt;cpu_diff();
  Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
  // Different pooling methods. We explicitly do the switch outside the for
  // loop to save time, although this results in more codes.
  caffe_set(bottom[0]-&gt;count(), Dtype(0), bottom_diff);
  // We'll output the mask to top[1] if it's of size &gt;1.
  const bool use_top_mask = top.size() &gt; 1;
  const int* mask = NULL;  // suppress warnings about uninitialized variables
  const Dtype* top_mask = NULL;
  switch (this-&gt;layer_param_.pooling_param().pool()) {
  case PoolingParameter_PoolMethod_MAX:
    // The main loop
    if (use_top_mask) {
      top_mask = top[1]-&gt;cpu_data();
    } else {
      mask = max_idx_.cpu_data();
    }
    for (int n = 0; n &lt; top[0]-&gt;num(); ++n) {
      for (int c = 0; c &lt; channels_; ++c) {
        for (int ph = 0; ph &lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &lt; pooled_width_; ++pw) {
            const int index = ph * pooled_width_ + pw;
            const int bottom_index =
                use_top_mask ? top_mask[index] : mask[index];
            bottom_diff[bottom_index] += top_diff[index];
          }
        }
        bottom_diff += bottom[0]-&gt;offset(0, 1);
        top_diff += top[0]-&gt;offset(0, 1);
        if (use_top_mask) {
          top_mask += top[0]-&gt;offset(0, 1);
        } else {
          mask += top[0]-&gt;offset(0, 1);
        }
      }
    }
    break;
  case PoolingParameter_PoolMethod_AVE:
    // The main loop
    for (int n = 0; n &lt; top[0]-&gt;num(); ++n) {
      for (int c = 0; c &lt; channels_; ++c) {
        for (int ph = 0; ph &lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &lt; pooled_width_; ++pw) {
            int hstart = ph * stride_h_ - pad_h_;
            int wstart = pw * stride_w_ - pad_w_;
            int hend = min(hstart + kernel_h_, height_ + pad_h_);
            int wend = min(wstart + kernel_w_, width_ + pad_w_);
            int pool_size = (hend - hstart) * (wend - wstart);
            hstart = max(hstart, 0);
            wstart = max(wstart, 0);
            hend = min(hend, height_);
            wend = min(wend, width_);
            for (int h = hstart; h &lt; hend; ++h) {
              for (int w = wstart; w &lt; wend; ++w) {
                bottom_diff[h * width_ + w] +=
                  top_diff[ph * pooled_width_ + pw] / pool_size;
              }
            }
          }
        }
        // offset
        bottom_diff += bottom[0]-&gt;offset(0, 1);
        top_diff += top[0]-&gt;offset(0, 1);
      }
    }
    break;
  case PoolingParameter_PoolMethod_STOCHASTIC:
    NOT_IMPLEMENTED;
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown pooling method.&quot;;
  }
}


#ifdef CPU_ONLY
STUB_GPU(PoolingLayer);
#endif

INSTANTIATE_CLASS(PoolingLayer);

}  // namespace caffe
</PRE>
</div>
  </div>
</body>
</html>
