
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 20, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-net.cpp</h3>
            <pre><code>1  #include &lt;algorithm&gt;
2  #include &lt;map&gt;
3  #include &lt;set&gt;
4  #include &lt;string&gt;
5  #include &lt;utility&gt;
6  #include &lt;vector&gt;
7  #include &lt;numeric&gt;
8  #include &quot;hdf5.h&quot;
9  #include &quot;boost/algorithm/string.hpp&quot;
10  #include &quot;caffe/common.hpp&quot;
11  #include &quot;caffe/layer.hpp&quot;
12  #include &quot;caffe/net.hpp&quot;
13  #include &quot;caffe/parallel.hpp&quot;
14  #include &quot;caffe/proto/caffe.pb.h&quot;
15  #include &quot;caffe/util/cpu_info.hpp&quot;
16  #include &quot;caffe/util/hdf5.hpp&quot;
17  #include &quot;caffe/util/insert_splits.hpp&quot;
18  #include &quot;caffe/util/math_functions.hpp&quot;
19  #include &quot;caffe/util/performance.hpp&quot;
20  #include &quot;caffe/util/upgrade_proto.hpp&quot;
21  #include &quot;caffe/multinode/mlsl.hpp&quot;
22  #include &quot;caffe/multinode/apply_mn_param.hpp&quot;
23  #include &quot;caffe/util/remove_batch_norm.hpp&quot;
24  #include &quot;caffe/util/apply_bn_stats_batch_size.hpp&quot;
25  #include &quot;caffe/syncedmem.hpp&quot;
26  PERFORMANCE_CREATE_MONITOR();
27  namespace caffe {
28  #ifdef CAFFE_PER_LAYER_TIMINGS
29  #define LAYER_TIMING_START(name, index) do { \
30    if (this-&gt;phase() == TRAIN) { \
31      this-&gt;name##_start_time_per_layer[index] = this-&gt;timer.Duration(); \
32    } \
33  }while(0)
34  #define LAYER_TIMING_STOP(name, index) do { \
35    if (this-&gt;phase() == TRAIN) { \
36      this-&gt;name##_stop_time_per_layer[index] = this-&gt;timer.Duration(); \
37      this-&gt;name##_time_per_layer[index] += (this-&gt;name##_stop_time_per_layer[index] - this-&gt;name##_start_time_per_layer[index]); \
38    } \
39  }while(0)
40  #define ITER_TIMING_START() do { \
41    if (this-&gt;phase() == TRAIN) { \
42      this-&gt;timer.Start(); \
43    } \
44  }while(0)
45  #define ITER_TIMING_STOP(name) do { \
46    if (this-&gt;phase() == TRAIN) { \
47      this-&gt;name##_time_per_iter += this-&gt;timer.MicroSeconds(); \
48    } \
49  }while(0)
50  #else
51  #define LAYER_TIMING_START(name,index)
52  #define LAYER_TIMING_STOP(name,index)
53  #define ITER_TIMING_START()
54  #define ITER_TIMING_STOP(name)
55  #endif &amp;bsol;* CAFFE_PER_LAYER_TIMINGS */
56  template &lt;typename Dtype&gt;
57  Net&lt;Dtype&gt;::Net(const NetParameter&amp; param, const Net* root_net)
58      : root_net_(root_net) {
59    Init(param);
60  }
61  template &lt;typename Dtype&gt;
62  Net&lt;Dtype&gt;::Net(const string&amp; param_file, Phase phase,
63      const int level, const vector&lt;string&gt;* stages,
64      const Net* root_net, std::string engine)
65      : root_net_(root_net) {
66    NetParameter param;
67    ReadNetParamsFromTextFileOrDie(param_file, &amp;param);
68    param.mutable_state()-&gt;set_phase(phase);
69    if (stages != NULL) {
70      for (int i = 0; i &lt; stages-&gt;size(); i++) {
71        param.mutable_state()-&gt;add_stage((*stages)[i]);
72      }
73    }
74    param.mutable_state()-&gt;set_level(level);
75    if (engine != &quot;&quot;)
76      param.set_engine(engine);
77    Init(param);
78  }
79  template &lt;typename Dtype&gt;
80  void Net&lt;Dtype&gt;::Init(const NetParameter&amp; in_param) {
81    CHECK(Caffe::root_solver() || root_net_)
82        &lt;&lt; &quot;root_net_ needs to be set for all non-root solvers&quot;;
83  #ifdef _OPENMP
84    static bool executed = false;
85    if (!executed) {
86      if (Caffe::mode() == Caffe::GPU) {
87        caffe::cpu::OpenMpManager::setGpuEnabled();
88      } else {
89        caffe::cpu::OpenMpManager::setGpuDisabled();
90      }
91      caffe::cpu::OpenMpManager::bindOpenMpThreads();
92      caffe::cpu::OpenMpManager::printVerboseInformation();
93    }
94  #endif
95    phase_ = in_param.state().phase();
96    NetParameter filtered_param;
97    FilterNet(in_param, &amp;filtered_param);
98  #ifdef USE_MKL2017_AS_DEFAULT_ENGINE
99    if (filtered_param.engine() == &quot;&quot;)
100      filtered_param.set_engine(&quot;MKL2017&quot;);
101  #endif
102  #ifdef USE_MKLDNN_AS_DEFAULT_ENGINE
103    if (filtered_param.engine() == &quot;&quot;)
104      filtered_param.set_engine(&quot;MKLDNN&quot;);
105  #endif
106    engine_name_ = filtered_param.engine();
107    NetParameter&amp; param = filtered_param;
108    NetParameter param_with_splits;
109    InsertSplits(param, &amp;param_with_splits);
110    param = param_with_splits;
111    NetParameter compiled_param;
112    CompileNet(param, &amp;compiled_param);
113    param = compiled_param;
114    this-&gt;bn_scale_remove_ = param.compile_net_state().bn_scale_remove();
115    this-&gt;bn_scale_merge_ = param.compile_net_state().bn_scale_merge();
116    int kept_bn_layers_num = param.compile_net_state().kept_bn_layers_size();
117    for (int idx = 0; idx &lt; kept_bn_layers_num; ++idx) {
118      this-&gt;kept_bn_layers_.push_back(param.compile_net_state().kept_bn_layers(idx));
119    }
120    NetParameter param_with_stats_batch_size;
121    if (param.has_bn_stats_batch_size()) {
122      ApplyBnStatsBatchSize(param, &amp;param_with_stats_batch_size);
123      param = param_with_stats_batch_size;
124    }
125  #ifdef USE_MLSL
126    NetParameter param_with_mn;
127    if (mn::is_multinode()) {
128      ApplyMultinodeParams&lt;Dtype&gt;(param, &amp;param_with_mn);
129      param = param_with_mn;
130    }
131  #endif
132    if (Caffe::root_solver()) {
133      LOG(INFO) &lt;&lt; &quot;Initializing net from parameters: &quot; &lt;&lt; std::endl;
134      LOG(INFO).flush();
135      fflush(0);
136      param.PrintDebugString();
137      fflush(0);
138    }
139    name_ = param.name();
140    map&lt;string, int&gt; blob_name_to_idx;
141    set&lt;string&gt; available_blobs;
142    memory_used_ = 0;
143    bottom_vecs_.resize(param.layer_size());
144    top_vecs_.resize(param.layer_size());
145    bottom_id_vecs_.resize(param.layer_size());
146    param_id_vecs_.resize(param.layer_size());
147    top_id_vecs_.resize(param.layer_size());
148    bottom_need_backward_.resize(param.layer_size());
149    max_blob_count = 0;
150    for (int layer_id = 0; layer_id &lt; param.layer_size(); ++layer_id) {
151      bool share_from_root = !Caffe::root_solver()
152          &amp;&amp; root_net_-&gt;layers_[layer_id]-&gt;ShareInParallel();
153      if (!param.layer(layer_id).has_phase()) {
154        param.mutable_layer(layer_id)-&gt;set_phase(phase_);
155      }
156      const LayerParameter&amp; layer_param = param.layer(layer_id);
157      if (param.engine() != &quot;&quot;) {
158        if (param.layer(layer_id).engine() == &quot;&quot;) {
159          param.mutable_layer(layer_id)-&gt;set_engine(param.engine());
160        }
161        else {
162          if ((!param.layer(layer_id).engine().compare(&quot;MKL2017&quot;) &amp;&amp; !param.engine().compare(&quot;MKLDNN&quot;)) 
163             || (!param.layer(layer_id).engine().compare(&quot;MKLDNN&quot;) &amp;&amp; !param.engine().compare(&quot;MKL2017&quot;))) {
164            param.mutable_layer(layer_id)-&gt;set_engine(param.engine());
165          }
166        }
167      }
168      if (layer_param.propagate_down_size() &gt; 0) {
169        CHECK_EQ(layer_param.propagate_down_size(),
170            layer_param.bottom_size())
171            &lt;&lt; &quot;propagate_down param must be specified &quot;
172            &lt;&lt; &quot;either 0 or bottom_size times &quot;;
173      }
174      if (share_from_root) {
175        LOG(INFO) &lt;&lt; &quot;Sharing layer &quot; &lt;&lt; layer_param.name() &lt;&lt; &quot; from root net&quot;;
176        layers_.push_back(root_net_-&gt;layers_[layer_id]);
177        layers_[layer_id]-&gt;SetShared(true);
178      } else {
179        layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));
180      }
181      layer_names_.push_back(layer_param.name());
182      LOG_IF(INFO, Caffe::root_solver())
183          &lt;&lt; &quot;Creating Layer &quot; &lt;&lt; layer_param.name();
184      bool need_backward = false;
185      for (int bottom_id = 0; bottom_id &lt; layer_param.bottom_size();
186           ++bottom_id) {
187        const int blob_id = AppendBottom(param, layer_id, bottom_id,
188                                         &amp;available_blobs, &amp;blob_name_to_idx);
189        need_backward |= blob_need_backward_[blob_id];
190      }
191      int num_top = layer_param.top_size();
192      for (int top_id = 0; top_id &lt; num_top; ++top_id) {
193        AppendTop(param, layer_id, top_id, &amp;available_blobs, &amp;blob_name_to_idx);
194        if (layer_param.type() == &quot;Input&quot;) {
195          const int blob_id = blobs_.size() - 1;
196          net_input_blob_indices_.push_back(blob_id);
197          net_input_blobs_.push_back(blobs_[blob_id].get());
198        }
199      }
200      Layer&lt;Dtype&gt;* layer = layers_[layer_id].get();
201      if (layer-&gt;AutoTopBlobs()) {
202        const int needed_num_top =
203            std::max(layer-&gt;MinTopBlobs(), layer-&gt;ExactNumTopBlobs());
204        for (; num_top &lt; needed_num_top; ++num_top) {
205          AppendTop(param, layer_id, num_top, NULL, NULL);
206        }
207      }
208  #ifdef USE_MLSL
209      if (caffe::TRAIN == param.state().phase()) {
210        int global_batch_size = mn::train::get_global_minibatch_size();
211        int fake_batch_size = mn::get_distrib()-&gt;get_data_parts();
212        if (mn::use_param_server() &amp;&amp; mn::is_param_server()) {
213          fake_batch_size = mn::nServer;
214        }
215        if (global_batch_size == 0) {
216          LOG(WARNING) &lt;&lt; &quot;SetMinibatchSize &quot; &lt;&lt; fake_batch_size;
217          mn::train::set_global_minibatch_size(fake_batch_size);
218        } else {
219          CHECK_EQ(global_batch_size, fake_batch_size);
220        }
221      }
222  #endif &amp;bsol;* USE_MLSL */
223      if (share_from_root) {
224        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; base_top = root_net_-&gt;top_vecs_[layer_id];
225        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; this_top = this-&gt;top_vecs_[layer_id];
226        for (int top_id = 0; top_id &lt; base_top.size(); ++top_id) {
227          this_top[top_id]-&gt;ReshapeLike(*base_top[top_id]);
228          LOG(INFO) &lt;&lt; &quot;Created top blob &quot; &lt;&lt; top_id &lt;&lt; &quot; (shape: &quot;
229              &lt;&lt; this_top[top_id]-&gt;shape_string() &lt;&lt;  &quot;) for shared layer &quot;
230              &lt;&lt; layer_param.name();
231        }
232      } else {
233        layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);
234      }
235      LOG_IF(INFO, Caffe::root_solver())
236          &lt;&lt; &quot;Setting up &quot; &lt;&lt; layer_names_[layer_id];
237      for (int top_id = 0; top_id &lt; top_vecs_[layer_id].size(); ++top_id) {
238        if (blob_loss_weights_.size() &lt;= top_id_vecs_[layer_id][top_id]) {
239          blob_loss_weights_.resize(top_id_vecs_[layer_id][top_id] + 1, Dtype(0));
240        }
241        blob_loss_weights_[top_id_vecs_[layer_id][top_id]] = layer-&gt;loss(top_id);
242        LOG_IF(INFO, Caffe::root_solver())
243            &lt;&lt; &quot;Top shape: &quot; &lt;&lt; top_vecs_[layer_id][top_id]-&gt;shape_string();
244        if (layer-&gt;loss(top_id)) {
245          LOG_IF(INFO, Caffe::root_solver())
246              &lt;&lt; &quot;    with loss weight &quot; &lt;&lt; layer-&gt;loss(top_id);
247        }
248        memory_used_ += top_vecs_[layer_id][top_id]-&gt;count();
249        if (max_blob_count &lt; top_vecs_[layer_id][top_id]-&gt;count()) max_blob_count = top_vecs_[layer_id][top_id]-&gt;count();
250      }
251      LOG_IF(INFO, Caffe::root_solver())
252          &lt;&lt; &quot;Memory required for data: &quot; &lt;&lt; memory_used_ * sizeof(Dtype);
253      LOG_IF(INFO, Caffe::root_solver())
254          &lt;&lt; &quot;Biggest Memory of single blob: &quot; &lt;&lt; max_blob_count * sizeof(Dtype);
255      const int param_size = layer_param.param_size();
256      const int num_param_blobs = layers_[layer_id]-&gt;blobs().size();
257      CHECK_LE(param_size, num_param_blobs)
258          &lt;&lt; &quot;Too many params specified for layer &quot; &lt;&lt; layer_param.name();
259      ParamSpec default_param_spec;
260      for (int param_id = 0; param_id &lt; num_param_blobs; ++param_id) {
261        const ParamSpec* param_spec = (param_id &lt; param_size) ?
262            &amp;layer_param.param(param_id) : &amp;default_param_spec;
263        const bool param_need_backward = param_spec-&gt;lr_mult() != 0;
264        need_backward |= param_need_backward;
265        layers_[layer_id]-&gt;set_param_propagate_down(param_id,
266                                                    param_need_backward);
267      }
268      for (int param_id = 0; param_id &lt; num_param_blobs; ++param_id) {
269        AppendParam(param, layer_id, param_id);
270      }
271      layer_need_backward_.push_back(need_backward);
272      if (need_backward) {
273        for (int top_id = 0; top_id &lt; top_id_vecs_[layer_id].size(); ++top_id) {
274          blob_need_backward_[top_id_vecs_[layer_id][top_id]] = true;
275        }
276      }
277    }
278    if ((phase_ == TEST) &amp;&amp; getenv(&quot;CAFFE_INFERENCE_MEM_OPT&quot;))
279      CircleBuf::Instance()-&gt;SetBufSize(max_blob_count * sizeof(Dtype));
280    set&lt;string&gt; blobs_under_loss;
281    set&lt;string&gt; blobs_skip_backp;
282    for (int layer_id = layers_.size() - 1; layer_id &gt;= 0; --layer_id) {
283      bool layer_contributes_loss = false;
284      bool layer_skip_propagate_down = true;
285      for (int top_id = 0; top_id &lt; top_vecs_[layer_id].size(); ++top_id) {
286        const string&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];
287        if (layers_[layer_id]-&gt;loss(top_id) ||
288            (blobs_under_loss.find(blob_name) != blobs_under_loss.end())) {
289          layer_contributes_loss = true;
290        }
291        if (blobs_skip_backp.find(blob_name) == blobs_skip_backp.end()) {
292          layer_skip_propagate_down = false;
293        }
294        if (layer_contributes_loss &amp;&amp; !layer_skip_propagate_down)
295          break;
296      }
297      if (layer_need_backward_[layer_id] &amp;&amp; layer_skip_propagate_down) {
298        layer_need_backward_[layer_id] = false;
299        for (int bottom_id = 0; bottom_id &lt; bottom_vecs_[layer_id].size();
300                 ++bottom_id) {
301          bottom_need_backward_[layer_id][bottom_id] = false;
302        }
303      }
304      if (!layer_contributes_loss) { layer_need_backward_[layer_id] = false; }
305      if (Caffe::root_solver()) {
306        if (layer_need_backward_[layer_id]) {
307          LOG(INFO) &lt;&lt; layer_names_[layer_id] &lt;&lt; &quot; needs backward computation.&quot;;
308        } else {
309          LOG(INFO) &lt;&lt; layer_names_[layer_id]
310              &lt;&lt; &quot; does not need backward computation.&quot;;
311        }
312      }
313      for (int bottom_id = 0; bottom_id &lt; bottom_vecs_[layer_id].size();
314           ++bottom_id) {
315        if (layer_contributes_loss) {
316          const string&amp; blob_name =
317              blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
318          blobs_under_loss.insert(blob_name);
319        } else {
320          bottom_need_backward_[layer_id][bottom_id] = false;
321        }
322        if (!bottom_need_backward_[layer_id][bottom_id]) {
323          const string&amp; blob_name =
324                     blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
325          blobs_skip_backp.insert(blob_name);
326        }
327      }
328    }
329    if (param.force_backward()) {
330      for (int layer_id = 0; layer_id &lt; layers_.size(); ++layer_id) {
331        layer_need_backward_[layer_id] = true;
332        for (int bottom_id = 0;
333             bottom_id &lt; bottom_need_backward_[layer_id].size(); ++bottom_id) {
334          bottom_need_backward_[layer_id][bottom_id] =
335              bottom_need_backward_[layer_id][bottom_id] ||
336              layers_[layer_id]-&gt;AllowForceBackward(bottom_id);
337          blob_need_backward_[bottom_id_vecs_[layer_id][bottom_id]] =
338              blob_need_backward_[bottom_id_vecs_[layer_id][bottom_id]] ||
339              bottom_need_backward_[layer_id][bottom_id];
340        }
341        for (int param_id = 0; param_id &lt; layers_[layer_id]-&gt;blobs().size();
342             ++param_id) {
343          layers_[layer_id]-&gt;set_param_propagate_down(param_id, true);
344        }
345      }
346    }
347    for (set&lt;string&gt;::iterator it = available_blobs.begin();
348        it != available_blobs.end(); ++it) {
349      LOG_IF(INFO, Caffe::root_solver())
350          &lt;&lt; &quot;This network produces output &quot; &lt;&lt; *it;
351      net_output_blobs_.push_back(blobs_[blob_name_to_idx[*it]].get());
352      net_output_blob_indices_.push_back(blob_name_to_idx[*it]);
353    }
354    for (size_t blob_id = 0; blob_id &lt; blob_names_.size(); ++blob_id) {
355      blob_names_index_[blob_names_[blob_id]] = blob_id;
356    }
357    for (size_t layer_id = 0; layer_id &lt; layer_names_.size(); ++layer_id) {
358      layer_names_index_[layer_names_[layer_id]] = layer_id;
359    }
360    ShareWeights();
361    debug_info_ = param.debug_info();
362  #ifdef USE_MLSL
363    if (this-&gt;phase_ == TRAIN) {
364        for (int layer_id = 0; layer_id &lt; param.layer_size(); ++layer_id) {
365          boost::shared_ptr&lt;Layer&lt;Dtype&gt;&gt; layer{ layers_[layer_id] };
366          if ((layer-&gt;layerOp != nullptr) &amp;&amp; layer-&gt;layerOp-&gt;HasParameterSets()) {
367                vector&lt;int&gt; param_ids = get_layer_learnable_param_ids(layer_id);
368                for (int i = 0; i &lt; param_ids.size(); i++) {
369                    int mlsl_weight_size = layer-&gt;layerOp-&gt;GetParameterSet(i)-&gt;GetLocalKernelCount()
370                                          * layer-&gt;layerOp-&gt;GetParameterSet(i)-&gt;GetKernelSize()
371                                          * sizeof(Dtype);
372                    int caffe_weight_size = learnable_params_[param_ids[i]]-&gt;count() * sizeof(Dtype);
373                    if (mlsl_weight_size &lt; caffe_weight_size)
374                        LOG(FATAL) &lt;&lt; &quot;InitNet: ERROR: check weight sizes for layer &quot; &lt;&lt; layer-&gt;type() &lt;&lt; &quot;, layer_id &quot; &lt;&lt; layer_id
375                                   &lt;&lt; &quot;, param_id &quot; &lt;&lt; param_ids[i]
376                                   &lt;&lt; &quot;, MLSL weight size in bytes &quot; &lt;&lt; mlsl_weight_size
377                                   &lt;&lt; &quot;, CAFFE weight size in bytes &quot; &lt;&lt; caffe_weight_size;
378                }
379            }
380        }
381    }
382  #endif &amp;bsol;* USE_MLSL */
383  #ifdef CAFFE_PER_LAYER_TIMINGS
384    InitTimers();
385  #endif
386    LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Network initialization done.&quot;;
387  }
388  template &lt;typename Dtype&gt;
389  void Net&lt;Dtype&gt;::FilterNet(const NetParameter&amp; param,
390      NetParameter* param_filtered) {
391    NetState net_state(param.state());
392    param_filtered-&gt;CopyFrom(param);
393    param_filtered-&gt;clear_layer();
394    for (int i = 0; i &lt; param.layer_size(); ++i) {
395      const LayerParameter&amp; layer_param = param.layer(i);
396      const string&amp; layer_name = layer_param.name();
397      CHECK(layer_param.include_size() == 0 || layer_param.exclude_size() == 0)
398            &lt;&lt; &quot;Specify either include rules or exclude rules; not both.&quot;;
399      bool layer_included = (layer_param.include_size() == 0);
400      for (int j = 0; layer_included &amp;&amp; j &lt; layer_param.exclude_size(); ++j) {
401        if (StateMeetsRule(net_state, layer_param.exclude(j), layer_name)) {
402          layer_included = false;
403        }
404      }
405      for (int j = 0; !layer_included &amp;&amp; j &lt; layer_param.include_size(); ++j) {
406        if (StateMeetsRule(net_state, layer_param.include(j), layer_name)) {
407          layer_included = true;
408        }
409      }
410      if (layer_included) {
411        param_filtered-&gt;add_layer()-&gt;CopyFrom(layer_param);
412      }
413    }
414  }
415  template &lt;typename Dtype&gt;
416  void Net&lt;Dtype&gt;::CompileNet(const NetParameter&amp; param,
417      NetParameter* param_compiled) {
418    #define NUM_OF_RULES sizeof(CompileRules)/sizeof(CompileRules[0])
419    #define COMPILE_BN_FOLDING_INDEX 0
420    #define COMPILE_CONV_RELU_FUSION_INDEX 2
421    #define COMPILE_BN_RELU_FUSION_INDEX 3
422    #define COMPILE_SPARSE_INDEX 5
423    #define COMPILE_CONV_SUM_FUSION_INDEX 6
424    #define COMPILE_FC_RELU_FUSION_INDEX 7
425    int i, current = 0;
426    NetParameter param_temp[2];
427    void (*CompileRules[]) (const NetParameter&amp; param, NetParameter* param_compiled) =
428      {RemoveBNScale&lt;Dtype&gt;, CompilationRuleRemoveScale, CompilationRuleConvReluFusion,
429      CompilationRuleFuseBnRelu, CompilationRuleBNInplace, CompilationRuleSparse, 
430      CompilationRuleConvSumFusion, CompilationRuleFuseFCRelu};
431    bool disabled[NUM_OF_RULES] = {false};
432  #ifdef DISABLE_BN_FOLDING
433    disabled[COMPILE_BN_FOLDING_INDEX] = true;
434  #endif
435  #ifdef DISABLE_CONV_RELU_FUSION
436    disabled[COMPILE_CONV_RELU_FUSION_INDEX] = true;
437  #endif
438  #ifdef DISABLE_BN_RELU_FUSION
439    disabled[COMPILE_BN_RELU_FUSION_INDEX] = true;
440  #endif
441  #ifdef DISABLE_CONV_SUM_FUSION
442    disabled[COMPILE_CONV_SUM_FUSION_INDEX] = true;
443  #endif
444  #ifdef DISABLE_SPARSE
445    disabled[COMPILE_SPARSE_INDEX] = true;
446  #endif
447  #ifdef DISABLE_FC_RELU_FUSION
448    disabled[COMPILE_FC_RELU_FUSION_INDEX] = true;
449  #endif
450    param_temp[current].CopyFrom(param);
451    for (i = 0; i &lt; NUM_OF_RULES; i++)
452      if (!disabled[i]) {
453        param_temp[1 - current].CopyFrom(param_temp[current]);
454        param_temp[1 - current].clear_layer();   
455        (*CompileRules[i]) (param_temp[current], &amp;param_temp[1 - current]);
456        current = 1 - current;
457      }
458    param_compiled-&gt;CopyFrom(param_temp[current]);
459    #undef NUM_OF_RULES
460    #undef COMPILE_BN_FOLDING_INDEX
461    #undef COMPILE_CONV_RELU_FUSION_INDEX
462    #undef COMPILE_BN_RELU_FUSION_INDEX
463    #undef COMPILE_SPARSE_INDEX
464    #undef COMPILE_CONV_SUM_FUSION_INDEX
465    #undef COMPILE_FC_RELU_FUSION_INDEX
466  }
467  template &lt;typename Dtype&gt;
468  void Net&lt;Dtype&gt;::CompilationRuleRemoveScale(const NetParameter&amp; param,
469                                      NetParameter* param_compiled) {
470    bool merge_bn_scale = false;
471    std::set&lt;std::string&gt; layers_to_drop;
472    for (int i = 0; i &lt; param.layer_size(); ++i) {
473      LayerParameter* layer_param =
474            (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
475      bool layer_included = true;
476      if (((layer_param-&gt;type().compare(&quot;BatchNorm&quot;) == 0) &amp;&amp;
477           ((layer_param-&gt;batch_norm_param().engine() == BatchNormParameter_Engine_MKL2017) ||
478            ((layer_param-&gt;batch_norm_param().engine() == BatchNormParameter_Engine_DEFAULT) &amp;&amp;
479             (layer_param-&gt;has_engine() == false)  &amp;&amp;
480             (param.engine().compare(&quot;MKL2017&quot;) == 0)) ||
481            ((layer_param-&gt;batch_norm_param().has_engine() == false) &amp;&amp;
482             (layer_param-&gt;engine().compare(&quot;MKL2017&quot;) == 0)))) ||
483          ((layer_param-&gt;type().compare(&quot;BatchNorm&quot;) == 0) &amp;&amp;
484           ((layer_param-&gt;batch_norm_param().engine() == BatchNormParameter_Engine_MKLDNN) ||
485            ((layer_param-&gt;batch_norm_param().engine() == BatchNormParameter_Engine_DEFAULT) &amp;&amp;
486             (layer_param-&gt;has_engine() == false)  &amp;&amp;
487             (param.engine().compare(&quot;MKLDNN&quot;) == 0)) ||
488            ((layer_param-&gt;batch_norm_param().has_engine() == false) &amp;&amp;
489             (layer_param-&gt;engine().compare(&quot;MKLDNN&quot;) == 0))))) {
490        std::vector&lt;const LayerParameter*&gt; consumer_layer_params;
491        GetBlobConsumers(consumer_layer_params,
492                         layer_param-&gt;top(0),
493                         param,
494                         i+1 &lt; param.layer_size() ? i+1 : i);
495        const LayerParameter&amp; consumer_layer_param =
496                                      consumer_layer_params.size() &gt; 0 ?
497                                      *(consumer_layer_params[0]) : *layer_param;
498        if ((consumer_layer_param.type().compare(&quot;Scale&quot;) == 0) &amp;&amp;
499             (consumer_layer_param.bottom_size() == 1)) {
500          string&amp; batchnorm_top_blob_name =
501              const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0));
502          const string&amp; scale_top_blob_name = consumer_layer_param.top(0);
503          layers_to_drop.insert(consumer_layer_param.name());
504          if (!merge_bn_scale) merge_bn_scale = true;
505          batchnorm_top_blob_name.resize(scale_top_blob_name.size());
506          batchnorm_top_blob_name.replace(0,
507                                          scale_top_blob_name.size(),
508                                          scale_top_blob_name);
509          bool scale_bias_term = consumer_layer_param.
510                                 scale_param().bias_term();
511          layer_param-&gt;mutable_batch_norm_param()-&gt;
512          set_bias_term(scale_bias_term);
513          if (consumer_layer_param.blobs_size() == 2) {
514            layer_param-&gt;add_blobs()-&gt;CopyFrom(consumer_layer_param.blobs(0));
515            layer_param-&gt;add_blobs()-&gt;CopyFrom(consumer_layer_param.blobs(1));
516          }
517          if (consumer_layer_param.param_size() == 2) {
518            layer_param-&gt;add_param()-&gt;CopyFrom(consumer_layer_param.param(0));
519            layer_param-&gt;add_param()-&gt;CopyFrom(consumer_layer_param.param(1));
520          }
521        }
522      }
523      if (layers_to_drop.find(layer_param-&gt;name()) != layers_to_drop.end()) {
524        LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Dropped layer: &quot;
525               &lt;&lt; layer_param-&gt;name() &lt;&lt; std::endl;
526        layer_included = false;
527        layers_to_drop.erase(layers_to_drop.find(layer_param-&gt;name()));
528      }
529      if (layer_included) {
530        param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
531      }
532    }
533    param_compiled-&gt;mutable_compile_net_state()-&gt;set_bn_scale_merge(merge_bn_scale);
534  }
535  template &lt;typename Dtype&gt;
536  void Net&lt;Dtype&gt;::CompilationRuleConvReluFusion(const NetParameter&amp; param,
537                                      NetParameter* param_compiled) {
538    std::set&lt;std::string&gt; layers_to_drop;
539    for (int i = 0; i &lt; param.layer_size(); ++i) {
540      LayerParameter* layer_param =
541            (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
542      bool layer_included = true;
543      if ((layer_param-&gt;type().compare(&quot;Convolution&quot;) == 0) &amp;&amp;
544          ((layer_param-&gt;convolution_param().engine() == ConvolutionParameter_Engine_MKLDNN) ||
545           ((layer_param-&gt;convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &amp;&amp;
546            (layer_param-&gt;engine().compare(0, 6, &quot;MKLDNN&quot;) == 0) &amp;&amp;
547            (layer_param-&gt;engine().find(&quot;:DLA&quot;, 6) == string::npos)) ||
548           ((layer_param-&gt;convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &amp;&amp;
549            (layer_param-&gt;engine() == &quot;&quot;) &amp;&amp;
550            (param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
551             param.engine().find(&quot;:DLA&quot;, 6) == string::npos)))) {
552        std::vector&lt;const LayerParameter*&gt; consumer_layer_params;
553        GetBlobConsumers(consumer_layer_params, layer_param-&gt;top(0),
554                         param, i+1 &lt; param.layer_size() ? i+1 : i);
555        const LayerParameter&amp; consumer_layer_param =
556                                      consumer_layer_params.size() &gt; 0 ?
557                                      *(consumer_layer_params[0]) : *layer_param;
558        if ((consumer_layer_param.type().compare(&quot;ReLU&quot;) == 0) &amp;&amp;
559            ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_MKLDNN) ||
560             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &amp;&amp;
561              (consumer_layer_param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
562               consumer_layer_param.engine().find(&quot;:DLA&quot;, 6) == string::npos)) ||
563             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &amp;&amp;
564              (consumer_layer_param.engine() == &quot;&quot;) &amp;&amp;
565              (param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
566               param.engine().find(&quot;:DLA&quot;, 6) == string::npos)))) {
567          string&amp; convolution_top_blob_name =
568              const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0));
569          float negative_slope1 =
570                    consumer_layer_param.relu_param().negative_slope();
571          layer_param-&gt;mutable_convolution_param()-&gt;set_relu(true);
572          layer_param-&gt;mutable_convolution_param()-&gt;set_negative_slope(negative_slope1);
573          if(param.state().phase() == TEST) {
574            const string&amp; scale_top_blob_name = consumer_layer_param.top(0);
575            layers_to_drop.insert(consumer_layer_param.name());
576            convolution_top_blob_name.resize(scale_top_blob_name.size());
577            convolution_top_blob_name.replace(0,
578                                            scale_top_blob_name.size(),
579                                            scale_top_blob_name);
580          }
581          if(param.state().phase() == TRAIN) {
582            if(i+1 &lt; param.layer_size()) {
583              LayerParameter* relu_layer_param =
584                (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i+1);
585              relu_layer_param-&gt;mutable_relu_param()-&gt;set_fuse(true);
586            }
587          }
588        }
589      }
590      if(param.state().phase() == TEST) {
591        if (layers_to_drop.find(layer_param-&gt;name()) != layers_to_drop.end()) {
592          LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Dropped layer: &quot;
593                 &lt;&lt; layer_param-&gt;name() &lt;&lt; std::endl;
594          layer_included = false;
595          layers_to_drop.erase(layers_to_drop.find(layer_param-&gt;name()));
596        }
597      }
598      if (layer_included) {
599        param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
600      }
601    }
602  }
603  template &lt;typename Dtype&gt;
604  void Net&lt;Dtype&gt;::CompilationRuleBNInplace(const NetParameter&amp; param,
605                                        NetParameter* param_compiled) {
606    for (int i = 0; i &lt; param.layer_size(); ++i) {
607      LayerParameter* layer_param =
608          (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
609      if (((layer_param-&gt;type().compare(&quot;BatchNorm&quot;) == 0) &amp;&amp;
610           (layer_param-&gt;batch_norm_param().engine() ==
611                BatchNormParameter_Engine_MKL2017 ||
612            ((layer_param-&gt;batch_norm_param().engine() ==
613              BatchNormParameter_Engine_DEFAULT) &amp;&amp;
614             param.engine().compare(&quot;MKL2017&quot;) == 0))) &amp;&amp;
615          (layer_param-&gt;top(0) == layer_param-&gt;bottom(0))) {
616        std::string&amp; batch_norm_top = const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0));
617        std::vector&lt;const LayerParameter*&gt; consumer_layer_params;
618        GetBlobConsumers(consumer_layer_params, batch_norm_top, param,
619                         i + 1 &lt; param.layer_size() ? i + 1 : i);
620        for (std::vector&lt;const LayerParameter*&gt;::iterator it =
621                 consumer_layer_params.begin();
622             it != consumer_layer_params.end(); ++it) {
623          if (((*it)-&gt;top_size() &gt; 0) &amp;&amp;
624              ((*it)-&gt;bottom(0).compare((*it)-&gt;top(0)) == 0)) {
625            const_cast&lt;string&amp;&gt;((*it)-&gt;top(0)).append(&quot;_x&quot;);
626          }
627          for (unsigned int i = 0; i &lt; (*it)-&gt;bottom_size(); ++i) {
628            if ((*it)-&gt;bottom(i).compare(batch_norm_top) == 0) {
629              const_cast&lt;string&amp;&gt;((*it)-&gt;bottom(i)).append(&quot;_x&quot;);
630            }
631          }
632        }
633        batch_norm_top.append(&quot;_x&quot;);
634      }
635      param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
636    }
637    if(param.state().phase() == TEST) return;
638    std::map&lt;string, int&gt; inplace_blob_name_to_index;
639    std::map&lt;string, int&gt; specified_layer_blob_name_to_index;
640    vector&lt;vector&lt;const LayerParameter*&gt;&gt; layer_pairs;
641    vector&lt;vector&lt;string&gt;&gt; specified_layer_input_blob_names;
642    vector&lt;string&gt; raise_non_inplace_layer_type_list;
643    raise_non_inplace_layer_type_list.push_back(&quot;Eltwise&quot;);
644    for (auto layer_type : raise_non_inplace_layer_type_list) {
645      specified_layer_input_blob_names.clear();
646      inplace_blob_name_to_index.clear();
647      layer_pairs.clear();
648      ParseNetInplaceStatus(
649          inplace_blob_name_to_index, specified_layer_blob_name_to_index,
650          specified_layer_input_blob_names, param_compiled, layer_type);
651      for (auto each_blob_list : specified_layer_input_blob_names) {
652        GetNeedToCancelInplaceLayers(
653            layer_pairs, specified_layer_blob_name_to_index,
654            inplace_blob_name_to_index, each_blob_list, *param_compiled);
655        for (auto each_layer_pair : layer_pairs) {
656          std::string&amp; layer_top =
657              const_cast&lt;string&amp;&gt;((each_layer_pair[0])-&gt;top(0));
658          for (unsigned int i = 0; i &lt; each_layer_pair[1]-&gt;bottom_size(); ++i) {
659            if (each_layer_pair[1]-&gt;bottom(i).compare(layer_top) == 0) {
660              const_cast&lt;string&amp;&gt;(each_layer_pair[1]-&gt;bottom(i)).append(&quot;_x&quot;);
661            }
662          }
663          const_cast&lt;string&amp;&gt;((each_layer_pair[0])-&gt;top(0)).append(&quot;_x&quot;);
664        }
665      }
666    }
667    return;
668  }
669  template &lt;typename Dtype&gt;
670  void Net&lt;Dtype&gt;::CompilationRuleConvSumFusion(const NetParameter&amp; param,
671                                                NetParameter* param_compiled) {
672    if (param.state().phase() != TEST || param.engine().compare(&quot;MKLDNN&quot;) != 0) {
673      param_compiled-&gt;CopyFrom(param);
674      return;
675    }
676    string blob_need_to_insert;
677    LayerParameter* need_to_convert_layer = NULL;
678    bool has_relu_flag = true;
679    bool need_fusion_flag;
680    bool switch_flag = false;
681    std::set&lt;string&gt; invalid_fusion_blob_names;
682    for (int i = 0; i &lt; param.layer_size(); i++) {
683      need_fusion_flag = true;
684      LayerParameter* layer_param =
685          (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
686      if (layer_param-&gt;type().compare(&quot;Split&quot;) == 0 &amp;&amp;
687          layer_param-&gt;top_size() &gt; 2) {
688        for (int j = 0; j &lt; layer_param-&gt;top_size() - 1; j++) {
689          invalid_fusion_blob_names.insert(layer_param-&gt;top(j));
690        }
691      }
692      if (layer_param-&gt;type().compare(&quot;Convolution&quot;) == 0 &amp;&amp;
693          (layer_param-&gt;has_engine() == false ||
694           (layer_param-&gt;has_engine() == true &amp;&amp;
695            layer_param-&gt;engine().compare(&quot;MKLDNN&quot;) == 0))) {
696        std::vector&lt;const LayerParameter*&gt; child_layers_params;
697        Net&lt;Dtype&gt;::GetBlobConsumers(child_layers_params, layer_param-&gt;top(0),
698                                     param,
699                                     i + 1 &lt; param.layer_size() ? i + 1 : i);
700        if (child_layers_params.size() &gt; 0 &amp;&amp;
701            child_layers_params[0]-&gt;type().compare(&quot;Eltwise&quot;) == 0) {
702          for (int k = 0; k &lt; child_layers_params[0]-&gt;bottom_size(); k++) {
703            if (invalid_fusion_blob_names.count(
704                    child_layers_params[0]-&gt;bottom(k)) &gt; 0) {
705              need_fusion_flag = false;
706              break;
707            }
708          }
709          if (!need_fusion_flag) {
710            param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
711            continue;
712          }
713          std::vector&lt;const LayerParameter*&gt; grand_child_layers_params;
714          Net&lt;Dtype&gt;::GetBlobConsumers(grand_child_layers_params,
715                                       child_layers_params[0]-&gt;top(0), param,
716                                       i + 1 &lt; param.layer_size() ? i + 1 : i);
717          const LayerParameter&amp; grand_child_layer_param =
718              grand_child_layers_params.size() &gt; 0
719                  ? *(grand_child_layers_params[0])
720                  : *layer_param;
721          if (grand_child_layer_param.type().compare(&quot;ReLU&quot;) != 0) {
722            has_relu_flag = false;
723          }
724          if (child_layers_params[0] !=
725              (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i + 1)) {
726            if (child_layers_params[0]-&gt;bottom(0) == layer_param-&gt;top(0)) {
727              switch_flag = true;
728            } else {
729              switch_flag = false;
730            }
731            param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
732            std::vector&lt;const LayerParameter*&gt; another_eltwise_input_layers_params;
733            if (switch_flag) {
734              Net&lt;Dtype&gt;::GetBlobProducers(another_eltwise_input_layers_params,
735                                          child_layers_params[0]-&gt;bottom(1), param,
736                                          i + 1 &lt; param.layer_size() ? i + 1 : i);
737            } else {
738              Net&lt;Dtype&gt;::GetBlobProducers(another_eltwise_input_layers_params,
739                                          child_layers_params[0]-&gt;bottom(0), param,
740                                          i + 1 &lt; param.layer_size() ? i + 1 : i);
741            }
742            const LayerParameter&amp; another_eltwise_layer_param =
743                another_eltwise_input_layers_params.size() &gt; 0
744                    ? *(another_eltwise_input_layers_params[0])
745                    : *layer_param;
746            if (another_eltwise_layer_param.type().compare(&quot;Convolution&quot;) == 0 ) {
747              need_to_convert_layer = layer_param;
748            } 
749            continue;
750          } else {
751            if (need_to_convert_layer == NULL) {
752              if (child_layers_params[0]-&gt;bottom(1) == layer_param-&gt;top(0)) {
753                switch_flag = true;
754              } else {
755                switch_flag = false;
756              }
757            }
758          }
759          if (has_relu_flag) {
760            const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0)) =
761                grand_child_layer_param.top(0);
762          } else {
763            const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0)) =
764                child_layers_params[0]-&gt;top(0);
765          }
766          if (need_to_convert_layer != NULL) {
767            layer_param-&gt;add_bottom(
768                const_cast&lt;string&amp;&gt;(need_to_convert_layer-&gt;top(0)));
769            need_to_convert_layer = NULL;
770          } else {
771            if (switch_flag) {
772              layer_param-&gt;add_bottom(
773                  const_cast&lt;string&amp;&gt;(child_layers_params[0]-&gt;bottom(0)));
774            } else {
775              layer_param-&gt;add_bottom(
776                  const_cast&lt;string&amp;&gt;(child_layers_params[0]-&gt;bottom(1)));
777            }
778          }
779          if (has_relu_flag) {
780            i += 2;  
781            layer_param-&gt;mutable_convolution_param()-&gt;set_relu(true);
782          } else {
783            i += 1;
784          }
785          layer_param-&gt;mutable_convolution_param()-&gt;set_fusion_type(
786              ConvolutionParameter::SUM_FUSION);
787          size_t coeff_size =
788              child_layers_params[0]-&gt;eltwise_param().coeff_size();
789          if (coeff_size &gt; 0) {
790            for (int i = 0; i &lt; coeff_size; ++i) {
791              layer_param-&gt;mutable_convolution_param()-&gt;add_coeff(
792                  child_layers_params[0]-&gt;eltwise_param().coeff(i));
793            }
794          }
795        }
796      }
797      param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
798    }
799    return;
800  }
801  template &lt;typename Dtype&gt;
802  void Net&lt;Dtype&gt;::CompilationRuleSparse(const NetParameter&amp; param,
803                                         NetParameter* param_compiled) {
804    if (param.state().phase() != TEST || param.engine().compare(&quot;MKLDNN&quot;) != 0) {
805      param_compiled-&gt;CopyFrom(param);
806      return;
807    }
808    LayerParameter* potential_sparse_layer = NULL;
809    LayerParameter* confirmed_sparse_layer = NULL;
810    LayerParameter* layer_param = NULL;
811    std::map&lt;string, string&gt; bottom_blob_layer_mapping;
812    std::map&lt;string, string&gt; top_blob_layer_mapping;
813    std::map&lt;string, std::vector&lt;LayerParameter*&gt;&gt; sparse_layer_name_mapping;  
814    std::vector&lt;LayerParameter*&gt; trigger_sparse_layers;
815    std::map&lt;string, int&gt; conv_layer_id_mapping;
816    std::map&lt;string, int&gt; eltwise_layer_id_mapping;
817    std::map&lt;string, int&gt; layer_name_id_mapping;
818    std::map&lt;int, int&gt; pooling_layer_id_stride;  
819    std::map&lt;int, int&gt; conv_layer_id_stride;  
820    std::map&lt;int, string&gt; pooling_layer_id_top_blob;
821    for (int index = 0; index &lt; param.layer_size(); index++) {
822      layer_param = (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(index);
823      layer_name_id_mapping[layer_param-&gt;name()] = index;
824      for (int j = 0; j &lt; layer_param-&gt;top_size(); j++) {
825        top_blob_layer_mapping[layer_param-&gt;top(j)] = layer_param-&gt;name();
826      }
827      for (int k = 0; k &lt; layer_param-&gt;bottom_size(); k++) {
828        bottom_blob_layer_mapping[layer_param-&gt;bottom(k)] = layer_param-&gt;name();
829      }
830      if (layer_param-&gt;type().compare(&quot;Eltwise&quot;) == 0) {
831        eltwise_layer_id_mapping[layer_param-&gt;name()] = index;
832      }
833      if (layer_param-&gt;type().compare(&quot;Convolution&quot;) == 0 &amp;&amp;
834          layer_param-&gt;has_convolution_param() &amp;&amp;
835          layer_param-&gt;convolution_param().kernel_size_size() &gt; 0 &amp;&amp;
836          layer_param-&gt;convolution_param().stride_size() &gt; 0) {
837        conv_layer_id_mapping[layer_param-&gt;name()] = index;
838        if (layer_param-&gt;convolution_param().kernel_size(0) &gt; 1) {
839          potential_sparse_layer = layer_param;
840        } else if (layer_param-&gt;convolution_param().kernel_size(0) == 1 &amp;&amp;
841                   layer_param-&gt;convolution_param().stride(0) &gt; 1  &amp;&amp;
842            (layer_param-&gt;convolution_param().pad_size() == 0 ||
843             (layer_param-&gt;convolution_param().pad_size() &gt; 0 &amp;&amp;
844              layer_param-&gt;convolution_param().pad(0) == 0))) {
845          if (potential_sparse_layer == NULL)
846              continue;
847          confirmed_sparse_layer = potential_sparse_layer;
848          if (trigger_sparse_layers.size() &gt; 0) {
849            for (int j = 0; j &lt; trigger_sparse_layers.size(); j++) {
850              if (top_blob_layer_mapping[trigger_sparse_layers[j]-&gt;bottom(0)] !=
851                  top_blob_layer_mapping[layer_param-&gt;bottom(0)]) {
852                trigger_sparse_layers.clear();
853                break;
854              }
855            }
856            trigger_sparse_layers.push_back(layer_param);
857            sparse_layer_name_mapping[confirmed_sparse_layer-&gt;name()] =
858                trigger_sparse_layers;
859          } else {
860            trigger_sparse_layers.push_back(layer_param);
861          }
862        }
863      }
864    }
865    if(trigger_sparse_layers.size() &gt; 1)
866      sparse_layer_name_mapping[confirmed_sparse_layer-&gt;name()] = trigger_sparse_layers;
867    std::map&lt;string, std::vector&lt;LayerParameter*&gt;&gt;::iterator sparse_it =
868        sparse_layer_name_mapping.begin();
869    while (sparse_it != sparse_layer_name_mapping.end() &amp;&amp; sparse_it-&gt;second.size() &gt; 1) {
870      if (sparse_it-&gt;second[0]-&gt;convolution_param().stride(0) !=
871          sparse_it-&gt;second[1]-&gt;convolution_param().stride(0)) {
872            continue;
873      }
874      LayerParameter* sparse_layer_param =
875          (const_cast&lt;NetParameter&amp;&gt;(param))
876              .mutable_layer(layer_name_id_mapping[sparse_it-&gt;first]);
877      int updated_stride_value =
878          sparse_layer_param-&gt;convolution_param().stride(0) *
879          sparse_it-&gt;second[0]-&gt;convolution_param().stride(0);
880      conv_layer_id_stride[conv_layer_id_mapping[sparse_it-&gt;first]] =
881          updated_stride_value;
882      conv_layer_id_stride[conv_layer_id_mapping[sparse_it-&gt;second[0]-&gt;name()]] = 1;
883      conv_layer_id_stride[conv_layer_id_mapping[sparse_it-&gt;second[1]-&gt;name()]] = 1;
884      std::map&lt;string, int&gt;::iterator eltwise_iter = eltwise_layer_id_mapping.begin();
885      while (eltwise_iter != eltwise_layer_id_mapping.end()) {
886        if (conv_layer_id_mapping[sparse_it-&gt;first] &lt; eltwise_iter-&gt;second &amp;&amp;
887            eltwise_iter-&gt;second &lt; conv_layer_id_mapping[sparse_it-&gt;second[0]-&gt;name()]) {
888          break;  
889        }
890        eltwise_iter++;
891      }
892      std::vector&lt;int&gt; need_add_pooling_layer_id;
893      LayerParameter* eltwise_layer_param =
894          (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(eltwise_iter-&gt;second);
895      for (int k = 0; k &lt; eltwise_layer_param-&gt;bottom_size() - 1; k++) {
896        need_add_pooling_layer_id.push_back(
897            layer_name_id_mapping
898                [top_blob_layer_mapping[eltwise_layer_param-&gt;bottom(k)]]);
899        int pooling_layer_id = layer_name_id_mapping
900            [top_blob_layer_mapping[eltwise_layer_param-&gt;bottom(k)]];
901        pooling_layer_id_stride[pooling_layer_id] = updated_stride_value;
902        pooling_layer_id_top_blob[pooling_layer_id] =
903            eltwise_layer_param-&gt;bottom(k);
904      }
905      sparse_it++;
906    }
907    for (int i = 0; i &lt; param.layer_size(); i++) {
908      LayerParameter* each_layer_param =
909          (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
910      if (conv_layer_id_stride.find(i) != conv_layer_id_stride.end()) {
911        each_layer_param-&gt;mutable_convolution_param()-&gt;set_stride(
912            0, conv_layer_id_stride[i]);
913      } else if (pooling_layer_id_stride.find(i) !=
914                 pooling_layer_id_stride.end()) {
915        param_compiled-&gt;add_layer()-&gt;CopyFrom(*each_layer_param);
916        each_layer_param = param_compiled-&gt;add_layer();
917        each_layer_param-&gt;Clear();
918        each_layer_param-&gt;set_type(&quot;Pooling&quot;);
919        each_layer_param-&gt;set_name(pooling_layer_id_top_blob[i] + &quot;_p&quot;);
920        each_layer_param-&gt;add_bottom(pooling_layer_id_top_blob[i]);
921        each_layer_param-&gt;add_top(pooling_layer_id_top_blob[i] + &quot;_p&quot;);
922        each_layer_param-&gt;mutable_pooling_param()-&gt;set_stride(
923            pooling_layer_id_stride[i]);
924        each_layer_param-&gt;mutable_pooling_param()-&gt;set_kernel_size(1);
925        each_layer_param-&gt;mutable_pooling_param()-&gt;set_pool(
926            PoolingParameter_PoolMethod_MAX);
927        int target_layer_id = layer_name_id_mapping
928            [bottom_blob_layer_mapping[pooling_layer_id_top_blob[i]]];
929        LayerParameter* target_layer_param =
930            (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(target_layer_id);
931        int target_blob_index = 0;
932        bool found_blob_flag = false;
933        for (; target_blob_index &lt; target_layer_param-&gt;bottom_size();
934             target_blob_index++) {
935          if (target_layer_param-&gt;bottom(target_blob_index) ==
936              pooling_layer_id_top_blob[i]) {
937            found_blob_flag = true;
938            break;
939          }
940        }
941        if (found_blob_flag) {
942          target_layer_param-&gt;set_bottom(target_blob_index,
943                                         pooling_layer_id_top_blob[i] + &quot;_p&quot;);
944          continue;
945        }
946      }
947      param_compiled-&gt;add_layer()-&gt;CopyFrom(*each_layer_param);
948    }
949  }
950  template &lt;typename Dtype&gt;
951  void Net&lt;Dtype&gt;::CompilationRuleFuseBnRelu(const NetParameter&amp; param,
952                                      NetParameter* param_compiled) {
953                       std::set&lt;std::string&gt; layers_to_drop;
954    for (int i = 0; i &lt; param.layer_size(); ++i) {
955      LayerParameter* layer_param =
956            (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
957      bool layer_included = true;
958      if (((layer_param-&gt;type().compare(&quot;BatchNorm&quot;) == 0) &amp;&amp;
959           ((layer_param-&gt;batch_norm_param().engine() == BatchNormParameter_Engine_MKLDNN) ||
960            ((layer_param-&gt;batch_norm_param().engine() == BatchNormParameter_Engine_DEFAULT) &amp;&amp;
961             (layer_param-&gt;has_engine() == false)  &amp;&amp;
962             (param.engine().compare(&quot;MKLDNN&quot;) == 0)) ||
963            (param.engine() == &quot;&quot; &amp;&amp; layer_param-&gt;engine().compare(&quot;MKLDNN&quot;) == 0)))) {
964        std::vector&lt;const LayerParameter*&gt; consumer_layer_params;
965        GetBlobConsumers(consumer_layer_params,
966                         layer_param-&gt;top(0),
967                         param,
968                         i+1 &lt; param.layer_size() ? i+1 : i);
969        const LayerParameter&amp; consumer_layer_param =
970                                      consumer_layer_params.size() &gt; 0 ?
971                                      *(consumer_layer_params[0]) : *layer_param;
972        if ((consumer_layer_param.type().compare(&quot;ReLU&quot;) == 0) &amp;&amp;
973            ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_MKLDNN) ||
974             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &amp;&amp;
975              (consumer_layer_param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
976               consumer_layer_param.engine().find(&quot;:DLA&quot;, 6) == string::npos)) ||
977             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &amp;&amp;
978              (consumer_layer_param.engine() == &quot;&quot;) &amp;&amp;
979              (param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
980               param.engine().find(&quot;:DLA&quot;, 6) == string::npos))) &amp;&amp;
981               !consumer_layer_param.relu_param().negative_slope()) {
982          string&amp; batchnorm_top_blob_name =
983              const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0));
984          if(param.state().phase() == TEST) {
985            const string&amp; relu_top_blob_name = consumer_layer_param.top(0);
986            layers_to_drop.insert(consumer_layer_param.name());
987            batchnorm_top_blob_name.resize(relu_top_blob_name.size());
988            batchnorm_top_blob_name.replace(0,
989                                            relu_top_blob_name.size(),
990                                            relu_top_blob_name);
991          }
992          layer_param-&gt;mutable_batch_norm_param()-&gt;set_relu(true);
993          if(param.state().phase() == TRAIN) {
994            if(i+1 &lt; param.layer_size()) {
995              LayerParameter* relu_layer_param =
996                (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i+1);
997              relu_layer_param-&gt;mutable_relu_param()-&gt;set_fuse(true);
998            }
999          }
1000        }
1001      }
1002      if(param.state().phase() == TEST) {
1003        if (layers_to_drop.find(layer_param-&gt;name()) != layers_to_drop.end()) {
1004          LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Dropped layer: &quot;
1005                 &lt;&lt; layer_param-&gt;name() &lt;&lt; std::endl;
1006          layer_included = false;
1007          layers_to_drop.erase(layers_to_drop.find(layer_param-&gt;name()));
1008        }
1009      }
1010      if (layer_included) {
1011        param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
1012      }
1013    }
1014  }
1015  template &lt;typename Dtype&gt;
1016  void Net&lt;Dtype&gt;::CompilationRuleFuseFCRelu(const NetParameter&amp; param,
1017                                      NetParameter* param_compiled) {
1018    std::set&lt;std::string&gt; layers_to_drop;
1019    for (int i = 0; i &lt; param.layer_size(); ++i) {
1020      LayerParameter* layer_param =
1021            (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i);
1022      bool layer_included = true;
1023      if ((layer_param-&gt;type().compare(&quot;InnerProduct&quot;) == 0) &amp;&amp;
1024          ((layer_param-&gt;convolution_param().engine() == ConvolutionParameter_Engine_MKLDNN) ||
1025           ((layer_param-&gt;convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &amp;&amp;
1026            (layer_param-&gt;engine().compare(0, 6, &quot;MKLDNN&quot;) == 0) &amp;&amp;
1027            (layer_param-&gt;engine().find(&quot;:DLA&quot;, 6) == string::npos)) ||
1028           ((layer_param-&gt;convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &amp;&amp;
1029            (layer_param-&gt;engine() == &quot;&quot;) &amp;&amp;
1030            (param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
1031             param.engine().find(&quot;:DLA&quot;, 6) == string::npos)))) {
1032        std::vector&lt;const LayerParameter*&gt; consumer_layer_params;
1033        GetBlobConsumers(consumer_layer_params, layer_param-&gt;top(0),
1034                         param, i+1 &lt; param.layer_size() ? i+1 : i);
1035        const LayerParameter&amp; consumer_layer_param =
1036                                      consumer_layer_params.size() &gt; 0 ?
1037                                      *(consumer_layer_params[0]) : *layer_param;
1038        if ((consumer_layer_param.type().compare(&quot;ReLU&quot;) == 0) &amp;&amp;
1039            ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_MKLDNN) ||
1040             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &amp;&amp;
1041              (consumer_layer_param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
1042               consumer_layer_param.engine().find(&quot;:DLA&quot;, 6) == string::npos)) ||
1043             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &amp;&amp;
1044              (consumer_layer_param.engine() == &quot;&quot;) &amp;&amp;
1045              (param.engine().compare(0, 6, &quot;MKLDNN&quot;) == 0 &amp;&amp;
1046               param.engine().find(&quot;:DLA&quot;, 6) == string::npos)))) {
1047          string&amp; convolution_top_blob_name =
1048              const_cast&lt;string&amp;&gt;(layer_param-&gt;top(0));
1049          float negative_slope =
1050                    consumer_layer_param.relu_param().negative_slope();
1051          layer_param-&gt;mutable_inner_product_param()-&gt;set_relu(true);
1052          layer_param-&gt;mutable_inner_product_param()-&gt;set_negative_slope(negative_slope);
1053          if(param.state().phase() == TEST) {
1054            const string&amp; scale_top_blob_name = consumer_layer_param.top(0);
1055            layers_to_drop.insert(consumer_layer_param.name());
1056            convolution_top_blob_name.resize(scale_top_blob_name.size());
1057            convolution_top_blob_name.replace(0,
1058                                            scale_top_blob_name.size(),
1059                                            scale_top_blob_name);
1060          }
1061          if(param.state().phase() == TRAIN) {
1062            if(i+1 &lt; param.layer_size()) {
1063              LayerParameter* relu_layer_param =
1064                (const_cast&lt;NetParameter&amp;&gt;(param)).mutable_layer(i+1);
1065              relu_layer_param-&gt;mutable_relu_param()-&gt;set_fuse(true);
1066            }
1067          }
1068        }
1069      }
1070      if(param.state().phase() == TEST) {
1071        if (layers_to_drop.find(layer_param-&gt;name()) != layers_to_drop.end()) {
1072          LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Dropped layer: &quot;
1073                 &lt;&lt; layer_param-&gt;name() &lt;&lt; std::endl;
1074          layer_included = false;
1075          layers_to_drop.erase(layers_to_drop.find(layer_param-&gt;name()));
1076        }
1077      }
1078      if (layer_included) {
1079        param_compiled-&gt;add_layer()-&gt;CopyFrom(*layer_param);
1080      }
1081    }
1082  }
1083  template &lt;typename Dtype&gt;
1084  void Net&lt;Dtype&gt;::GetBlobConsumers(
1085                    std::vector&lt;const LayerParameter*&gt;&amp; consumer_blobs,
1086                    const string&amp; blob_name_to_find,
1087                    const NetParameter&amp; param,
1088                    int layer_id_to_start_traversing_from) {
1089    consumer_blobs.clear();
1090    CHECK_GE(layer_id_to_start_traversing_from, 1);
1091    CHECK_LT(layer_id_to_start_traversing_from, param.layer_size());
1092    for (int i = layer_id_to_start_traversing_from; i &lt; param.layer_size(); ++i) {
1093      for (int j = 0; j &lt; param.layer(i).bottom_size(); ++j) {
1094        if (param.layer(i).bottom(j).compare(blob_name_to_find) == 0) {
1095          consumer_blobs.push_back(&amp;param.layer(i));
1096        }
1097      }
1098    }
1099  }
1100  template &lt;typename Dtype&gt;
1101  void Net&lt;Dtype&gt;::GetBlobProducers(
1102                    std::vector&lt;const LayerParameter*&gt;&amp; producers_blobs,
1103                    const string&amp; blob_name_to_find,
1104                    const NetParameter&amp; param,
1105                    int layer_id_to_start_traversing_from) {
1106    producers_blobs.clear();
1107    CHECK_GE(layer_id_to_start_traversing_from, 1);
1108    CHECK_LT(layer_id_to_start_traversing_from, param.layer_size());
1109    for (int i = layer_id_to_start_traversing_from; i &lt; param.layer_size(); ++i) {
1110      for (int j = 0; j &lt; param.layer(i).top_size(); ++j) {
1111        if (param.layer(i).top(j).compare(blob_name_to_find) == 0) {
1112          producers_blobs.push_back(&amp;param.layer(i));
1113        }
1114      }
1115    }
1116  }
1117  template &lt;typename Dtype&gt;
1118  void Net&lt;Dtype&gt;::ParseNetInplaceStatus(
1119      std::map&lt;string, int&gt;&amp; inplace_blob_name_to_index,
1120      std::map&lt;string, int&gt;&amp; specified_layer_blob_name_to_index,
1121      vector&lt;vector&lt;string&gt;&gt;&amp; specified_layer_input_blob_names,
1122      NetParameter* param, const string&amp; specified_layer_type) {
1123    for (int layer_index = 0; layer_index &lt; param-&gt;layer_size(); ++layer_index) {
1124      LayerParameter* layer_param =
1125          (const_cast&lt;NetParameter&amp;&gt;(*param)).mutable_layer(layer_index);
1126      if (!specified_layer_type.empty() &amp;&amp;
1127          layer_param-&gt;type().compare(specified_layer_type) != 0 &amp;&amp;
1128          layer_param-&gt;bottom_size() == 1 &amp;&amp; layer_param-&gt;top_size() == 1 &amp;&amp;
1129          layer_param-&gt;bottom(0) == layer_param-&gt;top(0)) {
1130        inplace_blob_name_to_index[layer_param-&gt;bottom(0)] = layer_index;
1131      }
1132      if (!specified_layer_type.empty() &amp;&amp;
1133          layer_param-&gt;type().compare(specified_layer_type) == 0) {
1134        vector&lt;string&gt; blob_names;
1135        for (unsigned int blob_index = 0; blob_index &lt; layer_param-&gt;bottom_size();
1136             blob_index++) {
1137          specified_layer_blob_name_to_index[layer_param-&gt;bottom(blob_index)] =
1138              layer_index;
1139          blob_names.push_back(layer_param-&gt;bottom(blob_index));
1140        }
1141        specified_layer_input_blob_names.push_back(blob_names);
1142      }
1143    }
1144  }
1145  template &lt;typename Dtype&gt;
1146  void Net&lt;Dtype&gt;::GetNeedToCancelInplaceLayers(
1147      vector&lt;vector&lt;const LayerParameter*&gt;&gt;&amp; layer_pairs,
1148      std::map&lt;string, int&gt;&amp; specified_layer_blob_name_to_index,
1149      std::map&lt;string, int&gt;&amp; inplace_blob_name_to_index,
1150      vector&lt;string&gt;&amp; each_blob_list, const NetParameter&amp; param) {
1151    if (param.engine().compare(&quot;MKLDNN&quot;) != 0 || each_blob_list.size() == 1)
1152      return;
1153    layer_pairs.clear();
1154    vector&lt;const LayerParameter*&gt; each_layer_pair;
1155    each_blob_list.erase(each_blob_list.begin());
1156    for (auto blob_name : each_blob_list) {
1157      each_layer_pair.clear();
1158      if (inplace_blob_name_to_index.find(blob_name) ==
1159              inplace_blob_name_to_index.end() ||
1160          specified_layer_blob_name_to_index.find(blob_name) ==
1161              specified_layer_blob_name_to_index.end()) {
1162        continue;
1163      }
1164      LayerParameter* bottom_layer =
1165          (const_cast&lt;NetParameter&amp;&gt;(param))
1166              .mutable_layer(inplace_blob_name_to_index[blob_name]);
1167      LayerParameter* top_layer =
1168          (const_cast&lt;NetParameter&amp;&gt;(param))
1169              .mutable_layer(specified_layer_blob_name_to_index[blob_name]);
1170      each_layer_pair.push_back(bottom_layer);
1171      each_layer_pair.push_back(top_layer);
1172      layer_pairs.push_back(each_layer_pair);
1173    }
1174  }
1175  template &lt;typename Dtype&gt;
1176  bool Net&lt;Dtype&gt;::StateMeetsRule(const NetState&amp; state,
1177      const NetStateRule&amp; rule, const string&amp; layer_name) {
1178    if (rule.has_phase()) {
1179        if (rule.phase() != state.phase()) {
1180          LOG_IF(INFO, Caffe::root_solver())
1181              &lt;&lt; &quot;The NetState phase (&quot; &lt;&lt; state.phase()
1182              &lt;&lt; &quot;) differed from the phase (&quot; &lt;&lt; rule.phase()
1183              &lt;&lt; &quot;) specified by a rule in layer &quot; &lt;&lt; layer_name;
1184          return false;
1185        }
1186    }
1187    if (rule.has_min_level()) {
1188      if (state.level() &lt; rule.min_level()) {
1189        LOG_IF(INFO, Caffe::root_solver())
1190            &lt;&lt; &quot;The NetState level (&quot; &lt;&lt; state.level()
1191            &lt;&lt; &quot;) is above the min_level (&quot; &lt;&lt; rule.min_level()
1192            &lt;&lt; &quot;) specified by a rule in layer &quot; &lt;&lt; layer_name;
1193        return false;
1194      }
1195    }
1196    if (rule.has_max_level()) {
1197      if (state.level() &gt; rule.max_level()) {
1198        LOG_IF(INFO, Caffe::root_solver())
1199            &lt;&lt; &quot;The NetState level (&quot; &lt;&lt; state.level()
1200            &lt;&lt; &quot;) is above the max_level (&quot; &lt;&lt; rule.max_level()
1201            &lt;&lt; &quot;) specified by a rule in layer &quot; &lt;&lt; layer_name;
1202        return false;
1203      }
1204    }
1205    for (int i = 0; i &lt; rule.stage_size(); ++i) {
1206      bool has_stage = false;
1207      for (int j = 0; !has_stage &amp;&amp; j &lt; state.stage_size(); ++j) {
1208        if (rule.stage(i) == state.stage(j)) { has_stage = true; }
1209      }
1210      if (!has_stage) {
1211        LOG_IF(INFO, Caffe::root_solver())
1212            &lt;&lt; &quot;The NetState did not contain stage &#x27;&quot; &lt;&lt; rule.stage(i)
1213            &lt;&lt; &quot;&#x27; specified by a rule in layer &quot; &lt;&lt; layer_name;
1214        return false;
1215      }
1216    }
1217    for (int i = 0; i &lt; rule.not_stage_size(); ++i) {
1218      bool has_stage = false;
1219      for (int j = 0; !has_stage &amp;&amp; j &lt; state.stage_size(); ++j) {
1220        if (rule.not_stage(i) == state.stage(j)) { has_stage = true; }
1221      }
1222      if (has_stage) {
1223        LOG_IF(INFO, Caffe::root_solver())
1224            &lt;&lt; &quot;The NetState contained a not_stage &#x27;&quot; &lt;&lt; rule.not_stage(i)
1225            &lt;&lt; &quot;&#x27; specified by a rule in layer &quot; &lt;&lt; layer_name;
1226        return false;
1227      }
1228    }
1229    return true;
1230  }
1231  template &lt;typename Dtype&gt;
1232  vector&lt;Dtype&gt; Net&lt;Dtype&gt;::FindMax(Blob&lt;Dtype&gt;* blob, bool is_single) {
1233    const Dtype* data = blob-&gt;cpu_data();
1234    int cnt = blob-&gt;count();
1235    vector&lt;Dtype&gt; max_vals;
1236    Dtype max_val = (Dtype)(-10);
1237    int index = 0;
1238    if(blob-&gt;shape().size() == 4) {
1239      if(is_single) {
1240        max_vals = vector&lt;Dtype&gt;(1, Dtype(-10));
1241        for (int i = 0; i &lt; cnt; ++i) {
1242          max_val = std::max(max_val, (Dtype)fabs(data[i]));
1243        }
1244        max_vals.at(0) = max_val;
1245      } else { 
1246        int height = blob-&gt;shape(2);
1247        int width = blob-&gt;shape(3);
1248        int channel = blob-&gt;shape(0);
1249        max_vals = vector&lt;Dtype&gt;(channel, Dtype(-10));
1250        int step = blob-&gt;shape(1) * height * width;
1251        for (int i = 0; i &lt; cnt; ++i) {
1252          if((i + 1) % step == 0) {
1253            max_vals.at(index) = std::max(max_val, (Dtype)fabs(data[i]));
1254            ++index;
1255            max_val = (Dtype)(-10);
1256          } else {
1257            max_val = std::max(max_val, (Dtype)fabs(data[i]));
1258          }
1259        }
1260      }
1261    } else {
1262      if(is_single) {
1263        max_vals = vector&lt;Dtype&gt;(1, Dtype(-10));
1264        for (int i = 0; i &lt; cnt; ++i) {
1265          max_val = std::max(max_val, (Dtype)fabs(data[i]));
1266        }
1267        max_vals.at(0) = max_val;
1268      } else { 
1269        int channel = blob-&gt;shape(0);
1270        max_vals = vector&lt;Dtype&gt;(channel, Dtype(-10));
1271        int step = blob-&gt;shape(1);
1272        for (int i = 0; i &lt; cnt; ++i) {
1273          if((i + 1) % step == 0) {
1274            max_vals.at(index) = std::max(max_val, (Dtype)fabs(data[i]));
1275            ++index;
1276            max_val = (Dtype)(-10);
1277          } else {
1278            max_val = std::max(max_val, (Dtype)fabs(data[i]));
1279          }
1280        }
1281      }
1282    }
1283    return max_vals;
1284  }
1285  template &lt;typename Dtype&gt;
1286  void Net&lt;Dtype&gt;::RangeInLayers(vector&lt;string&gt;* layer_name,
1287        vector&lt;Dtype&gt;* max_in, vector&lt;Dtype&gt;* max_out, vector&lt;vector&lt;Dtype&gt;&gt;* max_param, string scaling) {
1288    if(layer_name-&gt;size()==0) {
1289      for (int layer_id = 0; layer_id &lt; layers_.size(); ++layer_id) {
1290        if (strcmp(layers_[layer_id]-&gt;type(), &quot;Convolution&quot;) == 0) {
1291          layer_name-&gt;push_back(this-&gt;layer_names()[layer_id]);
1292          max_in-&gt;push_back(0);
1293          max_out-&gt;push_back(0);
1294          if (scaling == &quot;single&quot;) {
1295            max_param-&gt;push_back(vector&lt;Dtype&gt;(1, 0));
1296          }
1297          else {
1298            int param_shape = (&amp;(*layers_[layer_id]-&gt;blobs()[0]))-&gt;shape(0);
1299            max_param-&gt;push_back(vector&lt;Dtype&gt;(param_shape, 0));
1300          }
1301        }
1302      }
1303    }
1304    int index = 0;
1305    vector&lt;Dtype&gt; max_vals;
1306    for (int layer_id = 0; layer_id &lt; layers_.size(); ++layer_id) {
1307      if (strcmp(layers_[layer_id]-&gt;type(), &quot;Convolution&quot;) == 0) {
1308        max_vals = FindMax(bottom_vecs_[layer_id][0]);
1309        max_in-&gt;at(index) = std::max(max_in-&gt;at(index), max_vals.at(0)); 
1310        max_vals = FindMax(top_vecs_[layer_id][0]);
1311        max_out-&gt;at(index) = std::max(max_out-&gt;at(index), max_vals.at(0));
1312        if (scaling == &quot;single&quot;) {
1313          max_vals = FindMax(&amp;(*layers_[layer_id]-&gt;blobs()[0]));
1314          max_param-&gt;at(index).at(0) = std::max(max_param-&gt;at(index).at(0), max_vals.at(0));
1315        } else {
1316          max_vals = FindMax(&amp;(*layers_[layer_id]-&gt;blobs()[0]), false);
1317          for(int i = 0; i &lt; max_vals.size(); ++i) 
1318            max_param-&gt;at(index).at(i) = std::max(max_param-&gt;at(index).at(i), max_vals.at(i));
1319        }
1320        index++;
1321      }
1322    }
1323  }
1324  template &lt;typename Dtype&gt;
1325  void Net&lt;Dtype&gt;::AppendTop(const NetParameter&amp; param, const int layer_id,
1326                             const int top_id, set&lt;string&gt;* available_blobs,
1327                             map&lt;string, int&gt;* blob_name_to_idx) {
1328    shared_ptr&lt;LayerParameter&gt; layer_param(
1329        new LayerParameter(param.layer(layer_id)));
1330    const string&amp; blob_name = (layer_param-&gt;top_size() &gt; top_id) ?
1331        layer_param-&gt;top(top_id) : &quot;(automatic)&quot;;
1332    if (blob_name_to_idx &amp;&amp; layer_param-&gt;bottom_size() &gt; top_id &amp;&amp;
1333        blob_name == layer_param-&gt;bottom(top_id)) {
1334      LOG_IF(INFO, Caffe::root_solver())
1335          &lt;&lt; layer_param-&gt;name() &lt;&lt; &quot; -&gt; &quot; &lt;&lt; blob_name &lt;&lt; &quot; (in-place)&quot;;
1336      top_vecs_[layer_id].push_back(blobs_[(*blob_name_to_idx)[blob_name]].get());
1337      top_id_vecs_[layer_id].push_back((*blob_name_to_idx)[blob_name]);
1338    } else if (blob_name_to_idx &amp;&amp;
1339               blob_name_to_idx-&gt;find(blob_name) != blob_name_to_idx-&gt;end()) {
1340      LOG(FATAL) &lt;&lt; &quot;Top blob &#x27;&quot; &lt;&lt; blob_name
1341                 &lt;&lt; &quot;&#x27; produced by multiple sources.&quot;;
1342    } else {
1343      if (Caffe::root_solver()) {
1344        LOG(INFO) &lt;&lt; layer_param-&gt;name() &lt;&lt; &quot; -&gt; &quot; &lt;&lt; blob_name;
1345      }
1346      shared_ptr&lt;Blob&lt;Dtype&gt; &gt; blob_pointer(new Blob&lt;Dtype&gt;());
1347      const int blob_id = blobs_.size();
1348      blobs_.push_back(blob_pointer);
1349      blob_names_.push_back(blob_name);
1350      blob_need_backward_.push_back(false);
1351      if (blob_name_to_idx) { (*blob_name_to_idx)[blob_name] = blob_id; }
1352      top_id_vecs_[layer_id].push_back(blob_id);
1353      top_vecs_[layer_id].push_back(blob_pointer.get());
1354    }
1355    if (available_blobs) { available_blobs-&gt;insert(blob_name); }
1356  }
1357  template &lt;typename Dtype&gt;
1358  int Net&lt;Dtype&gt;::AppendBottom(const NetParameter&amp; param, const int layer_id,
1359      const int bottom_id, set&lt;string&gt;* available_blobs,
1360      map&lt;string, int&gt;* blob_name_to_idx) {
1361    const LayerParameter&amp; layer_param = param.layer(layer_id);
1362    const string&amp; blob_name = layer_param.bottom(bottom_id);
1363    if (available_blobs-&gt;find(blob_name) == available_blobs-&gt;end()) {
1364      LOG(FATAL) &lt;&lt; &quot;Unknown bottom blob &#x27;&quot; &lt;&lt; blob_name &lt;&lt; &quot;&#x27; (layer &#x27;&quot;
1365                 &lt;&lt; layer_param.name() &lt;&lt; &quot;&#x27;, bottom index &quot; &lt;&lt; bottom_id &lt;&lt; &quot;)&quot;;
1366    }
1367    const int blob_id = (*blob_name_to_idx)[blob_name];
1368    LOG_IF(INFO, Caffe::root_solver())
1369        &lt;&lt; layer_names_[layer_id] &lt;&lt; &quot; &lt;- &quot; &lt;&lt; blob_name;
1370    bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());
1371    bottom_id_vecs_[layer_id].push_back(blob_id);
1372    available_blobs-&gt;erase(blob_name);
1373    bool need_backward = blob_need_backward_[blob_id];
1374    if (layer_param.propagate_down_size() &gt; 0) {
1375      need_backward = layer_param.propagate_down(bottom_id);
1376    }
1377    bottom_need_backward_[layer_id].push_back(need_backward);
1378    return blob_id;
1379  }
1380  template &lt;typename Dtype&gt;
1381  void Net&lt;Dtype&gt;::AppendParam(const NetParameter&amp; param, const int layer_id,
1382                               const int param_id) {
1383    const LayerParameter&amp; layer_param = layers_[layer_id]-&gt;layer_param();
1384    const int param_size = layer_param.param_size();
1385    string param_name =
1386        (param_size &gt; param_id) ? layer_param.param(param_id).name() : &quot;&quot;;
1387    if (param_name.size()) {
1388      param_display_names_.push_back(param_name);
1389    } else {
1390      ostringstream param_display_name;
1391      param_display_name &lt;&lt; param_id;
1392      param_display_names_.push_back(param_display_name.str());
1393    }
1394    const int net_param_id = params_.size();
1395    params_.push_back(layers_[layer_id]-&gt;blobs()[param_id]);
1396    param_id_vecs_[layer_id].push_back(net_param_id);
1397    param_layer_indices_.push_back(make_pair(layer_id, param_id));
1398    ParamSpec default_param_spec;
1399    const ParamSpec* param_spec = (layer_param.param_size() &gt; param_id) ?
1400        &amp;layer_param.param(param_id) : &amp;default_param_spec;
1401    if (!param_size || !param_name.size() || (param_name.size() &amp;&amp;
1402        param_names_index_.find(param_name) == param_names_index_.end())) {
1403      param_owners_.push_back(-1);
1404      if (param_name.size()) {
1405        param_names_index_[param_name] = net_param_id;
1406      }
1407      const int learnable_param_id = learnable_params_.size();
1408      learnable_params_.push_back(params_[net_param_id].get());
1409      learnable_param_ids_.push_back(learnable_param_id);
1410      has_params_lr_.push_back(param_spec-&gt;has_lr_mult());
1411      has_params_decay_.push_back(param_spec-&gt;has_decay_mult());
1412      params_lr_.push_back(param_spec-&gt;lr_mult());
1413      params_weight_decay_.push_back(param_spec-&gt;decay_mult());
1414    } else {
1415      const int owner_net_param_id = param_names_index_[param_name];
1416      param_owners_.push_back(owner_net_param_id);
1417      const pair&lt;int, int&gt;&amp; owner_index =
1418          param_layer_indices_[owner_net_param_id];
1419      const int owner_layer_id = owner_index.first;
1420      const int owner_param_id = owner_index.second;
1421      LOG_IF(INFO, Caffe::root_solver()) &lt;&lt; &quot;Sharing parameters &#x27;&quot; &lt;&lt; param_name
1422          &lt;&lt; &quot;&#x27; owned by &quot;
1423          &lt;&lt; &quot;layer &#x27;&quot; &lt;&lt; layer_names_[owner_layer_id] &lt;&lt; &quot;&#x27;, param &quot;
1424          &lt;&lt; &quot;index &quot; &lt;&lt; owner_param_id;
1425      Blob&lt;Dtype&gt;* this_blob = layers_[layer_id]-&gt;blobs()[param_id].get();
1426      Blob&lt;Dtype&gt;* owner_blob =
1427          layers_[owner_layer_id]-&gt;blobs()[owner_param_id].get();
1428      const int param_size = layer_param.param_size();
1429      if (param_size &gt; param_id &amp;&amp; (layer_param.param(param_id).share_mode() ==
1430                                    ParamSpec_DimCheckMode_PERMISSIVE)) {
1431        CHECK_EQ(this_blob-&gt;count(), owner_blob-&gt;count())
1432            &lt;&lt; &quot;Cannot share param &#x27;&quot; &lt;&lt; param_name &lt;&lt; &quot;&#x27; owned by layer &#x27;&quot;
1433            &lt;&lt; layer_names_[owner_layer_id] &lt;&lt; &quot;&#x27; with layer &#x27;&quot;
1434            &lt;&lt; layer_names_[layer_id] &lt;&lt; &quot;&#x27;; count mismatch.  Owner layer param &quot;
1435            &lt;&lt; &quot;shape is &quot; &lt;&lt; owner_blob-&gt;shape_string() &lt;&lt; &quot;; sharing layer &quot;
1436            &lt;&lt; &quot;shape is &quot; &lt;&lt; this_blob-&gt;shape_string();
1437      } else {
1438        CHECK(this_blob-&gt;shape() == owner_blob-&gt;shape())
1439            &lt;&lt; &quot;Cannot share param &#x27;&quot; &lt;&lt; param_name &lt;&lt; &quot;&#x27; owned by layer &#x27;&quot;
1440            &lt;&lt; layer_names_[owner_layer_id] &lt;&lt; &quot;&#x27; with layer &#x27;&quot;
1441            &lt;&lt; layer_names_[layer_id] &lt;&lt; &quot;&#x27;; shape mismatch.  Owner layer param &quot;
1442            &lt;&lt; &quot;shape is &quot; &lt;&lt; owner_blob-&gt;shape_string() &lt;&lt; &quot;; sharing layer &quot;
1443            &lt;&lt; &quot;expects shape &quot; &lt;&lt; this_blob-&gt;shape_string();
1444      }
1445      const int learnable_param_id = learnable_param_ids_[owner_net_param_id];
1446      learnable_param_ids_.push_back(learnable_param_id);
1447      if (param_spec-&gt;has_lr_mult()) {
1448        if (has_params_lr_[learnable_param_id]) {
1449          CHECK_EQ(param_spec-&gt;lr_mult(), params_lr_[learnable_param_id])
1450              &lt;&lt; &quot;Shared param &#x27;&quot; &lt;&lt; param_name &lt;&lt; &quot;&#x27; has mismatched lr_mult.&quot;;
1451        } else {
1452          has_params_lr_[learnable_param_id] = true;
1453          params_lr_[learnable_param_id] = param_spec-&gt;lr_mult();
1454        }
1455      }
1456      if (param_spec-&gt;has_decay_mult()) {
1457        if (has_params_decay_[learnable_param_id]) {
1458          CHECK_EQ(param_spec-&gt;decay_mult(),
1459                   params_weight_decay_[learnable_param_id])
1460              &lt;&lt; &quot;Shared param &#x27;&quot; &lt;&lt; param_name &lt;&lt; &quot;&#x27; has mismatched decay_mult.&quot;;
1461        } else {
1462          has_params_decay_[learnable_param_id] = true;
1463          params_weight_decay_[learnable_param_id] = param_spec-&gt;decay_mult();
1464        }
1465      }
1466    }
1467  }
1468  template &lt;typename Dtype&gt;
1469  Dtype Net&lt;Dtype&gt;::ForwardFromTo(int start, int end) {
1470    CHECK_GE(start, 0);
1471    CHECK_LT(end, layers_.size());
1472    Dtype loss = 0;
1473    for (int i = start; i &lt;= end; ++i) {
1474      LAYER_TIMING_START(forward, i);
1475      PERFORMANCE_MEASUREMENT_BEGIN();
1476      Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);
1477      PERFORMANCE_MEASUREMENT_END((std::string(&quot;FW_&quot;) + layer_names_[i]).c_str());
1478      LAYER_TIMING_STOP(forward, i);
1479      loss += layer_loss;
1480      if (debug_info_) { ForwardDebugInfo(i); }
1481    }
1482    return loss;
1483  }
1484  template &lt;typename Dtype&gt;
1485  Dtype Net&lt;Dtype&gt;::ForwardFrom(int start) {
1486    return ForwardFromTo(start, layers_.size() - 1);
1487  }
1488  template &lt;typename Dtype&gt;
1489  Dtype Net&lt;Dtype&gt;::ForwardTo(int end) {
1490    return ForwardFromTo(0, end);
1491  }
1492  template &lt;typename Dtype&gt;
1493  const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; Net&lt;Dtype&gt;::Forward(Dtype* loss) {
1494    if (loss != NULL) {
1495      *loss = ForwardFromTo(0, layers_.size() - 1);
1496    } else {
1497      ForwardFromTo(0, layers_.size() - 1);
1498    }
1499    return net_output_blobs_;
1500  }
1501  template &lt;typename Dtype&gt;
1502  const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; Net&lt;Dtype&gt;::Forward(
1503      const vector&lt;Blob&lt;Dtype&gt;*&gt; &amp; bottom, Dtype* loss) {
1504    LOG_EVERY_N(WARNING, 1000) &lt;&lt; &quot;DEPRECATED: Forward(bottom, loss) &quot;
1505        &lt;&lt; &quot;will be removed in a future version. Use Forward(loss).&quot;;
1506    for (int i = 0; i &lt; bottom.size(); ++i) {
1507      net_input_blobs_[i]-&gt;CopyFrom(*bottom[i]);
1508    }
1509    return Forward(loss);
1510  }
1511  template &lt;typename Dtype&gt;
1512  void Net&lt;Dtype&gt;::BackwardFromTo(int start, int end) {
1513    CHECK_GE(end, 0);
1514    CHECK_LT(start, layers_.size());
1515    for (int i = start; i &gt;= end; --i) {
1516      if (layer_need_backward_[i]) {
1517        LAYER_TIMING_START(backward, i);
1518        PERFORMANCE_MEASUREMENT_BEGIN();
1519        layers_[i]-&gt;Backward(
1520            top_vecs_[i], bottom_need_backward_[i], bottom_vecs_[i]);
1521        PERFORMANCE_MEASUREMENT_END((std::string(&quot;BW_&quot;)+layer_names_[i]).c_str());
1522        LAYER_TIMING_STOP(backward, i);
1523        if (debug_info_) { BackwardDebugInfo(i); }
1524      }
1525    }
1526  }
1527  template &lt;typename Dtype&gt;
1528  void Net&lt;Dtype&gt;::ForwardDebugInfo(const int layer_id) {
1529    for (int top_id = 0; top_id &lt; top_vecs_[layer_id].size(); ++top_id) {
1530      const Blob&lt;Dtype&gt;&amp; blob = *top_vecs_[layer_id][top_id];
1531      const string&amp; blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];
1532      const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
1533      LOG_IF(INFO, Caffe::root_solver())
1534          &lt;&lt; &quot;    [Forward] &quot;
1535          &lt;&lt; &quot;Layer &quot; &lt;&lt; layer_names_[layer_id]
1536          &lt;&lt; &quot;, top blob &quot; &lt;&lt; blob_name
1537          &lt;&lt; &quot; data: &quot; &lt;&lt; data_abs_val_mean;
1538    }
1539    for (int param_id = 0; param_id &lt; layers_[layer_id]-&gt;blobs().size();
1540         ++param_id) {
1541      const Blob&lt;Dtype&gt;&amp; blob = *layers_[layer_id]-&gt;blobs()[param_id];
1542      const int net_param_id = param_id_vecs_[layer_id][param_id];
1543      const string&amp; blob_name = param_display_names_[net_param_id];
1544      const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
1545      LOG_IF(INFO, Caffe::root_solver())
1546          &lt;&lt; &quot;    [Forward] &quot;
1547          &lt;&lt; &quot;Layer &quot; &lt;&lt; layer_names_[layer_id]
1548          &lt;&lt; &quot;, param blob &quot; &lt;&lt; blob_name
1549          &lt;&lt; &quot; data: &quot; &lt;&lt; data_abs_val_mean;
1550    }
1551  }
1552  template &lt;typename Dtype&gt;
1553  void Net&lt;Dtype&gt;::BackwardDebugInfo(const int layer_id) {
1554    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom_vec = bottom_vecs_[layer_id];
1555    for (int bottom_id = 0; bottom_id &lt; bottom_vec.size(); ++bottom_id) {
1556      if (!bottom_need_backward_[layer_id][bottom_id]) { continue; }
1557      const Blob&lt;Dtype&gt;&amp; blob = *bottom_vec[bottom_id];
1558      const string&amp; blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
1559      const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
1560      LOG_IF(INFO, Caffe::root_solver())
1561          &lt;&lt; &quot;    [Backward] &quot;
1562          &lt;&lt; &quot;Layer &quot; &lt;&lt; layer_names_[layer_id]
1563          &lt;&lt; &quot;, bottom blob &quot; &lt;&lt; blob_name
1564          &lt;&lt; &quot; diff: &quot; &lt;&lt; diff_abs_val_mean;
1565    }
1566    for (int param_id = 0; param_id &lt; layers_[layer_id]-&gt;blobs().size();
1567         ++param_id) {
1568      if (!layers_[layer_id]-&gt;param_propagate_down(param_id)) { continue; }
1569      const Blob&lt;Dtype&gt;&amp; blob = *layers_[layer_id]-&gt;blobs()[param_id];
1570      const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
1571      LOG_IF(INFO, Caffe::root_solver())
1572          &lt;&lt; &quot;    [Backward] &quot;
1573          &lt;&lt; &quot;Layer &quot; &lt;&lt; layer_names_[layer_id]
1574          &lt;&lt; &quot;, param blob &quot; &lt;&lt; param_id
1575          &lt;&lt; &quot; diff: &quot; &lt;&lt; diff_abs_val_mean;
1576    }
1577  }
1578  template &lt;typename Dtype&gt;
1579  void Net&lt;Dtype&gt;::UpdateDebugInfo(const int param_id) {
1580    const Blob&lt;Dtype&gt;&amp; blob = *params_[param_id];
1581    const int param_owner = param_owners_[param_id];
1582    const string&amp; layer_name = layer_names_[param_layer_indices_[param_id].first];
1583    const string&amp; param_display_name = param_display_names_[param_id];
1584    const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
1585    if (param_owner &lt; 0) {
1586      const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
1587      LOG_IF(INFO, Caffe::root_solver())
1588          &lt;&lt; &quot;    [Update] Layer &quot; &lt;&lt; layer_name
1589          &lt;&lt; &quot;, param &quot; &lt;&lt; param_display_name
1590          &lt;&lt; &quot; data: &quot; &lt;&lt; data_abs_val_mean
1591          &lt;&lt; &quot;; diff: &quot; &lt;&lt; diff_abs_val_mean;
1592    } else {
1593      const string&amp; owner_layer_name =
1594          layer_names_[param_layer_indices_[param_owner].first];
1595      LOG_IF(INFO, Caffe::root_solver())
1596          &lt;&lt; &quot;    [Update] Layer &quot; &lt;&lt; layer_name
1597          &lt;&lt; &quot;, param blob &quot; &lt;&lt; param_display_name
1598          &lt;&lt; &quot; (owned by layer &quot; &lt;&lt; owner_layer_name &lt;&lt; &quot;, &quot; &lt;&lt; &quot;param &quot;
1599          &lt;&lt; param_display_names_[param_owners_[param_id]] &lt;&lt; &quot;)&quot;
1600          &lt;&lt; &quot; diff: &quot; &lt;&lt; diff_abs_val_mean;
1601    }
1602  }
1603  template &lt;typename Dtype&gt;
1604  void Net&lt;Dtype&gt;::ShareTrainedLayersWith(const Net* other) {
1605      if (this-&gt;bn_scale_remove_) {
1606      NetParameter temp_net_param;
1607      NetParameter complete_net_param;
1608      other-&gt;ToProto(&amp;temp_net_param, false);
1609      for (vector&lt;string&gt;::iterator it = kept_bn_layers_.begin(); it != kept_bn_layers_.end(); it++) {
1610        temp_net_param.mutable_compile_net_state()-&gt;add_kept_bn_layers(*it);
1611      }
1612      complete_net_param.CopyFrom(temp_net_param);
1613      if (other-&gt;bn_scale_merge_) {
1614        complete_net_param.clear_layer();
1615        RecoverBNScaleMergedNet&lt;Dtype&gt;(&amp;temp_net_param, &amp;complete_net_param);
1616      }
1617      CopyTrainedLayersFrom(complete_net_param);
1618      return ;
1619    }
1620    int num_source_layers = other-&gt;layers().size();
1621    for (int i = 0; i &lt; num_source_layers; ++i) {
<span onclick='openModal()' class='match'>1622      Layer&lt;Dtype&gt;* source_layer = other-&gt;layers()[i].get();
1623      const string&amp; source_layer_name = other-&gt;layer_names()[i];
</span>1624      int target_layer_id = 0;
1625      while (target_layer_id != layer_names_.size() &amp;&amp;
1626          layer_names_[target_layer_id] != source_layer_name) {
1627        ++target_layer_id;
1628      }
1629      if (target_layer_id == layer_names_.size()) {
1630        LOG(INFO) &lt;&lt; &quot;Ignoring source layer &quot; &lt;&lt; source_layer_name;
1631        continue;
1632      }
1633      DLOG(INFO) &lt;&lt; &quot;Copying source layer &quot; &lt;&lt; source_layer_name;
1634      vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; target_blobs =
1635          layers_[target_layer_id]-&gt;blobs();
1636      CHECK_EQ(target_blobs.size(), source_layer-&gt;blobs().size())
1637          &lt;&lt; &quot;Incompatible number of blobs for layer &quot; &lt;&lt; source_layer_name;
1638      for (int j = 0; j &lt; target_blobs.size(); ++j) {
1639        Blob&lt;Dtype&gt;* source_blob = source_layer-&gt;blobs()[j].get();
1640        CHECK(target_blobs[j]-&gt;shape() == source_blob-&gt;shape())
1641            &lt;&lt; &quot;Cannot share param &quot; &lt;&lt; j &lt;&lt; &quot; weights from layer &#x27;&quot;
1642            &lt;&lt; source_layer_name &lt;&lt; &quot;&#x27;; shape mismatch.  Source param shape is &quot;
1643            &lt;&lt; source_blob-&gt;shape_string() &lt;&lt; &quot;; target param shape is &quot;
1644            &lt;&lt; target_blobs[j]-&gt;shape_string();
1645        target_blobs[j]-&gt;ShareData(*source_blob);
1646      }
1647    }
1648  }
1649  template &lt;typename Dtype&gt;
1650  void Net&lt;Dtype&gt;::BackwardFrom(int start) {
1651    BackwardFromTo(start, 0);
1652  }
1653  template &lt;typename Dtype&gt;
1654  void Net&lt;Dtype&gt;::BackwardTo(int end) {
1655    BackwardFromTo(layers_.size() - 1, end);
1656  }
1657  template &lt;typename Dtype&gt;
1658  void Net&lt;Dtype&gt;::Backward() {
1659    BackwardFromTo(layers_.size() - 1, 0);
1660    if (debug_info_) {
1661      Dtype asum_data = 0, asum_diff = 0, sumsq_data = 0, sumsq_diff = 0;
1662      for (int i = 0; i &lt; learnable_params_.size(); ++i) {
1663        asum_data += learnable_params_[i]-&gt;asum_data();
1664        asum_diff += learnable_params_[i]-&gt;asum_diff();
1665        sumsq_data += learnable_params_[i]-&gt;sumsq_data();
1666        sumsq_diff += learnable_params_[i]-&gt;sumsq_diff();
1667      }
1668      const Dtype l2norm_data = std::sqrt(sumsq_data);
1669      const Dtype l2norm_diff = std::sqrt(sumsq_diff);
1670      LOG(ERROR) &lt;&lt; &quot;    [Backward] All net params (data, diff): &quot;
1671                 &lt;&lt; &quot;L1 norm = (&quot; &lt;&lt; asum_data &lt;&lt; &quot;, &quot; &lt;&lt; asum_diff &lt;&lt; &quot;); &quot;
1672                 &lt;&lt; &quot;L2 norm = (&quot; &lt;&lt; l2norm_data &lt;&lt; &quot;, &quot; &lt;&lt; l2norm_diff &lt;&lt; &quot;)&quot;;
1673    }
1674  }
1675  template &lt;typename Dtype&gt;
1676  void Net&lt;Dtype&gt;::Reshape() {
1677    for (int i = 0; i &lt; layers_.size(); ++i) {
1678      layers_[i]-&gt;Reshape(bottom_vecs_[i], top_vecs_[i]);
1679    }
1680  }
1681  template &lt;typename Dtype&gt;
1682  void Net&lt;Dtype&gt;::CopyTrainedLayersFrom(const NetParameter&amp; param_inp) {
1683    NetParameter param_tmp = param_inp;
1684    NetParameter &amp;param = param_tmp;
1685    param.set_engine(engine_name_);
1686    param_tmp.mutable_state()-&gt;set_phase(phase_);
1687    param_tmp.mutable_compile_net_state()-&gt;set_is_init(false);
1688    for (vector&lt;string&gt;::iterator it = this-&gt;kept_bn_layers_.begin(); it != this-&gt;kept_bn_layers_.end(); it++) {
1689      param_tmp.mutable_compile_net_state()-&gt;add_kept_bn_layers(*it);
1690    }
1691    int num_source_layers = param.layer_size();
1692    for (int i = 0; i &lt; num_source_layers; ++i) {
1693      LayerParameter* source_layer = param.mutable_layer(i);
1694      const string&amp; source_layer_name = source_layer-&gt;name();
1695      int target_layer_id = 0;
1696      while (target_layer_id != layer_names_.size() &amp;&amp;
1697          layer_names_[target_layer_id] != source_layer_name) {
1698        ++target_layer_id;
1699      }
1700      if (target_layer_id == layer_names_.size()) {
1701        continue;
1702      }
1703      const LayerParameter&amp; layer_param = layers_[target_layer_id]-&gt;layer_param();
1704      const string&amp; engine_name = layer_param.engine();
1705      source_layer-&gt;set_engine(engine_name);
1706      if ((layer_param.type().compare(&quot;BatchNorm&quot;) == 0) &amp;&amp;
1707          (layer_param.batch_norm_param().has_engine())) {
1708        source_layer-&gt;mutable_batch_norm_param()-&gt;set_engine(layer_param.batch_norm_param().engine());
1709      }
1710    }
1711    NetParameter param_compiled;
1712    CompileNet(param, &amp;param_compiled);
1713    param = param_compiled;
1714  #ifdef USE_MLSL
1715    NetParameter param_mn;
1716    if (mn::is_multinode()) {
1717      CopyMultinodeParamsFromNet&lt;Dtype&gt;(this, &amp;param);
1718      ApplyMultinodeParams&lt;Dtype&gt;(param, &amp;param_mn);
1719      param = param_mn;
1720    }
1721  #endif
1722    num_source_layers = param.layer_size();
1723    for (int i = 0; i &lt; num_source_layers; ++i) {
1724      const LayerParameter&amp; source_layer = param.layer(i);
1725      const string&amp; source_layer_name = source_layer.name();
1726      int target_layer_id = 0;
1727      while (target_layer_id != layer_names_.size() &amp;&amp;
1728          layer_names_[target_layer_id] != source_layer_name) {
1729        ++target_layer_id;
1730      }
1731      if (target_layer_id == layer_names_.size()) {
1732        LOG(INFO) &lt;&lt; &quot;Ignoring source layer &quot; &lt;&lt; source_layer_name;
1733        continue;
1734      }
1735      DLOG(INFO) &lt;&lt; &quot;Copying source layer &quot; &lt;&lt; source_layer_name;
1736      vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; target_blobs =
1737          layers_[target_layer_id]-&gt;blobs();
1738      CHECK_EQ(target_blobs.size(), source_layer.blobs_size())
1739          &lt;&lt; &quot;Incompatible number of blobs for layer &quot; &lt;&lt; source_layer_name;
1740      for (int j = 0; j &lt; target_blobs.size(); ++j) {
1741        if (!target_blobs[j]-&gt;ShapeEquals(source_layer.blobs(j))) {
1742          Blob&lt;Dtype&gt; source_blob;
1743          const bool kReshape = true;
1744          source_blob.FromProto(source_layer.blobs(j), kReshape);
1745          LOG(FATAL) &lt;&lt; &quot;Cannot copy param &quot; &lt;&lt; j &lt;&lt; &quot; weights from layer &#x27;&quot;
1746              &lt;&lt; source_layer_name &lt;&lt; &quot;&#x27;; shape mismatch.  Source param shape is &quot;
1747              &lt;&lt; source_blob.shape_string() &lt;&lt; &quot;; target param shape is &quot;
1748              &lt;&lt; target_blobs[j]-&gt;shape_string() &lt;&lt; &quot;. &quot;
1749              &lt;&lt; &quot;To learn this layer&#x27;s parameters from scratch rather than &quot;
1750              &lt;&lt; &quot;copying from a saved net, rename the layer.&quot;;
1751        }
1752        const bool kReshape = false;
1753        target_blobs[j]-&gt;FromProto(source_layer.blobs(j), kReshape);
1754      }
1755    }
1756  }
1757  template &lt;typename Dtype&gt;
1758  void Net&lt;Dtype&gt;::CopyTrainedLayersFrom(const string trained_filename) {
1759    if (trained_filename.size() &gt;= 3 &amp;&amp;
1760        trained_filename.compare(trained_filename.size() - 3, 3, &quot;.h5&quot;) == 0) {
1761      CopyTrainedLayersFromHDF5(trained_filename);
1762    } else {
1763      CopyTrainedLayersFromBinaryProto(trained_filename);
1764    }
1765  }
1766  template &lt;typename Dtype&gt;
1767  void Net&lt;Dtype&gt;::CopyTrainedLayersFromBinaryProto(
1768      const string trained_filename) {
1769    NetParameter param;
1770    ReadNetParamsFromBinaryFileOrDie(trained_filename, &amp;param);
1771    CopyTrainedLayersFrom(param);
1772  }
1773  template &lt;typename Dtype&gt;
1774  void Net&lt;Dtype&gt;::CopyTrainedLayersFromHDF5(const string trained_filename) {
1775    hid_t file_hid = H5Fopen(trained_filename.c_str(), H5F_ACC_RDONLY,
1776                             H5P_DEFAULT);
1777    CHECK_GE(file_hid, 0) &lt;&lt; &quot;Couldn&#x27;t open &quot; &lt;&lt; trained_filename;
1778    hid_t data_hid = H5Gopen2(file_hid, &quot;data&quot;, H5P_DEFAULT);
1779    CHECK_GE(data_hid, 0) &lt;&lt; &quot;Error reading weights from &quot; &lt;&lt; trained_filename;
1780    int num_layers = hdf5_get_num_links(data_hid);
1781    for (int i = 0; i &lt; num_layers; ++i) {
1782      string source_layer_name = hdf5_get_name_by_idx(data_hid, i);
1783      if (!layer_names_index_.count(source_layer_name)) {
1784        LOG(INFO) &lt;&lt; &quot;Ignoring source layer &quot; &lt;&lt; source_layer_name;
1785        continue;
1786      }
1787      int target_layer_id = layer_names_index_[source_layer_name];
1788      DLOG(INFO) &lt;&lt; &quot;Copying source layer &quot; &lt;&lt; source_layer_name;
1789      vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; target_blobs =
1790          layers_[target_layer_id]-&gt;blobs();
1791      hid_t layer_hid = H5Gopen2(data_hid, source_layer_name.c_str(),
1792          H5P_DEFAULT);
1793      CHECK_GE(layer_hid, 0)
1794          &lt;&lt; &quot;Error reading weights from &quot; &lt;&lt; trained_filename;
1795      int num_source_params = hdf5_get_num_links(layer_hid);
1796      CHECK_LE(num_source_params, target_blobs.size())
1797          &lt;&lt; &quot;Incompatible number of blobs for layer &quot; &lt;&lt; source_layer_name;
1798      for (int j = 0; j &lt; target_blobs.size(); ++j) {
1799        ostringstream oss;
1800        oss &lt;&lt; j;
1801        string dataset_name = oss.str();
1802        int target_net_param_id = param_id_vecs_[target_layer_id][j];
1803        if (!H5Lexists(layer_hid, dataset_name.c_str(), H5P_DEFAULT)) {
1804          if (param_owners_[target_net_param_id] != -1) {
1805            continue;
1806          } else {
1807            LOG(FATAL) &lt;&lt; &quot;Incompatible number of blobs for layer &quot;
1808                &lt;&lt; source_layer_name;
1809          }
1810        }
1811  #ifdef USE_MLSL
1812        const MultinodeLayerParameter &amp;mn_layer_param =
1813          layers_[target_layer_id]-&gt;layer_param().multinode();
1814        int num_nodes = mn_layer_param.num_nodes();
1815        int model_parts = mn_layer_param.model_parts();
1816        mn::GetCanonicalMnParam(num_nodes, model_parts);
1817        Blob&lt;Dtype&gt; orig_blob;
1818        vector&lt;int&gt; shape = target_blobs[j]-&gt;shape();
1819        CHECK_GT(shape.size(), 0);
1820        int offset = 0;
1821        if (model_parts &gt; 1) {
1822          shape[0] *= model_parts;
1823          offset = target_blobs[j]-&gt;count() * (mn::get_node_id() % model_parts);
1824        }
1825        orig_blob.Reshape(shape);
1826        hdf5_load_nd_dataset(layer_hid, dataset_name.c_str(), 0, kMaxBlobAxes,
1827            &amp;orig_blob);
1828        caffe_copy(target_blobs[j]-&gt;count(), orig_blob.cpu_data() + offset,
1829                   target_blobs[j]-&gt;mutable_cpu_data());
1830  #else
1831        hdf5_load_nd_dataset(layer_hid, dataset_name.c_str(), 0, kMaxBlobAxes,
1832            target_blobs[j].get());
1833  #endif
1834      }
1835      H5Gclose(layer_hid);
1836    }
1837    H5Gclose(data_hid);
1838    H5Fclose(file_hid);
1839  }
1840  template &lt;typename Dtype&gt;
1841  void Net&lt;Dtype&gt;::ToProto(NetParameter* param, bool write_diff) const {
1842    param-&gt;Clear();
1843    param-&gt;set_name(name_);
1844    DLOG(INFO) &lt;&lt; &quot;Serializing &quot; &lt;&lt; layers_.size() &lt;&lt; &quot; layers&quot;;
1845    for (int i = 0; i &lt; layers_.size(); ++i) {
1846      LayerParameter* layer_param = param-&gt;add_layer();
1847      layers_[i]-&gt;ToProto(layer_param, write_diff);
1848    }
1849  #ifdef USE_MLSL
1850    if (mn::is_multinode()) {
1851      RevertMultinodeParams&lt;Dtype&gt;(param, write_diff);
1852    }
1853  #endif
1854  }
1855  template &lt;typename Dtype&gt;
1856  void Net&lt;Dtype&gt;::ToHDF5(const string&amp; filename, bool write_diff) const {
1857    hid_t file_hid = -1;
1858    hid_t data_hid = -1;
1859    hid_t diff_hid = -1;
1860  #ifdef USE_MLSL
1861    if (mn::is_root()) {
1862  #endif
1863    file_hid = H5Fcreate(filename.c_str(), H5F_ACC_TRUNC, H5P_DEFAULT,
1864        H5P_DEFAULT);
1865    CHECK_GE(file_hid, 0)
1866        &lt;&lt; &quot;Couldn&#x27;t open &quot; &lt;&lt; filename &lt;&lt; &quot; to save weights.&quot;;
1867    data_hid = H5Gcreate2(file_hid, &quot;data&quot;, H5P_DEFAULT, H5P_DEFAULT,
1868        H5P_DEFAULT);
1869    CHECK_GE(data_hid, 0) &lt;&lt; &quot;Error saving weights to &quot; &lt;&lt; filename &lt;&lt; &quot;.&quot;;
1870    if (write_diff) {
1871      diff_hid = H5Gcreate2(file_hid, &quot;diff&quot;, H5P_DEFAULT, H5P_DEFAULT,
1872          H5P_DEFAULT);
1873      CHECK_GE(diff_hid, 0) &lt;&lt; &quot;Error saving weights to &quot; &lt;&lt; filename &lt;&lt; &quot;.&quot;;
1874    }
1875  #ifdef USE_MLSL
1876    }
1877  #endif
1878    for (int layer_id = 0; layer_id &lt; layers_.size(); ++layer_id) {
1879      const LayerParameter&amp; layer_param = layers_[layer_id]-&gt;layer_param();
1880  #ifdef USE_MLSL
1881      if (layer_param.type() == &quot;MnActivation&quot;) continue;
1882  #endif
1883      hid_t layer_data_hid = -1;
1884      hid_t layer_diff_hid = -1;
1885  #ifdef USE_MLSL
1886      if (mn::is_root()) {
1887  #endif
1888        string layer_name = layer_param.name();
1889        layer_data_hid = H5Gcreate2(data_hid, layer_name.c_str(),
1890            H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
1891        CHECK_GE(layer_data_hid, 0)
1892          &lt;&lt; &quot;Error saving weights to &quot; &lt;&lt; filename &lt;&lt; &quot;.&quot;;
1893        if (write_diff) {
1894          layer_diff_hid = H5Gcreate2(diff_hid, layer_name.c_str(),
1895              H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
1896          CHECK_GE(layer_diff_hid, 0)
1897            &lt;&lt; &quot;Error saving weights to &quot; &lt;&lt; filename &lt;&lt; &quot;.&quot;;
1898        }
1899  #ifdef USE_MLSL
1900      }
1901  #endif
1902      int num_params = layers_[layer_id]-&gt;blobs().size();
1903      for (int param_id = 0; param_id &lt; num_params; ++param_id) {
1904        ostringstream dataset_name;
1905        dataset_name &lt;&lt; param_id;
1906        const int net_param_id = param_id_vecs_[layer_id][param_id];
1907  #ifdef USE_MLSL
1908        const MultinodeLayerParameter &amp;mn_layer_param = layer_param.multinode();
1909        int num_nodes = mn_layer_param.num_nodes();
1910        int model_parts = mn_layer_param.model_parts();
1911        mn::GetCanonicalMnParam(num_nodes, model_parts);
1912        Blob&lt;Dtype&gt; new_blob;
1913        vector&lt;int&gt; shape = params_[net_param_id]-&gt;shape();
1914        CHECK_GT(shape.size(), 0);
1915        if (model_parts &gt; 1) {
1916          mn::Distribution *distrib = mn::get_distrib(num_nodes/model_parts, model_parts);
1917          shape[0] *= model_parts;
1918          new_blob.Reshape(shape);
1919          distrib-&gt;allgather&lt;Dtype,MLSL::GT_MODEL&gt;(
1920            params_[net_param_id]-&gt;mutable_cpu_data(),
1921            params_[net_param_id]-&gt;count(),
1922            new_blob.mutable_cpu_data());
1923          if (write_diff) {
1924            distrib-&gt;allgather&lt;Dtype,MLSL::GT_MODEL&gt;(
1925              params_[net_param_id]-&gt;mutable_cpu_diff(),
1926              params_[net_param_id]-&gt;count(),
1927              new_blob.mutable_cpu_diff());
1928          }
1929        } else {
1930          new_blob.Reshape(shape);
1931          caffe_copy(new_blob.count(), params_[net_param_id]-&gt;cpu_data(),
1932                     new_blob.mutable_cpu_data());
1933          if (write_diff) {
1934            caffe_copy(new_blob.count(), params_[net_param_id]-&gt;cpu_diff(),
1935                       new_blob.mutable_cpu_diff());
1936          }
1937        }
1938        if (mn::is_root()) {
1939          if (param_owners_[net_param_id] == -1) {
1940            hdf5_save_nd_dataset&lt;Dtype&gt;(layer_data_hid, dataset_name.str(),
1941                new_blob);
1942          }
1943          if (write_diff) {
1944            hdf5_save_nd_dataset&lt;Dtype&gt;(layer_diff_hid, dataset_name.str(),
1945                new_blob, true);
1946          }
1947        }
1948  #else
1949        if (param_owners_[net_param_id] == -1) {
1950          hdf5_save_nd_dataset&lt;Dtype&gt;(layer_data_hid, dataset_name.str(),
1951              *params_[net_param_id]);
1952        }
1953        if (write_diff) {
1954          hdf5_save_nd_dataset&lt;Dtype&gt;(layer_diff_hid, dataset_name.str(),
1955              *params_[net_param_id], true);
1956        }
1957  #endif
1958      }
1959  #ifdef USE_MLSL
1960      if (mn::is_root()) {
1961  #endif
1962      H5Gclose(layer_data_hid);
1963      if (write_diff) {
1964        H5Gclose(layer_diff_hid);
1965      }
1966  #ifdef USE_MLSL
1967      }
1968  #endif
1969    }
1970  #ifdef USE_MLSL
1971    if (mn::is_root()) {
1972  #endif
1973    H5Gclose(data_hid);
1974    if (write_diff) {
1975      H5Gclose(diff_hid);
1976    }
1977    H5Fclose(file_hid);
1978  #ifdef USE_MLSL
1979    }
1980  #endif
1981  }
1982  template &lt;typename Dtype&gt;
1983  void Net&lt;Dtype&gt;::Update() {
1984    for (int i = 0; i &lt; learnable_params_.size(); ++i) {
1985      learnable_params_[i]-&gt;Update();
1986    }
1987  }
1988  template &lt;typename Dtype&gt;
1989  void Net&lt;Dtype&gt;::ClearParamDiffs(int learnable_param_id) {
1990    Blob&lt;Dtype&gt;* blob = learnable_params_[learnable_param_id];
1991    switch (Caffe::mode()) {
1992    case Caffe::CPU:
1993        if (blob-&gt;prv_diff())
1994          caffe_set(blob-&gt;prv_diff_count(), static_cast&lt;Dtype&gt;(0),
1995                    blob-&gt;mutable_prv_diff());
1996        else
1997          caffe_set(blob-&gt;count(), static_cast&lt;Dtype&gt;(0),
1998                    blob-&gt;mutable_cpu_diff());
1999      break;
2000    case Caffe::GPU:
2001  #ifndef CPU_ONLY
2002      caffe_gpu_set(blob-&gt;count(), static_cast&lt;Dtype&gt;(0),
2003                    blob-&gt;mutable_gpu_diff());
2004  #else
2005      NO_GPU;
2006  #endif
2007      break;
2008    }
2009  }
2010  template &lt;typename Dtype&gt;
2011  void Net&lt;Dtype&gt;::ClearParamDiffs() {
2012    ITER_TIMING_START();
2013    for (int i = 0; i &lt; learnable_params_.size(); ++i) {
2014      ClearParamDiffs(i);
2015    }
2016    ITER_TIMING_STOP(cleardiffs);
2017  }
2018  template &lt;typename Dtype&gt;
2019  void Net&lt;Dtype&gt;::ShareWeights() {
2020    for (int i = 0; i &lt; params_.size(); ++i) {
2021      if (param_owners_[i] &lt; 0) { continue; }
2022      params_[i]-&gt;ShareData(*params_[param_owners_[i]]);
2023      params_[i]-&gt;ShareDiff(*params_[param_owners_[i]]);
2024    }
2025  }
2026  template &lt;typename Dtype&gt;
2027  vector&lt;int&gt; Net&lt;Dtype&gt;::get_layer_learnable_param_ids(int layer_id) const {
2028    CHECK_GE(layer_id, 0);
2029    CHECK(layer_id &lt; param_id_vecs_.size());
2030    const vector&lt;int&gt;&amp; layer_param_ids = param_id_vecs_[layer_id];
2031    vector&lt;int&gt; ret;
2032    for (int i = 0; i &lt; layer_param_ids.size(); ++i) {
2033      ret.push_back(learnable_param_ids_[layer_param_ids[i]]);
2034      CHECK(params_[layer_param_ids[i]].get() == learnable_params_[ret.back()]);
2035    }
2036    return ret;
2037  }
2038  template &lt;typename Dtype&gt;
2039  bool Net&lt;Dtype&gt;::has_blob(const string&amp; blob_name) const {
2040    return blob_names_index_.find(blob_name) != blob_names_index_.end();
2041  }
2042  template &lt;typename Dtype&gt;
2043  const shared_ptr&lt;Blob&lt;Dtype&gt; &gt; Net&lt;Dtype&gt;::blob_by_name(
2044      const string&amp; blob_name) const {
2045    shared_ptr&lt;Blob&lt;Dtype&gt; &gt; blob_ptr;
2046    if (has_blob(blob_name)) {
2047      blob_ptr = blobs_[blob_names_index_.find(blob_name)-&gt;second];
2048    } else {
2049      blob_ptr.reset((Blob&lt;Dtype&gt;*)(NULL));
2050      LOG(WARNING) &lt;&lt; &quot;Unknown blob name &quot; &lt;&lt; blob_name;
2051    }
2052    return blob_ptr;
2053  }
2054  template &lt;typename Dtype&gt;
2055  bool Net&lt;Dtype&gt;::has_layer(const string&amp; layer_name) const {
2056    return layer_names_index_.find(layer_name) != layer_names_index_.end();
2057  }
2058  template &lt;typename Dtype&gt;
2059  const shared_ptr&lt;Layer&lt;Dtype&gt; &gt; Net&lt;Dtype&gt;::layer_by_name(
2060      const string&amp; layer_name) const {
2061    shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer_ptr;
2062    if (has_layer(layer_name)) {
2063      layer_ptr = layers_[layer_names_index_.find(layer_name)-&gt;second];
2064    } else {
2065      layer_ptr.reset((Layer&lt;Dtype&gt;*)(NULL));
2066      LOG(WARNING) &lt;&lt; &quot;Unknown layer name &quot; &lt;&lt; layer_name;
2067    }
2068    return layer_ptr;
2069  }
2070  #ifdef CAFFE_PER_LAYER_TIMINGS
2071  template &lt;typename Dtype&gt;
2072  void Net&lt;Dtype&gt;::InitTimers() {
2073    int layer_count = layers().size();
2074    this-&gt;forward_time_per_layer.resize(layer_count, 0.0);
2075    this-&gt;backward_time_per_layer.resize(layer_count, 0.0);
2076    this-&gt;update_time_per_layer.resize(layer_count, 0.0);
2077    this-&gt;cleardiffs_time_per_iter = 0.0;
2078    this-&gt;forward_time_per_layer_total.resize(layer_count, 0.0);
2079    this-&gt;backward_time_per_layer_total.resize(layer_count, 0.0);
2080    this-&gt;update_time_per_layer_total.resize(layer_count, 0.0);
2081    this-&gt;cleardiffs_time_per_iter_total = 0.0;
2082    this-&gt;forward_start_time_per_layer.resize(layer_count, 0.0);
2083    this-&gt;forward_stop_time_per_layer.resize(layer_count, 0.0);
2084    this-&gt;backward_start_time_per_layer.resize(layer_count, 0.0);
2085    this-&gt;backward_stop_time_per_layer.resize(layer_count, 0.0);
2086    this-&gt;update_start_time_per_layer.resize(layer_count, 0.0);
2087    this-&gt;update_stop_time_per_layer.resize(layer_count, 0.0);
2088  #ifdef USE_MLSL
2089    this-&gt;startcomm_time_per_layer.resize(layer_count, 0.0);
2090    this-&gt;waitcomm_time_per_layer.resize(layer_count, 0.0);
2091    this-&gt;startcomm_time_per_layer_total.resize(layer_count, 0.0);
2092    this-&gt;waitcomm_time_per_layer_total.resize(layer_count, 0.0);
2093    this-&gt;startcomm_start_time_per_layer.resize(layer_count, 0.0);
2094    this-&gt;startcomm_stop_time_per_layer.resize(layer_count, 0.0);
2095  #ifdef FW_OVERLAP_OPT
2096    this-&gt;first_update_start_time_per_layer.resize(layer_count, 0.0);
2097    this-&gt;first_update_stop_time_per_layer.resize(layer_count, 0.0);
2098    this-&gt;first_waitcomm_start_time_per_layer.resize(layer_count, 0.0);
2099    this-&gt;first_waitcomm_stop_time_per_layer.resize(layer_count, 0.0);
2100  #endif
2101    this-&gt;waitcomm_start_time_per_layer.resize(layer_count, 0.0);
2102    this-&gt;waitcomm_stop_time_per_layer.resize(layer_count, 0.0);
2103  #endif
2104    timer.InitTime();
2105  #ifdef FW_OVERLAP_OPT
2106    wait_timer.InitTime(timer);
2107  #endif
2108  }
2109  template &lt;typename Dtype&gt;
2110  void Net&lt;Dtype&gt;::ResetTimers() {
2111    std::transform(this-&gt;forward_time_per_layer_total.begin(),
2112        this-&gt;forward_time_per_layer_total.end(),
2113        this-&gt;forward_time_per_layer.begin(),
2114        this-&gt;forward_time_per_layer_total.begin(),
2115        std::plus&lt;double&gt;());
2116    std::transform(this-&gt;backward_time_per_layer_total.begin(),
2117        this-&gt;backward_time_per_layer_total.end(),
2118        this-&gt;backward_time_per_layer.begin(),
2119        this-&gt;backward_time_per_layer_total.begin(),
2120        std::plus&lt;double&gt;());
2121    std::transform(this-&gt;update_time_per_layer_total.begin(),
2122        this-&gt;update_time_per_layer_total.end(),
2123        this-&gt;update_time_per_layer.begin(),
2124        this-&gt;update_time_per_layer_total.begin(),
2125        std::plus&lt;double&gt;());
2126    this-&gt;cleardiffs_time_per_iter_total += this-&gt;cleardiffs_time_per_iter;
2127  #ifdef USE_MLSL
2128    std::transform(this-&gt;startcomm_time_per_layer_total.begin(),
2129        this-&gt;startcomm_time_per_layer_total.end(),
2130        this-&gt;startcomm_time_per_layer.begin(),
2131        this-&gt;startcomm_time_per_layer_total.begin(),
2132        std::plus&lt;double&gt;());
2133    std::transform(this-&gt;waitcomm_time_per_layer_total.begin(),
2134        this-&gt;waitcomm_time_per_layer_total.end(),
2135        this-&gt;waitcomm_time_per_layer.begin(),
2136        this-&gt;waitcomm_time_per_layer_total.begin(),
2137        std::plus&lt;double&gt;());
2138  #endif
2139    std::fill(this-&gt;forward_time_per_layer.begin(),
2140        this-&gt;forward_time_per_layer.end(), 0.0);
2141    std::fill(this-&gt;backward_time_per_layer.begin(),
2142        this-&gt;backward_time_per_layer.end(), 0.0);
2143    std::fill(this-&gt;update_time_per_layer.begin(),
2144        this-&gt;update_time_per_layer.end(), 0.0);
2145    this-&gt;cleardiffs_time_per_iter = 0.0;
2146  #ifdef USE_MLSL
2147    std::fill(this-&gt;startcomm_time_per_layer.begin(),
2148        this-&gt;startcomm_time_per_layer.end(), 0.0);
2149    std::fill(this-&gt;waitcomm_time_per_layer.begin(),
2150        this-&gt;waitcomm_time_per_layer.end(), 0.0);
2151  #endif
2152  }
2153  template &lt;typename Dtype&gt;
2154  void Net&lt;Dtype&gt;::PrintTimers(bool printTotal) {
2155  #ifdef USE_MLSL
2156    if (mn::get_node_id() != 0)
2157      return;
2158  #endif
2159    LOG(WARNING) &lt;&lt; std::endl;
2160    LOG(WARNING) &lt;&lt; &quot;####################################################&quot;;
2161    std::vector&lt;double&gt;&amp; forward_timers = printTotal ?
2162      forward_time_per_layer_total : forward_time_per_layer;
2163    std::vector&lt;double&gt;&amp; backward_timers = printTotal ?
2164      backward_time_per_layer_total : backward_time_per_layer;
2165    std::vector&lt;double&gt;&amp; update_timers = printTotal ?
2166      update_time_per_layer_total : update_time_per_layer;
2167    double cleardiffs_timer = printTotal ?
2168      cleardiffs_time_per_iter_total : cleardiffs_time_per_iter;
2169  #ifdef USE_MLSL
2170    std::vector&lt;double&gt;&amp; startcomm_timers = printTotal ?
2171      startcomm_time_per_layer_total : startcomm_time_per_layer;
2172    std::vector&lt;double&gt;&amp; waitcomm_timers = printTotal ?
2173      waitcomm_time_per_layer_total : waitcomm_time_per_layer;
2174  #endif
2175    std::string prefix = printTotal ? &quot;TOTAL &quot; : &quot;DELTA &quot;;
2176    double forward_time = std::accumulate(forward_timers.begin(),
2177        forward_timers.end(), 0.0) / 1000.0;
2178    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;FORWARD TIME: &quot; &lt;&lt; forward_time &lt;&lt; &quot; ms&quot;;
2179    for (int layer_idx = 0; layer_idx &lt; layers().size(); layer_idx++) {
2180      LOG(WARNING) &lt;&lt; &quot;LAYER-&quot; &lt;&lt; layer_idx &lt;&lt; &quot; &quot;
2181        &lt;&lt; layers()[layer_idx]-&gt;type()
2182        &lt;&lt; &quot;: forward_time: &quot; &lt;&lt; forward_timers[layer_idx] / 1000.0
2183        &lt;&lt; &quot; ms&quot;;
2184    }
2185    LOG(WARNING) &lt;&lt; std::endl;
2186    double backward_time = std::accumulate(backward_timers.begin(),
2187        backward_timers.end(), 0.0) / 1000.0;
2188    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;BACKWARD TIME: &quot; &lt;&lt; backward_time &lt;&lt; &quot; ms&quot;;
2189    for (int layer_idx = 0; layer_idx &lt; layers().size(); layer_idx++) {
2190      LOG(WARNING) &lt;&lt; &quot;LAYER-&quot; &lt;&lt; layer_idx &lt;&lt; &quot; &quot;
2191        &lt;&lt; layers()[layer_idx]-&gt;type()
2192        &lt;&lt; &quot;: backward_time: &quot; &lt;&lt; backward_timers[layer_idx] / 1000.0
2193        &lt;&lt; &quot; ms&quot;;
2194    }
2195    LOG(WARNING) &lt;&lt; std::endl;
2196    double update_time = std::accumulate(update_timers.begin(),
2197        update_timers.end(), 0.0) / 1000.0;
2198    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;UPDATE TIME: &quot; &lt;&lt; update_time &lt;&lt; &quot; ms&quot;;
2199    for (int layer_idx = 0; layer_idx &lt; layers().size(); layer_idx++) {
2200      LOG(WARNING) &lt;&lt; &quot;LAYER-&quot; &lt;&lt; layer_idx &lt;&lt; &quot; &quot;
2201        &lt;&lt; layers()[layer_idx]-&gt;type()
2202        &lt;&lt; &quot;: update_time: &quot; &lt;&lt; update_timers[layer_idx] / 1000.0
2203        &lt;&lt; &quot; ms&quot;;
2204    }
2205    LOG(WARNING) &lt;&lt; std::endl;
2206    double cleardiffs_time = cleardiffs_timer / 1000.0;
2207    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;CLEAR PARAMETER DIFFS TIME: &quot; &lt;&lt; cleardiffs_time &lt;&lt; &quot; ms&quot;;
2208    LOG(WARNING) &lt;&lt; std::endl;
2209  #ifdef USE_MLSL
2210    double startcomm_time = std::accumulate(startcomm_timers.begin(),
2211        startcomm_timers.end(), 0.0) / 1000.0;
2212    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;START COMMUNICATION TIME: &quot; &lt;&lt; startcomm_time &lt;&lt; &quot; ms&quot;;
2213    for (int layer_idx = 0; layer_idx &lt; layers().size(); layer_idx++) {
2214      LOG(WARNING) &lt;&lt; &quot;LAYER-&quot; &lt;&lt; layer_idx &lt;&lt; &quot; &quot;
2215        &lt;&lt; layers()[layer_idx]-&gt;type()
2216        &lt;&lt; &quot;: startcomm_time: &quot; &lt;&lt; startcomm_timers[layer_idx] / 1000.0
2217        &lt;&lt; &quot; ms&quot;;
2218    }
2219    LOG(WARNING) &lt;&lt; std::endl;
2220    double waitcomm_time = std::accumulate(waitcomm_timers.begin(),
2221        waitcomm_timers.end(), 0.0) / 1000.0;
2222    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;WAIT COMMUNICATION TIME: &quot; &lt;&lt; waitcomm_time &lt;&lt; &quot; ms&quot;;
2223    for (int layer_idx = 0; layer_idx &lt; layers().size(); layer_idx++) {
2224      LOG(WARNING) &lt;&lt; &quot;LAYER-&quot; &lt;&lt; layer_idx &lt;&lt; &quot; &quot;
2225        &lt;&lt; layers()[layer_idx]-&gt;type()
2226        &lt;&lt; &quot;: waitcomm_time: &quot; &lt;&lt; waitcomm_timers[layer_idx] / 1000.0
2227        &lt;&lt; &quot; ms&quot;;
2228    }
2229    LOG(WARNING) &lt;&lt; std::endl;
2230    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;TIME (Computation + Communication): &quot; &lt;&lt; (forward_time +
2231        backward_time + update_time + cleardiffs_time + startcomm_time + waitcomm_time) / 1000.0
2232      &lt;&lt; &quot; sec&quot;;
2233  #else
2234    LOG(WARNING) &lt;&lt; prefix &lt;&lt; &quot;TIME (Computation): &quot; &lt;&lt; (forward_time +
2235        backward_time + update_time + cleardiffs_time) / 1000.0 &lt;&lt; &quot; sec&quot;;
2236  #endif
2237    LOG(WARNING) &lt;&lt; &quot;####################################################&quot;;
2238    LOG(WARNING) &lt;&lt; std::endl;
2239  }
2240  template &lt;typename Dtype&gt;
2241  void Net&lt;Dtype&gt;::SaveTimeline() {
2242    static bool initialized = false;
2243    std::ofstream time_file;
2244    string filename = name() + &quot;_timeline&quot;
2245  #ifdef USE_MLSL
2246          + &quot;_&quot; + std::to_string(mn::get_node_id())
2247  #endif
2248          + &quot;.txt&quot;;
2249    if (initialized)
2250      time_file.open(filename, std::ios_base::app);
2251    else {
2252      initialized = true;
2253      time_file.open(filename);
2254    }
2255    time_file &lt;&lt; &quot;start,end,type,OP&quot; &lt;&lt; std::endl;
2256    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2257      if (forward_start_time_per_layer[layer_idx] == 0
2258          || forward_stop_time_per_layer[layer_idx] == 0)
2259          continue;
2260      time_file &lt;&lt; forward_start_time_per_layer[layer_idx] / 1000
2261          &lt;&lt; &quot;,&quot; &lt;&lt; forward_stop_time_per_layer[layer_idx] / 1000
2262          &lt;&lt; &quot;,Comp,&quot; &lt;&lt; layers()[layer_idx]-&gt;type()
2263          &lt;&lt; std::endl;
2264    }
2265    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2266      if (backward_start_time_per_layer[layer_idx] == 0
2267          || backward_stop_time_per_layer[layer_idx] == 0)
2268          continue;
2269      time_file &lt;&lt; backward_start_time_per_layer[layer_idx] / 1000
2270          &lt;&lt; &quot;,&quot; &lt;&lt; backward_stop_time_per_layer[layer_idx] / 1000
2271          &lt;&lt; &quot;,Comp,&quot; &lt;&lt; layers()[layer_idx]-&gt;type() &lt;&lt; &quot;Grad&quot;
2272          &lt;&lt; std::endl;
2273    }
2274  #if defined(USE_MLSL) &amp;&amp; defined(FW_OVERLAP_OPT) 
2275    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2276      if (first_update_start_time_per_layer[layer_idx] == 0
2277          || first_update_stop_time_per_layer[layer_idx] == 0)
2278          continue;
2279      time_file &lt;&lt; first_update_start_time_per_layer[layer_idx] / 1000
2280          &lt;&lt; &quot;,&quot; &lt;&lt; first_update_stop_time_per_layer[layer_idx] / 1000
2281          &lt;&lt; &quot;,Comp,&quot; &lt;&lt; layers()[layer_idx]-&gt;type() &lt;&lt; &quot;FirstUpdate&quot;
2282          &lt;&lt; std::endl;
2283    }
2284  #endif
2285    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2286      if (update_start_time_per_layer[layer_idx] == 0
2287          || update_stop_time_per_layer[layer_idx] == 0)
2288          continue;
2289      time_file &lt;&lt; update_start_time_per_layer[layer_idx] / 1000
2290          &lt;&lt; &quot;,&quot; &lt;&lt; update_stop_time_per_layer[layer_idx] / 1000
2291          &lt;&lt; &quot;,Comp,&quot; &lt;&lt; layers()[layer_idx]-&gt;type() &lt;&lt; &quot;Update&quot;
2292          &lt;&lt; std::endl;
2293    }
2294  #ifdef USE_MLSL
2295    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2296      if (startcomm_start_time_per_layer[layer_idx] == 0
2297          || startcomm_stop_time_per_layer[layer_idx] == 0)
2298          continue;
2299      time_file &lt;&lt; startcomm_start_time_per_layer[layer_idx] / 1000
2300          &lt;&lt; &quot;,&quot; &lt;&lt; startcomm_stop_time_per_layer[layer_idx] / 1000
2301          &lt;&lt; &quot;,Comm,&quot; &lt;&lt; layers()[layer_idx]-&gt;type() &lt;&lt; &quot;Start&quot;
2302          &lt;&lt; std::endl;
2303    }
2304  #ifdef FW_OVERLAP_OPT
2305    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2306      if (first_waitcomm_start_time_per_layer[layer_idx] == 0
2307          || first_waitcomm_stop_time_per_layer[layer_idx] == 0)
2308          continue;
2309      time_file &lt;&lt; first_waitcomm_start_time_per_layer[layer_idx] / 1000
2310          &lt;&lt; &quot;,&quot; &lt;&lt; first_waitcomm_stop_time_per_layer[layer_idx] / 1000
2311          &lt;&lt; &quot;,Comm,&quot; &lt;&lt; layers()[layer_idx]-&gt;type() &lt;&lt; &quot;FirstWait&quot;
2312          &lt;&lt; std::endl;
2313    }
2314  #endif
2315    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2316      if (waitcomm_start_time_per_layer[layer_idx] == 0
2317          || waitcomm_stop_time_per_layer[layer_idx] == 0)
2318          continue;
2319      time_file &lt;&lt; waitcomm_start_time_per_layer[layer_idx] / 1000
2320          &lt;&lt; &quot;,&quot; &lt;&lt; waitcomm_stop_time_per_layer[layer_idx] / 1000
2321          &lt;&lt; &quot;,Comm,&quot; &lt;&lt; layers()[layer_idx]-&gt;type() &lt;&lt; &quot;Wait&quot;
2322          &lt;&lt; std::endl;
2323    }
2324  #endif
2325    time_file.close();
2326  }
2327  template &lt;typename Dtype&gt;
2328  void Net&lt;Dtype&gt;::PrintPayloadSize() {
2329  #ifdef USE_MLSL
2330    if (mn::get_node_id() != 0)
2331      return;
2332  #endif
2333    int total_payload_size = 0;
2334    const vector&lt;Blob&lt;Dtype&gt; *&gt; &amp;net_params (learnable_params());
2335    LOG(WARNING) &lt;&lt; std::endl;
2336    LOG(WARNING) &lt;&lt; &quot;####################################################&quot;;
2337    for (int layer_idx = 0; layer_idx &lt; layers().size(); ++layer_idx) {
2338      std::vector&lt;int&gt; param_ids = get_layer_learnable_param_ids(layer_idx);
2339      for (int j = 0; j &lt; param_ids.size(); j++) {
2340        int layer_payload_size = net_params[param_ids[j]]-&gt;count();
2341        LOG(WARNING) &lt;&lt; &quot;LAYER-&quot; &lt;&lt; layer_idx &lt;&lt; &quot; &quot;
2342          &lt;&lt; layers()[layer_idx]-&gt;type()
2343          &lt;&lt; &quot;: payload_size: &quot; &lt;&lt; layer_payload_size
2344          &lt;&lt; &quot; units&quot;;
2345        total_payload_size += layer_payload_size;
2346      }
2347    }
2348    LOG(WARNING) &lt;&lt; &quot;TOTAL PAYLOAD SIZE: &quot; &lt;&lt; total_payload_size &lt;&lt; &quot; units&quot;;
2349    LOG(WARNING) &lt;&lt; &quot;####################################################&quot;;
2350    LOG(WARNING) &lt;&lt; std::endl;
2351  }
2352  #endif &amp;bsol;* CAFFE_PER_LAYER_TIMINGS */
2353  INSTANTIATE_CLASS(Net);
2354  }  
2355  #if defined(FOUNDED_MLSL_ROOT)
2356  #define DEF_MLSL(str) \
2357  const char *mlsl_root = #str; 
2358  __attribute__((constructor)) void lib_ctor()  {
2359      DEF_MLSL(FOUNDED_MLSL_ROOT);
2360      setenv(&quot;MLSL_ROOT&quot;, mlsl_root, 0);
2361  }
2362  #endif
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkl_deconvolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include &lt;vector&gt;
3  #include &quot;gtest/gtest.h&quot;
4  #include &quot;caffe/blob.hpp&quot;
5  #include &quot;caffe/common.hpp&quot;
6  #include &quot;caffe/filler.hpp&quot;
7  #include &quot;caffe/layers/mkl_layers.hpp&quot;
8  #include &quot;caffe/layers/deconv_layer.hpp&quot;
9  #include &quot;caffe/test/test_caffe_main.hpp&quot;
10  #include &quot;caffe/test/test_gradient_check_util.hpp&quot;
11  namespace caffe {
12  template &lt;typename TypeParam&gt;
13  class MKLDeconvolutionLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
14    typedef typename TypeParam::Dtype Dtype;
15   protected:
16    MKLDeconvolutionLayerTest()
17        : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
18          ref_blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
19          blob_bottom_2_(new Blob&lt;Dtype&gt;(2, 3, 6, 4)),
20          blob_top_(new Blob&lt;Dtype&gt;()),
21          ref_blob_top_(new Blob&lt;Dtype&gt;()),
22          blob_top_2_(new Blob&lt;Dtype&gt;()) {}
23    virtual void SetUp() {
24      FillerParameter filler_param;
25      filler_param.set_value(1.);
26      GaussianFiller&lt;Dtype&gt; filler(filler_param);
27      filler.Fill(this-&gt;blob_bottom_);
28      filler.Fill(this-&gt;ref_blob_bottom_);
29      filler.Fill(this-&gt;blob_bottom_2_);
30      blob_bottom_vec_.push_back(blob_bottom_);
31      blob_top_vec_.push_back(blob_top_);
32      ref_blob_bottom_vec_.push_back(ref_blob_bottom_);
33      ref_blob_top_vec_.push_back(ref_blob_top_);
34    }
35    virtual ~MKLDeconvolutionLayerTest() {
36      delete blob_bottom_;
37      delete ref_blob_bottom_;
38      delete blob_bottom_2_;
39      delete blob_top_;
40      delete ref_blob_top_;
41      delete blob_top_2_;
42    }
43    Blob&lt;Dtype&gt;* const blob_bottom_;
44    Blob&lt;Dtype&gt;* const ref_blob_bottom_;
45    Blob&lt;Dtype&gt;* const blob_bottom_2_;
46    Blob&lt;Dtype&gt;* const blob_top_;
47    Blob&lt;Dtype&gt;* const ref_blob_top_;
48    Blob&lt;Dtype&gt;* const blob_top_2_;
49    vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
50    vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
51    vector&lt;Blob&lt;Dtype&gt;*&gt; ref_blob_bottom_vec_;
52    vector&lt;Blob&lt;Dtype&gt;*&gt; ref_blob_top_vec_;
53  };
54  TYPED_TEST_CASE(MKLDeconvolutionLayerTest, TestDtypesAndDevices);
55  TYPED_TEST(MKLDeconvolutionLayerTest, TestSetup) {
56    typedef typename TypeParam::Dtype Dtype;
57    LayerParameter layer_param;
58    ConvolutionParameter* convolution_param =
59        layer_param.mutable_convolution_param();
60    convolution_param-&gt;add_kernel_size(3);
61    convolution_param-&gt;add_stride(2);
62    convolution_param-&gt;set_num_output(4);
63    this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
64    this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
65    shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
66        new MKLDeconvolutionLayer&lt;Dtype&gt;(layer_param));
67    layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
68    EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
69    EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 4);
70    EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 13);
71    EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 9);
72    EXPECT_EQ(this-&gt;blob_top_2_-&gt;num(), 2);
73    EXPECT_EQ(this-&gt;blob_top_2_-&gt;channels(), 4);
74    EXPECT_EQ(this-&gt;blob_top_2_-&gt;height(), 13);
75    EXPECT_EQ(this-&gt;blob_top_2_-&gt;width(), 9);
76    convolution_param-&gt;set_num_output(3);
77    convolution_param-&gt;set_group(3);
78    layer.reset(new MKLDeconvolutionLayer&lt;Dtype&gt;(layer_param));
79    layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
80    EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
81    EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 3);
82    EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 13);
83    EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 9);
84    EXPECT_EQ(this-&gt;blob_top_2_-&gt;num(), 2);
85    EXPECT_EQ(this-&gt;blob_top_2_-&gt;channels(), 3);
86    EXPECT_EQ(this-&gt;blob_top_2_-&gt;height(), 13);
87    EXPECT_EQ(this-&gt;blob_top_2_-&gt;width(), 9);
88  }
89  TYPED_TEST(MKLDeconvolutionLayerTest, TestSimpleMKLDeconvolution) {
90    typedef typename TypeParam::Dtype Dtype;
91    LayerParameter layer_param;
92    ConvolutionParameter* convolution_param =
93        layer_param.mutable_convolution_param();
94    convolution_param-&gt;add_kernel_size(3);
95    convolution_param-&gt;add_stride(2);
96    convolution_param-&gt;set_num_output(4);
97    convolution_param-&gt;mutable_weight_filler()-&gt;set_type(&quot;constant&quot;);
98    convolution_param-&gt;mutable_weight_filler()-&gt;set_value(1);
99    convolution_param-&gt;mutable_bias_filler()-&gt;set_type(&quot;constant&quot;);
100    convolution_param-&gt;mutable_bias_filler()-&gt;set_value(0.1);
101    shared_ptr&lt;Layer&lt;Dtype&gt; &gt; layer(
102        new MKLDeconvolutionLayer&lt;Dtype&gt;(layer_param));
103    shared_ptr&lt;Layer&lt;Dtype&gt; &gt; ref_layer(
104        new DeconvolutionLayer&lt;Dtype&gt;(layer_param));
105    layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
106    ref_layer-&gt;SetUp(this-&gt;ref_blob_bottom_vec_, this-&gt;ref_blob_top_vec_);
107    FillerParameter filler_param;
108    filler_param.set_value(1.);
109    ConstantFiller&lt;Dtype&gt; filler(filler_param);
110    filler.Fill(this-&gt;blob_bottom_);
111    filler.Fill(this-&gt;ref_blob_bottom_);
112    layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
113    ref_layer-&gt;Forward(this-&gt;ref_blob_bottom_vec_, this-&gt;ref_blob_top_vec_);
114    const Dtype* top_data = this-&gt;blob_top_-&gt;cpu_data();
115    const Dtype* ref_top_data = this-&gt;ref_blob_top_-&gt;cpu_data();
116    for (int n = 0; n &lt; this-&gt;blob_top_-&gt;num(); ++n) {
117      for (int c = 0; c &lt; this-&gt;blob_top_-&gt;channels(); ++c) {
118        for (int h = 0; h &lt; this-&gt;blob_top_-&gt;height(); ++h) {
119          for (int w = 0; w &lt; this-&gt;blob_top_-&gt;width(); ++w) {
120            Dtype expected = 3.1;
121            bool h_overlap = h % 2 == 0 &amp;&amp; h &gt; 0
122              &amp;&amp; h &lt; this-&gt;blob_top_-&gt;height() - 1;
123            bool w_overlap = w % 2 == 0 &amp;&amp; w &gt; 0
124              &amp;&amp; w &lt; this-&gt;blob_top_-&gt;width() - 1;
125            if (h_overlap &amp;&amp; w_overlap) {
126              expected += 9;
127            } else if (h_overlap || w_overlap) {
128              expected += 3;
129            }
130            EXPECT_NEAR(top_data[this-&gt;blob_top_-&gt;offset(n, c, h, w)],
131                expected, 1e-4);
132            EXPECT_NEAR(ref_top_data[this-&gt;blob_top_-&gt;offset(n, c, h, w)],
133                expected, 1e-4);
134          }
135        }
136      }
137    }
138    Dtype* top_diff = this-&gt;blob_top_-&gt;mutable_cpu_diff();
139    Dtype* ref_top_diff = this-&gt;ref_blob_top_-&gt;mutable_cpu_diff();
140    for( int n = 0; n &lt; this-&gt;blob_top_-&gt;num(); ++n) {
141        for( int c = 0; c &lt; this-&gt;blob_top_-&gt;channels(); ++c) {
142            for( int h=0; h &lt; this-&gt;blob_top_-&gt;height(); ++h) {
143                for(int w = 0; w &lt; this-&gt;blob_top_-&gt;width(); ++w) {
144                  top_diff[this-&gt;blob_top_-&gt;offset(n, c, h, w)] = ref_top_data[this-&gt;blob_top_-&gt;offset(n, c, h, w)];
145                  ref_top_diff[this-&gt;blob_top_-&gt;offset(n, c, h, w)] = ref_top_data[this-&gt;blob_top_-&gt;offset(n, c, h, w)];
146                }
147            }
148        }
149    }
150    vector&lt;bool&gt; need_backward({true});
151    layer-&gt;Backward(this-&gt;blob_top_vec_, need_backward, this-&gt;blob_bottom_vec_);
152    ref_layer-&gt;Backward(this-&gt;ref_blob_top_vec_, need_backward, this-&gt;ref_blob_bottom_vec_);
153    const Dtype* bottom_diff = this-&gt;blob_bottom_-&gt;cpu_diff();
154    const Dtype* ref_bottom_diff = this-&gt;ref_blob_bottom_-&gt;cpu_diff();
155    for( int n = 0; n &lt; this-&gt;blob_bottom_-&gt;num(); ++n) {
156        for( int c = 0; c &lt; this-&gt;blob_bottom_-&gt;channels(); ++c) {
157            for( int h=0; h &lt; this-&gt;blob_bottom_-&gt;height(); ++h) {
158                for(int w = 0; w &lt; this-&gt;blob_bottom_-&gt;width(); ++w) {
159                  EXPECT_NEAR(bottom_diff[this-&gt;blob_bottom_-&gt;offset(n, c, h, w)],
160                          ref_bottom_diff[this-&gt;blob_bottom_-&gt;offset(n, c, h, w)],
161                          1e-4);
162                }
163            }
164        }
165    }
166    for (int i = 0; i &lt; layer-&gt;blobs().size(); ++i) {
167        Blob&lt;Dtype&gt;* blob = layer-&gt;blobs()[i].get();
<span onclick='openModal()' class='match'>168        Blob&lt;Dtype&gt;* ref_blob = ref_layer-&gt;blobs()[i].get();
169        const Dtype* weights_diff = blob-&gt;cpu_diff();
</span>170        const Dtype* ref_weights_diff = ref_blob-&gt;cpu_diff();
171        for( int n = 0; n &lt; blob-&gt;num(); ++n) {
172            for( int c = 0; c &lt;blob-&gt;channels(); ++c) {
173                for( int h = 0; h &lt; blob-&gt;height(); ++h) {
174                    for( int w =0; w &lt; blob-&gt;width(); ++w) {
175                      EXPECT_NEAR(weights_diff[blob-&gt;offset(n, c, h, w)],
176                              ref_weights_diff[ref_blob-&gt;offset(n, c, h, w)],
177                              1e-3);
178                    }
179                }
180            }
181        }
182    }
183  }
184  TYPED_TEST(MKLDeconvolutionLayerTest, TestGradient) {
185    typedef typename TypeParam::Dtype Dtype;
186    LayerParameter layer_param;
187    ConvolutionParameter* convolution_param =
188        layer_param.mutable_convolution_param();
189    this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_2_);
190    this-&gt;blob_top_vec_.push_back(this-&gt;blob_top_2_);
191    convolution_param-&gt;add_kernel_size(2);
192    convolution_param-&gt;add_stride(1);
193    convolution_param-&gt;set_num_output(1);
194    convolution_param-&gt;mutable_weight_filler()-&gt;set_type(&quot;gaussian&quot;);
195    convolution_param-&gt;mutable_bias_filler()-&gt;set_type(&quot;gaussian&quot;);
196    MKLDeconvolutionLayer&lt;Dtype&gt; layer(layer_param);
197    GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
198    checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
199        this-&gt;blob_top_vec_);
200  }
201  TYPED_TEST(MKLDeconvolutionLayerTest, TestNDAgainst2D) {
202    typedef typename TypeParam::Dtype Dtype;
203    const int kernel_h = 11;
204    const int kernel_w = 13;
205    vector&lt;int&gt; bottom_shape(4);
206    bottom_shape[0] = 15;
207    bottom_shape[1] = 12;
208    bottom_shape[2] = kernel_h * 2;
209    bottom_shape[3] = kernel_w * 2;
210    FillerParameter filler_param;
211    GaussianFiller&lt;Dtype&gt; filler(filler_param);
212    for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
213      this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
214      filler.Fill(this-&gt;blob_bottom_vec_[i]);
215    }
216    LayerParameter layer_param;
217    ConvolutionParameter* convolution_param =
218        layer_param.mutable_convolution_param();
219    convolution_param-&gt;set_num_output(18);
220    convolution_param-&gt;set_bias_term(false);
221    convolution_param-&gt;set_group(6);
222    convolution_param-&gt;set_kernel_h(kernel_h);
223    convolution_param-&gt;set_kernel_w(kernel_w);
224    convolution_param-&gt;mutable_weight_filler()-&gt;set_type(&quot;gaussian&quot;);
225    Blob&lt;Dtype&gt; weights;
226    Blob&lt;Dtype&gt; top_diff;
227    bool copy_diff;
228    bool reshape;
229    {
230      MKLDeconvolutionLayer&lt;Dtype&gt; layer(layer_param);
231      layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
232      top_diff.ReshapeLike(*this-&gt;blob_top_);
233      filler.Fill(&amp;top_diff);
234      ASSERT_EQ(1, layer.blobs().size());
235      copy_diff = false; reshape = true;
236      weights.CopyFrom(*layer.blobs()[0], copy_diff, reshape);
237    }
238    vector&lt;bool&gt; propagate_down(1, true);
239    Blob&lt;Dtype&gt; result_2d;
240    Blob&lt;Dtype&gt; backward_result_2d;
241    Blob&lt;Dtype&gt; backward_weight_result_2d;
242    {
243      caffe_set(this-&gt;blob_top_-&gt;count(), Dtype(0),
244                this-&gt;blob_top_-&gt;mutable_cpu_data());
245      caffe_set(this-&gt;blob_bottom_-&gt;count(), Dtype(0),
246                this-&gt;blob_bottom_-&gt;mutable_cpu_diff());
247      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
248      convolution_param-&gt;set_force_nd_im2col(false);
249      MKLDeconvolutionLayer&lt;Dtype&gt; layer_2d(layer_param);
250      layer_2d.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
251      ASSERT_EQ(1, layer_2d.blobs().size());
252      copy_diff = false; reshape = false;
253      layer_2d.blobs()[0]-&gt;CopyFrom(weights, copy_diff, reshape);
254      layer_2d.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
255      copy_diff = false; reshape = true;
256      result_2d.CopyFrom(*this-&gt;blob_top_, copy_diff, reshape);
257      ASSERT_EQ(this-&gt;blob_top_-&gt;shape(), top_diff.shape());
258      caffe_copy(top_diff.count(), top_diff.cpu_data(),
259                 this-&gt;blob_top_-&gt;mutable_cpu_diff());
260      layer_2d.Backward(this-&gt;blob_top_vec_, propagate_down,
261                        this-&gt;blob_bottom_vec_);
262      copy_diff = true; reshape = true;
263      backward_result_2d.CopyFrom(*this-&gt;blob_bottom_, copy_diff, reshape);
264      backward_weight_result_2d.CopyFrom(weights, copy_diff, reshape);
265    }
266    Blob&lt;Dtype&gt; result_nd;
267    Blob&lt;Dtype&gt; backward_result_nd;
268    Blob&lt;Dtype&gt; backward_weight_result_nd;
269    {
270      caffe_set(this-&gt;blob_top_-&gt;count(), Dtype(0),
271                this-&gt;blob_top_-&gt;mutable_cpu_data());
272      caffe_set(this-&gt;blob_bottom_-&gt;count(), Dtype(0),
273                this-&gt;blob_bottom_-&gt;mutable_cpu_diff());
274      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
275      convolution_param-&gt;set_force_nd_im2col(true);
276      MKLDeconvolutionLayer&lt;Dtype&gt; layer_nd(layer_param);
277      layer_nd.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
278      ASSERT_EQ(1, layer_nd.blobs().size());
279      copy_diff = false; reshape = false;
280      layer_nd.blobs()[0]-&gt;CopyFrom(weights, copy_diff, reshape);
281      layer_nd.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
282      copy_diff = false; reshape = true;
283      result_nd.CopyFrom(*this-&gt;blob_top_, copy_diff, reshape);
284      ASSERT_EQ(this-&gt;blob_top_-&gt;shape(), top_diff.shape());
285      caffe_copy(top_diff.count(), top_diff.cpu_data(),
286                 this-&gt;blob_top_-&gt;mutable_cpu_diff());
287      layer_nd.Backward(this-&gt;blob_top_vec_, propagate_down,
288                        this-&gt;blob_bottom_vec_);
289      copy_diff = true; reshape = true;
290      backward_result_nd.CopyFrom(*this-&gt;blob_bottom_, copy_diff, reshape);
291      backward_weight_result_nd.CopyFrom(weights, copy_diff, reshape);
292    }
293    ASSERT_EQ(result_nd.count(), result_2d.count());
294    for (int i = 0; i &lt; result_2d.count(); ++i)  {
295      EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
296    }
297    ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
298    for (int i = 0; i &lt; backward_result_2d.count(); ++i) {
299      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
300                backward_result_nd.cpu_diff()[i]);
301    }
302    ASSERT_EQ(backward_weight_result_nd.count(),
303              backward_weight_result_2d.count());
304    for (int i = 0; i &lt; backward_weight_result_2d.count(); ++i) {
305      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
306                backward_weight_result_nd.cpu_diff()[i]);
307    }
308  }
309  #if 0
310  TYPED_TEST(MKLDeconvolutionLayerTest, TestGradient3D) {
311    typedef typename TypeParam::Dtype Dtype;
312    vector&lt;int&gt; bottom_shape(5);
313    bottom_shape[0] = this-&gt;blob_bottom_vec_[0]-&gt;shape(0);
314    bottom_shape[1] = this-&gt;blob_bottom_vec_[0]-&gt;shape(1);
315    bottom_shape[2] = 2;
316    bottom_shape[3] = 3;
317    bottom_shape[4] = 2;
318    FillerParameter filler_param;
319    GaussianFiller&lt;Dtype&gt; filler(filler_param);
320    for (int i = 0; i &lt; this-&gt;blob_bottom_vec_.size(); ++i) {
321      this-&gt;blob_bottom_vec_[i]-&gt;Reshape(bottom_shape);
322      filler.Fill(this-&gt;blob_bottom_vec_[i]);
323    }
324    LayerParameter layer_param;
325    ConvolutionParameter* convolution_param =
326        layer_param.mutable_convolution_param();
327    convolution_param-&gt;add_kernel_size(2);
328    convolution_param-&gt;add_stride(2);
329    convolution_param-&gt;add_pad(1);
330    convolution_param-&gt;set_num_output(2);
331    convolution_param-&gt;mutable_weight_filler()-&gt;set_type(&quot;gaussian&quot;);
332    convolution_param-&gt;mutable_bias_filler()-&gt;set_type(&quot;gaussian&quot;);
333    MKLDeconvolutionLayer&lt;Dtype&gt; layer(layer_param);
334    GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
335    checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
336        this-&gt;blob_top_vec_);
337  }
338  #endif
339  }  
340  #endif
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-net.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkl_deconvolution_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>1622      Layer&lt;Dtype&gt;* source_layer = other-&gt;layers()[i].get();
1623      const string&amp; source_layer_name = other-&gt;layer_names()[i];
</pre></code></div>
                <div class="column column_space"><pre><code>168        Blob&lt;Dtype&gt;* ref_blob = ref_layer-&gt;blobs()[i].get();
169        const Dtype* weights_diff = blob-&gt;cpu_diff();
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    