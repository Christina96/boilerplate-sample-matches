<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for lstm_layer.hpp & deconv_layer.hpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for lstm_layer.hpp & deconv_layer.hpp
      </h3>
      <h1 align="center">
        48.2%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>lstm_layer.hpp (35.0%)<TH>deconv_layer.hpp (77.77778%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match1197-0.html#0',2,'match1197-1.html#0',3)" NAME="0">(80-145)<TD><A HREF="javascript:ZweiFrames('match1197-0.html#0',2,'match1197-1.html#0',3)" NAME="0">(34-44)</A><TD ALIGN=center><FONT COLOR="#ff0000">23</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match1197-0.html#1',2,'match1197-1.html#1',3)" NAME="1">(47-53)<TD><A HREF="javascript:ZweiFrames('match1197-0.html#1',2,'match1197-1.html#1',3)" NAME="1">(28-34)</A><TD ALIGN=center><FONT COLOR="#850000">12</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>lstm_layer.hpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#ifndef CAFFE_LSTM_LAYER_HPP_
#define CAFFE_LSTM_LAYER_HPP_

#include &lt;string&gt;
#include &lt;utility&gt;
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/layer.hpp&quot;
#include &quot;caffe/layers/recurrent_layer.hpp&quot;
#include &quot;caffe/net.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;

namespace caffe {

template &lt;typename Dtype&gt; class RecurrentLayer;

/**
 * @brief Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM)
 *        [1] style recurrent neural network (RNN). Implemented by unrolling
 *        the LSTM computation through time.
 *
 * The specific architecture used in this implementation is as described in
 * &quot;Learning to Execute&quot; [2], reproduced below:
 *     i_t := \sigmoid[ W_{hi} * h_{t-1} + W_{xi} * x_t + b_i ]
 *     f_t := \sigmoid[ W_{hf} * h_{t-1} + W_{xf} * x_t + b_f ]
 *     o_t := \sigmoid[ W_{ho} * h_{t-1} + W_{xo} * x_t + b_o ]
 *     g_t :=    \tanh[ W_{hg} * h_{t-1} + W_{xg} * x_t + b_g ]
 *     c_t := (f_t .* c_{t-1}) + (i_t .* g_t)
 *     h_t := o_t .* \tanh[c_t]
 * In the implementation, the i, f, o, and g computations are performed as a
 * single inner product.
 *
 * Notably, this implementation lacks the &quot;diagonal&quot; gates, as used in the
 * LSTM architectures described by Alex Graves [3] and others.
 *
 * [1] Hochreiter, Sepp, and Schmidhuber, JÃ¼rgen. &quot;Long short-term memory.&quot;
 *     Neural Computation 9, no. 8 (1997): 1735-1780.
 *
 * [2] Zaremba, Wojciech, and Sutskever, Ilya. &quot;Learning to execute.&quot;
 *     arXiv preprint arXiv:1410.4615 (2014).
 *
<A NAME="1"></A> * [3] Graves, Alex. &quot;Generating sequences with recurrent neural networks.&quot;
 *     arXiv preprint arXiv:1308.0850 (2013).
 */
<FONT color="#f63526"><A HREF="javascript:ZweiFrames('match1197-1.html#1',3,'match1197-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>template &lt;typename Dtype&gt;
class LSTMLayer : public RecurrentLayer&lt;Dtype&gt; {
 public:
  explicit LSTMLayer(const LayerParameter&amp; param)
      : RecurrentLayer&lt;Dtype&gt;(param) {}

  virtual inline const char* type() const { return &quot;LSTM&quot;; }</B></FONT>

 protected:
  virtual void FillUnrolledNet(NetParameter* net_param) const;
  virtual void RecurrentInputBlobNames(vector&lt;string&gt;* names) const;
  virtual void RecurrentOutputBlobNames(vector&lt;string&gt;* names) const;
  virtual void RecurrentInputShapes(vector&lt;BlobShape&gt;* shapes) const;
  virtual void OutputBlobNames(vector&lt;string&gt;* names) const;
};

/**
 * @brief A helper for LSTMLayer: computes a single timestep of the
 *        non-linearity of the LSTM, producing the updated cell and hidden
 *        states.
 */
template &lt;typename Dtype&gt;
class LSTMUnitLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit LSTMUnitLayer(const LayerParameter&amp; param)
      : Layer&lt;Dtype&gt;(param) {}
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  virtual inline const char* type() const { return &quot;LSTMUnit&quot;; }
<A NAME="0"></A>  virtual inline int ExactNumBottomBlobs() const { return 3; }
  virtual inline int ExactNumTopBlobs() const { return 2; }

<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match1197-1.html#0',3,'match1197-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>  virtual inline bool AllowForceBackward(const int bottom_index) const {
    // Can't propagate to sequence continuation indicators.
    return bottom_index != 2;
  }

 protected:
  /**
   * @param bottom input Blob vector (length 3)
   *   -# @f$ (1 \times N \times D) @f$
   *      the previous timestep cell state @f$ c_{t-1} @f$
   *   -# @f$ (1 \times N \times 4D) @f$
   *      the &quot;gate inputs&quot; @f$ [i_t', f_t', o_t', g_t'] @f$
   *   -# @f$ (1 \times N) @f$
   *      the sequence continuation indicators  @f$ \delta_t @f$
   * @param top output Blob vector (length 2)
   *   -# @f$ (1 \times N \times D) @f$
   *      the updated cell state @f$ c_t @f$, computed as:
   *          i_t := \sigmoid[i_t']
   *          f_t := \sigmoid[f_t']
   *          o_t := \sigmoid[o_t']
   *          g_t := \tanh[g_t']
   *          c_t := cont_t * (f_t .* c_{t-1}) + (i_t .* g_t)
   *   -# @f$ (1 \times N \times D) @f$
   *      the updated hidden state @f$ h_t @f$, computed as:
   *          h_t := o_t .* \tanh[c_t]
   */
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);

  /**
   * @brief Computes the error gradient w.r.t. the LSTMUnit inputs.
   *
   * @param top output Blob vector (length 2), providing the error gradient with
   *        respect to the outputs
   *   -# @f$ (1 \times N \times D) @f$:
   *      containing error gradients @f$ \frac{\partial E}{\partial c_t} @f$
   *      with respect to the updated cell state @f$ c_t @f$
   *   -# @f$ (1 \times N \times D) @f$:
   *      containing error gradients @f$ \frac{\partial E}{\partial h_t} @f$
   *      with respect to the updated cell state @f$ h_t @f$
   * @param propagate_down see Layer::Backward.
   * @param bottom input Blob vector (length 3), into which the error gradients
   *        with respect to the LSTMUnit inputs @f$ c_{t-1} @f$ and the gate
   *        inputs are computed.  Computatation of the error gradients w.r.t.
   *        the sequence indicators is not implemented.
   *   -# @f$ (1 \times N \times D) @f$
   *      the error gradient w.r.t. the previous timestep cell state
   *      @f$ c_{t-1} @f$
   *   -# @f$ (1 \times N \times 4D) @f$
   *      the error gradient w.r.t. the &quot;gate inputs&quot;
   *      @f$ [
   *          \frac{\partial E}{\partial i_t}
   *          \frac{\partial E}{\partial f_t}
   *          \frac{\partial E}{\partial o_t}
   *          \frac{\partial E}{\partial g_t}
   *          ] @f$
   *   -# @f$ (1 \times 1 \times N) @f$
   *      the gradient w.r.t. the sequence continuation indicators
   *      @f$ \delta_t @f$ is currently not computed.
   */
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);</B></FONT>

  /// @brief The hidden and output dimension.
  int hidden_dim_;
  Blob&lt;Dtype&gt; X_acts_;
};

}  // namespace caffe

#endif  // CAFFE_LSTM_LAYER_HPP_
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>deconv_layer.hpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#ifndef CAFFE_DECONV_LAYER_HPP_
#define CAFFE_DECONV_LAYER_HPP_

#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/layer.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;

#include &quot;caffe/layers/base_conv_layer.hpp&quot;

namespace caffe {

/**
 * @brief Convolve the input with a bank of learned filters, and (optionally)
 *        add biases, treating filters and convolution parameters in the
 *        opposite sense as ConvolutionLayer.
 *
 *   ConvolutionLayer computes each output value by dotting an input window with
 *   a filter; DeconvolutionLayer multiplies each input value by a filter
 *   elementwise, and sums over the resulting output windows. In other words,
 *   DeconvolutionLayer is ConvolutionLayer with the forward and backward passes
 *   reversed. DeconvolutionLayer reuses ConvolutionParameter for its
 *   parameters, but they take the opposite sense as in ConvolutionLayer (so
<A NAME="1"></A> *   padding is removed from the output rather than added to the input, and
 *   stride results in upsampling rather than downsampling).
 */
<FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match1197-0.html#1',2,'match1197-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>template &lt;typename Dtype&gt;
class DeconvolutionLayer : public BaseConvolutionLayer&lt;Dtype&gt; {
 public:
<A NAME="0"></A>  explicit DeconvolutionLayer(const LayerParameter&amp; param)
      : BaseConvolutionLayer&lt;Dtype&gt;(param) {}

</B></FONT><FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match1197-0.html#0',2,'match1197-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>  virtual inline const char* type() const { return &quot;Deconvolution&quot;; }

 protected:
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);</B></FONT>
  virtual inline bool reverse_dimensions() { return true; }
  virtual void compute_output_shape();
};

}  // namespace caffe

#endif  // CAFFE_DECONV_LAYER_HPP_
</PRE>
</div>
  </div>
</body>
</html>
