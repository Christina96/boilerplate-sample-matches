
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 1.2077294685990339%, Tokens: 10, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-blob.cpp</h3>
            <pre><code>1  #include <climits>
2  #include <vector>
3  #include "caffe/blob.hpp"
4  #include "caffe/common.hpp"
5  #include "caffe/syncedmem.hpp"
6  #include "caffe/util/math_functions.hpp"
7  namespace caffe {
8  template <typename Dtype>
9  void Blob<Dtype>::Reshape(const int num, const int channels, const int height,
10      const int width) {
11    vector<int> shape(4);
12    shape[0] = num;
13    shape[1] = channels;
14    shape[2] = height;
15    shape[3] = width;
16    Reshape(shape);
17  }
18  template <typename Dtype>
19  void Blob<Dtype>::Reshape(const vector<int>& shape, bool reinitialize) {
20    CHECK_LE(shape.size(), kMaxBlobAxes);
21    count_ = 1;
22    shape_.resize(shape.size());
23  #ifndef CPU_ONLY
24    if (!shape_data_ || shape_data_->size() < shape.size() * sizeof(int)) {
25      shape_data_.reset(new SyncedMemory(shape.size() * sizeof(int)));
26    }
27    int* shape_data = static_cast<int*>(shape_data_->mutable_cpu_data());
28  #endif
29    bool actual_reshaping = false;
30    for (int i = 0; i < shape.size(); ++i) {
31      CHECK_GE(shape[i], 0);
32      if (count_ != 0) {
33        CHECK_LE(shape[i], LONG_MAX / count_) << "blob size exceeds LONG_MAX";
34      }
35      count_ *= shape[i];
36      if (shape_[i] != shape[i]) {
37        actual_reshaping = true;
38        shape_[i] = shape[i];
39      }
40  #ifndef CPU_ONLY
41      shape_data[i] = shape[i];
42  #endif
43    }
44    if ( reinitialize && ((actual_reshaping == true) || (count_ > capacity_))) {
45      capacity_ = count_;
46      data_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
47      diff_.reset(new SyncedMemory(capacity_ * sizeof(Dtype)));
48    }
49  }
50  template <typename Dtype>
51  void Blob<Dtype>::Reshape(const BlobShape& shape) {
52    CHECK_LE(shape.dim_size(), kMaxBlobAxes);
53    vector<int> shape_vec(shape.dim_size());
54    for (int i = 0; i < shape.dim_size(); ++i) {
55      shape_vec[i] = shape.dim(i);
56    }
57    Reshape(shape_vec);
58  }
59  template <typename Dtype>
60  void Blob<Dtype>::ReshapeLike(const Blob<Dtype>& other) {
61    Reshape(other.shape());
62  }
63  template <typename Dtype>
64  Blob<Dtype>::Blob(const int num, const int channels, const int height,
65      const int width)
66    : capacity_(0) {
67    Reshape(num, channels, height, width);
68  }
69  template <typename Dtype>
70  Blob<Dtype>::Blob(const vector<int>& shape)
71    : capacity_(0) {
72    Reshape(shape);
73  }
74  #ifndef CPU_ONLY
75  template <typename Dtype>
76  const int* Blob<Dtype>::gpu_shape() const {
77    CHECK(shape_data_);
78    return (const int*)shape_data_->gpu_data();
79  }
80  #endif
81  template <typename Dtype>
82  const Dtype* Blob<Dtype>::cpu_data() const {
83    CHECK(data_);
84    return (const Dtype*)data_->cpu_data();
85  }
86  template <typename Dtype>
87  void Blob<Dtype>::set_cpu_data(Dtype* data) {
88    CHECK(data);
89    data_->set_cpu_data(data);
90  }
91  template <typename Dtype>
92  void Blob<Dtype>::set_cpu_diff(Dtype* diff) {
93    CHECK(diff);
94    diff_->set_cpu_data(diff);
95  }
96  template <typename Dtype>
97  const Dtype* Blob<Dtype>::gpu_data() const {
98    CHECK(data_);
99    return (const Dtype*)data_->gpu_data();
100  }
101  template <typename Dtype>
102  const Dtype* Blob<Dtype>::cpu_diff() const {
103    CHECK(diff_);
104    return (const Dtype*)diff_->cpu_data();
105  }
106  #ifdef CO_SIM
107  template <typename Dtype>
108  Dtype* Blob<Dtype>::cpu_data_cosim() const {
109    CHECK(data_);
110    return (Dtype*)data_->cpu_data_cosim();
111  }
112  template <typename Dtype>
113  Dtype* Blob<Dtype>::cpu_diff_cosim() const {
114    CHECK(diff_);
115     return (Dtype*)diff_->cpu_data_cosim();
116  }
117  #endif
118  template <typename Dtype>
119  const Dtype* Blob<Dtype>::gpu_diff() const {
120    CHECK(diff_);
121    return (const Dtype*)diff_->gpu_data();
122  }
123  template <typename Dtype>
124  Dtype* Blob<Dtype>::mutable_cpu_data() {
125    CHECK(data_);
126    return static_cast<Dtype*>(data_->mutable_cpu_data());
127  }
128  template <typename Dtype>
129  Dtype* Blob<Dtype>::mutable_gpu_data() {
130    CHECK(data_);
131    return static_cast<Dtype*>(data_->mutable_gpu_data());
132  }
133  template <typename Dtype>
134  Dtype* Blob<Dtype>::mutable_cpu_diff() {
135    CHECK(diff_);
136    return static_cast<Dtype*>(diff_->mutable_cpu_data());
137  }
138  template <typename Dtype>
139  Dtype* Blob<Dtype>::mutable_gpu_diff() {
140    CHECK(diff_);
141    return static_cast<Dtype*>(diff_->mutable_gpu_data());
142  }
143  template <typename Dtype>
144  const Dtype* Blob<Dtype>::prv_data() const {
145    CHECK(data_);
146    return (const Dtype*)data_->prv_data();
147  }
148  template <typename Dtype>
149  Dtype* Blob<Dtype>::mutable_prv_data() {
150    CHECK(data_);
151    return static_cast<Dtype*>(data_->mutable_prv_data());
152  }
153  template <typename Dtype>
154  const Dtype* Blob<Dtype>::prv_diff() const {
155    CHECK(diff_);
156    return (const Dtype*)diff_->prv_data();
157  }
158  template <typename Dtype>
159  Dtype* Blob<Dtype>::mutable_prv_diff() {
160    CHECK(diff_);
161    return static_cast<Dtype*>(diff_->mutable_prv_data());
162  }
163  template <typename Dtype>
164  void Blob<Dtype>::set_prv_data_descriptor(shared_ptr<PrvMemDescr> descriptor,
165           bool same_data) {
166      CHECK(data_);
167      data_->set_prv_descriptor(descriptor, same_data);
168  }
169  template <typename Dtype>
170  void Blob<Dtype>::set_prv_diff_descriptor(shared_ptr<PrvMemDescr> descriptor,
171                   bool same_data) {
172    CHECK(diff_);
173    diff_->set_prv_descriptor(descriptor, same_data);
174  }
175  template <typename Dtype>
176  shared_ptr<PrvMemDescr> Blob<Dtype>::get_prv_data_descriptor() {
177    CHECK(data_);
178    return data_->prv_descriptor_;
179  }
180  template <typename Dtype>
181  shared_ptr<PrvMemDescr> Blob<Dtype>::get_prv_diff_descriptor() {
182    CHECK(diff_);
183    return diff_->prv_descriptor_;
184  }
185  template <typename Dtype>
186  void Blob<Dtype>::ShareData(const Blob& other) {
187    CHECK_EQ(count_, other.count());
188    if(this->shape() != other.shape() && other.data()) other.data()->mutable_cpu_data();
189    data_ = other.data();
190  }
191  template <typename Dtype>
192  void Blob<Dtype>::ShareDiff(const Blob& other) {
193    CHECK_EQ(count_, other.count());
194    if(this->shape() != other.shape() && other.diff()) other.diff()->mutable_cpu_data();
195    diff_ = other.diff();
196  }
197  template <> void Blob<unsigned int>::Update() { NOT_IMPLEMENTED; }
198  template <> void Blob<int>::Update() { NOT_IMPLEMENTED; }
199  template <> void Blob<bool>::Update() { NOT_IMPLEMENTED; }
200  template <typename Dtype>
201  void Blob<Dtype>::Update() {
202    switch (data_->head()) {
203    case SyncedMemory::SYNCED_PRV:
204    case SyncedMemory::HEAD_AT_PRV:
205      if (((diff_->head() == SyncedMemory::SYNCED_PRV) ||
206           (diff_->head() == SyncedMemory::HEAD_AT_PRV)) &&
207          diff_->prv_data() &&
208          data_->prv_data() &&
209          get_prv_data_descriptor()->layout_compare(
210                  get_prv_diff_descriptor())) {
211        caffe_axpy<Dtype>(prv_diff_count(), Dtype(-1),
212            static_cast<const Dtype*>(diff_->prv_data()),
213            static_cast<Dtype*>(data_->mutable_prv_data()));
214        break;
215      }
216    case SyncedMemory::HEAD_AT_CPU:
217      caffe_axpy<Dtype>(count_, Dtype(-1),
218          static_cast<const Dtype*>(diff_->cpu_data()),
219          static_cast<Dtype*>(data_->mutable_cpu_data()));
220      break;
221    case SyncedMemory::HEAD_AT_GPU:
222    case SyncedMemory::SYNCED:
223  #ifndef CPU_ONLY
224      caffe_gpu_axpy<Dtype>(count_, Dtype(-1),
225          static_cast<const Dtype*>(diff_->gpu_data()),
226          static_cast<Dtype*>(data_->mutable_gpu_data()));
227  #else
228      NO_GPU;
229  #endif
230      break;
231    default:
232      LOG(FATAL) << "Syncedmem not initialized.";
233    }
234  }
235  template <> unsigned int Blob<unsigned int>::asum_data() const {
236    NOT_IMPLEMENTED;
237    return 0;
238  }
239  template <> int Blob<int>::asum_data() const {
240    NOT_IMPLEMENTED;
241    return 0;
242  }
243  template <> bool Blob<bool>::asum_data() const {
244    NOT_IMPLEMENTED;
245    return 0;
246  }
247  template <typename Dtype>
248  Dtype Blob<Dtype>::asum_data() const {
249    if (!data_) { return 0; }
250    switch (data_->head()) {
251    case SyncedMemory::SYNCED_PRV:
252    case SyncedMemory::HEAD_AT_PRV:
253        {
254          if (prv_data() == NULL)
255            return caffe_cpu_asum(count_, cpu_data());
256          else
257            return caffe_cpu_asum(prv_data_count(), prv_data());
258        }
259    case SyncedMemory::HEAD_AT_CPU:
260      return caffe_cpu_asum(count_, cpu_data());
261    case SyncedMemory::HEAD_AT_GPU:
262    case SyncedMemory::SYNCED:
263  #ifndef CPU_ONLY
264    {
265      Dtype asum;
266      caffe_gpu_asum(count_, gpu_data(), &asum);
267      return asum;
268    }
269  #else
270      NO_GPU;
271  #endif
272    case SyncedMemory::UNINITIALIZED:
273      return 0;
274    default:
275      LOG(FATAL) << "Unknown SyncedMemory head state: " << data_->head();
276    }
277    return 0;
278  }
279  template <> unsigned int Blob<unsigned int>::asum_diff() const {
280    NOT_IMPLEMENTED;
281    return 0;
282  }
283  template <> int Blob<int>::asum_diff() const {
284    NOT_IMPLEMENTED;
285    return 0;
286  }
287  template <> bool Blob<bool>::asum_diff() const {
288    NOT_IMPLEMENTED;
289    return 0;
290  }
291  template <typename Dtype>
292  Dtype Blob<Dtype>::asum_diff() const {
293    if (!diff_) { return 0; }
294    switch (diff_->head()) {
295    case SyncedMemory::SYNCED_PRV:
296    case SyncedMemory::HEAD_AT_PRV:
297      {
298        if (prv_diff() == NULL) {
299          return caffe_cpu_asum(count_, cpu_diff());
300        }
301        else
302          return caffe_cpu_asum(prv_diff_count(), prv_diff());
303      }
304    case SyncedMemory::HEAD_AT_CPU:
305      return caffe_cpu_asum(count_, cpu_diff());
306    case SyncedMemory::HEAD_AT_GPU:
307    case SyncedMemory::SYNCED:
308  #ifndef CPU_ONLY
309    {
310      Dtype asum;
311      caffe_gpu_asum(count_, gpu_diff(), &asum);
312      return asum;
313    }
314  #else
315      NO_GPU;
316  #endif
317    case SyncedMemory::UNINITIALIZED:
318      return 0;
319    default:
320      LOG(FATAL) << "Unknown SyncedMemory head state: " << diff_->head();
321    }
322    return 0;
323  }
324  template <> unsigned int Blob<unsigned int>::sumsq_data() const {
325    NOT_IMPLEMENTED;
326    return 0;
327  }
328  template <> int Blob<int>::sumsq_data() const {
329    NOT_IMPLEMENTED;
330    return 0;
331  }
332  template <> bool Blob<bool>::sumsq_data() const {
333    NOT_IMPLEMENTED;
334    return 0;
335  }
336  template <typename Dtype>
337  Dtype Blob<Dtype>::sumsq_data() const {
338    Dtype sumsq;
339    const Dtype* data;
340    if (!data_) { return 0; }
341    switch (data_->head()) {
342    case SyncedMemory::SYNCED_PRV:
343    case SyncedMemory::HEAD_AT_PRV:
344        data = prv_data();
345        if (data == NULL) {
346          data = cpu_data();
347          sumsq = caffe_cpu_dot(count_, data, data);
348        } else {
349          sumsq = caffe_cpu_dot(prv_data_count(), data, data);
350        }
351        break;
352    case SyncedMemory::HEAD_AT_CPU:
353      data = cpu_data();
354      sumsq = caffe_cpu_dot(count_, data, data);
355      break;
356    case SyncedMemory::HEAD_AT_GPU:
357    case SyncedMemory::SYNCED:
358  #ifndef CPU_ONLY
359      data = gpu_data();
360      caffe_gpu_dot(count_, data, data, &sumsq);
361  #else
362      NO_GPU;
363  #endif
364      break;
365    case SyncedMemory::UNINITIALIZED:
366      return 0;
367    default:
368      LOG(FATAL) << "Unknown SyncedMemory head state: " << data_->head();
369    }
370    return sumsq;
371  }
372  template <> unsigned int Blob<unsigned int>::sumsq_diff() const {
373    NOT_IMPLEMENTED;
374    return 0;
375  }
376  template <> int Blob<int>::sumsq_diff() const {
377    NOT_IMPLEMENTED;
378    return 0;
379  }
380  template <> bool Blob<bool>::sumsq_diff() const {
381    NOT_IMPLEMENTED;
382    return 0;
383  }
384  template <typename Dtype>
385  Dtype Blob<Dtype>::sumsq_diff() const {
386    Dtype sumsq;
387    const Dtype* diff;
388    if (!diff_) { return 0; }
389    switch (diff_->head()) {
390    case SyncedMemory::SYNCED_PRV:
391    case SyncedMemory::HEAD_AT_PRV:
392        diff = prv_diff();
393        if (diff == NULL) {
394          diff = cpu_diff();
395          sumsq = caffe_cpu_dot(count_, diff, diff); 
396        } else {
397          sumsq = caffe_cpu_dot(prv_diff_count(), diff, diff);
398        }
399        break;
400    case SyncedMemory::HEAD_AT_CPU:
401      diff = cpu_diff();
402      sumsq = caffe_cpu_dot(count_, diff, diff);
403      break;
404    case SyncedMemory::HEAD_AT_GPU:
405    case SyncedMemory::SYNCED:
406  #ifndef CPU_ONLY
407      diff = gpu_diff();
408      caffe_gpu_dot(count_, diff, diff, &sumsq);
409      break;
410  #else
411      NO_GPU;
412  #endif
413    case SyncedMemory::UNINITIALIZED:
414      return 0;
415    default:
416      LOG(FATAL) << "Unknown SyncedMemory head state: " << diff_->head();
417    }
418    return sumsq;
419  }
420  template <> void Blob<unsigned int>::scale_data(unsigned int scale_factor) {
421    NOT_IMPLEMENTED;
422  }
423  template <> void Blob<int>::scale_data(int scale_factor) {
424    NOT_IMPLEMENTED;
425  }
426  template <> void Blob<bool>::scale_data(bool scale_factor) {
427    NOT_IMPLEMENTED;
428  }
429  template <typename Dtype>
430  void Blob<Dtype>::scale_data(Dtype scale_factor) {
431    Dtype* data;
432    if (!data_) { return; }
433    switch (data_->head()) {
434    case SyncedMemory::SYNCED_PRV:
435    case SyncedMemory::HEAD_AT_PRV:
436        if (prv_data() == NULL) {
437          data = mutable_cpu_data();
438          caffe_scal(count_, scale_factor, data);
439        } else {
440          data = mutable_prv_data();
441          caffe_scal(prv_data_count(), scale_factor, data);
442        }
443        break;
444    case SyncedMemory::HEAD_AT_CPU:
445      data = mutable_cpu_data();
446      caffe_scal(count_, scale_factor, data);
447      return;
448    case SyncedMemory::HEAD_AT_GPU:
449    case SyncedMemory::SYNCED:
450  #ifndef CPU_ONLY
451      data = mutable_gpu_data();
452      caffe_gpu_scal(count_, scale_factor, data);
453      return;
454  #else
455      NO_GPU;
456  #endif
457    case SyncedMemory::UNINITIALIZED:
458      return;
459    default:
460      LOG(FATAL) << "Unknown SyncedMemory head state: " << data_->head();
461    }
462  }
463  template <> void Blob<unsigned int>::scale_diff(unsigned int scale_factor) {
464    NOT_IMPLEMENTED;
465  }
466  template <> void Blob<int>::scale_diff(int scale_factor) {
467    NOT_IMPLEMENTED;
468  }
469  template <> void Blob<bool>::scale_diff(bool scale_factor) {
470    NOT_IMPLEMENTED;
471  }
472  template <typename Dtype>
473  void Blob<Dtype>::scale_diff(Dtype scale_factor) {
474    Dtype* diff;
475    if (!diff_) { return; }
476    switch (diff_->head()) {
477    case SyncedMemory::SYNCED_PRV:
478    case SyncedMemory::HEAD_AT_PRV:
479        if (prv_diff() == NULL) {
480          diff = mutable_cpu_diff();
481          caffe_scal(count_, scale_factor, diff);
482        } else {
483          diff = mutable_prv_diff();
484          caffe_scal(prv_diff_count(), scale_factor, diff);
485        }
486        break;
487    case SyncedMemory::HEAD_AT_CPU:
488      diff = mutable_cpu_diff();
489      caffe_scal(count_, scale_factor, diff);
490      return;
491    case SyncedMemory::HEAD_AT_GPU:
492    case SyncedMemory::SYNCED:
493  #ifndef CPU_ONLY
494      diff = mutable_gpu_diff();
495      caffe_gpu_scal(count_, scale_factor, diff);
496      return;
497  #else
498      NO_GPU;
499  #endif
500    case SyncedMemory::UNINITIALIZED:
501      return;
502    default:
503      LOG(FATAL) << "Unknown SyncedMemory head state: " << diff_->head();
504    }
505  }
506  template <typename Dtype>
507  bool Blob<Dtype>::ShapeEquals(const BlobProto& other) {
508    if (other.has_num() || other.has_channels() ||
509        other.has_height() || other.has_width()) {
510      return shape_.size() <= 4 &&
511             LegacyShape(-4) == other.num() &&
512             LegacyShape(-3) == other.channels() &&
513             LegacyShape(-2) == other.height() &&
514             LegacyShape(-1) == other.width();
515    }
516    vector<int> other_shape(other.shape().dim_size());
517    for (int i = 0; i < other.shape().dim_size(); ++i) {
518      other_shape[i] = other.shape().dim(i);
519    }
520    return shape_ == other_shape;
521  }
522  template <typename Dtype>
523  void Blob<Dtype>::CopyFrom(const Blob& source, bool copy_diff, bool reshape) {
524    if (source.count() != count_ || source.shape() != shape_) {
525      if (reshape) {
526        ReshapeLike(source);
527      } else {
528        LOG(FATAL) << "Trying to copy blobs of different sizes.";
529      }
530    }
531    switch (Caffe::mode()) {
532    case Caffe::GPU:
533      if (copy_diff) {
534        caffe_copy(count_, source.gpu_diff(),
535            static_cast<Dtype*>(diff_->mutable_gpu_data()));
536      } else {
537        caffe_copy(count_, source.gpu_data(),
538            static_cast<Dtype*>(data_->mutable_gpu_data()));
539      }
540      break;
541    case Caffe::CPU:
542      if (copy_diff) {
543        caffe_copy(count_, source.cpu_diff(),
544            static_cast<Dtype*>(diff_->mutable_cpu_data()));
545      } else {
546        caffe_copy(count_, source.cpu_data(),
547            static_cast<Dtype*>(data_->mutable_cpu_data()));
548      }
549      break;
550    default:
551      LOG(FATAL) << "Unknown caffe mode.";
552    }
553  }
554  template <typename Dtype>
555  void Blob<Dtype>::FromProto(const BlobProto& proto, bool reshape) {
556    if (reshape) {
<span onclick='openModal()' class='match'>557      vector<int> shape;
558      if (proto.has_num() || proto.has_channels() ||
559          proto.has_height() || proto.has_width()) {
560        shape.resize(4);
561        shape[0] = proto.num();
562        shape[1] = proto.channels();
563        shape[2] = proto.height();
564        shape[3] = proto.width();
565      } else {
</span>566        shape.resize(proto.shape().dim_size());
567        for (int i = 0; i < proto.shape().dim_size(); ++i) {
568          shape[i] = proto.shape().dim(i);
569        }
570      }
571      Reshape(shape);
572    } else {
573      CHECK(ShapeEquals(proto)) << "shape mismatch (reshape not set)";
574    }
575    Dtype* data_vec = mutable_cpu_data();
576    if (proto.double_data_size() > 0) {
577      CHECK_EQ(count_, proto.double_data_size());
578      for (int i = 0; i < count_; ++i) {
579        data_vec[i] = proto.double_data(i);
580      }
581    } else {
582      CHECK_EQ(count_, proto.data_size());
583      for (int i = 0; i < count_; ++i) {
584        data_vec[i] = proto.data(i);
585      }
586    }
587    if (proto.double_diff_size() > 0) {
588      CHECK_EQ(count_, proto.double_diff_size());
589      Dtype* diff_vec = mutable_cpu_diff();
590      for (int i = 0; i < count_; ++i) {
591        diff_vec[i] = proto.double_diff(i);
592      }
593    } else if (proto.diff_size() > 0) {
594      CHECK_EQ(count_, proto.diff_size());
595      Dtype* diff_vec = mutable_cpu_diff();
596      for (int i = 0; i < count_; ++i) {
597        diff_vec[i] = proto.diff(i);
598      }
599    }
600  }
601  template <>
602  void Blob<double>::ToProto(BlobProto* proto, bool write_diff) const {
603    proto->clear_shape();
604    for (int i = 0; i < shape_.size(); ++i) {
605      proto->mutable_shape()->add_dim(shape_[i]);
606    }
607    proto->clear_double_data();
608    proto->clear_double_diff();
609    const double* data_vec = cpu_data();
610    for (int i = 0; i < count_; ++i) {
611      proto->add_double_data(data_vec[i]);
612    }
613    if (write_diff) {
614      const double* diff_vec = cpu_diff();
615      for (int i = 0; i < count_; ++i) {
616        proto->add_double_diff(diff_vec[i]);
617      }
618    }
619  }
620  template <>
621  void Blob<float>::ToProto(BlobProto* proto, bool write_diff) const {
622    proto->clear_shape();
623    for (int i = 0; i < shape_.size(); ++i) {
624      proto->mutable_shape()->add_dim(shape_[i]);
625    }
626    proto->clear_data();
627    proto->clear_diff();
628    const float* data_vec = cpu_data();
629    for (int i = 0; i < count_; ++i) {
630      proto->add_data(data_vec[i]);
631    }
632    if (write_diff) {
633      const float* diff_vec = cpu_diff();
634      for (int i = 0; i < count_; ++i) {
635        proto->add_diff(diff_vec[i]);
636      }
637    }
638  }
639  INSTANTIATE_CLASS(Blob);
640  template class Blob<bool>;
641  template class Blob<int>;
642  template class Blob<size_t>;
643  template class Blob<unsigned int>;
644  }  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkl_convolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include <vector>
3  #include "gtest/gtest.h"
4  #include "caffe/blob.hpp"
5  #include "caffe/common.hpp"
6  #include "caffe/filler.hpp"
7  #include "caffe/layers/mkl_layers.hpp"
8  #include "caffe/test/test_caffe_main.hpp"
9  #include "caffe/test/test_gradient_check_util.hpp"
10  namespace caffe {
11  template <typename Dtype>
12  void caffe_conv(const Blob<Dtype>* in, ConvolutionParameter* conv_param,
13      const vector<shared_ptr<Blob<Dtype> > >& weights,
14      Blob<Dtype>* out) {
15    const bool has_depth = (out->num_axes() == 5);
16    if (!has_depth) { CHECK_EQ(4, out->num_axes()); }
17    int kernel_h, kernel_w;
18    if (conv_param->has_kernel_h() || conv_param->has_kernel_w()) {
19      kernel_h = conv_param->kernel_h();
20      kernel_w = conv_param->kernel_w();
21    } else {
22      kernel_h = kernel_w = conv_param->kernel_size(0);
23    }
24    int pad_h, pad_w;
25    if (conv_param->has_pad_h() || conv_param->has_pad_w()) {
26      pad_h = conv_param->pad_h();
27      pad_w = conv_param->pad_w();
28    } else {
29      pad_h = pad_w = conv_param->pad_size() ? conv_param->pad(0) : 0;
30    }
31    int stride_h, stride_w;
32    if (conv_param->has_stride_h() || conv_param->has_stride_w()) {
33      stride_h = conv_param->stride_h();
34      stride_w = conv_param->stride_w();
35    } else {
36      stride_h = stride_w = conv_param->stride_size() ? conv_param->stride(0) : 1;
37    }
38    int dilation_h, dilation_w;
39    dilation_h = dilation_w = conv_param->dilation_size() ?
40                              conv_param->dilation(0) : 1;
<span onclick='openModal()' class='match'>41    int kernel_d, pad_d, stride_d, dilation_d;
42    if (has_depth) {
43      kernel_d = kernel_h;
44      stride_d = stride_h;
45      pad_d = pad_h;
46      dilation_d = dilation_h;
47    } else {
</span>48      kernel_d = stride_d = dilation_d = 1;
49      pad_d = 0;
50    }
51    int groups = conv_param->group();
52    int o_g = out->shape(1) / groups;
53    int k_g = in->shape(1) / groups;
54    int o_head, k_head;
55    vector<int> weight_offset(4 + has_depth);
56    vector<int> in_offset(4 + has_depth);
57    vector<int> out_offset(4 + has_depth);
58    Dtype* out_data = out->mutable_cpu_data();
59    for (int n = 0; n < out->shape(0); n++) {
60      for (int g = 0; g < groups; g++) {
61        o_head = o_g * g;
62        k_head = k_g * g;
63        for (int o = 0; o < o_g; o++) {
64          for (int k = 0; k < k_g; k++) {
65            for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
66              for (int y = 0; y < out->shape(2 + has_depth); y++) {
67                for (int x = 0; x < out->shape(3 + has_depth); x++) {
68                  for (int r = 0; r < kernel_d; r++) {
69                    for (int p = 0; p < kernel_h; p++) {
70                      for (int q = 0; q < kernel_w; q++) {
71                        int in_z = z * stride_d - pad_d + r * dilation_d;
72                        int in_y = y * stride_h - pad_h + p * dilation_h;
73                        int in_x = x * stride_w - pad_w + q * dilation_w;
74                        if (in_z >= 0 && in_z < (has_depth ? in->shape(2) : 1)
75                            && in_y >= 0 && in_y < in->shape(2 + has_depth)
76                            && in_x >= 0 && in_x < in->shape(3 + has_depth)) {
77                          weight_offset[0] = o + o_head;
78                          weight_offset[1] = k;
79                          if (has_depth) { weight_offset[2] = r; }
80                          weight_offset[2 + has_depth] = p;
81                          weight_offset[3 + has_depth] = q;
82                          in_offset[0] = n;
83                          in_offset[1] = k + k_head;
84                          if (has_depth) { in_offset[2] = in_z; }
85                          in_offset[2 + has_depth] = in_y;
86                          in_offset[3 + has_depth] = in_x;
87                          out_offset[0] = n;
88                          out_offset[1] = o + o_head;
89                          if (has_depth) { out_offset[2] = z; }
90                          out_offset[2 + has_depth] = y;
91                          out_offset[3 + has_depth] = x;
92                          out_data[out->offset(out_offset)] +=
93                              in->data_at(in_offset)
94                              * weights[0]->data_at(weight_offset);
95                        }
96                      }
97                    }
98                  }
99                }
100              }
101            }
102          }
103        }
104      }
105    }
106    if (conv_param->bias_term()) {
107      const Dtype* bias_data = weights[1]->cpu_data();
108      for (int n = 0; n < out->shape(0); n++) {
109        for (int o = 0; o < out->shape(1); o++) {
110          for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
111            for (int y = 0; y < out->shape(2 + has_depth); y++) {
112              for (int x = 0; x < out->shape(3 + has_depth); x++) {
113                out_offset[0] = n;
114                out_offset[1] = o;
115                if (has_depth) { out_offset[2] = z; }
116                out_offset[2 + has_depth] = y;
117                out_offset[3 + has_depth] = x;
118                out_data[out->offset(out_offset)] += bias_data[o];
119              }
120            }
121          }
122        }
123      }
124    }
125    if (conv_param->relu()) {
126      for (int n = 0; n < out->shape(0); n++) {
127        for (int o = 0; o < out->shape(1); o++) {
128          for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
129            for (int y = 0; y < out->shape(2 + has_depth); y++) {
130              for (int x = 0; x < out->shape(3 + has_depth); x++) {
131                out_offset[0] = n;
132                out_offset[1] = o;
133                if (has_depth) { out_offset[2] = z; }
134                out_offset[2 + has_depth] = y;
135                out_offset[3 + has_depth] = x;
136                if(out_data[out->offset(out_offset)] < 0) out_data[out->offset(out_offset)] = 0;
137              }
138            }
139          }
140        }
141      }
142    }
143  }
144  template void caffe_conv(const Blob<float>* in,
145      ConvolutionParameter* conv_param,
146      const vector<shared_ptr<Blob<float> > >& weights,
147      Blob<float>* out);
148  template void caffe_conv(const Blob<double>* in,
149      ConvolutionParameter* conv_param,
150      const vector<shared_ptr<Blob<double> > >& weights,
151      Blob<double>* out);
152  template <typename TypeParam>
153  class MKLConvolutionLayerTest : public MultiDeviceTest<TypeParam> {
154    typedef typename TypeParam::Dtype Dtype;
155   protected:
156    MKLConvolutionLayerTest()
157        : blob_bottom_(new Blob<Dtype>(2, 3, 6, 4)),
158          blob_bottom_2_(new Blob<Dtype>(2, 3, 6, 4)),
159          blob_top_(new Blob<Dtype>()),
160          blob_top_2_(new Blob<Dtype>()) {}
161    virtual void SetUp() {
162      FillerParameter filler_param;
163      filler_param.set_value(1.);
164      GaussianFiller<Dtype> filler(filler_param);
165      filler.Fill(this->blob_bottom_);
166      filler.Fill(this->blob_bottom_2_);
167      blob_bottom_vec_.push_back(blob_bottom_);
168      blob_top_vec_.push_back(blob_top_);
169    }
170    virtual ~MKLConvolutionLayerTest() {
171      delete blob_bottom_;
172      delete blob_bottom_2_;
173      delete blob_top_;
174      delete blob_top_2_;
175    }
176    virtual Blob<Dtype>* MakeReferenceTop(Blob<Dtype>* top) {
177      this->ref_blob_top_.reset(new Blob<Dtype>());
178      this->ref_blob_top_->ReshapeLike(*top);
179      return this->ref_blob_top_.get();
180    }
181    Blob<Dtype>* const blob_bottom_;
182    Blob<Dtype>* const blob_bottom_2_;
183    Blob<Dtype>* const blob_top_;
184    Blob<Dtype>* const blob_top_2_;
185    shared_ptr<Blob<Dtype> > ref_blob_top_;
186    vector<Blob<Dtype>*> blob_bottom_vec_;
187    vector<Blob<Dtype>*> blob_top_vec_;
188  };
189  typedef ::testing::Types<CPUDevice<float>,
190                           CPUDevice<double> > TestDtypesCPU;
191  TYPED_TEST_CASE(MKLConvolutionLayerTest, TestDtypesCPU);
192  TYPED_TEST(MKLConvolutionLayerTest, TestSetupMKL) {
193    typedef typename TypeParam::Dtype Dtype;
194    LayerParameter layer_param;
195    ConvolutionParameter* convolution_param =
196        layer_param.mutable_convolution_param();
197    convolution_param->add_kernel_size(3);
198    convolution_param->add_stride(2);
199    convolution_param->set_num_output(4);
200    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
201    this->blob_top_vec_.push_back(this->blob_top_2_);
202    shared_ptr<Layer<Dtype> > layer(
203        new MKLConvolutionLayer<Dtype>(layer_param));
204    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
205    EXPECT_EQ(this->blob_top_->num(), 2);
206    EXPECT_EQ(this->blob_top_->channels(), 4);
207    EXPECT_EQ(this->blob_top_->height(), 2);
208    EXPECT_EQ(this->blob_top_->width(), 1);
209    EXPECT_EQ(this->blob_top_2_->num(), 2);
210    EXPECT_EQ(this->blob_top_2_->channels(), 4);
211    EXPECT_EQ(this->blob_top_2_->height(), 2);
212    EXPECT_EQ(this->blob_top_2_->width(), 1);
213    convolution_param->set_num_output(3);
214    convolution_param->set_group(3);
215    layer.reset(new MKLConvolutionLayer<Dtype>(layer_param));
216    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
217    EXPECT_EQ(this->blob_top_->num(), 2);
218    EXPECT_EQ(this->blob_top_->channels(), 3);
219    EXPECT_EQ(this->blob_top_->height(), 2);
220    EXPECT_EQ(this->blob_top_->width(), 1);
221    EXPECT_EQ(this->blob_top_2_->num(), 2);
222    EXPECT_EQ(this->blob_top_2_->channels(), 3);
223    EXPECT_EQ(this->blob_top_2_->height(), 2);
224    EXPECT_EQ(this->blob_top_2_->width(), 1);
225  }
226  TYPED_TEST(MKLConvolutionLayerTest, TestSimpleConvolutionMKL) {
227    typedef typename TypeParam::Dtype Dtype;
228    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
229    this->blob_top_vec_.push_back(this->blob_top_2_);
230    LayerParameter layer_param;
231    ConvolutionParameter* convolution_param =
232        layer_param.mutable_convolution_param();
233    convolution_param->add_kernel_size(3);
234    convolution_param->add_stride(2);
235    convolution_param->set_num_output(4);
236    convolution_param->mutable_weight_filler()->set_type("gaussian");
237    convolution_param->mutable_bias_filler()->set_type("constant");
238    convolution_param->mutable_bias_filler()->set_value(0.1);
239    shared_ptr<Layer<Dtype> > layer(
240        new MKLConvolutionLayer<Dtype>(layer_param));
241    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
242    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
243    const Dtype* top_data;
244    const Dtype* ref_top_data;
245    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
246        this->MakeReferenceTop(this->blob_top_));
247    top_data = this->blob_top_->cpu_data();
248    ref_top_data = this->ref_blob_top_->cpu_data();
249    for (int i = 0; i < this->blob_top_->count(); ++i) {
250      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
251    }
252  #if 0   
253    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
254        this->MakeReferenceTop(this->blob_top_2_));
255    top_data = this->blob_top_2_->cpu_data();
256    ref_top_data = this->ref_blob_top_->cpu_data();
257    for (int i = 0; i < this->blob_top_->count(); ++i) {
258      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
259    }
260  #endif
261  }
262  #if 0
263  TYPED_TEST(MKLConvolutionLayerTest, TestDilatedConvolutionMKL) {
264    typedef typename TypeParam::Dtype Dtype;
265    vector<int> bottom_shape;
266    bottom_shape.push_back(2);
267    bottom_shape.push_back(3);
268    bottom_shape.push_back(8);
269    bottom_shape.push_back(7);
270    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
271    this->blob_top_vec_.push_back(this->blob_top_2_);
272    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
273      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
274    }
275    LayerParameter layer_param;
276    ConvolutionParameter* convolution_param =
277        layer_param.mutable_convolution_param();
278    convolution_param->add_kernel_size(3);
279    convolution_param->add_dilation(2);
280    convolution_param->set_num_output(4);
281    convolution_param->mutable_weight_filler()->set_type("gaussian");
282    convolution_param->mutable_bias_filler()->set_type("constant");
283    convolution_param->mutable_bias_filler()->set_value(0.1);
284    shared_ptr<Layer<Dtype> > layer(
285        new MKLConvolutionLayer<Dtype>(layer_param));
286    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
287    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
288    const Dtype* top_data;
289    const Dtype* ref_top_data;
290    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
291               this->MakeReferenceTop(this->blob_top_));
292    top_data = this->blob_top_->cpu_data();
293    ref_top_data = this->ref_blob_top_->cpu_data();
294    for (int i = 0; i < this->blob_top_->count(); ++i) {
295      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
296    }
297  #if 0   
298    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
299               this->MakeReferenceTop(this->blob_top_2_));
300    top_data = this->blob_top_2_->cpu_data();
301    ref_top_data = this->ref_blob_top_->cpu_data();
302    for (int i = 0; i < this->blob_top_->count(); ++i) {
303      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
304    }
305  #endif
306  }
307  #endif
308  #if 0
309  TYPED_TEST(MKLConvolutionLayerTest, Test0DConvolutionMKL) {
310    typedef typename TypeParam::Dtype Dtype;
311    LayerParameter layer_param;
312    ConvolutionParameter* convolution_param =
313        layer_param.mutable_convolution_param();
314    const int kNumOutput = 3;
315    convolution_param->set_num_output(kNumOutput);
316    convolution_param->set_axis(3);
317    convolution_param->mutable_weight_filler()->set_type("gaussian");
318    convolution_param->mutable_bias_filler()->set_type("gaussian");
319    shared_ptr<Layer<Dtype> > layer(
320        new MKLConvolutionLayer<Dtype>(layer_param));
321    vector<int> top_shape = this->blob_bottom_->shape();
322    top_shape[3] = kNumOutput;
323    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
324    EXPECT_EQ(top_shape, this->blob_top_->shape());
325    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
326    vector<int> weight_offset(2);
327    const Blob<Dtype>* weight = layer->blobs()[0].get();
328    const Blob<Dtype>* bias = layer->blobs()[1].get();
329    const int num = this->blob_top_->count(3);
330    const int dim = this->blob_top_->shape(3);
331    const int bottom_dim = this->blob_bottom_->shape(3);
332    for (int n = 0; n < num; ++n) {
333      for (int d = 0; d < dim; ++d) {
334        weight_offset[0] = d;
335        Dtype value = bias->cpu_data()[d];
336        for (int bottom_d = 0; bottom_d < bottom_dim; ++bottom_d) {
337          weight_offset[1] = bottom_d;
338          value += weight->data_at(weight_offset) *
339                   this->blob_bottom_->cpu_data()[n * bottom_dim + bottom_d];
340        }
341        EXPECT_NEAR(value, this->blob_top_->cpu_data()[n * dim + d], 1e-4);
342      }
343    }
344  }
345  #endif
346  #if 0
347  TYPED_TEST(MKLConvolutionLayerTest, TestSimple3DConvolution) {
348    typedef typename TypeParam::Dtype Dtype;
349    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
350    this->blob_top_vec_.push_back(this->blob_top_2_);
351    vector<int> bottom_shape(5);
352    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
353    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
354    bottom_shape[2] = 5;
355    bottom_shape[3] = this->blob_bottom_vec_[0]->shape(2);
356    bottom_shape[4] = this->blob_bottom_vec_[0]->shape(3);
357    FillerParameter filler_param;
358    GaussianFiller<Dtype> filler(filler_param);
359    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
360      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
361      filler.Fill(this->blob_bottom_vec_[i]);
362    }
363    LayerParameter layer_param;
364    ConvolutionParameter* convolution_param =
365        layer_param.mutable_convolution_param();
366    convolution_param->add_kernel_size(3);
367    convolution_param->add_stride(2);
368    convolution_param->set_num_output(4);
369    convolution_param->mutable_weight_filler()->set_type("gaussian");
370    convolution_param->mutable_bias_filler()->set_type("gaussian");
371    shared_ptr<Layer<Dtype> > layer(
372        new MKLConvolutionLayer<Dtype>(layer_param));
373    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
374    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
375    const Dtype* top_data;
376    const Dtype* ref_top_data;
377    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
378        this->MakeReferenceTop(this->blob_top_));
379    top_data = this->blob_top_->cpu_data();
380    ref_top_data = this->ref_blob_top_->cpu_data();
381    for (int i = 0; i < this->blob_top_->count(); ++i) {
382      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
383    }
384  #if 0   
385    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
386        this->MakeReferenceTop(this->blob_top_2_));
387    top_data = this->blob_top_2_->cpu_data();
388    ref_top_data = this->ref_blob_top_->cpu_data();
389    for (int i = 0; i < this->blob_top_->count(); ++i) {
390      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
391    }
392  #endif
393  }
394  #endif
395  #if 0
396  TYPED_TEST(MKLConvolutionLayerTest, TestDilated3DConvolution) {
397    typedef typename TypeParam::Dtype Dtype;
398    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
399    this->blob_top_vec_.push_back(this->blob_top_2_);
400    vector<int> bottom_shape(5);
401    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
402    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
403    bottom_shape[2] = 6;
404    bottom_shape[3] = 7;
405    bottom_shape[4] = 8;
406    FillerParameter filler_param;
407    GaussianFiller<Dtype> filler(filler_param);
408    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
409      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
410      filler.Fill(this->blob_bottom_vec_[i]);
411    }
412    LayerParameter layer_param;
413    ConvolutionParameter* convolution_param =
414        layer_param.mutable_convolution_param();
415    convolution_param->add_kernel_size(3);
416    convolution_param->add_dilation(2);
417    convolution_param->set_num_output(4);
418    convolution_param->mutable_weight_filler()->set_type("gaussian");
419    convolution_param->mutable_bias_filler()->set_type("gaussian");
420    shared_ptr<Layer<Dtype> > layer(
421        new MKLConvolutionLayer<Dtype>(layer_param));
422    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
423    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
424    const Dtype* top_data;
425    const Dtype* ref_top_data;
426    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
427               this->MakeReferenceTop(this->blob_top_));
428    top_data = this->blob_top_->cpu_data();
429    ref_top_data = this->ref_blob_top_->cpu_data();
430    for (int i = 0; i < this->blob_top_->count(); ++i) {
431      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
432    }
433    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
434               this->MakeReferenceTop(this->blob_top_2_));
435    top_data = this->blob_top_2_->cpu_data();
436    ref_top_data = this->ref_blob_top_->cpu_data();
437    for (int i = 0; i < this->blob_top_->count(); ++i) {
438      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
439    }
440  }
441  #endif
442  TYPED_TEST(MKLConvolutionLayerTest, Test1x1Convolution) {
443    typedef typename TypeParam::Dtype Dtype;
444    LayerParameter layer_param;
445    ConvolutionParameter* convolution_param =
446        layer_param.mutable_convolution_param();
447    convolution_param->add_kernel_size(1);
448    convolution_param->add_stride(1);
449    convolution_param->set_num_output(4);
450    convolution_param->mutable_weight_filler()->set_type("gaussian");
451    convolution_param->mutable_bias_filler()->set_type("constant");
452    convolution_param->mutable_bias_filler()->set_value(0.1);
453    shared_ptr<Layer<Dtype> > layer(
454        new MKLConvolutionLayer<Dtype>(layer_param));
455    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
456    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
457    const Dtype* top_data;
458    const Dtype* ref_top_data;
459    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
460        this->MakeReferenceTop(this->blob_top_));
461    top_data = this->blob_top_->cpu_data();
462    ref_top_data = this->ref_blob_top_->cpu_data();
463    for (int i = 0; i < this->blob_top_->count(); ++i) {
464      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
465    }
466  }
467  TYPED_TEST(MKLConvolutionLayerTest, TestSimpleConvolutionGroup) {
468    typedef typename TypeParam::Dtype Dtype;
469    LayerParameter layer_param;
470    ConvolutionParameter* convolution_param =
471        layer_param.mutable_convolution_param();
472    convolution_param->add_kernel_size(3);
473    convolution_param->add_stride(2);
474    convolution_param->set_num_output(3);
475    convolution_param->set_group(3);
476    convolution_param->mutable_weight_filler()->set_type("gaussian");
477    convolution_param->mutable_bias_filler()->set_type("constant");
478    convolution_param->mutable_bias_filler()->set_value(0.1);
479    shared_ptr<Layer<Dtype> > layer(
480        new MKLConvolutionLayer<Dtype>(layer_param));
481    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
482    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
483    const Dtype* top_data;
484    const Dtype* ref_top_data;
485    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
486        this->MakeReferenceTop(this->blob_top_));
487    top_data = this->blob_top_->cpu_data();
488    ref_top_data = this->ref_blob_top_->cpu_data();
489    for (int i = 0; i < this->blob_top_->count(); ++i) {
490      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
491    }
492  }
493  #if 0
494  TYPED_TEST(MKLConvolutionLayerTest, TestSobelConvolution) {
495    typedef typename TypeParam::Dtype Dtype;
496    shared_ptr<GaussianFiller<Dtype> > filler;
497    FillerParameter filler_param;
498    filler_param.set_value(1.);
499    filler.reset(new GaussianFiller<Dtype>(filler_param));
500    filler->Fill(this->blob_bottom_);
501    this->blob_bottom_2_->CopyFrom(*this->blob_bottom_);
502    LayerParameter layer_param;
503    ConvolutionParameter* convolution_param =
504        layer_param.mutable_convolution_param();
505    convolution_param->add_kernel_size(3);
506    convolution_param->add_stride(2);
507    convolution_param->set_num_output(1);
508    convolution_param->set_bias_term(false);
509    shared_ptr<Layer<Dtype> > layer(
510        new MKLConvolutionLayer<Dtype>(layer_param));
511    layer->blobs().resize(1);
512    layer->blobs()[0].reset(new Blob<Dtype>(1, 3, 3, 3));
513    Dtype* weights = layer->blobs()[0]->mutable_cpu_data();
514    for (int c = 0; c < 3; ++c) {
515      int i = c * 9;  
516      weights[i +  0] = -1;
517      weights[i +  1] =  0;
518      weights[i +  2] =  1;
519      weights[i +  3] = -2;
520      weights[i +  4] =  0;
521      weights[i +  5] =  2;
522      weights[i +  6] = -1;
523      weights[i +  7] =  0;
524      weights[i +  8] =  1;
525    }
526    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
527    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
528    vector<Blob<Dtype>*> sep_blob_bottom_vec;
529    vector<Blob<Dtype>*> sep_blob_top_vec;
530    shared_ptr<Blob<Dtype> > blob_sep(new Blob<Dtype>());
531    sep_blob_bottom_vec.push_back(this->blob_bottom_2_);
532    sep_blob_top_vec.push_back(this->blob_top_2_);
533    convolution_param->clear_kernel_size();
534    convolution_param->clear_stride();
535    convolution_param->set_kernel_h(3);
536    convolution_param->set_kernel_w(1);
537    convolution_param->set_stride_h(2);
538    convolution_param->set_stride_w(1);
539    convolution_param->set_num_output(1);
540    convolution_param->set_bias_term(false);
541    layer.reset(new MKLConvolutionLayer<Dtype>(layer_param));
542    layer->blobs().resize(1);
543    layer->blobs()[0].reset(new Blob<Dtype>(1, 3, 3, 1));
544    Dtype* weights_1 = layer->blobs()[0]->mutable_cpu_data();
545    for (int c = 0; c < 3; ++c) {
546      int i = c * 3;  
547      weights_1[i +  0] = 1;
548      weights_1[i +  1] = 2;
549      weights_1[i +  2] = 1;
550    }
551    layer->SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
552    layer->Forward(sep_blob_bottom_vec, sep_blob_top_vec);
553    blob_sep->CopyFrom(*this->blob_top_2_, false, true);
554    sep_blob_bottom_vec.clear();
555    sep_blob_bottom_vec.push_back(blob_sep.get());
556    convolution_param->set_kernel_h(1);
557    convolution_param->set_kernel_w(3);
558    convolution_param->set_stride_h(1);
559    convolution_param->set_stride_w(2);
560    convolution_param->set_num_output(1);
561    convolution_param->set_bias_term(false);
562    layer.reset(new MKLConvolutionLayer<Dtype>(layer_param));
563    layer->blobs().resize(1);
564    layer->blobs()[0].reset(new Blob<Dtype>(1, 1, 1, 3));
565    Dtype* weights_2 = layer->blobs()[0]->mutable_cpu_data();
566    weights_2[0] = -1;
567    weights_2[1] =  0;
568    weights_2[2] =  1;
569    layer->SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
570    layer->Forward(sep_blob_bottom_vec, sep_blob_top_vec);
571    const Dtype* top_data = this->blob_top_->cpu_data();
572    const Dtype* sep_top_data = this->blob_top_2_->cpu_data();
573    for (int i = 0; i < this->blob_top_->count(); ++i) {
574      EXPECT_NEAR(top_data[i], sep_top_data[i], 1e-4);
575    }
576  }
577  #endif
578  #if 0
579  TYPED_TEST(MKLConvolutionLayerTest, TestNDAgainst2D) {
580    typedef typename TypeParam::Dtype Dtype;
581    const int kernel_h = 11;
582    const int kernel_w = 13;
583    vector<int> bottom_shape(4);
584    bottom_shape[0] = 15;
585    bottom_shape[1] = 18;
586    bottom_shape[2] = kernel_h * 2;
587    bottom_shape[3] = kernel_w * 2;
588    FillerParameter filler_param;
589    GaussianFiller<Dtype> filler(filler_param);
590    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
591      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
592      filler.Fill(this->blob_bottom_vec_[i]);
593    }
594    LayerParameter layer_param;
595    ConvolutionParameter* convolution_param =
596        layer_param.mutable_convolution_param();
597    convolution_param->set_num_output(12);
598    convolution_param->set_bias_term(false);
599    convolution_param->set_group(6);
600    convolution_param->set_kernel_h(kernel_h);
601    convolution_param->set_kernel_w(kernel_w);
602    convolution_param->mutable_weight_filler()->set_type("gaussian");
603    Blob<Dtype> weights;
604    Blob<Dtype> top_diff;
605    bool copy_diff;
606    bool reshape;
607    {
608      MKLConvolutionLayer<Dtype> layer(layer_param);
609      layer.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
610      top_diff.ReshapeLike(*this->blob_top_);
611      filler.Fill(&top_diff);
612      ASSERT_EQ(1, layer.blobs().size());
613      copy_diff = false; reshape = true;
614      weights.CopyFrom(*layer.blobs()[0], copy_diff, reshape);
615    }
616    vector<bool> propagate_down(1, true);
617    Blob<Dtype> result_2d;
618    Blob<Dtype> backward_result_2d;
619    Blob<Dtype> backward_weight_result_2d;
620    {
621      caffe_set(this->blob_top_->count(), Dtype(0),
622                this->blob_top_->mutable_cpu_data());
623      caffe_set(this->blob_bottom_->count(), Dtype(0),
624                this->blob_bottom_->mutable_cpu_diff());
625      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
626      convolution_param->set_force_nd_im2col(false);
627      MKLConvolutionLayer<Dtype> layer_2d(layer_param);
628      layer_2d.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
629      ASSERT_EQ(1, layer_2d.blobs().size());
630      copy_diff = false; reshape = false;
631      layer_2d.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
632      layer_2d.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
633      copy_diff = false; reshape = true;
634      result_2d.CopyFrom(*this->blob_top_, copy_diff, reshape);
635      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
636      caffe_copy(top_diff.count(), top_diff.cpu_data(),
637                 this->blob_top_->mutable_cpu_diff());
638      layer_2d.Backward(this->blob_top_vec_, propagate_down,
639                        this->blob_bottom_vec_);
640      copy_diff = true; reshape = true;
641      backward_result_2d.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
642      backward_weight_result_2d.CopyFrom(weights, copy_diff, reshape);
643    }
644    Blob<Dtype> result_nd;
645    Blob<Dtype> backward_result_nd;
646    Blob<Dtype> backward_weight_result_nd;
647    {
648      caffe_set(this->blob_top_->count(), Dtype(0),
649                this->blob_top_->mutable_cpu_data());
650      caffe_set(this->blob_bottom_->count(), Dtype(0),
651                this->blob_bottom_->mutable_cpu_diff());
652      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
653      convolution_param->set_force_nd_im2col(true);
654      MKLConvolutionLayer<Dtype> layer_nd(layer_param);
655      layer_nd.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
656      ASSERT_EQ(1, layer_nd.blobs().size());
657      copy_diff = false; reshape = false;
658      layer_nd.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
659      layer_nd.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
660      copy_diff = false; reshape = true;
661      result_nd.CopyFrom(*this->blob_top_, copy_diff, reshape);
662      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
663      caffe_copy(top_diff.count(), top_diff.cpu_data(),
664                 this->blob_top_->mutable_cpu_diff());
665      layer_nd.Backward(this->blob_top_vec_, propagate_down,
666                        this->blob_bottom_vec_);
667      copy_diff = true; reshape = true;
668      backward_result_nd.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
669      backward_weight_result_nd.CopyFrom(weights, copy_diff, reshape);
670    }
671    ASSERT_EQ(result_nd.count(), result_2d.count());
672    for (int i = 0; i < result_2d.count(); ++i)  {
673      EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
674    }
675    ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
676    for (int i = 0; i < backward_result_2d.count(); ++i) {
677      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
678                backward_result_nd.cpu_diff()[i]);
679    }
680    ASSERT_EQ(backward_weight_result_nd.count(),
681              backward_weight_result_2d.count());
682    for (int i = 0; i < backward_weight_result_2d.count(); ++i) {
683      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
684                backward_weight_result_nd.cpu_diff()[i]);
685    }
686  }
687  #endif
688  TYPED_TEST(MKLConvolutionLayerTest, TestGradient) {
689    typedef typename TypeParam::Dtype Dtype;
690    LayerParameter layer_param;
691    ConvolutionParameter* convolution_param =
692        layer_param.mutable_convolution_param();
693    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
694    this->blob_top_vec_.push_back(this->blob_top_2_);
695    convolution_param->add_kernel_size(3);
696    convolution_param->add_stride(2);
697    convolution_param->set_num_output(2);
698    convolution_param->mutable_weight_filler()->set_type("gaussian");
699    convolution_param->mutable_bias_filler()->set_type("gaussian");
700    MKLConvolutionLayer<Dtype> layer(layer_param);
701    GradientChecker<Dtype> checker(1e-2, 1e-3);
702    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
703        this->blob_top_vec_);
704  }
705  #if 0
706  TYPED_TEST(MKLConvolutionLayerTest, TestDilatedGradient) {
707    typedef typename TypeParam::Dtype Dtype;
708    LayerParameter layer_param;
709    ConvolutionParameter* convolution_param =
710        layer_param.mutable_convolution_param();
711    vector<int> bottom_shape;
712    bottom_shape.push_back(2);
713    bottom_shape.push_back(3);
714    bottom_shape.push_back(5);
715    bottom_shape.push_back(6);
716    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
717      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
718    }
719    convolution_param->add_kernel_size(3);
720    convolution_param->add_dilation(2);
721    convolution_param->set_num_output(2);
722    convolution_param->mutable_weight_filler()->set_type("gaussian");
723    convolution_param->mutable_bias_filler()->set_type("gaussian");
724    MKLConvolutionLayer<Dtype> layer(layer_param);
725    GradientChecker<Dtype> checker(1e-2, 1e-3);
726    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
727                                    this->blob_top_vec_);
728  }
729  #endif
730  #if 0
731  TYPED_TEST(MKLConvolutionLayerTest, TestGradient3D) {
732    typedef typename TypeParam::Dtype Dtype;
733    LayerParameter layer_param;
734    ConvolutionParameter* convolution_param =
735        layer_param.mutable_convolution_param();
736    vector<int> bottom_shape(5);
737    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
738    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
739    bottom_shape[2] = 5;
740    bottom_shape[3] = this->blob_bottom_vec_[0]->shape(2);
741    bottom_shape[4] = this->blob_bottom_vec_[0]->shape(3);
742    FillerParameter filler_param;
743    GaussianFiller<Dtype> filler(filler_param);
744    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
745      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
746      filler.Fill(this->blob_bottom_vec_[i]);
747    }
748    convolution_param->add_kernel_size(3);
749    convolution_param->add_stride(2);
750    convolution_param->set_num_output(2);
751    convolution_param->mutable_weight_filler()->set_type("gaussian");
752    convolution_param->mutable_bias_filler()->set_type("gaussian");
753    MKLConvolutionLayer<Dtype> layer(layer_param);
754    GradientChecker<Dtype> checker(1e-2, 1e-3);
755    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
756        this->blob_top_vec_);
757  }
758  #endif
759  TYPED_TEST(MKLConvolutionLayerTest, Test1x1Gradient) {
760    typedef typename TypeParam::Dtype Dtype;
761    LayerParameter layer_param;
762    ConvolutionParameter* convolution_param =
763        layer_param.mutable_convolution_param();
764    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
765    this->blob_top_vec_.push_back(this->blob_top_2_);
766    convolution_param->add_kernel_size(1);
767    convolution_param->add_stride(1);
768    convolution_param->set_num_output(2);
769    convolution_param->mutable_weight_filler()->set_type("gaussian");
770    convolution_param->mutable_bias_filler()->set_type("gaussian");
771    MKLConvolutionLayer<Dtype> layer(layer_param);
772    GradientChecker<Dtype> checker(1e-2, 1e-3);
773    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
774        this->blob_top_vec_);
775  }
776  TYPED_TEST(MKLConvolutionLayerTest, TestGradientGroup) {
777    typedef typename TypeParam::Dtype Dtype;
778    LayerParameter layer_param;
779    ConvolutionParameter* convolution_param =
780        layer_param.mutable_convolution_param();
781    convolution_param->add_kernel_size(3);
782    convolution_param->add_stride(2);
783    convolution_param->set_num_output(3);
784    convolution_param->set_group(3);
785    convolution_param->mutable_weight_filler()->set_type("gaussian");
786    convolution_param->mutable_bias_filler()->set_type("gaussian");
787    MKLConvolutionLayer<Dtype> layer(layer_param);
788    GradientChecker<Dtype> checker(1e-2, 1e-3);
789    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
790        this->blob_top_vec_);
791  }
792  }  
793  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-blob.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkl_convolution_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>557      vector<int> shape;
558      if (proto.has_num() || proto.has_channels() ||
559          proto.has_height() || proto.has_width()) {
560        shape.resize(4);
561        shape[0] = proto.num();
562        shape[1] = proto.channels();
563        shape[2] = proto.height();
564        shape[3] = proto.width();
565      } else {
</pre></code></div>
                <div class="column column_space"><pre><code>41    int kernel_d, pad_d, stride_d, dilation_d;
42    if (has_depth) {
43      kernel_d = kernel_h;
44      stride_d = stride_h;
45      pad_d = pad_h;
46      dilation_d = dilation_h;
47    } else {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    