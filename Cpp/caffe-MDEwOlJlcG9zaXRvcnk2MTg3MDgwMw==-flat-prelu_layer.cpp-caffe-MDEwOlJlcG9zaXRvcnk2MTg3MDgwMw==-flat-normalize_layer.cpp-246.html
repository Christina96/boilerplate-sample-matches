
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 100, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-prelu_layer.cpp</h3>
            <pre><code>1  #include &lt;algorithm&gt;
2  #include &lt;vector&gt;
3  #include &quot;caffe/filler.hpp&quot;
4  #include &quot;caffe/layers/neuron_layer.hpp&quot;
5  #include &quot;caffe/layers/prelu_layer.hpp&quot;
6  namespace caffe {
7  template &lt;typename Dtype&gt;
8  void PReLULayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
9      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
10    CHECK_GE(bottom[0]-&gt;num_axes(), 2)
11        &lt;&lt; &quot;Number of axes of bottom blob must be &gt;=2.&quot;;
12    PReLUParameter prelu_param = this-&gt;layer_param().prelu_param();
13    int channels = bottom[0]-&gt;shape(1);
14    channel_shared_ = prelu_param.channel_shared();
15    if (this-&gt;blobs_.size() &gt; 0) {
16      LOG(INFO) &lt;&lt; &quot;Skipping parameter initialization&quot;;
17    } else {
18      this-&gt;blobs_.resize(1);
19      if (channel_shared_) {
20        this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(vector&lt;int&gt;(0)));
21      } else {
22        this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(vector&lt;int&gt;(1, channels)));
23      }
24      shared_ptr&lt;Filler&lt;Dtype&gt; &gt; filler;
25      if (prelu_param.has_filler()) {
26        filler.reset(GetFiller&lt;Dtype&gt;(prelu_param.filler()));
27      } else {
28        FillerParameter filler_param;
29        filler_param.set_type(&quot;constant&quot;);
<span onclick='openModal()' class='match'>30        filler_param.set_value(0.25);
31        filler.reset(GetFiller&lt;Dtype&gt;(filler_param));
32      }
33      filler-&gt;Fill(this-&gt;blobs_[0].get());
34    }
35    if (channel_shared_) {
36      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), 1)
37          &lt;&lt; &quot;Negative slope size is inconsistent with prototxt config&quot;;
38    } else {
39      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), channels)
40          &lt;&lt; &quot;Negative slope size is inconsistent with prototxt config&quot;;
41    }
42    this-&gt;param_propagate_down_.resize(this-&gt;blobs_.size(), true);
43    multiplier_.Reshape(vector&lt;int&gt;(1, bottom[0]-&gt;count(1)));
</span>44    backward_buff_.Reshape(vector&lt;int&gt;(1, bottom[0]-&gt;count(1)));
45    caffe_set(multiplier_.count(), Dtype(1), multiplier_.mutable_cpu_data());
46  }
47  template &lt;typename Dtype&gt;
48  void PReLULayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
49      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
50    CHECK_GE(bottom[0]-&gt;num_axes(), 2)
51        &lt;&lt; &quot;Number of axes of bottom blob must be &gt;=2.&quot;;
52    top[0]-&gt;ReshapeLike(*bottom[0]);
53    if (bottom[0] == top[0]) {
54      bottom_memory_.ReshapeLike(*bottom[0]);
55    }
56  }
57  template &lt;typename Dtype&gt;
58  void PReLULayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
59      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
60    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
61    Dtype* top_data = top[0]-&gt;mutable_cpu_data();
62    const long count = bottom[0]-&gt;count();
63    const int dim = bottom[0]-&gt;count(2);
64    const int channels = bottom[0]-&gt;shape(1);
65    const Dtype* slope_data = this-&gt;blobs_[0]-&gt;cpu_data();
66    if (bottom[0] == top[0]) {
67      caffe_copy(count, bottom_data, bottom_memory_.mutable_cpu_data());
68    }
69    const int div_factor = channel_shared_ ? channels : 1;
70  #ifdef _OPENMP
71  #pragma omp parallel for
72  #endif
73    for (long i = 0; i &lt; count; ++i) {
74      long c = (i / dim) % channels / div_factor;
75      top_data[i] = std::max(bottom_data[i], Dtype(0))
76          + slope_data[c] * std::min(bottom_data[i], Dtype(0));
77    }
78  }
79  template &lt;typename Dtype&gt;
80  void PReLULayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
81      const vector&lt;bool&gt;&amp; propagate_down,
82      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
83    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
84    const Dtype* slope_data = this-&gt;blobs_[0]-&gt;cpu_data();
85    const Dtype* top_diff = top[0]-&gt;cpu_diff();
86    const long count = bottom[0]-&gt;count();
87    const long dim = bottom[0]-&gt;count(2);
88    const int channels = bottom[0]-&gt;shape(1);
89    if (top[0] == bottom[0]) {
90      bottom_data = bottom_memory_.cpu_data();
91    }
92    const int div_factor = channel_shared_ ? channels : 1;
93    if (this-&gt;param_propagate_down_[0]) {
94      Dtype* slope_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff();
95      long i;
96  #if !defined(__INTEL_COMPILER) || __INTEL_COMPILER &gt;= 1901
97  #if defined(_OPENMP) &amp;&amp; _OPENMP &gt;= 201511
98      long reduce = channel_shared_ ? 1 : channels;
99  #pragma omp parallel for private(i) reduction(+: slope_diff[:reduce])
100  #endif
101  #endif
102      for (i = 0; i &lt; count; ++i) {
103        long c = (i / dim) % channels / div_factor;
104        slope_diff[c] += top_diff[i] * bottom_data[i] * (bottom_data[i] &lt;= 0);
105      }
106    }
107    if (propagate_down[0]) {
108      Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
109  #ifdef _OPENMP
110  #pragma omp parallel for
111  #endif
112      for (long i = 0; i &lt; count; ++i) {
113        long c = (i / dim) % channels / div_factor;
114        bottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; 0)
115            + slope_data[c] * (bottom_data[i] &lt;= 0));
116      }
117    }
118  }
119  #ifdef CPU_ONLY
120  STUB_GPU(PReLULayer);
121  #endif
122  INSTANTIATE_CLASS(PReLULayer);
123  REGISTER_LAYER_CLASS(PReLU);
124  }  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-normalize_layer.cpp</h3>
            <pre><code>1  #include &lt;vector&gt;
2  #include &quot;caffe/filler.hpp&quot;
3  #include &quot;caffe/layers/normalize_layer.hpp&quot;
4  namespace caffe {
5  template &lt;typename Dtype&gt;
6  void NormalizeLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
7        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
8    CHECK_GE(bottom[0]-&gt;num_axes(), 2)
9        &lt;&lt; &quot;Number of axes of bottom blob must be &gt;=2.&quot;;
10    buffer_.Reshape(1, bottom[0]-&gt;channels(),
11                     bottom[0]-&gt;height(), bottom[0]-&gt;width());
12    buffer_channel_.Reshape(1, bottom[0]-&gt;channels(), 1, 1);
13    buffer_spatial_.Reshape(1, 1, bottom[0]-&gt;height(), bottom[0]-&gt;width());
14    NormalizeParameter norm_param = this-&gt;layer_param().norm_param();
15    across_spatial_ = norm_param.across_spatial();
16    if (across_spatial_) {
17      norm_.Reshape(bottom[0]-&gt;num(), 1, 1, 1);
18    } else {
19      norm_.Reshape(bottom[0]-&gt;num(), 1, bottom[0]-&gt;height(), bottom[0]-&gt;width());
20    }
21    eps_ = norm_param.eps();
22    int channels = bottom[0]-&gt;channels();
23    int spatial_dim = bottom[0]-&gt;width() * bottom[0]-&gt;height();
24    sum_channel_multiplier_.Reshape(1, channels, 1, 1);
25    caffe_set(channels, Dtype(1), sum_channel_multiplier_.mutable_cpu_data());
26    sum_spatial_multiplier_.Reshape(
27        1, 1, bottom[0]-&gt;height(), bottom[0]-&gt;width());
28    caffe_set(spatial_dim, Dtype(1), sum_spatial_multiplier_.mutable_cpu_data());
29    channel_shared_ = norm_param.channel_shared();
30    if (this-&gt;blobs_.size() &gt; 0) {
31      LOG(INFO) &lt;&lt; &quot;Skipping parameter initialization&quot;;
32    } else {
33      this-&gt;blobs_.resize(1);
34      if (channel_shared_) {
35        this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(vector&lt;int&gt;(0)));
36      } else {
37        this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(vector&lt;int&gt;(1, channels)));
38      }
39      shared_ptr&lt;Filler&lt;Dtype&gt; &gt; scale_filler;
40      if (norm_param.has_scale_filler()) {
41        scale_filler.reset(GetFiller&lt;Dtype&gt;(norm_param.scale_filler()));
42      } else {
43        FillerParameter filler_param;
44        filler_param.set_type(&quot;constant&quot;);
<span onclick='openModal()' class='match'>45        filler_param.set_value(1.0);
46        scale_filler.reset(GetFiller&lt;Dtype&gt;(filler_param));
47      }
48      scale_filler-&gt;Fill(this-&gt;blobs_[0].get());
49    }
50    if (channel_shared_) {
51      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), 1)
52          &lt;&lt; &quot;Scale size is inconsistent with prototxt config&quot;;
53    } else {
54      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), channels)
55          &lt;&lt; &quot;Scale size is inconsistent with prototxt config&quot;;
56    }
57    this-&gt;param_propagate_down_.resize(this-&gt;blobs_.size(), true);
58  }
</span>59  template &lt;typename Dtype&gt;
60  void NormalizeLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
61        const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
62    CHECK_GE(bottom[0]-&gt;num_axes(), 2)
63        &lt;&lt; &quot;Number of axes of bottom blob must be &gt;=2.&quot;;
64    top[0]-&gt;ReshapeLike(*bottom[0]);
65    buffer_.Reshape(1, bottom[0]-&gt;channels(),
66                     bottom[0]-&gt;height(), bottom[0]-&gt;width());
67    if (!across_spatial_) {
68      norm_.Reshape(bottom[0]-&gt;num(), 1, bottom[0]-&gt;height(), bottom[0]-&gt;width());
69    }
70    int spatial_dim = bottom[0]-&gt;height() * bottom[0]-&gt;width();
71    if (spatial_dim != sum_spatial_multiplier_.count()) {
72      sum_spatial_multiplier_.Reshape(
73          1, 1, bottom[0]-&gt;height(), bottom[0]-&gt;width());
74      caffe_set(spatial_dim, Dtype(1),
75                sum_spatial_multiplier_.mutable_cpu_data());
76      buffer_spatial_.Reshape(1, 1, bottom[0]-&gt;height(), bottom[0]-&gt;width());
77    }
78  }
79  template &lt;typename Dtype&gt;
80  void NormalizeLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
81      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
82    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
83    Dtype* top_data = top[0]-&gt;mutable_cpu_data();
84    const Dtype* scale = this-&gt;blobs_[0]-&gt;cpu_data();
85    Dtype* buffer_data = buffer_.mutable_cpu_data();
86    Dtype* norm_data = norm_.mutable_cpu_data();
87    caffe_set&lt;Dtype&gt;(norm_.count(), Dtype(eps_), norm_data);
88    const Dtype* sum_channel_multiplier = sum_channel_multiplier_.cpu_data();
89    const Dtype* sum_spatial_multiplier = sum_spatial_multiplier_.cpu_data();
90    int num = bottom[0]-&gt;num();
91    int dim = bottom[0]-&gt;count() / num;
92    int spatial_dim = bottom[0]-&gt;height() * bottom[0]-&gt;width();
93    int channels = bottom[0]-&gt;channels();
94    for (int n = 0; n &lt; num; ++n) {
95      caffe_sqr&lt;Dtype&gt;(dim, bottom_data, buffer_data);
96      if (across_spatial_) {
97        norm_data[n] = pow(caffe_cpu_asum&lt;Dtype&gt;(dim, buffer_data)+eps_,
98                           Dtype(0.5));
99        caffe_cpu_scale&lt;Dtype&gt;(dim, Dtype(1.0 / norm_data[n]), bottom_data,
100                               top_data);
101      } else {
102        caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, channels, spatial_dim, Dtype(1),
103                              buffer_data, sum_channel_multiplier, Dtype(1),
104                              norm_data);
105        caffe_powx&lt;Dtype&gt;(spatial_dim, norm_data, Dtype(0.5), norm_data);
106        caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, spatial_dim,
107                              1, Dtype(1), sum_channel_multiplier, norm_data,
108                              Dtype(0), buffer_data);
109        caffe_div&lt;Dtype&gt;(dim, bottom_data, buffer_data, top_data);
110        norm_data += spatial_dim;
111      }
112      if (channel_shared_) {
113        caffe_scal&lt;Dtype&gt;(dim, scale[0], top_data);
114      } else {
115        caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, spatial_dim,
116                              1, Dtype(1), scale, sum_spatial_multiplier,
117                              Dtype(0),
118                              buffer_data);
119        caffe_mul&lt;Dtype&gt;(dim, top_data, buffer_data, top_data);
120      }
121      bottom_data += dim;
122      top_data += dim;
123    }
124  }
125  template &lt;typename Dtype&gt;
126  void NormalizeLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
127      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
128    const Dtype* top_diff = top[0]-&gt;cpu_diff();
129    const Dtype* top_data = top[0]-&gt;cpu_data();
130    const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
131    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
132    const Dtype* scale = this-&gt;blobs_[0]-&gt;cpu_data();
133    const Dtype* norm_data = norm_.cpu_data();
134    Dtype* buffer_data = buffer_.mutable_cpu_data();
135    Dtype* buffer_channel = buffer_channel_.mutable_cpu_data();
136    Dtype* buffer_spatial = buffer_spatial_.mutable_cpu_data();
137    const Dtype* sum_channel_multiplier = sum_channel_multiplier_.cpu_data();
138    const Dtype* sum_spatial_multiplier = sum_spatial_multiplier_.cpu_data();
139    int count = top[0]-&gt;count();
140    int num = top[0]-&gt;num();
141    int dim = count / num;
142    int spatial_dim = top[0]-&gt;height() * top[0]-&gt;width();
143    int channels = top[0]-&gt;channels();
144    if (this-&gt;param_propagate_down_[0]) {
145      Dtype* scale_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff();
146      if (channel_shared_) {
147        scale_diff[0] +=
148            caffe_cpu_dot&lt;Dtype&gt;(count, top_data, top_diff) / scale[0];
149      } else {
150        for (int n = 0; n &lt; num; ++n) {
151          caffe_mul&lt;Dtype&gt;(dim, top_data+n*dim, top_diff+n*dim, buffer_data);
152          caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels, spatial_dim, Dtype(1),
153                                buffer_data, sum_spatial_multiplier, Dtype(0),
154                                buffer_channel);
155          caffe_div&lt;Dtype&gt;(channels, buffer_channel, scale, buffer_channel);
156          caffe_add&lt;Dtype&gt;(channels, buffer_channel, scale_diff, scale_diff);
157        }
158      }
159    }
160    if (propagate_down[0]) {
161      for (int n = 0; n &lt; num; ++n) {
162        if (across_spatial_) {
163          Dtype a = caffe_cpu_dot&lt;Dtype&gt;(dim, bottom_data, top_diff);
164          caffe_cpu_scale&lt;Dtype&gt;(dim, a / norm_data[n] / norm_data[n],
165                                 bottom_data, bottom_diff);
166          caffe_sub&lt;Dtype&gt;(dim, top_diff, bottom_diff, bottom_diff);
167          caffe_scal&lt;Dtype&gt;(dim, Dtype(1.0 / norm_data[n]), bottom_diff);
168        } else {
169          caffe_mul&lt;Dtype&gt;(dim, bottom_data, top_diff, buffer_data);
170          caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, channels, spatial_dim, Dtype(1),
171                                buffer_data, sum_channel_multiplier, Dtype(0),
172                                buffer_spatial);
173          caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, spatial_dim,
174                                1, Dtype(1), sum_channel_multiplier,
175                                buffer_spatial, Dtype(0), buffer_data);
176          caffe_mul&lt;Dtype&gt;(dim, bottom_data, buffer_data, bottom_diff);
177          caffe_powx&lt;Dtype&gt;(spatial_dim, norm_data, Dtype(2), buffer_spatial);
178          caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, spatial_dim,
179                                1, Dtype(1), sum_channel_multiplier,
180                                buffer_spatial, Dtype(0), buffer_data);
181          caffe_div&lt;Dtype&gt;(dim, bottom_diff, buffer_data, bottom_diff);
182          caffe_sub&lt;Dtype&gt;(dim, top_diff, bottom_diff, bottom_diff);
183          caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, spatial_dim,
184                                1, Dtype(1), sum_channel_multiplier, norm_data,
185                                Dtype(0), buffer_data);
186          caffe_div&lt;Dtype&gt;(dim, bottom_diff, buffer_data, bottom_diff);
187          norm_data += spatial_dim;
188        }
189        if (channel_shared_) {
190          caffe_scal&lt;Dtype&gt;(dim, scale[0], bottom_diff);
191        } else {
192          caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels, spatial_dim,
193                                1, Dtype(1), scale, sum_spatial_multiplier,
194                                Dtype(0), buffer_data);
195          caffe_mul&lt;Dtype&gt;(dim, bottom_diff, buffer_data, bottom_diff);
196        }
197        bottom_data += dim;
198        top_diff += dim;
199        bottom_diff += dim;
200      }
201    }
202  }
203  #ifdef CPU_ONLY
204  STUB_GPU(NormalizeLayer);
205  #endif
206  INSTANTIATE_CLASS(NormalizeLayer);
207  REGISTER_LAYER_CLASS(Normalize);
208  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-prelu_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-normalize_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>30        filler_param.set_value(0.25);
31        filler.reset(GetFiller&lt;Dtype&gt;(filler_param));
32      }
33      filler-&gt;Fill(this-&gt;blobs_[0].get());
34    }
35    if (channel_shared_) {
36      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), 1)
37          &lt;&lt; &quot;Negative slope size is inconsistent with prototxt config&quot;;
38    } else {
39      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), channels)
40          &lt;&lt; &quot;Negative slope size is inconsistent with prototxt config&quot;;
41    }
42    this-&gt;param_propagate_down_.resize(this-&gt;blobs_.size(), true);
43    multiplier_.Reshape(vector&lt;int&gt;(1, bottom[0]-&gt;count(1)));
</pre></code></div>
                <div class="column column_space"><pre><code>45        filler_param.set_value(1.0);
46        scale_filler.reset(GetFiller&lt;Dtype&gt;(filler_param));
47      }
48      scale_filler-&gt;Fill(this-&gt;blobs_[0].get());
49    }
50    if (channel_shared_) {
51      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), 1)
52          &lt;&lt; &quot;Scale size is inconsistent with prototxt config&quot;;
53    } else {
54      CHECK_EQ(this-&gt;blobs_[0]-&gt;count(), channels)
55          &lt;&lt; &quot;Scale size is inconsistent with prototxt config&quot;;
56    }
57    this-&gt;param_propagate_down_.resize(this-&gt;blobs_.size(), true);
58  }
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    