
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 16, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-net.cpp</h3>
            <pre><code>1  #include <algorithm>
2  #include <map>
3  #include <set>
4  #include <string>
5  #include <utility>
6  #include <vector>
7  #include <numeric>
8  #include "hdf5.h"
9  #include "boost/algorithm/string.hpp"
10  #include "caffe/common.hpp"
11  #include "caffe/layer.hpp"
12  #include "caffe/net.hpp"
13  #include "caffe/parallel.hpp"
14  #include "caffe/proto/caffe.pb.h"
15  #include "caffe/util/cpu_info.hpp"
16  #include "caffe/util/hdf5.hpp"
17  #include "caffe/util/insert_splits.hpp"
18  #include "caffe/util/math_functions.hpp"
19  #include "caffe/util/performance.hpp"
20  #include "caffe/util/upgrade_proto.hpp"
21  #include "caffe/multinode/mlsl.hpp"
22  #include "caffe/multinode/apply_mn_param.hpp"
23  #include "caffe/util/remove_batch_norm.hpp"
24  #include "caffe/util/apply_bn_stats_batch_size.hpp"
25  #include "caffe/syncedmem.hpp"
26  PERFORMANCE_CREATE_MONITOR();
27  namespace caffe {
28  #ifdef CAFFE_PER_LAYER_TIMINGS
29  #define LAYER_TIMING_START(name, index) do { \
30    if (this->phase() == TRAIN) { \
31      this->name##_start_time_per_layer[index] = this->timer.Duration(); \
32    } \
33  }while(0)
34  #define LAYER_TIMING_STOP(name, index) do { \
35    if (this->phase() == TRAIN) { \
36      this->name##_stop_time_per_layer[index] = this->timer.Duration(); \
37      this->name##_time_per_layer[index] += (this->name##_stop_time_per_layer[index] - this->name##_start_time_per_layer[index]); \
38    } \
39  }while(0)
40  #define ITER_TIMING_START() do { \
41    if (this->phase() == TRAIN) { \
42      this->timer.Start(); \
43    } \
44  }while(0)
45  #define ITER_TIMING_STOP(name) do { \
46    if (this->phase() == TRAIN) { \
47      this->name##_time_per_iter += this->timer.MicroSeconds(); \
48    } \
49  }while(0)
50  #else
51  #define LAYER_TIMING_START(name,index)
52  #define LAYER_TIMING_STOP(name,index)
53  #define ITER_TIMING_START()
54  #define ITER_TIMING_STOP(name)
55  #endif &bsol;* CAFFE_PER_LAYER_TIMINGS */
56  template <typename Dtype>
57  Net<Dtype>::Net(const NetParameter& param, const Net* root_net)
58      : root_net_(root_net) {
59    Init(param);
60  }
61  template <typename Dtype>
62  Net<Dtype>::Net(const string& param_file, Phase phase,
63      const int level, const vector<string>* stages,
64      const Net* root_net, std::string engine)
65      : root_net_(root_net) {
66    NetParameter param;
67    ReadNetParamsFromTextFileOrDie(param_file, &param);
68    param.mutable_state()->set_phase(phase);
69    if (stages != NULL) {
70      for (int i = 0; i < stages->size(); i++) {
71        param.mutable_state()->add_stage((*stages)[i]);
72      }
73    }
74    param.mutable_state()->set_level(level);
75    if (engine != "")
76      param.set_engine(engine);
77    Init(param);
78  }
79  template <typename Dtype>
80  void Net<Dtype>::Init(const NetParameter& in_param) {
81    CHECK(Caffe::root_solver() || root_net_)
82        << "root_net_ needs to be set for all non-root solvers";
83  #ifdef _OPENMP
84    static bool executed = false;
85    if (!executed) {
86      if (Caffe::mode() == Caffe::GPU) {
87        caffe::cpu::OpenMpManager::setGpuEnabled();
88      } else {
89        caffe::cpu::OpenMpManager::setGpuDisabled();
90      }
91      caffe::cpu::OpenMpManager::bindOpenMpThreads();
92      caffe::cpu::OpenMpManager::printVerboseInformation();
93    }
94  #endif
95    phase_ = in_param.state().phase();
96    NetParameter filtered_param;
97    FilterNet(in_param, &filtered_param);
98  #ifdef USE_MKL2017_AS_DEFAULT_ENGINE
99    if (filtered_param.engine() == "")
100      filtered_param.set_engine("MKL2017");
101  #endif
102  #ifdef USE_MKLDNN_AS_DEFAULT_ENGINE
103    if (filtered_param.engine() == "")
104      filtered_param.set_engine("MKLDNN");
105  #endif
106    engine_name_ = filtered_param.engine();
107    NetParameter& param = filtered_param;
108    NetParameter param_with_splits;
109    InsertSplits(param, &param_with_splits);
110    param = param_with_splits;
111    NetParameter compiled_param;
112    CompileNet(param, &compiled_param);
113    param = compiled_param;
114    this->bn_scale_remove_ = param.compile_net_state().bn_scale_remove();
115    this->bn_scale_merge_ = param.compile_net_state().bn_scale_merge();
116    int kept_bn_layers_num = param.compile_net_state().kept_bn_layers_size();
117    for (int idx = 0; idx < kept_bn_layers_num; ++idx) {
118      this->kept_bn_layers_.push_back(param.compile_net_state().kept_bn_layers(idx));
119    }
120    NetParameter param_with_stats_batch_size;
121    if (param.has_bn_stats_batch_size()) {
122      ApplyBnStatsBatchSize(param, &param_with_stats_batch_size);
123      param = param_with_stats_batch_size;
124    }
125  #ifdef USE_MLSL
126    NetParameter param_with_mn;
127    if (mn::is_multinode()) {
128      ApplyMultinodeParams<Dtype>(param, &param_with_mn);
129      param = param_with_mn;
130    }
131  #endif
132    if (Caffe::root_solver()) {
133      LOG(INFO) << "Initializing net from parameters: " << std::endl;
134      LOG(INFO).flush();
135      fflush(0);
136      param.PrintDebugString();
137      fflush(0);
138    }
139    name_ = param.name();
140    map<string, int> blob_name_to_idx;
141    set<string> available_blobs;
142    memory_used_ = 0;
143    bottom_vecs_.resize(param.layer_size());
144    top_vecs_.resize(param.layer_size());
145    bottom_id_vecs_.resize(param.layer_size());
146    param_id_vecs_.resize(param.layer_size());
147    top_id_vecs_.resize(param.layer_size());
148    bottom_need_backward_.resize(param.layer_size());
149    max_blob_count = 0;
150    for (int layer_id = 0; layer_id < param.layer_size(); ++layer_id) {
151      bool share_from_root = !Caffe::root_solver()
152          && root_net_->layers_[layer_id]->ShareInParallel();
153      if (!param.layer(layer_id).has_phase()) {
154        param.mutable_layer(layer_id)->set_phase(phase_);
155      }
156      const LayerParameter& layer_param = param.layer(layer_id);
157      if (param.engine() != "") {
158        if (param.layer(layer_id).engine() == "") {
159          param.mutable_layer(layer_id)->set_engine(param.engine());
160        }
161        else {
162          if ((!param.layer(layer_id).engine().compare("MKL2017") && !param.engine().compare("MKLDNN")) 
163             || (!param.layer(layer_id).engine().compare("MKLDNN") && !param.engine().compare("MKL2017"))) {
164            param.mutable_layer(layer_id)->set_engine(param.engine());
165          }
166        }
167      }
168      if (layer_param.propagate_down_size() > 0) {
169        CHECK_EQ(layer_param.propagate_down_size(),
170            layer_param.bottom_size())
171            << "propagate_down param must be specified "
172            << "either 0 or bottom_size times ";
173      }
174      if (share_from_root) {
175        LOG(INFO) << "Sharing layer " << layer_param.name() << " from root net";
176        layers_.push_back(root_net_->layers_[layer_id]);
177        layers_[layer_id]->SetShared(true);
178      } else {
179        layers_.push_back(LayerRegistry<Dtype>::CreateLayer(layer_param));
180      }
181      layer_names_.push_back(layer_param.name());
182      LOG_IF(INFO, Caffe::root_solver())
183          << "Creating Layer " << layer_param.name();
184      bool need_backward = false;
185      for (int bottom_id = 0; bottom_id < layer_param.bottom_size();
186           ++bottom_id) {
187        const int blob_id = AppendBottom(param, layer_id, bottom_id,
188                                         &available_blobs, &blob_name_to_idx);
189        need_backward |= blob_need_backward_[blob_id];
190      }
191      int num_top = layer_param.top_size();
192      for (int top_id = 0; top_id < num_top; ++top_id) {
193        AppendTop(param, layer_id, top_id, &available_blobs, &blob_name_to_idx);
194        if (layer_param.type() == "Input") {
195          const int blob_id = blobs_.size() - 1;
196          net_input_blob_indices_.push_back(blob_id);
197          net_input_blobs_.push_back(blobs_[blob_id].get());
198        }
199      }
200      Layer<Dtype>* layer = layers_[layer_id].get();
201      if (layer->AutoTopBlobs()) {
202        const int needed_num_top =
203            std::max(layer->MinTopBlobs(), layer->ExactNumTopBlobs());
204        for (; num_top < needed_num_top; ++num_top) {
205          AppendTop(param, layer_id, num_top, NULL, NULL);
206        }
207      }
208  #ifdef USE_MLSL
209      if (caffe::TRAIN == param.state().phase()) {
210        int global_batch_size = mn::train::get_global_minibatch_size();
211        int fake_batch_size = mn::get_distrib()->get_data_parts();
212        if (mn::use_param_server() && mn::is_param_server()) {
213          fake_batch_size = mn::nServer;
214        }
215        if (global_batch_size == 0) {
216          LOG(WARNING) << "SetMinibatchSize " << fake_batch_size;
217          mn::train::set_global_minibatch_size(fake_batch_size);
218        } else {
219          CHECK_EQ(global_batch_size, fake_batch_size);
220        }
221      }
222  #endif &bsol;* USE_MLSL */
223      if (share_from_root) {
224        const vector<Blob<Dtype>*>& base_top = root_net_->top_vecs_[layer_id];
225        const vector<Blob<Dtype>*>& this_top = this->top_vecs_[layer_id];
226        for (int top_id = 0; top_id < base_top.size(); ++top_id) {
227          this_top[top_id]->ReshapeLike(*base_top[top_id]);
228          LOG(INFO) << "Created top blob " << top_id << " (shape: "
229              << this_top[top_id]->shape_string() <<  ") for shared layer "
230              << layer_param.name();
231        }
232      } else {
233        layers_[layer_id]->SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);
234      }
235      LOG_IF(INFO, Caffe::root_solver())
236          << "Setting up " << layer_names_[layer_id];
237      for (int top_id = 0; top_id < top_vecs_[layer_id].size(); ++top_id) {
238        if (blob_loss_weights_.size() <= top_id_vecs_[layer_id][top_id]) {
239          blob_loss_weights_.resize(top_id_vecs_[layer_id][top_id] + 1, Dtype(0));
240        }
241        blob_loss_weights_[top_id_vecs_[layer_id][top_id]] = layer->loss(top_id);
242        LOG_IF(INFO, Caffe::root_solver())
243            << "Top shape: " << top_vecs_[layer_id][top_id]->shape_string();
244        if (layer->loss(top_id)) {
245          LOG_IF(INFO, Caffe::root_solver())
246              << "    with loss weight " << layer->loss(top_id);
247        }
248        memory_used_ += top_vecs_[layer_id][top_id]->count();
249        if (max_blob_count < top_vecs_[layer_id][top_id]->count()) max_blob_count = top_vecs_[layer_id][top_id]->count();
250      }
251      LOG_IF(INFO, Caffe::root_solver())
252          << "Memory required for data: " << memory_used_ * sizeof(Dtype);
253      LOG_IF(INFO, Caffe::root_solver())
254          << "Biggest Memory of single blob: " << max_blob_count * sizeof(Dtype);
255      const int param_size = layer_param.param_size();
256      const int num_param_blobs = layers_[layer_id]->blobs().size();
257      CHECK_LE(param_size, num_param_blobs)
258          << "Too many params specified for layer " << layer_param.name();
259      ParamSpec default_param_spec;
260      for (int param_id = 0; param_id < num_param_blobs; ++param_id) {
261        const ParamSpec* param_spec = (param_id < param_size) ?
262            &layer_param.param(param_id) : &default_param_spec;
263        const bool param_need_backward = param_spec->lr_mult() != 0;
264        need_backward |= param_need_backward;
265        layers_[layer_id]->set_param_propagate_down(param_id,
266                                                    param_need_backward);
267      }
268      for (int param_id = 0; param_id < num_param_blobs; ++param_id) {
269        AppendParam(param, layer_id, param_id);
270      }
271      layer_need_backward_.push_back(need_backward);
272      if (need_backward) {
273        for (int top_id = 0; top_id < top_id_vecs_[layer_id].size(); ++top_id) {
274          blob_need_backward_[top_id_vecs_[layer_id][top_id]] = true;
275        }
276      }
277    }
278    if ((phase_ == TEST) && getenv("CAFFE_INFERENCE_MEM_OPT"))
279      CircleBuf::Instance()->SetBufSize(max_blob_count * sizeof(Dtype));
280    set<string> blobs_under_loss;
281    set<string> blobs_skip_backp;
282    for (int layer_id = layers_.size() - 1; layer_id >= 0; --layer_id) {
283      bool layer_contributes_loss = false;
284      bool layer_skip_propagate_down = true;
285      for (int top_id = 0; top_id < top_vecs_[layer_id].size(); ++top_id) {
286        const string& blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];
287        if (layers_[layer_id]->loss(top_id) ||
288            (blobs_under_loss.find(blob_name) != blobs_under_loss.end())) {
289          layer_contributes_loss = true;
290        }
291        if (blobs_skip_backp.find(blob_name) == blobs_skip_backp.end()) {
292          layer_skip_propagate_down = false;
293        }
294        if (layer_contributes_loss && !layer_skip_propagate_down)
295          break;
296      }
297      if (layer_need_backward_[layer_id] && layer_skip_propagate_down) {
298        layer_need_backward_[layer_id] = false;
299        for (int bottom_id = 0; bottom_id < bottom_vecs_[layer_id].size();
300                 ++bottom_id) {
301          bottom_need_backward_[layer_id][bottom_id] = false;
302        }
303      }
304      if (!layer_contributes_loss) { layer_need_backward_[layer_id] = false; }
305      if (Caffe::root_solver()) {
306        if (layer_need_backward_[layer_id]) {
307          LOG(INFO) << layer_names_[layer_id] << " needs backward computation.";
308        } else {
309          LOG(INFO) << layer_names_[layer_id]
310              << " does not need backward computation.";
311        }
312      }
313      for (int bottom_id = 0; bottom_id < bottom_vecs_[layer_id].size();
314           ++bottom_id) {
315        if (layer_contributes_loss) {
316          const string& blob_name =
317              blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
318          blobs_under_loss.insert(blob_name);
319        } else {
320          bottom_need_backward_[layer_id][bottom_id] = false;
321        }
322        if (!bottom_need_backward_[layer_id][bottom_id]) {
323          const string& blob_name =
324                     blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
325          blobs_skip_backp.insert(blob_name);
326        }
327      }
328    }
329    if (param.force_backward()) {
330      for (int layer_id = 0; layer_id < layers_.size(); ++layer_id) {
331        layer_need_backward_[layer_id] = true;
332        for (int bottom_id = 0;
333             bottom_id < bottom_need_backward_[layer_id].size(); ++bottom_id) {
334          bottom_need_backward_[layer_id][bottom_id] =
335              bottom_need_backward_[layer_id][bottom_id] ||
336              layers_[layer_id]->AllowForceBackward(bottom_id);
337          blob_need_backward_[bottom_id_vecs_[layer_id][bottom_id]] =
338              blob_need_backward_[bottom_id_vecs_[layer_id][bottom_id]] ||
339              bottom_need_backward_[layer_id][bottom_id];
340        }
341        for (int param_id = 0; param_id < layers_[layer_id]->blobs().size();
342             ++param_id) {
343          layers_[layer_id]->set_param_propagate_down(param_id, true);
344        }
345      }
346    }
347    for (set<string>::iterator it = available_blobs.begin();
348        it != available_blobs.end(); ++it) {
349      LOG_IF(INFO, Caffe::root_solver())
350          << "This network produces output " << *it;
351      net_output_blobs_.push_back(blobs_[blob_name_to_idx[*it]].get());
352      net_output_blob_indices_.push_back(blob_name_to_idx[*it]);
353    }
354    for (size_t blob_id = 0; blob_id < blob_names_.size(); ++blob_id) {
355      blob_names_index_[blob_names_[blob_id]] = blob_id;
356    }
357    for (size_t layer_id = 0; layer_id < layer_names_.size(); ++layer_id) {
358      layer_names_index_[layer_names_[layer_id]] = layer_id;
359    }
360    ShareWeights();
361    debug_info_ = param.debug_info();
362  #ifdef USE_MLSL
363    if (this->phase_ == TRAIN) {
364        for (int layer_id = 0; layer_id < param.layer_size(); ++layer_id) {
365          boost::shared_ptr<Layer<Dtype>> layer{ layers_[layer_id] };
366          if ((layer->layerOp != nullptr) && layer->layerOp->HasParameterSets()) {
367                vector<int> param_ids = get_layer_learnable_param_ids(layer_id);
368                for (int i = 0; i < param_ids.size(); i++) {
369                    int mlsl_weight_size = layer->layerOp->GetParameterSet(i)->GetLocalKernelCount()
370                                          * layer->layerOp->GetParameterSet(i)->GetKernelSize()
371                                          * sizeof(Dtype);
372                    int caffe_weight_size = learnable_params_[param_ids[i]]->count() * sizeof(Dtype);
373                    if (mlsl_weight_size < caffe_weight_size)
374                        LOG(FATAL) << "InitNet: ERROR: check weight sizes for layer " << layer->type() << ", layer_id " << layer_id
375                                   << ", param_id " << param_ids[i]
376                                   << ", MLSL weight size in bytes " << mlsl_weight_size
377                                   << ", CAFFE weight size in bytes " << caffe_weight_size;
378                }
379            }
380        }
381    }
382  #endif &bsol;* USE_MLSL */
383  #ifdef CAFFE_PER_LAYER_TIMINGS
384    InitTimers();
385  #endif
386    LOG_IF(INFO, Caffe::root_solver()) << "Network initialization done.";
387  }
388  template <typename Dtype>
389  void Net<Dtype>::FilterNet(const NetParameter& param,
390      NetParameter* param_filtered) {
391    NetState net_state(param.state());
392    param_filtered->CopyFrom(param);
393    param_filtered->clear_layer();
394    for (int i = 0; i < param.layer_size(); ++i) {
395      const LayerParameter& layer_param = param.layer(i);
396      const string& layer_name = layer_param.name();
397      CHECK(layer_param.include_size() == 0 || layer_param.exclude_size() == 0)
398            << "Specify either include rules or exclude rules; not both.";
399      bool layer_included = (layer_param.include_size() == 0);
400      for (int j = 0; layer_included && j < layer_param.exclude_size(); ++j) {
401        if (StateMeetsRule(net_state, layer_param.exclude(j), layer_name)) {
402          layer_included = false;
403        }
404      }
405      for (int j = 0; !layer_included && j < layer_param.include_size(); ++j) {
406        if (StateMeetsRule(net_state, layer_param.include(j), layer_name)) {
407          layer_included = true;
408        }
409      }
410      if (layer_included) {
411        param_filtered->add_layer()->CopyFrom(layer_param);
412      }
413    }
414  }
415  template <typename Dtype>
416  void Net<Dtype>::CompileNet(const NetParameter& param,
417      NetParameter* param_compiled) {
418    #define NUM_OF_RULES sizeof(CompileRules)/sizeof(CompileRules[0])
419    #define COMPILE_BN_FOLDING_INDEX 0
420    #define COMPILE_CONV_RELU_FUSION_INDEX 2
421    #define COMPILE_BN_RELU_FUSION_INDEX 3
422    #define COMPILE_SPARSE_INDEX 5
423    #define COMPILE_CONV_SUM_FUSION_INDEX 6
424    #define COMPILE_FC_RELU_FUSION_INDEX 7
425    int i, current = 0;
426    NetParameter param_temp[2];
427    void (*CompileRules[]) (const NetParameter& param, NetParameter* param_compiled) =
428      {RemoveBNScale<Dtype>, CompilationRuleRemoveScale, CompilationRuleConvReluFusion,
429      CompilationRuleFuseBnRelu, CompilationRuleBNInplace, CompilationRuleSparse, 
430      CompilationRuleConvSumFusion, CompilationRuleFuseFCRelu};
431    bool disabled[NUM_OF_RULES] = {false};
432  #ifdef DISABLE_BN_FOLDING
433    disabled[COMPILE_BN_FOLDING_INDEX] = true;
434  #endif
435  #ifdef DISABLE_CONV_RELU_FUSION
436    disabled[COMPILE_CONV_RELU_FUSION_INDEX] = true;
437  #endif
438  #ifdef DISABLE_BN_RELU_FUSION
439    disabled[COMPILE_BN_RELU_FUSION_INDEX] = true;
440  #endif
441  #ifdef DISABLE_CONV_SUM_FUSION
442    disabled[COMPILE_CONV_SUM_FUSION_INDEX] = true;
443  #endif
444  #ifdef DISABLE_SPARSE
445    disabled[COMPILE_SPARSE_INDEX] = true;
446  #endif
447  #ifdef DISABLE_FC_RELU_FUSION
448    disabled[COMPILE_FC_RELU_FUSION_INDEX] = true;
449  #endif
450    param_temp[current].CopyFrom(param);
451    for (i = 0; i < NUM_OF_RULES; i++)
452      if (!disabled[i]) {
453        param_temp[1 - current].CopyFrom(param_temp[current]);
454        param_temp[1 - current].clear_layer();   
455        (*CompileRules[i]) (param_temp[current], &param_temp[1 - current]);
456        current = 1 - current;
457      }
458    param_compiled->CopyFrom(param_temp[current]);
459    #undef NUM_OF_RULES
460    #undef COMPILE_BN_FOLDING_INDEX
461    #undef COMPILE_CONV_RELU_FUSION_INDEX
462    #undef COMPILE_BN_RELU_FUSION_INDEX
463    #undef COMPILE_SPARSE_INDEX
464    #undef COMPILE_CONV_SUM_FUSION_INDEX
465    #undef COMPILE_FC_RELU_FUSION_INDEX
466  }
467  template <typename Dtype>
468  void Net<Dtype>::CompilationRuleRemoveScale(const NetParameter& param,
469                                      NetParameter* param_compiled) {
470    bool merge_bn_scale = false;
471    std::set<std::string> layers_to_drop;
472    for (int i = 0; i < param.layer_size(); ++i) {
473      LayerParameter* layer_param =
474            (const_cast<NetParameter&>(param)).mutable_layer(i);
475      bool layer_included = true;
476      if (((layer_param->type().compare("BatchNorm") == 0) &&
477           ((layer_param->batch_norm_param().engine() == BatchNormParameter_Engine_MKL2017) ||
478            ((layer_param->batch_norm_param().engine() == BatchNormParameter_Engine_DEFAULT) &&
479             (layer_param->has_engine() == false)  &&
480             (param.engine().compare("MKL2017") == 0)) ||
481            ((layer_param->batch_norm_param().has_engine() == false) &&
482             (layer_param->engine().compare("MKL2017") == 0)))) ||
483          ((layer_param->type().compare("BatchNorm") == 0) &&
484           ((layer_param->batch_norm_param().engine() == BatchNormParameter_Engine_MKLDNN) ||
485            ((layer_param->batch_norm_param().engine() == BatchNormParameter_Engine_DEFAULT) &&
486             (layer_param->has_engine() == false)  &&
487             (param.engine().compare("MKLDNN") == 0)) ||
488            ((layer_param->batch_norm_param().has_engine() == false) &&
489             (layer_param->engine().compare("MKLDNN") == 0))))) {
490        std::vector<const LayerParameter*> consumer_layer_params;
491        GetBlobConsumers(consumer_layer_params,
492                         layer_param->top(0),
493                         param,
494                         i+1 < param.layer_size() ? i+1 : i);
495        const LayerParameter& consumer_layer_param =
496                                      consumer_layer_params.size() > 0 ?
497                                      *(consumer_layer_params[0]) : *layer_param;
498        if ((consumer_layer_param.type().compare("Scale") == 0) &&
499             (consumer_layer_param.bottom_size() == 1)) {
500          string& batchnorm_top_blob_name =
501              const_cast<string&>(layer_param->top(0));
502          const string& scale_top_blob_name = consumer_layer_param.top(0);
503          layers_to_drop.insert(consumer_layer_param.name());
504          if (!merge_bn_scale) merge_bn_scale = true;
505          batchnorm_top_blob_name.resize(scale_top_blob_name.size());
506          batchnorm_top_blob_name.replace(0,
507                                          scale_top_blob_name.size(),
508                                          scale_top_blob_name);
509          bool scale_bias_term = consumer_layer_param.
510                                 scale_param().bias_term();
511          layer_param->mutable_batch_norm_param()->
512          set_bias_term(scale_bias_term);
513          if (consumer_layer_param.blobs_size() == 2) {
514            layer_param->add_blobs()->CopyFrom(consumer_layer_param.blobs(0));
515            layer_param->add_blobs()->CopyFrom(consumer_layer_param.blobs(1));
516          }
517          if (consumer_layer_param.param_size() == 2) {
518            layer_param->add_param()->CopyFrom(consumer_layer_param.param(0));
519            layer_param->add_param()->CopyFrom(consumer_layer_param.param(1));
520          }
521        }
522      }
523      if (layers_to_drop.find(layer_param->name()) != layers_to_drop.end()) {
524        LOG_IF(INFO, Caffe::root_solver()) << "Dropped layer: "
525               << layer_param->name() << std::endl;
526        layer_included = false;
527        layers_to_drop.erase(layers_to_drop.find(layer_param->name()));
528      }
529      if (layer_included) {
530        param_compiled->add_layer()->CopyFrom(*layer_param);
531      }
532    }
533    param_compiled->mutable_compile_net_state()->set_bn_scale_merge(merge_bn_scale);
534  }
535  template <typename Dtype>
536  void Net<Dtype>::CompilationRuleConvReluFusion(const NetParameter& param,
537                                      NetParameter* param_compiled) {
538    std::set<std::string> layers_to_drop;
539    for (int i = 0; i < param.layer_size(); ++i) {
540      LayerParameter* layer_param =
541            (const_cast<NetParameter&>(param)).mutable_layer(i);
542      bool layer_included = true;
543      if ((layer_param->type().compare("Convolution") == 0) &&
544          ((layer_param->convolution_param().engine() == ConvolutionParameter_Engine_MKLDNN) ||
545           ((layer_param->convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &&
546            (layer_param->engine().compare(0, 6, "MKLDNN") == 0) &&
547            (layer_param->engine().find(":DLA", 6) == string::npos)) ||
548           ((layer_param->convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &&
549            (layer_param->engine() == "") &&
550            (param.engine().compare(0, 6, "MKLDNN") == 0 &&
551             param.engine().find(":DLA", 6) == string::npos)))) {
552        std::vector<const LayerParameter*> consumer_layer_params;
553        GetBlobConsumers(consumer_layer_params, layer_param->top(0),
554                         param, i+1 < param.layer_size() ? i+1 : i);
555        const LayerParameter& consumer_layer_param =
556                                      consumer_layer_params.size() > 0 ?
557                                      *(consumer_layer_params[0]) : *layer_param;
558        if ((consumer_layer_param.type().compare("ReLU") == 0) &&
559            ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_MKLDNN) ||
560             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &&
561              (consumer_layer_param.engine().compare(0, 6, "MKLDNN") == 0 &&
562               consumer_layer_param.engine().find(":DLA", 6) == string::npos)) ||
563             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &&
564              (consumer_layer_param.engine() == "") &&
565              (param.engine().compare(0, 6, "MKLDNN") == 0 &&
566               param.engine().find(":DLA", 6) == string::npos)))) {
567          string& convolution_top_blob_name =
568              const_cast<string&>(layer_param->top(0));
569          float negative_slope1 =
570                    consumer_layer_param.relu_param().negative_slope();
571          layer_param->mutable_convolution_param()->set_relu(true);
572          layer_param->mutable_convolution_param()->set_negative_slope(negative_slope1);
573          if(param.state().phase() == TEST) {
574            const string& scale_top_blob_name = consumer_layer_param.top(0);
575            layers_to_drop.insert(consumer_layer_param.name());
576            convolution_top_blob_name.resize(scale_top_blob_name.size());
577            convolution_top_blob_name.replace(0,
578                                            scale_top_blob_name.size(),
579                                            scale_top_blob_name);
580          }
581          if(param.state().phase() == TRAIN) {
582            if(i+1 < param.layer_size()) {
583              LayerParameter* relu_layer_param =
584                (const_cast<NetParameter&>(param)).mutable_layer(i+1);
585              relu_layer_param->mutable_relu_param()->set_fuse(true);
586            }
587          }
588        }
589      }
590      if(param.state().phase() == TEST) {
591        if (layers_to_drop.find(layer_param->name()) != layers_to_drop.end()) {
592          LOG_IF(INFO, Caffe::root_solver()) << "Dropped layer: "
593                 << layer_param->name() << std::endl;
594          layer_included = false;
595          layers_to_drop.erase(layers_to_drop.find(layer_param->name()));
596        }
597      }
598      if (layer_included) {
599        param_compiled->add_layer()->CopyFrom(*layer_param);
600      }
601    }
602  }
603  template <typename Dtype>
604  void Net<Dtype>::CompilationRuleBNInplace(const NetParameter& param,
605                                        NetParameter* param_compiled) {
606    for (int i = 0; i < param.layer_size(); ++i) {
607      LayerParameter* layer_param =
608          (const_cast<NetParameter&>(param)).mutable_layer(i);
609      if (((layer_param->type().compare("BatchNorm") == 0) &&
610           (layer_param->batch_norm_param().engine() ==
611                BatchNormParameter_Engine_MKL2017 ||
612            ((layer_param->batch_norm_param().engine() ==
613              BatchNormParameter_Engine_DEFAULT) &&
614             param.engine().compare("MKL2017") == 0))) &&
615          (layer_param->top(0) == layer_param->bottom(0))) {
616        std::string& batch_norm_top = const_cast<string&>(layer_param->top(0));
617        std::vector<const LayerParameter*> consumer_layer_params;
618        GetBlobConsumers(consumer_layer_params, batch_norm_top, param,
619                         i + 1 < param.layer_size() ? i + 1 : i);
620        for (std::vector<const LayerParameter*>::iterator it =
621                 consumer_layer_params.begin();
622             it != consumer_layer_params.end(); ++it) {
623          if (((*it)->top_size() > 0) &&
624              ((*it)->bottom(0).compare((*it)->top(0)) == 0)) {
625            const_cast<string&>((*it)->top(0)).append("_x");
626          }
627          for (unsigned int i = 0; i < (*it)->bottom_size(); ++i) {
628            if ((*it)->bottom(i).compare(batch_norm_top) == 0) {
629              const_cast<string&>((*it)->bottom(i)).append("_x");
630            }
631          }
632        }
633        batch_norm_top.append("_x");
634      }
635      param_compiled->add_layer()->CopyFrom(*layer_param);
636    }
637    if(param.state().phase() == TEST) return;
638    std::map<string, int> inplace_blob_name_to_index;
639    std::map<string, int> specified_layer_blob_name_to_index;
640    vector<vector<const LayerParameter*>> layer_pairs;
641    vector<vector<string>> specified_layer_input_blob_names;
642    vector<string> raise_non_inplace_layer_type_list;
643    raise_non_inplace_layer_type_list.push_back("Eltwise");
644    for (auto layer_type : raise_non_inplace_layer_type_list) {
645      specified_layer_input_blob_names.clear();
646      inplace_blob_name_to_index.clear();
647      layer_pairs.clear();
648      ParseNetInplaceStatus(
649          inplace_blob_name_to_index, specified_layer_blob_name_to_index,
650          specified_layer_input_blob_names, param_compiled, layer_type);
651      for (auto each_blob_list : specified_layer_input_blob_names) {
652        GetNeedToCancelInplaceLayers(
653            layer_pairs, specified_layer_blob_name_to_index,
654            inplace_blob_name_to_index, each_blob_list, *param_compiled);
655        for (auto each_layer_pair : layer_pairs) {
656          std::string& layer_top =
657              const_cast<string&>((each_layer_pair[0])->top(0));
658          for (unsigned int i = 0; i < each_layer_pair[1]->bottom_size(); ++i) {
659            if (each_layer_pair[1]->bottom(i).compare(layer_top) == 0) {
660              const_cast<string&>(each_layer_pair[1]->bottom(i)).append("_x");
661            }
662          }
663          const_cast<string&>((each_layer_pair[0])->top(0)).append("_x");
664        }
665      }
666    }
667    return;
668  }
669  template <typename Dtype>
670  void Net<Dtype>::CompilationRuleConvSumFusion(const NetParameter& param,
671                                                NetParameter* param_compiled) {
672    if (param.state().phase() != TEST || param.engine().compare("MKLDNN") != 0) {
673      param_compiled->CopyFrom(param);
674      return;
675    }
676    string blob_need_to_insert;
677    LayerParameter* need_to_convert_layer = NULL;
678    bool has_relu_flag = true;
679    bool need_fusion_flag;
680    bool switch_flag = false;
681    std::set<string> invalid_fusion_blob_names;
682    for (int i = 0; i < param.layer_size(); i++) {
683      need_fusion_flag = true;
684      LayerParameter* layer_param =
685          (const_cast<NetParameter&>(param)).mutable_layer(i);
686      if (layer_param->type().compare("Split") == 0 &&
687          layer_param->top_size() > 2) {
688        for (int j = 0; j < layer_param->top_size() - 1; j++) {
689          invalid_fusion_blob_names.insert(layer_param->top(j));
690        }
691      }
692      if (layer_param->type().compare("Convolution") == 0 &&
693          (layer_param->has_engine() == false ||
694           (layer_param->has_engine() == true &&
695            layer_param->engine().compare("MKLDNN") == 0))) {
696        std::vector<const LayerParameter*> child_layers_params;
697        Net<Dtype>::GetBlobConsumers(child_layers_params, layer_param->top(0),
698                                     param,
699                                     i + 1 < param.layer_size() ? i + 1 : i);
700        if (child_layers_params.size() > 0 &&
701            child_layers_params[0]->type().compare("Eltwise") == 0) {
702          for (int k = 0; k < child_layers_params[0]->bottom_size(); k++) {
703            if (invalid_fusion_blob_names.count(
704                    child_layers_params[0]->bottom(k)) > 0) {
705              need_fusion_flag = false;
706              break;
707            }
708          }
709          if (!need_fusion_flag) {
710            param_compiled->add_layer()->CopyFrom(*layer_param);
711            continue;
712          }
713          std::vector<const LayerParameter*> grand_child_layers_params;
714          Net<Dtype>::GetBlobConsumers(grand_child_layers_params,
715                                       child_layers_params[0]->top(0), param,
716                                       i + 1 < param.layer_size() ? i + 1 : i);
717          const LayerParameter& grand_child_layer_param =
718              grand_child_layers_params.size() > 0
719                  ? *(grand_child_layers_params[0])
720                  : *layer_param;
721          if (grand_child_layer_param.type().compare("ReLU") != 0) {
722            has_relu_flag = false;
723          }
724          if (child_layers_params[0] !=
725              (const_cast<NetParameter&>(param)).mutable_layer(i + 1)) {
726            if (child_layers_params[0]->bottom(0) == layer_param->top(0)) {
727              switch_flag = true;
728            } else {
729              switch_flag = false;
730            }
731            param_compiled->add_layer()->CopyFrom(*layer_param);
732            std::vector<const LayerParameter*> another_eltwise_input_layers_params;
733            if (switch_flag) {
734              Net<Dtype>::GetBlobProducers(another_eltwise_input_layers_params,
735                                          child_layers_params[0]->bottom(1), param,
736                                          i + 1 < param.layer_size() ? i + 1 : i);
737            } else {
738              Net<Dtype>::GetBlobProducers(another_eltwise_input_layers_params,
739                                          child_layers_params[0]->bottom(0), param,
740                                          i + 1 < param.layer_size() ? i + 1 : i);
741            }
742            const LayerParameter& another_eltwise_layer_param =
743                another_eltwise_input_layers_params.size() > 0
744                    ? *(another_eltwise_input_layers_params[0])
745                    : *layer_param;
746            if (another_eltwise_layer_param.type().compare("Convolution") == 0 ) {
747              need_to_convert_layer = layer_param;
748            } 
749            continue;
750          } else {
751            if (need_to_convert_layer == NULL) {
752              if (child_layers_params[0]->bottom(1) == layer_param->top(0)) {
753                switch_flag = true;
754              } else {
755                switch_flag = false;
756              }
757            }
758          }
759          if (has_relu_flag) {
760            const_cast<string&>(layer_param->top(0)) =
761                grand_child_layer_param.top(0);
762          } else {
763            const_cast<string&>(layer_param->top(0)) =
764                child_layers_params[0]->top(0);
765          }
766          if (need_to_convert_layer != NULL) {
767            layer_param->add_bottom(
768                const_cast<string&>(need_to_convert_layer->top(0)));
769            need_to_convert_layer = NULL;
770          } else {
771            if (switch_flag) {
772              layer_param->add_bottom(
773                  const_cast<string&>(child_layers_params[0]->bottom(0)));
774            } else {
775              layer_param->add_bottom(
776                  const_cast<string&>(child_layers_params[0]->bottom(1)));
777            }
778          }
779          if (has_relu_flag) {
780            i += 2;  
781            layer_param->mutable_convolution_param()->set_relu(true);
782          } else {
783            i += 1;
784          }
785          layer_param->mutable_convolution_param()->set_fusion_type(
786              ConvolutionParameter::SUM_FUSION);
787          size_t coeff_size =
788              child_layers_params[0]->eltwise_param().coeff_size();
789          if (coeff_size > 0) {
790            for (int i = 0; i < coeff_size; ++i) {
791              layer_param->mutable_convolution_param()->add_coeff(
792                  child_layers_params[0]->eltwise_param().coeff(i));
793            }
794          }
795        }
796      }
797      param_compiled->add_layer()->CopyFrom(*layer_param);
798    }
799    return;
800  }
801  template <typename Dtype>
802  void Net<Dtype>::CompilationRuleSparse(const NetParameter& param,
803                                         NetParameter* param_compiled) {
804    if (param.state().phase() != TEST || param.engine().compare("MKLDNN") != 0) {
805      param_compiled->CopyFrom(param);
806      return;
807    }
808    LayerParameter* potential_sparse_layer = NULL;
809    LayerParameter* confirmed_sparse_layer = NULL;
810    LayerParameter* layer_param = NULL;
811    std::map<string, string> bottom_blob_layer_mapping;
812    std::map<string, string> top_blob_layer_mapping;
813    std::map<string, std::vector<LayerParameter*>> sparse_layer_name_mapping;  
814    std::vector<LayerParameter*> trigger_sparse_layers;
815    std::map<string, int> conv_layer_id_mapping;
816    std::map<string, int> eltwise_layer_id_mapping;
817    std::map<string, int> layer_name_id_mapping;
818    std::map<int, int> pooling_layer_id_stride;  
819    std::map<int, int> conv_layer_id_stride;  
820    std::map<int, string> pooling_layer_id_top_blob;
821    for (int index = 0; index < param.layer_size(); index++) {
822      layer_param = (const_cast<NetParameter&>(param)).mutable_layer(index);
823      layer_name_id_mapping[layer_param->name()] = index;
824      for (int j = 0; j < layer_param->top_size(); j++) {
825        top_blob_layer_mapping[layer_param->top(j)] = layer_param->name();
826      }
827      for (int k = 0; k < layer_param->bottom_size(); k++) {
828        bottom_blob_layer_mapping[layer_param->bottom(k)] = layer_param->name();
829      }
830      if (layer_param->type().compare("Eltwise") == 0) {
831        eltwise_layer_id_mapping[layer_param->name()] = index;
832      }
833      if (layer_param->type().compare("Convolution") == 0 &&
834          layer_param->has_convolution_param() &&
835          layer_param->convolution_param().kernel_size_size() > 0 &&
836          layer_param->convolution_param().stride_size() > 0) {
837        conv_layer_id_mapping[layer_param->name()] = index;
838        if (layer_param->convolution_param().kernel_size(0) > 1) {
839          potential_sparse_layer = layer_param;
840        } else if (layer_param->convolution_param().kernel_size(0) == 1 &&
841                   layer_param->convolution_param().stride(0) > 1  &&
842            (layer_param->convolution_param().pad_size() == 0 ||
843             (layer_param->convolution_param().pad_size() > 0 &&
844              layer_param->convolution_param().pad(0) == 0))) {
845          if (potential_sparse_layer == NULL)
846              continue;
847          confirmed_sparse_layer = potential_sparse_layer;
848          if (trigger_sparse_layers.size() > 0) {
849            for (int j = 0; j < trigger_sparse_layers.size(); j++) {
850              if (top_blob_layer_mapping[trigger_sparse_layers[j]->bottom(0)] !=
851                  top_blob_layer_mapping[layer_param->bottom(0)]) {
852                trigger_sparse_layers.clear();
853                break;
854              }
855            }
856            trigger_sparse_layers.push_back(layer_param);
857            sparse_layer_name_mapping[confirmed_sparse_layer->name()] =
858                trigger_sparse_layers;
859          } else {
860            trigger_sparse_layers.push_back(layer_param);
861          }
862        }
863      }
864    }
865    if(trigger_sparse_layers.size() > 1)
866      sparse_layer_name_mapping[confirmed_sparse_layer->name()] = trigger_sparse_layers;
867    std::map<string, std::vector<LayerParameter*>>::iterator sparse_it =
868        sparse_layer_name_mapping.begin();
869    while (sparse_it != sparse_layer_name_mapping.end() && sparse_it->second.size() > 1) {
870      if (sparse_it->second[0]->convolution_param().stride(0) !=
871          sparse_it->second[1]->convolution_param().stride(0)) {
872            continue;
873      }
874      LayerParameter* sparse_layer_param =
875          (const_cast<NetParameter&>(param))
876              .mutable_layer(layer_name_id_mapping[sparse_it->first]);
877      int updated_stride_value =
878          sparse_layer_param->convolution_param().stride(0) *
879          sparse_it->second[0]->convolution_param().stride(0);
880      conv_layer_id_stride[conv_layer_id_mapping[sparse_it->first]] =
881          updated_stride_value;
882      conv_layer_id_stride[conv_layer_id_mapping[sparse_it->second[0]->name()]] = 1;
883      conv_layer_id_stride[conv_layer_id_mapping[sparse_it->second[1]->name()]] = 1;
884      std::map<string, int>::iterator eltwise_iter = eltwise_layer_id_mapping.begin();
885      while (eltwise_iter != eltwise_layer_id_mapping.end()) {
886        if (conv_layer_id_mapping[sparse_it->first] < eltwise_iter->second &&
887            eltwise_iter->second < conv_layer_id_mapping[sparse_it->second[0]->name()]) {
888          break;  
889        }
890        eltwise_iter++;
891      }
892      std::vector<int> need_add_pooling_layer_id;
893      LayerParameter* eltwise_layer_param =
894          (const_cast<NetParameter&>(param)).mutable_layer(eltwise_iter->second);
895      for (int k = 0; k < eltwise_layer_param->bottom_size() - 1; k++) {
896        need_add_pooling_layer_id.push_back(
897            layer_name_id_mapping
898                [top_blob_layer_mapping[eltwise_layer_param->bottom(k)]]);
899        int pooling_layer_id = layer_name_id_mapping
900            [top_blob_layer_mapping[eltwise_layer_param->bottom(k)]];
901        pooling_layer_id_stride[pooling_layer_id] = updated_stride_value;
902        pooling_layer_id_top_blob[pooling_layer_id] =
903            eltwise_layer_param->bottom(k);
904      }
905      sparse_it++;
906    }
907    for (int i = 0; i < param.layer_size(); i++) {
908      LayerParameter* each_layer_param =
909          (const_cast<NetParameter&>(param)).mutable_layer(i);
910      if (conv_layer_id_stride.find(i) != conv_layer_id_stride.end()) {
911        each_layer_param->mutable_convolution_param()->set_stride(
912            0, conv_layer_id_stride[i]);
913      } else if (pooling_layer_id_stride.find(i) !=
914                 pooling_layer_id_stride.end()) {
915        param_compiled->add_layer()->CopyFrom(*each_layer_param);
916        each_layer_param = param_compiled->add_layer();
917        each_layer_param->Clear();
918        each_layer_param->set_type("Pooling");
919        each_layer_param->set_name(pooling_layer_id_top_blob[i] + "_p");
920        each_layer_param->add_bottom(pooling_layer_id_top_blob[i]);
921        each_layer_param->add_top(pooling_layer_id_top_blob[i] + "_p");
922        each_layer_param->mutable_pooling_param()->set_stride(
923            pooling_layer_id_stride[i]);
924        each_layer_param->mutable_pooling_param()->set_kernel_size(1);
925        each_layer_param->mutable_pooling_param()->set_pool(
926            PoolingParameter_PoolMethod_MAX);
927        int target_layer_id = layer_name_id_mapping
928            [bottom_blob_layer_mapping[pooling_layer_id_top_blob[i]]];
929        LayerParameter* target_layer_param =
930            (const_cast<NetParameter&>(param)).mutable_layer(target_layer_id);
931        int target_blob_index = 0;
932        bool found_blob_flag = false;
933        for (; target_blob_index < target_layer_param->bottom_size();
934             target_blob_index++) {
935          if (target_layer_param->bottom(target_blob_index) ==
936              pooling_layer_id_top_blob[i]) {
937            found_blob_flag = true;
938            break;
939          }
940        }
941        if (found_blob_flag) {
942          target_layer_param->set_bottom(target_blob_index,
943                                         pooling_layer_id_top_blob[i] + "_p");
944          continue;
945        }
946      }
947      param_compiled->add_layer()->CopyFrom(*each_layer_param);
948    }
949  }
950  template <typename Dtype>
951  void Net<Dtype>::CompilationRuleFuseBnRelu(const NetParameter& param,
952                                      NetParameter* param_compiled) {
953                       std::set<std::string> layers_to_drop;
954    for (int i = 0; i < param.layer_size(); ++i) {
955      LayerParameter* layer_param =
956            (const_cast<NetParameter&>(param)).mutable_layer(i);
957      bool layer_included = true;
958      if (((layer_param->type().compare("BatchNorm") == 0) &&
959           ((layer_param->batch_norm_param().engine() == BatchNormParameter_Engine_MKLDNN) ||
960            ((layer_param->batch_norm_param().engine() == BatchNormParameter_Engine_DEFAULT) &&
961             (layer_param->has_engine() == false)  &&
962             (param.engine().compare("MKLDNN") == 0)) ||
963            (param.engine() == "" && layer_param->engine().compare("MKLDNN") == 0)))) {
964        std::vector<const LayerParameter*> consumer_layer_params;
965        GetBlobConsumers(consumer_layer_params,
966                         layer_param->top(0),
967                         param,
968                         i+1 < param.layer_size() ? i+1 : i);
969        const LayerParameter& consumer_layer_param =
970                                      consumer_layer_params.size() > 0 ?
971                                      *(consumer_layer_params[0]) : *layer_param;
972        if ((consumer_layer_param.type().compare("ReLU") == 0) &&
973            ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_MKLDNN) ||
974             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &&
975              (consumer_layer_param.engine().compare(0, 6, "MKLDNN") == 0 &&
976               consumer_layer_param.engine().find(":DLA", 6) == string::npos)) ||
977             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &&
978              (consumer_layer_param.engine() == "") &&
979              (param.engine().compare(0, 6, "MKLDNN") == 0 &&
980               param.engine().find(":DLA", 6) == string::npos))) &&
981               !consumer_layer_param.relu_param().negative_slope()) {
982          string& batchnorm_top_blob_name =
983              const_cast<string&>(layer_param->top(0));
984          if(param.state().phase() == TEST) {
985            const string& relu_top_blob_name = consumer_layer_param.top(0);
986            layers_to_drop.insert(consumer_layer_param.name());
987            batchnorm_top_blob_name.resize(relu_top_blob_name.size());
988            batchnorm_top_blob_name.replace(0,
989                                            relu_top_blob_name.size(),
990                                            relu_top_blob_name);
991          }
992          layer_param->mutable_batch_norm_param()->set_relu(true);
993          if(param.state().phase() == TRAIN) {
994            if(i+1 < param.layer_size()) {
995              LayerParameter* relu_layer_param =
996                (const_cast<NetParameter&>(param)).mutable_layer(i+1);
997              relu_layer_param->mutable_relu_param()->set_fuse(true);
998            }
999          }
1000        }
1001      }
1002      if(param.state().phase() == TEST) {
1003        if (layers_to_drop.find(layer_param->name()) != layers_to_drop.end()) {
1004          LOG_IF(INFO, Caffe::root_solver()) << "Dropped layer: "
1005                 << layer_param->name() << std::endl;
1006          layer_included = false;
1007          layers_to_drop.erase(layers_to_drop.find(layer_param->name()));
1008        }
1009      }
1010      if (layer_included) {
1011        param_compiled->add_layer()->CopyFrom(*layer_param);
1012      }
1013    }
1014  }
1015  template <typename Dtype>
1016  void Net<Dtype>::CompilationRuleFuseFCRelu(const NetParameter& param,
1017                                      NetParameter* param_compiled) {
1018    std::set<std::string> layers_to_drop;
1019    for (int i = 0; i < param.layer_size(); ++i) {
1020      LayerParameter* layer_param =
1021            (const_cast<NetParameter&>(param)).mutable_layer(i);
1022      bool layer_included = true;
1023      if ((layer_param->type().compare("InnerProduct") == 0) &&
1024          ((layer_param->convolution_param().engine() == ConvolutionParameter_Engine_MKLDNN) ||
1025           ((layer_param->convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &&
1026            (layer_param->engine().compare(0, 6, "MKLDNN") == 0) &&
1027            (layer_param->engine().find(":DLA", 6) == string::npos)) ||
1028           ((layer_param->convolution_param().engine() == ConvolutionParameter_Engine_DEFAULT) &&
1029            (layer_param->engine() == "") &&
1030            (param.engine().compare(0, 6, "MKLDNN") == 0 &&
1031             param.engine().find(":DLA", 6) == string::npos)))) {
1032        std::vector<const LayerParameter*> consumer_layer_params;
1033        GetBlobConsumers(consumer_layer_params, layer_param->top(0),
1034                         param, i+1 < param.layer_size() ? i+1 : i);
1035        const LayerParameter& consumer_layer_param =
1036                                      consumer_layer_params.size() > 0 ?
1037                                      *(consumer_layer_params[0]) : *layer_param;
1038        if ((consumer_layer_param.type().compare("ReLU") == 0) &&
1039            ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_MKLDNN) ||
1040             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &&
1041              (consumer_layer_param.engine().compare(0, 6, "MKLDNN") == 0 &&
1042               consumer_layer_param.engine().find(":DLA", 6) == string::npos)) ||
1043             ((consumer_layer_param.relu_param().engine() == ReLUParameter_Engine_DEFAULT) &&
1044              (consumer_layer_param.engine() == "") &&
1045              (param.engine().compare(0, 6, "MKLDNN") == 0 &&
1046               param.engine().find(":DLA", 6) == string::npos)))) {
1047          string& convolution_top_blob_name =
1048              const_cast<string&>(layer_param->top(0));
1049          float negative_slope =
1050                    consumer_layer_param.relu_param().negative_slope();
1051          layer_param->mutable_inner_product_param()->set_relu(true);
1052          layer_param->mutable_inner_product_param()->set_negative_slope(negative_slope);
1053          if(param.state().phase() == TEST) {
1054            const string& scale_top_blob_name = consumer_layer_param.top(0);
1055            layers_to_drop.insert(consumer_layer_param.name());
1056            convolution_top_blob_name.resize(scale_top_blob_name.size());
1057            convolution_top_blob_name.replace(0,
1058                                            scale_top_blob_name.size(),
1059                                            scale_top_blob_name);
1060          }
1061          if(param.state().phase() == TRAIN) {
1062            if(i+1 < param.layer_size()) {
1063              LayerParameter* relu_layer_param =
1064                (const_cast<NetParameter&>(param)).mutable_layer(i+1);
1065              relu_layer_param->mutable_relu_param()->set_fuse(true);
1066            }
1067          }
1068        }
1069      }
1070      if(param.state().phase() == TEST) {
1071        if (layers_to_drop.find(layer_param->name()) != layers_to_drop.end()) {
1072          LOG_IF(INFO, Caffe::root_solver()) << "Dropped layer: "
1073                 << layer_param->name() << std::endl;
1074          layer_included = false;
1075          layers_to_drop.erase(layers_to_drop.find(layer_param->name()));
1076        }
1077      }
1078      if (layer_included) {
1079        param_compiled->add_layer()->CopyFrom(*layer_param);
1080      }
1081    }
1082  }
1083  template <typename Dtype>
1084  void Net<Dtype>::GetBlobConsumers(
1085                    std::vector<const LayerParameter*>& consumer_blobs,
1086                    const string& blob_name_to_find,
1087                    const NetParameter& param,
1088                    int layer_id_to_start_traversing_from) {
1089    consumer_blobs.clear();
1090    CHECK_GE(layer_id_to_start_traversing_from, 1);
1091    CHECK_LT(layer_id_to_start_traversing_from, param.layer_size());
1092    for (int i = layer_id_to_start_traversing_from; i < param.layer_size(); ++i) {
1093      for (int j = 0; j < param.layer(i).bottom_size(); ++j) {
1094        if (param.layer(i).bottom(j).compare(blob_name_to_find) == 0) {
1095          consumer_blobs.push_back(&param.layer(i));
1096        }
1097      }
1098    }
1099  }
1100  template <typename Dtype>
1101  void Net<Dtype>::GetBlobProducers(
1102                    std::vector<const LayerParameter*>& producers_blobs,
1103                    const string& blob_name_to_find,
1104                    const NetParameter& param,
1105                    int layer_id_to_start_traversing_from) {
1106    producers_blobs.clear();
1107    CHECK_GE(layer_id_to_start_traversing_from, 1);
1108    CHECK_LT(layer_id_to_start_traversing_from, param.layer_size());
1109    for (int i = layer_id_to_start_traversing_from; i < param.layer_size(); ++i) {
1110      for (int j = 0; j < param.layer(i).top_size(); ++j) {
1111        if (param.layer(i).top(j).compare(blob_name_to_find) == 0) {
1112          producers_blobs.push_back(&param.layer(i));
1113        }
1114      }
1115    }
1116  }
1117  template <typename Dtype>
1118  void Net<Dtype>::ParseNetInplaceStatus(
1119      std::map<string, int>& inplace_blob_name_to_index,
1120      std::map<string, int>& specified_layer_blob_name_to_index,
1121      vector<vector<string>>& specified_layer_input_blob_names,
1122      NetParameter* param, const string& specified_layer_type) {
1123    for (int layer_index = 0; layer_index < param->layer_size(); ++layer_index) {
1124      LayerParameter* layer_param =
1125          (const_cast<NetParameter&>(*param)).mutable_layer(layer_index);
1126      if (!specified_layer_type.empty() &&
1127          layer_param->type().compare(specified_layer_type) != 0 &&
1128          layer_param->bottom_size() == 1 && layer_param->top_size() == 1 &&
1129          layer_param->bottom(0) == layer_param->top(0)) {
1130        inplace_blob_name_to_index[layer_param->bottom(0)] = layer_index;
1131      }
1132      if (!specified_layer_type.empty() &&
1133          layer_param->type().compare(specified_layer_type) == 0) {
1134        vector<string> blob_names;
1135        for (unsigned int blob_index = 0; blob_index < layer_param->bottom_size();
1136             blob_index++) {
1137          specified_layer_blob_name_to_index[layer_param->bottom(blob_index)] =
1138              layer_index;
1139          blob_names.push_back(layer_param->bottom(blob_index));
1140        }
1141        specified_layer_input_blob_names.push_back(blob_names);
1142      }
1143    }
1144  }
1145  template <typename Dtype>
1146  void Net<Dtype>::GetNeedToCancelInplaceLayers(
1147      vector<vector<const LayerParameter*>>& layer_pairs,
1148      std::map<string, int>& specified_layer_blob_name_to_index,
1149      std::map<string, int>& inplace_blob_name_to_index,
1150      vector<string>& each_blob_list, const NetParameter& param) {
1151    if (param.engine().compare("MKLDNN") != 0 || each_blob_list.size() == 1)
1152      return;
1153    layer_pairs.clear();
1154    vector<const LayerParameter*> each_layer_pair;
1155    each_blob_list.erase(each_blob_list.begin());
1156    for (auto blob_name : each_blob_list) {
1157      each_layer_pair.clear();
1158      if (inplace_blob_name_to_index.find(blob_name) ==
1159              inplace_blob_name_to_index.end() ||
1160          specified_layer_blob_name_to_index.find(blob_name) ==
1161              specified_layer_blob_name_to_index.end()) {
1162        continue;
1163      }
1164      LayerParameter* bottom_layer =
1165          (const_cast<NetParameter&>(param))
1166              .mutable_layer(inplace_blob_name_to_index[blob_name]);
1167      LayerParameter* top_layer =
1168          (const_cast<NetParameter&>(param))
1169              .mutable_layer(specified_layer_blob_name_to_index[blob_name]);
1170      each_layer_pair.push_back(bottom_layer);
1171      each_layer_pair.push_back(top_layer);
1172      layer_pairs.push_back(each_layer_pair);
1173    }
1174  }
1175  template <typename Dtype>
1176  bool Net<Dtype>::StateMeetsRule(const NetState& state,
1177      const NetStateRule& rule, const string& layer_name) {
1178    if (rule.has_phase()) {
1179        if (rule.phase() != state.phase()) {
1180          LOG_IF(INFO, Caffe::root_solver())
1181              << "The NetState phase (" << state.phase()
1182              << ") differed from the phase (" << rule.phase()
1183              << ") specified by a rule in layer " << layer_name;
1184          return false;
1185        }
1186    }
1187    if (rule.has_min_level()) {
1188      if (state.level() < rule.min_level()) {
1189        LOG_IF(INFO, Caffe::root_solver())
<span onclick='openModal()' class='match'>1190            << "The NetState level (" << state.level()
1191            << ") is above the min_level (" << rule.min_level()
1192            << ") specified by a rule in layer " << layer_name;
</span>1193        return false;
1194      }
1195    }
1196    if (rule.has_max_level()) {
1197      if (state.level() > rule.max_level()) {
1198        LOG_IF(INFO, Caffe::root_solver())
1199            << "The NetState level (" << state.level()
1200            << ") is above the max_level (" << rule.max_level()
1201            << ") specified by a rule in layer " << layer_name;
1202        return false;
1203      }
1204    }
1205    for (int i = 0; i < rule.stage_size(); ++i) {
1206      bool has_stage = false;
1207      for (int j = 0; !has_stage && j < state.stage_size(); ++j) {
1208        if (rule.stage(i) == state.stage(j)) { has_stage = true; }
1209      }
1210      if (!has_stage) {
1211        LOG_IF(INFO, Caffe::root_solver())
1212            << "The NetState did not contain stage '" << rule.stage(i)
1213            << "' specified by a rule in layer " << layer_name;
1214        return false;
1215      }
1216    }
1217    for (int i = 0; i < rule.not_stage_size(); ++i) {
1218      bool has_stage = false;
1219      for (int j = 0; !has_stage && j < state.stage_size(); ++j) {
1220        if (rule.not_stage(i) == state.stage(j)) { has_stage = true; }
1221      }
1222      if (has_stage) {
1223        LOG_IF(INFO, Caffe::root_solver())
1224            << "The NetState contained a not_stage '" << rule.not_stage(i)
1225            << "' specified by a rule in layer " << layer_name;
1226        return false;
1227      }
1228    }
1229    return true;
1230  }
1231  template <typename Dtype>
1232  vector<Dtype> Net<Dtype>::FindMax(Blob<Dtype>* blob, bool is_single) {
1233    const Dtype* data = blob->cpu_data();
1234    int cnt = blob->count();
1235    vector<Dtype> max_vals;
1236    Dtype max_val = (Dtype)(-10);
1237    int index = 0;
1238    if(blob->shape().size() == 4) {
1239      if(is_single) {
1240        max_vals = vector<Dtype>(1, Dtype(-10));
1241        for (int i = 0; i < cnt; ++i) {
1242          max_val = std::max(max_val, (Dtype)fabs(data[i]));
1243        }
1244        max_vals.at(0) = max_val;
1245      } else { 
1246        int height = blob->shape(2);
1247        int width = blob->shape(3);
1248        int channel = blob->shape(0);
1249        max_vals = vector<Dtype>(channel, Dtype(-10));
1250        int step = blob->shape(1) * height * width;
1251        for (int i = 0; i < cnt; ++i) {
1252          if((i + 1) % step == 0) {
1253            max_vals.at(index) = std::max(max_val, (Dtype)fabs(data[i]));
1254            ++index;
1255            max_val = (Dtype)(-10);
1256          } else {
1257            max_val = std::max(max_val, (Dtype)fabs(data[i]));
1258          }
1259        }
1260      }
1261    } else {
1262      if(is_single) {
1263        max_vals = vector<Dtype>(1, Dtype(-10));
1264        for (int i = 0; i < cnt; ++i) {
1265          max_val = std::max(max_val, (Dtype)fabs(data[i]));
1266        }
1267        max_vals.at(0) = max_val;
1268      } else { 
1269        int channel = blob->shape(0);
1270        max_vals = vector<Dtype>(channel, Dtype(-10));
1271        int step = blob->shape(1);
1272        for (int i = 0; i < cnt; ++i) {
1273          if((i + 1) % step == 0) {
1274            max_vals.at(index) = std::max(max_val, (Dtype)fabs(data[i]));
1275            ++index;
1276            max_val = (Dtype)(-10);
1277          } else {
1278            max_val = std::max(max_val, (Dtype)fabs(data[i]));
1279          }
1280        }
1281      }
1282    }
1283    return max_vals;
1284  }
1285  template <typename Dtype>
1286  void Net<Dtype>::RangeInLayers(vector<string>* layer_name,
1287        vector<Dtype>* max_in, vector<Dtype>* max_out, vector<vector<Dtype>>* max_param, string scaling) {
1288    if(layer_name->size()==0) {
1289      for (int layer_id = 0; layer_id < layers_.size(); ++layer_id) {
1290        if (strcmp(layers_[layer_id]->type(), "Convolution") == 0) {
1291          layer_name->push_back(this->layer_names()[layer_id]);
1292          max_in->push_back(0);
1293          max_out->push_back(0);
1294          if (scaling == "single") {
1295            max_param->push_back(vector<Dtype>(1, 0));
1296          }
1297          else {
1298            int param_shape = (&(*layers_[layer_id]->blobs()[0]))->shape(0);
1299            max_param->push_back(vector<Dtype>(param_shape, 0));
1300          }
1301        }
1302      }
1303    }
1304    int index = 0;
1305    vector<Dtype> max_vals;
1306    for (int layer_id = 0; layer_id < layers_.size(); ++layer_id) {
1307      if (strcmp(layers_[layer_id]->type(), "Convolution") == 0) {
1308        max_vals = FindMax(bottom_vecs_[layer_id][0]);
1309        max_in->at(index) = std::max(max_in->at(index), max_vals.at(0)); 
1310        max_vals = FindMax(top_vecs_[layer_id][0]);
1311        max_out->at(index) = std::max(max_out->at(index), max_vals.at(0));
1312        if (scaling == "single") {
1313          max_vals = FindMax(&(*layers_[layer_id]->blobs()[0]));
1314          max_param->at(index).at(0) = std::max(max_param->at(index).at(0), max_vals.at(0));
1315        } else {
1316          max_vals = FindMax(&(*layers_[layer_id]->blobs()[0]), false);
1317          for(int i = 0; i < max_vals.size(); ++i) 
1318            max_param->at(index).at(i) = std::max(max_param->at(index).at(i), max_vals.at(i));
1319        }
1320        index++;
1321      }
1322    }
1323  }
1324  template <typename Dtype>
1325  void Net<Dtype>::AppendTop(const NetParameter& param, const int layer_id,
1326                             const int top_id, set<string>* available_blobs,
1327                             map<string, int>* blob_name_to_idx) {
1328    shared_ptr<LayerParameter> layer_param(
1329        new LayerParameter(param.layer(layer_id)));
1330    const string& blob_name = (layer_param->top_size() > top_id) ?
1331        layer_param->top(top_id) : "(automatic)";
1332    if (blob_name_to_idx && layer_param->bottom_size() > top_id &&
1333        blob_name == layer_param->bottom(top_id)) {
1334      LOG_IF(INFO, Caffe::root_solver())
1335          << layer_param->name() << " -> " << blob_name << " (in-place)";
1336      top_vecs_[layer_id].push_back(blobs_[(*blob_name_to_idx)[blob_name]].get());
1337      top_id_vecs_[layer_id].push_back((*blob_name_to_idx)[blob_name]);
1338    } else if (blob_name_to_idx &&
1339               blob_name_to_idx->find(blob_name) != blob_name_to_idx->end()) {
1340      LOG(FATAL) << "Top blob '" << blob_name
1341                 << "' produced by multiple sources.";
1342    } else {
1343      if (Caffe::root_solver()) {
1344        LOG(INFO) << layer_param->name() << " -> " << blob_name;
1345      }
1346      shared_ptr<Blob<Dtype> > blob_pointer(new Blob<Dtype>());
1347      const int blob_id = blobs_.size();
1348      blobs_.push_back(blob_pointer);
1349      blob_names_.push_back(blob_name);
1350      blob_need_backward_.push_back(false);
1351      if (blob_name_to_idx) { (*blob_name_to_idx)[blob_name] = blob_id; }
1352      top_id_vecs_[layer_id].push_back(blob_id);
1353      top_vecs_[layer_id].push_back(blob_pointer.get());
1354    }
1355    if (available_blobs) { available_blobs->insert(blob_name); }
1356  }
1357  template <typename Dtype>
1358  int Net<Dtype>::AppendBottom(const NetParameter& param, const int layer_id,
1359      const int bottom_id, set<string>* available_blobs,
1360      map<string, int>* blob_name_to_idx) {
1361    const LayerParameter& layer_param = param.layer(layer_id);
1362    const string& blob_name = layer_param.bottom(bottom_id);
1363    if (available_blobs->find(blob_name) == available_blobs->end()) {
1364      LOG(FATAL) << "Unknown bottom blob '" << blob_name << "' (layer '"
1365                 << layer_param.name() << "', bottom index " << bottom_id << ")";
1366    }
1367    const int blob_id = (*blob_name_to_idx)[blob_name];
1368    LOG_IF(INFO, Caffe::root_solver())
1369        << layer_names_[layer_id] << " <- " << blob_name;
1370    bottom_vecs_[layer_id].push_back(blobs_[blob_id].get());
1371    bottom_id_vecs_[layer_id].push_back(blob_id);
1372    available_blobs->erase(blob_name);
1373    bool need_backward = blob_need_backward_[blob_id];
1374    if (layer_param.propagate_down_size() > 0) {
1375      need_backward = layer_param.propagate_down(bottom_id);
1376    }
1377    bottom_need_backward_[layer_id].push_back(need_backward);
1378    return blob_id;
1379  }
1380  template <typename Dtype>
1381  void Net<Dtype>::AppendParam(const NetParameter& param, const int layer_id,
1382                               const int param_id) {
1383    const LayerParameter& layer_param = layers_[layer_id]->layer_param();
1384    const int param_size = layer_param.param_size();
1385    string param_name =
1386        (param_size > param_id) ? layer_param.param(param_id).name() : "";
1387    if (param_name.size()) {
1388      param_display_names_.push_back(param_name);
1389    } else {
1390      ostringstream param_display_name;
1391      param_display_name << param_id;
1392      param_display_names_.push_back(param_display_name.str());
1393    }
1394    const int net_param_id = params_.size();
1395    params_.push_back(layers_[layer_id]->blobs()[param_id]);
1396    param_id_vecs_[layer_id].push_back(net_param_id);
1397    param_layer_indices_.push_back(make_pair(layer_id, param_id));
1398    ParamSpec default_param_spec;
1399    const ParamSpec* param_spec = (layer_param.param_size() > param_id) ?
1400        &layer_param.param(param_id) : &default_param_spec;
1401    if (!param_size || !param_name.size() || (param_name.size() &&
1402        param_names_index_.find(param_name) == param_names_index_.end())) {
1403      param_owners_.push_back(-1);
1404      if (param_name.size()) {
1405        param_names_index_[param_name] = net_param_id;
1406      }
1407      const int learnable_param_id = learnable_params_.size();
1408      learnable_params_.push_back(params_[net_param_id].get());
1409      learnable_param_ids_.push_back(learnable_param_id);
1410      has_params_lr_.push_back(param_spec->has_lr_mult());
1411      has_params_decay_.push_back(param_spec->has_decay_mult());
1412      params_lr_.push_back(param_spec->lr_mult());
1413      params_weight_decay_.push_back(param_spec->decay_mult());
1414    } else {
1415      const int owner_net_param_id = param_names_index_[param_name];
1416      param_owners_.push_back(owner_net_param_id);
1417      const pair<int, int>& owner_index =
1418          param_layer_indices_[owner_net_param_id];
1419      const int owner_layer_id = owner_index.first;
1420      const int owner_param_id = owner_index.second;
1421      LOG_IF(INFO, Caffe::root_solver()) << "Sharing parameters '" << param_name
1422          << "' owned by "
1423          << "layer '" << layer_names_[owner_layer_id] << "', param "
1424          << "index " << owner_param_id;
1425      Blob<Dtype>* this_blob = layers_[layer_id]->blobs()[param_id].get();
1426      Blob<Dtype>* owner_blob =
1427          layers_[owner_layer_id]->blobs()[owner_param_id].get();
1428      const int param_size = layer_param.param_size();
1429      if (param_size > param_id && (layer_param.param(param_id).share_mode() ==
1430                                    ParamSpec_DimCheckMode_PERMISSIVE)) {
1431        CHECK_EQ(this_blob->count(), owner_blob->count())
1432            << "Cannot share param '" << param_name << "' owned by layer '"
1433            << layer_names_[owner_layer_id] << "' with layer '"
1434            << layer_names_[layer_id] << "'; count mismatch.  Owner layer param "
1435            << "shape is " << owner_blob->shape_string() << "; sharing layer "
1436            << "shape is " << this_blob->shape_string();
1437      } else {
1438        CHECK(this_blob->shape() == owner_blob->shape())
1439            << "Cannot share param '" << param_name << "' owned by layer '"
1440            << layer_names_[owner_layer_id] << "' with layer '"
1441            << layer_names_[layer_id] << "'; shape mismatch.  Owner layer param "
1442            << "shape is " << owner_blob->shape_string() << "; sharing layer "
1443            << "expects shape " << this_blob->shape_string();
1444      }
1445      const int learnable_param_id = learnable_param_ids_[owner_net_param_id];
1446      learnable_param_ids_.push_back(learnable_param_id);
1447      if (param_spec->has_lr_mult()) {
1448        if (has_params_lr_[learnable_param_id]) {
1449          CHECK_EQ(param_spec->lr_mult(), params_lr_[learnable_param_id])
1450              << "Shared param '" << param_name << "' has mismatched lr_mult.";
1451        } else {
1452          has_params_lr_[learnable_param_id] = true;
1453          params_lr_[learnable_param_id] = param_spec->lr_mult();
1454        }
1455      }
1456      if (param_spec->has_decay_mult()) {
1457        if (has_params_decay_[learnable_param_id]) {
1458          CHECK_EQ(param_spec->decay_mult(),
1459                   params_weight_decay_[learnable_param_id])
1460              << "Shared param '" << param_name << "' has mismatched decay_mult.";
1461        } else {
1462          has_params_decay_[learnable_param_id] = true;
1463          params_weight_decay_[learnable_param_id] = param_spec->decay_mult();
1464        }
1465      }
1466    }
1467  }
1468  template <typename Dtype>
1469  Dtype Net<Dtype>::ForwardFromTo(int start, int end) {
1470    CHECK_GE(start, 0);
1471    CHECK_LT(end, layers_.size());
1472    Dtype loss = 0;
1473    for (int i = start; i <= end; ++i) {
1474      LAYER_TIMING_START(forward, i);
1475      PERFORMANCE_MEASUREMENT_BEGIN();
1476      Dtype layer_loss = layers_[i]->Forward(bottom_vecs_[i], top_vecs_[i]);
1477      PERFORMANCE_MEASUREMENT_END((std::string("FW_") + layer_names_[i]).c_str());
1478      LAYER_TIMING_STOP(forward, i);
1479      loss += layer_loss;
1480      if (debug_info_) { ForwardDebugInfo(i); }
1481    }
1482    return loss;
1483  }
1484  template <typename Dtype>
1485  Dtype Net<Dtype>::ForwardFrom(int start) {
1486    return ForwardFromTo(start, layers_.size() - 1);
1487  }
1488  template <typename Dtype>
1489  Dtype Net<Dtype>::ForwardTo(int end) {
1490    return ForwardFromTo(0, end);
1491  }
1492  template <typename Dtype>
1493  const vector<Blob<Dtype>*>& Net<Dtype>::Forward(Dtype* loss) {
1494    if (loss != NULL) {
1495      *loss = ForwardFromTo(0, layers_.size() - 1);
1496    } else {
1497      ForwardFromTo(0, layers_.size() - 1);
1498    }
1499    return net_output_blobs_;
1500  }
1501  template <typename Dtype>
1502  const vector<Blob<Dtype>*>& Net<Dtype>::Forward(
1503      const vector<Blob<Dtype>*> & bottom, Dtype* loss) {
1504    LOG_EVERY_N(WARNING, 1000) << "DEPRECATED: Forward(bottom, loss) "
1505        << "will be removed in a future version. Use Forward(loss).";
1506    for (int i = 0; i < bottom.size(); ++i) {
1507      net_input_blobs_[i]->CopyFrom(*bottom[i]);
1508    }
1509    return Forward(loss);
1510  }
1511  template <typename Dtype>
1512  void Net<Dtype>::BackwardFromTo(int start, int end) {
1513    CHECK_GE(end, 0);
1514    CHECK_LT(start, layers_.size());
1515    for (int i = start; i >= end; --i) {
1516      if (layer_need_backward_[i]) {
1517        LAYER_TIMING_START(backward, i);
1518        PERFORMANCE_MEASUREMENT_BEGIN();
1519        layers_[i]->Backward(
1520            top_vecs_[i], bottom_need_backward_[i], bottom_vecs_[i]);
1521        PERFORMANCE_MEASUREMENT_END((std::string("BW_")+layer_names_[i]).c_str());
1522        LAYER_TIMING_STOP(backward, i);
1523        if (debug_info_) { BackwardDebugInfo(i); }
1524      }
1525    }
1526  }
1527  template <typename Dtype>
1528  void Net<Dtype>::ForwardDebugInfo(const int layer_id) {
1529    for (int top_id = 0; top_id < top_vecs_[layer_id].size(); ++top_id) {
1530      const Blob<Dtype>& blob = *top_vecs_[layer_id][top_id];
1531      const string& blob_name = blob_names_[top_id_vecs_[layer_id][top_id]];
1532      const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
1533      LOG_IF(INFO, Caffe::root_solver())
1534          << "    [Forward] "
1535          << "Layer " << layer_names_[layer_id]
1536          << ", top blob " << blob_name
1537          << " data: " << data_abs_val_mean;
1538    }
1539    for (int param_id = 0; param_id < layers_[layer_id]->blobs().size();
1540         ++param_id) {
1541      const Blob<Dtype>& blob = *layers_[layer_id]->blobs()[param_id];
1542      const int net_param_id = param_id_vecs_[layer_id][param_id];
1543      const string& blob_name = param_display_names_[net_param_id];
1544      const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
1545      LOG_IF(INFO, Caffe::root_solver())
1546          << "    [Forward] "
1547          << "Layer " << layer_names_[layer_id]
1548          << ", param blob " << blob_name
1549          << " data: " << data_abs_val_mean;
1550    }
1551  }
1552  template <typename Dtype>
1553  void Net<Dtype>::BackwardDebugInfo(const int layer_id) {
1554    const vector<Blob<Dtype>*>& bottom_vec = bottom_vecs_[layer_id];
1555    for (int bottom_id = 0; bottom_id < bottom_vec.size(); ++bottom_id) {
1556      if (!bottom_need_backward_[layer_id][bottom_id]) { continue; }
1557      const Blob<Dtype>& blob = *bottom_vec[bottom_id];
1558      const string& blob_name = blob_names_[bottom_id_vecs_[layer_id][bottom_id]];
1559      const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
1560      LOG_IF(INFO, Caffe::root_solver())
1561          << "    [Backward] "
1562          << "Layer " << layer_names_[layer_id]
1563          << ", bottom blob " << blob_name
1564          << " diff: " << diff_abs_val_mean;
1565    }
1566    for (int param_id = 0; param_id < layers_[layer_id]->blobs().size();
1567         ++param_id) {
1568      if (!layers_[layer_id]->param_propagate_down(param_id)) { continue; }
1569      const Blob<Dtype>& blob = *layers_[layer_id]->blobs()[param_id];
1570      const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
1571      LOG_IF(INFO, Caffe::root_solver())
1572          << "    [Backward] "
1573          << "Layer " << layer_names_[layer_id]
1574          << ", param blob " << param_id
1575          << " diff: " << diff_abs_val_mean;
1576    }
1577  }
1578  template <typename Dtype>
1579  void Net<Dtype>::UpdateDebugInfo(const int param_id) {
1580    const Blob<Dtype>& blob = *params_[param_id];
1581    const int param_owner = param_owners_[param_id];
1582    const string& layer_name = layer_names_[param_layer_indices_[param_id].first];
1583    const string& param_display_name = param_display_names_[param_id];
1584    const Dtype diff_abs_val_mean = blob.asum_diff() / blob.count();
1585    if (param_owner < 0) {
1586      const Dtype data_abs_val_mean = blob.asum_data() / blob.count();
1587      LOG_IF(INFO, Caffe::root_solver())
1588          << "    [Update] Layer " << layer_name
1589          << ", param " << param_display_name
1590          << " data: " << data_abs_val_mean
1591          << "; diff: " << diff_abs_val_mean;
1592    } else {
1593      const string& owner_layer_name =
1594          layer_names_[param_layer_indices_[param_owner].first];
1595      LOG_IF(INFO, Caffe::root_solver())
1596          << "    [Update] Layer " << layer_name
1597          << ", param blob " << param_display_name
1598          << " (owned by layer " << owner_layer_name << ", " << "param "
1599          << param_display_names_[param_owners_[param_id]] << ")"
1600          << " diff: " << diff_abs_val_mean;
1601    }
1602  }
1603  template <typename Dtype>
1604  void Net<Dtype>::ShareTrainedLayersWith(const Net* other) {
1605      if (this->bn_scale_remove_) {
1606      NetParameter temp_net_param;
1607      NetParameter complete_net_param;
1608      other->ToProto(&temp_net_param, false);
1609      for (vector<string>::iterator it = kept_bn_layers_.begin(); it != kept_bn_layers_.end(); it++) {
1610        temp_net_param.mutable_compile_net_state()->add_kept_bn_layers(*it);
1611      }
1612      complete_net_param.CopyFrom(temp_net_param);
1613      if (other->bn_scale_merge_) {
1614        complete_net_param.clear_layer();
1615        RecoverBNScaleMergedNet<Dtype>(&temp_net_param, &complete_net_param);
1616      }
1617      CopyTrainedLayersFrom(complete_net_param);
1618      return ;
1619    }
1620    int num_source_layers = other->layers().size();
1621    for (int i = 0; i < num_source_layers; ++i) {
1622      Layer<Dtype>* source_layer = other->layers()[i].get();
1623      const string& source_layer_name = other->layer_names()[i];
1624      int target_layer_id = 0;
1625      while (target_layer_id != layer_names_.size() &&
1626          layer_names_[target_layer_id] != source_layer_name) {
1627        ++target_layer_id;
1628      }
1629      if (target_layer_id == layer_names_.size()) {
1630        LOG(INFO) << "Ignoring source layer " << source_layer_name;
1631        continue;
1632      }
1633      DLOG(INFO) << "Copying source layer " << source_layer_name;
1634      vector<shared_ptr<Blob<Dtype> > >& target_blobs =
1635          layers_[target_layer_id]->blobs();
1636      CHECK_EQ(target_blobs.size(), source_layer->blobs().size())
1637          << "Incompatible number of blobs for layer " << source_layer_name;
1638      for (int j = 0; j < target_blobs.size(); ++j) {
1639        Blob<Dtype>* source_blob = source_layer->blobs()[j].get();
1640        CHECK(target_blobs[j]->shape() == source_blob->shape())
1641            << "Cannot share param " << j << " weights from layer '"
1642            << source_layer_name << "'; shape mismatch.  Source param shape is "
1643            << source_blob->shape_string() << "; target param shape is "
1644            << target_blobs[j]->shape_string();
1645        target_blobs[j]->ShareData(*source_blob);
1646      }
1647    }
1648  }
1649  template <typename Dtype>
1650  void Net<Dtype>::BackwardFrom(int start) {
1651    BackwardFromTo(start, 0);
1652  }
1653  template <typename Dtype>
1654  void Net<Dtype>::BackwardTo(int end) {
1655    BackwardFromTo(layers_.size() - 1, end);
1656  }
1657  template <typename Dtype>
1658  void Net<Dtype>::Backward() {
1659    BackwardFromTo(layers_.size() - 1, 0);
1660    if (debug_info_) {
1661      Dtype asum_data = 0, asum_diff = 0, sumsq_data = 0, sumsq_diff = 0;
1662      for (int i = 0; i < learnable_params_.size(); ++i) {
1663        asum_data += learnable_params_[i]->asum_data();
1664        asum_diff += learnable_params_[i]->asum_diff();
1665        sumsq_data += learnable_params_[i]->sumsq_data();
1666        sumsq_diff += learnable_params_[i]->sumsq_diff();
1667      }
1668      const Dtype l2norm_data = std::sqrt(sumsq_data);
1669      const Dtype l2norm_diff = std::sqrt(sumsq_diff);
1670      LOG(ERROR) << "    [Backward] All net params (data, diff): "
1671                 << "L1 norm = (" << asum_data << ", " << asum_diff << "); "
1672                 << "L2 norm = (" << l2norm_data << ", " << l2norm_diff << ")";
1673    }
1674  }
1675  template <typename Dtype>
1676  void Net<Dtype>::Reshape() {
1677    for (int i = 0; i < layers_.size(); ++i) {
1678      layers_[i]->Reshape(bottom_vecs_[i], top_vecs_[i]);
1679    }
1680  }
1681  template <typename Dtype>
1682  void Net<Dtype>::CopyTrainedLayersFrom(const NetParameter& param_inp) {
1683    NetParameter param_tmp = param_inp;
1684    NetParameter &param = param_tmp;
1685    param.set_engine(engine_name_);
1686    param_tmp.mutable_state()->set_phase(phase_);
1687    param_tmp.mutable_compile_net_state()->set_is_init(false);
1688    for (vector<string>::iterator it = this->kept_bn_layers_.begin(); it != this->kept_bn_layers_.end(); it++) {
1689      param_tmp.mutable_compile_net_state()->add_kept_bn_layers(*it);
1690    }
1691    int num_source_layers = param.layer_size();
1692    for (int i = 0; i < num_source_layers; ++i) {
1693      LayerParameter* source_layer = param.mutable_layer(i);
1694      const string& source_layer_name = source_layer->name();
1695      int target_layer_id = 0;
1696      while (target_layer_id != layer_names_.size() &&
1697          layer_names_[target_layer_id] != source_layer_name) {
1698        ++target_layer_id;
1699      }
1700      if (target_layer_id == layer_names_.size()) {
1701        continue;
1702      }
1703      const LayerParameter& layer_param = layers_[target_layer_id]->layer_param();
1704      const string& engine_name = layer_param.engine();
1705      source_layer->set_engine(engine_name);
1706      if ((layer_param.type().compare("BatchNorm") == 0) &&
1707          (layer_param.batch_norm_param().has_engine())) {
1708        source_layer->mutable_batch_norm_param()->set_engine(layer_param.batch_norm_param().engine());
1709      }
1710    }
1711    NetParameter param_compiled;
1712    CompileNet(param, &param_compiled);
1713    param = param_compiled;
1714  #ifdef USE_MLSL
1715    NetParameter param_mn;
1716    if (mn::is_multinode()) {
1717      CopyMultinodeParamsFromNet<Dtype>(this, &param);
1718      ApplyMultinodeParams<Dtype>(param, &param_mn);
1719      param = param_mn;
1720    }
1721  #endif
1722    num_source_layers = param.layer_size();
1723    for (int i = 0; i < num_source_layers; ++i) {
1724      const LayerParameter& source_layer = param.layer(i);
1725      const string& source_layer_name = source_layer.name();
1726      int target_layer_id = 0;
1727      while (target_layer_id != layer_names_.size() &&
1728          layer_names_[target_layer_id] != source_layer_name) {
1729        ++target_layer_id;
1730      }
1731      if (target_layer_id == layer_names_.size()) {
1732        LOG(INFO) << "Ignoring source layer " << source_layer_name;
1733        continue;
1734      }
1735      DLOG(INFO) << "Copying source layer " << source_layer_name;
1736      vector<shared_ptr<Blob<Dtype> > >& target_blobs =
1737          layers_[target_layer_id]->blobs();
1738      CHECK_EQ(target_blobs.size(), source_layer.blobs_size())
1739          << "Incompatible number of blobs for layer " << source_layer_name;
1740      for (int j = 0; j < target_blobs.size(); ++j) {
1741        if (!target_blobs[j]->ShapeEquals(source_layer.blobs(j))) {
1742          Blob<Dtype> source_blob;
1743          const bool kReshape = true;
1744          source_blob.FromProto(source_layer.blobs(j), kReshape);
1745          LOG(FATAL) << "Cannot copy param " << j << " weights from layer '"
1746              << source_layer_name << "'; shape mismatch.  Source param shape is "
1747              << source_blob.shape_string() << "; target param shape is "
1748              << target_blobs[j]->shape_string() << ". "
1749              << "To learn this layer's parameters from scratch rather than "
1750              << "copying from a saved net, rename the layer.";
1751        }
1752        const bool kReshape = false;
1753        target_blobs[j]->FromProto(source_layer.blobs(j), kReshape);
1754      }
1755    }
1756  }
1757  template <typename Dtype>
1758  void Net<Dtype>::CopyTrainedLayersFrom(const string trained_filename) {
1759    if (trained_filename.size() >= 3 &&
1760        trained_filename.compare(trained_filename.size() - 3, 3, ".h5") == 0) {
1761      CopyTrainedLayersFromHDF5(trained_filename);
1762    } else {
1763      CopyTrainedLayersFromBinaryProto(trained_filename);
1764    }
1765  }
1766  template <typename Dtype>
1767  void Net<Dtype>::CopyTrainedLayersFromBinaryProto(
1768      const string trained_filename) {
1769    NetParameter param;
1770    ReadNetParamsFromBinaryFileOrDie(trained_filename, &param);
1771    CopyTrainedLayersFrom(param);
1772  }
1773  template <typename Dtype>
1774  void Net<Dtype>::CopyTrainedLayersFromHDF5(const string trained_filename) {
1775    hid_t file_hid = H5Fopen(trained_filename.c_str(), H5F_ACC_RDONLY,
1776                             H5P_DEFAULT);
1777    CHECK_GE(file_hid, 0) << "Couldn't open " << trained_filename;
1778    hid_t data_hid = H5Gopen2(file_hid, "data", H5P_DEFAULT);
1779    CHECK_GE(data_hid, 0) << "Error reading weights from " << trained_filename;
1780    int num_layers = hdf5_get_num_links(data_hid);
1781    for (int i = 0; i < num_layers; ++i) {
1782      string source_layer_name = hdf5_get_name_by_idx(data_hid, i);
1783      if (!layer_names_index_.count(source_layer_name)) {
1784        LOG(INFO) << "Ignoring source layer " << source_layer_name;
1785        continue;
1786      }
1787      int target_layer_id = layer_names_index_[source_layer_name];
1788      DLOG(INFO) << "Copying source layer " << source_layer_name;
1789      vector<shared_ptr<Blob<Dtype> > >& target_blobs =
1790          layers_[target_layer_id]->blobs();
1791      hid_t layer_hid = H5Gopen2(data_hid, source_layer_name.c_str(),
1792          H5P_DEFAULT);
1793      CHECK_GE(layer_hid, 0)
1794          << "Error reading weights from " << trained_filename;
1795      int num_source_params = hdf5_get_num_links(layer_hid);
1796      CHECK_LE(num_source_params, target_blobs.size())
1797          << "Incompatible number of blobs for layer " << source_layer_name;
1798      for (int j = 0; j < target_blobs.size(); ++j) {
1799        ostringstream oss;
1800        oss << j;
1801        string dataset_name = oss.str();
1802        int target_net_param_id = param_id_vecs_[target_layer_id][j];
1803        if (!H5Lexists(layer_hid, dataset_name.c_str(), H5P_DEFAULT)) {
1804          if (param_owners_[target_net_param_id] != -1) {
1805            continue;
1806          } else {
1807            LOG(FATAL) << "Incompatible number of blobs for layer "
1808                << source_layer_name;
1809          }
1810        }
1811  #ifdef USE_MLSL
1812        const MultinodeLayerParameter &mn_layer_param =
1813          layers_[target_layer_id]->layer_param().multinode();
1814        int num_nodes = mn_layer_param.num_nodes();
1815        int model_parts = mn_layer_param.model_parts();
1816        mn::GetCanonicalMnParam(num_nodes, model_parts);
1817        Blob<Dtype> orig_blob;
1818        vector<int> shape = target_blobs[j]->shape();
1819        CHECK_GT(shape.size(), 0);
1820        int offset = 0;
1821        if (model_parts > 1) {
1822          shape[0] *= model_parts;
1823          offset = target_blobs[j]->count() * (mn::get_node_id() % model_parts);
1824        }
1825        orig_blob.Reshape(shape);
1826        hdf5_load_nd_dataset(layer_hid, dataset_name.c_str(), 0, kMaxBlobAxes,
1827            &orig_blob);
1828        caffe_copy(target_blobs[j]->count(), orig_blob.cpu_data() + offset,
1829                   target_blobs[j]->mutable_cpu_data());
1830  #else
1831        hdf5_load_nd_dataset(layer_hid, dataset_name.c_str(), 0, kMaxBlobAxes,
1832            target_blobs[j].get());
1833  #endif
1834      }
1835      H5Gclose(layer_hid);
1836    }
1837    H5Gclose(data_hid);
1838    H5Fclose(file_hid);
1839  }
1840  template <typename Dtype>
1841  void Net<Dtype>::ToProto(NetParameter* param, bool write_diff) const {
1842    param->Clear();
1843    param->set_name(name_);
1844    DLOG(INFO) << "Serializing " << layers_.size() << " layers";
1845    for (int i = 0; i < layers_.size(); ++i) {
1846      LayerParameter* layer_param = param->add_layer();
1847      layers_[i]->ToProto(layer_param, write_diff);
1848    }
1849  #ifdef USE_MLSL
1850    if (mn::is_multinode()) {
1851      RevertMultinodeParams<Dtype>(param, write_diff);
1852    }
1853  #endif
1854  }
1855  template <typename Dtype>
1856  void Net<Dtype>::ToHDF5(const string& filename, bool write_diff) const {
1857    hid_t file_hid = -1;
1858    hid_t data_hid = -1;
1859    hid_t diff_hid = -1;
1860  #ifdef USE_MLSL
1861    if (mn::is_root()) {
1862  #endif
1863    file_hid = H5Fcreate(filename.c_str(), H5F_ACC_TRUNC, H5P_DEFAULT,
1864        H5P_DEFAULT);
1865    CHECK_GE(file_hid, 0)
1866        << "Couldn't open " << filename << " to save weights.";
1867    data_hid = H5Gcreate2(file_hid, "data", H5P_DEFAULT, H5P_DEFAULT,
1868        H5P_DEFAULT);
1869    CHECK_GE(data_hid, 0) << "Error saving weights to " << filename << ".";
1870    if (write_diff) {
1871      diff_hid = H5Gcreate2(file_hid, "diff", H5P_DEFAULT, H5P_DEFAULT,
1872          H5P_DEFAULT);
1873      CHECK_GE(diff_hid, 0) << "Error saving weights to " << filename << ".";
1874    }
1875  #ifdef USE_MLSL
1876    }
1877  #endif
1878    for (int layer_id = 0; layer_id < layers_.size(); ++layer_id) {
1879      const LayerParameter& layer_param = layers_[layer_id]->layer_param();
1880  #ifdef USE_MLSL
1881      if (layer_param.type() == "MnActivation") continue;
1882  #endif
1883      hid_t layer_data_hid = -1;
1884      hid_t layer_diff_hid = -1;
1885  #ifdef USE_MLSL
1886      if (mn::is_root()) {
1887  #endif
1888        string layer_name = layer_param.name();
1889        layer_data_hid = H5Gcreate2(data_hid, layer_name.c_str(),
1890            H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
1891        CHECK_GE(layer_data_hid, 0)
1892          << "Error saving weights to " << filename << ".";
1893        if (write_diff) {
1894          layer_diff_hid = H5Gcreate2(diff_hid, layer_name.c_str(),
1895              H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
1896          CHECK_GE(layer_diff_hid, 0)
1897            << "Error saving weights to " << filename << ".";
1898        }
1899  #ifdef USE_MLSL
1900      }
1901  #endif
1902      int num_params = layers_[layer_id]->blobs().size();
1903      for (int param_id = 0; param_id < num_params; ++param_id) {
1904        ostringstream dataset_name;
1905        dataset_name << param_id;
1906        const int net_param_id = param_id_vecs_[layer_id][param_id];
1907  #ifdef USE_MLSL
1908        const MultinodeLayerParameter &mn_layer_param = layer_param.multinode();
1909        int num_nodes = mn_layer_param.num_nodes();
1910        int model_parts = mn_layer_param.model_parts();
1911        mn::GetCanonicalMnParam(num_nodes, model_parts);
1912        Blob<Dtype> new_blob;
1913        vector<int> shape = params_[net_param_id]->shape();
1914        CHECK_GT(shape.size(), 0);
1915        if (model_parts > 1) {
1916          mn::Distribution *distrib = mn::get_distrib(num_nodes/model_parts, model_parts);
1917          shape[0] *= model_parts;
1918          new_blob.Reshape(shape);
1919          distrib->allgather<Dtype,MLSL::GT_MODEL>(
1920            params_[net_param_id]->mutable_cpu_data(),
1921            params_[net_param_id]->count(),
1922            new_blob.mutable_cpu_data());
1923          if (write_diff) {
1924            distrib->allgather<Dtype,MLSL::GT_MODEL>(
1925              params_[net_param_id]->mutable_cpu_diff(),
1926              params_[net_param_id]->count(),
1927              new_blob.mutable_cpu_diff());
1928          }
1929        } else {
1930          new_blob.Reshape(shape);
1931          caffe_copy(new_blob.count(), params_[net_param_id]->cpu_data(),
1932                     new_blob.mutable_cpu_data());
1933          if (write_diff) {
1934            caffe_copy(new_blob.count(), params_[net_param_id]->cpu_diff(),
1935                       new_blob.mutable_cpu_diff());
1936          }
1937        }
1938        if (mn::is_root()) {
1939          if (param_owners_[net_param_id] == -1) {
1940            hdf5_save_nd_dataset<Dtype>(layer_data_hid, dataset_name.str(),
1941                new_blob);
1942          }
1943          if (write_diff) {
1944            hdf5_save_nd_dataset<Dtype>(layer_diff_hid, dataset_name.str(),
1945                new_blob, true);
1946          }
1947        }
1948  #else
1949        if (param_owners_[net_param_id] == -1) {
1950          hdf5_save_nd_dataset<Dtype>(layer_data_hid, dataset_name.str(),
1951              *params_[net_param_id]);
1952        }
1953        if (write_diff) {
1954          hdf5_save_nd_dataset<Dtype>(layer_diff_hid, dataset_name.str(),
1955              *params_[net_param_id], true);
1956        }
1957  #endif
1958      }
1959  #ifdef USE_MLSL
1960      if (mn::is_root()) {
1961  #endif
1962      H5Gclose(layer_data_hid);
1963      if (write_diff) {
1964        H5Gclose(layer_diff_hid);
1965      }
1966  #ifdef USE_MLSL
1967      }
1968  #endif
1969    }
1970  #ifdef USE_MLSL
1971    if (mn::is_root()) {
1972  #endif
1973    H5Gclose(data_hid);
1974    if (write_diff) {
1975      H5Gclose(diff_hid);
1976    }
1977    H5Fclose(file_hid);
1978  #ifdef USE_MLSL
1979    }
1980  #endif
1981  }
1982  template <typename Dtype>
1983  void Net<Dtype>::Update() {
1984    for (int i = 0; i < learnable_params_.size(); ++i) {
1985      learnable_params_[i]->Update();
1986    }
1987  }
1988  template <typename Dtype>
1989  void Net<Dtype>::ClearParamDiffs(int learnable_param_id) {
1990    Blob<Dtype>* blob = learnable_params_[learnable_param_id];
1991    switch (Caffe::mode()) {
1992    case Caffe::CPU:
1993        if (blob->prv_diff())
1994          caffe_set(blob->prv_diff_count(), static_cast<Dtype>(0),
1995                    blob->mutable_prv_diff());
1996        else
1997          caffe_set(blob->count(), static_cast<Dtype>(0),
1998                    blob->mutable_cpu_diff());
1999      break;
2000    case Caffe::GPU:
2001  #ifndef CPU_ONLY
2002      caffe_gpu_set(blob->count(), static_cast<Dtype>(0),
2003                    blob->mutable_gpu_diff());
2004  #else
2005      NO_GPU;
2006  #endif
2007      break;
2008    }
2009  }
2010  template <typename Dtype>
2011  void Net<Dtype>::ClearParamDiffs() {
2012    ITER_TIMING_START();
2013    for (int i = 0; i < learnable_params_.size(); ++i) {
2014      ClearParamDiffs(i);
2015    }
2016    ITER_TIMING_STOP(cleardiffs);
2017  }
2018  template <typename Dtype>
2019  void Net<Dtype>::ShareWeights() {
2020    for (int i = 0; i < params_.size(); ++i) {
2021      if (param_owners_[i] < 0) { continue; }
2022      params_[i]->ShareData(*params_[param_owners_[i]]);
2023      params_[i]->ShareDiff(*params_[param_owners_[i]]);
2024    }
2025  }
2026  template <typename Dtype>
2027  vector<int> Net<Dtype>::get_layer_learnable_param_ids(int layer_id) const {
2028    CHECK_GE(layer_id, 0);
2029    CHECK(layer_id < param_id_vecs_.size());
2030    const vector<int>& layer_param_ids = param_id_vecs_[layer_id];
2031    vector<int> ret;
2032    for (int i = 0; i < layer_param_ids.size(); ++i) {
2033      ret.push_back(learnable_param_ids_[layer_param_ids[i]]);
2034      CHECK(params_[layer_param_ids[i]].get() == learnable_params_[ret.back()]);
2035    }
2036    return ret;
2037  }
2038  template <typename Dtype>
2039  bool Net<Dtype>::has_blob(const string& blob_name) const {
2040    return blob_names_index_.find(blob_name) != blob_names_index_.end();
2041  }
2042  template <typename Dtype>
2043  const shared_ptr<Blob<Dtype> > Net<Dtype>::blob_by_name(
2044      const string& blob_name) const {
2045    shared_ptr<Blob<Dtype> > blob_ptr;
2046    if (has_blob(blob_name)) {
2047      blob_ptr = blobs_[blob_names_index_.find(blob_name)->second];
2048    } else {
2049      blob_ptr.reset((Blob<Dtype>*)(NULL));
2050      LOG(WARNING) << "Unknown blob name " << blob_name;
2051    }
2052    return blob_ptr;
2053  }
2054  template <typename Dtype>
2055  bool Net<Dtype>::has_layer(const string& layer_name) const {
2056    return layer_names_index_.find(layer_name) != layer_names_index_.end();
2057  }
2058  template <typename Dtype>
2059  const shared_ptr<Layer<Dtype> > Net<Dtype>::layer_by_name(
2060      const string& layer_name) const {
2061    shared_ptr<Layer<Dtype> > layer_ptr;
2062    if (has_layer(layer_name)) {
2063      layer_ptr = layers_[layer_names_index_.find(layer_name)->second];
2064    } else {
2065      layer_ptr.reset((Layer<Dtype>*)(NULL));
2066      LOG(WARNING) << "Unknown layer name " << layer_name;
2067    }
2068    return layer_ptr;
2069  }
2070  #ifdef CAFFE_PER_LAYER_TIMINGS
2071  template <typename Dtype>
2072  void Net<Dtype>::InitTimers() {
2073    int layer_count = layers().size();
2074    this->forward_time_per_layer.resize(layer_count, 0.0);
2075    this->backward_time_per_layer.resize(layer_count, 0.0);
2076    this->update_time_per_layer.resize(layer_count, 0.0);
2077    this->cleardiffs_time_per_iter = 0.0;
2078    this->forward_time_per_layer_total.resize(layer_count, 0.0);
2079    this->backward_time_per_layer_total.resize(layer_count, 0.0);
2080    this->update_time_per_layer_total.resize(layer_count, 0.0);
2081    this->cleardiffs_time_per_iter_total = 0.0;
2082    this->forward_start_time_per_layer.resize(layer_count, 0.0);
2083    this->forward_stop_time_per_layer.resize(layer_count, 0.0);
2084    this->backward_start_time_per_layer.resize(layer_count, 0.0);
2085    this->backward_stop_time_per_layer.resize(layer_count, 0.0);
2086    this->update_start_time_per_layer.resize(layer_count, 0.0);
2087    this->update_stop_time_per_layer.resize(layer_count, 0.0);
2088  #ifdef USE_MLSL
2089    this->startcomm_time_per_layer.resize(layer_count, 0.0);
2090    this->waitcomm_time_per_layer.resize(layer_count, 0.0);
2091    this->startcomm_time_per_layer_total.resize(layer_count, 0.0);
2092    this->waitcomm_time_per_layer_total.resize(layer_count, 0.0);
2093    this->startcomm_start_time_per_layer.resize(layer_count, 0.0);
2094    this->startcomm_stop_time_per_layer.resize(layer_count, 0.0);
2095  #ifdef FW_OVERLAP_OPT
2096    this->first_update_start_time_per_layer.resize(layer_count, 0.0);
2097    this->first_update_stop_time_per_layer.resize(layer_count, 0.0);
2098    this->first_waitcomm_start_time_per_layer.resize(layer_count, 0.0);
2099    this->first_waitcomm_stop_time_per_layer.resize(layer_count, 0.0);
2100  #endif
2101    this->waitcomm_start_time_per_layer.resize(layer_count, 0.0);
2102    this->waitcomm_stop_time_per_layer.resize(layer_count, 0.0);
2103  #endif
2104    timer.InitTime();
2105  #ifdef FW_OVERLAP_OPT
2106    wait_timer.InitTime(timer);
2107  #endif
2108  }
2109  template <typename Dtype>
2110  void Net<Dtype>::ResetTimers() {
2111    std::transform(this->forward_time_per_layer_total.begin(),
2112        this->forward_time_per_layer_total.end(),
2113        this->forward_time_per_layer.begin(),
2114        this->forward_time_per_layer_total.begin(),
2115        std::plus<double>());
2116    std::transform(this->backward_time_per_layer_total.begin(),
2117        this->backward_time_per_layer_total.end(),
2118        this->backward_time_per_layer.begin(),
2119        this->backward_time_per_layer_total.begin(),
2120        std::plus<double>());
2121    std::transform(this->update_time_per_layer_total.begin(),
2122        this->update_time_per_layer_total.end(),
2123        this->update_time_per_layer.begin(),
2124        this->update_time_per_layer_total.begin(),
2125        std::plus<double>());
2126    this->cleardiffs_time_per_iter_total += this->cleardiffs_time_per_iter;
2127  #ifdef USE_MLSL
2128    std::transform(this->startcomm_time_per_layer_total.begin(),
2129        this->startcomm_time_per_layer_total.end(),
2130        this->startcomm_time_per_layer.begin(),
2131        this->startcomm_time_per_layer_total.begin(),
2132        std::plus<double>());
2133    std::transform(this->waitcomm_time_per_layer_total.begin(),
2134        this->waitcomm_time_per_layer_total.end(),
2135        this->waitcomm_time_per_layer.begin(),
2136        this->waitcomm_time_per_layer_total.begin(),
2137        std::plus<double>());
2138  #endif
2139    std::fill(this->forward_time_per_layer.begin(),
2140        this->forward_time_per_layer.end(), 0.0);
2141    std::fill(this->backward_time_per_layer.begin(),
2142        this->backward_time_per_layer.end(), 0.0);
2143    std::fill(this->update_time_per_layer.begin(),
2144        this->update_time_per_layer.end(), 0.0);
2145    this->cleardiffs_time_per_iter = 0.0;
2146  #ifdef USE_MLSL
2147    std::fill(this->startcomm_time_per_layer.begin(),
2148        this->startcomm_time_per_layer.end(), 0.0);
2149    std::fill(this->waitcomm_time_per_layer.begin(),
2150        this->waitcomm_time_per_layer.end(), 0.0);
2151  #endif
2152  }
2153  template <typename Dtype>
2154  void Net<Dtype>::PrintTimers(bool printTotal) {
2155  #ifdef USE_MLSL
2156    if (mn::get_node_id() != 0)
2157      return;
2158  #endif
2159    LOG(WARNING) << std::endl;
2160    LOG(WARNING) << "####################################################";
2161    std::vector<double>& forward_timers = printTotal ?
2162      forward_time_per_layer_total : forward_time_per_layer;
2163    std::vector<double>& backward_timers = printTotal ?
2164      backward_time_per_layer_total : backward_time_per_layer;
2165    std::vector<double>& update_timers = printTotal ?
2166      update_time_per_layer_total : update_time_per_layer;
2167    double cleardiffs_timer = printTotal ?
2168      cleardiffs_time_per_iter_total : cleardiffs_time_per_iter;
2169  #ifdef USE_MLSL
2170    std::vector<double>& startcomm_timers = printTotal ?
2171      startcomm_time_per_layer_total : startcomm_time_per_layer;
2172    std::vector<double>& waitcomm_timers = printTotal ?
2173      waitcomm_time_per_layer_total : waitcomm_time_per_layer;
2174  #endif
2175    std::string prefix = printTotal ? "TOTAL " : "DELTA ";
2176    double forward_time = std::accumulate(forward_timers.begin(),
2177        forward_timers.end(), 0.0) / 1000.0;
2178    LOG(WARNING) << prefix << "FORWARD TIME: " << forward_time << " ms";
2179    for (int layer_idx = 0; layer_idx < layers().size(); layer_idx++) {
2180      LOG(WARNING) << "LAYER-" << layer_idx << " "
2181        << layers()[layer_idx]->type()
2182        << ": forward_time: " << forward_timers[layer_idx] / 1000.0
2183        << " ms";
2184    }
2185    LOG(WARNING) << std::endl;
2186    double backward_time = std::accumulate(backward_timers.begin(),
2187        backward_timers.end(), 0.0) / 1000.0;
2188    LOG(WARNING) << prefix << "BACKWARD TIME: " << backward_time << " ms";
2189    for (int layer_idx = 0; layer_idx < layers().size(); layer_idx++) {
2190      LOG(WARNING) << "LAYER-" << layer_idx << " "
2191        << layers()[layer_idx]->type()
2192        << ": backward_time: " << backward_timers[layer_idx] / 1000.0
2193        << " ms";
2194    }
2195    LOG(WARNING) << std::endl;
2196    double update_time = std::accumulate(update_timers.begin(),
2197        update_timers.end(), 0.0) / 1000.0;
2198    LOG(WARNING) << prefix << "UPDATE TIME: " << update_time << " ms";
2199    for (int layer_idx = 0; layer_idx < layers().size(); layer_idx++) {
2200      LOG(WARNING) << "LAYER-" << layer_idx << " "
2201        << layers()[layer_idx]->type()
2202        << ": update_time: " << update_timers[layer_idx] / 1000.0
2203        << " ms";
2204    }
2205    LOG(WARNING) << std::endl;
2206    double cleardiffs_time = cleardiffs_timer / 1000.0;
2207    LOG(WARNING) << prefix << "CLEAR PARAMETER DIFFS TIME: " << cleardiffs_time << " ms";
2208    LOG(WARNING) << std::endl;
2209  #ifdef USE_MLSL
2210    double startcomm_time = std::accumulate(startcomm_timers.begin(),
2211        startcomm_timers.end(), 0.0) / 1000.0;
2212    LOG(WARNING) << prefix << "START COMMUNICATION TIME: " << startcomm_time << " ms";
2213    for (int layer_idx = 0; layer_idx < layers().size(); layer_idx++) {
2214      LOG(WARNING) << "LAYER-" << layer_idx << " "
2215        << layers()[layer_idx]->type()
2216        << ": startcomm_time: " << startcomm_timers[layer_idx] / 1000.0
2217        << " ms";
2218    }
2219    LOG(WARNING) << std::endl;
2220    double waitcomm_time = std::accumulate(waitcomm_timers.begin(),
2221        waitcomm_timers.end(), 0.0) / 1000.0;
2222    LOG(WARNING) << prefix << "WAIT COMMUNICATION TIME: " << waitcomm_time << " ms";
2223    for (int layer_idx = 0; layer_idx < layers().size(); layer_idx++) {
2224      LOG(WARNING) << "LAYER-" << layer_idx << " "
2225        << layers()[layer_idx]->type()
2226        << ": waitcomm_time: " << waitcomm_timers[layer_idx] / 1000.0
2227        << " ms";
2228    }
2229    LOG(WARNING) << std::endl;
2230    LOG(WARNING) << prefix << "TIME (Computation + Communication): " << (forward_time +
2231        backward_time + update_time + cleardiffs_time + startcomm_time + waitcomm_time) / 1000.0
2232      << " sec";
2233  #else
2234    LOG(WARNING) << prefix << "TIME (Computation): " << (forward_time +
2235        backward_time + update_time + cleardiffs_time) / 1000.0 << " sec";
2236  #endif
2237    LOG(WARNING) << "####################################################";
2238    LOG(WARNING) << std::endl;
2239  }
2240  template <typename Dtype>
2241  void Net<Dtype>::SaveTimeline() {
2242    static bool initialized = false;
2243    std::ofstream time_file;
2244    string filename = name() + "_timeline"
2245  #ifdef USE_MLSL
2246          + "_" + std::to_string(mn::get_node_id())
2247  #endif
2248          + ".txt";
2249    if (initialized)
2250      time_file.open(filename, std::ios_base::app);
2251    else {
2252      initialized = true;
2253      time_file.open(filename);
2254    }
2255    time_file << "start,end,type,OP" << std::endl;
2256    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2257      if (forward_start_time_per_layer[layer_idx] == 0
2258          || forward_stop_time_per_layer[layer_idx] == 0)
2259          continue;
2260      time_file << forward_start_time_per_layer[layer_idx] / 1000
2261          << "," << forward_stop_time_per_layer[layer_idx] / 1000
2262          << ",Comp," << layers()[layer_idx]->type()
2263          << std::endl;
2264    }
2265    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2266      if (backward_start_time_per_layer[layer_idx] == 0
2267          || backward_stop_time_per_layer[layer_idx] == 0)
2268          continue;
2269      time_file << backward_start_time_per_layer[layer_idx] / 1000
2270          << "," << backward_stop_time_per_layer[layer_idx] / 1000
2271          << ",Comp," << layers()[layer_idx]->type() << "Grad"
2272          << std::endl;
2273    }
2274  #if defined(USE_MLSL) && defined(FW_OVERLAP_OPT) 
2275    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2276      if (first_update_start_time_per_layer[layer_idx] == 0
2277          || first_update_stop_time_per_layer[layer_idx] == 0)
2278          continue;
2279      time_file << first_update_start_time_per_layer[layer_idx] / 1000
2280          << "," << first_update_stop_time_per_layer[layer_idx] / 1000
2281          << ",Comp," << layers()[layer_idx]->type() << "FirstUpdate"
2282          << std::endl;
2283    }
2284  #endif
2285    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2286      if (update_start_time_per_layer[layer_idx] == 0
2287          || update_stop_time_per_layer[layer_idx] == 0)
2288          continue;
2289      time_file << update_start_time_per_layer[layer_idx] / 1000
2290          << "," << update_stop_time_per_layer[layer_idx] / 1000
2291          << ",Comp," << layers()[layer_idx]->type() << "Update"
2292          << std::endl;
2293    }
2294  #ifdef USE_MLSL
2295    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2296      if (startcomm_start_time_per_layer[layer_idx] == 0
2297          || startcomm_stop_time_per_layer[layer_idx] == 0)
2298          continue;
2299      time_file << startcomm_start_time_per_layer[layer_idx] / 1000
2300          << "," << startcomm_stop_time_per_layer[layer_idx] / 1000
2301          << ",Comm," << layers()[layer_idx]->type() << "Start"
2302          << std::endl;
2303    }
2304  #ifdef FW_OVERLAP_OPT
2305    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2306      if (first_waitcomm_start_time_per_layer[layer_idx] == 0
2307          || first_waitcomm_stop_time_per_layer[layer_idx] == 0)
2308          continue;
2309      time_file << first_waitcomm_start_time_per_layer[layer_idx] / 1000
2310          << "," << first_waitcomm_stop_time_per_layer[layer_idx] / 1000
2311          << ",Comm," << layers()[layer_idx]->type() << "FirstWait"
2312          << std::endl;
2313    }
2314  #endif
2315    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2316      if (waitcomm_start_time_per_layer[layer_idx] == 0
2317          || waitcomm_stop_time_per_layer[layer_idx] == 0)
2318          continue;
2319      time_file << waitcomm_start_time_per_layer[layer_idx] / 1000
2320          << "," << waitcomm_stop_time_per_layer[layer_idx] / 1000
2321          << ",Comm," << layers()[layer_idx]->type() << "Wait"
2322          << std::endl;
2323    }
2324  #endif
2325    time_file.close();
2326  }
2327  template <typename Dtype>
2328  void Net<Dtype>::PrintPayloadSize() {
2329  #ifdef USE_MLSL
2330    if (mn::get_node_id() != 0)
2331      return;
2332  #endif
2333    int total_payload_size = 0;
2334    const vector<Blob<Dtype> *> &net_params (learnable_params());
2335    LOG(WARNING) << std::endl;
2336    LOG(WARNING) << "####################################################";
2337    for (int layer_idx = 0; layer_idx < layers().size(); ++layer_idx) {
2338      std::vector<int> param_ids = get_layer_learnable_param_ids(layer_idx);
2339      for (int j = 0; j < param_ids.size(); j++) {
2340        int layer_payload_size = net_params[param_ids[j]]->count();
2341        LOG(WARNING) << "LAYER-" << layer_idx << " "
2342          << layers()[layer_idx]->type()
2343          << ": payload_size: " << layer_payload_size
2344          << " units";
2345        total_payload_size += layer_payload_size;
2346      }
2347    }
2348    LOG(WARNING) << "TOTAL PAYLOAD SIZE: " << total_payload_size << " units";
2349    LOG(WARNING) << "####################################################";
2350    LOG(WARNING) << std::endl;
2351  }
2352  #endif &bsol;* CAFFE_PER_LAYER_TIMINGS */
2353  INSTANTIATE_CLASS(Net);
2354  }  
2355  #if defined(FOUNDED_MLSL_ROOT)
2356  #define DEF_MLSL(str) \
2357  const char *mlsl_root = #str; 
2358  __attribute__((constructor)) void lib_ctor()  {
2359      DEF_MLSL(FOUNDED_MLSL_ROOT);
2360      setenv("MLSL_ROOT", mlsl_root, 0);
2361  }
2362  #endif
</code></pre>
        </div>
        <div class="column">
            <h3>tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-combine_tessdata.cpp</h3>
            <pre><code>1  #include "commontraining.h" 
2  #include "lstmrecognizer.h"
3  #include "tessdatamanager.h"
4  #include <cerrno>
5  #include <iostream> 
6  using namespace tesseract;
7  static int list_components(TessdataManager &tm, const char *filename) {
8    if (filename != nullptr && !tm.Init(filename)) {
9      tprintf("Failed to read %s\n", filename);
10      return EXIT_FAILURE;
11    }
12    tm.Directory();
13    return EXIT_SUCCESS;
14  }
15  static int list_network(TessdataManager &tm, const char *filename) {
16    if (filename != nullptr && !tm.Init(filename)) {
17      tprintf("Failed to read %s\n", filename);
18      return EXIT_FAILURE;
19    }
20    tesseract::TFile fp;
21    if (tm.GetComponent(tesseract::TESSDATA_LSTM, &fp)) {
22      tesseract::LSTMRecognizer recognizer;
23      if (!recognizer.DeSerialize(&tm, &fp)) {
24        tprintf("Failed to deserialize LSTM in %s!\n", filename);
25        return EXIT_FAILURE;
26      }
27      std::cout << "LSTM: network=" << recognizer.GetNetwork()
28                << ", int_mode=" << recognizer.IsIntMode()
29                << ", recoding=" << recognizer.IsRecoding()
<span onclick='openModal()' class='match'>30                << ", iteration=" << recognizer.training_iteration()
31                << ", sample_iteration=" << recognizer.sample_iteration()
32                << ", null_char=" << recognizer.null_char()
</span>33                << ", learning_rate=" << recognizer.learning_rate()
34                << ", momentum=" << recognizer.GetMomentum()
35                << ", adam_beta=" << recognizer.GetAdamBeta() << '\n';
36      std::cout << "Layer Learning Rates: ";
37      auto layers = recognizer.EnumerateLayers();
38      for (const auto &id : layers) {
39        auto layer = recognizer.GetLayer(id);
40        std::cout << id << "(" << layer->name() << ")"
41                  << "=" << recognizer.GetLayerLearningRate(id)
42                  << (layers[layers.size() - 1] != id ? ", " : "");
43      }
44      std::cout << "\n";
45    }
46    return EXIT_SUCCESS;
47  }
48  int main(int argc, char **argv) {
49    tesseract::CheckSharedLibraryVersion();
50    int i;
51    tesseract::TessdataManager tm;
52    if (argc > 1 && (!strcmp(argv[1], "-v") || !strcmp(argv[1], "--version"))) {
53      printf("%s\n", tesseract::TessBaseAPI::Version());
54      return EXIT_SUCCESS;
55    } else if (argc == 2) {
56      printf("Combining tessdata files\n");
57      std::string lang = argv[1];
58      char *last = &argv[1][strlen(argv[1]) - 1];
59      if (*last != '.') {
60        lang += '.';
61      }
62      std::string output_file = lang;
63      output_file += kTrainedDataSuffix;
64      if (!tm.CombineDataFiles(lang.c_str(), output_file.c_str())) {
65        printf("Error combining tessdata files into %s\n", output_file.c_str());
66      } else {
67        printf("Output %s created successfully.\n", output_file.c_str());
68      }
69    } else if (argc >= 4 &&
70               (strcmp(argv[1], "-e") == 0 || strcmp(argv[1], "-u") == 0)) {
71      if (!tm.Init(argv[2])) {
72        tprintf("Failed to read %s\n", argv[2]);
73        return EXIT_FAILURE;
74      }
75      printf("Extracting tessdata components from %s\n", argv[2]);
76      if (strcmp(argv[1], "-e") == 0) {
77        for (i = 3; i < argc; ++i) {
78          errno = 0;
79          if (tm.ExtractToFile(argv[i])) {
80            printf("Wrote %s\n", argv[i]);
81          } else if (errno == 0) {
82            printf(
83                "Not extracting %s, since this component"
84                " is not present\n",
85                argv[i]);
86            return EXIT_FAILURE;
87          } else {
88            printf("Error, could not extract %s: %s\n", argv[i], strerror(errno));
89            return EXIT_FAILURE;
90          }
91        }
92      } else { 
93        for (i = 0; i < tesseract::TESSDATA_NUM_ENTRIES; ++i) {
94          std::string filename = argv[3];
95          char *last = &argv[3][strlen(argv[3]) - 1];
96          if (*last != '.') {
97            filename += '.';
98          }
99          filename += tesseract::kTessdataFileSuffixes[i];
100          errno = 0;
101          if (tm.ExtractToFile(filename.c_str())) {
102            printf("Wrote %s\n", filename.c_str());
103          } else if (errno != 0) {
104            printf("Error, could not extract %s: %s\n", filename.c_str(),
105                   strerror(errno));
106            return EXIT_FAILURE;
107          }
108        }
109      }
110    } else if (argc >= 4 && strcmp(argv[1], "-o") == 0) {
111      const char *new_traineddata_filename = argv[2];
112      std::string traineddata_filename = new_traineddata_filename;
113      traineddata_filename += ".__tmp__";
114      if (rename(new_traineddata_filename, traineddata_filename.c_str()) != 0) {
115        tprintf("Failed to create a temporary file %s\n",
116                traineddata_filename.c_str());
117        return EXIT_FAILURE;
118      }
119      tm.Init(traineddata_filename.c_str());
120      tm.OverwriteComponents(new_traineddata_filename, argv + 3, argc - 3);
121    } else if (argc == 3 && strcmp(argv[1], "-c") == 0) {
122      if (!tm.Init(argv[2])) {
123        tprintf("Failed to read %s\n", argv[2]);
124        return EXIT_FAILURE;
125      }
126      tesseract::TFile fp;
127      if (!tm.GetComponent(tesseract::TESSDATA_LSTM, &fp)) {
128        tprintf("No LSTM Component found in %s!\n", argv[2]);
129        return EXIT_FAILURE;
130      }
131      tesseract::LSTMRecognizer recognizer;
132      if (!recognizer.DeSerialize(&tm, &fp)) {
133        tprintf("Failed to deserialize LSTM in %s!\n", argv[2]);
134        return EXIT_FAILURE;
135      }
136      recognizer.ConvertToInt();
137      std::vector<char> lstm_data;
138      fp.OpenWrite(&lstm_data);
139      ASSERT_HOST(recognizer.Serialize(&tm, &fp));
140      tm.OverwriteEntry(tesseract::TESSDATA_LSTM, &lstm_data[0],
141                        lstm_data.size());
142      if (!tm.SaveFile(argv[2], nullptr)) {
143        tprintf("Failed to write modified traineddata:%s!\n", argv[2]);
144        return EXIT_FAILURE;
145      }
146    } else if (argc == 3 && strcmp(argv[1], "-d") == 0) {
147      return list_components(tm, argv[2]);
148    } else if (argc == 3 && strcmp(argv[1], "-l") == 0) {
149      return list_network(tm, argv[2]);
150    } else if (argc == 3 && strcmp(argv[1], "-dl") == 0) {
151      int result = list_components(tm, argv[2]);
152      if (result == EXIT_SUCCESS) {
153        result = list_network(tm, nullptr);
154      }
155      return result;
156    } else if (argc == 3 && strcmp(argv[1], "-ld") == 0) {
157      int result = list_network(tm, argv[2]);
158      if (result == EXIT_SUCCESS) {
159        result = list_components(tm, nullptr);
160      }
161      return result;
162    } else {
163      printf(
164          "Usage for combining tessdata components:\n"
165          "  %s language_data_path_prefix\n"
166          "  (e.g. %s tessdata/eng.)\n\n",
167          argv[0], argv[0]);
168      printf(
169          "Usage for extracting tessdata components:\n"
170          "  %s -e traineddata_file [output_component_file...]\n"
171          "  (e.g. %s -e eng.traineddata eng.unicharset)\n\n",
172          argv[0], argv[0]);
173      printf(
174          "Usage for overwriting tessdata components:\n"
175          "  %s -o traineddata_file [input_component_file...]\n"
176          "  (e.g. %s -o eng.traineddata eng.unicharset)\n\n",
177          argv[0], argv[0]);
178      printf(
179          "Usage for unpacking all tessdata components:\n"
180          "  %s -u traineddata_file output_path_prefix\n"
181          "  (e.g. %s -u eng.traineddata tmp/eng.)\n\n",
182          argv[0], argv[0]);
183      printf(
184          "Usage for listing the network information\n"
185          "  %s -l traineddata_file\n"
186          "  (e.g. %s -l eng.traineddata)\n\n",
187          argv[0], argv[0]);
188      printf(
189          "Usage for listing directory of components:\n"
190          "  %s -d traineddata_file\n\n",
191          argv[0]);
192      printf(
193          "Usage for compacting LSTM component to int:\n"
194          "  %s -c traineddata_file\n",
195          argv[0]);
196      return EXIT_FAILURE;
197    }
198    tm.Directory();
199    return EXIT_SUCCESS;
200  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-net.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-combine_tessdata.cpp</div>
                </div>
                <div class="column column_space"><pre><code>1190            << "The NetState level (" << state.level()
1191            << ") is above the min_level (" << rule.min_level()
1192            << ") specified by a rule in layer " << layer_name;
</pre></code></div>
                <div class="column column_space"><pre><code>30                << ", iteration=" << recognizer.training_iteration()
31                << ", sample_iteration=" << recognizer.sample_iteration()
32                << ", null_char=" << recognizer.null_char()
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    