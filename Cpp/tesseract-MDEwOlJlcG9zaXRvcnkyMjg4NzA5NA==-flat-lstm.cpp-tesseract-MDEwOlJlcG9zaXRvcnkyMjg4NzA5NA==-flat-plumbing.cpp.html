
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 22, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-lstm.cpp</h3>
            <pre><code>1  #ifdef HAVE_CONFIG_H
2  #  include "config_auto.h"
3  #endif
4  #include "lstm.h"
5  #ifdef _OPENMP
6  #  include <omp.h>
7  #endif
8  #include <cstdio>
9  #include <cstdlib>
10  #include <sstream> 
11  #if defined(_MSC_VER) && !defined(__clang__)
12  #  include <intrin.h> 
13  #endif
14  #include "fullyconnected.h"
15  #include "functions.h"
16  #include "networkscratch.h"
17  #include "tprintf.h"
18  #ifdef _OPENMP
19  #  define PARALLEL_IF_OPENMP(__num_threads)                                  \
20      PRAGMA(omp parallel if (__num_threads > 1) num_threads(__num_threads)) { \
21        PRAGMA(omp sections nowait) {                                          \
22          PRAGMA(omp section) {
23  #  define SECTION_IF_OPENMP \
24      }                       \
25      PRAGMA(omp section) {
26  #  define END_PARALLEL_IF_OPENMP \
27      }                            \
28      } &bsol;* end of sections */      \
29      } &bsol;* end of parallel section */
30  #  ifdef _MSC_VER 
31  #    define PRAGMA(x) __pragma(x)
32  #  else
33  #    define PRAGMA(x) _Pragma(#    x)
34  #  endif 
35  #else 
36  #  define PARALLEL_IF_OPENMP(__num_threads)
37  #  define SECTION_IF_OPENMP
38  #  define END_PARALLEL_IF_OPENMP
39  #endif 
40  namespace tesseract {
41  const TFloat kStateClip = 100.0;
42  const TFloat kErrClip = 1.0f;
43  static inline uint32_t ceil_log2(uint32_t n) {
44  #if defined(__GNUC__)
45    uint32_t l2 = 31 - __builtin_clz(n);
46  #elif defined(_MSC_VER)
47    unsigned long l2 = 0;
48    _BitScanReverse(&l2, n);
49  #else
50    if (n == 0)
51      return UINT_MAX;
52    if (n == 1)
53      return 0;
54    uint32_t val = n;
55    uint32_t l2 = 0;
56    while (val > 1) {
57      val >>= 1;
58      l2++;
59    }
60  #endif
61    return (n == (1u << l2)) ? l2 : l2 + 1;
62  }
63  LSTM::LSTM(const std::string &name, int ni, int ns, int no, bool two_dimensional, NetworkType type)
64      : Network(type, name, ni, no)
65      , na_(ni + ns)
66      , ns_(ns)
67      , nf_(0)
68      , is_2d_(two_dimensional)
69      , softmax_(nullptr)
70      , input_width_(0) {
71    if (two_dimensional) {
72      na_ += ns_;
73    }
74    if (type_ == NT_LSTM || type_ == NT_LSTM_SUMMARY) {
75      nf_ = 0;
76      ASSERT_HOST(no == ns);
77    } else if (type_ == NT_LSTM_SOFTMAX || type_ == NT_LSTM_SOFTMAX_ENCODED) {
78      nf_ = type_ == NT_LSTM_SOFTMAX ? no_ : ceil_log2(no_);
79      softmax_ = new FullyConnected("LSTM Softmax", ns_, no_, NT_SOFTMAX);
80    } else {
81      tprintf("%d is invalid type of LSTM!\n", type);
82      ASSERT_HOST(false);
83    }
84    na_ += nf_;
85  }
86  LSTM::~LSTM() {
87    delete softmax_;
88  }
89  StaticShape LSTM::OutputShape(const StaticShape &input_shape) const {
90    StaticShape result = input_shape;
91    result.set_depth(no_);
92    if (type_ == NT_LSTM_SUMMARY) {
93      result.set_width(1);
94    }
95    if (softmax_ != nullptr) {
96      return softmax_->OutputShape(result);
97    }
98    return result;
99  }
100  void LSTM::SetEnableTraining(TrainingState state) {
101    if (state == TS_RE_ENABLE) {
102      if (training_ == TS_TEMP_DISABLE) {
103        training_ = TS_ENABLED;
104      }
105    } else if (state == TS_TEMP_DISABLE) {
106      if (training_ == TS_ENABLED) {
107        training_ = state;
108      }
109    } else {
110      if (state == TS_ENABLED && training_ != TS_ENABLED) {
111        for (int w = 0; w < WT_COUNT; ++w) {
112          if (w == GFS && !Is2D()) {
113            continue;
114          }
115          gate_weights_[w].InitBackward();
116        }
117      }
118      training_ = state;
119    }
120    if (softmax_ != nullptr) {
<span onclick='openModal()' class='match'>121      softmax_->SetEnableTraining(state);
122    }
123  }
124  int LSTM::InitWeights(float range, TRand *randomizer) {
125    Network::SetRandomizer(randomizer);
</span>126    num_weights_ = 0;
127    for (int w = 0; w < WT_COUNT; ++w) {
128      if (w == GFS && !Is2D()) {
129        continue;
130      }
131      num_weights_ +=
132          gate_weights_[w].InitWeightsFloat(ns_, na_ + 1, TestFlag(NF_ADAM), range, randomizer);
133    }
134    if (softmax_ != nullptr) {
135      num_weights_ += softmax_->InitWeights(range, randomizer);
136    }
137    return num_weights_;
138  }
139  int LSTM::RemapOutputs(int old_no, const std::vector<int> &code_map) {
140    if (softmax_ != nullptr) {
141      num_weights_ -= softmax_->num_weights();
142      num_weights_ += softmax_->RemapOutputs(old_no, code_map);
143    }
144    return num_weights_;
145  }
146  void LSTM::ConvertToInt() {
147    for (int w = 0; w < WT_COUNT; ++w) {
148      if (w == GFS && !Is2D()) {
149        continue;
150      }
151      gate_weights_[w].ConvertToInt();
152    }
153    if (softmax_ != nullptr) {
154      softmax_->ConvertToInt();
155    }
156  }
157  void LSTM::DebugWeights() {
158    for (int w = 0; w < WT_COUNT; ++w) {
159      if (w == GFS && !Is2D()) {
160        continue;
161      }
162      std::ostringstream msg;
163      msg << name_ << " Gate weights " << w;
164      gate_weights_[w].Debug2D(msg.str().c_str());
165    }
166    if (softmax_ != nullptr) {
167      softmax_->DebugWeights();
168    }
169  }
170  bool LSTM::Serialize(TFile *fp) const {
171    if (!Network::Serialize(fp)) {
172      return false;
173    }
174    if (!fp->Serialize(&na_)) {
175      return false;
176    }
177    for (int w = 0; w < WT_COUNT; ++w) {
178      if (w == GFS && !Is2D()) {
179        continue;
180      }
181      if (!gate_weights_[w].Serialize(IsTraining(), fp)) {
182        return false;
183      }
184    }
185    if (softmax_ != nullptr && !softmax_->Serialize(fp)) {
186      return false;
187    }
188    return true;
189  }
190  bool LSTM::DeSerialize(TFile *fp) {
191    if (!fp->DeSerialize(&na_)) {
192      return false;
193    }
194    if (type_ == NT_LSTM_SOFTMAX) {
195      nf_ = no_;
196    } else if (type_ == NT_LSTM_SOFTMAX_ENCODED) {
197      nf_ = ceil_log2(no_);
198    } else {
199      nf_ = 0;
200    }
201    is_2d_ = false;
202    for (int w = 0; w < WT_COUNT; ++w) {
203      if (w == GFS && !Is2D()) {
204        continue;
205      }
206      if (!gate_weights_[w].DeSerialize(IsTraining(), fp)) {
207        return false;
208      }
209      if (w == CI) {
210        ns_ = gate_weights_[CI].NumOutputs();
211        is_2d_ = na_ - nf_ == ni_ + 2 * ns_;
212      }
213    }
214    delete softmax_;
215    if (type_ == NT_LSTM_SOFTMAX || type_ == NT_LSTM_SOFTMAX_ENCODED) {
216      softmax_ = static_cast<FullyConnected *>(Network::CreateFromFile(fp));
217      if (softmax_ == nullptr) {
218        return false;
219      }
220    } else {
221      softmax_ = nullptr;
222    }
223    return true;
224  }
225  void LSTM::Forward(bool debug, const NetworkIO &input, const TransposedArray *input_transpose,
226                     NetworkScratch *scratch, NetworkIO *output) {
227    input_map_ = input.stride_map();
228    input_width_ = input.Width();
229    if (softmax_ != nullptr) {
230      output->ResizeFloat(input, no_);
231    } else if (type_ == NT_LSTM_SUMMARY) {
232      output->ResizeXTo1(input, no_);
233    } else {
234      output->Resize(input, no_);
235    }
236    ResizeForward(input);
237    NetworkScratch::FloatVec temp_lines[WT_COUNT];
238    int ro = ns_;
239    if (source_.int_mode() && IntSimdMatrix::intSimdMatrix) {
240      ro = IntSimdMatrix::intSimdMatrix->RoundOutputs(ro);
241    }
242    for (auto &temp_line : temp_lines) {
243      temp_line.Init(ns_, ro, scratch);
244    }
245    NetworkScratch::FloatVec curr_state, curr_output;
246    curr_state.Init(ns_, scratch);
247    ZeroVector<TFloat>(ns_, curr_state);
248    curr_output.Init(ns_, scratch);
249    ZeroVector<TFloat>(ns_, curr_output);
250    int buf_width = Is2D() ? input_map_.Size(FD_WIDTH) : 1;
251    std::vector<NetworkScratch::FloatVec> states, outputs;
252    if (Is2D()) {
253      states.resize(buf_width);
254      outputs.resize(buf_width);
255      for (int i = 0; i < buf_width; ++i) {
256        states[i].Init(ns_, scratch);
257        ZeroVector<TFloat>(ns_, states[i]);
258        outputs[i].Init(ns_, scratch);
259        ZeroVector<TFloat>(ns_, outputs[i]);
260      }
261    }
262    NetworkScratch::FloatVec softmax_output;
263    NetworkScratch::IO int_output;
264    if (softmax_ != nullptr) {
265      softmax_output.Init(no_, scratch);
266      ZeroVector<TFloat>(no_, softmax_output);
267      int rounded_softmax_inputs = gate_weights_[CI].RoundInputs(ns_);
268      if (input.int_mode()) {
269        int_output.Resize2d(true, 1, rounded_softmax_inputs, scratch);
270      }
271      softmax_->SetupForward(input, nullptr);
272    }
273    NetworkScratch::FloatVec curr_input;
274    curr_input.Init(na_, scratch);
275    StrideMap::Index src_index(input_map_);
276    StrideMap::Index dest_index(output->stride_map());
277    do {
278      int t = src_index.t();
279      bool valid_2d = Is2D();
280      if (valid_2d) {
281        StrideMap::Index dim_index(src_index);
282        if (!dim_index.AddOffset(-1, FD_HEIGHT)) {
283          valid_2d = false;
284        }
285      }
286      int mod_t = Modulo(t, buf_width); 
287      source_.CopyTimeStepGeneral(t, 0, ni_, input, t, 0);
288      if (softmax_ != nullptr) {
289        source_.WriteTimeStepPart(t, ni_, nf_, softmax_output);
290      }
291      source_.WriteTimeStepPart(t, ni_ + nf_, ns_, curr_output);
292      if (Is2D()) {
293        source_.WriteTimeStepPart(t, ni_ + nf_ + ns_, ns_, outputs[mod_t]);
294      }
295      if (!source_.int_mode()) {
296        source_.ReadTimeStep(t, curr_input);
297      }
298      PARALLEL_IF_OPENMP(GFS)
299      if (source_.int_mode()) {
300        gate_weights_[CI].MatrixDotVector(source_.i(t), temp_lines[CI]);
301      } else {
302        gate_weights_[CI].MatrixDotVector(curr_input, temp_lines[CI]);
303      }
304      FuncInplace<GFunc>(ns_, temp_lines[CI]);
305      SECTION_IF_OPENMP
306      if (source_.int_mode()) {
307        gate_weights_[GI].MatrixDotVector(source_.i(t), temp_lines[GI]);
308      } else {
309        gate_weights_[GI].MatrixDotVector(curr_input, temp_lines[GI]);
310      }
311      FuncInplace<FFunc>(ns_, temp_lines[GI]);
312      SECTION_IF_OPENMP
313      if (source_.int_mode()) {
314        gate_weights_[GF1].MatrixDotVector(source_.i(t), temp_lines[GF1]);
315      } else {
316        gate_weights_[GF1].MatrixDotVector(curr_input, temp_lines[GF1]);
317      }
318      FuncInplace<FFunc>(ns_, temp_lines[GF1]);
319      if (Is2D()) {
320        if (source_.int_mode()) {
321          gate_weights_[GFS].MatrixDotVector(source_.i(t), temp_lines[GFS]);
322        } else {
323          gate_weights_[GFS].MatrixDotVector(curr_input, temp_lines[GFS]);
324        }
325        FuncInplace<FFunc>(ns_, temp_lines[GFS]);
326      }
327      SECTION_IF_OPENMP
328      if (source_.int_mode()) {
329        gate_weights_[GO].MatrixDotVector(source_.i(t), temp_lines[GO]);
330      } else {
331        gate_weights_[GO].MatrixDotVector(curr_input, temp_lines[GO]);
332      }
333      FuncInplace<FFunc>(ns_, temp_lines[GO]);
334      END_PARALLEL_IF_OPENMP
335      MultiplyVectorsInPlace(ns_, temp_lines[GF1], curr_state);
336      if (Is2D()) {
337        int8_t *which_fg_col = which_fg_[t];
338        memset(which_fg_col, 1, ns_ * sizeof(which_fg_col[0]));
339        if (valid_2d) {
340          const TFloat *stepped_state = states[mod_t];
341          for (int i = 0; i < ns_; ++i) {
342            if (temp_lines[GF1][i] < temp_lines[GFS][i]) {
343              curr_state[i] = temp_lines[GFS][i] * stepped_state[i];
344              which_fg_col[i] = 2;
345            }
346          }
347        }
348      }
349      MultiplyAccumulate(ns_, temp_lines[CI], temp_lines[GI], curr_state);
350      ClipVector<TFloat>(ns_, -kStateClip, kStateClip, curr_state);
351      if (IsTraining()) {
352        node_values_[CI].WriteTimeStep(t, temp_lines[CI]);
353        node_values_[GI].WriteTimeStep(t, temp_lines[GI]);
354        node_values_[GF1].WriteTimeStep(t, temp_lines[GF1]);
355        node_values_[GO].WriteTimeStep(t, temp_lines[GO]);
356        if (Is2D()) {
357          node_values_[GFS].WriteTimeStep(t, temp_lines[GFS]);
358        }
359      }
360      FuncMultiply<HFunc>(curr_state, temp_lines[GO], ns_, curr_output);
361      if (IsTraining()) {
362        state_.WriteTimeStep(t, curr_state);
363      }
364      if (softmax_ != nullptr) {
365        if (input.int_mode()) {
366          int_output->WriteTimeStepPart(0, 0, ns_, curr_output);
367          softmax_->ForwardTimeStep(int_output->i(0), t, softmax_output);
368        } else {
369          softmax_->ForwardTimeStep(curr_output, t, softmax_output);
370        }
371        output->WriteTimeStep(t, softmax_output);
372        if (type_ == NT_LSTM_SOFTMAX_ENCODED) {
373          CodeInBinary(no_, nf_, softmax_output);
374        }
375      } else if (type_ == NT_LSTM_SUMMARY) {
376        if (src_index.IsLast(FD_WIDTH)) {
377          output->WriteTimeStep(dest_index.t(), curr_output);
378          dest_index.Increment();
379        }
380      } else {
381        output->WriteTimeStep(t, curr_output);
382      }
383      if (Is2D()) {
384        CopyVector(ns_, curr_state, states[mod_t]);
385        CopyVector(ns_, curr_output, outputs[mod_t]);
386      }
387      if (src_index.IsLast(FD_WIDTH)) {
388        ZeroVector<TFloat>(ns_, curr_state);
389        ZeroVector<TFloat>(ns_, curr_output);
390      }
391    } while (src_index.Increment());
392  #if DEBUG_DETAIL > 0
393    tprintf("Source:%s\n", name_.c_str());
394    source_.Print(10);
395    tprintf("State:%s\n", name_.c_str());
396    state_.Print(10);
397    tprintf("Output:%s\n", name_.c_str());
398    output->Print(10);
399  #endif
400  #ifndef GRAPHICS_DISABLED
401    if (debug) {
402      DisplayForward(*output);
403    }
404  #endif
405  }
406  bool LSTM::Backward(bool debug, const NetworkIO &fwd_deltas, NetworkScratch *scratch,
407                      NetworkIO *back_deltas) {
408  #ifndef GRAPHICS_DISABLED
409    if (debug) {
410      DisplayBackward(fwd_deltas);
411    }
412  #endif
413    back_deltas->ResizeToMap(fwd_deltas.int_mode(), input_map_, ni_);
414    NetworkScratch::FloatVec outputerr;
415    outputerr.Init(ns_, scratch);
416    NetworkScratch::FloatVec curr_stateerr, curr_sourceerr;
417    curr_stateerr.Init(ns_, scratch);
418    curr_sourceerr.Init(na_, scratch);
419    ZeroVector<TFloat>(ns_, curr_stateerr);
420    ZeroVector<TFloat>(na_, curr_sourceerr);
421    NetworkScratch::FloatVec gate_errors[WT_COUNT];
422    for (auto &gate_error : gate_errors) {
423      gate_error.Init(ns_, scratch);
424    }
425    int buf_width = Is2D() ? input_map_.Size(FD_WIDTH) : 1;
426    std::vector<NetworkScratch::FloatVec> stateerr, sourceerr;
427    if (Is2D()) {
428      stateerr.resize(buf_width);
429      sourceerr.resize(buf_width);
430      for (int t = 0; t < buf_width; ++t) {
431        stateerr[t].Init(ns_, scratch);
432        sourceerr[t].Init(na_, scratch);
433        ZeroVector<TFloat>(ns_, stateerr[t]);
434        ZeroVector<TFloat>(na_, sourceerr[t]);
435      }
436    }
437    NetworkScratch::FloatVec sourceerr_temps[WT_COUNT];
438    for (auto &sourceerr_temp : sourceerr_temps) {
439      sourceerr_temp.Init(na_, scratch);
440    }
441    int width = input_width_;
442    NetworkScratch::GradientStore gate_errors_t[WT_COUNT];
443    for (auto &w : gate_errors_t) {
444      w.Init(ns_, width, scratch);
445    }
446    NetworkScratch::FloatVec softmax_errors;
447    NetworkScratch::GradientStore softmax_errors_t;
448    if (softmax_ != nullptr) {
449      softmax_errors.Init(no_, scratch);
450      softmax_errors_t.Init(no_, width, scratch);
451    }
452    TFloat state_clip = Is2D() ? 9.0 : 4.0;
453  #if DEBUG_DETAIL > 1
454    tprintf("fwd_deltas:%s\n", name_.c_str());
455    fwd_deltas.Print(10);
456  #endif
457    StrideMap::Index dest_index(input_map_);
458    dest_index.InitToLast();
459    StrideMap::Index src_index(fwd_deltas.stride_map());
460    src_index.InitToLast();
461    do {
462      int t = dest_index.t();
463      bool at_last_x = dest_index.IsLast(FD_WIDTH);
464      int up_pos = -1;
465      int down_pos = -1;
466      if (Is2D()) {
467        if (dest_index.index(FD_HEIGHT) > 0) {
468          StrideMap::Index up_index(dest_index);
469          if (up_index.AddOffset(-1, FD_HEIGHT)) {
470            up_pos = up_index.t();
471          }
472        }
473        if (!dest_index.IsLast(FD_HEIGHT)) {
474          StrideMap::Index down_index(dest_index);
475          if (down_index.AddOffset(1, FD_HEIGHT)) {
476            down_pos = down_index.t();
477          }
478        }
479      }
480      int mod_t = Modulo(t, buf_width); 
481      if (at_last_x) {
482        ZeroVector<TFloat>(na_, curr_sourceerr);
483        ZeroVector<TFloat>(ns_, curr_stateerr);
484      }
485      if (type_ == NT_LSTM_SUMMARY) {
486        if (dest_index.IsLast(FD_WIDTH)) {
487          fwd_deltas.ReadTimeStep(src_index.t(), outputerr);
488          src_index.Decrement();
489        } else {
490          ZeroVector<TFloat>(ns_, outputerr);
491        }
492      } else if (softmax_ == nullptr) {
493        fwd_deltas.ReadTimeStep(t, outputerr);
494      } else {
495        softmax_->BackwardTimeStep(fwd_deltas, t, softmax_errors, softmax_errors_t.get(), outputerr);
496      }
497      if (!at_last_x) {
498        AccumulateVector(ns_, curr_sourceerr + ni_ + nf_, outputerr);
499      }
500      if (down_pos >= 0) {
501        AccumulateVector(ns_, sourceerr[mod_t] + ni_ + nf_ + ns_, outputerr);
502      }
503      if (!at_last_x) {
504        const float *next_node_gf1 = node_values_[GF1].f(t + 1);
505        for (int i = 0; i < ns_; ++i) {
506          curr_stateerr[i] *= next_node_gf1[i];
507        }
508      }
509      if (Is2D() && t + 1 < width) {
510        for (int i = 0; i < ns_; ++i) {
511          if (which_fg_[t + 1][i] != 1) {
512            curr_stateerr[i] = 0.0;
513          }
514        }
515        if (down_pos >= 0) {
516          const float *right_node_gfs = node_values_[GFS].f(down_pos);
517          const TFloat *right_stateerr = stateerr[mod_t];
518          for (int i = 0; i < ns_; ++i) {
519            if (which_fg_[down_pos][i] == 2) {
520              curr_stateerr[i] += right_stateerr[i] * right_node_gfs[i];
521            }
522          }
523        }
524      }
525      state_.FuncMultiply3Add<HPrime>(node_values_[GO], t, outputerr, curr_stateerr);
526      ClipVector<TFloat>(ns_, -state_clip, state_clip, curr_stateerr);
527  #if DEBUG_DETAIL > 1
528      if (t + 10 > width) {
529        tprintf("t=%d, stateerr=", t);
530        for (int i = 0; i < ns_; ++i)
531          tprintf(" %g,%g,%g", curr_stateerr[i], outputerr[i], curr_sourceerr[ni_ + nf_ + i]);
532        tprintf("\n");
533      }
534  #endif
535      PARALLEL_IF_OPENMP(GFS)
536      node_values_[CI].FuncMultiply3<GPrime>(t, node_values_[GI], t, curr_stateerr, gate_errors[CI]);
537      ClipVector(ns_, -kErrClip, kErrClip, gate_errors[CI].get());
538      gate_weights_[CI].VectorDotMatrix(gate_errors[CI], sourceerr_temps[CI]);
539      gate_errors_t[CI].get()->WriteStrided(t, gate_errors[CI]);
540      SECTION_IF_OPENMP
541      node_values_[GI].FuncMultiply3<FPrime>(t, node_values_[CI], t, curr_stateerr, gate_errors[GI]);
542      ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GI].get());
543      gate_weights_[GI].VectorDotMatrix(gate_errors[GI], sourceerr_temps[GI]);
544      gate_errors_t[GI].get()->WriteStrided(t, gate_errors[GI]);
545      SECTION_IF_OPENMP
546      if (t > 0) {
547        node_values_[GF1].FuncMultiply3<FPrime>(t, state_, t - 1, curr_stateerr, gate_errors[GF1]);
548        ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GF1].get());
549        gate_weights_[GF1].VectorDotMatrix(gate_errors[GF1], sourceerr_temps[GF1]);
550      } else {
551        memset(gate_errors[GF1], 0, ns_ * sizeof(gate_errors[GF1][0]));
552        memset(sourceerr_temps[GF1], 0, na_ * sizeof(*sourceerr_temps[GF1]));
553      }
554      gate_errors_t[GF1].get()->WriteStrided(t, gate_errors[GF1]);
555      if (up_pos >= 0) {
556        node_values_[GFS].FuncMultiply3<FPrime>(t, state_, up_pos, curr_stateerr, gate_errors[GFS]);
557        ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GFS].get());
558        gate_weights_[GFS].VectorDotMatrix(gate_errors[GFS], sourceerr_temps[GFS]);
559      } else {
560        memset(gate_errors[GFS], 0, ns_ * sizeof(gate_errors[GFS][0]));
561        memset(sourceerr_temps[GFS], 0, na_ * sizeof(*sourceerr_temps[GFS]));
562      }
563      if (Is2D()) {
564        gate_errors_t[GFS].get()->WriteStrided(t, gate_errors[GFS]);
565      }
566      SECTION_IF_OPENMP
567      state_.Func2Multiply3<HFunc, FPrime>(node_values_[GO], t, outputerr, gate_errors[GO]);
568      ClipVector(ns_, -kErrClip, kErrClip, gate_errors[GO].get());
569      gate_weights_[GO].VectorDotMatrix(gate_errors[GO], sourceerr_temps[GO]);
570      gate_errors_t[GO].get()->WriteStrided(t, gate_errors[GO]);
571      END_PARALLEL_IF_OPENMP
572      SumVectors(na_, sourceerr_temps[CI], sourceerr_temps[GI], sourceerr_temps[GF1],
573                 sourceerr_temps[GO], sourceerr_temps[GFS], curr_sourceerr);
574      back_deltas->WriteTimeStep(t, curr_sourceerr);
575      if (Is2D()) {
576        CopyVector(ns_, curr_stateerr, stateerr[mod_t]);
577        CopyVector(na_, curr_sourceerr, sourceerr[mod_t]);
578      }
579    } while (dest_index.Decrement());
580  #if DEBUG_DETAIL > 2
581    for (int w = 0; w < WT_COUNT; ++w) {
582      tprintf("%s gate errors[%d]\n", name_.c_str(), w);
583      gate_errors_t[w].get()->PrintUnTransposed(10);
584    }
585  #endif
586    NetworkScratch::GradientStore source_t, state_t;
587    source_t.Init(na_, width, scratch);
588    source_.Transpose(source_t.get());
589    state_t.Init(ns_, width, scratch);
590    state_.Transpose(state_t.get());
591  #ifdef _OPENMP
592  #  pragma omp parallel for num_threads(GFS) if (!Is2D())
593  #endif
594    for (int w = 0; w < WT_COUNT; ++w) {
595      if (w == GFS && !Is2D()) {
596        continue;
597      }
598      gate_weights_[w].SumOuterTransposed(*gate_errors_t[w], *source_t, false);
599    }
600    if (softmax_ != nullptr) {
601      softmax_->FinishBackward(*softmax_errors_t);
602    }
603    return needs_to_backprop_;
604  }
605  void LSTM::Update(float learning_rate, float momentum, float adam_beta, int num_samples) {
606  #if DEBUG_DETAIL > 3
607    PrintW();
608  #endif
609    for (int w = 0; w < WT_COUNT; ++w) {
610      if (w == GFS && !Is2D()) {
611        continue;
612      }
613      gate_weights_[w].Update(learning_rate, momentum, adam_beta, num_samples);
614    }
615    if (softmax_ != nullptr) {
616      softmax_->Update(learning_rate, momentum, adam_beta, num_samples);
617    }
618  #if DEBUG_DETAIL > 3
619    PrintDW();
620  #endif
621  }
622  void LSTM::CountAlternators(const Network &other, TFloat *same, TFloat *changed) const {
623    ASSERT_HOST(other.type() == type_);
624    const LSTM *lstm = static_cast<const LSTM *>(&other);
625    for (int w = 0; w < WT_COUNT; ++w) {
626      if (w == GFS && !Is2D()) {
627        continue;
628      }
629      gate_weights_[w].CountAlternators(lstm->gate_weights_[w], same, changed);
630    }
631    if (softmax_ != nullptr) {
632      softmax_->CountAlternators(*lstm->softmax_, same, changed);
633    }
634  }
635  #if DEBUG_DETAIL > 3
636  void LSTM::PrintW() {
637    tprintf("Weight state:%s\n", name_.c_str());
638    for (int w = 0; w < WT_COUNT; ++w) {
639      if (w == GFS && !Is2D()) {
640        continue;
641      }
642      tprintf("Gate %d, inputs\n", w);
643      for (int i = 0; i < ni_; ++i) {
644        tprintf("Row %d:", i);
645        for (int s = 0; s < ns_; ++s) {
646          tprintf(" %g", gate_weights_[w].GetWeights(s)[i]);
647        }
648        tprintf("\n");
649      }
650      tprintf("Gate %d, outputs\n", w);
651      for (int i = ni_; i < ni_ + ns_; ++i) {
652        tprintf("Row %d:", i - ni_);
653        for (int s = 0; s < ns_; ++s) {
654          tprintf(" %g", gate_weights_[w].GetWeights(s)[i]);
655        }
656        tprintf("\n");
657      }
658      tprintf("Gate %d, bias\n", w);
659      for (int s = 0; s < ns_; ++s) {
660        tprintf(" %g", gate_weights_[w].GetWeights(s)[na_]);
661      }
662      tprintf("\n");
663    }
664  }
665  void LSTM::PrintDW() {
666    tprintf("Delta state:%s\n", name_.c_str());
667    for (int w = 0; w < WT_COUNT; ++w) {
668      if (w == GFS && !Is2D()) {
669        continue;
670      }
671      tprintf("Gate %d, inputs\n", w);
672      for (int i = 0; i < ni_; ++i) {
673        tprintf("Row %d:", i);
674        for (int s = 0; s < ns_; ++s) {
675          tprintf(" %g", gate_weights_[w].GetDW(s, i));
676        }
677        tprintf("\n");
678      }
679      tprintf("Gate %d, outputs\n", w);
680      for (int i = ni_; i < ni_ + ns_; ++i) {
681        tprintf("Row %d:", i - ni_);
682        for (int s = 0; s < ns_; ++s) {
683          tprintf(" %g", gate_weights_[w].GetDW(s, i));
684        }
685        tprintf("\n");
686      }
687      tprintf("Gate %d, bias\n", w);
688      for (int s = 0; s < ns_; ++s) {
689        tprintf(" %g", gate_weights_[w].GetDW(s, na_));
690      }
691      tprintf("\n");
692    }
693  }
694  #endif
695  void LSTM::ResizeForward(const NetworkIO &input) {
696    int rounded_inputs = gate_weights_[CI].RoundInputs(na_);
697    source_.Resize(input, rounded_inputs);
698    which_fg_.ResizeNoInit(input.Width(), ns_);
699    if (IsTraining()) {
700      state_.ResizeFloat(input, ns_);
701      for (int w = 0; w < WT_COUNT; ++w) {
702        if (w == GFS && !Is2D()) {
703          continue;
704        }
705        node_values_[w].ResizeFloat(input, ns_);
706      }
707    }
708  }
709  } 
</code></pre>
        </div>
        <div class="column">
            <h3>tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-plumbing.cpp</h3>
            <pre><code>1  #include "plumbing.h"
2  namespace tesseract {
3  Plumbing::Plumbing(const std::string &name) : Network(NT_PARALLEL, name, 0, 0) {}
4  void Plumbing::SetEnableTraining(TrainingState state) {
5    Network::SetEnableTraining(state);
6    for (auto &i : stack_) {
7      i->SetEnableTraining(state);
8    }
9  }
10  void Plumbing::SetNetworkFlags(uint32_t flags) {
11    Network::SetNetworkFlags(flags);
12    for (auto &i : stack_) {
<span onclick='openModal()' class='match'>13      i->SetNetworkFlags(flags);
14    }
15  }
16  int Plumbing::InitWeights(float range, TRand *randomizer) {
17    num_weights_ = 0;
</span>18    for (auto &i : stack_) {
19      num_weights_ += i->InitWeights(range, randomizer);
20    }
21    return num_weights_;
22  }
23  int Plumbing::RemapOutputs(int old_no, const std::vector<int> &code_map) {
24    num_weights_ = 0;
25    for (auto &i : stack_) {
26      num_weights_ += i->RemapOutputs(old_no, code_map);
27    }
28    return num_weights_;
29  }
30  void Plumbing::ConvertToInt() {
31    for (auto &i : stack_) {
32      i->ConvertToInt();
33    }
34  }
35  void Plumbing::SetRandomizer(TRand *randomizer) {
36    for (auto &i : stack_) {
37      i->SetRandomizer(randomizer);
38    }
39  }
40  void Plumbing::AddToStack(Network *network) {
41    if (stack_.empty()) {
42      ni_ = network->NumInputs();
43      no_ = network->NumOutputs();
44    } else if (type_ == NT_SERIES) {
45      ASSERT_HOST(no_ == network->NumInputs());
46      no_ = network->NumOutputs();
47    } else {
48      ASSERT_HOST(ni_ == network->NumInputs());
49      no_ += network->NumOutputs();
50    }
51    stack_.push_back(network);
52  }
53  bool Plumbing::SetupNeedsBackprop(bool needs_backprop) {
54    if (IsTraining()) {
55      needs_to_backprop_ = needs_backprop;
56      bool retval = needs_backprop;
57      for (auto &i : stack_) {
58        if (i->SetupNeedsBackprop(needs_backprop)) {
59          retval = true;
60        }
61      }
62      return retval;
63    }
64    needs_to_backprop_ = false;
65    return false;
66  }
67  int Plumbing::XScaleFactor() const {
68    return stack_[0]->XScaleFactor();
69  }
70  void Plumbing::CacheXScaleFactor(int factor) {
71    for (auto &i : stack_) {
72      i->CacheXScaleFactor(factor);
73    }
74  }
75  void Plumbing::DebugWeights() {
76    for (auto &i : stack_) {
77      i->DebugWeights();
78    }
79  }
80  void Plumbing::EnumerateLayers(const std::string *prefix, std::vector<std::string> &layers) const {
81    for (size_t i = 0; i < stack_.size(); ++i) {
82      std::string layer_name;
83      if (prefix) {
84        layer_name = *prefix;
85      }
86      layer_name += ":" + std::to_string(i);
87      if (stack_[i]->IsPlumbingType()) {
88        auto *plumbing = static_cast<Plumbing *>(stack_[i]);
89        plumbing->EnumerateLayers(&layer_name, layers);
90      } else {
91        layers.push_back(layer_name);
92      }
93    }
94  }
95  Network *Plumbing::GetLayer(const char *id) const {
96    char *next_id;
97    int index = strtol(id, &next_id, 10);
98    if (index < 0 || static_cast<unsigned>(index) >= stack_.size()) {
99      return nullptr;
100    }
101    if (stack_[index]->IsPlumbingType()) {
102      auto *plumbing = static_cast<Plumbing *>(stack_[index]);
103      ASSERT_HOST(*next_id == ':');
104      return plumbing->GetLayer(next_id + 1);
105    }
106    return stack_[index];
107  }
108  float *Plumbing::LayerLearningRatePtr(const char *id) {
109    char *next_id;
110    int index = strtol(id, &next_id, 10);
111    if (index < 0 || static_cast<unsigned>(index) >= stack_.size()) {
112      return nullptr;
113    }
114    if (stack_[index]->IsPlumbingType()) {
115      auto *plumbing = static_cast<Plumbing *>(stack_[index]);
116      ASSERT_HOST(*next_id == ':');
117      return plumbing->LayerLearningRatePtr(next_id + 1);
118    }
119    if (static_cast<unsigned>(index) >= learning_rates_.size()) {
120      return nullptr;
121    }
122    return &learning_rates_[index];
123  }
124  bool Plumbing::Serialize(TFile *fp) const {
125    if (!Network::Serialize(fp)) {
126      return false;
127    }
128    uint32_t size = stack_.size();
129    if (!fp->Serialize(&size)) {
130      return false;
131    }
132    for (uint32_t i = 0; i < size; ++i) {
133      if (!stack_[i]->Serialize(fp)) {
134        return false;
135      }
136    }
137    if ((network_flags_ & NF_LAYER_SPECIFIC_LR) && !fp->Serialize(learning_rates_)) {
138      return false;
139    }
140    return true;
141  }
142  bool Plumbing::DeSerialize(TFile *fp) {
143    for (auto data : stack_) {
144      delete data;
145    }
146    stack_.clear();
147    no_ = 0; 
148    uint32_t size;
149    if (!fp->DeSerialize(&size)) {
150      return false;
151    }
152    for (uint32_t i = 0; i < size; ++i) {
153      Network *network = CreateFromFile(fp);
154      if (network == nullptr) {
155        return false;
156      }
157      AddToStack(network);
158    }
159    if ((network_flags_ & NF_LAYER_SPECIFIC_LR) && !fp->DeSerialize(learning_rates_)) {
160      return false;
161    }
162    return true;
163  }
164  void Plumbing::Update(float learning_rate, float momentum, float adam_beta, int num_samples) {
165    for (size_t i = 0; i < stack_.size(); ++i) {
166      if (network_flags_ & NF_LAYER_SPECIFIC_LR) {
167        if (i < learning_rates_.size()) {
168          learning_rate = learning_rates_[i];
169        } else {
170          learning_rates_.push_back(learning_rate);
171        }
172      }
173      if (stack_[i]->IsTraining()) {
174        stack_[i]->Update(learning_rate, momentum, adam_beta, num_samples);
175      }
176    }
177  }
178  void Plumbing::CountAlternators(const Network &other, TFloat *same, TFloat *changed) const {
179    ASSERT_HOST(other.type() == type_);
180    const auto *plumbing = static_cast<const Plumbing *>(&other);
181    ASSERT_HOST(plumbing->stack_.size() == stack_.size());
182    for (size_t i = 0; i < stack_.size(); ++i) {
183      stack_[i]->CountAlternators(*plumbing->stack_[i], same, changed);
184    }
185  }
186  } 
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-lstm.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from tesseract-MDEwOlJlcG9zaXRvcnkyMjg4NzA5NA==-flat-plumbing.cpp</div>
                </div>
                <div class="column column_space"><pre><code>121      softmax_->SetEnableTraining(state);
122    }
123  }
124  int LSTM::InitWeights(float range, TRand *randomizer) {
125    Network::SetRandomizer(randomizer);
</pre></code></div>
                <div class="column column_space"><pre><code>13      i->SetNetworkFlags(flags);
14    }
15  }
16  int Plumbing::InitWeights(float range, TRand *randomizer) {
17    num_weights_ = 0;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    