
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 17, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_layers.hpp</h3>
            <pre><code>1  #ifndef CAFFE_MKLDNN_LAYERS_HPP_
2  #define CAFFE_MKLDNN_LAYERS_HPP_
3  #include <string>
4  #include <vector>
5  #include "boost/enable_shared_from_this.hpp"
6  #include "caffe/blob.hpp"
7  #include "caffe/common.hpp"
8  #include "caffe/engine_parser.hpp"
9  #include "caffe/layers/base_conv_layer.hpp"
10  #include "caffe/layers/conv_layer.hpp"
11  #include "caffe/layers/inner_product_layer.hpp"
12  #include "caffe/layers/neuron_layer.hpp"
13  #include "caffe/proto/caffe.pb.h"
14  #include "caffe/mkldnn_memory.hpp"
15  #include "mkldnn.hpp"
16  #include "caffe/util/performance.hpp"
17  using namespace mkldnn;
18  namespace caffe {
19  template <typename Dtype>
20  inline void info_mem_pd(shared_ptr<memory::primitive_desc> mem_pd, string name) {
21  #ifdef DEBUG        
22      LOG(INFO) << name;
23      switch (mem_pd->desc().data.format) {
24          case memory::format::nchw: LOG(INFO) << "format: nchw"; break;
25          case memory::format::nhwc: LOG(INFO) << "format: nhwc"; break;
26          case memory::format::nChw8c: LOG(INFO) << "format: nChw8c"; break;
27          case memory::format::nChw16c: LOG(INFO) << "format: nChw16c"; break;
28          case memory::format::nc: LOG(INFO) << "format: nc"; break;
29          default: assert(!"Error format");
30      }
31      switch (mem_pd->desc().data.data_type) {
32          case memory::data_type::f32: LOG(INFO) << "data_type: f32"; break;
33          case memory::data_type::u8: LOG(INFO) << "data_type: u8"; break;
34          case memory::data_type::s8: LOG(INFO) << "data_type: s8"; break;
35          case memory::data_type::s32: LOG(INFO) << "data_type: s32"; break;
36          default: assert(!"Error data_type");
37      }
38  #endif
39  }
40  template <typename Dtype>
41  class MKLDNNBatchNormLayer : public MKLDNNLayer<Dtype>, public Layer<Dtype> {
42  public:
43      explicit MKLDNNBatchNormLayer(const LayerParameter& param)
44          : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param)
45          , fwd_top_data(), fwd_bottom_data()
46          , bwd_top_diff(), bwd_bottom_diff()
47          , BatchNormFwd_pd(), BatchNormBwd_pd()
48          , scaleshift_memory(), bwd_scaleshift_diff_memory()
49          , output_memory(), bwd_bottom_diff_memory()
50          , input_primitive(), bwd_top_diff_primitive()
51          {
52            PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
53            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
54      }
55      ~MKLDNNBatchNormLayer() {}
56  #ifdef USE_MLSL
57      virtual bool ParamNeedReduce(int param_id) { return param_id >= 3; }
58  #endif
59  protected:
60      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
61      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
62      virtual inline const char* type() const { return "BatchNorm"; }
63      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
64      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
65      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
66                                  , const vector<Blob<Dtype>*>& bottom);
67      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
68                                  , const vector<Blob<Dtype>*>& bottom);
69  private:
70      void InitBatchNorm(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
71      void InitBatchNormBwd(const vector<Blob<Dtype>*>& top,
72              const vector<bool>& propagate_down,
73              const vector<Blob<Dtype>*>& bottom);
74      void InitBatchNormFwdPrimitive(int stats_batch_idx);
75      void InitBatchNormBwdPrimitive(int stats_batch_idx);
76      template <bool diff> shared_ptr<memory> GetStatsBatchMemory(
77        shared_ptr<MKLDNNMemoryDescriptor<Dtype, diff> > mkldnn_data, int idx);
78      void InitStatsBatchVars(int batch_size);
79      shared_ptr<MKLDNNData<Dtype> > fwd_top_data, fwd_bottom_data;
80      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff, bwd_bottom_diff;
81      shared_ptr<batch_normalization_forward::primitive_desc> BatchNormFwd_pd;
82      shared_ptr<batch_normalization_backward::primitive_desc> BatchNormBwd_pd;
83      vector<MKLDNNPrimitive<Dtype> > BatchNormFwd, BatchNormBwd;
84      vector<shared_ptr<memory> > mean_memory, variance_memory;
85      shared_ptr<memory> scaleshift_memory, bwd_scaleshift_diff_memory;
86      shared_ptr<memory> output_memory, bwd_bottom_diff_memory;
87      vector<shared_ptr<memory> > input_stats, output_stats, top_diff_stats, bottom_diff_stats;
88      shared_ptr<primitive> input_primitive, bwd_top_diff_primitive;
89      vector<int> shape_;
90      Dtype eps_, moving_average_fraction_;
91      bool use_weight_bias_, bias_term_, use_global_stats_;
92      int num_stats_batches_;
93      int stats_batch_size_;
94      shared_ptr<Blob<Dtype> > scaleshift_blob_;
95      shared_ptr<Blob<Dtype> > scaleshift_acc_;
96      Blob<Dtype> inplace_buffer;
97      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
98      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
99  };
100  template <typename Dtype>
101  class MKLDNNConvolutionLayer : public MKLDNNLayer<Dtype> , public ConvolutionLayer<Dtype> {
102  public:
103      explicit MKLDNNConvolutionLayer(const LayerParameter& param);
104      virtual ~MKLDNNConvolutionLayer() {}
105      int GetKernelWidth()  { return kernel_w_; }
106      int GetKernelHeight() { return kernel_h_; }
107      int GetKernelDepth()  { return kernel_d_; }
108      int GetStrideWidth()  { return stride_w_; }
109      int GetStrideHeight() { return stride_h_; }
110      int GetStrideDepth()  { return stride_d_; }
111      int GetPadWidth()     { return pad_w_; }
112      int GetPadHeight()    { return pad_h_; }
113      int GetPadDepth()     { return pad_d_; }
114  protected:
115      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
116      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
117      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
118                                  , const vector<Blob<Dtype>*>& bottom);
119      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
120                                  , const vector<Blob<Dtype>*>& bottom);
121      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
122      void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
123  private:
124      virtual void compute_output_shape();
125      virtual void init_properties(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
126      void InitConvolutionFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
127      void InitConvolutionBwd(const vector<Blob<Dtype>*>& top
128                          , const vector<bool>& propagate_down
129                          , const vector<Blob<Dtype>*>& bottom);
130      shared_ptr<MKLDNNData<Dtype> > fwd_bottom_data, fwd_top_data, fwd_weights_data, fwd_bias_data
131                      , bwdd_weights_data, bwdw_bottom_data;
132      shared_ptr<MKLDNNDiff<Dtype> > bwdd_bottom_diff, bwdd_top_diff
133                      , bwdw_top_diff, bwdw_weights_diff, bwdw_bias_diff;
134      shared_ptr<convolution_forward::primitive_desc> convFwd_pd;
135      shared_ptr<convolution_backward_data::primitive_desc> convBwdData_pd;
136      shared_ptr<convolution_backward_weights::primitive_desc> convBwdWeights_pd;
137      MKLDNNPrimitive<Dtype> convFwd, convBwdData, convBwdWeights;
138      shared_ptr<memory> fwd_top_data_memory, bwdd_bottom_diff_memory
139                      , bwdw_weights_diff_memory,  bwdw_bias_diff_memory;
140      shared_ptr<primitive> fwd_bottom_data_primitive, fwd_weights_data_primitive, fwd_bias_data_primitive
141                      , bwdd_top_diff_primitive, bwdd_weights_data_primitive
142                      , bwdw_top_diff_primitive, bwdw_bottom_data_primitive;
143      int32_t width_, height_, depth_, width_out_, height_out_, depth_out_, kernel_w_, kernel_h_, kernel_d_, stride_w_, stride_h_, stride_d_;
144      int  pad_w_, pad_h_, pad_d_;
145      mkldnn::algorithm  conv_algorithm;
146      shared_ptr<MKLDNNDiff<Dtype> > bwdw_weights_diff_iter, bwdw_bias_diff_iter;
147      shared_ptr<memory> bwdw_weights_diff_memory_iter, bwdw_bias_diff_memory_iter;
148      shared_ptr<Blob<Dtype> > bwdw_weights_diff_iter_blob, bwdw_bias_diff_iter_blob;
149      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
150      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
151      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_weights_);
152  };
153  template <typename Dtype>
154  class MKLDNNInnerProductLayer : public MKLDNNLayer<Dtype> , public InnerProductLayer<Dtype>  {
155  public:
156      explicit MKLDNNInnerProductLayer(const LayerParameter& param);
157      virtual ~MKLDNNInnerProductLayer();
158  protected:
159      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
160      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
161      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
162                                  , const vector<Blob<Dtype>*>& bottom);
163      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
164                                  , const vector<Blob<Dtype>*>& bottom);
165      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
166      void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
167  private:
168      void InitInnerProductFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
169      void InitInnerProductBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
170                                  , const vector<Blob<Dtype>*>& bottom);
171      shared_ptr<MKLDNNData<Dtype> > fwd_bottom_data, fwd_top_data, fwd_weights_data, fwd_bias_data
172                      , bwdd_weights_data, bwdw_bottom_data;
173      shared_ptr<MKLDNNDiff<Dtype> > bwdd_bottom_diff, bwdd_top_diff
174                      , bwdw_top_diff, bwdw_weights_diff, bwdw_bias_diff;
175      shared_ptr<inner_product_forward::primitive_desc> ipFwd_pd;
176      shared_ptr<inner_product_backward_data::primitive_desc> ipBwdData_pd;
177      shared_ptr<inner_product_backward_weights::primitive_desc> ipBwdWeights_pd;
178      MKLDNNPrimitive<Dtype> ipFwd, ipBwdData, ipBwdWeights;
179      shared_ptr<memory> fwd_top_data_memory, bwdd_bottom_diff_memory
180                      , bwdw_weights_diff_memory,  bwdw_bias_diff_memory;
181      shared_ptr<primitive> fwd_bottom_data_primitive, fwd_weights_data_primitive, fwd_bias_data_primitive
182                      , bwdd_top_diff_primitive, bwdd_weights_data_primitive
183                      , bwdw_top_diff_primitive, bwdw_bottom_data_primitive;
184      int32_t w_, h_;
185      shared_ptr<MKLDNNDiff<Dtype> > bwdw_weights_diff_iter, bwdw_bias_diff_iter;
186      shared_ptr<memory> bwdw_weights_diff_memory_iter, bwdw_bias_diff_memory_iter;
187      shared_ptr<Blob<Dtype> > bwdw_weights_diff_iter_blob, bwdw_bias_diff_iter_blob;
188      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
<span onclick='openModal()' class='match'>189      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
190      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_weights_);
191  };
192  template <typename Dtype>
193  class MKLDNNLRNLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype>  {
</span>194  public:
195      explicit MKLDNNLRNLayer(const LayerParameter& param);
196      virtual ~MKLDNNLRNLayer() {}
197  protected:
198      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
199      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
200      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
201      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
202                                  , const vector<Blob<Dtype>*>& bottom);
203      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
204      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
205                                  , const vector<Blob<Dtype>*>& bottom);
206      virtual inline const char* type() const { return "LRN"; }
207      virtual inline int ExactNumBottomBlobs() const { return 1; }
208      virtual inline int ExactNumTopBlobs() const { return 1; }
209  private:
210      void InitLRNFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
211      void InitLRNBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
212                                  , const vector<Blob<Dtype>*>& bottom);
213      shared_ptr<MKLDNNData<Dtype> > fwd_top_data, fwd_bottom_data;
214      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff, bwd_bottom_diff;
215      shared_ptr<lrn_forward::primitive_desc> lrnFwd_pd;
216      shared_ptr<lrn_backward::primitive_desc> lrnBwd_pd;
217      MKLDNNPrimitive<Dtype> lrnFwd;
218      MKLDNNPrimitive<Dtype> lrnBwd;
219      shared_ptr<memory::desc> bottom_md;
220      int fl;
221      float scale;
222      shared_ptr<memory> buffer;
223      MKLDNNPrimitive<Dtype> lrn_reorder;
224      shared_ptr<memory> fwd_top_data_memory, bwd_bottom_diff_memory, scratch_memory;
225      shared_ptr<primitive> fwd_bottom_data_primitive, bwd_top_diff_primitive;
226      Dtype alpha_, beta_, k_;
227      int size_, num_, width_, height_, channels_;
228      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
229      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
230  };
231  template <typename Dtype>
232  class MKLDNNPoolingLayer : public MKLDNNLayer<Dtype>, public Layer<Dtype>  {
233  public:
234      explicit MKLDNNPoolingLayer(const LayerParameter& param)
235              : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param)
236              , fwd_bottom_data(), fwd_top_data()
237              , bwd_top_diff(), bwd_bottom_diff()
238              , poolingFwd_pd()
239              , poolingBwd_pd()
240              , indices_pd()
241              , indices_memory(), fwd_top_data_memory(), bwd_bottom_diff_memory()
242              , fwd_bottom_data_primitive(), bwd_top_diff_primitive()
243              , num_(0), channels_(0), width_(0), height_(0), width_out_(0), height_out_(0)
244              , kernel_w_(0), kernel_h_(0), stride_w_(0), stride_h_(0)
245              , pad_t_(0),pad_b_(0), pad_l_(0), pad_r_(0)
246              , global_pooling_(false)
247              , force_exclude_padding_flag_(false)
248              {
249                PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
250                PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
251              }
252      ~MKLDNNPoolingLayer() {}
253  protected:
254      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
255      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
256      virtual inline const char* type() const { return "Pooling"; }
257      virtual inline int ExactNumBottomBlobs() const { return 1; }
258      virtual inline int MinTopBlobs() const { return 1; }
259      virtual inline int MaxTopBlobs() const {
260          return (this->layer_param_.pooling_param().pool() == PoolingParameter_PoolMethod_MAX) ? 2 : 1;
261      }
262  protected:
263      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
264      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
265      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top,const vector<bool>& propagate_down
266                                  ,const vector<Blob<Dtype>*>& bottom);
267      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
268                                  ,const vector<Blob<Dtype>*>& bottom);
269      virtual void compute_output_shape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
270  private:
271      void InitPoolingFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
272      void InitPoolingBwd(const vector<Blob<Dtype>*>& bottom
273                          , const vector<bool>& propagate_down
274                          , const vector<Blob<Dtype>*>& top);
275      shared_ptr<MKLDNNData<Dtype>> fwd_bottom_data, fwd_top_data;
276      shared_ptr<MKLDNNDiff<Dtype>> bwd_top_diff, bwd_bottom_diff;
277      shared_ptr<pooling_forward::primitive_desc> poolingFwd_pd;
278      shared_ptr<pooling_backward::primitive_desc> poolingBwd_pd;
279      MKLDNNPrimitive<Dtype> poolingFwd, poolingBwd;
280      shared_ptr<memory::primitive_desc> indices_pd;
281      shared_ptr<memory> indices_memory, fwd_top_data_memory, bwd_bottom_diff_memory;
282      shared_ptr<primitive> fwd_bottom_data_primitive, bwd_top_diff_primitive;
283      int32_t num_, channels_, width_, height_, width_out_, height_out_;
284      int32_t kernel_w_, kernel_h_, stride_w_, stride_h_;
285      int32_t  pad_t_, pad_b_, pad_l_, pad_r_;
286      Blob<uint32_t> max_idx_;
287      bool global_pooling_;
288      bool force_exclude_padding_flag_;
289      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
290      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
291  };
292  template <typename Dtype>
293  class MKLDNNReLULayer : public MKLDNNLayer<Dtype> , public NeuronLayer<Dtype>  {
294  public:
295    explicit MKLDNNReLULayer(const LayerParameter& param)
296      : MKLDNNLayer<Dtype>(param), NeuronLayer<Dtype>(param)
297      , fwd_top_data(), fwd_bottom_data()
298      , bwd_top_diff(), bwd_bottom_diff()
299      , reluFwd_pd(), reluBwd_pd()
300      , fwd_top_data_memory(), bwd_bottom_diff_memory()
301      , fwd_bottom_data_primitive(), bwd_top_diff_primitive()
302      , shape_(0)
303    {
304      PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
305      PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
306    }
307    ~MKLDNNReLULayer() {}
308  protected:
309      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
310      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
311      virtual inline const char* type() const { return "ReLU"; }
312      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
313      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
314      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
315                                  , const vector<Blob<Dtype>*>& bottom);
316      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
317                                  , const vector<Blob<Dtype>*>& bottom);
318  private:
319      void InitReLUFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
320      void InitReLUBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
321                                  , const vector<Blob<Dtype>*>& bottom);
322      shared_ptr<MKLDNNData<Dtype> > fwd_top_data, fwd_bottom_data, bwd_bottom_data;
323      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff, bwd_bottom_diff;
324      shared_ptr<eltwise_forward::primitive_desc> reluFwd_pd;
325      shared_ptr<eltwise_backward::primitive_desc> reluBwd_pd;
326      MKLDNNPrimitive<Dtype> reluFwd, reluBwd;
327      shared_ptr<memory> fwd_top_data_memory, bwd_bottom_diff_memory;
328      shared_ptr<primitive> fwd_bottom_data_primitive, bwd_top_diff_primitive, bwd_bottom_data_primitive;
329      vector<int> shape_;
330      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
331      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
332  };
333  template <typename Dtype>
334  class MKLDNNConcatLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype> {
335  public:
336      explicit MKLDNNConcatLayer(const LayerParameter& param)
337              : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param),
338              concatFwd_pd(), fwd_output_memory(),
339              bwd_reorder_input_memory(), bwd_reorder_output_memory(),
340              fwd_top_data(), fwd_bottom_data(), split_dims() {
341                PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
342                PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
343      }
344  protected:
345      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
346      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
347      virtual inline const char* type() const { return "Concat"; }
348      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
349      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
350      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
351                                  , const vector<Blob<Dtype>*>& bottom);
352      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
353                                  , const vector<Blob<Dtype>*>& bottom);
354  private:
355      void InitConcatFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
356      void InitConcatBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
357                                  , const vector<Blob<Dtype>*>& bottom);
358      shared_ptr<concat::primitive_desc> concatFwd_pd;
359      shared_ptr<memory> fwd_output_memory;
360      shared_ptr<primitive> bwd_reorder_input_memory;
361      vector<shared_ptr<memory>> bwd_reorder_output_memory;
362      vector<shared_ptr<memory>> bwd_bottom_memory_;
363      vector<shared_ptr<primitive>> fwd_input_primitives_;
364      vector<primitive::at> fwd_input_primitives_at_;
365      MKLDNNPrimitive<Dtype> concatFwd;
366      shared_ptr<MKLDNNData<Dtype> > fwd_top_data;
367      vector<shared_ptr<MKLDNNData<Dtype> > > fwd_bottom_data;
368      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff;
369      vector<shared_ptr<MKLDNNDiff<Dtype> > > bwd_bottom_diff;
370      vector<MKLDNNPrimitive<Dtype> > reorders;
371      vector<int> split_dims;
372      bool in_place_;
373      int32_t num_concats_;
374      vector<int> shape_;
375      int concat_dimension;
376      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
377      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
378  };
379  template <typename Dtype>
380  class MKLDNNSplitLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype> {
381  public:
382      explicit MKLDNNSplitLayer(const LayerParameter& param)
383              : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param),
384                splitBwd_pd_(), bwd_bottom_diff_memory_()
385              {
386                PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
387      }
388      ~MKLDNNSplitLayer();
389  protected:
390      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
391      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
392      virtual inline const char* type() const { return "Split"; }
393      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
394      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
395      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
396                                  , const vector<Blob<Dtype>*>& bottom);
397      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
398                                  , const vector<Blob<Dtype>*>& bottom);
399  private:
400      void InitSplitFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
401      void InitSplitBwd(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom);
402    private:
403      vector<size_t> sizes_src_;
404      vector<size_t> strides_src_;
405      MKLDNNPrimitive<Dtype> splitBwd_;
406      shared_ptr<sum::primitive_desc> splitBwd_pd_;
407      shared_ptr<memory> bwd_bottom_diff_memory_;
408      shared_ptr<MKLDNNDiff<Dtype> > bwd_bottom_diff_;
409      vector<shared_ptr<primitive>> bwd_top_diff_primitives_;
410      vector<primitive::at> bwd_top_diffs_primitives_at_;
411      vector<shared_ptr<MKLDNNDiff<Dtype> > > bwd_top_diffs_;
412      bool first;
413      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
414  };
415  template <typename Dtype>
416  class MKLDNNEltwiseLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype>  {
417  public:
418    explicit MKLDNNEltwiseLayer(const LayerParameter& param)
419      : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param)
420      , fwd_top_data(), fwd_bottom_data()
421      , eltwiseFwd_pd()
422      , fwd_top_data_memory()
423      , fwd_bottom_data_primitives_()
424      , shape_(0)
425      , num_bottoms_(0)
426    {
427      PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
428    }
429    ~MKLDNNEltwiseLayer() {}
430  protected:
431      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
432      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
433      virtual inline const char* type() const { return "Eltwise"; }
434      virtual inline int MinBottomBlobs() const { return 2; }
435      virtual inline int ExactNumTopBlobs() const { return 1; }
436      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
437      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
438      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
439                                  , const vector<Blob<Dtype>*>& bottom);
440      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
441                                  , const vector<Blob<Dtype>*>& bottom);
442  private:
443      void InitEltwiseFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
444      void InitEltwiseBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
445                                  , const vector<Blob<Dtype>*>& bottom);
446      shared_ptr<MKLDNNData<Dtype> > fwd_top_data;
447      vector<shared_ptr<MKLDNNData<Dtype> > > fwd_bottom_data;
448      shared_ptr<sum::primitive_desc> eltwiseFwd_pd;
449      MKLDNNPrimitive<Dtype> eltwiseFwd;
450      shared_ptr<memory> fwd_top_data_memory;
451      vector<shared_ptr<primitive>> fwd_bottom_data_primitives_;
452      vector<primitive::at> fwd_bottom_data_primitives_at_;
453      EltwiseParameter_EltwiseOp op_;
454      vector<Dtype> coeffs_;
455      Blob<int> max_idx_;
456      vector<int> shape_;
457      int32_t num_bottoms_;
458      bool stable_prod_grad_;
459      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
460  };
461  }  
462  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_layers.hpp</h3>
            <pre><code>1  #ifndef CAFFE_MKLDNN_LAYERS_HPP_
2  #define CAFFE_MKLDNN_LAYERS_HPP_
3  #include <string>
4  #include <vector>
5  #include "boost/enable_shared_from_this.hpp"
6  #include "caffe/blob.hpp"
7  #include "caffe/common.hpp"
8  #include "caffe/engine_parser.hpp"
9  #include "caffe/layers/base_conv_layer.hpp"
10  #include "caffe/layers/conv_layer.hpp"
11  #include "caffe/layers/inner_product_layer.hpp"
12  #include "caffe/layers/neuron_layer.hpp"
13  #include "caffe/proto/caffe.pb.h"
14  #include "caffe/mkldnn_memory.hpp"
15  #include "mkldnn.hpp"
16  #include "caffe/util/performance.hpp"
17  using namespace mkldnn;
18  namespace caffe {
19  template <typename Dtype>
20  inline void info_mem_pd(shared_ptr<memory::primitive_desc> mem_pd, string name) {
21  #ifdef DEBUG        
22      LOG(INFO) << name;
23      switch (mem_pd->desc().data.format) {
24          case memory::format::nchw: LOG(INFO) << "format: nchw"; break;
25          case memory::format::nhwc: LOG(INFO) << "format: nhwc"; break;
26          case memory::format::nChw8c: LOG(INFO) << "format: nChw8c"; break;
27          case memory::format::nChw16c: LOG(INFO) << "format: nChw16c"; break;
28          case memory::format::nc: LOG(INFO) << "format: nc"; break;
29          default: assert(!"Error format");
30      }
31      switch (mem_pd->desc().data.data_type) {
32          case memory::data_type::f32: LOG(INFO) << "data_type: f32"; break;
33          case memory::data_type::u8: LOG(INFO) << "data_type: u8"; break;
34          case memory::data_type::s8: LOG(INFO) << "data_type: s8"; break;
35          case memory::data_type::s32: LOG(INFO) << "data_type: s32"; break;
36          default: assert(!"Error data_type");
37      }
38  #endif
39  }
40  template <typename Dtype>
41  class MKLDNNBatchNormLayer : public MKLDNNLayer<Dtype>, public Layer<Dtype> {
42  public:
43      explicit MKLDNNBatchNormLayer(const LayerParameter& param)
44          : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param)
45          , fwd_top_data(), fwd_bottom_data()
46          , bwd_top_diff(), bwd_bottom_diff()
47          , BatchNormFwd_pd(), BatchNormBwd_pd()
48          , scaleshift_memory(), bwd_scaleshift_diff_memory()
49          , output_memory(), bwd_bottom_diff_memory()
50          , input_primitive(), bwd_top_diff_primitive()
51          {
52            PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
53            PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
54      }
55      ~MKLDNNBatchNormLayer() {}
56  #ifdef USE_MLSL
57      virtual bool ParamNeedReduce(int param_id) { return param_id >= 3; }
58  #endif
59  protected:
60      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
61      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
62      virtual inline const char* type() const { return "BatchNorm"; }
63      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
64      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
65      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
66                                  , const vector<Blob<Dtype>*>& bottom);
67      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
68                                  , const vector<Blob<Dtype>*>& bottom);
69  private:
70      void InitBatchNorm(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
71      void InitBatchNormBwd(const vector<Blob<Dtype>*>& top,
72              const vector<bool>& propagate_down,
73              const vector<Blob<Dtype>*>& bottom);
74      void InitBatchNormFwdPrimitive(int stats_batch_idx);
75      void InitBatchNormBwdPrimitive(int stats_batch_idx);
76      template <bool diff> shared_ptr<memory> GetStatsBatchMemory(
77        shared_ptr<MKLDNNMemoryDescriptor<Dtype, diff> > mkldnn_data, int idx);
78      void InitStatsBatchVars(int batch_size);
79      shared_ptr<MKLDNNData<Dtype> > fwd_top_data, fwd_bottom_data;
80      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff, bwd_bottom_diff;
81      shared_ptr<batch_normalization_forward::primitive_desc> BatchNormFwd_pd;
82      shared_ptr<batch_normalization_backward::primitive_desc> BatchNormBwd_pd;
83      vector<MKLDNNPrimitive<Dtype> > BatchNormFwd, BatchNormBwd;
84      vector<shared_ptr<memory> > mean_memory, variance_memory;
85      shared_ptr<memory> scaleshift_memory, bwd_scaleshift_diff_memory;
86      shared_ptr<memory> output_memory, bwd_bottom_diff_memory;
87      vector<shared_ptr<memory> > input_stats, output_stats, top_diff_stats, bottom_diff_stats;
88      shared_ptr<primitive> input_primitive, bwd_top_diff_primitive;
89      vector<int> shape_;
90      Dtype eps_, moving_average_fraction_;
91      bool use_weight_bias_, bias_term_, use_global_stats_;
92      int num_stats_batches_;
93      int stats_batch_size_;
94      shared_ptr<Blob<Dtype> > scaleshift_blob_;
95      shared_ptr<Blob<Dtype> > scaleshift_acc_;
96      Blob<Dtype> inplace_buffer;
97      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
98      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
99  };
100  template <typename Dtype>
101  class MKLDNNConvolutionLayer : public MKLDNNLayer<Dtype> , public ConvolutionLayer<Dtype> {
102  public:
103      explicit MKLDNNConvolutionLayer(const LayerParameter& param);
104      virtual ~MKLDNNConvolutionLayer() {}
105      int GetKernelWidth()  { return kernel_w_; }
106      int GetKernelHeight() { return kernel_h_; }
107      int GetKernelDepth()  { return kernel_d_; }
108      int GetStrideWidth()  { return stride_w_; }
109      int GetStrideHeight() { return stride_h_; }
110      int GetStrideDepth()  { return stride_d_; }
111      int GetPadWidth()     { return pad_w_; }
112      int GetPadHeight()    { return pad_h_; }
113      int GetPadDepth()     { return pad_d_; }
114  protected:
115      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
116      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
117      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
118                                  , const vector<Blob<Dtype>*>& bottom);
119      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
120                                  , const vector<Blob<Dtype>*>& bottom);
121      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
122      void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
123  private:
124      virtual void compute_output_shape();
125      virtual void init_properties(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
126      void InitConvolutionFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
127      void InitConvolutionBwd(const vector<Blob<Dtype>*>& top
128                          , const vector<bool>& propagate_down
129                          , const vector<Blob<Dtype>*>& bottom);
130      shared_ptr<MKLDNNData<Dtype> > fwd_bottom_data, fwd_top_data, fwd_weights_data, fwd_bias_data
131                      , bwdd_weights_data, bwdw_bottom_data;
132      shared_ptr<MKLDNNDiff<Dtype> > bwdd_bottom_diff, bwdd_top_diff
133                      , bwdw_top_diff, bwdw_weights_diff, bwdw_bias_diff;
134      shared_ptr<convolution_forward::primitive_desc> convFwd_pd;
135      shared_ptr<convolution_backward_data::primitive_desc> convBwdData_pd;
136      shared_ptr<convolution_backward_weights::primitive_desc> convBwdWeights_pd;
137      MKLDNNPrimitive<Dtype> convFwd, convBwdData, convBwdWeights;
138      shared_ptr<memory> fwd_top_data_memory, bwdd_bottom_diff_memory
139                      , bwdw_weights_diff_memory,  bwdw_bias_diff_memory;
140      shared_ptr<primitive> fwd_bottom_data_primitive, fwd_weights_data_primitive, fwd_bias_data_primitive
141                      , bwdd_top_diff_primitive, bwdd_weights_data_primitive
142                      , bwdw_top_diff_primitive, bwdw_bottom_data_primitive;
143      int32_t width_, height_, depth_, width_out_, height_out_, depth_out_, kernel_w_, kernel_h_, kernel_d_, stride_w_, stride_h_, stride_d_;
144      int  pad_w_, pad_h_, pad_d_;
145      mkldnn::algorithm  conv_algorithm;
146      shared_ptr<MKLDNNDiff<Dtype> > bwdw_weights_diff_iter, bwdw_bias_diff_iter;
147      shared_ptr<memory> bwdw_weights_diff_memory_iter, bwdw_bias_diff_memory_iter;
148      shared_ptr<Blob<Dtype> > bwdw_weights_diff_iter_blob, bwdw_bias_diff_iter_blob;
149      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
150      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
151      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_weights_);
152  };
153  template <typename Dtype>
154  class MKLDNNInnerProductLayer : public MKLDNNLayer<Dtype> , public InnerProductLayer<Dtype>  {
155  public:
156      explicit MKLDNNInnerProductLayer(const LayerParameter& param);
157      virtual ~MKLDNNInnerProductLayer();
158  protected:
159      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
160      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
161      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
162                                  , const vector<Blob<Dtype>*>& bottom);
163      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
164                                  , const vector<Blob<Dtype>*>& bottom);
165      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
166      void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
167  private:
168      void InitInnerProductFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
169      void InitInnerProductBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
170                                  , const vector<Blob<Dtype>*>& bottom);
171      shared_ptr<MKLDNNData<Dtype> > fwd_bottom_data, fwd_top_data, fwd_weights_data, fwd_bias_data
172                      , bwdd_weights_data, bwdw_bottom_data;
173      shared_ptr<MKLDNNDiff<Dtype> > bwdd_bottom_diff, bwdd_top_diff
174                      , bwdw_top_diff, bwdw_weights_diff, bwdw_bias_diff;
175      shared_ptr<inner_product_forward::primitive_desc> ipFwd_pd;
176      shared_ptr<inner_product_backward_data::primitive_desc> ipBwdData_pd;
177      shared_ptr<inner_product_backward_weights::primitive_desc> ipBwdWeights_pd;
178      MKLDNNPrimitive<Dtype> ipFwd, ipBwdData, ipBwdWeights;
179      shared_ptr<memory> fwd_top_data_memory, bwdd_bottom_diff_memory
180                      , bwdw_weights_diff_memory,  bwdw_bias_diff_memory;
181      shared_ptr<primitive> fwd_bottom_data_primitive, fwd_weights_data_primitive, fwd_bias_data_primitive
182                      , bwdd_top_diff_primitive, bwdd_weights_data_primitive
183                      , bwdw_top_diff_primitive, bwdw_bottom_data_primitive;
184      int32_t w_, h_;
185      shared_ptr<MKLDNNDiff<Dtype> > bwdw_weights_diff_iter, bwdw_bias_diff_iter;
186      shared_ptr<memory> bwdw_weights_diff_memory_iter, bwdw_bias_diff_memory_iter;
187      shared_ptr<Blob<Dtype> > bwdw_weights_diff_iter_blob, bwdw_bias_diff_iter_blob;
188      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
189      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
190      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_weights_);
191  };
192  template <typename Dtype>
193  class MKLDNNLRNLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype>  {
194  public:
195      explicit MKLDNNLRNLayer(const LayerParameter& param);
196      virtual ~MKLDNNLRNLayer() {}
197  protected:
198      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
199      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
200      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
201      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
202                                  , const vector<Blob<Dtype>*>& bottom);
203      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
204      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
205                                  , const vector<Blob<Dtype>*>& bottom);
206      virtual inline const char* type() const { return "LRN"; }
207      virtual inline int ExactNumBottomBlobs() const { return 1; }
208      virtual inline int ExactNumTopBlobs() const { return 1; }
209  private:
210      void InitLRNFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
211      void InitLRNBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
212                                  , const vector<Blob<Dtype>*>& bottom);
213      shared_ptr<MKLDNNData<Dtype> > fwd_top_data, fwd_bottom_data;
214      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff, bwd_bottom_diff;
215      shared_ptr<lrn_forward::primitive_desc> lrnFwd_pd;
216      shared_ptr<lrn_backward::primitive_desc> lrnBwd_pd;
217      MKLDNNPrimitive<Dtype> lrnFwd;
218      MKLDNNPrimitive<Dtype> lrnBwd;
219      shared_ptr<memory::desc> bottom_md;
220      int fl;
221      float scale;
222      shared_ptr<memory> buffer;
223      MKLDNNPrimitive<Dtype> lrn_reorder;
224      shared_ptr<memory> fwd_top_data_memory, bwd_bottom_diff_memory, scratch_memory;
225      shared_ptr<primitive> fwd_bottom_data_primitive, bwd_top_diff_primitive;
226      Dtype alpha_, beta_, k_;
227      int size_, num_, width_, height_, channels_;
228      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
229      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
230  };
231  template <typename Dtype>
232  class MKLDNNPoolingLayer : public MKLDNNLayer<Dtype>, public Layer<Dtype>  {
233  public:
234      explicit MKLDNNPoolingLayer(const LayerParameter& param)
235              : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param)
236              , fwd_bottom_data(), fwd_top_data()
237              , bwd_top_diff(), bwd_bottom_diff()
238              , poolingFwd_pd()
239              , poolingBwd_pd()
240              , indices_pd()
241              , indices_memory(), fwd_top_data_memory(), bwd_bottom_diff_memory()
242              , fwd_bottom_data_primitive(), bwd_top_diff_primitive()
243              , num_(0), channels_(0), width_(0), height_(0), width_out_(0), height_out_(0)
244              , kernel_w_(0), kernel_h_(0), stride_w_(0), stride_h_(0)
245              , pad_t_(0),pad_b_(0), pad_l_(0), pad_r_(0)
246              , global_pooling_(false)
247              , force_exclude_padding_flag_(false)
248              {
249                PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
250                PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
251              }
252      ~MKLDNNPoolingLayer() {}
253  protected:
254      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
255      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
256      virtual inline const char* type() const { return "Pooling"; }
257      virtual inline int ExactNumBottomBlobs() const { return 1; }
258      virtual inline int MinTopBlobs() const { return 1; }
259      virtual inline int MaxTopBlobs() const {
260          return (this->layer_param_.pooling_param().pool() == PoolingParameter_PoolMethod_MAX) ? 2 : 1;
261      }
262  protected:
263      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
264      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
265      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top,const vector<bool>& propagate_down
266                                  ,const vector<Blob<Dtype>*>& bottom);
267      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
268                                  ,const vector<Blob<Dtype>*>& bottom);
269      virtual void compute_output_shape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
270  private:
271      void InitPoolingFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
272      void InitPoolingBwd(const vector<Blob<Dtype>*>& bottom
273                          , const vector<bool>& propagate_down
274                          , const vector<Blob<Dtype>*>& top);
275      shared_ptr<MKLDNNData<Dtype>> fwd_bottom_data, fwd_top_data;
276      shared_ptr<MKLDNNDiff<Dtype>> bwd_top_diff, bwd_bottom_diff;
277      shared_ptr<pooling_forward::primitive_desc> poolingFwd_pd;
278      shared_ptr<pooling_backward::primitive_desc> poolingBwd_pd;
279      MKLDNNPrimitive<Dtype> poolingFwd, poolingBwd;
280      shared_ptr<memory::primitive_desc> indices_pd;
281      shared_ptr<memory> indices_memory, fwd_top_data_memory, bwd_bottom_diff_memory;
282      shared_ptr<primitive> fwd_bottom_data_primitive, bwd_top_diff_primitive;
283      int32_t num_, channels_, width_, height_, width_out_, height_out_;
284      int32_t kernel_w_, kernel_h_, stride_w_, stride_h_;
285      int32_t  pad_t_, pad_b_, pad_l_, pad_r_;
286      Blob<uint32_t> max_idx_;
287      bool global_pooling_;
288      bool force_exclude_padding_flag_;
<span onclick='openModal()' class='match'>289      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
290      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
291  };
292  template <typename Dtype>
293  class MKLDNNReLULayer : public MKLDNNLayer<Dtype> , public NeuronLayer<Dtype>  {
</span>294  public:
295    explicit MKLDNNReLULayer(const LayerParameter& param)
296      : MKLDNNLayer<Dtype>(param), NeuronLayer<Dtype>(param)
297      , fwd_top_data(), fwd_bottom_data()
298      , bwd_top_diff(), bwd_bottom_diff()
299      , reluFwd_pd(), reluBwd_pd()
300      , fwd_top_data_memory(), bwd_bottom_diff_memory()
301      , fwd_bottom_data_primitive(), bwd_top_diff_primitive()
302      , shape_(0)
303    {
304      PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
305      PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
306    }
307    ~MKLDNNReLULayer() {}
308  protected:
309      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
310      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
311      virtual inline const char* type() const { return "ReLU"; }
312      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
313      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
314      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
315                                  , const vector<Blob<Dtype>*>& bottom);
316      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
317                                  , const vector<Blob<Dtype>*>& bottom);
318  private:
319      void InitReLUFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
320      void InitReLUBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
321                                  , const vector<Blob<Dtype>*>& bottom);
322      shared_ptr<MKLDNNData<Dtype> > fwd_top_data, fwd_bottom_data, bwd_bottom_data;
323      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff, bwd_bottom_diff;
324      shared_ptr<eltwise_forward::primitive_desc> reluFwd_pd;
325      shared_ptr<eltwise_backward::primitive_desc> reluBwd_pd;
326      MKLDNNPrimitive<Dtype> reluFwd, reluBwd;
327      shared_ptr<memory> fwd_top_data_memory, bwd_bottom_diff_memory;
328      shared_ptr<primitive> fwd_bottom_data_primitive, bwd_top_diff_primitive, bwd_bottom_data_primitive;
329      vector<int> shape_;
330      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
331      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
332  };
333  template <typename Dtype>
334  class MKLDNNConcatLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype> {
335  public:
336      explicit MKLDNNConcatLayer(const LayerParameter& param)
337              : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param),
338              concatFwd_pd(), fwd_output_memory(),
339              bwd_reorder_input_memory(), bwd_reorder_output_memory(),
340              fwd_top_data(), fwd_bottom_data(), split_dims() {
341                PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
342                PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
343      }
344  protected:
345      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
346      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
347      virtual inline const char* type() const { return "Concat"; }
348      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
349      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
350      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
351                                  , const vector<Blob<Dtype>*>& bottom);
352      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
353                                  , const vector<Blob<Dtype>*>& bottom);
354  private:
355      void InitConcatFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
356      void InitConcatBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
357                                  , const vector<Blob<Dtype>*>& bottom);
358      shared_ptr<concat::primitive_desc> concatFwd_pd;
359      shared_ptr<memory> fwd_output_memory;
360      shared_ptr<primitive> bwd_reorder_input_memory;
361      vector<shared_ptr<memory>> bwd_reorder_output_memory;
362      vector<shared_ptr<memory>> bwd_bottom_memory_;
363      vector<shared_ptr<primitive>> fwd_input_primitives_;
364      vector<primitive::at> fwd_input_primitives_at_;
365      MKLDNNPrimitive<Dtype> concatFwd;
366      shared_ptr<MKLDNNData<Dtype> > fwd_top_data;
367      vector<shared_ptr<MKLDNNData<Dtype> > > fwd_bottom_data;
368      shared_ptr<MKLDNNDiff<Dtype> > bwd_top_diff;
369      vector<shared_ptr<MKLDNNDiff<Dtype> > > bwd_bottom_diff;
370      vector<MKLDNNPrimitive<Dtype> > reorders;
371      vector<int> split_dims;
372      bool in_place_;
373      int32_t num_concats_;
374      vector<int> shape_;
375      int concat_dimension;
376      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
377      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
378  };
379  template <typename Dtype>
380  class MKLDNNSplitLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype> {
381  public:
382      explicit MKLDNNSplitLayer(const LayerParameter& param)
383              : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param),
384                splitBwd_pd_(), bwd_bottom_diff_memory_()
385              {
386                PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
387      }
388      ~MKLDNNSplitLayer();
389  protected:
390      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
391      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
392      virtual inline const char* type() const { return "Split"; }
393      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
394      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
395      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
396                                  , const vector<Blob<Dtype>*>& bottom);
397      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
398                                  , const vector<Blob<Dtype>*>& bottom);
399  private:
400      void InitSplitFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
401      void InitSplitBwd(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom);
402    private:
403      vector<size_t> sizes_src_;
404      vector<size_t> strides_src_;
405      MKLDNNPrimitive<Dtype> splitBwd_;
406      shared_ptr<sum::primitive_desc> splitBwd_pd_;
407      shared_ptr<memory> bwd_bottom_diff_memory_;
408      shared_ptr<MKLDNNDiff<Dtype> > bwd_bottom_diff_;
409      vector<shared_ptr<primitive>> bwd_top_diff_primitives_;
410      vector<primitive::at> bwd_top_diffs_primitives_at_;
411      vector<shared_ptr<MKLDNNDiff<Dtype> > > bwd_top_diffs_;
412      bool first;
413      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
414  };
415  template <typename Dtype>
416  class MKLDNNEltwiseLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype>  {
417  public:
418    explicit MKLDNNEltwiseLayer(const LayerParameter& param)
419      : MKLDNNLayer<Dtype>(param), Layer<Dtype>(param)
420      , fwd_top_data(), fwd_bottom_data()
421      , eltwiseFwd_pd()
422      , fwd_top_data_memory()
423      , fwd_bottom_data_primitives_()
424      , shape_(0)
425      , num_bottoms_(0)
426    {
427      PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
428    }
429    ~MKLDNNEltwiseLayer() {}
430  protected:
431      virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
432      virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
433      virtual inline const char* type() const { return "Eltwise"; }
434      virtual inline int MinBottomBlobs() const { return 2; }
435      virtual inline int ExactNumTopBlobs() const { return 1; }
436      virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
437      virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
438      virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
439                                  , const vector<Blob<Dtype>*>& bottom);
440      virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
441                                  , const vector<Blob<Dtype>*>& bottom);
442  private:
443      void InitEltwiseFwd(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
444      void InitEltwiseBwd(const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down
445                                  , const vector<Blob<Dtype>*>& bottom);
446      shared_ptr<MKLDNNData<Dtype> > fwd_top_data;
447      vector<shared_ptr<MKLDNNData<Dtype> > > fwd_bottom_data;
448      shared_ptr<sum::primitive_desc> eltwiseFwd_pd;
449      MKLDNNPrimitive<Dtype> eltwiseFwd;
450      shared_ptr<memory> fwd_top_data_memory;
451      vector<shared_ptr<primitive>> fwd_bottom_data_primitives_;
452      vector<primitive::at> fwd_bottom_data_primitives_at_;
453      EltwiseParameter_EltwiseOp op_;
454      vector<Dtype> coeffs_;
455      Blob<int> max_idx_;
456      vector<int> shape_;
457      int32_t num_bottoms_;
458      bool stable_prod_grad_;
459      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
460  };
461  }  
462  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_layers.hpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_layers.hpp</div>
                </div>
                <div class="column column_space"><pre><code>189      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
190      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_weights_);
191  };
192  template <typename Dtype>
193  class MKLDNNLRNLayer : public MKLDNNLayer<Dtype> , public Layer<Dtype>  {
</pre></code></div>
                <div class="column column_space"><pre><code>289      PERFORMANCE_EVENT_ID_DECL(perf_id_fw_);
290      PERFORMANCE_EVENT_ID_DECL(perf_id_bw_);
291  };
292  template <typename Dtype>
293  class MKLDNNReLULayer : public MKLDNNLayer<Dtype> , public NeuronLayer<Dtype>  {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    