
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 22, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_batch_norm_layer.cpp</h3>
            <pre><code>1  #if defined(MKL2017_SUPPORTED)
2  #include <vector>
3  #include "caffe/filler.hpp"
4  #include "caffe/layer.hpp"
5  #include "caffe/layers/mkl_layers.hpp"
6  #include "caffe/util/math_functions.hpp"
7  #include "caffe/util/performance.hpp"
8  namespace caffe {
9  template <typename Dtype>
10  MKLBatchNormLayer<Dtype>::~MKLBatchNormLayer() {
11    dnnDelete<Dtype>(batchNormFwd);
12    dnnDelete<Dtype>(batchNormFwdInference);
13    dnnDelete<Dtype>(batchNormBwd);
14    dnnLayoutDelete<Dtype>(layout_usr_);
15    for (int i = 0; i < mean_buffers_.size(); i++) {
16      dnnReleaseBuffer<Dtype>(mean_buffers_[i]);
17    }
18    for (int i = 0; i < variance_buffers_.size(); i++) {
19      dnnReleaseBuffer<Dtype>(variance_buffers_[i]);
20    }
21    dnnReleaseBuffer<Dtype>(scaleShift_buffer_);
22    dnnReleaseBuffer<Dtype>(diffScaleShift_buffer_);
23  }
24  template <typename Dtype>
25  void MKLBatchNormLayer<Dtype>::Init(const vector<Blob<Dtype>*>& bottom,
26        const vector<Blob<Dtype>*>& top) {
27    moving_average_fraction_ =
28                  this->layer_param_.batch_norm_param().moving_average_fraction();
29    eps_ = this->layer_param_.batch_norm_param().eps();
30    use_weight_bias_ = this->layer_param_.batch_norm_param().use_weight_bias();
31    bias_term_ = this->layer_param_.batch_norm_param().bias_term();
32    use_global_stats_ = this->phase_ == TEST;
33    if (this->layer_param_.batch_norm_param().has_use_global_stats())
34      use_global_stats_ = this->layer_param_.batch_norm_param().use_global_stats();
35    num_stats_batches_ = 1;
36    stats_batch_size_ = bottom[0]->shape(0);
37    BatchNormParameter param = this->layer_param_.batch_norm_param();
38    if (!use_global_stats_ && param.stats_batch_size() > 0) {
39      CHECK_EQ(bottom[0]->shape(0) % param.stats_batch_size(), 0);
40      num_stats_batches_ = bottom[0]->shape(0) / param.stats_batch_size();
41      stats_batch_size_ = param.stats_batch_size();
42    }
43    CHECK(use_weight_bias_) << "BatchNorm without scaling have not supported yet";
44    size_t dim = 4, sizes[4], strides[4];
45    channels_ = bottom[0]->channels();
46    height_   = bottom[0]->height();
47    width_    = bottom[0]->width();
48    num_      = bottom[0]->num();
49    sizes[0] = width_;
50    sizes[1] = height_;
51    sizes[2] = channels_;
52    sizes[3] = num_;
53    strides[0] = 1;
54    strides[1] = sizes[0];
55    strides[2] = sizes[0]*sizes[1];
56    strides[3] = sizes[0]*sizes[1]*sizes[2];
57    fwd_bottom_data->name = "fwd_bottom_data   @ " + this->layer_param_.name();
58    fwd_top_data->name =    "fwd_top_data      @ " + this->layer_param_.name();
59    bwd_bottom_diff->name = "bwd_bottom_diff   @ " + this->layer_param_.name();
60    bwd_top_diff->name =    "bwd_top_diff      @ " + this->layer_param_.name();
61    fwd_bottom_data->create_user_layout(dim, sizes, strides, false);
62    fwd_top_data   ->create_user_layout(dim, sizes, strides, false);
63    bwd_bottom_diff->create_user_layout(dim, sizes, strides, false);
64    bwd_top_diff   ->create_user_layout(dim, sizes, strides, false);
65    sizes[3] /= num_stats_batches_;
66    dnnError_t e;
67    dnnLayoutDelete<Dtype>(layout_usr_);
68    e = dnnLayoutCreate<Dtype>(&layout_usr_, dim, sizes, strides);
69    CHECK_EQ(e, E_SUCCESS);
70    for (int i = 0; i < mean_buffers_.size(); i++) {
71      dnnReleaseBuffer<Dtype>(mean_buffers_[i]);
72    }
73    for (int i = 0; i < variance_buffers_.size(); i++) {
74      dnnReleaseBuffer<Dtype>(variance_buffers_[i]);
75    }
76    mean_buffers_.resize(num_stats_batches_, NULL);
77    variance_buffers_.resize(num_stats_batches_, NULL);
78    dnnReleaseBuffer<Dtype>(scaleShift_buffer_);
79    dnnReleaseBuffer<Dtype>(diffScaleShift_buffer_);
80    dnnDelete<Dtype>(batchNormFwd);
81    dnnDelete<Dtype>(batchNormFwdInference);
82    dnnDelete<Dtype>(batchNormBwd);
83    this->blobs_.resize(3);
84    if (use_weight_bias_) {
85      if ( bias_term_ ) {
86          this->blobs_.resize(5);
87      } else {
88          this->blobs_.resize(4);
89      }
90      vector<int> scaleshift_shape(1);
91      scaleshift_shape[0] = channels_;
92      this->blobs_[3].reset(new Blob<Dtype>(scaleshift_shape));
93      FillerParameter filler_param(
94        this->layer_param_.batch_norm_param().filler());
95      if (!this->layer_param_.batch_norm_param().has_filler()) {
96        filler_param.set_type("constant");
97        filler_param.set_value(1);
98      }
99      shared_ptr<Filler<Dtype> > filler(GetFiller<Dtype>(filler_param));
100      filler->Fill(this->blobs_[3].get());
101      if ( bias_term_ ) {
102        this->blobs_[4].reset(new Blob<Dtype>(scaleshift_shape));
103        FillerParameter bias_filler_param(
104          this->layer_param_.batch_norm_param().bias_filler());
105        if (!this->layer_param_.batch_norm_param().has_bias_filler()) {
106          bias_filler_param.set_type("constant");
107          bias_filler_param.set_value(0);
108        }
109        shared_ptr<Filler<Dtype> > bias_filler(
110          GetFiller<Dtype>(bias_filler_param));
111        bias_filler->Fill(this->blobs_[4].get());
112      }
113    }
114    vector<int> sz;
115    sz.push_back(channels_);
116    this->blobs_[0].reset(new Blob<Dtype>(sz));
117    this->blobs_[1].reset(new Blob<Dtype>(sz));
118    sz[0]=1;
119    this->blobs_[2].reset(new Blob<Dtype>(sz));
120    for (int i = 0; i < 3; ++i) {
121      caffe_set(this->blobs_[i]->count(), Dtype(0),
122                this->blobs_[i]->mutable_cpu_data());
123    }
124    for (int i = 0; i < 3; ++i) {
125      if (this->layer_param_.param_size() == i) {
126        ParamSpec* fixed_param_spec = this->layer_param_.add_param();
127        fixed_param_spec->set_lr_mult(0.f);
128      } else {
129        CHECK_EQ(this->layer_param_.param(i).lr_mult(), 0.f)
130            << "Cannot configure batch normalization statistics as layer "
131            << "parameters.";
132      }
133    }
134  }
135  template <typename Dtype>
136  void MKLBatchNormLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
137        const vector<Blob<Dtype>*>& top) {
138    Init(bottom, top);
139  }
140  template <typename Dtype>
141  void MKLBatchNormLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
142        const vector<Blob<Dtype>*>& top) {
143    bool re_init = true;
144    if (channels_ == bottom[0]->channels() &&
145        height_ == bottom[0]->height() &&
146        width_ == bottom[0]->width()) {
147      re_init = false;
148    }
149    if (bottom[0] == top[0]) {  
150      temp_.ReshapeLike(*bottom[0]);
151    } else {
152      channels_ = bottom[0]->channels();
153      height_ = bottom[0]->height();
154      width_ = bottom[0]->width();
155      num_ = bottom[0]->num();
156      top[0]->Reshape(num_, channels_, height_, width_);
157    }
158    if (re_init == true) {
159      Init(bottom, top);
160    } else if (num_ != bottom[0]->num()) { 
161      size_t dim = 4, sizes[4], strides[4];
162      sizes[0] = width_;
163      sizes[1] = height_;
164      sizes[2] = channels_;
165      sizes[3] = num_;
166      strides[0] = 1;
167      strides[1] = sizes[0];
168      strides[2] = sizes[0]*sizes[1];
169      strides[3] = sizes[0]*sizes[1]*sizes[2];
170      fwd_bottom_data->create_user_layout(dim, sizes, strides, false);
171      fwd_top_data   ->create_user_layout(dim, sizes, strides, false);
172      bwd_bottom_diff->create_user_layout(dim, sizes, strides, false);
173      bwd_top_diff   ->create_user_layout(dim, sizes, strides, false);
174      sizes[3] /= num_stats_batches_;
175      dnnError_t e;
176      dnnLayoutDelete<Dtype>(layout_usr_);
177      e = dnnLayoutCreate<Dtype>(&layout_usr_, dim, sizes, strides);
178      CHECK_EQ(e, E_SUCCESS);
179    }
180  }
181  template <typename Dtype>
182  void MKLBatchNormLayer<Dtype>::ForwardStatsBatch_cpu(const vector<Blob<Dtype>*>& bottom,
183      const vector<Blob<Dtype>*>& top, int stats_batch_idx) {
184    long data_offset = stats_batch_idx * stats_batch_size_ * bottom[0]->count(1);
185    void* bottom_data =
186      reinterpret_cast<void *>(const_cast<Dtype*>(bottom[0]->prv_data()));
187    int is_first_pass = 0;
188    long amount_to_copy =0;
189    if (NULL != bottom_data && num_stats_batches_ == 1) {
190      amount_to_copy = bottom[0]->prv_data_count();
191      if (batchNormFwd == NULL) {
192        is_first_pass = 1;
193        CHECK((bottom[0]->get_prv_data_descriptor())->get_descr_type() ==
194          PrvMemDescr::PRV_DESCR_MKL2017);
195        shared_ptr<MKLData<Dtype> > mem_descr
196          =  boost::static_pointer_cast<MKLData<Dtype> >(
197             bottom[0]->get_prv_data_descriptor());
198        CHECK(mem_descr != NULL);
199        DLOG(INFO) << "Using layout of " << mem_descr->name
200                << " as input layout for " << this->layer_param_.name();
201        fwd_bottom_data = mem_descr;
202        dnnError_t e;
203        e = dnnBatchNormalizationCreateForward<Dtype>(
204          &batchNormFwd, NULL, mem_descr->layout_int, eps_, dnnUseScaleShift);
205        CHECK_EQ(e, E_SUCCESS);
206        e = dnnBatchNormalizationCreateForward<Dtype>(
207          &batchNormFwdInference, NULL, mem_descr->layout_int, eps_,
208                                      dnnUseScaleShift | dnnUseInputMeanVariance);
209        CHECK_EQ(e, E_SUCCESS);
210        fwd_top_data   ->create_internal_layout(batchNormFwd, dnnResourceDst);
211        bwd_top_diff   ->create_internal_layout(batchNormFwd, dnnResourceDst);
212        bwd_bottom_diff->create_internal_layout(batchNormFwd, dnnResourceSrc);
213         if (!use_global_stats_) {
214           e = dnnBatchNormalizationCreateBackward<Dtype>(
215              &batchNormBwd, NULL, mem_descr->layout_int, eps_, dnnUseScaleShift);
216           CHECK_EQ(e, E_SUCCESS);
217         } else {
218           e = dnnBatchNormalizationCreateBackward<Dtype>(
219              &batchNormBwd, NULL, mem_descr->layout_int, eps_, dnnUseScaleShift | dnnUseInputMeanVariance);
220           CHECK_EQ(e, E_SUCCESS);
221         }
222      }
223    } else {
224      DLOG(INFO) << "Using cpu_data in MKLBatchNormLayer.";
225      if (batchNormFwd == NULL) {
226        is_first_pass = 1;
227        dnnError_t e;
228        e = dnnBatchNormalizationCreateForward<Dtype>(
229          &batchNormFwd, NULL, layout_usr_, eps_, dnnUseScaleShift);
230        CHECK_EQ(e, E_SUCCESS);
231        e = dnnBatchNormalizationCreateForward<Dtype>(
232          &batchNormFwdInference, NULL, layout_usr_, eps_,
233                                      dnnUseScaleShift | dnnUseInputMeanVariance);
234        CHECK_EQ(e, E_SUCCESS);
235        if (!use_global_stats_) {
236          e = dnnBatchNormalizationCreateBackward<Dtype>(
237            &batchNormBwd, NULL, layout_usr_, eps_, dnnUseScaleShift);
238          CHECK_EQ(e, E_SUCCESS);
239        } else {
240          e = dnnBatchNormalizationCreateBackward<Dtype>(
241            &batchNormBwd, NULL, layout_usr_, eps_, dnnUseScaleShift | dnnUseInputMeanVariance);
242          CHECK_EQ(e, E_SUCCESS);
243        }
244      }
245      bottom_data =
246        reinterpret_cast<void *>(const_cast<Dtype*>(bottom[0]->cpu_data()));
247      amount_to_copy = bottom[0]->count() / num_stats_batches_;
248    }
249    if (is_first_pass == 1) {
250        dnnError_t e;
251        dnnLayout_t mean_buffer_l = NULL;
252        e = dnnLayoutCreateFromPrimitive<Dtype>(
253          &mean_buffer_l, batchNormFwd, dnnResourceMean);
254        CHECK_EQ(e, E_SUCCESS);
255        for (int i = 0; i < num_stats_batches_; i++) {
256          e = dnnAllocateBuffer<Dtype>(
257            reinterpret_cast<void**>(&mean_buffers_[i]), mean_buffer_l);
258          CHECK_EQ(e, E_SUCCESS);
259        }
260        dnnLayoutDelete<Dtype>(mean_buffer_l);
261        dnnLayout_t variance_buffer_l = NULL;
262        e = dnnLayoutCreateFromPrimitive<Dtype>(
263          &variance_buffer_l, batchNormFwd, dnnResourceVariance);
264        CHECK_EQ(e, E_SUCCESS);
265        for (int i = 0; i < num_stats_batches_; i++) {
266          e = dnnAllocateBuffer<Dtype>(
267            reinterpret_cast<void**>(&variance_buffers_[i]), variance_buffer_l);
268          CHECK_EQ(e, E_SUCCESS);
269        }
270        dnnLayoutDelete<Dtype>(variance_buffer_l);
271         dnnLayout_t diffScaleShift_buffer_l = NULL;
272        e = dnnLayoutCreateFromPrimitive<Dtype>(
273          &diffScaleShift_buffer_l, batchNormBwd, dnnResourceDiffScaleShift);
274        CHECK_EQ(e, E_SUCCESS);
275        e = dnnAllocateBuffer<Dtype>(
276          reinterpret_cast<void**>(&diffScaleShift_buffer_), diffScaleShift_buffer_l);
277        CHECK_EQ(e, E_SUCCESS);
278        dnnLayoutDelete<Dtype>(diffScaleShift_buffer_l);
279        dnnLayout_t scaleShift_buffer_l = NULL;
280        e = dnnLayoutCreateFromPrimitive<Dtype>(
281          &scaleShift_buffer_l, batchNormFwd, dnnResourceScaleShift);
282        CHECK_EQ(e, E_SUCCESS);
283        e = dnnAllocateBuffer<Dtype>(
284          reinterpret_cast<void**>(&scaleShift_buffer_), scaleShift_buffer_l);
285        CHECK_EQ(e, E_SUCCESS);
286        dnnLayoutDelete<Dtype>(scaleShift_buffer_l);
287        if (!use_weight_bias_) {
288           for (int i = 0; i < channels_; i++) {
289              scaleShift_buffer_[i] = 1.0;
290              scaleShift_buffer_[channels_ + i] = 0;
291           }
292        }
293    }
294    if (use_weight_bias_) {
295      for (int i = 0; i < channels_; i++) {
296        scaleShift_buffer_[i] = this->blobs_[3]->cpu_data()[i];
297        scaleShift_buffer_[channels_ + i] = 0;
298        if (bias_term_) {
299           scaleShift_buffer_[channels_ + i] = this->blobs_[4]->cpu_data()[i];
300        }
301      }
302    }
303    if (bottom[0] == top[0] && this->phase_ == TRAIN) {
304      caffe_copy(amount_to_copy, static_cast<Dtype*>(bottom_data) + data_offset,
305                 temp_.mutable_cpu_data() + data_offset);
306    }
307    if (use_global_stats_) {
308      const Dtype scale_factor = this->blobs_[2]->cpu_data()[0] == 0 ?
309                                 0 : 1 / this->blobs_[2]->cpu_data()[0];
310      caffe_cpu_scale(this->blobs_[0]->count(), scale_factor,
311                      this->blobs_[0]->cpu_data(), mean_buffers_[stats_batch_idx]);
312      caffe_cpu_scale(this->blobs_[1]->count(), scale_factor,
313                      this->blobs_[1]->cpu_data(), variance_buffers_[stats_batch_idx]);
314    }
315    dnnError_t e;
316    void* BatchNorm_res[dnnResourceNumber];
317    BatchNorm_res[dnnResourceMean] = mean_buffers_[stats_batch_idx];
318    BatchNorm_res[dnnResourceVariance] = variance_buffers_[stats_batch_idx];
319    BatchNorm_res[dnnResourceSrc] = (Dtype*)bottom_data + data_offset;
320    BatchNorm_res[dnnResourceScaleShift] = scaleShift_buffer_;
321    if (fwd_top_data->conversion_needed()) {
322      top[0]->set_prv_data_descriptor(fwd_top_data);
323      data_offset = stats_batch_idx * (top[0]->prv_data_count() / num_stats_batches_);
324      BatchNorm_res[dnnResourceDst] =
325              reinterpret_cast<void *>(top[0]->mutable_prv_data() + data_offset);
326    } else {
327      BatchNorm_res[dnnResourceDst] =
328              reinterpret_cast<void *>(top[0]->mutable_cpu_data() + data_offset);
<span onclick='openModal()' class='match'>329      DLOG(INFO) << "Using cpu_data for top in DnnBatchNorm.";
330    }
331    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME("FW"));
332    PERFORMANCE_MEASUREMENT_BEGIN();
333    e = dnnExecute<Dtype>(use_global_stats_? batchNormFwdInference : batchNormFwd,
</span>334                                                                   BatchNorm_res);
335    PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
336    CHECK_EQ(e, E_SUCCESS);
337    if (!use_global_stats_) {
338      this->blobs_[2]->mutable_cpu_data()[0] *= moving_average_fraction_;
339      this->blobs_[2]->mutable_cpu_data()[0] += 1;
340      caffe_cpu_axpby(this->blobs_[0]->count(), Dtype(1), mean_buffers_[stats_batch_idx],
341          moving_average_fraction_, this->blobs_[0]->mutable_cpu_data());
342      int m = bottom[0]->count()/num_stats_batches_/channels_;
343      Dtype bias_correction_factor = m > 1 ? Dtype(m)/(m-1) : 1;
344      caffe_cpu_axpby(this->blobs_[1]->count(), bias_correction_factor,
345          variance_buffers_[stats_batch_idx], moving_average_fraction_,
346          this->blobs_[1]->mutable_cpu_data());
347    }
348  }
349  template <typename Dtype>
350  void MKLBatchNormLayer<Dtype>::BackwardStatsBatch_cpu(const vector<Blob<Dtype>*>& top,
351      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom,
352      int stats_batch_idx) {
353    long data_offset = stats_batch_idx * stats_batch_size_ * bottom[0]->count(1);
354    void *bottom_data = NULL;
355    if (bottom[0] == top[0]) {
356      bottom_data = reinterpret_cast<void *>(
357                          const_cast<Dtype*>(temp_.cpu_data()));
358    } else {
359      bottom_data =
360              reinterpret_cast<void *>(
361                          const_cast<Dtype*>(bottom[0]->prv_data()));
362      if (NULL == bottom_data || num_stats_batches_ > 1)
363        bottom_data =
364              reinterpret_cast<void *>(
365                          const_cast<Dtype*>(bottom[0]->cpu_data()));
366    }
367    dnnError_t e;
368    void* BatchNorm_res[dnnResourceNumber];
369    BatchNorm_res[dnnResourceMean] = mean_buffers_[stats_batch_idx];
370    BatchNorm_res[dnnResourceVariance] = variance_buffers_[stats_batch_idx];
371    BatchNorm_res[dnnResourceSrc] = (Dtype*)bottom_data + data_offset;
372    BatchNorm_res[dnnResourceScaleShift] = scaleShift_buffer_;
373    BatchNorm_res[dnnResourceDiffScaleShift] = diffScaleShift_buffer_;
374    BatchNorm_res[dnnResourceDiffDst] =
375      bwd_top_diff->get_converted_prv(top[0], true) + data_offset;
376    if (bwd_bottom_diff->conversion_needed()) {
377      bottom[0]->set_prv_diff_descriptor(bwd_bottom_diff);
378      data_offset = stats_batch_idx * (bottom[0]->prv_diff_count() / num_stats_batches_);
379      BatchNorm_res[dnnResourceDiffSrc] = bottom[0]->mutable_prv_diff() + data_offset;
380    } else {
381      BatchNorm_res[dnnResourceDiffSrc] = bottom[0]->mutable_cpu_diff() + data_offset;
382    }
383    PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKL_NAME("BW"));
384    PERFORMANCE_MEASUREMENT_BEGIN();
385    e = dnnExecute<Dtype>(batchNormBwd, BatchNorm_res);
386    PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
387    CHECK_EQ(e, E_SUCCESS);
388    if (use_weight_bias_) {
389      caffe_cpu_axpby(this->blobs_[3]->count(), (Dtype)1.,
390                      diffScaleShift_buffer_, (Dtype)1., this->blobs_[3]->mutable_cpu_diff());
391      if (bias_term_)
392        caffe_cpu_axpby(this->blobs_[4]->count(), (Dtype)1.,
393                        diffScaleShift_buffer_ + channels_,
394                        (Dtype)1., this->blobs_[4]->mutable_cpu_diff());
395      else
396        caffe_set(this->blobs_[4]->count(),
397                      static_cast<Dtype>(0), this->blobs_[4]->mutable_cpu_diff());
398    }
399  }
400  template <typename Dtype>
401  void MKLBatchNormLayer<Dtype>::Forward_cpu(
402      const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
403    for (int i = 0; i < num_stats_batches_; i++) {
404      ForwardStatsBatch_cpu(bottom, top, i);
405    }
406  }
407  template <typename Dtype>
408  void MKLBatchNormLayer<Dtype>::Backward_cpu(
409      const vector<Blob<Dtype>*>& top, const vector<bool>& propagate_down,
410      const vector<Blob<Dtype>*>& bottom) {
411    for (int i = 0; i < num_stats_batches_; i++) {
412      BackwardStatsBatch_cpu(top, propagate_down, bottom, i);
413    }
414  }
415  #ifdef CPU_ONLY
416  STUB_GPU(MKLBatchNormLayer);
417  #else
418  template <typename Dtype>
419  void MKLBatchNormLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,
420      const vector<Blob<Dtype>*>& top) {NOT_IMPLEMENTED;}
421  template <typename Dtype>
422  void MKLBatchNormLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,
423      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom)
424    {NOT_IMPLEMENTED;}
425  #endif
426  INSTANTIATE_CLASS(MKLBatchNormLayer);
427  }  
428  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_pooling_layer.cpp</h3>
            <pre><code>1  #ifdef MKL2017_SUPPORTED
2  #include <algorithm>
3  #include <cfloat>
4  #include <vector>
5  #include "caffe/common.hpp"
6  #include "caffe/layer.hpp"
7  #include "caffe/layers/mkl_layers.hpp"
8  #include "caffe/syncedmem.hpp"
9  #include "caffe/util/math_functions.hpp"
10  #include "caffe/util/performance.hpp"
11  namespace caffe {
12  template <typename Dtype>
13  MKLPoolingLayer<Dtype>::~MKLPoolingLayer() {
14    dnnDelete<Dtype>(poolingFwd);
15    dnnDelete<Dtype>(poolingBwd);
16  }
17  template <typename Dtype>
18  void MKLPoolingLayer<Dtype>::Init(
19        const vector<Blob<Dtype>*>& bottom,
20        const vector<Blob<Dtype>*>& top) {
21    PoolingParameter pool_param = this->layer_param_.pooling_param();
22    channels_ = bottom[0]->channels();
23    height_ = bottom[0]->height();
24    width_ = bottom[0]->width();
25    num_ = bottom[0]->num();
26    if (pool_param.global_pooling()) {
27      CHECK(!(pool_param.has_kernel_size() ||
28        pool_param.has_kernel_h() || pool_param.has_kernel_w()))
29        << "With Global_pooling: true Filter size cannot specified";
30    } else {
31      CHECK(!pool_param.has_kernel_size() !=
32        !(pool_param.has_kernel_h() && pool_param.has_kernel_w()))
33        << "Filter size is kernel_size OR kernel_h and kernel_w; not both";
34      CHECK(pool_param.has_kernel_size() ||
35        (pool_param.has_kernel_h() && pool_param.has_kernel_w()))
36        << "For non-square filters both kernel_h and kernel_w are required.";
37    }
38    CHECK((!pool_param.has_pad() && pool_param.has_pad_h()
39        && pool_param.has_pad_w())
40        || (!pool_param.has_pad_h() && !pool_param.has_pad_w()))
41        << "pad is pad OR pad_h and pad_w are required.";
42    CHECK((!pool_param.has_stride() && pool_param.has_stride_h()
43        && pool_param.has_stride_w())
44        || (!pool_param.has_stride_h() && !pool_param.has_stride_w()))
45        << "Stride is stride OR stride_h and stride_w are required.";
46    global_pooling_ = pool_param.global_pooling();
47    if (global_pooling_) {
48      kernel_h_ = bottom[0]->height();
49      kernel_w_ = bottom[0]->width();
50    } else {
51      if (pool_param.has_kernel_size()) {
52        kernel_h_ = kernel_w_ = pool_param.kernel_size();
53      } else {
54        kernel_h_ = pool_param.kernel_h();
55        kernel_w_ = pool_param.kernel_w();
56      }
57    }
58    CHECK_GT(kernel_h_, 0) << "Filter dimensions cannot be zero.";
59    CHECK_GT(kernel_w_, 0) << "Filter dimensions cannot be zero.";
60    if (!pool_param.has_pad_h()) {
61      pad_h_ = pad_w_ = pool_param.pad();
62    } else {
63      pad_h_ = pool_param.pad_h();
64      pad_w_ = pool_param.pad_w();
65    }
66    if (!pool_param.has_stride_h()) {
67      stride_h_ = stride_w_ = pool_param.stride();
68    } else {
69      stride_h_ = pool_param.stride_h();
70      stride_w_ = pool_param.stride_w();
71    }
72    if (global_pooling_) {
73      CHECK(pad_h_ == 0 && pad_w_ == 0 && stride_h_ == 1 && stride_w_ == 1)
74        << "With Global_pooling: true; only pad = 0 and stride = 1";
75    }
76    if (pad_h_ != 0 || pad_w_ != 0) {
77      CHECK(this->layer_param_.pooling_param().pool()
78          == PoolingParameter_PoolMethod_AVE
79          || this->layer_param_.pooling_param().pool()
80          == PoolingParameter_PoolMethod_MAX)
81          << "Padding implemented only for average and max pooling.";
82      CHECK_LT(pad_h_, kernel_h_);
83      CHECK_LT(pad_w_, kernel_w_);
84    }
85    pooled_height_ = static_cast<int>(ceil(static_cast<float>(
86        bottom[0]->height() + 2 * pad_h_ - kernel_h_) / stride_h_)) + 1;
87    pooled_width_ = static_cast<int>(ceil(static_cast<float>(
88        bottom[0]->width() + 2 * pad_w_ - kernel_w_) / stride_w_)) + 1;
89    bool force_exclude_padding_flag_ = false;
90    if (pad_h_ || pad_w_ || kernel_h_ == 1 || kernel_w_ == 1) {
91      if ((pooled_height_ - 1) * stride_h_ >= bottom[0]->height() + pad_h_) {
92        --pooled_height_;
93      }
94      if ((pooled_width_ - 1) * stride_w_ >= bottom[0]->width() + pad_w_) {
95        --pooled_width_;
96      }
97      CHECK_LT((pooled_height_ - 1) * stride_h_, bottom[0]->height() + pad_h_);
98      CHECK_LT((pooled_width_ - 1) * stride_w_, bottom[0]->width() + pad_w_);
99    }
100    else
101    {
102      force_exclude_padding_flag_ = true;
103    }
104    top[0]->Reshape(bottom[0]->num(), channels_, pooled_height_,
105        pooled_width_);
106    if (top.size() > 1) {
107      (reinterpret_cast<Blob<size_t>* > (top[1]) )->Reshape(bottom[0]->num(),
108              channels_, pooled_height_, pooled_width_);
109    }
110    if (top.size() == 1) {
111      max_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,
112              pooled_width_);
113    }
114    if (this->layer_param_.pooling_param().pool() ==
115        PoolingParameter_PoolMethod_STOCHASTIC) {
116      rand_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,
117        pooled_width_);
118    }
119    switch (this->layer_param_.pooling_param().pool()) {
120    case PoolingParameter_PoolMethod_MAX:
121      this->algorithm = dnnAlgorithmPoolingMax;
122      break;
123    case PoolingParameter_PoolMethod_AVE:
124      if (this->layer_param_.pooling_param().avg_include_pad()) {
125          this->algorithm = dnnAlgorithmPoolingAvgIncludePadding;
126      }
127      else {
128          this->algorithm = dnnAlgorithmPoolingAvgExcludePadding;
129      }
130      if (force_exclude_padding_flag_ == true)
131      {
132          this->algorithm = dnnAlgorithmPoolingAvgExcludePadding;
133      }
134      break;
135    case PoolingParameter_PoolMethod_STOCHASTIC:
136      NOT_IMPLEMENTED;
137      break;
138    default:
139      LOG(FATAL) << "Unknown pooling method.";
140    }
141    dim = 4;
142    src_sizes[0] = bottom[0]->width();
143    src_sizes[1] = bottom[0]->height();
144    src_sizes[2] = bottom[0]->channels();
145    src_sizes[3] = bottom[0]->num();
146    src_strides[0] = 1;
147    src_strides[1] = src_sizes[0];
148    src_strides[2] = src_sizes[0]*src_sizes[1];
149    src_strides[3] = src_sizes[0]*src_sizes[1]*src_sizes[2];
150    dst_sizes[0] = pooled_width_;
151    dst_sizes[1] = pooled_height_;
152    dst_sizes[2] = src_sizes[2];
153    dst_sizes[3] = src_sizes[3];
154    dst_strides[0] = 1;
155    dst_strides[1] = dst_sizes[0];
156    dst_strides[2] = dst_sizes[0]*dst_sizes[1];
157    dst_strides[3] = dst_sizes[0]*dst_sizes[1]*dst_sizes[2];
158    src_offset[0] = -pad_w_;
159    src_offset[1] = -pad_h_;
160    kernel_stride[0] = stride_w_;
161    kernel_stride[1] = stride_h_;
162    kernel_size[0] = kernel_w_;
163    kernel_size[1] = kernel_h_;
164    fwd_bottom_data->name = "fwd_bottom_data   @ " + this->layer_param_.name();
165    fwd_top_data->name =    "fwd_top_data      @ " + this->layer_param_.name();
166    bwd_top_diff->name =    "bwd_top_diff      @ " + this->layer_param_.name();
167    bwd_bottom_diff->name = "bwd_bottom_diff   @ " + this->layer_param_.name();
168    fwd_top_data   ->create_user_layout(dim, dst_sizes, dst_strides, false);
169    bwd_bottom_diff->create_user_layout(dim, src_sizes, src_strides, false);
170    bwd_top_diff   ->create_user_layout(dim, dst_sizes, dst_strides, false);
171    dnnDelete<Dtype>(poolingFwd);
172    dnnDelete<Dtype>(poolingBwd);
173  }
174  template <typename Dtype>
175  void MKLPoolingLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom,
176        const vector<Blob<Dtype>*>& top) {
177    Init(bottom, top);
178  }
179  template <typename Dtype>
180  void MKLPoolingLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom,
181        const vector<Blob<Dtype>*>& top) {
182    CHECK_EQ(4, bottom[0]->num_axes()) << "Input must have 4 axes, "
183        << "corresponding to (num, channels, height, width)";
184    if (channels_ == bottom[0]->channels() &&
185        height_ == bottom[0]->height() &&
186        width_ == bottom[0]->width() &&
187        num_ == bottom[0]->num()) {
188      reshape = false;
189      return;
190    }
191    reshape = true;
192    Init(bottom, top);
193  }
194  template <typename Dtype>
195  void MKLPoolingLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
196        const vector<Blob<Dtype>*>& top) {
197    size_t* mask = NULL;  
198    const bool use_top_mask = top.size() > 1;
199    dnnError_t status;
200    void* pooling_res[dnnResourceNumber];
201    mask = (use_top_mask) ?
202        reinterpret_cast<size_t*>(top[1]->mutable_cpu_data()) :
203        (max_idx_.mutable_cpu_data());
204    pooling_res[dnnResourceWorkspace] = reinterpret_cast<void*>(mask);
205    void* bottom_data =
206      reinterpret_cast<void *>(const_cast<Dtype*>(bottom[0]->prv_data()));
207    if (NULL == bottom_data) {
208      bottom_data =
209        reinterpret_cast<void *>(const_cast<Dtype*>(bottom[0]->cpu_data()));
210      if (NULL == poolingFwd || reshape) {
211        fwd_bottom_data->create_user_layout(dim, src_sizes, src_strides, false);
212        status = dnnPoolingCreateForward<Dtype>(&poolingFwd, NULL,
213                this->algorithm, fwd_bottom_data->layout_usr,
214                kernel_size, kernel_stride, src_offset, dnnBorderZeros);
215        CHECK_EQ(status, E_SUCCESS);
216        status = dnnPoolingCreateBackward<Dtype>(&poolingBwd, NULL,
217                this->algorithm, fwd_bottom_data->layout_usr,
218                kernel_size, kernel_stride, src_offset, dnnBorderZeros);
219        CHECK_EQ(status, E_SUCCESS);
220      }
221    } else if (NULL == poolingFwd || reshape) {
222      CHECK_EQ((bottom[0]->get_prv_data_descriptor())->get_descr_type(),
223              PrvMemDescr::PRV_DESCR_MKL2017);
224      shared_ptr<MKLData<Dtype> > mem_descr
225        =  boost::static_pointer_cast<MKLData<Dtype> >
226              (bottom[0]->get_prv_data_descriptor());
227      CHECK(mem_descr != NULL);
228      DLOG(INFO) << "Using layout of " << mem_descr->name
229              << " as input layout for " << this->layer_param_.name();
230      fwd_bottom_data = mem_descr;
231      status = dnnPoolingCreateForward<Dtype>(&poolingFwd, NULL,
232              this->algorithm, fwd_bottom_data->layout_int, kernel_size,
233              kernel_stride, src_offset, dnnBorderZeros);
234      CHECK_EQ(status, E_SUCCESS);
235      fwd_top_data->create_internal_layout(poolingFwd, dnnResourceDst);
236      status = dnnPoolingCreateBackward<Dtype>(&poolingBwd, NULL,
237              this->algorithm, fwd_bottom_data->layout_int, kernel_size,
238              kernel_stride, src_offset, dnnBorderZeros);
239      CHECK_EQ(status, E_SUCCESS);
240      bwd_top_diff   ->create_internal_layout(poolingFwd, dnnResourceDst);
241      bwd_bottom_diff->create_internal_layout(poolingFwd, dnnResourceSrc);
242    }
243    pooling_res[dnnResourceSrc] = bottom_data;
244    if (fwd_top_data->conversion_needed()) {
245      top[0]->set_prv_data_descriptor(fwd_top_data);
246      pooling_res[dnnResourceDst] =
247              reinterpret_cast<void *>(top[0]->mutable_prv_data());
248    } else {
249      pooling_res[dnnResourceDst] =
250              reinterpret_cast<void *>(top[0]->mutable_cpu_data());
<span onclick='openModal()' class='match'>251      DLOG(INFO) << "Using cpu_data for top in DnnPooling.";
252    }
253    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME("FW"));
254    PERFORMANCE_MEASUREMENT_BEGIN();
255    status = dnnExecute<Dtype>(poolingFwd, pooling_res);
</span>256    PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
257    CHECK_EQ(status, E_SUCCESS);
258  }
259  template <typename Dtype>
260  void MKLPoolingLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
261        const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {
262    if (!propagate_down[0]) {
263      return;
264    }
265    const size_t* mask = NULL;  
266    dnnError_t e;
267    void* pooling_res[dnnResourceNumber];
268    mask = (top.size() > 1) ?
269      reinterpret_cast<const size_t*>(top[1]->cpu_data()) :
270      (max_idx_.cpu_data());
271    pooling_res[dnnResourceWorkspace] =
272      reinterpret_cast<void *>(const_cast<size_t*>(mask));
273    pooling_res[dnnResourceDiffDst] = bwd_top_diff->get_converted_prv(top[0],
274            true);
275    if (bwd_bottom_diff->conversion_needed()) {
276      bottom[0]->set_prv_diff_descriptor(bwd_bottom_diff);
277      pooling_res[dnnResourceDiffSrc] = bottom[0]->mutable_prv_diff();
278    } else {
279      pooling_res[dnnResourceDiffSrc] = bottom[0]->mutable_cpu_diff();
280    }
281    caffe_set(bottom[0]->count(), Dtype(0),
282            reinterpret_cast<Dtype *>(pooling_res[dnnResourceDiffSrc]));
283    PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKL_NAME("BW"));
284    PERFORMANCE_MEASUREMENT_BEGIN();
285    e = dnnExecute<Dtype>(poolingBwd, pooling_res);
286    PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
287    CHECK_EQ(e, E_SUCCESS);
288  }
289  #ifdef CPU_ONLY
290  STUB_GPU(MKLPoolingLayer);
291  #else
292  template <typename Dtype>
293  void MKLPoolingLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,
294      const vector<Blob<Dtype>*>& top) {NOT_IMPLEMENTED;}
295  template <typename Dtype>
296  void MKLPoolingLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,
297      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom)
298    {NOT_IMPLEMENTED;}
299  #endif
300  INSTANTIATE_CLASS(MKLPoolingLayer);
301  }  
302  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_batch_norm_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkl_pooling_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>329      DLOG(INFO) << "Using cpu_data for top in DnnBatchNorm.";
330    }
331    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME("FW"));
332    PERFORMANCE_MEASUREMENT_BEGIN();
333    e = dnnExecute<Dtype>(use_global_stats_? batchNormFwdInference : batchNormFwd,
</pre></code></div>
                <div class="column column_space"><pre><code>251      DLOG(INFO) << "Using cpu_data for top in DnnPooling.";
252    }
253    PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKL_NAME("FW"));
254    PERFORMANCE_MEASUREMENT_BEGIN();
255    status = dnnExecute<Dtype>(poolingFwd, pooling_res);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    