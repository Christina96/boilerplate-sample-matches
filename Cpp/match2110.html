<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for test_softmax_with_loss_layer.cpp & test_split_layer.cpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for test_softmax_with_loss_layer.cpp & test_split_layer.cpp
      </h3>
      <h1 align="center">
        28.1%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>test_softmax_with_loss_layer.cpp (38.157894%)<TH>test_split_layer.cpp (22.307692%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match2110-0.html#0',2,'match2110-1.html#0',3)" NAME="0">(39-61)<TD><A HREF="javascript:ZweiFrames('match2110-0.html#0',2,'match2110-1.html#0',3)" NAME="0">(35-54)</A><TD ALIGN=center><FONT COLOR="#ff0000">16</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match2110-0.html#1',2,'match2110-1.html#1',3)" NAME="1">(17-32)<TD><A HREF="javascript:ZweiFrames('match2110-0.html#1',2,'match2110-1.html#1',3)" NAME="1">(17-31)</A><TD ALIGN=center><FONT COLOR="#cf0000">13</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_softmax_with_loss_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;cmath&gt;
#include &lt;vector&gt;

#include &quot;boost/scoped_ptr.hpp&quot;
#include &quot;gtest/gtest.h&quot;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/filler.hpp&quot;
#include &quot;caffe/layers/softmax_loss_layer.hpp&quot;

#include &quot;caffe/test/test_caffe_main.hpp&quot;
#include &quot;caffe/test/test_gradient_check_util.hpp&quot;
<A NAME="1"></A>
using boost::scoped_ptr;

<FONT color="#f63526"><A HREF="javascript:ZweiFrames('match2110-1.html#1',3,'match2110-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>namespace caffe {

template &lt;typename TypeParam&gt;
class SoftmaxWithLossLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;

 protected:
  SoftmaxWithLossLayerTest()
      : blob_bottom_data_(new Blob&lt;Dtype&gt;(10, 5, 2, 3)),
        blob_bottom_label_(new Blob&lt;Dtype&gt;(10, 1, 2, 3)),
        blob_top_loss_(new Blob&lt;Dtype&gt;()) {
    // fill the values
    FillerParameter filler_param;
    filler_param.set_std(10);
    GaussianFiller&lt;Dtype&gt; filler(filler_param);
    filler.Fill(this-&gt;blob_bottom_data_);</B></FONT>
    blob_bottom_vec_.push_back(blob_bottom_data_);
    for (int i = 0; i &lt; blob_bottom_label_-&gt;count(); ++i) {
      blob_bottom_label_-&gt;mutable_cpu_data()[i] = caffe_rng_rand() % 5;
<A NAME="0"></A>    }
    blob_bottom_vec_.push_back(blob_bottom_label_);
    blob_top_vec_.push_back(blob_top_loss_);
<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match2110-1.html#0',3,'match2110-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>  }
  virtual ~SoftmaxWithLossLayerTest() {
    delete blob_bottom_data_;
    delete blob_bottom_label_;
    delete blob_top_loss_;
  }
  Blob&lt;Dtype&gt;* const blob_bottom_data_;
  Blob&lt;Dtype&gt;* const blob_bottom_label_;
  Blob&lt;Dtype&gt;* const blob_top_loss_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
};

TYPED_TEST_CASE(SoftmaxWithLossLayerTest, TestDtypesAndDevices);

TYPED_TEST(SoftmaxWithLossLayerTest, TestGradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  layer_param.add_loss_weight(3);
  SoftmaxWithLossLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-2, 1701);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_, 0);</B></FONT>
}

TYPED_TEST(SoftmaxWithLossLayerTest, TestForwardIgnoreLabel) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  layer_param.mutable_loss_param()-&gt;set_normalize(false);
  // First, compute the loss with all labels
  scoped_ptr&lt;SoftmaxWithLossLayer&lt;Dtype&gt; &gt; layer(
      new SoftmaxWithLossLayer&lt;Dtype&gt;(layer_param));
  layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  Dtype full_loss = this-&gt;blob_top_loss_-&gt;cpu_data()[0];
  // Now, accumulate the loss, ignoring each label in {0, ..., 4} in turn.
  Dtype accum_loss = 0;
  for (int label = 0; label &lt; 5; ++label) {
    layer_param.mutable_loss_param()-&gt;set_ignore_label(label);
    layer.reset(new SoftmaxWithLossLayer&lt;Dtype&gt;(layer_param));
    layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
    accum_loss += this-&gt;blob_top_loss_-&gt;cpu_data()[0];
  }
  // Check that each label was included all but once.
  EXPECT_NEAR(4 * full_loss, accum_loss, 1e-4);
}

TYPED_TEST(SoftmaxWithLossLayerTest, TestGradientIgnoreLabel) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  // labels are in {0, ..., 4}, so we'll ignore about a fifth of them
  layer_param.mutable_loss_param()-&gt;set_ignore_label(0);
  SoftmaxWithLossLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-2, 1701);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_, 0);
}

TYPED_TEST(SoftmaxWithLossLayerTest, TestGradientUnnormalized) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  layer_param.mutable_loss_param()-&gt;set_normalize(false);
  SoftmaxWithLossLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-2, 1701);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_, 0);
}

}  // namespace caffe
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_split_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;string&gt;
#include &lt;vector&gt;

#include &quot;google/protobuf/text_format.h&quot;
#include &quot;gtest/gtest.h&quot;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/filler.hpp&quot;
#include &quot;caffe/layers/split_layer.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;
#include &quot;caffe/util/insert_splits.hpp&quot;

<A NAME="1"></A>#include &quot;caffe/test/test_caffe_main.hpp&quot;
#include &quot;caffe/test/test_gradient_check_util.hpp&quot;

<FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2110-0.html#1',2,'match2110-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>namespace caffe {

template &lt;typename TypeParam&gt;
class SplitLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;

 protected:
  SplitLayerTest()
      : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 6, 5)),
        blob_top_a_(new Blob&lt;Dtype&gt;()),
        blob_top_b_(new Blob&lt;Dtype&gt;()) {
    // fill the values
    FillerParameter filler_param;
    GaussianFiller&lt;Dtype&gt; filler(filler_param);
    filler.Fill(this-&gt;blob_bottom_);</B></FONT>
<A NAME="0"></A>    blob_bottom_vec_.push_back(blob_bottom_);
    blob_top_vec_.push_back(blob_top_a_);
    blob_top_vec_.push_back(blob_top_b_);
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2110-0.html#0',2,'match2110-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>  }
  virtual ~SplitLayerTest() {
    delete blob_bottom_;
    delete blob_top_a_;
    delete blob_top_b_;
  }
  Blob&lt;Dtype&gt;* const blob_bottom_;
  Blob&lt;Dtype&gt;* const blob_top_a_;
  Blob&lt;Dtype&gt;* const blob_top_b_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
};

TYPED_TEST_CASE(SplitLayerTest, TestDtypesAndDevices);

TYPED_TEST(SplitLayerTest, TestSetup) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  SplitLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);</B></FONT>
  EXPECT_EQ(this-&gt;blob_top_a_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_a_-&gt;channels(), 3);
  EXPECT_EQ(this-&gt;blob_top_a_-&gt;height(), 6);
  EXPECT_EQ(this-&gt;blob_top_a_-&gt;width(), 5);
  EXPECT_EQ(this-&gt;blob_top_b_-&gt;num(), 2);
  EXPECT_EQ(this-&gt;blob_top_b_-&gt;channels(), 3);
  EXPECT_EQ(this-&gt;blob_top_b_-&gt;height(), 6);
  EXPECT_EQ(this-&gt;blob_top_b_-&gt;width(), 5);
}

TYPED_TEST(SplitLayerTest, Test) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  SplitLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  for (int i = 0; i &lt; this-&gt;blob_bottom_-&gt;count(); ++i) {
    Dtype bottom_value = this-&gt;blob_bottom_-&gt;cpu_data()[i];
    EXPECT_EQ(bottom_value, this-&gt;blob_top_a_-&gt;cpu_data()[i]);
    EXPECT_EQ(bottom_value, this-&gt;blob_top_b_-&gt;cpu_data()[i]);
  }
}

TYPED_TEST(SplitLayerTest, TestGradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  SplitLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-2);
  checker.CheckGradientEltwise(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}


class SplitLayerInsertionTest : public ::testing::Test {
 protected:
  void RunInsertionTest(
      const string&amp; input_param_string, const string&amp; output_param_string) {
    // Test that InsertSplits called on the proto specified by
    // input_param_string results in the proto specified by
    // output_param_string.
    NetParameter input_param;
    CHECK(google::protobuf::TextFormat::ParseFromString(
        input_param_string, &amp;input_param));
    NetParameter expected_output_param;
    CHECK(google::protobuf::TextFormat::ParseFromString(
        output_param_string, &amp;expected_output_param));
    NetParameter actual_output_param;
    InsertSplits(input_param, &amp;actual_output_param);
    EXPECT_EQ(expected_output_param.DebugString(),
        actual_output_param.DebugString());
    // Also test idempotence.
    NetParameter double_split_insert_param;
    InsertSplits(actual_output_param, &amp;double_split_insert_param);
    EXPECT_EQ(actual_output_param.DebugString(),
       double_split_insert_param.DebugString());
  }
};

TEST_F(SplitLayerInsertionTest, TestNoInsertion1) {
  const string&amp; input_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss' &quot;
      &quot;  type: 'SoftmaxWithLoss' &quot;
      &quot;  bottom: 'innerprod' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, input_proto);
}

TEST_F(SplitLayerInsertionTest, TestNoInsertion2) {
  const string&amp; input_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'data_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'data_split_0' &quot;
      &quot;  top: 'data_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_split_0' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_split_1' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, input_proto);
}

TEST_F(SplitLayerInsertionTest, TestNoInsertionImageNet) {
  const string&amp; input_proto =
      &quot;name: 'CaffeNet' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  data_param { &quot;
      &quot;    source: '/home/jiayq/Data/ILSVRC12/train-leveldb' &quot;
      &quot;    batch_size: 256 &quot;
      &quot;  } &quot;
      &quot;  transform_param { &quot;
      &quot;    crop_size: 227 &quot;
      &quot;    mirror: true &quot;
      &quot;    mean_file: '/home/jiayq/Data/ILSVRC12/image_mean.binaryproto' &quot;
      &quot;  } &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'conv1' &quot;
      &quot;  type: 'Convolution' &quot;
      &quot;  convolution_param { &quot;
      &quot;    num_output: 96 &quot;
      &quot;    kernel_size: 11 &quot;
      &quot;    stride: 4 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 0. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'conv1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu1' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'conv1' &quot;
      &quot;  top: 'conv1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'pool1' &quot;
      &quot;  type: 'Pooling' &quot;
      &quot;  pooling_param { &quot;
      &quot;    pool: MAX &quot;
      &quot;    kernel_size: 3 &quot;
      &quot;    stride: 2 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'conv1' &quot;
      &quot;  top: 'pool1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'norm1' &quot;
      &quot;  type: 'LRN' &quot;
      &quot;  lrn_param { &quot;
      &quot;    local_size: 5 &quot;
      &quot;    alpha: 0.0001 &quot;
      &quot;    beta: 0.75 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'pool1' &quot;
      &quot;  top: 'norm1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'conv2' &quot;
      &quot;  type: 'Convolution' &quot;
      &quot;  convolution_param { &quot;
      &quot;    num_output: 256 &quot;
      &quot;    group: 2 &quot;
      &quot;    kernel_size: 5 &quot;
      &quot;    pad: 2 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 1. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'norm1' &quot;
      &quot;  top: 'conv2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu2' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'conv2' &quot;
      &quot;  top: 'conv2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'pool2' &quot;
      &quot;  type: 'Pooling' &quot;
      &quot;  pooling_param { &quot;
      &quot;    pool: MAX &quot;
      &quot;    kernel_size: 3 &quot;
      &quot;    stride: 2 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'conv2' &quot;
      &quot;  top: 'pool2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'norm2' &quot;
      &quot;  type: 'LRN' &quot;
      &quot;  lrn_param { &quot;
      &quot;    local_size: 5 &quot;
      &quot;    alpha: 0.0001 &quot;
      &quot;    beta: 0.75 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'pool2' &quot;
      &quot;  top: 'norm2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'conv3' &quot;
      &quot;  type: 'Convolution' &quot;
      &quot;  convolution_param { &quot;
      &quot;    num_output: 384 &quot;
      &quot;    kernel_size: 3 &quot;
      &quot;    pad: 1 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 0. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'norm2' &quot;
      &quot;  top: 'conv3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu3' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'conv3' &quot;
      &quot;  top: 'conv3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'conv4' &quot;
      &quot;  type: 'Convolution' &quot;
      &quot;  convolution_param { &quot;
      &quot;    num_output: 384 &quot;
      &quot;    group: 2 &quot;
      &quot;    kernel_size: 3 &quot;
      &quot;    pad: 1 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 1. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'conv3' &quot;
      &quot;  top: 'conv4' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu4' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'conv4' &quot;
      &quot;  top: 'conv4' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'conv5' &quot;
      &quot;  type: 'Convolution' &quot;
      &quot;  convolution_param { &quot;
      &quot;    num_output: 256 &quot;
      &quot;    group: 2 &quot;
      &quot;    kernel_size: 3 &quot;
      &quot;    pad: 1 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 1. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'conv4' &quot;
      &quot;  top: 'conv5' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu5' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'conv5' &quot;
      &quot;  top: 'conv5' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'pool5' &quot;
      &quot;  type: 'Pooling' &quot;
      &quot;  pooling_param { &quot;
      &quot;    kernel_size: 3 &quot;
      &quot;    pool: MAX &quot;
      &quot;    stride: 2 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'conv5' &quot;
      &quot;  top: 'pool5' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'fc6' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 4096 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.005 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 1. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'pool5' &quot;
      &quot;  top: 'fc6' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu6' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'fc6' &quot;
      &quot;  top: 'fc6' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'drop6' &quot;
      &quot;  type: 'Dropout' &quot;
      &quot;  dropout_param { &quot;
      &quot;    dropout_ratio: 0.5 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'fc6' &quot;
      &quot;  top: 'fc6' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'fc7' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 4096 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.005 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 1. &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'fc6' &quot;
      &quot;  top: 'fc7' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu7' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'fc7' &quot;
      &quot;  top: 'fc7' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'drop7' &quot;
      &quot;  type: 'Dropout' &quot;
      &quot;  dropout_param { &quot;
      &quot;    dropout_ratio: 0.5 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'fc7' &quot;
      &quot;  top: 'fc7' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'fc8' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 1000 &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;    bias_filler { &quot;
      &quot;      type: 'constant' &quot;
      &quot;      value: 0 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 1 &quot;
      &quot;    decay_mult: 1 &quot;
      &quot;  } &quot;
      &quot;  param { &quot;
      &quot;    lr_mult: 2 &quot;
      &quot;    decay_mult: 0 &quot;
      &quot;  } &quot;
      &quot;  bottom: 'fc7' &quot;
      &quot;  top: 'fc8' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss' &quot;
      &quot;  type: 'SoftmaxWithLoss' &quot;
      &quot;  bottom: 'fc8' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, input_proto);
}

TEST_F(SplitLayerInsertionTest, TestNoInsertionWithInPlace) {
  const string&amp; input_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'innerprod' &quot;
      &quot;  top: 'innerprod' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss' &quot;
      &quot;  type: 'SoftmaxWithLoss' &quot;
      &quot;  bottom: 'innerprod' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, input_proto);
}

TEST_F(SplitLayerInsertionTest, TestLossInsertion) {
  const string&amp; input_proto =
      &quot;name: 'UnsharedWeightsNetwork' &quot;
      &quot;force_backward: true &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'DummyData' &quot;
      &quot;  dummy_data_param { &quot;
      &quot;    num: 5 &quot;
      &quot;    channels: 2 &quot;
      &quot;    height: 3 &quot;
      &quot;    width: 4 &quot;
      &quot;    data_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  top: 'data' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerproduct1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 10 &quot;
      &quot;    bias_term: false &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 10 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { name: 'unsharedweights1' } &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerproduct1' &quot;
      &quot;  loss_weight: 2.5 &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerproduct2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 10 &quot;
      &quot;    bias_term: false &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 10 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { name: 'unsharedweights2' } &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerproduct2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerproduct1' &quot;
      &quot;  bottom: 'innerproduct2' &quot;
      &quot;} &quot;;
  const string&amp; expected_output_proto =
      &quot;name: 'UnsharedWeightsNetwork' &quot;
      &quot;force_backward: true &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'DummyData' &quot;
      &quot;  dummy_data_param { &quot;
      &quot;    num: 5 &quot;
      &quot;    channels: 2 &quot;
      &quot;    height: 3 &quot;
      &quot;    width: 4 &quot;
      &quot;    data_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 0.01 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  top: 'data' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'data_data_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'data_data_0_split_0' &quot;
      &quot;  top: 'data_data_0_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerproduct1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 10 &quot;
      &quot;    bias_term: false &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 10 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { name: 'unsharedweights1' } &quot;
      &quot;  bottom: 'data_data_0_split_0' &quot;
      &quot;  top: 'innerproduct1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerproduct1_innerproduct1_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'innerproduct1' &quot;
      &quot;  top: 'innerproduct1_innerproduct1_0_split_0' &quot;
      &quot;  top: 'innerproduct1_innerproduct1_0_split_1' &quot;
      &quot;  loss_weight: 2.5 &quot;
      &quot;  loss_weight: 0 &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerproduct2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  inner_product_param { &quot;
      &quot;    num_output: 10 &quot;
      &quot;    bias_term: false &quot;
      &quot;    weight_filler { &quot;
      &quot;      type: 'gaussian' &quot;
      &quot;      std: 10 &quot;
      &quot;    } &quot;
      &quot;  } &quot;
      &quot;  param { name: 'unsharedweights2' } &quot;
      &quot;  bottom: 'data_data_0_split_1' &quot;
      &quot;  top: 'innerproduct2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerproduct1_innerproduct1_0_split_1' &quot;
      &quot;  bottom: 'innerproduct2' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, expected_output_proto);
}

TEST_F(SplitLayerInsertionTest, TestInsertion) {
  const string&amp; input_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod3' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss1' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss2' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;  bottom: 'innerprod3' &quot;
      &quot;} &quot;;
  const string&amp; expected_output_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'data_data_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'data_data_0_split_0' &quot;
      &quot;  top: 'data_data_0_split_1' &quot;
      &quot;  top: 'data_data_0_split_2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_data_0_split_0' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_data_0_split_1' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2_innerprod2_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;  top: 'innerprod2_innerprod2_0_split_0' &quot;
      &quot;  top: 'innerprod2_innerprod2_0_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod3' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_data_0_split_2' &quot;
      &quot;  top: 'innerprod3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss1' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  bottom: 'innerprod2_innerprod2_0_split_0' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss2' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod2_innerprod2_0_split_1' &quot;
      &quot;  bottom: 'innerprod3' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, expected_output_proto);
}

TEST_F(SplitLayerInsertionTest, TestInsertionTwoTop) {
  const string&amp; input_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod3' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod4' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;  top: 'innerprod4' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss1' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  bottom: 'innerprod3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss2' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;  bottom: 'innerprod4' &quot;
      &quot;} &quot;;
  const string&amp; expected_output_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'data_data_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'data_data_0_split_0' &quot;
      &quot;  top: 'data_data_0_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'label_data_1_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;  top: 'label_data_1_split_0' &quot;
      &quot;  top: 'label_data_1_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_data_0_split_0' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'label_data_1_split_0' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod3' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_data_0_split_1' &quot;
      &quot;  top: 'innerprod3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod4' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'label_data_1_split_1' &quot;
      &quot;  top: 'innerprod4' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss1' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  bottom: 'innerprod3' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss2' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;  bottom: 'innerprod4' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, expected_output_proto);
}

TEST_F(SplitLayerInsertionTest, TestWithInPlace) {
  const string&amp; input_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu1' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss1' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss2' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;} &quot;;
  const string&amp; expected_output_proto =
      &quot;name: 'TestNetwork' &quot;
      &quot;layer { &quot;
      &quot;  name: 'data' &quot;
      &quot;  type: 'Data' &quot;
      &quot;  top: 'data' &quot;
      &quot;  top: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'data_data_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'data' &quot;
      &quot;  top: 'data_data_0_split_0' &quot;
      &quot;  top: 'data_data_0_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'data_data_0_split_0' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'relu1' &quot;
      &quot;  type: 'ReLU' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  top: 'innerprod1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod1_relu1_0_split' &quot;
      &quot;  type: 'Split' &quot;
      &quot;  bottom: 'innerprod1' &quot;
      &quot;  top: 'innerprod1_relu1_0_split_0' &quot;
      &quot;  top: 'innerprod1_relu1_0_split_1' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'innerprod2' &quot;
      &quot;  type: 'InnerProduct' &quot;
      &quot;  bottom: 'innerprod1_relu1_0_split_0' &quot;
      &quot;  top: 'innerprod2' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss1' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod1_relu1_0_split_1' &quot;
      &quot;  bottom: 'label' &quot;
      &quot;} &quot;
      &quot;layer { &quot;
      &quot;  name: 'loss2' &quot;
      &quot;  type: 'EuclideanLoss' &quot;
      &quot;  bottom: 'innerprod2' &quot;
      &quot;  bottom: 'data_data_0_split_1' &quot;
      &quot;} &quot;;
  this-&gt;RunInsertionTest(input_proto, expected_output_proto);
}

}  // namespace caffe
</PRE>
</div>
  </div>
</body>
</html>
