<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for lrn_layer.cpp &amp; test_mvn_layer.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for lrn_layer.cpp &amp; test_mvn_layer.cpp
      </h3>
<h1 align="center">
        6.1%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>lrn_layer.cpp (6.703911%)<th>test_mvn_layer.cpp (5.633803%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(118-128)<td><a href="#" name="0">(52-54)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>lrn_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;vector&gt;
2 #include "caffe/layers/lrn_layer.hpp"
3 #include "caffe/util/math_functions.hpp"
4 namespace caffe {
5 template &lt;typename Dtype&gt;
6 void LRNLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
7       const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
8   size_ = this-&gt;layer_param_.lrn_param().local_size();
9   CHECK_EQ(size_ % 2, 1) &lt;&lt; "LRN only supports odd values for local_size";
10   pre_pad_ = (size_ - 1) / 2;
11   alpha_ = this-&gt;layer_param_.lrn_param().alpha();
12   beta_ = this-&gt;layer_param_.lrn_param().beta();
13   k_ = this-&gt;layer_param_.lrn_param().k();
14   if (this-&gt;layer_param_.lrn_param().norm_region() ==
15       LRNParameter_NormRegion_WITHIN_CHANNEL) {
16     split_top_vec_.clear();
17     split_top_vec_.push_back(&amp;product_input_);
18     split_top_vec_.push_back(&amp;square_input_);
19     LayerParameter split_param;
20     split_layer_.reset(new SplitLayer&lt;Dtype&gt;(split_param));
21     split_layer_-&gt;SetUp(bottom, split_top_vec_);
22     square_bottom_vec_.clear();
23     square_top_vec_.clear();
24     square_bottom_vec_.push_back(&amp;square_input_);
25     square_top_vec_.push_back(&amp;square_output_);
26     LayerParameter square_param;
27     square_param.mutable_power_param()-&gt;set_power(Dtype(2));
28     square_layer_.reset(new PowerLayer&lt;Dtype&gt;(square_param));
29     square_layer_-&gt;SetUp(square_bottom_vec_, square_top_vec_);
30     pool_top_vec_.clear();
31     pool_top_vec_.push_back(&amp;pool_output_);
32     LayerParameter pool_param;
33     pool_param.mutable_pooling_param()-&gt;set_pool(
34         PoolingParameter_PoolMethod_AVE);
35     pool_param.mutable_pooling_param()-&gt;set_pad(pre_pad_);
36     pool_param.mutable_pooling_param()-&gt;set_kernel_size(size_);
37     pool_layer_.reset(new PoolingLayer&lt;Dtype&gt;(pool_param));
38     pool_layer_-&gt;SetUp(square_top_vec_, pool_top_vec_);
39     power_top_vec_.clear();
40     power_top_vec_.push_back(&amp;power_output_);
41     LayerParameter power_param;
42     power_param.mutable_power_param()-&gt;set_power(-beta_);
43     power_param.mutable_power_param()-&gt;set_scale(alpha_);
44     power_param.mutable_power_param()-&gt;set_shift(Dtype(1));
45     power_layer_.reset(new PowerLayer&lt;Dtype&gt;(power_param));
46     power_layer_-&gt;SetUp(pool_top_vec_, power_top_vec_);
47     product_bottom_vec_.clear();
48     product_bottom_vec_.push_back(&amp;product_input_);
49     product_bottom_vec_.push_back(&amp;power_output_);
50     LayerParameter product_param;
51     EltwiseParameter* eltwise_param = product_param.mutable_eltwise_param();
52     eltwise_param-&gt;set_operation(EltwiseParameter_EltwiseOp_PROD);
53     product_layer_.reset(new EltwiseLayer&lt;Dtype&gt;(product_param));
54     product_layer_-&gt;SetUp(product_bottom_vec_, top);
55   }
56 }
57 template &lt;typename Dtype&gt;
58 void LRNLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
59       const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
60   CHECK_EQ(4, bottom[0]-&gt;num_axes()) &lt;&lt; "Input must have 4 axes, "
61       &lt;&lt; "corresponding to (num, channels, height, width)";
62   num_ = bottom[0]-&gt;num();
63   channels_ = bottom[0]-&gt;channels();
64   height_ = bottom[0]-&gt;height();
65   width_ = bottom[0]-&gt;width();
66   switch (this-&gt;layer_param_.lrn_param().norm_region()) {
67   case LRNParameter_NormRegion_ACROSS_CHANNELS:
68     top[0]-&gt;Reshape(num_, channels_, height_, width_);
69     scale_.Reshape(num_, channels_, height_, width_);
70     break;
71   case LRNParameter_NormRegion_WITHIN_CHANNEL:
72     split_layer_-&gt;Reshape(bottom, split_top_vec_);
73     square_layer_-&gt;Reshape(square_bottom_vec_, square_top_vec_);
74     pool_layer_-&gt;Reshape(square_top_vec_, pool_top_vec_);
75     power_layer_-&gt;Reshape(pool_top_vec_, power_top_vec_);
76     product_layer_-&gt;Reshape(product_bottom_vec_, top);
77     break;
78   }
79 }
80 template &lt;typename Dtype&gt;
81 void LRNLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
82     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
83   switch (this-&gt;layer_param_.lrn_param().norm_region()) {
84   case LRNParameter_NormRegion_ACROSS_CHANNELS:
85     CrossChannelForward_cpu(bottom, top);
86     break;
87   case LRNParameter_NormRegion_WITHIN_CHANNEL:
88     WithinChannelForward(bottom, top);
89     break;
90   default:
91     LOG(FATAL) &lt;&lt; "Unknown normalization region.";
92   }
93 }
94 template &lt;typename Dtype&gt;
95 void LRNLayer&lt;Dtype&gt;::CrossChannelForward_cpu(
96     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
97   const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
98   Dtype* top_data = top[0]-&gt;mutable_cpu_data();
99   Dtype* scale_data = scale_.mutable_cpu_data();
100   for (int i = 0; i &lt; scale_.count(); ++i) {
101 <a name="0"></a>    scale_data[i] = k_;
102   }
103   Blob&lt;Dtype&gt; padded_square(1, channels_ + size_ - 1, height_, width_);
104 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  Dtype* padded_square_data = padded_square.mutable_cpu_data();
105   caffe_set(padded_square.count(), Dtype(0), padded_square_data);
106   Dtype alpha_over_size = alpha_ / size_;
107   for (int n = 0; n &lt; num_; ++n) {
108     caffe_sqr(channels_ * height_ * width_,
109         bottom_data + bottom[0]-&gt;offset(n),
110         padded_square_data + padded_square.offset(0, pre_pad_));
111     for (int c = 0; c &lt; size_; ++c) {</b></font>
112       caffe_axpy&lt;Dtype&gt;(height_ * width_, alpha_over_size,
113           padded_square_data + padded_square.offset(0, c),
114           scale_data + scale_.offset(n, 0));
115     }
116     for (int c = 1; c &lt; channels_; ++c) {
117       caffe_copy&lt;Dtype&gt;(height_ * width_,
118           scale_data + scale_.offset(n, c - 1),
119           scale_data + scale_.offset(n, c));
120       caffe_axpy&lt;Dtype&gt;(height_ * width_, alpha_over_size,
121           padded_square_data + padded_square.offset(0, c + size_ - 1),
122           scale_data + scale_.offset(n, c));
123       caffe_axpy&lt;Dtype&gt;(height_ * width_, -alpha_over_size,
124           padded_square_data + padded_square.offset(0, c - 1),
125           scale_data + scale_.offset(n, c));
126     }
127   }
128   caffe_powx&lt;Dtype&gt;(scale_.count(), scale_data, -beta_, top_data);
129   caffe_mul&lt;Dtype&gt;(scale_.count(), top_data, bottom_data, top_data);
130 }
131 template &lt;typename Dtype&gt;
132 void LRNLayer&lt;Dtype&gt;::WithinChannelForward(
133     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
134   split_layer_-&gt;Forward(bottom, split_top_vec_);
135   square_layer_-&gt;Forward(square_bottom_vec_, square_top_vec_);
136   pool_layer_-&gt;Forward(square_top_vec_, pool_top_vec_);
137   power_layer_-&gt;Forward(pool_top_vec_, power_top_vec_);
138   product_layer_-&gt;Forward(product_bottom_vec_, top);
139 }
140 template &lt;typename Dtype&gt;
141 void LRNLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
142     const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
143   switch (this-&gt;layer_param_.lrn_param().norm_region()) {
144   case LRNParameter_NormRegion_ACROSS_CHANNELS:
145     CrossChannelBackward_cpu(top, propagate_down, bottom);
146     break;
147   case LRNParameter_NormRegion_WITHIN_CHANNEL:
148     WithinChannelBackward(top, propagate_down, bottom);
149     break;
150   default:
151     LOG(FATAL) &lt;&lt; "Unknown normalization region.";
152   }
153 }
154 template &lt;typename Dtype&gt;
155 void LRNLayer&lt;Dtype&gt;::CrossChannelBackward_cpu(
156     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
157     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
158   const Dtype* top_diff = top[0]-&gt;cpu_diff();
159   const Dtype* top_data = top[0]-&gt;cpu_data();
160   const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
161   const Dtype* scale_data = scale_.cpu_data();
162   Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
163   Blob&lt;Dtype&gt; padded_ratio(1, channels_ + size_ - 1, height_, width_);
164   Blob&lt;Dtype&gt; accum_ratio(1, 1, height_, width_);
165   Dtype* padded_ratio_data = padded_ratio.mutable_cpu_data();
166   Dtype* accum_ratio_data = accum_ratio.mutable_cpu_data();
167   Dtype* accum_ratio_times_bottom = accum_ratio.mutable_cpu_diff();
168   caffe_set(padded_ratio.count(), Dtype(0), padded_ratio_data);
169   Dtype cache_ratio_value = 2. * alpha_ * beta_ / size_;
170   caffe_powx&lt;Dtype&gt;(scale_.count(), scale_data, -beta_, bottom_diff);
171   caffe_mul&lt;Dtype&gt;(scale_.count(), top_diff, bottom_diff, bottom_diff);
172   int inverse_pre_pad = size_ - (size_ + 1) / 2;
173   for (int n = 0; n &lt; num_; ++n) {
174     int block_offset = scale_.offset(n);
175     caffe_mul&lt;Dtype&gt;(channels_ * height_ * width_,
176         top_diff + block_offset, top_data + block_offset,
177         padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad));
178     caffe_div&lt;Dtype&gt;(channels_ * height_ * width_,
179         padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad),
180         scale_data + block_offset,
181         padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad));
182     caffe_set(accum_ratio.count(), Dtype(0), accum_ratio_data);
183     for (int c = 0; c &lt; size_ - 1; ++c) {
184       caffe_axpy&lt;Dtype&gt;(height_ * width_, 1.,
185           padded_ratio_data + padded_ratio.offset(0, c), accum_ratio_data);
186     }
187     for (int c = 0; c &lt; channels_; ++c) {
188       caffe_axpy&lt;Dtype&gt;(height_ * width_, 1.,
189           padded_ratio_data + padded_ratio.offset(0, c + size_ - 1),
190           accum_ratio_data);
191       caffe_mul&lt;Dtype&gt;(height_ * width_,
192           bottom_data + top[0]-&gt;offset(n, c),
193           accum_ratio_data, accum_ratio_times_bottom);
194       caffe_axpy&lt;Dtype&gt;(height_ * width_, -cache_ratio_value,
195           accum_ratio_times_bottom, bottom_diff + top[0]-&gt;offset(n, c));
196       caffe_axpy&lt;Dtype&gt;(height_ * width_, -1.,
197           padded_ratio_data + padded_ratio.offset(0, c), accum_ratio_data);
198     }
199   }
200 }
201 template &lt;typename Dtype&gt;
202 void LRNLayer&lt;Dtype&gt;::WithinChannelBackward(
203     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
204     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
205   if (propagate_down[0]) {
206     vector&lt;bool&gt; product_propagate_down(2, true);
207     product_layer_-&gt;Backward(top, product_propagate_down, product_bottom_vec_);
208     power_layer_-&gt;Backward(power_top_vec_, propagate_down, pool_top_vec_);
209     pool_layer_-&gt;Backward(pool_top_vec_, propagate_down, square_top_vec_);
210     square_layer_-&gt;Backward(square_top_vec_, propagate_down,
211                             square_bottom_vec_);
212     split_layer_-&gt;Backward(split_top_vec_, propagate_down, bottom);
213   }
214 }
215 #ifdef CPU_ONLY
216 STUB_GPU(LRNLayer);
217 STUB_GPU_FORWARD(LRNLayer, CrossChannelForward);
218 STUB_GPU_BACKWARD(LRNLayer, CrossChannelBackward);
219 #endif
220 INSTANTIATE_CLASS(LRNLayer);
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_mvn_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;vector&gt;
2 #include "caffe/blob.hpp"
3 #include "caffe/common.hpp"
4 #include "caffe/filler.hpp"
5 #include "caffe/layers/mvn_layer.hpp"
6 #include "google/protobuf/text_format.h"
7 #include "gtest/gtest.h"
8 #include "caffe/test/test_caffe_main.hpp"
9 #include "caffe/test/test_gradient_check_util.hpp"
10 namespace caffe {
11 template &lt;typename TypeParam&gt;
12 class MVNLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
13   typedef typename TypeParam::Dtype Dtype;
14  protected:
15   MVNLayerTest()
16       : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 4, 5)),
17         blob_top_(new Blob&lt;Dtype&gt;()) {
18     FillerParameter filler_param;
19     GaussianFiller&lt;Dtype&gt; filler(filler_param);
20     filler.Fill(this-&gt;blob_bottom_);
21     blob_bottom_vec_.push_back(blob_bottom_);
22     blob_top_vec_.push_back(blob_top_);
23   }
24   virtual ~MVNLayerTest() { delete blob_bottom_; delete blob_top_; }
25   Blob&lt;Dtype&gt;* const blob_bottom_;
26   Blob&lt;Dtype&gt;* const blob_top_;
27   vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
28   vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
29 };
30 TYPED_TEST_CASE(MVNLayerTest, TestDtypesAndDevices);
31 TYPED_TEST(MVNLayerTest, TestForward) {
32   typedef typename TypeParam::Dtype Dtype;
33   LayerParameter layer_param;
34   MVNLayer&lt;Dtype&gt; layer(layer_param);
35   layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
36   layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
37   int num = this-&gt;blob_bottom_-&gt;num();
38   int channels = this-&gt;blob_bottom_-&gt;channels();
39   int height = this-&gt;blob_bottom_-&gt;height();
40   int width = this-&gt;blob_bottom_-&gt;width();
41 <a name="0"></a>
42   for (int i = 0; i &lt; num; ++i) {
43     for (int j = 0; j &lt; channels; ++j) {
44 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      Dtype sum = 0, var = 0;
45       for (int k = 0; k &lt; height; ++k) {
46         for (int l = 0; l &lt; width; ++l) {</b></font>
47           Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
48           sum += data;
49           var += data * data;
50         }
51       }
52       sum /= height * width;
53       var /= height * width;
54       const Dtype kErrorBound = 0.001;
55       EXPECT_NEAR(0, sum, kErrorBound);
56       EXPECT_NEAR(1, var, kErrorBound);
57     }
58   }
59 }
60 TYPED_TEST(MVNLayerTest, TestForwardMeanOnly) {
61   typedef typename TypeParam::Dtype Dtype;
62   LayerParameter layer_param;
63   CHECK(google::protobuf::TextFormat::ParseFromString(
64       "mvn_param{normalize_variance: false}", &amp;layer_param));
65   MVNLayer&lt;Dtype&gt; layer(layer_param);
66   layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
67   layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
68   int num = this-&gt;blob_bottom_-&gt;num();
69   int channels = this-&gt;blob_bottom_-&gt;channels();
70   int height = this-&gt;blob_bottom_-&gt;height();
71   int width = this-&gt;blob_bottom_-&gt;width();
72   for (int i = 0; i &lt; num; ++i) {
73     for (int j = 0; j &lt; channels; ++j) {
74       Dtype sum = 0, var = 0;
75       for (int k = 0; k &lt; height; ++k) {
76         for (int l = 0; l &lt; width; ++l) {
77           Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
78           sum += data;
79           var += data * data;
80         }
81       }
82       sum /= height * width;
83       const Dtype kErrorBound = 0.001;
84       EXPECT_NEAR(0, sum, kErrorBound);
85     }
86   }
87 }
88 TYPED_TEST(MVNLayerTest, TestForwardAcrossChannels) {
89   typedef typename TypeParam::Dtype Dtype;
90   LayerParameter layer_param;
91   CHECK(google::protobuf::TextFormat::ParseFromString(
92       "mvn_param{across_channels: true}", &amp;layer_param));
93   MVNLayer&lt;Dtype&gt; layer(layer_param);
94   layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
95   layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
96   int num = this-&gt;blob_bottom_-&gt;num();
97   int channels = this-&gt;blob_bottom_-&gt;channels();
98   int height = this-&gt;blob_bottom_-&gt;height();
99   int width = this-&gt;blob_bottom_-&gt;width();
100   for (int i = 0; i &lt; num; ++i) {
101     Dtype sum = 0, var = 0;
102     for (int j = 0; j &lt; channels; ++j) {
103       for (int k = 0; k &lt; height; ++k) {
104         for (int l = 0; l &lt; width; ++l) {
105           Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
106           sum += data;
107           var += data * data;
108         }
109       }
110     }
111     sum /= height * width * channels;
112     var /= height * width * channels;
113     const Dtype kErrorBound = 0.001;
114     EXPECT_NEAR(0, sum, kErrorBound);
115     EXPECT_NEAR(1, var, kErrorBound);
116   }
117 }
118 TYPED_TEST(MVNLayerTest, TestGradient) {
119   typedef typename TypeParam::Dtype Dtype;
120   LayerParameter layer_param;
121   MVNLayer&lt;Dtype&gt; layer(layer_param);
122   GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
123   checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
124       this-&gt;blob_top_vec_);
125 }
126 TYPED_TEST(MVNLayerTest, TestGradientMeanOnly) {
127   typedef typename TypeParam::Dtype Dtype;
128   LayerParameter layer_param;
129   CHECK(google::protobuf::TextFormat::ParseFromString(
130       "mvn_param{normalize_variance: false}", &amp;layer_param));
131   MVNLayer&lt;Dtype&gt; layer(layer_param);
132   GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
133   checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
134       this-&gt;blob_top_vec_);
135 }
136 TYPED_TEST(MVNLayerTest, TestGradientAcrossChannels) {
137   typedef typename TypeParam::Dtype Dtype;
138   LayerParameter layer_param;
139   CHECK(google::protobuf::TextFormat::ParseFromString(
140       "mvn_param{across_channels: true}", &amp;layer_param));
141   MVNLayer&lt;Dtype&gt; layer(layer_param);
142   GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
143   checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
144       this-&gt;blob_top_vec_);
145 }
}  </pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
