<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for lrn_layer.cpp &amp; test_mvn_layer.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for lrn_layer.cpp &amp; test_mvn_layer.cpp
      </h3>
<h1 align="center">
        6.1%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>lrn_layer.cpp (6.703911%)<th>test_mvn_layer.cpp (5.633803%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(118-128)<td><a href="#" name="0">(52-54)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>lrn_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;vector&gt;
#include "caffe/layers/lrn_layer.hpp"
#include "caffe/util/math_functions.hpp"
namespace caffe {
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  size_ = this-&gt;layer_param_.lrn_param().local_size();
  CHECK_EQ(size_ % 2, 1) &lt;&lt; "LRN only supports odd values for local_size";
  pre_pad_ = (size_ - 1) / 2;
  alpha_ = this-&gt;layer_param_.lrn_param().alpha();
  beta_ = this-&gt;layer_param_.lrn_param().beta();
  k_ = this-&gt;layer_param_.lrn_param().k();
  if (this-&gt;layer_param_.lrn_param().norm_region() ==
      LRNParameter_NormRegion_WITHIN_CHANNEL) {
    split_top_vec_.clear();
    split_top_vec_.push_back(&amp;product_input_);
    split_top_vec_.push_back(&amp;square_input_);
    LayerParameter split_param;
    split_layer_.reset(new SplitLayer&lt;Dtype&gt;(split_param));
    split_layer_-&gt;SetUp(bottom, split_top_vec_);
    square_bottom_vec_.clear();
    square_top_vec_.clear();
    square_bottom_vec_.push_back(&amp;square_input_);
    square_top_vec_.push_back(&amp;square_output_);
    LayerParameter square_param;
    square_param.mutable_power_param()-&gt;set_power(Dtype(2));
    square_layer_.reset(new PowerLayer&lt;Dtype&gt;(square_param));
    square_layer_-&gt;SetUp(square_bottom_vec_, square_top_vec_);
    pool_top_vec_.clear();
    pool_top_vec_.push_back(&amp;pool_output_);
    LayerParameter pool_param;
    pool_param.mutable_pooling_param()-&gt;set_pool(
        PoolingParameter_PoolMethod_AVE);
    pool_param.mutable_pooling_param()-&gt;set_pad(pre_pad_);
    pool_param.mutable_pooling_param()-&gt;set_kernel_size(size_);
    pool_layer_.reset(new PoolingLayer&lt;Dtype&gt;(pool_param));
    pool_layer_-&gt;SetUp(square_top_vec_, pool_top_vec_);
    power_top_vec_.clear();
    power_top_vec_.push_back(&amp;power_output_);
    LayerParameter power_param;
    power_param.mutable_power_param()-&gt;set_power(-beta_);
    power_param.mutable_power_param()-&gt;set_scale(alpha_);
    power_param.mutable_power_param()-&gt;set_shift(Dtype(1));
    power_layer_.reset(new PowerLayer&lt;Dtype&gt;(power_param));
    power_layer_-&gt;SetUp(pool_top_vec_, power_top_vec_);
    product_bottom_vec_.clear();
    product_bottom_vec_.push_back(&amp;product_input_);
    product_bottom_vec_.push_back(&amp;power_output_);
    LayerParameter product_param;
    EltwiseParameter* eltwise_param = product_param.mutable_eltwise_param();
    eltwise_param-&gt;set_operation(EltwiseParameter_EltwiseOp_PROD);
    product_layer_.reset(new EltwiseLayer&lt;Dtype&gt;(product_param));
    product_layer_-&gt;SetUp(product_bottom_vec_, top);
  }
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  CHECK_EQ(4, bottom[0]-&gt;num_axes()) &lt;&lt; "Input must have 4 axes, "
      &lt;&lt; "corresponding to (num, channels, height, width)";
  num_ = bottom[0]-&gt;num();
  channels_ = bottom[0]-&gt;channels();
  height_ = bottom[0]-&gt;height();
  width_ = bottom[0]-&gt;width();
  switch (this-&gt;layer_param_.lrn_param().norm_region()) {
  case LRNParameter_NormRegion_ACROSS_CHANNELS:
    top[0]-&gt;Reshape(num_, channels_, height_, width_);
    scale_.Reshape(num_, channels_, height_, width_);
    break;
  case LRNParameter_NormRegion_WITHIN_CHANNEL:
    split_layer_-&gt;Reshape(bottom, split_top_vec_);
    square_layer_-&gt;Reshape(square_bottom_vec_, square_top_vec_);
    pool_layer_-&gt;Reshape(square_top_vec_, pool_top_vec_);
    power_layer_-&gt;Reshape(pool_top_vec_, power_top_vec_);
    product_layer_-&gt;Reshape(product_bottom_vec_, top);
    break;
  }
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  switch (this-&gt;layer_param_.lrn_param().norm_region()) {
  case LRNParameter_NormRegion_ACROSS_CHANNELS:
    CrossChannelForward_cpu(bottom, top);
    break;
  case LRNParameter_NormRegion_WITHIN_CHANNEL:
    WithinChannelForward(bottom, top);
    break;
  default:
    LOG(FATAL) &lt;&lt; "Unknown normalization region.";
  }
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::CrossChannelForward_cpu(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  Dtype* scale_data = scale_.mutable_cpu_data();
  for (int i = 0; i &lt; scale_.count(); ++i) {
<a name="0"></a>    scale_data[i] = k_;
  }
  Blob&lt;Dtype&gt; padded_square(1, channels_ + size_ - 1, height_, width_);
<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  Dtype* padded_square_data = padded_square.mutable_cpu_data();
  caffe_set(padded_square.count(), Dtype(0), padded_square_data);
  Dtype alpha_over_size = alpha_ / size_;
  for (int n = 0; n &lt; num_; ++n) {
    caffe_sqr(channels_ * height_ * width_,
        bottom_data + bottom[0]-&gt;offset(n),
        padded_square_data + padded_square.offset(0, pre_pad_));
    for (int c = 0; c &lt; size_; ++c) {</b></font>
      caffe_axpy&lt;Dtype&gt;(height_ * width_, alpha_over_size,
          padded_square_data + padded_square.offset(0, c),
          scale_data + scale_.offset(n, 0));
    }
    for (int c = 1; c &lt; channels_; ++c) {
      caffe_copy&lt;Dtype&gt;(height_ * width_,
          scale_data + scale_.offset(n, c - 1),
          scale_data + scale_.offset(n, c));
      caffe_axpy&lt;Dtype&gt;(height_ * width_, alpha_over_size,
          padded_square_data + padded_square.offset(0, c + size_ - 1),
          scale_data + scale_.offset(n, c));
      caffe_axpy&lt;Dtype&gt;(height_ * width_, -alpha_over_size,
          padded_square_data + padded_square.offset(0, c - 1),
          scale_data + scale_.offset(n, c));
    }
  }
  caffe_powx&lt;Dtype&gt;(scale_.count(), scale_data, -beta_, top_data);
  caffe_mul&lt;Dtype&gt;(scale_.count(), top_data, bottom_data, top_data);
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::WithinChannelForward(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  split_layer_-&gt;Forward(bottom, split_top_vec_);
  square_layer_-&gt;Forward(square_bottom_vec_, square_top_vec_);
  pool_layer_-&gt;Forward(square_top_vec_, pool_top_vec_);
  power_layer_-&gt;Forward(pool_top_vec_, power_top_vec_);
  product_layer_-&gt;Forward(product_bottom_vec_, top);
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  switch (this-&gt;layer_param_.lrn_param().norm_region()) {
  case LRNParameter_NormRegion_ACROSS_CHANNELS:
    CrossChannelBackward_cpu(top, propagate_down, bottom);
    break;
  case LRNParameter_NormRegion_WITHIN_CHANNEL:
    WithinChannelBackward(top, propagate_down, bottom);
    break;
  default:
    LOG(FATAL) &lt;&lt; "Unknown normalization region.";
  }
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::CrossChannelBackward_cpu(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  const Dtype* top_diff = top[0]-&gt;cpu_diff();
  const Dtype* top_data = top[0]-&gt;cpu_data();
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  const Dtype* scale_data = scale_.cpu_data();
  Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
  Blob&lt;Dtype&gt; padded_ratio(1, channels_ + size_ - 1, height_, width_);
  Blob&lt;Dtype&gt; accum_ratio(1, 1, height_, width_);
  Dtype* padded_ratio_data = padded_ratio.mutable_cpu_data();
  Dtype* accum_ratio_data = accum_ratio.mutable_cpu_data();
  Dtype* accum_ratio_times_bottom = accum_ratio.mutable_cpu_diff();
  caffe_set(padded_ratio.count(), Dtype(0), padded_ratio_data);
  Dtype cache_ratio_value = 2. * alpha_ * beta_ / size_;
  caffe_powx&lt;Dtype&gt;(scale_.count(), scale_data, -beta_, bottom_diff);
  caffe_mul&lt;Dtype&gt;(scale_.count(), top_diff, bottom_diff, bottom_diff);
  int inverse_pre_pad = size_ - (size_ + 1) / 2;
  for (int n = 0; n &lt; num_; ++n) {
    int block_offset = scale_.offset(n);
    caffe_mul&lt;Dtype&gt;(channels_ * height_ * width_,
        top_diff + block_offset, top_data + block_offset,
        padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad));
    caffe_div&lt;Dtype&gt;(channels_ * height_ * width_,
        padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad),
        scale_data + block_offset,
        padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad));
    caffe_set(accum_ratio.count(), Dtype(0), accum_ratio_data);
    for (int c = 0; c &lt; size_ - 1; ++c) {
      caffe_axpy&lt;Dtype&gt;(height_ * width_, 1.,
          padded_ratio_data + padded_ratio.offset(0, c), accum_ratio_data);
    }
    for (int c = 0; c &lt; channels_; ++c) {
      caffe_axpy&lt;Dtype&gt;(height_ * width_, 1.,
          padded_ratio_data + padded_ratio.offset(0, c + size_ - 1),
          accum_ratio_data);
      caffe_mul&lt;Dtype&gt;(height_ * width_,
          bottom_data + top[0]-&gt;offset(n, c),
          accum_ratio_data, accum_ratio_times_bottom);
      caffe_axpy&lt;Dtype&gt;(height_ * width_, -cache_ratio_value,
          accum_ratio_times_bottom, bottom_diff + top[0]-&gt;offset(n, c));
      caffe_axpy&lt;Dtype&gt;(height_ * width_, -1.,
          padded_ratio_data + padded_ratio.offset(0, c), accum_ratio_data);
    }
  }
}
template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::WithinChannelBackward(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (propagate_down[0]) {
    vector&lt;bool&gt; product_propagate_down(2, true);
    product_layer_-&gt;Backward(top, product_propagate_down, product_bottom_vec_);
    power_layer_-&gt;Backward(power_top_vec_, propagate_down, pool_top_vec_);
    pool_layer_-&gt;Backward(pool_top_vec_, propagate_down, square_top_vec_);
    square_layer_-&gt;Backward(square_top_vec_, propagate_down,
                            square_bottom_vec_);
    split_layer_-&gt;Backward(split_top_vec_, propagate_down, bottom);
  }
}
#ifdef CPU_ONLY
STUB_GPU(LRNLayer);
STUB_GPU_FORWARD(LRNLayer, CrossChannelForward);
STUB_GPU_BACKWARD(LRNLayer, CrossChannelBackward);
#endif
INSTANTIATE_CLASS(LRNLayer);
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_mvn_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#include &lt;vector&gt;
#include "caffe/blob.hpp"
#include "caffe/common.hpp"
#include "caffe/filler.hpp"
#include "caffe/layers/mvn_layer.hpp"
#include "google/protobuf/text_format.h"
#include "gtest/gtest.h"
#include "caffe/test/test_caffe_main.hpp"
#include "caffe/test/test_gradient_check_util.hpp"
namespace caffe {
template &lt;typename TypeParam&gt;
class MVNLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  MVNLayerTest()
      : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 4, 5)),
        blob_top_(new Blob&lt;Dtype&gt;()) {
    FillerParameter filler_param;
    GaussianFiller&lt;Dtype&gt; filler(filler_param);
    filler.Fill(this-&gt;blob_bottom_);
    blob_bottom_vec_.push_back(blob_bottom_);
    blob_top_vec_.push_back(blob_top_);
  }
  virtual ~MVNLayerTest() { delete blob_bottom_; delete blob_top_; }
  Blob&lt;Dtype&gt;* const blob_bottom_;
  Blob&lt;Dtype&gt;* const blob_top_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
};
TYPED_TEST_CASE(MVNLayerTest, TestDtypesAndDevices);
TYPED_TEST(MVNLayerTest, TestForward) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  int num = this-&gt;blob_bottom_-&gt;num();
  int channels = this-&gt;blob_bottom_-&gt;channels();
  int height = this-&gt;blob_bottom_-&gt;height();
  int width = this-&gt;blob_bottom_-&gt;width();
<a name="0"></a>
  for (int i = 0; i &lt; num; ++i) {
    for (int j = 0; j &lt; channels; ++j) {
<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      Dtype sum = 0, var = 0;
      for (int k = 0; k &lt; height; ++k) {
        for (int l = 0; l &lt; width; ++l) {</b></font>
          Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
          sum += data;
          var += data * data;
        }
      }
      sum /= height * width;
      var /= height * width;
      const Dtype kErrorBound = 0.001;
      EXPECT_NEAR(0, sum, kErrorBound);
      EXPECT_NEAR(1, var, kErrorBound);
    }
  }
}
TYPED_TEST(MVNLayerTest, TestForwardMeanOnly) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      "mvn_param{normalize_variance: false}", &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  int num = this-&gt;blob_bottom_-&gt;num();
  int channels = this-&gt;blob_bottom_-&gt;channels();
  int height = this-&gt;blob_bottom_-&gt;height();
  int width = this-&gt;blob_bottom_-&gt;width();
  for (int i = 0; i &lt; num; ++i) {
    for (int j = 0; j &lt; channels; ++j) {
      Dtype sum = 0, var = 0;
      for (int k = 0; k &lt; height; ++k) {
        for (int l = 0; l &lt; width; ++l) {
          Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
          sum += data;
          var += data * data;
        }
      }
      sum /= height * width;
      const Dtype kErrorBound = 0.001;
      EXPECT_NEAR(0, sum, kErrorBound);
    }
  }
}
TYPED_TEST(MVNLayerTest, TestForwardAcrossChannels) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      "mvn_param{across_channels: true}", &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  int num = this-&gt;blob_bottom_-&gt;num();
  int channels = this-&gt;blob_bottom_-&gt;channels();
  int height = this-&gt;blob_bottom_-&gt;height();
  int width = this-&gt;blob_bottom_-&gt;width();
  for (int i = 0; i &lt; num; ++i) {
    Dtype sum = 0, var = 0;
    for (int j = 0; j &lt; channels; ++j) {
      for (int k = 0; k &lt; height; ++k) {
        for (int l = 0; l &lt; width; ++l) {
          Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
          sum += data;
          var += data * data;
        }
      }
    }
    sum /= height * width * channels;
    var /= height * width * channels;
    const Dtype kErrorBound = 0.001;
    EXPECT_NEAR(0, sum, kErrorBound);
    EXPECT_NEAR(1, var, kErrorBound);
  }
}
TYPED_TEST(MVNLayerTest, TestGradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}
TYPED_TEST(MVNLayerTest, TestGradientMeanOnly) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      "mvn_param{normalize_variance: false}", &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}
TYPED_TEST(MVNLayerTest, TestGradientAcrossChannels) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      "mvn_param{across_channels: true}", &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}
}  </pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
