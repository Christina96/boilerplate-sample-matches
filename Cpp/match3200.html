<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for lrn_layer.cpp & test_mvn_layer.cpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for lrn_layer.cpp & test_mvn_layer.cpp
      </h3>
      <h1 align="center">
        6.1%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>lrn_layer.cpp (6.703911%)<TH>test_mvn_layer.cpp (5.633803%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match3200-0.html#0',2,'match3200-1.html#0',3)" NAME="0">(118-128)<TD><A HREF="javascript:ZweiFrames('match3200-0.html#0',2,'match3200-1.html#0',3)" NAME="0">(52-54)</A><TD ALIGN=center><FONT COLOR="#ff0000">12</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>lrn_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;vector&gt;

#include &quot;caffe/layers/lrn_layer.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  size_ = this-&gt;layer_param_.lrn_param().local_size();
  CHECK_EQ(size_ % 2, 1) &lt;&lt; &quot;LRN only supports odd values for local_size&quot;;
  pre_pad_ = (size_ - 1) / 2;
  alpha_ = this-&gt;layer_param_.lrn_param().alpha();
  beta_ = this-&gt;layer_param_.lrn_param().beta();
  k_ = this-&gt;layer_param_.lrn_param().k();
  if (this-&gt;layer_param_.lrn_param().norm_region() ==
      LRNParameter_NormRegion_WITHIN_CHANNEL) {
    // Set up split_layer_ to use inputs in the numerator and denominator.
    split_top_vec_.clear();
    split_top_vec_.push_back(&amp;product_input_);
    split_top_vec_.push_back(&amp;square_input_);
    LayerParameter split_param;
    split_layer_.reset(new SplitLayer&lt;Dtype&gt;(split_param));
    split_layer_-&gt;SetUp(bottom, split_top_vec_);
    // Set up square_layer_ to square the inputs.
    square_bottom_vec_.clear();
    square_top_vec_.clear();
    square_bottom_vec_.push_back(&amp;square_input_);
    square_top_vec_.push_back(&amp;square_output_);
    LayerParameter square_param;
    square_param.mutable_power_param()-&gt;set_power(Dtype(2));
    square_layer_.reset(new PowerLayer&lt;Dtype&gt;(square_param));
    square_layer_-&gt;SetUp(square_bottom_vec_, square_top_vec_);
    // Set up pool_layer_ to sum over square neighborhoods of the input.
    pool_top_vec_.clear();
    pool_top_vec_.push_back(&amp;pool_output_);
    LayerParameter pool_param;
    pool_param.mutable_pooling_param()-&gt;set_pool(
        PoolingParameter_PoolMethod_AVE);
    pool_param.mutable_pooling_param()-&gt;set_pad(pre_pad_);
    pool_param.mutable_pooling_param()-&gt;set_kernel_size(size_);
    pool_layer_.reset(new PoolingLayer&lt;Dtype&gt;(pool_param));
    pool_layer_-&gt;SetUp(square_top_vec_, pool_top_vec_);
    // Set up power_layer_ to compute (1 + alpha_/N^2 s)^-beta_, where s is
    // the sum of a squared neighborhood (the output of pool_layer_).
    power_top_vec_.clear();
    power_top_vec_.push_back(&amp;power_output_);
    LayerParameter power_param;
    power_param.mutable_power_param()-&gt;set_power(-beta_);
    power_param.mutable_power_param()-&gt;set_scale(alpha_);
    power_param.mutable_power_param()-&gt;set_shift(Dtype(1));
    power_layer_.reset(new PowerLayer&lt;Dtype&gt;(power_param));
    power_layer_-&gt;SetUp(pool_top_vec_, power_top_vec_);
    // Set up a product_layer_ to compute outputs by multiplying inputs by the
    // inverse demoninator computed by the power layer.
    product_bottom_vec_.clear();
    product_bottom_vec_.push_back(&amp;product_input_);
    product_bottom_vec_.push_back(&amp;power_output_);
    LayerParameter product_param;
    EltwiseParameter* eltwise_param = product_param.mutable_eltwise_param();
    eltwise_param-&gt;set_operation(EltwiseParameter_EltwiseOp_PROD);
    product_layer_.reset(new EltwiseLayer&lt;Dtype&gt;(product_param));
    product_layer_-&gt;SetUp(product_bottom_vec_, top);
  }
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  CHECK_EQ(4, bottom[0]-&gt;num_axes()) &lt;&lt; &quot;Input must have 4 axes, &quot;
      &lt;&lt; &quot;corresponding to (num, channels, height, width)&quot;;
  num_ = bottom[0]-&gt;num();
  channels_ = bottom[0]-&gt;channels();
  height_ = bottom[0]-&gt;height();
  width_ = bottom[0]-&gt;width();
  switch (this-&gt;layer_param_.lrn_param().norm_region()) {
  case LRNParameter_NormRegion_ACROSS_CHANNELS:
    top[0]-&gt;Reshape(num_, channels_, height_, width_);
    scale_.Reshape(num_, channels_, height_, width_);
    break;
  case LRNParameter_NormRegion_WITHIN_CHANNEL:
    split_layer_-&gt;Reshape(bottom, split_top_vec_);
    square_layer_-&gt;Reshape(square_bottom_vec_, square_top_vec_);
    pool_layer_-&gt;Reshape(square_top_vec_, pool_top_vec_);
    power_layer_-&gt;Reshape(pool_top_vec_, power_top_vec_);
    product_layer_-&gt;Reshape(product_bottom_vec_, top);
    break;
  }
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  switch (this-&gt;layer_param_.lrn_param().norm_region()) {
  case LRNParameter_NormRegion_ACROSS_CHANNELS:
    CrossChannelForward_cpu(bottom, top);
    break;
  case LRNParameter_NormRegion_WITHIN_CHANNEL:
    WithinChannelForward(bottom, top);
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown normalization region.&quot;;
  }
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::CrossChannelForward_cpu(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  Dtype* scale_data = scale_.mutable_cpu_data();
  // start with the constant value
  for (int i = 0; i &lt; scale_.count(); ++i) {
<A NAME="0"></A>    scale_data[i] = k_;
  }
  Blob&lt;Dtype&gt; padded_square(1, channels_ + size_ - 1, height_, width_);
<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match3200-1.html#0',3,'match3200-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>  Dtype* padded_square_data = padded_square.mutable_cpu_data();
  caffe_set(padded_square.count(), Dtype(0), padded_square_data);
  Dtype alpha_over_size = alpha_ / size_;
  // go through the images
  for (int n = 0; n &lt; num_; ++n) {
    // compute the padded square
    caffe_sqr(channels_ * height_ * width_,
        bottom_data + bottom[0]-&gt;offset(n),
        padded_square_data + padded_square.offset(0, pre_pad_));
    // Create the first channel scale
    for (int c = 0; c &lt; size_; ++c) {</B></FONT>
      caffe_axpy&lt;Dtype&gt;(height_ * width_, alpha_over_size,
          padded_square_data + padded_square.offset(0, c),
          scale_data + scale_.offset(n, 0));
    }
    for (int c = 1; c &lt; channels_; ++c) {
      // copy previous scale
      caffe_copy&lt;Dtype&gt;(height_ * width_,
          scale_data + scale_.offset(n, c - 1),
          scale_data + scale_.offset(n, c));
      // add head
      caffe_axpy&lt;Dtype&gt;(height_ * width_, alpha_over_size,
          padded_square_data + padded_square.offset(0, c + size_ - 1),
          scale_data + scale_.offset(n, c));
      // subtract tail
      caffe_axpy&lt;Dtype&gt;(height_ * width_, -alpha_over_size,
          padded_square_data + padded_square.offset(0, c - 1),
          scale_data + scale_.offset(n, c));
    }
  }

  // In the end, compute output
  caffe_powx&lt;Dtype&gt;(scale_.count(), scale_data, -beta_, top_data);
  caffe_mul&lt;Dtype&gt;(scale_.count(), top_data, bottom_data, top_data);
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::WithinChannelForward(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  split_layer_-&gt;Forward(bottom, split_top_vec_);
  square_layer_-&gt;Forward(square_bottom_vec_, square_top_vec_);
  pool_layer_-&gt;Forward(square_top_vec_, pool_top_vec_);
  power_layer_-&gt;Forward(pool_top_vec_, power_top_vec_);
  product_layer_-&gt;Forward(product_bottom_vec_, top);
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  switch (this-&gt;layer_param_.lrn_param().norm_region()) {
  case LRNParameter_NormRegion_ACROSS_CHANNELS:
    CrossChannelBackward_cpu(top, propagate_down, bottom);
    break;
  case LRNParameter_NormRegion_WITHIN_CHANNEL:
    WithinChannelBackward(top, propagate_down, bottom);
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown normalization region.&quot;;
  }
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::CrossChannelBackward_cpu(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  const Dtype* top_diff = top[0]-&gt;cpu_diff();
  const Dtype* top_data = top[0]-&gt;cpu_data();
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  const Dtype* scale_data = scale_.cpu_data();
  Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
  Blob&lt;Dtype&gt; padded_ratio(1, channels_ + size_ - 1, height_, width_);
  Blob&lt;Dtype&gt; accum_ratio(1, 1, height_, width_);
  Dtype* padded_ratio_data = padded_ratio.mutable_cpu_data();
  Dtype* accum_ratio_data = accum_ratio.mutable_cpu_data();
  // We hack a little bit by using the diff() to store an additional result
  Dtype* accum_ratio_times_bottom = accum_ratio.mutable_cpu_diff();
  caffe_set(padded_ratio.count(), Dtype(0), padded_ratio_data);
  Dtype cache_ratio_value = 2. * alpha_ * beta_ / size_;

  caffe_powx&lt;Dtype&gt;(scale_.count(), scale_data, -beta_, bottom_diff);
  caffe_mul&lt;Dtype&gt;(scale_.count(), top_diff, bottom_diff, bottom_diff);

  // go through individual data
  int inverse_pre_pad = size_ - (size_ + 1) / 2;
  for (int n = 0; n &lt; num_; ++n) {
    int block_offset = scale_.offset(n);
    // first, compute diff_i * y_i / s_i
    caffe_mul&lt;Dtype&gt;(channels_ * height_ * width_,
        top_diff + block_offset, top_data + block_offset,
        padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad));
    caffe_div&lt;Dtype&gt;(channels_ * height_ * width_,
        padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad),
        scale_data + block_offset,
        padded_ratio_data + padded_ratio.offset(0, inverse_pre_pad));
    // Now, compute the accumulated ratios and the bottom diff
    caffe_set(accum_ratio.count(), Dtype(0), accum_ratio_data);
    for (int c = 0; c &lt; size_ - 1; ++c) {
      caffe_axpy&lt;Dtype&gt;(height_ * width_, 1.,
          padded_ratio_data + padded_ratio.offset(0, c), accum_ratio_data);
    }
    for (int c = 0; c &lt; channels_; ++c) {
      caffe_axpy&lt;Dtype&gt;(height_ * width_, 1.,
          padded_ratio_data + padded_ratio.offset(0, c + size_ - 1),
          accum_ratio_data);
      // compute bottom diff
      caffe_mul&lt;Dtype&gt;(height_ * width_,
          bottom_data + top[0]-&gt;offset(n, c),
          accum_ratio_data, accum_ratio_times_bottom);
      caffe_axpy&lt;Dtype&gt;(height_ * width_, -cache_ratio_value,
          accum_ratio_times_bottom, bottom_diff + top[0]-&gt;offset(n, c));
      caffe_axpy&lt;Dtype&gt;(height_ * width_, -1.,
          padded_ratio_data + padded_ratio.offset(0, c), accum_ratio_data);
    }
  }
}

template &lt;typename Dtype&gt;
void LRNLayer&lt;Dtype&gt;::WithinChannelBackward(
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (propagate_down[0]) {
    vector&lt;bool&gt; product_propagate_down(2, true);
    product_layer_-&gt;Backward(top, product_propagate_down, product_bottom_vec_);
    power_layer_-&gt;Backward(power_top_vec_, propagate_down, pool_top_vec_);
    pool_layer_-&gt;Backward(pool_top_vec_, propagate_down, square_top_vec_);
    square_layer_-&gt;Backward(square_top_vec_, propagate_down,
                            square_bottom_vec_);
    split_layer_-&gt;Backward(split_top_vec_, propagate_down, bottom);
  }
}

#ifdef CPU_ONLY
STUB_GPU(LRNLayer);
STUB_GPU_FORWARD(LRNLayer, CrossChannelForward);
STUB_GPU_BACKWARD(LRNLayer, CrossChannelBackward);
#endif

INSTANTIATE_CLASS(LRNLayer);

}  // namespace caffe
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_mvn_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;vector&gt;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/filler.hpp&quot;
#include &quot;caffe/layers/mvn_layer.hpp&quot;
#include &quot;google/protobuf/text_format.h&quot;
#include &quot;gtest/gtest.h&quot;

#include &quot;caffe/test/test_caffe_main.hpp&quot;
#include &quot;caffe/test/test_gradient_check_util.hpp&quot;

namespace caffe {

template &lt;typename TypeParam&gt;
class MVNLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
  typedef typename TypeParam::Dtype Dtype;
 protected:
  MVNLayerTest()
      : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 4, 5)),
        blob_top_(new Blob&lt;Dtype&gt;()) {
    // fill the values
    FillerParameter filler_param;
    GaussianFiller&lt;Dtype&gt; filler(filler_param);
    filler.Fill(this-&gt;blob_bottom_);
    blob_bottom_vec_.push_back(blob_bottom_);
    blob_top_vec_.push_back(blob_top_);
  }
  virtual ~MVNLayerTest() { delete blob_bottom_; delete blob_top_; }
  Blob&lt;Dtype&gt;* const blob_bottom_;
  Blob&lt;Dtype&gt;* const blob_top_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
};

TYPED_TEST_CASE(MVNLayerTest, TestDtypesAndDevices);

TYPED_TEST(MVNLayerTest, TestForward) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Test mean
  int num = this-&gt;blob_bottom_-&gt;num();
  int channels = this-&gt;blob_bottom_-&gt;channels();
  int height = this-&gt;blob_bottom_-&gt;height();
  int width = this-&gt;blob_bottom_-&gt;width();
<A NAME="0"></A>
  for (int i = 0; i &lt; num; ++i) {
    for (int j = 0; j &lt; channels; ++j) {
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match3200-0.html#0',2,'match3200-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>      Dtype sum = 0, var = 0;
      for (int k = 0; k &lt; height; ++k) {
        for (int l = 0; l &lt; width; ++l) {</B></FONT>
          Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
          sum += data;
          var += data * data;
        }
      }
      sum /= height * width;
      var /= height * width;

      const Dtype kErrorBound = 0.001;
      // expect zero mean
      EXPECT_NEAR(0, sum, kErrorBound);
      // expect unit variance
      EXPECT_NEAR(1, var, kErrorBound);
    }
  }
}

TYPED_TEST(MVNLayerTest, TestForwardMeanOnly) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      &quot;mvn_param{normalize_variance: false}&quot;, &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Test mean
  int num = this-&gt;blob_bottom_-&gt;num();
  int channels = this-&gt;blob_bottom_-&gt;channels();
  int height = this-&gt;blob_bottom_-&gt;height();
  int width = this-&gt;blob_bottom_-&gt;width();

  for (int i = 0; i &lt; num; ++i) {
    for (int j = 0; j &lt; channels; ++j) {
      Dtype sum = 0, var = 0;
      for (int k = 0; k &lt; height; ++k) {
        for (int l = 0; l &lt; width; ++l) {
          Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
          sum += data;
          var += data * data;
        }
      }
      sum /= height * width;

      const Dtype kErrorBound = 0.001;
      // expect zero mean
      EXPECT_NEAR(0, sum, kErrorBound);
    }
  }
}

TYPED_TEST(MVNLayerTest, TestForwardAcrossChannels) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      &quot;mvn_param{across_channels: true}&quot;, &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  layer.SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  layer.Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
  // Test mean
  int num = this-&gt;blob_bottom_-&gt;num();
  int channels = this-&gt;blob_bottom_-&gt;channels();
  int height = this-&gt;blob_bottom_-&gt;height();
  int width = this-&gt;blob_bottom_-&gt;width();

  for (int i = 0; i &lt; num; ++i) {
    Dtype sum = 0, var = 0;
    for (int j = 0; j &lt; channels; ++j) {
      for (int k = 0; k &lt; height; ++k) {
        for (int l = 0; l &lt; width; ++l) {
          Dtype data = this-&gt;blob_top_-&gt;data_at(i, j, k, l);
          sum += data;
          var += data * data;
        }
      }
    }
    sum /= height * width * channels;
    var /= height * width * channels;

    const Dtype kErrorBound = 0.001;
    // expect zero mean
    EXPECT_NEAR(0, sum, kErrorBound);
    // expect unit variance
    EXPECT_NEAR(1, var, kErrorBound);
  }
}

TYPED_TEST(MVNLayerTest, TestGradient) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

TYPED_TEST(MVNLayerTest, TestGradientMeanOnly) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      &quot;mvn_param{normalize_variance: false}&quot;, &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

TYPED_TEST(MVNLayerTest, TestGradientAcrossChannels) {
  typedef typename TypeParam::Dtype Dtype;
  LayerParameter layer_param;
  CHECK(google::protobuf::TextFormat::ParseFromString(
      &quot;mvn_param{across_channels: true}&quot;, &amp;layer_param));
  MVNLayer&lt;Dtype&gt; layer(layer_param);
  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
  checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
      this-&gt;blob_top_vec_);
}

}  // namespace caffe
</PRE>
</div>
  </div>
</body>
</html>
