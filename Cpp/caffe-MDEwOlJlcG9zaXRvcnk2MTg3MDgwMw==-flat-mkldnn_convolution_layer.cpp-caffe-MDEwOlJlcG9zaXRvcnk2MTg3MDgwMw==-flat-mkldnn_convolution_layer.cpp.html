
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 20, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_convolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKLDNN_SUPPORTED
2  #include <algorithm>
3  #include <cstdlib>
4  #include <vector>
5  #include "caffe/filler.hpp"
6  #include "caffe/layer.hpp"
7  #include "caffe/layers/mkldnn_layers.hpp"
8  #include "caffe/util/cpu_info.hpp"
9  namespace caffe {
10  template <typename Dtype>
11  MKLDNNConvolutionLayer<Dtype>::MKLDNNConvolutionLayer(const LayerParameter& param)
12              : MKLDNNLayer<Dtype>(param), ConvolutionLayer<Dtype>(param)
13              , fwd_bottom_data(NULL), fwd_top_data(NULL), fwd_weights_data(NULL), fwd_bias_data(NULL)
14              , bwdd_weights_data(NULL), bwdw_bottom_data(NULL)
15              , bwdd_bottom_diff(NULL), bwdd_top_diff(NULL)
16              , bwdw_top_diff(NULL), bwdw_weights_diff(NULL), bwdw_bias_diff(NULL)
17              , convFwd_pd(NULL), convBwdData_pd(NULL), convBwdWeights_pd(NULL)
18              , fwd_top_data_memory(NULL), bwdd_bottom_diff_memory(NULL)
19              , bwdw_weights_diff_memory(NULL), bwdw_bias_diff_memory(NULL)
20              , fwd_bottom_data_primitive(NULL), fwd_weights_data_primitive(NULL), fwd_bias_data_primitive(NULL)
21              , bwdd_top_diff_primitive(NULL), bwdd_weights_data_primitive(NULL)
22              , bwdw_top_diff_primitive(NULL), bwdw_bottom_data_primitive(NULL)
23              , width_(0), height_(0), depth_(0), width_out_(0), height_out_(0), depth_out_(0), kernel_w_(0), kernel_h_(0), kernel_d_(0)
24              , stride_w_(0), stride_h_(0), stride_d_(0), pad_w_(0), pad_h_(0), pad_d_(0),
25              bwdw_weights_diff_iter(NULL),
26              bwdw_bias_diff_iter(NULL),
27              bwdw_weights_diff_memory_iter(NULL),
28              bwdw_bias_diff_memory_iter(NULL)
29  {
30    PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
31    PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
32    PERFORMANCE_EVENT_ID_RESET(perf_id_bw_weights_);
33  }
34  template <typename Dtype>
35  void MKLDNNConvolutionLayer<Dtype>::compute_output_shape()
36  {
37      ConvolutionLayer<Dtype>::compute_output_shape();
38      CHECK_GT(this->output_shape_.size(), 1) << "MKLDNN Convolution layer expects at least 2D spatial output dimension!";
39      CHECK_LT(this->output_shape_.size(), 4) << "MKLDNN Convolution layer expects at most 3D spatial output dimension!";
40      if (this->output_shape_.size() == 2) {
41        this->height_out_ = this->output_shape_[0];
42        this->width_out_ = this->output_shape_[1];
43      } else {
44        this->depth_out_ = this->output_shape_[0];
45        this->height_out_ = this->output_shape_[1];
46        this->width_out_ = this->output_shape_[2];
47      }
48  }
49  template <typename Dtype>
50  void MKLDNNConvolutionLayer<Dtype>::init_properties(const vector<Blob<Dtype>*>& bottom
51                                                  , const vector<Blob<Dtype>*>& top)
52  {
53      CHECK_GT(this->num_spatial_axes_, 1) << "MKLDNN Convolution layer expects at least 2D spatial input dimension!";
54      CHECK_LT(this->num_spatial_axes_, 4) << "MKLDNN Convolution layer expects at most 3D spatial input dimension!";
55      this->num_ = bottom[0]->shape(0);
56      this->channels_ = bottom[0]->shape(1);
57      if (this->num_spatial_axes_ == 2) {
58        this->height_ = bottom[0]->shape(2);
59        this->width_ = bottom[0]->shape(3);
60        this->stride_h_ = this->stride_.cpu_data()[0];
61        this->stride_w_ = this->stride_.cpu_data()[1];
62        this->pad_h_ = this->pad_.cpu_data()[0];
63        this->pad_w_ = this->pad_.cpu_data()[1];
64        this->kernel_h_  = this->kernel_shape_.cpu_data()[0];
65        this->kernel_w_ = this->kernel_shape_.cpu_data()[1];
66      } else {
67        this->depth_ = bottom[0]->shape(2);
68        this->height_ = bottom[0]->shape(3);
69        this->width_ = bottom[0]->shape(4);
70        this->stride_d_ = this->stride_.cpu_data()[0];
71        this->stride_h_ = this->stride_.cpu_data()[1];
72        this->stride_w_ = this->stride_.cpu_data()[2];
73        this->pad_d_ = this->pad_.cpu_data()[0];
74        this->pad_h_ = this->pad_.cpu_data()[1];
75        this->pad_w_ = this->pad_.cpu_data()[2];
76        this->kernel_d_ = this->kernel_shape_.cpu_data()[0];
77        this->kernel_h_  = this->kernel_shape_.cpu_data()[1];
78        this->kernel_w_ = this->kernel_shape_.cpu_data()[2];
79      }
80      string _conv_algorithm = this->layer_param_.convolution_param().conv_algorithm();
81      if(_conv_algorithm == "direct")
82      {
83          conv_algorithm = algorithm::convolution_direct;
84      }
85      else if(_conv_algorithm == "winograd")
86      {
87          conv_algorithm = algorithm::convolution_winograd;
88      }
89      else if(_conv_algorithm == "auto")
90      {
91          conv_algorithm = algorithm::convolution_auto;
92      }
93      else
94      {
95          LOG(ERROR) << "Unsupported convolution algorithm.";
96          CHECK(false);
97      }
98  }
99  template <typename Dtype>
100  void MKLDNNConvolutionLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom
101                                              , const vector<Blob<Dtype>*>& top)
102  {
103      VLOG(1) << "<< MKLDNNConvolutionLayer<Dtype>::LayerSetUp: " << this->layer_param_.name();
104      if (this->layer_param_.has_quantization_param() && this->phase_ == TEST) this->need_quantize_ = true;
105      ConvolutionLayer<Dtype>::LayerSetUp(bottom, top);
106      init_properties(bottom, top);
107      this->bottom_shape_ = &bottom[0]->shape();
108      bwdw_weights_diff_iter_blob.reset(new Blob<Dtype>());
109      bwdw_weights_diff_iter_blob->ReshapeLike(*(this->blobs_[0]));
110      if (this->bias_term_) {
111        bwdw_bias_diff_iter_blob.reset(new Blob<Dtype>());
112        bwdw_bias_diff_iter_blob->ReshapeLike(*(this->blobs_[1]));
113      }
114  }
115  template <typename Dtype>
116  void MKLDNNConvolutionLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom
117                                              , const vector<Blob<Dtype>*>& top)
118  {
119      VLOG(1) << " MKLDNNConvolutionLayer<Dtype>::Reshape: " << this->layer_param_.name();
120      if (this->num_spatial_axes_ == 2) {
121        this->reshape = (this->width_ == bottom[0]->shape(3) &&
122                         this->height_ == bottom[0]->shape(2) &&
123                         this->channels_ == bottom[0]->shape(1) &&
124                         this->num_ == bottom[0]->shape(0)) ? false : true;
125      } else {
126        this->reshape = (this->depth_ == bottom[0]->shape(2) &&
127                         this->width_ == bottom[0]->shape(4) &&
128                         this->height_ == bottom[0]->shape(3) &&
129                         this->channels_ == bottom[0]->shape(1) &&
130                         this->num_ == bottom[0]->shape(0)) ? false : true;
131      }
132      init_properties(bottom, top);
133      BaseConvolutionLayer<Dtype>::ReshapeForMKL(bottom, top);
134      if (this->layer_param_.convolution_param().fusion_type() ==
135              ConvolutionParameter::SUM_FUSION &&
136          bottom.size() > 1) {
137        top[0]->ShareData(*bottom[1]);
138      }
139  }
140  template <typename Dtype>
141  void MKLDNNConvolutionLayer<Dtype>::InitConvolutionFwd(const vector<Blob<Dtype>*>& bottom
142                                                  , const vector<Blob<Dtype>*>& top)
143  {
144      if (std::is_same<Dtype, double>::value)   NOT_IMPLEMENTED;
145      auto propagation = this->phase_ == TEST ? prop_kind::forward_scoring : prop_kind::forward_training;
146      bool relu = this->layer_param_.convolution_param().relu();
147      Dtype negative_slope = 0;
148      if(relu)
149      {
150          negative_slope = this->layer_param_.convolution_param().negative_slope();
151      }
152      int32_t g  = std::max(this->group_, 1);
153      int32_t n  = this->num_;
154      int32_t iw = this->width_;
155      int32_t ih = this->height_;
156      int32_t ic = this->channels_;
157      int32_t id = this->depth_;
158      int32_t ow = this->width_out_;
159      int32_t oh = this->height_out_;
160      int32_t oc = this->num_output_;
161      int32_t od = this->depth_out_;
162      int32_t kw = this->kernel_w_;
163      int32_t kh = this->kernel_h_;
164      int32_t kd = this->kernel_d_;
165      int32_t sw = this->stride_w_;
166      int32_t sh = this->stride_h_;
167      int32_t sd = this->stride_d_;
168      int32_t pw = this->pad_w_;
169      int32_t ph = this->pad_h_;
170      int32_t pd = this->pad_d_;
171      memory::dims convolutionStrides;
172      memory::dims padding;
173      memory::dims padding_r;
174      memory::dims dilation;
175      bool dilated_conv = false;
176      const int* dilation_data = this->dilation_.cpu_data();
177      for (int i = 0; i < this->num_spatial_axes_; ++i) {
178        dilation.push_back(dilation_data[i] - 1);
179        if (dilation_data[i] != 1) dilated_conv = true;
180      }
181      if (this->num_spatial_axes_ == 2) {
182        convolutionStrides = {sh, sw};
183        padding = {ph, pw};
184        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[0]) + 1) - ph);
185        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[1]) + 1) - pw);
186      } else {
187        convolutionStrides = {sd, sh, sw};
188        padding = {pd, ph, pw};
189        padding_r.push_back((od - 1) * sd - id + ((kd - 1) * (dilation_data[0]) + 1) - pd);
190        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[1]) + 1) - ph);
191        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[2]) + 1) - pw);
192      }
193      memory::data_type mpcsn = memory::data_type::f32;
194      memory::data_type bottom_dt = memory::data_type::f32;
195      if (this->need_quantize_) {
196        if (this->layer_param_.quantization_param().is_negative_input())
197          bottom_dt = memory::data_type::s8;
198        else
199          bottom_dt = memory::data_type::u8;
200      }
201      memory::data_type top_dt = memory::data_type::f32;
202      if (this->need_quantize_) {
203        if (this->bw_layer_out_ == 8) {
204          if (relu && negative_slope == 0) {
205            top_dt = memory::data_type::u8;
206          }
207          else {
208            top_dt = memory::data_type::s8;
209          }
210        }
211        else {
212          top_dt = memory::data_type::f32;
213        }
214      }
215      bool is_sum = false;
216      if (this->layer_param_.convolution_param().fusion_type() ==
217              ConvolutionParameter::SUM_FUSION &&
218          bottom.size() > 1) {
219        if(relu)
220            is_sum = true;
221        memory::data_type bottom_1_dt = memory::data_type::f32;
222        if (const_cast<Dtype*>(bottom[1]->prv_data()) != NULL){
223          shared_ptr<MKLDNNMemoryDescriptor<Dtype, false> > bottom_1_desc =
224              get_mkldnn_prv_descriptor<Dtype, false>(bottom[1]);
225          bottom_1_dt = static_cast<memory::data_type>(bottom_1_desc->prv_memory_pd()->desc().data.data_type);
226        } 
227        if (top_dt != bottom_1_dt) {
228          top_dt = bottom_1_dt;
229          if(top_dt ==  memory::data_type::f32){
230            this->need_quantize_ = false;
231            bottom_dt = memory::data_type::f32;
232          }
233        }
234      }
235      memory::data_type weights_dt = this->need_quantize_ ? memory::data_type::s8 : memory::data_type::f32;
236      memory::data_type bias_dt = this->need_quantize_ ? memory::data_type::s32 : memory::data_type::f32;
237      memory::format mfmt_any = memory::format::any;
238      memory::dims bottom_tz;
239      memory::dims bias_tz;
240      memory::dims top_tz;
241      memory::dims weights_tz;
242      if (this->num_spatial_axes_ == 2) {
243        bottom_tz = {n, ic, ih, iw};
244        bias_tz = {oc};
245        top_tz = {n, oc, oh, ow};
246        weights_tz = (g!= 1) ? memory::dims{g, oc/g, ic/g, kh, kw} : memory::dims{oc, ic, kh, kw};
247      } else {
248        bottom_tz = {n, ic, id, ih, iw};
249        bias_tz = {oc};
250        top_tz = {n, oc, od, oh, ow};
251        weights_tz = (g!= 1) ? memory::dims{g, oc/g, ic/g, kd, kh, kw} : memory::dims{oc, ic, kd, kh, kw};
252      }
253      memory::desc init_bottom_md({bottom_tz}, bottom_dt, mfmt_any);
254      memory::desc init_bias_md({bias_tz}, bias_dt, mfmt_any);
255      memory::desc init_top_md({top_tz}, top_dt, mfmt_any);
256      memory::desc init_weights_md({weights_tz}, weights_dt, mfmt_any);
257      size_t coeff_size = this->layer_param_.convolution_param().coeff_size();
258      float coeff0 = 1;
259      float coeff1 = 1;
260      if (coeff_size == 2) 
261      {
262        coeff0 = this->layer_param_.convolution_param().coeff(0);
263        coeff1 = this->layer_param_.convolution_param().coeff(1);
264      }
265      primitive_attr attr;
266      if (this->need_quantize_) {
267        int mask = 0;
268        int count = 1; 
269        if(this->fl_params_.size() > 1 || this->scale_params_.size() > 1){
270            int oc_dim_id = 1;
271            mask = 1 << oc_dim_id;
272            count = oc;  
273        }
274        std::vector<float> scales(count);
275        float scale;
276        #ifdef _OPENMP
277        #pragma omp parallel for if (count > 1)
278        #endif
279        for(int i=0; i<count; i++){
280          if (this->scale_params_[i] == 0.0)
281              scale = this->scale_out_[0] * coeff1;
282          else
283              scale = this->scale_out_[0] / (this->scale_in_[0] * this->scale_params_[i]) * coeff1;
284          scales[i] = scale;
285        }
286        attr.set_output_scales(mask, scales);
287        attr.set_int_output_round_mode(round_nearest);
288      }
289      std::string subengines = this->layer_param_.engine();
290      if (subengines.find("MKLDNN") == std::string::npos || subengines == "MKLDNN")
291        subengines = "MKLDNN:CPU";
292      EngineParser ep(subengines);
293      unsigned subEngineIndex = 0;
294      mkldnn::algorithm eligibleAlgorithms[2] = {conv_algorithm, algorithm::convolution_direct};
295      convFwd_pd = NULL;
296      mkldnn::post_ops ops;
297      float scale = 1.0f;
298      Dtype alpha = negative_slope;  
299      float beta = 1.0f;             
300      if (this->layer_param_.convolution_param().fusion_type() ==
301              ConvolutionParameter::SUM_FUSION &&
302          bottom.size() > 1) {
303        if (this->need_quantize_) {
304          float sum_scale;
305          sum_scale =
306              this->scale_out_[0] /
307              get_mkldnn_prv_descriptor<Dtype, false>(bottom[1])->get_scale(0) * coeff0;
308          ops.append_sum(sum_scale);
309        } else {
310          ops.append_sum(1.0f);
311        }
312      }
313      if (relu) ops.append_eltwise(scale, eltwise_relu, alpha, beta);
314      attr.set_post_ops(ops);
315      for (auto& convAlgorithm : eligibleAlgorithms) {
316        shared_ptr<convolution_forward::desc> convFwd_desc;
317        if (this->bias_term_) {
318            if (dilated_conv)
319                convFwd_desc.reset(new convolution_forward::desc(
320                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
321                    init_bias_md, init_top_md, convolutionStrides, dilation, padding, padding_r,
322                    padding_kind::zero));
323            else
324                convFwd_desc.reset(new convolution_forward::desc(
325                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
326                    init_bias_md, init_top_md, convolutionStrides, padding, padding,
327                    padding_kind::zero));
328        } else {
329            if (dilated_conv)
330                convFwd_desc.reset(new convolution_forward::desc(
331                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
332                    init_top_md, convolutionStrides, dilation, padding, padding_r,
333                    padding_kind::zero));
334            else
335                convFwd_desc.reset(new convolution_forward::desc(
336                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
337                    init_top_md, convolutionStrides, padding, padding,
338                    padding_kind::zero));
339        }
340        for (subEngineIndex = 0; subEngineIndex < ep.getNumberOfSubEngines();
341             subEngineIndex++) {
342          try {
343            if (this->need_quantize_ || relu ||
344                (this->layer_param_.convolution_param().fusion_type() ==
345                     ConvolutionParameter::SUM_FUSION &&
346                 bottom.size() > 1)) {
347              convFwd_pd.reset(new convolution_forward::primitive_desc(
348                  *convFwd_desc, attr, ep.getMKLDNNSubEngine(subEngineIndex)));
349            } else {
350              convFwd_pd.reset(new convolution_forward::primitive_desc(
351                  *convFwd_desc, ep.getMKLDNNSubEngine(subEngineIndex)));
352            }
353          } catch (...) {
354              continue;
355          }
356          break;
357        }
358        if (convFwd_pd) break;
359      }
360      CHECK(convFwd_pd);
361      engine cpu_engine = CpuEngine::Instance().get_engine();
362      typedef typename memory::primitive_desc MemPD; 
363      shared_ptr<MemPD> prv_fwd_bottom_data_memory_pd(new MemPD(convFwd_pd->src_primitive_desc()));
364      shared_ptr<MemPD> prv_fwd_top_data_memory_pd(new MemPD(convFwd_pd->dst_primitive_desc()));
365      shared_ptr<MemPD> prv_fwd_weights_data_memory_pd(new MemPD(convFwd_pd->weights_primitive_desc()));
366      info_mem_pd<Dtype>(prv_fwd_bottom_data_memory_pd, "conv_src:" + this->layer_param_.name());
367      info_mem_pd<Dtype>(prv_fwd_top_data_memory_pd, "conv_dst:" + this->layer_param_.name());
368      memory::format data_mfmt;
369      memory::format weights_mfmt;
370      if (this->num_spatial_axes_ == 2) {
371        data_mfmt = memory::format::nchw;
<span onclick='openModal()' class='match'>372        weights_mfmt = (g!= 1) ? memory::format::goihw : memory::format::oihw;
373      } else {
</span>374        data_mfmt = memory::format::ncdhw;
375        weights_mfmt = (g!= 1) ? memory::format::goidhw : memory::format::oidhw;
376      } 
377      shared_ptr<MemPD> usr_bottom_data_memory_pd(new MemPD({{bottom_tz}, mpcsn, data_mfmt}, cpu_engine));
378      shared_ptr<MemPD> usr_bias_data_memory_pd(new MemPD({{bias_tz}, mpcsn, memory::format::x}, cpu_engine));
379      shared_ptr<MemPD> usr_top_data_memory_pd(new MemPD({{top_tz}, mpcsn, data_mfmt}, cpu_engine));
380      shared_ptr<MemPD> usr_weights_data_memory_pd(new MemPD({{weights_tz}, mpcsn, weights_mfmt}, cpu_engine));
381      if (this->need_quantize_){
382        std::vector<float> scale_bottom(1, this->layer_param_.quantization_param().force_u8_input()? 1.0f : this->scale_in_[0] );
383        fwd_bottom_data.reset(new MKLDNNData<Dtype>(usr_bottom_data_memory_pd, prv_fwd_bottom_data_memory_pd, bottom[0], this, scale_bottom));
384      } else {
385        fwd_bottom_data.reset(new MKLDNNData<Dtype>(usr_bottom_data_memory_pd, prv_fwd_bottom_data_memory_pd, bottom[0], this));
386      }
387      fwd_bottom_data->name = "fwd_bottom_data   @ " + this->layer_param_.name();
388      fwd_bottom_data_primitive = fwd_bottom_data->create_input(false);
389      if (this->need_quantize_){
390        std::vector<float> scale_top(1, this->scale_out_[0]);
391        fwd_top_data.reset(new MKLDNNData<Dtype>(usr_top_data_memory_pd, prv_fwd_top_data_memory_pd, top[0], this, scale_top, 0, is_sum));
392      } else{
393        fwd_top_data.reset(new MKLDNNData<Dtype>(usr_top_data_memory_pd, prv_fwd_top_data_memory_pd, top[0], this));
394      }
395      fwd_top_data->name = "fwd_top_data      @ " + this->layer_param_.name();
396      fwd_top_data_memory = fwd_top_data->create_output_memory();
397      bool is_wino = (prv_fwd_weights_data_memory_pd->desc().data.format == memory::format::wino_fmt); 
398    #ifdef _OPENMP
399      int node = caffe::cpu::OpenMpManager::getNumaNode();
400    #else
401      int node = 0;
402    #endif
403      if (fwd_weights_data == NULL) {
404        std::string name = "numa" + std::to_string(node) + "@fwd_weights_data@" + this->layer_param_.name();
405        if (this->need_quantize_){
406          int count = 1; 
407          int reorder_mask = 0;
408          if(this->scale_params_.size() > 1){
409              count = oc;  
410              reorder_mask = (g!= 1) ? (1<<1)+(1<<0) : 1<<0;
411          }
412          std::vector<float> scale_weight(count);
413          #ifdef _OPENMP
414          #pragma omp parallel for if (count > 1)
415          #endif
416          for(int i=0; i<count; i++){
417            scale_weight[i] = this->scale_params_[i];
418          }
419          fwd_weights_data.reset(new MKLDNNData<Dtype>(usr_weights_data_memory_pd, prv_fwd_weights_data_memory_pd, this->blobs_[0].get(), this, scale_weight, reorder_mask, false, false, true, name));
420        } else{
421          fwd_weights_data.reset(new MKLDNNData<Dtype>(usr_weights_data_memory_pd, prv_fwd_weights_data_memory_pd, this->blobs_[0].get(), this, {1.}, 0,  is_sum, is_wino, false, name));
422        }
423        fwd_weights_data->name.assign(name);
424        fwd_weights_data_primitive = fwd_weights_data->create_input(true);
425      }
426      if (this->bias_term_) {
427          if (fwd_bias_data == NULL) {
428            shared_ptr<MemPD> prv_fwd_bias_data_memory_pd(new MemPD(convFwd_pd->bias_primitive_desc()));
429            std::string name = "numa" + std::to_string(node) + "@fwd_bias_data@" + this->layer_param_.name();
430            if (this->need_quantize_){
431            int count = 1;  
432            int reorder_mask = 0;
433              if(this->scale_params_.size() > 1){
434                  count = oc;  
435                  reorder_mask = 1<<0;
436              }
437              std::vector<float> scale_bias(count);
438              #ifdef _OPENMP
439              #pragma omp parallel for if (count > 1)
440              #endif
441              for(int i=0; i<count; i++){
442                if (this->scale_params_[i] == 0.0)
443                    scale_bias[i] = 1.0;
444                else
445                    scale_bias[i] = this->scale_in_[0] * this->scale_params_[i];
446              }
447              fwd_bias_data.reset(new MKLDNNData<Dtype>(usr_bias_data_memory_pd, prv_fwd_bias_data_memory_pd, this->blobs_[1].get(), this, scale_bias, reorder_mask, false, false, false, name));
448            } else{
449              fwd_bias_data.reset(new MKLDNNData<Dtype>(usr_bias_data_memory_pd, prv_fwd_bias_data_memory_pd, this->blobs_[1].get(), this, {1.}, 0,  false, false, false, name));
450            }
451            fwd_bias_data->name.assign(name);
452            fwd_bias_data_primitive = fwd_bias_data->create_input(true);
453          }
454          convFwd.reset(new convolution_forward(*convFwd_pd
455                          , *fwd_bottom_data_primitive, *fwd_weights_data_primitive
456                          , *fwd_bias_data_primitive, *fwd_top_data_memory));
457          MKLDNNPrimitive<Dtype> fwd_bias_data_primitive_transfer(fwd_bias_data_primitive);
458          fwd_bias_data->set_mkldnn_primitive(fwd_bias_data_primitive_transfer);
459      } else {
460          convFwd.reset(new convolution_forward(*convFwd_pd
461                          , *fwd_bottom_data_primitive, *fwd_weights_data_primitive
462                          , *fwd_top_data_memory));
463      }
464      MKLDNNPrimitive<Dtype> fwd_bottom_data_primitive_transfer(fwd_bottom_data_primitive);
465      fwd_bottom_data->set_mkldnn_primitive(fwd_bottom_data_primitive_transfer);
466      MKLDNNPrimitive<Dtype> fwd_top_data_memory_transfer(fwd_top_data_memory);
467      fwd_top_data->set_mkldnn_primitive(fwd_top_data_memory_transfer);
468      MKLDNNPrimitive<Dtype> fwd_weights_data_primitive_transfer(fwd_weights_data_primitive);
469      fwd_weights_data->set_mkldnn_primitive(fwd_weights_data_primitive_transfer);
470  }
471  template <typename Dtype>
472  void MKLDNNConvolutionLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom
473                                                  , const vector<Blob<Dtype>*>& top)
474  {
475      VLOG(1) << "MKLDNNConvolutionLayer<Dtype>::Forward_cpu: " << this->layer_param_.name();
476      bool _mkldnn_primitive = false;
477      if( convFwd_pd == NULL || this->reshape){
478          InitConvolutionFwd(bottom, top);
479           if(getenv("CAFFE_INFERENCE_MEM_OPT")){
480               fwd_weights_data->sync_before_read();
481               if (this->bias_term_)
482                   fwd_bias_data->sync_before_read();
483               _mkldnn_primitive = true;
484           }
485      }
486      fwd_bottom_data->sync_before_read(); 
487      if(!getenv("CAFFE_INFERENCE_MEM_OPT")){
488          fwd_weights_data->sync_before_read();
489          if (this->bias_term_)
490              fwd_bias_data->sync_before_read();
491       } 
492      fwd_top_data->sync_before_write();
493      PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKLDNN_NAME("FW"));
494      PERFORMANCE_MEASUREMENT_BEGIN();
495      convFwd.submit();
496      if(_mkldnn_primitive) {
497        CircleBuf::Instance()->DecRefCnt(bottom[0]->prv_data());
498      }
499      PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
500  }
501  template <typename Dtype>
502  void MKLDNNConvolutionLayer<Dtype>::InitConvolutionBwd(const vector<Blob<Dtype>*>& top
503                                                      , const vector<bool>& propagate_down
504                                                      , const vector<Blob<Dtype>*>& bottom)
505  {
506      if (std::is_same<Dtype, double>::value)   NOT_IMPLEMENTED;
507      int32_t g  = std::max(this->group_, 1);
508      int32_t n  = this->num_;
509      int32_t iw = this->width_;
510      int32_t ih = this->height_;
511      int32_t ic = this->channels_;
512      int32_t id = this->depth_;
513      int32_t ow = this->width_out_;
514      int32_t oh = this->height_out_;
515      int32_t od = this->depth_out_;
516      int32_t oc = this->num_output_;
517      int32_t kw = this->kernel_w_;
518      int32_t kh = this->kernel_h_;
519      int32_t kd = this->kernel_d_;
520      int32_t sw = this->stride_w_;
521      int32_t sh = this->stride_h_;
522      int32_t sd = this->stride_d_;
523      int32_t pw = this->pad_w_;
524      int32_t ph = this->pad_h_;
525      int32_t pd = this->pad_d_;
526      memory::dims convolutionStrides;
527      memory::dims padding;
528      memory::dims padding_r;
529      memory::dims dilation;
530      bool dilated_conv = false;
531      const int* dilation_data = this->dilation_.cpu_data();
532      for (int i = 0; i < this->num_spatial_axes_; ++i) {
533        dilation.push_back(dilation_data[i] - 1);
534        if (dilation_data[i] != 1) dilated_conv = true;
535      }
536      if (this->num_spatial_axes_ == 2) {
537        convolutionStrides = {sh, sw};
538        padding = {ph, pw};
539        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[0]) + 1) - ph);
540        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[1]) + 1) - pw);
541      } else {
542        convolutionStrides = {sd, sh, sw};
543        padding = {pd, ph, pw};
544        padding_r.push_back((od - 1) * sd - id + ((kd - 1) * (dilation_data[0]) + 1) - pd);
545        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[1]) + 1) - ph);
546        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[2]) + 1) - pw);
547      }
548      memory::data_type mpcsn = memory::data_type::f32;
549      memory::format mfmt_any = memory::format::any;
550      memory::dims bottom_tz;
551      memory::dims bias_tz;
552      memory::dims top_tz;
553      memory::dims weights_tz;
554      if (this->num_spatial_axes_ == 2) {
555        bottom_tz = {n, ic, ih, iw};
556        bias_tz = {oc};
557        top_tz = {n, oc, oh, ow};
558        weights_tz = ( g!= 1) ? memory::dims{g, oc/g, ic/g, kh, kw} : memory::dims{oc, ic, kh, kw};
559      } else {
560        bottom_tz = {n, ic, id, ih, iw};
561        bias_tz = {oc};
562        top_tz = {n, oc, od, oh, ow};
563        weights_tz = ( g!= 1) ? memory::dims{g, oc/g, ic/g, kd, kh, kw} : memory::dims{oc, ic, kd, kh, kw};
564      } 
565      memory::desc init_bottom_md({bottom_tz}, mpcsn, mfmt_any);
566      memory::desc init_bias_md({bias_tz}, mpcsn, mfmt_any);
567      memory::desc init_top_md({top_tz}, mpcsn, mfmt_any);
568      memory::desc init_weights_md({weights_tz}, mpcsn, mfmt_any);
569      std::string subengines = this->layer_param_.engine();
570      if (subengines.find("MKLDNN") == std::string::npos || subengines == "MKLDNN")
571        subengines = "MKLDNN:CPU";
572      EngineParser ep(subengines);
573      unsigned subEngineIndex = 0;
574      auto eligibleAlgorithms = {conv_algorithm, algorithm::convolution_direct};
575      convBwdData_pd = NULL;
576      convBwdWeights_pd = NULL;
577      for (auto &convAlgorithm : eligibleAlgorithms) {
578          shared_ptr<convolution_backward_data::desc> convBwdData_desc;
579          shared_ptr<convolution_backward_weights::desc> convBwdWeights_desc;
580          if (this->bias_term_) {
581              if (dilated_conv)
582                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
583                              , init_bottom_md, init_weights_md, init_bias_md, init_top_md
584                              , convolutionStrides, dilation, padding, padding_r, padding_kind::zero));
585              else
586                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
587                              , init_bottom_md, init_weights_md, init_bias_md, init_top_md
588                              , convolutionStrides, padding, padding, padding_kind::zero));
589          } else {
590              if (dilated_conv)
591                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
592                              , init_bottom_md, init_weights_md, init_top_md
593                              , convolutionStrides, dilation, padding, padding_r, padding_kind::zero));
594              else
595                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
596                              , init_bottom_md, init_weights_md, init_top_md
597                              , convolutionStrides, padding, padding, padding_kind::zero));
598          }
599          if (dilated_conv)
600              convBwdData_desc.reset(new convolution_backward_data::desc(convAlgorithm
601                              , init_bottom_md, init_weights_md, init_top_md
602                              , convolutionStrides, dilation, padding, padding_r, padding_kind::zero));
603          else
604              convBwdData_desc.reset(new convolution_backward_data::desc(convAlgorithm
605                              , init_bottom_md, init_weights_md, init_top_md
606                              , convolutionStrides, padding, padding, padding_kind::zero));
607          for(subEngineIndex=0; subEngineIndex < ep.getNumberOfSubEngines(); subEngineIndex++) {
608              try {
609                  convBwdData_pd.reset(new convolution_backward_data::primitive_desc(*convBwdData_desc,
610                                            ep.getMKLDNNSubEngine(subEngineIndex), *convFwd_pd));
611                  convBwdWeights_pd.reset(new convolution_backward_weights::primitive_desc(*convBwdWeights_desc,
612                                            ep.getMKLDNNSubEngine(subEngineIndex), *convFwd_pd));
613              }
614              catch(...) {
615                  continue;
616              }
617              break;
618          }
619          if (convBwdData_pd && convBwdWeights_pd)
620              break;
621      }
622      CHECK(convBwdData_pd);
623      CHECK(convBwdWeights_pd);
624      engine cpu_engine = CpuEngine::Instance().get_engine();
625      typedef typename memory::primitive_desc MemPD; 
626      shared_ptr<MemPD> prv_bwdd_bottom_diff_memory_pd(new MemPD(convBwdData_pd->diff_src_primitive_desc()));
627      shared_ptr<MemPD> prv_bwdd_top_diff_memory_pd(new MemPD(convBwdData_pd->diff_dst_primitive_desc()));
628      shared_ptr<MemPD> prv_bwdd_weights_data_memory_pd(new MemPD(convBwdData_pd->weights_primitive_desc()));
629      shared_ptr<MemPD> prv_bwdw_bottom_data_memory_pd(new MemPD(convBwdWeights_pd->src_primitive_desc()));
630      shared_ptr<MemPD> prv_bwdw_top_diff_memory_pd(new MemPD(convBwdWeights_pd->diff_dst_primitive_desc()));
631      shared_ptr<MemPD> prv_bwdw_weights_diff_memory_pd(new MemPD(convBwdWeights_pd->diff_weights_primitive_desc()));
632      memory::format data_mfmt;
633      memory::format weights_mfmt;
634      if (this->num_spatial_axes_ == 2) {
635        data_mfmt = memory::format::nchw;
636        weights_mfmt = ( g!= 1) ? memory::format::goihw : memory::format::oihw;
637      } else {
638        data_mfmt = memory::format::ncdhw;
639        weights_mfmt = ( g!= 1) ? memory::format::goidhw : memory::format::oidhw;
640      }
641      shared_ptr<MemPD> usr_bottom_data_memory_pd(new MemPD({{bottom_tz}, mpcsn, data_mfmt}, cpu_engine));
642      shared_ptr<MemPD> usr_bias_data_memory_pd(new MemPD({{bias_tz}, mpcsn, memory::format::x}, cpu_engine));
643      shared_ptr<MemPD> usr_top_data_memory_pd(new MemPD({{top_tz}, mpcsn, data_mfmt}, cpu_engine));
644      shared_ptr<MemPD> usr_weights_data_memory_pd(new MemPD({{weights_tz}, mpcsn, weights_mfmt}, cpu_engine));
645      bwdd_bottom_diff.reset(new MKLDNNDiff<Dtype>(usr_bottom_data_memory_pd, prv_bwdd_bottom_diff_memory_pd, bottom[0], this));
646      bwdd_bottom_diff ->name = "bwdd_bottom_diff   @ " + this->layer_param_.name();
647      bwdd_bottom_diff_memory = bwdd_bottom_diff->create_output_memory();
648      bwdw_bottom_data.reset(new MKLDNNData<Dtype>(usr_bottom_data_memory_pd, prv_bwdw_bottom_data_memory_pd, bottom[0], this));
649      bwdw_bottom_data ->name = "bwdw_bottom_data   @ " + this->layer_param_.name();
650      bwdw_bottom_data_primitive = bwdw_bottom_data->create_input(false);
651      bwdd_top_diff.reset(new MKLDNNDiff<Dtype>(usr_top_data_memory_pd, prv_bwdd_top_diff_memory_pd, top[0], this));
652      bwdd_top_diff    ->name = "bwdd_top_diff      @ " + this->layer_param_.name();
653      bwdd_top_diff_primitive = bwdd_top_diff->create_input(false);
654      bwdw_top_diff.reset(new MKLDNNDiff<Dtype>(usr_top_data_memory_pd, prv_bwdw_top_diff_memory_pd, top[0], this));
655      bwdw_top_diff    ->name = "bwdw_top_diff      @ " + this->layer_param_.name();
656      bwdw_top_diff_primitive = bwdw_top_diff->create_input(false);
657      bwdd_weights_data.reset(new MKLDNNData<Dtype>(usr_weights_data_memory_pd, prv_bwdd_weights_data_memory_pd, this->blobs_[0].get(), this));
658      bwdd_weights_data->name = "bwdd_weights_data  @ " + this->layer_param_.name();
659      bwdd_weights_data_primitive = bwdd_weights_data->create_input(false);
660      bwdw_weights_diff.reset(new MKLDNNDiff<Dtype>(usr_weights_data_memory_pd, prv_bwdw_weights_diff_memory_pd, this->blobs_[0].get(), this));
661      bwdw_weights_diff->name = "bwdw_weights_diff  @ " + this->layer_param_.name();
662      bwdw_weights_diff_memory = bwdw_weights_diff->create_output_memory();
663      if (Caffe::iter_size() > 1) {
664        shared_ptr<MemPD> prv_bwdw_weights_diff_memory_iter_pd(new MemPD(convBwdWeights_pd->diff_weights_primitive_desc()));
665        bwdw_weights_diff_iter.reset(new MKLDNNDiff<Dtype>(usr_weights_data_memory_pd, prv_bwdw_weights_diff_memory_iter_pd, bwdw_weights_diff_iter_blob.get(), this));
666        bwdw_weights_diff_memory_iter = bwdw_weights_diff_iter->create_output_memory();
667      }
668      if (this->bias_term_) {
669          shared_ptr<MemPD> prv_bwdw_bias_diff_memory_pd(new MemPD(convBwdWeights_pd->diff_bias_primitive_desc()));
670          bwdw_bias_diff.reset(new MKLDNNDiff<Dtype>(usr_bias_data_memory_pd, prv_bwdw_bias_diff_memory_pd, this->blobs_[1].get(), this));
671          bwdw_bias_diff->name = "bwdw_bias_diff     @ " + this->layer_param_.name();
672          bwdw_bias_diff_memory = bwdw_bias_diff->create_output_memory();
673          if (Caffe::iter_size() > 1) {
674            shared_ptr<MemPD> prv_bwdw_bias_diff_memory_iter_pd(new MemPD(convBwdWeights_pd->diff_bias_primitive_desc()));
675            bwdw_bias_diff_iter.reset(new MKLDNNDiff<Dtype>(usr_bias_data_memory_pd, prv_bwdw_bias_diff_memory_iter_pd, bwdw_bias_diff_iter_blob.get(), this));
676            bwdw_bias_diff_memory_iter = bwdw_bias_diff_iter->create_output_memory();
677            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
678                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
679                          , *bwdw_weights_diff_memory_iter, *bwdw_bias_diff_memory_iter));
680          } else {
681            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
682                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
683                          , *bwdw_weights_diff_memory, *bwdw_bias_diff_memory));
684          }
685          MKLDNNPrimitive<Dtype> bwdw_bias_diff_memory_transfer(bwdw_bias_diff_memory);
686          bwdw_bias_diff->set_mkldnn_primitive(bwdw_bias_diff_memory_transfer);
687          if (Caffe::iter_size() > 1) {
688            MKLDNNPrimitive<Dtype> bwdw_bias_diff_memory_iter_transfer(bwdw_bias_diff_memory_iter);
689            bwdw_bias_diff_iter->set_mkldnn_primitive(bwdw_bias_diff_memory_iter_transfer);
690          }
691      } else {
692          if (Caffe::iter_size() > 1) {
693            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
694                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
695                          , *bwdw_weights_diff_memory_iter));
696          } else {
697            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
698                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
699                          , *bwdw_weights_diff_memory));
700          }
701      }
702      convBwdData.reset(new convolution_backward_data(*convBwdData_pd
703                      , *bwdd_top_diff_primitive, *bwdd_weights_data_primitive
704                      , *bwdd_bottom_diff_memory));
705      MKLDNNPrimitive<Dtype> bwdd_bottom_diff_memory_transfer(bwdd_bottom_diff_memory);
706      bwdd_bottom_diff->set_mkldnn_primitive(bwdd_bottom_diff_memory_transfer);
707      MKLDNNPrimitive<Dtype> bwdd_top_diff_primitive_transfer(bwdd_top_diff_primitive);
708      bwdd_top_diff->set_mkldnn_primitive(bwdd_top_diff_primitive_transfer);
709      MKLDNNPrimitive<Dtype> bwdd_weights_data_primitive_transfer(bwdd_weights_data_primitive);
710      bwdd_weights_data->set_mkldnn_primitive(bwdd_weights_data_primitive_transfer);
711      MKLDNNPrimitive<Dtype> bwdw_bottom_data_primitive_transfer(bwdw_bottom_data_primitive);
712      bwdw_bottom_data->set_mkldnn_primitive(bwdw_bottom_data_primitive_transfer);
713      MKLDNNPrimitive<Dtype> bwdw_top_diff_primitive_transfer(bwdw_top_diff_primitive);
714      bwdw_top_diff->set_mkldnn_primitive(bwdw_top_diff_primitive_transfer);
715      MKLDNNPrimitive<Dtype> bwdw_weights_diff_memory_transfer(bwdw_weights_diff_memory);
716      bwdw_weights_diff->set_mkldnn_primitive(bwdw_weights_diff_memory_transfer);
717      if (Caffe::iter_size() > 1) {
718        MKLDNNPrimitive<Dtype> bwdw_weights_diff_memory_iter_transfer(bwdw_weights_diff_memory_iter);
719        bwdw_weights_diff_iter->set_mkldnn_primitive(bwdw_weights_diff_memory_iter_transfer);
720      }
721  }
722  template <typename Dtype>
723  void MKLDNNConvolutionLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top
724                                                  , const vector<bool>& propagate_down
725                                                  , const vector<Blob<Dtype>*>& bottom)
726  {
727      VLOG(1) << "MKLDNNConvolutionLayer<Dtype>::Backward_cpu: " << this->layer_param_.name();
728      bool top_diff_is_prv = (const_cast<Dtype*>(top[0]->prv_diff()) != NULL);
729      if( convBwdData_pd == NULL || this->reshape)
730          InitConvolutionBwd(top, propagate_down, bottom);
731      if (propagate_down[0]) {
732          bwdd_top_diff->sync_before_read();
733          bwdd_weights_data->sync_before_read();
734          bwdd_bottom_diff->sync_before_write();
735          PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKLDNN_NAME("BW"));
736          PERFORMANCE_MEASUREMENT_BEGIN();
737  #ifdef DEBUG
738          if (bottom[0]->prv_data() != NULL)
739          {
740              LOG(INFO) << "Debug: Bottom prv data: " << *bottom[0]->prv_data();
741          }
742          else
743          {
744              LOG(INFO) << "Debug: Bottom prv data is NULL!";
745          }
746          if (top[0]->prv_diff() != NULL)
747          {
748              LOG(INFO) << "Debug: Top prv diff: " << *top[0]->prv_diff();
749          }
750          else
751          {
752              LOG(INFO) << "Debug: Top prv diff is NULL!";
753              LOG(INFO) << "Debug: Top cpu diff: " << *top[0]->cpu_diff();
754          }
755          if (this->blobs_[0]->prv_data() != NULL)
756          {
757              LOG(INFO) << "Debug: Weights prv data from blobs_[0]: " << *this->blobs_[0]->prv_data();
758          }
759          else
760          {
761              LOG(INFO) << "Debug: Weights prv data is NULL!";
762              LOG(INFO) << "Debug: Weights cpu data: " << *this->blobs_[0]->cpu_data();
763          }
764          LOG(INFO) << "Debug: Weights prv data from get_prv_ptr: " << *bwdd_weights_data->get_prv_ptr();
765  #endif
766          convBwdData.submit();
767  #ifdef DEBUG
768          if (bottom[0]->prv_diff() != NULL)
769          {
770              LOG(INFO) << "Debug: Bottom prv diff: " << *bottom[0]->prv_diff();
771          }
772          else
773          {
774              LOG(INFO) << "Debug: Bottom prv diff is NULL!";
775              LOG(INFO) << "Debug: Bottom cpu diff: " << *bottom[0]->cpu_diff();
776          }
777  #endif
778          PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
779      }
780      if (this->param_propagate_down(0)) {
781          if (!top_diff_is_prv && propagate_down[0])
782            top[0]->mutable_cpu_diff();
783          bwdw_top_diff->sync_before_read();
784          bwdw_bottom_data->sync_before_read();
785          bwdw_weights_diff->sync_before_write();
786          if (this->param_propagate_down(1)) {
787              CHECK(bwdw_bias_diff);
788              bwdw_bias_diff->sync_before_write();
789          }
790          PERFORMANCE_EVENT_ID_INIT(perf_id_bw_weights_,
791            PERFORMANCE_MKLDNN_NAME_DETAILED("BW", "_weights"));
792          PERFORMANCE_MEASUREMENT_BEGIN();
793          convBwdWeights.submit();
794          PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_weights_);
795          if (Caffe::iter_size() > 1) {
796            if (this->blobs_[0]->prv_diff() != NULL) {
797              caffe_axpy(this->blobs_[0]->prv_diff_count(), Dtype(1),
798                (Dtype*)(bwdw_weights_diff_memory_iter->get_data_handle()),
799                this->blobs_[0]->mutable_prv_diff());
800            } else {
801              caffe_axpy(this->blobs_[0]->count(), Dtype(1),
802                (Dtype*)(bwdw_weights_diff_memory_iter->get_data_handle()),
803                this->blobs_[0]->mutable_cpu_diff());
804            }
805          }
806          if (this->param_propagate_down(1)) {
807            if (Caffe::iter_size() > 1) {
808              if (this->blobs_[1]->prv_diff() != NULL) {
809                caffe_axpy(this->blobs_[1]->prv_diff_count(), Dtype(1),
810                  (Dtype*)(bwdw_bias_diff_memory_iter->get_data_handle()),
811                  this->blobs_[1]->mutable_prv_diff());
812              } else {
813                caffe_axpy(this->blobs_[1]->count(), Dtype(1),
814                  (Dtype*)(bwdw_bias_diff_memory_iter->get_data_handle()),
815                  this->blobs_[1]->mutable_cpu_diff());
816              }
817            }
818          }
819      }
820  }
821  #ifdef CPU_ONLY
822  STUB_GPU(MKLDNNConvolutionLayer);
823  #else
824  template <typename Dtype>
825  void MKLDNNConvolutionLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom
826                                                  , const vector<Blob<Dtype>*>& top)
827  {
828      NOT_IMPLEMENTED;
829  }
830  template <typename Dtype>
831  void MKLDNNConvolutionLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top
832                                                  , const vector<bool>& propagate_down
833                                                  , const vector<Blob<Dtype>*>& bottom)
834  {
835      NOT_IMPLEMENTED;
836  }
837  #endif
838  INSTANTIATE_CLASS(MKLDNNConvolutionLayer);
839  }  
840  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_convolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKLDNN_SUPPORTED
2  #include <algorithm>
3  #include <cstdlib>
4  #include <vector>
5  #include "caffe/filler.hpp"
6  #include "caffe/layer.hpp"
7  #include "caffe/layers/mkldnn_layers.hpp"
8  #include "caffe/util/cpu_info.hpp"
9  namespace caffe {
10  template <typename Dtype>
11  MKLDNNConvolutionLayer<Dtype>::MKLDNNConvolutionLayer(const LayerParameter& param)
12              : MKLDNNLayer<Dtype>(param), ConvolutionLayer<Dtype>(param)
13              , fwd_bottom_data(NULL), fwd_top_data(NULL), fwd_weights_data(NULL), fwd_bias_data(NULL)
14              , bwdd_weights_data(NULL), bwdw_bottom_data(NULL)
15              , bwdd_bottom_diff(NULL), bwdd_top_diff(NULL)
16              , bwdw_top_diff(NULL), bwdw_weights_diff(NULL), bwdw_bias_diff(NULL)
17              , convFwd_pd(NULL), convBwdData_pd(NULL), convBwdWeights_pd(NULL)
18              , fwd_top_data_memory(NULL), bwdd_bottom_diff_memory(NULL)
19              , bwdw_weights_diff_memory(NULL), bwdw_bias_diff_memory(NULL)
20              , fwd_bottom_data_primitive(NULL), fwd_weights_data_primitive(NULL), fwd_bias_data_primitive(NULL)
21              , bwdd_top_diff_primitive(NULL), bwdd_weights_data_primitive(NULL)
22              , bwdw_top_diff_primitive(NULL), bwdw_bottom_data_primitive(NULL)
23              , width_(0), height_(0), depth_(0), width_out_(0), height_out_(0), depth_out_(0), kernel_w_(0), kernel_h_(0), kernel_d_(0)
24              , stride_w_(0), stride_h_(0), stride_d_(0), pad_w_(0), pad_h_(0), pad_d_(0),
25              bwdw_weights_diff_iter(NULL),
26              bwdw_bias_diff_iter(NULL),
27              bwdw_weights_diff_memory_iter(NULL),
28              bwdw_bias_diff_memory_iter(NULL)
29  {
30    PERFORMANCE_EVENT_ID_RESET(perf_id_fw_);
31    PERFORMANCE_EVENT_ID_RESET(perf_id_bw_);
32    PERFORMANCE_EVENT_ID_RESET(perf_id_bw_weights_);
33  }
34  template <typename Dtype>
35  void MKLDNNConvolutionLayer<Dtype>::compute_output_shape()
36  {
37      ConvolutionLayer<Dtype>::compute_output_shape();
38      CHECK_GT(this->output_shape_.size(), 1) << "MKLDNN Convolution layer expects at least 2D spatial output dimension!";
39      CHECK_LT(this->output_shape_.size(), 4) << "MKLDNN Convolution layer expects at most 3D spatial output dimension!";
40      if (this->output_shape_.size() == 2) {
41        this->height_out_ = this->output_shape_[0];
42        this->width_out_ = this->output_shape_[1];
43      } else {
44        this->depth_out_ = this->output_shape_[0];
45        this->height_out_ = this->output_shape_[1];
46        this->width_out_ = this->output_shape_[2];
47      }
48  }
49  template <typename Dtype>
50  void MKLDNNConvolutionLayer<Dtype>::init_properties(const vector<Blob<Dtype>*>& bottom
51                                                  , const vector<Blob<Dtype>*>& top)
52  {
53      CHECK_GT(this->num_spatial_axes_, 1) << "MKLDNN Convolution layer expects at least 2D spatial input dimension!";
54      CHECK_LT(this->num_spatial_axes_, 4) << "MKLDNN Convolution layer expects at most 3D spatial input dimension!";
55      this->num_ = bottom[0]->shape(0);
56      this->channels_ = bottom[0]->shape(1);
57      if (this->num_spatial_axes_ == 2) {
58        this->height_ = bottom[0]->shape(2);
59        this->width_ = bottom[0]->shape(3);
60        this->stride_h_ = this->stride_.cpu_data()[0];
61        this->stride_w_ = this->stride_.cpu_data()[1];
62        this->pad_h_ = this->pad_.cpu_data()[0];
63        this->pad_w_ = this->pad_.cpu_data()[1];
64        this->kernel_h_  = this->kernel_shape_.cpu_data()[0];
65        this->kernel_w_ = this->kernel_shape_.cpu_data()[1];
66      } else {
67        this->depth_ = bottom[0]->shape(2);
68        this->height_ = bottom[0]->shape(3);
69        this->width_ = bottom[0]->shape(4);
70        this->stride_d_ = this->stride_.cpu_data()[0];
71        this->stride_h_ = this->stride_.cpu_data()[1];
72        this->stride_w_ = this->stride_.cpu_data()[2];
73        this->pad_d_ = this->pad_.cpu_data()[0];
74        this->pad_h_ = this->pad_.cpu_data()[1];
75        this->pad_w_ = this->pad_.cpu_data()[2];
76        this->kernel_d_ = this->kernel_shape_.cpu_data()[0];
77        this->kernel_h_  = this->kernel_shape_.cpu_data()[1];
78        this->kernel_w_ = this->kernel_shape_.cpu_data()[2];
79      }
80      string _conv_algorithm = this->layer_param_.convolution_param().conv_algorithm();
81      if(_conv_algorithm == "direct")
82      {
83          conv_algorithm = algorithm::convolution_direct;
84      }
85      else if(_conv_algorithm == "winograd")
86      {
87          conv_algorithm = algorithm::convolution_winograd;
88      }
89      else if(_conv_algorithm == "auto")
90      {
91          conv_algorithm = algorithm::convolution_auto;
92      }
93      else
94      {
95          LOG(ERROR) << "Unsupported convolution algorithm.";
96          CHECK(false);
97      }
98  }
99  template <typename Dtype>
100  void MKLDNNConvolutionLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom
101                                              , const vector<Blob<Dtype>*>& top)
102  {
103      VLOG(1) << "<< MKLDNNConvolutionLayer<Dtype>::LayerSetUp: " << this->layer_param_.name();
104      if (this->layer_param_.has_quantization_param() && this->phase_ == TEST) this->need_quantize_ = true;
105      ConvolutionLayer<Dtype>::LayerSetUp(bottom, top);
106      init_properties(bottom, top);
107      this->bottom_shape_ = &bottom[0]->shape();
108      bwdw_weights_diff_iter_blob.reset(new Blob<Dtype>());
109      bwdw_weights_diff_iter_blob->ReshapeLike(*(this->blobs_[0]));
110      if (this->bias_term_) {
111        bwdw_bias_diff_iter_blob.reset(new Blob<Dtype>());
112        bwdw_bias_diff_iter_blob->ReshapeLike(*(this->blobs_[1]));
113      }
114  }
115  template <typename Dtype>
116  void MKLDNNConvolutionLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom
117                                              , const vector<Blob<Dtype>*>& top)
118  {
119      VLOG(1) << " MKLDNNConvolutionLayer<Dtype>::Reshape: " << this->layer_param_.name();
120      if (this->num_spatial_axes_ == 2) {
121        this->reshape = (this->width_ == bottom[0]->shape(3) &&
122                         this->height_ == bottom[0]->shape(2) &&
123                         this->channels_ == bottom[0]->shape(1) &&
124                         this->num_ == bottom[0]->shape(0)) ? false : true;
125      } else {
126        this->reshape = (this->depth_ == bottom[0]->shape(2) &&
127                         this->width_ == bottom[0]->shape(4) &&
128                         this->height_ == bottom[0]->shape(3) &&
129                         this->channels_ == bottom[0]->shape(1) &&
130                         this->num_ == bottom[0]->shape(0)) ? false : true;
131      }
132      init_properties(bottom, top);
133      BaseConvolutionLayer<Dtype>::ReshapeForMKL(bottom, top);
134      if (this->layer_param_.convolution_param().fusion_type() ==
135              ConvolutionParameter::SUM_FUSION &&
136          bottom.size() > 1) {
137        top[0]->ShareData(*bottom[1]);
138      }
139  }
140  template <typename Dtype>
141  void MKLDNNConvolutionLayer<Dtype>::InitConvolutionFwd(const vector<Blob<Dtype>*>& bottom
142                                                  , const vector<Blob<Dtype>*>& top)
143  {
144      if (std::is_same<Dtype, double>::value)   NOT_IMPLEMENTED;
145      auto propagation = this->phase_ == TEST ? prop_kind::forward_scoring : prop_kind::forward_training;
146      bool relu = this->layer_param_.convolution_param().relu();
147      Dtype negative_slope = 0;
148      if(relu)
149      {
150          negative_slope = this->layer_param_.convolution_param().negative_slope();
151      }
152      int32_t g  = std::max(this->group_, 1);
153      int32_t n  = this->num_;
154      int32_t iw = this->width_;
155      int32_t ih = this->height_;
156      int32_t ic = this->channels_;
157      int32_t id = this->depth_;
158      int32_t ow = this->width_out_;
159      int32_t oh = this->height_out_;
160      int32_t oc = this->num_output_;
161      int32_t od = this->depth_out_;
162      int32_t kw = this->kernel_w_;
163      int32_t kh = this->kernel_h_;
164      int32_t kd = this->kernel_d_;
165      int32_t sw = this->stride_w_;
166      int32_t sh = this->stride_h_;
167      int32_t sd = this->stride_d_;
168      int32_t pw = this->pad_w_;
169      int32_t ph = this->pad_h_;
170      int32_t pd = this->pad_d_;
171      memory::dims convolutionStrides;
172      memory::dims padding;
173      memory::dims padding_r;
174      memory::dims dilation;
175      bool dilated_conv = false;
176      const int* dilation_data = this->dilation_.cpu_data();
177      for (int i = 0; i < this->num_spatial_axes_; ++i) {
178        dilation.push_back(dilation_data[i] - 1);
179        if (dilation_data[i] != 1) dilated_conv = true;
180      }
181      if (this->num_spatial_axes_ == 2) {
182        convolutionStrides = {sh, sw};
183        padding = {ph, pw};
184        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[0]) + 1) - ph);
185        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[1]) + 1) - pw);
186      } else {
187        convolutionStrides = {sd, sh, sw};
188        padding = {pd, ph, pw};
189        padding_r.push_back((od - 1) * sd - id + ((kd - 1) * (dilation_data[0]) + 1) - pd);
190        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[1]) + 1) - ph);
191        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[2]) + 1) - pw);
192      }
193      memory::data_type mpcsn = memory::data_type::f32;
194      memory::data_type bottom_dt = memory::data_type::f32;
195      if (this->need_quantize_) {
196        if (this->layer_param_.quantization_param().is_negative_input())
197          bottom_dt = memory::data_type::s8;
198        else
199          bottom_dt = memory::data_type::u8;
200      }
201      memory::data_type top_dt = memory::data_type::f32;
202      if (this->need_quantize_) {
203        if (this->bw_layer_out_ == 8) {
204          if (relu && negative_slope == 0) {
205            top_dt = memory::data_type::u8;
206          }
207          else {
208            top_dt = memory::data_type::s8;
209          }
210        }
211        else {
212          top_dt = memory::data_type::f32;
213        }
214      }
215      bool is_sum = false;
216      if (this->layer_param_.convolution_param().fusion_type() ==
217              ConvolutionParameter::SUM_FUSION &&
218          bottom.size() > 1) {
219        if(relu)
220            is_sum = true;
221        memory::data_type bottom_1_dt = memory::data_type::f32;
222        if (const_cast<Dtype*>(bottom[1]->prv_data()) != NULL){
223          shared_ptr<MKLDNNMemoryDescriptor<Dtype, false> > bottom_1_desc =
224              get_mkldnn_prv_descriptor<Dtype, false>(bottom[1]);
225          bottom_1_dt = static_cast<memory::data_type>(bottom_1_desc->prv_memory_pd()->desc().data.data_type);
226        } 
227        if (top_dt != bottom_1_dt) {
228          top_dt = bottom_1_dt;
229          if(top_dt ==  memory::data_type::f32){
230            this->need_quantize_ = false;
231            bottom_dt = memory::data_type::f32;
232          }
233        }
234      }
235      memory::data_type weights_dt = this->need_quantize_ ? memory::data_type::s8 : memory::data_type::f32;
236      memory::data_type bias_dt = this->need_quantize_ ? memory::data_type::s32 : memory::data_type::f32;
237      memory::format mfmt_any = memory::format::any;
238      memory::dims bottom_tz;
239      memory::dims bias_tz;
240      memory::dims top_tz;
241      memory::dims weights_tz;
242      if (this->num_spatial_axes_ == 2) {
243        bottom_tz = {n, ic, ih, iw};
244        bias_tz = {oc};
245        top_tz = {n, oc, oh, ow};
246        weights_tz = (g!= 1) ? memory::dims{g, oc/g, ic/g, kh, kw} : memory::dims{oc, ic, kh, kw};
247      } else {
248        bottom_tz = {n, ic, id, ih, iw};
249        bias_tz = {oc};
250        top_tz = {n, oc, od, oh, ow};
251        weights_tz = (g!= 1) ? memory::dims{g, oc/g, ic/g, kd, kh, kw} : memory::dims{oc, ic, kd, kh, kw};
252      }
253      memory::desc init_bottom_md({bottom_tz}, bottom_dt, mfmt_any);
254      memory::desc init_bias_md({bias_tz}, bias_dt, mfmt_any);
255      memory::desc init_top_md({top_tz}, top_dt, mfmt_any);
256      memory::desc init_weights_md({weights_tz}, weights_dt, mfmt_any);
257      size_t coeff_size = this->layer_param_.convolution_param().coeff_size();
258      float coeff0 = 1;
259      float coeff1 = 1;
260      if (coeff_size == 2) 
261      {
262        coeff0 = this->layer_param_.convolution_param().coeff(0);
263        coeff1 = this->layer_param_.convolution_param().coeff(1);
264      }
265      primitive_attr attr;
266      if (this->need_quantize_) {
267        int mask = 0;
268        int count = 1; 
269        if(this->fl_params_.size() > 1 || this->scale_params_.size() > 1){
270            int oc_dim_id = 1;
271            mask = 1 << oc_dim_id;
272            count = oc;  
273        }
274        std::vector<float> scales(count);
275        float scale;
276        #ifdef _OPENMP
277        #pragma omp parallel for if (count > 1)
278        #endif
279        for(int i=0; i<count; i++){
280          if (this->scale_params_[i] == 0.0)
281              scale = this->scale_out_[0] * coeff1;
282          else
283              scale = this->scale_out_[0] / (this->scale_in_[0] * this->scale_params_[i]) * coeff1;
284          scales[i] = scale;
285        }
286        attr.set_output_scales(mask, scales);
287        attr.set_int_output_round_mode(round_nearest);
288      }
289      std::string subengines = this->layer_param_.engine();
290      if (subengines.find("MKLDNN") == std::string::npos || subengines == "MKLDNN")
291        subengines = "MKLDNN:CPU";
292      EngineParser ep(subengines);
293      unsigned subEngineIndex = 0;
294      mkldnn::algorithm eligibleAlgorithms[2] = {conv_algorithm, algorithm::convolution_direct};
295      convFwd_pd = NULL;
296      mkldnn::post_ops ops;
297      float scale = 1.0f;
298      Dtype alpha = negative_slope;  
299      float beta = 1.0f;             
300      if (this->layer_param_.convolution_param().fusion_type() ==
301              ConvolutionParameter::SUM_FUSION &&
302          bottom.size() > 1) {
303        if (this->need_quantize_) {
304          float sum_scale;
305          sum_scale =
306              this->scale_out_[0] /
307              get_mkldnn_prv_descriptor<Dtype, false>(bottom[1])->get_scale(0) * coeff0;
308          ops.append_sum(sum_scale);
309        } else {
310          ops.append_sum(1.0f);
311        }
312      }
313      if (relu) ops.append_eltwise(scale, eltwise_relu, alpha, beta);
314      attr.set_post_ops(ops);
315      for (auto& convAlgorithm : eligibleAlgorithms) {
316        shared_ptr<convolution_forward::desc> convFwd_desc;
317        if (this->bias_term_) {
318            if (dilated_conv)
319                convFwd_desc.reset(new convolution_forward::desc(
320                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
321                    init_bias_md, init_top_md, convolutionStrides, dilation, padding, padding_r,
322                    padding_kind::zero));
323            else
324                convFwd_desc.reset(new convolution_forward::desc(
325                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
326                    init_bias_md, init_top_md, convolutionStrides, padding, padding,
327                    padding_kind::zero));
328        } else {
329            if (dilated_conv)
330                convFwd_desc.reset(new convolution_forward::desc(
331                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
332                    init_top_md, convolutionStrides, dilation, padding, padding_r,
333                    padding_kind::zero));
334            else
335                convFwd_desc.reset(new convolution_forward::desc(
336                    propagation, convAlgorithm, init_bottom_md, init_weights_md,
337                    init_top_md, convolutionStrides, padding, padding,
338                    padding_kind::zero));
339        }
340        for (subEngineIndex = 0; subEngineIndex < ep.getNumberOfSubEngines();
341             subEngineIndex++) {
342          try {
343            if (this->need_quantize_ || relu ||
344                (this->layer_param_.convolution_param().fusion_type() ==
345                     ConvolutionParameter::SUM_FUSION &&
346                 bottom.size() > 1)) {
347              convFwd_pd.reset(new convolution_forward::primitive_desc(
348                  *convFwd_desc, attr, ep.getMKLDNNSubEngine(subEngineIndex)));
349            } else {
350              convFwd_pd.reset(new convolution_forward::primitive_desc(
351                  *convFwd_desc, ep.getMKLDNNSubEngine(subEngineIndex)));
352            }
353          } catch (...) {
354              continue;
355          }
356          break;
357        }
358        if (convFwd_pd) break;
359      }
360      CHECK(convFwd_pd);
361      engine cpu_engine = CpuEngine::Instance().get_engine();
362      typedef typename memory::primitive_desc MemPD; 
363      shared_ptr<MemPD> prv_fwd_bottom_data_memory_pd(new MemPD(convFwd_pd->src_primitive_desc()));
364      shared_ptr<MemPD> prv_fwd_top_data_memory_pd(new MemPD(convFwd_pd->dst_primitive_desc()));
365      shared_ptr<MemPD> prv_fwd_weights_data_memory_pd(new MemPD(convFwd_pd->weights_primitive_desc()));
366      info_mem_pd<Dtype>(prv_fwd_bottom_data_memory_pd, "conv_src:" + this->layer_param_.name());
367      info_mem_pd<Dtype>(prv_fwd_top_data_memory_pd, "conv_dst:" + this->layer_param_.name());
368      memory::format data_mfmt;
369      memory::format weights_mfmt;
370      if (this->num_spatial_axes_ == 2) {
371        data_mfmt = memory::format::nchw;
372        weights_mfmt = (g!= 1) ? memory::format::goihw : memory::format::oihw;
373      } else {
374        data_mfmt = memory::format::ncdhw;
<span onclick='openModal()' class='match'>375        weights_mfmt = (g!= 1) ? memory::format::goidhw : memory::format::oidhw;
376      } 
377      shared_ptr<MemPD> usr_bottom_data_memory_pd(new MemPD({{bottom_tz}, mpcsn, data_mfmt}, cpu_engine));
</span>378      shared_ptr<MemPD> usr_bias_data_memory_pd(new MemPD({{bias_tz}, mpcsn, memory::format::x}, cpu_engine));
379      shared_ptr<MemPD> usr_top_data_memory_pd(new MemPD({{top_tz}, mpcsn, data_mfmt}, cpu_engine));
380      shared_ptr<MemPD> usr_weights_data_memory_pd(new MemPD({{weights_tz}, mpcsn, weights_mfmt}, cpu_engine));
381      if (this->need_quantize_){
382        std::vector<float> scale_bottom(1, this->layer_param_.quantization_param().force_u8_input()? 1.0f : this->scale_in_[0] );
383        fwd_bottom_data.reset(new MKLDNNData<Dtype>(usr_bottom_data_memory_pd, prv_fwd_bottom_data_memory_pd, bottom[0], this, scale_bottom));
384      } else {
385        fwd_bottom_data.reset(new MKLDNNData<Dtype>(usr_bottom_data_memory_pd, prv_fwd_bottom_data_memory_pd, bottom[0], this));
386      }
387      fwd_bottom_data->name = "fwd_bottom_data   @ " + this->layer_param_.name();
388      fwd_bottom_data_primitive = fwd_bottom_data->create_input(false);
389      if (this->need_quantize_){
390        std::vector<float> scale_top(1, this->scale_out_[0]);
391        fwd_top_data.reset(new MKLDNNData<Dtype>(usr_top_data_memory_pd, prv_fwd_top_data_memory_pd, top[0], this, scale_top, 0, is_sum));
392      } else{
393        fwd_top_data.reset(new MKLDNNData<Dtype>(usr_top_data_memory_pd, prv_fwd_top_data_memory_pd, top[0], this));
394      }
395      fwd_top_data->name = "fwd_top_data      @ " + this->layer_param_.name();
396      fwd_top_data_memory = fwd_top_data->create_output_memory();
397      bool is_wino = (prv_fwd_weights_data_memory_pd->desc().data.format == memory::format::wino_fmt); 
398    #ifdef _OPENMP
399      int node = caffe::cpu::OpenMpManager::getNumaNode();
400    #else
401      int node = 0;
402    #endif
403      if (fwd_weights_data == NULL) {
404        std::string name = "numa" + std::to_string(node) + "@fwd_weights_data@" + this->layer_param_.name();
405        if (this->need_quantize_){
406          int count = 1; 
407          int reorder_mask = 0;
408          if(this->scale_params_.size() > 1){
409              count = oc;  
410              reorder_mask = (g!= 1) ? (1<<1)+(1<<0) : 1<<0;
411          }
412          std::vector<float> scale_weight(count);
413          #ifdef _OPENMP
414          #pragma omp parallel for if (count > 1)
415          #endif
416          for(int i=0; i<count; i++){
417            scale_weight[i] = this->scale_params_[i];
418          }
419          fwd_weights_data.reset(new MKLDNNData<Dtype>(usr_weights_data_memory_pd, prv_fwd_weights_data_memory_pd, this->blobs_[0].get(), this, scale_weight, reorder_mask, false, false, true, name));
420        } else{
421          fwd_weights_data.reset(new MKLDNNData<Dtype>(usr_weights_data_memory_pd, prv_fwd_weights_data_memory_pd, this->blobs_[0].get(), this, {1.}, 0,  is_sum, is_wino, false, name));
422        }
423        fwd_weights_data->name.assign(name);
424        fwd_weights_data_primitive = fwd_weights_data->create_input(true);
425      }
426      if (this->bias_term_) {
427          if (fwd_bias_data == NULL) {
428            shared_ptr<MemPD> prv_fwd_bias_data_memory_pd(new MemPD(convFwd_pd->bias_primitive_desc()));
429            std::string name = "numa" + std::to_string(node) + "@fwd_bias_data@" + this->layer_param_.name();
430            if (this->need_quantize_){
431            int count = 1;  
432            int reorder_mask = 0;
433              if(this->scale_params_.size() > 1){
434                  count = oc;  
435                  reorder_mask = 1<<0;
436              }
437              std::vector<float> scale_bias(count);
438              #ifdef _OPENMP
439              #pragma omp parallel for if (count > 1)
440              #endif
441              for(int i=0; i<count; i++){
442                if (this->scale_params_[i] == 0.0)
443                    scale_bias[i] = 1.0;
444                else
445                    scale_bias[i] = this->scale_in_[0] * this->scale_params_[i];
446              }
447              fwd_bias_data.reset(new MKLDNNData<Dtype>(usr_bias_data_memory_pd, prv_fwd_bias_data_memory_pd, this->blobs_[1].get(), this, scale_bias, reorder_mask, false, false, false, name));
448            } else{
449              fwd_bias_data.reset(new MKLDNNData<Dtype>(usr_bias_data_memory_pd, prv_fwd_bias_data_memory_pd, this->blobs_[1].get(), this, {1.}, 0,  false, false, false, name));
450            }
451            fwd_bias_data->name.assign(name);
452            fwd_bias_data_primitive = fwd_bias_data->create_input(true);
453          }
454          convFwd.reset(new convolution_forward(*convFwd_pd
455                          , *fwd_bottom_data_primitive, *fwd_weights_data_primitive
456                          , *fwd_bias_data_primitive, *fwd_top_data_memory));
457          MKLDNNPrimitive<Dtype> fwd_bias_data_primitive_transfer(fwd_bias_data_primitive);
458          fwd_bias_data->set_mkldnn_primitive(fwd_bias_data_primitive_transfer);
459      } else {
460          convFwd.reset(new convolution_forward(*convFwd_pd
461                          , *fwd_bottom_data_primitive, *fwd_weights_data_primitive
462                          , *fwd_top_data_memory));
463      }
464      MKLDNNPrimitive<Dtype> fwd_bottom_data_primitive_transfer(fwd_bottom_data_primitive);
465      fwd_bottom_data->set_mkldnn_primitive(fwd_bottom_data_primitive_transfer);
466      MKLDNNPrimitive<Dtype> fwd_top_data_memory_transfer(fwd_top_data_memory);
467      fwd_top_data->set_mkldnn_primitive(fwd_top_data_memory_transfer);
468      MKLDNNPrimitive<Dtype> fwd_weights_data_primitive_transfer(fwd_weights_data_primitive);
469      fwd_weights_data->set_mkldnn_primitive(fwd_weights_data_primitive_transfer);
470  }
471  template <typename Dtype>
472  void MKLDNNConvolutionLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom
473                                                  , const vector<Blob<Dtype>*>& top)
474  {
475      VLOG(1) << "MKLDNNConvolutionLayer<Dtype>::Forward_cpu: " << this->layer_param_.name();
476      bool _mkldnn_primitive = false;
477      if( convFwd_pd == NULL || this->reshape){
478          InitConvolutionFwd(bottom, top);
479           if(getenv("CAFFE_INFERENCE_MEM_OPT")){
480               fwd_weights_data->sync_before_read();
481               if (this->bias_term_)
482                   fwd_bias_data->sync_before_read();
483               _mkldnn_primitive = true;
484           }
485      }
486      fwd_bottom_data->sync_before_read(); 
487      if(!getenv("CAFFE_INFERENCE_MEM_OPT")){
488          fwd_weights_data->sync_before_read();
489          if (this->bias_term_)
490              fwd_bias_data->sync_before_read();
491       } 
492      fwd_top_data->sync_before_write();
493      PERFORMANCE_EVENT_ID_INIT(perf_id_fw_, PERFORMANCE_MKLDNN_NAME("FW"));
494      PERFORMANCE_MEASUREMENT_BEGIN();
495      convFwd.submit();
496      if(_mkldnn_primitive) {
497        CircleBuf::Instance()->DecRefCnt(bottom[0]->prv_data());
498      }
499      PERFORMANCE_MEASUREMENT_END_ID(perf_id_fw_);
500  }
501  template <typename Dtype>
502  void MKLDNNConvolutionLayer<Dtype>::InitConvolutionBwd(const vector<Blob<Dtype>*>& top
503                                                      , const vector<bool>& propagate_down
504                                                      , const vector<Blob<Dtype>*>& bottom)
505  {
506      if (std::is_same<Dtype, double>::value)   NOT_IMPLEMENTED;
507      int32_t g  = std::max(this->group_, 1);
508      int32_t n  = this->num_;
509      int32_t iw = this->width_;
510      int32_t ih = this->height_;
511      int32_t ic = this->channels_;
512      int32_t id = this->depth_;
513      int32_t ow = this->width_out_;
514      int32_t oh = this->height_out_;
515      int32_t od = this->depth_out_;
516      int32_t oc = this->num_output_;
517      int32_t kw = this->kernel_w_;
518      int32_t kh = this->kernel_h_;
519      int32_t kd = this->kernel_d_;
520      int32_t sw = this->stride_w_;
521      int32_t sh = this->stride_h_;
522      int32_t sd = this->stride_d_;
523      int32_t pw = this->pad_w_;
524      int32_t ph = this->pad_h_;
525      int32_t pd = this->pad_d_;
526      memory::dims convolutionStrides;
527      memory::dims padding;
528      memory::dims padding_r;
529      memory::dims dilation;
530      bool dilated_conv = false;
531      const int* dilation_data = this->dilation_.cpu_data();
532      for (int i = 0; i < this->num_spatial_axes_; ++i) {
533        dilation.push_back(dilation_data[i] - 1);
534        if (dilation_data[i] != 1) dilated_conv = true;
535      }
536      if (this->num_spatial_axes_ == 2) {
537        convolutionStrides = {sh, sw};
538        padding = {ph, pw};
539        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[0]) + 1) - ph);
540        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[1]) + 1) - pw);
541      } else {
542        convolutionStrides = {sd, sh, sw};
543        padding = {pd, ph, pw};
544        padding_r.push_back((od - 1) * sd - id + ((kd - 1) * (dilation_data[0]) + 1) - pd);
545        padding_r.push_back((oh - 1) * sh - ih + ((kh - 1) * (dilation_data[1]) + 1) - ph);
546        padding_r.push_back((ow - 1) * sw - iw + ((kw - 1) * (dilation_data[2]) + 1) - pw);
547      }
548      memory::data_type mpcsn = memory::data_type::f32;
549      memory::format mfmt_any = memory::format::any;
550      memory::dims bottom_tz;
551      memory::dims bias_tz;
552      memory::dims top_tz;
553      memory::dims weights_tz;
554      if (this->num_spatial_axes_ == 2) {
555        bottom_tz = {n, ic, ih, iw};
556        bias_tz = {oc};
557        top_tz = {n, oc, oh, ow};
558        weights_tz = ( g!= 1) ? memory::dims{g, oc/g, ic/g, kh, kw} : memory::dims{oc, ic, kh, kw};
559      } else {
560        bottom_tz = {n, ic, id, ih, iw};
561        bias_tz = {oc};
562        top_tz = {n, oc, od, oh, ow};
563        weights_tz = ( g!= 1) ? memory::dims{g, oc/g, ic/g, kd, kh, kw} : memory::dims{oc, ic, kd, kh, kw};
564      } 
565      memory::desc init_bottom_md({bottom_tz}, mpcsn, mfmt_any);
566      memory::desc init_bias_md({bias_tz}, mpcsn, mfmt_any);
567      memory::desc init_top_md({top_tz}, mpcsn, mfmt_any);
568      memory::desc init_weights_md({weights_tz}, mpcsn, mfmt_any);
569      std::string subengines = this->layer_param_.engine();
570      if (subengines.find("MKLDNN") == std::string::npos || subengines == "MKLDNN")
571        subengines = "MKLDNN:CPU";
572      EngineParser ep(subengines);
573      unsigned subEngineIndex = 0;
574      auto eligibleAlgorithms = {conv_algorithm, algorithm::convolution_direct};
575      convBwdData_pd = NULL;
576      convBwdWeights_pd = NULL;
577      for (auto &convAlgorithm : eligibleAlgorithms) {
578          shared_ptr<convolution_backward_data::desc> convBwdData_desc;
579          shared_ptr<convolution_backward_weights::desc> convBwdWeights_desc;
580          if (this->bias_term_) {
581              if (dilated_conv)
582                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
583                              , init_bottom_md, init_weights_md, init_bias_md, init_top_md
584                              , convolutionStrides, dilation, padding, padding_r, padding_kind::zero));
585              else
586                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
587                              , init_bottom_md, init_weights_md, init_bias_md, init_top_md
588                              , convolutionStrides, padding, padding, padding_kind::zero));
589          } else {
590              if (dilated_conv)
591                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
592                              , init_bottom_md, init_weights_md, init_top_md
593                              , convolutionStrides, dilation, padding, padding_r, padding_kind::zero));
594              else
595                  convBwdWeights_desc.reset(new convolution_backward_weights::desc(convAlgorithm
596                              , init_bottom_md, init_weights_md, init_top_md
597                              , convolutionStrides, padding, padding, padding_kind::zero));
598          }
599          if (dilated_conv)
600              convBwdData_desc.reset(new convolution_backward_data::desc(convAlgorithm
601                              , init_bottom_md, init_weights_md, init_top_md
602                              , convolutionStrides, dilation, padding, padding_r, padding_kind::zero));
603          else
604              convBwdData_desc.reset(new convolution_backward_data::desc(convAlgorithm
605                              , init_bottom_md, init_weights_md, init_top_md
606                              , convolutionStrides, padding, padding, padding_kind::zero));
607          for(subEngineIndex=0; subEngineIndex < ep.getNumberOfSubEngines(); subEngineIndex++) {
608              try {
609                  convBwdData_pd.reset(new convolution_backward_data::primitive_desc(*convBwdData_desc,
610                                            ep.getMKLDNNSubEngine(subEngineIndex), *convFwd_pd));
611                  convBwdWeights_pd.reset(new convolution_backward_weights::primitive_desc(*convBwdWeights_desc,
612                                            ep.getMKLDNNSubEngine(subEngineIndex), *convFwd_pd));
613              }
614              catch(...) {
615                  continue;
616              }
617              break;
618          }
619          if (convBwdData_pd && convBwdWeights_pd)
620              break;
621      }
622      CHECK(convBwdData_pd);
623      CHECK(convBwdWeights_pd);
624      engine cpu_engine = CpuEngine::Instance().get_engine();
625      typedef typename memory::primitive_desc MemPD; 
626      shared_ptr<MemPD> prv_bwdd_bottom_diff_memory_pd(new MemPD(convBwdData_pd->diff_src_primitive_desc()));
627      shared_ptr<MemPD> prv_bwdd_top_diff_memory_pd(new MemPD(convBwdData_pd->diff_dst_primitive_desc()));
628      shared_ptr<MemPD> prv_bwdd_weights_data_memory_pd(new MemPD(convBwdData_pd->weights_primitive_desc()));
629      shared_ptr<MemPD> prv_bwdw_bottom_data_memory_pd(new MemPD(convBwdWeights_pd->src_primitive_desc()));
630      shared_ptr<MemPD> prv_bwdw_top_diff_memory_pd(new MemPD(convBwdWeights_pd->diff_dst_primitive_desc()));
631      shared_ptr<MemPD> prv_bwdw_weights_diff_memory_pd(new MemPD(convBwdWeights_pd->diff_weights_primitive_desc()));
632      memory::format data_mfmt;
633      memory::format weights_mfmt;
634      if (this->num_spatial_axes_ == 2) {
635        data_mfmt = memory::format::nchw;
636        weights_mfmt = ( g!= 1) ? memory::format::goihw : memory::format::oihw;
637      } else {
638        data_mfmt = memory::format::ncdhw;
639        weights_mfmt = ( g!= 1) ? memory::format::goidhw : memory::format::oidhw;
640      }
641      shared_ptr<MemPD> usr_bottom_data_memory_pd(new MemPD({{bottom_tz}, mpcsn, data_mfmt}, cpu_engine));
642      shared_ptr<MemPD> usr_bias_data_memory_pd(new MemPD({{bias_tz}, mpcsn, memory::format::x}, cpu_engine));
643      shared_ptr<MemPD> usr_top_data_memory_pd(new MemPD({{top_tz}, mpcsn, data_mfmt}, cpu_engine));
644      shared_ptr<MemPD> usr_weights_data_memory_pd(new MemPD({{weights_tz}, mpcsn, weights_mfmt}, cpu_engine));
645      bwdd_bottom_diff.reset(new MKLDNNDiff<Dtype>(usr_bottom_data_memory_pd, prv_bwdd_bottom_diff_memory_pd, bottom[0], this));
646      bwdd_bottom_diff ->name = "bwdd_bottom_diff   @ " + this->layer_param_.name();
647      bwdd_bottom_diff_memory = bwdd_bottom_diff->create_output_memory();
648      bwdw_bottom_data.reset(new MKLDNNData<Dtype>(usr_bottom_data_memory_pd, prv_bwdw_bottom_data_memory_pd, bottom[0], this));
649      bwdw_bottom_data ->name = "bwdw_bottom_data   @ " + this->layer_param_.name();
650      bwdw_bottom_data_primitive = bwdw_bottom_data->create_input(false);
651      bwdd_top_diff.reset(new MKLDNNDiff<Dtype>(usr_top_data_memory_pd, prv_bwdd_top_diff_memory_pd, top[0], this));
652      bwdd_top_diff    ->name = "bwdd_top_diff      @ " + this->layer_param_.name();
653      bwdd_top_diff_primitive = bwdd_top_diff->create_input(false);
654      bwdw_top_diff.reset(new MKLDNNDiff<Dtype>(usr_top_data_memory_pd, prv_bwdw_top_diff_memory_pd, top[0], this));
655      bwdw_top_diff    ->name = "bwdw_top_diff      @ " + this->layer_param_.name();
656      bwdw_top_diff_primitive = bwdw_top_diff->create_input(false);
657      bwdd_weights_data.reset(new MKLDNNData<Dtype>(usr_weights_data_memory_pd, prv_bwdd_weights_data_memory_pd, this->blobs_[0].get(), this));
658      bwdd_weights_data->name = "bwdd_weights_data  @ " + this->layer_param_.name();
659      bwdd_weights_data_primitive = bwdd_weights_data->create_input(false);
660      bwdw_weights_diff.reset(new MKLDNNDiff<Dtype>(usr_weights_data_memory_pd, prv_bwdw_weights_diff_memory_pd, this->blobs_[0].get(), this));
661      bwdw_weights_diff->name = "bwdw_weights_diff  @ " + this->layer_param_.name();
662      bwdw_weights_diff_memory = bwdw_weights_diff->create_output_memory();
663      if (Caffe::iter_size() > 1) {
664        shared_ptr<MemPD> prv_bwdw_weights_diff_memory_iter_pd(new MemPD(convBwdWeights_pd->diff_weights_primitive_desc()));
665        bwdw_weights_diff_iter.reset(new MKLDNNDiff<Dtype>(usr_weights_data_memory_pd, prv_bwdw_weights_diff_memory_iter_pd, bwdw_weights_diff_iter_blob.get(), this));
666        bwdw_weights_diff_memory_iter = bwdw_weights_diff_iter->create_output_memory();
667      }
668      if (this->bias_term_) {
669          shared_ptr<MemPD> prv_bwdw_bias_diff_memory_pd(new MemPD(convBwdWeights_pd->diff_bias_primitive_desc()));
670          bwdw_bias_diff.reset(new MKLDNNDiff<Dtype>(usr_bias_data_memory_pd, prv_bwdw_bias_diff_memory_pd, this->blobs_[1].get(), this));
671          bwdw_bias_diff->name = "bwdw_bias_diff     @ " + this->layer_param_.name();
672          bwdw_bias_diff_memory = bwdw_bias_diff->create_output_memory();
673          if (Caffe::iter_size() > 1) {
674            shared_ptr<MemPD> prv_bwdw_bias_diff_memory_iter_pd(new MemPD(convBwdWeights_pd->diff_bias_primitive_desc()));
675            bwdw_bias_diff_iter.reset(new MKLDNNDiff<Dtype>(usr_bias_data_memory_pd, prv_bwdw_bias_diff_memory_iter_pd, bwdw_bias_diff_iter_blob.get(), this));
676            bwdw_bias_diff_memory_iter = bwdw_bias_diff_iter->create_output_memory();
677            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
678                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
679                          , *bwdw_weights_diff_memory_iter, *bwdw_bias_diff_memory_iter));
680          } else {
681            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
682                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
683                          , *bwdw_weights_diff_memory, *bwdw_bias_diff_memory));
684          }
685          MKLDNNPrimitive<Dtype> bwdw_bias_diff_memory_transfer(bwdw_bias_diff_memory);
686          bwdw_bias_diff->set_mkldnn_primitive(bwdw_bias_diff_memory_transfer);
687          if (Caffe::iter_size() > 1) {
688            MKLDNNPrimitive<Dtype> bwdw_bias_diff_memory_iter_transfer(bwdw_bias_diff_memory_iter);
689            bwdw_bias_diff_iter->set_mkldnn_primitive(bwdw_bias_diff_memory_iter_transfer);
690          }
691      } else {
692          if (Caffe::iter_size() > 1) {
693            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
694                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
695                          , *bwdw_weights_diff_memory_iter));
696          } else {
697            convBwdWeights.reset(new convolution_backward_weights(*convBwdWeights_pd
698                          , *bwdw_bottom_data_primitive, *bwdw_top_diff_primitive
699                          , *bwdw_weights_diff_memory));
700          }
701      }
702      convBwdData.reset(new convolution_backward_data(*convBwdData_pd
703                      , *bwdd_top_diff_primitive, *bwdd_weights_data_primitive
704                      , *bwdd_bottom_diff_memory));
705      MKLDNNPrimitive<Dtype> bwdd_bottom_diff_memory_transfer(bwdd_bottom_diff_memory);
706      bwdd_bottom_diff->set_mkldnn_primitive(bwdd_bottom_diff_memory_transfer);
707      MKLDNNPrimitive<Dtype> bwdd_top_diff_primitive_transfer(bwdd_top_diff_primitive);
708      bwdd_top_diff->set_mkldnn_primitive(bwdd_top_diff_primitive_transfer);
709      MKLDNNPrimitive<Dtype> bwdd_weights_data_primitive_transfer(bwdd_weights_data_primitive);
710      bwdd_weights_data->set_mkldnn_primitive(bwdd_weights_data_primitive_transfer);
711      MKLDNNPrimitive<Dtype> bwdw_bottom_data_primitive_transfer(bwdw_bottom_data_primitive);
712      bwdw_bottom_data->set_mkldnn_primitive(bwdw_bottom_data_primitive_transfer);
713      MKLDNNPrimitive<Dtype> bwdw_top_diff_primitive_transfer(bwdw_top_diff_primitive);
714      bwdw_top_diff->set_mkldnn_primitive(bwdw_top_diff_primitive_transfer);
715      MKLDNNPrimitive<Dtype> bwdw_weights_diff_memory_transfer(bwdw_weights_diff_memory);
716      bwdw_weights_diff->set_mkldnn_primitive(bwdw_weights_diff_memory_transfer);
717      if (Caffe::iter_size() > 1) {
718        MKLDNNPrimitive<Dtype> bwdw_weights_diff_memory_iter_transfer(bwdw_weights_diff_memory_iter);
719        bwdw_weights_diff_iter->set_mkldnn_primitive(bwdw_weights_diff_memory_iter_transfer);
720      }
721  }
722  template <typename Dtype>
723  void MKLDNNConvolutionLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top
724                                                  , const vector<bool>& propagate_down
725                                                  , const vector<Blob<Dtype>*>& bottom)
726  {
727      VLOG(1) << "MKLDNNConvolutionLayer<Dtype>::Backward_cpu: " << this->layer_param_.name();
728      bool top_diff_is_prv = (const_cast<Dtype*>(top[0]->prv_diff()) != NULL);
729      if( convBwdData_pd == NULL || this->reshape)
730          InitConvolutionBwd(top, propagate_down, bottom);
731      if (propagate_down[0]) {
732          bwdd_top_diff->sync_before_read();
733          bwdd_weights_data->sync_before_read();
734          bwdd_bottom_diff->sync_before_write();
735          PERFORMANCE_EVENT_ID_INIT(perf_id_bw_, PERFORMANCE_MKLDNN_NAME("BW"));
736          PERFORMANCE_MEASUREMENT_BEGIN();
737  #ifdef DEBUG
738          if (bottom[0]->prv_data() != NULL)
739          {
740              LOG(INFO) << "Debug: Bottom prv data: " << *bottom[0]->prv_data();
741          }
742          else
743          {
744              LOG(INFO) << "Debug: Bottom prv data is NULL!";
745          }
746          if (top[0]->prv_diff() != NULL)
747          {
748              LOG(INFO) << "Debug: Top prv diff: " << *top[0]->prv_diff();
749          }
750          else
751          {
752              LOG(INFO) << "Debug: Top prv diff is NULL!";
753              LOG(INFO) << "Debug: Top cpu diff: " << *top[0]->cpu_diff();
754          }
755          if (this->blobs_[0]->prv_data() != NULL)
756          {
757              LOG(INFO) << "Debug: Weights prv data from blobs_[0]: " << *this->blobs_[0]->prv_data();
758          }
759          else
760          {
761              LOG(INFO) << "Debug: Weights prv data is NULL!";
762              LOG(INFO) << "Debug: Weights cpu data: " << *this->blobs_[0]->cpu_data();
763          }
764          LOG(INFO) << "Debug: Weights prv data from get_prv_ptr: " << *bwdd_weights_data->get_prv_ptr();
765  #endif
766          convBwdData.submit();
767  #ifdef DEBUG
768          if (bottom[0]->prv_diff() != NULL)
769          {
770              LOG(INFO) << "Debug: Bottom prv diff: " << *bottom[0]->prv_diff();
771          }
772          else
773          {
774              LOG(INFO) << "Debug: Bottom prv diff is NULL!";
775              LOG(INFO) << "Debug: Bottom cpu diff: " << *bottom[0]->cpu_diff();
776          }
777  #endif
778          PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_);
779      }
780      if (this->param_propagate_down(0)) {
781          if (!top_diff_is_prv && propagate_down[0])
782            top[0]->mutable_cpu_diff();
783          bwdw_top_diff->sync_before_read();
784          bwdw_bottom_data->sync_before_read();
785          bwdw_weights_diff->sync_before_write();
786          if (this->param_propagate_down(1)) {
787              CHECK(bwdw_bias_diff);
788              bwdw_bias_diff->sync_before_write();
789          }
790          PERFORMANCE_EVENT_ID_INIT(perf_id_bw_weights_,
791            PERFORMANCE_MKLDNN_NAME_DETAILED("BW", "_weights"));
792          PERFORMANCE_MEASUREMENT_BEGIN();
793          convBwdWeights.submit();
794          PERFORMANCE_MEASUREMENT_END_ID(perf_id_bw_weights_);
795          if (Caffe::iter_size() > 1) {
796            if (this->blobs_[0]->prv_diff() != NULL) {
797              caffe_axpy(this->blobs_[0]->prv_diff_count(), Dtype(1),
798                (Dtype*)(bwdw_weights_diff_memory_iter->get_data_handle()),
799                this->blobs_[0]->mutable_prv_diff());
800            } else {
801              caffe_axpy(this->blobs_[0]->count(), Dtype(1),
802                (Dtype*)(bwdw_weights_diff_memory_iter->get_data_handle()),
803                this->blobs_[0]->mutable_cpu_diff());
804            }
805          }
806          if (this->param_propagate_down(1)) {
807            if (Caffe::iter_size() > 1) {
808              if (this->blobs_[1]->prv_diff() != NULL) {
809                caffe_axpy(this->blobs_[1]->prv_diff_count(), Dtype(1),
810                  (Dtype*)(bwdw_bias_diff_memory_iter->get_data_handle()),
811                  this->blobs_[1]->mutable_prv_diff());
812              } else {
813                caffe_axpy(this->blobs_[1]->count(), Dtype(1),
814                  (Dtype*)(bwdw_bias_diff_memory_iter->get_data_handle()),
815                  this->blobs_[1]->mutable_cpu_diff());
816              }
817            }
818          }
819      }
820  }
821  #ifdef CPU_ONLY
822  STUB_GPU(MKLDNNConvolutionLayer);
823  #else
824  template <typename Dtype>
825  void MKLDNNConvolutionLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom
826                                                  , const vector<Blob<Dtype>*>& top)
827  {
828      NOT_IMPLEMENTED;
829  }
830  template <typename Dtype>
831  void MKLDNNConvolutionLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top
832                                                  , const vector<bool>& propagate_down
833                                                  , const vector<Blob<Dtype>*>& bottom)
834  {
835      NOT_IMPLEMENTED;
836  }
837  #endif
838  INSTANTIATE_CLASS(MKLDNNConvolutionLayer);
839  }  
840  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_convolution_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-mkldnn_convolution_layer.cpp</div>
                </div>
                <div class="column column_space"><pre><code>372        weights_mfmt = (g!= 1) ? memory::format::goihw : memory::format::oihw;
373      } else {
</pre></code></div>
                <div class="column column_space"><pre><code>375        weights_mfmt = (g!= 1) ? memory::format::goidhw : memory::format::oidhw;
376      } 
377      shared_ptr<MemPD> usr_bottom_data_memory_pd(new MemPD({{bottom_tz}, mpcsn, data_mfmt}, cpu_engine));
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    