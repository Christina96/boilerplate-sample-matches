<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for im2col_layer.cpp &amp; test_inner_product_layer.cpp</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for im2col_layer.cpp &amp; test_inner_product_layer.cpp
      </h3>
<h1 align="center">
        8.4%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>im2col_layer.cpp (11.881188%)<th>test_inner_product_layer.cpp (6.53951%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(111-114)<td><a href="#" name="0">(352-355)</a><td align="center"><font color="#ff0000">12</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(92-100)<td><a href="#" name="1">(181-184)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>im2col_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;vector&gt;
2 #include "caffe/layers/im2col_layer.hpp"
3 #include "caffe/util/im2col.hpp"
4 namespace caffe {
5 template &lt;typename Dtype&gt;
6 void Im2colLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
7       const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
8   ConvolutionParameter conv_param = this-&gt;layer_param_.convolution_param();
9   force_nd_im2col_ = conv_param.force_nd_im2col();
10   const int input_num_dims = bottom[0]-&gt;shape().size();
11   channel_axis_ = bottom[0]-&gt;CanonicalAxisIndex(conv_param.axis());
12   const int first_spatial_dim = channel_axis_ + 1;
13   num_spatial_axes_ = input_num_dims - first_spatial_dim;
14   CHECK_GE(num_spatial_axes_, 1);
15   vector&lt;int&gt; dim_blob_shape(1, num_spatial_axes_);
16   kernel_shape_.Reshape(dim_blob_shape);
17   int* kernel_shape_data = kernel_shape_.mutable_cpu_data();
18   if (conv_param.has_kernel_h() || conv_param.has_kernel_w()) {
19     CHECK_EQ(num_spatial_axes_, 2)
20         &lt;&lt; "kernel_h &amp; kernel_w can only be used for 2D convolution.";
21     CHECK_EQ(0, conv_param.kernel_size_size())
22         &lt;&lt; "Either kernel_size or kernel_h/w should be specified; not both.";
23     kernel_shape_data[0] = conv_param.kernel_h();
24     kernel_shape_data[1] = conv_param.kernel_w();
25   } else {
26     const int num_kernel_dims = conv_param.kernel_size_size();
27     CHECK(num_kernel_dims == 1 || num_kernel_dims == num_spatial_axes_)
28         &lt;&lt; "kernel_size must be specified once, or once per spatial dimension "
29         &lt;&lt; "(kernel_size specified " &lt;&lt; num_kernel_dims &lt;&lt; " times; "
30         &lt;&lt; num_spatial_axes_ &lt;&lt; " spatial dims);";
31       for (int i = 0; i &lt; num_spatial_axes_; ++i) {
32         kernel_shape_data[i] =
33             conv_param.kernel_size((num_kernel_dims == 1) ? 0 : i);
34       }
35   }
36   for (int i = 0; i &lt; num_spatial_axes_; ++i) {
37     CHECK_GT(kernel_shape_data[i], 0) &lt;&lt; "Filter dimensions must be nonzero.";
38   }
39   stride_.Reshape(dim_blob_shape);
40   int* stride_data = stride_.mutable_cpu_data();
41   if (conv_param.has_stride_h() || conv_param.has_stride_w()) {
42     CHECK_EQ(num_spatial_axes_, 2)
43         &lt;&lt; "stride_h &amp; stride_w can only be used for 2D convolution.";
44     CHECK_EQ(0, conv_param.stride_size())
45         &lt;&lt; "Either stride or stride_h/w should be specified; not both.";
46     stride_data[0] = conv_param.stride_h();
47     stride_data[1] = conv_param.stride_w();
48   } else {
49     const int num_stride_dims = conv_param.stride_size();
50     CHECK(num_stride_dims == 0 || num_stride_dims == 1 ||
51           num_stride_dims == num_spatial_axes_)
52         &lt;&lt; "stride must be specified once, or once per spatial dimension "
53         &lt;&lt; "(stride specified " &lt;&lt; num_stride_dims &lt;&lt; " times; "
54         &lt;&lt; num_spatial_axes_ &lt;&lt; " spatial dims);";
55     const int kDefaultStride = 1;
56     for (int i = 0; i &lt; num_spatial_axes_; ++i) {
57       stride_data[i] = (num_stride_dims == 0) ? kDefaultStride :
58           conv_param.stride((num_stride_dims == 1) ? 0 : i);
59       CHECK_GT(stride_data[i], 0) &lt;&lt; "Stride dimensions must be nonzero.";
60     }
61   }
62   pad_.Reshape(dim_blob_shape);
63   int* pad_data = pad_.mutable_cpu_data();
64   if (conv_param.has_pad_h() || conv_param.has_pad_w()) {
65     CHECK_EQ(num_spatial_axes_, 2)
66         &lt;&lt; "pad_h &amp; pad_w can only be used for 2D convolution.";
67     CHECK_EQ(0, conv_param.pad_size())
68         &lt;&lt; "Either pad or pad_h/w should be specified; not both.";
69     pad_data[0] = conv_param.pad_h();
70     pad_data[1] = conv_param.pad_w();
71   } else {
72     const int num_pad_dims = conv_param.pad_size();
73     CHECK(num_pad_dims == 0 || num_pad_dims == 1 ||
74           num_pad_dims == num_spatial_axes_)
75         &lt;&lt; "pad must be specified once, or once per spatial dimension "
76         &lt;&lt; "(pad specified " &lt;&lt; num_pad_dims &lt;&lt; " times; "
77         &lt;&lt; num_spatial_axes_ &lt;&lt; " spatial dims);";
78     const int kDefaultPad = 0;
79     for (int i = 0; i &lt; num_spatial_axes_; ++i) {
80       pad_data[i] = (num_pad_dims == 0) ? kDefaultPad :
81           conv_param.pad((num_pad_dims == 1) ? 0 : i);
82     }
83 <a name="1"></a>  }
84   dilation_.Reshape(dim_blob_shape);
85 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  int* dilation_data = dilation_.mutable_cpu_data();
86   const int num_dilation_dims = conv_param.dilation_size();
87   CHECK(num_dilation_dims == 0 || num_dilation_dims == 1 ||
88         num_dilation_dims == num_spatial_axes_)
89       &lt;&lt; "dilation must be specified once, or once per spatial dimension "
90       &lt;&lt; "(dilation specified " &lt;&lt; num_dilation_dims &lt;&lt; " times; "
91       &lt;&lt; num_spatial_axes_ &lt;&lt; " spatial dims).";
92   const int kDefaultDilation = 1;
93   for (int i = 0; i &lt; num_spatial_axes_; ++i) {</b></font>
94     dilation_data[i] = (num_dilation_dims == 0) ? kDefaultDilation :
95                        conv_param.dilation((num_dilation_dims == 1) ? 0 : i);
96   }
97 }
98 template &lt;typename Dtype&gt;
99 void Im2colLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
100 <a name="0"></a>      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
101   vector&lt;int&gt; top_shape = bottom[0]-&gt;shape();
102   const int* kernel_shape_data = kernel_shape_.cpu_data();
103 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>  const int* stride_data = stride_.cpu_data();
104   const int* pad_data = pad_.cpu_data();
105   const int* dilation_data = dilation_.cpu_data();
106   for (int i = 0; i &lt; num_spatial_axes_; ++i) {</b></font>
107     top_shape[channel_axis_] *= kernel_shape_data[i];
108     const int input_dim = bottom[0]-&gt;shape(channel_axis_ + i + 1);
109     const int kernel_extent = dilation_data[i] * (kernel_shape_data[i] - 1) + 1;
110     const int output_dim = (input_dim + 2 * pad_data[i] - kernel_extent)
111         / stride_data[i] + 1;
112     top_shape[channel_axis_ + i + 1] = output_dim;
113   }
114   top[0]-&gt;Reshape(top_shape);
115   num_ = bottom[0]-&gt;count(0, channel_axis_);
116   bottom_dim_ = bottom[0]-&gt;count(channel_axis_);
117   top_dim_ = top[0]-&gt;count(channel_axis_);
118   channels_ = bottom[0]-&gt;shape(channel_axis_);
119 }
120 template &lt;typename Dtype&gt;
121 void Im2colLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
122       const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
123   const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
124   Dtype* top_data = top[0]-&gt;mutable_cpu_data();
125   for (int n = 0; n &lt; num_; ++n) {
126     DCHECK_EQ(bottom[0]-&gt;shape().size() - channel_axis_, num_spatial_axes_ + 1);
127     DCHECK_EQ(top[0]-&gt;shape().size() - channel_axis_, num_spatial_axes_ + 1);
128     DCHECK_EQ(kernel_shape_.count(), num_spatial_axes_);
129     DCHECK_EQ(pad_.count(), num_spatial_axes_);
130     DCHECK_EQ(stride_.count(), num_spatial_axes_);
131     DCHECK_EQ(dilation_.count(), num_spatial_axes_);
132     if (!force_nd_im2col_ &amp;&amp; num_spatial_axes_ == 2) {
133       im2col_cpu(bottom_data + n * bottom_dim_, channels_,
134           bottom[0]-&gt;shape(channel_axis_ + 1),
135           bottom[0]-&gt;shape(channel_axis_ + 2),
136           kernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],
137           pad_.cpu_data()[0], pad_.cpu_data()[1],
138           stride_.cpu_data()[0], stride_.cpu_data()[1],
139           dilation_.cpu_data()[0], dilation_.cpu_data()[1],
140           top_data + n * top_dim_);
141     } else {
142       im2col_nd_cpu(bottom_data + n * bottom_dim_, num_spatial_axes_,
143           bottom[0]-&gt;shape().data() + channel_axis_,
144           top[0]-&gt;shape().data() + channel_axis_,
145           kernel_shape_.cpu_data(), pad_.cpu_data(), stride_.cpu_data(),
146           dilation_.cpu_data(), top_data + n * top_dim_);
147     }
148   }
149 }
150 template &lt;typename Dtype&gt;
151 void Im2colLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
152       const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
153   const Dtype* top_diff = top[0]-&gt;cpu_diff();
154   Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
155   for (int n = 0; n &lt; num_; ++n) {
156     if (!force_nd_im2col_ &amp;&amp; num_spatial_axes_ == 2) {
157       col2im_cpu(top_diff + n * top_dim_, channels_,
158           bottom[0]-&gt;shape(channel_axis_ + 1),
159           bottom[0]-&gt;shape(channel_axis_ + 2),
160           kernel_shape_.cpu_data()[0], kernel_shape_.cpu_data()[1],
161           pad_.cpu_data()[0], pad_.cpu_data()[1],
162           stride_.cpu_data()[0], stride_.cpu_data()[1],
163           dilation_.cpu_data()[0], dilation_.cpu_data()[1],
164           bottom_diff + n * bottom_dim_);
165     } else {
166       col2im_nd_cpu(top_diff + n * top_dim_, num_spatial_axes_,
167           bottom[0]-&gt;shape().data() + channel_axis_,
168           top[0]-&gt;shape().data() + channel_axis_,
169           kernel_shape_.cpu_data(), pad_.cpu_data(), stride_.cpu_data(),
170           dilation_.cpu_data(), bottom_diff + n * bottom_dim_);
171     }
172   }
173 }
174 #ifdef CPU_ONLY
175 STUB_GPU(Im2colLayer);
176 #endif
177 INSTANTIATE_CLASS(Im2colLayer);
178 REGISTER_LAYER_CLASS(Im2col);
}  </pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_inner_product_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 #include &lt;vector&gt;
2 #include "gtest/gtest.h"
3 #include "caffe/blob.hpp"
4 #include "caffe/common.hpp"
5 #include "caffe/filler.hpp"
6 #include "caffe/layers/inner_product_layer.hpp"
7 #include "caffe/test/test_caffe_main.hpp"
8 #include "caffe/test/test_gradient_check_util.hpp"
9 namespace caffe {
10 #ifndef CPU_ONLY
11 extern cudaDeviceProp CAFFE_TEST_CUDA_PROP;
12 #endif
13 template &lt;typename TypeParam&gt;
14 class InnerProductLayerTest : public MultiDeviceTest&lt;TypeParam&gt; {
15   typedef typename TypeParam::Dtype Dtype;
16  protected:
17   InnerProductLayerTest()
18       : blob_bottom_(new Blob&lt;Dtype&gt;(2, 3, 4, 5)),
19         blob_bottom_nobatch_(new Blob&lt;Dtype&gt;(1, 2, 3, 4)),
20         blob_top_(new Blob&lt;Dtype&gt;()) {
21     FillerParameter filler_param;
22     UniformFiller&lt;Dtype&gt; filler(filler_param);
23     filler.Fill(this-&gt;blob_bottom_);
24     blob_top_vec_.push_back(blob_top_);
25   }
26   virtual ~InnerProductLayerTest() {
27     delete blob_bottom_;
28     delete blob_bottom_nobatch_;
29     delete blob_top_;
30   }
31   Blob&lt;Dtype&gt;* const blob_bottom_;
32   Blob&lt;Dtype&gt;* const blob_bottom_nobatch_;
33   Blob&lt;Dtype&gt;* const blob_top_;
34   vector&lt;Blob&lt;Dtype&gt;*&gt; blob_bottom_vec_;
35   vector&lt;Blob&lt;Dtype&gt;*&gt; blob_top_vec_;
36 };
37 TYPED_TEST_CASE(InnerProductLayerTest, TestDtypesAndDevices);
38 TYPED_TEST(InnerProductLayerTest, TestSetUp) {
39   typedef typename TypeParam::Dtype Dtype;
40   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
41   LayerParameter layer_param;
42   InnerProductParameter* inner_product_param =
43       layer_param.mutable_inner_product_param();
44   inner_product_param-&gt;set_num_output(10);
45   shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
46       new InnerProductLayer&lt;Dtype&gt;(layer_param));
47   layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
48   EXPECT_EQ(this-&gt;blob_top_-&gt;num(), 2);
49   EXPECT_EQ(this-&gt;blob_top_-&gt;height(), 1);
50   EXPECT_EQ(this-&gt;blob_top_-&gt;width(), 1);
51   EXPECT_EQ(this-&gt;blob_top_-&gt;channels(), 10);
52 }
53 TYPED_TEST(InnerProductLayerTest, TestSetUpTransposeFalse) {
54   typedef typename TypeParam::Dtype Dtype;
55   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
56   LayerParameter layer_param;
57   InnerProductParameter* inner_product_param =
58       layer_param.mutable_inner_product_param();
59   inner_product_param-&gt;set_num_output(10);
60   inner_product_param-&gt;set_transpose(false);
61   shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
62       new InnerProductLayer&lt;Dtype&gt;(layer_param));
63   layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
64   EXPECT_EQ(2, this-&gt;blob_top_-&gt;num());
65   EXPECT_EQ(1, this-&gt;blob_top_-&gt;height());
66   EXPECT_EQ(1, this-&gt;blob_top_-&gt;width());
67   EXPECT_EQ(10, this-&gt;blob_top_-&gt;channels());
68   EXPECT_EQ(2, layer-&gt;blobs()[0]-&gt;num_axes());
69   EXPECT_EQ(10, layer-&gt;blobs()[0]-&gt;shape(0));
70   EXPECT_EQ(60, layer-&gt;blobs()[0]-&gt;shape(1));
71 }
72 TYPED_TEST(InnerProductLayerTest, TestSetUpTransposeTrue) {
73   typedef typename TypeParam::Dtype Dtype;
74   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
75   LayerParameter layer_param;
76   InnerProductParameter* inner_product_param =
77       layer_param.mutable_inner_product_param();
78   inner_product_param-&gt;set_num_output(10);
79   inner_product_param-&gt;set_transpose(true);
80   shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
81       new InnerProductLayer&lt;Dtype&gt;(layer_param));
82   layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
83   EXPECT_EQ(2, this-&gt;blob_top_-&gt;num());
84   EXPECT_EQ(1, this-&gt;blob_top_-&gt;height());
85   EXPECT_EQ(1, this-&gt;blob_top_-&gt;width());
86   EXPECT_EQ(10, this-&gt;blob_top_-&gt;channels());
87   EXPECT_EQ(2, layer-&gt;blobs()[0]-&gt;num_axes());
88   EXPECT_EQ(60, layer-&gt;blobs()[0]-&gt;shape(0));
89   EXPECT_EQ(10, layer-&gt;blobs()[0]-&gt;shape(1));
90 }
91 TYPED_TEST(InnerProductLayerTest, TestForward) {
92   typedef typename TypeParam::Dtype Dtype;
93   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
94   bool IS_VALID_CUDA = false;
95 #ifndef CPU_ONLY
96   IS_VALID_CUDA = CAFFE_TEST_CUDA_PROP.major &gt;= 2;
97 #endif
98   if (Caffe::mode() == Caffe::CPU ||
99       sizeof(Dtype) == 4 || IS_VALID_CUDA) {
100     LayerParameter layer_param;
101     InnerProductParameter* inner_product_param =
102         layer_param.mutable_inner_product_param();
103     inner_product_param-&gt;set_num_output(10);
104     inner_product_param-&gt;mutable_weight_filler()-&gt;set_type("uniform");
105     inner_product_param-&gt;mutable_bias_filler()-&gt;set_type("uniform");
106     inner_product_param-&gt;mutable_bias_filler()-&gt;set_min(1);
107     inner_product_param-&gt;mutable_bias_filler()-&gt;set_max(2);
108     shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
109         new InnerProductLayer&lt;Dtype&gt;(layer_param));
110     layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
111     layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
112     const Dtype* data = this-&gt;blob_top_-&gt;cpu_data();
113     const int count = this-&gt;blob_top_-&gt;count();
114     for (int i = 0; i &lt; count; ++i) {
115       EXPECT_GE(data[i], 1.);
116     }
117   } else {
118     LOG(ERROR) &lt;&lt; "Skipping test due to old architecture.";
119   }
120 }
121 TYPED_TEST(InnerProductLayerTest, TestForwardTranspose) {
122   typedef typename TypeParam::Dtype Dtype;
123   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
124   bool IS_VALID_CUDA = false;
125 #ifndef CPU_ONLY
126   IS_VALID_CUDA = CAFFE_TEST_CUDA_PROP.major &gt;= 2;
127 #endif
128   if (Caffe::mode() == Caffe::CPU ||
129       sizeof(Dtype) == 4 || IS_VALID_CUDA) {
130     LayerParameter layer_param;
131     InnerProductParameter* inner_product_param =
132         layer_param.mutable_inner_product_param();
133     inner_product_param-&gt;set_num_output(10);
134     inner_product_param-&gt;mutable_weight_filler()-&gt;set_type("uniform");
135     inner_product_param-&gt;mutable_bias_filler()-&gt;set_type("uniform");
136     inner_product_param-&gt;mutable_bias_filler()-&gt;set_min(1);
137     inner_product_param-&gt;mutable_bias_filler()-&gt;set_max(2);
138     inner_product_param-&gt;set_transpose(false);
139     shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
140         new InnerProductLayer&lt;Dtype&gt;(layer_param));
141     layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
142     layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
143     const int count = this-&gt;blob_top_-&gt;count();
144     Blob&lt;Dtype&gt;* const top = new Blob&lt;Dtype&gt;();
145     top-&gt;ReshapeLike(*this-&gt;blob_top_);
146     caffe_copy(count, this-&gt;blob_top_-&gt;cpu_data(), top-&gt;mutable_cpu_data());
147     this-&gt;blob_top_vec_.clear();
148     this-&gt;blob_top_vec_.push_back(new Blob&lt;Dtype&gt;());
149     inner_product_param-&gt;set_transpose(true);
150     shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; ip_t(
151         new InnerProductLayer&lt;Dtype&gt;(layer_param));
152     ip_t-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
153     const int count_w = layer-&gt;blobs()[0]-&gt;count();
154 <a name="1"></a>    EXPECT_EQ(count_w, ip_t-&gt;blobs()[0]-&gt;count());
155     const Dtype* w = layer-&gt;blobs()[0]-&gt;cpu_data();
156 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>    Dtype* w_t = ip_t-&gt;blobs()[0]-&gt;mutable_cpu_data();
157     const int width = layer-&gt;blobs()[0]-&gt;shape(1);
158     const int width_t = ip_t-&gt;blobs()[0]-&gt;shape(1);
159     for (int i = 0; i &lt; count_w; ++i) {</b></font>
160       int r = i / width;
161       int c = i % width;
162       w_t[c*width_t+r] = w[r*width+c];      }
163     ASSERT_EQ(layer-&gt;blobs()[1]-&gt;count(), ip_t-&gt;blobs()[1]-&gt;count());
164     caffe_copy(layer-&gt;blobs()[1]-&gt;count(), layer-&gt;blobs()[1]-&gt;cpu_data(),
165         ip_t-&gt;blobs()[1]-&gt;mutable_cpu_data());
166     ip_t-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
167     EXPECT_EQ(count, this-&gt;blob_top_-&gt;count())
168         &lt;&lt; "Invalid count for top blob for IP with transpose.";
169     Blob&lt;Dtype&gt;* const top_t = new Blob&lt;Dtype&gt;();\
170     top_t-&gt;ReshapeLike(*this-&gt;blob_top_vec_[0]);
171     caffe_copy(count,
172       this-&gt;blob_top_vec_[0]-&gt;cpu_data(),
173       top_t-&gt;mutable_cpu_data());
174     const Dtype* data = top-&gt;cpu_data();
175     const Dtype* data_t = top_t-&gt;cpu_data();
176     for (int i = 0; i &lt; count; ++i) {
177       EXPECT_FLOAT_EQ(data[i], data_t[i]);
178     }
179   } else {
180     LOG(ERROR) &lt;&lt; "Skipping test due to old architecture.";
181   }
182 }
183 TYPED_TEST(InnerProductLayerTest, TestForwardNoBatch) {
184   typedef typename TypeParam::Dtype Dtype;
185   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_nobatch_);
186   bool IS_VALID_CUDA = false;
187 #ifndef CPU_ONLY
188   IS_VALID_CUDA = CAFFE_TEST_CUDA_PROP.major &gt;= 2;
189 #endif
190   if (Caffe::mode() == Caffe::CPU ||
191       sizeof(Dtype) == 4 || IS_VALID_CUDA) {
192     LayerParameter layer_param;
193     InnerProductParameter* inner_product_param =
194         layer_param.mutable_inner_product_param();
195     inner_product_param-&gt;set_num_output(10);
196     inner_product_param-&gt;mutable_weight_filler()-&gt;set_type("uniform");
197     inner_product_param-&gt;mutable_bias_filler()-&gt;set_type("uniform");
198     inner_product_param-&gt;mutable_bias_filler()-&gt;set_min(1);
199     inner_product_param-&gt;mutable_bias_filler()-&gt;set_max(2);
200     shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
201         new InnerProductLayer&lt;Dtype&gt;(layer_param));
202     layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
203     layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
204     const Dtype* data = this-&gt;blob_top_-&gt;cpu_data();
205     const int count = this-&gt;blob_top_-&gt;count();
206     for (int i = 0; i &lt; count; ++i) {
207       EXPECT_GE(data[i], 1.);
208     }
209   } else {
210     LOG(ERROR) &lt;&lt; "Skipping test due to old architecture.";
211   }
212 }
213 TYPED_TEST(InnerProductLayerTest, TestGradient) {
214   typedef typename TypeParam::Dtype Dtype;
215   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
216   bool IS_VALID_CUDA = false;
217 #ifndef CPU_ONLY
218   IS_VALID_CUDA = CAFFE_TEST_CUDA_PROP.major &gt;= 2;
219 #endif
220   if (Caffe::mode() == Caffe::CPU ||
221       sizeof(Dtype) == 4 || IS_VALID_CUDA) {
222     LayerParameter layer_param;
223     InnerProductParameter* inner_product_param =
224         layer_param.mutable_inner_product_param();
225     inner_product_param-&gt;set_num_output(10);
226     inner_product_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
227     inner_product_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
228     inner_product_param-&gt;mutable_bias_filler()-&gt;set_min(1);
229     inner_product_param-&gt;mutable_bias_filler()-&gt;set_max(2);
230     InnerProductLayer&lt;Dtype&gt; layer(layer_param);
231     GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
232     checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
233         this-&gt;blob_top_vec_);
234   } else {
235     LOG(ERROR) &lt;&lt; "Skipping test due to old architecture.";
236   }
237 }
238 TYPED_TEST(InnerProductLayerTest, TestGradientTranspose) {
239   typedef typename TypeParam::Dtype Dtype;
240   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
241   bool IS_VALID_CUDA = false;
242 #ifndef CPU_ONLY
243   IS_VALID_CUDA = CAFFE_TEST_CUDA_PROP.major &gt;= 2;
244 #endif
245   if (Caffe::mode() == Caffe::CPU ||
246       sizeof(Dtype) == 4 || IS_VALID_CUDA) {
247     LayerParameter layer_param;
248     InnerProductParameter* inner_product_param =
249         layer_param.mutable_inner_product_param();
250     inner_product_param-&gt;set_num_output(11);
251     inner_product_param-&gt;mutable_weight_filler()-&gt;set_type("gaussian");
252     inner_product_param-&gt;mutable_bias_filler()-&gt;set_type("gaussian");
253     inner_product_param-&gt;mutable_bias_filler()-&gt;set_min(1);
254     inner_product_param-&gt;mutable_bias_filler()-&gt;set_max(2);
255     inner_product_param-&gt;set_transpose(true);
256     InnerProductLayer&lt;Dtype&gt; layer(layer_param);
257     GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-3);
258     checker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,
259         this-&gt;blob_top_vec_);
260   } else {
261     LOG(ERROR) &lt;&lt; "Skipping test due to old architecture.";
262   }
263 }
264 TYPED_TEST(InnerProductLayerTest, TestBackwardTranspose) {
265   typedef typename TypeParam::Dtype Dtype;
266   this-&gt;blob_bottom_vec_.push_back(this-&gt;blob_bottom_);
267   bool IS_VALID_CUDA = false;
268 #ifndef CPU_ONLY
269   IS_VALID_CUDA = CAFFE_TEST_CUDA_PROP.major &gt;= 2;
270 #endif
271   if (Caffe::mode() == Caffe::CPU ||
272       sizeof(Dtype) == 4 || IS_VALID_CUDA) {
273     LayerParameter layer_param;
274     InnerProductParameter* inner_product_param =
275         layer_param.mutable_inner_product_param();
276     inner_product_param-&gt;set_num_output(10);
277     inner_product_param-&gt;mutable_weight_filler()-&gt;set_type("uniform");
278     inner_product_param-&gt;mutable_bias_filler()-&gt;set_type("uniform");
279     inner_product_param-&gt;mutable_bias_filler()-&gt;set_min(1);
280     inner_product_param-&gt;mutable_bias_filler()-&gt;set_max(2);
281     inner_product_param-&gt;set_transpose(false);
282     shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; layer(
283         new InnerProductLayer&lt;Dtype&gt;(layer_param));
284     layer-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
285     layer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
286     Blob&lt;Dtype&gt;* const top = new Blob&lt;Dtype&gt;();
287     top-&gt;CopyFrom(*this-&gt;blob_top_, false, true);
288     Blob&lt;Dtype&gt;* const diff = new Blob&lt;Dtype&gt;();
289     diff-&gt;ReshapeLike(*this-&gt;blob_top_);
290     {
291       FillerParameter filler_param;
292       UniformFiller&lt;Dtype&gt; filler(filler_param);
293       filler.Fill(diff);
294     }
295     caffe_copy(this-&gt;blob_top_vec_[0]-&gt;count(),
296       diff-&gt;cpu_data(),
297       this-&gt;blob_top_vec_[0]-&gt;mutable_cpu_diff());
298     vector&lt;bool&gt; propagate_down(1, true);
299     layer-&gt;Backward(this-&gt;blob_top_vec_,
300         propagate_down,
301         this-&gt;blob_bottom_vec_);
302     Blob&lt;Dtype&gt;* const w = new Blob&lt;Dtype&gt;();
303     w-&gt;CopyFrom(*layer-&gt;blobs()[0], false, true);
304     w-&gt;CopyFrom(*layer-&gt;blobs()[0], true, true);
305     Blob&lt;Dtype&gt;* const bottom_diff = new Blob&lt;Dtype&gt;();
306     bottom_diff-&gt;CopyFrom(*this-&gt;blob_bottom_vec_[0], true, true);
307     this-&gt;blob_top_vec_.clear();
308     this-&gt;blob_top_vec_.push_back(new Blob&lt;Dtype&gt;());
309     inner_product_param-&gt;set_transpose(true);
310     shared_ptr&lt;InnerProductLayer&lt;Dtype&gt; &gt; ip_t(
311         new InnerProductLayer&lt;Dtype&gt;(layer_param));
312     ip_t-&gt;SetUp(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
313 <a name="0"></a>        {
314       const Dtype* w_src = w-&gt;cpu_data();
315 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>      Dtype* w_t = ip_t-&gt;blobs()[0]-&gt;mutable_cpu_data();
316       const int width = layer-&gt;blobs()[0]-&gt;shape(1);
317       const int width_t = ip_t-&gt;blobs()[0]-&gt;shape(1);
318       for (int i = 0; i &lt; layer-&gt;blobs()[0]-&gt;count(); ++i) {</b></font>
319         int r = i / width;
320         int c = i % width;
321         w_t[c*width_t+r] = w_src[r*width+c];        }
322       ASSERT_EQ(layer-&gt;blobs()[1]-&gt;count(), ip_t-&gt;blobs()[1]-&gt;count());
323       caffe_copy(layer-&gt;blobs()[1]-&gt;count(), layer-&gt;blobs()[1]-&gt;cpu_data(),
324           ip_t-&gt;blobs()[1]-&gt;mutable_cpu_data());
325     }
326     ip_t-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);
327     caffe_copy(this-&gt;blob_top_vec_[0]-&gt;count(),
328       diff-&gt;cpu_data(),
329       this-&gt;blob_top_vec_[0]-&gt;mutable_cpu_diff());
330     ip_t-&gt;Backward(this-&gt;blob_top_vec_, propagate_down, this-&gt;blob_bottom_vec_);
331     const Dtype* data = w-&gt;cpu_diff();
332     const Dtype* data_t = ip_t-&gt;blobs()[0]-&gt;cpu_diff();
333     const int WIDTH = layer-&gt;blobs()[0]-&gt;shape(1);
334     const int WIDTH_T = ip_t-&gt;blobs()[0]-&gt;shape(1);
335     for (int i = 0; i &lt; layer-&gt;blobs()[0]-&gt;count(); ++i) {
336       int r = i / WIDTH;
337       int c = i % WIDTH;
338       EXPECT_NE(Dtype(0.), data[r*WIDTH+c]);
339       EXPECT_FLOAT_EQ(data[r*WIDTH+c], data_t[c*WIDTH_T+r]);
340     }
341     data = bottom_diff-&gt;cpu_diff();
342     data_t = this-&gt;blob_bottom_vec_[0]-&gt;cpu_diff();
343     for (int i = 0; i &lt; this-&gt;blob_bottom_vec_[0]-&gt;count(); ++i) {
344       EXPECT_NE(Dtype(0.), data[i]);
345       EXPECT_FLOAT_EQ(data[i], data_t[i]);
346     }
347   } else {
348     LOG(ERROR) &lt;&lt; "Skipping test due to old architecture.";
349   }
350 }
}  </pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
