
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 3.7548707049238397%, Tokens: 13, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkldnn_convolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKLDNN_SUPPORTED
2  #include <vector>
3  #include "gtest/gtest.h"
4  #include "caffe/blob.hpp"
5  #include "caffe/common.hpp"
6  #include "caffe/filler.hpp"
7  #include "caffe/layers/mkldnn_layers.hpp"
8  #include "caffe/test/test_caffe_main.hpp"
9  #include "caffe/test/test_gradient_check_util.hpp"
10  namespace caffe {
11  template <typename Dtype>
12  void caffe_conv(const Blob<Dtype>* in, ConvolutionParameter* conv_param,
13      const vector<shared_ptr<Blob<Dtype> > >& weights,
14      Blob<Dtype>* out) {
15    const bool has_depth = (out->num_axes() == 5);
16    if (!has_depth) { CHECK_EQ(4, out->num_axes()); }
17    int kernel_h, kernel_w;
18    if (conv_param->has_kernel_h() || conv_param->has_kernel_w()) {
19      kernel_h = conv_param->kernel_h();
20      kernel_w = conv_param->kernel_w();
21    } else {
22      kernel_h = kernel_w = conv_param->kernel_size(0);
23    }
24    int pad_h, pad_w;
25    if (conv_param->has_pad_h() || conv_param->has_pad_w()) {
26      pad_h = conv_param->pad_h();
27      pad_w = conv_param->pad_w();
28    } else {
29      pad_h = pad_w = conv_param->pad_size() ? conv_param->pad(0) : 0;
30    }
31    int stride_h, stride_w;
32    if (conv_param->has_stride_h() || conv_param->has_stride_w()) {
33      stride_h = conv_param->stride_h();
34      stride_w = conv_param->stride_w();
35    } else {
36      stride_h = stride_w = conv_param->stride_size() ? conv_param->stride(0) : 1;
37    }
38    int dilation_h, dilation_w;
39    dilation_h = dilation_w = conv_param->dilation_size() ?
40                              conv_param->dilation(0) : 1;
41    int kernel_d, pad_d, stride_d, dilation_d;
42    if (has_depth) {
43      kernel_d = kernel_h;
44      stride_d = stride_h;
45      pad_d = pad_h;
46      dilation_d = dilation_h;
47    } else {
48      kernel_d = stride_d = dilation_d = 1;
49      pad_d = 0;
50    }
51    int groups = conv_param->group();
52    int o_g = out->shape(1) / groups;
53    int k_g = in->shape(1) / groups;
54    int o_head, k_head;
55    vector<int> weight_offset(4 + has_depth);
56    vector<int> in_offset(4 + has_depth);
57    vector<int> out_offset(4 + has_depth);
58    Dtype* out_data = out->mutable_cpu_data();
59    for (int n = 0; n < out->shape(0); n++) {
60      for (int g = 0; g < groups; g++) {
61        o_head = o_g * g;
62        k_head = k_g * g;
63        for (int o = 0; o < o_g; o++) {
64          for (int k = 0; k < k_g; k++) {
65            for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
66              for (int y = 0; y < out->shape(2 + has_depth); y++) {
67                for (int x = 0; x < out->shape(3 + has_depth); x++) {
68                  for (int r = 0; r < kernel_d; r++) {
69                    for (int p = 0; p < kernel_h; p++) {
70                      for (int q = 0; q < kernel_w; q++) {
71                        int in_z = z * stride_d - pad_d + r * dilation_d;
72                        int in_y = y * stride_h - pad_h + p * dilation_h;
73                        int in_x = x * stride_w - pad_w + q * dilation_w;
74                        if (in_z >= 0 && in_z < (has_depth ? in->shape(2) : 1)
75                            && in_y >= 0 && in_y < in->shape(2 + has_depth)
76                            && in_x >= 0 && in_x < in->shape(3 + has_depth)) {
77                          weight_offset[0] = o + o_head;
78                          weight_offset[1] = k;
79                          if (has_depth) { weight_offset[2] = r; }
80                          weight_offset[2 + has_depth] = p;
81                          weight_offset[3 + has_depth] = q;
82                          in_offset[0] = n;
83                          in_offset[1] = k + k_head;
84                          if (has_depth) { in_offset[2] = in_z; }
85                          in_offset[2 + has_depth] = in_y;
86                          in_offset[3 + has_depth] = in_x;
87                          out_offset[0] = n;
88                          out_offset[1] = o + o_head;
89                          if (has_depth) { out_offset[2] = z; }
90                          out_offset[2 + has_depth] = y;
91                          out_offset[3 + has_depth] = x;
92                          out_data[out->offset(out_offset)] +=
93                              in->data_at(in_offset)
94                              * weights[0]->data_at(weight_offset);
95                        }
96                      }
97                    }
98                  }
99                }
100              }
101            }
102          }
103        }
104      }
105    }
106    if (conv_param->bias_term()) {
107      const Dtype* bias_data = weights[1]->cpu_data();
108      for (int n = 0; n < out->shape(0); n++) {
109        for (int o = 0; o < out->shape(1); o++) {
110          for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
111            for (int y = 0; y < out->shape(2 + has_depth); y++) {
112              for (int x = 0; x < out->shape(3 + has_depth); x++) {
113                out_offset[0] = n;
114                out_offset[1] = o;
115                if (has_depth) { out_offset[2] = z; }
116                out_offset[2 + has_depth] = y;
117                out_offset[3 + has_depth] = x;
118                out_data[out->offset(out_offset)] += bias_data[o];
119              }
120            }
121          }
122        }
123      }
124    }
125    if (conv_param->relu()){
126      for (int n = 0; n < out->shape(0); n++) {
127        for (int o = 0; o < out->shape(1); o++) {
128          for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
129            for (int y = 0; y < out->shape(2 + has_depth); y++) {
130              for (int x = 0; x < out->shape(3 + has_depth); x++) {
131                out_offset[0] = n;
132                out_offset[1] = o;
133                if (has_depth) { out_offset[2] = z; }
134                out_offset[2 + has_depth] = y;
135                out_offset[3 + has_depth] = x;
136                if(out_data[out->offset(out_offset)] < 0) out_data[out->offset(out_offset)] = 0;
137              }
138            }
139          }
140        }
141      }
142    }
143  }
144  template void caffe_conv(const Blob<float>* in,
145      ConvolutionParameter* conv_param,
146      const vector<shared_ptr<Blob<float> > >& weights,
147      Blob<float>* out);
148  template void caffe_conv(const Blob<double>* in,
149      ConvolutionParameter* conv_param,
150      const vector<shared_ptr<Blob<double> > >& weights,
151      Blob<double>* out);
152  template <typename TypeParam>
153  class MKLDNNConvolutionLayerTest : public MultiDeviceTest<TypeParam> {
154    typedef typename TypeParam::Dtype Dtype;
155  #define MB 2
156  #define IC 8
157  #define OC 8
158  #define IH 5
159  #define IW 5
160  #define OH 5
161  #define OW 5
162  #define KH 3
163  #define KW 3
164  #define CS 1
165  #define GR 2
166  #define PD 1
167   protected:
168    MKLDNNConvolutionLayerTest()
169        : blob_bottom_(new Blob<Dtype>(MB, IC, IH, IW)),
170          blob_bottom_2_(new Blob<Dtype>(MB, IC, IH, IW)),
171          blob_top_(new Blob<Dtype>()),
172          blob_top_2_(new Blob<Dtype>()) {}
173    virtual void SetUp() {
174      FillerParameter filler_param;
175      filler_param.set_value(1.);
176      GaussianFiller<Dtype> filler(filler_param);
177      filler.Fill(this->blob_bottom_);
178      filler.Fill(this->blob_bottom_2_);
179      blob_bottom_vec_.push_back(blob_bottom_);
180      blob_top_vec_.push_back(blob_top_);
181    }
182    virtual ~MKLDNNConvolutionLayerTest() {
183      delete blob_bottom_;
184      delete blob_bottom_2_;
185      delete blob_top_;
186      delete blob_top_2_;
187    }
188    virtual Blob<Dtype>* MakeReferenceTop(Blob<Dtype>* top) {
189      this->ref_blob_top_.reset(new Blob<Dtype>());
190      this->ref_blob_top_->ReshapeLike(*top);
191      return this->ref_blob_top_.get();
192    }
193    Blob<Dtype>* const blob_bottom_;
194    Blob<Dtype>* const blob_bottom_2_;
195    Blob<Dtype>* const blob_top_;
196    Blob<Dtype>* const blob_top_2_;
197    shared_ptr<Blob<Dtype> > ref_blob_top_;
198    vector<Blob<Dtype>*> blob_bottom_vec_;
199    vector<Blob<Dtype>*> blob_top_vec_;
200  };
201  typedef ::testing::Types<CPUDevice<float>
202                          > TestDtypesCPU;
203  TYPED_TEST_CASE(MKLDNNConvolutionLayerTest, TestDtypesCPU);
204  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSetupMKLDNN) {
205    typedef typename TypeParam::Dtype Dtype;
206    LayerParameter layer_param;
207    ConvolutionParameter* convolution_param =
208        layer_param.mutable_convolution_param();
209    convolution_param->add_kernel_size(KH);
210    convolution_param->add_stride(CS);
211    convolution_param->set_num_output(OC);
212    convolution_param->add_pad(PD);
213    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
214    this->blob_top_vec_.push_back(this->blob_top_2_);
215    shared_ptr<Layer<Dtype> > layer(
216        new MKLDNNConvolutionLayer<Dtype>(layer_param));
217    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
218    EXPECT_EQ(this->blob_top_->num(), MB);
219    EXPECT_EQ(this->blob_top_->channels(), OC);
220    EXPECT_EQ(this->blob_top_->height(), OH);
221    EXPECT_EQ(this->blob_top_->width(), OW);
222    EXPECT_EQ(this->blob_top_2_->num(), MB);
223    EXPECT_EQ(this->blob_top_2_->channels(), OC );
224    EXPECT_EQ(this->blob_top_2_->height(), OH);
225    EXPECT_EQ(this->blob_top_2_->width(), OW);
226    convolution_param->set_num_output(OC);
227    convolution_param->set_group(GR);
228    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
229    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
230    EXPECT_EQ(this->blob_top_->num(), MB);
231    EXPECT_EQ(this->blob_top_->channels(), OC);
232    EXPECT_EQ(this->blob_top_->height(), OH);
233    EXPECT_EQ(this->blob_top_->width(), OW);
234    EXPECT_EQ(this->blob_top_2_->num(), MB);
235    EXPECT_EQ(this->blob_top_2_->channels(), OC);
236    EXPECT_EQ(this->blob_top_2_->height(), OH);
237    EXPECT_EQ(this->blob_top_2_->width(), OW);
238  }
239  #if 0
240  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSetupMKLDNNWithRectangeKernelStridePad) {
241    typedef typename TypeParam::Dtype Dtype;
242    LayerParameter layer_param;
243    ConvolutionParameter* convolution_param =
244        layer_param.mutable_convolution_param();
245    convolution_param->set_kernel_h(4);
246    convolution_param->set_kernel_w(1);
247    convolution_param->set_stride_h(3);
248    convolution_param->set_stride_w(1);
249    convolution_param->set_num_output(OC);
250    convolution_param->set_pad_h(2);
251    convolution_param->set_pad_w(1);
252    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
253    this->blob_top_vec_.push_back(this->blob_top_2_);
254    shared_ptr<MKLDNNConvolutionLayer<Dtype> > layer(
255        new MKLDNNConvolutionLayer<Dtype>(layer_param));
256    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
257    EXPECT_EQ(convolution_param->kernel_h(), 4);
258    EXPECT_EQ(layer->GetKernelHeight(), 4);
259    EXPECT_EQ(convolution_param->kernel_w(), 1);
260    EXPECT_EQ(layer->GetKernelWidth(), 1);
261    EXPECT_EQ(convolution_param->stride_h(), 3);
262    EXPECT_EQ(layer->GetStrideHeight(), 3);
263    EXPECT_EQ(convolution_param->stride_w(), 1);
264    EXPECT_EQ(layer->GetStrideWidth(), 1);
265    EXPECT_EQ(convolution_param->pad_h(), 2);
266    EXPECT_EQ(layer->GetPadHeight(), 2);
267    EXPECT_EQ(convolution_param->pad_w(), 1);
268    EXPECT_EQ(layer->GetPadWidth(), 1);
269    convolution_param->set_num_output(OC);
270    convolution_param->set_group(GR);
271    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
272    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
273    EXPECT_EQ(convolution_param->kernel_h(), 4);
274    EXPECT_EQ(layer->GetKernelHeight(), 4);
275    EXPECT_EQ(convolution_param->kernel_w(), 1);
276    EXPECT_EQ(layer->GetKernelWidth(), 1);
277    EXPECT_EQ(convolution_param->stride_h(), 3);
278    EXPECT_EQ(layer->GetStrideHeight(), 3);
279    EXPECT_EQ(convolution_param->stride_w(), 1);
280    EXPECT_EQ(layer->GetStrideWidth(), 1);
281    EXPECT_EQ(convolution_param->pad_h(), 2);
282    EXPECT_EQ(layer->GetPadHeight(), 2);
283    EXPECT_EQ(convolution_param->pad_w(), 1);
284    EXPECT_EQ(layer->GetPadWidth(), 1);
285  }
286  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionMKLDNN) {
287    typedef typename TypeParam::Dtype Dtype;
288    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
289    this->blob_top_vec_.push_back(this->blob_top_2_);
290    LayerParameter layer_param;
291    ConvolutionParameter* convolution_param =
292        layer_param.mutable_convolution_param();
293    convolution_param->add_kernel_size(KH);
294    convolution_param->add_stride(CS);
295    convolution_param->set_num_output(OC);
296    convolution_param->add_pad(PD);
297    convolution_param->mutable_weight_filler()->set_type("gaussian");
298    convolution_param->mutable_bias_filler()->set_type("constant");
299    convolution_param->mutable_bias_filler()->set_value(0.1);
300    shared_ptr<Layer<Dtype> > layer(
301        new MKLDNNConvolutionLayer<Dtype>(layer_param));
302    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
303    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
304    const Dtype* top_data;
305    const Dtype* ref_top_data;
306    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
307        this->MakeReferenceTop(this->blob_top_));
308    top_data = this->blob_top_->cpu_data();
309    ref_top_data = this->ref_blob_top_->cpu_data();
310    for (int i = 0; i < this->blob_top_->count(); ++i) {
311      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
312    }
313  #if 0   
314    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
315        this->MakeReferenceTop(this->blob_top_2_));
316    top_data = this->blob_top_2_->cpu_data();
317    ref_top_data = this->ref_blob_top_->cpu_data();
318    for (int i = 0; i < this->blob_top_->count(); ++i) {
319      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
320    }
321  #endif
322  }
323  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionReLUMKLDNN) {
324    typedef typename TypeParam::Dtype Dtype;
325    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
326    this->blob_top_vec_.push_back(this->blob_top_2_);
327    LayerParameter layer_param;
328    ConvolutionParameter* convolution_param =
329        layer_param.mutable_convolution_param();
330    convolution_param->add_kernel_size(3);
331    convolution_param->add_stride(2);
332    convolution_param->set_num_output(OC);
333    convolution_param->set_relu(true);
334    convolution_param->mutable_weight_filler()->set_type("gaussian");
335    convolution_param->mutable_bias_filler()->set_type("constant");
336    convolution_param->mutable_bias_filler()->set_value(0.1);
337    shared_ptr<Layer<Dtype> > layer(
338        new MKLDNNConvolutionLayer<Dtype>(layer_param));
339    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
340    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
341    const Dtype* top_data;
342    const Dtype* ref_top_data;
343    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
344        this->MakeReferenceTop(this->blob_top_));
345    top_data = this->blob_top_->cpu_data();
346    ref_top_data = this->ref_blob_top_->cpu_data();
347    for (int i = 0; i < this->blob_top_->count(); ++i) {
348      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
349    }
350  }
351  TYPED_TEST(MKLDNNConvolutionLayerTest, TestDilatedConvolutionMKLDNN) {
352    typedef typename TypeParam::Dtype Dtype;
353    vector<int> bottom_shape;
354    bottom_shape.push_back(2);
355    bottom_shape.push_back(3);
356    bottom_shape.push_back(8);
357    bottom_shape.push_back(7);
358    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
359    this->blob_top_vec_.push_back(this->blob_top_2_);
360    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
361      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
362    }
363    LayerParameter layer_param;
364    ConvolutionParameter* convolution_param =
365        layer_param.mutable_convolution_param();
366    convolution_param->add_kernel_size(3);
367    convolution_param->add_dilation(2);
368    convolution_param->set_num_output(4);
369    convolution_param->mutable_weight_filler()->set_type("gaussian");
370    convolution_param->mutable_bias_filler()->set_type("constant");
371    convolution_param->mutable_bias_filler()->set_value(0.1);
372    shared_ptr<Layer<Dtype> > layer(
373        new MKLDNNConvolutionLayer<Dtype>(layer_param));
374    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
375    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
376    const Dtype* top_data;
377    const Dtype* ref_top_data;
378    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
379               this->MakeReferenceTop(this->blob_top_));
380    top_data = this->blob_top_->cpu_data();
381    ref_top_data = this->ref_blob_top_->cpu_data();
382    for (int i = 0; i < this->blob_top_->count(); ++i) {
383      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
384    }
385  #if 0   
386    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
387               this->MakeReferenceTop(this->blob_top_2_));
388    top_data = this->blob_top_2_->cpu_data();
389    ref_top_data = this->ref_blob_top_->cpu_data();
390    for (int i = 0; i < this->blob_top_->count(); ++i) {
391      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
392    }
393  #endif
394  }
395  #endif
396  #if 0
397  TYPED_TEST(MKLDNNConvolutionLayerTest, Test0DConvolutionMKLDNN) {
398    typedef typename TypeParam::Dtype Dtype;
399    LayerParameter layer_param;
400    ConvolutionParameter* convolution_param =
401        layer_param.mutable_convolution_param();
402    const int kNumOutput = 3;
403    convolution_param->set_num_output(kNumOutput);
404    convolution_param->set_axis(3);
405    convolution_param->mutable_weight_filler()->set_type("gaussian");
406    convolution_param->mutable_bias_filler()->set_type("gaussian");
407    shared_ptr<Layer<Dtype> > layer(
408        new MKLDNNConvolutionLayer<Dtype>(layer_param));
409    vector<int> top_shape = this->blob_bottom_->shape();
410    top_shape[3] = kNumOutput;
411    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
412    EXPECT_EQ(top_shape, this->blob_top_->shape());
413    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
414    vector<int> weight_offset(2);
415    const Blob<Dtype>* weight = layer->blobs()[0].get();
416    const Blob<Dtype>* bias = layer->blobs()[1].get();
417    const int num = this->blob_top_->count(3);
418    const int dim = this->blob_top_->shape(3);
419    const int bottom_dim = this->blob_bottom_->shape(3);
420    for (int n = 0; n < num; ++n) {
421      for (int d = 0; d < dim; ++d) {
422        weight_offset[0] = d;
423        Dtype value = bias->cpu_data()[d];
424        for (int bottom_d = 0; bottom_d < bottom_dim; ++bottom_d) {
425          weight_offset[1] = bottom_d;
426          value += weight->data_at(weight_offset) *
427                   this->blob_bottom_->cpu_data()[n * bottom_dim + bottom_d];
428        }
429        EXPECT_NEAR(value, this->blob_top_->cpu_data()[n * dim + d], 1e-4);
430      }
431    }
432  }
433  #endif
434  #if 0
435  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimple3DConvolution) {
436    typedef typename TypeParam::Dtype Dtype;
437    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
438    this->blob_top_vec_.push_back(this->blob_top_2_);
439    vector<int> bottom_shape(5);
440    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
441    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
442    bottom_shape[2] = 5;
443    bottom_shape[3] = this->blob_bottom_vec_[0]->shape(2);
444    bottom_shape[4] = this->blob_bottom_vec_[0]->shape(3);
445    FillerParameter filler_param;
446    GaussianFiller<Dtype> filler(filler_param);
447    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
448      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
449      filler.Fill(this->blob_bottom_vec_[i]);
450    }
451    LayerParameter layer_param;
452    ConvolutionParameter* convolution_param =
453        layer_param.mutable_convolution_param();
454    convolution_param->add_kernel_size(3);
455    convolution_param->add_stride(2);
456    convolution_param->set_num_output(4);
457    convolution_param->mutable_weight_filler()->set_type("gaussian");
458    convolution_param->mutable_bias_filler()->set_type("gaussian");
459    shared_ptr<Layer<Dtype> > layer(
460        new MKLDNNConvolutionLayer<Dtype>(layer_param));
461    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
462    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
463    const Dtype* top_data;
464    const Dtype* ref_top_data;
465    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
466        this->MakeReferenceTop(this->blob_top_));
467    top_data = this->blob_top_->cpu_data();
468    ref_top_data = this->ref_blob_top_->cpu_data();
469    for (int i = 0; i < this->blob_top_->count(); ++i) {
470      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
471    }
472  #if 0   
473    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
474        this->MakeReferenceTop(this->blob_top_2_));
475    top_data = this->blob_top_2_->cpu_data();
476    ref_top_data = this->ref_blob_top_->cpu_data();
477    for (int i = 0; i < this->blob_top_->count(); ++i) {
478      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
479    }
480  #endif
481  }
482  #endif
483  #if 0
484  TYPED_TEST(MKLDNNConvolutionLayerTest, TestDilated3DConvolution) {
485    typedef typename TypeParam::Dtype Dtype;
486    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
487    this->blob_top_vec_.push_back(this->blob_top_2_);
488    vector<int> bottom_shape(5);
489    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
490    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
491    bottom_shape[2] = 6;
492    bottom_shape[3] = 7;
493    bottom_shape[4] = 8;
494    FillerParameter filler_param;
495    GaussianFiller<Dtype> filler(filler_param);
496    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
497      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
498      filler.Fill(this->blob_bottom_vec_[i]);
499    }
500    LayerParameter layer_param;
501    ConvolutionParameter* convolution_param =
502        layer_param.mutable_convolution_param();
503    convolution_param->add_kernel_size(3);
504    convolution_param->add_dilation(2);
505    convolution_param->set_num_output(4);
506    convolution_param->mutable_weight_filler()->set_type("gaussian");
507    convolution_param->mutable_bias_filler()->set_type("gaussian");
508    shared_ptr<Layer<Dtype> > layer(
509        new MKLDNNConvolutionLayer<Dtype>(layer_param));
510    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
511    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
512    const Dtype* top_data;
513    const Dtype* ref_top_data;
514    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
515               this->MakeReferenceTop(this->blob_top_));
516    top_data = this->blob_top_->cpu_data();
517    ref_top_data = this->ref_blob_top_->cpu_data();
518    for (int i = 0; i < this->blob_top_->count(); ++i) {
519      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
520    }
521    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
522               this->MakeReferenceTop(this->blob_top_2_));
523    top_data = this->blob_top_2_->cpu_data();
524    ref_top_data = this->ref_blob_top_->cpu_data();
525    for (int i = 0; i < this->blob_top_->count(); ++i) {
526      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
527    }
528  }
529  #endif
530  TYPED_TEST(MKLDNNConvolutionLayerTest, Test1x1Convolution) {
531    typedef typename TypeParam::Dtype Dtype;
532    LayerParameter layer_param;
533    ConvolutionParameter* convolution_param =
534        layer_param.mutable_convolution_param();
535    convolution_param->add_kernel_size(1);
536    convolution_param->add_stride(1);
537    convolution_param->set_num_output(OC);
538    convolution_param->mutable_weight_filler()->set_type("gaussian");
539    convolution_param->mutable_bias_filler()->set_type("constant");
540    convolution_param->mutable_bias_filler()->set_value(0.1);
541    shared_ptr<Layer<Dtype> > layer(
542        new MKLDNNConvolutionLayer<Dtype>(layer_param));
543    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
544    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
545    const Dtype* top_data;
546    const Dtype* ref_top_data;
547    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
548        this->MakeReferenceTop(this->blob_top_));
549    top_data = this->blob_top_->cpu_data();
550    ref_top_data = this->ref_blob_top_->cpu_data();
551    for (int i = 0; i < this->blob_top_->count(); ++i) {
552      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
553    }
554  }
555  TYPED_TEST(MKLDNNConvolutionLayerTest, Test1x1ConvolutionReLU) {
556    typedef typename TypeParam::Dtype Dtype;
557    LayerParameter layer_param;
558    ConvolutionParameter* convolution_param =
559        layer_param.mutable_convolution_param();
560    convolution_param->add_kernel_size(1);
561    convolution_param->add_stride(1);
562    convolution_param->set_num_output(OC);
563    convolution_param->set_relu(true);
564    convolution_param->mutable_weight_filler()->set_type("gaussian");
565    convolution_param->mutable_bias_filler()->set_type("constant");
566    convolution_param->mutable_bias_filler()->set_value(0.1);
567    shared_ptr<Layer<Dtype> > layer(
568        new MKLDNNConvolutionLayer<Dtype>(layer_param));
569    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
570    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
571    const Dtype* top_data;
572    const Dtype* ref_top_data;
573    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
574        this->MakeReferenceTop(this->blob_top_));
575    top_data = this->blob_top_->cpu_data();
576    ref_top_data = this->ref_blob_top_->cpu_data();
577    for (int i = 0; i < this->blob_top_->count(); ++i) {
578      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
579    }
580  }
581  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionGroup) {
582    typedef typename TypeParam::Dtype Dtype;
583    LayerParameter layer_param;
584    ConvolutionParameter* convolution_param =
585        layer_param.mutable_convolution_param();
586    convolution_param->add_kernel_size(KH);
587    convolution_param->add_stride(CS);
588    convolution_param->set_num_output(OC);
589    convolution_param->set_group(GR);
590    convolution_param->add_pad(PD);
591    convolution_param->mutable_weight_filler()->set_type("gaussian");
592    convolution_param->mutable_bias_filler()->set_type("constant");
593    convolution_param->mutable_bias_filler()->set_value(0.1);
594    shared_ptr<Layer<Dtype> > layer(
595        new MKLDNNConvolutionLayer<Dtype>(layer_param));
596    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
597    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
598    const Dtype* top_data;
599    const Dtype* ref_top_data;
600    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
601        this->MakeReferenceTop(this->blob_top_));
602    top_data = this->blob_top_->cpu_data();
603    ref_top_data = this->ref_blob_top_->cpu_data();
604    for (int i = 0; i < this->blob_top_->count(); ++i) {
605      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
606    }
607  }
608  #if 0
609  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionReLUGroup) {
610    typedef typename TypeParam::Dtype Dtype;
611    LayerParameter layer_param;
612    ConvolutionParameter* convolution_param =
613        layer_param.mutable_convolution_param();
614    convolution_param->add_kernel_size(3);
615    convolution_param->add_stride(2);
616    convolution_param->set_num_output(OC);
617    convolution_param->set_relu(true);
618    convolution_param->set_group(GR);
619    convolution_param->mutable_weight_filler()->set_type("gaussian");
620    convolution_param->mutable_bias_filler()->set_type("constant");
621    convolution_param->mutable_bias_filler()->set_value(0.1);
622    shared_ptr<Layer<Dtype> > layer(
623        new MKLDNNConvolutionLayer<Dtype>(layer_param));
624    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
625    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
626    const Dtype* top_data;
627    const Dtype* ref_top_data;
628    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
629        this->MakeReferenceTop(this->blob_top_));
630    top_data = this->blob_top_->cpu_data();
631    ref_top_data = this->ref_blob_top_->cpu_data();
632    for (int i = 0; i < this->blob_top_->count(); ++i) {
633      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
634    }
635  }
636  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSobelConvolution) {
637    typedef typename TypeParam::Dtype Dtype;
638    shared_ptr<GaussianFiller<Dtype> > filler;
639    FillerParameter filler_param;
640    filler_param.set_value(1.);
641    filler.reset(new GaussianFiller<Dtype>(filler_param));
642    filler->Fill(this->blob_bottom_);
643    this->blob_bottom_2_->CopyFrom(*this->blob_bottom_);
644    LayerParameter layer_param;
645    ConvolutionParameter* convolution_param =
646        layer_param.mutable_convolution_param();
647    convolution_param->add_kernel_size(3);
648    convolution_param->add_stride(2);
649    convolution_param->set_num_output(1);
650    convolution_param->set_bias_term(false);
651    shared_ptr<Layer<Dtype> > layer(
652        new MKLDNNConvolutionLayer<Dtype>(layer_param));
653    layer->blobs().resize(1);
654    layer->blobs()[0].reset(new Blob<Dtype>(1, 3, 3, 3));
655    Dtype* weights = layer->blobs()[0]->mutable_cpu_data();
656    for (int c = 0; c < 3; ++c) {
657      int i = c * 9;  
658      weights[i +  0] = -1;
659      weights[i +  1] =  0;
660      weights[i +  2] =  1;
661      weights[i +  3] = -2;
662      weights[i +  4] =  0;
663      weights[i +  5] =  2;
664      weights[i +  6] = -1;
665      weights[i +  7] =  0;
666      weights[i +  8] =  1;
667    }
668    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
669    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
670    vector<Blob<Dtype>*> sep_blob_bottom_vec;
671    vector<Blob<Dtype>*> sep_blob_top_vec;
672    shared_ptr<Blob<Dtype> > blob_sep(new Blob<Dtype>());
673    sep_blob_bottom_vec.push_back(this->blob_bottom_2_);
674    sep_blob_top_vec.push_back(this->blob_top_2_);
675    convolution_param->clear_kernel_size();
676    convolution_param->clear_stride();
677    convolution_param->set_kernel_h(3);
678    convolution_param->set_kernel_w(1);
679    convolution_param->set_stride_h(2);
680    convolution_param->set_stride_w(1);
681    convolution_param->set_num_output(1);
682    convolution_param->set_bias_term(false);
683    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
684    layer->blobs().resize(1);
685    layer->blobs()[0].reset(new Blob<Dtype>(1, 3, 3, 1));
686    Dtype* weights_1 = layer->blobs()[0]->mutable_cpu_data();
687    for (int c = 0; c < 3; ++c) {
688      int i = c * 3;  
689      weights_1[i +  0] = 1;
690      weights_1[i +  1] = 2;
691      weights_1[i +  2] = 1;
692    }
693    layer->SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
694    layer->Forward(sep_blob_bottom_vec, sep_blob_top_vec);
695    blob_sep->CopyFrom(*this->blob_top_2_, false, true);
696    sep_blob_bottom_vec.clear();
697    sep_blob_bottom_vec.push_back(blob_sep.get());
698    convolution_param->set_kernel_h(1);
699    convolution_param->set_kernel_w(3);
700    convolution_param->set_stride_h(1);
701    convolution_param->set_stride_w(2);
702    convolution_param->set_num_output(1);
703    convolution_param->set_bias_term(false);
704    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
705    layer->blobs().resize(1);
706    layer->blobs()[0].reset(new Blob<Dtype>(1, 1, 1, 3));
707    Dtype* weights_2 = layer->blobs()[0]->mutable_cpu_data();
708    weights_2[0] = -1;
709    weights_2[1] =  0;
710    weights_2[2] =  1;
711    layer->SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
712    layer->Forward(sep_blob_bottom_vec, sep_blob_top_vec);
713    const Dtype* top_data = this->blob_top_->cpu_data();
714    const Dtype* sep_top_data = this->blob_top_2_->cpu_data();
715    for (int i = 0; i < this->blob_top_->count(); ++i) {
716      EXPECT_NEAR(top_data[i], sep_top_data[i], 1e-4);
717    }
718  }
719  #endif
720  #if 0
721  TYPED_TEST(MKLDNNConvolutionLayerTest, TestNDAgainst2D) {
722    typedef typename TypeParam::Dtype Dtype;
723    const int kernel_h = 11;
724    const int kernel_w = 13;
725    vector<int> bottom_shape(4);
726    bottom_shape[0] = 15;
727    bottom_shape[1] = 18;
728    bottom_shape[2] = kernel_h * 2;
729    bottom_shape[3] = kernel_w * 2;
730    FillerParameter filler_param;
731    GaussianFiller<Dtype> filler(filler_param);
732    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
733      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
734      filler.Fill(this->blob_bottom_vec_[i]);
735    }
736    LayerParameter layer_param;
737    ConvolutionParameter* convolution_param =
738        layer_param.mutable_convolution_param();
739    convolution_param->set_num_output(12);
740    convolution_param->set_bias_term(false);
741    convolution_param->set_group(6);
742    convolution_param->set_kernel_h(kernel_h);
743    convolution_param->set_kernel_w(kernel_w);
744    convolution_param->mutable_weight_filler()->set_type("gaussian");
745    Blob<Dtype> weights;
746    Blob<Dtype> top_diff;
747    bool copy_diff;
748    bool reshape;
749    {
750      MKLDNNConvolutionLayer<Dtype> layer(layer_param);
751      layer.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
752      top_diff.ReshapeLike(*this->blob_top_);
753      filler.Fill(&top_diff);
754      ASSERT_EQ(1, layer.blobs().size());
755      copy_diff = false; reshape = true;
756      weights.CopyFrom(*layer.blobs()[0], copy_diff, reshape);
757    }
758    vector<bool> propagate_down(1, true);
759    Blob<Dtype> result_2d;
760    Blob<Dtype> backward_result_2d;
761    Blob<Dtype> backward_weight_result_2d;
762    {
763      caffe_set(this->blob_top_->count(), Dtype(0),
764                this->blob_top_->mutable_cpu_data());
765      caffe_set(this->blob_bottom_->count(), Dtype(0),
766                this->blob_bottom_->mutable_cpu_diff());
767      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
768      convolution_param->set_force_nd_im2col(false);
769      MKLDNNConvolutionLayer<Dtype> layer_2d(layer_param);
770      layer_2d.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
771      ASSERT_EQ(1, layer_2d.blobs().size());
772      copy_diff = false; reshape = false;
773      layer_2d.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
774      layer_2d.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
775      copy_diff = false; reshape = true;
776      result_2d.CopyFrom(*this->blob_top_, copy_diff, reshape);
777      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
778      caffe_copy(top_diff.count(), top_diff.cpu_data(),
779                 this->blob_top_->mutable_cpu_diff());
780      layer_2d.Backward(this->blob_top_vec_, propagate_down,
781                        this->blob_bottom_vec_);
782      copy_diff = true; reshape = true;
783      backward_result_2d.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
784      backward_weight_result_2d.CopyFrom(weights, copy_diff, reshape);
785    }
786    Blob<Dtype> result_nd;
787    Blob<Dtype> backward_result_nd;
788    Blob<Dtype> backward_weight_result_nd;
789    {
790      caffe_set(this->blob_top_->count(), Dtype(0),
791                this->blob_top_->mutable_cpu_data());
792      caffe_set(this->blob_bottom_->count(), Dtype(0),
793                this->blob_bottom_->mutable_cpu_diff());
794      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
795      convolution_param->set_force_nd_im2col(true);
796      MKLDNNConvolutionLayer<Dtype> layer_nd(layer_param);
797      layer_nd.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
798      ASSERT_EQ(1, layer_nd.blobs().size());
799      copy_diff = false; reshape = false;
800      layer_nd.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
801      layer_nd.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
802      copy_diff = false; reshape = true;
803      result_nd.CopyFrom(*this->blob_top_, copy_diff, reshape);
804      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
805      caffe_copy(top_diff.count(), top_diff.cpu_data(),
806                 this->blob_top_->mutable_cpu_diff());
807      layer_nd.Backward(this->blob_top_vec_, propagate_down,
808                        this->blob_bottom_vec_);
809      copy_diff = true; reshape = true;
810      backward_result_nd.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
811      backward_weight_result_nd.CopyFrom(weights, copy_diff, reshape);
812    }
813    ASSERT_EQ(result_nd.count(), result_2d.count());
814    for (int i = 0; i < result_2d.count(); ++i)  {
815      EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
816    }
817    ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
<span onclick='openModal()' class='match'>818    for (int i = 0; i < backward_result_2d.count(); ++i) {
819      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
820                backward_result_nd.cpu_diff()[i]);
821    }
822    ASSERT_EQ(backward_weight_result_nd.count(),
823              backward_weight_result_2d.count());
824    for (int i = 0; i < backward_weight_result_2d.count(); ++i) {
825      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
826                backward_weight_result_nd.cpu_diff()[i]);
827    }
828  }
</span>829  #endif
830  TYPED_TEST(MKLDNNConvolutionLayerTest, DISABLED_TestGradient) {
831    typedef typename TypeParam::Dtype Dtype;
832    LayerParameter layer_param;
833    ConvolutionParameter* convolution_param =
834        layer_param.mutable_convolution_param();
835    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
836    this->blob_top_vec_.push_back(this->blob_top_2_);
837    convolution_param->add_kernel_size(KH);
838    convolution_param->add_stride(CS);
839    convolution_param->set_num_output(OC);
840    convolution_param->add_pad(PD);
841    convolution_param->mutable_weight_filler()->set_type("gaussian");
842    convolution_param->mutable_bias_filler()->set_type("gaussian");
843    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
844    GradientChecker<Dtype> checker(1e-2, 1e-3);
845    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
846        this->blob_top_vec_);
847  }
848  #if 0
849  TYPED_TEST(MKLDNNConvolutionLayerTest, TestDilatedGradient) {
850    typedef typename TypeParam::Dtype Dtype;
851    LayerParameter layer_param;
852    ConvolutionParameter* convolution_param =
853        layer_param.mutable_convolution_param();
854    vector<int> bottom_shape;
855    bottom_shape.push_back(2);
856    bottom_shape.push_back(3);
857    bottom_shape.push_back(5);
858    bottom_shape.push_back(6);
859    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
860      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
861    }
862    convolution_param->add_kernel_size(3);
863    convolution_param->add_dilation(2);
864    convolution_param->set_num_output(2);
865    convolution_param->mutable_weight_filler()->set_type("gaussian");
866    convolution_param->mutable_bias_filler()->set_type("gaussian");
867    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
868    GradientChecker<Dtype> checker(1e-2, 1e-3);
869    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
870                                    this->blob_top_vec_);
871  }
872  #endif
873  #if 0
874  TYPED_TEST(MKLDNNConvolutionLayerTest, TestGradient3D) {
875    typedef typename TypeParam::Dtype Dtype;
876    LayerParameter layer_param;
877    ConvolutionParameter* convolution_param =
878        layer_param.mutable_convolution_param();
879    vector<int> bottom_shape(5);
880    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
881    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
882    bottom_shape[2] = 5;
883    bottom_shape[3] = this->blob_bottom_vec_[0]->shape(2);
884    bottom_shape[4] = this->blob_bottom_vec_[0]->shape(3);
885    FillerParameter filler_param;
886    GaussianFiller<Dtype> filler(filler_param);
887    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
888      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
889      filler.Fill(this->blob_bottom_vec_[i]);
890    }
891    convolution_param->add_kernel_size(3);
892    convolution_param->add_stride(2);
893    convolution_param->set_num_output(2);
894    convolution_param->mutable_weight_filler()->set_type("gaussian");
895    convolution_param->mutable_bias_filler()->set_type("gaussian");
896    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
897    GradientChecker<Dtype> checker(1e-2, 1e-3);
898    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
899        this->blob_top_vec_);
900  }
901  TYPED_TEST(MKLDNNConvolutionLayerTest, Test1x1Gradient) {
902    typedef typename TypeParam::Dtype Dtype;
903    LayerParameter layer_param;
904    ConvolutionParameter* convolution_param =
905        layer_param.mutable_convolution_param();
906    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
907    this->blob_top_vec_.push_back(this->blob_top_2_);
908    convolution_param->add_kernel_size(1);
909    convolution_param->add_stride(1);
910    convolution_param->set_num_output(2);
911    convolution_param->mutable_weight_filler()->set_type("gaussian");
912    convolution_param->mutable_bias_filler()->set_type("gaussian");
913    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
914    GradientChecker<Dtype> checker(1e-2, 1e-3);
915    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
916        this->blob_top_vec_);
917  }
918  #endif
919  TYPED_TEST(MKLDNNConvolutionLayerTest, TestGradientGroup) {
920    typedef typename TypeParam::Dtype Dtype;
921    LayerParameter layer_param;
922    ConvolutionParameter* convolution_param =
923        layer_param.mutable_convolution_param();
924    convolution_param->add_kernel_size(3);
925    convolution_param->add_stride(2);
926    convolution_param->set_num_output(2);
927    convolution_param->set_group(GR);
928    convolution_param->mutable_weight_filler()->set_type("gaussian");
929    convolution_param->mutable_bias_filler()->set_type("gaussian");
930    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
931    GradientChecker<Dtype> checker(1e-2, 1e-3);
932    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
933        this->blob_top_vec_);
934  }
935  }  
936  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-graph_2.cpp</h3>
            <pre><code>1  bool TUNGraph::HasFlag(const TGraphFlag& Flag) const {
2    return HasGraphFlag(TUNGraph::TNet, Flag);
3  }
4  int TUNGraph::AddNode(int NId) {
5    if (NId == -1) {
6      NId = MxNId;  MxNId++;
7    } else {
8      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
9      MxNId = TMath::Mx(NId+1, MxNId());
10    }
11    NodeH.AddDat(NId, TNode(NId));
12    return NId;
13  }
14  int TUNGraph::AddNodeUnchecked(int NId) {
15    if (IsNode(NId)) { return -1;}
16    MxNId = TMath::Mx(NId+1, MxNId());
17    NodeH.AddDat(NId, TNode(NId));
18    return NId;
19  }
20  int TUNGraph::AddNode(const int& NId, const TIntV& NbrNIdV) {
21    int NewNId;
22    if (NId == -1) {
23      NewNId = MxNId;  MxNId++;
24    } else {
25      IAssertR(! IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
26      NewNId = NId;
27      MxNId = TMath::Mx(NewNId+1, MxNId());
28    }
29    TNode& Node = NodeH.AddDat(NewNId);
30    Node.Id = NewNId;
31    Node.NIdV = NbrNIdV;
32    Node.NIdV.Sort();
33    NEdges += Node.GetDeg();
34    for (int i = 0; i < NbrNIdV.Len(); i++) {
35      GetNode(NbrNIdV[i]).NIdV.AddSorted(NewNId);
36    }
37    return NewNId;
38  }
39  int TUNGraph::AddNode(const int& NId, const TVecPool<TInt>& Pool, const int& NIdVId) {
40    int NewNId;
41    if (NId == -1) {
42      NewNId = MxNId;  MxNId++;
43    } else {
44      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
45      NewNId = NId;
46      MxNId = TMath::Mx(NewNId+1, MxNId()); 
47    }
48    TNode& Node = NodeH.AddDat(NewNId);
49    Node.Id = NewNId;
50    Node.NIdV.GenExt(Pool.GetValVPt(NIdVId), Pool.GetVLen(NIdVId));
51    Node.NIdV.Sort();
52    NEdges += Node.GetDeg();
53    return NewNId;
54  }
55  void TUNGraph::DelNode(const int& NId) {
56    { AssertR(IsNode(NId), TStr::Fmt("NodeId %d does not exist", NId));
57    TNode& Node = GetNode(NId);
58    NEdges -= Node.GetDeg();
59    for (int e = 0; e < Node.GetDeg(); e++) {
60      const int nbr = Node.GetNbrNId(e);
61      if (nbr == NId) { continue; }
62      TNode& N = GetNode(nbr);
63      const int n = N.NIdV.SearchBin(NId);
64      IAssert(n != -1); 
65      if (n!= -1) { N.NIdV.Del(n); }
66    } }
67    NodeH.DelKey(NId);
68  }
69  int TUNGraph::GetEdges() const {
70    return NEdges;
71  }
72  int TUNGraph::AddEdge(const int& SrcNId, const int& DstNId) {
73    IAssertR(IsNode(SrcNId) && IsNode(DstNId), TStr::Fmt("%d or %d not a node.", SrcNId, DstNId).CStr());
74    if (IsEdge(SrcNId, DstNId)) { return -2; } 
75    GetNode(SrcNId).NIdV.AddSorted(DstNId);
76    if (SrcNId!=DstNId) { 
77      GetNode(DstNId).NIdV.AddSorted(SrcNId); }
78    NEdges++;
79    return -1; 
80  }
81  int TUNGraph::AddEdgeUnchecked(const int& SrcNId, const int& DstNId) {
82    GetNode(SrcNId).NIdV.Add(DstNId);
83    if (SrcNId!=DstNId) { 
84      GetNode(DstNId).NIdV.Add(SrcNId); }
85    NEdges++;
86    return -1; 
87  }
88  int TUNGraph::AddEdge2(const int& SrcNId, const int& DstNId) {
89    if (! IsNode(SrcNId)) { AddNode(SrcNId); }
90    if (! IsNode(DstNId)) { AddNode(DstNId); }
91    if (GetNode(SrcNId).IsNbrNId(DstNId)) { return -2; } 
92    GetNode(SrcNId).NIdV.AddSorted(DstNId);
93    if (SrcNId!=DstNId) { 
94      GetNode(DstNId).NIdV.AddSorted(SrcNId); }
95    NEdges++;
96    return -1; 
97  }
98  void TUNGraph::DelEdge(const int& SrcNId, const int& DstNId) {
99    IAssertR(IsNode(SrcNId) && IsNode(DstNId), TStr::Fmt("%d or %d not a node.", SrcNId, DstNId).CStr());
100    { TNode& N = GetNode(SrcNId);
101    const int n = N.NIdV.SearchBin(DstNId);
102    if (n!= -1) { N.NIdV.Del(n);  NEdges--; } }
103    if (SrcNId != DstNId) { 
104      TNode& N = GetNode(DstNId);
105      const int n = N.NIdV.SearchBin(SrcNId);
106      if (n!= -1) { N.NIdV.Del(n); }
107    }
108  }
109  bool TUNGraph::IsEdge(const int& SrcNId, const int& DstNId) const {
110    if (! IsNode(SrcNId) || ! IsNode(DstNId)) return false;
111    return GetNode(SrcNId).IsNbrNId(DstNId);
112  }
113  TUNGraph::TEdgeI TUNGraph::GetEI(const int& SrcNId, const int& DstNId) const {
114    const int MnNId = TMath::Mn(SrcNId, DstNId);
115    const int MxNId = TMath::Mx(SrcNId, DstNId);
116    const TNodeI SrcNI = GetNI(MnNId);
117    const int NodeN = SrcNI.NodeHI.GetDat().NIdV.SearchBin(MxNId);
118    IAssert(NodeN != -1);
119    return TEdgeI(SrcNI, EndNI(), NodeN);
120  }
121  void TUNGraph::GetNIdV(TIntV& NIdV) const {
122    NIdV.Gen(GetNodes(), 0);
123    for (int N=NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
124      NIdV.Add(NodeH.GetKey(N)); }
125  }
126  void TUNGraph::Defrag(const bool& OnlyNodeLinks) {
127    for (int n = NodeH.FFirstKeyId(); NodeH.FNextKeyId(n); ) {
128      NodeH[n].NIdV.Pack();
129    }
130    if (! OnlyNodeLinks && ! NodeH.IsKeyIdEqKeyN()) {
131      NodeH.Defrag();
132    }
133  }
134  bool TUNGraph::IsOk(const bool& ThrowExcept) const {
135    bool RetVal = true;
136    for (int N = NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
137      const TNode& Node = NodeH[N];
138      if (! Node.NIdV.IsSorted()) {
139        const TStr Msg = TStr::Fmt("Neighbor list of node %d is not sorted.", Node.GetId());
140        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); }
141        RetVal=false;
142      }
143      int prevNId = -1;
144      for (int e = 0; e < Node.GetDeg(); e++) {
145        if (! IsNode(Node.GetNbrNId(e))) {
146          const TStr Msg = TStr::Fmt("Edge %d --> %d: node %d does not exist.",
147            Node.GetId(), Node.GetNbrNId(e), Node.GetNbrNId(e));
148          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); }
149          RetVal=false;
150        }
151        if (e > 0 && prevNId == Node.GetNbrNId(e)) {
152          const TStr Msg = TStr::Fmt("Node %d has duplicate edge %d --> %d.",
153            Node.GetId(), Node.GetId(), Node.GetNbrNId(e));
154          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); }
155          RetVal=false;
156        }
157        prevNId = Node.GetNbrNId(e);
158      }
159    }
160    int EdgeCnt = 0;
161    for (TEdgeI EI = BegEI(); EI < EndEI(); EI++) { EdgeCnt++; }
162    if (EdgeCnt != GetEdges()) {
163      const TStr Msg = TStr::Fmt("Number of edges counter is corrupted: GetEdges():%d, EdgeCount:%d.", GetEdges(), EdgeCnt);
164      if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); }
165      RetVal=false;
166    }
167    return RetVal;
168  }
169  void TUNGraph::Dump(FILE *OutF) const {
170    const int NodePlaces = (int) ceil(log10((double) GetNodes()));
171    fprintf(OutF, "-------------------------------------------------\nUndirected Node Graph: nodes: %d, edges: %d\n", GetNodes(), GetEdges());
172    for (int N = NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
173      const TNode& Node = NodeH[N];
174      fprintf(OutF, "  %*d [%d] ", NodePlaces, Node.GetId(), Node.GetDeg());
175      for (int edge = 0; edge < Node.GetDeg(); edge++) {
176        fprintf(OutF, " %*d", NodePlaces, Node.GetNbrNId(edge)); }
177      fprintf(OutF, "\n");
178    }
179    fprintf(OutF, "\n");
180  }
181  PUNGraph TUNGraph::GetSmallGraph() {
182    PUNGraph Graph = TUNGraph::New();
183    for (int i = 0; i < 5; i++) { Graph->AddNode(i); }
184    Graph->AddEdge(0,1);  Graph->AddEdge(0,2);
185    Graph->AddEdge(0,3);  Graph->AddEdge(0,4);
186    Graph->AddEdge(1,2);
187    return Graph;
188  }
189  bool TNGraph::HasFlag(const TGraphFlag& Flag) const {
190    return HasGraphFlag(TNGraph::TNet, Flag);
191  }
192  int TNGraph::AddNode(int NId) {
193    if (NId == -1) {
194      NId = MxNId;  MxNId++;
195    } else {
196      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
197      MxNId = TMath::Mx(NId+1, MxNId());
198    }
199    NodeH.AddDat(NId, TNode(NId));
200    return NId;
201  }
202  int TNGraph::AddNodeUnchecked(int NId) {
203    if (IsNode(NId)) { return NId;}
204    MxNId = TMath::Mx(NId+1, MxNId());
205    NodeH.AddDat(NId, TNode(NId));
206    return NId;
207  }
208  int TNGraph::AddNode(const int& NId, const TIntV& InNIdV, const TIntV& OutNIdV) {
209    int NewNId;
210    if (NId == -1) {
211      NewNId = MxNId;  MxNId++;
212    } else {
213      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
214      NewNId = NId;
215      MxNId = TMath::Mx(NewNId+1, MxNId());
216    }
217    TNode& Node = NodeH.AddDat(NewNId);
218    Node.Id = NewNId;
219    Node.InNIdV = InNIdV;
220    Node.OutNIdV = OutNIdV;
221    Node.InNIdV.Sort();
222    Node.OutNIdV.Sort();
223    return NewNId;
224  }
225  int TNGraph::AddNode(const int& NId, const TVecPool<TInt>& Pool, const int& SrcVId, const int& DstVId) {
226    int NewNId;
227    if (NId == -1) {
228      NewNId = MxNId;  MxNId++;
229    } else {
230      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
231      NewNId = NId;
232      MxNId = TMath::Mx(NewNId+1, MxNId());
233    }
234    TNode& Node = NodeH.AddDat(NewNId);
235    Node.Id = NewNId;
236    Node.InNIdV.GenExt(Pool.GetValVPt(SrcVId), Pool.GetVLen(SrcVId));
237    Node.OutNIdV.GenExt(Pool.GetValVPt(DstVId), Pool.GetVLen(DstVId));
238    Node.InNIdV.Sort();
239    Node.OutNIdV.Sort();
240    return NewNId;
241  }
242  void TNGraph::DelNode(const int& NId) {
243    { TNode& Node = GetNode(NId);
244    for (int e = 0; e < Node.GetOutDeg(); e++) {
245    const int nbr = Node.GetOutNId(e);
246    if (nbr == NId) { continue; }
247      TNode& N = GetNode(nbr);
248      const int n = N.InNIdV.SearchBin(NId);
249      if (n!= -1) { N.InNIdV.Del(n); }
250    }
251    for (int e = 0; e < Node.GetInDeg(); e++) {
252    const int nbr = Node.GetInNId(e);
253    if (nbr == NId) { continue; }
254      TNode& N = GetNode(nbr);
255      const int n = N.OutNIdV.SearchBin(NId);
256      if (n!= -1) { N.OutNIdV.Del(n); }
257    } }
258    NodeH.DelKey(NId);
259  }
260  int TNGraph::GetEdges() const {
261    int edges=0;
262    for (int N=NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
263      edges+=NodeH[N].GetOutDeg();
264    }
265    return edges;
266  }
267  int TNGraph::AddEdge(const int& SrcNId, const int& DstNId) {
268    IAssertR(IsNode(SrcNId) && IsNode(DstNId), TStr::Fmt("%d or %d not a node.", SrcNId, DstNId).CStr());
269    if (IsEdge(SrcNId, DstNId)) { return -2; }
270    GetNode(SrcNId).OutNIdV.AddSorted(DstNId);
271    GetNode(DstNId).InNIdV.AddSorted(SrcNId);
272    return -1; 
273  }
274  int TNGraph::AddEdgeUnchecked(const int& SrcNId, const int& DstNId) {
275    GetNode(SrcNId).OutNIdV.Add(DstNId);
276    GetNode(DstNId).InNIdV.Add(SrcNId);
277    return -1; 
278  }
279  int TNGraph::AddEdge2(const int& SrcNId, const int& DstNId) {
280    if (! IsNode(SrcNId)) { AddNode(SrcNId); }
281    if (! IsNode(DstNId)) { AddNode(DstNId); }
282    if (GetNode(SrcNId).IsOutNId(DstNId)) { return -2; } 
283    GetNode(SrcNId).OutNIdV.AddSorted(DstNId);
284    GetNode(DstNId).InNIdV.AddSorted(SrcNId);
285    return -1; 
286  }
287  void TNGraph::DelEdge(const int& SrcNId, const int& DstNId, const bool& IsDir) {
288    IAssertR(IsNode(SrcNId) && IsNode(DstNId), TStr::Fmt("%d or %d not a node.", SrcNId, DstNId).CStr());
289    { TNode& N = GetNode(SrcNId);
290    const int n = N.OutNIdV.SearchBin(DstNId);
291    if (n!= -1) { N.OutNIdV.Del(n); } }
292    { TNode& N = GetNode(DstNId);
293    const int n = N.InNIdV.SearchBin(SrcNId);
294    if (n!= -1) { N.InNIdV.Del(n); } }
295    if (! IsDir) {
296      { TNode& N = GetNode(SrcNId);
297      const int n = N.InNIdV.SearchBin(DstNId);
298      if (n!= -1) { N.InNIdV.Del(n); } }
299      { TNode& N = GetNode(DstNId);
300      const int n = N.OutNIdV.SearchBin(SrcNId);
301      if (n!= -1) { N.OutNIdV.Del(n); } }
302    }
303  }
304  bool TNGraph::IsEdge(const int& SrcNId, const int& DstNId, const bool& IsDir) const {
305    if (! IsNode(SrcNId) || ! IsNode(DstNId)) { return false; }
306    if (IsDir) { return GetNode(SrcNId).IsOutNId(DstNId); }
307    else { return GetNode(SrcNId).IsOutNId(DstNId) || GetNode(DstNId).IsOutNId(SrcNId); }
308  }
309  TNGraph::TEdgeI TNGraph::GetEI(const int& SrcNId, const int& DstNId) const {
310    const TNodeI SrcNI = GetNI(SrcNId);
311    const int NodeN = SrcNI.NodeHI.GetDat().OutNIdV.SearchBin(DstNId);
312    IAssert(NodeN != -1);
313    return TEdgeI(SrcNI, EndNI(), NodeN);
314  }
315  void TNGraph::GetNIdV(TIntV& NIdV) const {
316    NIdV.Gen(GetNodes(), 0);
317    for (int N=NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
318      NIdV.Add(NodeH.GetKey(N)); }
319  }
320  void TNGraph::Defrag(const bool& OnlyNodeLinks) {
321    for (int n = NodeH.FFirstKeyId(); NodeH.FNextKeyId(n); ) {
322      TNode& Node = NodeH[n];
323      Node.InNIdV.Pack();  Node.OutNIdV.Pack();
324    }
325    if (! OnlyNodeLinks && ! NodeH.IsKeyIdEqKeyN()) { NodeH.Defrag(); }
326  }
327  bool TNGraph::IsOk(const bool& ThrowExcept) const {
328    bool RetVal = true;
329    for (int N = NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
330      const TNode& Node = NodeH[N];
331      if (! Node.OutNIdV.IsSorted()) {
332        const TStr Msg = TStr::Fmt("Out-neighbor list of node %d is not sorted.", Node.GetId());
333        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
334      }
335      if (! Node.InNIdV.IsSorted()) {
336        const TStr Msg = TStr::Fmt("In-neighbor list of node %d is not sorted.", Node.GetId());
337        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
338      }
339      int prevNId = -1;
340      for (int e = 0; e < Node.GetOutDeg(); e++) {
341        if (! IsNode(Node.GetOutNId(e))) {
342          const TStr Msg = TStr::Fmt("Out-edge %d --> %d: node %d does not exist.",
343            Node.GetId(), Node.GetOutNId(e), Node.GetOutNId(e));
344          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
345        }
346        if (e > 0 && prevNId == Node.GetOutNId(e)) {
347          const TStr Msg = TStr::Fmt("Node %d has duplidate out-edge %d --> %d.",
348            Node.GetId(), Node.GetId(), Node.GetOutNId(e));
349          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
350        }
351        prevNId = Node.GetOutNId(e);
352      }
353      prevNId = -1;
354      for (int e = 0; e < Node.GetInDeg(); e++) {
355        if (! IsNode(Node.GetInNId(e))) {
356          const TStr Msg = TStr::Fmt("In-edge %d <-- %d: node %d does not exist.",
357            Node.GetId(), Node.GetInNId(e), Node.GetInNId(e));
358          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
359        }
360        if (e > 0 && prevNId == Node.GetInNId(e)) {
361          const TStr Msg = TStr::Fmt("Node %d has duplidate in-edge %d <-- %d.",
362            Node.GetId(), Node.GetId(), Node.GetInNId(e));
363          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
364        }
365        prevNId = Node.GetInNId(e);
366      }
367    }
368    return RetVal;
369  }
370  void TNGraph::Dump(FILE *OutF) const {
371    const int NodePlaces = (int) ceil(log10((double) GetNodes()));
372    fprintf(OutF, "-------------------------------------------------\nDirected Node Graph: nodes: %d, edges: %d\n", GetNodes(), GetEdges());
373    for (int N = NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
374      const TNode& Node = NodeH[N];
375      fprintf(OutF, "  %*d]\n", NodePlaces, Node.GetId());
376      fprintf(OutF, "    in [%d]", Node.GetInDeg());
<span onclick='openModal()' class='match'>377      for (int edge = 0; edge < Node.GetInDeg(); edge++) {
378        fprintf(OutF, " %*d", NodePlaces, Node.GetInNId(edge)); }
379      fprintf(OutF, "\n    out[%d]", Node.GetOutDeg());
380      for (int edge = 0; edge < Node.GetOutDeg(); edge++) {
381        fprintf(OutF, " %*d", NodePlaces, Node.GetOutNId(edge)); }
382      fprintf(OutF, "\n");
383    }
</span>384    fprintf(OutF, "\n");
385  }
386  PNGraph TNGraph::GetSmallGraph() {
387    PNGraph G = TNGraph::New();
388    for (int i = 0; i < 5; i++) { G->AddNode(i); }
389    G->AddEdge(0,1); G->AddEdge(1,2); G->AddEdge(0,2);
390    G->AddEdge(1,3); G->AddEdge(3,4); G->AddEdge(2,3);
391    return G;
392  }
393  bool TNEGraph::HasFlag(const TGraphFlag& Flag) const {
394    return HasGraphFlag(TNEGraph::TNet, Flag);
395  }
396  bool TNEGraph::TNodeI::IsInNId(const int& NId) const {
397    const TNode& Node = NodeHI.GetDat();
398    for (int edge = 0; edge < Node.GetInDeg(); edge++) {
399      if (NId == Graph->GetEdge(Node.GetInEId(edge)).GetSrcNId())
400        return true;
401    }
402    return false;
403  }
404  bool TNEGraph::TNodeI::IsOutNId(const int& NId) const {
405    const TNode& Node = NodeHI.GetDat();
406    for (int edge = 0; edge < Node.GetOutDeg(); edge++) {
407      if (NId == Graph->GetEdge(Node.GetOutEId(edge)).GetDstNId())
408        return true;
409    }
410    return false;
411  }
412  int TNEGraph::AddNode(int NId) {
413    if (NId == -1) {
414      NId = MxNId;  MxNId++;
415    } else {
416      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
417      MxNId = TMath::Mx(NId+1, MxNId());
418    }
419    NodeH.AddDat(NId, TNode(NId));
420    return NId;
421  }
422  void TNEGraph::DelNode(const int& NId) {
423    const TNode& Node = GetNode(NId);
424    for (int out = 0; out < Node.GetOutDeg(); out++) {
425      const int EId = Node.GetOutEId(out);
426      const TEdge& Edge = GetEdge(EId);
427      IAssert(Edge.GetSrcNId() == NId);
428      GetNode(Edge.GetDstNId()).InEIdV.DelIfIn(EId);
429      EdgeH.DelKey(EId);
430    }
431    for (int in = 0; in < Node.GetInDeg(); in++) {
432      const int EId = Node.GetInEId(in);
433      const TEdge& Edge = GetEdge(EId);
434      IAssert(Edge.GetDstNId() == NId);
435      GetNode(Edge.GetSrcNId()).OutEIdV.DelIfIn(EId);
436      EdgeH.DelKey(EId);
437    }
438    NodeH.DelKey(NId);
439  }
440  int TNEGraph::AddEdge(const int& SrcNId, const int& DstNId, int EId) {
441    if (EId == -1) { EId = MxEId;  MxEId++; }
442    else { MxEId = TMath::Mx(EId+1, MxEId()); }
443    IAssertR(!IsEdge(EId), TStr::Fmt("EdgeId %d already exists", EId));
444    IAssertR(IsNode(SrcNId) && IsNode(DstNId), TStr::Fmt("%d or %d not a node.", SrcNId, DstNId).CStr());
445    EdgeH.AddDat(EId, TEdge(EId, SrcNId, DstNId));
446    GetNode(SrcNId).OutEIdV.AddSorted(EId);
447    GetNode(DstNId).InEIdV.AddSorted(EId);
448    return EId;
449  }
450  void TNEGraph::DelEdge(const int& EId) {
451    IAssert(IsEdge(EId));
452    const int SrcNId = GetEdge(EId).GetSrcNId();
453    const int DstNId = GetEdge(EId).GetDstNId();
454    GetNode(SrcNId).OutEIdV.DelIfIn(EId);
455    GetNode(DstNId).InEIdV.DelIfIn(EId);
456    EdgeH.DelKey(EId);
457  }
458  void TNEGraph::DelEdge(const int& SrcNId, const int& DstNId, const bool& IsDir) {
459    int EId;
460    IAssert(IsEdge(SrcNId, DstNId, EId, IsDir)); 
461    while (IsEdge(SrcNId, DstNId, EId, IsDir)) {
462      GetNode(SrcNId).OutEIdV.DelIfIn(EId);
463      GetNode(DstNId).InEIdV.DelIfIn(EId);
464    }
465    EdgeH.DelKey(EId);
466  }
467  bool TNEGraph::IsEdge(const int& SrcNId, const int& DstNId, int& EId, const bool& IsDir) const {
468    const TNode& SrcNode = GetNode(SrcNId);
469    for (int edge = 0; edge < SrcNode.GetOutDeg(); edge++) {
470      const TEdge& Edge = GetEdge(SrcNode.GetOutEId(edge));
471      if (DstNId == Edge.GetDstNId()) {
472        EId = Edge.GetId();  return true; }
473    }
474    if (! IsDir) {
475      for (int edge = 0; edge < SrcNode.GetInDeg(); edge++) {
476      const TEdge& Edge = GetEdge(SrcNode.GetInEId(edge));
477      if (DstNId == Edge.GetSrcNId()) {
478        EId = Edge.GetId();  return true; }
479      }
480    }
481    return false;
482  }
483  void TNEGraph::GetNIdV(TIntV& NIdV) const {
484    NIdV.Gen(GetNodes(), 0);
485    for (int N=NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
486      NIdV.Add(NodeH.GetKey(N)); }
487  }
488  void TNEGraph::GetEIdV(TIntV& EIdV) const {
489    EIdV.Gen(GetEdges(), 0);
490    for (int E=EdgeH.FFirstKeyId(); EdgeH.FNextKeyId(E); ) {
491      EIdV.Add(EdgeH.GetKey(E));
492    }
493  }
494  void TNEGraph::Defrag(const bool& OnlyNodeLinks) {
495    for (int kid = NodeH.FFirstKeyId(); NodeH.FNextKeyId(kid); ) {
496      TNode& Node = NodeH[kid];
497      Node.InEIdV.Pack();  Node.OutEIdV.Pack();
498    }
499    if (! OnlyNodeLinks && ! NodeH.IsKeyIdEqKeyN()) { NodeH.Defrag(); }
500    if (! OnlyNodeLinks && ! EdgeH.IsKeyIdEqKeyN()) { EdgeH.Defrag(); }
501  }
502  bool TNEGraph::IsOk(const bool& ThrowExcept) const {
503  bool RetVal = true;
504    for (int N = NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
505      const TNode& Node = NodeH[N];
506      if (! Node.OutEIdV.IsSorted()) {
507        const TStr Msg = TStr::Fmt("Out-edge list of node %d is not sorted.", Node.GetId());
508        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
509      }
510      if (! Node.InEIdV.IsSorted()) {
511        const TStr Msg = TStr::Fmt("In-edge list of node %d is not sorted.", Node.GetId());
512        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
513      }
514      int prevEId = -1;
515      for (int e = 0; e < Node.GetOutDeg(); e++) {
516        if (! IsEdge(Node.GetOutEId(e))) {
517          const TStr Msg = TStr::Fmt("Out-edge id %d of node %d does not exist.",  Node.GetOutEId(e), Node.GetId());
518          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
519        }
520        if (e > 0 && prevEId == Node.GetOutEId(e)) {
521          const TStr Msg = TStr::Fmt("Node %d has duplidate out-edge id %d.", Node.GetId(), Node.GetOutEId(e));
522          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
523        }
524        prevEId = Node.GetOutEId(e);
525      }
526      prevEId = -1;
527      for (int e = 0; e < Node.GetInDeg(); e++) {
528        if (! IsEdge(Node.GetInEId(e))) {
529          const TStr Msg = TStr::Fmt("Out-edge id %d of node %d does not exist.",  Node.GetInEId(e), Node.GetId());
530          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
531        }
532        if (e > 0 && prevEId == Node.GetInEId(e)) {
533          const TStr Msg = TStr::Fmt("Node %d has duplidate out-edge id %d.", Node.GetId(), Node.GetInEId(e));
534          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
535        }
536        prevEId = Node.GetInEId(e);
537      }
538    }
539    for (int E = EdgeH.FFirstKeyId(); EdgeH.FNextKeyId(E); ) {
540      const TEdge& Edge = EdgeH[E];
541      if (! IsNode(Edge.GetSrcNId())) {
542        const TStr Msg = TStr::Fmt("Edge %d source node %d does not exist.", Edge.GetId(), Edge.GetSrcNId());
543        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
544      }
545      if (! IsNode(Edge.GetDstNId())) {
546        const TStr Msg = TStr::Fmt("Edge %d destination node %d does not exist.", Edge.GetId(), Edge.GetDstNId());
547        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false;
548      }
549    }
550    return RetVal;
551  }
552  void TNEGraph::Dump(FILE *OutF) const {
553    const int NodePlaces = (int) ceil(log10((double) GetNodes()));
554    const int EdgePlaces = (int) ceil(log10((double) GetEdges()));
555    fprintf(OutF, "-------------------------------------------------\nDirected Node-Edge Graph: nodes: %d, edges: %d\n", GetNodes(), GetEdges());
556    for (TNodeI NodeI = BegNI(); NodeI < EndNI(); NodeI++) {
557      fprintf(OutF, "  %*d]\n", NodePlaces, NodeI.GetId());
558      fprintf(OutF, "    in[%d]", NodeI.GetInDeg());
559      for (int edge = 0; edge < NodeI.GetInDeg(); edge++) {
560        fprintf(OutF, " %*d", EdgePlaces, NodeI.GetInEId(edge)); }
561      fprintf(OutF, "\n    out[%d]", NodeI.GetOutDeg());
562      for (int edge = 0; edge < NodeI.GetOutDeg(); edge++) {
563        fprintf(OutF, " %*d", EdgePlaces, NodeI.GetOutEId(edge)); }
564      fprintf(OutF, "\n");
565    }
566    for (TEdgeI EdgeI = BegEI(); EdgeI < EndEI(); EdgeI++) {
567      fprintf(OutF, "  %*d]  %*d  ->  %*d\n", EdgePlaces, EdgeI.GetId(), NodePlaces, EdgeI.GetSrcNId(), NodePlaces, EdgeI.GetDstNId());
568    }
569    fprintf(OutF, "\n");
570  }
571  PNEGraph TNEGraph::GetSmallGraph() {
572    PNEGraph Graph = TNEGraph::New();
573    for (int i = 0; i < 5; i++) { Graph->AddNode(i); }
574    Graph->AddEdge(0,1);  Graph->AddEdge(0,2);
575    Graph->AddEdge(0,3);  Graph->AddEdge(0,4);
576    Graph->AddEdge(1,2);  Graph->AddEdge(1,2);
577    return Graph;
578  }
579  int TBPGraph::AddNode(int NId, const bool& LeftNode) {
580    if (NId == -1) { NId = MxNId;  MxNId++; }
581    else if (IsLNode(NId)) { IAssertR(LeftNode, TStr::Fmt("Node with id %s already exists on the 'left'.", NId));  return NId; }
582    else if (IsRNode(NId)) { IAssertR(! LeftNode, TStr::Fmt("Node with id %s already exists on the 'right'.", NId));  return NId; }
583    else { MxNId = TMath::Mx(NId+1, MxNId()); }
584    if (LeftNode) { LeftH.AddDat(NId, TNode(NId)); }
585    else { RightH.AddDat(NId, TNode(NId)); }
586    return NId;
587  }
588  void TBPGraph::DelNode(const int& NId) {
589    AssertR(IsNode(NId), TStr::Fmt("NodeId %d does not exist", NId));
590    THash<TInt, TNode>& SrcH = IsLNode(NId) ? LeftH : RightH;
591    THash<TInt, TNode>& DstH = IsLNode(NId) ? RightH : LeftH;
592    { TNode& Node = SrcH.GetDat(NId);
593    for (int e = 0; e < Node.GetOutDeg(); e++) {
594      const int nbr = Node.GetOutNId(e);
595      IAssertR(nbr != NId, "Bipartite graph has a loop!");
596      TNode& N = DstH.GetDat(nbr);
597      const int n = N.NIdV.SearchBin(NId);
598      IAssert(n!= -1); 
599      N.NIdV.Del(n);
600    } }
601    SrcH.DelKey(NId);
602  }
603  int TBPGraph::GetEdges() const {
604    int Edges = 0;
605    for (int N=LeftH.FFirstKeyId(); LeftH.FNextKeyId(N); ) {
606      Edges += LeftH[N].GetDeg(); }
607    return Edges;
608  }
609  int TBPGraph::AddEdge(const int& LeftNId, const int& RightNId) {
610    const bool IsLL = IsLNode(LeftNId), IsLR = IsRNode(LeftNId);
611    const bool IsRL = IsLNode(RightNId), IsRR = IsRNode(RightNId);
612    IAssertR((IsLL||IsLR)&&(IsRL||IsRR), TStr::Fmt("%d or %d is not a node.", LeftNId, RightNId).CStr());
613    IAssertR(LeftNId!=RightNId, "No self-edges are allowed."); 
614    IAssertR((IsLL&&!IsLR&&!IsRL&&IsRR)||(!IsLL&&IsLR&&IsRL&&!IsRR), "One node should be on the 'left' and the other on the 'right'.");
615    const int LNId = IsLL ? LeftNId : RightNId; 
616    const int RNId = IsLL ? RightNId : LeftNId; 
617    if (LeftH.GetDat(LNId).IsOutNId(RNId)) { return -2; } 
618    LeftH.GetDat(LNId).NIdV.AddSorted(RNId);
619    RightH.GetDat(RNId).NIdV.AddSorted(LNId);
620    return -1; 
621  }
622  void TBPGraph::DelEdge(const int& LeftNId, const int& RightNId) {
623    const bool IsLL = IsLNode(LeftNId), IsLR = IsRNode(LeftNId);
624    const bool IsRL = IsLNode(RightNId), IsRR = IsRNode(RightNId);
625    IAssertR((IsLL||IsLR)&&(IsRL||IsRR), TStr::Fmt("%d or %d is not a node.", LeftNId, RightNId).CStr());
626    IAssertR(LeftNId!=RightNId, "No self-edges are allowed."); 
627    IAssertR((IsLL&&!IsLR&&!IsRL&&IsRR)||(!IsLL&&IsLR&&IsRL&&!IsRR), "One node should be on the 'left' and the other on the 'right'.");
628    const int LNId = IsLL ? LeftNId : RightNId; 
629    const int RNId = IsLL ? RightNId : LeftNId; 
630    { TIntV& NIdV = LeftH.GetDat(LNId).NIdV;
631    const int n = NIdV.SearchBin(RNId);
632    if (n != -1) { NIdV.Del(n); } }
633    { TIntV& NIdV = RightH.GetDat(RNId).NIdV;
634    const int n = NIdV.SearchBin(LNId);
635    if (n != -1) { NIdV.Del(n); } }
636  }
637  bool TBPGraph::IsEdge(const int& LeftNId, const int& RightNId) const {
638    if (! IsNode(LeftNId) || ! IsNode(RightNId)) { return false; }
639    return IsLNode(LeftNId) ? LeftH.GetDat(LeftNId).IsOutNId(RightNId) : RightH.GetDat(LeftNId).IsOutNId(RightNId);
640  }
641  TBPGraph::TEdgeI TBPGraph::GetEI(const int& LeftNId, const int& RightNId) const {
642    const bool IsLL = IsLNode(LeftNId), IsLR = IsRNode(LeftNId);
643    const bool IsRL = IsLNode(RightNId), IsRR = IsRNode(RightNId);
644    IAssertR((IsLL||IsLR)&&(IsRL||IsRR), TStr::Fmt("%d or %d is not a node.", LeftNId, RightNId).CStr());
645    IAssertR(LeftNId!=RightNId, "No self-edges are allowed."); 
646    IAssertR((IsLL&&!IsLR&&!IsRL&&IsRR)||(!IsLL&&IsLR&&IsRL&&!IsRR), "One node should be on the 'left' and the other on the 'right'.");
647    const int LNId = IsLL ? LeftNId : RightNId; 
648    const int RNId = IsLL ? RightNId : LeftNId; 
649    const TNodeI SrcNI = GetNI(LNId);
650    const int NodeN = SrcNI.LeftHI.GetDat().NIdV.SearchBin(RNId);
651    IAssertR(NodeN != -1, "Right edge endpoint does not exists!");
652    return TEdgeI(SrcNI, EndNI(), NodeN);
653  }
654  int TBPGraph::GetRndNId(TRnd& Rnd) { 
655    const int NNodes = GetNodes();
656    if (Rnd.GetUniDevInt(NNodes) < GetLNodes()) {
657      return GetRndLNId(Rnd); }
658    else {
659      return GetRndRNId(Rnd); }
660  }
661  int TBPGraph::GetRndLNId(TRnd& Rnd) { 
662    return LeftH.GetKey(LeftH.GetRndKeyId(Rnd, 0.8)); 
663  }
664  int TBPGraph::GetRndRNId(TRnd& Rnd) { 
665    return RightH.GetKey(RightH.GetRndKeyId(Rnd, 0.8)); 
666  }
667  void TBPGraph::GetNIdV(TIntV& NIdV) const {
668    NIdV.Gen(GetNodes(), 0);
669    for (int N=LeftH.FFirstKeyId(); LeftH.FNextKeyId(N); ) {
670      NIdV.Add(LeftH.GetKey(N)); }
671    for (int N=RightH.FFirstKeyId(); RightH.FNextKeyId(N); ) {
672      NIdV.Add(RightH.GetKey(N)); }
673  }
674  void TBPGraph::GetLNIdV(TIntV& NIdV) const {
675    NIdV.Gen(GetLNodes(), 0);
676    for (int N=LeftH.FFirstKeyId(); LeftH.FNextKeyId(N); ) {
677      NIdV.Add(LeftH.GetKey(N)); }
678  }
679  void TBPGraph::GetRNIdV(TIntV& NIdV) const {
680    NIdV.Gen(GetRNodes(), 0);
681    for (int N=RightH.FFirstKeyId(); RightH.FNextKeyId(N); ) {
682      NIdV.Add(RightH.GetKey(N)); }
683  }
684  void TBPGraph::Reserve(const int& Nodes, const int& Edges) { 
685    if (Nodes>0) { LeftH.Gen(Nodes/2); RightH.Gen(Nodes/2); } 
686  }
687  void TBPGraph::Defrag(const bool& OnlyNodeLinks) {
688    for (int n = LeftH.FFirstKeyId(); LeftH.FNextKeyId(n); ) {
689      LeftH[n].NIdV.Pack(); }
690    for (int n = RightH.FFirstKeyId(); RightH.FNextKeyId(n); ) {
691      RightH[n].NIdV.Pack(); }
692    if (! OnlyNodeLinks && ! LeftH.IsKeyIdEqKeyN()) { LeftH.Defrag(); }
693    if (! OnlyNodeLinks && ! RightH.IsKeyIdEqKeyN()) { RightH.Defrag(); }
694  }
695  bool TBPGraph::IsOk(const bool& ThrowExcept) const {
696    bool RetVal = false;
697    for (int n = LeftH.FFirstKeyId(); LeftH.FNextKeyId(n); ) {
698      if (! LeftH[n].NIdV.IsSorted()) {
699        const TStr Msg = TStr::Fmt("Neighbor list of node %d is not sorted.", LeftH[n].GetId());
700        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false; }
701    }
702    for (int n = RightH.FFirstKeyId(); RightH.FNextKeyId(n); ) {
703      if (! RightH[n].NIdV.IsSorted()) {
704        const TStr Msg = TStr::Fmt("Neighbor list of node %d is not sorted.", RightH[n].GetId());
705        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false; }
706    }
707    for (int n = LeftH.FFirstKeyId(); LeftH.FNextKeyId(n); ) {
708      if (RightH.IsKey(LeftH[n].GetId())) {
709        const TStr Msg = TStr::Fmt("'Left' node %d also appears on the 'right'.", LeftH[n].GetId());
710        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false; }
711    } 
712    for (int n = RightH.FFirstKeyId(); RightH.FNextKeyId(n); ) {
713      if (LeftH.IsKey(RightH[n].GetId())) {
714        const TStr Msg = TStr::Fmt("'Right' node %d also appears on the 'left'.", RightH[n].GetId());
715        if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false; }
716    }
717    for (int n = LeftH.FFirstKeyId(); LeftH.FNextKeyId(n); ) {
718      for (int e = 0; e < LeftH[n].NIdV.Len(); e++) {
719        if (! RightH.IsKey(LeftH[n].NIdV[e]) || ! RightH.GetDat(LeftH[n].NIdV[e]).NIdV.IsIn(LeftH[n].GetId())) {
720          const TStr Msg = TStr::Fmt("'Left' node %d does not point to the 'right' node %d.", LeftH[n].GetId(), LeftH[n].NIdV[e]());
721          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false; }
722      }
723    }
724    for (int n = RightH.FFirstKeyId(); RightH.FNextKeyId(n); ) {
725      for (int e = 0; e < RightH[n].NIdV.Len(); e++) {
726        if (! LeftH.IsKey(RightH[n].NIdV[e]) || ! LeftH.GetDat(RightH[n].NIdV[e]).NIdV.IsIn(RightH[n].GetId())) {
727          const TStr Msg = TStr::Fmt("'Left' node %d does not point to the 'right' node %d.", RightH[n].GetId(), RightH[n].NIdV[e]());
728          if (ThrowExcept) { EAssertR(false, Msg); } else { ErrNotify(Msg.CStr()); } RetVal=false; }
729      }
730    }
731    return RetVal;
732  }
733  void TBPGraph::Dump(FILE *OutF) const {
734    const int NodePlaces = (int) ceil(log10((double) GetNodes()));
735    fprintf(OutF, "-------------------------------------------------\nBipartite Graph: nodes: %d+%d=%d, edges: %d\n", GetLNodes(), GetRNodes(), GetNodes(), GetEdges());
736    for (int N = LeftH.FFirstKeyId(); LeftH.FNextKeyId(N); ) {
737      const TNode& Node = LeftH[N];
738      fprintf(OutF, "  %*d [%d] ", NodePlaces, Node.GetId(), Node.GetDeg());
739      for (int edge = 0; edge < Node.GetDeg(); edge++) {
740        fprintf(OutF, " %*d", NodePlaces, Node.GetNbrNId(edge)); }
741      fprintf(OutF, "\n");
742    }
743    fprintf(OutF, "\n");
744  }
745  PBPGraph TBPGraph::GetSmallGraph() {
746    PBPGraph BP = TBPGraph::New();
747    BP->AddNode(0, true);
748    BP->AddNode(1, true);
749    BP->AddNode(2, false);
750    BP->AddNode(3, false);
751    BP->AddNode(4, false);
752    BP->AddEdge(0, 2);
753    BP->AddEdge(0, 3);
754    BP->AddEdge(1, 2);
755    BP->AddEdge(1, 3);
756    BP->AddEdge(1, 4);
757    return BP;
758  }
759  bool THGraph::HasFlag(const TGraphFlag& Flag) const {
760    return HasGraphFlag(THGraph::TNet, Flag);
761  }
762  int THGraph::AddNode(int NId, TStr NName) {
763    if (NId == -1) {
764      NId = MxNId;  MxNId++;
765    } else {
766      IAssertR(!IsNode(NId), TStr::Fmt("NodeId %d already exists", NId));
767      MxNId = TMath::Mx(NId+1, MxNId());
768    }
769    NodeH.AddDat(NId, TNode(NId, NName));
770    return NId;
771  }
772  int THGraph::AddNodeUnchecked(int NId, TStr NName) {
773    if (IsNode(NId)) { return -1;}
774    MxNId = TMath::Mx(NId+1, MxNId());
775    NodeH.AddDat(NId, TNode(NId, NName));
776    return NId;
777  }
778  void THGraph::DelNode(const int& NId) {
779    AssertR(IsNode(NId), TStr::Fmt("NodeId %d does not exist", NId));
780    TNode& Node = GetNode(NId);
781    int EId = Node.EIdV.GetVal(0);
782    int NumNei;
783    TIntSet& ENIdsHS = GetEdge(EId).NeiNIdSH;
784    for (int e = 0; e < Node.GetDeg(); e++) {
785      EId = Node.EIdV.GetVal(e);
786      ENIdsHS = GetEdge(EId).NeiNIdSH;
787      int iKey;
788      for (int i=0; i < ENIdsHS.Len(); i++) {
789        iKey = ENIdsHS.GetKey(i);
790        if (iKey == NId) { continue; }
791        GetNode(iKey).DelNeighbor(NId);
792      }
793      NumNei = ENIdsHS.Len();
794      N2Edges = N2Edges - (NumNei*(NumNei-1))/2 + ((NumNei-1)*(NumNei-2))/2;
795      if (ENIdsHS.Len() > 2) {
796        GetEdge(EId).NeiNIdSH.DelKey(NId);
797      } else { DelEdge(EId); }
798      NodeH.DelKey(NId);
799    }
800    delete &Node;
801  }
802  void THGraph::TNode::UpdEInfo(const int& EId, const TIntSet& ENodesHS){
803    if (! EIdV.IsIn(EId)) {
804      EIdV.Add(EId);
805      for (int j = 0; j < ENodesHS.Len(); j++) {
806        if (Id == ENodesHS.GetKey(j)) { continue; }
807        if (! NbrNIdENumH.IsKey(ENodesHS.GetKey(j))) {
808          NbrNIdENumH.AddDat(ENodesHS.GetKey(j), 1);
809        } else {
810          NbrNIdENumH.AddDat(ENodesHS.GetKey(j), 1 + NbrNIdENumH.GetDat(ENodesHS.GetKey(j)));
811        }
812      }
813    }
814  }
815  inline THGraph::TEdge::TEdge( THGraph* GraphPt, const TIntSet& NodeIdsHS) {
816    TInt EIdCandidate = GraphPt->NEdges;
817    while (GraphPt->EdgeH.IsKey(EIdCandidate)) { EIdCandidate++; }
818    TEdge(EIdCandidate, NodeIdsHS, GraphPt);
819  }
820  inline THGraph::TEdge::TEdge( THGraph* GraphPt, const TIntV& NodeIdsV) {
821    TInt EIdCandidate = GraphPt->NEdges;
822    while (GraphPt->EdgeH.IsKey(EIdCandidate)) { EIdCandidate++; }
823    TEdge(EIdCandidate, NodeIdsV, GraphPt);
824  }
825  void THGraph::TEdge::UpdNEInfo(const TIntSet& ENodesHS){
826    for (int i = 0; i < ENodesHS.Len(); i++) {
827      Graph->GetNode(ENodesHS.GetKey(i)).UpdEInfo(Id, ENodesHS);
828    }
829  }
830  bool THGraph::IsEdge(const TIntSet& NIdH) {
831    if (NIdH.Len() < 2) { return false; }
832    if (! IsNode(NIdH.GetKey(0))) { return false; }
833    int NId = NIdH.GetKey(0);
834    TIntSet SharedEIdsH;
835    NodeH.GetDat(NId).GetEIDs(SharedEIdsH);
836    for (int n = 1; n< NIdH.Len(); n++){
837      int N1Id = NIdH.GetKey(n);
838      TIntSet NeiEIdH;
839      NodeH.GetDat(N1Id).GetEIDs(NeiEIdH);
840      TIntersect(SharedEIdsH, NeiEIdH);
841      if (SharedEIdsH.Len()==0) { return false; }
842    }
843    for (THashSetKeyI<TInt> e = SharedEIdsH.BegI(); e < SharedEIdsH.EndI(); e++) {
844      if(GetEI(e.GetKey()).Len() == NIdH.Len()) {
845        return true;
846      }
847    }
848    return false;
849  }
850  int THGraph::AddEdge(const TIntSet& NIdH, int& EId) {
851    if (IsEdge(NIdH)) { return -1; }
852    EId = TMath::Mx(EId, MxEId());
853    MxEId = EId + 1;
854    IAssertR(!IsEdgeId(EId), TStr::Fmt("EdgeId %d already exists", EId));
855    EdgeH.AddDat(EId, TEdge(EId, NIdH, this));
856    EdgeH.GetDat(EId).UpdNEInfo(NIdH);
857    NEdges++;
858    N2Edges += (NIdH.Len() * (NIdH.Len()-1))/2;
859    return EId;
860  }
861  int THGraph::AssertNodes(const TIntSet& NodesIS) {
862    int NKey;
863    for (int N=0; N < NodesIS.Len(); N++) {
864      NKey = NodesIS.GetKey(N);
865      if (! IsNode(NKey)) {return NKey;}
866    }
867    return -1;
868  }
869  void THGraph::DelEdge(const int& EId) {
870    IAssertR(IsEdgeId(EId), TStr::Fmt("EdgeId %d not found", EId));
871    TIntSet NodeIdsHS = GetEdge(EId).NeiNIdSH;
872    int iKey, jKey;
873    for (int i=NodeIdsHS.FFirstKeyId(); NodeIdsHS.FNextKeyId(i); ) {
874      iKey = NodeIdsHS.GetKey(i);
875      for (int j=NodeIdsHS.FFirstKeyId(); NodeIdsHS.FNextKeyId(j); ) {
876        if (i==j) { continue; }
877        jKey = NodeIdsHS.GetKey(j);
878        GetNode(iKey).DelNeighbor(jKey);
879      }
880    }
881    EdgeH.DelKey(EId);
882    int ESize = GetEdge(EId).NeiNIdSH.Len();
883    NEdges--;
884    N2Edges -= (ESize*(ESize-1))/2;
885    delete &GetEdge(EId);
886  }
887  void THGraph::GetNIdV(TIntV& NIdV) const {
888    NIdV.Gen(GetNodes(), 0);
889    for (int N=NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
890      NIdV.Add(NodeH.GetKey(N)); }
891  }
892  void THGraph::Defrag(const bool& OnlyNodeLinks) {
893    int nKey;
894    for (int n = EdgeH.FFirstKeyId(); EdgeH.FNextKeyId(n); ) {
895      nKey = EdgeH.GetKey(n);
896      if (! EdgeH.GetDat(nKey).NeiNIdSH.IsKeyIdEqKeyN()) {
897        EdgeH.GetDat(nKey).NeiNIdSH.Defrag();
898      }
899    }
900    for (int n = NodeH.FFirstKeyId(); NodeH.FNextKeyId(n); ) {
901      nKey = NodeH.GetKey(n);
902      NodeH.GetDat(nKey).EIdV.Pack();
903      if (! NodeH.GetDat(nKey).NbrNIdENumH.IsKeyIdEqKeyN()) {
904        NodeH.GetDat(nKey).NbrNIdENumH.Defrag();
905      }
906    }
907    if (! OnlyNodeLinks) {
908      if (! NodeH.IsKeyIdEqKeyN()){ NodeH.Defrag(); }
909      if (! EdgeH.IsKeyIdEqKeyN()){ EdgeH.Defrag(); }
910    }
911  }
912  void THGraph::PrintEdge(const int EId) {
913    if (! EdgeH.IsKey(EId)) {
914      printf("\nEdge Not Found!\n");
915    }
916    TIntV NV;
917    TStr EStr("Edge nodes: ");
918    EdgeH.GetDat(EId).GetNodesV(NV);
919    for (int i = 0; i < NV.Len(); i++) {
920      EStr += (" " + NV[i].GetStr());
921    }
922    EStr += "\n";
923    printf(EStr.GetCStr());
924  }
925  void THGraph::Dump(FILE *OutF) const {
926    const int NodePlaces = (int) ceil(log10((double) GetNodes()));
927    int NKey;
928    for (int N = NodeH.FFirstKeyId(); NodeH.FNextKeyId(N); ) {
929      NKey = NodeH.GetKey(N);
930      const TNode& Node = NodeH.GetDat(NKey);
931      fprintf(OutF, "  %*d [%d] ", NodePlaces, Node.GetId(), Node.GetDeg());
932      for (int edge = 0; edge < Node.GetDeg(); edge++) {
933        fprintf(OutF, " %*d", NodePlaces, Node.GetNbrNId(edge)); }
934      fprintf(OutF, "\n");
935    }
936    fprintf(OutF, "\n");
937  }
938  PHGraph THGraph::GetSmallGraph() {
939    PHGraph Graph = THGraph::New();
940    for (int i = 0; i < 5; i++) { Graph->AddNode(i); }
941    TIntV EV(3);
942    int el[] = {0,1,3};
943    for (int i = 0; i < 3; i++)
944      EV.Add(el[i]);
945    Graph->AddEdge(EV);
946    return Graph;
947  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkldnn_convolution_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-graph_2.cpp</div>
                </div>
                <div class="column column_space"><pre><code>818    for (int i = 0; i < backward_result_2d.count(); ++i) {
819      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
820                backward_result_nd.cpu_diff()[i]);
821    }
822    ASSERT_EQ(backward_weight_result_nd.count(),
823              backward_weight_result_2d.count());
824    for (int i = 0; i < backward_weight_result_2d.count(); ++i) {
825      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
826                backward_weight_result_nd.cpu_diff()[i]);
827    }
828  }
</pre></code></div>
                <div class="column column_space"><pre><code>377      for (int edge = 0; edge < Node.GetInDeg(); edge++) {
378        fprintf(OutF, " %*d", NodePlaces, Node.GetInNId(edge)); }
379      fprintf(OutF, "\n    out[%d]", Node.GetOutDeg());
380      for (int edge = 0; edge < Node.GetOutDeg(); edge++) {
381        fprintf(OutF, " %*d", NodePlaces, Node.GetOutNId(edge)); }
382      fprintf(OutF, "\n");
383    }
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    