<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for power_layer.cpp & spp_layer.cpp</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for power_layer.cpp & spp_layer.cpp
      </h3>
      <h1 align="center">
        9.3%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>power_layer.cpp (12.121212%)<TH>spp_layer.cpp (7.643312%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match2908-0.html#0',2,'match2908-1.html#0',3)" NAME="0">(38-47)<TD><A HREF="javascript:ZweiFrames('match2908-0.html#0',2,'match2908-1.html#0',3)" NAME="0">(195-207)</A><TD ALIGN=center><FONT COLOR="#ff0000">12</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>power_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;vector&gt;

#include &quot;caffe/layers/power_layer.hpp&quot;
#include &quot;caffe/util/math_functions.hpp&quot;

namespace caffe {

template &lt;typename Dtype&gt;
void PowerLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  NeuronLayer&lt;Dtype&gt;::LayerSetUp(bottom, top);
  power_ = this-&gt;layer_param_.power_param().power();
  scale_ = this-&gt;layer_param_.power_param().scale();
  shift_ = this-&gt;layer_param_.power_param().shift();
  diff_scale_ = power_  * scale_;
}

// Compute y = (shift + scale * x)^power
template &lt;typename Dtype&gt;
void PowerLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  Dtype* top_data = top[0]-&gt;mutable_cpu_data();
  const int count = bottom[0]-&gt;count();
  // Special case where we can ignore the input: scale or power is 0.
  if (diff_scale_ == Dtype(0)) {
    Dtype value = (power_ == 0) ? Dtype(1) : pow(shift_, power_);
    caffe_set(count, value, top_data);
    return;
  }
  const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
  caffe_copy(count, bottom_data, top_data);
  if (scale_ != Dtype(1)) {
    caffe_scal(count, scale_, top_data);
  }
<A NAME="0"></A>  if (shift_ != Dtype(0)) {
    caffe_add_scalar(count, shift_, top_data);
  }
<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match2908-1.html#0',3,'match2908-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>  if (power_ != Dtype(1)) {
    caffe_powx(count, top_data, power_, top_data);
  }
}

template &lt;typename Dtype&gt;
void PowerLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
    const vector&lt;bool&gt;&amp; propagate_down,
    const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (propagate_down[0]) {</B></FONT>
    Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();
    const int count = bottom[0]-&gt;count();
    const Dtype* top_diff = top[0]-&gt;cpu_diff();
    if (diff_scale_ == Dtype(0) || power_ == Dtype(1)) {
      caffe_set(count, diff_scale_, bottom_diff);
    } else {
      const Dtype* bottom_data = bottom[0]-&gt;cpu_data();
      // Compute dy/dx = scale * power * (shift + scale * x)^(power - 1)
      //               = diff_scale * y / (shift + scale * x)
      if (power_ == Dtype(2)) {
        // Special case for y = (shift + scale * x)^2
        //     -&gt; dy/dx = 2 * scale * (shift + scale * x)
        //              = diff_scale * shift + diff_scale * scale * x
        caffe_cpu_axpby(count, diff_scale_ * scale_, bottom_data,
            Dtype(0), bottom_diff);
        if (shift_ != Dtype(0)) {
          caffe_add_scalar(count, diff_scale_ * shift_, bottom_diff);
        }
      } else if (shift_ == Dtype(0)) {
        // Special case for y = (scale * x)^power
        //     -&gt; dy/dx = scale * power * (scale * x)^(power - 1)
        //              = scale * power * (scale * x)^power * (scale * x)^(-1)
        //              = power * y / x
        const Dtype* top_data = top[0]-&gt;cpu_data();
        caffe_div(count, top_data, bottom_data, bottom_diff);
        caffe_scal(count, power_, bottom_diff);
      } else {
        caffe_copy(count, bottom_data, bottom_diff);
        if (scale_ != Dtype(1)) {
          caffe_scal(count, scale_, bottom_diff);
        }
        if (shift_ != Dtype(0)) {
          caffe_add_scalar(count, shift_, bottom_diff);
        }
        const Dtype* top_data = top[0]-&gt;cpu_data();
        caffe_div&lt;Dtype&gt;(count, top_data, bottom_diff, bottom_diff);
        if (diff_scale_ != Dtype(1)) {
          caffe_scal(count, diff_scale_, bottom_diff);
        }
      }
    }
    if (diff_scale_ != Dtype(0)) {
      caffe_mul(count, top_diff, bottom_diff, bottom_diff);
    }
  }
}

#ifdef CPU_ONLY
STUB_GPU(PowerLayer);
#endif

INSTANTIATE_CLASS(PowerLayer);
REGISTER_LAYER_CLASS(Power);

}  // namespace caffe
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>spp_layer.cpp</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#include &lt;algorithm&gt;
#include &lt;vector&gt;

#include &quot;caffe/layer.hpp&quot;
#include &quot;caffe/layers/concat_layer.hpp&quot;
#include &quot;caffe/layers/flatten_layer.hpp&quot;
#include &quot;caffe/layers/pooling_layer.hpp&quot;
#include &quot;caffe/layers/split_layer.hpp&quot;
#include &quot;caffe/layers/spp_layer.hpp&quot;

namespace caffe {

using std::min;
using std::max;

template &lt;typename Dtype&gt;
LayerParameter SPPLayer&lt;Dtype&gt;::GetPoolingParam(const int pyramid_level,
      const int bottom_h, const int bottom_w, const SPPParameter spp_param) {
  LayerParameter pooling_param;
  int num_bins = pow(2, pyramid_level);

  // find padding and kernel size so that the pooling is
  // performed across the entire image
  int kernel_h = ceil(bottom_h / static_cast&lt;double&gt;(num_bins));
  // remainder_h is the min number of pixels that need to be padded before
  // entire image height is pooled over with the chosen kernel dimension
  int remainder_h = kernel_h * num_bins - bottom_h;
  // pooling layer pads (2 * pad_h) pixels on the top and bottom of the
  // image.
  int pad_h = (remainder_h + 1) / 2;

  // similar logic for width
  int kernel_w = ceil(bottom_w / static_cast&lt;double&gt;(num_bins));
  int remainder_w = kernel_w * num_bins - bottom_w;
  int pad_w = (remainder_w + 1) / 2;

  pooling_param.mutable_pooling_param()-&gt;set_pad_h(pad_h);
  pooling_param.mutable_pooling_param()-&gt;set_pad_w(pad_w);
  pooling_param.mutable_pooling_param()-&gt;set_kernel_h(kernel_h);
  pooling_param.mutable_pooling_param()-&gt;set_kernel_w(kernel_w);
  pooling_param.mutable_pooling_param()-&gt;set_stride_h(kernel_h);
  pooling_param.mutable_pooling_param()-&gt;set_stride_w(kernel_w);

  switch (spp_param.pool()) {
  case SPPParameter_PoolMethod_MAX:
    pooling_param.mutable_pooling_param()-&gt;set_pool(
        PoolingParameter_PoolMethod_MAX);
    break;
  case SPPParameter_PoolMethod_AVE:
    pooling_param.mutable_pooling_param()-&gt;set_pool(
        PoolingParameter_PoolMethod_AVE);
    break;
  case SPPParameter_PoolMethod_STOCHASTIC:
    pooling_param.mutable_pooling_param()-&gt;set_pool(
        PoolingParameter_PoolMethod_STOCHASTIC);
    break;
  default:
    LOG(FATAL) &lt;&lt; &quot;Unknown pooling method.&quot;;
  }

  return pooling_param;
}

template &lt;typename Dtype&gt;
void SPPLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  SPPParameter spp_param = this-&gt;layer_param_.spp_param();

  num_ = bottom[0]-&gt;num();
  channels_ = bottom[0]-&gt;channels();
  bottom_h_ = bottom[0]-&gt;height();
  bottom_w_ = bottom[0]-&gt;width();
  reshaped_first_time_ = false;
  CHECK_GT(bottom_h_, 0) &lt;&lt; &quot;Input dimensions cannot be zero.&quot;;
  CHECK_GT(bottom_w_, 0) &lt;&lt; &quot;Input dimensions cannot be zero.&quot;;

  pyramid_height_ = spp_param.pyramid_height();
  split_top_vec_.clear();
  pooling_bottom_vecs_.clear();
  pooling_layers_.clear();
  pooling_top_vecs_.clear();
  pooling_outputs_.clear();
  flatten_layers_.clear();
  flatten_top_vecs_.clear();
  flatten_outputs_.clear();
  concat_bottom_vec_.clear();

  if (pyramid_height_ == 1) {
    // pooling layer setup
    LayerParameter pooling_param = GetPoolingParam(0, bottom_h_, bottom_w_,
        spp_param);
    pooling_layers_.push_back(shared_ptr&lt;PoolingLayer&lt;Dtype&gt; &gt; (
        new PoolingLayer&lt;Dtype&gt;(pooling_param)));
    pooling_layers_[0]-&gt;SetUp(bottom, top);
    return;
  }
  // split layer output holders setup
  for (int i = 0; i &lt; pyramid_height_; i++) {
    split_top_vec_.push_back(new Blob&lt;Dtype&gt;());
  }

  // split layer setup
  LayerParameter split_param;
  split_layer_.reset(new SplitLayer&lt;Dtype&gt;(split_param));
  split_layer_-&gt;SetUp(bottom, split_top_vec_);

  for (int i = 0; i &lt; pyramid_height_; i++) {
    // pooling layer input holders setup
    pooling_bottom_vecs_.push_back(new vector&lt;Blob&lt;Dtype&gt;*&gt;);
    pooling_bottom_vecs_[i]-&gt;push_back(split_top_vec_[i]);

    // pooling layer output holders setup
    pooling_outputs_.push_back(new Blob&lt;Dtype&gt;());
    pooling_top_vecs_.push_back(new vector&lt;Blob&lt;Dtype&gt;*&gt;);
    pooling_top_vecs_[i]-&gt;push_back(pooling_outputs_[i]);

    // pooling layer setup
    LayerParameter pooling_param = GetPoolingParam(
        i, bottom_h_, bottom_w_, spp_param);

    pooling_layers_.push_back(shared_ptr&lt;PoolingLayer&lt;Dtype&gt; &gt; (
        new PoolingLayer&lt;Dtype&gt;(pooling_param)));
    pooling_layers_[i]-&gt;SetUp(*pooling_bottom_vecs_[i], *pooling_top_vecs_[i]);

    // flatten layer output holders setup
    flatten_outputs_.push_back(new Blob&lt;Dtype&gt;());
    flatten_top_vecs_.push_back(new vector&lt;Blob&lt;Dtype&gt;*&gt;);
    flatten_top_vecs_[i]-&gt;push_back(flatten_outputs_[i]);

    // flatten layer setup
    LayerParameter flatten_param;
    flatten_layers_.push_back(new FlattenLayer&lt;Dtype&gt;(flatten_param));
    flatten_layers_[i]-&gt;SetUp(*pooling_top_vecs_[i], *flatten_top_vecs_[i]);

    // concat layer input holders setup
    concat_bottom_vec_.push_back(flatten_outputs_[i]);
  }

  // concat layer setup
  LayerParameter concat_param;
  concat_layer_.reset(new ConcatLayer&lt;Dtype&gt;(concat_param));
  concat_layer_-&gt;SetUp(concat_bottom_vec_, top);
}

template &lt;typename Dtype&gt;
void SPPLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  CHECK_EQ(4, bottom[0]-&gt;num_axes()) &lt;&lt; &quot;Input must have 4 axes, &quot;
      &lt;&lt; &quot;corresponding to (num, channels, height, width)&quot;;
  // Do nothing if bottom shape is unchanged since last Reshape
  if (num_ == bottom[0]-&gt;num() &amp;&amp; channels_ == bottom[0]-&gt;channels() &amp;&amp;
      bottom_h_ == bottom[0]-&gt;height() &amp;&amp; bottom_w_ == bottom[0]-&gt;width() &amp;&amp;
      reshaped_first_time_) {
    return;
  }
  num_ = bottom[0]-&gt;num();
  channels_ = bottom[0]-&gt;channels();
  bottom_h_ = bottom[0]-&gt;height();
  bottom_w_ = bottom[0]-&gt;width();
  reshaped_first_time_ = true;
  SPPParameter spp_param = this-&gt;layer_param_.spp_param();
  if (pyramid_height_ == 1) {
    LayerParameter pooling_param = GetPoolingParam(0, bottom_h_, bottom_w_,
        spp_param);
    pooling_layers_[0].reset(new PoolingLayer&lt;Dtype&gt;(pooling_param));
    pooling_layers_[0]-&gt;SetUp(bottom, top);
    pooling_layers_[0]-&gt;Reshape(bottom, top);
    return;
  }
  split_layer_-&gt;Reshape(bottom, split_top_vec_);
  for (int i = 0; i &lt; pyramid_height_; i++) {
    LayerParameter pooling_param = GetPoolingParam(
        i, bottom_h_, bottom_w_, spp_param);

    pooling_layers_[i].reset(
        new PoolingLayer&lt;Dtype&gt;(pooling_param));
    pooling_layers_[i]-&gt;SetUp(
        *pooling_bottom_vecs_[i], *pooling_top_vecs_[i]);
    pooling_layers_[i]-&gt;Reshape(
        *pooling_bottom_vecs_[i], *pooling_top_vecs_[i]);
    flatten_layers_[i]-&gt;Reshape(
        *pooling_top_vecs_[i], *flatten_top_vecs_[i]);
  }
  concat_layer_-&gt;Reshape(concat_bottom_vec_, top);
}

template &lt;typename Dtype&gt;
void SPPLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {
  if (pyramid_height_ == 1) {
    pooling_layers_[0]-&gt;Forward(bottom, top);
<A NAME="0"></A>    return;
  }
  split_layer_-&gt;Forward(bottom, split_top_vec_);
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2908-0.html#0',2,'match2908-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>  for (int i = 0; i &lt; pyramid_height_; i++) {
    pooling_layers_[i]-&gt;Forward(
        *pooling_bottom_vecs_[i], *pooling_top_vecs_[i]);
    flatten_layers_[i]-&gt;Forward(
        *pooling_top_vecs_[i], *flatten_top_vecs_[i]);
  }
  concat_layer_-&gt;Forward(concat_bottom_vec_, top);
}

template &lt;typename Dtype&gt;
void SPPLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,
      const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {
  if (!propagate_down[0]) {</B></FONT>
    return;
  }
  if (pyramid_height_ == 1) {
    pooling_layers_[0]-&gt;Backward(top, propagate_down, bottom);
    return;
  }
  vector&lt;bool&gt; concat_propagate_down(pyramid_height_, true);
  concat_layer_-&gt;Backward(top, concat_propagate_down, concat_bottom_vec_);
  for (int i = 0; i &lt; pyramid_height_; i++) {
    flatten_layers_[i]-&gt;Backward(
        *flatten_top_vecs_[i], propagate_down, *pooling_top_vecs_[i]);
    pooling_layers_[i]-&gt;Backward(
        *pooling_top_vecs_[i], propagate_down, *pooling_bottom_vecs_[i]);
  }
  split_layer_-&gt;Backward(split_top_vec_, propagate_down, bottom);
}

INSTANTIATE_CLASS(SPPLayer);
REGISTER_LAYER_CLASS(SPP);

}  // namespace caffe
</PRE>
</div>
  </div>
</body>
</html>
