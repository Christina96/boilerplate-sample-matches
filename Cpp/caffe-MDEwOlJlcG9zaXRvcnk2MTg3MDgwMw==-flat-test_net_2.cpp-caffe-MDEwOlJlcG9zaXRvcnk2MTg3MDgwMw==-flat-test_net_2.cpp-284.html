
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 27, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</h3>
            <pre><code>1  #include &lt;string&gt;
2  #include &lt;utility&gt;
3  #include &lt;vector&gt;
4  #include &quot;google/protobuf/text_format.h&quot;
5  #include &quot;gtest/gtest.h&quot;
6  #include &quot;caffe/common.hpp&quot;
7  #include &quot;caffe/filler.hpp&quot;
8  #include &quot;caffe/net.hpp&quot;
9  #include &quot;caffe/util/io.hpp&quot;
10  #include &quot;caffe/util/math_functions.hpp&quot;
11  #include &quot;caffe/test/test_caffe_main.hpp&quot;
12  #include &quot;caffe/test/test_gradient_check_util.hpp&quot;
13  namespace caffe {
14  template &lt;typename ParentType&gt;
15  class ParentTest : public ParentType {
16    typedef typename ParentType::Dtype Dtype;
17   protected:
18    ParentTest() : seed_(1701) {}
19    virtual void InitNetFromProtoString(const string&amp; proto) {
20      NetParameter param;
21      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &amp;param));
22      net_.reset(new Net&lt;Dtype&gt;(param));
23    }
24    virtual void InitNetFromProtoFileWithState(const string&amp; proto,
25        Phase phase = caffe::TRAIN, const int level = 0,
26        const vector&lt;string&gt;* stages = NULL) {
27      NetParameter param;
28      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &amp;param));
29      string param_file;
30      MakeTempFilename(&amp;param_file);
31      WriteProtoToTextFile(param, param_file);
32      net_.reset(new Net&lt;Dtype&gt;(param_file, phase, level, stages));
33    }
34    virtual void CopyNetBlobs(const bool copy_diff,
35        vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;* blobs_copy) {
36      CHECK(net_);
37      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; net_blobs = net_-&gt;blobs();
38      blobs_copy-&gt;clear();
39      blobs_copy-&gt;resize(net_blobs.size());
40      const bool kReshape = true;
41      for (int i = 0; i &lt; net_blobs.size(); ++i) {
42        (*blobs_copy)[i].reset(new Blob&lt;Dtype&gt;());
43        (*blobs_copy)[i]-&gt;CopyFrom(*net_blobs[i], copy_diff, kReshape);
44      }
45    }
46    virtual void CopyNetParams(const bool copy_diff,
47        vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;* params_copy) {
48      CHECK(net_);
49      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; net_params = net_-&gt;params();
50      params_copy-&gt;clear();
51      params_copy-&gt;resize(net_params.size());
52      const bool kReshape = true;
53      for (int i = 0; i &lt; net_params.size(); ++i) {
54        (*params_copy)[i].reset(new Blob&lt;Dtype&gt;());
55        (*params_copy)[i]-&gt;CopyFrom(*net_params[i], copy_diff, kReshape);
56      }
57    }
58    virtual void InitTinyNet(const bool force_backward = false,
59                             const bool accuracy_layer = false) {
60      string proto =
61          &quot;name: &#x27;TinyTestNetwork&#x27; &quot;
62          &quot;layer { &quot;
63          &quot;  name: &#x27;data&#x27; &quot;
64          &quot;  type: &#x27;DummyData&#x27; &quot;
65          &quot;  dummy_data_param { &quot;
66          &quot;    shape { &quot;
67          &quot;      dim: 5 &quot;
68          &quot;      dim: 2 &quot;
69          &quot;      dim: 3 &quot;
70          &quot;      dim: 4 &quot;
71          &quot;    } &quot;
72          &quot;    data_filler { &quot;
73          &quot;      type: &#x27;gaussian&#x27; &quot;
74          &quot;      std: 0.01 &quot;
75          &quot;    } &quot;
76          &quot;    shape { &quot;
77          &quot;      dim: 5 &quot;
78          &quot;    } &quot;
79          &quot;    data_filler { &quot;
80          &quot;      type: &#x27;constant&#x27; &quot;
81          &quot;      value: 0 &quot;
82          &quot;    } &quot;
83          &quot;  } &quot;
84          &quot;  top: &#x27;data&#x27; &quot;
85          &quot;  top: &#x27;label&#x27; &quot;
86          &quot;} &quot;
87          &quot;layer { &quot;
88          &quot;  name: &#x27;innerproduct&#x27; &quot;
89          &quot;  type: &#x27;InnerProduct&#x27; &quot;
90          &quot;  inner_product_param { &quot;
91          &quot;    num_output: 1000 &quot;
92          &quot;    weight_filler { &quot;
93          &quot;      type: &#x27;gaussian&#x27; &quot;
94          &quot;      std: 0.01 &quot;
95          &quot;    } &quot;
96          &quot;    bias_filler { &quot;
97          &quot;      type: &#x27;constant&#x27; &quot;
98          &quot;      value: 0 &quot;
99          &quot;    } &quot;
100          &quot;  } &quot;
101          &quot;  param { &quot;
102          &quot;    lr_mult: 1 &quot;
103          &quot;    decay_mult: 1 &quot;
104          &quot;  } &quot;
105          &quot;  param { &quot;
106          &quot;    lr_mult: 2 &quot;
107          &quot;    decay_mult: 0 &quot;
108          &quot;  } &quot;
109          &quot;  bottom: &#x27;data&#x27; &quot;
110          &quot;  top: &#x27;innerproduct&#x27; &quot;
111          &quot;} &quot;
112          &quot;layer { &quot;
113          &quot;  name: &#x27;loss&#x27; &quot;
114          &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
115          &quot;  bottom: &#x27;innerproduct&#x27; &quot;
116          &quot;  bottom: &#x27;label&#x27; &quot;
117          &quot;  top: &#x27;top_loss&#x27; &quot;
118          &quot;} &quot;;
119      if (accuracy_layer) {
120        proto +=
121            &quot;layer { &quot;
122            &quot;  name: &#x27;loss&#x27; &quot;
123            &quot;  type: &#x27;Accuracy&#x27; &quot;
124            &quot;  bottom: &#x27;innerproduct&#x27; &quot;
125            &quot;  bottom: &#x27;label&#x27; &quot;
126            &quot;  top: &#x27;accuracy&#x27; &quot;
127            &quot;} &quot;;
128      }
129      if (force_backward) {
130        proto += &quot;force_backward: true &quot;;
131      }
132      InitNetFromProtoString(proto);
133    }
134    virtual void InitTinyNetEuclidean(const bool force_backward = false) {
135      string proto =
136          &quot;name: &#x27;TinyTestEuclidLossNetwork&#x27; &quot;
137          &quot;layer { &quot;
138          &quot;  name: &#x27;data&#x27; &quot;
139          &quot;  type: &#x27;DummyData&#x27; &quot;
140          &quot;  dummy_data_param { &quot;
141          &quot;    num: 5 &quot;
142          &quot;    channels: 2 &quot;
143          &quot;    height: 3 &quot;
144          &quot;    width: 4 &quot;
145          &quot;    num: 5 &quot;
146          &quot;    channels: 1 &quot;
147          &quot;    height: 1 &quot;
148          &quot;    width: 1 &quot;
149          &quot;    data_filler { &quot;
150          &quot;      type: &#x27;gaussian&#x27; &quot;
151          &quot;      std: 0.01 &quot;
152          &quot;    } &quot;
153          &quot;  } &quot;
154          &quot;  top: &#x27;data&#x27; &quot;
155          &quot;  top: &#x27;label&#x27; &quot;
156          &quot;} &quot;
157          &quot;layer { &quot;
158          &quot;  name: &#x27;innerproduct&#x27; &quot;
159          &quot;  type: &#x27;InnerProduct&#x27; &quot;
160          &quot;  inner_product_param { &quot;
161          &quot;    num_output: 1 &quot;
162          &quot;    weight_filler { &quot;
163          &quot;      type: &#x27;gaussian&#x27; &quot;
164          &quot;      std: 0.01 &quot;
165          &quot;    } &quot;
166          &quot;    bias_filler { &quot;
167          &quot;      type: &#x27;constant&#x27; &quot;
168          &quot;      value: 0 &quot;
169          &quot;    } &quot;
170          &quot;  } &quot;
171          &quot;  param { &quot;
172          &quot;    lr_mult: 1 &quot;
173          &quot;    decay_mult: 1 &quot;
174          &quot;  } &quot;
175          &quot;  param { &quot;
176          &quot;    lr_mult: 2 &quot;
177          &quot;    decay_mult: 0 &quot;
178          &quot;  } &quot;
179          &quot;  bottom: &#x27;data&#x27; &quot;
180          &quot;  top: &#x27;innerproduct&#x27; &quot;
181          &quot;} &quot;
182          &quot;layer { &quot;
183          &quot;  name: &#x27;loss&#x27; &quot;
184          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
185          &quot;  bottom: &#x27;innerproduct&#x27; &quot;
186          &quot;  bottom: &#x27;label&#x27; &quot;
187          &quot;} &quot;;
188      if (force_backward) {
189        proto += &quot;force_backward: true &quot;;
190      }
191      InitNetFromProtoString(proto);
192    }
193    virtual void InitTrickyNet(Dtype* loss_weight = NULL) {
194      ostringstream loss_weight_stream;
195      if (loss_weight) {
196        loss_weight_stream &lt;&lt; &quot;  loss_weight: &quot; &lt;&lt; *loss_weight &lt;&lt; &quot; &quot;;
197      }
198      const string&amp; proto =
199          &quot;name: &#x27;TrickyTestNetwork&#x27; &quot;
200          &quot;layer { &quot;
201          &quot;  name: &#x27;data&#x27; &quot;
202          &quot;  type: &#x27;DummyData&#x27; &quot;
203          &quot;  dummy_data_param { &quot;
204          &quot;    num: 5 &quot;
205          &quot;    channels: 2 &quot;
206          &quot;    height: 3 &quot;
207          &quot;    width: 4 &quot;
208          &quot;    num: 5 &quot;
209          &quot;    channels: 1 &quot;
210          &quot;    height: 1 &quot;
211          &quot;    width: 1 &quot;
212          &quot;    data_filler { &quot;
213          &quot;      type: &#x27;gaussian&#x27; &quot;
214          &quot;      std: 0.01 &quot;
215          &quot;    } &quot;
216          &quot;  } &quot;
217          &quot;  top: &#x27;data&#x27; &quot;
218          &quot;  top: &#x27;label&#x27; &quot;
219          &quot;} &quot;
220          &quot;layer { &quot;
221          &quot;  name: &#x27;innerproduct&#x27; &quot;
222          &quot;  type: &#x27;InnerProduct&#x27; &quot;
223          &quot;  inner_product_param { &quot;
224          &quot;    num_output: 1000 &quot;
225          &quot;    weight_filler { &quot;
226          &quot;      type: &#x27;gaussian&#x27; &quot;
227          &quot;      std: 0.01 &quot;
228          &quot;    } &quot;
229          &quot;    bias_filler { &quot;
230          &quot;      type: &#x27;constant&#x27; &quot;
231          &quot;      value: 0 &quot;
232          &quot;    } &quot;
233          &quot;  } &quot;
234          &quot;  param { &quot;
235          &quot;    lr_mult: 1 &quot;
236          &quot;    decay_mult: 1 &quot;
237          &quot;  } &quot;
238          &quot;  param { &quot;
239          &quot;    lr_mult: 2 &quot;
240          &quot;    decay_mult: 0 &quot;
241          &quot;  } &quot;
242          &quot;  bottom: &#x27;data&#x27; &quot;
243          &quot;  top: &#x27;transformed_data&#x27; &quot;
244          &quot;} &quot;
245          &quot;layer { &quot;
246          &quot;  name: &#x27;innerproduct&#x27; &quot;
247          &quot;  type: &#x27;InnerProduct&#x27; &quot;
248          &quot;  inner_product_param { &quot;
249          &quot;    num_output: 1 &quot;
250          &quot;    weight_filler { &quot;
251          &quot;      type: &#x27;gaussian&#x27; &quot;
252          &quot;      std: 0.01 &quot;
253          &quot;    } &quot;
254          &quot;    bias_filler { &quot;
255          &quot;      type: &#x27;constant&#x27; &quot;
256          &quot;      value: 0 &quot;
257          &quot;    } &quot;
258          &quot;  } &quot;
259          &quot;  param { &quot;
260          &quot;    lr_mult: 1 &quot;
261          &quot;    decay_mult: 1 &quot;
262          &quot;  } &quot;
263          &quot;  param { &quot;
264          &quot;    lr_mult: 2 &quot;
265          &quot;    decay_mult: 0 &quot;
266          &quot;  } &quot;
267          &quot;  bottom: &#x27;label&#x27; &quot;
268          &quot;  top: &#x27;transformed_label&#x27; &quot;
269          &quot;} &quot;
270          &quot;layer { &quot;
271          &quot;  name: &#x27;loss&#x27; &quot;
272          &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot; +
273          loss_weight_stream.str() +
274          &quot;  bottom: &#x27;transformed_data&#x27; &quot;
275          &quot;  bottom: &#x27;transformed_label&#x27; &quot;
276          &quot;} &quot;;
277      InitNetFromProtoString(proto);
278    }
279    virtual void InitUnsharedWeightsNet(const Dtype* loss_weight = NULL,
280        const Dtype* midnet_loss_weight = NULL,
281        const bool force_backward = false, const bool bias_term = false,
282        const Dtype blobs_lr_w1 = 1, const Dtype blobs_lr_b1 = 2,
283        const Dtype blobs_lr_w2 = 1, const Dtype blobs_lr_b2 = 2) {
284      string bias_str = bias_term ? &quot;true &quot;:&quot;false &quot;;
285      ostringstream proto;
286      proto &lt;&lt; &quot;name: &#x27;UnsharedWeightsNetwork&#x27; &quot;;
287      if (force_backward) {
288        proto &lt;&lt; &quot;force_backward: true &quot;;
289      }
290      proto &lt;&lt;
291          &quot;layer { &quot;
292          &quot;  name: &#x27;data&#x27; &quot;
293          &quot;  type: &#x27;DummyData&#x27; &quot;
294          &quot;  dummy_data_param { &quot;
295          &quot;    num: 5 &quot;
296          &quot;    channels: 2 &quot;
297          &quot;    height: 3 &quot;
298          &quot;    width: 4 &quot;
299          &quot;    data_filler { &quot;
300          &quot;      type: &#x27;gaussian&#x27; &quot;
301          &quot;      std: 0.01 &quot;
302          &quot;    } &quot;
303          &quot;  } &quot;
304          &quot;  top: &#x27;data&#x27; &quot;
305          &quot;} &quot;
306          &quot;layer { &quot;
307          &quot;  name: &#x27;innerproduct1&#x27; &quot;
308          &quot;  type: &#x27;InnerProduct&#x27; &quot;
309          &quot;  inner_product_param { &quot;
310          &quot;    num_output: 10 &quot;
311          &quot;    bias_term: &quot; &lt;&lt; bias_str &lt;&lt;
312          &quot;    weight_filler { &quot;
313          &quot;      type: &#x27;gaussian&#x27; &quot;
314          &quot;      std: 10 &quot;
<span onclick='openModal()' class='match'>315          &quot;    } &quot;
316          &quot;  } &quot;
317          &quot;  param { &quot;
318          &quot;    name: &#x27;unsharedweights1&#x27; &quot;
319          &quot;    lr_mult: &quot; &lt;&lt; blobs_lr_w1 &lt;&lt;
320          &quot;  } &quot;;
321      if (bias_term) {
322        proto &lt;&lt; &quot;  param { lr_mult: &quot; &lt;&lt; blobs_lr_b1 &lt;&lt; &quot; } &quot;;
323      }
324      proto &lt;&lt;
325          &quot;  bottom: &#x27;data&#x27; &quot;
326          &quot;  top: &#x27;innerproduct1&#x27; &quot;;
</span>327      if (midnet_loss_weight) {
328        proto &lt;&lt; &quot;  loss_weight: &quot; &lt;&lt; *midnet_loss_weight &lt;&lt; &quot; &quot;;
329      }
330      proto &lt;&lt;
331          &quot;} &quot;
332          &quot;layer { &quot;
333          &quot;  name: &#x27;innerproduct2&#x27; &quot;
334          &quot;  type: &#x27;InnerProduct&#x27; &quot;
335          &quot;  inner_product_param { &quot;
336          &quot;    num_output: 10 &quot;
337          &quot;    bias_term: &quot; &lt;&lt; bias_str &lt;&lt;
338          &quot;    weight_filler { &quot;
339          &quot;      type: &#x27;gaussian&#x27; &quot;
340          &quot;      std: 10 &quot;
341          &quot;    } &quot;
342          &quot;  } &quot;
343          &quot;  param { &quot;
344          &quot;    name: &#x27;unsharedweights2&#x27; &quot;
345          &quot;    lr_mult: &quot; &lt;&lt; blobs_lr_w2 &lt;&lt;
346          &quot;  } &quot;;
347      if (bias_term) {
348        proto &lt;&lt; &quot;  param { lr_mult: &quot; &lt;&lt; blobs_lr_b2 &lt;&lt; &quot; } &quot;;
349      }
350      proto &lt;&lt;
351          &quot;  bottom: &#x27;data&#x27; &quot;
352          &quot;  top: &#x27;innerproduct2&#x27; &quot;
353          &quot;} &quot;
354          &quot;layer { &quot;
355          &quot;  name: &#x27;loss&#x27; &quot;
356          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;;
357      if (loss_weight) {
358        proto &lt;&lt; &quot;  loss_weight: &quot; &lt;&lt; *loss_weight &lt;&lt; &quot; &quot;;
359      }
360      proto &lt;&lt;
361          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
362          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
363          &quot;} &quot;;
364      InitNetFromProtoString(proto.str());
365    }
366    virtual void InitSharedWeightsNet() {
367      const string&amp; proto =
368          &quot;name: &#x27;SharedWeightsNetwork&#x27; &quot;
369          &quot;layer { &quot;
370          &quot;  name: &#x27;data&#x27; &quot;
371          &quot;  type: &#x27;DummyData&#x27; &quot;
372          &quot;  dummy_data_param { &quot;
373          &quot;    num: 5 &quot;
374          &quot;    channels: 2 &quot;
375          &quot;    height: 3 &quot;
376          &quot;    width: 4 &quot;
377          &quot;    data_filler { &quot;
378          &quot;      type: &#x27;gaussian&#x27; &quot;
379          &quot;      std: 0.01 &quot;
380          &quot;    } &quot;
381          &quot;  } &quot;
382          &quot;  top: &#x27;data&#x27; &quot;
383          &quot;} &quot;
384          &quot;layer { &quot;
385          &quot;  name: &#x27;innerproduct1&#x27; &quot;
386          &quot;  type: &#x27;InnerProduct&#x27; &quot;
387          &quot;  inner_product_param { &quot;
388          &quot;    num_output: 10 &quot;
389          &quot;    bias_term: false &quot;
390          &quot;    weight_filler { &quot;
391          &quot;      type: &#x27;gaussian&#x27; &quot;
392          &quot;      std: 10 &quot;
393          &quot;    } &quot;
394          &quot;  } &quot;
395          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
396          &quot;  bottom: &#x27;data&#x27; &quot;
397          &quot;  top: &#x27;innerproduct1&#x27; &quot;
398          &quot;} &quot;
399          &quot;layer { &quot;
400          &quot;  name: &#x27;innerproduct2&#x27; &quot;
401          &quot;  type: &#x27;InnerProduct&#x27; &quot;
402          &quot;  inner_product_param { &quot;
403          &quot;    num_output: 10 &quot;
404          &quot;    bias_term: false &quot;
405          &quot;    weight_filler { &quot;
406          &quot;      type: &#x27;gaussian&#x27; &quot;
407          &quot;      std: 10 &quot;
408          &quot;    } &quot;
409          &quot;  } &quot;
410          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
411          &quot;  bottom: &#x27;data&#x27; &quot;
412          &quot;  top: &#x27;innerproduct2&#x27; &quot;
413          &quot;} &quot;
414          &quot;layer { &quot;
415          &quot;  name: &#x27;loss&#x27; &quot;
416          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
417          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
418          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
419          &quot;} &quot;;
420      InitNetFromProtoString(proto);
421    }
422    virtual void InitDiffDataUnsharedWeightsNet() {
423      const string&amp; proto =
424          &quot;name: &#x27;DiffDataUnsharedWeightsNetwork&#x27; &quot;
425          &quot;layer { &quot;
426          &quot;  name: &#x27;data&#x27; &quot;
427          &quot;  type: &#x27;DummyData&#x27; &quot;
428          &quot;  dummy_data_param { &quot;
429          &quot;    num: 10 &quot;
430          &quot;    channels: 10 &quot;
431          &quot;    height: 1 &quot;
432          &quot;    width: 1 &quot;
433          &quot;    num: 10 &quot;
434          &quot;    channels: 10 &quot;
435          &quot;    height: 1 &quot;
436          &quot;    width: 1 &quot;
437          &quot;    data_filler { &quot;
438          &quot;      type: &#x27;gaussian&#x27; &quot;
439          &quot;      std: 10 &quot;
440          &quot;    } &quot;
441          &quot;  } &quot;
442          &quot;  top: &#x27;data1&#x27; &quot;
443          &quot;  top: &#x27;data2&#x27; &quot;
444          &quot;} &quot;
445          &quot;layer { &quot;
446          &quot;  name: &#x27;innerproduct1&#x27; &quot;
447          &quot;  type: &#x27;InnerProduct&#x27; &quot;
448          &quot;  inner_product_param { &quot;
449          &quot;    num_output: 10 &quot;
450          &quot;    bias_term: false &quot;
451          &quot;    weight_filler { &quot;
452          &quot;      type: &#x27;constant&#x27; &quot;
453          &quot;      value: 0.5 &quot;
454          &quot;    } &quot;
455          &quot;  } &quot;
456          &quot;  param { name: &#x27;unsharedweights1&#x27; } &quot;
457          &quot;  bottom: &#x27;data1&#x27; &quot;
458          &quot;  top: &#x27;innerproduct1&#x27; &quot;
459          &quot;} &quot;
460          &quot;layer { &quot;
461          &quot;  name: &#x27;innerproduct2&#x27; &quot;
462          &quot;  type: &#x27;InnerProduct&#x27; &quot;
463          &quot;  inner_product_param { &quot;
464          &quot;    num_output: 10 &quot;
465          &quot;    bias_term: false &quot;
466          &quot;    weight_filler { &quot;
467          &quot;      type: &#x27;constant&#x27; &quot;
468          &quot;      value: 0.5 &quot;
469          &quot;    } &quot;
470          &quot;  } &quot;
471          &quot;  param { name: &#x27;unsharedweights2&#x27; } &quot;
472          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
473          &quot;  top: &#x27;innerproduct2&#x27; &quot;
474          &quot;} &quot;
475          &quot;layer { &quot;
476          &quot;  name: &#x27;loss&#x27; &quot;
477          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
478          &quot;  bottom: &#x27;data2&#x27; &quot;
479          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
480          &quot;} &quot;;
481      InitNetFromProtoString(proto);
482    }
483    virtual void InitDiffDataSharedWeightsNet() {
484      const string&amp; proto =
485          &quot;name: &#x27;DiffDataSharedWeightsNetwork&#x27; &quot;
486          &quot;layer { &quot;
487          &quot;  name: &#x27;data&#x27; &quot;
488          &quot;  type: &#x27;DummyData&#x27; &quot;
489          &quot;  dummy_data_param { &quot;
490          &quot;    num: 10 &quot;
491          &quot;    channels: 10 &quot;
492          &quot;    height: 1 &quot;
493          &quot;    width: 1 &quot;
494          &quot;    num: 10 &quot;
495          &quot;    channels: 10 &quot;
496          &quot;    height: 1 &quot;
497          &quot;    width: 1 &quot;
498          &quot;    data_filler { &quot;
499          &quot;      type: &#x27;gaussian&#x27; &quot;
500          &quot;      std: 10 &quot;
501          &quot;    } &quot;
502          &quot;  } &quot;
503          &quot;  top: &#x27;data1&#x27; &quot;
504          &quot;  top: &#x27;data2&#x27; &quot;
505          &quot;} &quot;
506          &quot;layer { &quot;
507          &quot;  name: &#x27;innerproduct1&#x27; &quot;
508          &quot;  type: &#x27;InnerProduct&#x27; &quot;
509          &quot;  inner_product_param { &quot;
510          &quot;    num_output: 10 &quot;
511          &quot;    bias_term: false &quot;
512          &quot;    weight_filler { &quot;
513          &quot;      type: &#x27;constant&#x27; &quot;
514          &quot;      value: 0.5 &quot;
515          &quot;    } &quot;
516          &quot;  } &quot;
517          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
518          &quot;  bottom: &#x27;data1&#x27; &quot;
519          &quot;  top: &#x27;innerproduct1&#x27; &quot;
520          &quot;} &quot;
521          &quot;layer { &quot;
522          &quot;  name: &#x27;innerproduct2&#x27; &quot;
523          &quot;  type: &#x27;InnerProduct&#x27; &quot;
524          &quot;  inner_product_param { &quot;
525          &quot;    num_output: 10 &quot;
526          &quot;    bias_term: false &quot;
527          &quot;    weight_filler { &quot;
528          &quot;      type: &#x27;constant&#x27; &quot;
529          &quot;      value: 0.5 &quot;
530          &quot;    } &quot;
531          &quot;  } &quot;
532          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
533          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
534          &quot;  top: &#x27;innerproduct2&#x27; &quot;
535          &quot;} &quot;
536          &quot;layer { &quot;
537          &quot;  name: &#x27;loss&#x27; &quot;
538          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
539          &quot;  bottom: &#x27;data2&#x27; &quot;
540          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
541          &quot;} &quot;;
542      InitNetFromProtoString(proto);
543    }
544    virtual void InitReshapableNet() {
545      const string&amp; proto =
546          &quot;name: &#x27;ReshapableNetwork&#x27; &quot;
547          &quot;layer { &quot;
548          &quot;  name: &#x27;data&#x27; &quot;
549          &quot;  type: &#x27;Input&#x27; &quot;
550          &quot;  top: &#x27;data&#x27; &quot;
551          &quot;  input_param { &quot;
552          &quot;  shape: { dim: 1 dim: 3 dim: 100 dim: 100 } &quot;
553          &quot;  } &quot;
554          &quot;} &quot;
555          &quot;layer { &quot;
556          &quot;  name: &#x27;conv1&#x27; &quot;
557          &quot;  type: &#x27;Convolution&#x27; &quot;
558          &quot;  bottom: &#x27;data&#x27; &quot;
559          &quot;  top: &#x27;conv1&#x27; &quot;
560          &quot;  convolution_param { &quot;
561          &quot;    num_output: 5 &quot;
562          &quot;    kernel_size: 3 &quot;
563          &quot;    stride: 2 &quot;
564          &quot;    weight_filler { &quot;
565          &quot;      type: &#x27;gaussian&#x27; &quot;
566          &quot;      std: 0.01 &quot;
567          &quot;    } &quot;
568          &quot;    bias_filler { &quot;
569          &quot;      type: &#x27;constant&#x27; &quot;
570          &quot;      value: 0.2 &quot;
571          &quot;    } &quot;
572          &quot;  } &quot;
573          &quot;} &quot;
574          &quot;layer { &quot;
575          &quot;  name: &#x27;relu1&#x27; &quot;
576          &quot;  type: &#x27;ReLU&#x27; &quot;
577          &quot;  bottom: &#x27;conv1&#x27; &quot;
578          &quot;  top: &#x27;conv1&#x27; &quot;
579          &quot;} &quot;
580          &quot;layer { &quot;
581          &quot;  name: &#x27;pool1&#x27; &quot;
582          &quot;  type: &#x27;Pooling&#x27; &quot;
583          &quot;  bottom: &#x27;conv1&#x27; &quot;
584          &quot;  top: &#x27;pool1&#x27; &quot;
585          &quot;  pooling_param { &quot;
586          &quot;    pool: MAX &quot;
587          &quot;    kernel_size: 2 &quot;
588          &quot;    stride: 2 &quot;
589          &quot;  } &quot;
590          &quot;} &quot;
591          &quot;layer { &quot;
592          &quot;  name: &#x27;norm1&#x27; &quot;
593          &quot;  type: &#x27;LRN&#x27; &quot;
594          &quot;  bottom: &#x27;pool1&#x27; &quot;
595          &quot;  top: &#x27;norm1&#x27; &quot;
596          &quot;  lrn_param { &quot;
597          &quot;    local_size: 3 &quot;
598          &quot;  } &quot;
599          &quot;} &quot;
600          &quot;layer { &quot;
601          &quot;  name: &#x27;softmax&#x27; &quot;
602          &quot;  type: &#x27;Softmax&#x27; &quot;
603          &quot;  bottom: &#x27;norm1&#x27; &quot;
604          &quot;  top: &#x27;softmax&#x27; &quot;
605          &quot;} &quot;;
606      InitNetFromProtoString(proto);
607    }
608    virtual void InitSkipPropNet(bool test_skip_true) {
609      string proto =
610        &quot;name: &#x27;SkipPropTestNetwork&#x27; &quot;
611        &quot;layer { &quot;
612        &quot;  name: &#x27;data&#x27; &quot;
613        &quot;  type: &#x27;DummyData&#x27; &quot;
614        &quot;  dummy_data_param { &quot;
615        &quot;    shape { &quot;
616        &quot;      dim: 5 &quot;
617        &quot;      dim: 2 &quot;
618        &quot;      dim: 3 &quot;
619        &quot;      dim: 4 &quot;
620        &quot;    } &quot;
621        &quot;    data_filler { &quot;
622        &quot;      type: &#x27;gaussian&#x27; &quot;
623        &quot;      std: 0.01 &quot;
624        &quot;    } &quot;
625        &quot;    shape { &quot;
626        &quot;      dim: 5 &quot;
627        &quot;    } &quot;
628        &quot;    data_filler { &quot;
629        &quot;      type: &#x27;constant&#x27; &quot;
630        &quot;      value: 0 &quot;
631        &quot;    } &quot;
632        &quot;  } &quot;
633        &quot;  top: &#x27;data&#x27; &quot;
634        &quot;  top: &#x27;label&#x27; &quot;
635        &quot;} &quot;
636        &quot;layer { &quot;
637        &quot;  name: &#x27;silence&#x27; &quot;
638        &quot;  bottom: &#x27;label&#x27; &quot;
639        &quot;  type: &#x27;Silence&#x27; &quot;
640        &quot;} &quot;
641        &quot;layer { &quot;
642        &quot;  name: &#x27;innerproduct&#x27; &quot;
643        &quot;  type: &#x27;InnerProduct&#x27; &quot;
644        &quot;  inner_product_param { &quot;
645        &quot;    num_output: 1 &quot;
646        &quot;    weight_filler { &quot;
647        &quot;      type: &#x27;gaussian&#x27; &quot;
648        &quot;      std: 0.01 &quot;
649        &quot;    } &quot;
650        &quot;    bias_filler { &quot;
651        &quot;      type: &#x27;constant&#x27; &quot;
652        &quot;      value: 0 &quot;
653        &quot;    } &quot;
654        &quot;  } &quot;
655        &quot;  param { &quot;
656        &quot;    lr_mult: 1 &quot;
657        &quot;    decay_mult: 1 &quot;
658        &quot;  } &quot;
659        &quot;  param { &quot;
660        &quot;    lr_mult: 2 &quot;
661        &quot;    decay_mult: 0 &quot;
662        &quot;  } &quot;
663        &quot;  bottom: &#x27;data&#x27; &quot;
664        &quot;  top: &#x27;innerproduct&#x27; &quot;
665        &quot;} &quot;
666        &quot;layer { &quot;
667        &quot;  name: &#x27;ip_fake_labels&#x27; &quot;
668        &quot;  type: &#x27;InnerProduct&#x27; &quot;
669        &quot;  inner_product_param { &quot;
670        &quot;    num_output: 1 &quot;
671        &quot;    weight_filler { &quot;
672        &quot;      type: &#x27;gaussian&#x27; &quot;
673        &quot;      std: 0.01 &quot;
674        &quot;    } &quot;
675        &quot;    bias_filler { &quot;
676        &quot;      type: &#x27;constant&#x27; &quot;
677        &quot;      value: 0 &quot;
678        &quot;    } &quot;
679        &quot;  } &quot;
680        &quot;  bottom: &#x27;data&#x27; &quot;
681        &quot;  top: &#x27;fake_labels&#x27; &quot;
682        &quot;} &quot;
683        &quot;layer { &quot;
684        &quot;  name: &#x27;argmax&#x27; &quot;
685        &quot;  bottom: &#x27;fake_labels&#x27; &quot;
686        &quot;  top: &#x27;label_argmax&#x27; &quot;
687        &quot;  type: &#x27;ArgMax&#x27; &quot;
688        &quot;} &quot;
689        &quot;layer { &quot;
690        &quot;  name: &#x27;loss&#x27; &quot;
691        &quot;  bottom: &#x27;innerproduct&#x27; &quot;
692        &quot;  bottom: &#x27;label_argmax&#x27; &quot;;
693      if (test_skip_true)
694        proto += &quot;  propagate_down: true &quot;
695                 &quot;  propagate_down: false &quot;;
696      else
697        proto += &quot;  propagate_down: true &quot;
698                 &quot;  propagate_down: true &quot;;
699      proto +=
700        &quot;  top: &#x27;cross_entropy_loss&#x27; &quot;
701        &quot;  type: &#x27;SigmoidCrossEntropyLoss&#x27; &quot;
702        &quot;  loss_weight: 0.1 &quot;
703        &quot;} &quot;;
704      InitNetFromProtoString(proto);
705    }
706    virtual void InitForcePropNet(bool test_force_true) {
707      string proto =
708        &quot;name: &#x27;ForcePropTestNetwork&#x27; &quot;
709        &quot;layer { &quot;
710        &quot;  name: &#x27;data&#x27; &quot;
711        &quot;  type: &#x27;DummyData&#x27; &quot;
712        &quot;  dummy_data_param { &quot;
713        &quot;    shape { &quot;
714        &quot;      dim: 5 &quot;
715        &quot;      dim: 2 &quot;
716        &quot;      dim: 3 &quot;
717        &quot;      dim: 4 &quot;
718        &quot;    } &quot;
719        &quot;    data_filler { &quot;
720        &quot;      type: &#x27;gaussian&#x27; &quot;
721        &quot;      std: 0.01 &quot;
722        &quot;    } &quot;
723        &quot;    shape { &quot;
724        &quot;      dim: 5 &quot;
725        &quot;    } &quot;
726        &quot;    data_filler { &quot;
727        &quot;      type: &#x27;constant&#x27; &quot;
728        &quot;      value: 0 &quot;
729        &quot;    } &quot;
730        &quot;  } &quot;
731        &quot;  top: &#x27;data&#x27; &quot;
732        &quot;  top: &#x27;label&#x27; &quot;
733        &quot;} &quot;
734        &quot;layer { &quot;
735        &quot;  name: &#x27;innerproduct&#x27; &quot;
736        &quot;  type: &#x27;InnerProduct&#x27; &quot;
737        &quot;  inner_product_param { &quot;
738        &quot;    num_output: 1 &quot;
739        &quot;    weight_filler { &quot;
740        &quot;      type: &#x27;gaussian&#x27; &quot;
741        &quot;      std: 0.01 &quot;
742        &quot;    } &quot;
743        &quot;  } &quot;
744        &quot;  bottom: &#x27;data&#x27; &quot;
745        &quot;  top: &#x27;innerproduct&#x27; &quot;;
746      if (test_force_true) {
747        proto += &quot;  propagate_down: true &quot;;
748      }
749      proto +=
750        &quot;} &quot;
751        &quot;layer { &quot;
752        &quot;  name: &#x27;loss&#x27; &quot;
753        &quot;  bottom: &#x27;innerproduct&#x27; &quot;
754        &quot;  bottom: &#x27;label&#x27; &quot;
755        &quot;  top: &#x27;cross_entropy_loss&#x27; &quot;
756        &quot;  type: &#x27;SigmoidCrossEntropyLoss&#x27; &quot;
757        &quot;} &quot;;
758      InitNetFromProtoString(proto);
759    }
760    virtual void InitAllInOneNet(Phase phase = caffe::TRAIN,
761        const int level = 0, const vector&lt;string&gt;* stages = NULL) {
762      string proto =
763        &quot;name: &#x27;All-in-one Network&#x27;&quot;
764        &quot;layer { &quot;
765        &quot;  name: &#x27;train-data&#x27; &quot;
766        &quot;  type: &#x27;DummyData&#x27; &quot;
767        &quot;  top: &#x27;data&#x27; &quot;
768        &quot;  top: &#x27;label&#x27; &quot;
769        &quot;  dummy_data_param { &quot;
770        &quot;    shape { dim: 1 dim: 10 } &quot;
771        &quot;    shape { dim: 1 dim: 1 } &quot;
772        &quot;  } &quot;
773        &quot;  include { phase: TRAIN stage: &#x27;train&#x27; } &quot;
774        &quot;} &quot;
775        &quot;layer { &quot;
776        &quot;  name: &#x27;val-data&#x27; &quot;
777        &quot;  type: &#x27;DummyData&#x27; &quot;
778        &quot;  top: &#x27;data&#x27; &quot;
779        &quot;  top: &#x27;label&#x27; &quot;
780        &quot;  dummy_data_param { &quot;
781        &quot;    shape { dim: 1 dim: 10 } &quot;
782        &quot;    shape { dim: 1 dim: 1 } &quot;
783        &quot;  } &quot;
784        &quot;  include { phase: TEST stage: &#x27;val&#x27; } &quot;
785        &quot;} &quot;
786        &quot;layer { &quot;
787        &quot;  name: &#x27;deploy-data&#x27; &quot;
788        &quot;  type: &#x27;Input&#x27; &quot;
789        &quot;  top: &#x27;data&#x27; &quot;
790        &quot;  input_param { &quot;
791        &quot;    shape { dim: 1 dim: 10 } &quot;
792        &quot;  } &quot;
793        &quot;  include { phase: TEST stage: &#x27;deploy&#x27; } &quot;
794        &quot;} &quot;
795        &quot;layer { &quot;
796        &quot;  name: &#x27;ip&#x27; &quot;
797        &quot;  type: &#x27;InnerProduct&#x27; &quot;
798        &quot;  bottom: &#x27;data&#x27; &quot;
799        &quot;  top: &#x27;ip&#x27; &quot;
800        &quot;  inner_product_param { &quot;
801        &quot;    num_output: 2 &quot;
802        &quot;  } &quot;
803        &quot;} &quot;
804        &quot;layer { &quot;
805        &quot;  name: &#x27;loss&#x27; &quot;
806        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
807        &quot;  bottom: &#x27;ip&#x27; &quot;
808        &quot;  bottom: &#x27;label&#x27; &quot;
809        &quot;  top: &#x27;loss&#x27; &quot;
810        &quot;  include { phase: TRAIN stage: &#x27;train&#x27; } &quot;
811        &quot;  include { phase: TEST stage: &#x27;val&#x27; } &quot;
812        &quot;} &quot;;
813      InitNetFromProtoFileWithState(proto, phase, level, stages);
814    }
815    int seed_;
816    shared_ptr&lt;Net&lt;Dtype&gt; &gt; net_;
817  };
818  template &lt;typename TypeParam&gt;
819  class NetTest : public ParentTest&lt;MultiDeviceTest&lt;TypeParam&gt;&gt; {
820  };
821  template &lt;typename TypeParam&gt;
822  class NetTestCPU : public ParentTest&lt;CPUDeviceTest&lt;TypeParam&gt;&gt; {
823  };
824  #ifdef USE_MKLDNN_AS_DEFAULT_ENGINE
825  TYPED_TEST_CASE(NetTest, MKLDNNTestDtypesAndDevices);
826  #else
827  TYPED_TEST_CASE(NetTest, TestDtypesAndDevices);
828  #endif
829  TYPED_TEST_CASE(NetTestCPU, TestDtypes);
830  TYPED_TEST(NetTest, TestHasBlob) {
831    this-&gt;InitTinyNet();
832    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;data&quot;));
833    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;label&quot;));
834    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;innerproduct&quot;));
835    EXPECT_FALSE(this-&gt;net_-&gt;has_blob(&quot;loss&quot;));
836    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;top_loss&quot;));
837  }
838  TYPED_TEST(NetTest, TestGetBlob) {
839    this-&gt;InitTinyNet();
840    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;data&quot;), this-&gt;net_-&gt;blobs()[0]);
841    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;label&quot;), this-&gt;net_-&gt;blobs()[1]);
842    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;innerproduct&quot;), this-&gt;net_-&gt;blobs()[2]);
843    EXPECT_FALSE(this-&gt;net_-&gt;blob_by_name(&quot;loss&quot;));
844    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;top_loss&quot;), this-&gt;net_-&gt;blobs()[3]);
845  }
846  TYPED_TEST(NetTest, TestHasLayer) {
847    this-&gt;InitTinyNet();
848    EXPECT_TRUE(this-&gt;net_-&gt;has_layer(&quot;data&quot;));
849    EXPECT_TRUE(this-&gt;net_-&gt;has_layer(&quot;innerproduct&quot;));
850    EXPECT_TRUE(this-&gt;net_-&gt;has_layer(&quot;loss&quot;));
851    EXPECT_FALSE(this-&gt;net_-&gt;has_layer(&quot;label&quot;));
852  }
853  TYPED_TEST(NetTest, TestGetLayerByName) {
854    this-&gt;InitTinyNet();
855    EXPECT_EQ(this-&gt;net_-&gt;layer_by_name(&quot;data&quot;), this-&gt;net_-&gt;layers()[0]);
856    EXPECT_EQ(this-&gt;net_-&gt;layer_by_name(&quot;innerproduct&quot;), this-&gt;net_-&gt;layers()[1]);
857    EXPECT_EQ(this-&gt;net_-&gt;layer_by_name(&quot;loss&quot;), this-&gt;net_-&gt;layers()[2]);
858    EXPECT_FALSE(this-&gt;net_-&gt;layer_by_name(&quot;label&quot;));
859  }
860  TYPED_TEST(NetTest, TestBottomNeedBackward) {
861    this-&gt;InitTinyNet();
862    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
863        this-&gt;net_-&gt;bottom_need_backward();
864    EXPECT_EQ(3, bottom_need_backward.size());
865    EXPECT_EQ(0, bottom_need_backward[0].size());
866    EXPECT_EQ(1, bottom_need_backward[1].size());
867    EXPECT_EQ(false, bottom_need_backward[1][0]);
868    EXPECT_EQ(2, bottom_need_backward[2].size());
869    EXPECT_EQ(true, bottom_need_backward[2][0]);
870    EXPECT_EQ(false, bottom_need_backward[2][1]);
871  }
872  TYPED_TEST(NetTest, TestBottomNeedBackwardForce) {
873    const bool force_backward = true;
874    this-&gt;InitTinyNet(force_backward);
875    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
876        this-&gt;net_-&gt;bottom_need_backward();
877    EXPECT_EQ(3, bottom_need_backward.size());
878    EXPECT_EQ(0, bottom_need_backward[0].size());
879    EXPECT_EQ(1, bottom_need_backward[1].size());
880    EXPECT_EQ(true, bottom_need_backward[1][0]);
881    EXPECT_EQ(2, bottom_need_backward[2].size());
882    EXPECT_EQ(true, bottom_need_backward[2][0]);
883    EXPECT_EQ(false, bottom_need_backward[2][1]);
884  }
885  TYPED_TEST(NetTest, TestBottomNeedBackwardEuclideanForce) {
886    const bool force_backward = true;
887    this-&gt;InitTinyNetEuclidean(force_backward);
888    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
889        this-&gt;net_-&gt;bottom_need_backward();
890    EXPECT_EQ(3, bottom_need_backward.size());
891    EXPECT_EQ(0, bottom_need_backward[0].size());
892    EXPECT_EQ(1, bottom_need_backward[1].size());
893    EXPECT_EQ(true, bottom_need_backward[1][0]);
894    EXPECT_EQ(2, bottom_need_backward[2].size());
895    EXPECT_EQ(true, bottom_need_backward[2][0]);
896    EXPECT_EQ(true, bottom_need_backward[2][1]);
897  }
898  TYPED_TEST(NetTest, TestBottomNeedBackwardTricky) {
899    this-&gt;InitTrickyNet();
900    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
901        this-&gt;net_-&gt;bottom_need_backward();
902    EXPECT_EQ(4, bottom_need_backward.size());
903    EXPECT_EQ(0, bottom_need_backward[0].size());
904    EXPECT_EQ(1, bottom_need_backward[1].size());
905    EXPECT_EQ(false, bottom_need_backward[1][0]);
906    EXPECT_EQ(1, bottom_need_backward[2].size());
907    EXPECT_EQ(false, bottom_need_backward[2][0]);
908    EXPECT_EQ(2, bottom_need_backward[3].size());
909    EXPECT_EQ(true, bottom_need_backward[3][0]);
910    EXPECT_EQ(true, bottom_need_backward[3][1]);
911  }
912  TYPED_TEST(NetTest, TestLossWeight) {
913    typedef typename TypeParam::Dtype Dtype;
914    vector&lt;Blob&lt;Dtype&gt;*&gt; bottom;
915    Caffe::set_random_seed(this-&gt;seed_);
916    const bool kForceBackward = true;
917    this-&gt;InitUnsharedWeightsNet(NULL, NULL, kForceBackward);
918    const Dtype loss = this-&gt;net_-&gt;ForwardBackward();
919    const bool kCopyDiff = true;
920    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blob_grads;
921    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads);
922    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_grads;
923    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads);
924    const Dtype kMinLossAbsValue = 1e-2;
925    ASSERT_GE(fabs(loss), kMinLossAbsValue);
926    const Dtype kErrorMargin = 1e-4;
927    const int kNumLossWeights = 6;
928    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
929    for (int i = 0; i &lt; kNumLossWeights; ++i) {
930      Caffe::set_random_seed(this-&gt;seed_);
931      this-&gt;InitUnsharedWeightsNet(&amp;kLossWeights[i], NULL, kForceBackward);
932      const Dtype weighted_loss = this-&gt;net_-&gt;ForwardBackward();
933      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
934      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
935          &lt;&lt; &quot;loss weight = &quot; &lt;&lt; kLossWeights[i];
936      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; weighted_blobs =
937          this-&gt;net_-&gt;blobs();
938      ASSERT_EQ(blob_grads.size(), weighted_blobs.size());
939      for (int j = 0; j &lt; blob_grads.size(); ++j) {
940        ASSERT_EQ(blob_grads[j]-&gt;count(), weighted_blobs[j]-&gt;count());
941        for (int k = 0; k &lt; blob_grads[j]-&gt;count(); ++k) {
942          EXPECT_NEAR(blob_grads[j]-&gt;cpu_diff()[k] * kLossWeights[i],
943                      weighted_blobs[j]-&gt;cpu_diff()[k], error_margin);
944        }
945      }
946      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; weighted_params =
947          this-&gt;net_-&gt;params();
948      ASSERT_EQ(param_grads.size(), weighted_params.size());
949      for (int j = 0; j &lt; param_grads.size(); ++j) {
950        ASSERT_EQ(param_grads[j]-&gt;count(), weighted_params[j]-&gt;count());
951        for (int k = 0; k &lt; param_grads[j]-&gt;count(); ++k) {
952          EXPECT_NEAR(param_grads[j]-&gt;cpu_diff()[k] * kLossWeights[i],
953                      weighted_params[j]-&gt;cpu_diff()[k], error_margin);
954        }
955      }
956    }
957  }
958  TYPED_TEST(NetTest, TestLossWeightMidNet) {
959    typedef typename TypeParam::Dtype Dtype;
960    Caffe::set_random_seed(this-&gt;seed_);
961    const bool kForceBackward = true;
962    Dtype loss_weight = 0;
963    Dtype midnet_loss_weight = 1;
964    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
965                                 kForceBackward);
966    const Dtype loss = this-&gt;net_-&gt;ForwardBackward();
967    const bool kCopyDiff = true;
968    const bool kReshape = true;
969    Blob&lt;Dtype&gt; data_grad;
970    data_grad.CopyFrom(*this-&gt;net_-&gt;blob_by_name(&quot;data&quot;), kCopyDiff, kReshape);
971    const Dtype kMinLossAbsValue = 1e-2;
972    ASSERT_GE(fabs(loss), kMinLossAbsValue);
973    const Dtype kErrorMargin = 1e-4;
974    const int kNumLossWeights = 6;
975    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
976    for (int i = 0; i &lt; kNumLossWeights; ++i) {
977      Caffe::set_random_seed(this-&gt;seed_);
978      this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;kLossWeights[i],
979                                   kForceBackward);
980      const Dtype weighted_loss = this-&gt;net_-&gt;ForwardBackward();
981      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
982      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
983          &lt;&lt; &quot;loss weight = &quot; &lt;&lt; kLossWeights[i];
984      const shared_ptr&lt;Blob&lt;Dtype&gt; &gt;&amp; weighted_blob =
985          this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
986      ASSERT_EQ(data_grad.count(), weighted_blob-&gt;count());
987      for (int j = 0; j &lt; data_grad.count(); ++j) {
988        EXPECT_NEAR(data_grad.cpu_diff()[j] * kLossWeights[i],
989                    weighted_blob-&gt;cpu_diff()[j], error_margin);
990      }
991    }
992  }
993  TYPED_TEST(NetTest, TestComboLossWeight) {
994    typedef typename TypeParam::Dtype Dtype;
995    Dtype loss_weight;
996    Dtype midnet_loss_weight;
997    const bool kForceBackward = true;
998    const Dtype kErrorMargin = 1e-4;
999    loss_weight = 1;
1000    midnet_loss_weight = 1;
1001    Caffe::set_random_seed(this-&gt;seed_);
1002    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1003                                 kForceBackward);
1004    const Dtype loss = this-&gt;net_-&gt;ForwardBackward();
1005    const bool kCopyDiff = true;
1006    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blob_grads;
1007    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads);
1008    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_grads;
1009    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads);
1010    loss_weight = 2;
1011    midnet_loss_weight = 1;
1012    Caffe::set_random_seed(this-&gt;seed_);
1013    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1014                                 kForceBackward);
1015    const Dtype loss_main_2 = this-&gt;net_-&gt;ForwardBackward();
1016    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blob_grads_loss_2;
1017    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads_loss_2);
1018    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_grads_loss_2;
1019    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads_loss_2);
1020    loss_weight = 3;
1021    midnet_loss_weight = 1;
1022    Caffe::set_random_seed(this-&gt;seed_);
1023    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1024                                 kForceBackward);
1025    const Dtype loss_main_3 = this-&gt;net_-&gt;ForwardBackward();
1026    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blob_grads_loss_3 =
1027        this-&gt;net_-&gt;blobs();
1028    ASSERT_EQ(blob_grads.size(), blob_grads_loss_3.size());
1029    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_loss_3.size());
1030    for (int j = 0; j &lt; blob_grads.size(); ++j) {
1031      const string&amp; blob_name = this-&gt;net_-&gt;blob_names()[j];
1032      bool grad_should_change = true;
1033      if (blob_name == &quot;innerproduct1_innerproduct1_0_split_0&quot;) {
1034        grad_should_change = false;
1035      }
1036      ASSERT_EQ(blob_grads[j]-&gt;count(), blob_grads_loss_3[j]-&gt;count());
1037      ASSERT_EQ(blob_grads_loss_2[j]-&gt;count(), blob_grads_loss_3[j]-&gt;count());
1038      for (int k = 0; k &lt; blob_grads[j]-&gt;count(); ++k) {
1039        const Dtype grad_diff_2 = blob_grads_loss_2[j]-&gt;cpu_diff()[k] -
1040                                      blob_grads[j]-&gt;cpu_diff()[k];
1041        const Dtype grad_diff_3 = blob_grads_loss_3[j]-&gt;cpu_diff()[k] -
1042                                      blob_grads[j]-&gt;cpu_diff()[k];
1043        if (grad_should_change) {
1044          const Dtype kMinGradDiffAbsValue = 1e-4;
1045          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) &lt;&lt; blob_name;
1046          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) &lt;&lt; blob_name;
1047        } else {
1048          EXPECT_EQ(0, grad_diff_2) &lt;&lt; blob_name;
1049          EXPECT_EQ(0, grad_diff_3) &lt;&lt; blob_name;
1050        }
1051      }
1052    }
1053    loss_weight = 1;
1054    midnet_loss_weight = 2;
1055    Caffe::set_random_seed(this-&gt;seed_);
1056    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1057                                 kForceBackward);
1058    const Dtype loss_midnet_2 = this-&gt;net_-&gt;ForwardBackward();
1059    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads_loss_2);
1060    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads_loss_2);
1061    loss_weight = 1;
1062    midnet_loss_weight = 3;
1063    Caffe::set_random_seed(this-&gt;seed_);
1064    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1065                                 kForceBackward);
1066    const Dtype loss_midnet_3 = this-&gt;net_-&gt;ForwardBackward();
1067    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blob_grads_midnet_loss_3 =
1068        this-&gt;net_-&gt;blobs();
1069    ASSERT_EQ(blob_grads.size(), blob_grads_midnet_loss_3.size());
1070    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_midnet_loss_3.size());
1071    const vector&lt;string&gt;&amp; blob_names = this-&gt;net_-&gt;blob_names();
1072    for (int j = 0; j &lt; blob_grads.size(); ++j) {
1073      const string&amp; blob_name = blob_names[j];
1074      bool grad_should_change = false;
1075      if (blob_name == &quot;innerproduct1&quot; ||
1076          blob_name == &quot;innerproduct1_innerproduct1_0_split_0&quot; ||
1077          blob_name == &quot;data_data_0_split_0&quot; || blob_name == &quot;data&quot;) {
1078        grad_should_change = true;
1079      }
1080      ASSERT_EQ(blob_grads[j]-&gt;count(), blob_grads_midnet_loss_3[j]-&gt;count());
1081      ASSERT_EQ(blob_grads[j]-&gt;count(), blob_grads_loss_2[j]-&gt;count());
1082      for (int k = 0; k &lt; blob_grads[j]-&gt;count(); ++k) {
1083        const Dtype grad_diff_2 = blob_grads_loss_2[j]-&gt;cpu_diff()[k] -
1084                                      blob_grads[j]-&gt;cpu_diff()[k];
1085        const Dtype grad_diff_3 = blob_grads_midnet_loss_3[j]-&gt;cpu_diff()[k] -
1086                                      blob_grads[j]-&gt;cpu_diff()[k];
1087        if (grad_should_change) {
1088          const Dtype kMinGradDiffAbsValue = 1e-4;
1089          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) &lt;&lt; blob_name;
1090          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) &lt;&lt; blob_name;
1091        } else {
1092          EXPECT_EQ(0, grad_diff_2) &lt;&lt; blob_name;
1093          EXPECT_EQ(0, grad_diff_3) &lt;&lt; blob_name;
1094        }
1095      }
1096    }
1097    const Dtype kMinLossDiffAbsValue = 1e-4;
1098    Dtype loss_diff_2 = loss_main_2 - loss;
1099    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1100    Dtype loss_diff_3 = loss_main_3 - loss;
1101    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1102    loss_diff_2 = loss_midnet_2 - loss;
1103    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1104    loss_diff_3 = loss_midnet_3 - loss;
1105    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1106  }
1107  TYPED_TEST(NetTest, TestBackwardWithAccuracyLayer) {
1108    const bool kForceBackward = false;
1109    const bool kAccuracyLayer = true;
1110    this-&gt;InitTinyNet(kForceBackward, kAccuracyLayer);
1111    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;accuracy&quot;));
1112    this-&gt;net_-&gt;ForwardBackward();
1113  }
1114  TYPED_TEST(NetTest, TestUnsharedWeightsDataNet) {
1115    typedef typename TypeParam::Dtype Dtype;
1116    this-&gt;InitUnsharedWeightsNet();
1117    Dtype loss;
1118    this-&gt;net_-&gt;Forward(&amp;loss);
1119    EXPECT_GT(loss, 0);
1120  }
1121  TYPED_TEST(NetTest, TestSharedWeightsDataNet) {
1122    typedef typename TypeParam::Dtype Dtype;
1123    this-&gt;InitSharedWeightsNet();
1124    Dtype loss;
1125    this-&gt;net_-&gt;Forward(&amp;loss);
1126    EXPECT_FLOAT_EQ(loss, 0);
1127  }
1128  TYPED_TEST(NetTest, TestUnsharedWeightsDiffNet) {
1129    typedef typename TypeParam::Dtype Dtype;
1130    this-&gt;InitUnsharedWeightsNet();
1131    Net&lt;Dtype&gt;* net = this-&gt;net_.get();
1132    net-&gt;Forward();
1133    net-&gt;Backward();
1134    Layer&lt;Dtype&gt;* ip1_layer = net-&gt;layer_by_name(&quot;innerproduct1&quot;).get();
1135    Layer&lt;Dtype&gt;* ip2_layer = net-&gt;layer_by_name(&quot;innerproduct2&quot;).get();
1136    const int count = ip1_layer-&gt;blobs()[0]-&gt;count();
1137    const Dtype* grad1 = ip1_layer-&gt;blobs()[0]-&gt;cpu_diff();
1138    const Dtype* grad2 = ip2_layer-&gt;blobs()[0]-&gt;cpu_diff();
1139    for (int i = 0; i &lt; count; ++i) {
1140      EXPECT_GT(fabs(grad1[i]), 0);
1141      EXPECT_FLOAT_EQ(-1 * grad1[i], grad2[i]);
1142    }
1143  }
1144  TYPED_TEST(NetTest, TestSharedWeightsDiffNet) {
1145    typedef typename TypeParam::Dtype Dtype;
1146    this-&gt;InitSharedWeightsNet();
1147    Net&lt;Dtype&gt;* net = this-&gt;net_.get();
1148    Dtype loss;
1149    net-&gt;Forward(&amp;loss);
1150    net-&gt;Backward();
1151    EXPECT_FLOAT_EQ(loss, 0);
1152    Layer&lt;Dtype&gt;* ip1_layer = net-&gt;layer_by_name(&quot;innerproduct1&quot;).get();
1153    Layer&lt;Dtype&gt;* ip2_layer = net-&gt;layer_by_name(&quot;innerproduct2&quot;).get();
1154    const int count = ip1_layer-&gt;blobs()[0]-&gt;count();
1155    const Dtype* grad1 = ip1_layer-&gt;blobs()[0]-&gt;cpu_diff();
1156    const Dtype* grad2 = ip2_layer-&gt;blobs()[0]-&gt;cpu_diff();
1157    for (int i = 0; i &lt; count; ++i) {
1158      EXPECT_FLOAT_EQ(0, grad1[i]);
1159      EXPECT_FLOAT_EQ(0, grad2[i]);
1160    }
1161  }
1162  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1163  TYPED_TEST(NetTest, TestSharedWeightsUpdate) {
1164    typedef typename TypeParam::Dtype Dtype;
1165    Caffe::set_random_seed(this-&gt;seed_);
1166    this-&gt;InitDiffDataSharedWeightsNet();
1167    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[1], &quot;innerproduct1&quot;);
1168    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[2], &quot;innerproduct2&quot;);
1169    Blob&lt;Dtype&gt;* ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1170    Blob&lt;Dtype&gt;* ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1171    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1172    EXPECT_EQ(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1173    this-&gt;net_-&gt;Forward();
1174    this-&gt;net_-&gt;Backward();
1175    Blob&lt;Dtype&gt; shared_params;
1176    const bool reshape = true;
1177    const bool copy_diff = false;
1178    shared_params.CopyFrom(*ip1_weights, copy_diff, reshape);
1179    shared_params.CopyFrom(*ip1_weights, !copy_diff, reshape);
1180    const int count = ip1_weights-&gt;count();
1181    for (int i = 0; i &lt; count; ++i) {
1182      EXPECT_NE(0, ip1_weights-&gt;cpu_diff()[i]);
1183    }
1184    caffe_axpy(count, Dtype(-1), shared_params.cpu_diff(),
1185               shared_params.mutable_cpu_data());
1186    const Dtype* expected_updated_params = shared_params.cpu_data();
1187    this-&gt;net_-&gt;Update();
1188    const Dtype* actual_updated_params = ip1_weights-&gt;cpu_data();
1189    for (int i = 0; i &lt; count; ++i) {
1190      EXPECT_EQ(expected_updated_params[i], actual_updated_params[i]);
1191    }
1192    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1193    Caffe::set_random_seed(this-&gt;seed_);
1194    this-&gt;InitDiffDataUnsharedWeightsNet();
1195    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[1], &quot;innerproduct1&quot;);
1196    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[2], &quot;innerproduct2&quot;);
1197    ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1198    ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1199    EXPECT_NE(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1200    EXPECT_NE(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1201    this-&gt;net_-&gt;Forward();
1202    this-&gt;net_-&gt;Backward();
1203    Blob&lt;Dtype&gt; unshared_params1;
1204    unshared_params1.CopyFrom(*ip1_weights, copy_diff, reshape);
1205    unshared_params1.CopyFrom(*ip1_weights, !copy_diff, reshape);
1206    Blob&lt;Dtype&gt; unshared_params2;
1207    unshared_params2.CopyFrom(*ip2_weights, copy_diff, reshape);
1208    unshared_params2.CopyFrom(*ip2_weights, !copy_diff, reshape);
1209    for (int i = 0; i &lt; count; ++i) {
1210      EXPECT_NE(0, ip1_weights-&gt;cpu_diff()[i]);
1211      EXPECT_NE(0, ip2_weights-&gt;cpu_diff()[i]);
1212      EXPECT_NE(ip1_weights-&gt;cpu_diff()[i], ip2_weights-&gt;cpu_diff()[i]);
1213      EXPECT_FLOAT_EQ(ip1_weights-&gt;cpu_diff()[i] + ip2_weights-&gt;cpu_diff()[i],
1214                      shared_params.cpu_diff()[i]);
1215    }
1216    caffe_axpy(count, Dtype(-1), ip1_weights-&gt;cpu_diff(),
1217               unshared_params1.mutable_cpu_data());
1218    caffe_axpy(count, Dtype(-1), ip2_weights-&gt;cpu_diff(),
1219               unshared_params2.mutable_cpu_data());
1220    const Dtype* expected_updated_params1 = unshared_params1.cpu_data();
1221    const Dtype* expected_updated_params2 = unshared_params2.cpu_data();
1222    this-&gt;net_-&gt;Update();
1223    const Dtype* actual_updated_params1 = ip1_weights-&gt;cpu_data();
1224    const Dtype* actual_updated_params2 = ip2_weights-&gt;cpu_data();
1225    for (int i = 0; i &lt; count; ++i) {
1226      EXPECT_EQ(expected_updated_params1[i], actual_updated_params1[i]);
1227      EXPECT_EQ(expected_updated_params2[i], actual_updated_params2[i]);
1228      EXPECT_NE(actual_updated_params1[i], actual_updated_params2[i]);
1229      EXPECT_NE(expected_updated_params, expected_updated_params1);
1230    }
1231  }
1232  #endif
1233  TYPED_TEST(NetTest, TestSharedWeightsResume) {
1234    typedef typename TypeParam::Dtype Dtype;
1235    Caffe::set_random_seed(this-&gt;seed_);
1236    this-&gt;InitDiffDataSharedWeightsNet();
1237    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[1], &quot;innerproduct1&quot;);
1238    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[2], &quot;innerproduct2&quot;);
1239    Blob&lt;Dtype&gt;* ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1240    Blob&lt;Dtype&gt;* ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1241    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1242    EXPECT_EQ(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1243    this-&gt;net_-&gt;ForwardBackward();
1244    this-&gt;net_-&gt;Update();
1245    Blob&lt;Dtype&gt; shared_params;
1246    const bool kReshape = true;
1247    const bool kCopyDiff = false;
1248    shared_params.CopyFrom(*ip1_weights, kCopyDiff, kReshape);
1249    const int count = ip1_weights-&gt;count();
1250    NetParameter net_param;
1251    this-&gt;net_-&gt;ToProto(&amp;net_param);
1252    Caffe::set_random_seed(this-&gt;seed_);
1253    this-&gt;InitDiffDataSharedWeightsNet();
1254    this-&gt;net_-&gt;CopyTrainedLayersFrom(net_param);
1255    ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1256    ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1257    ASSERT_FALSE(NULL == ip1_weights);
1258    ASSERT_FALSE(NULL == ip2_weights);
1259    EXPECT_NE(ip1_weights, ip2_weights);
1260    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1261    EXPECT_EQ(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1262    for (int i = 0; i &lt; count; ++i) {
1263      EXPECT_FLOAT_EQ(shared_params.cpu_data()[i], ip1_weights-&gt;cpu_data()[i]);
1264    }
1265  }
1266  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1267  TYPED_TEST(NetTest, TestParamPropagateDown) {
1268    typedef typename TypeParam::Dtype Dtype;
1269    const bool kBiasTerm = true, kForceBackward = false;
1270    const Dtype* kLossWeight1 = NULL;
1271    const Dtype* kLossWeight2 = NULL;
1272    Caffe::set_random_seed(this-&gt;seed_);
1273    Dtype blobs_lr_w1 = 1, blobs_lr_w2 = 1, blobs_lr_b1 = 2, blobs_lr_b2 = 2;
1274    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1275        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1276    this-&gt;net_-&gt;Forward();
1277    this-&gt;net_-&gt;Backward();
1278    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params = this-&gt;net_-&gt;params();
1279    const int num_params = params.size();
1280    ASSERT_EQ(4, num_params);
1281    const Dtype kNonZeroTestMin = 1e-3;
1282    vector&lt;Dtype&gt; param_asums(params.size());
1283    for (int i = 0; i &lt; num_params; ++i) {
1284      const Dtype param_asum =
1285         caffe_cpu_asum(params[i]-&gt;count(), params[i]-&gt;cpu_diff());
1286      param_asums[i] = param_asum;
1287      EXPECT_GT(param_asum, kNonZeroTestMin);
1288    }
1289    Caffe::set_random_seed(this-&gt;seed_);
1290    blobs_lr_w1 *= 2, blobs_lr_w2 *= 2, blobs_lr_b1 *= 2, blobs_lr_b2 *= 2;
1291    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1292        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1293    this-&gt;net_-&gt;Forward();
1294    this-&gt;net_-&gt;Backward();
1295    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params2 = this-&gt;net_-&gt;params();
1296    ASSERT_EQ(num_params, params2.size());
1297    for (int i = 0; i &lt; num_params; ++i) {
1298      const Dtype param_asum =
1299         caffe_cpu_asum(params2[i]-&gt;count(), params2[i]-&gt;cpu_diff());
1300      EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1301    }
1302    Caffe::set_random_seed(this-&gt;seed_);
1303    blobs_lr_w1 = 1, blobs_lr_w2 = 0, blobs_lr_b1 = 0, blobs_lr_b2 = 1;
1304    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1305        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1306    this-&gt;net_-&gt;Forward();
1307    this-&gt;net_-&gt;Backward();
1308    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params3 = this-&gt;net_-&gt;params();
1309    ASSERT_EQ(num_params, params3.size());
1310    for (int i = 0; i &lt; num_params; ++i) {
1311      const Dtype param_asum =
1312         caffe_cpu_asum(params3[i]-&gt;count(), params3[i]-&gt;cpu_diff());
1313      if (i == 1 || i == 2) {
1314        EXPECT_FLOAT_EQ(0, param_asum);
1315      } else {
1316        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1317      }
1318    }
1319    Caffe::set_random_seed(this-&gt;seed_);
1320    blobs_lr_w1 = 0, blobs_lr_w2 = 1, blobs_lr_b1 = 1, blobs_lr_b2 = 0;
1321    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1322        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1323    this-&gt;net_-&gt;Forward();
1324    this-&gt;net_-&gt;Backward();
1325    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params4 = this-&gt;net_-&gt;params();
1326    ASSERT_EQ(num_params, params4.size());
1327    for (int i = 0; i &lt; num_params; ++i) {
1328      const Dtype param_asum =
1329         caffe_cpu_asum(params4[i]-&gt;count(), params4[i]-&gt;cpu_diff());
1330      if (i == 0 || i == 3) {
1331        EXPECT_FLOAT_EQ(0, param_asum);
1332      } else {
1333        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1334      }
1335    }
1336  }
1337  #endif
1338  TYPED_TEST(NetTest, TestFromTo) {
1339    typedef typename TypeParam::Dtype Dtype;
1340    this-&gt;InitTinyNet();
1341    Blob&lt;Dtype&gt; data;
1342    data.ReshapeLike(*this-&gt;net_-&gt;blob_by_name(&quot;data&quot;));
1343    this-&gt;net_-&gt;Forward();
1344    this-&gt;net_-&gt;Backward();
1345    data.CopyFrom(*this-&gt;net_-&gt;blob_by_name(&quot;data&quot;), true, true);
1346    const Dtype *loss_ptr = this-&gt;net_-&gt;output_blobs()[0]-&gt;cpu_data();
1347    Dtype loss = *loss_ptr;
1348    for (int i = 1; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
1349      this-&gt;net_-&gt;ForwardFromTo(1, 1);
1350      if (i &lt; this-&gt;net_-&gt;layers().size() - 1) {
1351        this-&gt;net_-&gt;ForwardFrom(i + 1);
1352      }
1353      EXPECT_EQ(loss, *loss_ptr);
1354    }
1355    for (int i = 1; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
1356      this-&gt;net_-&gt;BackwardTo(i);
1357      this-&gt;net_-&gt;BackwardFrom(i - 1);
1358      for (int j = 0; j &lt; data.count(); ++j) {
1359        EXPECT_EQ(data.cpu_diff()[j],
1360            this-&gt;net_-&gt;blob_by_name(&quot;data&quot;)-&gt;cpu_diff()[j]);
1361      }
1362    }
1363  }
1364  class FilterNetTest : public ::testing::Test {
1365   protected:
1366    void RunFilterNetTest(
1367        const string&amp; input_param_string, const string&amp; filtered_param_string) {
1368      NetParameter input_param;
1369      CHECK(google::protobuf::TextFormat::ParseFromString(
1370          input_param_string, &amp;input_param));
1371      NetParameter expected_filtered_param;
1372      CHECK(google::protobuf::TextFormat::ParseFromString(
1373          filtered_param_string, &amp;expected_filtered_param));
1374      NetParameter actual_filtered_param;
1375      Net&lt;float&gt;::FilterNet(input_param, &amp;actual_filtered_param);
1376      EXPECT_EQ(expected_filtered_param.DebugString(),
1377          actual_filtered_param.DebugString());
1378      NetParameter double_filtered_param;
1379      Net&lt;float&gt;::FilterNet(actual_filtered_param, &amp;double_filtered_param);
1380      EXPECT_EQ(actual_filtered_param.DebugString(),
1381         double_filtered_param.DebugString());
1382    }
1383  };
1384  TEST_F(FilterNetTest, TestNoFilter) {
1385    const string&amp; input_proto =
1386        &quot;name: &#x27;TestNetwork&#x27; &quot;
1387        &quot;layer { &quot;
1388        &quot;  name: &#x27;data&#x27; &quot;
1389        &quot;  type: &#x27;Data&#x27; &quot;
1390        &quot;  top: &#x27;data&#x27; &quot;
1391        &quot;  top: &#x27;label&#x27; &quot;
1392        &quot;} &quot;
1393        &quot;layer { &quot;
1394        &quot;  name: &#x27;innerprod&#x27; &quot;
1395        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1396        &quot;  bottom: &#x27;data&#x27; &quot;
1397        &quot;  top: &#x27;innerprod&#x27; &quot;
1398        &quot;} &quot;
1399        &quot;layer { &quot;
1400        &quot;  name: &#x27;loss&#x27; &quot;
1401        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1402        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1403        &quot;  bottom: &#x27;label&#x27; &quot;
1404        &quot;} &quot;;
1405    this-&gt;RunFilterNetTest(input_proto, input_proto);
1406  }
1407  TEST_F(FilterNetTest, TestFilterLeNetTrainTest) {
1408    const string&amp; input_proto =
1409        &quot;name: &#x27;LeNet&#x27; &quot;
1410        &quot;layer { &quot;
1411        &quot;  name: &#x27;mnist&#x27; &quot;
1412        &quot;  type: &#x27;Data&#x27; &quot;
1413        &quot;  top: &#x27;data&#x27; &quot;
1414        &quot;  top: &#x27;label&#x27; &quot;
1415        &quot;  data_param { &quot;
1416        &quot;    source: &#x27;mnist-train-leveldb&#x27; &quot;
1417        &quot;    batch_size: 64 &quot;
1418        &quot;  } &quot;
1419        &quot;  transform_param { &quot;
1420        &quot;    scale: 0.00390625 &quot;
1421        &quot;  } &quot;
1422        &quot;  include: { phase: TRAIN } &quot;
1423        &quot;} &quot;
1424        &quot;layer { &quot;
1425        &quot;  name: &#x27;mnist&#x27; &quot;
1426        &quot;  type: &#x27;Data&#x27; &quot;
1427        &quot;  top: &#x27;data&#x27; &quot;
1428        &quot;  top: &#x27;label&#x27; &quot;
1429        &quot;  data_param { &quot;
1430        &quot;    source: &#x27;mnist-test-leveldb&#x27; &quot;
1431        &quot;    batch_size: 100 &quot;
1432        &quot;  } &quot;
1433        &quot;  transform_param { &quot;
1434        &quot;    scale: 0.00390625 &quot;
1435        &quot;  } &quot;
1436        &quot;  include: { phase: TEST } &quot;
1437        &quot;} &quot;
1438        &quot;layer { &quot;
1439        &quot;  name: &#x27;conv1&#x27; &quot;
1440        &quot;  type: &#x27;Convolution&#x27; &quot;
1441        &quot;  bottom: &#x27;data&#x27; &quot;
1442        &quot;  top: &#x27;conv1&#x27; &quot;
1443        &quot;  param { &quot;
1444        &quot;    lr_mult: 1 &quot;
1445        &quot;  } &quot;
1446        &quot;  param { &quot;
1447        &quot;    lr_mult: 2 &quot;
1448        &quot;  } &quot;
1449        &quot;  convolution_param { &quot;
1450        &quot;    num_output: 20 &quot;
1451        &quot;    kernel_size: 5 &quot;
1452        &quot;    stride: 1 &quot;
1453        &quot;    weight_filler { &quot;
1454        &quot;      type: &#x27;xavier&#x27; &quot;
1455        &quot;    } &quot;
1456        &quot;    bias_filler { &quot;
1457        &quot;      type: &#x27;constant&#x27; &quot;
1458        &quot;    } &quot;
1459        &quot;  } &quot;
1460        &quot;} &quot;
1461        &quot;layer { &quot;
1462        &quot;  name: &#x27;ip1&#x27; &quot;
1463        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1464        &quot;  bottom: &#x27;conv1&#x27; &quot;
1465        &quot;  top: &#x27;ip1&#x27; &quot;
1466        &quot;  param { &quot;
1467        &quot;    lr_mult: 1 &quot;
1468        &quot;  } &quot;
1469        &quot;  param { &quot;
1470        &quot;    lr_mult: 2 &quot;
1471        &quot;  } &quot;
1472        &quot;  inner_product_param { &quot;
1473        &quot;    num_output: 10 &quot;
1474        &quot;    weight_filler { &quot;
1475        &quot;      type: &#x27;xavier&#x27; &quot;
1476        &quot;    } &quot;
1477        &quot;    bias_filler { &quot;
1478        &quot;      type: &#x27;constant&#x27; &quot;
1479        &quot;    } &quot;
1480        &quot;  } &quot;
1481        &quot;} &quot;
1482        &quot;layer { &quot;
1483        &quot;  name: &#x27;accuracy&#x27; &quot;
1484        &quot;  type: &#x27;Accuracy&#x27; &quot;
1485        &quot;  bottom: &#x27;ip1&#x27; &quot;
1486        &quot;  bottom: &#x27;label&#x27; &quot;
1487        &quot;  top: &#x27;accuracy&#x27; &quot;
1488        &quot;  include: { phase: TEST } &quot;
1489        &quot;} &quot;
1490        &quot;layer { &quot;
1491        &quot;  name: &#x27;loss&#x27; &quot;
1492        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1493        &quot;  bottom: &#x27;ip2&#x27; &quot;
1494        &quot;  bottom: &#x27;label&#x27; &quot;
1495        &quot;  top: &#x27;loss&#x27; &quot;
1496        &quot;} &quot;;
1497    const string input_proto_train = &quot;state: { phase: TRAIN } &quot; + input_proto;
1498    const string input_proto_test = &quot;state: { phase: TEST } &quot; + input_proto;
1499    const string output_proto_train =
1500        &quot;name: &#x27;LeNet&#x27; &quot;
1501        &quot;layer { &quot;
1502        &quot;  name: &#x27;mnist&#x27; &quot;
1503        &quot;  type: &#x27;Data&#x27; &quot;
1504        &quot;  top: &#x27;data&#x27; &quot;
1505        &quot;  top: &#x27;label&#x27; &quot;
1506        &quot;  data_param { &quot;
1507        &quot;    source: &#x27;mnist-train-leveldb&#x27; &quot;
1508        &quot;    batch_size: 64 &quot;
1509        &quot;  } &quot;
1510        &quot;  transform_param { &quot;
1511        &quot;    scale: 0.00390625 &quot;
1512        &quot;  } &quot;
1513        &quot;  include: { phase: TRAIN } &quot;
1514        &quot;} &quot;
1515        &quot;layer { &quot;
1516        &quot;  name: &#x27;conv1&#x27; &quot;
1517        &quot;  type: &#x27;Convolution&#x27; &quot;
1518        &quot;  bottom: &#x27;data&#x27; &quot;
1519        &quot;  top: &#x27;conv1&#x27; &quot;
1520        &quot;  param { &quot;
1521        &quot;    lr_mult: 1 &quot;
1522        &quot;  } &quot;
1523        &quot;  param { &quot;
1524        &quot;    lr_mult: 2 &quot;
1525        &quot;  } &quot;
1526        &quot;  convolution_param { &quot;
1527        &quot;    num_output: 20 &quot;
1528        &quot;    kernel_size: 5 &quot;
1529        &quot;    stride: 1 &quot;
1530        &quot;    weight_filler { &quot;
1531        &quot;      type: &#x27;xavier&#x27; &quot;
1532        &quot;    } &quot;
1533        &quot;    bias_filler { &quot;
1534        &quot;      type: &#x27;constant&#x27; &quot;
1535        &quot;    } &quot;
1536        &quot;  } &quot;
1537        &quot;} &quot;
1538        &quot;layer { &quot;
1539        &quot;  name: &#x27;ip1&#x27; &quot;
1540        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1541        &quot;  bottom: &#x27;conv1&#x27; &quot;
1542        &quot;  top: &#x27;ip1&#x27; &quot;
1543        &quot;  param { &quot;
1544        &quot;    lr_mult: 1 &quot;
1545        &quot;  } &quot;
1546        &quot;  param { &quot;
1547        &quot;    lr_mult: 2 &quot;
1548        &quot;  } &quot;
1549        &quot;  inner_product_param { &quot;
1550        &quot;    num_output: 10 &quot;
1551        &quot;    weight_filler { &quot;
1552        &quot;      type: &#x27;xavier&#x27; &quot;
1553        &quot;    } &quot;
1554        &quot;    bias_filler { &quot;
1555        &quot;      type: &#x27;constant&#x27; &quot;
1556        &quot;    } &quot;
1557        &quot;  } &quot;
1558        &quot;} &quot;
1559        &quot;layer { &quot;
1560        &quot;  name: &#x27;loss&#x27; &quot;
1561        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1562        &quot;  bottom: &#x27;ip2&#x27; &quot;
1563        &quot;  bottom: &#x27;label&#x27; &quot;
1564        &quot;  top: &#x27;loss&#x27; &quot;
1565        &quot;} &quot;;
1566    const string&amp; output_proto_test =
1567        &quot;name: &#x27;LeNet&#x27; &quot;
1568        &quot;layer { &quot;
1569        &quot;  name: &#x27;mnist&#x27; &quot;
1570        &quot;  type: &#x27;Data&#x27; &quot;
1571        &quot;  top: &#x27;data&#x27; &quot;
1572        &quot;  top: &#x27;label&#x27; &quot;
1573        &quot;  data_param { &quot;
1574        &quot;    source: &#x27;mnist-test-leveldb&#x27; &quot;
1575        &quot;    batch_size: 100 &quot;
1576        &quot;  } &quot;
1577        &quot;  transform_param { &quot;
1578        &quot;    scale: 0.00390625 &quot;
1579        &quot;  } &quot;
1580        &quot;  include: { phase: TEST } &quot;
1581        &quot;} &quot;
1582        &quot;layer { &quot;
1583        &quot;  name: &#x27;conv1&#x27; &quot;
1584        &quot;  type: &#x27;Convolution&#x27; &quot;
1585        &quot;  bottom: &#x27;data&#x27; &quot;
1586        &quot;  top: &#x27;conv1&#x27; &quot;
1587        &quot;  param { &quot;
1588        &quot;    lr_mult: 1 &quot;
1589        &quot;  } &quot;
1590        &quot;  param { &quot;
1591        &quot;    lr_mult: 2 &quot;
1592        &quot;  } &quot;
1593        &quot;  convolution_param { &quot;
1594        &quot;    num_output: 20 &quot;
1595        &quot;    kernel_size: 5 &quot;
1596        &quot;    stride: 1 &quot;
1597        &quot;    weight_filler { &quot;
1598        &quot;      type: &#x27;xavier&#x27; &quot;
1599        &quot;    } &quot;
1600        &quot;    bias_filler { &quot;
1601        &quot;      type: &#x27;constant&#x27; &quot;
1602        &quot;    } &quot;
1603        &quot;  } &quot;
1604        &quot;} &quot;
1605        &quot;layer { &quot;
1606        &quot;  name: &#x27;ip1&#x27; &quot;
1607        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1608        &quot;  bottom: &#x27;conv1&#x27; &quot;
1609        &quot;  top: &#x27;ip1&#x27; &quot;
1610        &quot;  param { &quot;
1611        &quot;    lr_mult: 1 &quot;
1612        &quot;  } &quot;
1613        &quot;  param { &quot;
1614        &quot;    lr_mult: 2 &quot;
1615        &quot;  } &quot;
1616        &quot;  inner_product_param { &quot;
1617        &quot;    num_output: 10 &quot;
1618        &quot;    weight_filler { &quot;
1619        &quot;      type: &#x27;xavier&#x27; &quot;
1620        &quot;    } &quot;
1621        &quot;    bias_filler { &quot;
1622        &quot;      type: &#x27;constant&#x27; &quot;
1623        &quot;    } &quot;
1624        &quot;  } &quot;
1625        &quot;} &quot;
1626        &quot;layer { &quot;
1627        &quot;  name: &#x27;accuracy&#x27; &quot;
1628        &quot;  type: &#x27;Accuracy&#x27; &quot;
1629        &quot;  bottom: &#x27;ip1&#x27; &quot;
1630        &quot;  bottom: &#x27;label&#x27; &quot;
1631        &quot;  top: &#x27;accuracy&#x27; &quot;
1632        &quot;  include: { phase: TEST } &quot;
1633        &quot;} &quot;
1634        &quot;layer { &quot;
1635        &quot;  name: &#x27;loss&#x27; &quot;
1636        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1637        &quot;  bottom: &#x27;ip2&#x27; &quot;
1638        &quot;  bottom: &#x27;label&#x27; &quot;
1639        &quot;  top: &#x27;loss&#x27; &quot;
1640        &quot;} &quot;;
1641    const string output_proto_train_explicit =
1642        output_proto_train + &quot; state: { phase: TRAIN } &quot;;
1643    const string output_proto_test_explicit =
1644        output_proto_test + &quot; state: { phase: TEST } &quot;;
1645    this-&gt;RunFilterNetTest(input_proto_train, output_proto_train_explicit);
1646    this-&gt;RunFilterNetTest(input_proto_test, output_proto_test_explicit);
1647  }
1648  TEST_F(FilterNetTest, TestFilterOutByStage) {
1649    const string&amp; input_proto =
1650        &quot;name: &#x27;TestNetwork&#x27; &quot;
1651        &quot;layer { &quot;
1652        &quot;  name: &#x27;data&#x27; &quot;
1653        &quot;  type: &#x27;Data&#x27; &quot;
1654        &quot;  top: &#x27;data&#x27; &quot;
1655        &quot;  top: &#x27;label&#x27; &quot;
1656        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1657        &quot;} &quot;
1658        &quot;layer { &quot;
1659        &quot;  name: &#x27;innerprod&#x27; &quot;
1660        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1661        &quot;  bottom: &#x27;data&#x27; &quot;
1662        &quot;  top: &#x27;innerprod&#x27; &quot;
1663        &quot;} &quot;
1664        &quot;layer { &quot;
1665        &quot;  name: &#x27;loss&#x27; &quot;
1666        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1667        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1668        &quot;  bottom: &#x27;label&#x27; &quot;
1669        &quot;} &quot;;
1670    const string&amp; output_proto =
1671        &quot;name: &#x27;TestNetwork&#x27; &quot;
1672        &quot;layer { &quot;
1673        &quot;  name: &#x27;innerprod&#x27; &quot;
1674        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1675        &quot;  bottom: &#x27;data&#x27; &quot;
1676        &quot;  top: &#x27;innerprod&#x27; &quot;
1677        &quot;} &quot;
1678        &quot;layer { &quot;
1679        &quot;  name: &#x27;loss&#x27; &quot;
1680        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1681        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1682        &quot;  bottom: &#x27;label&#x27; &quot;
1683        &quot;} &quot;;
1684    this-&gt;RunFilterNetTest(input_proto, output_proto);
1685  }
1686  TEST_F(FilterNetTest, TestFilterOutByStage2) {
1687    const string&amp; input_proto =
1688        &quot;name: &#x27;TestNetwork&#x27; &quot;
1689        &quot;layer { &quot;
1690        &quot;  name: &#x27;data&#x27; &quot;
1691        &quot;  type: &#x27;Data&#x27; &quot;
1692        &quot;  top: &#x27;data&#x27; &quot;
1693        &quot;  top: &#x27;label&#x27; &quot;
1694        &quot;} &quot;
1695        &quot;layer { &quot;
1696        &quot;  name: &#x27;innerprod&#x27; &quot;
1697        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1698        &quot;  bottom: &#x27;data&#x27; &quot;
1699        &quot;  top: &#x27;innerprod&#x27; &quot;
1700        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1701        &quot;} &quot;
1702        &quot;layer { &quot;
1703        &quot;  name: &#x27;loss&#x27; &quot;
1704        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1705        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1706        &quot;  bottom: &#x27;label&#x27; &quot;
1707        &quot;} &quot;;
1708    const string&amp; output_proto =
1709        &quot;name: &#x27;TestNetwork&#x27; &quot;
1710        &quot;layer { &quot;
1711        &quot;  name: &#x27;data&#x27; &quot;
1712        &quot;  type: &#x27;Data&#x27; &quot;
1713        &quot;  top: &#x27;data&#x27; &quot;
1714        &quot;  top: &#x27;label&#x27; &quot;
1715        &quot;} &quot;
1716        &quot;layer { &quot;
1717        &quot;  name: &#x27;loss&#x27; &quot;
1718        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1719        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1720        &quot;  bottom: &#x27;label&#x27; &quot;
1721        &quot;} &quot;;
1722    this-&gt;RunFilterNetTest(input_proto, output_proto);
1723  }
1724  TEST_F(FilterNetTest, TestFilterInByStage) {
1725    const string&amp; input_proto =
1726        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1727        &quot;name: &#x27;TestNetwork&#x27; &quot;
1728        &quot;layer { &quot;
1729        &quot;  name: &#x27;data&#x27; &quot;
1730        &quot;  type: &#x27;Data&#x27; &quot;
1731        &quot;  top: &#x27;data&#x27; &quot;
1732        &quot;  top: &#x27;label&#x27; &quot;
1733        &quot;} &quot;
1734        &quot;layer { &quot;
1735        &quot;  name: &#x27;innerprod&#x27; &quot;
1736        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1737        &quot;  bottom: &#x27;data&#x27; &quot;
1738        &quot;  top: &#x27;innerprod&#x27; &quot;
1739        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1740        &quot;} &quot;
1741        &quot;layer { &quot;
1742        &quot;  name: &#x27;loss&#x27; &quot;
1743        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1744        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1745        &quot;  bottom: &#x27;label&#x27; &quot;
1746        &quot;} &quot;;
1747    this-&gt;RunFilterNetTest(input_proto, input_proto);
1748  }
1749  TEST_F(FilterNetTest, TestFilterInByStage2) {
1750    const string&amp; input_proto =
1751        &quot;name: &#x27;TestNetwork&#x27; &quot;
1752        &quot;layer { &quot;
1753        &quot;  name: &#x27;data&#x27; &quot;
1754        &quot;  type: &#x27;Data&#x27; &quot;
1755        &quot;  top: &#x27;data&#x27; &quot;
1756        &quot;  top: &#x27;label&#x27; &quot;
1757        &quot;} &quot;
1758        &quot;layer { &quot;
1759        &quot;  name: &#x27;innerprod&#x27; &quot;
1760        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1761        &quot;  bottom: &#x27;data&#x27; &quot;
1762        &quot;  top: &#x27;innerprod&#x27; &quot;
1763        &quot;  exclude: { stage: &#x27;mystage&#x27; } &quot;
1764        &quot;} &quot;
1765        &quot;layer { &quot;
1766        &quot;  name: &#x27;loss&#x27; &quot;
1767        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1768        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1769        &quot;  bottom: &#x27;label&#x27; &quot;
1770        &quot;} &quot;;
1771    this-&gt;RunFilterNetTest(input_proto, input_proto);
1772  }
1773  TEST_F(FilterNetTest, TestFilterOutByMultipleStage) {
1774    const string&amp; input_proto =
1775        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1776        &quot;name: &#x27;TestNetwork&#x27; &quot;
1777        &quot;layer { &quot;
1778        &quot;  name: &#x27;data&#x27; &quot;
1779        &quot;  type: &#x27;Data&#x27; &quot;
1780        &quot;  top: &#x27;data&#x27; &quot;
1781        &quot;  top: &#x27;label&#x27; &quot;
1782        &quot;} &quot;
1783        &quot;layer { &quot;
1784        &quot;  name: &#x27;innerprod&#x27; &quot;
1785        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1786        &quot;  bottom: &#x27;data&#x27; &quot;
1787        &quot;  top: &#x27;innerprod&#x27; &quot;
1788        &quot;  include: { stage: &#x27;mystage&#x27; stage: &#x27;myotherstage&#x27; } &quot;
1789        &quot;} &quot;
1790        &quot;layer { &quot;
1791        &quot;  name: &#x27;loss&#x27; &quot;
1792        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1793        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1794        &quot;  bottom: &#x27;label&#x27; &quot;
1795        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1796        &quot;} &quot;;
1797    const string&amp; output_proto =
1798        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1799        &quot;name: &#x27;TestNetwork&#x27; &quot;
1800        &quot;layer { &quot;
1801        &quot;  name: &#x27;data&#x27; &quot;
1802        &quot;  type: &#x27;Data&#x27; &quot;
1803        &quot;  top: &#x27;data&#x27; &quot;
1804        &quot;  top: &#x27;label&#x27; &quot;
1805        &quot;} &quot;
1806        &quot;layer { &quot;
1807        &quot;  name: &#x27;loss&#x27; &quot;
1808        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1809        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1810        &quot;  bottom: &#x27;label&#x27; &quot;
1811        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1812        &quot;} &quot;;
1813    this-&gt;RunFilterNetTest(input_proto, output_proto);
1814  }
1815  TEST_F(FilterNetTest, TestFilterInByMultipleStage) {
1816    const string&amp; input_proto =
1817        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1818        &quot;name: &#x27;TestNetwork&#x27; &quot;
1819        &quot;layer { &quot;
1820        &quot;  name: &#x27;data&#x27; &quot;
1821        &quot;  type: &#x27;Data&#x27; &quot;
1822        &quot;  top: &#x27;data&#x27; &quot;
1823        &quot;  top: &#x27;label&#x27; &quot;
1824        &quot;} &quot;
1825        &quot;layer { &quot;
1826        &quot;  name: &#x27;innerprod&#x27; &quot;
1827        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1828        &quot;  bottom: &#x27;data&#x27; &quot;
1829        &quot;  top: &#x27;innerprod&#x27; &quot;
1830        &quot;  include: { stage: &#x27;myotherstage&#x27; } &quot;
1831        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1832        &quot;} &quot;
1833        &quot;layer { &quot;
1834        &quot;  name: &#x27;loss&#x27; &quot;
1835        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1836        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1837        &quot;  bottom: &#x27;label&#x27; &quot;
1838        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1839        &quot;} &quot;;
1840    this-&gt;RunFilterNetTest(input_proto, input_proto);
1841  }
1842  TEST_F(FilterNetTest, TestFilterInByMultipleStage2) {
1843    const string&amp; input_proto =
1844        &quot;state: { stage: &#x27;mystage&#x27; stage: &#x27;myotherstage&#x27; } &quot;
1845        &quot;name: &#x27;TestNetwork&#x27; &quot;
1846        &quot;layer { &quot;
1847        &quot;  name: &#x27;data&#x27; &quot;
1848        &quot;  type: &#x27;Data&#x27; &quot;
1849        &quot;  top: &#x27;data&#x27; &quot;
1850        &quot;  top: &#x27;label&#x27; &quot;
1851        &quot;} &quot;
1852        &quot;layer { &quot;
1853        &quot;  name: &#x27;innerprod&#x27; &quot;
1854        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1855        &quot;  bottom: &#x27;data&#x27; &quot;
1856        &quot;  top: &#x27;innerprod&#x27; &quot;
1857        &quot;  include: { stage: &#x27;mystage&#x27; stage: &#x27;myotherstage&#x27; } &quot;
1858        &quot;} &quot;
1859        &quot;layer { &quot;
1860        &quot;  name: &#x27;loss&#x27; &quot;
1861        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1862        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1863        &quot;  bottom: &#x27;label&#x27; &quot;
1864        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1865        &quot;} &quot;;
1866    this-&gt;RunFilterNetTest(input_proto, input_proto);
1867  }
1868  TEST_F(FilterNetTest, TestFilterInByNotStage) {
1869    const string&amp; input_proto =
1870        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1871        &quot;name: &#x27;TestNetwork&#x27; &quot;
1872        &quot;layer { &quot;
1873        &quot;  name: &#x27;data&#x27; &quot;
1874        &quot;  type: &#x27;Data&#x27; &quot;
1875        &quot;  top: &#x27;data&#x27; &quot;
1876        &quot;  top: &#x27;label&#x27; &quot;
1877        &quot;} &quot;
1878        &quot;layer { &quot;
1879        &quot;  name: &#x27;innerprod&#x27; &quot;
1880        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1881        &quot;  bottom: &#x27;data&#x27; &quot;
1882        &quot;  top: &#x27;innerprod&#x27; &quot;
1883        &quot;  include: { not_stage: &#x27;myotherstage&#x27; } &quot;
1884        &quot;} &quot;
1885        &quot;layer { &quot;
1886        &quot;  name: &#x27;loss&#x27; &quot;
1887        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1888        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1889        &quot;  bottom: &#x27;label&#x27; &quot;
1890        &quot;  include: { not_stage: &#x27;myotherstage&#x27; } &quot;
1891        &quot;} &quot;;
1892    this-&gt;RunFilterNetTest(input_proto, input_proto);
1893  }
1894  TEST_F(FilterNetTest, TestFilterOutByNotStage) {
1895    const string&amp; input_proto =
1896        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1897        &quot;name: &#x27;TestNetwork&#x27; &quot;
1898        &quot;layer { &quot;
1899        &quot;  name: &#x27;data&#x27; &quot;
1900        &quot;  type: &#x27;Data&#x27; &quot;
1901        &quot;  top: &#x27;data&#x27; &quot;
1902        &quot;  top: &#x27;label&#x27; &quot;
1903        &quot;} &quot;
1904        &quot;layer { &quot;
1905        &quot;  name: &#x27;innerprod&#x27; &quot;
1906        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1907        &quot;  bottom: &#x27;data&#x27; &quot;
1908        &quot;  top: &#x27;innerprod&#x27; &quot;
1909        &quot;  include: { not_stage: &#x27;mystage&#x27; } &quot;
1910        &quot;} &quot;
1911        &quot;layer { &quot;
1912        &quot;  name: &#x27;loss&#x27; &quot;
1913        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1914        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1915        &quot;  bottom: &#x27;label&#x27; &quot;
1916        &quot;  include: { not_stage: &#x27;mystage&#x27; } &quot;
1917        &quot;} &quot;;
1918    const string&amp; output_proto =
1919        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1920        &quot;name: &#x27;TestNetwork&#x27; &quot;
1921        &quot;layer { &quot;
1922        &quot;  name: &#x27;data&#x27; &quot;
1923        &quot;  type: &#x27;Data&#x27; &quot;
1924        &quot;  top: &#x27;data&#x27; &quot;
1925        &quot;  top: &#x27;label&#x27; &quot;
1926        &quot;} &quot;;
1927    this-&gt;RunFilterNetTest(input_proto, output_proto);
1928  }
1929  TEST_F(FilterNetTest, TestFilterOutByMinLevel) {
1930    const string&amp; input_proto =
1931        &quot;name: &#x27;TestNetwork&#x27; &quot;
1932        &quot;layer { &quot;
1933        &quot;  name: &#x27;data&#x27; &quot;
1934        &quot;  type: &#x27;Data&#x27; &quot;
1935        &quot;  top: &#x27;data&#x27; &quot;
1936        &quot;  top: &#x27;label&#x27; &quot;
1937        &quot;} &quot;
1938        &quot;layer { &quot;
1939        &quot;  name: &#x27;innerprod&#x27; &quot;
1940        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1941        &quot;  bottom: &#x27;data&#x27; &quot;
1942        &quot;  top: &#x27;innerprod&#x27; &quot;
1943        &quot;  include: { min_level: 3 } &quot;
1944        &quot;} &quot;
1945        &quot;layer { &quot;
1946        &quot;  name: &#x27;loss&#x27; &quot;
1947        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1948        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1949        &quot;  bottom: &#x27;label&#x27; &quot;
1950        &quot;} &quot;;
1951    const string&amp; output_proto =
1952        &quot;name: &#x27;TestNetwork&#x27; &quot;
1953        &quot;layer { &quot;
1954        &quot;  name: &#x27;data&#x27; &quot;
1955        &quot;  type: &#x27;Data&#x27; &quot;
1956        &quot;  top: &#x27;data&#x27; &quot;
1957        &quot;  top: &#x27;label&#x27; &quot;
1958        &quot;} &quot;
1959        &quot;layer { &quot;
1960        &quot;  name: &#x27;loss&#x27; &quot;
1961        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1962        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1963        &quot;  bottom: &#x27;label&#x27; &quot;
1964        &quot;} &quot;;
1965    this-&gt;RunFilterNetTest(input_proto, output_proto);
1966  }
1967  TEST_F(FilterNetTest, TestFilterOutByMaxLevel) {
1968    const string&amp; input_proto =
1969        &quot;name: &#x27;TestNetwork&#x27; &quot;
1970        &quot;layer { &quot;
1971        &quot;  name: &#x27;data&#x27; &quot;
1972        &quot;  type: &#x27;Data&#x27; &quot;
1973        &quot;  top: &#x27;data&#x27; &quot;
1974        &quot;  top: &#x27;label&#x27; &quot;
1975        &quot;} &quot;
1976        &quot;layer { &quot;
1977        &quot;  name: &#x27;innerprod&#x27; &quot;
1978        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1979        &quot;  bottom: &#x27;data&#x27; &quot;
1980        &quot;  top: &#x27;innerprod&#x27; &quot;
1981        &quot;  include: { max_level: -3 } &quot;
1982        &quot;} &quot;
1983        &quot;layer { &quot;
1984        &quot;  name: &#x27;loss&#x27; &quot;
1985        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1986        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1987        &quot;  bottom: &#x27;label&#x27; &quot;
1988        &quot;} &quot;;
1989    const string&amp; output_proto =
1990        &quot;name: &#x27;TestNetwork&#x27; &quot;
1991        &quot;layer { &quot;
1992        &quot;  name: &#x27;data&#x27; &quot;
1993        &quot;  type: &#x27;Data&#x27; &quot;
1994        &quot;  top: &#x27;data&#x27; &quot;
1995        &quot;  top: &#x27;label&#x27; &quot;
1996        &quot;} &quot;
1997        &quot;layer { &quot;
1998        &quot;  name: &#x27;loss&#x27; &quot;
1999        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2000        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2001        &quot;  bottom: &#x27;label&#x27; &quot;
2002        &quot;} &quot;;
2003    this-&gt;RunFilterNetTest(input_proto, output_proto);
2004  }
2005  TEST_F(FilterNetTest, TestFilterInByMinLevel) {
2006    const string&amp; input_proto =
2007        &quot;name: &#x27;TestNetwork&#x27; &quot;
2008        &quot;layer { &quot;
2009        &quot;  name: &#x27;data&#x27; &quot;
2010        &quot;  type: &#x27;Data&#x27; &quot;
2011        &quot;  top: &#x27;data&#x27; &quot;
2012        &quot;  top: &#x27;label&#x27; &quot;
2013        &quot;} &quot;
2014        &quot;layer { &quot;
2015        &quot;  name: &#x27;innerprod&#x27; &quot;
2016        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2017        &quot;  bottom: &#x27;data&#x27; &quot;
2018        &quot;  top: &#x27;innerprod&#x27; &quot;
2019        &quot;  include: { min_level: 0 } &quot;
2020        &quot;} &quot;
2021        &quot;layer { &quot;
2022        &quot;  name: &#x27;loss&#x27; &quot;
2023        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2024        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2025        &quot;  bottom: &#x27;label&#x27; &quot;
2026        &quot;} &quot;;
2027    this-&gt;RunFilterNetTest(input_proto, input_proto);
2028  }
2029  TEST_F(FilterNetTest, TestFilterInByMinLevel2) {
2030    const string&amp; input_proto =
2031        &quot;state: { level: 7 } &quot;
2032        &quot;name: &#x27;TestNetwork&#x27; &quot;
2033        &quot;layer { &quot;
2034        &quot;  name: &#x27;data&#x27; &quot;
2035        &quot;  type: &#x27;Data&#x27; &quot;
2036        &quot;  top: &#x27;data&#x27; &quot;
2037        &quot;  top: &#x27;label&#x27; &quot;
2038        &quot;} &quot;
2039        &quot;layer { &quot;
2040        &quot;  name: &#x27;innerprod&#x27; &quot;
2041        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2042        &quot;  bottom: &#x27;data&#x27; &quot;
2043        &quot;  top: &#x27;innerprod&#x27; &quot;
2044        &quot;  include: { min_level: 3 } &quot;
2045        &quot;} &quot;
2046        &quot;layer { &quot;
2047        &quot;  name: &#x27;loss&#x27; &quot;
2048        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2049        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2050        &quot;  bottom: &#x27;label&#x27; &quot;
2051        &quot;} &quot;;
2052    this-&gt;RunFilterNetTest(input_proto, input_proto);
2053  }
2054  TEST_F(FilterNetTest, TestFilterInByMaxLevel) {
2055    const string&amp; input_proto =
2056        &quot;name: &#x27;TestNetwork&#x27; &quot;
2057        &quot;layer { &quot;
2058        &quot;  name: &#x27;data&#x27; &quot;
2059        &quot;  type: &#x27;Data&#x27; &quot;
2060        &quot;  top: &#x27;data&#x27; &quot;
2061        &quot;  top: &#x27;label&#x27; &quot;
2062        &quot;} &quot;
2063        &quot;layer { &quot;
2064        &quot;  name: &#x27;innerprod&#x27; &quot;
2065        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2066        &quot;  bottom: &#x27;data&#x27; &quot;
2067        &quot;  top: &#x27;innerprod&#x27; &quot;
2068        &quot;  include: { max_level: 0 } &quot;
2069        &quot;} &quot;
2070        &quot;layer { &quot;
2071        &quot;  name: &#x27;loss&#x27; &quot;
2072        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2073        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2074        &quot;  bottom: &#x27;label&#x27; &quot;
2075        &quot;} &quot;;
2076    this-&gt;RunFilterNetTest(input_proto, input_proto);
2077  }
2078  TEST_F(FilterNetTest, TestFilterInByMaxLevel2) {
2079    const string&amp; input_proto =
2080        &quot;state: { level: -7 } &quot;
2081        &quot;name: &#x27;TestNetwork&#x27; &quot;
2082        &quot;layer { &quot;
2083        &quot;  name: &#x27;data&#x27; &quot;
2084        &quot;  type: &#x27;Data&#x27; &quot;
2085        &quot;  top: &#x27;data&#x27; &quot;
2086        &quot;  top: &#x27;label&#x27; &quot;
2087        &quot;} &quot;
2088        &quot;layer { &quot;
2089        &quot;  name: &#x27;innerprod&#x27; &quot;
2090        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2091        &quot;  bottom: &#x27;data&#x27; &quot;
2092        &quot;  top: &#x27;innerprod&#x27; &quot;
2093        &quot;  include: { max_level: -3 } &quot;
2094        &quot;} &quot;
2095        &quot;layer { &quot;
2096        &quot;  name: &#x27;loss&#x27; &quot;
2097        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2098        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2099        &quot;  bottom: &#x27;label&#x27; &quot;
2100        &quot;} &quot;;
2101    this-&gt;RunFilterNetTest(input_proto, input_proto);
2102  }
2103  TEST_F(FilterNetTest, TestFilterInOutByIncludeMultiRule) {
2104    const string&amp; input_proto =
2105        &quot;name: &#x27;TestNetwork&#x27; &quot;
2106        &quot;layer { &quot;
2107        &quot;  name: &#x27;data&#x27; &quot;
2108        &quot;  type: &#x27;Data&#x27; &quot;
2109        &quot;  top: &#x27;data&#x27; &quot;
2110        &quot;  top: &#x27;label&#x27; &quot;
2111        &quot;} &quot;
2112        &quot;layer { &quot;
2113        &quot;  name: &#x27;innerprod&#x27; &quot;
2114        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2115        &quot;  bottom: &#x27;data&#x27; &quot;
2116        &quot;  top: &#x27;innerprod&#x27; &quot;
2117        &quot;  include: { min_level: 2  phase: TRAIN } &quot;
2118        &quot;} &quot;
2119        &quot;layer { &quot;
2120        &quot;  name: &#x27;loss&#x27; &quot;
2121        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2122        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2123        &quot;  bottom: &#x27;label&#x27; &quot;
2124        &quot;  include: { min_level: 2  phase: TEST } &quot;
2125        &quot;} &quot;;
2126    const string&amp; input_proto_train =
2127        &quot;state: { level: 4  phase: TRAIN } &quot; + input_proto;
2128    const string&amp; input_proto_test =
2129        &quot;state: { level: 4  phase: TEST } &quot; + input_proto;
2130    const string&amp; output_proto_train =
2131        &quot;state: { level: 4  phase: TRAIN } &quot;
2132        &quot;name: &#x27;TestNetwork&#x27; &quot;
2133        &quot;layer { &quot;
2134        &quot;  name: &#x27;data&#x27; &quot;
2135        &quot;  type: &#x27;Data&#x27; &quot;
2136        &quot;  top: &#x27;data&#x27; &quot;
2137        &quot;  top: &#x27;label&#x27; &quot;
2138        &quot;} &quot;
2139        &quot;layer { &quot;
2140        &quot;  name: &#x27;innerprod&#x27; &quot;
2141        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2142        &quot;  bottom: &#x27;data&#x27; &quot;
2143        &quot;  top: &#x27;innerprod&#x27; &quot;
2144        &quot;  include: { min_level: 2  phase: TRAIN } &quot;
2145        &quot;} &quot;;
2146    const string&amp; output_proto_test =
2147        &quot;state: { level: 4  phase: TEST } &quot;
2148        &quot;name: &#x27;TestNetwork&#x27; &quot;
2149        &quot;layer { &quot;
2150        &quot;  name: &#x27;data&#x27; &quot;
2151        &quot;  type: &#x27;Data&#x27; &quot;
2152        &quot;  top: &#x27;data&#x27; &quot;
2153        &quot;  top: &#x27;label&#x27; &quot;
2154        &quot;} &quot;
2155        &quot;layer { &quot;
2156        &quot;  name: &#x27;loss&#x27; &quot;
2157        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2158        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2159        &quot;  bottom: &#x27;label&#x27; &quot;
2160        &quot;  include: { min_level: 2  phase: TEST } &quot;
2161        &quot;} &quot;;
2162    this-&gt;RunFilterNetTest(input_proto_train, output_proto_train);
2163    this-&gt;RunFilterNetTest(input_proto_test, output_proto_test);
2164  }
2165  TEST_F(FilterNetTest, TestFilterInByIncludeMultiRule) {
2166    const string&amp; input_proto =
2167        &quot;name: &#x27;TestNetwork&#x27; &quot;
2168        &quot;layer { &quot;
2169        &quot;  name: &#x27;data&#x27; &quot;
2170        &quot;  type: &#x27;Data&#x27; &quot;
2171        &quot;  top: &#x27;data&#x27; &quot;
2172        &quot;  top: &#x27;label&#x27; &quot;
2173        &quot;} &quot;
2174        &quot;layer { &quot;
2175        &quot;  name: &#x27;innerprod&#x27; &quot;
2176        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2177        &quot;  bottom: &#x27;data&#x27; &quot;
2178        &quot;  top: &#x27;innerprod&#x27; &quot;
2179        &quot;  include: { min_level: 2  phase: TRAIN } &quot;
2180        &quot;  include: { phase: TEST } &quot;
2181        &quot;} &quot;
2182        &quot;layer { &quot;
2183        &quot;  name: &#x27;loss&#x27; &quot;
2184        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2185        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2186        &quot;  bottom: &#x27;label&#x27; &quot;
2187        &quot;  include: { min_level: 2  phase: TEST } &quot;
2188        &quot;  include: { phase: TRAIN } &quot;
2189        &quot;} &quot;;
2190    const string&amp; input_proto_train =
2191        &quot;state: { level: 2  phase: TRAIN } &quot; + input_proto;
2192    const string&amp; input_proto_test =
2193        &quot;state: { level: 2  phase: TEST } &quot; + input_proto;
2194    this-&gt;RunFilterNetTest(input_proto_train, input_proto_train);
2195    this-&gt;RunFilterNetTest(input_proto_test, input_proto_test);
2196  }
2197  TEST_F(FilterNetTest, TestFilterInOutByExcludeMultiRule) {
2198    const string&amp; input_proto =
2199        &quot;name: &#x27;TestNetwork&#x27; &quot;
2200        &quot;layer { &quot;
2201        &quot;  name: &#x27;data&#x27; &quot;
2202        &quot;  type: &#x27;Data&#x27; &quot;
2203        &quot;  top: &#x27;data&#x27; &quot;
2204        &quot;  top: &#x27;label&#x27; &quot;
2205        &quot;} &quot;
2206        &quot;layer { &quot;
2207        &quot;  name: &#x27;innerprod&#x27; &quot;
2208        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2209        &quot;  bottom: &#x27;data&#x27; &quot;
2210        &quot;  top: &#x27;innerprod&#x27; &quot;
2211        &quot;  exclude: { min_level: 2  phase: TRAIN } &quot;
2212        &quot;} &quot;
2213        &quot;layer { &quot;
2214        &quot;  name: &#x27;loss&#x27; &quot;
2215        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2216        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2217        &quot;  bottom: &#x27;label&#x27; &quot;
2218        &quot;  exclude: { min_level: 2  phase: TEST } &quot;
2219        &quot;} &quot;;
2220    const string&amp; input_proto_train =
2221        &quot;state: { level: 4  phase: TRAIN } &quot; + input_proto;
2222    const string&amp; input_proto_test =
2223        &quot;state: { level: 4  phase: TEST } &quot; + input_proto;
2224    const string&amp; output_proto_train =
2225        &quot;state: { level: 4  phase: TRAIN } &quot;
2226        &quot;name: &#x27;TestNetwork&#x27; &quot;
2227        &quot;layer { &quot;
2228        &quot;  name: &#x27;data&#x27; &quot;
2229        &quot;  type: &#x27;Data&#x27; &quot;
2230        &quot;  top: &#x27;data&#x27; &quot;
2231        &quot;  top: &#x27;label&#x27; &quot;
2232        &quot;} &quot;
2233        &quot;layer { &quot;
2234        &quot;  name: &#x27;loss&#x27; &quot;
2235        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2236        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2237        &quot;  bottom: &#x27;label&#x27; &quot;
2238        &quot;  exclude: { min_level: 2  phase: TEST } &quot;
2239        &quot;} &quot;;
2240    const string&amp; output_proto_test =
2241        &quot;state: { level: 4  phase: TEST } &quot;
2242        &quot;name: &#x27;TestNetwork&#x27; &quot;
2243        &quot;layer { &quot;
2244        &quot;  name: &#x27;data&#x27; &quot;
2245        &quot;  type: &#x27;Data&#x27; &quot;
2246        &quot;  top: &#x27;data&#x27; &quot;
2247        &quot;  top: &#x27;label&#x27; &quot;
2248        &quot;} &quot;
2249        &quot;layer { &quot;
2250        &quot;  name: &#x27;innerprod&#x27; &quot;
2251        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2252        &quot;  bottom: &#x27;data&#x27; &quot;
2253        &quot;  top: &#x27;innerprod&#x27; &quot;
2254        &quot;  exclude: { min_level: 2  phase: TRAIN } &quot;
2255        &quot;} &quot;;
2256    this-&gt;RunFilterNetTest(input_proto_train, output_proto_train);
2257    this-&gt;RunFilterNetTest(input_proto_test, output_proto_test);
2258  }
2259  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
2260  TYPED_TEST(NetTest, TestReshape) {
2261    typedef typename TypeParam::Dtype Dtype;
2262    Caffe::set_random_seed(this-&gt;seed_);
2263    Caffe::set_mode(Caffe::CPU);
2264    FillerParameter filler_param;
2265    filler_param.set_std(1);
2266    GaussianFiller&lt;Dtype&gt; filler(filler_param);
2267    Blob&lt;Dtype&gt; blob1(2, 3, 12, 10);
2268    Blob&lt;Dtype&gt; blob2(4, 3, 9, 11);
2269    ASSERT_LT(blob1.count(), blob2.count());
2270    filler.Fill(&amp;blob1);
2271    filler.Fill(&amp;blob2);
2272    this-&gt;InitReshapableNet();
2273    shared_ptr&lt;Blob&lt;Dtype&gt; &gt; input_blob = this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
2274    Blob&lt;Dtype&gt;* output_blob = this-&gt;net_-&gt;output_blobs()[0];
2275    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2276        blob1.width());
2277    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2278    this-&gt;net_-&gt;Forward();
2279    this-&gt;net_-&gt;Backward();
2280    Blob&lt;Dtype&gt; output1(output_blob-&gt;num(), output_blob-&gt;channels(),
2281        output_blob-&gt;height(), output_blob-&gt;width());
2282    caffe_copy(output1.count(), output_blob-&gt;cpu_data(),
2283        output1.mutable_cpu_data());
2284    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2285        blob2.width());
2286    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2287    this-&gt;net_-&gt;Forward();
2288    this-&gt;net_-&gt;Backward();
2289    Blob&lt;Dtype&gt; output2(output_blob-&gt;num(), output_blob-&gt;channels(),
2290        output_blob-&gt;height(), output_blob-&gt;width());
2291    caffe_copy(output2.count(), output_blob-&gt;cpu_data(),
2292        output2.mutable_cpu_data());
2293    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2294        blob1.width());
2295    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2296    this-&gt;net_-&gt;Forward();
2297    this-&gt;net_-&gt;Backward();
2298    for (int i = 0; i &lt; output1.count(); ++i) {
2299      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2300    }
2301    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2302        blob2.width());
2303    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2304    this-&gt;net_-&gt;Forward();
2305    this-&gt;net_-&gt;Backward();
2306    for (int i = 0; i &lt; output2.count(); ++i) {
2307      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2308    }
2309    EXPECT_EQ(output1.num(), blob1.num());
2310    EXPECT_EQ(output2.num(), blob2.num());
2311    bool same_spatial_shape = true;
2312    const int kFirstSpatialAxis = 2;
2313    for (int i = kFirstSpatialAxis; i &lt; output1.num_axes(); ++i) {
2314      if (output1.shape(i) != output2.shape(i)) {
2315        same_spatial_shape = false;
2316        break;
2317      }
2318    }
2319    EXPECT_FALSE(same_spatial_shape);
2320  }
2321  #endif
2322  #ifdef MKL2017_SUPPORTED
2323  TYPED_TEST(NetTestCPU, TestForwardReshapeForward) {
2324    typedef TypeParam Dtype;
2325    const string&amp; proto =
2326        &quot;name: &#x27;TestNetwork&#x27; &quot;
2327        &quot; layer {&quot;
2328        &quot;   top: &#x27;data&#x27;&quot;
2329        &quot;   top: &#x27;label&#x27;&quot;
2330        &quot;   name: &#x27;data&#x27;&quot;
2331        &quot;   type: &#x27;DummyData&#x27;&quot;
2332        &quot;   dummy_data_param {&quot;
2333        &quot;     shape: { dim: 32 dim: 3 dim: 227 dim: 227 }&quot;
2334        &quot;     data_filler {&quot;
2335        &quot;       type: &#x27;constant&#x27;&quot;
2336        &quot;       value: 0.01&quot;
2337        &quot;     }&quot;
2338        &quot;   }&quot;
2339        &quot;   transform_param {&quot;
2340        &quot;     mirror: true&quot;
2341        &quot;     crop_size: 224&quot;
2342        &quot;     mean_value: 104&quot;
2343        &quot;     mean_value: 117&quot;
2344        &quot;     mean_value: 123&quot;
2345        &quot;   }&quot;
2346        &quot; }&quot;
2347        &quot; layer {&quot;
2348        &quot;  bottom: &#x27;data&#x27;&quot;
2349        &quot;   top: &#x27;conv&#x27;&quot;
2350        &quot;   name: &#x27;conv1&#x27;&quot;
2351        &quot;   type: &#x27;Convolution&#x27;&quot;
2352        &quot;   param {&quot;
2353        &quot;     lr_mult: 1&quot;
2354        &quot;     decay_mult: 1&quot;
2355        &quot;   }&quot;
2356        &quot;   convolution_param {&quot;
2357        &quot;     &quot;
2358        &quot;     num_output: 64&quot;
2359        &quot;     engine: MKL2017 &quot;
2360        &quot;     pad: 3&quot;
2361        &quot;     kernel_size: 7&quot;
2362        &quot;     stride: 2&quot;
2363        &quot;     weight_filler {&quot;
2364        &quot;       type: &#x27;xavier&#x27;&quot;
2365        &quot;     }&quot;
2366        &quot;     bias_term: false&quot;
2367        &quot;   }&quot;
2368        &quot; }&quot;
2369        &quot; layer {&quot;
2370        &quot;   bottom: &#x27;conv&#x27;&quot;
2371        &quot;   top: &#x27;relu1&#x27;&quot;
2372        &quot;   name: &#x27;relu1&#x27;&quot;
2373        &quot;   type: &#x27;ReLU&#x27;&quot;
2374        &quot;   relu_param {&quot;
2375        &quot;     engine: MKL2017 &quot;
2376        &quot;     &quot;
2377        &quot;   }&quot;
2378        &quot; }&quot;
2379        &quot; layer {&quot;
2380        &quot;   bottom: &#x27;conv&#x27;&quot;
2381        &quot;   top: &#x27;relu2&#x27;&quot;
2382        &quot;   name: &#x27;relu2&#x27;&quot;
2383        &quot;   type: &#x27;ReLU&#x27;&quot;
2384        &quot;   relu_param {&quot;
2385        &quot;     engine: MKL2017 &quot;
2386        &quot;     &quot;
2387        &quot;   }&quot;
2388        &quot; }&quot;
2389        &quot; layer {&quot;
2390        &quot;   bottom: &#x27;relu1&#x27;&quot;
2391        &quot;   bottom: &#x27;relu2&#x27;&quot;
2392        &quot;   top: &#x27;concat&#x27;&quot;
2393        &quot;   name: &#x27;concat&#x27;&quot;
2394        &quot;   type: &#x27;Concat&#x27;&quot;
2395        &quot;   concat_param {&quot;
2396        &quot;     engine: MKL2017 &quot;
2397        &quot;     &quot;
2398        &quot;   }&quot;
2399        &quot; } &quot;
2400        &quot; layer {&quot;
2401        &quot;   bottom: &#x27;concat&#x27;&quot;
2402        &quot;   top: &#x27;lrn&#x27;&quot;
2403        &quot;   name: &#x27;LRN&#x27;&quot;
2404        &quot;   type: &#x27;LRN&#x27;&quot;
2405        &quot;   lrn_param {&quot;
2406        &quot;     engine: MKL2017 &quot;
2407        &quot;     local_size: 5&quot;
2408        &quot;     alpha: 0.0001&quot;
2409        &quot;     beta: 0.75&quot;
2410        &quot;   }&quot;
2411        &quot; }&quot;
2412        &quot; layer {&quot;
2413        &quot;   bottom: &#x27;lrn&#x27;&quot;
2414        &quot;   top: &#x27;pooling&#x27;&quot;
2415        &quot;   name: &#x27;Pooling&#x27;&quot;
2416        &quot;   type: &#x27;Pooling&#x27;&quot;
2417        &quot;   pooling_param {&quot;
2418        &quot;     engine: MKL2017 &quot;
2419        &quot;     kernel_size: 5&quot;
2420        &quot;     stride: 2&quot;
2421        &quot;     pool: MAX&quot;
2422        &quot;   }&quot;
2423        &quot; }&quot;
2424        &quot; layer {&quot;
2425        &quot;   bottom: &#x27;pooling&#x27;&quot;
2426        &quot;   top: &#x27;bn&#x27;&quot;
2427        &quot;   name: &#x27;BatchNorm&#x27;&quot;
2428        &quot;   type: &#x27;BatchNorm&#x27;&quot;
2429        &quot;   batch_norm_param {&quot;
2430        &quot;     engine: MKL2017 &quot;
2431        &quot;   }&quot;
2432        &quot; }&quot;;
2433      this-&gt;InitNetFromProtoString(proto);
2434      this-&gt;net_-&gt;Forward();
2435      shared_ptr&lt;Blob&lt;Dtype&gt; &gt; input_blob = this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
2436      input_blob-&gt;Reshape(1, 3, 1280, 720);
2437      this-&gt;net_-&gt;Forward();
2438  }
2439  #if 0
2440  TYPED_TEST(NetTest, TestTotalForwardReshape) {
2441    typedef typename TypeParam::Dtype Dtype;
2442    Caffe::set_random_seed(this-&gt;seed_);
2443    Caffe::set_mode(Caffe::CPU);
2444    FillerParameter filler_param;
2445    filler_param.set_std(1);
2446    GaussianFiller&lt;Dtype&gt; filler(filler_param);
2447    Blob&lt;Dtype&gt; blob1(2, 3, 12, 10);
2448    Blob&lt;Dtype&gt; blob2(4, 3, 9, 11);
2449    ASSERT_LT(blob1.count(), blob2.count());
2450    filler.Fill(&amp;blob1);
2451    filler.Fill(&amp;blob2);
2452    const string&amp; proto =
2453        &quot;name: &#x27;TestNetwork&#x27; &quot;
2454        &quot; layer {&quot;
2455        &quot;   top: &#x27;data&#x27;&quot;
2456        &quot;   top: &#x27;label&#x27;&quot;
2457        &quot;   name: &#x27;data&#x27;&quot;
2458        &quot;   type: &#x27;DummyData&#x27;&quot;
2459        &quot;   dummy_data_param {&quot;
2460        &quot;     shape: { dim: 3 dim: 3 dim: 13 dim: 11 }&quot;
2461        &quot;     data_filler {&quot;
2462        &quot;       type: &#x27;constant&#x27;&quot;
2463        &quot;       value: 0.01&quot;
2464        &quot;     }&quot;
2465        &quot;   }&quot;
2466        &quot;   transform_param {&quot;
2467        &quot;     mirror: true&quot;
2468        &quot;     crop_size: 224&quot;
2469        &quot;     mean_value: 104&quot;
2470        &quot;     mean_value: 117&quot;
2471        &quot;     mean_value: 123&quot;
2472        &quot;   }&quot;
2473        &quot; }&quot;
2474        &quot; layer {&quot;
2475        &quot;  bottom: &#x27;data&#x27;&quot;
2476        &quot;   top: &#x27;conv&#x27;&quot;
2477        &quot;   name: &#x27;conv1&#x27;&quot;
2478        &quot;   type: &#x27;Convolution&#x27;&quot;
2479        &quot;   param {&quot;
2480        &quot;     lr_mult: 1&quot;
2481        &quot;     decay_mult: 1&quot;
2482        &quot;   }&quot;
2483        &quot;   convolution_param {&quot;
2484        &quot;     &quot;
2485        &quot;     num_output: 64&quot;
2486        &quot;     engine: MKL2017 &quot;
2487        &quot;     pad: 3&quot;
2488        &quot;     kernel_size: 7&quot;
2489        &quot;     stride: 2&quot;
2490        &quot;     weight_filler {&quot;
2491        &quot;       type: &#x27;xavier&#x27;&quot;
2492        &quot;     }&quot;
2493        &quot;     bias_term: false&quot;
2494        &quot;   }&quot;
2495        &quot; }&quot;
2496        &quot; layer {&quot;
2497        &quot;   bottom: &#x27;conv&#x27;&quot;
2498        &quot;   top: &#x27;relu1&#x27;&quot;
2499        &quot;   name: &#x27;relu1&#x27;&quot;
2500        &quot;   type: &#x27;ReLU&#x27;&quot;
2501        &quot;   relu_param {&quot;
2502        &quot;     engine: MKL2017 &quot;
2503        &quot;     &quot;
2504        &quot;   }&quot;
2505        &quot; }&quot;
2506        &quot; layer {&quot;
2507        &quot;   bottom: &#x27;conv&#x27;&quot;
2508        &quot;   top: &#x27;relu2&#x27;&quot;
2509        &quot;   name: &#x27;relu2&#x27;&quot;
2510        &quot;   type: &#x27;ReLU&#x27;&quot;
2511        &quot;   relu_param {&quot;
2512        &quot;     engine: MKL2017 &quot;
2513        &quot;     &quot;
2514        &quot;   }&quot;
2515        &quot; }&quot;
2516        &quot; layer {&quot;
2517        &quot;   bottom: &#x27;relu1&#x27;&quot;
2518        &quot;   bottom: &#x27;relu2&#x27;&quot;
2519        &quot;   top: &#x27;concat&#x27;&quot;
2520        &quot;   name: &#x27;concat&#x27;&quot;
2521        &quot;   type: &#x27;Concat&#x27;&quot;
2522        &quot;   concat_param {&quot;
2523        &quot;     engine: MKL2017 &quot;
2524        &quot;     &quot;
2525        &quot;   }&quot;
2526        &quot; } &quot;
2527        &quot; layer {&quot;
2528        &quot;   bottom: &#x27;concat&#x27;&quot;
2529        &quot;   top: &#x27;lrn&#x27;&quot;
2530        &quot;   name: &#x27;LRN&#x27;&quot;
2531        &quot;   type: &#x27;LRN&#x27;&quot;
2532        &quot;   lrn_param {&quot;
2533        &quot;     engine: MKL2017 &quot;
2534        &quot;     local_size: 5&quot;
2535        &quot;     alpha: 0.0001&quot;
2536        &quot;     beta: 0.75&quot;
2537        &quot;   }&quot;
2538        &quot; }&quot;
2539        &quot; layer {&quot;
2540        &quot;   bottom: &#x27;lrn&#x27;&quot;
2541        &quot;   top: &#x27;pooling&#x27;&quot;
2542        &quot;   name: &#x27;Pooling&#x27;&quot;
2543        &quot;   type: &#x27;Pooling&#x27;&quot;
2544        &quot;   pooling_param {&quot;
2545        &quot;     engine: MKL2017 &quot;
2546        &quot;     kernel_size: 5&quot;
2547        &quot;     stride: 2&quot;
2548        &quot;     pool: MAX&quot;
2549        &quot;   }&quot;
2550        &quot; }&quot;
2551        &quot; layer {&quot;
2552        &quot;   bottom: &#x27;pooling&#x27;&quot;
2553        &quot;   top: &#x27;bn&#x27;&quot;
2554        &quot;   name: &#x27;BatchNorm&#x27;&quot;
2555        &quot;   type: &#x27;BatchNorm&#x27;&quot;
2556        &quot;   batch_norm_param {&quot;
2557        &quot;     engine: MKL2017 &quot;
2558        &quot;   }&quot;
2559        &quot; }&quot;;
2560      this-&gt;InitNetFromProtoString(proto);
2561    shared_ptr&lt;Blob&lt;Dtype&gt; &gt; input_blob = this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
2562    Blob&lt;Dtype&gt;* output_blob = this-&gt;net_-&gt;output_blobs()[0];
2563    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2564        blob1.width());
2565    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2566    this-&gt;net_-&gt;Forward();
2567    this-&gt;net_-&gt;Backward();
2568    Blob&lt;Dtype&gt; output1(output_blob-&gt;num(), output_blob-&gt;channels(),
2569        output_blob-&gt;height(), output_blob-&gt;width());
2570    caffe_copy(output1.count(), output_blob-&gt;cpu_data(),
2571        output1.mutable_cpu_data());
2572    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2573        blob2.width());
2574    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2575    this-&gt;net_-&gt;Forward();
2576    this-&gt;net_-&gt;Backward();
2577    Blob&lt;Dtype&gt; output2(output_blob-&gt;num(), output_blob-&gt;channels(),
2578        output_blob-&gt;height(), output_blob-&gt;width());
2579    caffe_copy(output2.count(), output_blob-&gt;cpu_data(),
2580        output2.mutable_cpu_data());
2581    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2582        blob1.width());
2583    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2584    this-&gt;net_-&gt;Forward();
2585    this-&gt;net_-&gt;Backward();
2586    for (int i = 0; i &lt; output1.count(); ++i) {
2587      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2588    }
2589    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2590        blob2.width());
2591    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2592    this-&gt;net_-&gt;Forward();
2593    this-&gt;net_-&gt;Backward();
2594    for (int i = 0; i &lt; output2.count(); ++i) {
2595      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2596    }
2597    EXPECT_EQ(output1.num(), blob1.num());
2598    EXPECT_EQ(output2.num(), blob2.num());
2599    bool same_spatial_shape = true;
2600    const int kFirstSpatialAxis = 2;
2601    for (int i = kFirstSpatialAxis; i &lt; output1.num_axes(); ++i) {
2602      if (output1.shape(i) != output2.shape(i)) {
2603        same_spatial_shape = false;
2604        break;
2605      }
2606    }
2607    EXPECT_FALSE(same_spatial_shape);
2608  }
2609  #endif
2610  #endif
2611  TYPED_TEST(NetTest, TestSkipPropagateDown) {
2612    this-&gt;InitSkipPropNet(false);
2613    vector&lt;bool&gt; vec_layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2614    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2615      string layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2616      if (layer_name == &quot;loss&quot;) {
2617        bool need_back = this-&gt;net_-&gt;bottom_need_backward()[layer_id][1];
2618        EXPECT_TRUE(need_back) &lt;&lt; &quot;bottom_need_backward should be True&quot;;
2619      }
2620      if (layer_name.find(&quot;data&quot;) != std::string::npos ||
2621            layer_name == &quot;silence&quot;) {
2622        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2623            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be False&quot;;
2624      } else {
2625        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2626            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be True&quot;;
2627      }
2628    }
2629    this-&gt;InitSkipPropNet(true);
2630    vec_layer_need_backward.clear();
2631    vec_layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2632    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2633      string layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2634      if (layer_name == &quot;loss&quot;) {
2635        bool need_back = this-&gt;net_-&gt;bottom_need_backward()[layer_id][1];
2636        EXPECT_FALSE(need_back) &lt;&lt; &quot;bottom_need_backward should be False&quot;;
2637      }
2638      if (layer_name == &quot;innerproduct&quot; || layer_name == &quot;loss&quot;) {
2639        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2640            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be True&quot;;
2641      } else {
2642        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2643            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be False&quot;;
2644      }
2645    }
2646  }
2647  TYPED_TEST(NetTest, TestForcePropagateDown) {
2648    this-&gt;InitForcePropNet(false);
2649    vector&lt;bool&gt; layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2650    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2651      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2652      const vector&lt;bool&gt; need_backward =
2653          this-&gt;net_-&gt;bottom_need_backward()[layer_id];
2654      if (layer_name == &quot;data&quot;) {
2655        ASSERT_EQ(need_backward.size(), 0);
2656        EXPECT_FALSE(layer_need_backward[layer_id]);
2657      } else if (layer_name == &quot;innerproduct&quot;) {
2658        ASSERT_EQ(need_backward.size(), 1);
2659        EXPECT_FALSE(need_backward[0]);  
2660        EXPECT_TRUE(layer_need_backward[layer_id]);
2661      } else if (layer_name == &quot;loss&quot;) {
2662        ASSERT_EQ(need_backward.size(), 2);
2663        EXPECT_TRUE(need_backward[0]);   
2664        EXPECT_FALSE(need_backward[1]);  
2665        EXPECT_TRUE(layer_need_backward[layer_id]);
2666      } else {
2667        LOG(FATAL) &lt;&lt; &quot;Unknown layer: &quot; &lt;&lt; layer_name;
2668      }
2669    }
2670    this-&gt;InitForcePropNet(true);
2671    layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2672    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2673      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2674      const vector&lt;bool&gt; need_backward =
2675          this-&gt;net_-&gt;bottom_need_backward()[layer_id];
2676      if (layer_name == &quot;data&quot;) {
2677        ASSERT_EQ(need_backward.size(), 0);
2678        EXPECT_FALSE(layer_need_backward[layer_id]);
2679      } else if (layer_name == &quot;innerproduct&quot;) {
2680        ASSERT_EQ(need_backward.size(), 1);
2681        EXPECT_TRUE(need_backward[0]);  
2682        EXPECT_TRUE(layer_need_backward[layer_id]);
2683      } else if (layer_name == &quot;loss&quot;) {
2684        ASSERT_EQ(need_backward.size(), 2);
2685        EXPECT_TRUE(need_backward[0]);   
2686        EXPECT_FALSE(need_backward[1]);  
2687        EXPECT_TRUE(layer_need_backward[layer_id]);
2688      } else {
2689        LOG(FATAL) &lt;&lt; &quot;Unknown layer: &quot; &lt;&lt; layer_name;
2690      }
2691    }
2692  }
2693  TYPED_TEST(NetTest, TestAllInOneNetTrain) {
2694    vector&lt;string&gt; stages;
2695    stages.push_back(&quot;train&quot;);
2696    this-&gt;InitAllInOneNet(caffe::TRAIN, 0, &amp;stages);
2697    bool found_data = false;
2698    bool found_loss = false;
2699    for (int i = 0; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
2700      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[i];
2701      if (layer_name == &quot;train-data&quot;) {
2702        found_data = true;
2703      } else if (layer_name == &quot;loss&quot;) {
2704        found_loss = true;
2705      } else {
2706        ASSERT_NE(layer_name, &quot;val-data&quot;);
2707        ASSERT_NE(layer_name, &quot;deploy-data&quot;);
2708      }
2709    }
2710    ASSERT_TRUE(found_data);
2711    ASSERT_TRUE(found_loss);
2712  }
2713  TYPED_TEST(NetTest, TestAllInOneNetVal) {
2714    vector&lt;string&gt; stages;
2715    stages.push_back(&quot;val&quot;);
2716    this-&gt;InitAllInOneNet(caffe::TEST, 0, &amp;stages);
2717    bool found_data = false;
2718    bool found_loss = false;
2719    for (int i = 0; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
2720      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[i];
2721      if (layer_name == &quot;val-data&quot;) {
2722        found_data = true;
2723      } else if (layer_name == &quot;loss&quot;) {
2724        found_loss = true;
2725      } else {
2726        ASSERT_NE(layer_name, &quot;train-data&quot;);
2727        ASSERT_NE(layer_name, &quot;deploy-data&quot;);
2728      }
2729    }
2730    ASSERT_TRUE(found_data);
2731    ASSERT_TRUE(found_loss);
2732  }
2733  TYPED_TEST(NetTest, TestAllInOneNetDeploy) {
2734    vector&lt;string&gt; stages;
2735    stages.push_back(&quot;deploy&quot;);
2736    this-&gt;InitAllInOneNet(caffe::TEST, 0, &amp;stages);
2737    bool found_data = false;
2738    for (int i = 0; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
2739      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[i];
2740      if (layer_name == &quot;deploy-data&quot;) {
2741        found_data = true;
2742      } else {
2743        ASSERT_NE(layer_name, &quot;train-data&quot;);
2744        ASSERT_NE(layer_name, &quot;val-data&quot;);
2745        ASSERT_NE(layer_name, &quot;loss&quot;);
2746      }
2747    }
2748    ASSERT_TRUE(found_data);
2749  }
2750  class CompileNetTest : public ::testing::Test {
2751   protected:
2752    void RunCompilerNetTest(
2753        const string&amp; input_param_string, const string&amp; compiled_param_string) {
2754      NetParameter input_param;
2755      CHECK(google::protobuf::TextFormat::ParseFromString(
2756          input_param_string, &amp;input_param));
2757      NetParameter expected_compiled_param;
2758      CHECK(google::protobuf::TextFormat::ParseFromString(
2759          compiled_param_string, &amp;expected_compiled_param));
2760      NetParameter actual_compiled_param;
2761      Net&lt;float&gt;::CompileNet(input_param, &amp;actual_compiled_param);
2762      actual_compiled_param.mutable_compile_net_state()-&gt;Clear();
2763      expected_compiled_param.mutable_compile_net_state()-&gt;Clear();
2764      string expect_net_string = expected_compiled_param.DebugString();
2765      string actual_net_string = actual_compiled_param.DebugString();
2766      EXPECT_EQ(expect_net_string,
2767          actual_net_string);
2768      NetParameter double_compiled_param;
2769      Net&lt;float&gt;::CompileNet(actual_compiled_param, &amp;double_compiled_param);
2770      double_compiled_param.mutable_compile_net_state()-&gt;Clear();
2771      string double_net_string = double_compiled_param.DebugString();
2772      EXPECT_EQ(actual_net_string,
2773         double_net_string);
2774    }
2775  };
2776  #ifndef DISABLE_BN_FOLDING
2777  TEST_F(CompileNetTest, TestRemoveBatchNorm1) {
2778    const string&amp; input_proto = 
2779        &quot;name: &#x27;TestNetwork&#x27; &quot;
2780        &quot;layer { &quot;
2781        &quot;  name: &#x27;data&#x27; &quot;
2782        &quot;  type: &#x27;Data&#x27; &quot;
2783        &quot;  top: &#x27;data&#x27; &quot;
2784        &quot;  top: &#x27;label&#x27; &quot;
2785        &quot;} &quot;
2786        &quot;layer { &quot;
2787        &quot;  bottom: &#x27;data&#x27; &quot;
2788        &quot;  name: &#x27;conv&#x27; &quot;
2789        &quot;  top: &#x27;conv&#x27; &quot;
2790        &quot;  type: &#x27;Convolution&#x27; &quot;
2791        &quot;} &quot;
2792        &quot;layer { &quot;
2793        &quot;  bottom: &#x27;conv&#x27; &quot;
2794        &quot;  name: &#x27;bn&#x27; &quot;
2795        &quot;  top: &#x27;conv&#x27; &quot;
2796        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2797        &quot;} &quot;
2798        &quot;layer { &quot;
2799        &quot;  name: &#x27;loss&#x27; &quot;
2800        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2801        &quot;  bottom: &#x27;conv&#x27; &quot;
2802        &quot;  bottom: &#x27;label&#x27; &quot;
2803        &quot;} &quot;;
2804    const string&amp; output_proto =
2805        &quot;name: &#x27;TestNetwork&#x27; &quot;
2806        &quot;layer { &quot;
2807        &quot;  name: &#x27;data&#x27; &quot;
2808        &quot;  type: &#x27;Data&#x27; &quot;
2809        &quot;  top: &#x27;data&#x27; &quot;
2810        &quot;  top: &#x27;label&#x27; &quot;
2811        &quot;} &quot;
2812        &quot;layer { &quot;
2813        &quot;  bottom: &#x27;data&#x27; &quot;
2814        &quot;  name: &#x27;conv&#x27; &quot;
2815        &quot;  top: &#x27;conv&#x27; &quot;
2816        &quot;  type: &#x27;Convolution&#x27; &quot;
2817        &quot;} &quot;
2818        &quot;layer { &quot;
2819        &quot;  name: &#x27;loss&#x27; &quot;
2820        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2821        &quot;  bottom: &#x27;conv&#x27; &quot;
2822        &quot;  bottom: &#x27;label&#x27; &quot;
2823        &quot;} &quot;;
2824    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2825  }
2826  TEST_F(CompileNetTest, TestRemoveBatchNorm2) {
2827    const string&amp; input_proto = 
2828        &quot;name: &#x27;TestNetwork&#x27; &quot;
2829        &quot;layer { &quot;
2830        &quot;  name: &#x27;data&#x27; &quot;
2831        &quot;  type: &#x27;Data&#x27; &quot;
2832        &quot;  top: &#x27;data&#x27; &quot;
2833        &quot;  top: &#x27;label&#x27; &quot;
2834        &quot;} &quot;
2835        &quot;layer { &quot;
2836        &quot;  bottom: &#x27;data&#x27; &quot;
2837        &quot;  name: &#x27;fc1&#x27; &quot;
2838        &quot;  top: &#x27;fc1&#x27; &quot;
2839        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2840        &quot;} &quot;
2841        &quot;layer { &quot;
2842        &quot;  bottom: &#x27;fc1&#x27; &quot;
2843        &quot;  name: &#x27;bn&#x27; &quot;
2844        &quot;  top: &#x27;bn&#x27; &quot;
2845        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2846        &quot;} &quot;
2847        &quot;layer { &quot;
2848        &quot;  name: &#x27;loss&#x27; &quot;
2849        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2850        &quot;  bottom: &#x27;bn&#x27; &quot;
2851        &quot;  bottom: &#x27;label&#x27; &quot;
2852        &quot;} &quot;;
2853    const string&amp; output_proto =
2854        &quot;name: &#x27;TestNetwork&#x27; &quot;
2855        &quot;layer { &quot;
2856        &quot;  name: &#x27;data&#x27; &quot;
2857        &quot;  type: &#x27;Data&#x27; &quot;
2858        &quot;  top: &#x27;data&#x27; &quot;
2859        &quot;  top: &#x27;label&#x27; &quot;
2860        &quot;} &quot;
2861        &quot;layer { &quot;
2862        &quot;  bottom: &#x27;data&#x27; &quot;
2863        &quot;  name: &#x27;fc1&#x27; &quot;
2864        &quot;  top: &#x27;fc1&#x27; &quot;
2865        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2866        &quot;} &quot;
2867        &quot;layer { &quot;
2868        &quot;  bottom: &#x27;fc1&#x27; &quot;
2869        &quot;  name: &#x27;bn&#x27; &quot;
2870        &quot;  top: &#x27;bn&#x27; &quot;
2871        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2872        &quot;} &quot;
2873        &quot;layer { &quot;
2874        &quot;  name: &#x27;loss&#x27; &quot;
2875        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2876        &quot;  bottom: &#x27;bn&#x27; &quot;
2877        &quot;  bottom: &#x27;label&#x27; &quot;
2878        &quot;} &quot;;
2879    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2880  }
2881  TEST_F(CompileNetTest, TestRemoveBatchNorm3) {
2882    const string&amp; input_proto = 
2883        &quot;name: &#x27;TestNetwork&#x27; &quot;
2884        &quot;layer { &quot;
2885        &quot;  name: &#x27;data&#x27; &quot;
2886        &quot;  type: &#x27;Data&#x27; &quot;
2887        &quot;  top: &#x27;data&#x27; &quot;
2888        &quot;  top: &#x27;label&#x27; &quot;
2889        &quot;} &quot;
2890        &quot;layer { &quot;
2891        &quot;  bottom: &#x27;data&#x27; &quot;
2892        &quot;  name: &#x27;conv&#x27; &quot;
2893        &quot;  top: &#x27;conv&#x27; &quot;
2894        &quot;  type: &#x27;Convolution&#x27; &quot;
2895        &quot;} &quot;
2896        &quot;layer { &quot;
2897        &quot;  bottom: &#x27;conv&#x27; &quot;
2898        &quot;  name: &#x27;bn&#x27; &quot;
2899        &quot;  top: &#x27;conv&#x27; &quot;
2900        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2901  	  &quot;  batch_norm_param { &quot;
2902  	  &quot;    use_global_stats: false&quot;
2903  	  &quot;  }&quot;
2904        &quot;} &quot;
2905        &quot;layer { &quot;
2906        &quot;  name: &#x27;loss&#x27; &quot;
2907        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2908        &quot;  bottom: &#x27;conv&#x27; &quot;
2909        &quot;  bottom: &#x27;label&#x27; &quot;
2910        &quot;} &quot;;
2911    const string&amp; output_proto =
2912        &quot;name: &#x27;TestNetwork&#x27; &quot;
2913        &quot;layer { &quot;
2914        &quot;  name: &#x27;data&#x27; &quot;
2915        &quot;  type: &#x27;Data&#x27; &quot;
2916        &quot;  top: &#x27;data&#x27; &quot;
2917        &quot;  top: &#x27;label&#x27; &quot;
2918        &quot;} &quot;
2919        &quot;layer { &quot;
2920        &quot;  bottom: &#x27;data&#x27; &quot;
2921        &quot;  name: &#x27;conv&#x27; &quot;
2922        &quot;  top: &#x27;conv&#x27; &quot;
2923        &quot;  type: &#x27;Convolution&#x27; &quot;
2924        &quot;} &quot;
2925        &quot;layer { &quot;
2926        &quot;  bottom: &#x27;conv&#x27; &quot;
2927        &quot;  name: &#x27;bn&#x27; &quot;
2928        &quot;  top: &#x27;conv&#x27; &quot;
2929        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2930  	  &quot;  batch_norm_param { &quot;
2931  	  &quot;    use_global_stats: false&quot;
2932  	  &quot;  }&quot;
2933        &quot;} &quot;
2934        &quot;layer { &quot;
2935        &quot;  name: &#x27;loss&#x27; &quot;
2936        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2937        &quot;  bottom: &#x27;conv&#x27; &quot;
2938        &quot;  bottom: &#x27;label&#x27; &quot;
2939        &quot;} &quot;;
2940    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2941  }
2942  TEST_F(CompileNetTest, TestRemoveBatchNorm4) {
2943    const string&amp; input_proto = 
2944        &quot;name: &#x27;TestNetwork&#x27; &quot;
2945        &quot;layer { &quot;
2946        &quot;  name: &#x27;data&#x27; &quot;
2947        &quot;  type: &#x27;Data&#x27; &quot;
2948        &quot;  top: &#x27;data&#x27; &quot;
2949        &quot;  top: &#x27;label&#x27; &quot;
2950        &quot;} &quot;
2951        &quot;layer { &quot;
2952        &quot;  bottom: &#x27;data&#x27; &quot;
2953        &quot;  name: &#x27;conv&#x27; &quot;
2954        &quot;  top: &#x27;conv&#x27; &quot;
2955        &quot;  type: &#x27;Convolution&#x27; &quot;
2956        &quot;} &quot;
2957        &quot;layer { &quot;
2958        &quot;  bottom: &#x27;conv&#x27; &quot;
2959        &quot;  name: &#x27;bn&#x27; &quot;
2960        &quot;  top: &#x27;conv&#x27; &quot;
2961        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2962  	  &quot;  batch_norm_param { &quot;
2963  	  &quot;    use_global_stats: true&quot;
2964  	  &quot;  }&quot;
2965        &quot;} &quot;
2966        &quot;layer { &quot;
2967        &quot;  name: &#x27;loss&#x27; &quot;
2968        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2969        &quot;  bottom: &#x27;conv&#x27; &quot;
2970        &quot;  bottom: &#x27;label&#x27; &quot;
2971        &quot;} &quot;;
2972    const string&amp; output_proto =
2973        &quot;name: &#x27;TestNetwork&#x27; &quot;
2974        &quot;layer { &quot;
2975        &quot;  name: &#x27;data&#x27; &quot;
2976        &quot;  type: &#x27;Data&#x27; &quot;
2977        &quot;  top: &#x27;data&#x27; &quot;
2978        &quot;  top: &#x27;label&#x27; &quot;
2979        &quot;} &quot;
2980        &quot;layer { &quot;
2981        &quot;  bottom: &#x27;data&#x27; &quot;
2982        &quot;  name: &#x27;conv&#x27; &quot;
2983        &quot;  top: &#x27;conv&#x27; &quot;
2984        &quot;  type: &#x27;Convolution&#x27; &quot;
2985        &quot;} &quot;
2986        &quot;layer { &quot;
2987        &quot;  name: &#x27;loss&#x27; &quot;
2988        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2989        &quot;  bottom: &#x27;conv&#x27; &quot;
2990        &quot;  bottom: &#x27;label&#x27; &quot;
2991        &quot;} &quot;;
2992    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2993  }
2994  #endif
2995  #ifdef MKL2017_SUPPORTED
2996  TEST_F(CompileNetTest, TestCompileNetBatchNorm) {
2997    const string&amp; input_proto =
2998        &quot;name: &#x27;TestNetwork&#x27; &quot;
2999        &quot;layer { &quot;
3000        &quot;  name: &#x27;data&#x27; &quot;
3001        &quot;  type: &#x27;Data&#x27; &quot;
3002        &quot;  top: &#x27;data&#x27; &quot;
3003        &quot;  top: &#x27;label&#x27; &quot;
3004        &quot;} &quot;
3005        &quot;layer { &quot;
3006        &quot;  bottom: &#x27;data&#x27; &quot;
3007        &quot;  name: &#x27;bn&#x27; &quot;
3008        &quot;  top: &#x27;bn&#x27; &quot;
3009        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3010        &quot;  batch_norm_param { &quot;
3011        &quot;   engine: MKL2017 &quot;
3012        &quot;  } &quot;
3013        &quot;} &quot;
3014        &quot;layer { &quot;
3015        &quot; bottom: &#x27;bn&#x27; &quot;
3016        &quot; top: &#x27;sc&#x27; &quot;
3017        &quot; name: &#x27;sc&#x27; &quot;
3018        &quot; type: &#x27;Scale&#x27; &quot;
3019        &quot; scale_param { &quot;
3020        &quot;   bias_term: true &quot;
3021        &quot; }&quot;
3022        &quot;}&quot;
3023        &quot;layer { &quot;
3024        &quot;  name: &#x27;loss&#x27; &quot;
3025        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3026        &quot;  bottom: &#x27;sc&#x27; &quot;
3027        &quot;  bottom: &#x27;label&#x27; &quot;
3028        &quot;} &quot;;
3029    const string&amp; output_proto =
3030        &quot;name: &#x27;TestNetwork&#x27; &quot;
3031        &quot;layer { &quot;
3032        &quot;  name: &#x27;data&#x27; &quot;
3033        &quot;  type: &#x27;Data&#x27; &quot;
3034        &quot;  top: &#x27;data&#x27; &quot;
3035        &quot;  top: &#x27;label&#x27; &quot;
3036        &quot;} &quot;
3037        &quot;layer { &quot;
3038        &quot;  bottom: &#x27;data&#x27; &quot;
3039        &quot;  name: &#x27;bn&#x27; &quot;
3040        &quot;  top: &#x27;sc&#x27; &quot;
3041        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3042        &quot;  batch_norm_param { &quot;
3043        &quot;   engine: MKL2017 &quot;
3044        &quot;   bias_term: true &quot;
3045        &quot;  } &quot;
3046        &quot;} &quot;
3047        &quot;layer { &quot;
3048        &quot;  name: &#x27;loss&#x27; &quot;
3049        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3050        &quot;  bottom: &#x27;sc&#x27; &quot;
3051        &quot;  bottom: &#x27;label&#x27; &quot;
3052        &quot;} &quot;;
3053    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3054  }
3055  TEST_F(CompileNetTest, TestCompileNetBatchNormInPlace) {
3056    const string&amp; input_proto =
3057        &quot;name: &#x27;TestNetwork&#x27; &quot;
3058        &quot;layer { &quot;
3059        &quot;  name: &#x27;data&#x27; &quot;
3060        &quot;  type: &#x27;Data&#x27; &quot;
3061        &quot;  top: &#x27;data&#x27; &quot;
3062        &quot;  top: &#x27;label&#x27; &quot;
3063        &quot;} &quot;
3064        &quot;layer { &quot;
3065        &quot;  bottom: &#x27;data&#x27; &quot;
3066        &quot;  name: &#x27;bn&#x27; &quot;
3067        &quot;  top: &#x27;data&#x27; &quot;
3068        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3069        &quot;  batch_norm_param { &quot;
3070        &quot;   engine: MKL2017 &quot;
3071        &quot;  } &quot;
3072        &quot;} &quot;
3073        &quot;layer { &quot;
3074        &quot; bottom: &#x27;data&#x27; &quot;
3075        &quot; top: &#x27;data&#x27; &quot;
3076        &quot; name: &#x27;sc&#x27; &quot;
3077        &quot; type: &#x27;Scale&#x27; &quot;
3078        &quot; scale_param { &quot;
3079        &quot;   bias_term: true &quot;
3080        &quot; }&quot;
3081        &quot;}&quot;
3082        &quot;layer { &quot;
3083        &quot; bottom: &#x27;data&#x27; &quot;
3084        &quot; top: &#x27;data&#x27; &quot;
3085        &quot; name: &#x27;relu&#x27; &quot;
3086        &quot; type: &#x27;ReLU&#x27; &quot;
3087        &quot; relu_param { &quot;
3088        &quot;  engine: MKL2017 &quot;
3089        &quot; } &quot;
3090        &quot;}&quot;
3091        &quot;layer { &quot;
3092        &quot;  name: &#x27;loss&#x27; &quot;
3093        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3094        &quot;  bottom: &#x27;data&#x27; &quot;
3095        &quot;  bottom: &#x27;label&#x27; &quot;
3096        &quot;} &quot;;
3097    const string&amp; output_proto =
3098        &quot;name: &#x27;TestNetwork&#x27; &quot;
3099        &quot;layer { &quot;
3100        &quot;  name: &#x27;data&#x27; &quot;
3101        &quot;  type: &#x27;Data&#x27; &quot;
3102        &quot;  top: &#x27;data&#x27; &quot;
3103        &quot;  top: &#x27;label&#x27; &quot;
3104        &quot;} &quot;
3105        &quot;layer { &quot;
3106        &quot;  bottom: &#x27;data&#x27; &quot;
3107        &quot;  name: &#x27;bn&#x27; &quot;
3108        &quot;  top: &#x27;data_x&#x27; &quot;
3109        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3110        &quot;  batch_norm_param { &quot;
3111        &quot;   engine: MKL2017 &quot;
3112        &quot;   bias_term: true &quot;
3113        &quot;  } &quot;
3114        &quot;} &quot;
3115        &quot;layer { &quot;
3116        &quot; bottom: &#x27;data_x&#x27; &quot;
3117        &quot; top: &#x27;data_x&#x27; &quot;
3118        &quot; name: &#x27;relu&#x27; &quot;
3119        &quot; type: &#x27;ReLU&#x27; &quot;
3120        &quot; relu_param { &quot;
3121        &quot;  engine: MKL2017 &quot;
3122        &quot; } &quot;
3123        &quot;}&quot;
3124        &quot;layer { &quot;
3125        &quot;  name: &#x27;loss&#x27; &quot;
3126        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3127        &quot;  bottom: &#x27;data_x&#x27; &quot;
3128        &quot;  bottom: &#x27;label&#x27; &quot;
3129        &quot;} &quot;;
3130    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3131  }
3132  #endif
3133  #if defined(MKL2017_SUPPORTED) &amp;&amp; defined(MKLDNN_SUPPORTED)
3134  TEST_F(CompileNetTest, TestCompileNetBatchNormConvolution) {
3135    const string&amp; input_proto =
3136        &quot;name: &#x27;TestNetwork&#x27; &quot;
3137        &quot;layer { &quot;
3138        &quot;  name: &#x27;data&#x27; &quot;
3139        &quot;  type: &#x27;Data&#x27; &quot;
3140        &quot;  top: &#x27;data&#x27; &quot;
3141        &quot;  top: &#x27;label&#x27; &quot;
3142        &quot;} &quot;
3143        &quot;layer { &quot;
3144        &quot;  bottom: &#x27;data&#x27; &quot;
3145        &quot;  name: &#x27;bn&#x27; &quot;
3146        &quot;  top: &#x27;bn&#x27; &quot;
3147        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3148        &quot;  batch_norm_param { &quot;
3149        &quot;   engine: MKL2017 &quot;
3150        &quot;  } &quot;
3151        &quot;} &quot;
3152        &quot;layer { &quot;
3153        &quot; bottom: &#x27;bn&#x27; &quot;
3154        &quot; top: &#x27;conv&#x27; &quot;
3155        &quot; name: &#x27;sc&#x27; &quot;
3156        &quot; type: &#x27;Scale&#x27; &quot;
3157        &quot; scale_param { &quot;
3158        &quot;   bias_term: true &quot;
3159        &quot; }&quot;
3160        &quot;}&quot;
3161        &quot;layer { &quot;
3162        &quot;  bottom: &#x27;conv&#x27; &quot;
3163        &quot;  name: &#x27;conv&#x27; &quot;
3164        &quot;  top: &#x27;relu&#x27; &quot;
3165        &quot;  type: &#x27;Convolution&#x27; &quot;
3166        &quot;  convolution_param { &quot;
3167        &quot;   engine: MKLDNN &quot;
3168        &quot;  } &quot;
3169        &quot;} &quot;
3170        &quot;layer { &quot;
3171        &quot; bottom: &#x27;relu&#x27; &quot;
3172        &quot; top: &#x27;relu&#x27; &quot;
3173        &quot; name: &#x27;relu&#x27; &quot;
3174        &quot; type: &#x27;ReLU&#x27; &quot;
3175        &quot; relu_param { &quot;
3176        &quot;  engine: MKLDNN &quot;
3177        &quot; } &quot;
3178        &quot;}&quot;
3179        &quot;layer { &quot;
3180        &quot;  name: &#x27;loss&#x27; &quot;
3181        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3182        &quot;  bottom: &#x27;relu&#x27; &quot;
3183        &quot;  bottom: &#x27;label&#x27; &quot;
3184        &quot;} &quot;;
3185    const string&amp; output_proto =
3186        &quot;name: &#x27;TestNetwork&#x27; &quot;
3187        &quot;layer { &quot;
3188        &quot;  name: &#x27;data&#x27; &quot;
3189        &quot;  type: &#x27;Data&#x27; &quot;
3190        &quot;  top: &#x27;data&#x27; &quot;
3191        &quot;  top: &#x27;label&#x27; &quot;
3192        &quot;} &quot;
3193        &quot;layer { &quot;
3194        &quot;  bottom: &#x27;data&#x27; &quot;
3195        &quot;  name: &#x27;bn&#x27; &quot;
3196        &quot;  top: &#x27;conv&#x27; &quot;
3197        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3198        &quot;  batch_norm_param { &quot;
3199        &quot;   engine: MKL2017 &quot;
3200        &quot;   bias_term: true &quot;
3201        &quot;  } &quot;
3202        &quot;} &quot;
3203        &quot;layer { &quot;
3204        &quot;  bottom: &#x27;conv&#x27; &quot;
3205        &quot;  name: &#x27;conv&#x27; &quot;
3206        &quot;  top: &#x27;relu&#x27; &quot;
3207        &quot;  type: &#x27;Convolution&#x27; &quot;
3208        &quot;  convolution_param { &quot;
3209        &quot;   engine: MKLDNN &quot;
3210        &quot;   relu: true &quot;
3211        &quot;negative_slope: 0&quot;
3212        &quot;  } &quot;
3213        &quot;} &quot;
3214        &quot;layer { &quot;
3215        &quot;  name: &#x27;loss&#x27; &quot;
3216        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3217        &quot;  bottom: &#x27;relu&#x27; &quot;
3218        &quot;  bottom: &#x27;label&#x27; &quot;
3219        &quot;} &quot;;
3220    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3221  }
3222  #endif
3223  #ifndef DISABLE_CONV_SUM_FUSION
3224  TEST_F(CompileNetTest, TestCompileNetConvEltReluFusionMKLDNN) {
3225    const string&amp; input_proto =
3226        &quot;name: &#x27;TestNetwork&#x27; &quot;
3227        &quot;layer { &quot;
3228        &quot;  name: &#x27;data&#x27; &quot;
3229        &quot;  type: &#x27;Data&#x27; &quot;
3230        &quot;  top: &#x27;data&#x27; &quot;
3231        &quot;  top: &#x27;label&#x27; &quot;
3232        &quot;} &quot;
3233        &quot;layer { &quot;
3234        &quot;  bottom: &#x27;data&#x27; &quot;
3235        &quot;  name: &#x27;conv1&#x27; &quot;
3236        &quot;  top: &#x27;conv1&#x27; &quot;
3237        &quot;  type: &#x27;Convolution&#x27; &quot;
3238        &quot;  convolution_param { &quot;
3239        &quot;   engine: MKLDNN &quot;
3240        &quot;  } &quot;
3241        &quot;} &quot;
3242        &quot;layer { &quot;
3243        &quot;  bottom: &#x27;data&#x27; &quot;
3244        &quot;  name: &#x27;conv2&#x27; &quot;
3245        &quot;  top: &#x27;conv2&#x27; &quot;
3246        &quot;  type: &#x27;Convolution&#x27; &quot;
3247        &quot;  convolution_param { &quot;
3248        &quot;   engine: MKLDNN &quot;
3249        &quot;  } &quot;
3250        &quot;} &quot;
3251        &quot;layer { &quot;
3252        &quot;  bottom: &#x27;conv1&#x27; &quot;
3253        &quot;  name: &#x27;conv3&#x27; &quot;
3254        &quot;  top: &#x27;conv3&#x27; &quot;
3255        &quot;  type: &#x27;Convolution&#x27; &quot;
3256        &quot;  convolution_param { &quot;
3257        &quot;   engine: MKLDNN &quot;
3258        &quot;  } &quot;
3259        &quot;} &quot;
3260        &quot;layer { &quot;
3261        &quot;  bottom: &#x27;conv2&#x27; &quot;
3262        &quot;  bottom: &#x27;conv3&#x27; &quot;
3263        &quot;  name: &#x27;conv4&#x27; &quot;
3264        &quot;  top: &#x27;relu&#x27; &quot;
3265        &quot;  type: &#x27;Eltwise&#x27; &quot;
3266        &quot;} &quot;
3267        &quot;layer { &quot;
3268        &quot; bottom: &#x27;relu&#x27; &quot;
3269        &quot; top: &#x27;relu&#x27; &quot;
3270        &quot; name: &#x27;relu&#x27; &quot;
3271        &quot; type: &#x27;ReLU&#x27; &quot;
3272        &quot;}&quot;
3273        &quot;layer { &quot;
3274        &quot;  name: &#x27;loss&#x27; &quot;
3275        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3276        &quot;  bottom: &#x27;relu&#x27; &quot;
3277        &quot;  bottom: &#x27;label&#x27; &quot;
3278        &quot;} &quot;;
3279    const string&amp; output_proto =
3280        &quot;name: &#x27;TestNetwork&#x27; &quot;
3281        &quot;layer { &quot;
3282        &quot;  name: &#x27;data&#x27; &quot;
3283        &quot;  type: &#x27;Data&#x27; &quot;
3284        &quot;  top: &#x27;data&#x27; &quot;
3285        &quot;  top: &#x27;label&#x27; &quot;
3286        &quot;} &quot;
3287        &quot;layer { &quot;
3288        &quot;  bottom: &#x27;data&#x27; &quot;
3289        &quot;  name: &#x27;conv1&#x27; &quot;
3290        &quot;  top: &#x27;conv1&#x27; &quot;
3291        &quot;  type: &#x27;Convolution&#x27; &quot;
3292        &quot;  convolution_param { &quot;
3293        &quot;   engine: MKLDNN &quot;
3294        &quot;  } &quot;
3295        &quot;} &quot;
3296        &quot;layer { &quot;
3297        &quot;  bottom: &#x27;data&#x27; &quot;
3298        &quot;  name: &#x27;conv2&#x27; &quot;
3299        &quot;  top: &#x27;conv2&#x27; &quot;
3300        &quot;  type: &#x27;Convolution&#x27; &quot;
3301        &quot;  convolution_param { &quot;
3302        &quot;   engine: MKLDNN &quot;
3303        &quot;  } &quot;
3304        &quot;} &quot;
3305        &quot;layer { &quot;
3306        &quot;  bottom: &#x27;conv1&#x27; &quot;
3307        &quot;  bottom: &#x27;conv2&#x27; &quot;
3308        &quot;  name: &#x27;conv3&#x27; &quot;
3309        &quot;  top: &#x27;relu&#x27; &quot;
3310        &quot;  type: &#x27;Convolution&#x27; &quot;
3311        &quot;  convolution_param { &quot;
3312        &quot;   engine: MKLDNN &quot;
3313        &quot;   relu: true &quot;
3314        &quot;   fusion_type: SUM_FUSION &quot;
3315        &quot;  } &quot;
3316        &quot;} &quot;
3317        &quot;layer { &quot;
3318        &quot;  name: &#x27;loss&#x27; &quot;
3319        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3320        &quot;  bottom: &#x27;relu&#x27; &quot;
3321        &quot;  bottom: &#x27;label&#x27; &quot;
3322        &quot;} &quot;;
3323   const string input_proto_test = &quot;state: { phase: TEST } engine: &#x27;MKLDNN&#x27;&quot; + input_proto;
3324   const string output_proto_test = &quot;state: { phase: TEST } engine: &#x27;MKLDNN&#x27;&quot; + output_proto;
3325   this-&gt;RunCompilerNetTest(input_proto_test, output_proto_test);
3326  }
3327  #endif
3328  #ifdef MKLDNN_SUPPORTED
3329  TEST_F(CompileNetTest, TestCompileNetBatchNormMKLDNN) {
3330      const string&amp; input_proto =
3331        &quot;name: &#x27;TestNetwork&#x27; &quot;
3332        &quot;layer { &quot;
3333        &quot;  name: &#x27;data&#x27; &quot;
3334        &quot;  type: &#x27;Data&#x27; &quot;
3335        &quot;  top: &#x27;data&#x27; &quot;
3336        &quot;  top: &#x27;label&#x27; &quot;
3337        &quot;} &quot;
3338        &quot;layer { &quot;
3339        &quot;  bottom: &#x27;data&#x27; &quot;
3340        &quot;  name: &#x27;bn&#x27; &quot;
3341        &quot;  top: &#x27;bn&#x27; &quot;
3342        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3343        &quot;  batch_norm_param { &quot;
3344        &quot;   engine: MKLDNN &quot;
3345        &quot;  } &quot;
3346        &quot;} &quot;
3347        &quot;layer { &quot;
3348        &quot; bottom: &#x27;bn&#x27; &quot;
3349        &quot; top: &#x27;sc&#x27; &quot;
3350        &quot; name: &#x27;sc&#x27; &quot;
3351        &quot; type: &#x27;Scale&#x27; &quot;
3352        &quot; scale_param { &quot;
3353        &quot;   bias_term: true &quot;
3354        &quot; }&quot;
3355        &quot;}&quot;
3356        &quot;layer { &quot;
3357        &quot;  name: &#x27;loss&#x27; &quot;
3358        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3359        &quot;  bottom: &#x27;sc&#x27; &quot;
3360        &quot;  bottom: &#x27;label&#x27; &quot;
3361        &quot;} &quot;;
3362    const string&amp; output_proto =
3363        &quot;name: &#x27;TestNetwork&#x27; &quot;
3364        &quot;layer { &quot;
3365        &quot;  name: &#x27;data&#x27; &quot;
3366        &quot;  type: &#x27;Data&#x27; &quot;
3367        &quot;  top: &#x27;data&#x27; &quot;
3368        &quot;  top: &#x27;label&#x27; &quot;
3369        &quot;} &quot;
3370        &quot;layer { &quot;
3371        &quot;  bottom: &#x27;data&#x27; &quot;
3372        &quot;  name: &#x27;bn&#x27; &quot;
3373        &quot;  top: &#x27;sc&#x27; &quot;
3374        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3375        &quot;  batch_norm_param { &quot;
3376        &quot;   engine: MKLDNN &quot;
3377        &quot;   bias_term: true &quot;
3378        &quot;  } &quot;
3379        &quot;} &quot;
3380        &quot;layer { &quot;
3381        &quot;  name: &#x27;loss&#x27; &quot;
3382        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3383        &quot;  bottom: &#x27;sc&#x27; &quot;
3384        &quot;  bottom: &#x27;label&#x27; &quot;
3385        &quot;} &quot;;
3386    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3387  }
3388  TEST_F(CompileNetTest, TestCompileNetConvolution) {
3389    const string&amp; input_proto =
3390        &quot;name: &#x27;TestNetwork&#x27; &quot;
3391        &quot;layer { &quot;
3392        &quot;  name: &#x27;data&#x27; &quot;
3393        &quot;  type: &#x27;Data&#x27; &quot;
3394        &quot;  top: &#x27;data&#x27; &quot;
3395        &quot;  top: &#x27;label&#x27; &quot;
3396        &quot;} &quot;
3397        &quot;layer { &quot;
3398        &quot;  bottom: &#x27;data&#x27; &quot;
3399        &quot;  name: &#x27;conv&#x27; &quot;
3400        &quot;  top: &#x27;relu&#x27; &quot;
3401        &quot;  type: &#x27;Convolution&#x27; &quot;
3402        &quot;  convolution_param { &quot;
3403        &quot;   engine: MKLDNN &quot;
3404        &quot;  } &quot;
3405        &quot;} &quot;
3406        &quot;layer { &quot;
3407        &quot; bottom: &#x27;relu&#x27; &quot;
3408        &quot; top: &#x27;relu&#x27; &quot;
3409        &quot; name: &#x27;relu&#x27; &quot;
3410        &quot; type: &#x27;ReLU&#x27; &quot;
3411        &quot; relu_param { &quot;
3412        &quot;  engine: MKLDNN &quot;
3413        &quot; } &quot;
3414        &quot;}&quot;
3415        &quot;layer { &quot;
3416        &quot;  name: &#x27;loss&#x27; &quot;
3417        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3418        &quot;  bottom: &#x27;relu&#x27; &quot;
3419        &quot;  bottom: &#x27;label&#x27; &quot;
3420        &quot;} &quot;;
3421    const string&amp; output_proto =
3422        &quot;name: &#x27;TestNetwork&#x27; &quot;
3423        &quot;layer { &quot;
3424        &quot;  name: &#x27;data&#x27; &quot;
3425        &quot;  type: &#x27;Data&#x27; &quot;
3426        &quot;  top: &#x27;data&#x27; &quot;
3427        &quot;  top: &#x27;label&#x27; &quot;
3428        &quot;} &quot;
3429        &quot;layer { &quot;
3430        &quot;  bottom: &#x27;data&#x27; &quot;
3431        &quot;  name: &#x27;conv&#x27; &quot;
3432        &quot;  top: &#x27;relu&#x27; &quot;
3433        &quot;  type: &#x27;Convolution&#x27; &quot;
3434        &quot;  convolution_param { &quot;
3435        &quot;   engine: MKLDNN &quot;
3436        &quot;   relu: true &quot;
3437        &quot;negative_slope: 0&quot;
3438        &quot;  } &quot;
3439        &quot;} &quot;
3440        &quot;layer { &quot;
3441        &quot;  name: &#x27;loss&#x27; &quot;
3442        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3443        &quot;  bottom: &#x27;relu&#x27; &quot;
3444        &quot;  bottom: &#x27;label&#x27; &quot;
3445        &quot;} &quot;;
3446    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3447  }
3448  TEST_F(CompileNetTest, TestCompileNetLayerParamEngineConvolution) {
3449    const string&amp; input_proto =
3450        &quot;name: &#x27;TestNetwork&#x27; &quot;
3451        &quot;layer { &quot;
3452        &quot;  name: &#x27;data&#x27; &quot;
3453        &quot;  type: &#x27;Data&#x27; &quot;
3454        &quot;  top: &#x27;data&#x27; &quot;
3455        &quot;  top: &#x27;label&#x27; &quot;
3456        &quot;} &quot;
3457        &quot;layer { &quot;
3458        &quot;  bottom: &#x27;data&#x27; &quot;
3459        &quot;  name: &#x27;conv&#x27; &quot;
3460        &quot;  top: &#x27;relu&#x27; &quot;
3461        &quot;  type: &#x27;Convolution&#x27; &quot;
3462        &quot;  engine: &#x27;MKLDNN:CPU&#x27; &quot;
3463        &quot;  convolution_param { &quot;
3464        &quot;  } &quot;
3465        &quot;} &quot;
3466        &quot;layer { &quot;
3467        &quot; bottom: &#x27;relu&#x27; &quot;
3468        &quot; top: &#x27;relu&#x27; &quot;
3469        &quot; name: &#x27;relu&#x27; &quot;
3470        &quot; type: &#x27;ReLU&#x27; &quot;
3471        &quot; engine: &#x27;MKLDNN:CPU&#x27; &quot;
3472        &quot; relu_param { &quot;
3473        &quot; } &quot;
3474        &quot;}&quot;
3475        &quot;layer { &quot;
3476        &quot;  name: &#x27;loss&#x27; &quot;
3477        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3478        &quot;  bottom: &#x27;relu&#x27; &quot;
3479        &quot;  bottom: &#x27;label&#x27; &quot;
3480        &quot;} &quot;;
3481    const string&amp; output_proto =
3482        &quot;name: &#x27;TestNetwork&#x27; &quot;
3483        &quot;layer { &quot;
3484        &quot;  name: &#x27;data&#x27; &quot;
3485        &quot;  type: &#x27;Data&#x27; &quot;
3486        &quot;  top: &#x27;data&#x27; &quot;
3487        &quot;  top: &#x27;label&#x27; &quot;
3488        &quot;} &quot;
3489        &quot;layer { &quot;
3490        &quot;  bottom: &#x27;data&#x27; &quot;
3491        &quot;  name: &#x27;conv&#x27; &quot;
3492        &quot;  top: &#x27;relu&#x27; &quot;
3493        &quot;  type: &#x27;Convolution&#x27; &quot;
3494        &quot;  engine: &#x27;MKLDNN:CPU&#x27; &quot;
3495        &quot;  convolution_param { &quot;
3496        &quot;   relu: true &quot;
3497        &quot;negative_slope: 0&quot;
3498        &quot;  } &quot;
3499        &quot;} &quot;
3500        &quot;layer { &quot;
3501        &quot;  name: &#x27;loss&#x27; &quot;
3502        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3503        &quot;  bottom: &#x27;relu&#x27; &quot;
3504        &quot;  bottom: &#x27;label&#x27; &quot;
3505        &quot;} &quot;;
3506    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3507  }
3508  TEST_F(CompileNetTest, TestNoCompileNetLayerParamEngineConvolution) {
3509    const string&amp; input_proto =
3510        &quot;name: &#x27;TestNetwork&#x27; &quot;
3511        &quot;layer { &quot;
3512        &quot;  name: &#x27;data&#x27; &quot;
3513        &quot;  type: &#x27;Data&#x27; &quot;
3514        &quot;  top: &#x27;data&#x27; &quot;
3515        &quot;  top: &#x27;label&#x27; &quot;
3516        &quot;} &quot;
3517        &quot;layer { &quot;
3518        &quot;  bottom: &#x27;data&#x27; &quot;
3519        &quot;  name: &#x27;conv&#x27; &quot;
3520        &quot;  top: &#x27;relu&#x27; &quot;
3521        &quot;  type: &#x27;Convolution&#x27; &quot;
3522        &quot;  engine: &#x27;MKLDNN:DLA,CPU&#x27; &quot;
3523        &quot;  convolution_param { &quot;
3524        &quot;  } &quot;
3525        &quot;} &quot;
3526        &quot;layer { &quot;
3527        &quot; bottom: &#x27;relu&#x27; &quot;
3528        &quot; top: &#x27;relu&#x27; &quot;
3529        &quot; name: &#x27;relu&#x27; &quot;
3530        &quot; type: &#x27;ReLU&#x27; &quot;
3531        &quot;  engine: &#x27;MKLDNN:DLA,CPU&#x27; &quot;
3532        &quot; relu_param { &quot;
3533        &quot; } &quot;
3534        &quot;}&quot;
3535        &quot;layer { &quot;
3536        &quot;  name: &#x27;loss&#x27; &quot;
3537        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3538        &quot;  bottom: &#x27;relu&#x27; &quot;
3539        &quot;  bottom: &#x27;label&#x27; &quot;
3540        &quot;} &quot;;
3541    this-&gt;RunCompilerNetTest(input_proto, input_proto);
3542  }
3543  #endif
3544  TEST_F(CompileNetTest, TestNoCompileNet) {
3545    const string&amp; input_proto=
3546        &quot;name: &#x27;TestNetwork&#x27; &quot;
3547        &quot;layer { &quot;
3548        &quot;  name: &#x27;data&#x27; &quot;
3549        &quot;  type: &#x27;Data&#x27; &quot;
3550        &quot;  top: &#x27;data&#x27; &quot;
3551        &quot;  top: &#x27;label&#x27; &quot;
3552        &quot;} &quot;
3553        &quot;layer { &quot;
3554        &quot;  bottom: &#x27;data&#x27; &quot;
3555        &quot;  name: &#x27;bn&#x27; &quot;
3556        &quot;  top: &#x27;bn&#x27; &quot;
3557        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3558        &quot;  batch_norm_param { &quot;
3559        &quot;   engine: CAFFE &quot;
3560        &quot;  } &quot;
3561        &quot;} &quot;
3562        &quot;layer { &quot;
3563        &quot; bottom: &#x27;bn&#x27; &quot;
3564        &quot; top: &#x27;sc&#x27; &quot;
3565        &quot; name: &#x27;sc&#x27; &quot;
3566        &quot; type: &#x27;Scale&#x27; &quot;
3567        &quot; scale_param { &quot;
3568        &quot;   bias_term: true &quot;
3569        &quot; }&quot;
3570        &quot;}&quot;
3571        &quot;layer { &quot;
3572        &quot;  name: &#x27;loss&#x27; &quot;
3573        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3574        &quot;  bottom: &#x27;sc&#x27; &quot;
3575        &quot;  bottom: &#x27;label&#x27; &quot;
3576        &quot;} &quot;;
3577    this-&gt;RunCompilerNetTest(input_proto, input_proto);
3578  }
3579  }  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</h3>
            <pre><code>1  #include &lt;string&gt;
2  #include &lt;utility&gt;
3  #include &lt;vector&gt;
4  #include &quot;google/protobuf/text_format.h&quot;
5  #include &quot;gtest/gtest.h&quot;
6  #include &quot;caffe/common.hpp&quot;
7  #include &quot;caffe/filler.hpp&quot;
8  #include &quot;caffe/net.hpp&quot;
9  #include &quot;caffe/util/io.hpp&quot;
10  #include &quot;caffe/util/math_functions.hpp&quot;
11  #include &quot;caffe/test/test_caffe_main.hpp&quot;
12  #include &quot;caffe/test/test_gradient_check_util.hpp&quot;
13  namespace caffe {
14  template &lt;typename ParentType&gt;
15  class ParentTest : public ParentType {
16    typedef typename ParentType::Dtype Dtype;
17   protected:
18    ParentTest() : seed_(1701) {}
19    virtual void InitNetFromProtoString(const string&amp; proto) {
20      NetParameter param;
21      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &amp;param));
22      net_.reset(new Net&lt;Dtype&gt;(param));
23    }
24    virtual void InitNetFromProtoFileWithState(const string&amp; proto,
25        Phase phase = caffe::TRAIN, const int level = 0,
26        const vector&lt;string&gt;* stages = NULL) {
27      NetParameter param;
28      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &amp;param));
29      string param_file;
30      MakeTempFilename(&amp;param_file);
31      WriteProtoToTextFile(param, param_file);
32      net_.reset(new Net&lt;Dtype&gt;(param_file, phase, level, stages));
33    }
34    virtual void CopyNetBlobs(const bool copy_diff,
35        vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;* blobs_copy) {
36      CHECK(net_);
37      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; net_blobs = net_-&gt;blobs();
38      blobs_copy-&gt;clear();
39      blobs_copy-&gt;resize(net_blobs.size());
40      const bool kReshape = true;
41      for (int i = 0; i &lt; net_blobs.size(); ++i) {
42        (*blobs_copy)[i].reset(new Blob&lt;Dtype&gt;());
43        (*blobs_copy)[i]-&gt;CopyFrom(*net_blobs[i], copy_diff, kReshape);
44      }
45    }
46    virtual void CopyNetParams(const bool copy_diff,
47        vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;* params_copy) {
48      CHECK(net_);
49      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; net_params = net_-&gt;params();
50      params_copy-&gt;clear();
51      params_copy-&gt;resize(net_params.size());
52      const bool kReshape = true;
53      for (int i = 0; i &lt; net_params.size(); ++i) {
54        (*params_copy)[i].reset(new Blob&lt;Dtype&gt;());
55        (*params_copy)[i]-&gt;CopyFrom(*net_params[i], copy_diff, kReshape);
56      }
57    }
58    virtual void InitTinyNet(const bool force_backward = false,
59                             const bool accuracy_layer = false) {
60      string proto =
61          &quot;name: &#x27;TinyTestNetwork&#x27; &quot;
62          &quot;layer { &quot;
63          &quot;  name: &#x27;data&#x27; &quot;
64          &quot;  type: &#x27;DummyData&#x27; &quot;
65          &quot;  dummy_data_param { &quot;
66          &quot;    shape { &quot;
67          &quot;      dim: 5 &quot;
68          &quot;      dim: 2 &quot;
69          &quot;      dim: 3 &quot;
70          &quot;      dim: 4 &quot;
71          &quot;    } &quot;
72          &quot;    data_filler { &quot;
73          &quot;      type: &#x27;gaussian&#x27; &quot;
74          &quot;      std: 0.01 &quot;
75          &quot;    } &quot;
76          &quot;    shape { &quot;
77          &quot;      dim: 5 &quot;
78          &quot;    } &quot;
79          &quot;    data_filler { &quot;
80          &quot;      type: &#x27;constant&#x27; &quot;
81          &quot;      value: 0 &quot;
82          &quot;    } &quot;
83          &quot;  } &quot;
84          &quot;  top: &#x27;data&#x27; &quot;
85          &quot;  top: &#x27;label&#x27; &quot;
86          &quot;} &quot;
87          &quot;layer { &quot;
88          &quot;  name: &#x27;innerproduct&#x27; &quot;
89          &quot;  type: &#x27;InnerProduct&#x27; &quot;
90          &quot;  inner_product_param { &quot;
91          &quot;    num_output: 1000 &quot;
92          &quot;    weight_filler { &quot;
93          &quot;      type: &#x27;gaussian&#x27; &quot;
94          &quot;      std: 0.01 &quot;
95          &quot;    } &quot;
96          &quot;    bias_filler { &quot;
97          &quot;      type: &#x27;constant&#x27; &quot;
98          &quot;      value: 0 &quot;
99          &quot;    } &quot;
100          &quot;  } &quot;
101          &quot;  param { &quot;
102          &quot;    lr_mult: 1 &quot;
103          &quot;    decay_mult: 1 &quot;
104          &quot;  } &quot;
105          &quot;  param { &quot;
106          &quot;    lr_mult: 2 &quot;
107          &quot;    decay_mult: 0 &quot;
108          &quot;  } &quot;
109          &quot;  bottom: &#x27;data&#x27; &quot;
110          &quot;  top: &#x27;innerproduct&#x27; &quot;
111          &quot;} &quot;
112          &quot;layer { &quot;
113          &quot;  name: &#x27;loss&#x27; &quot;
114          &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
115          &quot;  bottom: &#x27;innerproduct&#x27; &quot;
116          &quot;  bottom: &#x27;label&#x27; &quot;
117          &quot;  top: &#x27;top_loss&#x27; &quot;
118          &quot;} &quot;;
119      if (accuracy_layer) {
120        proto +=
121            &quot;layer { &quot;
122            &quot;  name: &#x27;loss&#x27; &quot;
123            &quot;  type: &#x27;Accuracy&#x27; &quot;
124            &quot;  bottom: &#x27;innerproduct&#x27; &quot;
125            &quot;  bottom: &#x27;label&#x27; &quot;
126            &quot;  top: &#x27;accuracy&#x27; &quot;
127            &quot;} &quot;;
128      }
129      if (force_backward) {
130        proto += &quot;force_backward: true &quot;;
131      }
132      InitNetFromProtoString(proto);
133    }
134    virtual void InitTinyNetEuclidean(const bool force_backward = false) {
135      string proto =
136          &quot;name: &#x27;TinyTestEuclidLossNetwork&#x27; &quot;
137          &quot;layer { &quot;
138          &quot;  name: &#x27;data&#x27; &quot;
139          &quot;  type: &#x27;DummyData&#x27; &quot;
140          &quot;  dummy_data_param { &quot;
141          &quot;    num: 5 &quot;
142          &quot;    channels: 2 &quot;
143          &quot;    height: 3 &quot;
144          &quot;    width: 4 &quot;
145          &quot;    num: 5 &quot;
146          &quot;    channels: 1 &quot;
147          &quot;    height: 1 &quot;
148          &quot;    width: 1 &quot;
149          &quot;    data_filler { &quot;
150          &quot;      type: &#x27;gaussian&#x27; &quot;
151          &quot;      std: 0.01 &quot;
152          &quot;    } &quot;
153          &quot;  } &quot;
154          &quot;  top: &#x27;data&#x27; &quot;
155          &quot;  top: &#x27;label&#x27; &quot;
156          &quot;} &quot;
157          &quot;layer { &quot;
158          &quot;  name: &#x27;innerproduct&#x27; &quot;
159          &quot;  type: &#x27;InnerProduct&#x27; &quot;
160          &quot;  inner_product_param { &quot;
161          &quot;    num_output: 1 &quot;
162          &quot;    weight_filler { &quot;
163          &quot;      type: &#x27;gaussian&#x27; &quot;
164          &quot;      std: 0.01 &quot;
165          &quot;    } &quot;
166          &quot;    bias_filler { &quot;
167          &quot;      type: &#x27;constant&#x27; &quot;
168          &quot;      value: 0 &quot;
169          &quot;    } &quot;
170          &quot;  } &quot;
171          &quot;  param { &quot;
172          &quot;    lr_mult: 1 &quot;
173          &quot;    decay_mult: 1 &quot;
174          &quot;  } &quot;
175          &quot;  param { &quot;
176          &quot;    lr_mult: 2 &quot;
177          &quot;    decay_mult: 0 &quot;
178          &quot;  } &quot;
179          &quot;  bottom: &#x27;data&#x27; &quot;
180          &quot;  top: &#x27;innerproduct&#x27; &quot;
181          &quot;} &quot;
182          &quot;layer { &quot;
183          &quot;  name: &#x27;loss&#x27; &quot;
184          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
185          &quot;  bottom: &#x27;innerproduct&#x27; &quot;
186          &quot;  bottom: &#x27;label&#x27; &quot;
187          &quot;} &quot;;
188      if (force_backward) {
189        proto += &quot;force_backward: true &quot;;
190      }
191      InitNetFromProtoString(proto);
192    }
193    virtual void InitTrickyNet(Dtype* loss_weight = NULL) {
194      ostringstream loss_weight_stream;
195      if (loss_weight) {
196        loss_weight_stream &lt;&lt; &quot;  loss_weight: &quot; &lt;&lt; *loss_weight &lt;&lt; &quot; &quot;;
197      }
198      const string&amp; proto =
199          &quot;name: &#x27;TrickyTestNetwork&#x27; &quot;
200          &quot;layer { &quot;
201          &quot;  name: &#x27;data&#x27; &quot;
202          &quot;  type: &#x27;DummyData&#x27; &quot;
203          &quot;  dummy_data_param { &quot;
204          &quot;    num: 5 &quot;
205          &quot;    channels: 2 &quot;
206          &quot;    height: 3 &quot;
207          &quot;    width: 4 &quot;
208          &quot;    num: 5 &quot;
209          &quot;    channels: 1 &quot;
210          &quot;    height: 1 &quot;
211          &quot;    width: 1 &quot;
212          &quot;    data_filler { &quot;
213          &quot;      type: &#x27;gaussian&#x27; &quot;
214          &quot;      std: 0.01 &quot;
215          &quot;    } &quot;
216          &quot;  } &quot;
217          &quot;  top: &#x27;data&#x27; &quot;
218          &quot;  top: &#x27;label&#x27; &quot;
219          &quot;} &quot;
220          &quot;layer { &quot;
221          &quot;  name: &#x27;innerproduct&#x27; &quot;
222          &quot;  type: &#x27;InnerProduct&#x27; &quot;
223          &quot;  inner_product_param { &quot;
224          &quot;    num_output: 1000 &quot;
225          &quot;    weight_filler { &quot;
226          &quot;      type: &#x27;gaussian&#x27; &quot;
227          &quot;      std: 0.01 &quot;
228          &quot;    } &quot;
229          &quot;    bias_filler { &quot;
230          &quot;      type: &#x27;constant&#x27; &quot;
231          &quot;      value: 0 &quot;
232          &quot;    } &quot;
233          &quot;  } &quot;
234          &quot;  param { &quot;
235          &quot;    lr_mult: 1 &quot;
236          &quot;    decay_mult: 1 &quot;
237          &quot;  } &quot;
238          &quot;  param { &quot;
239          &quot;    lr_mult: 2 &quot;
240          &quot;    decay_mult: 0 &quot;
241          &quot;  } &quot;
242          &quot;  bottom: &#x27;data&#x27; &quot;
243          &quot;  top: &#x27;transformed_data&#x27; &quot;
244          &quot;} &quot;
245          &quot;layer { &quot;
246          &quot;  name: &#x27;innerproduct&#x27; &quot;
247          &quot;  type: &#x27;InnerProduct&#x27; &quot;
248          &quot;  inner_product_param { &quot;
249          &quot;    num_output: 1 &quot;
250          &quot;    weight_filler { &quot;
251          &quot;      type: &#x27;gaussian&#x27; &quot;
252          &quot;      std: 0.01 &quot;
253          &quot;    } &quot;
254          &quot;    bias_filler { &quot;
255          &quot;      type: &#x27;constant&#x27; &quot;
256          &quot;      value: 0 &quot;
257          &quot;    } &quot;
258          &quot;  } &quot;
259          &quot;  param { &quot;
260          &quot;    lr_mult: 1 &quot;
261          &quot;    decay_mult: 1 &quot;
262          &quot;  } &quot;
263          &quot;  param { &quot;
264          &quot;    lr_mult: 2 &quot;
265          &quot;    decay_mult: 0 &quot;
266          &quot;  } &quot;
267          &quot;  bottom: &#x27;label&#x27; &quot;
268          &quot;  top: &#x27;transformed_label&#x27; &quot;
269          &quot;} &quot;
270          &quot;layer { &quot;
271          &quot;  name: &#x27;loss&#x27; &quot;
272          &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot; +
273          loss_weight_stream.str() +
274          &quot;  bottom: &#x27;transformed_data&#x27; &quot;
275          &quot;  bottom: &#x27;transformed_label&#x27; &quot;
276          &quot;} &quot;;
277      InitNetFromProtoString(proto);
278    }
279    virtual void InitUnsharedWeightsNet(const Dtype* loss_weight = NULL,
280        const Dtype* midnet_loss_weight = NULL,
281        const bool force_backward = false, const bool bias_term = false,
282        const Dtype blobs_lr_w1 = 1, const Dtype blobs_lr_b1 = 2,
283        const Dtype blobs_lr_w2 = 1, const Dtype blobs_lr_b2 = 2) {
284      string bias_str = bias_term ? &quot;true &quot;:&quot;false &quot;;
285      ostringstream proto;
286      proto &lt;&lt; &quot;name: &#x27;UnsharedWeightsNetwork&#x27; &quot;;
287      if (force_backward) {
288        proto &lt;&lt; &quot;force_backward: true &quot;;
289      }
290      proto &lt;&lt;
291          &quot;layer { &quot;
292          &quot;  name: &#x27;data&#x27; &quot;
293          &quot;  type: &#x27;DummyData&#x27; &quot;
294          &quot;  dummy_data_param { &quot;
295          &quot;    num: 5 &quot;
296          &quot;    channels: 2 &quot;
297          &quot;    height: 3 &quot;
298          &quot;    width: 4 &quot;
299          &quot;    data_filler { &quot;
300          &quot;      type: &#x27;gaussian&#x27; &quot;
301          &quot;      std: 0.01 &quot;
302          &quot;    } &quot;
303          &quot;  } &quot;
304          &quot;  top: &#x27;data&#x27; &quot;
305          &quot;} &quot;
306          &quot;layer { &quot;
307          &quot;  name: &#x27;innerproduct1&#x27; &quot;
308          &quot;  type: &#x27;InnerProduct&#x27; &quot;
309          &quot;  inner_product_param { &quot;
310          &quot;    num_output: 10 &quot;
311          &quot;    bias_term: &quot; &lt;&lt; bias_str &lt;&lt;
312          &quot;    weight_filler { &quot;
313          &quot;      type: &#x27;gaussian&#x27; &quot;
314          &quot;      std: 10 &quot;
315          &quot;    } &quot;
316          &quot;  } &quot;
317          &quot;  param { &quot;
318          &quot;    name: &#x27;unsharedweights1&#x27; &quot;
319          &quot;    lr_mult: &quot; &lt;&lt; blobs_lr_w1 &lt;&lt;
320          &quot;  } &quot;;
321      if (bias_term) {
322        proto &lt;&lt; &quot;  param { lr_mult: &quot; &lt;&lt; blobs_lr_b1 &lt;&lt; &quot; } &quot;;
323      }
324      proto &lt;&lt;
325          &quot;  bottom: &#x27;data&#x27; &quot;
326          &quot;  top: &#x27;innerproduct1&#x27; &quot;;
327      if (midnet_loss_weight) {
328        proto &lt;&lt; &quot;  loss_weight: &quot; &lt;&lt; *midnet_loss_weight &lt;&lt; &quot; &quot;;
329      }
330      proto &lt;&lt;
331          &quot;} &quot;
332          &quot;layer { &quot;
333          &quot;  name: &#x27;innerproduct2&#x27; &quot;
334          &quot;  type: &#x27;InnerProduct&#x27; &quot;
335          &quot;  inner_product_param { &quot;
336          &quot;    num_output: 10 &quot;
337          &quot;    bias_term: &quot; &lt;&lt; bias_str &lt;&lt;
338          &quot;    weight_filler { &quot;
339          &quot;      type: &#x27;gaussian&#x27; &quot;
340          &quot;      std: 10 &quot;
<span onclick='openModal()' class='match'>341          &quot;    } &quot;
342          &quot;  } &quot;
343          &quot;  param { &quot;
344          &quot;    name: &#x27;unsharedweights2&#x27; &quot;
345          &quot;    lr_mult: &quot; &lt;&lt; blobs_lr_w2 &lt;&lt;
346          &quot;  } &quot;;
347      if (bias_term) {
348        proto &lt;&lt; &quot;  param { lr_mult: &quot; &lt;&lt; blobs_lr_b2 &lt;&lt; &quot; } &quot;;
349      }
350      proto &lt;&lt;
351          &quot;  bottom: &#x27;data&#x27; &quot;
352          &quot;  top: &#x27;innerproduct2&#x27; &quot;
353          &quot;} &quot;
</span>354          &quot;layer { &quot;
355          &quot;  name: &#x27;loss&#x27; &quot;
356          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;;
357      if (loss_weight) {
358        proto &lt;&lt; &quot;  loss_weight: &quot; &lt;&lt; *loss_weight &lt;&lt; &quot; &quot;;
359      }
360      proto &lt;&lt;
361          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
362          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
363          &quot;} &quot;;
364      InitNetFromProtoString(proto.str());
365    }
366    virtual void InitSharedWeightsNet() {
367      const string&amp; proto =
368          &quot;name: &#x27;SharedWeightsNetwork&#x27; &quot;
369          &quot;layer { &quot;
370          &quot;  name: &#x27;data&#x27; &quot;
371          &quot;  type: &#x27;DummyData&#x27; &quot;
372          &quot;  dummy_data_param { &quot;
373          &quot;    num: 5 &quot;
374          &quot;    channels: 2 &quot;
375          &quot;    height: 3 &quot;
376          &quot;    width: 4 &quot;
377          &quot;    data_filler { &quot;
378          &quot;      type: &#x27;gaussian&#x27; &quot;
379          &quot;      std: 0.01 &quot;
380          &quot;    } &quot;
381          &quot;  } &quot;
382          &quot;  top: &#x27;data&#x27; &quot;
383          &quot;} &quot;
384          &quot;layer { &quot;
385          &quot;  name: &#x27;innerproduct1&#x27; &quot;
386          &quot;  type: &#x27;InnerProduct&#x27; &quot;
387          &quot;  inner_product_param { &quot;
388          &quot;    num_output: 10 &quot;
389          &quot;    bias_term: false &quot;
390          &quot;    weight_filler { &quot;
391          &quot;      type: &#x27;gaussian&#x27; &quot;
392          &quot;      std: 10 &quot;
393          &quot;    } &quot;
394          &quot;  } &quot;
395          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
396          &quot;  bottom: &#x27;data&#x27; &quot;
397          &quot;  top: &#x27;innerproduct1&#x27; &quot;
398          &quot;} &quot;
399          &quot;layer { &quot;
400          &quot;  name: &#x27;innerproduct2&#x27; &quot;
401          &quot;  type: &#x27;InnerProduct&#x27; &quot;
402          &quot;  inner_product_param { &quot;
403          &quot;    num_output: 10 &quot;
404          &quot;    bias_term: false &quot;
405          &quot;    weight_filler { &quot;
406          &quot;      type: &#x27;gaussian&#x27; &quot;
407          &quot;      std: 10 &quot;
408          &quot;    } &quot;
409          &quot;  } &quot;
410          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
411          &quot;  bottom: &#x27;data&#x27; &quot;
412          &quot;  top: &#x27;innerproduct2&#x27; &quot;
413          &quot;} &quot;
414          &quot;layer { &quot;
415          &quot;  name: &#x27;loss&#x27; &quot;
416          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
417          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
418          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
419          &quot;} &quot;;
420      InitNetFromProtoString(proto);
421    }
422    virtual void InitDiffDataUnsharedWeightsNet() {
423      const string&amp; proto =
424          &quot;name: &#x27;DiffDataUnsharedWeightsNetwork&#x27; &quot;
425          &quot;layer { &quot;
426          &quot;  name: &#x27;data&#x27; &quot;
427          &quot;  type: &#x27;DummyData&#x27; &quot;
428          &quot;  dummy_data_param { &quot;
429          &quot;    num: 10 &quot;
430          &quot;    channels: 10 &quot;
431          &quot;    height: 1 &quot;
432          &quot;    width: 1 &quot;
433          &quot;    num: 10 &quot;
434          &quot;    channels: 10 &quot;
435          &quot;    height: 1 &quot;
436          &quot;    width: 1 &quot;
437          &quot;    data_filler { &quot;
438          &quot;      type: &#x27;gaussian&#x27; &quot;
439          &quot;      std: 10 &quot;
440          &quot;    } &quot;
441          &quot;  } &quot;
442          &quot;  top: &#x27;data1&#x27; &quot;
443          &quot;  top: &#x27;data2&#x27; &quot;
444          &quot;} &quot;
445          &quot;layer { &quot;
446          &quot;  name: &#x27;innerproduct1&#x27; &quot;
447          &quot;  type: &#x27;InnerProduct&#x27; &quot;
448          &quot;  inner_product_param { &quot;
449          &quot;    num_output: 10 &quot;
450          &quot;    bias_term: false &quot;
451          &quot;    weight_filler { &quot;
452          &quot;      type: &#x27;constant&#x27; &quot;
453          &quot;      value: 0.5 &quot;
454          &quot;    } &quot;
455          &quot;  } &quot;
456          &quot;  param { name: &#x27;unsharedweights1&#x27; } &quot;
457          &quot;  bottom: &#x27;data1&#x27; &quot;
458          &quot;  top: &#x27;innerproduct1&#x27; &quot;
459          &quot;} &quot;
460          &quot;layer { &quot;
461          &quot;  name: &#x27;innerproduct2&#x27; &quot;
462          &quot;  type: &#x27;InnerProduct&#x27; &quot;
463          &quot;  inner_product_param { &quot;
464          &quot;    num_output: 10 &quot;
465          &quot;    bias_term: false &quot;
466          &quot;    weight_filler { &quot;
467          &quot;      type: &#x27;constant&#x27; &quot;
468          &quot;      value: 0.5 &quot;
469          &quot;    } &quot;
470          &quot;  } &quot;
471          &quot;  param { name: &#x27;unsharedweights2&#x27; } &quot;
472          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
473          &quot;  top: &#x27;innerproduct2&#x27; &quot;
474          &quot;} &quot;
475          &quot;layer { &quot;
476          &quot;  name: &#x27;loss&#x27; &quot;
477          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
478          &quot;  bottom: &#x27;data2&#x27; &quot;
479          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
480          &quot;} &quot;;
481      InitNetFromProtoString(proto);
482    }
483    virtual void InitDiffDataSharedWeightsNet() {
484      const string&amp; proto =
485          &quot;name: &#x27;DiffDataSharedWeightsNetwork&#x27; &quot;
486          &quot;layer { &quot;
487          &quot;  name: &#x27;data&#x27; &quot;
488          &quot;  type: &#x27;DummyData&#x27; &quot;
489          &quot;  dummy_data_param { &quot;
490          &quot;    num: 10 &quot;
491          &quot;    channels: 10 &quot;
492          &quot;    height: 1 &quot;
493          &quot;    width: 1 &quot;
494          &quot;    num: 10 &quot;
495          &quot;    channels: 10 &quot;
496          &quot;    height: 1 &quot;
497          &quot;    width: 1 &quot;
498          &quot;    data_filler { &quot;
499          &quot;      type: &#x27;gaussian&#x27; &quot;
500          &quot;      std: 10 &quot;
501          &quot;    } &quot;
502          &quot;  } &quot;
503          &quot;  top: &#x27;data1&#x27; &quot;
504          &quot;  top: &#x27;data2&#x27; &quot;
505          &quot;} &quot;
506          &quot;layer { &quot;
507          &quot;  name: &#x27;innerproduct1&#x27; &quot;
508          &quot;  type: &#x27;InnerProduct&#x27; &quot;
509          &quot;  inner_product_param { &quot;
510          &quot;    num_output: 10 &quot;
511          &quot;    bias_term: false &quot;
512          &quot;    weight_filler { &quot;
513          &quot;      type: &#x27;constant&#x27; &quot;
514          &quot;      value: 0.5 &quot;
515          &quot;    } &quot;
516          &quot;  } &quot;
517          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
518          &quot;  bottom: &#x27;data1&#x27; &quot;
519          &quot;  top: &#x27;innerproduct1&#x27; &quot;
520          &quot;} &quot;
521          &quot;layer { &quot;
522          &quot;  name: &#x27;innerproduct2&#x27; &quot;
523          &quot;  type: &#x27;InnerProduct&#x27; &quot;
524          &quot;  inner_product_param { &quot;
525          &quot;    num_output: 10 &quot;
526          &quot;    bias_term: false &quot;
527          &quot;    weight_filler { &quot;
528          &quot;      type: &#x27;constant&#x27; &quot;
529          &quot;      value: 0.5 &quot;
530          &quot;    } &quot;
531          &quot;  } &quot;
532          &quot;  param { name: &#x27;sharedweights&#x27; } &quot;
533          &quot;  bottom: &#x27;innerproduct1&#x27; &quot;
534          &quot;  top: &#x27;innerproduct2&#x27; &quot;
535          &quot;} &quot;
536          &quot;layer { &quot;
537          &quot;  name: &#x27;loss&#x27; &quot;
538          &quot;  type: &#x27;EuclideanLoss&#x27; &quot;
539          &quot;  bottom: &#x27;data2&#x27; &quot;
540          &quot;  bottom: &#x27;innerproduct2&#x27; &quot;
541          &quot;} &quot;;
542      InitNetFromProtoString(proto);
543    }
544    virtual void InitReshapableNet() {
545      const string&amp; proto =
546          &quot;name: &#x27;ReshapableNetwork&#x27; &quot;
547          &quot;layer { &quot;
548          &quot;  name: &#x27;data&#x27; &quot;
549          &quot;  type: &#x27;Input&#x27; &quot;
550          &quot;  top: &#x27;data&#x27; &quot;
551          &quot;  input_param { &quot;
552          &quot;  shape: { dim: 1 dim: 3 dim: 100 dim: 100 } &quot;
553          &quot;  } &quot;
554          &quot;} &quot;
555          &quot;layer { &quot;
556          &quot;  name: &#x27;conv1&#x27; &quot;
557          &quot;  type: &#x27;Convolution&#x27; &quot;
558          &quot;  bottom: &#x27;data&#x27; &quot;
559          &quot;  top: &#x27;conv1&#x27; &quot;
560          &quot;  convolution_param { &quot;
561          &quot;    num_output: 5 &quot;
562          &quot;    kernel_size: 3 &quot;
563          &quot;    stride: 2 &quot;
564          &quot;    weight_filler { &quot;
565          &quot;      type: &#x27;gaussian&#x27; &quot;
566          &quot;      std: 0.01 &quot;
567          &quot;    } &quot;
568          &quot;    bias_filler { &quot;
569          &quot;      type: &#x27;constant&#x27; &quot;
570          &quot;      value: 0.2 &quot;
571          &quot;    } &quot;
572          &quot;  } &quot;
573          &quot;} &quot;
574          &quot;layer { &quot;
575          &quot;  name: &#x27;relu1&#x27; &quot;
576          &quot;  type: &#x27;ReLU&#x27; &quot;
577          &quot;  bottom: &#x27;conv1&#x27; &quot;
578          &quot;  top: &#x27;conv1&#x27; &quot;
579          &quot;} &quot;
580          &quot;layer { &quot;
581          &quot;  name: &#x27;pool1&#x27; &quot;
582          &quot;  type: &#x27;Pooling&#x27; &quot;
583          &quot;  bottom: &#x27;conv1&#x27; &quot;
584          &quot;  top: &#x27;pool1&#x27; &quot;
585          &quot;  pooling_param { &quot;
586          &quot;    pool: MAX &quot;
587          &quot;    kernel_size: 2 &quot;
588          &quot;    stride: 2 &quot;
589          &quot;  } &quot;
590          &quot;} &quot;
591          &quot;layer { &quot;
592          &quot;  name: &#x27;norm1&#x27; &quot;
593          &quot;  type: &#x27;LRN&#x27; &quot;
594          &quot;  bottom: &#x27;pool1&#x27; &quot;
595          &quot;  top: &#x27;norm1&#x27; &quot;
596          &quot;  lrn_param { &quot;
597          &quot;    local_size: 3 &quot;
598          &quot;  } &quot;
599          &quot;} &quot;
600          &quot;layer { &quot;
601          &quot;  name: &#x27;softmax&#x27; &quot;
602          &quot;  type: &#x27;Softmax&#x27; &quot;
603          &quot;  bottom: &#x27;norm1&#x27; &quot;
604          &quot;  top: &#x27;softmax&#x27; &quot;
605          &quot;} &quot;;
606      InitNetFromProtoString(proto);
607    }
608    virtual void InitSkipPropNet(bool test_skip_true) {
609      string proto =
610        &quot;name: &#x27;SkipPropTestNetwork&#x27; &quot;
611        &quot;layer { &quot;
612        &quot;  name: &#x27;data&#x27; &quot;
613        &quot;  type: &#x27;DummyData&#x27; &quot;
614        &quot;  dummy_data_param { &quot;
615        &quot;    shape { &quot;
616        &quot;      dim: 5 &quot;
617        &quot;      dim: 2 &quot;
618        &quot;      dim: 3 &quot;
619        &quot;      dim: 4 &quot;
620        &quot;    } &quot;
621        &quot;    data_filler { &quot;
622        &quot;      type: &#x27;gaussian&#x27; &quot;
623        &quot;      std: 0.01 &quot;
624        &quot;    } &quot;
625        &quot;    shape { &quot;
626        &quot;      dim: 5 &quot;
627        &quot;    } &quot;
628        &quot;    data_filler { &quot;
629        &quot;      type: &#x27;constant&#x27; &quot;
630        &quot;      value: 0 &quot;
631        &quot;    } &quot;
632        &quot;  } &quot;
633        &quot;  top: &#x27;data&#x27; &quot;
634        &quot;  top: &#x27;label&#x27; &quot;
635        &quot;} &quot;
636        &quot;layer { &quot;
637        &quot;  name: &#x27;silence&#x27; &quot;
638        &quot;  bottom: &#x27;label&#x27; &quot;
639        &quot;  type: &#x27;Silence&#x27; &quot;
640        &quot;} &quot;
641        &quot;layer { &quot;
642        &quot;  name: &#x27;innerproduct&#x27; &quot;
643        &quot;  type: &#x27;InnerProduct&#x27; &quot;
644        &quot;  inner_product_param { &quot;
645        &quot;    num_output: 1 &quot;
646        &quot;    weight_filler { &quot;
647        &quot;      type: &#x27;gaussian&#x27; &quot;
648        &quot;      std: 0.01 &quot;
649        &quot;    } &quot;
650        &quot;    bias_filler { &quot;
651        &quot;      type: &#x27;constant&#x27; &quot;
652        &quot;      value: 0 &quot;
653        &quot;    } &quot;
654        &quot;  } &quot;
655        &quot;  param { &quot;
656        &quot;    lr_mult: 1 &quot;
657        &quot;    decay_mult: 1 &quot;
658        &quot;  } &quot;
659        &quot;  param { &quot;
660        &quot;    lr_mult: 2 &quot;
661        &quot;    decay_mult: 0 &quot;
662        &quot;  } &quot;
663        &quot;  bottom: &#x27;data&#x27; &quot;
664        &quot;  top: &#x27;innerproduct&#x27; &quot;
665        &quot;} &quot;
666        &quot;layer { &quot;
667        &quot;  name: &#x27;ip_fake_labels&#x27; &quot;
668        &quot;  type: &#x27;InnerProduct&#x27; &quot;
669        &quot;  inner_product_param { &quot;
670        &quot;    num_output: 1 &quot;
671        &quot;    weight_filler { &quot;
672        &quot;      type: &#x27;gaussian&#x27; &quot;
673        &quot;      std: 0.01 &quot;
674        &quot;    } &quot;
675        &quot;    bias_filler { &quot;
676        &quot;      type: &#x27;constant&#x27; &quot;
677        &quot;      value: 0 &quot;
678        &quot;    } &quot;
679        &quot;  } &quot;
680        &quot;  bottom: &#x27;data&#x27; &quot;
681        &quot;  top: &#x27;fake_labels&#x27; &quot;
682        &quot;} &quot;
683        &quot;layer { &quot;
684        &quot;  name: &#x27;argmax&#x27; &quot;
685        &quot;  bottom: &#x27;fake_labels&#x27; &quot;
686        &quot;  top: &#x27;label_argmax&#x27; &quot;
687        &quot;  type: &#x27;ArgMax&#x27; &quot;
688        &quot;} &quot;
689        &quot;layer { &quot;
690        &quot;  name: &#x27;loss&#x27; &quot;
691        &quot;  bottom: &#x27;innerproduct&#x27; &quot;
692        &quot;  bottom: &#x27;label_argmax&#x27; &quot;;
693      if (test_skip_true)
694        proto += &quot;  propagate_down: true &quot;
695                 &quot;  propagate_down: false &quot;;
696      else
697        proto += &quot;  propagate_down: true &quot;
698                 &quot;  propagate_down: true &quot;;
699      proto +=
700        &quot;  top: &#x27;cross_entropy_loss&#x27; &quot;
701        &quot;  type: &#x27;SigmoidCrossEntropyLoss&#x27; &quot;
702        &quot;  loss_weight: 0.1 &quot;
703        &quot;} &quot;;
704      InitNetFromProtoString(proto);
705    }
706    virtual void InitForcePropNet(bool test_force_true) {
707      string proto =
708        &quot;name: &#x27;ForcePropTestNetwork&#x27; &quot;
709        &quot;layer { &quot;
710        &quot;  name: &#x27;data&#x27; &quot;
711        &quot;  type: &#x27;DummyData&#x27; &quot;
712        &quot;  dummy_data_param { &quot;
713        &quot;    shape { &quot;
714        &quot;      dim: 5 &quot;
715        &quot;      dim: 2 &quot;
716        &quot;      dim: 3 &quot;
717        &quot;      dim: 4 &quot;
718        &quot;    } &quot;
719        &quot;    data_filler { &quot;
720        &quot;      type: &#x27;gaussian&#x27; &quot;
721        &quot;      std: 0.01 &quot;
722        &quot;    } &quot;
723        &quot;    shape { &quot;
724        &quot;      dim: 5 &quot;
725        &quot;    } &quot;
726        &quot;    data_filler { &quot;
727        &quot;      type: &#x27;constant&#x27; &quot;
728        &quot;      value: 0 &quot;
729        &quot;    } &quot;
730        &quot;  } &quot;
731        &quot;  top: &#x27;data&#x27; &quot;
732        &quot;  top: &#x27;label&#x27; &quot;
733        &quot;} &quot;
734        &quot;layer { &quot;
735        &quot;  name: &#x27;innerproduct&#x27; &quot;
736        &quot;  type: &#x27;InnerProduct&#x27; &quot;
737        &quot;  inner_product_param { &quot;
738        &quot;    num_output: 1 &quot;
739        &quot;    weight_filler { &quot;
740        &quot;      type: &#x27;gaussian&#x27; &quot;
741        &quot;      std: 0.01 &quot;
742        &quot;    } &quot;
743        &quot;  } &quot;
744        &quot;  bottom: &#x27;data&#x27; &quot;
745        &quot;  top: &#x27;innerproduct&#x27; &quot;;
746      if (test_force_true) {
747        proto += &quot;  propagate_down: true &quot;;
748      }
749      proto +=
750        &quot;} &quot;
751        &quot;layer { &quot;
752        &quot;  name: &#x27;loss&#x27; &quot;
753        &quot;  bottom: &#x27;innerproduct&#x27; &quot;
754        &quot;  bottom: &#x27;label&#x27; &quot;
755        &quot;  top: &#x27;cross_entropy_loss&#x27; &quot;
756        &quot;  type: &#x27;SigmoidCrossEntropyLoss&#x27; &quot;
757        &quot;} &quot;;
758      InitNetFromProtoString(proto);
759    }
760    virtual void InitAllInOneNet(Phase phase = caffe::TRAIN,
761        const int level = 0, const vector&lt;string&gt;* stages = NULL) {
762      string proto =
763        &quot;name: &#x27;All-in-one Network&#x27;&quot;
764        &quot;layer { &quot;
765        &quot;  name: &#x27;train-data&#x27; &quot;
766        &quot;  type: &#x27;DummyData&#x27; &quot;
767        &quot;  top: &#x27;data&#x27; &quot;
768        &quot;  top: &#x27;label&#x27; &quot;
769        &quot;  dummy_data_param { &quot;
770        &quot;    shape { dim: 1 dim: 10 } &quot;
771        &quot;    shape { dim: 1 dim: 1 } &quot;
772        &quot;  } &quot;
773        &quot;  include { phase: TRAIN stage: &#x27;train&#x27; } &quot;
774        &quot;} &quot;
775        &quot;layer { &quot;
776        &quot;  name: &#x27;val-data&#x27; &quot;
777        &quot;  type: &#x27;DummyData&#x27; &quot;
778        &quot;  top: &#x27;data&#x27; &quot;
779        &quot;  top: &#x27;label&#x27; &quot;
780        &quot;  dummy_data_param { &quot;
781        &quot;    shape { dim: 1 dim: 10 } &quot;
782        &quot;    shape { dim: 1 dim: 1 } &quot;
783        &quot;  } &quot;
784        &quot;  include { phase: TEST stage: &#x27;val&#x27; } &quot;
785        &quot;} &quot;
786        &quot;layer { &quot;
787        &quot;  name: &#x27;deploy-data&#x27; &quot;
788        &quot;  type: &#x27;Input&#x27; &quot;
789        &quot;  top: &#x27;data&#x27; &quot;
790        &quot;  input_param { &quot;
791        &quot;    shape { dim: 1 dim: 10 } &quot;
792        &quot;  } &quot;
793        &quot;  include { phase: TEST stage: &#x27;deploy&#x27; } &quot;
794        &quot;} &quot;
795        &quot;layer { &quot;
796        &quot;  name: &#x27;ip&#x27; &quot;
797        &quot;  type: &#x27;InnerProduct&#x27; &quot;
798        &quot;  bottom: &#x27;data&#x27; &quot;
799        &quot;  top: &#x27;ip&#x27; &quot;
800        &quot;  inner_product_param { &quot;
801        &quot;    num_output: 2 &quot;
802        &quot;  } &quot;
803        &quot;} &quot;
804        &quot;layer { &quot;
805        &quot;  name: &#x27;loss&#x27; &quot;
806        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
807        &quot;  bottom: &#x27;ip&#x27; &quot;
808        &quot;  bottom: &#x27;label&#x27; &quot;
809        &quot;  top: &#x27;loss&#x27; &quot;
810        &quot;  include { phase: TRAIN stage: &#x27;train&#x27; } &quot;
811        &quot;  include { phase: TEST stage: &#x27;val&#x27; } &quot;
812        &quot;} &quot;;
813      InitNetFromProtoFileWithState(proto, phase, level, stages);
814    }
815    int seed_;
816    shared_ptr&lt;Net&lt;Dtype&gt; &gt; net_;
817  };
818  template &lt;typename TypeParam&gt;
819  class NetTest : public ParentTest&lt;MultiDeviceTest&lt;TypeParam&gt;&gt; {
820  };
821  template &lt;typename TypeParam&gt;
822  class NetTestCPU : public ParentTest&lt;CPUDeviceTest&lt;TypeParam&gt;&gt; {
823  };
824  #ifdef USE_MKLDNN_AS_DEFAULT_ENGINE
825  TYPED_TEST_CASE(NetTest, MKLDNNTestDtypesAndDevices);
826  #else
827  TYPED_TEST_CASE(NetTest, TestDtypesAndDevices);
828  #endif
829  TYPED_TEST_CASE(NetTestCPU, TestDtypes);
830  TYPED_TEST(NetTest, TestHasBlob) {
831    this-&gt;InitTinyNet();
832    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;data&quot;));
833    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;label&quot;));
834    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;innerproduct&quot;));
835    EXPECT_FALSE(this-&gt;net_-&gt;has_blob(&quot;loss&quot;));
836    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;top_loss&quot;));
837  }
838  TYPED_TEST(NetTest, TestGetBlob) {
839    this-&gt;InitTinyNet();
840    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;data&quot;), this-&gt;net_-&gt;blobs()[0]);
841    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;label&quot;), this-&gt;net_-&gt;blobs()[1]);
842    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;innerproduct&quot;), this-&gt;net_-&gt;blobs()[2]);
843    EXPECT_FALSE(this-&gt;net_-&gt;blob_by_name(&quot;loss&quot;));
844    EXPECT_EQ(this-&gt;net_-&gt;blob_by_name(&quot;top_loss&quot;), this-&gt;net_-&gt;blobs()[3]);
845  }
846  TYPED_TEST(NetTest, TestHasLayer) {
847    this-&gt;InitTinyNet();
848    EXPECT_TRUE(this-&gt;net_-&gt;has_layer(&quot;data&quot;));
849    EXPECT_TRUE(this-&gt;net_-&gt;has_layer(&quot;innerproduct&quot;));
850    EXPECT_TRUE(this-&gt;net_-&gt;has_layer(&quot;loss&quot;));
851    EXPECT_FALSE(this-&gt;net_-&gt;has_layer(&quot;label&quot;));
852  }
853  TYPED_TEST(NetTest, TestGetLayerByName) {
854    this-&gt;InitTinyNet();
855    EXPECT_EQ(this-&gt;net_-&gt;layer_by_name(&quot;data&quot;), this-&gt;net_-&gt;layers()[0]);
856    EXPECT_EQ(this-&gt;net_-&gt;layer_by_name(&quot;innerproduct&quot;), this-&gt;net_-&gt;layers()[1]);
857    EXPECT_EQ(this-&gt;net_-&gt;layer_by_name(&quot;loss&quot;), this-&gt;net_-&gt;layers()[2]);
858    EXPECT_FALSE(this-&gt;net_-&gt;layer_by_name(&quot;label&quot;));
859  }
860  TYPED_TEST(NetTest, TestBottomNeedBackward) {
861    this-&gt;InitTinyNet();
862    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
863        this-&gt;net_-&gt;bottom_need_backward();
864    EXPECT_EQ(3, bottom_need_backward.size());
865    EXPECT_EQ(0, bottom_need_backward[0].size());
866    EXPECT_EQ(1, bottom_need_backward[1].size());
867    EXPECT_EQ(false, bottom_need_backward[1][0]);
868    EXPECT_EQ(2, bottom_need_backward[2].size());
869    EXPECT_EQ(true, bottom_need_backward[2][0]);
870    EXPECT_EQ(false, bottom_need_backward[2][1]);
871  }
872  TYPED_TEST(NetTest, TestBottomNeedBackwardForce) {
873    const bool force_backward = true;
874    this-&gt;InitTinyNet(force_backward);
875    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
876        this-&gt;net_-&gt;bottom_need_backward();
877    EXPECT_EQ(3, bottom_need_backward.size());
878    EXPECT_EQ(0, bottom_need_backward[0].size());
879    EXPECT_EQ(1, bottom_need_backward[1].size());
880    EXPECT_EQ(true, bottom_need_backward[1][0]);
881    EXPECT_EQ(2, bottom_need_backward[2].size());
882    EXPECT_EQ(true, bottom_need_backward[2][0]);
883    EXPECT_EQ(false, bottom_need_backward[2][1]);
884  }
885  TYPED_TEST(NetTest, TestBottomNeedBackwardEuclideanForce) {
886    const bool force_backward = true;
887    this-&gt;InitTinyNetEuclidean(force_backward);
888    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
889        this-&gt;net_-&gt;bottom_need_backward();
890    EXPECT_EQ(3, bottom_need_backward.size());
891    EXPECT_EQ(0, bottom_need_backward[0].size());
892    EXPECT_EQ(1, bottom_need_backward[1].size());
893    EXPECT_EQ(true, bottom_need_backward[1][0]);
894    EXPECT_EQ(2, bottom_need_backward[2].size());
895    EXPECT_EQ(true, bottom_need_backward[2][0]);
896    EXPECT_EQ(true, bottom_need_backward[2][1]);
897  }
898  TYPED_TEST(NetTest, TestBottomNeedBackwardTricky) {
899    this-&gt;InitTrickyNet();
900    const vector&lt;vector&lt;bool&gt; &gt;&amp; bottom_need_backward =
901        this-&gt;net_-&gt;bottom_need_backward();
902    EXPECT_EQ(4, bottom_need_backward.size());
903    EXPECT_EQ(0, bottom_need_backward[0].size());
904    EXPECT_EQ(1, bottom_need_backward[1].size());
905    EXPECT_EQ(false, bottom_need_backward[1][0]);
906    EXPECT_EQ(1, bottom_need_backward[2].size());
907    EXPECT_EQ(false, bottom_need_backward[2][0]);
908    EXPECT_EQ(2, bottom_need_backward[3].size());
909    EXPECT_EQ(true, bottom_need_backward[3][0]);
910    EXPECT_EQ(true, bottom_need_backward[3][1]);
911  }
912  TYPED_TEST(NetTest, TestLossWeight) {
913    typedef typename TypeParam::Dtype Dtype;
914    vector&lt;Blob&lt;Dtype&gt;*&gt; bottom;
915    Caffe::set_random_seed(this-&gt;seed_);
916    const bool kForceBackward = true;
917    this-&gt;InitUnsharedWeightsNet(NULL, NULL, kForceBackward);
918    const Dtype loss = this-&gt;net_-&gt;ForwardBackward();
919    const bool kCopyDiff = true;
920    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blob_grads;
921    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads);
922    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_grads;
923    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads);
924    const Dtype kMinLossAbsValue = 1e-2;
925    ASSERT_GE(fabs(loss), kMinLossAbsValue);
926    const Dtype kErrorMargin = 1e-4;
927    const int kNumLossWeights = 6;
928    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
929    for (int i = 0; i &lt; kNumLossWeights; ++i) {
930      Caffe::set_random_seed(this-&gt;seed_);
931      this-&gt;InitUnsharedWeightsNet(&amp;kLossWeights[i], NULL, kForceBackward);
932      const Dtype weighted_loss = this-&gt;net_-&gt;ForwardBackward();
933      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
934      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
935          &lt;&lt; &quot;loss weight = &quot; &lt;&lt; kLossWeights[i];
936      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; weighted_blobs =
937          this-&gt;net_-&gt;blobs();
938      ASSERT_EQ(blob_grads.size(), weighted_blobs.size());
939      for (int j = 0; j &lt; blob_grads.size(); ++j) {
940        ASSERT_EQ(blob_grads[j]-&gt;count(), weighted_blobs[j]-&gt;count());
941        for (int k = 0; k &lt; blob_grads[j]-&gt;count(); ++k) {
942          EXPECT_NEAR(blob_grads[j]-&gt;cpu_diff()[k] * kLossWeights[i],
943                      weighted_blobs[j]-&gt;cpu_diff()[k], error_margin);
944        }
945      }
946      const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; weighted_params =
947          this-&gt;net_-&gt;params();
948      ASSERT_EQ(param_grads.size(), weighted_params.size());
949      for (int j = 0; j &lt; param_grads.size(); ++j) {
950        ASSERT_EQ(param_grads[j]-&gt;count(), weighted_params[j]-&gt;count());
951        for (int k = 0; k &lt; param_grads[j]-&gt;count(); ++k) {
952          EXPECT_NEAR(param_grads[j]-&gt;cpu_diff()[k] * kLossWeights[i],
953                      weighted_params[j]-&gt;cpu_diff()[k], error_margin);
954        }
955      }
956    }
957  }
958  TYPED_TEST(NetTest, TestLossWeightMidNet) {
959    typedef typename TypeParam::Dtype Dtype;
960    Caffe::set_random_seed(this-&gt;seed_);
961    const bool kForceBackward = true;
962    Dtype loss_weight = 0;
963    Dtype midnet_loss_weight = 1;
964    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
965                                 kForceBackward);
966    const Dtype loss = this-&gt;net_-&gt;ForwardBackward();
967    const bool kCopyDiff = true;
968    const bool kReshape = true;
969    Blob&lt;Dtype&gt; data_grad;
970    data_grad.CopyFrom(*this-&gt;net_-&gt;blob_by_name(&quot;data&quot;), kCopyDiff, kReshape);
971    const Dtype kMinLossAbsValue = 1e-2;
972    ASSERT_GE(fabs(loss), kMinLossAbsValue);
973    const Dtype kErrorMargin = 1e-4;
974    const int kNumLossWeights = 6;
975    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
976    for (int i = 0; i &lt; kNumLossWeights; ++i) {
977      Caffe::set_random_seed(this-&gt;seed_);
978      this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;kLossWeights[i],
979                                   kForceBackward);
980      const Dtype weighted_loss = this-&gt;net_-&gt;ForwardBackward();
981      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
982      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
983          &lt;&lt; &quot;loss weight = &quot; &lt;&lt; kLossWeights[i];
984      const shared_ptr&lt;Blob&lt;Dtype&gt; &gt;&amp; weighted_blob =
985          this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
986      ASSERT_EQ(data_grad.count(), weighted_blob-&gt;count());
987      for (int j = 0; j &lt; data_grad.count(); ++j) {
988        EXPECT_NEAR(data_grad.cpu_diff()[j] * kLossWeights[i],
989                    weighted_blob-&gt;cpu_diff()[j], error_margin);
990      }
991    }
992  }
993  TYPED_TEST(NetTest, TestComboLossWeight) {
994    typedef typename TypeParam::Dtype Dtype;
995    Dtype loss_weight;
996    Dtype midnet_loss_weight;
997    const bool kForceBackward = true;
998    const Dtype kErrorMargin = 1e-4;
999    loss_weight = 1;
1000    midnet_loss_weight = 1;
1001    Caffe::set_random_seed(this-&gt;seed_);
1002    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1003                                 kForceBackward);
1004    const Dtype loss = this-&gt;net_-&gt;ForwardBackward();
1005    const bool kCopyDiff = true;
1006    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blob_grads;
1007    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads);
1008    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_grads;
1009    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads);
1010    loss_weight = 2;
1011    midnet_loss_weight = 1;
1012    Caffe::set_random_seed(this-&gt;seed_);
1013    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1014                                 kForceBackward);
1015    const Dtype loss_main_2 = this-&gt;net_-&gt;ForwardBackward();
1016    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blob_grads_loss_2;
1017    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads_loss_2);
1018    vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; param_grads_loss_2;
1019    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads_loss_2);
1020    loss_weight = 3;
1021    midnet_loss_weight = 1;
1022    Caffe::set_random_seed(this-&gt;seed_);
1023    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1024                                 kForceBackward);
1025    const Dtype loss_main_3 = this-&gt;net_-&gt;ForwardBackward();
1026    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blob_grads_loss_3 =
1027        this-&gt;net_-&gt;blobs();
1028    ASSERT_EQ(blob_grads.size(), blob_grads_loss_3.size());
1029    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_loss_3.size());
1030    for (int j = 0; j &lt; blob_grads.size(); ++j) {
1031      const string&amp; blob_name = this-&gt;net_-&gt;blob_names()[j];
1032      bool grad_should_change = true;
1033      if (blob_name == &quot;innerproduct1_innerproduct1_0_split_0&quot;) {
1034        grad_should_change = false;
1035      }
1036      ASSERT_EQ(blob_grads[j]-&gt;count(), blob_grads_loss_3[j]-&gt;count());
1037      ASSERT_EQ(blob_grads_loss_2[j]-&gt;count(), blob_grads_loss_3[j]-&gt;count());
1038      for (int k = 0; k &lt; blob_grads[j]-&gt;count(); ++k) {
1039        const Dtype grad_diff_2 = blob_grads_loss_2[j]-&gt;cpu_diff()[k] -
1040                                      blob_grads[j]-&gt;cpu_diff()[k];
1041        const Dtype grad_diff_3 = blob_grads_loss_3[j]-&gt;cpu_diff()[k] -
1042                                      blob_grads[j]-&gt;cpu_diff()[k];
1043        if (grad_should_change) {
1044          const Dtype kMinGradDiffAbsValue = 1e-4;
1045          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) &lt;&lt; blob_name;
1046          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) &lt;&lt; blob_name;
1047        } else {
1048          EXPECT_EQ(0, grad_diff_2) &lt;&lt; blob_name;
1049          EXPECT_EQ(0, grad_diff_3) &lt;&lt; blob_name;
1050        }
1051      }
1052    }
1053    loss_weight = 1;
1054    midnet_loss_weight = 2;
1055    Caffe::set_random_seed(this-&gt;seed_);
1056    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1057                                 kForceBackward);
1058    const Dtype loss_midnet_2 = this-&gt;net_-&gt;ForwardBackward();
1059    this-&gt;CopyNetBlobs(kCopyDiff, &amp;blob_grads_loss_2);
1060    this-&gt;CopyNetParams(kCopyDiff, &amp;param_grads_loss_2);
1061    loss_weight = 1;
1062    midnet_loss_weight = 3;
1063    Caffe::set_random_seed(this-&gt;seed_);
1064    this-&gt;InitUnsharedWeightsNet(&amp;loss_weight, &amp;midnet_loss_weight,
1065                                 kForceBackward);
1066    const Dtype loss_midnet_3 = this-&gt;net_-&gt;ForwardBackward();
1067    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; blob_grads_midnet_loss_3 =
1068        this-&gt;net_-&gt;blobs();
1069    ASSERT_EQ(blob_grads.size(), blob_grads_midnet_loss_3.size());
1070    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_midnet_loss_3.size());
1071    const vector&lt;string&gt;&amp; blob_names = this-&gt;net_-&gt;blob_names();
1072    for (int j = 0; j &lt; blob_grads.size(); ++j) {
1073      const string&amp; blob_name = blob_names[j];
1074      bool grad_should_change = false;
1075      if (blob_name == &quot;innerproduct1&quot; ||
1076          blob_name == &quot;innerproduct1_innerproduct1_0_split_0&quot; ||
1077          blob_name == &quot;data_data_0_split_0&quot; || blob_name == &quot;data&quot;) {
1078        grad_should_change = true;
1079      }
1080      ASSERT_EQ(blob_grads[j]-&gt;count(), blob_grads_midnet_loss_3[j]-&gt;count());
1081      ASSERT_EQ(blob_grads[j]-&gt;count(), blob_grads_loss_2[j]-&gt;count());
1082      for (int k = 0; k &lt; blob_grads[j]-&gt;count(); ++k) {
1083        const Dtype grad_diff_2 = blob_grads_loss_2[j]-&gt;cpu_diff()[k] -
1084                                      blob_grads[j]-&gt;cpu_diff()[k];
1085        const Dtype grad_diff_3 = blob_grads_midnet_loss_3[j]-&gt;cpu_diff()[k] -
1086                                      blob_grads[j]-&gt;cpu_diff()[k];
1087        if (grad_should_change) {
1088          const Dtype kMinGradDiffAbsValue = 1e-4;
1089          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) &lt;&lt; blob_name;
1090          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) &lt;&lt; blob_name;
1091        } else {
1092          EXPECT_EQ(0, grad_diff_2) &lt;&lt; blob_name;
1093          EXPECT_EQ(0, grad_diff_3) &lt;&lt; blob_name;
1094        }
1095      }
1096    }
1097    const Dtype kMinLossDiffAbsValue = 1e-4;
1098    Dtype loss_diff_2 = loss_main_2 - loss;
1099    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1100    Dtype loss_diff_3 = loss_main_3 - loss;
1101    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1102    loss_diff_2 = loss_midnet_2 - loss;
1103    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1104    loss_diff_3 = loss_midnet_3 - loss;
1105    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1106  }
1107  TYPED_TEST(NetTest, TestBackwardWithAccuracyLayer) {
1108    const bool kForceBackward = false;
1109    const bool kAccuracyLayer = true;
1110    this-&gt;InitTinyNet(kForceBackward, kAccuracyLayer);
1111    EXPECT_TRUE(this-&gt;net_-&gt;has_blob(&quot;accuracy&quot;));
1112    this-&gt;net_-&gt;ForwardBackward();
1113  }
1114  TYPED_TEST(NetTest, TestUnsharedWeightsDataNet) {
1115    typedef typename TypeParam::Dtype Dtype;
1116    this-&gt;InitUnsharedWeightsNet();
1117    Dtype loss;
1118    this-&gt;net_-&gt;Forward(&amp;loss);
1119    EXPECT_GT(loss, 0);
1120  }
1121  TYPED_TEST(NetTest, TestSharedWeightsDataNet) {
1122    typedef typename TypeParam::Dtype Dtype;
1123    this-&gt;InitSharedWeightsNet();
1124    Dtype loss;
1125    this-&gt;net_-&gt;Forward(&amp;loss);
1126    EXPECT_FLOAT_EQ(loss, 0);
1127  }
1128  TYPED_TEST(NetTest, TestUnsharedWeightsDiffNet) {
1129    typedef typename TypeParam::Dtype Dtype;
1130    this-&gt;InitUnsharedWeightsNet();
1131    Net&lt;Dtype&gt;* net = this-&gt;net_.get();
1132    net-&gt;Forward();
1133    net-&gt;Backward();
1134    Layer&lt;Dtype&gt;* ip1_layer = net-&gt;layer_by_name(&quot;innerproduct1&quot;).get();
1135    Layer&lt;Dtype&gt;* ip2_layer = net-&gt;layer_by_name(&quot;innerproduct2&quot;).get();
1136    const int count = ip1_layer-&gt;blobs()[0]-&gt;count();
1137    const Dtype* grad1 = ip1_layer-&gt;blobs()[0]-&gt;cpu_diff();
1138    const Dtype* grad2 = ip2_layer-&gt;blobs()[0]-&gt;cpu_diff();
1139    for (int i = 0; i &lt; count; ++i) {
1140      EXPECT_GT(fabs(grad1[i]), 0);
1141      EXPECT_FLOAT_EQ(-1 * grad1[i], grad2[i]);
1142    }
1143  }
1144  TYPED_TEST(NetTest, TestSharedWeightsDiffNet) {
1145    typedef typename TypeParam::Dtype Dtype;
1146    this-&gt;InitSharedWeightsNet();
1147    Net&lt;Dtype&gt;* net = this-&gt;net_.get();
1148    Dtype loss;
1149    net-&gt;Forward(&amp;loss);
1150    net-&gt;Backward();
1151    EXPECT_FLOAT_EQ(loss, 0);
1152    Layer&lt;Dtype&gt;* ip1_layer = net-&gt;layer_by_name(&quot;innerproduct1&quot;).get();
1153    Layer&lt;Dtype&gt;* ip2_layer = net-&gt;layer_by_name(&quot;innerproduct2&quot;).get();
1154    const int count = ip1_layer-&gt;blobs()[0]-&gt;count();
1155    const Dtype* grad1 = ip1_layer-&gt;blobs()[0]-&gt;cpu_diff();
1156    const Dtype* grad2 = ip2_layer-&gt;blobs()[0]-&gt;cpu_diff();
1157    for (int i = 0; i &lt; count; ++i) {
1158      EXPECT_FLOAT_EQ(0, grad1[i]);
1159      EXPECT_FLOAT_EQ(0, grad2[i]);
1160    }
1161  }
1162  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1163  TYPED_TEST(NetTest, TestSharedWeightsUpdate) {
1164    typedef typename TypeParam::Dtype Dtype;
1165    Caffe::set_random_seed(this-&gt;seed_);
1166    this-&gt;InitDiffDataSharedWeightsNet();
1167    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[1], &quot;innerproduct1&quot;);
1168    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[2], &quot;innerproduct2&quot;);
1169    Blob&lt;Dtype&gt;* ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1170    Blob&lt;Dtype&gt;* ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1171    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1172    EXPECT_EQ(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1173    this-&gt;net_-&gt;Forward();
1174    this-&gt;net_-&gt;Backward();
1175    Blob&lt;Dtype&gt; shared_params;
1176    const bool reshape = true;
1177    const bool copy_diff = false;
1178    shared_params.CopyFrom(*ip1_weights, copy_diff, reshape);
1179    shared_params.CopyFrom(*ip1_weights, !copy_diff, reshape);
1180    const int count = ip1_weights-&gt;count();
1181    for (int i = 0; i &lt; count; ++i) {
1182      EXPECT_NE(0, ip1_weights-&gt;cpu_diff()[i]);
1183    }
1184    caffe_axpy(count, Dtype(-1), shared_params.cpu_diff(),
1185               shared_params.mutable_cpu_data());
1186    const Dtype* expected_updated_params = shared_params.cpu_data();
1187    this-&gt;net_-&gt;Update();
1188    const Dtype* actual_updated_params = ip1_weights-&gt;cpu_data();
1189    for (int i = 0; i &lt; count; ++i) {
1190      EXPECT_EQ(expected_updated_params[i], actual_updated_params[i]);
1191    }
1192    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1193    Caffe::set_random_seed(this-&gt;seed_);
1194    this-&gt;InitDiffDataUnsharedWeightsNet();
1195    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[1], &quot;innerproduct1&quot;);
1196    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[2], &quot;innerproduct2&quot;);
1197    ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1198    ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1199    EXPECT_NE(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1200    EXPECT_NE(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1201    this-&gt;net_-&gt;Forward();
1202    this-&gt;net_-&gt;Backward();
1203    Blob&lt;Dtype&gt; unshared_params1;
1204    unshared_params1.CopyFrom(*ip1_weights, copy_diff, reshape);
1205    unshared_params1.CopyFrom(*ip1_weights, !copy_diff, reshape);
1206    Blob&lt;Dtype&gt; unshared_params2;
1207    unshared_params2.CopyFrom(*ip2_weights, copy_diff, reshape);
1208    unshared_params2.CopyFrom(*ip2_weights, !copy_diff, reshape);
1209    for (int i = 0; i &lt; count; ++i) {
1210      EXPECT_NE(0, ip1_weights-&gt;cpu_diff()[i]);
1211      EXPECT_NE(0, ip2_weights-&gt;cpu_diff()[i]);
1212      EXPECT_NE(ip1_weights-&gt;cpu_diff()[i], ip2_weights-&gt;cpu_diff()[i]);
1213      EXPECT_FLOAT_EQ(ip1_weights-&gt;cpu_diff()[i] + ip2_weights-&gt;cpu_diff()[i],
1214                      shared_params.cpu_diff()[i]);
1215    }
1216    caffe_axpy(count, Dtype(-1), ip1_weights-&gt;cpu_diff(),
1217               unshared_params1.mutable_cpu_data());
1218    caffe_axpy(count, Dtype(-1), ip2_weights-&gt;cpu_diff(),
1219               unshared_params2.mutable_cpu_data());
1220    const Dtype* expected_updated_params1 = unshared_params1.cpu_data();
1221    const Dtype* expected_updated_params2 = unshared_params2.cpu_data();
1222    this-&gt;net_-&gt;Update();
1223    const Dtype* actual_updated_params1 = ip1_weights-&gt;cpu_data();
1224    const Dtype* actual_updated_params2 = ip2_weights-&gt;cpu_data();
1225    for (int i = 0; i &lt; count; ++i) {
1226      EXPECT_EQ(expected_updated_params1[i], actual_updated_params1[i]);
1227      EXPECT_EQ(expected_updated_params2[i], actual_updated_params2[i]);
1228      EXPECT_NE(actual_updated_params1[i], actual_updated_params2[i]);
1229      EXPECT_NE(expected_updated_params, expected_updated_params1);
1230    }
1231  }
1232  #endif
1233  TYPED_TEST(NetTest, TestSharedWeightsResume) {
1234    typedef typename TypeParam::Dtype Dtype;
1235    Caffe::set_random_seed(this-&gt;seed_);
1236    this-&gt;InitDiffDataSharedWeightsNet();
1237    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[1], &quot;innerproduct1&quot;);
1238    EXPECT_EQ(this-&gt;net_-&gt;layer_names()[2], &quot;innerproduct2&quot;);
1239    Blob&lt;Dtype&gt;* ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1240    Blob&lt;Dtype&gt;* ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1241    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1242    EXPECT_EQ(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1243    this-&gt;net_-&gt;ForwardBackward();
1244    this-&gt;net_-&gt;Update();
1245    Blob&lt;Dtype&gt; shared_params;
1246    const bool kReshape = true;
1247    const bool kCopyDiff = false;
1248    shared_params.CopyFrom(*ip1_weights, kCopyDiff, kReshape);
1249    const int count = ip1_weights-&gt;count();
1250    NetParameter net_param;
1251    this-&gt;net_-&gt;ToProto(&amp;net_param);
1252    Caffe::set_random_seed(this-&gt;seed_);
1253    this-&gt;InitDiffDataSharedWeightsNet();
1254    this-&gt;net_-&gt;CopyTrainedLayersFrom(net_param);
1255    ip1_weights = this-&gt;net_-&gt;layers()[1]-&gt;blobs()[0].get();
1256    ip2_weights = this-&gt;net_-&gt;layers()[2]-&gt;blobs()[0].get();
1257    ASSERT_FALSE(NULL == ip1_weights);
1258    ASSERT_FALSE(NULL == ip2_weights);
1259    EXPECT_NE(ip1_weights, ip2_weights);
1260    EXPECT_EQ(ip1_weights-&gt;cpu_data(), ip2_weights-&gt;cpu_data());
1261    EXPECT_EQ(ip1_weights-&gt;cpu_diff(), ip2_weights-&gt;cpu_diff());
1262    for (int i = 0; i &lt; count; ++i) {
1263      EXPECT_FLOAT_EQ(shared_params.cpu_data()[i], ip1_weights-&gt;cpu_data()[i]);
1264    }
1265  }
1266  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1267  TYPED_TEST(NetTest, TestParamPropagateDown) {
1268    typedef typename TypeParam::Dtype Dtype;
1269    const bool kBiasTerm = true, kForceBackward = false;
1270    const Dtype* kLossWeight1 = NULL;
1271    const Dtype* kLossWeight2 = NULL;
1272    Caffe::set_random_seed(this-&gt;seed_);
1273    Dtype blobs_lr_w1 = 1, blobs_lr_w2 = 1, blobs_lr_b1 = 2, blobs_lr_b2 = 2;
1274    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1275        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1276    this-&gt;net_-&gt;Forward();
1277    this-&gt;net_-&gt;Backward();
1278    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params = this-&gt;net_-&gt;params();
1279    const int num_params = params.size();
1280    ASSERT_EQ(4, num_params);
1281    const Dtype kNonZeroTestMin = 1e-3;
1282    vector&lt;Dtype&gt; param_asums(params.size());
1283    for (int i = 0; i &lt; num_params; ++i) {
1284      const Dtype param_asum =
1285         caffe_cpu_asum(params[i]-&gt;count(), params[i]-&gt;cpu_diff());
1286      param_asums[i] = param_asum;
1287      EXPECT_GT(param_asum, kNonZeroTestMin);
1288    }
1289    Caffe::set_random_seed(this-&gt;seed_);
1290    blobs_lr_w1 *= 2, blobs_lr_w2 *= 2, blobs_lr_b1 *= 2, blobs_lr_b2 *= 2;
1291    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1292        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1293    this-&gt;net_-&gt;Forward();
1294    this-&gt;net_-&gt;Backward();
1295    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params2 = this-&gt;net_-&gt;params();
1296    ASSERT_EQ(num_params, params2.size());
1297    for (int i = 0; i &lt; num_params; ++i) {
1298      const Dtype param_asum =
1299         caffe_cpu_asum(params2[i]-&gt;count(), params2[i]-&gt;cpu_diff());
1300      EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1301    }
1302    Caffe::set_random_seed(this-&gt;seed_);
1303    blobs_lr_w1 = 1, blobs_lr_w2 = 0, blobs_lr_b1 = 0, blobs_lr_b2 = 1;
1304    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1305        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1306    this-&gt;net_-&gt;Forward();
1307    this-&gt;net_-&gt;Backward();
1308    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params3 = this-&gt;net_-&gt;params();
1309    ASSERT_EQ(num_params, params3.size());
1310    for (int i = 0; i &lt; num_params; ++i) {
1311      const Dtype param_asum =
1312         caffe_cpu_asum(params3[i]-&gt;count(), params3[i]-&gt;cpu_diff());
1313      if (i == 1 || i == 2) {
1314        EXPECT_FLOAT_EQ(0, param_asum);
1315      } else {
1316        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1317      }
1318    }
1319    Caffe::set_random_seed(this-&gt;seed_);
1320    blobs_lr_w1 = 0, blobs_lr_w2 = 1, blobs_lr_b1 = 1, blobs_lr_b2 = 0;
1321    this-&gt;InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1322        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1323    this-&gt;net_-&gt;Forward();
1324    this-&gt;net_-&gt;Backward();
1325    const vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt;&amp; params4 = this-&gt;net_-&gt;params();
1326    ASSERT_EQ(num_params, params4.size());
1327    for (int i = 0; i &lt; num_params; ++i) {
1328      const Dtype param_asum =
1329         caffe_cpu_asum(params4[i]-&gt;count(), params4[i]-&gt;cpu_diff());
1330      if (i == 0 || i == 3) {
1331        EXPECT_FLOAT_EQ(0, param_asum);
1332      } else {
1333        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1334      }
1335    }
1336  }
1337  #endif
1338  TYPED_TEST(NetTest, TestFromTo) {
1339    typedef typename TypeParam::Dtype Dtype;
1340    this-&gt;InitTinyNet();
1341    Blob&lt;Dtype&gt; data;
1342    data.ReshapeLike(*this-&gt;net_-&gt;blob_by_name(&quot;data&quot;));
1343    this-&gt;net_-&gt;Forward();
1344    this-&gt;net_-&gt;Backward();
1345    data.CopyFrom(*this-&gt;net_-&gt;blob_by_name(&quot;data&quot;), true, true);
1346    const Dtype *loss_ptr = this-&gt;net_-&gt;output_blobs()[0]-&gt;cpu_data();
1347    Dtype loss = *loss_ptr;
1348    for (int i = 1; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
1349      this-&gt;net_-&gt;ForwardFromTo(1, 1);
1350      if (i &lt; this-&gt;net_-&gt;layers().size() - 1) {
1351        this-&gt;net_-&gt;ForwardFrom(i + 1);
1352      }
1353      EXPECT_EQ(loss, *loss_ptr);
1354    }
1355    for (int i = 1; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
1356      this-&gt;net_-&gt;BackwardTo(i);
1357      this-&gt;net_-&gt;BackwardFrom(i - 1);
1358      for (int j = 0; j &lt; data.count(); ++j) {
1359        EXPECT_EQ(data.cpu_diff()[j],
1360            this-&gt;net_-&gt;blob_by_name(&quot;data&quot;)-&gt;cpu_diff()[j]);
1361      }
1362    }
1363  }
1364  class FilterNetTest : public ::testing::Test {
1365   protected:
1366    void RunFilterNetTest(
1367        const string&amp; input_param_string, const string&amp; filtered_param_string) {
1368      NetParameter input_param;
1369      CHECK(google::protobuf::TextFormat::ParseFromString(
1370          input_param_string, &amp;input_param));
1371      NetParameter expected_filtered_param;
1372      CHECK(google::protobuf::TextFormat::ParseFromString(
1373          filtered_param_string, &amp;expected_filtered_param));
1374      NetParameter actual_filtered_param;
1375      Net&lt;float&gt;::FilterNet(input_param, &amp;actual_filtered_param);
1376      EXPECT_EQ(expected_filtered_param.DebugString(),
1377          actual_filtered_param.DebugString());
1378      NetParameter double_filtered_param;
1379      Net&lt;float&gt;::FilterNet(actual_filtered_param, &amp;double_filtered_param);
1380      EXPECT_EQ(actual_filtered_param.DebugString(),
1381         double_filtered_param.DebugString());
1382    }
1383  };
1384  TEST_F(FilterNetTest, TestNoFilter) {
1385    const string&amp; input_proto =
1386        &quot;name: &#x27;TestNetwork&#x27; &quot;
1387        &quot;layer { &quot;
1388        &quot;  name: &#x27;data&#x27; &quot;
1389        &quot;  type: &#x27;Data&#x27; &quot;
1390        &quot;  top: &#x27;data&#x27; &quot;
1391        &quot;  top: &#x27;label&#x27; &quot;
1392        &quot;} &quot;
1393        &quot;layer { &quot;
1394        &quot;  name: &#x27;innerprod&#x27; &quot;
1395        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1396        &quot;  bottom: &#x27;data&#x27; &quot;
1397        &quot;  top: &#x27;innerprod&#x27; &quot;
1398        &quot;} &quot;
1399        &quot;layer { &quot;
1400        &quot;  name: &#x27;loss&#x27; &quot;
1401        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1402        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1403        &quot;  bottom: &#x27;label&#x27; &quot;
1404        &quot;} &quot;;
1405    this-&gt;RunFilterNetTest(input_proto, input_proto);
1406  }
1407  TEST_F(FilterNetTest, TestFilterLeNetTrainTest) {
1408    const string&amp; input_proto =
1409        &quot;name: &#x27;LeNet&#x27; &quot;
1410        &quot;layer { &quot;
1411        &quot;  name: &#x27;mnist&#x27; &quot;
1412        &quot;  type: &#x27;Data&#x27; &quot;
1413        &quot;  top: &#x27;data&#x27; &quot;
1414        &quot;  top: &#x27;label&#x27; &quot;
1415        &quot;  data_param { &quot;
1416        &quot;    source: &#x27;mnist-train-leveldb&#x27; &quot;
1417        &quot;    batch_size: 64 &quot;
1418        &quot;  } &quot;
1419        &quot;  transform_param { &quot;
1420        &quot;    scale: 0.00390625 &quot;
1421        &quot;  } &quot;
1422        &quot;  include: { phase: TRAIN } &quot;
1423        &quot;} &quot;
1424        &quot;layer { &quot;
1425        &quot;  name: &#x27;mnist&#x27; &quot;
1426        &quot;  type: &#x27;Data&#x27; &quot;
1427        &quot;  top: &#x27;data&#x27; &quot;
1428        &quot;  top: &#x27;label&#x27; &quot;
1429        &quot;  data_param { &quot;
1430        &quot;    source: &#x27;mnist-test-leveldb&#x27; &quot;
1431        &quot;    batch_size: 100 &quot;
1432        &quot;  } &quot;
1433        &quot;  transform_param { &quot;
1434        &quot;    scale: 0.00390625 &quot;
1435        &quot;  } &quot;
1436        &quot;  include: { phase: TEST } &quot;
1437        &quot;} &quot;
1438        &quot;layer { &quot;
1439        &quot;  name: &#x27;conv1&#x27; &quot;
1440        &quot;  type: &#x27;Convolution&#x27; &quot;
1441        &quot;  bottom: &#x27;data&#x27; &quot;
1442        &quot;  top: &#x27;conv1&#x27; &quot;
1443        &quot;  param { &quot;
1444        &quot;    lr_mult: 1 &quot;
1445        &quot;  } &quot;
1446        &quot;  param { &quot;
1447        &quot;    lr_mult: 2 &quot;
1448        &quot;  } &quot;
1449        &quot;  convolution_param { &quot;
1450        &quot;    num_output: 20 &quot;
1451        &quot;    kernel_size: 5 &quot;
1452        &quot;    stride: 1 &quot;
1453        &quot;    weight_filler { &quot;
1454        &quot;      type: &#x27;xavier&#x27; &quot;
1455        &quot;    } &quot;
1456        &quot;    bias_filler { &quot;
1457        &quot;      type: &#x27;constant&#x27; &quot;
1458        &quot;    } &quot;
1459        &quot;  } &quot;
1460        &quot;} &quot;
1461        &quot;layer { &quot;
1462        &quot;  name: &#x27;ip1&#x27; &quot;
1463        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1464        &quot;  bottom: &#x27;conv1&#x27; &quot;
1465        &quot;  top: &#x27;ip1&#x27; &quot;
1466        &quot;  param { &quot;
1467        &quot;    lr_mult: 1 &quot;
1468        &quot;  } &quot;
1469        &quot;  param { &quot;
1470        &quot;    lr_mult: 2 &quot;
1471        &quot;  } &quot;
1472        &quot;  inner_product_param { &quot;
1473        &quot;    num_output: 10 &quot;
1474        &quot;    weight_filler { &quot;
1475        &quot;      type: &#x27;xavier&#x27; &quot;
1476        &quot;    } &quot;
1477        &quot;    bias_filler { &quot;
1478        &quot;      type: &#x27;constant&#x27; &quot;
1479        &quot;    } &quot;
1480        &quot;  } &quot;
1481        &quot;} &quot;
1482        &quot;layer { &quot;
1483        &quot;  name: &#x27;accuracy&#x27; &quot;
1484        &quot;  type: &#x27;Accuracy&#x27; &quot;
1485        &quot;  bottom: &#x27;ip1&#x27; &quot;
1486        &quot;  bottom: &#x27;label&#x27; &quot;
1487        &quot;  top: &#x27;accuracy&#x27; &quot;
1488        &quot;  include: { phase: TEST } &quot;
1489        &quot;} &quot;
1490        &quot;layer { &quot;
1491        &quot;  name: &#x27;loss&#x27; &quot;
1492        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1493        &quot;  bottom: &#x27;ip2&#x27; &quot;
1494        &quot;  bottom: &#x27;label&#x27; &quot;
1495        &quot;  top: &#x27;loss&#x27; &quot;
1496        &quot;} &quot;;
1497    const string input_proto_train = &quot;state: { phase: TRAIN } &quot; + input_proto;
1498    const string input_proto_test = &quot;state: { phase: TEST } &quot; + input_proto;
1499    const string output_proto_train =
1500        &quot;name: &#x27;LeNet&#x27; &quot;
1501        &quot;layer { &quot;
1502        &quot;  name: &#x27;mnist&#x27; &quot;
1503        &quot;  type: &#x27;Data&#x27; &quot;
1504        &quot;  top: &#x27;data&#x27; &quot;
1505        &quot;  top: &#x27;label&#x27; &quot;
1506        &quot;  data_param { &quot;
1507        &quot;    source: &#x27;mnist-train-leveldb&#x27; &quot;
1508        &quot;    batch_size: 64 &quot;
1509        &quot;  } &quot;
1510        &quot;  transform_param { &quot;
1511        &quot;    scale: 0.00390625 &quot;
1512        &quot;  } &quot;
1513        &quot;  include: { phase: TRAIN } &quot;
1514        &quot;} &quot;
1515        &quot;layer { &quot;
1516        &quot;  name: &#x27;conv1&#x27; &quot;
1517        &quot;  type: &#x27;Convolution&#x27; &quot;
1518        &quot;  bottom: &#x27;data&#x27; &quot;
1519        &quot;  top: &#x27;conv1&#x27; &quot;
1520        &quot;  param { &quot;
1521        &quot;    lr_mult: 1 &quot;
1522        &quot;  } &quot;
1523        &quot;  param { &quot;
1524        &quot;    lr_mult: 2 &quot;
1525        &quot;  } &quot;
1526        &quot;  convolution_param { &quot;
1527        &quot;    num_output: 20 &quot;
1528        &quot;    kernel_size: 5 &quot;
1529        &quot;    stride: 1 &quot;
1530        &quot;    weight_filler { &quot;
1531        &quot;      type: &#x27;xavier&#x27; &quot;
1532        &quot;    } &quot;
1533        &quot;    bias_filler { &quot;
1534        &quot;      type: &#x27;constant&#x27; &quot;
1535        &quot;    } &quot;
1536        &quot;  } &quot;
1537        &quot;} &quot;
1538        &quot;layer { &quot;
1539        &quot;  name: &#x27;ip1&#x27; &quot;
1540        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1541        &quot;  bottom: &#x27;conv1&#x27; &quot;
1542        &quot;  top: &#x27;ip1&#x27; &quot;
1543        &quot;  param { &quot;
1544        &quot;    lr_mult: 1 &quot;
1545        &quot;  } &quot;
1546        &quot;  param { &quot;
1547        &quot;    lr_mult: 2 &quot;
1548        &quot;  } &quot;
1549        &quot;  inner_product_param { &quot;
1550        &quot;    num_output: 10 &quot;
1551        &quot;    weight_filler { &quot;
1552        &quot;      type: &#x27;xavier&#x27; &quot;
1553        &quot;    } &quot;
1554        &quot;    bias_filler { &quot;
1555        &quot;      type: &#x27;constant&#x27; &quot;
1556        &quot;    } &quot;
1557        &quot;  } &quot;
1558        &quot;} &quot;
1559        &quot;layer { &quot;
1560        &quot;  name: &#x27;loss&#x27; &quot;
1561        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1562        &quot;  bottom: &#x27;ip2&#x27; &quot;
1563        &quot;  bottom: &#x27;label&#x27; &quot;
1564        &quot;  top: &#x27;loss&#x27; &quot;
1565        &quot;} &quot;;
1566    const string&amp; output_proto_test =
1567        &quot;name: &#x27;LeNet&#x27; &quot;
1568        &quot;layer { &quot;
1569        &quot;  name: &#x27;mnist&#x27; &quot;
1570        &quot;  type: &#x27;Data&#x27; &quot;
1571        &quot;  top: &#x27;data&#x27; &quot;
1572        &quot;  top: &#x27;label&#x27; &quot;
1573        &quot;  data_param { &quot;
1574        &quot;    source: &#x27;mnist-test-leveldb&#x27; &quot;
1575        &quot;    batch_size: 100 &quot;
1576        &quot;  } &quot;
1577        &quot;  transform_param { &quot;
1578        &quot;    scale: 0.00390625 &quot;
1579        &quot;  } &quot;
1580        &quot;  include: { phase: TEST } &quot;
1581        &quot;} &quot;
1582        &quot;layer { &quot;
1583        &quot;  name: &#x27;conv1&#x27; &quot;
1584        &quot;  type: &#x27;Convolution&#x27; &quot;
1585        &quot;  bottom: &#x27;data&#x27; &quot;
1586        &quot;  top: &#x27;conv1&#x27; &quot;
1587        &quot;  param { &quot;
1588        &quot;    lr_mult: 1 &quot;
1589        &quot;  } &quot;
1590        &quot;  param { &quot;
1591        &quot;    lr_mult: 2 &quot;
1592        &quot;  } &quot;
1593        &quot;  convolution_param { &quot;
1594        &quot;    num_output: 20 &quot;
1595        &quot;    kernel_size: 5 &quot;
1596        &quot;    stride: 1 &quot;
1597        &quot;    weight_filler { &quot;
1598        &quot;      type: &#x27;xavier&#x27; &quot;
1599        &quot;    } &quot;
1600        &quot;    bias_filler { &quot;
1601        &quot;      type: &#x27;constant&#x27; &quot;
1602        &quot;    } &quot;
1603        &quot;  } &quot;
1604        &quot;} &quot;
1605        &quot;layer { &quot;
1606        &quot;  name: &#x27;ip1&#x27; &quot;
1607        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1608        &quot;  bottom: &#x27;conv1&#x27; &quot;
1609        &quot;  top: &#x27;ip1&#x27; &quot;
1610        &quot;  param { &quot;
1611        &quot;    lr_mult: 1 &quot;
1612        &quot;  } &quot;
1613        &quot;  param { &quot;
1614        &quot;    lr_mult: 2 &quot;
1615        &quot;  } &quot;
1616        &quot;  inner_product_param { &quot;
1617        &quot;    num_output: 10 &quot;
1618        &quot;    weight_filler { &quot;
1619        &quot;      type: &#x27;xavier&#x27; &quot;
1620        &quot;    } &quot;
1621        &quot;    bias_filler { &quot;
1622        &quot;      type: &#x27;constant&#x27; &quot;
1623        &quot;    } &quot;
1624        &quot;  } &quot;
1625        &quot;} &quot;
1626        &quot;layer { &quot;
1627        &quot;  name: &#x27;accuracy&#x27; &quot;
1628        &quot;  type: &#x27;Accuracy&#x27; &quot;
1629        &quot;  bottom: &#x27;ip1&#x27; &quot;
1630        &quot;  bottom: &#x27;label&#x27; &quot;
1631        &quot;  top: &#x27;accuracy&#x27; &quot;
1632        &quot;  include: { phase: TEST } &quot;
1633        &quot;} &quot;
1634        &quot;layer { &quot;
1635        &quot;  name: &#x27;loss&#x27; &quot;
1636        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1637        &quot;  bottom: &#x27;ip2&#x27; &quot;
1638        &quot;  bottom: &#x27;label&#x27; &quot;
1639        &quot;  top: &#x27;loss&#x27; &quot;
1640        &quot;} &quot;;
1641    const string output_proto_train_explicit =
1642        output_proto_train + &quot; state: { phase: TRAIN } &quot;;
1643    const string output_proto_test_explicit =
1644        output_proto_test + &quot; state: { phase: TEST } &quot;;
1645    this-&gt;RunFilterNetTest(input_proto_train, output_proto_train_explicit);
1646    this-&gt;RunFilterNetTest(input_proto_test, output_proto_test_explicit);
1647  }
1648  TEST_F(FilterNetTest, TestFilterOutByStage) {
1649    const string&amp; input_proto =
1650        &quot;name: &#x27;TestNetwork&#x27; &quot;
1651        &quot;layer { &quot;
1652        &quot;  name: &#x27;data&#x27; &quot;
1653        &quot;  type: &#x27;Data&#x27; &quot;
1654        &quot;  top: &#x27;data&#x27; &quot;
1655        &quot;  top: &#x27;label&#x27; &quot;
1656        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1657        &quot;} &quot;
1658        &quot;layer { &quot;
1659        &quot;  name: &#x27;innerprod&#x27; &quot;
1660        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1661        &quot;  bottom: &#x27;data&#x27; &quot;
1662        &quot;  top: &#x27;innerprod&#x27; &quot;
1663        &quot;} &quot;
1664        &quot;layer { &quot;
1665        &quot;  name: &#x27;loss&#x27; &quot;
1666        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1667        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1668        &quot;  bottom: &#x27;label&#x27; &quot;
1669        &quot;} &quot;;
1670    const string&amp; output_proto =
1671        &quot;name: &#x27;TestNetwork&#x27; &quot;
1672        &quot;layer { &quot;
1673        &quot;  name: &#x27;innerprod&#x27; &quot;
1674        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1675        &quot;  bottom: &#x27;data&#x27; &quot;
1676        &quot;  top: &#x27;innerprod&#x27; &quot;
1677        &quot;} &quot;
1678        &quot;layer { &quot;
1679        &quot;  name: &#x27;loss&#x27; &quot;
1680        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1681        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1682        &quot;  bottom: &#x27;label&#x27; &quot;
1683        &quot;} &quot;;
1684    this-&gt;RunFilterNetTest(input_proto, output_proto);
1685  }
1686  TEST_F(FilterNetTest, TestFilterOutByStage2) {
1687    const string&amp; input_proto =
1688        &quot;name: &#x27;TestNetwork&#x27; &quot;
1689        &quot;layer { &quot;
1690        &quot;  name: &#x27;data&#x27; &quot;
1691        &quot;  type: &#x27;Data&#x27; &quot;
1692        &quot;  top: &#x27;data&#x27; &quot;
1693        &quot;  top: &#x27;label&#x27; &quot;
1694        &quot;} &quot;
1695        &quot;layer { &quot;
1696        &quot;  name: &#x27;innerprod&#x27; &quot;
1697        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1698        &quot;  bottom: &#x27;data&#x27; &quot;
1699        &quot;  top: &#x27;innerprod&#x27; &quot;
1700        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1701        &quot;} &quot;
1702        &quot;layer { &quot;
1703        &quot;  name: &#x27;loss&#x27; &quot;
1704        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1705        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1706        &quot;  bottom: &#x27;label&#x27; &quot;
1707        &quot;} &quot;;
1708    const string&amp; output_proto =
1709        &quot;name: &#x27;TestNetwork&#x27; &quot;
1710        &quot;layer { &quot;
1711        &quot;  name: &#x27;data&#x27; &quot;
1712        &quot;  type: &#x27;Data&#x27; &quot;
1713        &quot;  top: &#x27;data&#x27; &quot;
1714        &quot;  top: &#x27;label&#x27; &quot;
1715        &quot;} &quot;
1716        &quot;layer { &quot;
1717        &quot;  name: &#x27;loss&#x27; &quot;
1718        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1719        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1720        &quot;  bottom: &#x27;label&#x27; &quot;
1721        &quot;} &quot;;
1722    this-&gt;RunFilterNetTest(input_proto, output_proto);
1723  }
1724  TEST_F(FilterNetTest, TestFilterInByStage) {
1725    const string&amp; input_proto =
1726        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1727        &quot;name: &#x27;TestNetwork&#x27; &quot;
1728        &quot;layer { &quot;
1729        &quot;  name: &#x27;data&#x27; &quot;
1730        &quot;  type: &#x27;Data&#x27; &quot;
1731        &quot;  top: &#x27;data&#x27; &quot;
1732        &quot;  top: &#x27;label&#x27; &quot;
1733        &quot;} &quot;
1734        &quot;layer { &quot;
1735        &quot;  name: &#x27;innerprod&#x27; &quot;
1736        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1737        &quot;  bottom: &#x27;data&#x27; &quot;
1738        &quot;  top: &#x27;innerprod&#x27; &quot;
1739        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1740        &quot;} &quot;
1741        &quot;layer { &quot;
1742        &quot;  name: &#x27;loss&#x27; &quot;
1743        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1744        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1745        &quot;  bottom: &#x27;label&#x27; &quot;
1746        &quot;} &quot;;
1747    this-&gt;RunFilterNetTest(input_proto, input_proto);
1748  }
1749  TEST_F(FilterNetTest, TestFilterInByStage2) {
1750    const string&amp; input_proto =
1751        &quot;name: &#x27;TestNetwork&#x27; &quot;
1752        &quot;layer { &quot;
1753        &quot;  name: &#x27;data&#x27; &quot;
1754        &quot;  type: &#x27;Data&#x27; &quot;
1755        &quot;  top: &#x27;data&#x27; &quot;
1756        &quot;  top: &#x27;label&#x27; &quot;
1757        &quot;} &quot;
1758        &quot;layer { &quot;
1759        &quot;  name: &#x27;innerprod&#x27; &quot;
1760        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1761        &quot;  bottom: &#x27;data&#x27; &quot;
1762        &quot;  top: &#x27;innerprod&#x27; &quot;
1763        &quot;  exclude: { stage: &#x27;mystage&#x27; } &quot;
1764        &quot;} &quot;
1765        &quot;layer { &quot;
1766        &quot;  name: &#x27;loss&#x27; &quot;
1767        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1768        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1769        &quot;  bottom: &#x27;label&#x27; &quot;
1770        &quot;} &quot;;
1771    this-&gt;RunFilterNetTest(input_proto, input_proto);
1772  }
1773  TEST_F(FilterNetTest, TestFilterOutByMultipleStage) {
1774    const string&amp; input_proto =
1775        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1776        &quot;name: &#x27;TestNetwork&#x27; &quot;
1777        &quot;layer { &quot;
1778        &quot;  name: &#x27;data&#x27; &quot;
1779        &quot;  type: &#x27;Data&#x27; &quot;
1780        &quot;  top: &#x27;data&#x27; &quot;
1781        &quot;  top: &#x27;label&#x27; &quot;
1782        &quot;} &quot;
1783        &quot;layer { &quot;
1784        &quot;  name: &#x27;innerprod&#x27; &quot;
1785        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1786        &quot;  bottom: &#x27;data&#x27; &quot;
1787        &quot;  top: &#x27;innerprod&#x27; &quot;
1788        &quot;  include: { stage: &#x27;mystage&#x27; stage: &#x27;myotherstage&#x27; } &quot;
1789        &quot;} &quot;
1790        &quot;layer { &quot;
1791        &quot;  name: &#x27;loss&#x27; &quot;
1792        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1793        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1794        &quot;  bottom: &#x27;label&#x27; &quot;
1795        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1796        &quot;} &quot;;
1797    const string&amp; output_proto =
1798        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1799        &quot;name: &#x27;TestNetwork&#x27; &quot;
1800        &quot;layer { &quot;
1801        &quot;  name: &#x27;data&#x27; &quot;
1802        &quot;  type: &#x27;Data&#x27; &quot;
1803        &quot;  top: &#x27;data&#x27; &quot;
1804        &quot;  top: &#x27;label&#x27; &quot;
1805        &quot;} &quot;
1806        &quot;layer { &quot;
1807        &quot;  name: &#x27;loss&#x27; &quot;
1808        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1809        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1810        &quot;  bottom: &#x27;label&#x27; &quot;
1811        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1812        &quot;} &quot;;
1813    this-&gt;RunFilterNetTest(input_proto, output_proto);
1814  }
1815  TEST_F(FilterNetTest, TestFilterInByMultipleStage) {
1816    const string&amp; input_proto =
1817        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1818        &quot;name: &#x27;TestNetwork&#x27; &quot;
1819        &quot;layer { &quot;
1820        &quot;  name: &#x27;data&#x27; &quot;
1821        &quot;  type: &#x27;Data&#x27; &quot;
1822        &quot;  top: &#x27;data&#x27; &quot;
1823        &quot;  top: &#x27;label&#x27; &quot;
1824        &quot;} &quot;
1825        &quot;layer { &quot;
1826        &quot;  name: &#x27;innerprod&#x27; &quot;
1827        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1828        &quot;  bottom: &#x27;data&#x27; &quot;
1829        &quot;  top: &#x27;innerprod&#x27; &quot;
1830        &quot;  include: { stage: &#x27;myotherstage&#x27; } &quot;
1831        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1832        &quot;} &quot;
1833        &quot;layer { &quot;
1834        &quot;  name: &#x27;loss&#x27; &quot;
1835        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1836        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1837        &quot;  bottom: &#x27;label&#x27; &quot;
1838        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1839        &quot;} &quot;;
1840    this-&gt;RunFilterNetTest(input_proto, input_proto);
1841  }
1842  TEST_F(FilterNetTest, TestFilterInByMultipleStage2) {
1843    const string&amp; input_proto =
1844        &quot;state: { stage: &#x27;mystage&#x27; stage: &#x27;myotherstage&#x27; } &quot;
1845        &quot;name: &#x27;TestNetwork&#x27; &quot;
1846        &quot;layer { &quot;
1847        &quot;  name: &#x27;data&#x27; &quot;
1848        &quot;  type: &#x27;Data&#x27; &quot;
1849        &quot;  top: &#x27;data&#x27; &quot;
1850        &quot;  top: &#x27;label&#x27; &quot;
1851        &quot;} &quot;
1852        &quot;layer { &quot;
1853        &quot;  name: &#x27;innerprod&#x27; &quot;
1854        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1855        &quot;  bottom: &#x27;data&#x27; &quot;
1856        &quot;  top: &#x27;innerprod&#x27; &quot;
1857        &quot;  include: { stage: &#x27;mystage&#x27; stage: &#x27;myotherstage&#x27; } &quot;
1858        &quot;} &quot;
1859        &quot;layer { &quot;
1860        &quot;  name: &#x27;loss&#x27; &quot;
1861        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1862        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1863        &quot;  bottom: &#x27;label&#x27; &quot;
1864        &quot;  include: { stage: &#x27;mystage&#x27; } &quot;
1865        &quot;} &quot;;
1866    this-&gt;RunFilterNetTest(input_proto, input_proto);
1867  }
1868  TEST_F(FilterNetTest, TestFilterInByNotStage) {
1869    const string&amp; input_proto =
1870        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1871        &quot;name: &#x27;TestNetwork&#x27; &quot;
1872        &quot;layer { &quot;
1873        &quot;  name: &#x27;data&#x27; &quot;
1874        &quot;  type: &#x27;Data&#x27; &quot;
1875        &quot;  top: &#x27;data&#x27; &quot;
1876        &quot;  top: &#x27;label&#x27; &quot;
1877        &quot;} &quot;
1878        &quot;layer { &quot;
1879        &quot;  name: &#x27;innerprod&#x27; &quot;
1880        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1881        &quot;  bottom: &#x27;data&#x27; &quot;
1882        &quot;  top: &#x27;innerprod&#x27; &quot;
1883        &quot;  include: { not_stage: &#x27;myotherstage&#x27; } &quot;
1884        &quot;} &quot;
1885        &quot;layer { &quot;
1886        &quot;  name: &#x27;loss&#x27; &quot;
1887        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1888        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1889        &quot;  bottom: &#x27;label&#x27; &quot;
1890        &quot;  include: { not_stage: &#x27;myotherstage&#x27; } &quot;
1891        &quot;} &quot;;
1892    this-&gt;RunFilterNetTest(input_proto, input_proto);
1893  }
1894  TEST_F(FilterNetTest, TestFilterOutByNotStage) {
1895    const string&amp; input_proto =
1896        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1897        &quot;name: &#x27;TestNetwork&#x27; &quot;
1898        &quot;layer { &quot;
1899        &quot;  name: &#x27;data&#x27; &quot;
1900        &quot;  type: &#x27;Data&#x27; &quot;
1901        &quot;  top: &#x27;data&#x27; &quot;
1902        &quot;  top: &#x27;label&#x27; &quot;
1903        &quot;} &quot;
1904        &quot;layer { &quot;
1905        &quot;  name: &#x27;innerprod&#x27; &quot;
1906        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1907        &quot;  bottom: &#x27;data&#x27; &quot;
1908        &quot;  top: &#x27;innerprod&#x27; &quot;
1909        &quot;  include: { not_stage: &#x27;mystage&#x27; } &quot;
1910        &quot;} &quot;
1911        &quot;layer { &quot;
1912        &quot;  name: &#x27;loss&#x27; &quot;
1913        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1914        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1915        &quot;  bottom: &#x27;label&#x27; &quot;
1916        &quot;  include: { not_stage: &#x27;mystage&#x27; } &quot;
1917        &quot;} &quot;;
1918    const string&amp; output_proto =
1919        &quot;state: { stage: &#x27;mystage&#x27; } &quot;
1920        &quot;name: &#x27;TestNetwork&#x27; &quot;
1921        &quot;layer { &quot;
1922        &quot;  name: &#x27;data&#x27; &quot;
1923        &quot;  type: &#x27;Data&#x27; &quot;
1924        &quot;  top: &#x27;data&#x27; &quot;
1925        &quot;  top: &#x27;label&#x27; &quot;
1926        &quot;} &quot;;
1927    this-&gt;RunFilterNetTest(input_proto, output_proto);
1928  }
1929  TEST_F(FilterNetTest, TestFilterOutByMinLevel) {
1930    const string&amp; input_proto =
1931        &quot;name: &#x27;TestNetwork&#x27; &quot;
1932        &quot;layer { &quot;
1933        &quot;  name: &#x27;data&#x27; &quot;
1934        &quot;  type: &#x27;Data&#x27; &quot;
1935        &quot;  top: &#x27;data&#x27; &quot;
1936        &quot;  top: &#x27;label&#x27; &quot;
1937        &quot;} &quot;
1938        &quot;layer { &quot;
1939        &quot;  name: &#x27;innerprod&#x27; &quot;
1940        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1941        &quot;  bottom: &#x27;data&#x27; &quot;
1942        &quot;  top: &#x27;innerprod&#x27; &quot;
1943        &quot;  include: { min_level: 3 } &quot;
1944        &quot;} &quot;
1945        &quot;layer { &quot;
1946        &quot;  name: &#x27;loss&#x27; &quot;
1947        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1948        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1949        &quot;  bottom: &#x27;label&#x27; &quot;
1950        &quot;} &quot;;
1951    const string&amp; output_proto =
1952        &quot;name: &#x27;TestNetwork&#x27; &quot;
1953        &quot;layer { &quot;
1954        &quot;  name: &#x27;data&#x27; &quot;
1955        &quot;  type: &#x27;Data&#x27; &quot;
1956        &quot;  top: &#x27;data&#x27; &quot;
1957        &quot;  top: &#x27;label&#x27; &quot;
1958        &quot;} &quot;
1959        &quot;layer { &quot;
1960        &quot;  name: &#x27;loss&#x27; &quot;
1961        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1962        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1963        &quot;  bottom: &#x27;label&#x27; &quot;
1964        &quot;} &quot;;
1965    this-&gt;RunFilterNetTest(input_proto, output_proto);
1966  }
1967  TEST_F(FilterNetTest, TestFilterOutByMaxLevel) {
1968    const string&amp; input_proto =
1969        &quot;name: &#x27;TestNetwork&#x27; &quot;
1970        &quot;layer { &quot;
1971        &quot;  name: &#x27;data&#x27; &quot;
1972        &quot;  type: &#x27;Data&#x27; &quot;
1973        &quot;  top: &#x27;data&#x27; &quot;
1974        &quot;  top: &#x27;label&#x27; &quot;
1975        &quot;} &quot;
1976        &quot;layer { &quot;
1977        &quot;  name: &#x27;innerprod&#x27; &quot;
1978        &quot;  type: &#x27;InnerProduct&#x27; &quot;
1979        &quot;  bottom: &#x27;data&#x27; &quot;
1980        &quot;  top: &#x27;innerprod&#x27; &quot;
1981        &quot;  include: { max_level: -3 } &quot;
1982        &quot;} &quot;
1983        &quot;layer { &quot;
1984        &quot;  name: &#x27;loss&#x27; &quot;
1985        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
1986        &quot;  bottom: &#x27;innerprod&#x27; &quot;
1987        &quot;  bottom: &#x27;label&#x27; &quot;
1988        &quot;} &quot;;
1989    const string&amp; output_proto =
1990        &quot;name: &#x27;TestNetwork&#x27; &quot;
1991        &quot;layer { &quot;
1992        &quot;  name: &#x27;data&#x27; &quot;
1993        &quot;  type: &#x27;Data&#x27; &quot;
1994        &quot;  top: &#x27;data&#x27; &quot;
1995        &quot;  top: &#x27;label&#x27; &quot;
1996        &quot;} &quot;
1997        &quot;layer { &quot;
1998        &quot;  name: &#x27;loss&#x27; &quot;
1999        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2000        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2001        &quot;  bottom: &#x27;label&#x27; &quot;
2002        &quot;} &quot;;
2003    this-&gt;RunFilterNetTest(input_proto, output_proto);
2004  }
2005  TEST_F(FilterNetTest, TestFilterInByMinLevel) {
2006    const string&amp; input_proto =
2007        &quot;name: &#x27;TestNetwork&#x27; &quot;
2008        &quot;layer { &quot;
2009        &quot;  name: &#x27;data&#x27; &quot;
2010        &quot;  type: &#x27;Data&#x27; &quot;
2011        &quot;  top: &#x27;data&#x27; &quot;
2012        &quot;  top: &#x27;label&#x27; &quot;
2013        &quot;} &quot;
2014        &quot;layer { &quot;
2015        &quot;  name: &#x27;innerprod&#x27; &quot;
2016        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2017        &quot;  bottom: &#x27;data&#x27; &quot;
2018        &quot;  top: &#x27;innerprod&#x27; &quot;
2019        &quot;  include: { min_level: 0 } &quot;
2020        &quot;} &quot;
2021        &quot;layer { &quot;
2022        &quot;  name: &#x27;loss&#x27; &quot;
2023        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2024        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2025        &quot;  bottom: &#x27;label&#x27; &quot;
2026        &quot;} &quot;;
2027    this-&gt;RunFilterNetTest(input_proto, input_proto);
2028  }
2029  TEST_F(FilterNetTest, TestFilterInByMinLevel2) {
2030    const string&amp; input_proto =
2031        &quot;state: { level: 7 } &quot;
2032        &quot;name: &#x27;TestNetwork&#x27; &quot;
2033        &quot;layer { &quot;
2034        &quot;  name: &#x27;data&#x27; &quot;
2035        &quot;  type: &#x27;Data&#x27; &quot;
2036        &quot;  top: &#x27;data&#x27; &quot;
2037        &quot;  top: &#x27;label&#x27; &quot;
2038        &quot;} &quot;
2039        &quot;layer { &quot;
2040        &quot;  name: &#x27;innerprod&#x27; &quot;
2041        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2042        &quot;  bottom: &#x27;data&#x27; &quot;
2043        &quot;  top: &#x27;innerprod&#x27; &quot;
2044        &quot;  include: { min_level: 3 } &quot;
2045        &quot;} &quot;
2046        &quot;layer { &quot;
2047        &quot;  name: &#x27;loss&#x27; &quot;
2048        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2049        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2050        &quot;  bottom: &#x27;label&#x27; &quot;
2051        &quot;} &quot;;
2052    this-&gt;RunFilterNetTest(input_proto, input_proto);
2053  }
2054  TEST_F(FilterNetTest, TestFilterInByMaxLevel) {
2055    const string&amp; input_proto =
2056        &quot;name: &#x27;TestNetwork&#x27; &quot;
2057        &quot;layer { &quot;
2058        &quot;  name: &#x27;data&#x27; &quot;
2059        &quot;  type: &#x27;Data&#x27; &quot;
2060        &quot;  top: &#x27;data&#x27; &quot;
2061        &quot;  top: &#x27;label&#x27; &quot;
2062        &quot;} &quot;
2063        &quot;layer { &quot;
2064        &quot;  name: &#x27;innerprod&#x27; &quot;
2065        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2066        &quot;  bottom: &#x27;data&#x27; &quot;
2067        &quot;  top: &#x27;innerprod&#x27; &quot;
2068        &quot;  include: { max_level: 0 } &quot;
2069        &quot;} &quot;
2070        &quot;layer { &quot;
2071        &quot;  name: &#x27;loss&#x27; &quot;
2072        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2073        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2074        &quot;  bottom: &#x27;label&#x27; &quot;
2075        &quot;} &quot;;
2076    this-&gt;RunFilterNetTest(input_proto, input_proto);
2077  }
2078  TEST_F(FilterNetTest, TestFilterInByMaxLevel2) {
2079    const string&amp; input_proto =
2080        &quot;state: { level: -7 } &quot;
2081        &quot;name: &#x27;TestNetwork&#x27; &quot;
2082        &quot;layer { &quot;
2083        &quot;  name: &#x27;data&#x27; &quot;
2084        &quot;  type: &#x27;Data&#x27; &quot;
2085        &quot;  top: &#x27;data&#x27; &quot;
2086        &quot;  top: &#x27;label&#x27; &quot;
2087        &quot;} &quot;
2088        &quot;layer { &quot;
2089        &quot;  name: &#x27;innerprod&#x27; &quot;
2090        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2091        &quot;  bottom: &#x27;data&#x27; &quot;
2092        &quot;  top: &#x27;innerprod&#x27; &quot;
2093        &quot;  include: { max_level: -3 } &quot;
2094        &quot;} &quot;
2095        &quot;layer { &quot;
2096        &quot;  name: &#x27;loss&#x27; &quot;
2097        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2098        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2099        &quot;  bottom: &#x27;label&#x27; &quot;
2100        &quot;} &quot;;
2101    this-&gt;RunFilterNetTest(input_proto, input_proto);
2102  }
2103  TEST_F(FilterNetTest, TestFilterInOutByIncludeMultiRule) {
2104    const string&amp; input_proto =
2105        &quot;name: &#x27;TestNetwork&#x27; &quot;
2106        &quot;layer { &quot;
2107        &quot;  name: &#x27;data&#x27; &quot;
2108        &quot;  type: &#x27;Data&#x27; &quot;
2109        &quot;  top: &#x27;data&#x27; &quot;
2110        &quot;  top: &#x27;label&#x27; &quot;
2111        &quot;} &quot;
2112        &quot;layer { &quot;
2113        &quot;  name: &#x27;innerprod&#x27; &quot;
2114        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2115        &quot;  bottom: &#x27;data&#x27; &quot;
2116        &quot;  top: &#x27;innerprod&#x27; &quot;
2117        &quot;  include: { min_level: 2  phase: TRAIN } &quot;
2118        &quot;} &quot;
2119        &quot;layer { &quot;
2120        &quot;  name: &#x27;loss&#x27; &quot;
2121        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2122        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2123        &quot;  bottom: &#x27;label&#x27; &quot;
2124        &quot;  include: { min_level: 2  phase: TEST } &quot;
2125        &quot;} &quot;;
2126    const string&amp; input_proto_train =
2127        &quot;state: { level: 4  phase: TRAIN } &quot; + input_proto;
2128    const string&amp; input_proto_test =
2129        &quot;state: { level: 4  phase: TEST } &quot; + input_proto;
2130    const string&amp; output_proto_train =
2131        &quot;state: { level: 4  phase: TRAIN } &quot;
2132        &quot;name: &#x27;TestNetwork&#x27; &quot;
2133        &quot;layer { &quot;
2134        &quot;  name: &#x27;data&#x27; &quot;
2135        &quot;  type: &#x27;Data&#x27; &quot;
2136        &quot;  top: &#x27;data&#x27; &quot;
2137        &quot;  top: &#x27;label&#x27; &quot;
2138        &quot;} &quot;
2139        &quot;layer { &quot;
2140        &quot;  name: &#x27;innerprod&#x27; &quot;
2141        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2142        &quot;  bottom: &#x27;data&#x27; &quot;
2143        &quot;  top: &#x27;innerprod&#x27; &quot;
2144        &quot;  include: { min_level: 2  phase: TRAIN } &quot;
2145        &quot;} &quot;;
2146    const string&amp; output_proto_test =
2147        &quot;state: { level: 4  phase: TEST } &quot;
2148        &quot;name: &#x27;TestNetwork&#x27; &quot;
2149        &quot;layer { &quot;
2150        &quot;  name: &#x27;data&#x27; &quot;
2151        &quot;  type: &#x27;Data&#x27; &quot;
2152        &quot;  top: &#x27;data&#x27; &quot;
2153        &quot;  top: &#x27;label&#x27; &quot;
2154        &quot;} &quot;
2155        &quot;layer { &quot;
2156        &quot;  name: &#x27;loss&#x27; &quot;
2157        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2158        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2159        &quot;  bottom: &#x27;label&#x27; &quot;
2160        &quot;  include: { min_level: 2  phase: TEST } &quot;
2161        &quot;} &quot;;
2162    this-&gt;RunFilterNetTest(input_proto_train, output_proto_train);
2163    this-&gt;RunFilterNetTest(input_proto_test, output_proto_test);
2164  }
2165  TEST_F(FilterNetTest, TestFilterInByIncludeMultiRule) {
2166    const string&amp; input_proto =
2167        &quot;name: &#x27;TestNetwork&#x27; &quot;
2168        &quot;layer { &quot;
2169        &quot;  name: &#x27;data&#x27; &quot;
2170        &quot;  type: &#x27;Data&#x27; &quot;
2171        &quot;  top: &#x27;data&#x27; &quot;
2172        &quot;  top: &#x27;label&#x27; &quot;
2173        &quot;} &quot;
2174        &quot;layer { &quot;
2175        &quot;  name: &#x27;innerprod&#x27; &quot;
2176        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2177        &quot;  bottom: &#x27;data&#x27; &quot;
2178        &quot;  top: &#x27;innerprod&#x27; &quot;
2179        &quot;  include: { min_level: 2  phase: TRAIN } &quot;
2180        &quot;  include: { phase: TEST } &quot;
2181        &quot;} &quot;
2182        &quot;layer { &quot;
2183        &quot;  name: &#x27;loss&#x27; &quot;
2184        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2185        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2186        &quot;  bottom: &#x27;label&#x27; &quot;
2187        &quot;  include: { min_level: 2  phase: TEST } &quot;
2188        &quot;  include: { phase: TRAIN } &quot;
2189        &quot;} &quot;;
2190    const string&amp; input_proto_train =
2191        &quot;state: { level: 2  phase: TRAIN } &quot; + input_proto;
2192    const string&amp; input_proto_test =
2193        &quot;state: { level: 2  phase: TEST } &quot; + input_proto;
2194    this-&gt;RunFilterNetTest(input_proto_train, input_proto_train);
2195    this-&gt;RunFilterNetTest(input_proto_test, input_proto_test);
2196  }
2197  TEST_F(FilterNetTest, TestFilterInOutByExcludeMultiRule) {
2198    const string&amp; input_proto =
2199        &quot;name: &#x27;TestNetwork&#x27; &quot;
2200        &quot;layer { &quot;
2201        &quot;  name: &#x27;data&#x27; &quot;
2202        &quot;  type: &#x27;Data&#x27; &quot;
2203        &quot;  top: &#x27;data&#x27; &quot;
2204        &quot;  top: &#x27;label&#x27; &quot;
2205        &quot;} &quot;
2206        &quot;layer { &quot;
2207        &quot;  name: &#x27;innerprod&#x27; &quot;
2208        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2209        &quot;  bottom: &#x27;data&#x27; &quot;
2210        &quot;  top: &#x27;innerprod&#x27; &quot;
2211        &quot;  exclude: { min_level: 2  phase: TRAIN } &quot;
2212        &quot;} &quot;
2213        &quot;layer { &quot;
2214        &quot;  name: &#x27;loss&#x27; &quot;
2215        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2216        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2217        &quot;  bottom: &#x27;label&#x27; &quot;
2218        &quot;  exclude: { min_level: 2  phase: TEST } &quot;
2219        &quot;} &quot;;
2220    const string&amp; input_proto_train =
2221        &quot;state: { level: 4  phase: TRAIN } &quot; + input_proto;
2222    const string&amp; input_proto_test =
2223        &quot;state: { level: 4  phase: TEST } &quot; + input_proto;
2224    const string&amp; output_proto_train =
2225        &quot;state: { level: 4  phase: TRAIN } &quot;
2226        &quot;name: &#x27;TestNetwork&#x27; &quot;
2227        &quot;layer { &quot;
2228        &quot;  name: &#x27;data&#x27; &quot;
2229        &quot;  type: &#x27;Data&#x27; &quot;
2230        &quot;  top: &#x27;data&#x27; &quot;
2231        &quot;  top: &#x27;label&#x27; &quot;
2232        &quot;} &quot;
2233        &quot;layer { &quot;
2234        &quot;  name: &#x27;loss&#x27; &quot;
2235        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2236        &quot;  bottom: &#x27;innerprod&#x27; &quot;
2237        &quot;  bottom: &#x27;label&#x27; &quot;
2238        &quot;  exclude: { min_level: 2  phase: TEST } &quot;
2239        &quot;} &quot;;
2240    const string&amp; output_proto_test =
2241        &quot;state: { level: 4  phase: TEST } &quot;
2242        &quot;name: &#x27;TestNetwork&#x27; &quot;
2243        &quot;layer { &quot;
2244        &quot;  name: &#x27;data&#x27; &quot;
2245        &quot;  type: &#x27;Data&#x27; &quot;
2246        &quot;  top: &#x27;data&#x27; &quot;
2247        &quot;  top: &#x27;label&#x27; &quot;
2248        &quot;} &quot;
2249        &quot;layer { &quot;
2250        &quot;  name: &#x27;innerprod&#x27; &quot;
2251        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2252        &quot;  bottom: &#x27;data&#x27; &quot;
2253        &quot;  top: &#x27;innerprod&#x27; &quot;
2254        &quot;  exclude: { min_level: 2  phase: TRAIN } &quot;
2255        &quot;} &quot;;
2256    this-&gt;RunFilterNetTest(input_proto_train, output_proto_train);
2257    this-&gt;RunFilterNetTest(input_proto_test, output_proto_test);
2258  }
2259  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
2260  TYPED_TEST(NetTest, TestReshape) {
2261    typedef typename TypeParam::Dtype Dtype;
2262    Caffe::set_random_seed(this-&gt;seed_);
2263    Caffe::set_mode(Caffe::CPU);
2264    FillerParameter filler_param;
2265    filler_param.set_std(1);
2266    GaussianFiller&lt;Dtype&gt; filler(filler_param);
2267    Blob&lt;Dtype&gt; blob1(2, 3, 12, 10);
2268    Blob&lt;Dtype&gt; blob2(4, 3, 9, 11);
2269    ASSERT_LT(blob1.count(), blob2.count());
2270    filler.Fill(&amp;blob1);
2271    filler.Fill(&amp;blob2);
2272    this-&gt;InitReshapableNet();
2273    shared_ptr&lt;Blob&lt;Dtype&gt; &gt; input_blob = this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
2274    Blob&lt;Dtype&gt;* output_blob = this-&gt;net_-&gt;output_blobs()[0];
2275    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2276        blob1.width());
2277    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2278    this-&gt;net_-&gt;Forward();
2279    this-&gt;net_-&gt;Backward();
2280    Blob&lt;Dtype&gt; output1(output_blob-&gt;num(), output_blob-&gt;channels(),
2281        output_blob-&gt;height(), output_blob-&gt;width());
2282    caffe_copy(output1.count(), output_blob-&gt;cpu_data(),
2283        output1.mutable_cpu_data());
2284    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2285        blob2.width());
2286    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2287    this-&gt;net_-&gt;Forward();
2288    this-&gt;net_-&gt;Backward();
2289    Blob&lt;Dtype&gt; output2(output_blob-&gt;num(), output_blob-&gt;channels(),
2290        output_blob-&gt;height(), output_blob-&gt;width());
2291    caffe_copy(output2.count(), output_blob-&gt;cpu_data(),
2292        output2.mutable_cpu_data());
2293    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2294        blob1.width());
2295    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2296    this-&gt;net_-&gt;Forward();
2297    this-&gt;net_-&gt;Backward();
2298    for (int i = 0; i &lt; output1.count(); ++i) {
2299      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2300    }
2301    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2302        blob2.width());
2303    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2304    this-&gt;net_-&gt;Forward();
2305    this-&gt;net_-&gt;Backward();
2306    for (int i = 0; i &lt; output2.count(); ++i) {
2307      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2308    }
2309    EXPECT_EQ(output1.num(), blob1.num());
2310    EXPECT_EQ(output2.num(), blob2.num());
2311    bool same_spatial_shape = true;
2312    const int kFirstSpatialAxis = 2;
2313    for (int i = kFirstSpatialAxis; i &lt; output1.num_axes(); ++i) {
2314      if (output1.shape(i) != output2.shape(i)) {
2315        same_spatial_shape = false;
2316        break;
2317      }
2318    }
2319    EXPECT_FALSE(same_spatial_shape);
2320  }
2321  #endif
2322  #ifdef MKL2017_SUPPORTED
2323  TYPED_TEST(NetTestCPU, TestForwardReshapeForward) {
2324    typedef TypeParam Dtype;
2325    const string&amp; proto =
2326        &quot;name: &#x27;TestNetwork&#x27; &quot;
2327        &quot; layer {&quot;
2328        &quot;   top: &#x27;data&#x27;&quot;
2329        &quot;   top: &#x27;label&#x27;&quot;
2330        &quot;   name: &#x27;data&#x27;&quot;
2331        &quot;   type: &#x27;DummyData&#x27;&quot;
2332        &quot;   dummy_data_param {&quot;
2333        &quot;     shape: { dim: 32 dim: 3 dim: 227 dim: 227 }&quot;
2334        &quot;     data_filler {&quot;
2335        &quot;       type: &#x27;constant&#x27;&quot;
2336        &quot;       value: 0.01&quot;
2337        &quot;     }&quot;
2338        &quot;   }&quot;
2339        &quot;   transform_param {&quot;
2340        &quot;     mirror: true&quot;
2341        &quot;     crop_size: 224&quot;
2342        &quot;     mean_value: 104&quot;
2343        &quot;     mean_value: 117&quot;
2344        &quot;     mean_value: 123&quot;
2345        &quot;   }&quot;
2346        &quot; }&quot;
2347        &quot; layer {&quot;
2348        &quot;  bottom: &#x27;data&#x27;&quot;
2349        &quot;   top: &#x27;conv&#x27;&quot;
2350        &quot;   name: &#x27;conv1&#x27;&quot;
2351        &quot;   type: &#x27;Convolution&#x27;&quot;
2352        &quot;   param {&quot;
2353        &quot;     lr_mult: 1&quot;
2354        &quot;     decay_mult: 1&quot;
2355        &quot;   }&quot;
2356        &quot;   convolution_param {&quot;
2357        &quot;     &quot;
2358        &quot;     num_output: 64&quot;
2359        &quot;     engine: MKL2017 &quot;
2360        &quot;     pad: 3&quot;
2361        &quot;     kernel_size: 7&quot;
2362        &quot;     stride: 2&quot;
2363        &quot;     weight_filler {&quot;
2364        &quot;       type: &#x27;xavier&#x27;&quot;
2365        &quot;     }&quot;
2366        &quot;     bias_term: false&quot;
2367        &quot;   }&quot;
2368        &quot; }&quot;
2369        &quot; layer {&quot;
2370        &quot;   bottom: &#x27;conv&#x27;&quot;
2371        &quot;   top: &#x27;relu1&#x27;&quot;
2372        &quot;   name: &#x27;relu1&#x27;&quot;
2373        &quot;   type: &#x27;ReLU&#x27;&quot;
2374        &quot;   relu_param {&quot;
2375        &quot;     engine: MKL2017 &quot;
2376        &quot;     &quot;
2377        &quot;   }&quot;
2378        &quot; }&quot;
2379        &quot; layer {&quot;
2380        &quot;   bottom: &#x27;conv&#x27;&quot;
2381        &quot;   top: &#x27;relu2&#x27;&quot;
2382        &quot;   name: &#x27;relu2&#x27;&quot;
2383        &quot;   type: &#x27;ReLU&#x27;&quot;
2384        &quot;   relu_param {&quot;
2385        &quot;     engine: MKL2017 &quot;
2386        &quot;     &quot;
2387        &quot;   }&quot;
2388        &quot; }&quot;
2389        &quot; layer {&quot;
2390        &quot;   bottom: &#x27;relu1&#x27;&quot;
2391        &quot;   bottom: &#x27;relu2&#x27;&quot;
2392        &quot;   top: &#x27;concat&#x27;&quot;
2393        &quot;   name: &#x27;concat&#x27;&quot;
2394        &quot;   type: &#x27;Concat&#x27;&quot;
2395        &quot;   concat_param {&quot;
2396        &quot;     engine: MKL2017 &quot;
2397        &quot;     &quot;
2398        &quot;   }&quot;
2399        &quot; } &quot;
2400        &quot; layer {&quot;
2401        &quot;   bottom: &#x27;concat&#x27;&quot;
2402        &quot;   top: &#x27;lrn&#x27;&quot;
2403        &quot;   name: &#x27;LRN&#x27;&quot;
2404        &quot;   type: &#x27;LRN&#x27;&quot;
2405        &quot;   lrn_param {&quot;
2406        &quot;     engine: MKL2017 &quot;
2407        &quot;     local_size: 5&quot;
2408        &quot;     alpha: 0.0001&quot;
2409        &quot;     beta: 0.75&quot;
2410        &quot;   }&quot;
2411        &quot; }&quot;
2412        &quot; layer {&quot;
2413        &quot;   bottom: &#x27;lrn&#x27;&quot;
2414        &quot;   top: &#x27;pooling&#x27;&quot;
2415        &quot;   name: &#x27;Pooling&#x27;&quot;
2416        &quot;   type: &#x27;Pooling&#x27;&quot;
2417        &quot;   pooling_param {&quot;
2418        &quot;     engine: MKL2017 &quot;
2419        &quot;     kernel_size: 5&quot;
2420        &quot;     stride: 2&quot;
2421        &quot;     pool: MAX&quot;
2422        &quot;   }&quot;
2423        &quot; }&quot;
2424        &quot; layer {&quot;
2425        &quot;   bottom: &#x27;pooling&#x27;&quot;
2426        &quot;   top: &#x27;bn&#x27;&quot;
2427        &quot;   name: &#x27;BatchNorm&#x27;&quot;
2428        &quot;   type: &#x27;BatchNorm&#x27;&quot;
2429        &quot;   batch_norm_param {&quot;
2430        &quot;     engine: MKL2017 &quot;
2431        &quot;   }&quot;
2432        &quot; }&quot;;
2433      this-&gt;InitNetFromProtoString(proto);
2434      this-&gt;net_-&gt;Forward();
2435      shared_ptr&lt;Blob&lt;Dtype&gt; &gt; input_blob = this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
2436      input_blob-&gt;Reshape(1, 3, 1280, 720);
2437      this-&gt;net_-&gt;Forward();
2438  }
2439  #if 0
2440  TYPED_TEST(NetTest, TestTotalForwardReshape) {
2441    typedef typename TypeParam::Dtype Dtype;
2442    Caffe::set_random_seed(this-&gt;seed_);
2443    Caffe::set_mode(Caffe::CPU);
2444    FillerParameter filler_param;
2445    filler_param.set_std(1);
2446    GaussianFiller&lt;Dtype&gt; filler(filler_param);
2447    Blob&lt;Dtype&gt; blob1(2, 3, 12, 10);
2448    Blob&lt;Dtype&gt; blob2(4, 3, 9, 11);
2449    ASSERT_LT(blob1.count(), blob2.count());
2450    filler.Fill(&amp;blob1);
2451    filler.Fill(&amp;blob2);
2452    const string&amp; proto =
2453        &quot;name: &#x27;TestNetwork&#x27; &quot;
2454        &quot; layer {&quot;
2455        &quot;   top: &#x27;data&#x27;&quot;
2456        &quot;   top: &#x27;label&#x27;&quot;
2457        &quot;   name: &#x27;data&#x27;&quot;
2458        &quot;   type: &#x27;DummyData&#x27;&quot;
2459        &quot;   dummy_data_param {&quot;
2460        &quot;     shape: { dim: 3 dim: 3 dim: 13 dim: 11 }&quot;
2461        &quot;     data_filler {&quot;
2462        &quot;       type: &#x27;constant&#x27;&quot;
2463        &quot;       value: 0.01&quot;
2464        &quot;     }&quot;
2465        &quot;   }&quot;
2466        &quot;   transform_param {&quot;
2467        &quot;     mirror: true&quot;
2468        &quot;     crop_size: 224&quot;
2469        &quot;     mean_value: 104&quot;
2470        &quot;     mean_value: 117&quot;
2471        &quot;     mean_value: 123&quot;
2472        &quot;   }&quot;
2473        &quot; }&quot;
2474        &quot; layer {&quot;
2475        &quot;  bottom: &#x27;data&#x27;&quot;
2476        &quot;   top: &#x27;conv&#x27;&quot;
2477        &quot;   name: &#x27;conv1&#x27;&quot;
2478        &quot;   type: &#x27;Convolution&#x27;&quot;
2479        &quot;   param {&quot;
2480        &quot;     lr_mult: 1&quot;
2481        &quot;     decay_mult: 1&quot;
2482        &quot;   }&quot;
2483        &quot;   convolution_param {&quot;
2484        &quot;     &quot;
2485        &quot;     num_output: 64&quot;
2486        &quot;     engine: MKL2017 &quot;
2487        &quot;     pad: 3&quot;
2488        &quot;     kernel_size: 7&quot;
2489        &quot;     stride: 2&quot;
2490        &quot;     weight_filler {&quot;
2491        &quot;       type: &#x27;xavier&#x27;&quot;
2492        &quot;     }&quot;
2493        &quot;     bias_term: false&quot;
2494        &quot;   }&quot;
2495        &quot; }&quot;
2496        &quot; layer {&quot;
2497        &quot;   bottom: &#x27;conv&#x27;&quot;
2498        &quot;   top: &#x27;relu1&#x27;&quot;
2499        &quot;   name: &#x27;relu1&#x27;&quot;
2500        &quot;   type: &#x27;ReLU&#x27;&quot;
2501        &quot;   relu_param {&quot;
2502        &quot;     engine: MKL2017 &quot;
2503        &quot;     &quot;
2504        &quot;   }&quot;
2505        &quot; }&quot;
2506        &quot; layer {&quot;
2507        &quot;   bottom: &#x27;conv&#x27;&quot;
2508        &quot;   top: &#x27;relu2&#x27;&quot;
2509        &quot;   name: &#x27;relu2&#x27;&quot;
2510        &quot;   type: &#x27;ReLU&#x27;&quot;
2511        &quot;   relu_param {&quot;
2512        &quot;     engine: MKL2017 &quot;
2513        &quot;     &quot;
2514        &quot;   }&quot;
2515        &quot; }&quot;
2516        &quot; layer {&quot;
2517        &quot;   bottom: &#x27;relu1&#x27;&quot;
2518        &quot;   bottom: &#x27;relu2&#x27;&quot;
2519        &quot;   top: &#x27;concat&#x27;&quot;
2520        &quot;   name: &#x27;concat&#x27;&quot;
2521        &quot;   type: &#x27;Concat&#x27;&quot;
2522        &quot;   concat_param {&quot;
2523        &quot;     engine: MKL2017 &quot;
2524        &quot;     &quot;
2525        &quot;   }&quot;
2526        &quot; } &quot;
2527        &quot; layer {&quot;
2528        &quot;   bottom: &#x27;concat&#x27;&quot;
2529        &quot;   top: &#x27;lrn&#x27;&quot;
2530        &quot;   name: &#x27;LRN&#x27;&quot;
2531        &quot;   type: &#x27;LRN&#x27;&quot;
2532        &quot;   lrn_param {&quot;
2533        &quot;     engine: MKL2017 &quot;
2534        &quot;     local_size: 5&quot;
2535        &quot;     alpha: 0.0001&quot;
2536        &quot;     beta: 0.75&quot;
2537        &quot;   }&quot;
2538        &quot; }&quot;
2539        &quot; layer {&quot;
2540        &quot;   bottom: &#x27;lrn&#x27;&quot;
2541        &quot;   top: &#x27;pooling&#x27;&quot;
2542        &quot;   name: &#x27;Pooling&#x27;&quot;
2543        &quot;   type: &#x27;Pooling&#x27;&quot;
2544        &quot;   pooling_param {&quot;
2545        &quot;     engine: MKL2017 &quot;
2546        &quot;     kernel_size: 5&quot;
2547        &quot;     stride: 2&quot;
2548        &quot;     pool: MAX&quot;
2549        &quot;   }&quot;
2550        &quot; }&quot;
2551        &quot; layer {&quot;
2552        &quot;   bottom: &#x27;pooling&#x27;&quot;
2553        &quot;   top: &#x27;bn&#x27;&quot;
2554        &quot;   name: &#x27;BatchNorm&#x27;&quot;
2555        &quot;   type: &#x27;BatchNorm&#x27;&quot;
2556        &quot;   batch_norm_param {&quot;
2557        &quot;     engine: MKL2017 &quot;
2558        &quot;   }&quot;
2559        &quot; }&quot;;
2560      this-&gt;InitNetFromProtoString(proto);
2561    shared_ptr&lt;Blob&lt;Dtype&gt; &gt; input_blob = this-&gt;net_-&gt;blob_by_name(&quot;data&quot;);
2562    Blob&lt;Dtype&gt;* output_blob = this-&gt;net_-&gt;output_blobs()[0];
2563    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2564        blob1.width());
2565    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2566    this-&gt;net_-&gt;Forward();
2567    this-&gt;net_-&gt;Backward();
2568    Blob&lt;Dtype&gt; output1(output_blob-&gt;num(), output_blob-&gt;channels(),
2569        output_blob-&gt;height(), output_blob-&gt;width());
2570    caffe_copy(output1.count(), output_blob-&gt;cpu_data(),
2571        output1.mutable_cpu_data());
2572    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2573        blob2.width());
2574    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2575    this-&gt;net_-&gt;Forward();
2576    this-&gt;net_-&gt;Backward();
2577    Blob&lt;Dtype&gt; output2(output_blob-&gt;num(), output_blob-&gt;channels(),
2578        output_blob-&gt;height(), output_blob-&gt;width());
2579    caffe_copy(output2.count(), output_blob-&gt;cpu_data(),
2580        output2.mutable_cpu_data());
2581    input_blob-&gt;Reshape(blob1.num(), blob1.channels(), blob1.height(),
2582        blob1.width());
2583    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob-&gt;mutable_cpu_data());
2584    this-&gt;net_-&gt;Forward();
2585    this-&gt;net_-&gt;Backward();
2586    for (int i = 0; i &lt; output1.count(); ++i) {
2587      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2588    }
2589    input_blob-&gt;Reshape(blob2.num(), blob2.channels(), blob2.height(),
2590        blob2.width());
2591    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob-&gt;mutable_cpu_data());
2592    this-&gt;net_-&gt;Forward();
2593    this-&gt;net_-&gt;Backward();
2594    for (int i = 0; i &lt; output2.count(); ++i) {
2595      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob-&gt;cpu_data() + i));
2596    }
2597    EXPECT_EQ(output1.num(), blob1.num());
2598    EXPECT_EQ(output2.num(), blob2.num());
2599    bool same_spatial_shape = true;
2600    const int kFirstSpatialAxis = 2;
2601    for (int i = kFirstSpatialAxis; i &lt; output1.num_axes(); ++i) {
2602      if (output1.shape(i) != output2.shape(i)) {
2603        same_spatial_shape = false;
2604        break;
2605      }
2606    }
2607    EXPECT_FALSE(same_spatial_shape);
2608  }
2609  #endif
2610  #endif
2611  TYPED_TEST(NetTest, TestSkipPropagateDown) {
2612    this-&gt;InitSkipPropNet(false);
2613    vector&lt;bool&gt; vec_layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2614    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2615      string layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2616      if (layer_name == &quot;loss&quot;) {
2617        bool need_back = this-&gt;net_-&gt;bottom_need_backward()[layer_id][1];
2618        EXPECT_TRUE(need_back) &lt;&lt; &quot;bottom_need_backward should be True&quot;;
2619      }
2620      if (layer_name.find(&quot;data&quot;) != std::string::npos ||
2621            layer_name == &quot;silence&quot;) {
2622        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2623            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be False&quot;;
2624      } else {
2625        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2626            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be True&quot;;
2627      }
2628    }
2629    this-&gt;InitSkipPropNet(true);
2630    vec_layer_need_backward.clear();
2631    vec_layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2632    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2633      string layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2634      if (layer_name == &quot;loss&quot;) {
2635        bool need_back = this-&gt;net_-&gt;bottom_need_backward()[layer_id][1];
2636        EXPECT_FALSE(need_back) &lt;&lt; &quot;bottom_need_backward should be False&quot;;
2637      }
2638      if (layer_name == &quot;innerproduct&quot; || layer_name == &quot;loss&quot;) {
2639        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2640            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be True&quot;;
2641      } else {
2642        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2643            &lt;&lt; &quot;layer_need_backward for &quot; &lt;&lt; layer_name &lt;&lt; &quot; should be False&quot;;
2644      }
2645    }
2646  }
2647  TYPED_TEST(NetTest, TestForcePropagateDown) {
2648    this-&gt;InitForcePropNet(false);
2649    vector&lt;bool&gt; layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2650    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2651      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2652      const vector&lt;bool&gt; need_backward =
2653          this-&gt;net_-&gt;bottom_need_backward()[layer_id];
2654      if (layer_name == &quot;data&quot;) {
2655        ASSERT_EQ(need_backward.size(), 0);
2656        EXPECT_FALSE(layer_need_backward[layer_id]);
2657      } else if (layer_name == &quot;innerproduct&quot;) {
2658        ASSERT_EQ(need_backward.size(), 1);
2659        EXPECT_FALSE(need_backward[0]);  
2660        EXPECT_TRUE(layer_need_backward[layer_id]);
2661      } else if (layer_name == &quot;loss&quot;) {
2662        ASSERT_EQ(need_backward.size(), 2);
2663        EXPECT_TRUE(need_backward[0]);   
2664        EXPECT_FALSE(need_backward[1]);  
2665        EXPECT_TRUE(layer_need_backward[layer_id]);
2666      } else {
2667        LOG(FATAL) &lt;&lt; &quot;Unknown layer: &quot; &lt;&lt; layer_name;
2668      }
2669    }
2670    this-&gt;InitForcePropNet(true);
2671    layer_need_backward = this-&gt;net_-&gt;layer_need_backward();
2672    for (int layer_id = 0; layer_id &lt; this-&gt;net_-&gt;layers().size(); ++layer_id) {
2673      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[layer_id];
2674      const vector&lt;bool&gt; need_backward =
2675          this-&gt;net_-&gt;bottom_need_backward()[layer_id];
2676      if (layer_name == &quot;data&quot;) {
2677        ASSERT_EQ(need_backward.size(), 0);
2678        EXPECT_FALSE(layer_need_backward[layer_id]);
2679      } else if (layer_name == &quot;innerproduct&quot;) {
2680        ASSERT_EQ(need_backward.size(), 1);
2681        EXPECT_TRUE(need_backward[0]);  
2682        EXPECT_TRUE(layer_need_backward[layer_id]);
2683      } else if (layer_name == &quot;loss&quot;) {
2684        ASSERT_EQ(need_backward.size(), 2);
2685        EXPECT_TRUE(need_backward[0]);   
2686        EXPECT_FALSE(need_backward[1]);  
2687        EXPECT_TRUE(layer_need_backward[layer_id]);
2688      } else {
2689        LOG(FATAL) &lt;&lt; &quot;Unknown layer: &quot; &lt;&lt; layer_name;
2690      }
2691    }
2692  }
2693  TYPED_TEST(NetTest, TestAllInOneNetTrain) {
2694    vector&lt;string&gt; stages;
2695    stages.push_back(&quot;train&quot;);
2696    this-&gt;InitAllInOneNet(caffe::TRAIN, 0, &amp;stages);
2697    bool found_data = false;
2698    bool found_loss = false;
2699    for (int i = 0; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
2700      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[i];
2701      if (layer_name == &quot;train-data&quot;) {
2702        found_data = true;
2703      } else if (layer_name == &quot;loss&quot;) {
2704        found_loss = true;
2705      } else {
2706        ASSERT_NE(layer_name, &quot;val-data&quot;);
2707        ASSERT_NE(layer_name, &quot;deploy-data&quot;);
2708      }
2709    }
2710    ASSERT_TRUE(found_data);
2711    ASSERT_TRUE(found_loss);
2712  }
2713  TYPED_TEST(NetTest, TestAllInOneNetVal) {
2714    vector&lt;string&gt; stages;
2715    stages.push_back(&quot;val&quot;);
2716    this-&gt;InitAllInOneNet(caffe::TEST, 0, &amp;stages);
2717    bool found_data = false;
2718    bool found_loss = false;
2719    for (int i = 0; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
2720      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[i];
2721      if (layer_name == &quot;val-data&quot;) {
2722        found_data = true;
2723      } else if (layer_name == &quot;loss&quot;) {
2724        found_loss = true;
2725      } else {
2726        ASSERT_NE(layer_name, &quot;train-data&quot;);
2727        ASSERT_NE(layer_name, &quot;deploy-data&quot;);
2728      }
2729    }
2730    ASSERT_TRUE(found_data);
2731    ASSERT_TRUE(found_loss);
2732  }
2733  TYPED_TEST(NetTest, TestAllInOneNetDeploy) {
2734    vector&lt;string&gt; stages;
2735    stages.push_back(&quot;deploy&quot;);
2736    this-&gt;InitAllInOneNet(caffe::TEST, 0, &amp;stages);
2737    bool found_data = false;
2738    for (int i = 0; i &lt; this-&gt;net_-&gt;layers().size(); ++i) {
2739      const string&amp; layer_name = this-&gt;net_-&gt;layer_names()[i];
2740      if (layer_name == &quot;deploy-data&quot;) {
2741        found_data = true;
2742      } else {
2743        ASSERT_NE(layer_name, &quot;train-data&quot;);
2744        ASSERT_NE(layer_name, &quot;val-data&quot;);
2745        ASSERT_NE(layer_name, &quot;loss&quot;);
2746      }
2747    }
2748    ASSERT_TRUE(found_data);
2749  }
2750  class CompileNetTest : public ::testing::Test {
2751   protected:
2752    void RunCompilerNetTest(
2753        const string&amp; input_param_string, const string&amp; compiled_param_string) {
2754      NetParameter input_param;
2755      CHECK(google::protobuf::TextFormat::ParseFromString(
2756          input_param_string, &amp;input_param));
2757      NetParameter expected_compiled_param;
2758      CHECK(google::protobuf::TextFormat::ParseFromString(
2759          compiled_param_string, &amp;expected_compiled_param));
2760      NetParameter actual_compiled_param;
2761      Net&lt;float&gt;::CompileNet(input_param, &amp;actual_compiled_param);
2762      actual_compiled_param.mutable_compile_net_state()-&gt;Clear();
2763      expected_compiled_param.mutable_compile_net_state()-&gt;Clear();
2764      string expect_net_string = expected_compiled_param.DebugString();
2765      string actual_net_string = actual_compiled_param.DebugString();
2766      EXPECT_EQ(expect_net_string,
2767          actual_net_string);
2768      NetParameter double_compiled_param;
2769      Net&lt;float&gt;::CompileNet(actual_compiled_param, &amp;double_compiled_param);
2770      double_compiled_param.mutable_compile_net_state()-&gt;Clear();
2771      string double_net_string = double_compiled_param.DebugString();
2772      EXPECT_EQ(actual_net_string,
2773         double_net_string);
2774    }
2775  };
2776  #ifndef DISABLE_BN_FOLDING
2777  TEST_F(CompileNetTest, TestRemoveBatchNorm1) {
2778    const string&amp; input_proto = 
2779        &quot;name: &#x27;TestNetwork&#x27; &quot;
2780        &quot;layer { &quot;
2781        &quot;  name: &#x27;data&#x27; &quot;
2782        &quot;  type: &#x27;Data&#x27; &quot;
2783        &quot;  top: &#x27;data&#x27; &quot;
2784        &quot;  top: &#x27;label&#x27; &quot;
2785        &quot;} &quot;
2786        &quot;layer { &quot;
2787        &quot;  bottom: &#x27;data&#x27; &quot;
2788        &quot;  name: &#x27;conv&#x27; &quot;
2789        &quot;  top: &#x27;conv&#x27; &quot;
2790        &quot;  type: &#x27;Convolution&#x27; &quot;
2791        &quot;} &quot;
2792        &quot;layer { &quot;
2793        &quot;  bottom: &#x27;conv&#x27; &quot;
2794        &quot;  name: &#x27;bn&#x27; &quot;
2795        &quot;  top: &#x27;conv&#x27; &quot;
2796        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2797        &quot;} &quot;
2798        &quot;layer { &quot;
2799        &quot;  name: &#x27;loss&#x27; &quot;
2800        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2801        &quot;  bottom: &#x27;conv&#x27; &quot;
2802        &quot;  bottom: &#x27;label&#x27; &quot;
2803        &quot;} &quot;;
2804    const string&amp; output_proto =
2805        &quot;name: &#x27;TestNetwork&#x27; &quot;
2806        &quot;layer { &quot;
2807        &quot;  name: &#x27;data&#x27; &quot;
2808        &quot;  type: &#x27;Data&#x27; &quot;
2809        &quot;  top: &#x27;data&#x27; &quot;
2810        &quot;  top: &#x27;label&#x27; &quot;
2811        &quot;} &quot;
2812        &quot;layer { &quot;
2813        &quot;  bottom: &#x27;data&#x27; &quot;
2814        &quot;  name: &#x27;conv&#x27; &quot;
2815        &quot;  top: &#x27;conv&#x27; &quot;
2816        &quot;  type: &#x27;Convolution&#x27; &quot;
2817        &quot;} &quot;
2818        &quot;layer { &quot;
2819        &quot;  name: &#x27;loss&#x27; &quot;
2820        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2821        &quot;  bottom: &#x27;conv&#x27; &quot;
2822        &quot;  bottom: &#x27;label&#x27; &quot;
2823        &quot;} &quot;;
2824    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2825  }
2826  TEST_F(CompileNetTest, TestRemoveBatchNorm2) {
2827    const string&amp; input_proto = 
2828        &quot;name: &#x27;TestNetwork&#x27; &quot;
2829        &quot;layer { &quot;
2830        &quot;  name: &#x27;data&#x27; &quot;
2831        &quot;  type: &#x27;Data&#x27; &quot;
2832        &quot;  top: &#x27;data&#x27; &quot;
2833        &quot;  top: &#x27;label&#x27; &quot;
2834        &quot;} &quot;
2835        &quot;layer { &quot;
2836        &quot;  bottom: &#x27;data&#x27; &quot;
2837        &quot;  name: &#x27;fc1&#x27; &quot;
2838        &quot;  top: &#x27;fc1&#x27; &quot;
2839        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2840        &quot;} &quot;
2841        &quot;layer { &quot;
2842        &quot;  bottom: &#x27;fc1&#x27; &quot;
2843        &quot;  name: &#x27;bn&#x27; &quot;
2844        &quot;  top: &#x27;bn&#x27; &quot;
2845        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2846        &quot;} &quot;
2847        &quot;layer { &quot;
2848        &quot;  name: &#x27;loss&#x27; &quot;
2849        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2850        &quot;  bottom: &#x27;bn&#x27; &quot;
2851        &quot;  bottom: &#x27;label&#x27; &quot;
2852        &quot;} &quot;;
2853    const string&amp; output_proto =
2854        &quot;name: &#x27;TestNetwork&#x27; &quot;
2855        &quot;layer { &quot;
2856        &quot;  name: &#x27;data&#x27; &quot;
2857        &quot;  type: &#x27;Data&#x27; &quot;
2858        &quot;  top: &#x27;data&#x27; &quot;
2859        &quot;  top: &#x27;label&#x27; &quot;
2860        &quot;} &quot;
2861        &quot;layer { &quot;
2862        &quot;  bottom: &#x27;data&#x27; &quot;
2863        &quot;  name: &#x27;fc1&#x27; &quot;
2864        &quot;  top: &#x27;fc1&#x27; &quot;
2865        &quot;  type: &#x27;InnerProduct&#x27; &quot;
2866        &quot;} &quot;
2867        &quot;layer { &quot;
2868        &quot;  bottom: &#x27;fc1&#x27; &quot;
2869        &quot;  name: &#x27;bn&#x27; &quot;
2870        &quot;  top: &#x27;bn&#x27; &quot;
2871        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2872        &quot;} &quot;
2873        &quot;layer { &quot;
2874        &quot;  name: &#x27;loss&#x27; &quot;
2875        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2876        &quot;  bottom: &#x27;bn&#x27; &quot;
2877        &quot;  bottom: &#x27;label&#x27; &quot;
2878        &quot;} &quot;;
2879    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2880  }
2881  TEST_F(CompileNetTest, TestRemoveBatchNorm3) {
2882    const string&amp; input_proto = 
2883        &quot;name: &#x27;TestNetwork&#x27; &quot;
2884        &quot;layer { &quot;
2885        &quot;  name: &#x27;data&#x27; &quot;
2886        &quot;  type: &#x27;Data&#x27; &quot;
2887        &quot;  top: &#x27;data&#x27; &quot;
2888        &quot;  top: &#x27;label&#x27; &quot;
2889        &quot;} &quot;
2890        &quot;layer { &quot;
2891        &quot;  bottom: &#x27;data&#x27; &quot;
2892        &quot;  name: &#x27;conv&#x27; &quot;
2893        &quot;  top: &#x27;conv&#x27; &quot;
2894        &quot;  type: &#x27;Convolution&#x27; &quot;
2895        &quot;} &quot;
2896        &quot;layer { &quot;
2897        &quot;  bottom: &#x27;conv&#x27; &quot;
2898        &quot;  name: &#x27;bn&#x27; &quot;
2899        &quot;  top: &#x27;conv&#x27; &quot;
2900        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2901  	  &quot;  batch_norm_param { &quot;
2902  	  &quot;    use_global_stats: false&quot;
2903  	  &quot;  }&quot;
2904        &quot;} &quot;
2905        &quot;layer { &quot;
2906        &quot;  name: &#x27;loss&#x27; &quot;
2907        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2908        &quot;  bottom: &#x27;conv&#x27; &quot;
2909        &quot;  bottom: &#x27;label&#x27; &quot;
2910        &quot;} &quot;;
2911    const string&amp; output_proto =
2912        &quot;name: &#x27;TestNetwork&#x27; &quot;
2913        &quot;layer { &quot;
2914        &quot;  name: &#x27;data&#x27; &quot;
2915        &quot;  type: &#x27;Data&#x27; &quot;
2916        &quot;  top: &#x27;data&#x27; &quot;
2917        &quot;  top: &#x27;label&#x27; &quot;
2918        &quot;} &quot;
2919        &quot;layer { &quot;
2920        &quot;  bottom: &#x27;data&#x27; &quot;
2921        &quot;  name: &#x27;conv&#x27; &quot;
2922        &quot;  top: &#x27;conv&#x27; &quot;
2923        &quot;  type: &#x27;Convolution&#x27; &quot;
2924        &quot;} &quot;
2925        &quot;layer { &quot;
2926        &quot;  bottom: &#x27;conv&#x27; &quot;
2927        &quot;  name: &#x27;bn&#x27; &quot;
2928        &quot;  top: &#x27;conv&#x27; &quot;
2929        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2930  	  &quot;  batch_norm_param { &quot;
2931  	  &quot;    use_global_stats: false&quot;
2932  	  &quot;  }&quot;
2933        &quot;} &quot;
2934        &quot;layer { &quot;
2935        &quot;  name: &#x27;loss&#x27; &quot;
2936        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2937        &quot;  bottom: &#x27;conv&#x27; &quot;
2938        &quot;  bottom: &#x27;label&#x27; &quot;
2939        &quot;} &quot;;
2940    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2941  }
2942  TEST_F(CompileNetTest, TestRemoveBatchNorm4) {
2943    const string&amp; input_proto = 
2944        &quot;name: &#x27;TestNetwork&#x27; &quot;
2945        &quot;layer { &quot;
2946        &quot;  name: &#x27;data&#x27; &quot;
2947        &quot;  type: &#x27;Data&#x27; &quot;
2948        &quot;  top: &#x27;data&#x27; &quot;
2949        &quot;  top: &#x27;label&#x27; &quot;
2950        &quot;} &quot;
2951        &quot;layer { &quot;
2952        &quot;  bottom: &#x27;data&#x27; &quot;
2953        &quot;  name: &#x27;conv&#x27; &quot;
2954        &quot;  top: &#x27;conv&#x27; &quot;
2955        &quot;  type: &#x27;Convolution&#x27; &quot;
2956        &quot;} &quot;
2957        &quot;layer { &quot;
2958        &quot;  bottom: &#x27;conv&#x27; &quot;
2959        &quot;  name: &#x27;bn&#x27; &quot;
2960        &quot;  top: &#x27;conv&#x27; &quot;
2961        &quot;  type: &#x27;BatchNorm&#x27; &quot;
2962  	  &quot;  batch_norm_param { &quot;
2963  	  &quot;    use_global_stats: true&quot;
2964  	  &quot;  }&quot;
2965        &quot;} &quot;
2966        &quot;layer { &quot;
2967        &quot;  name: &#x27;loss&#x27; &quot;
2968        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2969        &quot;  bottom: &#x27;conv&#x27; &quot;
2970        &quot;  bottom: &#x27;label&#x27; &quot;
2971        &quot;} &quot;;
2972    const string&amp; output_proto =
2973        &quot;name: &#x27;TestNetwork&#x27; &quot;
2974        &quot;layer { &quot;
2975        &quot;  name: &#x27;data&#x27; &quot;
2976        &quot;  type: &#x27;Data&#x27; &quot;
2977        &quot;  top: &#x27;data&#x27; &quot;
2978        &quot;  top: &#x27;label&#x27; &quot;
2979        &quot;} &quot;
2980        &quot;layer { &quot;
2981        &quot;  bottom: &#x27;data&#x27; &quot;
2982        &quot;  name: &#x27;conv&#x27; &quot;
2983        &quot;  top: &#x27;conv&#x27; &quot;
2984        &quot;  type: &#x27;Convolution&#x27; &quot;
2985        &quot;} &quot;
2986        &quot;layer { &quot;
2987        &quot;  name: &#x27;loss&#x27; &quot;
2988        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
2989        &quot;  bottom: &#x27;conv&#x27; &quot;
2990        &quot;  bottom: &#x27;label&#x27; &quot;
2991        &quot;} &quot;;
2992    this-&gt;RunCompilerNetTest(input_proto, output_proto);
2993  }
2994  #endif
2995  #ifdef MKL2017_SUPPORTED
2996  TEST_F(CompileNetTest, TestCompileNetBatchNorm) {
2997    const string&amp; input_proto =
2998        &quot;name: &#x27;TestNetwork&#x27; &quot;
2999        &quot;layer { &quot;
3000        &quot;  name: &#x27;data&#x27; &quot;
3001        &quot;  type: &#x27;Data&#x27; &quot;
3002        &quot;  top: &#x27;data&#x27; &quot;
3003        &quot;  top: &#x27;label&#x27; &quot;
3004        &quot;} &quot;
3005        &quot;layer { &quot;
3006        &quot;  bottom: &#x27;data&#x27; &quot;
3007        &quot;  name: &#x27;bn&#x27; &quot;
3008        &quot;  top: &#x27;bn&#x27; &quot;
3009        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3010        &quot;  batch_norm_param { &quot;
3011        &quot;   engine: MKL2017 &quot;
3012        &quot;  } &quot;
3013        &quot;} &quot;
3014        &quot;layer { &quot;
3015        &quot; bottom: &#x27;bn&#x27; &quot;
3016        &quot; top: &#x27;sc&#x27; &quot;
3017        &quot; name: &#x27;sc&#x27; &quot;
3018        &quot; type: &#x27;Scale&#x27; &quot;
3019        &quot; scale_param { &quot;
3020        &quot;   bias_term: true &quot;
3021        &quot; }&quot;
3022        &quot;}&quot;
3023        &quot;layer { &quot;
3024        &quot;  name: &#x27;loss&#x27; &quot;
3025        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3026        &quot;  bottom: &#x27;sc&#x27; &quot;
3027        &quot;  bottom: &#x27;label&#x27; &quot;
3028        &quot;} &quot;;
3029    const string&amp; output_proto =
3030        &quot;name: &#x27;TestNetwork&#x27; &quot;
3031        &quot;layer { &quot;
3032        &quot;  name: &#x27;data&#x27; &quot;
3033        &quot;  type: &#x27;Data&#x27; &quot;
3034        &quot;  top: &#x27;data&#x27; &quot;
3035        &quot;  top: &#x27;label&#x27; &quot;
3036        &quot;} &quot;
3037        &quot;layer { &quot;
3038        &quot;  bottom: &#x27;data&#x27; &quot;
3039        &quot;  name: &#x27;bn&#x27; &quot;
3040        &quot;  top: &#x27;sc&#x27; &quot;
3041        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3042        &quot;  batch_norm_param { &quot;
3043        &quot;   engine: MKL2017 &quot;
3044        &quot;   bias_term: true &quot;
3045        &quot;  } &quot;
3046        &quot;} &quot;
3047        &quot;layer { &quot;
3048        &quot;  name: &#x27;loss&#x27; &quot;
3049        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3050        &quot;  bottom: &#x27;sc&#x27; &quot;
3051        &quot;  bottom: &#x27;label&#x27; &quot;
3052        &quot;} &quot;;
3053    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3054  }
3055  TEST_F(CompileNetTest, TestCompileNetBatchNormInPlace) {
3056    const string&amp; input_proto =
3057        &quot;name: &#x27;TestNetwork&#x27; &quot;
3058        &quot;layer { &quot;
3059        &quot;  name: &#x27;data&#x27; &quot;
3060        &quot;  type: &#x27;Data&#x27; &quot;
3061        &quot;  top: &#x27;data&#x27; &quot;
3062        &quot;  top: &#x27;label&#x27; &quot;
3063        &quot;} &quot;
3064        &quot;layer { &quot;
3065        &quot;  bottom: &#x27;data&#x27; &quot;
3066        &quot;  name: &#x27;bn&#x27; &quot;
3067        &quot;  top: &#x27;data&#x27; &quot;
3068        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3069        &quot;  batch_norm_param { &quot;
3070        &quot;   engine: MKL2017 &quot;
3071        &quot;  } &quot;
3072        &quot;} &quot;
3073        &quot;layer { &quot;
3074        &quot; bottom: &#x27;data&#x27; &quot;
3075        &quot; top: &#x27;data&#x27; &quot;
3076        &quot; name: &#x27;sc&#x27; &quot;
3077        &quot; type: &#x27;Scale&#x27; &quot;
3078        &quot; scale_param { &quot;
3079        &quot;   bias_term: true &quot;
3080        &quot; }&quot;
3081        &quot;}&quot;
3082        &quot;layer { &quot;
3083        &quot; bottom: &#x27;data&#x27; &quot;
3084        &quot; top: &#x27;data&#x27; &quot;
3085        &quot; name: &#x27;relu&#x27; &quot;
3086        &quot; type: &#x27;ReLU&#x27; &quot;
3087        &quot; relu_param { &quot;
3088        &quot;  engine: MKL2017 &quot;
3089        &quot; } &quot;
3090        &quot;}&quot;
3091        &quot;layer { &quot;
3092        &quot;  name: &#x27;loss&#x27; &quot;
3093        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3094        &quot;  bottom: &#x27;data&#x27; &quot;
3095        &quot;  bottom: &#x27;label&#x27; &quot;
3096        &quot;} &quot;;
3097    const string&amp; output_proto =
3098        &quot;name: &#x27;TestNetwork&#x27; &quot;
3099        &quot;layer { &quot;
3100        &quot;  name: &#x27;data&#x27; &quot;
3101        &quot;  type: &#x27;Data&#x27; &quot;
3102        &quot;  top: &#x27;data&#x27; &quot;
3103        &quot;  top: &#x27;label&#x27; &quot;
3104        &quot;} &quot;
3105        &quot;layer { &quot;
3106        &quot;  bottom: &#x27;data&#x27; &quot;
3107        &quot;  name: &#x27;bn&#x27; &quot;
3108        &quot;  top: &#x27;data_x&#x27; &quot;
3109        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3110        &quot;  batch_norm_param { &quot;
3111        &quot;   engine: MKL2017 &quot;
3112        &quot;   bias_term: true &quot;
3113        &quot;  } &quot;
3114        &quot;} &quot;
3115        &quot;layer { &quot;
3116        &quot; bottom: &#x27;data_x&#x27; &quot;
3117        &quot; top: &#x27;data_x&#x27; &quot;
3118        &quot; name: &#x27;relu&#x27; &quot;
3119        &quot; type: &#x27;ReLU&#x27; &quot;
3120        &quot; relu_param { &quot;
3121        &quot;  engine: MKL2017 &quot;
3122        &quot; } &quot;
3123        &quot;}&quot;
3124        &quot;layer { &quot;
3125        &quot;  name: &#x27;loss&#x27; &quot;
3126        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3127        &quot;  bottom: &#x27;data_x&#x27; &quot;
3128        &quot;  bottom: &#x27;label&#x27; &quot;
3129        &quot;} &quot;;
3130    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3131  }
3132  #endif
3133  #if defined(MKL2017_SUPPORTED) &amp;&amp; defined(MKLDNN_SUPPORTED)
3134  TEST_F(CompileNetTest, TestCompileNetBatchNormConvolution) {
3135    const string&amp; input_proto =
3136        &quot;name: &#x27;TestNetwork&#x27; &quot;
3137        &quot;layer { &quot;
3138        &quot;  name: &#x27;data&#x27; &quot;
3139        &quot;  type: &#x27;Data&#x27; &quot;
3140        &quot;  top: &#x27;data&#x27; &quot;
3141        &quot;  top: &#x27;label&#x27; &quot;
3142        &quot;} &quot;
3143        &quot;layer { &quot;
3144        &quot;  bottom: &#x27;data&#x27; &quot;
3145        &quot;  name: &#x27;bn&#x27; &quot;
3146        &quot;  top: &#x27;bn&#x27; &quot;
3147        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3148        &quot;  batch_norm_param { &quot;
3149        &quot;   engine: MKL2017 &quot;
3150        &quot;  } &quot;
3151        &quot;} &quot;
3152        &quot;layer { &quot;
3153        &quot; bottom: &#x27;bn&#x27; &quot;
3154        &quot; top: &#x27;conv&#x27; &quot;
3155        &quot; name: &#x27;sc&#x27; &quot;
3156        &quot; type: &#x27;Scale&#x27; &quot;
3157        &quot; scale_param { &quot;
3158        &quot;   bias_term: true &quot;
3159        &quot; }&quot;
3160        &quot;}&quot;
3161        &quot;layer { &quot;
3162        &quot;  bottom: &#x27;conv&#x27; &quot;
3163        &quot;  name: &#x27;conv&#x27; &quot;
3164        &quot;  top: &#x27;relu&#x27; &quot;
3165        &quot;  type: &#x27;Convolution&#x27; &quot;
3166        &quot;  convolution_param { &quot;
3167        &quot;   engine: MKLDNN &quot;
3168        &quot;  } &quot;
3169        &quot;} &quot;
3170        &quot;layer { &quot;
3171        &quot; bottom: &#x27;relu&#x27; &quot;
3172        &quot; top: &#x27;relu&#x27; &quot;
3173        &quot; name: &#x27;relu&#x27; &quot;
3174        &quot; type: &#x27;ReLU&#x27; &quot;
3175        &quot; relu_param { &quot;
3176        &quot;  engine: MKLDNN &quot;
3177        &quot; } &quot;
3178        &quot;}&quot;
3179        &quot;layer { &quot;
3180        &quot;  name: &#x27;loss&#x27; &quot;
3181        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3182        &quot;  bottom: &#x27;relu&#x27; &quot;
3183        &quot;  bottom: &#x27;label&#x27; &quot;
3184        &quot;} &quot;;
3185    const string&amp; output_proto =
3186        &quot;name: &#x27;TestNetwork&#x27; &quot;
3187        &quot;layer { &quot;
3188        &quot;  name: &#x27;data&#x27; &quot;
3189        &quot;  type: &#x27;Data&#x27; &quot;
3190        &quot;  top: &#x27;data&#x27; &quot;
3191        &quot;  top: &#x27;label&#x27; &quot;
3192        &quot;} &quot;
3193        &quot;layer { &quot;
3194        &quot;  bottom: &#x27;data&#x27; &quot;
3195        &quot;  name: &#x27;bn&#x27; &quot;
3196        &quot;  top: &#x27;conv&#x27; &quot;
3197        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3198        &quot;  batch_norm_param { &quot;
3199        &quot;   engine: MKL2017 &quot;
3200        &quot;   bias_term: true &quot;
3201        &quot;  } &quot;
3202        &quot;} &quot;
3203        &quot;layer { &quot;
3204        &quot;  bottom: &#x27;conv&#x27; &quot;
3205        &quot;  name: &#x27;conv&#x27; &quot;
3206        &quot;  top: &#x27;relu&#x27; &quot;
3207        &quot;  type: &#x27;Convolution&#x27; &quot;
3208        &quot;  convolution_param { &quot;
3209        &quot;   engine: MKLDNN &quot;
3210        &quot;   relu: true &quot;
3211        &quot;negative_slope: 0&quot;
3212        &quot;  } &quot;
3213        &quot;} &quot;
3214        &quot;layer { &quot;
3215        &quot;  name: &#x27;loss&#x27; &quot;
3216        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3217        &quot;  bottom: &#x27;relu&#x27; &quot;
3218        &quot;  bottom: &#x27;label&#x27; &quot;
3219        &quot;} &quot;;
3220    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3221  }
3222  #endif
3223  #ifndef DISABLE_CONV_SUM_FUSION
3224  TEST_F(CompileNetTest, TestCompileNetConvEltReluFusionMKLDNN) {
3225    const string&amp; input_proto =
3226        &quot;name: &#x27;TestNetwork&#x27; &quot;
3227        &quot;layer { &quot;
3228        &quot;  name: &#x27;data&#x27; &quot;
3229        &quot;  type: &#x27;Data&#x27; &quot;
3230        &quot;  top: &#x27;data&#x27; &quot;
3231        &quot;  top: &#x27;label&#x27; &quot;
3232        &quot;} &quot;
3233        &quot;layer { &quot;
3234        &quot;  bottom: &#x27;data&#x27; &quot;
3235        &quot;  name: &#x27;conv1&#x27; &quot;
3236        &quot;  top: &#x27;conv1&#x27; &quot;
3237        &quot;  type: &#x27;Convolution&#x27; &quot;
3238        &quot;  convolution_param { &quot;
3239        &quot;   engine: MKLDNN &quot;
3240        &quot;  } &quot;
3241        &quot;} &quot;
3242        &quot;layer { &quot;
3243        &quot;  bottom: &#x27;data&#x27; &quot;
3244        &quot;  name: &#x27;conv2&#x27; &quot;
3245        &quot;  top: &#x27;conv2&#x27; &quot;
3246        &quot;  type: &#x27;Convolution&#x27; &quot;
3247        &quot;  convolution_param { &quot;
3248        &quot;   engine: MKLDNN &quot;
3249        &quot;  } &quot;
3250        &quot;} &quot;
3251        &quot;layer { &quot;
3252        &quot;  bottom: &#x27;conv1&#x27; &quot;
3253        &quot;  name: &#x27;conv3&#x27; &quot;
3254        &quot;  top: &#x27;conv3&#x27; &quot;
3255        &quot;  type: &#x27;Convolution&#x27; &quot;
3256        &quot;  convolution_param { &quot;
3257        &quot;   engine: MKLDNN &quot;
3258        &quot;  } &quot;
3259        &quot;} &quot;
3260        &quot;layer { &quot;
3261        &quot;  bottom: &#x27;conv2&#x27; &quot;
3262        &quot;  bottom: &#x27;conv3&#x27; &quot;
3263        &quot;  name: &#x27;conv4&#x27; &quot;
3264        &quot;  top: &#x27;relu&#x27; &quot;
3265        &quot;  type: &#x27;Eltwise&#x27; &quot;
3266        &quot;} &quot;
3267        &quot;layer { &quot;
3268        &quot; bottom: &#x27;relu&#x27; &quot;
3269        &quot; top: &#x27;relu&#x27; &quot;
3270        &quot; name: &#x27;relu&#x27; &quot;
3271        &quot; type: &#x27;ReLU&#x27; &quot;
3272        &quot;}&quot;
3273        &quot;layer { &quot;
3274        &quot;  name: &#x27;loss&#x27; &quot;
3275        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3276        &quot;  bottom: &#x27;relu&#x27; &quot;
3277        &quot;  bottom: &#x27;label&#x27; &quot;
3278        &quot;} &quot;;
3279    const string&amp; output_proto =
3280        &quot;name: &#x27;TestNetwork&#x27; &quot;
3281        &quot;layer { &quot;
3282        &quot;  name: &#x27;data&#x27; &quot;
3283        &quot;  type: &#x27;Data&#x27; &quot;
3284        &quot;  top: &#x27;data&#x27; &quot;
3285        &quot;  top: &#x27;label&#x27; &quot;
3286        &quot;} &quot;
3287        &quot;layer { &quot;
3288        &quot;  bottom: &#x27;data&#x27; &quot;
3289        &quot;  name: &#x27;conv1&#x27; &quot;
3290        &quot;  top: &#x27;conv1&#x27; &quot;
3291        &quot;  type: &#x27;Convolution&#x27; &quot;
3292        &quot;  convolution_param { &quot;
3293        &quot;   engine: MKLDNN &quot;
3294        &quot;  } &quot;
3295        &quot;} &quot;
3296        &quot;layer { &quot;
3297        &quot;  bottom: &#x27;data&#x27; &quot;
3298        &quot;  name: &#x27;conv2&#x27; &quot;
3299        &quot;  top: &#x27;conv2&#x27; &quot;
3300        &quot;  type: &#x27;Convolution&#x27; &quot;
3301        &quot;  convolution_param { &quot;
3302        &quot;   engine: MKLDNN &quot;
3303        &quot;  } &quot;
3304        &quot;} &quot;
3305        &quot;layer { &quot;
3306        &quot;  bottom: &#x27;conv1&#x27; &quot;
3307        &quot;  bottom: &#x27;conv2&#x27; &quot;
3308        &quot;  name: &#x27;conv3&#x27; &quot;
3309        &quot;  top: &#x27;relu&#x27; &quot;
3310        &quot;  type: &#x27;Convolution&#x27; &quot;
3311        &quot;  convolution_param { &quot;
3312        &quot;   engine: MKLDNN &quot;
3313        &quot;   relu: true &quot;
3314        &quot;   fusion_type: SUM_FUSION &quot;
3315        &quot;  } &quot;
3316        &quot;} &quot;
3317        &quot;layer { &quot;
3318        &quot;  name: &#x27;loss&#x27; &quot;
3319        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3320        &quot;  bottom: &#x27;relu&#x27; &quot;
3321        &quot;  bottom: &#x27;label&#x27; &quot;
3322        &quot;} &quot;;
3323   const string input_proto_test = &quot;state: { phase: TEST } engine: &#x27;MKLDNN&#x27;&quot; + input_proto;
3324   const string output_proto_test = &quot;state: { phase: TEST } engine: &#x27;MKLDNN&#x27;&quot; + output_proto;
3325   this-&gt;RunCompilerNetTest(input_proto_test, output_proto_test);
3326  }
3327  #endif
3328  #ifdef MKLDNN_SUPPORTED
3329  TEST_F(CompileNetTest, TestCompileNetBatchNormMKLDNN) {
3330      const string&amp; input_proto =
3331        &quot;name: &#x27;TestNetwork&#x27; &quot;
3332        &quot;layer { &quot;
3333        &quot;  name: &#x27;data&#x27; &quot;
3334        &quot;  type: &#x27;Data&#x27; &quot;
3335        &quot;  top: &#x27;data&#x27; &quot;
3336        &quot;  top: &#x27;label&#x27; &quot;
3337        &quot;} &quot;
3338        &quot;layer { &quot;
3339        &quot;  bottom: &#x27;data&#x27; &quot;
3340        &quot;  name: &#x27;bn&#x27; &quot;
3341        &quot;  top: &#x27;bn&#x27; &quot;
3342        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3343        &quot;  batch_norm_param { &quot;
3344        &quot;   engine: MKLDNN &quot;
3345        &quot;  } &quot;
3346        &quot;} &quot;
3347        &quot;layer { &quot;
3348        &quot; bottom: &#x27;bn&#x27; &quot;
3349        &quot; top: &#x27;sc&#x27; &quot;
3350        &quot; name: &#x27;sc&#x27; &quot;
3351        &quot; type: &#x27;Scale&#x27; &quot;
3352        &quot; scale_param { &quot;
3353        &quot;   bias_term: true &quot;
3354        &quot; }&quot;
3355        &quot;}&quot;
3356        &quot;layer { &quot;
3357        &quot;  name: &#x27;loss&#x27; &quot;
3358        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3359        &quot;  bottom: &#x27;sc&#x27; &quot;
3360        &quot;  bottom: &#x27;label&#x27; &quot;
3361        &quot;} &quot;;
3362    const string&amp; output_proto =
3363        &quot;name: &#x27;TestNetwork&#x27; &quot;
3364        &quot;layer { &quot;
3365        &quot;  name: &#x27;data&#x27; &quot;
3366        &quot;  type: &#x27;Data&#x27; &quot;
3367        &quot;  top: &#x27;data&#x27; &quot;
3368        &quot;  top: &#x27;label&#x27; &quot;
3369        &quot;} &quot;
3370        &quot;layer { &quot;
3371        &quot;  bottom: &#x27;data&#x27; &quot;
3372        &quot;  name: &#x27;bn&#x27; &quot;
3373        &quot;  top: &#x27;sc&#x27; &quot;
3374        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3375        &quot;  batch_norm_param { &quot;
3376        &quot;   engine: MKLDNN &quot;
3377        &quot;   bias_term: true &quot;
3378        &quot;  } &quot;
3379        &quot;} &quot;
3380        &quot;layer { &quot;
3381        &quot;  name: &#x27;loss&#x27; &quot;
3382        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3383        &quot;  bottom: &#x27;sc&#x27; &quot;
3384        &quot;  bottom: &#x27;label&#x27; &quot;
3385        &quot;} &quot;;
3386    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3387  }
3388  TEST_F(CompileNetTest, TestCompileNetConvolution) {
3389    const string&amp; input_proto =
3390        &quot;name: &#x27;TestNetwork&#x27; &quot;
3391        &quot;layer { &quot;
3392        &quot;  name: &#x27;data&#x27; &quot;
3393        &quot;  type: &#x27;Data&#x27; &quot;
3394        &quot;  top: &#x27;data&#x27; &quot;
3395        &quot;  top: &#x27;label&#x27; &quot;
3396        &quot;} &quot;
3397        &quot;layer { &quot;
3398        &quot;  bottom: &#x27;data&#x27; &quot;
3399        &quot;  name: &#x27;conv&#x27; &quot;
3400        &quot;  top: &#x27;relu&#x27; &quot;
3401        &quot;  type: &#x27;Convolution&#x27; &quot;
3402        &quot;  convolution_param { &quot;
3403        &quot;   engine: MKLDNN &quot;
3404        &quot;  } &quot;
3405        &quot;} &quot;
3406        &quot;layer { &quot;
3407        &quot; bottom: &#x27;relu&#x27; &quot;
3408        &quot; top: &#x27;relu&#x27; &quot;
3409        &quot; name: &#x27;relu&#x27; &quot;
3410        &quot; type: &#x27;ReLU&#x27; &quot;
3411        &quot; relu_param { &quot;
3412        &quot;  engine: MKLDNN &quot;
3413        &quot; } &quot;
3414        &quot;}&quot;
3415        &quot;layer { &quot;
3416        &quot;  name: &#x27;loss&#x27; &quot;
3417        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3418        &quot;  bottom: &#x27;relu&#x27; &quot;
3419        &quot;  bottom: &#x27;label&#x27; &quot;
3420        &quot;} &quot;;
3421    const string&amp; output_proto =
3422        &quot;name: &#x27;TestNetwork&#x27; &quot;
3423        &quot;layer { &quot;
3424        &quot;  name: &#x27;data&#x27; &quot;
3425        &quot;  type: &#x27;Data&#x27; &quot;
3426        &quot;  top: &#x27;data&#x27; &quot;
3427        &quot;  top: &#x27;label&#x27; &quot;
3428        &quot;} &quot;
3429        &quot;layer { &quot;
3430        &quot;  bottom: &#x27;data&#x27; &quot;
3431        &quot;  name: &#x27;conv&#x27; &quot;
3432        &quot;  top: &#x27;relu&#x27; &quot;
3433        &quot;  type: &#x27;Convolution&#x27; &quot;
3434        &quot;  convolution_param { &quot;
3435        &quot;   engine: MKLDNN &quot;
3436        &quot;   relu: true &quot;
3437        &quot;negative_slope: 0&quot;
3438        &quot;  } &quot;
3439        &quot;} &quot;
3440        &quot;layer { &quot;
3441        &quot;  name: &#x27;loss&#x27; &quot;
3442        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3443        &quot;  bottom: &#x27;relu&#x27; &quot;
3444        &quot;  bottom: &#x27;label&#x27; &quot;
3445        &quot;} &quot;;
3446    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3447  }
3448  TEST_F(CompileNetTest, TestCompileNetLayerParamEngineConvolution) {
3449    const string&amp; input_proto =
3450        &quot;name: &#x27;TestNetwork&#x27; &quot;
3451        &quot;layer { &quot;
3452        &quot;  name: &#x27;data&#x27; &quot;
3453        &quot;  type: &#x27;Data&#x27; &quot;
3454        &quot;  top: &#x27;data&#x27; &quot;
3455        &quot;  top: &#x27;label&#x27; &quot;
3456        &quot;} &quot;
3457        &quot;layer { &quot;
3458        &quot;  bottom: &#x27;data&#x27; &quot;
3459        &quot;  name: &#x27;conv&#x27; &quot;
3460        &quot;  top: &#x27;relu&#x27; &quot;
3461        &quot;  type: &#x27;Convolution&#x27; &quot;
3462        &quot;  engine: &#x27;MKLDNN:CPU&#x27; &quot;
3463        &quot;  convolution_param { &quot;
3464        &quot;  } &quot;
3465        &quot;} &quot;
3466        &quot;layer { &quot;
3467        &quot; bottom: &#x27;relu&#x27; &quot;
3468        &quot; top: &#x27;relu&#x27; &quot;
3469        &quot; name: &#x27;relu&#x27; &quot;
3470        &quot; type: &#x27;ReLU&#x27; &quot;
3471        &quot; engine: &#x27;MKLDNN:CPU&#x27; &quot;
3472        &quot; relu_param { &quot;
3473        &quot; } &quot;
3474        &quot;}&quot;
3475        &quot;layer { &quot;
3476        &quot;  name: &#x27;loss&#x27; &quot;
3477        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3478        &quot;  bottom: &#x27;relu&#x27; &quot;
3479        &quot;  bottom: &#x27;label&#x27; &quot;
3480        &quot;} &quot;;
3481    const string&amp; output_proto =
3482        &quot;name: &#x27;TestNetwork&#x27; &quot;
3483        &quot;layer { &quot;
3484        &quot;  name: &#x27;data&#x27; &quot;
3485        &quot;  type: &#x27;Data&#x27; &quot;
3486        &quot;  top: &#x27;data&#x27; &quot;
3487        &quot;  top: &#x27;label&#x27; &quot;
3488        &quot;} &quot;
3489        &quot;layer { &quot;
3490        &quot;  bottom: &#x27;data&#x27; &quot;
3491        &quot;  name: &#x27;conv&#x27; &quot;
3492        &quot;  top: &#x27;relu&#x27; &quot;
3493        &quot;  type: &#x27;Convolution&#x27; &quot;
3494        &quot;  engine: &#x27;MKLDNN:CPU&#x27; &quot;
3495        &quot;  convolution_param { &quot;
3496        &quot;   relu: true &quot;
3497        &quot;negative_slope: 0&quot;
3498        &quot;  } &quot;
3499        &quot;} &quot;
3500        &quot;layer { &quot;
3501        &quot;  name: &#x27;loss&#x27; &quot;
3502        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3503        &quot;  bottom: &#x27;relu&#x27; &quot;
3504        &quot;  bottom: &#x27;label&#x27; &quot;
3505        &quot;} &quot;;
3506    this-&gt;RunCompilerNetTest(input_proto, output_proto);
3507  }
3508  TEST_F(CompileNetTest, TestNoCompileNetLayerParamEngineConvolution) {
3509    const string&amp; input_proto =
3510        &quot;name: &#x27;TestNetwork&#x27; &quot;
3511        &quot;layer { &quot;
3512        &quot;  name: &#x27;data&#x27; &quot;
3513        &quot;  type: &#x27;Data&#x27; &quot;
3514        &quot;  top: &#x27;data&#x27; &quot;
3515        &quot;  top: &#x27;label&#x27; &quot;
3516        &quot;} &quot;
3517        &quot;layer { &quot;
3518        &quot;  bottom: &#x27;data&#x27; &quot;
3519        &quot;  name: &#x27;conv&#x27; &quot;
3520        &quot;  top: &#x27;relu&#x27; &quot;
3521        &quot;  type: &#x27;Convolution&#x27; &quot;
3522        &quot;  engine: &#x27;MKLDNN:DLA,CPU&#x27; &quot;
3523        &quot;  convolution_param { &quot;
3524        &quot;  } &quot;
3525        &quot;} &quot;
3526        &quot;layer { &quot;
3527        &quot; bottom: &#x27;relu&#x27; &quot;
3528        &quot; top: &#x27;relu&#x27; &quot;
3529        &quot; name: &#x27;relu&#x27; &quot;
3530        &quot; type: &#x27;ReLU&#x27; &quot;
3531        &quot;  engine: &#x27;MKLDNN:DLA,CPU&#x27; &quot;
3532        &quot; relu_param { &quot;
3533        &quot; } &quot;
3534        &quot;}&quot;
3535        &quot;layer { &quot;
3536        &quot;  name: &#x27;loss&#x27; &quot;
3537        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3538        &quot;  bottom: &#x27;relu&#x27; &quot;
3539        &quot;  bottom: &#x27;label&#x27; &quot;
3540        &quot;} &quot;;
3541    this-&gt;RunCompilerNetTest(input_proto, input_proto);
3542  }
3543  #endif
3544  TEST_F(CompileNetTest, TestNoCompileNet) {
3545    const string&amp; input_proto=
3546        &quot;name: &#x27;TestNetwork&#x27; &quot;
3547        &quot;layer { &quot;
3548        &quot;  name: &#x27;data&#x27; &quot;
3549        &quot;  type: &#x27;Data&#x27; &quot;
3550        &quot;  top: &#x27;data&#x27; &quot;
3551        &quot;  top: &#x27;label&#x27; &quot;
3552        &quot;} &quot;
3553        &quot;layer { &quot;
3554        &quot;  bottom: &#x27;data&#x27; &quot;
3555        &quot;  name: &#x27;bn&#x27; &quot;
3556        &quot;  top: &#x27;bn&#x27; &quot;
3557        &quot;  type: &#x27;BatchNorm&#x27; &quot;
3558        &quot;  batch_norm_param { &quot;
3559        &quot;   engine: CAFFE &quot;
3560        &quot;  } &quot;
3561        &quot;} &quot;
3562        &quot;layer { &quot;
3563        &quot; bottom: &#x27;bn&#x27; &quot;
3564        &quot; top: &#x27;sc&#x27; &quot;
3565        &quot; name: &#x27;sc&#x27; &quot;
3566        &quot; type: &#x27;Scale&#x27; &quot;
3567        &quot; scale_param { &quot;
3568        &quot;   bias_term: true &quot;
3569        &quot; }&quot;
3570        &quot;}&quot;
3571        &quot;layer { &quot;
3572        &quot;  name: &#x27;loss&#x27; &quot;
3573        &quot;  type: &#x27;SoftmaxWithLoss&#x27; &quot;
3574        &quot;  bottom: &#x27;sc&#x27; &quot;
3575        &quot;  bottom: &#x27;label&#x27; &quot;
3576        &quot;} &quot;;
3577    this-&gt;RunCompilerNetTest(input_proto, input_proto);
3578  }
3579  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</div>
                </div>
                <div class="column column_space"><pre><code>315          &quot;    } &quot;
316          &quot;  } &quot;
317          &quot;  param { &quot;
318          &quot;    name: &#x27;unsharedweights1&#x27; &quot;
319          &quot;    lr_mult: &quot; &lt;&lt; blobs_lr_w1 &lt;&lt;
320          &quot;  } &quot;;
321      if (bias_term) {
322        proto &lt;&lt; &quot;  param { lr_mult: &quot; &lt;&lt; blobs_lr_b1 &lt;&lt; &quot; } &quot;;
323      }
324      proto &lt;&lt;
325          &quot;  bottom: &#x27;data&#x27; &quot;
326          &quot;  top: &#x27;innerproduct1&#x27; &quot;;
</pre></code></div>
                <div class="column column_space"><pre><code>341          &quot;    } &quot;
342          &quot;  } &quot;
343          &quot;  param { &quot;
344          &quot;    name: &#x27;unsharedweights2&#x27; &quot;
345          &quot;    lr_mult: &quot; &lt;&lt; blobs_lr_w2 &lt;&lt;
346          &quot;  } &quot;;
347      if (bias_term) {
348        proto &lt;&lt; &quot;  param { lr_mult: &quot; &lt;&lt; blobs_lr_b2 &lt;&lt; &quot; } &quot;;
349      }
350      proto &lt;&lt;
351          &quot;  bottom: &#x27;data&#x27; &quot;
352          &quot;  top: &#x27;innerproduct2&#x27; &quot;
353          &quot;} &quot;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    