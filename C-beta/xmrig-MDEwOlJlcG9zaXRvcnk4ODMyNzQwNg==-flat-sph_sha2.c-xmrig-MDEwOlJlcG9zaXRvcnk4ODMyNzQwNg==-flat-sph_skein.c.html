
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 31.758034026465026%, Tokens: 26, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_sha2.c</h3>
            <pre><code>1  #include <stddef.h>
2  #include <string.h>
3  #include "sph_sha2.h"
4  #if SPH_SMALL_FOOTPRINT && !defined SPH_SMALL_FOOTPRINT_SHA2
5  #define SPH_SMALL_FOOTPRINT_SHA2   1
6  #endif
7  #define CH(X, Y, Z)    ((((Y) ^ (Z)) & (X)) ^ (Z))
8  #define MAJ( X, Y, Z )   ( Y  ^ ( ( X_xor_Y = X ^ Y ) & ( Y_xor_Z ) ) )
9  #define ROTR    SPH_ROTR32
10  #define BSG2_0(x)      (ROTR(x, 2) ^ ROTR(x, 13) ^ ROTR(x, 22))
11  #define BSG2_1(x)      (ROTR(x, 6) ^ ROTR(x, 11) ^ ROTR(x, 25))
12  #define SSG2_0(x)      (ROTR(x, 7) ^ ROTR(x, 18) ^ SPH_T32((x) >> 3))
13  #define SSG2_1(x)      (ROTR(x, 17) ^ ROTR(x, 19) ^ SPH_T32((x) >> 10))
<span onclick='openModal()' class='match'>14  static const sph_u32 H224[8] = {
15  	SPH_C32(0xC1059ED8), SPH_C32(0x367CD507), SPH_C32(0x3070DD17),
16  	SPH_C32(0xF70E5939), SPH_C32(0xFFC00B31), SPH_C32(0x68581511),
17  	SPH_C32(0x64F98FA7), SPH_C32(0xBEFA4FA4)
18  };
19  static const sph_u32 H256[8] = {
20  	SPH_C32(0x6A09E667), SPH_C32(0xBB67AE85), SPH_C32(0x3C6EF372),
21  	SPH_C32(0xA54FF53A), SPH_C32(0x510E527F), SPH_C32(0x9B05688C),
22  	SPH_C32(0x1F83D9AB), SPH_C32(0x5BE0CD19)
23  };
24  #if SPH_SMALL_FOOTPRINT_SHA2
25  #define SHA2_MEXP1(in, pc)   do { \
26  		W[pc] = in(pc); \
27  	} while (0)
28  #define SHA2_MEXP2(in, pc)   do { \
29  		W[(pc) & 0x0F] = SPH_T32(SSG2_1(W[((pc) - 2) & 0x0F]) \
30  			+ W[((pc) - 7) & 0x0F] \
31  			+ SSG2_0(W[((pc) - 15) & 0x0F]) + W[(pc) & 0x0F]); \
32  	} while (0)
33  #define SHA2_STEPn(n, a, b, c, d, e, f, g, h, in, pc)   do { \
34  		sph_u32 t1, t2; \
35  		SHA2_MEXP ## n(in, pc); \
36  		t1 = SPH_T32(h + BSG2_1(e) + CH(e, f, g) \
37  			+ K[pcount + (pc)] + W[(pc) & 0x0F]); \
38  		t2 = SPH_T32(BSG2_0(a) + MAJ(a, b, c)); \
39        Y_xor_Z = X_xor_Y; \
40  		d = SPH_T32(d + t1); \
41  		h = SPH_T32(t1 + t2); \
42  	} while (0)
43  #define SHA2_STEP1(a, b, c, d, e, f, g, h, in, pc) \
44  	SHA2_STEPn(1, a, b, c, d, e, f, g, h, in, pc)
45  #define SHA2_STEP2(a, b, c, d, e, f, g, h, in, pc) \
46  	SHA2_STEPn(2, a, b, c, d, e, f, g, h, in, pc)
47  #define SHA2_ROUND_BODY(in, r)   do { \
48  		sph_u32 A, B, C, D, E, F, G, H, X_xor_Y, Y_xor_Z; \
49  		sph_u32 W[16]; \
50  		unsigned pcount; \
51   \
52  		A = (r)[0]; \
53  		B = (r)[1]; \
54  		C = (r)[2]; \
55  		D = (r)[3]; \
56  		E = (r)[4]; \
57  		F = (r)[5]; \
58  		G = (r)[6]; \
59  		H = (r)[7]; \
60  		pcount = 0; \
61        Y_xor_Z = B ^ C; \
62  		SHA2_STEP1(A, B, C, D, E, F, G, H, in,  0); \
63  		SHA2_STEP1(H, A, B, C, D, E, F, G, in,  1); \
64  		SHA2_STEP1(G, H, A, B, C, D, E, F, in,  2); \
65  		SHA2_STEP1(F, G, H, A, B, C, D, E, in,  3); \
66  		SHA2_STEP1(E, F, G, H, A, B, C, D, in,  4); \
67  		SHA2_STEP1(D, E, F, G, H, A, B, C, in,  5); \
68  		SHA2_STEP1(C, D, E, F, G, H, A, B, in,  6); \
69  		SHA2_STEP1(B, C, D, E, F, G, H, A, in,  7); \
70  		SHA2_STEP1(A, B, C, D, E, F, G, H, in,  8); \
71  		SHA2_STEP1(H, A, B, C, D, E, F, G, in,  9); \
72  		SHA2_STEP1(G, H, A, B, C, D, E, F, in, 10); \
73  		SHA2_STEP1(F, G, H, A, B, C, D, E, in, 11); \
74  		SHA2_STEP1(E, F, G, H, A, B, C, D, in, 12); \
75  		SHA2_STEP1(D, E, F, G, H, A, B, C, in, 13); \
76  		SHA2_STEP1(C, D, E, F, G, H, A, B, in, 14); \
77  		SHA2_STEP1(B, C, D, E, F, G, H, A, in, 15); \
78  		for (pcount = 16; pcount < 64; pcount += 16) { \
79  			SHA2_STEP2(A, B, C, D, E, F, G, H, in,  0); \
80  			SHA2_STEP2(H, A, B, C, D, E, F, G, in,  1); \
81  			SHA2_STEP2(G, H, A, B, C, D, E, F, in,  2); \
82  			SHA2_STEP2(F, G, H, A, B, C, D, E, in,  3); \
83  			SHA2_STEP2(E, F, G, H, A, B, C, D, in,  4); \
84  			SHA2_STEP2(D, E, F, G, H, A, B, C, in,  5); \
85  			SHA2_STEP2(C, D, E, F, G, H, A, B, in,  6); \
86  			SHA2_STEP2(B, C, D, E, F, G, H, A, in,  7); \
87  			SHA2_STEP2(A, B, C, D, E, F, G, H, in,  8); \
88  			SHA2_STEP2(H, A, B, C, D, E, F, G, in,  9); \
89  			SHA2_STEP2(G, H, A, B, C, D, E, F, in, 10); \
90  			SHA2_STEP2(F, G, H, A, B, C, D, E, in, 11); \
91  			SHA2_STEP2(E, F, G, H, A, B, C, D, in, 12); \
92  			SHA2_STEP2(D, E, F, G, H, A, B, C, in, 13); \
93  			SHA2_STEP2(C, D, E, F, G, H, A, B, in, 14); \
94  			SHA2_STEP2(B, C, D, E, F, G, H, A, in, 15); \
95  		} \
96  		(r)[0] = SPH_T32((r)[0] + A); \
97  		(r)[1] = SPH_T32((r)[1] + B); \
98  		(r)[2] = SPH_T32((r)[2] + C); \
99  		(r)[3] = SPH_T32((r)[3] + D); \
100  		(r)[4] = SPH_T32((r)[4] + E); \
101  		(r)[5] = SPH_T32((r)[5] + F); \
102  		(r)[6] = SPH_T32((r)[6] + G); \
103  		(r)[7] = SPH_T32((r)[7] + H); \
104  	} while (0)
105  #else  
106  #define SHA2_ROUND_BODY(in, r)   do { \
107  		sph_u32 A, B, C, D, E, F, G, H, T1, T2, X_xor_Y, Y_xor_Z;; \
108  		sph_u32 W00, W01, W02, W03, W04, W05, W06, W07; \
109  		sph_u32 W08, W09, W10, W11, W12, W13, W14, W15; \
110   \
111  		A = (r)[0]; \
112  		B = (r)[1]; \
113  		C = (r)[2]; \
114  		D = (r)[3]; \
115  		E = (r)[4]; \
116  		F = (r)[5]; \
117  		G = (r)[6]; \
118  		H = (r)[7]; \
119        Y_xor_Z = B ^ C; \
120  		W00 = in(0); \
121  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
122  			+ SPH_C32(0x428A2F98) + W00); \
123  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
124        Y_xor_Z = X_xor_Y; \
125  		D = SPH_T32(D + T1); \
126  		H = SPH_T32(T1 + T2); \
127  		W01 = in(1); \
128  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
129  			+ SPH_C32(0x71374491) + W01); \
130  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
131        Y_xor_Z = X_xor_Y; \
132  		C = SPH_T32(C + T1); \
133  		G = SPH_T32(T1 + T2); \
134  		W02 = in(2); \
135  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
136  			+ SPH_C32(0xB5C0FBCF) + W02); \
137  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
138        Y_xor_Z = X_xor_Y; \
139  		B = SPH_T32(B + T1); \
140  		F = SPH_T32(T1 + T2); \
141  		W03 = in(3); \
142  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
143  			+ SPH_C32(0xE9B5DBA5) + W03); \
144  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
145        Y_xor_Z = X_xor_Y; \
146  		A = SPH_T32(A + T1); \
147  		E = SPH_T32(T1 + T2); \
148  		W04 = in(4); \
149  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
150  			+ SPH_C32(0x3956C25B) + W04); \
151  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
152        Y_xor_Z = X_xor_Y; \
153  		H = SPH_T32(H + T1); \
154  		D = SPH_T32(T1 + T2); \
155  		W05 = in(5); \
156  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
157  			+ SPH_C32(0x59F111F1) + W05); \
158  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
159        Y_xor_Z = X_xor_Y; \
160  		G = SPH_T32(G + T1); \
161  		C = SPH_T32(T1 + T2); \
162  		W06 = in(6); \
163  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
164  			+ SPH_C32(0x923F82A4) + W06); \
165  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
166        Y_xor_Z = X_xor_Y; \
167  		F = SPH_T32(F + T1); \
168  		B = SPH_T32(T1 + T2); \
169  		W07 = in(7); \
170  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
171  			+ SPH_C32(0xAB1C5ED5) + W07); \
172  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
173        Y_xor_Z = X_xor_Y; \
174  		E = SPH_T32(E + T1); \
175  		A = SPH_T32(T1 + T2); \
176  		W08 = in(8); \
177  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
178  			+ SPH_C32(0xD807AA98) + W08); \
179  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
180        Y_xor_Z = X_xor_Y; \
181  		D = SPH_T32(D + T1); \
182  		H = SPH_T32(T1 + T2); \
183  		W09 = in(9); \
184  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
185  			+ SPH_C32(0x12835B01) + W09); \
186  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
187        Y_xor_Z = X_xor_Y; \
188  		C = SPH_T32(C + T1); \
189  		G = SPH_T32(T1 + T2); \
190  		W10 = in(10); \
191  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
192  			+ SPH_C32(0x243185BE) + W10); \
193  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
194        Y_xor_Z = X_xor_Y; \
195  		B = SPH_T32(B + T1); \
196  		F = SPH_T32(T1 + T2); \
197  		W11 = in(11); \
198  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
199  			+ SPH_C32(0x550C7DC3) + W11); \
200  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
201        Y_xor_Z = X_xor_Y; \
202  		A = SPH_T32(A + T1); \
203  		E = SPH_T32(T1 + T2); \
204  		W12 = in(12); \
205  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
206  			+ SPH_C32(0x72BE5D74) + W12); \
207  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
208        Y_xor_Z = X_xor_Y; \
209  		H = SPH_T32(H + T1); \
210  		D = SPH_T32(T1 + T2); \
211  		W13 = in(13); \
212  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
213  			+ SPH_C32(0x80DEB1FE) + W13); \
214  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
215        Y_xor_Z = X_xor_Y; \
216  		G = SPH_T32(G + T1); \
217  		C = SPH_T32(T1 + T2); \
218  		W14 = in(14); \
219  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
220  			+ SPH_C32(0x9BDC06A7) + W14); \
221  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
222        Y_xor_Z = X_xor_Y; \
223  		F = SPH_T32(F + T1); \
224  		B = SPH_T32(T1 + T2); \
225  		W15 = in(15); \
226  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
227  			+ SPH_C32(0xC19BF174) + W15); \
228  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
229        Y_xor_Z = X_xor_Y; \
230  		E = SPH_T32(E + T1); \
231  		A = SPH_T32(T1 + T2); \
232  		W00 = SPH_T32(SSG2_1(W14) + W09 + SSG2_0(W01) + W00); \
233  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
234  			+ SPH_C32(0xE49B69C1) + W00); \
235  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
236        Y_xor_Z = X_xor_Y; \
237  		D = SPH_T32(D + T1); \
238  		H = SPH_T32(T1 + T2); \
239  		W01 = SPH_T32(SSG2_1(W15) + W10 + SSG2_0(W02) + W01); \
240  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
241  			+ SPH_C32(0xEFBE4786) + W01); \
242  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
243        Y_xor_Z = X_xor_Y; \
244  		C = SPH_T32(C + T1); \
245  		G = SPH_T32(T1 + T2); \
246  		W02 = SPH_T32(SSG2_1(W00) + W11 + SSG2_0(W03) + W02); \
247  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
248  			+ SPH_C32(0x0FC19DC6) + W02); \
249  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
250        Y_xor_Z = X_xor_Y; \
251  		B = SPH_T32(B + T1); \
252  		F = SPH_T32(T1 + T2); \
253  		W03 = SPH_T32(SSG2_1(W01) + W12 + SSG2_0(W04) + W03); \
254  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
255  			+ SPH_C32(0x240CA1CC) + W03); \
256  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
257        Y_xor_Z = X_xor_Y; \
258  		A = SPH_T32(A + T1); \
259  		E = SPH_T32(T1 + T2); \
260  		W04 = SPH_T32(SSG2_1(W02) + W13 + SSG2_0(W05) + W04); \
261  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
262  			+ SPH_C32(0x2DE92C6F) + W04); \
263  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
264        Y_xor_Z = X_xor_Y; \
265  		H = SPH_T32(H + T1); \
266  		D = SPH_T32(T1 + T2); \
267  		W05 = SPH_T32(SSG2_1(W03) + W14 + SSG2_0(W06) + W05); \
268  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
269  			+ SPH_C32(0x4A7484AA) + W05); \
270  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
271        Y_xor_Z = X_xor_Y; \
272  		G = SPH_T32(G + T1); \
273  		C = SPH_T32(T1 + T2); \
274  		W06 = SPH_T32(SSG2_1(W04) + W15 + SSG2_0(W07) + W06); \
275  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
276  			+ SPH_C32(0x5CB0A9DC) + W06); \
277  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
278        Y_xor_Z = X_xor_Y; \
279  		F = SPH_T32(F + T1); \
280  		B = SPH_T32(T1 + T2); \
281  		W07 = SPH_T32(SSG2_1(W05) + W00 + SSG2_0(W08) + W07); \
282  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
283  			+ SPH_C32(0x76F988DA) + W07); \
284  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
285        Y_xor_Z = X_xor_Y; \
286  		E = SPH_T32(E + T1); \
287  		A = SPH_T32(T1 + T2); \
288  		W08 = SPH_T32(SSG2_1(W06) + W01 + SSG2_0(W09) + W08); \
289  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
290  			+ SPH_C32(0x983E5152) + W08); \
291  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
292        Y_xor_Z = X_xor_Y; \
293  		D = SPH_T32(D + T1); \
294  		H = SPH_T32(T1 + T2); \
295  		W09 = SPH_T32(SSG2_1(W07) + W02 + SSG2_0(W10) + W09); \
296  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
297  			+ SPH_C32(0xA831C66D) + W09); \
298  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
299        Y_xor_Z = X_xor_Y; \
300  		C = SPH_T32(C + T1); \
301  		G = SPH_T32(T1 + T2); \
302  		W10 = SPH_T32(SSG2_1(W08) + W03 + SSG2_0(W11) + W10); \
303  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
304  			+ SPH_C32(0xB00327C8) + W10); \
305  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
306        Y_xor_Z = X_xor_Y; \
307  		B = SPH_T32(B + T1); \
308  		F = SPH_T32(T1 + T2); \
309  		W11 = SPH_T32(SSG2_1(W09) + W04 + SSG2_0(W12) + W11); \
310  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
311  			+ SPH_C32(0xBF597FC7) + W11); \
312  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
313        Y_xor_Z = X_xor_Y; \
314  		A = SPH_T32(A + T1); \
315  		E = SPH_T32(T1 + T2); \
316  		W12 = SPH_T32(SSG2_1(W10) + W05 + SSG2_0(W13) + W12); \
317  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
318  			+ SPH_C32(0xC6E00BF3) + W12); \
319  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
320        Y_xor_Z = X_xor_Y; \
321  		H = SPH_T32(H + T1); \
322  		D = SPH_T32(T1 + T2); \
323  		W13 = SPH_T32(SSG2_1(W11) + W06 + SSG2_0(W14) + W13); \
324  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
325  			+ SPH_C32(0xD5A79147) + W13); \
326  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
327        Y_xor_Z = X_xor_Y; \
328  		G = SPH_T32(G + T1); \
329  		C = SPH_T32(T1 + T2); \
330  		W14 = SPH_T32(SSG2_1(W12) + W07 + SSG2_0(W15) + W14); \
331  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
332  			+ SPH_C32(0x06CA6351) + W14); \
333  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
334        Y_xor_Z = X_xor_Y; \
335  		F = SPH_T32(F + T1); \
336  		B = SPH_T32(T1 + T2); \
337  		W15 = SPH_T32(SSG2_1(W13) + W08 + SSG2_0(W00) + W15); \
338  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
339  			+ SPH_C32(0x14292967) + W15); \
340  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
341        Y_xor_Z = X_xor_Y; \
342  		E = SPH_T32(E + T1); \
343  		A = SPH_T32(T1 + T2); \
344  		W00 = SPH_T32(SSG2_1(W14) + W09 + SSG2_0(W01) + W00); \
345  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
346  			+ SPH_C32(0x27B70A85) + W00); \
347  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
348        Y_xor_Z = X_xor_Y; \
349  		D = SPH_T32(D + T1); \
350  		H = SPH_T32(T1 + T2); \
351  		W01 = SPH_T32(SSG2_1(W15) + W10 + SSG2_0(W02) + W01); \
352  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
353  			+ SPH_C32(0x2E1B2138) + W01); \
354  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
355        Y_xor_Z = X_xor_Y; \
356  		C = SPH_T32(C + T1); \
357  		G = SPH_T32(T1 + T2); \
358  		W02 = SPH_T32(SSG2_1(W00) + W11 + SSG2_0(W03) + W02); \
359  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
360  			+ SPH_C32(0x4D2C6DFC) + W02); \
361  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
362        Y_xor_Z = X_xor_Y; \
363  		B = SPH_T32(B + T1); \
364  		F = SPH_T32(T1 + T2); \
365  		W03 = SPH_T32(SSG2_1(W01) + W12 + SSG2_0(W04) + W03); \
366  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
367  			+ SPH_C32(0x53380D13) + W03); \
368  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
369        Y_xor_Z = X_xor_Y; \
370  		A = SPH_T32(A + T1); \
371  		E = SPH_T32(T1 + T2); \
372  		W04 = SPH_T32(SSG2_1(W02) + W13 + SSG2_0(W05) + W04); \
373  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
374  			+ SPH_C32(0x650A7354) + W04); \
375  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
376        Y_xor_Z = X_xor_Y; \
377  		H = SPH_T32(H + T1); \
378  		D = SPH_T32(T1 + T2); \
379  		W05 = SPH_T32(SSG2_1(W03) + W14 + SSG2_0(W06) + W05); \
380  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
381  			+ SPH_C32(0x766A0ABB) + W05); \
382  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
383        Y_xor_Z = X_xor_Y; \
384  		G = SPH_T32(G + T1); \
385  		C = SPH_T32(T1 + T2); \
386  		W06 = SPH_T32(SSG2_1(W04) + W15 + SSG2_0(W07) + W06); \
387  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
388  			+ SPH_C32(0x81C2C92E) + W06); \
389  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
390        Y_xor_Z = X_xor_Y; \
391  		F = SPH_T32(F + T1); \
392  		B = SPH_T32(T1 + T2); \
393  		W07 = SPH_T32(SSG2_1(W05) + W00 + SSG2_0(W08) + W07); \
394  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
395  			+ SPH_C32(0x92722C85) + W07); \
396  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
397        Y_xor_Z = X_xor_Y; \
398  		E = SPH_T32(E + T1); \
399  		A = SPH_T32(T1 + T2); \
400  		W08 = SPH_T32(SSG2_1(W06) + W01 + SSG2_0(W09) + W08); \
401  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
402  			+ SPH_C32(0xA2BFE8A1) + W08); \
403  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
404        Y_xor_Z = X_xor_Y; \
405  		D = SPH_T32(D + T1); \
406  		H = SPH_T32(T1 + T2); \
407  		W09 = SPH_T32(SSG2_1(W07) + W02 + SSG2_0(W10) + W09); \
408  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
409  			+ SPH_C32(0xA81A664B) + W09); \
410  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
411        Y_xor_Z = X_xor_Y; \
412  		C = SPH_T32(C + T1); \
413  		G = SPH_T32(T1 + T2); \
414  		W10 = SPH_T32(SSG2_1(W08) + W03 + SSG2_0(W11) + W10); \
415  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
416  			+ SPH_C32(0xC24B8B70) + W10); \
417  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
418        Y_xor_Z = X_xor_Y; \
419  		B = SPH_T32(B + T1); \
420  		F = SPH_T32(T1 + T2); \
421  		W11 = SPH_T32(SSG2_1(W09) + W04 + SSG2_0(W12) + W11); \
422  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
423  			+ SPH_C32(0xC76C51A3) + W11); \
424  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
425        Y_xor_Z = X_xor_Y; \
426  		A = SPH_T32(A + T1); \
427  		E = SPH_T32(T1 + T2); \
428  		W12 = SPH_T32(SSG2_1(W10) + W05 + SSG2_0(W13) + W12); \
429  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
430  			+ SPH_C32(0xD192E819) + W12); \
431  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
432        Y_xor_Z = X_xor_Y; \
433  		H = SPH_T32(H + T1); \
434  		D = SPH_T32(T1 + T2); \
435  		W13 = SPH_T32(SSG2_1(W11) + W06 + SSG2_0(W14) + W13); \
436  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
437  			+ SPH_C32(0xD6990624) + W13); \
438  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
439        Y_xor_Z = X_xor_Y; \
440  		G = SPH_T32(G + T1); \
441  		C = SPH_T32(T1 + T2); \
442  		W14 = SPH_T32(SSG2_1(W12) + W07 + SSG2_0(W15) + W14); \
443  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
444  			+ SPH_C32(0xF40E3585) + W14); \
445  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
446        Y_xor_Z = X_xor_Y; \
447  		F = SPH_T32(F + T1); \
448  		B = SPH_T32(T1 + T2); \
449  		W15 = SPH_T32(SSG2_1(W13) + W08 + SSG2_0(W00) + W15); \
450  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
451  			+ SPH_C32(0x106AA070) + W15); \
452  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
453        Y_xor_Z = X_xor_Y; \
454  		E = SPH_T32(E + T1); \
455  		A = SPH_T32(T1 + T2); \
456  		W00 = SPH_T32(SSG2_1(W14) + W09 + SSG2_0(W01) + W00); \
457  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
458  			+ SPH_C32(0x19A4C116) + W00); \
459  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
460        Y_xor_Z = X_xor_Y; \
461  		D = SPH_T32(D + T1); \
462  		H = SPH_T32(T1 + T2); \
463  		W01 = SPH_T32(SSG2_1(W15) + W10 + SSG2_0(W02) + W01); \
464  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
465  			+ SPH_C32(0x1E376C08) + W01); \
466  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
467        Y_xor_Z = X_xor_Y; \
468  		C = SPH_T32(C + T1); \
469  		G = SPH_T32(T1 + T2); \
470  		W02 = SPH_T32(SSG2_1(W00) + W11 + SSG2_0(W03) + W02); \
471  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
472  			+ SPH_C32(0x2748774C) + W02); \
473  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
474        Y_xor_Z = X_xor_Y; \
475  		B = SPH_T32(B + T1); \
476  		F = SPH_T32(T1 + T2); \
477  		W03 = SPH_T32(SSG2_1(W01) + W12 + SSG2_0(W04) + W03); \
478  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
479  			+ SPH_C32(0x34B0BCB5) + W03); \
480  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
481        Y_xor_Z = X_xor_Y; \
482  		A = SPH_T32(A + T1); \
483  		E = SPH_T32(T1 + T2); \
484  		W04 = SPH_T32(SSG2_1(W02) + W13 + SSG2_0(W05) + W04); \
485  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
486  			+ SPH_C32(0x391C0CB3) + W04); \
487  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
488        Y_xor_Z = X_xor_Y; \
489  		H = SPH_T32(H + T1); \
490  		D = SPH_T32(T1 + T2); \
491  		W05 = SPH_T32(SSG2_1(W03) + W14 + SSG2_0(W06) + W05); \
492  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
493  			+ SPH_C32(0x4ED8AA4A) + W05); \
494  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
495        Y_xor_Z = X_xor_Y; \
496  		G = SPH_T32(G + T1); \
497  		C = SPH_T32(T1 + T2); \
498  		W06 = SPH_T32(SSG2_1(W04) + W15 + SSG2_0(W07) + W06); \
499  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
500  			+ SPH_C32(0x5B9CCA4F) + W06); \
501  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
502        Y_xor_Z = X_xor_Y; \
503  		F = SPH_T32(F + T1); \
504  		B = SPH_T32(T1 + T2); \
505  		W07 = SPH_T32(SSG2_1(W05) + W00 + SSG2_0(W08) + W07); \
506  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
507  			+ SPH_C32(0x682E6FF3) + W07); \
508  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
509        Y_xor_Z = X_xor_Y; \
510  		E = SPH_T32(E + T1); \
511  		A = SPH_T32(T1 + T2); \
512  		W08 = SPH_T32(SSG2_1(W06) + W01 + SSG2_0(W09) + W08); \
513  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
514  			+ SPH_C32(0x748F82EE) + W08); \
515  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
516        Y_xor_Z = X_xor_Y; \
517  		D = SPH_T32(D + T1); \
518  		H = SPH_T32(T1 + T2); \
519  		W09 = SPH_T32(SSG2_1(W07) + W02 + SSG2_0(W10) + W09); \
520  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
521  			+ SPH_C32(0x78A5636F) + W09); \
522  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
523        Y_xor_Z = X_xor_Y; \
524  		C = SPH_T32(C + T1); \
525  		G = SPH_T32(T1 + T2); \
526  		W10 = SPH_T32(SSG2_1(W08) + W03 + SSG2_0(W11) + W10); \
527  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
528  			+ SPH_C32(0x84C87814) + W10); \
529  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
530        Y_xor_Z = X_xor_Y; \
531  		B = SPH_T32(B + T1); \
532  		F = SPH_T32(T1 + T2); \
533  		W11 = SPH_T32(SSG2_1(W09) + W04 + SSG2_0(W12) + W11); \
534  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
535  			+ SPH_C32(0x8CC70208) + W11); \
536  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
537        Y_xor_Z = X_xor_Y; \
538  		A = SPH_T32(A + T1); \
539  		E = SPH_T32(T1 + T2); \
540  		W12 = SPH_T32(SSG2_1(W10) + W05 + SSG2_0(W13) + W12); \
541  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
542  			+ SPH_C32(0x90BEFFFA) + W12); \
543  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
544        Y_xor_Z = X_xor_Y; \
545  		H = SPH_T32(H + T1); \
546  		D = SPH_T32(T1 + T2); \
547  		W13 = SPH_T32(SSG2_1(W11) + W06 + SSG2_0(W14) + W13); \
548  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
549  			+ SPH_C32(0xA4506CEB) + W13); \
550  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
551        Y_xor_Z = X_xor_Y; \
552  		G = SPH_T32(G + T1); \
553  		C = SPH_T32(T1 + T2); \
554  		W14 = SPH_T32(SSG2_1(W12) + W07 + SSG2_0(W15) + W14); \
555  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
556  			+ SPH_C32(0xBEF9A3F7) + W14); \
557  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
558        Y_xor_Z = X_xor_Y; \
559  		F = SPH_T32(F + T1); \
560  		B = SPH_T32(T1 + T2); \
561  		W15 = SPH_T32(SSG2_1(W13) + W08 + SSG2_0(W00) + W15); \
562  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
563  			+ SPH_C32(0xC67178F2) + W15); \
564  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
565        Y_xor_Z = X_xor_Y; \
566  		E = SPH_T32(E + T1); \
567  		A = SPH_T32(T1 + T2); \
568  		(r)[0] = SPH_T32((r)[0] + A); \
569  		(r)[1] = SPH_T32((r)[1] + B); \
570  		(r)[2] = SPH_T32((r)[2] + C); \
571  		(r)[3] = SPH_T32((r)[3] + D); \
572  		(r)[4] = SPH_T32((r)[4] + E); \
573  		(r)[5] = SPH_T32((r)[5] + F); \
574  		(r)[6] = SPH_T32((r)[6] + G); \
575  		(r)[7] = SPH_T32((r)[7] + H); \
576  	} while (0)
577  #endif  
578  static void
579  sha2_round(const unsigned char *data, sph_u32 r[8])
</span>580  {
581  #define SHA2_IN(x)   sph_dec32be_aligned(data + (4 * (x)))
582  	SHA2_ROUND_BODY(SHA2_IN, r);
583  #undef SHA2_IN
584  }
585  void sph_sha256_transform_le( uint32_t *state_out, const uint32_t *data,
586                                const uint32_t *state_in )
587  {
588  memcpy( state_out, state_in, 32 );
589  #define SHA2_IN(x)   (data[x])
590     SHA2_ROUND_BODY( SHA2_IN, state_out );
591  #undef SHA2_IN
592  }
593  void sph_sha256_transform_be( uint32_t *state_out, const uint32_t *data,
594                                const uint32_t *state_in )
595  {
596  memcpy( state_out, state_in, 32 );
597  #define SHA2_IN(x)   sph_dec32be_aligned( data+(x) )
598     SHA2_ROUND_BODY( SHA2_IN, state_out );
599  #undef SHA2_IN
600  }
601  void
602  sph_sha224_init(void *cc)
603  {
604  	sph_sha224_context *sc;
605  	sc = cc;
606  	memcpy(sc->val, H224, sizeof H224);
607  #if SPH_64
608  	sc->count = 0;
609  #else
610  	sc->count_high = sc->count_low = 0;
611  #endif
612  }
613  void
614  sph_sha256_init(void *cc)
615  {
616  	sph_sha256_context *sc;
617  	sc = cc;
618  	memcpy(sc->val, H256, sizeof H256);
619  #if SPH_64
620  	sc->count = 0;
621  #else
622  	sc->count_high = sc->count_low = 0;
623  #endif
624  }
625  #define RFUN   sha2_round
626  #define HASH   sha224
627  #define BE32   1
628  #include "md_helper.c"
629  void
630  sph_sha224_close(void *cc, void *dst)
631  {
632  	sha224_close(cc, dst, 7);
633  }
634  void
635  sph_sha224_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
636  {
637  	sha224_addbits_and_close(cc, ub, n, dst, 7);
638  }
639  void
640  sph_sha256_close(void *cc, void *dst)
641  {
642  	sha224_close(cc, dst, 8);
643  }
644  void
645  sph_sha256_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
646  {
647  	sha224_addbits_and_close(cc, ub, n, dst, 8);
648  }
649  void sph_sha256_full( void *dst, const void *data, size_t len )
650  {
651     sph_sha256_context cc;
652     sph_sha256_init( &cc );
653     sph_sha256( &cc, data, len );
654     sph_sha256_close( &cc, dst );
655  }
656  void sha256d(void* hash, const void* data, int len)
657  {
658      sph_sha256_full(hash, data, len);
659      sph_sha256_full(hash, hash, 32);
660  }
</code></pre>
        </div>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_skein.c</h3>
            <pre><code>1  #include <stddef.h>
2  #include <string.h>
3  #include "sph_skein.h"
4  #ifdef __cplusplus
5  extern "C"{
6  #endif
7  #if SPH_SMALL_FOOTPRINT && !defined SPH_SMALL_FOOTPRINT_SKEIN
8  #define SPH_SMALL_FOOTPRINT_SKEIN   1
9  #endif
10  #ifdef _MSC_VER
11  #pragma warning (disable: 4146)
12  #endif
13  #if SPH_64
14  #if 0
15  #define M5_0_0    0
16  #define M5_0_1    1
17  #define M5_0_2    2
18  #define M5_0_3    3
19  #define M5_1_0    1
20  #define M5_1_1    2
21  #define M5_1_2    3
22  #define M5_1_3    4
23  #define M5_2_0    2
24  #define M5_2_1    3
25  #define M5_2_2    4
26  #define M5_2_3    0
27  #define M5_3_0    3
28  #define M5_3_1    4
29  #define M5_3_2    0
30  #define M5_3_3    1
31  #define M5_4_0    4
32  #define M5_4_1    0
33  #define M5_4_2    1
34  #define M5_4_3    2
35  #define M5_5_0    0
36  #define M5_5_1    1
37  #define M5_5_2    2
38  #define M5_5_3    3
39  #define M5_6_0    1
40  #define M5_6_1    2
41  #define M5_6_2    3
42  #define M5_6_3    4
43  #define M5_7_0    2
44  #define M5_7_1    3
45  #define M5_7_2    4
46  #define M5_7_3    0
47  #define M5_8_0    3
48  #define M5_8_1    4
49  #define M5_8_2    0
50  #define M5_8_3    1
51  #define M5_9_0    4
52  #define M5_9_1    0
53  #define M5_9_2    1
54  #define M5_9_3    2
55  #define M5_10_0   0
56  #define M5_10_1   1
57  #define M5_10_2   2
58  #define M5_10_3   3
59  #define M5_11_0   1
60  #define M5_11_1   2
61  #define M5_11_2   3
62  #define M5_11_3   4
63  #define M5_12_0   2
64  #define M5_12_1   3
65  #define M5_12_2   4
66  #define M5_12_3   0
67  #define M5_13_0   3
68  #define M5_13_1   4
69  #define M5_13_2   0
70  #define M5_13_3   1
71  #define M5_14_0   4
72  #define M5_14_1   0
73  #define M5_14_2   1
74  #define M5_14_3   2
75  #define M5_15_0   0
76  #define M5_15_1   1
77  #define M5_15_2   2
78  #define M5_15_3   3
79  #define M5_16_0   1
80  #define M5_16_1   2
81  #define M5_16_2   3
82  #define M5_16_3   4
83  #define M5_17_0   2
84  #define M5_17_1   3
85  #define M5_17_2   4
86  #define M5_17_3   0
87  #define M5_18_0   3
88  #define M5_18_1   4
89  #define M5_18_2   0
90  #define M5_18_3   1
91  #endif
92  #define M9_0_0    0
93  #define M9_0_1    1
94  #define M9_0_2    2
95  #define M9_0_3    3
96  #define M9_0_4    4
97  #define M9_0_5    5
98  #define M9_0_6    6
99  #define M9_0_7    7
100  #define M9_1_0    1
101  #define M9_1_1    2
102  #define M9_1_2    3
103  #define M9_1_3    4
104  #define M9_1_4    5
105  #define M9_1_5    6
106  #define M9_1_6    7
107  #define M9_1_7    8
108  #define M9_2_0    2
109  #define M9_2_1    3
110  #define M9_2_2    4
111  #define M9_2_3    5
112  #define M9_2_4    6
113  #define M9_2_5    7
114  #define M9_2_6    8
115  #define M9_2_7    0
116  #define M9_3_0    3
117  #define M9_3_1    4
118  #define M9_3_2    5
119  #define M9_3_3    6
120  #define M9_3_4    7
121  #define M9_3_5    8
122  #define M9_3_6    0
123  #define M9_3_7    1
124  #define M9_4_0    4
125  #define M9_4_1    5
126  #define M9_4_2    6
127  #define M9_4_3    7
128  #define M9_4_4    8
129  #define M9_4_5    0
130  #define M9_4_6    1
131  #define M9_4_7    2
132  #define M9_5_0    5
133  #define M9_5_1    6
134  #define M9_5_2    7
135  #define M9_5_3    8
136  #define M9_5_4    0
137  #define M9_5_5    1
138  #define M9_5_6    2
139  #define M9_5_7    3
140  #define M9_6_0    6
141  #define M9_6_1    7
142  #define M9_6_2    8
143  #define M9_6_3    0
144  #define M9_6_4    1
145  #define M9_6_5    2
146  #define M9_6_6    3
147  #define M9_6_7    4
148  #define M9_7_0    7
149  #define M9_7_1    8
150  #define M9_7_2    0
151  #define M9_7_3    1
152  #define M9_7_4    2
153  #define M9_7_5    3
154  #define M9_7_6    4
155  #define M9_7_7    5
156  #define M9_8_0    8
157  #define M9_8_1    0
158  #define M9_8_2    1
159  #define M9_8_3    2
160  #define M9_8_4    3
161  #define M9_8_5    4
162  #define M9_8_6    5
163  #define M9_8_7    6
164  #define M9_9_0    0
165  #define M9_9_1    1
166  #define M9_9_2    2
167  #define M9_9_3    3
168  #define M9_9_4    4
169  #define M9_9_5    5
170  #define M9_9_6    6
171  #define M9_9_7    7
172  #define M9_10_0   1
173  #define M9_10_1   2
174  #define M9_10_2   3
175  #define M9_10_3   4
176  #define M9_10_4   5
177  #define M9_10_5   6
178  #define M9_10_6   7
179  #define M9_10_7   8
180  #define M9_11_0   2
181  #define M9_11_1   3
182  #define M9_11_2   4
183  #define M9_11_3   5
184  #define M9_11_4   6
185  #define M9_11_5   7
186  #define M9_11_6   8
187  #define M9_11_7   0
188  #define M9_12_0   3
189  #define M9_12_1   4
190  #define M9_12_2   5
191  #define M9_12_3   6
192  #define M9_12_4   7
193  #define M9_12_5   8
194  #define M9_12_6   0
195  #define M9_12_7   1
196  #define M9_13_0   4
197  #define M9_13_1   5
198  #define M9_13_2   6
199  #define M9_13_3   7
200  #define M9_13_4   8
201  #define M9_13_5   0
202  #define M9_13_6   1
203  #define M9_13_7   2
204  #define M9_14_0   5
205  #define M9_14_1   6
206  #define M9_14_2   7
207  #define M9_14_3   8
208  #define M9_14_4   0
209  #define M9_14_5   1
210  #define M9_14_6   2
211  #define M9_14_7   3
212  #define M9_15_0   6
213  #define M9_15_1   7
214  #define M9_15_2   8
215  #define M9_15_3   0
216  #define M9_15_4   1
217  #define M9_15_5   2
218  #define M9_15_6   3
219  #define M9_15_7   4
220  #define M9_16_0   7
221  #define M9_16_1   8
222  #define M9_16_2   0
223  #define M9_16_3   1
224  #define M9_16_4   2
225  #define M9_16_5   3
226  #define M9_16_6   4
227  #define M9_16_7   5
228  #define M9_17_0   8
229  #define M9_17_1   0
230  #define M9_17_2   1
231  #define M9_17_3   2
232  #define M9_17_4   3
233  #define M9_17_5   4
234  #define M9_17_6   5
235  #define M9_17_7   6
236  #define M9_18_0   0
237  #define M9_18_1   1
238  #define M9_18_2   2
239  #define M9_18_3   3
240  #define M9_18_4   4
241  #define M9_18_5   5
242  #define M9_18_6   6
243  #define M9_18_7   7
244  #define M3_0_0    0
245  #define M3_0_1    1
246  #define M3_1_0    1
247  #define M3_1_1    2
248  #define M3_2_0    2
249  #define M3_2_1    0
250  #define M3_3_0    0
251  #define M3_3_1    1
252  #define M3_4_0    1
253  #define M3_4_1    2
254  #define M3_5_0    2
255  #define M3_5_1    0
256  #define M3_6_0    0
257  #define M3_6_1    1
258  #define M3_7_0    1
259  #define M3_7_1    2
260  #define M3_8_0    2
261  #define M3_8_1    0
262  #define M3_9_0    0
263  #define M3_9_1    1
264  #define M3_10_0   1
265  #define M3_10_1   2
266  #define M3_11_0   2
267  #define M3_11_1   0
268  #define M3_12_0   0
269  #define M3_12_1   1
270  #define M3_13_0   1
271  #define M3_13_1   2
272  #define M3_14_0   2
273  #define M3_14_1   0
274  #define M3_15_0   0
275  #define M3_15_1   1
276  #define M3_16_0   1
277  #define M3_16_1   2
278  #define M3_17_0   2
279  #define M3_17_1   0
280  #define M3_18_0   0
281  #define M3_18_1   1
282  #define XCAT(x, y)     XCAT_(x, y)
283  #define XCAT_(x, y)    x ## y
284  #if 0
285  #define SKSI(k, s, i)   XCAT(k, XCAT(XCAT(XCAT(M5_, s), _), i))
286  #define SKST(t, s, v)   XCAT(t, XCAT(XCAT(XCAT(M3_, s), _), v))
287  #endif
288  #define SKBI(k, s, i)   XCAT(k, XCAT(XCAT(XCAT(M9_, s), _), i))
289  #define SKBT(t, s, v)   XCAT(t, XCAT(XCAT(XCAT(M3_, s), _), v))
290  #if 0
291  #define TFSMALL_KINIT(k0, k1, k2, k3, k4, t0, t1, t2)   do { \
292  		k4 = (k0 ^ k1) ^ (k2 ^ k3) ^ SPH_C64(0x1BD11BDAA9FC1A22); \
293  		t2 = t0 ^ t1; \
294  	} while (0)
295  #endif
296  #define TFBIG_KINIT(k0, k1, k2, k3, k4, k5, k6, k7, k8, t0, t1, t2)   do { \
297  		k8 = ((k0 ^ k1) ^ (k2 ^ k3)) ^ ((k4 ^ k5) ^ (k6 ^ k7)) \
298  			^ SPH_C64(0x1BD11BDAA9FC1A22); \
299  		t2 = t0 ^ t1; \
300  	} while (0)
301  #if 0
302  #define TFSMALL_ADDKEY(w0, w1, w2, w3, k, t, s)   do { \
303  		w0 = SPH_T64(w0 + SKSI(k, s, 0)); \
304  		w1 = SPH_T64(w1 + SKSI(k, s, 1) + SKST(t, s, 0)); \
305  		w2 = SPH_T64(w2 + SKSI(k, s, 2) + SKST(t, s, 1)); \
306  		w3 = SPH_T64(w3 + SKSI(k, s, 3) + (sph_u64)s); \
307  	} while (0)
308  #endif
309  #if SPH_SMALL_FOOTPRINT_SKEIN
310  #define TFBIG_ADDKEY(s, tt0, tt1)   do { \
311  		p0 = SPH_T64(p0 + h[s + 0]); \
312  		p1 = SPH_T64(p1 + h[s + 1]); \
313  		p2 = SPH_T64(p2 + h[s + 2]); \
314  		p3 = SPH_T64(p3 + h[s + 3]); \
315  		p4 = SPH_T64(p4 + h[s + 4]); \
316  		p5 = SPH_T64(p5 + h[s + 5] + tt0); \
317  		p6 = SPH_T64(p6 + h[s + 6] + tt1); \
318  		p7 = SPH_T64(p7 + h[s + 7] + (sph_u64)s); \
319  	} while (0)
320  #else
321  #define TFBIG_ADDKEY(w0, w1, w2, w3, w4, w5, w6, w7, k, t, s)   do { \
322  		w0 = SPH_T64(w0 + SKBI(k, s, 0)); \
323  		w1 = SPH_T64(w1 + SKBI(k, s, 1)); \
324  		w2 = SPH_T64(w2 + SKBI(k, s, 2)); \
325  		w3 = SPH_T64(w3 + SKBI(k, s, 3)); \
326  		w4 = SPH_T64(w4 + SKBI(k, s, 4)); \
327  		w5 = SPH_T64(w5 + SKBI(k, s, 5) + SKBT(t, s, 0)); \
328  		w6 = SPH_T64(w6 + SKBI(k, s, 6) + SKBT(t, s, 1)); \
329  		w7 = SPH_T64(w7 + SKBI(k, s, 7) + (sph_u64)s); \
330  	} while (0)
331  #endif
332  #if 0
333  #define TFSMALL_MIX(x0, x1, rc)   do { \
334  		x0 = SPH_T64(x0 + x1); \
335  		x1 = SPH_ROTL64(x1, rc) ^ x0; \
336  	} while (0)
337  #endif
338  #define TFBIG_MIX(x0, x1, rc)   do { \
339  		x0 = SPH_T64(x0 + x1); \
340  		x1 = SPH_ROTL64(x1, rc) ^ x0; \
341  	} while (0)
342  #if 0
343  #define TFSMALL_MIX4(w0, w1, w2, w3, rc0, rc1)  do { \
344  		TFSMALL_MIX(w0, w1, rc0); \
345  		TFSMALL_MIX(w2, w3, rc1); \
346  	} while (0)
347  #endif
348  #define TFBIG_MIX8(w0, w1, w2, w3, w4, w5, w6, w7, rc0, rc1, rc2, rc3)  do { \
349  		TFBIG_MIX(w0, w1, rc0); \
350  		TFBIG_MIX(w2, w3, rc1); \
351  		TFBIG_MIX(w4, w5, rc2); \
352  		TFBIG_MIX(w6, w7, rc3); \
353  	} while (0)
354  #if 0
355  #define TFSMALL_4e(s)   do { \
356  		TFSMALL_ADDKEY(p0, p1, p2, p3, h, t, s); \
357  		TFSMALL_MIX4(p0, p1, p2, p3, 14, 16); \
358  		TFSMALL_MIX4(p0, p3, p2, p1, 52, 57); \
359  		TFSMALL_MIX4(p0, p1, p2, p3, 23, 40); \
360  		TFSMALL_MIX4(p0, p3, p2, p1,  5, 37); \
361  	} while (0)
362  #define TFSMALL_4o(s)   do { \
363  		TFSMALL_ADDKEY(p0, p1, p2, p3, h, t, s); \
364  		TFSMALL_MIX4(p0, p1, p2, p3, 25, 33); \
365  		TFSMALL_MIX4(p0, p3, p2, p1, 46, 12); \
366  		TFSMALL_MIX4(p0, p1, p2, p3, 58, 22); \
367  		TFSMALL_MIX4(p0, p3, p2, p1, 32, 32); \
368  	} while (0)
369  #endif
370  #if SPH_SMALL_FOOTPRINT_SKEIN
371  #define TFBIG_4e(s)   do { \
372  		TFBIG_ADDKEY(s, t0, t1); \
373  		TFBIG_MIX8(p0, p1, p2, p3, p4, p5, p6, p7, 46, 36, 19, 37); \
374  		TFBIG_MIX8(p2, p1, p4, p7, p6, p5, p0, p3, 33, 27, 14, 42); \
375  		TFBIG_MIX8(p4, p1, p6, p3, p0, p5, p2, p7, 17, 49, 36, 39); \
376  		TFBIG_MIX8(p6, p1, p0, p7, p2, p5, p4, p3, 44,  9, 54, 56); \
377  	} while (0)
378  #define TFBIG_4o(s)   do { \
379  		TFBIG_ADDKEY(s, t1, t2); \
380  		TFBIG_MIX8(p0, p1, p2, p3, p4, p5, p6, p7, 39, 30, 34, 24); \
381  		TFBIG_MIX8(p2, p1, p4, p7, p6, p5, p0, p3, 13, 50, 10, 17); \
382  		TFBIG_MIX8(p4, p1, p6, p3, p0, p5, p2, p7, 25, 29, 39, 43); \
383  		TFBIG_MIX8(p6, p1, p0, p7, p2, p5, p4, p3,  8, 35, 56, 22); \
384  	} while (0)
385  #else
386  #define TFBIG_4e(s)   do { \
387  		TFBIG_ADDKEY(p0, p1, p2, p3, p4, p5, p6, p7, h, t, s); \
388  		TFBIG_MIX8(p0, p1, p2, p3, p4, p5, p6, p7, 46, 36, 19, 37); \
389  		TFBIG_MIX8(p2, p1, p4, p7, p6, p5, p0, p3, 33, 27, 14, 42); \
390  		TFBIG_MIX8(p4, p1, p6, p3, p0, p5, p2, p7, 17, 49, 36, 39); \
391  		TFBIG_MIX8(p6, p1, p0, p7, p2, p5, p4, p3, 44,  9, 54, 56); \
392  	} while (0)
393  #define TFBIG_4o(s)   do { \
394  		TFBIG_ADDKEY(p0, p1, p2, p3, p4, p5, p6, p7, h, t, s); \
395  		TFBIG_MIX8(p0, p1, p2, p3, p4, p5, p6, p7, 39, 30, 34, 24); \
396  		TFBIG_MIX8(p2, p1, p4, p7, p6, p5, p0, p3, 13, 50, 10, 17); \
397  		TFBIG_MIX8(p4, p1, p6, p3, p0, p5, p2, p7, 25, 29, 39, 43); \
398  		TFBIG_MIX8(p6, p1, p0, p7, p2, p5, p4, p3,  8, 35, 56, 22); \
399  	} while (0)
400  #endif
401  #if 0
402  #define UBI_SMALL(etype, extra)  do { \
403  		sph_u64 h4, t0, t1, t2; \
404  		sph_u64 m0 = sph_dec64le(buf +  0); \
405  		sph_u64 m1 = sph_dec64le(buf +  8); \
406  		sph_u64 m2 = sph_dec64le(buf + 16); \
407  		sph_u64 m3 = sph_dec64le(buf + 24); \
408  		sph_u64 p0 = m0; \
409  		sph_u64 p1 = m1; \
410  		sph_u64 p2 = m2; \
411  		sph_u64 p3 = m3; \
412  		t0 = SPH_T64(bcount << 5) + (sph_u64)(extra); \
413  		t1 = (bcount >> 59) + ((sph_u64)(etype) << 55); \
414  		TFSMALL_KINIT(h0, h1, h2, h3, h4, t0, t1, t2); \
415  		TFSMALL_4e(0); \
416  		TFSMALL_4o(1); \
417  		TFSMALL_4e(2); \
418  		TFSMALL_4o(3); \
419  		TFSMALL_4e(4); \
420  		TFSMALL_4o(5); \
421  		TFSMALL_4e(6); \
422  		TFSMALL_4o(7); \
423  		TFSMALL_4e(8); \
424  		TFSMALL_4o(9); \
425  		TFSMALL_4e(10); \
426  		TFSMALL_4o(11); \
427  		TFSMALL_4e(12); \
428  		TFSMALL_4o(13); \
429  		TFSMALL_4e(14); \
430  		TFSMALL_4o(15); \
431  		TFSMALL_4e(16); \
432  		TFSMALL_4o(17); \
433  		TFSMALL_ADDKEY(p0, p1, p2, p3, h, t, 18); \
434  		h0 = m0 ^ p0; \
435  		h1 = m1 ^ p1; \
436  		h2 = m2 ^ p2; \
437  		h3 = m3 ^ p3; \
438  	} while (0)
439  #endif
440  #if SPH_SMALL_FOOTPRINT_SKEIN
441  #define UBI_BIG(etype, extra)  do { \
442  		sph_u64 t0, t1, t2; \
443  		unsigned u; \
444  		sph_u64 m0 = sph_dec64le_aligned(buf +  0); \
445  		sph_u64 m1 = sph_dec64le_aligned(buf +  8); \
446  		sph_u64 m2 = sph_dec64le_aligned(buf + 16); \
447  		sph_u64 m3 = sph_dec64le_aligned(buf + 24); \
448  		sph_u64 m4 = sph_dec64le_aligned(buf + 32); \
449  		sph_u64 m5 = sph_dec64le_aligned(buf + 40); \
450  		sph_u64 m6 = sph_dec64le_aligned(buf + 48); \
451  		sph_u64 m7 = sph_dec64le_aligned(buf + 56); \
452  		sph_u64 p0 = m0; \
453  		sph_u64 p1 = m1; \
454  		sph_u64 p2 = m2; \
455  		sph_u64 p3 = m3; \
456  		sph_u64 p4 = m4; \
457  		sph_u64 p5 = m5; \
458  		sph_u64 p6 = m6; \
459  		sph_u64 p7 = m7; \
460  		t0 = SPH_T64(bcount << 6) + (sph_u64)(extra); \
461  		t1 = (bcount >> 58) + ((sph_u64)(etype) << 55); \
462  		TFBIG_KINIT(h[0], h[1], h[2], h[3], h[4], h[5], \
463  			h[6], h[7], h[8], t0, t1, t2); \
464  		for (u = 0; u <= 15; u += 3) { \
465  			h[u +  9] = h[u + 0]; \
466  			h[u + 10] = h[u + 1]; \
467  			h[u + 11] = h[u + 2]; \
468  		} \
469  		for (u = 0; u < 9; u ++) { \
470  			sph_u64 s = u << 1; \
471  			sph_u64 tmp; \
472  			TFBIG_4e(s); \
473  			TFBIG_4o(s + 1); \
474  			tmp = t2; \
475  			t2 = t1; \
476  			t1 = t0; \
477  			t0 = tmp; \
478  		} \
479  		TFBIG_ADDKEY(18, t0, t1); \
480  		h[0] = m0 ^ p0; \
481  		h[1] = m1 ^ p1; \
482  		h[2] = m2 ^ p2; \
483  		h[3] = m3 ^ p3; \
484  		h[4] = m4 ^ p4; \
485  		h[5] = m5 ^ p5; \
486  		h[6] = m6 ^ p6; \
487  		h[7] = m7 ^ p7; \
488  	} while (0)
489  #else
490  #define UBI_BIG(etype, extra)  do { \
491  		sph_u64 h8, t0, t1, t2; \
492  		sph_u64 m0 = sph_dec64le_aligned(buf +  0); \
493  		sph_u64 m1 = sph_dec64le_aligned(buf +  8); \
494  		sph_u64 m2 = sph_dec64le_aligned(buf + 16); \
495  		sph_u64 m3 = sph_dec64le_aligned(buf + 24); \
496  		sph_u64 m4 = sph_dec64le_aligned(buf + 32); \
497  		sph_u64 m5 = sph_dec64le_aligned(buf + 40); \
498  		sph_u64 m6 = sph_dec64le_aligned(buf + 48); \
499  		sph_u64 m7 = sph_dec64le_aligned(buf + 56); \
500  		sph_u64 p0 = m0; \
501  		sph_u64 p1 = m1; \
502  		sph_u64 p2 = m2; \
503  		sph_u64 p3 = m3; \
504  		sph_u64 p4 = m4; \
505  		sph_u64 p5 = m5; \
506  		sph_u64 p6 = m6; \
507  		sph_u64 p7 = m7; \
508  		t0 = SPH_T64(bcount << 6) + (sph_u64)(extra); \
509  		t1 = (bcount >> 58) + ((sph_u64)(etype) << 55); \
510  		TFBIG_KINIT(h0, h1, h2, h3, h4, h5, h6, h7, h8, t0, t1, t2); \
511  		TFBIG_4e(0); \
512  		TFBIG_4o(1); \
513  		TFBIG_4e(2); \
514  		TFBIG_4o(3); \
515  		TFBIG_4e(4); \
516  		TFBIG_4o(5); \
517  		TFBIG_4e(6); \
518  		TFBIG_4o(7); \
519  		TFBIG_4e(8); \
520  		TFBIG_4o(9); \
521  		TFBIG_4e(10); \
522  		TFBIG_4o(11); \
523  		TFBIG_4e(12); \
524  		TFBIG_4o(13); \
525  		TFBIG_4e(14); \
526  		TFBIG_4o(15); \
527  		TFBIG_4e(16); \
528  		TFBIG_4o(17); \
529  		TFBIG_ADDKEY(p0, p1, p2, p3, p4, p5, p6, p7, h, t, 18); \
530  		h0 = m0 ^ p0; \
531  		h1 = m1 ^ p1; \
532  		h2 = m2 ^ p2; \
533  		h3 = m3 ^ p3; \
534  		h4 = m4 ^ p4; \
535  		h5 = m5 ^ p5; \
536  		h6 = m6 ^ p6; \
537  		h7 = m7 ^ p7; \
538  	} while (0)
539  #endif
540  #if 0
541  #define DECL_STATE_SMALL \
542  	sph_u64 h0, h1, h2, h3; \
543  	sph_u64 bcount;
544  #define READ_STATE_SMALL(sc)   do { \
545  		h0 = (sc)->h0; \
546  		h1 = (sc)->h1; \
547  		h2 = (sc)->h2; \
548  		h3 = (sc)->h3; \
549  		bcount = sc->bcount; \
550  	} while (0)
551  #define WRITE_STATE_SMALL(sc)   do { \
552  		(sc)->h0 = h0; \
553  		(sc)->h1 = h1; \
554  		(sc)->h2 = h2; \
555  		(sc)->h3 = h3; \
556  		sc->bcount = bcount; \
557  	} while (0)
558  #endif
559  #if SPH_SMALL_FOOTPRINT_SKEIN
560  #define DECL_STATE_BIG \
561  	sph_u64 h[27]; \
562  	sph_u64 bcount;
563  #define READ_STATE_BIG(sc)   do { \
564  		h[0] = (sc)->h0; \
565  		h[1] = (sc)->h1; \
566  		h[2] = (sc)->h2; \
567  		h[3] = (sc)->h3; \
568  		h[4] = (sc)->h4; \
569  		h[5] = (sc)->h5; \
570  		h[6] = (sc)->h6; \
571  		h[7] = (sc)->h7; \
572  		bcount = sc->bcount; \
573  	} while (0)
574  #define WRITE_STATE_BIG(sc)   do { \
575  		(sc)->h0 = h[0]; \
576  		(sc)->h1 = h[1]; \
577  		(sc)->h2 = h[2]; \
578  		(sc)->h3 = h[3]; \
579  		(sc)->h4 = h[4]; \
580  		(sc)->h5 = h[5]; \
581  		(sc)->h6 = h[6]; \
582  		(sc)->h7 = h[7]; \
583  		sc->bcount = bcount; \
584  	} while (0)
585  #else
586  #define DECL_STATE_BIG \
587  	sph_u64 h0, h1, h2, h3, h4, h5, h6, h7; \
588  	sph_u64 bcount;
589  #define READ_STATE_BIG(sc)   do { \
590  		h0 = (sc)->h0; \
591  		h1 = (sc)->h1; \
592  		h2 = (sc)->h2; \
593  		h3 = (sc)->h3; \
594  		h4 = (sc)->h4; \
595  		h5 = (sc)->h5; \
596  		h6 = (sc)->h6; \
597  		h7 = (sc)->h7; \
598  		bcount = sc->bcount; \
599  	} while (0)
600  #define WRITE_STATE_BIG(sc)   do { \
601  		(sc)->h0 = h0; \
602  		(sc)->h1 = h1; \
603  		(sc)->h2 = h2; \
604  		(sc)->h3 = h3; \
605  		(sc)->h4 = h4; \
606  		(sc)->h5 = h5; \
607  		(sc)->h6 = h6; \
608  		(sc)->h7 = h7; \
609  		sc->bcount = bcount; \
610  	} while (0)
611  #endif
612  #if 0
613  static void
614  skein_small_init(sph_skein_small_context *sc, const sph_u64 *iv)
615  {
616  	sc->h0 = iv[0];
617  	sc->h1 = iv[1];
618  	sc->h2 = iv[2];
619  	sc->h3 = iv[3];
620  	sc->bcount = 0;
621  	sc->ptr = 0;
622  }
623  #endif
624  static void
625  skein_big_init(sph_skein_big_context *sc, const sph_u64 *iv)
626  {
627  	sc->h0 = iv[0];
628  	sc->h1 = iv[1];
629  	sc->h2 = iv[2];
630  	sc->h3 = iv[3];
631  	sc->h4 = iv[4];
632  	sc->h5 = iv[5];
633  	sc->h6 = iv[6];
634  	sc->h7 = iv[7];
635  	sc->bcount = 0;
636  	sc->ptr = 0;
637  }
638  #if 0
639  static void
640  skein_small_core(sph_skein_small_context *sc, const void *data, size_t len)
641  {
642  	unsigned char *buf;
643  	size_t ptr, clen;
644  	unsigned first;
645  	DECL_STATE_SMALL
646  	buf = sc->buf;
647  	ptr = sc->ptr;
648  	clen = (sizeof sc->buf) - ptr;
649  	if (len <= clen) {
650  		memcpy(buf + ptr, data, len);
651  		sc->ptr = ptr + len;
652  		return;
653  	}
654  	if (clen != 0) {
655  		memcpy(buf + ptr, data, clen);
656  		data = (const unsigned char *)data + clen;
657  		len -= clen;
658  	}
659  #if SPH_SMALL_FOOTPRINT_SKEIN
660  	READ_STATE_SMALL(sc);
661  	first = (bcount == 0) << 7;
662  	for (;;) {
663  		bcount ++;
664  		UBI_SMALL(96 + first, 0);
665  		if (len <= sizeof sc->buf)
666  			break;
667  		first = 0;
668  		memcpy(buf, data, sizeof sc->buf);
669  		data = (const unsigned char *)data + sizeof sc->buf;
670  		len -= sizeof sc->buf;
671  	}
672  	WRITE_STATE_SMALL(sc);
673  	sc->ptr = len;
674  	memcpy(buf, data, len);
675  #else
676  	READ_STATE_SMALL(sc);
677  	first = (bcount == 0) << 7;
678  	for (;;) {
679  		bcount ++;
680  		UBI_SMALL(96 + first, 0);
681  		if (len <= sizeof sc->buf)
682  			break;
683  		buf = (unsigned char *)data;
684  		bcount ++;
685  		UBI_SMALL(96, 0);
686  		if (len <= 2 * sizeof sc->buf) {
687  			data = buf + sizeof sc->buf;
688  			len -= sizeof sc->buf;
689  			break;
690  		}
691  		buf += sizeof sc->buf;
692  		data = buf + sizeof sc->buf;
693  		first = 0;
694  		len -= 2 * sizeof sc->buf;
695  	}
696  	WRITE_STATE_SMALL(sc);
697  	sc->ptr = len;
698  	memcpy(sc->buf, data, len);
699  #endif
700  }
701  #endif
702  static void
703  skein_big_core(sph_skein_big_context *sc, const void *data, size_t len)
704  {
705  	unsigned char *buf;
706  	size_t ptr;
707  	unsigned first;
708  	DECL_STATE_BIG
709  	buf = sc->buf;
710  	ptr = sc->ptr;
711  	if (len <= (sizeof sc->buf) - ptr) {
712  		memcpy(buf + ptr, data, len);
713  		ptr += len;
714  		sc->ptr = ptr;
715  		return;
716  	}
717  	READ_STATE_BIG(sc);
718  	first = (bcount == 0) << 7;
719  	do {
720  		size_t clen;
721  		if (ptr == sizeof sc->buf) {
722  			bcount ++;
723  			UBI_BIG(96 + first, 0);
724  			first = 0;
725  			ptr = 0;
726  		}
727  		clen = (sizeof sc->buf) - ptr;
728  		if (clen > len)
729  			clen = len;
730  		memcpy(buf + ptr, data, clen);
731  		ptr += clen;
732  		data = (const unsigned char *)data + clen;
733  		len -= clen;
734  	} while (len > 0);
735  	WRITE_STATE_BIG(sc);
736  	sc->ptr = ptr;
737  }
738  #if 0
739  static void
740  skein_small_close(sph_skein_small_context *sc, unsigned ub, unsigned n,
741  	void *dst, size_t out_len)
742  {
743  	unsigned char *buf;
744  	size_t ptr;
745  	unsigned et;
746  	int i;
747  	DECL_STATE_SMALL
748  	if (n != 0) {
749  		unsigned z;
750  		unsigned char x;
751  		z = 0x80 >> n;
752  		x = ((ub & -z) | z) & 0xFF;
753  		skein_small_core(sc, &x, 1);
754  	}
755  	buf = sc->buf;
756  	ptr = sc->ptr;
757  	READ_STATE_SMALL(sc);
758  	memset(buf + ptr, 0, (sizeof sc->buf) - ptr);
759  	et = 352 + ((bcount == 0) << 7) + (n != 0);
760  	for (i = 0; i < 2; i ++) {
761  		UBI_SMALL(et, ptr);
762  		if (i == 0) {
763  			memset(buf, 0, sizeof sc->buf);
764  			bcount = 0;
765  			et = 510;
766  			ptr = 8;
767  		}
768  	}
769  	sph_enc64le_aligned(buf +  0, h0);
770  	sph_enc64le_aligned(buf +  8, h1);
771  	sph_enc64le_aligned(buf + 16, h2);
772  	sph_enc64le_aligned(buf + 24, h3);
773  	memcpy(dst, buf, out_len);
774  }
775  #endif
776  static void
777  skein_big_close(sph_skein_big_context *sc, unsigned ub, unsigned n,
778  	void *dst, size_t out_len)
779  {
780  	unsigned char *buf;
781  	size_t ptr;
782  	unsigned et;
783  	int i;
784  #if SPH_SMALL_FOOTPRINT_SKEIN
785  	size_t u;
786  #endif
787  	DECL_STATE_BIG
788  	if (n != 0) {
789  		unsigned z;
790  		unsigned char x;
791  		z = 0x80 >> n;
792  		x = ((ub & -z) | z) & 0xFF;
793  		skein_big_core(sc, &x, 1);
794  	}
795  	buf = sc->buf;
796  	ptr = sc->ptr;
797  	READ_STATE_BIG(sc);
798  	memset(buf + ptr, 0, (sizeof sc->buf) - ptr);
799  	et = 352 + ((bcount == 0) << 7) + (n != 0);
800  	for (i = 0; i < 2; i ++) {
801  		UBI_BIG(et, ptr);
802  		if (i == 0) {
803  			memset(buf, 0, sizeof sc->buf);
804  			bcount = 0;
805  			et = 510;
806  			ptr = 8;
807  		}
808  	}
809  #if SPH_SMALL_FOOTPRINT_SKEIN
810  	for (u = 0; u < out_len; u += 8)
811  		sph_enc64le_aligned(buf + u, h[u >> 3]);
812  	memcpy(dst, buf, out_len);
813  #else
814  	sph_enc64le_aligned(buf +  0, h0);
815  	sph_enc64le_aligned(buf +  8, h1);
816  	sph_enc64le_aligned(buf + 16, h2);
817  	sph_enc64le_aligned(buf + 24, h3);
818  	sph_enc64le_aligned(buf + 32, h4);
819  	sph_enc64le_aligned(buf + 40, h5);
820  	sph_enc64le_aligned(buf + 48, h6);
821  	sph_enc64le_aligned(buf + 56, h7);
822  	memcpy(dst, buf, out_len);
823  #endif
824  }
825  #if 0
826  static const sph_u64 IV224[] = {
827  	SPH_C64(0xC6098A8C9AE5EA0B), SPH_C64(0x876D568608C5191C),
828  	SPH_C64(0x99CB88D7D7F53884), SPH_C64(0x384BDDB1AEDDB5DE)
829  };
830  static const sph_u64 IV256[] = {
831  	SPH_C64(0xFC9DA860D048B449), SPH_C64(0x2FCA66479FA7D833),
832  	SPH_C64(0xB33BC3896656840F), SPH_C64(0x6A54E920FDE8DA69)
833  };
834  #endif
835  static const sph_u64 IV224[] = {
836  	SPH_C64(0xCCD0616248677224), SPH_C64(0xCBA65CF3A92339EF),
837  	SPH_C64(0x8CCD69D652FF4B64), SPH_C64(0x398AED7B3AB890B4),
838  	SPH_C64(0x0F59D1B1457D2BD0), SPH_C64(0x6776FE6575D4EB3D),
839  	SPH_C64(0x99FBC70E997413E9), SPH_C64(0x9E2CFCCFE1C41EF7)
840  };
841  static const sph_u64 IV256[] = {
842  	SPH_C64(0xCCD044A12FDB3E13), SPH_C64(0xE83590301A79A9EB),
843  	SPH_C64(0x55AEA0614F816E6F), SPH_C64(0x2A2767A4AE9B94DB),
844  	SPH_C64(0xEC06025E74DD7683), SPH_C64(0xE7A436CDC4746251),
845  	SPH_C64(0xC36FBAF9393AD185), SPH_C64(0x3EEDBA1833EDFC13)
846  };
<span onclick='openModal()' class='match'>847  static const sph_u64 IV384[] = {
848  	SPH_C64(0xA3F6C6BF3A75EF5F), SPH_C64(0xB0FEF9CCFD84FAA4),
849  	SPH_C64(0x9D77DD663D770CFE), SPH_C64(0xD798CBF3B468FDDA),
850  	SPH_C64(0x1BC4A6668A0E4465), SPH_C64(0x7ED7D434E5807407),
851  	SPH_C64(0x548FC1ACD4EC44D6), SPH_C64(0x266E17546AA18FF8)
852  };
853  static const sph_u64 IV512[] = {
854  	SPH_C64(0x4903ADFF749C51CE), SPH_C64(0x0D95DE399746DF03),
855  	SPH_C64(0x8FD1934127C79BCE), SPH_C64(0x9A255629FF352CB1),
856  	SPH_C64(0x5DB62599DF6CA7B0), SPH_C64(0xEABE394CA9D5C3F4),
857  	SPH_C64(0x991112C71A75B523), SPH_C64(0xAE18A40B660FCC33)
858  };
859  #if 0
860  void
861  sph_skein224_init(void *cc)
</span>862  {
863  	skein_small_init(cc, IV224);
864  }
865  void
866  sph_skein224(void *cc, const void *data, size_t len)
867  {
868  	skein_small_core(cc, data, len);
869  }
870  void
871  sph_skein224_close(void *cc, void *dst)
872  {
873  	sph_skein224_addbits_and_close(cc, 0, 0, dst);
874  }
875  void
876  sph_skein224_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
877  {
878  	skein_small_close(cc, ub, n, dst, 28);
879  	sph_skein224_init(cc);
880  }
881  void
882  sph_skein256_init(void *cc)
883  {
884  	skein_small_init(cc, IV256);
885  }
886  void
887  sph_skein256(void *cc, const void *data, size_t len)
888  {
889  	skein_small_core(cc, data, len);
890  }
891  void
892  sph_skein256_close(void *cc, void *dst)
893  {
894  	sph_skein256_addbits_and_close(cc, 0, 0, dst);
895  }
896  void
897  sph_skein256_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
898  {
899  	skein_small_close(cc, ub, n, dst, 32);
900  	sph_skein256_init(cc);
901  }
902  #endif
903  void
904  sph_skein224_init(void *cc)
905  {
906  	skein_big_init(cc, IV224);
907  }
908  void
909  sph_skein224(void *cc, const void *data, size_t len)
910  {
911  	skein_big_core(cc, data, len);
912  }
913  void
914  sph_skein224_close(void *cc, void *dst)
915  {
916  	sph_skein224_addbits_and_close(cc, 0, 0, dst);
917  }
918  void
919  sph_skein224_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
920  {
921  	skein_big_close(cc, ub, n, dst, 28);
922  }
923  void
924  sph_skein256_init(void *cc)
925  {
926  	skein_big_init(cc, IV256);
927  }
928  void
929  sph_skein256(void *cc, const void *data, size_t len)
930  {
931  	skein_big_core(cc, data, len);
932  }
933  void
934  sph_skein256_close(void *cc, void *dst)
935  {
936  	sph_skein256_addbits_and_close(cc, 0, 0, dst);
937  }
938  void
939  sph_skein256_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
940  {
941  	skein_big_close(cc, ub, n, dst, 32);
942  }
943  void
944  sph_skein384_init(void *cc)
945  {
946  	skein_big_init(cc, IV384);
947  }
948  void
949  sph_skein384(void *cc, const void *data, size_t len)
950  {
951  	skein_big_core(cc, data, len);
952  }
953  void
954  sph_skein384_close(void *cc, void *dst)
955  {
956  	sph_skein384_addbits_and_close(cc, 0, 0, dst);
957  }
958  void
959  sph_skein384_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
960  {
961  	skein_big_close(cc, ub, n, dst, 48);
962  }
963  void
964  sph_skein512_init(void *cc)
965  {
966  	skein_big_init(cc, IV512);
967  }
968  void
969  sph_skein512(void *cc, const void *data, size_t len)
970  {
971  	skein_big_core(cc, data, len);
972  }
973  void
974  sph_skein512_close(void *cc, void *dst)
975  {
976  	sph_skein512_addbits_and_close(cc, 0, 0, dst);
977  }
978  void
979  sph_skein512_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
980  {
981  	skein_big_close(cc, ub, n, dst, 64);
982  }
983  #endif
984  #ifdef __cplusplus
985  }
986  #endif
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_sha2.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_skein.c</div>
                </div>
                <div class="column column_space"><pre><code>14  static const sph_u32 H224[8] = {
15  	SPH_C32(0xC1059ED8), SPH_C32(0x367CD507), SPH_C32(0x3070DD17),
16  	SPH_C32(0xF70E5939), SPH_C32(0xFFC00B31), SPH_C32(0x68581511),
17  	SPH_C32(0x64F98FA7), SPH_C32(0xBEFA4FA4)
18  };
19  static const sph_u32 H256[8] = {
20  	SPH_C32(0x6A09E667), SPH_C32(0xBB67AE85), SPH_C32(0x3C6EF372),
21  	SPH_C32(0xA54FF53A), SPH_C32(0x510E527F), SPH_C32(0x9B05688C),
22  	SPH_C32(0x1F83D9AB), SPH_C32(0x5BE0CD19)
23  };
24  #if SPH_SMALL_FOOTPRINT_SHA2
25  #define SHA2_MEXP1(in, pc)   do { \
26  		W[pc] = in(pc); \
27  	} while (0)
28  #define SHA2_MEXP2(in, pc)   do { \
29  		W[(pc) & 0x0F] = SPH_T32(SSG2_1(W[((pc) - 2) & 0x0F]) \
30  			+ W[((pc) - 7) & 0x0F] \
31  			+ SSG2_0(W[((pc) - 15) & 0x0F]) + W[(pc) & 0x0F]); \
32  	} while (0)
33  #define SHA2_STEPn(n, a, b, c, d, e, f, g, h, in, pc)   do { \
34  		sph_u32 t1, t2; \
35  		SHA2_MEXP ## n(in, pc); \
36  		t1 = SPH_T32(h + BSG2_1(e) + CH(e, f, g) \
37  			+ K[pcount + (pc)] + W[(pc) & 0x0F]); \
38  		t2 = SPH_T32(BSG2_0(a) + MAJ(a, b, c)); \
39        Y_xor_Z = X_xor_Y; \
40  		d = SPH_T32(d + t1); \
41  		h = SPH_T32(t1 + t2); \
42  	} while (0)
43  #define SHA2_STEP1(a, b, c, d, e, f, g, h, in, pc) \
44  	SHA2_STEPn(1, a, b, c, d, e, f, g, h, in, pc)
45  #define SHA2_STEP2(a, b, c, d, e, f, g, h, in, pc) \
46  	SHA2_STEPn(2, a, b, c, d, e, f, g, h, in, pc)
47  #define SHA2_ROUND_BODY(in, r)   do { \
48  		sph_u32 A, B, C, D, E, F, G, H, X_xor_Y, Y_xor_Z; \
49  		sph_u32 W[16]; \
50  		unsigned pcount; \
51   \
52  		A = (r)[0]; \
53  		B = (r)[1]; \
54  		C = (r)[2]; \
55  		D = (r)[3]; \
56  		E = (r)[4]; \
57  		F = (r)[5]; \
58  		G = (r)[6]; \
59  		H = (r)[7]; \
60  		pcount = 0; \
61        Y_xor_Z = B ^ C; \
62  		SHA2_STEP1(A, B, C, D, E, F, G, H, in,  0); \
63  		SHA2_STEP1(H, A, B, C, D, E, F, G, in,  1); \
64  		SHA2_STEP1(G, H, A, B, C, D, E, F, in,  2); \
65  		SHA2_STEP1(F, G, H, A, B, C, D, E, in,  3); \
66  		SHA2_STEP1(E, F, G, H, A, B, C, D, in,  4); \
67  		SHA2_STEP1(D, E, F, G, H, A, B, C, in,  5); \
68  		SHA2_STEP1(C, D, E, F, G, H, A, B, in,  6); \
69  		SHA2_STEP1(B, C, D, E, F, G, H, A, in,  7); \
70  		SHA2_STEP1(A, B, C, D, E, F, G, H, in,  8); \
71  		SHA2_STEP1(H, A, B, C, D, E, F, G, in,  9); \
72  		SHA2_STEP1(G, H, A, B, C, D, E, F, in, 10); \
73  		SHA2_STEP1(F, G, H, A, B, C, D, E, in, 11); \
74  		SHA2_STEP1(E, F, G, H, A, B, C, D, in, 12); \
75  		SHA2_STEP1(D, E, F, G, H, A, B, C, in, 13); \
76  		SHA2_STEP1(C, D, E, F, G, H, A, B, in, 14); \
77  		SHA2_STEP1(B, C, D, E, F, G, H, A, in, 15); \
78  		for (pcount = 16; pcount < 64; pcount += 16) { \
79  			SHA2_STEP2(A, B, C, D, E, F, G, H, in,  0); \
80  			SHA2_STEP2(H, A, B, C, D, E, F, G, in,  1); \
81  			SHA2_STEP2(G, H, A, B, C, D, E, F, in,  2); \
82  			SHA2_STEP2(F, G, H, A, B, C, D, E, in,  3); \
83  			SHA2_STEP2(E, F, G, H, A, B, C, D, in,  4); \
84  			SHA2_STEP2(D, E, F, G, H, A, B, C, in,  5); \
85  			SHA2_STEP2(C, D, E, F, G, H, A, B, in,  6); \
86  			SHA2_STEP2(B, C, D, E, F, G, H, A, in,  7); \
87  			SHA2_STEP2(A, B, C, D, E, F, G, H, in,  8); \
88  			SHA2_STEP2(H, A, B, C, D, E, F, G, in,  9); \
89  			SHA2_STEP2(G, H, A, B, C, D, E, F, in, 10); \
90  			SHA2_STEP2(F, G, H, A, B, C, D, E, in, 11); \
91  			SHA2_STEP2(E, F, G, H, A, B, C, D, in, 12); \
92  			SHA2_STEP2(D, E, F, G, H, A, B, C, in, 13); \
93  			SHA2_STEP2(C, D, E, F, G, H, A, B, in, 14); \
94  			SHA2_STEP2(B, C, D, E, F, G, H, A, in, 15); \
95  		} \
96  		(r)[0] = SPH_T32((r)[0] + A); \
97  		(r)[1] = SPH_T32((r)[1] + B); \
98  		(r)[2] = SPH_T32((r)[2] + C); \
99  		(r)[3] = SPH_T32((r)[3] + D); \
100  		(r)[4] = SPH_T32((r)[4] + E); \
101  		(r)[5] = SPH_T32((r)[5] + F); \
102  		(r)[6] = SPH_T32((r)[6] + G); \
103  		(r)[7] = SPH_T32((r)[7] + H); \
104  	} while (0)
105  #else  
106  #define SHA2_ROUND_BODY(in, r)   do { \
107  		sph_u32 A, B, C, D, E, F, G, H, T1, T2, X_xor_Y, Y_xor_Z;; \
108  		sph_u32 W00, W01, W02, W03, W04, W05, W06, W07; \
109  		sph_u32 W08, W09, W10, W11, W12, W13, W14, W15; \
110   \
111  		A = (r)[0]; \
112  		B = (r)[1]; \
113  		C = (r)[2]; \
114  		D = (r)[3]; \
115  		E = (r)[4]; \
116  		F = (r)[5]; \
117  		G = (r)[6]; \
118  		H = (r)[7]; \
119        Y_xor_Z = B ^ C; \
120  		W00 = in(0); \
121  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
122  			+ SPH_C32(0x428A2F98) + W00); \
123  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
124        Y_xor_Z = X_xor_Y; \
125  		D = SPH_T32(D + T1); \
126  		H = SPH_T32(T1 + T2); \
127  		W01 = in(1); \
128  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
129  			+ SPH_C32(0x71374491) + W01); \
130  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
131        Y_xor_Z = X_xor_Y; \
132  		C = SPH_T32(C + T1); \
133  		G = SPH_T32(T1 + T2); \
134  		W02 = in(2); \
135  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
136  			+ SPH_C32(0xB5C0FBCF) + W02); \
137  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
138        Y_xor_Z = X_xor_Y; \
139  		B = SPH_T32(B + T1); \
140  		F = SPH_T32(T1 + T2); \
141  		W03 = in(3); \
142  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
143  			+ SPH_C32(0xE9B5DBA5) + W03); \
144  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
145        Y_xor_Z = X_xor_Y; \
146  		A = SPH_T32(A + T1); \
147  		E = SPH_T32(T1 + T2); \
148  		W04 = in(4); \
149  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
150  			+ SPH_C32(0x3956C25B) + W04); \
151  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
152        Y_xor_Z = X_xor_Y; \
153  		H = SPH_T32(H + T1); \
154  		D = SPH_T32(T1 + T2); \
155  		W05 = in(5); \
156  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
157  			+ SPH_C32(0x59F111F1) + W05); \
158  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
159        Y_xor_Z = X_xor_Y; \
160  		G = SPH_T32(G + T1); \
161  		C = SPH_T32(T1 + T2); \
162  		W06 = in(6); \
163  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
164  			+ SPH_C32(0x923F82A4) + W06); \
165  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
166        Y_xor_Z = X_xor_Y; \
167  		F = SPH_T32(F + T1); \
168  		B = SPH_T32(T1 + T2); \
169  		W07 = in(7); \
170  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
171  			+ SPH_C32(0xAB1C5ED5) + W07); \
172  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
173        Y_xor_Z = X_xor_Y; \
174  		E = SPH_T32(E + T1); \
175  		A = SPH_T32(T1 + T2); \
176  		W08 = in(8); \
177  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
178  			+ SPH_C32(0xD807AA98) + W08); \
179  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
180        Y_xor_Z = X_xor_Y; \
181  		D = SPH_T32(D + T1); \
182  		H = SPH_T32(T1 + T2); \
183  		W09 = in(9); \
184  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
185  			+ SPH_C32(0x12835B01) + W09); \
186  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
187        Y_xor_Z = X_xor_Y; \
188  		C = SPH_T32(C + T1); \
189  		G = SPH_T32(T1 + T2); \
190  		W10 = in(10); \
191  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
192  			+ SPH_C32(0x243185BE) + W10); \
193  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
194        Y_xor_Z = X_xor_Y; \
195  		B = SPH_T32(B + T1); \
196  		F = SPH_T32(T1 + T2); \
197  		W11 = in(11); \
198  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
199  			+ SPH_C32(0x550C7DC3) + W11); \
200  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
201        Y_xor_Z = X_xor_Y; \
202  		A = SPH_T32(A + T1); \
203  		E = SPH_T32(T1 + T2); \
204  		W12 = in(12); \
205  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
206  			+ SPH_C32(0x72BE5D74) + W12); \
207  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
208        Y_xor_Z = X_xor_Y; \
209  		H = SPH_T32(H + T1); \
210  		D = SPH_T32(T1 + T2); \
211  		W13 = in(13); \
212  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
213  			+ SPH_C32(0x80DEB1FE) + W13); \
214  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
215        Y_xor_Z = X_xor_Y; \
216  		G = SPH_T32(G + T1); \
217  		C = SPH_T32(T1 + T2); \
218  		W14 = in(14); \
219  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
220  			+ SPH_C32(0x9BDC06A7) + W14); \
221  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
222        Y_xor_Z = X_xor_Y; \
223  		F = SPH_T32(F + T1); \
224  		B = SPH_T32(T1 + T2); \
225  		W15 = in(15); \
226  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
227  			+ SPH_C32(0xC19BF174) + W15); \
228  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
229        Y_xor_Z = X_xor_Y; \
230  		E = SPH_T32(E + T1); \
231  		A = SPH_T32(T1 + T2); \
232  		W00 = SPH_T32(SSG2_1(W14) + W09 + SSG2_0(W01) + W00); \
233  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
234  			+ SPH_C32(0xE49B69C1) + W00); \
235  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
236        Y_xor_Z = X_xor_Y; \
237  		D = SPH_T32(D + T1); \
238  		H = SPH_T32(T1 + T2); \
239  		W01 = SPH_T32(SSG2_1(W15) + W10 + SSG2_0(W02) + W01); \
240  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
241  			+ SPH_C32(0xEFBE4786) + W01); \
242  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
243        Y_xor_Z = X_xor_Y; \
244  		C = SPH_T32(C + T1); \
245  		G = SPH_T32(T1 + T2); \
246  		W02 = SPH_T32(SSG2_1(W00) + W11 + SSG2_0(W03) + W02); \
247  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
248  			+ SPH_C32(0x0FC19DC6) + W02); \
249  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
250        Y_xor_Z = X_xor_Y; \
251  		B = SPH_T32(B + T1); \
252  		F = SPH_T32(T1 + T2); \
253  		W03 = SPH_T32(SSG2_1(W01) + W12 + SSG2_0(W04) + W03); \
254  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
255  			+ SPH_C32(0x240CA1CC) + W03); \
256  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
257        Y_xor_Z = X_xor_Y; \
258  		A = SPH_T32(A + T1); \
259  		E = SPH_T32(T1 + T2); \
260  		W04 = SPH_T32(SSG2_1(W02) + W13 + SSG2_0(W05) + W04); \
261  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
262  			+ SPH_C32(0x2DE92C6F) + W04); \
263  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
264        Y_xor_Z = X_xor_Y; \
265  		H = SPH_T32(H + T1); \
266  		D = SPH_T32(T1 + T2); \
267  		W05 = SPH_T32(SSG2_1(W03) + W14 + SSG2_0(W06) + W05); \
268  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
269  			+ SPH_C32(0x4A7484AA) + W05); \
270  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
271        Y_xor_Z = X_xor_Y; \
272  		G = SPH_T32(G + T1); \
273  		C = SPH_T32(T1 + T2); \
274  		W06 = SPH_T32(SSG2_1(W04) + W15 + SSG2_0(W07) + W06); \
275  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
276  			+ SPH_C32(0x5CB0A9DC) + W06); \
277  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
278        Y_xor_Z = X_xor_Y; \
279  		F = SPH_T32(F + T1); \
280  		B = SPH_T32(T1 + T2); \
281  		W07 = SPH_T32(SSG2_1(W05) + W00 + SSG2_0(W08) + W07); \
282  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
283  			+ SPH_C32(0x76F988DA) + W07); \
284  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
285        Y_xor_Z = X_xor_Y; \
286  		E = SPH_T32(E + T1); \
287  		A = SPH_T32(T1 + T2); \
288  		W08 = SPH_T32(SSG2_1(W06) + W01 + SSG2_0(W09) + W08); \
289  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
290  			+ SPH_C32(0x983E5152) + W08); \
291  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
292        Y_xor_Z = X_xor_Y; \
293  		D = SPH_T32(D + T1); \
294  		H = SPH_T32(T1 + T2); \
295  		W09 = SPH_T32(SSG2_1(W07) + W02 + SSG2_0(W10) + W09); \
296  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
297  			+ SPH_C32(0xA831C66D) + W09); \
298  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
299        Y_xor_Z = X_xor_Y; \
300  		C = SPH_T32(C + T1); \
301  		G = SPH_T32(T1 + T2); \
302  		W10 = SPH_T32(SSG2_1(W08) + W03 + SSG2_0(W11) + W10); \
303  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
304  			+ SPH_C32(0xB00327C8) + W10); \
305  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
306        Y_xor_Z = X_xor_Y; \
307  		B = SPH_T32(B + T1); \
308  		F = SPH_T32(T1 + T2); \
309  		W11 = SPH_T32(SSG2_1(W09) + W04 + SSG2_0(W12) + W11); \
310  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
311  			+ SPH_C32(0xBF597FC7) + W11); \
312  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
313        Y_xor_Z = X_xor_Y; \
314  		A = SPH_T32(A + T1); \
315  		E = SPH_T32(T1 + T2); \
316  		W12 = SPH_T32(SSG2_1(W10) + W05 + SSG2_0(W13) + W12); \
317  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
318  			+ SPH_C32(0xC6E00BF3) + W12); \
319  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
320        Y_xor_Z = X_xor_Y; \
321  		H = SPH_T32(H + T1); \
322  		D = SPH_T32(T1 + T2); \
323  		W13 = SPH_T32(SSG2_1(W11) + W06 + SSG2_0(W14) + W13); \
324  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
325  			+ SPH_C32(0xD5A79147) + W13); \
326  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
327        Y_xor_Z = X_xor_Y; \
328  		G = SPH_T32(G + T1); \
329  		C = SPH_T32(T1 + T2); \
330  		W14 = SPH_T32(SSG2_1(W12) + W07 + SSG2_0(W15) + W14); \
331  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
332  			+ SPH_C32(0x06CA6351) + W14); \
333  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
334        Y_xor_Z = X_xor_Y; \
335  		F = SPH_T32(F + T1); \
336  		B = SPH_T32(T1 + T2); \
337  		W15 = SPH_T32(SSG2_1(W13) + W08 + SSG2_0(W00) + W15); \
338  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
339  			+ SPH_C32(0x14292967) + W15); \
340  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
341        Y_xor_Z = X_xor_Y; \
342  		E = SPH_T32(E + T1); \
343  		A = SPH_T32(T1 + T2); \
344  		W00 = SPH_T32(SSG2_1(W14) + W09 + SSG2_0(W01) + W00); \
345  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
346  			+ SPH_C32(0x27B70A85) + W00); \
347  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
348        Y_xor_Z = X_xor_Y; \
349  		D = SPH_T32(D + T1); \
350  		H = SPH_T32(T1 + T2); \
351  		W01 = SPH_T32(SSG2_1(W15) + W10 + SSG2_0(W02) + W01); \
352  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
353  			+ SPH_C32(0x2E1B2138) + W01); \
354  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
355        Y_xor_Z = X_xor_Y; \
356  		C = SPH_T32(C + T1); \
357  		G = SPH_T32(T1 + T2); \
358  		W02 = SPH_T32(SSG2_1(W00) + W11 + SSG2_0(W03) + W02); \
359  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
360  			+ SPH_C32(0x4D2C6DFC) + W02); \
361  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
362        Y_xor_Z = X_xor_Y; \
363  		B = SPH_T32(B + T1); \
364  		F = SPH_T32(T1 + T2); \
365  		W03 = SPH_T32(SSG2_1(W01) + W12 + SSG2_0(W04) + W03); \
366  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
367  			+ SPH_C32(0x53380D13) + W03); \
368  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
369        Y_xor_Z = X_xor_Y; \
370  		A = SPH_T32(A + T1); \
371  		E = SPH_T32(T1 + T2); \
372  		W04 = SPH_T32(SSG2_1(W02) + W13 + SSG2_0(W05) + W04); \
373  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
374  			+ SPH_C32(0x650A7354) + W04); \
375  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
376        Y_xor_Z = X_xor_Y; \
377  		H = SPH_T32(H + T1); \
378  		D = SPH_T32(T1 + T2); \
379  		W05 = SPH_T32(SSG2_1(W03) + W14 + SSG2_0(W06) + W05); \
380  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
381  			+ SPH_C32(0x766A0ABB) + W05); \
382  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
383        Y_xor_Z = X_xor_Y; \
384  		G = SPH_T32(G + T1); \
385  		C = SPH_T32(T1 + T2); \
386  		W06 = SPH_T32(SSG2_1(W04) + W15 + SSG2_0(W07) + W06); \
387  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
388  			+ SPH_C32(0x81C2C92E) + W06); \
389  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
390        Y_xor_Z = X_xor_Y; \
391  		F = SPH_T32(F + T1); \
392  		B = SPH_T32(T1 + T2); \
393  		W07 = SPH_T32(SSG2_1(W05) + W00 + SSG2_0(W08) + W07); \
394  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
395  			+ SPH_C32(0x92722C85) + W07); \
396  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
397        Y_xor_Z = X_xor_Y; \
398  		E = SPH_T32(E + T1); \
399  		A = SPH_T32(T1 + T2); \
400  		W08 = SPH_T32(SSG2_1(W06) + W01 + SSG2_0(W09) + W08); \
401  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
402  			+ SPH_C32(0xA2BFE8A1) + W08); \
403  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
404        Y_xor_Z = X_xor_Y; \
405  		D = SPH_T32(D + T1); \
406  		H = SPH_T32(T1 + T2); \
407  		W09 = SPH_T32(SSG2_1(W07) + W02 + SSG2_0(W10) + W09); \
408  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
409  			+ SPH_C32(0xA81A664B) + W09); \
410  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
411        Y_xor_Z = X_xor_Y; \
412  		C = SPH_T32(C + T1); \
413  		G = SPH_T32(T1 + T2); \
414  		W10 = SPH_T32(SSG2_1(W08) + W03 + SSG2_0(W11) + W10); \
415  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
416  			+ SPH_C32(0xC24B8B70) + W10); \
417  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
418        Y_xor_Z = X_xor_Y; \
419  		B = SPH_T32(B + T1); \
420  		F = SPH_T32(T1 + T2); \
421  		W11 = SPH_T32(SSG2_1(W09) + W04 + SSG2_0(W12) + W11); \
422  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
423  			+ SPH_C32(0xC76C51A3) + W11); \
424  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
425        Y_xor_Z = X_xor_Y; \
426  		A = SPH_T32(A + T1); \
427  		E = SPH_T32(T1 + T2); \
428  		W12 = SPH_T32(SSG2_1(W10) + W05 + SSG2_0(W13) + W12); \
429  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
430  			+ SPH_C32(0xD192E819) + W12); \
431  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
432        Y_xor_Z = X_xor_Y; \
433  		H = SPH_T32(H + T1); \
434  		D = SPH_T32(T1 + T2); \
435  		W13 = SPH_T32(SSG2_1(W11) + W06 + SSG2_0(W14) + W13); \
436  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
437  			+ SPH_C32(0xD6990624) + W13); \
438  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
439        Y_xor_Z = X_xor_Y; \
440  		G = SPH_T32(G + T1); \
441  		C = SPH_T32(T1 + T2); \
442  		W14 = SPH_T32(SSG2_1(W12) + W07 + SSG2_0(W15) + W14); \
443  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
444  			+ SPH_C32(0xF40E3585) + W14); \
445  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
446        Y_xor_Z = X_xor_Y; \
447  		F = SPH_T32(F + T1); \
448  		B = SPH_T32(T1 + T2); \
449  		W15 = SPH_T32(SSG2_1(W13) + W08 + SSG2_0(W00) + W15); \
450  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
451  			+ SPH_C32(0x106AA070) + W15); \
452  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
453        Y_xor_Z = X_xor_Y; \
454  		E = SPH_T32(E + T1); \
455  		A = SPH_T32(T1 + T2); \
456  		W00 = SPH_T32(SSG2_1(W14) + W09 + SSG2_0(W01) + W00); \
457  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
458  			+ SPH_C32(0x19A4C116) + W00); \
459  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
460        Y_xor_Z = X_xor_Y; \
461  		D = SPH_T32(D + T1); \
462  		H = SPH_T32(T1 + T2); \
463  		W01 = SPH_T32(SSG2_1(W15) + W10 + SSG2_0(W02) + W01); \
464  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
465  			+ SPH_C32(0x1E376C08) + W01); \
466  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
467        Y_xor_Z = X_xor_Y; \
468  		C = SPH_T32(C + T1); \
469  		G = SPH_T32(T1 + T2); \
470  		W02 = SPH_T32(SSG2_1(W00) + W11 + SSG2_0(W03) + W02); \
471  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
472  			+ SPH_C32(0x2748774C) + W02); \
473  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
474        Y_xor_Z = X_xor_Y; \
475  		B = SPH_T32(B + T1); \
476  		F = SPH_T32(T1 + T2); \
477  		W03 = SPH_T32(SSG2_1(W01) + W12 + SSG2_0(W04) + W03); \
478  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
479  			+ SPH_C32(0x34B0BCB5) + W03); \
480  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
481        Y_xor_Z = X_xor_Y; \
482  		A = SPH_T32(A + T1); \
483  		E = SPH_T32(T1 + T2); \
484  		W04 = SPH_T32(SSG2_1(W02) + W13 + SSG2_0(W05) + W04); \
485  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
486  			+ SPH_C32(0x391C0CB3) + W04); \
487  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
488        Y_xor_Z = X_xor_Y; \
489  		H = SPH_T32(H + T1); \
490  		D = SPH_T32(T1 + T2); \
491  		W05 = SPH_T32(SSG2_1(W03) + W14 + SSG2_0(W06) + W05); \
492  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
493  			+ SPH_C32(0x4ED8AA4A) + W05); \
494  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
495        Y_xor_Z = X_xor_Y; \
496  		G = SPH_T32(G + T1); \
497  		C = SPH_T32(T1 + T2); \
498  		W06 = SPH_T32(SSG2_1(W04) + W15 + SSG2_0(W07) + W06); \
499  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
500  			+ SPH_C32(0x5B9CCA4F) + W06); \
501  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
502        Y_xor_Z = X_xor_Y; \
503  		F = SPH_T32(F + T1); \
504  		B = SPH_T32(T1 + T2); \
505  		W07 = SPH_T32(SSG2_1(W05) + W00 + SSG2_0(W08) + W07); \
506  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
507  			+ SPH_C32(0x682E6FF3) + W07); \
508  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
509        Y_xor_Z = X_xor_Y; \
510  		E = SPH_T32(E + T1); \
511  		A = SPH_T32(T1 + T2); \
512  		W08 = SPH_T32(SSG2_1(W06) + W01 + SSG2_0(W09) + W08); \
513  		T1 = SPH_T32(H + BSG2_1(E) + CH(E, F, G) \
514  			+ SPH_C32(0x748F82EE) + W08); \
515  		T2 = SPH_T32(BSG2_0(A) + MAJ(A, B, C)); \
516        Y_xor_Z = X_xor_Y; \
517  		D = SPH_T32(D + T1); \
518  		H = SPH_T32(T1 + T2); \
519  		W09 = SPH_T32(SSG2_1(W07) + W02 + SSG2_0(W10) + W09); \
520  		T1 = SPH_T32(G + BSG2_1(D) + CH(D, E, F) \
521  			+ SPH_C32(0x78A5636F) + W09); \
522  		T2 = SPH_T32(BSG2_0(H) + MAJ(H, A, B)); \
523        Y_xor_Z = X_xor_Y; \
524  		C = SPH_T32(C + T1); \
525  		G = SPH_T32(T1 + T2); \
526  		W10 = SPH_T32(SSG2_1(W08) + W03 + SSG2_0(W11) + W10); \
527  		T1 = SPH_T32(F + BSG2_1(C) + CH(C, D, E) \
528  			+ SPH_C32(0x84C87814) + W10); \
529  		T2 = SPH_T32(BSG2_0(G) + MAJ(G, H, A)); \
530        Y_xor_Z = X_xor_Y; \
531  		B = SPH_T32(B + T1); \
532  		F = SPH_T32(T1 + T2); \
533  		W11 = SPH_T32(SSG2_1(W09) + W04 + SSG2_0(W12) + W11); \
534  		T1 = SPH_T32(E + BSG2_1(B) + CH(B, C, D) \
535  			+ SPH_C32(0x8CC70208) + W11); \
536  		T2 = SPH_T32(BSG2_0(F) + MAJ(F, G, H)); \
537        Y_xor_Z = X_xor_Y; \
538  		A = SPH_T32(A + T1); \
539  		E = SPH_T32(T1 + T2); \
540  		W12 = SPH_T32(SSG2_1(W10) + W05 + SSG2_0(W13) + W12); \
541  		T1 = SPH_T32(D + BSG2_1(A) + CH(A, B, C) \
542  			+ SPH_C32(0x90BEFFFA) + W12); \
543  		T2 = SPH_T32(BSG2_0(E) + MAJ(E, F, G)); \
544        Y_xor_Z = X_xor_Y; \
545  		H = SPH_T32(H + T1); \
546  		D = SPH_T32(T1 + T2); \
547  		W13 = SPH_T32(SSG2_1(W11) + W06 + SSG2_0(W14) + W13); \
548  		T1 = SPH_T32(C + BSG2_1(H) + CH(H, A, B) \
549  			+ SPH_C32(0xA4506CEB) + W13); \
550  		T2 = SPH_T32(BSG2_0(D) + MAJ(D, E, F)); \
551        Y_xor_Z = X_xor_Y; \
552  		G = SPH_T32(G + T1); \
553  		C = SPH_T32(T1 + T2); \
554  		W14 = SPH_T32(SSG2_1(W12) + W07 + SSG2_0(W15) + W14); \
555  		T1 = SPH_T32(B + BSG2_1(G) + CH(G, H, A) \
556  			+ SPH_C32(0xBEF9A3F7) + W14); \
557  		T2 = SPH_T32(BSG2_0(C) + MAJ(C, D, E)); \
558        Y_xor_Z = X_xor_Y; \
559  		F = SPH_T32(F + T1); \
560  		B = SPH_T32(T1 + T2); \
561  		W15 = SPH_T32(SSG2_1(W13) + W08 + SSG2_0(W00) + W15); \
562  		T1 = SPH_T32(A + BSG2_1(F) + CH(F, G, H) \
563  			+ SPH_C32(0xC67178F2) + W15); \
564  		T2 = SPH_T32(BSG2_0(B) + MAJ(B, C, D)); \
565        Y_xor_Z = X_xor_Y; \
566  		E = SPH_T32(E + T1); \
567  		A = SPH_T32(T1 + T2); \
568  		(r)[0] = SPH_T32((r)[0] + A); \
569  		(r)[1] = SPH_T32((r)[1] + B); \
570  		(r)[2] = SPH_T32((r)[2] + C); \
571  		(r)[3] = SPH_T32((r)[3] + D); \
572  		(r)[4] = SPH_T32((r)[4] + E); \
573  		(r)[5] = SPH_T32((r)[5] + F); \
574  		(r)[6] = SPH_T32((r)[6] + G); \
575  		(r)[7] = SPH_T32((r)[7] + H); \
576  	} while (0)
577  #endif  
578  static void
579  sha2_round(const unsigned char *data, sph_u32 r[8])
</pre></code></div>
                <div class="column column_space"><pre><code>847  static const sph_u64 IV384[] = {
848  	SPH_C64(0xA3F6C6BF3A75EF5F), SPH_C64(0xB0FEF9CCFD84FAA4),
849  	SPH_C64(0x9D77DD663D770CFE), SPH_C64(0xD798CBF3B468FDDA),
850  	SPH_C64(0x1BC4A6668A0E4465), SPH_C64(0x7ED7D434E5807407),
851  	SPH_C64(0x548FC1ACD4EC44D6), SPH_C64(0x266E17546AA18FF8)
852  };
853  static const sph_u64 IV512[] = {
854  	SPH_C64(0x4903ADFF749C51CE), SPH_C64(0x0D95DE399746DF03),
855  	SPH_C64(0x8FD1934127C79BCE), SPH_C64(0x9A255629FF352CB1),
856  	SPH_C64(0x5DB62599DF6CA7B0), SPH_C64(0xEABE394CA9D5C3F4),
857  	SPH_C64(0x991112C71A75B523), SPH_C64(0xAE18A40B660FCC33)
858  };
859  #if 0
860  void
861  sph_skein224_init(void *cc)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    