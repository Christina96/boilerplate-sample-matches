
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 2.840909090909091%, Tokens: 10, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>Adafruit_nRF52_Arduino-MDEwOlJlcG9zaXRvcnk3NDM1NDcyOQ==-flat-nrfx_pwm.c</h3>
            <pre><code>1  #include <nrfx.h>
2  #if NRFX_CHECK(NRFX_PWM_ENABLED)
3  #if !(NRFX_CHECK(NRFX_PWM0_ENABLED) || NRFX_CHECK(NRFX_PWM1_ENABLED) || \
4        NRFX_CHECK(NRFX_PWM2_ENABLED) || NRFX_CHECK(NRFX_PWM3_ENABLED))
5  #error "No enabled PWM instances. Check <nrfx_config.h>."
6  #endif
7  #include <nrfx_pwm.h>
8  #include <hal/nrf_gpio.h>
9  #define NRFX_LOG_MODULE PWM
10  #include <nrfx_log.h>
11  #if NRFX_CHECK(NRFX_PWM_NRF52_ANOMALY_109_WORKAROUND_ENABLED)
12  #include <hal/nrf_egu.h>
13  #define USE_DMA_ISSUE_WORKAROUND
14  #endif
15  #if defined(USE_DMA_ISSUE_WORKAROUND)
16  #define EGU_IRQn(i)         EGU_IRQn_(i)
17  #define EGU_IRQn_(i)        SWI##i##_EGU##i##_IRQn
18  #define EGU_IRQHandler(i)   EGU_IRQHandler_(i)
19  #define EGU_IRQHandler_(i)  nrfx_egu_##i##_irq_handler
20  #define DMA_ISSUE_EGU_IDX           NRFX_PWM_NRF52_ANOMALY_109_EGU_INSTANCE
21  #define DMA_ISSUE_EGU               NRFX_CONCAT_2(NRF_EGU, DMA_ISSUE_EGU_IDX)
22  #define DMA_ISSUE_EGU_IRQn          EGU_IRQn(DMA_ISSUE_EGU_IDX)
23  #define DMA_ISSUE_EGU_IRQHandler    EGU_IRQHandler(DMA_ISSUE_EGU_IDX)
24  #endif
25  typedef struct
26  {
27  #if defined(USE_DMA_ISSUE_WORKAROUND)
28      uint32_t                  starting_task_address;
29  #endif
30      nrfx_pwm_handler_t        handler;
31      void *                    p_context;
32      nrfx_drv_state_t volatile state;
33      uint8_t                   flags;
34  } pwm_control_block_t;
35  static pwm_control_block_t m_cb[NRFX_PWM_ENABLED_COUNT];
36  static void configure_pins(nrfx_pwm_t const *        p_instance,
37                             nrfx_pwm_config_t const * p_config)
38  {
39      uint32_t out_pins[NRF_PWM_CHANNEL_COUNT];
40      uint8_t i;
41      for (i = 0; i < NRF_PWM_CHANNEL_COUNT; ++i)
42      {
43          uint8_t output_pin = p_config->output_pins[i];
44          if (output_pin != NRFX_PWM_PIN_NOT_USED)
45          {
46              bool inverted = output_pin &  NRFX_PWM_PIN_INVERTED;
47              out_pins[i]   = output_pin & ~NRFX_PWM_PIN_INVERTED;
48              if (!p_config->skip_gpio_cfg)
49              {
50                  if (inverted)
51                  {
52                      nrf_gpio_pin_set(out_pins[i]);
53                  }
54                  else
55                  {
56                      nrf_gpio_pin_clear(out_pins[i]);
57                  }
58                  nrf_gpio_cfg_output(out_pins[i]);
59              }
60          }
61          else
62          {
63              out_pins[i] = NRF_PWM_PIN_NOT_CONNECTED;
64          }
65      }
66      nrf_pwm_pins_set(p_instance->p_registers, out_pins);
67  }
68  nrfx_err_t nrfx_pwm_init(nrfx_pwm_t const *        p_instance,
69                           nrfx_pwm_config_t const * p_config,
70                           nrfx_pwm_handler_t        handler,
71                           void *                    p_context)
72  {
73      NRFX_ASSERT(p_config);
74      nrfx_err_t err_code;
75      pwm_control_block_t * p_cb  = &m_cb[p_instance->drv_inst_idx];
76      if (p_cb->state != NRFX_DRV_STATE_UNINITIALIZED)
77      {
78          err_code = NRFX_ERROR_INVALID_STATE;
79          NRFX_LOG_WARNING("Function: %s, error code: %s.",
80                           __func__,
81                           NRFX_LOG_ERROR_STRING_GET(err_code));
82          return err_code;
83      }
84      p_cb->handler = handler;
85      p_cb->p_context = p_context;
86      configure_pins(p_instance, p_config);
87      nrf_pwm_enable(p_instance->p_registers);
88      nrf_pwm_configure(p_instance->p_registers,
89          p_config->base_clock, p_config->count_mode, p_config->top_value);
90      nrf_pwm_decoder_set(p_instance->p_registers,
91          p_config->load_mode, p_config->step_mode);
92      nrf_pwm_shorts_set(p_instance->p_registers, 0);
93      nrf_pwm_int_set(p_instance->p_registers, 0);
94      nrf_pwm_event_clear(p_instance->p_registers, NRF_PWM_EVENT_LOOPSDONE);
95      nrf_pwm_event_clear(p_instance->p_registers, NRF_PWM_EVENT_SEQEND0);
96      nrf_pwm_event_clear(p_instance->p_registers, NRF_PWM_EVENT_SEQEND1);
97      nrf_pwm_event_clear(p_instance->p_registers, NRF_PWM_EVENT_STOPPED);
98  #if defined(USE_DMA_ISSUE_WORKAROUND)
99      NRFX_IRQ_PRIORITY_SET(DMA_ISSUE_EGU_IRQn, p_config->irq_priority);
100      NRFX_IRQ_ENABLE(DMA_ISSUE_EGU_IRQn);
101  #else
102      if (p_cb->handler)
103  #endif
104      {
105          NRFX_IRQ_PRIORITY_SET(nrfx_get_irq_number(p_instance->p_registers),
106              p_config->irq_priority);
107          NRFX_IRQ_ENABLE(nrfx_get_irq_number(p_instance->p_registers));
108      }
109      p_cb->state = NRFX_DRV_STATE_INITIALIZED;
110      err_code = NRFX_SUCCESS;
111      NRFX_LOG_INFO("Function: %s, error code: %s.", __func__, NRFX_LOG_ERROR_STRING_GET(err_code));
112      return err_code;
113  }
114  void nrfx_pwm_uninit(nrfx_pwm_t const * p_instance)
115  {
116      pwm_control_block_t * p_cb  = &m_cb[p_instance->drv_inst_idx];
117      NRFX_ASSERT(p_cb->state != NRFX_DRV_STATE_UNINITIALIZED);
118      NRFX_IRQ_DISABLE(nrfx_get_irq_number(p_instance->p_registers));
119  #if defined(USE_DMA_ISSUE_WORKAROUND)
120      NRFX_IRQ_DISABLE(DMA_ISSUE_EGU_IRQn);
121  #endif
122      nrf_pwm_disable(p_instance->p_registers);
123      p_cb->state = NRFX_DRV_STATE_UNINITIALIZED;
124  }
125  static uint32_t start_playback(nrfx_pwm_t const * p_instance,
126                                 pwm_control_block_t * p_cb,
127                                 uint8_t               flags,
128                                 nrf_pwm_task_t        starting_task)
129  {
130      p_cb->state = NRFX_DRV_STATE_POWERED_ON;
131      p_cb->flags = flags;
132      if (p_cb->handler)
133      {
134          uint32_t int_mask = NRF_PWM_INT_LOOPSDONE_MASK |
135                              NRF_PWM_INT_STOPPED_MASK;
136  #if defined(USE_DMA_ISSUE_WORKAROUND)
137          int_mask |= NRF_PWM_INT_SEQEND0_MASK | NRF_PWM_INT_SEQEND1_MASK;
138  #else
139          if (flags & NRFX_PWM_FLAG_SIGNAL_END_SEQ0)
140          {
141              int_mask |= NRF_PWM_INT_SEQEND0_MASK;
142          }
143          if (flags & NRFX_PWM_FLAG_SIGNAL_END_SEQ1)
144          {
145              int_mask |= NRF_PWM_INT_SEQEND1_MASK;
146          }
147  #endif
148          if (flags & NRFX_PWM_FLAG_NO_EVT_FINISHED)
149          {
150              int_mask &= ~NRF_PWM_INT_LOOPSDONE_MASK;
151          }
152          nrf_pwm_int_set(p_instance->p_registers, int_mask);
153      }
154  #if defined(USE_DMA_ISSUE_WORKAROUND)
155      else
156      {
157          nrf_pwm_int_set(p_instance->p_registers,
158              NRF_PWM_INT_SEQEND0_MASK | NRF_PWM_INT_SEQEND1_MASK);
159      }
160  #endif
161      nrf_pwm_event_clear(p_instance->p_registers, NRF_PWM_EVENT_STOPPED);
162      if (flags & NRFX_PWM_FLAG_START_VIA_TASK)
163      {
164          uint32_t starting_task_address =
165              nrf_pwm_task_address_get(p_instance->p_registers, starting_task);
166  #if defined(USE_DMA_ISSUE_WORKAROUND)
167          p_cb->starting_task_address = starting_task_address;
168          nrf_egu_int_enable(DMA_ISSUE_EGU, nrf_egu_channel_int_get(p_instance->drv_inst_idx));
169          return nrf_egu_task_address_get(DMA_ISSUE_EGU,
170                                          nrf_egu_trigger_task_get(p_instance->drv_inst_idx));
171  #else
<span onclick='openModal()' class='match'>172          return starting_task_address;
173  #endif
174      }
175      nrf_pwm_task_trigger(p_instance->p_registers, starting_task);
176      return 0;
177  }
178  uint32_t nrfx_pwm_simple_playback(nrfx_pwm_t const *         p_instance,
179                                    nrf_pwm_sequence_t const * p_sequence,
180                                    uint16_t                   playback_count,
181                                    uint32_t                   flags)
182  {
183      pwm_control_block_t * p_cb  = &m_cb[p_instance->drv_inst_idx];
</span>184      NRFX_ASSERT(p_cb->state != NRFX_DRV_STATE_UNINITIALIZED);
185      NRFX_ASSERT(playback_count > 0);
186      NRFX_ASSERT(nrfx_is_in_ram(p_sequence->values.p_raw));
187      nrf_pwm_sequence_set(p_instance->p_registers, 0, p_sequence);
188      nrf_pwm_sequence_set(p_instance->p_registers, 1, p_sequence);
189      bool odd = (playback_count & 1);
190      nrf_pwm_loop_set(p_instance->p_registers,
191          (playback_count / 2) + (odd ? 1 : 0));
192      uint32_t shorts_mask;
193      if (flags & NRFX_PWM_FLAG_STOP)
194      {
195          shorts_mask = NRF_PWM_SHORT_LOOPSDONE_STOP_MASK;
196      }
197      else if (flags & NRFX_PWM_FLAG_LOOP)
198      {
199          shorts_mask = odd ? NRF_PWM_SHORT_LOOPSDONE_SEQSTART1_MASK
200                            : NRF_PWM_SHORT_LOOPSDONE_SEQSTART0_MASK;
201      }
202      else
203      {
204          shorts_mask = 0;
205      }
206      nrf_pwm_shorts_set(p_instance->p_registers, shorts_mask);
207      NRFX_LOG_INFO("Function: %s, sequence length: %d.",
208                    __func__,
209                    p_sequence->length);
210      NRFX_LOG_DEBUG("Sequence data:");
211      NRFX_LOG_HEXDUMP_DEBUG((uint8_t *)p_sequence->values.p_raw,
212                             p_sequence->length * sizeof(uint16_t));
213      return start_playback(p_instance, p_cb, flags,
214          odd ? NRF_PWM_TASK_SEQSTART1 : NRF_PWM_TASK_SEQSTART0);
215  }
216  uint32_t nrfx_pwm_complex_playback(nrfx_pwm_t const *         p_instance,
217                                     nrf_pwm_sequence_t const * p_sequence_0,
218                                     nrf_pwm_sequence_t const * p_sequence_1,
219                                     uint16_t                   playback_count,
220                                     uint32_t                   flags)
221  {
222      pwm_control_block_t * p_cb  = &m_cb[p_instance->drv_inst_idx];
223      NRFX_ASSERT(p_cb->state != NRFX_DRV_STATE_UNINITIALIZED);
224      NRFX_ASSERT(playback_count > 0);
225      NRFX_ASSERT(nrfx_is_in_ram(p_sequence_0->values.p_raw));
226      NRFX_ASSERT(nrfx_is_in_ram(p_sequence_1->values.p_raw));
227      nrf_pwm_sequence_set(p_instance->p_registers, 0, p_sequence_0);
228      nrf_pwm_sequence_set(p_instance->p_registers, 1, p_sequence_1);
229      nrf_pwm_loop_set(p_instance->p_registers, playback_count);
230      uint32_t shorts_mask;
231      if (flags & NRFX_PWM_FLAG_STOP)
232      {
233          shorts_mask = NRF_PWM_SHORT_LOOPSDONE_STOP_MASK;
234      }
235      else if (flags & NRFX_PWM_FLAG_LOOP)
236      {
237          shorts_mask = NRF_PWM_SHORT_LOOPSDONE_SEQSTART0_MASK;
238      }
239      else
240      {
241          shorts_mask = 0;
242      }
243      nrf_pwm_shorts_set(p_instance->p_registers, shorts_mask);
244      NRFX_LOG_INFO("Function: %s, sequence 0 length: %d.",
245                    __func__,
246                    p_sequence_0->length);
247      NRFX_LOG_INFO("Function: %s, sequence 1 length: %d.",
248                    __func__,
249                    p_sequence_1->length);
250      NRFX_LOG_DEBUG("Sequence 0 data:");
251      NRFX_LOG_HEXDUMP_DEBUG(p_sequence_0->values.p_raw,
252                             p_sequence_0->length * sizeof(uint16_t));
253      NRFX_LOG_DEBUG("Sequence 1 data:");
254      NRFX_LOG_HEXDUMP_DEBUG(p_sequence_1->values.p_raw,
255                             p_sequence_1->length * sizeof(uint16_t));
256      return start_playback(p_instance, p_cb, flags, NRF_PWM_TASK_SEQSTART0);
257  }
258  bool nrfx_pwm_stop(nrfx_pwm_t const * p_instance,
259                     bool               wait_until_stopped)
260  {
261      NRFX_ASSERT(m_cb[p_instance->drv_inst_idx].state != NRFX_DRV_STATE_UNINITIALIZED);
262      bool ret_val = false;
263      nrf_pwm_shorts_set(p_instance->p_registers, 0);
264      nrf_pwm_task_trigger(p_instance->p_registers, NRF_PWM_TASK_STOP);
265      if (nrfx_pwm_is_stopped(p_instance))
266      {
267          ret_val = true;
268      }
269      else
270      {
271          do {
272              if (nrfx_pwm_is_stopped(p_instance))
273              {
274                  ret_val = true;
275                  break;
276              }
277          } while (wait_until_stopped);
278      }
279      NRFX_LOG_INFO("%s returned %d.", __func__, ret_val);
280      return ret_val;
281  }
282  bool nrfx_pwm_is_stopped(nrfx_pwm_t const * p_instance)
283  {
284      pwm_control_block_t * p_cb  = &m_cb[p_instance->drv_inst_idx];
285      NRFX_ASSERT(p_cb->state != NRFX_DRV_STATE_UNINITIALIZED);
286      bool ret_val = false;
287      if (p_cb->state != NRFX_DRV_STATE_POWERED_ON)
288      {
289          ret_val = true;
290      }
291      if (nrf_pwm_event_check(p_instance->p_registers, NRF_PWM_EVENT_STOPPED))
292      {
293          p_cb->state = NRFX_DRV_STATE_INITIALIZED;
294          NRFX_LOG_INFO("Disabled.");
295          ret_val = true;
296      }
297      NRFX_LOG_INFO("%s returned %d.", __func__, ret_val);
298      return ret_val;
299  }
300  static void irq_handler(NRF_PWM_Type * p_pwm, pwm_control_block_t * p_cb)
301  {
302      if (nrf_pwm_event_check(p_pwm, NRF_PWM_EVENT_SEQEND0))
303      {
304          nrf_pwm_event_clear(p_pwm, NRF_PWM_EVENT_SEQEND0);
305          if ((p_cb->flags & NRFX_PWM_FLAG_SIGNAL_END_SEQ0) && p_cb->handler)
306          {
307              p_cb->handler(NRFX_PWM_EVT_END_SEQ0, p_cb->p_context);
308          }
309      }
310      if (nrf_pwm_event_check(p_pwm, NRF_PWM_EVENT_SEQEND1))
311      {
312          nrf_pwm_event_clear(p_pwm, NRF_PWM_EVENT_SEQEND1);
313          if ((p_cb->flags & NRFX_PWM_FLAG_SIGNAL_END_SEQ1) && p_cb->handler)
314          {
315              p_cb->handler(NRFX_PWM_EVT_END_SEQ1, p_cb->p_context);
316          }
317      }
318      if (nrf_pwm_event_check(p_pwm, NRF_PWM_EVENT_LOOPSDONE))
319      {
320          nrf_pwm_event_clear(p_pwm, NRF_PWM_EVENT_LOOPSDONE);
321          if (!(p_cb->flags & NRFX_PWM_FLAG_NO_EVT_FINISHED) && p_cb->handler)
322          {
323              p_cb->handler(NRFX_PWM_EVT_FINISHED, p_cb->p_context);
324          }
325      }
326      if (nrf_pwm_event_check(p_pwm, NRF_PWM_EVENT_STOPPED))
327      {
328          nrf_pwm_event_clear(p_pwm, NRF_PWM_EVENT_STOPPED);
329          p_cb->state = NRFX_DRV_STATE_INITIALIZED;
330          if (p_cb->handler)
331          {
332              p_cb->handler(NRFX_PWM_EVT_STOPPED, p_cb->p_context);
333          }
334      }
335  }
336  #if defined(USE_DMA_ISSUE_WORKAROUND)
337  void DMA_ISSUE_EGU_IRQHandler(void)
338  {
339      for (uint8_t i = 0; i < NRFX_PWM_ENABLED_COUNT; i++)
340      {
341          nrf_egu_event_t event = nrf_egu_triggered_event_get(i);
342          if (nrf_egu_event_check(DMA_ISSUE_EGU, event))
343          {
344              nrf_egu_event_clear(DMA_ISSUE_EGU, event);
345              *(volatile uint32_t *)(m_cb[i].starting_task_address) = 1;
346          }
347      }
348  }
349  #endif
350  #if NRFX_CHECK(NRFX_PWM0_ENABLED)
351  void nrfx_pwm_0_irq_handler(void)
352  {
353      irq_handler(NRF_PWM0, &m_cb[NRFX_PWM0_INST_IDX]);
354  }
355  #endif
356  #if NRFX_CHECK(NRFX_PWM1_ENABLED)
357  void nrfx_pwm_1_irq_handler(void)
358  {
359      irq_handler(NRF_PWM1, &m_cb[NRFX_PWM1_INST_IDX]);
360  }
361  #endif
362  #if NRFX_CHECK(NRFX_PWM2_ENABLED)
363  void nrfx_pwm_2_irq_handler(void)
364  {
365      irq_handler(NRF_PWM2, &m_cb[NRFX_PWM2_INST_IDX]);
366  }
367  #endif
368  #if NRFX_CHECK(NRFX_PWM3_ENABLED)
369  void nrfx_pwm_3_irq_handler(void)
370  {
371      irq_handler(NRF_PWM3, &m_cb[NRFX_PWM3_INST_IDX]);
372  }
373  #endif
374  #endif 
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-ctl.c</h3>
            <pre><code>1  #define JEMALLOC_CTL_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/ctl.h"
6  #include "jemalloc/internal/extent_dss.h"
7  #include "jemalloc/internal/extent_mmap.h"
8  #include "jemalloc/internal/mutex.h"
9  #include "jemalloc/internal/nstime.h"
10  #include "jemalloc/internal/sc.h"
11  #include "jemalloc/internal/util.h"
12  static malloc_mutex_t	ctl_mtx;
13  static bool		ctl_initialized;
14  static ctl_stats_t	*ctl_stats;
15  static ctl_arenas_t	*ctl_arenas;
16  static const ctl_named_node_t *
17  ctl_named_node(const ctl_node_t *node) {
18  	return ((node->named) ? (const ctl_named_node_t *)node : NULL);
19  }
20  static const ctl_named_node_t *
21  ctl_named_children(const ctl_named_node_t *node, size_t index) {
22  	const ctl_named_node_t *children = ctl_named_node(node->children);
23  	return (children ? &children[index] : NULL);
24  }
25  static const ctl_indexed_node_t *
26  ctl_indexed_node(const ctl_node_t *node) {
27  	return (!node->named ? (const ctl_indexed_node_t *)node : NULL);
28  }
29  #define CTL_PROTO(n)							\
30  static int	n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,	\
31      void *oldp, size_t *oldlenp, void *newp, size_t newlen);
32  #define INDEX_PROTO(n)							\
33  static const ctl_named_node_t	*n##_index(tsdn_t *tsdn,		\
34      const size_t *mib, size_t miblen, size_t i);
35  CTL_PROTO(version)
36  CTL_PROTO(epoch)
37  CTL_PROTO(background_thread)
38  CTL_PROTO(max_background_threads)
39  CTL_PROTO(thread_tcache_enabled)
40  CTL_PROTO(thread_tcache_flush)
41  CTL_PROTO(thread_prof_name)
42  CTL_PROTO(thread_prof_active)
43  CTL_PROTO(thread_arena)
44  CTL_PROTO(thread_allocated)
45  CTL_PROTO(thread_allocatedp)
46  CTL_PROTO(thread_deallocated)
47  CTL_PROTO(thread_deallocatedp)
48  CTL_PROTO(config_cache_oblivious)
49  CTL_PROTO(config_debug)
50  CTL_PROTO(config_fill)
51  CTL_PROTO(config_lazy_lock)
52  CTL_PROTO(config_malloc_conf)
53  CTL_PROTO(config_opt_safety_checks)
54  CTL_PROTO(config_prof)
55  CTL_PROTO(config_prof_libgcc)
56  CTL_PROTO(config_prof_libunwind)
57  CTL_PROTO(config_stats)
58  CTL_PROTO(config_utrace)
59  CTL_PROTO(config_xmalloc)
60  CTL_PROTO(opt_abort)
61  CTL_PROTO(opt_abort_conf)
62  CTL_PROTO(opt_confirm_conf)
63  CTL_PROTO(opt_metadata_thp)
64  CTL_PROTO(opt_retain)
65  CTL_PROTO(opt_dss)
66  CTL_PROTO(opt_narenas)
67  CTL_PROTO(opt_percpu_arena)
68  CTL_PROTO(opt_oversize_threshold)
69  CTL_PROTO(opt_background_thread)
70  CTL_PROTO(opt_max_background_threads)
71  CTL_PROTO(opt_dirty_decay_ms)
72  CTL_PROTO(opt_muzzy_decay_ms)
73  CTL_PROTO(opt_stats_print)
74  CTL_PROTO(opt_stats_print_opts)
75  CTL_PROTO(opt_junk)
76  CTL_PROTO(opt_zero)
77  CTL_PROTO(opt_utrace)
78  CTL_PROTO(opt_xmalloc)
79  CTL_PROTO(opt_tcache)
80  CTL_PROTO(opt_thp)
81  CTL_PROTO(opt_lg_extent_max_active_fit)
82  CTL_PROTO(opt_lg_tcache_max)
83  CTL_PROTO(opt_prof)
84  CTL_PROTO(opt_prof_prefix)
85  CTL_PROTO(opt_prof_active)
86  CTL_PROTO(opt_prof_thread_active_init)
87  CTL_PROTO(opt_lg_prof_sample)
88  CTL_PROTO(opt_lg_prof_interval)
89  CTL_PROTO(opt_prof_gdump)
90  CTL_PROTO(opt_prof_final)
91  CTL_PROTO(opt_prof_leak)
92  CTL_PROTO(opt_prof_accum)
93  CTL_PROTO(tcache_create)
94  CTL_PROTO(tcache_flush)
95  CTL_PROTO(tcache_destroy)
96  CTL_PROTO(arena_i_initialized)
97  CTL_PROTO(arena_i_decay)
98  CTL_PROTO(arena_i_purge)
99  CTL_PROTO(arena_i_reset)
100  CTL_PROTO(arena_i_destroy)
101  CTL_PROTO(arena_i_dss)
102  CTL_PROTO(arena_i_dirty_decay_ms)
103  CTL_PROTO(arena_i_muzzy_decay_ms)
104  CTL_PROTO(arena_i_extent_hooks)
105  CTL_PROTO(arena_i_retain_grow_limit)
106  INDEX_PROTO(arena_i)
107  CTL_PROTO(arenas_bin_i_size)
108  CTL_PROTO(arenas_bin_i_nregs)
109  CTL_PROTO(arenas_bin_i_slab_size)
110  CTL_PROTO(arenas_bin_i_nshards)
111  INDEX_PROTO(arenas_bin_i)
112  CTL_PROTO(arenas_lextent_i_size)
113  INDEX_PROTO(arenas_lextent_i)
114  CTL_PROTO(arenas_narenas)
115  CTL_PROTO(arenas_dirty_decay_ms)
116  CTL_PROTO(arenas_muzzy_decay_ms)
117  CTL_PROTO(arenas_quantum)
118  CTL_PROTO(arenas_page)
119  CTL_PROTO(arenas_tcache_max)
120  CTL_PROTO(arenas_nbins)
121  CTL_PROTO(arenas_nhbins)
122  CTL_PROTO(arenas_nlextents)
123  CTL_PROTO(arenas_create)
124  CTL_PROTO(arenas_lookup)
125  CTL_PROTO(prof_thread_active_init)
126  CTL_PROTO(prof_active)
127  CTL_PROTO(prof_dump)
128  CTL_PROTO(prof_gdump)
129  CTL_PROTO(prof_reset)
130  CTL_PROTO(prof_interval)
131  CTL_PROTO(lg_prof_sample)
132  CTL_PROTO(prof_log_start)
133  CTL_PROTO(prof_log_stop)
134  CTL_PROTO(stats_arenas_i_small_allocated)
135  CTL_PROTO(stats_arenas_i_small_nmalloc)
136  CTL_PROTO(stats_arenas_i_small_ndalloc)
137  CTL_PROTO(stats_arenas_i_small_nrequests)
138  CTL_PROTO(stats_arenas_i_small_nfills)
139  CTL_PROTO(stats_arenas_i_small_nflushes)
140  CTL_PROTO(stats_arenas_i_large_allocated)
141  CTL_PROTO(stats_arenas_i_large_nmalloc)
142  CTL_PROTO(stats_arenas_i_large_ndalloc)
143  CTL_PROTO(stats_arenas_i_large_nrequests)
144  CTL_PROTO(stats_arenas_i_large_nfills)
145  CTL_PROTO(stats_arenas_i_large_nflushes)
146  CTL_PROTO(stats_arenas_i_bins_j_nmalloc)
147  CTL_PROTO(stats_arenas_i_bins_j_ndalloc)
148  CTL_PROTO(stats_arenas_i_bins_j_nrequests)
149  CTL_PROTO(stats_arenas_i_bins_j_curregs)
150  CTL_PROTO(stats_arenas_i_bins_j_nfills)
151  CTL_PROTO(stats_arenas_i_bins_j_nflushes)
152  CTL_PROTO(stats_arenas_i_bins_j_nslabs)
153  CTL_PROTO(stats_arenas_i_bins_j_nreslabs)
154  CTL_PROTO(stats_arenas_i_bins_j_curslabs)
155  CTL_PROTO(stats_arenas_i_bins_j_nonfull_slabs)
156  INDEX_PROTO(stats_arenas_i_bins_j)
157  CTL_PROTO(stats_arenas_i_lextents_j_nmalloc)
158  CTL_PROTO(stats_arenas_i_lextents_j_ndalloc)
159  CTL_PROTO(stats_arenas_i_lextents_j_nrequests)
160  CTL_PROTO(stats_arenas_i_lextents_j_curlextents)
161  INDEX_PROTO(stats_arenas_i_lextents_j)
162  CTL_PROTO(stats_arenas_i_extents_j_ndirty)
163  CTL_PROTO(stats_arenas_i_extents_j_nmuzzy)
164  CTL_PROTO(stats_arenas_i_extents_j_nretained)
165  CTL_PROTO(stats_arenas_i_extents_j_dirty_bytes)
166  CTL_PROTO(stats_arenas_i_extents_j_muzzy_bytes)
167  CTL_PROTO(stats_arenas_i_extents_j_retained_bytes)
168  INDEX_PROTO(stats_arenas_i_extents_j)
169  CTL_PROTO(stats_arenas_i_nthreads)
170  CTL_PROTO(stats_arenas_i_uptime)
171  CTL_PROTO(stats_arenas_i_dss)
172  CTL_PROTO(stats_arenas_i_dirty_decay_ms)
173  CTL_PROTO(stats_arenas_i_muzzy_decay_ms)
174  CTL_PROTO(stats_arenas_i_pactive)
175  CTL_PROTO(stats_arenas_i_pdirty)
176  CTL_PROTO(stats_arenas_i_pmuzzy)
177  CTL_PROTO(stats_arenas_i_mapped)
178  CTL_PROTO(stats_arenas_i_retained)
179  CTL_PROTO(stats_arenas_i_extent_avail)
180  CTL_PROTO(stats_arenas_i_dirty_npurge)
181  CTL_PROTO(stats_arenas_i_dirty_nmadvise)
182  CTL_PROTO(stats_arenas_i_dirty_purged)
183  CTL_PROTO(stats_arenas_i_muzzy_npurge)
184  CTL_PROTO(stats_arenas_i_muzzy_nmadvise)
185  CTL_PROTO(stats_arenas_i_muzzy_purged)
186  CTL_PROTO(stats_arenas_i_base)
187  CTL_PROTO(stats_arenas_i_internal)
188  CTL_PROTO(stats_arenas_i_metadata_thp)
189  CTL_PROTO(stats_arenas_i_tcache_bytes)
190  CTL_PROTO(stats_arenas_i_resident)
191  CTL_PROTO(stats_arenas_i_abandoned_vm)
192  INDEX_PROTO(stats_arenas_i)
193  CTL_PROTO(stats_allocated)
194  CTL_PROTO(stats_active)
195  CTL_PROTO(stats_background_thread_num_threads)
196  CTL_PROTO(stats_background_thread_num_runs)
197  CTL_PROTO(stats_background_thread_run_interval)
198  CTL_PROTO(stats_metadata)
199  CTL_PROTO(stats_metadata_thp)
200  CTL_PROTO(stats_resident)
201  CTL_PROTO(stats_mapped)
202  CTL_PROTO(stats_retained)
203  CTL_PROTO(experimental_hooks_install)
204  CTL_PROTO(experimental_hooks_remove)
205  CTL_PROTO(experimental_utilization_query)
206  CTL_PROTO(experimental_utilization_batch_query)
207  CTL_PROTO(experimental_arenas_i_pactivep)
208  INDEX_PROTO(experimental_arenas_i)
209  #define MUTEX_STATS_CTL_PROTO_GEN(n)					\
210  CTL_PROTO(stats_##n##_num_ops)						\
211  CTL_PROTO(stats_##n##_num_wait)						\
212  CTL_PROTO(stats_##n##_num_spin_acq)					\
213  CTL_PROTO(stats_##n##_num_owner_switch)					\
214  CTL_PROTO(stats_##n##_total_wait_time)					\
215  CTL_PROTO(stats_##n##_max_wait_time)					\
216  CTL_PROTO(stats_##n##_max_num_thds)
217  #define OP(mtx) MUTEX_STATS_CTL_PROTO_GEN(mutexes_##mtx)
218  MUTEX_PROF_GLOBAL_MUTEXES
219  #undef OP
220  #define OP(mtx) MUTEX_STATS_CTL_PROTO_GEN(arenas_i_mutexes_##mtx)
221  MUTEX_PROF_ARENA_MUTEXES
222  #undef OP
223  MUTEX_STATS_CTL_PROTO_GEN(arenas_i_bins_j_mutex)
224  #undef MUTEX_STATS_CTL_PROTO_GEN
225  CTL_PROTO(stats_mutexes_reset)
226  #define NAME(n)	{true},	n
227  #define CHILD(t, c)							\
228  	sizeof(c##_node) / sizeof(ctl_##t##_node_t),			\
229  	(ctl_node_t *)c##_node,						\
230  	NULL
231  #define CTL(c)	0, NULL, c##_ctl
232  #define INDEX(i)	{false},	i##_index
233  static const ctl_named_node_t	thread_tcache_node[] = {
234  	{NAME("enabled"),	CTL(thread_tcache_enabled)},
235  	{NAME("flush"),		CTL(thread_tcache_flush)}
236  };
237  static const ctl_named_node_t	thread_prof_node[] = {
238  	{NAME("name"),		CTL(thread_prof_name)},
239  	{NAME("active"),	CTL(thread_prof_active)}
240  };
241  static const ctl_named_node_t	thread_node[] = {
242  	{NAME("arena"),		CTL(thread_arena)},
243  	{NAME("allocated"),	CTL(thread_allocated)},
244  	{NAME("allocatedp"),	CTL(thread_allocatedp)},
245  	{NAME("deallocated"),	CTL(thread_deallocated)},
246  	{NAME("deallocatedp"),	CTL(thread_deallocatedp)},
247  	{NAME("tcache"),	CHILD(named, thread_tcache)},
248  	{NAME("prof"),		CHILD(named, thread_prof)}
249  };
250  static const ctl_named_node_t	config_node[] = {
251  	{NAME("cache_oblivious"), CTL(config_cache_oblivious)},
252  	{NAME("debug"),		CTL(config_debug)},
253  	{NAME("fill"),		CTL(config_fill)},
254  	{NAME("lazy_lock"),	CTL(config_lazy_lock)},
255  	{NAME("malloc_conf"),	CTL(config_malloc_conf)},
256  	{NAME("opt_safety_checks"),	CTL(config_opt_safety_checks)},
257  	{NAME("prof"),		CTL(config_prof)},
258  	{NAME("prof_libgcc"),	CTL(config_prof_libgcc)},
259  	{NAME("prof_libunwind"), CTL(config_prof_libunwind)},
260  	{NAME("stats"),		CTL(config_stats)},
261  	{NAME("utrace"),	CTL(config_utrace)},
262  	{NAME("xmalloc"),	CTL(config_xmalloc)}
263  };
264  static const ctl_named_node_t opt_node[] = {
265  	{NAME("abort"),		CTL(opt_abort)},
266  	{NAME("abort_conf"),	CTL(opt_abort_conf)},
267  	{NAME("confirm_conf"),	CTL(opt_confirm_conf)},
268  	{NAME("metadata_thp"),	CTL(opt_metadata_thp)},
269  	{NAME("retain"),	CTL(opt_retain)},
270  	{NAME("dss"),		CTL(opt_dss)},
271  	{NAME("narenas"),	CTL(opt_narenas)},
272  	{NAME("percpu_arena"),	CTL(opt_percpu_arena)},
273  	{NAME("oversize_threshold"),	CTL(opt_oversize_threshold)},
274  	{NAME("background_thread"),	CTL(opt_background_thread)},
275  	{NAME("max_background_threads"),	CTL(opt_max_background_threads)},
276  	{NAME("dirty_decay_ms"), CTL(opt_dirty_decay_ms)},
277  	{NAME("muzzy_decay_ms"), CTL(opt_muzzy_decay_ms)},
278  	{NAME("stats_print"),	CTL(opt_stats_print)},
279  	{NAME("stats_print_opts"),	CTL(opt_stats_print_opts)},
280  	{NAME("junk"),		CTL(opt_junk)},
281  	{NAME("zero"),		CTL(opt_zero)},
282  	{NAME("utrace"),	CTL(opt_utrace)},
283  	{NAME("xmalloc"),	CTL(opt_xmalloc)},
284  	{NAME("tcache"),	CTL(opt_tcache)},
285  	{NAME("thp"),		CTL(opt_thp)},
286  	{NAME("lg_extent_max_active_fit"), CTL(opt_lg_extent_max_active_fit)},
287  	{NAME("lg_tcache_max"),	CTL(opt_lg_tcache_max)},
288  	{NAME("prof"),		CTL(opt_prof)},
289  	{NAME("prof_prefix"),	CTL(opt_prof_prefix)},
290  	{NAME("prof_active"),	CTL(opt_prof_active)},
291  	{NAME("prof_thread_active_init"), CTL(opt_prof_thread_active_init)},
292  	{NAME("lg_prof_sample"), CTL(opt_lg_prof_sample)},
293  	{NAME("lg_prof_interval"), CTL(opt_lg_prof_interval)},
294  	{NAME("prof_gdump"),	CTL(opt_prof_gdump)},
295  	{NAME("prof_final"),	CTL(opt_prof_final)},
296  	{NAME("prof_leak"),	CTL(opt_prof_leak)},
297  	{NAME("prof_accum"),	CTL(opt_prof_accum)}
298  };
299  static const ctl_named_node_t	tcache_node[] = {
300  	{NAME("create"),	CTL(tcache_create)},
301  	{NAME("flush"),		CTL(tcache_flush)},
302  	{NAME("destroy"),	CTL(tcache_destroy)}
303  };
304  static const ctl_named_node_t arena_i_node[] = {
305  	{NAME("initialized"),	CTL(arena_i_initialized)},
306  	{NAME("decay"),		CTL(arena_i_decay)},
307  	{NAME("purge"),		CTL(arena_i_purge)},
308  	{NAME("reset"),		CTL(arena_i_reset)},
309  	{NAME("destroy"),	CTL(arena_i_destroy)},
310  	{NAME("dss"),		CTL(arena_i_dss)},
311  	{NAME("dirty_decay_ms"), CTL(arena_i_dirty_decay_ms)},
312  	{NAME("muzzy_decay_ms"), CTL(arena_i_muzzy_decay_ms)},
313  	{NAME("extent_hooks"),	CTL(arena_i_extent_hooks)},
314  	{NAME("retain_grow_limit"),	CTL(arena_i_retain_grow_limit)}
315  };
316  static const ctl_named_node_t super_arena_i_node[] = {
317  	{NAME(""),		CHILD(named, arena_i)}
318  };
319  static const ctl_indexed_node_t arena_node[] = {
320  	{INDEX(arena_i)}
321  };
322  static const ctl_named_node_t arenas_bin_i_node[] = {
323  	{NAME("size"),		CTL(arenas_bin_i_size)},
324  	{NAME("nregs"),		CTL(arenas_bin_i_nregs)},
325  	{NAME("slab_size"),	CTL(arenas_bin_i_slab_size)},
326  	{NAME("nshards"),	CTL(arenas_bin_i_nshards)}
327  };
328  static const ctl_named_node_t super_arenas_bin_i_node[] = {
329  	{NAME(""),		CHILD(named, arenas_bin_i)}
330  };
331  static const ctl_indexed_node_t arenas_bin_node[] = {
332  	{INDEX(arenas_bin_i)}
333  };
334  static const ctl_named_node_t arenas_lextent_i_node[] = {
335  	{NAME("size"),		CTL(arenas_lextent_i_size)}
336  };
337  static const ctl_named_node_t super_arenas_lextent_i_node[] = {
338  	{NAME(""),		CHILD(named, arenas_lextent_i)}
339  };
340  static const ctl_indexed_node_t arenas_lextent_node[] = {
341  	{INDEX(arenas_lextent_i)}
342  };
343  static const ctl_named_node_t arenas_node[] = {
344  	{NAME("narenas"),	CTL(arenas_narenas)},
345  	{NAME("dirty_decay_ms"), CTL(arenas_dirty_decay_ms)},
346  	{NAME("muzzy_decay_ms"), CTL(arenas_muzzy_decay_ms)},
347  	{NAME("quantum"),	CTL(arenas_quantum)},
348  	{NAME("page"),		CTL(arenas_page)},
349  	{NAME("tcache_max"),	CTL(arenas_tcache_max)},
350  	{NAME("nbins"),		CTL(arenas_nbins)},
351  	{NAME("nhbins"),	CTL(arenas_nhbins)},
352  	{NAME("bin"),		CHILD(indexed, arenas_bin)},
353  	{NAME("nlextents"),	CTL(arenas_nlextents)},
354  	{NAME("lextent"),	CHILD(indexed, arenas_lextent)},
355  	{NAME("create"),	CTL(arenas_create)},
356  	{NAME("lookup"),	CTL(arenas_lookup)}
357  };
358  static const ctl_named_node_t	prof_node[] = {
359  	{NAME("thread_active_init"), CTL(prof_thread_active_init)},
360  	{NAME("active"),	CTL(prof_active)},
361  	{NAME("dump"),		CTL(prof_dump)},
362  	{NAME("gdump"),		CTL(prof_gdump)},
363  	{NAME("reset"),		CTL(prof_reset)},
364  	{NAME("interval"),	CTL(prof_interval)},
365  	{NAME("lg_sample"),	CTL(lg_prof_sample)},
366  	{NAME("log_start"),	CTL(prof_log_start)},
367  	{NAME("log_stop"),	CTL(prof_log_stop)}
368  };
369  static const ctl_named_node_t stats_arenas_i_small_node[] = {
370  	{NAME("allocated"),	CTL(stats_arenas_i_small_allocated)},
371  	{NAME("nmalloc"),	CTL(stats_arenas_i_small_nmalloc)},
372  	{NAME("ndalloc"),	CTL(stats_arenas_i_small_ndalloc)},
373  	{NAME("nrequests"),	CTL(stats_arenas_i_small_nrequests)},
374  	{NAME("nfills"),	CTL(stats_arenas_i_small_nfills)},
375  	{NAME("nflushes"),	CTL(stats_arenas_i_small_nflushes)}
376  };
377  static const ctl_named_node_t stats_arenas_i_large_node[] = {
378  	{NAME("allocated"),	CTL(stats_arenas_i_large_allocated)},
379  	{NAME("nmalloc"),	CTL(stats_arenas_i_large_nmalloc)},
380  	{NAME("ndalloc"),	CTL(stats_arenas_i_large_ndalloc)},
381  	{NAME("nrequests"),	CTL(stats_arenas_i_large_nrequests)},
382  	{NAME("nfills"),	CTL(stats_arenas_i_large_nfills)},
383  	{NAME("nflushes"),	CTL(stats_arenas_i_large_nflushes)}
384  };
385  #define MUTEX_PROF_DATA_NODE(prefix)					\
386  static const ctl_named_node_t stats_##prefix##_node[] = {		\
387  	{NAME("num_ops"),						\
388  	 CTL(stats_##prefix##_num_ops)},				\
389  	{NAME("num_wait"),						\
390  	 CTL(stats_##prefix##_num_wait)},				\
391  	{NAME("num_spin_acq"),						\
392  	 CTL(stats_##prefix##_num_spin_acq)},				\
393  	{NAME("num_owner_switch"),					\
394  	 CTL(stats_##prefix##_num_owner_switch)},			\
395  	{NAME("total_wait_time"),					\
396  	 CTL(stats_##prefix##_total_wait_time)},			\
397  	{NAME("max_wait_time"),						\
398  	 CTL(stats_##prefix##_max_wait_time)},				\
399  	{NAME("max_num_thds"),						\
400  	 CTL(stats_##prefix##_max_num_thds)}				\
401  		\
402  };
403  MUTEX_PROF_DATA_NODE(arenas_i_bins_j_mutex)
404  static const ctl_named_node_t stats_arenas_i_bins_j_node[] = {
405  	{NAME("nmalloc"),	CTL(stats_arenas_i_bins_j_nmalloc)},
406  	{NAME("ndalloc"),	CTL(stats_arenas_i_bins_j_ndalloc)},
407  	{NAME("nrequests"),	CTL(stats_arenas_i_bins_j_nrequests)},
408  	{NAME("curregs"),	CTL(stats_arenas_i_bins_j_curregs)},
409  	{NAME("nfills"),	CTL(stats_arenas_i_bins_j_nfills)},
410  	{NAME("nflushes"),	CTL(stats_arenas_i_bins_j_nflushes)},
411  	{NAME("nslabs"),	CTL(stats_arenas_i_bins_j_nslabs)},
412  	{NAME("nreslabs"),	CTL(stats_arenas_i_bins_j_nreslabs)},
413  	{NAME("curslabs"),	CTL(stats_arenas_i_bins_j_curslabs)},
414  	{NAME("nonfull_slabs"),	CTL(stats_arenas_i_bins_j_nonfull_slabs)},
415  	{NAME("mutex"),		CHILD(named, stats_arenas_i_bins_j_mutex)}
416  };
417  static const ctl_named_node_t super_stats_arenas_i_bins_j_node[] = {
418  	{NAME(""),		CHILD(named, stats_arenas_i_bins_j)}
419  };
420  static const ctl_indexed_node_t stats_arenas_i_bins_node[] = {
421  	{INDEX(stats_arenas_i_bins_j)}
422  };
423  static const ctl_named_node_t stats_arenas_i_lextents_j_node[] = {
424  	{NAME("nmalloc"),	CTL(stats_arenas_i_lextents_j_nmalloc)},
425  	{NAME("ndalloc"),	CTL(stats_arenas_i_lextents_j_ndalloc)},
426  	{NAME("nrequests"),	CTL(stats_arenas_i_lextents_j_nrequests)},
427  	{NAME("curlextents"),	CTL(stats_arenas_i_lextents_j_curlextents)}
428  };
429  static const ctl_named_node_t super_stats_arenas_i_lextents_j_node[] = {
430  	{NAME(""),		CHILD(named, stats_arenas_i_lextents_j)}
431  };
432  static const ctl_indexed_node_t stats_arenas_i_lextents_node[] = {
433  	{INDEX(stats_arenas_i_lextents_j)}
434  };
435  static const ctl_named_node_t stats_arenas_i_extents_j_node[] = {
436  	{NAME("ndirty"),	CTL(stats_arenas_i_extents_j_ndirty)},
437  	{NAME("nmuzzy"),	CTL(stats_arenas_i_extents_j_nmuzzy)},
438  	{NAME("nretained"),	CTL(stats_arenas_i_extents_j_nretained)},
439  	{NAME("dirty_bytes"),	CTL(stats_arenas_i_extents_j_dirty_bytes)},
440  	{NAME("muzzy_bytes"),	CTL(stats_arenas_i_extents_j_muzzy_bytes)},
441  	{NAME("retained_bytes"), CTL(stats_arenas_i_extents_j_retained_bytes)}
442  };
443  static const ctl_named_node_t super_stats_arenas_i_extents_j_node[] = {
444  	{NAME(""),		CHILD(named, stats_arenas_i_extents_j)}
445  };
446  static const ctl_indexed_node_t stats_arenas_i_extents_node[] = {
447  	{INDEX(stats_arenas_i_extents_j)}
448  };
449  #define OP(mtx)  MUTEX_PROF_DATA_NODE(arenas_i_mutexes_##mtx)
450  MUTEX_PROF_ARENA_MUTEXES
451  #undef OP
452  static const ctl_named_node_t stats_arenas_i_mutexes_node[] = {
453  #define OP(mtx) {NAME(#mtx), CHILD(named, stats_arenas_i_mutexes_##mtx)},
454  MUTEX_PROF_ARENA_MUTEXES
455  #undef OP
456  };
457  static const ctl_named_node_t stats_arenas_i_node[] = {
458  	{NAME("nthreads"),	CTL(stats_arenas_i_nthreads)},
459  	{NAME("uptime"),	CTL(stats_arenas_i_uptime)},
460  	{NAME("dss"),		CTL(stats_arenas_i_dss)},
461  	{NAME("dirty_decay_ms"), CTL(stats_arenas_i_dirty_decay_ms)},
462  	{NAME("muzzy_decay_ms"), CTL(stats_arenas_i_muzzy_decay_ms)},
463  	{NAME("pactive"),	CTL(stats_arenas_i_pactive)},
464  	{NAME("pdirty"),	CTL(stats_arenas_i_pdirty)},
465  	{NAME("pmuzzy"),	CTL(stats_arenas_i_pmuzzy)},
466  	{NAME("mapped"),	CTL(stats_arenas_i_mapped)},
467  	{NAME("retained"),	CTL(stats_arenas_i_retained)},
468  	{NAME("extent_avail"),	CTL(stats_arenas_i_extent_avail)},
469  	{NAME("dirty_npurge"),	CTL(stats_arenas_i_dirty_npurge)},
470  	{NAME("dirty_nmadvise"), CTL(stats_arenas_i_dirty_nmadvise)},
471  	{NAME("dirty_purged"),	CTL(stats_arenas_i_dirty_purged)},
472  	{NAME("muzzy_npurge"),	CTL(stats_arenas_i_muzzy_npurge)},
473  	{NAME("muzzy_nmadvise"), CTL(stats_arenas_i_muzzy_nmadvise)},
474  	{NAME("muzzy_purged"),	CTL(stats_arenas_i_muzzy_purged)},
475  	{NAME("base"),		CTL(stats_arenas_i_base)},
476  	{NAME("internal"),	CTL(stats_arenas_i_internal)},
477  	{NAME("metadata_thp"),	CTL(stats_arenas_i_metadata_thp)},
478  	{NAME("tcache_bytes"),	CTL(stats_arenas_i_tcache_bytes)},
479  	{NAME("resident"),	CTL(stats_arenas_i_resident)},
480  	{NAME("abandoned_vm"),	CTL(stats_arenas_i_abandoned_vm)},
481  	{NAME("small"),		CHILD(named, stats_arenas_i_small)},
482  	{NAME("large"),		CHILD(named, stats_arenas_i_large)},
483  	{NAME("bins"),		CHILD(indexed, stats_arenas_i_bins)},
484  	{NAME("lextents"),	CHILD(indexed, stats_arenas_i_lextents)},
485  	{NAME("extents"),	CHILD(indexed, stats_arenas_i_extents)},
486  	{NAME("mutexes"),	CHILD(named, stats_arenas_i_mutexes)}
487  };
488  static const ctl_named_node_t super_stats_arenas_i_node[] = {
489  	{NAME(""),		CHILD(named, stats_arenas_i)}
490  };
491  static const ctl_indexed_node_t stats_arenas_node[] = {
492  	{INDEX(stats_arenas_i)}
493  };
494  static const ctl_named_node_t stats_background_thread_node[] = {
495  	{NAME("num_threads"),	CTL(stats_background_thread_num_threads)},
496  	{NAME("num_runs"),	CTL(stats_background_thread_num_runs)},
497  	{NAME("run_interval"),	CTL(stats_background_thread_run_interval)}
498  };
499  #define OP(mtx) MUTEX_PROF_DATA_NODE(mutexes_##mtx)
500  MUTEX_PROF_GLOBAL_MUTEXES
501  #undef OP
502  static const ctl_named_node_t stats_mutexes_node[] = {
503  #define OP(mtx) {NAME(#mtx), CHILD(named, stats_mutexes_##mtx)},
504  MUTEX_PROF_GLOBAL_MUTEXES
505  #undef OP
506  	{NAME("reset"),		CTL(stats_mutexes_reset)}
507  };
508  #undef MUTEX_PROF_DATA_NODE
509  static const ctl_named_node_t stats_node[] = {
510  	{NAME("allocated"),	CTL(stats_allocated)},
511  	{NAME("active"),	CTL(stats_active)},
512  	{NAME("metadata"),	CTL(stats_metadata)},
513  	{NAME("metadata_thp"),	CTL(stats_metadata_thp)},
514  	{NAME("resident"),	CTL(stats_resident)},
515  	{NAME("mapped"),	CTL(stats_mapped)},
516  	{NAME("retained"),	CTL(stats_retained)},
517  	{NAME("background_thread"),
518  	 CHILD(named, stats_background_thread)},
519  	{NAME("mutexes"),	CHILD(named, stats_mutexes)},
520  	{NAME("arenas"),	CHILD(indexed, stats_arenas)}
521  };
522  static const ctl_named_node_t experimental_hooks_node[] = {
523  	{NAME("install"),	CTL(experimental_hooks_install)},
524  	{NAME("remove"),	CTL(experimental_hooks_remove)}
525  };
526  static const ctl_named_node_t experimental_utilization_node[] = {
527  	{NAME("query"),		CTL(experimental_utilization_query)},
528  	{NAME("batch_query"),	CTL(experimental_utilization_batch_query)}
529  };
530  static const ctl_named_node_t experimental_arenas_i_node[] = {
531  	{NAME("pactivep"),	CTL(experimental_arenas_i_pactivep)}
532  };
533  static const ctl_named_node_t super_experimental_arenas_i_node[] = {
534  	{NAME(""),		CHILD(named, experimental_arenas_i)}
535  };
536  static const ctl_indexed_node_t experimental_arenas_node[] = {
537  	{INDEX(experimental_arenas_i)}
538  };
539  static const ctl_named_node_t experimental_node[] = {
540  	{NAME("hooks"),		CHILD(named, experimental_hooks)},
541  	{NAME("utilization"),	CHILD(named, experimental_utilization)},
542  	{NAME("arenas"),	CHILD(indexed, experimental_arenas)}
543  };
544  static const ctl_named_node_t	root_node[] = {
545  	{NAME("version"),	CTL(version)},
546  	{NAME("epoch"),		CTL(epoch)},
547  	{NAME("background_thread"),	CTL(background_thread)},
548  	{NAME("max_background_threads"),	CTL(max_background_threads)},
549  	{NAME("thread"),	CHILD(named, thread)},
550  	{NAME("config"),	CHILD(named, config)},
551  	{NAME("opt"),		CHILD(named, opt)},
552  	{NAME("tcache"),	CHILD(named, tcache)},
553  	{NAME("arena"),		CHILD(indexed, arena)},
554  	{NAME("arenas"),	CHILD(named, arenas)},
555  	{NAME("prof"),		CHILD(named, prof)},
556  	{NAME("stats"),		CHILD(named, stats)},
557  	{NAME("experimental"),	CHILD(named, experimental)}
558  };
559  static const ctl_named_node_t super_root_node[] = {
560  	{NAME(""),		CHILD(named, root)}
561  };
562  #undef NAME
563  #undef CHILD
564  #undef CTL
565  #undef INDEX
566  static void
567  ctl_accum_arena_stats_u64(arena_stats_u64_t *dst, arena_stats_u64_t *src) {
568  #ifdef JEMALLOC_ATOMIC_U64
569  	uint64_t cur_dst = atomic_load_u64(dst, ATOMIC_RELAXED);
570  	uint64_t cur_src = atomic_load_u64(src, ATOMIC_RELAXED);
571  	atomic_store_u64(dst, cur_dst + cur_src, ATOMIC_RELAXED);
572  #else
573  	*dst += *src;
574  #endif
575  }
576  static uint64_t
577  ctl_arena_stats_read_u64(arena_stats_u64_t *p) {
578  #ifdef JEMALLOC_ATOMIC_U64
579  	return atomic_load_u64(p, ATOMIC_RELAXED);
580  #else
581  	return *p;
582  #endif
583  }
584  static void
585  accum_atomic_zu(atomic_zu_t *dst, atomic_zu_t *src) {
586  	size_t cur_dst = atomic_load_zu(dst, ATOMIC_RELAXED);
587  	size_t cur_src = atomic_load_zu(src, ATOMIC_RELAXED);
588  	atomic_store_zu(dst, cur_dst + cur_src, ATOMIC_RELAXED);
589  }
590  static unsigned
591  arenas_i2a_impl(size_t i, bool compat, bool validate) {
592  	unsigned a;
593  	switch (i) {
594  	case MALLCTL_ARENAS_ALL:
595  		a = 0;
596  		break;
597  	case MALLCTL_ARENAS_DESTROYED:
598  		a = 1;
599  		break;
600  	default:
601  		if (compat && i == ctl_arenas->narenas) {
602  			a = 0;
603  		} else if (validate && i >= ctl_arenas->narenas) {
604  			a = UINT_MAX;
605  		} else {
606  			assert(i < ctl_arenas->narenas || (!validate && i ==
607  			    ctl_arenas->narenas));
608  			a = (unsigned)i + 2;
609  		}
610  		break;
611  	}
612  	return a;
613  }
614  static unsigned
615  arenas_i2a(size_t i) {
616  	return arenas_i2a_impl(i, true, false);
617  }
618  static ctl_arena_t *
619  arenas_i_impl(tsd_t *tsd, size_t i, bool compat, bool init) {
620  	ctl_arena_t *ret;
621  	assert(!compat || !init);
622  	ret = ctl_arenas->arenas[arenas_i2a_impl(i, compat, false)];
623  	if (init && ret == NULL) {
624  		if (config_stats) {
625  			struct container_s {
626  				ctl_arena_t		ctl_arena;
627  				ctl_arena_stats_t	astats;
628  			};
629  			struct container_s *cont =
630  			    (struct container_s *)base_alloc(tsd_tsdn(tsd),
631  			    b0get(), sizeof(struct container_s), QUANTUM);
632  			if (cont == NULL) {
633  				return NULL;
634  			}
635  			ret = &cont->ctl_arena;
636  			ret->astats = &cont->astats;
637  		} else {
638  			ret = (ctl_arena_t *)base_alloc(tsd_tsdn(tsd), b0get(),
639  			    sizeof(ctl_arena_t), QUANTUM);
640  			if (ret == NULL) {
641  				return NULL;
642  			}
643  		}
644  		ret->arena_ind = (unsigned)i;
645  		ctl_arenas->arenas[arenas_i2a_impl(i, compat, false)] = ret;
646  	}
647  	assert(ret == NULL || arenas_i2a(ret->arena_ind) == arenas_i2a(i));
648  	return ret;
649  }
650  static ctl_arena_t *
651  arenas_i(size_t i) {
652  	ctl_arena_t *ret = arenas_i_impl(tsd_fetch(), i, true, false);
653  	assert(ret != NULL);
654  	return ret;
655  }
656  static void
657  ctl_arena_clear(ctl_arena_t *ctl_arena) {
658  	ctl_arena->nthreads = 0;
659  	ctl_arena->dss = dss_prec_names[dss_prec_limit];
660  	ctl_arena->dirty_decay_ms = -1;
661  	ctl_arena->muzzy_decay_ms = -1;
662  	ctl_arena->pactive = 0;
663  	ctl_arena->pdirty = 0;
664  	ctl_arena->pmuzzy = 0;
665  	if (config_stats) {
666  		memset(&ctl_arena->astats->astats, 0, sizeof(arena_stats_t));
667  		ctl_arena->astats->allocated_small = 0;
668  		ctl_arena->astats->nmalloc_small = 0;
669  		ctl_arena->astats->ndalloc_small = 0;
670  		ctl_arena->astats->nrequests_small = 0;
671  		ctl_arena->astats->nfills_small = 0;
672  		ctl_arena->astats->nflushes_small = 0;
673  		memset(ctl_arena->astats->bstats, 0, SC_NBINS *
674  		    sizeof(bin_stats_t));
675  		memset(ctl_arena->astats->lstats, 0, (SC_NSIZES - SC_NBINS) *
676  		    sizeof(arena_stats_large_t));
677  		memset(ctl_arena->astats->estats, 0, SC_NPSIZES *
678  		    sizeof(arena_stats_extents_t));
679  	}
680  }
681  static void
682  ctl_arena_stats_amerge(tsdn_t *tsdn, ctl_arena_t *ctl_arena, arena_t *arena) {
683  	unsigned i;
684  	if (config_stats) {
685  		arena_stats_merge(tsdn, arena, &ctl_arena->nthreads,
686  		    &ctl_arena->dss, &ctl_arena->dirty_decay_ms,
687  		    &ctl_arena->muzzy_decay_ms, &ctl_arena->pactive,
688  		    &ctl_arena->pdirty, &ctl_arena->pmuzzy,
689  		    &ctl_arena->astats->astats, ctl_arena->astats->bstats,
690  		    ctl_arena->astats->lstats, ctl_arena->astats->estats);
691  		for (i = 0; i < SC_NBINS; i++) {
692  			ctl_arena->astats->allocated_small +=
693  			    ctl_arena->astats->bstats[i].curregs *
694  			    sz_index2size(i);
695  			ctl_arena->astats->nmalloc_small +=
696  			    ctl_arena->astats->bstats[i].nmalloc;
697  			ctl_arena->astats->ndalloc_small +=
698  			    ctl_arena->astats->bstats[i].ndalloc;
699  			ctl_arena->astats->nrequests_small +=
700  			    ctl_arena->astats->bstats[i].nrequests;
701  			ctl_arena->astats->nfills_small +=
702  			    ctl_arena->astats->bstats[i].nfills;
703  			ctl_arena->astats->nflushes_small +=
704  			    ctl_arena->astats->bstats[i].nflushes;
705  		}
706  	} else {
707  		arena_basic_stats_merge(tsdn, arena, &ctl_arena->nthreads,
708  		    &ctl_arena->dss, &ctl_arena->dirty_decay_ms,
709  		    &ctl_arena->muzzy_decay_ms, &ctl_arena->pactive,
710  		    &ctl_arena->pdirty, &ctl_arena->pmuzzy);
711  	}
712  }
713  static void
714  ctl_arena_stats_sdmerge(ctl_arena_t *ctl_sdarena, ctl_arena_t *ctl_arena,
715      bool destroyed) {
716  	unsigned i;
717  	if (!destroyed) {
718  		ctl_sdarena->nthreads += ctl_arena->nthreads;
719  		ctl_sdarena->pactive += ctl_arena->pactive;
720  		ctl_sdarena->pdirty += ctl_arena->pdirty;
721  		ctl_sdarena->pmuzzy += ctl_arena->pmuzzy;
722  	} else {
723  		assert(ctl_arena->nthreads == 0);
724  		assert(ctl_arena->pactive == 0);
725  		assert(ctl_arena->pdirty == 0);
726  		assert(ctl_arena->pmuzzy == 0);
727  	}
728  	if (config_stats) {
729  		ctl_arena_stats_t *sdstats = ctl_sdarena->astats;
730  		ctl_arena_stats_t *astats = ctl_arena->astats;
731  		if (!destroyed) {
732  			accum_atomic_zu(&sdstats->astats.mapped,
733  			    &astats->astats.mapped);
734  			accum_atomic_zu(&sdstats->astats.retained,
735  			    &astats->astats.retained);
736  			accum_atomic_zu(&sdstats->astats.extent_avail,
737  			    &astats->astats.extent_avail);
738  		}
739  		ctl_accum_arena_stats_u64(&sdstats->astats.decay_dirty.npurge,
740  		    &astats->astats.decay_dirty.npurge);
741  		ctl_accum_arena_stats_u64(&sdstats->astats.decay_dirty.nmadvise,
742  		    &astats->astats.decay_dirty.nmadvise);
743  		ctl_accum_arena_stats_u64(&sdstats->astats.decay_dirty.purged,
744  		    &astats->astats.decay_dirty.purged);
745  		ctl_accum_arena_stats_u64(&sdstats->astats.decay_muzzy.npurge,
746  		    &astats->astats.decay_muzzy.npurge);
747  		ctl_accum_arena_stats_u64(&sdstats->astats.decay_muzzy.nmadvise,
748  		    &astats->astats.decay_muzzy.nmadvise);
749  		ctl_accum_arena_stats_u64(&sdstats->astats.decay_muzzy.purged,
750  		    &astats->astats.decay_muzzy.purged);
751  #define OP(mtx) malloc_mutex_prof_merge(				\
752  		    &(sdstats->astats.mutex_prof_data[			\
753  		        arena_prof_mutex_##mtx]),			\
754  		    &(astats->astats.mutex_prof_data[			\
755  		        arena_prof_mutex_##mtx]));
756  MUTEX_PROF_ARENA_MUTEXES
757  #undef OP
758  		if (!destroyed) {
759  			accum_atomic_zu(&sdstats->astats.base,
760  			    &astats->astats.base);
761  			accum_atomic_zu(&sdstats->astats.internal,
762  			    &astats->astats.internal);
763  			accum_atomic_zu(&sdstats->astats.resident,
764  			    &astats->astats.resident);
765  			accum_atomic_zu(&sdstats->astats.metadata_thp,
766  			    &astats->astats.metadata_thp);
767  		} else {
768  			assert(atomic_load_zu(
769  			    &astats->astats.internal, ATOMIC_RELAXED) == 0);
770  		}
771  		if (!destroyed) {
772  			sdstats->allocated_small += astats->allocated_small;
773  		} else {
774  			assert(astats->allocated_small == 0);
775  		}
776  		sdstats->nmalloc_small += astats->nmalloc_small;
777  		sdstats->ndalloc_small += astats->ndalloc_small;
778  		sdstats->nrequests_small += astats->nrequests_small;
779  		sdstats->nfills_small += astats->nfills_small;
780  		sdstats->nflushes_small += astats->nflushes_small;
781  		if (!destroyed) {
782  			accum_atomic_zu(&sdstats->astats.allocated_large,
783  			    &astats->astats.allocated_large);
784  		} else {
785  			assert(atomic_load_zu(&astats->astats.allocated_large,
786  			    ATOMIC_RELAXED) == 0);
787  		}
788  		ctl_accum_arena_stats_u64(&sdstats->astats.nmalloc_large,
789  		    &astats->astats.nmalloc_large);
790  		ctl_accum_arena_stats_u64(&sdstats->astats.ndalloc_large,
791  		    &astats->astats.ndalloc_large);
792  		ctl_accum_arena_stats_u64(&sdstats->astats.nrequests_large,
793  		    &astats->astats.nrequests_large);
794  		accum_atomic_zu(&sdstats->astats.abandoned_vm,
795  		    &astats->astats.abandoned_vm);
796  		accum_atomic_zu(&sdstats->astats.tcache_bytes,
797  		    &astats->astats.tcache_bytes);
798  		if (ctl_arena->arena_ind == 0) {
799  			sdstats->astats.uptime = astats->astats.uptime;
800  		}
801  		for (i = 0; i < SC_NBINS; i++) {
802  			sdstats->bstats[i].nmalloc += astats->bstats[i].nmalloc;
803  			sdstats->bstats[i].ndalloc += astats->bstats[i].ndalloc;
804  			sdstats->bstats[i].nrequests +=
805  			    astats->bstats[i].nrequests;
806  			if (!destroyed) {
807  				sdstats->bstats[i].curregs +=
808  				    astats->bstats[i].curregs;
809  			} else {
810  				assert(astats->bstats[i].curregs == 0);
811  			}
812  			sdstats->bstats[i].nfills += astats->bstats[i].nfills;
813  			sdstats->bstats[i].nflushes +=
814  			    astats->bstats[i].nflushes;
815  			sdstats->bstats[i].nslabs += astats->bstats[i].nslabs;
816  			sdstats->bstats[i].reslabs += astats->bstats[i].reslabs;
817  			if (!destroyed) {
818  				sdstats->bstats[i].curslabs +=
819  				    astats->bstats[i].curslabs;
820  				sdstats->bstats[i].nonfull_slabs +=
821  				    astats->bstats[i].nonfull_slabs;
822  			} else {
823  				assert(astats->bstats[i].curslabs == 0);
824  				assert(astats->bstats[i].nonfull_slabs == 0);
825  			}
826  			malloc_mutex_prof_merge(&sdstats->bstats[i].mutex_data,
827  			    &astats->bstats[i].mutex_data);
828  		}
829  		for (i = 0; i < SC_NSIZES - SC_NBINS; i++) {
830  			ctl_accum_arena_stats_u64(&sdstats->lstats[i].nmalloc,
831  			    &astats->lstats[i].nmalloc);
832  			ctl_accum_arena_stats_u64(&sdstats->lstats[i].ndalloc,
833  			    &astats->lstats[i].ndalloc);
834  			ctl_accum_arena_stats_u64(&sdstats->lstats[i].nrequests,
835  			    &astats->lstats[i].nrequests);
836  			if (!destroyed) {
837  				sdstats->lstats[i].curlextents +=
838  				    astats->lstats[i].curlextents;
839  			} else {
840  				assert(astats->lstats[i].curlextents == 0);
841  			}
842  		}
843  		for (i = 0; i < SC_NPSIZES; i++) {
844  			accum_atomic_zu(&sdstats->estats[i].ndirty,
845  			    &astats->estats[i].ndirty);
846  			accum_atomic_zu(&sdstats->estats[i].nmuzzy,
847  			    &astats->estats[i].nmuzzy);
848  			accum_atomic_zu(&sdstats->estats[i].nretained,
849  			    &astats->estats[i].nretained);
850  			accum_atomic_zu(&sdstats->estats[i].dirty_bytes,
851  			    &astats->estats[i].dirty_bytes);
852  			accum_atomic_zu(&sdstats->estats[i].muzzy_bytes,
853  			    &astats->estats[i].muzzy_bytes);
854  			accum_atomic_zu(&sdstats->estats[i].retained_bytes,
855  			    &astats->estats[i].retained_bytes);
856  		}
857  	}
858  }
859  static void
860  ctl_arena_refresh(tsdn_t *tsdn, arena_t *arena, ctl_arena_t *ctl_sdarena,
861      unsigned i, bool destroyed) {
862  	ctl_arena_t *ctl_arena = arenas_i(i);
863  	ctl_arena_clear(ctl_arena);
864  	ctl_arena_stats_amerge(tsdn, ctl_arena, arena);
865  	ctl_arena_stats_sdmerge(ctl_sdarena, ctl_arena, destroyed);
866  }
867  static unsigned
868  ctl_arena_init(tsd_t *tsd, extent_hooks_t *extent_hooks) {
869  	unsigned arena_ind;
870  	ctl_arena_t *ctl_arena;
871  	if ((ctl_arena = ql_last(&ctl_arenas->destroyed, destroyed_link)) !=
872  	    NULL) {
873  		ql_remove(&ctl_arenas->destroyed, ctl_arena, destroyed_link);
874  		arena_ind = ctl_arena->arena_ind;
875  	} else {
876  		arena_ind = ctl_arenas->narenas;
877  	}
878  	if (arenas_i_impl(tsd, arena_ind, false, true) == NULL) {
879  		return UINT_MAX;
880  	}
881  	if (arena_init(tsd_tsdn(tsd), arena_ind, extent_hooks) == NULL) {
882  		return UINT_MAX;
883  	}
884  	if (arena_ind == ctl_arenas->narenas) {
885  		ctl_arenas->narenas++;
886  	}
887  	return arena_ind;
888  }
889  static void
890  ctl_background_thread_stats_read(tsdn_t *tsdn) {
891  	background_thread_stats_t *stats = &ctl_stats->background_thread;
892  	if (!have_background_thread ||
893  	    background_thread_stats_read(tsdn, stats)) {
894  		memset(stats, 0, sizeof(background_thread_stats_t));
895  		nstime_init(&stats->run_interval, 0);
896  	}
897  }
898  static void
899  ctl_refresh(tsdn_t *tsdn) {
900  	unsigned i;
901  	ctl_arena_t *ctl_sarena = arenas_i(MALLCTL_ARENAS_ALL);
902  	VARIABLE_ARRAY(arena_t *, tarenas, ctl_arenas->narenas);
903  	ctl_arena_clear(ctl_sarena);
904  	for (i = 0; i < ctl_arenas->narenas; i++) {
905  		tarenas[i] = arena_get(tsdn, i, false);
906  	}
907  	for (i = 0; i < ctl_arenas->narenas; i++) {
908  		ctl_arena_t *ctl_arena = arenas_i(i);
909  		bool initialized = (tarenas[i] != NULL);
910  		ctl_arena->initialized = initialized;
911  		if (initialized) {
912  			ctl_arena_refresh(tsdn, tarenas[i], ctl_sarena, i,
913  			    false);
914  		}
915  	}
916  	if (config_stats) {
917  		ctl_stats->allocated = ctl_sarena->astats->allocated_small +
918  		    atomic_load_zu(&ctl_sarena->astats->astats.allocated_large,
919  			ATOMIC_RELAXED);
920  		ctl_stats->active = (ctl_sarena->pactive << LG_PAGE);
921  		ctl_stats->metadata = atomic_load_zu(
922  		    &ctl_sarena->astats->astats.base, ATOMIC_RELAXED) +
923  		    atomic_load_zu(&ctl_sarena->astats->astats.internal,
924  			ATOMIC_RELAXED);
925  		ctl_stats->metadata_thp = atomic_load_zu(
926  		    &ctl_sarena->astats->astats.metadata_thp, ATOMIC_RELAXED);
927  		ctl_stats->resident = atomic_load_zu(
928  		    &ctl_sarena->astats->astats.resident, ATOMIC_RELAXED);
929  		ctl_stats->mapped = atomic_load_zu(
930  		    &ctl_sarena->astats->astats.mapped, ATOMIC_RELAXED);
931  		ctl_stats->retained = atomic_load_zu(
932  		    &ctl_sarena->astats->astats.retained, ATOMIC_RELAXED);
933  		ctl_background_thread_stats_read(tsdn);
934  #define READ_GLOBAL_MUTEX_PROF_DATA(i, mtx)				\
935      malloc_mutex_lock(tsdn, &mtx);					\
936      malloc_mutex_prof_read(tsdn, &ctl_stats->mutex_prof_data[i], &mtx);	\
937      malloc_mutex_unlock(tsdn, &mtx);
938  		if (config_prof && opt_prof) {
939  			READ_GLOBAL_MUTEX_PROF_DATA(global_prof_mutex_prof,
940  			    bt2gctx_mtx);
941  		}
942  		if (have_background_thread) {
943  			READ_GLOBAL_MUTEX_PROF_DATA(
944  			    global_prof_mutex_background_thread,
945  			    background_thread_lock);
946  		} else {
947  			memset(&ctl_stats->mutex_prof_data[
948  			    global_prof_mutex_background_thread], 0,
949  			    sizeof(mutex_prof_data_t));
950  		}
951  		malloc_mutex_prof_read(tsdn,
952  		    &ctl_stats->mutex_prof_data[global_prof_mutex_ctl],
953  		    &ctl_mtx);
954  #undef READ_GLOBAL_MUTEX_PROF_DATA
955  	}
956  	ctl_arenas->epoch++;
957  }
958  static bool
959  ctl_init(tsd_t *tsd) {
960  	bool ret;
961  	tsdn_t *tsdn = tsd_tsdn(tsd);
962  	malloc_mutex_lock(tsdn, &ctl_mtx);
963  	if (!ctl_initialized) {
964  		ctl_arena_t *ctl_sarena, *ctl_darena;
965  		unsigned i;
966  		if (ctl_arenas == NULL) {
967  			ctl_arenas = (ctl_arenas_t *)base_alloc(tsdn,
968  			    b0get(), sizeof(ctl_arenas_t), QUANTUM);
969  			if (ctl_arenas == NULL) {
970  				ret = true;
971  				goto label_return;
972  			}
973  		}
974  		if (config_stats && ctl_stats == NULL) {
975  			ctl_stats = (ctl_stats_t *)base_alloc(tsdn, b0get(),
976  			    sizeof(ctl_stats_t), QUANTUM);
977  			if (ctl_stats == NULL) {
978  				ret = true;
979  				goto label_return;
980  			}
981  		}
982  		if ((ctl_sarena = arenas_i_impl(tsd, MALLCTL_ARENAS_ALL, false,
983  		    true)) == NULL) {
984  			ret = true;
985  			goto label_return;
986  		}
987  		ctl_sarena->initialized = true;
988  		if ((ctl_darena = arenas_i_impl(tsd, MALLCTL_ARENAS_DESTROYED,
989  		    false, true)) == NULL) {
990  			ret = true;
991  			goto label_return;
992  		}
993  		ctl_arena_clear(ctl_darena);
994  		ctl_arenas->narenas = narenas_total_get();
995  		for (i = 0; i < ctl_arenas->narenas; i++) {
996  			if (arenas_i_impl(tsd, i, false, true) == NULL) {
997  				ret = true;
998  				goto label_return;
999  			}
1000  		}
1001  		ql_new(&ctl_arenas->destroyed);
1002  		ctl_refresh(tsdn);
1003  		ctl_initialized = true;
1004  	}
1005  	ret = false;
1006  label_return:
1007  	malloc_mutex_unlock(tsdn, &ctl_mtx);
1008  	return ret;
1009  }
1010  static int
1011  ctl_lookup(tsdn_t *tsdn, const char *name, ctl_node_t const **nodesp,
1012      size_t *mibp, size_t *depthp) {
1013  	int ret;
1014  	const char *elm, *tdot, *dot;
1015  	size_t elen, i, j;
1016  	const ctl_named_node_t *node;
1017  	elm = name;
1018  	dot = ((tdot = strchr(elm, '.')) != NULL) ? tdot : strchr(elm, '\0');
1019  	elen = (size_t)((uintptr_t)dot - (uintptr_t)elm);
1020  	if (elen == 0) {
1021  		ret = ENOENT;
1022  		goto label_return;
1023  	}
1024  	node = super_root_node;
1025  	for (i = 0; i < *depthp; i++) {
1026  		assert(node);
1027  		assert(node->nchildren > 0);
1028  		if (ctl_named_node(node->children) != NULL) {
1029  			const ctl_named_node_t *pnode = node;
1030  			for (j = 0; j < node->nchildren; j++) {
1031  				const ctl_named_node_t *child =
1032  				    ctl_named_children(node, j);
1033  				if (strlen(child->name) == elen &&
1034  				    strncmp(elm, child->name, elen) == 0) {
1035  					node = child;
1036  					if (nodesp != NULL) {
1037  						nodesp[i] =
1038  						    (const ctl_node_t *)node;
1039  					}
1040  					mibp[i] = j;
1041  					break;
1042  				}
1043  			}
1044  			if (node == pnode) {
1045  				ret = ENOENT;
1046  				goto label_return;
1047  			}
1048  		} else {
1049  			uintmax_t index;
1050  			const ctl_indexed_node_t *inode;
1051  			index = malloc_strtoumax(elm, NULL, 10);
1052  			if (index == UINTMAX_MAX || index > SIZE_T_MAX) {
1053  				ret = ENOENT;
1054  				goto label_return;
1055  			}
1056  			inode = ctl_indexed_node(node->children);
1057  			node = inode->index(tsdn, mibp, *depthp, (size_t)index);
1058  			if (node == NULL) {
1059  				ret = ENOENT;
1060  				goto label_return;
1061  			}
1062  			if (nodesp != NULL) {
1063  				nodesp[i] = (const ctl_node_t *)node;
1064  			}
1065  			mibp[i] = (size_t)index;
1066  		}
1067  		if (node->ctl != NULL) {
1068  			if (*dot != '\0') {
1069  				ret = ENOENT;
1070  				goto label_return;
1071  			}
1072  			*depthp = i + 1;
1073  			break;
1074  		}
1075  		if (*dot == '\0') {
1076  			ret = ENOENT;
1077  			goto label_return;
1078  		}
1079  		elm = &dot[1];
1080  		dot = ((tdot = strchr(elm, '.')) != NULL) ? tdot :
1081  		    strchr(elm, '\0');
1082  		elen = (size_t)((uintptr_t)dot - (uintptr_t)elm);
1083  	}
1084  	ret = 0;
1085  label_return:
1086  	return ret;
1087  }
1088  int
1089  ctl_byname(tsd_t *tsd, const char *name, void *oldp, size_t *oldlenp,
1090      void *newp, size_t newlen) {
1091  	int ret;
1092  	size_t depth;
1093  	ctl_node_t const *nodes[CTL_MAX_DEPTH];
1094  	size_t mib[CTL_MAX_DEPTH];
1095  	const ctl_named_node_t *node;
1096  	if (!ctl_initialized && ctl_init(tsd)) {
1097  		ret = EAGAIN;
1098  		goto label_return;
1099  	}
1100  	depth = CTL_MAX_DEPTH;
1101  	ret = ctl_lookup(tsd_tsdn(tsd), name, nodes, mib, &depth);
1102  	if (ret != 0) {
1103  		goto label_return;
1104  	}
1105  	node = ctl_named_node(nodes[depth-1]);
1106  	if (node != NULL && node->ctl) {
1107  		ret = node->ctl(tsd, mib, depth, oldp, oldlenp, newp, newlen);
1108  	} else {
1109  		ret = ENOENT;
1110  	}
1111  label_return:
1112  	return(ret);
1113  }
1114  int
1115  ctl_nametomib(tsd_t *tsd, const char *name, size_t *mibp, size_t *miblenp) {
1116  	int ret;
1117  	if (!ctl_initialized && ctl_init(tsd)) {
1118  		ret = EAGAIN;
1119  		goto label_return;
1120  	}
1121  	ret = ctl_lookup(tsd_tsdn(tsd), name, NULL, mibp, miblenp);
1122  label_return:
1123  	return(ret);
1124  }
1125  int
1126  ctl_bymib(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1127      size_t *oldlenp, void *newp, size_t newlen) {
1128  	int ret;
1129  	const ctl_named_node_t *node;
1130  	size_t i;
1131  	if (!ctl_initialized && ctl_init(tsd)) {
1132  		ret = EAGAIN;
1133  		goto label_return;
1134  	}
1135  	node = super_root_node;
1136  	for (i = 0; i < miblen; i++) {
1137  		assert(node);
1138  		assert(node->nchildren > 0);
1139  		if (ctl_named_node(node->children) != NULL) {
1140  			if (node->nchildren <= mib[i]) {
1141  				ret = ENOENT;
1142  				goto label_return;
1143  			}
1144  			node = ctl_named_children(node, mib[i]);
1145  		} else {
1146  			const ctl_indexed_node_t *inode;
1147  			inode = ctl_indexed_node(node->children);
1148  			node = inode->index(tsd_tsdn(tsd), mib, miblen, mib[i]);
1149  			if (node == NULL) {
1150  				ret = ENOENT;
1151  				goto label_return;
1152  			}
1153  		}
1154  	}
1155  	if (node && node->ctl) {
1156  		ret = node->ctl(tsd, mib, miblen, oldp, oldlenp, newp, newlen);
1157  	} else {
1158  		ret = ENOENT;
1159  	}
1160  label_return:
1161  	return(ret);
1162  }
1163  bool
1164  ctl_boot(void) {
1165  	if (malloc_mutex_init(&ctl_mtx, "ctl", WITNESS_RANK_CTL,
1166  	    malloc_mutex_rank_exclusive)) {
1167  		return true;
1168  	}
1169  	ctl_initialized = false;
1170  	return false;
1171  }
1172  void
1173  ctl_prefork(tsdn_t *tsdn) {
1174  	malloc_mutex_prefork(tsdn, &ctl_mtx);
1175  }
1176  void
1177  ctl_postfork_parent(tsdn_t *tsdn) {
1178  	malloc_mutex_postfork_parent(tsdn, &ctl_mtx);
1179  }
1180  void
1181  ctl_postfork_child(tsdn_t *tsdn) {
1182  	malloc_mutex_postfork_child(tsdn, &ctl_mtx);
1183  }
1184  #define READONLY()	do {						\
1185  	if (newp != NULL || newlen != 0) {				\
1186  		ret = EPERM;						\
1187  		goto label_return;					\
1188  	}								\
1189  } while (0)
1190  #define WRITEONLY()	do {						\
1191  	if (oldp != NULL || oldlenp != NULL) {				\
1192  		ret = EPERM;						\
1193  		goto label_return;					\
1194  	}								\
1195  } while (0)
1196  #define READ_XOR_WRITE()	do {					\
1197  	if ((oldp != NULL && oldlenp != NULL) && (newp != NULL ||	\
1198  	    newlen != 0)) {						\
1199  		ret = EPERM;						\
1200  		goto label_return;					\
1201  	}								\
1202  } while (0)
1203  #define READ(v, t)	do {						\
1204  	if (oldp != NULL && oldlenp != NULL) {				\
1205  		if (*oldlenp != sizeof(t)) {				\
1206  			size_t	copylen = (sizeof(t) <= *oldlenp)	\
1207  			    ? sizeof(t) : *oldlenp;			\
1208  			memcpy(oldp, (void *)&(v), copylen);		\
1209  			ret = EINVAL;					\
1210  			goto label_return;				\
1211  		}							\
1212  		*(t *)oldp = (v);					\
1213  	}								\
1214  } while (0)
1215  #define WRITE(v, t)	do {						\
1216  	if (newp != NULL) {						\
1217  		if (newlen != sizeof(t)) {				\
1218  			ret = EINVAL;					\
1219  			goto label_return;				\
1220  		}							\
1221  		(v) = *(t *)newp;					\
1222  	}								\
1223  } while (0)
1224  #define MIB_UNSIGNED(v, i) do {						\
1225  	if (mib[i] > UINT_MAX) {					\
1226  		ret = EFAULT;						\
1227  		goto label_return;					\
1228  	}								\
1229  	v = (unsigned)mib[i];						\
1230  } while (0)
1231  #define CTL_RO_CLGEN(c, l, n, v, t)					\
1232  static int								\
1233  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,	\
1234      size_t *oldlenp, void *newp, size_t newlen) {			\
1235  	int ret;							\
1236  	t oldval;							\
1237  									\
1238  	if (!(c)) {							\
1239  		return ENOENT;						\
1240  	}								\
1241  	if (l) {							\
1242  		malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);		\
1243  	}								\
1244  	READONLY();							\
1245  	oldval = (v);							\
1246  	READ(oldval, t);						\
1247  									\
1248  	ret = 0;							\
1249  label_return:								\
1250  	if (l) {							\
1251  		malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);		\
1252  	}								\
1253  	return ret;							\
1254  }
1255  #define CTL_RO_CGEN(c, n, v, t)						\
1256  static int								\
1257  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1258      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1259  	int ret;							\
1260  	t oldval;							\
1261  									\
1262  	if (!(c)) {							\
1263  		return ENOENT;						\
1264  	}								\
1265  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);			\
1266  	READONLY();							\
1267  	oldval = (v);							\
1268  	READ(oldval, t);						\
1269  									\
1270  	ret = 0;							\
1271  label_return:								\
1272  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);			\
1273  	return ret;							\
1274  }
1275  #define CTL_RO_GEN(n, v, t)						\
1276  static int								\
1277  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,	\
1278      size_t *oldlenp, void *newp, size_t newlen) {			\
1279  	int ret;							\
1280  	t oldval;							\
1281  									\
1282  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);			\
1283  	READONLY();							\
1284  	oldval = (v);							\
1285  	READ(oldval, t);						\
1286  									\
1287  	ret = 0;							\
1288  label_return:								\
1289  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);			\
1290  	return ret;							\
1291  }
1292  #define CTL_RO_NL_CGEN(c, n, v, t)					\
1293  static int								\
1294  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1295      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1296  	int ret;							\
1297  	t oldval;							\
1298  									\
1299  	if (!(c)) {							\
1300  		return ENOENT;						\
1301  	}								\
1302  	READONLY();							\
1303  	oldval = (v);							\
1304  	READ(oldval, t);						\
1305  									\
1306  	ret = 0;							\
1307  label_return:								\
1308  	return ret;							\
1309  }
1310  #define CTL_RO_NL_GEN(n, v, t)						\
1311  static int								\
1312  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1313      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1314  	int ret;							\
1315  	t oldval;							\
1316  									\
1317  	READONLY();							\
1318  	oldval = (v);							\
1319  	READ(oldval, t);						\
1320  									\
1321  	ret = 0;							\
1322  label_return:								\
1323  	return ret;							\
1324  }
1325  #define CTL_TSD_RO_NL_CGEN(c, n, m, t)					\
1326  static int								\
1327  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,	\
1328      size_t *oldlenp, void *newp, size_t newlen) {			\
1329  	int ret;							\
1330  	t oldval;							\
1331  									\
1332  	if (!(c)) {							\
1333  		return ENOENT;						\
1334  	}								\
1335  	READONLY();							\
1336  	oldval = (m(tsd));						\
1337  	READ(oldval, t);						\
1338  									\
1339  	ret = 0;							\
1340  label_return:								\
1341  	return ret;							\
1342  }
1343  #define CTL_RO_CONFIG_GEN(n, t)						\
1344  static int								\
1345  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1346      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1347  	int ret;							\
1348  	t oldval;							\
1349  									\
1350  	READONLY();							\
1351  	oldval = n;							\
1352  	READ(oldval, t);						\
1353  									\
1354  	ret = 0;							\
1355  label_return:								\
1356  	return ret;							\
1357  }
1358  CTL_RO_NL_GEN(version, JEMALLOC_VERSION, const char *)
1359  static int
1360  epoch_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1361      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1362  	int ret;
1363  	UNUSED uint64_t newval;
1364  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
1365  	WRITE(newval, uint64_t);
1366  	if (newp != NULL) {
1367  		ctl_refresh(tsd_tsdn(tsd));
1368  	}
1369  	READ(ctl_arenas->epoch, uint64_t);
1370  	ret = 0;
1371  label_return:
1372  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
1373  	return ret;
1374  }
1375  static int
1376  background_thread_ctl(tsd_t *tsd, const size_t *mib,
1377      size_t miblen, void *oldp, size_t *oldlenp,
1378      void *newp, size_t newlen) {
1379  	int ret;
1380  	bool oldval;
1381  	if (!have_background_thread) {
1382  		return ENOENT;
1383  	}
1384  	background_thread_ctl_init(tsd_tsdn(tsd));
1385  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
1386  	malloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);
1387  	if (newp == NULL) {
1388  		oldval = background_thread_enabled();
1389  		READ(oldval, bool);
1390  	} else {
1391  		if (newlen != sizeof(bool)) {
1392  			ret = EINVAL;
1393  			goto label_return;
1394  		}
1395  		oldval = background_thread_enabled();
1396  		READ(oldval, bool);
1397  		bool newval = *(bool *)newp;
1398  		if (newval == oldval) {
1399  			ret = 0;
1400  			goto label_return;
1401  		}
1402  		background_thread_enabled_set(tsd_tsdn(tsd), newval);
1403  		if (newval) {
1404  			if (background_threads_enable(tsd)) {
1405  				ret = EFAULT;
1406  				goto label_return;
1407  			}
1408  		} else {
1409  			if (background_threads_disable(tsd)) {
1410  				ret = EFAULT;
1411  				goto label_return;
1412  			}
1413  		}
1414  	}
1415  	ret = 0;
1416  label_return:
1417  	malloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);
1418  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
1419  	return ret;
1420  }
1421  static int
1422  max_background_threads_ctl(tsd_t *tsd, const size_t *mib,
1423      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1424      size_t newlen) {
1425  	int ret;
1426  	size_t oldval;
1427  	if (!have_background_thread) {
1428  		return ENOENT;
1429  	}
1430  	background_thread_ctl_init(tsd_tsdn(tsd));
1431  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
1432  	malloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);
1433  	if (newp == NULL) {
1434  		oldval = max_background_threads;
1435  		READ(oldval, size_t);
1436  	} else {
1437  		if (newlen != sizeof(size_t)) {
1438  			ret = EINVAL;
1439  			goto label_return;
1440  		}
1441  		oldval = max_background_threads;
1442  		READ(oldval, size_t);
1443  		size_t newval = *(size_t *)newp;
1444  		if (newval == oldval) {
1445  			ret = 0;
1446  			goto label_return;
1447  		}
1448  		if (newval > opt_max_background_threads) {
1449  			ret = EINVAL;
1450  			goto label_return;
1451  		}
1452  		if (background_thread_enabled()) {
1453  			background_thread_enabled_set(tsd_tsdn(tsd), false);
1454  			if (background_threads_disable(tsd)) {
1455  				ret = EFAULT;
1456  				goto label_return;
1457  			}
1458  			max_background_threads = newval;
1459  			background_thread_enabled_set(tsd_tsdn(tsd), true);
1460  			if (background_threads_enable(tsd)) {
1461  				ret = EFAULT;
1462  				goto label_return;
1463  			}
1464  		} else {
1465  			max_background_threads = newval;
1466  		}
1467  	}
1468  	ret = 0;
1469  label_return:
1470  	malloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);
1471  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
1472  	return ret;
1473  }
1474  CTL_RO_CONFIG_GEN(config_cache_oblivious, bool)
1475  CTL_RO_CONFIG_GEN(config_debug, bool)
1476  CTL_RO_CONFIG_GEN(config_fill, bool)
1477  CTL_RO_CONFIG_GEN(config_lazy_lock, bool)
1478  CTL_RO_CONFIG_GEN(config_malloc_conf, const char *)
1479  CTL_RO_CONFIG_GEN(config_opt_safety_checks, bool)
1480  CTL_RO_CONFIG_GEN(config_prof, bool)
1481  CTL_RO_CONFIG_GEN(config_prof_libgcc, bool)
1482  CTL_RO_CONFIG_GEN(config_prof_libunwind, bool)
1483  CTL_RO_CONFIG_GEN(config_stats, bool)
1484  CTL_RO_CONFIG_GEN(config_utrace, bool)
1485  CTL_RO_CONFIG_GEN(config_xmalloc, bool)
1486  CTL_RO_NL_GEN(opt_abort, opt_abort, bool)
1487  CTL_RO_NL_GEN(opt_abort_conf, opt_abort_conf, bool)
1488  CTL_RO_NL_GEN(opt_confirm_conf, opt_confirm_conf, bool)
1489  CTL_RO_NL_GEN(opt_metadata_thp, metadata_thp_mode_names[opt_metadata_thp],
1490      const char *)
1491  CTL_RO_NL_GEN(opt_retain, opt_retain, bool)
1492  CTL_RO_NL_GEN(opt_dss, opt_dss, const char *)
1493  CTL_RO_NL_GEN(opt_narenas, opt_narenas, unsigned)
1494  CTL_RO_NL_GEN(opt_percpu_arena, percpu_arena_mode_names[opt_percpu_arena],
1495      const char *)
1496  CTL_RO_NL_GEN(opt_oversize_threshold, opt_oversize_threshold, size_t)
1497  CTL_RO_NL_GEN(opt_background_thread, opt_background_thread, bool)
1498  CTL_RO_NL_GEN(opt_max_background_threads, opt_max_background_threads, size_t)
1499  CTL_RO_NL_GEN(opt_dirty_decay_ms, opt_dirty_decay_ms, ssize_t)
1500  CTL_RO_NL_GEN(opt_muzzy_decay_ms, opt_muzzy_decay_ms, ssize_t)
1501  CTL_RO_NL_GEN(opt_stats_print, opt_stats_print, bool)
1502  CTL_RO_NL_GEN(opt_stats_print_opts, opt_stats_print_opts, const char *)
1503  CTL_RO_NL_CGEN(config_fill, opt_junk, opt_junk, const char *)
1504  CTL_RO_NL_CGEN(config_fill, opt_zero, opt_zero, bool)
1505  CTL_RO_NL_CGEN(config_utrace, opt_utrace, opt_utrace, bool)
1506  CTL_RO_NL_CGEN(config_xmalloc, opt_xmalloc, opt_xmalloc, bool)
1507  CTL_RO_NL_GEN(opt_tcache, opt_tcache, bool)
1508  CTL_RO_NL_GEN(opt_thp, thp_mode_names[opt_thp], const char *)
1509  CTL_RO_NL_GEN(opt_lg_extent_max_active_fit, opt_lg_extent_max_active_fit,
1510      size_t)
1511  CTL_RO_NL_GEN(opt_lg_tcache_max, opt_lg_tcache_max, ssize_t)
1512  CTL_RO_NL_CGEN(config_prof, opt_prof, opt_prof, bool)
1513  CTL_RO_NL_CGEN(config_prof, opt_prof_prefix, opt_prof_prefix, const char *)
1514  CTL_RO_NL_CGEN(config_prof, opt_prof_active, opt_prof_active, bool)
1515  CTL_RO_NL_CGEN(config_prof, opt_prof_thread_active_init,
1516      opt_prof_thread_active_init, bool)
1517  CTL_RO_NL_CGEN(config_prof, opt_lg_prof_sample, opt_lg_prof_sample, size_t)
1518  CTL_RO_NL_CGEN(config_prof, opt_prof_accum, opt_prof_accum, bool)
1519  CTL_RO_NL_CGEN(config_prof, opt_lg_prof_interval, opt_lg_prof_interval, ssize_t)
1520  CTL_RO_NL_CGEN(config_prof, opt_prof_gdump, opt_prof_gdump, bool)
1521  CTL_RO_NL_CGEN(config_prof, opt_prof_final, opt_prof_final, bool)
1522  CTL_RO_NL_CGEN(config_prof, opt_prof_leak, opt_prof_leak, bool)
1523  static int
1524  thread_arena_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1525      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1526  	int ret;
1527  	arena_t *oldarena;
1528  	unsigned newind, oldind;
1529  	oldarena = arena_choose(tsd, NULL);
1530  	if (oldarena == NULL) {
1531  		return EAGAIN;
1532  	}
1533  	newind = oldind = arena_ind_get(oldarena);
1534  	WRITE(newind, unsigned);
1535  	READ(oldind, unsigned);
1536  	if (newind != oldind) {
1537  		arena_t *newarena;
1538  		if (newind >= narenas_total_get()) {
1539  			ret = EFAULT;
1540  			goto label_return;
1541  		}
1542  		if (have_percpu_arena &&
1543  		    PERCPU_ARENA_ENABLED(opt_percpu_arena)) {
1544  			if (newind < percpu_arena_ind_limit(opt_percpu_arena)) {
1545  				ret = EPERM;
1546  				goto label_return;
1547  			}
1548  		}
1549  		newarena = arena_get(tsd_tsdn(tsd), newind, true);
1550  		if (newarena == NULL) {
1551  			ret = EAGAIN;
1552  			goto label_return;
1553  		}
1554  		arena_migrate(tsd, oldind, newind);
1555  		if (tcache_available(tsd)) {
1556  			tcache_arena_reassociate(tsd_tsdn(tsd),
1557  			    tsd_tcachep_get(tsd), newarena);
1558  		}
1559  	}
1560  	ret = 0;
1561  label_return:
1562  	return ret;
1563  }
1564  CTL_TSD_RO_NL_CGEN(config_stats, thread_allocated, tsd_thread_allocated_get,
1565      uint64_t)
1566  CTL_TSD_RO_NL_CGEN(config_stats, thread_allocatedp, tsd_thread_allocatedp_get,
1567      uint64_t *)
1568  CTL_TSD_RO_NL_CGEN(config_stats, thread_deallocated, tsd_thread_deallocated_get,
1569      uint64_t)
1570  CTL_TSD_RO_NL_CGEN(config_stats, thread_deallocatedp,
1571      tsd_thread_deallocatedp_get, uint64_t *)
1572  static int
1573  thread_tcache_enabled_ctl(tsd_t *tsd, const size_t *mib,
1574      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1575      size_t newlen) {
1576  	int ret;
1577  	bool oldval;
1578  	oldval = tcache_enabled_get(tsd);
1579  	if (newp != NULL) {
1580  		if (newlen != sizeof(bool)) {
1581  			ret = EINVAL;
1582  			goto label_return;
1583  		}
1584  		tcache_enabled_set(tsd, *(bool *)newp);
1585  	}
1586  	READ(oldval, bool);
1587  	ret = 0;
1588  label_return:
1589  	return ret;
1590  }
1591  static int
1592  thread_tcache_flush_ctl(tsd_t *tsd, const size_t *mib,
1593      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1594      size_t newlen) {
1595  	int ret;
1596  	if (!tcache_available(tsd)) {
1597  		ret = EFAULT;
1598  		goto label_return;
1599  	}
1600  	READONLY();
1601  	WRITEONLY();
1602  	tcache_flush(tsd);
1603  	ret = 0;
1604  label_return:
1605  	return ret;
1606  }
1607  static int
1608  thread_prof_name_ctl(tsd_t *tsd, const size_t *mib,
1609      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1610      size_t newlen) {
1611  	int ret;
1612  	if (!config_prof) {
1613  		return ENOENT;
1614  	}
1615  	READ_XOR_WRITE();
1616  	if (newp != NULL) {
1617  		if (newlen != sizeof(const char *)) {
1618  			ret = EINVAL;
1619  			goto label_return;
1620  		}
1621  		if ((ret = prof_thread_name_set(tsd, *(const char **)newp)) !=
1622  		    0) {
1623  			goto label_return;
1624  		}
1625  	} else {
1626  		const char *oldname = prof_thread_name_get(tsd);
1627  		READ(oldname, const char *);
1628  	}
1629  	ret = 0;
1630  label_return:
1631  	return ret;
1632  }
1633  static int
1634  thread_prof_active_ctl(tsd_t *tsd, const size_t *mib,
1635      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1636      size_t newlen) {
1637  	int ret;
1638  	bool oldval;
1639  	if (!config_prof) {
1640  		return ENOENT;
1641  	}
1642  	oldval = prof_thread_active_get(tsd);
1643  	if (newp != NULL) {
1644  		if (newlen != sizeof(bool)) {
1645  			ret = EINVAL;
1646  			goto label_return;
1647  		}
1648  		if (prof_thread_active_set(tsd, *(bool *)newp)) {
1649  			ret = EAGAIN;
1650  			goto label_return;
1651  		}
1652  	}
1653  	READ(oldval, bool);
1654  	ret = 0;
1655  label_return:
1656  	return ret;
1657  }
1658  static int
1659  tcache_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1660      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1661  	int ret;
1662  	unsigned tcache_ind;
1663  	READONLY();
1664  	if (tcaches_create(tsd, &tcache_ind)) {
1665  		ret = EFAULT;
1666  		goto label_return;
1667  	}
1668  	READ(tcache_ind, unsigned);
1669  	ret = 0;
1670  label_return:
1671  	return ret;
1672  }
1673  static int
1674  tcache_flush_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1675      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1676  	int ret;
1677  	unsigned tcache_ind;
1678  	WRITEONLY();
1679  	tcache_ind = UINT_MAX;
1680  	WRITE(tcache_ind, unsigned);
1681  	if (tcache_ind == UINT_MAX) {
1682  		ret = EFAULT;
1683  		goto label_return;
1684  	}
1685  	tcaches_flush(tsd, tcache_ind);
1686  	ret = 0;
1687  label_return:
1688  	return ret;
1689  }
1690  static int
1691  tcache_destroy_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1692      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1693  	int ret;
1694  	unsigned tcache_ind;
1695  	WRITEONLY();
1696  	tcache_ind = UINT_MAX;
1697  	WRITE(tcache_ind, unsigned);
1698  	if (tcache_ind == UINT_MAX) {
1699  		ret = EFAULT;
1700  		goto label_return;
1701  	}
1702  	tcaches_destroy(tsd, tcache_ind);
1703  	ret = 0;
1704  label_return:
1705  	return ret;
1706  }
1707  static int
1708  arena_i_initialized_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1709      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1710  	int ret;
1711  	tsdn_t *tsdn = tsd_tsdn(tsd);
1712  	unsigned arena_ind;
1713  	bool initialized;
1714  	READONLY();
1715  	MIB_UNSIGNED(arena_ind, 1);
1716  	malloc_mutex_lock(tsdn, &ctl_mtx);
1717  	initialized = arenas_i(arena_ind)->initialized;
1718  	malloc_mutex_unlock(tsdn, &ctl_mtx);
1719  	READ(initialized, bool);
1720  	ret = 0;
1721  label_return:
1722  	return ret;
1723  }
1724  static void
1725  arena_i_decay(tsdn_t *tsdn, unsigned arena_ind, bool all) {
1726  	malloc_mutex_lock(tsdn, &ctl_mtx);
1727  	{
1728  		unsigned narenas = ctl_arenas->narenas;
1729  		if (arena_ind == MALLCTL_ARENAS_ALL || arena_ind == narenas) {
1730  			unsigned i;
1731  			VARIABLE_ARRAY(arena_t *, tarenas, narenas);
1732  			for (i = 0; i < narenas; i++) {
1733  				tarenas[i] = arena_get(tsdn, i, false);
1734  			}
1735  			malloc_mutex_unlock(tsdn, &ctl_mtx);
1736  			for (i = 0; i < narenas; i++) {
1737  				if (tarenas[i] != NULL) {
1738  					arena_decay(tsdn, tarenas[i], false,
1739  					    all);
1740  				}
1741  			}
1742  		} else {
1743  			arena_t *tarena;
1744  			assert(arena_ind < narenas);
1745  			tarena = arena_get(tsdn, arena_ind, false);
1746  			malloc_mutex_unlock(tsdn, &ctl_mtx);
1747  			if (tarena != NULL) {
1748  				arena_decay(tsdn, tarena, false, all);
1749  			}
1750  		}
1751  	}
1752  }
1753  static int
1754  arena_i_decay_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1755      size_t *oldlenp, void *newp, size_t newlen) {
1756  	int ret;
1757  	unsigned arena_ind;
1758  	READONLY();
1759  	WRITEONLY();
1760  	MIB_UNSIGNED(arena_ind, 1);
1761  	arena_i_decay(tsd_tsdn(tsd), arena_ind, false);
1762  	ret = 0;
1763  label_return:
1764  	return ret;
1765  }
1766  static int
1767  arena_i_purge_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1768      size_t *oldlenp, void *newp, size_t newlen) {
1769  	int ret;
1770  	unsigned arena_ind;
1771  	READONLY();
1772  	WRITEONLY();
1773  	MIB_UNSIGNED(arena_ind, 1);
1774  	arena_i_decay(tsd_tsdn(tsd), arena_ind, true);
1775  	ret = 0;
1776  label_return:
1777  	return ret;
1778  }
1779  static int
1780  arena_i_reset_destroy_helper(tsd_t *tsd, const size_t *mib, size_t miblen,
1781      void *oldp, size_t *oldlenp, void *newp, size_t newlen, unsigned *arena_ind,
1782      arena_t **arena) {
1783  	int ret;
1784  	READONLY();
1785  	WRITEONLY();
1786  	MIB_UNSIGNED(*arena_ind, 1);
1787  	*arena = arena_get(tsd_tsdn(tsd), *arena_ind, false);
1788  	if (*arena == NULL || arena_is_auto(*arena)) {
1789  		ret = EFAULT;
1790  		goto label_return;
1791  	}
1792  	ret = 0;
1793  label_return:
1794  	return ret;
1795  }
1796  static void
1797  arena_reset_prepare_background_thread(tsd_t *tsd, unsigned arena_ind) {
1798  	if (have_background_thread) {
1799  		malloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);
1800  		if (background_thread_enabled()) {
1801  			background_thread_info_t *info =
1802  			    background_thread_info_get(arena_ind);
1803  			assert(info->state == background_thread_started);
1804  			malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
1805  			info->state = background_thread_paused;
1806  			malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
1807  		}
1808  	}
1809  }
1810  static void
1811  arena_reset_finish_background_thread(tsd_t *tsd, unsigned arena_ind) {
1812  	if (have_background_thread) {
1813  		if (background_thread_enabled()) {
1814  			background_thread_info_t *info =
1815  			    background_thread_info_get(arena_ind);
1816  			assert(info->state == background_thread_paused);
1817  			malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
1818  			info->state = background_thread_started;
1819  			malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
1820  		}
1821  		malloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);
1822  	}
1823  }
1824  static int
1825  arena_i_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1826      size_t *oldlenp, void *newp, size_t newlen) {
1827  	int ret;
1828  	unsigned arena_ind;
1829  	arena_t *arena;
1830  	ret = arena_i_reset_destroy_helper(tsd, mib, miblen, oldp, oldlenp,
1831  	    newp, newlen, &arena_ind, &arena);
1832  	if (ret != 0) {
1833  		return ret;
1834  	}
1835  	arena_reset_prepare_background_thread(tsd, arena_ind);
1836  	arena_reset(tsd, arena);
1837  	arena_reset_finish_background_thread(tsd, arena_ind);
1838  	return ret;
1839  }
1840  static int
1841  arena_i_destroy_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1842      size_t *oldlenp, void *newp, size_t newlen) {
1843  	int ret;
1844  	unsigned arena_ind;
1845  	arena_t *arena;
1846  	ctl_arena_t *ctl_darena, *ctl_arena;
1847  	ret = arena_i_reset_destroy_helper(tsd, mib, miblen, oldp, oldlenp,
1848  	    newp, newlen, &arena_ind, &arena);
1849  	if (ret != 0) {
1850  		goto label_return;
1851  	}
1852  	if (arena_nthreads_get(arena, false) != 0 || arena_nthreads_get(arena,
1853  	    true) != 0) {
1854  		ret = EFAULT;
1855  		goto label_return;
1856  	}
1857  	arena_reset_prepare_background_thread(tsd, arena_ind);
1858  	arena_reset(tsd, arena);
1859  	arena_decay(tsd_tsdn(tsd), arena, false, true);
1860  	ctl_darena = arenas_i(MALLCTL_ARENAS_DESTROYED);
1861  	ctl_darena->initialized = true;
1862  	ctl_arena_refresh(tsd_tsdn(tsd), arena, ctl_darena, arena_ind, true);
1863  	arena_destroy(tsd, arena);
1864  	ctl_arena = arenas_i(arena_ind);
1865  	ctl_arena->initialized = false;
1866  	ql_elm_new(ctl_arena, destroyed_link);
1867  	ql_tail_insert(&ctl_arenas->destroyed, ctl_arena, destroyed_link);
1868  	arena_reset_finish_background_thread(tsd, arena_ind);
1869  	assert(ret == 0);
1870  label_return:
1871  	return ret;
1872  }
1873  static int
1874  arena_i_dss_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1875      size_t *oldlenp, void *newp, size_t newlen) {
1876  	int ret;
1877  	const char *dss = NULL;
1878  	unsigned arena_ind;
1879  	dss_prec_t dss_prec_old = dss_prec_limit;
1880  	dss_prec_t dss_prec = dss_prec_limit;
1881  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
1882  	WRITE(dss, const char *);
1883  	MIB_UNSIGNED(arena_ind, 1);
1884  	if (dss != NULL) {
1885  		int i;
1886  		bool match = false;
1887  		for (i = 0; i < dss_prec_limit; i++) {
1888  			if (strcmp(dss_prec_names[i], dss) == 0) {
1889  				dss_prec = i;
1890  				match = true;
1891  				break;
1892  			}
1893  		}
1894  		if (!match) {
1895  			ret = EINVAL;
1896  			goto label_return;
1897  		}
1898  	}
1899  	if (arena_ind == MALLCTL_ARENAS_ALL || arena_ind ==
1900  	    ctl_arenas->narenas) {
1901  		if (dss_prec != dss_prec_limit &&
1902  		    extent_dss_prec_set(dss_prec)) {
1903  			ret = EFAULT;
1904  			goto label_return;
1905  		}
1906  		dss_prec_old = extent_dss_prec_get();
1907  	} else {
1908  		arena_t *arena = arena_get(tsd_tsdn(tsd), arena_ind, false);
1909  		if (arena == NULL || (dss_prec != dss_prec_limit &&
1910  		    arena_dss_prec_set(arena, dss_prec))) {
1911  			ret = EFAULT;
1912  			goto label_return;
1913  		}
1914  		dss_prec_old = arena_dss_prec_get(arena);
1915  	}
1916  	dss = dss_prec_names[dss_prec_old];
1917  	READ(dss, const char *);
1918  	ret = 0;
1919  label_return:
1920  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
1921  	return ret;
1922  }
1923  static int
1924  arena_i_decay_ms_ctl_impl(tsd_t *tsd, const size_t *mib, size_t miblen,
1925      void *oldp, size_t *oldlenp, void *newp, size_t newlen, bool dirty) {
1926  	int ret;
1927  	unsigned arena_ind;
1928  	arena_t *arena;
1929  	MIB_UNSIGNED(arena_ind, 1);
1930  	arena = arena_get(tsd_tsdn(tsd), arena_ind, false);
1931  	if (arena == NULL) {
1932  		ret = EFAULT;
1933  		goto label_return;
1934  	}
1935  	if (oldp != NULL && oldlenp != NULL) {
1936  		size_t oldval = dirty ? arena_dirty_decay_ms_get(arena) :
1937  		    arena_muzzy_decay_ms_get(arena);
1938  		READ(oldval, ssize_t);
1939  	}
1940  	if (newp != NULL) {
1941  		if (newlen != sizeof(ssize_t)) {
1942  			ret = EINVAL;
1943  			goto label_return;
1944  		}
1945  		if (arena_is_huge(arena_ind) && *(ssize_t *)newp > 0) {
1946  			if (background_thread_create(tsd, arena_ind)) {
1947  				ret = EFAULT;
1948  				goto label_return;
1949  			}
1950  		}
1951  		if (dirty ? arena_dirty_decay_ms_set(tsd_tsdn(tsd), arena,
1952  		    *(ssize_t *)newp) : arena_muzzy_decay_ms_set(tsd_tsdn(tsd),
1953  		    arena, *(ssize_t *)newp)) {
1954  			ret = EFAULT;
1955  			goto label_return;
1956  		}
1957  	}
1958  	ret = 0;
1959  label_return:
1960  	return ret;
1961  }
1962  static int
1963  arena_i_dirty_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1964      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1965  	return arena_i_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
1966  	    newlen, true);
1967  }
1968  static int
1969  arena_i_muzzy_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1970      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1971  	return arena_i_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
1972  	    newlen, false);
1973  }
1974  static int
1975  arena_i_extent_hooks_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1976      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1977  	int ret;
1978  	unsigned arena_ind;
1979  	arena_t *arena;
1980  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
1981  	MIB_UNSIGNED(arena_ind, 1);
1982  	if (arena_ind < narenas_total_get()) {
1983  		extent_hooks_t *old_extent_hooks;
1984  		arena = arena_get(tsd_tsdn(tsd), arena_ind, false);
1985  		if (arena == NULL) {
1986  			if (arena_ind >= narenas_auto) {
1987  				ret = EFAULT;
1988  				goto label_return;
1989  			}
1990  			old_extent_hooks =
1991  			    (extent_hooks_t *)&extent_hooks_default;
1992  			READ(old_extent_hooks, extent_hooks_t *);
1993  			if (newp != NULL) {
1994  				extent_hooks_t *new_extent_hooks
1995  				    JEMALLOC_CC_SILENCE_INIT(NULL);
1996  				WRITE(new_extent_hooks, extent_hooks_t *);
1997  				arena = arena_init(tsd_tsdn(tsd), arena_ind,
1998  				    new_extent_hooks);
1999  				if (arena == NULL) {
2000  					ret = EFAULT;
2001  					goto label_return;
2002  				}
2003  			}
2004  		} else {
2005  			if (newp != NULL) {
2006  				extent_hooks_t *new_extent_hooks
2007  				    JEMALLOC_CC_SILENCE_INIT(NULL);
2008  				WRITE(new_extent_hooks, extent_hooks_t *);
2009  				old_extent_hooks = extent_hooks_set(tsd, arena,
2010  				    new_extent_hooks);
2011  				READ(old_extent_hooks, extent_hooks_t *);
2012  			} else {
2013  				old_extent_hooks = extent_hooks_get(arena);
2014  				READ(old_extent_hooks, extent_hooks_t *);
2015  			}
2016  		}
2017  	} else {
2018  		ret = EFAULT;
2019  		goto label_return;
2020  	}
2021  	ret = 0;
2022  label_return:
2023  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
2024  	return ret;
2025  }
2026  static int
2027  arena_i_retain_grow_limit_ctl(tsd_t *tsd, const size_t *mib,
2028      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2029      size_t newlen) {
2030  	int ret;
2031  	unsigned arena_ind;
2032  	arena_t *arena;
2033  	if (!opt_retain) {
2034  		return ENOENT;
2035  	}
2036  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
2037  	MIB_UNSIGNED(arena_ind, 1);
2038  	if (arena_ind < narenas_total_get() && (arena =
2039  	    arena_get(tsd_tsdn(tsd), arena_ind, false)) != NULL) {
2040  		size_t old_limit, new_limit;
2041  		if (newp != NULL) {
2042  			WRITE(new_limit, size_t);
2043  		}
2044  		bool err = arena_retain_grow_limit_get_set(tsd, arena,
2045  		    &old_limit, newp != NULL ? &new_limit : NULL);
2046  		if (!err) {
2047  			READ(old_limit, size_t);
2048  			ret = 0;
2049  		} else {
2050  			ret = EFAULT;
2051  		}
2052  	} else {
2053  		ret = EFAULT;
2054  	}
2055  label_return:
2056  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
2057  	return ret;
2058  }
2059  static const ctl_named_node_t *
2060  arena_i_index(tsdn_t *tsdn, const size_t *mib, size_t miblen,
2061      size_t i) {
2062  	const ctl_named_node_t *ret;
2063  	malloc_mutex_lock(tsdn, &ctl_mtx);
2064  	switch (i) {
2065  	case MALLCTL_ARENAS_ALL:
2066  	case MALLCTL_ARENAS_DESTROYED:
2067  		break;
2068  	default:
2069  		if (i > ctl_arenas->narenas) {
2070  			ret = NULL;
2071  			goto label_return;
2072  		}
2073  		break;
2074  	}
2075  	ret = super_arena_i_node;
2076  label_return:
2077  	malloc_mutex_unlock(tsdn, &ctl_mtx);
2078  	return ret;
2079  }
2080  static int
2081  arenas_narenas_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2082      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2083  	int ret;
2084  	unsigned narenas;
2085  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
2086  	READONLY();
2087  	if (*oldlenp != sizeof(unsigned)) {
2088  		ret = EINVAL;
2089  		goto label_return;
2090  	}
2091  	narenas = ctl_arenas->narenas;
2092  	READ(narenas, unsigned);
2093  	ret = 0;
2094  label_return:
2095  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
2096  	return ret;
2097  }
2098  static int
2099  arenas_decay_ms_ctl_impl(tsd_t *tsd, const size_t *mib,
2100      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2101      size_t newlen, bool dirty) {
2102  	int ret;
2103  	if (oldp != NULL && oldlenp != NULL) {
2104  		size_t oldval = (dirty ? arena_dirty_decay_ms_default_get() :
2105  		    arena_muzzy_decay_ms_default_get());
2106  		READ(oldval, ssize_t);
2107  	}
2108  	if (newp != NULL) {
2109  		if (newlen != sizeof(ssize_t)) {
2110  			ret = EINVAL;
2111  			goto label_return;
2112  		}
2113  		if (dirty ? arena_dirty_decay_ms_default_set(*(ssize_t *)newp)
2114  		    : arena_muzzy_decay_ms_default_set(*(ssize_t *)newp)) {
2115  			ret = EFAULT;
2116  			goto label_return;
2117  		}
2118  	}
2119  	ret = 0;
2120  label_return:
2121  	return ret;
2122  }
2123  static int
2124  arenas_dirty_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2125      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2126  	return arenas_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
2127  	    newlen, true);
2128  }
2129  static int
2130  arenas_muzzy_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2131      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2132  	return arenas_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
2133  	    newlen, false);
2134  }
2135  CTL_RO_NL_GEN(arenas_quantum, QUANTUM, size_t)
2136  CTL_RO_NL_GEN(arenas_page, PAGE, size_t)
2137  CTL_RO_NL_GEN(arenas_tcache_max, tcache_maxclass, size_t)
2138  CTL_RO_NL_GEN(arenas_nbins, SC_NBINS, unsigned)
2139  CTL_RO_NL_GEN(arenas_nhbins, nhbins, unsigned)
2140  CTL_RO_NL_GEN(arenas_bin_i_size, bin_infos[mib[2]].reg_size, size_t)
2141  CTL_RO_NL_GEN(arenas_bin_i_nregs, bin_infos[mib[2]].nregs, uint32_t)
2142  CTL_RO_NL_GEN(arenas_bin_i_slab_size, bin_infos[mib[2]].slab_size, size_t)
2143  CTL_RO_NL_GEN(arenas_bin_i_nshards, bin_infos[mib[2]].n_shards, uint32_t)
2144  static const ctl_named_node_t *
2145  arenas_bin_i_index(tsdn_t *tsdn, const size_t *mib,
2146      size_t miblen, size_t i) {
2147  	if (i > SC_NBINS) {
2148  		return NULL;
2149  	}
2150  	return super_arenas_bin_i_node;
2151  }
2152  CTL_RO_NL_GEN(arenas_nlextents, SC_NSIZES - SC_NBINS, unsigned)
2153  CTL_RO_NL_GEN(arenas_lextent_i_size, sz_index2size(SC_NBINS+(szind_t)mib[2]),
2154      size_t)
2155  static const ctl_named_node_t *
2156  arenas_lextent_i_index(tsdn_t *tsdn, const size_t *mib,
2157      size_t miblen, size_t i) {
2158  	if (i > SC_NSIZES - SC_NBINS) {
<span onclick='openModal()' class='match'>2159  		return NULL;
2160  	}
2161  	return super_arenas_lextent_i_node;
2162  }
2163  static int
2164  arenas_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2165      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
</span>2166  	int ret;
2167  	extent_hooks_t *extent_hooks;
2168  	unsigned arena_ind;
2169  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
2170  	extent_hooks = (extent_hooks_t *)&extent_hooks_default;
2171  	WRITE(extent_hooks, extent_hooks_t *);
2172  	if ((arena_ind = ctl_arena_init(tsd, extent_hooks)) == UINT_MAX) {
2173  		ret = EAGAIN;
2174  		goto label_return;
2175  	}
2176  	READ(arena_ind, unsigned);
2177  	ret = 0;
2178  label_return:
2179  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
2180  	return ret;
2181  }
2182  static int
2183  arenas_lookup_ctl(tsd_t *tsd, const size_t *mib,
2184      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2185      size_t newlen) {
2186  	int ret;
2187  	unsigned arena_ind;
2188  	void *ptr;
2189  	extent_t *extent;
2190  	arena_t *arena;
2191  	ptr = NULL;
2192  	ret = EINVAL;
2193  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
2194  	WRITE(ptr, void *);
2195  	extent = iealloc(tsd_tsdn(tsd), ptr);
2196  	if (extent == NULL)
2197  		goto label_return;
2198  	arena = extent_arena_get(extent);
2199  	if (arena == NULL)
2200  		goto label_return;
2201  	arena_ind = arena_ind_get(arena);
2202  	READ(arena_ind, unsigned);
2203  	ret = 0;
2204  label_return:
2205  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
2206  	return ret;
2207  }
2208  static int
2209  prof_thread_active_init_ctl(tsd_t *tsd, const size_t *mib,
2210      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2211      size_t newlen) {
2212  	int ret;
2213  	bool oldval;
2214  	if (!config_prof) {
2215  		return ENOENT;
2216  	}
2217  	if (newp != NULL) {
2218  		if (newlen != sizeof(bool)) {
2219  			ret = EINVAL;
2220  			goto label_return;
2221  		}
2222  		oldval = prof_thread_active_init_set(tsd_tsdn(tsd),
2223  		    *(bool *)newp);
2224  	} else {
2225  		oldval = prof_thread_active_init_get(tsd_tsdn(tsd));
2226  	}
2227  	READ(oldval, bool);
2228  	ret = 0;
2229  label_return:
2230  	return ret;
2231  }
2232  static int
2233  prof_active_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2234      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2235  	int ret;
2236  	bool oldval;
2237  	if (!config_prof) {
2238  		return ENOENT;
2239  	}
2240  	if (newp != NULL) {
2241  		if (newlen != sizeof(bool)) {
2242  			ret = EINVAL;
2243  			goto label_return;
2244  		}
2245  		oldval = prof_active_set(tsd_tsdn(tsd), *(bool *)newp);
2246  	} else {
2247  		oldval = prof_active_get(tsd_tsdn(tsd));
2248  	}
2249  	READ(oldval, bool);
2250  	ret = 0;
2251  label_return:
2252  	return ret;
2253  }
2254  static int
2255  prof_dump_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2256      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2257  	int ret;
2258  	const char *filename = NULL;
2259  	if (!config_prof) {
2260  		return ENOENT;
2261  	}
2262  	WRITEONLY();
2263  	WRITE(filename, const char *);
2264  	if (prof_mdump(tsd, filename)) {
2265  		ret = EFAULT;
2266  		goto label_return;
2267  	}
2268  	ret = 0;
2269  label_return:
2270  	return ret;
2271  }
2272  static int
2273  prof_gdump_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2274      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2275  	int ret;
2276  	bool oldval;
2277  	if (!config_prof) {
2278  		return ENOENT;
2279  	}
2280  	if (newp != NULL) {
2281  		if (newlen != sizeof(bool)) {
2282  			ret = EINVAL;
2283  			goto label_return;
2284  		}
2285  		oldval = prof_gdump_set(tsd_tsdn(tsd), *(bool *)newp);
2286  	} else {
2287  		oldval = prof_gdump_get(tsd_tsdn(tsd));
2288  	}
2289  	READ(oldval, bool);
2290  	ret = 0;
2291  label_return:
2292  	return ret;
2293  }
2294  static int
2295  prof_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2296      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2297  	int ret;
2298  	size_t lg_sample = lg_prof_sample;
2299  	if (!config_prof) {
2300  		return ENOENT;
2301  	}
2302  	WRITEONLY();
2303  	WRITE(lg_sample, size_t);
2304  	if (lg_sample >= (sizeof(uint64_t) << 3)) {
2305  		lg_sample = (sizeof(uint64_t) << 3) - 1;
2306  	}
2307  	prof_reset(tsd, lg_sample);
2308  	ret = 0;
2309  label_return:
2310  	return ret;
2311  }
2312  CTL_RO_NL_CGEN(config_prof, prof_interval, prof_interval, uint64_t)
2313  CTL_RO_NL_CGEN(config_prof, lg_prof_sample, lg_prof_sample, size_t)
2314  static int
2315  prof_log_start_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
2316      size_t *oldlenp, void *newp, size_t newlen) {
2317  	int ret;
2318  	const char *filename = NULL;
2319  	if (!config_prof) {
2320  		return ENOENT;
2321  	}
2322  	WRITEONLY();
2323  	WRITE(filename, const char *);
2324  	if (prof_log_start(tsd_tsdn(tsd), filename)) {
2325  		ret = EFAULT;
2326  		goto label_return;
2327  	}
2328  	ret = 0;
2329  label_return:
2330  	return ret;
2331  }
2332  static int
2333  prof_log_stop_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
2334      size_t *oldlenp, void *newp, size_t newlen) {
2335  	if (!config_prof) {
2336  		return ENOENT;
2337  	}
2338  	if (prof_log_stop(tsd_tsdn(tsd))) {
2339  		return EFAULT;
2340  	}
2341  	return 0;
2342  }
2343  CTL_RO_CGEN(config_stats, stats_allocated, ctl_stats->allocated, size_t)
2344  CTL_RO_CGEN(config_stats, stats_active, ctl_stats->active, size_t)
2345  CTL_RO_CGEN(config_stats, stats_metadata, ctl_stats->metadata, size_t)
2346  CTL_RO_CGEN(config_stats, stats_metadata_thp, ctl_stats->metadata_thp, size_t)
2347  CTL_RO_CGEN(config_stats, stats_resident, ctl_stats->resident, size_t)
2348  CTL_RO_CGEN(config_stats, stats_mapped, ctl_stats->mapped, size_t)
2349  CTL_RO_CGEN(config_stats, stats_retained, ctl_stats->retained, size_t)
2350  CTL_RO_CGEN(config_stats, stats_background_thread_num_threads,
2351      ctl_stats->background_thread.num_threads, size_t)
2352  CTL_RO_CGEN(config_stats, stats_background_thread_num_runs,
2353      ctl_stats->background_thread.num_runs, uint64_t)
2354  CTL_RO_CGEN(config_stats, stats_background_thread_run_interval,
2355      nstime_ns(&ctl_stats->background_thread.run_interval), uint64_t)
2356  CTL_RO_GEN(stats_arenas_i_dss, arenas_i(mib[2])->dss, const char *)
2357  CTL_RO_GEN(stats_arenas_i_dirty_decay_ms, arenas_i(mib[2])->dirty_decay_ms,
2358      ssize_t)
2359  CTL_RO_GEN(stats_arenas_i_muzzy_decay_ms, arenas_i(mib[2])->muzzy_decay_ms,
2360      ssize_t)
2361  CTL_RO_GEN(stats_arenas_i_nthreads, arenas_i(mib[2])->nthreads, unsigned)
2362  CTL_RO_GEN(stats_arenas_i_uptime,
2363      nstime_ns(&arenas_i(mib[2])->astats->astats.uptime), uint64_t)
2364  CTL_RO_GEN(stats_arenas_i_pactive, arenas_i(mib[2])->pactive, size_t)
2365  CTL_RO_GEN(stats_arenas_i_pdirty, arenas_i(mib[2])->pdirty, size_t)
2366  CTL_RO_GEN(stats_arenas_i_pmuzzy, arenas_i(mib[2])->pmuzzy, size_t)
2367  CTL_RO_CGEN(config_stats, stats_arenas_i_mapped,
2368      atomic_load_zu(&arenas_i(mib[2])->astats->astats.mapped, ATOMIC_RELAXED),
2369      size_t)
2370  CTL_RO_CGEN(config_stats, stats_arenas_i_retained,
2371      atomic_load_zu(&arenas_i(mib[2])->astats->astats.retained, ATOMIC_RELAXED),
2372      size_t)
2373  CTL_RO_CGEN(config_stats, stats_arenas_i_extent_avail,
2374      atomic_load_zu(&arenas_i(mib[2])->astats->astats.extent_avail,
2375          ATOMIC_RELAXED),
2376      size_t)
2377  CTL_RO_CGEN(config_stats, stats_arenas_i_dirty_npurge,
2378      ctl_arena_stats_read_u64(
2379      &arenas_i(mib[2])->astats->astats.decay_dirty.npurge), uint64_t)
2380  CTL_RO_CGEN(config_stats, stats_arenas_i_dirty_nmadvise,
2381      ctl_arena_stats_read_u64(
2382      &arenas_i(mib[2])->astats->astats.decay_dirty.nmadvise), uint64_t)
2383  CTL_RO_CGEN(config_stats, stats_arenas_i_dirty_purged,
2384      ctl_arena_stats_read_u64(
2385      &arenas_i(mib[2])->astats->astats.decay_dirty.purged), uint64_t)
2386  CTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_npurge,
2387      ctl_arena_stats_read_u64(
2388      &arenas_i(mib[2])->astats->astats.decay_muzzy.npurge), uint64_t)
2389  CTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_nmadvise,
2390      ctl_arena_stats_read_u64(
2391      &arenas_i(mib[2])->astats->astats.decay_muzzy.nmadvise), uint64_t)
2392  CTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_purged,
2393      ctl_arena_stats_read_u64(
2394      &arenas_i(mib[2])->astats->astats.decay_muzzy.purged), uint64_t)
2395  CTL_RO_CGEN(config_stats, stats_arenas_i_base,
2396      atomic_load_zu(&arenas_i(mib[2])->astats->astats.base, ATOMIC_RELAXED),
2397      size_t)
2398  CTL_RO_CGEN(config_stats, stats_arenas_i_internal,
2399      atomic_load_zu(&arenas_i(mib[2])->astats->astats.internal, ATOMIC_RELAXED),
2400      size_t)
2401  CTL_RO_CGEN(config_stats, stats_arenas_i_metadata_thp,
2402      atomic_load_zu(&arenas_i(mib[2])->astats->astats.metadata_thp,
2403      ATOMIC_RELAXED), size_t)
2404  CTL_RO_CGEN(config_stats, stats_arenas_i_tcache_bytes,
2405      atomic_load_zu(&arenas_i(mib[2])->astats->astats.tcache_bytes,
2406      ATOMIC_RELAXED), size_t)
2407  CTL_RO_CGEN(config_stats, stats_arenas_i_resident,
2408      atomic_load_zu(&arenas_i(mib[2])->astats->astats.resident, ATOMIC_RELAXED),
2409      size_t)
2410  CTL_RO_CGEN(config_stats, stats_arenas_i_abandoned_vm,
2411      atomic_load_zu(&arenas_i(mib[2])->astats->astats.abandoned_vm,
2412      ATOMIC_RELAXED), size_t)
2413  CTL_RO_CGEN(config_stats, stats_arenas_i_small_allocated,
2414      arenas_i(mib[2])->astats->allocated_small, size_t)
2415  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nmalloc,
2416      arenas_i(mib[2])->astats->nmalloc_small, uint64_t)
2417  CTL_RO_CGEN(config_stats, stats_arenas_i_small_ndalloc,
2418      arenas_i(mib[2])->astats->ndalloc_small, uint64_t)
2419  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nrequests,
2420      arenas_i(mib[2])->astats->nrequests_small, uint64_t)
2421  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nfills,
2422      arenas_i(mib[2])->astats->nfills_small, uint64_t)
2423  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nflushes,
2424      arenas_i(mib[2])->astats->nflushes_small, uint64_t)
2425  CTL_RO_CGEN(config_stats, stats_arenas_i_large_allocated,
2426      atomic_load_zu(&arenas_i(mib[2])->astats->astats.allocated_large,
2427      ATOMIC_RELAXED), size_t)
2428  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nmalloc,
2429      ctl_arena_stats_read_u64(
2430      &arenas_i(mib[2])->astats->astats.nmalloc_large), uint64_t)
2431  CTL_RO_CGEN(config_stats, stats_arenas_i_large_ndalloc,
2432      ctl_arena_stats_read_u64(
2433      &arenas_i(mib[2])->astats->astats.ndalloc_large), uint64_t)
2434  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nrequests,
2435      ctl_arena_stats_read_u64(
2436      &arenas_i(mib[2])->astats->astats.nrequests_large), uint64_t)
2437  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nfills,
2438      ctl_arena_stats_read_u64(
2439      &arenas_i(mib[2])->astats->astats.nmalloc_large), uint64_t)
2440  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nflushes,
2441      ctl_arena_stats_read_u64(
2442      &arenas_i(mib[2])->astats->astats.nflushes_large), uint64_t)
2443  #define RO_MUTEX_CTL_GEN(n, l)						\
2444  CTL_RO_CGEN(config_stats, stats_##n##_num_ops,				\
2445      l.n_lock_ops, uint64_t)						\
2446  CTL_RO_CGEN(config_stats, stats_##n##_num_wait,				\
2447      l.n_wait_times, uint64_t)						\
2448  CTL_RO_CGEN(config_stats, stats_##n##_num_spin_acq,			\
2449      l.n_spin_acquired, uint64_t)					\
2450  CTL_RO_CGEN(config_stats, stats_##n##_num_owner_switch,			\
2451      l.n_owner_switches, uint64_t) 					\
2452  CTL_RO_CGEN(config_stats, stats_##n##_total_wait_time,			\
2453      nstime_ns(&l.tot_wait_time), uint64_t)				\
2454  CTL_RO_CGEN(config_stats, stats_##n##_max_wait_time,			\
2455      nstime_ns(&l.max_wait_time), uint64_t)				\
2456  CTL_RO_CGEN(config_stats, stats_##n##_max_num_thds,			\
2457      l.max_n_thds, uint32_t)
2458  #define OP(mtx)								\
2459      RO_MUTEX_CTL_GEN(mutexes_##mtx,					\
2460          ctl_stats->mutex_prof_data[global_prof_mutex_##mtx])
2461  MUTEX_PROF_GLOBAL_MUTEXES
2462  #undef OP
2463  #define OP(mtx) RO_MUTEX_CTL_GEN(arenas_i_mutexes_##mtx,		\
2464      arenas_i(mib[2])->astats->astats.mutex_prof_data[arena_prof_mutex_##mtx])
2465  MUTEX_PROF_ARENA_MUTEXES
2466  #undef OP
2467  RO_MUTEX_CTL_GEN(arenas_i_bins_j_mutex,
2468      arenas_i(mib[2])->astats->bstats[mib[4]].mutex_data)
2469  #undef RO_MUTEX_CTL_GEN
2470  static int
2471  stats_mutexes_reset_ctl(tsd_t *tsd, const size_t *mib,
2472      size_t miblen, void *oldp, size_t *oldlenp,
2473      void *newp, size_t newlen) {
2474  	if (!config_stats) {
2475  		return ENOENT;
2476  	}
2477  	tsdn_t *tsdn = tsd_tsdn(tsd);
2478  #define MUTEX_PROF_RESET(mtx)						\
2479      malloc_mutex_lock(tsdn, &mtx);					\
2480      malloc_mutex_prof_data_reset(tsdn, &mtx);				\
2481      malloc_mutex_unlock(tsdn, &mtx);
2482  	MUTEX_PROF_RESET(ctl_mtx);
2483  	if (have_background_thread) {
2484  		MUTEX_PROF_RESET(background_thread_lock);
2485  	}
2486  	if (config_prof && opt_prof) {
2487  		MUTEX_PROF_RESET(bt2gctx_mtx);
2488  	}
2489  	unsigned n = narenas_total_get();
2490  	for (unsigned i = 0; i < n; i++) {
2491  		arena_t *arena = arena_get(tsdn, i, false);
2492  		if (!arena) {
2493  			continue;
2494  		}
2495  		MUTEX_PROF_RESET(arena->large_mtx);
2496  		MUTEX_PROF_RESET(arena->extent_avail_mtx);
2497  		MUTEX_PROF_RESET(arena->extents_dirty.mtx);
2498  		MUTEX_PROF_RESET(arena->extents_muzzy.mtx);
2499  		MUTEX_PROF_RESET(arena->extents_retained.mtx);
2500  		MUTEX_PROF_RESET(arena->decay_dirty.mtx);
2501  		MUTEX_PROF_RESET(arena->decay_muzzy.mtx);
2502  		MUTEX_PROF_RESET(arena->tcache_ql_mtx);
2503  		MUTEX_PROF_RESET(arena->base->mtx);
2504  		for (szind_t i = 0; i < SC_NBINS; i++) {
2505  			for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
2506  				bin_t *bin = &arena->bins[i].bin_shards[j];
2507  				MUTEX_PROF_RESET(bin->lock);
2508  			}
2509  		}
2510  	}
2511  #undef MUTEX_PROF_RESET
2512  	return 0;
2513  }
2514  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nmalloc,
2515      arenas_i(mib[2])->astats->bstats[mib[4]].nmalloc, uint64_t)
2516  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_ndalloc,
2517      arenas_i(mib[2])->astats->bstats[mib[4]].ndalloc, uint64_t)
2518  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nrequests,
2519      arenas_i(mib[2])->astats->bstats[mib[4]].nrequests, uint64_t)
2520  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_curregs,
2521      arenas_i(mib[2])->astats->bstats[mib[4]].curregs, size_t)
2522  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nfills,
2523      arenas_i(mib[2])->astats->bstats[mib[4]].nfills, uint64_t)
2524  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nflushes,
2525      arenas_i(mib[2])->astats->bstats[mib[4]].nflushes, uint64_t)
2526  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nslabs,
2527      arenas_i(mib[2])->astats->bstats[mib[4]].nslabs, uint64_t)
2528  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nreslabs,
2529      arenas_i(mib[2])->astats->bstats[mib[4]].reslabs, uint64_t)
2530  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_curslabs,
2531      arenas_i(mib[2])->astats->bstats[mib[4]].curslabs, size_t)
2532  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nonfull_slabs,
2533      arenas_i(mib[2])->astats->bstats[mib[4]].nonfull_slabs, size_t)
2534  static const ctl_named_node_t *
2535  stats_arenas_i_bins_j_index(tsdn_t *tsdn, const size_t *mib,
2536      size_t miblen, size_t j) {
2537  	if (j > SC_NBINS) {
2538  		return NULL;
2539  	}
2540  	return super_stats_arenas_i_bins_j_node;
2541  }
2542  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_nmalloc,
2543      ctl_arena_stats_read_u64(
2544      &arenas_i(mib[2])->astats->lstats[mib[4]].nmalloc), uint64_t)
2545  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_ndalloc,
2546      ctl_arena_stats_read_u64(
2547      &arenas_i(mib[2])->astats->lstats[mib[4]].ndalloc), uint64_t)
2548  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_nrequests,
2549      ctl_arena_stats_read_u64(
2550      &arenas_i(mib[2])->astats->lstats[mib[4]].nrequests), uint64_t)
2551  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_curlextents,
2552      arenas_i(mib[2])->astats->lstats[mib[4]].curlextents, size_t)
2553  static const ctl_named_node_t *
2554  stats_arenas_i_lextents_j_index(tsdn_t *tsdn, const size_t *mib,
2555      size_t miblen, size_t j) {
2556  	if (j > SC_NSIZES - SC_NBINS) {
2557  		return NULL;
2558  	}
2559  	return super_stats_arenas_i_lextents_j_node;
2560  }
2561  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_ndirty,
2562      atomic_load_zu(
2563          &arenas_i(mib[2])->astats->estats[mib[4]].ndirty,
2564  	ATOMIC_RELAXED), size_t);
2565  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_nmuzzy,
2566      atomic_load_zu(
2567          &arenas_i(mib[2])->astats->estats[mib[4]].nmuzzy,
2568  	ATOMIC_RELAXED), size_t);
2569  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_nretained,
2570      atomic_load_zu(
2571          &arenas_i(mib[2])->astats->estats[mib[4]].nretained,
2572  	ATOMIC_RELAXED), size_t);
2573  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_dirty_bytes,
2574      atomic_load_zu(
2575          &arenas_i(mib[2])->astats->estats[mib[4]].dirty_bytes,
2576  	ATOMIC_RELAXED), size_t);
2577  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_muzzy_bytes,
2578      atomic_load_zu(
2579          &arenas_i(mib[2])->astats->estats[mib[4]].muzzy_bytes,
2580  	ATOMIC_RELAXED), size_t);
2581  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_retained_bytes,
2582      atomic_load_zu(
2583          &arenas_i(mib[2])->astats->estats[mib[4]].retained_bytes,
2584  	ATOMIC_RELAXED), size_t);
2585  static const ctl_named_node_t *
2586  stats_arenas_i_extents_j_index(tsdn_t *tsdn, const size_t *mib,
2587      size_t miblen, size_t j) {
2588  	if (j >= SC_NPSIZES) {
2589  		return NULL;
2590  	}
2591  	return super_stats_arenas_i_extents_j_node;
2592  }
2593  static bool
2594  ctl_arenas_i_verify(size_t i) {
2595  	size_t a = arenas_i2a_impl(i, true, true);
2596  	if (a == UINT_MAX || !ctl_arenas->arenas[a]->initialized) {
2597  		return true;
2598  	}
2599  	return false;
2600  }
2601  static const ctl_named_node_t *
2602  stats_arenas_i_index(tsdn_t *tsdn, const size_t *mib,
2603      size_t miblen, size_t i) {
2604  	const ctl_named_node_t *ret;
2605  	malloc_mutex_lock(tsdn, &ctl_mtx);
2606  	if (ctl_arenas_i_verify(i)) {
2607  		ret = NULL;
2608  		goto label_return;
2609  	}
2610  	ret = super_stats_arenas_i_node;
2611  label_return:
2612  	malloc_mutex_unlock(tsdn, &ctl_mtx);
2613  	return ret;
2614  }
2615  static int
2616  experimental_hooks_install_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2617      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2618  	int ret;
2619  	if (oldp == NULL || oldlenp == NULL|| newp == NULL) {
2620  		ret = EINVAL;
2621  		goto label_return;
2622  	}
2623  	hooks_t hooks;
2624  	WRITE(hooks, hooks_t);
2625  	void *handle = hook_install(tsd_tsdn(tsd), &hooks);
2626  	if (handle == NULL) {
2627  		ret = EAGAIN;
2628  		goto label_return;
2629  	}
2630  	READ(handle, void *);
2631  	ret = 0;
2632  label_return:
2633  	return ret;
2634  }
2635  static int
2636  experimental_hooks_remove_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2637      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2638  	int ret;
2639  	WRITEONLY();
2640  	void *handle = NULL;
2641  	WRITE(handle, void *);
2642  	if (handle == NULL) {
2643  		ret = EINVAL;
2644  		goto label_return;
2645  	}
2646  	hook_remove(tsd_tsdn(tsd), handle);
2647  	ret = 0;
2648  label_return:
2649  	return ret;
2650  }
2651  static int
2652  experimental_utilization_query_ctl(tsd_t *tsd, const size_t *mib,
2653      size_t miblen, void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2654  	int ret;
2655  	assert(sizeof(extent_util_stats_verbose_t)
2656  	    == sizeof(void *) + sizeof(size_t) * 5);
2657  	if (oldp == NULL || oldlenp == NULL
2658  	    || *oldlenp != sizeof(extent_util_stats_verbose_t)
2659  	    || newp == NULL) {
2660  		ret = EINVAL;
2661  		goto label_return;
2662  	}
2663  	void *ptr = NULL;
2664  	WRITE(ptr, void *);
2665  	extent_util_stats_verbose_t *util_stats
2666  	    = (extent_util_stats_verbose_t *)oldp;
2667  	extent_util_stats_verbose_get(tsd_tsdn(tsd), ptr,
2668  	    &util_stats->nfree, &util_stats->nregs, &util_stats->size,
2669  	    &util_stats->bin_nfree, &util_stats->bin_nregs,
2670  	    &util_stats->slabcur_addr);
2671  	ret = 0;
2672  label_return:
2673  	return ret;
2674  }
2675  static int
2676  experimental_utilization_batch_query_ctl(tsd_t *tsd, const size_t *mib,
2677      size_t miblen, void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2678  	int ret;
2679  	assert(sizeof(extent_util_stats_t) == sizeof(size_t) * 3);
2680  	const size_t len = newlen / sizeof(const void *);
2681  	if (oldp == NULL || oldlenp == NULL || newp == NULL || newlen == 0
2682  	    || newlen != len * sizeof(const void *)
2683  	    || *oldlenp != len * sizeof(extent_util_stats_t)) {
2684  		ret = EINVAL;
2685  		goto label_return;
2686  	}
2687  	void **ptrs = (void **)newp;
2688  	extent_util_stats_t *util_stats = (extent_util_stats_t *)oldp;
2689  	size_t i;
2690  	for (i = 0; i < len; ++i) {
2691  		extent_util_stats_get(tsd_tsdn(tsd), ptrs[i],
2692  		    &util_stats[i].nfree, &util_stats[i].nregs,
2693  		    &util_stats[i].size);
2694  	}
2695  	ret = 0;
2696  label_return:
2697  	return ret;
2698  }
2699  static const ctl_named_node_t *
2700  experimental_arenas_i_index(tsdn_t *tsdn, const size_t *mib,
2701      size_t miblen, size_t i) {
2702  	const ctl_named_node_t *ret;
2703  	malloc_mutex_lock(tsdn, &ctl_mtx);
2704  	if (ctl_arenas_i_verify(i)) {
2705  		ret = NULL;
2706  		goto label_return;
2707  	}
2708  	ret = super_experimental_arenas_i_node;
2709  label_return:
2710  	malloc_mutex_unlock(tsdn, &ctl_mtx);
2711  	return ret;
2712  }
2713  static int
2714  experimental_arenas_i_pactivep_ctl(tsd_t *tsd, const size_t *mib,
2715      size_t miblen, void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2716  	if (!config_stats) {
2717  		return ENOENT;
2718  	}
2719  	if (oldp == NULL || oldlenp == NULL || *oldlenp != sizeof(size_t *)) {
2720  		return EINVAL;
2721  	}
2722  	unsigned arena_ind;
2723  	arena_t *arena;
2724  	int ret;
2725  	size_t *pactivep;
2726  	malloc_mutex_lock(tsd_tsdn(tsd), &ctl_mtx);
2727  	READONLY();
2728  	MIB_UNSIGNED(arena_ind, 2);
2729  	if (arena_ind < narenas_total_get() && (arena =
2730  	    arena_get(tsd_tsdn(tsd), arena_ind, false)) != NULL) {
2731  #if defined(JEMALLOC_GCC_ATOMIC_ATOMICS) ||				\
2732      defined(JEMALLOC_GCC_SYNC_ATOMICS) || defined(_MSC_VER)
2733  		pactivep = (size_t *)&(arena->nactive.repr);
2734  		READ(pactivep, size_t *);
2735  		ret = 0;
2736  #else
2737  		ret = EFAULT;
2738  #endif
2739  	} else {
2740  		ret = EFAULT;
2741  	}
2742  label_return:
2743  	malloc_mutex_unlock(tsd_tsdn(tsd), &ctl_mtx);
2744  	return ret;
2745  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from Adafruit_nRF52_Arduino-MDEwOlJlcG9zaXRvcnk3NDM1NDcyOQ==-flat-nrfx_pwm.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-ctl.c</div>
                </div>
                <div class="column column_space"><pre><code>172          return starting_task_address;
173  #endif
174      }
175      nrf_pwm_task_trigger(p_instance->p_registers, starting_task);
176      return 0;
177  }
178  uint32_t nrfx_pwm_simple_playback(nrfx_pwm_t const *         p_instance,
179                                    nrf_pwm_sequence_t const * p_sequence,
180                                    uint16_t                   playback_count,
181                                    uint32_t                   flags)
182  {
183      pwm_control_block_t * p_cb  = &m_cb[p_instance->drv_inst_idx];
</pre></code></div>
                <div class="column column_space"><pre><code>2159  		return NULL;
2160  	}
2161  	return super_arenas_lextent_i_node;
2162  }
2163  static int
2164  arenas_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2165      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    