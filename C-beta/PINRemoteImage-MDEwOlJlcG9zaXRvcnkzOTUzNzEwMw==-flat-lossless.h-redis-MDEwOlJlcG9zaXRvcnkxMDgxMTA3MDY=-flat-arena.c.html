
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 8.994484514212983%, Tokens: 21, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-lossless.h</h3>
            <pre><code>1  #ifndef WEBP_DSP_LOSSLESS_H_
2  #define WEBP_DSP_LOSSLESS_H_
3  #include "src/webp/types.h"
4  #include "src/webp/decode.h"
5  #include "src/enc/histogram_enc.h"
6  #include "src/utils/utils.h"
7  #ifdef __cplusplus
8  extern "C" {
9  #endif
10  typedef uint32_t (*VP8LPredictorFunc)(uint32_t left, const uint32_t* const top);
11  extern VP8LPredictorFunc VP8LPredictors[16];
12  extern VP8LPredictorFunc VP8LPredictors_C[16];
13  typedef void (*VP8LPredictorAddSubFunc)(const uint32_t* in,
14                                          const uint32_t* upper, int num_pixels,
15                                          uint32_t* out);
16  extern VP8LPredictorAddSubFunc VP8LPredictorsAdd[16];
17  extern VP8LPredictorAddSubFunc VP8LPredictorsAdd_C[16];
18  typedef void (*VP8LProcessDecBlueAndRedFunc)(const uint32_t* src,
19                                               int num_pixels, uint32_t* dst);
20  extern VP8LProcessDecBlueAndRedFunc VP8LAddGreenToBlueAndRed;
21  typedef struct {
22    uint8_t green_to_red_;
23    uint8_t green_to_blue_;
24    uint8_t red_to_blue_;
25  } VP8LMultipliers;
<span onclick='openModal()' class='match'>26  typedef void (*VP8LTransformColorInverseFunc)(const VP8LMultipliers* const m,
27                                                const uint32_t* src,
28                                                int num_pixels, uint32_t* dst);
29  extern VP8LTransformColorInverseFunc VP8LTransformColorInverse;
30  struct VP8LTransform;  
31  void VP8LInverseTransform(const struct VP8LTransform* const transform,
32                            int row_start, int row_end,
33                            const uint32_t* const in, uint32_t* const out);
34  typedef void (*VP8LConvertFunc)(const uint32_t* src, int num_pixels,
35                                  uint8_t* dst);
36  extern VP8LConvertFunc VP8LConvertBGRAToRGB;
37  extern VP8LConvertFunc VP8LConvertBGRAToRGBA;
38  extern VP8LConvertFunc VP8LConvertBGRAToRGBA4444;
39  extern VP8LConvertFunc VP8LConvertBGRAToRGB565;
40  extern VP8LConvertFunc VP8LConvertBGRAToBGR;
41  void VP8LConvertFromBGRA(const uint32_t* const in_data, int num_pixels,
42                           WEBP_CSP_MODE out_colorspace, uint8_t* const rgba);
</span>43  typedef void (*VP8LMapARGBFunc)(const uint32_t* src,
44                                  const uint32_t* const color_map,
45                                  uint32_t* dst, int y_start,
46                                  int y_end, int width);
47  typedef void (*VP8LMapAlphaFunc)(const uint8_t* src,
48                                   const uint32_t* const color_map,
49                                   uint8_t* dst, int y_start,
50                                   int y_end, int width);
51  extern VP8LMapARGBFunc VP8LMapColor32b;
52  extern VP8LMapAlphaFunc VP8LMapColor8b;
53  void VP8LColorIndexInverseTransformAlpha(
54      const struct VP8LTransform* const transform, int y_start, int y_end,
55      const uint8_t* src, uint8_t* dst);
56  void VP8LTransformColorInverse_C(const VP8LMultipliers* const m,
57                                   const uint32_t* src, int num_pixels,
58                                   uint32_t* dst);
59  void VP8LConvertBGRAToRGB_C(const uint32_t* src, int num_pixels, uint8_t* dst);
60  void VP8LConvertBGRAToRGBA_C(const uint32_t* src, int num_pixels, uint8_t* dst);
61  void VP8LConvertBGRAToRGBA4444_C(const uint32_t* src,
62                                   int num_pixels, uint8_t* dst);
63  void VP8LConvertBGRAToRGB565_C(const uint32_t* src,
64                                 int num_pixels, uint8_t* dst);
65  void VP8LConvertBGRAToBGR_C(const uint32_t* src, int num_pixels, uint8_t* dst);
66  void VP8LAddGreenToBlueAndRed_C(const uint32_t* src, int num_pixels,
67                                  uint32_t* dst);
68  void VP8LDspInit(void);
69  typedef void (*VP8LProcessEncBlueAndRedFunc)(uint32_t* dst, int num_pixels);
70  extern VP8LProcessEncBlueAndRedFunc VP8LSubtractGreenFromBlueAndRed;
71  typedef void (*VP8LTransformColorFunc)(const VP8LMultipliers* const m,
72                                         uint32_t* dst, int num_pixels);
73  extern VP8LTransformColorFunc VP8LTransformColor;
74  typedef void (*VP8LCollectColorBlueTransformsFunc)(
75      const uint32_t* argb, int stride,
76      int tile_width, int tile_height,
77      int green_to_blue, int red_to_blue, int histo[]);
78  extern VP8LCollectColorBlueTransformsFunc VP8LCollectColorBlueTransforms;
79  typedef void (*VP8LCollectColorRedTransformsFunc)(
80      const uint32_t* argb, int stride,
81      int tile_width, int tile_height,
82      int green_to_red, int histo[]);
83  extern VP8LCollectColorRedTransformsFunc VP8LCollectColorRedTransforms;
84  void VP8LTransformColor_C(const VP8LMultipliers* const m,
85                            uint32_t* data, int num_pixels);
86  void VP8LSubtractGreenFromBlueAndRed_C(uint32_t* argb_data, int num_pixels);
87  void VP8LCollectColorRedTransforms_C(const uint32_t* argb, int stride,
88                                       int tile_width, int tile_height,
89                                       int green_to_red, int histo[]);
90  void VP8LCollectColorBlueTransforms_C(const uint32_t* argb, int stride,
91                                        int tile_width, int tile_height,
92                                        int green_to_blue, int red_to_blue,
93                                        int histo[]);
94  extern VP8LPredictorAddSubFunc VP8LPredictorsSub[16];
95  extern VP8LPredictorAddSubFunc VP8LPredictorsSub_C[16];
96  typedef double (*VP8LCostFunc)(const uint32_t* population, int length);
97  typedef double (*VP8LCostCombinedFunc)(const uint32_t* X, const uint32_t* Y,
98                                         int length);
99  typedef float (*VP8LCombinedShannonEntropyFunc)(const int X[256],
100                                                  const int Y[256]);
101  extern VP8LCostFunc VP8LExtraCost;
102  extern VP8LCostCombinedFunc VP8LExtraCostCombined;
103  extern VP8LCombinedShannonEntropyFunc VP8LCombinedShannonEntropy;
104  typedef struct {        
105    int counts[2];        
106    int streaks[2][2];    
107  } VP8LStreaks;
108  typedef struct {            
109    double entropy;           
110    uint32_t sum;             
111    int nonzeros;             
112    uint32_t max_val;         
113    uint32_t nonzero_code;    
114  } VP8LBitEntropy;
115  void VP8LBitEntropyInit(VP8LBitEntropy* const entropy);
116  typedef void (*VP8LGetCombinedEntropyUnrefinedFunc)(
117      const uint32_t X[], const uint32_t Y[], int length,
118      VP8LBitEntropy* const bit_entropy, VP8LStreaks* const stats);
119  extern VP8LGetCombinedEntropyUnrefinedFunc VP8LGetCombinedEntropyUnrefined;
120  typedef void (*VP8LGetEntropyUnrefinedFunc)(const uint32_t X[], int length,
121                                              VP8LBitEntropy* const bit_entropy,
122                                              VP8LStreaks* const stats);
123  extern VP8LGetEntropyUnrefinedFunc VP8LGetEntropyUnrefined;
124  void VP8LBitsEntropyUnrefined(const uint32_t* const array, int n,
125                                VP8LBitEntropy* const entropy);
126  typedef void (*VP8LAddVectorFunc)(const uint32_t* a, const uint32_t* b,
127                                    uint32_t* out, int size);
128  extern VP8LAddVectorFunc VP8LAddVector;
129  typedef void (*VP8LAddVectorEqFunc)(const uint32_t* a, uint32_t* out, int size);
130  extern VP8LAddVectorEqFunc VP8LAddVectorEq;
131  void VP8LHistogramAdd(const VP8LHistogram* const a,
132                        const VP8LHistogram* const b,
133                        VP8LHistogram* const out);
134  typedef int (*VP8LVectorMismatchFunc)(const uint32_t* const array1,
135                                        const uint32_t* const array2, int length);
136  extern VP8LVectorMismatchFunc VP8LVectorMismatch;
137  typedef void (*VP8LBundleColorMapFunc)(const uint8_t* const row, int width,
138                                         int xbits, uint32_t* dst);
139  extern VP8LBundleColorMapFunc VP8LBundleColorMap;
140  void VP8LBundleColorMap_C(const uint8_t* const row, int width, int xbits,
141                            uint32_t* dst);
142  void VP8LEncDspInit(void);
143  #ifdef __cplusplus
144  }    
145  #endif
146  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-arena.c</h3>
            <pre><code>1  #define JEMALLOC_ARENA_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/div.h"
6  #include "jemalloc/internal/extent_dss.h"
7  #include "jemalloc/internal/extent_mmap.h"
8  #include "jemalloc/internal/mutex.h"
9  #include "jemalloc/internal/rtree.h"
10  #include "jemalloc/internal/safety_check.h"
11  #include "jemalloc/internal/util.h"
12  JEMALLOC_DIAGNOSTIC_DISABLE_SPURIOUS
13  const char *percpu_arena_mode_names[] = {
14  	"percpu",
15  	"phycpu",
16  	"disabled",
17  	"percpu",
18  	"phycpu"
19  };
20  percpu_arena_mode_t opt_percpu_arena = PERCPU_ARENA_DEFAULT;
21  ssize_t opt_dirty_decay_ms = DIRTY_DECAY_MS_DEFAULT;
22  ssize_t opt_muzzy_decay_ms = MUZZY_DECAY_MS_DEFAULT;
23  static atomic_zd_t dirty_decay_ms_default;
24  static atomic_zd_t muzzy_decay_ms_default;
25  const uint64_t h_steps[SMOOTHSTEP_NSTEPS] = {
26  #define STEP(step, h, x, y)			\
27  		h,
28  		SMOOTHSTEP
29  #undef STEP
30  };
31  static div_info_t arena_binind_div_info[SC_NBINS];
32  size_t opt_oversize_threshold = OVERSIZE_THRESHOLD_DEFAULT;
33  size_t oversize_threshold = OVERSIZE_THRESHOLD_DEFAULT;
<span onclick='openModal()' class='match'>34  static unsigned huge_arena_ind;
35  static void arena_decay_to_limit(tsdn_t *tsdn, arena_t *arena,
36      arena_decay_t *decay, extents_t *extents, bool all, size_t npages_limit,
37      size_t npages_decay_max, bool is_background_thread);
38  static bool arena_decay_dirty(tsdn_t *tsdn, arena_t *arena,
39      bool is_background_thread, bool all);
40  static void arena_dalloc_bin_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
41      bin_t *bin);
42  static void arena_bin_lower_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
43      bin_t *bin);
</span>44  void
45  arena_basic_stats_merge(tsdn_t *tsdn, arena_t *arena, unsigned *nthreads,
46      const char **dss, ssize_t *dirty_decay_ms, ssize_t *muzzy_decay_ms,
47      size_t *nactive, size_t *ndirty, size_t *nmuzzy) {
48  	*nthreads += arena_nthreads_get(arena, false);
49  	*dss = dss_prec_names[arena_dss_prec_get(arena)];
50  	*dirty_decay_ms = arena_dirty_decay_ms_get(arena);
51  	*muzzy_decay_ms = arena_muzzy_decay_ms_get(arena);
52  	*nactive += atomic_load_zu(&arena->nactive, ATOMIC_RELAXED);
53  	*ndirty += extents_npages_get(&arena->extents_dirty);
54  	*nmuzzy += extents_npages_get(&arena->extents_muzzy);
55  }
56  void
57  arena_stats_merge(tsdn_t *tsdn, arena_t *arena, unsigned *nthreads,
58      const char **dss, ssize_t *dirty_decay_ms, ssize_t *muzzy_decay_ms,
59      size_t *nactive, size_t *ndirty, size_t *nmuzzy, arena_stats_t *astats,
60      bin_stats_t *bstats, arena_stats_large_t *lstats,
61      arena_stats_extents_t *estats) {
62  	cassert(config_stats);
63  	arena_basic_stats_merge(tsdn, arena, nthreads, dss, dirty_decay_ms,
64  	    muzzy_decay_ms, nactive, ndirty, nmuzzy);
65  	size_t base_allocated, base_resident, base_mapped, metadata_thp;
66  	base_stats_get(tsdn, arena->base, &base_allocated, &base_resident,
67  	    &base_mapped, &metadata_thp);
68  	arena_stats_lock(tsdn, &arena->stats);
69  	arena_stats_accum_zu(&astats->mapped, base_mapped
70  	    + arena_stats_read_zu(tsdn, &arena->stats, &arena->stats.mapped));
71  	arena_stats_accum_zu(&astats->retained,
72  	    extents_npages_get(&arena->extents_retained) << LG_PAGE);
73  	atomic_store_zu(&astats->extent_avail,
74  	    atomic_load_zu(&arena->extent_avail_cnt, ATOMIC_RELAXED),
75  	    ATOMIC_RELAXED);
76  	arena_stats_accum_u64(&astats->decay_dirty.npurge,
77  	    arena_stats_read_u64(tsdn, &arena->stats,
78  	    &arena->stats.decay_dirty.npurge));
79  	arena_stats_accum_u64(&astats->decay_dirty.nmadvise,
80  	    arena_stats_read_u64(tsdn, &arena->stats,
81  	    &arena->stats.decay_dirty.nmadvise));
82  	arena_stats_accum_u64(&astats->decay_dirty.purged,
83  	    arena_stats_read_u64(tsdn, &arena->stats,
84  	    &arena->stats.decay_dirty.purged));
85  	arena_stats_accum_u64(&astats->decay_muzzy.npurge,
86  	    arena_stats_read_u64(tsdn, &arena->stats,
87  	    &arena->stats.decay_muzzy.npurge));
88  	arena_stats_accum_u64(&astats->decay_muzzy.nmadvise,
89  	    arena_stats_read_u64(tsdn, &arena->stats,
90  	    &arena->stats.decay_muzzy.nmadvise));
91  	arena_stats_accum_u64(&astats->decay_muzzy.purged,
92  	    arena_stats_read_u64(tsdn, &arena->stats,
93  	    &arena->stats.decay_muzzy.purged));
94  	arena_stats_accum_zu(&astats->base, base_allocated);
95  	arena_stats_accum_zu(&astats->internal, arena_internal_get(arena));
96  	arena_stats_accum_zu(&astats->metadata_thp, metadata_thp);
97  	arena_stats_accum_zu(&astats->resident, base_resident +
98  	    (((atomic_load_zu(&arena->nactive, ATOMIC_RELAXED) +
99  	    extents_npages_get(&arena->extents_dirty) +
100  	    extents_npages_get(&arena->extents_muzzy)) << LG_PAGE)));
101  	arena_stats_accum_zu(&astats->abandoned_vm, atomic_load_zu(
102  	    &arena->stats.abandoned_vm, ATOMIC_RELAXED));
103  	for (szind_t i = 0; i < SC_NSIZES - SC_NBINS; i++) {
104  		uint64_t nmalloc = arena_stats_read_u64(tsdn, &arena->stats,
105  		    &arena->stats.lstats[i].nmalloc);
106  		arena_stats_accum_u64(&lstats[i].nmalloc, nmalloc);
107  		arena_stats_accum_u64(&astats->nmalloc_large, nmalloc);
108  		uint64_t ndalloc = arena_stats_read_u64(tsdn, &arena->stats,
109  		    &arena->stats.lstats[i].ndalloc);
110  		arena_stats_accum_u64(&lstats[i].ndalloc, ndalloc);
111  		arena_stats_accum_u64(&astats->ndalloc_large, ndalloc);
112  		uint64_t nrequests = arena_stats_read_u64(tsdn, &arena->stats,
113  		    &arena->stats.lstats[i].nrequests);
114  		arena_stats_accum_u64(&lstats[i].nrequests,
115  		    nmalloc + nrequests);
116  		arena_stats_accum_u64(&astats->nrequests_large,
117  		    nmalloc + nrequests);
118  		arena_stats_accum_u64(&lstats[i].nfills, nmalloc);
119  		arena_stats_accum_u64(&astats->nfills_large, nmalloc);
120  		uint64_t nflush = arena_stats_read_u64(tsdn, &arena->stats,
121  		    &arena->stats.lstats[i].nflushes);
122  		arena_stats_accum_u64(&lstats[i].nflushes, nflush);
123  		arena_stats_accum_u64(&astats->nflushes_large, nflush);
124  		assert(nmalloc >= ndalloc);
125  		assert(nmalloc - ndalloc <= SIZE_T_MAX);
126  		size_t curlextents = (size_t)(nmalloc - ndalloc);
127  		lstats[i].curlextents += curlextents;
128  		arena_stats_accum_zu(&astats->allocated_large,
129  		    curlextents * sz_index2size(SC_NBINS + i));
130  	}
131  	for (pszind_t i = 0; i < SC_NPSIZES; i++) {
132  		size_t dirty, muzzy, retained, dirty_bytes, muzzy_bytes,
133  		    retained_bytes;
134  		dirty = extents_nextents_get(&arena->extents_dirty, i);
135  		muzzy = extents_nextents_get(&arena->extents_muzzy, i);
136  		retained = extents_nextents_get(&arena->extents_retained, i);
137  		dirty_bytes = extents_nbytes_get(&arena->extents_dirty, i);
138  		muzzy_bytes = extents_nbytes_get(&arena->extents_muzzy, i);
139  		retained_bytes =
140  		    extents_nbytes_get(&arena->extents_retained, i);
141  		atomic_store_zu(&estats[i].ndirty, dirty, ATOMIC_RELAXED);
142  		atomic_store_zu(&estats[i].nmuzzy, muzzy, ATOMIC_RELAXED);
143  		atomic_store_zu(&estats[i].nretained, retained, ATOMIC_RELAXED);
144  		atomic_store_zu(&estats[i].dirty_bytes, dirty_bytes,
145  		    ATOMIC_RELAXED);
146  		atomic_store_zu(&estats[i].muzzy_bytes, muzzy_bytes,
147  		    ATOMIC_RELAXED);
148  		atomic_store_zu(&estats[i].retained_bytes, retained_bytes,
149  		    ATOMIC_RELAXED);
150  	}
151  	arena_stats_unlock(tsdn, &arena->stats);
152  	atomic_store_zu(&astats->tcache_bytes, 0, ATOMIC_RELAXED);
153  	malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
154  	cache_bin_array_descriptor_t *descriptor;
155  	ql_foreach(descriptor, &arena->cache_bin_array_descriptor_ql, link) {
156  		szind_t i = 0;
157  		for (; i < SC_NBINS; i++) {
158  			cache_bin_t *tbin = &descriptor->bins_small[i];
159  			arena_stats_accum_zu(&astats->tcache_bytes,
160  			    tbin->ncached * sz_index2size(i));
161  		}
162  		for (; i < nhbins; i++) {
163  			cache_bin_t *tbin = &descriptor->bins_large[i];
164  			arena_stats_accum_zu(&astats->tcache_bytes,
165  			    tbin->ncached * sz_index2size(i));
166  		}
167  	}
168  	malloc_mutex_prof_read(tsdn,
169  	    &astats->mutex_prof_data[arena_prof_mutex_tcache_list],
170  	    &arena->tcache_ql_mtx);
171  	malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
172  #define READ_ARENA_MUTEX_PROF_DATA(mtx, ind)				\
173      malloc_mutex_lock(tsdn, &arena->mtx);				\
174      malloc_mutex_prof_read(tsdn, &astats->mutex_prof_data[ind],		\
175          &arena->mtx);							\
176      malloc_mutex_unlock(tsdn, &arena->mtx);
177  	READ_ARENA_MUTEX_PROF_DATA(large_mtx, arena_prof_mutex_large);
178  	READ_ARENA_MUTEX_PROF_DATA(extent_avail_mtx,
179  	    arena_prof_mutex_extent_avail)
180  	READ_ARENA_MUTEX_PROF_DATA(extents_dirty.mtx,
181  	    arena_prof_mutex_extents_dirty)
182  	READ_ARENA_MUTEX_PROF_DATA(extents_muzzy.mtx,
183  	    arena_prof_mutex_extents_muzzy)
184  	READ_ARENA_MUTEX_PROF_DATA(extents_retained.mtx,
185  	    arena_prof_mutex_extents_retained)
186  	READ_ARENA_MUTEX_PROF_DATA(decay_dirty.mtx,
187  	    arena_prof_mutex_decay_dirty)
188  	READ_ARENA_MUTEX_PROF_DATA(decay_muzzy.mtx,
189  	    arena_prof_mutex_decay_muzzy)
190  	READ_ARENA_MUTEX_PROF_DATA(base->mtx,
191  	    arena_prof_mutex_base)
192  #undef READ_ARENA_MUTEX_PROF_DATA
193  	nstime_copy(&astats->uptime, &arena->create_time);
194  	nstime_update(&astats->uptime);
195  	nstime_subtract(&astats->uptime, &arena->create_time);
196  	for (szind_t i = 0; i < SC_NBINS; i++) {
197  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
198  			bin_stats_merge(tsdn, &bstats[i],
199  			    &arena->bins[i].bin_shards[j]);
200  		}
201  	}
202  }
203  void
204  arena_extents_dirty_dalloc(tsdn_t *tsdn, arena_t *arena,
205      extent_hooks_t **r_extent_hooks, extent_t *extent) {
206  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
207  	    WITNESS_RANK_CORE, 0);
208  	extents_dalloc(tsdn, arena, r_extent_hooks, &arena->extents_dirty,
209  	    extent);
210  	if (arena_dirty_decay_ms_get(arena) == 0) {
211  		arena_decay_dirty(tsdn, arena, false, true);
212  	} else {
213  		arena_background_thread_inactivity_check(tsdn, arena, false);
214  	}
215  }
216  static void *
217  arena_slab_reg_alloc(extent_t *slab, const bin_info_t *bin_info) {
218  	void *ret;
219  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
220  	size_t regind;
221  	assert(extent_nfree_get(slab) > 0);
222  	assert(!bitmap_full(slab_data->bitmap, &bin_info->bitmap_info));
223  	regind = bitmap_sfu(slab_data->bitmap, &bin_info->bitmap_info);
224  	ret = (void *)((uintptr_t)extent_addr_get(slab) +
225  	    (uintptr_t)(bin_info->reg_size * regind));
226  	extent_nfree_dec(slab);
227  	return ret;
228  }
229  static void
230  arena_slab_reg_alloc_batch(extent_t *slab, const bin_info_t *bin_info,
231  			   unsigned cnt, void** ptrs) {
232  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
233  	assert(extent_nfree_get(slab) >= cnt);
234  	assert(!bitmap_full(slab_data->bitmap, &bin_info->bitmap_info));
235  #if (! defined JEMALLOC_INTERNAL_POPCOUNTL) || (defined BITMAP_USE_TREE)
236  	for (unsigned i = 0; i < cnt; i++) {
237  		size_t regind = bitmap_sfu(slab_data->bitmap,
238  					   &bin_info->bitmap_info);
239  		*(ptrs + i) = (void *)((uintptr_t)extent_addr_get(slab) +
240  		    (uintptr_t)(bin_info->reg_size * regind));
241  	}
242  #else
243  	unsigned group = 0;
244  	bitmap_t g = slab_data->bitmap[group];
245  	unsigned i = 0;
246  	while (i < cnt) {
247  		while (g == 0) {
248  			g = slab_data->bitmap[++group];
249  		}
250  		size_t shift = group << LG_BITMAP_GROUP_NBITS;
251  		size_t pop = popcount_lu(g);
252  		if (pop > (cnt - i)) {
253  			pop = cnt - i;
254  		}
255  		uintptr_t base = (uintptr_t)extent_addr_get(slab);
256  		uintptr_t regsize = (uintptr_t)bin_info->reg_size;
257  		while (pop--) {
258  			size_t bit = cfs_lu(&g);
259  			size_t regind = shift + bit;
260  			*(ptrs + i) = (void *)(base + regsize * regind);
261  			i++;
262  		}
263  		slab_data->bitmap[group] = g;
264  	}
265  #endif
266  	extent_nfree_sub(slab, cnt);
267  }
268  #ifndef JEMALLOC_JET
269  static
270  #endif
271  size_t
272  arena_slab_regind(extent_t *slab, szind_t binind, const void *ptr) {
273  	size_t diff, regind;
274  	assert((uintptr_t)ptr >= (uintptr_t)extent_addr_get(slab));
275  	assert((uintptr_t)ptr < (uintptr_t)extent_past_get(slab));
276  	assert(((uintptr_t)ptr - (uintptr_t)extent_addr_get(slab)) %
277  	    (uintptr_t)bin_infos[binind].reg_size == 0);
278  	diff = (size_t)((uintptr_t)ptr - (uintptr_t)extent_addr_get(slab));
279  	regind = div_compute(&arena_binind_div_info[binind], diff);
280  	assert(regind < bin_infos[binind].nregs);
281  	return regind;
282  }
283  static void
284  arena_slab_reg_dalloc(extent_t *slab, arena_slab_data_t *slab_data, void *ptr) {
285  	szind_t binind = extent_szind_get(slab);
286  	const bin_info_t *bin_info = &bin_infos[binind];
287  	size_t regind = arena_slab_regind(slab, binind, ptr);
288  	assert(extent_nfree_get(slab) < bin_info->nregs);
289  	assert(bitmap_get(slab_data->bitmap, &bin_info->bitmap_info, regind));
290  	bitmap_unset(slab_data->bitmap, &bin_info->bitmap_info, regind);
291  	extent_nfree_inc(slab);
292  }
293  static void
294  arena_nactive_add(arena_t *arena, size_t add_pages) {
295  	atomic_fetch_add_zu(&arena->nactive, add_pages, ATOMIC_RELAXED);
296  }
297  static void
298  arena_nactive_sub(arena_t *arena, size_t sub_pages) {
299  	assert(atomic_load_zu(&arena->nactive, ATOMIC_RELAXED) >= sub_pages);
300  	atomic_fetch_sub_zu(&arena->nactive, sub_pages, ATOMIC_RELAXED);
301  }
302  static void
303  arena_large_malloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
304  	szind_t index, hindex;
305  	cassert(config_stats);
306  	if (usize < SC_LARGE_MINCLASS) {
307  		usize = SC_LARGE_MINCLASS;
308  	}
309  	index = sz_size2index(usize);
310  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
311  	arena_stats_add_u64(tsdn, &arena->stats,
312  	    &arena->stats.lstats[hindex].nmalloc, 1);
313  }
314  static void
315  arena_large_dalloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
316  	szind_t index, hindex;
317  	cassert(config_stats);
318  	if (usize < SC_LARGE_MINCLASS) {
319  		usize = SC_LARGE_MINCLASS;
320  	}
321  	index = sz_size2index(usize);
322  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
323  	arena_stats_add_u64(tsdn, &arena->stats,
324  	    &arena->stats.lstats[hindex].ndalloc, 1);
325  }
326  static void
327  arena_large_ralloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t oldusize,
328      size_t usize) {
329  	arena_large_dalloc_stats_update(tsdn, arena, oldusize);
330  	arena_large_malloc_stats_update(tsdn, arena, usize);
331  }
332  static bool
333  arena_may_have_muzzy(arena_t *arena) {
334  	return (pages_can_purge_lazy && (arena_muzzy_decay_ms_get(arena) != 0));
335  }
336  extent_t *
337  arena_extent_alloc_large(tsdn_t *tsdn, arena_t *arena, size_t usize,
338      size_t alignment, bool *zero) {
339  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
340  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
341  	    WITNESS_RANK_CORE, 0);
342  	szind_t szind = sz_size2index(usize);
343  	size_t mapped_add;
344  	bool commit = true;
345  	extent_t *extent = extents_alloc(tsdn, arena, &extent_hooks,
346  	    &arena->extents_dirty, NULL, usize, sz_large_pad, alignment, false,
347  	    szind, zero, &commit);
348  	if (extent == NULL && arena_may_have_muzzy(arena)) {
349  		extent = extents_alloc(tsdn, arena, &extent_hooks,
350  		    &arena->extents_muzzy, NULL, usize, sz_large_pad, alignment,
351  		    false, szind, zero, &commit);
352  	}
353  	size_t size = usize + sz_large_pad;
354  	if (extent == NULL) {
355  		extent = extent_alloc_wrapper(tsdn, arena, &extent_hooks, NULL,
356  		    usize, sz_large_pad, alignment, false, szind, zero,
357  		    &commit);
358  		if (config_stats) {
359  			mapped_add = size;
360  		}
361  	} else if (config_stats) {
362  		mapped_add = 0;
363  	}
364  	if (extent != NULL) {
365  		if (config_stats) {
366  			arena_stats_lock(tsdn, &arena->stats);
367  			arena_large_malloc_stats_update(tsdn, arena, usize);
368  			if (mapped_add != 0) {
369  				arena_stats_add_zu(tsdn, &arena->stats,
370  				    &arena->stats.mapped, mapped_add);
371  			}
372  			arena_stats_unlock(tsdn, &arena->stats);
373  		}
374  		arena_nactive_add(arena, size >> LG_PAGE);
375  	}
376  	return extent;
377  }
378  void
379  arena_extent_dalloc_large_prep(tsdn_t *tsdn, arena_t *arena, extent_t *extent) {
380  	if (config_stats) {
381  		arena_stats_lock(tsdn, &arena->stats);
382  		arena_large_dalloc_stats_update(tsdn, arena,
383  		    extent_usize_get(extent));
384  		arena_stats_unlock(tsdn, &arena->stats);
385  	}
386  	arena_nactive_sub(arena, extent_size_get(extent) >> LG_PAGE);
387  }
388  void
389  arena_extent_ralloc_large_shrink(tsdn_t *tsdn, arena_t *arena, extent_t *extent,
390      size_t oldusize) {
391  	size_t usize = extent_usize_get(extent);
392  	size_t udiff = oldusize - usize;
393  	if (config_stats) {
394  		arena_stats_lock(tsdn, &arena->stats);
395  		arena_large_ralloc_stats_update(tsdn, arena, oldusize, usize);
396  		arena_stats_unlock(tsdn, &arena->stats);
397  	}
398  	arena_nactive_sub(arena, udiff >> LG_PAGE);
399  }
400  void
401  arena_extent_ralloc_large_expand(tsdn_t *tsdn, arena_t *arena, extent_t *extent,
402      size_t oldusize) {
403  	size_t usize = extent_usize_get(extent);
404  	size_t udiff = usize - oldusize;
405  	if (config_stats) {
406  		arena_stats_lock(tsdn, &arena->stats);
407  		arena_large_ralloc_stats_update(tsdn, arena, oldusize, usize);
408  		arena_stats_unlock(tsdn, &arena->stats);
409  	}
410  	arena_nactive_add(arena, udiff >> LG_PAGE);
411  }
412  static ssize_t
413  arena_decay_ms_read(arena_decay_t *decay) {
414  	return atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);
415  }
416  static void
417  arena_decay_ms_write(arena_decay_t *decay, ssize_t decay_ms) {
418  	atomic_store_zd(&decay->time_ms, decay_ms, ATOMIC_RELAXED);
419  }
420  static void
421  arena_decay_deadline_init(arena_decay_t *decay) {
422  	nstime_copy(&decay->deadline, &decay->epoch);
423  	nstime_add(&decay->deadline, &decay->interval);
424  	if (arena_decay_ms_read(decay) > 0) {
425  		nstime_t jitter;
426  		nstime_init(&jitter, prng_range_u64(&decay->jitter_state,
427  		    nstime_ns(&decay->interval)));
428  		nstime_add(&decay->deadline, &jitter);
429  	}
430  }
431  static bool
432  arena_decay_deadline_reached(const arena_decay_t *decay, const nstime_t *time) {
433  	return (nstime_compare(&decay->deadline, time) <= 0);
434  }
435  static size_t
436  arena_decay_backlog_npages_limit(const arena_decay_t *decay) {
437  	uint64_t sum;
438  	size_t npages_limit_backlog;
439  	unsigned i;
440  	sum = 0;
441  	for (i = 0; i < SMOOTHSTEP_NSTEPS; i++) {
442  		sum += decay->backlog[i] * h_steps[i];
443  	}
444  	npages_limit_backlog = (size_t)(sum >> SMOOTHSTEP_BFP);
445  	return npages_limit_backlog;
446  }
447  static void
448  arena_decay_backlog_update_last(arena_decay_t *decay, size_t current_npages) {
449  	size_t npages_delta = (current_npages > decay->nunpurged) ?
450  	    current_npages - decay->nunpurged : 0;
451  	decay->backlog[SMOOTHSTEP_NSTEPS-1] = npages_delta;
452  	if (config_debug) {
453  		if (current_npages > decay->ceil_npages) {
454  			decay->ceil_npages = current_npages;
455  		}
456  		size_t npages_limit = arena_decay_backlog_npages_limit(decay);
457  		assert(decay->ceil_npages >= npages_limit);
458  		if (decay->ceil_npages > npages_limit) {
459  			decay->ceil_npages = npages_limit;
460  		}
461  	}
462  }
463  static void
464  arena_decay_backlog_update(arena_decay_t *decay, uint64_t nadvance_u64,
465      size_t current_npages) {
466  	if (nadvance_u64 >= SMOOTHSTEP_NSTEPS) {
467  		memset(decay->backlog, 0, (SMOOTHSTEP_NSTEPS-1) *
468  		    sizeof(size_t));
469  	} else {
470  		size_t nadvance_z = (size_t)nadvance_u64;
471  		assert((uint64_t)nadvance_z == nadvance_u64);
472  		memmove(decay->backlog, &decay->backlog[nadvance_z],
473  		    (SMOOTHSTEP_NSTEPS - nadvance_z) * sizeof(size_t));
474  		if (nadvance_z > 1) {
475  			memset(&decay->backlog[SMOOTHSTEP_NSTEPS -
476  			    nadvance_z], 0, (nadvance_z-1) * sizeof(size_t));
477  		}
478  	}
479  	arena_decay_backlog_update_last(decay, current_npages);
480  }
481  static void
482  arena_decay_try_purge(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
483      extents_t *extents, size_t current_npages, size_t npages_limit,
484      bool is_background_thread) {
485  	if (current_npages > npages_limit) {
486  		arena_decay_to_limit(tsdn, arena, decay, extents, false,
487  		    npages_limit, current_npages - npages_limit,
488  		    is_background_thread);
489  	}
490  }
491  static void
492  arena_decay_epoch_advance_helper(arena_decay_t *decay, const nstime_t *time,
493      size_t current_npages) {
494  	assert(arena_decay_deadline_reached(decay, time));
495  	nstime_t delta;
496  	nstime_copy(&delta, time);
497  	nstime_subtract(&delta, &decay->epoch);
498  	uint64_t nadvance_u64 = nstime_divide(&delta, &decay->interval);
499  	assert(nadvance_u64 > 0);
500  	nstime_copy(&delta, &decay->interval);
501  	nstime_imultiply(&delta, nadvance_u64);
502  	nstime_add(&decay->epoch, &delta);
503  	arena_decay_deadline_init(decay);
504  	arena_decay_backlog_update(decay, nadvance_u64, current_npages);
505  }
506  static void
507  arena_decay_epoch_advance(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
508      extents_t *extents, const nstime_t *time, bool is_background_thread) {
509  	size_t current_npages = extents_npages_get(extents);
510  	arena_decay_epoch_advance_helper(decay, time, current_npages);
511  	size_t npages_limit = arena_decay_backlog_npages_limit(decay);
512  	decay->nunpurged = (npages_limit > current_npages) ? npages_limit :
513  	    current_npages;
514  	if (!background_thread_enabled() || is_background_thread) {
515  		arena_decay_try_purge(tsdn, arena, decay, extents,
516  		    current_npages, npages_limit, is_background_thread);
517  	}
518  }
519  static void
520  arena_decay_reinit(arena_decay_t *decay, ssize_t decay_ms) {
521  	arena_decay_ms_write(decay, decay_ms);
522  	if (decay_ms > 0) {
523  		nstime_init(&decay->interval, (uint64_t)decay_ms *
524  		    KQU(1000000));
525  		nstime_idivide(&decay->interval, SMOOTHSTEP_NSTEPS);
526  	}
527  	nstime_init(&decay->epoch, 0);
528  	nstime_update(&decay->epoch);
529  	decay->jitter_state = (uint64_t)(uintptr_t)decay;
530  	arena_decay_deadline_init(decay);
531  	decay->nunpurged = 0;
532  	memset(decay->backlog, 0, SMOOTHSTEP_NSTEPS * sizeof(size_t));
533  }
534  static bool
535  arena_decay_init(arena_decay_t *decay, ssize_t decay_ms,
536      arena_stats_decay_t *stats) {
537  	if (config_debug) {
538  		for (size_t i = 0; i < sizeof(arena_decay_t); i++) {
539  			assert(((char *)decay)[i] == 0);
540  		}
541  		decay->ceil_npages = 0;
542  	}
543  	if (malloc_mutex_init(&decay->mtx, "decay", WITNESS_RANK_DECAY,
544  	    malloc_mutex_rank_exclusive)) {
545  		return true;
546  	}
547  	decay->purging = false;
548  	arena_decay_reinit(decay, decay_ms);
549  	if (config_stats) {
550  		decay->stats = stats;
551  	}
552  	return false;
553  }
554  static bool
555  arena_decay_ms_valid(ssize_t decay_ms) {
556  	if (decay_ms < -1) {
557  		return false;
558  	}
559  	if (decay_ms == -1 || (uint64_t)decay_ms <= NSTIME_SEC_MAX *
560  	    KQU(1000)) {
561  		return true;
562  	}
563  	return false;
564  }
565  static bool
566  arena_maybe_decay(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
567      extents_t *extents, bool is_background_thread) {
568  	malloc_mutex_assert_owner(tsdn, &decay->mtx);
569  	ssize_t decay_ms = arena_decay_ms_read(decay);
570  	if (decay_ms <= 0) {
571  		if (decay_ms == 0) {
572  			arena_decay_to_limit(tsdn, arena, decay, extents, false,
573  			    0, extents_npages_get(extents),
574  			    is_background_thread);
575  		}
576  		return false;
577  	}
578  	nstime_t time;
579  	nstime_init(&time, 0);
580  	nstime_update(&time);
581  	if (unlikely(!nstime_monotonic() && nstime_compare(&decay->epoch, &time)
582  	    > 0)) {
583  		nstime_copy(&decay->epoch, &time);
584  		arena_decay_deadline_init(decay);
585  	} else {
586  		assert(nstime_compare(&decay->epoch, &time) <= 0);
587  	}
588  	bool advance_epoch = arena_decay_deadline_reached(decay, &time);
589  	if (advance_epoch) {
590  		arena_decay_epoch_advance(tsdn, arena, decay, extents, &time,
591  		    is_background_thread);
592  	} else if (is_background_thread) {
593  		arena_decay_try_purge(tsdn, arena, decay, extents,
594  		    extents_npages_get(extents),
595  		    arena_decay_backlog_npages_limit(decay),
596  		    is_background_thread);
597  	}
598  	return advance_epoch;
599  }
600  static ssize_t
601  arena_decay_ms_get(arena_decay_t *decay) {
602  	return arena_decay_ms_read(decay);
603  }
604  ssize_t
605  arena_dirty_decay_ms_get(arena_t *arena) {
606  	return arena_decay_ms_get(&arena->decay_dirty);
607  }
608  ssize_t
609  arena_muzzy_decay_ms_get(arena_t *arena) {
610  	return arena_decay_ms_get(&arena->decay_muzzy);
611  }
612  static bool
613  arena_decay_ms_set(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
614      extents_t *extents, ssize_t decay_ms) {
615  	if (!arena_decay_ms_valid(decay_ms)) {
616  		return true;
617  	}
618  	malloc_mutex_lock(tsdn, &decay->mtx);
619  	arena_decay_reinit(decay, decay_ms);
620  	arena_maybe_decay(tsdn, arena, decay, extents, false);
621  	malloc_mutex_unlock(tsdn, &decay->mtx);
622  	return false;
623  }
624  bool
625  arena_dirty_decay_ms_set(tsdn_t *tsdn, arena_t *arena,
626      ssize_t decay_ms) {
627  	return arena_decay_ms_set(tsdn, arena, &arena->decay_dirty,
628  	    &arena->extents_dirty, decay_ms);
629  }
630  bool
631  arena_muzzy_decay_ms_set(tsdn_t *tsdn, arena_t *arena,
632      ssize_t decay_ms) {
633  	return arena_decay_ms_set(tsdn, arena, &arena->decay_muzzy,
634  	    &arena->extents_muzzy, decay_ms);
635  }
636  static size_t
637  arena_stash_decayed(tsdn_t *tsdn, arena_t *arena,
638      extent_hooks_t **r_extent_hooks, extents_t *extents, size_t npages_limit,
639  	size_t npages_decay_max, extent_list_t *decay_extents) {
640  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
641  	    WITNESS_RANK_CORE, 0);
642  	size_t nstashed = 0;
643  	extent_t *extent;
644  	while (nstashed < npages_decay_max &&
645  	    (extent = extents_evict(tsdn, arena, r_extent_hooks, extents,
646  	    npages_limit)) != NULL) {
647  		extent_list_append(decay_extents, extent);
648  		nstashed += extent_size_get(extent) >> LG_PAGE;
649  	}
650  	return nstashed;
651  }
652  static size_t
653  arena_decay_stashed(tsdn_t *tsdn, arena_t *arena,
654      extent_hooks_t **r_extent_hooks, arena_decay_t *decay, extents_t *extents,
655      bool all, extent_list_t *decay_extents, bool is_background_thread) {
656  	size_t nmadvise, nunmapped;
657  	size_t npurged;
658  	if (config_stats) {
659  		nmadvise = 0;
660  		nunmapped = 0;
661  	}
662  	npurged = 0;
663  	ssize_t muzzy_decay_ms = arena_muzzy_decay_ms_get(arena);
664  	for (extent_t *extent = extent_list_first(decay_extents); extent !=
665  	    NULL; extent = extent_list_first(decay_extents)) {
666  		if (config_stats) {
667  			nmadvise++;
668  		}
669  		size_t npages = extent_size_get(extent) >> LG_PAGE;
670  		npurged += npages;
671  		extent_list_remove(decay_extents, extent);
672  		switch (extents_state_get(extents)) {
673  		case extent_state_active:
674  			not_reached();
675  		case extent_state_dirty:
676  			if (!all && muzzy_decay_ms != 0 &&
677  			    !extent_purge_lazy_wrapper(tsdn, arena,
678  			    r_extent_hooks, extent, 0,
679  			    extent_size_get(extent))) {
680  				extents_dalloc(tsdn, arena, r_extent_hooks,
681  				    &arena->extents_muzzy, extent);
682  				arena_background_thread_inactivity_check(tsdn,
683  				    arena, is_background_thread);
684  				break;
685  			}
686  		case extent_state_muzzy:
687  			extent_dalloc_wrapper(tsdn, arena, r_extent_hooks,
688  			    extent);
689  			if (config_stats) {
690  				nunmapped += npages;
691  			}
692  			break;
693  		case extent_state_retained:
694  		default:
695  			not_reached();
696  		}
697  	}
698  	if (config_stats) {
699  		arena_stats_lock(tsdn, &arena->stats);
700  		arena_stats_add_u64(tsdn, &arena->stats, &decay->stats->npurge,
701  		    1);
702  		arena_stats_add_u64(tsdn, &arena->stats,
703  		    &decay->stats->nmadvise, nmadvise);
704  		arena_stats_add_u64(tsdn, &arena->stats, &decay->stats->purged,
705  		    npurged);
706  		arena_stats_sub_zu(tsdn, &arena->stats, &arena->stats.mapped,
707  		    nunmapped << LG_PAGE);
708  		arena_stats_unlock(tsdn, &arena->stats);
709  	}
710  	return npurged;
711  }
712  static void
713  arena_decay_to_limit(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
714      extents_t *extents, bool all, size_t npages_limit, size_t npages_decay_max,
715      bool is_background_thread) {
716  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
717  	    WITNESS_RANK_CORE, 1);
718  	malloc_mutex_assert_owner(tsdn, &decay->mtx);
719  	if (decay->purging) {
720  		return;
721  	}
722  	decay->purging = true;
723  	malloc_mutex_unlock(tsdn, &decay->mtx);
724  	extent_hooks_t *extent_hooks = extent_hooks_get(arena);
725  	extent_list_t decay_extents;
726  	extent_list_init(&decay_extents);
727  	size_t npurge = arena_stash_decayed(tsdn, arena, &extent_hooks, extents,
728  	    npages_limit, npages_decay_max, &decay_extents);
729  	if (npurge != 0) {
730  		size_t npurged = arena_decay_stashed(tsdn, arena,
731  		    &extent_hooks, decay, extents, all, &decay_extents,
732  		    is_background_thread);
733  		assert(npurged == npurge);
734  	}
735  	malloc_mutex_lock(tsdn, &decay->mtx);
736  	decay->purging = false;
737  }
738  static bool
739  arena_decay_impl(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
740      extents_t *extents, bool is_background_thread, bool all) {
741  	if (all) {
742  		malloc_mutex_lock(tsdn, &decay->mtx);
743  		arena_decay_to_limit(tsdn, arena, decay, extents, all, 0,
744  		    extents_npages_get(extents), is_background_thread);
745  		malloc_mutex_unlock(tsdn, &decay->mtx);
746  		return false;
747  	}
748  	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
749  		return true;
750  	}
751  	bool epoch_advanced = arena_maybe_decay(tsdn, arena, decay, extents,
752  	    is_background_thread);
753  	size_t npages_new;
754  	if (epoch_advanced) {
755  		npages_new = decay->backlog[SMOOTHSTEP_NSTEPS-1];
756  	}
757  	malloc_mutex_unlock(tsdn, &decay->mtx);
758  	if (have_background_thread && background_thread_enabled() &&
759  	    epoch_advanced && !is_background_thread) {
760  		background_thread_interval_check(tsdn, arena, decay,
761  		    npages_new);
762  	}
763  	return false;
764  }
765  static bool
766  arena_decay_dirty(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
767      bool all) {
768  	return arena_decay_impl(tsdn, arena, &arena->decay_dirty,
769  	    &arena->extents_dirty, is_background_thread, all);
770  }
771  static bool
772  arena_decay_muzzy(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
773      bool all) {
774  	return arena_decay_impl(tsdn, arena, &arena->decay_muzzy,
775  	    &arena->extents_muzzy, is_background_thread, all);
776  }
777  void
778  arena_decay(tsdn_t *tsdn, arena_t *arena, bool is_background_thread, bool all) {
779  	if (arena_decay_dirty(tsdn, arena, is_background_thread, all)) {
780  		return;
781  	}
782  	arena_decay_muzzy(tsdn, arena, is_background_thread, all);
783  }
784  static void
785  arena_slab_dalloc(tsdn_t *tsdn, arena_t *arena, extent_t *slab) {
786  	arena_nactive_sub(arena, extent_size_get(slab) >> LG_PAGE);
787  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
788  	arena_extents_dirty_dalloc(tsdn, arena, &extent_hooks, slab);
789  }
790  static void
791  arena_bin_slabs_nonfull_insert(bin_t *bin, extent_t *slab) {
792  	assert(extent_nfree_get(slab) > 0);
793  	extent_heap_insert(&bin->slabs_nonfull, slab);
794  	if (config_stats) {
795  		bin->stats.nonfull_slabs++;
796  	}
797  }
798  static void
799  arena_bin_slabs_nonfull_remove(bin_t *bin, extent_t *slab) {
800  	extent_heap_remove(&bin->slabs_nonfull, slab);
801  	if (config_stats) {
802  		bin->stats.nonfull_slabs--;
803  	}
804  }
805  static extent_t *
806  arena_bin_slabs_nonfull_tryget(bin_t *bin) {
807  	extent_t *slab = extent_heap_remove_first(&bin->slabs_nonfull);
808  	if (slab == NULL) {
809  		return NULL;
810  	}
811  	if (config_stats) {
812  		bin->stats.reslabs++;
813  		bin->stats.nonfull_slabs--;
814  	}
815  	return slab;
816  }
817  static void
818  arena_bin_slabs_full_insert(arena_t *arena, bin_t *bin, extent_t *slab) {
819  	assert(extent_nfree_get(slab) == 0);
820  	if (arena_is_auto(arena)) {
821  		return;
822  	}
823  	extent_list_append(&bin->slabs_full, slab);
824  }
825  static void
826  arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, extent_t *slab) {
827  	if (arena_is_auto(arena)) {
828  		return;
829  	}
830  	extent_list_remove(&bin->slabs_full, slab);
831  }
832  static void
833  arena_bin_reset(tsd_t *tsd, arena_t *arena, bin_t *bin) {
834  	extent_t *slab;
835  	malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
836  	if (bin->slabcur != NULL) {
837  		slab = bin->slabcur;
838  		bin->slabcur = NULL;
839  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
840  		arena_slab_dalloc(tsd_tsdn(tsd), arena, slab);
841  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
842  	}
843  	while ((slab = extent_heap_remove_first(&bin->slabs_nonfull)) != NULL) {
844  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
845  		arena_slab_dalloc(tsd_tsdn(tsd), arena, slab);
846  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
847  	}
848  	for (slab = extent_list_first(&bin->slabs_full); slab != NULL;
849  	     slab = extent_list_first(&bin->slabs_full)) {
850  		arena_bin_slabs_full_remove(arena, bin, slab);
851  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
852  		arena_slab_dalloc(tsd_tsdn(tsd), arena, slab);
853  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
854  	}
855  	if (config_stats) {
856  		bin->stats.curregs = 0;
857  		bin->stats.curslabs = 0;
858  	}
859  	malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
860  }
861  void
862  arena_reset(tsd_t *tsd, arena_t *arena) {
863  	malloc_mutex_lock(tsd_tsdn(tsd), &arena->large_mtx);
864  	for (extent_t *extent = extent_list_first(&arena->large); extent !=
865  	    NULL; extent = extent_list_first(&arena->large)) {
866  		void *ptr = extent_base_get(extent);
867  		size_t usize;
868  		malloc_mutex_unlock(tsd_tsdn(tsd), &arena->large_mtx);
869  		alloc_ctx_t alloc_ctx;
870  		rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
871  		rtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,
872  		    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);
873  		assert(alloc_ctx.szind != SC_NSIZES);
874  		if (config_stats || (config_prof && opt_prof)) {
875  			usize = sz_index2size(alloc_ctx.szind);
876  			assert(usize == isalloc(tsd_tsdn(tsd), ptr));
877  		}
878  		if (config_prof && opt_prof) {
879  			prof_free(tsd, ptr, usize, &alloc_ctx);
880  		}
881  		large_dalloc(tsd_tsdn(tsd), extent);
882  		malloc_mutex_lock(tsd_tsdn(tsd), &arena->large_mtx);
883  	}
884  	malloc_mutex_unlock(tsd_tsdn(tsd), &arena->large_mtx);
885  	for (unsigned i = 0; i < SC_NBINS; i++) {
886  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
887  			arena_bin_reset(tsd, arena,
888  			    &arena->bins[i].bin_shards[j]);
889  		}
890  	}
891  	atomic_store_zu(&arena->nactive, 0, ATOMIC_RELAXED);
892  }
893  static void
894  arena_destroy_retained(tsdn_t *tsdn, arena_t *arena) {
895  	extent_hooks_t *extent_hooks = extent_hooks_get(arena);
896  	extent_t *extent;
897  	while ((extent = extents_evict(tsdn, arena, &extent_hooks,
898  	    &arena->extents_retained, 0)) != NULL) {
899  		extent_destroy_wrapper(tsdn, arena, &extent_hooks, extent);
900  	}
901  }
902  void
903  arena_destroy(tsd_t *tsd, arena_t *arena) {
904  	assert(base_ind_get(arena->base) >= narenas_auto);
905  	assert(arena_nthreads_get(arena, false) == 0);
906  	assert(arena_nthreads_get(arena, true) == 0);
907  	assert(extents_npages_get(&arena->extents_dirty) == 0);
908  	assert(extents_npages_get(&arena->extents_muzzy) == 0);
909  	arena_destroy_retained(tsd_tsdn(tsd), arena);
910  	arena_set(base_ind_get(arena->base), NULL);
911  	base_delete(tsd_tsdn(tsd), arena->base);
912  }
913  static extent_t *
914  arena_slab_alloc_hard(tsdn_t *tsdn, arena_t *arena,
915      extent_hooks_t **r_extent_hooks, const bin_info_t *bin_info,
916      szind_t szind) {
917  	extent_t *slab;
918  	bool zero, commit;
919  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
920  	    WITNESS_RANK_CORE, 0);
921  	zero = false;
922  	commit = true;
923  	slab = extent_alloc_wrapper(tsdn, arena, r_extent_hooks, NULL,
924  	    bin_info->slab_size, 0, PAGE, true, szind, &zero, &commit);
925  	if (config_stats && slab != NULL) {
926  		arena_stats_mapped_add(tsdn, &arena->stats,
927  		    bin_info->slab_size);
928  	}
929  	return slab;
930  }
931  static extent_t *
932  arena_slab_alloc(tsdn_t *tsdn, arena_t *arena, szind_t binind, unsigned binshard,
933      const bin_info_t *bin_info) {
934  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
935  	    WITNESS_RANK_CORE, 0);
936  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
937  	szind_t szind = sz_size2index(bin_info->reg_size);
938  	bool zero = false;
939  	bool commit = true;
940  	extent_t *slab = extents_alloc(tsdn, arena, &extent_hooks,
941  	    &arena->extents_dirty, NULL, bin_info->slab_size, 0, PAGE, true,
942  	    binind, &zero, &commit);
943  	if (slab == NULL && arena_may_have_muzzy(arena)) {
944  		slab = extents_alloc(tsdn, arena, &extent_hooks,
945  		    &arena->extents_muzzy, NULL, bin_info->slab_size, 0, PAGE,
946  		    true, binind, &zero, &commit);
947  	}
948  	if (slab == NULL) {
949  		slab = arena_slab_alloc_hard(tsdn, arena, &extent_hooks,
950  		    bin_info, szind);
951  		if (slab == NULL) {
952  			return NULL;
953  		}
954  	}
955  	assert(extent_slab_get(slab));
956  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
957  	extent_nfree_binshard_set(slab, bin_info->nregs, binshard);
958  	bitmap_init(slab_data->bitmap, &bin_info->bitmap_info, false);
959  	arena_nactive_add(arena, extent_size_get(slab) >> LG_PAGE);
960  	return slab;
961  }
962  static extent_t *
963  arena_bin_nonfull_slab_get(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
964      szind_t binind, unsigned binshard) {
965  	extent_t *slab;
966  	const bin_info_t *bin_info;
967  	slab = arena_bin_slabs_nonfull_tryget(bin);
968  	if (slab != NULL) {
969  		return slab;
970  	}
971  	bin_info = &bin_infos[binind];
972  	malloc_mutex_unlock(tsdn, &bin->lock);
973  	slab = arena_slab_alloc(tsdn, arena, binind, binshard, bin_info);
974  	malloc_mutex_lock(tsdn, &bin->lock);
975  	if (slab != NULL) {
976  		if (config_stats) {
977  			bin->stats.nslabs++;
978  			bin->stats.curslabs++;
979  		}
980  		return slab;
981  	}
982  	slab = arena_bin_slabs_nonfull_tryget(bin);
983  	if (slab != NULL) {
984  		return slab;
985  	}
986  	return NULL;
987  }
988  static void *
989  arena_bin_malloc_hard(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
990      szind_t binind, unsigned binshard) {
991  	const bin_info_t *bin_info;
992  	extent_t *slab;
993  	bin_info = &bin_infos[binind];
994  	if (!arena_is_auto(arena) && bin->slabcur != NULL) {
995  		arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
996  		bin->slabcur = NULL;
997  	}
998  	slab = arena_bin_nonfull_slab_get(tsdn, arena, bin, binind, binshard);
999  	if (bin->slabcur != NULL) {
1000  		if (extent_nfree_get(bin->slabcur) > 0) {
1001  			void *ret = arena_slab_reg_alloc(bin->slabcur,
1002  			    bin_info);
1003  			if (slab != NULL) {
1004  				if (extent_nfree_get(slab) == bin_info->nregs) {
1005  					arena_dalloc_bin_slab(tsdn, arena, slab,
1006  					    bin);
1007  				} else {
1008  					arena_bin_lower_slab(tsdn, arena, slab,
1009  					    bin);
1010  				}
1011  			}
1012  			return ret;
1013  		}
1014  		arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
1015  		bin->slabcur = NULL;
1016  	}
1017  	if (slab == NULL) {
1018  		return NULL;
1019  	}
1020  	bin->slabcur = slab;
1021  	assert(extent_nfree_get(bin->slabcur) > 0);
1022  	return arena_slab_reg_alloc(slab, bin_info);
1023  }
1024  bin_t *
1025  arena_bin_choose_lock(tsdn_t *tsdn, arena_t *arena, szind_t binind,
1026      unsigned *binshard) {
1027  	bin_t *bin;
1028  	if (tsdn_null(tsdn) || tsd_arena_get(tsdn_tsd(tsdn)) == NULL) {
1029  		*binshard = 0;
1030  	} else {
1031  		*binshard = tsd_binshardsp_get(tsdn_tsd(tsdn))->binshard[binind];
1032  	}
1033  	assert(*binshard < bin_infos[binind].n_shards);
1034  	bin = &arena->bins[binind].bin_shards[*binshard];
1035  	malloc_mutex_lock(tsdn, &bin->lock);
1036  	return bin;
1037  }
1038  void
1039  arena_tcache_fill_small(tsdn_t *tsdn, arena_t *arena, tcache_t *tcache,
1040      cache_bin_t *tbin, szind_t binind, uint64_t prof_accumbytes) {
1041  	unsigned i, nfill, cnt;
1042  	assert(tbin->ncached == 0);
1043  	if (config_prof && arena_prof_accum(tsdn, arena, prof_accumbytes)) {
1044  		prof_idump(tsdn);
1045  	}
1046  	unsigned binshard;
1047  	bin_t *bin = arena_bin_choose_lock(tsdn, arena, binind, &binshard);
1048  	for (i = 0, nfill = (tcache_bin_info[binind].ncached_max >>
1049  	    tcache->lg_fill_div[binind]); i < nfill; i += cnt) {
1050  		extent_t *slab;
1051  		if ((slab = bin->slabcur) != NULL && extent_nfree_get(slab) >
1052  		    0) {
1053  			unsigned tofill = nfill - i;
1054  			cnt = tofill < extent_nfree_get(slab) ?
1055  				tofill : extent_nfree_get(slab);
1056  			arena_slab_reg_alloc_batch(
1057  			   slab, &bin_infos[binind], cnt,
1058  			   tbin->avail - nfill + i);
1059  		} else {
1060  			cnt = 1;
1061  			void *ptr = arena_bin_malloc_hard(tsdn, arena, bin,
1062  			    binind, binshard);
1063  			if (ptr == NULL) {
1064  				if (i > 0) {
1065  					memmove(tbin->avail - i,
1066  						tbin->avail - nfill,
1067  						i * sizeof(void *));
1068  				}
1069  				break;
1070  			}
1071  			*(tbin->avail - nfill + i) = ptr;
1072  		}
1073  		if (config_fill && unlikely(opt_junk_alloc)) {
1074  			for (unsigned j = 0; j < cnt; j++) {
1075  				void* ptr = *(tbin->avail - nfill + i + j);
1076  				arena_alloc_junk_small(ptr, &bin_infos[binind],
1077  							true);
1078  			}
1079  		}
1080  	}
1081  	if (config_stats) {
1082  		bin->stats.nmalloc += i;
1083  		bin->stats.nrequests += tbin->tstats.nrequests;
1084  		bin->stats.curregs += i;
1085  		bin->stats.nfills++;
1086  		tbin->tstats.nrequests = 0;
1087  	}
1088  	malloc_mutex_unlock(tsdn, &bin->lock);
1089  	tbin->ncached = i;
1090  	arena_decay_tick(tsdn, arena);
1091  }
1092  void
1093  arena_alloc_junk_small(void *ptr, const bin_info_t *bin_info, bool zero) {
1094  	if (!zero) {
1095  		memset(ptr, JEMALLOC_ALLOC_JUNK, bin_info->reg_size);
1096  	}
1097  }
1098  static void
1099  arena_dalloc_junk_small_impl(void *ptr, const bin_info_t *bin_info) {
1100  	memset(ptr, JEMALLOC_FREE_JUNK, bin_info->reg_size);
1101  }
1102  arena_dalloc_junk_small_t *JET_MUTABLE arena_dalloc_junk_small =
1103      arena_dalloc_junk_small_impl;
1104  static void *
1105  arena_malloc_small(tsdn_t *tsdn, arena_t *arena, szind_t binind, bool zero) {
1106  	void *ret;
1107  	bin_t *bin;
1108  	size_t usize;
1109  	extent_t *slab;
1110  	assert(binind < SC_NBINS);
1111  	usize = sz_index2size(binind);
1112  	unsigned binshard;
1113  	bin = arena_bin_choose_lock(tsdn, arena, binind, &binshard);
1114  	if ((slab = bin->slabcur) != NULL && extent_nfree_get(slab) > 0) {
1115  		ret = arena_slab_reg_alloc(slab, &bin_infos[binind]);
1116  	} else {
1117  		ret = arena_bin_malloc_hard(tsdn, arena, bin, binind, binshard);
1118  	}
1119  	if (ret == NULL) {
1120  		malloc_mutex_unlock(tsdn, &bin->lock);
1121  		return NULL;
1122  	}
1123  	if (config_stats) {
1124  		bin->stats.nmalloc++;
1125  		bin->stats.nrequests++;
1126  		bin->stats.curregs++;
1127  	}
1128  	malloc_mutex_unlock(tsdn, &bin->lock);
1129  	if (config_prof && arena_prof_accum(tsdn, arena, usize)) {
1130  		prof_idump(tsdn);
1131  	}
1132  	if (!zero) {
1133  		if (config_fill) {
1134  			if (unlikely(opt_junk_alloc)) {
1135  				arena_alloc_junk_small(ret,
1136  				    &bin_infos[binind], false);
1137  			} else if (unlikely(opt_zero)) {
1138  				memset(ret, 0, usize);
1139  			}
1140  		}
1141  	} else {
1142  		if (config_fill && unlikely(opt_junk_alloc)) {
1143  			arena_alloc_junk_small(ret, &bin_infos[binind],
1144  			    true);
1145  		}
1146  		memset(ret, 0, usize);
1147  	}
1148  	arena_decay_tick(tsdn, arena);
1149  	return ret;
1150  }
1151  void *
1152  arena_malloc_hard(tsdn_t *tsdn, arena_t *arena, size_t size, szind_t ind,
1153      bool zero) {
1154  	assert(!tsdn_null(tsdn) || arena != NULL);
1155  	if (likely(!tsdn_null(tsdn))) {
1156  		arena = arena_choose_maybe_huge(tsdn_tsd(tsdn), arena, size);
1157  	}
1158  	if (unlikely(arena == NULL)) {
1159  		return NULL;
1160  	}
1161  	if (likely(size <= SC_SMALL_MAXCLASS)) {
1162  		return arena_malloc_small(tsdn, arena, ind, zero);
1163  	}
1164  	return large_malloc(tsdn, arena, sz_index2size(ind), zero);
1165  }
1166  void *
1167  arena_palloc(tsdn_t *tsdn, arena_t *arena, size_t usize, size_t alignment,
1168      bool zero, tcache_t *tcache) {
1169  	void *ret;
1170  	if (usize <= SC_SMALL_MAXCLASS
1171  	    && (alignment < PAGE
1172  	    || (alignment == PAGE && (usize & PAGE_MASK) == 0))) {
1173  		ret = arena_malloc(tsdn, arena, usize, sz_size2index(usize),
1174  		    zero, tcache, true);
1175  	} else {
1176  		if (likely(alignment <= CACHELINE)) {
1177  			ret = large_malloc(tsdn, arena, usize, zero);
1178  		} else {
1179  			ret = large_palloc(tsdn, arena, usize, alignment, zero);
1180  		}
1181  	}
1182  	return ret;
1183  }
1184  void
1185  arena_prof_promote(tsdn_t *tsdn, void *ptr, size_t usize) {
1186  	cassert(config_prof);
1187  	assert(ptr != NULL);
1188  	assert(isalloc(tsdn, ptr) == SC_LARGE_MINCLASS);
1189  	assert(usize <= SC_SMALL_MAXCLASS);
1190  	if (config_opt_safety_checks) {
1191  		safety_check_set_redzone(ptr, usize, SC_LARGE_MINCLASS);
1192  	}
1193  	rtree_ctx_t rtree_ctx_fallback;
1194  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1195  	extent_t *extent = rtree_extent_read(tsdn, &extents_rtree, rtree_ctx,
1196  	    (uintptr_t)ptr, true);
1197  	arena_t *arena = extent_arena_get(extent);
1198  	szind_t szind = sz_size2index(usize);
1199  	extent_szind_set(extent, szind);
1200  	rtree_szind_slab_update(tsdn, &extents_rtree, rtree_ctx, (uintptr_t)ptr,
1201  	    szind, false);
1202  	prof_accum_cancel(tsdn, &arena->prof_accum, usize);
1203  	assert(isalloc(tsdn, ptr) == usize);
1204  }
1205  static size_t
1206  arena_prof_demote(tsdn_t *tsdn, extent_t *extent, const void *ptr) {
1207  	cassert(config_prof);
1208  	assert(ptr != NULL);
1209  	extent_szind_set(extent, SC_NBINS);
1210  	rtree_ctx_t rtree_ctx_fallback;
1211  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1212  	rtree_szind_slab_update(tsdn, &extents_rtree, rtree_ctx, (uintptr_t)ptr,
1213  	    SC_NBINS, false);
1214  	assert(isalloc(tsdn, ptr) == SC_LARGE_MINCLASS);
1215  	return SC_LARGE_MINCLASS;
1216  }
1217  void
1218  arena_dalloc_promoted(tsdn_t *tsdn, void *ptr, tcache_t *tcache,
1219      bool slow_path) {
1220  	cassert(config_prof);
1221  	assert(opt_prof);
1222  	extent_t *extent = iealloc(tsdn, ptr);
1223  	size_t usize = extent_usize_get(extent);
1224  	size_t bumped_usize = arena_prof_demote(tsdn, extent, ptr);
1225  	if (config_opt_safety_checks && usize < SC_LARGE_MINCLASS) {
1226  		assert(bumped_usize == SC_LARGE_MINCLASS);
1227  		safety_check_verify_redzone(ptr, usize, bumped_usize);
1228  	}
1229  	if (bumped_usize <= tcache_maxclass && tcache != NULL) {
1230  		tcache_dalloc_large(tsdn_tsd(tsdn), tcache, ptr,
1231  		    sz_size2index(bumped_usize), slow_path);
1232  	} else {
1233  		large_dalloc(tsdn, extent);
1234  	}
1235  }
1236  static void
1237  arena_dissociate_bin_slab(arena_t *arena, extent_t *slab, bin_t *bin) {
1238  	if (slab == bin->slabcur) {
1239  		bin->slabcur = NULL;
1240  	} else {
1241  		szind_t binind = extent_szind_get(slab);
1242  		const bin_info_t *bin_info = &bin_infos[binind];
1243  		if (bin_info->nregs == 1) {
1244  			arena_bin_slabs_full_remove(arena, bin, slab);
1245  		} else {
1246  			arena_bin_slabs_nonfull_remove(bin, slab);
1247  		}
1248  	}
1249  }
1250  static void
1251  arena_dalloc_bin_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
1252      bin_t *bin) {
1253  	assert(slab != bin->slabcur);
1254  	malloc_mutex_unlock(tsdn, &bin->lock);
1255  	arena_slab_dalloc(tsdn, arena, slab);
1256  	malloc_mutex_lock(tsdn, &bin->lock);
1257  	if (config_stats) {
1258  		bin->stats.curslabs--;
1259  	}
1260  }
1261  static void
1262  arena_bin_lower_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
1263      bin_t *bin) {
1264  	assert(extent_nfree_get(slab) > 0);
1265  	if (bin->slabcur != NULL && extent_snad_comp(bin->slabcur, slab) > 0) {
1266  		if (extent_nfree_get(bin->slabcur) > 0) {
1267  			arena_bin_slabs_nonfull_insert(bin, bin->slabcur);
1268  		} else {
1269  			arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
1270  		}
1271  		bin->slabcur = slab;
1272  		if (config_stats) {
1273  			bin->stats.reslabs++;
1274  		}
1275  	} else {
1276  		arena_bin_slabs_nonfull_insert(bin, slab);
1277  	}
1278  }
1279  static void
1280  arena_dalloc_bin_locked_impl(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
1281      szind_t binind, extent_t *slab, void *ptr, bool junked) {
1282  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
1283  	const bin_info_t *bin_info = &bin_infos[binind];
1284  	if (!junked && config_fill && unlikely(opt_junk_free)) {
1285  		arena_dalloc_junk_small(ptr, bin_info);
1286  	}
1287  	arena_slab_reg_dalloc(slab, slab_data, ptr);
1288  	unsigned nfree = extent_nfree_get(slab);
1289  	if (nfree == bin_info->nregs) {
1290  		arena_dissociate_bin_slab(arena, slab, bin);
1291  		arena_dalloc_bin_slab(tsdn, arena, slab, bin);
1292  	} else if (nfree == 1 && slab != bin->slabcur) {
1293  		arena_bin_slabs_full_remove(arena, bin, slab);
1294  		arena_bin_lower_slab(tsdn, arena, slab, bin);
1295  	}
1296  	if (config_stats) {
1297  		bin->stats.ndalloc++;
1298  		bin->stats.curregs--;
1299  	}
1300  }
1301  void
1302  arena_dalloc_bin_junked_locked(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
1303      szind_t binind, extent_t *extent, void *ptr) {
1304  	arena_dalloc_bin_locked_impl(tsdn, arena, bin, binind, extent, ptr,
1305  	    true);
1306  }
1307  static void
1308  arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, extent_t *extent, void *ptr) {
1309  	szind_t binind = extent_szind_get(extent);
1310  	unsigned binshard = extent_binshard_get(extent);
1311  	bin_t *bin = &arena->bins[binind].bin_shards[binshard];
1312  	malloc_mutex_lock(tsdn, &bin->lock);
1313  	arena_dalloc_bin_locked_impl(tsdn, arena, bin, binind, extent, ptr,
1314  	    false);
1315  	malloc_mutex_unlock(tsdn, &bin->lock);
1316  }
1317  void
1318  arena_dalloc_small(tsdn_t *tsdn, void *ptr) {
1319  	extent_t *extent = iealloc(tsdn, ptr);
1320  	arena_t *arena = extent_arena_get(extent);
1321  	arena_dalloc_bin(tsdn, arena, extent, ptr);
1322  	arena_decay_tick(tsdn, arena);
1323  }
1324  bool
1325  arena_ralloc_no_move(tsdn_t *tsdn, void *ptr, size_t oldsize, size_t size,
1326      size_t extra, bool zero, size_t *newsize) {
1327  	bool ret;
1328  	assert(extra == 0 || size + extra <= SC_LARGE_MAXCLASS);
1329  	extent_t *extent = iealloc(tsdn, ptr);
1330  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
1331  		ret = true;
1332  		goto done;
1333  	}
1334  	size_t usize_min = sz_s2u(size);
1335  	size_t usize_max = sz_s2u(size + extra);
1336  	if (likely(oldsize <= SC_SMALL_MAXCLASS && usize_min
1337  	    <= SC_SMALL_MAXCLASS)) {
1338  		assert(bin_infos[sz_size2index(oldsize)].reg_size ==
1339  		    oldsize);
1340  		if ((usize_max > SC_SMALL_MAXCLASS
1341  		    || sz_size2index(usize_max) != sz_size2index(oldsize))
1342  		    && (size > oldsize || usize_max < oldsize)) {
1343  			ret = true;
1344  			goto done;
1345  		}
1346  		arena_decay_tick(tsdn, extent_arena_get(extent));
1347  		ret = false;
1348  	} else if (oldsize >= SC_LARGE_MINCLASS
1349  	    && usize_max >= SC_LARGE_MINCLASS) {
1350  		ret = large_ralloc_no_move(tsdn, extent, usize_min, usize_max,
1351  		    zero);
1352  	} else {
1353  		ret = true;
1354  	}
1355  done:
1356  	assert(extent == iealloc(tsdn, ptr));
1357  	*newsize = extent_usize_get(extent);
1358  	return ret;
1359  }
1360  static void *
1361  arena_ralloc_move_helper(tsdn_t *tsdn, arena_t *arena, size_t usize,
1362      size_t alignment, bool zero, tcache_t *tcache) {
1363  	if (alignment == 0) {
1364  		return arena_malloc(tsdn, arena, usize, sz_size2index(usize),
1365  		    zero, tcache, true);
1366  	}
1367  	usize = sz_sa2u(usize, alignment);
1368  	if (unlikely(usize == 0 || usize > SC_LARGE_MAXCLASS)) {
1369  		return NULL;
1370  	}
1371  	return ipalloct(tsdn, usize, alignment, zero, tcache, arena);
1372  }
1373  void *
1374  arena_ralloc(tsdn_t *tsdn, arena_t *arena, void *ptr, size_t oldsize,
1375      size_t size, size_t alignment, bool zero, tcache_t *tcache,
1376      hook_ralloc_args_t *hook_args) {
1377  	size_t usize = sz_s2u(size);
1378  	if (unlikely(usize == 0 || size > SC_LARGE_MAXCLASS)) {
1379  		return NULL;
1380  	}
1381  	if (likely(usize <= SC_SMALL_MAXCLASS)) {
1382  		UNUSED size_t newsize;
1383  		if (!arena_ralloc_no_move(tsdn, ptr, oldsize, usize, 0, zero,
1384  		    &newsize)) {
1385  			hook_invoke_expand(hook_args->is_realloc
1386  			    ? hook_expand_realloc : hook_expand_rallocx,
1387  			    ptr, oldsize, usize, (uintptr_t)ptr,
1388  			    hook_args->args);
1389  			return ptr;
1390  		}
1391  	}
1392  	if (oldsize >= SC_LARGE_MINCLASS
1393  	    && usize >= SC_LARGE_MINCLASS) {
1394  		return large_ralloc(tsdn, arena, ptr, usize,
1395  		    alignment, zero, tcache, hook_args);
1396  	}
1397  	void *ret = arena_ralloc_move_helper(tsdn, arena, usize, alignment,
1398  	    zero, tcache);
1399  	if (ret == NULL) {
1400  		return NULL;
1401  	}
1402  	hook_invoke_alloc(hook_args->is_realloc
1403  	    ? hook_alloc_realloc : hook_alloc_rallocx, ret, (uintptr_t)ret,
1404  	    hook_args->args);
1405  	hook_invoke_dalloc(hook_args->is_realloc
1406  	    ? hook_dalloc_realloc : hook_dalloc_rallocx, ptr, hook_args->args);
1407  	size_t copysize = (usize < oldsize) ? usize : oldsize;
1408  	memcpy(ret, ptr, copysize);
1409  	isdalloct(tsdn, ptr, oldsize, tcache, NULL, true);
1410  	return ret;
1411  }
1412  dss_prec_t
1413  arena_dss_prec_get(arena_t *arena) {
1414  	return (dss_prec_t)atomic_load_u(&arena->dss_prec, ATOMIC_ACQUIRE);
1415  }
1416  bool
1417  arena_dss_prec_set(arena_t *arena, dss_prec_t dss_prec) {
1418  	if (!have_dss) {
1419  		return (dss_prec != dss_prec_disabled);
1420  	}
1421  	atomic_store_u(&arena->dss_prec, (unsigned)dss_prec, ATOMIC_RELEASE);
1422  	return false;
1423  }
1424  ssize_t
1425  arena_dirty_decay_ms_default_get(void) {
1426  	return atomic_load_zd(&dirty_decay_ms_default, ATOMIC_RELAXED);
1427  }
1428  bool
1429  arena_dirty_decay_ms_default_set(ssize_t decay_ms) {
1430  	if (!arena_decay_ms_valid(decay_ms)) {
1431  		return true;
1432  	}
1433  	atomic_store_zd(&dirty_decay_ms_default, decay_ms, ATOMIC_RELAXED);
1434  	return false;
1435  }
1436  ssize_t
1437  arena_muzzy_decay_ms_default_get(void) {
1438  	return atomic_load_zd(&muzzy_decay_ms_default, ATOMIC_RELAXED);
1439  }
1440  bool
1441  arena_muzzy_decay_ms_default_set(ssize_t decay_ms) {
1442  	if (!arena_decay_ms_valid(decay_ms)) {
1443  		return true;
1444  	}
1445  	atomic_store_zd(&muzzy_decay_ms_default, decay_ms, ATOMIC_RELAXED);
1446  	return false;
1447  }
1448  bool
1449  arena_retain_grow_limit_get_set(tsd_t *tsd, arena_t *arena, size_t *old_limit,
1450      size_t *new_limit) {
1451  	assert(opt_retain);
1452  	pszind_t new_ind JEMALLOC_CC_SILENCE_INIT(0);
1453  	if (new_limit != NULL) {
1454  		size_t limit = *new_limit;
1455  		if ((new_ind = sz_psz2ind(limit + 1) - 1) >= SC_NPSIZES) {
1456  			return true;
1457  		}
1458  	}
1459  	malloc_mutex_lock(tsd_tsdn(tsd), &arena->extent_grow_mtx);
1460  	if (old_limit != NULL) {
1461  		*old_limit = sz_pind2sz(arena->retain_grow_limit);
1462  	}
1463  	if (new_limit != NULL) {
1464  		arena->retain_grow_limit = new_ind;
1465  	}
1466  	malloc_mutex_unlock(tsd_tsdn(tsd), &arena->extent_grow_mtx);
1467  	return false;
1468  }
1469  unsigned
1470  arena_nthreads_get(arena_t *arena, bool internal) {
1471  	return atomic_load_u(&arena->nthreads[internal], ATOMIC_RELAXED);
1472  }
1473  void
1474  arena_nthreads_inc(arena_t *arena, bool internal) {
1475  	atomic_fetch_add_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
1476  }
1477  void
1478  arena_nthreads_dec(arena_t *arena, bool internal) {
1479  	atomic_fetch_sub_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
1480  }
1481  size_t
1482  arena_extent_sn_next(arena_t *arena) {
1483  	return atomic_fetch_add_zu(&arena->extent_sn_next, 1, ATOMIC_RELAXED);
1484  }
1485  arena_t *
1486  arena_new(tsdn_t *tsdn, unsigned ind, extent_hooks_t *extent_hooks) {
1487  	arena_t *arena;
1488  	base_t *base;
1489  	unsigned i;
1490  	if (ind == 0) {
1491  		base = b0get();
1492  	} else {
1493  		base = base_new(tsdn, ind, extent_hooks);
1494  		if (base == NULL) {
1495  			return NULL;
1496  		}
1497  	}
1498  	unsigned nbins_total = 0;
1499  	for (i = 0; i < SC_NBINS; i++) {
1500  		nbins_total += bin_infos[i].n_shards;
1501  	}
1502  	size_t arena_size = sizeof(arena_t) + sizeof(bin_t) * nbins_total;
1503  	arena = (arena_t *)base_alloc(tsdn, base, arena_size, CACHELINE);
1504  	if (arena == NULL) {
1505  		goto label_error;
1506  	}
1507  	atomic_store_u(&arena->nthreads[0], 0, ATOMIC_RELAXED);
1508  	atomic_store_u(&arena->nthreads[1], 0, ATOMIC_RELAXED);
1509  	arena->last_thd = NULL;
1510  	if (config_stats) {
1511  		if (arena_stats_init(tsdn, &arena->stats)) {
1512  			goto label_error;
1513  		}
1514  		ql_new(&arena->tcache_ql);
1515  		ql_new(&arena->cache_bin_array_descriptor_ql);
1516  		if (malloc_mutex_init(&arena->tcache_ql_mtx, "tcache_ql",
1517  		    WITNESS_RANK_TCACHE_QL, malloc_mutex_rank_exclusive)) {
1518  			goto label_error;
1519  		}
1520  	}
1521  	if (config_prof) {
1522  		if (prof_accum_init(tsdn, &arena->prof_accum)) {
1523  			goto label_error;
1524  		}
1525  	}
1526  	if (config_cache_oblivious) {
1527  		atomic_store_zu(&arena->offset_state, config_debug ? ind :
1528  		    (size_t)(uintptr_t)arena, ATOMIC_RELAXED);
1529  	}
1530  	atomic_store_zu(&arena->extent_sn_next, 0, ATOMIC_RELAXED);
1531  	atomic_store_u(&arena->dss_prec, (unsigned)extent_dss_prec_get(),
1532  	    ATOMIC_RELAXED);
1533  	atomic_store_zu(&arena->nactive, 0, ATOMIC_RELAXED);
1534  	extent_list_init(&arena->large);
1535  	if (malloc_mutex_init(&arena->large_mtx, "arena_large",
1536  	    WITNESS_RANK_ARENA_LARGE, malloc_mutex_rank_exclusive)) {
1537  		goto label_error;
1538  	}
1539  	if (extents_init(tsdn, &arena->extents_dirty, extent_state_dirty,
1540  	    true)) {
1541  		goto label_error;
1542  	}
1543  	if (extents_init(tsdn, &arena->extents_muzzy, extent_state_muzzy,
1544  	    false)) {
1545  		goto label_error;
1546  	}
1547  	if (extents_init(tsdn, &arena->extents_retained, extent_state_retained,
1548  	    false)) {
1549  		goto label_error;
1550  	}
1551  	if (arena_decay_init(&arena->decay_dirty,
1552  	    arena_dirty_decay_ms_default_get(), &arena->stats.decay_dirty)) {
1553  		goto label_error;
1554  	}
1555  	if (arena_decay_init(&arena->decay_muzzy,
1556  	    arena_muzzy_decay_ms_default_get(), &arena->stats.decay_muzzy)) {
1557  		goto label_error;
1558  	}
1559  	arena->extent_grow_next = sz_psz2ind(HUGEPAGE);
1560  	arena->retain_grow_limit = sz_psz2ind(SC_LARGE_MAXCLASS);
1561  	if (malloc_mutex_init(&arena->extent_grow_mtx, "extent_grow",
1562  	    WITNESS_RANK_EXTENT_GROW, malloc_mutex_rank_exclusive)) {
1563  		goto label_error;
1564  	}
1565  	extent_avail_new(&arena->extent_avail);
1566  	if (malloc_mutex_init(&arena->extent_avail_mtx, "extent_avail",
1567  	    WITNESS_RANK_EXTENT_AVAIL, malloc_mutex_rank_exclusive)) {
1568  		goto label_error;
1569  	}
1570  	uintptr_t bin_addr = (uintptr_t)arena + sizeof(arena_t);
1571  	atomic_store_u(&arena->binshard_next, 0, ATOMIC_RELEASE);
1572  	for (i = 0; i < SC_NBINS; i++) {
1573  		unsigned nshards = bin_infos[i].n_shards;
1574  		arena->bins[i].bin_shards = (bin_t *)bin_addr;
1575  		bin_addr += nshards * sizeof(bin_t);
1576  		for (unsigned j = 0; j < nshards; j++) {
1577  			bool err = bin_init(&arena->bins[i].bin_shards[j]);
1578  			if (err) {
1579  				goto label_error;
1580  			}
1581  		}
1582  	}
1583  	assert(bin_addr == (uintptr_t)arena + arena_size);
1584  	arena->base = base;
1585  	arena_set(ind, arena);
1586  	nstime_init(&arena->create_time, 0);
1587  	nstime_update(&arena->create_time);
1588  	if (ind != 0) {
1589  		assert(!tsdn_null(tsdn));
1590  		pre_reentrancy(tsdn_tsd(tsdn), arena);
1591  		if (test_hooks_arena_new_hook) {
1592  			test_hooks_arena_new_hook();
1593  		}
1594  		post_reentrancy(tsdn_tsd(tsdn));
1595  	}
1596  	return arena;
1597  label_error:
1598  	if (ind != 0) {
1599  		base_delete(tsdn, base);
1600  	}
1601  	return NULL;
1602  }
1603  arena_t *
1604  arena_choose_huge(tsd_t *tsd) {
1605  	if (huge_arena_ind == 0) {
1606  		assert(!malloc_initialized());
1607  	}
1608  	arena_t *huge_arena = arena_get(tsd_tsdn(tsd), huge_arena_ind, false);
1609  	if (huge_arena == NULL) {
1610  		assert(huge_arena_ind != 0);
1611  		huge_arena = arena_get(tsd_tsdn(tsd), huge_arena_ind, true);
1612  		if (huge_arena == NULL) {
1613  			return NULL;
1614  		}
1615  		if (arena_dirty_decay_ms_default_get() > 0) {
1616  			arena_dirty_decay_ms_set(tsd_tsdn(tsd), huge_arena, 0);
1617  		}
1618  		if (arena_muzzy_decay_ms_default_get() > 0) {
1619  			arena_muzzy_decay_ms_set(tsd_tsdn(tsd), huge_arena, 0);
1620  		}
1621  	}
1622  	return huge_arena;
1623  }
1624  bool
1625  arena_init_huge(void) {
1626  	bool huge_enabled;
1627  	if (opt_oversize_threshold > SC_LARGE_MAXCLASS ||
1628  	    opt_oversize_threshold < SC_LARGE_MINCLASS) {
1629  		opt_oversize_threshold = 0;
1630  		oversize_threshold = SC_LARGE_MAXCLASS + PAGE;
1631  		huge_enabled = false;
1632  	} else {
1633  		huge_arena_ind = narenas_total_get();
1634  		oversize_threshold = opt_oversize_threshold;
1635  		huge_enabled = true;
1636  	}
1637  	return huge_enabled;
1638  }
1639  bool
1640  arena_is_huge(unsigned arena_ind) {
1641  	if (huge_arena_ind == 0) {
1642  		return false;
1643  	}
1644  	return (arena_ind == huge_arena_ind);
1645  }
1646  void
1647  arena_boot(sc_data_t *sc_data) {
1648  	arena_dirty_decay_ms_default_set(opt_dirty_decay_ms);
1649  	arena_muzzy_decay_ms_default_set(opt_muzzy_decay_ms);
1650  	for (unsigned i = 0; i < SC_NBINS; i++) {
1651  		sc_t *sc = &sc_data->sc[i];
1652  		div_init(&arena_binind_div_info[i],
1653  		    (1U << sc->lg_base) + (sc->ndelta << sc->lg_delta));
1654  	}
1655  }
1656  void
1657  arena_prefork0(tsdn_t *tsdn, arena_t *arena) {
1658  	malloc_mutex_prefork(tsdn, &arena->decay_dirty.mtx);
1659  	malloc_mutex_prefork(tsdn, &arena->decay_muzzy.mtx);
1660  }
1661  void
1662  arena_prefork1(tsdn_t *tsdn, arena_t *arena) {
1663  	if (config_stats) {
1664  		malloc_mutex_prefork(tsdn, &arena->tcache_ql_mtx);
1665  	}
1666  }
1667  void
1668  arena_prefork2(tsdn_t *tsdn, arena_t *arena) {
1669  	malloc_mutex_prefork(tsdn, &arena->extent_grow_mtx);
1670  }
1671  void
1672  arena_prefork3(tsdn_t *tsdn, arena_t *arena) {
1673  	extents_prefork(tsdn, &arena->extents_dirty);
1674  	extents_prefork(tsdn, &arena->extents_muzzy);
1675  	extents_prefork(tsdn, &arena->extents_retained);
1676  }
1677  void
1678  arena_prefork4(tsdn_t *tsdn, arena_t *arena) {
1679  	malloc_mutex_prefork(tsdn, &arena->extent_avail_mtx);
1680  }
1681  void
1682  arena_prefork5(tsdn_t *tsdn, arena_t *arena) {
1683  	base_prefork(tsdn, arena->base);
1684  }
1685  void
1686  arena_prefork6(tsdn_t *tsdn, arena_t *arena) {
1687  	malloc_mutex_prefork(tsdn, &arena->large_mtx);
1688  }
1689  void
1690  arena_prefork7(tsdn_t *tsdn, arena_t *arena) {
1691  	for (unsigned i = 0; i < SC_NBINS; i++) {
1692  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
1693  			bin_prefork(tsdn, &arena->bins[i].bin_shards[j]);
1694  		}
1695  	}
1696  }
1697  void
1698  arena_postfork_parent(tsdn_t *tsdn, arena_t *arena) {
1699  	unsigned i;
1700  	for (i = 0; i < SC_NBINS; i++) {
1701  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
1702  			bin_postfork_parent(tsdn,
1703  			    &arena->bins[i].bin_shards[j]);
1704  		}
1705  	}
1706  	malloc_mutex_postfork_parent(tsdn, &arena->large_mtx);
1707  	base_postfork_parent(tsdn, arena->base);
1708  	malloc_mutex_postfork_parent(tsdn, &arena->extent_avail_mtx);
1709  	extents_postfork_parent(tsdn, &arena->extents_dirty);
1710  	extents_postfork_parent(tsdn, &arena->extents_muzzy);
1711  	extents_postfork_parent(tsdn, &arena->extents_retained);
1712  	malloc_mutex_postfork_parent(tsdn, &arena->extent_grow_mtx);
1713  	malloc_mutex_postfork_parent(tsdn, &arena->decay_dirty.mtx);
1714  	malloc_mutex_postfork_parent(tsdn, &arena->decay_muzzy.mtx);
1715  	if (config_stats) {
1716  		malloc_mutex_postfork_parent(tsdn, &arena->tcache_ql_mtx);
1717  	}
1718  }
1719  void
1720  arena_postfork_child(tsdn_t *tsdn, arena_t *arena) {
1721  	unsigned i;
1722  	atomic_store_u(&arena->nthreads[0], 0, ATOMIC_RELAXED);
1723  	atomic_store_u(&arena->nthreads[1], 0, ATOMIC_RELAXED);
1724  	if (tsd_arena_get(tsdn_tsd(tsdn)) == arena) {
1725  		arena_nthreads_inc(arena, false);
1726  	}
1727  	if (tsd_iarena_get(tsdn_tsd(tsdn)) == arena) {
1728  		arena_nthreads_inc(arena, true);
1729  	}
1730  	if (config_stats) {
1731  		ql_new(&arena->tcache_ql);
1732  		ql_new(&arena->cache_bin_array_descriptor_ql);
1733  		tcache_t *tcache = tcache_get(tsdn_tsd(tsdn));
1734  		if (tcache != NULL && tcache->arena == arena) {
1735  			ql_elm_new(tcache, link);
1736  			ql_tail_insert(&arena->tcache_ql, tcache, link);
1737  			cache_bin_array_descriptor_init(
1738  			    &tcache->cache_bin_array_descriptor,
1739  			    tcache->bins_small, tcache->bins_large);
1740  			ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
1741  			    &tcache->cache_bin_array_descriptor, link);
1742  		}
1743  	}
1744  	for (i = 0; i < SC_NBINS; i++) {
1745  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
1746  			bin_postfork_child(tsdn, &arena->bins[i].bin_shards[j]);
1747  		}
1748  	}
1749  	malloc_mutex_postfork_child(tsdn, &arena->large_mtx);
1750  	base_postfork_child(tsdn, arena->base);
1751  	malloc_mutex_postfork_child(tsdn, &arena->extent_avail_mtx);
1752  	extents_postfork_child(tsdn, &arena->extents_dirty);
1753  	extents_postfork_child(tsdn, &arena->extents_muzzy);
1754  	extents_postfork_child(tsdn, &arena->extents_retained);
1755  	malloc_mutex_postfork_child(tsdn, &arena->extent_grow_mtx);
1756  	malloc_mutex_postfork_child(tsdn, &arena->decay_dirty.mtx);
1757  	malloc_mutex_postfork_child(tsdn, &arena->decay_muzzy.mtx);
1758  	if (config_stats) {
1759  		malloc_mutex_postfork_child(tsdn, &arena->tcache_ql_mtx);
1760  	}
1761  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-lossless.h</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-arena.c</div>
                </div>
                <div class="column column_space"><pre><code>26  typedef void (*VP8LTransformColorInverseFunc)(const VP8LMultipliers* const m,
27                                                const uint32_t* src,
28                                                int num_pixels, uint32_t* dst);
29  extern VP8LTransformColorInverseFunc VP8LTransformColorInverse;
30  struct VP8LTransform;  
31  void VP8LInverseTransform(const struct VP8LTransform* const transform,
32                            int row_start, int row_end,
33                            const uint32_t* const in, uint32_t* const out);
34  typedef void (*VP8LConvertFunc)(const uint32_t* src, int num_pixels,
35                                  uint8_t* dst);
36  extern VP8LConvertFunc VP8LConvertBGRAToRGB;
37  extern VP8LConvertFunc VP8LConvertBGRAToRGBA;
38  extern VP8LConvertFunc VP8LConvertBGRAToRGBA4444;
39  extern VP8LConvertFunc VP8LConvertBGRAToRGB565;
40  extern VP8LConvertFunc VP8LConvertBGRAToBGR;
41  void VP8LConvertFromBGRA(const uint32_t* const in_data, int num_pixels,
42                           WEBP_CSP_MODE out_colorspace, uint8_t* const rgba);
</pre></code></div>
                <div class="column column_space"><pre><code>34  static unsigned huge_arena_ind;
35  static void arena_decay_to_limit(tsdn_t *tsdn, arena_t *arena,
36      arena_decay_t *decay, extents_t *extents, bool all, size_t npages_limit,
37      size_t npages_decay_max, bool is_background_thread);
38  static bool arena_decay_dirty(tsdn_t *tsdn, arena_t *arena,
39      bool is_background_thread, bool all);
40  static void arena_dalloc_bin_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
41      bin_t *bin);
42  static void arena_bin_lower_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
43      bin_t *bin);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    