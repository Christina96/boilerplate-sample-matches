
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 34, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-enc_neon.c</h3>
            <pre><code>1  #include "src/dsp/dsp.h"
2  #if defined(WEBP_USE_NEON)
3  #include <assert.h>
4  #include "src/dsp/neon.h"
5  #include "src/enc/vp8i_enc.h"
6  static const int16_t kC1 = 20091;
7  static const int16_t kC2 = 17734;  
8  #if defined(WEBP_USE_INTRINSICS)
9  static WEBP_INLINE int16x8_t ConvertU8ToS16_NEON(uint32x2_t v) {
10    return vreinterpretq_s16_u16(vmovl_u8(vreinterpret_u8_u32(v)));
11  }
12  static WEBP_INLINE void SaturateAndStore4x4_NEON(uint8_t* const dst,
13                                                   const int16x8_t dst01,
14                                                   const int16x8_t dst23) {
15    const uint8x8_t dst01_u8 = vqmovun_s16(dst01);
16    const uint8x8_t dst23_u8 = vqmovun_s16(dst23);
17    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), vreinterpret_u32_u8(dst01_u8), 0);
18    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), vreinterpret_u32_u8(dst01_u8), 1);
19    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), vreinterpret_u32_u8(dst23_u8), 0);
20    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), vreinterpret_u32_u8(dst23_u8), 1);
21  }
22  static WEBP_INLINE void Add4x4_NEON(const int16x8_t row01,
23                                      const int16x8_t row23,
24                                      const uint8_t* const ref,
25                                      uint8_t* const dst) {
26    uint32x2_t dst01 = vdup_n_u32(0);
27    uint32x2_t dst23 = vdup_n_u32(0);
28    dst01 = vld1_lane_u32((uint32_t*)(ref + 0 * BPS), dst01, 0);
29    dst23 = vld1_lane_u32((uint32_t*)(ref + 2 * BPS), dst23, 0);
30    dst01 = vld1_lane_u32((uint32_t*)(ref + 1 * BPS), dst01, 1);
31    dst23 = vld1_lane_u32((uint32_t*)(ref + 3 * BPS), dst23, 1);
32    {
33      const int16x8_t dst01_s16 = ConvertU8ToS16_NEON(dst01);
34      const int16x8_t dst23_s16 = ConvertU8ToS16_NEON(dst23);
35      const int16x8_t out01 = vrsraq_n_s16(dst01_s16, row01, 3);
36      const int16x8_t out23 = vrsraq_n_s16(dst23_s16, row23, 3);
37      SaturateAndStore4x4_NEON(dst, out01, out23);
38    }
39  }
40  static WEBP_INLINE void Transpose8x2_NEON(const int16x8_t in0,
41                                            const int16x8_t in1,
42                                            int16x8x2_t* const out) {
43    const int16x8x2_t tmp0 = vzipq_s16(in0, in1);   
44    *out = vzipq_s16(tmp0.val[0], tmp0.val[1]);
45  }
46  static WEBP_INLINE void TransformPass_NEON(int16x8x2_t* const rows) {
47    const int16x8_t B1 =
48        vcombine_s16(vget_high_s16(rows->val[0]), vget_high_s16(rows->val[1]));
49    const int16x8_t C0 = vsraq_n_s16(B1, vqdmulhq_n_s16(B1, kC1), 1);
50    const int16x8_t C1 = vqdmulhq_n_s16(B1, kC2);
51    const int16x4_t a = vqadd_s16(vget_low_s16(rows->val[0]),
52                                  vget_low_s16(rows->val[1]));   
53    const int16x4_t b = vqsub_s16(vget_low_s16(rows->val[0]),
54                                  vget_low_s16(rows->val[1]));   
55    const int16x4_t c = vqsub_s16(vget_low_s16(C1), vget_high_s16(C0));
56    const int16x4_t d = vqadd_s16(vget_low_s16(C0), vget_high_s16(C1));
57    const int16x8_t D0 = vcombine_s16(a, b);      
58    const int16x8_t D1 = vcombine_s16(d, c);      
59    const int16x8_t E0 = vqaddq_s16(D0, D1);      
60    const int16x8_t E_tmp = vqsubq_s16(D0, D1);   
61    const int16x8_t E1 = vcombine_s16(vget_high_s16(E_tmp), vget_low_s16(E_tmp));
62    Transpose8x2_NEON(E0, E1, rows);
63  }
64  static void ITransformOne_NEON(const uint8_t* ref,
65                                 const int16_t* in, uint8_t* dst) {
<span onclick='openModal()' class='match'>66    int16x8x2_t rows;
67    INIT_VECTOR2(rows, vld1q_s16(in + 0), vld1q_s16(in + 8));
68    TransformPass_NEON(&rows);
69    TransformPass_NEON(&rows);
70    Add4x4_NEON(rows.val[0], rows.val[1], ref, dst);
</span>71  }
72  #else
73  static void ITransformOne_NEON(const uint8_t* ref,
74                                 const int16_t* in, uint8_t* dst) {
75    const int kBPS = BPS;
76    const int16_t kC1C2[] = { kC1, kC2, 0, 0 };
77    __asm__ volatile (
78      "vld1.16         {q1, q2}, [%[in]]           \n"
79      "vld1.16         {d0}, [%[kC1C2]]            \n"
80      "vswp            d3, d4                      \n"
81      "vqdmulh.s16     q8, q2, d0[0]               \n"
82      "vqdmulh.s16     q9, q2, d0[1]               \n"
83      "vqadd.s16       d22, d2, d3                 \n"
84      "vqsub.s16       d23, d2, d3                 \n"
85      "vshr.s16        q8, q8, #1                  \n"
86      "vqadd.s16       q8, q2, q8                  \n"
87      "vqsub.s16       d20, d18, d17               \n"
88      "vqadd.s16       d21, d19, d16               \n"
89      "vqadd.s16       d2, d22, d21                \n"
90      "vqadd.s16       d3, d23, d20                \n"
91      "vqsub.s16       d4, d23, d20                \n"
92      "vqsub.s16       d5, d22, d21                \n"
93      "vzip.16         q1, q2                      \n"
94      "vzip.16         q1, q2                      \n"
95      "vswp            d3, d4                      \n"
96      "vqdmulh.s16     q8, q2, d0[0]               \n"
97      "vqdmulh.s16     q9, q2, d0[1]               \n"
98      "vqadd.s16       d22, d2, d3                 \n"
99      "vqsub.s16       d23, d2, d3                 \n"
100      "vshr.s16        q8, q8, #1                  \n"
101      "vqadd.s16       q8, q2, q8                  \n"
102      "vqsub.s16       d20, d18, d17               \n"
103      "vqadd.s16       d21, d19, d16               \n"
104      "vqadd.s16       d2, d22, d21                \n"
105      "vqadd.s16       d3, d23, d20                \n"
106      "vqsub.s16       d4, d23, d20                \n"
107      "vqsub.s16       d5, d22, d21                \n"
108      "vld1.32         d6[0], [%[ref]], %[kBPS]    \n"
109      "vld1.32         d6[1], [%[ref]], %[kBPS]    \n"
110      "vld1.32         d7[0], [%[ref]], %[kBPS]    \n"
111      "vld1.32         d7[1], [%[ref]], %[kBPS]    \n"
112      "sub         %[ref], %[ref], %[kBPS], lsl #2 \n"
113      "vrshr.s16       d2, d2, #3                  \n"
114      "vrshr.s16       d3, d3, #3                  \n"
115      "vrshr.s16       d4, d4, #3                  \n"
116      "vrshr.s16       d5, d5, #3                  \n"
117      "vzip.16         q1, q2                      \n"
118      "vzip.16         q1, q2                      \n"
119      "vmovl.u8        q8, d6                      \n"
120      "vmovl.u8        q9, d7                      \n"
121      "vqadd.s16       q1, q1, q8                  \n"
122      "vqadd.s16       q2, q2, q9                  \n"
123      "vqmovun.s16     d0, q1                      \n"
124      "vqmovun.s16     d1, q2                      \n"
125      "vst1.32         d0[0], [%[dst]], %[kBPS]    \n"
126      "vst1.32         d0[1], [%[dst]], %[kBPS]    \n"
127      "vst1.32         d1[0], [%[dst]], %[kBPS]    \n"
128      "vst1.32         d1[1], [%[dst]]             \n"
129      : [in] "+r"(in), [dst] "+r"(dst)               
130      : [kBPS] "r"(kBPS), [kC1C2] "r"(kC1C2), [ref] "r"(ref)  
131      : "memory", "q0", "q1", "q2", "q8", "q9", "q10", "q11"  
132    );
133  }
134  #endif    
135  static void ITransform_NEON(const uint8_t* ref,
136                              const int16_t* in, uint8_t* dst, int do_two) {
137    ITransformOne_NEON(ref, in, dst);
138    if (do_two) {
139      ITransformOne_NEON(ref + 4, in + 16, dst + 4);
140    }
141  }
142  static uint8x16_t Load4x4_NEON(const uint8_t* src) {
143    uint32x4_t out = vdupq_n_u32(0);
144    out = vld1q_lane_u32((const uint32_t*)(src + 0 * BPS), out, 0);
145    out = vld1q_lane_u32((const uint32_t*)(src + 1 * BPS), out, 1);
146    out = vld1q_lane_u32((const uint32_t*)(src + 2 * BPS), out, 2);
147    out = vld1q_lane_u32((const uint32_t*)(src + 3 * BPS), out, 3);
148    return vreinterpretq_u8_u32(out);
149  }
150  #if defined(WEBP_USE_INTRINSICS)
151  static WEBP_INLINE void Transpose4x4_S16_NEON(const int16x4_t A,
152                                                const int16x4_t B,
153                                                const int16x4_t C,
154                                                const int16x4_t D,
155                                                int16x8_t* const out01,
156                                                int16x8_t* const out32) {
157    const int16x4x2_t AB = vtrn_s16(A, B);
158    const int16x4x2_t CD = vtrn_s16(C, D);
159    const int32x2x2_t tmp02 = vtrn_s32(vreinterpret_s32_s16(AB.val[0]),
160                                       vreinterpret_s32_s16(CD.val[0]));
161    const int32x2x2_t tmp13 = vtrn_s32(vreinterpret_s32_s16(AB.val[1]),
162                                       vreinterpret_s32_s16(CD.val[1]));
163    *out01 = vreinterpretq_s16_s64(
164        vcombine_s64(vreinterpret_s64_s32(tmp02.val[0]),
165                     vreinterpret_s64_s32(tmp13.val[0])));
166    *out32 = vreinterpretq_s16_s64(
167        vcombine_s64(vreinterpret_s64_s32(tmp13.val[1]),
168                     vreinterpret_s64_s32(tmp02.val[1])));
169  }
170  static WEBP_INLINE int16x8_t DiffU8ToS16_NEON(const uint8x8_t a,
171                                                const uint8x8_t b) {
172    return vreinterpretq_s16_u16(vsubl_u8(a, b));
173  }
174  static void FTransform_NEON(const uint8_t* src, const uint8_t* ref,
175                              int16_t* out) {
176    int16x8_t d0d1, d3d2;   
177    {
178      const uint8x16_t S0 = Load4x4_NEON(src);
179      const uint8x16_t R0 = Load4x4_NEON(ref);
180      const int16x8_t D0D1 = DiffU8ToS16_NEON(vget_low_u8(S0), vget_low_u8(R0));
181      const int16x8_t D2D3 = DiffU8ToS16_NEON(vget_high_u8(S0), vget_high_u8(R0));
182      const int16x4_t D0 = vget_low_s16(D0D1);
183      const int16x4_t D1 = vget_high_s16(D0D1);
184      const int16x4_t D2 = vget_low_s16(D2D3);
185      const int16x4_t D3 = vget_high_s16(D2D3);
186      Transpose4x4_S16_NEON(D0, D1, D2, D3, &d0d1, &d3d2);
187    }
188    {    
189      const int32x4_t kCst937 = vdupq_n_s32(937);
190      const int32x4_t kCst1812 = vdupq_n_s32(1812);
191      const int16x8_t a0a1 = vaddq_s16(d0d1, d3d2);   
192      const int16x8_t a3a2 = vsubq_s16(d0d1, d3d2);   
193      const int16x8_t a0a1_2 = vshlq_n_s16(a0a1, 3);
194      const int16x4_t tmp0 = vadd_s16(vget_low_s16(a0a1_2),
195                                      vget_high_s16(a0a1_2));
196      const int16x4_t tmp2 = vsub_s16(vget_low_s16(a0a1_2),
197                                      vget_high_s16(a0a1_2));
198      const int32x4_t a3_2217 = vmull_n_s16(vget_low_s16(a3a2), 2217);
199      const int32x4_t a2_2217 = vmull_n_s16(vget_high_s16(a3a2), 2217);
200      const int32x4_t a2_p_a3 = vmlal_n_s16(a2_2217, vget_low_s16(a3a2), 5352);
201      const int32x4_t a3_m_a2 = vmlsl_n_s16(a3_2217, vget_high_s16(a3a2), 5352);
202      const int16x4_t tmp1 = vshrn_n_s32(vaddq_s32(a2_p_a3, kCst1812), 9);
203      const int16x4_t tmp3 = vshrn_n_s32(vaddq_s32(a3_m_a2, kCst937), 9);
204      Transpose4x4_S16_NEON(tmp0, tmp1, tmp2, tmp3, &d0d1, &d3d2);
205    }
206    {    
207      const int32x4_t kCst12000 = vdupq_n_s32(12000 + (1 << 16));
208      const int32x4_t kCst51000 = vdupq_n_s32(51000);
209      const int16x8_t a0a1 = vaddq_s16(d0d1, d3d2);   
210      const int16x8_t a3a2 = vsubq_s16(d0d1, d3d2);   
211      const int16x4_t a0_k7 = vadd_s16(vget_low_s16(a0a1), vdup_n_s16(7));
212      const int16x4_t out0 = vshr_n_s16(vadd_s16(a0_k7, vget_high_s16(a0a1)), 4);
213      const int16x4_t out2 = vshr_n_s16(vsub_s16(a0_k7, vget_high_s16(a0a1)), 4);
214      const int32x4_t a3_2217 = vmull_n_s16(vget_low_s16(a3a2), 2217);
215      const int32x4_t a2_2217 = vmull_n_s16(vget_high_s16(a3a2), 2217);
216      const int32x4_t a2_p_a3 = vmlal_n_s16(a2_2217, vget_low_s16(a3a2), 5352);
217      const int32x4_t a3_m_a2 = vmlsl_n_s16(a3_2217, vget_high_s16(a3a2), 5352);
218      const int16x4_t tmp1 = vaddhn_s32(a2_p_a3, kCst12000);
219      const int16x4_t out3 = vaddhn_s32(a3_m_a2, kCst51000);
220      const int16x4_t a3_eq_0 =
221          vreinterpret_s16_u16(vceq_s16(vget_low_s16(a3a2), vdup_n_s16(0)));
222      const int16x4_t out1 = vadd_s16(tmp1, a3_eq_0);
223      vst1_s16(out +  0, out0);
224      vst1_s16(out +  4, out1);
225      vst1_s16(out +  8, out2);
226      vst1_s16(out + 12, out3);
227    }
228  }
229  #else
230  static const int16_t kCoeff16[] = {
231    5352,  5352,  5352, 5352, 2217,  2217,  2217, 2217
232  };
233  static const int32_t kCoeff32[] = {
234     1812,  1812,  1812,  1812,
235      937,   937,   937,   937,
236    12000, 12000, 12000, 12000,
237    51000, 51000, 51000, 51000
238  };
239  static void FTransform_NEON(const uint8_t* src, const uint8_t* ref,
240                              int16_t* out) {
241    const int kBPS = BPS;
242    const uint8_t* src_ptr = src;
243    const uint8_t* ref_ptr = ref;
244    const int16_t* coeff16 = kCoeff16;
245    const int32_t* coeff32 = kCoeff32;
246    __asm__ volatile (
247      "vld1.8 {d8},  [%[src_ptr]], %[kBPS]      \n"
248      "vld1.8 {d10}, [%[src_ptr]], %[kBPS]      \n"
249      "vld1.8 {d9},  [%[src_ptr]], %[kBPS]      \n"
250      "vld1.8 {d11}, [%[src_ptr]]               \n"
251      "vld1.8 {d12}, [%[ref_ptr]], %[kBPS]      \n"
252      "vld1.8 {d14}, [%[ref_ptr]], %[kBPS]      \n"
253      "vld1.8 {d13}, [%[ref_ptr]], %[kBPS]      \n"
254      "vld1.8 {d15}, [%[ref_ptr]]               \n"
255      "vtrn.32     q4, q5                       \n"
256      "vtrn.32     q6, q7                       \n"
257      "vsubl.u8    q0, d8, d12                  \n"
258      "vsubl.u8    q1, d9, d13                  \n"
259      "vld1.16     {q8}, [%[coeff16]]           \n"
260      "vld1.32     {q9, q10}, [%[coeff32]]!     \n"
261      "vld1.32     {q11,q12}, [%[coeff32]]      \n"
262      "vtrn.32         d0, d2                   \n"
263      "vtrn.32         d1, d3                   \n"
264      "vtrn.16         d0, d1                   \n"
265      "vtrn.16         d2, d3                   \n"
266      "vadd.s16        d4, d0, d3               \n" 
267      "vadd.s16        d5, d1, d2               \n" 
268      "vsub.s16        d6, d1, d2               \n" 
269      "vsub.s16        d7, d0, d3               \n" 
270      "vadd.s16        d0, d4, d5               \n" 
271      "vshl.s16        d0, d0, #3               \n" 
272      "vsub.s16        d2, d4, d5               \n" 
273      "vshl.s16        d2, d2, #3               \n" 
274      "vmlal.s16       q9, d7, d16              \n" 
275      "vmlal.s16       q10, d7, d17             \n" 
276      "vmlal.s16       q9, d6, d17              \n" 
277      "vmlsl.s16       q10, d6, d16             \n" 
278      "vshrn.s32       d1, q9, #9               \n"
279      "vshrn.s32       d3, q10, #9              \n"
280      "vtrn.32         d0, d2                   \n"
281      "vtrn.32         d1, d3                   \n"
282      "vtrn.16         d0, d1                   \n"
283      "vtrn.16         d2, d3                   \n"
284      "vmov.s16        d26, #7                  \n"
285      "vadd.s16        d4, d0, d3               \n" 
286      "vadd.s16        d5, d1, d2               \n" 
287      "vsub.s16        d6, d1, d2               \n" 
288      "vadd.s16        d4, d4, d26              \n" 
289      "vsub.s16        d7, d0, d3               \n" 
290      "vadd.s16        d0, d4, d5               \n" 
291      "vsub.s16        d2, d4, d5               \n" 
292      "vmlal.s16       q11, d7, d16             \n" 
293      "vmlal.s16       q12, d7, d17             \n" 
294      "vceq.s16        d4, d7, #0               \n"
295      "vshr.s16        d0, d0, #4               \n"
296      "vshr.s16        d2, d2, #4               \n"
297      "vmlal.s16       q11, d6, d17             \n" 
298      "vmlsl.s16       q12, d6, d16             \n" 
299      "vmvn            d4, d4                   \n" 
300      "vshrn.s32       d1, q11, #16             \n"
301      "vsub.s16        d1, d1, d4               \n"
302      "vshrn.s32       d3, q12, #16             \n"
303      "vst1.16         {q0, q1}, [%[out]]   \n"
304      : [src_ptr] "+r"(src_ptr), [ref_ptr] "+r"(ref_ptr),
305        [coeff32] "+r"(coeff32)          
306      : [kBPS] "r"(kBPS), [coeff16] "r"(coeff16),
307        [out] "r"(out)                   
308      : "memory", "q0", "q1", "q2", "q3", "q4", "q5", "q6", "q7", "q8", "q9",
309        "q10", "q11", "q12", "q13"       
310    );
311  }
312  #endif
313  #define LOAD_LANE_16b(VALUE, LANE) do {             \
314    (VALUE) = vld1_lane_s16(src, (VALUE), (LANE));    \
315    src += stride;                                    \
316  } while (0)
317  static void FTransformWHT_NEON(const int16_t* src, int16_t* out) {
318    const int stride = 16;
319    const int16x4_t zero = vdup_n_s16(0);
320    int32x4x4_t tmp0;
321    int16x4x4_t in;
322    INIT_VECTOR4(in, zero, zero, zero, zero);
323    LOAD_LANE_16b(in.val[0], 0);
324    LOAD_LANE_16b(in.val[1], 0);
325    LOAD_LANE_16b(in.val[2], 0);
326    LOAD_LANE_16b(in.val[3], 0);
327    LOAD_LANE_16b(in.val[0], 1);
328    LOAD_LANE_16b(in.val[1], 1);
329    LOAD_LANE_16b(in.val[2], 1);
330    LOAD_LANE_16b(in.val[3], 1);
331    LOAD_LANE_16b(in.val[0], 2);
332    LOAD_LANE_16b(in.val[1], 2);
333    LOAD_LANE_16b(in.val[2], 2);
334    LOAD_LANE_16b(in.val[3], 2);
335    LOAD_LANE_16b(in.val[0], 3);
336    LOAD_LANE_16b(in.val[1], 3);
337    LOAD_LANE_16b(in.val[2], 3);
338    LOAD_LANE_16b(in.val[3], 3);
339    {
340      const int32x4_t a0 = vaddl_s16(in.val[0], in.val[2]);
341      const int32x4_t a1 = vaddl_s16(in.val[1], in.val[3]);
342      const int32x4_t a2 = vsubl_s16(in.val[1], in.val[3]);
343      const int32x4_t a3 = vsubl_s16(in.val[0], in.val[2]);
344      tmp0.val[0] = vaddq_s32(a0, a1);
345      tmp0.val[1] = vaddq_s32(a3, a2);
346      tmp0.val[2] = vsubq_s32(a3, a2);
347      tmp0.val[3] = vsubq_s32(a0, a1);
348    }
349    {
350      const int32x4x4_t tmp1 = Transpose4x4_NEON(tmp0);
351      const int32x4_t a0 = vaddq_s32(tmp1.val[0], tmp1.val[2]);
352      const int32x4_t a1 = vaddq_s32(tmp1.val[1], tmp1.val[3]);
353      const int32x4_t a2 = vsubq_s32(tmp1.val[1], tmp1.val[3]);
354      const int32x4_t a3 = vsubq_s32(tmp1.val[0], tmp1.val[2]);
355      const int32x4_t b0 = vhaddq_s32(a0, a1);  
356      const int32x4_t b1 = vhaddq_s32(a3, a2);  
357      const int32x4_t b2 = vhsubq_s32(a3, a2);  
358      const int32x4_t b3 = vhsubq_s32(a0, a1);  
359      const int16x4_t out0 = vmovn_s32(b0);
360      const int16x4_t out1 = vmovn_s32(b1);
361      const int16x4_t out2 = vmovn_s32(b2);
362      const int16x4_t out3 = vmovn_s32(b3);
363      vst1_s16(out +  0, out0);
364      vst1_s16(out +  4, out1);
365      vst1_s16(out +  8, out2);
366      vst1_s16(out + 12, out3);
367    }
368  }
369  #undef LOAD_LANE_16b
370  static WEBP_INLINE int16x8x4_t DistoTranspose4x4S16_NEON(int16x8x4_t q4_in) {
371    const int16x8x2_t q2_tmp0 = vtrnq_s16(q4_in.val[0], q4_in.val[1]);
372    const int16x8x2_t q2_tmp1 = vtrnq_s16(q4_in.val[2], q4_in.val[3]);
373    const int32x4x2_t q2_tmp2 = vtrnq_s32(vreinterpretq_s32_s16(q2_tmp0.val[0]),
374                                          vreinterpretq_s32_s16(q2_tmp1.val[0]));
375    const int32x4x2_t q2_tmp3 = vtrnq_s32(vreinterpretq_s32_s16(q2_tmp0.val[1]),
376                                          vreinterpretq_s32_s16(q2_tmp1.val[1]));
377    q4_in.val[0] = vreinterpretq_s16_s32(q2_tmp2.val[0]);
378    q4_in.val[2] = vreinterpretq_s16_s32(q2_tmp2.val[1]);
379    q4_in.val[1] = vreinterpretq_s16_s32(q2_tmp3.val[0]);
380    q4_in.val[3] = vreinterpretq_s16_s32(q2_tmp3.val[1]);
381    return q4_in;
382  }
383  static WEBP_INLINE int16x8x4_t DistoHorizontalPass_NEON(
384      const int16x8x4_t q4_in) {
385    const int16x8_t q_a0 = vaddq_s16(q4_in.val[0], q4_in.val[2]);
386    const int16x8_t q_a1 = vaddq_s16(q4_in.val[1], q4_in.val[3]);
387    const int16x8_t q_a3 = vsubq_s16(q4_in.val[0], q4_in.val[2]);
388    const int16x8_t q_a2 = vsubq_s16(q4_in.val[1], q4_in.val[3]);
389    int16x8x4_t q4_out;
390    INIT_VECTOR4(q4_out,
391                 vabsq_s16(vaddq_s16(q_a0, q_a1)),
392                 vabsq_s16(vaddq_s16(q_a3, q_a2)),
393                 vabdq_s16(q_a3, q_a2), vabdq_s16(q_a0, q_a1));
394    return q4_out;
395  }
396  static WEBP_INLINE int16x8x4_t DistoVerticalPass_NEON(const uint8x8x4_t q4_in) {
397    const int16x8_t q_a0 = vreinterpretq_s16_u16(vaddl_u8(q4_in.val[0],
398                                                          q4_in.val[2]));
399    const int16x8_t q_a1 = vreinterpretq_s16_u16(vaddl_u8(q4_in.val[1],
400                                                          q4_in.val[3]));
401    const int16x8_t q_a2 = vreinterpretq_s16_u16(vsubl_u8(q4_in.val[1],
402                                                          q4_in.val[3]));
403    const int16x8_t q_a3 = vreinterpretq_s16_u16(vsubl_u8(q4_in.val[0],
404                                                          q4_in.val[2]));
405    int16x8x4_t q4_out;
406    INIT_VECTOR4(q4_out,
407                 vaddq_s16(q_a0, q_a1), vaddq_s16(q_a3, q_a2),
408                 vsubq_s16(q_a3, q_a2), vsubq_s16(q_a0, q_a1));
409    return q4_out;
410  }
411  static WEBP_INLINE int16x4x4_t DistoLoadW_NEON(const uint16_t* w) {
412    const uint16x8_t q_w07 = vld1q_u16(&w[0]);
413    const uint16x8_t q_w8f = vld1q_u16(&w[8]);
414    int16x4x4_t d4_w;
415    INIT_VECTOR4(d4_w,
416                 vget_low_s16(vreinterpretq_s16_u16(q_w07)),
417                 vget_high_s16(vreinterpretq_s16_u16(q_w07)),
418                 vget_low_s16(vreinterpretq_s16_u16(q_w8f)),
419                 vget_high_s16(vreinterpretq_s16_u16(q_w8f)));
420    return d4_w;
421  }
422  static WEBP_INLINE int32x2_t DistoSum_NEON(const int16x8x4_t q4_in,
423                                             const int16x4x4_t d4_w) {
424    int32x2_t d_sum;
425    int32x4_t q_sum0 = vmull_s16(d4_w.val[0], vget_low_s16(q4_in.val[0]));
426    int32x4_t q_sum1 = vmull_s16(d4_w.val[1], vget_low_s16(q4_in.val[1]));
427    int32x4_t q_sum2 = vmull_s16(d4_w.val[2], vget_low_s16(q4_in.val[2]));
428    int32x4_t q_sum3 = vmull_s16(d4_w.val[3], vget_low_s16(q4_in.val[3]));
429    q_sum0 = vmlsl_s16(q_sum0, d4_w.val[0], vget_high_s16(q4_in.val[0]));
430    q_sum1 = vmlsl_s16(q_sum1, d4_w.val[1], vget_high_s16(q4_in.val[1]));
431    q_sum2 = vmlsl_s16(q_sum2, d4_w.val[2], vget_high_s16(q4_in.val[2]));
432    q_sum3 = vmlsl_s16(q_sum3, d4_w.val[3], vget_high_s16(q4_in.val[3]));
433    q_sum0 = vaddq_s32(q_sum0, q_sum1);
434    q_sum2 = vaddq_s32(q_sum2, q_sum3);
435    q_sum2 = vaddq_s32(q_sum0, q_sum2);
436    d_sum = vpadd_s32(vget_low_s32(q_sum2), vget_high_s32(q_sum2));
437    d_sum = vpadd_s32(d_sum, d_sum);
438    return d_sum;
439  }
440  #define LOAD_LANE_32b(src, VALUE, LANE) \
441      (VALUE) = vld1_lane_u32((const uint32_t*)(src), (VALUE), (LANE))
442  static int Disto4x4_NEON(const uint8_t* const a, const uint8_t* const b,
443                           const uint16_t* const w) {
444    uint32x2_t d_in_ab_0123 = vdup_n_u32(0);
445    uint32x2_t d_in_ab_4567 = vdup_n_u32(0);
446    uint32x2_t d_in_ab_89ab = vdup_n_u32(0);
447    uint32x2_t d_in_ab_cdef = vdup_n_u32(0);
448    uint8x8x4_t d4_in;
449    LOAD_LANE_32b(a + 0 * BPS, d_in_ab_0123, 0);
450    LOAD_LANE_32b(a + 1 * BPS, d_in_ab_4567, 0);
451    LOAD_LANE_32b(a + 2 * BPS, d_in_ab_89ab, 0);
452    LOAD_LANE_32b(a + 3 * BPS, d_in_ab_cdef, 0);
453    LOAD_LANE_32b(b + 0 * BPS, d_in_ab_0123, 1);
454    LOAD_LANE_32b(b + 1 * BPS, d_in_ab_4567, 1);
455    LOAD_LANE_32b(b + 2 * BPS, d_in_ab_89ab, 1);
456    LOAD_LANE_32b(b + 3 * BPS, d_in_ab_cdef, 1);
457    INIT_VECTOR4(d4_in,
458                 vreinterpret_u8_u32(d_in_ab_0123),
459                 vreinterpret_u8_u32(d_in_ab_4567),
460                 vreinterpret_u8_u32(d_in_ab_89ab),
461                 vreinterpret_u8_u32(d_in_ab_cdef));
462    {
463      const int16x8x4_t q4_v = DistoVerticalPass_NEON(d4_in);
464      const int16x4x4_t d4_w = DistoLoadW_NEON(w);
465      const int16x8x4_t q4_t = DistoTranspose4x4S16_NEON(q4_v);
466      const int16x8x4_t q4_h = DistoHorizontalPass_NEON(q4_t);
467      int32x2_t d_sum = DistoSum_NEON(q4_h, d4_w);
468      d_sum = vabs_s32(d_sum);
469      d_sum = vshr_n_s32(d_sum, 5);
470      return vget_lane_s32(d_sum, 0);
471    }
472  }
473  #undef LOAD_LANE_32b
474  static int Disto16x16_NEON(const uint8_t* const a, const uint8_t* const b,
475                             const uint16_t* const w) {
476    int D = 0;
477    int x, y;
478    for (y = 0; y < 16 * BPS; y += 4 * BPS) {
479      for (x = 0; x < 16; x += 4) {
480        D += Disto4x4_NEON(a + x + y, b + x + y, w);
481      }
482    }
483    return D;
484  }
485  static void CollectHistogram_NEON(const uint8_t* ref, const uint8_t* pred,
486                                    int start_block, int end_block,
487                                    VP8Histogram* const histo) {
488    const uint16x8_t max_coeff_thresh = vdupq_n_u16(MAX_COEFF_THRESH);
489    int j;
490    int distribution[MAX_COEFF_THRESH + 1] = { 0 };
491    for (j = start_block; j < end_block; ++j) {
492      int16_t out[16];
493      FTransform_NEON(ref + VP8DspScan[j], pred + VP8DspScan[j], out);
494      {
495        int k;
496        const int16x8_t a0 = vld1q_s16(out + 0);
497        const int16x8_t b0 = vld1q_s16(out + 8);
498        const uint16x8_t a1 = vreinterpretq_u16_s16(vabsq_s16(a0));
499        const uint16x8_t b1 = vreinterpretq_u16_s16(vabsq_s16(b0));
500        const uint16x8_t a2 = vshrq_n_u16(a1, 3);
501        const uint16x8_t b2 = vshrq_n_u16(b1, 3);
502        const uint16x8_t a3 = vminq_u16(a2, max_coeff_thresh);
503        const uint16x8_t b3 = vminq_u16(b2, max_coeff_thresh);
504        vst1q_s16(out + 0, vreinterpretq_s16_u16(a3));
505        vst1q_s16(out + 8, vreinterpretq_s16_u16(b3));
506        for (k = 0; k < 16; ++k) {
507          ++distribution[out[k]];
508        }
509      }
510    }
511    VP8SetHistogramData(distribution, histo);
512  }
513  static WEBP_INLINE void AccumulateSSE16_NEON(const uint8_t* const a,
514                                               const uint8_t* const b,
515                                               uint32x4_t* const sum) {
516    const uint8x16_t a0 = vld1q_u8(a);
517    const uint8x16_t b0 = vld1q_u8(b);
518    const uint8x16_t abs_diff = vabdq_u8(a0, b0);
519    const uint16x8_t prod1 = vmull_u8(vget_low_u8(abs_diff),
520                                      vget_low_u8(abs_diff));
521    const uint16x8_t prod2 = vmull_u8(vget_high_u8(abs_diff),
522                                      vget_high_u8(abs_diff));
523    const uint32x4_t sum1 = vpaddlq_u16(prod1);
524    const uint32x4_t sum2 = vpaddlq_u16(prod2);
525    *sum = vaddq_u32(*sum, vaddq_u32(sum1, sum2));
526  }
527  static int SumToInt_NEON(uint32x4_t sum) {
528    const uint64x2_t sum2 = vpaddlq_u32(sum);
529    const uint64_t sum3 = vgetq_lane_u64(sum2, 0) + vgetq_lane_u64(sum2, 1);
530    return (int)sum3;
531  }
532  static int SSE16x16_NEON(const uint8_t* a, const uint8_t* b) {
533    uint32x4_t sum = vdupq_n_u32(0);
534    int y;
535    for (y = 0; y < 16; ++y) {
536      AccumulateSSE16_NEON(a + y * BPS, b + y * BPS, &sum);
537    }
538    return SumToInt_NEON(sum);
539  }
540  static int SSE16x8_NEON(const uint8_t* a, const uint8_t* b) {
541    uint32x4_t sum = vdupq_n_u32(0);
542    int y;
543    for (y = 0; y < 8; ++y) {
544      AccumulateSSE16_NEON(a + y * BPS, b + y * BPS, &sum);
545    }
546    return SumToInt_NEON(sum);
547  }
548  static int SSE8x8_NEON(const uint8_t* a, const uint8_t* b) {
549    uint32x4_t sum = vdupq_n_u32(0);
550    int y;
551    for (y = 0; y < 8; ++y) {
552      const uint8x8_t a0 = vld1_u8(a + y * BPS);
553      const uint8x8_t b0 = vld1_u8(b + y * BPS);
554      const uint8x8_t abs_diff = vabd_u8(a0, b0);
555      const uint16x8_t prod = vmull_u8(abs_diff, abs_diff);
556      sum = vpadalq_u16(sum, prod);
557    }
558    return SumToInt_NEON(sum);
559  }
560  static int SSE4x4_NEON(const uint8_t* a, const uint8_t* b) {
561    const uint8x16_t a0 = Load4x4_NEON(a);
562    const uint8x16_t b0 = Load4x4_NEON(b);
563    const uint8x16_t abs_diff = vabdq_u8(a0, b0);
564    const uint16x8_t prod1 = vmull_u8(vget_low_u8(abs_diff),
565                                      vget_low_u8(abs_diff));
566    const uint16x8_t prod2 = vmull_u8(vget_high_u8(abs_diff),
567                                      vget_high_u8(abs_diff));
568    const uint32x4_t sum1 = vpaddlq_u16(prod1);
569    const uint32x4_t sum2 = vpaddlq_u16(prod2);
570    return SumToInt_NEON(vaddq_u32(sum1, sum2));
571  }
572  #if !defined(WORK_AROUND_GCC)
573  static int16x8_t Quantize_NEON(int16_t* const in,
574                                 const VP8Matrix* const mtx, int offset) {
575    const uint16x8_t sharp = vld1q_u16(&mtx->sharpen_[offset]);
576    const uint16x8_t q = vld1q_u16(&mtx->q_[offset]);
577    const uint16x8_t iq = vld1q_u16(&mtx->iq_[offset]);
578    const uint32x4_t bias0 = vld1q_u32(&mtx->bias_[offset + 0]);
579    const uint32x4_t bias1 = vld1q_u32(&mtx->bias_[offset + 4]);
580    const int16x8_t a = vld1q_s16(in + offset);                
581    const uint16x8_t b = vreinterpretq_u16_s16(vabsq_s16(a));  
582    const int16x8_t sign = vshrq_n_s16(a, 15);                 
583    const uint16x8_t c = vaddq_u16(b, sharp);                  
584    const uint32x4_t m0 = vmull_u16(vget_low_u16(c), vget_low_u16(iq));
585    const uint32x4_t m1 = vmull_u16(vget_high_u16(c), vget_high_u16(iq));
586    const uint32x4_t m2 = vhaddq_u32(m0, bias0);
587    const uint32x4_t m3 = vhaddq_u32(m1, bias1);     
588    const uint16x8_t c0 = vcombine_u16(vshrn_n_u32(m2, 16),
589                                       vshrn_n_u32(m3, 16));   
590    const uint16x8_t c1 = vminq_u16(c0, vdupq_n_u16(MAX_LEVEL));
591    const int16x8_t c2 = veorq_s16(vreinterpretq_s16_u16(c1), sign);
592    const int16x8_t c3 = vsubq_s16(c2, sign);                  
593    const int16x8_t c4 = vmulq_s16(c3, vreinterpretq_s16_u16(q));
594    vst1q_s16(in + offset, c4);
595    assert(QFIX == 17);  
596    return c3;
597  }
598  static const uint8_t kShuffles[4][8] = {
599    { 0,   1,  2,  3,  8,  9, 16, 17 },
600    { 10, 11,  4,  5,  6,  7, 12, 13 },
601    { 18, 19, 24, 25, 26, 27, 20, 21 },
602    { 14, 15, 22, 23, 28, 29, 30, 31 }
603  };
604  static int QuantizeBlock_NEON(int16_t in[16], int16_t out[16],
605                                const VP8Matrix* const mtx) {
606    const int16x8_t out0 = Quantize_NEON(in, mtx, 0);
607    const int16x8_t out1 = Quantize_NEON(in, mtx, 8);
608    uint8x8x4_t shuffles;
609  #if defined(__APPLE__) && defined(__aarch64__) && \
610      defined(__apple_build_version__) && (__apple_build_version__< 6020037)
611    uint8x16x2_t all_out;
612    INIT_VECTOR2(all_out, vreinterpretq_u8_s16(out0), vreinterpretq_u8_s16(out1));
613    INIT_VECTOR4(shuffles,
614                 vtbl2q_u8(all_out, vld1_u8(kShuffles[0])),
615                 vtbl2q_u8(all_out, vld1_u8(kShuffles[1])),
616                 vtbl2q_u8(all_out, vld1_u8(kShuffles[2])),
617                 vtbl2q_u8(all_out, vld1_u8(kShuffles[3])));
618  #else
619    uint8x8x4_t all_out;
620    INIT_VECTOR4(all_out,
621                 vreinterpret_u8_s16(vget_low_s16(out0)),
622                 vreinterpret_u8_s16(vget_high_s16(out0)),
623                 vreinterpret_u8_s16(vget_low_s16(out1)),
624                 vreinterpret_u8_s16(vget_high_s16(out1)));
625    INIT_VECTOR4(shuffles,
626                 vtbl4_u8(all_out, vld1_u8(kShuffles[0])),
627                 vtbl4_u8(all_out, vld1_u8(kShuffles[1])),
628                 vtbl4_u8(all_out, vld1_u8(kShuffles[2])),
629                 vtbl4_u8(all_out, vld1_u8(kShuffles[3])));
630  #endif
631    vst1_u8((uint8_t*)(out +  0), shuffles.val[0]);
632    vst1_u8((uint8_t*)(out +  4), shuffles.val[1]);
633    vst1_u8((uint8_t*)(out +  8), shuffles.val[2]);
634    vst1_u8((uint8_t*)(out + 12), shuffles.val[3]);
635    if (*(uint64_t*)(out +  0) != 0) return 1;
636    if (*(uint64_t*)(out +  4) != 0) return 1;
637    if (*(uint64_t*)(out +  8) != 0) return 1;
638    if (*(uint64_t*)(out + 12) != 0) return 1;
639    return 0;
640  }
641  static int Quantize2Blocks_NEON(int16_t in[32], int16_t out[32],
642                                  const VP8Matrix* const mtx) {
643    int nz;
644    nz  = QuantizeBlock_NEON(in + 0 * 16, out + 0 * 16, mtx) << 0;
645    nz |= QuantizeBlock_NEON(in + 1 * 16, out + 1 * 16, mtx) << 1;
646    return nz;
647  }
648  #endif   
649  extern void VP8EncDspInitNEON(void);
650  WEBP_TSAN_IGNORE_FUNCTION void VP8EncDspInitNEON(void) {
651    VP8ITransform = ITransform_NEON;
652    VP8FTransform = FTransform_NEON;
653    VP8FTransformWHT = FTransformWHT_NEON;
654    VP8TDisto4x4 = Disto4x4_NEON;
655    VP8TDisto16x16 = Disto16x16_NEON;
656    VP8CollectHistogram = CollectHistogram_NEON;
657    VP8SSE16x16 = SSE16x16_NEON;
658    VP8SSE16x8 = SSE16x8_NEON;
659    VP8SSE8x8 = SSE8x8_NEON;
660    VP8SSE4x4 = SSE4x4_NEON;
661  #if !defined(WORK_AROUND_GCC)
662    VP8EncQuantizeBlock = QuantizeBlock_NEON;
663    VP8EncQuantize2Blocks = Quantize2Blocks_NEON;
664  #endif
665  }
666  #else  
667  WEBP_DSP_INIT_STUB(VP8EncDspInitNEON)
668  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-dec_neon.c</h3>
            <pre><code>1  #include "src/dsp/dsp.h"
2  #if defined(WEBP_USE_NEON)
3  #include "src/dsp/neon.h"
4  #include "src/dec/vp8i_dec.h"
5  #if !defined(WORK_AROUND_GCC)
6  static WEBP_INLINE uint8x8x4_t Load4x8_NEON(const uint8_t* const src,
7                                              int stride) {
8    const uint8x8_t zero = vdup_n_u8(0);
9    uint8x8x4_t out;
10    INIT_VECTOR4(out, zero, zero, zero, zero);
11    out = vld4_lane_u8(src + 0 * stride, out, 0);
12    out = vld4_lane_u8(src + 1 * stride, out, 1);
13    out = vld4_lane_u8(src + 2 * stride, out, 2);
14    out = vld4_lane_u8(src + 3 * stride, out, 3);
15    out = vld4_lane_u8(src + 4 * stride, out, 4);
16    out = vld4_lane_u8(src + 5 * stride, out, 5);
17    out = vld4_lane_u8(src + 6 * stride, out, 6);
18    out = vld4_lane_u8(src + 7 * stride, out, 7);
19    return out;
20  }
21  static WEBP_INLINE void Load4x16_NEON(const uint8_t* const src, int stride,
22                                        uint8x16_t* const p1,
23                                        uint8x16_t* const p0,
24                                        uint8x16_t* const q0,
25                                        uint8x16_t* const q1) {
26    const uint8x8x4_t row0 = Load4x8_NEON(src - 2 + 0 * stride, stride);
27    const uint8x8x4_t row8 = Load4x8_NEON(src - 2 + 8 * stride, stride);
28    *p1 = vcombine_u8(row0.val[0], row8.val[0]);
29    *p0 = vcombine_u8(row0.val[1], row8.val[1]);
30    *q0 = vcombine_u8(row0.val[2], row8.val[2]);
31    *q1 = vcombine_u8(row0.val[3], row8.val[3]);
32  }
33  #else  
34  #define LOADQ_LANE_32b(VALUE, LANE) do {                             \
35    (VALUE) = vld1q_lane_u32((const uint32_t*)src, (VALUE), (LANE));   \
36    src += stride;                                                     \
37  } while (0)
38  static WEBP_INLINE void Load4x16_NEON(const uint8_t* src, int stride,
39                                        uint8x16_t* const p1,
40                                        uint8x16_t* const p0,
41                                        uint8x16_t* const q0,
42                                        uint8x16_t* const q1) {
43    const uint32x4_t zero = vdupq_n_u32(0);
44    uint32x4x4_t in;
45    INIT_VECTOR4(in, zero, zero, zero, zero);
46    src -= 2;
47    LOADQ_LANE_32b(in.val[0], 0);
48    LOADQ_LANE_32b(in.val[1], 0);
49    LOADQ_LANE_32b(in.val[2], 0);
50    LOADQ_LANE_32b(in.val[3], 0);
51    LOADQ_LANE_32b(in.val[0], 1);
52    LOADQ_LANE_32b(in.val[1], 1);
53    LOADQ_LANE_32b(in.val[2], 1);
54    LOADQ_LANE_32b(in.val[3], 1);
55    LOADQ_LANE_32b(in.val[0], 2);
56    LOADQ_LANE_32b(in.val[1], 2);
57    LOADQ_LANE_32b(in.val[2], 2);
58    LOADQ_LANE_32b(in.val[3], 2);
59    LOADQ_LANE_32b(in.val[0], 3);
60    LOADQ_LANE_32b(in.val[1], 3);
61    LOADQ_LANE_32b(in.val[2], 3);
62    LOADQ_LANE_32b(in.val[3], 3);
63    {
64      const uint8x16x2_t row01 = vtrnq_u8(vreinterpretq_u8_u32(in.val[0]),
65                                          vreinterpretq_u8_u32(in.val[1]));
66      const uint8x16x2_t row23 = vtrnq_u8(vreinterpretq_u8_u32(in.val[2]),
67                                          vreinterpretq_u8_u32(in.val[3]));
68      const uint16x8x2_t row02 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[0]),
69                                           vreinterpretq_u16_u8(row23.val[0]));
70      const uint16x8x2_t row13 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[1]),
71                                           vreinterpretq_u16_u8(row23.val[1]));
72      *p1 = vreinterpretq_u8_u16(row02.val[0]);
73      *p0 = vreinterpretq_u8_u16(row13.val[0]);
74      *q0 = vreinterpretq_u8_u16(row02.val[1]);
75      *q1 = vreinterpretq_u8_u16(row13.val[1]);
76    }
77  }
78  #undef LOADQ_LANE_32b
79  #endif  
80  static WEBP_INLINE void Load8x16_NEON(
81      const uint8_t* const src, int stride,
82      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
83      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
84      uint8x16_t* const q2, uint8x16_t* const q3) {
85    Load4x16_NEON(src - 2, stride, p3, p2, p1, p0);
86    Load4x16_NEON(src + 2, stride, q0, q1, q2, q3);
87  }
88  static WEBP_INLINE void Load16x4_NEON(const uint8_t* const src, int stride,
89                                        uint8x16_t* const p1,
90                                        uint8x16_t* const p0,
91                                        uint8x16_t* const q0,
92                                        uint8x16_t* const q1) {
93    *p1 = vld1q_u8(src - 2 * stride);
94    *p0 = vld1q_u8(src - 1 * stride);
95    *q0 = vld1q_u8(src + 0 * stride);
96    *q1 = vld1q_u8(src + 1 * stride);
97  }
98  static WEBP_INLINE void Load16x8_NEON(
99      const uint8_t* const src, int stride,
100      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
101      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
102      uint8x16_t* const q2, uint8x16_t* const q3) {
103    Load16x4_NEON(src - 2  * stride, stride, p3, p2, p1, p0);
104    Load16x4_NEON(src + 2  * stride, stride, q0, q1, q2, q3);
105  }
106  static WEBP_INLINE void Load8x8x2_NEON(
107      const uint8_t* const u, const uint8_t* const v, int stride,
108      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
109      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
110      uint8x16_t* const q2, uint8x16_t* const q3) {
111    *p3 = vcombine_u8(vld1_u8(u - 4 * stride), vld1_u8(v - 4 * stride));
112    *p2 = vcombine_u8(vld1_u8(u - 3 * stride), vld1_u8(v - 3 * stride));
113    *p1 = vcombine_u8(vld1_u8(u - 2 * stride), vld1_u8(v - 2 * stride));
114    *p0 = vcombine_u8(vld1_u8(u - 1 * stride), vld1_u8(v - 1 * stride));
115    *q0 = vcombine_u8(vld1_u8(u + 0 * stride), vld1_u8(v + 0 * stride));
116    *q1 = vcombine_u8(vld1_u8(u + 1 * stride), vld1_u8(v + 1 * stride));
117    *q2 = vcombine_u8(vld1_u8(u + 2 * stride), vld1_u8(v + 2 * stride));
118    *q3 = vcombine_u8(vld1_u8(u + 3 * stride), vld1_u8(v + 3 * stride));
119  }
120  #if !defined(WORK_AROUND_GCC)
121  #define LOAD_UV_8(ROW) \
122    vcombine_u8(vld1_u8(u - 4 + (ROW) * stride), vld1_u8(v - 4 + (ROW) * stride))
123  static WEBP_INLINE void Load8x8x2T_NEON(
124      const uint8_t* const u, const uint8_t* const v, int stride,
125      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
126      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
127      uint8x16_t* const q2, uint8x16_t* const q3) {
128    const uint8x16_t row0 = LOAD_UV_8(0);
129    const uint8x16_t row1 = LOAD_UV_8(1);
130    const uint8x16_t row2 = LOAD_UV_8(2);
131    const uint8x16_t row3 = LOAD_UV_8(3);
132    const uint8x16_t row4 = LOAD_UV_8(4);
133    const uint8x16_t row5 = LOAD_UV_8(5);
134    const uint8x16_t row6 = LOAD_UV_8(6);
135    const uint8x16_t row7 = LOAD_UV_8(7);
136    const uint8x16x2_t row01 = vtrnq_u8(row0, row1);  
137    const uint8x16x2_t row23 = vtrnq_u8(row2, row3);  
138    const uint8x16x2_t row45 = vtrnq_u8(row4, row5);  
139    const uint8x16x2_t row67 = vtrnq_u8(row6, row7);  
140    const uint16x8x2_t row02 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[0]),
141                                         vreinterpretq_u16_u8(row23.val[0]));
142    const uint16x8x2_t row13 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[1]),
143                                         vreinterpretq_u16_u8(row23.val[1]));
144    const uint16x8x2_t row46 = vtrnq_u16(vreinterpretq_u16_u8(row45.val[0]),
145                                         vreinterpretq_u16_u8(row67.val[0]));
146    const uint16x8x2_t row57 = vtrnq_u16(vreinterpretq_u16_u8(row45.val[1]),
147                                         vreinterpretq_u16_u8(row67.val[1]));
148    const uint32x4x2_t row04 = vtrnq_u32(vreinterpretq_u32_u16(row02.val[0]),
149                                         vreinterpretq_u32_u16(row46.val[0]));
150    const uint32x4x2_t row26 = vtrnq_u32(vreinterpretq_u32_u16(row02.val[1]),
151                                         vreinterpretq_u32_u16(row46.val[1]));
152    const uint32x4x2_t row15 = vtrnq_u32(vreinterpretq_u32_u16(row13.val[0]),
153                                         vreinterpretq_u32_u16(row57.val[0]));
154    const uint32x4x2_t row37 = vtrnq_u32(vreinterpretq_u32_u16(row13.val[1]),
155                                         vreinterpretq_u32_u16(row57.val[1]));
156    *p3 = vreinterpretq_u8_u32(row04.val[0]);
157    *p2 = vreinterpretq_u8_u32(row15.val[0]);
158    *p1 = vreinterpretq_u8_u32(row26.val[0]);
159    *p0 = vreinterpretq_u8_u32(row37.val[0]);
160    *q0 = vreinterpretq_u8_u32(row04.val[1]);
161    *q1 = vreinterpretq_u8_u32(row15.val[1]);
162    *q2 = vreinterpretq_u8_u32(row26.val[1]);
163    *q3 = vreinterpretq_u8_u32(row37.val[1]);
164  }
165  #undef LOAD_UV_8
166  #endif  
167  static WEBP_INLINE void Store2x8_NEON(const uint8x8x2_t v,
168                                        uint8_t* const dst, int stride) {
169    vst2_lane_u8(dst + 0 * stride, v, 0);
170    vst2_lane_u8(dst + 1 * stride, v, 1);
171    vst2_lane_u8(dst + 2 * stride, v, 2);
172    vst2_lane_u8(dst + 3 * stride, v, 3);
173    vst2_lane_u8(dst + 4 * stride, v, 4);
174    vst2_lane_u8(dst + 5 * stride, v, 5);
175    vst2_lane_u8(dst + 6 * stride, v, 6);
176    vst2_lane_u8(dst + 7 * stride, v, 7);
177  }
178  static WEBP_INLINE void Store2x16_NEON(const uint8x16_t p0, const uint8x16_t q0,
179                                         uint8_t* const dst, int stride) {
180    uint8x8x2_t lo, hi;
181    lo.val[0] = vget_low_u8(p0);
182    lo.val[1] = vget_low_u8(q0);
183    hi.val[0] = vget_high_u8(p0);
184    hi.val[1] = vget_high_u8(q0);
185    Store2x8_NEON(lo, dst - 1 + 0 * stride, stride);
186    Store2x8_NEON(hi, dst - 1 + 8 * stride, stride);
187  }
188  #if !defined(WORK_AROUND_GCC)
189  static WEBP_INLINE void Store4x8_NEON(const uint8x8x4_t v,
190                                        uint8_t* const dst, int stride) {
191    vst4_lane_u8(dst + 0 * stride, v, 0);
192    vst4_lane_u8(dst + 1 * stride, v, 1);
193    vst4_lane_u8(dst + 2 * stride, v, 2);
194    vst4_lane_u8(dst + 3 * stride, v, 3);
195    vst4_lane_u8(dst + 4 * stride, v, 4);
196    vst4_lane_u8(dst + 5 * stride, v, 5);
197    vst4_lane_u8(dst + 6 * stride, v, 6);
198    vst4_lane_u8(dst + 7 * stride, v, 7);
199  }
200  static WEBP_INLINE void Store4x16_NEON(const uint8x16_t p1, const uint8x16_t p0,
201                                         const uint8x16_t q0, const uint8x16_t q1,
202                                         uint8_t* const dst, int stride) {
203    uint8x8x4_t lo, hi;
204    INIT_VECTOR4(lo,
205                 vget_low_u8(p1), vget_low_u8(p0),
206                 vget_low_u8(q0), vget_low_u8(q1));
207    INIT_VECTOR4(hi,
208                 vget_high_u8(p1), vget_high_u8(p0),
209                 vget_high_u8(q0), vget_high_u8(q1));
210    Store4x8_NEON(lo, dst - 2 + 0 * stride, stride);
211    Store4x8_NEON(hi, dst - 2 + 8 * stride, stride);
212  }
213  #endif  
214  static WEBP_INLINE void Store16x2_NEON(const uint8x16_t p0, const uint8x16_t q0,
215                                         uint8_t* const dst, int stride) {
216    vst1q_u8(dst - stride, p0);
217    vst1q_u8(dst, q0);
218  }
219  static WEBP_INLINE void Store16x4_NEON(const uint8x16_t p1, const uint8x16_t p0,
220                                         const uint8x16_t q0, const uint8x16_t q1,
221                                         uint8_t* const dst, int stride) {
222    Store16x2_NEON(p1, p0, dst - stride, stride);
223    Store16x2_NEON(q0, q1, dst + stride, stride);
224  }
225  static WEBP_INLINE void Store8x2x2_NEON(const uint8x16_t p0,
226                                          const uint8x16_t q0,
227                                          uint8_t* const u, uint8_t* const v,
228                                          int stride) {
229    vst1_u8(u - stride, vget_low_u8(p0));
230    vst1_u8(u,          vget_low_u8(q0));
231    vst1_u8(v - stride, vget_high_u8(p0));
232    vst1_u8(v,          vget_high_u8(q0));
233  }
234  static WEBP_INLINE void Store8x4x2_NEON(const uint8x16_t p1,
235                                          const uint8x16_t p0,
236                                          const uint8x16_t q0,
237                                          const uint8x16_t q1,
238                                          uint8_t* const u, uint8_t* const v,
239                                          int stride) {
240    Store8x2x2_NEON(p1, p0, u - stride, v - stride, stride);
241    Store8x2x2_NEON(q0, q1, u + stride, v + stride, stride);
242  }
243  #if !defined(WORK_AROUND_GCC)
244  #define STORE6_LANE(DST, VAL0, VAL1, LANE) do {   \
245    vst3_lane_u8((DST) - 3, (VAL0), (LANE));        \
246    vst3_lane_u8((DST) + 0, (VAL1), (LANE));        \
247    (DST) += stride;                                \
248  } while (0)
249  static WEBP_INLINE void Store6x8x2_NEON(
250      const uint8x16_t p2, const uint8x16_t p1, const uint8x16_t p0,
251      const uint8x16_t q0, const uint8x16_t q1, const uint8x16_t q2,
252      uint8_t* u, uint8_t* v, int stride) {
253    uint8x8x3_t u0, u1, v0, v1;
254    INIT_VECTOR3(u0, vget_low_u8(p2), vget_low_u8(p1), vget_low_u8(p0));
255    INIT_VECTOR3(u1, vget_low_u8(q0), vget_low_u8(q1), vget_low_u8(q2));
256    INIT_VECTOR3(v0, vget_high_u8(p2), vget_high_u8(p1), vget_high_u8(p0));
257    INIT_VECTOR3(v1, vget_high_u8(q0), vget_high_u8(q1), vget_high_u8(q2));
258    STORE6_LANE(u, u0, u1, 0);
259    STORE6_LANE(u, u0, u1, 1);
260    STORE6_LANE(u, u0, u1, 2);
261    STORE6_LANE(u, u0, u1, 3);
262    STORE6_LANE(u, u0, u1, 4);
263    STORE6_LANE(u, u0, u1, 5);
264    STORE6_LANE(u, u0, u1, 6);
265    STORE6_LANE(u, u0, u1, 7);
266    STORE6_LANE(v, v0, v1, 0);
267    STORE6_LANE(v, v0, v1, 1);
268    STORE6_LANE(v, v0, v1, 2);
269    STORE6_LANE(v, v0, v1, 3);
270    STORE6_LANE(v, v0, v1, 4);
271    STORE6_LANE(v, v0, v1, 5);
272    STORE6_LANE(v, v0, v1, 6);
273    STORE6_LANE(v, v0, v1, 7);
274  }
275  #undef STORE6_LANE
276  static WEBP_INLINE void Store4x8x2_NEON(const uint8x16_t p1,
277                                          const uint8x16_t p0,
278                                          const uint8x16_t q0,
279                                          const uint8x16_t q1,
280                                          uint8_t* const u, uint8_t* const v,
281                                          int stride) {
282    uint8x8x4_t u0, v0;
283    INIT_VECTOR4(u0,
284                 vget_low_u8(p1), vget_low_u8(p0),
285                 vget_low_u8(q0), vget_low_u8(q1));
286    INIT_VECTOR4(v0,
287                 vget_high_u8(p1), vget_high_u8(p0),
288                 vget_high_u8(q0), vget_high_u8(q1));
289    vst4_lane_u8(u - 2 + 0 * stride, u0, 0);
290    vst4_lane_u8(u - 2 + 1 * stride, u0, 1);
291    vst4_lane_u8(u - 2 + 2 * stride, u0, 2);
292    vst4_lane_u8(u - 2 + 3 * stride, u0, 3);
293    vst4_lane_u8(u - 2 + 4 * stride, u0, 4);
294    vst4_lane_u8(u - 2 + 5 * stride, u0, 5);
295    vst4_lane_u8(u - 2 + 6 * stride, u0, 6);
296    vst4_lane_u8(u - 2 + 7 * stride, u0, 7);
297    vst4_lane_u8(v - 2 + 0 * stride, v0, 0);
298    vst4_lane_u8(v - 2 + 1 * stride, v0, 1);
299    vst4_lane_u8(v - 2 + 2 * stride, v0, 2);
300    vst4_lane_u8(v - 2 + 3 * stride, v0, 3);
301    vst4_lane_u8(v - 2 + 4 * stride, v0, 4);
302    vst4_lane_u8(v - 2 + 5 * stride, v0, 5);
303    vst4_lane_u8(v - 2 + 6 * stride, v0, 6);
304    vst4_lane_u8(v - 2 + 7 * stride, v0, 7);
305  }
306  #endif  
307  static WEBP_INLINE int16x8_t ConvertU8ToS16_NEON(uint8x8_t v) {
308    return vreinterpretq_s16_u16(vmovl_u8(v));
309  }
310  static WEBP_INLINE void SaturateAndStore4x4_NEON(uint8_t* const dst,
311                                                   const int16x8_t dst01,
312                                                   const int16x8_t dst23) {
313    const uint8x8_t dst01_u8 = vqmovun_s16(dst01);
314    const uint8x8_t dst23_u8 = vqmovun_s16(dst23);
315    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), vreinterpret_u32_u8(dst01_u8), 0);
316    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), vreinterpret_u32_u8(dst01_u8), 1);
317    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), vreinterpret_u32_u8(dst23_u8), 0);
318    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), vreinterpret_u32_u8(dst23_u8), 1);
319  }
320  static WEBP_INLINE void Add4x4_NEON(const int16x8_t row01,
321                                      const int16x8_t row23,
322                                      uint8_t* const dst) {
323    uint32x2_t dst01 = vdup_n_u32(0);
324    uint32x2_t dst23 = vdup_n_u32(0);
325    dst01 = vld1_lane_u32((uint32_t*)(dst + 0 * BPS), dst01, 0);
326    dst23 = vld1_lane_u32((uint32_t*)(dst + 2 * BPS), dst23, 0);
327    dst01 = vld1_lane_u32((uint32_t*)(dst + 1 * BPS), dst01, 1);
328    dst23 = vld1_lane_u32((uint32_t*)(dst + 3 * BPS), dst23, 1);
329    {
330      const int16x8_t dst01_s16 = ConvertU8ToS16_NEON(vreinterpret_u8_u32(dst01));
331      const int16x8_t dst23_s16 = ConvertU8ToS16_NEON(vreinterpret_u8_u32(dst23));
332      const int16x8_t out01 = vrsraq_n_s16(dst01_s16, row01, 3);
333      const int16x8_t out23 = vrsraq_n_s16(dst23_s16, row23, 3);
334      SaturateAndStore4x4_NEON(dst, out01, out23);
335    }
336  }
337  static uint8x16_t NeedsFilter_NEON(const uint8x16_t p1, const uint8x16_t p0,
338                                     const uint8x16_t q0, const uint8x16_t q1,
339                                     int thresh) {
340    const uint8x16_t thresh_v = vdupq_n_u8((uint8_t)thresh);
341    const uint8x16_t a_p0_q0 = vabdq_u8(p0, q0);               
342    const uint8x16_t a_p1_q1 = vabdq_u8(p1, q1);               
343    const uint8x16_t a_p0_q0_2 = vqaddq_u8(a_p0_q0, a_p0_q0);  
344    const uint8x16_t a_p1_q1_2 = vshrq_n_u8(a_p1_q1, 1);       
345    const uint8x16_t sum = vqaddq_u8(a_p0_q0_2, a_p1_q1_2);
346    const uint8x16_t mask = vcgeq_u8(thresh_v, sum);
347    return mask;
348  }
349  static int8x16_t FlipSign_NEON(const uint8x16_t v) {
350    const uint8x16_t sign_bit = vdupq_n_u8(0x80);
351    return vreinterpretq_s8_u8(veorq_u8(v, sign_bit));
352  }
353  static uint8x16_t FlipSignBack_NEON(const int8x16_t v) {
354    const int8x16_t sign_bit = vdupq_n_s8(0x80);
355    return vreinterpretq_u8_s8(veorq_s8(v, sign_bit));
356  }
357  static int8x16_t GetBaseDelta_NEON(const int8x16_t p1, const int8x16_t p0,
358                                     const int8x16_t q0, const int8x16_t q1) {
359    const int8x16_t q0_p0 = vqsubq_s8(q0, p0);      
360    const int8x16_t p1_q1 = vqsubq_s8(p1, q1);      
361    const int8x16_t s1 = vqaddq_s8(p1_q1, q0_p0);   
362    const int8x16_t s2 = vqaddq_s8(q0_p0, s1);      
363    const int8x16_t s3 = vqaddq_s8(q0_p0, s2);      
364    return s3;
365  }
366  static int8x16_t GetBaseDelta0_NEON(const int8x16_t p0, const int8x16_t q0) {
367    const int8x16_t q0_p0 = vqsubq_s8(q0, p0);      
368    const int8x16_t s1 = vqaddq_s8(q0_p0, q0_p0);   
369    const int8x16_t s2 = vqaddq_s8(q0_p0, s1);      
370    return s2;
371  }
372  static void ApplyFilter2NoFlip_NEON(const int8x16_t p0s, const int8x16_t q0s,
373                                      const int8x16_t delta,
374                                      int8x16_t* const op0,
375                                      int8x16_t* const oq0) {
376    const int8x16_t kCst3 = vdupq_n_s8(0x03);
377    const int8x16_t kCst4 = vdupq_n_s8(0x04);
378    const int8x16_t delta_p3 = vqaddq_s8(delta, kCst3);
379    const int8x16_t delta_p4 = vqaddq_s8(delta, kCst4);
380    const int8x16_t delta3 = vshrq_n_s8(delta_p3, 3);
381    const int8x16_t delta4 = vshrq_n_s8(delta_p4, 3);
382    *op0 = vqaddq_s8(p0s, delta3);
383    *oq0 = vqsubq_s8(q0s, delta4);
384  }
385  #if defined(WEBP_USE_INTRINSICS)
386  static void ApplyFilter2_NEON(const int8x16_t p0s, const int8x16_t q0s,
387                                const int8x16_t delta,
388                                uint8x16_t* const op0, uint8x16_t* const oq0) {
389    const int8x16_t kCst3 = vdupq_n_s8(0x03);
390    const int8x16_t kCst4 = vdupq_n_s8(0x04);
391    const int8x16_t delta_p3 = vqaddq_s8(delta, kCst3);
392    const int8x16_t delta_p4 = vqaddq_s8(delta, kCst4);
393    const int8x16_t delta3 = vshrq_n_s8(delta_p3, 3);
394    const int8x16_t delta4 = vshrq_n_s8(delta_p4, 3);
395    const int8x16_t sp0 = vqaddq_s8(p0s, delta3);
396    const int8x16_t sq0 = vqsubq_s8(q0s, delta4);
397    *op0 = FlipSignBack_NEON(sp0);
398    *oq0 = FlipSignBack_NEON(sq0);
399  }
400  static void DoFilter2_NEON(const uint8x16_t p1, const uint8x16_t p0,
401                             const uint8x16_t q0, const uint8x16_t q1,
402                             const uint8x16_t mask,
403                             uint8x16_t* const op0, uint8x16_t* const oq0) {
404    const int8x16_t p1s = FlipSign_NEON(p1);
405    const int8x16_t p0s = FlipSign_NEON(p0);
406    const int8x16_t q0s = FlipSign_NEON(q0);
407    const int8x16_t q1s = FlipSign_NEON(q1);
408    const int8x16_t delta0 = GetBaseDelta_NEON(p1s, p0s, q0s, q1s);
409    const int8x16_t delta1 = vandq_s8(delta0, vreinterpretq_s8_u8(mask));
410    ApplyFilter2_NEON(p0s, q0s, delta1, op0, oq0);
411  }
412  static void SimpleVFilter16_NEON(uint8_t* p, int stride, int thresh) {
413    uint8x16_t p1, p0, q0, q1, op0, oq0;
414    Load16x4_NEON(p, stride, &p1, &p0, &q0, &q1);
415    {
416      const uint8x16_t mask = NeedsFilter_NEON(p1, p0, q0, q1, thresh);
417      DoFilter2_NEON(p1, p0, q0, q1, mask, &op0, &oq0);
418    }
419    Store16x2_NEON(op0, oq0, p, stride);
420  }
421  static void SimpleHFilter16_NEON(uint8_t* p, int stride, int thresh) {
422    uint8x16_t p1, p0, q0, q1, oq0, op0;
423    Load4x16_NEON(p, stride, &p1, &p0, &q0, &q1);
424    {
425      const uint8x16_t mask = NeedsFilter_NEON(p1, p0, q0, q1, thresh);
426      DoFilter2_NEON(p1, p0, q0, q1, mask, &op0, &oq0);
427    }
428    Store2x16_NEON(op0, oq0, p, stride);
429  }
430  #else
431  #define LOAD8x4(c1, c2, c3, c4, b1, b2, stride)                                \
432    "vld4.8 {" #c1 "[0]," #c2 "[0]," #c3 "[0]," #c4 "[0]}," #b1 "," #stride "\n" \
433    "vld4.8 {" #c1 "[1]," #c2 "[1]," #c3 "[1]," #c4 "[1]}," #b2 "," #stride "\n" \
434    "vld4.8 {" #c1 "[2]," #c2 "[2]," #c3 "[2]," #c4 "[2]}," #b1 "," #stride "\n" \
435    "vld4.8 {" #c1 "[3]," #c2 "[3]," #c3 "[3]," #c4 "[3]}," #b2 "," #stride "\n" \
436    "vld4.8 {" #c1 "[4]," #c2 "[4]," #c3 "[4]," #c4 "[4]}," #b1 "," #stride "\n" \
437    "vld4.8 {" #c1 "[5]," #c2 "[5]," #c3 "[5]," #c4 "[5]}," #b2 "," #stride "\n" \
438    "vld4.8 {" #c1 "[6]," #c2 "[6]," #c3 "[6]," #c4 "[6]}," #b1 "," #stride "\n" \
439    "vld4.8 {" #c1 "[7]," #c2 "[7]," #c3 "[7]," #c4 "[7]}," #b2 "," #stride "\n"
440  #define STORE8x2(c1, c2, p, stride)                                            \
441    "vst2.8   {" #c1 "[0], " #c2 "[0]}," #p "," #stride " \n"                    \
442    "vst2.8   {" #c1 "[1], " #c2 "[1]}," #p "," #stride " \n"                    \
443    "vst2.8   {" #c1 "[2], " #c2 "[2]}," #p "," #stride " \n"                    \
444    "vst2.8   {" #c1 "[3], " #c2 "[3]}," #p "," #stride " \n"                    \
445    "vst2.8   {" #c1 "[4], " #c2 "[4]}," #p "," #stride " \n"                    \
446    "vst2.8   {" #c1 "[5], " #c2 "[5]}," #p "," #stride " \n"                    \
447    "vst2.8   {" #c1 "[6], " #c2 "[6]}," #p "," #stride " \n"                    \
448    "vst2.8   {" #c1 "[7], " #c2 "[7]}," #p "," #stride " \n"
449  #define QRegs "q0", "q1", "q2", "q3",                                          \
450                "q8", "q9", "q10", "q11", "q12", "q13", "q14", "q15"
451  #define FLIP_SIGN_BIT2(a, b, s)                                                \
452    "veor     " #a "," #a "," #s "               \n"                             \
453    "veor     " #b "," #b "," #s "               \n"                             \
454  
455  #define FLIP_SIGN_BIT4(a, b, c, d, s)                                          \
456    FLIP_SIGN_BIT2(a, b, s)                                                      \
457    FLIP_SIGN_BIT2(c, d, s)                                                      \
458  
459  #define NEEDS_FILTER(p1, p0, q0, q1, thresh, mask)                             \
460    "vabd.u8    q15," #p0 "," #q0 "         \n"  &bsol;* abs(p0 - q0) */              \
461    "vabd.u8    q14," #p1 "," #q1 "         \n"  &bsol;* abs(p1 - q1) */              \
462    "vqadd.u8   q15, q15, q15               \n"  &bsol;* abs(p0 - q0) * 2 */          \
463    "vshr.u8    q14, q14, #1                \n"  &bsol;* abs(p1 - q1) / 2 */          \
464    "vqadd.u8   q15, q15, q14     \n"  &bsol;* abs(p0 - q0) * 2 + abs(p1 - q1) / 2 */ \
465    "vdup.8     q14, " #thresh "            \n"                                  \
466    "vcge.u8   " #mask ", q14, q15          \n"  &bsol;* mask <= thresh */
467  #define GET_BASE_DELTA(p1, p0, q0, q1, o)                                      \
468    "vqsub.s8   q15," #q0 "," #p0 "         \n"  &bsol;* (q0 - p0) */                 \
469    "vqsub.s8  " #o "," #p1 "," #q1 "       \n"  &bsol;* (p1 - q1) */                 \
470    "vqadd.s8  " #o "," #o ", q15           \n"  &bsol;* (p1 - q1) + 1 * (p0 - q0) */ \
471    "vqadd.s8  " #o "," #o ", q15           \n"  &bsol;* (p1 - q1) + 2 * (p0 - q0) */ \
472    "vqadd.s8  " #o "," #o ", q15           \n"  &bsol;* (p1 - q1) + 3 * (p0 - q0) */
473  #define DO_SIMPLE_FILTER(p0, q0, fl)                                           \
474    "vmov.i8    q15, #0x03                  \n"                                  \
475    "vqadd.s8   q15, q15, " #fl "           \n"  &bsol;* filter1 = filter + 3 */      \
476    "vshr.s8    q15, q15, #3                \n"  &bsol;* filter1 >> 3 */              \
477    "vqadd.s8  " #p0 "," #p0 ", q15         \n"  &bsol;* p0 += filter1 */             \
478                                                                                 \
479    "vmov.i8    q15, #0x04                  \n"                                  \
480    "vqadd.s8   q15, q15, " #fl "           \n"  &bsol;* filter1 = filter + 4 */      \
481    "vshr.s8    q15, q15, #3                \n"  &bsol;* filter2 >> 3 */              \
482    "vqsub.s8  " #q0 "," #q0 ", q15         \n"  &bsol;* q0 -= filter2 */
483  #define DO_FILTER2(p1, p0, q0, q1, thresh)                                     \
484    NEEDS_FILTER(p1, p0, q0, q1, thresh, q9)     &bsol;* filter mask in q9 */         \
485    "vmov.i8    q10, #0x80                  \n"  &bsol;* sign bit */                  \
486    FLIP_SIGN_BIT4(p1, p0, q0, q1, q10)          &bsol;* convert to signed value */   \
487    GET_BASE_DELTA(p1, p0, q0, q1, q11)          &bsol;* get filter level  */         \
488    "vand       q9, q9, q11                 \n"  &bsol;* apply filter mask */         \
489    DO_SIMPLE_FILTER(p0, q0, q9)                 &bsol;* apply filter */              \
490    FLIP_SIGN_BIT2(p0, q0, q10)
491  static void SimpleVFilter16_NEON(uint8_t* p, int stride, int thresh) {
492    __asm__ volatile (
493      "sub        %[p], %[p], %[stride], lsl #1  \n"  
494      "vld1.u8    {q1}, [%[p]], %[stride]        \n"  
495      "vld1.u8    {q2}, [%[p]], %[stride]        \n"  
496      "vld1.u8    {q3}, [%[p]], %[stride]        \n"  
497      "vld1.u8    {q12}, [%[p]]                  \n"  
498      DO_FILTER2(q1, q2, q3, q12, %[thresh])
499      "sub        %[p], %[p], %[stride], lsl #1  \n"  
500      "vst1.u8    {q2}, [%[p]], %[stride]        \n"  
501      "vst1.u8    {q3}, [%[p]]                   \n"  
502      : [p] "+r"(p)
503      : [stride] "r"(stride), [thresh] "r"(thresh)
504      : "memory", QRegs
505    );
506  }
507  static void SimpleHFilter16_NEON(uint8_t* p, int stride, int thresh) {
508    __asm__ volatile (
509      "sub        r4, %[p], #2                   \n"  
510      "lsl        r6, %[stride], #1              \n"  
511      "add        r5, r4, %[stride]              \n"  
512      LOAD8x4(d2, d3, d4, d5, [r4], [r5], r6)
513      LOAD8x4(d24, d25, d26, d27, [r4], [r5], r6)
514      "vswp       d3, d24                        \n"  
515      "vswp       d5, d26                        \n"  
516      "vswp       q2, q12                        \n"  
517      DO_FILTER2(q1, q2, q12, q13, %[thresh])
518      "sub        %[p], %[p], #1                 \n"  
519      "vswp        d5, d24                       \n"
520      STORE8x2(d4, d5, [%[p]], %[stride])
521      STORE8x2(d24, d25, [%[p]], %[stride])
522      : [p] "+r"(p)
523      : [stride] "r"(stride), [thresh] "r"(thresh)
524      : "memory", "r4", "r5", "r6", QRegs
525    );
526  }
527  #undef LOAD8x4
528  #undef STORE8x2
529  #endif    
530  static void SimpleVFilter16i_NEON(uint8_t* p, int stride, int thresh) {
531    uint32_t k;
532    for (k = 3; k != 0; --k) {
533      p += 4 * stride;
534      SimpleVFilter16_NEON(p, stride, thresh);
535    }
536  }
537  static void SimpleHFilter16i_NEON(uint8_t* p, int stride, int thresh) {
538    uint32_t k;
539    for (k = 3; k != 0; --k) {
540      p += 4;
541      SimpleHFilter16_NEON(p, stride, thresh);
542    }
543  }
544  static uint8x16_t NeedsHev_NEON(const uint8x16_t p1, const uint8x16_t p0,
545                                  const uint8x16_t q0, const uint8x16_t q1,
546                                  int hev_thresh) {
547    const uint8x16_t hev_thresh_v = vdupq_n_u8((uint8_t)hev_thresh);
548    const uint8x16_t a_p1_p0 = vabdq_u8(p1, p0);  
549    const uint8x16_t a_q1_q0 = vabdq_u8(q1, q0);  
550    const uint8x16_t a_max = vmaxq_u8(a_p1_p0, a_q1_q0);
551    const uint8x16_t mask = vcgtq_u8(a_max, hev_thresh_v);
552    return mask;
553  }
554  static uint8x16_t NeedsFilter2_NEON(const uint8x16_t p3, const uint8x16_t p2,
555                                      const uint8x16_t p1, const uint8x16_t p0,
556                                      const uint8x16_t q0, const uint8x16_t q1,
557                                      const uint8x16_t q2, const uint8x16_t q3,
558                                      int ithresh, int thresh) {
559    const uint8x16_t ithresh_v = vdupq_n_u8((uint8_t)ithresh);
560    const uint8x16_t a_p3_p2 = vabdq_u8(p3, p2);  
561    const uint8x16_t a_p2_p1 = vabdq_u8(p2, p1);  
562    const uint8x16_t a_p1_p0 = vabdq_u8(p1, p0);  
563    const uint8x16_t a_q3_q2 = vabdq_u8(q3, q2);  
564    const uint8x16_t a_q2_q1 = vabdq_u8(q2, q1);  
565    const uint8x16_t a_q1_q0 = vabdq_u8(q1, q0);  
566    const uint8x16_t max1 = vmaxq_u8(a_p3_p2, a_p2_p1);
567    const uint8x16_t max2 = vmaxq_u8(a_p1_p0, a_q3_q2);
568    const uint8x16_t max3 = vmaxq_u8(a_q2_q1, a_q1_q0);
569    const uint8x16_t max12 = vmaxq_u8(max1, max2);
570    const uint8x16_t max123 = vmaxq_u8(max12, max3);
571    const uint8x16_t mask2 = vcgeq_u8(ithresh_v, max123);
572    const uint8x16_t mask1 = NeedsFilter_NEON(p1, p0, q0, q1, thresh);
573    const uint8x16_t mask = vandq_u8(mask1, mask2);
574    return mask;
575  }
576  static void ApplyFilter4_NEON(
577      const int8x16_t p1, const int8x16_t p0,
578      const int8x16_t q0, const int8x16_t q1,
579      const int8x16_t delta0,
580      uint8x16_t* const op1, uint8x16_t* const op0,
581      uint8x16_t* const oq0, uint8x16_t* const oq1) {
582    const int8x16_t kCst3 = vdupq_n_s8(0x03);
583    const int8x16_t kCst4 = vdupq_n_s8(0x04);
584    const int8x16_t delta1 = vqaddq_s8(delta0, kCst4);
585    const int8x16_t delta2 = vqaddq_s8(delta0, kCst3);
586    const int8x16_t a1 = vshrq_n_s8(delta1, 3);
587    const int8x16_t a2 = vshrq_n_s8(delta2, 3);
588    const int8x16_t a3 = vrshrq_n_s8(a1, 1);   
589    *op0 = FlipSignBack_NEON(vqaddq_s8(p0, a2));  
590    *oq0 = FlipSignBack_NEON(vqsubq_s8(q0, a1));  
591    *op1 = FlipSignBack_NEON(vqaddq_s8(p1, a3));  
592    *oq1 = FlipSignBack_NEON(vqsubq_s8(q1, a3));  
593  }
594  static void DoFilter4_NEON(
595      const uint8x16_t p1, const uint8x16_t p0,
596      const uint8x16_t q0, const uint8x16_t q1,
597      const uint8x16_t mask, const uint8x16_t hev_mask,
598      uint8x16_t* const op1, uint8x16_t* const op0,
599      uint8x16_t* const oq0, uint8x16_t* const oq1) {
600    const int8x16_t p1s = FlipSign_NEON(p1);
601    int8x16_t p0s = FlipSign_NEON(p0);
602    int8x16_t q0s = FlipSign_NEON(q0);
603    const int8x16_t q1s = FlipSign_NEON(q1);
604    const uint8x16_t simple_lf_mask = vandq_u8(mask, hev_mask);
605    {
606      const int8x16_t delta = GetBaseDelta_NEON(p1s, p0s, q0s, q1s);
607      const int8x16_t simple_lf_delta =
608          vandq_s8(delta, vreinterpretq_s8_u8(simple_lf_mask));
609      ApplyFilter2NoFlip_NEON(p0s, q0s, simple_lf_delta, &p0s, &q0s);
610    }
611    {
612      const int8x16_t delta0 = GetBaseDelta0_NEON(p0s, q0s);
613      const uint8x16_t complex_lf_mask = veorq_u8(simple_lf_mask, mask);
614      const int8x16_t complex_lf_delta =
615          vandq_s8(delta0, vreinterpretq_s8_u8(complex_lf_mask));
616      ApplyFilter4_NEON(p1s, p0s, q0s, q1s, complex_lf_delta, op1, op0, oq0, oq1);
617    }
618  }
619  static void ApplyFilter6_NEON(
620      const int8x16_t p2, const int8x16_t p1, const int8x16_t p0,
621      const int8x16_t q0, const int8x16_t q1, const int8x16_t q2,
622      const int8x16_t delta,
623      uint8x16_t* const op2, uint8x16_t* const op1, uint8x16_t* const op0,
624      uint8x16_t* const oq0, uint8x16_t* const oq1, uint8x16_t* const oq2) {
625    const int8x8_t delta_lo = vget_low_s8(delta);
626    const int8x8_t delta_hi = vget_high_s8(delta);
627    const int8x8_t kCst9 = vdup_n_s8(9);
628    const int16x8_t kCstm1 = vdupq_n_s16(-1);
629    const int8x8_t kCst18 = vdup_n_s8(18);
630    const int16x8_t S_lo = vmlal_s8(kCstm1, kCst9, delta_lo);  
631    const int16x8_t S_hi = vmlal_s8(kCstm1, kCst9, delta_hi);
632    const int16x8_t Z_lo = vmlal_s8(S_lo, kCst18, delta_lo);   
633    const int16x8_t Z_hi = vmlal_s8(S_hi, kCst18, delta_hi);
634    const int8x8_t a3_lo = vqrshrn_n_s16(S_lo, 7);   
635    const int8x8_t a3_hi = vqrshrn_n_s16(S_hi, 7);
636    const int8x8_t a2_lo = vqrshrn_n_s16(S_lo, 6);   
637    const int8x8_t a2_hi = vqrshrn_n_s16(S_hi, 6);
638    const int8x8_t a1_lo = vqrshrn_n_s16(Z_lo, 7);   
639    const int8x8_t a1_hi = vqrshrn_n_s16(Z_hi, 7);
640    const int8x16_t a1 = vcombine_s8(a1_lo, a1_hi);
641    const int8x16_t a2 = vcombine_s8(a2_lo, a2_hi);
642    const int8x16_t a3 = vcombine_s8(a3_lo, a3_hi);
643    *op0 = FlipSignBack_NEON(vqaddq_s8(p0, a1));  
644    *oq0 = FlipSignBack_NEON(vqsubq_s8(q0, a1));  
645    *oq1 = FlipSignBack_NEON(vqsubq_s8(q1, a2));  
646    *op1 = FlipSignBack_NEON(vqaddq_s8(p1, a2));  
647    *oq2 = FlipSignBack_NEON(vqsubq_s8(q2, a3));  
648    *op2 = FlipSignBack_NEON(vqaddq_s8(p2, a3));  
649  }
650  static void DoFilter6_NEON(
651      const uint8x16_t p2, const uint8x16_t p1, const uint8x16_t p0,
652      const uint8x16_t q0, const uint8x16_t q1, const uint8x16_t q2,
653      const uint8x16_t mask, const uint8x16_t hev_mask,
654      uint8x16_t* const op2, uint8x16_t* const op1, uint8x16_t* const op0,
655      uint8x16_t* const oq0, uint8x16_t* const oq1, uint8x16_t* const oq2) {
656    const int8x16_t p2s = FlipSign_NEON(p2);
657    const int8x16_t p1s = FlipSign_NEON(p1);
658    int8x16_t p0s = FlipSign_NEON(p0);
659    int8x16_t q0s = FlipSign_NEON(q0);
660    const int8x16_t q1s = FlipSign_NEON(q1);
661    const int8x16_t q2s = FlipSign_NEON(q2);
662    const uint8x16_t simple_lf_mask = vandq_u8(mask, hev_mask);
663    const int8x16_t delta0 = GetBaseDelta_NEON(p1s, p0s, q0s, q1s);
664    {
665      const int8x16_t simple_lf_delta =
666          vandq_s8(delta0, vreinterpretq_s8_u8(simple_lf_mask));
667      ApplyFilter2NoFlip_NEON(p0s, q0s, simple_lf_delta, &p0s, &q0s);
668    }
669    {
670      const uint8x16_t complex_lf_mask = veorq_u8(simple_lf_mask, mask);
671      const int8x16_t complex_lf_delta =
672          vandq_s8(delta0, vreinterpretq_s8_u8(complex_lf_mask));
673      ApplyFilter6_NEON(p2s, p1s, p0s, q0s, q1s, q2s, complex_lf_delta,
674                        op2, op1, op0, oq0, oq1, oq2);
675    }
676  }
677  static void VFilter16_NEON(uint8_t* p, int stride,
678                             int thresh, int ithresh, int hev_thresh) {
679    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
680    Load16x8_NEON(p, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
681    {
682      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
683                                                ithresh, thresh);
684      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
685      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
686      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
687                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
688      Store16x2_NEON(op2, op1, p - 2 * stride, stride);
689      Store16x2_NEON(op0, oq0, p + 0 * stride, stride);
690      Store16x2_NEON(oq1, oq2, p + 2 * stride, stride);
691    }
692  }
693  static void HFilter16_NEON(uint8_t* p, int stride,
694                             int thresh, int ithresh, int hev_thresh) {
695    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
696    Load8x16_NEON(p, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
697    {
698      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
699                                                ithresh, thresh);
700      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
701      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
702      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
703                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
704      Store2x16_NEON(op2, op1, p - 2, stride);
705      Store2x16_NEON(op0, oq0, p + 0, stride);
706      Store2x16_NEON(oq1, oq2, p + 2, stride);
707    }
708  }
709  static void VFilter16i_NEON(uint8_t* p, int stride,
710                              int thresh, int ithresh, int hev_thresh) {
711    uint32_t k;
712    uint8x16_t p3, p2, p1, p0;
713    Load16x4_NEON(p + 2  * stride, stride, &p3, &p2, &p1, &p0);
714    for (k = 3; k != 0; --k) {
715      uint8x16_t q0, q1, q2, q3;
716      p += 4 * stride;
717      Load16x4_NEON(p + 2  * stride, stride, &q0, &q1, &q2, &q3);
718      {
719        const uint8x16_t mask =
720            NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3, ithresh, thresh);
721        const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
722        DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &p1, &p0, &p3, &p2);
723        Store16x4_NEON(p1, p0, p3, p2, p, stride);
724        p1 = q2;
725        p0 = q3;
726      }
727    }
728  }
729  #if !defined(WORK_AROUND_GCC)
730  static void HFilter16i_NEON(uint8_t* p, int stride,
731                              int thresh, int ithresh, int hev_thresh) {
732    uint32_t k;
733    uint8x16_t p3, p2, p1, p0;
734    Load4x16_NEON(p + 2, stride, &p3, &p2, &p1, &p0);
735    for (k = 3; k != 0; --k) {
736      uint8x16_t q0, q1, q2, q3;
737      p += 4;
738      Load4x16_NEON(p + 2, stride, &q0, &q1, &q2, &q3);
739      {
740        const uint8x16_t mask =
741            NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3, ithresh, thresh);
742        const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
743        DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &p1, &p0, &p3, &p2);
744        Store4x16_NEON(p1, p0, p3, p2, p, stride);
745        p1 = q2;
746        p0 = q3;
747      }
748    }
749  }
750  #endif  
751  static void VFilter8_NEON(uint8_t* u, uint8_t* v, int stride,
752                            int thresh, int ithresh, int hev_thresh) {
753    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
754    Load8x8x2_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
755    {
756      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
757                                                ithresh, thresh);
758      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
759      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
760      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
761                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
762      Store8x2x2_NEON(op2, op1, u - 2 * stride, v - 2 * stride, stride);
763      Store8x2x2_NEON(op0, oq0, u + 0 * stride, v + 0 * stride, stride);
764      Store8x2x2_NEON(oq1, oq2, u + 2 * stride, v + 2 * stride, stride);
765    }
766  }
767  static void VFilter8i_NEON(uint8_t* u, uint8_t* v, int stride,
768                             int thresh, int ithresh, int hev_thresh) {
769    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
770    u += 4 * stride;
771    v += 4 * stride;
772    Load8x8x2_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
773    {
774      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
775                                                ithresh, thresh);
776      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
777      uint8x16_t op1, op0, oq0, oq1;
778      DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &op1, &op0, &oq0, &oq1);
779      Store8x4x2_NEON(op1, op0, oq0, oq1, u, v, stride);
780    }
781  }
782  #if !defined(WORK_AROUND_GCC)
783  static void HFilter8_NEON(uint8_t* u, uint8_t* v, int stride,
784                            int thresh, int ithresh, int hev_thresh) {
785    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
786    Load8x8x2T_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
787    {
788      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
789                                                ithresh, thresh);
790      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
791      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
792      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
793                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
794      Store6x8x2_NEON(op2, op1, op0, oq0, oq1, oq2, u, v, stride);
795    }
796  }
797  static void HFilter8i_NEON(uint8_t* u, uint8_t* v, int stride,
798                             int thresh, int ithresh, int hev_thresh) {
799    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
800    u += 4;
801    v += 4;
802    Load8x8x2T_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
803    {
804      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
805                                                ithresh, thresh);
806      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
807      uint8x16_t op1, op0, oq0, oq1;
808      DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &op1, &op0, &oq0, &oq1);
809      Store4x8x2_NEON(op1, op0, oq0, oq1, u, v, stride);
810    }
811  }
812  #endif  
813  static const int16_t kC1 = 20091;
814  static const int16_t kC2 = 17734;  
815  #if defined(WEBP_USE_INTRINSICS)
816  static WEBP_INLINE void Transpose8x2_NEON(const int16x8_t in0,
817                                            const int16x8_t in1,
818                                            int16x8x2_t* const out) {
819    const int16x8x2_t tmp0 = vzipq_s16(in0, in1);   
820    *out = vzipq_s16(tmp0.val[0], tmp0.val[1]);
821  }
822  static WEBP_INLINE void TransformPass_NEON(int16x8x2_t* const rows) {
823    const int16x8_t B1 =
824        vcombine_s16(vget_high_s16(rows->val[0]), vget_high_s16(rows->val[1]));
825    const int16x8_t C0 = vsraq_n_s16(B1, vqdmulhq_n_s16(B1, kC1), 1);
826    const int16x8_t C1 = vqdmulhq_n_s16(B1, kC2);
827    const int16x4_t a = vqadd_s16(vget_low_s16(rows->val[0]),
828                                  vget_low_s16(rows->val[1]));   
829    const int16x4_t b = vqsub_s16(vget_low_s16(rows->val[0]),
830                                  vget_low_s16(rows->val[1]));   
831    const int16x4_t c = vqsub_s16(vget_low_s16(C1), vget_high_s16(C0));
832    const int16x4_t d = vqadd_s16(vget_low_s16(C0), vget_high_s16(C1));
833    const int16x8_t D0 = vcombine_s16(a, b);      
834    const int16x8_t D1 = vcombine_s16(d, c);      
835    const int16x8_t E0 = vqaddq_s16(D0, D1);      
836    const int16x8_t E_tmp = vqsubq_s16(D0, D1);   
837    const int16x8_t E1 = vcombine_s16(vget_high_s16(E_tmp), vget_low_s16(E_tmp));
838    Transpose8x2_NEON(E0, E1, rows);
839  }
840  static void TransformOne_NEON(const int16_t* in, uint8_t* dst) {
<span onclick='openModal()' class='match'>841    int16x8x2_t rows;
842    INIT_VECTOR2(rows, vld1q_s16(in + 0), vld1q_s16(in + 8));
843    TransformPass_NEON(&rows);
844    TransformPass_NEON(&rows);
845    Add4x4_NEON(rows.val[0], rows.val[1], dst);
</span>846  }
847  #else
848  static void TransformOne_NEON(const int16_t* in, uint8_t* dst) {
849    const int kBPS = BPS;
850    const int16_t constants[4] = { kC1, kC2, 0, 0 };
851    __asm__ volatile (
852      "vld1.16         {q1, q2}, [%[in]]           \n"
853      "vld1.16         {d0}, [%[constants]]        \n"
854      "vswp            d3, d4                      \n"
855      "vqdmulh.s16     q8, q2, d0[0]               \n"
856      "vqdmulh.s16     q9, q2, d0[1]               \n"
857      "vqadd.s16       d22, d2, d3                 \n"
858      "vqsub.s16       d23, d2, d3                 \n"
859      "vshr.s16        q8, q8, #1                  \n"
860      "vqadd.s16       q8, q2, q8                  \n"
861      "vqsub.s16       d20, d18, d17               \n"
862      "vqadd.s16       d21, d19, d16               \n"
863      "vqadd.s16       d2, d22, d21                \n"
864      "vqadd.s16       d3, d23, d20                \n"
865      "vqsub.s16       d4, d23, d20                \n"
866      "vqsub.s16       d5, d22, d21                \n"
867      "vzip.16         q1, q2                      \n"
868      "vzip.16         q1, q2                      \n"
869      "vswp            d3, d4                      \n"
870      "vqdmulh.s16     q8, q2, d0[0]               \n"
871      "vqdmulh.s16     q9, q2, d0[1]               \n"
872      "vqadd.s16       d22, d2, d3                 \n"
873      "vqsub.s16       d23, d2, d3                 \n"
874      "vshr.s16        q8, q8, #1                  \n"
875      "vqadd.s16       q8, q2, q8                  \n"
876      "vqsub.s16       d20, d18, d17               \n"
877      "vqadd.s16       d21, d19, d16               \n"
878      "vqadd.s16       d2, d22, d21                \n"
879      "vqadd.s16       d3, d23, d20                \n"
880      "vqsub.s16       d4, d23, d20                \n"
881      "vqsub.s16       d5, d22, d21                \n"
882      "vld1.32         d6[0], [%[dst]], %[kBPS]    \n"
883      "vld1.32         d6[1], [%[dst]], %[kBPS]    \n"
884      "vld1.32         d7[0], [%[dst]], %[kBPS]    \n"
885      "vld1.32         d7[1], [%[dst]], %[kBPS]    \n"
886      "sub         %[dst], %[dst], %[kBPS], lsl #2 \n"
887      "vrshr.s16       d2, d2, #3                  \n"
888      "vrshr.s16       d3, d3, #3                  \n"
889      "vrshr.s16       d4, d4, #3                  \n"
890      "vrshr.s16       d5, d5, #3                  \n"
891      "vzip.16         q1, q2                      \n"
892      "vzip.16         q1, q2                      \n"
893      "vmovl.u8        q8, d6                      \n"
894      "vmovl.u8        q9, d7                      \n"
895      "vqadd.s16       q1, q1, q8                  \n"
896      "vqadd.s16       q2, q2, q9                  \n"
897      "vqmovun.s16     d0, q1                      \n"
898      "vqmovun.s16     d1, q2                      \n"
899      "vst1.32         d0[0], [%[dst]], %[kBPS]    \n"
900      "vst1.32         d0[1], [%[dst]], %[kBPS]    \n"
901      "vst1.32         d1[0], [%[dst]], %[kBPS]    \n"
902      "vst1.32         d1[1], [%[dst]]             \n"
903      : [in] "+r"(in), [dst] "+r"(dst)  &bsol;* modified registers */
904      : [kBPS] "r"(kBPS), [constants] "r"(constants)  &bsol;* constants */
905      : "memory", "q0", "q1", "q2", "q8", "q9", "q10", "q11"  &bsol;* clobbered */
906    );
907  }
908  #endif    
909  static void TransformTwo_NEON(const int16_t* in, uint8_t* dst, int do_two) {
910    TransformOne_NEON(in, dst);
911    if (do_two) {
912      TransformOne_NEON(in + 16, dst + 4);
913    }
914  }
915  static void TransformDC_NEON(const int16_t* in, uint8_t* dst) {
916    const int16x8_t DC = vdupq_n_s16(in[0]);
917    Add4x4_NEON(DC, DC, dst);
918  }
919  #define STORE_WHT(dst, col, rows) do {                  \
920    *dst = vgetq_lane_s32(rows.val[0], col); (dst) += 16; \
921    *dst = vgetq_lane_s32(rows.val[1], col); (dst) += 16; \
922    *dst = vgetq_lane_s32(rows.val[2], col); (dst) += 16; \
923    *dst = vgetq_lane_s32(rows.val[3], col); (dst) += 16; \
924  } while (0)
925  static void TransformWHT_NEON(const int16_t* in, int16_t* out) {
926    int32x4x4_t tmp;
927    {
928      const int16x4_t in00_03 = vld1_s16(in + 0);
929      const int16x4_t in04_07 = vld1_s16(in + 4);
930      const int16x4_t in08_11 = vld1_s16(in + 8);
931      const int16x4_t in12_15 = vld1_s16(in + 12);
932      const int32x4_t a0 = vaddl_s16(in00_03, in12_15);  
933      const int32x4_t a1 = vaddl_s16(in04_07, in08_11);  
934      const int32x4_t a2 = vsubl_s16(in04_07, in08_11);  
935      const int32x4_t a3 = vsubl_s16(in00_03, in12_15);  
936      tmp.val[0] = vaddq_s32(a0, a1);
937      tmp.val[1] = vaddq_s32(a3, a2);
938      tmp.val[2] = vsubq_s32(a0, a1);
939      tmp.val[3] = vsubq_s32(a3, a2);
940      tmp = Transpose4x4_NEON(tmp);
941    }
942    {
943      const int32x4_t kCst3 = vdupq_n_s32(3);
944      const int32x4_t dc = vaddq_s32(tmp.val[0], kCst3);  
945      const int32x4_t a0 = vaddq_s32(dc, tmp.val[3]);
946      const int32x4_t a1 = vaddq_s32(tmp.val[1], tmp.val[2]);
947      const int32x4_t a2 = vsubq_s32(tmp.val[1], tmp.val[2]);
948      const int32x4_t a3 = vsubq_s32(dc, tmp.val[3]);
949      tmp.val[0] = vaddq_s32(a0, a1);
950      tmp.val[1] = vaddq_s32(a3, a2);
951      tmp.val[2] = vsubq_s32(a0, a1);
952      tmp.val[3] = vsubq_s32(a3, a2);
953      tmp.val[0] = vshrq_n_s32(tmp.val[0], 3);
954      tmp.val[1] = vshrq_n_s32(tmp.val[1], 3);
955      tmp.val[2] = vshrq_n_s32(tmp.val[2], 3);
956      tmp.val[3] = vshrq_n_s32(tmp.val[3], 3);
957      STORE_WHT(out, 0, tmp);
958      STORE_WHT(out, 1, tmp);
959      STORE_WHT(out, 2, tmp);
960      STORE_WHT(out, 3, tmp);
961    }
962  }
963  #undef STORE_WHT
964  #define MUL(a, b) (((a) * (b)) >> 16)
965  static void TransformAC3_NEON(const int16_t* in, uint8_t* dst) {
966    static const int kC1_full = 20091 + (1 << 16);
967    static const int kC2_full = 35468;
968    const int16x4_t A = vld1_dup_s16(in);
969    const int16x4_t c4 = vdup_n_s16(MUL(in[4], kC2_full));
970    const int16x4_t d4 = vdup_n_s16(MUL(in[4], kC1_full));
971    const int c1 = MUL(in[1], kC2_full);
972    const int d1 = MUL(in[1], kC1_full);
973    const uint64_t cd = (uint64_t)( d1 & 0xffff) <<  0 |
974                        (uint64_t)( c1 & 0xffff) << 16 |
975                        (uint64_t)(-c1 & 0xffff) << 32 |
976                        (uint64_t)(-d1 & 0xffff) << 48;
977    const int16x4_t CD = vcreate_s16(cd);
978    const int16x4_t B = vqadd_s16(A, CD);
979    const int16x8_t m0_m1 = vcombine_s16(vqadd_s16(B, d4), vqadd_s16(B, c4));
980    const int16x8_t m2_m3 = vcombine_s16(vqsub_s16(B, c4), vqsub_s16(B, d4));
981    Add4x4_NEON(m0_m1, m2_m3, dst);
982  }
983  #undef MUL
984  static void DC4_NEON(uint8_t* dst) {    
985    const uint8x8_t A = vld1_u8(dst - BPS);  
986    const uint16x4_t p0 = vpaddl_u8(A);  
987    const uint16x4_t p1 = vpadd_u16(p0, p0);
988    const uint16x8_t L0 = vmovl_u8(vld1_u8(dst + 0 * BPS - 1));
989    const uint16x8_t L1 = vmovl_u8(vld1_u8(dst + 1 * BPS - 1));
990    const uint16x8_t L2 = vmovl_u8(vld1_u8(dst + 2 * BPS - 1));
991    const uint16x8_t L3 = vmovl_u8(vld1_u8(dst + 3 * BPS - 1));
992    const uint16x8_t s0 = vaddq_u16(L0, L1);
993    const uint16x8_t s1 = vaddq_u16(L2, L3);
994    const uint16x8_t s01 = vaddq_u16(s0, s1);
995    const uint16x8_t sum = vaddq_u16(s01, vcombine_u16(p1, p1));
996    const uint8x8_t dc0 = vrshrn_n_u16(sum, 3);  
997    const uint8x8_t dc = vdup_lane_u8(dc0, 0);
998    int i;
999    for (i = 0; i < 4; ++i) {
1000      vst1_lane_u32((uint32_t*)(dst + i * BPS), vreinterpret_u32_u8(dc), 0);
1001    }
1002  }
1003  static WEBP_INLINE void TrueMotion_NEON(uint8_t* dst, int size) {
1004    const uint8x8_t TL = vld1_dup_u8(dst - BPS - 1);  
1005    const uint8x8_t T = vld1_u8(dst - BPS);  
1006    const int16x8_t d = vreinterpretq_s16_u16(vsubl_u8(T, TL));  
1007    int y;
1008    for (y = 0; y < size; y += 4) {
1009      const int16x8_t L0 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 0 * BPS - 1));
1010      const int16x8_t L1 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 1 * BPS - 1));
1011      const int16x8_t L2 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 2 * BPS - 1));
1012      const int16x8_t L3 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 3 * BPS - 1));
1013      const int16x8_t r0 = vaddq_s16(L0, d);  
1014      const int16x8_t r1 = vaddq_s16(L1, d);
1015      const int16x8_t r2 = vaddq_s16(L2, d);
1016      const int16x8_t r3 = vaddq_s16(L3, d);
1017      const uint32x2_t r0_u32 = vreinterpret_u32_u8(vqmovun_s16(r0));
1018      const uint32x2_t r1_u32 = vreinterpret_u32_u8(vqmovun_s16(r1));
1019      const uint32x2_t r2_u32 = vreinterpret_u32_u8(vqmovun_s16(r2));
1020      const uint32x2_t r3_u32 = vreinterpret_u32_u8(vqmovun_s16(r3));
1021      if (size == 4) {
1022        vst1_lane_u32((uint32_t*)(dst + 0 * BPS), r0_u32, 0);
1023        vst1_lane_u32((uint32_t*)(dst + 1 * BPS), r1_u32, 0);
1024        vst1_lane_u32((uint32_t*)(dst + 2 * BPS), r2_u32, 0);
1025        vst1_lane_u32((uint32_t*)(dst + 3 * BPS), r3_u32, 0);
1026      } else {
1027        vst1_u32((uint32_t*)(dst + 0 * BPS), r0_u32);
1028        vst1_u32((uint32_t*)(dst + 1 * BPS), r1_u32);
1029        vst1_u32((uint32_t*)(dst + 2 * BPS), r2_u32);
1030        vst1_u32((uint32_t*)(dst + 3 * BPS), r3_u32);
1031      }
1032      dst += 4 * BPS;
1033    }
1034  }
1035  static void TM4_NEON(uint8_t* dst) { TrueMotion_NEON(dst, 4); }
1036  static void VE4_NEON(uint8_t* dst) {    
1037    const uint64x1_t A0 = vreinterpret_u64_u8(vld1_u8(dst - BPS - 1));  
1038    const uint64x1_t A1 = vshr_n_u64(A0, 8);
1039    const uint64x1_t A2 = vshr_n_u64(A0, 16);
1040    const uint8x8_t ABCDEFGH = vreinterpret_u8_u64(A0);
1041    const uint8x8_t BCDEFGH0 = vreinterpret_u8_u64(A1);
1042    const uint8x8_t CDEFGH00 = vreinterpret_u8_u64(A2);
1043    const uint8x8_t b = vhadd_u8(ABCDEFGH, CDEFGH00);
1044    const uint8x8_t avg = vrhadd_u8(b, BCDEFGH0);
1045    int i;
1046    for (i = 0; i < 4; ++i) {
1047      vst1_lane_u32((uint32_t*)(dst + i * BPS), vreinterpret_u32_u8(avg), 0);
1048    }
1049  }
1050  static void RD4_NEON(uint8_t* dst) {   
1051    const uint8x8_t XABCD_u8 = vld1_u8(dst - BPS - 1);
1052    const uint64x1_t XABCD = vreinterpret_u64_u8(XABCD_u8);
1053    const uint64x1_t ____XABC = vshl_n_u64(XABCD, 32);
1054    const uint32_t I = dst[-1 + 0 * BPS];
1055    const uint32_t J = dst[-1 + 1 * BPS];
1056    const uint32_t K = dst[-1 + 2 * BPS];
1057    const uint32_t L = dst[-1 + 3 * BPS];
1058    const uint64x1_t LKJI____ =
1059        vcreate_u64((uint64_t)L | (K << 8) | (J << 16) | (I << 24));
1060    const uint64x1_t LKJIXABC = vorr_u64(LKJI____, ____XABC);
1061    const uint8x8_t KJIXABC_ = vreinterpret_u8_u64(vshr_n_u64(LKJIXABC, 8));
1062    const uint8x8_t JIXABC__ = vreinterpret_u8_u64(vshr_n_u64(LKJIXABC, 16));
1063    const uint8_t D = vget_lane_u8(XABCD_u8, 4);
1064    const uint8x8_t JIXABCD_ = vset_lane_u8(D, JIXABC__, 6);
1065    const uint8x8_t LKJIXABC_u8 = vreinterpret_u8_u64(LKJIXABC);
1066    const uint8x8_t avg1 = vhadd_u8(JIXABCD_, LKJIXABC_u8);
1067    const uint8x8_t avg2 = vrhadd_u8(avg1, KJIXABC_);
1068    const uint64x1_t avg2_u64 = vreinterpret_u64_u8(avg2);
1069    const uint32x2_t r3 = vreinterpret_u32_u8(avg2);
1070    const uint32x2_t r2 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 8));
1071    const uint32x2_t r1 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 16));
1072    const uint32x2_t r0 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 24));
1073    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), r0, 0);
1074    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), r1, 0);
1075    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), r2, 0);
1076    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), r3, 0);
1077  }
1078  static void LD4_NEON(uint8_t* dst) {    
1079    const uint8x8_t ABCDEFGH = vld1_u8(dst - BPS + 0);
1080    const uint8x8_t BCDEFGH0 = vld1_u8(dst - BPS + 1);
1081    const uint8x8_t CDEFGH00 = vld1_u8(dst - BPS + 2);
1082    const uint8x8_t CDEFGHH0 = vset_lane_u8(dst[-BPS + 7], CDEFGH00, 6);
1083    const uint8x8_t avg1 = vhadd_u8(ABCDEFGH, CDEFGHH0);
1084    const uint8x8_t avg2 = vrhadd_u8(avg1, BCDEFGH0);
1085    const uint64x1_t avg2_u64 = vreinterpret_u64_u8(avg2);
1086    const uint32x2_t r0 = vreinterpret_u32_u8(avg2);
1087    const uint32x2_t r1 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 8));
1088    const uint32x2_t r2 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 16));
1089    const uint32x2_t r3 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 24));
1090    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), r0, 0);
1091    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), r1, 0);
1092    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), r2, 0);
1093    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), r3, 0);
1094  }
1095  static void VE8uv_NEON(uint8_t* dst) {    
1096    const uint8x8_t top = vld1_u8(dst - BPS);
1097    int j;
1098    for (j = 0; j < 8; ++j) {
1099      vst1_u8(dst + j * BPS, top);
1100    }
1101  }
1102  static void HE8uv_NEON(uint8_t* dst) {    
1103    int j;
1104    for (j = 0; j < 8; ++j) {
1105      const uint8x8_t left = vld1_dup_u8(dst - 1);
1106      vst1_u8(dst, left);
1107      dst += BPS;
1108    }
1109  }
1110  static WEBP_INLINE void DC8_NEON(uint8_t* dst, int do_top, int do_left) {
1111    uint16x8_t sum_top;
1112    uint16x8_t sum_left;
1113    uint8x8_t dc0;
1114    if (do_top) {
1115      const uint8x8_t A = vld1_u8(dst - BPS);  
1116  #if defined(__aarch64__)
1117      const uint16x8_t B = vmovl_u8(A);
1118      const uint16_t p2 = vaddvq_u16(B);
1119      sum_top = vdupq_n_u16(p2);
1120  #else
1121      const uint16x4_t p0 = vpaddl_u8(A);  
1122      const uint16x4_t p1 = vpadd_u16(p0, p0);
1123      const uint16x4_t p2 = vpadd_u16(p1, p1);
1124      sum_top = vcombine_u16(p2, p2);
1125  #endif
1126    }
1127    if (do_left) {
1128      const uint16x8_t L0 = vmovl_u8(vld1_u8(dst + 0 * BPS - 1));
1129      const uint16x8_t L1 = vmovl_u8(vld1_u8(dst + 1 * BPS - 1));
1130      const uint16x8_t L2 = vmovl_u8(vld1_u8(dst + 2 * BPS - 1));
1131      const uint16x8_t L3 = vmovl_u8(vld1_u8(dst + 3 * BPS - 1));
1132      const uint16x8_t L4 = vmovl_u8(vld1_u8(dst + 4 * BPS - 1));
1133      const uint16x8_t L5 = vmovl_u8(vld1_u8(dst + 5 * BPS - 1));
1134      const uint16x8_t L6 = vmovl_u8(vld1_u8(dst + 6 * BPS - 1));
1135      const uint16x8_t L7 = vmovl_u8(vld1_u8(dst + 7 * BPS - 1));
1136      const uint16x8_t s0 = vaddq_u16(L0, L1);
1137      const uint16x8_t s1 = vaddq_u16(L2, L3);
1138      const uint16x8_t s2 = vaddq_u16(L4, L5);
1139      const uint16x8_t s3 = vaddq_u16(L6, L7);
1140      const uint16x8_t s01 = vaddq_u16(s0, s1);
1141      const uint16x8_t s23 = vaddq_u16(s2, s3);
1142      sum_left = vaddq_u16(s01, s23);
1143    }
1144    if (do_top && do_left) {
1145      const uint16x8_t sum = vaddq_u16(sum_left, sum_top);
1146      dc0 = vrshrn_n_u16(sum, 4);
1147    } else if (do_top) {
1148      dc0 = vrshrn_n_u16(sum_top, 3);
1149    } else if (do_left) {
1150      dc0 = vrshrn_n_u16(sum_left, 3);
1151    } else {
1152      dc0 = vdup_n_u8(0x80);
1153    }
1154    {
1155      const uint8x8_t dc = vdup_lane_u8(dc0, 0);
1156      int i;
1157      for (i = 0; i < 8; ++i) {
1158        vst1_u32((uint32_t*)(dst + i * BPS), vreinterpret_u32_u8(dc));
1159      }
1160    }
1161  }
1162  static void DC8uv_NEON(uint8_t* dst) { DC8_NEON(dst, 1, 1); }
1163  static void DC8uvNoTop_NEON(uint8_t* dst) { DC8_NEON(dst, 0, 1); }
1164  static void DC8uvNoLeft_NEON(uint8_t* dst) { DC8_NEON(dst, 1, 0); }
1165  static void DC8uvNoTopLeft_NEON(uint8_t* dst) { DC8_NEON(dst, 0, 0); }
1166  static void TM8uv_NEON(uint8_t* dst) { TrueMotion_NEON(dst, 8); }
1167  static void VE16_NEON(uint8_t* dst) {     
1168    const uint8x16_t top = vld1q_u8(dst - BPS);
1169    int j;
1170    for (j = 0; j < 16; ++j) {
1171      vst1q_u8(dst + j * BPS, top);
1172    }
1173  }
1174  static void HE16_NEON(uint8_t* dst) {     
1175    int j;
1176    for (j = 0; j < 16; ++j) {
1177      const uint8x16_t left = vld1q_dup_u8(dst - 1);
1178      vst1q_u8(dst, left);
1179      dst += BPS;
1180    }
1181  }
1182  static WEBP_INLINE void DC16_NEON(uint8_t* dst, int do_top, int do_left) {
1183    uint16x8_t sum_top;
1184    uint16x8_t sum_left;
1185    uint8x8_t dc0;
1186    if (do_top) {
1187      const uint8x16_t A = vld1q_u8(dst - BPS);  
1188      const uint16x8_t p0 = vpaddlq_u8(A);  
1189      const uint16x4_t p1 = vadd_u16(vget_low_u16(p0), vget_high_u16(p0));
1190      const uint16x4_t p2 = vpadd_u16(p1, p1);
1191      const uint16x4_t p3 = vpadd_u16(p2, p2);
1192      sum_top = vcombine_u16(p3, p3);
1193    }
1194    if (do_left) {
1195      int i;
1196      sum_left = vdupq_n_u16(0);
1197      for (i = 0; i < 16; i += 8) {
1198        const uint16x8_t L0 = vmovl_u8(vld1_u8(dst + (i + 0) * BPS - 1));
1199        const uint16x8_t L1 = vmovl_u8(vld1_u8(dst + (i + 1) * BPS - 1));
1200        const uint16x8_t L2 = vmovl_u8(vld1_u8(dst + (i + 2) * BPS - 1));
1201        const uint16x8_t L3 = vmovl_u8(vld1_u8(dst + (i + 3) * BPS - 1));
1202        const uint16x8_t L4 = vmovl_u8(vld1_u8(dst + (i + 4) * BPS - 1));
1203        const uint16x8_t L5 = vmovl_u8(vld1_u8(dst + (i + 5) * BPS - 1));
1204        const uint16x8_t L6 = vmovl_u8(vld1_u8(dst + (i + 6) * BPS - 1));
1205        const uint16x8_t L7 = vmovl_u8(vld1_u8(dst + (i + 7) * BPS - 1));
1206        const uint16x8_t s0 = vaddq_u16(L0, L1);
1207        const uint16x8_t s1 = vaddq_u16(L2, L3);
1208        const uint16x8_t s2 = vaddq_u16(L4, L5);
1209        const uint16x8_t s3 = vaddq_u16(L6, L7);
1210        const uint16x8_t s01 = vaddq_u16(s0, s1);
1211        const uint16x8_t s23 = vaddq_u16(s2, s3);
1212        const uint16x8_t sum = vaddq_u16(s01, s23);
1213        sum_left = vaddq_u16(sum_left, sum);
1214      }
1215    }
1216    if (do_top && do_left) {
1217      const uint16x8_t sum = vaddq_u16(sum_left, sum_top);
1218      dc0 = vrshrn_n_u16(sum, 5);
1219    } else if (do_top) {
1220      dc0 = vrshrn_n_u16(sum_top, 4);
1221    } else if (do_left) {
1222      dc0 = vrshrn_n_u16(sum_left, 4);
1223    } else {
1224      dc0 = vdup_n_u8(0x80);
1225    }
1226    {
1227      const uint8x16_t dc = vdupq_lane_u8(dc0, 0);
1228      int i;
1229      for (i = 0; i < 16; ++i) {
1230        vst1q_u8(dst + i * BPS, dc);
1231      }
1232    }
1233  }
1234  static void DC16TopLeft_NEON(uint8_t* dst) { DC16_NEON(dst, 1, 1); }
1235  static void DC16NoTop_NEON(uint8_t* dst) { DC16_NEON(dst, 0, 1); }
1236  static void DC16NoLeft_NEON(uint8_t* dst) { DC16_NEON(dst, 1, 0); }
1237  static void DC16NoTopLeft_NEON(uint8_t* dst) { DC16_NEON(dst, 0, 0); }
1238  static void TM16_NEON(uint8_t* dst) {
1239    const uint8x8_t TL = vld1_dup_u8(dst - BPS - 1);  
1240    const uint8x16_t T = vld1q_u8(dst - BPS);  
1241    const int16x8_t d_lo = vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(T), TL));
1242    const int16x8_t d_hi = vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(T), TL));
1243    int y;
1244    for (y = 0; y < 16; y += 4) {
1245      const int16x8_t L0 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 0 * BPS - 1));
1246      const int16x8_t L1 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 1 * BPS - 1));
1247      const int16x8_t L2 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 2 * BPS - 1));
1248      const int16x8_t L3 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 3 * BPS - 1));
1249      const int16x8_t r0_lo = vaddq_s16(L0, d_lo);  
1250      const int16x8_t r1_lo = vaddq_s16(L1, d_lo);
1251      const int16x8_t r2_lo = vaddq_s16(L2, d_lo);
1252      const int16x8_t r3_lo = vaddq_s16(L3, d_lo);
1253      const int16x8_t r0_hi = vaddq_s16(L0, d_hi);
1254      const int16x8_t r1_hi = vaddq_s16(L1, d_hi);
1255      const int16x8_t r2_hi = vaddq_s16(L2, d_hi);
1256      const int16x8_t r3_hi = vaddq_s16(L3, d_hi);
1257      const uint8x16_t row0 = vcombine_u8(vqmovun_s16(r0_lo), vqmovun_s16(r0_hi));
1258      const uint8x16_t row1 = vcombine_u8(vqmovun_s16(r1_lo), vqmovun_s16(r1_hi));
1259      const uint8x16_t row2 = vcombine_u8(vqmovun_s16(r2_lo), vqmovun_s16(r2_hi));
1260      const uint8x16_t row3 = vcombine_u8(vqmovun_s16(r3_lo), vqmovun_s16(r3_hi));
1261      vst1q_u8(dst + 0 * BPS, row0);
1262      vst1q_u8(dst + 1 * BPS, row1);
1263      vst1q_u8(dst + 2 * BPS, row2);
1264      vst1q_u8(dst + 3 * BPS, row3);
1265      dst += 4 * BPS;
1266    }
1267  }
1268  extern void VP8DspInitNEON(void);
1269  WEBP_TSAN_IGNORE_FUNCTION void VP8DspInitNEON(void) {
1270    VP8Transform = TransformTwo_NEON;
1271    VP8TransformAC3 = TransformAC3_NEON;
1272    VP8TransformDC = TransformDC_NEON;
1273    VP8TransformWHT = TransformWHT_NEON;
1274    VP8VFilter16 = VFilter16_NEON;
1275    VP8VFilter16i = VFilter16i_NEON;
1276    VP8HFilter16 = HFilter16_NEON;
1277  #if !defined(WORK_AROUND_GCC)
1278    VP8HFilter16i = HFilter16i_NEON;
1279  #endif
1280    VP8VFilter8 = VFilter8_NEON;
1281    VP8VFilter8i = VFilter8i_NEON;
1282  #if !defined(WORK_AROUND_GCC)
1283    VP8HFilter8 = HFilter8_NEON;
1284    VP8HFilter8i = HFilter8i_NEON;
1285  #endif
1286    VP8SimpleVFilter16 = SimpleVFilter16_NEON;
1287    VP8SimpleHFilter16 = SimpleHFilter16_NEON;
1288    VP8SimpleVFilter16i = SimpleVFilter16i_NEON;
1289    VP8SimpleHFilter16i = SimpleHFilter16i_NEON;
1290    VP8PredLuma4[0] = DC4_NEON;
1291    VP8PredLuma4[1] = TM4_NEON;
1292    VP8PredLuma4[2] = VE4_NEON;
1293    VP8PredLuma4[4] = RD4_NEON;
1294    VP8PredLuma4[6] = LD4_NEON;
1295    VP8PredLuma16[0] = DC16TopLeft_NEON;
1296    VP8PredLuma16[1] = TM16_NEON;
1297    VP8PredLuma16[2] = VE16_NEON;
1298    VP8PredLuma16[3] = HE16_NEON;
1299    VP8PredLuma16[4] = DC16NoTop_NEON;
1300    VP8PredLuma16[5] = DC16NoLeft_NEON;
1301    VP8PredLuma16[6] = DC16NoTopLeft_NEON;
1302    VP8PredChroma8[0] = DC8uv_NEON;
1303    VP8PredChroma8[1] = TM8uv_NEON;
1304    VP8PredChroma8[2] = VE8uv_NEON;
1305    VP8PredChroma8[3] = HE8uv_NEON;
1306    VP8PredChroma8[4] = DC8uvNoTop_NEON;
1307    VP8PredChroma8[5] = DC8uvNoLeft_NEON;
1308    VP8PredChroma8[6] = DC8uvNoTopLeft_NEON;
1309  }
1310  #else  
1311  WEBP_DSP_INIT_STUB(VP8DspInitNEON)
1312  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-enc_neon.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-dec_neon.c</div>
                </div>
                <div class="column column_space"><pre><code>66    int16x8x2_t rows;
67    INIT_VECTOR2(rows, vld1q_s16(in + 0), vld1q_s16(in + 8));
68    TransformPass_NEON(&rows);
69    TransformPass_NEON(&rows);
70    Add4x4_NEON(rows.val[0], rows.val[1], ref, dst);
</pre></code></div>
                <div class="column column_space"><pre><code>841    int16x8x2_t rows;
842    INIT_VECTOR2(rows, vld1q_s16(in + 0), vld1q_s16(in + 8));
843    TransformPass_NEON(&rows);
844    TransformPass_NEON(&rows);
845    Add4x4_NEON(rows.val[0], rows.val[1], dst);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    