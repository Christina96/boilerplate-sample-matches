
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 55, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-prof.c</h3>
            <pre><code>1  #define JEMALLOC_PROF_C_
2  #include &quot;jemalloc/internal/jemalloc_preamble.h&quot;
3  #include &quot;jemalloc/internal/jemalloc_internal_includes.h&quot;
4  #include &quot;jemalloc/internal/assert.h&quot;
5  #include &quot;jemalloc/internal/ckh.h&quot;
6  #include &quot;jemalloc/internal/hash.h&quot;
7  #include &quot;jemalloc/internal/malloc_io.h&quot;
8  #include &quot;jemalloc/internal/mutex.h&quot;
9  #include &quot;jemalloc/internal/emitter.h&quot;
10  #ifdef JEMALLOC_PROF_LIBUNWIND
11  #define UNW_LOCAL_ONLY
12  #include &lt;libunwind.h&gt;
13  #endif
14  #ifdef JEMALLOC_PROF_LIBGCC
15  #undef _Unwind_Backtrace
16  #include &lt;unwind.h&gt;
17  #define _Unwind_Backtrace JEMALLOC_HOOK(_Unwind_Backtrace, test_hooks_libc_hook)
18  #endif
19  bool		opt_prof = false;
20  bool		opt_prof_active = true;
21  bool		opt_prof_thread_active_init = true;
22  size_t		opt_lg_prof_sample = LG_PROF_SAMPLE_DEFAULT;
23  ssize_t		opt_lg_prof_interval = LG_PROF_INTERVAL_DEFAULT;
24  bool		opt_prof_gdump = false;
25  bool		opt_prof_final = false;
26  bool		opt_prof_leak = false;
27  bool		opt_prof_accum = false;
28  bool		opt_prof_log = false;
29  char		opt_prof_prefix[
30  #ifdef JEMALLOC_PROF
31      PATH_MAX +
32  #endif
33      1];
34  bool			prof_active;
35  static malloc_mutex_t	prof_active_mtx;
36  static bool		prof_thread_active_init;
37  static malloc_mutex_t	prof_thread_active_init_mtx;
38  bool			prof_gdump_val;
39  static malloc_mutex_t	prof_gdump_mtx;
40  uint64_t	prof_interval = 0;
41  size_t		lg_prof_sample;
42  typedef enum prof_logging_state_e prof_logging_state_t;
43  enum prof_logging_state_e {
44  	prof_logging_state_stopped,
45  	prof_logging_state_started,
46  	prof_logging_state_dumping
47  };
48  prof_logging_state_t prof_logging_state = prof_logging_state_stopped;
49  #ifdef JEMALLOC_JET
50  static bool prof_log_dummy = false;
51  #endif
52  static uint64_t log_seq = 0;
53  static char log_filename[
54  #ifdef JEMALLOC_PROF
55      PATH_MAX +
56  #endif
57      1];
58  static nstime_t log_start_timestamp = NSTIME_ZERO_INITIALIZER;
59  static size_t log_bt_index = 0;
60  static size_t log_thr_index = 0;
61  typedef struct prof_bt_node_s prof_bt_node_t;
62  struct prof_bt_node_s {
63  	prof_bt_node_t *next;
64  	size_t index;
65  	prof_bt_t bt;
66  	void *vec[1];
67  };
68  typedef struct prof_thr_node_s prof_thr_node_t;
69  struct prof_thr_node_s {
70  	prof_thr_node_t *next;
71  	size_t index;
72  	uint64_t thr_uid;
73  	char name[1];
74  };
75  typedef struct prof_alloc_node_s prof_alloc_node_t;
76  struct prof_alloc_node_s {
77  	prof_alloc_node_t *next;
78  	size_t alloc_thr_ind;
79  	size_t free_thr_ind;
80  	size_t alloc_bt_ind;
81  	size_t free_bt_ind;
82  	uint64_t alloc_time_ns;
83  	uint64_t free_time_ns;
84  	size_t usize;
85  };
86  static bool log_tables_initialized = false;
87  static ckh_t log_bt_node_set;
88  static ckh_t log_thr_node_set;
89  static prof_bt_node_t *log_bt_first = NULL;
90  static prof_bt_node_t *log_bt_last = NULL;
91  static prof_thr_node_t *log_thr_first = NULL;
92  static prof_thr_node_t *log_thr_last = NULL;
93  static prof_alloc_node_t *log_alloc_first = NULL;
94  static prof_alloc_node_t *log_alloc_last = NULL;
95  static malloc_mutex_t log_mtx;
96  static malloc_mutex_t	*gctx_locks;
97  static atomic_u_t	cum_gctxs; &amp;bsol;* Atomic counter. */
98  static malloc_mutex_t	*tdata_locks;
99  static ckh_t		bt2gctx;
100  malloc_mutex_t		bt2gctx_mtx;
101  static prof_tdata_tree_t	tdatas;
102  static malloc_mutex_t	tdatas_mtx;
103  static uint64_t		next_thr_uid;
104  static malloc_mutex_t	next_thr_uid_mtx;
105  static malloc_mutex_t	prof_dump_seq_mtx;
106  static uint64_t		prof_dump_seq;
107  static uint64_t		prof_dump_iseq;
108  static uint64_t		prof_dump_mseq;
109  static uint64_t		prof_dump_useq;
110  static malloc_mutex_t	prof_dump_mtx;
111  static char		prof_dump_buf[
112  #ifdef JEMALLOC_PROF
113      PROF_DUMP_BUFSIZE
114  #else
115      1
116  #endif
117  ];
118  static size_t		prof_dump_buf_end;
119  static int		prof_dump_fd;
120  static bool		prof_booted = false;
121  static bool	prof_tctx_should_destroy(tsdn_t *tsdn, prof_tctx_t *tctx);
122  static void	prof_tctx_destroy(tsd_t *tsd, prof_tctx_t *tctx);
123  static bool	prof_tdata_should_destroy(tsdn_t *tsdn, prof_tdata_t *tdata,
124      bool even_if_attached);
125  static void	prof_tdata_destroy(tsd_t *tsd, prof_tdata_t *tdata,
126      bool even_if_attached);
127  static char	*prof_thread_name_alloc(tsdn_t *tsdn, const char *thread_name);
128  static void prof_thr_node_hash(const void *key, size_t r_hash[2]);
129  static bool prof_thr_node_keycomp(const void *k1, const void *k2);
130  static void prof_bt_node_hash(const void *key, size_t r_hash[2]);
131  static bool prof_bt_node_keycomp(const void *k1, const void *k2);
132  static int
133  prof_tctx_comp(const prof_tctx_t *a, const prof_tctx_t *b) {
134  	uint64_t a_thr_uid = a-&gt;thr_uid;
135  	uint64_t b_thr_uid = b-&gt;thr_uid;
136  	int ret = (a_thr_uid &gt; b_thr_uid) - (a_thr_uid &lt; b_thr_uid);
137  	if (ret == 0) {
138  		uint64_t a_thr_discrim = a-&gt;thr_discrim;
139  		uint64_t b_thr_discrim = b-&gt;thr_discrim;
140  		ret = (a_thr_discrim &gt; b_thr_discrim) - (a_thr_discrim &lt;
141  		    b_thr_discrim);
142  		if (ret == 0) {
143  			uint64_t a_tctx_uid = a-&gt;tctx_uid;
144  			uint64_t b_tctx_uid = b-&gt;tctx_uid;
145  			ret = (a_tctx_uid &gt; b_tctx_uid) - (a_tctx_uid &lt;
146  			    b_tctx_uid);
147  		}
148  	}
149  	return ret;
150  }
151  rb_gen(static UNUSED, tctx_tree_, prof_tctx_tree_t, prof_tctx_t,
152      tctx_link, prof_tctx_comp)
153  static int
154  prof_gctx_comp(const prof_gctx_t *a, const prof_gctx_t *b) {
155  	unsigned a_len = a-&gt;bt.len;
156  	unsigned b_len = b-&gt;bt.len;
157  	unsigned comp_len = (a_len &lt; b_len) ? a_len : b_len;
158  	int ret = memcmp(a-&gt;bt.vec, b-&gt;bt.vec, comp_len * sizeof(void *));
159  	if (ret == 0) {
160  		ret = (a_len &gt; b_len) - (a_len &lt; b_len);
161  	}
162  	return ret;
163  }
164  rb_gen(static UNUSED, gctx_tree_, prof_gctx_tree_t, prof_gctx_t, dump_link,
165      prof_gctx_comp)
166  static int
167  prof_tdata_comp(const prof_tdata_t *a, const prof_tdata_t *b) {
168  	int ret;
169  	uint64_t a_uid = a-&gt;thr_uid;
170  	uint64_t b_uid = b-&gt;thr_uid;
171  	ret = ((a_uid &gt; b_uid) - (a_uid &lt; b_uid));
172  	if (ret == 0) {
173  		uint64_t a_discrim = a-&gt;thr_discrim;
174  		uint64_t b_discrim = b-&gt;thr_discrim;
175  		ret = ((a_discrim &gt; b_discrim) - (a_discrim &lt; b_discrim));
176  	}
177  	return ret;
178  }
179  rb_gen(static UNUSED, tdata_tree_, prof_tdata_tree_t, prof_tdata_t, tdata_link,
180      prof_tdata_comp)
181  void
182  prof_alloc_rollback(tsd_t *tsd, prof_tctx_t *tctx, bool updated) {
183  	prof_tdata_t *tdata;
184  	cassert(config_prof);
185  	if (updated) {
186  		tdata = prof_tdata_get(tsd, true);
187  		if (tdata != NULL) {
188  			prof_sample_threshold_update(tdata);
189  		}
190  	}
191  	if ((uintptr_t)tctx &gt; (uintptr_t)1U) {
192  		malloc_mutex_lock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
193  		tctx-&gt;prepared = false;
194  		if (prof_tctx_should_destroy(tsd_tsdn(tsd), tctx)) {
195  			prof_tctx_destroy(tsd, tctx);
196  		} else {
197  			malloc_mutex_unlock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
198  		}
199  	}
200  }
201  void
202  prof_malloc_sample_object(tsdn_t *tsdn, const void *ptr, size_t usize,
203      prof_tctx_t *tctx) {
204  	prof_tctx_set(tsdn, ptr, usize, NULL, tctx);
205  	nstime_t t = NSTIME_ZERO_INITIALIZER;
206  	nstime_update(&amp;t);
207  	prof_alloc_time_set(tsdn, ptr, NULL, t);
208  	malloc_mutex_lock(tsdn, tctx-&gt;tdata-&gt;lock);
209  	tctx-&gt;cnts.curobjs++;
210  	tctx-&gt;cnts.curbytes += usize;
211  	if (opt_prof_accum) {
212  		tctx-&gt;cnts.accumobjs++;
213  		tctx-&gt;cnts.accumbytes += usize;
214  	}
215  	tctx-&gt;prepared = false;
216  	malloc_mutex_unlock(tsdn, tctx-&gt;tdata-&gt;lock);
217  }
218  static size_t
219  prof_log_bt_index(tsd_t *tsd, prof_bt_t *bt) {
220  	assert(prof_logging_state == prof_logging_state_started);
221  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;log_mtx);
222  	prof_bt_node_t dummy_node;
223  	dummy_node.bt = *bt;
224  	prof_bt_node_t *node;
225  	if (ckh_search(&amp;log_bt_node_set, (void *)(&amp;dummy_node),
226  	    (void **)(&amp;node), NULL)) {
227  		size_t sz = offsetof(prof_bt_node_t, vec) +
228  			        (bt-&gt;len * sizeof(void *));
229  		prof_bt_node_t *new_node = (prof_bt_node_t *)
230  		    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL,
<span onclick='openModal()' class='match'>231  		    true, arena_get(TSDN_NULL, 0, true), true);
232  		if (log_bt_first == NULL) {
233  			log_bt_first = new_node;
234  			log_bt_last = new_node;
235  		} else {
236  			log_bt_last-&gt;next = new_node;
237  			log_bt_last = new_node;
238  		}
239  		new_node-&gt;next = NULL;
240  		new_node-&gt;index = log_bt_index;
241  		new_node-&gt;bt.len = bt-&gt;len;
</span>242  		memcpy(new_node-&gt;vec, bt-&gt;vec, bt-&gt;len * sizeof(void *));
243  		new_node-&gt;bt.vec = new_node-&gt;vec;
244  		log_bt_index++;
245  		ckh_insert(tsd, &amp;log_bt_node_set, (void *)new_node, NULL);
246  		return new_node-&gt;index;
247  	} else {
248  		return node-&gt;index;
249  	}
250  }
251  static size_t
252  prof_log_thr_index(tsd_t *tsd, uint64_t thr_uid, const char *name) {
253  	assert(prof_logging_state == prof_logging_state_started);
254  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;log_mtx);
255  	prof_thr_node_t dummy_node;
256  	dummy_node.thr_uid = thr_uid;
257  	prof_thr_node_t *node;
258  	if (ckh_search(&amp;log_thr_node_set, (void *)(&amp;dummy_node),
259  	    (void **)(&amp;node), NULL)) {
260  		size_t sz = offsetof(prof_thr_node_t, name) + strlen(name) + 1;
261  		prof_thr_node_t *new_node = (prof_thr_node_t *)
262  		    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL,
263  		    true, arena_get(TSDN_NULL, 0, true), true);
264  		if (log_thr_first == NULL) {
265  			log_thr_first = new_node;
266  			log_thr_last = new_node;
267  		} else {
268  			log_thr_last-&gt;next = new_node;
269  			log_thr_last = new_node;
270  		}
271  		new_node-&gt;next = NULL;
272  		new_node-&gt;index = log_thr_index;
273  		new_node-&gt;thr_uid = thr_uid;
274  		strcpy(new_node-&gt;name, name);
275  		log_thr_index++;
276  		ckh_insert(tsd, &amp;log_thr_node_set, (void *)new_node, NULL);
277  		return new_node-&gt;index;
278  	} else {
279  		return node-&gt;index;
280  	}
281  }
282  static void
283  prof_try_log(tsd_t *tsd, const void *ptr, size_t usize, prof_tctx_t *tctx) {
284  	malloc_mutex_assert_owner(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
285  	prof_tdata_t *cons_tdata = prof_tdata_get(tsd, false);
286  	if (cons_tdata == NULL) {
287  		return;
288  	}
289  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;log_mtx);
290  	if (prof_logging_state != prof_logging_state_started) {
291  		goto label_done;
292  	}
293  	if (!log_tables_initialized) {
294  		bool err1 = ckh_new(tsd, &amp;log_bt_node_set, PROF_CKH_MINITEMS,
295  				prof_bt_node_hash, prof_bt_node_keycomp);
296  		bool err2 = ckh_new(tsd, &amp;log_thr_node_set, PROF_CKH_MINITEMS,
297  				prof_thr_node_hash, prof_thr_node_keycomp);
298  		if (err1 || err2) {
299  			goto label_done;
300  		}
301  		log_tables_initialized = true;
302  	}
303  	nstime_t alloc_time = prof_alloc_time_get(tsd_tsdn(tsd), ptr,
304  			          (alloc_ctx_t *)NULL);
305  	nstime_t free_time = NSTIME_ZERO_INITIALIZER;
306  	nstime_update(&amp;free_time);
307  	size_t sz = sizeof(prof_alloc_node_t);
308  	prof_alloc_node_t *new_node = (prof_alloc_node_t *)
309  	    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL, true,
310  	    arena_get(TSDN_NULL, 0, true), true);
311  	const char *prod_thr_name = (tctx-&gt;tdata-&gt;thread_name == NULL)?
312  				        &quot;&quot; : tctx-&gt;tdata-&gt;thread_name;
313  	const char *cons_thr_name = prof_thread_name_get(tsd);
314  	prof_bt_t bt;
315  	bt_init(&amp;bt, cons_tdata-&gt;vec);
316  	prof_backtrace(&amp;bt);
317  	prof_bt_t *cons_bt = &amp;bt;
318  	prof_bt_t *prod_bt = &amp;tctx-&gt;gctx-&gt;bt;
319  	new_node-&gt;next = NULL;
320  	new_node-&gt;alloc_thr_ind = prof_log_thr_index(tsd, tctx-&gt;tdata-&gt;thr_uid,
321  				      prod_thr_name);
322  	new_node-&gt;free_thr_ind = prof_log_thr_index(tsd, cons_tdata-&gt;thr_uid,
323  				     cons_thr_name);
324  	new_node-&gt;alloc_bt_ind = prof_log_bt_index(tsd, prod_bt);
325  	new_node-&gt;free_bt_ind = prof_log_bt_index(tsd, cons_bt);
326  	new_node-&gt;alloc_time_ns = nstime_ns(&amp;alloc_time);
327  	new_node-&gt;free_time_ns = nstime_ns(&amp;free_time);
328  	new_node-&gt;usize = usize;
329  	if (log_alloc_first == NULL) {
330  		log_alloc_first = new_node;
331  		log_alloc_last = new_node;
332  	} else {
333  		log_alloc_last-&gt;next = new_node;
334  		log_alloc_last = new_node;
335  	}
336  label_done:
337  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;log_mtx);
338  }
339  void
340  prof_free_sampled_object(tsd_t *tsd, const void *ptr, size_t usize,
341      prof_tctx_t *tctx) {
342  	malloc_mutex_lock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
343  	assert(tctx-&gt;cnts.curobjs &gt; 0);
344  	assert(tctx-&gt;cnts.curbytes &gt;= usize);
345  	tctx-&gt;cnts.curobjs--;
346  	tctx-&gt;cnts.curbytes -= usize;
347  	prof_try_log(tsd, ptr, usize, tctx);
348  	if (prof_tctx_should_destroy(tsd_tsdn(tsd), tctx)) {
349  		prof_tctx_destroy(tsd, tctx);
350  	} else {
351  		malloc_mutex_unlock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
352  	}
353  }
354  void
355  bt_init(prof_bt_t *bt, void **vec) {
356  	cassert(config_prof);
357  	bt-&gt;vec = vec;
358  	bt-&gt;len = 0;
359  }
360  static void
361  prof_enter(tsd_t *tsd, prof_tdata_t *tdata) {
362  	cassert(config_prof);
363  	assert(tdata == prof_tdata_get(tsd, false));
364  	if (tdata != NULL) {
365  		assert(!tdata-&gt;enq);
366  		tdata-&gt;enq = true;
367  	}
368  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
369  }
370  static void
371  prof_leave(tsd_t *tsd, prof_tdata_t *tdata) {
372  	cassert(config_prof);
373  	assert(tdata == prof_tdata_get(tsd, false));
374  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
375  	if (tdata != NULL) {
376  		bool idump, gdump;
377  		assert(tdata-&gt;enq);
378  		tdata-&gt;enq = false;
379  		idump = tdata-&gt;enq_idump;
380  		tdata-&gt;enq_idump = false;
381  		gdump = tdata-&gt;enq_gdump;
382  		tdata-&gt;enq_gdump = false;
383  		if (idump) {
384  			prof_idump(tsd_tsdn(tsd));
385  		}
386  		if (gdump) {
387  			prof_gdump(tsd_tsdn(tsd));
388  		}
389  	}
390  }
391  #ifdef JEMALLOC_PROF_LIBUNWIND
392  void
393  prof_backtrace(prof_bt_t *bt) {
394  	int nframes;
395  	cassert(config_prof);
396  	assert(bt-&gt;len == 0);
397  	assert(bt-&gt;vec != NULL);
398  	nframes = unw_backtrace(bt-&gt;vec, PROF_BT_MAX);
399  	if (nframes &lt;= 0) {
400  		return;
401  	}
402  	bt-&gt;len = nframes;
403  }
404  #elif (defined(JEMALLOC_PROF_LIBGCC))
405  static _Unwind_Reason_Code
406  prof_unwind_init_callback(struct _Unwind_Context *context, void *arg) {
407  	cassert(config_prof);
408  	return _URC_NO_REASON;
409  }
410  static _Unwind_Reason_Code
411  prof_unwind_callback(struct _Unwind_Context *context, void *arg) {
412  	prof_unwind_data_t *data = (prof_unwind_data_t *)arg;
413  	void *ip;
414  	cassert(config_prof);
415  	ip = (void *)_Unwind_GetIP(context);
416  	if (ip == NULL) {
417  		return _URC_END_OF_STACK;
418  	}
419  	data-&gt;bt-&gt;vec[data-&gt;bt-&gt;len] = ip;
420  	data-&gt;bt-&gt;len++;
421  	if (data-&gt;bt-&gt;len == data-&gt;max) {
422  		return _URC_END_OF_STACK;
423  	}
424  	return _URC_NO_REASON;
425  }
426  void
427  prof_backtrace(prof_bt_t *bt) {
428  	prof_unwind_data_t data = {bt, PROF_BT_MAX};
429  	cassert(config_prof);
430  	_Unwind_Backtrace(prof_unwind_callback, &amp;data);
431  }
432  #elif (defined(JEMALLOC_PROF_GCC))
433  void
434  prof_backtrace(prof_bt_t *bt) {
435  #define BT_FRAME(i)							\
436  	if ((i) &lt; PROF_BT_MAX) {					\
437  		void *p;						\
438  		if (__builtin_frame_address(i) == 0) {			\
439  			return;						\
440  		}							\
441  		p = __builtin_return_address(i);			\
442  		if (p == NULL) {					\
443  			return;						\
444  		}							\
445  		bt-&gt;vec[(i)] = p;					\
446  		bt-&gt;len = (i) + 1;					\
447  	} else {							\
448  		return;							\
449  	}
450  	cassert(config_prof);
451  	BT_FRAME(0)
452  	BT_FRAME(1)
453  	BT_FRAME(2)
454  	BT_FRAME(3)
455  	BT_FRAME(4)
456  	BT_FRAME(5)
457  	BT_FRAME(6)
458  	BT_FRAME(7)
459  	BT_FRAME(8)
460  	BT_FRAME(9)
461  	BT_FRAME(10)
462  	BT_FRAME(11)
463  	BT_FRAME(12)
464  	BT_FRAME(13)
465  	BT_FRAME(14)
466  	BT_FRAME(15)
467  	BT_FRAME(16)
468  	BT_FRAME(17)
469  	BT_FRAME(18)
470  	BT_FRAME(19)
471  	BT_FRAME(20)
472  	BT_FRAME(21)
473  	BT_FRAME(22)
474  	BT_FRAME(23)
475  	BT_FRAME(24)
476  	BT_FRAME(25)
477  	BT_FRAME(26)
478  	BT_FRAME(27)
479  	BT_FRAME(28)
480  	BT_FRAME(29)
481  	BT_FRAME(30)
482  	BT_FRAME(31)
483  	BT_FRAME(32)
484  	BT_FRAME(33)
485  	BT_FRAME(34)
486  	BT_FRAME(35)
487  	BT_FRAME(36)
488  	BT_FRAME(37)
489  	BT_FRAME(38)
490  	BT_FRAME(39)
491  	BT_FRAME(40)
492  	BT_FRAME(41)
493  	BT_FRAME(42)
494  	BT_FRAME(43)
495  	BT_FRAME(44)
496  	BT_FRAME(45)
497  	BT_FRAME(46)
498  	BT_FRAME(47)
499  	BT_FRAME(48)
500  	BT_FRAME(49)
501  	BT_FRAME(50)
502  	BT_FRAME(51)
503  	BT_FRAME(52)
504  	BT_FRAME(53)
505  	BT_FRAME(54)
506  	BT_FRAME(55)
507  	BT_FRAME(56)
508  	BT_FRAME(57)
509  	BT_FRAME(58)
510  	BT_FRAME(59)
511  	BT_FRAME(60)
512  	BT_FRAME(61)
513  	BT_FRAME(62)
514  	BT_FRAME(63)
515  	BT_FRAME(64)
516  	BT_FRAME(65)
517  	BT_FRAME(66)
518  	BT_FRAME(67)
519  	BT_FRAME(68)
520  	BT_FRAME(69)
521  	BT_FRAME(70)
522  	BT_FRAME(71)
523  	BT_FRAME(72)
524  	BT_FRAME(73)
525  	BT_FRAME(74)
526  	BT_FRAME(75)
527  	BT_FRAME(76)
528  	BT_FRAME(77)
529  	BT_FRAME(78)
530  	BT_FRAME(79)
531  	BT_FRAME(80)
532  	BT_FRAME(81)
533  	BT_FRAME(82)
534  	BT_FRAME(83)
535  	BT_FRAME(84)
536  	BT_FRAME(85)
537  	BT_FRAME(86)
538  	BT_FRAME(87)
539  	BT_FRAME(88)
540  	BT_FRAME(89)
541  	BT_FRAME(90)
542  	BT_FRAME(91)
543  	BT_FRAME(92)
544  	BT_FRAME(93)
545  	BT_FRAME(94)
546  	BT_FRAME(95)
547  	BT_FRAME(96)
548  	BT_FRAME(97)
549  	BT_FRAME(98)
550  	BT_FRAME(99)
551  	BT_FRAME(100)
552  	BT_FRAME(101)
553  	BT_FRAME(102)
554  	BT_FRAME(103)
555  	BT_FRAME(104)
556  	BT_FRAME(105)
557  	BT_FRAME(106)
558  	BT_FRAME(107)
559  	BT_FRAME(108)
560  	BT_FRAME(109)
561  	BT_FRAME(110)
562  	BT_FRAME(111)
563  	BT_FRAME(112)
564  	BT_FRAME(113)
565  	BT_FRAME(114)
566  	BT_FRAME(115)
567  	BT_FRAME(116)
568  	BT_FRAME(117)
569  	BT_FRAME(118)
570  	BT_FRAME(119)
571  	BT_FRAME(120)
572  	BT_FRAME(121)
573  	BT_FRAME(122)
574  	BT_FRAME(123)
575  	BT_FRAME(124)
576  	BT_FRAME(125)
577  	BT_FRAME(126)
578  	BT_FRAME(127)
579  #undef BT_FRAME
580  }
581  #else
582  void
583  prof_backtrace(prof_bt_t *bt) {
584  	cassert(config_prof);
585  	not_reached();
586  }
587  #endif
588  static malloc_mutex_t *
589  prof_gctx_mutex_choose(void) {
590  	unsigned ngctxs = atomic_fetch_add_u(&amp;cum_gctxs, 1, ATOMIC_RELAXED);
591  	return &amp;gctx_locks[(ngctxs - 1) % PROF_NCTX_LOCKS];
592  }
593  static malloc_mutex_t *
594  prof_tdata_mutex_choose(uint64_t thr_uid) {
595  	return &amp;tdata_locks[thr_uid % PROF_NTDATA_LOCKS];
596  }
597  static prof_gctx_t *
598  prof_gctx_create(tsdn_t *tsdn, prof_bt_t *bt) {
599  	size_t size = offsetof(prof_gctx_t, vec) + (bt-&gt;len * sizeof(void *));
600  	prof_gctx_t *gctx = (prof_gctx_t *)iallocztm(tsdn, size,
601  	    sz_size2index(size), false, NULL, true, arena_get(TSDN_NULL, 0, true),
602  	    true);
603  	if (gctx == NULL) {
604  		return NULL;
605  	}
606  	gctx-&gt;lock = prof_gctx_mutex_choose();
607  	gctx-&gt;nlimbo = 1;
608  	tctx_tree_new(&amp;gctx-&gt;tctxs);
609  	memcpy(gctx-&gt;vec, bt-&gt;vec, bt-&gt;len * sizeof(void *));
610  	gctx-&gt;bt.vec = gctx-&gt;vec;
611  	gctx-&gt;bt.len = bt-&gt;len;
612  	return gctx;
613  }
614  static void
615  prof_gctx_try_destroy(tsd_t *tsd, prof_tdata_t *tdata_self, prof_gctx_t *gctx,
616      prof_tdata_t *tdata) {
617  	cassert(config_prof);
618  	prof_enter(tsd, tdata_self);
619  	malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
620  	assert(gctx-&gt;nlimbo != 0);
621  	if (tctx_tree_empty(&amp;gctx-&gt;tctxs) &amp;&amp; gctx-&gt;nlimbo == 1) {
622  		if (ckh_remove(tsd, &amp;bt2gctx, &amp;gctx-&gt;bt, NULL, NULL)) {
623  			not_reached();
624  		}
625  		prof_leave(tsd, tdata_self);
626  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
627  		idalloctm(tsd_tsdn(tsd), gctx, NULL, NULL, true, true);
628  	} else {
629  		gctx-&gt;nlimbo--;
630  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
631  		prof_leave(tsd, tdata_self);
632  	}
633  }
634  static bool
635  prof_tctx_should_destroy(tsdn_t *tsdn, prof_tctx_t *tctx) {
636  	malloc_mutex_assert_owner(tsdn, tctx-&gt;tdata-&gt;lock);
637  	if (opt_prof_accum) {
638  		return false;
639  	}
640  	if (tctx-&gt;cnts.curobjs != 0) {
641  		return false;
642  	}
643  	if (tctx-&gt;prepared) {
644  		return false;
645  	}
646  	return true;
647  }
648  static bool
649  prof_gctx_should_destroy(prof_gctx_t *gctx) {
650  	if (opt_prof_accum) {
651  		return false;
652  	}
653  	if (!tctx_tree_empty(&amp;gctx-&gt;tctxs)) {
654  		return false;
655  	}
656  	if (gctx-&gt;nlimbo != 0) {
657  		return false;
658  	}
659  	return true;
660  }
661  static void
662  prof_tctx_destroy(tsd_t *tsd, prof_tctx_t *tctx) {
663  	prof_tdata_t *tdata = tctx-&gt;tdata;
664  	prof_gctx_t *gctx = tctx-&gt;gctx;
665  	bool destroy_tdata, destroy_tctx, destroy_gctx;
666  	malloc_mutex_assert_owner(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
667  	assert(tctx-&gt;cnts.curobjs == 0);
668  	assert(tctx-&gt;cnts.curbytes == 0);
669  	assert(!opt_prof_accum);
670  	assert(tctx-&gt;cnts.accumobjs == 0);
671  	assert(tctx-&gt;cnts.accumbytes == 0);
672  	ckh_remove(tsd, &amp;tdata-&gt;bt2tctx, &amp;gctx-&gt;bt, NULL, NULL);
673  	destroy_tdata = prof_tdata_should_destroy(tsd_tsdn(tsd), tdata, false);
674  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
675  	malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
676  	switch (tctx-&gt;state) {
677  	case prof_tctx_state_nominal:
678  		tctx_tree_remove(&amp;gctx-&gt;tctxs, tctx);
679  		destroy_tctx = true;
680  		if (prof_gctx_should_destroy(gctx)) {
681  			gctx-&gt;nlimbo++;
682  			destroy_gctx = true;
683  		} else {
684  			destroy_gctx = false;
685  		}
686  		break;
687  	case prof_tctx_state_dumping:
688  		tctx-&gt;state = prof_tctx_state_purgatory;
689  		destroy_tctx = false;
690  		destroy_gctx = false;
691  		break;
692  	default:
693  		not_reached();
694  		destroy_tctx = false;
695  		destroy_gctx = false;
696  	}
697  	malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
698  	if (destroy_gctx) {
699  		prof_gctx_try_destroy(tsd, prof_tdata_get(tsd, false), gctx,
700  		    tdata);
701  	}
702  	malloc_mutex_assert_not_owner(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
703  	if (destroy_tdata) {
704  		prof_tdata_destroy(tsd, tdata, false);
705  	}
706  	if (destroy_tctx) {
707  		idalloctm(tsd_tsdn(tsd), tctx, NULL, NULL, true, true);
708  	}
709  }
710  static bool
711  prof_lookup_global(tsd_t *tsd, prof_bt_t *bt, prof_tdata_t *tdata,
712      void **p_btkey, prof_gctx_t **p_gctx, bool *p_new_gctx) {
713  	union {
714  		prof_gctx_t	*p;
715  		void		*v;
716  	} gctx, tgctx;
717  	union {
718  		prof_bt_t	*p;
719  		void		*v;
720  	} btkey;
721  	bool new_gctx;
722  	prof_enter(tsd, tdata);
723  	if (ckh_search(&amp;bt2gctx, bt, &amp;btkey.v, &amp;gctx.v)) {
724  		prof_leave(tsd, tdata);
725  		tgctx.p = prof_gctx_create(tsd_tsdn(tsd), bt);
726  		if (tgctx.v == NULL) {
727  			return true;
728  		}
729  		prof_enter(tsd, tdata);
730  		if (ckh_search(&amp;bt2gctx, bt, &amp;btkey.v, &amp;gctx.v)) {
731  			gctx.p = tgctx.p;
732  			btkey.p = &amp;gctx.p-&gt;bt;
733  			if (ckh_insert(tsd, &amp;bt2gctx, btkey.v, gctx.v)) {
734  				prof_leave(tsd, tdata);
735  				idalloctm(tsd_tsdn(tsd), gctx.v, NULL, NULL,
736  				    true, true);
737  				return true;
738  			}
739  			new_gctx = true;
740  		} else {
741  			new_gctx = false;
742  		}
743  	} else {
744  		tgctx.v = NULL;
745  		new_gctx = false;
746  	}
747  	if (!new_gctx) {
748  		malloc_mutex_lock(tsd_tsdn(tsd), gctx.p-&gt;lock);
749  		gctx.p-&gt;nlimbo++;
750  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx.p-&gt;lock);
751  		new_gctx = false;
752  		if (tgctx.v != NULL) {
753  			idalloctm(tsd_tsdn(tsd), tgctx.v, NULL, NULL, true,
754  			    true);
755  		}
756  	}
757  	prof_leave(tsd, tdata);
758  	*p_btkey = btkey.v;
759  	*p_gctx = gctx.p;
760  	*p_new_gctx = new_gctx;
761  	return false;
762  }
763  prof_tctx_t *
764  prof_lookup(tsd_t *tsd, prof_bt_t *bt) {
765  	union {
766  		prof_tctx_t	*p;
767  		void		*v;
768  	} ret;
769  	prof_tdata_t *tdata;
770  	bool not_found;
771  	cassert(config_prof);
772  	tdata = prof_tdata_get(tsd, false);
773  	if (tdata == NULL) {
774  		return NULL;
775  	}
776  	malloc_mutex_lock(tsd_tsdn(tsd), tdata-&gt;lock);
777  	not_found = ckh_search(&amp;tdata-&gt;bt2tctx, bt, NULL, &amp;ret.v);
778  	if (!not_found) { &amp;bsol;* Note double negative! */
779  		ret.p-&gt;prepared = true;
780  	}
781  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
782  	if (not_found) {
783  		void *btkey;
784  		prof_gctx_t *gctx;
785  		bool new_gctx, error;
786  		if (prof_lookup_global(tsd, bt, tdata, &amp;btkey, &amp;gctx,
787  		    &amp;new_gctx)) {
788  			return NULL;
789  		}
790  		ret.v = iallocztm(tsd_tsdn(tsd), sizeof(prof_tctx_t),
791  		    sz_size2index(sizeof(prof_tctx_t)), false, NULL, true,
792  		    arena_ichoose(tsd, NULL), true);
793  		if (ret.p == NULL) {
794  			if (new_gctx) {
795  				prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
796  			}
797  			return NULL;
798  		}
799  		ret.p-&gt;tdata = tdata;
800  		ret.p-&gt;thr_uid = tdata-&gt;thr_uid;
801  		ret.p-&gt;thr_discrim = tdata-&gt;thr_discrim;
802  		memset(&amp;ret.p-&gt;cnts, 0, sizeof(prof_cnt_t));
803  		ret.p-&gt;gctx = gctx;
804  		ret.p-&gt;tctx_uid = tdata-&gt;tctx_uid_next++;
805  		ret.p-&gt;prepared = true;
806  		ret.p-&gt;state = prof_tctx_state_initializing;
807  		malloc_mutex_lock(tsd_tsdn(tsd), tdata-&gt;lock);
808  		error = ckh_insert(tsd, &amp;tdata-&gt;bt2tctx, btkey, ret.v);
809  		malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
810  		if (error) {
811  			if (new_gctx) {
812  				prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
813  			}
814  			idalloctm(tsd_tsdn(tsd), ret.v, NULL, NULL, true, true);
815  			return NULL;
816  		}
817  		malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
818  		ret.p-&gt;state = prof_tctx_state_nominal;
819  		tctx_tree_insert(&amp;gctx-&gt;tctxs, ret.p);
820  		gctx-&gt;nlimbo--;
821  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
822  	}
823  	return ret.p;
824  }
825  void
826  prof_sample_threshold_update(prof_tdata_t *tdata) {
827  #ifdef JEMALLOC_PROF
828  	if (!config_prof) {
829  		return;
830  	}
831  	if (lg_prof_sample == 0) {
832  		tsd_bytes_until_sample_set(tsd_fetch(), 0);
833  		return;
834  	}
835  	uint64_t r = prng_lg_range_u64(&amp;tdata-&gt;prng_state, 53);
836  	double u = (double)r * (1.0/9007199254740992.0L);
837  	uint64_t bytes_until_sample = (uint64_t)(log(u) /
838  	    log(1.0 - (1.0 / (double)((uint64_t)1U &lt;&lt; lg_prof_sample))))
839  	    + (uint64_t)1U;
840  	if (bytes_until_sample &gt; SSIZE_MAX) {
841  		bytes_until_sample = SSIZE_MAX;
842  	}
843  	tsd_bytes_until_sample_set(tsd_fetch(), bytes_until_sample);
844  #endif
845  }
846  #ifdef JEMALLOC_JET
847  static prof_tdata_t *
848  prof_tdata_count_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
849      void *arg) {
850  	size_t *tdata_count = (size_t *)arg;
851  	(*tdata_count)++;
852  	return NULL;
853  }
854  size_t
855  prof_tdata_count(void) {
856  	size_t tdata_count = 0;
857  	tsdn_t *tsdn;
858  	tsdn = tsdn_fetch();
859  	malloc_mutex_lock(tsdn, &amp;tdatas_mtx);
860  	tdata_tree_iter(&amp;tdatas, NULL, prof_tdata_count_iter,
861  	    (void *)&amp;tdata_count);
862  	malloc_mutex_unlock(tsdn, &amp;tdatas_mtx);
863  	return tdata_count;
864  }
865  size_t
866  prof_bt_count(void) {
867  	size_t bt_count;
868  	tsd_t *tsd;
869  	prof_tdata_t *tdata;
870  	tsd = tsd_fetch();
871  	tdata = prof_tdata_get(tsd, false);
872  	if (tdata == NULL) {
873  		return 0;
874  	}
875  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
876  	bt_count = ckh_count(&amp;bt2gctx);
877  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
878  	return bt_count;
879  }
880  #endif
881  static int
882  prof_dump_open_impl(bool propagate_err, const char *filename) {
883  	int fd;
884  	fd = creat(filename, 0644);
885  	if (fd == -1 &amp;&amp; !propagate_err) {
886  		malloc_printf(&quot;&lt;jemalloc&gt;: creat(\&quot;%s\&quot;), 0644) failed\n&quot;,
887  		    filename);
888  		if (opt_abort) {
889  			abort();
890  		}
891  	}
892  	return fd;
893  }
894  prof_dump_open_t *JET_MUTABLE prof_dump_open = prof_dump_open_impl;
895  static bool
896  prof_dump_flush(bool propagate_err) {
897  	bool ret = false;
898  	ssize_t err;
899  	cassert(config_prof);
900  	err = malloc_write_fd(prof_dump_fd, prof_dump_buf, prof_dump_buf_end);
901  	if (err == -1) {
902  		if (!propagate_err) {
903  			malloc_write(&quot;&lt;jemalloc&gt;: write() failed during heap &quot;
904  			    &quot;profile flush\n&quot;);
905  			if (opt_abort) {
906  				abort();
907  			}
908  		}
909  		ret = true;
910  	}
911  	prof_dump_buf_end = 0;
912  	return ret;
913  }
914  static bool
915  prof_dump_close(bool propagate_err) {
916  	bool ret;
917  	assert(prof_dump_fd != -1);
918  	ret = prof_dump_flush(propagate_err);
919  	close(prof_dump_fd);
920  	prof_dump_fd = -1;
921  	return ret;
922  }
923  static bool
924  prof_dump_write(bool propagate_err, const char *s) {
925  	size_t i, slen, n;
926  	cassert(config_prof);
927  	i = 0;
928  	slen = strlen(s);
929  	while (i &lt; slen) {
930  		if (prof_dump_buf_end == PROF_DUMP_BUFSIZE) {
931  			if (prof_dump_flush(propagate_err) &amp;&amp; propagate_err) {
932  				return true;
933  			}
934  		}
935  		if (prof_dump_buf_end + slen - i &lt;= PROF_DUMP_BUFSIZE) {
936  			n = slen - i;
937  		} else {
938  			n = PROF_DUMP_BUFSIZE - prof_dump_buf_end;
939  		}
940  		memcpy(&amp;prof_dump_buf[prof_dump_buf_end], &amp;s[i], n);
941  		prof_dump_buf_end += n;
942  		i += n;
943  	}
944  	assert(i == slen);
945  	return false;
946  }
947  JEMALLOC_FORMAT_PRINTF(2, 3)
948  static bool
949  prof_dump_printf(bool propagate_err, const char *format, ...) {
950  	bool ret;
951  	va_list ap;
952  	char buf[PROF_PRINTF_BUFSIZE];
953  	va_start(ap, format);
954  	malloc_vsnprintf(buf, sizeof(buf), format, ap);
955  	va_end(ap);
956  	ret = prof_dump_write(propagate_err, buf);
957  	return ret;
958  }
959  static void
960  prof_tctx_merge_tdata(tsdn_t *tsdn, prof_tctx_t *tctx, prof_tdata_t *tdata) {
961  	malloc_mutex_assert_owner(tsdn, tctx-&gt;tdata-&gt;lock);
962  	malloc_mutex_lock(tsdn, tctx-&gt;gctx-&gt;lock);
963  	switch (tctx-&gt;state) {
964  	case prof_tctx_state_initializing:
965  		malloc_mutex_unlock(tsdn, tctx-&gt;gctx-&gt;lock);
966  		return;
967  	case prof_tctx_state_nominal:
968  		tctx-&gt;state = prof_tctx_state_dumping;
969  		malloc_mutex_unlock(tsdn, tctx-&gt;gctx-&gt;lock);
970  		memcpy(&amp;tctx-&gt;dump_cnts, &amp;tctx-&gt;cnts, sizeof(prof_cnt_t));
971  		tdata-&gt;cnt_summed.curobjs += tctx-&gt;dump_cnts.curobjs;
972  		tdata-&gt;cnt_summed.curbytes += tctx-&gt;dump_cnts.curbytes;
973  		if (opt_prof_accum) {
974  			tdata-&gt;cnt_summed.accumobjs +=
975  			    tctx-&gt;dump_cnts.accumobjs;
976  			tdata-&gt;cnt_summed.accumbytes +=
977  			    tctx-&gt;dump_cnts.accumbytes;
978  		}
979  		break;
980  	case prof_tctx_state_dumping:
981  	case prof_tctx_state_purgatory:
982  		not_reached();
983  	}
984  }
985  static void
986  prof_tctx_merge_gctx(tsdn_t *tsdn, prof_tctx_t *tctx, prof_gctx_t *gctx) {
987  	malloc_mutex_assert_owner(tsdn, gctx-&gt;lock);
988  	gctx-&gt;cnt_summed.curobjs += tctx-&gt;dump_cnts.curobjs;
989  	gctx-&gt;cnt_summed.curbytes += tctx-&gt;dump_cnts.curbytes;
990  	if (opt_prof_accum) {
991  		gctx-&gt;cnt_summed.accumobjs += tctx-&gt;dump_cnts.accumobjs;
992  		gctx-&gt;cnt_summed.accumbytes += tctx-&gt;dump_cnts.accumbytes;
993  	}
994  }
995  static prof_tctx_t *
996  prof_tctx_merge_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *arg) {
997  	tsdn_t *tsdn = (tsdn_t *)arg;
998  	malloc_mutex_assert_owner(tsdn, tctx-&gt;gctx-&gt;lock);
999  	switch (tctx-&gt;state) {
1000  	case prof_tctx_state_nominal:
1001  		break;
1002  	case prof_tctx_state_dumping:
1003  	case prof_tctx_state_purgatory:
1004  		prof_tctx_merge_gctx(tsdn, tctx, tctx-&gt;gctx);
1005  		break;
1006  	default:
1007  		not_reached();
1008  	}
1009  	return NULL;
1010  }
1011  struct prof_tctx_dump_iter_arg_s {
1012  	tsdn_t	*tsdn;
1013  	bool	propagate_err;
1014  };
1015  static prof_tctx_t *
1016  prof_tctx_dump_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *opaque) {
1017  	struct prof_tctx_dump_iter_arg_s *arg =
1018  	    (struct prof_tctx_dump_iter_arg_s *)opaque;
1019  	malloc_mutex_assert_owner(arg-&gt;tsdn, tctx-&gt;gctx-&gt;lock);
1020  	switch (tctx-&gt;state) {
1021  	case prof_tctx_state_initializing:
1022  	case prof_tctx_state_nominal:
1023  		break;
1024  	case prof_tctx_state_dumping:
1025  	case prof_tctx_state_purgatory:
1026  		if (prof_dump_printf(arg-&gt;propagate_err,
1027  		    &quot;  t%&quot;FMTu64&quot;: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: &quot;
1028  		    &quot;%&quot;FMTu64&quot;]\n&quot;, tctx-&gt;thr_uid, tctx-&gt;dump_cnts.curobjs,
1029  		    tctx-&gt;dump_cnts.curbytes, tctx-&gt;dump_cnts.accumobjs,
1030  		    tctx-&gt;dump_cnts.accumbytes)) {
1031  			return tctx;
1032  		}
1033  		break;
1034  	default:
1035  		not_reached();
1036  	}
1037  	return NULL;
1038  }
1039  static prof_tctx_t *
1040  prof_tctx_finish_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *arg) {
1041  	tsdn_t *tsdn = (tsdn_t *)arg;
1042  	prof_tctx_t *ret;
1043  	malloc_mutex_assert_owner(tsdn, tctx-&gt;gctx-&gt;lock);
1044  	switch (tctx-&gt;state) {
1045  	case prof_tctx_state_nominal:
1046  		break;
1047  	case prof_tctx_state_dumping:
1048  		tctx-&gt;state = prof_tctx_state_nominal;
1049  		break;
1050  	case prof_tctx_state_purgatory:
1051  		ret = tctx;
1052  		goto label_return;
1053  	default:
1054  		not_reached();
1055  	}
1056  	ret = NULL;
1057  label_return:
1058  	return ret;
1059  }
1060  static void
1061  prof_dump_gctx_prep(tsdn_t *tsdn, prof_gctx_t *gctx, prof_gctx_tree_t *gctxs) {
1062  	cassert(config_prof);
1063  	malloc_mutex_lock(tsdn, gctx-&gt;lock);
1064  	gctx-&gt;nlimbo++;
1065  	gctx_tree_insert(gctxs, gctx);
1066  	memset(&amp;gctx-&gt;cnt_summed, 0, sizeof(prof_cnt_t));
1067  	malloc_mutex_unlock(tsdn, gctx-&gt;lock);
1068  }
1069  struct prof_gctx_merge_iter_arg_s {
1070  	tsdn_t	*tsdn;
1071  	size_t	leak_ngctx;
1072  };
1073  static prof_gctx_t *
1074  prof_gctx_merge_iter(prof_gctx_tree_t *gctxs, prof_gctx_t *gctx, void *opaque) {
1075  	struct prof_gctx_merge_iter_arg_s *arg =
1076  	    (struct prof_gctx_merge_iter_arg_s *)opaque;
1077  	malloc_mutex_lock(arg-&gt;tsdn, gctx-&gt;lock);
1078  	tctx_tree_iter(&amp;gctx-&gt;tctxs, NULL, prof_tctx_merge_iter,
1079  	    (void *)arg-&gt;tsdn);
1080  	if (gctx-&gt;cnt_summed.curobjs != 0) {
1081  		arg-&gt;leak_ngctx++;
1082  	}
1083  	malloc_mutex_unlock(arg-&gt;tsdn, gctx-&gt;lock);
1084  	return NULL;
1085  }
1086  static void
1087  prof_gctx_finish(tsd_t *tsd, prof_gctx_tree_t *gctxs) {
1088  	prof_tdata_t *tdata = prof_tdata_get(tsd, false);
1089  	prof_gctx_t *gctx;
1090  	while ((gctx = gctx_tree_first(gctxs)) != NULL) {
1091  		gctx_tree_remove(gctxs, gctx);
1092  		malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
1093  		{
1094  			prof_tctx_t *next;
1095  			next = NULL;
1096  			do {
1097  				prof_tctx_t *to_destroy =
1098  				    tctx_tree_iter(&amp;gctx-&gt;tctxs, next,
1099  				    prof_tctx_finish_iter,
1100  				    (void *)tsd_tsdn(tsd));
1101  				if (to_destroy != NULL) {
1102  					next = tctx_tree_next(&amp;gctx-&gt;tctxs,
1103  					    to_destroy);
1104  					tctx_tree_remove(&amp;gctx-&gt;tctxs,
1105  					    to_destroy);
1106  					idalloctm(tsd_tsdn(tsd), to_destroy,
1107  					    NULL, NULL, true, true);
1108  				} else {
1109  					next = NULL;
1110  				}
1111  			} while (next != NULL);
1112  		}
1113  		gctx-&gt;nlimbo--;
1114  		if (prof_gctx_should_destroy(gctx)) {
1115  			gctx-&gt;nlimbo++;
1116  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
1117  			prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
1118  		} else {
1119  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
1120  		}
1121  	}
1122  }
1123  struct prof_tdata_merge_iter_arg_s {
1124  	tsdn_t		*tsdn;
1125  	prof_cnt_t	cnt_all;
1126  };
1127  static prof_tdata_t *
1128  prof_tdata_merge_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1129      void *opaque) {
1130  	struct prof_tdata_merge_iter_arg_s *arg =
1131  	    (struct prof_tdata_merge_iter_arg_s *)opaque;
1132  	malloc_mutex_lock(arg-&gt;tsdn, tdata-&gt;lock);
1133  	if (!tdata-&gt;expired) {
1134  		size_t tabind;
1135  		union {
1136  			prof_tctx_t	*p;
1137  			void		*v;
1138  		} tctx;
1139  		tdata-&gt;dumping = true;
1140  		memset(&amp;tdata-&gt;cnt_summed, 0, sizeof(prof_cnt_t));
1141  		for (tabind = 0; !ckh_iter(&amp;tdata-&gt;bt2tctx, &amp;tabind, NULL,
1142  		    &amp;tctx.v);) {
1143  			prof_tctx_merge_tdata(arg-&gt;tsdn, tctx.p, tdata);
1144  		}
1145  		arg-&gt;cnt_all.curobjs += tdata-&gt;cnt_summed.curobjs;
1146  		arg-&gt;cnt_all.curbytes += tdata-&gt;cnt_summed.curbytes;
1147  		if (opt_prof_accum) {
1148  			arg-&gt;cnt_all.accumobjs += tdata-&gt;cnt_summed.accumobjs;
1149  			arg-&gt;cnt_all.accumbytes += tdata-&gt;cnt_summed.accumbytes;
1150  		}
1151  	} else {
1152  		tdata-&gt;dumping = false;
1153  	}
1154  	malloc_mutex_unlock(arg-&gt;tsdn, tdata-&gt;lock);
1155  	return NULL;
1156  }
1157  static prof_tdata_t *
1158  prof_tdata_dump_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1159      void *arg) {
1160  	bool propagate_err = *(bool *)arg;
1161  	if (!tdata-&gt;dumping) {
1162  		return NULL;
1163  	}
1164  	if (prof_dump_printf(propagate_err,
1165  	    &quot;  t%&quot;FMTu64&quot;: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: %&quot;FMTu64&quot;]%s%s\n&quot;,
1166  	    tdata-&gt;thr_uid, tdata-&gt;cnt_summed.curobjs,
1167  	    tdata-&gt;cnt_summed.curbytes, tdata-&gt;cnt_summed.accumobjs,
1168  	    tdata-&gt;cnt_summed.accumbytes,
1169  	    (tdata-&gt;thread_name != NULL) ? &quot; &quot; : &quot;&quot;,
1170  	    (tdata-&gt;thread_name != NULL) ? tdata-&gt;thread_name : &quot;&quot;)) {
1171  		return tdata;
1172  	}
1173  	return NULL;
1174  }
1175  static bool
1176  prof_dump_header_impl(tsdn_t *tsdn, bool propagate_err,
1177      const prof_cnt_t *cnt_all) {
1178  	bool ret;
1179  	if (prof_dump_printf(propagate_err,
1180  	    &quot;heap_v2/%&quot;FMTu64&quot;\n&quot;
1181  	    &quot;  t*: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: %&quot;FMTu64&quot;]\n&quot;,
1182  	    ((uint64_t)1U &lt;&lt; lg_prof_sample), cnt_all-&gt;curobjs,
1183  	    cnt_all-&gt;curbytes, cnt_all-&gt;accumobjs, cnt_all-&gt;accumbytes)) {
1184  		return true;
1185  	}
1186  	malloc_mutex_lock(tsdn, &amp;tdatas_mtx);
1187  	ret = (tdata_tree_iter(&amp;tdatas, NULL, prof_tdata_dump_iter,
1188  	    (void *)&amp;propagate_err) != NULL);
1189  	malloc_mutex_unlock(tsdn, &amp;tdatas_mtx);
1190  	return ret;
1191  }
1192  prof_dump_header_t *JET_MUTABLE prof_dump_header = prof_dump_header_impl;
1193  static bool
1194  prof_dump_gctx(tsdn_t *tsdn, bool propagate_err, prof_gctx_t *gctx,
1195      const prof_bt_t *bt, prof_gctx_tree_t *gctxs) {
1196  	bool ret;
1197  	unsigned i;
1198  	struct prof_tctx_dump_iter_arg_s prof_tctx_dump_iter_arg;
1199  	cassert(config_prof);
1200  	malloc_mutex_assert_owner(tsdn, gctx-&gt;lock);
1201  	if ((!opt_prof_accum &amp;&amp; gctx-&gt;cnt_summed.curobjs == 0) ||
1202  	    (opt_prof_accum &amp;&amp; gctx-&gt;cnt_summed.accumobjs == 0)) {
1203  		assert(gctx-&gt;cnt_summed.curobjs == 0);
1204  		assert(gctx-&gt;cnt_summed.curbytes == 0);
1205  		assert(gctx-&gt;cnt_summed.accumobjs == 0);
1206  		assert(gctx-&gt;cnt_summed.accumbytes == 0);
1207  		ret = false;
1208  		goto label_return;
1209  	}
1210  	if (prof_dump_printf(propagate_err, &quot;@&quot;)) {
1211  		ret = true;
1212  		goto label_return;
1213  	}
1214  	for (i = 0; i &lt; bt-&gt;len; i++) {
1215  		if (prof_dump_printf(propagate_err, &quot; %#&quot;FMTxPTR,
1216  		    (uintptr_t)bt-&gt;vec[i])) {
1217  			ret = true;
1218  			goto label_return;
1219  		}
1220  	}
1221  	if (prof_dump_printf(propagate_err,
1222  	    &quot;\n&quot;
1223  	    &quot;  t*: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: %&quot;FMTu64&quot;]\n&quot;,
1224  	    gctx-&gt;cnt_summed.curobjs, gctx-&gt;cnt_summed.curbytes,
1225  	    gctx-&gt;cnt_summed.accumobjs, gctx-&gt;cnt_summed.accumbytes)) {
1226  		ret = true;
1227  		goto label_return;
1228  	}
1229  	prof_tctx_dump_iter_arg.tsdn = tsdn;
1230  	prof_tctx_dump_iter_arg.propagate_err = propagate_err;
1231  	if (tctx_tree_iter(&amp;gctx-&gt;tctxs, NULL, prof_tctx_dump_iter,
1232  	    (void *)&amp;prof_tctx_dump_iter_arg) != NULL) {
1233  		ret = true;
1234  		goto label_return;
1235  	}
1236  	ret = false;
1237  label_return:
1238  	return ret;
1239  }
1240  #ifndef _WIN32
1241  JEMALLOC_FORMAT_PRINTF(1, 2)
1242  static int
1243  prof_open_maps(const char *format, ...) {
1244  	int mfd;
1245  	va_list ap;
1246  	char filename[PATH_MAX + 1];
1247  	va_start(ap, format);
1248  	malloc_vsnprintf(filename, sizeof(filename), format, ap);
1249  	va_end(ap);
1250  #if defined(O_CLOEXEC)
1251  	mfd = open(filename, O_RDONLY | O_CLOEXEC);
1252  #else
1253  	mfd = open(filename, O_RDONLY);
1254  	if (mfd != -1) {
1255  		fcntl(mfd, F_SETFD, fcntl(mfd, F_GETFD) | FD_CLOEXEC);
1256  	}
1257  #endif
1258  	return mfd;
1259  }
1260  #endif
1261  static int
1262  prof_getpid(void) {
1263  #ifdef _WIN32
1264  	return GetCurrentProcessId();
1265  #else
1266  	return getpid();
1267  #endif
1268  }
1269  static bool
1270  prof_dump_maps(bool propagate_err) {
1271  	bool ret;
1272  	int mfd;
1273  	cassert(config_prof);
1274  #ifdef __FreeBSD__
1275  	mfd = prof_open_maps(&quot;/proc/curproc/map&quot;);
1276  #elif defined(_WIN32)
1277  	mfd = -1; 
1278  #else
1279  	{
1280  		int pid = prof_getpid();
1281  		mfd = prof_open_maps(&quot;/proc/%d/task/%d/maps&quot;, pid, pid);
1282  		if (mfd == -1) {
1283  			mfd = prof_open_maps(&quot;/proc/%d/maps&quot;, pid);
1284  		}
1285  	}
1286  #endif
1287  	if (mfd != -1) {
1288  		ssize_t nread;
1289  		if (prof_dump_write(propagate_err, &quot;\nMAPPED_LIBRARIES:\n&quot;) &amp;&amp;
1290  		    propagate_err) {
1291  			ret = true;
1292  			goto label_return;
1293  		}
1294  		nread = 0;
1295  		do {
1296  			prof_dump_buf_end += nread;
1297  			if (prof_dump_buf_end == PROF_DUMP_BUFSIZE) {
1298  				if (prof_dump_flush(propagate_err) &amp;&amp;
1299  				    propagate_err) {
1300  					ret = true;
1301  					goto label_return;
1302  				}
1303  			}
1304  			nread = malloc_read_fd(mfd,
1305  			    &amp;prof_dump_buf[prof_dump_buf_end], PROF_DUMP_BUFSIZE
1306  			    - prof_dump_buf_end);
1307  		} while (nread &gt; 0);
1308  	} else {
1309  		ret = true;
1310  		goto label_return;
1311  	}
1312  	ret = false;
1313  label_return:
1314  	if (mfd != -1) {
1315  		close(mfd);
1316  	}
1317  	return ret;
1318  }
1319  static void
1320  prof_leakcheck(const prof_cnt_t *cnt_all, size_t leak_ngctx,
1321      const char *filename) {
1322  #ifdef JEMALLOC_PROF
1323  	if (cnt_all-&gt;curbytes != 0) {
1324  		double sample_period = (double)((uint64_t)1 &lt;&lt; lg_prof_sample);
1325  		double ratio = (((double)cnt_all-&gt;curbytes) /
1326  		    (double)cnt_all-&gt;curobjs) / sample_period;
1327  		double scale_factor = 1.0 / (1.0 - exp(-ratio));
1328  		uint64_t curbytes = (uint64_t)round(((double)cnt_all-&gt;curbytes)
1329  		    * scale_factor);
1330  		uint64_t curobjs = (uint64_t)round(((double)cnt_all-&gt;curobjs) *
1331  		    scale_factor);
1332  		malloc_printf(&quot;&lt;jemalloc&gt;: Leak approximation summary: ~%&quot;FMTu64
1333  		    &quot; byte%s, ~%&quot;FMTu64&quot; object%s, &gt;= %zu context%s\n&quot;,
1334  		    curbytes, (curbytes != 1) ? &quot;s&quot; : &quot;&quot;, curobjs, (curobjs !=
1335  		    1) ? &quot;s&quot; : &quot;&quot;, leak_ngctx, (leak_ngctx != 1) ? &quot;s&quot; : &quot;&quot;);
1336  		malloc_printf(
1337  		    &quot;&lt;jemalloc&gt;: Run jeprof on \&quot;%s\&quot; for leak detail\n&quot;,
1338  		    filename);
1339  	}
1340  #endif
1341  }
1342  struct prof_gctx_dump_iter_arg_s {
1343  	tsdn_t	*tsdn;
1344  	bool	propagate_err;
1345  };
1346  static prof_gctx_t *
1347  prof_gctx_dump_iter(prof_gctx_tree_t *gctxs, prof_gctx_t *gctx, void *opaque) {
1348  	prof_gctx_t *ret;
1349  	struct prof_gctx_dump_iter_arg_s *arg =
1350  	    (struct prof_gctx_dump_iter_arg_s *)opaque;
1351  	malloc_mutex_lock(arg-&gt;tsdn, gctx-&gt;lock);
1352  	if (prof_dump_gctx(arg-&gt;tsdn, arg-&gt;propagate_err, gctx, &amp;gctx-&gt;bt,
1353  	    gctxs)) {
1354  		ret = gctx;
1355  		goto label_return;
1356  	}
1357  	ret = NULL;
1358  label_return:
1359  	malloc_mutex_unlock(arg-&gt;tsdn, gctx-&gt;lock);
1360  	return ret;
1361  }
1362  static void
1363  prof_dump_prep(tsd_t *tsd, prof_tdata_t *tdata,
1364      struct prof_tdata_merge_iter_arg_s *prof_tdata_merge_iter_arg,
1365      struct prof_gctx_merge_iter_arg_s *prof_gctx_merge_iter_arg,
1366      prof_gctx_tree_t *gctxs) {
1367  	size_t tabind;
1368  	union {
1369  		prof_gctx_t	*p;
1370  		void		*v;
1371  	} gctx;
1372  	prof_enter(tsd, tdata);
1373  	gctx_tree_new(gctxs);
1374  	for (tabind = 0; !ckh_iter(&amp;bt2gctx, &amp;tabind, NULL, &amp;gctx.v);) {
1375  		prof_dump_gctx_prep(tsd_tsdn(tsd), gctx.p, gctxs);
1376  	}
1377  	prof_tdata_merge_iter_arg-&gt;tsdn = tsd_tsdn(tsd);
1378  	memset(&amp;prof_tdata_merge_iter_arg-&gt;cnt_all, 0, sizeof(prof_cnt_t));
1379  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1380  	tdata_tree_iter(&amp;tdatas, NULL, prof_tdata_merge_iter,
1381  	    (void *)prof_tdata_merge_iter_arg);
1382  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1383  	prof_gctx_merge_iter_arg-&gt;tsdn = tsd_tsdn(tsd);
1384  	prof_gctx_merge_iter_arg-&gt;leak_ngctx = 0;
1385  	gctx_tree_iter(gctxs, NULL, prof_gctx_merge_iter,
1386  	    (void *)prof_gctx_merge_iter_arg);
1387  	prof_leave(tsd, tdata);
1388  }
1389  static bool
1390  prof_dump_file(tsd_t *tsd, bool propagate_err, const char *filename,
1391      bool leakcheck, prof_tdata_t *tdata,
1392      struct prof_tdata_merge_iter_arg_s *prof_tdata_merge_iter_arg,
1393      struct prof_gctx_merge_iter_arg_s *prof_gctx_merge_iter_arg,
1394      struct prof_gctx_dump_iter_arg_s *prof_gctx_dump_iter_arg,
1395      prof_gctx_tree_t *gctxs) {
1396  	if ((prof_dump_fd = prof_dump_open(propagate_err, filename)) == -1) {
1397  		return true;
1398  	}
1399  	if (prof_dump_header(tsd_tsdn(tsd), propagate_err,
1400  	    &amp;prof_tdata_merge_iter_arg-&gt;cnt_all)) {
1401  		goto label_write_error;
1402  	}
1403  	prof_gctx_dump_iter_arg-&gt;tsdn = tsd_tsdn(tsd);
1404  	prof_gctx_dump_iter_arg-&gt;propagate_err = propagate_err;
1405  	if (gctx_tree_iter(gctxs, NULL, prof_gctx_dump_iter,
1406  	    (void *)prof_gctx_dump_iter_arg) != NULL) {
1407  		goto label_write_error;
1408  	}
1409  	if (prof_dump_maps(propagate_err)) {
1410  		goto label_write_error;
1411  	}
1412  	if (prof_dump_close(propagate_err)) {
1413  		return true;
1414  	}
1415  	return false;
1416  label_write_error:
1417  	prof_dump_close(propagate_err);
1418  	return true;
1419  }
1420  static bool
1421  prof_dump(tsd_t *tsd, bool propagate_err, const char *filename,
1422      bool leakcheck) {
1423  	cassert(config_prof);
1424  	assert(tsd_reentrancy_level_get(tsd) == 0);
1425  	prof_tdata_t * tdata = prof_tdata_get(tsd, true);
1426  	if (tdata == NULL) {
1427  		return true;
1428  	}
1429  	pre_reentrancy(tsd, NULL);
1430  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1431  	prof_gctx_tree_t gctxs;
1432  	struct prof_tdata_merge_iter_arg_s prof_tdata_merge_iter_arg;
1433  	struct prof_gctx_merge_iter_arg_s prof_gctx_merge_iter_arg;
1434  	struct prof_gctx_dump_iter_arg_s prof_gctx_dump_iter_arg;
1435  	prof_dump_prep(tsd, tdata, &amp;prof_tdata_merge_iter_arg,
1436  	    &amp;prof_gctx_merge_iter_arg, &amp;gctxs);
1437  	bool err = prof_dump_file(tsd, propagate_err, filename, leakcheck, tdata,
1438  	    &amp;prof_tdata_merge_iter_arg, &amp;prof_gctx_merge_iter_arg,
1439  	    &amp;prof_gctx_dump_iter_arg, &amp;gctxs);
1440  	prof_gctx_finish(tsd, &amp;gctxs);
1441  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1442  	post_reentrancy(tsd);
1443  	if (err) {
1444  		return true;
1445  	}
1446  	if (leakcheck) {
1447  		prof_leakcheck(&amp;prof_tdata_merge_iter_arg.cnt_all,
1448  		    prof_gctx_merge_iter_arg.leak_ngctx, filename);
1449  	}
1450  	return false;
1451  }
1452  #ifdef JEMALLOC_JET
1453  void
1454  prof_cnt_all(uint64_t *curobjs, uint64_t *curbytes, uint64_t *accumobjs,
1455      uint64_t *accumbytes) {
1456  	tsd_t *tsd;
1457  	prof_tdata_t *tdata;
1458  	struct prof_tdata_merge_iter_arg_s prof_tdata_merge_iter_arg;
1459  	struct prof_gctx_merge_iter_arg_s prof_gctx_merge_iter_arg;
1460  	prof_gctx_tree_t gctxs;
1461  	tsd = tsd_fetch();
1462  	tdata = prof_tdata_get(tsd, false);
1463  	if (tdata == NULL) {
1464  		if (curobjs != NULL) {
1465  			*curobjs = 0;
1466  		}
1467  		if (curbytes != NULL) {
1468  			*curbytes = 0;
1469  		}
1470  		if (accumobjs != NULL) {
1471  			*accumobjs = 0;
1472  		}
1473  		if (accumbytes != NULL) {
1474  			*accumbytes = 0;
1475  		}
1476  		return;
1477  	}
1478  	prof_dump_prep(tsd, tdata, &amp;prof_tdata_merge_iter_arg,
1479  	    &amp;prof_gctx_merge_iter_arg, &amp;gctxs);
1480  	prof_gctx_finish(tsd, &amp;gctxs);
1481  	if (curobjs != NULL) {
1482  		*curobjs = prof_tdata_merge_iter_arg.cnt_all.curobjs;
1483  	}
1484  	if (curbytes != NULL) {
1485  		*curbytes = prof_tdata_merge_iter_arg.cnt_all.curbytes;
1486  	}
1487  	if (accumobjs != NULL) {
1488  		*accumobjs = prof_tdata_merge_iter_arg.cnt_all.accumobjs;
1489  	}
1490  	if (accumbytes != NULL) {
1491  		*accumbytes = prof_tdata_merge_iter_arg.cnt_all.accumbytes;
1492  	}
1493  }
1494  #endif
1495  #define DUMP_FILENAME_BUFSIZE	(PATH_MAX + 1)
1496  #define VSEQ_INVALID		UINT64_C(0xffffffffffffffff)
1497  static void
1498  prof_dump_filename(char *filename, char v, uint64_t vseq) {
1499  	cassert(config_prof);
1500  	if (vseq != VSEQ_INVALID) {
1501  		malloc_snprintf(filename, DUMP_FILENAME_BUFSIZE,
1502  		    &quot;%s.%d.%&quot;FMTu64&quot;.%c%&quot;FMTu64&quot;.heap&quot;,
1503  		    opt_prof_prefix, prof_getpid(), prof_dump_seq, v, vseq);
1504  	} else {
1505  		malloc_snprintf(filename, DUMP_FILENAME_BUFSIZE,
1506  		    &quot;%s.%d.%&quot;FMTu64&quot;.%c.heap&quot;,
1507  		    opt_prof_prefix, prof_getpid(), prof_dump_seq, v);
1508  	}
1509  	prof_dump_seq++;
1510  }
1511  static void
1512  prof_fdump(void) {
1513  	tsd_t *tsd;
1514  	char filename[DUMP_FILENAME_BUFSIZE];
1515  	cassert(config_prof);
1516  	assert(opt_prof_final);
1517  	assert(opt_prof_prefix[0] != &#x27;\0&#x27;);
1518  	if (!prof_booted) {
1519  		return;
1520  	}
1521  	tsd = tsd_fetch();
1522  	assert(tsd_reentrancy_level_get(tsd) == 0);
1523  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1524  	prof_dump_filename(filename, &#x27;f&#x27;, VSEQ_INVALID);
1525  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1526  	prof_dump(tsd, false, filename, opt_prof_leak);
1527  }
1528  bool
1529  prof_accum_init(tsdn_t *tsdn, prof_accum_t *prof_accum) {
1530  	cassert(config_prof);
1531  #ifndef JEMALLOC_ATOMIC_U64
1532  	if (malloc_mutex_init(&amp;prof_accum-&gt;mtx, &quot;prof_accum&quot;,
1533  	    WITNESS_RANK_PROF_ACCUM, malloc_mutex_rank_exclusive)) {
1534  		return true;
1535  	}
1536  	prof_accum-&gt;accumbytes = 0;
1537  #else
1538  	atomic_store_u64(&amp;prof_accum-&gt;accumbytes, 0, ATOMIC_RELAXED);
1539  #endif
1540  	return false;
1541  }
1542  void
1543  prof_idump(tsdn_t *tsdn) {
1544  	tsd_t *tsd;
1545  	prof_tdata_t *tdata;
1546  	cassert(config_prof);
1547  	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
1548  		return;
1549  	}
1550  	tsd = tsdn_tsd(tsdn);
1551  	if (tsd_reentrancy_level_get(tsd) &gt; 0) {
1552  		return;
1553  	}
1554  	tdata = prof_tdata_get(tsd, false);
1555  	if (tdata == NULL) {
1556  		return;
1557  	}
1558  	if (tdata-&gt;enq) {
1559  		tdata-&gt;enq_idump = true;
1560  		return;
1561  	}
1562  	if (opt_prof_prefix[0] != &#x27;\0&#x27;) {
1563  		char filename[PATH_MAX + 1];
1564  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1565  		prof_dump_filename(filename, &#x27;i&#x27;, prof_dump_iseq);
1566  		prof_dump_iseq++;
1567  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1568  		prof_dump(tsd, false, filename, false);
1569  	}
1570  }
1571  bool
1572  prof_mdump(tsd_t *tsd, const char *filename) {
1573  	cassert(config_prof);
1574  	assert(tsd_reentrancy_level_get(tsd) == 0);
1575  	if (!opt_prof || !prof_booted) {
1576  		return true;
1577  	}
1578  	char filename_buf[DUMP_FILENAME_BUFSIZE];
1579  	if (filename == NULL) {
1580  		if (opt_prof_prefix[0] == &#x27;\0&#x27;) {
1581  			return true;
1582  		}
1583  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1584  		prof_dump_filename(filename_buf, &#x27;m&#x27;, prof_dump_mseq);
1585  		prof_dump_mseq++;
1586  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1587  		filename = filename_buf;
1588  	}
1589  	return prof_dump(tsd, true, filename, false);
1590  }
1591  void
1592  prof_gdump(tsdn_t *tsdn) {
1593  	tsd_t *tsd;
1594  	prof_tdata_t *tdata;
1595  	cassert(config_prof);
1596  	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
1597  		return;
1598  	}
1599  	tsd = tsdn_tsd(tsdn);
1600  	if (tsd_reentrancy_level_get(tsd) &gt; 0) {
1601  		return;
1602  	}
1603  	tdata = prof_tdata_get(tsd, false);
1604  	if (tdata == NULL) {
1605  		return;
1606  	}
1607  	if (tdata-&gt;enq) {
1608  		tdata-&gt;enq_gdump = true;
1609  		return;
1610  	}
1611  	if (opt_prof_prefix[0] != &#x27;\0&#x27;) {
1612  		char filename[DUMP_FILENAME_BUFSIZE];
1613  		malloc_mutex_lock(tsdn, &amp;prof_dump_seq_mtx);
1614  		prof_dump_filename(filename, &#x27;u&#x27;, prof_dump_useq);
1615  		prof_dump_useq++;
1616  		malloc_mutex_unlock(tsdn, &amp;prof_dump_seq_mtx);
1617  		prof_dump(tsd, false, filename, false);
1618  	}
1619  }
1620  static void
1621  prof_bt_hash(const void *key, size_t r_hash[2]) {
1622  	prof_bt_t *bt = (prof_bt_t *)key;
1623  	cassert(config_prof);
1624  	hash(bt-&gt;vec, bt-&gt;len * sizeof(void *), 0x94122f33U, r_hash);
1625  }
1626  static bool
1627  prof_bt_keycomp(const void *k1, const void *k2) {
1628  	const prof_bt_t *bt1 = (prof_bt_t *)k1;
1629  	const prof_bt_t *bt2 = (prof_bt_t *)k2;
1630  	cassert(config_prof);
1631  	if (bt1-&gt;len != bt2-&gt;len) {
1632  		return false;
1633  	}
1634  	return (memcmp(bt1-&gt;vec, bt2-&gt;vec, bt1-&gt;len * sizeof(void *)) == 0);
1635  }
1636  static void
1637  prof_bt_node_hash(const void *key, size_t r_hash[2]) {
1638  	const prof_bt_node_t *bt_node = (prof_bt_node_t *)key;
1639  	prof_bt_hash((void *)(&amp;bt_node-&gt;bt), r_hash);
1640  }
1641  static bool
1642  prof_bt_node_keycomp(const void *k1, const void *k2) {
1643  	const prof_bt_node_t *bt_node1 = (prof_bt_node_t *)k1;
1644  	const prof_bt_node_t *bt_node2 = (prof_bt_node_t *)k2;
1645  	return prof_bt_keycomp((void *)(&amp;bt_node1-&gt;bt),
1646  	    (void *)(&amp;bt_node2-&gt;bt));
1647  }
1648  static void
1649  prof_thr_node_hash(const void *key, size_t r_hash[2]) {
1650  	const prof_thr_node_t *thr_node = (prof_thr_node_t *)key;
1651  	hash(&amp;thr_node-&gt;thr_uid, sizeof(uint64_t), 0x94122f35U, r_hash);
1652  }
1653  static bool
1654  prof_thr_node_keycomp(const void *k1, const void *k2) {
1655  	const prof_thr_node_t *thr_node1 = (prof_thr_node_t *)k1;
1656  	const prof_thr_node_t *thr_node2 = (prof_thr_node_t *)k2;
1657  	return thr_node1-&gt;thr_uid == thr_node2-&gt;thr_uid;
1658  }
1659  static uint64_t
1660  prof_thr_uid_alloc(tsdn_t *tsdn) {
1661  	uint64_t thr_uid;
1662  	malloc_mutex_lock(tsdn, &amp;next_thr_uid_mtx);
1663  	thr_uid = next_thr_uid;
1664  	next_thr_uid++;
1665  	malloc_mutex_unlock(tsdn, &amp;next_thr_uid_mtx);
1666  	return thr_uid;
1667  }
1668  static prof_tdata_t *
1669  prof_tdata_init_impl(tsd_t *tsd, uint64_t thr_uid, uint64_t thr_discrim,
1670      char *thread_name, bool active) {
1671  	prof_tdata_t *tdata;
1672  	cassert(config_prof);
1673  	tdata = (prof_tdata_t *)iallocztm(tsd_tsdn(tsd), sizeof(prof_tdata_t),
1674  	    sz_size2index(sizeof(prof_tdata_t)), false, NULL, true,
1675  	    arena_get(TSDN_NULL, 0, true), true);
1676  	if (tdata == NULL) {
1677  		return NULL;
1678  	}
1679  	tdata-&gt;lock = prof_tdata_mutex_choose(thr_uid);
1680  	tdata-&gt;thr_uid = thr_uid;
1681  	tdata-&gt;thr_discrim = thr_discrim;
1682  	tdata-&gt;thread_name = thread_name;
1683  	tdata-&gt;attached = true;
1684  	tdata-&gt;expired = false;
1685  	tdata-&gt;tctx_uid_next = 0;
1686  	if (ckh_new(tsd, &amp;tdata-&gt;bt2tctx, PROF_CKH_MINITEMS, prof_bt_hash,
1687  	    prof_bt_keycomp)) {
1688  		idalloctm(tsd_tsdn(tsd), tdata, NULL, NULL, true, true);
1689  		return NULL;
1690  	}
1691  	tdata-&gt;prng_state = (uint64_t)(uintptr_t)tdata;
1692  	prof_sample_threshold_update(tdata);
1693  	tdata-&gt;enq = false;
1694  	tdata-&gt;enq_idump = false;
1695  	tdata-&gt;enq_gdump = false;
1696  	tdata-&gt;dumping = false;
1697  	tdata-&gt;active = active;
1698  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1699  	tdata_tree_insert(&amp;tdatas, tdata);
1700  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1701  	return tdata;
1702  }
1703  prof_tdata_t *
1704  prof_tdata_init(tsd_t *tsd) {
1705  	return prof_tdata_init_impl(tsd, prof_thr_uid_alloc(tsd_tsdn(tsd)), 0,
1706  	    NULL, prof_thread_active_init_get(tsd_tsdn(tsd)));
1707  }
1708  static bool
1709  prof_tdata_should_destroy_unlocked(prof_tdata_t *tdata, bool even_if_attached) {
1710  	if (tdata-&gt;attached &amp;&amp; !even_if_attached) {
1711  		return false;
1712  	}
1713  	if (ckh_count(&amp;tdata-&gt;bt2tctx) != 0) {
1714  		return false;
1715  	}
1716  	return true;
1717  }
1718  static bool
1719  prof_tdata_should_destroy(tsdn_t *tsdn, prof_tdata_t *tdata,
1720      bool even_if_attached) {
1721  	malloc_mutex_assert_owner(tsdn, tdata-&gt;lock);
1722  	return prof_tdata_should_destroy_unlocked(tdata, even_if_attached);
1723  }
1724  static void
1725  prof_tdata_destroy_locked(tsd_t *tsd, prof_tdata_t *tdata,
1726      bool even_if_attached) {
1727  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;tdatas_mtx);
1728  	tdata_tree_remove(&amp;tdatas, tdata);
1729  	assert(prof_tdata_should_destroy_unlocked(tdata, even_if_attached));
1730  	if (tdata-&gt;thread_name != NULL) {
1731  		idalloctm(tsd_tsdn(tsd), tdata-&gt;thread_name, NULL, NULL, true,
1732  		    true);
1733  	}
1734  	ckh_delete(tsd, &amp;tdata-&gt;bt2tctx);
1735  	idalloctm(tsd_tsdn(tsd), tdata, NULL, NULL, true, true);
1736  }
1737  static void
1738  prof_tdata_destroy(tsd_t *tsd, prof_tdata_t *tdata, bool even_if_attached) {
1739  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1740  	prof_tdata_destroy_locked(tsd, tdata, even_if_attached);
1741  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1742  }
1743  static void
1744  prof_tdata_detach(tsd_t *tsd, prof_tdata_t *tdata) {
1745  	bool destroy_tdata;
1746  	malloc_mutex_lock(tsd_tsdn(tsd), tdata-&gt;lock);
1747  	if (tdata-&gt;attached) {
1748  		destroy_tdata = prof_tdata_should_destroy(tsd_tsdn(tsd), tdata,
1749  		    true);
1750  		if (!destroy_tdata) {
1751  			tdata-&gt;attached = false;
1752  		}
1753  		tsd_prof_tdata_set(tsd, NULL);
1754  	} else {
1755  		destroy_tdata = false;
1756  	}
1757  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
1758  	if (destroy_tdata) {
1759  		prof_tdata_destroy(tsd, tdata, true);
1760  	}
1761  }
1762  prof_tdata_t *
1763  prof_tdata_reinit(tsd_t *tsd, prof_tdata_t *tdata) {
1764  	uint64_t thr_uid = tdata-&gt;thr_uid;
1765  	uint64_t thr_discrim = tdata-&gt;thr_discrim + 1;
1766  	char *thread_name = (tdata-&gt;thread_name != NULL) ?
1767  	    prof_thread_name_alloc(tsd_tsdn(tsd), tdata-&gt;thread_name) : NULL;
1768  	bool active = tdata-&gt;active;
1769  	prof_tdata_detach(tsd, tdata);
1770  	return prof_tdata_init_impl(tsd, thr_uid, thr_discrim, thread_name,
1771  	    active);
1772  }
1773  static bool
1774  prof_tdata_expire(tsdn_t *tsdn, prof_tdata_t *tdata) {
1775  	bool destroy_tdata;
1776  	malloc_mutex_lock(tsdn, tdata-&gt;lock);
1777  	if (!tdata-&gt;expired) {
1778  		tdata-&gt;expired = true;
1779  		destroy_tdata = tdata-&gt;attached ? false :
1780  		    prof_tdata_should_destroy(tsdn, tdata, false);
1781  	} else {
1782  		destroy_tdata = false;
1783  	}
1784  	malloc_mutex_unlock(tsdn, tdata-&gt;lock);
1785  	return destroy_tdata;
1786  }
1787  static prof_tdata_t *
1788  prof_tdata_reset_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1789      void *arg) {
1790  	tsdn_t *tsdn = (tsdn_t *)arg;
1791  	return (prof_tdata_expire(tsdn, tdata) ? tdata : NULL);
1792  }
1793  void
1794  prof_reset(tsd_t *tsd, size_t lg_sample) {
1795  	prof_tdata_t *next;
1796  	assert(lg_sample &lt; (sizeof(uint64_t) &lt;&lt; 3));
1797  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1798  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1799  	lg_prof_sample = lg_sample;
1800  	next = NULL;
1801  	do {
1802  		prof_tdata_t *to_destroy = tdata_tree_iter(&amp;tdatas, next,
1803  		    prof_tdata_reset_iter, (void *)tsd);
1804  		if (to_destroy != NULL) {
1805  			next = tdata_tree_next(&amp;tdatas, to_destroy);
1806  			prof_tdata_destroy_locked(tsd, to_destroy, false);
1807  		} else {
1808  			next = NULL;
1809  		}
1810  	} while (next != NULL);
1811  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1812  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1813  }
1814  void
1815  prof_tdata_cleanup(tsd_t *tsd) {
1816  	prof_tdata_t *tdata;
1817  	if (!config_prof) {
1818  		return;
1819  	}
1820  	tdata = tsd_prof_tdata_get(tsd);
1821  	if (tdata != NULL) {
1822  		prof_tdata_detach(tsd, tdata);
1823  	}
1824  }
1825  bool
1826  prof_active_get(tsdn_t *tsdn) {
1827  	bool prof_active_current;
1828  	malloc_mutex_lock(tsdn, &amp;prof_active_mtx);
1829  	prof_active_current = prof_active;
1830  	malloc_mutex_unlock(tsdn, &amp;prof_active_mtx);
1831  	return prof_active_current;
1832  }
1833  bool
1834  prof_active_set(tsdn_t *tsdn, bool active) {
1835  	bool prof_active_old;
1836  	malloc_mutex_lock(tsdn, &amp;prof_active_mtx);
1837  	prof_active_old = prof_active;
1838  	prof_active = active;
1839  	malloc_mutex_unlock(tsdn, &amp;prof_active_mtx);
1840  	return prof_active_old;
1841  }
1842  #ifdef JEMALLOC_JET
1843  size_t
1844  prof_log_bt_count(void) {
1845  	size_t cnt = 0;
1846  	prof_bt_node_t *node = log_bt_first;
1847  	while (node != NULL) {
1848  		cnt++;
1849  		node = node-&gt;next;
1850  	}
1851  	return cnt;
1852  }
1853  size_t
1854  prof_log_alloc_count(void) {
1855  	size_t cnt = 0;
1856  	prof_alloc_node_t *node = log_alloc_first;
1857  	while (node != NULL) {
1858  		cnt++;
1859  		node = node-&gt;next;
1860  	}
1861  	return cnt;
1862  }
1863  size_t
1864  prof_log_thr_count(void) {
1865  	size_t cnt = 0;
1866  	prof_thr_node_t *node = log_thr_first;
1867  	while (node != NULL) {
1868  		cnt++;
1869  		node = node-&gt;next;
1870  	}
1871  	return cnt;
1872  }
1873  bool
1874  prof_log_is_logging(void) {
1875  	return prof_logging_state == prof_logging_state_started;
1876  }
1877  bool
1878  prof_log_rep_check(void) {
1879  	if (prof_logging_state == prof_logging_state_stopped
1880  	    &amp;&amp; log_tables_initialized) {
1881  		return true;
1882  	}
1883  	if (log_bt_last != NULL &amp;&amp; log_bt_last-&gt;next != NULL) {
1884  		return true;
1885  	}
1886  	if (log_thr_last != NULL &amp;&amp; log_thr_last-&gt;next != NULL) {
1887  		return true;
1888  	}
1889  	if (log_alloc_last != NULL &amp;&amp; log_alloc_last-&gt;next != NULL) {
1890  		return true;
1891  	}
1892  	size_t bt_count = prof_log_bt_count();
1893  	size_t thr_count = prof_log_thr_count();
1894  	size_t alloc_count = prof_log_alloc_count();
1895  	if (prof_logging_state == prof_logging_state_stopped) {
1896  		if (bt_count != 0 || thr_count != 0 || alloc_count || 0) {
1897  			return true;
1898  		}
1899  	}
1900  	prof_alloc_node_t *node = log_alloc_first;
1901  	while (node != NULL) {
1902  		if (node-&gt;alloc_bt_ind &gt;= bt_count) {
1903  			return true;
1904  		}
1905  		if (node-&gt;free_bt_ind &gt;= bt_count) {
1906  			return true;
1907  		}
1908  		if (node-&gt;alloc_thr_ind &gt;= thr_count) {
1909  			return true;
1910  		}
1911  		if (node-&gt;free_thr_ind &gt;= thr_count) {
1912  			return true;
1913  		}
1914  		if (node-&gt;alloc_time_ns &gt; node-&gt;free_time_ns) {
1915  			return true;
1916  		}
1917  		node = node-&gt;next;
1918  	}
1919  	return false;
1920  }
1921  void
1922  prof_log_dummy_set(bool new_value) {
1923  	prof_log_dummy = new_value;
1924  }
1925  #endif
1926  bool
1927  prof_log_start(tsdn_t *tsdn, const char *filename) {
1928  	if (!opt_prof || !prof_booted) {
1929  		return true;
1930  	}
1931  	bool ret = false;
1932  	size_t buf_size = PATH_MAX + 1;
1933  	malloc_mutex_lock(tsdn, &amp;log_mtx);
1934  	if (prof_logging_state != prof_logging_state_stopped) {
1935  		ret = true;
1936  	} else if (filename == NULL) {
1937  		malloc_snprintf(log_filename, buf_size, &quot;%s.%d.%&quot;FMTu64&quot;.json&quot;,
1938  		    opt_prof_prefix, prof_getpid(), log_seq);
1939  		log_seq++;
1940  		prof_logging_state = prof_logging_state_started;
1941  	} else if (strlen(filename) &gt;= buf_size) {
1942  		ret = true;
1943  	} else {
1944  		strcpy(log_filename, filename);
1945  		prof_logging_state = prof_logging_state_started;
1946  	}
1947  	if (!ret) {
1948  		nstime_update(&amp;log_start_timestamp);
1949  	}
1950  	malloc_mutex_unlock(tsdn, &amp;log_mtx);
1951  	return ret;
1952  }
1953  static void
1954  prof_log_stop_final(void) {
1955  	tsd_t *tsd = tsd_fetch();
1956  	prof_log_stop(tsd_tsdn(tsd));
1957  }
1958  struct prof_emitter_cb_arg_s {
1959  	int fd;
1960  	ssize_t ret;
1961  };
1962  static void
1963  prof_emitter_write_cb(void *opaque, const char *to_write) {
1964  	struct prof_emitter_cb_arg_s *arg =
1965  	    (struct prof_emitter_cb_arg_s *)opaque;
1966  	size_t bytes = strlen(to_write);
1967  #ifdef JEMALLOC_JET
1968  	if (prof_log_dummy) {
1969  		return;
1970  	}
1971  #endif
1972  	arg-&gt;ret = write(arg-&gt;fd, (void *)to_write, bytes);
1973  }
1974  static void
1975  prof_log_emit_threads(tsd_t *tsd, emitter_t *emitter) {
1976  	emitter_json_array_kv_begin(emitter, &quot;threads&quot;);
1977  	prof_thr_node_t *thr_node = log_thr_first;
1978  	prof_thr_node_t *thr_old_node;
1979  	while (thr_node != NULL) {
1980  		emitter_json_object_begin(emitter);
1981  		emitter_json_kv(emitter, &quot;thr_uid&quot;, emitter_type_uint64,
1982  		    &amp;thr_node-&gt;thr_uid);
1983  		char *thr_name = thr_node-&gt;name;
1984  		emitter_json_kv(emitter, &quot;thr_name&quot;, emitter_type_string,
1985  		    &amp;thr_name);
1986  		emitter_json_object_end(emitter);
1987  		thr_old_node = thr_node;
1988  		thr_node = thr_node-&gt;next;
1989  		idalloc(tsd, thr_old_node);
1990  	}
1991  	emitter_json_array_end(emitter);
1992  }
1993  static void
1994  prof_log_emit_traces(tsd_t *tsd, emitter_t *emitter) {
1995  	emitter_json_array_kv_begin(emitter, &quot;stack_traces&quot;);
1996  	prof_bt_node_t *bt_node = log_bt_first;
1997  	prof_bt_node_t *bt_old_node;
1998  	char buf[2 * sizeof(intptr_t) + 3];
1999  	size_t buf_sz = sizeof(buf);
2000  	while (bt_node != NULL) {
2001  		emitter_json_array_begin(emitter);
2002  		size_t i;
2003  		for (i = 0; i &lt; bt_node-&gt;bt.len; i++) {
2004  			malloc_snprintf(buf, buf_sz, &quot;%p&quot;, bt_node-&gt;bt.vec[i]);
2005  			char *trace_str = buf;
2006  			emitter_json_value(emitter, emitter_type_string,
2007  			    &amp;trace_str);
2008  		}
2009  		emitter_json_array_end(emitter);
2010  		bt_old_node = bt_node;
2011  		bt_node = bt_node-&gt;next;
2012  		idalloc(tsd, bt_old_node);
2013  	}
2014  	emitter_json_array_end(emitter);
2015  }
2016  static void
2017  prof_log_emit_allocs(tsd_t *tsd, emitter_t *emitter) {
2018  	emitter_json_array_kv_begin(emitter, &quot;allocations&quot;);
2019  	prof_alloc_node_t *alloc_node = log_alloc_first;
2020  	prof_alloc_node_t *alloc_old_node;
2021  	while (alloc_node != NULL) {
2022  		emitter_json_object_begin(emitter);
2023  		emitter_json_kv(emitter, &quot;alloc_thread&quot;, emitter_type_size,
2024  		    &amp;alloc_node-&gt;alloc_thr_ind);
2025  		emitter_json_kv(emitter, &quot;free_thread&quot;, emitter_type_size,
2026  		    &amp;alloc_node-&gt;free_thr_ind);
2027  		emitter_json_kv(emitter, &quot;alloc_trace&quot;, emitter_type_size,
2028  		    &amp;alloc_node-&gt;alloc_bt_ind);
2029  		emitter_json_kv(emitter, &quot;free_trace&quot;, emitter_type_size,
2030  		    &amp;alloc_node-&gt;free_bt_ind);
2031  		emitter_json_kv(emitter, &quot;alloc_timestamp&quot;,
2032  		    emitter_type_uint64, &amp;alloc_node-&gt;alloc_time_ns);
2033  		emitter_json_kv(emitter, &quot;free_timestamp&quot;, emitter_type_uint64,
2034  		    &amp;alloc_node-&gt;free_time_ns);
2035  		emitter_json_kv(emitter, &quot;usize&quot;, emitter_type_uint64,
2036  		    &amp;alloc_node-&gt;usize);
2037  		emitter_json_object_end(emitter);
2038  		alloc_old_node = alloc_node;
2039  		alloc_node = alloc_node-&gt;next;
2040  		idalloc(tsd, alloc_old_node);
2041  	}
2042  	emitter_json_array_end(emitter);
2043  }
2044  static void
2045  prof_log_emit_metadata(emitter_t *emitter) {
2046  	emitter_json_object_kv_begin(emitter, &quot;info&quot;);
2047  	nstime_t now = NSTIME_ZERO_INITIALIZER;
2048  	nstime_update(&amp;now);
2049  	uint64_t ns = nstime_ns(&amp;now) - nstime_ns(&amp;log_start_timestamp);
2050  	emitter_json_kv(emitter, &quot;duration&quot;, emitter_type_uint64, &amp;ns);
2051  	char *vers = JEMALLOC_VERSION;
2052  	emitter_json_kv(emitter, &quot;version&quot;,
2053  	    emitter_type_string, &amp;vers);
2054  	emitter_json_kv(emitter, &quot;lg_sample_rate&quot;,
2055  	    emitter_type_int, &amp;lg_prof_sample);
2056  	int pid = prof_getpid();
2057  	emitter_json_kv(emitter, &quot;pid&quot;, emitter_type_int, &amp;pid);
2058  	emitter_json_object_end(emitter);
2059  }
2060  bool
2061  prof_log_stop(tsdn_t *tsdn) {
2062  	if (!opt_prof || !prof_booted) {
2063  		return true;
2064  	}
2065  	tsd_t *tsd = tsdn_tsd(tsdn);
2066  	malloc_mutex_lock(tsdn, &amp;log_mtx);
2067  	if (prof_logging_state != prof_logging_state_started) {
2068  		malloc_mutex_unlock(tsdn, &amp;log_mtx);
2069  		return true;
2070  	}
2071  	prof_logging_state = prof_logging_state_dumping;
2072  	malloc_mutex_unlock(tsdn, &amp;log_mtx);
2073  	emitter_t emitter;
2074  	int fd;
2075  #ifdef JEMALLOC_JET
2076  	if (prof_log_dummy) {
2077  		fd = 0;
2078  	} else {
2079  		fd = creat(log_filename, 0644);
2080  	}
2081  #else
2082  	fd = creat(log_filename, 0644);
2083  #endif
2084  	if (fd == -1) {
2085  		malloc_printf(&quot;&lt;jemalloc&gt;: creat() for log file \&quot;%s\&quot; &quot;
2086  			      &quot; failed with %d\n&quot;, log_filename, errno);
2087  		if (opt_abort) {
2088  			abort();
2089  		}
2090  		return true;
2091  	}
2092  	struct prof_emitter_cb_arg_s arg;
2093  	arg.fd = fd;
2094  	emitter_init(&amp;emitter, emitter_output_json, &amp;prof_emitter_write_cb,
2095  	    (void *)(&amp;arg));
2096  	emitter_begin(&amp;emitter);
2097  	prof_log_emit_metadata(&amp;emitter);
2098  	prof_log_emit_threads(tsd, &amp;emitter);
2099  	prof_log_emit_traces(tsd, &amp;emitter);
2100  	prof_log_emit_allocs(tsd, &amp;emitter);
2101  	emitter_end(&amp;emitter);
2102  	if (log_tables_initialized) {
2103  		ckh_delete(tsd, &amp;log_bt_node_set);
2104  		ckh_delete(tsd, &amp;log_thr_node_set);
2105  	}
2106  	log_tables_initialized = false;
2107  	log_bt_index = 0;
2108  	log_thr_index = 0;
2109  	log_bt_first = NULL;
2110  	log_bt_last = NULL;
2111  	log_thr_first = NULL;
2112  	log_thr_last = NULL;
2113  	log_alloc_first = NULL;
2114  	log_alloc_last = NULL;
2115  	malloc_mutex_lock(tsdn, &amp;log_mtx);
2116  	prof_logging_state = prof_logging_state_stopped;
2117  	malloc_mutex_unlock(tsdn, &amp;log_mtx);
2118  #ifdef JEMALLOC_JET
2119  	if (prof_log_dummy) {
2120  		return false;
2121  	}
2122  #endif
2123  	return close(fd);
2124  }
2125  const char *
2126  prof_thread_name_get(tsd_t *tsd) {
2127  	prof_tdata_t *tdata;
2128  	tdata = prof_tdata_get(tsd, true);
2129  	if (tdata == NULL) {
2130  		return &quot;&quot;;
2131  	}
2132  	return (tdata-&gt;thread_name != NULL ? tdata-&gt;thread_name : &quot;&quot;);
2133  }
2134  static char *
2135  prof_thread_name_alloc(tsdn_t *tsdn, const char *thread_name) {
2136  	char *ret;
2137  	size_t size;
2138  	if (thread_name == NULL) {
2139  		return NULL;
2140  	}
2141  	size = strlen(thread_name) + 1;
2142  	if (size == 1) {
2143  		return &quot;&quot;;
2144  	}
2145  	ret = iallocztm(tsdn, size, sz_size2index(size), false, NULL, true,
2146  	    arena_get(TSDN_NULL, 0, true), true);
2147  	if (ret == NULL) {
2148  		return NULL;
2149  	}
2150  	memcpy(ret, thread_name, size);
2151  	return ret;
2152  }
2153  int
2154  prof_thread_name_set(tsd_t *tsd, const char *thread_name) {
2155  	prof_tdata_t *tdata;
2156  	unsigned i;
2157  	char *s;
2158  	tdata = prof_tdata_get(tsd, true);
2159  	if (tdata == NULL) {
2160  		return EAGAIN;
2161  	}
2162  	if (thread_name == NULL) {
2163  		return EFAULT;
2164  	}
2165  	for (i = 0; thread_name[i] != &#x27;\0&#x27;; i++) {
2166  		char c = thread_name[i];
2167  		if (!isgraph(c) &amp;&amp; !isblank(c)) {
2168  			return EFAULT;
2169  		}
2170  	}
2171  	s = prof_thread_name_alloc(tsd_tsdn(tsd), thread_name);
2172  	if (s == NULL) {
2173  		return EAGAIN;
2174  	}
2175  	if (tdata-&gt;thread_name != NULL) {
2176  		idalloctm(tsd_tsdn(tsd), tdata-&gt;thread_name, NULL, NULL, true,
2177  		    true);
2178  		tdata-&gt;thread_name = NULL;
2179  	}
2180  	if (strlen(s) &gt; 0) {
2181  		tdata-&gt;thread_name = s;
2182  	}
2183  	return 0;
2184  }
2185  bool
2186  prof_thread_active_get(tsd_t *tsd) {
2187  	prof_tdata_t *tdata;
2188  	tdata = prof_tdata_get(tsd, true);
2189  	if (tdata == NULL) {
2190  		return false;
2191  	}
2192  	return tdata-&gt;active;
2193  }
2194  bool
2195  prof_thread_active_set(tsd_t *tsd, bool active) {
2196  	prof_tdata_t *tdata;
2197  	tdata = prof_tdata_get(tsd, true);
2198  	if (tdata == NULL) {
2199  		return true;
2200  	}
2201  	tdata-&gt;active = active;
2202  	return false;
2203  }
2204  bool
2205  prof_thread_active_init_get(tsdn_t *tsdn) {
2206  	bool active_init;
2207  	malloc_mutex_lock(tsdn, &amp;prof_thread_active_init_mtx);
2208  	active_init = prof_thread_active_init;
2209  	malloc_mutex_unlock(tsdn, &amp;prof_thread_active_init_mtx);
2210  	return active_init;
2211  }
2212  bool
2213  prof_thread_active_init_set(tsdn_t *tsdn, bool active_init) {
2214  	bool active_init_old;
2215  	malloc_mutex_lock(tsdn, &amp;prof_thread_active_init_mtx);
2216  	active_init_old = prof_thread_active_init;
2217  	prof_thread_active_init = active_init;
2218  	malloc_mutex_unlock(tsdn, &amp;prof_thread_active_init_mtx);
2219  	return active_init_old;
2220  }
2221  bool
2222  prof_gdump_get(tsdn_t *tsdn) {
2223  	bool prof_gdump_current;
2224  	malloc_mutex_lock(tsdn, &amp;prof_gdump_mtx);
2225  	prof_gdump_current = prof_gdump_val;
2226  	malloc_mutex_unlock(tsdn, &amp;prof_gdump_mtx);
2227  	return prof_gdump_current;
2228  }
2229  bool
2230  prof_gdump_set(tsdn_t *tsdn, bool gdump) {
2231  	bool prof_gdump_old;
2232  	malloc_mutex_lock(tsdn, &amp;prof_gdump_mtx);
2233  	prof_gdump_old = prof_gdump_val;
2234  	prof_gdump_val = gdump;
2235  	malloc_mutex_unlock(tsdn, &amp;prof_gdump_mtx);
2236  	return prof_gdump_old;
2237  }
2238  void
2239  prof_boot0(void) {
2240  	cassert(config_prof);
2241  	memcpy(opt_prof_prefix, PROF_PREFIX_DEFAULT,
2242  	    sizeof(PROF_PREFIX_DEFAULT));
2243  }
2244  void
2245  prof_boot1(void) {
2246  	cassert(config_prof);
2247  	if (opt_prof_leak &amp;&amp; !opt_prof) {
2248  		opt_prof = true;
2249  		opt_prof_gdump = false;
2250  	} else if (opt_prof) {
2251  		if (opt_lg_prof_interval &gt;= 0) {
2252  			prof_interval = (((uint64_t)1U) &lt;&lt;
2253  			    opt_lg_prof_interval);
2254  		}
2255  	}
2256  }
2257  bool
2258  prof_boot2(tsd_t *tsd) {
2259  	cassert(config_prof);
2260  	if (opt_prof) {
2261  		unsigned i;
2262  		lg_prof_sample = opt_lg_prof_sample;
2263  		prof_active = opt_prof_active;
2264  		if (malloc_mutex_init(&amp;prof_active_mtx, &quot;prof_active&quot;,
2265  		    WITNESS_RANK_PROF_ACTIVE, malloc_mutex_rank_exclusive)) {
2266  			return true;
2267  		}
2268  		prof_gdump_val = opt_prof_gdump;
2269  		if (malloc_mutex_init(&amp;prof_gdump_mtx, &quot;prof_gdump&quot;,
2270  		    WITNESS_RANK_PROF_GDUMP, malloc_mutex_rank_exclusive)) {
2271  			return true;
2272  		}
2273  		prof_thread_active_init = opt_prof_thread_active_init;
2274  		if (malloc_mutex_init(&amp;prof_thread_active_init_mtx,
2275  		    &quot;prof_thread_active_init&quot;,
2276  		    WITNESS_RANK_PROF_THREAD_ACTIVE_INIT,
2277  		    malloc_mutex_rank_exclusive)) {
2278  			return true;
2279  		}
2280  		if (ckh_new(tsd, &amp;bt2gctx, PROF_CKH_MINITEMS, prof_bt_hash,
2281  		    prof_bt_keycomp)) {
2282  			return true;
2283  		}
2284  		if (malloc_mutex_init(&amp;bt2gctx_mtx, &quot;prof_bt2gctx&quot;,
2285  		    WITNESS_RANK_PROF_BT2GCTX, malloc_mutex_rank_exclusive)) {
2286  			return true;
2287  		}
2288  		tdata_tree_new(&amp;tdatas);
2289  		if (malloc_mutex_init(&amp;tdatas_mtx, &quot;prof_tdatas&quot;,
2290  		    WITNESS_RANK_PROF_TDATAS, malloc_mutex_rank_exclusive)) {
2291  			return true;
2292  		}
2293  		next_thr_uid = 0;
2294  		if (malloc_mutex_init(&amp;next_thr_uid_mtx, &quot;prof_next_thr_uid&quot;,
2295  		    WITNESS_RANK_PROF_NEXT_THR_UID, malloc_mutex_rank_exclusive)) {
2296  			return true;
2297  		}
2298  		if (malloc_mutex_init(&amp;prof_dump_seq_mtx, &quot;prof_dump_seq&quot;,
2299  		    WITNESS_RANK_PROF_DUMP_SEQ, malloc_mutex_rank_exclusive)) {
2300  			return true;
2301  		}
2302  		if (malloc_mutex_init(&amp;prof_dump_mtx, &quot;prof_dump&quot;,
2303  		    WITNESS_RANK_PROF_DUMP, malloc_mutex_rank_exclusive)) {
2304  			return true;
2305  		}
2306  		if (opt_prof_final &amp;&amp; opt_prof_prefix[0] != &#x27;\0&#x27; &amp;&amp;
2307  		    atexit(prof_fdump) != 0) {
2308  			malloc_write(&quot;&lt;jemalloc&gt;: Error in atexit()\n&quot;);
2309  			if (opt_abort) {
2310  				abort();
2311  			}
2312  		}
2313  		if (opt_prof_log) {
2314  			prof_log_start(tsd_tsdn(tsd), NULL);
2315  		}
2316  		if (atexit(prof_log_stop_final) != 0) {
2317  			malloc_write(&quot;&lt;jemalloc&gt;: Error in atexit() &quot;
2318  				     &quot;for logging\n&quot;);
2319  			if (opt_abort) {
2320  				abort();
2321  			}
2322  		}
2323  		if (malloc_mutex_init(&amp;log_mtx, &quot;prof_log&quot;,
2324  		    WITNESS_RANK_PROF_LOG, malloc_mutex_rank_exclusive)) {
2325  			return true;
2326  		}
2327  		if (ckh_new(tsd, &amp;log_bt_node_set, PROF_CKH_MINITEMS,
2328  		    prof_bt_node_hash, prof_bt_node_keycomp)) {
2329  			return true;
2330  		}
2331  		if (ckh_new(tsd, &amp;log_thr_node_set, PROF_CKH_MINITEMS,
2332  		    prof_thr_node_hash, prof_thr_node_keycomp)) {
2333  			return true;
2334  		}
2335  		log_tables_initialized = true;
2336  		gctx_locks = (malloc_mutex_t *)base_alloc(tsd_tsdn(tsd),
2337  		    b0get(), PROF_NCTX_LOCKS * sizeof(malloc_mutex_t),
2338  		    CACHELINE);
2339  		if (gctx_locks == NULL) {
2340  			return true;
2341  		}
2342  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2343  			if (malloc_mutex_init(&amp;gctx_locks[i], &quot;prof_gctx&quot;,
2344  			    WITNESS_RANK_PROF_GCTX,
2345  			    malloc_mutex_rank_exclusive)) {
2346  				return true;
2347  			}
2348  		}
2349  		tdata_locks = (malloc_mutex_t *)base_alloc(tsd_tsdn(tsd),
2350  		    b0get(), PROF_NTDATA_LOCKS * sizeof(malloc_mutex_t),
2351  		    CACHELINE);
2352  		if (tdata_locks == NULL) {
2353  			return true;
2354  		}
2355  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2356  			if (malloc_mutex_init(&amp;tdata_locks[i], &quot;prof_tdata&quot;,
2357  			    WITNESS_RANK_PROF_TDATA,
2358  			    malloc_mutex_rank_exclusive)) {
2359  				return true;
2360  			}
2361  		}
2362  #ifdef JEMALLOC_PROF_LIBGCC
2363  		_Unwind_Backtrace(prof_unwind_init_callback, NULL);
2364  #endif
2365  	}
2366  	prof_booted = true;
2367  	return false;
2368  }
2369  void
2370  prof_prefork0(tsdn_t *tsdn) {
2371  	if (config_prof &amp;&amp; opt_prof) {
2372  		unsigned i;
2373  		malloc_mutex_prefork(tsdn, &amp;prof_dump_mtx);
2374  		malloc_mutex_prefork(tsdn, &amp;bt2gctx_mtx);
2375  		malloc_mutex_prefork(tsdn, &amp;tdatas_mtx);
2376  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2377  			malloc_mutex_prefork(tsdn, &amp;tdata_locks[i]);
2378  		}
2379  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2380  			malloc_mutex_prefork(tsdn, &amp;gctx_locks[i]);
2381  		}
2382  	}
2383  }
2384  void
2385  prof_prefork1(tsdn_t *tsdn) {
2386  	if (config_prof &amp;&amp; opt_prof) {
2387  		malloc_mutex_prefork(tsdn, &amp;prof_active_mtx);
2388  		malloc_mutex_prefork(tsdn, &amp;prof_dump_seq_mtx);
2389  		malloc_mutex_prefork(tsdn, &amp;prof_gdump_mtx);
2390  		malloc_mutex_prefork(tsdn, &amp;next_thr_uid_mtx);
2391  		malloc_mutex_prefork(tsdn, &amp;prof_thread_active_init_mtx);
2392  	}
2393  }
2394  void
2395  prof_postfork_parent(tsdn_t *tsdn) {
2396  	if (config_prof &amp;&amp; opt_prof) {
2397  		unsigned i;
2398  		malloc_mutex_postfork_parent(tsdn,
2399  		    &amp;prof_thread_active_init_mtx);
2400  		malloc_mutex_postfork_parent(tsdn, &amp;next_thr_uid_mtx);
2401  		malloc_mutex_postfork_parent(tsdn, &amp;prof_gdump_mtx);
2402  		malloc_mutex_postfork_parent(tsdn, &amp;prof_dump_seq_mtx);
2403  		malloc_mutex_postfork_parent(tsdn, &amp;prof_active_mtx);
2404  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2405  			malloc_mutex_postfork_parent(tsdn, &amp;gctx_locks[i]);
2406  		}
2407  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2408  			malloc_mutex_postfork_parent(tsdn, &amp;tdata_locks[i]);
2409  		}
2410  		malloc_mutex_postfork_parent(tsdn, &amp;tdatas_mtx);
2411  		malloc_mutex_postfork_parent(tsdn, &amp;bt2gctx_mtx);
2412  		malloc_mutex_postfork_parent(tsdn, &amp;prof_dump_mtx);
2413  	}
2414  }
2415  void
2416  prof_postfork_child(tsdn_t *tsdn) {
2417  	if (config_prof &amp;&amp; opt_prof) {
2418  		unsigned i;
2419  		malloc_mutex_postfork_child(tsdn, &amp;prof_thread_active_init_mtx);
2420  		malloc_mutex_postfork_child(tsdn, &amp;next_thr_uid_mtx);
2421  		malloc_mutex_postfork_child(tsdn, &amp;prof_gdump_mtx);
2422  		malloc_mutex_postfork_child(tsdn, &amp;prof_dump_seq_mtx);
2423  		malloc_mutex_postfork_child(tsdn, &amp;prof_active_mtx);
2424  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2425  			malloc_mutex_postfork_child(tsdn, &amp;gctx_locks[i]);
2426  		}
2427  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2428  			malloc_mutex_postfork_child(tsdn, &amp;tdata_locks[i]);
2429  		}
2430  		malloc_mutex_postfork_child(tsdn, &amp;tdatas_mtx);
2431  		malloc_mutex_postfork_child(tsdn, &amp;bt2gctx_mtx);
2432  		malloc_mutex_postfork_child(tsdn, &amp;prof_dump_mtx);
2433  	}
2434  }
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-prof.c</h3>
            <pre><code>1  #define JEMALLOC_PROF_C_
2  #include &quot;jemalloc/internal/jemalloc_preamble.h&quot;
3  #include &quot;jemalloc/internal/jemalloc_internal_includes.h&quot;
4  #include &quot;jemalloc/internal/assert.h&quot;
5  #include &quot;jemalloc/internal/ckh.h&quot;
6  #include &quot;jemalloc/internal/hash.h&quot;
7  #include &quot;jemalloc/internal/malloc_io.h&quot;
8  #include &quot;jemalloc/internal/mutex.h&quot;
9  #include &quot;jemalloc/internal/emitter.h&quot;
10  #ifdef JEMALLOC_PROF_LIBUNWIND
11  #define UNW_LOCAL_ONLY
12  #include &lt;libunwind.h&gt;
13  #endif
14  #ifdef JEMALLOC_PROF_LIBGCC
15  #undef _Unwind_Backtrace
16  #include &lt;unwind.h&gt;
17  #define _Unwind_Backtrace JEMALLOC_HOOK(_Unwind_Backtrace, test_hooks_libc_hook)
18  #endif
19  bool		opt_prof = false;
20  bool		opt_prof_active = true;
21  bool		opt_prof_thread_active_init = true;
22  size_t		opt_lg_prof_sample = LG_PROF_SAMPLE_DEFAULT;
23  ssize_t		opt_lg_prof_interval = LG_PROF_INTERVAL_DEFAULT;
24  bool		opt_prof_gdump = false;
25  bool		opt_prof_final = false;
26  bool		opt_prof_leak = false;
27  bool		opt_prof_accum = false;
28  bool		opt_prof_log = false;
29  char		opt_prof_prefix[
30  #ifdef JEMALLOC_PROF
31      PATH_MAX +
32  #endif
33      1];
34  bool			prof_active;
35  static malloc_mutex_t	prof_active_mtx;
36  static bool		prof_thread_active_init;
37  static malloc_mutex_t	prof_thread_active_init_mtx;
38  bool			prof_gdump_val;
39  static malloc_mutex_t	prof_gdump_mtx;
40  uint64_t	prof_interval = 0;
41  size_t		lg_prof_sample;
42  typedef enum prof_logging_state_e prof_logging_state_t;
43  enum prof_logging_state_e {
44  	prof_logging_state_stopped,
45  	prof_logging_state_started,
46  	prof_logging_state_dumping
47  };
48  prof_logging_state_t prof_logging_state = prof_logging_state_stopped;
49  #ifdef JEMALLOC_JET
50  static bool prof_log_dummy = false;
51  #endif
52  static uint64_t log_seq = 0;
53  static char log_filename[
54  #ifdef JEMALLOC_PROF
55      PATH_MAX +
56  #endif
57      1];
58  static nstime_t log_start_timestamp = NSTIME_ZERO_INITIALIZER;
59  static size_t log_bt_index = 0;
60  static size_t log_thr_index = 0;
61  typedef struct prof_bt_node_s prof_bt_node_t;
62  struct prof_bt_node_s {
63  	prof_bt_node_t *next;
64  	size_t index;
65  	prof_bt_t bt;
66  	void *vec[1];
67  };
68  typedef struct prof_thr_node_s prof_thr_node_t;
69  struct prof_thr_node_s {
70  	prof_thr_node_t *next;
71  	size_t index;
72  	uint64_t thr_uid;
73  	char name[1];
74  };
75  typedef struct prof_alloc_node_s prof_alloc_node_t;
76  struct prof_alloc_node_s {
77  	prof_alloc_node_t *next;
78  	size_t alloc_thr_ind;
79  	size_t free_thr_ind;
80  	size_t alloc_bt_ind;
81  	size_t free_bt_ind;
82  	uint64_t alloc_time_ns;
83  	uint64_t free_time_ns;
84  	size_t usize;
85  };
86  static bool log_tables_initialized = false;
87  static ckh_t log_bt_node_set;
88  static ckh_t log_thr_node_set;
89  static prof_bt_node_t *log_bt_first = NULL;
90  static prof_bt_node_t *log_bt_last = NULL;
91  static prof_thr_node_t *log_thr_first = NULL;
92  static prof_thr_node_t *log_thr_last = NULL;
93  static prof_alloc_node_t *log_alloc_first = NULL;
94  static prof_alloc_node_t *log_alloc_last = NULL;
95  static malloc_mutex_t log_mtx;
96  static malloc_mutex_t	*gctx_locks;
97  static atomic_u_t	cum_gctxs; &amp;bsol;* Atomic counter. */
98  static malloc_mutex_t	*tdata_locks;
99  static ckh_t		bt2gctx;
100  malloc_mutex_t		bt2gctx_mtx;
101  static prof_tdata_tree_t	tdatas;
102  static malloc_mutex_t	tdatas_mtx;
103  static uint64_t		next_thr_uid;
104  static malloc_mutex_t	next_thr_uid_mtx;
105  static malloc_mutex_t	prof_dump_seq_mtx;
106  static uint64_t		prof_dump_seq;
107  static uint64_t		prof_dump_iseq;
108  static uint64_t		prof_dump_mseq;
109  static uint64_t		prof_dump_useq;
110  static malloc_mutex_t	prof_dump_mtx;
111  static char		prof_dump_buf[
112  #ifdef JEMALLOC_PROF
113      PROF_DUMP_BUFSIZE
114  #else
115      1
116  #endif
117  ];
118  static size_t		prof_dump_buf_end;
119  static int		prof_dump_fd;
120  static bool		prof_booted = false;
121  static bool	prof_tctx_should_destroy(tsdn_t *tsdn, prof_tctx_t *tctx);
122  static void	prof_tctx_destroy(tsd_t *tsd, prof_tctx_t *tctx);
123  static bool	prof_tdata_should_destroy(tsdn_t *tsdn, prof_tdata_t *tdata,
124      bool even_if_attached);
125  static void	prof_tdata_destroy(tsd_t *tsd, prof_tdata_t *tdata,
126      bool even_if_attached);
127  static char	*prof_thread_name_alloc(tsdn_t *tsdn, const char *thread_name);
128  static void prof_thr_node_hash(const void *key, size_t r_hash[2]);
129  static bool prof_thr_node_keycomp(const void *k1, const void *k2);
130  static void prof_bt_node_hash(const void *key, size_t r_hash[2]);
131  static bool prof_bt_node_keycomp(const void *k1, const void *k2);
132  static int
133  prof_tctx_comp(const prof_tctx_t *a, const prof_tctx_t *b) {
134  	uint64_t a_thr_uid = a-&gt;thr_uid;
135  	uint64_t b_thr_uid = b-&gt;thr_uid;
136  	int ret = (a_thr_uid &gt; b_thr_uid) - (a_thr_uid &lt; b_thr_uid);
137  	if (ret == 0) {
138  		uint64_t a_thr_discrim = a-&gt;thr_discrim;
139  		uint64_t b_thr_discrim = b-&gt;thr_discrim;
140  		ret = (a_thr_discrim &gt; b_thr_discrim) - (a_thr_discrim &lt;
141  		    b_thr_discrim);
142  		if (ret == 0) {
143  			uint64_t a_tctx_uid = a-&gt;tctx_uid;
144  			uint64_t b_tctx_uid = b-&gt;tctx_uid;
145  			ret = (a_tctx_uid &gt; b_tctx_uid) - (a_tctx_uid &lt;
146  			    b_tctx_uid);
147  		}
148  	}
149  	return ret;
150  }
151  rb_gen(static UNUSED, tctx_tree_, prof_tctx_tree_t, prof_tctx_t,
152      tctx_link, prof_tctx_comp)
153  static int
154  prof_gctx_comp(const prof_gctx_t *a, const prof_gctx_t *b) {
155  	unsigned a_len = a-&gt;bt.len;
156  	unsigned b_len = b-&gt;bt.len;
157  	unsigned comp_len = (a_len &lt; b_len) ? a_len : b_len;
158  	int ret = memcmp(a-&gt;bt.vec, b-&gt;bt.vec, comp_len * sizeof(void *));
159  	if (ret == 0) {
160  		ret = (a_len &gt; b_len) - (a_len &lt; b_len);
161  	}
162  	return ret;
163  }
164  rb_gen(static UNUSED, gctx_tree_, prof_gctx_tree_t, prof_gctx_t, dump_link,
165      prof_gctx_comp)
166  static int
167  prof_tdata_comp(const prof_tdata_t *a, const prof_tdata_t *b) {
168  	int ret;
169  	uint64_t a_uid = a-&gt;thr_uid;
170  	uint64_t b_uid = b-&gt;thr_uid;
171  	ret = ((a_uid &gt; b_uid) - (a_uid &lt; b_uid));
172  	if (ret == 0) {
173  		uint64_t a_discrim = a-&gt;thr_discrim;
174  		uint64_t b_discrim = b-&gt;thr_discrim;
175  		ret = ((a_discrim &gt; b_discrim) - (a_discrim &lt; b_discrim));
176  	}
177  	return ret;
178  }
179  rb_gen(static UNUSED, tdata_tree_, prof_tdata_tree_t, prof_tdata_t, tdata_link,
180      prof_tdata_comp)
181  void
182  prof_alloc_rollback(tsd_t *tsd, prof_tctx_t *tctx, bool updated) {
183  	prof_tdata_t *tdata;
184  	cassert(config_prof);
185  	if (updated) {
186  		tdata = prof_tdata_get(tsd, true);
187  		if (tdata != NULL) {
188  			prof_sample_threshold_update(tdata);
189  		}
190  	}
191  	if ((uintptr_t)tctx &gt; (uintptr_t)1U) {
192  		malloc_mutex_lock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
193  		tctx-&gt;prepared = false;
194  		if (prof_tctx_should_destroy(tsd_tsdn(tsd), tctx)) {
195  			prof_tctx_destroy(tsd, tctx);
196  		} else {
197  			malloc_mutex_unlock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
198  		}
199  	}
200  }
201  void
202  prof_malloc_sample_object(tsdn_t *tsdn, const void *ptr, size_t usize,
203      prof_tctx_t *tctx) {
204  	prof_tctx_set(tsdn, ptr, usize, NULL, tctx);
205  	nstime_t t = NSTIME_ZERO_INITIALIZER;
206  	nstime_update(&amp;t);
207  	prof_alloc_time_set(tsdn, ptr, NULL, t);
208  	malloc_mutex_lock(tsdn, tctx-&gt;tdata-&gt;lock);
209  	tctx-&gt;cnts.curobjs++;
210  	tctx-&gt;cnts.curbytes += usize;
211  	if (opt_prof_accum) {
212  		tctx-&gt;cnts.accumobjs++;
213  		tctx-&gt;cnts.accumbytes += usize;
214  	}
215  	tctx-&gt;prepared = false;
216  	malloc_mutex_unlock(tsdn, tctx-&gt;tdata-&gt;lock);
217  }
218  static size_t
219  prof_log_bt_index(tsd_t *tsd, prof_bt_t *bt) {
220  	assert(prof_logging_state == prof_logging_state_started);
221  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;log_mtx);
222  	prof_bt_node_t dummy_node;
223  	dummy_node.bt = *bt;
224  	prof_bt_node_t *node;
225  	if (ckh_search(&amp;log_bt_node_set, (void *)(&amp;dummy_node),
226  	    (void **)(&amp;node), NULL)) {
227  		size_t sz = offsetof(prof_bt_node_t, vec) +
228  			        (bt-&gt;len * sizeof(void *));
229  		prof_bt_node_t *new_node = (prof_bt_node_t *)
230  		    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL,
231  		    true, arena_get(TSDN_NULL, 0, true), true);
232  		if (log_bt_first == NULL) {
233  			log_bt_first = new_node;
234  			log_bt_last = new_node;
235  		} else {
236  			log_bt_last-&gt;next = new_node;
237  			log_bt_last = new_node;
238  		}
239  		new_node-&gt;next = NULL;
240  		new_node-&gt;index = log_bt_index;
241  		new_node-&gt;bt.len = bt-&gt;len;
242  		memcpy(new_node-&gt;vec, bt-&gt;vec, bt-&gt;len * sizeof(void *));
243  		new_node-&gt;bt.vec = new_node-&gt;vec;
244  		log_bt_index++;
245  		ckh_insert(tsd, &amp;log_bt_node_set, (void *)new_node, NULL);
246  		return new_node-&gt;index;
247  	} else {
248  		return node-&gt;index;
249  	}
250  }
251  static size_t
252  prof_log_thr_index(tsd_t *tsd, uint64_t thr_uid, const char *name) {
253  	assert(prof_logging_state == prof_logging_state_started);
254  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;log_mtx);
255  	prof_thr_node_t dummy_node;
256  	dummy_node.thr_uid = thr_uid;
257  	prof_thr_node_t *node;
258  	if (ckh_search(&amp;log_thr_node_set, (void *)(&amp;dummy_node),
259  	    (void **)(&amp;node), NULL)) {
260  		size_t sz = offsetof(prof_thr_node_t, name) + strlen(name) + 1;
261  		prof_thr_node_t *new_node = (prof_thr_node_t *)
262  		    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL,
<span onclick='openModal()' class='match'>263  		    true, arena_get(TSDN_NULL, 0, true), true);
264  		if (log_thr_first == NULL) {
265  			log_thr_first = new_node;
266  			log_thr_last = new_node;
267  		} else {
268  			log_thr_last-&gt;next = new_node;
269  			log_thr_last = new_node;
270  		}
271  		new_node-&gt;next = NULL;
272  		new_node-&gt;index = log_thr_index;
273  		new_node-&gt;thr_uid = thr_uid;
</span>274  		strcpy(new_node-&gt;name, name);
275  		log_thr_index++;
276  		ckh_insert(tsd, &amp;log_thr_node_set, (void *)new_node, NULL);
277  		return new_node-&gt;index;
278  	} else {
279  		return node-&gt;index;
280  	}
281  }
282  static void
283  prof_try_log(tsd_t *tsd, const void *ptr, size_t usize, prof_tctx_t *tctx) {
284  	malloc_mutex_assert_owner(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
285  	prof_tdata_t *cons_tdata = prof_tdata_get(tsd, false);
286  	if (cons_tdata == NULL) {
287  		return;
288  	}
289  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;log_mtx);
290  	if (prof_logging_state != prof_logging_state_started) {
291  		goto label_done;
292  	}
293  	if (!log_tables_initialized) {
294  		bool err1 = ckh_new(tsd, &amp;log_bt_node_set, PROF_CKH_MINITEMS,
295  				prof_bt_node_hash, prof_bt_node_keycomp);
296  		bool err2 = ckh_new(tsd, &amp;log_thr_node_set, PROF_CKH_MINITEMS,
297  				prof_thr_node_hash, prof_thr_node_keycomp);
298  		if (err1 || err2) {
299  			goto label_done;
300  		}
301  		log_tables_initialized = true;
302  	}
303  	nstime_t alloc_time = prof_alloc_time_get(tsd_tsdn(tsd), ptr,
304  			          (alloc_ctx_t *)NULL);
305  	nstime_t free_time = NSTIME_ZERO_INITIALIZER;
306  	nstime_update(&amp;free_time);
307  	size_t sz = sizeof(prof_alloc_node_t);
308  	prof_alloc_node_t *new_node = (prof_alloc_node_t *)
309  	    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL, true,
310  	    arena_get(TSDN_NULL, 0, true), true);
311  	const char *prod_thr_name = (tctx-&gt;tdata-&gt;thread_name == NULL)?
312  				        &quot;&quot; : tctx-&gt;tdata-&gt;thread_name;
313  	const char *cons_thr_name = prof_thread_name_get(tsd);
314  	prof_bt_t bt;
315  	bt_init(&amp;bt, cons_tdata-&gt;vec);
316  	prof_backtrace(&amp;bt);
317  	prof_bt_t *cons_bt = &amp;bt;
318  	prof_bt_t *prod_bt = &amp;tctx-&gt;gctx-&gt;bt;
319  	new_node-&gt;next = NULL;
320  	new_node-&gt;alloc_thr_ind = prof_log_thr_index(tsd, tctx-&gt;tdata-&gt;thr_uid,
321  				      prod_thr_name);
322  	new_node-&gt;free_thr_ind = prof_log_thr_index(tsd, cons_tdata-&gt;thr_uid,
323  				     cons_thr_name);
324  	new_node-&gt;alloc_bt_ind = prof_log_bt_index(tsd, prod_bt);
325  	new_node-&gt;free_bt_ind = prof_log_bt_index(tsd, cons_bt);
326  	new_node-&gt;alloc_time_ns = nstime_ns(&amp;alloc_time);
327  	new_node-&gt;free_time_ns = nstime_ns(&amp;free_time);
328  	new_node-&gt;usize = usize;
329  	if (log_alloc_first == NULL) {
330  		log_alloc_first = new_node;
331  		log_alloc_last = new_node;
332  	} else {
333  		log_alloc_last-&gt;next = new_node;
334  		log_alloc_last = new_node;
335  	}
336  label_done:
337  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;log_mtx);
338  }
339  void
340  prof_free_sampled_object(tsd_t *tsd, const void *ptr, size_t usize,
341      prof_tctx_t *tctx) {
342  	malloc_mutex_lock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
343  	assert(tctx-&gt;cnts.curobjs &gt; 0);
344  	assert(tctx-&gt;cnts.curbytes &gt;= usize);
345  	tctx-&gt;cnts.curobjs--;
346  	tctx-&gt;cnts.curbytes -= usize;
347  	prof_try_log(tsd, ptr, usize, tctx);
348  	if (prof_tctx_should_destroy(tsd_tsdn(tsd), tctx)) {
349  		prof_tctx_destroy(tsd, tctx);
350  	} else {
351  		malloc_mutex_unlock(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
352  	}
353  }
354  void
355  bt_init(prof_bt_t *bt, void **vec) {
356  	cassert(config_prof);
357  	bt-&gt;vec = vec;
358  	bt-&gt;len = 0;
359  }
360  static void
361  prof_enter(tsd_t *tsd, prof_tdata_t *tdata) {
362  	cassert(config_prof);
363  	assert(tdata == prof_tdata_get(tsd, false));
364  	if (tdata != NULL) {
365  		assert(!tdata-&gt;enq);
366  		tdata-&gt;enq = true;
367  	}
368  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
369  }
370  static void
371  prof_leave(tsd_t *tsd, prof_tdata_t *tdata) {
372  	cassert(config_prof);
373  	assert(tdata == prof_tdata_get(tsd, false));
374  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
375  	if (tdata != NULL) {
376  		bool idump, gdump;
377  		assert(tdata-&gt;enq);
378  		tdata-&gt;enq = false;
379  		idump = tdata-&gt;enq_idump;
380  		tdata-&gt;enq_idump = false;
381  		gdump = tdata-&gt;enq_gdump;
382  		tdata-&gt;enq_gdump = false;
383  		if (idump) {
384  			prof_idump(tsd_tsdn(tsd));
385  		}
386  		if (gdump) {
387  			prof_gdump(tsd_tsdn(tsd));
388  		}
389  	}
390  }
391  #ifdef JEMALLOC_PROF_LIBUNWIND
392  void
393  prof_backtrace(prof_bt_t *bt) {
394  	int nframes;
395  	cassert(config_prof);
396  	assert(bt-&gt;len == 0);
397  	assert(bt-&gt;vec != NULL);
398  	nframes = unw_backtrace(bt-&gt;vec, PROF_BT_MAX);
399  	if (nframes &lt;= 0) {
400  		return;
401  	}
402  	bt-&gt;len = nframes;
403  }
404  #elif (defined(JEMALLOC_PROF_LIBGCC))
405  static _Unwind_Reason_Code
406  prof_unwind_init_callback(struct _Unwind_Context *context, void *arg) {
407  	cassert(config_prof);
408  	return _URC_NO_REASON;
409  }
410  static _Unwind_Reason_Code
411  prof_unwind_callback(struct _Unwind_Context *context, void *arg) {
412  	prof_unwind_data_t *data = (prof_unwind_data_t *)arg;
413  	void *ip;
414  	cassert(config_prof);
415  	ip = (void *)_Unwind_GetIP(context);
416  	if (ip == NULL) {
417  		return _URC_END_OF_STACK;
418  	}
419  	data-&gt;bt-&gt;vec[data-&gt;bt-&gt;len] = ip;
420  	data-&gt;bt-&gt;len++;
421  	if (data-&gt;bt-&gt;len == data-&gt;max) {
422  		return _URC_END_OF_STACK;
423  	}
424  	return _URC_NO_REASON;
425  }
426  void
427  prof_backtrace(prof_bt_t *bt) {
428  	prof_unwind_data_t data = {bt, PROF_BT_MAX};
429  	cassert(config_prof);
430  	_Unwind_Backtrace(prof_unwind_callback, &amp;data);
431  }
432  #elif (defined(JEMALLOC_PROF_GCC))
433  void
434  prof_backtrace(prof_bt_t *bt) {
435  #define BT_FRAME(i)							\
436  	if ((i) &lt; PROF_BT_MAX) {					\
437  		void *p;						\
438  		if (__builtin_frame_address(i) == 0) {			\
439  			return;						\
440  		}							\
441  		p = __builtin_return_address(i);			\
442  		if (p == NULL) {					\
443  			return;						\
444  		}							\
445  		bt-&gt;vec[(i)] = p;					\
446  		bt-&gt;len = (i) + 1;					\
447  	} else {							\
448  		return;							\
449  	}
450  	cassert(config_prof);
451  	BT_FRAME(0)
452  	BT_FRAME(1)
453  	BT_FRAME(2)
454  	BT_FRAME(3)
455  	BT_FRAME(4)
456  	BT_FRAME(5)
457  	BT_FRAME(6)
458  	BT_FRAME(7)
459  	BT_FRAME(8)
460  	BT_FRAME(9)
461  	BT_FRAME(10)
462  	BT_FRAME(11)
463  	BT_FRAME(12)
464  	BT_FRAME(13)
465  	BT_FRAME(14)
466  	BT_FRAME(15)
467  	BT_FRAME(16)
468  	BT_FRAME(17)
469  	BT_FRAME(18)
470  	BT_FRAME(19)
471  	BT_FRAME(20)
472  	BT_FRAME(21)
473  	BT_FRAME(22)
474  	BT_FRAME(23)
475  	BT_FRAME(24)
476  	BT_FRAME(25)
477  	BT_FRAME(26)
478  	BT_FRAME(27)
479  	BT_FRAME(28)
480  	BT_FRAME(29)
481  	BT_FRAME(30)
482  	BT_FRAME(31)
483  	BT_FRAME(32)
484  	BT_FRAME(33)
485  	BT_FRAME(34)
486  	BT_FRAME(35)
487  	BT_FRAME(36)
488  	BT_FRAME(37)
489  	BT_FRAME(38)
490  	BT_FRAME(39)
491  	BT_FRAME(40)
492  	BT_FRAME(41)
493  	BT_FRAME(42)
494  	BT_FRAME(43)
495  	BT_FRAME(44)
496  	BT_FRAME(45)
497  	BT_FRAME(46)
498  	BT_FRAME(47)
499  	BT_FRAME(48)
500  	BT_FRAME(49)
501  	BT_FRAME(50)
502  	BT_FRAME(51)
503  	BT_FRAME(52)
504  	BT_FRAME(53)
505  	BT_FRAME(54)
506  	BT_FRAME(55)
507  	BT_FRAME(56)
508  	BT_FRAME(57)
509  	BT_FRAME(58)
510  	BT_FRAME(59)
511  	BT_FRAME(60)
512  	BT_FRAME(61)
513  	BT_FRAME(62)
514  	BT_FRAME(63)
515  	BT_FRAME(64)
516  	BT_FRAME(65)
517  	BT_FRAME(66)
518  	BT_FRAME(67)
519  	BT_FRAME(68)
520  	BT_FRAME(69)
521  	BT_FRAME(70)
522  	BT_FRAME(71)
523  	BT_FRAME(72)
524  	BT_FRAME(73)
525  	BT_FRAME(74)
526  	BT_FRAME(75)
527  	BT_FRAME(76)
528  	BT_FRAME(77)
529  	BT_FRAME(78)
530  	BT_FRAME(79)
531  	BT_FRAME(80)
532  	BT_FRAME(81)
533  	BT_FRAME(82)
534  	BT_FRAME(83)
535  	BT_FRAME(84)
536  	BT_FRAME(85)
537  	BT_FRAME(86)
538  	BT_FRAME(87)
539  	BT_FRAME(88)
540  	BT_FRAME(89)
541  	BT_FRAME(90)
542  	BT_FRAME(91)
543  	BT_FRAME(92)
544  	BT_FRAME(93)
545  	BT_FRAME(94)
546  	BT_FRAME(95)
547  	BT_FRAME(96)
548  	BT_FRAME(97)
549  	BT_FRAME(98)
550  	BT_FRAME(99)
551  	BT_FRAME(100)
552  	BT_FRAME(101)
553  	BT_FRAME(102)
554  	BT_FRAME(103)
555  	BT_FRAME(104)
556  	BT_FRAME(105)
557  	BT_FRAME(106)
558  	BT_FRAME(107)
559  	BT_FRAME(108)
560  	BT_FRAME(109)
561  	BT_FRAME(110)
562  	BT_FRAME(111)
563  	BT_FRAME(112)
564  	BT_FRAME(113)
565  	BT_FRAME(114)
566  	BT_FRAME(115)
567  	BT_FRAME(116)
568  	BT_FRAME(117)
569  	BT_FRAME(118)
570  	BT_FRAME(119)
571  	BT_FRAME(120)
572  	BT_FRAME(121)
573  	BT_FRAME(122)
574  	BT_FRAME(123)
575  	BT_FRAME(124)
576  	BT_FRAME(125)
577  	BT_FRAME(126)
578  	BT_FRAME(127)
579  #undef BT_FRAME
580  }
581  #else
582  void
583  prof_backtrace(prof_bt_t *bt) {
584  	cassert(config_prof);
585  	not_reached();
586  }
587  #endif
588  static malloc_mutex_t *
589  prof_gctx_mutex_choose(void) {
590  	unsigned ngctxs = atomic_fetch_add_u(&amp;cum_gctxs, 1, ATOMIC_RELAXED);
591  	return &amp;gctx_locks[(ngctxs - 1) % PROF_NCTX_LOCKS];
592  }
593  static malloc_mutex_t *
594  prof_tdata_mutex_choose(uint64_t thr_uid) {
595  	return &amp;tdata_locks[thr_uid % PROF_NTDATA_LOCKS];
596  }
597  static prof_gctx_t *
598  prof_gctx_create(tsdn_t *tsdn, prof_bt_t *bt) {
599  	size_t size = offsetof(prof_gctx_t, vec) + (bt-&gt;len * sizeof(void *));
600  	prof_gctx_t *gctx = (prof_gctx_t *)iallocztm(tsdn, size,
601  	    sz_size2index(size), false, NULL, true, arena_get(TSDN_NULL, 0, true),
602  	    true);
603  	if (gctx == NULL) {
604  		return NULL;
605  	}
606  	gctx-&gt;lock = prof_gctx_mutex_choose();
607  	gctx-&gt;nlimbo = 1;
608  	tctx_tree_new(&amp;gctx-&gt;tctxs);
609  	memcpy(gctx-&gt;vec, bt-&gt;vec, bt-&gt;len * sizeof(void *));
610  	gctx-&gt;bt.vec = gctx-&gt;vec;
611  	gctx-&gt;bt.len = bt-&gt;len;
612  	return gctx;
613  }
614  static void
615  prof_gctx_try_destroy(tsd_t *tsd, prof_tdata_t *tdata_self, prof_gctx_t *gctx,
616      prof_tdata_t *tdata) {
617  	cassert(config_prof);
618  	prof_enter(tsd, tdata_self);
619  	malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
620  	assert(gctx-&gt;nlimbo != 0);
621  	if (tctx_tree_empty(&amp;gctx-&gt;tctxs) &amp;&amp; gctx-&gt;nlimbo == 1) {
622  		if (ckh_remove(tsd, &amp;bt2gctx, &amp;gctx-&gt;bt, NULL, NULL)) {
623  			not_reached();
624  		}
625  		prof_leave(tsd, tdata_self);
626  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
627  		idalloctm(tsd_tsdn(tsd), gctx, NULL, NULL, true, true);
628  	} else {
629  		gctx-&gt;nlimbo--;
630  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
631  		prof_leave(tsd, tdata_self);
632  	}
633  }
634  static bool
635  prof_tctx_should_destroy(tsdn_t *tsdn, prof_tctx_t *tctx) {
636  	malloc_mutex_assert_owner(tsdn, tctx-&gt;tdata-&gt;lock);
637  	if (opt_prof_accum) {
638  		return false;
639  	}
640  	if (tctx-&gt;cnts.curobjs != 0) {
641  		return false;
642  	}
643  	if (tctx-&gt;prepared) {
644  		return false;
645  	}
646  	return true;
647  }
648  static bool
649  prof_gctx_should_destroy(prof_gctx_t *gctx) {
650  	if (opt_prof_accum) {
651  		return false;
652  	}
653  	if (!tctx_tree_empty(&amp;gctx-&gt;tctxs)) {
654  		return false;
655  	}
656  	if (gctx-&gt;nlimbo != 0) {
657  		return false;
658  	}
659  	return true;
660  }
661  static void
662  prof_tctx_destroy(tsd_t *tsd, prof_tctx_t *tctx) {
663  	prof_tdata_t *tdata = tctx-&gt;tdata;
664  	prof_gctx_t *gctx = tctx-&gt;gctx;
665  	bool destroy_tdata, destroy_tctx, destroy_gctx;
666  	malloc_mutex_assert_owner(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
667  	assert(tctx-&gt;cnts.curobjs == 0);
668  	assert(tctx-&gt;cnts.curbytes == 0);
669  	assert(!opt_prof_accum);
670  	assert(tctx-&gt;cnts.accumobjs == 0);
671  	assert(tctx-&gt;cnts.accumbytes == 0);
672  	ckh_remove(tsd, &amp;tdata-&gt;bt2tctx, &amp;gctx-&gt;bt, NULL, NULL);
673  	destroy_tdata = prof_tdata_should_destroy(tsd_tsdn(tsd), tdata, false);
674  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
675  	malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
676  	switch (tctx-&gt;state) {
677  	case prof_tctx_state_nominal:
678  		tctx_tree_remove(&amp;gctx-&gt;tctxs, tctx);
679  		destroy_tctx = true;
680  		if (prof_gctx_should_destroy(gctx)) {
681  			gctx-&gt;nlimbo++;
682  			destroy_gctx = true;
683  		} else {
684  			destroy_gctx = false;
685  		}
686  		break;
687  	case prof_tctx_state_dumping:
688  		tctx-&gt;state = prof_tctx_state_purgatory;
689  		destroy_tctx = false;
690  		destroy_gctx = false;
691  		break;
692  	default:
693  		not_reached();
694  		destroy_tctx = false;
695  		destroy_gctx = false;
696  	}
697  	malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
698  	if (destroy_gctx) {
699  		prof_gctx_try_destroy(tsd, prof_tdata_get(tsd, false), gctx,
700  		    tdata);
701  	}
702  	malloc_mutex_assert_not_owner(tsd_tsdn(tsd), tctx-&gt;tdata-&gt;lock);
703  	if (destroy_tdata) {
704  		prof_tdata_destroy(tsd, tdata, false);
705  	}
706  	if (destroy_tctx) {
707  		idalloctm(tsd_tsdn(tsd), tctx, NULL, NULL, true, true);
708  	}
709  }
710  static bool
711  prof_lookup_global(tsd_t *tsd, prof_bt_t *bt, prof_tdata_t *tdata,
712      void **p_btkey, prof_gctx_t **p_gctx, bool *p_new_gctx) {
713  	union {
714  		prof_gctx_t	*p;
715  		void		*v;
716  	} gctx, tgctx;
717  	union {
718  		prof_bt_t	*p;
719  		void		*v;
720  	} btkey;
721  	bool new_gctx;
722  	prof_enter(tsd, tdata);
723  	if (ckh_search(&amp;bt2gctx, bt, &amp;btkey.v, &amp;gctx.v)) {
724  		prof_leave(tsd, tdata);
725  		tgctx.p = prof_gctx_create(tsd_tsdn(tsd), bt);
726  		if (tgctx.v == NULL) {
727  			return true;
728  		}
729  		prof_enter(tsd, tdata);
730  		if (ckh_search(&amp;bt2gctx, bt, &amp;btkey.v, &amp;gctx.v)) {
731  			gctx.p = tgctx.p;
732  			btkey.p = &amp;gctx.p-&gt;bt;
733  			if (ckh_insert(tsd, &amp;bt2gctx, btkey.v, gctx.v)) {
734  				prof_leave(tsd, tdata);
735  				idalloctm(tsd_tsdn(tsd), gctx.v, NULL, NULL,
736  				    true, true);
737  				return true;
738  			}
739  			new_gctx = true;
740  		} else {
741  			new_gctx = false;
742  		}
743  	} else {
744  		tgctx.v = NULL;
745  		new_gctx = false;
746  	}
747  	if (!new_gctx) {
748  		malloc_mutex_lock(tsd_tsdn(tsd), gctx.p-&gt;lock);
749  		gctx.p-&gt;nlimbo++;
750  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx.p-&gt;lock);
751  		new_gctx = false;
752  		if (tgctx.v != NULL) {
753  			idalloctm(tsd_tsdn(tsd), tgctx.v, NULL, NULL, true,
754  			    true);
755  		}
756  	}
757  	prof_leave(tsd, tdata);
758  	*p_btkey = btkey.v;
759  	*p_gctx = gctx.p;
760  	*p_new_gctx = new_gctx;
761  	return false;
762  }
763  prof_tctx_t *
764  prof_lookup(tsd_t *tsd, prof_bt_t *bt) {
765  	union {
766  		prof_tctx_t	*p;
767  		void		*v;
768  	} ret;
769  	prof_tdata_t *tdata;
770  	bool not_found;
771  	cassert(config_prof);
772  	tdata = prof_tdata_get(tsd, false);
773  	if (tdata == NULL) {
774  		return NULL;
775  	}
776  	malloc_mutex_lock(tsd_tsdn(tsd), tdata-&gt;lock);
777  	not_found = ckh_search(&amp;tdata-&gt;bt2tctx, bt, NULL, &amp;ret.v);
778  	if (!not_found) { &amp;bsol;* Note double negative! */
779  		ret.p-&gt;prepared = true;
780  	}
781  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
782  	if (not_found) {
783  		void *btkey;
784  		prof_gctx_t *gctx;
785  		bool new_gctx, error;
786  		if (prof_lookup_global(tsd, bt, tdata, &amp;btkey, &amp;gctx,
787  		    &amp;new_gctx)) {
788  			return NULL;
789  		}
790  		ret.v = iallocztm(tsd_tsdn(tsd), sizeof(prof_tctx_t),
791  		    sz_size2index(sizeof(prof_tctx_t)), false, NULL, true,
792  		    arena_ichoose(tsd, NULL), true);
793  		if (ret.p == NULL) {
794  			if (new_gctx) {
795  				prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
796  			}
797  			return NULL;
798  		}
799  		ret.p-&gt;tdata = tdata;
800  		ret.p-&gt;thr_uid = tdata-&gt;thr_uid;
801  		ret.p-&gt;thr_discrim = tdata-&gt;thr_discrim;
802  		memset(&amp;ret.p-&gt;cnts, 0, sizeof(prof_cnt_t));
803  		ret.p-&gt;gctx = gctx;
804  		ret.p-&gt;tctx_uid = tdata-&gt;tctx_uid_next++;
805  		ret.p-&gt;prepared = true;
806  		ret.p-&gt;state = prof_tctx_state_initializing;
807  		malloc_mutex_lock(tsd_tsdn(tsd), tdata-&gt;lock);
808  		error = ckh_insert(tsd, &amp;tdata-&gt;bt2tctx, btkey, ret.v);
809  		malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
810  		if (error) {
811  			if (new_gctx) {
812  				prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
813  			}
814  			idalloctm(tsd_tsdn(tsd), ret.v, NULL, NULL, true, true);
815  			return NULL;
816  		}
817  		malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
818  		ret.p-&gt;state = prof_tctx_state_nominal;
819  		tctx_tree_insert(&amp;gctx-&gt;tctxs, ret.p);
820  		gctx-&gt;nlimbo--;
821  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
822  	}
823  	return ret.p;
824  }
825  void
826  prof_sample_threshold_update(prof_tdata_t *tdata) {
827  #ifdef JEMALLOC_PROF
828  	if (!config_prof) {
829  		return;
830  	}
831  	if (lg_prof_sample == 0) {
832  		tsd_bytes_until_sample_set(tsd_fetch(), 0);
833  		return;
834  	}
835  	uint64_t r = prng_lg_range_u64(&amp;tdata-&gt;prng_state, 53);
836  	double u = (double)r * (1.0/9007199254740992.0L);
837  	uint64_t bytes_until_sample = (uint64_t)(log(u) /
838  	    log(1.0 - (1.0 / (double)((uint64_t)1U &lt;&lt; lg_prof_sample))))
839  	    + (uint64_t)1U;
840  	if (bytes_until_sample &gt; SSIZE_MAX) {
841  		bytes_until_sample = SSIZE_MAX;
842  	}
843  	tsd_bytes_until_sample_set(tsd_fetch(), bytes_until_sample);
844  #endif
845  }
846  #ifdef JEMALLOC_JET
847  static prof_tdata_t *
848  prof_tdata_count_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
849      void *arg) {
850  	size_t *tdata_count = (size_t *)arg;
851  	(*tdata_count)++;
852  	return NULL;
853  }
854  size_t
855  prof_tdata_count(void) {
856  	size_t tdata_count = 0;
857  	tsdn_t *tsdn;
858  	tsdn = tsdn_fetch();
859  	malloc_mutex_lock(tsdn, &amp;tdatas_mtx);
860  	tdata_tree_iter(&amp;tdatas, NULL, prof_tdata_count_iter,
861  	    (void *)&amp;tdata_count);
862  	malloc_mutex_unlock(tsdn, &amp;tdatas_mtx);
863  	return tdata_count;
864  }
865  size_t
866  prof_bt_count(void) {
867  	size_t bt_count;
868  	tsd_t *tsd;
869  	prof_tdata_t *tdata;
870  	tsd = tsd_fetch();
871  	tdata = prof_tdata_get(tsd, false);
872  	if (tdata == NULL) {
873  		return 0;
874  	}
875  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
876  	bt_count = ckh_count(&amp;bt2gctx);
877  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;bt2gctx_mtx);
878  	return bt_count;
879  }
880  #endif
881  static int
882  prof_dump_open_impl(bool propagate_err, const char *filename) {
883  	int fd;
884  	fd = creat(filename, 0644);
885  	if (fd == -1 &amp;&amp; !propagate_err) {
886  		malloc_printf(&quot;&lt;jemalloc&gt;: creat(\&quot;%s\&quot;), 0644) failed\n&quot;,
887  		    filename);
888  		if (opt_abort) {
889  			abort();
890  		}
891  	}
892  	return fd;
893  }
894  prof_dump_open_t *JET_MUTABLE prof_dump_open = prof_dump_open_impl;
895  static bool
896  prof_dump_flush(bool propagate_err) {
897  	bool ret = false;
898  	ssize_t err;
899  	cassert(config_prof);
900  	err = malloc_write_fd(prof_dump_fd, prof_dump_buf, prof_dump_buf_end);
901  	if (err == -1) {
902  		if (!propagate_err) {
903  			malloc_write(&quot;&lt;jemalloc&gt;: write() failed during heap &quot;
904  			    &quot;profile flush\n&quot;);
905  			if (opt_abort) {
906  				abort();
907  			}
908  		}
909  		ret = true;
910  	}
911  	prof_dump_buf_end = 0;
912  	return ret;
913  }
914  static bool
915  prof_dump_close(bool propagate_err) {
916  	bool ret;
917  	assert(prof_dump_fd != -1);
918  	ret = prof_dump_flush(propagate_err);
919  	close(prof_dump_fd);
920  	prof_dump_fd = -1;
921  	return ret;
922  }
923  static bool
924  prof_dump_write(bool propagate_err, const char *s) {
925  	size_t i, slen, n;
926  	cassert(config_prof);
927  	i = 0;
928  	slen = strlen(s);
929  	while (i &lt; slen) {
930  		if (prof_dump_buf_end == PROF_DUMP_BUFSIZE) {
931  			if (prof_dump_flush(propagate_err) &amp;&amp; propagate_err) {
932  				return true;
933  			}
934  		}
935  		if (prof_dump_buf_end + slen - i &lt;= PROF_DUMP_BUFSIZE) {
936  			n = slen - i;
937  		} else {
938  			n = PROF_DUMP_BUFSIZE - prof_dump_buf_end;
939  		}
940  		memcpy(&amp;prof_dump_buf[prof_dump_buf_end], &amp;s[i], n);
941  		prof_dump_buf_end += n;
942  		i += n;
943  	}
944  	assert(i == slen);
945  	return false;
946  }
947  JEMALLOC_FORMAT_PRINTF(2, 3)
948  static bool
949  prof_dump_printf(bool propagate_err, const char *format, ...) {
950  	bool ret;
951  	va_list ap;
952  	char buf[PROF_PRINTF_BUFSIZE];
953  	va_start(ap, format);
954  	malloc_vsnprintf(buf, sizeof(buf), format, ap);
955  	va_end(ap);
956  	ret = prof_dump_write(propagate_err, buf);
957  	return ret;
958  }
959  static void
960  prof_tctx_merge_tdata(tsdn_t *tsdn, prof_tctx_t *tctx, prof_tdata_t *tdata) {
961  	malloc_mutex_assert_owner(tsdn, tctx-&gt;tdata-&gt;lock);
962  	malloc_mutex_lock(tsdn, tctx-&gt;gctx-&gt;lock);
963  	switch (tctx-&gt;state) {
964  	case prof_tctx_state_initializing:
965  		malloc_mutex_unlock(tsdn, tctx-&gt;gctx-&gt;lock);
966  		return;
967  	case prof_tctx_state_nominal:
968  		tctx-&gt;state = prof_tctx_state_dumping;
969  		malloc_mutex_unlock(tsdn, tctx-&gt;gctx-&gt;lock);
970  		memcpy(&amp;tctx-&gt;dump_cnts, &amp;tctx-&gt;cnts, sizeof(prof_cnt_t));
971  		tdata-&gt;cnt_summed.curobjs += tctx-&gt;dump_cnts.curobjs;
972  		tdata-&gt;cnt_summed.curbytes += tctx-&gt;dump_cnts.curbytes;
973  		if (opt_prof_accum) {
974  			tdata-&gt;cnt_summed.accumobjs +=
975  			    tctx-&gt;dump_cnts.accumobjs;
976  			tdata-&gt;cnt_summed.accumbytes +=
977  			    tctx-&gt;dump_cnts.accumbytes;
978  		}
979  		break;
980  	case prof_tctx_state_dumping:
981  	case prof_tctx_state_purgatory:
982  		not_reached();
983  	}
984  }
985  static void
986  prof_tctx_merge_gctx(tsdn_t *tsdn, prof_tctx_t *tctx, prof_gctx_t *gctx) {
987  	malloc_mutex_assert_owner(tsdn, gctx-&gt;lock);
988  	gctx-&gt;cnt_summed.curobjs += tctx-&gt;dump_cnts.curobjs;
989  	gctx-&gt;cnt_summed.curbytes += tctx-&gt;dump_cnts.curbytes;
990  	if (opt_prof_accum) {
991  		gctx-&gt;cnt_summed.accumobjs += tctx-&gt;dump_cnts.accumobjs;
992  		gctx-&gt;cnt_summed.accumbytes += tctx-&gt;dump_cnts.accumbytes;
993  	}
994  }
995  static prof_tctx_t *
996  prof_tctx_merge_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *arg) {
997  	tsdn_t *tsdn = (tsdn_t *)arg;
998  	malloc_mutex_assert_owner(tsdn, tctx-&gt;gctx-&gt;lock);
999  	switch (tctx-&gt;state) {
1000  	case prof_tctx_state_nominal:
1001  		break;
1002  	case prof_tctx_state_dumping:
1003  	case prof_tctx_state_purgatory:
1004  		prof_tctx_merge_gctx(tsdn, tctx, tctx-&gt;gctx);
1005  		break;
1006  	default:
1007  		not_reached();
1008  	}
1009  	return NULL;
1010  }
1011  struct prof_tctx_dump_iter_arg_s {
1012  	tsdn_t	*tsdn;
1013  	bool	propagate_err;
1014  };
1015  static prof_tctx_t *
1016  prof_tctx_dump_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *opaque) {
1017  	struct prof_tctx_dump_iter_arg_s *arg =
1018  	    (struct prof_tctx_dump_iter_arg_s *)opaque;
1019  	malloc_mutex_assert_owner(arg-&gt;tsdn, tctx-&gt;gctx-&gt;lock);
1020  	switch (tctx-&gt;state) {
1021  	case prof_tctx_state_initializing:
1022  	case prof_tctx_state_nominal:
1023  		break;
1024  	case prof_tctx_state_dumping:
1025  	case prof_tctx_state_purgatory:
1026  		if (prof_dump_printf(arg-&gt;propagate_err,
1027  		    &quot;  t%&quot;FMTu64&quot;: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: &quot;
1028  		    &quot;%&quot;FMTu64&quot;]\n&quot;, tctx-&gt;thr_uid, tctx-&gt;dump_cnts.curobjs,
1029  		    tctx-&gt;dump_cnts.curbytes, tctx-&gt;dump_cnts.accumobjs,
1030  		    tctx-&gt;dump_cnts.accumbytes)) {
1031  			return tctx;
1032  		}
1033  		break;
1034  	default:
1035  		not_reached();
1036  	}
1037  	return NULL;
1038  }
1039  static prof_tctx_t *
1040  prof_tctx_finish_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *arg) {
1041  	tsdn_t *tsdn = (tsdn_t *)arg;
1042  	prof_tctx_t *ret;
1043  	malloc_mutex_assert_owner(tsdn, tctx-&gt;gctx-&gt;lock);
1044  	switch (tctx-&gt;state) {
1045  	case prof_tctx_state_nominal:
1046  		break;
1047  	case prof_tctx_state_dumping:
1048  		tctx-&gt;state = prof_tctx_state_nominal;
1049  		break;
1050  	case prof_tctx_state_purgatory:
1051  		ret = tctx;
1052  		goto label_return;
1053  	default:
1054  		not_reached();
1055  	}
1056  	ret = NULL;
1057  label_return:
1058  	return ret;
1059  }
1060  static void
1061  prof_dump_gctx_prep(tsdn_t *tsdn, prof_gctx_t *gctx, prof_gctx_tree_t *gctxs) {
1062  	cassert(config_prof);
1063  	malloc_mutex_lock(tsdn, gctx-&gt;lock);
1064  	gctx-&gt;nlimbo++;
1065  	gctx_tree_insert(gctxs, gctx);
1066  	memset(&amp;gctx-&gt;cnt_summed, 0, sizeof(prof_cnt_t));
1067  	malloc_mutex_unlock(tsdn, gctx-&gt;lock);
1068  }
1069  struct prof_gctx_merge_iter_arg_s {
1070  	tsdn_t	*tsdn;
1071  	size_t	leak_ngctx;
1072  };
1073  static prof_gctx_t *
1074  prof_gctx_merge_iter(prof_gctx_tree_t *gctxs, prof_gctx_t *gctx, void *opaque) {
1075  	struct prof_gctx_merge_iter_arg_s *arg =
1076  	    (struct prof_gctx_merge_iter_arg_s *)opaque;
1077  	malloc_mutex_lock(arg-&gt;tsdn, gctx-&gt;lock);
1078  	tctx_tree_iter(&amp;gctx-&gt;tctxs, NULL, prof_tctx_merge_iter,
1079  	    (void *)arg-&gt;tsdn);
1080  	if (gctx-&gt;cnt_summed.curobjs != 0) {
1081  		arg-&gt;leak_ngctx++;
1082  	}
1083  	malloc_mutex_unlock(arg-&gt;tsdn, gctx-&gt;lock);
1084  	return NULL;
1085  }
1086  static void
1087  prof_gctx_finish(tsd_t *tsd, prof_gctx_tree_t *gctxs) {
1088  	prof_tdata_t *tdata = prof_tdata_get(tsd, false);
1089  	prof_gctx_t *gctx;
1090  	while ((gctx = gctx_tree_first(gctxs)) != NULL) {
1091  		gctx_tree_remove(gctxs, gctx);
1092  		malloc_mutex_lock(tsd_tsdn(tsd), gctx-&gt;lock);
1093  		{
1094  			prof_tctx_t *next;
1095  			next = NULL;
1096  			do {
1097  				prof_tctx_t *to_destroy =
1098  				    tctx_tree_iter(&amp;gctx-&gt;tctxs, next,
1099  				    prof_tctx_finish_iter,
1100  				    (void *)tsd_tsdn(tsd));
1101  				if (to_destroy != NULL) {
1102  					next = tctx_tree_next(&amp;gctx-&gt;tctxs,
1103  					    to_destroy);
1104  					tctx_tree_remove(&amp;gctx-&gt;tctxs,
1105  					    to_destroy);
1106  					idalloctm(tsd_tsdn(tsd), to_destroy,
1107  					    NULL, NULL, true, true);
1108  				} else {
1109  					next = NULL;
1110  				}
1111  			} while (next != NULL);
1112  		}
1113  		gctx-&gt;nlimbo--;
1114  		if (prof_gctx_should_destroy(gctx)) {
1115  			gctx-&gt;nlimbo++;
1116  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
1117  			prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
1118  		} else {
1119  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx-&gt;lock);
1120  		}
1121  	}
1122  }
1123  struct prof_tdata_merge_iter_arg_s {
1124  	tsdn_t		*tsdn;
1125  	prof_cnt_t	cnt_all;
1126  };
1127  static prof_tdata_t *
1128  prof_tdata_merge_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1129      void *opaque) {
1130  	struct prof_tdata_merge_iter_arg_s *arg =
1131  	    (struct prof_tdata_merge_iter_arg_s *)opaque;
1132  	malloc_mutex_lock(arg-&gt;tsdn, tdata-&gt;lock);
1133  	if (!tdata-&gt;expired) {
1134  		size_t tabind;
1135  		union {
1136  			prof_tctx_t	*p;
1137  			void		*v;
1138  		} tctx;
1139  		tdata-&gt;dumping = true;
1140  		memset(&amp;tdata-&gt;cnt_summed, 0, sizeof(prof_cnt_t));
1141  		for (tabind = 0; !ckh_iter(&amp;tdata-&gt;bt2tctx, &amp;tabind, NULL,
1142  		    &amp;tctx.v);) {
1143  			prof_tctx_merge_tdata(arg-&gt;tsdn, tctx.p, tdata);
1144  		}
1145  		arg-&gt;cnt_all.curobjs += tdata-&gt;cnt_summed.curobjs;
1146  		arg-&gt;cnt_all.curbytes += tdata-&gt;cnt_summed.curbytes;
1147  		if (opt_prof_accum) {
1148  			arg-&gt;cnt_all.accumobjs += tdata-&gt;cnt_summed.accumobjs;
1149  			arg-&gt;cnt_all.accumbytes += tdata-&gt;cnt_summed.accumbytes;
1150  		}
1151  	} else {
1152  		tdata-&gt;dumping = false;
1153  	}
1154  	malloc_mutex_unlock(arg-&gt;tsdn, tdata-&gt;lock);
1155  	return NULL;
1156  }
1157  static prof_tdata_t *
1158  prof_tdata_dump_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1159      void *arg) {
1160  	bool propagate_err = *(bool *)arg;
1161  	if (!tdata-&gt;dumping) {
1162  		return NULL;
1163  	}
1164  	if (prof_dump_printf(propagate_err,
1165  	    &quot;  t%&quot;FMTu64&quot;: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: %&quot;FMTu64&quot;]%s%s\n&quot;,
1166  	    tdata-&gt;thr_uid, tdata-&gt;cnt_summed.curobjs,
1167  	    tdata-&gt;cnt_summed.curbytes, tdata-&gt;cnt_summed.accumobjs,
1168  	    tdata-&gt;cnt_summed.accumbytes,
1169  	    (tdata-&gt;thread_name != NULL) ? &quot; &quot; : &quot;&quot;,
1170  	    (tdata-&gt;thread_name != NULL) ? tdata-&gt;thread_name : &quot;&quot;)) {
1171  		return tdata;
1172  	}
1173  	return NULL;
1174  }
1175  static bool
1176  prof_dump_header_impl(tsdn_t *tsdn, bool propagate_err,
1177      const prof_cnt_t *cnt_all) {
1178  	bool ret;
1179  	if (prof_dump_printf(propagate_err,
1180  	    &quot;heap_v2/%&quot;FMTu64&quot;\n&quot;
1181  	    &quot;  t*: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: %&quot;FMTu64&quot;]\n&quot;,
1182  	    ((uint64_t)1U &lt;&lt; lg_prof_sample), cnt_all-&gt;curobjs,
1183  	    cnt_all-&gt;curbytes, cnt_all-&gt;accumobjs, cnt_all-&gt;accumbytes)) {
1184  		return true;
1185  	}
1186  	malloc_mutex_lock(tsdn, &amp;tdatas_mtx);
1187  	ret = (tdata_tree_iter(&amp;tdatas, NULL, prof_tdata_dump_iter,
1188  	    (void *)&amp;propagate_err) != NULL);
1189  	malloc_mutex_unlock(tsdn, &amp;tdatas_mtx);
1190  	return ret;
1191  }
1192  prof_dump_header_t *JET_MUTABLE prof_dump_header = prof_dump_header_impl;
1193  static bool
1194  prof_dump_gctx(tsdn_t *tsdn, bool propagate_err, prof_gctx_t *gctx,
1195      const prof_bt_t *bt, prof_gctx_tree_t *gctxs) {
1196  	bool ret;
1197  	unsigned i;
1198  	struct prof_tctx_dump_iter_arg_s prof_tctx_dump_iter_arg;
1199  	cassert(config_prof);
1200  	malloc_mutex_assert_owner(tsdn, gctx-&gt;lock);
1201  	if ((!opt_prof_accum &amp;&amp; gctx-&gt;cnt_summed.curobjs == 0) ||
1202  	    (opt_prof_accum &amp;&amp; gctx-&gt;cnt_summed.accumobjs == 0)) {
1203  		assert(gctx-&gt;cnt_summed.curobjs == 0);
1204  		assert(gctx-&gt;cnt_summed.curbytes == 0);
1205  		assert(gctx-&gt;cnt_summed.accumobjs == 0);
1206  		assert(gctx-&gt;cnt_summed.accumbytes == 0);
1207  		ret = false;
1208  		goto label_return;
1209  	}
1210  	if (prof_dump_printf(propagate_err, &quot;@&quot;)) {
1211  		ret = true;
1212  		goto label_return;
1213  	}
1214  	for (i = 0; i &lt; bt-&gt;len; i++) {
1215  		if (prof_dump_printf(propagate_err, &quot; %#&quot;FMTxPTR,
1216  		    (uintptr_t)bt-&gt;vec[i])) {
1217  			ret = true;
1218  			goto label_return;
1219  		}
1220  	}
1221  	if (prof_dump_printf(propagate_err,
1222  	    &quot;\n&quot;
1223  	    &quot;  t*: %&quot;FMTu64&quot;: %&quot;FMTu64&quot; [%&quot;FMTu64&quot;: %&quot;FMTu64&quot;]\n&quot;,
1224  	    gctx-&gt;cnt_summed.curobjs, gctx-&gt;cnt_summed.curbytes,
1225  	    gctx-&gt;cnt_summed.accumobjs, gctx-&gt;cnt_summed.accumbytes)) {
1226  		ret = true;
1227  		goto label_return;
1228  	}
1229  	prof_tctx_dump_iter_arg.tsdn = tsdn;
1230  	prof_tctx_dump_iter_arg.propagate_err = propagate_err;
1231  	if (tctx_tree_iter(&amp;gctx-&gt;tctxs, NULL, prof_tctx_dump_iter,
1232  	    (void *)&amp;prof_tctx_dump_iter_arg) != NULL) {
1233  		ret = true;
1234  		goto label_return;
1235  	}
1236  	ret = false;
1237  label_return:
1238  	return ret;
1239  }
1240  #ifndef _WIN32
1241  JEMALLOC_FORMAT_PRINTF(1, 2)
1242  static int
1243  prof_open_maps(const char *format, ...) {
1244  	int mfd;
1245  	va_list ap;
1246  	char filename[PATH_MAX + 1];
1247  	va_start(ap, format);
1248  	malloc_vsnprintf(filename, sizeof(filename), format, ap);
1249  	va_end(ap);
1250  #if defined(O_CLOEXEC)
1251  	mfd = open(filename, O_RDONLY | O_CLOEXEC);
1252  #else
1253  	mfd = open(filename, O_RDONLY);
1254  	if (mfd != -1) {
1255  		fcntl(mfd, F_SETFD, fcntl(mfd, F_GETFD) | FD_CLOEXEC);
1256  	}
1257  #endif
1258  	return mfd;
1259  }
1260  #endif
1261  static int
1262  prof_getpid(void) {
1263  #ifdef _WIN32
1264  	return GetCurrentProcessId();
1265  #else
1266  	return getpid();
1267  #endif
1268  }
1269  static bool
1270  prof_dump_maps(bool propagate_err) {
1271  	bool ret;
1272  	int mfd;
1273  	cassert(config_prof);
1274  #ifdef __FreeBSD__
1275  	mfd = prof_open_maps(&quot;/proc/curproc/map&quot;);
1276  #elif defined(_WIN32)
1277  	mfd = -1; 
1278  #else
1279  	{
1280  		int pid = prof_getpid();
1281  		mfd = prof_open_maps(&quot;/proc/%d/task/%d/maps&quot;, pid, pid);
1282  		if (mfd == -1) {
1283  			mfd = prof_open_maps(&quot;/proc/%d/maps&quot;, pid);
1284  		}
1285  	}
1286  #endif
1287  	if (mfd != -1) {
1288  		ssize_t nread;
1289  		if (prof_dump_write(propagate_err, &quot;\nMAPPED_LIBRARIES:\n&quot;) &amp;&amp;
1290  		    propagate_err) {
1291  			ret = true;
1292  			goto label_return;
1293  		}
1294  		nread = 0;
1295  		do {
1296  			prof_dump_buf_end += nread;
1297  			if (prof_dump_buf_end == PROF_DUMP_BUFSIZE) {
1298  				if (prof_dump_flush(propagate_err) &amp;&amp;
1299  				    propagate_err) {
1300  					ret = true;
1301  					goto label_return;
1302  				}
1303  			}
1304  			nread = malloc_read_fd(mfd,
1305  			    &amp;prof_dump_buf[prof_dump_buf_end], PROF_DUMP_BUFSIZE
1306  			    - prof_dump_buf_end);
1307  		} while (nread &gt; 0);
1308  	} else {
1309  		ret = true;
1310  		goto label_return;
1311  	}
1312  	ret = false;
1313  label_return:
1314  	if (mfd != -1) {
1315  		close(mfd);
1316  	}
1317  	return ret;
1318  }
1319  static void
1320  prof_leakcheck(const prof_cnt_t *cnt_all, size_t leak_ngctx,
1321      const char *filename) {
1322  #ifdef JEMALLOC_PROF
1323  	if (cnt_all-&gt;curbytes != 0) {
1324  		double sample_period = (double)((uint64_t)1 &lt;&lt; lg_prof_sample);
1325  		double ratio = (((double)cnt_all-&gt;curbytes) /
1326  		    (double)cnt_all-&gt;curobjs) / sample_period;
1327  		double scale_factor = 1.0 / (1.0 - exp(-ratio));
1328  		uint64_t curbytes = (uint64_t)round(((double)cnt_all-&gt;curbytes)
1329  		    * scale_factor);
1330  		uint64_t curobjs = (uint64_t)round(((double)cnt_all-&gt;curobjs) *
1331  		    scale_factor);
1332  		malloc_printf(&quot;&lt;jemalloc&gt;: Leak approximation summary: ~%&quot;FMTu64
1333  		    &quot; byte%s, ~%&quot;FMTu64&quot; object%s, &gt;= %zu context%s\n&quot;,
1334  		    curbytes, (curbytes != 1) ? &quot;s&quot; : &quot;&quot;, curobjs, (curobjs !=
1335  		    1) ? &quot;s&quot; : &quot;&quot;, leak_ngctx, (leak_ngctx != 1) ? &quot;s&quot; : &quot;&quot;);
1336  		malloc_printf(
1337  		    &quot;&lt;jemalloc&gt;: Run jeprof on \&quot;%s\&quot; for leak detail\n&quot;,
1338  		    filename);
1339  	}
1340  #endif
1341  }
1342  struct prof_gctx_dump_iter_arg_s {
1343  	tsdn_t	*tsdn;
1344  	bool	propagate_err;
1345  };
1346  static prof_gctx_t *
1347  prof_gctx_dump_iter(prof_gctx_tree_t *gctxs, prof_gctx_t *gctx, void *opaque) {
1348  	prof_gctx_t *ret;
1349  	struct prof_gctx_dump_iter_arg_s *arg =
1350  	    (struct prof_gctx_dump_iter_arg_s *)opaque;
1351  	malloc_mutex_lock(arg-&gt;tsdn, gctx-&gt;lock);
1352  	if (prof_dump_gctx(arg-&gt;tsdn, arg-&gt;propagate_err, gctx, &amp;gctx-&gt;bt,
1353  	    gctxs)) {
1354  		ret = gctx;
1355  		goto label_return;
1356  	}
1357  	ret = NULL;
1358  label_return:
1359  	malloc_mutex_unlock(arg-&gt;tsdn, gctx-&gt;lock);
1360  	return ret;
1361  }
1362  static void
1363  prof_dump_prep(tsd_t *tsd, prof_tdata_t *tdata,
1364      struct prof_tdata_merge_iter_arg_s *prof_tdata_merge_iter_arg,
1365      struct prof_gctx_merge_iter_arg_s *prof_gctx_merge_iter_arg,
1366      prof_gctx_tree_t *gctxs) {
1367  	size_t tabind;
1368  	union {
1369  		prof_gctx_t	*p;
1370  		void		*v;
1371  	} gctx;
1372  	prof_enter(tsd, tdata);
1373  	gctx_tree_new(gctxs);
1374  	for (tabind = 0; !ckh_iter(&amp;bt2gctx, &amp;tabind, NULL, &amp;gctx.v);) {
1375  		prof_dump_gctx_prep(tsd_tsdn(tsd), gctx.p, gctxs);
1376  	}
1377  	prof_tdata_merge_iter_arg-&gt;tsdn = tsd_tsdn(tsd);
1378  	memset(&amp;prof_tdata_merge_iter_arg-&gt;cnt_all, 0, sizeof(prof_cnt_t));
1379  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1380  	tdata_tree_iter(&amp;tdatas, NULL, prof_tdata_merge_iter,
1381  	    (void *)prof_tdata_merge_iter_arg);
1382  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1383  	prof_gctx_merge_iter_arg-&gt;tsdn = tsd_tsdn(tsd);
1384  	prof_gctx_merge_iter_arg-&gt;leak_ngctx = 0;
1385  	gctx_tree_iter(gctxs, NULL, prof_gctx_merge_iter,
1386  	    (void *)prof_gctx_merge_iter_arg);
1387  	prof_leave(tsd, tdata);
1388  }
1389  static bool
1390  prof_dump_file(tsd_t *tsd, bool propagate_err, const char *filename,
1391      bool leakcheck, prof_tdata_t *tdata,
1392      struct prof_tdata_merge_iter_arg_s *prof_tdata_merge_iter_arg,
1393      struct prof_gctx_merge_iter_arg_s *prof_gctx_merge_iter_arg,
1394      struct prof_gctx_dump_iter_arg_s *prof_gctx_dump_iter_arg,
1395      prof_gctx_tree_t *gctxs) {
1396  	if ((prof_dump_fd = prof_dump_open(propagate_err, filename)) == -1) {
1397  		return true;
1398  	}
1399  	if (prof_dump_header(tsd_tsdn(tsd), propagate_err,
1400  	    &amp;prof_tdata_merge_iter_arg-&gt;cnt_all)) {
1401  		goto label_write_error;
1402  	}
1403  	prof_gctx_dump_iter_arg-&gt;tsdn = tsd_tsdn(tsd);
1404  	prof_gctx_dump_iter_arg-&gt;propagate_err = propagate_err;
1405  	if (gctx_tree_iter(gctxs, NULL, prof_gctx_dump_iter,
1406  	    (void *)prof_gctx_dump_iter_arg) != NULL) {
1407  		goto label_write_error;
1408  	}
1409  	if (prof_dump_maps(propagate_err)) {
1410  		goto label_write_error;
1411  	}
1412  	if (prof_dump_close(propagate_err)) {
1413  		return true;
1414  	}
1415  	return false;
1416  label_write_error:
1417  	prof_dump_close(propagate_err);
1418  	return true;
1419  }
1420  static bool
1421  prof_dump(tsd_t *tsd, bool propagate_err, const char *filename,
1422      bool leakcheck) {
1423  	cassert(config_prof);
1424  	assert(tsd_reentrancy_level_get(tsd) == 0);
1425  	prof_tdata_t * tdata = prof_tdata_get(tsd, true);
1426  	if (tdata == NULL) {
1427  		return true;
1428  	}
1429  	pre_reentrancy(tsd, NULL);
1430  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1431  	prof_gctx_tree_t gctxs;
1432  	struct prof_tdata_merge_iter_arg_s prof_tdata_merge_iter_arg;
1433  	struct prof_gctx_merge_iter_arg_s prof_gctx_merge_iter_arg;
1434  	struct prof_gctx_dump_iter_arg_s prof_gctx_dump_iter_arg;
1435  	prof_dump_prep(tsd, tdata, &amp;prof_tdata_merge_iter_arg,
1436  	    &amp;prof_gctx_merge_iter_arg, &amp;gctxs);
1437  	bool err = prof_dump_file(tsd, propagate_err, filename, leakcheck, tdata,
1438  	    &amp;prof_tdata_merge_iter_arg, &amp;prof_gctx_merge_iter_arg,
1439  	    &amp;prof_gctx_dump_iter_arg, &amp;gctxs);
1440  	prof_gctx_finish(tsd, &amp;gctxs);
1441  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1442  	post_reentrancy(tsd);
1443  	if (err) {
1444  		return true;
1445  	}
1446  	if (leakcheck) {
1447  		prof_leakcheck(&amp;prof_tdata_merge_iter_arg.cnt_all,
1448  		    prof_gctx_merge_iter_arg.leak_ngctx, filename);
1449  	}
1450  	return false;
1451  }
1452  #ifdef JEMALLOC_JET
1453  void
1454  prof_cnt_all(uint64_t *curobjs, uint64_t *curbytes, uint64_t *accumobjs,
1455      uint64_t *accumbytes) {
1456  	tsd_t *tsd;
1457  	prof_tdata_t *tdata;
1458  	struct prof_tdata_merge_iter_arg_s prof_tdata_merge_iter_arg;
1459  	struct prof_gctx_merge_iter_arg_s prof_gctx_merge_iter_arg;
1460  	prof_gctx_tree_t gctxs;
1461  	tsd = tsd_fetch();
1462  	tdata = prof_tdata_get(tsd, false);
1463  	if (tdata == NULL) {
1464  		if (curobjs != NULL) {
1465  			*curobjs = 0;
1466  		}
1467  		if (curbytes != NULL) {
1468  			*curbytes = 0;
1469  		}
1470  		if (accumobjs != NULL) {
1471  			*accumobjs = 0;
1472  		}
1473  		if (accumbytes != NULL) {
1474  			*accumbytes = 0;
1475  		}
1476  		return;
1477  	}
1478  	prof_dump_prep(tsd, tdata, &amp;prof_tdata_merge_iter_arg,
1479  	    &amp;prof_gctx_merge_iter_arg, &amp;gctxs);
1480  	prof_gctx_finish(tsd, &amp;gctxs);
1481  	if (curobjs != NULL) {
1482  		*curobjs = prof_tdata_merge_iter_arg.cnt_all.curobjs;
1483  	}
1484  	if (curbytes != NULL) {
1485  		*curbytes = prof_tdata_merge_iter_arg.cnt_all.curbytes;
1486  	}
1487  	if (accumobjs != NULL) {
1488  		*accumobjs = prof_tdata_merge_iter_arg.cnt_all.accumobjs;
1489  	}
1490  	if (accumbytes != NULL) {
1491  		*accumbytes = prof_tdata_merge_iter_arg.cnt_all.accumbytes;
1492  	}
1493  }
1494  #endif
1495  #define DUMP_FILENAME_BUFSIZE	(PATH_MAX + 1)
1496  #define VSEQ_INVALID		UINT64_C(0xffffffffffffffff)
1497  static void
1498  prof_dump_filename(char *filename, char v, uint64_t vseq) {
1499  	cassert(config_prof);
1500  	if (vseq != VSEQ_INVALID) {
1501  		malloc_snprintf(filename, DUMP_FILENAME_BUFSIZE,
1502  		    &quot;%s.%d.%&quot;FMTu64&quot;.%c%&quot;FMTu64&quot;.heap&quot;,
1503  		    opt_prof_prefix, prof_getpid(), prof_dump_seq, v, vseq);
1504  	} else {
1505  		malloc_snprintf(filename, DUMP_FILENAME_BUFSIZE,
1506  		    &quot;%s.%d.%&quot;FMTu64&quot;.%c.heap&quot;,
1507  		    opt_prof_prefix, prof_getpid(), prof_dump_seq, v);
1508  	}
1509  	prof_dump_seq++;
1510  }
1511  static void
1512  prof_fdump(void) {
1513  	tsd_t *tsd;
1514  	char filename[DUMP_FILENAME_BUFSIZE];
1515  	cassert(config_prof);
1516  	assert(opt_prof_final);
1517  	assert(opt_prof_prefix[0] != &#x27;\0&#x27;);
1518  	if (!prof_booted) {
1519  		return;
1520  	}
1521  	tsd = tsd_fetch();
1522  	assert(tsd_reentrancy_level_get(tsd) == 0);
1523  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1524  	prof_dump_filename(filename, &#x27;f&#x27;, VSEQ_INVALID);
1525  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1526  	prof_dump(tsd, false, filename, opt_prof_leak);
1527  }
1528  bool
1529  prof_accum_init(tsdn_t *tsdn, prof_accum_t *prof_accum) {
1530  	cassert(config_prof);
1531  #ifndef JEMALLOC_ATOMIC_U64
1532  	if (malloc_mutex_init(&amp;prof_accum-&gt;mtx, &quot;prof_accum&quot;,
1533  	    WITNESS_RANK_PROF_ACCUM, malloc_mutex_rank_exclusive)) {
1534  		return true;
1535  	}
1536  	prof_accum-&gt;accumbytes = 0;
1537  #else
1538  	atomic_store_u64(&amp;prof_accum-&gt;accumbytes, 0, ATOMIC_RELAXED);
1539  #endif
1540  	return false;
1541  }
1542  void
1543  prof_idump(tsdn_t *tsdn) {
1544  	tsd_t *tsd;
1545  	prof_tdata_t *tdata;
1546  	cassert(config_prof);
1547  	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
1548  		return;
1549  	}
1550  	tsd = tsdn_tsd(tsdn);
1551  	if (tsd_reentrancy_level_get(tsd) &gt; 0) {
1552  		return;
1553  	}
1554  	tdata = prof_tdata_get(tsd, false);
1555  	if (tdata == NULL) {
1556  		return;
1557  	}
1558  	if (tdata-&gt;enq) {
1559  		tdata-&gt;enq_idump = true;
1560  		return;
1561  	}
1562  	if (opt_prof_prefix[0] != &#x27;\0&#x27;) {
1563  		char filename[PATH_MAX + 1];
1564  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1565  		prof_dump_filename(filename, &#x27;i&#x27;, prof_dump_iseq);
1566  		prof_dump_iseq++;
1567  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1568  		prof_dump(tsd, false, filename, false);
1569  	}
1570  }
1571  bool
1572  prof_mdump(tsd_t *tsd, const char *filename) {
1573  	cassert(config_prof);
1574  	assert(tsd_reentrancy_level_get(tsd) == 0);
1575  	if (!opt_prof || !prof_booted) {
1576  		return true;
1577  	}
1578  	char filename_buf[DUMP_FILENAME_BUFSIZE];
1579  	if (filename == NULL) {
1580  		if (opt_prof_prefix[0] == &#x27;\0&#x27;) {
1581  			return true;
1582  		}
1583  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1584  		prof_dump_filename(filename_buf, &#x27;m&#x27;, prof_dump_mseq);
1585  		prof_dump_mseq++;
1586  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_seq_mtx);
1587  		filename = filename_buf;
1588  	}
1589  	return prof_dump(tsd, true, filename, false);
1590  }
1591  void
1592  prof_gdump(tsdn_t *tsdn) {
1593  	tsd_t *tsd;
1594  	prof_tdata_t *tdata;
1595  	cassert(config_prof);
1596  	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
1597  		return;
1598  	}
1599  	tsd = tsdn_tsd(tsdn);
1600  	if (tsd_reentrancy_level_get(tsd) &gt; 0) {
1601  		return;
1602  	}
1603  	tdata = prof_tdata_get(tsd, false);
1604  	if (tdata == NULL) {
1605  		return;
1606  	}
1607  	if (tdata-&gt;enq) {
1608  		tdata-&gt;enq_gdump = true;
1609  		return;
1610  	}
1611  	if (opt_prof_prefix[0] != &#x27;\0&#x27;) {
1612  		char filename[DUMP_FILENAME_BUFSIZE];
1613  		malloc_mutex_lock(tsdn, &amp;prof_dump_seq_mtx);
1614  		prof_dump_filename(filename, &#x27;u&#x27;, prof_dump_useq);
1615  		prof_dump_useq++;
1616  		malloc_mutex_unlock(tsdn, &amp;prof_dump_seq_mtx);
1617  		prof_dump(tsd, false, filename, false);
1618  	}
1619  }
1620  static void
1621  prof_bt_hash(const void *key, size_t r_hash[2]) {
1622  	prof_bt_t *bt = (prof_bt_t *)key;
1623  	cassert(config_prof);
1624  	hash(bt-&gt;vec, bt-&gt;len * sizeof(void *), 0x94122f33U, r_hash);
1625  }
1626  static bool
1627  prof_bt_keycomp(const void *k1, const void *k2) {
1628  	const prof_bt_t *bt1 = (prof_bt_t *)k1;
1629  	const prof_bt_t *bt2 = (prof_bt_t *)k2;
1630  	cassert(config_prof);
1631  	if (bt1-&gt;len != bt2-&gt;len) {
1632  		return false;
1633  	}
1634  	return (memcmp(bt1-&gt;vec, bt2-&gt;vec, bt1-&gt;len * sizeof(void *)) == 0);
1635  }
1636  static void
1637  prof_bt_node_hash(const void *key, size_t r_hash[2]) {
1638  	const prof_bt_node_t *bt_node = (prof_bt_node_t *)key;
1639  	prof_bt_hash((void *)(&amp;bt_node-&gt;bt), r_hash);
1640  }
1641  static bool
1642  prof_bt_node_keycomp(const void *k1, const void *k2) {
1643  	const prof_bt_node_t *bt_node1 = (prof_bt_node_t *)k1;
1644  	const prof_bt_node_t *bt_node2 = (prof_bt_node_t *)k2;
1645  	return prof_bt_keycomp((void *)(&amp;bt_node1-&gt;bt),
1646  	    (void *)(&amp;bt_node2-&gt;bt));
1647  }
1648  static void
1649  prof_thr_node_hash(const void *key, size_t r_hash[2]) {
1650  	const prof_thr_node_t *thr_node = (prof_thr_node_t *)key;
1651  	hash(&amp;thr_node-&gt;thr_uid, sizeof(uint64_t), 0x94122f35U, r_hash);
1652  }
1653  static bool
1654  prof_thr_node_keycomp(const void *k1, const void *k2) {
1655  	const prof_thr_node_t *thr_node1 = (prof_thr_node_t *)k1;
1656  	const prof_thr_node_t *thr_node2 = (prof_thr_node_t *)k2;
1657  	return thr_node1-&gt;thr_uid == thr_node2-&gt;thr_uid;
1658  }
1659  static uint64_t
1660  prof_thr_uid_alloc(tsdn_t *tsdn) {
1661  	uint64_t thr_uid;
1662  	malloc_mutex_lock(tsdn, &amp;next_thr_uid_mtx);
1663  	thr_uid = next_thr_uid;
1664  	next_thr_uid++;
1665  	malloc_mutex_unlock(tsdn, &amp;next_thr_uid_mtx);
1666  	return thr_uid;
1667  }
1668  static prof_tdata_t *
1669  prof_tdata_init_impl(tsd_t *tsd, uint64_t thr_uid, uint64_t thr_discrim,
1670      char *thread_name, bool active) {
1671  	prof_tdata_t *tdata;
1672  	cassert(config_prof);
1673  	tdata = (prof_tdata_t *)iallocztm(tsd_tsdn(tsd), sizeof(prof_tdata_t),
1674  	    sz_size2index(sizeof(prof_tdata_t)), false, NULL, true,
1675  	    arena_get(TSDN_NULL, 0, true), true);
1676  	if (tdata == NULL) {
1677  		return NULL;
1678  	}
1679  	tdata-&gt;lock = prof_tdata_mutex_choose(thr_uid);
1680  	tdata-&gt;thr_uid = thr_uid;
1681  	tdata-&gt;thr_discrim = thr_discrim;
1682  	tdata-&gt;thread_name = thread_name;
1683  	tdata-&gt;attached = true;
1684  	tdata-&gt;expired = false;
1685  	tdata-&gt;tctx_uid_next = 0;
1686  	if (ckh_new(tsd, &amp;tdata-&gt;bt2tctx, PROF_CKH_MINITEMS, prof_bt_hash,
1687  	    prof_bt_keycomp)) {
1688  		idalloctm(tsd_tsdn(tsd), tdata, NULL, NULL, true, true);
1689  		return NULL;
1690  	}
1691  	tdata-&gt;prng_state = (uint64_t)(uintptr_t)tdata;
1692  	prof_sample_threshold_update(tdata);
1693  	tdata-&gt;enq = false;
1694  	tdata-&gt;enq_idump = false;
1695  	tdata-&gt;enq_gdump = false;
1696  	tdata-&gt;dumping = false;
1697  	tdata-&gt;active = active;
1698  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1699  	tdata_tree_insert(&amp;tdatas, tdata);
1700  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1701  	return tdata;
1702  }
1703  prof_tdata_t *
1704  prof_tdata_init(tsd_t *tsd) {
1705  	return prof_tdata_init_impl(tsd, prof_thr_uid_alloc(tsd_tsdn(tsd)), 0,
1706  	    NULL, prof_thread_active_init_get(tsd_tsdn(tsd)));
1707  }
1708  static bool
1709  prof_tdata_should_destroy_unlocked(prof_tdata_t *tdata, bool even_if_attached) {
1710  	if (tdata-&gt;attached &amp;&amp; !even_if_attached) {
1711  		return false;
1712  	}
1713  	if (ckh_count(&amp;tdata-&gt;bt2tctx) != 0) {
1714  		return false;
1715  	}
1716  	return true;
1717  }
1718  static bool
1719  prof_tdata_should_destroy(tsdn_t *tsdn, prof_tdata_t *tdata,
1720      bool even_if_attached) {
1721  	malloc_mutex_assert_owner(tsdn, tdata-&gt;lock);
1722  	return prof_tdata_should_destroy_unlocked(tdata, even_if_attached);
1723  }
1724  static void
1725  prof_tdata_destroy_locked(tsd_t *tsd, prof_tdata_t *tdata,
1726      bool even_if_attached) {
1727  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;tdatas_mtx);
1728  	tdata_tree_remove(&amp;tdatas, tdata);
1729  	assert(prof_tdata_should_destroy_unlocked(tdata, even_if_attached));
1730  	if (tdata-&gt;thread_name != NULL) {
1731  		idalloctm(tsd_tsdn(tsd), tdata-&gt;thread_name, NULL, NULL, true,
1732  		    true);
1733  	}
1734  	ckh_delete(tsd, &amp;tdata-&gt;bt2tctx);
1735  	idalloctm(tsd_tsdn(tsd), tdata, NULL, NULL, true, true);
1736  }
1737  static void
1738  prof_tdata_destroy(tsd_t *tsd, prof_tdata_t *tdata, bool even_if_attached) {
1739  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1740  	prof_tdata_destroy_locked(tsd, tdata, even_if_attached);
1741  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1742  }
1743  static void
1744  prof_tdata_detach(tsd_t *tsd, prof_tdata_t *tdata) {
1745  	bool destroy_tdata;
1746  	malloc_mutex_lock(tsd_tsdn(tsd), tdata-&gt;lock);
1747  	if (tdata-&gt;attached) {
1748  		destroy_tdata = prof_tdata_should_destroy(tsd_tsdn(tsd), tdata,
1749  		    true);
1750  		if (!destroy_tdata) {
1751  			tdata-&gt;attached = false;
1752  		}
1753  		tsd_prof_tdata_set(tsd, NULL);
1754  	} else {
1755  		destroy_tdata = false;
1756  	}
1757  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata-&gt;lock);
1758  	if (destroy_tdata) {
1759  		prof_tdata_destroy(tsd, tdata, true);
1760  	}
1761  }
1762  prof_tdata_t *
1763  prof_tdata_reinit(tsd_t *tsd, prof_tdata_t *tdata) {
1764  	uint64_t thr_uid = tdata-&gt;thr_uid;
1765  	uint64_t thr_discrim = tdata-&gt;thr_discrim + 1;
1766  	char *thread_name = (tdata-&gt;thread_name != NULL) ?
1767  	    prof_thread_name_alloc(tsd_tsdn(tsd), tdata-&gt;thread_name) : NULL;
1768  	bool active = tdata-&gt;active;
1769  	prof_tdata_detach(tsd, tdata);
1770  	return prof_tdata_init_impl(tsd, thr_uid, thr_discrim, thread_name,
1771  	    active);
1772  }
1773  static bool
1774  prof_tdata_expire(tsdn_t *tsdn, prof_tdata_t *tdata) {
1775  	bool destroy_tdata;
1776  	malloc_mutex_lock(tsdn, tdata-&gt;lock);
1777  	if (!tdata-&gt;expired) {
1778  		tdata-&gt;expired = true;
1779  		destroy_tdata = tdata-&gt;attached ? false :
1780  		    prof_tdata_should_destroy(tsdn, tdata, false);
1781  	} else {
1782  		destroy_tdata = false;
1783  	}
1784  	malloc_mutex_unlock(tsdn, tdata-&gt;lock);
1785  	return destroy_tdata;
1786  }
1787  static prof_tdata_t *
1788  prof_tdata_reset_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1789      void *arg) {
1790  	tsdn_t *tsdn = (tsdn_t *)arg;
1791  	return (prof_tdata_expire(tsdn, tdata) ? tdata : NULL);
1792  }
1793  void
1794  prof_reset(tsd_t *tsd, size_t lg_sample) {
1795  	prof_tdata_t *next;
1796  	assert(lg_sample &lt; (sizeof(uint64_t) &lt;&lt; 3));
1797  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1798  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1799  	lg_prof_sample = lg_sample;
1800  	next = NULL;
1801  	do {
1802  		prof_tdata_t *to_destroy = tdata_tree_iter(&amp;tdatas, next,
1803  		    prof_tdata_reset_iter, (void *)tsd);
1804  		if (to_destroy != NULL) {
1805  			next = tdata_tree_next(&amp;tdatas, to_destroy);
1806  			prof_tdata_destroy_locked(tsd, to_destroy, false);
1807  		} else {
1808  			next = NULL;
1809  		}
1810  	} while (next != NULL);
1811  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;tdatas_mtx);
1812  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;prof_dump_mtx);
1813  }
1814  void
1815  prof_tdata_cleanup(tsd_t *tsd) {
1816  	prof_tdata_t *tdata;
1817  	if (!config_prof) {
1818  		return;
1819  	}
1820  	tdata = tsd_prof_tdata_get(tsd);
1821  	if (tdata != NULL) {
1822  		prof_tdata_detach(tsd, tdata);
1823  	}
1824  }
1825  bool
1826  prof_active_get(tsdn_t *tsdn) {
1827  	bool prof_active_current;
1828  	malloc_mutex_lock(tsdn, &amp;prof_active_mtx);
1829  	prof_active_current = prof_active;
1830  	malloc_mutex_unlock(tsdn, &amp;prof_active_mtx);
1831  	return prof_active_current;
1832  }
1833  bool
1834  prof_active_set(tsdn_t *tsdn, bool active) {
1835  	bool prof_active_old;
1836  	malloc_mutex_lock(tsdn, &amp;prof_active_mtx);
1837  	prof_active_old = prof_active;
1838  	prof_active = active;
1839  	malloc_mutex_unlock(tsdn, &amp;prof_active_mtx);
1840  	return prof_active_old;
1841  }
1842  #ifdef JEMALLOC_JET
1843  size_t
1844  prof_log_bt_count(void) {
1845  	size_t cnt = 0;
1846  	prof_bt_node_t *node = log_bt_first;
1847  	while (node != NULL) {
1848  		cnt++;
1849  		node = node-&gt;next;
1850  	}
1851  	return cnt;
1852  }
1853  size_t
1854  prof_log_alloc_count(void) {
1855  	size_t cnt = 0;
1856  	prof_alloc_node_t *node = log_alloc_first;
1857  	while (node != NULL) {
1858  		cnt++;
1859  		node = node-&gt;next;
1860  	}
1861  	return cnt;
1862  }
1863  size_t
1864  prof_log_thr_count(void) {
1865  	size_t cnt = 0;
1866  	prof_thr_node_t *node = log_thr_first;
1867  	while (node != NULL) {
1868  		cnt++;
1869  		node = node-&gt;next;
1870  	}
1871  	return cnt;
1872  }
1873  bool
1874  prof_log_is_logging(void) {
1875  	return prof_logging_state == prof_logging_state_started;
1876  }
1877  bool
1878  prof_log_rep_check(void) {
1879  	if (prof_logging_state == prof_logging_state_stopped
1880  	    &amp;&amp; log_tables_initialized) {
1881  		return true;
1882  	}
1883  	if (log_bt_last != NULL &amp;&amp; log_bt_last-&gt;next != NULL) {
1884  		return true;
1885  	}
1886  	if (log_thr_last != NULL &amp;&amp; log_thr_last-&gt;next != NULL) {
1887  		return true;
1888  	}
1889  	if (log_alloc_last != NULL &amp;&amp; log_alloc_last-&gt;next != NULL) {
1890  		return true;
1891  	}
1892  	size_t bt_count = prof_log_bt_count();
1893  	size_t thr_count = prof_log_thr_count();
1894  	size_t alloc_count = prof_log_alloc_count();
1895  	if (prof_logging_state == prof_logging_state_stopped) {
1896  		if (bt_count != 0 || thr_count != 0 || alloc_count || 0) {
1897  			return true;
1898  		}
1899  	}
1900  	prof_alloc_node_t *node = log_alloc_first;
1901  	while (node != NULL) {
1902  		if (node-&gt;alloc_bt_ind &gt;= bt_count) {
1903  			return true;
1904  		}
1905  		if (node-&gt;free_bt_ind &gt;= bt_count) {
1906  			return true;
1907  		}
1908  		if (node-&gt;alloc_thr_ind &gt;= thr_count) {
1909  			return true;
1910  		}
1911  		if (node-&gt;free_thr_ind &gt;= thr_count) {
1912  			return true;
1913  		}
1914  		if (node-&gt;alloc_time_ns &gt; node-&gt;free_time_ns) {
1915  			return true;
1916  		}
1917  		node = node-&gt;next;
1918  	}
1919  	return false;
1920  }
1921  void
1922  prof_log_dummy_set(bool new_value) {
1923  	prof_log_dummy = new_value;
1924  }
1925  #endif
1926  bool
1927  prof_log_start(tsdn_t *tsdn, const char *filename) {
1928  	if (!opt_prof || !prof_booted) {
1929  		return true;
1930  	}
1931  	bool ret = false;
1932  	size_t buf_size = PATH_MAX + 1;
1933  	malloc_mutex_lock(tsdn, &amp;log_mtx);
1934  	if (prof_logging_state != prof_logging_state_stopped) {
1935  		ret = true;
1936  	} else if (filename == NULL) {
1937  		malloc_snprintf(log_filename, buf_size, &quot;%s.%d.%&quot;FMTu64&quot;.json&quot;,
1938  		    opt_prof_prefix, prof_getpid(), log_seq);
1939  		log_seq++;
1940  		prof_logging_state = prof_logging_state_started;
1941  	} else if (strlen(filename) &gt;= buf_size) {
1942  		ret = true;
1943  	} else {
1944  		strcpy(log_filename, filename);
1945  		prof_logging_state = prof_logging_state_started;
1946  	}
1947  	if (!ret) {
1948  		nstime_update(&amp;log_start_timestamp);
1949  	}
1950  	malloc_mutex_unlock(tsdn, &amp;log_mtx);
1951  	return ret;
1952  }
1953  static void
1954  prof_log_stop_final(void) {
1955  	tsd_t *tsd = tsd_fetch();
1956  	prof_log_stop(tsd_tsdn(tsd));
1957  }
1958  struct prof_emitter_cb_arg_s {
1959  	int fd;
1960  	ssize_t ret;
1961  };
1962  static void
1963  prof_emitter_write_cb(void *opaque, const char *to_write) {
1964  	struct prof_emitter_cb_arg_s *arg =
1965  	    (struct prof_emitter_cb_arg_s *)opaque;
1966  	size_t bytes = strlen(to_write);
1967  #ifdef JEMALLOC_JET
1968  	if (prof_log_dummy) {
1969  		return;
1970  	}
1971  #endif
1972  	arg-&gt;ret = write(arg-&gt;fd, (void *)to_write, bytes);
1973  }
1974  static void
1975  prof_log_emit_threads(tsd_t *tsd, emitter_t *emitter) {
1976  	emitter_json_array_kv_begin(emitter, &quot;threads&quot;);
1977  	prof_thr_node_t *thr_node = log_thr_first;
1978  	prof_thr_node_t *thr_old_node;
1979  	while (thr_node != NULL) {
1980  		emitter_json_object_begin(emitter);
1981  		emitter_json_kv(emitter, &quot;thr_uid&quot;, emitter_type_uint64,
1982  		    &amp;thr_node-&gt;thr_uid);
1983  		char *thr_name = thr_node-&gt;name;
1984  		emitter_json_kv(emitter, &quot;thr_name&quot;, emitter_type_string,
1985  		    &amp;thr_name);
1986  		emitter_json_object_end(emitter);
1987  		thr_old_node = thr_node;
1988  		thr_node = thr_node-&gt;next;
1989  		idalloc(tsd, thr_old_node);
1990  	}
1991  	emitter_json_array_end(emitter);
1992  }
1993  static void
1994  prof_log_emit_traces(tsd_t *tsd, emitter_t *emitter) {
1995  	emitter_json_array_kv_begin(emitter, &quot;stack_traces&quot;);
1996  	prof_bt_node_t *bt_node = log_bt_first;
1997  	prof_bt_node_t *bt_old_node;
1998  	char buf[2 * sizeof(intptr_t) + 3];
1999  	size_t buf_sz = sizeof(buf);
2000  	while (bt_node != NULL) {
2001  		emitter_json_array_begin(emitter);
2002  		size_t i;
2003  		for (i = 0; i &lt; bt_node-&gt;bt.len; i++) {
2004  			malloc_snprintf(buf, buf_sz, &quot;%p&quot;, bt_node-&gt;bt.vec[i]);
2005  			char *trace_str = buf;
2006  			emitter_json_value(emitter, emitter_type_string,
2007  			    &amp;trace_str);
2008  		}
2009  		emitter_json_array_end(emitter);
2010  		bt_old_node = bt_node;
2011  		bt_node = bt_node-&gt;next;
2012  		idalloc(tsd, bt_old_node);
2013  	}
2014  	emitter_json_array_end(emitter);
2015  }
2016  static void
2017  prof_log_emit_allocs(tsd_t *tsd, emitter_t *emitter) {
2018  	emitter_json_array_kv_begin(emitter, &quot;allocations&quot;);
2019  	prof_alloc_node_t *alloc_node = log_alloc_first;
2020  	prof_alloc_node_t *alloc_old_node;
2021  	while (alloc_node != NULL) {
2022  		emitter_json_object_begin(emitter);
2023  		emitter_json_kv(emitter, &quot;alloc_thread&quot;, emitter_type_size,
2024  		    &amp;alloc_node-&gt;alloc_thr_ind);
2025  		emitter_json_kv(emitter, &quot;free_thread&quot;, emitter_type_size,
2026  		    &amp;alloc_node-&gt;free_thr_ind);
2027  		emitter_json_kv(emitter, &quot;alloc_trace&quot;, emitter_type_size,
2028  		    &amp;alloc_node-&gt;alloc_bt_ind);
2029  		emitter_json_kv(emitter, &quot;free_trace&quot;, emitter_type_size,
2030  		    &amp;alloc_node-&gt;free_bt_ind);
2031  		emitter_json_kv(emitter, &quot;alloc_timestamp&quot;,
2032  		    emitter_type_uint64, &amp;alloc_node-&gt;alloc_time_ns);
2033  		emitter_json_kv(emitter, &quot;free_timestamp&quot;, emitter_type_uint64,
2034  		    &amp;alloc_node-&gt;free_time_ns);
2035  		emitter_json_kv(emitter, &quot;usize&quot;, emitter_type_uint64,
2036  		    &amp;alloc_node-&gt;usize);
2037  		emitter_json_object_end(emitter);
2038  		alloc_old_node = alloc_node;
2039  		alloc_node = alloc_node-&gt;next;
2040  		idalloc(tsd, alloc_old_node);
2041  	}
2042  	emitter_json_array_end(emitter);
2043  }
2044  static void
2045  prof_log_emit_metadata(emitter_t *emitter) {
2046  	emitter_json_object_kv_begin(emitter, &quot;info&quot;);
2047  	nstime_t now = NSTIME_ZERO_INITIALIZER;
2048  	nstime_update(&amp;now);
2049  	uint64_t ns = nstime_ns(&amp;now) - nstime_ns(&amp;log_start_timestamp);
2050  	emitter_json_kv(emitter, &quot;duration&quot;, emitter_type_uint64, &amp;ns);
2051  	char *vers = JEMALLOC_VERSION;
2052  	emitter_json_kv(emitter, &quot;version&quot;,
2053  	    emitter_type_string, &amp;vers);
2054  	emitter_json_kv(emitter, &quot;lg_sample_rate&quot;,
2055  	    emitter_type_int, &amp;lg_prof_sample);
2056  	int pid = prof_getpid();
2057  	emitter_json_kv(emitter, &quot;pid&quot;, emitter_type_int, &amp;pid);
2058  	emitter_json_object_end(emitter);
2059  }
2060  bool
2061  prof_log_stop(tsdn_t *tsdn) {
2062  	if (!opt_prof || !prof_booted) {
2063  		return true;
2064  	}
2065  	tsd_t *tsd = tsdn_tsd(tsdn);
2066  	malloc_mutex_lock(tsdn, &amp;log_mtx);
2067  	if (prof_logging_state != prof_logging_state_started) {
2068  		malloc_mutex_unlock(tsdn, &amp;log_mtx);
2069  		return true;
2070  	}
2071  	prof_logging_state = prof_logging_state_dumping;
2072  	malloc_mutex_unlock(tsdn, &amp;log_mtx);
2073  	emitter_t emitter;
2074  	int fd;
2075  #ifdef JEMALLOC_JET
2076  	if (prof_log_dummy) {
2077  		fd = 0;
2078  	} else {
2079  		fd = creat(log_filename, 0644);
2080  	}
2081  #else
2082  	fd = creat(log_filename, 0644);
2083  #endif
2084  	if (fd == -1) {
2085  		malloc_printf(&quot;&lt;jemalloc&gt;: creat() for log file \&quot;%s\&quot; &quot;
2086  			      &quot; failed with %d\n&quot;, log_filename, errno);
2087  		if (opt_abort) {
2088  			abort();
2089  		}
2090  		return true;
2091  	}
2092  	struct prof_emitter_cb_arg_s arg;
2093  	arg.fd = fd;
2094  	emitter_init(&amp;emitter, emitter_output_json, &amp;prof_emitter_write_cb,
2095  	    (void *)(&amp;arg));
2096  	emitter_begin(&amp;emitter);
2097  	prof_log_emit_metadata(&amp;emitter);
2098  	prof_log_emit_threads(tsd, &amp;emitter);
2099  	prof_log_emit_traces(tsd, &amp;emitter);
2100  	prof_log_emit_allocs(tsd, &amp;emitter);
2101  	emitter_end(&amp;emitter);
2102  	if (log_tables_initialized) {
2103  		ckh_delete(tsd, &amp;log_bt_node_set);
2104  		ckh_delete(tsd, &amp;log_thr_node_set);
2105  	}
2106  	log_tables_initialized = false;
2107  	log_bt_index = 0;
2108  	log_thr_index = 0;
2109  	log_bt_first = NULL;
2110  	log_bt_last = NULL;
2111  	log_thr_first = NULL;
2112  	log_thr_last = NULL;
2113  	log_alloc_first = NULL;
2114  	log_alloc_last = NULL;
2115  	malloc_mutex_lock(tsdn, &amp;log_mtx);
2116  	prof_logging_state = prof_logging_state_stopped;
2117  	malloc_mutex_unlock(tsdn, &amp;log_mtx);
2118  #ifdef JEMALLOC_JET
2119  	if (prof_log_dummy) {
2120  		return false;
2121  	}
2122  #endif
2123  	return close(fd);
2124  }
2125  const char *
2126  prof_thread_name_get(tsd_t *tsd) {
2127  	prof_tdata_t *tdata;
2128  	tdata = prof_tdata_get(tsd, true);
2129  	if (tdata == NULL) {
2130  		return &quot;&quot;;
2131  	}
2132  	return (tdata-&gt;thread_name != NULL ? tdata-&gt;thread_name : &quot;&quot;);
2133  }
2134  static char *
2135  prof_thread_name_alloc(tsdn_t *tsdn, const char *thread_name) {
2136  	char *ret;
2137  	size_t size;
2138  	if (thread_name == NULL) {
2139  		return NULL;
2140  	}
2141  	size = strlen(thread_name) + 1;
2142  	if (size == 1) {
2143  		return &quot;&quot;;
2144  	}
2145  	ret = iallocztm(tsdn, size, sz_size2index(size), false, NULL, true,
2146  	    arena_get(TSDN_NULL, 0, true), true);
2147  	if (ret == NULL) {
2148  		return NULL;
2149  	}
2150  	memcpy(ret, thread_name, size);
2151  	return ret;
2152  }
2153  int
2154  prof_thread_name_set(tsd_t *tsd, const char *thread_name) {
2155  	prof_tdata_t *tdata;
2156  	unsigned i;
2157  	char *s;
2158  	tdata = prof_tdata_get(tsd, true);
2159  	if (tdata == NULL) {
2160  		return EAGAIN;
2161  	}
2162  	if (thread_name == NULL) {
2163  		return EFAULT;
2164  	}
2165  	for (i = 0; thread_name[i] != &#x27;\0&#x27;; i++) {
2166  		char c = thread_name[i];
2167  		if (!isgraph(c) &amp;&amp; !isblank(c)) {
2168  			return EFAULT;
2169  		}
2170  	}
2171  	s = prof_thread_name_alloc(tsd_tsdn(tsd), thread_name);
2172  	if (s == NULL) {
2173  		return EAGAIN;
2174  	}
2175  	if (tdata-&gt;thread_name != NULL) {
2176  		idalloctm(tsd_tsdn(tsd), tdata-&gt;thread_name, NULL, NULL, true,
2177  		    true);
2178  		tdata-&gt;thread_name = NULL;
2179  	}
2180  	if (strlen(s) &gt; 0) {
2181  		tdata-&gt;thread_name = s;
2182  	}
2183  	return 0;
2184  }
2185  bool
2186  prof_thread_active_get(tsd_t *tsd) {
2187  	prof_tdata_t *tdata;
2188  	tdata = prof_tdata_get(tsd, true);
2189  	if (tdata == NULL) {
2190  		return false;
2191  	}
2192  	return tdata-&gt;active;
2193  }
2194  bool
2195  prof_thread_active_set(tsd_t *tsd, bool active) {
2196  	prof_tdata_t *tdata;
2197  	tdata = prof_tdata_get(tsd, true);
2198  	if (tdata == NULL) {
2199  		return true;
2200  	}
2201  	tdata-&gt;active = active;
2202  	return false;
2203  }
2204  bool
2205  prof_thread_active_init_get(tsdn_t *tsdn) {
2206  	bool active_init;
2207  	malloc_mutex_lock(tsdn, &amp;prof_thread_active_init_mtx);
2208  	active_init = prof_thread_active_init;
2209  	malloc_mutex_unlock(tsdn, &amp;prof_thread_active_init_mtx);
2210  	return active_init;
2211  }
2212  bool
2213  prof_thread_active_init_set(tsdn_t *tsdn, bool active_init) {
2214  	bool active_init_old;
2215  	malloc_mutex_lock(tsdn, &amp;prof_thread_active_init_mtx);
2216  	active_init_old = prof_thread_active_init;
2217  	prof_thread_active_init = active_init;
2218  	malloc_mutex_unlock(tsdn, &amp;prof_thread_active_init_mtx);
2219  	return active_init_old;
2220  }
2221  bool
2222  prof_gdump_get(tsdn_t *tsdn) {
2223  	bool prof_gdump_current;
2224  	malloc_mutex_lock(tsdn, &amp;prof_gdump_mtx);
2225  	prof_gdump_current = prof_gdump_val;
2226  	malloc_mutex_unlock(tsdn, &amp;prof_gdump_mtx);
2227  	return prof_gdump_current;
2228  }
2229  bool
2230  prof_gdump_set(tsdn_t *tsdn, bool gdump) {
2231  	bool prof_gdump_old;
2232  	malloc_mutex_lock(tsdn, &amp;prof_gdump_mtx);
2233  	prof_gdump_old = prof_gdump_val;
2234  	prof_gdump_val = gdump;
2235  	malloc_mutex_unlock(tsdn, &amp;prof_gdump_mtx);
2236  	return prof_gdump_old;
2237  }
2238  void
2239  prof_boot0(void) {
2240  	cassert(config_prof);
2241  	memcpy(opt_prof_prefix, PROF_PREFIX_DEFAULT,
2242  	    sizeof(PROF_PREFIX_DEFAULT));
2243  }
2244  void
2245  prof_boot1(void) {
2246  	cassert(config_prof);
2247  	if (opt_prof_leak &amp;&amp; !opt_prof) {
2248  		opt_prof = true;
2249  		opt_prof_gdump = false;
2250  	} else if (opt_prof) {
2251  		if (opt_lg_prof_interval &gt;= 0) {
2252  			prof_interval = (((uint64_t)1U) &lt;&lt;
2253  			    opt_lg_prof_interval);
2254  		}
2255  	}
2256  }
2257  bool
2258  prof_boot2(tsd_t *tsd) {
2259  	cassert(config_prof);
2260  	if (opt_prof) {
2261  		unsigned i;
2262  		lg_prof_sample = opt_lg_prof_sample;
2263  		prof_active = opt_prof_active;
2264  		if (malloc_mutex_init(&amp;prof_active_mtx, &quot;prof_active&quot;,
2265  		    WITNESS_RANK_PROF_ACTIVE, malloc_mutex_rank_exclusive)) {
2266  			return true;
2267  		}
2268  		prof_gdump_val = opt_prof_gdump;
2269  		if (malloc_mutex_init(&amp;prof_gdump_mtx, &quot;prof_gdump&quot;,
2270  		    WITNESS_RANK_PROF_GDUMP, malloc_mutex_rank_exclusive)) {
2271  			return true;
2272  		}
2273  		prof_thread_active_init = opt_prof_thread_active_init;
2274  		if (malloc_mutex_init(&amp;prof_thread_active_init_mtx,
2275  		    &quot;prof_thread_active_init&quot;,
2276  		    WITNESS_RANK_PROF_THREAD_ACTIVE_INIT,
2277  		    malloc_mutex_rank_exclusive)) {
2278  			return true;
2279  		}
2280  		if (ckh_new(tsd, &amp;bt2gctx, PROF_CKH_MINITEMS, prof_bt_hash,
2281  		    prof_bt_keycomp)) {
2282  			return true;
2283  		}
2284  		if (malloc_mutex_init(&amp;bt2gctx_mtx, &quot;prof_bt2gctx&quot;,
2285  		    WITNESS_RANK_PROF_BT2GCTX, malloc_mutex_rank_exclusive)) {
2286  			return true;
2287  		}
2288  		tdata_tree_new(&amp;tdatas);
2289  		if (malloc_mutex_init(&amp;tdatas_mtx, &quot;prof_tdatas&quot;,
2290  		    WITNESS_RANK_PROF_TDATAS, malloc_mutex_rank_exclusive)) {
2291  			return true;
2292  		}
2293  		next_thr_uid = 0;
2294  		if (malloc_mutex_init(&amp;next_thr_uid_mtx, &quot;prof_next_thr_uid&quot;,
2295  		    WITNESS_RANK_PROF_NEXT_THR_UID, malloc_mutex_rank_exclusive)) {
2296  			return true;
2297  		}
2298  		if (malloc_mutex_init(&amp;prof_dump_seq_mtx, &quot;prof_dump_seq&quot;,
2299  		    WITNESS_RANK_PROF_DUMP_SEQ, malloc_mutex_rank_exclusive)) {
2300  			return true;
2301  		}
2302  		if (malloc_mutex_init(&amp;prof_dump_mtx, &quot;prof_dump&quot;,
2303  		    WITNESS_RANK_PROF_DUMP, malloc_mutex_rank_exclusive)) {
2304  			return true;
2305  		}
2306  		if (opt_prof_final &amp;&amp; opt_prof_prefix[0] != &#x27;\0&#x27; &amp;&amp;
2307  		    atexit(prof_fdump) != 0) {
2308  			malloc_write(&quot;&lt;jemalloc&gt;: Error in atexit()\n&quot;);
2309  			if (opt_abort) {
2310  				abort();
2311  			}
2312  		}
2313  		if (opt_prof_log) {
2314  			prof_log_start(tsd_tsdn(tsd), NULL);
2315  		}
2316  		if (atexit(prof_log_stop_final) != 0) {
2317  			malloc_write(&quot;&lt;jemalloc&gt;: Error in atexit() &quot;
2318  				     &quot;for logging\n&quot;);
2319  			if (opt_abort) {
2320  				abort();
2321  			}
2322  		}
2323  		if (malloc_mutex_init(&amp;log_mtx, &quot;prof_log&quot;,
2324  		    WITNESS_RANK_PROF_LOG, malloc_mutex_rank_exclusive)) {
2325  			return true;
2326  		}
2327  		if (ckh_new(tsd, &amp;log_bt_node_set, PROF_CKH_MINITEMS,
2328  		    prof_bt_node_hash, prof_bt_node_keycomp)) {
2329  			return true;
2330  		}
2331  		if (ckh_new(tsd, &amp;log_thr_node_set, PROF_CKH_MINITEMS,
2332  		    prof_thr_node_hash, prof_thr_node_keycomp)) {
2333  			return true;
2334  		}
2335  		log_tables_initialized = true;
2336  		gctx_locks = (malloc_mutex_t *)base_alloc(tsd_tsdn(tsd),
2337  		    b0get(), PROF_NCTX_LOCKS * sizeof(malloc_mutex_t),
2338  		    CACHELINE);
2339  		if (gctx_locks == NULL) {
2340  			return true;
2341  		}
2342  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2343  			if (malloc_mutex_init(&amp;gctx_locks[i], &quot;prof_gctx&quot;,
2344  			    WITNESS_RANK_PROF_GCTX,
2345  			    malloc_mutex_rank_exclusive)) {
2346  				return true;
2347  			}
2348  		}
2349  		tdata_locks = (malloc_mutex_t *)base_alloc(tsd_tsdn(tsd),
2350  		    b0get(), PROF_NTDATA_LOCKS * sizeof(malloc_mutex_t),
2351  		    CACHELINE);
2352  		if (tdata_locks == NULL) {
2353  			return true;
2354  		}
2355  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2356  			if (malloc_mutex_init(&amp;tdata_locks[i], &quot;prof_tdata&quot;,
2357  			    WITNESS_RANK_PROF_TDATA,
2358  			    malloc_mutex_rank_exclusive)) {
2359  				return true;
2360  			}
2361  		}
2362  #ifdef JEMALLOC_PROF_LIBGCC
2363  		_Unwind_Backtrace(prof_unwind_init_callback, NULL);
2364  #endif
2365  	}
2366  	prof_booted = true;
2367  	return false;
2368  }
2369  void
2370  prof_prefork0(tsdn_t *tsdn) {
2371  	if (config_prof &amp;&amp; opt_prof) {
2372  		unsigned i;
2373  		malloc_mutex_prefork(tsdn, &amp;prof_dump_mtx);
2374  		malloc_mutex_prefork(tsdn, &amp;bt2gctx_mtx);
2375  		malloc_mutex_prefork(tsdn, &amp;tdatas_mtx);
2376  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2377  			malloc_mutex_prefork(tsdn, &amp;tdata_locks[i]);
2378  		}
2379  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2380  			malloc_mutex_prefork(tsdn, &amp;gctx_locks[i]);
2381  		}
2382  	}
2383  }
2384  void
2385  prof_prefork1(tsdn_t *tsdn) {
2386  	if (config_prof &amp;&amp; opt_prof) {
2387  		malloc_mutex_prefork(tsdn, &amp;prof_active_mtx);
2388  		malloc_mutex_prefork(tsdn, &amp;prof_dump_seq_mtx);
2389  		malloc_mutex_prefork(tsdn, &amp;prof_gdump_mtx);
2390  		malloc_mutex_prefork(tsdn, &amp;next_thr_uid_mtx);
2391  		malloc_mutex_prefork(tsdn, &amp;prof_thread_active_init_mtx);
2392  	}
2393  }
2394  void
2395  prof_postfork_parent(tsdn_t *tsdn) {
2396  	if (config_prof &amp;&amp; opt_prof) {
2397  		unsigned i;
2398  		malloc_mutex_postfork_parent(tsdn,
2399  		    &amp;prof_thread_active_init_mtx);
2400  		malloc_mutex_postfork_parent(tsdn, &amp;next_thr_uid_mtx);
2401  		malloc_mutex_postfork_parent(tsdn, &amp;prof_gdump_mtx);
2402  		malloc_mutex_postfork_parent(tsdn, &amp;prof_dump_seq_mtx);
2403  		malloc_mutex_postfork_parent(tsdn, &amp;prof_active_mtx);
2404  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2405  			malloc_mutex_postfork_parent(tsdn, &amp;gctx_locks[i]);
2406  		}
2407  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2408  			malloc_mutex_postfork_parent(tsdn, &amp;tdata_locks[i]);
2409  		}
2410  		malloc_mutex_postfork_parent(tsdn, &amp;tdatas_mtx);
2411  		malloc_mutex_postfork_parent(tsdn, &amp;bt2gctx_mtx);
2412  		malloc_mutex_postfork_parent(tsdn, &amp;prof_dump_mtx);
2413  	}
2414  }
2415  void
2416  prof_postfork_child(tsdn_t *tsdn) {
2417  	if (config_prof &amp;&amp; opt_prof) {
2418  		unsigned i;
2419  		malloc_mutex_postfork_child(tsdn, &amp;prof_thread_active_init_mtx);
2420  		malloc_mutex_postfork_child(tsdn, &amp;next_thr_uid_mtx);
2421  		malloc_mutex_postfork_child(tsdn, &amp;prof_gdump_mtx);
2422  		malloc_mutex_postfork_child(tsdn, &amp;prof_dump_seq_mtx);
2423  		malloc_mutex_postfork_child(tsdn, &amp;prof_active_mtx);
2424  		for (i = 0; i &lt; PROF_NCTX_LOCKS; i++) {
2425  			malloc_mutex_postfork_child(tsdn, &amp;gctx_locks[i]);
2426  		}
2427  		for (i = 0; i &lt; PROF_NTDATA_LOCKS; i++) {
2428  			malloc_mutex_postfork_child(tsdn, &amp;tdata_locks[i]);
2429  		}
2430  		malloc_mutex_postfork_child(tsdn, &amp;tdatas_mtx);
2431  		malloc_mutex_postfork_child(tsdn, &amp;bt2gctx_mtx);
2432  		malloc_mutex_postfork_child(tsdn, &amp;prof_dump_mtx);
2433  	}
2434  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-prof.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-prof.c</div>
                </div>
                <div class="column column_space"><pre><code>231  		    true, arena_get(TSDN_NULL, 0, true), true);
232  		if (log_bt_first == NULL) {
233  			log_bt_first = new_node;
234  			log_bt_last = new_node;
235  		} else {
236  			log_bt_last-&gt;next = new_node;
237  			log_bt_last = new_node;
238  		}
239  		new_node-&gt;next = NULL;
240  		new_node-&gt;index = log_bt_index;
241  		new_node-&gt;bt.len = bt-&gt;len;
</pre></code></div>
                <div class="column column_space"><pre><code>263  		    true, arena_get(TSDN_NULL, 0, true), true);
264  		if (log_thr_first == NULL) {
265  			log_thr_first = new_node;
266  			log_thr_last = new_node;
267  		} else {
268  			log_thr_last-&gt;next = new_node;
269  			log_thr_last = new_node;
270  		}
271  		new_node-&gt;next = NULL;
272  		new_node-&gt;index = log_thr_index;
273  		new_node-&gt;thr_uid = thr_uid;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    