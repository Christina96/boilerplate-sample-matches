
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 16, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-background_thread_22.c</h3>
            <pre><code>1  #define JEMALLOC_BACKGROUND_THREAD_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  JEMALLOC_DIAGNOSTIC_DISABLE_SPURIOUS
6  #define BACKGROUND_THREAD_DEFAULT false
7  bool opt_background_thread = BACKGROUND_THREAD_DEFAULT;
8  size_t opt_max_background_threads = MAX_BACKGROUND_THREAD_LIMIT + 1;
9  malloc_mutex_t background_thread_lock;
10  atomic_b_t background_thread_enabled_state;
11  size_t n_background_threads;
12  size_t max_background_threads;
13  background_thread_info_t *background_thread_info;
14  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
15  static int (*pthread_create_fptr)(pthread_t *__restrict, const pthread_attr_t *,
16      void *(*)(void *), void *__restrict);
17  static void
18  pthread_create_wrapper_init(void) {
19  #ifdef JEMALLOC_LAZY_LOCK
20  	if (!isthreaded) {
21  		isthreaded = true;
22  	}
23  #endif
24  }
25  int
26  pthread_create_wrapper(pthread_t *__restrict thread, const pthread_attr_t *attr,
27      void *(*start_routine)(void *), void *__restrict arg) {
28  	pthread_create_wrapper_init();
29  	return pthread_create_fptr(thread, attr, start_routine, arg);
30  }
31  #endif &bsol;* JEMALLOC_PTHREAD_CREATE_WRAPPER */
32  #ifndef JEMALLOC_BACKGROUND_THREAD
33  #define NOT_REACHED { not_reached(); }
34  bool background_thread_create(tsd_t *tsd, unsigned arena_ind) NOT_REACHED
35  bool background_threads_enable(tsd_t *tsd) NOT_REACHED
36  bool background_threads_disable(tsd_t *tsd) NOT_REACHED
37  void background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,
38      arena_decay_t *decay, size_t npages_new) NOT_REACHED
39  void background_thread_prefork0(tsdn_t *tsdn) NOT_REACHED
40  void background_thread_prefork1(tsdn_t *tsdn) NOT_REACHED
41  void background_thread_postfork_parent(tsdn_t *tsdn) NOT_REACHED
42  void background_thread_postfork_child(tsdn_t *tsdn) NOT_REACHED
43  bool background_thread_stats_read(tsdn_t *tsdn,
44      background_thread_stats_t *stats) NOT_REACHED
45  void background_thread_ctl_init(tsdn_t *tsdn) NOT_REACHED
46  #undef NOT_REACHED
47  #else
48  static bool background_thread_enabled_at_fork;
49  static void
50  background_thread_info_init(tsdn_t *tsdn, background_thread_info_t *info) {
51  	background_thread_wakeup_time_set(tsdn, info, 0);
52  	info->npages_to_purge_new = 0;
53  	if (config_stats) {
54  		info->tot_n_runs = 0;
55  		nstime_init(&info->tot_sleep_time, 0);
56  	}
57  }
58  static inline bool
59  set_current_thread_affinity(int cpu) {
60  #if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
61  	cpu_set_t cpuset;
62  	CPU_ZERO(&cpuset);
63  	CPU_SET(cpu, &cpuset);
64  	int ret = sched_setaffinity(0, sizeof(cpu_set_t), &cpuset);
65  	return (ret != 0);
66  #else
67  	return false;
68  #endif
69  }
70  #define BACKGROUND_THREAD_NPAGES_THRESHOLD UINT64_C(1024)
71  #define BILLION UINT64_C(1000000000)
72  #define BACKGROUND_THREAD_MIN_INTERVAL_NS (BILLION / 10)
73  static inline size_t
74  decay_npurge_after_interval(arena_decay_t *decay, size_t interval) {
75  	size_t i;
76  	uint64_t sum = 0;
77  	for (i = 0; i < interval; i++) {
78  		sum += decay->backlog[i] * h_steps[i];
79  	}
80  	for (; i < SMOOTHSTEP_NSTEPS; i++) {
81  		sum += decay->backlog[i] * (h_steps[i] - h_steps[i - interval]);
82  	}
83  	return (size_t)(sum >> SMOOTHSTEP_BFP);
84  }
85  static uint64_t
86  arena_decay_compute_purge_interval_impl(tsdn_t *tsdn, arena_decay_t *decay,
87      extents_t *extents) {
88  	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
89  		return BACKGROUND_THREAD_MIN_INTERVAL_NS;
90  	}
91  	uint64_t interval;
92  	ssize_t decay_time = atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);
93  	if (decay_time <= 0) {
94  		interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
95  		goto label_done;
96  	}
97  	uint64_t decay_interval_ns = nstime_ns(&decay->interval);
98  	assert(decay_interval_ns > 0);
99  	size_t npages = extents_npages_get(extents);
100  	if (npages == 0) {
101  		unsigned i;
102  		for (i = 0; i < SMOOTHSTEP_NSTEPS; i++) {
103  			if (decay->backlog[i] > 0) {
104  				break;
105  			}
106  		}
107  		if (i == SMOOTHSTEP_NSTEPS) {
108  			interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
109  			goto label_done;
110  		}
111  	}
112  	if (npages <= BACKGROUND_THREAD_NPAGES_THRESHOLD) {
113  		interval = decay_interval_ns * SMOOTHSTEP_NSTEPS;
114  		goto label_done;
115  	}
116  	size_t lb = BACKGROUND_THREAD_MIN_INTERVAL_NS / decay_interval_ns;
117  	size_t ub = SMOOTHSTEP_NSTEPS;
118  	lb = (lb < 2) ? 2 : lb;
119  	if ((decay_interval_ns * ub <= BACKGROUND_THREAD_MIN_INTERVAL_NS) ||
120  	    (lb + 2 > ub)) {
121  		interval = BACKGROUND_THREAD_MIN_INTERVAL_NS;
122  		goto label_done;
123  	}
124  	assert(lb + 2 <= ub);
125  	size_t npurge_lb, npurge_ub;
126  	npurge_lb = decay_npurge_after_interval(decay, lb);
127  	if (npurge_lb > BACKGROUND_THREAD_NPAGES_THRESHOLD) {
128  		interval = decay_interval_ns * lb;
129  		goto label_done;
130  	}
<span onclick='openModal()' class='match'>131  	npurge_ub = decay_npurge_after_interval(decay, ub);
132  	if (npurge_ub < BACKGROUND_THREAD_NPAGES_THRESHOLD) {
133  		interval = decay_interval_ns * ub;
</span>134  		goto label_done;
135  	}
136  	unsigned n_search = 0;
137  	size_t target, npurge;
138  	while ((npurge_lb + BACKGROUND_THREAD_NPAGES_THRESHOLD < npurge_ub)
139  	    && (lb + 2 < ub)) {
140  		target = (lb + ub) / 2;
141  		npurge = decay_npurge_after_interval(decay, target);
142  		if (npurge > BACKGROUND_THREAD_NPAGES_THRESHOLD) {
143  			ub = target;
144  			npurge_ub = npurge;
145  		} else {
146  			lb = target;
147  			npurge_lb = npurge;
148  		}
149  		assert(n_search++ < lg_floor(SMOOTHSTEP_NSTEPS) + 1);
150  	}
151  	interval = decay_interval_ns * (ub + lb) / 2;
152  label_done:
153  	interval = (interval < BACKGROUND_THREAD_MIN_INTERVAL_NS) ?
154  	    BACKGROUND_THREAD_MIN_INTERVAL_NS : interval;
155  	malloc_mutex_unlock(tsdn, &decay->mtx);
156  	return interval;
157  }
158  static uint64_t
159  arena_decay_compute_purge_interval(tsdn_t *tsdn, arena_t *arena) {
160  	uint64_t i1, i2;
161  	i1 = arena_decay_compute_purge_interval_impl(tsdn, &arena->decay_dirty,
162  	    &arena->extents_dirty);
163  	if (i1 == BACKGROUND_THREAD_MIN_INTERVAL_NS) {
164  		return i1;
165  	}
166  	i2 = arena_decay_compute_purge_interval_impl(tsdn, &arena->decay_muzzy,
167  	    &arena->extents_muzzy);
168  	return i1 < i2 ? i1 : i2;
169  }
170  static void
171  background_thread_sleep(tsdn_t *tsdn, background_thread_info_t *info,
172      uint64_t interval) {
173  	if (config_stats) {
174  		info->tot_n_runs++;
175  	}
176  	info->npages_to_purge_new = 0;
177  	struct timeval tv;
178  	gettimeofday(&tv, NULL);
179  	nstime_t before_sleep;
180  	nstime_init2(&before_sleep, tv.tv_sec, tv.tv_usec * 1000);
181  	int ret;
182  	if (interval == BACKGROUND_THREAD_INDEFINITE_SLEEP) {
183  		assert(background_thread_indefinite_sleep(info));
184  		ret = pthread_cond_wait(&info->cond, &info->mtx.lock);
185  		assert(ret == 0);
186  	} else {
187  		assert(interval >= BACKGROUND_THREAD_MIN_INTERVAL_NS &&
188  		    interval <= BACKGROUND_THREAD_INDEFINITE_SLEEP);
189  		nstime_t next_wakeup;
190  		nstime_init(&next_wakeup, 0);
191  		nstime_update(&next_wakeup);
192  		nstime_iadd(&next_wakeup, interval);
193  		assert(nstime_ns(&next_wakeup) <
194  		    BACKGROUND_THREAD_INDEFINITE_SLEEP);
195  		background_thread_wakeup_time_set(tsdn, info,
196  		    nstime_ns(&next_wakeup));
197  		nstime_t ts_wakeup;
198  		nstime_copy(&ts_wakeup, &before_sleep);
199  		nstime_iadd(&ts_wakeup, interval);
200  		struct timespec ts;
201  		ts.tv_sec = (size_t)nstime_sec(&ts_wakeup);
202  		ts.tv_nsec = (size_t)nstime_nsec(&ts_wakeup);
203  		assert(!background_thread_indefinite_sleep(info));
204  		ret = pthread_cond_timedwait(&info->cond, &info->mtx.lock, &ts);
205  		assert(ret == ETIMEDOUT || ret == 0);
206  		background_thread_wakeup_time_set(tsdn, info,
207  		    BACKGROUND_THREAD_INDEFINITE_SLEEP);
208  	}
209  	if (config_stats) {
210  		gettimeofday(&tv, NULL);
211  		nstime_t after_sleep;
212  		nstime_init2(&after_sleep, tv.tv_sec, tv.tv_usec * 1000);
213  		if (nstime_compare(&after_sleep, &before_sleep) > 0) {
214  			nstime_subtract(&after_sleep, &before_sleep);
215  			nstime_add(&info->tot_sleep_time, &after_sleep);
216  		}
217  	}
218  }
219  static bool
220  background_thread_pause_check(tsdn_t *tsdn, background_thread_info_t *info) {
221  	if (unlikely(info->state == background_thread_paused)) {
222  		malloc_mutex_unlock(tsdn, &info->mtx);
223  		malloc_mutex_lock(tsdn, &background_thread_lock);
224  		malloc_mutex_unlock(tsdn, &background_thread_lock);
225  		malloc_mutex_lock(tsdn, &info->mtx);
226  		return true;
227  	}
228  	return false;
229  }
230  static inline void
231  background_work_sleep_once(tsdn_t *tsdn, background_thread_info_t *info, unsigned ind) {
232  	uint64_t min_interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
233  	unsigned narenas = narenas_total_get();
234  	for (unsigned i = ind; i < narenas; i += max_background_threads) {
235  		arena_t *arena = arena_get(tsdn, i, false);
236  		if (!arena) {
237  			continue;
238  		}
239  		arena_decay(tsdn, arena, true, false);
240  		if (min_interval == BACKGROUND_THREAD_MIN_INTERVAL_NS) {
241  			continue;
242  		}
243  		uint64_t interval = arena_decay_compute_purge_interval(tsdn,
244  		    arena);
245  		assert(interval >= BACKGROUND_THREAD_MIN_INTERVAL_NS);
246  		if (min_interval > interval) {
247  			min_interval = interval;
248  		}
249  	}
250  	background_thread_sleep(tsdn, info, min_interval);
251  }
252  static bool
253  background_threads_disable_single(tsd_t *tsd, background_thread_info_t *info) {
254  	if (info == &background_thread_info[0]) {
255  		malloc_mutex_assert_owner(tsd_tsdn(tsd),
256  		    &background_thread_lock);
257  	} else {
258  		malloc_mutex_assert_not_owner(tsd_tsdn(tsd),
259  		    &background_thread_lock);
260  	}
261  	pre_reentrancy(tsd, NULL);
262  	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
263  	bool has_thread;
264  	assert(info->state != background_thread_paused);
265  	if (info->state == background_thread_started) {
266  		has_thread = true;
267  		info->state = background_thread_stopped;
268  		pthread_cond_signal(&info->cond);
269  	} else {
270  		has_thread = false;
271  	}
272  	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
273  	if (!has_thread) {
274  		post_reentrancy(tsd);
275  		return false;
276  	}
277  	void *ret;
278  	if (pthread_join(info->thread, &ret)) {
279  		post_reentrancy(tsd);
280  		return true;
281  	}
282  	assert(ret == NULL);
283  	n_background_threads--;
284  	post_reentrancy(tsd);
285  	return false;
286  }
287  static void *background_thread_entry(void *ind_arg);
288  static int
289  background_thread_create_signals_masked(pthread_t *thread,
290      const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg) {
291  	sigset_t set;
292  	sigfillset(&set);
293  	sigset_t oldset;
294  	int mask_err = pthread_sigmask(SIG_SETMASK, &set, &oldset);
295  	if (mask_err != 0) {
296  		return mask_err;
297  	}
298  	int create_err = pthread_create_wrapper(thread, attr, start_routine,
299  	    arg);
300  	int restore_err = pthread_sigmask(SIG_SETMASK, &oldset, NULL);
301  	if (restore_err != 0) {
302  		malloc_printf("<jemalloc>: background thread creation "
303  		    "failed (%d), and signal mask restoration failed "
304  		    "(%d)\n", create_err, restore_err);
305  		if (opt_abort) {
306  			abort();
307  		}
308  	}
309  	return create_err;
310  }
311  static bool
312  check_background_thread_creation(tsd_t *tsd, unsigned *n_created,
313      bool *created_threads) {
314  	bool ret = false;
315  	if (likely(*n_created == n_background_threads)) {
316  		return ret;
317  	}
318  	tsdn_t *tsdn = tsd_tsdn(tsd);
319  	malloc_mutex_unlock(tsdn, &background_thread_info[0].mtx);
320  	for (unsigned i = 1; i < max_background_threads; i++) {
321  		if (created_threads[i]) {
322  			continue;
323  		}
324  		background_thread_info_t *info = &background_thread_info[i];
325  		malloc_mutex_lock(tsdn, &info->mtx);
326  		bool create = (info->state == background_thread_started);
327  		malloc_mutex_unlock(tsdn, &info->mtx);
328  		if (!create) {
329  			continue;
330  		}
331  		pre_reentrancy(tsd, NULL);
332  		int err = background_thread_create_signals_masked(&info->thread,
333  		    NULL, background_thread_entry, (void *)(uintptr_t)i);
334  		post_reentrancy(tsd);
335  		if (err == 0) {
336  			(*n_created)++;
337  			created_threads[i] = true;
338  		} else {
339  			malloc_printf("<jemalloc>: background thread "
340  			    "creation failed (%d)\n", err);
341  			if (opt_abort) {
342  				abort();
343  			}
344  		}
345  		ret = true;
346  		break;
347  	}
348  	malloc_mutex_lock(tsdn, &background_thread_info[0].mtx);
349  	return ret;
350  }
351  static void
352  background_thread0_work(tsd_t *tsd) {
353  	VARIABLE_ARRAY(bool, created_threads, max_background_threads);
354  	unsigned i;
355  	for (i = 1; i < max_background_threads; i++) {
356  		created_threads[i] = false;
357  	}
358  	unsigned n_created = 1;
359  	while (background_thread_info[0].state != background_thread_stopped) {
360  		if (background_thread_pause_check(tsd_tsdn(tsd),
361  		    &background_thread_info[0])) {
362  			continue;
363  		}
364  		if (check_background_thread_creation(tsd, &n_created,
365  		    (bool *)&created_threads)) {
366  			continue;
367  		}
368  		background_work_sleep_once(tsd_tsdn(tsd),
369  		    &background_thread_info[0], 0);
370  	}
371  	assert(!background_thread_enabled());
372  	for (i = 1; i < max_background_threads; i++) {
373  		background_thread_info_t *info = &background_thread_info[i];
374  		assert(info->state != background_thread_paused);
375  		if (created_threads[i]) {
376  			background_threads_disable_single(tsd, info);
377  		} else {
378  			malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
379  			if (info->state != background_thread_stopped) {
380  				assert(info->state ==
381  				    background_thread_started);
382  				n_background_threads--;
383  				info->state = background_thread_stopped;
384  			}
385  			malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
386  		}
387  	}
388  	background_thread_info[0].state = background_thread_stopped;
389  	assert(n_background_threads == 1);
390  }
391  static void
392  background_work(tsd_t *tsd, unsigned ind) {
393  	background_thread_info_t *info = &background_thread_info[ind];
394  	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
395  	background_thread_wakeup_time_set(tsd_tsdn(tsd), info,
396  	    BACKGROUND_THREAD_INDEFINITE_SLEEP);
397  	if (ind == 0) {
398  		background_thread0_work(tsd);
399  	} else {
400  		while (info->state != background_thread_stopped) {
401  			if (background_thread_pause_check(tsd_tsdn(tsd),
402  			    info)) {
403  				continue;
404  			}
405  			background_work_sleep_once(tsd_tsdn(tsd), info, ind);
406  		}
407  	}
408  	assert(info->state == background_thread_stopped);
409  	background_thread_wakeup_time_set(tsd_tsdn(tsd), info, 0);
410  	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
411  }
412  static void *
413  background_thread_entry(void *ind_arg) {
414  	unsigned thread_ind = (unsigned)(uintptr_t)ind_arg;
415  	assert(thread_ind < max_background_threads);
416  #ifdef JEMALLOC_HAVE_PTHREAD_SETNAME_NP
417  	pthread_setname_np(pthread_self(), "jemalloc_bg_thd");
418  #elif defined(__FreeBSD__)
419  	pthread_set_name_np(pthread_self(), "jemalloc_bg_thd");
420  #endif
421  	if (opt_percpu_arena != percpu_arena_disabled) {
422  		set_current_thread_affinity((int)thread_ind);
423  	}
424  	background_work(tsd_internal_fetch(), thread_ind);
425  	assert(pthread_equal(pthread_self(),
426  	    background_thread_info[thread_ind].thread));
427  	return NULL;
428  }
429  static void
430  background_thread_init(tsd_t *tsd, background_thread_info_t *info) {
431  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
432  	info->state = background_thread_started;
433  	background_thread_info_init(tsd_tsdn(tsd), info);
434  	n_background_threads++;
435  }
436  static bool
437  background_thread_create_locked(tsd_t *tsd, unsigned arena_ind) {
438  	assert(have_background_thread);
439  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
440  	size_t thread_ind = arena_ind % max_background_threads;
441  	background_thread_info_t *info = &background_thread_info[thread_ind];
442  	bool need_new_thread;
443  	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
444  	need_new_thread = background_thread_enabled() &&
445  	    (info->state == background_thread_stopped);
446  	if (need_new_thread) {
447  		background_thread_init(tsd, info);
448  	}
449  	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
450  	if (!need_new_thread) {
451  		return false;
452  	}
453  	if (arena_ind != 0) {
454  		background_thread_info_t *t0 = &background_thread_info[0];
455  		malloc_mutex_lock(tsd_tsdn(tsd), &t0->mtx);
456  		assert(t0->state == background_thread_started);
457  		pthread_cond_signal(&t0->cond);
458  		malloc_mutex_unlock(tsd_tsdn(tsd), &t0->mtx);
459  		return false;
460  	}
461  	pre_reentrancy(tsd, NULL);
462  	int err = background_thread_create_signals_masked(&info->thread, NULL,
463  	    background_thread_entry, (void *)thread_ind);
464  	post_reentrancy(tsd);
465  	if (err != 0) {
466  		malloc_printf("<jemalloc>: arena 0 background thread creation "
467  		    "failed (%d)\n", err);
468  		malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
469  		info->state = background_thread_stopped;
470  		n_background_threads--;
471  		malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
472  		return true;
473  	}
474  	return false;
475  }
476  bool
477  background_thread_create(tsd_t *tsd, unsigned arena_ind) {
478  	assert(have_background_thread);
479  	bool ret;
480  	malloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);
481  	ret = background_thread_create_locked(tsd, arena_ind);
482  	malloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);
483  	return ret;
484  }
485  bool
486  background_threads_enable(tsd_t *tsd) {
487  	assert(n_background_threads == 0);
488  	assert(background_thread_enabled());
489  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
490  	VARIABLE_ARRAY(bool, marked, max_background_threads);
491  	unsigned i, nmarked;
492  	for (i = 0; i < max_background_threads; i++) {
493  		marked[i] = false;
494  	}
495  	nmarked = 0;
496  	marked[0] = true;
497  	unsigned n = narenas_total_get();
498  	for (i = 1; i < n; i++) {
499  		if (marked[i % max_background_threads] ||
500  		    arena_get(tsd_tsdn(tsd), i, false) == NULL) {
501  			continue;
502  		}
503  		background_thread_info_t *info = &background_thread_info[
504  		    i % max_background_threads];
505  		malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
506  		assert(info->state == background_thread_stopped);
507  		background_thread_init(tsd, info);
508  		malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
509  		marked[i % max_background_threads] = true;
510  		if (++nmarked == max_background_threads) {
511  			break;
512  		}
513  	}
514  	return background_thread_create_locked(tsd, 0);
515  }
516  bool
517  background_threads_disable(tsd_t *tsd) {
518  	assert(!background_thread_enabled());
519  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
520  	if (background_threads_disable_single(tsd,
521  	    &background_thread_info[0])) {
522  		return true;
523  	}
524  	assert(n_background_threads == 0);
525  	return false;
526  }
527  void
528  background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,
529      arena_decay_t *decay, size_t npages_new) {
530  	background_thread_info_t *info = arena_background_thread_info_get(
531  	    arena);
532  	if (malloc_mutex_trylock(tsdn, &info->mtx)) {
533  		return;
534  	}
535  	if (info->state != background_thread_started) {
536  		goto label_done;
537  	}
538  	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
539  		goto label_done;
540  	}
541  	ssize_t decay_time = atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);
542  	if (decay_time <= 0) {
543  		goto label_done_unlock2;
544  	}
545  	uint64_t decay_interval_ns = nstime_ns(&decay->interval);
546  	assert(decay_interval_ns > 0);
547  	nstime_t diff;
548  	nstime_init(&diff, background_thread_wakeup_time_get(info));
549  	if (nstime_compare(&diff, &decay->epoch) <= 0) {
550  		goto label_done_unlock2;
551  	}
552  	nstime_subtract(&diff, &decay->epoch);
553  	if (nstime_ns(&diff) < BACKGROUND_THREAD_MIN_INTERVAL_NS) {
554  		goto label_done_unlock2;
555  	}
556  	if (npages_new > 0) {
557  		size_t n_epoch = (size_t)(nstime_ns(&diff) / decay_interval_ns);
558  		uint64_t npurge_new;
559  		if (n_epoch >= SMOOTHSTEP_NSTEPS) {
560  			npurge_new = npages_new;
561  		} else {
562  			uint64_t h_steps_max = h_steps[SMOOTHSTEP_NSTEPS - 1];
563  			assert(h_steps_max >=
564  			    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);
565  			npurge_new = npages_new * (h_steps_max -
566  			    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);
567  			npurge_new >>= SMOOTHSTEP_BFP;
568  		}
569  		info->npages_to_purge_new += npurge_new;
570  	}
571  	bool should_signal;
572  	if (info->npages_to_purge_new > BACKGROUND_THREAD_NPAGES_THRESHOLD) {
573  		should_signal = true;
574  	} else if (unlikely(background_thread_indefinite_sleep(info)) &&
575  	    (extents_npages_get(&arena->extents_dirty) > 0 ||
576  	    extents_npages_get(&arena->extents_muzzy) > 0 ||
577  	    info->npages_to_purge_new > 0)) {
578  		should_signal = true;
579  	} else {
580  		should_signal = false;
581  	}
582  	if (should_signal) {
583  		info->npages_to_purge_new = 0;
584  		pthread_cond_signal(&info->cond);
585  	}
586  label_done_unlock2:
587  	malloc_mutex_unlock(tsdn, &decay->mtx);
588  label_done:
589  	malloc_mutex_unlock(tsdn, &info->mtx);
590  }
591  void
592  background_thread_prefork0(tsdn_t *tsdn) {
593  	malloc_mutex_prefork(tsdn, &background_thread_lock);
594  	background_thread_enabled_at_fork = background_thread_enabled();
595  }
596  void
597  background_thread_prefork1(tsdn_t *tsdn) {
598  	for (unsigned i = 0; i < max_background_threads; i++) {
599  		malloc_mutex_prefork(tsdn, &background_thread_info[i].mtx);
600  	}
601  }
602  void
603  background_thread_postfork_parent(tsdn_t *tsdn) {
604  	for (unsigned i = 0; i < max_background_threads; i++) {
605  		malloc_mutex_postfork_parent(tsdn,
606  		    &background_thread_info[i].mtx);
607  	}
608  	malloc_mutex_postfork_parent(tsdn, &background_thread_lock);
609  }
610  void
611  background_thread_postfork_child(tsdn_t *tsdn) {
612  	for (unsigned i = 0; i < max_background_threads; i++) {
613  		malloc_mutex_postfork_child(tsdn,
614  		    &background_thread_info[i].mtx);
615  	}
616  	malloc_mutex_postfork_child(tsdn, &background_thread_lock);
617  	if (!background_thread_enabled_at_fork) {
618  		return;
619  	}
620  	malloc_mutex_lock(tsdn, &background_thread_lock);
621  	n_background_threads = 0;
622  	background_thread_enabled_set(tsdn, false);
623  	for (unsigned i = 0; i < max_background_threads; i++) {
624  		background_thread_info_t *info = &background_thread_info[i];
625  		malloc_mutex_lock(tsdn, &info->mtx);
626  		info->state = background_thread_stopped;
627  		int ret = pthread_cond_init(&info->cond, NULL);
628  		assert(ret == 0);
629  		background_thread_info_init(tsdn, info);
630  		malloc_mutex_unlock(tsdn, &info->mtx);
631  	}
632  	malloc_mutex_unlock(tsdn, &background_thread_lock);
633  }
634  bool
635  background_thread_stats_read(tsdn_t *tsdn, background_thread_stats_t *stats) {
636  	assert(config_stats);
637  	malloc_mutex_lock(tsdn, &background_thread_lock);
638  	if (!background_thread_enabled()) {
639  		malloc_mutex_unlock(tsdn, &background_thread_lock);
640  		return true;
641  	}
642  	stats->num_threads = n_background_threads;
643  	uint64_t num_runs = 0;
644  	nstime_init(&stats->run_interval, 0);
645  	for (unsigned i = 0; i < max_background_threads; i++) {
646  		background_thread_info_t *info = &background_thread_info[i];
647  		if (malloc_mutex_trylock(tsdn, &info->mtx)) {
648  			continue;
649  		}
650  		if (info->state != background_thread_stopped) {
651  			num_runs += info->tot_n_runs;
652  			nstime_add(&stats->run_interval, &info->tot_sleep_time);
653  		}
654  		malloc_mutex_unlock(tsdn, &info->mtx);
655  	}
656  	stats->num_runs = num_runs;
657  	if (num_runs > 0) {
658  		nstime_idivide(&stats->run_interval, num_runs);
659  	}
660  	malloc_mutex_unlock(tsdn, &background_thread_lock);
661  	return false;
662  }
663  #undef BACKGROUND_THREAD_NPAGES_THRESHOLD
664  #undef BILLION
665  #undef BACKGROUND_THREAD_MIN_INTERVAL_NS
666  #ifdef JEMALLOC_HAVE_DLSYM
667  #include <dlfcn.h>
668  #endif
669  static bool
670  pthread_create_fptr_init(void) {
671  	if (pthread_create_fptr != NULL) {
672  		return false;
673  	}
674  #ifdef JEMALLOC_HAVE_DLSYM
675  	pthread_create_fptr = dlsym(RTLD_NEXT, "pthread_create");
676  #else
677  	pthread_create_fptr = NULL;
678  #endif
679  	if (pthread_create_fptr == NULL) {
680  		if (config_lazy_lock) {
681  			malloc_write("<jemalloc>: Error in dlsym(RTLD_NEXT, "
682  			    "\"pthread_create\")\n");
683  			abort();
684  		} else {
685  			pthread_create_fptr = pthread_create;
686  		}
687  	}
688  	return false;
689  }
690  void
691  background_thread_ctl_init(tsdn_t *tsdn) {
692  	malloc_mutex_assert_not_owner(tsdn, &background_thread_lock);
693  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
694  	pthread_create_fptr_init();
695  	pthread_create_wrapper_init();
696  #endif
697  }
698  #endif &bsol;* defined(JEMALLOC_BACKGROUND_THREAD) */
699  bool
700  background_thread_boot0(void) {
701  	if (!have_background_thread && opt_background_thread) {
702  		malloc_printf("<jemalloc>: option background_thread currently "
703  		    "supports pthread only\n");
704  		return true;
705  	}
706  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
707  	if ((config_lazy_lock || opt_background_thread) &&
708  	    pthread_create_fptr_init()) {
709  		return true;
710  	}
711  #endif
712  	return false;
713  }
714  bool
715  background_thread_boot1(tsdn_t *tsdn) {
716  #ifdef JEMALLOC_BACKGROUND_THREAD
717  	assert(have_background_thread);
718  	assert(narenas_total_get() > 0);
719  	if (opt_max_background_threads > MAX_BACKGROUND_THREAD_LIMIT) {
720  		opt_max_background_threads = DEFAULT_NUM_BACKGROUND_THREAD;
721  	}
722  	max_background_threads = opt_max_background_threads;
723  	background_thread_enabled_set(tsdn, opt_background_thread);
724  	if (malloc_mutex_init(&background_thread_lock,
725  	    "background_thread_global",
726  	    WITNESS_RANK_BACKGROUND_THREAD_GLOBAL,
727  	    malloc_mutex_rank_exclusive)) {
728  		return true;
729  	}
730  	background_thread_info = (background_thread_info_t *)base_alloc(tsdn,
731  	    b0get(), opt_max_background_threads *
732  	    sizeof(background_thread_info_t), CACHELINE);
733  	if (background_thread_info == NULL) {
734  		return true;
735  	}
736  	for (unsigned i = 0; i < max_background_threads; i++) {
737  		background_thread_info_t *info = &background_thread_info[i];
738  		if (malloc_mutex_init(&info->mtx, "background_thread",
739  		    WITNESS_RANK_BACKGROUND_THREAD,
740  		    malloc_mutex_address_ordered)) {
741  			return true;
742  		}
743  		if (pthread_cond_init(&info->cond, NULL)) {
744  			return true;
745  		}
746  		malloc_mutex_lock(tsdn, &info->mtx);
747  		info->state = background_thread_stopped;
748  		background_thread_info_init(tsdn, info);
749  		malloc_mutex_unlock(tsdn, &info->mtx);
750  	}
751  #endif
752  	return false;
753  }
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-background_thread_22.c</h3>
            <pre><code>1  #define JEMALLOC_BACKGROUND_THREAD_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  JEMALLOC_DIAGNOSTIC_DISABLE_SPURIOUS
6  #define BACKGROUND_THREAD_DEFAULT false
7  bool opt_background_thread = BACKGROUND_THREAD_DEFAULT;
8  size_t opt_max_background_threads = MAX_BACKGROUND_THREAD_LIMIT + 1;
9  malloc_mutex_t background_thread_lock;
10  atomic_b_t background_thread_enabled_state;
11  size_t n_background_threads;
12  size_t max_background_threads;
13  background_thread_info_t *background_thread_info;
14  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
15  static int (*pthread_create_fptr)(pthread_t *__restrict, const pthread_attr_t *,
16      void *(*)(void *), void *__restrict);
17  static void
18  pthread_create_wrapper_init(void) {
19  #ifdef JEMALLOC_LAZY_LOCK
20  	if (!isthreaded) {
21  		isthreaded = true;
22  	}
23  #endif
24  }
25  int
26  pthread_create_wrapper(pthread_t *__restrict thread, const pthread_attr_t *attr,
27      void *(*start_routine)(void *), void *__restrict arg) {
28  	pthread_create_wrapper_init();
29  	return pthread_create_fptr(thread, attr, start_routine, arg);
30  }
31  #endif &bsol;* JEMALLOC_PTHREAD_CREATE_WRAPPER */
32  #ifndef JEMALLOC_BACKGROUND_THREAD
33  #define NOT_REACHED { not_reached(); }
34  bool background_thread_create(tsd_t *tsd, unsigned arena_ind) NOT_REACHED
35  bool background_threads_enable(tsd_t *tsd) NOT_REACHED
36  bool background_threads_disable(tsd_t *tsd) NOT_REACHED
37  void background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,
38      arena_decay_t *decay, size_t npages_new) NOT_REACHED
39  void background_thread_prefork0(tsdn_t *tsdn) NOT_REACHED
40  void background_thread_prefork1(tsdn_t *tsdn) NOT_REACHED
41  void background_thread_postfork_parent(tsdn_t *tsdn) NOT_REACHED
42  void background_thread_postfork_child(tsdn_t *tsdn) NOT_REACHED
43  bool background_thread_stats_read(tsdn_t *tsdn,
44      background_thread_stats_t *stats) NOT_REACHED
45  void background_thread_ctl_init(tsdn_t *tsdn) NOT_REACHED
46  #undef NOT_REACHED
47  #else
48  static bool background_thread_enabled_at_fork;
49  static void
50  background_thread_info_init(tsdn_t *tsdn, background_thread_info_t *info) {
51  	background_thread_wakeup_time_set(tsdn, info, 0);
52  	info->npages_to_purge_new = 0;
53  	if (config_stats) {
54  		info->tot_n_runs = 0;
55  		nstime_init(&info->tot_sleep_time, 0);
56  	}
57  }
58  static inline bool
59  set_current_thread_affinity(int cpu) {
60  #if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
61  	cpu_set_t cpuset;
62  	CPU_ZERO(&cpuset);
63  	CPU_SET(cpu, &cpuset);
64  	int ret = sched_setaffinity(0, sizeof(cpu_set_t), &cpuset);
65  	return (ret != 0);
66  #else
67  	return false;
68  #endif
69  }
70  #define BACKGROUND_THREAD_NPAGES_THRESHOLD UINT64_C(1024)
71  #define BILLION UINT64_C(1000000000)
72  #define BACKGROUND_THREAD_MIN_INTERVAL_NS (BILLION / 10)
73  static inline size_t
74  decay_npurge_after_interval(arena_decay_t *decay, size_t interval) {
75  	size_t i;
76  	uint64_t sum = 0;
77  	for (i = 0; i < interval; i++) {
78  		sum += decay->backlog[i] * h_steps[i];
79  	}
80  	for (; i < SMOOTHSTEP_NSTEPS; i++) {
81  		sum += decay->backlog[i] * (h_steps[i] - h_steps[i - interval]);
82  	}
83  	return (size_t)(sum >> SMOOTHSTEP_BFP);
84  }
85  static uint64_t
86  arena_decay_compute_purge_interval_impl(tsdn_t *tsdn, arena_decay_t *decay,
87      extents_t *extents) {
88  	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
89  		return BACKGROUND_THREAD_MIN_INTERVAL_NS;
90  	}
91  	uint64_t interval;
92  	ssize_t decay_time = atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);
93  	if (decay_time <= 0) {
94  		interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
95  		goto label_done;
96  	}
97  	uint64_t decay_interval_ns = nstime_ns(&decay->interval);
98  	assert(decay_interval_ns > 0);
99  	size_t npages = extents_npages_get(extents);
100  	if (npages == 0) {
101  		unsigned i;
102  		for (i = 0; i < SMOOTHSTEP_NSTEPS; i++) {
103  			if (decay->backlog[i] > 0) {
104  				break;
105  			}
106  		}
107  		if (i == SMOOTHSTEP_NSTEPS) {
108  			interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
109  			goto label_done;
110  		}
111  	}
112  	if (npages <= BACKGROUND_THREAD_NPAGES_THRESHOLD) {
113  		interval = decay_interval_ns * SMOOTHSTEP_NSTEPS;
114  		goto label_done;
115  	}
116  	size_t lb = BACKGROUND_THREAD_MIN_INTERVAL_NS / decay_interval_ns;
117  	size_t ub = SMOOTHSTEP_NSTEPS;
118  	lb = (lb < 2) ? 2 : lb;
119  	if ((decay_interval_ns * ub <= BACKGROUND_THREAD_MIN_INTERVAL_NS) ||
120  	    (lb + 2 > ub)) {
121  		interval = BACKGROUND_THREAD_MIN_INTERVAL_NS;
122  		goto label_done;
123  	}
124  	assert(lb + 2 <= ub);
125  	size_t npurge_lb, npurge_ub;
126  	npurge_lb = decay_npurge_after_interval(decay, lb);
127  	if (npurge_lb > BACKGROUND_THREAD_NPAGES_THRESHOLD) {
128  		interval = decay_interval_ns * lb;
129  		goto label_done;
130  	}
<span onclick='openModal()' class='match'>131  	npurge_ub = decay_npurge_after_interval(decay, ub);
132  	if (npurge_ub < BACKGROUND_THREAD_NPAGES_THRESHOLD) {
133  		interval = decay_interval_ns * ub;
</span>134  		goto label_done;
135  	}
136  	unsigned n_search = 0;
137  	size_t target, npurge;
138  	while ((npurge_lb + BACKGROUND_THREAD_NPAGES_THRESHOLD < npurge_ub)
139  	    && (lb + 2 < ub)) {
140  		target = (lb + ub) / 2;
141  		npurge = decay_npurge_after_interval(decay, target);
142  		if (npurge > BACKGROUND_THREAD_NPAGES_THRESHOLD) {
143  			ub = target;
144  			npurge_ub = npurge;
145  		} else {
146  			lb = target;
147  			npurge_lb = npurge;
148  		}
149  		assert(n_search++ < lg_floor(SMOOTHSTEP_NSTEPS) + 1);
150  	}
151  	interval = decay_interval_ns * (ub + lb) / 2;
152  label_done:
153  	interval = (interval < BACKGROUND_THREAD_MIN_INTERVAL_NS) ?
154  	    BACKGROUND_THREAD_MIN_INTERVAL_NS : interval;
155  	malloc_mutex_unlock(tsdn, &decay->mtx);
156  	return interval;
157  }
158  static uint64_t
159  arena_decay_compute_purge_interval(tsdn_t *tsdn, arena_t *arena) {
160  	uint64_t i1, i2;
161  	i1 = arena_decay_compute_purge_interval_impl(tsdn, &arena->decay_dirty,
162  	    &arena->extents_dirty);
163  	if (i1 == BACKGROUND_THREAD_MIN_INTERVAL_NS) {
164  		return i1;
165  	}
166  	i2 = arena_decay_compute_purge_interval_impl(tsdn, &arena->decay_muzzy,
167  	    &arena->extents_muzzy);
168  	return i1 < i2 ? i1 : i2;
169  }
170  static void
171  background_thread_sleep(tsdn_t *tsdn, background_thread_info_t *info,
172      uint64_t interval) {
173  	if (config_stats) {
174  		info->tot_n_runs++;
175  	}
176  	info->npages_to_purge_new = 0;
177  	struct timeval tv;
178  	gettimeofday(&tv, NULL);
179  	nstime_t before_sleep;
180  	nstime_init2(&before_sleep, tv.tv_sec, tv.tv_usec * 1000);
181  	int ret;
182  	if (interval == BACKGROUND_THREAD_INDEFINITE_SLEEP) {
183  		assert(background_thread_indefinite_sleep(info));
184  		ret = pthread_cond_wait(&info->cond, &info->mtx.lock);
185  		assert(ret == 0);
186  	} else {
187  		assert(interval >= BACKGROUND_THREAD_MIN_INTERVAL_NS &&
188  		    interval <= BACKGROUND_THREAD_INDEFINITE_SLEEP);
189  		nstime_t next_wakeup;
190  		nstime_init(&next_wakeup, 0);
191  		nstime_update(&next_wakeup);
192  		nstime_iadd(&next_wakeup, interval);
193  		assert(nstime_ns(&next_wakeup) <
194  		    BACKGROUND_THREAD_INDEFINITE_SLEEP);
195  		background_thread_wakeup_time_set(tsdn, info,
196  		    nstime_ns(&next_wakeup));
197  		nstime_t ts_wakeup;
198  		nstime_copy(&ts_wakeup, &before_sleep);
199  		nstime_iadd(&ts_wakeup, interval);
200  		struct timespec ts;
201  		ts.tv_sec = (size_t)nstime_sec(&ts_wakeup);
202  		ts.tv_nsec = (size_t)nstime_nsec(&ts_wakeup);
203  		assert(!background_thread_indefinite_sleep(info));
204  		ret = pthread_cond_timedwait(&info->cond, &info->mtx.lock, &ts);
205  		assert(ret == ETIMEDOUT || ret == 0);
206  		background_thread_wakeup_time_set(tsdn, info,
207  		    BACKGROUND_THREAD_INDEFINITE_SLEEP);
208  	}
209  	if (config_stats) {
210  		gettimeofday(&tv, NULL);
211  		nstime_t after_sleep;
212  		nstime_init2(&after_sleep, tv.tv_sec, tv.tv_usec * 1000);
213  		if (nstime_compare(&after_sleep, &before_sleep) > 0) {
214  			nstime_subtract(&after_sleep, &before_sleep);
215  			nstime_add(&info->tot_sleep_time, &after_sleep);
216  		}
217  	}
218  }
219  static bool
220  background_thread_pause_check(tsdn_t *tsdn, background_thread_info_t *info) {
221  	if (unlikely(info->state == background_thread_paused)) {
222  		malloc_mutex_unlock(tsdn, &info->mtx);
223  		malloc_mutex_lock(tsdn, &background_thread_lock);
224  		malloc_mutex_unlock(tsdn, &background_thread_lock);
225  		malloc_mutex_lock(tsdn, &info->mtx);
226  		return true;
227  	}
228  	return false;
229  }
230  static inline void
231  background_work_sleep_once(tsdn_t *tsdn, background_thread_info_t *info, unsigned ind) {
232  	uint64_t min_interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
233  	unsigned narenas = narenas_total_get();
234  	for (unsigned i = ind; i < narenas; i += max_background_threads) {
235  		arena_t *arena = arena_get(tsdn, i, false);
236  		if (!arena) {
237  			continue;
238  		}
239  		arena_decay(tsdn, arena, true, false);
240  		if (min_interval == BACKGROUND_THREAD_MIN_INTERVAL_NS) {
241  			continue;
242  		}
243  		uint64_t interval = arena_decay_compute_purge_interval(tsdn,
244  		    arena);
245  		assert(interval >= BACKGROUND_THREAD_MIN_INTERVAL_NS);
246  		if (min_interval > interval) {
247  			min_interval = interval;
248  		}
249  	}
250  	background_thread_sleep(tsdn, info, min_interval);
251  }
252  static bool
253  background_threads_disable_single(tsd_t *tsd, background_thread_info_t *info) {
254  	if (info == &background_thread_info[0]) {
255  		malloc_mutex_assert_owner(tsd_tsdn(tsd),
256  		    &background_thread_lock);
257  	} else {
258  		malloc_mutex_assert_not_owner(tsd_tsdn(tsd),
259  		    &background_thread_lock);
260  	}
261  	pre_reentrancy(tsd, NULL);
262  	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
263  	bool has_thread;
264  	assert(info->state != background_thread_paused);
265  	if (info->state == background_thread_started) {
266  		has_thread = true;
267  		info->state = background_thread_stopped;
268  		pthread_cond_signal(&info->cond);
269  	} else {
270  		has_thread = false;
271  	}
272  	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
273  	if (!has_thread) {
274  		post_reentrancy(tsd);
275  		return false;
276  	}
277  	void *ret;
278  	if (pthread_join(info->thread, &ret)) {
279  		post_reentrancy(tsd);
280  		return true;
281  	}
282  	assert(ret == NULL);
283  	n_background_threads--;
284  	post_reentrancy(tsd);
285  	return false;
286  }
287  static void *background_thread_entry(void *ind_arg);
288  static int
289  background_thread_create_signals_masked(pthread_t *thread,
290      const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg) {
291  	sigset_t set;
292  	sigfillset(&set);
293  	sigset_t oldset;
294  	int mask_err = pthread_sigmask(SIG_SETMASK, &set, &oldset);
295  	if (mask_err != 0) {
296  		return mask_err;
297  	}
298  	int create_err = pthread_create_wrapper(thread, attr, start_routine,
299  	    arg);
300  	int restore_err = pthread_sigmask(SIG_SETMASK, &oldset, NULL);
301  	if (restore_err != 0) {
302  		malloc_printf("<jemalloc>: background thread creation "
303  		    "failed (%d), and signal mask restoration failed "
304  		    "(%d)\n", create_err, restore_err);
305  		if (opt_abort) {
306  			abort();
307  		}
308  	}
309  	return create_err;
310  }
311  static bool
312  check_background_thread_creation(tsd_t *tsd, unsigned *n_created,
313      bool *created_threads) {
314  	bool ret = false;
315  	if (likely(*n_created == n_background_threads)) {
316  		return ret;
317  	}
318  	tsdn_t *tsdn = tsd_tsdn(tsd);
319  	malloc_mutex_unlock(tsdn, &background_thread_info[0].mtx);
320  	for (unsigned i = 1; i < max_background_threads; i++) {
321  		if (created_threads[i]) {
322  			continue;
323  		}
324  		background_thread_info_t *info = &background_thread_info[i];
325  		malloc_mutex_lock(tsdn, &info->mtx);
326  		bool create = (info->state == background_thread_started);
327  		malloc_mutex_unlock(tsdn, &info->mtx);
328  		if (!create) {
329  			continue;
330  		}
331  		pre_reentrancy(tsd, NULL);
332  		int err = background_thread_create_signals_masked(&info->thread,
333  		    NULL, background_thread_entry, (void *)(uintptr_t)i);
334  		post_reentrancy(tsd);
335  		if (err == 0) {
336  			(*n_created)++;
337  			created_threads[i] = true;
338  		} else {
339  			malloc_printf("<jemalloc>: background thread "
340  			    "creation failed (%d)\n", err);
341  			if (opt_abort) {
342  				abort();
343  			}
344  		}
345  		ret = true;
346  		break;
347  	}
348  	malloc_mutex_lock(tsdn, &background_thread_info[0].mtx);
349  	return ret;
350  }
351  static void
352  background_thread0_work(tsd_t *tsd) {
353  	VARIABLE_ARRAY(bool, created_threads, max_background_threads);
354  	unsigned i;
355  	for (i = 1; i < max_background_threads; i++) {
356  		created_threads[i] = false;
357  	}
358  	unsigned n_created = 1;
359  	while (background_thread_info[0].state != background_thread_stopped) {
360  		if (background_thread_pause_check(tsd_tsdn(tsd),
361  		    &background_thread_info[0])) {
362  			continue;
363  		}
364  		if (check_background_thread_creation(tsd, &n_created,
365  		    (bool *)&created_threads)) {
366  			continue;
367  		}
368  		background_work_sleep_once(tsd_tsdn(tsd),
369  		    &background_thread_info[0], 0);
370  	}
371  	assert(!background_thread_enabled());
372  	for (i = 1; i < max_background_threads; i++) {
373  		background_thread_info_t *info = &background_thread_info[i];
374  		assert(info->state != background_thread_paused);
375  		if (created_threads[i]) {
376  			background_threads_disable_single(tsd, info);
377  		} else {
378  			malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
379  			if (info->state != background_thread_stopped) {
380  				assert(info->state ==
381  				    background_thread_started);
382  				n_background_threads--;
383  				info->state = background_thread_stopped;
384  			}
385  			malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
386  		}
387  	}
388  	background_thread_info[0].state = background_thread_stopped;
389  	assert(n_background_threads == 1);
390  }
391  static void
392  background_work(tsd_t *tsd, unsigned ind) {
393  	background_thread_info_t *info = &background_thread_info[ind];
394  	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
395  	background_thread_wakeup_time_set(tsd_tsdn(tsd), info,
396  	    BACKGROUND_THREAD_INDEFINITE_SLEEP);
397  	if (ind == 0) {
398  		background_thread0_work(tsd);
399  	} else {
400  		while (info->state != background_thread_stopped) {
401  			if (background_thread_pause_check(tsd_tsdn(tsd),
402  			    info)) {
403  				continue;
404  			}
405  			background_work_sleep_once(tsd_tsdn(tsd), info, ind);
406  		}
407  	}
408  	assert(info->state == background_thread_stopped);
409  	background_thread_wakeup_time_set(tsd_tsdn(tsd), info, 0);
410  	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
411  }
412  static void *
413  background_thread_entry(void *ind_arg) {
414  	unsigned thread_ind = (unsigned)(uintptr_t)ind_arg;
415  	assert(thread_ind < max_background_threads);
416  #ifdef JEMALLOC_HAVE_PTHREAD_SETNAME_NP
417  	pthread_setname_np(pthread_self(), "jemalloc_bg_thd");
418  #elif defined(__FreeBSD__)
419  	pthread_set_name_np(pthread_self(), "jemalloc_bg_thd");
420  #endif
421  	if (opt_percpu_arena != percpu_arena_disabled) {
422  		set_current_thread_affinity((int)thread_ind);
423  	}
424  	background_work(tsd_internal_fetch(), thread_ind);
425  	assert(pthread_equal(pthread_self(),
426  	    background_thread_info[thread_ind].thread));
427  	return NULL;
428  }
429  static void
430  background_thread_init(tsd_t *tsd, background_thread_info_t *info) {
431  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
432  	info->state = background_thread_started;
433  	background_thread_info_init(tsd_tsdn(tsd), info);
434  	n_background_threads++;
435  }
436  static bool
437  background_thread_create_locked(tsd_t *tsd, unsigned arena_ind) {
438  	assert(have_background_thread);
439  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
440  	size_t thread_ind = arena_ind % max_background_threads;
441  	background_thread_info_t *info = &background_thread_info[thread_ind];
442  	bool need_new_thread;
443  	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
444  	need_new_thread = background_thread_enabled() &&
445  	    (info->state == background_thread_stopped);
446  	if (need_new_thread) {
447  		background_thread_init(tsd, info);
448  	}
449  	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
450  	if (!need_new_thread) {
451  		return false;
452  	}
453  	if (arena_ind != 0) {
454  		background_thread_info_t *t0 = &background_thread_info[0];
455  		malloc_mutex_lock(tsd_tsdn(tsd), &t0->mtx);
456  		assert(t0->state == background_thread_started);
457  		pthread_cond_signal(&t0->cond);
458  		malloc_mutex_unlock(tsd_tsdn(tsd), &t0->mtx);
459  		return false;
460  	}
461  	pre_reentrancy(tsd, NULL);
462  	int err = background_thread_create_signals_masked(&info->thread, NULL,
463  	    background_thread_entry, (void *)thread_ind);
464  	post_reentrancy(tsd);
465  	if (err != 0) {
466  		malloc_printf("<jemalloc>: arena 0 background thread creation "
467  		    "failed (%d)\n", err);
468  		malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
469  		info->state = background_thread_stopped;
470  		n_background_threads--;
471  		malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
472  		return true;
473  	}
474  	return false;
475  }
476  bool
477  background_thread_create(tsd_t *tsd, unsigned arena_ind) {
478  	assert(have_background_thread);
479  	bool ret;
480  	malloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);
481  	ret = background_thread_create_locked(tsd, arena_ind);
482  	malloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);
483  	return ret;
484  }
485  bool
486  background_threads_enable(tsd_t *tsd) {
487  	assert(n_background_threads == 0);
488  	assert(background_thread_enabled());
489  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
490  	VARIABLE_ARRAY(bool, marked, max_background_threads);
491  	unsigned i, nmarked;
492  	for (i = 0; i < max_background_threads; i++) {
493  		marked[i] = false;
494  	}
495  	nmarked = 0;
496  	marked[0] = true;
497  	unsigned n = narenas_total_get();
498  	for (i = 1; i < n; i++) {
499  		if (marked[i % max_background_threads] ||
500  		    arena_get(tsd_tsdn(tsd), i, false) == NULL) {
501  			continue;
502  		}
503  		background_thread_info_t *info = &background_thread_info[
504  		    i % max_background_threads];
505  		malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
506  		assert(info->state == background_thread_stopped);
507  		background_thread_init(tsd, info);
508  		malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
509  		marked[i % max_background_threads] = true;
510  		if (++nmarked == max_background_threads) {
511  			break;
512  		}
513  	}
514  	return background_thread_create_locked(tsd, 0);
515  }
516  bool
517  background_threads_disable(tsd_t *tsd) {
518  	assert(!background_thread_enabled());
519  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
520  	if (background_threads_disable_single(tsd,
521  	    &background_thread_info[0])) {
522  		return true;
523  	}
524  	assert(n_background_threads == 0);
525  	return false;
526  }
527  void
528  background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,
529      arena_decay_t *decay, size_t npages_new) {
530  	background_thread_info_t *info = arena_background_thread_info_get(
531  	    arena);
532  	if (malloc_mutex_trylock(tsdn, &info->mtx)) {
533  		return;
534  	}
535  	if (info->state != background_thread_started) {
536  		goto label_done;
537  	}
538  	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
539  		goto label_done;
540  	}
541  	ssize_t decay_time = atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);
542  	if (decay_time <= 0) {
543  		goto label_done_unlock2;
544  	}
545  	uint64_t decay_interval_ns = nstime_ns(&decay->interval);
546  	assert(decay_interval_ns > 0);
547  	nstime_t diff;
548  	nstime_init(&diff, background_thread_wakeup_time_get(info));
549  	if (nstime_compare(&diff, &decay->epoch) <= 0) {
550  		goto label_done_unlock2;
551  	}
552  	nstime_subtract(&diff, &decay->epoch);
553  	if (nstime_ns(&diff) < BACKGROUND_THREAD_MIN_INTERVAL_NS) {
554  		goto label_done_unlock2;
555  	}
556  	if (npages_new > 0) {
557  		size_t n_epoch = (size_t)(nstime_ns(&diff) / decay_interval_ns);
558  		uint64_t npurge_new;
559  		if (n_epoch >= SMOOTHSTEP_NSTEPS) {
560  			npurge_new = npages_new;
561  		} else {
562  			uint64_t h_steps_max = h_steps[SMOOTHSTEP_NSTEPS - 1];
563  			assert(h_steps_max >=
564  			    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);
565  			npurge_new = npages_new * (h_steps_max -
566  			    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);
567  			npurge_new >>= SMOOTHSTEP_BFP;
568  		}
569  		info->npages_to_purge_new += npurge_new;
570  	}
571  	bool should_signal;
572  	if (info->npages_to_purge_new > BACKGROUND_THREAD_NPAGES_THRESHOLD) {
573  		should_signal = true;
574  	} else if (unlikely(background_thread_indefinite_sleep(info)) &&
575  	    (extents_npages_get(&arena->extents_dirty) > 0 ||
576  	    extents_npages_get(&arena->extents_muzzy) > 0 ||
577  	    info->npages_to_purge_new > 0)) {
578  		should_signal = true;
579  	} else {
580  		should_signal = false;
581  	}
582  	if (should_signal) {
583  		info->npages_to_purge_new = 0;
584  		pthread_cond_signal(&info->cond);
585  	}
586  label_done_unlock2:
587  	malloc_mutex_unlock(tsdn, &decay->mtx);
588  label_done:
589  	malloc_mutex_unlock(tsdn, &info->mtx);
590  }
591  void
592  background_thread_prefork0(tsdn_t *tsdn) {
593  	malloc_mutex_prefork(tsdn, &background_thread_lock);
594  	background_thread_enabled_at_fork = background_thread_enabled();
595  }
596  void
597  background_thread_prefork1(tsdn_t *tsdn) {
598  	for (unsigned i = 0; i < max_background_threads; i++) {
599  		malloc_mutex_prefork(tsdn, &background_thread_info[i].mtx);
600  	}
601  }
602  void
603  background_thread_postfork_parent(tsdn_t *tsdn) {
604  	for (unsigned i = 0; i < max_background_threads; i++) {
605  		malloc_mutex_postfork_parent(tsdn,
606  		    &background_thread_info[i].mtx);
607  	}
608  	malloc_mutex_postfork_parent(tsdn, &background_thread_lock);
609  }
610  void
611  background_thread_postfork_child(tsdn_t *tsdn) {
612  	for (unsigned i = 0; i < max_background_threads; i++) {
613  		malloc_mutex_postfork_child(tsdn,
614  		    &background_thread_info[i].mtx);
615  	}
616  	malloc_mutex_postfork_child(tsdn, &background_thread_lock);
617  	if (!background_thread_enabled_at_fork) {
618  		return;
619  	}
620  	malloc_mutex_lock(tsdn, &background_thread_lock);
621  	n_background_threads = 0;
622  	background_thread_enabled_set(tsdn, false);
623  	for (unsigned i = 0; i < max_background_threads; i++) {
624  		background_thread_info_t *info = &background_thread_info[i];
625  		malloc_mutex_lock(tsdn, &info->mtx);
626  		info->state = background_thread_stopped;
627  		int ret = pthread_cond_init(&info->cond, NULL);
628  		assert(ret == 0);
629  		background_thread_info_init(tsdn, info);
630  		malloc_mutex_unlock(tsdn, &info->mtx);
631  	}
632  	malloc_mutex_unlock(tsdn, &background_thread_lock);
633  }
634  bool
635  background_thread_stats_read(tsdn_t *tsdn, background_thread_stats_t *stats) {
636  	assert(config_stats);
637  	malloc_mutex_lock(tsdn, &background_thread_lock);
638  	if (!background_thread_enabled()) {
639  		malloc_mutex_unlock(tsdn, &background_thread_lock);
640  		return true;
641  	}
642  	stats->num_threads = n_background_threads;
643  	uint64_t num_runs = 0;
644  	nstime_init(&stats->run_interval, 0);
645  	for (unsigned i = 0; i < max_background_threads; i++) {
646  		background_thread_info_t *info = &background_thread_info[i];
647  		if (malloc_mutex_trylock(tsdn, &info->mtx)) {
648  			continue;
649  		}
650  		if (info->state != background_thread_stopped) {
651  			num_runs += info->tot_n_runs;
652  			nstime_add(&stats->run_interval, &info->tot_sleep_time);
653  		}
654  		malloc_mutex_unlock(tsdn, &info->mtx);
655  	}
656  	stats->num_runs = num_runs;
657  	if (num_runs > 0) {
658  		nstime_idivide(&stats->run_interval, num_runs);
659  	}
660  	malloc_mutex_unlock(tsdn, &background_thread_lock);
661  	return false;
662  }
663  #undef BACKGROUND_THREAD_NPAGES_THRESHOLD
664  #undef BILLION
665  #undef BACKGROUND_THREAD_MIN_INTERVAL_NS
666  #ifdef JEMALLOC_HAVE_DLSYM
667  #include <dlfcn.h>
668  #endif
669  static bool
670  pthread_create_fptr_init(void) {
671  	if (pthread_create_fptr != NULL) {
672  		return false;
673  	}
674  #ifdef JEMALLOC_HAVE_DLSYM
675  	pthread_create_fptr = dlsym(RTLD_NEXT, "pthread_create");
676  #else
677  	pthread_create_fptr = NULL;
678  #endif
679  	if (pthread_create_fptr == NULL) {
680  		if (config_lazy_lock) {
681  			malloc_write("<jemalloc>: Error in dlsym(RTLD_NEXT, "
682  			    "\"pthread_create\")\n");
683  			abort();
684  		} else {
685  			pthread_create_fptr = pthread_create;
686  		}
687  	}
688  	return false;
689  }
690  void
691  background_thread_ctl_init(tsdn_t *tsdn) {
692  	malloc_mutex_assert_not_owner(tsdn, &background_thread_lock);
693  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
694  	pthread_create_fptr_init();
695  	pthread_create_wrapper_init();
696  #endif
697  }
698  #endif &bsol;* defined(JEMALLOC_BACKGROUND_THREAD) */
699  bool
700  background_thread_boot0(void) {
701  	if (!have_background_thread && opt_background_thread) {
702  		malloc_printf("<jemalloc>: option background_thread currently "
703  		    "supports pthread only\n");
704  		return true;
705  	}
706  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
707  	if ((config_lazy_lock || opt_background_thread) &&
708  	    pthread_create_fptr_init()) {
709  		return true;
710  	}
711  #endif
712  	return false;
713  }
714  bool
715  background_thread_boot1(tsdn_t *tsdn) {
716  #ifdef JEMALLOC_BACKGROUND_THREAD
717  	assert(have_background_thread);
718  	assert(narenas_total_get() > 0);
719  	if (opt_max_background_threads > MAX_BACKGROUND_THREAD_LIMIT) {
720  		opt_max_background_threads = DEFAULT_NUM_BACKGROUND_THREAD;
721  	}
722  	max_background_threads = opt_max_background_threads;
723  	background_thread_enabled_set(tsdn, opt_background_thread);
724  	if (malloc_mutex_init(&background_thread_lock,
725  	    "background_thread_global",
726  	    WITNESS_RANK_BACKGROUND_THREAD_GLOBAL,
727  	    malloc_mutex_rank_exclusive)) {
728  		return true;
729  	}
730  	background_thread_info = (background_thread_info_t *)base_alloc(tsdn,
731  	    b0get(), opt_max_background_threads *
732  	    sizeof(background_thread_info_t), CACHELINE);
733  	if (background_thread_info == NULL) {
734  		return true;
735  	}
736  	for (unsigned i = 0; i < max_background_threads; i++) {
737  		background_thread_info_t *info = &background_thread_info[i];
738  		if (malloc_mutex_init(&info->mtx, "background_thread",
739  		    WITNESS_RANK_BACKGROUND_THREAD,
740  		    malloc_mutex_address_ordered)) {
741  			return true;
742  		}
743  		if (pthread_cond_init(&info->cond, NULL)) {
744  			return true;
745  		}
746  		malloc_mutex_lock(tsdn, &info->mtx);
747  		info->state = background_thread_stopped;
748  		background_thread_info_init(tsdn, info);
749  		malloc_mutex_unlock(tsdn, &info->mtx);
750  	}
751  #endif
752  	return false;
753  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-background_thread_22.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-background_thread_22.c</div>
                </div>
                <div class="column column_space"><pre><code>131  	npurge_ub = decay_npurge_after_interval(decay, ub);
132  	if (npurge_ub < BACKGROUND_THREAD_NPAGES_THRESHOLD) {
133  		interval = decay_interval_ns * ub;
</pre></code></div>
                <div class="column column_space"><pre><code>131  	npurge_ub = decay_npurge_after_interval(decay, ub);
132  	if (npurge_ub < BACKGROUND_THREAD_NPAGES_THRESHOLD) {
133  		interval = decay_interval_ns * ub;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    