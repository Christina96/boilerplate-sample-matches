
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 23, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-topology-x86.c</h3>
            <pre><code>1  #include "private/autogen/config.h"
2  #include "hwloc.h"
3  #include "private/private.h"
4  #include "private/debug.h"
5  #include "private/misc.h"
6  #include "private/cpuid-x86.h"
7  #include <sys/types.h>
8  #ifdef HAVE_DIRENT_H
9  #include <dirent.h>
10  #endif
11  #ifdef HAVE_VALGRIND_VALGRIND_H
12  #include <valgrind/valgrind.h>
13  #endif
14  struct hwloc_x86_backend_data_s {
15    unsigned nbprocs;
16    hwloc_bitmap_t apicid_set;
17    int apicid_unique;
18    char *src_cpuiddump_path;
19    int is_knl;
20  };
21  struct cpuiddump {
22    unsigned nr;
23    struct cpuiddump_entry {
24      unsigned inmask; &bsol;* which of ine[abcd]x are set on input */
25      unsigned ineax;
26      unsigned inebx;
27      unsigned inecx;
28      unsigned inedx;
29      unsigned outeax;
30      unsigned outebx;
31      unsigned outecx;
32      unsigned outedx;
33    } *entries;
34  };
35  static void
36  cpuiddump_free(struct cpuiddump *cpuiddump)
37  {
38    if (cpuiddump->nr)
39      free(cpuiddump->entries);
40    free(cpuiddump);
41  }
42  static struct cpuiddump *
43  cpuiddump_read(const char *dirpath, unsigned idx)
44  {
45    struct cpuiddump *cpuiddump;
46    struct cpuiddump_entry *cur;
47    size_t filenamelen;
48    char *filename;
49    FILE *file;
50    char line[128];
51    unsigned nr;
52    cpuiddump = malloc(sizeof(*cpuiddump));
53    if (!cpuiddump) {
54      fprintf(stderr, "Failed to allocate cpuiddump for PU #%u, ignoring cpuiddump.\n", idx);
55      goto out;
56    }
57    filenamelen = strlen(dirpath) + 15;
58    filename = malloc(filenamelen);
59    if (!filename)
60      goto out_with_dump;
61    snprintf(filename, filenamelen, "%s/pu%u", dirpath, idx);
62    file = fopen(filename, "r");
63    if (!file) {
64      fprintf(stderr, "Could not read dumped cpuid file %s, ignoring cpuiddump.\n", filename);
65      goto out_with_filename;
66    }
67    nr = 0;
68    while (fgets(line, sizeof(line), file))
69      nr++;
70    cpuiddump->entries = malloc(nr * sizeof(struct cpuiddump_entry));
71    if (!cpuiddump->entries) {
72      fprintf(stderr, "Failed to allocate %u cpuiddump entries for PU #%u, ignoring cpuiddump.\n", nr, idx);
73      goto out_with_file;
74    }
75    fseek(file, 0, SEEK_SET);
76    cur = &cpuiddump->entries[0];
77    nr = 0;
78    while (fgets(line, sizeof(line), file)) {
79      if (*line == '#')
80        continue;
81      if (sscanf(line, "%x %x %x %x %x => %x %x %x %x",
82  	      &cur->inmask,
83  	      &cur->ineax, &cur->inebx, &cur->inecx, &cur->inedx,
84  	      &cur->outeax, &cur->outebx, &cur->outecx, &cur->outedx) == 9) {
85        cur++;
86        nr++;
87      }
88    }
89    cpuiddump->nr = nr;
90    fclose(file);
91    free(filename);
92    return cpuiddump;
93   out_with_file:
94    fclose(file);
95   out_with_filename:
96    free(filename);
97   out_with_dump:
98    free(cpuiddump);
99   out:
100    return NULL;
101  }
102  static void
103  cpuiddump_find_by_input(unsigned *eax, unsigned *ebx, unsigned *ecx, unsigned *edx, struct cpuiddump *cpuiddump)
104  {
105    unsigned i;
106    for(i=0; i<cpuiddump->nr; i++) {
107      struct cpuiddump_entry *entry = &cpuiddump->entries[i];
108      if ((entry->inmask & 0x1) && *eax != entry->ineax)
109        continue;
110      if ((entry->inmask & 0x2) && *ebx != entry->inebx)
111        continue;
112      if ((entry->inmask & 0x4) && *ecx != entry->inecx)
113        continue;
114      if ((entry->inmask & 0x8) && *edx != entry->inedx)
115        continue;
116      *eax = entry->outeax;
117      *ebx = entry->outebx;
118      *ecx = entry->outecx;
119      *edx = entry->outedx;
120      return;
121    }
122    fprintf(stderr, "Couldn't find %x,%x,%x,%x in dumped cpuid, returning 0s.\n",
123  	  *eax, *ebx, *ecx, *edx);
124    *eax = 0;
125    *ebx = 0;
126    *ecx = 0;
127    *edx = 0;
128  }
129  static void cpuid_or_from_dump(unsigned *eax, unsigned *ebx, unsigned *ecx, unsigned *edx, struct cpuiddump *src_cpuiddump)
130  {
131    if (src_cpuiddump) {
132      cpuiddump_find_by_input(eax, ebx, ecx, edx, src_cpuiddump);
133    } else {
134      hwloc_x86_cpuid(eax, ebx, ecx, edx);
135    }
136  }
137  enum hwloc_x86_disc_flags {
138    HWLOC_X86_DISC_FLAG_FULL = (1<<0), &bsol;* discover everything instead of only annotating */
139    HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES = (1<<1) &bsol;* use AMD topoext numanode information */
140  };
141  #define has_topoext(features) ((features)[6] & (1 << 22))
142  #define has_x2apic(features) ((features)[4] & (1 << 21))
143  #define has_hybrid(features) ((features)[18] & (1 << 15))
144  struct cacheinfo {
145    hwloc_obj_cache_type_t type;
146    unsigned level;
147    unsigned nbthreads_sharing;
148    unsigned cacheid;
149    unsigned linesize;
150    unsigned linepart;
151    int inclusive;
152    int ways;
153    unsigned sets;
154    unsigned long size;
155  };
156  struct procinfo {
157    unsigned present;
158    unsigned apicid;
159  #define PKG 0
160  #define CORE 1
161  #define NODE 2
162  #define UNIT 3
163  #define TILE 4
164  #define MODULE 5
165  #define DIE 6
166  #define HWLOC_X86_PROCINFO_ID_NR 7
167    unsigned ids[HWLOC_X86_PROCINFO_ID_NR];
168    unsigned *otherids;
169    unsigned levels;
170    unsigned numcaches;
171    struct cacheinfo *cache;
172    char cpuvendor[13];
173    char cpumodel[3*4*4+1];
174    unsigned cpustepping;
175    unsigned cpumodelnumber;
176    unsigned cpufamilynumber;
177    unsigned hybridcoretype;
178    unsigned hybridnativemodel;
179  };
180  enum cpuid_type {
181    intel,
182    amd,
183    zhaoxin,
184    hygon,
185    unknown
186  };
187  static void setup__amd_cache_legacy(struct procinfo *infos, unsigned level, hwloc_obj_cache_type_t type, unsigned nbthreads_sharing, unsigned cpuid)
188  {
189    struct cacheinfo *cache, *tmpcaches;
190    unsigned cachenum;
191    unsigned long size = 0;
192    if (level == 1)
193      size = ((cpuid >> 24)) << 10;
194    else if (level == 2)
195      size = ((cpuid >> 16)) << 10;
196    else if (level == 3)
197      size = ((cpuid >> 18)) << 19;
198    if (!size)
199      return;
200    tmpcaches = realloc(infos->cache, (infos->numcaches+1)*sizeof(*infos->cache));
201    if (!tmpcaches)
202      return;
203    infos->cache = tmpcaches;
204    cachenum = infos->numcaches++;
205    cache = &infos->cache[cachenum];
206    cache->type = type;
207    cache->level = level;
208    cache->nbthreads_sharing = nbthreads_sharing;
209    cache->linesize = cpuid & 0xff;
210    cache->linepart = 0;
211    cache->inclusive = 0; &bsol;* old AMD (K8-K10) supposed to have exclusive caches */
212    if (level == 1) {
213      cache->ways = (cpuid >> 16) & 0xff;
214      if (cache->ways == 0xff)
215        cache->ways = -1;
216    } else {
217      static const unsigned ways_tab[] = { 0, 1, 2, 0, 4, 0, 8, 0, 16, 0, 32, 48, 64, 96, 128, -1 };
218      unsigned ways = (cpuid >> 12) & 0xf;
219      cache->ways = ways_tab[ways];
220    }
221    cache->size = size;
222    cache->sets = 0;
223    hwloc_debug("cache L%u t%u linesize %u ways %d size %luKB\n", cache->level, cache->nbthreads_sharing, cache->linesize, cache->ways, cache->size >> 10);
224  }
225  static void read_amd_caches_legacy(struct procinfo *infos, struct cpuiddump *src_cpuiddump, unsigned legacy_max_log_proc)
226  {
227    unsigned eax, ebx, ecx, edx;
228    eax = 0x80000005;
229    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
230    setup__amd_cache_legacy(infos, 1, HWLOC_OBJ_CACHE_DATA, 1, ecx); &bsol;* private L1d */
231    setup__amd_cache_legacy(infos, 1, HWLOC_OBJ_CACHE_INSTRUCTION, 1, edx); &bsol;* private L1i */
232    eax = 0x80000006;
233    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
234    if (ecx & 0xf000)
235      setup__amd_cache_legacy(infos, 2, HWLOC_OBJ_CACHE_UNIFIED, 1, ecx); &bsol;* private L2u */
236    if (edx & 0xf000)
237      setup__amd_cache_legacy(infos, 3, HWLOC_OBJ_CACHE_UNIFIED, legacy_max_log_proc, edx); &bsol;* package-wide L3u */
238  }
239  static void read_amd_caches_topoext(struct procinfo *infos, struct cpuiddump *src_cpuiddump)
240  {
241    unsigned eax, ebx, ecx, edx;
242    unsigned cachenum;
243    struct cacheinfo *cache;
244    assert(!infos->numcaches);
245    for (cachenum = 0; ; cachenum++) {
246      eax = 0x8000001d;
247      ecx = cachenum;
248      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
249      if ((eax & 0x1f) == 0)
250        break;
251      infos->numcaches++;
252    }
253    cache = infos->cache = malloc(infos->numcaches * sizeof(*infos->cache));
254    if (cache) {
255      for (cachenum = 0; ; cachenum++) {
256        unsigned long linesize, linepart, ways, sets;
257        eax = 0x8000001d;
258        ecx = cachenum;
259        cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
260        if ((eax & 0x1f) == 0)
261  	break;
262        switch (eax & 0x1f) {
263        case 1: cache->type = HWLOC_OBJ_CACHE_DATA; break;
264        case 2: cache->type = HWLOC_OBJ_CACHE_INSTRUCTION; break;
265        default: cache->type = HWLOC_OBJ_CACHE_UNIFIED; break;
266        }
267        cache->level = (eax >> 5) & 0x7;
268        cache->nbthreads_sharing = ((eax >> 14) &  0xfff) + 1;
269        cache->linesize = linesize = (ebx & 0xfff) + 1;
270        cache->linepart = linepart = ((ebx >> 12) & 0x3ff) + 1;
271        ways = ((ebx >> 22) & 0x3ff) + 1;
272        if (eax & (1 << 9))
273  	cache->ways = -1;
274        else
275  	cache->ways = ways;
276        cache->sets = sets = ecx + 1;
277        cache->size = linesize * linepart * ways * sets;
278        cache->inclusive = edx & 0x2;
279        hwloc_debug("cache %u L%u%c t%u linesize %lu linepart %lu ways %lu sets %lu, size %luKB\n",
280  		  cachenum, cache->level,
281  		  cache->type == HWLOC_OBJ_CACHE_DATA ? 'd' : cache->type == HWLOC_OBJ_CACHE_INSTRUCTION ? 'i' : 'u',
<span onclick='openModal()' class='match'>282  		  cache->nbthreads_sharing, linesize, linepart, ways, sets, cache->size >> 10);
283        cache++;
284      }
285    } else {
</span>286      infos->numcaches = 0;
287    }
288  }
289  static void read_intel_caches(struct hwloc_x86_backend_data_s *data, struct procinfo *infos, struct cpuiddump *src_cpuiddump)
290  {
291    unsigned level;
292    struct cacheinfo *tmpcaches;
293    unsigned eax, ebx, ecx, edx;
294    unsigned oldnumcaches = infos->numcaches; &bsol;* in case we got caches above */
295    unsigned cachenum;
296    struct cacheinfo *cache;
297    for (cachenum = 0; ; cachenum++) {
298      eax = 0x04;
299      ecx = cachenum;
300      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
301      hwloc_debug("cache %u type %u\n", cachenum, eax & 0x1f);
302      if ((eax & 0x1f) == 0)
303        break;
304      level = (eax >> 5) & 0x7;
305      if (data->is_knl && level == 3)
306        break;
307      infos->numcaches++;
308    }
309    tmpcaches = realloc(infos->cache, infos->numcaches * sizeof(*infos->cache));
310    if (!tmpcaches) {
311      infos->numcaches = oldnumcaches;
312    } else {
313      infos->cache = tmpcaches;
314      cache = &infos->cache[oldnumcaches];
315      for (cachenum = 0; ; cachenum++) {
316        unsigned long linesize, linepart, ways, sets;
317        eax = 0x04;
318        ecx = cachenum;
319        cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
320        if ((eax & 0x1f) == 0)
321  	break;
322        level = (eax >> 5) & 0x7;
323        if (data->is_knl && level == 3)
324  	break;
325        switch (eax & 0x1f) {
326        case 1: cache->type = HWLOC_OBJ_CACHE_DATA; break;
327        case 2: cache->type = HWLOC_OBJ_CACHE_INSTRUCTION; break;
328        default: cache->type = HWLOC_OBJ_CACHE_UNIFIED; break;
329        }
330        cache->level = level;
331        cache->nbthreads_sharing = ((eax >> 14) & 0xfff) + 1;
332        cache->linesize = linesize = (ebx & 0xfff) + 1;
333        cache->linepart = linepart = ((ebx >> 12) & 0x3ff) + 1;
334        ways = ((ebx >> 22) & 0x3ff) + 1;
335        if (eax & (1 << 9))
336          cache->ways = -1;
337        else
338          cache->ways = ways;
339        cache->sets = sets = ecx + 1;
340        cache->size = linesize * linepart * ways * sets;
341        cache->inclusive = edx & 0x2;
342        hwloc_debug("cache %u L%u%c t%u linesize %lu linepart %lu ways %lu sets %lu, size %luKB\n",
343  		  cachenum, cache->level,
344  		  cache->type == HWLOC_OBJ_CACHE_DATA ? 'd' : cache->type == HWLOC_OBJ_CACHE_INSTRUCTION ? 'i' : 'u',
345  		  cache->nbthreads_sharing, linesize, linepart, ways, sets, cache->size >> 10);
346        cache++;
347      }
348    }
349  }
350  static void read_amd_cores_legacy(struct procinfo *infos, struct cpuiddump *src_cpuiddump)
351  {
352    unsigned eax, ebx, ecx, edx;
353    unsigned max_nbcores;
354    unsigned max_nbthreads;
355    unsigned coreidsize;
356    unsigned logprocid;
357    unsigned threadid __hwloc_attribute_unused;
358    eax = 0x80000008;
359    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
360    coreidsize = (ecx >> 12) & 0xf;
361    hwloc_debug("core ID size: %u\n", coreidsize);
362    if (!coreidsize) {
363      max_nbcores = (ecx & 0xff) + 1;
364    } else
365      max_nbcores = 1 << coreidsize;
366    hwloc_debug("Thus max # of cores: %u\n", max_nbcores);
367    max_nbthreads = 1 ;
368    hwloc_debug("and max # of threads: %u\n", max_nbthreads);
369    infos->ids[PKG] = infos->apicid / max_nbcores;
370    logprocid = infos->apicid % max_nbcores;
371    infos->ids[CORE] = logprocid / max_nbthreads;
372    threadid = logprocid % max_nbthreads;
373    hwloc_debug("this is thread %u of core %u\n", threadid, infos->ids[CORE]);
374  }
375  static void read_amd_cores_topoext(struct procinfo *infos, unsigned long flags, struct cpuiddump *src_cpuiddump)
376  {
377    unsigned apic_id, nodes_per_proc = 0;
378    unsigned eax, ebx, ecx, edx;
379    eax = 0x8000001e;
380    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
381    infos->apicid = apic_id = eax;
382    if (flags & HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES) {
383      if (infos->cpufamilynumber == 0x16) {
384        infos->ids[NODE] = 0;
385        nodes_per_proc = 1;
386      } else {
387        infos->ids[NODE] = ecx & 0xff;
388        nodes_per_proc = ((ecx >> 8) & 7) + 1;
389      }
390      if ((infos->cpufamilynumber == 0x15 && nodes_per_proc > 2)
391  	|| ((infos->cpufamilynumber == 0x17 || infos->cpufamilynumber == 0x18) && nodes_per_proc > 4)
392          || (infos->cpufamilynumber == 0x19 && nodes_per_proc > 1)) {
393        hwloc_debug("warning: undefined nodes_per_proc value %u, assuming it means %u\n", nodes_per_proc, nodes_per_proc);
394      }
395    }
396    if (infos->cpufamilynumber <= 0x16) { &bsol;* topoext appeared in 0x15 and compute-units were only used in 0x15 and 0x16 */
397      unsigned cores_per_unit;
398      infos->ids[UNIT] = ebx & 0xff;
399      cores_per_unit = ((ebx >> 8) & 0xff) + 1;
400      hwloc_debug("topoext %08x, %u nodes, node %u, %u cores in unit %u\n", apic_id, nodes_per_proc, infos->ids[NODE], cores_per_unit, infos->ids[UNIT]);
401    } else {
402      unsigned threads_per_core;
403      infos->ids[CORE] = ebx & 0xff;
404      threads_per_core = ((ebx >> 8) & 0xff) + 1;
405      hwloc_debug("topoext %08x, %u nodes, node %u, %u threads in core %u\n", apic_id, nodes_per_proc, infos->ids[NODE], threads_per_core, infos->ids[CORE]);
406    }
407  }
408  static void read_intel_cores_exttopoenum(struct procinfo *infos, unsigned leaf, struct cpuiddump *src_cpuiddump)
409  {
410    unsigned level, apic_nextshift, apic_number, apic_type, apic_id = 0, apic_shift = 0, id;
411    unsigned threadid __hwloc_attribute_unused = 0; &bsol;* shut-up compiler */
412    unsigned eax, ebx, ecx = 0, edx;
413    int apic_packageshift = 0;
414    for (level = 0; ; level++) {
415      ecx = level;
416      eax = leaf;
417      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
418      if (!eax && !ebx)
419        break;
420      apic_packageshift = eax & 0x1f;
421    }
422    if (level) {
423      infos->otherids = malloc(level * sizeof(*infos->otherids));
424      if (infos->otherids) {
425        infos->levels = level;
426        for (level = 0; ; level++) {
427  	ecx = level;
428  	eax = leaf;
429  	cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
430  	if (!eax && !ebx)
431  	  break;
432  	apic_nextshift = eax & 0x1f;
433  	apic_number = ebx & 0xffff;
434  	apic_type = (ecx & 0xff00) >> 8;
435  	apic_id = edx;
436  	id = (apic_id >> apic_shift) & ((1 << (apic_packageshift - apic_shift)) - 1);
437  	hwloc_debug("x2APIC %08x %u: nextshift %u num %2u type %u id %2u\n", apic_id, level, apic_nextshift, apic_number, apic_type, id);
438  	infos->apicid = apic_id;
439  	infos->otherids[level] = UINT_MAX;
440  	switch (apic_type) {
441  	case 1:
442  	  threadid = id;
443  	  break;
444  	case 2:
445  	  infos->ids[CORE] = id;
446  	  break;
447  	case 3:
448  	  infos->ids[MODULE] = id;
449  	  break;
450  	case 4:
451  	  infos->ids[TILE] = id;
452  	  break;
453  	case 5:
454  	  infos->ids[DIE] = id;
455  	  break;
456  	default:
457  	  hwloc_debug("x2APIC %u: unknown type %u\n", level, apic_type);
458  	  infos->otherids[level] = apic_id >> apic_shift;
459  	  break;
460  	}
461  	apic_shift = apic_nextshift;
462        }
463        infos->apicid = apic_id;
464        infos->ids[PKG] = apic_id >> apic_shift;
465        hwloc_debug("x2APIC remainder: %u\n", infos->ids[PKG]);
466        hwloc_debug("this is thread %u of core %u\n", threadid, infos->ids[CORE]);
467      }
468    }
469  }
470  static void look_proc(struct hwloc_backend *backend, struct procinfo *infos, unsigned long flags, unsigned highest_cpuid, unsigned highest_ext_cpuid, unsigned *features, enum cpuid_type cpuid_type, struct cpuiddump *src_cpuiddump)
471  {
472    struct hwloc_x86_backend_data_s *data = backend->private_data;
473    unsigned eax, ebx, ecx = 0, edx;
474    unsigned cachenum;
475    struct cacheinfo *cache;
476    unsigned regs[4];
477    unsigned legacy_max_log_proc; &bsol;* not valid on Intel processors with > 256 threads, or when cpuid 0x80000008 is supported */
478    unsigned legacy_log_proc_id;
479    unsigned _model, _extendedmodel, _family, _extendedfamily;
480    infos->present = 1;
481    eax = 0x01;
482    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
483    infos->apicid = ebx >> 24;
484    if (edx & (1 << 28)) {
485      legacy_max_log_proc = 1 << hwloc_flsl(((ebx >> 16) & 0xff) - 1);
486    } else {
487      hwloc_debug("HTT bit not set in CPUID 0x01.edx, assuming legacy_max_log_proc = 1\n");
488      legacy_max_log_proc = 1;
489    }
490    hwloc_debug("APIC ID 0x%02x legacy_max_log_proc %u\n", infos->apicid, legacy_max_log_proc);
491    infos->ids[PKG] = infos->apicid / legacy_max_log_proc;
492    legacy_log_proc_id = infos->apicid % legacy_max_log_proc;
493    hwloc_debug("phys %u legacy thread %u\n", infos->ids[PKG], legacy_log_proc_id);
494    _model          = (eax>>4) & 0xf;
495    _extendedmodel  = (eax>>16) & 0xf;
496    _family         = (eax>>8) & 0xf;
497    _extendedfamily = (eax>>20) & 0xff;
498    if ((cpuid_type == intel || cpuid_type == amd || cpuid_type == hygon) && _family == 0xf) {
499      infos->cpufamilynumber = _family + _extendedfamily;
500    } else {
501      infos->cpufamilynumber = _family;
502    }
503    if ((cpuid_type == intel && (_family == 0x6 || _family == 0xf))
504        || ((cpuid_type == amd || cpuid_type == hygon) && _family == 0xf)
505        || (cpuid_type == zhaoxin && (_family == 0x6 || _family == 0x7))) {
506      infos->cpumodelnumber = _model + (_extendedmodel << 4);
507    } else {
508      infos->cpumodelnumber = _model;
509    }
510    infos->cpustepping = eax & 0xf;
511    if (cpuid_type == intel && infos->cpufamilynumber == 0x6 &&
512        (infos->cpumodelnumber == 0x57 || infos->cpumodelnumber == 0x85))
513      data->is_knl = 1; &bsol;* KNM is the same as KNL */
514    memset(regs, 0, sizeof(regs));
515    regs[0] = 0;
516    cpuid_or_from_dump(&regs[0], &regs[1], &regs[3], &regs[2], src_cpuiddump);
517    memcpy(infos->cpuvendor, regs+1, 4*3);
518    if (highest_ext_cpuid >= 0x80000004) {
519      memset(regs, 0, sizeof(regs));
520      regs[0] = 0x80000002;
521      cpuid_or_from_dump(&regs[0], &regs[1], &regs[2], &regs[3], src_cpuiddump);
522      memcpy(infos->cpumodel, regs, 4*4);
523      regs[0] = 0x80000003;
524      cpuid_or_from_dump(&regs[0], &regs[1], &regs[2], &regs[3], src_cpuiddump);
525      memcpy(infos->cpumodel + 4*4, regs, 4*4);
526      regs[0] = 0x80000004;
527      cpuid_or_from_dump(&regs[0], &regs[1], &regs[2], &regs[3], src_cpuiddump);
528      memcpy(infos->cpumodel + 4*4*2, regs, 4*4);
529    }
530    if ((cpuid_type != amd && cpuid_type != hygon) && highest_cpuid >= 0x04) {
531      eax = 0x04;
532      ecx = 0;
533      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
534      if ((eax & 0x1f) != 0) {
535        unsigned max_nbcores;
536        unsigned max_nbthreads;
537        unsigned threadid __hwloc_attribute_unused;
538        hwloc_debug("Trying to get core/thread IDs from 0x04...\n");
539        max_nbcores = ((eax >> 26) & 0x3f) + 1;
540        hwloc_debug("found %u cores max\n", max_nbcores);
541        if (!max_nbcores) {
542          hwloc_debug("cannot detect core/thread IDs from 0x04 without a valid max of cores\n");
543        } else {
544          max_nbthreads = legacy_max_log_proc / max_nbcores;
545          hwloc_debug("found %u threads max\n", max_nbthreads);
546          if (!max_nbthreads) {
547            hwloc_debug("cannot detect core/thread IDs from 0x04 without a valid max of threads\n");
548          } else {
549            threadid = legacy_log_proc_id % max_nbthreads;
550            infos->ids[CORE] = legacy_log_proc_id / max_nbthreads;
551            hwloc_debug("this is thread %u of core %u\n", threadid, infos->ids[CORE]);
552          }
553        }
554      }
555    }
556    if (highest_cpuid >= 0x1a && has_hybrid(features)) {
557      eax = 0x1a;
558      ecx = 0;
559      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
560      infos->hybridcoretype = eax >> 24;
561      infos->hybridnativemodel = eax & 0xffffff;
562    }
563    if (cpuid_type != intel && cpuid_type != zhaoxin && highest_ext_cpuid >= 0x80000008 && !has_x2apic(features)) {
564      read_amd_cores_legacy(infos, src_cpuiddump);
565    }
566    if (cpuid_type != intel && cpuid_type != zhaoxin && has_topoext(features)) {
567      read_amd_cores_topoext(infos, flags, src_cpuiddump);
568    }
569    if ((cpuid_type == intel) && highest_cpuid >= 0x1f) {
570      read_intel_cores_exttopoenum(infos, 0x1f, src_cpuiddump);
571    } else if ((cpuid_type == intel || cpuid_type == amd || cpuid_type == zhaoxin)
572  	     && highest_cpuid >= 0x0b && has_x2apic(features)) {
573      read_intel_cores_exttopoenum(infos, 0x0b, src_cpuiddump);
574    }
575    infos->numcaches = 0;
576    infos->cache = NULL;
577    if (cpuid_type != intel && cpuid_type != zhaoxin && has_topoext(features)) {
578      read_amd_caches_topoext(infos, src_cpuiddump);
579    } else if (cpuid_type != intel && cpuid_type != zhaoxin && highest_ext_cpuid >= 0x80000006) {
580      read_amd_caches_legacy(infos, src_cpuiddump, legacy_max_log_proc);
581    }
582    if ((cpuid_type != amd && cpuid_type != hygon) && highest_cpuid >= 0x04) {
583      read_intel_caches(data, infos, src_cpuiddump);
584    }
585    for (cachenum = 0; cachenum < infos->numcaches; cachenum++) {
586      cache = &infos->cache[cachenum];
587      cache->cacheid = infos->apicid / cache->nbthreads_sharing;
588      if (cpuid_type == intel) {
589        unsigned bits = hwloc_flsl(cache->nbthreads_sharing-1);
590        unsigned mask = ~((1U<<bits) - 1);
591        cache->cacheid = infos->apicid & mask;
592      } else if (cpuid_type == amd) {
593        if (infos->cpufamilynumber >= 0x17 && cache->level == 3) {
594          unsigned nbapics_sharing = cache->nbthreads_sharing;
595          if (nbapics_sharing & (nbapics_sharing-1))
596            nbapics_sharing = 1U<<(1+hwloc_ffsl(nbapics_sharing));
597  	cache->cacheid = infos->apicid / nbapics_sharing;
598        } else if (infos->cpufamilynumber== 0x10 && infos->cpumodelnumber == 0x9
599  	  && cache->level == 3
600  	  && (cache->ways == -1 || (cache->ways % 2 == 0)) && cache->nbthreads_sharing >= 8) {
601  	if (cache->nbthreads_sharing == 16)
602  	  cache->nbthreads_sharing = 12; &bsol;* nbthreads_sharing is a power of 2 but the processor actually has 8 or 12 cores */
603  	cache->nbthreads_sharing /= 2;
604  	cache->size /= 2;
605  	if (cache->ways != -1)
606  	  cache->ways /= 2;
607  	cache->cacheid = (infos->apicid % legacy_max_log_proc) / cache->nbthreads_sharing &bsol;* cacheid within the package */
608  	  + 2 * (infos->apicid / legacy_max_log_proc); &bsol;* add 2 caches per previous package */
609        } else if (infos->cpufamilynumber == 0x15
610  		 && (infos->cpumodelnumber == 0x1 &bsol;* Bulldozer */ || infos->cpumodelnumber == 0x2 &bsol;* Piledriver */)
611  		 && cache->level == 3 && cache->nbthreads_sharing == 6) {
612  	cache->cacheid = (infos->apicid % legacy_max_log_proc) / cache->nbthreads_sharing &bsol;* cacheid within the package */
613  	  + 2 * (infos->apicid / legacy_max_log_proc); &bsol;* add 2 cache per previous package */
614        }
615      } else if (cpuid_type == hygon) {
616        if (infos->cpufamilynumber == 0x18
617  	  && cache->level == 3 && cache->nbthreads_sharing == 6) {
618          cache->cacheid = infos->apicid / 8;
619        }
620      }
621    }
622    if (hwloc_bitmap_isset(data->apicid_set, infos->apicid))
623      data->apicid_unique = 0;
624    else
625      hwloc_bitmap_set(data->apicid_set, infos->apicid);
626  }
627  static void
628  hwloc_x86_add_cpuinfos(hwloc_obj_t obj, struct procinfo *info, int replace)
629  {
630    char number[12];
631    if (info->cpuvendor[0])
632      hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUVendor", info->cpuvendor, replace);
633    snprintf(number, sizeof(number), "%u", info->cpufamilynumber);
634    hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUFamilyNumber", number, replace);
635    snprintf(number, sizeof(number), "%u", info->cpumodelnumber);
636    hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUModelNumber", number, replace);
637    if (info->cpumodel[0]) {
638      const char *c = info->cpumodel;
639      while (*c == ' ')
640        c++;
641      hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUModel", c, replace);
642    }
643    snprintf(number, sizeof(number), "%u", info->cpustepping);
644    hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUStepping", number, replace);
645  }
646  static void
647  hwloc_x86_add_groups(hwloc_topology_t topology,
648  		     struct procinfo *infos,
649  		     unsigned nbprocs,
650  		     hwloc_bitmap_t remaining_cpuset,
651  		     unsigned type,
652  		     const char *subtype,
653  		     unsigned kind,
654  		     int dont_merge)
655  {
656    hwloc_bitmap_t obj_cpuset;
657    hwloc_obj_t obj;
658    unsigned i, j;
659    while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
660      unsigned packageid = infos[i].ids[PKG];
661      unsigned id = infos[i].ids[type];
662      if (id == (unsigned)-1) {
663        hwloc_bitmap_clr(remaining_cpuset, i);
664        continue;
665      }
666      obj_cpuset = hwloc_bitmap_alloc();
667      for (j = i; j < nbprocs; j++) {
668        if (infos[j].ids[type] == (unsigned) -1) {
669  	hwloc_bitmap_clr(remaining_cpuset, j);
670  	continue;
671        }
672        if (infos[j].ids[PKG] == packageid && infos[j].ids[type] == id) {
673  	hwloc_bitmap_set(obj_cpuset, j);
674  	hwloc_bitmap_clr(remaining_cpuset, j);
675        }
676      }
677      obj = hwloc_alloc_setup_object(topology, HWLOC_OBJ_GROUP, id);
678      obj->cpuset = obj_cpuset;
679      obj->subtype = strdup(subtype);
680      obj->attr->group.kind = kind;
681      obj->attr->group.dont_merge = dont_merge;
682      hwloc_debug_2args_bitmap("os %s %u has cpuset %s\n",
683  			     subtype, id, obj_cpuset);
684      hwloc__insert_object_by_cpuset(topology, NULL, obj, "x86:group");
685    }
686  }
687  static void summarize(struct hwloc_backend *backend, struct procinfo *infos, unsigned long flags)
688  {
689    struct hwloc_topology *topology = backend->topology;
690    struct hwloc_x86_backend_data_s *data = backend->private_data;
691    unsigned nbprocs = data->nbprocs;
692    hwloc_bitmap_t complete_cpuset = hwloc_bitmap_alloc();
693    unsigned i, j, l, level;
694    int one = -1;
695    hwloc_bitmap_t remaining_cpuset;
696    int gotnuma = 0;
697    int fulldiscovery = (flags & HWLOC_X86_DISC_FLAG_FULL);
698  #ifdef HWLOC_DEBUG
699    hwloc_debug("\nSummary of x86 CPUID topology:\n");
700    for(i=0; i<nbprocs; i++) {
701      hwloc_debug("PU %u present=%u apicid=%u on PKG %d CORE %d DIE %d NODE %d\n",
702                  i, infos[i].present, infos[i].apicid,
703                  infos[i].ids[PKG], infos[i].ids[CORE], infos[i].ids[DIE], infos[i].ids[NODE]);
704    }
705    hwloc_debug("\n");
706  #endif
707    for (i = 0; i < nbprocs; i++)
708      if (infos[i].present) {
709        hwloc_bitmap_set(complete_cpuset, i);
710        one = i;
711      }
712    if (one == -1) {
713      hwloc_bitmap_free(complete_cpuset);
714      return;
715    }
716    remaining_cpuset = hwloc_bitmap_alloc();
717    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_PACKAGE)) {
718      hwloc_obj_t package;
719      hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
720      while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
721        if (fulldiscovery) {
722  	unsigned packageid = infos[i].ids[PKG];
723  	hwloc_bitmap_t package_cpuset = hwloc_bitmap_alloc();
724  	for (j = i; j < nbprocs; j++) {
725  	  if (infos[j].ids[PKG] == packageid) {
726  	    hwloc_bitmap_set(package_cpuset, j);
727  	    hwloc_bitmap_clr(remaining_cpuset, j);
728  	  }
729  	}
730  	package = hwloc_alloc_setup_object(topology, HWLOC_OBJ_PACKAGE, packageid);
731  	package->cpuset = package_cpuset;
732  	hwloc_x86_add_cpuinfos(package, &infos[i], 0);
733  	hwloc_debug_1arg_bitmap("os package %u has cpuset %s\n",
734  				packageid, package_cpuset);
735  	hwloc__insert_object_by_cpuset(topology, NULL, package, "x86:package");
736        } else {
737  	hwloc_bitmap_t set = hwloc_bitmap_alloc();
738  	hwloc_bitmap_set(set, i);
739  	package = hwloc_get_next_obj_covering_cpuset_by_type(topology, set, HWLOC_OBJ_PACKAGE, NULL);
740  	hwloc_bitmap_free(set);
741  	if (package) {
742  	  hwloc_x86_add_cpuinfos(package, &infos[i], 1);
743  	  hwloc_bitmap_andnot(remaining_cpuset, remaining_cpuset, package->cpuset);
744  	} else {
745  	  hwloc_x86_add_cpuinfos(hwloc_get_root_obj(topology), &infos[i], 1);
746  	  break;
747  	}
748        }
749      }
750    }
751    if (fulldiscovery && (flags & HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES)) {
752      hwloc_bitmap_t node_cpuset;
753      hwloc_obj_t node;
754      hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
755      while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
756        unsigned packageid = infos[i].ids[PKG];
757        unsigned nodeid = infos[i].ids[NODE];
758        if (nodeid == (unsigned)-1) {
759          hwloc_bitmap_clr(remaining_cpuset, i);
760  	continue;
761        }
762        node_cpuset = hwloc_bitmap_alloc();
763        for (j = i; j < nbprocs; j++) {
764  	if (infos[j].ids[NODE] == (unsigned) -1) {
765  	  hwloc_bitmap_clr(remaining_cpuset, j);
766  	  continue;
767  	}
768          if (infos[j].ids[PKG] == packageid && infos[j].ids[NODE] == nodeid) {
769            hwloc_bitmap_set(node_cpuset, j);
770            hwloc_bitmap_clr(remaining_cpuset, j);
771          }
772        }
773        node = hwloc_alloc_setup_object(topology, HWLOC_OBJ_NUMANODE, nodeid);
774        node->cpuset = node_cpuset;
775        node->nodeset = hwloc_bitmap_alloc();
776        hwloc_bitmap_set(node->nodeset, nodeid);
777        hwloc_debug_1arg_bitmap("os node %u has cpuset %s\n",
778            nodeid, node_cpuset);
779        hwloc__insert_object_by_cpuset(topology, NULL, node, "x86:numa");
780        gotnuma++;
781      }
782    }
783    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_GROUP)) {
784      if (fulldiscovery) {
785        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
786        hwloc_x86_add_groups(topology, infos, nbprocs, remaining_cpuset,
787  			   UNIT, "Compute Unit",
788  			   HWLOC_GROUP_KIND_AMD_COMPUTE_UNIT, 0);
789        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
790        hwloc_x86_add_groups(topology, infos, nbprocs, remaining_cpuset,
791  			   MODULE, "Module",
792  			   HWLOC_GROUP_KIND_INTEL_MODULE, 0);
793        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
794        hwloc_x86_add_groups(topology, infos, nbprocs, remaining_cpuset,
795  			   TILE, "Tile",
796  			   HWLOC_GROUP_KIND_INTEL_TILE, 0);
797        if (infos[one].otherids) {
798  	for (level = infos[one].levels-1; level <= infos[one].levels-1; level--) {
799  	  if (infos[one].otherids[level] != UINT_MAX) {
800  	    hwloc_bitmap_t unknown_cpuset;
801  	    hwloc_obj_t unknown_obj;
802  	    hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
803  	    while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
804  	      unsigned unknownid = infos[i].otherids[level];
805  	      unknown_cpuset = hwloc_bitmap_alloc();
806  	      for (j = i; j < nbprocs; j++) {
807  		if (infos[j].otherids[level] == unknownid) {
808  		  hwloc_bitmap_set(unknown_cpuset, j);
809  		  hwloc_bitmap_clr(remaining_cpuset, j);
810  		}
811  	      }
812  	      unknown_obj = hwloc_alloc_setup_object(topology, HWLOC_OBJ_GROUP, unknownid);
813  	      unknown_obj->cpuset = unknown_cpuset;
814  	      unknown_obj->attr->group.kind = HWLOC_GROUP_KIND_INTEL_EXTTOPOENUM_UNKNOWN;
815  	      unknown_obj->attr->group.subkind = level;
816  	      hwloc_debug_2args_bitmap("os unknown%u %u has cpuset %s\n",
817  				       level, unknownid, unknown_cpuset);
818  	      hwloc__insert_object_by_cpuset(topology, NULL, unknown_obj, "x86:group:unknown");
819  	    }
820  	  }
821  	}
822        }
823      }
824    }
825    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_DIE)) {
826      if (fulldiscovery) {
827        hwloc_bitmap_t die_cpuset;
828        hwloc_obj_t die;
829        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
830        while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
831  	unsigned packageid = infos[i].ids[PKG];
832  	unsigned dieid = infos[i].ids[DIE];
833  	if (dieid == (unsigned) -1) {
834  	  hwloc_bitmap_clr(remaining_cpuset, i);
835  	  continue;
836  	}
837  	die_cpuset = hwloc_bitmap_alloc();
838  	for (j = i; j < nbprocs; j++) {
839  	  if (infos[j].ids[DIE] == (unsigned) -1) {
840  	    hwloc_bitmap_clr(remaining_cpuset, j);
841  	    continue;
842  	  }
843  	  if (infos[j].ids[PKG] == packageid && infos[j].ids[DIE] == dieid) {
844  	    hwloc_bitmap_set(die_cpuset, j);
845  	    hwloc_bitmap_clr(remaining_cpuset, j);
846  	  }
847  	}
848  	die = hwloc_alloc_setup_object(topology, HWLOC_OBJ_DIE, dieid);
849  	die->cpuset = die_cpuset;
850  	hwloc_debug_1arg_bitmap("os die %u has cpuset %s\n",
851  				dieid, die_cpuset);
852  	hwloc__insert_object_by_cpuset(topology, NULL, die, "x86:die");
853        }
854      }
855    }
856    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_CORE)) {
857      if (fulldiscovery) {
858        hwloc_bitmap_t core_cpuset;
859        hwloc_obj_t core;
860        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
861        while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
862  	unsigned packageid = infos[i].ids[PKG];
863  	unsigned nodeid = infos[i].ids[NODE];
864  	unsigned coreid = infos[i].ids[CORE];
865  	if (coreid == (unsigned) -1) {
866  	  hwloc_bitmap_clr(remaining_cpuset, i);
867  	  continue;
868  	}
869  	core_cpuset = hwloc_bitmap_alloc();
870  	for (j = i; j < nbprocs; j++) {
871  	  if (infos[j].ids[CORE] == (unsigned) -1) {
872  	    hwloc_bitmap_clr(remaining_cpuset, j);
873  	    continue;
874  	  }
875  	  if (infos[j].ids[PKG] == packageid && infos[j].ids[NODE] == nodeid && infos[j].ids[CORE] == coreid) {
876  	    hwloc_bitmap_set(core_cpuset, j);
877  	    hwloc_bitmap_clr(remaining_cpuset, j);
878  	  }
879  	}
880  	core = hwloc_alloc_setup_object(topology, HWLOC_OBJ_CORE, coreid);
881  	core->cpuset = core_cpuset;
882  	hwloc_debug_1arg_bitmap("os core %u has cpuset %s\n",
883  				coreid, core_cpuset);
884  	hwloc__insert_object_by_cpuset(topology, NULL, core, "x86:core");
885        }
886      }
887    }
888    if (fulldiscovery) {
889      hwloc_debug("%s", "\n\n * CPU cpusets *\n\n");
890      for (i=0; i<nbprocs; i++)
891        if(infos[i].present) { &bsol;* Only add present PU. We don't know if others actually exist */
892         struct hwloc_obj *obj = hwloc_alloc_setup_object(topology, HWLOC_OBJ_PU, i);
893         obj->cpuset = hwloc_bitmap_alloc();
894         hwloc_bitmap_only(obj->cpuset, i);
895         hwloc_debug_1arg_bitmap("PU %u has cpuset %s\n", i, obj->cpuset);
896         hwloc__insert_object_by_cpuset(topology, NULL, obj, "x86:pu");
897       }
898    }
899    level = 0;
900    for (i = 0; i < nbprocs; i++)
901      for (j = 0; j < infos[i].numcaches; j++)
902        if (infos[i].cache[j].level > level)
903          level = infos[i].cache[j].level;
904    while (level > 0) {
905      hwloc_obj_cache_type_t type;
906      HWLOC_BUILD_ASSERT(HWLOC_OBJ_CACHE_DATA == HWLOC_OBJ_CACHE_UNIFIED+1);
907      HWLOC_BUILD_ASSERT(HWLOC_OBJ_CACHE_INSTRUCTION == HWLOC_OBJ_CACHE_DATA+1);
908      for (type = HWLOC_OBJ_CACHE_UNIFIED; type <= HWLOC_OBJ_CACHE_INSTRUCTION; type++) {
909        hwloc_obj_type_t otype;
910        hwloc_obj_t cache;
911        otype = hwloc_cache_type_by_depth_type(level, type);
912        if (otype == HWLOC_OBJ_TYPE_NONE)
913  	continue;
914        if (!hwloc_filter_check_keep_object_type(topology, otype))
915  	continue;
916        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
917        while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
918  	hwloc_bitmap_t puset;
919  	for (l = 0; l < infos[i].numcaches; l++) {
920  	  if (infos[i].cache[l].level == level && infos[i].cache[l].type == type)
921  	    break;
922  	}
923  	if (l == infos[i].numcaches) {
924  	  hwloc_bitmap_clr(remaining_cpuset, i);
925  	  continue;
926  	}
927  	puset = hwloc_bitmap_alloc();
928  	hwloc_bitmap_set(puset, i);
929  	cache = hwloc_get_next_obj_covering_cpuset_by_type(topology, puset, otype, NULL);
930  	hwloc_bitmap_free(puset);
931  	if (cache) {
932  	  if (!hwloc_obj_get_info_by_name(cache, "Inclusive"))
933  	    hwloc_obj_add_info(cache, "Inclusive", infos[i].cache[l].inclusive ? "1" : "0");
934  	  hwloc_bitmap_andnot(remaining_cpuset, remaining_cpuset, cache->cpuset);
935  	} else {
936  	  hwloc_bitmap_t cache_cpuset;
937  	  unsigned packageid = infos[i].ids[PKG];
938  	  unsigned cacheid = infos[i].cache[l].cacheid;
939  	  cache_cpuset = hwloc_bitmap_alloc();
940  	  for (j = i; j < nbprocs; j++) {
941  	    unsigned l2;
942  	    for (l2 = 0; l2 < infos[j].numcaches; l2++) {
943  	      if (infos[j].cache[l2].level == level && infos[j].cache[l2].type == type)
944  		break;
945  	    }
946  	    if (l2 == infos[j].numcaches) {
947  	      hwloc_bitmap_clr(remaining_cpuset, j);
948  	      continue;
949  	    }
950  	    if (infos[j].ids[PKG] == packageid && infos[j].cache[l2].cacheid == cacheid) {
951  	      hwloc_bitmap_set(cache_cpuset, j);
952  	      hwloc_bitmap_clr(remaining_cpuset, j);
953  	    }
954  	  }
955  	  cache = hwloc_alloc_setup_object(topology, otype, HWLOC_UNKNOWN_INDEX);
956  	  cache->attr->cache.depth = level;
957  	  cache->attr->cache.size = infos[i].cache[l].size;
958  	  cache->attr->cache.linesize = infos[i].cache[l].linesize;
959  	  cache->attr->cache.associativity = infos[i].cache[l].ways;
960  	  cache->attr->cache.type = infos[i].cache[l].type;
961  	  cache->cpuset = cache_cpuset;
962  	  hwloc_obj_add_info(cache, "Inclusive", infos[i].cache[l].inclusive ? "1" : "0");
963  	  hwloc_debug_2args_bitmap("os L%u cache %u has cpuset %s\n",
964  				   level, cacheid, cache_cpuset);
965  	  hwloc__insert_object_by_cpuset(topology, NULL, cache, "x86:cache");
966  	}
967        }
968      }
969      level--;
970    }
971    hwloc_bitmap_free(remaining_cpuset);
972    hwloc_bitmap_free(complete_cpuset);
973    if (gotnuma)
974      topology->support.discovery->numa = 1;
975  }
976  static int
977  look_procs(struct hwloc_backend *backend, struct procinfo *infos, unsigned long flags,
978  	   unsigned highest_cpuid, unsigned highest_ext_cpuid, unsigned *features, enum cpuid_type cpuid_type,
979  	   int (*get_cpubind)(hwloc_topology_t topology, hwloc_cpuset_t set, int flags),
980  	   int (*set_cpubind)(hwloc_topology_t topology, hwloc_const_cpuset_t set, int flags),
981             hwloc_bitmap_t restrict_set)
982  {
983    struct hwloc_x86_backend_data_s *data = backend->private_data;
984    struct hwloc_topology *topology = backend->topology;
985    unsigned nbprocs = data->nbprocs;
986    hwloc_bitmap_t orig_cpuset = NULL;
987    hwloc_bitmap_t set = NULL;
988    unsigned i;
989    if (!data->src_cpuiddump_path) {
990      orig_cpuset = hwloc_bitmap_alloc();
991      if (get_cpubind(topology, orig_cpuset, HWLOC_CPUBIND_STRICT)) {
992        hwloc_bitmap_free(orig_cpuset);
993        return -1;
994      }
995      set = hwloc_bitmap_alloc();
996    }
997    for (i = 0; i < nbprocs; i++) {
998      struct cpuiddump *src_cpuiddump = NULL;
999      if (restrict_set && !hwloc_bitmap_isset(restrict_set, i)) {
1000        continue;
1001      }
1002      if (data->src_cpuiddump_path) {
1003        src_cpuiddump = cpuiddump_read(data->src_cpuiddump_path, i);
1004        if (!src_cpuiddump)
1005  	continue;
1006      } else {
1007        hwloc_bitmap_only(set, i);
1008        hwloc_debug("binding to CPU%u\n", i);
1009        if (set_cpubind(topology, set, HWLOC_CPUBIND_STRICT)) {
1010  	hwloc_debug("could not bind to CPU%u: %s\n", i, strerror(errno));
1011  	continue;
1012        }
1013      }
1014      look_proc(backend, &infos[i], flags, highest_cpuid, highest_ext_cpuid, features, cpuid_type, src_cpuiddump);
1015      if (data->src_cpuiddump_path) {
1016        cpuiddump_free(src_cpuiddump);
1017      }
1018    }
1019    if (!data->src_cpuiddump_path) {
1020      set_cpubind(topology, orig_cpuset, 0);
1021      hwloc_bitmap_free(set);
1022      hwloc_bitmap_free(orig_cpuset);
1023    }
1024    if (data->apicid_unique) {
1025      summarize(backend, infos, flags);
1026      if (has_hybrid(features) && !(topology->flags & HWLOC_TOPOLOGY_FLAG_NO_CPUKINDS)) {
1027        hwloc_bitmap_t atomset = hwloc_bitmap_alloc();
1028        hwloc_bitmap_t coreset = hwloc_bitmap_alloc();
1029        for(i=0; i<nbprocs; i++) {
1030          if (infos[i].hybridcoretype == 0x20)
1031            hwloc_bitmap_set(atomset, i);
1032          else if (infos[i].hybridcoretype == 0x40)
1033            hwloc_bitmap_set(coreset, i);
1034        }
1035        if (!hwloc_bitmap_iszero(atomset)) {
1036          struct hwloc_info_s infoattr;
1037          infoattr.name = (char *) "CoreType";
1038          infoattr.value = (char *) "IntelAtom";
1039          hwloc_internal_cpukinds_register(topology, atomset, HWLOC_CPUKIND_EFFICIENCY_UNKNOWN, &infoattr, 1, 0);
1040        } else {
1041          hwloc_bitmap_free(atomset);
1042        }
1043        if (!hwloc_bitmap_iszero(coreset)) {
1044          struct hwloc_info_s infoattr;
1045          infoattr.name = (char *) "CoreType";
1046          infoattr.value = (char *) "IntelCore";
1047          hwloc_internal_cpukinds_register(topology, coreset, HWLOC_CPUKIND_EFFICIENCY_UNKNOWN, &infoattr, 1, 0);
1048        } else {
1049          hwloc_bitmap_free(coreset);
1050        }
1051      }
1052    }
1053    return 0;
1054  }
1055  #if defined HWLOC_FREEBSD_SYS && defined HAVE_CPUSET_SETID
1056  #include <sys/param.h>
1057  #include <sys/cpuset.h>
1058  typedef cpusetid_t hwloc_x86_os_state_t;
1059  static void hwloc_x86_os_state_save(hwloc_x86_os_state_t *state, struct cpuiddump *src_cpuiddump)
1060  {
1061    if (!src_cpuiddump) {
1062      cpuset_getid(CPU_LEVEL_CPUSET, CPU_WHICH_PID, -1, state);
1063      cpuset_setid(CPU_WHICH_PID, -1, 0);
1064    }
1065  }
1066  static void hwloc_x86_os_state_restore(hwloc_x86_os_state_t *state, struct cpuiddump *src_cpuiddump)
1067  {
1068    if (!src_cpuiddump) {
1069      cpuset_setid(CPU_WHICH_PID, -1, *state);
1070    }
1071  }
1072  #else &bsol;* !defined HWLOC_FREEBSD_SYS || !defined HAVE_CPUSET_SETID */
1073  typedef void * hwloc_x86_os_state_t;
1074  static void hwloc_x86_os_state_save(hwloc_x86_os_state_t *state __hwloc_attribute_unused, struct cpuiddump *src_cpuiddump __hwloc_attribute_unused) { }
1075  static void hwloc_x86_os_state_restore(hwloc_x86_os_state_t *state __hwloc_attribute_unused, struct cpuiddump *src_cpuiddump __hwloc_attribute_unused) { }
1076  #endif &bsol;* !defined HWLOC_FREEBSD_SYS || !defined HAVE_CPUSET_SETID */
1077  #define INTEL_EBX ('G' | ('e'<<8) | ('n'<<16) | ('u'<<24))
1078  #define INTEL_EDX ('i' | ('n'<<8) | ('e'<<16) | ('I'<<24))
1079  #define INTEL_ECX ('n' | ('t'<<8) | ('e'<<16) | ('l'<<24))
1080  #define AMD_EBX ('A' | ('u'<<8) | ('t'<<16) | ('h'<<24))
1081  #define AMD_EDX ('e' | ('n'<<8) | ('t'<<16) | ('i'<<24))
1082  #define AMD_ECX ('c' | ('A'<<8) | ('M'<<16) | ('D'<<24))
1083  #define HYGON_EBX ('H' | ('y'<<8) | ('g'<<16) | ('o'<<24))
1084  #define HYGON_EDX ('n' | ('G'<<8) | ('e'<<16) | ('n'<<24))
1085  #define HYGON_ECX ('u' | ('i'<<8) | ('n'<<16) | ('e'<<24))
1086  #define ZX_EBX ('C' | ('e'<<8) | ('n'<<16) | ('t'<<24))
1087  #define ZX_EDX ('a' | ('u'<<8) | ('r'<<16) | ('H'<<24))
1088  #define ZX_ECX ('a' | ('u'<<8) | ('l'<<16) | ('s'<<24))
1089  #define SH_EBX (' ' | (' '<<8) | ('S'<<16) | ('h'<<24))
1090  #define SH_EDX ('a' | ('n'<<8) | ('g'<<16) | ('h'<<24))
1091  #define SH_ECX ('a' | ('i'<<8) | (' '<<16) | (' '<<24))
1092  static int fake_get_cpubind(hwloc_topology_t topology __hwloc_attribute_unused,
1093  			    hwloc_cpuset_t set __hwloc_attribute_unused,
1094  			    int flags __hwloc_attribute_unused)
1095  {
1096    return 0;
1097  }
1098  static int fake_set_cpubind(hwloc_topology_t topology __hwloc_attribute_unused,
1099  			    hwloc_const_cpuset_t set __hwloc_attribute_unused,
1100  			    int flags __hwloc_attribute_unused)
1101  {
1102    return 0;
1103  }
1104  static
1105  int hwloc_look_x86(struct hwloc_backend *backend, unsigned long flags)
1106  {
1107    struct hwloc_x86_backend_data_s *data = backend->private_data;
1108    struct hwloc_topology *topology = backend->topology;
1109    unsigned nbprocs = data->nbprocs;
1110    unsigned eax, ebx, ecx = 0, edx;
1111    unsigned i;
1112    unsigned highest_cpuid;
1113    unsigned highest_ext_cpuid;
1114    unsigned features[19] = { 0 };
1115    struct procinfo *infos = NULL;
1116    enum cpuid_type cpuid_type = unknown;
1117    hwloc_x86_os_state_t os_state;
1118    struct hwloc_binding_hooks hooks;
1119    struct hwloc_topology_support support;
1120    struct hwloc_topology_membind_support memsupport __hwloc_attribute_unused;
1121    int (*get_cpubind)(hwloc_topology_t topology, hwloc_cpuset_t set, int flags) = NULL;
1122    int (*set_cpubind)(hwloc_topology_t topology, hwloc_const_cpuset_t set, int flags) = NULL;
1123    hwloc_bitmap_t restrict_set = NULL;
1124    struct cpuiddump *src_cpuiddump = NULL;
1125    int ret = -1;
1126    memset(&hooks, 0, sizeof(hooks));
1127    support.membind = &memsupport;
1128    hwloc_set_native_binding_hooks(&hooks, &support);
1129    if (data->src_cpuiddump_path) {
1130      src_cpuiddump = cpuiddump_read(data->src_cpuiddump_path, 0);
1131      if (!src_cpuiddump)
1132        goto out;
1133    } else {
1134      if (hooks.get_thisthread_cpubind && hooks.set_thisthread_cpubind) {
1135        get_cpubind = hooks.get_thisthread_cpubind;
1136        set_cpubind = hooks.set_thisthread_cpubind;
1137      } else if (hooks.get_thisproc_cpubind && hooks.set_thisproc_cpubind) {
1138        get_cpubind = hooks.get_thisproc_cpubind;
1139        set_cpubind = hooks.set_thisproc_cpubind;
1140      } else {
1141        if (nbprocs > 1)
1142  	goto out;
1143        get_cpubind = fake_get_cpubind;
1144        set_cpubind = fake_set_cpubind;
1145      }
1146    }
1147    if (topology->flags & HWLOC_TOPOLOGY_FLAG_RESTRICT_TO_CPUBINDING) {
1148      restrict_set = hwloc_bitmap_alloc();
1149      if (!restrict_set)
1150        goto out;
1151      if (hooks.get_thisproc_cpubind)
1152        hooks.get_thisproc_cpubind(topology, restrict_set, 0);
1153      else if (hooks.get_thisthread_cpubind)
1154        hooks.get_thisthread_cpubind(topology, restrict_set, 0);
1155      if (hwloc_bitmap_iszero(restrict_set)) {
1156        hwloc_bitmap_free(restrict_set);
1157        restrict_set = NULL;
1158      }
1159    }
1160    if (!src_cpuiddump && !hwloc_have_x86_cpuid())
1161      goto out;
1162    infos = calloc(nbprocs, sizeof(struct procinfo));
1163    if (NULL == infos)
1164      goto out;
1165    for (i = 0; i < nbprocs; i++) {
1166      infos[i].ids[PKG] = (unsigned) -1;
1167      infos[i].ids[CORE] = (unsigned) -1;
1168      infos[i].ids[NODE] = (unsigned) -1;
1169      infos[i].ids[UNIT] = (unsigned) -1;
1170      infos[i].ids[TILE] = (unsigned) -1;
1171      infos[i].ids[MODULE] = (unsigned) -1;
1172      infos[i].ids[DIE] = (unsigned) -1;
1173    }
1174    eax = 0x00;
1175    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1176    highest_cpuid = eax;
1177    if (ebx == INTEL_EBX && ecx == INTEL_ECX && edx == INTEL_EDX)
1178      cpuid_type = intel;
1179    else if (ebx == AMD_EBX && ecx == AMD_ECX && edx == AMD_EDX)
1180      cpuid_type = amd;
1181    else if ((ebx == ZX_EBX && ecx == ZX_ECX && edx == ZX_EDX)
1182  	   || (ebx == SH_EBX && ecx == SH_ECX && edx == SH_EDX))
1183      cpuid_type = zhaoxin;
1184    else if (ebx == HYGON_EBX && ecx == HYGON_ECX && edx == HYGON_EDX)
1185      cpuid_type = hygon;
1186    hwloc_debug("highest cpuid %x, cpuid type %u\n", highest_cpuid, cpuid_type);
1187    if (highest_cpuid < 0x01) {
1188        goto out_with_infos;
1189    }
1190    eax = 0x01;
1191    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1192    features[0] = edx;
1193    features[4] = ecx;
1194    eax = 0x80000000;
1195    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1196    highest_ext_cpuid = eax;
1197    hwloc_debug("highest extended cpuid %x\n", highest_ext_cpuid);
1198    if (highest_cpuid >= 0x7) {
1199      eax = 0x7;
1200      ecx = 0;
1201      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1202      features[9] = ebx;
1203      features[18] = edx;
1204    }
1205    if (cpuid_type != intel && highest_ext_cpuid >= 0x80000001) {
1206      eax = 0x80000001;
1207      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1208      features[1] = edx;
1209      features[6] = ecx;
1210    }
1211    hwloc_x86_os_state_save(&os_state, src_cpuiddump);
1212    ret = look_procs(backend, infos, flags,
1213  		   highest_cpuid, highest_ext_cpuid, features, cpuid_type,
1214  		   get_cpubind, set_cpubind, restrict_set);
1215    if (!ret)
1216      goto out_with_os_state;
1217    if (nbprocs == 1) {
1218      look_proc(backend, &infos[0], flags, highest_cpuid, highest_ext_cpuid, features, cpuid_type, src_cpuiddump);
1219      summarize(backend, infos, flags);
1220      ret = 0;
1221    }
1222  out_with_os_state:
1223    hwloc_x86_os_state_restore(&os_state, src_cpuiddump);
1224  out_with_infos:
1225    if (NULL != infos) {
1226      for (i = 0; i < nbprocs; i++) {
1227        free(infos[i].cache);
1228        free(infos[i].otherids);
1229      }
1230      free(infos);
1231    }
1232  out:
1233    hwloc_bitmap_free(restrict_set);
1234    if (src_cpuiddump)
1235      cpuiddump_free(src_cpuiddump);
1236    return ret;
1237  }
1238  static int
1239  hwloc_x86_discover(struct hwloc_backend *backend, struct hwloc_disc_status *dstatus)
1240  {
1241    struct hwloc_x86_backend_data_s *data = backend->private_data;
1242    struct hwloc_topology *topology = backend->topology;
1243    unsigned long flags = 0;
1244    int alreadypus = 0;
1245    int ret;
1246    assert(dstatus->phase == HWLOC_DISC_PHASE_CPU);
1247    if (topology->flags & HWLOC_TOPOLOGY_FLAG_DONT_CHANGE_BINDING) {
1248      return 0;
1249    }
1250    if (getenv("HWLOC_X86_TOPOEXT_NUMANODES")) {
1251      flags |= HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES;
1252    }
1253  #if HAVE_DECL_RUNNING_ON_VALGRIND
1254    if (RUNNING_ON_VALGRIND && !data->src_cpuiddump_path) {
1255      fprintf(stderr, "hwloc x86 backend cannot work under Valgrind, disabling.\n"
1256  	    "May be reenabled by dumping CPUIDs with hwloc-gather-cpuid\n"
1257  	    "and reloading them under Valgrind with HWLOC_CPUID_PATH.\n");
1258      return 0;
1259    }
1260  #endif
1261    if (data->src_cpuiddump_path) {
1262      assert(data->nbprocs > 0); &bsol;* enforced by hwloc_x86_component_instantiate() */
1263      topology->support.discovery->pu = 1;
1264    } else {
1265      int nbprocs = hwloc_fallback_nbprocessors(HWLOC_FALLBACK_NBPROCESSORS_INCLUDE_OFFLINE);
1266      if (nbprocs >= 1)
1267        topology->support.discovery->pu = 1;
1268      else
1269        nbprocs = 1;
1270      data->nbprocs = (unsigned) nbprocs;
1271    }
1272    if (topology->levels[0][0]->cpuset) {
1273      hwloc_topology_reconnect(topology, 0);
1274      if (topology->nb_levels == 2 && topology->level_nbobjects[1] == data->nbprocs) {
1275        alreadypus = 1;
1276        goto fulldiscovery;
1277      }
1278      ret = hwloc_look_x86(backend, flags);
1279      if (ret)
1280        hwloc_obj_add_info(topology->levels[0][0], "Backend", "x86");
1281      return 0;
1282    } else {
1283      hwloc_alloc_root_sets(topology->levels[0][0]);
1284    }
1285  fulldiscovery:
1286    if (hwloc_look_x86(backend, flags | HWLOC_X86_DISC_FLAG_FULL) < 0) {
1287      if (!alreadypus)
1288        hwloc_setup_pu_level(topology, data->nbprocs);
1289    }
1290    hwloc_obj_add_info(topology->levels[0][0], "Backend", "x86");
1291    if (!data->src_cpuiddump_path) { &bsol;* CPUID dump works for both x86 and x86_64 */
1292  #ifdef HAVE_UNAME
1293      hwloc_add_uname_info(topology, NULL); &bsol;* we already know is_thissystem() is true */
1294  #else
1295  #ifdef HWLOC_X86_64_ARCH
1296      hwloc_obj_add_info(topology->levels[0][0], "Architecture", "x86_64");
1297  #else
1298      hwloc_obj_add_info(topology->levels[0][0], "Architecture", "x86");
1299  #endif
1300  #endif
1301    }
1302    return 1;
1303  }
1304  static int
1305  hwloc_x86_check_cpuiddump_input(const char *src_cpuiddump_path, hwloc_bitmap_t set)
1306  {
1307  #if !(defined HWLOC_WIN_SYS && !defined __MINGW32__ && !defined __CYGWIN__) &bsol;* needs a lot of work */
1308    struct dirent *dirent;
1309    DIR *dir;
1310    char *path;
1311    FILE *file;
1312    char line [32];
1313    dir = opendir(src_cpuiddump_path);
1314    if (!dir) 
1315      return -1;
1316    path = malloc(strlen(src_cpuiddump_path) + strlen("/hwloc-cpuid-info") + 1);
1317    if (!path)
1318      goto out_with_dir;
1319    sprintf(path, "%s/hwloc-cpuid-info", src_cpuiddump_path);
1320    file = fopen(path, "r");
1321    if (!file) {
1322      fprintf(stderr, "Couldn't open dumped cpuid summary %s\n", path);
1323      goto out_with_path;
1324    }
1325    if (!fgets(line, sizeof(line), file)) {
1326      fprintf(stderr, "Found read dumped cpuid summary in %s\n", path);
1327      fclose(file);
1328      goto out_with_path;
1329    }
1330    fclose(file);
1331    if (strcmp(line, "Architecture: x86\n")) {
1332      fprintf(stderr, "Found non-x86 dumped cpuid summary in %s: %s\n", path, line);
1333      goto out_with_path;
1334    }
1335    free(path);
1336    while ((dirent = readdir(dir)) != NULL) {
1337      if (!strncmp(dirent->d_name, "pu", 2)) {
1338        char *end;
1339        unsigned long idx = strtoul(dirent->d_name+2, &end, 10);
1340        if (!*end)
1341  	hwloc_bitmap_set(set, idx);
1342        else
1343  	fprintf(stderr, "Ignoring invalid dirent `%s' in dumped cpuid directory `%s'\n",
1344  		dirent->d_name, src_cpuiddump_path);
1345      }
1346    }
1347    closedir(dir);
1348    if (hwloc_bitmap_iszero(set)) {
1349      fprintf(stderr, "Did not find any valid pu%%u entry in dumped cpuid directory `%s'\n",
1350  	    src_cpuiddump_path);
1351      return -1;
1352    } else if (hwloc_bitmap_last(set) != hwloc_bitmap_weight(set) - 1) {
1353      fprintf(stderr, "Found non-contigous pu%%u range in dumped cpuid directory `%s'\n",
1354  	    src_cpuiddump_path);
1355      return -1;
1356    }
1357    return 0;
1358   out_with_path:
1359    free(path);
1360   out_with_dir:
1361    closedir(dir);
1362  #endif &bsol;* HWLOC_WIN_SYS & !__MINGW32__ needs a lot of work */
1363    return -1;
1364  }
1365  static void
1366  hwloc_x86_backend_disable(struct hwloc_backend *backend)
1367  {
1368    struct hwloc_x86_backend_data_s *data = backend->private_data;
1369    hwloc_bitmap_free(data->apicid_set);
1370    free(data->src_cpuiddump_path);
1371    free(data);
1372  }
1373  static struct hwloc_backend *
1374  hwloc_x86_component_instantiate(struct hwloc_topology *topology,
1375  				struct hwloc_disc_component *component,
1376  				unsigned excluded_phases __hwloc_attribute_unused,
1377  				const void *_data1 __hwloc_attribute_unused,
1378  				const void *_data2 __hwloc_attribute_unused,
1379  				const void *_data3 __hwloc_attribute_unused)
1380  {
1381    struct hwloc_backend *backend;
1382    struct hwloc_x86_backend_data_s *data;
1383    const char *src_cpuiddump_path;
1384    backend = hwloc_backend_alloc(topology, component);
1385    if (!backend)
1386      goto out;
1387    data = malloc(sizeof(*data));
1388    if (!data) {
1389      errno = ENOMEM;
1390      goto out_with_backend;
1391    }
1392    backend->private_data = data;
1393    backend->discover = hwloc_x86_discover;
1394    backend->disable = hwloc_x86_backend_disable;
1395    data->is_knl = 0;
1396    data->apicid_set = hwloc_bitmap_alloc();
1397    data->apicid_unique = 1;
1398    data->src_cpuiddump_path = NULL;
1399    src_cpuiddump_path = getenv("HWLOC_CPUID_PATH");
1400    if (src_cpuiddump_path) {
1401      hwloc_bitmap_t set = hwloc_bitmap_alloc();
1402      if (!hwloc_x86_check_cpuiddump_input(src_cpuiddump_path, set)) {
1403        backend->is_thissystem = 0;
1404        data->src_cpuiddump_path = strdup(src_cpuiddump_path);
1405        assert(!hwloc_bitmap_iszero(set)); &bsol;* enforced by hwloc_x86_check_cpuiddump_input() */
1406        data->nbprocs = hwloc_bitmap_weight(set);
1407      } else {
1408        fprintf(stderr, "Ignoring dumped cpuid directory.\n");
1409      }
1410      hwloc_bitmap_free(set);
1411    }
1412    return backend;
1413   out_with_backend:
1414    free(backend);
1415   out:
1416    return NULL;
1417  }
1418  static struct hwloc_disc_component hwloc_x86_disc_component = {
1419    "x86",
1420    HWLOC_DISC_PHASE_CPU,
1421    HWLOC_DISC_PHASE_GLOBAL,
1422    hwloc_x86_component_instantiate,
1423    45, &bsol;* between native and no_os */
1424    1,
1425    NULL
1426  };
1427  const struct hwloc_component hwloc_x86_component = {
1428    HWLOC_COMPONENT_ABI,
1429    NULL, NULL,
1430    HWLOC_COMPONENT_TYPE_DISC,
1431    0,
1432    &hwloc_x86_disc_component
1433  };
</code></pre>
        </div>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-topology-x86.c</h3>
            <pre><code>1  #include "private/autogen/config.h"
2  #include "hwloc.h"
3  #include "private/private.h"
4  #include "private/debug.h"
5  #include "private/misc.h"
6  #include "private/cpuid-x86.h"
7  #include <sys/types.h>
8  #ifdef HAVE_DIRENT_H
9  #include <dirent.h>
10  #endif
11  #ifdef HAVE_VALGRIND_VALGRIND_H
12  #include <valgrind/valgrind.h>
13  #endif
14  struct hwloc_x86_backend_data_s {
15    unsigned nbprocs;
16    hwloc_bitmap_t apicid_set;
17    int apicid_unique;
18    char *src_cpuiddump_path;
19    int is_knl;
20  };
21  struct cpuiddump {
22    unsigned nr;
23    struct cpuiddump_entry {
24      unsigned inmask; &bsol;* which of ine[abcd]x are set on input */
25      unsigned ineax;
26      unsigned inebx;
27      unsigned inecx;
28      unsigned inedx;
29      unsigned outeax;
30      unsigned outebx;
31      unsigned outecx;
32      unsigned outedx;
33    } *entries;
34  };
35  static void
36  cpuiddump_free(struct cpuiddump *cpuiddump)
37  {
38    if (cpuiddump->nr)
39      free(cpuiddump->entries);
40    free(cpuiddump);
41  }
42  static struct cpuiddump *
43  cpuiddump_read(const char *dirpath, unsigned idx)
44  {
45    struct cpuiddump *cpuiddump;
46    struct cpuiddump_entry *cur;
47    size_t filenamelen;
48    char *filename;
49    FILE *file;
50    char line[128];
51    unsigned nr;
52    cpuiddump = malloc(sizeof(*cpuiddump));
53    if (!cpuiddump) {
54      fprintf(stderr, "Failed to allocate cpuiddump for PU #%u, ignoring cpuiddump.\n", idx);
55      goto out;
56    }
57    filenamelen = strlen(dirpath) + 15;
58    filename = malloc(filenamelen);
59    if (!filename)
60      goto out_with_dump;
61    snprintf(filename, filenamelen, "%s/pu%u", dirpath, idx);
62    file = fopen(filename, "r");
63    if (!file) {
64      fprintf(stderr, "Could not read dumped cpuid file %s, ignoring cpuiddump.\n", filename);
65      goto out_with_filename;
66    }
67    nr = 0;
68    while (fgets(line, sizeof(line), file))
69      nr++;
70    cpuiddump->entries = malloc(nr * sizeof(struct cpuiddump_entry));
71    if (!cpuiddump->entries) {
72      fprintf(stderr, "Failed to allocate %u cpuiddump entries for PU #%u, ignoring cpuiddump.\n", nr, idx);
73      goto out_with_file;
74    }
75    fseek(file, 0, SEEK_SET);
76    cur = &cpuiddump->entries[0];
77    nr = 0;
78    while (fgets(line, sizeof(line), file)) {
79      if (*line == '#')
80        continue;
81      if (sscanf(line, "%x %x %x %x %x => %x %x %x %x",
82  	      &cur->inmask,
83  	      &cur->ineax, &cur->inebx, &cur->inecx, &cur->inedx,
84  	      &cur->outeax, &cur->outebx, &cur->outecx, &cur->outedx) == 9) {
85        cur++;
86        nr++;
87      }
88    }
89    cpuiddump->nr = nr;
90    fclose(file);
91    free(filename);
92    return cpuiddump;
93   out_with_file:
94    fclose(file);
95   out_with_filename:
96    free(filename);
97   out_with_dump:
98    free(cpuiddump);
99   out:
100    return NULL;
101  }
102  static void
103  cpuiddump_find_by_input(unsigned *eax, unsigned *ebx, unsigned *ecx, unsigned *edx, struct cpuiddump *cpuiddump)
104  {
105    unsigned i;
106    for(i=0; i<cpuiddump->nr; i++) {
107      struct cpuiddump_entry *entry = &cpuiddump->entries[i];
108      if ((entry->inmask & 0x1) && *eax != entry->ineax)
109        continue;
110      if ((entry->inmask & 0x2) && *ebx != entry->inebx)
111        continue;
112      if ((entry->inmask & 0x4) && *ecx != entry->inecx)
113        continue;
114      if ((entry->inmask & 0x8) && *edx != entry->inedx)
115        continue;
116      *eax = entry->outeax;
117      *ebx = entry->outebx;
118      *ecx = entry->outecx;
119      *edx = entry->outedx;
120      return;
121    }
122    fprintf(stderr, "Couldn't find %x,%x,%x,%x in dumped cpuid, returning 0s.\n",
123  	  *eax, *ebx, *ecx, *edx);
124    *eax = 0;
125    *ebx = 0;
126    *ecx = 0;
127    *edx = 0;
128  }
129  static void cpuid_or_from_dump(unsigned *eax, unsigned *ebx, unsigned *ecx, unsigned *edx, struct cpuiddump *src_cpuiddump)
130  {
131    if (src_cpuiddump) {
132      cpuiddump_find_by_input(eax, ebx, ecx, edx, src_cpuiddump);
133    } else {
134      hwloc_x86_cpuid(eax, ebx, ecx, edx);
135    }
136  }
137  enum hwloc_x86_disc_flags {
138    HWLOC_X86_DISC_FLAG_FULL = (1<<0), &bsol;* discover everything instead of only annotating */
139    HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES = (1<<1) &bsol;* use AMD topoext numanode information */
140  };
141  #define has_topoext(features) ((features)[6] & (1 << 22))
142  #define has_x2apic(features) ((features)[4] & (1 << 21))
143  #define has_hybrid(features) ((features)[18] & (1 << 15))
144  struct cacheinfo {
145    hwloc_obj_cache_type_t type;
146    unsigned level;
147    unsigned nbthreads_sharing;
148    unsigned cacheid;
149    unsigned linesize;
150    unsigned linepart;
151    int inclusive;
152    int ways;
153    unsigned sets;
154    unsigned long size;
155  };
156  struct procinfo {
157    unsigned present;
158    unsigned apicid;
159  #define PKG 0
160  #define CORE 1
161  #define NODE 2
162  #define UNIT 3
163  #define TILE 4
164  #define MODULE 5
165  #define DIE 6
166  #define HWLOC_X86_PROCINFO_ID_NR 7
167    unsigned ids[HWLOC_X86_PROCINFO_ID_NR];
168    unsigned *otherids;
169    unsigned levels;
170    unsigned numcaches;
171    struct cacheinfo *cache;
172    char cpuvendor[13];
173    char cpumodel[3*4*4+1];
174    unsigned cpustepping;
175    unsigned cpumodelnumber;
176    unsigned cpufamilynumber;
177    unsigned hybridcoretype;
178    unsigned hybridnativemodel;
179  };
180  enum cpuid_type {
181    intel,
182    amd,
183    zhaoxin,
184    hygon,
185    unknown
186  };
187  static void setup__amd_cache_legacy(struct procinfo *infos, unsigned level, hwloc_obj_cache_type_t type, unsigned nbthreads_sharing, unsigned cpuid)
188  {
189    struct cacheinfo *cache, *tmpcaches;
190    unsigned cachenum;
191    unsigned long size = 0;
192    if (level == 1)
193      size = ((cpuid >> 24)) << 10;
194    else if (level == 2)
195      size = ((cpuid >> 16)) << 10;
196    else if (level == 3)
197      size = ((cpuid >> 18)) << 19;
198    if (!size)
199      return;
200    tmpcaches = realloc(infos->cache, (infos->numcaches+1)*sizeof(*infos->cache));
201    if (!tmpcaches)
202      return;
203    infos->cache = tmpcaches;
204    cachenum = infos->numcaches++;
205    cache = &infos->cache[cachenum];
206    cache->type = type;
207    cache->level = level;
208    cache->nbthreads_sharing = nbthreads_sharing;
209    cache->linesize = cpuid & 0xff;
210    cache->linepart = 0;
211    cache->inclusive = 0; &bsol;* old AMD (K8-K10) supposed to have exclusive caches */
212    if (level == 1) {
213      cache->ways = (cpuid >> 16) & 0xff;
214      if (cache->ways == 0xff)
215        cache->ways = -1;
216    } else {
217      static const unsigned ways_tab[] = { 0, 1, 2, 0, 4, 0, 8, 0, 16, 0, 32, 48, 64, 96, 128, -1 };
218      unsigned ways = (cpuid >> 12) & 0xf;
219      cache->ways = ways_tab[ways];
220    }
221    cache->size = size;
222    cache->sets = 0;
223    hwloc_debug("cache L%u t%u linesize %u ways %d size %luKB\n", cache->level, cache->nbthreads_sharing, cache->linesize, cache->ways, cache->size >> 10);
224  }
225  static void read_amd_caches_legacy(struct procinfo *infos, struct cpuiddump *src_cpuiddump, unsigned legacy_max_log_proc)
226  {
227    unsigned eax, ebx, ecx, edx;
228    eax = 0x80000005;
229    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
230    setup__amd_cache_legacy(infos, 1, HWLOC_OBJ_CACHE_DATA, 1, ecx); &bsol;* private L1d */
231    setup__amd_cache_legacy(infos, 1, HWLOC_OBJ_CACHE_INSTRUCTION, 1, edx); &bsol;* private L1i */
232    eax = 0x80000006;
233    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
234    if (ecx & 0xf000)
235      setup__amd_cache_legacy(infos, 2, HWLOC_OBJ_CACHE_UNIFIED, 1, ecx); &bsol;* private L2u */
236    if (edx & 0xf000)
237      setup__amd_cache_legacy(infos, 3, HWLOC_OBJ_CACHE_UNIFIED, legacy_max_log_proc, edx); &bsol;* package-wide L3u */
238  }
239  static void read_amd_caches_topoext(struct procinfo *infos, struct cpuiddump *src_cpuiddump)
240  {
241    unsigned eax, ebx, ecx, edx;
242    unsigned cachenum;
243    struct cacheinfo *cache;
244    assert(!infos->numcaches);
245    for (cachenum = 0; ; cachenum++) {
246      eax = 0x8000001d;
247      ecx = cachenum;
248      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
249      if ((eax & 0x1f) == 0)
250        break;
251      infos->numcaches++;
252    }
253    cache = infos->cache = malloc(infos->numcaches * sizeof(*infos->cache));
254    if (cache) {
255      for (cachenum = 0; ; cachenum++) {
256        unsigned long linesize, linepart, ways, sets;
257        eax = 0x8000001d;
258        ecx = cachenum;
259        cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
260        if ((eax & 0x1f) == 0)
261  	break;
262        switch (eax & 0x1f) {
263        case 1: cache->type = HWLOC_OBJ_CACHE_DATA; break;
264        case 2: cache->type = HWLOC_OBJ_CACHE_INSTRUCTION; break;
265        default: cache->type = HWLOC_OBJ_CACHE_UNIFIED; break;
266        }
267        cache->level = (eax >> 5) & 0x7;
268        cache->nbthreads_sharing = ((eax >> 14) &  0xfff) + 1;
269        cache->linesize = linesize = (ebx & 0xfff) + 1;
270        cache->linepart = linepart = ((ebx >> 12) & 0x3ff) + 1;
271        ways = ((ebx >> 22) & 0x3ff) + 1;
272        if (eax & (1 << 9))
273  	cache->ways = -1;
274        else
275  	cache->ways = ways;
276        cache->sets = sets = ecx + 1;
277        cache->size = linesize * linepart * ways * sets;
278        cache->inclusive = edx & 0x2;
279        hwloc_debug("cache %u L%u%c t%u linesize %lu linepart %lu ways %lu sets %lu, size %luKB\n",
280  		  cachenum, cache->level,
281  		  cache->type == HWLOC_OBJ_CACHE_DATA ? 'd' : cache->type == HWLOC_OBJ_CACHE_INSTRUCTION ? 'i' : 'u',
<span onclick='openModal()' class='match'>282  		  cache->nbthreads_sharing, linesize, linepart, ways, sets, cache->size >> 10);
283        cache++;
284      }
285    } else {
</span>286      infos->numcaches = 0;
287    }
288  }
289  static void read_intel_caches(struct hwloc_x86_backend_data_s *data, struct procinfo *infos, struct cpuiddump *src_cpuiddump)
290  {
291    unsigned level;
292    struct cacheinfo *tmpcaches;
293    unsigned eax, ebx, ecx, edx;
294    unsigned oldnumcaches = infos->numcaches; &bsol;* in case we got caches above */
295    unsigned cachenum;
296    struct cacheinfo *cache;
297    for (cachenum = 0; ; cachenum++) {
298      eax = 0x04;
299      ecx = cachenum;
300      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
301      hwloc_debug("cache %u type %u\n", cachenum, eax & 0x1f);
302      if ((eax & 0x1f) == 0)
303        break;
304      level = (eax >> 5) & 0x7;
305      if (data->is_knl && level == 3)
306        break;
307      infos->numcaches++;
308    }
309    tmpcaches = realloc(infos->cache, infos->numcaches * sizeof(*infos->cache));
310    if (!tmpcaches) {
311      infos->numcaches = oldnumcaches;
312    } else {
313      infos->cache = tmpcaches;
314      cache = &infos->cache[oldnumcaches];
315      for (cachenum = 0; ; cachenum++) {
316        unsigned long linesize, linepart, ways, sets;
317        eax = 0x04;
318        ecx = cachenum;
319        cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
320        if ((eax & 0x1f) == 0)
321  	break;
322        level = (eax >> 5) & 0x7;
323        if (data->is_knl && level == 3)
324  	break;
325        switch (eax & 0x1f) {
326        case 1: cache->type = HWLOC_OBJ_CACHE_DATA; break;
327        case 2: cache->type = HWLOC_OBJ_CACHE_INSTRUCTION; break;
328        default: cache->type = HWLOC_OBJ_CACHE_UNIFIED; break;
329        }
330        cache->level = level;
331        cache->nbthreads_sharing = ((eax >> 14) & 0xfff) + 1;
332        cache->linesize = linesize = (ebx & 0xfff) + 1;
333        cache->linepart = linepart = ((ebx >> 12) & 0x3ff) + 1;
334        ways = ((ebx >> 22) & 0x3ff) + 1;
335        if (eax & (1 << 9))
336          cache->ways = -1;
337        else
338          cache->ways = ways;
339        cache->sets = sets = ecx + 1;
340        cache->size = linesize * linepart * ways * sets;
341        cache->inclusive = edx & 0x2;
342        hwloc_debug("cache %u L%u%c t%u linesize %lu linepart %lu ways %lu sets %lu, size %luKB\n",
343  		  cachenum, cache->level,
344  		  cache->type == HWLOC_OBJ_CACHE_DATA ? 'd' : cache->type == HWLOC_OBJ_CACHE_INSTRUCTION ? 'i' : 'u',
345  		  cache->nbthreads_sharing, linesize, linepart, ways, sets, cache->size >> 10);
346        cache++;
347      }
348    }
349  }
350  static void read_amd_cores_legacy(struct procinfo *infos, struct cpuiddump *src_cpuiddump)
351  {
352    unsigned eax, ebx, ecx, edx;
353    unsigned max_nbcores;
354    unsigned max_nbthreads;
355    unsigned coreidsize;
356    unsigned logprocid;
357    unsigned threadid __hwloc_attribute_unused;
358    eax = 0x80000008;
359    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
360    coreidsize = (ecx >> 12) & 0xf;
361    hwloc_debug("core ID size: %u\n", coreidsize);
362    if (!coreidsize) {
363      max_nbcores = (ecx & 0xff) + 1;
364    } else
365      max_nbcores = 1 << coreidsize;
366    hwloc_debug("Thus max # of cores: %u\n", max_nbcores);
367    max_nbthreads = 1 ;
368    hwloc_debug("and max # of threads: %u\n", max_nbthreads);
369    infos->ids[PKG] = infos->apicid / max_nbcores;
370    logprocid = infos->apicid % max_nbcores;
371    infos->ids[CORE] = logprocid / max_nbthreads;
372    threadid = logprocid % max_nbthreads;
373    hwloc_debug("this is thread %u of core %u\n", threadid, infos->ids[CORE]);
374  }
375  static void read_amd_cores_topoext(struct procinfo *infos, unsigned long flags, struct cpuiddump *src_cpuiddump)
376  {
377    unsigned apic_id, nodes_per_proc = 0;
378    unsigned eax, ebx, ecx, edx;
379    eax = 0x8000001e;
380    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
381    infos->apicid = apic_id = eax;
382    if (flags & HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES) {
383      if (infos->cpufamilynumber == 0x16) {
384        infos->ids[NODE] = 0;
385        nodes_per_proc = 1;
386      } else {
387        infos->ids[NODE] = ecx & 0xff;
388        nodes_per_proc = ((ecx >> 8) & 7) + 1;
389      }
390      if ((infos->cpufamilynumber == 0x15 && nodes_per_proc > 2)
391  	|| ((infos->cpufamilynumber == 0x17 || infos->cpufamilynumber == 0x18) && nodes_per_proc > 4)
392          || (infos->cpufamilynumber == 0x19 && nodes_per_proc > 1)) {
393        hwloc_debug("warning: undefined nodes_per_proc value %u, assuming it means %u\n", nodes_per_proc, nodes_per_proc);
394      }
395    }
396    if (infos->cpufamilynumber <= 0x16) { &bsol;* topoext appeared in 0x15 and compute-units were only used in 0x15 and 0x16 */
397      unsigned cores_per_unit;
398      infos->ids[UNIT] = ebx & 0xff;
399      cores_per_unit = ((ebx >> 8) & 0xff) + 1;
400      hwloc_debug("topoext %08x, %u nodes, node %u, %u cores in unit %u\n", apic_id, nodes_per_proc, infos->ids[NODE], cores_per_unit, infos->ids[UNIT]);
401    } else {
402      unsigned threads_per_core;
403      infos->ids[CORE] = ebx & 0xff;
404      threads_per_core = ((ebx >> 8) & 0xff) + 1;
405      hwloc_debug("topoext %08x, %u nodes, node %u, %u threads in core %u\n", apic_id, nodes_per_proc, infos->ids[NODE], threads_per_core, infos->ids[CORE]);
406    }
407  }
408  static void read_intel_cores_exttopoenum(struct procinfo *infos, unsigned leaf, struct cpuiddump *src_cpuiddump)
409  {
410    unsigned level, apic_nextshift, apic_number, apic_type, apic_id = 0, apic_shift = 0, id;
411    unsigned threadid __hwloc_attribute_unused = 0; &bsol;* shut-up compiler */
412    unsigned eax, ebx, ecx = 0, edx;
413    int apic_packageshift = 0;
414    for (level = 0; ; level++) {
415      ecx = level;
416      eax = leaf;
417      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
418      if (!eax && !ebx)
419        break;
420      apic_packageshift = eax & 0x1f;
421    }
422    if (level) {
423      infos->otherids = malloc(level * sizeof(*infos->otherids));
424      if (infos->otherids) {
425        infos->levels = level;
426        for (level = 0; ; level++) {
427  	ecx = level;
428  	eax = leaf;
429  	cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
430  	if (!eax && !ebx)
431  	  break;
432  	apic_nextshift = eax & 0x1f;
433  	apic_number = ebx & 0xffff;
434  	apic_type = (ecx & 0xff00) >> 8;
435  	apic_id = edx;
436  	id = (apic_id >> apic_shift) & ((1 << (apic_packageshift - apic_shift)) - 1);
437  	hwloc_debug("x2APIC %08x %u: nextshift %u num %2u type %u id %2u\n", apic_id, level, apic_nextshift, apic_number, apic_type, id);
438  	infos->apicid = apic_id;
439  	infos->otherids[level] = UINT_MAX;
440  	switch (apic_type) {
441  	case 1:
442  	  threadid = id;
443  	  break;
444  	case 2:
445  	  infos->ids[CORE] = id;
446  	  break;
447  	case 3:
448  	  infos->ids[MODULE] = id;
449  	  break;
450  	case 4:
451  	  infos->ids[TILE] = id;
452  	  break;
453  	case 5:
454  	  infos->ids[DIE] = id;
455  	  break;
456  	default:
457  	  hwloc_debug("x2APIC %u: unknown type %u\n", level, apic_type);
458  	  infos->otherids[level] = apic_id >> apic_shift;
459  	  break;
460  	}
461  	apic_shift = apic_nextshift;
462        }
463        infos->apicid = apic_id;
464        infos->ids[PKG] = apic_id >> apic_shift;
465        hwloc_debug("x2APIC remainder: %u\n", infos->ids[PKG]);
466        hwloc_debug("this is thread %u of core %u\n", threadid, infos->ids[CORE]);
467      }
468    }
469  }
470  static void look_proc(struct hwloc_backend *backend, struct procinfo *infos, unsigned long flags, unsigned highest_cpuid, unsigned highest_ext_cpuid, unsigned *features, enum cpuid_type cpuid_type, struct cpuiddump *src_cpuiddump)
471  {
472    struct hwloc_x86_backend_data_s *data = backend->private_data;
473    unsigned eax, ebx, ecx = 0, edx;
474    unsigned cachenum;
475    struct cacheinfo *cache;
476    unsigned regs[4];
477    unsigned legacy_max_log_proc; &bsol;* not valid on Intel processors with > 256 threads, or when cpuid 0x80000008 is supported */
478    unsigned legacy_log_proc_id;
479    unsigned _model, _extendedmodel, _family, _extendedfamily;
480    infos->present = 1;
481    eax = 0x01;
482    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
483    infos->apicid = ebx >> 24;
484    if (edx & (1 << 28)) {
485      legacy_max_log_proc = 1 << hwloc_flsl(((ebx >> 16) & 0xff) - 1);
486    } else {
487      hwloc_debug("HTT bit not set in CPUID 0x01.edx, assuming legacy_max_log_proc = 1\n");
488      legacy_max_log_proc = 1;
489    }
490    hwloc_debug("APIC ID 0x%02x legacy_max_log_proc %u\n", infos->apicid, legacy_max_log_proc);
491    infos->ids[PKG] = infos->apicid / legacy_max_log_proc;
492    legacy_log_proc_id = infos->apicid % legacy_max_log_proc;
493    hwloc_debug("phys %u legacy thread %u\n", infos->ids[PKG], legacy_log_proc_id);
494    _model          = (eax>>4) & 0xf;
495    _extendedmodel  = (eax>>16) & 0xf;
496    _family         = (eax>>8) & 0xf;
497    _extendedfamily = (eax>>20) & 0xff;
498    if ((cpuid_type == intel || cpuid_type == amd || cpuid_type == hygon) && _family == 0xf) {
499      infos->cpufamilynumber = _family + _extendedfamily;
500    } else {
501      infos->cpufamilynumber = _family;
502    }
503    if ((cpuid_type == intel && (_family == 0x6 || _family == 0xf))
504        || ((cpuid_type == amd || cpuid_type == hygon) && _family == 0xf)
505        || (cpuid_type == zhaoxin && (_family == 0x6 || _family == 0x7))) {
506      infos->cpumodelnumber = _model + (_extendedmodel << 4);
507    } else {
508      infos->cpumodelnumber = _model;
509    }
510    infos->cpustepping = eax & 0xf;
511    if (cpuid_type == intel && infos->cpufamilynumber == 0x6 &&
512        (infos->cpumodelnumber == 0x57 || infos->cpumodelnumber == 0x85))
513      data->is_knl = 1; &bsol;* KNM is the same as KNL */
514    memset(regs, 0, sizeof(regs));
515    regs[0] = 0;
516    cpuid_or_from_dump(&regs[0], &regs[1], &regs[3], &regs[2], src_cpuiddump);
517    memcpy(infos->cpuvendor, regs+1, 4*3);
518    if (highest_ext_cpuid >= 0x80000004) {
519      memset(regs, 0, sizeof(regs));
520      regs[0] = 0x80000002;
521      cpuid_or_from_dump(&regs[0], &regs[1], &regs[2], &regs[3], src_cpuiddump);
522      memcpy(infos->cpumodel, regs, 4*4);
523      regs[0] = 0x80000003;
524      cpuid_or_from_dump(&regs[0], &regs[1], &regs[2], &regs[3], src_cpuiddump);
525      memcpy(infos->cpumodel + 4*4, regs, 4*4);
526      regs[0] = 0x80000004;
527      cpuid_or_from_dump(&regs[0], &regs[1], &regs[2], &regs[3], src_cpuiddump);
528      memcpy(infos->cpumodel + 4*4*2, regs, 4*4);
529    }
530    if ((cpuid_type != amd && cpuid_type != hygon) && highest_cpuid >= 0x04) {
531      eax = 0x04;
532      ecx = 0;
533      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
534      if ((eax & 0x1f) != 0) {
535        unsigned max_nbcores;
536        unsigned max_nbthreads;
537        unsigned threadid __hwloc_attribute_unused;
538        hwloc_debug("Trying to get core/thread IDs from 0x04...\n");
539        max_nbcores = ((eax >> 26) & 0x3f) + 1;
540        hwloc_debug("found %u cores max\n", max_nbcores);
541        if (!max_nbcores) {
542          hwloc_debug("cannot detect core/thread IDs from 0x04 without a valid max of cores\n");
543        } else {
544          max_nbthreads = legacy_max_log_proc / max_nbcores;
545          hwloc_debug("found %u threads max\n", max_nbthreads);
546          if (!max_nbthreads) {
547            hwloc_debug("cannot detect core/thread IDs from 0x04 without a valid max of threads\n");
548          } else {
549            threadid = legacy_log_proc_id % max_nbthreads;
550            infos->ids[CORE] = legacy_log_proc_id / max_nbthreads;
551            hwloc_debug("this is thread %u of core %u\n", threadid, infos->ids[CORE]);
552          }
553        }
554      }
555    }
556    if (highest_cpuid >= 0x1a && has_hybrid(features)) {
557      eax = 0x1a;
558      ecx = 0;
559      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
560      infos->hybridcoretype = eax >> 24;
561      infos->hybridnativemodel = eax & 0xffffff;
562    }
563    if (cpuid_type != intel && cpuid_type != zhaoxin && highest_ext_cpuid >= 0x80000008 && !has_x2apic(features)) {
564      read_amd_cores_legacy(infos, src_cpuiddump);
565    }
566    if (cpuid_type != intel && cpuid_type != zhaoxin && has_topoext(features)) {
567      read_amd_cores_topoext(infos, flags, src_cpuiddump);
568    }
569    if ((cpuid_type == intel) && highest_cpuid >= 0x1f) {
570      read_intel_cores_exttopoenum(infos, 0x1f, src_cpuiddump);
571    } else if ((cpuid_type == intel || cpuid_type == amd || cpuid_type == zhaoxin)
572  	     && highest_cpuid >= 0x0b && has_x2apic(features)) {
573      read_intel_cores_exttopoenum(infos, 0x0b, src_cpuiddump);
574    }
575    infos->numcaches = 0;
576    infos->cache = NULL;
577    if (cpuid_type != intel && cpuid_type != zhaoxin && has_topoext(features)) {
578      read_amd_caches_topoext(infos, src_cpuiddump);
579    } else if (cpuid_type != intel && cpuid_type != zhaoxin && highest_ext_cpuid >= 0x80000006) {
580      read_amd_caches_legacy(infos, src_cpuiddump, legacy_max_log_proc);
581    }
582    if ((cpuid_type != amd && cpuid_type != hygon) && highest_cpuid >= 0x04) {
583      read_intel_caches(data, infos, src_cpuiddump);
584    }
585    for (cachenum = 0; cachenum < infos->numcaches; cachenum++) {
586      cache = &infos->cache[cachenum];
587      cache->cacheid = infos->apicid / cache->nbthreads_sharing;
588      if (cpuid_type == intel) {
589        unsigned bits = hwloc_flsl(cache->nbthreads_sharing-1);
590        unsigned mask = ~((1U<<bits) - 1);
591        cache->cacheid = infos->apicid & mask;
592      } else if (cpuid_type == amd) {
593        if (infos->cpufamilynumber >= 0x17 && cache->level == 3) {
594          unsigned nbapics_sharing = cache->nbthreads_sharing;
595          if (nbapics_sharing & (nbapics_sharing-1))
596            nbapics_sharing = 1U<<(1+hwloc_ffsl(nbapics_sharing));
597  	cache->cacheid = infos->apicid / nbapics_sharing;
598        } else if (infos->cpufamilynumber== 0x10 && infos->cpumodelnumber == 0x9
599  	  && cache->level == 3
600  	  && (cache->ways == -1 || (cache->ways % 2 == 0)) && cache->nbthreads_sharing >= 8) {
601  	if (cache->nbthreads_sharing == 16)
602  	  cache->nbthreads_sharing = 12; &bsol;* nbthreads_sharing is a power of 2 but the processor actually has 8 or 12 cores */
603  	cache->nbthreads_sharing /= 2;
604  	cache->size /= 2;
605  	if (cache->ways != -1)
606  	  cache->ways /= 2;
607  	cache->cacheid = (infos->apicid % legacy_max_log_proc) / cache->nbthreads_sharing &bsol;* cacheid within the package */
608  	  + 2 * (infos->apicid / legacy_max_log_proc); &bsol;* add 2 caches per previous package */
609        } else if (infos->cpufamilynumber == 0x15
610  		 && (infos->cpumodelnumber == 0x1 &bsol;* Bulldozer */ || infos->cpumodelnumber == 0x2 &bsol;* Piledriver */)
611  		 && cache->level == 3 && cache->nbthreads_sharing == 6) {
612  	cache->cacheid = (infos->apicid % legacy_max_log_proc) / cache->nbthreads_sharing &bsol;* cacheid within the package */
613  	  + 2 * (infos->apicid / legacy_max_log_proc); &bsol;* add 2 cache per previous package */
614        }
615      } else if (cpuid_type == hygon) {
616        if (infos->cpufamilynumber == 0x18
617  	  && cache->level == 3 && cache->nbthreads_sharing == 6) {
618          cache->cacheid = infos->apicid / 8;
619        }
620      }
621    }
622    if (hwloc_bitmap_isset(data->apicid_set, infos->apicid))
623      data->apicid_unique = 0;
624    else
625      hwloc_bitmap_set(data->apicid_set, infos->apicid);
626  }
627  static void
628  hwloc_x86_add_cpuinfos(hwloc_obj_t obj, struct procinfo *info, int replace)
629  {
630    char number[12];
631    if (info->cpuvendor[0])
632      hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUVendor", info->cpuvendor, replace);
633    snprintf(number, sizeof(number), "%u", info->cpufamilynumber);
634    hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUFamilyNumber", number, replace);
635    snprintf(number, sizeof(number), "%u", info->cpumodelnumber);
636    hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUModelNumber", number, replace);
637    if (info->cpumodel[0]) {
638      const char *c = info->cpumodel;
639      while (*c == ' ')
640        c++;
641      hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUModel", c, replace);
642    }
643    snprintf(number, sizeof(number), "%u", info->cpustepping);
644    hwloc__add_info_nodup(&obj->infos, &obj->infos_count, "CPUStepping", number, replace);
645  }
646  static void
647  hwloc_x86_add_groups(hwloc_topology_t topology,
648  		     struct procinfo *infos,
649  		     unsigned nbprocs,
650  		     hwloc_bitmap_t remaining_cpuset,
651  		     unsigned type,
652  		     const char *subtype,
653  		     unsigned kind,
654  		     int dont_merge)
655  {
656    hwloc_bitmap_t obj_cpuset;
657    hwloc_obj_t obj;
658    unsigned i, j;
659    while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
660      unsigned packageid = infos[i].ids[PKG];
661      unsigned id = infos[i].ids[type];
662      if (id == (unsigned)-1) {
663        hwloc_bitmap_clr(remaining_cpuset, i);
664        continue;
665      }
666      obj_cpuset = hwloc_bitmap_alloc();
667      for (j = i; j < nbprocs; j++) {
668        if (infos[j].ids[type] == (unsigned) -1) {
669  	hwloc_bitmap_clr(remaining_cpuset, j);
670  	continue;
671        }
672        if (infos[j].ids[PKG] == packageid && infos[j].ids[type] == id) {
673  	hwloc_bitmap_set(obj_cpuset, j);
674  	hwloc_bitmap_clr(remaining_cpuset, j);
675        }
676      }
677      obj = hwloc_alloc_setup_object(topology, HWLOC_OBJ_GROUP, id);
678      obj->cpuset = obj_cpuset;
679      obj->subtype = strdup(subtype);
680      obj->attr->group.kind = kind;
681      obj->attr->group.dont_merge = dont_merge;
682      hwloc_debug_2args_bitmap("os %s %u has cpuset %s\n",
683  			     subtype, id, obj_cpuset);
684      hwloc__insert_object_by_cpuset(topology, NULL, obj, "x86:group");
685    }
686  }
687  static void summarize(struct hwloc_backend *backend, struct procinfo *infos, unsigned long flags)
688  {
689    struct hwloc_topology *topology = backend->topology;
690    struct hwloc_x86_backend_data_s *data = backend->private_data;
691    unsigned nbprocs = data->nbprocs;
692    hwloc_bitmap_t complete_cpuset = hwloc_bitmap_alloc();
693    unsigned i, j, l, level;
694    int one = -1;
695    hwloc_bitmap_t remaining_cpuset;
696    int gotnuma = 0;
697    int fulldiscovery = (flags & HWLOC_X86_DISC_FLAG_FULL);
698  #ifdef HWLOC_DEBUG
699    hwloc_debug("\nSummary of x86 CPUID topology:\n");
700    for(i=0; i<nbprocs; i++) {
701      hwloc_debug("PU %u present=%u apicid=%u on PKG %d CORE %d DIE %d NODE %d\n",
702                  i, infos[i].present, infos[i].apicid,
703                  infos[i].ids[PKG], infos[i].ids[CORE], infos[i].ids[DIE], infos[i].ids[NODE]);
704    }
705    hwloc_debug("\n");
706  #endif
707    for (i = 0; i < nbprocs; i++)
708      if (infos[i].present) {
709        hwloc_bitmap_set(complete_cpuset, i);
710        one = i;
711      }
712    if (one == -1) {
713      hwloc_bitmap_free(complete_cpuset);
714      return;
715    }
716    remaining_cpuset = hwloc_bitmap_alloc();
717    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_PACKAGE)) {
718      hwloc_obj_t package;
719      hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
720      while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
721        if (fulldiscovery) {
722  	unsigned packageid = infos[i].ids[PKG];
723  	hwloc_bitmap_t package_cpuset = hwloc_bitmap_alloc();
724  	for (j = i; j < nbprocs; j++) {
725  	  if (infos[j].ids[PKG] == packageid) {
726  	    hwloc_bitmap_set(package_cpuset, j);
727  	    hwloc_bitmap_clr(remaining_cpuset, j);
728  	  }
729  	}
730  	package = hwloc_alloc_setup_object(topology, HWLOC_OBJ_PACKAGE, packageid);
731  	package->cpuset = package_cpuset;
732  	hwloc_x86_add_cpuinfos(package, &infos[i], 0);
733  	hwloc_debug_1arg_bitmap("os package %u has cpuset %s\n",
734  				packageid, package_cpuset);
735  	hwloc__insert_object_by_cpuset(topology, NULL, package, "x86:package");
736        } else {
737  	hwloc_bitmap_t set = hwloc_bitmap_alloc();
738  	hwloc_bitmap_set(set, i);
739  	package = hwloc_get_next_obj_covering_cpuset_by_type(topology, set, HWLOC_OBJ_PACKAGE, NULL);
740  	hwloc_bitmap_free(set);
741  	if (package) {
742  	  hwloc_x86_add_cpuinfos(package, &infos[i], 1);
743  	  hwloc_bitmap_andnot(remaining_cpuset, remaining_cpuset, package->cpuset);
744  	} else {
745  	  hwloc_x86_add_cpuinfos(hwloc_get_root_obj(topology), &infos[i], 1);
746  	  break;
747  	}
748        }
749      }
750    }
751    if (fulldiscovery && (flags & HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES)) {
752      hwloc_bitmap_t node_cpuset;
753      hwloc_obj_t node;
754      hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
755      while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
756        unsigned packageid = infos[i].ids[PKG];
757        unsigned nodeid = infos[i].ids[NODE];
758        if (nodeid == (unsigned)-1) {
759          hwloc_bitmap_clr(remaining_cpuset, i);
760  	continue;
761        }
762        node_cpuset = hwloc_bitmap_alloc();
763        for (j = i; j < nbprocs; j++) {
764  	if (infos[j].ids[NODE] == (unsigned) -1) {
765  	  hwloc_bitmap_clr(remaining_cpuset, j);
766  	  continue;
767  	}
768          if (infos[j].ids[PKG] == packageid && infos[j].ids[NODE] == nodeid) {
769            hwloc_bitmap_set(node_cpuset, j);
770            hwloc_bitmap_clr(remaining_cpuset, j);
771          }
772        }
773        node = hwloc_alloc_setup_object(topology, HWLOC_OBJ_NUMANODE, nodeid);
774        node->cpuset = node_cpuset;
775        node->nodeset = hwloc_bitmap_alloc();
776        hwloc_bitmap_set(node->nodeset, nodeid);
777        hwloc_debug_1arg_bitmap("os node %u has cpuset %s\n",
778            nodeid, node_cpuset);
779        hwloc__insert_object_by_cpuset(topology, NULL, node, "x86:numa");
780        gotnuma++;
781      }
782    }
783    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_GROUP)) {
784      if (fulldiscovery) {
785        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
786        hwloc_x86_add_groups(topology, infos, nbprocs, remaining_cpuset,
787  			   UNIT, "Compute Unit",
788  			   HWLOC_GROUP_KIND_AMD_COMPUTE_UNIT, 0);
789        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
790        hwloc_x86_add_groups(topology, infos, nbprocs, remaining_cpuset,
791  			   MODULE, "Module",
792  			   HWLOC_GROUP_KIND_INTEL_MODULE, 0);
793        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
794        hwloc_x86_add_groups(topology, infos, nbprocs, remaining_cpuset,
795  			   TILE, "Tile",
796  			   HWLOC_GROUP_KIND_INTEL_TILE, 0);
797        if (infos[one].otherids) {
798  	for (level = infos[one].levels-1; level <= infos[one].levels-1; level--) {
799  	  if (infos[one].otherids[level] != UINT_MAX) {
800  	    hwloc_bitmap_t unknown_cpuset;
801  	    hwloc_obj_t unknown_obj;
802  	    hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
803  	    while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
804  	      unsigned unknownid = infos[i].otherids[level];
805  	      unknown_cpuset = hwloc_bitmap_alloc();
806  	      for (j = i; j < nbprocs; j++) {
807  		if (infos[j].otherids[level] == unknownid) {
808  		  hwloc_bitmap_set(unknown_cpuset, j);
809  		  hwloc_bitmap_clr(remaining_cpuset, j);
810  		}
811  	      }
812  	      unknown_obj = hwloc_alloc_setup_object(topology, HWLOC_OBJ_GROUP, unknownid);
813  	      unknown_obj->cpuset = unknown_cpuset;
814  	      unknown_obj->attr->group.kind = HWLOC_GROUP_KIND_INTEL_EXTTOPOENUM_UNKNOWN;
815  	      unknown_obj->attr->group.subkind = level;
816  	      hwloc_debug_2args_bitmap("os unknown%u %u has cpuset %s\n",
817  				       level, unknownid, unknown_cpuset);
818  	      hwloc__insert_object_by_cpuset(topology, NULL, unknown_obj, "x86:group:unknown");
819  	    }
820  	  }
821  	}
822        }
823      }
824    }
825    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_DIE)) {
826      if (fulldiscovery) {
827        hwloc_bitmap_t die_cpuset;
828        hwloc_obj_t die;
829        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
830        while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
831  	unsigned packageid = infos[i].ids[PKG];
832  	unsigned dieid = infos[i].ids[DIE];
833  	if (dieid == (unsigned) -1) {
834  	  hwloc_bitmap_clr(remaining_cpuset, i);
835  	  continue;
836  	}
837  	die_cpuset = hwloc_bitmap_alloc();
838  	for (j = i; j < nbprocs; j++) {
839  	  if (infos[j].ids[DIE] == (unsigned) -1) {
840  	    hwloc_bitmap_clr(remaining_cpuset, j);
841  	    continue;
842  	  }
843  	  if (infos[j].ids[PKG] == packageid && infos[j].ids[DIE] == dieid) {
844  	    hwloc_bitmap_set(die_cpuset, j);
845  	    hwloc_bitmap_clr(remaining_cpuset, j);
846  	  }
847  	}
848  	die = hwloc_alloc_setup_object(topology, HWLOC_OBJ_DIE, dieid);
849  	die->cpuset = die_cpuset;
850  	hwloc_debug_1arg_bitmap("os die %u has cpuset %s\n",
851  				dieid, die_cpuset);
852  	hwloc__insert_object_by_cpuset(topology, NULL, die, "x86:die");
853        }
854      }
855    }
856    if (hwloc_filter_check_keep_object_type(topology, HWLOC_OBJ_CORE)) {
857      if (fulldiscovery) {
858        hwloc_bitmap_t core_cpuset;
859        hwloc_obj_t core;
860        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
861        while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
862  	unsigned packageid = infos[i].ids[PKG];
863  	unsigned nodeid = infos[i].ids[NODE];
864  	unsigned coreid = infos[i].ids[CORE];
865  	if (coreid == (unsigned) -1) {
866  	  hwloc_bitmap_clr(remaining_cpuset, i);
867  	  continue;
868  	}
869  	core_cpuset = hwloc_bitmap_alloc();
870  	for (j = i; j < nbprocs; j++) {
871  	  if (infos[j].ids[CORE] == (unsigned) -1) {
872  	    hwloc_bitmap_clr(remaining_cpuset, j);
873  	    continue;
874  	  }
875  	  if (infos[j].ids[PKG] == packageid && infos[j].ids[NODE] == nodeid && infos[j].ids[CORE] == coreid) {
876  	    hwloc_bitmap_set(core_cpuset, j);
877  	    hwloc_bitmap_clr(remaining_cpuset, j);
878  	  }
879  	}
880  	core = hwloc_alloc_setup_object(topology, HWLOC_OBJ_CORE, coreid);
881  	core->cpuset = core_cpuset;
882  	hwloc_debug_1arg_bitmap("os core %u has cpuset %s\n",
883  				coreid, core_cpuset);
884  	hwloc__insert_object_by_cpuset(topology, NULL, core, "x86:core");
885        }
886      }
887    }
888    if (fulldiscovery) {
889      hwloc_debug("%s", "\n\n * CPU cpusets *\n\n");
890      for (i=0; i<nbprocs; i++)
891        if(infos[i].present) { &bsol;* Only add present PU. We don't know if others actually exist */
892         struct hwloc_obj *obj = hwloc_alloc_setup_object(topology, HWLOC_OBJ_PU, i);
893         obj->cpuset = hwloc_bitmap_alloc();
894         hwloc_bitmap_only(obj->cpuset, i);
895         hwloc_debug_1arg_bitmap("PU %u has cpuset %s\n", i, obj->cpuset);
896         hwloc__insert_object_by_cpuset(topology, NULL, obj, "x86:pu");
897       }
898    }
899    level = 0;
900    for (i = 0; i < nbprocs; i++)
901      for (j = 0; j < infos[i].numcaches; j++)
902        if (infos[i].cache[j].level > level)
903          level = infos[i].cache[j].level;
904    while (level > 0) {
905      hwloc_obj_cache_type_t type;
906      HWLOC_BUILD_ASSERT(HWLOC_OBJ_CACHE_DATA == HWLOC_OBJ_CACHE_UNIFIED+1);
907      HWLOC_BUILD_ASSERT(HWLOC_OBJ_CACHE_INSTRUCTION == HWLOC_OBJ_CACHE_DATA+1);
908      for (type = HWLOC_OBJ_CACHE_UNIFIED; type <= HWLOC_OBJ_CACHE_INSTRUCTION; type++) {
909        hwloc_obj_type_t otype;
910        hwloc_obj_t cache;
911        otype = hwloc_cache_type_by_depth_type(level, type);
912        if (otype == HWLOC_OBJ_TYPE_NONE)
913  	continue;
914        if (!hwloc_filter_check_keep_object_type(topology, otype))
915  	continue;
916        hwloc_bitmap_copy(remaining_cpuset, complete_cpuset);
917        while ((i = hwloc_bitmap_first(remaining_cpuset)) != (unsigned) -1) {
918  	hwloc_bitmap_t puset;
919  	for (l = 0; l < infos[i].numcaches; l++) {
920  	  if (infos[i].cache[l].level == level && infos[i].cache[l].type == type)
921  	    break;
922  	}
923  	if (l == infos[i].numcaches) {
924  	  hwloc_bitmap_clr(remaining_cpuset, i);
925  	  continue;
926  	}
927  	puset = hwloc_bitmap_alloc();
928  	hwloc_bitmap_set(puset, i);
929  	cache = hwloc_get_next_obj_covering_cpuset_by_type(topology, puset, otype, NULL);
930  	hwloc_bitmap_free(puset);
931  	if (cache) {
932  	  if (!hwloc_obj_get_info_by_name(cache, "Inclusive"))
933  	    hwloc_obj_add_info(cache, "Inclusive", infos[i].cache[l].inclusive ? "1" : "0");
934  	  hwloc_bitmap_andnot(remaining_cpuset, remaining_cpuset, cache->cpuset);
935  	} else {
936  	  hwloc_bitmap_t cache_cpuset;
937  	  unsigned packageid = infos[i].ids[PKG];
938  	  unsigned cacheid = infos[i].cache[l].cacheid;
939  	  cache_cpuset = hwloc_bitmap_alloc();
940  	  for (j = i; j < nbprocs; j++) {
941  	    unsigned l2;
942  	    for (l2 = 0; l2 < infos[j].numcaches; l2++) {
943  	      if (infos[j].cache[l2].level == level && infos[j].cache[l2].type == type)
944  		break;
945  	    }
946  	    if (l2 == infos[j].numcaches) {
947  	      hwloc_bitmap_clr(remaining_cpuset, j);
948  	      continue;
949  	    }
950  	    if (infos[j].ids[PKG] == packageid && infos[j].cache[l2].cacheid == cacheid) {
951  	      hwloc_bitmap_set(cache_cpuset, j);
952  	      hwloc_bitmap_clr(remaining_cpuset, j);
953  	    }
954  	  }
955  	  cache = hwloc_alloc_setup_object(topology, otype, HWLOC_UNKNOWN_INDEX);
956  	  cache->attr->cache.depth = level;
957  	  cache->attr->cache.size = infos[i].cache[l].size;
958  	  cache->attr->cache.linesize = infos[i].cache[l].linesize;
959  	  cache->attr->cache.associativity = infos[i].cache[l].ways;
960  	  cache->attr->cache.type = infos[i].cache[l].type;
961  	  cache->cpuset = cache_cpuset;
962  	  hwloc_obj_add_info(cache, "Inclusive", infos[i].cache[l].inclusive ? "1" : "0");
963  	  hwloc_debug_2args_bitmap("os L%u cache %u has cpuset %s\n",
964  				   level, cacheid, cache_cpuset);
965  	  hwloc__insert_object_by_cpuset(topology, NULL, cache, "x86:cache");
966  	}
967        }
968      }
969      level--;
970    }
971    hwloc_bitmap_free(remaining_cpuset);
972    hwloc_bitmap_free(complete_cpuset);
973    if (gotnuma)
974      topology->support.discovery->numa = 1;
975  }
976  static int
977  look_procs(struct hwloc_backend *backend, struct procinfo *infos, unsigned long flags,
978  	   unsigned highest_cpuid, unsigned highest_ext_cpuid, unsigned *features, enum cpuid_type cpuid_type,
979  	   int (*get_cpubind)(hwloc_topology_t topology, hwloc_cpuset_t set, int flags),
980  	   int (*set_cpubind)(hwloc_topology_t topology, hwloc_const_cpuset_t set, int flags),
981             hwloc_bitmap_t restrict_set)
982  {
983    struct hwloc_x86_backend_data_s *data = backend->private_data;
984    struct hwloc_topology *topology = backend->topology;
985    unsigned nbprocs = data->nbprocs;
986    hwloc_bitmap_t orig_cpuset = NULL;
987    hwloc_bitmap_t set = NULL;
988    unsigned i;
989    if (!data->src_cpuiddump_path) {
990      orig_cpuset = hwloc_bitmap_alloc();
991      if (get_cpubind(topology, orig_cpuset, HWLOC_CPUBIND_STRICT)) {
992        hwloc_bitmap_free(orig_cpuset);
993        return -1;
994      }
995      set = hwloc_bitmap_alloc();
996    }
997    for (i = 0; i < nbprocs; i++) {
998      struct cpuiddump *src_cpuiddump = NULL;
999      if (restrict_set && !hwloc_bitmap_isset(restrict_set, i)) {
1000        continue;
1001      }
1002      if (data->src_cpuiddump_path) {
1003        src_cpuiddump = cpuiddump_read(data->src_cpuiddump_path, i);
1004        if (!src_cpuiddump)
1005  	continue;
1006      } else {
1007        hwloc_bitmap_only(set, i);
1008        hwloc_debug("binding to CPU%u\n", i);
1009        if (set_cpubind(topology, set, HWLOC_CPUBIND_STRICT)) {
1010  	hwloc_debug("could not bind to CPU%u: %s\n", i, strerror(errno));
1011  	continue;
1012        }
1013      }
1014      look_proc(backend, &infos[i], flags, highest_cpuid, highest_ext_cpuid, features, cpuid_type, src_cpuiddump);
1015      if (data->src_cpuiddump_path) {
1016        cpuiddump_free(src_cpuiddump);
1017      }
1018    }
1019    if (!data->src_cpuiddump_path) {
1020      set_cpubind(topology, orig_cpuset, 0);
1021      hwloc_bitmap_free(set);
1022      hwloc_bitmap_free(orig_cpuset);
1023    }
1024    if (data->apicid_unique) {
1025      summarize(backend, infos, flags);
1026      if (has_hybrid(features) && !(topology->flags & HWLOC_TOPOLOGY_FLAG_NO_CPUKINDS)) {
1027        hwloc_bitmap_t atomset = hwloc_bitmap_alloc();
1028        hwloc_bitmap_t coreset = hwloc_bitmap_alloc();
1029        for(i=0; i<nbprocs; i++) {
1030          if (infos[i].hybridcoretype == 0x20)
1031            hwloc_bitmap_set(atomset, i);
1032          else if (infos[i].hybridcoretype == 0x40)
1033            hwloc_bitmap_set(coreset, i);
1034        }
1035        if (!hwloc_bitmap_iszero(atomset)) {
1036          struct hwloc_info_s infoattr;
1037          infoattr.name = (char *) "CoreType";
1038          infoattr.value = (char *) "IntelAtom";
1039          hwloc_internal_cpukinds_register(topology, atomset, HWLOC_CPUKIND_EFFICIENCY_UNKNOWN, &infoattr, 1, 0);
1040        } else {
1041          hwloc_bitmap_free(atomset);
1042        }
1043        if (!hwloc_bitmap_iszero(coreset)) {
1044          struct hwloc_info_s infoattr;
1045          infoattr.name = (char *) "CoreType";
1046          infoattr.value = (char *) "IntelCore";
1047          hwloc_internal_cpukinds_register(topology, coreset, HWLOC_CPUKIND_EFFICIENCY_UNKNOWN, &infoattr, 1, 0);
1048        } else {
1049          hwloc_bitmap_free(coreset);
1050        }
1051      }
1052    }
1053    return 0;
1054  }
1055  #if defined HWLOC_FREEBSD_SYS && defined HAVE_CPUSET_SETID
1056  #include <sys/param.h>
1057  #include <sys/cpuset.h>
1058  typedef cpusetid_t hwloc_x86_os_state_t;
1059  static void hwloc_x86_os_state_save(hwloc_x86_os_state_t *state, struct cpuiddump *src_cpuiddump)
1060  {
1061    if (!src_cpuiddump) {
1062      cpuset_getid(CPU_LEVEL_CPUSET, CPU_WHICH_PID, -1, state);
1063      cpuset_setid(CPU_WHICH_PID, -1, 0);
1064    }
1065  }
1066  static void hwloc_x86_os_state_restore(hwloc_x86_os_state_t *state, struct cpuiddump *src_cpuiddump)
1067  {
1068    if (!src_cpuiddump) {
1069      cpuset_setid(CPU_WHICH_PID, -1, *state);
1070    }
1071  }
1072  #else &bsol;* !defined HWLOC_FREEBSD_SYS || !defined HAVE_CPUSET_SETID */
1073  typedef void * hwloc_x86_os_state_t;
1074  static void hwloc_x86_os_state_save(hwloc_x86_os_state_t *state __hwloc_attribute_unused, struct cpuiddump *src_cpuiddump __hwloc_attribute_unused) { }
1075  static void hwloc_x86_os_state_restore(hwloc_x86_os_state_t *state __hwloc_attribute_unused, struct cpuiddump *src_cpuiddump __hwloc_attribute_unused) { }
1076  #endif &bsol;* !defined HWLOC_FREEBSD_SYS || !defined HAVE_CPUSET_SETID */
1077  #define INTEL_EBX ('G' | ('e'<<8) | ('n'<<16) | ('u'<<24))
1078  #define INTEL_EDX ('i' | ('n'<<8) | ('e'<<16) | ('I'<<24))
1079  #define INTEL_ECX ('n' | ('t'<<8) | ('e'<<16) | ('l'<<24))
1080  #define AMD_EBX ('A' | ('u'<<8) | ('t'<<16) | ('h'<<24))
1081  #define AMD_EDX ('e' | ('n'<<8) | ('t'<<16) | ('i'<<24))
1082  #define AMD_ECX ('c' | ('A'<<8) | ('M'<<16) | ('D'<<24))
1083  #define HYGON_EBX ('H' | ('y'<<8) | ('g'<<16) | ('o'<<24))
1084  #define HYGON_EDX ('n' | ('G'<<8) | ('e'<<16) | ('n'<<24))
1085  #define HYGON_ECX ('u' | ('i'<<8) | ('n'<<16) | ('e'<<24))
1086  #define ZX_EBX ('C' | ('e'<<8) | ('n'<<16) | ('t'<<24))
1087  #define ZX_EDX ('a' | ('u'<<8) | ('r'<<16) | ('H'<<24))
1088  #define ZX_ECX ('a' | ('u'<<8) | ('l'<<16) | ('s'<<24))
1089  #define SH_EBX (' ' | (' '<<8) | ('S'<<16) | ('h'<<24))
1090  #define SH_EDX ('a' | ('n'<<8) | ('g'<<16) | ('h'<<24))
1091  #define SH_ECX ('a' | ('i'<<8) | (' '<<16) | (' '<<24))
1092  static int fake_get_cpubind(hwloc_topology_t topology __hwloc_attribute_unused,
1093  			    hwloc_cpuset_t set __hwloc_attribute_unused,
1094  			    int flags __hwloc_attribute_unused)
1095  {
1096    return 0;
1097  }
1098  static int fake_set_cpubind(hwloc_topology_t topology __hwloc_attribute_unused,
1099  			    hwloc_const_cpuset_t set __hwloc_attribute_unused,
1100  			    int flags __hwloc_attribute_unused)
1101  {
1102    return 0;
1103  }
1104  static
1105  int hwloc_look_x86(struct hwloc_backend *backend, unsigned long flags)
1106  {
1107    struct hwloc_x86_backend_data_s *data = backend->private_data;
1108    struct hwloc_topology *topology = backend->topology;
1109    unsigned nbprocs = data->nbprocs;
1110    unsigned eax, ebx, ecx = 0, edx;
1111    unsigned i;
1112    unsigned highest_cpuid;
1113    unsigned highest_ext_cpuid;
1114    unsigned features[19] = { 0 };
1115    struct procinfo *infos = NULL;
1116    enum cpuid_type cpuid_type = unknown;
1117    hwloc_x86_os_state_t os_state;
1118    struct hwloc_binding_hooks hooks;
1119    struct hwloc_topology_support support;
1120    struct hwloc_topology_membind_support memsupport __hwloc_attribute_unused;
1121    int (*get_cpubind)(hwloc_topology_t topology, hwloc_cpuset_t set, int flags) = NULL;
1122    int (*set_cpubind)(hwloc_topology_t topology, hwloc_const_cpuset_t set, int flags) = NULL;
1123    hwloc_bitmap_t restrict_set = NULL;
1124    struct cpuiddump *src_cpuiddump = NULL;
1125    int ret = -1;
1126    memset(&hooks, 0, sizeof(hooks));
1127    support.membind = &memsupport;
1128    hwloc_set_native_binding_hooks(&hooks, &support);
1129    if (data->src_cpuiddump_path) {
1130      src_cpuiddump = cpuiddump_read(data->src_cpuiddump_path, 0);
1131      if (!src_cpuiddump)
1132        goto out;
1133    } else {
1134      if (hooks.get_thisthread_cpubind && hooks.set_thisthread_cpubind) {
1135        get_cpubind = hooks.get_thisthread_cpubind;
1136        set_cpubind = hooks.set_thisthread_cpubind;
1137      } else if (hooks.get_thisproc_cpubind && hooks.set_thisproc_cpubind) {
1138        get_cpubind = hooks.get_thisproc_cpubind;
1139        set_cpubind = hooks.set_thisproc_cpubind;
1140      } else {
1141        if (nbprocs > 1)
1142  	goto out;
1143        get_cpubind = fake_get_cpubind;
1144        set_cpubind = fake_set_cpubind;
1145      }
1146    }
1147    if (topology->flags & HWLOC_TOPOLOGY_FLAG_RESTRICT_TO_CPUBINDING) {
1148      restrict_set = hwloc_bitmap_alloc();
1149      if (!restrict_set)
1150        goto out;
1151      if (hooks.get_thisproc_cpubind)
1152        hooks.get_thisproc_cpubind(topology, restrict_set, 0);
1153      else if (hooks.get_thisthread_cpubind)
1154        hooks.get_thisthread_cpubind(topology, restrict_set, 0);
1155      if (hwloc_bitmap_iszero(restrict_set)) {
1156        hwloc_bitmap_free(restrict_set);
1157        restrict_set = NULL;
1158      }
1159    }
1160    if (!src_cpuiddump && !hwloc_have_x86_cpuid())
1161      goto out;
1162    infos = calloc(nbprocs, sizeof(struct procinfo));
1163    if (NULL == infos)
1164      goto out;
1165    for (i = 0; i < nbprocs; i++) {
1166      infos[i].ids[PKG] = (unsigned) -1;
1167      infos[i].ids[CORE] = (unsigned) -1;
1168      infos[i].ids[NODE] = (unsigned) -1;
1169      infos[i].ids[UNIT] = (unsigned) -1;
1170      infos[i].ids[TILE] = (unsigned) -1;
1171      infos[i].ids[MODULE] = (unsigned) -1;
1172      infos[i].ids[DIE] = (unsigned) -1;
1173    }
1174    eax = 0x00;
1175    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1176    highest_cpuid = eax;
1177    if (ebx == INTEL_EBX && ecx == INTEL_ECX && edx == INTEL_EDX)
1178      cpuid_type = intel;
1179    else if (ebx == AMD_EBX && ecx == AMD_ECX && edx == AMD_EDX)
1180      cpuid_type = amd;
1181    else if ((ebx == ZX_EBX && ecx == ZX_ECX && edx == ZX_EDX)
1182  	   || (ebx == SH_EBX && ecx == SH_ECX && edx == SH_EDX))
1183      cpuid_type = zhaoxin;
1184    else if (ebx == HYGON_EBX && ecx == HYGON_ECX && edx == HYGON_EDX)
1185      cpuid_type = hygon;
1186    hwloc_debug("highest cpuid %x, cpuid type %u\n", highest_cpuid, cpuid_type);
1187    if (highest_cpuid < 0x01) {
1188        goto out_with_infos;
1189    }
1190    eax = 0x01;
1191    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1192    features[0] = edx;
1193    features[4] = ecx;
1194    eax = 0x80000000;
1195    cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1196    highest_ext_cpuid = eax;
1197    hwloc_debug("highest extended cpuid %x\n", highest_ext_cpuid);
1198    if (highest_cpuid >= 0x7) {
1199      eax = 0x7;
1200      ecx = 0;
1201      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1202      features[9] = ebx;
1203      features[18] = edx;
1204    }
1205    if (cpuid_type != intel && highest_ext_cpuid >= 0x80000001) {
1206      eax = 0x80000001;
1207      cpuid_or_from_dump(&eax, &ebx, &ecx, &edx, src_cpuiddump);
1208      features[1] = edx;
1209      features[6] = ecx;
1210    }
1211    hwloc_x86_os_state_save(&os_state, src_cpuiddump);
1212    ret = look_procs(backend, infos, flags,
1213  		   highest_cpuid, highest_ext_cpuid, features, cpuid_type,
1214  		   get_cpubind, set_cpubind, restrict_set);
1215    if (!ret)
1216      goto out_with_os_state;
1217    if (nbprocs == 1) {
1218      look_proc(backend, &infos[0], flags, highest_cpuid, highest_ext_cpuid, features, cpuid_type, src_cpuiddump);
1219      summarize(backend, infos, flags);
1220      ret = 0;
1221    }
1222  out_with_os_state:
1223    hwloc_x86_os_state_restore(&os_state, src_cpuiddump);
1224  out_with_infos:
1225    if (NULL != infos) {
1226      for (i = 0; i < nbprocs; i++) {
1227        free(infos[i].cache);
1228        free(infos[i].otherids);
1229      }
1230      free(infos);
1231    }
1232  out:
1233    hwloc_bitmap_free(restrict_set);
1234    if (src_cpuiddump)
1235      cpuiddump_free(src_cpuiddump);
1236    return ret;
1237  }
1238  static int
1239  hwloc_x86_discover(struct hwloc_backend *backend, struct hwloc_disc_status *dstatus)
1240  {
1241    struct hwloc_x86_backend_data_s *data = backend->private_data;
1242    struct hwloc_topology *topology = backend->topology;
1243    unsigned long flags = 0;
1244    int alreadypus = 0;
1245    int ret;
1246    assert(dstatus->phase == HWLOC_DISC_PHASE_CPU);
1247    if (topology->flags & HWLOC_TOPOLOGY_FLAG_DONT_CHANGE_BINDING) {
1248      return 0;
1249    }
1250    if (getenv("HWLOC_X86_TOPOEXT_NUMANODES")) {
1251      flags |= HWLOC_X86_DISC_FLAG_TOPOEXT_NUMANODES;
1252    }
1253  #if HAVE_DECL_RUNNING_ON_VALGRIND
1254    if (RUNNING_ON_VALGRIND && !data->src_cpuiddump_path) {
1255      fprintf(stderr, "hwloc x86 backend cannot work under Valgrind, disabling.\n"
1256  	    "May be reenabled by dumping CPUIDs with hwloc-gather-cpuid\n"
1257  	    "and reloading them under Valgrind with HWLOC_CPUID_PATH.\n");
1258      return 0;
1259    }
1260  #endif
1261    if (data->src_cpuiddump_path) {
1262      assert(data->nbprocs > 0); &bsol;* enforced by hwloc_x86_component_instantiate() */
1263      topology->support.discovery->pu = 1;
1264    } else {
1265      int nbprocs = hwloc_fallback_nbprocessors(HWLOC_FALLBACK_NBPROCESSORS_INCLUDE_OFFLINE);
1266      if (nbprocs >= 1)
1267        topology->support.discovery->pu = 1;
1268      else
1269        nbprocs = 1;
1270      data->nbprocs = (unsigned) nbprocs;
1271    }
1272    if (topology->levels[0][0]->cpuset) {
1273      hwloc_topology_reconnect(topology, 0);
1274      if (topology->nb_levels == 2 && topology->level_nbobjects[1] == data->nbprocs) {
1275        alreadypus = 1;
1276        goto fulldiscovery;
1277      }
1278      ret = hwloc_look_x86(backend, flags);
1279      if (ret)
1280        hwloc_obj_add_info(topology->levels[0][0], "Backend", "x86");
1281      return 0;
1282    } else {
1283      hwloc_alloc_root_sets(topology->levels[0][0]);
1284    }
1285  fulldiscovery:
1286    if (hwloc_look_x86(backend, flags | HWLOC_X86_DISC_FLAG_FULL) < 0) {
1287      if (!alreadypus)
1288        hwloc_setup_pu_level(topology, data->nbprocs);
1289    }
1290    hwloc_obj_add_info(topology->levels[0][0], "Backend", "x86");
1291    if (!data->src_cpuiddump_path) { &bsol;* CPUID dump works for both x86 and x86_64 */
1292  #ifdef HAVE_UNAME
1293      hwloc_add_uname_info(topology, NULL); &bsol;* we already know is_thissystem() is true */
1294  #else
1295  #ifdef HWLOC_X86_64_ARCH
1296      hwloc_obj_add_info(topology->levels[0][0], "Architecture", "x86_64");
1297  #else
1298      hwloc_obj_add_info(topology->levels[0][0], "Architecture", "x86");
1299  #endif
1300  #endif
1301    }
1302    return 1;
1303  }
1304  static int
1305  hwloc_x86_check_cpuiddump_input(const char *src_cpuiddump_path, hwloc_bitmap_t set)
1306  {
1307  #if !(defined HWLOC_WIN_SYS && !defined __MINGW32__ && !defined __CYGWIN__) &bsol;* needs a lot of work */
1308    struct dirent *dirent;
1309    DIR *dir;
1310    char *path;
1311    FILE *file;
1312    char line [32];
1313    dir = opendir(src_cpuiddump_path);
1314    if (!dir) 
1315      return -1;
1316    path = malloc(strlen(src_cpuiddump_path) + strlen("/hwloc-cpuid-info") + 1);
1317    if (!path)
1318      goto out_with_dir;
1319    sprintf(path, "%s/hwloc-cpuid-info", src_cpuiddump_path);
1320    file = fopen(path, "r");
1321    if (!file) {
1322      fprintf(stderr, "Couldn't open dumped cpuid summary %s\n", path);
1323      goto out_with_path;
1324    }
1325    if (!fgets(line, sizeof(line), file)) {
1326      fprintf(stderr, "Found read dumped cpuid summary in %s\n", path);
1327      fclose(file);
1328      goto out_with_path;
1329    }
1330    fclose(file);
1331    if (strcmp(line, "Architecture: x86\n")) {
1332      fprintf(stderr, "Found non-x86 dumped cpuid summary in %s: %s\n", path, line);
1333      goto out_with_path;
1334    }
1335    free(path);
1336    while ((dirent = readdir(dir)) != NULL) {
1337      if (!strncmp(dirent->d_name, "pu", 2)) {
1338        char *end;
1339        unsigned long idx = strtoul(dirent->d_name+2, &end, 10);
1340        if (!*end)
1341  	hwloc_bitmap_set(set, idx);
1342        else
1343  	fprintf(stderr, "Ignoring invalid dirent `%s' in dumped cpuid directory `%s'\n",
1344  		dirent->d_name, src_cpuiddump_path);
1345      }
1346    }
1347    closedir(dir);
1348    if (hwloc_bitmap_iszero(set)) {
1349      fprintf(stderr, "Did not find any valid pu%%u entry in dumped cpuid directory `%s'\n",
1350  	    src_cpuiddump_path);
1351      return -1;
1352    } else if (hwloc_bitmap_last(set) != hwloc_bitmap_weight(set) - 1) {
1353      fprintf(stderr, "Found non-contigous pu%%u range in dumped cpuid directory `%s'\n",
1354  	    src_cpuiddump_path);
1355      return -1;
1356    }
1357    return 0;
1358   out_with_path:
1359    free(path);
1360   out_with_dir:
1361    closedir(dir);
1362  #endif &bsol;* HWLOC_WIN_SYS & !__MINGW32__ needs a lot of work */
1363    return -1;
1364  }
1365  static void
1366  hwloc_x86_backend_disable(struct hwloc_backend *backend)
1367  {
1368    struct hwloc_x86_backend_data_s *data = backend->private_data;
1369    hwloc_bitmap_free(data->apicid_set);
1370    free(data->src_cpuiddump_path);
1371    free(data);
1372  }
1373  static struct hwloc_backend *
1374  hwloc_x86_component_instantiate(struct hwloc_topology *topology,
1375  				struct hwloc_disc_component *component,
1376  				unsigned excluded_phases __hwloc_attribute_unused,
1377  				const void *_data1 __hwloc_attribute_unused,
1378  				const void *_data2 __hwloc_attribute_unused,
1379  				const void *_data3 __hwloc_attribute_unused)
1380  {
1381    struct hwloc_backend *backend;
1382    struct hwloc_x86_backend_data_s *data;
1383    const char *src_cpuiddump_path;
1384    backend = hwloc_backend_alloc(topology, component);
1385    if (!backend)
1386      goto out;
1387    data = malloc(sizeof(*data));
1388    if (!data) {
1389      errno = ENOMEM;
1390      goto out_with_backend;
1391    }
1392    backend->private_data = data;
1393    backend->discover = hwloc_x86_discover;
1394    backend->disable = hwloc_x86_backend_disable;
1395    data->is_knl = 0;
1396    data->apicid_set = hwloc_bitmap_alloc();
1397    data->apicid_unique = 1;
1398    data->src_cpuiddump_path = NULL;
1399    src_cpuiddump_path = getenv("HWLOC_CPUID_PATH");
1400    if (src_cpuiddump_path) {
1401      hwloc_bitmap_t set = hwloc_bitmap_alloc();
1402      if (!hwloc_x86_check_cpuiddump_input(src_cpuiddump_path, set)) {
1403        backend->is_thissystem = 0;
1404        data->src_cpuiddump_path = strdup(src_cpuiddump_path);
1405        assert(!hwloc_bitmap_iszero(set)); &bsol;* enforced by hwloc_x86_check_cpuiddump_input() */
1406        data->nbprocs = hwloc_bitmap_weight(set);
1407      } else {
1408        fprintf(stderr, "Ignoring dumped cpuid directory.\n");
1409      }
1410      hwloc_bitmap_free(set);
1411    }
1412    return backend;
1413   out_with_backend:
1414    free(backend);
1415   out:
1416    return NULL;
1417  }
1418  static struct hwloc_disc_component hwloc_x86_disc_component = {
1419    "x86",
1420    HWLOC_DISC_PHASE_CPU,
1421    HWLOC_DISC_PHASE_GLOBAL,
1422    hwloc_x86_component_instantiate,
1423    45, &bsol;* between native and no_os */
1424    1,
1425    NULL
1426  };
1427  const struct hwloc_component hwloc_x86_component = {
1428    HWLOC_COMPONENT_ABI,
1429    NULL, NULL,
1430    HWLOC_COMPONENT_TYPE_DISC,
1431    0,
1432    &hwloc_x86_disc_component
1433  };
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-topology-x86.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-topology-x86.c</div>
                </div>
                <div class="column column_space"><pre><code>282  		  cache->nbthreads_sharing, linesize, linepart, ways, sets, cache->size >> 10);
283        cache++;
284      }
285    } else {
</pre></code></div>
                <div class="column column_space"><pre><code>282  		  cache->nbthreads_sharing, linesize, linepart, ways, sets, cache->size >> 10);
283        cache++;
284      }
285    } else {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    