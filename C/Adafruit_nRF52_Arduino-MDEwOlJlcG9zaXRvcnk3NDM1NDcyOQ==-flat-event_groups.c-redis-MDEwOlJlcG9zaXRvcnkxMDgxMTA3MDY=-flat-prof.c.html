
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 4.752475247524752%, Tokens: 9</h2>
        <div class="column">
            <h3>Adafruit_nRF52_Arduino-MDEwOlJlcG9zaXRvcnk3NDM1NDcyOQ==-flat-event_groups.c</h3>
            <pre><code>1  #include <stdlib.h>
2  #define MPU_WRAPPERS_INCLUDED_FROM_API_FILE
3  #include "FreeRTOS.h"
4  #include "task.h"
5  #include "timers.h"
6  #include "event_groups.h"
7  #undef MPU_WRAPPERS_INCLUDED_FROM_API_FILE &bsol;*lint !e961 !e750. */
8  #if configUSE_16_BIT_TICKS == 1
9  	#define eventCLEAR_EVENTS_ON_EXIT_BIT	0x0100U
10  	#define eventUNBLOCKED_DUE_TO_BIT_SET	0x0200U
11  	#define eventWAIT_FOR_ALL_BITS			0x0400U
12  	#define eventEVENT_BITS_CONTROL_BYTES	0xff00U
13  #else
14  	#define eventCLEAR_EVENTS_ON_EXIT_BIT	0x01000000UL
15  	#define eventUNBLOCKED_DUE_TO_BIT_SET	0x02000000UL
16  	#define eventWAIT_FOR_ALL_BITS			0x04000000UL
17  	#define eventEVENT_BITS_CONTROL_BYTES	0xff000000UL
18  #endif
19  typedef struct xEventGroupDefinition
20  {
21  	EventBits_t uxEventBits;
22  	List_t xTasksWaitingForBits;		&bsol;*< List of tasks waiting for a bit to be set. */
23  	#if( configUSE_TRACE_FACILITY == 1 )
24  		UBaseType_t uxEventGroupNumber;
25  	#endif
26  	#if( ( configSUPPORT_STATIC_ALLOCATION == 1 ) && ( configSUPPORT_DYNAMIC_ALLOCATION == 1 ) )
27  		uint8_t ucStaticallyAllocated; &bsol;*< Set to pdTRUE if the event group is statically allocated to ensure no attempt is made to free the memory. */
28  	#endif
29  } EventGroup_t;
30  static BaseType_t prvTestWaitCondition( const EventBits_t uxCurrentEventBits, const EventBits_t uxBitsToWaitFor, const BaseType_t xWaitForAllBits ) PRIVILEGED_FUNCTION;
31  #if( configSUPPORT_STATIC_ALLOCATION == 1 )
32  	EventGroupHandle_t xEventGroupCreateStatic( StaticEventGroup_t *pxEventGroupBuffer )
33  	{
34  	EventGroup_t *pxEventBits;
35  		configASSERT( pxEventGroupBuffer );
36  		#if( configASSERT_DEFINED == 1 )
37  		{
38  			volatile size_t xSize = sizeof( StaticEventGroup_t );
39  			configASSERT( xSize == sizeof( EventGroup_t ) );
40  		}
41  		#endif &bsol;* configASSERT_DEFINED */
42  		pxEventBits = ( EventGroup_t * ) pxEventGroupBuffer; &bsol;*lint !e740 EventGroup_t and StaticEventGroup_t are guaranteed to have the same size and alignment requirement - checked by configASSERT(). */
43  		if( pxEventBits != NULL )
44  		{
45  			pxEventBits->uxEventBits = 0;
46  			vListInitialise( &( pxEventBits->xTasksWaitingForBits ) );
47  			#if( configSUPPORT_DYNAMIC_ALLOCATION == 1 )
48  			{
49  				pxEventBits->ucStaticallyAllocated = pdTRUE;
50  			}
51  			#endif &bsol;* configSUPPORT_DYNAMIC_ALLOCATION */
52  			traceEVENT_GROUP_CREATE( pxEventBits );
53  		}
54  		else
55  		{
56  			traceEVENT_GROUP_CREATE_FAILED();
57  		}
58  		return ( EventGroupHandle_t ) pxEventBits;
59  	}
60  #endif &bsol;* configSUPPORT_STATIC_ALLOCATION */
61  #if( configSUPPORT_DYNAMIC_ALLOCATION == 1 )
62  	EventGroupHandle_t xEventGroupCreate( void )
63  	{
64  	EventGroup_t *pxEventBits;
65  		pxEventBits = ( EventGroup_t * ) pvPortMalloc( sizeof( EventGroup_t ) );
66  		if( pxEventBits != NULL )
67  		{
68  			pxEventBits->uxEventBits = 0;
69  			vListInitialise( &( pxEventBits->xTasksWaitingForBits ) );
70  			#if( configSUPPORT_STATIC_ALLOCATION == 1 )
71  			{
72  				pxEventBits->ucStaticallyAllocated = pdFALSE;
73  			}
74  			#endif &bsol;* configSUPPORT_STATIC_ALLOCATION */
75  			traceEVENT_GROUP_CREATE( pxEventBits );
76  		}
77  		else
78  		{
79  			traceEVENT_GROUP_CREATE_FAILED();
80  		}
81  		return ( EventGroupHandle_t ) pxEventBits;
82  	}
83  #endif &bsol;* configSUPPORT_DYNAMIC_ALLOCATION */
84  EventBits_t xEventGroupSync( EventGroupHandle_t xEventGroup, const EventBits_t uxBitsToSet, const EventBits_t uxBitsToWaitFor, TickType_t xTicksToWait )
85  {
86  EventBits_t uxOriginalBitValue, uxReturn;
87  EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
88  BaseType_t xAlreadyYielded;
89  BaseType_t xTimeoutOccurred = pdFALSE;
90  	configASSERT( ( uxBitsToWaitFor & eventEVENT_BITS_CONTROL_BYTES ) == 0 );
91  	configASSERT( uxBitsToWaitFor != 0 );
92  	#if ( ( INCLUDE_xTaskGetSchedulerState == 1 ) || ( configUSE_TIMERS == 1 ) )
93  	{
94  		configASSERT( !( ( xTaskGetSchedulerState() == taskSCHEDULER_SUSPENDED ) && ( xTicksToWait != 0 ) ) );
95  	}
96  	#endif
97  	vTaskSuspendAll();
98  	{
99  		uxOriginalBitValue = pxEventBits->uxEventBits;
100  		( void ) xEventGroupSetBits( xEventGroup, uxBitsToSet );
101  		if( ( ( uxOriginalBitValue | uxBitsToSet ) & uxBitsToWaitFor ) == uxBitsToWaitFor )
102  		{
103  			uxReturn = ( uxOriginalBitValue | uxBitsToSet );
104  			pxEventBits->uxEventBits &= ~uxBitsToWaitFor;
105  			xTicksToWait = 0;
106  		}
107  		else
108  		{
109  			if( xTicksToWait != ( TickType_t ) 0 )
110  			{
111  				traceEVENT_GROUP_SYNC_BLOCK( xEventGroup, uxBitsToSet, uxBitsToWaitFor );
112  				vTaskPlaceOnUnorderedEventList( &( pxEventBits->xTasksWaitingForBits ), ( uxBitsToWaitFor | eventCLEAR_EVENTS_ON_EXIT_BIT | eventWAIT_FOR_ALL_BITS ), xTicksToWait );
113  				uxReturn = 0;
114  			}
115  			else
116  			{
117  				uxReturn = pxEventBits->uxEventBits;
118  				xTimeoutOccurred = pdTRUE;
119  			}
120  		}
121  	}
122  	xAlreadyYielded = xTaskResumeAll();
123  	if( xTicksToWait != ( TickType_t ) 0 )
124  	{
125  		if( xAlreadyYielded == pdFALSE )
126  		{
127  			portYIELD_WITHIN_API();
128  		}
129  		else
130  		{
131  			mtCOVERAGE_TEST_MARKER();
132  		}
133  		uxReturn = uxTaskResetEventItemValue();
134  		if( ( uxReturn & eventUNBLOCKED_DUE_TO_BIT_SET ) == ( EventBits_t ) 0 )
135  		{
136  			taskENTER_CRITICAL();
137  			{
<span onclick='openModal()' class='match'>138  				uxReturn = pxEventBits->uxEventBits;
139  				if( ( uxReturn & uxBitsToWaitFor ) == uxBitsToWaitFor )
140  				{
141  					pxEventBits->uxEventBits &= ~uxBitsToWaitFor;
142  				}
143  				else
144  				{
145  					mtCOVERAGE_TEST_MARKER();
146  				}
147  			}
</span>148  			taskEXIT_CRITICAL();
149  			xTimeoutOccurred = pdTRUE;
150  		}
151  		else
152  		{
153  		}
154  		uxReturn &= ~eventEVENT_BITS_CONTROL_BYTES;
155  	}
156  	traceEVENT_GROUP_SYNC_END( xEventGroup, uxBitsToSet, uxBitsToWaitFor, xTimeoutOccurred );
157  	( void ) xTimeoutOccurred;
158  	return uxReturn;
159  }
160  EventBits_t xEventGroupWaitBits( EventGroupHandle_t xEventGroup, const EventBits_t uxBitsToWaitFor, const BaseType_t xClearOnExit, const BaseType_t xWaitForAllBits, TickType_t xTicksToWait )
161  {
162  EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
163  EventBits_t uxReturn, uxControlBits = 0;
164  BaseType_t xWaitConditionMet, xAlreadyYielded;
165  BaseType_t xTimeoutOccurred = pdFALSE;
166  	configASSERT( xEventGroup );
167  	configASSERT( ( uxBitsToWaitFor & eventEVENT_BITS_CONTROL_BYTES ) == 0 );
168  	configASSERT( uxBitsToWaitFor != 0 );
169  	#if ( ( INCLUDE_xTaskGetSchedulerState == 1 ) || ( configUSE_TIMERS == 1 ) )
170  	{
171  		configASSERT( !( ( xTaskGetSchedulerState() == taskSCHEDULER_SUSPENDED ) && ( xTicksToWait != 0 ) ) );
172  	}
173  	#endif
174  	vTaskSuspendAll();
175  	{
176  		const EventBits_t uxCurrentEventBits = pxEventBits->uxEventBits;
177  		xWaitConditionMet = prvTestWaitCondition( uxCurrentEventBits, uxBitsToWaitFor, xWaitForAllBits );
178  		if( xWaitConditionMet != pdFALSE )
179  		{
180  			uxReturn = uxCurrentEventBits;
181  			xTicksToWait = ( TickType_t ) 0;
182  			if( xClearOnExit != pdFALSE )
183  			{
184  				pxEventBits->uxEventBits &= ~uxBitsToWaitFor;
185  			}
186  			else
187  			{
188  				mtCOVERAGE_TEST_MARKER();
189  			}
190  		}
191  		else if( xTicksToWait == ( TickType_t ) 0 )
192  		{
193  			uxReturn = uxCurrentEventBits;
194  			xTimeoutOccurred = pdTRUE;
195  		}
196  		else
197  		{
198  			if( xClearOnExit != pdFALSE )
199  			{
200  				uxControlBits |= eventCLEAR_EVENTS_ON_EXIT_BIT;
201  			}
202  			else
203  			{
204  				mtCOVERAGE_TEST_MARKER();
205  			}
206  			if( xWaitForAllBits != pdFALSE )
207  			{
208  				uxControlBits |= eventWAIT_FOR_ALL_BITS;
209  			}
210  			else
211  			{
212  				mtCOVERAGE_TEST_MARKER();
213  			}
214  			vTaskPlaceOnUnorderedEventList( &( pxEventBits->xTasksWaitingForBits ), ( uxBitsToWaitFor | uxControlBits ), xTicksToWait );
215  			uxReturn = 0;
216  			traceEVENT_GROUP_WAIT_BITS_BLOCK( xEventGroup, uxBitsToWaitFor );
217  		}
218  	}
219  	xAlreadyYielded = xTaskResumeAll();
220  	if( xTicksToWait != ( TickType_t ) 0 )
221  	{
222  		if( xAlreadyYielded == pdFALSE )
223  		{
224  			portYIELD_WITHIN_API();
225  		}
226  		else
227  		{
228  			mtCOVERAGE_TEST_MARKER();
229  		}
230  		uxReturn = uxTaskResetEventItemValue();
231  		if( ( uxReturn & eventUNBLOCKED_DUE_TO_BIT_SET ) == ( EventBits_t ) 0 )
232  		{
233  			taskENTER_CRITICAL();
234  			{
235  				uxReturn = pxEventBits->uxEventBits;
236  				if( prvTestWaitCondition( uxReturn, uxBitsToWaitFor, xWaitForAllBits ) != pdFALSE )
237  				{
238  					if( xClearOnExit != pdFALSE )
239  					{
240  						pxEventBits->uxEventBits &= ~uxBitsToWaitFor;
241  					}
242  					else
243  					{
244  						mtCOVERAGE_TEST_MARKER();
245  					}
246  				}
247  				else
248  				{
249  					mtCOVERAGE_TEST_MARKER();
250  				}
251  				xTimeoutOccurred = pdTRUE;
252  			}
253  			taskEXIT_CRITICAL();
254  		}
255  		else
256  		{
257  		}
258  		uxReturn &= ~eventEVENT_BITS_CONTROL_BYTES;
259  	}
260  	traceEVENT_GROUP_WAIT_BITS_END( xEventGroup, uxBitsToWaitFor, xTimeoutOccurred );
261  	( void ) xTimeoutOccurred;
262  	return uxReturn;
263  }
264  EventBits_t xEventGroupClearBits( EventGroupHandle_t xEventGroup, const EventBits_t uxBitsToClear )
265  {
266  EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
267  EventBits_t uxReturn;
268  	configASSERT( xEventGroup );
269  	configASSERT( ( uxBitsToClear & eventEVENT_BITS_CONTROL_BYTES ) == 0 );
270  	taskENTER_CRITICAL();
271  	{
272  		traceEVENT_GROUP_CLEAR_BITS( xEventGroup, uxBitsToClear );
273  		uxReturn = pxEventBits->uxEventBits;
274  		pxEventBits->uxEventBits &= ~uxBitsToClear;
275  	}
276  	taskEXIT_CRITICAL();
277  	return uxReturn;
278  }
279  #if ( ( configUSE_TRACE_FACILITY == 1 ) && ( INCLUDE_xTimerPendFunctionCall == 1 ) && ( configUSE_TIMERS == 1 ) )
280  	BaseType_t xEventGroupClearBitsFromISR( EventGroupHandle_t xEventGroup, const EventBits_t uxBitsToClear )
281  	{
282  		BaseType_t xReturn;
283  		traceEVENT_GROUP_CLEAR_BITS_FROM_ISR( xEventGroup, uxBitsToClear );
284  		xReturn = xTimerPendFunctionCallFromISR( vEventGroupClearBitsCallback, ( void * ) xEventGroup, ( uint32_t ) uxBitsToClear, NULL );
285  		return xReturn;
286  	}
287  #endif
288  EventBits_t xEventGroupGetBitsFromISR( EventGroupHandle_t xEventGroup )
289  {
290  UBaseType_t uxSavedInterruptStatus;
291  EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
292  EventBits_t uxReturn;
293  	uxSavedInterruptStatus = portSET_INTERRUPT_MASK_FROM_ISR();
294  	{
295  		uxReturn = pxEventBits->uxEventBits;
296  	}
297  	portCLEAR_INTERRUPT_MASK_FROM_ISR( uxSavedInterruptStatus );
298  	return uxReturn;
299  }
300  EventBits_t xEventGroupSetBits( EventGroupHandle_t xEventGroup, const EventBits_t uxBitsToSet )
301  {
302  ListItem_t *pxListItem, *pxNext;
303  ListItem_t const *pxListEnd;
304  List_t *pxList;
305  EventBits_t uxBitsToClear = 0, uxBitsWaitedFor, uxControlBits;
306  EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
307  BaseType_t xMatchFound = pdFALSE;
308  	configASSERT( xEventGroup );
309  	configASSERT( ( uxBitsToSet & eventEVENT_BITS_CONTROL_BYTES ) == 0 );
310  	pxList = &( pxEventBits->xTasksWaitingForBits );
311  	pxListEnd = listGET_END_MARKER( pxList ); &bsol;*lint !e826 !e740 The mini list structure is used as the list end to save RAM.  This is checked and valid. */
312  	vTaskSuspendAll();
313  	{
314  		traceEVENT_GROUP_SET_BITS( xEventGroup, uxBitsToSet );
315  		pxListItem = listGET_HEAD_ENTRY( pxList );
316  		pxEventBits->uxEventBits |= uxBitsToSet;
317  		while( pxListItem != pxListEnd )
318  		{
319  			pxNext = listGET_NEXT( pxListItem );
320  			uxBitsWaitedFor = listGET_LIST_ITEM_VALUE( pxListItem );
321  			xMatchFound = pdFALSE;
322  			uxControlBits = uxBitsWaitedFor & eventEVENT_BITS_CONTROL_BYTES;
323  			uxBitsWaitedFor &= ~eventEVENT_BITS_CONTROL_BYTES;
324  			if( ( uxControlBits & eventWAIT_FOR_ALL_BITS ) == ( EventBits_t ) 0 )
325  			{
326  				if( ( uxBitsWaitedFor & pxEventBits->uxEventBits ) != ( EventBits_t ) 0 )
327  				{
328  					xMatchFound = pdTRUE;
329  				}
330  				else
331  				{
332  					mtCOVERAGE_TEST_MARKER();
333  				}
334  			}
335  			else if( ( uxBitsWaitedFor & pxEventBits->uxEventBits ) == uxBitsWaitedFor )
336  			{
337  				xMatchFound = pdTRUE;
338  			}
339  			else
340  			{
341  			}
342  			if( xMatchFound != pdFALSE )
343  			{
344  				if( ( uxControlBits & eventCLEAR_EVENTS_ON_EXIT_BIT ) != ( EventBits_t ) 0 )
345  				{
346  					uxBitsToClear |= uxBitsWaitedFor;
347  				}
348  				else
349  				{
350  					mtCOVERAGE_TEST_MARKER();
351  				}
352  				vTaskRemoveFromUnorderedEventList( pxListItem, pxEventBits->uxEventBits | eventUNBLOCKED_DUE_TO_BIT_SET );
353  			}
354  			pxListItem = pxNext;
355  		}
356  		pxEventBits->uxEventBits &= ~uxBitsToClear;
357  	}
358  	( void ) xTaskResumeAll();
359  	return pxEventBits->uxEventBits;
360  }
361  void vEventGroupDelete( EventGroupHandle_t xEventGroup )
362  {
363  EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
364  const List_t *pxTasksWaitingForBits = &( pxEventBits->xTasksWaitingForBits );
365  	vTaskSuspendAll();
366  	{
367  		traceEVENT_GROUP_DELETE( xEventGroup );
368  		while( listCURRENT_LIST_LENGTH( pxTasksWaitingForBits ) > ( UBaseType_t ) 0 )
369  		{
370  			configASSERT( pxTasksWaitingForBits->xListEnd.pxNext != ( const ListItem_t * ) &( pxTasksWaitingForBits->xListEnd ) );
371  			vTaskRemoveFromUnorderedEventList( pxTasksWaitingForBits->xListEnd.pxNext, eventUNBLOCKED_DUE_TO_BIT_SET );
372  		}
373  		#if( ( configSUPPORT_DYNAMIC_ALLOCATION == 1 ) && ( configSUPPORT_STATIC_ALLOCATION == 0 ) )
374  		{
375  			vPortFree( pxEventBits );
376  		}
377  		#elif( ( configSUPPORT_DYNAMIC_ALLOCATION == 1 ) && ( configSUPPORT_STATIC_ALLOCATION == 1 ) )
378  		{
379  			if( pxEventBits->ucStaticallyAllocated == ( uint8_t ) pdFALSE )
380  			{
381  				vPortFree( pxEventBits );
382  			}
383  			else
384  			{
385  				mtCOVERAGE_TEST_MARKER();
386  			}
387  		}
388  		#endif &bsol;* configSUPPORT_DYNAMIC_ALLOCATION */
389  	}
390  	( void ) xTaskResumeAll();
391  }
392  void vEventGroupSetBitsCallback( void *pvEventGroup, const uint32_t ulBitsToSet )
393  {
394  	( void ) xEventGroupSetBits( pvEventGroup, ( EventBits_t ) ulBitsToSet );
395  }
396  void vEventGroupClearBitsCallback( void *pvEventGroup, const uint32_t ulBitsToClear )
397  {
398  	( void ) xEventGroupClearBits( pvEventGroup, ( EventBits_t ) ulBitsToClear );
399  }
400  static BaseType_t prvTestWaitCondition( const EventBits_t uxCurrentEventBits, const EventBits_t uxBitsToWaitFor, const BaseType_t xWaitForAllBits )
401  {
402  BaseType_t xWaitConditionMet = pdFALSE;
403  	if( xWaitForAllBits == pdFALSE )
404  	{
405  		if( ( uxCurrentEventBits & uxBitsToWaitFor ) != ( EventBits_t ) 0 )
406  		{
407  			xWaitConditionMet = pdTRUE;
408  		}
409  		else
410  		{
411  			mtCOVERAGE_TEST_MARKER();
412  		}
413  	}
414  	else
415  	{
416  		if( ( uxCurrentEventBits & uxBitsToWaitFor ) == uxBitsToWaitFor )
417  		{
418  			xWaitConditionMet = pdTRUE;
419  		}
420  		else
421  		{
422  			mtCOVERAGE_TEST_MARKER();
423  		}
424  	}
425  	return xWaitConditionMet;
426  }
427  #if ( ( configUSE_TRACE_FACILITY == 1 ) && ( INCLUDE_xTimerPendFunctionCall == 1 ) && ( configUSE_TIMERS == 1 ) )
428  	BaseType_t xEventGroupSetBitsFromISR( EventGroupHandle_t xEventGroup, const EventBits_t uxBitsToSet, BaseType_t *pxHigherPriorityTaskWoken )
429  	{
430  	BaseType_t xReturn;
431  		traceEVENT_GROUP_SET_BITS_FROM_ISR( xEventGroup, uxBitsToSet );
432  		xReturn = xTimerPendFunctionCallFromISR( vEventGroupSetBitsCallback, ( void * ) xEventGroup, ( uint32_t ) uxBitsToSet, pxHigherPriorityTaskWoken );
433  		return xReturn;
434  	}
435  #endif
436  #if (configUSE_TRACE_FACILITY == 1)
437  	UBaseType_t uxEventGroupGetNumber( void* xEventGroup )
438  	{
439  	UBaseType_t xReturn;
440  	EventGroup_t *pxEventBits = ( EventGroup_t * ) xEventGroup;
441  		if( xEventGroup == NULL )
442  		{
443  			xReturn = 0;
444  		}
445  		else
446  		{
447  			xReturn = pxEventBits->uxEventGroupNumber;
448  		}
449  		return xReturn;
450  	}
451  #endif &bsol;* configUSE_TRACE_FACILITY */
452  #if ( configUSE_TRACE_FACILITY == 1 )
453  	void vEventGroupSetNumber( void * xEventGroup, UBaseType_t uxEventGroupNumber )
454  	{
455  		( ( EventGroup_t * ) xEventGroup )->uxEventGroupNumber = uxEventGroupNumber;
456  	}
457  #endif &bsol;* configUSE_TRACE_FACILITY */
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-prof.c</h3>
            <pre><code>1  #define JEMALLOC_PROF_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/ckh.h"
6  #include "jemalloc/internal/hash.h"
7  #include "jemalloc/internal/malloc_io.h"
8  #include "jemalloc/internal/mutex.h"
9  #include "jemalloc/internal/emitter.h"
10  #ifdef JEMALLOC_PROF_LIBUNWIND
11  #define UNW_LOCAL_ONLY
12  #include <libunwind.h>
13  #endif
14  #ifdef JEMALLOC_PROF_LIBGCC
15  #undef _Unwind_Backtrace
16  #include <unwind.h>
17  #define _Unwind_Backtrace JEMALLOC_HOOK(_Unwind_Backtrace, test_hooks_libc_hook)
18  #endif
19  bool		opt_prof = false;
20  bool		opt_prof_active = true;
21  bool		opt_prof_thread_active_init = true;
22  size_t		opt_lg_prof_sample = LG_PROF_SAMPLE_DEFAULT;
23  ssize_t		opt_lg_prof_interval = LG_PROF_INTERVAL_DEFAULT;
24  bool		opt_prof_gdump = false;
25  bool		opt_prof_final = false;
26  bool		opt_prof_leak = false;
27  bool		opt_prof_accum = false;
28  bool		opt_prof_log = false;
29  char		opt_prof_prefix[
30  #ifdef JEMALLOC_PROF
31      PATH_MAX +
32  #endif
33      1];
34  bool			prof_active;
35  static malloc_mutex_t	prof_active_mtx;
36  static bool		prof_thread_active_init;
37  static malloc_mutex_t	prof_thread_active_init_mtx;
38  bool			prof_gdump_val;
39  static malloc_mutex_t	prof_gdump_mtx;
40  uint64_t	prof_interval = 0;
41  size_t		lg_prof_sample;
42  typedef enum prof_logging_state_e prof_logging_state_t;
43  enum prof_logging_state_e {
44  	prof_logging_state_stopped,
45  	prof_logging_state_started,
46  	prof_logging_state_dumping
47  };
48  prof_logging_state_t prof_logging_state = prof_logging_state_stopped;
49  #ifdef JEMALLOC_JET
50  static bool prof_log_dummy = false;
51  #endif
52  static uint64_t log_seq = 0;
53  static char log_filename[
54  #ifdef JEMALLOC_PROF
55      PATH_MAX +
56  #endif
57      1];
58  static nstime_t log_start_timestamp = NSTIME_ZERO_INITIALIZER;
59  static size_t log_bt_index = 0;
60  static size_t log_thr_index = 0;
61  typedef struct prof_bt_node_s prof_bt_node_t;
62  struct prof_bt_node_s {
63  	prof_bt_node_t *next;
64  	size_t index;
65  	prof_bt_t bt;
66  	void *vec[1];
67  };
68  typedef struct prof_thr_node_s prof_thr_node_t;
69  struct prof_thr_node_s {
70  	prof_thr_node_t *next;
71  	size_t index;
72  	uint64_t thr_uid;
73  	char name[1];
74  };
75  typedef struct prof_alloc_node_s prof_alloc_node_t;
76  struct prof_alloc_node_s {
77  	prof_alloc_node_t *next;
78  	size_t alloc_thr_ind;
79  	size_t free_thr_ind;
80  	size_t alloc_bt_ind;
81  	size_t free_bt_ind;
82  	uint64_t alloc_time_ns;
83  	uint64_t free_time_ns;
84  	size_t usize;
85  };
86  static bool log_tables_initialized = false;
87  static ckh_t log_bt_node_set;
88  static ckh_t log_thr_node_set;
89  static prof_bt_node_t *log_bt_first = NULL;
90  static prof_bt_node_t *log_bt_last = NULL;
91  static prof_thr_node_t *log_thr_first = NULL;
92  static prof_thr_node_t *log_thr_last = NULL;
93  static prof_alloc_node_t *log_alloc_first = NULL;
94  static prof_alloc_node_t *log_alloc_last = NULL;
95  static malloc_mutex_t log_mtx;
96  static malloc_mutex_t	*gctx_locks;
97  static atomic_u_t	cum_gctxs; &bsol;* Atomic counter. */
98  static malloc_mutex_t	*tdata_locks;
99  static ckh_t		bt2gctx;
100  malloc_mutex_t		bt2gctx_mtx;
101  static prof_tdata_tree_t	tdatas;
102  static malloc_mutex_t	tdatas_mtx;
103  static uint64_t		next_thr_uid;
104  static malloc_mutex_t	next_thr_uid_mtx;
105  static malloc_mutex_t	prof_dump_seq_mtx;
106  static uint64_t		prof_dump_seq;
107  static uint64_t		prof_dump_iseq;
108  static uint64_t		prof_dump_mseq;
109  static uint64_t		prof_dump_useq;
110  static malloc_mutex_t	prof_dump_mtx;
111  static char		prof_dump_buf[
112  #ifdef JEMALLOC_PROF
113      PROF_DUMP_BUFSIZE
114  #else
115      1
116  #endif
117  ];
118  static size_t		prof_dump_buf_end;
119  static int		prof_dump_fd;
120  static bool		prof_booted = false;
121  static bool	prof_tctx_should_destroy(tsdn_t *tsdn, prof_tctx_t *tctx);
122  static void	prof_tctx_destroy(tsd_t *tsd, prof_tctx_t *tctx);
123  static bool	prof_tdata_should_destroy(tsdn_t *tsdn, prof_tdata_t *tdata,
124      bool even_if_attached);
125  static void	prof_tdata_destroy(tsd_t *tsd, prof_tdata_t *tdata,
126      bool even_if_attached);
127  static char	*prof_thread_name_alloc(tsdn_t *tsdn, const char *thread_name);
128  static void prof_thr_node_hash(const void *key, size_t r_hash[2]);
129  static bool prof_thr_node_keycomp(const void *k1, const void *k2);
130  static void prof_bt_node_hash(const void *key, size_t r_hash[2]);
131  static bool prof_bt_node_keycomp(const void *k1, const void *k2);
132  static int
133  prof_tctx_comp(const prof_tctx_t *a, const prof_tctx_t *b) {
134  	uint64_t a_thr_uid = a->thr_uid;
135  	uint64_t b_thr_uid = b->thr_uid;
136  	int ret = (a_thr_uid > b_thr_uid) - (a_thr_uid < b_thr_uid);
137  	if (ret == 0) {
138  		uint64_t a_thr_discrim = a->thr_discrim;
139  		uint64_t b_thr_discrim = b->thr_discrim;
140  		ret = (a_thr_discrim > b_thr_discrim) - (a_thr_discrim <
141  		    b_thr_discrim);
142  		if (ret == 0) {
143  			uint64_t a_tctx_uid = a->tctx_uid;
144  			uint64_t b_tctx_uid = b->tctx_uid;
145  			ret = (a_tctx_uid > b_tctx_uid) - (a_tctx_uid <
146  			    b_tctx_uid);
147  		}
148  	}
149  	return ret;
150  }
151  rb_gen(static UNUSED, tctx_tree_, prof_tctx_tree_t, prof_tctx_t,
152      tctx_link, prof_tctx_comp)
153  static int
154  prof_gctx_comp(const prof_gctx_t *a, const prof_gctx_t *b) {
155  	unsigned a_len = a->bt.len;
156  	unsigned b_len = b->bt.len;
157  	unsigned comp_len = (a_len < b_len) ? a_len : b_len;
158  	int ret = memcmp(a->bt.vec, b->bt.vec, comp_len * sizeof(void *));
159  	if (ret == 0) {
160  		ret = (a_len > b_len) - (a_len < b_len);
161  	}
162  	return ret;
163  }
164  rb_gen(static UNUSED, gctx_tree_, prof_gctx_tree_t, prof_gctx_t, dump_link,
165      prof_gctx_comp)
166  static int
167  prof_tdata_comp(const prof_tdata_t *a, const prof_tdata_t *b) {
168  	int ret;
169  	uint64_t a_uid = a->thr_uid;
170  	uint64_t b_uid = b->thr_uid;
171  	ret = ((a_uid > b_uid) - (a_uid < b_uid));
172  	if (ret == 0) {
173  		uint64_t a_discrim = a->thr_discrim;
174  		uint64_t b_discrim = b->thr_discrim;
175  		ret = ((a_discrim > b_discrim) - (a_discrim < b_discrim));
176  	}
177  	return ret;
178  }
179  rb_gen(static UNUSED, tdata_tree_, prof_tdata_tree_t, prof_tdata_t, tdata_link,
180      prof_tdata_comp)
181  void
182  prof_alloc_rollback(tsd_t *tsd, prof_tctx_t *tctx, bool updated) {
183  	prof_tdata_t *tdata;
184  	cassert(config_prof);
185  	if (updated) {
186  		tdata = prof_tdata_get(tsd, true);
187  		if (tdata != NULL) {
188  			prof_sample_threshold_update(tdata);
189  		}
190  	}
191  	if ((uintptr_t)tctx > (uintptr_t)1U) {
192  		malloc_mutex_lock(tsd_tsdn(tsd), tctx->tdata->lock);
193  		tctx->prepared = false;
194  		if (prof_tctx_should_destroy(tsd_tsdn(tsd), tctx)) {
195  			prof_tctx_destroy(tsd, tctx);
196  		} else {
197  			malloc_mutex_unlock(tsd_tsdn(tsd), tctx->tdata->lock);
198  		}
199  	}
200  }
201  void
202  prof_malloc_sample_object(tsdn_t *tsdn, const void *ptr, size_t usize,
203      prof_tctx_t *tctx) {
204  	prof_tctx_set(tsdn, ptr, usize, NULL, tctx);
205  	nstime_t t = NSTIME_ZERO_INITIALIZER;
206  	nstime_update(&t);
207  	prof_alloc_time_set(tsdn, ptr, NULL, t);
208  	malloc_mutex_lock(tsdn, tctx->tdata->lock);
209  	tctx->cnts.curobjs++;
210  	tctx->cnts.curbytes += usize;
211  	if (opt_prof_accum) {
212  		tctx->cnts.accumobjs++;
213  		tctx->cnts.accumbytes += usize;
214  	}
215  	tctx->prepared = false;
216  	malloc_mutex_unlock(tsdn, tctx->tdata->lock);
217  }
218  static size_t
219  prof_log_bt_index(tsd_t *tsd, prof_bt_t *bt) {
220  	assert(prof_logging_state == prof_logging_state_started);
221  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &log_mtx);
222  	prof_bt_node_t dummy_node;
223  	dummy_node.bt = *bt;
224  	prof_bt_node_t *node;
225  	if (ckh_search(&log_bt_node_set, (void *)(&dummy_node),
226  	    (void **)(&node), NULL)) {
227  		size_t sz = offsetof(prof_bt_node_t, vec) +
228  			        (bt->len * sizeof(void *));
229  		prof_bt_node_t *new_node = (prof_bt_node_t *)
230  		    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL,
231  		    true, arena_get(TSDN_NULL, 0, true), true);
232  		if (log_bt_first == NULL) {
233  			log_bt_first = new_node;
234  			log_bt_last = new_node;
235  		} else {
236  			log_bt_last->next = new_node;
237  			log_bt_last = new_node;
238  		}
239  		new_node->next = NULL;
240  		new_node->index = log_bt_index;
241  		new_node->bt.len = bt->len;
242  		memcpy(new_node->vec, bt->vec, bt->len * sizeof(void *));
243  		new_node->bt.vec = new_node->vec;
244  		log_bt_index++;
245  		ckh_insert(tsd, &log_bt_node_set, (void *)new_node, NULL);
246  		return new_node->index;
247  	} else {
248  		return node->index;
249  	}
250  }
251  static size_t
252  prof_log_thr_index(tsd_t *tsd, uint64_t thr_uid, const char *name) {
253  	assert(prof_logging_state == prof_logging_state_started);
254  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &log_mtx);
255  	prof_thr_node_t dummy_node;
256  	dummy_node.thr_uid = thr_uid;
257  	prof_thr_node_t *node;
258  	if (ckh_search(&log_thr_node_set, (void *)(&dummy_node),
259  	    (void **)(&node), NULL)) {
260  		size_t sz = offsetof(prof_thr_node_t, name) + strlen(name) + 1;
261  		prof_thr_node_t *new_node = (prof_thr_node_t *)
262  		    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL,
263  		    true, arena_get(TSDN_NULL, 0, true), true);
264  		if (log_thr_first == NULL) {
265  			log_thr_first = new_node;
266  			log_thr_last = new_node;
267  		} else {
268  			log_thr_last->next = new_node;
269  			log_thr_last = new_node;
270  		}
271  		new_node->next = NULL;
272  		new_node->index = log_thr_index;
273  		new_node->thr_uid = thr_uid;
274  		strcpy(new_node->name, name);
275  		log_thr_index++;
276  		ckh_insert(tsd, &log_thr_node_set, (void *)new_node, NULL);
277  		return new_node->index;
278  	} else {
279  		return node->index;
280  	}
281  }
282  static void
283  prof_try_log(tsd_t *tsd, const void *ptr, size_t usize, prof_tctx_t *tctx) {
284  	malloc_mutex_assert_owner(tsd_tsdn(tsd), tctx->tdata->lock);
285  	prof_tdata_t *cons_tdata = prof_tdata_get(tsd, false);
286  	if (cons_tdata == NULL) {
287  		return;
288  	}
289  	malloc_mutex_lock(tsd_tsdn(tsd), &log_mtx);
290  	if (prof_logging_state != prof_logging_state_started) {
291  		goto label_done;
292  	}
293  	if (!log_tables_initialized) {
294  		bool err1 = ckh_new(tsd, &log_bt_node_set, PROF_CKH_MINITEMS,
295  				prof_bt_node_hash, prof_bt_node_keycomp);
296  		bool err2 = ckh_new(tsd, &log_thr_node_set, PROF_CKH_MINITEMS,
297  				prof_thr_node_hash, prof_thr_node_keycomp);
298  		if (err1 || err2) {
299  			goto label_done;
300  		}
301  		log_tables_initialized = true;
302  	}
303  	nstime_t alloc_time = prof_alloc_time_get(tsd_tsdn(tsd), ptr,
304  			          (alloc_ctx_t *)NULL);
305  	nstime_t free_time = NSTIME_ZERO_INITIALIZER;
306  	nstime_update(&free_time);
307  	size_t sz = sizeof(prof_alloc_node_t);
308  	prof_alloc_node_t *new_node = (prof_alloc_node_t *)
309  	    iallocztm(tsd_tsdn(tsd), sz, sz_size2index(sz), false, NULL, true,
310  	    arena_get(TSDN_NULL, 0, true), true);
311  	const char *prod_thr_name = (tctx->tdata->thread_name == NULL)?
312  				        "" : tctx->tdata->thread_name;
313  	const char *cons_thr_name = prof_thread_name_get(tsd);
314  	prof_bt_t bt;
315  	bt_init(&bt, cons_tdata->vec);
316  	prof_backtrace(&bt);
317  	prof_bt_t *cons_bt = &bt;
318  	prof_bt_t *prod_bt = &tctx->gctx->bt;
319  	new_node->next = NULL;
320  	new_node->alloc_thr_ind = prof_log_thr_index(tsd, tctx->tdata->thr_uid,
321  				      prod_thr_name);
322  	new_node->free_thr_ind = prof_log_thr_index(tsd, cons_tdata->thr_uid,
323  				     cons_thr_name);
324  	new_node->alloc_bt_ind = prof_log_bt_index(tsd, prod_bt);
325  	new_node->free_bt_ind = prof_log_bt_index(tsd, cons_bt);
326  	new_node->alloc_time_ns = nstime_ns(&alloc_time);
327  	new_node->free_time_ns = nstime_ns(&free_time);
328  	new_node->usize = usize;
329  	if (log_alloc_first == NULL) {
330  		log_alloc_first = new_node;
331  		log_alloc_last = new_node;
332  	} else {
333  		log_alloc_last->next = new_node;
334  		log_alloc_last = new_node;
335  	}
336  label_done:
337  	malloc_mutex_unlock(tsd_tsdn(tsd), &log_mtx);
338  }
339  void
340  prof_free_sampled_object(tsd_t *tsd, const void *ptr, size_t usize,
341      prof_tctx_t *tctx) {
342  	malloc_mutex_lock(tsd_tsdn(tsd), tctx->tdata->lock);
343  	assert(tctx->cnts.curobjs > 0);
344  	assert(tctx->cnts.curbytes >= usize);
345  	tctx->cnts.curobjs--;
346  	tctx->cnts.curbytes -= usize;
347  	prof_try_log(tsd, ptr, usize, tctx);
348  	if (prof_tctx_should_destroy(tsd_tsdn(tsd), tctx)) {
349  		prof_tctx_destroy(tsd, tctx);
350  	} else {
351  		malloc_mutex_unlock(tsd_tsdn(tsd), tctx->tdata->lock);
352  	}
353  }
354  void
355  bt_init(prof_bt_t *bt, void **vec) {
356  	cassert(config_prof);
357  	bt->vec = vec;
358  	bt->len = 0;
359  }
360  static void
361  prof_enter(tsd_t *tsd, prof_tdata_t *tdata) {
362  	cassert(config_prof);
363  	assert(tdata == prof_tdata_get(tsd, false));
364  	if (tdata != NULL) {
365  		assert(!tdata->enq);
366  		tdata->enq = true;
367  	}
368  	malloc_mutex_lock(tsd_tsdn(tsd), &bt2gctx_mtx);
369  }
370  static void
371  prof_leave(tsd_t *tsd, prof_tdata_t *tdata) {
372  	cassert(config_prof);
373  	assert(tdata == prof_tdata_get(tsd, false));
374  	malloc_mutex_unlock(tsd_tsdn(tsd), &bt2gctx_mtx);
375  	if (tdata != NULL) {
376  		bool idump, gdump;
377  		assert(tdata->enq);
378  		tdata->enq = false;
379  		idump = tdata->enq_idump;
380  		tdata->enq_idump = false;
381  		gdump = tdata->enq_gdump;
382  		tdata->enq_gdump = false;
383  		if (idump) {
384  			prof_idump(tsd_tsdn(tsd));
385  		}
386  		if (gdump) {
387  			prof_gdump(tsd_tsdn(tsd));
388  		}
389  	}
390  }
391  #ifdef JEMALLOC_PROF_LIBUNWIND
392  void
393  prof_backtrace(prof_bt_t *bt) {
394  	int nframes;
395  	cassert(config_prof);
396  	assert(bt->len == 0);
397  	assert(bt->vec != NULL);
398  	nframes = unw_backtrace(bt->vec, PROF_BT_MAX);
399  	if (nframes <= 0) {
400  		return;
401  	}
402  	bt->len = nframes;
403  }
404  #elif (defined(JEMALLOC_PROF_LIBGCC))
405  static _Unwind_Reason_Code
406  prof_unwind_init_callback(struct _Unwind_Context *context, void *arg) {
407  	cassert(config_prof);
408  	return _URC_NO_REASON;
409  }
410  static _Unwind_Reason_Code
411  prof_unwind_callback(struct _Unwind_Context *context, void *arg) {
412  	prof_unwind_data_t *data = (prof_unwind_data_t *)arg;
413  	void *ip;
414  	cassert(config_prof);
415  	ip = (void *)_Unwind_GetIP(context);
416  	if (ip == NULL) {
417  		return _URC_END_OF_STACK;
418  	}
419  	data->bt->vec[data->bt->len] = ip;
420  	data->bt->len++;
421  	if (data->bt->len == data->max) {
422  		return _URC_END_OF_STACK;
423  	}
424  	return _URC_NO_REASON;
425  }
426  void
427  prof_backtrace(prof_bt_t *bt) {
428  	prof_unwind_data_t data = {bt, PROF_BT_MAX};
429  	cassert(config_prof);
430  	_Unwind_Backtrace(prof_unwind_callback, &data);
431  }
432  #elif (defined(JEMALLOC_PROF_GCC))
433  void
434  prof_backtrace(prof_bt_t *bt) {
435  #define BT_FRAME(i)							\
436  	if ((i) < PROF_BT_MAX) {					\
437  		void *p;						\
438  		if (__builtin_frame_address(i) == 0) {			\
439  			return;						\
440  		}							\
441  		p = __builtin_return_address(i);			\
442  		if (p == NULL) {					\
443  			return;						\
444  		}							\
445  		bt->vec[(i)] = p;					\
446  		bt->len = (i) + 1;					\
447  	} else {							\
448  		return;							\
449  	}
450  	cassert(config_prof);
451  	BT_FRAME(0)
452  	BT_FRAME(1)
453  	BT_FRAME(2)
454  	BT_FRAME(3)
455  	BT_FRAME(4)
456  	BT_FRAME(5)
457  	BT_FRAME(6)
458  	BT_FRAME(7)
459  	BT_FRAME(8)
460  	BT_FRAME(9)
461  	BT_FRAME(10)
462  	BT_FRAME(11)
463  	BT_FRAME(12)
464  	BT_FRAME(13)
465  	BT_FRAME(14)
466  	BT_FRAME(15)
467  	BT_FRAME(16)
468  	BT_FRAME(17)
469  	BT_FRAME(18)
470  	BT_FRAME(19)
471  	BT_FRAME(20)
472  	BT_FRAME(21)
473  	BT_FRAME(22)
474  	BT_FRAME(23)
475  	BT_FRAME(24)
476  	BT_FRAME(25)
477  	BT_FRAME(26)
478  	BT_FRAME(27)
479  	BT_FRAME(28)
480  	BT_FRAME(29)
481  	BT_FRAME(30)
482  	BT_FRAME(31)
483  	BT_FRAME(32)
484  	BT_FRAME(33)
485  	BT_FRAME(34)
486  	BT_FRAME(35)
487  	BT_FRAME(36)
488  	BT_FRAME(37)
489  	BT_FRAME(38)
490  	BT_FRAME(39)
491  	BT_FRAME(40)
492  	BT_FRAME(41)
493  	BT_FRAME(42)
494  	BT_FRAME(43)
495  	BT_FRAME(44)
496  	BT_FRAME(45)
497  	BT_FRAME(46)
498  	BT_FRAME(47)
499  	BT_FRAME(48)
500  	BT_FRAME(49)
501  	BT_FRAME(50)
502  	BT_FRAME(51)
503  	BT_FRAME(52)
504  	BT_FRAME(53)
505  	BT_FRAME(54)
506  	BT_FRAME(55)
507  	BT_FRAME(56)
508  	BT_FRAME(57)
509  	BT_FRAME(58)
510  	BT_FRAME(59)
511  	BT_FRAME(60)
512  	BT_FRAME(61)
513  	BT_FRAME(62)
514  	BT_FRAME(63)
515  	BT_FRAME(64)
516  	BT_FRAME(65)
517  	BT_FRAME(66)
518  	BT_FRAME(67)
519  	BT_FRAME(68)
520  	BT_FRAME(69)
521  	BT_FRAME(70)
522  	BT_FRAME(71)
523  	BT_FRAME(72)
524  	BT_FRAME(73)
525  	BT_FRAME(74)
526  	BT_FRAME(75)
527  	BT_FRAME(76)
528  	BT_FRAME(77)
529  	BT_FRAME(78)
530  	BT_FRAME(79)
531  	BT_FRAME(80)
532  	BT_FRAME(81)
533  	BT_FRAME(82)
534  	BT_FRAME(83)
535  	BT_FRAME(84)
536  	BT_FRAME(85)
537  	BT_FRAME(86)
538  	BT_FRAME(87)
539  	BT_FRAME(88)
540  	BT_FRAME(89)
541  	BT_FRAME(90)
542  	BT_FRAME(91)
543  	BT_FRAME(92)
544  	BT_FRAME(93)
545  	BT_FRAME(94)
546  	BT_FRAME(95)
547  	BT_FRAME(96)
548  	BT_FRAME(97)
549  	BT_FRAME(98)
550  	BT_FRAME(99)
551  	BT_FRAME(100)
552  	BT_FRAME(101)
553  	BT_FRAME(102)
554  	BT_FRAME(103)
555  	BT_FRAME(104)
556  	BT_FRAME(105)
557  	BT_FRAME(106)
558  	BT_FRAME(107)
559  	BT_FRAME(108)
560  	BT_FRAME(109)
561  	BT_FRAME(110)
562  	BT_FRAME(111)
563  	BT_FRAME(112)
564  	BT_FRAME(113)
565  	BT_FRAME(114)
566  	BT_FRAME(115)
567  	BT_FRAME(116)
568  	BT_FRAME(117)
569  	BT_FRAME(118)
570  	BT_FRAME(119)
571  	BT_FRAME(120)
572  	BT_FRAME(121)
573  	BT_FRAME(122)
574  	BT_FRAME(123)
575  	BT_FRAME(124)
576  	BT_FRAME(125)
577  	BT_FRAME(126)
578  	BT_FRAME(127)
579  #undef BT_FRAME
580  }
581  #else
582  void
583  prof_backtrace(prof_bt_t *bt) {
584  	cassert(config_prof);
585  	not_reached();
586  }
587  #endif
588  static malloc_mutex_t *
589  prof_gctx_mutex_choose(void) {
590  	unsigned ngctxs = atomic_fetch_add_u(&cum_gctxs, 1, ATOMIC_RELAXED);
591  	return &gctx_locks[(ngctxs - 1) % PROF_NCTX_LOCKS];
592  }
593  static malloc_mutex_t *
594  prof_tdata_mutex_choose(uint64_t thr_uid) {
595  	return &tdata_locks[thr_uid % PROF_NTDATA_LOCKS];
596  }
597  static prof_gctx_t *
598  prof_gctx_create(tsdn_t *tsdn, prof_bt_t *bt) {
599  	size_t size = offsetof(prof_gctx_t, vec) + (bt->len * sizeof(void *));
600  	prof_gctx_t *gctx = (prof_gctx_t *)iallocztm(tsdn, size,
601  	    sz_size2index(size), false, NULL, true, arena_get(TSDN_NULL, 0, true),
602  	    true);
603  	if (gctx == NULL) {
604  		return NULL;
605  	}
606  	gctx->lock = prof_gctx_mutex_choose();
607  	gctx->nlimbo = 1;
608  	tctx_tree_new(&gctx->tctxs);
609  	memcpy(gctx->vec, bt->vec, bt->len * sizeof(void *));
610  	gctx->bt.vec = gctx->vec;
611  	gctx->bt.len = bt->len;
612  	return gctx;
613  }
614  static void
615  prof_gctx_try_destroy(tsd_t *tsd, prof_tdata_t *tdata_self, prof_gctx_t *gctx,
616      prof_tdata_t *tdata) {
617  	cassert(config_prof);
618  	prof_enter(tsd, tdata_self);
619  	malloc_mutex_lock(tsd_tsdn(tsd), gctx->lock);
620  	assert(gctx->nlimbo != 0);
621  	if (tctx_tree_empty(&gctx->tctxs) && gctx->nlimbo == 1) {
622  		if (ckh_remove(tsd, &bt2gctx, &gctx->bt, NULL, NULL)) {
623  			not_reached();
624  		}
625  		prof_leave(tsd, tdata_self);
626  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
627  		idalloctm(tsd_tsdn(tsd), gctx, NULL, NULL, true, true);
628  	} else {
629  		gctx->nlimbo--;
630  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
631  		prof_leave(tsd, tdata_self);
632  	}
633  }
634  static bool
635  prof_tctx_should_destroy(tsdn_t *tsdn, prof_tctx_t *tctx) {
636  	malloc_mutex_assert_owner(tsdn, tctx->tdata->lock);
637  	if (opt_prof_accum) {
638  		return false;
639  	}
640  	if (tctx->cnts.curobjs != 0) {
641  		return false;
642  	}
643  	if (tctx->prepared) {
644  		return false;
645  	}
646  	return true;
647  }
648  static bool
649  prof_gctx_should_destroy(prof_gctx_t *gctx) {
650  	if (opt_prof_accum) {
651  		return false;
652  	}
653  	if (!tctx_tree_empty(&gctx->tctxs)) {
654  		return false;
655  	}
656  	if (gctx->nlimbo != 0) {
657  		return false;
658  	}
659  	return true;
660  }
661  static void
662  prof_tctx_destroy(tsd_t *tsd, prof_tctx_t *tctx) {
663  	prof_tdata_t *tdata = tctx->tdata;
664  	prof_gctx_t *gctx = tctx->gctx;
665  	bool destroy_tdata, destroy_tctx, destroy_gctx;
666  	malloc_mutex_assert_owner(tsd_tsdn(tsd), tctx->tdata->lock);
667  	assert(tctx->cnts.curobjs == 0);
668  	assert(tctx->cnts.curbytes == 0);
669  	assert(!opt_prof_accum);
670  	assert(tctx->cnts.accumobjs == 0);
671  	assert(tctx->cnts.accumbytes == 0);
672  	ckh_remove(tsd, &tdata->bt2tctx, &gctx->bt, NULL, NULL);
673  	destroy_tdata = prof_tdata_should_destroy(tsd_tsdn(tsd), tdata, false);
674  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata->lock);
675  	malloc_mutex_lock(tsd_tsdn(tsd), gctx->lock);
676  	switch (tctx->state) {
677  	case prof_tctx_state_nominal:
678  		tctx_tree_remove(&gctx->tctxs, tctx);
679  		destroy_tctx = true;
680  		if (prof_gctx_should_destroy(gctx)) {
681  			gctx->nlimbo++;
682  			destroy_gctx = true;
683  		} else {
684  			destroy_gctx = false;
685  		}
686  		break;
687  	case prof_tctx_state_dumping:
688  		tctx->state = prof_tctx_state_purgatory;
689  		destroy_tctx = false;
690  		destroy_gctx = false;
691  		break;
692  	default:
693  		not_reached();
694  		destroy_tctx = false;
695  		destroy_gctx = false;
696  	}
697  	malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
698  	if (destroy_gctx) {
699  		prof_gctx_try_destroy(tsd, prof_tdata_get(tsd, false), gctx,
700  		    tdata);
701  	}
702  	malloc_mutex_assert_not_owner(tsd_tsdn(tsd), tctx->tdata->lock);
703  	if (destroy_tdata) {
704  		prof_tdata_destroy(tsd, tdata, false);
705  	}
706  	if (destroy_tctx) {
707  		idalloctm(tsd_tsdn(tsd), tctx, NULL, NULL, true, true);
708  	}
709  }
710  static bool
711  prof_lookup_global(tsd_t *tsd, prof_bt_t *bt, prof_tdata_t *tdata,
712      void **p_btkey, prof_gctx_t **p_gctx, bool *p_new_gctx) {
713  	union {
714  		prof_gctx_t	*p;
715  		void		*v;
716  	} gctx, tgctx;
717  	union {
718  		prof_bt_t	*p;
719  		void		*v;
720  	} btkey;
721  	bool new_gctx;
722  	prof_enter(tsd, tdata);
723  	if (ckh_search(&bt2gctx, bt, &btkey.v, &gctx.v)) {
724  		prof_leave(tsd, tdata);
725  		tgctx.p = prof_gctx_create(tsd_tsdn(tsd), bt);
726  		if (tgctx.v == NULL) {
727  			return true;
728  		}
729  		prof_enter(tsd, tdata);
730  		if (ckh_search(&bt2gctx, bt, &btkey.v, &gctx.v)) {
731  			gctx.p = tgctx.p;
732  			btkey.p = &gctx.p->bt;
733  			if (ckh_insert(tsd, &bt2gctx, btkey.v, gctx.v)) {
734  				prof_leave(tsd, tdata);
735  				idalloctm(tsd_tsdn(tsd), gctx.v, NULL, NULL,
736  				    true, true);
737  				return true;
738  			}
739  			new_gctx = true;
740  		} else {
741  			new_gctx = false;
742  		}
743  	} else {
744  		tgctx.v = NULL;
745  		new_gctx = false;
746  	}
747  	if (!new_gctx) {
748  		malloc_mutex_lock(tsd_tsdn(tsd), gctx.p->lock);
749  		gctx.p->nlimbo++;
750  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx.p->lock);
751  		new_gctx = false;
752  		if (tgctx.v != NULL) {
753  			idalloctm(tsd_tsdn(tsd), tgctx.v, NULL, NULL, true,
754  			    true);
755  		}
756  	}
757  	prof_leave(tsd, tdata);
758  	*p_btkey = btkey.v;
759  	*p_gctx = gctx.p;
760  	*p_new_gctx = new_gctx;
761  	return false;
762  }
763  prof_tctx_t *
764  prof_lookup(tsd_t *tsd, prof_bt_t *bt) {
765  	union {
766  		prof_tctx_t	*p;
767  		void		*v;
768  	} ret;
769  	prof_tdata_t *tdata;
770  	bool not_found;
771  	cassert(config_prof);
772  	tdata = prof_tdata_get(tsd, false);
773  	if (tdata == NULL) {
774  		return NULL;
775  	}
776  	malloc_mutex_lock(tsd_tsdn(tsd), tdata->lock);
777  	not_found = ckh_search(&tdata->bt2tctx, bt, NULL, &ret.v);
778  	if (!not_found) { &bsol;* Note double negative! */
779  		ret.p->prepared = true;
780  	}
781  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata->lock);
782  	if (not_found) {
783  		void *btkey;
784  		prof_gctx_t *gctx;
785  		bool new_gctx, error;
786  		if (prof_lookup_global(tsd, bt, tdata, &btkey, &gctx,
787  		    &new_gctx)) {
788  			return NULL;
789  		}
790  		ret.v = iallocztm(tsd_tsdn(tsd), sizeof(prof_tctx_t),
791  		    sz_size2index(sizeof(prof_tctx_t)), false, NULL, true,
792  		    arena_ichoose(tsd, NULL), true);
793  		if (ret.p == NULL) {
794  			if (new_gctx) {
795  				prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
796  			}
797  			return NULL;
798  		}
799  		ret.p->tdata = tdata;
800  		ret.p->thr_uid = tdata->thr_uid;
801  		ret.p->thr_discrim = tdata->thr_discrim;
802  		memset(&ret.p->cnts, 0, sizeof(prof_cnt_t));
803  		ret.p->gctx = gctx;
804  		ret.p->tctx_uid = tdata->tctx_uid_next++;
805  		ret.p->prepared = true;
806  		ret.p->state = prof_tctx_state_initializing;
807  		malloc_mutex_lock(tsd_tsdn(tsd), tdata->lock);
808  		error = ckh_insert(tsd, &tdata->bt2tctx, btkey, ret.v);
809  		malloc_mutex_unlock(tsd_tsdn(tsd), tdata->lock);
810  		if (error) {
811  			if (new_gctx) {
812  				prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
813  			}
814  			idalloctm(tsd_tsdn(tsd), ret.v, NULL, NULL, true, true);
815  			return NULL;
816  		}
817  		malloc_mutex_lock(tsd_tsdn(tsd), gctx->lock);
818  		ret.p->state = prof_tctx_state_nominal;
819  		tctx_tree_insert(&gctx->tctxs, ret.p);
820  		gctx->nlimbo--;
821  		malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
822  	}
823  	return ret.p;
824  }
825  void
826  prof_sample_threshold_update(prof_tdata_t *tdata) {
827  #ifdef JEMALLOC_PROF
828  	if (!config_prof) {
829  		return;
830  	}
831  	if (lg_prof_sample == 0) {
832  		tsd_bytes_until_sample_set(tsd_fetch(), 0);
833  		return;
834  	}
835  	uint64_t r = prng_lg_range_u64(&tdata->prng_state, 53);
836  	double u = (double)r * (1.0/9007199254740992.0L);
837  	uint64_t bytes_until_sample = (uint64_t)(log(u) /
838  	    log(1.0 - (1.0 / (double)((uint64_t)1U << lg_prof_sample))))
839  	    + (uint64_t)1U;
840  	if (bytes_until_sample > SSIZE_MAX) {
841  		bytes_until_sample = SSIZE_MAX;
842  	}
843  	tsd_bytes_until_sample_set(tsd_fetch(), bytes_until_sample);
844  #endif
845  }
846  #ifdef JEMALLOC_JET
847  static prof_tdata_t *
848  prof_tdata_count_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
849      void *arg) {
850  	size_t *tdata_count = (size_t *)arg;
851  	(*tdata_count)++;
852  	return NULL;
853  }
854  size_t
855  prof_tdata_count(void) {
856  	size_t tdata_count = 0;
857  	tsdn_t *tsdn;
858  	tsdn = tsdn_fetch();
859  	malloc_mutex_lock(tsdn, &tdatas_mtx);
860  	tdata_tree_iter(&tdatas, NULL, prof_tdata_count_iter,
861  	    (void *)&tdata_count);
862  	malloc_mutex_unlock(tsdn, &tdatas_mtx);
863  	return tdata_count;
864  }
865  size_t
866  prof_bt_count(void) {
867  	size_t bt_count;
868  	tsd_t *tsd;
869  	prof_tdata_t *tdata;
870  	tsd = tsd_fetch();
871  	tdata = prof_tdata_get(tsd, false);
872  	if (tdata == NULL) {
873  		return 0;
874  	}
875  	malloc_mutex_lock(tsd_tsdn(tsd), &bt2gctx_mtx);
876  	bt_count = ckh_count(&bt2gctx);
877  	malloc_mutex_unlock(tsd_tsdn(tsd), &bt2gctx_mtx);
878  	return bt_count;
879  }
880  #endif
881  static int
882  prof_dump_open_impl(bool propagate_err, const char *filename) {
883  	int fd;
884  	fd = creat(filename, 0644);
885  	if (fd == -1 && !propagate_err) {
886  		malloc_printf("<jemalloc>: creat(\"%s\"), 0644) failed\n",
887  		    filename);
888  		if (opt_abort) {
889  			abort();
890  		}
891  	}
892  	return fd;
893  }
894  prof_dump_open_t *JET_MUTABLE prof_dump_open = prof_dump_open_impl;
895  static bool
896  prof_dump_flush(bool propagate_err) {
897  	bool ret = false;
898  	ssize_t err;
899  	cassert(config_prof);
900  	err = malloc_write_fd(prof_dump_fd, prof_dump_buf, prof_dump_buf_end);
901  	if (err == -1) {
902  		if (!propagate_err) {
903  			malloc_write("<jemalloc>: write() failed during heap "
904  			    "profile flush\n");
905  			if (opt_abort) {
906  				abort();
907  			}
908  		}
909  		ret = true;
910  	}
911  	prof_dump_buf_end = 0;
912  	return ret;
913  }
914  static bool
915  prof_dump_close(bool propagate_err) {
916  	bool ret;
917  	assert(prof_dump_fd != -1);
918  	ret = prof_dump_flush(propagate_err);
919  	close(prof_dump_fd);
920  	prof_dump_fd = -1;
921  	return ret;
922  }
923  static bool
924  prof_dump_write(bool propagate_err, const char *s) {
925  	size_t i, slen, n;
926  	cassert(config_prof);
927  	i = 0;
928  	slen = strlen(s);
929  	while (i < slen) {
930  		if (prof_dump_buf_end == PROF_DUMP_BUFSIZE) {
931  			if (prof_dump_flush(propagate_err) && propagate_err) {
932  				return true;
933  			}
934  		}
935  		if (prof_dump_buf_end + slen - i <= PROF_DUMP_BUFSIZE) {
936  			n = slen - i;
937  		} else {
938  			n = PROF_DUMP_BUFSIZE - prof_dump_buf_end;
939  		}
940  		memcpy(&prof_dump_buf[prof_dump_buf_end], &s[i], n);
941  		prof_dump_buf_end += n;
942  		i += n;
943  	}
944  	assert(i == slen);
945  	return false;
946  }
947  JEMALLOC_FORMAT_PRINTF(2, 3)
948  static bool
949  prof_dump_printf(bool propagate_err, const char *format, ...) {
950  	bool ret;
951  	va_list ap;
952  	char buf[PROF_PRINTF_BUFSIZE];
953  	va_start(ap, format);
954  	malloc_vsnprintf(buf, sizeof(buf), format, ap);
955  	va_end(ap);
956  	ret = prof_dump_write(propagate_err, buf);
957  	return ret;
958  }
959  static void
960  prof_tctx_merge_tdata(tsdn_t *tsdn, prof_tctx_t *tctx, prof_tdata_t *tdata) {
961  	malloc_mutex_assert_owner(tsdn, tctx->tdata->lock);
962  	malloc_mutex_lock(tsdn, tctx->gctx->lock);
963  	switch (tctx->state) {
964  	case prof_tctx_state_initializing:
965  		malloc_mutex_unlock(tsdn, tctx->gctx->lock);
966  		return;
967  	case prof_tctx_state_nominal:
968  		tctx->state = prof_tctx_state_dumping;
969  		malloc_mutex_unlock(tsdn, tctx->gctx->lock);
970  		memcpy(&tctx->dump_cnts, &tctx->cnts, sizeof(prof_cnt_t));
971  		tdata->cnt_summed.curobjs += tctx->dump_cnts.curobjs;
972  		tdata->cnt_summed.curbytes += tctx->dump_cnts.curbytes;
973  		if (opt_prof_accum) {
974  			tdata->cnt_summed.accumobjs +=
975  			    tctx->dump_cnts.accumobjs;
976  			tdata->cnt_summed.accumbytes +=
977  			    tctx->dump_cnts.accumbytes;
978  		}
979  		break;
980  	case prof_tctx_state_dumping:
981  	case prof_tctx_state_purgatory:
982  		not_reached();
983  	}
984  }
985  static void
986  prof_tctx_merge_gctx(tsdn_t *tsdn, prof_tctx_t *tctx, prof_gctx_t *gctx) {
987  	malloc_mutex_assert_owner(tsdn, gctx->lock);
988  	gctx->cnt_summed.curobjs += tctx->dump_cnts.curobjs;
989  	gctx->cnt_summed.curbytes += tctx->dump_cnts.curbytes;
990  	if (opt_prof_accum) {
991  		gctx->cnt_summed.accumobjs += tctx->dump_cnts.accumobjs;
992  		gctx->cnt_summed.accumbytes += tctx->dump_cnts.accumbytes;
993  	}
994  }
995  static prof_tctx_t *
996  prof_tctx_merge_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *arg) {
997  	tsdn_t *tsdn = (tsdn_t *)arg;
998  	malloc_mutex_assert_owner(tsdn, tctx->gctx->lock);
999  	switch (tctx->state) {
1000  	case prof_tctx_state_nominal:
1001  		break;
1002  	case prof_tctx_state_dumping:
1003  	case prof_tctx_state_purgatory:
1004  		prof_tctx_merge_gctx(tsdn, tctx, tctx->gctx);
1005  		break;
1006  	default:
1007  		not_reached();
1008  	}
1009  	return NULL;
1010  }
1011  struct prof_tctx_dump_iter_arg_s {
1012  	tsdn_t	*tsdn;
1013  	bool	propagate_err;
1014  };
1015  static prof_tctx_t *
1016  prof_tctx_dump_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *opaque) {
1017  	struct prof_tctx_dump_iter_arg_s *arg =
1018  	    (struct prof_tctx_dump_iter_arg_s *)opaque;
1019  	malloc_mutex_assert_owner(arg->tsdn, tctx->gctx->lock);
1020  	switch (tctx->state) {
1021  	case prof_tctx_state_initializing:
1022  	case prof_tctx_state_nominal:
1023  		break;
1024  	case prof_tctx_state_dumping:
1025  	case prof_tctx_state_purgatory:
1026  		if (prof_dump_printf(arg->propagate_err,
1027  		    "  t%"FMTu64": %"FMTu64": %"FMTu64" [%"FMTu64": "
1028  		    "%"FMTu64"]\n", tctx->thr_uid, tctx->dump_cnts.curobjs,
1029  		    tctx->dump_cnts.curbytes, tctx->dump_cnts.accumobjs,
1030  		    tctx->dump_cnts.accumbytes)) {
1031  			return tctx;
1032  		}
1033  		break;
1034  	default:
1035  		not_reached();
1036  	}
1037  	return NULL;
1038  }
1039  static prof_tctx_t *
1040  prof_tctx_finish_iter(prof_tctx_tree_t *tctxs, prof_tctx_t *tctx, void *arg) {
1041  	tsdn_t *tsdn = (tsdn_t *)arg;
1042  	prof_tctx_t *ret;
1043  	malloc_mutex_assert_owner(tsdn, tctx->gctx->lock);
1044  	switch (tctx->state) {
1045  	case prof_tctx_state_nominal:
1046  		break;
1047  	case prof_tctx_state_dumping:
1048  		tctx->state = prof_tctx_state_nominal;
1049  		break;
1050  	case prof_tctx_state_purgatory:
1051  		ret = tctx;
1052  		goto label_return;
1053  	default:
1054  		not_reached();
1055  	}
1056  	ret = NULL;
1057  label_return:
1058  	return ret;
1059  }
1060  static void
1061  prof_dump_gctx_prep(tsdn_t *tsdn, prof_gctx_t *gctx, prof_gctx_tree_t *gctxs) {
1062  	cassert(config_prof);
1063  	malloc_mutex_lock(tsdn, gctx->lock);
1064  	gctx->nlimbo++;
1065  	gctx_tree_insert(gctxs, gctx);
1066  	memset(&gctx->cnt_summed, 0, sizeof(prof_cnt_t));
1067  	malloc_mutex_unlock(tsdn, gctx->lock);
1068  }
1069  struct prof_gctx_merge_iter_arg_s {
1070  	tsdn_t	*tsdn;
1071  	size_t	leak_ngctx;
1072  };
1073  static prof_gctx_t *
1074  prof_gctx_merge_iter(prof_gctx_tree_t *gctxs, prof_gctx_t *gctx, void *opaque) {
1075  	struct prof_gctx_merge_iter_arg_s *arg =
1076  	    (struct prof_gctx_merge_iter_arg_s *)opaque;
1077  	malloc_mutex_lock(arg->tsdn, gctx->lock);
1078  	tctx_tree_iter(&gctx->tctxs, NULL, prof_tctx_merge_iter,
1079  	    (void *)arg->tsdn);
1080  	if (gctx->cnt_summed.curobjs != 0) {
1081  		arg->leak_ngctx++;
1082  	}
1083  	malloc_mutex_unlock(arg->tsdn, gctx->lock);
1084  	return NULL;
1085  }
1086  static void
1087  prof_gctx_finish(tsd_t *tsd, prof_gctx_tree_t *gctxs) {
1088  	prof_tdata_t *tdata = prof_tdata_get(tsd, false);
1089  	prof_gctx_t *gctx;
1090  	while ((gctx = gctx_tree_first(gctxs)) != NULL) {
1091  		gctx_tree_remove(gctxs, gctx);
1092  		malloc_mutex_lock(tsd_tsdn(tsd), gctx->lock);
1093  		{
1094  			prof_tctx_t *next;
1095  			next = NULL;
1096  			do {
1097  				prof_tctx_t *to_destroy =
1098  				    tctx_tree_iter(&gctx->tctxs, next,
1099  				    prof_tctx_finish_iter,
1100  				    (void *)tsd_tsdn(tsd));
1101  				if (to_destroy != NULL) {
1102  					next = tctx_tree_next(&gctx->tctxs,
1103  					    to_destroy);
1104  					tctx_tree_remove(&gctx->tctxs,
1105  					    to_destroy);
1106  					idalloctm(tsd_tsdn(tsd), to_destroy,
1107  					    NULL, NULL, true, true);
1108  				} else {
1109  					next = NULL;
1110  				}
1111  			} while (next != NULL);
1112  		}
<span onclick='openModal()' class='match'>1113  		gctx->nlimbo--;
1114  		if (prof_gctx_should_destroy(gctx)) {
1115  			gctx->nlimbo++;
1116  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
1117  			prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
1118  		} else {
1119  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
1120  		}
1121  	}
</span>1122  }
1123  struct prof_tdata_merge_iter_arg_s {
1124  	tsdn_t		*tsdn;
1125  	prof_cnt_t	cnt_all;
1126  };
1127  static prof_tdata_t *
1128  prof_tdata_merge_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1129      void *opaque) {
1130  	struct prof_tdata_merge_iter_arg_s *arg =
1131  	    (struct prof_tdata_merge_iter_arg_s *)opaque;
1132  	malloc_mutex_lock(arg->tsdn, tdata->lock);
1133  	if (!tdata->expired) {
1134  		size_t tabind;
1135  		union {
1136  			prof_tctx_t	*p;
1137  			void		*v;
1138  		} tctx;
1139  		tdata->dumping = true;
1140  		memset(&tdata->cnt_summed, 0, sizeof(prof_cnt_t));
1141  		for (tabind = 0; !ckh_iter(&tdata->bt2tctx, &tabind, NULL,
1142  		    &tctx.v);) {
1143  			prof_tctx_merge_tdata(arg->tsdn, tctx.p, tdata);
1144  		}
1145  		arg->cnt_all.curobjs += tdata->cnt_summed.curobjs;
1146  		arg->cnt_all.curbytes += tdata->cnt_summed.curbytes;
1147  		if (opt_prof_accum) {
1148  			arg->cnt_all.accumobjs += tdata->cnt_summed.accumobjs;
1149  			arg->cnt_all.accumbytes += tdata->cnt_summed.accumbytes;
1150  		}
1151  	} else {
1152  		tdata->dumping = false;
1153  	}
1154  	malloc_mutex_unlock(arg->tsdn, tdata->lock);
1155  	return NULL;
1156  }
1157  static prof_tdata_t *
1158  prof_tdata_dump_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1159      void *arg) {
1160  	bool propagate_err = *(bool *)arg;
1161  	if (!tdata->dumping) {
1162  		return NULL;
1163  	}
1164  	if (prof_dump_printf(propagate_err,
1165  	    "  t%"FMTu64": %"FMTu64": %"FMTu64" [%"FMTu64": %"FMTu64"]%s%s\n",
1166  	    tdata->thr_uid, tdata->cnt_summed.curobjs,
1167  	    tdata->cnt_summed.curbytes, tdata->cnt_summed.accumobjs,
1168  	    tdata->cnt_summed.accumbytes,
1169  	    (tdata->thread_name != NULL) ? " " : "",
1170  	    (tdata->thread_name != NULL) ? tdata->thread_name : "")) {
1171  		return tdata;
1172  	}
1173  	return NULL;
1174  }
1175  static bool
1176  prof_dump_header_impl(tsdn_t *tsdn, bool propagate_err,
1177      const prof_cnt_t *cnt_all) {
1178  	bool ret;
1179  	if (prof_dump_printf(propagate_err,
1180  	    "heap_v2/%"FMTu64"\n"
1181  	    "  t*: %"FMTu64": %"FMTu64" [%"FMTu64": %"FMTu64"]\n",
1182  	    ((uint64_t)1U << lg_prof_sample), cnt_all->curobjs,
1183  	    cnt_all->curbytes, cnt_all->accumobjs, cnt_all->accumbytes)) {
1184  		return true;
1185  	}
1186  	malloc_mutex_lock(tsdn, &tdatas_mtx);
1187  	ret = (tdata_tree_iter(&tdatas, NULL, prof_tdata_dump_iter,
1188  	    (void *)&propagate_err) != NULL);
1189  	malloc_mutex_unlock(tsdn, &tdatas_mtx);
1190  	return ret;
1191  }
1192  prof_dump_header_t *JET_MUTABLE prof_dump_header = prof_dump_header_impl;
1193  static bool
1194  prof_dump_gctx(tsdn_t *tsdn, bool propagate_err, prof_gctx_t *gctx,
1195      const prof_bt_t *bt, prof_gctx_tree_t *gctxs) {
1196  	bool ret;
1197  	unsigned i;
1198  	struct prof_tctx_dump_iter_arg_s prof_tctx_dump_iter_arg;
1199  	cassert(config_prof);
1200  	malloc_mutex_assert_owner(tsdn, gctx->lock);
1201  	if ((!opt_prof_accum && gctx->cnt_summed.curobjs == 0) ||
1202  	    (opt_prof_accum && gctx->cnt_summed.accumobjs == 0)) {
1203  		assert(gctx->cnt_summed.curobjs == 0);
1204  		assert(gctx->cnt_summed.curbytes == 0);
1205  		assert(gctx->cnt_summed.accumobjs == 0);
1206  		assert(gctx->cnt_summed.accumbytes == 0);
1207  		ret = false;
1208  		goto label_return;
1209  	}
1210  	if (prof_dump_printf(propagate_err, "@")) {
1211  		ret = true;
1212  		goto label_return;
1213  	}
1214  	for (i = 0; i < bt->len; i++) {
1215  		if (prof_dump_printf(propagate_err, " %#"FMTxPTR,
1216  		    (uintptr_t)bt->vec[i])) {
1217  			ret = true;
1218  			goto label_return;
1219  		}
1220  	}
1221  	if (prof_dump_printf(propagate_err,
1222  	    "\n"
1223  	    "  t*: %"FMTu64": %"FMTu64" [%"FMTu64": %"FMTu64"]\n",
1224  	    gctx->cnt_summed.curobjs, gctx->cnt_summed.curbytes,
1225  	    gctx->cnt_summed.accumobjs, gctx->cnt_summed.accumbytes)) {
1226  		ret = true;
1227  		goto label_return;
1228  	}
1229  	prof_tctx_dump_iter_arg.tsdn = tsdn;
1230  	prof_tctx_dump_iter_arg.propagate_err = propagate_err;
1231  	if (tctx_tree_iter(&gctx->tctxs, NULL, prof_tctx_dump_iter,
1232  	    (void *)&prof_tctx_dump_iter_arg) != NULL) {
1233  		ret = true;
1234  		goto label_return;
1235  	}
1236  	ret = false;
1237  label_return:
1238  	return ret;
1239  }
1240  #ifndef _WIN32
1241  JEMALLOC_FORMAT_PRINTF(1, 2)
1242  static int
1243  prof_open_maps(const char *format, ...) {
1244  	int mfd;
1245  	va_list ap;
1246  	char filename[PATH_MAX + 1];
1247  	va_start(ap, format);
1248  	malloc_vsnprintf(filename, sizeof(filename), format, ap);
1249  	va_end(ap);
1250  #if defined(O_CLOEXEC)
1251  	mfd = open(filename, O_RDONLY | O_CLOEXEC);
1252  #else
1253  	mfd = open(filename, O_RDONLY);
1254  	if (mfd != -1) {
1255  		fcntl(mfd, F_SETFD, fcntl(mfd, F_GETFD) | FD_CLOEXEC);
1256  	}
1257  #endif
1258  	return mfd;
1259  }
1260  #endif
1261  static int
1262  prof_getpid(void) {
1263  #ifdef _WIN32
1264  	return GetCurrentProcessId();
1265  #else
1266  	return getpid();
1267  #endif
1268  }
1269  static bool
1270  prof_dump_maps(bool propagate_err) {
1271  	bool ret;
1272  	int mfd;
1273  	cassert(config_prof);
1274  #ifdef __FreeBSD__
1275  	mfd = prof_open_maps("/proc/curproc/map");
1276  #elif defined(_WIN32)
1277  	mfd = -1; 
1278  #else
1279  	{
1280  		int pid = prof_getpid();
1281  		mfd = prof_open_maps("/proc/%d/task/%d/maps", pid, pid);
1282  		if (mfd == -1) {
1283  			mfd = prof_open_maps("/proc/%d/maps", pid);
1284  		}
1285  	}
1286  #endif
1287  	if (mfd != -1) {
1288  		ssize_t nread;
1289  		if (prof_dump_write(propagate_err, "\nMAPPED_LIBRARIES:\n") &&
1290  		    propagate_err) {
1291  			ret = true;
1292  			goto label_return;
1293  		}
1294  		nread = 0;
1295  		do {
1296  			prof_dump_buf_end += nread;
1297  			if (prof_dump_buf_end == PROF_DUMP_BUFSIZE) {
1298  				if (prof_dump_flush(propagate_err) &&
1299  				    propagate_err) {
1300  					ret = true;
1301  					goto label_return;
1302  				}
1303  			}
1304  			nread = malloc_read_fd(mfd,
1305  			    &prof_dump_buf[prof_dump_buf_end], PROF_DUMP_BUFSIZE
1306  			    - prof_dump_buf_end);
1307  		} while (nread > 0);
1308  	} else {
1309  		ret = true;
1310  		goto label_return;
1311  	}
1312  	ret = false;
1313  label_return:
1314  	if (mfd != -1) {
1315  		close(mfd);
1316  	}
1317  	return ret;
1318  }
1319  static void
1320  prof_leakcheck(const prof_cnt_t *cnt_all, size_t leak_ngctx,
1321      const char *filename) {
1322  #ifdef JEMALLOC_PROF
1323  	if (cnt_all->curbytes != 0) {
1324  		double sample_period = (double)((uint64_t)1 << lg_prof_sample);
1325  		double ratio = (((double)cnt_all->curbytes) /
1326  		    (double)cnt_all->curobjs) / sample_period;
1327  		double scale_factor = 1.0 / (1.0 - exp(-ratio));
1328  		uint64_t curbytes = (uint64_t)round(((double)cnt_all->curbytes)
1329  		    * scale_factor);
1330  		uint64_t curobjs = (uint64_t)round(((double)cnt_all->curobjs) *
1331  		    scale_factor);
1332  		malloc_printf("<jemalloc>: Leak approximation summary: ~%"FMTu64
1333  		    " byte%s, ~%"FMTu64" object%s, >= %zu context%s\n",
1334  		    curbytes, (curbytes != 1) ? "s" : "", curobjs, (curobjs !=
1335  		    1) ? "s" : "", leak_ngctx, (leak_ngctx != 1) ? "s" : "");
1336  		malloc_printf(
1337  		    "<jemalloc>: Run jeprof on \"%s\" for leak detail\n",
1338  		    filename);
1339  	}
1340  #endif
1341  }
1342  struct prof_gctx_dump_iter_arg_s {
1343  	tsdn_t	*tsdn;
1344  	bool	propagate_err;
1345  };
1346  static prof_gctx_t *
1347  prof_gctx_dump_iter(prof_gctx_tree_t *gctxs, prof_gctx_t *gctx, void *opaque) {
1348  	prof_gctx_t *ret;
1349  	struct prof_gctx_dump_iter_arg_s *arg =
1350  	    (struct prof_gctx_dump_iter_arg_s *)opaque;
1351  	malloc_mutex_lock(arg->tsdn, gctx->lock);
1352  	if (prof_dump_gctx(arg->tsdn, arg->propagate_err, gctx, &gctx->bt,
1353  	    gctxs)) {
1354  		ret = gctx;
1355  		goto label_return;
1356  	}
1357  	ret = NULL;
1358  label_return:
1359  	malloc_mutex_unlock(arg->tsdn, gctx->lock);
1360  	return ret;
1361  }
1362  static void
1363  prof_dump_prep(tsd_t *tsd, prof_tdata_t *tdata,
1364      struct prof_tdata_merge_iter_arg_s *prof_tdata_merge_iter_arg,
1365      struct prof_gctx_merge_iter_arg_s *prof_gctx_merge_iter_arg,
1366      prof_gctx_tree_t *gctxs) {
1367  	size_t tabind;
1368  	union {
1369  		prof_gctx_t	*p;
1370  		void		*v;
1371  	} gctx;
1372  	prof_enter(tsd, tdata);
1373  	gctx_tree_new(gctxs);
1374  	for (tabind = 0; !ckh_iter(&bt2gctx, &tabind, NULL, &gctx.v);) {
1375  		prof_dump_gctx_prep(tsd_tsdn(tsd), gctx.p, gctxs);
1376  	}
1377  	prof_tdata_merge_iter_arg->tsdn = tsd_tsdn(tsd);
1378  	memset(&prof_tdata_merge_iter_arg->cnt_all, 0, sizeof(prof_cnt_t));
1379  	malloc_mutex_lock(tsd_tsdn(tsd), &tdatas_mtx);
1380  	tdata_tree_iter(&tdatas, NULL, prof_tdata_merge_iter,
1381  	    (void *)prof_tdata_merge_iter_arg);
1382  	malloc_mutex_unlock(tsd_tsdn(tsd), &tdatas_mtx);
1383  	prof_gctx_merge_iter_arg->tsdn = tsd_tsdn(tsd);
1384  	prof_gctx_merge_iter_arg->leak_ngctx = 0;
1385  	gctx_tree_iter(gctxs, NULL, prof_gctx_merge_iter,
1386  	    (void *)prof_gctx_merge_iter_arg);
1387  	prof_leave(tsd, tdata);
1388  }
1389  static bool
1390  prof_dump_file(tsd_t *tsd, bool propagate_err, const char *filename,
1391      bool leakcheck, prof_tdata_t *tdata,
1392      struct prof_tdata_merge_iter_arg_s *prof_tdata_merge_iter_arg,
1393      struct prof_gctx_merge_iter_arg_s *prof_gctx_merge_iter_arg,
1394      struct prof_gctx_dump_iter_arg_s *prof_gctx_dump_iter_arg,
1395      prof_gctx_tree_t *gctxs) {
1396  	if ((prof_dump_fd = prof_dump_open(propagate_err, filename)) == -1) {
1397  		return true;
1398  	}
1399  	if (prof_dump_header(tsd_tsdn(tsd), propagate_err,
1400  	    &prof_tdata_merge_iter_arg->cnt_all)) {
1401  		goto label_write_error;
1402  	}
1403  	prof_gctx_dump_iter_arg->tsdn = tsd_tsdn(tsd);
1404  	prof_gctx_dump_iter_arg->propagate_err = propagate_err;
1405  	if (gctx_tree_iter(gctxs, NULL, prof_gctx_dump_iter,
1406  	    (void *)prof_gctx_dump_iter_arg) != NULL) {
1407  		goto label_write_error;
1408  	}
1409  	if (prof_dump_maps(propagate_err)) {
1410  		goto label_write_error;
1411  	}
1412  	if (prof_dump_close(propagate_err)) {
1413  		return true;
1414  	}
1415  	return false;
1416  label_write_error:
1417  	prof_dump_close(propagate_err);
1418  	return true;
1419  }
1420  static bool
1421  prof_dump(tsd_t *tsd, bool propagate_err, const char *filename,
1422      bool leakcheck) {
1423  	cassert(config_prof);
1424  	assert(tsd_reentrancy_level_get(tsd) == 0);
1425  	prof_tdata_t * tdata = prof_tdata_get(tsd, true);
1426  	if (tdata == NULL) {
1427  		return true;
1428  	}
1429  	pre_reentrancy(tsd, NULL);
1430  	malloc_mutex_lock(tsd_tsdn(tsd), &prof_dump_mtx);
1431  	prof_gctx_tree_t gctxs;
1432  	struct prof_tdata_merge_iter_arg_s prof_tdata_merge_iter_arg;
1433  	struct prof_gctx_merge_iter_arg_s prof_gctx_merge_iter_arg;
1434  	struct prof_gctx_dump_iter_arg_s prof_gctx_dump_iter_arg;
1435  	prof_dump_prep(tsd, tdata, &prof_tdata_merge_iter_arg,
1436  	    &prof_gctx_merge_iter_arg, &gctxs);
1437  	bool err = prof_dump_file(tsd, propagate_err, filename, leakcheck, tdata,
1438  	    &prof_tdata_merge_iter_arg, &prof_gctx_merge_iter_arg,
1439  	    &prof_gctx_dump_iter_arg, &gctxs);
1440  	prof_gctx_finish(tsd, &gctxs);
1441  	malloc_mutex_unlock(tsd_tsdn(tsd), &prof_dump_mtx);
1442  	post_reentrancy(tsd);
1443  	if (err) {
1444  		return true;
1445  	}
1446  	if (leakcheck) {
1447  		prof_leakcheck(&prof_tdata_merge_iter_arg.cnt_all,
1448  		    prof_gctx_merge_iter_arg.leak_ngctx, filename);
1449  	}
1450  	return false;
1451  }
1452  #ifdef JEMALLOC_JET
1453  void
1454  prof_cnt_all(uint64_t *curobjs, uint64_t *curbytes, uint64_t *accumobjs,
1455      uint64_t *accumbytes) {
1456  	tsd_t *tsd;
1457  	prof_tdata_t *tdata;
1458  	struct prof_tdata_merge_iter_arg_s prof_tdata_merge_iter_arg;
1459  	struct prof_gctx_merge_iter_arg_s prof_gctx_merge_iter_arg;
1460  	prof_gctx_tree_t gctxs;
1461  	tsd = tsd_fetch();
1462  	tdata = prof_tdata_get(tsd, false);
1463  	if (tdata == NULL) {
1464  		if (curobjs != NULL) {
1465  			*curobjs = 0;
1466  		}
1467  		if (curbytes != NULL) {
1468  			*curbytes = 0;
1469  		}
1470  		if (accumobjs != NULL) {
1471  			*accumobjs = 0;
1472  		}
1473  		if (accumbytes != NULL) {
1474  			*accumbytes = 0;
1475  		}
1476  		return;
1477  	}
1478  	prof_dump_prep(tsd, tdata, &prof_tdata_merge_iter_arg,
1479  	    &prof_gctx_merge_iter_arg, &gctxs);
1480  	prof_gctx_finish(tsd, &gctxs);
1481  	if (curobjs != NULL) {
1482  		*curobjs = prof_tdata_merge_iter_arg.cnt_all.curobjs;
1483  	}
1484  	if (curbytes != NULL) {
1485  		*curbytes = prof_tdata_merge_iter_arg.cnt_all.curbytes;
1486  	}
1487  	if (accumobjs != NULL) {
1488  		*accumobjs = prof_tdata_merge_iter_arg.cnt_all.accumobjs;
1489  	}
1490  	if (accumbytes != NULL) {
1491  		*accumbytes = prof_tdata_merge_iter_arg.cnt_all.accumbytes;
1492  	}
1493  }
1494  #endif
1495  #define DUMP_FILENAME_BUFSIZE	(PATH_MAX + 1)
1496  #define VSEQ_INVALID		UINT64_C(0xffffffffffffffff)
1497  static void
1498  prof_dump_filename(char *filename, char v, uint64_t vseq) {
1499  	cassert(config_prof);
1500  	if (vseq != VSEQ_INVALID) {
1501  		malloc_snprintf(filename, DUMP_FILENAME_BUFSIZE,
1502  		    "%s.%d.%"FMTu64".%c%"FMTu64".heap",
1503  		    opt_prof_prefix, prof_getpid(), prof_dump_seq, v, vseq);
1504  	} else {
1505  		malloc_snprintf(filename, DUMP_FILENAME_BUFSIZE,
1506  		    "%s.%d.%"FMTu64".%c.heap",
1507  		    opt_prof_prefix, prof_getpid(), prof_dump_seq, v);
1508  	}
1509  	prof_dump_seq++;
1510  }
1511  static void
1512  prof_fdump(void) {
1513  	tsd_t *tsd;
1514  	char filename[DUMP_FILENAME_BUFSIZE];
1515  	cassert(config_prof);
1516  	assert(opt_prof_final);
1517  	assert(opt_prof_prefix[0] != '\0');
1518  	if (!prof_booted) {
1519  		return;
1520  	}
1521  	tsd = tsd_fetch();
1522  	assert(tsd_reentrancy_level_get(tsd) == 0);
1523  	malloc_mutex_lock(tsd_tsdn(tsd), &prof_dump_seq_mtx);
1524  	prof_dump_filename(filename, 'f', VSEQ_INVALID);
1525  	malloc_mutex_unlock(tsd_tsdn(tsd), &prof_dump_seq_mtx);
1526  	prof_dump(tsd, false, filename, opt_prof_leak);
1527  }
1528  bool
1529  prof_accum_init(tsdn_t *tsdn, prof_accum_t *prof_accum) {
1530  	cassert(config_prof);
1531  #ifndef JEMALLOC_ATOMIC_U64
1532  	if (malloc_mutex_init(&prof_accum->mtx, "prof_accum",
1533  	    WITNESS_RANK_PROF_ACCUM, malloc_mutex_rank_exclusive)) {
1534  		return true;
1535  	}
1536  	prof_accum->accumbytes = 0;
1537  #else
1538  	atomic_store_u64(&prof_accum->accumbytes, 0, ATOMIC_RELAXED);
1539  #endif
1540  	return false;
1541  }
1542  void
1543  prof_idump(tsdn_t *tsdn) {
1544  	tsd_t *tsd;
1545  	prof_tdata_t *tdata;
1546  	cassert(config_prof);
1547  	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
1548  		return;
1549  	}
1550  	tsd = tsdn_tsd(tsdn);
1551  	if (tsd_reentrancy_level_get(tsd) > 0) {
1552  		return;
1553  	}
1554  	tdata = prof_tdata_get(tsd, false);
1555  	if (tdata == NULL) {
1556  		return;
1557  	}
1558  	if (tdata->enq) {
1559  		tdata->enq_idump = true;
1560  		return;
1561  	}
1562  	if (opt_prof_prefix[0] != '\0') {
1563  		char filename[PATH_MAX + 1];
1564  		malloc_mutex_lock(tsd_tsdn(tsd), &prof_dump_seq_mtx);
1565  		prof_dump_filename(filename, 'i', prof_dump_iseq);
1566  		prof_dump_iseq++;
1567  		malloc_mutex_unlock(tsd_tsdn(tsd), &prof_dump_seq_mtx);
1568  		prof_dump(tsd, false, filename, false);
1569  	}
1570  }
1571  bool
1572  prof_mdump(tsd_t *tsd, const char *filename) {
1573  	cassert(config_prof);
1574  	assert(tsd_reentrancy_level_get(tsd) == 0);
1575  	if (!opt_prof || !prof_booted) {
1576  		return true;
1577  	}
1578  	char filename_buf[DUMP_FILENAME_BUFSIZE];
1579  	if (filename == NULL) {
1580  		if (opt_prof_prefix[0] == '\0') {
1581  			return true;
1582  		}
1583  		malloc_mutex_lock(tsd_tsdn(tsd), &prof_dump_seq_mtx);
1584  		prof_dump_filename(filename_buf, 'm', prof_dump_mseq);
1585  		prof_dump_mseq++;
1586  		malloc_mutex_unlock(tsd_tsdn(tsd), &prof_dump_seq_mtx);
1587  		filename = filename_buf;
1588  	}
1589  	return prof_dump(tsd, true, filename, false);
1590  }
1591  void
1592  prof_gdump(tsdn_t *tsdn) {
1593  	tsd_t *tsd;
1594  	prof_tdata_t *tdata;
1595  	cassert(config_prof);
1596  	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
1597  		return;
1598  	}
1599  	tsd = tsdn_tsd(tsdn);
1600  	if (tsd_reentrancy_level_get(tsd) > 0) {
1601  		return;
1602  	}
1603  	tdata = prof_tdata_get(tsd, false);
1604  	if (tdata == NULL) {
1605  		return;
1606  	}
1607  	if (tdata->enq) {
1608  		tdata->enq_gdump = true;
1609  		return;
1610  	}
1611  	if (opt_prof_prefix[0] != '\0') {
1612  		char filename[DUMP_FILENAME_BUFSIZE];
1613  		malloc_mutex_lock(tsdn, &prof_dump_seq_mtx);
1614  		prof_dump_filename(filename, 'u', prof_dump_useq);
1615  		prof_dump_useq++;
1616  		malloc_mutex_unlock(tsdn, &prof_dump_seq_mtx);
1617  		prof_dump(tsd, false, filename, false);
1618  	}
1619  }
1620  static void
1621  prof_bt_hash(const void *key, size_t r_hash[2]) {
1622  	prof_bt_t *bt = (prof_bt_t *)key;
1623  	cassert(config_prof);
1624  	hash(bt->vec, bt->len * sizeof(void *), 0x94122f33U, r_hash);
1625  }
1626  static bool
1627  prof_bt_keycomp(const void *k1, const void *k2) {
1628  	const prof_bt_t *bt1 = (prof_bt_t *)k1;
1629  	const prof_bt_t *bt2 = (prof_bt_t *)k2;
1630  	cassert(config_prof);
1631  	if (bt1->len != bt2->len) {
1632  		return false;
1633  	}
1634  	return (memcmp(bt1->vec, bt2->vec, bt1->len * sizeof(void *)) == 0);
1635  }
1636  static void
1637  prof_bt_node_hash(const void *key, size_t r_hash[2]) {
1638  	const prof_bt_node_t *bt_node = (prof_bt_node_t *)key;
1639  	prof_bt_hash((void *)(&bt_node->bt), r_hash);
1640  }
1641  static bool
1642  prof_bt_node_keycomp(const void *k1, const void *k2) {
1643  	const prof_bt_node_t *bt_node1 = (prof_bt_node_t *)k1;
1644  	const prof_bt_node_t *bt_node2 = (prof_bt_node_t *)k2;
1645  	return prof_bt_keycomp((void *)(&bt_node1->bt),
1646  	    (void *)(&bt_node2->bt));
1647  }
1648  static void
1649  prof_thr_node_hash(const void *key, size_t r_hash[2]) {
1650  	const prof_thr_node_t *thr_node = (prof_thr_node_t *)key;
1651  	hash(&thr_node->thr_uid, sizeof(uint64_t), 0x94122f35U, r_hash);
1652  }
1653  static bool
1654  prof_thr_node_keycomp(const void *k1, const void *k2) {
1655  	const prof_thr_node_t *thr_node1 = (prof_thr_node_t *)k1;
1656  	const prof_thr_node_t *thr_node2 = (prof_thr_node_t *)k2;
1657  	return thr_node1->thr_uid == thr_node2->thr_uid;
1658  }
1659  static uint64_t
1660  prof_thr_uid_alloc(tsdn_t *tsdn) {
1661  	uint64_t thr_uid;
1662  	malloc_mutex_lock(tsdn, &next_thr_uid_mtx);
1663  	thr_uid = next_thr_uid;
1664  	next_thr_uid++;
1665  	malloc_mutex_unlock(tsdn, &next_thr_uid_mtx);
1666  	return thr_uid;
1667  }
1668  static prof_tdata_t *
1669  prof_tdata_init_impl(tsd_t *tsd, uint64_t thr_uid, uint64_t thr_discrim,
1670      char *thread_name, bool active) {
1671  	prof_tdata_t *tdata;
1672  	cassert(config_prof);
1673  	tdata = (prof_tdata_t *)iallocztm(tsd_tsdn(tsd), sizeof(prof_tdata_t),
1674  	    sz_size2index(sizeof(prof_tdata_t)), false, NULL, true,
1675  	    arena_get(TSDN_NULL, 0, true), true);
1676  	if (tdata == NULL) {
1677  		return NULL;
1678  	}
1679  	tdata->lock = prof_tdata_mutex_choose(thr_uid);
1680  	tdata->thr_uid = thr_uid;
1681  	tdata->thr_discrim = thr_discrim;
1682  	tdata->thread_name = thread_name;
1683  	tdata->attached = true;
1684  	tdata->expired = false;
1685  	tdata->tctx_uid_next = 0;
1686  	if (ckh_new(tsd, &tdata->bt2tctx, PROF_CKH_MINITEMS, prof_bt_hash,
1687  	    prof_bt_keycomp)) {
1688  		idalloctm(tsd_tsdn(tsd), tdata, NULL, NULL, true, true);
1689  		return NULL;
1690  	}
1691  	tdata->prng_state = (uint64_t)(uintptr_t)tdata;
1692  	prof_sample_threshold_update(tdata);
1693  	tdata->enq = false;
1694  	tdata->enq_idump = false;
1695  	tdata->enq_gdump = false;
1696  	tdata->dumping = false;
1697  	tdata->active = active;
1698  	malloc_mutex_lock(tsd_tsdn(tsd), &tdatas_mtx);
1699  	tdata_tree_insert(&tdatas, tdata);
1700  	malloc_mutex_unlock(tsd_tsdn(tsd), &tdatas_mtx);
1701  	return tdata;
1702  }
1703  prof_tdata_t *
1704  prof_tdata_init(tsd_t *tsd) {
1705  	return prof_tdata_init_impl(tsd, prof_thr_uid_alloc(tsd_tsdn(tsd)), 0,
1706  	    NULL, prof_thread_active_init_get(tsd_tsdn(tsd)));
1707  }
1708  static bool
1709  prof_tdata_should_destroy_unlocked(prof_tdata_t *tdata, bool even_if_attached) {
1710  	if (tdata->attached && !even_if_attached) {
1711  		return false;
1712  	}
1713  	if (ckh_count(&tdata->bt2tctx) != 0) {
1714  		return false;
1715  	}
1716  	return true;
1717  }
1718  static bool
1719  prof_tdata_should_destroy(tsdn_t *tsdn, prof_tdata_t *tdata,
1720      bool even_if_attached) {
1721  	malloc_mutex_assert_owner(tsdn, tdata->lock);
1722  	return prof_tdata_should_destroy_unlocked(tdata, even_if_attached);
1723  }
1724  static void
1725  prof_tdata_destroy_locked(tsd_t *tsd, prof_tdata_t *tdata,
1726      bool even_if_attached) {
1727  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &tdatas_mtx);
1728  	tdata_tree_remove(&tdatas, tdata);
1729  	assert(prof_tdata_should_destroy_unlocked(tdata, even_if_attached));
1730  	if (tdata->thread_name != NULL) {
1731  		idalloctm(tsd_tsdn(tsd), tdata->thread_name, NULL, NULL, true,
1732  		    true);
1733  	}
1734  	ckh_delete(tsd, &tdata->bt2tctx);
1735  	idalloctm(tsd_tsdn(tsd), tdata, NULL, NULL, true, true);
1736  }
1737  static void
1738  prof_tdata_destroy(tsd_t *tsd, prof_tdata_t *tdata, bool even_if_attached) {
1739  	malloc_mutex_lock(tsd_tsdn(tsd), &tdatas_mtx);
1740  	prof_tdata_destroy_locked(tsd, tdata, even_if_attached);
1741  	malloc_mutex_unlock(tsd_tsdn(tsd), &tdatas_mtx);
1742  }
1743  static void
1744  prof_tdata_detach(tsd_t *tsd, prof_tdata_t *tdata) {
1745  	bool destroy_tdata;
1746  	malloc_mutex_lock(tsd_tsdn(tsd), tdata->lock);
1747  	if (tdata->attached) {
1748  		destroy_tdata = prof_tdata_should_destroy(tsd_tsdn(tsd), tdata,
1749  		    true);
1750  		if (!destroy_tdata) {
1751  			tdata->attached = false;
1752  		}
1753  		tsd_prof_tdata_set(tsd, NULL);
1754  	} else {
1755  		destroy_tdata = false;
1756  	}
1757  	malloc_mutex_unlock(tsd_tsdn(tsd), tdata->lock);
1758  	if (destroy_tdata) {
1759  		prof_tdata_destroy(tsd, tdata, true);
1760  	}
1761  }
1762  prof_tdata_t *
1763  prof_tdata_reinit(tsd_t *tsd, prof_tdata_t *tdata) {
1764  	uint64_t thr_uid = tdata->thr_uid;
1765  	uint64_t thr_discrim = tdata->thr_discrim + 1;
1766  	char *thread_name = (tdata->thread_name != NULL) ?
1767  	    prof_thread_name_alloc(tsd_tsdn(tsd), tdata->thread_name) : NULL;
1768  	bool active = tdata->active;
1769  	prof_tdata_detach(tsd, tdata);
1770  	return prof_tdata_init_impl(tsd, thr_uid, thr_discrim, thread_name,
1771  	    active);
1772  }
1773  static bool
1774  prof_tdata_expire(tsdn_t *tsdn, prof_tdata_t *tdata) {
1775  	bool destroy_tdata;
1776  	malloc_mutex_lock(tsdn, tdata->lock);
1777  	if (!tdata->expired) {
1778  		tdata->expired = true;
1779  		destroy_tdata = tdata->attached ? false :
1780  		    prof_tdata_should_destroy(tsdn, tdata, false);
1781  	} else {
1782  		destroy_tdata = false;
1783  	}
1784  	malloc_mutex_unlock(tsdn, tdata->lock);
1785  	return destroy_tdata;
1786  }
1787  static prof_tdata_t *
1788  prof_tdata_reset_iter(prof_tdata_tree_t *tdatas, prof_tdata_t *tdata,
1789      void *arg) {
1790  	tsdn_t *tsdn = (tsdn_t *)arg;
1791  	return (prof_tdata_expire(tsdn, tdata) ? tdata : NULL);
1792  }
1793  void
1794  prof_reset(tsd_t *tsd, size_t lg_sample) {
1795  	prof_tdata_t *next;
1796  	assert(lg_sample < (sizeof(uint64_t) << 3));
1797  	malloc_mutex_lock(tsd_tsdn(tsd), &prof_dump_mtx);
1798  	malloc_mutex_lock(tsd_tsdn(tsd), &tdatas_mtx);
1799  	lg_prof_sample = lg_sample;
1800  	next = NULL;
1801  	do {
1802  		prof_tdata_t *to_destroy = tdata_tree_iter(&tdatas, next,
1803  		    prof_tdata_reset_iter, (void *)tsd);
1804  		if (to_destroy != NULL) {
1805  			next = tdata_tree_next(&tdatas, to_destroy);
1806  			prof_tdata_destroy_locked(tsd, to_destroy, false);
1807  		} else {
1808  			next = NULL;
1809  		}
1810  	} while (next != NULL);
1811  	malloc_mutex_unlock(tsd_tsdn(tsd), &tdatas_mtx);
1812  	malloc_mutex_unlock(tsd_tsdn(tsd), &prof_dump_mtx);
1813  }
1814  void
1815  prof_tdata_cleanup(tsd_t *tsd) {
1816  	prof_tdata_t *tdata;
1817  	if (!config_prof) {
1818  		return;
1819  	}
1820  	tdata = tsd_prof_tdata_get(tsd);
1821  	if (tdata != NULL) {
1822  		prof_tdata_detach(tsd, tdata);
1823  	}
1824  }
1825  bool
1826  prof_active_get(tsdn_t *tsdn) {
1827  	bool prof_active_current;
1828  	malloc_mutex_lock(tsdn, &prof_active_mtx);
1829  	prof_active_current = prof_active;
1830  	malloc_mutex_unlock(tsdn, &prof_active_mtx);
1831  	return prof_active_current;
1832  }
1833  bool
1834  prof_active_set(tsdn_t *tsdn, bool active) {
1835  	bool prof_active_old;
1836  	malloc_mutex_lock(tsdn, &prof_active_mtx);
1837  	prof_active_old = prof_active;
1838  	prof_active = active;
1839  	malloc_mutex_unlock(tsdn, &prof_active_mtx);
1840  	return prof_active_old;
1841  }
1842  #ifdef JEMALLOC_JET
1843  size_t
1844  prof_log_bt_count(void) {
1845  	size_t cnt = 0;
1846  	prof_bt_node_t *node = log_bt_first;
1847  	while (node != NULL) {
1848  		cnt++;
1849  		node = node->next;
1850  	}
1851  	return cnt;
1852  }
1853  size_t
1854  prof_log_alloc_count(void) {
1855  	size_t cnt = 0;
1856  	prof_alloc_node_t *node = log_alloc_first;
1857  	while (node != NULL) {
1858  		cnt++;
1859  		node = node->next;
1860  	}
1861  	return cnt;
1862  }
1863  size_t
1864  prof_log_thr_count(void) {
1865  	size_t cnt = 0;
1866  	prof_thr_node_t *node = log_thr_first;
1867  	while (node != NULL) {
1868  		cnt++;
1869  		node = node->next;
1870  	}
1871  	return cnt;
1872  }
1873  bool
1874  prof_log_is_logging(void) {
1875  	return prof_logging_state == prof_logging_state_started;
1876  }
1877  bool
1878  prof_log_rep_check(void) {
1879  	if (prof_logging_state == prof_logging_state_stopped
1880  	    && log_tables_initialized) {
1881  		return true;
1882  	}
1883  	if (log_bt_last != NULL && log_bt_last->next != NULL) {
1884  		return true;
1885  	}
1886  	if (log_thr_last != NULL && log_thr_last->next != NULL) {
1887  		return true;
1888  	}
1889  	if (log_alloc_last != NULL && log_alloc_last->next != NULL) {
1890  		return true;
1891  	}
1892  	size_t bt_count = prof_log_bt_count();
1893  	size_t thr_count = prof_log_thr_count();
1894  	size_t alloc_count = prof_log_alloc_count();
1895  	if (prof_logging_state == prof_logging_state_stopped) {
1896  		if (bt_count != 0 || thr_count != 0 || alloc_count || 0) {
1897  			return true;
1898  		}
1899  	}
1900  	prof_alloc_node_t *node = log_alloc_first;
1901  	while (node != NULL) {
1902  		if (node->alloc_bt_ind >= bt_count) {
1903  			return true;
1904  		}
1905  		if (node->free_bt_ind >= bt_count) {
1906  			return true;
1907  		}
1908  		if (node->alloc_thr_ind >= thr_count) {
1909  			return true;
1910  		}
1911  		if (node->free_thr_ind >= thr_count) {
1912  			return true;
1913  		}
1914  		if (node->alloc_time_ns > node->free_time_ns) {
1915  			return true;
1916  		}
1917  		node = node->next;
1918  	}
1919  	return false;
1920  }
1921  void
1922  prof_log_dummy_set(bool new_value) {
1923  	prof_log_dummy = new_value;
1924  }
1925  #endif
1926  bool
1927  prof_log_start(tsdn_t *tsdn, const char *filename) {
1928  	if (!opt_prof || !prof_booted) {
1929  		return true;
1930  	}
1931  	bool ret = false;
1932  	size_t buf_size = PATH_MAX + 1;
1933  	malloc_mutex_lock(tsdn, &log_mtx);
1934  	if (prof_logging_state != prof_logging_state_stopped) {
1935  		ret = true;
1936  	} else if (filename == NULL) {
1937  		malloc_snprintf(log_filename, buf_size, "%s.%d.%"FMTu64".json",
1938  		    opt_prof_prefix, prof_getpid(), log_seq);
1939  		log_seq++;
1940  		prof_logging_state = prof_logging_state_started;
1941  	} else if (strlen(filename) >= buf_size) {
1942  		ret = true;
1943  	} else {
1944  		strcpy(log_filename, filename);
1945  		prof_logging_state = prof_logging_state_started;
1946  	}
1947  	if (!ret) {
1948  		nstime_update(&log_start_timestamp);
1949  	}
1950  	malloc_mutex_unlock(tsdn, &log_mtx);
1951  	return ret;
1952  }
1953  static void
1954  prof_log_stop_final(void) {
1955  	tsd_t *tsd = tsd_fetch();
1956  	prof_log_stop(tsd_tsdn(tsd));
1957  }
1958  struct prof_emitter_cb_arg_s {
1959  	int fd;
1960  	ssize_t ret;
1961  };
1962  static void
1963  prof_emitter_write_cb(void *opaque, const char *to_write) {
1964  	struct prof_emitter_cb_arg_s *arg =
1965  	    (struct prof_emitter_cb_arg_s *)opaque;
1966  	size_t bytes = strlen(to_write);
1967  #ifdef JEMALLOC_JET
1968  	if (prof_log_dummy) {
1969  		return;
1970  	}
1971  #endif
1972  	arg->ret = write(arg->fd, (void *)to_write, bytes);
1973  }
1974  static void
1975  prof_log_emit_threads(tsd_t *tsd, emitter_t *emitter) {
1976  	emitter_json_array_kv_begin(emitter, "threads");
1977  	prof_thr_node_t *thr_node = log_thr_first;
1978  	prof_thr_node_t *thr_old_node;
1979  	while (thr_node != NULL) {
1980  		emitter_json_object_begin(emitter);
1981  		emitter_json_kv(emitter, "thr_uid", emitter_type_uint64,
1982  		    &thr_node->thr_uid);
1983  		char *thr_name = thr_node->name;
1984  		emitter_json_kv(emitter, "thr_name", emitter_type_string,
1985  		    &thr_name);
1986  		emitter_json_object_end(emitter);
1987  		thr_old_node = thr_node;
1988  		thr_node = thr_node->next;
1989  		idalloc(tsd, thr_old_node);
1990  	}
1991  	emitter_json_array_end(emitter);
1992  }
1993  static void
1994  prof_log_emit_traces(tsd_t *tsd, emitter_t *emitter) {
1995  	emitter_json_array_kv_begin(emitter, "stack_traces");
1996  	prof_bt_node_t *bt_node = log_bt_first;
1997  	prof_bt_node_t *bt_old_node;
1998  	char buf[2 * sizeof(intptr_t) + 3];
1999  	size_t buf_sz = sizeof(buf);
2000  	while (bt_node != NULL) {
2001  		emitter_json_array_begin(emitter);
2002  		size_t i;
2003  		for (i = 0; i < bt_node->bt.len; i++) {
2004  			malloc_snprintf(buf, buf_sz, "%p", bt_node->bt.vec[i]);
2005  			char *trace_str = buf;
2006  			emitter_json_value(emitter, emitter_type_string,
2007  			    &trace_str);
2008  		}
2009  		emitter_json_array_end(emitter);
2010  		bt_old_node = bt_node;
2011  		bt_node = bt_node->next;
2012  		idalloc(tsd, bt_old_node);
2013  	}
2014  	emitter_json_array_end(emitter);
2015  }
2016  static void
2017  prof_log_emit_allocs(tsd_t *tsd, emitter_t *emitter) {
2018  	emitter_json_array_kv_begin(emitter, "allocations");
2019  	prof_alloc_node_t *alloc_node = log_alloc_first;
2020  	prof_alloc_node_t *alloc_old_node;
2021  	while (alloc_node != NULL) {
2022  		emitter_json_object_begin(emitter);
2023  		emitter_json_kv(emitter, "alloc_thread", emitter_type_size,
2024  		    &alloc_node->alloc_thr_ind);
2025  		emitter_json_kv(emitter, "free_thread", emitter_type_size,
2026  		    &alloc_node->free_thr_ind);
2027  		emitter_json_kv(emitter, "alloc_trace", emitter_type_size,
2028  		    &alloc_node->alloc_bt_ind);
2029  		emitter_json_kv(emitter, "free_trace", emitter_type_size,
2030  		    &alloc_node->free_bt_ind);
2031  		emitter_json_kv(emitter, "alloc_timestamp",
2032  		    emitter_type_uint64, &alloc_node->alloc_time_ns);
2033  		emitter_json_kv(emitter, "free_timestamp", emitter_type_uint64,
2034  		    &alloc_node->free_time_ns);
2035  		emitter_json_kv(emitter, "usize", emitter_type_uint64,
2036  		    &alloc_node->usize);
2037  		emitter_json_object_end(emitter);
2038  		alloc_old_node = alloc_node;
2039  		alloc_node = alloc_node->next;
2040  		idalloc(tsd, alloc_old_node);
2041  	}
2042  	emitter_json_array_end(emitter);
2043  }
2044  static void
2045  prof_log_emit_metadata(emitter_t *emitter) {
2046  	emitter_json_object_kv_begin(emitter, "info");
2047  	nstime_t now = NSTIME_ZERO_INITIALIZER;
2048  	nstime_update(&now);
2049  	uint64_t ns = nstime_ns(&now) - nstime_ns(&log_start_timestamp);
2050  	emitter_json_kv(emitter, "duration", emitter_type_uint64, &ns);
2051  	char *vers = JEMALLOC_VERSION;
2052  	emitter_json_kv(emitter, "version",
2053  	    emitter_type_string, &vers);
2054  	emitter_json_kv(emitter, "lg_sample_rate",
2055  	    emitter_type_int, &lg_prof_sample);
2056  	int pid = prof_getpid();
2057  	emitter_json_kv(emitter, "pid", emitter_type_int, &pid);
2058  	emitter_json_object_end(emitter);
2059  }
2060  bool
2061  prof_log_stop(tsdn_t *tsdn) {
2062  	if (!opt_prof || !prof_booted) {
2063  		return true;
2064  	}
2065  	tsd_t *tsd = tsdn_tsd(tsdn);
2066  	malloc_mutex_lock(tsdn, &log_mtx);
2067  	if (prof_logging_state != prof_logging_state_started) {
2068  		malloc_mutex_unlock(tsdn, &log_mtx);
2069  		return true;
2070  	}
2071  	prof_logging_state = prof_logging_state_dumping;
2072  	malloc_mutex_unlock(tsdn, &log_mtx);
2073  	emitter_t emitter;
2074  	int fd;
2075  #ifdef JEMALLOC_JET
2076  	if (prof_log_dummy) {
2077  		fd = 0;
2078  	} else {
2079  		fd = creat(log_filename, 0644);
2080  	}
2081  #else
2082  	fd = creat(log_filename, 0644);
2083  #endif
2084  	if (fd == -1) {
2085  		malloc_printf("<jemalloc>: creat() for log file \"%s\" "
2086  			      " failed with %d\n", log_filename, errno);
2087  		if (opt_abort) {
2088  			abort();
2089  		}
2090  		return true;
2091  	}
2092  	struct prof_emitter_cb_arg_s arg;
2093  	arg.fd = fd;
2094  	emitter_init(&emitter, emitter_output_json, &prof_emitter_write_cb,
2095  	    (void *)(&arg));
2096  	emitter_begin(&emitter);
2097  	prof_log_emit_metadata(&emitter);
2098  	prof_log_emit_threads(tsd, &emitter);
2099  	prof_log_emit_traces(tsd, &emitter);
2100  	prof_log_emit_allocs(tsd, &emitter);
2101  	emitter_end(&emitter);
2102  	if (log_tables_initialized) {
2103  		ckh_delete(tsd, &log_bt_node_set);
2104  		ckh_delete(tsd, &log_thr_node_set);
2105  	}
2106  	log_tables_initialized = false;
2107  	log_bt_index = 0;
2108  	log_thr_index = 0;
2109  	log_bt_first = NULL;
2110  	log_bt_last = NULL;
2111  	log_thr_first = NULL;
2112  	log_thr_last = NULL;
2113  	log_alloc_first = NULL;
2114  	log_alloc_last = NULL;
2115  	malloc_mutex_lock(tsdn, &log_mtx);
2116  	prof_logging_state = prof_logging_state_stopped;
2117  	malloc_mutex_unlock(tsdn, &log_mtx);
2118  #ifdef JEMALLOC_JET
2119  	if (prof_log_dummy) {
2120  		return false;
2121  	}
2122  #endif
2123  	return close(fd);
2124  }
2125  const char *
2126  prof_thread_name_get(tsd_t *tsd) {
2127  	prof_tdata_t *tdata;
2128  	tdata = prof_tdata_get(tsd, true);
2129  	if (tdata == NULL) {
2130  		return "";
2131  	}
2132  	return (tdata->thread_name != NULL ? tdata->thread_name : "");
2133  }
2134  static char *
2135  prof_thread_name_alloc(tsdn_t *tsdn, const char *thread_name) {
2136  	char *ret;
2137  	size_t size;
2138  	if (thread_name == NULL) {
2139  		return NULL;
2140  	}
2141  	size = strlen(thread_name) + 1;
2142  	if (size == 1) {
2143  		return "";
2144  	}
2145  	ret = iallocztm(tsdn, size, sz_size2index(size), false, NULL, true,
2146  	    arena_get(TSDN_NULL, 0, true), true);
2147  	if (ret == NULL) {
2148  		return NULL;
2149  	}
2150  	memcpy(ret, thread_name, size);
2151  	return ret;
2152  }
2153  int
2154  prof_thread_name_set(tsd_t *tsd, const char *thread_name) {
2155  	prof_tdata_t *tdata;
2156  	unsigned i;
2157  	char *s;
2158  	tdata = prof_tdata_get(tsd, true);
2159  	if (tdata == NULL) {
2160  		return EAGAIN;
2161  	}
2162  	if (thread_name == NULL) {
2163  		return EFAULT;
2164  	}
2165  	for (i = 0; thread_name[i] != '\0'; i++) {
2166  		char c = thread_name[i];
2167  		if (!isgraph(c) && !isblank(c)) {
2168  			return EFAULT;
2169  		}
2170  	}
2171  	s = prof_thread_name_alloc(tsd_tsdn(tsd), thread_name);
2172  	if (s == NULL) {
2173  		return EAGAIN;
2174  	}
2175  	if (tdata->thread_name != NULL) {
2176  		idalloctm(tsd_tsdn(tsd), tdata->thread_name, NULL, NULL, true,
2177  		    true);
2178  		tdata->thread_name = NULL;
2179  	}
2180  	if (strlen(s) > 0) {
2181  		tdata->thread_name = s;
2182  	}
2183  	return 0;
2184  }
2185  bool
2186  prof_thread_active_get(tsd_t *tsd) {
2187  	prof_tdata_t *tdata;
2188  	tdata = prof_tdata_get(tsd, true);
2189  	if (tdata == NULL) {
2190  		return false;
2191  	}
2192  	return tdata->active;
2193  }
2194  bool
2195  prof_thread_active_set(tsd_t *tsd, bool active) {
2196  	prof_tdata_t *tdata;
2197  	tdata = prof_tdata_get(tsd, true);
2198  	if (tdata == NULL) {
2199  		return true;
2200  	}
2201  	tdata->active = active;
2202  	return false;
2203  }
2204  bool
2205  prof_thread_active_init_get(tsdn_t *tsdn) {
2206  	bool active_init;
2207  	malloc_mutex_lock(tsdn, &prof_thread_active_init_mtx);
2208  	active_init = prof_thread_active_init;
2209  	malloc_mutex_unlock(tsdn, &prof_thread_active_init_mtx);
2210  	return active_init;
2211  }
2212  bool
2213  prof_thread_active_init_set(tsdn_t *tsdn, bool active_init) {
2214  	bool active_init_old;
2215  	malloc_mutex_lock(tsdn, &prof_thread_active_init_mtx);
2216  	active_init_old = prof_thread_active_init;
2217  	prof_thread_active_init = active_init;
2218  	malloc_mutex_unlock(tsdn, &prof_thread_active_init_mtx);
2219  	return active_init_old;
2220  }
2221  bool
2222  prof_gdump_get(tsdn_t *tsdn) {
2223  	bool prof_gdump_current;
2224  	malloc_mutex_lock(tsdn, &prof_gdump_mtx);
2225  	prof_gdump_current = prof_gdump_val;
2226  	malloc_mutex_unlock(tsdn, &prof_gdump_mtx);
2227  	return prof_gdump_current;
2228  }
2229  bool
2230  prof_gdump_set(tsdn_t *tsdn, bool gdump) {
2231  	bool prof_gdump_old;
2232  	malloc_mutex_lock(tsdn, &prof_gdump_mtx);
2233  	prof_gdump_old = prof_gdump_val;
2234  	prof_gdump_val = gdump;
2235  	malloc_mutex_unlock(tsdn, &prof_gdump_mtx);
2236  	return prof_gdump_old;
2237  }
2238  void
2239  prof_boot0(void) {
2240  	cassert(config_prof);
2241  	memcpy(opt_prof_prefix, PROF_PREFIX_DEFAULT,
2242  	    sizeof(PROF_PREFIX_DEFAULT));
2243  }
2244  void
2245  prof_boot1(void) {
2246  	cassert(config_prof);
2247  	if (opt_prof_leak && !opt_prof) {
2248  		opt_prof = true;
2249  		opt_prof_gdump = false;
2250  	} else if (opt_prof) {
2251  		if (opt_lg_prof_interval >= 0) {
2252  			prof_interval = (((uint64_t)1U) <<
2253  			    opt_lg_prof_interval);
2254  		}
2255  	}
2256  }
2257  bool
2258  prof_boot2(tsd_t *tsd) {
2259  	cassert(config_prof);
2260  	if (opt_prof) {
2261  		unsigned i;
2262  		lg_prof_sample = opt_lg_prof_sample;
2263  		prof_active = opt_prof_active;
2264  		if (malloc_mutex_init(&prof_active_mtx, "prof_active",
2265  		    WITNESS_RANK_PROF_ACTIVE, malloc_mutex_rank_exclusive)) {
2266  			return true;
2267  		}
2268  		prof_gdump_val = opt_prof_gdump;
2269  		if (malloc_mutex_init(&prof_gdump_mtx, "prof_gdump",
2270  		    WITNESS_RANK_PROF_GDUMP, malloc_mutex_rank_exclusive)) {
2271  			return true;
2272  		}
2273  		prof_thread_active_init = opt_prof_thread_active_init;
2274  		if (malloc_mutex_init(&prof_thread_active_init_mtx,
2275  		    "prof_thread_active_init",
2276  		    WITNESS_RANK_PROF_THREAD_ACTIVE_INIT,
2277  		    malloc_mutex_rank_exclusive)) {
2278  			return true;
2279  		}
2280  		if (ckh_new(tsd, &bt2gctx, PROF_CKH_MINITEMS, prof_bt_hash,
2281  		    prof_bt_keycomp)) {
2282  			return true;
2283  		}
2284  		if (malloc_mutex_init(&bt2gctx_mtx, "prof_bt2gctx",
2285  		    WITNESS_RANK_PROF_BT2GCTX, malloc_mutex_rank_exclusive)) {
2286  			return true;
2287  		}
2288  		tdata_tree_new(&tdatas);
2289  		if (malloc_mutex_init(&tdatas_mtx, "prof_tdatas",
2290  		    WITNESS_RANK_PROF_TDATAS, malloc_mutex_rank_exclusive)) {
2291  			return true;
2292  		}
2293  		next_thr_uid = 0;
2294  		if (malloc_mutex_init(&next_thr_uid_mtx, "prof_next_thr_uid",
2295  		    WITNESS_RANK_PROF_NEXT_THR_UID, malloc_mutex_rank_exclusive)) {
2296  			return true;
2297  		}
2298  		if (malloc_mutex_init(&prof_dump_seq_mtx, "prof_dump_seq",
2299  		    WITNESS_RANK_PROF_DUMP_SEQ, malloc_mutex_rank_exclusive)) {
2300  			return true;
2301  		}
2302  		if (malloc_mutex_init(&prof_dump_mtx, "prof_dump",
2303  		    WITNESS_RANK_PROF_DUMP, malloc_mutex_rank_exclusive)) {
2304  			return true;
2305  		}
2306  		if (opt_prof_final && opt_prof_prefix[0] != '\0' &&
2307  		    atexit(prof_fdump) != 0) {
2308  			malloc_write("<jemalloc>: Error in atexit()\n");
2309  			if (opt_abort) {
2310  				abort();
2311  			}
2312  		}
2313  		if (opt_prof_log) {
2314  			prof_log_start(tsd_tsdn(tsd), NULL);
2315  		}
2316  		if (atexit(prof_log_stop_final) != 0) {
2317  			malloc_write("<jemalloc>: Error in atexit() "
2318  				     "for logging\n");
2319  			if (opt_abort) {
2320  				abort();
2321  			}
2322  		}
2323  		if (malloc_mutex_init(&log_mtx, "prof_log",
2324  		    WITNESS_RANK_PROF_LOG, malloc_mutex_rank_exclusive)) {
2325  			return true;
2326  		}
2327  		if (ckh_new(tsd, &log_bt_node_set, PROF_CKH_MINITEMS,
2328  		    prof_bt_node_hash, prof_bt_node_keycomp)) {
2329  			return true;
2330  		}
2331  		if (ckh_new(tsd, &log_thr_node_set, PROF_CKH_MINITEMS,
2332  		    prof_thr_node_hash, prof_thr_node_keycomp)) {
2333  			return true;
2334  		}
2335  		log_tables_initialized = true;
2336  		gctx_locks = (malloc_mutex_t *)base_alloc(tsd_tsdn(tsd),
2337  		    b0get(), PROF_NCTX_LOCKS * sizeof(malloc_mutex_t),
2338  		    CACHELINE);
2339  		if (gctx_locks == NULL) {
2340  			return true;
2341  		}
2342  		for (i = 0; i < PROF_NCTX_LOCKS; i++) {
2343  			if (malloc_mutex_init(&gctx_locks[i], "prof_gctx",
2344  			    WITNESS_RANK_PROF_GCTX,
2345  			    malloc_mutex_rank_exclusive)) {
2346  				return true;
2347  			}
2348  		}
2349  		tdata_locks = (malloc_mutex_t *)base_alloc(tsd_tsdn(tsd),
2350  		    b0get(), PROF_NTDATA_LOCKS * sizeof(malloc_mutex_t),
2351  		    CACHELINE);
2352  		if (tdata_locks == NULL) {
2353  			return true;
2354  		}
2355  		for (i = 0; i < PROF_NTDATA_LOCKS; i++) {
2356  			if (malloc_mutex_init(&tdata_locks[i], "prof_tdata",
2357  			    WITNESS_RANK_PROF_TDATA,
2358  			    malloc_mutex_rank_exclusive)) {
2359  				return true;
2360  			}
2361  		}
2362  #ifdef JEMALLOC_PROF_LIBGCC
2363  		_Unwind_Backtrace(prof_unwind_init_callback, NULL);
2364  #endif
2365  	}
2366  	prof_booted = true;
2367  	return false;
2368  }
2369  void
2370  prof_prefork0(tsdn_t *tsdn) {
2371  	if (config_prof && opt_prof) {
2372  		unsigned i;
2373  		malloc_mutex_prefork(tsdn, &prof_dump_mtx);
2374  		malloc_mutex_prefork(tsdn, &bt2gctx_mtx);
2375  		malloc_mutex_prefork(tsdn, &tdatas_mtx);
2376  		for (i = 0; i < PROF_NTDATA_LOCKS; i++) {
2377  			malloc_mutex_prefork(tsdn, &tdata_locks[i]);
2378  		}
2379  		for (i = 0; i < PROF_NCTX_LOCKS; i++) {
2380  			malloc_mutex_prefork(tsdn, &gctx_locks[i]);
2381  		}
2382  	}
2383  }
2384  void
2385  prof_prefork1(tsdn_t *tsdn) {
2386  	if (config_prof && opt_prof) {
2387  		malloc_mutex_prefork(tsdn, &prof_active_mtx);
2388  		malloc_mutex_prefork(tsdn, &prof_dump_seq_mtx);
2389  		malloc_mutex_prefork(tsdn, &prof_gdump_mtx);
2390  		malloc_mutex_prefork(tsdn, &next_thr_uid_mtx);
2391  		malloc_mutex_prefork(tsdn, &prof_thread_active_init_mtx);
2392  	}
2393  }
2394  void
2395  prof_postfork_parent(tsdn_t *tsdn) {
2396  	if (config_prof && opt_prof) {
2397  		unsigned i;
2398  		malloc_mutex_postfork_parent(tsdn,
2399  		    &prof_thread_active_init_mtx);
2400  		malloc_mutex_postfork_parent(tsdn, &next_thr_uid_mtx);
2401  		malloc_mutex_postfork_parent(tsdn, &prof_gdump_mtx);
2402  		malloc_mutex_postfork_parent(tsdn, &prof_dump_seq_mtx);
2403  		malloc_mutex_postfork_parent(tsdn, &prof_active_mtx);
2404  		for (i = 0; i < PROF_NCTX_LOCKS; i++) {
2405  			malloc_mutex_postfork_parent(tsdn, &gctx_locks[i]);
2406  		}
2407  		for (i = 0; i < PROF_NTDATA_LOCKS; i++) {
2408  			malloc_mutex_postfork_parent(tsdn, &tdata_locks[i]);
2409  		}
2410  		malloc_mutex_postfork_parent(tsdn, &tdatas_mtx);
2411  		malloc_mutex_postfork_parent(tsdn, &bt2gctx_mtx);
2412  		malloc_mutex_postfork_parent(tsdn, &prof_dump_mtx);
2413  	}
2414  }
2415  void
2416  prof_postfork_child(tsdn_t *tsdn) {
2417  	if (config_prof && opt_prof) {
2418  		unsigned i;
2419  		malloc_mutex_postfork_child(tsdn, &prof_thread_active_init_mtx);
2420  		malloc_mutex_postfork_child(tsdn, &next_thr_uid_mtx);
2421  		malloc_mutex_postfork_child(tsdn, &prof_gdump_mtx);
2422  		malloc_mutex_postfork_child(tsdn, &prof_dump_seq_mtx);
2423  		malloc_mutex_postfork_child(tsdn, &prof_active_mtx);
2424  		for (i = 0; i < PROF_NCTX_LOCKS; i++) {
2425  			malloc_mutex_postfork_child(tsdn, &gctx_locks[i]);
2426  		}
2427  		for (i = 0; i < PROF_NTDATA_LOCKS; i++) {
2428  			malloc_mutex_postfork_child(tsdn, &tdata_locks[i]);
2429  		}
2430  		malloc_mutex_postfork_child(tsdn, &tdatas_mtx);
2431  		malloc_mutex_postfork_child(tsdn, &bt2gctx_mtx);
2432  		malloc_mutex_postfork_child(tsdn, &prof_dump_mtx);
2433  	}
2434  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from Adafruit_nRF52_Arduino-MDEwOlJlcG9zaXRvcnk3NDM1NDcyOQ==-flat-event_groups.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-prof.c</div>
                </div>
                <div class="column column_space"><pre><code>138  				uxReturn = pxEventBits->uxEventBits;
139  				if( ( uxReturn & uxBitsToWaitFor ) == uxBitsToWaitFor )
140  				{
141  					pxEventBits->uxEventBits &= ~uxBitsToWaitFor;
142  				}
143  				else
144  				{
145  					mtCOVERAGE_TEST_MARKER();
146  				}
147  			}
</pre></code></div>
                <div class="column column_space"><pre><code>1113  		gctx->nlimbo--;
1114  		if (prof_gctx_should_destroy(gctx)) {
1115  			gctx->nlimbo++;
1116  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
1117  			prof_gctx_try_destroy(tsd, tdata, gctx, tdata);
1118  		} else {
1119  			malloc_mutex_unlock(tsd_tsdn(tsd), gctx->lock);
1120  		}
1121  	}
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    