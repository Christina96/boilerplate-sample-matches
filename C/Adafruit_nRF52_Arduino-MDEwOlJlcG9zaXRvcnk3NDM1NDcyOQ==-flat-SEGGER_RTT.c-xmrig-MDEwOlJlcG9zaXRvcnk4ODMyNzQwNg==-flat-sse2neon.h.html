
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 1.6497461928934012%, Tokens: 8</h2>
        <div class="column">
            <h3>Adafruit_nRF52_Arduino-MDEwOlJlcG9zaXRvcnk3NDM1NDcyOQ==-flat-SEGGER_RTT.c</h3>
            <pre><code>1  #include "SEGGER_RTT.h"
2  #include <string.h>                 
3  #ifndef   BUFFER_SIZE_UP
4    #define BUFFER_SIZE_UP                                  1024  
5  #endif
6  #ifndef   BUFFER_SIZE_DOWN
7    #define BUFFER_SIZE_DOWN                                16    
8  #endif
9  #ifndef   SEGGER_RTT_MAX_NUM_UP_BUFFERS
10    #define SEGGER_RTT_MAX_NUM_UP_BUFFERS                    2    
11  #endif
12  #ifndef   SEGGER_RTT_MAX_NUM_DOWN_BUFFERS
13    #define SEGGER_RTT_MAX_NUM_DOWN_BUFFERS                  2    
14  #endif
15  #ifndef SEGGER_RTT_BUFFER_SECTION
16    #if defined(SEGGER_RTT_SECTION)
17      #define SEGGER_RTT_BUFFER_SECTION SEGGER_RTT_SECTION
18    #endif
19  #endif
20  #ifndef   SEGGER_RTT_ALIGNMENT
21    #define SEGGER_RTT_ALIGNMENT                            0
22  #endif
23  #ifndef   SEGGER_RTT_BUFFER_ALIGNMENT
24    #define SEGGER_RTT_BUFFER_ALIGNMENT                     0
25  #endif
26  #ifndef   SEGGER_RTT_MODE_DEFAULT
27    #define SEGGER_RTT_MODE_DEFAULT                         SEGGER_RTT_MODE_NO_BLOCK_SKIP
28  #endif
29  #ifndef   SEGGER_RTT_LOCK
30    #define SEGGER_RTT_LOCK()
31  #endif
32  #ifndef   SEGGER_RTT_UNLOCK
33    #define SEGGER_RTT_UNLOCK()
34  #endif
35  #ifndef   STRLEN
36    #define STRLEN(a)                                       strlen((a))
37  #endif
38  #ifndef   STRCPY
39    #define STRCPY(pDest, pSrc, NumBytes)                   strcpy((pDest), (pSrc))
40  #endif
41  #ifndef   SEGGER_RTT_MEMCPY_USE_BYTELOOP
42    #define SEGGER_RTT_MEMCPY_USE_BYTELOOP                  0
43  #endif
44  #ifndef   SEGGER_RTT_MEMCPY
45    #ifdef  MEMCPY
46      #define SEGGER_RTT_MEMCPY(pDest, pSrc, NumBytes)      MEMCPY((pDest), (pSrc), (NumBytes))
47    #else
48      #define SEGGER_RTT_MEMCPY(pDest, pSrc, NumBytes)      memcpy((pDest), (pSrc), (NumBytes))
49    #endif
50  #endif
51  #ifndef   MIN
52    #define MIN(a, b)         (((a) < (b)) ? (a) : (b))
53  #endif
54  #ifndef   MAX
55    #define MAX(a, b)         (((a) > (b)) ? (a) : (b))
56  #endif
57  #ifndef NULL
58    #define NULL 0
59  #endif
60  #if (defined __ICCARM__) || (defined __ICCRX__)
61    #define RTT_PRAGMA(P) _Pragma(#P)
62  #endif
63  #if SEGGER_RTT_ALIGNMENT || SEGGER_RTT_BUFFER_ALIGNMENT
64    #if (defined __GNUC__)
65      #define SEGGER_RTT_ALIGN(Var, Alignment) Var __attribute__ ((aligned (Alignment)))
66    #elif (defined __ICCARM__) || (defined __ICCRX__)
67      #define PRAGMA(A) _Pragma(#A)
68  #define SEGGER_RTT_ALIGN(Var, Alignment) RTT_PRAGMA(data_alignment=Alignment) \
69                                    Var
70    #elif (defined __CC_ARM)
71      #define SEGGER_RTT_ALIGN(Var, Alignment) Var __attribute__ ((aligned (Alignment)))
72    #else
73      #error "Alignment not supported for this compiler."
74    #endif
75  #else
76    #define SEGGER_RTT_ALIGN(Var, Alignment) Var
77  #endif
78  #if defined(SEGGER_RTT_SECTION) || defined (SEGGER_RTT_BUFFER_SECTION)
79    #if (defined __GNUC__)
80      #define SEGGER_RTT_PUT_SECTION(Var, Section) __attribute__ ((section (Section))) Var
81    #elif (defined __ICCARM__) || (defined __ICCRX__)
82  #define SEGGER_RTT_PUT_SECTION(Var, Section) RTT_PRAGMA(location=Section) \
83                                          Var
84    #elif (defined __CC_ARM)
85      #define SEGGER_RTT_PUT_SECTION(Var, Section) __attribute__ ((section (Section), zero_init))  Var
86    #else
87      #error "Section placement not supported for this compiler."
88    #endif
89  #else
90    #define SEGGER_RTT_PUT_SECTION(Var, Section) Var
91  #endif
92  #if SEGGER_RTT_ALIGNMENT
93    #define SEGGER_RTT_CB_ALIGN(Var)  SEGGER_RTT_ALIGN(Var, SEGGER_RTT_ALIGNMENT)
94  #else
95    #define SEGGER_RTT_CB_ALIGN(Var)  Var
96  #endif
97  #if SEGGER_RTT_BUFFER_ALIGNMENT
98    #define SEGGER_RTT_BUFFER_ALIGN(Var)  SEGGER_RTT_ALIGN(Var, SEGGER_RTT_BUFFER_ALIGNMENT)
99  #else
100    #define SEGGER_RTT_BUFFER_ALIGN(Var)  Var
101  #endif
102  #if defined(SEGGER_RTT_SECTION)
103    #define SEGGER_RTT_PUT_CB_SECTION(Var) SEGGER_RTT_PUT_SECTION(Var, SEGGER_RTT_SECTION)
104  #else
105    #define SEGGER_RTT_PUT_CB_SECTION(Var) Var
106  #endif
107  #if defined(SEGGER_RTT_BUFFER_SECTION)
108    #define SEGGER_RTT_PUT_BUFFER_SECTION(Var) SEGGER_RTT_PUT_SECTION(Var, SEGGER_RTT_BUFFER_SECTION)
109  #else
110    #define SEGGER_RTT_PUT_BUFFER_SECTION(Var) Var
111  #endif
112  static unsigned char _aTerminalId[16] = { '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F' };
113  SEGGER_RTT_PUT_CB_SECTION(SEGGER_RTT_CB_ALIGN(SEGGER_RTT_CB _SEGGER_RTT));
114  SEGGER_RTT_PUT_BUFFER_SECTION(SEGGER_RTT_BUFFER_ALIGN(static char _acUpBuffer  [BUFFER_SIZE_UP]));
115  SEGGER_RTT_PUT_BUFFER_SECTION(SEGGER_RTT_BUFFER_ALIGN(static char _acDownBuffer[BUFFER_SIZE_DOWN]));
116  static unsigned char _ActiveTerminal;
117  #define INIT()  do {                                            \
118                    if (_SEGGER_RTT.acID[0] == '\0') { _DoInit(); }  \
119                  } while (0)
120  static void _DoInit(void) {
121    SEGGER_RTT_CB* p;
122    p = &_SEGGER_RTT;
123    p->MaxNumUpBuffers    = SEGGER_RTT_MAX_NUM_UP_BUFFERS;
124    p->MaxNumDownBuffers  = SEGGER_RTT_MAX_NUM_DOWN_BUFFERS;
125    p->aUp[0].sName         = "Terminal";
126    p->aUp[0].pBuffer       = _acUpBuffer;
127    p->aUp[0].SizeOfBuffer  = sizeof(_acUpBuffer);
128    p->aUp[0].RdOff         = 0u;
129    p->aUp[0].WrOff         = 0u;
130    p->aUp[0].Flags         = SEGGER_RTT_MODE_DEFAULT;
131    p->aDown[0].sName         = "Terminal";
132    p->aDown[0].pBuffer       = _acDownBuffer;
133    p->aDown[0].SizeOfBuffer  = sizeof(_acDownBuffer);
134    p->aDown[0].RdOff         = 0u;
135    p->aDown[0].WrOff         = 0u;
136    p->aDown[0].Flags         = SEGGER_RTT_MODE_DEFAULT;
137    STRCPY(&p->acID[7], "RTT", 9);
138    STRCPY(&p->acID[0], "SEGGER", 7);
139    p->acID[6] = ' ';
140  }
141  static unsigned _WriteBlocking(SEGGER_RTT_BUFFER_UP* pRing, const char* pBuffer, unsigned NumBytes) {
142    unsigned NumBytesToWrite;
143    unsigned NumBytesWritten;
144    unsigned RdOff;
145    unsigned WrOff;
146  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
147    char*    pDst;
148  #endif
149    NumBytesWritten = 0u;
150    WrOff = pRing->WrOff;
151    do {
152      RdOff = pRing->RdOff;                         
153      if (RdOff > WrOff) {
154        NumBytesToWrite = RdOff - WrOff - 1u;
155      } else {
156        NumBytesToWrite = pRing->SizeOfBuffer - (WrOff - RdOff + 1u);
157      }
158      NumBytesToWrite = MIN(NumBytesToWrite, (pRing->SizeOfBuffer - WrOff));      
159      NumBytesToWrite = MIN(NumBytesToWrite, NumBytes);
160  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
161      pDst = pRing->pBuffer + WrOff;
162      NumBytesWritten += NumBytesToWrite;
163      NumBytes        -= NumBytesToWrite;
164      WrOff           += NumBytesToWrite;
165      while (NumBytesToWrite--) {
166        *pDst++ = *pBuffer++;
167      };
168  #else
169      SEGGER_RTT_MEMCPY(pRing->pBuffer + WrOff, pBuffer, NumBytesToWrite);
170      NumBytesWritten += NumBytesToWrite;
171      pBuffer         += NumBytesToWrite;
172      NumBytes        -= NumBytesToWrite;
173      WrOff           += NumBytesToWrite;
174  #endif
175      if (WrOff == pRing->SizeOfBuffer) {
176        WrOff = 0u;
177      }
178      pRing->WrOff = WrOff;
179    } while (NumBytes);
180    return NumBytesWritten;
181  }
182  static void _WriteNoCheck(SEGGER_RTT_BUFFER_UP* pRing, const char* pData, unsigned NumBytes) {
183    unsigned NumBytesAtOnce;
184    unsigned WrOff;
185    unsigned Rem;
186  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
187    char*    pDst;
188  #endif
189    WrOff = pRing->WrOff;
190    Rem = pRing->SizeOfBuffer - WrOff;
191    if (Rem > NumBytes) {
192  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
193      pDst = pRing->pBuffer + WrOff;
194      WrOff += NumBytes;
195      while (NumBytes--) {
196        *pDst++ = *pData++;
197      };
198      pRing->WrOff = WrOff;
199  #else
200      SEGGER_RTT_MEMCPY(pRing->pBuffer + WrOff, pData, NumBytes);
201      pRing->WrOff = WrOff + NumBytes;
202  #endif
203    } else {
204  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
205      pDst = pRing->pBuffer + WrOff;
206      NumBytesAtOnce = Rem;
207      while (NumBytesAtOnce--) {
208        *pDst++ = *pData++;
209      };
210      pDst = pRing->pBuffer;
211      NumBytesAtOnce = NumBytes - Rem;
212      while (NumBytesAtOnce--) {
213        *pDst++ = *pData++;
214      };
215      pRing->WrOff = NumBytes - Rem;
216  #else
217      NumBytesAtOnce = Rem;
218      SEGGER_RTT_MEMCPY(pRing->pBuffer + WrOff, pData, NumBytesAtOnce);
219      NumBytesAtOnce = NumBytes - Rem;
220      SEGGER_RTT_MEMCPY(pRing->pBuffer, pData + Rem, NumBytesAtOnce);
221      pRing->WrOff = NumBytesAtOnce;
222  #endif
223    }
224  }
225  static void _PostTerminalSwitch(SEGGER_RTT_BUFFER_UP* pRing, unsigned char TerminalId) {
226    unsigned char ac[2];
227    ac[0] = 0xFFu;
228    ac[1] = _aTerminalId[TerminalId];  
229    _WriteBlocking(pRing, (const char*)ac, 2u);
230  }
231  static unsigned _GetAvailWriteSpace(SEGGER_RTT_BUFFER_UP* pRing) {
232    unsigned RdOff;
233    unsigned WrOff;
234    unsigned r;
235    RdOff = pRing->RdOff;
236    WrOff = pRing->WrOff;
237    if (RdOff <= WrOff) {
238      r = pRing->SizeOfBuffer - 1u - WrOff + RdOff;
239    } else {
240      r = RdOff - WrOff - 1u;
241    }
242    return r;
243  }
244  unsigned SEGGER_RTT_ReadUpBufferNoLock(unsigned BufferIndex, void* pData, unsigned BufferSize) {
245    unsigned                NumBytesRem;
246    unsigned                NumBytesRead;
247    unsigned                RdOff;
248    unsigned                WrOff;
249    unsigned char*          pBuffer;
250    SEGGER_RTT_BUFFER_UP*   pRing;
251  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
252    const char*             pSrc;
253  #endif
254    INIT();
255    pRing = &_SEGGER_RTT.aUp[BufferIndex];
256    pBuffer = (unsigned char*)pData;
257    RdOff = pRing->RdOff;
258    WrOff = pRing->WrOff;
259    NumBytesRead = 0u;
<span onclick='openModal()' class='match'>260    if (RdOff > WrOff) {
261      NumBytesRem = pRing->SizeOfBuffer - RdOff;
262      NumBytesRem = MIN(NumBytesRem, BufferSize);
263  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
264      pSrc = pRing->pBuffer + RdOff;
265      NumBytesRead += NumBytesRem;
266      BufferSize   -= NumBytesRem;
267      RdOff        += NumBytesRem;
</span>268      while (NumBytesRem--) {
269        *pBuffer++ = *pSrc++;
270      };
271  #else
272      SEGGER_RTT_MEMCPY(pBuffer, pRing->pBuffer + RdOff, NumBytesRem);
273      NumBytesRead += NumBytesRem;
274      pBuffer      += NumBytesRem;
275      BufferSize   -= NumBytesRem;
276      RdOff        += NumBytesRem;
277  #endif
278      if (RdOff == pRing->SizeOfBuffer) {
279        RdOff = 0u;
280      }
281    }
282    NumBytesRem = WrOff - RdOff;
283    NumBytesRem = MIN(NumBytesRem, BufferSize);
284    if (NumBytesRem > 0u) {
285  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
286      pSrc = pRing->pBuffer + RdOff;
287      NumBytesRead += NumBytesRem;
288      BufferSize   -= NumBytesRem;
289      RdOff        += NumBytesRem;
290      while (NumBytesRem--) {
291        *pBuffer++ = *pSrc++;
292      };
293  #else
294      SEGGER_RTT_MEMCPY(pBuffer, pRing->pBuffer + RdOff, NumBytesRem);
295      NumBytesRead += NumBytesRem;
296      pBuffer      += NumBytesRem;
297      BufferSize   -= NumBytesRem;
298      RdOff        += NumBytesRem;
299  #endif
300    }
301    if (NumBytesRead) {
302      pRing->RdOff = RdOff;
303    }
304    return NumBytesRead;
305  }
306  unsigned SEGGER_RTT_ReadNoLock(unsigned BufferIndex, void* pData, unsigned BufferSize) {
307    unsigned                NumBytesRem;
308    unsigned                NumBytesRead;
309    unsigned                RdOff;
310    unsigned                WrOff;
311    unsigned char*          pBuffer;
312    SEGGER_RTT_BUFFER_DOWN* pRing;
313  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
314    const char*             pSrc;
315  #endif
316    INIT();
317    pRing = &_SEGGER_RTT.aDown[BufferIndex];
318    pBuffer = (unsigned char*)pData;
319    RdOff = pRing->RdOff;
320    WrOff = pRing->WrOff;
321    NumBytesRead = 0u;
322    if (RdOff > WrOff) {
323      NumBytesRem = pRing->SizeOfBuffer - RdOff;
324      NumBytesRem = MIN(NumBytesRem, BufferSize);
325  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
326      pSrc = pRing->pBuffer + RdOff;
327      NumBytesRead += NumBytesRem;
328      BufferSize   -= NumBytesRem;
329      RdOff        += NumBytesRem;
330      while (NumBytesRem--) {
331        *pBuffer++ = *pSrc++;
332      };
333  #else
334      SEGGER_RTT_MEMCPY(pBuffer, pRing->pBuffer + RdOff, NumBytesRem);
335      NumBytesRead += NumBytesRem;
336      pBuffer      += NumBytesRem;
337      BufferSize   -= NumBytesRem;
338      RdOff        += NumBytesRem;
339  #endif
340      if (RdOff == pRing->SizeOfBuffer) {
341        RdOff = 0u;
342      }
343    }
344    NumBytesRem = WrOff - RdOff;
345    NumBytesRem = MIN(NumBytesRem, BufferSize);
346    if (NumBytesRem > 0u) {
347  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
348      pSrc = pRing->pBuffer + RdOff;
349      NumBytesRead += NumBytesRem;
350      BufferSize   -= NumBytesRem;
351      RdOff        += NumBytesRem;
352      while (NumBytesRem--) {
353        *pBuffer++ = *pSrc++;
354      };
355  #else
356      SEGGER_RTT_MEMCPY(pBuffer, pRing->pBuffer + RdOff, NumBytesRem);
357      NumBytesRead += NumBytesRem;
358      pBuffer      += NumBytesRem;
359      BufferSize   -= NumBytesRem;
360      RdOff        += NumBytesRem;
361  #endif
362    }
363    if (NumBytesRead) {
364      pRing->RdOff = RdOff;
365    }
366    return NumBytesRead;
367  }
368  unsigned SEGGER_RTT_ReadUpBuffer(unsigned BufferIndex, void* pBuffer, unsigned BufferSize) {
369    unsigned NumBytesRead;
370    SEGGER_RTT_LOCK();
371    NumBytesRead = SEGGER_RTT_ReadUpBufferNoLock(BufferIndex, pBuffer, BufferSize);
372    SEGGER_RTT_UNLOCK();
373    return NumBytesRead;
374  }
375  unsigned SEGGER_RTT_Read(unsigned BufferIndex, void* pBuffer, unsigned BufferSize) {
376    unsigned NumBytesRead;
377    SEGGER_RTT_LOCK();
378    NumBytesRead = SEGGER_RTT_ReadNoLock(BufferIndex, pBuffer, BufferSize);
379    SEGGER_RTT_UNLOCK();
380    return NumBytesRead;
381  }
382  void SEGGER_RTT_WriteWithOverwriteNoLock(unsigned BufferIndex, const void* pBuffer, unsigned NumBytes) {
383    const char*           pData;
384    SEGGER_RTT_BUFFER_UP* pRing;
385    unsigned              Avail;
386  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
387    char*                 pDst;
388  #endif
389    pData = (const char *)pBuffer;
390    pRing = &_SEGGER_RTT.aUp[BufferIndex];
391    if (pRing->WrOff == pRing->RdOff) {
392      Avail = pRing->SizeOfBuffer - 1u;
393    } else if ( pRing->WrOff < pRing->RdOff) {
394      Avail = pRing->RdOff - pRing->WrOff - 1u;
395    } else {
396      Avail = pRing->RdOff - pRing->WrOff - 1u + pRing->SizeOfBuffer;
397    }
398    if (NumBytes > Avail) {
399      pRing->RdOff += (NumBytes - Avail);
400      while (pRing->RdOff >= pRing->SizeOfBuffer) {
401        pRing->RdOff -= pRing->SizeOfBuffer;
402      }
403    }
404    Avail = pRing->SizeOfBuffer - pRing->WrOff;
405    do {
406      if (Avail > NumBytes) {
407  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
408        pDst = pRing->pBuffer + pRing->WrOff;
409        Avail = NumBytes;
410        while (NumBytes--) {
411          *pDst++ = *pData++;
412        };
413        pRing->WrOff += Avail;
414  #else
415        SEGGER_RTT_MEMCPY(pRing->pBuffer + pRing->WrOff, pData, NumBytes);
416        pRing->WrOff += NumBytes;
417  #endif
418        break;
419      } else {
420  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
421        pDst = pRing->pBuffer + pRing->WrOff;
422        NumBytes -= Avail;
423        while (Avail--) {
424          *pDst++ = *pData++;
425        };
426        pRing->WrOff = 0;
427  #else
428        SEGGER_RTT_MEMCPY(pRing->pBuffer + pRing->WrOff, pData, Avail);
429        pData += Avail;
430        pRing->WrOff = 0;
431        NumBytes -= Avail;
432  #endif
433        Avail = (pRing->SizeOfBuffer - 1);
434      }
435    } while (NumBytes);
436  }
437  #if (RTT_USE_ASM == 0)
438  unsigned SEGGER_RTT_WriteSkipNoLock(unsigned BufferIndex, const void* pBuffer, unsigned NumBytes) {
439    const char*           pData;
440    SEGGER_RTT_BUFFER_UP* pRing;
441    unsigned              Avail;
442    unsigned              RdOff;
443    unsigned              WrOff;
444    unsigned              Rem;
445    pData = (const char *)pBuffer;
446    pRing = &_SEGGER_RTT.aUp[BufferIndex];
447    RdOff = pRing->RdOff;
448    WrOff = pRing->WrOff;
449    if (RdOff <= WrOff) {                                 
450      Avail = pRing->SizeOfBuffer - WrOff - 1u;           
451      if (Avail >= NumBytes) {                            
452  CopyStraight:
453        memcpy(pRing->pBuffer + WrOff, pData, NumBytes);
454        pRing->WrOff = WrOff + NumBytes;
455        return 1;
456      }
457      Avail += RdOff;                                     
458      if (Avail >= NumBytes) {                            
459        Rem = pRing->SizeOfBuffer - WrOff;                
460        memcpy(pRing->pBuffer + WrOff, pData, Rem);       
461        NumBytes -= Rem;
462        if (NumBytes) {
463          memcpy(pRing->pBuffer, pData + Rem, NumBytes);
464        }
465        pRing->WrOff = NumBytes;
466        return 1;
467      }
468    } else {                                             
469      Avail = RdOff - WrOff - 1u;
470      if (Avail >= NumBytes) {                           
471        goto CopyStraight;
472      }
473    }
474    return 0;     
475  }
476  #endif
477  unsigned SEGGER_RTT_WriteDownBufferNoLock(unsigned BufferIndex, const void* pBuffer, unsigned NumBytes) {
478    unsigned                Status;
479    unsigned                Avail;
480    const char*             pData;
481    SEGGER_RTT_BUFFER_UP*   pRing;
482    pData = (const char *)pBuffer;
483    pRing = (SEGGER_RTT_BUFFER_UP*)&_SEGGER_RTT.aDown[BufferIndex];
484    switch (pRing->Flags) {
485    case SEGGER_RTT_MODE_NO_BLOCK_SKIP:
486      Avail = _GetAvailWriteSpace(pRing);
487      if (Avail < NumBytes) {
488        Status = 0u;
489      } else {
490        Status = NumBytes;
491        _WriteNoCheck(pRing, pData, NumBytes);
492      }
493      break;
494    case SEGGER_RTT_MODE_NO_BLOCK_TRIM:
495      Avail = _GetAvailWriteSpace(pRing);
496      Status = Avail < NumBytes ? Avail : NumBytes;
497      _WriteNoCheck(pRing, pData, Status);
498      break;
499    case SEGGER_RTT_MODE_BLOCK_IF_FIFO_FULL:
500      Status = _WriteBlocking(pRing, pData, NumBytes);
501      break;
502    default:
503      Status = 0u;
504      break;
505    }
506    return Status;
507  }
508  unsigned SEGGER_RTT_WriteNoLock(unsigned BufferIndex, const void* pBuffer, unsigned NumBytes) {
509    unsigned              Status;
510    unsigned              Avail;
511    const char*           pData;
512    SEGGER_RTT_BUFFER_UP* pRing;
513    pData = (const char *)pBuffer;
514    pRing = &_SEGGER_RTT.aUp[BufferIndex];
515    switch (pRing->Flags) {
516    case SEGGER_RTT_MODE_NO_BLOCK_SKIP:
517      Avail = _GetAvailWriteSpace(pRing);
518      if (Avail < NumBytes) {
519        Status = 0u;
520      } else {
521        Status = NumBytes;
522        _WriteNoCheck(pRing, pData, NumBytes);
523      }
524      break;
525    case SEGGER_RTT_MODE_NO_BLOCK_TRIM:
526      Avail = _GetAvailWriteSpace(pRing);
527      Status = Avail < NumBytes ? Avail : NumBytes;
528      _WriteNoCheck(pRing, pData, Status);
529      break;
530    case SEGGER_RTT_MODE_BLOCK_IF_FIFO_FULL:
531      Status = _WriteBlocking(pRing, pData, NumBytes);
532      break;
533    default:
534      Status = 0u;
535      break;
536    }
537    return Status;
538  }
539  unsigned SEGGER_RTT_WriteDownBuffer(unsigned BufferIndex, const void* pBuffer, unsigned NumBytes) {
540    unsigned Status;
541    INIT();
542    SEGGER_RTT_LOCK();
543    Status = SEGGER_RTT_WriteDownBufferNoLock(BufferIndex, pBuffer, NumBytes);
544    SEGGER_RTT_UNLOCK();
545    return Status;
546  }
547  unsigned SEGGER_RTT_Write(unsigned BufferIndex, const void* pBuffer, unsigned NumBytes) {
548    unsigned Status;
549    INIT();
550    SEGGER_RTT_LOCK();
551    Status = SEGGER_RTT_WriteNoLock(BufferIndex, pBuffer, NumBytes);
552    SEGGER_RTT_UNLOCK();
553    return Status;
554  }
555  unsigned SEGGER_RTT_WriteString(unsigned BufferIndex, const char* s) {
556    unsigned Len;
557    Len = STRLEN(s);
558    return SEGGER_RTT_Write(BufferIndex, s, Len);
559  }
560  unsigned SEGGER_RTT_PutCharSkipNoLock(unsigned BufferIndex, char c) {
561    SEGGER_RTT_BUFFER_UP* pRing;
562    unsigned              WrOff;
563    unsigned              Status;
564    pRing = &_SEGGER_RTT.aUp[BufferIndex];
565    WrOff = pRing->WrOff + 1;
566    if (WrOff == pRing->SizeOfBuffer) {
567      WrOff = 0;
568    }
569    if (WrOff != pRing->RdOff) {
570      pRing->pBuffer[pRing->WrOff] = c;
571      pRing->WrOff = WrOff;
572      Status = 1;
573    } else {
574      Status = 0;
575    }
576    return Status;
577  }
578  unsigned SEGGER_RTT_PutCharSkip(unsigned BufferIndex, char c) {
579    SEGGER_RTT_BUFFER_UP* pRing;
580    unsigned              WrOff;
581    unsigned              Status;
582    INIT();
583    SEGGER_RTT_LOCK();
584    pRing = &_SEGGER_RTT.aUp[BufferIndex];
585    WrOff = pRing->WrOff + 1;
586    if (WrOff == pRing->SizeOfBuffer) {
587      WrOff = 0;
588    }
589    if (WrOff != pRing->RdOff) {
590      pRing->pBuffer[pRing->WrOff] = c;
591      pRing->WrOff = WrOff;
592      Status = 1;
593    } else {
594      Status = 0;
595    }
596    SEGGER_RTT_UNLOCK();
597    return Status;
598  }
599  unsigned SEGGER_RTT_PutChar(unsigned BufferIndex, char c) {
600    SEGGER_RTT_BUFFER_UP* pRing;
601    unsigned              WrOff;
602    unsigned              Status;
603    INIT();
604    SEGGER_RTT_LOCK();
605    pRing = &_SEGGER_RTT.aUp[BufferIndex];
606    WrOff = pRing->WrOff + 1;
607    if (WrOff == pRing->SizeOfBuffer) {
608      WrOff = 0;
609    }
610    if (pRing->Flags == SEGGER_RTT_MODE_BLOCK_IF_FIFO_FULL) {
611      while (WrOff == pRing->RdOff) {
612        ;
613      }
614    }
615    if (WrOff != pRing->RdOff) {
616      pRing->pBuffer[pRing->WrOff] = c;
617      pRing->WrOff = WrOff;
618      Status = 1;
619    } else {
620      Status = 0;
621    }
622    SEGGER_RTT_UNLOCK();
623    return Status;
624  }
625  int SEGGER_RTT_GetKey(void) {
626    char c;
627    int r;
628    r = (int)SEGGER_RTT_Read(0u, &c, 1u);
629    if (r == 1) {
630      r = (int)(unsigned char)c;
631    } else {
632      r = -1;
633    }
634    return r;
635  }
636  int SEGGER_RTT_WaitKey(void) {
637    int r;
638    do {
639      r = SEGGER_RTT_GetKey();
640    } while (r < 0);
641    return r;
642  }
643  int SEGGER_RTT_HasKey(void) {
644    unsigned RdOff;
645    int r;
646    INIT();
647    RdOff = _SEGGER_RTT.aDown[0].RdOff;
648    if (RdOff != _SEGGER_RTT.aDown[0].WrOff) {
649      r = 1;
650    } else {
651      r = 0;
652    }
653    return r;
654  }
655  unsigned SEGGER_RTT_HasData(unsigned BufferIndex) {
656    SEGGER_RTT_BUFFER_DOWN* pRing;
657    unsigned                v;
658    pRing = &_SEGGER_RTT.aDown[BufferIndex];
659    v = pRing->WrOff;
660    return v - pRing->RdOff;
661  }
662  unsigned SEGGER_RTT_HasDataUp(unsigned BufferIndex) {
663    SEGGER_RTT_BUFFER_UP* pRing;
664    unsigned                v;
665    pRing = &_SEGGER_RTT.aUp[BufferIndex];
666    v = pRing->RdOff;
667    return pRing->WrOff - v;
668  }
669  int SEGGER_RTT_AllocDownBuffer(const char* sName, void* pBuffer, unsigned BufferSize, unsigned Flags) {
670    int BufferIndex;
671    INIT();
672    SEGGER_RTT_LOCK();
673    BufferIndex = 0;
674    do {
675      if (_SEGGER_RTT.aDown[BufferIndex].pBuffer == NULL) {
676        break;
677      }
678      BufferIndex++;
679    } while (BufferIndex < _SEGGER_RTT.MaxNumDownBuffers);
680    if (BufferIndex < _SEGGER_RTT.MaxNumDownBuffers) {
681      _SEGGER_RTT.aDown[BufferIndex].sName        = sName;
682      _SEGGER_RTT.aDown[BufferIndex].pBuffer      = (char*)pBuffer;
683      _SEGGER_RTT.aDown[BufferIndex].SizeOfBuffer = BufferSize;
684      _SEGGER_RTT.aDown[BufferIndex].RdOff        = 0u;
685      _SEGGER_RTT.aDown[BufferIndex].WrOff        = 0u;
686      _SEGGER_RTT.aDown[BufferIndex].Flags        = Flags;
687    } else {
688      BufferIndex = -1;
689    }
690    SEGGER_RTT_UNLOCK();
691    return BufferIndex;
692  }
693  int SEGGER_RTT_AllocUpBuffer(const char* sName, void* pBuffer, unsigned BufferSize, unsigned Flags) {
694    int BufferIndex;
695    INIT();
696    SEGGER_RTT_LOCK();
697    BufferIndex = 0;
698    do {
699      if (_SEGGER_RTT.aUp[BufferIndex].pBuffer == NULL) {
700        break;
701      }
702      BufferIndex++;
703    } while (BufferIndex < _SEGGER_RTT.MaxNumUpBuffers);
704    if (BufferIndex < _SEGGER_RTT.MaxNumUpBuffers) {
705      _SEGGER_RTT.aUp[BufferIndex].sName        = sName;
706      _SEGGER_RTT.aUp[BufferIndex].pBuffer      = (char*)pBuffer;
707      _SEGGER_RTT.aUp[BufferIndex].SizeOfBuffer = BufferSize;
708      _SEGGER_RTT.aUp[BufferIndex].RdOff        = 0u;
709      _SEGGER_RTT.aUp[BufferIndex].WrOff        = 0u;
710      _SEGGER_RTT.aUp[BufferIndex].Flags        = Flags;
711    } else {
712      BufferIndex = -1;
713    }
714    SEGGER_RTT_UNLOCK();
715    return BufferIndex;
716  }
717  int SEGGER_RTT_ConfigUpBuffer(unsigned BufferIndex, const char* sName, void* pBuffer, unsigned BufferSize, unsigned Flags) {
718    int r;
719    INIT();
720    if (BufferIndex < (unsigned)_SEGGER_RTT.MaxNumUpBuffers) {
721      SEGGER_RTT_LOCK();
722      if (BufferIndex > 0u) {
723        _SEGGER_RTT.aUp[BufferIndex].sName        = sName;
724        _SEGGER_RTT.aUp[BufferIndex].pBuffer      = (char*)pBuffer;
725        _SEGGER_RTT.aUp[BufferIndex].SizeOfBuffer = BufferSize;
726        _SEGGER_RTT.aUp[BufferIndex].RdOff        = 0u;
727        _SEGGER_RTT.aUp[BufferIndex].WrOff        = 0u;
728      }
729      _SEGGER_RTT.aUp[BufferIndex].Flags          = Flags;
730      SEGGER_RTT_UNLOCK();
731      r =  0;
732    } else {
733      r = -1;
734    }
735    return r;
736  }
737  int SEGGER_RTT_ConfigDownBuffer(unsigned BufferIndex, const char* sName, void* pBuffer, unsigned BufferSize, unsigned Flags) {
738    int r;
739    INIT();
740    if (BufferIndex < (unsigned)_SEGGER_RTT.MaxNumDownBuffers) {
741      SEGGER_RTT_LOCK();
742      if (BufferIndex > 0u) {
743        _SEGGER_RTT.aDown[BufferIndex].sName        = sName;
744        _SEGGER_RTT.aDown[BufferIndex].pBuffer      = (char*)pBuffer;
745        _SEGGER_RTT.aDown[BufferIndex].SizeOfBuffer = BufferSize;
746        _SEGGER_RTT.aDown[BufferIndex].RdOff        = 0u;
747        _SEGGER_RTT.aDown[BufferIndex].WrOff        = 0u;
748      }
749      _SEGGER_RTT.aDown[BufferIndex].Flags          = Flags;
750      SEGGER_RTT_UNLOCK();
751      r =  0;
752    } else {
753      r = -1;
754    }
755    return r;
756  }
757  int SEGGER_RTT_SetNameUpBuffer(unsigned BufferIndex, const char* sName) {
758    int r;
759    INIT();
760    if (BufferIndex < (unsigned)_SEGGER_RTT.MaxNumUpBuffers) {
761      SEGGER_RTT_LOCK();
762      _SEGGER_RTT.aUp[BufferIndex].sName = sName;
763      SEGGER_RTT_UNLOCK();
764      r =  0;
765    } else {
766      r = -1;
767    }
768    return r;
769  }
770  int SEGGER_RTT_SetNameDownBuffer(unsigned BufferIndex, const char* sName) {
771    int r;
772    INIT();
773    if (BufferIndex < (unsigned)_SEGGER_RTT.MaxNumDownBuffers) {
774      SEGGER_RTT_LOCK();
775      _SEGGER_RTT.aDown[BufferIndex].sName = sName;
776      SEGGER_RTT_UNLOCK();
777      r =  0;
778    } else {
779      r = -1;
780    }
781    return r;
782  }
783  int SEGGER_RTT_SetFlagsUpBuffer(unsigned BufferIndex, unsigned Flags) {
784    int r;
785    INIT();
786    if (BufferIndex < (unsigned)_SEGGER_RTT.MaxNumUpBuffers) {
787      SEGGER_RTT_LOCK();
788      _SEGGER_RTT.aUp[BufferIndex].Flags = Flags;
789      SEGGER_RTT_UNLOCK();
790      r =  0;
791    } else {
792      r = -1;
793    }
794    return r;
795  }
796  int SEGGER_RTT_SetFlagsDownBuffer(unsigned BufferIndex, unsigned Flags) {
797    int r;
798    INIT();
799    if (BufferIndex < (unsigned)_SEGGER_RTT.MaxNumDownBuffers) {
800      SEGGER_RTT_LOCK();
801      _SEGGER_RTT.aDown[BufferIndex].Flags = Flags;
802      SEGGER_RTT_UNLOCK();
803      r =  0;
804    } else {
805      r = -1;
806    }
807    return r;
808  }
809  void SEGGER_RTT_Init (void) {
810    _DoInit();
811  }
812  int SEGGER_RTT_SetTerminal (unsigned char TerminalId) {
813    unsigned char         ac[2];
814    SEGGER_RTT_BUFFER_UP* pRing;
815    unsigned Avail;
816    int r;
817    INIT();
818    r = 0;
819    ac[0] = 0xFFu;
820    if (TerminalId < sizeof(_aTerminalId)) { 
821      ac[1] = _aTerminalId[TerminalId];
822      pRing = &_SEGGER_RTT.aUp[0];    
823      SEGGER_RTT_LOCK();    
824      if ((pRing->Flags & SEGGER_RTT_MODE_MASK) == SEGGER_RTT_MODE_BLOCK_IF_FIFO_FULL) {
825        _ActiveTerminal = TerminalId;
826        _WriteBlocking(pRing, (const char*)ac, 2u);
827      } else {                                                                            
828        Avail = _GetAvailWriteSpace(pRing);
829        if (Avail >= 2) {
830          _ActiveTerminal = TerminalId;    
831          _WriteNoCheck(pRing, (const char*)ac, 2u);
832        } else {
833          r = -1;
834        }
835      }
836      SEGGER_RTT_UNLOCK();
837    } else {
838      r = -1;
839    }
840    return r;
841  }
842  int SEGGER_RTT_TerminalOut (unsigned char TerminalId, const char* s) {
843    int                   Status;
844    unsigned              FragLen;
845    unsigned              Avail;
846    SEGGER_RTT_BUFFER_UP* pRing;
847    INIT();
848    if (TerminalId < (char)sizeof(_aTerminalId)) { 
849      pRing = &_SEGGER_RTT.aUp[0];
850      FragLen = STRLEN(s);
851      SEGGER_RTT_LOCK();
852      Avail = _GetAvailWriteSpace(pRing);
853      switch (pRing->Flags & SEGGER_RTT_MODE_MASK) {
854      case SEGGER_RTT_MODE_NO_BLOCK_SKIP:
855        if (Avail < (FragLen + 4u)) {
856          Status = 0;
857        } else {
858          _PostTerminalSwitch(pRing, TerminalId);
859          Status = (int)_WriteBlocking(pRing, s, FragLen);
860          _PostTerminalSwitch(pRing, _ActiveTerminal);
861        }
862        break;
863      case SEGGER_RTT_MODE_NO_BLOCK_TRIM:
864        if (Avail < 4u) {
865          Status = -1;
866        } else {
867          _PostTerminalSwitch(pRing, TerminalId);
868          Status = (int)_WriteBlocking(pRing, s, (FragLen < (Avail - 4u)) ? FragLen : (Avail - 4u));
869          _PostTerminalSwitch(pRing, _ActiveTerminal);
870        }
871        break;
872      case SEGGER_RTT_MODE_BLOCK_IF_FIFO_FULL:
873        _PostTerminalSwitch(pRing, TerminalId);
874        Status = (int)_WriteBlocking(pRing, s, FragLen);
875        _PostTerminalSwitch(pRing, _ActiveTerminal);
876        break;
877      default:
878        Status = -1;
879        break;
880      }
881      SEGGER_RTT_UNLOCK();
882    } else {
883      Status = -1;
884    }
885    return Status;
886  }
887  unsigned SEGGER_RTT_GetAvailWriteSpace (unsigned BufferIndex){
888    return _GetAvailWriteSpace(&_SEGGER_RTT.aUp[BufferIndex]);
889  }
890  unsigned SEGGER_RTT_GetBytesInBuffer(unsigned BufferIndex) {
891    unsigned RdOff;
892    unsigned WrOff;
893    unsigned r;
894    RdOff = _SEGGER_RTT.aUp[BufferIndex].RdOff;
895    WrOff = _SEGGER_RTT.aUp[BufferIndex].WrOff;
896    if (RdOff <= WrOff) {
897      r = WrOff - RdOff;
898    } else {
899      r = _SEGGER_RTT.aUp[BufferIndex].SizeOfBuffer - (WrOff - RdOff);
900    }
901    return r;
902  }
</code></pre>
        </div>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sse2neon.h</h3>
            <pre><code>1  #ifndef SSE2NEON_H
2  #define SSE2NEON_H
3  #ifndef SSE2NEON_PRECISE_MINMAX
4  #define SSE2NEON_PRECISE_MINMAX (0)
5  #endif
6  #ifndef SSE2NEON_PRECISE_DIV
7  #define SSE2NEON_PRECISE_DIV (0)
8  #endif
9  #ifndef SSE2NEON_PRECISE_SQRT
10  #define SSE2NEON_PRECISE_SQRT (0)
11  #endif
12  #ifndef SSE2NEON_PRECISE_DP
13  #define SSE2NEON_PRECISE_DP (0)
14  #endif
15  #if defined(__GNUC__) || defined(__clang__)
16  #pragma push_macro("FORCE_INLINE")
17  #pragma push_macro("ALIGN_STRUCT")
18  #define FORCE_INLINE static inline __attribute__((always_inline))
19  #define ALIGN_STRUCT(x) __attribute__((aligned(x)))
20  #define _sse2neon_likely(x) __builtin_expect(!!(x), 1)
21  #define _sse2neon_unlikely(x) __builtin_expect(!!(x), 0)
22  #else &bsol;* non-GNU / non-clang compilers */
23  #warning "Macro name collisions may happen with unsupported compiler."
24  #ifndef FORCE_INLINE
25  #define FORCE_INLINE static inline
26  #endif
27  #ifndef ALIGN_STRUCT
28  #define ALIGN_STRUCT(x) __declspec(align(x))
29  #endif
30  #define _sse2neon_likely(x) (x)
31  #define _sse2neon_unlikely(x) (x)
32  #endif
33  #ifdef __cplusplus
34  #define _sse2neon_const static const
35  #else
36  #define _sse2neon_const const
37  #endif
38  #include <stdint.h>
39  #include <stdlib.h>
40  #if defined(_WIN32)
41  #define SSE2NEON_ALLOC_DEFINED
42  #endif
43  #ifdef _MSC_VER
44  #include <intrin.h>
45  #if (defined(_M_AMD64) || defined(__x86_64__)) || \
46      (defined(_M_ARM) || defined(__arm__))
47  #define SSE2NEON_HAS_BITSCAN64
48  #endif
49  #endif
50  #define SSE2NEON_BARRIER()                     \
51      do {                                       \
52          __asm__ __volatile__("" ::: "memory"); \
53          (void) 0;                              \
54      } while (0)
55  #if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L)
56  #include <stdatomic.h>
57  #endif
58  FORCE_INLINE void _sse2neon_smp_mb(void)
59  {
60      SSE2NEON_BARRIER();
61  #if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L) && \
62      !defined(__STDC_NO_ATOMICS__)
63      atomic_thread_fence(memory_order_seq_cst);
64  #elif defined(__GNUC__) || defined(__clang__)
65      __atomic_thread_fence(__ATOMIC_SEQ_CST);
66  #else
67  #endif
68  }
69  #if defined(__GNUC__)
70  #if defined(__arm__) && __ARM_ARCH == 7
71  #if !defined(__ARM_NEON) || !defined(__ARM_NEON__)
72  #error "You must enable NEON instructions (e.g. -mfpu=neon) to use SSE2NEON."
73  #endif
74  #if !defined(__clang__)
75  #pragma GCC push_options
76  #pragma GCC target("fpu=neon")
77  #endif
78  #elif defined(__aarch64__)
79  #if !defined(__clang__)
80  #pragma GCC push_options
81  #pragma GCC target("+simd")
82  #endif
83  #elif __ARM_ARCH == 8
84  #if !defined(__ARM_NEON) || !defined(__ARM_NEON__)
85  #error \
86      "You must enable NEON instructions (e.g. -mfpu=neon-fp-armv8) to use SSE2NEON."
87  #endif
88  #if !defined(__clang__)
89  #pragma GCC push_options
90  #endif
91  #else
92  #error "Unsupported target. Must be either ARMv7-A+NEON or ARMv8-A."
93  #endif
94  #endif
95  #include <arm_neon.h>
96  #if !defined(__aarch64__) && (__ARM_ARCH == 8)
97  #if defined __has_include && __has_include(<arm_acle.h>)
98  #include <arm_acle.h>
99  #endif
100  #endif
101  #if defined(__APPLE__) && (defined(__aarch64__) || defined(__arm64__))
102  #define SSE2NEON_CACHELINE_SIZE 128
103  #else
104  #define SSE2NEON_CACHELINE_SIZE 64
105  #endif
106  #if !defined(__aarch64__)
107  #include <math.h>
108  #endif
109  #if !defined(__aarch64__)
110  #include <sys/time.h>
111  #endif
112  #ifndef __has_builtin &bsol;* GCC prior to 10 or non-clang compilers */
113  #if defined(__GNUC__) && (__GNUC__ <= 9)
114  #define __has_builtin(x) HAS##x
115  #define HAS__builtin_popcount 1
116  #define HAS__builtin_popcountll 1
117  #if (__GNUC__ >= 5) || ((__GNUC__ == 4) && (__GNUC_MINOR__ >= 7))
118  #define HAS__builtin_shuffle 1
119  #else
120  #define HAS__builtin_shuffle 0
121  #endif
122  #define HAS__builtin_shufflevector 0
123  #define HAS__builtin_nontemporal_store 0
124  #else
125  #define __has_builtin(x) 0
126  #endif
127  #endif
128  #define _MM_SHUFFLE(fp3, fp2, fp1, fp0) \
129      (((fp3) << 6) | ((fp2) << 4) | ((fp1) << 2) | ((fp0)))
130  #if __has_builtin(__builtin_shufflevector)
131  #define _sse2neon_shuffle(type, a, b, ...) \
132      __builtin_shufflevector(a, b, __VA_ARGS__)
133  #elif __has_builtin(__builtin_shuffle)
134  #define _sse2neon_shuffle(type, a, b, ...) \
135      __extension__({                        \
136          type tmp = {__VA_ARGS__};          \
137          __builtin_shuffle(a, b, tmp);      \
138      })
139  #endif
140  #ifdef _sse2neon_shuffle
141  #define vshuffle_s16(a, b, ...) _sse2neon_shuffle(int16x4_t, a, b, __VA_ARGS__)
142  #define vshuffleq_s16(a, b, ...) _sse2neon_shuffle(int16x8_t, a, b, __VA_ARGS__)
143  #define vshuffle_s32(a, b, ...) _sse2neon_shuffle(int32x2_t, a, b, __VA_ARGS__)
144  #define vshuffleq_s32(a, b, ...) _sse2neon_shuffle(int32x4_t, a, b, __VA_ARGS__)
145  #define vshuffle_s64(a, b, ...) _sse2neon_shuffle(int64x1_t, a, b, __VA_ARGS__)
146  #define vshuffleq_s64(a, b, ...) _sse2neon_shuffle(int64x2_t, a, b, __VA_ARGS__)
147  #endif
148  #define _MM_FROUND_TO_NEAREST_INT 0x00
149  #define _MM_FROUND_TO_NEG_INF 0x01
150  #define _MM_FROUND_TO_POS_INF 0x02
151  #define _MM_FROUND_TO_ZERO 0x03
152  #define _MM_FROUND_CUR_DIRECTION 0x04
153  #define _MM_FROUND_NO_EXC 0x08
154  #define _MM_FROUND_RAISE_EXC 0x00
155  #define _MM_FROUND_NINT (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_RAISE_EXC)
156  #define _MM_FROUND_FLOOR (_MM_FROUND_TO_NEG_INF | _MM_FROUND_RAISE_EXC)
157  #define _MM_FROUND_CEIL (_MM_FROUND_TO_POS_INF | _MM_FROUND_RAISE_EXC)
158  #define _MM_FROUND_TRUNC (_MM_FROUND_TO_ZERO | _MM_FROUND_RAISE_EXC)
159  #define _MM_FROUND_RINT (_MM_FROUND_CUR_DIRECTION | _MM_FROUND_RAISE_EXC)
160  #define _MM_FROUND_NEARBYINT (_MM_FROUND_CUR_DIRECTION | _MM_FROUND_NO_EXC)
161  #define _MM_ROUND_NEAREST 0x0000
162  #define _MM_ROUND_DOWN 0x2000
163  #define _MM_ROUND_UP 0x4000
164  #define _MM_ROUND_TOWARD_ZERO 0x6000
165  #define _MM_FLUSH_ZERO_MASK 0x8000
166  #define _MM_FLUSH_ZERO_ON 0x8000
167  #define _MM_FLUSH_ZERO_OFF 0x0000
168  #define _MM_DENORMALS_ZERO_MASK 0x0040
169  #define _MM_DENORMALS_ZERO_ON 0x0040
170  #define _MM_DENORMALS_ZERO_OFF 0x0000
171  #define __constrange(a, b) const
172  typedef int64x1_t __m64;
173  typedef float32x4_t __m128; &bsol;* 128-bit vector containing 4 floats */
174  #if defined(__aarch64__)
175  typedef float64x2_t __m128d; &bsol;* 128-bit vector containing 2 doubles */
176  #else
177  typedef float32x4_t __m128d;
178  #endif
179  typedef int64x2_t __m128i; &bsol;* 128-bit vector containing integers */
180  #if !(defined(_WIN32) || defined(_WIN64) || defined(__int64))
181  #if (defined(__x86_64__) || defined(__i386__))
182  #define __int64 long long
183  #else
184  #define __int64 int64_t
185  #endif
186  #endif
187  #define vreinterpretq_m128_f16(x) vreinterpretq_f32_f16(x)
188  #define vreinterpretq_m128_f32(x) (x)
189  #define vreinterpretq_m128_f64(x) vreinterpretq_f32_f64(x)
190  #define vreinterpretq_m128_u8(x) vreinterpretq_f32_u8(x)
191  #define vreinterpretq_m128_u16(x) vreinterpretq_f32_u16(x)
192  #define vreinterpretq_m128_u32(x) vreinterpretq_f32_u32(x)
193  #define vreinterpretq_m128_u64(x) vreinterpretq_f32_u64(x)
194  #define vreinterpretq_m128_s8(x) vreinterpretq_f32_s8(x)
195  #define vreinterpretq_m128_s16(x) vreinterpretq_f32_s16(x)
196  #define vreinterpretq_m128_s32(x) vreinterpretq_f32_s32(x)
197  #define vreinterpretq_m128_s64(x) vreinterpretq_f32_s64(x)
198  #define vreinterpretq_f16_m128(x) vreinterpretq_f16_f32(x)
199  #define vreinterpretq_f32_m128(x) (x)
200  #define vreinterpretq_f64_m128(x) vreinterpretq_f64_f32(x)
201  #define vreinterpretq_u8_m128(x) vreinterpretq_u8_f32(x)
202  #define vreinterpretq_u16_m128(x) vreinterpretq_u16_f32(x)
203  #define vreinterpretq_u32_m128(x) vreinterpretq_u32_f32(x)
204  #define vreinterpretq_u64_m128(x) vreinterpretq_u64_f32(x)
205  #define vreinterpretq_s8_m128(x) vreinterpretq_s8_f32(x)
206  #define vreinterpretq_s16_m128(x) vreinterpretq_s16_f32(x)
207  #define vreinterpretq_s32_m128(x) vreinterpretq_s32_f32(x)
208  #define vreinterpretq_s64_m128(x) vreinterpretq_s64_f32(x)
209  #define vreinterpretq_m128i_s8(x) vreinterpretq_s64_s8(x)
210  #define vreinterpretq_m128i_s16(x) vreinterpretq_s64_s16(x)
211  #define vreinterpretq_m128i_s32(x) vreinterpretq_s64_s32(x)
212  #define vreinterpretq_m128i_s64(x) (x)
213  #define vreinterpretq_m128i_u8(x) vreinterpretq_s64_u8(x)
214  #define vreinterpretq_m128i_u16(x) vreinterpretq_s64_u16(x)
215  #define vreinterpretq_m128i_u32(x) vreinterpretq_s64_u32(x)
216  #define vreinterpretq_m128i_u64(x) vreinterpretq_s64_u64(x)
217  #define vreinterpretq_f32_m128i(x) vreinterpretq_f32_s64(x)
218  #define vreinterpretq_f64_m128i(x) vreinterpretq_f64_s64(x)
219  #define vreinterpretq_s8_m128i(x) vreinterpretq_s8_s64(x)
220  #define vreinterpretq_s16_m128i(x) vreinterpretq_s16_s64(x)
221  #define vreinterpretq_s32_m128i(x) vreinterpretq_s32_s64(x)
222  #define vreinterpretq_s64_m128i(x) (x)
223  #define vreinterpretq_u8_m128i(x) vreinterpretq_u8_s64(x)
224  #define vreinterpretq_u16_m128i(x) vreinterpretq_u16_s64(x)
225  #define vreinterpretq_u32_m128i(x) vreinterpretq_u32_s64(x)
226  #define vreinterpretq_u64_m128i(x) vreinterpretq_u64_s64(x)
227  #define vreinterpret_m64_s8(x) vreinterpret_s64_s8(x)
228  #define vreinterpret_m64_s16(x) vreinterpret_s64_s16(x)
229  #define vreinterpret_m64_s32(x) vreinterpret_s64_s32(x)
230  #define vreinterpret_m64_s64(x) (x)
231  #define vreinterpret_m64_u8(x) vreinterpret_s64_u8(x)
232  #define vreinterpret_m64_u16(x) vreinterpret_s64_u16(x)
233  #define vreinterpret_m64_u32(x) vreinterpret_s64_u32(x)
234  #define vreinterpret_m64_u64(x) vreinterpret_s64_u64(x)
235  #define vreinterpret_m64_f16(x) vreinterpret_s64_f16(x)
236  #define vreinterpret_m64_f32(x) vreinterpret_s64_f32(x)
237  #define vreinterpret_m64_f64(x) vreinterpret_s64_f64(x)
238  #define vreinterpret_u8_m64(x) vreinterpret_u8_s64(x)
239  #define vreinterpret_u16_m64(x) vreinterpret_u16_s64(x)
240  #define vreinterpret_u32_m64(x) vreinterpret_u32_s64(x)
241  #define vreinterpret_u64_m64(x) vreinterpret_u64_s64(x)
242  #define vreinterpret_s8_m64(x) vreinterpret_s8_s64(x)
243  #define vreinterpret_s16_m64(x) vreinterpret_s16_s64(x)
244  #define vreinterpret_s32_m64(x) vreinterpret_s32_s64(x)
245  #define vreinterpret_s64_m64(x) (x)
246  #define vreinterpret_f32_m64(x) vreinterpret_f32_s64(x)
247  #if defined(__aarch64__)
248  #define vreinterpretq_m128d_s32(x) vreinterpretq_f64_s32(x)
249  #define vreinterpretq_m128d_s64(x) vreinterpretq_f64_s64(x)
250  #define vreinterpretq_m128d_u64(x) vreinterpretq_f64_u64(x)
251  #define vreinterpretq_m128d_f32(x) vreinterpretq_f64_f32(x)
252  #define vreinterpretq_m128d_f64(x) (x)
253  #define vreinterpretq_s64_m128d(x) vreinterpretq_s64_f64(x)
254  #define vreinterpretq_u32_m128d(x) vreinterpretq_u32_f64(x)
255  #define vreinterpretq_u64_m128d(x) vreinterpretq_u64_f64(x)
256  #define vreinterpretq_f64_m128d(x) (x)
257  #define vreinterpretq_f32_m128d(x) vreinterpretq_f32_f64(x)
258  #else
259  #define vreinterpretq_m128d_s32(x) vreinterpretq_f32_s32(x)
260  #define vreinterpretq_m128d_s64(x) vreinterpretq_f32_s64(x)
261  #define vreinterpretq_m128d_u32(x) vreinterpretq_f32_u32(x)
262  #define vreinterpretq_m128d_u64(x) vreinterpretq_f32_u64(x)
263  #define vreinterpretq_m128d_f32(x) (x)
264  #define vreinterpretq_s64_m128d(x) vreinterpretq_s64_f32(x)
265  #define vreinterpretq_u32_m128d(x) vreinterpretq_u32_f32(x)
266  #define vreinterpretq_u64_m128d(x) vreinterpretq_u64_f32(x)
267  #define vreinterpretq_f32_m128d(x) (x)
268  #endif
269  typedef union ALIGN_STRUCT(16) SIMDVec {
270      float m128_f32[4];     
271      int8_t m128_i8[16];    
272      int16_t m128_i16[8];   
273      int32_t m128_i32[4];   
274      int64_t m128_i64[2];   
275      uint8_t m128_u8[16];   
276      uint16_t m128_u16[8];  
277      uint32_t m128_u32[4];  
278      uint64_t m128_u64[2];  
279  } SIMDVec;
280  #define vreinterpretq_nth_u64_m128i(x, n) (((SIMDVec *) &x)->m128_u64[n])
281  #define vreinterpretq_nth_u32_m128i(x, n) (((SIMDVec *) &x)->m128_u32[n])
282  #define vreinterpretq_nth_u8_m128i(x, n) (((SIMDVec *) &x)->m128_u8[n])
283  #define _MM_GET_FLUSH_ZERO_MODE _sse2neon_mm_get_flush_zero_mode
284  #define _MM_SET_FLUSH_ZERO_MODE _sse2neon_mm_set_flush_zero_mode
285  #define _MM_GET_DENORMALS_ZERO_MODE _sse2neon_mm_get_denormals_zero_mode
286  #define _MM_SET_DENORMALS_ZERO_MODE _sse2neon_mm_set_denormals_zero_mode
287  FORCE_INLINE unsigned int _MM_GET_ROUNDING_MODE();
288  FORCE_INLINE __m128 _mm_move_ss(__m128, __m128);
289  FORCE_INLINE __m128 _mm_or_ps(__m128, __m128);
290  FORCE_INLINE __m128 _mm_set_ps1(float);
291  FORCE_INLINE __m128 _mm_setzero_ps(void);
292  FORCE_INLINE __m128i _mm_and_si128(__m128i, __m128i);
293  FORCE_INLINE __m128i _mm_castps_si128(__m128);
294  FORCE_INLINE __m128i _mm_cmpeq_epi32(__m128i, __m128i);
295  FORCE_INLINE __m128i _mm_cvtps_epi32(__m128);
296  FORCE_INLINE __m128d _mm_move_sd(__m128d, __m128d);
297  FORCE_INLINE __m128i _mm_or_si128(__m128i, __m128i);
298  FORCE_INLINE __m128i _mm_set_epi32(int, int, int, int);
299  FORCE_INLINE __m128i _mm_set_epi64x(int64_t, int64_t);
300  FORCE_INLINE __m128d _mm_set_pd(double, double);
301  FORCE_INLINE __m128i _mm_set1_epi32(int);
302  FORCE_INLINE __m128i _mm_setzero_si128();
303  FORCE_INLINE __m128d _mm_ceil_pd(__m128d);
304  FORCE_INLINE __m128 _mm_ceil_ps(__m128);
305  FORCE_INLINE __m128d _mm_floor_pd(__m128d);
306  FORCE_INLINE __m128 _mm_floor_ps(__m128);
307  FORCE_INLINE __m128d _mm_round_pd(__m128d, int);
308  FORCE_INLINE __m128 _mm_round_ps(__m128, int);
309  FORCE_INLINE uint32_t _mm_crc32_u8(uint32_t, uint8_t);
310  #if defined(__GNUC__) && !defined(__clang__) &&                        \
311      ((__GNUC__ <= 12 && defined(__arm__)) ||                           \
312       (__GNUC__ == 10 && __GNUC_MINOR__ < 3 && defined(__aarch64__)) || \
313       (__GNUC__ <= 9 && defined(__aarch64__)))
314  FORCE_INLINE uint8x16x4_t _sse2neon_vld1q_u8_x4(const uint8_t *p)
315  {
316      uint8x16x4_t ret;
317      ret.val[0] = vld1q_u8(p + 0);
318      ret.val[1] = vld1q_u8(p + 16);
319      ret.val[2] = vld1q_u8(p + 32);
320      ret.val[3] = vld1q_u8(p + 48);
321      return ret;
322  }
323  #else
324  FORCE_INLINE uint8x16x4_t _sse2neon_vld1q_u8_x4(const uint8_t *p)
325  {
326      return vld1q_u8_x4(p);
327  }
328  #endif
329  #if !defined(__aarch64__)
330  FORCE_INLINE uint8_t _sse2neon_vaddv_u8(uint8x8_t v8)
331  {
332      const uint64x1_t v1 = vpaddl_u32(vpaddl_u16(vpaddl_u8(v8)));
333      return vget_lane_u8(vreinterpret_u8_u64(v1), 0);
334  }
335  #else
336  FORCE_INLINE uint8_t _sse2neon_vaddv_u8(uint8x8_t v8)
337  {
338      return vaddv_u8(v8);
339  }
340  #endif
341  #if !defined(__aarch64__)
342  FORCE_INLINE uint8_t _sse2neon_vaddvq_u8(uint8x16_t a)
343  {
344      uint8x8_t tmp = vpadd_u8(vget_low_u8(a), vget_high_u8(a));
345      uint8_t res = 0;
346      for (int i = 0; i < 8; ++i)
347          res += tmp[i];
348      return res;
349  }
350  #else
351  FORCE_INLINE uint8_t _sse2neon_vaddvq_u8(uint8x16_t a)
352  {
353      return vaddvq_u8(a);
354  }
355  #endif
356  #if !defined(__aarch64__)
357  FORCE_INLINE uint16_t _sse2neon_vaddvq_u16(uint16x8_t a)
358  {
359      uint32x4_t m = vpaddlq_u16(a);
360      uint64x2_t n = vpaddlq_u32(m);
361      uint64x1_t o = vget_low_u64(n) + vget_high_u64(n);
362      return vget_lane_u32((uint32x2_t) o, 0);
363  }
364  #else
365  FORCE_INLINE uint16_t _sse2neon_vaddvq_u16(uint16x8_t a)
366  {
367      return vaddvq_u16(a);
368  }
369  #endif
370  enum _mm_hint {
371      _MM_HINT_NTA = 0, &bsol;* load data to L1 and L2 cache, mark it as NTA */
372      _MM_HINT_T0 = 1,  &bsol;* load data to L1 and L2 cache */
373      _MM_HINT_T1 = 2,  &bsol;* load data to L2 cache only */
374      _MM_HINT_T2 = 3,  &bsol;* load data to L2 cache only, mark it as NTA */
375  };
376  typedef struct {
377      uint16_t res0;
378      uint8_t res1 : 6;
379      uint8_t bit22 : 1;
380      uint8_t bit23 : 1;
381      uint8_t bit24 : 1;
382      uint8_t res2 : 7;
383  #if defined(__aarch64__)
384      uint32_t res3;
385  #endif
386  } fpcr_bitfield;
387  FORCE_INLINE __m128 _mm_shuffle_ps_1032(__m128 a, __m128 b)
388  {
389      float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
390      float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
391      return vreinterpretq_m128_f32(vcombine_f32(a32, b10));
392  }
393  FORCE_INLINE __m128 _mm_shuffle_ps_2301(__m128 a, __m128 b)
394  {
395      float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
396      float32x2_t b23 = vrev64_f32(vget_high_f32(vreinterpretq_f32_m128(b)));
397      return vreinterpretq_m128_f32(vcombine_f32(a01, b23));
398  }
399  FORCE_INLINE __m128 _mm_shuffle_ps_0321(__m128 a, __m128 b)
400  {
401      float32x2_t a21 = vget_high_f32(
402          vextq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 3));
403      float32x2_t b03 = vget_low_f32(
404          vextq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b), 3));
405      return vreinterpretq_m128_f32(vcombine_f32(a21, b03));
406  }
407  FORCE_INLINE __m128 _mm_shuffle_ps_2103(__m128 a, __m128 b)
408  {
409      float32x2_t a03 = vget_low_f32(
410          vextq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 3));
411      float32x2_t b21 = vget_high_f32(
412          vextq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b), 3));
413      return vreinterpretq_m128_f32(vcombine_f32(a03, b21));
414  }
415  FORCE_INLINE __m128 _mm_shuffle_ps_1010(__m128 a, __m128 b)
416  {
417      float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
418      float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
419      return vreinterpretq_m128_f32(vcombine_f32(a10, b10));
420  }
421  FORCE_INLINE __m128 _mm_shuffle_ps_1001(__m128 a, __m128 b)
422  {
423      float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
424      float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
425      return vreinterpretq_m128_f32(vcombine_f32(a01, b10));
426  }
427  FORCE_INLINE __m128 _mm_shuffle_ps_0101(__m128 a, __m128 b)
428  {
429      float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
430      float32x2_t b01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(b)));
431      return vreinterpretq_m128_f32(vcombine_f32(a01, b01));
432  }
433  FORCE_INLINE __m128 _mm_shuffle_ps_3210(__m128 a, __m128 b)
434  {
435      float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
436      float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
437      return vreinterpretq_m128_f32(vcombine_f32(a10, b32));
438  }
439  FORCE_INLINE __m128 _mm_shuffle_ps_0011(__m128 a, __m128 b)
440  {
441      float32x2_t a11 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(a)), 1);
442      float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
443      return vreinterpretq_m128_f32(vcombine_f32(a11, b00));
444  }
445  FORCE_INLINE __m128 _mm_shuffle_ps_0022(__m128 a, __m128 b)
446  {
447      float32x2_t a22 =
448          vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), 0);
449      float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
450      return vreinterpretq_m128_f32(vcombine_f32(a22, b00));
451  }
452  FORCE_INLINE __m128 _mm_shuffle_ps_2200(__m128 a, __m128 b)
453  {
454      float32x2_t a00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(a)), 0);
455      float32x2_t b22 =
456          vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(b)), 0);
457      return vreinterpretq_m128_f32(vcombine_f32(a00, b22));
458  }
459  FORCE_INLINE __m128 _mm_shuffle_ps_3202(__m128 a, __m128 b)
460  {
461      float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
462      float32x2_t a22 =
463          vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), 0);
464      float32x2_t a02 = vset_lane_f32(a0, a22, 1); &bsol;* TODO: use vzip ?*/
465      float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
466      return vreinterpretq_m128_f32(vcombine_f32(a02, b32));
467  }
468  FORCE_INLINE __m128 _mm_shuffle_ps_1133(__m128 a, __m128 b)
469  {
470      float32x2_t a33 =
471          vdup_lane_f32(vget_high_f32(vreinterpretq_f32_m128(a)), 1);
472      float32x2_t b11 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 1);
473      return vreinterpretq_m128_f32(vcombine_f32(a33, b11));
474  }
475  FORCE_INLINE __m128 _mm_shuffle_ps_2010(__m128 a, __m128 b)
476  {
477      float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
478      float32_t b2 = vgetq_lane_f32(vreinterpretq_f32_m128(b), 2);
479      float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
480      float32x2_t b20 = vset_lane_f32(b2, b00, 1);
481      return vreinterpretq_m128_f32(vcombine_f32(a10, b20));
482  }
483  FORCE_INLINE __m128 _mm_shuffle_ps_2001(__m128 a, __m128 b)
484  {
485      float32x2_t a01 = vrev64_f32(vget_low_f32(vreinterpretq_f32_m128(a)));
486      float32_t b2 = vgetq_lane_f32(b, 2);
487      float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
488      float32x2_t b20 = vset_lane_f32(b2, b00, 1);
489      return vreinterpretq_m128_f32(vcombine_f32(a01, b20));
490  }
491  FORCE_INLINE __m128 _mm_shuffle_ps_2032(__m128 a, __m128 b)
492  {
493      float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
494      float32_t b2 = vgetq_lane_f32(b, 2);
495      float32x2_t b00 = vdup_lane_f32(vget_low_f32(vreinterpretq_f32_m128(b)), 0);
496      float32x2_t b20 = vset_lane_f32(b2, b00, 1);
497      return vreinterpretq_m128_f32(vcombine_f32(a32, b20));
498  }
499  FORCE_INLINE void _sse2neon_kadd_f32(float *sum, float *c, float y)
500  {
501      y -= *c;
502      float t = *sum + y;
503      *c = (t - *sum) - y;
504      *sum = t;
505  }
506  #if defined(__ARM_FEATURE_CRYPTO) && \
507      (defined(__aarch64__) || __has_builtin(__builtin_arm_crypto_vmullp64))
508  FORCE_INLINE uint64x2_t _sse2neon_vmull_p64(uint64x1_t _a, uint64x1_t _b)
509  {
510      poly64_t a = vget_lane_p64(vreinterpret_p64_u64(_a), 0);
511      poly64_t b = vget_lane_p64(vreinterpret_p64_u64(_b), 0);
512      return vreinterpretq_u64_p128(vmull_p64(a, b));
513  }
514  #else  
515  static uint64x2_t _sse2neon_vmull_p64(uint64x1_t _a, uint64x1_t _b)
516  {
517      poly8x8_t a = vreinterpret_p8_u64(_a);
518      poly8x8_t b = vreinterpret_p8_u64(_b);
519      uint8x16_t k48_32 = vcombine_u8(vcreate_u8(0x0000ffffffffffff),
520                                      vcreate_u8(0x00000000ffffffff));
521      uint8x16_t k16_00 = vcombine_u8(vcreate_u8(0x000000000000ffff),
522                                      vcreate_u8(0x0000000000000000));
523      uint8x16_t d = vreinterpretq_u8_p16(vmull_p8(a, b));  
524      uint8x16_t e =
525          vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 1)));  
526      uint8x16_t f =
527          vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, 1), b));  
528      uint8x16_t g =
529          vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 2)));  
530      uint8x16_t h =
531          vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, 2), b));  
532      uint8x16_t i =
533          vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 3)));  
534      uint8x16_t j =
535          vreinterpretq_u8_p16(vmull_p8(vext_p8(a, a, 3), b));  
536      uint8x16_t k =
537          vreinterpretq_u8_p16(vmull_p8(a, vext_p8(b, b, 4)));  
538      uint8x16_t l = veorq_u8(e, f);  
539      uint8x16_t m = veorq_u8(g, h);  
540      uint8x16_t n = veorq_u8(i, j);  
541  #if defined(__aarch64__)
542      uint8x16_t lm_p0 = vreinterpretq_u8_u64(
543          vzip1q_u64(vreinterpretq_u64_u8(l), vreinterpretq_u64_u8(m)));
544      uint8x16_t lm_p1 = vreinterpretq_u8_u64(
545          vzip2q_u64(vreinterpretq_u64_u8(l), vreinterpretq_u64_u8(m)));
546      uint8x16_t nk_p0 = vreinterpretq_u8_u64(
547          vzip1q_u64(vreinterpretq_u64_u8(n), vreinterpretq_u64_u8(k)));
548      uint8x16_t nk_p1 = vreinterpretq_u8_u64(
549          vzip2q_u64(vreinterpretq_u64_u8(n), vreinterpretq_u64_u8(k)));
550  #else
551      uint8x16_t lm_p0 = vcombine_u8(vget_low_u8(l), vget_low_u8(m));
552      uint8x16_t lm_p1 = vcombine_u8(vget_high_u8(l), vget_high_u8(m));
553      uint8x16_t nk_p0 = vcombine_u8(vget_low_u8(n), vget_low_u8(k));
554      uint8x16_t nk_p1 = vcombine_u8(vget_high_u8(n), vget_high_u8(k));
555  #endif
556      uint8x16_t t0t1_tmp = veorq_u8(lm_p0, lm_p1);
557      uint8x16_t t0t1_h = vandq_u8(lm_p1, k48_32);
558      uint8x16_t t0t1_l = veorq_u8(t0t1_tmp, t0t1_h);
559      uint8x16_t t2t3_tmp = veorq_u8(nk_p0, nk_p1);
560      uint8x16_t t2t3_h = vandq_u8(nk_p1, k16_00);
561      uint8x16_t t2t3_l = veorq_u8(t2t3_tmp, t2t3_h);
562  #if defined(__aarch64__)
563      uint8x16_t t0 = vreinterpretq_u8_u64(
564          vuzp1q_u64(vreinterpretq_u64_u8(t0t1_l), vreinterpretq_u64_u8(t0t1_h)));
565      uint8x16_t t1 = vreinterpretq_u8_u64(
566          vuzp2q_u64(vreinterpretq_u64_u8(t0t1_l), vreinterpretq_u64_u8(t0t1_h)));
567      uint8x16_t t2 = vreinterpretq_u8_u64(
568          vuzp1q_u64(vreinterpretq_u64_u8(t2t3_l), vreinterpretq_u64_u8(t2t3_h)));
569      uint8x16_t t3 = vreinterpretq_u8_u64(
570          vuzp2q_u64(vreinterpretq_u64_u8(t2t3_l), vreinterpretq_u64_u8(t2t3_h)));
571  #else
572      uint8x16_t t1 = vcombine_u8(vget_high_u8(t0t1_l), vget_high_u8(t0t1_h));
573      uint8x16_t t0 = vcombine_u8(vget_low_u8(t0t1_l), vget_low_u8(t0t1_h));
574      uint8x16_t t3 = vcombine_u8(vget_high_u8(t2t3_l), vget_high_u8(t2t3_h));
575      uint8x16_t t2 = vcombine_u8(vget_low_u8(t2t3_l), vget_low_u8(t2t3_h));
576  #endif
577      uint8x16_t t0_shift = vextq_u8(t0, t0, 15);  
578      uint8x16_t t1_shift = vextq_u8(t1, t1, 14);  
579      uint8x16_t t2_shift = vextq_u8(t2, t2, 13);  
580      uint8x16_t t3_shift = vextq_u8(t3, t3, 12);  
581      uint8x16_t cross1 = veorq_u8(t0_shift, t1_shift);
582      uint8x16_t cross2 = veorq_u8(t2_shift, t3_shift);
583      uint8x16_t mix = veorq_u8(d, cross1);
584      uint8x16_t r = veorq_u8(mix, cross2);
585      return vreinterpretq_u64_u8(r);
586  }
587  #endif  
588  #define _mm_shuffle_epi32_default(a, imm)                                   \
589      __extension__({                                                         \
590          int32x4_t ret;                                                      \
591          ret = vmovq_n_s32(                                                  \
592              vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm) & (0x3)));     \
593          ret = vsetq_lane_s32(                                               \
594              vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) >> 2) & 0x3), \
595              ret, 1);                                                        \
596          ret = vsetq_lane_s32(                                               \
597              vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) >> 4) & 0x3), \
598              ret, 2);                                                        \
599          ret = vsetq_lane_s32(                                               \
600              vgetq_lane_s32(vreinterpretq_s32_m128i(a), ((imm) >> 6) & 0x3), \
601              ret, 3);                                                        \
602          vreinterpretq_m128i_s32(ret);                                       \
603      })
604  FORCE_INLINE __m128i _mm_shuffle_epi_1032(__m128i a)
605  {
606      int32x2_t a32 = vget_high_s32(vreinterpretq_s32_m128i(a));
607      int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));
608      return vreinterpretq_m128i_s32(vcombine_s32(a32, a10));
609  }
610  FORCE_INLINE __m128i _mm_shuffle_epi_2301(__m128i a)
611  {
612      int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
613      int32x2_t a23 = vrev64_s32(vget_high_s32(vreinterpretq_s32_m128i(a)));
614      return vreinterpretq_m128i_s32(vcombine_s32(a01, a23));
615  }
616  FORCE_INLINE __m128i _mm_shuffle_epi_0321(__m128i a)
617  {
618      return vreinterpretq_m128i_s32(
619          vextq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(a), 1));
620  }
621  FORCE_INLINE __m128i _mm_shuffle_epi_2103(__m128i a)
622  {
623      return vreinterpretq_m128i_s32(
624          vextq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(a), 3));
625  }
626  FORCE_INLINE __m128i _mm_shuffle_epi_1010(__m128i a)
627  {
628      int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));
629      return vreinterpretq_m128i_s32(vcombine_s32(a10, a10));
630  }
631  FORCE_INLINE __m128i _mm_shuffle_epi_1001(__m128i a)
632  {
633      int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
634      int32x2_t a10 = vget_low_s32(vreinterpretq_s32_m128i(a));
635      return vreinterpretq_m128i_s32(vcombine_s32(a01, a10));
636  }
637  FORCE_INLINE __m128i _mm_shuffle_epi_0101(__m128i a)
638  {
639      int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
640      return vreinterpretq_m128i_s32(vcombine_s32(a01, a01));
641  }
642  FORCE_INLINE __m128i _mm_shuffle_epi_2211(__m128i a)
643  {
644      int32x2_t a11 = vdup_lane_s32(vget_low_s32(vreinterpretq_s32_m128i(a)), 1);
645      int32x2_t a22 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), 0);
646      return vreinterpretq_m128i_s32(vcombine_s32(a11, a22));
647  }
648  FORCE_INLINE __m128i _mm_shuffle_epi_0122(__m128i a)
649  {
650      int32x2_t a22 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), 0);
651      int32x2_t a01 = vrev64_s32(vget_low_s32(vreinterpretq_s32_m128i(a)));
652      return vreinterpretq_m128i_s32(vcombine_s32(a22, a01));
653  }
654  FORCE_INLINE __m128i _mm_shuffle_epi_3332(__m128i a)
655  {
656      int32x2_t a32 = vget_high_s32(vreinterpretq_s32_m128i(a));
657      int32x2_t a33 = vdup_lane_s32(vget_high_s32(vreinterpretq_s32_m128i(a)), 1);
658      return vreinterpretq_m128i_s32(vcombine_s32(a32, a33));
659  }
660  #if defined(__aarch64__)
661  #define _mm_shuffle_epi32_splat(a, imm)                          \
662      __extension__({                                              \
663          vreinterpretq_m128i_s32(                                 \
664              vdupq_laneq_s32(vreinterpretq_s32_m128i(a), (imm))); \
665      })
666  #else
667  #define _mm_shuffle_epi32_splat(a, imm)                                      \
668      __extension__({                                                          \
669          vreinterpretq_m128i_s32(                                             \
670              vdupq_n_s32(vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm)))); \
671      })
672  #endif
673  #define _mm_shuffle_ps_default(a, b, imm)                                  \
674      __extension__({                                                        \
675          float32x4_t ret;                                                   \
676          ret = vmovq_n_f32(                                                 \
677              vgetq_lane_f32(vreinterpretq_f32_m128(a), (imm) & (0x3)));     \
678          ret = vsetq_lane_f32(                                              \
679              vgetq_lane_f32(vreinterpretq_f32_m128(a), ((imm) >> 2) & 0x3), \
680              ret, 1);                                                       \
681          ret = vsetq_lane_f32(                                              \
682              vgetq_lane_f32(vreinterpretq_f32_m128(b), ((imm) >> 4) & 0x3), \
683              ret, 2);                                                       \
684          ret = vsetq_lane_f32(                                              \
685              vgetq_lane_f32(vreinterpretq_f32_m128(b), ((imm) >> 6) & 0x3), \
686              ret, 3);                                                       \
687          vreinterpretq_m128_f32(ret);                                       \
688      })
689  #define _mm_shufflelo_epi16_function(a, imm)                                  \
690      __extension__({                                                           \
691          int16x8_t ret = vreinterpretq_s16_m128i(a);                           \
692          int16x4_t lowBits = vget_low_s16(ret);                                \
693          ret = vsetq_lane_s16(vget_lane_s16(lowBits, (imm) & (0x3)), ret, 0);  \
694          ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) >> 2) & 0x3), ret, \
695                               1);                                              \
696          ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) >> 4) & 0x3), ret, \
697                               2);                                              \
698          ret = vsetq_lane_s16(vget_lane_s16(lowBits, ((imm) >> 6) & 0x3), ret, \
699                               3);                                              \
700          vreinterpretq_m128i_s16(ret);                                         \
701      })
702  #define _mm_shufflehi_epi16_function(a, imm)                                   \
703      __extension__({                                                            \
704          int16x8_t ret = vreinterpretq_s16_m128i(a);                            \
705          int16x4_t highBits = vget_high_s16(ret);                               \
706          ret = vsetq_lane_s16(vget_lane_s16(highBits, (imm) & (0x3)), ret, 4);  \
707          ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) >> 2) & 0x3), ret, \
708                               5);                                               \
709          ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) >> 4) & 0x3), ret, \
710                               6);                                               \
711          ret = vsetq_lane_s16(vget_lane_s16(highBits, ((imm) >> 6) & 0x3), ret, \
712                               7);                                               \
713          vreinterpretq_m128i_s16(ret);                                          \
714      })
715  FORCE_INLINE void _mm_empty(void) {}
716  FORCE_INLINE __m128 _mm_add_ps(__m128 a, __m128 b)
717  {
718      return vreinterpretq_m128_f32(
719          vaddq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
720  }
721  FORCE_INLINE __m128 _mm_add_ss(__m128 a, __m128 b)
722  {
723      float32_t b0 = vgetq_lane_f32(vreinterpretq_f32_m128(b), 0);
724      float32x4_t value = vsetq_lane_f32(b0, vdupq_n_f32(0), 0);
725      return vreinterpretq_m128_f32(vaddq_f32(a, value));
726  }
727  FORCE_INLINE __m128 _mm_and_ps(__m128 a, __m128 b)
728  {
729      return vreinterpretq_m128_s32(
730          vandq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));
731  }
732  FORCE_INLINE __m128 _mm_andnot_ps(__m128 a, __m128 b)
733  {
734      return vreinterpretq_m128_s32(
735          vbicq_s32(vreinterpretq_s32_m128(b),
736                    vreinterpretq_s32_m128(a)));  
737  }
738  FORCE_INLINE __m64 _mm_avg_pu16(__m64 a, __m64 b)
739  {
740      return vreinterpret_m64_u16(
741          vrhadd_u16(vreinterpret_u16_m64(a), vreinterpret_u16_m64(b)));
742  }
743  FORCE_INLINE __m64 _mm_avg_pu8(__m64 a, __m64 b)
744  {
745      return vreinterpret_m64_u8(
746          vrhadd_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));
747  }
748  FORCE_INLINE __m128 _mm_cmpeq_ps(__m128 a, __m128 b)
749  {
750      return vreinterpretq_m128_u32(
751          vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
752  }
753  FORCE_INLINE __m128 _mm_cmpeq_ss(__m128 a, __m128 b)
754  {
755      return _mm_move_ss(a, _mm_cmpeq_ps(a, b));
756  }
757  FORCE_INLINE __m128 _mm_cmpge_ps(__m128 a, __m128 b)
758  {
759      return vreinterpretq_m128_u32(
760          vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
761  }
762  FORCE_INLINE __m128 _mm_cmpge_ss(__m128 a, __m128 b)
763  {
764      return _mm_move_ss(a, _mm_cmpge_ps(a, b));
765  }
766  FORCE_INLINE __m128 _mm_cmpgt_ps(__m128 a, __m128 b)
767  {
768      return vreinterpretq_m128_u32(
769          vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
770  }
771  FORCE_INLINE __m128 _mm_cmpgt_ss(__m128 a, __m128 b)
772  {
773      return _mm_move_ss(a, _mm_cmpgt_ps(a, b));
774  }
775  FORCE_INLINE __m128 _mm_cmple_ps(__m128 a, __m128 b)
776  {
777      return vreinterpretq_m128_u32(
778          vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
779  }
780  FORCE_INLINE __m128 _mm_cmple_ss(__m128 a, __m128 b)
781  {
782      return _mm_move_ss(a, _mm_cmple_ps(a, b));
783  }
784  FORCE_INLINE __m128 _mm_cmplt_ps(__m128 a, __m128 b)
785  {
786      return vreinterpretq_m128_u32(
787          vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
788  }
789  FORCE_INLINE __m128 _mm_cmplt_ss(__m128 a, __m128 b)
790  {
791      return _mm_move_ss(a, _mm_cmplt_ps(a, b));
792  }
793  FORCE_INLINE __m128 _mm_cmpneq_ps(__m128 a, __m128 b)
794  {
795      return vreinterpretq_m128_u32(vmvnq_u32(
796          vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
797  }
798  FORCE_INLINE __m128 _mm_cmpneq_ss(__m128 a, __m128 b)
799  {
800      return _mm_move_ss(a, _mm_cmpneq_ps(a, b));
801  }
802  FORCE_INLINE __m128 _mm_cmpnge_ps(__m128 a, __m128 b)
803  {
804      return vreinterpretq_m128_u32(vmvnq_u32(
805          vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
806  }
807  FORCE_INLINE __m128 _mm_cmpnge_ss(__m128 a, __m128 b)
808  {
809      return _mm_move_ss(a, _mm_cmpnge_ps(a, b));
810  }
811  FORCE_INLINE __m128 _mm_cmpngt_ps(__m128 a, __m128 b)
812  {
813      return vreinterpretq_m128_u32(vmvnq_u32(
814          vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
815  }
816  FORCE_INLINE __m128 _mm_cmpngt_ss(__m128 a, __m128 b)
817  {
818      return _mm_move_ss(a, _mm_cmpngt_ps(a, b));
819  }
820  FORCE_INLINE __m128 _mm_cmpnle_ps(__m128 a, __m128 b)
821  {
822      return vreinterpretq_m128_u32(vmvnq_u32(
823          vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
824  }
825  FORCE_INLINE __m128 _mm_cmpnle_ss(__m128 a, __m128 b)
826  {
827      return _mm_move_ss(a, _mm_cmpnle_ps(a, b));
828  }
829  FORCE_INLINE __m128 _mm_cmpnlt_ps(__m128 a, __m128 b)
830  {
831      return vreinterpretq_m128_u32(vmvnq_u32(
832          vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b))));
833  }
834  FORCE_INLINE __m128 _mm_cmpnlt_ss(__m128 a, __m128 b)
835  {
836      return _mm_move_ss(a, _mm_cmpnlt_ps(a, b));
837  }
838  FORCE_INLINE __m128 _mm_cmpord_ps(__m128 a, __m128 b)
839  {
840      uint32x4_t ceqaa =
841          vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));
842      uint32x4_t ceqbb =
843          vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));
844      return vreinterpretq_m128_u32(vandq_u32(ceqaa, ceqbb));
845  }
846  FORCE_INLINE __m128 _mm_cmpord_ss(__m128 a, __m128 b)
847  {
848      return _mm_move_ss(a, _mm_cmpord_ps(a, b));
849  }
850  FORCE_INLINE __m128 _mm_cmpunord_ps(__m128 a, __m128 b)
851  {
852      uint32x4_t f32a =
853          vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a));
854      uint32x4_t f32b =
855          vceqq_f32(vreinterpretq_f32_m128(b), vreinterpretq_f32_m128(b));
856      return vreinterpretq_m128_u32(vmvnq_u32(vandq_u32(f32a, f32b)));
857  }
858  FORCE_INLINE __m128 _mm_cmpunord_ss(__m128 a, __m128 b)
859  {
860      return _mm_move_ss(a, _mm_cmpunord_ps(a, b));
861  }
862  FORCE_INLINE int _mm_comieq_ss(__m128 a, __m128 b)
863  {
864      uint32x4_t a_eq_b =
865          vceqq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
866      return vgetq_lane_u32(a_eq_b, 0) & 0x1;
867  }
868  FORCE_INLINE int _mm_comige_ss(__m128 a, __m128 b)
869  {
870      uint32x4_t a_ge_b =
871          vcgeq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
872      return vgetq_lane_u32(a_ge_b, 0) & 0x1;
873  }
874  FORCE_INLINE int _mm_comigt_ss(__m128 a, __m128 b)
875  {
876      uint32x4_t a_gt_b =
877          vcgtq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
878      return vgetq_lane_u32(a_gt_b, 0) & 0x1;
879  }
880  FORCE_INLINE int _mm_comile_ss(__m128 a, __m128 b)
881  {
882      uint32x4_t a_le_b =
883          vcleq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
884      return vgetq_lane_u32(a_le_b, 0) & 0x1;
885  }
886  FORCE_INLINE int _mm_comilt_ss(__m128 a, __m128 b)
887  {
888      uint32x4_t a_lt_b =
889          vcltq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b));
890      return vgetq_lane_u32(a_lt_b, 0) & 0x1;
891  }
892  FORCE_INLINE int _mm_comineq_ss(__m128 a, __m128 b)
893  {
894      return !_mm_comieq_ss(a, b);
895  }
896  FORCE_INLINE __m128 _mm_cvt_pi2ps(__m128 a, __m64 b)
897  {
898      return vreinterpretq_m128_f32(
899          vcombine_f32(vcvt_f32_s32(vreinterpret_s32_m64(b)),
900                       vget_high_f32(vreinterpretq_f32_m128(a))));
901  }
902  FORCE_INLINE __m64 _mm_cvt_ps2pi(__m128 a)
903  {
904  #if defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
905      return vreinterpret_m64_s32(
906          vget_low_s32(vcvtnq_s32_f32(vrndiq_f32(vreinterpretq_f32_m128(a)))));
907  #else
908      return vreinterpret_m64_s32(vcvt_s32_f32(vget_low_f32(
909          vreinterpretq_f32_m128(_mm_round_ps(a, _MM_FROUND_CUR_DIRECTION)))));
910  #endif
911  }
912  FORCE_INLINE __m128 _mm_cvt_si2ss(__m128 a, int b)
913  {
914      return vreinterpretq_m128_f32(
915          vsetq_lane_f32((float) b, vreinterpretq_f32_m128(a), 0));
916  }
917  FORCE_INLINE int _mm_cvt_ss2si(__m128 a)
918  {
919  #if defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
920      return vgetq_lane_s32(vcvtnq_s32_f32(vrndiq_f32(vreinterpretq_f32_m128(a))),
921                            0);
922  #else
923      float32_t data = vgetq_lane_f32(
924          vreinterpretq_f32_m128(_mm_round_ps(a, _MM_FROUND_CUR_DIRECTION)), 0);
925      return (int32_t) data;
926  #endif
927  }
928  FORCE_INLINE __m128 _mm_cvtpi16_ps(__m64 a)
929  {
930      return vreinterpretq_m128_f32(
931          vcvtq_f32_s32(vmovl_s16(vreinterpret_s16_m64(a))));
932  }
933  FORCE_INLINE __m128 _mm_cvtpi32_ps(__m128 a, __m64 b)
934  {
935      return vreinterpretq_m128_f32(
936          vcombine_f32(vcvt_f32_s32(vreinterpret_s32_m64(b)),
937                       vget_high_f32(vreinterpretq_f32_m128(a))));
938  }
939  FORCE_INLINE __m128 _mm_cvtpi32x2_ps(__m64 a, __m64 b)
940  {
941      return vreinterpretq_m128_f32(vcvtq_f32_s32(
942          vcombine_s32(vreinterpret_s32_m64(a), vreinterpret_s32_m64(b))));
943  }
944  FORCE_INLINE __m128 _mm_cvtpi8_ps(__m64 a)
945  {
946      return vreinterpretq_m128_f32(vcvtq_f32_s32(
947          vmovl_s16(vget_low_s16(vmovl_s8(vreinterpret_s8_m64(a))))));
948  }
949  FORCE_INLINE __m64 _mm_cvtps_pi16(__m128 a)
950  {
951      return vreinterpret_m64_s16(
952          vqmovn_s32(vreinterpretq_s32_m128i(_mm_cvtps_epi32(a))));
953  }
954  #define _mm_cvtps_pi32(a) _mm_cvt_ps2pi(a)
955  FORCE_INLINE __m64 _mm_cvtps_pi8(__m128 a)
956  {
957      return vreinterpret_m64_s8(vqmovn_s16(
958          vcombine_s16(vreinterpret_s16_m64(_mm_cvtps_pi16(a)), vdup_n_s16(0))));
959  }
960  FORCE_INLINE __m128 _mm_cvtpu16_ps(__m64 a)
961  {
962      return vreinterpretq_m128_f32(
963          vcvtq_f32_u32(vmovl_u16(vreinterpret_u16_m64(a))));
964  }
965  FORCE_INLINE __m128 _mm_cvtpu8_ps(__m64 a)
966  {
967      return vreinterpretq_m128_f32(vcvtq_f32_u32(
968          vmovl_u16(vget_low_u16(vmovl_u8(vreinterpret_u8_m64(a))))));
969  }
970  #define _mm_cvtsi32_ss(a, b) _mm_cvt_si2ss(a, b)
971  FORCE_INLINE __m128 _mm_cvtsi64_ss(__m128 a, int64_t b)
972  {
973      return vreinterpretq_m128_f32(
974          vsetq_lane_f32((float) b, vreinterpretq_f32_m128(a), 0));
975  }
976  FORCE_INLINE float _mm_cvtss_f32(__m128 a)
977  {
978      return vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
979  }
980  #define _mm_cvtss_si32(a) _mm_cvt_ss2si(a)
981  FORCE_INLINE int64_t _mm_cvtss_si64(__m128 a)
982  {
983  #if defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
984      return (int64_t) vgetq_lane_f32(vrndiq_f32(vreinterpretq_f32_m128(a)), 0);
985  #else
986      float32_t data = vgetq_lane_f32(
987          vreinterpretq_f32_m128(_mm_round_ps(a, _MM_FROUND_CUR_DIRECTION)), 0);
988      return (int64_t) data;
989  #endif
990  }
991  FORCE_INLINE __m64 _mm_cvtt_ps2pi(__m128 a)
992  {
993      return vreinterpret_m64_s32(
994          vget_low_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a))));
995  }
996  FORCE_INLINE int _mm_cvtt_ss2si(__m128 a)
997  {
998      return vgetq_lane_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)), 0);
999  }
1000  #define _mm_cvttps_pi32(a) _mm_cvtt_ps2pi(a)
1001  #define _mm_cvttss_si32(a) _mm_cvtt_ss2si(a)
1002  FORCE_INLINE int64_t _mm_cvttss_si64(__m128 a)
1003  {
1004      return (int64_t) vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
1005  }
1006  FORCE_INLINE __m128 _mm_div_ps(__m128 a, __m128 b)
1007  {
1008  #if defined(__aarch64__) && !SSE2NEON_PRECISE_DIV
1009      return vreinterpretq_m128_f32(
1010          vdivq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1011  #else
1012      float32x4_t recip = vrecpeq_f32(vreinterpretq_f32_m128(b));
1013      recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(b)));
1014  #if SSE2NEON_PRECISE_DIV
1015      recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(b)));
1016  #endif
1017      return vreinterpretq_m128_f32(vmulq_f32(vreinterpretq_f32_m128(a), recip));
1018  #endif
1019  }
1020  FORCE_INLINE __m128 _mm_div_ss(__m128 a, __m128 b)
1021  {
1022      float32_t value =
1023          vgetq_lane_f32(vreinterpretq_f32_m128(_mm_div_ps(a, b)), 0);
1024      return vreinterpretq_m128_f32(
1025          vsetq_lane_f32(value, vreinterpretq_f32_m128(a), 0));
1026  }
1027  #define _mm_extract_pi16(a, imm) \
1028      (int32_t) vget_lane_u16(vreinterpret_u16_m64(a), (imm))
1029  #if !defined(SSE2NEON_ALLOC_DEFINED)
1030  FORCE_INLINE void _mm_free(void *addr)
1031  {
1032      free(addr);
1033  }
1034  #endif
1035  FORCE_INLINE unsigned int _sse2neon_mm_get_flush_zero_mode()
1036  {
1037      union {
1038          fpcr_bitfield field;
1039  #if defined(__aarch64__)
1040          uint64_t value;
1041  #else
1042          uint32_t value;
1043  #endif
1044      } r;
1045  #if defined(__aarch64__)
1046      __asm__ __volatile__("mrs %0, FPCR" : "=r"(r.value)); &bsol;* read */
1047  #else
1048      __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); &bsol;* read */
1049  #endif
1050      return r.field.bit24 ? _MM_FLUSH_ZERO_ON : _MM_FLUSH_ZERO_OFF;
1051  }
1052  FORCE_INLINE unsigned int _MM_GET_ROUNDING_MODE()
1053  {
1054      union {
1055          fpcr_bitfield field;
1056  #if defined(__aarch64__)
1057          uint64_t value;
1058  #else
1059          uint32_t value;
1060  #endif
1061      } r;
1062  #if defined(__aarch64__)
1063      __asm__ __volatile__("mrs %0, FPCR" : "=r"(r.value)); &bsol;* read */
1064  #else
1065      __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); &bsol;* read */
1066  #endif
1067      if (r.field.bit22) {
1068          return r.field.bit23 ? _MM_ROUND_TOWARD_ZERO : _MM_ROUND_UP;
1069      } else {
1070          return r.field.bit23 ? _MM_ROUND_DOWN : _MM_ROUND_NEAREST;
1071      }
1072  }
1073  #define _mm_insert_pi16(a, b, imm)                               \
1074      __extension__({                                              \
1075          vreinterpret_m64_s16(                                    \
1076              vset_lane_s16((b), vreinterpret_s16_m64(a), (imm))); \
1077      })
1078  FORCE_INLINE __m128 _mm_load_ps(const float *p)
1079  {
1080      return vreinterpretq_m128_f32(vld1q_f32(p));
1081  }
1082  #define _mm_load_ps1 _mm_load1_ps
1083  FORCE_INLINE __m128 _mm_load_ss(const float *p)
1084  {
1085      return vreinterpretq_m128_f32(vsetq_lane_f32(*p, vdupq_n_f32(0), 0));
1086  }
1087  FORCE_INLINE __m128 _mm_load1_ps(const float *p)
1088  {
1089      return vreinterpretq_m128_f32(vld1q_dup_f32(p));
1090  }
1091  FORCE_INLINE __m128 _mm_loadh_pi(__m128 a, __m64 const *p)
1092  {
1093      return vreinterpretq_m128_f32(
1094          vcombine_f32(vget_low_f32(a), vld1_f32((const float32_t *) p)));
1095  }
1096  FORCE_INLINE __m128 _mm_loadl_pi(__m128 a, __m64 const *p)
1097  {
1098      return vreinterpretq_m128_f32(
1099          vcombine_f32(vld1_f32((const float32_t *) p), vget_high_f32(a)));
1100  }
1101  FORCE_INLINE __m128 _mm_loadr_ps(const float *p)
1102  {
1103      float32x4_t v = vrev64q_f32(vld1q_f32(p));
1104      return vreinterpretq_m128_f32(vextq_f32(v, v, 2));
1105  }
1106  FORCE_INLINE __m128 _mm_loadu_ps(const float *p)
1107  {
1108      return vreinterpretq_m128_f32(vld1q_f32(p));
1109  }
1110  FORCE_INLINE __m128i _mm_loadu_si16(const void *p)
1111  {
1112      return vreinterpretq_m128i_s16(
1113          vsetq_lane_s16(*(const int16_t *) p, vdupq_n_s16(0), 0));
1114  }
1115  FORCE_INLINE __m128i _mm_loadu_si64(const void *p)
1116  {
1117      return vreinterpretq_m128i_s64(
1118          vcombine_s64(vld1_s64((const int64_t *) p), vdup_n_s64(0)));
1119  }
1120  #if !defined(SSE2NEON_ALLOC_DEFINED)
1121  FORCE_INLINE void *_mm_malloc(size_t size, size_t align)
1122  {
1123      void *ptr;
1124      if (align == 1)
1125          return malloc(size);
1126      if (align == 2 || (sizeof(void *) == 8 && align == 4))
1127          align = sizeof(void *);
1128      if (!posix_memalign(&ptr, align, size))
1129          return ptr;
1130      return NULL;
1131  }
1132  #endif
1133  FORCE_INLINE void _mm_maskmove_si64(__m64 a, __m64 mask, char *mem_addr)
1134  {
1135      int8x8_t shr_mask = vshr_n_s8(vreinterpret_s8_m64(mask), 7);
1136      __m128 b = _mm_load_ps((const float *) mem_addr);
1137      int8x8_t masked =
1138          vbsl_s8(vreinterpret_u8_s8(shr_mask), vreinterpret_s8_m64(a),
1139                  vreinterpret_s8_u64(vget_low_u64(vreinterpretq_u64_m128(b))));
1140      vst1_s8((int8_t *) mem_addr, masked);
1141  }
1142  #define _m_maskmovq(a, mask, mem_addr) _mm_maskmove_si64(a, mask, mem_addr)
1143  FORCE_INLINE __m64 _mm_max_pi16(__m64 a, __m64 b)
1144  {
1145      return vreinterpret_m64_s16(
1146          vmax_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));
1147  }
1148  FORCE_INLINE __m128 _mm_max_ps(__m128 a, __m128 b)
1149  {
1150  #if SSE2NEON_PRECISE_MINMAX
1151      float32x4_t _a = vreinterpretq_f32_m128(a);
1152      float32x4_t _b = vreinterpretq_f32_m128(b);
1153      return vreinterpretq_m128_f32(vbslq_f32(vcgtq_f32(_a, _b), _a, _b));
1154  #else
1155      return vreinterpretq_m128_f32(
1156          vmaxq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1157  #endif
1158  }
1159  FORCE_INLINE __m64 _mm_max_pu8(__m64 a, __m64 b)
1160  {
1161      return vreinterpret_m64_u8(
1162          vmax_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));
1163  }
1164  FORCE_INLINE __m128 _mm_max_ss(__m128 a, __m128 b)
1165  {
1166      float32_t value = vgetq_lane_f32(_mm_max_ps(a, b), 0);
1167      return vreinterpretq_m128_f32(
1168          vsetq_lane_f32(value, vreinterpretq_f32_m128(a), 0));
1169  }
1170  FORCE_INLINE __m64 _mm_min_pi16(__m64 a, __m64 b)
1171  {
1172      return vreinterpret_m64_s16(
1173          vmin_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));
1174  }
1175  FORCE_INLINE __m128 _mm_min_ps(__m128 a, __m128 b)
1176  {
1177  #if SSE2NEON_PRECISE_MINMAX
1178      float32x4_t _a = vreinterpretq_f32_m128(a);
1179      float32x4_t _b = vreinterpretq_f32_m128(b);
1180      return vreinterpretq_m128_f32(vbslq_f32(vcltq_f32(_a, _b), _a, _b));
1181  #else
1182      return vreinterpretq_m128_f32(
1183          vminq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1184  #endif
1185  }
1186  FORCE_INLINE __m64 _mm_min_pu8(__m64 a, __m64 b)
1187  {
1188      return vreinterpret_m64_u8(
1189          vmin_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)));
1190  }
1191  FORCE_INLINE __m128 _mm_min_ss(__m128 a, __m128 b)
1192  {
1193      float32_t value = vgetq_lane_f32(_mm_min_ps(a, b), 0);
1194      return vreinterpretq_m128_f32(
1195          vsetq_lane_f32(value, vreinterpretq_f32_m128(a), 0));
1196  }
1197  FORCE_INLINE __m128 _mm_move_ss(__m128 a, __m128 b)
1198  {
1199      return vreinterpretq_m128_f32(
1200          vsetq_lane_f32(vgetq_lane_f32(vreinterpretq_f32_m128(b), 0),
1201                         vreinterpretq_f32_m128(a), 0));
1202  }
1203  FORCE_INLINE __m128 _mm_movehl_ps(__m128 a, __m128 b)
1204  {
1205  #if defined(aarch64__)
1206      return vreinterpretq_m128_u64(
1207          vzip2q_u64(vreinterpretq_u64_m128(b), vreinterpretq_u64_m128(a)));
1208  #else
1209      float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
1210      float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
1211      return vreinterpretq_m128_f32(vcombine_f32(b32, a32));
1212  #endif
1213  }
1214  FORCE_INLINE __m128 _mm_movelh_ps(__m128 __A, __m128 __B)
1215  {
1216      float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(__A));
1217      float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(__B));
1218      return vreinterpretq_m128_f32(vcombine_f32(a10, b10));
1219  }
1220  FORCE_INLINE int _mm_movemask_pi8(__m64 a)
1221  {
1222      uint8x8_t input = vreinterpret_u8_m64(a);
1223  #if defined(__aarch64__)
1224      static const int8x8_t shift = {0, 1, 2, 3, 4, 5, 6, 7};
1225      uint8x8_t tmp = vshr_n_u8(input, 7);
1226      return vaddv_u8(vshl_u8(tmp, shift));
1227  #else
1228      uint16x4_t high_bits = vreinterpret_u16_u8(vshr_n_u8(input, 7));
1229      uint32x2_t paired16 =
1230          vreinterpret_u32_u16(vsra_n_u16(high_bits, high_bits, 7));
1231      uint8x8_t paired32 =
1232          vreinterpret_u8_u32(vsra_n_u32(paired16, paired16, 14));
1233      return vget_lane_u8(paired32, 0) | ((int) vget_lane_u8(paired32, 4) << 4);
1234  #endif
1235  }
1236  FORCE_INLINE int _mm_movemask_ps(__m128 a)
1237  {
1238      uint32x4_t input = vreinterpretq_u32_m128(a);
1239  #if defined(__aarch64__)
1240      static const int32x4_t shift = {0, 1, 2, 3};
1241      uint32x4_t tmp = vshrq_n_u32(input, 31);
1242      return vaddvq_u32(vshlq_u32(tmp, shift));
1243  #else
1244      uint64x2_t high_bits = vreinterpretq_u64_u32(vshrq_n_u32(input, 31));
1245      uint8x16_t paired =
1246          vreinterpretq_u8_u64(vsraq_n_u64(high_bits, high_bits, 31));
1247      return vgetq_lane_u8(paired, 0) | (vgetq_lane_u8(paired, 8) << 2);
1248  #endif
1249  }
1250  FORCE_INLINE __m128 _mm_mul_ps(__m128 a, __m128 b)
1251  {
1252      return vreinterpretq_m128_f32(
1253          vmulq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1254  }
1255  FORCE_INLINE __m128 _mm_mul_ss(__m128 a, __m128 b)
1256  {
1257      return _mm_move_ss(a, _mm_mul_ps(a, b));
1258  }
1259  FORCE_INLINE __m64 _mm_mulhi_pu16(__m64 a, __m64 b)
1260  {
1261      return vreinterpret_m64_u16(vshrn_n_u32(
1262          vmull_u16(vreinterpret_u16_m64(a), vreinterpret_u16_m64(b)), 16));
1263  }
1264  FORCE_INLINE __m128 _mm_or_ps(__m128 a, __m128 b)
1265  {
1266      return vreinterpretq_m128_s32(
1267          vorrq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));
1268  }
1269  #define _m_pavgb(a, b) _mm_avg_pu8(a, b)
1270  #define _m_pavgw(a, b) _mm_avg_pu16(a, b)
1271  #define _m_pextrw(a, imm) _mm_extract_pi16(a, imm)
1272  #define _m_pinsrw(a, i, imm) _mm_insert_pi16(a, i, imm)
1273  #define _m_pmaxsw(a, b) _mm_max_pi16(a, b)
1274  #define _m_pmaxub(a, b) _mm_max_pu8(a, b)
1275  #define _m_pminsw(a, b) _mm_min_pi16(a, b)
1276  #define _m_pminub(a, b) _mm_min_pu8(a, b)
1277  #define _m_pmovmskb(a) _mm_movemask_pi8(a)
1278  #define _m_pmulhuw(a, b) _mm_mulhi_pu16(a, b)
1279  FORCE_INLINE void _mm_prefetch(char const *p, int i)
1280  {
1281      switch (i) {
1282      case _MM_HINT_NTA:
1283          __builtin_prefetch(p, 0, 0);
1284          break;
1285      case _MM_HINT_T0:
1286          __builtin_prefetch(p, 0, 3);
1287          break;
1288      case _MM_HINT_T1:
1289          __builtin_prefetch(p, 0, 2);
1290          break;
1291      case _MM_HINT_T2:
1292          __builtin_prefetch(p, 0, 1);
1293          break;
1294      }
1295  }
1296  #define _m_psadbw(a, b) _mm_sad_pu8(a, b)
1297  #define _m_pshufw(a, imm) _mm_shuffle_pi16(a, imm)
1298  FORCE_INLINE __m128 _mm_rcp_ps(__m128 in)
1299  {
1300      float32x4_t recip = vrecpeq_f32(vreinterpretq_f32_m128(in));
1301      recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(in)));
1302  #if SSE2NEON_PRECISE_DIV
1303      recip = vmulq_f32(recip, vrecpsq_f32(recip, vreinterpretq_f32_m128(in)));
1304  #endif
1305      return vreinterpretq_m128_f32(recip);
1306  }
1307  FORCE_INLINE __m128 _mm_rcp_ss(__m128 a)
1308  {
1309      return _mm_move_ss(a, _mm_rcp_ps(a));
1310  }
1311  FORCE_INLINE __m128 _mm_rsqrt_ps(__m128 in)
1312  {
1313      float32x4_t out = vrsqrteq_f32(vreinterpretq_f32_m128(in));
1314  #if SSE2NEON_PRECISE_SQRT
1315      out = vmulq_f32(
1316          out, vrsqrtsq_f32(vmulq_f32(vreinterpretq_f32_m128(in), out), out));
1317      out = vmulq_f32(
1318          out, vrsqrtsq_f32(vmulq_f32(vreinterpretq_f32_m128(in), out), out));
1319  #endif
1320      return vreinterpretq_m128_f32(out);
1321  }
1322  FORCE_INLINE __m128 _mm_rsqrt_ss(__m128 in)
1323  {
1324      return vsetq_lane_f32(vgetq_lane_f32(_mm_rsqrt_ps(in), 0), in, 0);
1325  }
1326  FORCE_INLINE __m64 _mm_sad_pu8(__m64 a, __m64 b)
1327  {
1328      uint64x1_t t = vpaddl_u32(vpaddl_u16(
1329          vpaddl_u8(vabd_u8(vreinterpret_u8_m64(a), vreinterpret_u8_m64(b)))));
1330      return vreinterpret_m64_u16(
1331          vset_lane_u16(vget_lane_u64(t, 0), vdup_n_u16(0), 0));
1332  }
1333  FORCE_INLINE void _sse2neon_mm_set_flush_zero_mode(unsigned int flag)
1334  {
1335      union {
1336          fpcr_bitfield field;
1337  #if defined(__aarch64__)
1338          uint64_t value;
1339  #else
1340          uint32_t value;
1341  #endif
1342      } r;
1343  #if defined(__aarch64__)
1344      __asm__ __volatile__("mrs %0, FPCR" : "=r"(r.value)); &bsol;* read */
1345  #else
1346      __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); &bsol;* read */
1347  #endif
1348      r.field.bit24 = (flag & _MM_FLUSH_ZERO_MASK) == _MM_FLUSH_ZERO_ON;
1349  #if defined(__aarch64__)
1350      __asm__ __volatile__("msr FPCR, %0" ::"r"(r)); &bsol;* write */
1351  #else
1352      __asm__ __volatile__("vmsr FPSCR, %0" ::"r"(r));        &bsol;* write */
1353  #endif
1354  }
1355  FORCE_INLINE __m128 _mm_set_ps(float w, float z, float y, float x)
1356  {
1357      float ALIGN_STRUCT(16) data[4] = {x, y, z, w};
1358      return vreinterpretq_m128_f32(vld1q_f32(data));
1359  }
1360  FORCE_INLINE __m128 _mm_set_ps1(float _w)
1361  {
1362      return vreinterpretq_m128_f32(vdupq_n_f32(_w));
1363  }
1364  FORCE_INLINE void _MM_SET_ROUNDING_MODE(int rounding)
1365  {
1366      union {
1367          fpcr_bitfield field;
1368  #if defined(__aarch64__)
1369          uint64_t value;
1370  #else
1371          uint32_t value;
1372  #endif
1373      } r;
1374  #if defined(__aarch64__)
1375      __asm__ __volatile__("mrs %0, FPCR" : "=r"(r.value)); &bsol;* read */
1376  #else
1377      __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); &bsol;* read */
1378  #endif
1379      switch (rounding) {
1380      case _MM_ROUND_TOWARD_ZERO:
1381          r.field.bit22 = 1;
1382          r.field.bit23 = 1;
1383          break;
1384      case _MM_ROUND_DOWN:
1385          r.field.bit22 = 0;
1386          r.field.bit23 = 1;
1387          break;
1388      case _MM_ROUND_UP:
1389          r.field.bit22 = 1;
1390          r.field.bit23 = 0;
1391          break;
1392      default:  
1393          r.field.bit22 = 0;
1394          r.field.bit23 = 0;
1395      }
1396  #if defined(__aarch64__)
1397      __asm__ __volatile__("msr FPCR, %0" ::"r"(r)); &bsol;* write */
1398  #else
1399      __asm__ __volatile__("vmsr FPSCR, %0" ::"r"(r));        &bsol;* write */
1400  #endif
1401  }
1402  FORCE_INLINE __m128 _mm_set_ss(float a)
1403  {
1404      return vreinterpretq_m128_f32(vsetq_lane_f32(a, vdupq_n_f32(0), 0));
1405  }
1406  FORCE_INLINE __m128 _mm_set1_ps(float _w)
1407  {
1408      return vreinterpretq_m128_f32(vdupq_n_f32(_w));
1409  }
1410  FORCE_INLINE void _mm_setcsr(unsigned int a)
1411  {
1412      _MM_SET_ROUNDING_MODE(a);
1413  }
1414  FORCE_INLINE unsigned int _mm_getcsr()
1415  {
1416      return _MM_GET_ROUNDING_MODE();
1417  }
1418  FORCE_INLINE __m128 _mm_setr_ps(float w, float z, float y, float x)
1419  {
1420      float ALIGN_STRUCT(16) data[4] = {w, z, y, x};
1421      return vreinterpretq_m128_f32(vld1q_f32(data));
1422  }
1423  FORCE_INLINE __m128 _mm_setzero_ps(void)
1424  {
1425      return vreinterpretq_m128_f32(vdupq_n_f32(0));
1426  }
1427  #ifdef _sse2neon_shuffle
1428  #define _mm_shuffle_pi16(a, imm)                                           \
1429      __extension__({                                                        \
1430          vreinterpret_m64_s16(vshuffle_s16(                                 \
1431              vreinterpret_s16_m64(a), vreinterpret_s16_m64(a), (imm & 0x3), \
1432              ((imm >> 2) & 0x3), ((imm >> 4) & 0x3), ((imm >> 6) & 0x3)));  \
1433      })
1434  #else
1435  #define _mm_shuffle_pi16(a, imm)                                               \
1436      __extension__({                                                            \
1437          int16x4_t ret;                                                         \
1438          ret =                                                                  \
1439              vmov_n_s16(vget_lane_s16(vreinterpret_s16_m64(a), (imm) & (0x3))); \
1440          ret = vset_lane_s16(                                                   \
1441              vget_lane_s16(vreinterpret_s16_m64(a), ((imm) >> 2) & 0x3), ret,   \
1442              1);                                                                \
1443          ret = vset_lane_s16(                                                   \
1444              vget_lane_s16(vreinterpret_s16_m64(a), ((imm) >> 4) & 0x3), ret,   \
1445              2);                                                                \
1446          ret = vset_lane_s16(                                                   \
1447              vget_lane_s16(vreinterpret_s16_m64(a), ((imm) >> 6) & 0x3), ret,   \
1448              3);                                                                \
1449          vreinterpret_m64_s16(ret);                                             \
1450      })
1451  #endif
1452  FORCE_INLINE void _mm_sfence(void)
1453  {
1454      _sse2neon_smp_mb();
1455  }
1456  FORCE_INLINE void _mm_mfence(void)
1457  {
1458      _sse2neon_smp_mb();
1459  }
1460  FORCE_INLINE void _mm_lfence(void)
1461  {
1462      _sse2neon_smp_mb();
1463  }
1464  #ifdef _sse2neon_shuffle
1465  #define _mm_shuffle_ps(a, b, imm)                                              \
1466      __extension__({                                                            \
1467          float32x4_t _input1 = vreinterpretq_f32_m128(a);                       \
1468          float32x4_t _input2 = vreinterpretq_f32_m128(b);                       \
1469          float32x4_t _shuf =                                                    \
1470              vshuffleq_s32(_input1, _input2, (imm) & (0x3), ((imm) >> 2) & 0x3, \
1471                            (((imm) >> 4) & 0x3) + 4, (((imm) >> 6) & 0x3) + 4); \
1472          vreinterpretq_m128_f32(_shuf);                                         \
1473      })
1474  #else  
1475  #define _mm_shuffle_ps(a, b, imm)                          \
1476      __extension__({                                        \
1477          __m128 ret;                                        \
1478          switch (imm) {                                     \
1479          case _MM_SHUFFLE(1, 0, 3, 2):                      \
1480              ret = _mm_shuffle_ps_1032((a), (b));           \
1481              break;                                         \
1482          case _MM_SHUFFLE(2, 3, 0, 1):                      \
1483              ret = _mm_shuffle_ps_2301((a), (b));           \
1484              break;                                         \
1485          case _MM_SHUFFLE(0, 3, 2, 1):                      \
1486              ret = _mm_shuffle_ps_0321((a), (b));           \
1487              break;                                         \
1488          case _MM_SHUFFLE(2, 1, 0, 3):                      \
1489              ret = _mm_shuffle_ps_2103((a), (b));           \
1490              break;                                         \
1491          case _MM_SHUFFLE(1, 0, 1, 0):                      \
1492              ret = _mm_movelh_ps((a), (b));                 \
1493              break;                                         \
1494          case _MM_SHUFFLE(1, 0, 0, 1):                      \
1495              ret = _mm_shuffle_ps_1001((a), (b));           \
1496              break;                                         \
1497          case _MM_SHUFFLE(0, 1, 0, 1):                      \
1498              ret = _mm_shuffle_ps_0101((a), (b));           \
1499              break;                                         \
1500          case _MM_SHUFFLE(3, 2, 1, 0):                      \
1501              ret = _mm_shuffle_ps_3210((a), (b));           \
1502              break;                                         \
1503          case _MM_SHUFFLE(0, 0, 1, 1):                      \
1504              ret = _mm_shuffle_ps_0011((a), (b));           \
1505              break;                                         \
1506          case _MM_SHUFFLE(0, 0, 2, 2):                      \
1507              ret = _mm_shuffle_ps_0022((a), (b));           \
1508              break;                                         \
1509          case _MM_SHUFFLE(2, 2, 0, 0):                      \
1510              ret = _mm_shuffle_ps_2200((a), (b));           \
1511              break;                                         \
1512          case _MM_SHUFFLE(3, 2, 0, 2):                      \
1513              ret = _mm_shuffle_ps_3202((a), (b));           \
1514              break;                                         \
1515          case _MM_SHUFFLE(3, 2, 3, 2):                      \
1516              ret = _mm_movehl_ps((b), (a));                 \
1517              break;                                         \
1518          case _MM_SHUFFLE(1, 1, 3, 3):                      \
1519              ret = _mm_shuffle_ps_1133((a), (b));           \
1520              break;                                         \
1521          case _MM_SHUFFLE(2, 0, 1, 0):                      \
1522              ret = _mm_shuffle_ps_2010((a), (b));           \
1523              break;                                         \
1524          case _MM_SHUFFLE(2, 0, 0, 1):                      \
1525              ret = _mm_shuffle_ps_2001((a), (b));           \
1526              break;                                         \
1527          case _MM_SHUFFLE(2, 0, 3, 2):                      \
1528              ret = _mm_shuffle_ps_2032((a), (b));           \
1529              break;                                         \
1530          default:                                           \
1531              ret = _mm_shuffle_ps_default((a), (b), (imm)); \
1532              break;                                         \
1533          }                                                  \
1534          ret;                                               \
1535      })
1536  #endif
1537  FORCE_INLINE __m128 _mm_sqrt_ps(__m128 in)
1538  {
1539  #if SSE2NEON_PRECISE_SQRT
1540      float32x4_t recip = vrsqrteq_f32(vreinterpretq_f32_m128(in));
1541      const uint32x4_t pos_inf = vdupq_n_u32(0x7F800000);
1542      const uint32x4_t div_by_zero =
1543          vceqq_u32(pos_inf, vreinterpretq_u32_f32(recip));
1544      recip = vreinterpretq_f32_u32(
1545          vandq_u32(vmvnq_u32(div_by_zero), vreinterpretq_u32_f32(recip)));
1546      recip = vmulq_f32(
1547          vrsqrtsq_f32(vmulq_f32(recip, recip), vreinterpretq_f32_m128(in)),
1548          recip);
1549      recip = vmulq_f32(
1550          vrsqrtsq_f32(vmulq_f32(recip, recip), vreinterpretq_f32_m128(in)),
1551          recip);
1552      return vreinterpretq_m128_f32(vmulq_f32(vreinterpretq_f32_m128(in), recip));
1553  #elif defined(__aarch64__)
1554      return vreinterpretq_m128_f32(vsqrtq_f32(vreinterpretq_f32_m128(in)));
1555  #else
1556      float32x4_t recipsq = vrsqrteq_f32(vreinterpretq_f32_m128(in));
1557      float32x4_t sq = vrecpeq_f32(recipsq);
1558      return vreinterpretq_m128_f32(sq);
1559  #endif
1560  }
1561  FORCE_INLINE __m128 _mm_sqrt_ss(__m128 in)
1562  {
1563      float32_t value =
1564          vgetq_lane_f32(vreinterpretq_f32_m128(_mm_sqrt_ps(in)), 0);
1565      return vreinterpretq_m128_f32(
1566          vsetq_lane_f32(value, vreinterpretq_f32_m128(in), 0));
1567  }
1568  FORCE_INLINE void _mm_store_ps(float *p, __m128 a)
1569  {
1570      vst1q_f32(p, vreinterpretq_f32_m128(a));
1571  }
1572  FORCE_INLINE void _mm_store_ps1(float *p, __m128 a)
1573  {
1574      float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
1575      vst1q_f32(p, vdupq_n_f32(a0));
1576  }
1577  FORCE_INLINE void _mm_store_ss(float *p, __m128 a)
1578  {
1579      vst1q_lane_f32(p, vreinterpretq_f32_m128(a), 0);
1580  }
1581  #define _mm_store1_ps _mm_store_ps1
1582  FORCE_INLINE void _mm_storeh_pi(__m64 *p, __m128 a)
1583  {
1584      *p = vreinterpret_m64_f32(vget_high_f32(a));
1585  }
1586  FORCE_INLINE void _mm_storel_pi(__m64 *p, __m128 a)
1587  {
1588      *p = vreinterpret_m64_f32(vget_low_f32(a));
1589  }
1590  FORCE_INLINE void _mm_storer_ps(float *p, __m128 a)
1591  {
1592      float32x4_t tmp = vrev64q_f32(vreinterpretq_f32_m128(a));
1593      float32x4_t rev = vextq_f32(tmp, tmp, 2);
1594      vst1q_f32(p, rev);
1595  }
1596  FORCE_INLINE void _mm_storeu_ps(float *p, __m128 a)
1597  {
1598      vst1q_f32(p, vreinterpretq_f32_m128(a));
1599  }
1600  FORCE_INLINE void _mm_storeu_si16(void *p, __m128i a)
1601  {
1602      vst1q_lane_s16((int16_t *) p, vreinterpretq_s16_m128i(a), 0);
1603  }
1604  FORCE_INLINE void _mm_storeu_si64(void *p, __m128i a)
1605  {
1606      vst1q_lane_s64((int64_t *) p, vreinterpretq_s64_m128i(a), 0);
1607  }
1608  FORCE_INLINE void _mm_stream_pi(__m64 *p, __m64 a)
1609  {
1610      vst1_s64((int64_t *) p, vreinterpret_s64_m64(a));
1611  }
1612  FORCE_INLINE void _mm_stream_ps(float *p, __m128 a)
1613  {
1614  #if __has_builtin(__builtin_nontemporal_store)
1615      __builtin_nontemporal_store(a, (float32x4_t *) p);
1616  #else
1617      vst1q_f32(p, vreinterpretq_f32_m128(a));
1618  #endif
1619  }
1620  FORCE_INLINE __m128 _mm_sub_ps(__m128 a, __m128 b)
1621  {
1622      return vreinterpretq_m128_f32(
1623          vsubq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1624  }
1625  FORCE_INLINE __m128 _mm_sub_ss(__m128 a, __m128 b)
1626  {
1627      return _mm_move_ss(a, _mm_sub_ps(a, b));
1628  }
1629  #define _MM_TRANSPOSE4_PS(row0, row1, row2, row3)         \
1630      do {                                                  \
1631          float32x4x2_t ROW01 = vtrnq_f32(row0, row1);      \
1632          float32x4x2_t ROW23 = vtrnq_f32(row2, row3);      \
1633          row0 = vcombine_f32(vget_low_f32(ROW01.val[0]),   \
1634                              vget_low_f32(ROW23.val[0]));  \
1635          row1 = vcombine_f32(vget_low_f32(ROW01.val[1]),   \
1636                              vget_low_f32(ROW23.val[1]));  \
1637          row2 = vcombine_f32(vget_high_f32(ROW01.val[0]),  \
1638                              vget_high_f32(ROW23.val[0])); \
1639          row3 = vcombine_f32(vget_high_f32(ROW01.val[1]),  \
1640                              vget_high_f32(ROW23.val[1])); \
1641      } while (0)
1642  #define _mm_ucomieq_ss _mm_comieq_ss
1643  #define _mm_ucomige_ss _mm_comige_ss
1644  #define _mm_ucomigt_ss _mm_comigt_ss
1645  #define _mm_ucomile_ss _mm_comile_ss
1646  #define _mm_ucomilt_ss _mm_comilt_ss
1647  #define _mm_ucomineq_ss _mm_comineq_ss
1648  FORCE_INLINE __m128i _mm_undefined_si128(void)
1649  {
1650  #if defined(__GNUC__) || defined(__clang__)
1651  #pragma GCC diagnostic push
1652  #pragma GCC diagnostic ignored "-Wuninitialized"
1653  #endif
1654      __m128i a;
1655      return a;
1656  #if defined(__GNUC__) || defined(__clang__)
1657  #pragma GCC diagnostic pop
1658  #endif
1659  }
1660  FORCE_INLINE __m128 _mm_undefined_ps(void)
1661  {
1662  #if defined(__GNUC__) || defined(__clang__)
1663  #pragma GCC diagnostic push
1664  #pragma GCC diagnostic ignored "-Wuninitialized"
1665  #endif
1666      __m128 a;
1667      return a;
1668  #if defined(__GNUC__) || defined(__clang__)
1669  #pragma GCC diagnostic pop
1670  #endif
1671  }
1672  FORCE_INLINE __m128 _mm_unpackhi_ps(__m128 a, __m128 b)
1673  {
1674  #if defined(__aarch64__)
1675      return vreinterpretq_m128_f32(
1676          vzip2q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1677  #else
1678      float32x2_t a1 = vget_high_f32(vreinterpretq_f32_m128(a));
1679      float32x2_t b1 = vget_high_f32(vreinterpretq_f32_m128(b));
1680      float32x2x2_t result = vzip_f32(a1, b1);
1681      return vreinterpretq_m128_f32(vcombine_f32(result.val[0], result.val[1]));
1682  #endif
1683  }
1684  FORCE_INLINE __m128 _mm_unpacklo_ps(__m128 a, __m128 b)
1685  {
1686  #if defined(__aarch64__)
1687      return vreinterpretq_m128_f32(
1688          vzip1q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
1689  #else
1690      float32x2_t a1 = vget_low_f32(vreinterpretq_f32_m128(a));
1691      float32x2_t b1 = vget_low_f32(vreinterpretq_f32_m128(b));
1692      float32x2x2_t result = vzip_f32(a1, b1);
1693      return vreinterpretq_m128_f32(vcombine_f32(result.val[0], result.val[1]));
1694  #endif
1695  }
1696  FORCE_INLINE __m128 _mm_xor_ps(__m128 a, __m128 b)
1697  {
1698      return vreinterpretq_m128_s32(
1699          veorq_s32(vreinterpretq_s32_m128(a), vreinterpretq_s32_m128(b)));
1700  }
1701  FORCE_INLINE __m128i _mm_add_epi16(__m128i a, __m128i b)
1702  {
1703      return vreinterpretq_m128i_s16(
1704          vaddq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
1705  }
1706  FORCE_INLINE __m128i _mm_add_epi32(__m128i a, __m128i b)
1707  {
1708      return vreinterpretq_m128i_s32(
1709          vaddq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
1710  }
1711  FORCE_INLINE __m128i _mm_add_epi64(__m128i a, __m128i b)
1712  {
1713      return vreinterpretq_m128i_s64(
1714          vaddq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
1715  }
1716  FORCE_INLINE __m128i _mm_add_epi8(__m128i a, __m128i b)
1717  {
1718      return vreinterpretq_m128i_s8(
1719          vaddq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
1720  }
1721  FORCE_INLINE __m128d _mm_add_pd(__m128d a, __m128d b)
1722  {
1723  #if defined(__aarch64__)
1724      return vreinterpretq_m128d_f64(
1725          vaddq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
1726  #else
1727      double *da = (double *) &a;
1728      double *db = (double *) &b;
1729      double c[2];
1730      c[0] = da[0] + db[0];
1731      c[1] = da[1] + db[1];
1732      return vld1q_f32((float32_t *) c);
1733  #endif
1734  }
1735  FORCE_INLINE __m128d _mm_add_sd(__m128d a, __m128d b)
1736  {
1737  #if defined(__aarch64__)
1738      return _mm_move_sd(a, _mm_add_pd(a, b));
1739  #else
1740      double *da = (double *) &a;
1741      double *db = (double *) &b;
1742      double c[2];
1743      c[0] = da[0] + db[0];
1744      c[1] = da[1];
1745      return vld1q_f32((float32_t *) c);
1746  #endif
1747  }
1748  FORCE_INLINE __m64 _mm_add_si64(__m64 a, __m64 b)
1749  {
1750      return vreinterpret_m64_s64(
1751          vadd_s64(vreinterpret_s64_m64(a), vreinterpret_s64_m64(b)));
1752  }
1753  FORCE_INLINE __m128i _mm_adds_epi16(__m128i a, __m128i b)
1754  {
1755      return vreinterpretq_m128i_s16(
1756          vqaddq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
1757  }
1758  FORCE_INLINE __m128i _mm_adds_epi8(__m128i a, __m128i b)
1759  {
1760      return vreinterpretq_m128i_s8(
1761          vqaddq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
1762  }
1763  FORCE_INLINE __m128i _mm_adds_epu16(__m128i a, __m128i b)
1764  {
1765      return vreinterpretq_m128i_u16(
1766          vqaddq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
1767  }
1768  FORCE_INLINE __m128i _mm_adds_epu8(__m128i a, __m128i b)
1769  {
1770      return vreinterpretq_m128i_u8(
1771          vqaddq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
1772  }
1773  FORCE_INLINE __m128d _mm_and_pd(__m128d a, __m128d b)
1774  {
1775      return vreinterpretq_m128d_s64(
1776          vandq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));
1777  }
1778  FORCE_INLINE __m128i _mm_and_si128(__m128i a, __m128i b)
1779  {
1780      return vreinterpretq_m128i_s32(
1781          vandq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
1782  }
1783  FORCE_INLINE __m128d _mm_andnot_pd(__m128d a, __m128d b)
1784  {
1785      return vreinterpretq_m128d_s64(
1786          vbicq_s64(vreinterpretq_s64_m128d(b), vreinterpretq_s64_m128d(a)));
1787  }
1788  FORCE_INLINE __m128i _mm_andnot_si128(__m128i a, __m128i b)
1789  {
1790      return vreinterpretq_m128i_s32(
1791          vbicq_s32(vreinterpretq_s32_m128i(b),
1792                    vreinterpretq_s32_m128i(a)));  
1793  }
1794  FORCE_INLINE __m128i _mm_avg_epu16(__m128i a, __m128i b)
1795  {
1796      return (__m128i) vrhaddq_u16(vreinterpretq_u16_m128i(a),
1797                                   vreinterpretq_u16_m128i(b));
1798  }
1799  FORCE_INLINE __m128i _mm_avg_epu8(__m128i a, __m128i b)
1800  {
1801      return vreinterpretq_m128i_u8(
1802          vrhaddq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
1803  }
1804  #define _mm_bslli_si128(a, imm) _mm_slli_si128(a, imm)
1805  #define _mm_bsrli_si128(a, imm) _mm_srli_si128(a, imm)
1806  FORCE_INLINE __m128 _mm_castpd_ps(__m128d a)
1807  {
1808      return vreinterpretq_m128_s64(vreinterpretq_s64_m128d(a));
1809  }
1810  FORCE_INLINE __m128i _mm_castpd_si128(__m128d a)
1811  {
1812      return vreinterpretq_m128i_s64(vreinterpretq_s64_m128d(a));
1813  }
1814  FORCE_INLINE __m128d _mm_castps_pd(__m128 a)
1815  {
1816      return vreinterpretq_m128d_s32(vreinterpretq_s32_m128(a));
1817  }
1818  FORCE_INLINE __m128i _mm_castps_si128(__m128 a)
1819  {
1820      return vreinterpretq_m128i_s32(vreinterpretq_s32_m128(a));
1821  }
1822  FORCE_INLINE __m128d _mm_castsi128_pd(__m128i a)
1823  {
1824  #if defined(__aarch64__)
1825      return vreinterpretq_m128d_f64(vreinterpretq_f64_m128i(a));
1826  #else
1827      return vreinterpretq_m128d_f32(vreinterpretq_f32_m128i(a));
1828  #endif
1829  }
1830  FORCE_INLINE __m128 _mm_castsi128_ps(__m128i a)
1831  {
1832      return vreinterpretq_m128_s32(vreinterpretq_s32_m128i(a));
1833  }
1834  #if defined(__APPLE__)
1835  #include <libkern/OSCacheControl.h>
1836  #endif
1837  FORCE_INLINE void _mm_clflush(void const *p)
1838  {
1839      (void) p;
1840  #if defined(__APPLE__)
1841      sys_icache_invalidate((void *) (uintptr_t) p, SSE2NEON_CACHELINE_SIZE);
1842  #elif defined(__GNUC__) || defined(__clang__)
1843      uintptr_t ptr = (uintptr_t) p;
1844      __builtin___clear_cache((char *) ptr,
1845                              (char *) ptr + SSE2NEON_CACHELINE_SIZE);
1846  #else
1847  #endif
1848  }
1849  FORCE_INLINE __m128i _mm_cmpeq_epi16(__m128i a, __m128i b)
1850  {
1851      return vreinterpretq_m128i_u16(
1852          vceqq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
1853  }
1854  FORCE_INLINE __m128i _mm_cmpeq_epi32(__m128i a, __m128i b)
1855  {
1856      return vreinterpretq_m128i_u32(
1857          vceqq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
1858  }
1859  FORCE_INLINE __m128i _mm_cmpeq_epi8(__m128i a, __m128i b)
1860  {
1861      return vreinterpretq_m128i_u8(
1862          vceqq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
1863  }
1864  FORCE_INLINE __m128d _mm_cmpeq_pd(__m128d a, __m128d b)
1865  {
1866  #if defined(__aarch64__)
1867      return vreinterpretq_m128d_u64(
1868          vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
1869  #else
1870      uint32x4_t cmp =
1871          vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(b));
1872      uint32x4_t swapped = vrev64q_u32(cmp);
1873      return vreinterpretq_m128d_u32(vandq_u32(cmp, swapped));
1874  #endif
1875  }
1876  FORCE_INLINE __m128d _mm_cmpeq_sd(__m128d a, __m128d b)
1877  {
1878      return _mm_move_sd(a, _mm_cmpeq_pd(a, b));
1879  }
1880  FORCE_INLINE __m128d _mm_cmpge_pd(__m128d a, __m128d b)
1881  {
1882  #if defined(__aarch64__)
1883      return vreinterpretq_m128d_u64(
1884          vcgeq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
1885  #else
1886      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
1887      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
1888      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
1889      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
1890      uint64_t d[2];
1891      d[0] = (*(double *) &a0) >= (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
1892      d[1] = (*(double *) &a1) >= (*(double *) &b1) ? ~UINT64_C(0) : UINT64_C(0);
1893      return vreinterpretq_m128d_u64(vld1q_u64(d));
1894  #endif
1895  }
1896  FORCE_INLINE __m128d _mm_cmpge_sd(__m128d a, __m128d b)
1897  {
1898  #if defined(__aarch64__)
1899      return _mm_move_sd(a, _mm_cmpge_pd(a, b));
1900  #else
1901      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
1902      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
1903      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
1904      uint64_t d[2];
1905      d[0] = (*(double *) &a0) >= (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
1906      d[1] = a1;
1907      return vreinterpretq_m128d_u64(vld1q_u64(d));
1908  #endif
1909  }
1910  FORCE_INLINE __m128i _mm_cmpgt_epi16(__m128i a, __m128i b)
1911  {
1912      return vreinterpretq_m128i_u16(
1913          vcgtq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
1914  }
1915  FORCE_INLINE __m128i _mm_cmpgt_epi32(__m128i a, __m128i b)
1916  {
1917      return vreinterpretq_m128i_u32(
1918          vcgtq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
1919  }
1920  FORCE_INLINE __m128i _mm_cmpgt_epi8(__m128i a, __m128i b)
1921  {
1922      return vreinterpretq_m128i_u8(
1923          vcgtq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
1924  }
1925  FORCE_INLINE __m128d _mm_cmpgt_pd(__m128d a, __m128d b)
1926  {
1927  #if defined(__aarch64__)
1928      return vreinterpretq_m128d_u64(
1929          vcgtq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
1930  #else
1931      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
1932      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
1933      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
1934      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
1935      uint64_t d[2];
1936      d[0] = (*(double *) &a0) > (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
1937      d[1] = (*(double *) &a1) > (*(double *) &b1) ? ~UINT64_C(0) : UINT64_C(0);
1938      return vreinterpretq_m128d_u64(vld1q_u64(d));
1939  #endif
1940  }
1941  FORCE_INLINE __m128d _mm_cmpgt_sd(__m128d a, __m128d b)
1942  {
1943  #if defined(__aarch64__)
1944      return _mm_move_sd(a, _mm_cmpgt_pd(a, b));
1945  #else
1946      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
1947      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
1948      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
1949      uint64_t d[2];
1950      d[0] = (*(double *) &a0) > (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
1951      d[1] = a1;
1952      return vreinterpretq_m128d_u64(vld1q_u64(d));
1953  #endif
1954  }
1955  FORCE_INLINE __m128d _mm_cmple_pd(__m128d a, __m128d b)
1956  {
1957  #if defined(__aarch64__)
1958      return vreinterpretq_m128d_u64(
1959          vcleq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
1960  #else
1961      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
1962      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
1963      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
1964      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
1965      uint64_t d[2];
1966      d[0] = (*(double *) &a0) <= (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
1967      d[1] = (*(double *) &a1) <= (*(double *) &b1) ? ~UINT64_C(0) : UINT64_C(0);
1968      return vreinterpretq_m128d_u64(vld1q_u64(d));
1969  #endif
1970  }
1971  FORCE_INLINE __m128d _mm_cmple_sd(__m128d a, __m128d b)
1972  {
1973  #if defined(__aarch64__)
1974      return _mm_move_sd(a, _mm_cmple_pd(a, b));
1975  #else
1976      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
1977      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
1978      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
1979      uint64_t d[2];
1980      d[0] = (*(double *) &a0) <= (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
1981      d[1] = a1;
1982      return vreinterpretq_m128d_u64(vld1q_u64(d));
1983  #endif
1984  }
1985  FORCE_INLINE __m128i _mm_cmplt_epi16(__m128i a, __m128i b)
1986  {
1987      return vreinterpretq_m128i_u16(
1988          vcltq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
1989  }
1990  FORCE_INLINE __m128i _mm_cmplt_epi32(__m128i a, __m128i b)
1991  {
1992      return vreinterpretq_m128i_u32(
1993          vcltq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
1994  }
1995  FORCE_INLINE __m128i _mm_cmplt_epi8(__m128i a, __m128i b)
1996  {
1997      return vreinterpretq_m128i_u8(
1998          vcltq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
1999  }
2000  FORCE_INLINE __m128d _mm_cmplt_pd(__m128d a, __m128d b)
2001  {
2002  #if defined(__aarch64__)
2003      return vreinterpretq_m128d_u64(
2004          vcltq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
2005  #else
2006      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2007      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2008      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2009      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2010      uint64_t d[2];
2011      d[0] = (*(double *) &a0) < (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
2012      d[1] = (*(double *) &a1) < (*(double *) &b1) ? ~UINT64_C(0) : UINT64_C(0);
2013      return vreinterpretq_m128d_u64(vld1q_u64(d));
2014  #endif
2015  }
2016  FORCE_INLINE __m128d _mm_cmplt_sd(__m128d a, __m128d b)
2017  {
2018  #if defined(__aarch64__)
2019      return _mm_move_sd(a, _mm_cmplt_pd(a, b));
2020  #else
2021      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2022      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2023      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2024      uint64_t d[2];
2025      d[0] = (*(double *) &a0) < (*(double *) &b0) ? ~UINT64_C(0) : UINT64_C(0);
2026      d[1] = a1;
2027      return vreinterpretq_m128d_u64(vld1q_u64(d));
2028  #endif
2029  }
2030  FORCE_INLINE __m128d _mm_cmpneq_pd(__m128d a, __m128d b)
2031  {
2032  #if defined(__aarch64__)
2033      return vreinterpretq_m128d_s32(vmvnq_s32(vreinterpretq_s32_u64(
2034          vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)))));
2035  #else
2036      uint32x4_t cmp =
2037          vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(b));
2038      uint32x4_t swapped = vrev64q_u32(cmp);
2039      return vreinterpretq_m128d_u32(vmvnq_u32(vandq_u32(cmp, swapped)));
2040  #endif
2041  }
2042  FORCE_INLINE __m128d _mm_cmpneq_sd(__m128d a, __m128d b)
2043  {
2044      return _mm_move_sd(a, _mm_cmpneq_pd(a, b));
2045  }
2046  FORCE_INLINE __m128d _mm_cmpnge_pd(__m128d a, __m128d b)
2047  {
2048  #if defined(__aarch64__)
2049      return vreinterpretq_m128d_u64(veorq_u64(
2050          vcgeq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
2051          vdupq_n_u64(UINT64_MAX)));
2052  #else
2053      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2054      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2055      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2056      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2057      uint64_t d[2];
2058      d[0] =
2059          !((*(double *) &a0) >= (*(double *) &b0)) ? ~UINT64_C(0) : UINT64_C(0);
2060      d[1] =
2061          !((*(double *) &a1) >= (*(double *) &b1)) ? ~UINT64_C(0) : UINT64_C(0);
2062      return vreinterpretq_m128d_u64(vld1q_u64(d));
2063  #endif
2064  }
2065  FORCE_INLINE __m128d _mm_cmpnge_sd(__m128d a, __m128d b)
2066  {
2067      return _mm_move_sd(a, _mm_cmpnge_pd(a, b));
2068  }
2069  FORCE_INLINE __m128d _mm_cmpngt_pd(__m128d a, __m128d b)
2070  {
2071  #if defined(__aarch64__)
2072      return vreinterpretq_m128d_u64(veorq_u64(
2073          vcgtq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
2074          vdupq_n_u64(UINT64_MAX)));
2075  #else
2076      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2077      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2078      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2079      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2080      uint64_t d[2];
2081      d[0] =
2082          !((*(double *) &a0) > (*(double *) &b0)) ? ~UINT64_C(0) : UINT64_C(0);
2083      d[1] =
2084          !((*(double *) &a1) > (*(double *) &b1)) ? ~UINT64_C(0) : UINT64_C(0);
2085      return vreinterpretq_m128d_u64(vld1q_u64(d));
2086  #endif
2087  }
2088  FORCE_INLINE __m128d _mm_cmpngt_sd(__m128d a, __m128d b)
2089  {
2090      return _mm_move_sd(a, _mm_cmpngt_pd(a, b));
2091  }
2092  FORCE_INLINE __m128d _mm_cmpnle_pd(__m128d a, __m128d b)
2093  {
2094  #if defined(__aarch64__)
2095      return vreinterpretq_m128d_u64(veorq_u64(
2096          vcleq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
2097          vdupq_n_u64(UINT64_MAX)));
2098  #else
2099      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2100      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2101      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2102      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2103      uint64_t d[2];
2104      d[0] =
2105          !((*(double *) &a0) <= (*(double *) &b0)) ? ~UINT64_C(0) : UINT64_C(0);
2106      d[1] =
2107          !((*(double *) &a1) <= (*(double *) &b1)) ? ~UINT64_C(0) : UINT64_C(0);
2108      return vreinterpretq_m128d_u64(vld1q_u64(d));
2109  #endif
2110  }
2111  FORCE_INLINE __m128d _mm_cmpnle_sd(__m128d a, __m128d b)
2112  {
2113      return _mm_move_sd(a, _mm_cmpnle_pd(a, b));
2114  }
2115  FORCE_INLINE __m128d _mm_cmpnlt_pd(__m128d a, __m128d b)
2116  {
2117  #if defined(__aarch64__)
2118      return vreinterpretq_m128d_u64(veorq_u64(
2119          vcltq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)),
2120          vdupq_n_u64(UINT64_MAX)));
2121  #else
2122      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2123      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2124      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2125      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2126      uint64_t d[2];
2127      d[0] =
2128          !((*(double *) &a0) < (*(double *) &b0)) ? ~UINT64_C(0) : UINT64_C(0);
2129      d[1] =
2130          !((*(double *) &a1) < (*(double *) &b1)) ? ~UINT64_C(0) : UINT64_C(0);
2131      return vreinterpretq_m128d_u64(vld1q_u64(d));
2132  #endif
2133  }
2134  FORCE_INLINE __m128d _mm_cmpnlt_sd(__m128d a, __m128d b)
2135  {
2136      return _mm_move_sd(a, _mm_cmpnlt_pd(a, b));
2137  }
2138  FORCE_INLINE __m128d _mm_cmpord_pd(__m128d a, __m128d b)
2139  {
2140  #if defined(__aarch64__)
2141      uint64x2_t not_nan_a =
2142          vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(a));
2143      uint64x2_t not_nan_b =
2144          vceqq_f64(vreinterpretq_f64_m128d(b), vreinterpretq_f64_m128d(b));
2145      return vreinterpretq_m128d_u64(vandq_u64(not_nan_a, not_nan_b));
2146  #else
2147      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2148      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2149      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2150      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2151      uint64_t d[2];
2152      d[0] = ((*(double *) &a0) == (*(double *) &a0) &&
2153              (*(double *) &b0) == (*(double *) &b0))
2154                 ? ~UINT64_C(0)
2155                 : UINT64_C(0);
2156      d[1] = ((*(double *) &a1) == (*(double *) &a1) &&
2157              (*(double *) &b1) == (*(double *) &b1))
2158                 ? ~UINT64_C(0)
2159                 : UINT64_C(0);
2160      return vreinterpretq_m128d_u64(vld1q_u64(d));
2161  #endif
2162  }
2163  FORCE_INLINE __m128d _mm_cmpord_sd(__m128d a, __m128d b)
2164  {
2165  #if defined(__aarch64__)
2166      return _mm_move_sd(a, _mm_cmpord_pd(a, b));
2167  #else
2168      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2169      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2170      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2171      uint64_t d[2];
2172      d[0] = ((*(double *) &a0) == (*(double *) &a0) &&
2173              (*(double *) &b0) == (*(double *) &b0))
2174                 ? ~UINT64_C(0)
2175                 : UINT64_C(0);
2176      d[1] = a1;
2177      return vreinterpretq_m128d_u64(vld1q_u64(d));
2178  #endif
2179  }
2180  FORCE_INLINE __m128d _mm_cmpunord_pd(__m128d a, __m128d b)
2181  {
2182  #if defined(__aarch64__)
2183      uint64x2_t not_nan_a =
2184          vceqq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(a));
2185      uint64x2_t not_nan_b =
2186          vceqq_f64(vreinterpretq_f64_m128d(b), vreinterpretq_f64_m128d(b));
2187      return vreinterpretq_m128d_s32(
2188          vmvnq_s32(vreinterpretq_s32_u64(vandq_u64(not_nan_a, not_nan_b))));
2189  #else
2190      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2191      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2192      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2193      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2194      uint64_t d[2];
2195      d[0] = ((*(double *) &a0) == (*(double *) &a0) &&
2196              (*(double *) &b0) == (*(double *) &b0))
2197                 ? UINT64_C(0)
2198                 : ~UINT64_C(0);
2199      d[1] = ((*(double *) &a1) == (*(double *) &a1) &&
2200              (*(double *) &b1) == (*(double *) &b1))
2201                 ? UINT64_C(0)
2202                 : ~UINT64_C(0);
2203      return vreinterpretq_m128d_u64(vld1q_u64(d));
2204  #endif
2205  }
2206  FORCE_INLINE __m128d _mm_cmpunord_sd(__m128d a, __m128d b)
2207  {
2208  #if defined(__aarch64__)
2209      return _mm_move_sd(a, _mm_cmpunord_pd(a, b));
2210  #else
2211      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2212      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2213      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2214      uint64_t d[2];
2215      d[0] = ((*(double *) &a0) == (*(double *) &a0) &&
2216              (*(double *) &b0) == (*(double *) &b0))
2217                 ? UINT64_C(0)
2218                 : ~UINT64_C(0);
2219      d[1] = a1;
2220      return vreinterpretq_m128d_u64(vld1q_u64(d));
2221  #endif
2222  }
2223  FORCE_INLINE int _mm_comige_sd(__m128d a, __m128d b)
2224  {
2225  #if defined(__aarch64__)
2226      return vgetq_lane_u64(vcgeq_f64(a, b), 0) & 0x1;
2227  #else
2228      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2229      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2230      return (*(double *) &a0 >= *(double *) &b0);
2231  #endif
2232  }
2233  FORCE_INLINE int _mm_comigt_sd(__m128d a, __m128d b)
2234  {
2235  #if defined(__aarch64__)
2236      return vgetq_lane_u64(vcgtq_f64(a, b), 0) & 0x1;
2237  #else
2238      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2239      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2240      return (*(double *) &a0 > *(double *) &b0);
2241  #endif
2242  }
2243  FORCE_INLINE int _mm_comile_sd(__m128d a, __m128d b)
2244  {
2245  #if defined(__aarch64__)
2246      return vgetq_lane_u64(vcleq_f64(a, b), 0) & 0x1;
2247  #else
2248      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2249      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2250      return (*(double *) &a0 <= *(double *) &b0);
2251  #endif
2252  }
2253  FORCE_INLINE int _mm_comilt_sd(__m128d a, __m128d b)
2254  {
2255  #if defined(__aarch64__)
2256      return vgetq_lane_u64(vcltq_f64(a, b), 0) & 0x1;
2257  #else
2258      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2259      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2260      return (*(double *) &a0 < *(double *) &b0);
2261  #endif
2262  }
2263  FORCE_INLINE int _mm_comieq_sd(__m128d a, __m128d b)
2264  {
2265  #if defined(__aarch64__)
2266      return vgetq_lane_u64(vceqq_f64(a, b), 0) & 0x1;
2267  #else
2268      uint32x4_t a_not_nan =
2269          vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(a));
2270      uint32x4_t b_not_nan =
2271          vceqq_u32(vreinterpretq_u32_m128d(b), vreinterpretq_u32_m128d(b));
2272      uint32x4_t a_and_b_not_nan = vandq_u32(a_not_nan, b_not_nan);
2273      uint32x4_t a_eq_b =
2274          vceqq_u32(vreinterpretq_u32_m128d(a), vreinterpretq_u32_m128d(b));
2275      uint64x2_t and_results = vandq_u64(vreinterpretq_u64_u32(a_and_b_not_nan),
2276                                         vreinterpretq_u64_u32(a_eq_b));
2277      return vgetq_lane_u64(and_results, 0) & 0x1;
2278  #endif
2279  }
2280  FORCE_INLINE int _mm_comineq_sd(__m128d a, __m128d b)
2281  {
2282      return !_mm_comieq_sd(a, b);
2283  }
2284  FORCE_INLINE __m128d _mm_cvtepi32_pd(__m128i a)
2285  {
2286  #if defined(__aarch64__)
2287      return vreinterpretq_m128d_f64(
2288          vcvtq_f64_s64(vmovl_s32(vget_low_s32(vreinterpretq_s32_m128i(a)))));
2289  #else
2290      double a0 = (double) vgetq_lane_s32(vreinterpretq_s32_m128i(a), 0);
2291      double a1 = (double) vgetq_lane_s32(vreinterpretq_s32_m128i(a), 1);
2292      return _mm_set_pd(a1, a0);
2293  #endif
2294  }
2295  FORCE_INLINE __m128 _mm_cvtepi32_ps(__m128i a)
2296  {
2297      return vreinterpretq_m128_f32(vcvtq_f32_s32(vreinterpretq_s32_m128i(a)));
2298  }
2299  FORCE_INLINE __m128i _mm_cvtpd_epi32(__m128d a)
2300  {
2301  #if defined(__ARM_FEATURE_FRINT) && !defined(__clang__)
2302      float64x2_t rounded = vrnd32xq_f64(vreinterpretq_f64_m128d(a));
2303      int64x2_t integers = vcvtq_s64_f64(rounded);
2304      return vreinterpretq_m128i_s32(
2305          vcombine_s32(vmovn_s64(integers), vdup_n_s32(0)));
2306  #else
2307      __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
2308      double d0 = ((double *) &rnd)[0];
2309      double d1 = ((double *) &rnd)[1];
2310      return _mm_set_epi32(0, 0, (int32_t) d1, (int32_t) d0);
2311  #endif
2312  }
2313  FORCE_INLINE __m64 _mm_cvtpd_pi32(__m128d a)
2314  {
2315      __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
2316      double d0 = ((double *) &rnd)[0];
2317      double d1 = ((double *) &rnd)[1];
2318      int32_t ALIGN_STRUCT(16) data[2] = {(int32_t) d0, (int32_t) d1};
2319      return vreinterpret_m64_s32(vld1_s32(data));
2320  }
2321  FORCE_INLINE __m128 _mm_cvtpd_ps(__m128d a)
2322  {
2323  #if defined(__aarch64__)
2324      float32x2_t tmp = vcvt_f32_f64(vreinterpretq_f64_m128d(a));
2325      return vreinterpretq_m128_f32(vcombine_f32(tmp, vdup_n_f32(0)));
2326  #else
2327      float a0 = (float) ((double *) &a)[0];
2328      float a1 = (float) ((double *) &a)[1];
2329      return _mm_set_ps(0, 0, a1, a0);
2330  #endif
2331  }
2332  FORCE_INLINE __m128d _mm_cvtpi32_pd(__m64 a)
2333  {
2334  #if defined(__aarch64__)
2335      return vreinterpretq_m128d_f64(
2336          vcvtq_f64_s64(vmovl_s32(vreinterpret_s32_m64(a))));
2337  #else
2338      double a0 = (double) vget_lane_s32(vreinterpret_s32_m64(a), 0);
2339      double a1 = (double) vget_lane_s32(vreinterpret_s32_m64(a), 1);
2340      return _mm_set_pd(a1, a0);
2341  #endif
2342  }
2343  FORCE_INLINE __m128i _mm_cvtps_epi32(__m128 a)
2344  {
2345  #if defined(__ARM_FEATURE_FRINT)
2346      return vreinterpretq_m128i_s32(vcvtq_s32_f32(vrnd32xq_f32(a)));
2347  #elif defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
2348      switch (_MM_GET_ROUNDING_MODE()) {
2349      case _MM_ROUND_NEAREST:
2350          return vreinterpretq_m128i_s32(vcvtnq_s32_f32(a));
2351      case _MM_ROUND_DOWN:
2352          return vreinterpretq_m128i_s32(vcvtmq_s32_f32(a));
2353      case _MM_ROUND_UP:
2354          return vreinterpretq_m128i_s32(vcvtpq_s32_f32(a));
2355      default:  
2356          return vreinterpretq_m128i_s32(vcvtq_s32_f32(a));
2357      }
2358  #else
2359      float *f = (float *) &a;
2360      switch (_MM_GET_ROUNDING_MODE()) {
2361      case _MM_ROUND_NEAREST: {
2362          uint32x4_t signmask = vdupq_n_u32(0x80000000);
2363          float32x4_t half = vbslq_f32(signmask, vreinterpretq_f32_m128(a),
2364                                       vdupq_n_f32(0.5f)); &bsol;* +/- 0.5 */
2365          int32x4_t r_normal = vcvtq_s32_f32(vaddq_f32(
2366              vreinterpretq_f32_m128(a), half)); &bsol;* round to integer: [a + 0.5]*/
2367          int32x4_t r_trunc = vcvtq_s32_f32(
2368              vreinterpretq_f32_m128(a)); &bsol;* truncate to integer: [a] */
2369          int32x4_t plusone = vreinterpretq_s32_u32(vshrq_n_u32(
2370              vreinterpretq_u32_s32(vnegq_s32(r_trunc)), 31)); &bsol;* 1 or 0 */
2371          int32x4_t r_even = vbicq_s32(vaddq_s32(r_trunc, plusone),
2372                                       vdupq_n_s32(1)); &bsol;* ([a] + {0,1}) & ~1 */
2373          float32x4_t delta = vsubq_f32(
2374              vreinterpretq_f32_m128(a),
2375              vcvtq_f32_s32(r_trunc)); &bsol;* compute delta: delta = (a - [a]) */
2376          uint32x4_t is_delta_half =
2377              vceqq_f32(delta, half); &bsol;* delta == +/- 0.5 */
2378          return vreinterpretq_m128i_s32(
2379              vbslq_s32(is_delta_half, r_even, r_normal));
2380      }
2381      case _MM_ROUND_DOWN:
2382          return _mm_set_epi32(floorf(f[3]), floorf(f[2]), floorf(f[1]),
2383                               floorf(f[0]));
2384      case _MM_ROUND_UP:
2385          return _mm_set_epi32(ceilf(f[3]), ceilf(f[2]), ceilf(f[1]),
2386                               ceilf(f[0]));
2387      default:  
2388          return _mm_set_epi32((int32_t) f[3], (int32_t) f[2], (int32_t) f[1],
2389                               (int32_t) f[0]);
2390      }
2391  #endif
2392  }
2393  FORCE_INLINE __m128d _mm_cvtps_pd(__m128 a)
2394  {
2395  #if defined(__aarch64__)
2396      return vreinterpretq_m128d_f64(
2397          vcvt_f64_f32(vget_low_f32(vreinterpretq_f32_m128(a))));
2398  #else
2399      double a0 = (double) vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
2400      double a1 = (double) vgetq_lane_f32(vreinterpretq_f32_m128(a), 1);
2401      return _mm_set_pd(a1, a0);
2402  #endif
2403  }
2404  FORCE_INLINE double _mm_cvtsd_f64(__m128d a)
2405  {
2406  #if defined(__aarch64__)
2407      return (double) vgetq_lane_f64(vreinterpretq_f64_m128d(a), 0);
2408  #else
2409      return ((double *) &a)[0];
2410  #endif
2411  }
2412  FORCE_INLINE int32_t _mm_cvtsd_si32(__m128d a)
2413  {
2414  #if defined(__aarch64__)
2415      return (int32_t) vgetq_lane_f64(vrndiq_f64(vreinterpretq_f64_m128d(a)), 0);
2416  #else
2417      __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
2418      double ret = ((double *) &rnd)[0];
2419      return (int32_t) ret;
2420  #endif
2421  }
2422  FORCE_INLINE int64_t _mm_cvtsd_si64(__m128d a)
2423  {
2424  #if defined(__aarch64__)
2425      return (int64_t) vgetq_lane_f64(vrndiq_f64(vreinterpretq_f64_m128d(a)), 0);
2426  #else
2427      __m128d rnd = _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
2428      double ret = ((double *) &rnd)[0];
2429      return (int64_t) ret;
2430  #endif
2431  }
2432  #define _mm_cvtsd_si64x _mm_cvtsd_si64
2433  FORCE_INLINE __m128 _mm_cvtsd_ss(__m128 a, __m128d b)
2434  {
2435  #if defined(__aarch64__)
2436      return vreinterpretq_m128_f32(vsetq_lane_f32(
2437          vget_lane_f32(vcvt_f32_f64(vreinterpretq_f64_m128d(b)), 0),
2438          vreinterpretq_f32_m128(a), 0));
2439  #else
2440      return vreinterpretq_m128_f32(vsetq_lane_f32((float) ((double *) &b)[0],
2441                                                   vreinterpretq_f32_m128(a), 0));
2442  #endif
2443  }
2444  FORCE_INLINE int _mm_cvtsi128_si32(__m128i a)
2445  {
2446      return vgetq_lane_s32(vreinterpretq_s32_m128i(a), 0);
2447  }
2448  FORCE_INLINE int64_t _mm_cvtsi128_si64(__m128i a)
2449  {
2450      return vgetq_lane_s64(vreinterpretq_s64_m128i(a), 0);
2451  }
2452  #define _mm_cvtsi128_si64x(a) _mm_cvtsi128_si64(a)
2453  FORCE_INLINE __m128d _mm_cvtsi32_sd(__m128d a, int32_t b)
2454  {
2455  #if defined(__aarch64__)
2456      return vreinterpretq_m128d_f64(
2457          vsetq_lane_f64((double) b, vreinterpretq_f64_m128d(a), 0));
2458  #else
2459      double bf = (double) b;
2460      return vreinterpretq_m128d_s64(
2461          vsetq_lane_s64(*(int64_t *) &bf, vreinterpretq_s64_m128d(a), 0));
2462  #endif
2463  }
2464  #define _mm_cvtsi128_si64x(a) _mm_cvtsi128_si64(a)
2465  FORCE_INLINE __m128i _mm_cvtsi32_si128(int a)
2466  {
2467      return vreinterpretq_m128i_s32(vsetq_lane_s32(a, vdupq_n_s32(0), 0));
2468  }
2469  FORCE_INLINE __m128d _mm_cvtsi64_sd(__m128d a, int64_t b)
2470  {
2471  #if defined(__aarch64__)
2472      return vreinterpretq_m128d_f64(
2473          vsetq_lane_f64((double) b, vreinterpretq_f64_m128d(a), 0));
2474  #else
2475      double bf = (double) b;
2476      return vreinterpretq_m128d_s64(
2477          vsetq_lane_s64(*(int64_t *) &bf, vreinterpretq_s64_m128d(a), 0));
2478  #endif
2479  }
2480  FORCE_INLINE __m128i _mm_cvtsi64_si128(int64_t a)
2481  {
2482      return vreinterpretq_m128i_s64(vsetq_lane_s64(a, vdupq_n_s64(0), 0));
2483  }
2484  #define _mm_cvtsi64x_si128(a) _mm_cvtsi64_si128(a)
2485  #define _mm_cvtsi64x_sd(a, b) _mm_cvtsi64_sd(a, b)
2486  FORCE_INLINE __m128d _mm_cvtss_sd(__m128d a, __m128 b)
2487  {
2488      double d = (double) vgetq_lane_f32(vreinterpretq_f32_m128(b), 0);
2489  #if defined(__aarch64__)
2490      return vreinterpretq_m128d_f64(
2491          vsetq_lane_f64(d, vreinterpretq_f64_m128d(a), 0));
2492  #else
2493      return vreinterpretq_m128d_s64(
2494          vsetq_lane_s64(*(int64_t *) &d, vreinterpretq_s64_m128d(a), 0));
2495  #endif
2496  }
2497  FORCE_INLINE __m128i _mm_cvttpd_epi32(__m128d a)
2498  {
2499      double a0 = ((double *) &a)[0];
2500      double a1 = ((double *) &a)[1];
2501      return _mm_set_epi32(0, 0, (int32_t) a1, (int32_t) a0);
2502  }
2503  FORCE_INLINE __m64 _mm_cvttpd_pi32(__m128d a)
2504  {
2505      double a0 = ((double *) &a)[0];
2506      double a1 = ((double *) &a)[1];
2507      int32_t ALIGN_STRUCT(16) data[2] = {(int32_t) a0, (int32_t) a1};
2508      return vreinterpret_m64_s32(vld1_s32(data));
2509  }
2510  FORCE_INLINE __m128i _mm_cvttps_epi32(__m128 a)
2511  {
2512      return vreinterpretq_m128i_s32(vcvtq_s32_f32(vreinterpretq_f32_m128(a)));
2513  }
2514  FORCE_INLINE int32_t _mm_cvttsd_si32(__m128d a)
2515  {
2516      double ret = *((double *) &a);
2517      return (int32_t) ret;
2518  }
2519  FORCE_INLINE int64_t _mm_cvttsd_si64(__m128d a)
2520  {
2521  #if defined(__aarch64__)
2522      return vgetq_lane_s64(vcvtq_s64_f64(vreinterpretq_f64_m128d(a)), 0);
2523  #else
2524      double ret = *((double *) &a);
2525      return (int64_t) ret;
2526  #endif
2527  }
2528  #define _mm_cvttsd_si64x(a) _mm_cvttsd_si64(a)
2529  FORCE_INLINE __m128d _mm_div_pd(__m128d a, __m128d b)
2530  {
2531  #if defined(__aarch64__)
2532      return vreinterpretq_m128d_f64(
2533          vdivq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
2534  #else
2535      double *da = (double *) &a;
2536      double *db = (double *) &b;
2537      double c[2];
2538      c[0] = da[0] / db[0];
2539      c[1] = da[1] / db[1];
2540      return vld1q_f32((float32_t *) c);
2541  #endif
2542  }
2543  FORCE_INLINE __m128d _mm_div_sd(__m128d a, __m128d b)
2544  {
2545  #if defined(__aarch64__)
2546      float64x2_t tmp =
2547          vdivq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b));
2548      return vreinterpretq_m128d_f64(
2549          vsetq_lane_f64(vgetq_lane_f64(vreinterpretq_f64_m128d(a), 1), tmp, 1));
2550  #else
2551      return _mm_move_sd(a, _mm_div_pd(a, b));
2552  #endif
2553  }
2554  #define _mm_extract_epi16(a, imm) \
2555      vgetq_lane_u16(vreinterpretq_u16_m128i(a), (imm))
2556  #define _mm_insert_epi16(a, b, imm)                                  \
2557      __extension__({                                                  \
2558          vreinterpretq_m128i_s16(                                     \
2559              vsetq_lane_s16((b), vreinterpretq_s16_m128i(a), (imm))); \
2560      })
2561  FORCE_INLINE __m128d _mm_load_pd(const double *p)
2562  {
2563  #if defined(__aarch64__)
2564      return vreinterpretq_m128d_f64(vld1q_f64(p));
2565  #else
2566      const float *fp = (const float *) p;
2567      float ALIGN_STRUCT(16) data[4] = {fp[0], fp[1], fp[2], fp[3]};
2568      return vreinterpretq_m128d_f32(vld1q_f32(data));
2569  #endif
2570  }
2571  #define _mm_load_pd1 _mm_load1_pd
2572  FORCE_INLINE __m128d _mm_load_sd(const double *p)
2573  {
2574  #if defined(__aarch64__)
2575      return vreinterpretq_m128d_f64(vsetq_lane_f64(*p, vdupq_n_f64(0), 0));
2576  #else
2577      const float *fp = (const float *) p;
2578      float ALIGN_STRUCT(16) data[4] = {fp[0], fp[1], 0, 0};
2579      return vreinterpretq_m128d_f32(vld1q_f32(data));
2580  #endif
2581  }
2582  FORCE_INLINE __m128i _mm_load_si128(const __m128i *p)
2583  {
2584      return vreinterpretq_m128i_s32(vld1q_s32((const int32_t *) p));
2585  }
2586  FORCE_INLINE __m128d _mm_load1_pd(const double *p)
2587  {
2588  #if defined(__aarch64__)
2589      return vreinterpretq_m128d_f64(vld1q_dup_f64(p));
2590  #else
2591      return vreinterpretq_m128d_s64(vdupq_n_s64(*(const int64_t *) p));
2592  #endif
2593  }
2594  FORCE_INLINE __m128d _mm_loadh_pd(__m128d a, const double *p)
2595  {
2596  #if defined(__aarch64__)
2597      return vreinterpretq_m128d_f64(
2598          vcombine_f64(vget_low_f64(vreinterpretq_f64_m128d(a)), vld1_f64(p)));
2599  #else
2600      return vreinterpretq_m128d_f32(vcombine_f32(
2601          vget_low_f32(vreinterpretq_f32_m128d(a)), vld1_f32((const float *) p)));
2602  #endif
2603  }
2604  FORCE_INLINE __m128i _mm_loadl_epi64(__m128i const *p)
2605  {
2606      return vreinterpretq_m128i_s32(
2607          vcombine_s32(vld1_s32((int32_t const *) p), vcreate_s32(0)));
2608  }
2609  FORCE_INLINE __m128d _mm_loadl_pd(__m128d a, const double *p)
2610  {
2611  #if defined(__aarch64__)
2612      return vreinterpretq_m128d_f64(
2613          vcombine_f64(vld1_f64(p), vget_high_f64(vreinterpretq_f64_m128d(a))));
2614  #else
2615      return vreinterpretq_m128d_f32(
2616          vcombine_f32(vld1_f32((const float *) p),
2617                       vget_high_f32(vreinterpretq_f32_m128d(a))));
2618  #endif
2619  }
2620  FORCE_INLINE __m128d _mm_loadr_pd(const double *p)
2621  {
2622  #if defined(__aarch64__)
2623      float64x2_t v = vld1q_f64(p);
2624      return vreinterpretq_m128d_f64(vextq_f64(v, v, 1));
2625  #else
2626      int64x2_t v = vld1q_s64((const int64_t *) p);
2627      return vreinterpretq_m128d_s64(vextq_s64(v, v, 1));
2628  #endif
2629  }
2630  FORCE_INLINE __m128d _mm_loadu_pd(const double *p)
2631  {
2632      return _mm_load_pd(p);
2633  }
2634  FORCE_INLINE __m128i _mm_loadu_si128(const __m128i *p)
2635  {
2636      return vreinterpretq_m128i_s32(vld1q_s32((const int32_t *) p));
2637  }
2638  FORCE_INLINE __m128i _mm_loadu_si32(const void *p)
2639  {
2640      return vreinterpretq_m128i_s32(
2641          vsetq_lane_s32(*(const int32_t *) p, vdupq_n_s32(0), 0));
2642  }
2643  FORCE_INLINE __m128i _mm_madd_epi16(__m128i a, __m128i b)
2644  {
2645      int32x4_t low = vmull_s16(vget_low_s16(vreinterpretq_s16_m128i(a)),
2646                                vget_low_s16(vreinterpretq_s16_m128i(b)));
2647  #if defined(__aarch64__)
2648      int32x4_t high =
2649          vmull_high_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b));
2650      return vreinterpretq_m128i_s32(vpaddq_s32(low, high));
2651  #else
2652      int32x4_t high = vmull_s16(vget_high_s16(vreinterpretq_s16_m128i(a)),
2653                                 vget_high_s16(vreinterpretq_s16_m128i(b)));
2654      int32x2_t low_sum = vpadd_s32(vget_low_s32(low), vget_high_s32(low));
2655      int32x2_t high_sum = vpadd_s32(vget_low_s32(high), vget_high_s32(high));
2656      return vreinterpretq_m128i_s32(vcombine_s32(low_sum, high_sum));
2657  #endif
2658  }
2659  FORCE_INLINE void _mm_maskmoveu_si128(__m128i a, __m128i mask, char *mem_addr)
2660  {
2661      int8x16_t shr_mask = vshrq_n_s8(vreinterpretq_s8_m128i(mask), 7);
2662      __m128 b = _mm_load_ps((const float *) mem_addr);
2663      int8x16_t masked =
2664          vbslq_s8(vreinterpretq_u8_s8(shr_mask), vreinterpretq_s8_m128i(a),
2665                   vreinterpretq_s8_m128(b));
2666      vst1q_s8((int8_t *) mem_addr, masked);
2667  }
2668  FORCE_INLINE __m128i _mm_max_epi16(__m128i a, __m128i b)
2669  {
2670      return vreinterpretq_m128i_s16(
2671          vmaxq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
2672  }
2673  FORCE_INLINE __m128i _mm_max_epu8(__m128i a, __m128i b)
2674  {
2675      return vreinterpretq_m128i_u8(
2676          vmaxq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
2677  }
2678  FORCE_INLINE __m128d _mm_max_pd(__m128d a, __m128d b)
2679  {
2680  #if defined(__aarch64__)
2681  #if SSE2NEON_PRECISE_MINMAX
2682      float64x2_t _a = vreinterpretq_f64_m128d(a);
2683      float64x2_t _b = vreinterpretq_f64_m128d(b);
2684      return vreinterpretq_m128d_f64(vbslq_f64(vcgtq_f64(_a, _b), _a, _b));
2685  #else
2686      return vreinterpretq_m128d_f64(
2687          vmaxq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
2688  #endif
2689  #else
2690      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2691      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2692      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2693      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2694      uint64_t d[2];
2695      d[0] = (*(double *) &a0) > (*(double *) &b0) ? a0 : b0;
2696      d[1] = (*(double *) &a1) > (*(double *) &b1) ? a1 : b1;
2697      return vreinterpretq_m128d_u64(vld1q_u64(d));
2698  #endif
2699  }
2700  FORCE_INLINE __m128d _mm_max_sd(__m128d a, __m128d b)
2701  {
2702  #if defined(__aarch64__)
2703      return _mm_move_sd(a, _mm_max_pd(a, b));
2704  #else
2705      double *da = (double *) &a;
2706      double *db = (double *) &b;
2707      double c[2] = {da[0] > db[0] ? da[0] : db[0], da[1]};
2708      return vreinterpretq_m128d_f32(vld1q_f32((float32_t *) c));
2709  #endif
2710  }
2711  FORCE_INLINE __m128i _mm_min_epi16(__m128i a, __m128i b)
2712  {
2713      return vreinterpretq_m128i_s16(
2714          vminq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
2715  }
2716  FORCE_INLINE __m128i _mm_min_epu8(__m128i a, __m128i b)
2717  {
2718      return vreinterpretq_m128i_u8(
2719          vminq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
2720  }
2721  FORCE_INLINE __m128d _mm_min_pd(__m128d a, __m128d b)
2722  {
2723  #if defined(__aarch64__)
2724  #if SSE2NEON_PRECISE_MINMAX
2725      float64x2_t _a = vreinterpretq_f64_m128d(a);
2726      float64x2_t _b = vreinterpretq_f64_m128d(b);
2727      return vreinterpretq_m128d_f64(vbslq_f64(vcltq_f64(_a, _b), _a, _b));
2728  #else
2729      return vreinterpretq_m128d_f64(
2730          vminq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
2731  #endif
2732  #else
2733      uint64_t a0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(a));
2734      uint64_t a1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(a));
2735      uint64_t b0 = (uint64_t) vget_low_u64(vreinterpretq_u64_m128d(b));
2736      uint64_t b1 = (uint64_t) vget_high_u64(vreinterpretq_u64_m128d(b));
2737      uint64_t d[2];
2738      d[0] = (*(double *) &a0) < (*(double *) &b0) ? a0 : b0;
2739      d[1] = (*(double *) &a1) < (*(double *) &b1) ? a1 : b1;
2740      return vreinterpretq_m128d_u64(vld1q_u64(d));
2741  #endif
2742  }
2743  FORCE_INLINE __m128d _mm_min_sd(__m128d a, __m128d b)
2744  {
2745  #if defined(__aarch64__)
2746      return _mm_move_sd(a, _mm_min_pd(a, b));
2747  #else
2748      double *da = (double *) &a;
2749      double *db = (double *) &b;
2750      double c[2] = {da[0] < db[0] ? da[0] : db[0], da[1]};
2751      return vreinterpretq_m128d_f32(vld1q_f32((float32_t *) c));
2752  #endif
2753  }
2754  FORCE_INLINE __m128i _mm_move_epi64(__m128i a)
2755  {
2756      return vreinterpretq_m128i_s64(
2757          vsetq_lane_s64(0, vreinterpretq_s64_m128i(a), 1));
2758  }
2759  FORCE_INLINE __m128d _mm_move_sd(__m128d a, __m128d b)
2760  {
2761      return vreinterpretq_m128d_f32(
2762          vcombine_f32(vget_low_f32(vreinterpretq_f32_m128d(b)),
2763                       vget_high_f32(vreinterpretq_f32_m128d(a))));
2764  }
2765  FORCE_INLINE int _mm_movemask_epi8(__m128i a)
2766  {
2767      uint8x16_t input = vreinterpretq_u8_m128i(a);
2768      uint16x8_t high_bits = vreinterpretq_u16_u8(vshrq_n_u8(input, 7));
2769      uint32x4_t paired16 =
2770          vreinterpretq_u32_u16(vsraq_n_u16(high_bits, high_bits, 7));
2771      uint64x2_t paired32 =
2772          vreinterpretq_u64_u32(vsraq_n_u32(paired16, paired16, 14));
2773      uint8x16_t paired64 =
2774          vreinterpretq_u8_u64(vsraq_n_u64(paired32, paired32, 28));
2775      return vgetq_lane_u8(paired64, 0) | ((int) vgetq_lane_u8(paired64, 8) << 8);
2776  }
2777  FORCE_INLINE int _mm_movemask_pd(__m128d a)
2778  {
2779      uint64x2_t input = vreinterpretq_u64_m128d(a);
2780      uint64x2_t high_bits = vshrq_n_u64(input, 63);
2781      return vgetq_lane_u64(high_bits, 0) | (vgetq_lane_u64(high_bits, 1) << 1);
2782  }
2783  FORCE_INLINE __m64 _mm_movepi64_pi64(__m128i a)
2784  {
2785      return vreinterpret_m64_s64(vget_low_s64(vreinterpretq_s64_m128i(a)));
2786  }
2787  FORCE_INLINE __m128i _mm_movpi64_epi64(__m64 a)
2788  {
2789      return vreinterpretq_m128i_s64(
2790          vcombine_s64(vreinterpret_s64_m64(a), vdup_n_s64(0)));
2791  }
2792  FORCE_INLINE __m128i _mm_mul_epu32(__m128i a, __m128i b)
2793  {
2794      uint32x2_t a_lo = vmovn_u64(vreinterpretq_u64_m128i(a));
2795      uint32x2_t b_lo = vmovn_u64(vreinterpretq_u64_m128i(b));
2796      return vreinterpretq_m128i_u64(vmull_u32(a_lo, b_lo));
2797  }
2798  FORCE_INLINE __m128d _mm_mul_pd(__m128d a, __m128d b)
2799  {
2800  #if defined(__aarch64__)
2801      return vreinterpretq_m128d_f64(
2802          vmulq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
2803  #else
2804      double *da = (double *) &a;
2805      double *db = (double *) &b;
2806      double c[2];
2807      c[0] = da[0] * db[0];
2808      c[1] = da[1] * db[1];
2809      return vld1q_f32((float32_t *) c);
2810  #endif
2811  }
2812  FORCE_INLINE __m128d _mm_mul_sd(__m128d a, __m128d b)
2813  {
2814      return _mm_move_sd(a, _mm_mul_pd(a, b));
2815  }
2816  FORCE_INLINE __m64 _mm_mul_su32(__m64 a, __m64 b)
2817  {
2818      return vreinterpret_m64_u64(vget_low_u64(
2819          vmull_u32(vreinterpret_u32_m64(a), vreinterpret_u32_m64(b))));
2820  }
2821  FORCE_INLINE __m128i _mm_mulhi_epi16(__m128i a, __m128i b)
2822  {
2823      int16x4_t a3210 = vget_low_s16(vreinterpretq_s16_m128i(a));
2824      int16x4_t b3210 = vget_low_s16(vreinterpretq_s16_m128i(b));
2825      int32x4_t ab3210 = vmull_s16(a3210, b3210); &bsol;* 3333222211110000 */
2826      int16x4_t a7654 = vget_high_s16(vreinterpretq_s16_m128i(a));
2827      int16x4_t b7654 = vget_high_s16(vreinterpretq_s16_m128i(b));
2828      int32x4_t ab7654 = vmull_s16(a7654, b7654); &bsol;* 7777666655554444 */
2829      uint16x8x2_t r =
2830          vuzpq_u16(vreinterpretq_u16_s32(ab3210), vreinterpretq_u16_s32(ab7654));
2831      return vreinterpretq_m128i_u16(r.val[1]);
2832  }
2833  FORCE_INLINE __m128i _mm_mulhi_epu16(__m128i a, __m128i b)
2834  {
2835      uint16x4_t a3210 = vget_low_u16(vreinterpretq_u16_m128i(a));
2836      uint16x4_t b3210 = vget_low_u16(vreinterpretq_u16_m128i(b));
2837      uint32x4_t ab3210 = vmull_u16(a3210, b3210);
2838  #if defined(__aarch64__)
2839      uint32x4_t ab7654 =
2840          vmull_high_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b));
2841      uint16x8_t r = vuzp2q_u16(vreinterpretq_u16_u32(ab3210),
2842                                vreinterpretq_u16_u32(ab7654));
2843      return vreinterpretq_m128i_u16(r);
2844  #else
2845      uint16x4_t a7654 = vget_high_u16(vreinterpretq_u16_m128i(a));
2846      uint16x4_t b7654 = vget_high_u16(vreinterpretq_u16_m128i(b));
2847      uint32x4_t ab7654 = vmull_u16(a7654, b7654);
2848      uint16x8x2_t r =
2849          vuzpq_u16(vreinterpretq_u16_u32(ab3210), vreinterpretq_u16_u32(ab7654));
2850      return vreinterpretq_m128i_u16(r.val[1]);
2851  #endif
2852  }
2853  FORCE_INLINE __m128i _mm_mullo_epi16(__m128i a, __m128i b)
2854  {
2855      return vreinterpretq_m128i_s16(
2856          vmulq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
2857  }
2858  FORCE_INLINE __m128d _mm_or_pd(__m128d a, __m128d b)
2859  {
2860      return vreinterpretq_m128d_s64(
2861          vorrq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));
2862  }
2863  FORCE_INLINE __m128i _mm_or_si128(__m128i a, __m128i b)
2864  {
2865      return vreinterpretq_m128i_s32(
2866          vorrq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
2867  }
2868  FORCE_INLINE __m128i _mm_packs_epi16(__m128i a, __m128i b)
2869  {
2870      return vreinterpretq_m128i_s8(
2871          vcombine_s8(vqmovn_s16(vreinterpretq_s16_m128i(a)),
2872                      vqmovn_s16(vreinterpretq_s16_m128i(b))));
2873  }
2874  FORCE_INLINE __m128i _mm_packs_epi32(__m128i a, __m128i b)
2875  {
2876      return vreinterpretq_m128i_s16(
2877          vcombine_s16(vqmovn_s32(vreinterpretq_s32_m128i(a)),
2878                       vqmovn_s32(vreinterpretq_s32_m128i(b))));
2879  }
2880  FORCE_INLINE __m128i _mm_packus_epi16(const __m128i a, const __m128i b)
2881  {
2882      return vreinterpretq_m128i_u8(
2883          vcombine_u8(vqmovun_s16(vreinterpretq_s16_m128i(a)),
2884                      vqmovun_s16(vreinterpretq_s16_m128i(b))));
2885  }
2886  FORCE_INLINE void _mm_pause()
2887  {
2888      __asm__ __volatile__("isb\n");
2889  }
2890  FORCE_INLINE __m128i _mm_sad_epu8(__m128i a, __m128i b)
2891  {
2892      uint16x8_t t = vpaddlq_u8(vabdq_u8((uint8x16_t) a, (uint8x16_t) b));
2893      return vreinterpretq_m128i_u64(vpaddlq_u32(vpaddlq_u16(t)));
2894  }
2895  FORCE_INLINE __m128i _mm_set_epi16(short i7,
2896                                     short i6,
2897                                     short i5,
2898                                     short i4,
2899                                     short i3,
2900                                     short i2,
2901                                     short i1,
2902                                     short i0)
2903  {
2904      int16_t ALIGN_STRUCT(16) data[8] = {i0, i1, i2, i3, i4, i5, i6, i7};
2905      return vreinterpretq_m128i_s16(vld1q_s16(data));
2906  }
2907  FORCE_INLINE __m128i _mm_set_epi32(int i3, int i2, int i1, int i0)
2908  {
2909      int32_t ALIGN_STRUCT(16) data[4] = {i0, i1, i2, i3};
2910      return vreinterpretq_m128i_s32(vld1q_s32(data));
2911  }
2912  FORCE_INLINE __m128i _mm_set_epi64(__m64 i1, __m64 i2)
2913  {
2914      return _mm_set_epi64x((int64_t) i1, (int64_t) i2);
2915  }
2916  FORCE_INLINE __m128i _mm_set_epi64x(int64_t i1, int64_t i2)
2917  {
2918      return vreinterpretq_m128i_s64(
2919          vcombine_s64(vcreate_s64(i2), vcreate_s64(i1)));
2920  }
2921  FORCE_INLINE __m128i _mm_set_epi8(signed char b15,
2922                                    signed char b14,
2923                                    signed char b13,
2924                                    signed char b12,
2925                                    signed char b11,
2926                                    signed char b10,
2927                                    signed char b9,
2928                                    signed char b8,
2929                                    signed char b7,
2930                                    signed char b6,
2931                                    signed char b5,
2932                                    signed char b4,
2933                                    signed char b3,
2934                                    signed char b2,
2935                                    signed char b1,
2936                                    signed char b0)
2937  {
2938      int8_t ALIGN_STRUCT(16)
2939          data[16] = {(int8_t) b0,  (int8_t) b1,  (int8_t) b2,  (int8_t) b3,
2940                      (int8_t) b4,  (int8_t) b5,  (int8_t) b6,  (int8_t) b7,
2941                      (int8_t) b8,  (int8_t) b9,  (int8_t) b10, (int8_t) b11,
2942                      (int8_t) b12, (int8_t) b13, (int8_t) b14, (int8_t) b15};
2943      return (__m128i) vld1q_s8(data);
2944  }
2945  FORCE_INLINE __m128d _mm_set_pd(double e1, double e0)
2946  {
2947      double ALIGN_STRUCT(16) data[2] = {e0, e1};
2948  #if defined(__aarch64__)
2949      return vreinterpretq_m128d_f64(vld1q_f64((float64_t *) data));
2950  #else
2951      return vreinterpretq_m128d_f32(vld1q_f32((float32_t *) data));
2952  #endif
2953  }
2954  #define _mm_set_pd1 _mm_set1_pd
2955  FORCE_INLINE __m128d _mm_set_sd(double a)
2956  {
2957  #if defined(__aarch64__)
2958      return vreinterpretq_m128d_f64(vsetq_lane_f64(a, vdupq_n_f64(0), 0));
2959  #else
2960      return _mm_set_pd(0, a);
2961  #endif
2962  }
2963  FORCE_INLINE __m128i _mm_set1_epi16(short w)
2964  {
2965      return vreinterpretq_m128i_s16(vdupq_n_s16(w));
2966  }
2967  FORCE_INLINE __m128i _mm_set1_epi32(int _i)
2968  {
2969      return vreinterpretq_m128i_s32(vdupq_n_s32(_i));
2970  }
2971  FORCE_INLINE __m128i _mm_set1_epi64(__m64 _i)
2972  {
2973      return vreinterpretq_m128i_s64(vdupq_n_s64((int64_t) _i));
2974  }
2975  FORCE_INLINE __m128i _mm_set1_epi64x(int64_t _i)
2976  {
2977      return vreinterpretq_m128i_s64(vdupq_n_s64(_i));
2978  }
2979  FORCE_INLINE __m128i _mm_set1_epi8(signed char w)
2980  {
2981      return vreinterpretq_m128i_s8(vdupq_n_s8(w));
2982  }
2983  FORCE_INLINE __m128d _mm_set1_pd(double d)
2984  {
2985  #if defined(__aarch64__)
2986      return vreinterpretq_m128d_f64(vdupq_n_f64(d));
2987  #else
2988      return vreinterpretq_m128d_s64(vdupq_n_s64(*(int64_t *) &d));
2989  #endif
2990  }
2991  FORCE_INLINE __m128i _mm_setr_epi16(short w0,
2992                                      short w1,
2993                                      short w2,
2994                                      short w3,
2995                                      short w4,
2996                                      short w5,
2997                                      short w6,
2998                                      short w7)
2999  {
3000      int16_t ALIGN_STRUCT(16) data[8] = {w0, w1, w2, w3, w4, w5, w6, w7};
3001      return vreinterpretq_m128i_s16(vld1q_s16((int16_t *) data));
3002  }
3003  FORCE_INLINE __m128i _mm_setr_epi32(int i3, int i2, int i1, int i0)
3004  {
3005      int32_t ALIGN_STRUCT(16) data[4] = {i3, i2, i1, i0};
3006      return vreinterpretq_m128i_s32(vld1q_s32(data));
3007  }
3008  FORCE_INLINE __m128i _mm_setr_epi64(__m64 e1, __m64 e0)
3009  {
3010      return vreinterpretq_m128i_s64(vcombine_s64(e1, e0));
3011  }
3012  FORCE_INLINE __m128i _mm_setr_epi8(signed char b0,
3013                                     signed char b1,
3014                                     signed char b2,
3015                                     signed char b3,
3016                                     signed char b4,
3017                                     signed char b5,
3018                                     signed char b6,
3019                                     signed char b7,
3020                                     signed char b8,
3021                                     signed char b9,
3022                                     signed char b10,
3023                                     signed char b11,
3024                                     signed char b12,
3025                                     signed char b13,
3026                                     signed char b14,
3027                                     signed char b15)
3028  {
3029      int8_t ALIGN_STRUCT(16)
3030          data[16] = {(int8_t) b0,  (int8_t) b1,  (int8_t) b2,  (int8_t) b3,
3031                      (int8_t) b4,  (int8_t) b5,  (int8_t) b6,  (int8_t) b7,
3032                      (int8_t) b8,  (int8_t) b9,  (int8_t) b10, (int8_t) b11,
3033                      (int8_t) b12, (int8_t) b13, (int8_t) b14, (int8_t) b15};
3034      return (__m128i) vld1q_s8(data);
3035  }
3036  FORCE_INLINE __m128d _mm_setr_pd(double e1, double e0)
3037  {
3038      return _mm_set_pd(e0, e1);
3039  }
3040  FORCE_INLINE __m128d _mm_setzero_pd(void)
3041  {
3042  #if defined(__aarch64__)
3043      return vreinterpretq_m128d_f64(vdupq_n_f64(0));
3044  #else
3045      return vreinterpretq_m128d_f32(vdupq_n_f32(0));
3046  #endif
3047  }
3048  FORCE_INLINE __m128i _mm_setzero_si128(void)
3049  {
3050      return vreinterpretq_m128i_s32(vdupq_n_s32(0));
3051  }
3052  #ifdef _sse2neon_shuffle
3053  #define _mm_shuffle_epi32(a, imm)                                            \
3054      __extension__({                                                          \
3055          int32x4_t _input = vreinterpretq_s32_m128i(a);                       \
3056          int32x4_t _shuf =                                                    \
3057              vshuffleq_s32(_input, _input, (imm) & (0x3), ((imm) >> 2) & 0x3, \
3058                            ((imm) >> 4) & 0x3, ((imm) >> 6) & 0x3);           \
3059          vreinterpretq_m128i_s32(_shuf);                                      \
3060      })
3061  #else  
3062  #define _mm_shuffle_epi32(a, imm)                        \
3063      __extension__({                                      \
3064          __m128i ret;                                     \
3065          switch (imm) {                                   \
3066          case _MM_SHUFFLE(1, 0, 3, 2):                    \
3067              ret = _mm_shuffle_epi_1032((a));             \
3068              break;                                       \
3069          case _MM_SHUFFLE(2, 3, 0, 1):                    \
3070              ret = _mm_shuffle_epi_2301((a));             \
3071              break;                                       \
3072          case _MM_SHUFFLE(0, 3, 2, 1):                    \
3073              ret = _mm_shuffle_epi_0321((a));             \
3074              break;                                       \
3075          case _MM_SHUFFLE(2, 1, 0, 3):                    \
3076              ret = _mm_shuffle_epi_2103((a));             \
3077              break;                                       \
3078          case _MM_SHUFFLE(1, 0, 1, 0):                    \
3079              ret = _mm_shuffle_epi_1010((a));             \
3080              break;                                       \
3081          case _MM_SHUFFLE(1, 0, 0, 1):                    \
3082              ret = _mm_shuffle_epi_1001((a));             \
3083              break;                                       \
3084          case _MM_SHUFFLE(0, 1, 0, 1):                    \
3085              ret = _mm_shuffle_epi_0101((a));             \
3086              break;                                       \
3087          case _MM_SHUFFLE(2, 2, 1, 1):                    \
3088              ret = _mm_shuffle_epi_2211((a));             \
3089              break;                                       \
3090          case _MM_SHUFFLE(0, 1, 2, 2):                    \
3091              ret = _mm_shuffle_epi_0122((a));             \
3092              break;                                       \
3093          case _MM_SHUFFLE(3, 3, 3, 2):                    \
3094              ret = _mm_shuffle_epi_3332((a));             \
3095              break;                                       \
3096          case _MM_SHUFFLE(0, 0, 0, 0):                    \
3097              ret = _mm_shuffle_epi32_splat((a), 0);       \
3098              break;                                       \
3099          case _MM_SHUFFLE(1, 1, 1, 1):                    \
3100              ret = _mm_shuffle_epi32_splat((a), 1);       \
3101              break;                                       \
3102          case _MM_SHUFFLE(2, 2, 2, 2):                    \
3103              ret = _mm_shuffle_epi32_splat((a), 2);       \
3104              break;                                       \
3105          case _MM_SHUFFLE(3, 3, 3, 3):                    \
3106              ret = _mm_shuffle_epi32_splat((a), 3);       \
3107              break;                                       \
3108          default:                                         \
3109              ret = _mm_shuffle_epi32_default((a), (imm)); \
3110              break;                                       \
3111          }                                                \
3112          ret;                                             \
3113      })
3114  #endif
3115  #ifdef _sse2neon_shuffle
3116  #define _mm_shuffle_pd(a, b, imm8)                                            \
3117      vreinterpretq_m128d_s64(                                                  \
3118          vshuffleq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b), \
3119                        imm8 & 0x1, ((imm8 & 0x2) >> 1) + 2))
3120  #else
3121  #define _mm_shuffle_pd(a, b, imm8)                                     \
3122      _mm_castsi128_pd(_mm_set_epi64x(                                   \
3123          vgetq_lane_s64(vreinterpretq_s64_m128d(b), (imm8 & 0x2) >> 1), \
3124          vgetq_lane_s64(vreinterpretq_s64_m128d(a), imm8 & 0x1)))
3125  #endif
3126  #ifdef _sse2neon_shuffle
3127  #define _mm_shufflehi_epi16(a, imm)                                           \
3128      __extension__({                                                           \
3129          int16x8_t _input = vreinterpretq_s16_m128i(a);                        \
3130          int16x8_t _shuf =                                                     \
3131              vshuffleq_s16(_input, _input, 0, 1, 2, 3, ((imm) & (0x3)) + 4,    \
3132                            (((imm) >> 2) & 0x3) + 4, (((imm) >> 4) & 0x3) + 4, \
3133                            (((imm) >> 6) & 0x3) + 4);                          \
3134          vreinterpretq_m128i_s16(_shuf);                                       \
3135      })
3136  #else  
3137  #define _mm_shufflehi_epi16(a, imm) _mm_shufflehi_epi16_function((a), (imm))
3138  #endif
3139  #ifdef _sse2neon_shuffle
3140  #define _mm_shufflelo_epi16(a, imm)                                  \
3141      __extension__({                                                  \
3142          int16x8_t _input = vreinterpretq_s16_m128i(a);               \
3143          int16x8_t _shuf = vshuffleq_s16(                             \
3144              _input, _input, ((imm) & (0x3)), (((imm) >> 2) & 0x3),   \
3145              (((imm) >> 4) & 0x3), (((imm) >> 6) & 0x3), 4, 5, 6, 7); \
3146          vreinterpretq_m128i_s16(_shuf);                              \
3147      })
3148  #else  
3149  #define _mm_shufflelo_epi16(a, imm) _mm_shufflelo_epi16_function((a), (imm))
3150  #endif
3151  FORCE_INLINE __m128i _mm_sll_epi16(__m128i a, __m128i count)
3152  {
3153      uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
3154      if (_sse2neon_unlikely(c & ~15))
3155          return _mm_setzero_si128();
3156      int16x8_t vc = vdupq_n_s16((int16_t) c);
3157      return vreinterpretq_m128i_s16(vshlq_s16(vreinterpretq_s16_m128i(a), vc));
3158  }
3159  FORCE_INLINE __m128i _mm_sll_epi32(__m128i a, __m128i count)
3160  {
3161      uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
3162      if (_sse2neon_unlikely(c & ~31))
3163          return _mm_setzero_si128();
3164      int32x4_t vc = vdupq_n_s32((int32_t) c);
3165      return vreinterpretq_m128i_s32(vshlq_s32(vreinterpretq_s32_m128i(a), vc));
3166  }
3167  FORCE_INLINE __m128i _mm_sll_epi64(__m128i a, __m128i count)
3168  {
3169      uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
3170      if (_sse2neon_unlikely(c & ~63))
3171          return _mm_setzero_si128();
3172      int64x2_t vc = vdupq_n_s64((int64_t) c);
3173      return vreinterpretq_m128i_s64(vshlq_s64(vreinterpretq_s64_m128i(a), vc));
3174  }
3175  FORCE_INLINE __m128i _mm_slli_epi16(__m128i a, int imm)
3176  {
3177      if (_sse2neon_unlikely(imm & ~15))
3178          return _mm_setzero_si128();
3179      return vreinterpretq_m128i_s16(
3180          vshlq_s16(vreinterpretq_s16_m128i(a), vdupq_n_s16(imm)));
3181  }
3182  FORCE_INLINE __m128i _mm_slli_epi32(__m128i a, int imm)
3183  {
3184      if (_sse2neon_unlikely(imm & ~31))
3185          return _mm_setzero_si128();
3186      return vreinterpretq_m128i_s32(
3187          vshlq_s32(vreinterpretq_s32_m128i(a), vdupq_n_s32(imm)));
3188  }
3189  FORCE_INLINE __m128i _mm_slli_epi64(__m128i a, int imm)
3190  {
3191      if (_sse2neon_unlikely(imm & ~63))
3192          return _mm_setzero_si128();
3193      return vreinterpretq_m128i_s64(
3194          vshlq_s64(vreinterpretq_s64_m128i(a), vdupq_n_s64(imm)));
3195  }
3196  #define _mm_slli_si128(a, imm)                                         \
3197      __extension__({                                                    \
3198          int8x16_t ret;                                                 \
3199          if (_sse2neon_unlikely(imm == 0))                              \
3200              ret = vreinterpretq_s8_m128i(a);                           \
3201          else if (_sse2neon_unlikely((imm) & ~15))                      \
3202              ret = vdupq_n_s8(0);                                       \
3203          else                                                           \
3204              ret = vextq_s8(vdupq_n_s8(0), vreinterpretq_s8_m128i(a),   \
3205                             ((imm <= 0 || imm > 15) ? 0 : (16 - imm))); \
3206          vreinterpretq_m128i_s8(ret);                                   \
3207      })
3208  FORCE_INLINE __m128d _mm_sqrt_pd(__m128d a)
3209  {
3210  #if defined(__aarch64__)
3211      return vreinterpretq_m128d_f64(vsqrtq_f64(vreinterpretq_f64_m128d(a)));
3212  #else
3213      double a0 = sqrt(((double *) &a)[0]);
3214      double a1 = sqrt(((double *) &a)[1]);
3215      return _mm_set_pd(a1, a0);
3216  #endif
3217  }
3218  FORCE_INLINE __m128d _mm_sqrt_sd(__m128d a, __m128d b)
3219  {
3220  #if defined(__aarch64__)
3221      return _mm_move_sd(a, _mm_sqrt_pd(b));
3222  #else
3223      return _mm_set_pd(((double *) &a)[1], sqrt(((double *) &b)[0]));
3224  #endif
3225  }
3226  FORCE_INLINE __m128i _mm_sra_epi16(__m128i a, __m128i count)
3227  {
3228      int64_t c = (int64_t) vget_low_s64((int64x2_t) count);
3229      if (_sse2neon_unlikely(c & ~15))
3230          return _mm_cmplt_epi16(a, _mm_setzero_si128());
3231      return vreinterpretq_m128i_s16(vshlq_s16((int16x8_t) a, vdupq_n_s16(-c)));
3232  }
3233  FORCE_INLINE __m128i _mm_sra_epi32(__m128i a, __m128i count)
3234  {
3235      int64_t c = (int64_t) vget_low_s64((int64x2_t) count);
3236      if (_sse2neon_unlikely(c & ~31))
3237          return _mm_cmplt_epi32(a, _mm_setzero_si128());
3238      return vreinterpretq_m128i_s32(vshlq_s32((int32x4_t) a, vdupq_n_s32(-c)));
3239  }
3240  FORCE_INLINE __m128i _mm_srai_epi16(__m128i a, int imm)
3241  {
3242      const int count = (imm & ~15) ? 15 : imm;
3243      return (__m128i) vshlq_s16((int16x8_t) a, vdupq_n_s16(-count));
3244  }
3245  #define _mm_srai_epi32(a, imm)                                               \
3246      __extension__({                                                          \
3247          __m128i ret;                                                         \
3248          if (_sse2neon_unlikely((imm) == 0)) {                                \
3249              ret = a;                                                         \
3250          } else if (_sse2neon_likely(0 < (imm) && (imm) < 32)) {              \
3251              ret = vreinterpretq_m128i_s32(                                   \
3252                  vshlq_s32(vreinterpretq_s32_m128i(a), vdupq_n_s32(-(imm)))); \
3253          } else {                                                             \
3254              ret = vreinterpretq_m128i_s32(                                   \
3255                  vshrq_n_s32(vreinterpretq_s32_m128i(a), 31));                \
3256          }                                                                    \
3257          ret;                                                                 \
3258      })
3259  FORCE_INLINE __m128i _mm_srl_epi16(__m128i a, __m128i count)
3260  {
3261      uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
3262      if (_sse2neon_unlikely(c & ~15))
3263          return _mm_setzero_si128();
3264      int16x8_t vc = vdupq_n_s16(-(int16_t) c);
3265      return vreinterpretq_m128i_u16(vshlq_u16(vreinterpretq_u16_m128i(a), vc));
3266  }
3267  FORCE_INLINE __m128i _mm_srl_epi32(__m128i a, __m128i count)
3268  {
3269      uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
3270      if (_sse2neon_unlikely(c & ~31))
3271          return _mm_setzero_si128();
3272      int32x4_t vc = vdupq_n_s32(-(int32_t) c);
3273      return vreinterpretq_m128i_u32(vshlq_u32(vreinterpretq_u32_m128i(a), vc));
3274  }
3275  FORCE_INLINE __m128i _mm_srl_epi64(__m128i a, __m128i count)
3276  {
3277      uint64_t c = vreinterpretq_nth_u64_m128i(count, 0);
3278      if (_sse2neon_unlikely(c & ~63))
3279          return _mm_setzero_si128();
3280      int64x2_t vc = vdupq_n_s64(-(int64_t) c);
3281      return vreinterpretq_m128i_u64(vshlq_u64(vreinterpretq_u64_m128i(a), vc));
3282  }
3283  #define _mm_srli_epi16(a, imm)                                               \
3284      __extension__({                                                          \
3285          __m128i ret;                                                         \
3286          if (_sse2neon_unlikely((imm) & ~15)) {                               \
3287              ret = _mm_setzero_si128();                                       \
3288          } else {                                                             \
3289              ret = vreinterpretq_m128i_u16(                                   \
3290                  vshlq_u16(vreinterpretq_u16_m128i(a), vdupq_n_s16(-(imm)))); \
3291          }                                                                    \
3292          ret;                                                                 \
3293      })
3294  #define _mm_srli_epi32(a, imm)                                               \
3295      __extension__({                                                          \
3296          __m128i ret;                                                         \
3297          if (_sse2neon_unlikely((imm) & ~31)) {                               \
3298              ret = _mm_setzero_si128();                                       \
3299          } else {                                                             \
3300              ret = vreinterpretq_m128i_u32(                                   \
3301                  vshlq_u32(vreinterpretq_u32_m128i(a), vdupq_n_s32(-(imm)))); \
3302          }                                                                    \
3303          ret;                                                                 \
3304      })
3305  #define _mm_srli_epi64(a, imm)                                               \
3306      __extension__({                                                          \
3307          __m128i ret;                                                         \
3308          if (_sse2neon_unlikely((imm) & ~63)) {                               \
3309              ret = _mm_setzero_si128();                                       \
3310          } else {                                                             \
3311              ret = vreinterpretq_m128i_u64(                                   \
3312                  vshlq_u64(vreinterpretq_u64_m128i(a), vdupq_n_s64(-(imm)))); \
3313          }                                                                    \
3314          ret;                                                                 \
3315      })
3316  #define _mm_srli_si128(a, imm)                                       \
3317      __extension__({                                                  \
3318          int8x16_t ret;                                               \
3319          if (_sse2neon_unlikely((imm) & ~15))                         \
3320              ret = vdupq_n_s8(0);                                     \
3321          else                                                         \
3322              ret = vextq_s8(vreinterpretq_s8_m128i(a), vdupq_n_s8(0), \
3323                             (imm > 15 ? 0 : imm));                    \
3324          vreinterpretq_m128i_s8(ret);                                 \
3325      })
3326  FORCE_INLINE void _mm_store_pd(double *mem_addr, __m128d a)
3327  {
3328  #if defined(__aarch64__)
3329      vst1q_f64((float64_t *) mem_addr, vreinterpretq_f64_m128d(a));
3330  #else
3331      vst1q_f32((float32_t *) mem_addr, vreinterpretq_f32_m128d(a));
3332  #endif
3333  }
3334  FORCE_INLINE void _mm_store_pd1(double *mem_addr, __m128d a)
3335  {
3336  #if defined(__aarch64__)
3337      float64x1_t a_low = vget_low_f64(vreinterpretq_f64_m128d(a));
3338      vst1q_f64((float64_t *) mem_addr,
3339                vreinterpretq_f64_m128d(vcombine_f64(a_low, a_low)));
3340  #else
3341      float32x2_t a_low = vget_low_f32(vreinterpretq_f32_m128d(a));
3342      vst1q_f32((float32_t *) mem_addr,
3343                vreinterpretq_f32_m128d(vcombine_f32(a_low, a_low)));
3344  #endif
3345  }
3346  FORCE_INLINE void _mm_store_sd(double *mem_addr, __m128d a)
3347  {
3348  #if defined(__aarch64__)
3349      vst1_f64((float64_t *) mem_addr, vget_low_f64(vreinterpretq_f64_m128d(a)));
3350  #else
3351      vst1_u64((uint64_t *) mem_addr, vget_low_u64(vreinterpretq_u64_m128d(a)));
3352  #endif
3353  }
3354  FORCE_INLINE void _mm_store_si128(__m128i *p, __m128i a)
3355  {
3356      vst1q_s32((int32_t *) p, vreinterpretq_s32_m128i(a));
3357  }
3358  #define _mm_store1_pd _mm_store_pd1
3359  FORCE_INLINE void _mm_storeh_pd(double *mem_addr, __m128d a)
3360  {
3361  #if defined(__aarch64__)
3362      vst1_f64((float64_t *) mem_addr, vget_high_f64(vreinterpretq_f64_m128d(a)));
3363  #else
3364      vst1_f32((float32_t *) mem_addr, vget_high_f32(vreinterpretq_f32_m128d(a)));
3365  #endif
3366  }
3367  FORCE_INLINE void _mm_storel_epi64(__m128i *a, __m128i b)
3368  {
3369      vst1_u64((uint64_t *) a, vget_low_u64(vreinterpretq_u64_m128i(b)));
3370  }
3371  FORCE_INLINE void _mm_storel_pd(double *mem_addr, __m128d a)
3372  {
3373  #if defined(__aarch64__)
3374      vst1_f64((float64_t *) mem_addr, vget_low_f64(vreinterpretq_f64_m128d(a)));
3375  #else
3376      vst1_f32((float32_t *) mem_addr, vget_low_f32(vreinterpretq_f32_m128d(a)));
3377  #endif
3378  }
3379  FORCE_INLINE void _mm_storer_pd(double *mem_addr, __m128d a)
3380  {
3381      float32x4_t f = vreinterpretq_f32_m128d(a);
3382      _mm_store_pd(mem_addr, vreinterpretq_m128d_f32(vextq_f32(f, f, 2)));
3383  }
3384  FORCE_INLINE void _mm_storeu_pd(double *mem_addr, __m128d a)
3385  {
3386      _mm_store_pd(mem_addr, a);
3387  }
3388  FORCE_INLINE void _mm_storeu_si128(__m128i *p, __m128i a)
3389  {
3390      vst1q_s32((int32_t *) p, vreinterpretq_s32_m128i(a));
3391  }
3392  FORCE_INLINE void _mm_storeu_si32(void *p, __m128i a)
3393  {
3394      vst1q_lane_s32((int32_t *) p, vreinterpretq_s32_m128i(a), 0);
3395  }
3396  FORCE_INLINE void _mm_stream_pd(double *p, __m128d a)
3397  {
3398  #if __has_builtin(__builtin_nontemporal_store)
3399      __builtin_nontemporal_store(a, (float32x4_t *) p);
3400  #elif defined(__aarch64__)
3401      vst1q_f64(p, vreinterpretq_f64_m128d(a));
3402  #else
3403      vst1q_s64((int64_t *) p, vreinterpretq_s64_m128d(a));
3404  #endif
3405  }
3406  FORCE_INLINE void _mm_stream_si128(__m128i *p, __m128i a)
3407  {
3408  #if __has_builtin(__builtin_nontemporal_store)
3409      __builtin_nontemporal_store(a, p);
3410  #else
3411      vst1q_s64((int64_t *) p, vreinterpretq_s64_m128i(a));
3412  #endif
3413  }
3414  FORCE_INLINE void _mm_stream_si32(int *p, int a)
3415  {
3416      vst1q_lane_s32((int32_t *) p, vdupq_n_s32(a), 0);
3417  }
3418  FORCE_INLINE void _mm_stream_si64(__int64 *p, __int64 a)
3419  {
3420      vst1_s64((int64_t *) p, vdup_n_s64((int64_t) a));
3421  }
3422  FORCE_INLINE __m128i _mm_sub_epi16(__m128i a, __m128i b)
3423  {
3424      return vreinterpretq_m128i_s16(
3425          vsubq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
3426  }
3427  FORCE_INLINE __m128i _mm_sub_epi32(__m128i a, __m128i b)
3428  {
3429      return vreinterpretq_m128i_s32(
3430          vsubq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
3431  }
3432  FORCE_INLINE __m128i _mm_sub_epi64(__m128i a, __m128i b)
3433  {
3434      return vreinterpretq_m128i_s64(
3435          vsubq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
3436  }
3437  FORCE_INLINE __m128i _mm_sub_epi8(__m128i a, __m128i b)
3438  {
3439      return vreinterpretq_m128i_s8(
3440          vsubq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
3441  }
3442  FORCE_INLINE __m128d _mm_sub_pd(__m128d a, __m128d b)
3443  {
3444  #if defined(__aarch64__)
3445      return vreinterpretq_m128d_f64(
3446          vsubq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
3447  #else
3448      double *da = (double *) &a;
3449      double *db = (double *) &b;
3450      double c[2];
3451      c[0] = da[0] - db[0];
3452      c[1] = da[1] - db[1];
3453      return vld1q_f32((float32_t *) c);
3454  #endif
3455  }
3456  FORCE_INLINE __m128d _mm_sub_sd(__m128d a, __m128d b)
3457  {
3458      return _mm_move_sd(a, _mm_sub_pd(a, b));
3459  }
3460  FORCE_INLINE __m64 _mm_sub_si64(__m64 a, __m64 b)
3461  {
3462      return vreinterpret_m64_s64(
3463          vsub_s64(vreinterpret_s64_m64(a), vreinterpret_s64_m64(b)));
3464  }
3465  FORCE_INLINE __m128i _mm_subs_epi16(__m128i a, __m128i b)
3466  {
3467      return vreinterpretq_m128i_s16(
3468          vqsubq_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
3469  }
3470  FORCE_INLINE __m128i _mm_subs_epi8(__m128i a, __m128i b)
3471  {
3472      return vreinterpretq_m128i_s8(
3473          vqsubq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
3474  }
3475  FORCE_INLINE __m128i _mm_subs_epu16(__m128i a, __m128i b)
3476  {
3477      return vreinterpretq_m128i_u16(
3478          vqsubq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
3479  }
3480  FORCE_INLINE __m128i _mm_subs_epu8(__m128i a, __m128i b)
3481  {
3482      return vreinterpretq_m128i_u8(
3483          vqsubq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b)));
3484  }
3485  #define _mm_ucomieq_sd _mm_comieq_sd
3486  #define _mm_ucomige_sd _mm_comige_sd
3487  #define _mm_ucomigt_sd _mm_comigt_sd
3488  #define _mm_ucomile_sd _mm_comile_sd
3489  #define _mm_ucomilt_sd _mm_comilt_sd
3490  #define _mm_ucomineq_sd _mm_comineq_sd
3491  FORCE_INLINE __m128d _mm_undefined_pd(void)
3492  {
3493  #if defined(__GNUC__) || defined(__clang__)
3494  #pragma GCC diagnostic push
3495  #pragma GCC diagnostic ignored "-Wuninitialized"
3496  #endif
3497      __m128d a;
3498      return a;
3499  #if defined(__GNUC__) || defined(__clang__)
3500  #pragma GCC diagnostic pop
3501  #endif
3502  }
3503  FORCE_INLINE __m128i _mm_unpackhi_epi16(__m128i a, __m128i b)
3504  {
3505  #if defined(__aarch64__)
3506      return vreinterpretq_m128i_s16(
3507          vzip2q_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
3508  #else
3509      int16x4_t a1 = vget_high_s16(vreinterpretq_s16_m128i(a));
3510      int16x4_t b1 = vget_high_s16(vreinterpretq_s16_m128i(b));
3511      int16x4x2_t result = vzip_s16(a1, b1);
3512      return vreinterpretq_m128i_s16(vcombine_s16(result.val[0], result.val[1]));
3513  #endif
3514  }
3515  FORCE_INLINE __m128i _mm_unpackhi_epi32(__m128i a, __m128i b)
3516  {
3517  #if defined(__aarch64__)
3518      return vreinterpretq_m128i_s32(
3519          vzip2q_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
3520  #else
3521      int32x2_t a1 = vget_high_s32(vreinterpretq_s32_m128i(a));
3522      int32x2_t b1 = vget_high_s32(vreinterpretq_s32_m128i(b));
3523      int32x2x2_t result = vzip_s32(a1, b1);
3524      return vreinterpretq_m128i_s32(vcombine_s32(result.val[0], result.val[1]));
3525  #endif
3526  }
3527  FORCE_INLINE __m128i _mm_unpackhi_epi64(__m128i a, __m128i b)
3528  {
3529  #if defined(__aarch64__)
3530      return vreinterpretq_m128i_s64(
3531          vzip2q_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
3532  #else
3533      int64x1_t a_h = vget_high_s64(vreinterpretq_s64_m128i(a));
3534      int64x1_t b_h = vget_high_s64(vreinterpretq_s64_m128i(b));
3535      return vreinterpretq_m128i_s64(vcombine_s64(a_h, b_h));
3536  #endif
3537  }
3538  FORCE_INLINE __m128i _mm_unpackhi_epi8(__m128i a, __m128i b)
3539  {
3540  #if defined(__aarch64__)
3541      return vreinterpretq_m128i_s8(
3542          vzip2q_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
3543  #else
3544      int8x8_t a1 =
3545          vreinterpret_s8_s16(vget_high_s16(vreinterpretq_s16_m128i(a)));
3546      int8x8_t b1 =
3547          vreinterpret_s8_s16(vget_high_s16(vreinterpretq_s16_m128i(b)));
3548      int8x8x2_t result = vzip_s8(a1, b1);
3549      return vreinterpretq_m128i_s8(vcombine_s8(result.val[0], result.val[1]));
3550  #endif
3551  }
3552  FORCE_INLINE __m128d _mm_unpackhi_pd(__m128d a, __m128d b)
3553  {
3554  #if defined(__aarch64__)
3555      return vreinterpretq_m128d_f64(
3556          vzip2q_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
3557  #else
3558      return vreinterpretq_m128d_s64(
3559          vcombine_s64(vget_high_s64(vreinterpretq_s64_m128d(a)),
3560                       vget_high_s64(vreinterpretq_s64_m128d(b))));
3561  #endif
3562  }
3563  FORCE_INLINE __m128i _mm_unpacklo_epi16(__m128i a, __m128i b)
3564  {
3565  #if defined(__aarch64__)
3566      return vreinterpretq_m128i_s16(
3567          vzip1q_s16(vreinterpretq_s16_m128i(a), vreinterpretq_s16_m128i(b)));
3568  #else
3569      int16x4_t a1 = vget_low_s16(vreinterpretq_s16_m128i(a));
3570      int16x4_t b1 = vget_low_s16(vreinterpretq_s16_m128i(b));
3571      int16x4x2_t result = vzip_s16(a1, b1);
3572      return vreinterpretq_m128i_s16(vcombine_s16(result.val[0], result.val[1]));
3573  #endif
3574  }
3575  FORCE_INLINE __m128i _mm_unpacklo_epi32(__m128i a, __m128i b)
3576  {
3577  #if defined(__aarch64__)
3578      return vreinterpretq_m128i_s32(
3579          vzip1q_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
3580  #else
3581      int32x2_t a1 = vget_low_s32(vreinterpretq_s32_m128i(a));
3582      int32x2_t b1 = vget_low_s32(vreinterpretq_s32_m128i(b));
3583      int32x2x2_t result = vzip_s32(a1, b1);
3584      return vreinterpretq_m128i_s32(vcombine_s32(result.val[0], result.val[1]));
3585  #endif
3586  }
3587  FORCE_INLINE __m128i _mm_unpacklo_epi64(__m128i a, __m128i b)
3588  {
3589  #if defined(__aarch64__)
3590      return vreinterpretq_m128i_s64(
3591          vzip1q_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
3592  #else
3593      int64x1_t a_l = vget_low_s64(vreinterpretq_s64_m128i(a));
3594      int64x1_t b_l = vget_low_s64(vreinterpretq_s64_m128i(b));
3595      return vreinterpretq_m128i_s64(vcombine_s64(a_l, b_l));
3596  #endif
3597  }
3598  FORCE_INLINE __m128i _mm_unpacklo_epi8(__m128i a, __m128i b)
3599  {
3600  #if defined(__aarch64__)
3601      return vreinterpretq_m128i_s8(
3602          vzip1q_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
3603  #else
3604      int8x8_t a1 = vreinterpret_s8_s16(vget_low_s16(vreinterpretq_s16_m128i(a)));
3605      int8x8_t b1 = vreinterpret_s8_s16(vget_low_s16(vreinterpretq_s16_m128i(b)));
3606      int8x8x2_t result = vzip_s8(a1, b1);
3607      return vreinterpretq_m128i_s8(vcombine_s8(result.val[0], result.val[1]));
3608  #endif
3609  }
3610  FORCE_INLINE __m128d _mm_unpacklo_pd(__m128d a, __m128d b)
3611  {
3612  #if defined(__aarch64__)
3613      return vreinterpretq_m128d_f64(
3614          vzip1q_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
3615  #else
3616      return vreinterpretq_m128d_s64(
3617          vcombine_s64(vget_low_s64(vreinterpretq_s64_m128d(a)),
3618                       vget_low_s64(vreinterpretq_s64_m128d(b))));
3619  #endif
3620  }
3621  FORCE_INLINE __m128d _mm_xor_pd(__m128d a, __m128d b)
3622  {
3623      return vreinterpretq_m128d_s64(
3624          veorq_s64(vreinterpretq_s64_m128d(a), vreinterpretq_s64_m128d(b)));
3625  }
3626  FORCE_INLINE __m128i _mm_xor_si128(__m128i a, __m128i b)
3627  {
3628      return vreinterpretq_m128i_s32(
3629          veorq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
3630  }
3631  FORCE_INLINE __m128d _mm_addsub_pd(__m128d a, __m128d b)
3632  {
3633      _sse2neon_const __m128d mask = _mm_set_pd(1.0f, -1.0f);
3634  #if defined(__aarch64__)
3635      return vreinterpretq_m128d_f64(vfmaq_f64(vreinterpretq_f64_m128d(a),
3636                                               vreinterpretq_f64_m128d(b),
3637                                               vreinterpretq_f64_m128d(mask)));
3638  #else
3639      return _mm_add_pd(_mm_mul_pd(b, mask), a);
3640  #endif
3641  }
3642  FORCE_INLINE __m128 _mm_addsub_ps(__m128 a, __m128 b)
3643  {
3644      _sse2neon_const __m128 mask = _mm_setr_ps(-1.0f, 1.0f, -1.0f, 1.0f);
3645  #if defined(__aarch64__) || defined(__ARM_FEATURE_FMA) &bsol;* VFPv4+ */
3646      return vreinterpretq_m128_f32(vfmaq_f32(vreinterpretq_f32_m128(a),
3647                                              vreinterpretq_f32_m128(mask),
3648                                              vreinterpretq_f32_m128(b)));
3649  #else
3650      return _mm_add_ps(_mm_mul_ps(b, mask), a);
3651  #endif
3652  }
3653  FORCE_INLINE __m128d _mm_hadd_pd(__m128d a, __m128d b)
3654  {
3655  #if defined(__aarch64__)
3656      return vreinterpretq_m128d_f64(
3657          vpaddq_f64(vreinterpretq_f64_m128d(a), vreinterpretq_f64_m128d(b)));
3658  #else
3659      double *da = (double *) &a;
3660      double *db = (double *) &b;
3661      double c[] = {da[0] + da[1], db[0] + db[1]};
3662      return vreinterpretq_m128d_u64(vld1q_u64((uint64_t *) c));
3663  #endif
3664  }
3665  FORCE_INLINE __m128 _mm_hadd_ps(__m128 a, __m128 b)
3666  {
3667  #if defined(__aarch64__)
3668      return vreinterpretq_m128_f32(
3669          vpaddq_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(b)));
3670  #else
3671      float32x2_t a10 = vget_low_f32(vreinterpretq_f32_m128(a));
3672      float32x2_t a32 = vget_high_f32(vreinterpretq_f32_m128(a));
3673      float32x2_t b10 = vget_low_f32(vreinterpretq_f32_m128(b));
3674      float32x2_t b32 = vget_high_f32(vreinterpretq_f32_m128(b));
3675      return vreinterpretq_m128_f32(
3676          vcombine_f32(vpadd_f32(a10, a32), vpadd_f32(b10, b32)));
3677  #endif
3678  }
3679  FORCE_INLINE __m128d _mm_hsub_pd(__m128d _a, __m128d _b)
3680  {
3681  #if defined(__aarch64__)
3682      float64x2_t a = vreinterpretq_f64_m128d(_a);
3683      float64x2_t b = vreinterpretq_f64_m128d(_b);
3684      return vreinterpretq_m128d_f64(
3685          vsubq_f64(vuzp1q_f64(a, b), vuzp2q_f64(a, b)));
3686  #else
3687      double *da = (double *) &_a;
3688      double *db = (double *) &_b;
3689      double c[] = {da[0] - da[1], db[0] - db[1]};
3690      return vreinterpretq_m128d_u64(vld1q_u64((uint64_t *) c));
3691  #endif
3692  }
3693  FORCE_INLINE __m128 _mm_hsub_ps(__m128 _a, __m128 _b)
3694  {
3695      float32x4_t a = vreinterpretq_f32_m128(_a);
3696      float32x4_t b = vreinterpretq_f32_m128(_b);
3697  #if defined(__aarch64__)
3698      return vreinterpretq_m128_f32(
3699          vsubq_f32(vuzp1q_f32(a, b), vuzp2q_f32(a, b)));
3700  #else
3701      float32x4x2_t c = vuzpq_f32(a, b);
3702      return vreinterpretq_m128_f32(vsubq_f32(c.val[0], c.val[1]));
3703  #endif
3704  }
3705  #define _mm_lddqu_si128 _mm_loadu_si128
3706  #define _mm_loaddup_pd _mm_load1_pd
3707  FORCE_INLINE __m128d _mm_movedup_pd(__m128d a)
3708  {
3709  #if defined(__aarch64__)
3710      return vreinterpretq_m128d_f64(
3711          vdupq_laneq_f64(vreinterpretq_f64_m128d(a), 0));
3712  #else
3713      return vreinterpretq_m128d_u64(
3714          vdupq_n_u64(vgetq_lane_u64(vreinterpretq_u64_m128d(a), 0)));
3715  #endif
3716  }
3717  FORCE_INLINE __m128 _mm_movehdup_ps(__m128 a)
3718  {
3719  #if defined(__aarch64__)
3720      return vreinterpretq_m128_f32(
3721          vtrn2q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a)));
3722  #elif defined(_sse2neon_shuffle)
3723      return vreinterpretq_m128_f32(vshuffleq_s32(
3724          vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 1, 1, 3, 3));
3725  #else
3726      float32_t a1 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 1);
3727      float32_t a3 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 3);
3728      float ALIGN_STRUCT(16) data[4] = {a1, a1, a3, a3};
3729      return vreinterpretq_m128_f32(vld1q_f32(data));
3730  #endif
3731  }
3732  FORCE_INLINE __m128 _mm_moveldup_ps(__m128 a)
3733  {
3734  #if defined(__aarch64__)
3735      return vreinterpretq_m128_f32(
3736          vtrn1q_f32(vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a)));
3737  #elif defined(_sse2neon_shuffle)
3738      return vreinterpretq_m128_f32(vshuffleq_s32(
3739          vreinterpretq_f32_m128(a), vreinterpretq_f32_m128(a), 0, 0, 2, 2));
3740  #else
3741      float32_t a0 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 0);
3742      float32_t a2 = vgetq_lane_f32(vreinterpretq_f32_m128(a), 2);
3743      float ALIGN_STRUCT(16) data[4] = {a0, a0, a2, a2};
3744      return vreinterpretq_m128_f32(vld1q_f32(data));
3745  #endif
3746  }
3747  FORCE_INLINE __m128i _mm_abs_epi16(__m128i a)
3748  {
3749      return vreinterpretq_m128i_s16(vabsq_s16(vreinterpretq_s16_m128i(a)));
3750  }
3751  FORCE_INLINE __m128i _mm_abs_epi32(__m128i a)
3752  {
3753      return vreinterpretq_m128i_s32(vabsq_s32(vreinterpretq_s32_m128i(a)));
3754  }
3755  FORCE_INLINE __m128i _mm_abs_epi8(__m128i a)
3756  {
3757      return vreinterpretq_m128i_s8(vabsq_s8(vreinterpretq_s8_m128i(a)));
3758  }
3759  FORCE_INLINE __m64 _mm_abs_pi16(__m64 a)
3760  {
3761      return vreinterpret_m64_s16(vabs_s16(vreinterpret_s16_m64(a)));
3762  }
3763  FORCE_INLINE __m64 _mm_abs_pi32(__m64 a)
3764  {
3765      return vreinterpret_m64_s32(vabs_s32(vreinterpret_s32_m64(a)));
3766  }
3767  FORCE_INLINE __m64 _mm_abs_pi8(__m64 a)
3768  {
3769      return vreinterpret_m64_s8(vabs_s8(vreinterpret_s8_m64(a)));
3770  }
3771  #define _mm_alignr_epi8(a, b, imm)                                            \
3772      __extension__({                                                           \
3773          uint8x16_t _a = vreinterpretq_u8_m128i(a);                            \
3774          uint8x16_t _b = vreinterpretq_u8_m128i(b);                            \
3775          __m128i ret;                                                          \
3776          if (_sse2neon_unlikely((imm) & ~31))                                  \
3777              ret = vreinterpretq_m128i_u8(vdupq_n_u8(0));                      \
3778          else if (imm >= 16)                                                   \
3779              ret = _mm_srli_si128(a, imm >= 16 ? imm - 16 : 0);                \
3780          else                                                                  \
3781              ret =                                                             \
3782                  vreinterpretq_m128i_u8(vextq_u8(_b, _a, imm < 16 ? imm : 0)); \
3783          ret;                                                                  \
3784      })
3785  #define _mm_alignr_pi8(a, b, imm)                                           \
3786      __extension__({                                                         \
3787          __m64 ret;                                                          \
3788          if (_sse2neon_unlikely((imm) >= 16)) {                              \
3789              ret = vreinterpret_m64_s8(vdup_n_s8(0));                        \
3790          } else {                                                            \
3791              uint8x8_t tmp_low, tmp_high;                                    \
3792              if ((imm) >= 8) {                                               \
3793                  const int idx = (imm) -8;                                   \
3794                  tmp_low = vreinterpret_u8_m64(a);                           \
3795                  tmp_high = vdup_n_u8(0);                                    \
3796                  ret = vreinterpret_m64_u8(vext_u8(tmp_low, tmp_high, idx)); \
3797              } else {                                                        \
3798                  const int idx = (imm);                                      \
3799                  tmp_low = vreinterpret_u8_m64(b);                           \
3800                  tmp_high = vreinterpret_u8_m64(a);                          \
3801                  ret = vreinterpret_m64_u8(vext_u8(tmp_low, tmp_high, idx)); \
3802              }                                                               \
3803          }                                                                   \
3804          ret;                                                                \
3805      })
3806  FORCE_INLINE __m128i _mm_hadd_epi16(__m128i _a, __m128i _b)
3807  {
3808      int16x8_t a = vreinterpretq_s16_m128i(_a);
3809      int16x8_t b = vreinterpretq_s16_m128i(_b);
3810  #if defined(__aarch64__)
3811      return vreinterpretq_m128i_s16(vpaddq_s16(a, b));
3812  #else
3813      return vreinterpretq_m128i_s16(
3814          vcombine_s16(vpadd_s16(vget_low_s16(a), vget_high_s16(a)),
3815                       vpadd_s16(vget_low_s16(b), vget_high_s16(b))));
3816  #endif
3817  }
3818  FORCE_INLINE __m128i _mm_hadd_epi32(__m128i _a, __m128i _b)
3819  {
3820      int32x4_t a = vreinterpretq_s32_m128i(_a);
3821      int32x4_t b = vreinterpretq_s32_m128i(_b);
3822  #if defined(__aarch64__)
3823      return vreinterpretq_m128i_s32(vpaddq_s32(a, b));
3824  #else
3825      return vreinterpretq_m128i_s32(
3826          vcombine_s32(vpadd_s32(vget_low_s32(a), vget_high_s32(a)),
3827                       vpadd_s32(vget_low_s32(b), vget_high_s32(b))));
3828  #endif
3829  }
3830  FORCE_INLINE __m64 _mm_hadd_pi16(__m64 a, __m64 b)
3831  {
3832      return vreinterpret_m64_s16(
3833          vpadd_s16(vreinterpret_s16_m64(a), vreinterpret_s16_m64(b)));
3834  }
3835  FORCE_INLINE __m64 _mm_hadd_pi32(__m64 a, __m64 b)
3836  {
3837      return vreinterpret_m64_s32(
3838          vpadd_s32(vreinterpret_s32_m64(a), vreinterpret_s32_m64(b)));
3839  }
3840  FORCE_INLINE __m128i _mm_hadds_epi16(__m128i _a, __m128i _b)
3841  {
3842  #if defined(__aarch64__)
3843      int16x8_t a = vreinterpretq_s16_m128i(_a);
3844      int16x8_t b = vreinterpretq_s16_m128i(_b);
3845      return vreinterpretq_s64_s16(
3846          vqaddq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));
3847  #else
3848      int32x4_t a = vreinterpretq_s32_m128i(_a);
3849      int32x4_t b = vreinterpretq_s32_m128i(_b);
3850      int16x8_t ab0246 = vcombine_s16(vmovn_s32(a), vmovn_s32(b));
3851      int16x8_t ab1357 = vcombine_s16(vshrn_n_s32(a, 16), vshrn_n_s32(b, 16));
3852      return vreinterpretq_m128i_s16(vqaddq_s16(ab0246, ab1357));
3853  #endif
3854  }
3855  FORCE_INLINE __m64 _mm_hadds_pi16(__m64 _a, __m64 _b)
3856  {
3857      int16x4_t a = vreinterpret_s16_m64(_a);
3858      int16x4_t b = vreinterpret_s16_m64(_b);
3859  #if defined(__aarch64__)
3860      return vreinterpret_s64_s16(vqadd_s16(vuzp1_s16(a, b), vuzp2_s16(a, b)));
3861  #else
3862      int16x4x2_t res = vuzp_s16(a, b);
3863      return vreinterpret_s64_s16(vqadd_s16(res.val[0], res.val[1]));
3864  #endif
3865  }
3866  FORCE_INLINE __m128i _mm_hsub_epi16(__m128i _a, __m128i _b)
3867  {
3868      int16x8_t a = vreinterpretq_s16_m128i(_a);
3869      int16x8_t b = vreinterpretq_s16_m128i(_b);
3870  #if defined(__aarch64__)
3871      return vreinterpretq_m128i_s16(
3872          vsubq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));
3873  #else
3874      int16x8x2_t c = vuzpq_s16(a, b);
3875      return vreinterpretq_m128i_s16(vsubq_s16(c.val[0], c.val[1]));
3876  #endif
3877  }
3878  FORCE_INLINE __m128i _mm_hsub_epi32(__m128i _a, __m128i _b)
3879  {
3880      int32x4_t a = vreinterpretq_s32_m128i(_a);
3881      int32x4_t b = vreinterpretq_s32_m128i(_b);
3882  #if defined(__aarch64__)
3883      return vreinterpretq_m128i_s32(
3884          vsubq_s32(vuzp1q_s32(a, b), vuzp2q_s32(a, b)));
3885  #else
3886      int32x4x2_t c = vuzpq_s32(a, b);
3887      return vreinterpretq_m128i_s32(vsubq_s32(c.val[0], c.val[1]));
3888  #endif
3889  }
3890  FORCE_INLINE __m64 _mm_hsub_pi16(__m64 _a, __m64 _b)
3891  {
3892      int16x4_t a = vreinterpret_s16_m64(_a);
3893      int16x4_t b = vreinterpret_s16_m64(_b);
3894  #if defined(__aarch64__)
3895      return vreinterpret_m64_s16(vsub_s16(vuzp1_s16(a, b), vuzp2_s16(a, b)));
3896  #else
3897      int16x4x2_t c = vuzp_s16(a, b);
3898      return vreinterpret_m64_s16(vsub_s16(c.val[0], c.val[1]));
3899  #endif
3900  }
3901  FORCE_INLINE __m64 _mm_hsub_pi32(__m64 _a, __m64 _b)
3902  {
3903      int32x2_t a = vreinterpret_s32_m64(_a);
3904      int32x2_t b = vreinterpret_s32_m64(_b);
3905  #if defined(__aarch64__)
3906      return vreinterpret_m64_s32(vsub_s32(vuzp1_s32(a, b), vuzp2_s32(a, b)));
3907  #else
3908      int32x2x2_t c = vuzp_s32(a, b);
3909      return vreinterpret_m64_s32(vsub_s32(c.val[0], c.val[1]));
3910  #endif
3911  }
3912  FORCE_INLINE __m128i _mm_hsubs_epi16(__m128i _a, __m128i _b)
3913  {
3914      int16x8_t a = vreinterpretq_s16_m128i(_a);
3915      int16x8_t b = vreinterpretq_s16_m128i(_b);
3916  #if defined(__aarch64__)
3917      return vreinterpretq_m128i_s16(
3918          vqsubq_s16(vuzp1q_s16(a, b), vuzp2q_s16(a, b)));
3919  #else
3920      int16x8x2_t c = vuzpq_s16(a, b);
3921      return vreinterpretq_m128i_s16(vqsubq_s16(c.val[0], c.val[1]));
3922  #endif
3923  }
3924  FORCE_INLINE __m64 _mm_hsubs_pi16(__m64 _a, __m64 _b)
3925  {
3926      int16x4_t a = vreinterpret_s16_m64(_a);
3927      int16x4_t b = vreinterpret_s16_m64(_b);
3928  #if defined(__aarch64__)
3929      return vreinterpret_m64_s16(vqsub_s16(vuzp1_s16(a, b), vuzp2_s16(a, b)));
3930  #else
3931      int16x4x2_t c = vuzp_s16(a, b);
3932      return vreinterpret_m64_s16(vqsub_s16(c.val[0], c.val[1]));
3933  #endif
3934  }
3935  FORCE_INLINE __m128i _mm_maddubs_epi16(__m128i _a, __m128i _b)
3936  {
3937  #if defined(__aarch64__)
3938      uint8x16_t a = vreinterpretq_u8_m128i(_a);
3939      int8x16_t b = vreinterpretq_s8_m128i(_b);
3940      int16x8_t tl = vmulq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(a))),
3941                               vmovl_s8(vget_low_s8(b)));
3942      int16x8_t th = vmulq_s16(vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(a))),
3943                               vmovl_s8(vget_high_s8(b)));
3944      return vreinterpretq_m128i_s16(
3945          vqaddq_s16(vuzp1q_s16(tl, th), vuzp2q_s16(tl, th)));
3946  #else
3947      uint16x8_t a = vreinterpretq_u16_m128i(_a);
3948      int16x8_t b = vreinterpretq_s16_m128i(_b);
3949      int16x8_t a_odd = vreinterpretq_s16_u16(vshrq_n_u16(a, 8));
3950      int16x8_t a_even = vreinterpretq_s16_u16(vbicq_u16(a, vdupq_n_u16(0xff00)));
3951      int16x8_t b_even = vshrq_n_s16(vshlq_n_s16(b, 8), 8);
3952      int16x8_t b_odd = vshrq_n_s16(b, 8);
3953      int16x8_t prod1 = vmulq_s16(a_even, b_even);
3954      int16x8_t prod2 = vmulq_s16(a_odd, b_odd);
3955      return vreinterpretq_m128i_s16(vqaddq_s16(prod1, prod2));
3956  #endif
3957  }
3958  FORCE_INLINE __m64 _mm_maddubs_pi16(__m64 _a, __m64 _b)
3959  {
3960      uint16x4_t a = vreinterpret_u16_m64(_a);
3961      int16x4_t b = vreinterpret_s16_m64(_b);
3962      int16x4_t a_odd = vreinterpret_s16_u16(vshr_n_u16(a, 8));
3963      int16x4_t a_even = vreinterpret_s16_u16(vand_u16(a, vdup_n_u16(0xff)));
3964      int16x4_t b_even = vshr_n_s16(vshl_n_s16(b, 8), 8);
3965      int16x4_t b_odd = vshr_n_s16(b, 8);
3966      int16x4_t prod1 = vmul_s16(a_even, b_even);
3967      int16x4_t prod2 = vmul_s16(a_odd, b_odd);
3968      return vreinterpret_m64_s16(vqadd_s16(prod1, prod2));
3969  }
3970  FORCE_INLINE __m128i _mm_mulhrs_epi16(__m128i a, __m128i b)
3971  {
3972      int32x4_t mul_lo = vmull_s16(vget_low_s16(vreinterpretq_s16_m128i(a)),
3973                                   vget_low_s16(vreinterpretq_s16_m128i(b)));
3974      int32x4_t mul_hi = vmull_s16(vget_high_s16(vreinterpretq_s16_m128i(a)),
3975                                   vget_high_s16(vreinterpretq_s16_m128i(b)));
3976      int16x4_t narrow_lo = vrshrn_n_s32(mul_lo, 15);
3977      int16x4_t narrow_hi = vrshrn_n_s32(mul_hi, 15);
3978      return vreinterpretq_m128i_s16(vcombine_s16(narrow_lo, narrow_hi));
3979  }
3980  FORCE_INLINE __m64 _mm_mulhrs_pi16(__m64 a, __m64 b)
3981  {
3982      int32x4_t mul_extend =
3983          vmull_s16((vreinterpret_s16_m64(a)), (vreinterpret_s16_m64(b)));
3984      return vreinterpret_m64_s16(vrshrn_n_s32(mul_extend, 15));
3985  }
3986  FORCE_INLINE __m128i _mm_shuffle_epi8(__m128i a, __m128i b)
3987  {
3988      int8x16_t tbl = vreinterpretq_s8_m128i(a);   
3989      uint8x16_t idx = vreinterpretq_u8_m128i(b);  
3990      uint8x16_t idx_masked =
3991          vandq_u8(idx, vdupq_n_u8(0x8F));  
3992  #if defined(__aarch64__)
3993      return vreinterpretq_m128i_s8(vqtbl1q_s8(tbl, idx_masked));
3994  #elif defined(__GNUC__)
3995      int8x16_t ret;
3996      __asm__ __volatile__(
3997          "vtbl.8  %e[ret], {%e[tbl], %f[tbl]}, %e[idx]\n"
3998          "vtbl.8  %f[ret], {%e[tbl], %f[tbl]}, %f[idx]\n"
3999          : [ret] "=&w"(ret)
4000          : [tbl] "w"(tbl), [idx] "w"(idx_masked));
4001      return vreinterpretq_m128i_s8(ret);
4002  #else
4003      int8x8x2_t a_split = {vget_low_s8(tbl), vget_high_s8(tbl)};
4004      return vreinterpretq_m128i_s8(
4005          vcombine_s8(vtbl2_s8(a_split, vget_low_u8(idx_masked)),
4006                      vtbl2_s8(a_split, vget_high_u8(idx_masked))));
4007  #endif
4008  }
4009  FORCE_INLINE __m64 _mm_shuffle_pi8(__m64 a, __m64 b)
4010  {
4011      const int8x8_t controlMask =
4012          vand_s8(vreinterpret_s8_m64(b), vdup_n_s8((int8_t) (0x1 << 7 | 0x07)));
4013      int8x8_t res = vtbl1_s8(vreinterpret_s8_m64(a), controlMask);
4014      return vreinterpret_m64_s8(res);
4015  }
4016  FORCE_INLINE __m128i _mm_sign_epi16(__m128i _a, __m128i _b)
4017  {
4018      int16x8_t a = vreinterpretq_s16_m128i(_a);
4019      int16x8_t b = vreinterpretq_s16_m128i(_b);
4020      uint16x8_t ltMask = vreinterpretq_u16_s16(vshrq_n_s16(b, 15));
4021  #if defined(__aarch64__)
4022      int16x8_t zeroMask = vreinterpretq_s16_u16(vceqzq_s16(b));
4023  #else
4024      int16x8_t zeroMask = vreinterpretq_s16_u16(vceqq_s16(b, vdupq_n_s16(0)));
4025  #endif
4026      int16x8_t masked = vbslq_s16(ltMask, vnegq_s16(a), a);
4027      int16x8_t res = vbicq_s16(masked, zeroMask);
4028      return vreinterpretq_m128i_s16(res);
4029  }
4030  FORCE_INLINE __m128i _mm_sign_epi32(__m128i _a, __m128i _b)
4031  {
4032      int32x4_t a = vreinterpretq_s32_m128i(_a);
4033      int32x4_t b = vreinterpretq_s32_m128i(_b);
4034      uint32x4_t ltMask = vreinterpretq_u32_s32(vshrq_n_s32(b, 31));
4035  #if defined(__aarch64__)
4036      int32x4_t zeroMask = vreinterpretq_s32_u32(vceqzq_s32(b));
4037  #else
4038      int32x4_t zeroMask = vreinterpretq_s32_u32(vceqq_s32(b, vdupq_n_s32(0)));
4039  #endif
4040      int32x4_t masked = vbslq_s32(ltMask, vnegq_s32(a), a);
4041      int32x4_t res = vbicq_s32(masked, zeroMask);
4042      return vreinterpretq_m128i_s32(res);
4043  }
4044  FORCE_INLINE __m128i _mm_sign_epi8(__m128i _a, __m128i _b)
4045  {
4046      int8x16_t a = vreinterpretq_s8_m128i(_a);
4047      int8x16_t b = vreinterpretq_s8_m128i(_b);
4048      uint8x16_t ltMask = vreinterpretq_u8_s8(vshrq_n_s8(b, 7));
4049  #if defined(__aarch64__)
4050      int8x16_t zeroMask = vreinterpretq_s8_u8(vceqzq_s8(b));
4051  #else
4052      int8x16_t zeroMask = vreinterpretq_s8_u8(vceqq_s8(b, vdupq_n_s8(0)));
4053  #endif
4054      int8x16_t masked = vbslq_s8(ltMask, vnegq_s8(a), a);
4055      int8x16_t res = vbicq_s8(masked, zeroMask);
4056      return vreinterpretq_m128i_s8(res);
4057  }
4058  FORCE_INLINE __m64 _mm_sign_pi16(__m64 _a, __m64 _b)
4059  {
4060      int16x4_t a = vreinterpret_s16_m64(_a);
4061      int16x4_t b = vreinterpret_s16_m64(_b);
4062      uint16x4_t ltMask = vreinterpret_u16_s16(vshr_n_s16(b, 15));
4063  #if defined(__aarch64__)
4064      int16x4_t zeroMask = vreinterpret_s16_u16(vceqz_s16(b));
4065  #else
4066      int16x4_t zeroMask = vreinterpret_s16_u16(vceq_s16(b, vdup_n_s16(0)));
4067  #endif
4068      int16x4_t masked = vbsl_s16(ltMask, vneg_s16(a), a);
4069      int16x4_t res = vbic_s16(masked, zeroMask);
4070      return vreinterpret_m64_s16(res);
4071  }
4072  FORCE_INLINE __m64 _mm_sign_pi32(__m64 _a, __m64 _b)
4073  {
4074      int32x2_t a = vreinterpret_s32_m64(_a);
4075      int32x2_t b = vreinterpret_s32_m64(_b);
4076      uint32x2_t ltMask = vreinterpret_u32_s32(vshr_n_s32(b, 31));
4077  #if defined(__aarch64__)
4078      int32x2_t zeroMask = vreinterpret_s32_u32(vceqz_s32(b));
4079  #else
4080      int32x2_t zeroMask = vreinterpret_s32_u32(vceq_s32(b, vdup_n_s32(0)));
4081  #endif
4082      int32x2_t masked = vbsl_s32(ltMask, vneg_s32(a), a);
4083      int32x2_t res = vbic_s32(masked, zeroMask);
4084      return vreinterpret_m64_s32(res);
4085  }
4086  FORCE_INLINE __m64 _mm_sign_pi8(__m64 _a, __m64 _b)
4087  {
4088      int8x8_t a = vreinterpret_s8_m64(_a);
4089      int8x8_t b = vreinterpret_s8_m64(_b);
4090      uint8x8_t ltMask = vreinterpret_u8_s8(vshr_n_s8(b, 7));
4091  #if defined(__aarch64__)
4092      int8x8_t zeroMask = vreinterpret_s8_u8(vceqz_s8(b));
4093  #else
4094      int8x8_t zeroMask = vreinterpret_s8_u8(vceq_s8(b, vdup_n_s8(0)));
4095  #endif
4096      int8x8_t masked = vbsl_s8(ltMask, vneg_s8(a), a);
4097      int8x8_t res = vbic_s8(masked, zeroMask);
4098      return vreinterpret_m64_s8(res);
4099  }
4100  #define _mm_blend_epi16(a, b, imm)                                            \
4101      __extension__({                                                           \
4102          const uint16_t _mask[8] = {((imm) & (1 << 0)) ? (uint16_t) -1 : 0x0,  \
4103                                     ((imm) & (1 << 1)) ? (uint16_t) -1 : 0x0,  \
4104                                     ((imm) & (1 << 2)) ? (uint16_t) -1 : 0x0,  \
4105                                     ((imm) & (1 << 3)) ? (uint16_t) -1 : 0x0,  \
4106                                     ((imm) & (1 << 4)) ? (uint16_t) -1 : 0x0,  \
4107                                     ((imm) & (1 << 5)) ? (uint16_t) -1 : 0x0,  \
4108                                     ((imm) & (1 << 6)) ? (uint16_t) -1 : 0x0,  \
4109                                     ((imm) & (1 << 7)) ? (uint16_t) -1 : 0x0}; \
4110          uint16x8_t _mask_vec = vld1q_u16(_mask);                              \
4111          uint16x8_t _a = vreinterpretq_u16_m128i(a);                           \
4112          uint16x8_t _b = vreinterpretq_u16_m128i(b);                           \
4113          vreinterpretq_m128i_u16(vbslq_u16(_mask_vec, _b, _a));                \
4114      })
4115  #define _mm_blend_pd(a, b, imm)                                \
4116      __extension__({                                            \
4117          const uint64_t _mask[2] = {                            \
4118              ((imm) & (1 << 0)) ? ~UINT64_C(0) : UINT64_C(0),   \
4119              ((imm) & (1 << 1)) ? ~UINT64_C(0) : UINT64_C(0)};  \
4120          uint64x2_t _mask_vec = vld1q_u64(_mask);               \
4121          uint64x2_t _a = vreinterpretq_u64_m128d(a);            \
4122          uint64x2_t _b = vreinterpretq_u64_m128d(b);            \
4123          vreinterpretq_m128d_u64(vbslq_u64(_mask_vec, _b, _a)); \
4124      })
4125  FORCE_INLINE __m128 _mm_blend_ps(__m128 _a, __m128 _b, const char imm8)
4126  {
4127      const uint32_t ALIGN_STRUCT(16)
4128          data[4] = {((imm8) & (1 << 0)) ? UINT32_MAX : 0,
4129                     ((imm8) & (1 << 1)) ? UINT32_MAX : 0,
4130                     ((imm8) & (1 << 2)) ? UINT32_MAX : 0,
4131                     ((imm8) & (1 << 3)) ? UINT32_MAX : 0};
4132      uint32x4_t mask = vld1q_u32(data);
4133      float32x4_t a = vreinterpretq_f32_m128(_a);
4134      float32x4_t b = vreinterpretq_f32_m128(_b);
4135      return vreinterpretq_m128_f32(vbslq_f32(mask, b, a));
4136  }
4137  FORCE_INLINE __m128i _mm_blendv_epi8(__m128i _a, __m128i _b, __m128i _mask)
4138  {
4139      uint8x16_t mask =
4140          vreinterpretq_u8_s8(vshrq_n_s8(vreinterpretq_s8_m128i(_mask), 7));
4141      uint8x16_t a = vreinterpretq_u8_m128i(_a);
4142      uint8x16_t b = vreinterpretq_u8_m128i(_b);
4143      return vreinterpretq_m128i_u8(vbslq_u8(mask, b, a));
4144  }
4145  FORCE_INLINE __m128d _mm_blendv_pd(__m128d _a, __m128d _b, __m128d _mask)
4146  {
4147      uint64x2_t mask =
4148          vreinterpretq_u64_s64(vshrq_n_s64(vreinterpretq_s64_m128d(_mask), 63));
4149  #if defined(__aarch64__)
4150      float64x2_t a = vreinterpretq_f64_m128d(_a);
4151      float64x2_t b = vreinterpretq_f64_m128d(_b);
4152      return vreinterpretq_m128d_f64(vbslq_f64(mask, b, a));
4153  #else
4154      uint64x2_t a = vreinterpretq_u64_m128d(_a);
4155      uint64x2_t b = vreinterpretq_u64_m128d(_b);
4156      return vreinterpretq_m128d_u64(vbslq_u64(mask, b, a));
4157  #endif
4158  }
4159  FORCE_INLINE __m128 _mm_blendv_ps(__m128 _a, __m128 _b, __m128 _mask)
4160  {
4161      uint32x4_t mask =
4162          vreinterpretq_u32_s32(vshrq_n_s32(vreinterpretq_s32_m128(_mask), 31));
4163      float32x4_t a = vreinterpretq_f32_m128(_a);
4164      float32x4_t b = vreinterpretq_f32_m128(_b);
4165      return vreinterpretq_m128_f32(vbslq_f32(mask, b, a));
4166  }
4167  FORCE_INLINE __m128d _mm_ceil_pd(__m128d a)
4168  {
4169  #if defined(__aarch64__)
4170      return vreinterpretq_m128d_f64(vrndpq_f64(vreinterpretq_f64_m128d(a)));
4171  #else
4172      double *f = (double *) &a;
4173      return _mm_set_pd(ceil(f[1]), ceil(f[0]));
4174  #endif
4175  }
4176  FORCE_INLINE __m128 _mm_ceil_ps(__m128 a)
4177  {
4178  #if defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
4179      return vreinterpretq_m128_f32(vrndpq_f32(vreinterpretq_f32_m128(a)));
4180  #else
4181      float *f = (float *) &a;
4182      return _mm_set_ps(ceilf(f[3]), ceilf(f[2]), ceilf(f[1]), ceilf(f[0]));
4183  #endif
4184  }
4185  FORCE_INLINE __m128d _mm_ceil_sd(__m128d a, __m128d b)
4186  {
4187      return _mm_move_sd(a, _mm_ceil_pd(b));
4188  }
4189  FORCE_INLINE __m128 _mm_ceil_ss(__m128 a, __m128 b)
4190  {
4191      return _mm_move_ss(a, _mm_ceil_ps(b));
4192  }
4193  FORCE_INLINE __m128i _mm_cmpeq_epi64(__m128i a, __m128i b)
4194  {
4195  #if defined(__aarch64__)
4196      return vreinterpretq_m128i_u64(
4197          vceqq_u64(vreinterpretq_u64_m128i(a), vreinterpretq_u64_m128i(b)));
4198  #else
4199      uint32x4_t cmp =
4200          vceqq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b));
4201      uint32x4_t swapped = vrev64q_u32(cmp);
4202      return vreinterpretq_m128i_u32(vandq_u32(cmp, swapped));
4203  #endif
4204  }
4205  FORCE_INLINE __m128i _mm_cvtepi16_epi32(__m128i a)
4206  {
4207      return vreinterpretq_m128i_s32(
4208          vmovl_s16(vget_low_s16(vreinterpretq_s16_m128i(a))));
4209  }
4210  FORCE_INLINE __m128i _mm_cvtepi16_epi64(__m128i a)
4211  {
4212      int16x8_t s16x8 = vreinterpretq_s16_m128i(a);     &bsol;* xxxx xxxx xxxx 0B0A */
4213      int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); &bsol;* 000x 000x 000B 000A */
4214      int64x2_t s64x2 = vmovl_s32(vget_low_s32(s32x4)); &bsol;* 0000 000B 0000 000A */
4215      return vreinterpretq_m128i_s64(s64x2);
4216  }
4217  FORCE_INLINE __m128i _mm_cvtepi32_epi64(__m128i a)
4218  {
4219      return vreinterpretq_m128i_s64(
4220          vmovl_s32(vget_low_s32(vreinterpretq_s32_m128i(a))));
4221  }
4222  FORCE_INLINE __m128i _mm_cvtepi8_epi16(__m128i a)
4223  {
4224      int8x16_t s8x16 = vreinterpretq_s8_m128i(a);    &bsol;* xxxx xxxx xxxx DCBA */
4225      int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16)); &bsol;* 0x0x 0x0x 0D0C 0B0A */
4226      return vreinterpretq_m128i_s16(s16x8);
4227  }
4228  FORCE_INLINE __m128i _mm_cvtepi8_epi32(__m128i a)
4229  {
4230      int8x16_t s8x16 = vreinterpretq_s8_m128i(a);      &bsol;* xxxx xxxx xxxx DCBA */
4231      int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16));   &bsol;* 0x0x 0x0x 0D0C 0B0A */
4232      int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); &bsol;* 000D 000C 000B 000A */
4233      return vreinterpretq_m128i_s32(s32x4);
4234  }
4235  FORCE_INLINE __m128i _mm_cvtepi8_epi64(__m128i a)
4236  {
4237      int8x16_t s8x16 = vreinterpretq_s8_m128i(a);      &bsol;* xxxx xxxx xxxx xxBA */
4238      int16x8_t s16x8 = vmovl_s8(vget_low_s8(s8x16));   &bsol;* 0x0x 0x0x 0x0x 0B0A */
4239      int32x4_t s32x4 = vmovl_s16(vget_low_s16(s16x8)); &bsol;* 000x 000x 000B 000A */
4240      int64x2_t s64x2 = vmovl_s32(vget_low_s32(s32x4)); &bsol;* 0000 000B 0000 000A */
4241      return vreinterpretq_m128i_s64(s64x2);
4242  }
4243  FORCE_INLINE __m128i _mm_cvtepu16_epi32(__m128i a)
4244  {
4245      return vreinterpretq_m128i_u32(
4246          vmovl_u16(vget_low_u16(vreinterpretq_u16_m128i(a))));
4247  }
4248  FORCE_INLINE __m128i _mm_cvtepu16_epi64(__m128i a)
4249  {
4250      uint16x8_t u16x8 = vreinterpretq_u16_m128i(a);     &bsol;* xxxx xxxx xxxx 0B0A */
4251      uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); &bsol;* 000x 000x 000B 000A */
4252      uint64x2_t u64x2 = vmovl_u32(vget_low_u32(u32x4)); &bsol;* 0000 000B 0000 000A */
4253      return vreinterpretq_m128i_u64(u64x2);
4254  }
4255  FORCE_INLINE __m128i _mm_cvtepu32_epi64(__m128i a)
4256  {
4257      return vreinterpretq_m128i_u64(
4258          vmovl_u32(vget_low_u32(vreinterpretq_u32_m128i(a))));
4259  }
4260  FORCE_INLINE __m128i _mm_cvtepu8_epi16(__m128i a)
4261  {
4262      uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);    &bsol;* xxxx xxxx HGFE DCBA */
4263      uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16)); &bsol;* 0H0G 0F0E 0D0C 0B0A */
4264      return vreinterpretq_m128i_u16(u16x8);
4265  }
4266  FORCE_INLINE __m128i _mm_cvtepu8_epi32(__m128i a)
4267  {
4268      uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);      &bsol;* xxxx xxxx xxxx DCBA */
4269      uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16));   &bsol;* 0x0x 0x0x 0D0C 0B0A */
4270      uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); &bsol;* 000D 000C 000B 000A */
4271      return vreinterpretq_m128i_u32(u32x4);
4272  }
4273  FORCE_INLINE __m128i _mm_cvtepu8_epi64(__m128i a)
4274  {
4275      uint8x16_t u8x16 = vreinterpretq_u8_m128i(a);      &bsol;* xxxx xxxx xxxx xxBA */
4276      uint16x8_t u16x8 = vmovl_u8(vget_low_u8(u8x16));   &bsol;* 0x0x 0x0x 0x0x 0B0A */
4277      uint32x4_t u32x4 = vmovl_u16(vget_low_u16(u16x8)); &bsol;* 000x 000x 000B 000A */
4278      uint64x2_t u64x2 = vmovl_u32(vget_low_u32(u32x4)); &bsol;* 0000 000B 0000 000A */
4279      return vreinterpretq_m128i_u64(u64x2);
4280  }
4281  FORCE_INLINE __m128d _mm_dp_pd(__m128d a, __m128d b, const int imm)
4282  {
4283      const int64_t bit0Mask = imm & 0x01 ? UINT64_MAX : 0;
4284      const int64_t bit1Mask = imm & 0x02 ? UINT64_MAX : 0;
4285  #if !SSE2NEON_PRECISE_DP
4286      const int64_t bit4Mask = imm & 0x10 ? UINT64_MAX : 0;
4287      const int64_t bit5Mask = imm & 0x20 ? UINT64_MAX : 0;
4288  #endif
4289  #if !SSE2NEON_PRECISE_DP
4290      __m128d mul = _mm_mul_pd(a, b);
4291      const __m128d mulMask =
4292          _mm_castsi128_pd(_mm_set_epi64x(bit5Mask, bit4Mask));
4293      __m128d tmp = _mm_and_pd(mul, mulMask);
4294  #else
4295  #if defined(__aarch64__)
4296      double d0 = (imm & 0x10) ? vgetq_lane_f64(vreinterpretq_f64_m128d(a), 0) *
4297                                     vgetq_lane_f64(vreinterpretq_f64_m128d(b), 0)
4298                               : 0;
4299      double d1 = (imm & 0x20) ? vgetq_lane_f64(vreinterpretq_f64_m128d(a), 1) *
4300                                     vgetq_lane_f64(vreinterpretq_f64_m128d(b), 1)
4301                               : 0;
4302  #else
4303      double d0 = (imm & 0x10) ? ((double *) &a)[0] * ((double *) &b)[0] : 0;
4304      double d1 = (imm & 0x20) ? ((double *) &a)[1] * ((double *) &b)[1] : 0;
4305  #endif
4306      __m128d tmp = _mm_set_pd(d1, d0);
4307  #endif
4308  #if defined(__aarch64__)
4309      double sum = vpaddd_f64(vreinterpretq_f64_m128d(tmp));
4310  #else
4311      double sum = *((double *) &tmp) + *(((double *) &tmp) + 1);
4312  #endif
4313      const __m128d sumMask =
4314          _mm_castsi128_pd(_mm_set_epi64x(bit1Mask, bit0Mask));
4315      __m128d res = _mm_and_pd(_mm_set_pd1(sum), sumMask);
4316      return res;
4317  }
4318  FORCE_INLINE __m128 _mm_dp_ps(__m128 a, __m128 b, const int imm)
4319  {
4320  #if defined(__aarch64__)
4321      if (imm == 0xFF) {
4322          return _mm_set1_ps(vaddvq_f32(_mm_mul_ps(a, b)));
4323      }
4324      if (imm == 0x7F) {
4325          float32x4_t m = _mm_mul_ps(a, b);
4326          m[3] = 0;
4327          return _mm_set1_ps(vaddvq_f32(m));
4328      }
4329  #endif
4330      float s = 0, c = 0;
4331      float32x4_t f32a = vreinterpretq_f32_m128(a);
4332      float32x4_t f32b = vreinterpretq_f32_m128(b);
4333      if (imm & (1 << 4))
4334          _sse2neon_kadd_f32(&s, &c, f32a[0] * f32b[0]);
4335      if (imm & (1 << 5))
4336          _sse2neon_kadd_f32(&s, &c, f32a[1] * f32b[1]);
4337      if (imm & (1 << 6))
4338          _sse2neon_kadd_f32(&s, &c, f32a[2] * f32b[2]);
4339      if (imm & (1 << 7))
4340          _sse2neon_kadd_f32(&s, &c, f32a[3] * f32b[3]);
4341      s += c;
4342      float32x4_t res = {
4343          (imm & 0x1) ? s : 0,
4344          (imm & 0x2) ? s : 0,
4345          (imm & 0x4) ? s : 0,
4346          (imm & 0x8) ? s : 0,
4347      };
4348      return vreinterpretq_m128_f32(res);
4349  }
4350  #define _mm_extract_epi32(a, imm) \
4351      vgetq_lane_s32(vreinterpretq_s32_m128i(a), (imm))
4352  #define _mm_extract_epi64(a, imm) \
4353      vgetq_lane_s64(vreinterpretq_s64_m128i(a), (imm))
4354  #define _mm_extract_epi8(a, imm) vgetq_lane_u8(vreinterpretq_u8_m128i(a), (imm))
4355  #define _mm_extract_ps(a, imm) vgetq_lane_s32(vreinterpretq_s32_m128(a), (imm))
4356  FORCE_INLINE __m128d _mm_floor_pd(__m128d a)
4357  {
4358  #if defined(__aarch64__)
4359      return vreinterpretq_m128d_f64(vrndmq_f64(vreinterpretq_f64_m128d(a)));
4360  #else
4361      double *f = (double *) &a;
4362      return _mm_set_pd(floor(f[1]), floor(f[0]));
4363  #endif
4364  }
4365  FORCE_INLINE __m128 _mm_floor_ps(__m128 a)
4366  {
4367  #if defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
4368      return vreinterpretq_m128_f32(vrndmq_f32(vreinterpretq_f32_m128(a)));
4369  #else
4370      float *f = (float *) &a;
4371      return _mm_set_ps(floorf(f[3]), floorf(f[2]), floorf(f[1]), floorf(f[0]));
4372  #endif
4373  }
4374  FORCE_INLINE __m128d _mm_floor_sd(__m128d a, __m128d b)
4375  {
4376      return _mm_move_sd(a, _mm_floor_pd(b));
4377  }
4378  FORCE_INLINE __m128 _mm_floor_ss(__m128 a, __m128 b)
4379  {
4380      return _mm_move_ss(a, _mm_floor_ps(b));
4381  }
4382  #define _mm_insert_epi32(a, b, imm)                                  \
4383      __extension__({                                                  \
4384          vreinterpretq_m128i_s32(                                     \
4385              vsetq_lane_s32((b), vreinterpretq_s32_m128i(a), (imm))); \
4386      })
4387  #define _mm_insert_epi64(a, b, imm)                                  \
4388      __extension__({                                                  \
4389          vreinterpretq_m128i_s64(                                     \
4390              vsetq_lane_s64((b), vreinterpretq_s64_m128i(a), (imm))); \
4391      })
4392  #define _mm_insert_epi8(a, b, imm)                                 \
4393      __extension__({                                                \
4394          vreinterpretq_m128i_s8(                                    \
4395              vsetq_lane_s8((b), vreinterpretq_s8_m128i(a), (imm))); \
4396      })
4397  #define _mm_insert_ps(a, b, imm8)                                              \
4398      __extension__({                                                            \
4399          float32x4_t tmp1 =                                                     \
4400              vsetq_lane_f32(vgetq_lane_f32(b, (imm8 >> 6) & 0x3),               \
4401                             vreinterpretq_f32_m128(a), 0);                      \
4402          float32x4_t tmp2 =                                                     \
4403              vsetq_lane_f32(vgetq_lane_f32(tmp1, 0), vreinterpretq_f32_m128(a), \
4404                             ((imm8 >> 4) & 0x3));                               \
4405          const uint32_t data[4] = {((imm8) & (1 << 0)) ? UINT32_MAX : 0,        \
4406                                    ((imm8) & (1 << 1)) ? UINT32_MAX : 0,        \
4407                                    ((imm8) & (1 << 2)) ? UINT32_MAX : 0,        \
4408                                    ((imm8) & (1 << 3)) ? UINT32_MAX : 0};       \
4409          uint32x4_t mask = vld1q_u32(data);                                     \
4410          float32x4_t all_zeros = vdupq_n_f32(0);                                \
4411                                                                                 \
4412          vreinterpretq_m128_f32(                                                \
4413              vbslq_f32(mask, all_zeros, vreinterpretq_f32_m128(tmp2)));         \
4414      })
4415  FORCE_INLINE __m128i _mm_max_epi32(__m128i a, __m128i b)
4416  {
4417      return vreinterpretq_m128i_s32(
4418          vmaxq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
4419  }
4420  FORCE_INLINE __m128i _mm_max_epi8(__m128i a, __m128i b)
4421  {
4422      return vreinterpretq_m128i_s8(
4423          vmaxq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
4424  }
4425  FORCE_INLINE __m128i _mm_max_epu16(__m128i a, __m128i b)
4426  {
4427      return vreinterpretq_m128i_u16(
4428          vmaxq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
4429  }
4430  FORCE_INLINE __m128i _mm_max_epu32(__m128i a, __m128i b)
4431  {
4432      return vreinterpretq_m128i_u32(
4433          vmaxq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b)));
4434  }
4435  FORCE_INLINE __m128i _mm_min_epi32(__m128i a, __m128i b)
4436  {
4437      return vreinterpretq_m128i_s32(
4438          vminq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
4439  }
4440  FORCE_INLINE __m128i _mm_min_epi8(__m128i a, __m128i b)
4441  {
4442      return vreinterpretq_m128i_s8(
4443          vminq_s8(vreinterpretq_s8_m128i(a), vreinterpretq_s8_m128i(b)));
4444  }
4445  FORCE_INLINE __m128i _mm_min_epu16(__m128i a, __m128i b)
4446  {
4447      return vreinterpretq_m128i_u16(
4448          vminq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b)));
4449  }
4450  FORCE_INLINE __m128i _mm_min_epu32(__m128i a, __m128i b)
4451  {
4452      return vreinterpretq_m128i_u32(
4453          vminq_u32(vreinterpretq_u32_m128i(a), vreinterpretq_u32_m128i(b)));
4454  }
4455  FORCE_INLINE __m128i _mm_minpos_epu16(__m128i a)
4456  {
4457      __m128i dst;
4458      uint16_t min, idx = 0;
4459  #if defined(__aarch64__)
4460      min = vminvq_u16(vreinterpretq_u16_m128i(a));
4461      static const uint16_t idxv[] = {0, 1, 2, 3, 4, 5, 6, 7};
4462      uint16x8_t minv = vdupq_n_u16(min);
4463      uint16x8_t cmeq = vceqq_u16(minv, vreinterpretq_u16_m128i(a));
4464      idx = vminvq_u16(vornq_u16(vld1q_u16(idxv), cmeq));
4465  #else
4466      __m64 tmp;
4467      tmp = vreinterpret_m64_u16(
4468          vmin_u16(vget_low_u16(vreinterpretq_u16_m128i(a)),
4469                   vget_high_u16(vreinterpretq_u16_m128i(a))));
4470      tmp = vreinterpret_m64_u16(
4471          vpmin_u16(vreinterpret_u16_m64(tmp), vreinterpret_u16_m64(tmp)));
4472      tmp = vreinterpret_m64_u16(
4473          vpmin_u16(vreinterpret_u16_m64(tmp), vreinterpret_u16_m64(tmp)));
4474      min = vget_lane_u16(vreinterpret_u16_m64(tmp), 0);
4475      int i;
4476      for (i = 0; i < 8; i++) {
4477          if (min == vgetq_lane_u16(vreinterpretq_u16_m128i(a), 0)) {
4478              idx = (uint16_t) i;
4479              break;
4480          }
4481          a = _mm_srli_si128(a, 2);
4482      }
4483  #endif
4484      dst = _mm_setzero_si128();
4485      dst = vreinterpretq_m128i_u16(
4486          vsetq_lane_u16(min, vreinterpretq_u16_m128i(dst), 0));
4487      dst = vreinterpretq_m128i_u16(
4488          vsetq_lane_u16(idx, vreinterpretq_u16_m128i(dst), 1));
4489      return dst;
4490  }
4491  FORCE_INLINE __m128i _mm_mpsadbw_epu8(__m128i a, __m128i b, const int imm)
4492  {
4493      uint8x16_t _a, _b;
4494      switch (imm & 0x4) {
4495      case 0:
4496          _a = vreinterpretq_u8_m128i(a);
4497          break;
4498      case 4:
4499          _a = vreinterpretq_u8_u32(vextq_u32(vreinterpretq_u32_m128i(a),
4500                                              vreinterpretq_u32_m128i(a), 1));
4501          break;
4502      default:
4503  #if defined(__GNUC__) || defined(__clang__)
4504          __builtin_unreachable();
4505  #endif
4506          break;
4507      }
4508      switch (imm & 0x3) {
4509      case 0:
4510          _b = vreinterpretq_u8_u32(
4511              vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 0)));
4512          break;
4513      case 1:
4514          _b = vreinterpretq_u8_u32(
4515              vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 1)));
4516          break;
4517      case 2:
4518          _b = vreinterpretq_u8_u32(
4519              vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 2)));
4520          break;
4521      case 3:
4522          _b = vreinterpretq_u8_u32(
4523              vdupq_n_u32(vgetq_lane_u32(vreinterpretq_u32_m128i(b), 3)));
4524          break;
4525      default:
4526  #if defined(__GNUC__) || defined(__clang__)
4527          __builtin_unreachable();
4528  #endif
4529          break;
4530      }
4531      int16x8_t c04, c15, c26, c37;
4532      uint8x8_t low_b = vget_low_u8(_b);
4533      c04 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a), low_b));
4534      uint8x16_t _a_1 = vextq_u8(_a, _a, 1);
4535      c15 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a_1), low_b));
4536      uint8x16_t _a_2 = vextq_u8(_a, _a, 2);
4537      c26 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a_2), low_b));
4538      uint8x16_t _a_3 = vextq_u8(_a, _a, 3);
4539      c37 = vreinterpretq_s16_u16(vabdl_u8(vget_low_u8(_a_3), low_b));
4540  #if defined(__aarch64__)
4541      c04 = vpaddq_s16(c04, c26);
4542      c15 = vpaddq_s16(c15, c37);
4543      int32x4_t trn1_c =
4544          vtrn1q_s32(vreinterpretq_s32_s16(c04), vreinterpretq_s32_s16(c15));
4545      int32x4_t trn2_c =
4546          vtrn2q_s32(vreinterpretq_s32_s16(c04), vreinterpretq_s32_s16(c15));
4547      return vreinterpretq_m128i_s16(vpaddq_s16(vreinterpretq_s16_s32(trn1_c),
4548                                                vreinterpretq_s16_s32(trn2_c)));
4549  #else
4550      int16x4_t c01, c23, c45, c67;
4551      c01 = vpadd_s16(vget_low_s16(c04), vget_low_s16(c15));
4552      c23 = vpadd_s16(vget_low_s16(c26), vget_low_s16(c37));
4553      c45 = vpadd_s16(vget_high_s16(c04), vget_high_s16(c15));
4554      c67 = vpadd_s16(vget_high_s16(c26), vget_high_s16(c37));
4555      return vreinterpretq_m128i_s16(
4556          vcombine_s16(vpadd_s16(c01, c23), vpadd_s16(c45, c67)));
4557  #endif
4558  }
4559  FORCE_INLINE __m128i _mm_mul_epi32(__m128i a, __m128i b)
4560  {
4561      int32x2_t a_lo = vmovn_s64(vreinterpretq_s64_m128i(a));
4562      int32x2_t b_lo = vmovn_s64(vreinterpretq_s64_m128i(b));
4563      return vreinterpretq_m128i_s64(vmull_s32(a_lo, b_lo));
4564  }
4565  FORCE_INLINE __m128i _mm_mullo_epi32(__m128i a, __m128i b)
4566  {
4567      return vreinterpretq_m128i_s32(
4568          vmulq_s32(vreinterpretq_s32_m128i(a), vreinterpretq_s32_m128i(b)));
4569  }
4570  FORCE_INLINE __m128i _mm_packus_epi32(__m128i a, __m128i b)
4571  {
4572      return vreinterpretq_m128i_u16(
4573          vcombine_u16(vqmovun_s32(vreinterpretq_s32_m128i(a)),
4574                       vqmovun_s32(vreinterpretq_s32_m128i(b))));
4575  }
4576  FORCE_INLINE __m128d _mm_round_pd(__m128d a, int rounding)
4577  {
4578  #if defined(__aarch64__)
4579      switch (rounding) {
4580      case (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC):
4581          return vreinterpretq_m128d_f64(vrndnq_f64(vreinterpretq_f64_m128d(a)));
4582      case (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC):
4583          return _mm_floor_pd(a);
4584      case (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC):
4585          return _mm_ceil_pd(a);
4586      case (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC):
4587          return vreinterpretq_m128d_f64(vrndq_f64(vreinterpretq_f64_m128d(a)));
4588      default:  
4589          return vreinterpretq_m128d_f64(vrndiq_f64(vreinterpretq_f64_m128d(a)));
4590      }
4591  #else
4592      double *v_double = (double *) &a;
4593      if (rounding == (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) ||
4594          (rounding == _MM_FROUND_CUR_DIRECTION &&
4595           _MM_GET_ROUNDING_MODE() == _MM_ROUND_NEAREST)) {
4596          double res[2], tmp;
4597          for (int i = 0; i < 2; i++) {
4598              tmp = (v_double[i] < 0) ? -v_double[i] : v_double[i];
4599              double roundDown = floor(tmp);  
4600              double roundUp = ceil(tmp);     
4601              double diffDown = tmp - roundDown;
4602              double diffUp = roundUp - tmp;
4603              if (diffDown < diffUp) {
4604                  res[i] = roundDown;
4605              } else if (diffDown > diffUp) {
4606                  res[i] = roundUp;
4607              } else {
4608                  double half = roundDown / 2;
4609                  if (half != floor(half)) {
4610                      res[i] = roundUp;
4611                  } else {
4612                      res[i] = roundDown;
4613                  }
4614              }
4615              res[i] = (v_double[i] < 0) ? -res[i] : res[i];
4616          }
4617          return _mm_set_pd(res[1], res[0]);
4618      } else if (rounding == (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC) ||
4619                 (rounding == _MM_FROUND_CUR_DIRECTION &&
4620                  _MM_GET_ROUNDING_MODE() == _MM_ROUND_DOWN)) {
4621          return _mm_floor_pd(a);
4622      } else if (rounding == (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC) ||
4623                 (rounding == _MM_FROUND_CUR_DIRECTION &&
4624                  _MM_GET_ROUNDING_MODE() == _MM_ROUND_UP)) {
4625          return _mm_ceil_pd(a);
4626      }
4627      return _mm_set_pd(v_double[1] > 0 ? floor(v_double[1]) : ceil(v_double[1]),
4628                        v_double[0] > 0 ? floor(v_double[0]) : ceil(v_double[0]));
4629  #endif
4630  }
4631  FORCE_INLINE __m128 _mm_round_ps(__m128 a, int rounding)
4632  {
4633  #if defined(__aarch64__) || defined(__ARM_FEATURE_DIRECTED_ROUNDING)
4634      switch (rounding) {
4635      case (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC):
4636          return vreinterpretq_m128_f32(vrndnq_f32(vreinterpretq_f32_m128(a)));
4637      case (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC):
4638          return _mm_floor_ps(a);
4639      case (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC):
4640          return _mm_ceil_ps(a);
4641      case (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC):
4642          return vreinterpretq_m128_f32(vrndq_f32(vreinterpretq_f32_m128(a)));
4643      default:  
4644          return vreinterpretq_m128_f32(vrndiq_f32(vreinterpretq_f32_m128(a)));
4645      }
4646  #else
4647      float *v_float = (float *) &a;
<span onclick='openModal()' class='match'>4648      if (rounding == (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) ||
4649          (rounding == _MM_FROUND_CUR_DIRECTION &&
4650           _MM_GET_ROUNDING_MODE() == _MM_ROUND_NEAREST)) {
4651          uint32x4_t signmask = vdupq_n_u32(0x80000000);
4652          float32x4_t half = vbslq_f32(signmask, vreinterpretq_f32_m128(a),
4653                                       vdupq_n_f32(0.5f)); &bsol;* +/- 0.5 */
4654          int32x4_t r_normal = vcvtq_s32_f32(vaddq_f32(
4655              vreinterpretq_f32_m128(a), half)); &bsol;* round to integer: [a + 0.5]*/
4656          int32x4_t r_trunc = vcvtq_s32_f32(
4657              vreinterpretq_f32_m128(a)); &bsol;* truncate to integer: [a] */
4658          int32x4_t plusone = vreinterpretq_s32_u32(vshrq_n_u32(
4659              vreinterpretq_u32_s32(vnegq_s32(r_trunc)), 31)); &bsol;* 1 or 0 */
4660          int32x4_t r_even = vbicq_s32(vaddq_s32(r_trunc, plusone),
</span>4661                                       vdupq_n_s32(1)); &bsol;* ([a] + {0,1}) & ~1 */
4662          float32x4_t delta = vsubq_f32(
4663              vreinterpretq_f32_m128(a),
4664              vcvtq_f32_s32(r_trunc)); &bsol;* compute delta: delta = (a - [a]) */
4665          uint32x4_t is_delta_half =
4666              vceqq_f32(delta, half); &bsol;* delta == +/- 0.5 */
4667          return vreinterpretq_m128_f32(
4668              vcvtq_f32_s32(vbslq_s32(is_delta_half, r_even, r_normal)));
4669      } else if (rounding == (_MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC) ||
4670                 (rounding == _MM_FROUND_CUR_DIRECTION &&
4671                  _MM_GET_ROUNDING_MODE() == _MM_ROUND_DOWN)) {
4672          return _mm_floor_ps(a);
4673      } else if (rounding == (_MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC) ||
4674                 (rounding == _MM_FROUND_CUR_DIRECTION &&
4675                  _MM_GET_ROUNDING_MODE() == _MM_ROUND_UP)) {
4676          return _mm_ceil_ps(a);
4677      }
4678      return _mm_set_ps(v_float[3] > 0 ? floorf(v_float[3]) : ceilf(v_float[3]),
4679                        v_float[2] > 0 ? floorf(v_float[2]) : ceilf(v_float[2]),
4680                        v_float[1] > 0 ? floorf(v_float[1]) : ceilf(v_float[1]),
4681                        v_float[0] > 0 ? floorf(v_float[0]) : ceilf(v_float[0]));
4682  #endif
4683  }
4684  FORCE_INLINE __m128d _mm_round_sd(__m128d a, __m128d b, int rounding)
4685  {
4686      return _mm_move_sd(a, _mm_round_pd(b, rounding));
4687  }
4688  FORCE_INLINE __m128 _mm_round_ss(__m128 a, __m128 b, int rounding)
4689  {
4690      return _mm_move_ss(a, _mm_round_ps(b, rounding));
4691  }
4692  FORCE_INLINE __m128i _mm_stream_load_si128(__m128i *p)
4693  {
4694  #if __has_builtin(__builtin_nontemporal_store)
4695      return __builtin_nontemporal_load(p);
4696  #else
4697      return vreinterpretq_m128i_s64(vld1q_s64((int64_t *) p));
4698  #endif
4699  }
4700  FORCE_INLINE int _mm_test_all_ones(__m128i a)
4701  {
4702      return (uint64_t) (vgetq_lane_s64(a, 0) & vgetq_lane_s64(a, 1)) ==
4703             ~(uint64_t) 0;
4704  }
4705  FORCE_INLINE int _mm_test_all_zeros(__m128i a, __m128i mask)
4706  {
4707      int64x2_t a_and_mask =
4708          vandq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(mask));
4709      return !(vgetq_lane_s64(a_and_mask, 0) | vgetq_lane_s64(a_and_mask, 1));
4710  }
4711  FORCE_INLINE int _mm_test_mix_ones_zeros(__m128i a, __m128i mask)
4712  {
4713      uint64x2_t zf =
4714          vandq_u64(vreinterpretq_u64_m128i(mask), vreinterpretq_u64_m128i(a));
4715      uint64x2_t cf =
4716          vbicq_u64(vreinterpretq_u64_m128i(mask), vreinterpretq_u64_m128i(a));
4717      uint64x2_t result = vandq_u64(zf, cf);
4718      return !(vgetq_lane_u64(result, 0) | vgetq_lane_u64(result, 1));
4719  }
4720  FORCE_INLINE int _mm_testc_si128(__m128i a, __m128i b)
4721  {
4722      int64x2_t s64 =
4723          vbicq_s64(vreinterpretq_s64_m128i(b), vreinterpretq_s64_m128i(a));
4724      return !(vgetq_lane_s64(s64, 0) | vgetq_lane_s64(s64, 1));
4725  }
4726  #define _mm_testnzc_si128(a, b) _mm_test_mix_ones_zeros(a, b)
4727  FORCE_INLINE int _mm_testz_si128(__m128i a, __m128i b)
4728  {
4729      int64x2_t s64 =
4730          vandq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b));
4731      return !(vgetq_lane_s64(s64, 0) | vgetq_lane_s64(s64, 1));
4732  }
4733  const static uint16_t _sse2neon_cmpestr_mask16b[8] ALIGN_STRUCT(16) = {
4734      0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
4735  };
4736  const static uint8_t _sse2neon_cmpestr_mask8b[16] ALIGN_STRUCT(16) = {
4737      0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
4738      0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
4739  };
4740  #define _SIDD_UBYTE_OPS 0x00 &bsol;* unsigned 8-bit characters */
4741  #define _SIDD_UWORD_OPS 0x01 &bsol;* unsigned 16-bit characters */
4742  #define _SIDD_SBYTE_OPS 0x02 &bsol;* signed 8-bit characters */
4743  #define _SIDD_SWORD_OPS 0x03 &bsol;* signed 16-bit characters */
4744  #define _SIDD_CMP_EQUAL_ANY 0x00     &bsol;* compare equal any: strchr */
4745  #define _SIDD_CMP_RANGES 0x04        &bsol;* compare ranges */
4746  #define _SIDD_CMP_EQUAL_EACH 0x08    &bsol;* compare equal each: strcmp */
4747  #define _SIDD_CMP_EQUAL_ORDERED 0x0C &bsol;* compare equal ordered */
4748  #define _SIDD_POSITIVE_POLARITY 0x00
4749  #define _SIDD_MASKED_POSITIVE_POLARITY 0x20
4750  #define _SIDD_NEGATIVE_POLARITY 0x10 &bsol;* negate results */
4751  #define _SIDD_MASKED_NEGATIVE_POLARITY \
4752      0x30 &bsol;* negate results only before end of string */
4753  #define _SIDD_LEAST_SIGNIFICANT 0x00
4754  #define _SIDD_MOST_SIGNIFICANT 0x40
4755  #define _SIDD_BIT_MASK 0x00
4756  #define _SIDD_UNIT_MASK 0x40
4757  #define SSE2NEON_PRIMITIVE_CAT(a, ...) a##__VA_ARGS__
4758  #define SSE2NEON_CAT(a, b) SSE2NEON_PRIMITIVE_CAT(a, b)
4759  #define SSE2NEON_IIF(c) SSE2NEON_PRIMITIVE_CAT(SSE2NEON_IIF_, c)
4760  #define SSE2NEON_IIF_0(t, ...) __VA_ARGS__
4761  #define SSE2NEON_IIF_1(t, ...) t
4762  #define SSE2NEON_COMPL(b) SSE2NEON_PRIMITIVE_CAT(SSE2NEON_COMPL_, b)
4763  #define SSE2NEON_COMPL_0 1
4764  #define SSE2NEON_COMPL_1 0
4765  #define SSE2NEON_DEC(x) SSE2NEON_PRIMITIVE_CAT(SSE2NEON_DEC_, x)
4766  #define SSE2NEON_DEC_1 0
4767  #define SSE2NEON_DEC_2 1
4768  #define SSE2NEON_DEC_3 2
4769  #define SSE2NEON_DEC_4 3
4770  #define SSE2NEON_DEC_5 4
4771  #define SSE2NEON_DEC_6 5
4772  #define SSE2NEON_DEC_7 6
4773  #define SSE2NEON_DEC_8 7
4774  #define SSE2NEON_DEC_9 8
4775  #define SSE2NEON_DEC_10 9
4776  #define SSE2NEON_DEC_11 10
4777  #define SSE2NEON_DEC_12 11
4778  #define SSE2NEON_DEC_13 12
4779  #define SSE2NEON_DEC_14 13
4780  #define SSE2NEON_DEC_15 14
4781  #define SSE2NEON_DEC_16 15
4782  #define SSE2NEON_CHECK_N(x, n, ...) n
4783  #define SSE2NEON_CHECK(...) SSE2NEON_CHECK_N(__VA_ARGS__, 0, )
4784  #define SSE2NEON_PROBE(x) x, 1,
4785  #define SSE2NEON_NOT(x) SSE2NEON_CHECK(SSE2NEON_PRIMITIVE_CAT(SSE2NEON_NOT_, x))
4786  #define SSE2NEON_NOT_0 SSE2NEON_PROBE(~)
4787  #define SSE2NEON_BOOL(x) SSE2NEON_COMPL(SSE2NEON_NOT(x))
4788  #define SSE2NEON_IF(c) SSE2NEON_IIF(SSE2NEON_BOOL(c))
4789  #define SSE2NEON_EAT(...)
4790  #define SSE2NEON_EXPAND(...) __VA_ARGS__
4791  #define SSE2NEON_WHEN(c) SSE2NEON_IF(c)(SSE2NEON_EXPAND, SSE2NEON_EAT)
4792  #define SSE2NEON_EMPTY()
4793  #define SSE2NEON_DEFER(id) id SSE2NEON_EMPTY()
4794  #define SSE2NEON_OBSTRUCT(...) __VA_ARGS__ SSE2NEON_DEFER(SSE2NEON_EMPTY)()
4795  #define SSE2NEON_EXPAND(...) __VA_ARGS__
4796  #define SSE2NEON_EVAL(...) \
4797      SSE2NEON_EVAL1(SSE2NEON_EVAL1(SSE2NEON_EVAL1(__VA_ARGS__)))
4798  #define SSE2NEON_EVAL1(...) \
4799      SSE2NEON_EVAL2(SSE2NEON_EVAL2(SSE2NEON_EVAL2(__VA_ARGS__)))
4800  #define SSE2NEON_EVAL2(...) \
4801      SSE2NEON_EVAL3(SSE2NEON_EVAL3(SSE2NEON_EVAL3(__VA_ARGS__)))
4802  #define SSE2NEON_EVAL3(...) __VA_ARGS__
4803  #define SSE2NEON_REPEAT(count, macro, ...)                         \
4804      SSE2NEON_WHEN(count)                                           \
4805      (SSE2NEON_OBSTRUCT(SSE2NEON_REPEAT_INDIRECT)()(                \
4806          SSE2NEON_DEC(count), macro,                                \
4807          __VA_ARGS__) SSE2NEON_OBSTRUCT(macro)(SSE2NEON_DEC(count), \
4808                                                __VA_ARGS__))
4809  #define SSE2NEON_REPEAT_INDIRECT() SSE2NEON_REPEAT
4810  #define SSE2NEON_SIZE_OF_byte 8
4811  #define SSE2NEON_NUMBER_OF_LANES_byte 16
4812  #define SSE2NEON_SIZE_OF_word 16
4813  #define SSE2NEON_NUMBER_OF_LANES_word 8
4814  #define SSE2NEON_COMPARE_EQUAL_THEN_FILL_LANE(i, type)                         \
4815      mtx[i] = vreinterpretq_m128i_##type(vceqq_##type(                          \
4816          vdupq_n_##type(vgetq_lane_##type(vreinterpretq_##type##_m128i(b), i)), \
4817          vreinterpretq_##type##_m128i(a)));
4818  #define SSE2NEON_FILL_LANE(i, type) \
4819      vec_b[i] =                      \
4820          vdupq_n_##type(vgetq_lane_##type(vreinterpretq_##type##_m128i(b), i));
4821  #define PCMPSTR_RANGES(a, b, mtx, data_type_prefix, type_prefix, size,        \
4822                         number_of_lanes, byte_or_word)                         \
4823      do {                                                                      \
4824          SSE2NEON_CAT(                                                         \
4825              data_type_prefix,                                                 \
4826              SSE2NEON_CAT(size,                                                \
4827                           SSE2NEON_CAT(x, SSE2NEON_CAT(number_of_lanes, _t)))) \
4828          vec_b[number_of_lanes];                                               \
4829          __m128i mask = SSE2NEON_IIF(byte_or_word)(                            \
4830              vreinterpretq_m128i_u16(vdupq_n_u16(0xff)),                       \
4831              vreinterpretq_m128i_u32(vdupq_n_u32(0xffff)));                    \
4832          SSE2NEON_EVAL(SSE2NEON_REPEAT(number_of_lanes, SSE2NEON_FILL_LANE,    \
4833                                        SSE2NEON_CAT(type_prefix, size)))       \
4834          for (int i = 0; i < number_of_lanes; i++) {                           \
4835              mtx[i] = SSE2NEON_CAT(vreinterpretq_m128i_u,                      \
4836                                    size)(SSE2NEON_CAT(vbslq_u, size)(          \
4837                  SSE2NEON_CAT(vreinterpretq_u,                                 \
4838                               SSE2NEON_CAT(size, _m128i))(mask),               \
4839                  SSE2NEON_CAT(vcgeq_, SSE2NEON_CAT(type_prefix, size))(        \
4840                      vec_b[i],                                                 \
4841                      SSE2NEON_CAT(                                             \
4842                          vreinterpretq_,                                       \
4843                          SSE2NEON_CAT(type_prefix,                             \
4844                                       SSE2NEON_CAT(size, _m128i(a))))),        \
4845                  SSE2NEON_CAT(vcleq_, SSE2NEON_CAT(type_prefix, size))(        \
4846                      vec_b[i],                                                 \
4847                      SSE2NEON_CAT(                                             \
4848                          vreinterpretq_,                                       \
4849                          SSE2NEON_CAT(type_prefix,                             \
4850                                       SSE2NEON_CAT(size, _m128i(a)))))));      \
4851          }                                                                     \
4852      } while (0)
4853  #define PCMPSTR_EQ(a, b, mtx, size, number_of_lanes)                         \
4854      do {                                                                     \
4855          SSE2NEON_EVAL(SSE2NEON_REPEAT(number_of_lanes,                       \
4856                                        SSE2NEON_COMPARE_EQUAL_THEN_FILL_LANE, \
4857                                        SSE2NEON_CAT(u, size)))                \
4858      } while (0)
4859  #define SSE2NEON_CMP_EQUAL_ANY_IMPL(type)                                     \
4860      static int _sse2neon_cmp_##type##_equal_any(__m128i a, int la, __m128i b, \
4861                                                  int lb)                       \
4862      {                                                                         \
4863          __m128i mtx[16];                                                      \
4864          PCMPSTR_EQ(a, b, mtx, SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),          \
4865                     SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type));            \
4866          return SSE2NEON_CAT(                                                  \
4867              _sse2neon_aggregate_equal_any_,                                   \
4868              SSE2NEON_CAT(                                                     \
4869                  SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),                        \
4870                  SSE2NEON_CAT(x, SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_,       \
4871                                               type))))(la, lb, mtx);           \
4872      }
4873  #define SSE2NEON_CMP_RANGES_IMPL(type, data_type, us, byte_or_word)            \
4874      static int _sse2neon_cmp_##us##type##_ranges(__m128i a, int la, __m128i b, \
4875                                                   int lb)                       \
4876      {                                                                          \
4877          __m128i mtx[16];                                                       \
4878          PCMPSTR_RANGES(                                                        \
4879              a, b, mtx, data_type, us, SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),   \
4880              SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type), byte_or_word);      \
4881          return SSE2NEON_CAT(                                                   \
4882              _sse2neon_aggregate_ranges_,                                       \
4883              SSE2NEON_CAT(                                                      \
4884                  SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),                         \
4885                  SSE2NEON_CAT(x, SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_,        \
4886                                               type))))(la, lb, mtx);            \
4887      }
4888  #define SSE2NEON_CMP_EQUAL_ORDERED_IMPL(type)                                  \
4889      static int _sse2neon_cmp_##type##_equal_ordered(__m128i a, int la,         \
4890                                                      __m128i b, int lb)         \
4891      {                                                                          \
4892          __m128i mtx[16];                                                       \
4893          PCMPSTR_EQ(a, b, mtx, SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),           \
4894                     SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type));             \
4895          return SSE2NEON_CAT(                                                   \
4896              _sse2neon_aggregate_equal_ordered_,                                \
4897              SSE2NEON_CAT(                                                      \
4898                  SSE2NEON_CAT(SSE2NEON_SIZE_OF_, type),                         \
4899                  SSE2NEON_CAT(x,                                                \
4900                               SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type))))( \
4901              SSE2NEON_CAT(SSE2NEON_NUMBER_OF_LANES_, type), la, lb, mtx);       \
4902      }
4903  static int _sse2neon_aggregate_equal_any_8x16(int la, int lb, __m128i mtx[16])
4904  {
4905      int res = 0;
4906      int m = (1 << la) - 1;
4907      uint8x8_t vec_mask = vld1_u8(_sse2neon_cmpestr_mask8b);
4908      uint8x8_t t_lo = vtst_u8(vdup_n_u8(m & 0xff), vec_mask);
4909      uint8x8_t t_hi = vtst_u8(vdup_n_u8(m >> 8), vec_mask);
4910      uint8x16_t vec = vcombine_u8(t_lo, t_hi);
4911      for (int j = 0; j < lb; j++) {
4912          mtx[j] = vreinterpretq_m128i_u8(
4913              vandq_u8(vec, vreinterpretq_u8_m128i(mtx[j])));
4914          mtx[j] = vreinterpretq_m128i_u8(
4915              vshrq_n_u8(vreinterpretq_u8_m128i(mtx[j]), 7));
4916          int tmp = _sse2neon_vaddvq_u8(vreinterpretq_u8_m128i(mtx[j])) ? 1 : 0;
4917          res |= (tmp << j);
4918      }
4919      return res;
4920  }
4921  static int _sse2neon_aggregate_equal_any_16x8(int la, int lb, __m128i mtx[16])
4922  {
4923      int res = 0;
4924      int m = (1 << la) - 1;
4925      uint16x8_t vec =
4926          vtstq_u16(vdupq_n_u16(m), vld1q_u16(_sse2neon_cmpestr_mask16b));
4927      for (int j = 0; j < lb; j++) {
4928          mtx[j] = vreinterpretq_m128i_u16(
4929              vandq_u16(vec, vreinterpretq_u16_m128i(mtx[j])));
4930          mtx[j] = vreinterpretq_m128i_u16(
4931              vshrq_n_u16(vreinterpretq_u16_m128i(mtx[j]), 15));
4932          int tmp = _sse2neon_vaddvq_u16(vreinterpretq_u16_m128i(mtx[j])) ? 1 : 0;
4933          res |= (tmp << j);
4934      }
4935      return res;
4936  }
4937  #define SSE2NEON_GENERATE_CMP_EQUAL_ANY(prefix) \
4938      prefix##IMPL(byte) \
4939      prefix##IMPL(word)
4940  SSE2NEON_GENERATE_CMP_EQUAL_ANY(SSE2NEON_CMP_EQUAL_ANY_)
4941  static int _sse2neon_aggregate_ranges_16x8(int la, int lb, __m128i mtx[16])
4942  {
4943      int res = 0;
4944      int m = (1 << la) - 1;
4945      uint16x8_t vec =
4946          vtstq_u16(vdupq_n_u16(m), vld1q_u16(_sse2neon_cmpestr_mask16b));
4947      for (int j = 0; j < lb; j++) {
4948          mtx[j] = vreinterpretq_m128i_u16(
4949              vandq_u16(vec, vreinterpretq_u16_m128i(mtx[j])));
4950          mtx[j] = vreinterpretq_m128i_u16(
4951              vshrq_n_u16(vreinterpretq_u16_m128i(mtx[j]), 15));
4952          __m128i tmp = vreinterpretq_m128i_u32(
4953              vshrq_n_u32(vreinterpretq_u32_m128i(mtx[j]), 16));
4954          uint32x4_t vec_res = vandq_u32(vreinterpretq_u32_m128i(mtx[j]),
4955                                         vreinterpretq_u32_m128i(tmp));
4956  #if defined(__aarch64__)
4957          int t = vaddvq_u32(vec_res) ? 1 : 0;
4958  #else
4959          uint64x2_t sumh = vpaddlq_u32(vec_res);
4960          int t = vgetq_lane_u64(sumh, 0) + vgetq_lane_u64(sumh, 1);
4961  #endif
4962          res |= (t << j);
4963      }
4964      return res;
4965  }
4966  static int _sse2neon_aggregate_ranges_8x16(int la, int lb, __m128i mtx[16])
4967  {
4968      int res = 0;
4969      int m = (1 << la) - 1;
4970      uint8x8_t vec_mask = vld1_u8(_sse2neon_cmpestr_mask8b);
4971      uint8x8_t t_lo = vtst_u8(vdup_n_u8(m & 0xff), vec_mask);
4972      uint8x8_t t_hi = vtst_u8(vdup_n_u8(m >> 8), vec_mask);
4973      uint8x16_t vec = vcombine_u8(t_lo, t_hi);
4974      for (int j = 0; j < lb; j++) {
4975          mtx[j] = vreinterpretq_m128i_u8(
4976              vandq_u8(vec, vreinterpretq_u8_m128i(mtx[j])));
4977          mtx[j] = vreinterpretq_m128i_u8(
4978              vshrq_n_u8(vreinterpretq_u8_m128i(mtx[j]), 7));
4979          __m128i tmp = vreinterpretq_m128i_u16(
4980              vshrq_n_u16(vreinterpretq_u16_m128i(mtx[j]), 8));
4981          uint16x8_t vec_res = vandq_u16(vreinterpretq_u16_m128i(mtx[j]),
4982                                         vreinterpretq_u16_m128i(tmp));
4983          int t = _sse2neon_vaddvq_u16(vec_res) ? 1 : 0;
4984          res |= (t << j);
4985      }
4986      return res;
4987  }
4988  #define SSE2NEON_CMP_RANGES_IS_BYTE 1
4989  #define SSE2NEON_CMP_RANGES_IS_WORD 0
4990  #define SSE2NEON_GENERATE_CMP_RANGES(prefix)             \
4991      prefix##IMPL(byte, uint, u, prefix##IS_BYTE)         \
4992      prefix##IMPL(byte, int, s, prefix##IS_BYTE)          \
4993      prefix##IMPL(word, uint, u, prefix##IS_WORD)         \
4994      prefix##IMPL(word, int, s, prefix##IS_WORD)
4995  SSE2NEON_GENERATE_CMP_RANGES(SSE2NEON_CMP_RANGES_)
4996  #undef SSE2NEON_CMP_RANGES_IS_BYTE
4997  #undef SSE2NEON_CMP_RANGES_IS_WORD
4998  static int _sse2neon_cmp_byte_equal_each(__m128i a, int la, __m128i b, int lb)
4999  {
5000      uint8x16_t mtx =
5001          vceqq_u8(vreinterpretq_u8_m128i(a), vreinterpretq_u8_m128i(b));
5002      int m0 = (la < lb) ? 0 : ((1 << la) - (1 << lb));
5003      int m1 = 0x10000 - (1 << la);
5004      int tb = 0x10000 - (1 << lb);
5005      uint8x8_t vec_mask, vec0_lo, vec0_hi, vec1_lo, vec1_hi;
5006      uint8x8_t tmp_lo, tmp_hi, res_lo, res_hi;
5007      vec_mask = vld1_u8(_sse2neon_cmpestr_mask8b);
5008      vec0_lo = vtst_u8(vdup_n_u8(m0), vec_mask);
5009      vec0_hi = vtst_u8(vdup_n_u8(m0 >> 8), vec_mask);
5010      vec1_lo = vtst_u8(vdup_n_u8(m1), vec_mask);
5011      vec1_hi = vtst_u8(vdup_n_u8(m1 >> 8), vec_mask);
5012      tmp_lo = vtst_u8(vdup_n_u8(tb), vec_mask);
5013      tmp_hi = vtst_u8(vdup_n_u8(tb >> 8), vec_mask);
5014      res_lo = vbsl_u8(vec0_lo, vdup_n_u8(0), vget_low_u8(mtx));
5015      res_hi = vbsl_u8(vec0_hi, vdup_n_u8(0), vget_high_u8(mtx));
5016      res_lo = vbsl_u8(vec1_lo, tmp_lo, res_lo);
5017      res_hi = vbsl_u8(vec1_hi, tmp_hi, res_hi);
5018      res_lo = vand_u8(res_lo, vec_mask);
5019      res_hi = vand_u8(res_hi, vec_mask);
5020      int res = _sse2neon_vaddv_u8(res_lo) + (_sse2neon_vaddv_u8(res_hi) << 8);
5021      return res;
5022  }
5023  static int _sse2neon_cmp_word_equal_each(__m128i a, int la, __m128i b, int lb)
5024  {
5025      uint16x8_t mtx =
5026          vceqq_u16(vreinterpretq_u16_m128i(a), vreinterpretq_u16_m128i(b));
5027      int m0 = (la < lb) ? 0 : ((1 << la) - (1 << lb));
5028      int m1 = 0x100 - (1 << la);
5029      int tb = 0x100 - (1 << lb);
5030      uint16x8_t vec_mask = vld1q_u16(_sse2neon_cmpestr_mask16b);
5031      uint16x8_t vec0 = vtstq_u16(vdupq_n_u16(m0), vec_mask);
5032      uint16x8_t vec1 = vtstq_u16(vdupq_n_u16(m1), vec_mask);
5033      uint16x8_t tmp = vtstq_u16(vdupq_n_u16(tb), vec_mask);
5034      mtx = vbslq_u16(vec0, vdupq_n_u16(0), mtx);
5035      mtx = vbslq_u16(vec1, tmp, mtx);
5036      mtx = vandq_u16(mtx, vec_mask);
5037      return _sse2neon_vaddvq_u16(mtx);
5038  }
5039  #define SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UBYTE 1
5040  #define SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UWORD 0
5041  #define SSE2NEON_AGGREGATE_EQUAL_ORDER_IMPL(size, number_of_lanes, data_type)  \
5042      static int _sse2neon_aggregate_equal_ordered_##size##x##number_of_lanes(   \
5043          int bound, int la, int lb, __m128i mtx[16])                            \
5044      {                                                                          \
5045          int res = 0;                                                           \
5046          int m1 = SSE2NEON_IIF(data_type)(0x10000, 0x100) - (1 << la);          \
5047          uint##size##x8_t vec_mask = SSE2NEON_IIF(data_type)(                   \
5048              vld1_u##size(_sse2neon_cmpestr_mask##size##b),                     \
5049              vld1q_u##size(_sse2neon_cmpestr_mask##size##b));                   \
5050          uint##size##x##number_of_lanes##_t vec1 = SSE2NEON_IIF(data_type)(     \
5051              vcombine_u##size(vtst_u##size(vdup_n_u##size(m1), vec_mask),       \
5052                               vtst_u##size(vdup_n_u##size(m1 >> 8), vec_mask)), \
5053              vtstq_u##size(vdupq_n_u##size(m1), vec_mask));                     \
5054          uint##size##x##number_of_lanes##_t vec_minusone = vdupq_n_u##size(-1); \
5055          uint##size##x##number_of_lanes##_t vec_zero = vdupq_n_u##size(0);      \
5056          for (int j = 0; j < lb; j++) {                                         \
5057              mtx[j] = vreinterpretq_m128i_u##size(vbslq_u##size(                \
5058                  vec1, vec_minusone, vreinterpretq_u##size##_m128i(mtx[j])));   \
5059          }                                                                      \
5060          for (int j = lb; j < bound; j++) {                                     \
5061              mtx[j] = vreinterpretq_m128i_u##size(                              \
5062                  vbslq_u##size(vec1, vec_minusone, vec_zero));                  \
5063          }                                                                      \
5064          unsigned SSE2NEON_IIF(data_type)(char, short) *ptr =                   \
5065              (unsigned SSE2NEON_IIF(data_type)(char, short) *) mtx;             \
5066          for (int i = 0; i < bound; i++) {                                      \
5067              int val = 1;                                                       \
5068              for (int j = 0, k = i; j < bound - i && k < bound; j++, k++)       \
5069                  val &= ptr[k * bound + j];                                     \
5070              res += val << i;                                                   \
5071          }                                                                      \
5072          return res;                                                            \
5073      }
5074  #define SSE2NEON_GENERATE_AGGREGATE_EQUAL_ORDER(prefix) \
5075      prefix##IMPL(8, 16, prefix##IS_UBYTE)               \
5076      prefix##IMPL(16, 8, prefix##IS_UWORD)
5077  SSE2NEON_GENERATE_AGGREGATE_EQUAL_ORDER(SSE2NEON_AGGREGATE_EQUAL_ORDER_)
5078  #undef SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UBYTE
5079  #undef SSE2NEON_AGGREGATE_EQUAL_ORDER_IS_UWORD
5080  #define SSE2NEON_GENERATE_CMP_EQUAL_ORDERED(prefix) \
5081      prefix##IMPL(byte)                              \
5082      prefix##IMPL(word)
5083  SSE2NEON_GENERATE_CMP_EQUAL_ORDERED(SSE2NEON_CMP_EQUAL_ORDERED_)
5084  #define SSE2NEON_CMPESTR_LIST                          \
5085      _(CMP_UBYTE_EQUAL_ANY, cmp_byte_equal_any)         \
5086      _(CMP_UWORD_EQUAL_ANY, cmp_word_equal_any)         \
5087      _(CMP_SBYTE_EQUAL_ANY, cmp_byte_equal_any)         \
5088      _(CMP_SWORD_EQUAL_ANY, cmp_word_equal_any)         \
5089      _(CMP_UBYTE_RANGES, cmp_ubyte_ranges)              \
5090      _(CMP_UWORD_RANGES, cmp_uword_ranges)              \
5091      _(CMP_SBYTE_RANGES, cmp_sbyte_ranges)              \
5092      _(CMP_SWORD_RANGES, cmp_sword_ranges)              \
5093      _(CMP_UBYTE_EQUAL_EACH, cmp_byte_equal_each)       \
5094      _(CMP_UWORD_EQUAL_EACH, cmp_word_equal_each)       \
5095      _(CMP_SBYTE_EQUAL_EACH, cmp_byte_equal_each)       \
5096      _(CMP_SWORD_EQUAL_EACH, cmp_word_equal_each)       \
5097      _(CMP_UBYTE_EQUAL_ORDERED, cmp_byte_equal_ordered) \
5098      _(CMP_UWORD_EQUAL_ORDERED, cmp_word_equal_ordered) \
5099      _(CMP_SBYTE_EQUAL_ORDERED, cmp_byte_equal_ordered) \
5100      _(CMP_SWORD_EQUAL_ORDERED, cmp_word_equal_ordered)
5101  enum {
5102  #define _(name, func_suffix) name,
5103      SSE2NEON_CMPESTR_LIST
5104  #undef _
5105  };
5106  typedef int (*cmpestr_func_t)(__m128i a, int la, __m128i b, int lb);
5107  static cmpestr_func_t _sse2neon_cmpfunc_table[] = {
5108  #define _(name, func_suffix) _sse2neon_##func_suffix,
5109      SSE2NEON_CMPESTR_LIST
5110  #undef _
5111  };
5112  FORCE_INLINE int _sse2neon_sido_negative(int res, int lb, int imm8, int bound)
5113  {
5114      switch (imm8 & 0x30) {
5115      case _SIDD_NEGATIVE_POLARITY:
5116          res ^= 0xffffffff;
5117          break;
5118      case _SIDD_MASKED_NEGATIVE_POLARITY:
5119          res ^= (1 << lb) - 1;
5120          break;
5121      default:
5122          break;
5123      }
5124      return res & ((bound == 8) ? 0xFF : 0xFFFF);
5125  }
5126  FORCE_INLINE int _sse2neon_clz(unsigned int x)
5127  {
5128  #if _MSC_VER
5129      DWORD cnt = 0;
5130      if (_BitScanForward(&cnt, x))
5131          return cnt;
5132      return 32;
5133  #else
5134      return x != 0 ? __builtin_clz(x) : 32;
5135  #endif
5136  }
5137  FORCE_INLINE int _sse2neon_ctz(unsigned int x)
5138  {
5139  #if _MSC_VER
5140      DWORD cnt = 0;
5141      if (_BitScanReverse(&cnt, x))
5142          return 31 - cnt;
5143      return 32;
5144  #else
5145      return x != 0 ? __builtin_ctz(x) : 32;
5146  #endif
5147  }
5148  FORCE_INLINE int _sse2neon_ctzll(unsigned long long x)
5149  {
5150  #if _MSC_VER
5151      unsigned long cnt;
5152  #ifdef defined(SSE2NEON_HAS_BITSCAN64)
5153      (defined(_M_AMD64) || defined(__x86_64__))
5154          if((_BitScanForward64(&cnt, x))
5155              return (int)(cnt);
5156  #else
5157      if (_BitScanForward(&cnt, (unsigned long) (x)))
5158          return (int) cnt;
5159      if (_BitScanForward(&cnt, (unsigned long) (x >> 32)))
5160          return (int) (cnt + 32);
5161  #endif
5162      return 64;
5163  #else
5164      return x != 0 ? __builtin_ctzll(x) : 64;
5165  #endif
5166  }
5167  #define SSE2NEON_MIN(x, y) (x) < (y) ? (x) : (y)
5168  #define SSE2NEON_CMPSTR_SET_UPPER(var, imm) \
5169      const int var = (imm & 0x01) ? 8 : 16
5170  #define SSE2NEON_CMPESTRX_LEN_PAIR(a, b, la, lb) \
5171      int tmp1 = la ^ (la >> 31);                  \
5172      la = tmp1 - (la >> 31);                      \
5173      int tmp2 = lb ^ (lb >> 31);                  \
5174      lb = tmp2 - (lb >> 31);                      \
5175      la = SSE2NEON_MIN(la, bound);                \
5176      lb = SSE2NEON_MIN(lb, bound)
5177  #define SSE2NEON_COMP_AGG(a, b, la, lb, imm8, IE)                  \
5178      SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);                        \
5179      SSE2NEON_##IE##_LEN_PAIR(a, b, la, lb);                        \
5180      int r2 = (_sse2neon_cmpfunc_table[imm8 & 0x0f])(a, la, b, lb); \
5181      r2 = _sse2neon_sido_negative(r2, lb, imm8, bound)
5182  #define SSE2NEON_CMPSTR_GENERATE_INDEX(r2, bound, imm8)          \
5183      return (r2 == 0) ? bound                                     \
5184                       : ((imm8 & 0x40) ? (31 - _sse2neon_clz(r2)) \
5185                                        : _sse2neon_ctz(r2))
5186  #define SSE2NEON_CMPSTR_GENERATE_MASK(dst)                                     \
5187      __m128i dst = vreinterpretq_m128i_u8(vdupq_n_u8(0));                       \
5188      if (imm8 & 0x40) {                                                         \
5189          if (bound == 8) {                                                      \
5190              uint16x8_t tmp = vtstq_u16(vdupq_n_u16(r2),                        \
5191                                         vld1q_u16(_sse2neon_cmpestr_mask16b));  \
5192              dst = vreinterpretq_m128i_u16(vbslq_u16(                           \
5193                  tmp, vdupq_n_u16(-1), vreinterpretq_u16_m128i(dst)));          \
5194          } else {                                                               \
5195              uint8x16_t vec_r2 =                                                \
5196                  vcombine_u8(vdup_n_u8(r2), vdup_n_u8(r2 >> 8));                \
5197              uint8x16_t tmp =                                                   \
5198                  vtstq_u8(vec_r2, vld1q_u8(_sse2neon_cmpestr_mask8b));          \
5199              dst = vreinterpretq_m128i_u8(                                      \
5200                  vbslq_u8(tmp, vdupq_n_u8(-1), vreinterpretq_u8_m128i(dst)));   \
5201          }                                                                      \
5202      } else {                                                                   \
5203          if (bound == 16) {                                                     \
5204              dst = vreinterpretq_m128i_u16(                                     \
5205                  vsetq_lane_u16(r2 & 0xffff, vreinterpretq_u16_m128i(dst), 0)); \
5206          } else {                                                               \
5207              dst = vreinterpretq_m128i_u8(                                      \
5208                  vsetq_lane_u8(r2 & 0xff, vreinterpretq_u8_m128i(dst), 0));     \
5209          }                                                                      \
5210      }                                                                          \
5211      return dst
5212  FORCE_INLINE int _mm_cmpestra(__m128i a,
5213                                int la,
5214                                __m128i b,
5215                                int lb,
5216                                const int imm8)
5217  {
5218      int lb_cpy = lb;
5219      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
5220      return !r2 & (lb_cpy > bound);
5221  }
5222  FORCE_INLINE int _mm_cmpestrc(__m128i a,
5223                                int la,
5224                                __m128i b,
5225                                int lb,
5226                                const int imm8)
5227  {
5228      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
5229      return r2 != 0;
5230  }
5231  FORCE_INLINE int _mm_cmpestri(__m128i a,
5232                                int la,
5233                                __m128i b,
5234                                int lb,
5235                                const int imm8)
5236  {
5237      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
5238      SSE2NEON_CMPSTR_GENERATE_INDEX(r2, bound, imm8);
5239  }
5240  FORCE_INLINE __m128i
5241  _mm_cmpestrm(__m128i a, int la, __m128i b, int lb, const int imm8)
5242  {
5243      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
5244      SSE2NEON_CMPSTR_GENERATE_MASK(dst);
5245  }
5246  FORCE_INLINE int _mm_cmpestro(__m128i a,
5247                                int la,
5248                                __m128i b,
5249                                int lb,
5250                                const int imm8)
5251  {
5252      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPESTRX);
5253      return r2 & 1;
5254  }
5255  FORCE_INLINE int _mm_cmpestrs(__m128i a,
5256                                int la,
5257                                __m128i b,
5258                                int lb,
5259                                const int imm8)
5260  {
5261      SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
5262      return la <= (bound - 1);
5263  }
5264  FORCE_INLINE int _mm_cmpestrz(__m128i a,
5265                                int la,
5266                                __m128i b,
5267                                int lb,
5268                                const int imm8)
5269  {
5270      SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
5271      return lb <= (bound - 1);
5272  }
5273  #define SSE2NEON_CMPISTRX_LENGTH(str, len, imm8)                         \
5274      do {                                                                 \
5275          if (imm8 & 0x01) {                                               \
5276              uint16x8_t equal_mask_##str =                                \
5277                  vceqq_u16(vreinterpretq_u16_m128i(str), vdupq_n_u16(0)); \
5278              uint8x8_t res_##str = vshrn_n_u16(equal_mask_##str, 4);      \
5279              uint64_t matches_##str =                                     \
5280                  vget_lane_u64(vreinterpret_u64_u8(res_##str), 0);        \
5281              len = _sse2neon_ctzll(matches_##str) >> 3;                   \
5282          } else {                                                         \
5283              uint16x8_t equal_mask_##str = vreinterpretq_u16_u8(          \
5284                  vceqq_u8(vreinterpretq_u8_m128i(str), vdupq_n_u8(0)));   \
5285              uint8x8_t res_##str = vshrn_n_u16(equal_mask_##str, 4);      \
5286              uint64_t matches_##str =                                     \
5287                  vget_lane_u64(vreinterpret_u64_u8(res_##str), 0);        \
5288              len = _sse2neon_ctzll(matches_##str) >> 2;                   \
5289          }                                                                \
5290      } while (0)
5291  #define SSE2NEON_CMPISTRX_LEN_PAIR(a, b, la, lb) \
5292      int la, lb;                                  \
5293      do {                                         \
5294          SSE2NEON_CMPISTRX_LENGTH(a, la, imm8);   \
5295          SSE2NEON_CMPISTRX_LENGTH(b, lb, imm8);   \
5296      } while (0)
5297  FORCE_INLINE int _mm_cmpistra(__m128i a, __m128i b, const int imm8)
5298  {
5299      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
5300      return !r2 & (lb >= bound);
5301  }
5302  FORCE_INLINE int _mm_cmpistrc(__m128i a, __m128i b, const int imm8)
5303  {
5304      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
5305      return r2 != 0;
5306  }
5307  FORCE_INLINE int _mm_cmpistri(__m128i a, __m128i b, const int imm8)
5308  {
5309      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
5310      SSE2NEON_CMPSTR_GENERATE_INDEX(r2, bound, imm8);
5311  }
5312  FORCE_INLINE __m128i _mm_cmpistrm(__m128i a, __m128i b, const int imm8)
5313  {
5314      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
5315      SSE2NEON_CMPSTR_GENERATE_MASK(dst);
5316  }
5317  FORCE_INLINE int _mm_cmpistro(__m128i a, __m128i b, const int imm8)
5318  {
5319      SSE2NEON_COMP_AGG(a, b, la, lb, imm8, CMPISTRX);
5320      return r2 & 1;
5321  }
5322  FORCE_INLINE int _mm_cmpistrs(__m128i a, __m128i b, const int imm8)
5323  {
5324      SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
5325      int la;
5326      SSE2NEON_CMPISTRX_LENGTH(a, la, imm8);
5327      return la <= (bound - 1);
5328  }
5329  FORCE_INLINE int _mm_cmpistrz(__m128i a, __m128i b, const int imm8)
5330  {
5331      SSE2NEON_CMPSTR_SET_UPPER(bound, imm8);
5332      int lb;
5333      SSE2NEON_CMPISTRX_LENGTH(b, lb, imm8);
5334      return lb <= (bound - 1);
5335  }
5336  FORCE_INLINE __m128i _mm_cmpgt_epi64(__m128i a, __m128i b)
5337  {
5338  #if defined(__aarch64__)
5339      return vreinterpretq_m128i_u64(
5340          vcgtq_s64(vreinterpretq_s64_m128i(a), vreinterpretq_s64_m128i(b)));
5341  #else
5342      return vreinterpretq_m128i_s64(vshrq_n_s64(
5343          vqsubq_s64(vreinterpretq_s64_m128i(b), vreinterpretq_s64_m128i(a)),
5344          63));
5345  #endif
5346  }
5347  FORCE_INLINE uint32_t _mm_crc32_u16(uint32_t crc, uint16_t v)
5348  {
5349  #if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
5350      __asm__ __volatile__("crc32ch %w[c], %w[c], %w[v]\n\t"
5351                           : [c] "+r"(crc)
5352                           : [v] "r"(v));
5353  #elif (__ARM_ARCH == 8) && defined(__ARM_FEATURE_CRC32)
5354      crc = __crc32ch(crc, v);
5355  #else
5356      crc = _mm_crc32_u8(crc, v & 0xff);
5357      crc = _mm_crc32_u8(crc, (v >> 8) & 0xff);
5358  #endif
5359      return crc;
5360  }
5361  FORCE_INLINE uint32_t _mm_crc32_u32(uint32_t crc, uint32_t v)
5362  {
5363  #if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
5364      __asm__ __volatile__("crc32cw %w[c], %w[c], %w[v]\n\t"
5365                           : [c] "+r"(crc)
5366                           : [v] "r"(v));
5367  #elif (__ARM_ARCH == 8) && defined(__ARM_FEATURE_CRC32)
5368      crc = __crc32cw(crc, v);
5369  #else
5370      crc = _mm_crc32_u16(crc, v & 0xffff);
5371      crc = _mm_crc32_u16(crc, (v >> 16) & 0xffff);
5372  #endif
5373      return crc;
5374  }
5375  FORCE_INLINE uint64_t _mm_crc32_u64(uint64_t crc, uint64_t v)
5376  {
5377  #if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
5378      __asm__ __volatile__("crc32cx %w[c], %w[c], %x[v]\n\t"
5379                           : [c] "+r"(crc)
5380                           : [v] "r"(v));
5381  #else
5382      crc = _mm_crc32_u32((uint32_t) (crc), v & 0xffffffff);
5383      crc = _mm_crc32_u32((uint32_t) (crc), (v >> 32) & 0xffffffff);
5384  #endif
5385      return crc;
5386  }
5387  FORCE_INLINE uint32_t _mm_crc32_u8(uint32_t crc, uint8_t v)
5388  {
5389  #if defined(__aarch64__) && defined(__ARM_FEATURE_CRC32)
5390      __asm__ __volatile__("crc32cb %w[c], %w[c], %w[v]\n\t"
5391                           : [c] "+r"(crc)
5392                           : [v] "r"(v));
5393  #elif (__ARM_ARCH == 8) && defined(__ARM_FEATURE_CRC32)
5394      crc = __crc32cb(crc, v);
5395  #else
5396      crc ^= v;
5397      for (int bit = 0; bit < 8; bit++) {
5398          if (crc & 1)
5399              crc = (crc >> 1) ^ UINT32_C(0x82f63b78);
5400          else
5401              crc = (crc >> 1);
5402      }
5403  #endif
5404      return crc;
5405  }
5406  #if !defined(__ARM_FEATURE_CRYPTO)
5407  #define SSE2NEON_AES_SBOX(w)                                           \
5408      {                                                                  \
5409          w(0x63), w(0x7c), w(0x77), w(0x7b), w(0xf2), w(0x6b), w(0x6f), \
5410          w(0xc5), w(0x30), w(0x01), w(0x67), w(0x2b), w(0xfe), w(0xd7), \
5411          w(0xab), w(0x76), w(0xca), w(0x82), w(0xc9), w(0x7d), w(0xfa), \
5412          w(0x59), w(0x47), w(0xf0), w(0xad), w(0xd4), w(0xa2), w(0xaf), \
5413          w(0x9c), w(0xa4), w(0x72), w(0xc0), w(0xb7), w(0xfd), w(0x93), \
5414          w(0x26), w(0x36), w(0x3f), w(0xf7), w(0xcc), w(0x34), w(0xa5), \
5415          w(0xe5), w(0xf1), w(0x71), w(0xd8), w(0x31), w(0x15), w(0x04), \
5416          w(0xc7), w(0x23), w(0xc3), w(0x18), w(0x96), w(0x05), w(0x9a), \
5417          w(0x07), w(0x12), w(0x80), w(0xe2), w(0xeb), w(0x27), w(0xb2), \
5418          w(0x75), w(0x09), w(0x83), w(0x2c), w(0x1a), w(0x1b), w(0x6e), \
5419          w(0x5a), w(0xa0), w(0x52), w(0x3b), w(0xd6), w(0xb3), w(0x29), \
5420          w(0xe3), w(0x2f), w(0x84), w(0x53), w(0xd1), w(0x00), w(0xed), \
5421          w(0x20), w(0xfc), w(0xb1), w(0x5b), w(0x6a), w(0xcb), w(0xbe), \
5422          w(0x39), w(0x4a), w(0x4c), w(0x58), w(0xcf), w(0xd0), w(0xef), \
5423          w(0xaa), w(0xfb), w(0x43), w(0x4d), w(0x33), w(0x85), w(0x45), \
5424          w(0xf9), w(0x02), w(0x7f), w(0x50), w(0x3c), w(0x9f), w(0xa8), \
5425          w(0x51), w(0xa3), w(0x40), w(0x8f), w(0x92), w(0x9d), w(0x38), \
5426          w(0xf5), w(0xbc), w(0xb6), w(0xda), w(0x21), w(0x10), w(0xff), \
5427          w(0xf3), w(0xd2), w(0xcd), w(0x0c), w(0x13), w(0xec), w(0x5f), \
5428          w(0x97), w(0x44), w(0x17), w(0xc4), w(0xa7), w(0x7e), w(0x3d), \
5429          w(0x64), w(0x5d), w(0x19), w(0x73), w(0x60), w(0x81), w(0x4f), \
5430          w(0xdc), w(0x22), w(0x2a), w(0x90), w(0x88), w(0x46), w(0xee), \
5431          w(0xb8), w(0x14), w(0xde), w(0x5e), w(0x0b), w(0xdb), w(0xe0), \
5432          w(0x32), w(0x3a), w(0x0a), w(0x49), w(0x06), w(0x24), w(0x5c), \
5433          w(0xc2), w(0xd3), w(0xac), w(0x62), w(0x91), w(0x95), w(0xe4), \
5434          w(0x79), w(0xe7), w(0xc8), w(0x37), w(0x6d), w(0x8d), w(0xd5), \
5435          w(0x4e), w(0xa9), w(0x6c), w(0x56), w(0xf4), w(0xea), w(0x65), \
5436          w(0x7a), w(0xae), w(0x08), w(0xba), w(0x78), w(0x25), w(0x2e), \
5437          w(0x1c), w(0xa6), w(0xb4), w(0xc6), w(0xe8), w(0xdd), w(0x74), \
5438          w(0x1f), w(0x4b), w(0xbd), w(0x8b), w(0x8a), w(0x70), w(0x3e), \
5439          w(0xb5), w(0x66), w(0x48), w(0x03), w(0xf6), w(0x0e), w(0x61), \
5440          w(0x35), w(0x57), w(0xb9), w(0x86), w(0xc1), w(0x1d), w(0x9e), \
5441          w(0xe1), w(0xf8), w(0x98), w(0x11), w(0x69), w(0xd9), w(0x8e), \
5442          w(0x94), w(0x9b), w(0x1e), w(0x87), w(0xe9), w(0xce), w(0x55), \
5443          w(0x28), w(0xdf), w(0x8c), w(0xa1), w(0x89), w(0x0d), w(0xbf), \
5444          w(0xe6), w(0x42), w(0x68), w(0x41), w(0x99), w(0x2d), w(0x0f), \
5445          w(0xb0), w(0x54), w(0xbb), w(0x16)                             \
5446      }
5447  #define SSE2NEON_AES_RSBOX(w)                                          \
5448      {                                                                  \
5449          w(0x52), w(0x09), w(0x6a), w(0xd5), w(0x30), w(0x36), w(0xa5), \
5450          w(0x38), w(0xbf), w(0x40), w(0xa3), w(0x9e), w(0x81), w(0xf3), \
5451          w(0xd7), w(0xfb), w(0x7c), w(0xe3), w(0x39), w(0x82), w(0x9b), \
5452          w(0x2f), w(0xff), w(0x87), w(0x34), w(0x8e), w(0x43), w(0x44), \
5453          w(0xc4), w(0xde), w(0xe9), w(0xcb), w(0x54), w(0x7b), w(0x94), \
5454          w(0x32), w(0xa6), w(0xc2), w(0x23), w(0x3d), w(0xee), w(0x4c), \
5455          w(0x95), w(0x0b), w(0x42), w(0xfa), w(0xc3), w(0x4e), w(0x08), \
5456          w(0x2e), w(0xa1), w(0x66), w(0x28), w(0xd9), w(0x24), w(0xb2), \
5457          w(0x76), w(0x5b), w(0xa2), w(0x49), w(0x6d), w(0x8b), w(0xd1), \
5458          w(0x25), w(0x72), w(0xf8), w(0xf6), w(0x64), w(0x86), w(0x68), \
5459          w(0x98), w(0x16), w(0xd4), w(0xa4), w(0x5c), w(0xcc), w(0x5d), \
5460          w(0x65), w(0xb6), w(0x92), w(0x6c), w(0x70), w(0x48), w(0x50), \
5461          w(0xfd), w(0xed), w(0xb9), w(0xda), w(0x5e), w(0x15), w(0x46), \
5462          w(0x57), w(0xa7), w(0x8d), w(0x9d), w(0x84), w(0x90), w(0xd8), \
5463          w(0xab), w(0x00), w(0x8c), w(0xbc), w(0xd3), w(0x0a), w(0xf7), \
5464          w(0xe4), w(0x58), w(0x05), w(0xb8), w(0xb3), w(0x45), w(0x06), \
5465          w(0xd0), w(0x2c), w(0x1e), w(0x8f), w(0xca), w(0x3f), w(0x0f), \
5466          w(0x02), w(0xc1), w(0xaf), w(0xbd), w(0x03), w(0x01), w(0x13), \
5467          w(0x8a), w(0x6b), w(0x3a), w(0x91), w(0x11), w(0x41), w(0x4f), \
5468          w(0x67), w(0xdc), w(0xea), w(0x97), w(0xf2), w(0xcf), w(0xce), \
5469          w(0xf0), w(0xb4), w(0xe6), w(0x73), w(0x96), w(0xac), w(0x74), \
5470          w(0x22), w(0xe7), w(0xad), w(0x35), w(0x85), w(0xe2), w(0xf9), \
5471          w(0x37), w(0xe8), w(0x1c), w(0x75), w(0xdf), w(0x6e), w(0x47), \
5472          w(0xf1), w(0x1a), w(0x71), w(0x1d), w(0x29), w(0xc5), w(0x89), \
5473          w(0x6f), w(0xb7), w(0x62), w(0x0e), w(0xaa), w(0x18), w(0xbe), \
5474          w(0x1b), w(0xfc), w(0x56), w(0x3e), w(0x4b), w(0xc6), w(0xd2), \
5475          w(0x79), w(0x20), w(0x9a), w(0xdb), w(0xc0), w(0xfe), w(0x78), \
5476          w(0xcd), w(0x5a), w(0xf4), w(0x1f), w(0xdd), w(0xa8), w(0x33), \
5477          w(0x88), w(0x07), w(0xc7), w(0x31), w(0xb1), w(0x12), w(0x10), \
5478          w(0x59), w(0x27), w(0x80), w(0xec), w(0x5f), w(0x60), w(0x51), \
5479          w(0x7f), w(0xa9), w(0x19), w(0xb5), w(0x4a), w(0x0d), w(0x2d), \
5480          w(0xe5), w(0x7a), w(0x9f), w(0x93), w(0xc9), w(0x9c), w(0xef), \
5481          w(0xa0), w(0xe0), w(0x3b), w(0x4d), w(0xae), w(0x2a), w(0xf5), \
5482          w(0xb0), w(0xc8), w(0xeb), w(0xbb), w(0x3c), w(0x83), w(0x53), \
5483          w(0x99), w(0x61), w(0x17), w(0x2b), w(0x04), w(0x7e), w(0xba), \
5484          w(0x77), w(0xd6), w(0x26), w(0xe1), w(0x69), w(0x14), w(0x63), \
5485          w(0x55), w(0x21), w(0x0c), w(0x7d)                             \
5486      }
5487  #define SSE2NEON_AES_H0(x) (x)
5488  static const uint8_t _sse2neon_sbox[256] = SSE2NEON_AES_SBOX(SSE2NEON_AES_H0);
5489  static const uint8_t _sse2neon_rsbox[256] = SSE2NEON_AES_RSBOX(SSE2NEON_AES_H0);
5490  #undef SSE2NEON_AES_H0
5491  #if !defined(__aarch64__)
5492  #define SSE2NEON_XT(x) (((x) << 1) ^ ((((x) >> 7) & 1) * 0x1b))
5493  #define SSE2NEON_MULTIPLY(x, y)                                  \
5494      (((y & 1) * x) ^ ((y >> 1 & 1) * SSE2NEON_XT(x)) ^           \
5495       ((y >> 2 & 1) * SSE2NEON_XT(SSE2NEON_XT(x))) ^              \
5496       ((y >> 3 & 1) * SSE2NEON_XT(SSE2NEON_XT(SSE2NEON_XT(x)))) ^ \
5497       ((y >> 4 & 1) * SSE2NEON_XT(SSE2NEON_XT(SSE2NEON_XT(SSE2NEON_XT(x))))))
5498  #endif
5499  FORCE_INLINE __m128i _mm_aesenc_si128(__m128i a, __m128i RoundKey)
5500  {
5501  #if defined(__aarch64__)
5502      static const uint8_t shift_rows[] = {
5503          0x0, 0x5, 0xa, 0xf, 0x4, 0x9, 0xe, 0x3,
5504          0x8, 0xd, 0x2, 0x7, 0xc, 0x1, 0x6, 0xb,
5505      };
5506      static const uint8_t ror32by8[] = {
5507          0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,
5508          0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,
5509      };
5510      uint8x16_t v;
5511      uint8x16_t w = vreinterpretq_u8_m128i(a);
5512      w = vqtbl1q_u8(w, vld1q_u8(shift_rows));
5513      v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_sbox), w);
5514      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x40), w - 0x40);
5515      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x80), w - 0x80);
5516      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0xc0), w - 0xc0);
5517      w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
5518      w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);
5519      w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));
5520      return vreinterpretq_m128i_u8(w) ^ RoundKey;
5521  #else &bsol;* ARMv7-A implementation for a table-based AES */
5522  #define SSE2NEON_AES_B2W(b0, b1, b2, b3)                 \
5523      (((uint32_t) (b3) << 24) | ((uint32_t) (b2) << 16) | \
5524       ((uint32_t) (b1) << 8) | (uint32_t) (b0))
5525  #define SSE2NEON_AES_F2(x) ((x << 1) ^ (((x >> 7) & 1) * 0x011b &bsol;* WPOLY */))
5526  #define SSE2NEON_AES_F3(x) (SSE2NEON_AES_F2(x) ^ x)
5527  #define SSE2NEON_AES_U0(p) \
5528      SSE2NEON_AES_B2W(SSE2NEON_AES_F2(p), p, p, SSE2NEON_AES_F3(p))
5529  #define SSE2NEON_AES_U1(p) \
5530      SSE2NEON_AES_B2W(SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p), p, p)
5531  #define SSE2NEON_AES_U2(p) \
5532      SSE2NEON_AES_B2W(p, SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p), p)
5533  #define SSE2NEON_AES_U3(p) \
5534      SSE2NEON_AES_B2W(p, p, SSE2NEON_AES_F3(p), SSE2NEON_AES_F2(p))
5535      static const uint32_t ALIGN_STRUCT(16) aes_table[4][256] = {
5536          SSE2NEON_AES_SBOX(SSE2NEON_AES_U0),
5537          SSE2NEON_AES_SBOX(SSE2NEON_AES_U1),
5538          SSE2NEON_AES_SBOX(SSE2NEON_AES_U2),
5539          SSE2NEON_AES_SBOX(SSE2NEON_AES_U3),
5540      };
5541  #undef SSE2NEON_AES_B2W
5542  #undef SSE2NEON_AES_F2
5543  #undef SSE2NEON_AES_F3
5544  #undef SSE2NEON_AES_U0
5545  #undef SSE2NEON_AES_U1
5546  #undef SSE2NEON_AES_U2
5547  #undef SSE2NEON_AES_U3
5548      uint32_t x0 = _mm_cvtsi128_si32(a);  
5549      uint32_t x1 =
5550          _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0x55));  
5551      uint32_t x2 =
5552          _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0xAA));  
5553      uint32_t x3 =
5554          _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0xFF));  
5555      __m128i out = _mm_set_epi32(
5556          (aes_table[0][x3 & 0xff] ^ aes_table[1][(x0 >> 8) & 0xff] ^
5557           aes_table[2][(x1 >> 16) & 0xff] ^ aes_table[3][x2 >> 24]),
5558          (aes_table[0][x2 & 0xff] ^ aes_table[1][(x3 >> 8) & 0xff] ^
5559           aes_table[2][(x0 >> 16) & 0xff] ^ aes_table[3][x1 >> 24]),
5560          (aes_table[0][x1 & 0xff] ^ aes_table[1][(x2 >> 8) & 0xff] ^
5561           aes_table[2][(x3 >> 16) & 0xff] ^ aes_table[3][x0 >> 24]),
5562          (aes_table[0][x0 & 0xff] ^ aes_table[1][(x1 >> 8) & 0xff] ^
5563           aes_table[2][(x2 >> 16) & 0xff] ^ aes_table[3][x3 >> 24]));
5564      return _mm_xor_si128(out, RoundKey);
5565  #endif
5566  }
5567  FORCE_INLINE __m128i _mm_aesdec_si128(__m128i a, __m128i RoundKey)
5568  {
5569  #if defined(__aarch64__)
5570      static const uint8_t inv_shift_rows[] = {
5571          0x0, 0xd, 0xa, 0x7, 0x4, 0x1, 0xe, 0xb,
5572          0x8, 0x5, 0x2, 0xf, 0xc, 0x9, 0x6, 0x3,
5573      };
5574      static const uint8_t ror32by8[] = {
5575          0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,
5576          0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,
5577      };
5578      uint8x16_t v;
5579      uint8x16_t w = vreinterpretq_u8_m128i(a);
5580      w = vqtbl1q_u8(w, vld1q_u8(inv_shift_rows));
5581      v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_rsbox), w);
5582      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x40), w - 0x40);
5583      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x80), w - 0x80);
5584      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0xc0), w - 0xc0);
5585      w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
5586      w = (w << 1) ^ (uint8x16_t) (((int8x16_t) w >> 7) & 0x1b);
5587      v ^= w;
5588      v ^= (uint8x16_t) vrev32q_u16((uint16x8_t) w);
5589      w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) &
5590                                   0x1b);  
5591      w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);
5592      w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));
5593      return vreinterpretq_m128i_u8(w) ^ RoundKey;
5594  #else &bsol;* ARMv7-A NEON implementation */
5595      uint8_t i, e, f, g, h, v[4][4];
5596      uint8_t *_a = (uint8_t *) &a;
5597      for (i = 0; i < 16; ++i) {
5598          v[((i / 4) + (i % 4)) % 4][i % 4] = _sse2neon_rsbox[_a[i]];
5599      }
5600      for (i = 0; i < 4; ++i) {
5601          e = v[i][0];
5602          f = v[i][1];
5603          g = v[i][2];
5604          h = v[i][3];
5605          v[i][0] = SSE2NEON_MULTIPLY(e, 0x0e) ^ SSE2NEON_MULTIPLY(f, 0x0b) ^
5606                    SSE2NEON_MULTIPLY(g, 0x0d) ^ SSE2NEON_MULTIPLY(h, 0x09);
5607          v[i][1] = SSE2NEON_MULTIPLY(e, 0x09) ^ SSE2NEON_MULTIPLY(f, 0x0e) ^
5608                    SSE2NEON_MULTIPLY(g, 0x0b) ^ SSE2NEON_MULTIPLY(h, 0x0d);
5609          v[i][2] = SSE2NEON_MULTIPLY(e, 0x0d) ^ SSE2NEON_MULTIPLY(f, 0x09) ^
5610                    SSE2NEON_MULTIPLY(g, 0x0e) ^ SSE2NEON_MULTIPLY(h, 0x0b);
5611          v[i][3] = SSE2NEON_MULTIPLY(e, 0x0b) ^ SSE2NEON_MULTIPLY(f, 0x0d) ^
5612                    SSE2NEON_MULTIPLY(g, 0x09) ^ SSE2NEON_MULTIPLY(h, 0x0e);
5613      }
5614      return vreinterpretq_m128i_u8(vld1q_u8((uint8_t *) v)) ^ RoundKey;
5615  #endif
5616  }
5617  FORCE_INLINE __m128i _mm_aesenclast_si128(__m128i a, __m128i RoundKey)
5618  {
5619  #if defined(__aarch64__)
5620      static const uint8_t shift_rows[] = {
5621          0x0, 0x5, 0xa, 0xf, 0x4, 0x9, 0xe, 0x3,
5622          0x8, 0xd, 0x2, 0x7, 0xc, 0x1, 0x6, 0xb,
5623      };
5624      uint8x16_t v;
5625      uint8x16_t w = vreinterpretq_u8_m128i(a);
5626      w = vqtbl1q_u8(w, vld1q_u8(shift_rows));
5627      v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_sbox), w);
5628      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x40), w - 0x40);
5629      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x80), w - 0x80);
5630      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0xc0), w - 0xc0);
5631      return vreinterpretq_m128i_u8(v) ^ RoundKey;
5632  #else &bsol;* ARMv7-A implementation */
5633      uint8_t v[16] = {
5634          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 0)],
5635          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 5)],
5636          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 10)],
5637          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 15)],
5638          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 4)],
5639          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 9)],
5640          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 14)],
5641          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 3)],
5642          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 8)],
5643          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 13)],
5644          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 2)],
5645          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 7)],
5646          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 12)],
5647          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 1)],
5648          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 6)],
5649          _sse2neon_sbox[vgetq_lane_u8(vreinterpretq_u8_m128i(a), 11)],
5650      };
5651      return vreinterpretq_m128i_u8(vld1q_u8(v)) ^ RoundKey;
5652  #endif
5653  }
5654  FORCE_INLINE __m128i _mm_aesdeclast_si128(__m128i a, __m128i RoundKey)
5655  {
5656  #if defined(__aarch64__)
5657      static const uint8_t inv_shift_rows[] = {
5658          0x0, 0xd, 0xa, 0x7, 0x4, 0x1, 0xe, 0xb,
5659          0x8, 0x5, 0x2, 0xf, 0xc, 0x9, 0x6, 0x3,
5660      };
5661      uint8x16_t v;
5662      uint8x16_t w = vreinterpretq_u8_m128i(a);
5663      w = vqtbl1q_u8(w, vld1q_u8(inv_shift_rows));
5664      v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_rsbox), w);
5665      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x40), w - 0x40);
5666      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0x80), w - 0x80);
5667      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_rsbox + 0xc0), w - 0xc0);
5668      return vreinterpretq_m128i_u8(v) ^ RoundKey;
5669  #else &bsol;* ARMv7-A NEON implementation */
5670      uint8_t v[4][4];
5671      uint8_t *_a = (uint8_t *) &a;
5672      for (int i = 0; i < 16; ++i) {
5673          v[((i / 4) + (i % 4)) % 4][i % 4] = _sse2neon_rsbox[_a[i]];
5674      }
5675      return vreinterpretq_m128i_u8(vld1q_u8((uint8_t *) v)) ^ RoundKey;
5676  #endif
5677  }
5678  FORCE_INLINE __m128i _mm_aesimc_si128(__m128i a)
5679  {
5680  #if defined(__aarch64__)
5681      static const uint8_t ror32by8[] = {
5682          0x1, 0x2, 0x3, 0x0, 0x5, 0x6, 0x7, 0x4,
5683          0x9, 0xa, 0xb, 0x8, 0xd, 0xe, 0xf, 0xc,
5684      };
5685      uint8x16_t v = vreinterpretq_u8_m128i(a);
5686      uint8x16_t w;
5687      w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
5688      w = (w << 1) ^ (uint8x16_t) (((int8x16_t) w >> 7) & 0x1b);
5689      v ^= w;
5690      v ^= (uint8x16_t) vrev32q_u16((uint16x8_t) w);
5691      w = (v << 1) ^ (uint8x16_t) (((int8x16_t) v >> 7) & 0x1b);
5692      w ^= (uint8x16_t) vrev32q_u16((uint16x8_t) v);
5693      w ^= vqtbl1q_u8(v ^ w, vld1q_u8(ror32by8));
5694      return vreinterpretq_m128i_u8(w);
5695  #else &bsol;* ARMv7-A NEON implementation */
5696      uint8_t i, e, f, g, h, v[4][4];
5697      vst1q_u8((uint8_t *) v, vreinterpretq_u8_m128i(a));
5698      for (i = 0; i < 4; ++i) {
5699          e = v[i][0];
5700          f = v[i][1];
5701          g = v[i][2];
5702          h = v[i][3];
5703          v[i][0] = SSE2NEON_MULTIPLY(e, 0x0e) ^ SSE2NEON_MULTIPLY(f, 0x0b) ^
5704                    SSE2NEON_MULTIPLY(g, 0x0d) ^ SSE2NEON_MULTIPLY(h, 0x09);
5705          v[i][1] = SSE2NEON_MULTIPLY(e, 0x09) ^ SSE2NEON_MULTIPLY(f, 0x0e) ^
5706                    SSE2NEON_MULTIPLY(g, 0x0b) ^ SSE2NEON_MULTIPLY(h, 0x0d);
5707          v[i][2] = SSE2NEON_MULTIPLY(e, 0x0d) ^ SSE2NEON_MULTIPLY(f, 0x09) ^
5708                    SSE2NEON_MULTIPLY(g, 0x0e) ^ SSE2NEON_MULTIPLY(h, 0x0b);
5709          v[i][3] = SSE2NEON_MULTIPLY(e, 0x0b) ^ SSE2NEON_MULTIPLY(f, 0x0d) ^
5710                    SSE2NEON_MULTIPLY(g, 0x09) ^ SSE2NEON_MULTIPLY(h, 0x0e);
5711      }
5712      return vreinterpretq_m128i_u8(vld1q_u8((uint8_t *) v));
5713  #endif
5714  }
5715  FORCE_INLINE __m128i _mm_aeskeygenassist_si128(__m128i a, const int rcon)
5716  {
5717  #if defined(__aarch64__)
5718      uint8x16_t _a = vreinterpretq_u8_m128i(a);
5719      uint8x16_t v = vqtbl4q_u8(_sse2neon_vld1q_u8_x4(_sse2neon_sbox), _a);
5720      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x40), _a - 0x40);
5721      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0x80), _a - 0x80);
5722      v = vqtbx4q_u8(v, _sse2neon_vld1q_u8_x4(_sse2neon_sbox + 0xc0), _a - 0xc0);
5723      uint32x4_t v_u32 = vreinterpretq_u32_u8(v);
5724      uint32x4_t ror_v = vorrq_u32(vshrq_n_u32(v_u32, 8), vshlq_n_u32(v_u32, 24));
5725      uint32x4_t ror_xor_v = veorq_u32(ror_v, vdupq_n_u32(rcon));
5726      return vreinterpretq_m128i_u32(vtrn2q_u32(v_u32, ror_xor_v));
5727  #else &bsol;* ARMv7-A NEON implementation */
5728      uint32_t X1 = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0x55));
5729      uint32_t X3 = _mm_cvtsi128_si32(_mm_shuffle_epi32(a, 0xFF));
5730      for (int i = 0; i < 4; ++i) {
5731          ((uint8_t *) &X1)[i] = _sse2neon_sbox[((uint8_t *) &X1)[i]];
5732          ((uint8_t *) &X3)[i] = _sse2neon_sbox[((uint8_t *) &X3)[i]];
5733      }
5734      return _mm_set_epi32(((X3 >> 8) | (X3 << 24)) ^ rcon, X3,
5735                           ((X1 >> 8) | (X1 << 24)) ^ rcon, X1);
5736  #endif
5737  }
5738  #undef SSE2NEON_AES_SBOX
5739  #undef SSE2NEON_AES_RSBOX
5740  #if defined(__aarch64__)
5741  #undef SSE2NEON_XT
5742  #undef SSE2NEON_MULTIPLY
5743  #endif
5744  #else &bsol;* __ARM_FEATURE_CRYPTO */
5745  FORCE_INLINE __m128i _mm_aesenc_si128(__m128i a, __m128i b)
5746  {
5747      return vreinterpretq_m128i_u8(
5748          vaesmcq_u8(vaeseq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0))) ^
5749          vreinterpretq_u8_m128i(b));
5750  }
5751  FORCE_INLINE __m128i _mm_aesdec_si128(__m128i a, __m128i RoundKey)
5752  {
5753      return vreinterpretq_m128i_u8(veorq_u8(
5754          vaesimcq_u8(vaesdq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0))),
5755          vreinterpretq_u8_m128i(RoundKey)));
5756  }
5757  FORCE_INLINE __m128i _mm_aesenclast_si128(__m128i a, __m128i RoundKey)
5758  {
5759      return _mm_xor_si128(vreinterpretq_m128i_u8(vaeseq_u8(
5760                               vreinterpretq_u8_m128i(a), vdupq_n_u8(0))),
5761                           RoundKey);
5762  }
5763  FORCE_INLINE __m128i _mm_aesdeclast_si128(__m128i a, __m128i RoundKey)
5764  {
5765      return vreinterpretq_m128i_u8(
5766          vaesdq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0)) ^
5767          vreinterpretq_u8_m128i(RoundKey));
5768  }
5769  FORCE_INLINE __m128i _mm_aesimc_si128(__m128i a)
5770  {
5771      return vreinterpretq_m128i_u8(vaesimcq_u8(vreinterpretq_u8_m128i(a)));
5772  }
5773  FORCE_INLINE __m128i _mm_aeskeygenassist_si128(__m128i a, const int rcon)
5774  {
5775      uint8x16_t u8 = vaeseq_u8(vreinterpretq_u8_m128i(a), vdupq_n_u8(0));
5776      uint8x16_t dest = {
5777          u8[0x4], u8[0x1], u8[0xE], u8[0xB],  
5778          u8[0x1], u8[0xE], u8[0xB], u8[0x4],  
5779          u8[0xC], u8[0x9], u8[0x6], u8[0x3],  
5780          u8[0x9], u8[0x6], u8[0x3], u8[0xC],  
5781      };
5782      uint32x4_t r = {0, (unsigned) rcon, 0, (unsigned) rcon};
5783      return vreinterpretq_m128i_u8(dest) ^ vreinterpretq_m128i_u32(r);
5784  }
5785  #endif
5786  FORCE_INLINE __m128i _mm_clmulepi64_si128(__m128i _a, __m128i _b, const int imm)
5787  {
5788      uint64x2_t a = vreinterpretq_u64_m128i(_a);
5789      uint64x2_t b = vreinterpretq_u64_m128i(_b);
5790      switch (imm & 0x11) {
5791      case 0x00:
5792          return vreinterpretq_m128i_u64(
5793              _sse2neon_vmull_p64(vget_low_u64(a), vget_low_u64(b)));
5794      case 0x01:
5795          return vreinterpretq_m128i_u64(
5796              _sse2neon_vmull_p64(vget_high_u64(a), vget_low_u64(b)));
5797      case 0x10:
5798          return vreinterpretq_m128i_u64(
5799              _sse2neon_vmull_p64(vget_low_u64(a), vget_high_u64(b)));
5800      case 0x11:
5801          return vreinterpretq_m128i_u64(
5802              _sse2neon_vmull_p64(vget_high_u64(a), vget_high_u64(b)));
5803      default:
5804          abort();
5805      }
5806  }
5807  FORCE_INLINE unsigned int _sse2neon_mm_get_denormals_zero_mode()
5808  {
5809      union {
5810          fpcr_bitfield field;
5811  #if defined(__aarch64__)
5812          uint64_t value;
5813  #else
5814          uint32_t value;
5815  #endif
5816      } r;
5817  #if defined(__aarch64__)
5818      __asm__ __volatile__("mrs %0, FPCR" : "=r"(r.value)); &bsol;* read */
5819  #else
5820      __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); &bsol;* read */
5821  #endif
5822      return r.field.bit24 ? _MM_DENORMALS_ZERO_ON : _MM_DENORMALS_ZERO_OFF;
5823  }
5824  FORCE_INLINE int _mm_popcnt_u32(unsigned int a)
5825  {
5826  #if defined(__aarch64__)
5827  #if __has_builtin(__builtin_popcount)
5828      return __builtin_popcount(a);
5829  #else
5830      return (int) vaddlv_u8(vcnt_u8(vcreate_u8((uint64_t) a)));
5831  #endif
5832  #else
5833      uint32_t count = 0;
5834      uint8x8_t input_val, count8x8_val;
5835      uint16x4_t count16x4_val;
5836      uint32x2_t count32x2_val;
5837      input_val = vld1_u8((uint8_t *) &a);
5838      count8x8_val = vcnt_u8(input_val);
5839      count16x4_val = vpaddl_u8(count8x8_val);
5840      count32x2_val = vpaddl_u16(count16x4_val);
5841      vst1_u32(&count, count32x2_val);
5842      return count;
5843  #endif
5844  }
5845  FORCE_INLINE int64_t _mm_popcnt_u64(uint64_t a)
5846  {
5847  #if defined(__aarch64__)
5848  #if __has_builtin(__builtin_popcountll)
5849      return __builtin_popcountll(a);
5850  #else
5851      return (int64_t) vaddlv_u8(vcnt_u8(vcreate_u8(a)));
5852  #endif
5853  #else
5854      uint64_t count = 0;
5855      uint8x8_t input_val, count8x8_val;
5856      uint16x4_t count16x4_val;
5857      uint32x2_t count32x2_val;
5858      uint64x1_t count64x1_val;
5859      input_val = vld1_u8((uint8_t *) &a);
5860      count8x8_val = vcnt_u8(input_val);
5861      count16x4_val = vpaddl_u8(count8x8_val);
5862      count32x2_val = vpaddl_u16(count16x4_val);
5863      count64x1_val = vpaddl_u32(count32x2_val);
5864      vst1_u64(&count, count64x1_val);
5865      return count;
5866  #endif
5867  }
5868  FORCE_INLINE void _sse2neon_mm_set_denormals_zero_mode(unsigned int flag)
5869  {
5870      union {
5871          fpcr_bitfield field;
5872  #if defined(__aarch64__)
5873          uint64_t value;
5874  #else
5875          uint32_t value;
5876  #endif
5877      } r;
5878  #if defined(__aarch64__)
5879      __asm__ __volatile__("mrs %0, FPCR" : "=r"(r.value)); &bsol;* read */
5880  #else
5881      __asm__ __volatile__("vmrs %0, FPSCR" : "=r"(r.value)); &bsol;* read */
5882  #endif
5883      r.field.bit24 = (flag & _MM_DENORMALS_ZERO_MASK) == _MM_DENORMALS_ZERO_ON;
5884  #if defined(__aarch64__)
5885      __asm__ __volatile__("msr FPCR, %0" ::"r"(r)); &bsol;* write */
5886  #else
5887      __asm__ __volatile__("vmsr FPSCR, %0" ::"r"(r));        &bsol;* write */
5888  #endif
5889  }
5890  FORCE_INLINE uint64_t _rdtsc(void)
5891  {
5892  #if defined(__aarch64__)
5893      uint64_t val;
5894      __asm__ __volatile__("mrs %0, cntvct_el0" : "=r"(val));
5895      return val;
5896  #else
5897      uint32_t pmccntr, pmuseren, pmcntenset;
5898      __asm__ __volatile__("mrc p15, 0, %0, c9, c14, 0" : "=r"(pmuseren));
5899      if (pmuseren & 1) {  
5900          __asm__ __volatile__("mrc p15, 0, %0, c9, c12, 1" : "=r"(pmcntenset));
5901          if (pmcntenset & 0x80000000UL) {  
5902              __asm__ __volatile__("mrc p15, 0, %0, c9, c13, 0" : "=r"(pmccntr));
5903              return (uint64_t) (pmccntr) << 6;
5904          }
5905      }
5906      struct timeval tv;
5907      gettimeofday(&tv, NULL);
5908      return (uint64_t) (tv.tv_sec) * 1000000 + tv.tv_usec;
5909  #endif
5910  }
5911  #if defined(__GNUC__) || defined(__clang__)
5912  #pragma pop_macro("ALIGN_STRUCT")
5913  #pragma pop_macro("FORCE_INLINE")
5914  #endif
5915  #if defined(__GNUC__) && !defined(__clang__)
5916  #pragma GCC pop_options
5917  #endif
5918  #endif
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from Adafruit_nRF52_Arduino-MDEwOlJlcG9zaXRvcnk3NDM1NDcyOQ==-flat-SEGGER_RTT.c</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sse2neon.h</div>
                <div class="column column_space"><pre><code>260    if (RdOff > WrOff) {
261      NumBytesRem = pRing->SizeOfBuffer - RdOff;
262      NumBytesRem = MIN(NumBytesRem, BufferSize);
263  #if SEGGER_RTT_MEMCPY_USE_BYTELOOP
264      pSrc = pRing->pBuffer + RdOff;
265      NumBytesRead += NumBytesRem;
266      BufferSize   -= NumBytesRem;
267      RdOff        += NumBytesRem;
</pre></code></div>
                <div class="column column_space"><pre><code>4648      if (rounding == (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) ||
4649          (rounding == _MM_FROUND_CUR_DIRECTION &&
4650           _MM_GET_ROUNDING_MODE() == _MM_ROUND_NEAREST)) {
4651          uint32x4_t signmask = vdupq_n_u32(0x80000000);
4652          float32x4_t half = vbslq_f32(signmask, vreinterpretq_f32_m128(a),
4653                                       vdupq_n_f32(0.5f)); &bsol;* +/- 0.5 */
4654          int32x4_t r_normal = vcvtq_s32_f32(vaddq_f32(
4655              vreinterpretq_f32_m128(a), half)); &bsol;* round to integer: [a + 0.5]*/
4656          int32x4_t r_trunc = vcvtq_s32_f32(
4657              vreinterpretq_f32_m128(a)); &bsol;* truncate to integer: [a] */
4658          int32x4_t plusone = vreinterpretq_s32_u32(vshrq_n_u32(
4659              vreinterpretq_u32_s32(vnegq_s32(r_trunc)), 31)); &bsol;* 1 or 0 */
4660          int32x4_t r_even = vbicq_s32(vaddq_s32(r_trunc, plusone),
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    