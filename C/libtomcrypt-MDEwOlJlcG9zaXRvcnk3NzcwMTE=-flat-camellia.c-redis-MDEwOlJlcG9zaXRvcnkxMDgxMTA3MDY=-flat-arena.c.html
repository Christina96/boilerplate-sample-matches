
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 5.19893899204244%, Tokens: 8</h2>
        <div class="column">
            <h3>libtomcrypt-MDEwOlJlcG9zaXRvcnk3NzcwMTE=-flat-camellia.c</h3>
            <pre><code>1  #include "tomcrypt_private.h"
2  #ifdef LTC_CAMELLIA
3  const struct ltc_cipher_descriptor camellia_desc = {
4     "camellia",
5     23,
6     16, 32, 16, 18,
7     &camellia_setup,
8     &camellia_ecb_encrypt,
9     &camellia_ecb_decrypt,
10     &camellia_test,
11     &camellia_done,
12     &camellia_keysize,
13     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
14  };
15  static const ulong32 SP1110[] = {
16  0x70707000, 0x82828200, 0x2c2c2c00, 0xececec00, 0xb3b3b300, 0x27272700, 0xc0c0c000, 0xe5e5e500,
17  0xe4e4e400, 0x85858500, 0x57575700, 0x35353500, 0xeaeaea00, 0x0c0c0c00, 0xaeaeae00, 0x41414100,
18  0x23232300, 0xefefef00, 0x6b6b6b00, 0x93939300, 0x45454500, 0x19191900, 0xa5a5a500, 0x21212100,
19  0xededed00, 0x0e0e0e00, 0x4f4f4f00, 0x4e4e4e00, 0x1d1d1d00, 0x65656500, 0x92929200, 0xbdbdbd00,
20  0x86868600, 0xb8b8b800, 0xafafaf00, 0x8f8f8f00, 0x7c7c7c00, 0xebebeb00, 0x1f1f1f00, 0xcecece00,
21  0x3e3e3e00, 0x30303000, 0xdcdcdc00, 0x5f5f5f00, 0x5e5e5e00, 0xc5c5c500, 0x0b0b0b00, 0x1a1a1a00,
22  0xa6a6a600, 0xe1e1e100, 0x39393900, 0xcacaca00, 0xd5d5d500, 0x47474700, 0x5d5d5d00, 0x3d3d3d00,
23  0xd9d9d900, 0x01010100, 0x5a5a5a00, 0xd6d6d600, 0x51515100, 0x56565600, 0x6c6c6c00, 0x4d4d4d00,
24  0x8b8b8b00, 0x0d0d0d00, 0x9a9a9a00, 0x66666600, 0xfbfbfb00, 0xcccccc00, 0xb0b0b000, 0x2d2d2d00,
25  0x74747400, 0x12121200, 0x2b2b2b00, 0x20202000, 0xf0f0f000, 0xb1b1b100, 0x84848400, 0x99999900,
26  0xdfdfdf00, 0x4c4c4c00, 0xcbcbcb00, 0xc2c2c200, 0x34343400, 0x7e7e7e00, 0x76767600, 0x05050500,
27  0x6d6d6d00, 0xb7b7b700, 0xa9a9a900, 0x31313100, 0xd1d1d100, 0x17171700, 0x04040400, 0xd7d7d700,
28  0x14141400, 0x58585800, 0x3a3a3a00, 0x61616100, 0xdedede00, 0x1b1b1b00, 0x11111100, 0x1c1c1c00,
29  0x32323200, 0x0f0f0f00, 0x9c9c9c00, 0x16161600, 0x53535300, 0x18181800, 0xf2f2f200, 0x22222200,
30  0xfefefe00, 0x44444400, 0xcfcfcf00, 0xb2b2b200, 0xc3c3c300, 0xb5b5b500, 0x7a7a7a00, 0x91919100,
31  0x24242400, 0x08080800, 0xe8e8e800, 0xa8a8a800, 0x60606000, 0xfcfcfc00, 0x69696900, 0x50505000,
32  0xaaaaaa00, 0xd0d0d000, 0xa0a0a000, 0x7d7d7d00, 0xa1a1a100, 0x89898900, 0x62626200, 0x97979700,
33  0x54545400, 0x5b5b5b00, 0x1e1e1e00, 0x95959500, 0xe0e0e000, 0xffffff00, 0x64646400, 0xd2d2d200,
34  0x10101000, 0xc4c4c400, 0x00000000, 0x48484800, 0xa3a3a300, 0xf7f7f700, 0x75757500, 0xdbdbdb00,
35  0x8a8a8a00, 0x03030300, 0xe6e6e600, 0xdadada00, 0x09090900, 0x3f3f3f00, 0xdddddd00, 0x94949400,
36  0x87878700, 0x5c5c5c00, 0x83838300, 0x02020200, 0xcdcdcd00, 0x4a4a4a00, 0x90909000, 0x33333300,
37  0x73737300, 0x67676700, 0xf6f6f600, 0xf3f3f300, 0x9d9d9d00, 0x7f7f7f00, 0xbfbfbf00, 0xe2e2e200,
38  0x52525200, 0x9b9b9b00, 0xd8d8d800, 0x26262600, 0xc8c8c800, 0x37373700, 0xc6c6c600, 0x3b3b3b00,
39  0x81818100, 0x96969600, 0x6f6f6f00, 0x4b4b4b00, 0x13131300, 0xbebebe00, 0x63636300, 0x2e2e2e00,
40  0xe9e9e900, 0x79797900, 0xa7a7a700, 0x8c8c8c00, 0x9f9f9f00, 0x6e6e6e00, 0xbcbcbc00, 0x8e8e8e00,
41  0x29292900, 0xf5f5f500, 0xf9f9f900, 0xb6b6b600, 0x2f2f2f00, 0xfdfdfd00, 0xb4b4b400, 0x59595900,
42  0x78787800, 0x98989800, 0x06060600, 0x6a6a6a00, 0xe7e7e700, 0x46464600, 0x71717100, 0xbababa00,
43  0xd4d4d400, 0x25252500, 0xababab00, 0x42424200, 0x88888800, 0xa2a2a200, 0x8d8d8d00, 0xfafafa00,
44  0x72727200, 0x07070700, 0xb9b9b900, 0x55555500, 0xf8f8f800, 0xeeeeee00, 0xacacac00, 0x0a0a0a00,
45  0x36363600, 0x49494900, 0x2a2a2a00, 0x68686800, 0x3c3c3c00, 0x38383800, 0xf1f1f100, 0xa4a4a400,
46  0x40404000, 0x28282800, 0xd3d3d300, 0x7b7b7b00, 0xbbbbbb00, 0xc9c9c900, 0x43434300, 0xc1c1c100,
47  0x15151500, 0xe3e3e300, 0xadadad00, 0xf4f4f400, 0x77777700, 0xc7c7c700, 0x80808000, 0x9e9e9e00,
48  };
49  static const ulong32 SP0222[] = {
50  0x00e0e0e0, 0x00050505, 0x00585858, 0x00d9d9d9, 0x00676767, 0x004e4e4e, 0x00818181, 0x00cbcbcb,
51  0x00c9c9c9, 0x000b0b0b, 0x00aeaeae, 0x006a6a6a, 0x00d5d5d5, 0x00181818, 0x005d5d5d, 0x00828282,
52  0x00464646, 0x00dfdfdf, 0x00d6d6d6, 0x00272727, 0x008a8a8a, 0x00323232, 0x004b4b4b, 0x00424242,
53  0x00dbdbdb, 0x001c1c1c, 0x009e9e9e, 0x009c9c9c, 0x003a3a3a, 0x00cacaca, 0x00252525, 0x007b7b7b,
54  0x000d0d0d, 0x00717171, 0x005f5f5f, 0x001f1f1f, 0x00f8f8f8, 0x00d7d7d7, 0x003e3e3e, 0x009d9d9d,
55  0x007c7c7c, 0x00606060, 0x00b9b9b9, 0x00bebebe, 0x00bcbcbc, 0x008b8b8b, 0x00161616, 0x00343434,
56  0x004d4d4d, 0x00c3c3c3, 0x00727272, 0x00959595, 0x00ababab, 0x008e8e8e, 0x00bababa, 0x007a7a7a,
57  0x00b3b3b3, 0x00020202, 0x00b4b4b4, 0x00adadad, 0x00a2a2a2, 0x00acacac, 0x00d8d8d8, 0x009a9a9a,
58  0x00171717, 0x001a1a1a, 0x00353535, 0x00cccccc, 0x00f7f7f7, 0x00999999, 0x00616161, 0x005a5a5a,
59  0x00e8e8e8, 0x00242424, 0x00565656, 0x00404040, 0x00e1e1e1, 0x00636363, 0x00090909, 0x00333333,
60  0x00bfbfbf, 0x00989898, 0x00979797, 0x00858585, 0x00686868, 0x00fcfcfc, 0x00ececec, 0x000a0a0a,
61  0x00dadada, 0x006f6f6f, 0x00535353, 0x00626262, 0x00a3a3a3, 0x002e2e2e, 0x00080808, 0x00afafaf,
62  0x00282828, 0x00b0b0b0, 0x00747474, 0x00c2c2c2, 0x00bdbdbd, 0x00363636, 0x00222222, 0x00383838,
63  0x00646464, 0x001e1e1e, 0x00393939, 0x002c2c2c, 0x00a6a6a6, 0x00303030, 0x00e5e5e5, 0x00444444,
64  0x00fdfdfd, 0x00888888, 0x009f9f9f, 0x00656565, 0x00878787, 0x006b6b6b, 0x00f4f4f4, 0x00232323,
65  0x00484848, 0x00101010, 0x00d1d1d1, 0x00515151, 0x00c0c0c0, 0x00f9f9f9, 0x00d2d2d2, 0x00a0a0a0,
66  0x00555555, 0x00a1a1a1, 0x00414141, 0x00fafafa, 0x00434343, 0x00131313, 0x00c4c4c4, 0x002f2f2f,
67  0x00a8a8a8, 0x00b6b6b6, 0x003c3c3c, 0x002b2b2b, 0x00c1c1c1, 0x00ffffff, 0x00c8c8c8, 0x00a5a5a5,
68  0x00202020, 0x00898989, 0x00000000, 0x00909090, 0x00474747, 0x00efefef, 0x00eaeaea, 0x00b7b7b7,
69  0x00151515, 0x00060606, 0x00cdcdcd, 0x00b5b5b5, 0x00121212, 0x007e7e7e, 0x00bbbbbb, 0x00292929,
70  0x000f0f0f, 0x00b8b8b8, 0x00070707, 0x00040404, 0x009b9b9b, 0x00949494, 0x00212121, 0x00666666,
71  0x00e6e6e6, 0x00cecece, 0x00ededed, 0x00e7e7e7, 0x003b3b3b, 0x00fefefe, 0x007f7f7f, 0x00c5c5c5,
72  0x00a4a4a4, 0x00373737, 0x00b1b1b1, 0x004c4c4c, 0x00919191, 0x006e6e6e, 0x008d8d8d, 0x00767676,
73  0x00030303, 0x002d2d2d, 0x00dedede, 0x00969696, 0x00262626, 0x007d7d7d, 0x00c6c6c6, 0x005c5c5c,
74  0x00d3d3d3, 0x00f2f2f2, 0x004f4f4f, 0x00191919, 0x003f3f3f, 0x00dcdcdc, 0x00797979, 0x001d1d1d,
75  0x00525252, 0x00ebebeb, 0x00f3f3f3, 0x006d6d6d, 0x005e5e5e, 0x00fbfbfb, 0x00696969, 0x00b2b2b2,
76  0x00f0f0f0, 0x00313131, 0x000c0c0c, 0x00d4d4d4, 0x00cfcfcf, 0x008c8c8c, 0x00e2e2e2, 0x00757575,
77  0x00a9a9a9, 0x004a4a4a, 0x00575757, 0x00848484, 0x00111111, 0x00454545, 0x001b1b1b, 0x00f5f5f5,
78  0x00e4e4e4, 0x000e0e0e, 0x00737373, 0x00aaaaaa, 0x00f1f1f1, 0x00dddddd, 0x00595959, 0x00141414,
79  0x006c6c6c, 0x00929292, 0x00545454, 0x00d0d0d0, 0x00787878, 0x00707070, 0x00e3e3e3, 0x00494949,
80  0x00808080, 0x00505050, 0x00a7a7a7, 0x00f6f6f6, 0x00777777, 0x00939393, 0x00868686, 0x00838383,
81  0x002a2a2a, 0x00c7c7c7, 0x005b5b5b, 0x00e9e9e9, 0x00eeeeee, 0x008f8f8f, 0x00010101, 0x003d3d3d,
82  };
83  static const ulong32 SP3033[] = {
84  0x38003838, 0x41004141, 0x16001616, 0x76007676, 0xd900d9d9, 0x93009393, 0x60006060, 0xf200f2f2,
85  0x72007272, 0xc200c2c2, 0xab00abab, 0x9a009a9a, 0x75007575, 0x06000606, 0x57005757, 0xa000a0a0,
86  0x91009191, 0xf700f7f7, 0xb500b5b5, 0xc900c9c9, 0xa200a2a2, 0x8c008c8c, 0xd200d2d2, 0x90009090,
87  0xf600f6f6, 0x07000707, 0xa700a7a7, 0x27002727, 0x8e008e8e, 0xb200b2b2, 0x49004949, 0xde00dede,
88  0x43004343, 0x5c005c5c, 0xd700d7d7, 0xc700c7c7, 0x3e003e3e, 0xf500f5f5, 0x8f008f8f, 0x67006767,
89  0x1f001f1f, 0x18001818, 0x6e006e6e, 0xaf00afaf, 0x2f002f2f, 0xe200e2e2, 0x85008585, 0x0d000d0d,
90  0x53005353, 0xf000f0f0, 0x9c009c9c, 0x65006565, 0xea00eaea, 0xa300a3a3, 0xae00aeae, 0x9e009e9e,
91  0xec00ecec, 0x80008080, 0x2d002d2d, 0x6b006b6b, 0xa800a8a8, 0x2b002b2b, 0x36003636, 0xa600a6a6,
92  0xc500c5c5, 0x86008686, 0x4d004d4d, 0x33003333, 0xfd00fdfd, 0x66006666, 0x58005858, 0x96009696,
93  0x3a003a3a, 0x09000909, 0x95009595, 0x10001010, 0x78007878, 0xd800d8d8, 0x42004242, 0xcc00cccc,
94  0xef00efef, 0x26002626, 0xe500e5e5, 0x61006161, 0x1a001a1a, 0x3f003f3f, 0x3b003b3b, 0x82008282,
95  0xb600b6b6, 0xdb00dbdb, 0xd400d4d4, 0x98009898, 0xe800e8e8, 0x8b008b8b, 0x02000202, 0xeb00ebeb,
96  0x0a000a0a, 0x2c002c2c, 0x1d001d1d, 0xb000b0b0, 0x6f006f6f, 0x8d008d8d, 0x88008888, 0x0e000e0e,
97  0x19001919, 0x87008787, 0x4e004e4e, 0x0b000b0b, 0xa900a9a9, 0x0c000c0c, 0x79007979, 0x11001111,
98  0x7f007f7f, 0x22002222, 0xe700e7e7, 0x59005959, 0xe100e1e1, 0xda00dada, 0x3d003d3d, 0xc800c8c8,
99  0x12001212, 0x04000404, 0x74007474, 0x54005454, 0x30003030, 0x7e007e7e, 0xb400b4b4, 0x28002828,
100  0x55005555, 0x68006868, 0x50005050, 0xbe00bebe, 0xd000d0d0, 0xc400c4c4, 0x31003131, 0xcb00cbcb,
101  0x2a002a2a, 0xad00adad, 0x0f000f0f, 0xca00caca, 0x70007070, 0xff00ffff, 0x32003232, 0x69006969,
102  0x08000808, 0x62006262, 0x00000000, 0x24002424, 0xd100d1d1, 0xfb00fbfb, 0xba00baba, 0xed00eded,
103  0x45004545, 0x81008181, 0x73007373, 0x6d006d6d, 0x84008484, 0x9f009f9f, 0xee00eeee, 0x4a004a4a,
104  0xc300c3c3, 0x2e002e2e, 0xc100c1c1, 0x01000101, 0xe600e6e6, 0x25002525, 0x48004848, 0x99009999,
105  0xb900b9b9, 0xb300b3b3, 0x7b007b7b, 0xf900f9f9, 0xce00cece, 0xbf00bfbf, 0xdf00dfdf, 0x71007171,
106  0x29002929, 0xcd00cdcd, 0x6c006c6c, 0x13001313, 0x64006464, 0x9b009b9b, 0x63006363, 0x9d009d9d,
107  0xc000c0c0, 0x4b004b4b, 0xb700b7b7, 0xa500a5a5, 0x89008989, 0x5f005f5f, 0xb100b1b1, 0x17001717,
108  0xf400f4f4, 0xbc00bcbc, 0xd300d3d3, 0x46004646, 0xcf00cfcf, 0x37003737, 0x5e005e5e, 0x47004747,
109  0x94009494, 0xfa00fafa, 0xfc00fcfc, 0x5b005b5b, 0x97009797, 0xfe00fefe, 0x5a005a5a, 0xac00acac,
110  0x3c003c3c, 0x4c004c4c, 0x03000303, 0x35003535, 0xf300f3f3, 0x23002323, 0xb800b8b8, 0x5d005d5d,
111  0x6a006a6a, 0x92009292, 0xd500d5d5, 0x21002121, 0x44004444, 0x51005151, 0xc600c6c6, 0x7d007d7d,
112  0x39003939, 0x83008383, 0xdc00dcdc, 0xaa00aaaa, 0x7c007c7c, 0x77007777, 0x56005656, 0x05000505,
113  0x1b001b1b, 0xa400a4a4, 0x15001515, 0x34003434, 0x1e001e1e, 0x1c001c1c, 0xf800f8f8, 0x52005252,
114  0x20002020, 0x14001414, 0xe900e9e9, 0xbd00bdbd, 0xdd00dddd, 0xe400e4e4, 0xa100a1a1, 0xe000e0e0,
115  0x8a008a8a, 0xf100f1f1, 0xd600d6d6, 0x7a007a7a, 0xbb00bbbb, 0xe300e3e3, 0x40004040, 0x4f004f4f,
116  };
117  static const ulong32 SP4404[] = {
118  0x70700070, 0x2c2c002c, 0xb3b300b3, 0xc0c000c0, 0xe4e400e4, 0x57570057, 0xeaea00ea, 0xaeae00ae,
119  0x23230023, 0x6b6b006b, 0x45450045, 0xa5a500a5, 0xeded00ed, 0x4f4f004f, 0x1d1d001d, 0x92920092,
120  0x86860086, 0xafaf00af, 0x7c7c007c, 0x1f1f001f, 0x3e3e003e, 0xdcdc00dc, 0x5e5e005e, 0x0b0b000b,
121  0xa6a600a6, 0x39390039, 0xd5d500d5, 0x5d5d005d, 0xd9d900d9, 0x5a5a005a, 0x51510051, 0x6c6c006c,
122  0x8b8b008b, 0x9a9a009a, 0xfbfb00fb, 0xb0b000b0, 0x74740074, 0x2b2b002b, 0xf0f000f0, 0x84840084,
123  0xdfdf00df, 0xcbcb00cb, 0x34340034, 0x76760076, 0x6d6d006d, 0xa9a900a9, 0xd1d100d1, 0x04040004,
124  0x14140014, 0x3a3a003a, 0xdede00de, 0x11110011, 0x32320032, 0x9c9c009c, 0x53530053, 0xf2f200f2,
125  0xfefe00fe, 0xcfcf00cf, 0xc3c300c3, 0x7a7a007a, 0x24240024, 0xe8e800e8, 0x60600060, 0x69690069,
126  0xaaaa00aa, 0xa0a000a0, 0xa1a100a1, 0x62620062, 0x54540054, 0x1e1e001e, 0xe0e000e0, 0x64640064,
127  0x10100010, 0x00000000, 0xa3a300a3, 0x75750075, 0x8a8a008a, 0xe6e600e6, 0x09090009, 0xdddd00dd,
128  0x87870087, 0x83830083, 0xcdcd00cd, 0x90900090, 0x73730073, 0xf6f600f6, 0x9d9d009d, 0xbfbf00bf,
129  0x52520052, 0xd8d800d8, 0xc8c800c8, 0xc6c600c6, 0x81810081, 0x6f6f006f, 0x13130013, 0x63630063,
130  0xe9e900e9, 0xa7a700a7, 0x9f9f009f, 0xbcbc00bc, 0x29290029, 0xf9f900f9, 0x2f2f002f, 0xb4b400b4,
131  0x78780078, 0x06060006, 0xe7e700e7, 0x71710071, 0xd4d400d4, 0xabab00ab, 0x88880088, 0x8d8d008d,
132  0x72720072, 0xb9b900b9, 0xf8f800f8, 0xacac00ac, 0x36360036, 0x2a2a002a, 0x3c3c003c, 0xf1f100f1,
133  0x40400040, 0xd3d300d3, 0xbbbb00bb, 0x43430043, 0x15150015, 0xadad00ad, 0x77770077, 0x80800080,
134  0x82820082, 0xecec00ec, 0x27270027, 0xe5e500e5, 0x85850085, 0x35350035, 0x0c0c000c, 0x41410041,
135  0xefef00ef, 0x93930093, 0x19190019, 0x21210021, 0x0e0e000e, 0x4e4e004e, 0x65650065, 0xbdbd00bd,
136  0xb8b800b8, 0x8f8f008f, 0xebeb00eb, 0xcece00ce, 0x30300030, 0x5f5f005f, 0xc5c500c5, 0x1a1a001a,
137  0xe1e100e1, 0xcaca00ca, 0x47470047, 0x3d3d003d, 0x01010001, 0xd6d600d6, 0x56560056, 0x4d4d004d,
138  0x0d0d000d, 0x66660066, 0xcccc00cc, 0x2d2d002d, 0x12120012, 0x20200020, 0xb1b100b1, 0x99990099,
139  0x4c4c004c, 0xc2c200c2, 0x7e7e007e, 0x05050005, 0xb7b700b7, 0x31310031, 0x17170017, 0xd7d700d7,
140  0x58580058, 0x61610061, 0x1b1b001b, 0x1c1c001c, 0x0f0f000f, 0x16160016, 0x18180018, 0x22220022,
141  0x44440044, 0xb2b200b2, 0xb5b500b5, 0x91910091, 0x08080008, 0xa8a800a8, 0xfcfc00fc, 0x50500050,
142  0xd0d000d0, 0x7d7d007d, 0x89890089, 0x97970097, 0x5b5b005b, 0x95950095, 0xffff00ff, 0xd2d200d2,
143  0xc4c400c4, 0x48480048, 0xf7f700f7, 0xdbdb00db, 0x03030003, 0xdada00da, 0x3f3f003f, 0x94940094,
144  0x5c5c005c, 0x02020002, 0x4a4a004a, 0x33330033, 0x67670067, 0xf3f300f3, 0x7f7f007f, 0xe2e200e2,
145  0x9b9b009b, 0x26260026, 0x37370037, 0x3b3b003b, 0x96960096, 0x4b4b004b, 0xbebe00be, 0x2e2e002e,
146  0x79790079, 0x8c8c008c, 0x6e6e006e, 0x8e8e008e, 0xf5f500f5, 0xb6b600b6, 0xfdfd00fd, 0x59590059,
147  0x98980098, 0x6a6a006a, 0x46460046, 0xbaba00ba, 0x25250025, 0x42420042, 0xa2a200a2, 0xfafa00fa,
148  0x07070007, 0x55550055, 0xeeee00ee, 0x0a0a000a, 0x49490049, 0x68680068, 0x38380038, 0xa4a400a4,
149  0x28280028, 0x7b7b007b, 0xc9c900c9, 0xc1c100c1, 0xe3e300e3, 0xf4f400f4, 0xc7c700c7, 0x9e9e009e,
150  };
151  static const ulong64 key_sigma[] = {
152     CONST64(0xA09E667F3BCC908B),
153     CONST64(0xB67AE8584CAA73B2),
154     CONST64(0xC6EF372FE94F82BE),
155     CONST64(0x54FF53A5F1D36F1C),
156     CONST64(0x10E527FADE682D1D),
157     CONST64(0xB05688C2B3E6C1FD)
158  };
159  static ulong64 F(ulong64 x)
160  {
161     ulong32 D, U;
162  #define loc(i) ((8-i)*8)
163     D = SP1110[(x >> loc(8)) & 0xFF] ^ SP0222[(x >> loc(5)) & 0xFF] ^ SP3033[(x >> loc(6)) & 0xFF] ^ SP4404[(x >> loc(7)) & 0xFF];
164     U = SP1110[(x >> loc(1)) & 0xFF] ^ SP0222[(x >> loc(2)) & 0xFF] ^ SP3033[(x >> loc(3)) & 0xFF] ^ SP4404[(x >> loc(4)) & 0xFF];
165     D ^= U;
166     U = D ^ RORc(U, 8);
167     return ((ulong64)U) | (((ulong64)D) << CONST64(32));
168  }
169  static void rot_128(const unsigned char *in, unsigned count, unsigned char *out)
170  {
171     unsigned x, w, b;
172     w = count >> 3;
173     b = count & 7;
174     for (x = 0; x < 16; x++) {
175        out[x] = (in[(x+w)&15] << b) | (in[(x+w+1)&15] >> (8 - b));
176     }
177  }
178  int camellia_setup(const unsigned char *key, int keylen, int num_rounds, symmetric_key *skey)
179  {
180     unsigned char T[48], kA[16], kB[16], kR[16], kL[16];
181     int           x;
182     ulong64       A, B;
183     LTC_ARGCHK(key  != NULL);
184     LTC_ARGCHK(skey != NULL);
185     if (keylen != 16 && keylen != 24 && keylen != 32) {
186        return CRYPT_INVALID_KEYSIZE;
187     }
188     skey->camellia.R = (keylen == 16) ? 18 : 24;
189     if (num_rounds != 0 && num_rounds != skey->camellia.R) {
190        return CRYPT_INVALID_ROUNDS;
191     }
192     if (keylen == 16) {
193        for (x = 0; x < 16; x++) {
194            T[x]      = key[x];
195            T[x + 16] = 0;
196        }
197     } else if (keylen == 24) {
198        for (x = 0; x < 24; x++) {
199            T[x]      = key[x];
200        }
201        for (x = 24; x < 32; x++) {
202            T[x]      = key[x-8] ^ 0xFF;
203        }
204     } else {
205        for (x = 0; x < 32; x++) {
206            T[x]      = key[x];
207        }
208     }
209     for (x = 0; x < 16; x++) {
210        kL[x] = T[x];
211        kR[x] = T[x + 16];
212     }
213     for (x = 32; x < 48; x++) {
214        T[x] = T[x - 32] ^ T[x - 16];
215     }
216     LOAD64H(A, T+32); LOAD64H(B, T+40);
217     B ^= F(A ^ key_sigma[0]);
<span onclick='openModal()' class='match'>218     A ^= F(B ^ key_sigma[1]);
219     STORE64H(A, T+32); STORE64H(B, T+40);
220     for (x = 0; x < 16; x++) { T[x+32] ^= kL[x]; }
221     LOAD64H(A, T+32); LOAD64H(B, T+40);
222     B ^= F(A ^ key_sigma[2]);
</span>223     A ^= F(B ^ key_sigma[3]);
224     STORE64H(A, T+32); STORE64H(B, T+40);
225     for (x = 0; x < 16; x++) { kA[x] = T[x+32]; }
226     for (x = 0; x < 16; x++) { T[x+32] ^= kR[x]; }
227     if (keylen == 16) {
228        LOAD64H(skey->camellia.kw[0], kL);
229        LOAD64H(skey->camellia.kw[1], kL+8);
230        LOAD64H(skey->camellia.k[0], kA);
231        LOAD64H(skey->camellia.k[1], kA+8);
232        rot_128(kL, 15, T+32);
233        LOAD64H(skey->camellia.k[2], T+32);
234        LOAD64H(skey->camellia.k[3], T+40);
235        rot_128(kA, 15, T+32);
236        LOAD64H(skey->camellia.k[4], T+32);
237        LOAD64H(skey->camellia.k[5], T+40);
238        rot_128(kA, 30, T+32);
239        LOAD64H(skey->camellia.kl[0], T+32);
240        LOAD64H(skey->camellia.kl[1], T+40);
241        rot_128(kL, 45, T+32);
242        LOAD64H(skey->camellia.k[6], T+32);
243        LOAD64H(skey->camellia.k[7], T+40);
244        rot_128(kA, 45, T+32);
245        LOAD64H(skey->camellia.k[8], T+32);
246        rot_128(kL, 60, T+32);
247        LOAD64H(skey->camellia.k[9], T+40);
248        rot_128(kA, 60, T+32);
249        LOAD64H(skey->camellia.k[10], T+32);
250        LOAD64H(skey->camellia.k[11], T+40);
251        rot_128(kL, 77, T+32);
252        LOAD64H(skey->camellia.kl[2], T+32);
253        LOAD64H(skey->camellia.kl[3], T+40);
254        rot_128(kL, 94, T+32);
255        LOAD64H(skey->camellia.k[12], T+32);
256        LOAD64H(skey->camellia.k[13], T+40);
257        rot_128(kA, 94, T+32);
258        LOAD64H(skey->camellia.k[14], T+32);
259        LOAD64H(skey->camellia.k[15], T+40);
260        rot_128(kL, 111, T+32);
261        LOAD64H(skey->camellia.k[16], T+32);
262        LOAD64H(skey->camellia.k[17], T+40);
263        rot_128(kA, 111, T+32);
264        LOAD64H(skey->camellia.kw[2], T+32);
265        LOAD64H(skey->camellia.kw[3], T+40);
266     } else {
267        LOAD64H(A, T+32); LOAD64H(B, T+40);
268        B ^= F(A ^ key_sigma[4]);
269        A ^= F(B ^ key_sigma[5]);
270        STORE64H(A, T+32); STORE64H(B, T+40);
271        for (x = 0; x < 16; x++) { kB[x] = T[x+32]; }
272        LOAD64H(skey->camellia.kw[0], kL);
273        LOAD64H(skey->camellia.kw[1], kL+8);
274        LOAD64H(skey->camellia.k[0], kB);
275        LOAD64H(skey->camellia.k[1], kB+8);
276        rot_128(kR, 15, T+32);
277        LOAD64H(skey->camellia.k[2], T+32);
278        LOAD64H(skey->camellia.k[3], T+40);
279        rot_128(kA, 15, T+32);
280        LOAD64H(skey->camellia.k[4], T+32);
281        LOAD64H(skey->camellia.k[5], T+40);
282        rot_128(kR, 30, T+32);
283        LOAD64H(skey->camellia.kl[0], T+32);
284        LOAD64H(skey->camellia.kl[1], T+40);
285        rot_128(kB, 30, T+32);
286        LOAD64H(skey->camellia.k[6], T+32);
287        LOAD64H(skey->camellia.k[7], T+40);
288        rot_128(kL, 45, T+32);
289        LOAD64H(skey->camellia.k[8], T+32);
290        LOAD64H(skey->camellia.k[9], T+40);
291        rot_128(kA, 45, T+32);
292        LOAD64H(skey->camellia.k[10], T+32);
293        LOAD64H(skey->camellia.k[11], T+40);
294        rot_128(kL, 60, T+32);
295        LOAD64H(skey->camellia.kl[2], T+32);
296        LOAD64H(skey->camellia.kl[3], T+40);
297        rot_128(kR, 60, T+32);
298        LOAD64H(skey->camellia.k[12], T+32);
299        LOAD64H(skey->camellia.k[13], T+40);
300        rot_128(kB, 60, T+32);
301        LOAD64H(skey->camellia.k[14], T+32);
302        LOAD64H(skey->camellia.k[15], T+40);
303        rot_128(kL, 77, T+32);
304        LOAD64H(skey->camellia.k[16], T+32);
305        LOAD64H(skey->camellia.k[17], T+40);
306        rot_128(kA, 77, T+32);
307        LOAD64H(skey->camellia.kl[4], T+32);
308        LOAD64H(skey->camellia.kl[5], T+40);
309        rot_128(kR, 94, T+32);
310        LOAD64H(skey->camellia.k[18], T+32);
311        LOAD64H(skey->camellia.k[19], T+40);
312        rot_128(kA, 94, T+32);
313        LOAD64H(skey->camellia.k[20], T+32);
314        LOAD64H(skey->camellia.k[21], T+40);
315        rot_128(kL, 111, T+32);
316        LOAD64H(skey->camellia.k[22], T+32);
317        LOAD64H(skey->camellia.k[23], T+40);
318        rot_128(kB, 111, T+32);
319        LOAD64H(skey->camellia.kw[2], T+32);
320        LOAD64H(skey->camellia.kw[3], T+40);
321     }
322     return CRYPT_OK;
323  }
324  int camellia_ecb_encrypt(const unsigned char *pt, unsigned char *ct, const symmetric_key *skey)
325  {
326     ulong64 L, R;
327     ulong32 a, b;
328     LOAD64H(L, pt+0); LOAD64H(R, pt+8);
329     L ^= skey->camellia.kw[0];
330     R ^= skey->camellia.kw[1];
331     R ^= F(L ^ skey->camellia.k[0]);
332     L ^= F(R ^ skey->camellia.k[1]);
333     R ^= F(L ^ skey->camellia.k[2]);
334     L ^= F(R ^ skey->camellia.k[3]);
335     R ^= F(L ^ skey->camellia.k[4]);
336     L ^= F(R ^ skey->camellia.k[5]);
337     a = (ulong32)(L >> 32);
338     b = (ulong32)(L & 0xFFFFFFFFUL);
339     b ^= ROL((a & (ulong32)(skey->camellia.kl[0] >> 32)), 1);
340     a ^= b | (skey->camellia.kl[0] & 0xFFFFFFFFU);
341     L = (((ulong64)a) << 32) | b;
342     a = (ulong32)(R >> 32);
343     b = (ulong32)(R & 0xFFFFFFFFUL);
344     a ^= b | (skey->camellia.kl[1] & 0xFFFFFFFFU);
345     b ^= ROL((a & (ulong32)(skey->camellia.kl[1] >> 32)), 1);
346     R = (((ulong64)a) << 32) | b;
347     R ^= F(L ^ skey->camellia.k[6]);
348     L ^= F(R ^ skey->camellia.k[7]);
349     R ^= F(L ^ skey->camellia.k[8]);
350     L ^= F(R ^ skey->camellia.k[9]);
351     R ^= F(L ^ skey->camellia.k[10]);
352     L ^= F(R ^ skey->camellia.k[11]);
353     a = (ulong32)(L >> 32);
354     b = (ulong32)(L & 0xFFFFFFFFUL);
355     b ^= ROL((a & (ulong32)(skey->camellia.kl[2] >> 32)), 1);
356     a ^= b | (skey->camellia.kl[2] & 0xFFFFFFFFU);
357     L = (((ulong64)a) << 32) | b;
358     a = (ulong32)(R >> 32);
359     b = (ulong32)(R & 0xFFFFFFFFUL);
360     a ^= b | (skey->camellia.kl[3] & 0xFFFFFFFFU);
361     b ^= ROL((a & (ulong32)(skey->camellia.kl[3] >> 32)), 1);
362     R = (((ulong64)a) << 32) | b;
363     R ^= F(L ^ skey->camellia.k[12]);
364     L ^= F(R ^ skey->camellia.k[13]);
365     R ^= F(L ^ skey->camellia.k[14]);
366     L ^= F(R ^ skey->camellia.k[15]);
367     R ^= F(L ^ skey->camellia.k[16]);
368     L ^= F(R ^ skey->camellia.k[17]);
369     if (skey->camellia.R == 24) {
370        a = (ulong32)(L >> 32);
371        b = (ulong32)(L & 0xFFFFFFFFUL);
372        b ^= ROL((a & (ulong32)(skey->camellia.kl[4] >> 32)), 1);
373        a ^= b | (skey->camellia.kl[4] & 0xFFFFFFFFU);
374        L = (((ulong64)a) << 32) | b;
375        a = (ulong32)(R >> 32);
376        b = (ulong32)(R & 0xFFFFFFFFUL);
377        a ^= b | (skey->camellia.kl[5] & 0xFFFFFFFFU);
378        b ^= ROL((a & (ulong32)(skey->camellia.kl[5] >> 32)), 1);
379        R = (((ulong64)a) << 32) | b;
380        R ^= F(L ^ skey->camellia.k[18]);
381        L ^= F(R ^ skey->camellia.k[19]);
382        R ^= F(L ^ skey->camellia.k[20]);
383        L ^= F(R ^ skey->camellia.k[21]);
384        R ^= F(L ^ skey->camellia.k[22]);
385        L ^= F(R ^ skey->camellia.k[23]);
386     }
387     L ^= skey->camellia.kw[3];
388     R ^= skey->camellia.kw[2];
389     STORE64H(R, ct+0); STORE64H(L, ct+8);
390     return CRYPT_OK;
391  }
392  int camellia_ecb_decrypt(const unsigned char *ct, unsigned char *pt, const symmetric_key *skey)
393  {
394     ulong64 L, R;
395     ulong32 a, b;
396     LOAD64H(R, ct+0); LOAD64H(L, ct+8);
397     L ^= skey->camellia.kw[3];
398     R ^= skey->camellia.kw[2];
399     if (skey->camellia.R == 24) {
400        L ^= F(R ^ skey->camellia.k[23]);
401        R ^= F(L ^ skey->camellia.k[22]);
402        L ^= F(R ^ skey->camellia.k[21]);
403        R ^= F(L ^ skey->camellia.k[20]);
404        L ^= F(R ^ skey->camellia.k[19]);
405        R ^= F(L ^ skey->camellia.k[18]);
406        a = (ulong32)(L >> 32);
407        b = (ulong32)(L & 0xFFFFFFFFUL);
408        a ^= b | (skey->camellia.kl[4] & 0xFFFFFFFFU);
409        b ^= ROL((a & (ulong32)(skey->camellia.kl[4] >> 32)), 1);
410        L = (((ulong64)a) << 32) | b;
411        a = (ulong32)(R >> 32);
412        b = (ulong32)(R & 0xFFFFFFFFUL);
413        b ^= ROL((a & (ulong32)(skey->camellia.kl[5] >> 32)), 1);
414        a ^= b | (skey->camellia.kl[5] & 0xFFFFFFFFU);
415        R = (((ulong64)a) << 32) | b;
416     }
417     L ^= F(R ^ skey->camellia.k[17]);
418     R ^= F(L ^ skey->camellia.k[16]);
419     L ^= F(R ^ skey->camellia.k[15]);
420     R ^= F(L ^ skey->camellia.k[14]);
421     L ^= F(R ^ skey->camellia.k[13]);
422     R ^= F(L ^ skey->camellia.k[12]);
423     a = (ulong32)(L >> 32);
424     b = (ulong32)(L & 0xFFFFFFFFUL);
425     a ^= b | (skey->camellia.kl[2] & 0xFFFFFFFFU);
426     b ^= ROL((a & (ulong32)(skey->camellia.kl[2] >> 32)), 1);
427     L = (((ulong64)a) << 32) | b;
428     a = (ulong32)(R >> 32);
429     b = (ulong32)(R & 0xFFFFFFFFUL);
430     b ^= ROL((a & (ulong32)(skey->camellia.kl[3] >> 32)), 1);
431     a ^= b | (skey->camellia.kl[3] & 0xFFFFFFFFU);
432     R = (((ulong64)a) << 32) | b;
433     L ^= F(R ^ skey->camellia.k[11]);
434     R ^= F(L ^ skey->camellia.k[10]);
435     L ^= F(R ^ skey->camellia.k[9]);
436     R ^= F(L ^ skey->camellia.k[8]);
437     L ^= F(R ^ skey->camellia.k[7]);
438     R ^= F(L ^ skey->camellia.k[6]);
439     a = (ulong32)(L >> 32);
440     b = (ulong32)(L & 0xFFFFFFFFUL);
441     a ^= b | (skey->camellia.kl[0] & 0xFFFFFFFFU);
442     b ^= ROL((a & (ulong32)(skey->camellia.kl[0] >> 32)), 1);
443     L = (((ulong64)a) << 32) | b;
444     a = (ulong32)(R >> 32);
445     b = (ulong32)(R & 0xFFFFFFFFUL);
446     b ^= ROL((a & (ulong32)(skey->camellia.kl[1] >> 32)), 1);
447     a ^= b | (skey->camellia.kl[1] & 0xFFFFFFFFU);
448     R = (((ulong64)a) << 32) | b;
449     L ^= F(R ^ skey->camellia.k[5]);
450     R ^= F(L ^ skey->camellia.k[4]);
451     L ^= F(R ^ skey->camellia.k[3]);
452     R ^= F(L ^ skey->camellia.k[2]);
453     L ^= F(R ^ skey->camellia.k[1]);
454     R ^= F(L ^ skey->camellia.k[0]);
455     R ^= skey->camellia.kw[1];
456     L ^= skey->camellia.kw[0];
457     STORE64H(R, pt+8); STORE64H(L, pt+0);
458     return CRYPT_OK;
459  }
460  int camellia_test(void)
461  {
462  #ifndef LTC_TEST
463     return CRYPT_NOP;
464  #else
465     static const struct {
466        int keylen;
467        unsigned char key[32], pt[16], ct[16];
468     } tests[] = {
469  {
470     16,
471     { 0x01, 0x23, 0x45, 0x67, 0x89, 0xab, 0xcd, 0xef,
472       0xfe, 0xdc, 0xba, 0x98, 0x76, 0x54, 0x32, 0x10 },
473     { 0x01, 0x23, 0x45, 0x67, 0x89, 0xab, 0xcd, 0xef,
474       0xfe, 0xdc, 0xba, 0x98, 0x76, 0x54, 0x32, 0x10 },
475     { 0x67, 0x67, 0x31, 0x38, 0x54, 0x96, 0x69, 0x73,
476       0x08, 0x57, 0x06, 0x56, 0x48, 0xea, 0xbe, 0x43 }
477  },
478  {
479     24,
480     { 0x01, 0x23, 0x45, 0x67, 0x89, 0xab, 0xcd, 0xef,
481       0xfe, 0xdc, 0xba, 0x98, 0x76, 0x54, 0x32, 0x10,
482       0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77 },
483     { 0x01, 0x23, 0x45, 0x67, 0x89, 0xab, 0xcd, 0xef,
484       0xfe, 0xdc, 0xba, 0x98, 0x76, 0x54, 0x32, 0x10 },
485     { 0xb4, 0x99, 0x34, 0x01, 0xb3, 0xe9, 0x96, 0xf8,
486       0x4e, 0xe5, 0xce, 0xe7, 0xd7, 0x9b, 0x09, 0xb9 }
487  },
488  {
489     32,
490     { 0x01, 0x23, 0x45, 0x67, 0x89, 0xab, 0xcd, 0xef,
491       0xfe, 0xdc, 0xba, 0x98, 0x76, 0x54, 0x32, 0x10,
492       0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77,
493       0x88, 0x99, 0xaa, 0xbb, 0xcc, 0xdd, 0xee, 0xff },
494     { 0x01, 0x23, 0x45, 0x67, 0x89, 0xab, 0xcd, 0xef,
495       0xfe, 0xdc, 0xba, 0x98, 0x76, 0x54, 0x32, 0x10 },
496     { 0x9a, 0xcc, 0x23, 0x7d, 0xff, 0x16, 0xd7, 0x6c,
497       0x20, 0xef, 0x7c, 0x91, 0x9e, 0x3a, 0x75, 0x09 }
498  },
499  {
500     32,
501     { 0x60, 0x3D, 0xEB, 0x10, 0x15, 0xCA, 0x71, 0xBE,
502       0x2B, 0x73, 0xAE, 0xF0, 0x85, 0x7D, 0x77, 0x81,
503       0x1F, 0x35, 0x2C, 0x07, 0x3B, 0x61, 0x08, 0xD7,
504       0x2D, 0x98, 0x10, 0xA3, 0x09, 0x14, 0xDF, 0xF4 },
505     { 0xF6, 0x9F, 0x24, 0x45, 0xDF, 0x4F, 0x9B, 0x17,
506       0xAD, 0x2B, 0x41, 0x7B, 0xE6, 0x6C, 0x37, 0x10 },
507     { 0x79, 0x60, 0x10, 0x9F, 0xB6, 0xDC, 0x42, 0x94,
508       0x7F, 0xCF, 0xE5, 0x9E, 0xA3, 0xC5, 0xEB, 0x6B  }
509  }
510  };
511     unsigned char buf[2][16];
512     symmetric_key skey;
513     int err;
514     unsigned int x;
515     for (x = 0; x < sizeof(tests)/sizeof(tests[0]); x++) {
516        zeromem(&skey, sizeof(skey));
517        if ((err = camellia_setup(tests[x].key, tests[x].keylen, 0, &skey)) != CRYPT_OK) {
518           return err;
519        }
520        if ((err = camellia_ecb_encrypt(tests[x].pt, buf[0], &skey)) != CRYPT_OK) {
521           camellia_done(&skey);
522           return err;
523        }
524        if ((err = camellia_ecb_decrypt(tests[x].ct, buf[1], &skey)) != CRYPT_OK) {
525           camellia_done(&skey);
526           return err;
527        }
528        camellia_done(&skey);
529        if (compare_testvector(tests[x].ct, 16, buf[0], 16, "Camellia Encrypt", x) ||
530              compare_testvector(tests[x].pt, 16, buf[1], 16, "Camellia Decrypt", x)) {
531           return CRYPT_FAIL_TESTVECTOR;
532        }
533     }
534     return CRYPT_OK;
535  #endif
536  }
537  void camellia_done(symmetric_key *skey)
538  {
539    LTC_UNUSED_PARAM(skey);
540  }
541  int camellia_keysize(int *keysize)
542  {
543     if (*keysize >= 32) { *keysize = 32; }
544     else if (*keysize >= 24) { *keysize = 24; }
545     else if (*keysize >= 16) { *keysize = 16; }
546     else return CRYPT_INVALID_KEYSIZE;
547     return CRYPT_OK;
548  }
549  #endif
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-arena.c</h3>
            <pre><code>1  #define JEMALLOC_ARENA_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/div.h"
6  #include "jemalloc/internal/extent_dss.h"
7  #include "jemalloc/internal/extent_mmap.h"
8  #include "jemalloc/internal/mutex.h"
9  #include "jemalloc/internal/rtree.h"
10  #include "jemalloc/internal/safety_check.h"
11  #include "jemalloc/internal/util.h"
12  JEMALLOC_DIAGNOSTIC_DISABLE_SPURIOUS
13  const char *percpu_arena_mode_names[] = {
14  	"percpu",
15  	"phycpu",
16  	"disabled",
17  	"percpu",
18  	"phycpu"
19  };
20  percpu_arena_mode_t opt_percpu_arena = PERCPU_ARENA_DEFAULT;
21  ssize_t opt_dirty_decay_ms = DIRTY_DECAY_MS_DEFAULT;
22  ssize_t opt_muzzy_decay_ms = MUZZY_DECAY_MS_DEFAULT;
23  static atomic_zd_t dirty_decay_ms_default;
24  static atomic_zd_t muzzy_decay_ms_default;
25  const uint64_t h_steps[SMOOTHSTEP_NSTEPS] = {
26  #define STEP(step, h, x, y)			\
27  		h,
28  		SMOOTHSTEP
29  #undef STEP
30  };
31  static div_info_t arena_binind_div_info[SC_NBINS];
32  size_t opt_oversize_threshold = OVERSIZE_THRESHOLD_DEFAULT;
33  size_t oversize_threshold = OVERSIZE_THRESHOLD_DEFAULT;
34  static unsigned huge_arena_ind;
35  static void arena_decay_to_limit(tsdn_t *tsdn, arena_t *arena,
36      arena_decay_t *decay, extents_t *extents, bool all, size_t npages_limit,
37      size_t npages_decay_max, bool is_background_thread);
38  static bool arena_decay_dirty(tsdn_t *tsdn, arena_t *arena,
39      bool is_background_thread, bool all);
40  static void arena_dalloc_bin_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
41      bin_t *bin);
42  static void arena_bin_lower_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
43      bin_t *bin);
44  void
45  arena_basic_stats_merge(tsdn_t *tsdn, arena_t *arena, unsigned *nthreads,
46      const char **dss, ssize_t *dirty_decay_ms, ssize_t *muzzy_decay_ms,
47      size_t *nactive, size_t *ndirty, size_t *nmuzzy) {
48  	*nthreads += arena_nthreads_get(arena, false);
49  	*dss = dss_prec_names[arena_dss_prec_get(arena)];
50  	*dirty_decay_ms = arena_dirty_decay_ms_get(arena);
51  	*muzzy_decay_ms = arena_muzzy_decay_ms_get(arena);
52  	*nactive += atomic_load_zu(&arena->nactive, ATOMIC_RELAXED);
53  	*ndirty += extents_npages_get(&arena->extents_dirty);
54  	*nmuzzy += extents_npages_get(&arena->extents_muzzy);
55  }
56  void
57  arena_stats_merge(tsdn_t *tsdn, arena_t *arena, unsigned *nthreads,
58      const char **dss, ssize_t *dirty_decay_ms, ssize_t *muzzy_decay_ms,
59      size_t *nactive, size_t *ndirty, size_t *nmuzzy, arena_stats_t *astats,
60      bin_stats_t *bstats, arena_stats_large_t *lstats,
61      arena_stats_extents_t *estats) {
62  	cassert(config_stats);
63  	arena_basic_stats_merge(tsdn, arena, nthreads, dss, dirty_decay_ms,
64  	    muzzy_decay_ms, nactive, ndirty, nmuzzy);
65  	size_t base_allocated, base_resident, base_mapped, metadata_thp;
66  	base_stats_get(tsdn, arena->base, &base_allocated, &base_resident,
67  	    &base_mapped, &metadata_thp);
68  	arena_stats_lock(tsdn, &arena->stats);
69  	arena_stats_accum_zu(&astats->mapped, base_mapped
70  	    + arena_stats_read_zu(tsdn, &arena->stats, &arena->stats.mapped));
71  	arena_stats_accum_zu(&astats->retained,
72  	    extents_npages_get(&arena->extents_retained) << LG_PAGE);
73  	atomic_store_zu(&astats->extent_avail,
74  	    atomic_load_zu(&arena->extent_avail_cnt, ATOMIC_RELAXED),
75  	    ATOMIC_RELAXED);
76  	arena_stats_accum_u64(&astats->decay_dirty.npurge,
77  	    arena_stats_read_u64(tsdn, &arena->stats,
78  	    &arena->stats.decay_dirty.npurge));
79  	arena_stats_accum_u64(&astats->decay_dirty.nmadvise,
80  	    arena_stats_read_u64(tsdn, &arena->stats,
81  	    &arena->stats.decay_dirty.nmadvise));
82  	arena_stats_accum_u64(&astats->decay_dirty.purged,
83  	    arena_stats_read_u64(tsdn, &arena->stats,
84  	    &arena->stats.decay_dirty.purged));
85  	arena_stats_accum_u64(&astats->decay_muzzy.npurge,
86  	    arena_stats_read_u64(tsdn, &arena->stats,
87  	    &arena->stats.decay_muzzy.npurge));
88  	arena_stats_accum_u64(&astats->decay_muzzy.nmadvise,
89  	    arena_stats_read_u64(tsdn, &arena->stats,
90  	    &arena->stats.decay_muzzy.nmadvise));
91  	arena_stats_accum_u64(&astats->decay_muzzy.purged,
92  	    arena_stats_read_u64(tsdn, &arena->stats,
93  	    &arena->stats.decay_muzzy.purged));
94  	arena_stats_accum_zu(&astats->base, base_allocated);
95  	arena_stats_accum_zu(&astats->internal, arena_internal_get(arena));
96  	arena_stats_accum_zu(&astats->metadata_thp, metadata_thp);
97  	arena_stats_accum_zu(&astats->resident, base_resident +
98  	    (((atomic_load_zu(&arena->nactive, ATOMIC_RELAXED) +
99  	    extents_npages_get(&arena->extents_dirty) +
100  	    extents_npages_get(&arena->extents_muzzy)) << LG_PAGE)));
101  	arena_stats_accum_zu(&astats->abandoned_vm, atomic_load_zu(
102  	    &arena->stats.abandoned_vm, ATOMIC_RELAXED));
103  	for (szind_t i = 0; i < SC_NSIZES - SC_NBINS; i++) {
104  		uint64_t nmalloc = arena_stats_read_u64(tsdn, &arena->stats,
105  		    &arena->stats.lstats[i].nmalloc);
106  		arena_stats_accum_u64(&lstats[i].nmalloc, nmalloc);
107  		arena_stats_accum_u64(&astats->nmalloc_large, nmalloc);
108  		uint64_t ndalloc = arena_stats_read_u64(tsdn, &arena->stats,
109  		    &arena->stats.lstats[i].ndalloc);
110  		arena_stats_accum_u64(&lstats[i].ndalloc, ndalloc);
111  		arena_stats_accum_u64(&astats->ndalloc_large, ndalloc);
112  		uint64_t nrequests = arena_stats_read_u64(tsdn, &arena->stats,
113  		    &arena->stats.lstats[i].nrequests);
114  		arena_stats_accum_u64(&lstats[i].nrequests,
115  		    nmalloc + nrequests);
116  		arena_stats_accum_u64(&astats->nrequests_large,
117  		    nmalloc + nrequests);
118  		arena_stats_accum_u64(&lstats[i].nfills, nmalloc);
119  		arena_stats_accum_u64(&astats->nfills_large, nmalloc);
120  		uint64_t nflush = arena_stats_read_u64(tsdn, &arena->stats,
121  		    &arena->stats.lstats[i].nflushes);
122  		arena_stats_accum_u64(&lstats[i].nflushes, nflush);
123  		arena_stats_accum_u64(&astats->nflushes_large, nflush);
124  		assert(nmalloc >= ndalloc);
125  		assert(nmalloc - ndalloc <= SIZE_T_MAX);
126  		size_t curlextents = (size_t)(nmalloc - ndalloc);
127  		lstats[i].curlextents += curlextents;
128  		arena_stats_accum_zu(&astats->allocated_large,
129  		    curlextents * sz_index2size(SC_NBINS + i));
130  	}
131  	for (pszind_t i = 0; i < SC_NPSIZES; i++) {
132  		size_t dirty, muzzy, retained, dirty_bytes, muzzy_bytes,
133  		    retained_bytes;
134  		dirty = extents_nextents_get(&arena->extents_dirty, i);
135  		muzzy = extents_nextents_get(&arena->extents_muzzy, i);
136  		retained = extents_nextents_get(&arena->extents_retained, i);
137  		dirty_bytes = extents_nbytes_get(&arena->extents_dirty, i);
138  		muzzy_bytes = extents_nbytes_get(&arena->extents_muzzy, i);
139  		retained_bytes =
140  		    extents_nbytes_get(&arena->extents_retained, i);
141  		atomic_store_zu(&estats[i].ndirty, dirty, ATOMIC_RELAXED);
142  		atomic_store_zu(&estats[i].nmuzzy, muzzy, ATOMIC_RELAXED);
143  		atomic_store_zu(&estats[i].nretained, retained, ATOMIC_RELAXED);
144  		atomic_store_zu(&estats[i].dirty_bytes, dirty_bytes,
145  		    ATOMIC_RELAXED);
146  		atomic_store_zu(&estats[i].muzzy_bytes, muzzy_bytes,
147  		    ATOMIC_RELAXED);
148  		atomic_store_zu(&estats[i].retained_bytes, retained_bytes,
149  		    ATOMIC_RELAXED);
150  	}
151  	arena_stats_unlock(tsdn, &arena->stats);
152  	atomic_store_zu(&astats->tcache_bytes, 0, ATOMIC_RELAXED);
153  	malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
154  	cache_bin_array_descriptor_t *descriptor;
155  	ql_foreach(descriptor, &arena->cache_bin_array_descriptor_ql, link) {
156  		szind_t i = 0;
157  		for (; i < SC_NBINS; i++) {
158  			cache_bin_t *tbin = &descriptor->bins_small[i];
159  			arena_stats_accum_zu(&astats->tcache_bytes,
160  			    tbin->ncached * sz_index2size(i));
161  		}
162  		for (; i < nhbins; i++) {
163  			cache_bin_t *tbin = &descriptor->bins_large[i];
164  			arena_stats_accum_zu(&astats->tcache_bytes,
165  			    tbin->ncached * sz_index2size(i));
166  		}
167  	}
168  	malloc_mutex_prof_read(tsdn,
169  	    &astats->mutex_prof_data[arena_prof_mutex_tcache_list],
170  	    &arena->tcache_ql_mtx);
171  	malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
172  #define READ_ARENA_MUTEX_PROF_DATA(mtx, ind)				\
173      malloc_mutex_lock(tsdn, &arena->mtx);				\
174      malloc_mutex_prof_read(tsdn, &astats->mutex_prof_data[ind],		\
175          &arena->mtx);							\
176      malloc_mutex_unlock(tsdn, &arena->mtx);
177  	READ_ARENA_MUTEX_PROF_DATA(large_mtx, arena_prof_mutex_large);
178  	READ_ARENA_MUTEX_PROF_DATA(extent_avail_mtx,
179  	    arena_prof_mutex_extent_avail)
180  	READ_ARENA_MUTEX_PROF_DATA(extents_dirty.mtx,
181  	    arena_prof_mutex_extents_dirty)
182  	READ_ARENA_MUTEX_PROF_DATA(extents_muzzy.mtx,
183  	    arena_prof_mutex_extents_muzzy)
184  	READ_ARENA_MUTEX_PROF_DATA(extents_retained.mtx,
185  	    arena_prof_mutex_extents_retained)
186  	READ_ARENA_MUTEX_PROF_DATA(decay_dirty.mtx,
187  	    arena_prof_mutex_decay_dirty)
188  	READ_ARENA_MUTEX_PROF_DATA(decay_muzzy.mtx,
189  	    arena_prof_mutex_decay_muzzy)
190  	READ_ARENA_MUTEX_PROF_DATA(base->mtx,
191  	    arena_prof_mutex_base)
192  #undef READ_ARENA_MUTEX_PROF_DATA
193  	nstime_copy(&astats->uptime, &arena->create_time);
194  	nstime_update(&astats->uptime);
195  	nstime_subtract(&astats->uptime, &arena->create_time);
196  	for (szind_t i = 0; i < SC_NBINS; i++) {
197  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
198  			bin_stats_merge(tsdn, &bstats[i],
199  			    &arena->bins[i].bin_shards[j]);
200  		}
201  	}
202  }
203  void
204  arena_extents_dirty_dalloc(tsdn_t *tsdn, arena_t *arena,
205      extent_hooks_t **r_extent_hooks, extent_t *extent) {
206  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
207  	    WITNESS_RANK_CORE, 0);
208  	extents_dalloc(tsdn, arena, r_extent_hooks, &arena->extents_dirty,
209  	    extent);
210  	if (arena_dirty_decay_ms_get(arena) == 0) {
211  		arena_decay_dirty(tsdn, arena, false, true);
212  	} else {
213  		arena_background_thread_inactivity_check(tsdn, arena, false);
214  	}
215  }
216  static void *
217  arena_slab_reg_alloc(extent_t *slab, const bin_info_t *bin_info) {
218  	void *ret;
219  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
220  	size_t regind;
221  	assert(extent_nfree_get(slab) > 0);
222  	assert(!bitmap_full(slab_data->bitmap, &bin_info->bitmap_info));
223  	regind = bitmap_sfu(slab_data->bitmap, &bin_info->bitmap_info);
224  	ret = (void *)((uintptr_t)extent_addr_get(slab) +
225  	    (uintptr_t)(bin_info->reg_size * regind));
226  	extent_nfree_dec(slab);
227  	return ret;
228  }
229  static void
230  arena_slab_reg_alloc_batch(extent_t *slab, const bin_info_t *bin_info,
231  			   unsigned cnt, void** ptrs) {
232  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
233  	assert(extent_nfree_get(slab) >= cnt);
234  	assert(!bitmap_full(slab_data->bitmap, &bin_info->bitmap_info));
235  #if (! defined JEMALLOC_INTERNAL_POPCOUNTL) || (defined BITMAP_USE_TREE)
236  	for (unsigned i = 0; i < cnt; i++) {
237  		size_t regind = bitmap_sfu(slab_data->bitmap,
238  					   &bin_info->bitmap_info);
239  		*(ptrs + i) = (void *)((uintptr_t)extent_addr_get(slab) +
240  		    (uintptr_t)(bin_info->reg_size * regind));
241  	}
242  #else
243  	unsigned group = 0;
244  	bitmap_t g = slab_data->bitmap[group];
245  	unsigned i = 0;
246  	while (i < cnt) {
247  		while (g == 0) {
248  			g = slab_data->bitmap[++group];
249  		}
250  		size_t shift = group << LG_BITMAP_GROUP_NBITS;
251  		size_t pop = popcount_lu(g);
252  		if (pop > (cnt - i)) {
253  			pop = cnt - i;
254  		}
255  		uintptr_t base = (uintptr_t)extent_addr_get(slab);
256  		uintptr_t regsize = (uintptr_t)bin_info->reg_size;
257  		while (pop--) {
258  			size_t bit = cfs_lu(&g);
259  			size_t regind = shift + bit;
260  			*(ptrs + i) = (void *)(base + regsize * regind);
261  			i++;
262  		}
263  		slab_data->bitmap[group] = g;
264  	}
265  #endif
266  	extent_nfree_sub(slab, cnt);
267  }
268  #ifndef JEMALLOC_JET
269  static
270  #endif
271  size_t
272  arena_slab_regind(extent_t *slab, szind_t binind, const void *ptr) {
273  	size_t diff, regind;
274  	assert((uintptr_t)ptr >= (uintptr_t)extent_addr_get(slab));
275  	assert((uintptr_t)ptr < (uintptr_t)extent_past_get(slab));
276  	assert(((uintptr_t)ptr - (uintptr_t)extent_addr_get(slab)) %
277  	    (uintptr_t)bin_infos[binind].reg_size == 0);
278  	diff = (size_t)((uintptr_t)ptr - (uintptr_t)extent_addr_get(slab));
279  	regind = div_compute(&arena_binind_div_info[binind], diff);
280  	assert(regind < bin_infos[binind].nregs);
281  	return regind;
282  }
283  static void
284  arena_slab_reg_dalloc(extent_t *slab, arena_slab_data_t *slab_data, void *ptr) {
285  	szind_t binind = extent_szind_get(slab);
286  	const bin_info_t *bin_info = &bin_infos[binind];
287  	size_t regind = arena_slab_regind(slab, binind, ptr);
288  	assert(extent_nfree_get(slab) < bin_info->nregs);
289  	assert(bitmap_get(slab_data->bitmap, &bin_info->bitmap_info, regind));
290  	bitmap_unset(slab_data->bitmap, &bin_info->bitmap_info, regind);
291  	extent_nfree_inc(slab);
292  }
293  static void
294  arena_nactive_add(arena_t *arena, size_t add_pages) {
295  	atomic_fetch_add_zu(&arena->nactive, add_pages, ATOMIC_RELAXED);
296  }
297  static void
298  arena_nactive_sub(arena_t *arena, size_t sub_pages) {
299  	assert(atomic_load_zu(&arena->nactive, ATOMIC_RELAXED) >= sub_pages);
300  	atomic_fetch_sub_zu(&arena->nactive, sub_pages, ATOMIC_RELAXED);
301  }
302  static void
303  arena_large_malloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
304  	szind_t index, hindex;
305  	cassert(config_stats);
306  	if (usize < SC_LARGE_MINCLASS) {
307  		usize = SC_LARGE_MINCLASS;
308  	}
309  	index = sz_size2index(usize);
310  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
311  	arena_stats_add_u64(tsdn, &arena->stats,
312  	    &arena->stats.lstats[hindex].nmalloc, 1);
313  }
314  static void
315  arena_large_dalloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
316  	szind_t index, hindex;
317  	cassert(config_stats);
318  	if (usize < SC_LARGE_MINCLASS) {
319  		usize = SC_LARGE_MINCLASS;
320  	}
321  	index = sz_size2index(usize);
322  	hindex = (index >= SC_NBINS) ? index - SC_NBINS : 0;
323  	arena_stats_add_u64(tsdn, &arena->stats,
324  	    &arena->stats.lstats[hindex].ndalloc, 1);
325  }
326  static void
327  arena_large_ralloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t oldusize,
328      size_t usize) {
329  	arena_large_dalloc_stats_update(tsdn, arena, oldusize);
330  	arena_large_malloc_stats_update(tsdn, arena, usize);
331  }
332  static bool
333  arena_may_have_muzzy(arena_t *arena) {
334  	return (pages_can_purge_lazy && (arena_muzzy_decay_ms_get(arena) != 0));
335  }
336  extent_t *
337  arena_extent_alloc_large(tsdn_t *tsdn, arena_t *arena, size_t usize,
338      size_t alignment, bool *zero) {
339  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
340  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
341  	    WITNESS_RANK_CORE, 0);
342  	szind_t szind = sz_size2index(usize);
343  	size_t mapped_add;
344  	bool commit = true;
345  	extent_t *extent = extents_alloc(tsdn, arena, &extent_hooks,
346  	    &arena->extents_dirty, NULL, usize, sz_large_pad, alignment, false,
347  	    szind, zero, &commit);
348  	if (extent == NULL && arena_may_have_muzzy(arena)) {
349  		extent = extents_alloc(tsdn, arena, &extent_hooks,
350  		    &arena->extents_muzzy, NULL, usize, sz_large_pad, alignment,
351  		    false, szind, zero, &commit);
352  	}
353  	size_t size = usize + sz_large_pad;
354  	if (extent == NULL) {
355  		extent = extent_alloc_wrapper(tsdn, arena, &extent_hooks, NULL,
356  		    usize, sz_large_pad, alignment, false, szind, zero,
357  		    &commit);
358  		if (config_stats) {
359  			mapped_add = size;
360  		}
361  	} else if (config_stats) {
362  		mapped_add = 0;
363  	}
364  	if (extent != NULL) {
365  		if (config_stats) {
366  			arena_stats_lock(tsdn, &arena->stats);
367  			arena_large_malloc_stats_update(tsdn, arena, usize);
368  			if (mapped_add != 0) {
369  				arena_stats_add_zu(tsdn, &arena->stats,
370  				    &arena->stats.mapped, mapped_add);
371  			}
372  			arena_stats_unlock(tsdn, &arena->stats);
373  		}
374  		arena_nactive_add(arena, size >> LG_PAGE);
375  	}
376  	return extent;
377  }
378  void
379  arena_extent_dalloc_large_prep(tsdn_t *tsdn, arena_t *arena, extent_t *extent) {
380  	if (config_stats) {
381  		arena_stats_lock(tsdn, &arena->stats);
382  		arena_large_dalloc_stats_update(tsdn, arena,
383  		    extent_usize_get(extent));
384  		arena_stats_unlock(tsdn, &arena->stats);
385  	}
386  	arena_nactive_sub(arena, extent_size_get(extent) >> LG_PAGE);
387  }
388  void
389  arena_extent_ralloc_large_shrink(tsdn_t *tsdn, arena_t *arena, extent_t *extent,
390      size_t oldusize) {
391  	size_t usize = extent_usize_get(extent);
392  	size_t udiff = oldusize - usize;
393  	if (config_stats) {
394  		arena_stats_lock(tsdn, &arena->stats);
395  		arena_large_ralloc_stats_update(tsdn, arena, oldusize, usize);
396  		arena_stats_unlock(tsdn, &arena->stats);
397  	}
398  	arena_nactive_sub(arena, udiff >> LG_PAGE);
399  }
400  void
401  arena_extent_ralloc_large_expand(tsdn_t *tsdn, arena_t *arena, extent_t *extent,
402      size_t oldusize) {
403  	size_t usize = extent_usize_get(extent);
404  	size_t udiff = usize - oldusize;
405  	if (config_stats) {
406  		arena_stats_lock(tsdn, &arena->stats);
407  		arena_large_ralloc_stats_update(tsdn, arena, oldusize, usize);
408  		arena_stats_unlock(tsdn, &arena->stats);
409  	}
410  	arena_nactive_add(arena, udiff >> LG_PAGE);
411  }
412  static ssize_t
413  arena_decay_ms_read(arena_decay_t *decay) {
414  	return atomic_load_zd(&decay->time_ms, ATOMIC_RELAXED);
415  }
416  static void
417  arena_decay_ms_write(arena_decay_t *decay, ssize_t decay_ms) {
418  	atomic_store_zd(&decay->time_ms, decay_ms, ATOMIC_RELAXED);
419  }
420  static void
421  arena_decay_deadline_init(arena_decay_t *decay) {
422  	nstime_copy(&decay->deadline, &decay->epoch);
423  	nstime_add(&decay->deadline, &decay->interval);
424  	if (arena_decay_ms_read(decay) > 0) {
425  		nstime_t jitter;
426  		nstime_init(&jitter, prng_range_u64(&decay->jitter_state,
427  		    nstime_ns(&decay->interval)));
428  		nstime_add(&decay->deadline, &jitter);
429  	}
430  }
431  static bool
432  arena_decay_deadline_reached(const arena_decay_t *decay, const nstime_t *time) {
433  	return (nstime_compare(&decay->deadline, time) <= 0);
434  }
435  static size_t
436  arena_decay_backlog_npages_limit(const arena_decay_t *decay) {
437  	uint64_t sum;
438  	size_t npages_limit_backlog;
439  	unsigned i;
<span onclick='openModal()' class='match'>440  	sum = 0;
441  	for (i = 0; i < SMOOTHSTEP_NSTEPS; i++) {
442  		sum += decay->backlog[i] * h_steps[i];
443  	}
444  	npages_limit_backlog = (size_t)(sum >> SMOOTHSTEP_BFP);
</span>445  	return npages_limit_backlog;
446  }
447  static void
448  arena_decay_backlog_update_last(arena_decay_t *decay, size_t current_npages) {
449  	size_t npages_delta = (current_npages > decay->nunpurged) ?
450  	    current_npages - decay->nunpurged : 0;
451  	decay->backlog[SMOOTHSTEP_NSTEPS-1] = npages_delta;
452  	if (config_debug) {
453  		if (current_npages > decay->ceil_npages) {
454  			decay->ceil_npages = current_npages;
455  		}
456  		size_t npages_limit = arena_decay_backlog_npages_limit(decay);
457  		assert(decay->ceil_npages >= npages_limit);
458  		if (decay->ceil_npages > npages_limit) {
459  			decay->ceil_npages = npages_limit;
460  		}
461  	}
462  }
463  static void
464  arena_decay_backlog_update(arena_decay_t *decay, uint64_t nadvance_u64,
465      size_t current_npages) {
466  	if (nadvance_u64 >= SMOOTHSTEP_NSTEPS) {
467  		memset(decay->backlog, 0, (SMOOTHSTEP_NSTEPS-1) *
468  		    sizeof(size_t));
469  	} else {
470  		size_t nadvance_z = (size_t)nadvance_u64;
471  		assert((uint64_t)nadvance_z == nadvance_u64);
472  		memmove(decay->backlog, &decay->backlog[nadvance_z],
473  		    (SMOOTHSTEP_NSTEPS - nadvance_z) * sizeof(size_t));
474  		if (nadvance_z > 1) {
475  			memset(&decay->backlog[SMOOTHSTEP_NSTEPS -
476  			    nadvance_z], 0, (nadvance_z-1) * sizeof(size_t));
477  		}
478  	}
479  	arena_decay_backlog_update_last(decay, current_npages);
480  }
481  static void
482  arena_decay_try_purge(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
483      extents_t *extents, size_t current_npages, size_t npages_limit,
484      bool is_background_thread) {
485  	if (current_npages > npages_limit) {
486  		arena_decay_to_limit(tsdn, arena, decay, extents, false,
487  		    npages_limit, current_npages - npages_limit,
488  		    is_background_thread);
489  	}
490  }
491  static void
492  arena_decay_epoch_advance_helper(arena_decay_t *decay, const nstime_t *time,
493      size_t current_npages) {
494  	assert(arena_decay_deadline_reached(decay, time));
495  	nstime_t delta;
496  	nstime_copy(&delta, time);
497  	nstime_subtract(&delta, &decay->epoch);
498  	uint64_t nadvance_u64 = nstime_divide(&delta, &decay->interval);
499  	assert(nadvance_u64 > 0);
500  	nstime_copy(&delta, &decay->interval);
501  	nstime_imultiply(&delta, nadvance_u64);
502  	nstime_add(&decay->epoch, &delta);
503  	arena_decay_deadline_init(decay);
504  	arena_decay_backlog_update(decay, nadvance_u64, current_npages);
505  }
506  static void
507  arena_decay_epoch_advance(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
508      extents_t *extents, const nstime_t *time, bool is_background_thread) {
509  	size_t current_npages = extents_npages_get(extents);
510  	arena_decay_epoch_advance_helper(decay, time, current_npages);
511  	size_t npages_limit = arena_decay_backlog_npages_limit(decay);
512  	decay->nunpurged = (npages_limit > current_npages) ? npages_limit :
513  	    current_npages;
514  	if (!background_thread_enabled() || is_background_thread) {
515  		arena_decay_try_purge(tsdn, arena, decay, extents,
516  		    current_npages, npages_limit, is_background_thread);
517  	}
518  }
519  static void
520  arena_decay_reinit(arena_decay_t *decay, ssize_t decay_ms) {
521  	arena_decay_ms_write(decay, decay_ms);
522  	if (decay_ms > 0) {
523  		nstime_init(&decay->interval, (uint64_t)decay_ms *
524  		    KQU(1000000));
525  		nstime_idivide(&decay->interval, SMOOTHSTEP_NSTEPS);
526  	}
527  	nstime_init(&decay->epoch, 0);
528  	nstime_update(&decay->epoch);
529  	decay->jitter_state = (uint64_t)(uintptr_t)decay;
530  	arena_decay_deadline_init(decay);
531  	decay->nunpurged = 0;
532  	memset(decay->backlog, 0, SMOOTHSTEP_NSTEPS * sizeof(size_t));
533  }
534  static bool
535  arena_decay_init(arena_decay_t *decay, ssize_t decay_ms,
536      arena_stats_decay_t *stats) {
537  	if (config_debug) {
538  		for (size_t i = 0; i < sizeof(arena_decay_t); i++) {
539  			assert(((char *)decay)[i] == 0);
540  		}
541  		decay->ceil_npages = 0;
542  	}
543  	if (malloc_mutex_init(&decay->mtx, "decay", WITNESS_RANK_DECAY,
544  	    malloc_mutex_rank_exclusive)) {
545  		return true;
546  	}
547  	decay->purging = false;
548  	arena_decay_reinit(decay, decay_ms);
549  	if (config_stats) {
550  		decay->stats = stats;
551  	}
552  	return false;
553  }
554  static bool
555  arena_decay_ms_valid(ssize_t decay_ms) {
556  	if (decay_ms < -1) {
557  		return false;
558  	}
559  	if (decay_ms == -1 || (uint64_t)decay_ms <= NSTIME_SEC_MAX *
560  	    KQU(1000)) {
561  		return true;
562  	}
563  	return false;
564  }
565  static bool
566  arena_maybe_decay(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
567      extents_t *extents, bool is_background_thread) {
568  	malloc_mutex_assert_owner(tsdn, &decay->mtx);
569  	ssize_t decay_ms = arena_decay_ms_read(decay);
570  	if (decay_ms <= 0) {
571  		if (decay_ms == 0) {
572  			arena_decay_to_limit(tsdn, arena, decay, extents, false,
573  			    0, extents_npages_get(extents),
574  			    is_background_thread);
575  		}
576  		return false;
577  	}
578  	nstime_t time;
579  	nstime_init(&time, 0);
580  	nstime_update(&time);
581  	if (unlikely(!nstime_monotonic() && nstime_compare(&decay->epoch, &time)
582  	    > 0)) {
583  		nstime_copy(&decay->epoch, &time);
584  		arena_decay_deadline_init(decay);
585  	} else {
586  		assert(nstime_compare(&decay->epoch, &time) <= 0);
587  	}
588  	bool advance_epoch = arena_decay_deadline_reached(decay, &time);
589  	if (advance_epoch) {
590  		arena_decay_epoch_advance(tsdn, arena, decay, extents, &time,
591  		    is_background_thread);
592  	} else if (is_background_thread) {
593  		arena_decay_try_purge(tsdn, arena, decay, extents,
594  		    extents_npages_get(extents),
595  		    arena_decay_backlog_npages_limit(decay),
596  		    is_background_thread);
597  	}
598  	return advance_epoch;
599  }
600  static ssize_t
601  arena_decay_ms_get(arena_decay_t *decay) {
602  	return arena_decay_ms_read(decay);
603  }
604  ssize_t
605  arena_dirty_decay_ms_get(arena_t *arena) {
606  	return arena_decay_ms_get(&arena->decay_dirty);
607  }
608  ssize_t
609  arena_muzzy_decay_ms_get(arena_t *arena) {
610  	return arena_decay_ms_get(&arena->decay_muzzy);
611  }
612  static bool
613  arena_decay_ms_set(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
614      extents_t *extents, ssize_t decay_ms) {
615  	if (!arena_decay_ms_valid(decay_ms)) {
616  		return true;
617  	}
618  	malloc_mutex_lock(tsdn, &decay->mtx);
619  	arena_decay_reinit(decay, decay_ms);
620  	arena_maybe_decay(tsdn, arena, decay, extents, false);
621  	malloc_mutex_unlock(tsdn, &decay->mtx);
622  	return false;
623  }
624  bool
625  arena_dirty_decay_ms_set(tsdn_t *tsdn, arena_t *arena,
626      ssize_t decay_ms) {
627  	return arena_decay_ms_set(tsdn, arena, &arena->decay_dirty,
628  	    &arena->extents_dirty, decay_ms);
629  }
630  bool
631  arena_muzzy_decay_ms_set(tsdn_t *tsdn, arena_t *arena,
632      ssize_t decay_ms) {
633  	return arena_decay_ms_set(tsdn, arena, &arena->decay_muzzy,
634  	    &arena->extents_muzzy, decay_ms);
635  }
636  static size_t
637  arena_stash_decayed(tsdn_t *tsdn, arena_t *arena,
638      extent_hooks_t **r_extent_hooks, extents_t *extents, size_t npages_limit,
639  	size_t npages_decay_max, extent_list_t *decay_extents) {
640  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
641  	    WITNESS_RANK_CORE, 0);
642  	size_t nstashed = 0;
643  	extent_t *extent;
644  	while (nstashed < npages_decay_max &&
645  	    (extent = extents_evict(tsdn, arena, r_extent_hooks, extents,
646  	    npages_limit)) != NULL) {
647  		extent_list_append(decay_extents, extent);
648  		nstashed += extent_size_get(extent) >> LG_PAGE;
649  	}
650  	return nstashed;
651  }
652  static size_t
653  arena_decay_stashed(tsdn_t *tsdn, arena_t *arena,
654      extent_hooks_t **r_extent_hooks, arena_decay_t *decay, extents_t *extents,
655      bool all, extent_list_t *decay_extents, bool is_background_thread) {
656  	size_t nmadvise, nunmapped;
657  	size_t npurged;
658  	if (config_stats) {
659  		nmadvise = 0;
660  		nunmapped = 0;
661  	}
662  	npurged = 0;
663  	ssize_t muzzy_decay_ms = arena_muzzy_decay_ms_get(arena);
664  	for (extent_t *extent = extent_list_first(decay_extents); extent !=
665  	    NULL; extent = extent_list_first(decay_extents)) {
666  		if (config_stats) {
667  			nmadvise++;
668  		}
669  		size_t npages = extent_size_get(extent) >> LG_PAGE;
670  		npurged += npages;
671  		extent_list_remove(decay_extents, extent);
672  		switch (extents_state_get(extents)) {
673  		case extent_state_active:
674  			not_reached();
675  		case extent_state_dirty:
676  			if (!all && muzzy_decay_ms != 0 &&
677  			    !extent_purge_lazy_wrapper(tsdn, arena,
678  			    r_extent_hooks, extent, 0,
679  			    extent_size_get(extent))) {
680  				extents_dalloc(tsdn, arena, r_extent_hooks,
681  				    &arena->extents_muzzy, extent);
682  				arena_background_thread_inactivity_check(tsdn,
683  				    arena, is_background_thread);
684  				break;
685  			}
686  		case extent_state_muzzy:
687  			extent_dalloc_wrapper(tsdn, arena, r_extent_hooks,
688  			    extent);
689  			if (config_stats) {
690  				nunmapped += npages;
691  			}
692  			break;
693  		case extent_state_retained:
694  		default:
695  			not_reached();
696  		}
697  	}
698  	if (config_stats) {
699  		arena_stats_lock(tsdn, &arena->stats);
700  		arena_stats_add_u64(tsdn, &arena->stats, &decay->stats->npurge,
701  		    1);
702  		arena_stats_add_u64(tsdn, &arena->stats,
703  		    &decay->stats->nmadvise, nmadvise);
704  		arena_stats_add_u64(tsdn, &arena->stats, &decay->stats->purged,
705  		    npurged);
706  		arena_stats_sub_zu(tsdn, &arena->stats, &arena->stats.mapped,
707  		    nunmapped << LG_PAGE);
708  		arena_stats_unlock(tsdn, &arena->stats);
709  	}
710  	return npurged;
711  }
712  static void
713  arena_decay_to_limit(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
714      extents_t *extents, bool all, size_t npages_limit, size_t npages_decay_max,
715      bool is_background_thread) {
716  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
717  	    WITNESS_RANK_CORE, 1);
718  	malloc_mutex_assert_owner(tsdn, &decay->mtx);
719  	if (decay->purging) {
720  		return;
721  	}
722  	decay->purging = true;
723  	malloc_mutex_unlock(tsdn, &decay->mtx);
724  	extent_hooks_t *extent_hooks = extent_hooks_get(arena);
725  	extent_list_t decay_extents;
726  	extent_list_init(&decay_extents);
727  	size_t npurge = arena_stash_decayed(tsdn, arena, &extent_hooks, extents,
728  	    npages_limit, npages_decay_max, &decay_extents);
729  	if (npurge != 0) {
730  		size_t npurged = arena_decay_stashed(tsdn, arena,
731  		    &extent_hooks, decay, extents, all, &decay_extents,
732  		    is_background_thread);
733  		assert(npurged == npurge);
734  	}
735  	malloc_mutex_lock(tsdn, &decay->mtx);
736  	decay->purging = false;
737  }
738  static bool
739  arena_decay_impl(tsdn_t *tsdn, arena_t *arena, arena_decay_t *decay,
740      extents_t *extents, bool is_background_thread, bool all) {
741  	if (all) {
742  		malloc_mutex_lock(tsdn, &decay->mtx);
743  		arena_decay_to_limit(tsdn, arena, decay, extents, all, 0,
744  		    extents_npages_get(extents), is_background_thread);
745  		malloc_mutex_unlock(tsdn, &decay->mtx);
746  		return false;
747  	}
748  	if (malloc_mutex_trylock(tsdn, &decay->mtx)) {
749  		return true;
750  	}
751  	bool epoch_advanced = arena_maybe_decay(tsdn, arena, decay, extents,
752  	    is_background_thread);
753  	size_t npages_new;
754  	if (epoch_advanced) {
755  		npages_new = decay->backlog[SMOOTHSTEP_NSTEPS-1];
756  	}
757  	malloc_mutex_unlock(tsdn, &decay->mtx);
758  	if (have_background_thread && background_thread_enabled() &&
759  	    epoch_advanced && !is_background_thread) {
760  		background_thread_interval_check(tsdn, arena, decay,
761  		    npages_new);
762  	}
763  	return false;
764  }
765  static bool
766  arena_decay_dirty(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
767      bool all) {
768  	return arena_decay_impl(tsdn, arena, &arena->decay_dirty,
769  	    &arena->extents_dirty, is_background_thread, all);
770  }
771  static bool
772  arena_decay_muzzy(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
773      bool all) {
774  	return arena_decay_impl(tsdn, arena, &arena->decay_muzzy,
775  	    &arena->extents_muzzy, is_background_thread, all);
776  }
777  void
778  arena_decay(tsdn_t *tsdn, arena_t *arena, bool is_background_thread, bool all) {
779  	if (arena_decay_dirty(tsdn, arena, is_background_thread, all)) {
780  		return;
781  	}
782  	arena_decay_muzzy(tsdn, arena, is_background_thread, all);
783  }
784  static void
785  arena_slab_dalloc(tsdn_t *tsdn, arena_t *arena, extent_t *slab) {
786  	arena_nactive_sub(arena, extent_size_get(slab) >> LG_PAGE);
787  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
788  	arena_extents_dirty_dalloc(tsdn, arena, &extent_hooks, slab);
789  }
790  static void
791  arena_bin_slabs_nonfull_insert(bin_t *bin, extent_t *slab) {
792  	assert(extent_nfree_get(slab) > 0);
793  	extent_heap_insert(&bin->slabs_nonfull, slab);
794  	if (config_stats) {
795  		bin->stats.nonfull_slabs++;
796  	}
797  }
798  static void
799  arena_bin_slabs_nonfull_remove(bin_t *bin, extent_t *slab) {
800  	extent_heap_remove(&bin->slabs_nonfull, slab);
801  	if (config_stats) {
802  		bin->stats.nonfull_slabs--;
803  	}
804  }
805  static extent_t *
806  arena_bin_slabs_nonfull_tryget(bin_t *bin) {
807  	extent_t *slab = extent_heap_remove_first(&bin->slabs_nonfull);
808  	if (slab == NULL) {
809  		return NULL;
810  	}
811  	if (config_stats) {
812  		bin->stats.reslabs++;
813  		bin->stats.nonfull_slabs--;
814  	}
815  	return slab;
816  }
817  static void
818  arena_bin_slabs_full_insert(arena_t *arena, bin_t *bin, extent_t *slab) {
819  	assert(extent_nfree_get(slab) == 0);
820  	if (arena_is_auto(arena)) {
821  		return;
822  	}
823  	extent_list_append(&bin->slabs_full, slab);
824  }
825  static void
826  arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, extent_t *slab) {
827  	if (arena_is_auto(arena)) {
828  		return;
829  	}
830  	extent_list_remove(&bin->slabs_full, slab);
831  }
832  static void
833  arena_bin_reset(tsd_t *tsd, arena_t *arena, bin_t *bin) {
834  	extent_t *slab;
835  	malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
836  	if (bin->slabcur != NULL) {
837  		slab = bin->slabcur;
838  		bin->slabcur = NULL;
839  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
840  		arena_slab_dalloc(tsd_tsdn(tsd), arena, slab);
841  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
842  	}
843  	while ((slab = extent_heap_remove_first(&bin->slabs_nonfull)) != NULL) {
844  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
845  		arena_slab_dalloc(tsd_tsdn(tsd), arena, slab);
846  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
847  	}
848  	for (slab = extent_list_first(&bin->slabs_full); slab != NULL;
849  	     slab = extent_list_first(&bin->slabs_full)) {
850  		arena_bin_slabs_full_remove(arena, bin, slab);
851  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
852  		arena_slab_dalloc(tsd_tsdn(tsd), arena, slab);
853  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
854  	}
855  	if (config_stats) {
856  		bin->stats.curregs = 0;
857  		bin->stats.curslabs = 0;
858  	}
859  	malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
860  }
861  void
862  arena_reset(tsd_t *tsd, arena_t *arena) {
863  	malloc_mutex_lock(tsd_tsdn(tsd), &arena->large_mtx);
864  	for (extent_t *extent = extent_list_first(&arena->large); extent !=
865  	    NULL; extent = extent_list_first(&arena->large)) {
866  		void *ptr = extent_base_get(extent);
867  		size_t usize;
868  		malloc_mutex_unlock(tsd_tsdn(tsd), &arena->large_mtx);
869  		alloc_ctx_t alloc_ctx;
870  		rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
871  		rtree_szind_slab_read(tsd_tsdn(tsd), &extents_rtree, rtree_ctx,
872  		    (uintptr_t)ptr, true, &alloc_ctx.szind, &alloc_ctx.slab);
873  		assert(alloc_ctx.szind != SC_NSIZES);
874  		if (config_stats || (config_prof && opt_prof)) {
875  			usize = sz_index2size(alloc_ctx.szind);
876  			assert(usize == isalloc(tsd_tsdn(tsd), ptr));
877  		}
878  		if (config_prof && opt_prof) {
879  			prof_free(tsd, ptr, usize, &alloc_ctx);
880  		}
881  		large_dalloc(tsd_tsdn(tsd), extent);
882  		malloc_mutex_lock(tsd_tsdn(tsd), &arena->large_mtx);
883  	}
884  	malloc_mutex_unlock(tsd_tsdn(tsd), &arena->large_mtx);
885  	for (unsigned i = 0; i < SC_NBINS; i++) {
886  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
887  			arena_bin_reset(tsd, arena,
888  			    &arena->bins[i].bin_shards[j]);
889  		}
890  	}
891  	atomic_store_zu(&arena->nactive, 0, ATOMIC_RELAXED);
892  }
893  static void
894  arena_destroy_retained(tsdn_t *tsdn, arena_t *arena) {
895  	extent_hooks_t *extent_hooks = extent_hooks_get(arena);
896  	extent_t *extent;
897  	while ((extent = extents_evict(tsdn, arena, &extent_hooks,
898  	    &arena->extents_retained, 0)) != NULL) {
899  		extent_destroy_wrapper(tsdn, arena, &extent_hooks, extent);
900  	}
901  }
902  void
903  arena_destroy(tsd_t *tsd, arena_t *arena) {
904  	assert(base_ind_get(arena->base) >= narenas_auto);
905  	assert(arena_nthreads_get(arena, false) == 0);
906  	assert(arena_nthreads_get(arena, true) == 0);
907  	assert(extents_npages_get(&arena->extents_dirty) == 0);
908  	assert(extents_npages_get(&arena->extents_muzzy) == 0);
909  	arena_destroy_retained(tsd_tsdn(tsd), arena);
910  	arena_set(base_ind_get(arena->base), NULL);
911  	base_delete(tsd_tsdn(tsd), arena->base);
912  }
913  static extent_t *
914  arena_slab_alloc_hard(tsdn_t *tsdn, arena_t *arena,
915      extent_hooks_t **r_extent_hooks, const bin_info_t *bin_info,
916      szind_t szind) {
917  	extent_t *slab;
918  	bool zero, commit;
919  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
920  	    WITNESS_RANK_CORE, 0);
921  	zero = false;
922  	commit = true;
923  	slab = extent_alloc_wrapper(tsdn, arena, r_extent_hooks, NULL,
924  	    bin_info->slab_size, 0, PAGE, true, szind, &zero, &commit);
925  	if (config_stats && slab != NULL) {
926  		arena_stats_mapped_add(tsdn, &arena->stats,
927  		    bin_info->slab_size);
928  	}
929  	return slab;
930  }
931  static extent_t *
932  arena_slab_alloc(tsdn_t *tsdn, arena_t *arena, szind_t binind, unsigned binshard,
933      const bin_info_t *bin_info) {
934  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
935  	    WITNESS_RANK_CORE, 0);
936  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
937  	szind_t szind = sz_size2index(bin_info->reg_size);
938  	bool zero = false;
939  	bool commit = true;
940  	extent_t *slab = extents_alloc(tsdn, arena, &extent_hooks,
941  	    &arena->extents_dirty, NULL, bin_info->slab_size, 0, PAGE, true,
942  	    binind, &zero, &commit);
943  	if (slab == NULL && arena_may_have_muzzy(arena)) {
944  		slab = extents_alloc(tsdn, arena, &extent_hooks,
945  		    &arena->extents_muzzy, NULL, bin_info->slab_size, 0, PAGE,
946  		    true, binind, &zero, &commit);
947  	}
948  	if (slab == NULL) {
949  		slab = arena_slab_alloc_hard(tsdn, arena, &extent_hooks,
950  		    bin_info, szind);
951  		if (slab == NULL) {
952  			return NULL;
953  		}
954  	}
955  	assert(extent_slab_get(slab));
956  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
957  	extent_nfree_binshard_set(slab, bin_info->nregs, binshard);
958  	bitmap_init(slab_data->bitmap, &bin_info->bitmap_info, false);
959  	arena_nactive_add(arena, extent_size_get(slab) >> LG_PAGE);
960  	return slab;
961  }
962  static extent_t *
963  arena_bin_nonfull_slab_get(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
964      szind_t binind, unsigned binshard) {
965  	extent_t *slab;
966  	const bin_info_t *bin_info;
967  	slab = arena_bin_slabs_nonfull_tryget(bin);
968  	if (slab != NULL) {
969  		return slab;
970  	}
971  	bin_info = &bin_infos[binind];
972  	malloc_mutex_unlock(tsdn, &bin->lock);
973  	slab = arena_slab_alloc(tsdn, arena, binind, binshard, bin_info);
974  	malloc_mutex_lock(tsdn, &bin->lock);
975  	if (slab != NULL) {
976  		if (config_stats) {
977  			bin->stats.nslabs++;
978  			bin->stats.curslabs++;
979  		}
980  		return slab;
981  	}
982  	slab = arena_bin_slabs_nonfull_tryget(bin);
983  	if (slab != NULL) {
984  		return slab;
985  	}
986  	return NULL;
987  }
988  static void *
989  arena_bin_malloc_hard(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
990      szind_t binind, unsigned binshard) {
991  	const bin_info_t *bin_info;
992  	extent_t *slab;
993  	bin_info = &bin_infos[binind];
994  	if (!arena_is_auto(arena) && bin->slabcur != NULL) {
995  		arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
996  		bin->slabcur = NULL;
997  	}
998  	slab = arena_bin_nonfull_slab_get(tsdn, arena, bin, binind, binshard);
999  	if (bin->slabcur != NULL) {
1000  		if (extent_nfree_get(bin->slabcur) > 0) {
1001  			void *ret = arena_slab_reg_alloc(bin->slabcur,
1002  			    bin_info);
1003  			if (slab != NULL) {
1004  				if (extent_nfree_get(slab) == bin_info->nregs) {
1005  					arena_dalloc_bin_slab(tsdn, arena, slab,
1006  					    bin);
1007  				} else {
1008  					arena_bin_lower_slab(tsdn, arena, slab,
1009  					    bin);
1010  				}
1011  			}
1012  			return ret;
1013  		}
1014  		arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
1015  		bin->slabcur = NULL;
1016  	}
1017  	if (slab == NULL) {
1018  		return NULL;
1019  	}
1020  	bin->slabcur = slab;
1021  	assert(extent_nfree_get(bin->slabcur) > 0);
1022  	return arena_slab_reg_alloc(slab, bin_info);
1023  }
1024  bin_t *
1025  arena_bin_choose_lock(tsdn_t *tsdn, arena_t *arena, szind_t binind,
1026      unsigned *binshard) {
1027  	bin_t *bin;
1028  	if (tsdn_null(tsdn) || tsd_arena_get(tsdn_tsd(tsdn)) == NULL) {
1029  		*binshard = 0;
1030  	} else {
1031  		*binshard = tsd_binshardsp_get(tsdn_tsd(tsdn))->binshard[binind];
1032  	}
1033  	assert(*binshard < bin_infos[binind].n_shards);
1034  	bin = &arena->bins[binind].bin_shards[*binshard];
1035  	malloc_mutex_lock(tsdn, &bin->lock);
1036  	return bin;
1037  }
1038  void
1039  arena_tcache_fill_small(tsdn_t *tsdn, arena_t *arena, tcache_t *tcache,
1040      cache_bin_t *tbin, szind_t binind, uint64_t prof_accumbytes) {
1041  	unsigned i, nfill, cnt;
1042  	assert(tbin->ncached == 0);
1043  	if (config_prof && arena_prof_accum(tsdn, arena, prof_accumbytes)) {
1044  		prof_idump(tsdn);
1045  	}
1046  	unsigned binshard;
1047  	bin_t *bin = arena_bin_choose_lock(tsdn, arena, binind, &binshard);
1048  	for (i = 0, nfill = (tcache_bin_info[binind].ncached_max >>
1049  	    tcache->lg_fill_div[binind]); i < nfill; i += cnt) {
1050  		extent_t *slab;
1051  		if ((slab = bin->slabcur) != NULL && extent_nfree_get(slab) >
1052  		    0) {
1053  			unsigned tofill = nfill - i;
1054  			cnt = tofill < extent_nfree_get(slab) ?
1055  				tofill : extent_nfree_get(slab);
1056  			arena_slab_reg_alloc_batch(
1057  			   slab, &bin_infos[binind], cnt,
1058  			   tbin->avail - nfill + i);
1059  		} else {
1060  			cnt = 1;
1061  			void *ptr = arena_bin_malloc_hard(tsdn, arena, bin,
1062  			    binind, binshard);
1063  			if (ptr == NULL) {
1064  				if (i > 0) {
1065  					memmove(tbin->avail - i,
1066  						tbin->avail - nfill,
1067  						i * sizeof(void *));
1068  				}
1069  				break;
1070  			}
1071  			*(tbin->avail - nfill + i) = ptr;
1072  		}
1073  		if (config_fill && unlikely(opt_junk_alloc)) {
1074  			for (unsigned j = 0; j < cnt; j++) {
1075  				void* ptr = *(tbin->avail - nfill + i + j);
1076  				arena_alloc_junk_small(ptr, &bin_infos[binind],
1077  							true);
1078  			}
1079  		}
1080  	}
1081  	if (config_stats) {
1082  		bin->stats.nmalloc += i;
1083  		bin->stats.nrequests += tbin->tstats.nrequests;
1084  		bin->stats.curregs += i;
1085  		bin->stats.nfills++;
1086  		tbin->tstats.nrequests = 0;
1087  	}
1088  	malloc_mutex_unlock(tsdn, &bin->lock);
1089  	tbin->ncached = i;
1090  	arena_decay_tick(tsdn, arena);
1091  }
1092  void
1093  arena_alloc_junk_small(void *ptr, const bin_info_t *bin_info, bool zero) {
1094  	if (!zero) {
1095  		memset(ptr, JEMALLOC_ALLOC_JUNK, bin_info->reg_size);
1096  	}
1097  }
1098  static void
1099  arena_dalloc_junk_small_impl(void *ptr, const bin_info_t *bin_info) {
1100  	memset(ptr, JEMALLOC_FREE_JUNK, bin_info->reg_size);
1101  }
1102  arena_dalloc_junk_small_t *JET_MUTABLE arena_dalloc_junk_small =
1103      arena_dalloc_junk_small_impl;
1104  static void *
1105  arena_malloc_small(tsdn_t *tsdn, arena_t *arena, szind_t binind, bool zero) {
1106  	void *ret;
1107  	bin_t *bin;
1108  	size_t usize;
1109  	extent_t *slab;
1110  	assert(binind < SC_NBINS);
1111  	usize = sz_index2size(binind);
1112  	unsigned binshard;
1113  	bin = arena_bin_choose_lock(tsdn, arena, binind, &binshard);
1114  	if ((slab = bin->slabcur) != NULL && extent_nfree_get(slab) > 0) {
1115  		ret = arena_slab_reg_alloc(slab, &bin_infos[binind]);
1116  	} else {
1117  		ret = arena_bin_malloc_hard(tsdn, arena, bin, binind, binshard);
1118  	}
1119  	if (ret == NULL) {
1120  		malloc_mutex_unlock(tsdn, &bin->lock);
1121  		return NULL;
1122  	}
1123  	if (config_stats) {
1124  		bin->stats.nmalloc++;
1125  		bin->stats.nrequests++;
1126  		bin->stats.curregs++;
1127  	}
1128  	malloc_mutex_unlock(tsdn, &bin->lock);
1129  	if (config_prof && arena_prof_accum(tsdn, arena, usize)) {
1130  		prof_idump(tsdn);
1131  	}
1132  	if (!zero) {
1133  		if (config_fill) {
1134  			if (unlikely(opt_junk_alloc)) {
1135  				arena_alloc_junk_small(ret,
1136  				    &bin_infos[binind], false);
1137  			} else if (unlikely(opt_zero)) {
1138  				memset(ret, 0, usize);
1139  			}
1140  		}
1141  	} else {
1142  		if (config_fill && unlikely(opt_junk_alloc)) {
1143  			arena_alloc_junk_small(ret, &bin_infos[binind],
1144  			    true);
1145  		}
1146  		memset(ret, 0, usize);
1147  	}
1148  	arena_decay_tick(tsdn, arena);
1149  	return ret;
1150  }
1151  void *
1152  arena_malloc_hard(tsdn_t *tsdn, arena_t *arena, size_t size, szind_t ind,
1153      bool zero) {
1154  	assert(!tsdn_null(tsdn) || arena != NULL);
1155  	if (likely(!tsdn_null(tsdn))) {
1156  		arena = arena_choose_maybe_huge(tsdn_tsd(tsdn), arena, size);
1157  	}
1158  	if (unlikely(arena == NULL)) {
1159  		return NULL;
1160  	}
1161  	if (likely(size <= SC_SMALL_MAXCLASS)) {
1162  		return arena_malloc_small(tsdn, arena, ind, zero);
1163  	}
1164  	return large_malloc(tsdn, arena, sz_index2size(ind), zero);
1165  }
1166  void *
1167  arena_palloc(tsdn_t *tsdn, arena_t *arena, size_t usize, size_t alignment,
1168      bool zero, tcache_t *tcache) {
1169  	void *ret;
1170  	if (usize <= SC_SMALL_MAXCLASS
1171  	    && (alignment < PAGE
1172  	    || (alignment == PAGE && (usize & PAGE_MASK) == 0))) {
1173  		ret = arena_malloc(tsdn, arena, usize, sz_size2index(usize),
1174  		    zero, tcache, true);
1175  	} else {
1176  		if (likely(alignment <= CACHELINE)) {
1177  			ret = large_malloc(tsdn, arena, usize, zero);
1178  		} else {
1179  			ret = large_palloc(tsdn, arena, usize, alignment, zero);
1180  		}
1181  	}
1182  	return ret;
1183  }
1184  void
1185  arena_prof_promote(tsdn_t *tsdn, void *ptr, size_t usize) {
1186  	cassert(config_prof);
1187  	assert(ptr != NULL);
1188  	assert(isalloc(tsdn, ptr) == SC_LARGE_MINCLASS);
1189  	assert(usize <= SC_SMALL_MAXCLASS);
1190  	if (config_opt_safety_checks) {
1191  		safety_check_set_redzone(ptr, usize, SC_LARGE_MINCLASS);
1192  	}
1193  	rtree_ctx_t rtree_ctx_fallback;
1194  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1195  	extent_t *extent = rtree_extent_read(tsdn, &extents_rtree, rtree_ctx,
1196  	    (uintptr_t)ptr, true);
1197  	arena_t *arena = extent_arena_get(extent);
1198  	szind_t szind = sz_size2index(usize);
1199  	extent_szind_set(extent, szind);
1200  	rtree_szind_slab_update(tsdn, &extents_rtree, rtree_ctx, (uintptr_t)ptr,
1201  	    szind, false);
1202  	prof_accum_cancel(tsdn, &arena->prof_accum, usize);
1203  	assert(isalloc(tsdn, ptr) == usize);
1204  }
1205  static size_t
1206  arena_prof_demote(tsdn_t *tsdn, extent_t *extent, const void *ptr) {
1207  	cassert(config_prof);
1208  	assert(ptr != NULL);
1209  	extent_szind_set(extent, SC_NBINS);
1210  	rtree_ctx_t rtree_ctx_fallback;
1211  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1212  	rtree_szind_slab_update(tsdn, &extents_rtree, rtree_ctx, (uintptr_t)ptr,
1213  	    SC_NBINS, false);
1214  	assert(isalloc(tsdn, ptr) == SC_LARGE_MINCLASS);
1215  	return SC_LARGE_MINCLASS;
1216  }
1217  void
1218  arena_dalloc_promoted(tsdn_t *tsdn, void *ptr, tcache_t *tcache,
1219      bool slow_path) {
1220  	cassert(config_prof);
1221  	assert(opt_prof);
1222  	extent_t *extent = iealloc(tsdn, ptr);
1223  	size_t usize = extent_usize_get(extent);
1224  	size_t bumped_usize = arena_prof_demote(tsdn, extent, ptr);
1225  	if (config_opt_safety_checks && usize < SC_LARGE_MINCLASS) {
1226  		assert(bumped_usize == SC_LARGE_MINCLASS);
1227  		safety_check_verify_redzone(ptr, usize, bumped_usize);
1228  	}
1229  	if (bumped_usize <= tcache_maxclass && tcache != NULL) {
1230  		tcache_dalloc_large(tsdn_tsd(tsdn), tcache, ptr,
1231  		    sz_size2index(bumped_usize), slow_path);
1232  	} else {
1233  		large_dalloc(tsdn, extent);
1234  	}
1235  }
1236  static void
1237  arena_dissociate_bin_slab(arena_t *arena, extent_t *slab, bin_t *bin) {
1238  	if (slab == bin->slabcur) {
1239  		bin->slabcur = NULL;
1240  	} else {
1241  		szind_t binind = extent_szind_get(slab);
1242  		const bin_info_t *bin_info = &bin_infos[binind];
1243  		if (bin_info->nregs == 1) {
1244  			arena_bin_slabs_full_remove(arena, bin, slab);
1245  		} else {
1246  			arena_bin_slabs_nonfull_remove(bin, slab);
1247  		}
1248  	}
1249  }
1250  static void
1251  arena_dalloc_bin_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
1252      bin_t *bin) {
1253  	assert(slab != bin->slabcur);
1254  	malloc_mutex_unlock(tsdn, &bin->lock);
1255  	arena_slab_dalloc(tsdn, arena, slab);
1256  	malloc_mutex_lock(tsdn, &bin->lock);
1257  	if (config_stats) {
1258  		bin->stats.curslabs--;
1259  	}
1260  }
1261  static void
1262  arena_bin_lower_slab(tsdn_t *tsdn, arena_t *arena, extent_t *slab,
1263      bin_t *bin) {
1264  	assert(extent_nfree_get(slab) > 0);
1265  	if (bin->slabcur != NULL && extent_snad_comp(bin->slabcur, slab) > 0) {
1266  		if (extent_nfree_get(bin->slabcur) > 0) {
1267  			arena_bin_slabs_nonfull_insert(bin, bin->slabcur);
1268  		} else {
1269  			arena_bin_slabs_full_insert(arena, bin, bin->slabcur);
1270  		}
1271  		bin->slabcur = slab;
1272  		if (config_stats) {
1273  			bin->stats.reslabs++;
1274  		}
1275  	} else {
1276  		arena_bin_slabs_nonfull_insert(bin, slab);
1277  	}
1278  }
1279  static void
1280  arena_dalloc_bin_locked_impl(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
1281      szind_t binind, extent_t *slab, void *ptr, bool junked) {
1282  	arena_slab_data_t *slab_data = extent_slab_data_get(slab);
1283  	const bin_info_t *bin_info = &bin_infos[binind];
1284  	if (!junked && config_fill && unlikely(opt_junk_free)) {
1285  		arena_dalloc_junk_small(ptr, bin_info);
1286  	}
1287  	arena_slab_reg_dalloc(slab, slab_data, ptr);
1288  	unsigned nfree = extent_nfree_get(slab);
1289  	if (nfree == bin_info->nregs) {
1290  		arena_dissociate_bin_slab(arena, slab, bin);
1291  		arena_dalloc_bin_slab(tsdn, arena, slab, bin);
1292  	} else if (nfree == 1 && slab != bin->slabcur) {
1293  		arena_bin_slabs_full_remove(arena, bin, slab);
1294  		arena_bin_lower_slab(tsdn, arena, slab, bin);
1295  	}
1296  	if (config_stats) {
1297  		bin->stats.ndalloc++;
1298  		bin->stats.curregs--;
1299  	}
1300  }
1301  void
1302  arena_dalloc_bin_junked_locked(tsdn_t *tsdn, arena_t *arena, bin_t *bin,
1303      szind_t binind, extent_t *extent, void *ptr) {
1304  	arena_dalloc_bin_locked_impl(tsdn, arena, bin, binind, extent, ptr,
1305  	    true);
1306  }
1307  static void
1308  arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, extent_t *extent, void *ptr) {
1309  	szind_t binind = extent_szind_get(extent);
1310  	unsigned binshard = extent_binshard_get(extent);
1311  	bin_t *bin = &arena->bins[binind].bin_shards[binshard];
1312  	malloc_mutex_lock(tsdn, &bin->lock);
1313  	arena_dalloc_bin_locked_impl(tsdn, arena, bin, binind, extent, ptr,
1314  	    false);
1315  	malloc_mutex_unlock(tsdn, &bin->lock);
1316  }
1317  void
1318  arena_dalloc_small(tsdn_t *tsdn, void *ptr) {
1319  	extent_t *extent = iealloc(tsdn, ptr);
1320  	arena_t *arena = extent_arena_get(extent);
1321  	arena_dalloc_bin(tsdn, arena, extent, ptr);
1322  	arena_decay_tick(tsdn, arena);
1323  }
1324  bool
1325  arena_ralloc_no_move(tsdn_t *tsdn, void *ptr, size_t oldsize, size_t size,
1326      size_t extra, bool zero, size_t *newsize) {
1327  	bool ret;
1328  	assert(extra == 0 || size + extra <= SC_LARGE_MAXCLASS);
1329  	extent_t *extent = iealloc(tsdn, ptr);
1330  	if (unlikely(size > SC_LARGE_MAXCLASS)) {
1331  		ret = true;
1332  		goto done;
1333  	}
1334  	size_t usize_min = sz_s2u(size);
1335  	size_t usize_max = sz_s2u(size + extra);
1336  	if (likely(oldsize <= SC_SMALL_MAXCLASS && usize_min
1337  	    <= SC_SMALL_MAXCLASS)) {
1338  		assert(bin_infos[sz_size2index(oldsize)].reg_size ==
1339  		    oldsize);
1340  		if ((usize_max > SC_SMALL_MAXCLASS
1341  		    || sz_size2index(usize_max) != sz_size2index(oldsize))
1342  		    && (size > oldsize || usize_max < oldsize)) {
1343  			ret = true;
1344  			goto done;
1345  		}
1346  		arena_decay_tick(tsdn, extent_arena_get(extent));
1347  		ret = false;
1348  	} else if (oldsize >= SC_LARGE_MINCLASS
1349  	    && usize_max >= SC_LARGE_MINCLASS) {
1350  		ret = large_ralloc_no_move(tsdn, extent, usize_min, usize_max,
1351  		    zero);
1352  	} else {
1353  		ret = true;
1354  	}
1355  done:
1356  	assert(extent == iealloc(tsdn, ptr));
1357  	*newsize = extent_usize_get(extent);
1358  	return ret;
1359  }
1360  static void *
1361  arena_ralloc_move_helper(tsdn_t *tsdn, arena_t *arena, size_t usize,
1362      size_t alignment, bool zero, tcache_t *tcache) {
1363  	if (alignment == 0) {
1364  		return arena_malloc(tsdn, arena, usize, sz_size2index(usize),
1365  		    zero, tcache, true);
1366  	}
1367  	usize = sz_sa2u(usize, alignment);
1368  	if (unlikely(usize == 0 || usize > SC_LARGE_MAXCLASS)) {
1369  		return NULL;
1370  	}
1371  	return ipalloct(tsdn, usize, alignment, zero, tcache, arena);
1372  }
1373  void *
1374  arena_ralloc(tsdn_t *tsdn, arena_t *arena, void *ptr, size_t oldsize,
1375      size_t size, size_t alignment, bool zero, tcache_t *tcache,
1376      hook_ralloc_args_t *hook_args) {
1377  	size_t usize = sz_s2u(size);
1378  	if (unlikely(usize == 0 || size > SC_LARGE_MAXCLASS)) {
1379  		return NULL;
1380  	}
1381  	if (likely(usize <= SC_SMALL_MAXCLASS)) {
1382  		UNUSED size_t newsize;
1383  		if (!arena_ralloc_no_move(tsdn, ptr, oldsize, usize, 0, zero,
1384  		    &newsize)) {
1385  			hook_invoke_expand(hook_args->is_realloc
1386  			    ? hook_expand_realloc : hook_expand_rallocx,
1387  			    ptr, oldsize, usize, (uintptr_t)ptr,
1388  			    hook_args->args);
1389  			return ptr;
1390  		}
1391  	}
1392  	if (oldsize >= SC_LARGE_MINCLASS
1393  	    && usize >= SC_LARGE_MINCLASS) {
1394  		return large_ralloc(tsdn, arena, ptr, usize,
1395  		    alignment, zero, tcache, hook_args);
1396  	}
1397  	void *ret = arena_ralloc_move_helper(tsdn, arena, usize, alignment,
1398  	    zero, tcache);
1399  	if (ret == NULL) {
1400  		return NULL;
1401  	}
1402  	hook_invoke_alloc(hook_args->is_realloc
1403  	    ? hook_alloc_realloc : hook_alloc_rallocx, ret, (uintptr_t)ret,
1404  	    hook_args->args);
1405  	hook_invoke_dalloc(hook_args->is_realloc
1406  	    ? hook_dalloc_realloc : hook_dalloc_rallocx, ptr, hook_args->args);
1407  	size_t copysize = (usize < oldsize) ? usize : oldsize;
1408  	memcpy(ret, ptr, copysize);
1409  	isdalloct(tsdn, ptr, oldsize, tcache, NULL, true);
1410  	return ret;
1411  }
1412  dss_prec_t
1413  arena_dss_prec_get(arena_t *arena) {
1414  	return (dss_prec_t)atomic_load_u(&arena->dss_prec, ATOMIC_ACQUIRE);
1415  }
1416  bool
1417  arena_dss_prec_set(arena_t *arena, dss_prec_t dss_prec) {
1418  	if (!have_dss) {
1419  		return (dss_prec != dss_prec_disabled);
1420  	}
1421  	atomic_store_u(&arena->dss_prec, (unsigned)dss_prec, ATOMIC_RELEASE);
1422  	return false;
1423  }
1424  ssize_t
1425  arena_dirty_decay_ms_default_get(void) {
1426  	return atomic_load_zd(&dirty_decay_ms_default, ATOMIC_RELAXED);
1427  }
1428  bool
1429  arena_dirty_decay_ms_default_set(ssize_t decay_ms) {
1430  	if (!arena_decay_ms_valid(decay_ms)) {
1431  		return true;
1432  	}
1433  	atomic_store_zd(&dirty_decay_ms_default, decay_ms, ATOMIC_RELAXED);
1434  	return false;
1435  }
1436  ssize_t
1437  arena_muzzy_decay_ms_default_get(void) {
1438  	return atomic_load_zd(&muzzy_decay_ms_default, ATOMIC_RELAXED);
1439  }
1440  bool
1441  arena_muzzy_decay_ms_default_set(ssize_t decay_ms) {
1442  	if (!arena_decay_ms_valid(decay_ms)) {
1443  		return true;
1444  	}
1445  	atomic_store_zd(&muzzy_decay_ms_default, decay_ms, ATOMIC_RELAXED);
1446  	return false;
1447  }
1448  bool
1449  arena_retain_grow_limit_get_set(tsd_t *tsd, arena_t *arena, size_t *old_limit,
1450      size_t *new_limit) {
1451  	assert(opt_retain);
1452  	pszind_t new_ind JEMALLOC_CC_SILENCE_INIT(0);
1453  	if (new_limit != NULL) {
1454  		size_t limit = *new_limit;
1455  		if ((new_ind = sz_psz2ind(limit + 1) - 1) >= SC_NPSIZES) {
1456  			return true;
1457  		}
1458  	}
1459  	malloc_mutex_lock(tsd_tsdn(tsd), &arena->extent_grow_mtx);
1460  	if (old_limit != NULL) {
1461  		*old_limit = sz_pind2sz(arena->retain_grow_limit);
1462  	}
1463  	if (new_limit != NULL) {
1464  		arena->retain_grow_limit = new_ind;
1465  	}
1466  	malloc_mutex_unlock(tsd_tsdn(tsd), &arena->extent_grow_mtx);
1467  	return false;
1468  }
1469  unsigned
1470  arena_nthreads_get(arena_t *arena, bool internal) {
1471  	return atomic_load_u(&arena->nthreads[internal], ATOMIC_RELAXED);
1472  }
1473  void
1474  arena_nthreads_inc(arena_t *arena, bool internal) {
1475  	atomic_fetch_add_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
1476  }
1477  void
1478  arena_nthreads_dec(arena_t *arena, bool internal) {
1479  	atomic_fetch_sub_u(&arena->nthreads[internal], 1, ATOMIC_RELAXED);
1480  }
1481  size_t
1482  arena_extent_sn_next(arena_t *arena) {
1483  	return atomic_fetch_add_zu(&arena->extent_sn_next, 1, ATOMIC_RELAXED);
1484  }
1485  arena_t *
1486  arena_new(tsdn_t *tsdn, unsigned ind, extent_hooks_t *extent_hooks) {
1487  	arena_t *arena;
1488  	base_t *base;
1489  	unsigned i;
1490  	if (ind == 0) {
1491  		base = b0get();
1492  	} else {
1493  		base = base_new(tsdn, ind, extent_hooks);
1494  		if (base == NULL) {
1495  			return NULL;
1496  		}
1497  	}
1498  	unsigned nbins_total = 0;
1499  	for (i = 0; i < SC_NBINS; i++) {
1500  		nbins_total += bin_infos[i].n_shards;
1501  	}
1502  	size_t arena_size = sizeof(arena_t) + sizeof(bin_t) * nbins_total;
1503  	arena = (arena_t *)base_alloc(tsdn, base, arena_size, CACHELINE);
1504  	if (arena == NULL) {
1505  		goto label_error;
1506  	}
1507  	atomic_store_u(&arena->nthreads[0], 0, ATOMIC_RELAXED);
1508  	atomic_store_u(&arena->nthreads[1], 0, ATOMIC_RELAXED);
1509  	arena->last_thd = NULL;
1510  	if (config_stats) {
1511  		if (arena_stats_init(tsdn, &arena->stats)) {
1512  			goto label_error;
1513  		}
1514  		ql_new(&arena->tcache_ql);
1515  		ql_new(&arena->cache_bin_array_descriptor_ql);
1516  		if (malloc_mutex_init(&arena->tcache_ql_mtx, "tcache_ql",
1517  		    WITNESS_RANK_TCACHE_QL, malloc_mutex_rank_exclusive)) {
1518  			goto label_error;
1519  		}
1520  	}
1521  	if (config_prof) {
1522  		if (prof_accum_init(tsdn, &arena->prof_accum)) {
1523  			goto label_error;
1524  		}
1525  	}
1526  	if (config_cache_oblivious) {
1527  		atomic_store_zu(&arena->offset_state, config_debug ? ind :
1528  		    (size_t)(uintptr_t)arena, ATOMIC_RELAXED);
1529  	}
1530  	atomic_store_zu(&arena->extent_sn_next, 0, ATOMIC_RELAXED);
1531  	atomic_store_u(&arena->dss_prec, (unsigned)extent_dss_prec_get(),
1532  	    ATOMIC_RELAXED);
1533  	atomic_store_zu(&arena->nactive, 0, ATOMIC_RELAXED);
1534  	extent_list_init(&arena->large);
1535  	if (malloc_mutex_init(&arena->large_mtx, "arena_large",
1536  	    WITNESS_RANK_ARENA_LARGE, malloc_mutex_rank_exclusive)) {
1537  		goto label_error;
1538  	}
1539  	if (extents_init(tsdn, &arena->extents_dirty, extent_state_dirty,
1540  	    true)) {
1541  		goto label_error;
1542  	}
1543  	if (extents_init(tsdn, &arena->extents_muzzy, extent_state_muzzy,
1544  	    false)) {
1545  		goto label_error;
1546  	}
1547  	if (extents_init(tsdn, &arena->extents_retained, extent_state_retained,
1548  	    false)) {
1549  		goto label_error;
1550  	}
1551  	if (arena_decay_init(&arena->decay_dirty,
1552  	    arena_dirty_decay_ms_default_get(), &arena->stats.decay_dirty)) {
1553  		goto label_error;
1554  	}
1555  	if (arena_decay_init(&arena->decay_muzzy,
1556  	    arena_muzzy_decay_ms_default_get(), &arena->stats.decay_muzzy)) {
1557  		goto label_error;
1558  	}
1559  	arena->extent_grow_next = sz_psz2ind(HUGEPAGE);
1560  	arena->retain_grow_limit = sz_psz2ind(SC_LARGE_MAXCLASS);
1561  	if (malloc_mutex_init(&arena->extent_grow_mtx, "extent_grow",
1562  	    WITNESS_RANK_EXTENT_GROW, malloc_mutex_rank_exclusive)) {
1563  		goto label_error;
1564  	}
1565  	extent_avail_new(&arena->extent_avail);
1566  	if (malloc_mutex_init(&arena->extent_avail_mtx, "extent_avail",
1567  	    WITNESS_RANK_EXTENT_AVAIL, malloc_mutex_rank_exclusive)) {
1568  		goto label_error;
1569  	}
1570  	uintptr_t bin_addr = (uintptr_t)arena + sizeof(arena_t);
1571  	atomic_store_u(&arena->binshard_next, 0, ATOMIC_RELEASE);
1572  	for (i = 0; i < SC_NBINS; i++) {
1573  		unsigned nshards = bin_infos[i].n_shards;
1574  		arena->bins[i].bin_shards = (bin_t *)bin_addr;
1575  		bin_addr += nshards * sizeof(bin_t);
1576  		for (unsigned j = 0; j < nshards; j++) {
1577  			bool err = bin_init(&arena->bins[i].bin_shards[j]);
1578  			if (err) {
1579  				goto label_error;
1580  			}
1581  		}
1582  	}
1583  	assert(bin_addr == (uintptr_t)arena + arena_size);
1584  	arena->base = base;
1585  	arena_set(ind, arena);
1586  	nstime_init(&arena->create_time, 0);
1587  	nstime_update(&arena->create_time);
1588  	if (ind != 0) {
1589  		assert(!tsdn_null(tsdn));
1590  		pre_reentrancy(tsdn_tsd(tsdn), arena);
1591  		if (test_hooks_arena_new_hook) {
1592  			test_hooks_arena_new_hook();
1593  		}
1594  		post_reentrancy(tsdn_tsd(tsdn));
1595  	}
1596  	return arena;
1597  label_error:
1598  	if (ind != 0) {
1599  		base_delete(tsdn, base);
1600  	}
1601  	return NULL;
1602  }
1603  arena_t *
1604  arena_choose_huge(tsd_t *tsd) {
1605  	if (huge_arena_ind == 0) {
1606  		assert(!malloc_initialized());
1607  	}
1608  	arena_t *huge_arena = arena_get(tsd_tsdn(tsd), huge_arena_ind, false);
1609  	if (huge_arena == NULL) {
1610  		assert(huge_arena_ind != 0);
1611  		huge_arena = arena_get(tsd_tsdn(tsd), huge_arena_ind, true);
1612  		if (huge_arena == NULL) {
1613  			return NULL;
1614  		}
1615  		if (arena_dirty_decay_ms_default_get() > 0) {
1616  			arena_dirty_decay_ms_set(tsd_tsdn(tsd), huge_arena, 0);
1617  		}
1618  		if (arena_muzzy_decay_ms_default_get() > 0) {
1619  			arena_muzzy_decay_ms_set(tsd_tsdn(tsd), huge_arena, 0);
1620  		}
1621  	}
1622  	return huge_arena;
1623  }
1624  bool
1625  arena_init_huge(void) {
1626  	bool huge_enabled;
1627  	if (opt_oversize_threshold > SC_LARGE_MAXCLASS ||
1628  	    opt_oversize_threshold < SC_LARGE_MINCLASS) {
1629  		opt_oversize_threshold = 0;
1630  		oversize_threshold = SC_LARGE_MAXCLASS + PAGE;
1631  		huge_enabled = false;
1632  	} else {
1633  		huge_arena_ind = narenas_total_get();
1634  		oversize_threshold = opt_oversize_threshold;
1635  		huge_enabled = true;
1636  	}
1637  	return huge_enabled;
1638  }
1639  bool
1640  arena_is_huge(unsigned arena_ind) {
1641  	if (huge_arena_ind == 0) {
1642  		return false;
1643  	}
1644  	return (arena_ind == huge_arena_ind);
1645  }
1646  void
1647  arena_boot(sc_data_t *sc_data) {
1648  	arena_dirty_decay_ms_default_set(opt_dirty_decay_ms);
1649  	arena_muzzy_decay_ms_default_set(opt_muzzy_decay_ms);
1650  	for (unsigned i = 0; i < SC_NBINS; i++) {
1651  		sc_t *sc = &sc_data->sc[i];
1652  		div_init(&arena_binind_div_info[i],
1653  		    (1U << sc->lg_base) + (sc->ndelta << sc->lg_delta));
1654  	}
1655  }
1656  void
1657  arena_prefork0(tsdn_t *tsdn, arena_t *arena) {
1658  	malloc_mutex_prefork(tsdn, &arena->decay_dirty.mtx);
1659  	malloc_mutex_prefork(tsdn, &arena->decay_muzzy.mtx);
1660  }
1661  void
1662  arena_prefork1(tsdn_t *tsdn, arena_t *arena) {
1663  	if (config_stats) {
1664  		malloc_mutex_prefork(tsdn, &arena->tcache_ql_mtx);
1665  	}
1666  }
1667  void
1668  arena_prefork2(tsdn_t *tsdn, arena_t *arena) {
1669  	malloc_mutex_prefork(tsdn, &arena->extent_grow_mtx);
1670  }
1671  void
1672  arena_prefork3(tsdn_t *tsdn, arena_t *arena) {
1673  	extents_prefork(tsdn, &arena->extents_dirty);
1674  	extents_prefork(tsdn, &arena->extents_muzzy);
1675  	extents_prefork(tsdn, &arena->extents_retained);
1676  }
1677  void
1678  arena_prefork4(tsdn_t *tsdn, arena_t *arena) {
1679  	malloc_mutex_prefork(tsdn, &arena->extent_avail_mtx);
1680  }
1681  void
1682  arena_prefork5(tsdn_t *tsdn, arena_t *arena) {
1683  	base_prefork(tsdn, arena->base);
1684  }
1685  void
1686  arena_prefork6(tsdn_t *tsdn, arena_t *arena) {
1687  	malloc_mutex_prefork(tsdn, &arena->large_mtx);
1688  }
1689  void
1690  arena_prefork7(tsdn_t *tsdn, arena_t *arena) {
1691  	for (unsigned i = 0; i < SC_NBINS; i++) {
1692  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
1693  			bin_prefork(tsdn, &arena->bins[i].bin_shards[j]);
1694  		}
1695  	}
1696  }
1697  void
1698  arena_postfork_parent(tsdn_t *tsdn, arena_t *arena) {
1699  	unsigned i;
1700  	for (i = 0; i < SC_NBINS; i++) {
1701  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
1702  			bin_postfork_parent(tsdn,
1703  			    &arena->bins[i].bin_shards[j]);
1704  		}
1705  	}
1706  	malloc_mutex_postfork_parent(tsdn, &arena->large_mtx);
1707  	base_postfork_parent(tsdn, arena->base);
1708  	malloc_mutex_postfork_parent(tsdn, &arena->extent_avail_mtx);
1709  	extents_postfork_parent(tsdn, &arena->extents_dirty);
1710  	extents_postfork_parent(tsdn, &arena->extents_muzzy);
1711  	extents_postfork_parent(tsdn, &arena->extents_retained);
1712  	malloc_mutex_postfork_parent(tsdn, &arena->extent_grow_mtx);
1713  	malloc_mutex_postfork_parent(tsdn, &arena->decay_dirty.mtx);
1714  	malloc_mutex_postfork_parent(tsdn, &arena->decay_muzzy.mtx);
1715  	if (config_stats) {
1716  		malloc_mutex_postfork_parent(tsdn, &arena->tcache_ql_mtx);
1717  	}
1718  }
1719  void
1720  arena_postfork_child(tsdn_t *tsdn, arena_t *arena) {
1721  	unsigned i;
1722  	atomic_store_u(&arena->nthreads[0], 0, ATOMIC_RELAXED);
1723  	atomic_store_u(&arena->nthreads[1], 0, ATOMIC_RELAXED);
1724  	if (tsd_arena_get(tsdn_tsd(tsdn)) == arena) {
1725  		arena_nthreads_inc(arena, false);
1726  	}
1727  	if (tsd_iarena_get(tsdn_tsd(tsdn)) == arena) {
1728  		arena_nthreads_inc(arena, true);
1729  	}
1730  	if (config_stats) {
1731  		ql_new(&arena->tcache_ql);
1732  		ql_new(&arena->cache_bin_array_descriptor_ql);
1733  		tcache_t *tcache = tcache_get(tsdn_tsd(tsdn));
1734  		if (tcache != NULL && tcache->arena == arena) {
1735  			ql_elm_new(tcache, link);
1736  			ql_tail_insert(&arena->tcache_ql, tcache, link);
1737  			cache_bin_array_descriptor_init(
1738  			    &tcache->cache_bin_array_descriptor,
1739  			    tcache->bins_small, tcache->bins_large);
1740  			ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
1741  			    &tcache->cache_bin_array_descriptor, link);
1742  		}
1743  	}
1744  	for (i = 0; i < SC_NBINS; i++) {
1745  		for (unsigned j = 0; j < bin_infos[i].n_shards; j++) {
1746  			bin_postfork_child(tsdn, &arena->bins[i].bin_shards[j]);
1747  		}
1748  	}
1749  	malloc_mutex_postfork_child(tsdn, &arena->large_mtx);
1750  	base_postfork_child(tsdn, arena->base);
1751  	malloc_mutex_postfork_child(tsdn, &arena->extent_avail_mtx);
1752  	extents_postfork_child(tsdn, &arena->extents_dirty);
1753  	extents_postfork_child(tsdn, &arena->extents_muzzy);
1754  	extents_postfork_child(tsdn, &arena->extents_retained);
1755  	malloc_mutex_postfork_child(tsdn, &arena->extent_grow_mtx);
1756  	malloc_mutex_postfork_child(tsdn, &arena->decay_dirty.mtx);
1757  	malloc_mutex_postfork_child(tsdn, &arena->decay_muzzy.mtx);
1758  	if (config_stats) {
1759  		malloc_mutex_postfork_child(tsdn, &arena->tcache_ql_mtx);
1760  	}
1761  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from libtomcrypt-MDEwOlJlcG9zaXRvcnk3NzcwMTE=-flat-camellia.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-arena.c</div>
                </div>
                <div class="column column_space"><pre><code>218     A ^= F(B ^ key_sigma[1]);
219     STORE64H(A, T+32); STORE64H(B, T+40);
220     for (x = 0; x < 16; x++) { T[x+32] ^= kL[x]; }
221     LOAD64H(A, T+32); LOAD64H(B, T+40);
222     B ^= F(A ^ key_sigma[2]);
</pre></code></div>
                <div class="column column_space"><pre><code>440  	sum = 0;
441  	for (i = 0; i < SMOOTHSTEP_NSTEPS; i++) {
442  		sum += decay->backlog[i] * h_steps[i];
443  	}
444  	npages_limit_backlog = (size_t)(sum >> SMOOTHSTEP_BFP);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    