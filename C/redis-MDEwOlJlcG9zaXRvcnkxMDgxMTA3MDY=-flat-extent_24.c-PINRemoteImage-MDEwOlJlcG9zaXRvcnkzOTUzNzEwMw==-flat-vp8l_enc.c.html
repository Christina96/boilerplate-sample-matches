
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 4.024943310657596%, Tokens: 8, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-extent_24.c</h3>
            <pre><code>1  #define JEMALLOC_EXTENT_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/extent_dss.h"
6  #include "jemalloc/internal/extent_mmap.h"
7  #include "jemalloc/internal/ph.h"
8  #include "jemalloc/internal/rtree.h"
9  #include "jemalloc/internal/mutex.h"
10  #include "jemalloc/internal/mutex_pool.h"
11  rtree_t		extents_rtree;
12  mutex_pool_t	extent_mutex_pool;
13  size_t opt_lg_extent_max_active_fit = LG_EXTENT_MAX_ACTIVE_FIT_DEFAULT;
14  static const bitmap_info_t extents_bitmap_info =
15      BITMAP_INFO_INITIALIZER(SC_NPSIZES+1);
16  static void *extent_alloc_default(extent_hooks_t *extent_hooks, void *new_addr,
17      size_t size, size_t alignment, bool *zero, bool *commit,
18      unsigned arena_ind);
19  static bool extent_dalloc_default(extent_hooks_t *extent_hooks, void *addr,
20      size_t size, bool committed, unsigned arena_ind);
21  static void extent_destroy_default(extent_hooks_t *extent_hooks, void *addr,
22      size_t size, bool committed, unsigned arena_ind);
23  static bool extent_commit_default(extent_hooks_t *extent_hooks, void *addr,
24      size_t size, size_t offset, size_t length, unsigned arena_ind);
25  static bool extent_commit_impl(tsdn_t *tsdn, arena_t *arena,
26      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
27      size_t length, bool growing_retained);
28  static bool extent_decommit_default(extent_hooks_t *extent_hooks,
29      void *addr, size_t size, size_t offset, size_t length, unsigned arena_ind);
30  #ifdef PAGES_CAN_PURGE_LAZY
31  static bool extent_purge_lazy_default(extent_hooks_t *extent_hooks, void *addr,
32      size_t size, size_t offset, size_t length, unsigned arena_ind);
33  #endif
34  static bool extent_purge_lazy_impl(tsdn_t *tsdn, arena_t *arena,
35      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
36      size_t length, bool growing_retained);
37  #ifdef PAGES_CAN_PURGE_FORCED
38  static bool extent_purge_forced_default(extent_hooks_t *extent_hooks,
39      void *addr, size_t size, size_t offset, size_t length, unsigned arena_ind);
40  #endif
41  static bool extent_purge_forced_impl(tsdn_t *tsdn, arena_t *arena,
42      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
43      size_t length, bool growing_retained);
44  static bool extent_split_default(extent_hooks_t *extent_hooks, void *addr,
45      size_t size, size_t size_a, size_t size_b, bool committed,
46      unsigned arena_ind);
47  static extent_t *extent_split_impl(tsdn_t *tsdn, arena_t *arena,
48      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t size_a,
49      szind_t szind_a, bool slab_a, size_t size_b, szind_t szind_b, bool slab_b,
50      bool growing_retained);
51  static bool extent_merge_default(extent_hooks_t *extent_hooks, void *addr_a,
52      size_t size_a, void *addr_b, size_t size_b, bool committed,
53      unsigned arena_ind);
54  static bool extent_merge_impl(tsdn_t *tsdn, arena_t *arena,
55      extent_hooks_t **r_extent_hooks, extent_t *a, extent_t *b,
56      bool growing_retained);
57  const extent_hooks_t	extent_hooks_default = {
58  	extent_alloc_default,
59  	extent_dalloc_default,
60  	extent_destroy_default,
61  	extent_commit_default,
62  	extent_decommit_default
63  #ifdef PAGES_CAN_PURGE_LAZY
64  	,
65  	extent_purge_lazy_default
66  #else
67  	,
68  	NULL
69  #endif
70  #ifdef PAGES_CAN_PURGE_FORCED
71  	,
72  	extent_purge_forced_default
73  #else
74  	,
75  	NULL
76  #endif
77  	,
78  	extent_split_default,
79  	extent_merge_default
80  };
81  static atomic_zu_t curpages;
82  static atomic_zu_t highpages;
83  static void extent_deregister(tsdn_t *tsdn, extent_t *extent);
84  static extent_t *extent_recycle(tsdn_t *tsdn, arena_t *arena,
85      extent_hooks_t **r_extent_hooks, extents_t *extents, void *new_addr,
86      size_t usize, size_t pad, size_t alignment, bool slab, szind_t szind,
87      bool *zero, bool *commit, bool growing_retained);
88  static extent_t *extent_try_coalesce(tsdn_t *tsdn, arena_t *arena,
89      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
90      extent_t *extent, bool *coalesced, bool growing_retained);
91  static void extent_record(tsdn_t *tsdn, arena_t *arena,
92      extent_hooks_t **r_extent_hooks, extents_t *extents, extent_t *extent,
93      bool growing_retained);
94  #define ATTR_NONE &bsol;* does nothing */
95  ph_gen(ATTR_NONE, extent_avail_, extent_tree_t, extent_t, ph_link,
96      extent_esnead_comp)
97  #undef ATTR_NONE
98  typedef enum {
99  	lock_result_success,
100  	lock_result_failure,
101  	lock_result_no_extent
102  } lock_result_t;
103  static lock_result_t
104  extent_rtree_leaf_elm_try_lock(tsdn_t *tsdn, rtree_leaf_elm_t *elm,
105      extent_t **result, bool inactive_only) {
106  	extent_t *extent1 = rtree_leaf_elm_extent_read(tsdn, &extents_rtree,
107  	    elm, true);
108  	if (extent1 == NULL || (inactive_only && rtree_leaf_elm_slab_read(tsdn,
109  	    &extents_rtree, elm, true))) {
110  		return lock_result_no_extent;
111  	}
112  	extent_lock(tsdn, extent1);
113  	extent_t *extent2 = rtree_leaf_elm_extent_read(tsdn,
114  	    &extents_rtree, elm, true);
115  	if (extent1 == extent2) {
116  		*result = extent1;
<span onclick='openModal()' class='match'>117  		return lock_result_success;
118  	} else {
119  		extent_unlock(tsdn, extent1);
120  		return lock_result_failure;
121  	}
122  }
123  static extent_t *
</span>124  extent_lock_from_addr(tsdn_t *tsdn, rtree_ctx_t *rtree_ctx, void *addr,
125      bool inactive_only) {
126  	extent_t *ret = NULL;
127  	rtree_leaf_elm_t *elm = rtree_leaf_elm_lookup(tsdn, &extents_rtree,
128  	    rtree_ctx, (uintptr_t)addr, false, false);
129  	if (elm == NULL) {
130  		return NULL;
131  	}
132  	lock_result_t lock_result;
133  	do {
134  		lock_result = extent_rtree_leaf_elm_try_lock(tsdn, elm, &ret,
135  		    inactive_only);
136  	} while (lock_result == lock_result_failure);
137  	return ret;
138  }
139  extent_t *
140  extent_alloc(tsdn_t *tsdn, arena_t *arena) {
141  	malloc_mutex_lock(tsdn, &arena->extent_avail_mtx);
142  	extent_t *extent = extent_avail_first(&arena->extent_avail);
143  	if (extent == NULL) {
144  		malloc_mutex_unlock(tsdn, &arena->extent_avail_mtx);
145  		return base_alloc_extent(tsdn, arena->base);
146  	}
147  	extent_avail_remove(&arena->extent_avail, extent);
148  	atomic_fetch_sub_zu(&arena->extent_avail_cnt, 1, ATOMIC_RELAXED);
149  	malloc_mutex_unlock(tsdn, &arena->extent_avail_mtx);
150  	return extent;
151  }
152  void
153  extent_dalloc(tsdn_t *tsdn, arena_t *arena, extent_t *extent) {
154  	malloc_mutex_lock(tsdn, &arena->extent_avail_mtx);
155  	extent_avail_insert(&arena->extent_avail, extent);
156  	atomic_fetch_add_zu(&arena->extent_avail_cnt, 1, ATOMIC_RELAXED);
157  	malloc_mutex_unlock(tsdn, &arena->extent_avail_mtx);
158  }
159  extent_hooks_t *
160  extent_hooks_get(arena_t *arena) {
161  	return base_extent_hooks_get(arena->base);
162  }
163  extent_hooks_t *
164  extent_hooks_set(tsd_t *tsd, arena_t *arena, extent_hooks_t *extent_hooks) {
165  	background_thread_info_t *info;
166  	if (have_background_thread) {
167  		info = arena_background_thread_info_get(arena);
168  		malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
169  	}
170  	extent_hooks_t *ret = base_extent_hooks_set(arena->base, extent_hooks);
171  	if (have_background_thread) {
172  		malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
173  	}
174  	return ret;
175  }
176  static void
177  extent_hooks_assure_initialized(arena_t *arena,
178      extent_hooks_t **r_extent_hooks) {
179  	if (*r_extent_hooks == EXTENT_HOOKS_INITIALIZER) {
180  		*r_extent_hooks = extent_hooks_get(arena);
181  	}
182  }
183  #ifndef JEMALLOC_JET
184  static
185  #endif
186  size_t
187  extent_size_quantize_floor(size_t size) {
188  	size_t ret;
189  	pszind_t pind;
190  	assert(size > 0);
191  	assert((size & PAGE_MASK) == 0);
192  	pind = sz_psz2ind(size - sz_large_pad + 1);
193  	if (pind == 0) {
194  		return size;
195  	}
196  	ret = sz_pind2sz(pind - 1) + sz_large_pad;
197  	assert(ret <= size);
198  	return ret;
199  }
200  #ifndef JEMALLOC_JET
201  static
202  #endif
203  size_t
204  extent_size_quantize_ceil(size_t size) {
205  	size_t ret;
206  	assert(size > 0);
207  	assert(size - sz_large_pad <= SC_LARGE_MAXCLASS);
208  	assert((size & PAGE_MASK) == 0);
209  	ret = extent_size_quantize_floor(size);
210  	if (ret < size) {
211  		ret = sz_pind2sz(sz_psz2ind(ret - sz_large_pad + 1)) +
212  		    sz_large_pad;
213  	}
214  	return ret;
215  }
216  ph_gen(, extent_heap_, extent_heap_t, extent_t, ph_link, extent_snad_comp)
217  bool
218  extents_init(tsdn_t *tsdn, extents_t *extents, extent_state_t state,
219      bool delay_coalesce) {
220  	if (malloc_mutex_init(&extents->mtx, "extents", WITNESS_RANK_EXTENTS,
221  	    malloc_mutex_rank_exclusive)) {
222  		return true;
223  	}
224  	for (unsigned i = 0; i < SC_NPSIZES + 1; i++) {
225  		extent_heap_new(&extents->heaps[i]);
226  	}
227  	bitmap_init(extents->bitmap, &extents_bitmap_info, true);
228  	extent_list_init(&extents->lru);
229  	atomic_store_zu(&extents->npages, 0, ATOMIC_RELAXED);
230  	extents->state = state;
231  	extents->delay_coalesce = delay_coalesce;
232  	return false;
233  }
234  extent_state_t
235  extents_state_get(const extents_t *extents) {
236  	return extents->state;
237  }
238  size_t
239  extents_npages_get(extents_t *extents) {
240  	return atomic_load_zu(&extents->npages, ATOMIC_RELAXED);
241  }
242  size_t
243  extents_nextents_get(extents_t *extents, pszind_t pind) {
244  	return atomic_load_zu(&extents->nextents[pind], ATOMIC_RELAXED);
245  }
246  size_t
247  extents_nbytes_get(extents_t *extents, pszind_t pind) {
248  	return atomic_load_zu(&extents->nbytes[pind], ATOMIC_RELAXED);
249  }
250  static void
251  extents_stats_add(extents_t *extent, pszind_t pind, size_t sz) {
252  	size_t cur = atomic_load_zu(&extent->nextents[pind], ATOMIC_RELAXED);
253  	atomic_store_zu(&extent->nextents[pind], cur + 1, ATOMIC_RELAXED);
254  	cur = atomic_load_zu(&extent->nbytes[pind], ATOMIC_RELAXED);
255  	atomic_store_zu(&extent->nbytes[pind], cur + sz, ATOMIC_RELAXED);
256  }
257  static void
258  extents_stats_sub(extents_t *extent, pszind_t pind, size_t sz) {
259  	size_t cur = atomic_load_zu(&extent->nextents[pind], ATOMIC_RELAXED);
260  	atomic_store_zu(&extent->nextents[pind], cur - 1, ATOMIC_RELAXED);
261  	cur = atomic_load_zu(&extent->nbytes[pind], ATOMIC_RELAXED);
262  	atomic_store_zu(&extent->nbytes[pind], cur - sz, ATOMIC_RELAXED);
263  }
264  static void
265  extents_insert_locked(tsdn_t *tsdn, extents_t *extents, extent_t *extent) {
266  	malloc_mutex_assert_owner(tsdn, &extents->mtx);
267  	assert(extent_state_get(extent) == extents->state);
268  	size_t size = extent_size_get(extent);
269  	size_t psz = extent_size_quantize_floor(size);
270  	pszind_t pind = sz_psz2ind(psz);
271  	if (extent_heap_empty(&extents->heaps[pind])) {
272  		bitmap_unset(extents->bitmap, &extents_bitmap_info,
273  		    (size_t)pind);
274  	}
275  	extent_heap_insert(&extents->heaps[pind], extent);
276  	if (config_stats) {
277  		extents_stats_add(extents, pind, size);
278  	}
279  	extent_list_append(&extents->lru, extent);
280  	size_t npages = size >> LG_PAGE;
281  	size_t cur_extents_npages =
282  	    atomic_load_zu(&extents->npages, ATOMIC_RELAXED);
283  	atomic_store_zu(&extents->npages, cur_extents_npages + npages,
284  	    ATOMIC_RELAXED);
285  }
286  static void
287  extents_remove_locked(tsdn_t *tsdn, extents_t *extents, extent_t *extent) {
288  	malloc_mutex_assert_owner(tsdn, &extents->mtx);
289  	assert(extent_state_get(extent) == extents->state);
290  	size_t size = extent_size_get(extent);
291  	size_t psz = extent_size_quantize_floor(size);
292  	pszind_t pind = sz_psz2ind(psz);
293  	extent_heap_remove(&extents->heaps[pind], extent);
294  	if (config_stats) {
295  		extents_stats_sub(extents, pind, size);
296  	}
297  	if (extent_heap_empty(&extents->heaps[pind])) {
298  		bitmap_set(extents->bitmap, &extents_bitmap_info,
299  		    (size_t)pind);
300  	}
301  	extent_list_remove(&extents->lru, extent);
302  	size_t npages = size >> LG_PAGE;
303  	size_t cur_extents_npages =
304  	    atomic_load_zu(&extents->npages, ATOMIC_RELAXED);
305  	assert(cur_extents_npages >= npages);
306  	atomic_store_zu(&extents->npages,
307  	    cur_extents_npages - (size >> LG_PAGE), ATOMIC_RELAXED);
308  }
309  static extent_t *
310  extents_fit_alignment(extents_t *extents, size_t min_size, size_t max_size,
311      size_t alignment) {
312          pszind_t pind = sz_psz2ind(extent_size_quantize_ceil(min_size));
313          pszind_t pind_max = sz_psz2ind(extent_size_quantize_ceil(max_size));
314  	for (pszind_t i = (pszind_t)bitmap_ffu(extents->bitmap,
315  	    &extents_bitmap_info, (size_t)pind); i < pind_max; i =
316  	    (pszind_t)bitmap_ffu(extents->bitmap, &extents_bitmap_info,
317  	    (size_t)i+1)) {
318  		assert(i < SC_NPSIZES);
319  		assert(!extent_heap_empty(&extents->heaps[i]));
320  		extent_t *extent = extent_heap_first(&extents->heaps[i]);
321  		uintptr_t base = (uintptr_t)extent_base_get(extent);
322  		size_t candidate_size = extent_size_get(extent);
323  		assert(candidate_size >= min_size);
324  		uintptr_t next_align = ALIGNMENT_CEILING((uintptr_t)base,
325  		    PAGE_CEILING(alignment));
326  		if (base > next_align || base + candidate_size <= next_align) {
327  			continue;
328  		}
329  		size_t leadsize = next_align - base;
330  		if (candidate_size - leadsize >= min_size) {
331  			return extent;
332  		}
333  	}
334  	return NULL;
335  }
336  static extent_t *
337  extents_first_fit_locked(tsdn_t *tsdn, arena_t *arena, extents_t *extents,
338      size_t size) {
339  	extent_t *ret = NULL;
340  	pszind_t pind = sz_psz2ind(extent_size_quantize_ceil(size));
341  	if (!maps_coalesce && !opt_retain) {
342  		return extent_heap_empty(&extents->heaps[pind]) ? NULL :
343  		    extent_heap_first(&extents->heaps[pind]);
344  	}
345  	for (pszind_t i = (pszind_t)bitmap_ffu(extents->bitmap,
346  	    &extents_bitmap_info, (size_t)pind);
347  	    i < SC_NPSIZES + 1;
348  	    i = (pszind_t)bitmap_ffu(extents->bitmap, &extents_bitmap_info,
349  	    (size_t)i+1)) {
350  		assert(!extent_heap_empty(&extents->heaps[i]));
351  		extent_t *extent = extent_heap_first(&extents->heaps[i]);
352  		assert(extent_size_get(extent) >= size);
353  		if (extents->delay_coalesce &&
354  		    (sz_pind2sz(i) >> opt_lg_extent_max_active_fit) > size) {
355  			break;
356  		}
357  		if (ret == NULL || extent_snad_comp(extent, ret) < 0) {
358  			ret = extent;
359  		}
360  		if (i == SC_NPSIZES) {
361  			break;
362  		}
363  		assert(i < SC_NPSIZES);
364  	}
365  	return ret;
366  }
367  static extent_t *
368  extents_fit_locked(tsdn_t *tsdn, arena_t *arena, extents_t *extents,
369      size_t esize, size_t alignment) {
370  	malloc_mutex_assert_owner(tsdn, &extents->mtx);
371  	size_t max_size = esize + PAGE_CEILING(alignment) - PAGE;
372  	if (max_size < esize) {
373  		return NULL;
374  	}
375  	extent_t *extent =
376  	    extents_first_fit_locked(tsdn, arena, extents, max_size);
377  	if (alignment > PAGE && extent == NULL) {
378  		extent = extents_fit_alignment(extents, esize, max_size,
379  		    alignment);
380  	}
381  	return extent;
382  }
383  static bool
384  extent_try_delayed_coalesce(tsdn_t *tsdn, arena_t *arena,
385      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
386      extent_t *extent) {
387  	extent_state_set(extent, extent_state_active);
388  	bool coalesced;
389  	extent = extent_try_coalesce(tsdn, arena, r_extent_hooks, rtree_ctx,
390  	    extents, extent, &coalesced, false);
391  	extent_state_set(extent, extents_state_get(extents));
392  	if (!coalesced) {
393  		return true;
394  	}
395  	extents_insert_locked(tsdn, extents, extent);
396  	return false;
397  }
398  extent_t *
399  extents_alloc(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
400      extents_t *extents, void *new_addr, size_t size, size_t pad,
401      size_t alignment, bool slab, szind_t szind, bool *zero, bool *commit) {
402  	assert(size + pad != 0);
403  	assert(alignment != 0);
404  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
405  	    WITNESS_RANK_CORE, 0);
406  	extent_t *extent = extent_recycle(tsdn, arena, r_extent_hooks, extents,
407  	    new_addr, size, pad, alignment, slab, szind, zero, commit, false);
408  	assert(extent == NULL || extent_dumpable_get(extent));
409  	return extent;
410  }
411  void
412  extents_dalloc(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
413      extents_t *extents, extent_t *extent) {
414  	assert(extent_base_get(extent) != NULL);
415  	assert(extent_size_get(extent) != 0);
416  	assert(extent_dumpable_get(extent));
417  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
418  	    WITNESS_RANK_CORE, 0);
419  	extent_addr_set(extent, extent_base_get(extent));
420  	extent_zeroed_set(extent, false);
421  	extent_record(tsdn, arena, r_extent_hooks, extents, extent, false);
422  }
423  extent_t *
424  extents_evict(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
425      extents_t *extents, size_t npages_min) {
426  	rtree_ctx_t rtree_ctx_fallback;
427  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
428  	malloc_mutex_lock(tsdn, &extents->mtx);
429  	extent_t *extent;
430  	while (true) {
431  		extent = extent_list_first(&extents->lru);
432  		if (extent == NULL) {
433  			goto label_return;
434  		}
435  		size_t extents_npages = atomic_load_zu(&extents->npages,
436  		    ATOMIC_RELAXED);
437  		if (extents_npages <= npages_min) {
438  			extent = NULL;
439  			goto label_return;
440  		}
441  		extents_remove_locked(tsdn, extents, extent);
442  		if (!extents->delay_coalesce) {
443  			break;
444  		}
445  		if (extent_try_delayed_coalesce(tsdn, arena, r_extent_hooks,
446  		    rtree_ctx, extents, extent)) {
447  			break;
448  		}
449  	}
450  	switch (extents_state_get(extents)) {
451  	case extent_state_active:
452  		not_reached();
453  	case extent_state_dirty:
454  	case extent_state_muzzy:
455  		extent_state_set(extent, extent_state_active);
456  		break;
457  	case extent_state_retained:
458  		extent_deregister(tsdn, extent);
459  		break;
460  	default:
461  		not_reached();
462  	}
463  label_return:
464  	malloc_mutex_unlock(tsdn, &extents->mtx);
465  	return extent;
466  }
467  static void
468  extents_abandon_vm(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
469      extents_t *extents, extent_t *extent, bool growing_retained) {
470  	size_t sz = extent_size_get(extent);
471  	if (config_stats) {
472  		arena_stats_accum_zu(&arena->stats.abandoned_vm, sz);
473  	}
474  	if (extents_state_get(extents) == extent_state_dirty) {
475  		if (extent_purge_lazy_impl(tsdn, arena, r_extent_hooks,
476  		    extent, 0, sz, growing_retained)) {
477  			extent_purge_forced_impl(tsdn, arena, r_extent_hooks,
478  			    extent, 0, extent_size_get(extent),
479  			    growing_retained);
480  		}
481  	}
482  	extent_dalloc(tsdn, arena, extent);
483  }
484  void
485  extents_prefork(tsdn_t *tsdn, extents_t *extents) {
486  	malloc_mutex_prefork(tsdn, &extents->mtx);
487  }
488  void
489  extents_postfork_parent(tsdn_t *tsdn, extents_t *extents) {
490  	malloc_mutex_postfork_parent(tsdn, &extents->mtx);
491  }
492  void
493  extents_postfork_child(tsdn_t *tsdn, extents_t *extents) {
494  	malloc_mutex_postfork_child(tsdn, &extents->mtx);
495  }
496  static void
497  extent_deactivate_locked(tsdn_t *tsdn, arena_t *arena, extents_t *extents,
498      extent_t *extent) {
499  	assert(extent_arena_get(extent) == arena);
500  	assert(extent_state_get(extent) == extent_state_active);
501  	extent_state_set(extent, extents_state_get(extents));
502  	extents_insert_locked(tsdn, extents, extent);
503  }
504  static void
505  extent_deactivate(tsdn_t *tsdn, arena_t *arena, extents_t *extents,
506      extent_t *extent) {
507  	malloc_mutex_lock(tsdn, &extents->mtx);
508  	extent_deactivate_locked(tsdn, arena, extents, extent);
509  	malloc_mutex_unlock(tsdn, &extents->mtx);
510  }
511  static void
512  extent_activate_locked(tsdn_t *tsdn, arena_t *arena, extents_t *extents,
513      extent_t *extent) {
514  	assert(extent_arena_get(extent) == arena);
515  	assert(extent_state_get(extent) == extents_state_get(extents));
516  	extents_remove_locked(tsdn, extents, extent);
517  	extent_state_set(extent, extent_state_active);
518  }
519  static bool
520  extent_rtree_leaf_elms_lookup(tsdn_t *tsdn, rtree_ctx_t *rtree_ctx,
521      const extent_t *extent, bool dependent, bool init_missing,
522      rtree_leaf_elm_t **r_elm_a, rtree_leaf_elm_t **r_elm_b) {
523  	*r_elm_a = rtree_leaf_elm_lookup(tsdn, &extents_rtree, rtree_ctx,
524  	    (uintptr_t)extent_base_get(extent), dependent, init_missing);
525  	if (!dependent && *r_elm_a == NULL) {
526  		return true;
527  	}
528  	assert(*r_elm_a != NULL);
529  	*r_elm_b = rtree_leaf_elm_lookup(tsdn, &extents_rtree, rtree_ctx,
530  	    (uintptr_t)extent_last_get(extent), dependent, init_missing);
531  	if (!dependent && *r_elm_b == NULL) {
532  		return true;
533  	}
534  	assert(*r_elm_b != NULL);
535  	return false;
536  }
537  static void
538  extent_rtree_write_acquired(tsdn_t *tsdn, rtree_leaf_elm_t *elm_a,
539      rtree_leaf_elm_t *elm_b, extent_t *extent, szind_t szind, bool slab) {
540  	rtree_leaf_elm_write(tsdn, &extents_rtree, elm_a, extent, szind, slab);
541  	if (elm_b != NULL) {
542  		rtree_leaf_elm_write(tsdn, &extents_rtree, elm_b, extent, szind,
543  		    slab);
544  	}
545  }
546  static void
547  extent_interior_register(tsdn_t *tsdn, rtree_ctx_t *rtree_ctx, extent_t *extent,
548      szind_t szind) {
549  	assert(extent_slab_get(extent));
550  	for (size_t i = 1; i < (extent_size_get(extent) >> LG_PAGE) - 1; i++) {
551  		rtree_write(tsdn, &extents_rtree, rtree_ctx,
552  		    (uintptr_t)extent_base_get(extent) + (uintptr_t)(i <<
553  		    LG_PAGE), extent, szind, true);
554  	}
555  }
556  static void
557  extent_gdump_add(tsdn_t *tsdn, const extent_t *extent) {
558  	cassert(config_prof);
559  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
560  	    WITNESS_RANK_CORE, 0);
561  	if (opt_prof && extent_state_get(extent) == extent_state_active) {
562  		size_t nadd = extent_size_get(extent) >> LG_PAGE;
563  		size_t cur = atomic_fetch_add_zu(&curpages, nadd,
564  		    ATOMIC_RELAXED) + nadd;
565  		size_t high = atomic_load_zu(&highpages, ATOMIC_RELAXED);
566  		while (cur > high && !atomic_compare_exchange_weak_zu(
567  		    &highpages, &high, cur, ATOMIC_RELAXED, ATOMIC_RELAXED)) {
568  		}
569  		if (cur > high && prof_gdump_get_unlocked()) {
570  			prof_gdump(tsdn);
571  		}
572  	}
573  }
574  static void
575  extent_gdump_sub(tsdn_t *tsdn, const extent_t *extent) {
576  	cassert(config_prof);
577  	if (opt_prof && extent_state_get(extent) == extent_state_active) {
578  		size_t nsub = extent_size_get(extent) >> LG_PAGE;
579  		assert(atomic_load_zu(&curpages, ATOMIC_RELAXED) >= nsub);
580  		atomic_fetch_sub_zu(&curpages, nsub, ATOMIC_RELAXED);
581  	}
582  }
583  static bool
584  extent_register_impl(tsdn_t *tsdn, extent_t *extent, bool gdump_add) {
585  	rtree_ctx_t rtree_ctx_fallback;
586  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
587  	rtree_leaf_elm_t *elm_a, *elm_b;
588  	extent_lock(tsdn, extent);
589  	if (extent_rtree_leaf_elms_lookup(tsdn, rtree_ctx, extent, false, true,
590  	    &elm_a, &elm_b)) {
591  		extent_unlock(tsdn, extent);
592  		return true;
593  	}
594  	szind_t szind = extent_szind_get_maybe_invalid(extent);
595  	bool slab = extent_slab_get(extent);
596  	extent_rtree_write_acquired(tsdn, elm_a, elm_b, extent, szind, slab);
597  	if (slab) {
598  		extent_interior_register(tsdn, rtree_ctx, extent, szind);
599  	}
600  	extent_unlock(tsdn, extent);
601  	if (config_prof && gdump_add) {
602  		extent_gdump_add(tsdn, extent);
603  	}
604  	return false;
605  }
606  static bool
607  extent_register(tsdn_t *tsdn, extent_t *extent) {
608  	return extent_register_impl(tsdn, extent, true);
609  }
610  static bool
611  extent_register_no_gdump_add(tsdn_t *tsdn, extent_t *extent) {
612  	return extent_register_impl(tsdn, extent, false);
613  }
614  static void
615  extent_reregister(tsdn_t *tsdn, extent_t *extent) {
616  	bool err = extent_register(tsdn, extent);
617  	assert(!err);
618  }
619  static void
620  extent_interior_deregister(tsdn_t *tsdn, rtree_ctx_t *rtree_ctx,
621      extent_t *extent) {
622  	size_t i;
623  	assert(extent_slab_get(extent));
624  	for (i = 1; i < (extent_size_get(extent) >> LG_PAGE) - 1; i++) {
625  		rtree_clear(tsdn, &extents_rtree, rtree_ctx,
626  		    (uintptr_t)extent_base_get(extent) + (uintptr_t)(i <<
627  		    LG_PAGE));
628  	}
629  }
630  static void
631  extent_deregister_impl(tsdn_t *tsdn, extent_t *extent, bool gdump) {
632  	rtree_ctx_t rtree_ctx_fallback;
633  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
634  	rtree_leaf_elm_t *elm_a, *elm_b;
635  	extent_rtree_leaf_elms_lookup(tsdn, rtree_ctx, extent, true, false,
636  	    &elm_a, &elm_b);
637  	extent_lock(tsdn, extent);
638  	extent_rtree_write_acquired(tsdn, elm_a, elm_b, NULL, SC_NSIZES, false);
639  	if (extent_slab_get(extent)) {
640  		extent_interior_deregister(tsdn, rtree_ctx, extent);
641  		extent_slab_set(extent, false);
642  	}
643  	extent_unlock(tsdn, extent);
644  	if (config_prof && gdump) {
645  		extent_gdump_sub(tsdn, extent);
646  	}
647  }
648  static void
649  extent_deregister(tsdn_t *tsdn, extent_t *extent) {
650  	extent_deregister_impl(tsdn, extent, true);
651  }
652  static void
653  extent_deregister_no_gdump_sub(tsdn_t *tsdn, extent_t *extent) {
654  	extent_deregister_impl(tsdn, extent, false);
655  }
656  static extent_t *
657  extent_recycle_extract(tsdn_t *tsdn, arena_t *arena,
658      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
659      void *new_addr, size_t size, size_t pad, size_t alignment, bool slab,
660      bool growing_retained) {
661  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
662  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
663  	assert(alignment > 0);
664  	if (config_debug && new_addr != NULL) {
665  		assert(PAGE_ADDR2BASE(new_addr) == new_addr);
666  		assert(pad == 0);
667  		assert(alignment <= PAGE);
668  	}
669  	size_t esize = size + pad;
670  	malloc_mutex_lock(tsdn, &extents->mtx);
671  	extent_hooks_assure_initialized(arena, r_extent_hooks);
672  	extent_t *extent;
673  	if (new_addr != NULL) {
674  		extent = extent_lock_from_addr(tsdn, rtree_ctx, new_addr,
675  		    false);
676  		if (extent != NULL) {
677  			extent_t *unlock_extent = extent;
678  			assert(extent_base_get(extent) == new_addr);
679  			if (extent_arena_get(extent) != arena ||
680  			    extent_size_get(extent) < esize ||
681  			    extent_state_get(extent) !=
682  			    extents_state_get(extents)) {
683  				extent = NULL;
684  			}
685  			extent_unlock(tsdn, unlock_extent);
686  		}
687  	} else {
688  		extent = extents_fit_locked(tsdn, arena, extents, esize,
689  		    alignment);
690  	}
691  	if (extent == NULL) {
692  		malloc_mutex_unlock(tsdn, &extents->mtx);
693  		return NULL;
694  	}
695  	extent_activate_locked(tsdn, arena, extents, extent);
696  	malloc_mutex_unlock(tsdn, &extents->mtx);
697  	return extent;
698  }
699  typedef enum {
700  	extent_split_interior_ok,
701  	extent_split_interior_cant_alloc,
702  	extent_split_interior_error
703  } extent_split_interior_result_t;
704  static extent_split_interior_result_t
705  extent_split_interior(tsdn_t *tsdn, arena_t *arena,
706      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx,
707      extent_t **extent, extent_t **lead, extent_t **trail,
708      extent_t **to_leak, extent_t **to_salvage,
709      void *new_addr, size_t size, size_t pad, size_t alignment, bool slab,
710      szind_t szind, bool growing_retained) {
711  	size_t esize = size + pad;
712  	size_t leadsize = ALIGNMENT_CEILING((uintptr_t)extent_base_get(*extent),
713  	    PAGE_CEILING(alignment)) - (uintptr_t)extent_base_get(*extent);
714  	assert(new_addr == NULL || leadsize == 0);
715  	if (extent_size_get(*extent) < leadsize + esize) {
716  		return extent_split_interior_cant_alloc;
717  	}
718  	size_t trailsize = extent_size_get(*extent) - leadsize - esize;
719  	*lead = NULL;
720  	*trail = NULL;
721  	*to_leak = NULL;
722  	*to_salvage = NULL;
723  	if (leadsize != 0) {
724  		*lead = *extent;
725  		*extent = extent_split_impl(tsdn, arena, r_extent_hooks,
726  		    *lead, leadsize, SC_NSIZES, false, esize + trailsize, szind,
727  		    slab, growing_retained);
728  		if (*extent == NULL) {
729  			*to_leak = *lead;
730  			*lead = NULL;
731  			return extent_split_interior_error;
732  		}
733  	}
734  	if (trailsize != 0) {
735  		*trail = extent_split_impl(tsdn, arena, r_extent_hooks, *extent,
736  		    esize, szind, slab, trailsize, SC_NSIZES, false,
737  		    growing_retained);
738  		if (*trail == NULL) {
739  			*to_leak = *extent;
740  			*to_salvage = *lead;
741  			*lead = NULL;
742  			*extent = NULL;
743  			return extent_split_interior_error;
744  		}
745  	}
746  	if (leadsize == 0 && trailsize == 0) {
747  		extent_szind_set(*extent, szind);
748  		if (szind != SC_NSIZES) {
749  			rtree_szind_slab_update(tsdn, &extents_rtree, rtree_ctx,
750  			    (uintptr_t)extent_addr_get(*extent), szind, slab);
751  			if (slab && extent_size_get(*extent) > PAGE) {
752  				rtree_szind_slab_update(tsdn, &extents_rtree,
753  				    rtree_ctx,
754  				    (uintptr_t)extent_past_get(*extent) -
755  				    (uintptr_t)PAGE, szind, slab);
756  			}
757  		}
758  	}
759  	return extent_split_interior_ok;
760  }
761  static extent_t *
762  extent_recycle_split(tsdn_t *tsdn, arena_t *arena,
763      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
764      void *new_addr, size_t size, size_t pad, size_t alignment, bool slab,
765      szind_t szind, extent_t *extent, bool growing_retained) {
766  	extent_t *lead;
767  	extent_t *trail;
768  	extent_t *to_leak;
769  	extent_t *to_salvage;
770  	extent_split_interior_result_t result = extent_split_interior(
771  	    tsdn, arena, r_extent_hooks, rtree_ctx, &extent, &lead, &trail,
772  	    &to_leak, &to_salvage, new_addr, size, pad, alignment, slab, szind,
773  	    growing_retained);
774  	if (!maps_coalesce && result != extent_split_interior_ok
775  	    && !opt_retain) {
776  		assert(to_leak != NULL && lead == NULL && trail == NULL);
777  		extent_deactivate(tsdn, arena, extents, to_leak);
778  		return NULL;
779  	}
780  	if (result == extent_split_interior_ok) {
781  		if (lead != NULL) {
782  			extent_deactivate(tsdn, arena, extents, lead);
783  		}
784  		if (trail != NULL) {
785  			extent_deactivate(tsdn, arena, extents, trail);
786  		}
787  		return extent;
788  	} else {
789  		assert(result == extent_split_interior_error);
790  		if (to_salvage != NULL) {
791  			extent_deregister(tsdn, to_salvage);
792  		}
793  		if (to_leak != NULL) {
794  			void *leak = extent_base_get(to_leak);
795  			extent_deregister_no_gdump_sub(tsdn, to_leak);
796  			extents_abandon_vm(tsdn, arena, r_extent_hooks, extents,
797  			    to_leak, growing_retained);
798  			assert(extent_lock_from_addr(tsdn, rtree_ctx, leak,
799  			    false) == NULL);
800  		}
801  		return NULL;
802  	}
803  	unreachable();
804  }
805  static bool
806  extent_need_manual_zero(arena_t *arena) {
807  	return (!arena_has_default_hooks(arena) ||
808  		(opt_thp == thp_mode_always));
809  }
810  static extent_t *
811  extent_recycle(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
812      extents_t *extents, void *new_addr, size_t size, size_t pad,
813      size_t alignment, bool slab, szind_t szind, bool *zero, bool *commit,
814      bool growing_retained) {
815  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
816  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
817  	assert(new_addr == NULL || !slab);
818  	assert(pad == 0 || !slab);
819  	assert(!*zero || !slab);
820  	rtree_ctx_t rtree_ctx_fallback;
821  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
822  	extent_t *extent = extent_recycle_extract(tsdn, arena, r_extent_hooks,
823  	    rtree_ctx, extents, new_addr, size, pad, alignment, slab,
824  	    growing_retained);
825  	if (extent == NULL) {
826  		return NULL;
827  	}
828  	extent = extent_recycle_split(tsdn, arena, r_extent_hooks, rtree_ctx,
829  	    extents, new_addr, size, pad, alignment, slab, szind, extent,
830  	    growing_retained);
831  	if (extent == NULL) {
832  		return NULL;
833  	}
834  	if (*commit && !extent_committed_get(extent)) {
835  		if (extent_commit_impl(tsdn, arena, r_extent_hooks, extent,
836  		    0, extent_size_get(extent), growing_retained)) {
837  			extent_record(tsdn, arena, r_extent_hooks, extents,
838  			    extent, growing_retained);
839  			return NULL;
840  		}
841  		if (!extent_need_manual_zero(arena)) {
842  			extent_zeroed_set(extent, true);
843  		}
844  	}
845  	if (extent_committed_get(extent)) {
846  		*commit = true;
847  	}
848  	if (extent_zeroed_get(extent)) {
849  		*zero = true;
850  	}
851  	if (pad != 0) {
852  		extent_addr_randomize(tsdn, extent, alignment);
853  	}
854  	assert(extent_state_get(extent) == extent_state_active);
855  	if (slab) {
856  		extent_slab_set(extent, slab);
857  		extent_interior_register(tsdn, rtree_ctx, extent, szind);
858  	}
859  	if (*zero) {
860  		void *addr = extent_base_get(extent);
861  		if (!extent_zeroed_get(extent)) {
862  			size_t size = extent_size_get(extent);
863  			if (extent_need_manual_zero(arena) ||
864  			    pages_purge_forced(addr, size)) {
865  				memset(addr, 0, size);
866  			}
867  		} else if (config_debug) {
868  			size_t *p = (size_t *)(uintptr_t)addr;
869  			for (size_t i = 0; i < PAGE / sizeof(size_t); i++) {
870  				assert(p[i] == 0);
871  			}
872  		}
873  	}
874  	return extent;
875  }
876  static void *
877  extent_alloc_core(tsdn_t *tsdn, arena_t *arena, void *new_addr, size_t size,
878      size_t alignment, bool *zero, bool *commit, dss_prec_t dss_prec) {
879  	void *ret;
880  	assert(size != 0);
881  	assert(alignment != 0);
882  	if (have_dss && dss_prec == dss_prec_primary && (ret =
883  	    extent_alloc_dss(tsdn, arena, new_addr, size, alignment, zero,
884  	    commit)) != NULL) {
885  		return ret;
886  	}
887  	if ((ret = extent_alloc_mmap(new_addr, size, alignment, zero, commit))
888  	    != NULL) {
889  		return ret;
890  	}
891  	if (have_dss && dss_prec == dss_prec_secondary && (ret =
892  	    extent_alloc_dss(tsdn, arena, new_addr, size, alignment, zero,
893  	    commit)) != NULL) {
894  		return ret;
895  	}
896  	return NULL;
897  }
898  static void *
899  extent_alloc_default_impl(tsdn_t *tsdn, arena_t *arena, void *new_addr,
900      size_t size, size_t alignment, bool *zero, bool *commit) {
901  	void *ret = extent_alloc_core(tsdn, arena, new_addr, size, alignment, zero,
902  	    commit, (dss_prec_t)atomic_load_u(&arena->dss_prec,
903  	    ATOMIC_RELAXED));
904  	if (have_madvise_huge && ret) {
905  		pages_set_thp_state(ret, size);
906  	}
907  	return ret;
908  }
909  static void *
910  extent_alloc_default(extent_hooks_t *extent_hooks, void *new_addr, size_t size,
911      size_t alignment, bool *zero, bool *commit, unsigned arena_ind) {
912  	tsdn_t *tsdn;
913  	arena_t *arena;
914  	tsdn = tsdn_fetch();
915  	arena = arena_get(tsdn, arena_ind, false);
916  	assert(arena != NULL);
917  	return extent_alloc_default_impl(tsdn, arena, new_addr, size,
918  	    ALIGNMENT_CEILING(alignment, PAGE), zero, commit);
919  }
920  static void
921  extent_hook_pre_reentrancy(tsdn_t *tsdn, arena_t *arena) {
922  	tsd_t *tsd = tsdn_null(tsdn) ? tsd_fetch() : tsdn_tsd(tsdn);
923  	if (arena == arena_get(tsd_tsdn(tsd), 0, false)) {
924  		pre_reentrancy(tsd, NULL);
925  	} else {
926  		pre_reentrancy(tsd, arena);
927  	}
928  }
929  static void
930  extent_hook_post_reentrancy(tsdn_t *tsdn) {
931  	tsd_t *tsd = tsdn_null(tsdn) ? tsd_fetch() : tsdn_tsd(tsdn);
932  	post_reentrancy(tsd);
933  }
934  static extent_t *
935  extent_grow_retained(tsdn_t *tsdn, arena_t *arena,
936      extent_hooks_t **r_extent_hooks, size_t size, size_t pad, size_t alignment,
937      bool slab, szind_t szind, bool *zero, bool *commit) {
938  	malloc_mutex_assert_owner(tsdn, &arena->extent_grow_mtx);
939  	assert(pad == 0 || !slab);
940  	assert(!*zero || !slab);
941  	size_t esize = size + pad;
942  	size_t alloc_size_min = esize + PAGE_CEILING(alignment) - PAGE;
943  	if (alloc_size_min < esize) {
944  		goto label_err;
945  	}
946  	pszind_t egn_skip = 0;
947  	size_t alloc_size = sz_pind2sz(arena->extent_grow_next + egn_skip);
948  	while (alloc_size < alloc_size_min) {
949  		egn_skip++;
950  		if (arena->extent_grow_next + egn_skip >=
951  		    sz_psz2ind(SC_LARGE_MAXCLASS)) {
952  			goto label_err;
953  		}
954  		alloc_size = sz_pind2sz(arena->extent_grow_next + egn_skip);
955  	}
956  	extent_t *extent = extent_alloc(tsdn, arena);
957  	if (extent == NULL) {
958  		goto label_err;
959  	}
960  	bool zeroed = false;
961  	bool committed = false;
962  	void *ptr;
963  	if (*r_extent_hooks == &extent_hooks_default) {
964  		ptr = extent_alloc_default_impl(tsdn, arena, NULL,
965  		    alloc_size, PAGE, &zeroed, &committed);
966  	} else {
967  		extent_hook_pre_reentrancy(tsdn, arena);
968  		ptr = (*r_extent_hooks)->alloc(*r_extent_hooks, NULL,
969  		    alloc_size, PAGE, &zeroed, &committed,
970  		    arena_ind_get(arena));
971  		extent_hook_post_reentrancy(tsdn);
972  	}
973  	extent_init(extent, arena, ptr, alloc_size, false, SC_NSIZES,
974  	    arena_extent_sn_next(arena), extent_state_active, zeroed,
975  	    committed, true, EXTENT_IS_HEAD);
976  	if (ptr == NULL) {
977  		extent_dalloc(tsdn, arena, extent);
978  		goto label_err;
979  	}
980  	if (extent_register_no_gdump_add(tsdn, extent)) {
981  		extent_dalloc(tsdn, arena, extent);
982  		goto label_err;
983  	}
984  	if (extent_zeroed_get(extent) && extent_committed_get(extent)) {
985  		*zero = true;
986  	}
987  	if (extent_committed_get(extent)) {
988  		*commit = true;
989  	}
990  	rtree_ctx_t rtree_ctx_fallback;
991  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
992  	extent_t *lead;
993  	extent_t *trail;
994  	extent_t *to_leak;
995  	extent_t *to_salvage;
996  	extent_split_interior_result_t result = extent_split_interior(
997  	    tsdn, arena, r_extent_hooks, rtree_ctx, &extent, &lead, &trail,
998  	    &to_leak, &to_salvage, NULL, size, pad, alignment, slab, szind,
999  	    true);
1000  	if (result == extent_split_interior_ok) {
1001  		if (lead != NULL) {
1002  			extent_record(tsdn, arena, r_extent_hooks,
1003  			    &arena->extents_retained, lead, true);
1004  		}
1005  		if (trail != NULL) {
1006  			extent_record(tsdn, arena, r_extent_hooks,
1007  			    &arena->extents_retained, trail, true);
1008  		}
1009  	} else {
1010  		assert(result == extent_split_interior_error);
1011  		if (to_salvage != NULL) {
1012  			if (config_prof) {
1013  				extent_gdump_add(tsdn, to_salvage);
1014  			}
1015  			extent_record(tsdn, arena, r_extent_hooks,
1016  			    &arena->extents_retained, to_salvage, true);
1017  		}
1018  		if (to_leak != NULL) {
1019  			extent_deregister_no_gdump_sub(tsdn, to_leak);
1020  			extents_abandon_vm(tsdn, arena, r_extent_hooks,
1021  			    &arena->extents_retained, to_leak, true);
1022  		}
1023  		goto label_err;
1024  	}
1025  	if (*commit && !extent_committed_get(extent)) {
1026  		if (extent_commit_impl(tsdn, arena, r_extent_hooks, extent, 0,
1027  		    extent_size_get(extent), true)) {
1028  			extent_record(tsdn, arena, r_extent_hooks,
1029  			    &arena->extents_retained, extent, true);
1030  			goto label_err;
1031  		}
1032  		if (!extent_need_manual_zero(arena)) {
1033  			extent_zeroed_set(extent, true);
1034  		}
1035  	}
1036  	if (arena->extent_grow_next + egn_skip + 1 <=
1037  	    arena->retain_grow_limit) {
1038  		arena->extent_grow_next += egn_skip + 1;
1039  	} else {
1040  		arena->extent_grow_next = arena->retain_grow_limit;
1041  	}
1042  	malloc_mutex_unlock(tsdn, &arena->extent_grow_mtx);
1043  	if (config_prof) {
1044  		extent_gdump_add(tsdn, extent);
1045  	}
1046  	if (pad != 0) {
1047  		extent_addr_randomize(tsdn, extent, alignment);
1048  	}
1049  	if (slab) {
1050  		rtree_ctx_t rtree_ctx_fallback;
1051  		rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn,
1052  		    &rtree_ctx_fallback);
1053  		extent_slab_set(extent, true);
1054  		extent_interior_register(tsdn, rtree_ctx, extent, szind);
1055  	}
1056  	if (*zero && !extent_zeroed_get(extent)) {
1057  		void *addr = extent_base_get(extent);
1058  		size_t size = extent_size_get(extent);
1059  		if (extent_need_manual_zero(arena) ||
1060  		    pages_purge_forced(addr, size)) {
1061  			memset(addr, 0, size);
1062  		}
1063  	}
1064  	return extent;
1065  label_err:
1066  	malloc_mutex_unlock(tsdn, &arena->extent_grow_mtx);
1067  	return NULL;
1068  }
1069  static extent_t *
1070  extent_alloc_retained(tsdn_t *tsdn, arena_t *arena,
1071      extent_hooks_t **r_extent_hooks, void *new_addr, size_t size, size_t pad,
1072      size_t alignment, bool slab, szind_t szind, bool *zero, bool *commit) {
1073  	assert(size != 0);
1074  	assert(alignment != 0);
1075  	malloc_mutex_lock(tsdn, &arena->extent_grow_mtx);
1076  	extent_t *extent = extent_recycle(tsdn, arena, r_extent_hooks,
1077  	    &arena->extents_retained, new_addr, size, pad, alignment, slab,
1078  	    szind, zero, commit, true);
1079  	if (extent != NULL) {
1080  		malloc_mutex_unlock(tsdn, &arena->extent_grow_mtx);
1081  		if (config_prof) {
1082  			extent_gdump_add(tsdn, extent);
1083  		}
1084  	} else if (opt_retain && new_addr == NULL) {
1085  		extent = extent_grow_retained(tsdn, arena, r_extent_hooks, size,
1086  		    pad, alignment, slab, szind, zero, commit);
1087  	} else {
1088  		malloc_mutex_unlock(tsdn, &arena->extent_grow_mtx);
1089  	}
1090  	malloc_mutex_assert_not_owner(tsdn, &arena->extent_grow_mtx);
1091  	return extent;
1092  }
1093  static extent_t *
1094  extent_alloc_wrapper_hard(tsdn_t *tsdn, arena_t *arena,
1095      extent_hooks_t **r_extent_hooks, void *new_addr, size_t size, size_t pad,
1096      size_t alignment, bool slab, szind_t szind, bool *zero, bool *commit) {
1097  	size_t esize = size + pad;
1098  	extent_t *extent = extent_alloc(tsdn, arena);
1099  	if (extent == NULL) {
1100  		return NULL;
1101  	}
1102  	void *addr;
1103  	size_t palignment = ALIGNMENT_CEILING(alignment, PAGE);
1104  	if (*r_extent_hooks == &extent_hooks_default) {
1105  		addr = extent_alloc_default_impl(tsdn, arena, new_addr, esize,
1106  		    palignment, zero, commit);
1107  	} else {
1108  		extent_hook_pre_reentrancy(tsdn, arena);
1109  		addr = (*r_extent_hooks)->alloc(*r_extent_hooks, new_addr,
1110  		    esize, palignment, zero, commit, arena_ind_get(arena));
1111  		extent_hook_post_reentrancy(tsdn);
1112  	}
1113  	if (addr == NULL) {
1114  		extent_dalloc(tsdn, arena, extent);
1115  		return NULL;
1116  	}
1117  	extent_init(extent, arena, addr, esize, slab, szind,
1118  	    arena_extent_sn_next(arena), extent_state_active, *zero, *commit,
1119  	    true, EXTENT_NOT_HEAD);
1120  	if (pad != 0) {
1121  		extent_addr_randomize(tsdn, extent, alignment);
1122  	}
1123  	if (extent_register(tsdn, extent)) {
1124  		extent_dalloc(tsdn, arena, extent);
1125  		return NULL;
1126  	}
1127  	return extent;
1128  }
1129  extent_t *
1130  extent_alloc_wrapper(tsdn_t *tsdn, arena_t *arena,
1131      extent_hooks_t **r_extent_hooks, void *new_addr, size_t size, size_t pad,
1132      size_t alignment, bool slab, szind_t szind, bool *zero, bool *commit) {
1133  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1134  	    WITNESS_RANK_CORE, 0);
1135  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1136  	extent_t *extent = extent_alloc_retained(tsdn, arena, r_extent_hooks,
1137  	    new_addr, size, pad, alignment, slab, szind, zero, commit);
1138  	if (extent == NULL) {
1139  		if (opt_retain && new_addr != NULL) {
1140  			return NULL;
1141  		}
1142  		extent = extent_alloc_wrapper_hard(tsdn, arena, r_extent_hooks,
1143  		    new_addr, size, pad, alignment, slab, szind, zero, commit);
1144  	}
1145  	assert(extent == NULL || extent_dumpable_get(extent));
1146  	return extent;
1147  }
1148  static bool
1149  extent_can_coalesce(arena_t *arena, extents_t *extents, const extent_t *inner,
1150      const extent_t *outer) {
1151  	assert(extent_arena_get(inner) == arena);
1152  	if (extent_arena_get(outer) != arena) {
1153  		return false;
1154  	}
1155  	assert(extent_state_get(inner) == extent_state_active);
1156  	if (extent_state_get(outer) != extents->state) {
1157  		return false;
1158  	}
1159  	if (extent_committed_get(inner) != extent_committed_get(outer)) {
1160  		return false;
1161  	}
1162  	return true;
1163  }
1164  static bool
1165  extent_coalesce(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
1166      extents_t *extents, extent_t *inner, extent_t *outer, bool forward,
1167      bool growing_retained) {
1168  	assert(extent_can_coalesce(arena, extents, inner, outer));
1169  	extent_activate_locked(tsdn, arena, extents, outer);
1170  	malloc_mutex_unlock(tsdn, &extents->mtx);
1171  	bool err = extent_merge_impl(tsdn, arena, r_extent_hooks,
1172  	    forward ? inner : outer, forward ? outer : inner, growing_retained);
1173  	malloc_mutex_lock(tsdn, &extents->mtx);
1174  	if (err) {
1175  		extent_deactivate_locked(tsdn, arena, extents, outer);
1176  	}
1177  	return err;
1178  }
1179  static extent_t *
1180  extent_try_coalesce_impl(tsdn_t *tsdn, arena_t *arena,
1181      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
1182      extent_t *extent, bool *coalesced, bool growing_retained,
1183      bool inactive_only) {
1184  	bool again;
1185  	do {
1186  		again = false;
1187  		extent_t *next = extent_lock_from_addr(tsdn, rtree_ctx,
1188  		    extent_past_get(extent), inactive_only);
1189  		if (next != NULL) {
1190  			bool can_coalesce = extent_can_coalesce(arena, extents,
1191  			    extent, next);
1192  			extent_unlock(tsdn, next);
1193  			if (can_coalesce && !extent_coalesce(tsdn, arena,
1194  			    r_extent_hooks, extents, extent, next, true,
1195  			    growing_retained)) {
1196  				if (extents->delay_coalesce) {
1197  					*coalesced = true;
1198  					return extent;
1199  				}
1200  				again = true;
1201  			}
1202  		}
1203  		extent_t *prev = extent_lock_from_addr(tsdn, rtree_ctx,
1204  		    extent_before_get(extent), inactive_only);
1205  		if (prev != NULL) {
1206  			bool can_coalesce = extent_can_coalesce(arena, extents,
1207  			    extent, prev);
1208  			extent_unlock(tsdn, prev);
1209  			if (can_coalesce && !extent_coalesce(tsdn, arena,
1210  			    r_extent_hooks, extents, extent, prev, false,
1211  			    growing_retained)) {
1212  				extent = prev;
1213  				if (extents->delay_coalesce) {
1214  					*coalesced = true;
1215  					return extent;
1216  				}
1217  				again = true;
1218  			}
1219  		}
1220  	} while (again);
1221  	if (extents->delay_coalesce) {
1222  		*coalesced = false;
1223  	}
1224  	return extent;
1225  }
1226  static extent_t *
1227  extent_try_coalesce(tsdn_t *tsdn, arena_t *arena,
1228      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
1229      extent_t *extent, bool *coalesced, bool growing_retained) {
1230  	return extent_try_coalesce_impl(tsdn, arena, r_extent_hooks, rtree_ctx,
1231  	    extents, extent, coalesced, growing_retained, false);
1232  }
1233  static extent_t *
1234  extent_try_coalesce_large(tsdn_t *tsdn, arena_t *arena,
1235      extent_hooks_t **r_extent_hooks, rtree_ctx_t *rtree_ctx, extents_t *extents,
1236      extent_t *extent, bool *coalesced, bool growing_retained) {
1237  	return extent_try_coalesce_impl(tsdn, arena, r_extent_hooks, rtree_ctx,
1238  	    extents, extent, coalesced, growing_retained, true);
1239  }
1240  static void
1241  extent_record(tsdn_t *tsdn, arena_t *arena, extent_hooks_t **r_extent_hooks,
1242      extents_t *extents, extent_t *extent, bool growing_retained) {
1243  	rtree_ctx_t rtree_ctx_fallback;
1244  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1245  	assert((extents_state_get(extents) != extent_state_dirty &&
1246  	    extents_state_get(extents) != extent_state_muzzy) ||
1247  	    !extent_zeroed_get(extent));
1248  	malloc_mutex_lock(tsdn, &extents->mtx);
1249  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1250  	extent_szind_set(extent, SC_NSIZES);
1251  	if (extent_slab_get(extent)) {
1252  		extent_interior_deregister(tsdn, rtree_ctx, extent);
1253  		extent_slab_set(extent, false);
1254  	}
1255  	assert(rtree_extent_read(tsdn, &extents_rtree, rtree_ctx,
1256  	    (uintptr_t)extent_base_get(extent), true) == extent);
1257  	if (!extents->delay_coalesce) {
1258  		extent = extent_try_coalesce(tsdn, arena, r_extent_hooks,
1259  		    rtree_ctx, extents, extent, NULL, growing_retained);
1260  	} else if (extent_size_get(extent) >= SC_LARGE_MINCLASS) {
1261  		assert(extents == &arena->extents_dirty);
1262  		bool coalesced;
1263  		do {
1264  			assert(extent_state_get(extent) == extent_state_active);
1265  			extent = extent_try_coalesce_large(tsdn, arena,
1266  			    r_extent_hooks, rtree_ctx, extents, extent,
1267  			    &coalesced, growing_retained);
1268  		} while (coalesced);
1269  		if (extent_size_get(extent) >= oversize_threshold) {
1270  			malloc_mutex_unlock(tsdn, &extents->mtx);
1271  			arena_decay_extent(tsdn, arena, r_extent_hooks, extent);
1272  			return;
1273  		}
1274  	}
1275  	extent_deactivate_locked(tsdn, arena, extents, extent);
1276  	malloc_mutex_unlock(tsdn, &extents->mtx);
1277  }
1278  void
1279  extent_dalloc_gap(tsdn_t *tsdn, arena_t *arena, extent_t *extent) {
1280  	extent_hooks_t *extent_hooks = EXTENT_HOOKS_INITIALIZER;
1281  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1282  	    WITNESS_RANK_CORE, 0);
1283  	if (extent_register(tsdn, extent)) {
1284  		extent_dalloc(tsdn, arena, extent);
1285  		return;
1286  	}
1287  	extent_dalloc_wrapper(tsdn, arena, &extent_hooks, extent);
1288  }
1289  static bool
1290  extent_may_dalloc(void) {
1291  	return !opt_retain;
1292  }
1293  static bool
1294  extent_dalloc_default_impl(void *addr, size_t size) {
1295  	if (!have_dss || !extent_in_dss(addr)) {
1296  		return extent_dalloc_mmap(addr, size);
1297  	}
1298  	return true;
1299  }
1300  static bool
1301  extent_dalloc_default(extent_hooks_t *extent_hooks, void *addr, size_t size,
1302      bool committed, unsigned arena_ind) {
1303  	return extent_dalloc_default_impl(addr, size);
1304  }
1305  static bool
1306  extent_dalloc_wrapper_try(tsdn_t *tsdn, arena_t *arena,
1307      extent_hooks_t **r_extent_hooks, extent_t *extent) {
1308  	bool err;
1309  	assert(extent_base_get(extent) != NULL);
1310  	assert(extent_size_get(extent) != 0);
1311  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1312  	    WITNESS_RANK_CORE, 0);
1313  	extent_addr_set(extent, extent_base_get(extent));
1314  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1315  	if (*r_extent_hooks == &extent_hooks_default) {
1316  		err = extent_dalloc_default_impl(extent_base_get(extent),
1317  		    extent_size_get(extent));
1318  	} else {
1319  		extent_hook_pre_reentrancy(tsdn, arena);
1320  		err = ((*r_extent_hooks)->dalloc == NULL ||
1321  		    (*r_extent_hooks)->dalloc(*r_extent_hooks,
1322  		    extent_base_get(extent), extent_size_get(extent),
1323  		    extent_committed_get(extent), arena_ind_get(arena)));
1324  		extent_hook_post_reentrancy(tsdn);
1325  	}
1326  	if (!err) {
1327  		extent_dalloc(tsdn, arena, extent);
1328  	}
1329  	return err;
1330  }
1331  void
1332  extent_dalloc_wrapper(tsdn_t *tsdn, arena_t *arena,
1333      extent_hooks_t **r_extent_hooks, extent_t *extent) {
1334  	assert(extent_dumpable_get(extent));
1335  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1336  	    WITNESS_RANK_CORE, 0);
1337  	if (*r_extent_hooks != &extent_hooks_default || extent_may_dalloc()) {
1338  		extent_deregister(tsdn, extent);
1339  		if (!extent_dalloc_wrapper_try(tsdn, arena, r_extent_hooks,
1340  		    extent)) {
1341  			return;
1342  		}
1343  		extent_reregister(tsdn, extent);
1344  	}
1345  	if (*r_extent_hooks != &extent_hooks_default) {
1346  		extent_hook_pre_reentrancy(tsdn, arena);
1347  	}
1348  	bool zeroed;
1349  	if (!extent_committed_get(extent)) {
1350  		zeroed = true;
1351  	} else if (!extent_decommit_wrapper(tsdn, arena, r_extent_hooks, extent,
1352  	    0, extent_size_get(extent))) {
1353  		zeroed = true;
1354  	} else if ((*r_extent_hooks)->purge_forced != NULL &&
1355  	    !(*r_extent_hooks)->purge_forced(*r_extent_hooks,
1356  	    extent_base_get(extent), extent_size_get(extent), 0,
1357  	    extent_size_get(extent), arena_ind_get(arena))) {
1358  		zeroed = true;
1359  	} else if (extent_state_get(extent) == extent_state_muzzy ||
1360  	    ((*r_extent_hooks)->purge_lazy != NULL &&
1361  	    !(*r_extent_hooks)->purge_lazy(*r_extent_hooks,
1362  	    extent_base_get(extent), extent_size_get(extent), 0,
1363  	    extent_size_get(extent), arena_ind_get(arena)))) {
1364  		zeroed = false;
1365  	} else {
1366  		zeroed = false;
1367  	}
1368  	if (*r_extent_hooks != &extent_hooks_default) {
1369  		extent_hook_post_reentrancy(tsdn);
1370  	}
1371  	extent_zeroed_set(extent, zeroed);
1372  	if (config_prof) {
1373  		extent_gdump_sub(tsdn, extent);
1374  	}
1375  	extent_record(tsdn, arena, r_extent_hooks, &arena->extents_retained,
1376  	    extent, false);
1377  }
1378  static void
1379  extent_destroy_default_impl(void *addr, size_t size) {
1380  	if (!have_dss || !extent_in_dss(addr)) {
1381  		pages_unmap(addr, size);
1382  	}
1383  }
1384  static void
1385  extent_destroy_default(extent_hooks_t *extent_hooks, void *addr, size_t size,
1386      bool committed, unsigned arena_ind) {
1387  	extent_destroy_default_impl(addr, size);
1388  }
1389  void
1390  extent_destroy_wrapper(tsdn_t *tsdn, arena_t *arena,
1391      extent_hooks_t **r_extent_hooks, extent_t *extent) {
1392  	assert(extent_base_get(extent) != NULL);
1393  	assert(extent_size_get(extent) != 0);
1394  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1395  	    WITNESS_RANK_CORE, 0);
1396  	extent_deregister(tsdn, extent);
1397  	extent_addr_set(extent, extent_base_get(extent));
1398  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1399  	if (*r_extent_hooks == &extent_hooks_default) {
1400  		extent_destroy_default_impl(extent_base_get(extent),
1401  		    extent_size_get(extent));
1402  	} else if ((*r_extent_hooks)->destroy != NULL) {
1403  		extent_hook_pre_reentrancy(tsdn, arena);
1404  		(*r_extent_hooks)->destroy(*r_extent_hooks,
1405  		    extent_base_get(extent), extent_size_get(extent),
1406  		    extent_committed_get(extent), arena_ind_get(arena));
1407  		extent_hook_post_reentrancy(tsdn);
1408  	}
1409  	extent_dalloc(tsdn, arena, extent);
1410  }
1411  static bool
1412  extent_commit_default(extent_hooks_t *extent_hooks, void *addr, size_t size,
1413      size_t offset, size_t length, unsigned arena_ind) {
1414  	return pages_commit((void *)((uintptr_t)addr + (uintptr_t)offset),
1415  	    length);
1416  }
1417  static bool
1418  extent_commit_impl(tsdn_t *tsdn, arena_t *arena,
1419      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1420      size_t length, bool growing_retained) {
1421  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1422  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
1423  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1424  	if (*r_extent_hooks != &extent_hooks_default) {
1425  		extent_hook_pre_reentrancy(tsdn, arena);
1426  	}
1427  	bool err = ((*r_extent_hooks)->commit == NULL ||
1428  	    (*r_extent_hooks)->commit(*r_extent_hooks, extent_base_get(extent),
1429  	    extent_size_get(extent), offset, length, arena_ind_get(arena)));
1430  	if (*r_extent_hooks != &extent_hooks_default) {
1431  		extent_hook_post_reentrancy(tsdn);
1432  	}
1433  	extent_committed_set(extent, extent_committed_get(extent) || !err);
1434  	return err;
1435  }
1436  bool
1437  extent_commit_wrapper(tsdn_t *tsdn, arena_t *arena,
1438      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1439      size_t length) {
1440  	return extent_commit_impl(tsdn, arena, r_extent_hooks, extent, offset,
1441  	    length, false);
1442  }
1443  static bool
1444  extent_decommit_default(extent_hooks_t *extent_hooks, void *addr, size_t size,
1445      size_t offset, size_t length, unsigned arena_ind) {
1446  	return pages_decommit((void *)((uintptr_t)addr + (uintptr_t)offset),
1447  	    length);
1448  }
1449  bool
1450  extent_decommit_wrapper(tsdn_t *tsdn, arena_t *arena,
1451      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1452      size_t length) {
1453  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1454  	    WITNESS_RANK_CORE, 0);
1455  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1456  	if (*r_extent_hooks != &extent_hooks_default) {
1457  		extent_hook_pre_reentrancy(tsdn, arena);
1458  	}
1459  	bool err = ((*r_extent_hooks)->decommit == NULL ||
1460  	    (*r_extent_hooks)->decommit(*r_extent_hooks,
1461  	    extent_base_get(extent), extent_size_get(extent), offset, length,
1462  	    arena_ind_get(arena)));
1463  	if (*r_extent_hooks != &extent_hooks_default) {
1464  		extent_hook_post_reentrancy(tsdn);
1465  	}
1466  	extent_committed_set(extent, extent_committed_get(extent) && err);
1467  	return err;
1468  }
1469  #ifdef PAGES_CAN_PURGE_LAZY
1470  static bool
1471  extent_purge_lazy_default(extent_hooks_t *extent_hooks, void *addr, size_t size,
1472      size_t offset, size_t length, unsigned arena_ind) {
1473  	assert(addr != NULL);
1474  	assert((offset & PAGE_MASK) == 0);
1475  	assert(length != 0);
1476  	assert((length & PAGE_MASK) == 0);
1477  	return pages_purge_lazy((void *)((uintptr_t)addr + (uintptr_t)offset),
1478  	    length);
1479  }
1480  #endif
1481  static bool
1482  extent_purge_lazy_impl(tsdn_t *tsdn, arena_t *arena,
1483      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1484      size_t length, bool growing_retained) {
1485  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1486  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
1487  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1488  	if ((*r_extent_hooks)->purge_lazy == NULL) {
1489  		return true;
1490  	}
1491  	if (*r_extent_hooks != &extent_hooks_default) {
1492  		extent_hook_pre_reentrancy(tsdn, arena);
1493  	}
1494  	bool err = (*r_extent_hooks)->purge_lazy(*r_extent_hooks,
1495  	    extent_base_get(extent), extent_size_get(extent), offset, length,
1496  	    arena_ind_get(arena));
1497  	if (*r_extent_hooks != &extent_hooks_default) {
1498  		extent_hook_post_reentrancy(tsdn);
1499  	}
1500  	return err;
1501  }
1502  bool
1503  extent_purge_lazy_wrapper(tsdn_t *tsdn, arena_t *arena,
1504      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1505      size_t length) {
1506  	return extent_purge_lazy_impl(tsdn, arena, r_extent_hooks, extent,
1507  	    offset, length, false);
1508  }
1509  #ifdef PAGES_CAN_PURGE_FORCED
1510  static bool
1511  extent_purge_forced_default(extent_hooks_t *extent_hooks, void *addr,
1512      size_t size, size_t offset, size_t length, unsigned arena_ind) {
1513  	assert(addr != NULL);
1514  	assert((offset & PAGE_MASK) == 0);
1515  	assert(length != 0);
1516  	assert((length & PAGE_MASK) == 0);
1517  	return pages_purge_forced((void *)((uintptr_t)addr +
1518  	    (uintptr_t)offset), length);
1519  }
1520  #endif
1521  static bool
1522  extent_purge_forced_impl(tsdn_t *tsdn, arena_t *arena,
1523      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1524      size_t length, bool growing_retained) {
1525  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1526  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
1527  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1528  	if ((*r_extent_hooks)->purge_forced == NULL) {
1529  		return true;
1530  	}
1531  	if (*r_extent_hooks != &extent_hooks_default) {
1532  		extent_hook_pre_reentrancy(tsdn, arena);
1533  	}
1534  	bool err = (*r_extent_hooks)->purge_forced(*r_extent_hooks,
1535  	    extent_base_get(extent), extent_size_get(extent), offset, length,
1536  	    arena_ind_get(arena));
1537  	if (*r_extent_hooks != &extent_hooks_default) {
1538  		extent_hook_post_reentrancy(tsdn);
1539  	}
1540  	return err;
1541  }
1542  bool
1543  extent_purge_forced_wrapper(tsdn_t *tsdn, arena_t *arena,
1544      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t offset,
1545      size_t length) {
1546  	return extent_purge_forced_impl(tsdn, arena, r_extent_hooks, extent,
1547  	    offset, length, false);
1548  }
1549  static bool
1550  extent_split_default(extent_hooks_t *extent_hooks, void *addr, size_t size,
1551      size_t size_a, size_t size_b, bool committed, unsigned arena_ind) {
1552  	if (!maps_coalesce) {
1553  		return !opt_retain;
1554  	}
1555  	return false;
1556  }
1557  static extent_t *
1558  extent_split_impl(tsdn_t *tsdn, arena_t *arena,
1559      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t size_a,
1560      szind_t szind_a, bool slab_a, size_t size_b, szind_t szind_b, bool slab_b,
1561      bool growing_retained) {
1562  	assert(extent_size_get(extent) == size_a + size_b);
1563  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1564  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
1565  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1566  	if ((*r_extent_hooks)->split == NULL) {
1567  		return NULL;
1568  	}
1569  	extent_t *trail = extent_alloc(tsdn, arena);
1570  	if (trail == NULL) {
1571  		goto label_error_a;
1572  	}
1573  	extent_init(trail, arena, (void *)((uintptr_t)extent_base_get(extent) +
1574  	    size_a), size_b, slab_b, szind_b, extent_sn_get(extent),
1575  	    extent_state_get(extent), extent_zeroed_get(extent),
1576  	    extent_committed_get(extent), extent_dumpable_get(extent),
1577  	    EXTENT_NOT_HEAD);
1578  	rtree_ctx_t rtree_ctx_fallback;
1579  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1580  	rtree_leaf_elm_t *lead_elm_a, *lead_elm_b;
1581  	{
1582  		extent_t lead;
1583  		extent_init(&lead, arena, extent_addr_get(extent), size_a,
1584  		    slab_a, szind_a, extent_sn_get(extent),
1585  		    extent_state_get(extent), extent_zeroed_get(extent),
1586  		    extent_committed_get(extent), extent_dumpable_get(extent),
1587  		    EXTENT_NOT_HEAD);
1588  		extent_rtree_leaf_elms_lookup(tsdn, rtree_ctx, &lead, false,
1589  		    true, &lead_elm_a, &lead_elm_b);
1590  	}
1591  	rtree_leaf_elm_t *trail_elm_a, *trail_elm_b;
1592  	extent_rtree_leaf_elms_lookup(tsdn, rtree_ctx, trail, false, true,
1593  	    &trail_elm_a, &trail_elm_b);
1594  	if (lead_elm_a == NULL || lead_elm_b == NULL || trail_elm_a == NULL
1595  	    || trail_elm_b == NULL) {
1596  		goto label_error_b;
1597  	}
1598  	extent_lock2(tsdn, extent, trail);
1599  	if (*r_extent_hooks != &extent_hooks_default) {
1600  		extent_hook_pre_reentrancy(tsdn, arena);
1601  	}
1602  	bool err = (*r_extent_hooks)->split(*r_extent_hooks, extent_base_get(extent),
1603  	    size_a + size_b, size_a, size_b, extent_committed_get(extent),
1604  	    arena_ind_get(arena));
1605  	if (*r_extent_hooks != &extent_hooks_default) {
1606  		extent_hook_post_reentrancy(tsdn);
1607  	}
1608  	if (err) {
1609  		goto label_error_c;
1610  	}
1611  	extent_size_set(extent, size_a);
1612  	extent_szind_set(extent, szind_a);
1613  	extent_rtree_write_acquired(tsdn, lead_elm_a, lead_elm_b, extent,
1614  	    szind_a, slab_a);
1615  	extent_rtree_write_acquired(tsdn, trail_elm_a, trail_elm_b, trail,
1616  	    szind_b, slab_b);
1617  	extent_unlock2(tsdn, extent, trail);
1618  	return trail;
1619  label_error_c:
1620  	extent_unlock2(tsdn, extent, trail);
1621  label_error_b:
1622  	extent_dalloc(tsdn, arena, trail);
1623  label_error_a:
1624  	return NULL;
1625  }
1626  extent_t *
1627  extent_split_wrapper(tsdn_t *tsdn, arena_t *arena,
1628      extent_hooks_t **r_extent_hooks, extent_t *extent, size_t size_a,
1629      szind_t szind_a, bool slab_a, size_t size_b, szind_t szind_b, bool slab_b) {
1630  	return extent_split_impl(tsdn, arena, r_extent_hooks, extent, size_a,
1631  	    szind_a, slab_a, size_b, szind_b, slab_b, false);
1632  }
1633  static bool
1634  extent_merge_default_impl(void *addr_a, void *addr_b) {
1635  	if (!maps_coalesce && !opt_retain) {
1636  		return true;
1637  	}
1638  	if (have_dss && !extent_dss_mergeable(addr_a, addr_b)) {
1639  		return true;
1640  	}
1641  	return false;
1642  }
1643  static bool
1644  extent_head_no_merge(extent_t *a, extent_t *b) {
1645  	assert(extent_base_get(a) < extent_base_get(b));
1646  	if (maps_coalesce) {
1647  		return false;
1648  	}
1649  	if (!opt_retain) {
1650  		return true;
1651  	}
1652  	if (extent_is_head_get(b)) {
1653  		assert(extent_sn_comp(a, b) != 0);
1654  		return true;
1655  	}
1656  	assert(extent_sn_comp(a, b) == 0);
1657  	return false;
1658  }
1659  static bool
1660  extent_merge_default(extent_hooks_t *extent_hooks, void *addr_a, size_t size_a,
1661      void *addr_b, size_t size_b, bool committed, unsigned arena_ind) {
1662  	if (!maps_coalesce) {
1663  		tsdn_t *tsdn = tsdn_fetch();
1664  		extent_t *a = iealloc(tsdn, addr_a);
1665  		extent_t *b = iealloc(tsdn, addr_b);
1666  		if (extent_head_no_merge(a, b)) {
1667  			return true;
1668  		}
1669  	}
1670  	return extent_merge_default_impl(addr_a, addr_b);
1671  }
1672  static bool
1673  extent_merge_impl(tsdn_t *tsdn, arena_t *arena,
1674      extent_hooks_t **r_extent_hooks, extent_t *a, extent_t *b,
1675      bool growing_retained) {
1676  	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
1677  	    WITNESS_RANK_CORE, growing_retained ? 1 : 0);
1678  	assert(extent_base_get(a) < extent_base_get(b));
1679  	extent_hooks_assure_initialized(arena, r_extent_hooks);
1680  	if ((*r_extent_hooks)->merge == NULL || extent_head_no_merge(a, b)) {
1681  		return true;
1682  	}
1683  	bool err;
1684  	if (*r_extent_hooks == &extent_hooks_default) {
1685  		err = extent_merge_default_impl(extent_base_get(a),
1686  		    extent_base_get(b));
1687  	} else {
1688  		extent_hook_pre_reentrancy(tsdn, arena);
1689  		err = (*r_extent_hooks)->merge(*r_extent_hooks,
1690  		    extent_base_get(a), extent_size_get(a), extent_base_get(b),
1691  		    extent_size_get(b), extent_committed_get(a),
1692  		    arena_ind_get(arena));
1693  		extent_hook_post_reentrancy(tsdn);
1694  	}
1695  	if (err) {
1696  		return true;
1697  	}
1698  	rtree_ctx_t rtree_ctx_fallback;
1699  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
1700  	rtree_leaf_elm_t *a_elm_a, *a_elm_b, *b_elm_a, *b_elm_b;
1701  	extent_rtree_leaf_elms_lookup(tsdn, rtree_ctx, a, true, false, &a_elm_a,
1702  	    &a_elm_b);
1703  	extent_rtree_leaf_elms_lookup(tsdn, rtree_ctx, b, true, false, &b_elm_a,
1704  	    &b_elm_b);
1705  	extent_lock2(tsdn, a, b);
1706  	if (a_elm_b != NULL) {
1707  		rtree_leaf_elm_write(tsdn, &extents_rtree, a_elm_b, NULL,
1708  		    SC_NSIZES, false);
1709  	}
1710  	if (b_elm_b != NULL) {
1711  		rtree_leaf_elm_write(tsdn, &extents_rtree, b_elm_a, NULL,
1712  		    SC_NSIZES, false);
1713  	} else {
1714  		b_elm_b = b_elm_a;
1715  	}
1716  	extent_size_set(a, extent_size_get(a) + extent_size_get(b));
1717  	extent_szind_set(a, SC_NSIZES);
1718  	extent_sn_set(a, (extent_sn_get(a) < extent_sn_get(b)) ?
1719  	    extent_sn_get(a) : extent_sn_get(b));
1720  	extent_zeroed_set(a, extent_zeroed_get(a) && extent_zeroed_get(b));
1721  	extent_rtree_write_acquired(tsdn, a_elm_a, b_elm_b, a, SC_NSIZES,
1722  	    false);
1723  	extent_unlock2(tsdn, a, b);
1724  	extent_dalloc(tsdn, extent_arena_get(b), b);
1725  	return false;
1726  }
1727  bool
1728  extent_merge_wrapper(tsdn_t *tsdn, arena_t *arena,
1729      extent_hooks_t **r_extent_hooks, extent_t *a, extent_t *b) {
1730  	return extent_merge_impl(tsdn, arena, r_extent_hooks, a, b, false);
1731  }
1732  bool
1733  extent_boot(void) {
1734  	if (rtree_new(&extents_rtree, true)) {
1735  		return true;
1736  	}
1737  	if (mutex_pool_init(&extent_mutex_pool, "extent_mutex_pool",
1738  	    WITNESS_RANK_EXTENT_POOL)) {
1739  		return true;
1740  	}
1741  	if (have_dss) {
1742  		extent_dss_boot();
1743  	}
1744  	return false;
1745  }
1746  void
1747  extent_util_stats_get(tsdn_t *tsdn, const void *ptr,
1748      size_t *nfree, size_t *nregs, size_t *size) {
1749  	assert(ptr != NULL && nfree != NULL && nregs != NULL && size != NULL);
1750  	const extent_t *extent = iealloc(tsdn, ptr);
1751  	if (unlikely(extent == NULL)) {
1752  		*nfree = *nregs = *size = 0;
1753  		return;
1754  	}
1755  	*size = extent_size_get(extent);
1756  	if (!extent_slab_get(extent)) {
1757  		*nfree = 0;
1758  		*nregs = 1;
1759  	} else {
1760  		*nfree = extent_nfree_get(extent);
1761  		*nregs = bin_infos[extent_szind_get(extent)].nregs;
1762  		assert(*nfree <= *nregs);
1763  		assert(*nfree * extent_usize_get(extent) <= *size);
1764  	}
1765  }
1766  void
1767  extent_util_stats_verbose_get(tsdn_t *tsdn, const void *ptr,
1768      size_t *nfree, size_t *nregs, size_t *size,
1769      size_t *bin_nfree, size_t *bin_nregs, void **slabcur_addr) {
1770  	assert(ptr != NULL && nfree != NULL && nregs != NULL && size != NULL
1771  	    && bin_nfree != NULL && bin_nregs != NULL && slabcur_addr != NULL);
1772  	const extent_t *extent = iealloc(tsdn, ptr);
1773  	if (unlikely(extent == NULL)) {
1774  		*nfree = *nregs = *size = *bin_nfree = *bin_nregs = 0;
1775  		*slabcur_addr = NULL;
1776  		return;
1777  	}
1778  	*size = extent_size_get(extent);
1779  	if (!extent_slab_get(extent)) {
1780  		*nfree = *bin_nfree = *bin_nregs = 0;
1781  		*nregs = 1;
1782  		*slabcur_addr = NULL;
1783  		return;
1784  	}
1785  	*nfree = extent_nfree_get(extent);
1786  	const szind_t szind = extent_szind_get(extent);
1787  	*nregs = bin_infos[szind].nregs;
1788  	assert(*nfree <= *nregs);
1789  	assert(*nfree * extent_usize_get(extent) <= *size);
1790  	const arena_t *arena = extent_arena_get(extent);
1791  	assert(arena != NULL);
1792  	const unsigned binshard = extent_binshard_get(extent);
1793  	bin_t *bin = &arena->bins[szind].bin_shards[binshard];
1794  	malloc_mutex_lock(tsdn, &bin->lock);
1795  	if (config_stats) {
1796  		*bin_nregs = *nregs * bin->stats.curslabs;
1797  		assert(*bin_nregs >= bin->stats.curregs);
1798  		*bin_nfree = *bin_nregs - bin->stats.curregs;
1799  	} else {
1800  		*bin_nfree = *bin_nregs = 0;
1801  	}
1802  	*slabcur_addr = extent_addr_get(bin->slabcur);
1803  	assert(*slabcur_addr != NULL);
1804  	malloc_mutex_unlock(tsdn, &bin->lock);
1805  }
</code></pre>
        </div>
        <div class="column">
            <h3>PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-vp8l_enc.c</h3>
            <pre><code>1  #include <assert.h>
2  #include <stdlib.h>
3  #include "src/enc/backward_references_enc.h"
4  #include "src/enc/histogram_enc.h"
5  #include "src/enc/vp8i_enc.h"
6  #include "src/enc/vp8li_enc.h"
7  #include "src/dsp/lossless.h"
8  #include "src/dsp/lossless_common.h"
9  #include "src/utils/bit_writer_utils.h"
10  #include "src/utils/huffman_encode_utils.h"
11  #include "src/utils/utils.h"
12  #include "src/webp/format_constants.h"
13  #define MAX_HUFF_IMAGE_SIZE       2600
14  static int PaletteCompareColorsForQsort(const void* p1, const void* p2) {
15    const uint32_t a = WebPMemToUint32((uint8_t*)p1);
16    const uint32_t b = WebPMemToUint32((uint8_t*)p2);
17    assert(a != b);
18    return (a < b) ? -1 : 1;
19  }
20  static WEBP_INLINE uint32_t PaletteComponentDistance(uint32_t v) {
21    return (v <= 128) ? v : (256 - v);
22  }
23  static WEBP_INLINE uint32_t PaletteColorDistance(uint32_t col1, uint32_t col2) {
24    const uint32_t diff = VP8LSubPixels(col1, col2);
25    const int kMoreWeightForRGBThanForAlpha = 9;
26    uint32_t score;
27    score =  PaletteComponentDistance((diff >>  0) & 0xff);
28    score += PaletteComponentDistance((diff >>  8) & 0xff);
29    score += PaletteComponentDistance((diff >> 16) & 0xff);
30    score *= kMoreWeightForRGBThanForAlpha;
31    score += PaletteComponentDistance((diff >> 24) & 0xff);
32    return score;
33  }
34  static WEBP_INLINE void SwapColor(uint32_t* const col1, uint32_t* const col2) {
35    const uint32_t tmp = *col1;
36    *col1 = *col2;
37    *col2 = tmp;
38  }
39  static void GreedyMinimizeDeltas(uint32_t palette[], int num_colors) {
40    uint32_t predict = 0x00000000;
41    int i, k;
42    for (i = 0; i < num_colors; ++i) {
43      int best_ix = i;
44      uint32_t best_score = ~0U;
45      for (k = i; k < num_colors; ++k) {
46        const uint32_t cur_score = PaletteColorDistance(palette[k], predict);
47        if (best_score > cur_score) {
48          best_score = cur_score;
49          best_ix = k;
50        }
51      }
52      SwapColor(&palette[best_ix], &palette[i]);
53      predict = palette[i];
54    }
55  }
56  static int PaletteHasNonMonotonousDeltas(uint32_t palette[], int num_colors) {
57    uint32_t predict = 0x000000;
58    int i;
59    uint8_t sign_found = 0x00;
60    for (i = 0; i < num_colors; ++i) {
61      const uint32_t diff = VP8LSubPixels(palette[i], predict);
62      const uint8_t rd = (diff >> 16) & 0xff;
63      const uint8_t gd = (diff >>  8) & 0xff;
64      const uint8_t bd = (diff >>  0) & 0xff;
65      if (rd != 0x00) {
66        sign_found |= (rd < 0x80) ? 1 : 2;
67      }
68      if (gd != 0x00) {
69        sign_found |= (gd < 0x80) ? 8 : 16;
70      }
71      if (bd != 0x00) {
72        sign_found |= (bd < 0x80) ? 64 : 128;
73      }
74      predict = palette[i];
75    }
76    return (sign_found & (sign_found << 1)) != 0;  
77  }
78  static int AnalyzeAndCreatePalette(const WebPPicture* const pic,
79                                     int low_effort,
80                                     uint32_t palette[MAX_PALETTE_SIZE],
81                                     int* const palette_size) {
82    const int num_colors = WebPGetColorPalette(pic, palette);
83    if (num_colors > MAX_PALETTE_SIZE) {
84      *palette_size = 0;
85      return 0;
86    }
87    *palette_size = num_colors;
88    qsort(palette, num_colors, sizeof(*palette), PaletteCompareColorsForQsort);
89    if (!low_effort && PaletteHasNonMonotonousDeltas(palette, num_colors)) {
90      GreedyMinimizeDeltas(palette, num_colors);
91    }
92    return 1;
93  }
94  typedef enum {
95    kDirect = 0,
96    kSpatial = 1,
97    kSubGreen = 2,
98    kSpatialSubGreen = 3,
99    kPalette = 4,
100    kNumEntropyIx = 5
101  } EntropyIx;
102  typedef enum {
103    kHistoAlpha = 0,
104    kHistoAlphaPred,
105    kHistoGreen,
106    kHistoGreenPred,
107    kHistoRed,
108    kHistoRedPred,
109    kHistoBlue,
110    kHistoBluePred,
111    kHistoRedSubGreen,
112    kHistoRedPredSubGreen,
113    kHistoBlueSubGreen,
114    kHistoBluePredSubGreen,
115    kHistoPalette,
116    kHistoTotal  
117  } HistoIx;
118  static void AddSingleSubGreen(int p, uint32_t* const r, uint32_t* const b) {
119    const int green = p >> 8;  
120    ++r[((p >> 16) - green) & 0xff];
121    ++b[((p >>  0) - green) & 0xff];
122  }
123  static void AddSingle(uint32_t p,
124                        uint32_t* const a, uint32_t* const r,
125                        uint32_t* const g, uint32_t* const b) {
126    ++a[(p >> 24) & 0xff];
127    ++r[(p >> 16) & 0xff];
128    ++g[(p >>  8) & 0xff];
129    ++b[(p >>  0) & 0xff];
130  }
131  static WEBP_INLINE uint32_t HashPix(uint32_t pix) {
132    return ((((uint64_t)pix + (pix >> 19)) * 0x39c5fba7ull) & 0xffffffffu) >> 24;
133  }
134  static int AnalyzeEntropy(const uint32_t* argb,
135                            int width, int height, int argb_stride,
136                            int use_palette,
137                            int palette_size, int transform_bits,
138                            EntropyIx* const min_entropy_ix,
139                            int* const red_and_blue_always_zero) {
140    uint32_t* histo;
141    if (use_palette && palette_size <= 16) {
142      *min_entropy_ix = kPalette;
143      *red_and_blue_always_zero = 1;
144      return 1;
145    }
146    histo = (uint32_t*)WebPSafeCalloc(kHistoTotal, sizeof(*histo) * 256);
147    if (histo != NULL) {
148      int i, x, y;
149      const uint32_t* prev_row = NULL;
150      const uint32_t* curr_row = argb;
151      uint32_t pix_prev = argb[0];  
152      for (y = 0; y < height; ++y) {
153        for (x = 0; x < width; ++x) {
154          const uint32_t pix = curr_row[x];
155          const uint32_t pix_diff = VP8LSubPixels(pix, pix_prev);
156          pix_prev = pix;
157          if ((pix_diff == 0) || (prev_row != NULL && pix == prev_row[x])) {
158            continue;
159          }
160          AddSingle(pix,
161                    &histo[kHistoAlpha * 256],
162                    &histo[kHistoRed * 256],
163                    &histo[kHistoGreen * 256],
164                    &histo[kHistoBlue * 256]);
165          AddSingle(pix_diff,
166                    &histo[kHistoAlphaPred * 256],
167                    &histo[kHistoRedPred * 256],
168                    &histo[kHistoGreenPred * 256],
169                    &histo[kHistoBluePred * 256]);
170          AddSingleSubGreen(pix,
171                            &histo[kHistoRedSubGreen * 256],
172                            &histo[kHistoBlueSubGreen * 256]);
173          AddSingleSubGreen(pix_diff,
174                            &histo[kHistoRedPredSubGreen * 256],
175                            &histo[kHistoBluePredSubGreen * 256]);
176          {
177            const uint32_t hash = HashPix(pix);
178            ++histo[kHistoPalette * 256 + hash];
179          }
180        }
181        prev_row = curr_row;
182        curr_row += argb_stride;
183      }
184      {
185        double entropy_comp[kHistoTotal];
186        double entropy[kNumEntropyIx];
187        int k;
188        int last_mode_to_analyze = use_palette ? kPalette : kSpatialSubGreen;
189        int j;
190        ++histo[kHistoRedPredSubGreen * 256];
191        ++histo[kHistoBluePredSubGreen * 256];
192        ++histo[kHistoRedPred * 256];
193        ++histo[kHistoGreenPred * 256];
194        ++histo[kHistoBluePred * 256];
195        ++histo[kHistoAlphaPred * 256];
196        for (j = 0; j < kHistoTotal; ++j) {
197          entropy_comp[j] = VP8LBitsEntropy(&histo[j * 256], 256);
198        }
199        entropy[kDirect] = entropy_comp[kHistoAlpha] +
200            entropy_comp[kHistoRed] +
201            entropy_comp[kHistoGreen] +
202            entropy_comp[kHistoBlue];
203        entropy[kSpatial] = entropy_comp[kHistoAlphaPred] +
204            entropy_comp[kHistoRedPred] +
205            entropy_comp[kHistoGreenPred] +
206            entropy_comp[kHistoBluePred];
207        entropy[kSubGreen] = entropy_comp[kHistoAlpha] +
208            entropy_comp[kHistoRedSubGreen] +
209            entropy_comp[kHistoGreen] +
210            entropy_comp[kHistoBlueSubGreen];
211        entropy[kSpatialSubGreen] = entropy_comp[kHistoAlphaPred] +
212            entropy_comp[kHistoRedPredSubGreen] +
213            entropy_comp[kHistoGreenPred] +
214            entropy_comp[kHistoBluePredSubGreen];
215        entropy[kPalette] = entropy_comp[kHistoPalette];
216        entropy[kSpatial] += VP8LSubSampleSize(width, transform_bits) *
217                             VP8LSubSampleSize(height, transform_bits) *
218                             VP8LFastLog2(14);
219        entropy[kSpatialSubGreen] += VP8LSubSampleSize(width, transform_bits) *
220                                     VP8LSubSampleSize(height, transform_bits) *
221                                     VP8LFastLog2(24);
222        entropy[kPalette] += palette_size * 8;
223        *min_entropy_ix = kDirect;
224        for (k = kDirect + 1; k <= last_mode_to_analyze; ++k) {
225          if (entropy[*min_entropy_ix] > entropy[k]) {
226            *min_entropy_ix = (EntropyIx)k;
227          }
228        }
229        assert((int)*min_entropy_ix <= last_mode_to_analyze);
230        *red_and_blue_always_zero = 1;
231        {
232          static const uint8_t kHistoPairs[5][2] = {
233            { kHistoRed, kHistoBlue },
234            { kHistoRedPred, kHistoBluePred },
235            { kHistoRedSubGreen, kHistoBlueSubGreen },
236            { kHistoRedPredSubGreen, kHistoBluePredSubGreen },
237            { kHistoRed, kHistoBlue }
238          };
239          const uint32_t* const red_histo =
240              &histo[256 * kHistoPairs[*min_entropy_ix][0]];
241          const uint32_t* const blue_histo =
242              &histo[256 * kHistoPairs[*min_entropy_ix][1]];
243          for (i = 1; i < 256; ++i) {
244            if ((red_histo[i] | blue_histo[i]) != 0) {
245              *red_and_blue_always_zero = 0;
246              break;
247            }
248          }
249        }
250      }
251      WebPSafeFree(histo);
<span onclick='openModal()' class='match'>252      return 1;
253    } else {
254      return 0;
255    }
256  }
257  static int GetHistoBits(int method, int use_palette, int width, int height) {
</span>258    int histo_bits = (use_palette ? 9 : 7) - method;
259    while (1) {
260      const int huff_image_size = VP8LSubSampleSize(width, histo_bits) *
261                                  VP8LSubSampleSize(height, histo_bits);
262      if (huff_image_size <= MAX_HUFF_IMAGE_SIZE) break;
263      ++histo_bits;
264    }
265    return (histo_bits < MIN_HUFFMAN_BITS) ? MIN_HUFFMAN_BITS :
266           (histo_bits > MAX_HUFFMAN_BITS) ? MAX_HUFFMAN_BITS : histo_bits;
267  }
268  static int GetTransformBits(int method, int histo_bits) {
269    const int max_transform_bits = (method < 4) ? 6 : (method > 4) ? 4 : 5;
270    const int res =
271        (histo_bits > max_transform_bits) ? max_transform_bits : histo_bits;
272    assert(res <= MAX_TRANSFORM_BITS);
273    return res;
274  }
275  #define CRUNCH_CONFIGS_LZ77_MAX 2
276  typedef struct {
277    int entropy_idx_;
278    int lz77s_types_to_try_[CRUNCH_CONFIGS_LZ77_MAX];
279    int lz77s_types_to_try_size_;
280  } CrunchConfig;
281  #define CRUNCH_CONFIGS_MAX kNumEntropyIx
282  static int EncoderAnalyze(VP8LEncoder* const enc,
283                            CrunchConfig crunch_configs[CRUNCH_CONFIGS_MAX],
284                            int* const crunch_configs_size,
285                            int* const red_and_blue_always_zero) {
286    const WebPPicture* const pic = enc->pic_;
287    const int width = pic->width;
288    const int height = pic->height;
289    const WebPConfig* const config = enc->config_;
290    const int method = config->method;
291    const int low_effort = (config->method == 0);
292    int i;
293    int use_palette;
294    int n_lz77s;
295    assert(pic != NULL && pic->argb != NULL);
296    use_palette =
297        AnalyzeAndCreatePalette(pic, low_effort,
298                                enc->palette_, &enc->palette_size_);
299    enc->histo_bits_ = GetHistoBits(method, use_palette,
300                                    pic->width, pic->height);
301    enc->transform_bits_ = GetTransformBits(method, enc->histo_bits_);
302    if (low_effort) {
303      crunch_configs[0].entropy_idx_ = use_palette ? kPalette : kSpatialSubGreen;
304      n_lz77s = 1;
305      *crunch_configs_size = 1;
306    } else {
307      EntropyIx min_entropy_ix;
308      n_lz77s = (enc->palette_size_ > 0 && enc->palette_size_ <= 16) ? 2 : 1;
309      if (!AnalyzeEntropy(pic->argb, width, height, pic->argb_stride, use_palette,
310                          enc->palette_size_, enc->transform_bits_,
311                          &min_entropy_ix, red_and_blue_always_zero)) {
312        return 0;
313      }
314      if (method == 6 && config->quality == 100) {
315        *crunch_configs_size = 0;
316        for (i = 0; i < kNumEntropyIx; ++i) {
317          if (i != kPalette || use_palette) {
318            assert(*crunch_configs_size < CRUNCH_CONFIGS_MAX);
319            crunch_configs[(*crunch_configs_size)++].entropy_idx_ = i;
320          }
321        }
322      } else {
323        *crunch_configs_size = 1;
324        crunch_configs[0].entropy_idx_ = min_entropy_ix;
325      }
326    }
327    assert(n_lz77s <= CRUNCH_CONFIGS_LZ77_MAX);
328    for (i = 0; i < *crunch_configs_size; ++i) {
329      int j;
330      for (j = 0; j < n_lz77s; ++j) {
331        crunch_configs[i].lz77s_types_to_try_[j] =
332            (j == 0) ? kLZ77Standard | kLZ77RLE : kLZ77Box;
333      }
334      crunch_configs[i].lz77s_types_to_try_size_ = n_lz77s;
335    }
336    return 1;
337  }
338  static int EncoderInit(VP8LEncoder* const enc) {
339    const WebPPicture* const pic = enc->pic_;
340    const int width = pic->width;
341    const int height = pic->height;
342    const int pix_cnt = width * height;
343    const int refs_block_size = (pix_cnt - 1) / MAX_REFS_BLOCK_PER_IMAGE + 1;
344    int i;
345    if (!VP8LHashChainInit(&enc->hash_chain_, pix_cnt)) return 0;
346    for (i = 0; i < 3; ++i) VP8LBackwardRefsInit(&enc->refs_[i], refs_block_size);
347    return 1;
348  }
349  static int GetHuffBitLengthsAndCodes(
350      const VP8LHistogramSet* const histogram_image,
351      HuffmanTreeCode* const huffman_codes) {
352    int i, k;
353    int ok = 0;
354    uint64_t total_length_size = 0;
355    uint8_t* mem_buf = NULL;
356    const int histogram_image_size = histogram_image->size;
357    int max_num_symbols = 0;
358    uint8_t* buf_rle = NULL;
359    HuffmanTree* huff_tree = NULL;
360    for (i = 0; i < histogram_image_size; ++i) {
361      const VP8LHistogram* const histo = histogram_image->histograms[i];
362      HuffmanTreeCode* const codes = &huffman_codes[5 * i];
363      assert(histo != NULL);
364      for (k = 0; k < 5; ++k) {
365        const int num_symbols =
366            (k == 0) ? VP8LHistogramNumCodes(histo->palette_code_bits_) :
367            (k == 4) ? NUM_DISTANCE_CODES : 256;
368        codes[k].num_symbols = num_symbols;
369        total_length_size += num_symbols;
370      }
371    }
372    {
373      uint16_t* codes;
374      uint8_t* lengths;
375      mem_buf = (uint8_t*)WebPSafeCalloc(total_length_size,
376                                         sizeof(*lengths) + sizeof(*codes));
377      if (mem_buf == NULL) goto End;
378      codes = (uint16_t*)mem_buf;
379      lengths = (uint8_t*)&codes[total_length_size];
380      for (i = 0; i < 5 * histogram_image_size; ++i) {
381        const int bit_length = huffman_codes[i].num_symbols;
382        huffman_codes[i].codes = codes;
383        huffman_codes[i].code_lengths = lengths;
384        codes += bit_length;
385        lengths += bit_length;
386        if (max_num_symbols < bit_length) {
387          max_num_symbols = bit_length;
388        }
389      }
390    }
391    buf_rle = (uint8_t*)WebPSafeMalloc(1ULL, max_num_symbols);
392    huff_tree = (HuffmanTree*)WebPSafeMalloc(3ULL * max_num_symbols,
393                                             sizeof(*huff_tree));
394    if (buf_rle == NULL || huff_tree == NULL) goto End;
395    for (i = 0; i < histogram_image_size; ++i) {
396      HuffmanTreeCode* const codes = &huffman_codes[5 * i];
397      VP8LHistogram* const histo = histogram_image->histograms[i];
398      VP8LCreateHuffmanTree(histo->literal_, 15, buf_rle, huff_tree, codes + 0);
399      VP8LCreateHuffmanTree(histo->red_, 15, buf_rle, huff_tree, codes + 1);
400      VP8LCreateHuffmanTree(histo->blue_, 15, buf_rle, huff_tree, codes + 2);
401      VP8LCreateHuffmanTree(histo->alpha_, 15, buf_rle, huff_tree, codes + 3);
402      VP8LCreateHuffmanTree(histo->distance_, 15, buf_rle, huff_tree, codes + 4);
403    }
404    ok = 1;
405   End:
406    WebPSafeFree(huff_tree);
407    WebPSafeFree(buf_rle);
408    if (!ok) {
409      WebPSafeFree(mem_buf);
410      memset(huffman_codes, 0, 5 * histogram_image_size * sizeof(*huffman_codes));
411    }
412    return ok;
413  }
414  static void StoreHuffmanTreeOfHuffmanTreeToBitMask(
415      VP8LBitWriter* const bw, const uint8_t* code_length_bitdepth) {
416    static const uint8_t kStorageOrder[CODE_LENGTH_CODES] = {
417      17, 18, 0, 1, 2, 3, 4, 5, 16, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15
418    };
419    int i;
420    int codes_to_store = CODE_LENGTH_CODES;
421    for (; codes_to_store > 4; --codes_to_store) {
422      if (code_length_bitdepth[kStorageOrder[codes_to_store - 1]] != 0) {
423        break;
424      }
425    }
426    VP8LPutBits(bw, codes_to_store - 4, 4);
427    for (i = 0; i < codes_to_store; ++i) {
428      VP8LPutBits(bw, code_length_bitdepth[kStorageOrder[i]], 3);
429    }
430  }
431  static void ClearHuffmanTreeIfOnlyOneSymbol(
432      HuffmanTreeCode* const huffman_code) {
433    int k;
434    int count = 0;
435    for (k = 0; k < huffman_code->num_symbols; ++k) {
436      if (huffman_code->code_lengths[k] != 0) {
437        ++count;
438        if (count > 1) return;
439      }
440    }
441    for (k = 0; k < huffman_code->num_symbols; ++k) {
442      huffman_code->code_lengths[k] = 0;
443      huffman_code->codes[k] = 0;
444    }
445  }
446  static void StoreHuffmanTreeToBitMask(
447      VP8LBitWriter* const bw,
448      const HuffmanTreeToken* const tokens, const int num_tokens,
449      const HuffmanTreeCode* const huffman_code) {
450    int i;
451    for (i = 0; i < num_tokens; ++i) {
452      const int ix = tokens[i].code;
453      const int extra_bits = tokens[i].extra_bits;
454      VP8LPutBits(bw, huffman_code->codes[ix], huffman_code->code_lengths[ix]);
455      switch (ix) {
456        case 16:
457          VP8LPutBits(bw, extra_bits, 2);
458          break;
459        case 17:
460          VP8LPutBits(bw, extra_bits, 3);
461          break;
462        case 18:
463          VP8LPutBits(bw, extra_bits, 7);
464          break;
465      }
466    }
467  }
468  static void StoreFullHuffmanCode(VP8LBitWriter* const bw,
469                                   HuffmanTree* const huff_tree,
470                                   HuffmanTreeToken* const tokens,
471                                   const HuffmanTreeCode* const tree) {
472    uint8_t code_length_bitdepth[CODE_LENGTH_CODES] = { 0 };
473    uint16_t code_length_bitdepth_symbols[CODE_LENGTH_CODES] = { 0 };
474    const int max_tokens = tree->num_symbols;
475    int num_tokens;
476    HuffmanTreeCode huffman_code;
477    huffman_code.num_symbols = CODE_LENGTH_CODES;
478    huffman_code.code_lengths = code_length_bitdepth;
479    huffman_code.codes = code_length_bitdepth_symbols;
480    VP8LPutBits(bw, 0, 1);
481    num_tokens = VP8LCreateCompressedHuffmanTree(tree, tokens, max_tokens);
482    {
483      uint32_t histogram[CODE_LENGTH_CODES] = { 0 };
484      uint8_t buf_rle[CODE_LENGTH_CODES] = { 0 };
485      int i;
486      for (i = 0; i < num_tokens; ++i) {
487        ++histogram[tokens[i].code];
488      }
489      VP8LCreateHuffmanTree(histogram, 7, buf_rle, huff_tree, &huffman_code);
490    }
491    StoreHuffmanTreeOfHuffmanTreeToBitMask(bw, code_length_bitdepth);
492    ClearHuffmanTreeIfOnlyOneSymbol(&huffman_code);
493    {
494      int trailing_zero_bits = 0;
495      int trimmed_length = num_tokens;
496      int write_trimmed_length;
497      int length;
498      int i = num_tokens;
499      while (i-- > 0) {
500        const int ix = tokens[i].code;
501        if (ix == 0 || ix == 17 || ix == 18) {
502          --trimmed_length;   
503          trailing_zero_bits += code_length_bitdepth[ix];
504          if (ix == 17) {
505            trailing_zero_bits += 3;
506          } else if (ix == 18) {
507            trailing_zero_bits += 7;
508          }
509        } else {
510          break;
511        }
512      }
513      write_trimmed_length = (trimmed_length > 1 && trailing_zero_bits > 12);
514      length = write_trimmed_length ? trimmed_length : num_tokens;
515      VP8LPutBits(bw, write_trimmed_length, 1);
516      if (write_trimmed_length) {
517        if (trimmed_length == 2) {
518          VP8LPutBits(bw, 0, 3 + 2);     
519        } else {
520          const int nbits = BitsLog2Floor(trimmed_length - 2);
521          const int nbitpairs = nbits / 2 + 1;
522          assert(trimmed_length > 2);
523          assert(nbitpairs - 1 < 8);
524          VP8LPutBits(bw, nbitpairs - 1, 3);
525          VP8LPutBits(bw, trimmed_length - 2, nbitpairs * 2);
526        }
527      }
528      StoreHuffmanTreeToBitMask(bw, tokens, length, &huffman_code);
529    }
530  }
531  static void StoreHuffmanCode(VP8LBitWriter* const bw,
532                               HuffmanTree* const huff_tree,
533                               HuffmanTreeToken* const tokens,
534                               const HuffmanTreeCode* const huffman_code) {
535    int i;
536    int count = 0;
537    int symbols[2] = { 0, 0 };
538    const int kMaxBits = 8;
539    const int kMaxSymbol = 1 << kMaxBits;
540    for (i = 0; i < huffman_code->num_symbols && count < 3; ++i) {
541      if (huffman_code->code_lengths[i] != 0) {
542        if (count < 2) symbols[count] = i;
543        ++count;
544      }
545    }
546    if (count == 0) {   
547      VP8LPutBits(bw, 0x01, 4);
548    } else if (count <= 2 && symbols[0] < kMaxSymbol && symbols[1] < kMaxSymbol) {
549      VP8LPutBits(bw, 1, 1);  
550      VP8LPutBits(bw, count - 1, 1);
551      if (symbols[0] <= 1) {
552        VP8LPutBits(bw, 0, 1);  
553        VP8LPutBits(bw, symbols[0], 1);
554      } else {
555        VP8LPutBits(bw, 1, 1);
556        VP8LPutBits(bw, symbols[0], 8);
557      }
558      if (count == 2) {
559        VP8LPutBits(bw, symbols[1], 8);
560      }
561    } else {
562      StoreFullHuffmanCode(bw, huff_tree, tokens, huffman_code);
563    }
564  }
565  static WEBP_INLINE void WriteHuffmanCode(VP8LBitWriter* const bw,
566                               const HuffmanTreeCode* const code,
567                               int code_index) {
568    const int depth = code->code_lengths[code_index];
569    const int symbol = code->codes[code_index];
570    VP8LPutBits(bw, symbol, depth);
571  }
572  static WEBP_INLINE void WriteHuffmanCodeWithExtraBits(
573      VP8LBitWriter* const bw,
574      const HuffmanTreeCode* const code,
575      int code_index,
576      int bits,
577      int n_bits) {
578    const int depth = code->code_lengths[code_index];
579    const int symbol = code->codes[code_index];
580    VP8LPutBits(bw, (bits << depth) | symbol, depth + n_bits);
581  }
582  static WebPEncodingError StoreImageToBitMask(
583      VP8LBitWriter* const bw, int width, int histo_bits,
584      const VP8LBackwardRefs* const refs,
585      const uint16_t* histogram_symbols,
586      const HuffmanTreeCode* const huffman_codes) {
587    const int histo_xsize = histo_bits ? VP8LSubSampleSize(width, histo_bits) : 1;
588    const int tile_mask = (histo_bits == 0) ? 0 : -(1 << histo_bits);
589    int x = 0;
590    int y = 0;
591    int tile_x = x & tile_mask;
592    int tile_y = y & tile_mask;
593    int histogram_ix = histogram_symbols[0];
594    const HuffmanTreeCode* codes = huffman_codes + 5 * histogram_ix;
595    VP8LRefsCursor c = VP8LRefsCursorInit(refs);
596    while (VP8LRefsCursorOk(&c)) {
597      const PixOrCopy* const v = c.cur_pos;
598      if ((tile_x != (x & tile_mask)) || (tile_y != (y & tile_mask))) {
599        tile_x = x & tile_mask;
600        tile_y = y & tile_mask;
601        histogram_ix = histogram_symbols[(y >> histo_bits) * histo_xsize +
602                                         (x >> histo_bits)];
603        codes = huffman_codes + 5 * histogram_ix;
604      }
605      if (PixOrCopyIsLiteral(v)) {
606        static const uint8_t order[] = { 1, 2, 0, 3 };
607        int k;
608        for (k = 0; k < 4; ++k) {
609          const int code = PixOrCopyLiteral(v, order[k]);
610          WriteHuffmanCode(bw, codes + k, code);
611        }
612      } else if (PixOrCopyIsCacheIdx(v)) {
613        const int code = PixOrCopyCacheIdx(v);
614        const int literal_ix = 256 + NUM_LENGTH_CODES + code;
615        WriteHuffmanCode(bw, codes, literal_ix);
616      } else {
617        int bits, n_bits;
618        int code;
619        const int distance = PixOrCopyDistance(v);
620        VP8LPrefixEncode(v->len, &code, &n_bits, &bits);
621        WriteHuffmanCodeWithExtraBits(bw, codes, 256 + code, bits, n_bits);
622        VP8LPrefixEncode(distance, &code, &n_bits, &bits);
623        WriteHuffmanCode(bw, codes + 4, code);
624        VP8LPutBits(bw, bits, n_bits);
625      }
626      x += PixOrCopyLength(v);
627      while (x >= width) {
628        x -= width;
629        ++y;
630      }
631      VP8LRefsCursorNext(&c);
632    }
633    return bw->error_ ? VP8_ENC_ERROR_OUT_OF_MEMORY : VP8_ENC_OK;
634  }
635  static WebPEncodingError EncodeImageNoHuffman(VP8LBitWriter* const bw,
636                                                const uint32_t* const argb,
637                                                VP8LHashChain* const hash_chain,
638                                                VP8LBackwardRefs* const refs_tmp1,
639                                                VP8LBackwardRefs* const refs_tmp2,
640                                                int width, int height,
641                                                int quality, int low_effort) {
642    int i;
643    int max_tokens = 0;
644    WebPEncodingError err = VP8_ENC_OK;
645    VP8LBackwardRefs* refs;
646    HuffmanTreeToken* tokens = NULL;
647    HuffmanTreeCode huffman_codes[5] = { { 0, NULL, NULL } };
648    const uint16_t histogram_symbols[1] = { 0 };    
649    int cache_bits = 0;
650    VP8LHistogramSet* histogram_image = NULL;
651    HuffmanTree* const huff_tree = (HuffmanTree*)WebPSafeMalloc(
652          3ULL * CODE_LENGTH_CODES, sizeof(*huff_tree));
653    if (huff_tree == NULL) {
654      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
655      goto Error;
656    }
657    if (!VP8LHashChainFill(hash_chain, quality, argb, width, height,
658                           low_effort)) {
659      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
660      goto Error;
661    }
662    refs = VP8LGetBackwardReferences(width, height, argb, quality, 0,
663                                     kLZ77Standard | kLZ77RLE, &cache_bits,
664                                     hash_chain, refs_tmp1, refs_tmp2);
665    if (refs == NULL) {
666      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
667      goto Error;
668    }
669    histogram_image = VP8LAllocateHistogramSet(1, cache_bits);
670    if (histogram_image == NULL) {
671      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
672      goto Error;
673    }
674    VP8LHistogramSetClear(histogram_image);
675    VP8LHistogramStoreRefs(refs, histogram_image->histograms[0]);
676    assert(histogram_image->size == 1);
677    if (!GetHuffBitLengthsAndCodes(histogram_image, huffman_codes)) {
678      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
679      goto Error;
680    }
681    VP8LPutBits(bw, 0, 1);
682    for (i = 0; i < 5; ++i) {
683      HuffmanTreeCode* const codes = &huffman_codes[i];
684      if (max_tokens < codes->num_symbols) {
685        max_tokens = codes->num_symbols;
686      }
687    }
688    tokens = (HuffmanTreeToken*)WebPSafeMalloc(max_tokens, sizeof(*tokens));
689    if (tokens == NULL) {
690      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
691      goto Error;
692    }
693    for (i = 0; i < 5; ++i) {
694      HuffmanTreeCode* const codes = &huffman_codes[i];
695      StoreHuffmanCode(bw, huff_tree, tokens, codes);
696      ClearHuffmanTreeIfOnlyOneSymbol(codes);
697    }
698    err = StoreImageToBitMask(bw, width, 0, refs, histogram_symbols,
699                              huffman_codes);
700   Error:
701    WebPSafeFree(tokens);
702    WebPSafeFree(huff_tree);
703    VP8LFreeHistogramSet(histogram_image);
704    WebPSafeFree(huffman_codes[0].codes);
705    return err;
706  }
707  static WebPEncodingError EncodeImageInternal(
708      VP8LBitWriter* const bw, const uint32_t* const argb,
709      VP8LHashChain* const hash_chain, VP8LBackwardRefs refs_array[3], int width,
710      int height, int quality, int low_effort, int use_cache,
711      const CrunchConfig* const config, int* cache_bits, int histogram_bits,
712      size_t init_byte_position, int* const hdr_size, int* const data_size) {
713    WebPEncodingError err = VP8_ENC_OK;
714    const uint32_t histogram_image_xysize =
715        VP8LSubSampleSize(width, histogram_bits) *
716        VP8LSubSampleSize(height, histogram_bits);
717    VP8LHistogramSet* histogram_image = NULL;
718    VP8LHistogram* tmp_histo = NULL;
719    int histogram_image_size = 0;
720    size_t bit_array_size = 0;
721    HuffmanTree* const huff_tree = (HuffmanTree*)WebPSafeMalloc(
722        3ULL * CODE_LENGTH_CODES, sizeof(*huff_tree));
723    HuffmanTreeToken* tokens = NULL;
724    HuffmanTreeCode* huffman_codes = NULL;
725    VP8LBackwardRefs* refs_best;
726    VP8LBackwardRefs* refs_tmp;
727    uint16_t* const histogram_symbols =
728        (uint16_t*)WebPSafeMalloc(histogram_image_xysize,
729                                  sizeof(*histogram_symbols));
730    int lz77s_idx;
731    VP8LBitWriter bw_init = *bw, bw_best;
732    int hdr_size_tmp;
733    assert(histogram_bits >= MIN_HUFFMAN_BITS);
734    assert(histogram_bits <= MAX_HUFFMAN_BITS);
735    assert(hdr_size != NULL);
736    assert(data_size != NULL);
737    if (histogram_symbols == NULL) {
738      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
739      goto Error;
740    }
741    if (use_cache) {
742      if (*cache_bits == 0) *cache_bits = MAX_COLOR_CACHE_BITS;
743    } else {
744      *cache_bits = 0;
745    }
746    if (huff_tree == NULL ||
747        !VP8LHashChainFill(hash_chain, quality, argb, width, height,
748                           low_effort) ||
749        !VP8LBitWriterInit(&bw_best, 0) ||
750        (config->lz77s_types_to_try_size_ > 1 &&
751         !VP8LBitWriterClone(bw, &bw_best))) {
752      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
753      goto Error;
754    }
755    for (lz77s_idx = 0; lz77s_idx < config->lz77s_types_to_try_size_;
756         ++lz77s_idx) {
757      refs_best = VP8LGetBackwardReferences(
758          width, height, argb, quality, low_effort,
759          config->lz77s_types_to_try_[lz77s_idx], cache_bits, hash_chain,
760          &refs_array[0], &refs_array[1]);
761      if (refs_best == NULL) {
762        err = VP8_ENC_ERROR_OUT_OF_MEMORY;
763        goto Error;
764      }
765      refs_tmp = &refs_array[refs_best == &refs_array[0] ? 1 : 0];
766      histogram_image =
767          VP8LAllocateHistogramSet(histogram_image_xysize, *cache_bits);
768      tmp_histo = VP8LAllocateHistogram(*cache_bits);
769      if (histogram_image == NULL || tmp_histo == NULL) {
770        err = VP8_ENC_ERROR_OUT_OF_MEMORY;
771        goto Error;
772      }
773      if (!VP8LGetHistoImageSymbols(width, height, refs_best, quality, low_effort,
774                                    histogram_bits, *cache_bits, histogram_image,
775                                    tmp_histo, histogram_symbols)) {
776        err = VP8_ENC_ERROR_OUT_OF_MEMORY;
777        goto Error;
778      }
779      histogram_image_size = histogram_image->size;
780      bit_array_size = 5 * histogram_image_size;
781      huffman_codes = (HuffmanTreeCode*)WebPSafeCalloc(bit_array_size,
782                                                       sizeof(*huffman_codes));
783      if (huffman_codes == NULL ||
784          !GetHuffBitLengthsAndCodes(histogram_image, huffman_codes)) {
785        err = VP8_ENC_ERROR_OUT_OF_MEMORY;
786        goto Error;
787      }
788      VP8LFreeHistogramSet(histogram_image);
789      histogram_image = NULL;
790      VP8LFreeHistogram(tmp_histo);
791      tmp_histo = NULL;
792      if (*cache_bits > 0) {
793        VP8LPutBits(bw, 1, 1);
794        VP8LPutBits(bw, *cache_bits, 4);
795      } else {
796        VP8LPutBits(bw, 0, 1);
797      }
798      {
799        const int write_histogram_image = (histogram_image_size > 1);
800        VP8LPutBits(bw, write_histogram_image, 1);
801        if (write_histogram_image) {
802          uint32_t* const histogram_argb =
803              (uint32_t*)WebPSafeMalloc(histogram_image_xysize,
804                                        sizeof(*histogram_argb));
805          int max_index = 0;
806          uint32_t i;
807          if (histogram_argb == NULL) {
808            err = VP8_ENC_ERROR_OUT_OF_MEMORY;
809            goto Error;
810          }
811          for (i = 0; i < histogram_image_xysize; ++i) {
812            const int symbol_index = histogram_symbols[i] & 0xffff;
813            histogram_argb[i] = (symbol_index << 8);
814            if (symbol_index >= max_index) {
815              max_index = symbol_index + 1;
816            }
817          }
818          histogram_image_size = max_index;
819          VP8LPutBits(bw, histogram_bits - 2, 3);
820          err = EncodeImageNoHuffman(
821              bw, histogram_argb, hash_chain, refs_tmp, &refs_array[2],
822              VP8LSubSampleSize(width, histogram_bits),
823              VP8LSubSampleSize(height, histogram_bits), quality, low_effort);
824          WebPSafeFree(histogram_argb);
825          if (err != VP8_ENC_OK) goto Error;
826        }
827      }
828      {
829        int i;
830        int max_tokens = 0;
831        for (i = 0; i < 5 * histogram_image_size; ++i) {
832          HuffmanTreeCode* const codes = &huffman_codes[i];
833          if (max_tokens < codes->num_symbols) {
834            max_tokens = codes->num_symbols;
835          }
836        }
837        tokens = (HuffmanTreeToken*)WebPSafeMalloc(max_tokens, sizeof(*tokens));
838        if (tokens == NULL) {
839          err = VP8_ENC_ERROR_OUT_OF_MEMORY;
840          goto Error;
841        }
842        for (i = 0; i < 5 * histogram_image_size; ++i) {
843          HuffmanTreeCode* const codes = &huffman_codes[i];
844          StoreHuffmanCode(bw, huff_tree, tokens, codes);
845          ClearHuffmanTreeIfOnlyOneSymbol(codes);
846        }
847      }
848      hdr_size_tmp = (int)(VP8LBitWriterNumBytes(bw) - init_byte_position);
849      err = StoreImageToBitMask(bw, width, histogram_bits, refs_best,
850                                histogram_symbols, huffman_codes);
851      if (lz77s_idx == 0 ||
852          VP8LBitWriterNumBytes(bw) < VP8LBitWriterNumBytes(&bw_best)) {
853        *hdr_size = hdr_size_tmp;
854        *data_size =
855            (int)(VP8LBitWriterNumBytes(bw) - init_byte_position - *hdr_size);
856        VP8LBitWriterSwap(bw, &bw_best);
857      }
858      if (config->lz77s_types_to_try_size_ > 1) VP8LBitWriterReset(&bw_init, bw);
859      WebPSafeFree(tokens);
860      tokens = NULL;
861      if (huffman_codes != NULL) {
862        WebPSafeFree(huffman_codes->codes);
863        WebPSafeFree(huffman_codes);
864        huffman_codes = NULL;
865      }
866    }
867    VP8LBitWriterSwap(bw, &bw_best);
868   Error:
869    WebPSafeFree(tokens);
870    WebPSafeFree(huff_tree);
871    VP8LFreeHistogramSet(histogram_image);
872    VP8LFreeHistogram(tmp_histo);
873    if (huffman_codes != NULL) {
874      WebPSafeFree(huffman_codes->codes);
875      WebPSafeFree(huffman_codes);
876    }
877    WebPSafeFree(histogram_symbols);
878    VP8LBitWriterWipeOut(&bw_best);
879    return err;
880  }
881  static void ApplySubtractGreen(VP8LEncoder* const enc, int width, int height,
882                                 VP8LBitWriter* const bw) {
883    VP8LPutBits(bw, TRANSFORM_PRESENT, 1);
884    VP8LPutBits(bw, SUBTRACT_GREEN, 2);
885    VP8LSubtractGreenFromBlueAndRed(enc->argb_, width * height);
886  }
887  static WebPEncodingError ApplyPredictFilter(const VP8LEncoder* const enc,
888                                              int width, int height,
889                                              int quality, int low_effort,
890                                              int used_subtract_green,
891                                              VP8LBitWriter* const bw) {
892    const int pred_bits = enc->transform_bits_;
893    const int transform_width = VP8LSubSampleSize(width, pred_bits);
894    const int transform_height = VP8LSubSampleSize(height, pred_bits);
895    const int near_lossless_strength = enc->use_palette_ ? 100
896                                     : enc->config_->near_lossless;
897    VP8LResidualImage(width, height, pred_bits, low_effort, enc->argb_,
898                      enc->argb_scratch_, enc->transform_data_,
899                      near_lossless_strength, enc->config_->exact,
900                      used_subtract_green);
901    VP8LPutBits(bw, TRANSFORM_PRESENT, 1);
902    VP8LPutBits(bw, PREDICTOR_TRANSFORM, 2);
903    assert(pred_bits >= 2);
904    VP8LPutBits(bw, pred_bits - 2, 3);
905    return EncodeImageNoHuffman(
906        bw, enc->transform_data_, (VP8LHashChain*)&enc->hash_chain_,
907        (VP8LBackwardRefs*)&enc->refs_[0],  
908        (VP8LBackwardRefs*)&enc->refs_[1], transform_width, transform_height,
909        quality, low_effort);
910  }
911  static WebPEncodingError ApplyCrossColorFilter(const VP8LEncoder* const enc,
912                                                 int width, int height,
913                                                 int quality, int low_effort,
914                                                 VP8LBitWriter* const bw) {
915    const int ccolor_transform_bits = enc->transform_bits_;
916    const int transform_width = VP8LSubSampleSize(width, ccolor_transform_bits);
917    const int transform_height = VP8LSubSampleSize(height, ccolor_transform_bits);
918    VP8LColorSpaceTransform(width, height, ccolor_transform_bits, quality,
919                            enc->argb_, enc->transform_data_);
920    VP8LPutBits(bw, TRANSFORM_PRESENT, 1);
921    VP8LPutBits(bw, CROSS_COLOR_TRANSFORM, 2);
922    assert(ccolor_transform_bits >= 2);
923    VP8LPutBits(bw, ccolor_transform_bits - 2, 3);
924    return EncodeImageNoHuffman(
925        bw, enc->transform_data_, (VP8LHashChain*)&enc->hash_chain_,
926        (VP8LBackwardRefs*)&enc->refs_[0],  
927        (VP8LBackwardRefs*)&enc->refs_[1], transform_width, transform_height,
928        quality, low_effort);
929  }
930  static WebPEncodingError WriteRiffHeader(const WebPPicture* const pic,
931                                           size_t riff_size, size_t vp8l_size) {
932    uint8_t riff[RIFF_HEADER_SIZE + CHUNK_HEADER_SIZE + VP8L_SIGNATURE_SIZE] = {
933      'R', 'I', 'F', 'F', 0, 0, 0, 0, 'W', 'E', 'B', 'P',
934      'V', 'P', '8', 'L', 0, 0, 0, 0, VP8L_MAGIC_BYTE,
935    };
936    PutLE32(riff + TAG_SIZE, (uint32_t)riff_size);
937    PutLE32(riff + RIFF_HEADER_SIZE + TAG_SIZE, (uint32_t)vp8l_size);
938    if (!pic->writer(riff, sizeof(riff), pic)) {
939      return VP8_ENC_ERROR_BAD_WRITE;
940    }
941    return VP8_ENC_OK;
942  }
943  static int WriteImageSize(const WebPPicture* const pic,
944                            VP8LBitWriter* const bw) {
945    const int width = pic->width - 1;
946    const int height = pic->height - 1;
947    assert(width < WEBP_MAX_DIMENSION && height < WEBP_MAX_DIMENSION);
948    VP8LPutBits(bw, width, VP8L_IMAGE_SIZE_BITS);
949    VP8LPutBits(bw, height, VP8L_IMAGE_SIZE_BITS);
950    return !bw->error_;
951  }
952  static int WriteRealAlphaAndVersion(VP8LBitWriter* const bw, int has_alpha) {
953    VP8LPutBits(bw, has_alpha, 1);
954    VP8LPutBits(bw, VP8L_VERSION, VP8L_VERSION_BITS);
955    return !bw->error_;
956  }
957  static WebPEncodingError WriteImage(const WebPPicture* const pic,
958                                      VP8LBitWriter* const bw,
959                                      size_t* const coded_size) {
960    WebPEncodingError err = VP8_ENC_OK;
961    const uint8_t* const webpll_data = VP8LBitWriterFinish(bw);
962    const size_t webpll_size = VP8LBitWriterNumBytes(bw);
963    const size_t vp8l_size = VP8L_SIGNATURE_SIZE + webpll_size;
964    const size_t pad = vp8l_size & 1;
965    const size_t riff_size = TAG_SIZE + CHUNK_HEADER_SIZE + vp8l_size + pad;
966    err = WriteRiffHeader(pic, riff_size, vp8l_size);
967    if (err != VP8_ENC_OK) goto Error;
968    if (!pic->writer(webpll_data, webpll_size, pic)) {
969      err = VP8_ENC_ERROR_BAD_WRITE;
970      goto Error;
971    }
972    if (pad) {
973      const uint8_t pad_byte[1] = { 0 };
974      if (!pic->writer(pad_byte, 1, pic)) {
975        err = VP8_ENC_ERROR_BAD_WRITE;
976        goto Error;
977      }
978    }
979    *coded_size = CHUNK_HEADER_SIZE + riff_size;
980    return VP8_ENC_OK;
981   Error:
982    return err;
983  }
984  static void ClearTransformBuffer(VP8LEncoder* const enc) {
985    WebPSafeFree(enc->transform_mem_);
986    enc->transform_mem_ = NULL;
987    enc->transform_mem_size_ = 0;
988  }
989  static WebPEncodingError AllocateTransformBuffer(VP8LEncoder* const enc,
990                                                   int width, int height) {
991    WebPEncodingError err = VP8_ENC_OK;
992    const uint64_t image_size = width * height;
993    const uint64_t argb_scratch_size =
994        enc->use_predict_
995            ? (width + 1) * 2 +
996              (width * 2 + sizeof(uint32_t) - 1) / sizeof(uint32_t)
997            : 0;
998    const uint64_t transform_data_size =
999        (enc->use_predict_ || enc->use_cross_color_)
1000            ? VP8LSubSampleSize(width, enc->transform_bits_) *
1001                  VP8LSubSampleSize(height, enc->transform_bits_)
1002            : 0;
1003    const uint64_t max_alignment_in_words =
1004        (WEBP_ALIGN_CST + sizeof(uint32_t) - 1) / sizeof(uint32_t);
1005    const uint64_t mem_size =
1006        image_size + max_alignment_in_words +
1007        argb_scratch_size + max_alignment_in_words +
1008        transform_data_size;
1009    uint32_t* mem = enc->transform_mem_;
1010    if (mem == NULL || mem_size > enc->transform_mem_size_) {
1011      ClearTransformBuffer(enc);
1012      mem = (uint32_t*)WebPSafeMalloc(mem_size, sizeof(*mem));
1013      if (mem == NULL) {
1014        err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1015        goto Error;
1016      }
1017      enc->transform_mem_ = mem;
1018      enc->transform_mem_size_ = (size_t)mem_size;
1019      enc->argb_content_ = kEncoderNone;
1020    }
1021    enc->argb_ = mem;
1022    mem = (uint32_t*)WEBP_ALIGN(mem + image_size);
1023    enc->argb_scratch_ = mem;
1024    mem = (uint32_t*)WEBP_ALIGN(mem + argb_scratch_size);
1025    enc->transform_data_ = mem;
1026    enc->current_width_ = width;
1027   Error:
1028    return err;
1029  }
1030  static WebPEncodingError MakeInputImageCopy(VP8LEncoder* const enc) {
1031    WebPEncodingError err = VP8_ENC_OK;
1032    const WebPPicture* const picture = enc->pic_;
1033    const int width = picture->width;
1034    const int height = picture->height;
1035    err = AllocateTransformBuffer(enc, width, height);
1036    if (err != VP8_ENC_OK) return err;
1037    if (enc->argb_content_ == kEncoderARGB) return VP8_ENC_OK;
1038    {
1039      uint32_t* dst = enc->argb_;
1040      const uint32_t* src = picture->argb;
1041      int y;
1042      for (y = 0; y < height; ++y) {
1043        memcpy(dst, src, width * sizeof(*dst));
1044        dst += width;
1045        src += picture->argb_stride;
1046      }
1047    }
1048    enc->argb_content_ = kEncoderARGB;
1049    assert(enc->current_width_ == width);
1050    return VP8_ENC_OK;
1051  }
1052  static WEBP_INLINE int SearchColorNoIdx(const uint32_t sorted[], uint32_t color,
1053                                          int hi) {
1054    int low = 0;
1055    if (sorted[low] == color) return low;  
1056    while (1) {
1057      const int mid = (low + hi) >> 1;
1058      if (sorted[mid] == color) {
1059        return mid;
1060      } else if (sorted[mid] < color) {
1061        low = mid;
1062      } else {
1063        hi = mid;
1064      }
1065    }
1066  }
1067  #define APPLY_PALETTE_GREEDY_MAX 4
1068  static WEBP_INLINE uint32_t SearchColorGreedy(const uint32_t palette[],
1069                                                int palette_size,
1070                                                uint32_t color) {
1071    (void)palette_size;
1072    assert(palette_size < APPLY_PALETTE_GREEDY_MAX);
1073    assert(3 == APPLY_PALETTE_GREEDY_MAX - 1);
1074    if (color == palette[0]) return 0;
1075    if (color == palette[1]) return 1;
1076    if (color == palette[2]) return 2;
1077    return 3;
1078  }
1079  static WEBP_INLINE uint32_t ApplyPaletteHash0(uint32_t color) {
1080    return (color >> 8) & 0xff;
1081  }
1082  #define PALETTE_INV_SIZE_BITS 11
1083  #define PALETTE_INV_SIZE (1 << PALETTE_INV_SIZE_BITS)
1084  static WEBP_INLINE uint32_t ApplyPaletteHash1(uint32_t color) {
1085    return ((uint32_t)((color & 0x00ffffffu) * 4222244071ull)) >>
1086           (32 - PALETTE_INV_SIZE_BITS);
1087  }
1088  static WEBP_INLINE uint32_t ApplyPaletteHash2(uint32_t color) {
1089    return ((uint32_t)((color & 0x00ffffffu) * ((1ull << 31) - 1))) >>
1090           (32 - PALETTE_INV_SIZE_BITS);
1091  }
1092  static void PrepareMapToPalette(const uint32_t palette[], int num_colors,
1093                                  uint32_t sorted[], uint32_t idx_map[]) {
1094    int i;
1095    memcpy(sorted, palette, num_colors * sizeof(*sorted));
1096    qsort(sorted, num_colors, sizeof(*sorted), PaletteCompareColorsForQsort);
1097    for (i = 0; i < num_colors; ++i) {
1098      idx_map[SearchColorNoIdx(sorted, palette[i], num_colors)] = i;
1099    }
1100  }
1101  #define APPLY_PALETTE_FOR(COLOR_INDEX) do {         \
1102    uint32_t prev_pix = palette[0];                   \
1103    uint32_t prev_idx = 0;                            \
1104    for (y = 0; y < height; ++y) {                    \
1105      for (x = 0; x < width; ++x) {                   \
1106        const uint32_t pix = src[x];                  \
1107        if (pix != prev_pix) {                        \
1108          prev_idx = COLOR_INDEX;                     \
1109          prev_pix = pix;                             \
1110        }                                             \
1111        tmp_row[x] = prev_idx;                        \
1112      }                                               \
1113      VP8LBundleColorMap(tmp_row, width, xbits, dst); \
1114      src += src_stride;                              \
1115      dst += dst_stride;                              \
1116    }                                                 \
1117  } while (0)
1118  static WebPEncodingError ApplyPalette(const uint32_t* src, uint32_t src_stride,
1119                                        uint32_t* dst, uint32_t dst_stride,
1120                                        const uint32_t* palette, int palette_size,
1121                                        int width, int height, int xbits) {
1122    uint8_t* const tmp_row = (uint8_t*)WebPSafeMalloc(width, sizeof(*tmp_row));
1123    int x, y;
1124    if (tmp_row == NULL) return VP8_ENC_ERROR_OUT_OF_MEMORY;
1125    if (palette_size < APPLY_PALETTE_GREEDY_MAX) {
1126      APPLY_PALETTE_FOR(SearchColorGreedy(palette, palette_size, pix));
1127    } else {
1128      int i, j;
1129      uint16_t buffer[PALETTE_INV_SIZE];
1130      uint32_t (*const hash_functions[])(uint32_t) = {
1131          ApplyPaletteHash0, ApplyPaletteHash1, ApplyPaletteHash2
1132      };
1133      for (i = 0; i < 3; ++i) {
1134        int use_LUT = 1;
1135        memset(buffer, 0xff, sizeof(buffer));
1136        for (j = 0; j < palette_size; ++j) {
1137          const uint32_t ind = hash_functions[i](palette[j]);
1138          if (buffer[ind] != 0xffffu) {
1139            use_LUT = 0;
1140            break;
1141          } else {
1142            buffer[ind] = j;
1143          }
1144        }
1145        if (use_LUT) break;
1146      }
1147      if (i == 0) {
1148        APPLY_PALETTE_FOR(buffer[ApplyPaletteHash0(pix)]);
1149      } else if (i == 1) {
1150        APPLY_PALETTE_FOR(buffer[ApplyPaletteHash1(pix)]);
1151      } else if (i == 2) {
1152        APPLY_PALETTE_FOR(buffer[ApplyPaletteHash2(pix)]);
1153      } else {
1154        uint32_t idx_map[MAX_PALETTE_SIZE];
1155        uint32_t palette_sorted[MAX_PALETTE_SIZE];
1156        PrepareMapToPalette(palette, palette_size, palette_sorted, idx_map);
1157        APPLY_PALETTE_FOR(
1158            idx_map[SearchColorNoIdx(palette_sorted, pix, palette_size)]);
1159      }
1160    }
1161    WebPSafeFree(tmp_row);
1162    return VP8_ENC_OK;
1163  }
1164  #undef APPLY_PALETTE_FOR
1165  #undef PALETTE_INV_SIZE_BITS
1166  #undef PALETTE_INV_SIZE
1167  #undef APPLY_PALETTE_GREEDY_MAX
1168  static WebPEncodingError MapImageFromPalette(VP8LEncoder* const enc,
1169                                               int in_place) {
1170    WebPEncodingError err = VP8_ENC_OK;
1171    const WebPPicture* const pic = enc->pic_;
1172    const int width = pic->width;
1173    const int height = pic->height;
1174    const uint32_t* const palette = enc->palette_;
1175    const uint32_t* src = in_place ? enc->argb_ : pic->argb;
1176    const int src_stride = in_place ? enc->current_width_ : pic->argb_stride;
1177    const int palette_size = enc->palette_size_;
1178    int xbits;
1179    if (palette_size <= 4) {
1180      xbits = (palette_size <= 2) ? 3 : 2;
1181    } else {
1182      xbits = (palette_size <= 16) ? 1 : 0;
1183    }
1184    err = AllocateTransformBuffer(enc, VP8LSubSampleSize(width, xbits), height);
1185    if (err != VP8_ENC_OK) return err;
1186    err = ApplyPalette(src, src_stride,
1187                       enc->argb_, enc->current_width_,
1188                       palette, palette_size, width, height, xbits);
1189    enc->argb_content_ = kEncoderPalette;
1190    return err;
1191  }
1192  static WebPEncodingError EncodePalette(VP8LBitWriter* const bw, int low_effort,
1193                                         VP8LEncoder* const enc) {
1194    int i;
1195    uint32_t tmp_palette[MAX_PALETTE_SIZE];
1196    const int palette_size = enc->palette_size_;
1197    const uint32_t* const palette = enc->palette_;
1198    VP8LPutBits(bw, TRANSFORM_PRESENT, 1);
1199    VP8LPutBits(bw, COLOR_INDEXING_TRANSFORM, 2);
1200    assert(palette_size >= 1 && palette_size <= MAX_PALETTE_SIZE);
1201    VP8LPutBits(bw, palette_size - 1, 8);
1202    for (i = palette_size - 1; i >= 1; --i) {
1203      tmp_palette[i] = VP8LSubPixels(palette[i], palette[i - 1]);
1204    }
1205    tmp_palette[0] = palette[0];
1206    return EncodeImageNoHuffman(bw, tmp_palette, &enc->hash_chain_,
1207                                &enc->refs_[0], &enc->refs_[1], palette_size, 1,
1208                                20 &bsol;* quality */, low_effort);
1209  }
1210  static VP8LEncoder* VP8LEncoderNew(const WebPConfig* const config,
1211                                     const WebPPicture* const picture) {
1212    VP8LEncoder* const enc = (VP8LEncoder*)WebPSafeCalloc(1ULL, sizeof(*enc));
1213    if (enc == NULL) {
1214      WebPEncodingSetError(picture, VP8_ENC_ERROR_OUT_OF_MEMORY);
1215      return NULL;
1216    }
1217    enc->config_ = config;
1218    enc->pic_ = picture;
1219    enc->argb_content_ = kEncoderNone;
1220    VP8LEncDspInit();
1221    return enc;
1222  }
1223  static void VP8LEncoderDelete(VP8LEncoder* enc) {
1224    if (enc != NULL) {
1225      int i;
1226      VP8LHashChainClear(&enc->hash_chain_);
1227      for (i = 0; i < 3; ++i) VP8LBackwardRefsClear(&enc->refs_[i]);
1228      ClearTransformBuffer(enc);
1229      WebPSafeFree(enc);
1230    }
1231  }
1232  typedef struct {
1233    const WebPConfig* config_;
1234    const WebPPicture* picture_;
1235    VP8LBitWriter* bw_;
1236    VP8LEncoder* enc_;
1237    int use_cache_;
1238    CrunchConfig crunch_configs_[CRUNCH_CONFIGS_MAX];
1239    int num_crunch_configs_;
1240    int red_and_blue_always_zero_;
1241    WebPEncodingError err_;
1242    WebPAuxStats* stats_;
1243  } StreamEncodeContext;
1244  static int EncodeStreamHook(void* input, void* data2) {
1245    StreamEncodeContext* const params = (StreamEncodeContext*)input;
1246    const WebPConfig* const config = params->config_;
1247    const WebPPicture* const picture = params->picture_;
1248    VP8LBitWriter* const bw = params->bw_;
1249    VP8LEncoder* const enc = params->enc_;
1250    const int use_cache = params->use_cache_;
1251    const CrunchConfig* const crunch_configs = params->crunch_configs_;
1252    const int num_crunch_configs = params->num_crunch_configs_;
1253    const int red_and_blue_always_zero = params->red_and_blue_always_zero_;
1254  #if !defined(WEBP_DISABLE_STATS)
1255    WebPAuxStats* const stats = params->stats_;
1256  #endif
1257    WebPEncodingError err = VP8_ENC_OK;
1258    const int quality = (int)config->quality;
1259    const int low_effort = (config->method == 0);
1260  #if (WEBP_NEAR_LOSSLESS == 1)
1261    const int width = picture->width;
1262  #endif
1263    const int height = picture->height;
1264    const size_t byte_position = VP8LBitWriterNumBytes(bw);
1265  #if (WEBP_NEAR_LOSSLESS == 1)
1266    int use_near_lossless = 0;
1267  #endif
1268    int hdr_size = 0;
1269    int data_size = 0;
1270    int use_delta_palette = 0;
1271    int idx;
1272    size_t best_size = 0;
1273    VP8LBitWriter bw_init = *bw, bw_best;
1274    (void)data2;
1275    if (!VP8LBitWriterInit(&bw_best, 0) ||
1276        (num_crunch_configs > 1 && !VP8LBitWriterClone(bw, &bw_best))) {
1277      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1278      goto Error;
1279    }
1280    for (idx = 0; idx < num_crunch_configs; ++idx) {
1281      const int entropy_idx = crunch_configs[idx].entropy_idx_;
1282      enc->use_palette_ = (entropy_idx == kPalette);
1283      enc->use_subtract_green_ =
1284          (entropy_idx == kSubGreen) || (entropy_idx == kSpatialSubGreen);
1285      enc->use_predict_ =
1286          (entropy_idx == kSpatial) || (entropy_idx == kSpatialSubGreen);
1287      if (low_effort) {
1288        enc->use_cross_color_ = 0;
1289      } else {
1290        enc->use_cross_color_ = red_and_blue_always_zero ? 0 : enc->use_predict_;
1291      }
1292      enc->cache_bits_ = 0;
1293      VP8LBackwardRefsClear(&enc->refs_[0]);
1294      VP8LBackwardRefsClear(&enc->refs_[1]);
1295  #if (WEBP_NEAR_LOSSLESS == 1)
1296      use_near_lossless = (config->near_lossless < 100) && !enc->use_palette_ &&
1297                          !enc->use_predict_;
1298      if (use_near_lossless) {
1299        err = AllocateTransformBuffer(enc, width, height);
1300        if (err != VP8_ENC_OK) goto Error;
1301        if ((enc->argb_content_ != kEncoderNearLossless) &&
1302            !VP8ApplyNearLossless(picture, config->near_lossless, enc->argb_)) {
1303          err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1304          goto Error;
1305        }
1306        enc->argb_content_ = kEncoderNearLossless;
1307      } else {
1308        enc->argb_content_ = kEncoderNone;
1309      }
1310  #else
1311      enc->argb_content_ = kEncoderNone;
1312  #endif
1313      if (enc->use_palette_) {
1314        err = EncodePalette(bw, low_effort, enc);
1315        if (err != VP8_ENC_OK) goto Error;
1316        err = MapImageFromPalette(enc, use_delta_palette);
1317        if (err != VP8_ENC_OK) goto Error;
1318        if (use_cache && enc->palette_size_ < (1 << MAX_COLOR_CACHE_BITS)) {
1319          enc->cache_bits_ = BitsLog2Floor(enc->palette_size_) + 1;
1320        }
1321      }
1322      if (!use_delta_palette) {
1323        if (enc->argb_content_ != kEncoderNearLossless &&
1324            enc->argb_content_ != kEncoderPalette) {
1325          err = MakeInputImageCopy(enc);
1326          if (err != VP8_ENC_OK) goto Error;
1327        }
1328        if (enc->use_subtract_green_) {
1329          ApplySubtractGreen(enc, enc->current_width_, height, bw);
1330        }
1331        if (enc->use_predict_) {
1332          err = ApplyPredictFilter(enc, enc->current_width_, height, quality,
1333                                   low_effort, enc->use_subtract_green_, bw);
1334          if (err != VP8_ENC_OK) goto Error;
1335        }
1336        if (enc->use_cross_color_) {
1337          err = ApplyCrossColorFilter(enc, enc->current_width_, height, quality,
1338                                      low_effort, bw);
1339          if (err != VP8_ENC_OK) goto Error;
1340        }
1341      }
1342      VP8LPutBits(bw, !TRANSFORM_PRESENT, 1);  
1343      err = EncodeImageInternal(bw, enc->argb_, &enc->hash_chain_, enc->refs_,
1344                                enc->current_width_, height, quality, low_effort,
1345                                use_cache, &crunch_configs[idx],
1346                                &enc->cache_bits_, enc->histo_bits_,
1347                                byte_position, &hdr_size, &data_size);
1348      if (err != VP8_ENC_OK) goto Error;
1349      if (idx == 0 || VP8LBitWriterNumBytes(bw) < best_size) {
1350        best_size = VP8LBitWriterNumBytes(bw);
1351        VP8LBitWriterSwap(bw, &bw_best);
1352  #if !defined(WEBP_DISABLE_STATS)
1353        if (stats != NULL) {
1354          stats->lossless_features = 0;
1355          if (enc->use_predict_) stats->lossless_features |= 1;
1356          if (enc->use_cross_color_) stats->lossless_features |= 2;
1357          if (enc->use_subtract_green_) stats->lossless_features |= 4;
1358          if (enc->use_palette_) stats->lossless_features |= 8;
1359          stats->histogram_bits = enc->histo_bits_;
1360          stats->transform_bits = enc->transform_bits_;
1361          stats->cache_bits = enc->cache_bits_;
1362          stats->palette_size = enc->palette_size_;
1363          stats->lossless_size = (int)(best_size - byte_position);
1364          stats->lossless_hdr_size = hdr_size;
1365          stats->lossless_data_size = data_size;
1366        }
1367  #endif
1368      }
1369      if (num_crunch_configs > 1) VP8LBitWriterReset(&bw_init, bw);
1370    }
1371    VP8LBitWriterSwap(&bw_best, bw);
1372  Error:
1373    VP8LBitWriterWipeOut(&bw_best);
1374    params->err_ = err;
1375    return (err == VP8_ENC_OK);
1376  }
1377  WebPEncodingError VP8LEncodeStream(const WebPConfig* const config,
1378                                     const WebPPicture* const picture,
1379                                     VP8LBitWriter* const bw_main,
1380                                     int use_cache) {
1381    WebPEncodingError err = VP8_ENC_OK;
1382    VP8LEncoder* const enc_main = VP8LEncoderNew(config, picture);
1383    VP8LEncoder* enc_side = NULL;
1384    CrunchConfig crunch_configs[CRUNCH_CONFIGS_MAX];
1385    int num_crunch_configs_main, num_crunch_configs_side = 0;
1386    int idx;
1387    int red_and_blue_always_zero = 0;
1388    WebPWorker worker_main, worker_side;
1389    StreamEncodeContext params_main, params_side;
1390    WebPAuxStats stats_side;
1391    VP8LBitWriter bw_side;
1392    const WebPWorkerInterface* const worker_interface = WebPGetWorkerInterface();
1393    int ok_main;
1394    if (enc_main == NULL ||
1395        !EncoderAnalyze(enc_main, crunch_configs, &num_crunch_configs_main,
1396                        &red_and_blue_always_zero) ||
1397        !EncoderInit(enc_main) || !VP8LBitWriterInit(&bw_side, 0)) {
1398      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1399      goto Error;
1400    }
1401    if (config->thread_level > 0) {
1402      num_crunch_configs_side = num_crunch_configs_main / 2;
1403      for (idx = 0; idx < num_crunch_configs_side; ++idx) {
1404        params_side.crunch_configs_[idx] =
1405            crunch_configs[num_crunch_configs_main - num_crunch_configs_side +
1406                           idx];
1407      }
1408      params_side.num_crunch_configs_ = num_crunch_configs_side;
1409    }
1410    num_crunch_configs_main -= num_crunch_configs_side;
1411    for (idx = 0; idx < num_crunch_configs_main; ++idx) {
1412      params_main.crunch_configs_[idx] = crunch_configs[idx];
1413    }
1414    params_main.num_crunch_configs_ = num_crunch_configs_main;
1415    {
1416      const int params_size = (num_crunch_configs_side > 0) ? 2 : 1;
1417      for (idx = 0; idx < params_size; ++idx) {
1418        WebPWorker* const worker = (idx == 0) ? &worker_main : &worker_side;
1419        StreamEncodeContext* const param =
1420            (idx == 0) ? &params_main : &params_side;
1421        param->config_ = config;
1422        param->picture_ = picture;
1423        param->use_cache_ = use_cache;
1424        param->red_and_blue_always_zero_ = red_and_blue_always_zero;
1425        if (idx == 0) {
1426          param->stats_ = picture->stats;
1427          param->bw_ = bw_main;
1428          param->enc_ = enc_main;
1429        } else {
1430          param->stats_ = (picture->stats == NULL) ? NULL : &stats_side;
1431          if (!VP8LBitWriterClone(bw_main, &bw_side)) {
1432            err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1433            goto Error;
1434          }
1435          param->bw_ = &bw_side;
1436          enc_side = VP8LEncoderNew(config, picture);
1437          if (enc_side == NULL || !EncoderInit(enc_side)) {
1438            err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1439            goto Error;
1440          }
1441          enc_side->histo_bits_ = enc_main->histo_bits_;
1442          enc_side->transform_bits_ = enc_main->transform_bits_;
1443          enc_side->palette_size_ = enc_main->palette_size_;
1444          memcpy(enc_side->palette_, enc_main->palette_,
1445                 sizeof(enc_main->palette_));
1446          param->enc_ = enc_side;
1447        }
1448        worker_interface->Init(worker);
1449        worker->data1 = param;
1450        worker->data2 = NULL;
1451        worker->hook = EncodeStreamHook;
1452      }
1453    }
1454    if (num_crunch_configs_side != 0) {
1455      if (!worker_interface->Reset(&worker_side)) {
1456        err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1457        goto Error;
1458      }
1459  #if !defined(WEBP_DISABLE_STATS)
1460      if (picture->stats != NULL) {
1461        memcpy(&stats_side, picture->stats, sizeof(stats_side));
1462      }
1463  #endif
1464      params_side.err_ = VP8_ENC_OK;
1465      worker_interface->Launch(&worker_side);
1466    }
1467    worker_interface->Execute(&worker_main);
1468    ok_main = worker_interface->Sync(&worker_main);
1469    worker_interface->End(&worker_main);
1470    if (num_crunch_configs_side != 0) {
1471      const int ok_side = worker_interface->Sync(&worker_side);
1472      worker_interface->End(&worker_side);
1473      if (!ok_main || !ok_side) {
1474        err = ok_main ? params_side.err_ : params_main.err_;
1475        goto Error;
1476      }
1477      if (VP8LBitWriterNumBytes(&bw_side) < VP8LBitWriterNumBytes(bw_main)) {
1478        VP8LBitWriterSwap(bw_main, &bw_side);
1479  #if !defined(WEBP_DISABLE_STATS)
1480        if (picture->stats != NULL) {
1481          memcpy(picture->stats, &stats_side, sizeof(*picture->stats));
1482        }
1483  #endif
1484      }
1485    } else {
1486      if (!ok_main) {
1487        err = params_main.err_;
1488        goto Error;
1489      }
1490    }
1491  Error:
1492    VP8LBitWriterWipeOut(&bw_side);
1493    VP8LEncoderDelete(enc_main);
1494    VP8LEncoderDelete(enc_side);
1495    return err;
1496  }
1497  #undef CRUNCH_CONFIGS_MAX
1498  #undef CRUNCH_CONFIGS_LZ77_MAX
1499  int VP8LEncodeImage(const WebPConfig* const config,
1500                      const WebPPicture* const picture) {
1501    int width, height;
1502    int has_alpha;
1503    size_t coded_size;
1504    int percent = 0;
1505    int initial_size;
1506    WebPEncodingError err = VP8_ENC_OK;
1507    VP8LBitWriter bw;
1508    if (picture == NULL) return 0;
1509    if (config == NULL || picture->argb == NULL) {
1510      err = VP8_ENC_ERROR_NULL_PARAMETER;
1511      WebPEncodingSetError(picture, err);
1512      return 0;
1513    }
1514    width = picture->width;
1515    height = picture->height;
1516    initial_size = (config->image_hint == WEBP_HINT_GRAPH) ?
1517        width * height : width * height * 2;
1518    if (!VP8LBitWriterInit(&bw, initial_size)) {
1519      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1520      goto Error;
1521    }
1522    if (!WebPReportProgress(picture, 1, &percent)) {
1523   UserAbort:
1524      err = VP8_ENC_ERROR_USER_ABORT;
1525      goto Error;
1526    }
1527    if (picture->stats != NULL) {
1528      WebPAuxStats* const stats = picture->stats;
1529      memset(stats, 0, sizeof(*stats));
1530      stats->PSNR[0] = 99.f;
1531      stats->PSNR[1] = 99.f;
1532      stats->PSNR[2] = 99.f;
1533      stats->PSNR[3] = 99.f;
1534      stats->PSNR[4] = 99.f;
1535    }
1536    if (!WriteImageSize(picture, &bw)) {
1537      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1538      goto Error;
1539    }
1540    has_alpha = WebPPictureHasTransparency(picture);
1541    if (!WriteRealAlphaAndVersion(&bw, has_alpha)) {
1542      err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1543      goto Error;
1544    }
1545    if (!WebPReportProgress(picture, 5, &percent)) goto UserAbort;
1546    err = VP8LEncodeStream(config, picture, &bw, 1 &bsol;*use_cache*/);
1547    if (err != VP8_ENC_OK) goto Error;
1548    if (!WebPReportProgress(picture, 90, &percent)) goto UserAbort;
1549    err = WriteImage(picture, &bw, &coded_size);
1550    if (err != VP8_ENC_OK) goto Error;
1551    if (!WebPReportProgress(picture, 100, &percent)) goto UserAbort;
1552  #if !defined(WEBP_DISABLE_STATS)
1553    if (picture->stats != NULL) {
1554      picture->stats->coded_size += (int)coded_size;
1555      picture->stats->lossless_size = (int)coded_size;
1556    }
1557  #endif
1558    if (picture->extra_info != NULL) {
1559      const int mb_w = (width + 15) >> 4;
1560      const int mb_h = (height + 15) >> 4;
1561      memset(picture->extra_info, 0, mb_w * mb_h * sizeof(*picture->extra_info));
1562    }
1563   Error:
1564    if (bw.error_) err = VP8_ENC_ERROR_OUT_OF_MEMORY;
1565    VP8LBitWriterWipeOut(&bw);
1566    if (err != VP8_ENC_OK) {
1567      WebPEncodingSetError(picture, err);
1568      return 0;
1569    }
1570    return 1;
1571  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-extent_24.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-vp8l_enc.c</div>
                </div>
                <div class="column column_space"><pre><code>117  		return lock_result_success;
118  	} else {
119  		extent_unlock(tsdn, extent1);
120  		return lock_result_failure;
121  	}
122  }
123  static extent_t *
</pre></code></div>
                <div class="column column_space"><pre><code>252      return 1;
253    } else {
254      return 0;
255    }
256  }
257  static int GetHistoBits(int method, int use_palette, int width, int height) {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    