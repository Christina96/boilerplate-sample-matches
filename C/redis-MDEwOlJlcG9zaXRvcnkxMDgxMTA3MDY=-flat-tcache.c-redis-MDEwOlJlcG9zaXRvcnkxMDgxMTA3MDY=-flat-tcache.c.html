
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 15, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-tcache.c</h3>
            <pre><code>1  #define JEMALLOC_TCACHE_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/mutex.h"
6  #include "jemalloc/internal/safety_check.h"
7  #include "jemalloc/internal/sc.h"
8  bool	opt_tcache = true;
9  ssize_t	opt_lg_tcache_max = LG_TCACHE_MAXCLASS_DEFAULT;
10  cache_bin_info_t	*tcache_bin_info;
11  static unsigned		stack_nelms; &bsol;* Total stack elms per tcache. */
12  unsigned		nhbins;
13  size_t			tcache_maxclass;
14  tcaches_t		*tcaches;
15  static unsigned		tcaches_past;
16  static tcaches_t	*tcaches_avail;
17  static malloc_mutex_t	tcaches_mtx;
18  size_t
19  tcache_salloc(tsdn_t *tsdn, const void *ptr) {
20  	return arena_salloc(tsdn, ptr);
21  }
22  void
23  tcache_event_hard(tsd_t *tsd, tcache_t *tcache) {
24  	szind_t binind = tcache->next_gc_bin;
25  	cache_bin_t *tbin;
26  	if (binind < SC_NBINS) {
27  		tbin = tcache_small_bin_get(tcache, binind);
28  	} else {
29  		tbin = tcache_large_bin_get(tcache, binind);
30  	}
31  	if (tbin->low_water > 0) {
32  		if (binind < SC_NBINS) {
33  			tcache_bin_flush_small(tsd, tcache, tbin, binind,
34  			    tbin->ncached - tbin->low_water + (tbin->low_water
35  			    >> 2));
36  			cache_bin_info_t *tbin_info = &tcache_bin_info[binind];
37  			if ((tbin_info->ncached_max >>
38  			     (tcache->lg_fill_div[binind] + 1)) >= 1) {
39  				tcache->lg_fill_div[binind]++;
40  			}
41  		} else {
42  			tcache_bin_flush_large(tsd, tbin, binind, tbin->ncached
43  			    - tbin->low_water + (tbin->low_water >> 2), tcache);
44  		}
45  	} else if (tbin->low_water < 0) {
46  		if (binind < SC_NBINS && tcache->lg_fill_div[binind] > 1) {
47  			tcache->lg_fill_div[binind]--;
48  		}
49  	}
50  	tbin->low_water = tbin->ncached;
51  	tcache->next_gc_bin++;
52  	if (tcache->next_gc_bin == nhbins) {
53  		tcache->next_gc_bin = 0;
54  	}
55  }
56  void *
57  tcache_alloc_small_hard(tsdn_t *tsdn, arena_t *arena, tcache_t *tcache,
58      cache_bin_t *tbin, szind_t binind, bool *tcache_success) {
59  	void *ret;
60  	assert(tcache->arena != NULL);
61  	arena_tcache_fill_small(tsdn, arena, tcache, tbin, binind,
62  	    config_prof ? tcache->prof_accumbytes : 0);
63  	if (config_prof) {
64  		tcache->prof_accumbytes = 0;
65  	}
66  	ret = cache_bin_alloc_easy(tbin, tcache_success);
67  	return ret;
68  }
69  static void
70  tbin_extents_lookup_size_check(tsdn_t *tsdn, cache_bin_t *tbin, szind_t binind,
71      size_t nflush, extent_t **extents){
72  	rtree_ctx_t rtree_ctx_fallback;
73  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
74  	szind_t szind;
75  	size_t sz_sum = binind * nflush;
76  	for (unsigned i = 0 ; i < nflush; i++) {
77  		rtree_extent_szind_read(tsdn, &extents_rtree,
78  		    rtree_ctx, (uintptr_t)*(tbin->avail - 1 - i), true,
79  		    &extents[i], &szind);
80  		sz_sum -= szind;
81  	}
82  	if (sz_sum != 0) {
83  		safety_check_fail("<jemalloc>: size mismatch in thread cache "
84  		    "detected, likely caused by sized deallocation bugs by "
85  		    "application. Abort.\n");
86  		abort();
87  	}
88  }
89  void
90  tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *tbin,
91      szind_t binind, unsigned rem) {
92  	bool merged_stats = false;
93  	assert(binind < SC_NBINS);
94  	assert((cache_bin_sz_t)rem <= tbin->ncached);
95  	arena_t *arena = tcache->arena;
96  	assert(arena != NULL);
97  	unsigned nflush = tbin->ncached - rem;
98  	VARIABLE_ARRAY(extent_t *, item_extent, nflush);
99  	if (config_opt_safety_checks) {
100  		tbin_extents_lookup_size_check(tsd_tsdn(tsd), tbin, binind,
101  		    nflush, item_extent);
102  	} else {
103  		for (unsigned i = 0 ; i < nflush; i++) {
104  			item_extent[i] = iealloc(tsd_tsdn(tsd),
105  			    *(tbin->avail - 1 - i));
106  		}
107  	}
108  	while (nflush > 0) {
109  		extent_t *extent = item_extent[0];
110  		unsigned bin_arena_ind = extent_arena_ind_get(extent);
111  		arena_t *bin_arena = arena_get(tsd_tsdn(tsd), bin_arena_ind,
112  		    false);
113  		unsigned binshard = extent_binshard_get(extent);
114  		assert(binshard < bin_infos[binind].n_shards);
115  		bin_t *bin = &bin_arena->bins[binind].bin_shards[binshard];
116  		if (config_prof && bin_arena == arena) {
117  			if (arena_prof_accum(tsd_tsdn(tsd), arena,
118  			    tcache->prof_accumbytes)) {
119  				prof_idump(tsd_tsdn(tsd));
120  			}
121  			tcache->prof_accumbytes = 0;
122  		}
123  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
124  		if (config_stats && bin_arena == arena && !merged_stats) {
125  			merged_stats = true;
126  			bin->stats.nflushes++;
127  			bin->stats.nrequests += tbin->tstats.nrequests;
128  			tbin->tstats.nrequests = 0;
129  		}
130  		unsigned ndeferred = 0;
131  		for (unsigned i = 0; i < nflush; i++) {
132  			void *ptr = *(tbin->avail - 1 - i);
133  			extent = item_extent[i];
134  			assert(ptr != NULL && extent != NULL);
135  			if (extent_arena_ind_get(extent) == bin_arena_ind
136  			    && extent_binshard_get(extent) == binshard) {
137  				arena_dalloc_bin_junked_locked(tsd_tsdn(tsd),
138  				    bin_arena, bin, binind, extent, ptr);
139  			} else {
140  				*(tbin->avail - 1 - ndeferred) = ptr;
141  				item_extent[ndeferred] = extent;
142  				ndeferred++;
143  			}
144  		}
145  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
146  		arena_decay_ticks(tsd_tsdn(tsd), bin_arena, nflush - ndeferred);
147  		nflush = ndeferred;
148  	}
149  	if (config_stats && !merged_stats) {
150  		unsigned binshard;
151  		bin_t *bin = arena_bin_choose_lock(tsd_tsdn(tsd), arena, binind,
152  		    &binshard);
153  		bin->stats.nflushes++;
154  		bin->stats.nrequests += tbin->tstats.nrequests;
155  		tbin->tstats.nrequests = 0;
156  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
157  	}
158  	memmove(tbin->avail - rem, tbin->avail - tbin->ncached, rem *
159  	    sizeof(void *));
160  	tbin->ncached = rem;
161  	if (tbin->ncached < tbin->low_water) {
162  		tbin->low_water = tbin->ncached;
163  	}
164  }
165  void
166  tcache_bin_flush_large(tsd_t *tsd, cache_bin_t *tbin, szind_t binind,
167      unsigned rem, tcache_t *tcache) {
168  	bool merged_stats = false;
169  	assert(binind < nhbins);
170  	assert((cache_bin_sz_t)rem <= tbin->ncached);
171  	arena_t *tcache_arena = tcache->arena;
172  	assert(tcache_arena != NULL);
173  	unsigned nflush = tbin->ncached - rem;
174  	VARIABLE_ARRAY(extent_t *, item_extent, nflush);
175  #ifndef JEMALLOC_EXTRA_SIZE_CHECK
176  	for (unsigned i = 0 ; i < nflush; i++) {
177  		item_extent[i] = iealloc(tsd_tsdn(tsd), *(tbin->avail - 1 - i));
178  	}
179  #else
180  	tbin_extents_lookup_size_check(tsd_tsdn(tsd), tbin, binind, nflush,
181  	    item_extent);
182  #endif
183  	while (nflush > 0) {
184  		extent_t *extent = item_extent[0];
185  		unsigned locked_arena_ind = extent_arena_ind_get(extent);
186  		arena_t *locked_arena = arena_get(tsd_tsdn(tsd),
187  		    locked_arena_ind, false);
188  		bool idump;
189  		if (config_prof) {
190  			idump = false;
191  		}
192  		bool lock_large = !arena_is_auto(locked_arena);
193  		if (lock_large) {
194  			malloc_mutex_lock(tsd_tsdn(tsd), &locked_arena->large_mtx);
195  		}
196  		for (unsigned i = 0; i < nflush; i++) {
197  			void *ptr = *(tbin->avail - 1 - i);
198  			assert(ptr != NULL);
199  			extent = item_extent[i];
200  			if (extent_arena_ind_get(extent) == locked_arena_ind) {
201  				large_dalloc_prep_junked_locked(tsd_tsdn(tsd),
202  				    extent);
203  			}
204  		}
205  		if ((config_prof || config_stats) &&
206  		    (locked_arena == tcache_arena)) {
207  			if (config_prof) {
208  				idump = arena_prof_accum(tsd_tsdn(tsd),
209  				    tcache_arena, tcache->prof_accumbytes);
210  				tcache->prof_accumbytes = 0;
211  			}
212  			if (config_stats) {
213  				merged_stats = true;
214  				arena_stats_large_flush_nrequests_add(
215  				    tsd_tsdn(tsd), &tcache_arena->stats, binind,
216  				    tbin->tstats.nrequests);
217  				tbin->tstats.nrequests = 0;
218  			}
219  		}
220  		if (lock_large) {
221  			malloc_mutex_unlock(tsd_tsdn(tsd), &locked_arena->large_mtx);
222  		}
223  		unsigned ndeferred = 0;
224  		for (unsigned i = 0; i < nflush; i++) {
225  			void *ptr = *(tbin->avail - 1 - i);
226  			extent = item_extent[i];
227  			assert(ptr != NULL && extent != NULL);
228  			if (extent_arena_ind_get(extent) == locked_arena_ind) {
229  				large_dalloc_finish(tsd_tsdn(tsd), extent);
230  			} else {
231  				*(tbin->avail - 1 - ndeferred) = ptr;
232  				item_extent[ndeferred] = extent;
233  				ndeferred++;
234  			}
235  		}
236  		if (config_prof && idump) {
237  			prof_idump(tsd_tsdn(tsd));
238  		}
239  		arena_decay_ticks(tsd_tsdn(tsd), locked_arena, nflush -
240  		    ndeferred);
241  		nflush = ndeferred;
242  	}
243  	if (config_stats && !merged_stats) {
244  		arena_stats_large_flush_nrequests_add(tsd_tsdn(tsd),
245  		    &tcache_arena->stats, binind, tbin->tstats.nrequests);
246  		tbin->tstats.nrequests = 0;
247  	}
248  	memmove(tbin->avail - rem, tbin->avail - tbin->ncached, rem *
249  	    sizeof(void *));
250  	tbin->ncached = rem;
251  	if (tbin->ncached < tbin->low_water) {
252  		tbin->low_water = tbin->ncached;
253  	}
254  }
255  void
256  tcache_arena_associate(tsdn_t *tsdn, tcache_t *tcache, arena_t *arena) {
257  	assert(tcache->arena == NULL);
258  	tcache->arena = arena;
259  	if (config_stats) {
260  		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
261  		ql_elm_new(tcache, link);
262  		ql_tail_insert(&arena->tcache_ql, tcache, link);
263  		cache_bin_array_descriptor_init(
264  		    &tcache->cache_bin_array_descriptor, tcache->bins_small,
265  		    tcache->bins_large);
266  		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
267  		    &tcache->cache_bin_array_descriptor, link);
268  		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
269  	}
270  }
271  static void
272  tcache_arena_dissociate(tsdn_t *tsdn, tcache_t *tcache) {
273  	arena_t *arena = tcache->arena;
274  	assert(arena != NULL);
275  	if (config_stats) {
276  		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
277  		if (config_debug) {
278  			bool in_ql = false;
279  			tcache_t *iter;
280  			ql_foreach(iter, &arena->tcache_ql, link) {
<span onclick='openModal()' class='match'>281  				if (iter == tcache) {
282  					in_ql = true;
283  					break;
284  				}
285  			}
286  			assert(in_ql);
</span>287  		}
288  		ql_remove(&arena->tcache_ql, tcache, link);
289  		ql_remove(&arena->cache_bin_array_descriptor_ql,
290  		    &tcache->cache_bin_array_descriptor, link);
291  		tcache_stats_merge(tsdn, tcache, arena);
292  		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
293  	}
294  	tcache->arena = NULL;
295  }
296  void
297  tcache_arena_reassociate(tsdn_t *tsdn, tcache_t *tcache, arena_t *arena) {
298  	tcache_arena_dissociate(tsdn, tcache);
299  	tcache_arena_associate(tsdn, tcache, arena);
300  }
301  bool
302  tsd_tcache_enabled_data_init(tsd_t *tsd) {
303  	tsd_tcache_enabled_set(tsd, opt_tcache);
304  	tsd_slow_update(tsd);
305  	if (opt_tcache) {
306  		tsd_tcache_data_init(tsd);
307  	}
308  	return false;
309  }
310  static void
311  tcache_init(tsd_t *tsd, tcache_t *tcache, void *avail_stack) {
312  	memset(&tcache->link, 0, sizeof(ql_elm(tcache_t)));
313  	tcache->prof_accumbytes = 0;
314  	tcache->next_gc_bin = 0;
315  	tcache->arena = NULL;
316  	ticker_init(&tcache->gc_ticker, TCACHE_GC_INCR);
317  	size_t stack_offset = 0;
318  	assert((TCACHE_NSLOTS_SMALL_MAX & 1U) == 0);
319  	memset(tcache->bins_small, 0, sizeof(cache_bin_t) * SC_NBINS);
320  	memset(tcache->bins_large, 0, sizeof(cache_bin_t) * (nhbins - SC_NBINS));
321  	unsigned i = 0;
322  	for (; i < SC_NBINS; i++) {
323  		tcache->lg_fill_div[i] = 1;
324  		stack_offset += tcache_bin_info[i].ncached_max * sizeof(void *);
325  		tcache_small_bin_get(tcache, i)->avail =
326  		    (void **)((uintptr_t)avail_stack + (uintptr_t)stack_offset);
327  	}
328  	for (; i < nhbins; i++) {
329  		stack_offset += tcache_bin_info[i].ncached_max * sizeof(void *);
330  		tcache_large_bin_get(tcache, i)->avail =
331  		    (void **)((uintptr_t)avail_stack + (uintptr_t)stack_offset);
332  	}
333  	assert(stack_offset == stack_nelms * sizeof(void *));
334  }
335  bool
336  tsd_tcache_data_init(tsd_t *tsd) {
337  	tcache_t *tcache = tsd_tcachep_get_unsafe(tsd);
338  	assert(tcache_small_bin_get(tcache, 0)->avail == NULL);
339  	size_t size = stack_nelms * sizeof(void *);
340  	size = sz_sa2u(size, CACHELINE);
341  	void *avail_array = ipallocztm(tsd_tsdn(tsd), size, CACHELINE, true,
342  	    NULL, true, arena_get(TSDN_NULL, 0, true));
343  	if (avail_array == NULL) {
344  		return true;
345  	}
346  	tcache_init(tsd, tcache, avail_array);
347  	tcache->arena = NULL;
348  	arena_t *arena;
349  	if (!malloc_initialized()) {
350  		arena = arena_get(tsd_tsdn(tsd), 0, false);
351  		tcache_arena_associate(tsd_tsdn(tsd), tcache, arena);
352  	} else {
353  		arena = arena_choose(tsd, NULL);
354  		if (tcache->arena == NULL) {
355  			tcache_arena_associate(tsd_tsdn(tsd), tcache, arena);
356  		}
357  	}
358  	assert(arena == tcache->arena);
359  	return false;
360  }
361  tcache_t *
362  tcache_create_explicit(tsd_t *tsd) {
363  	tcache_t *tcache;
364  	size_t size, stack_offset;
365  	size = sizeof(tcache_t);
366  	size = PTR_CEILING(size);
367  	stack_offset = size;
368  	size += stack_nelms * sizeof(void *);
369  	size = sz_sa2u(size, CACHELINE);
370  	tcache = ipallocztm(tsd_tsdn(tsd), size, CACHELINE, true, NULL, true,
371  	    arena_get(TSDN_NULL, 0, true));
372  	if (tcache == NULL) {
373  		return NULL;
374  	}
375  	tcache_init(tsd, tcache,
376  	    (void *)((uintptr_t)tcache + (uintptr_t)stack_offset));
377  	tcache_arena_associate(tsd_tsdn(tsd), tcache, arena_ichoose(tsd, NULL));
378  	return tcache;
379  }
380  static void
381  tcache_flush_cache(tsd_t *tsd, tcache_t *tcache) {
382  	assert(tcache->arena != NULL);
383  	for (unsigned i = 0; i < SC_NBINS; i++) {
384  		cache_bin_t *tbin = tcache_small_bin_get(tcache, i);
385  		tcache_bin_flush_small(tsd, tcache, tbin, i, 0);
386  		if (config_stats) {
387  			assert(tbin->tstats.nrequests == 0);
388  		}
389  	}
390  	for (unsigned i = SC_NBINS; i < nhbins; i++) {
391  		cache_bin_t *tbin = tcache_large_bin_get(tcache, i);
392  		tcache_bin_flush_large(tsd, tbin, i, 0, tcache);
393  		if (config_stats) {
394  			assert(tbin->tstats.nrequests == 0);
395  		}
396  	}
397  	if (config_prof && tcache->prof_accumbytes > 0 &&
398  	    arena_prof_accum(tsd_tsdn(tsd), tcache->arena,
399  	    tcache->prof_accumbytes)) {
400  		prof_idump(tsd_tsdn(tsd));
401  	}
402  }
403  void
404  tcache_flush(tsd_t *tsd) {
405  	assert(tcache_available(tsd));
406  	tcache_flush_cache(tsd, tsd_tcachep_get(tsd));
407  }
408  static void
409  tcache_destroy(tsd_t *tsd, tcache_t *tcache, bool tsd_tcache) {
410  	tcache_flush_cache(tsd, tcache);
411  	arena_t *arena = tcache->arena;
412  	tcache_arena_dissociate(tsd_tsdn(tsd), tcache);
413  	if (tsd_tcache) {
414  		void *avail_array =
415  		    (void *)((uintptr_t)tcache_small_bin_get(tcache, 0)->avail -
416  		    (uintptr_t)tcache_bin_info[0].ncached_max * sizeof(void *));
417  		idalloctm(tsd_tsdn(tsd), avail_array, NULL, NULL, true, true);
418  	} else {
419  		idalloctm(tsd_tsdn(tsd), tcache, NULL, NULL, true, true);
420  	}
421  	arena_decay(tsd_tsdn(tsd), arena_get(tsd_tsdn(tsd), 0, false),
422  	    false, false);
423  	if (arena_nthreads_get(arena, false) == 0 &&
424  	    !background_thread_enabled()) {
425  		arena_decay(tsd_tsdn(tsd), arena, false, true);
426  	} else {
427  		arena_decay(tsd_tsdn(tsd), arena, false, false);
428  	}
429  }
430  void
431  tcache_cleanup(tsd_t *tsd) {
432  	tcache_t *tcache = tsd_tcachep_get(tsd);
433  	if (!tcache_available(tsd)) {
434  		assert(tsd_tcache_enabled_get(tsd) == false);
435  		if (config_debug) {
436  			assert(tcache_small_bin_get(tcache, 0)->avail == NULL);
437  		}
438  		return;
439  	}
440  	assert(tsd_tcache_enabled_get(tsd));
441  	assert(tcache_small_bin_get(tcache, 0)->avail != NULL);
442  	tcache_destroy(tsd, tcache, true);
443  	if (config_debug) {
444  		tcache_small_bin_get(tcache, 0)->avail = NULL;
445  	}
446  }
447  void
448  tcache_stats_merge(tsdn_t *tsdn, tcache_t *tcache, arena_t *arena) {
449  	unsigned i;
450  	cassert(config_stats);
451  	for (i = 0; i < SC_NBINS; i++) {
452  		cache_bin_t *tbin = tcache_small_bin_get(tcache, i);
453  		unsigned binshard;
454  		bin_t *bin = arena_bin_choose_lock(tsdn, arena, i, &binshard);
455  		bin->stats.nrequests += tbin->tstats.nrequests;
456  		malloc_mutex_unlock(tsdn, &bin->lock);
457  		tbin->tstats.nrequests = 0;
458  	}
459  	for (; i < nhbins; i++) {
460  		cache_bin_t *tbin = tcache_large_bin_get(tcache, i);
461  		arena_stats_large_flush_nrequests_add(tsdn, &arena->stats, i,
462  		    tbin->tstats.nrequests);
463  		tbin->tstats.nrequests = 0;
464  	}
465  }
466  static bool
467  tcaches_create_prep(tsd_t *tsd) {
468  	bool err;
469  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
470  	if (tcaches == NULL) {
471  		tcaches = base_alloc(tsd_tsdn(tsd), b0get(), sizeof(tcache_t *)
472  		    * (MALLOCX_TCACHE_MAX+1), CACHELINE);
473  		if (tcaches == NULL) {
474  			err = true;
475  			goto label_return;
476  		}
477  	}
478  	if (tcaches_avail == NULL && tcaches_past > MALLOCX_TCACHE_MAX) {
479  		err = true;
480  		goto label_return;
481  	}
482  	err = false;
483  label_return:
484  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
485  	return err;
486  }
487  bool
488  tcaches_create(tsd_t *tsd, unsigned *r_ind) {
489  	witness_assert_depth(tsdn_witness_tsdp_get(tsd_tsdn(tsd)), 0);
490  	bool err;
491  	if (tcaches_create_prep(tsd)) {
492  		err = true;
493  		goto label_return;
494  	}
495  	tcache_t *tcache = tcache_create_explicit(tsd);
496  	if (tcache == NULL) {
497  		err = true;
498  		goto label_return;
499  	}
500  	tcaches_t *elm;
501  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
502  	if (tcaches_avail != NULL) {
503  		elm = tcaches_avail;
504  		tcaches_avail = tcaches_avail->next;
505  		elm->tcache = tcache;
506  		*r_ind = (unsigned)(elm - tcaches);
507  	} else {
508  		elm = &tcaches[tcaches_past];
509  		elm->tcache = tcache;
510  		*r_ind = tcaches_past;
511  		tcaches_past++;
512  	}
513  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
514  	err = false;
515  label_return:
516  	witness_assert_depth(tsdn_witness_tsdp_get(tsd_tsdn(tsd)), 0);
517  	return err;
518  }
519  static tcache_t *
520  tcaches_elm_remove(tsd_t *tsd, tcaches_t *elm, bool allow_reinit) {
521  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &tcaches_mtx);
522  	if (elm->tcache == NULL) {
523  		return NULL;
524  	}
525  	tcache_t *tcache = elm->tcache;
526  	if (allow_reinit) {
527  		elm->tcache = TCACHES_ELM_NEED_REINIT;
528  	} else {
529  		elm->tcache = NULL;
530  	}
531  	if (tcache == TCACHES_ELM_NEED_REINIT) {
532  		return NULL;
533  	}
534  	return tcache;
535  }
536  void
537  tcaches_flush(tsd_t *tsd, unsigned ind) {
538  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
539  	tcache_t *tcache = tcaches_elm_remove(tsd, &tcaches[ind], true);
540  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
541  	if (tcache != NULL) {
542  		tcache_destroy(tsd, tcache, false);
543  	}
544  }
545  void
546  tcaches_destroy(tsd_t *tsd, unsigned ind) {
547  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
548  	tcaches_t *elm = &tcaches[ind];
549  	tcache_t *tcache = tcaches_elm_remove(tsd, elm, false);
550  	elm->next = tcaches_avail;
551  	tcaches_avail = elm;
552  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
553  	if (tcache != NULL) {
554  		tcache_destroy(tsd, tcache, false);
555  	}
556  }
557  bool
558  tcache_boot(tsdn_t *tsdn) {
559  	if (opt_lg_tcache_max < 0 || (ZU(1) << opt_lg_tcache_max) <
560  	    SC_SMALL_MAXCLASS) {
561  		tcache_maxclass = SC_SMALL_MAXCLASS;
562  	} else {
563  		tcache_maxclass = (ZU(1) << opt_lg_tcache_max);
564  	}
565  	if (malloc_mutex_init(&tcaches_mtx, "tcaches", WITNESS_RANK_TCACHES,
566  	    malloc_mutex_rank_exclusive)) {
567  		return true;
568  	}
569  	nhbins = sz_size2index(tcache_maxclass) + 1;
570  	tcache_bin_info = (cache_bin_info_t *)base_alloc(tsdn, b0get(), nhbins
571  	    * sizeof(cache_bin_info_t), CACHELINE);
572  	if (tcache_bin_info == NULL) {
573  		return true;
574  	}
575  	stack_nelms = 0;
576  	unsigned i;
577  	for (i = 0; i < SC_NBINS; i++) {
578  		if ((bin_infos[i].nregs << 1) <= TCACHE_NSLOTS_SMALL_MIN) {
579  			tcache_bin_info[i].ncached_max =
580  			    TCACHE_NSLOTS_SMALL_MIN;
581  		} else if ((bin_infos[i].nregs << 1) <=
582  		    TCACHE_NSLOTS_SMALL_MAX) {
583  			tcache_bin_info[i].ncached_max =
584  			    (bin_infos[i].nregs << 1);
585  		} else {
586  			tcache_bin_info[i].ncached_max =
587  			    TCACHE_NSLOTS_SMALL_MAX;
588  		}
589  		stack_nelms += tcache_bin_info[i].ncached_max;
590  	}
591  	for (; i < nhbins; i++) {
592  		tcache_bin_info[i].ncached_max = TCACHE_NSLOTS_LARGE;
593  		stack_nelms += tcache_bin_info[i].ncached_max;
594  	}
595  	return false;
596  }
597  void
598  tcache_prefork(tsdn_t *tsdn) {
599  	if (!config_prof && opt_tcache) {
600  		malloc_mutex_prefork(tsdn, &tcaches_mtx);
601  	}
602  }
603  void
604  tcache_postfork_parent(tsdn_t *tsdn) {
605  	if (!config_prof && opt_tcache) {
606  		malloc_mutex_postfork_parent(tsdn, &tcaches_mtx);
607  	}
608  }
609  void
610  tcache_postfork_child(tsdn_t *tsdn) {
611  	if (!config_prof && opt_tcache) {
612  		malloc_mutex_postfork_child(tsdn, &tcaches_mtx);
613  	}
614  }
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-tcache.c</h3>
            <pre><code>1  #define JEMALLOC_TCACHE_C_
2  #include "jemalloc/internal/jemalloc_preamble.h"
3  #include "jemalloc/internal/jemalloc_internal_includes.h"
4  #include "jemalloc/internal/assert.h"
5  #include "jemalloc/internal/mutex.h"
6  #include "jemalloc/internal/safety_check.h"
7  #include "jemalloc/internal/sc.h"
8  bool	opt_tcache = true;
9  ssize_t	opt_lg_tcache_max = LG_TCACHE_MAXCLASS_DEFAULT;
10  cache_bin_info_t	*tcache_bin_info;
11  static unsigned		stack_nelms; &bsol;* Total stack elms per tcache. */
12  unsigned		nhbins;
13  size_t			tcache_maxclass;
14  tcaches_t		*tcaches;
15  static unsigned		tcaches_past;
16  static tcaches_t	*tcaches_avail;
17  static malloc_mutex_t	tcaches_mtx;
18  size_t
19  tcache_salloc(tsdn_t *tsdn, const void *ptr) {
20  	return arena_salloc(tsdn, ptr);
21  }
22  void
23  tcache_event_hard(tsd_t *tsd, tcache_t *tcache) {
24  	szind_t binind = tcache->next_gc_bin;
25  	cache_bin_t *tbin;
26  	if (binind < SC_NBINS) {
27  		tbin = tcache_small_bin_get(tcache, binind);
28  	} else {
29  		tbin = tcache_large_bin_get(tcache, binind);
30  	}
31  	if (tbin->low_water > 0) {
32  		if (binind < SC_NBINS) {
33  			tcache_bin_flush_small(tsd, tcache, tbin, binind,
34  			    tbin->ncached - tbin->low_water + (tbin->low_water
35  			    >> 2));
36  			cache_bin_info_t *tbin_info = &tcache_bin_info[binind];
37  			if ((tbin_info->ncached_max >>
38  			     (tcache->lg_fill_div[binind] + 1)) >= 1) {
39  				tcache->lg_fill_div[binind]++;
40  			}
41  		} else {
42  			tcache_bin_flush_large(tsd, tbin, binind, tbin->ncached
43  			    - tbin->low_water + (tbin->low_water >> 2), tcache);
44  		}
45  	} else if (tbin->low_water < 0) {
46  		if (binind < SC_NBINS && tcache->lg_fill_div[binind] > 1) {
47  			tcache->lg_fill_div[binind]--;
48  		}
49  	}
50  	tbin->low_water = tbin->ncached;
51  	tcache->next_gc_bin++;
52  	if (tcache->next_gc_bin == nhbins) {
53  		tcache->next_gc_bin = 0;
54  	}
55  }
56  void *
57  tcache_alloc_small_hard(tsdn_t *tsdn, arena_t *arena, tcache_t *tcache,
58      cache_bin_t *tbin, szind_t binind, bool *tcache_success) {
59  	void *ret;
60  	assert(tcache->arena != NULL);
61  	arena_tcache_fill_small(tsdn, arena, tcache, tbin, binind,
62  	    config_prof ? tcache->prof_accumbytes : 0);
63  	if (config_prof) {
64  		tcache->prof_accumbytes = 0;
65  	}
66  	ret = cache_bin_alloc_easy(tbin, tcache_success);
67  	return ret;
68  }
69  static void
70  tbin_extents_lookup_size_check(tsdn_t *tsdn, cache_bin_t *tbin, szind_t binind,
71      size_t nflush, extent_t **extents){
72  	rtree_ctx_t rtree_ctx_fallback;
73  	rtree_ctx_t *rtree_ctx = tsdn_rtree_ctx(tsdn, &rtree_ctx_fallback);
74  	szind_t szind;
75  	size_t sz_sum = binind * nflush;
76  	for (unsigned i = 0 ; i < nflush; i++) {
77  		rtree_extent_szind_read(tsdn, &extents_rtree,
78  		    rtree_ctx, (uintptr_t)*(tbin->avail - 1 - i), true,
79  		    &extents[i], &szind);
80  		sz_sum -= szind;
81  	}
82  	if (sz_sum != 0) {
83  		safety_check_fail("<jemalloc>: size mismatch in thread cache "
84  		    "detected, likely caused by sized deallocation bugs by "
85  		    "application. Abort.\n");
86  		abort();
87  	}
88  }
89  void
90  tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *tbin,
91      szind_t binind, unsigned rem) {
92  	bool merged_stats = false;
93  	assert(binind < SC_NBINS);
94  	assert((cache_bin_sz_t)rem <= tbin->ncached);
95  	arena_t *arena = tcache->arena;
96  	assert(arena != NULL);
97  	unsigned nflush = tbin->ncached - rem;
98  	VARIABLE_ARRAY(extent_t *, item_extent, nflush);
99  	if (config_opt_safety_checks) {
100  		tbin_extents_lookup_size_check(tsd_tsdn(tsd), tbin, binind,
101  		    nflush, item_extent);
102  	} else {
103  		for (unsigned i = 0 ; i < nflush; i++) {
104  			item_extent[i] = iealloc(tsd_tsdn(tsd),
105  			    *(tbin->avail - 1 - i));
106  		}
107  	}
108  	while (nflush > 0) {
109  		extent_t *extent = item_extent[0];
110  		unsigned bin_arena_ind = extent_arena_ind_get(extent);
111  		arena_t *bin_arena = arena_get(tsd_tsdn(tsd), bin_arena_ind,
112  		    false);
113  		unsigned binshard = extent_binshard_get(extent);
114  		assert(binshard < bin_infos[binind].n_shards);
115  		bin_t *bin = &bin_arena->bins[binind].bin_shards[binshard];
116  		if (config_prof && bin_arena == arena) {
117  			if (arena_prof_accum(tsd_tsdn(tsd), arena,
118  			    tcache->prof_accumbytes)) {
119  				prof_idump(tsd_tsdn(tsd));
120  			}
121  			tcache->prof_accumbytes = 0;
122  		}
123  		malloc_mutex_lock(tsd_tsdn(tsd), &bin->lock);
124  		if (config_stats && bin_arena == arena && !merged_stats) {
125  			merged_stats = true;
126  			bin->stats.nflushes++;
127  			bin->stats.nrequests += tbin->tstats.nrequests;
128  			tbin->tstats.nrequests = 0;
129  		}
130  		unsigned ndeferred = 0;
131  		for (unsigned i = 0; i < nflush; i++) {
132  			void *ptr = *(tbin->avail - 1 - i);
133  			extent = item_extent[i];
134  			assert(ptr != NULL && extent != NULL);
135  			if (extent_arena_ind_get(extent) == bin_arena_ind
136  			    && extent_binshard_get(extent) == binshard) {
137  				arena_dalloc_bin_junked_locked(tsd_tsdn(tsd),
138  				    bin_arena, bin, binind, extent, ptr);
139  			} else {
140  				*(tbin->avail - 1 - ndeferred) = ptr;
141  				item_extent[ndeferred] = extent;
142  				ndeferred++;
143  			}
144  		}
145  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
146  		arena_decay_ticks(tsd_tsdn(tsd), bin_arena, nflush - ndeferred);
147  		nflush = ndeferred;
148  	}
149  	if (config_stats && !merged_stats) {
150  		unsigned binshard;
151  		bin_t *bin = arena_bin_choose_lock(tsd_tsdn(tsd), arena, binind,
152  		    &binshard);
153  		bin->stats.nflushes++;
154  		bin->stats.nrequests += tbin->tstats.nrequests;
155  		tbin->tstats.nrequests = 0;
156  		malloc_mutex_unlock(tsd_tsdn(tsd), &bin->lock);
157  	}
158  	memmove(tbin->avail - rem, tbin->avail - tbin->ncached, rem *
159  	    sizeof(void *));
160  	tbin->ncached = rem;
161  	if (tbin->ncached < tbin->low_water) {
162  		tbin->low_water = tbin->ncached;
163  	}
164  }
165  void
166  tcache_bin_flush_large(tsd_t *tsd, cache_bin_t *tbin, szind_t binind,
167      unsigned rem, tcache_t *tcache) {
168  	bool merged_stats = false;
169  	assert(binind < nhbins);
170  	assert((cache_bin_sz_t)rem <= tbin->ncached);
171  	arena_t *tcache_arena = tcache->arena;
172  	assert(tcache_arena != NULL);
173  	unsigned nflush = tbin->ncached - rem;
174  	VARIABLE_ARRAY(extent_t *, item_extent, nflush);
175  #ifndef JEMALLOC_EXTRA_SIZE_CHECK
176  	for (unsigned i = 0 ; i < nflush; i++) {
177  		item_extent[i] = iealloc(tsd_tsdn(tsd), *(tbin->avail - 1 - i));
178  	}
179  #else
180  	tbin_extents_lookup_size_check(tsd_tsdn(tsd), tbin, binind, nflush,
181  	    item_extent);
182  #endif
183  	while (nflush > 0) {
184  		extent_t *extent = item_extent[0];
185  		unsigned locked_arena_ind = extent_arena_ind_get(extent);
186  		arena_t *locked_arena = arena_get(tsd_tsdn(tsd),
187  		    locked_arena_ind, false);
188  		bool idump;
189  		if (config_prof) {
190  			idump = false;
191  		}
192  		bool lock_large = !arena_is_auto(locked_arena);
193  		if (lock_large) {
194  			malloc_mutex_lock(tsd_tsdn(tsd), &locked_arena->large_mtx);
195  		}
196  		for (unsigned i = 0; i < nflush; i++) {
197  			void *ptr = *(tbin->avail - 1 - i);
198  			assert(ptr != NULL);
199  			extent = item_extent[i];
200  			if (extent_arena_ind_get(extent) == locked_arena_ind) {
201  				large_dalloc_prep_junked_locked(tsd_tsdn(tsd),
202  				    extent);
203  			}
204  		}
205  		if ((config_prof || config_stats) &&
206  		    (locked_arena == tcache_arena)) {
207  			if (config_prof) {
208  				idump = arena_prof_accum(tsd_tsdn(tsd),
209  				    tcache_arena, tcache->prof_accumbytes);
210  				tcache->prof_accumbytes = 0;
211  			}
212  			if (config_stats) {
213  				merged_stats = true;
214  				arena_stats_large_flush_nrequests_add(
215  				    tsd_tsdn(tsd), &tcache_arena->stats, binind,
216  				    tbin->tstats.nrequests);
217  				tbin->tstats.nrequests = 0;
218  			}
219  		}
220  		if (lock_large) {
221  			malloc_mutex_unlock(tsd_tsdn(tsd), &locked_arena->large_mtx);
222  		}
223  		unsigned ndeferred = 0;
224  		for (unsigned i = 0; i < nflush; i++) {
225  			void *ptr = *(tbin->avail - 1 - i);
226  			extent = item_extent[i];
227  			assert(ptr != NULL && extent != NULL);
228  			if (extent_arena_ind_get(extent) == locked_arena_ind) {
229  				large_dalloc_finish(tsd_tsdn(tsd), extent);
230  			} else {
231  				*(tbin->avail - 1 - ndeferred) = ptr;
232  				item_extent[ndeferred] = extent;
233  				ndeferred++;
234  			}
235  		}
236  		if (config_prof && idump) {
237  			prof_idump(tsd_tsdn(tsd));
238  		}
239  		arena_decay_ticks(tsd_tsdn(tsd), locked_arena, nflush -
240  		    ndeferred);
241  		nflush = ndeferred;
242  	}
243  	if (config_stats && !merged_stats) {
244  		arena_stats_large_flush_nrequests_add(tsd_tsdn(tsd),
245  		    &tcache_arena->stats, binind, tbin->tstats.nrequests);
246  		tbin->tstats.nrequests = 0;
247  	}
248  	memmove(tbin->avail - rem, tbin->avail - tbin->ncached, rem *
249  	    sizeof(void *));
250  	tbin->ncached = rem;
251  	if (tbin->ncached < tbin->low_water) {
252  		tbin->low_water = tbin->ncached;
253  	}
254  }
255  void
256  tcache_arena_associate(tsdn_t *tsdn, tcache_t *tcache, arena_t *arena) {
257  	assert(tcache->arena == NULL);
258  	tcache->arena = arena;
259  	if (config_stats) {
260  		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
261  		ql_elm_new(tcache, link);
262  		ql_tail_insert(&arena->tcache_ql, tcache, link);
263  		cache_bin_array_descriptor_init(
264  		    &tcache->cache_bin_array_descriptor, tcache->bins_small,
265  		    tcache->bins_large);
266  		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
267  		    &tcache->cache_bin_array_descriptor, link);
268  		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
269  	}
270  }
271  static void
272  tcache_arena_dissociate(tsdn_t *tsdn, tcache_t *tcache) {
273  	arena_t *arena = tcache->arena;
274  	assert(arena != NULL);
275  	if (config_stats) {
276  		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);
277  		if (config_debug) {
278  			bool in_ql = false;
279  			tcache_t *iter;
280  			ql_foreach(iter, &arena->tcache_ql, link) {
<span onclick='openModal()' class='match'>281  				if (iter == tcache) {
282  					in_ql = true;
283  					break;
284  				}
285  			}
286  			assert(in_ql);
</span>287  		}
288  		ql_remove(&arena->tcache_ql, tcache, link);
289  		ql_remove(&arena->cache_bin_array_descriptor_ql,
290  		    &tcache->cache_bin_array_descriptor, link);
291  		tcache_stats_merge(tsdn, tcache, arena);
292  		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
293  	}
294  	tcache->arena = NULL;
295  }
296  void
297  tcache_arena_reassociate(tsdn_t *tsdn, tcache_t *tcache, arena_t *arena) {
298  	tcache_arena_dissociate(tsdn, tcache);
299  	tcache_arena_associate(tsdn, tcache, arena);
300  }
301  bool
302  tsd_tcache_enabled_data_init(tsd_t *tsd) {
303  	tsd_tcache_enabled_set(tsd, opt_tcache);
304  	tsd_slow_update(tsd);
305  	if (opt_tcache) {
306  		tsd_tcache_data_init(tsd);
307  	}
308  	return false;
309  }
310  static void
311  tcache_init(tsd_t *tsd, tcache_t *tcache, void *avail_stack) {
312  	memset(&tcache->link, 0, sizeof(ql_elm(tcache_t)));
313  	tcache->prof_accumbytes = 0;
314  	tcache->next_gc_bin = 0;
315  	tcache->arena = NULL;
316  	ticker_init(&tcache->gc_ticker, TCACHE_GC_INCR);
317  	size_t stack_offset = 0;
318  	assert((TCACHE_NSLOTS_SMALL_MAX & 1U) == 0);
319  	memset(tcache->bins_small, 0, sizeof(cache_bin_t) * SC_NBINS);
320  	memset(tcache->bins_large, 0, sizeof(cache_bin_t) * (nhbins - SC_NBINS));
321  	unsigned i = 0;
322  	for (; i < SC_NBINS; i++) {
323  		tcache->lg_fill_div[i] = 1;
324  		stack_offset += tcache_bin_info[i].ncached_max * sizeof(void *);
325  		tcache_small_bin_get(tcache, i)->avail =
326  		    (void **)((uintptr_t)avail_stack + (uintptr_t)stack_offset);
327  	}
328  	for (; i < nhbins; i++) {
329  		stack_offset += tcache_bin_info[i].ncached_max * sizeof(void *);
330  		tcache_large_bin_get(tcache, i)->avail =
331  		    (void **)((uintptr_t)avail_stack + (uintptr_t)stack_offset);
332  	}
333  	assert(stack_offset == stack_nelms * sizeof(void *));
334  }
335  bool
336  tsd_tcache_data_init(tsd_t *tsd) {
337  	tcache_t *tcache = tsd_tcachep_get_unsafe(tsd);
338  	assert(tcache_small_bin_get(tcache, 0)->avail == NULL);
339  	size_t size = stack_nelms * sizeof(void *);
340  	size = sz_sa2u(size, CACHELINE);
341  	void *avail_array = ipallocztm(tsd_tsdn(tsd), size, CACHELINE, true,
342  	    NULL, true, arena_get(TSDN_NULL, 0, true));
343  	if (avail_array == NULL) {
344  		return true;
345  	}
346  	tcache_init(tsd, tcache, avail_array);
347  	tcache->arena = NULL;
348  	arena_t *arena;
349  	if (!malloc_initialized()) {
350  		arena = arena_get(tsd_tsdn(tsd), 0, false);
351  		tcache_arena_associate(tsd_tsdn(tsd), tcache, arena);
352  	} else {
353  		arena = arena_choose(tsd, NULL);
354  		if (tcache->arena == NULL) {
355  			tcache_arena_associate(tsd_tsdn(tsd), tcache, arena);
356  		}
357  	}
358  	assert(arena == tcache->arena);
359  	return false;
360  }
361  tcache_t *
362  tcache_create_explicit(tsd_t *tsd) {
363  	tcache_t *tcache;
364  	size_t size, stack_offset;
365  	size = sizeof(tcache_t);
366  	size = PTR_CEILING(size);
367  	stack_offset = size;
368  	size += stack_nelms * sizeof(void *);
369  	size = sz_sa2u(size, CACHELINE);
370  	tcache = ipallocztm(tsd_tsdn(tsd), size, CACHELINE, true, NULL, true,
371  	    arena_get(TSDN_NULL, 0, true));
372  	if (tcache == NULL) {
373  		return NULL;
374  	}
375  	tcache_init(tsd, tcache,
376  	    (void *)((uintptr_t)tcache + (uintptr_t)stack_offset));
377  	tcache_arena_associate(tsd_tsdn(tsd), tcache, arena_ichoose(tsd, NULL));
378  	return tcache;
379  }
380  static void
381  tcache_flush_cache(tsd_t *tsd, tcache_t *tcache) {
382  	assert(tcache->arena != NULL);
383  	for (unsigned i = 0; i < SC_NBINS; i++) {
384  		cache_bin_t *tbin = tcache_small_bin_get(tcache, i);
385  		tcache_bin_flush_small(tsd, tcache, tbin, i, 0);
386  		if (config_stats) {
387  			assert(tbin->tstats.nrequests == 0);
388  		}
389  	}
390  	for (unsigned i = SC_NBINS; i < nhbins; i++) {
391  		cache_bin_t *tbin = tcache_large_bin_get(tcache, i);
392  		tcache_bin_flush_large(tsd, tbin, i, 0, tcache);
393  		if (config_stats) {
394  			assert(tbin->tstats.nrequests == 0);
395  		}
396  	}
397  	if (config_prof && tcache->prof_accumbytes > 0 &&
398  	    arena_prof_accum(tsd_tsdn(tsd), tcache->arena,
399  	    tcache->prof_accumbytes)) {
400  		prof_idump(tsd_tsdn(tsd));
401  	}
402  }
403  void
404  tcache_flush(tsd_t *tsd) {
405  	assert(tcache_available(tsd));
406  	tcache_flush_cache(tsd, tsd_tcachep_get(tsd));
407  }
408  static void
409  tcache_destroy(tsd_t *tsd, tcache_t *tcache, bool tsd_tcache) {
410  	tcache_flush_cache(tsd, tcache);
411  	arena_t *arena = tcache->arena;
412  	tcache_arena_dissociate(tsd_tsdn(tsd), tcache);
413  	if (tsd_tcache) {
414  		void *avail_array =
415  		    (void *)((uintptr_t)tcache_small_bin_get(tcache, 0)->avail -
416  		    (uintptr_t)tcache_bin_info[0].ncached_max * sizeof(void *));
417  		idalloctm(tsd_tsdn(tsd), avail_array, NULL, NULL, true, true);
418  	} else {
419  		idalloctm(tsd_tsdn(tsd), tcache, NULL, NULL, true, true);
420  	}
421  	arena_decay(tsd_tsdn(tsd), arena_get(tsd_tsdn(tsd), 0, false),
422  	    false, false);
423  	if (arena_nthreads_get(arena, false) == 0 &&
424  	    !background_thread_enabled()) {
425  		arena_decay(tsd_tsdn(tsd), arena, false, true);
426  	} else {
427  		arena_decay(tsd_tsdn(tsd), arena, false, false);
428  	}
429  }
430  void
431  tcache_cleanup(tsd_t *tsd) {
432  	tcache_t *tcache = tsd_tcachep_get(tsd);
433  	if (!tcache_available(tsd)) {
434  		assert(tsd_tcache_enabled_get(tsd) == false);
435  		if (config_debug) {
436  			assert(tcache_small_bin_get(tcache, 0)->avail == NULL);
437  		}
438  		return;
439  	}
440  	assert(tsd_tcache_enabled_get(tsd));
441  	assert(tcache_small_bin_get(tcache, 0)->avail != NULL);
442  	tcache_destroy(tsd, tcache, true);
443  	if (config_debug) {
444  		tcache_small_bin_get(tcache, 0)->avail = NULL;
445  	}
446  }
447  void
448  tcache_stats_merge(tsdn_t *tsdn, tcache_t *tcache, arena_t *arena) {
449  	unsigned i;
450  	cassert(config_stats);
451  	for (i = 0; i < SC_NBINS; i++) {
452  		cache_bin_t *tbin = tcache_small_bin_get(tcache, i);
453  		unsigned binshard;
454  		bin_t *bin = arena_bin_choose_lock(tsdn, arena, i, &binshard);
455  		bin->stats.nrequests += tbin->tstats.nrequests;
456  		malloc_mutex_unlock(tsdn, &bin->lock);
457  		tbin->tstats.nrequests = 0;
458  	}
459  	for (; i < nhbins; i++) {
460  		cache_bin_t *tbin = tcache_large_bin_get(tcache, i);
461  		arena_stats_large_flush_nrequests_add(tsdn, &arena->stats, i,
462  		    tbin->tstats.nrequests);
463  		tbin->tstats.nrequests = 0;
464  	}
465  }
466  static bool
467  tcaches_create_prep(tsd_t *tsd) {
468  	bool err;
469  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
470  	if (tcaches == NULL) {
471  		tcaches = base_alloc(tsd_tsdn(tsd), b0get(), sizeof(tcache_t *)
472  		    * (MALLOCX_TCACHE_MAX+1), CACHELINE);
473  		if (tcaches == NULL) {
474  			err = true;
475  			goto label_return;
476  		}
477  	}
478  	if (tcaches_avail == NULL && tcaches_past > MALLOCX_TCACHE_MAX) {
479  		err = true;
480  		goto label_return;
481  	}
482  	err = false;
483  label_return:
484  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
485  	return err;
486  }
487  bool
488  tcaches_create(tsd_t *tsd, unsigned *r_ind) {
489  	witness_assert_depth(tsdn_witness_tsdp_get(tsd_tsdn(tsd)), 0);
490  	bool err;
491  	if (tcaches_create_prep(tsd)) {
492  		err = true;
493  		goto label_return;
494  	}
495  	tcache_t *tcache = tcache_create_explicit(tsd);
496  	if (tcache == NULL) {
497  		err = true;
498  		goto label_return;
499  	}
500  	tcaches_t *elm;
501  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
502  	if (tcaches_avail != NULL) {
503  		elm = tcaches_avail;
504  		tcaches_avail = tcaches_avail->next;
505  		elm->tcache = tcache;
506  		*r_ind = (unsigned)(elm - tcaches);
507  	} else {
508  		elm = &tcaches[tcaches_past];
509  		elm->tcache = tcache;
510  		*r_ind = tcaches_past;
511  		tcaches_past++;
512  	}
513  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
514  	err = false;
515  label_return:
516  	witness_assert_depth(tsdn_witness_tsdp_get(tsd_tsdn(tsd)), 0);
517  	return err;
518  }
519  static tcache_t *
520  tcaches_elm_remove(tsd_t *tsd, tcaches_t *elm, bool allow_reinit) {
521  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &tcaches_mtx);
522  	if (elm->tcache == NULL) {
523  		return NULL;
524  	}
525  	tcache_t *tcache = elm->tcache;
526  	if (allow_reinit) {
527  		elm->tcache = TCACHES_ELM_NEED_REINIT;
528  	} else {
529  		elm->tcache = NULL;
530  	}
531  	if (tcache == TCACHES_ELM_NEED_REINIT) {
532  		return NULL;
533  	}
534  	return tcache;
535  }
536  void
537  tcaches_flush(tsd_t *tsd, unsigned ind) {
538  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
539  	tcache_t *tcache = tcaches_elm_remove(tsd, &tcaches[ind], true);
540  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
541  	if (tcache != NULL) {
542  		tcache_destroy(tsd, tcache, false);
543  	}
544  }
545  void
546  tcaches_destroy(tsd_t *tsd, unsigned ind) {
547  	malloc_mutex_lock(tsd_tsdn(tsd), &tcaches_mtx);
548  	tcaches_t *elm = &tcaches[ind];
549  	tcache_t *tcache = tcaches_elm_remove(tsd, elm, false);
550  	elm->next = tcaches_avail;
551  	tcaches_avail = elm;
552  	malloc_mutex_unlock(tsd_tsdn(tsd), &tcaches_mtx);
553  	if (tcache != NULL) {
554  		tcache_destroy(tsd, tcache, false);
555  	}
556  }
557  bool
558  tcache_boot(tsdn_t *tsdn) {
559  	if (opt_lg_tcache_max < 0 || (ZU(1) << opt_lg_tcache_max) <
560  	    SC_SMALL_MAXCLASS) {
561  		tcache_maxclass = SC_SMALL_MAXCLASS;
562  	} else {
563  		tcache_maxclass = (ZU(1) << opt_lg_tcache_max);
564  	}
565  	if (malloc_mutex_init(&tcaches_mtx, "tcaches", WITNESS_RANK_TCACHES,
566  	    malloc_mutex_rank_exclusive)) {
567  		return true;
568  	}
569  	nhbins = sz_size2index(tcache_maxclass) + 1;
570  	tcache_bin_info = (cache_bin_info_t *)base_alloc(tsdn, b0get(), nhbins
571  	    * sizeof(cache_bin_info_t), CACHELINE);
572  	if (tcache_bin_info == NULL) {
573  		return true;
574  	}
575  	stack_nelms = 0;
576  	unsigned i;
577  	for (i = 0; i < SC_NBINS; i++) {
578  		if ((bin_infos[i].nregs << 1) <= TCACHE_NSLOTS_SMALL_MIN) {
579  			tcache_bin_info[i].ncached_max =
580  			    TCACHE_NSLOTS_SMALL_MIN;
581  		} else if ((bin_infos[i].nregs << 1) <=
582  		    TCACHE_NSLOTS_SMALL_MAX) {
583  			tcache_bin_info[i].ncached_max =
584  			    (bin_infos[i].nregs << 1);
585  		} else {
586  			tcache_bin_info[i].ncached_max =
587  			    TCACHE_NSLOTS_SMALL_MAX;
588  		}
589  		stack_nelms += tcache_bin_info[i].ncached_max;
590  	}
591  	for (; i < nhbins; i++) {
592  		tcache_bin_info[i].ncached_max = TCACHE_NSLOTS_LARGE;
593  		stack_nelms += tcache_bin_info[i].ncached_max;
594  	}
595  	return false;
596  }
597  void
598  tcache_prefork(tsdn_t *tsdn) {
599  	if (!config_prof && opt_tcache) {
600  		malloc_mutex_prefork(tsdn, &tcaches_mtx);
601  	}
602  }
603  void
604  tcache_postfork_parent(tsdn_t *tsdn) {
605  	if (!config_prof && opt_tcache) {
606  		malloc_mutex_postfork_parent(tsdn, &tcaches_mtx);
607  	}
608  }
609  void
610  tcache_postfork_child(tsdn_t *tsdn) {
611  	if (!config_prof && opt_tcache) {
612  		malloc_mutex_postfork_child(tsdn, &tcaches_mtx);
613  	}
614  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-tcache.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-tcache.c</div>
                </div>
                <div class="column column_space"><pre><code>281  				if (iter == tcache) {
282  					in_ql = true;
283  					break;
284  				}
285  			}
286  			assert(in_ql);
</pre></code></div>
                <div class="column column_space"><pre><code>281  				if (iter == tcache) {
282  					in_ql = true;
283  					break;
284  				}
285  			}
286  			assert(in_ql);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    