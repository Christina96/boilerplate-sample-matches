
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 6.7114093959731544%, Tokens: 36, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>libtomcrypt-MDEwOlJlcG9zaXRvcnk3NzcwMTE=-flat-safer.c</h3>
            <pre><code>1  #include "tomcrypt_private.h"
2  #ifdef LTC_SAFER
3  #define LTC_SAFER_TAB_C
4  #include "safer_tab.c"
5  const struct ltc_cipher_descriptor safer_k64_desc = {
6     "safer-k64",
7     8, 8, 8, 8, LTC_SAFER_K64_DEFAULT_NOF_ROUNDS,
8     &safer_k64_setup,
9     &safer_ecb_encrypt,
10     &safer_ecb_decrypt,
11     &safer_k64_test,
12     &safer_done,
13     &safer_64_keysize,
14     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
15     },
16     safer_sk64_desc = {
17     "safer-sk64",
18     9, 8, 8, 8, LTC_SAFER_SK64_DEFAULT_NOF_ROUNDS,
19     &safer_sk64_setup,
20     &safer_ecb_encrypt,
21     &safer_ecb_decrypt,
22     &safer_sk64_test,
23     &safer_done,
24     &safer_64_keysize,
25     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
26     },
27     safer_k128_desc = {
28     "safer-k128",
29     10, 16, 16, 8, LTC_SAFER_K128_DEFAULT_NOF_ROUNDS,
30     &safer_k128_setup,
31     &safer_ecb_encrypt,
32     &safer_ecb_decrypt,
33     &safer_sk128_test,
34     &safer_done,
35     &safer_128_keysize,
36     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
37     },
38     safer_sk128_desc = {
39     "safer-sk128",
40     11, 16, 16, 8, LTC_SAFER_SK128_DEFAULT_NOF_ROUNDS,
41     &safer_sk128_setup,
42     &safer_ecb_encrypt,
43     &safer_ecb_decrypt,
44     &safer_sk128_test,
45     &safer_done,
46     &safer_128_keysize,
47     NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL
48     };
49  #define ROL8(x, n)   ((unsigned char)((unsigned int)(x) << (n)\
50                                       |(unsigned int)((x) & 0xFF) >> (8 - (n))))
51  #define EXP(x)       safer_ebox[(x) & 0xFF]
52  #define LOG(x)       safer_lbox[(x) & 0xFF]
53  #define PHT(x, y)    { y += x; x += y; }
54  #define IPHT(x, y)   { x -= y; y -= x; }
55  #ifdef LTC_CLEAN_STACK
56  static void s_safer_expand_userkey(const unsigned char *userkey_1,
57                                   const unsigned char *userkey_2,
58                                   unsigned int nof_rounds,
59                                   int strengthened,
60                                   safer_key_t key)
61  #else
62  static void safer_expand_userkey(const unsigned char *userkey_1,
63                                   const unsigned char *userkey_2,
64                                   unsigned int nof_rounds,
65                                   int strengthened,
66                                   safer_key_t key)
67  #endif
68  {   unsigned int i, j, k;
69      unsigned char ka[LTC_SAFER_BLOCK_LEN + 1];
70      unsigned char kb[LTC_SAFER_BLOCK_LEN + 1];
71      if (LTC_SAFER_MAX_NOF_ROUNDS < nof_rounds) {
72          nof_rounds = LTC_SAFER_MAX_NOF_ROUNDS;
73      }
74      *key++ = (unsigned char)nof_rounds;
75      ka[LTC_SAFER_BLOCK_LEN] = (unsigned char)0;
76      kb[LTC_SAFER_BLOCK_LEN] = (unsigned char)0;
77      k = 0;
78      for (j = 0; j < LTC_SAFER_BLOCK_LEN; j++) {
79          ka[j] = ROL8(userkey_1[j], 5);
80          ka[LTC_SAFER_BLOCK_LEN] ^= ka[j];
81          kb[j] = *key++ = userkey_2[j];
82          kb[LTC_SAFER_BLOCK_LEN] ^= kb[j];
83      }
84      for (i = 1; i <= nof_rounds; i++) {
85          for (j = 0; j < LTC_SAFER_BLOCK_LEN + 1; j++) {
86              ka[j] = ROL8(ka[j], 6);
87              kb[j] = ROL8(kb[j], 6);
88          }
89          if (strengthened) {
90             k = 2 * i - 1;
91             while (k >= (LTC_SAFER_BLOCK_LEN + 1)) { k -= LTC_SAFER_BLOCK_LEN + 1; }
92          }
93          for (j = 0; j < LTC_SAFER_BLOCK_LEN; j++) {
94              if (strengthened) {
95                  *key++ = (ka[k]
96                                  + safer_ebox[(int)safer_ebox[(int)((18 * i + j + 1)&0xFF)]]) & 0xFF;
97                  if (++k == (LTC_SAFER_BLOCK_LEN + 1)) { k = 0; }
98              } else {
99                  *key++ = (ka[j] + safer_ebox[(int)safer_ebox[(int)((18 * i + j + 1)&0xFF)]]) & 0xFF;
100              }
101          }
102          if (strengthened) {
103             k = 2 * i;
104             while (k >= (LTC_SAFER_BLOCK_LEN + 1)) { k -= LTC_SAFER_BLOCK_LEN + 1; }
105          }
106          for (j = 0; j < LTC_SAFER_BLOCK_LEN; j++) {
107              if (strengthened) {
108                  *key++ = (kb[k]
109                                  + safer_ebox[(int)safer_ebox[(int)((18 * i + j + 10)&0xFF)]]) & 0xFF;
110                  if (++k == (LTC_SAFER_BLOCK_LEN + 1)) { k = 0; }
111              } else {
112                  *key++ = (kb[j] + safer_ebox[(int)safer_ebox[(int)((18 * i + j + 10)&0xFF)]]) & 0xFF;
113              }
114          }
115      }
116  #ifdef LTC_CLEAN_STACK
117      zeromem(ka, sizeof(ka));
118      zeromem(kb, sizeof(kb));
119  #endif
120  }
121  #ifdef LTC_CLEAN_STACK
122  static void safer_expand_userkey(const unsigned char *userkey_1,
123                                   const unsigned char *userkey_2,
124                                   unsigned int nof_rounds,
125                                   int strengthened,
126                                   safer_key_t key)
127  {
128     s_safer_expand_userkey(userkey_1, userkey_2, nof_rounds, strengthened, key);
129     burn_stack(sizeof(unsigned char) * (2 * (LTC_SAFER_BLOCK_LEN + 1)) + sizeof(unsigned int)*2);
130  }
131  #endif
132  int safer_k64_setup(const unsigned char *key, int keylen, int num_rounds, symmetric_key *skey)
133  {
134     LTC_ARGCHK(key != NULL);
135     LTC_ARGCHK(skey != NULL);
136     if (num_rounds != 0 && (num_rounds < 6 || num_rounds > LTC_SAFER_MAX_NOF_ROUNDS)) {
137        return CRYPT_INVALID_ROUNDS;
138     }
139     if (keylen != 8) {
140        return CRYPT_INVALID_KEYSIZE;
141     }
142     safer_expand_userkey(key, key, (unsigned int)(num_rounds != 0 ?num_rounds:LTC_SAFER_K64_DEFAULT_NOF_ROUNDS), 0, skey->safer.key);
143     return CRYPT_OK;
144  }
145  int safer_sk64_setup(const unsigned char *key, int keylen, int num_rounds, symmetric_key *skey)
146  {
147     LTC_ARGCHK(key != NULL);
148     LTC_ARGCHK(skey != NULL);
149     if (num_rounds != 0 && (num_rounds < 6 || num_rounds > LTC_SAFER_MAX_NOF_ROUNDS)) {
150        return CRYPT_INVALID_ROUNDS;
151     }
152     if (keylen != 8) {
153        return CRYPT_INVALID_KEYSIZE;
154     }
155     safer_expand_userkey(key, key, (unsigned int)(num_rounds != 0 ?num_rounds:LTC_SAFER_SK64_DEFAULT_NOF_ROUNDS), 1, skey->safer.key);
156     return CRYPT_OK;
157  }
158  int safer_k128_setup(const unsigned char *key, int keylen, int num_rounds, symmetric_key *skey)
159  {
160     LTC_ARGCHK(key != NULL);
161     LTC_ARGCHK(skey != NULL);
162     if (num_rounds != 0 && (num_rounds < 6 || num_rounds > LTC_SAFER_MAX_NOF_ROUNDS)) {
163        return CRYPT_INVALID_ROUNDS;
164     }
165     if (keylen != 16) {
166        return CRYPT_INVALID_KEYSIZE;
167     }
168     safer_expand_userkey(key, key+8, (unsigned int)(num_rounds != 0 ?num_rounds:LTC_SAFER_K128_DEFAULT_NOF_ROUNDS), 0, skey->safer.key);
169     return CRYPT_OK;
170  }
171  int safer_sk128_setup(const unsigned char *key, int keylen, int num_rounds, symmetric_key *skey)
172  {
173     LTC_ARGCHK(key != NULL);
174     LTC_ARGCHK(skey != NULL);
175     if (num_rounds != 0 && (num_rounds < 6 || num_rounds > LTC_SAFER_MAX_NOF_ROUNDS)) {
176        return CRYPT_INVALID_ROUNDS;
177     }
178     if (keylen != 16) {
179        return CRYPT_INVALID_KEYSIZE;
180     }
181     safer_expand_userkey(key, key+8, (unsigned int)(num_rounds != 0?num_rounds:LTC_SAFER_SK128_DEFAULT_NOF_ROUNDS), 1, skey->safer.key);
182     return CRYPT_OK;
183  }
184  #ifdef LTC_CLEAN_STACK
185  static int s_safer_ecb_encrypt(const unsigned char *pt,
186                               unsigned char *ct,
187                               const symmetric_key *skey)
188  #else
189  int safer_ecb_encrypt(const unsigned char *pt,
190                               unsigned char *ct,
191                               const symmetric_key *skey)
192  #endif
193  {   unsigned char a, b, c, d, e, f, g, h, t;
194      unsigned int round;
195      const unsigned char *key;
196      LTC_ARGCHK(pt != NULL);
197      LTC_ARGCHK(ct != NULL);
198      LTC_ARGCHK(skey != NULL);
199      key = skey->safer.key;
200      a = pt[0]; b = pt[1]; c = pt[2]; d = pt[3];
201      e = pt[4]; f = pt[5]; g = pt[6]; h = pt[7];
202      if (LTC_SAFER_MAX_NOF_ROUNDS < (round = *key)) round = LTC_SAFER_MAX_NOF_ROUNDS;
203      while(round-- > 0)
<span onclick='openModal()' class='match'>204      {
205          a ^= *++key; b += *++key; c += *++key; d ^= *++key;
206          e ^= *++key; f += *++key; g += *++key; h ^= *++key;
207          a = EXP(a) + *++key; b = LOG(b) ^ *++key;
208          c = LOG(c) ^ *++key; d = EXP(d) + *++key;
209          e = EXP(e) + *++key; f = LOG(f) ^ *++key;
210          g = LOG(g) ^ *++key; h = EXP(h) + *++key;
211          PHT(a, b); PHT(c, d); PHT(e, f); PHT(g, h);
212          PHT(a, c); PHT(e, g); PHT(b, d); PHT(f, h);
213          PHT(a, e); PHT(b, f); PHT(c, g); PHT(d, h);
214          t = b; b = e; e = c; c = t; t = d; d = f; f = g; g = t;
</span>215      }
216      a ^= *++key; b += *++key; c += *++key; d ^= *++key;
217      e ^= *++key; f += *++key; g += *++key; h ^= *++key;
218      ct[0] = a & 0xFF; ct[1] = b & 0xFF;
219      ct[2] = c & 0xFF; ct[3] = d & 0xFF;
220      ct[4] = e & 0xFF; ct[5] = f & 0xFF;
221      ct[6] = g & 0xFF; ct[7] = h & 0xFF;
222      return CRYPT_OK;
223  }
224  #ifdef LTC_CLEAN_STACK
225  int safer_ecb_encrypt(const unsigned char *pt,
226                               unsigned char *ct,
227                               const symmetric_key *skey)
228  {
229      int err = s_safer_ecb_encrypt(pt, ct, skey);
230      burn_stack(sizeof(unsigned char) * 9 + sizeof(unsigned int) + sizeof(unsigned char *));
231      return err;
232  }
233  #endif
234  #ifdef LTC_CLEAN_STACK
235  static int s_safer_ecb_decrypt(const unsigned char *ct,
236                               unsigned char *pt,
237                               const symmetric_key *skey)
238  #else
239  int safer_ecb_decrypt(const unsigned char *ct,
240                               unsigned char *pt,
241                               const symmetric_key *skey)
242  #endif
243  {   unsigned char a, b, c, d, e, f, g, h, t;
244      unsigned int round;
245      const unsigned char *key;
246      LTC_ARGCHK(ct != NULL);
247      LTC_ARGCHK(pt != NULL);
248      LTC_ARGCHK(skey != NULL);
249      key = skey->safer.key;
250      a = ct[0]; b = ct[1]; c = ct[2]; d = ct[3];
251      e = ct[4]; f = ct[5]; g = ct[6]; h = ct[7];
252      if (LTC_SAFER_MAX_NOF_ROUNDS < (round = *key)) round = LTC_SAFER_MAX_NOF_ROUNDS;
253      key += LTC_SAFER_BLOCK_LEN * (1 + 2 * round);
254      h ^= *key; g -= *--key; f -= *--key; e ^= *--key;
255      d ^= *--key; c -= *--key; b -= *--key; a ^= *--key;
256      while (round--)
257      {
258          t = e; e = b; b = c; c = t; t = f; f = d; d = g; g = t;
259          IPHT(a, e); IPHT(b, f); IPHT(c, g); IPHT(d, h);
260          IPHT(a, c); IPHT(e, g); IPHT(b, d); IPHT(f, h);
261          IPHT(a, b); IPHT(c, d); IPHT(e, f); IPHT(g, h);
262          h -= *--key; g ^= *--key; f ^= *--key; e -= *--key;
263          d -= *--key; c ^= *--key; b ^= *--key; a -= *--key;
264          h = LOG(h) ^ *--key; g = EXP(g) - *--key;
265          f = EXP(f) - *--key; e = LOG(e) ^ *--key;
266          d = LOG(d) ^ *--key; c = EXP(c) - *--key;
267          b = EXP(b) - *--key; a = LOG(a) ^ *--key;
268      }
269      pt[0] = a & 0xFF; pt[1] = b & 0xFF;
270      pt[2] = c & 0xFF; pt[3] = d & 0xFF;
271      pt[4] = e & 0xFF; pt[5] = f & 0xFF;
272      pt[6] = g & 0xFF; pt[7] = h & 0xFF;
273      return CRYPT_OK;
274  }
275  #ifdef LTC_CLEAN_STACK
276  int safer_ecb_decrypt(const unsigned char *ct,
277                               unsigned char *pt,
278                               const symmetric_key *skey)
279  {
280      int err = s_safer_ecb_decrypt(ct, pt, skey);
281      burn_stack(sizeof(unsigned char) * 9 + sizeof(unsigned int) + sizeof(unsigned char *));
282      return err;
283  }
284  #endif
285  int safer_64_keysize(int *keysize)
286  {
287     LTC_ARGCHK(keysize != NULL);
288     if (*keysize < 8) {
289        return CRYPT_INVALID_KEYSIZE;
290     }
291     *keysize = 8;
292     return CRYPT_OK;
293  }
294  int safer_128_keysize(int *keysize)
295  {
296     LTC_ARGCHK(keysize != NULL);
297     if (*keysize < 16) {
298        return CRYPT_INVALID_KEYSIZE;
299     }
300     *keysize = 16;
301     return CRYPT_OK;
302  }
303  int safer_k64_test(void)
304  {
305   #ifndef LTC_TEST
306      return CRYPT_NOP;
307   #else
308     static const unsigned char k64_pt[]  = { 1, 2, 3, 4, 5, 6, 7, 8 },
309                                k64_key[] = { 8, 7, 6, 5, 4, 3, 2, 1 },
310                                k64_ct[]  = { 200, 242, 156, 221, 135, 120, 62, 217 };
311     symmetric_key skey;
312     unsigned char buf[2][8];
313     int err;
314     if ((err = safer_k64_setup(k64_key, 8, 6, &skey)) != CRYPT_OK) {
315        return err;
316     }
317     safer_ecb_encrypt(k64_pt, buf[0], &skey);
318     safer_ecb_decrypt(buf[0], buf[1], &skey);
319     if (compare_testvector(buf[0], 8, k64_ct, 8, "Safer K64 Encrypt", 0) != 0 ||
320           compare_testvector(buf[1], 8, k64_pt, 8, "Safer K64 Decrypt", 0) != 0) {
321        return CRYPT_FAIL_TESTVECTOR;
322     }
323     return CRYPT_OK;
324   #endif
325  }
326  int safer_sk64_test(void)
327  {
328   #ifndef LTC_TEST
329      return CRYPT_NOP;
330   #else
331     static const unsigned char sk64_pt[]  = { 1, 2, 3, 4, 5, 6, 7, 8 },
332                                sk64_key[] = { 1, 2, 3, 4, 5, 6, 7, 8 },
333                                sk64_ct[]  = { 95, 206, 155, 162, 5, 132, 56, 199 };
334     symmetric_key skey;
335     unsigned char buf[2][8];
336     int err, y;
337     if ((err = safer_sk64_setup(sk64_key, 8, 6, &skey)) != CRYPT_OK) {
338        return err;
339     }
340     safer_ecb_encrypt(sk64_pt, buf[0], &skey);
341     safer_ecb_decrypt(buf[0], buf[1], &skey);
342     if (compare_testvector(buf[0], 8, sk64_ct, 8, "Safer SK64 Encrypt", 0) != 0 ||
343           compare_testvector(buf[1], 8, sk64_pt, 8, "Safer SK64 Decrypt", 0) != 0) {
344        return CRYPT_FAIL_TESTVECTOR;
345     }
346     for (y = 0; y < 8; y++) buf[0][y] = 0;
347     for (y = 0; y < 1000; y++) safer_ecb_encrypt(buf[0], buf[0], &skey);
348     for (y = 0; y < 1000; y++) safer_ecb_decrypt(buf[0], buf[0], &skey);
349     for (y = 0; y < 8; y++) if (buf[0][y] != 0) return CRYPT_FAIL_TESTVECTOR;
350     return CRYPT_OK;
351    #endif
352  }
353  void safer_done(symmetric_key *skey)
354  {
355    LTC_UNUSED_PARAM(skey);
356  }
357  int safer_sk128_test(void)
358  {
359   #ifndef LTC_TEST
360      return CRYPT_NOP;
361   #else
362     static const unsigned char sk128_pt[]  = { 1, 2, 3, 4, 5, 6, 7, 8 },
363                                sk128_key[] = { 1, 2, 3, 4, 5, 6, 7, 8,
364                                                0, 0, 0, 0, 0, 0, 0, 0 },
365                                sk128_ct[]  = { 255, 120, 17, 228, 179, 167, 46, 113 };
366     symmetric_key skey;
367     unsigned char buf[2][8];
368     int err, y;
369     if ((err = safer_sk128_setup(sk128_key, 16, 0, &skey)) != CRYPT_OK) {
370        return err;
371     }
372     safer_ecb_encrypt(sk128_pt, buf[0], &skey);
373     safer_ecb_decrypt(buf[0], buf[1], &skey);
374     if (compare_testvector(buf[0], 8, sk128_ct, 8, "Safer SK128 Encrypt", 0) != 0 ||
375           compare_testvector(buf[1], 8, sk128_pt, 8, "Safer SK128 Decrypt", 0) != 0) {
376        return CRYPT_FAIL_TESTVECTOR;
377     }
378     for (y = 0; y < 8; y++) buf[0][y] = 0;
379     for (y = 0; y < 1000; y++) safer_ecb_encrypt(buf[0], buf[0], &skey);
380     for (y = 0; y < 1000; y++) safer_ecb_decrypt(buf[0], buf[0], &skey);
381     for (y = 0; y < 8; y++) if (buf[0][y] != 0) return CRYPT_FAIL_TESTVECTOR;
382     return CRYPT_OK;
383   #endif
384  }
385  #endif
</code></pre>
        </div>
        <div class="column">
            <h3>PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-dec_neon.c</h3>
            <pre><code>1  #include "src/dsp/dsp.h"
2  #if defined(WEBP_USE_NEON)
3  #include "src/dsp/neon.h"
4  #include "src/dec/vp8i_dec.h"
5  #if !defined(WORK_AROUND_GCC)
6  static WEBP_INLINE uint8x8x4_t Load4x8_NEON(const uint8_t* const src,
7                                              int stride) {
8    const uint8x8_t zero = vdup_n_u8(0);
9    uint8x8x4_t out;
10    INIT_VECTOR4(out, zero, zero, zero, zero);
11    out = vld4_lane_u8(src + 0 * stride, out, 0);
12    out = vld4_lane_u8(src + 1 * stride, out, 1);
13    out = vld4_lane_u8(src + 2 * stride, out, 2);
14    out = vld4_lane_u8(src + 3 * stride, out, 3);
15    out = vld4_lane_u8(src + 4 * stride, out, 4);
16    out = vld4_lane_u8(src + 5 * stride, out, 5);
17    out = vld4_lane_u8(src + 6 * stride, out, 6);
18    out = vld4_lane_u8(src + 7 * stride, out, 7);
19    return out;
20  }
21  static WEBP_INLINE void Load4x16_NEON(const uint8_t* const src, int stride,
22                                        uint8x16_t* const p1,
23                                        uint8x16_t* const p0,
24                                        uint8x16_t* const q0,
25                                        uint8x16_t* const q1) {
26    const uint8x8x4_t row0 = Load4x8_NEON(src - 2 + 0 * stride, stride);
27    const uint8x8x4_t row8 = Load4x8_NEON(src - 2 + 8 * stride, stride);
28    *p1 = vcombine_u8(row0.val[0], row8.val[0]);
29    *p0 = vcombine_u8(row0.val[1], row8.val[1]);
30    *q0 = vcombine_u8(row0.val[2], row8.val[2]);
31    *q1 = vcombine_u8(row0.val[3], row8.val[3]);
32  }
33  #else  
34  #define LOADQ_LANE_32b(VALUE, LANE) do {                             \
35    (VALUE) = vld1q_lane_u32((const uint32_t*)src, (VALUE), (LANE));   \
36    src += stride;                                                     \
37  } while (0)
38  static WEBP_INLINE void Load4x16_NEON(const uint8_t* src, int stride,
39                                        uint8x16_t* const p1,
40                                        uint8x16_t* const p0,
41                                        uint8x16_t* const q0,
42                                        uint8x16_t* const q1) {
43    const uint32x4_t zero = vdupq_n_u32(0);
44    uint32x4x4_t in;
45    INIT_VECTOR4(in, zero, zero, zero, zero);
46    src -= 2;
47    LOADQ_LANE_32b(in.val[0], 0);
48    LOADQ_LANE_32b(in.val[1], 0);
49    LOADQ_LANE_32b(in.val[2], 0);
50    LOADQ_LANE_32b(in.val[3], 0);
51    LOADQ_LANE_32b(in.val[0], 1);
52    LOADQ_LANE_32b(in.val[1], 1);
53    LOADQ_LANE_32b(in.val[2], 1);
54    LOADQ_LANE_32b(in.val[3], 1);
55    LOADQ_LANE_32b(in.val[0], 2);
56    LOADQ_LANE_32b(in.val[1], 2);
57    LOADQ_LANE_32b(in.val[2], 2);
58    LOADQ_LANE_32b(in.val[3], 2);
59    LOADQ_LANE_32b(in.val[0], 3);
60    LOADQ_LANE_32b(in.val[1], 3);
61    LOADQ_LANE_32b(in.val[2], 3);
62    LOADQ_LANE_32b(in.val[3], 3);
63    {
64      const uint8x16x2_t row01 = vtrnq_u8(vreinterpretq_u8_u32(in.val[0]),
65                                          vreinterpretq_u8_u32(in.val[1]));
66      const uint8x16x2_t row23 = vtrnq_u8(vreinterpretq_u8_u32(in.val[2]),
67                                          vreinterpretq_u8_u32(in.val[3]));
68      const uint16x8x2_t row02 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[0]),
69                                           vreinterpretq_u16_u8(row23.val[0]));
70      const uint16x8x2_t row13 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[1]),
71                                           vreinterpretq_u16_u8(row23.val[1]));
72      *p1 = vreinterpretq_u8_u16(row02.val[0]);
73      *p0 = vreinterpretq_u8_u16(row13.val[0]);
74      *q0 = vreinterpretq_u8_u16(row02.val[1]);
75      *q1 = vreinterpretq_u8_u16(row13.val[1]);
76    }
77  }
78  #undef LOADQ_LANE_32b
79  #endif  
80  static WEBP_INLINE void Load8x16_NEON(
81      const uint8_t* const src, int stride,
82      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
83      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
84      uint8x16_t* const q2, uint8x16_t* const q3) {
85    Load4x16_NEON(src - 2, stride, p3, p2, p1, p0);
86    Load4x16_NEON(src + 2, stride, q0, q1, q2, q3);
87  }
88  static WEBP_INLINE void Load16x4_NEON(const uint8_t* const src, int stride,
89                                        uint8x16_t* const p1,
90                                        uint8x16_t* const p0,
91                                        uint8x16_t* const q0,
92                                        uint8x16_t* const q1) {
93    *p1 = vld1q_u8(src - 2 * stride);
94    *p0 = vld1q_u8(src - 1 * stride);
95    *q0 = vld1q_u8(src + 0 * stride);
96    *q1 = vld1q_u8(src + 1 * stride);
97  }
98  static WEBP_INLINE void Load16x8_NEON(
99      const uint8_t* const src, int stride,
100      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
101      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
102      uint8x16_t* const q2, uint8x16_t* const q3) {
103    Load16x4_NEON(src - 2  * stride, stride, p3, p2, p1, p0);
104    Load16x4_NEON(src + 2  * stride, stride, q0, q1, q2, q3);
105  }
106  static WEBP_INLINE void Load8x8x2_NEON(
107      const uint8_t* const u, const uint8_t* const v, int stride,
108      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
109      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
110      uint8x16_t* const q2, uint8x16_t* const q3) {
111    *p3 = vcombine_u8(vld1_u8(u - 4 * stride), vld1_u8(v - 4 * stride));
112    *p2 = vcombine_u8(vld1_u8(u - 3 * stride), vld1_u8(v - 3 * stride));
113    *p1 = vcombine_u8(vld1_u8(u - 2 * stride), vld1_u8(v - 2 * stride));
114    *p0 = vcombine_u8(vld1_u8(u - 1 * stride), vld1_u8(v - 1 * stride));
115    *q0 = vcombine_u8(vld1_u8(u + 0 * stride), vld1_u8(v + 0 * stride));
116    *q1 = vcombine_u8(vld1_u8(u + 1 * stride), vld1_u8(v + 1 * stride));
117    *q2 = vcombine_u8(vld1_u8(u + 2 * stride), vld1_u8(v + 2 * stride));
118    *q3 = vcombine_u8(vld1_u8(u + 3 * stride), vld1_u8(v + 3 * stride));
119  }
120  #if !defined(WORK_AROUND_GCC)
121  #define LOAD_UV_8(ROW) \
122    vcombine_u8(vld1_u8(u - 4 + (ROW) * stride), vld1_u8(v - 4 + (ROW) * stride))
123  static WEBP_INLINE void Load8x8x2T_NEON(
124      const uint8_t* const u, const uint8_t* const v, int stride,
125      uint8x16_t* const p3, uint8x16_t* const p2, uint8x16_t* const p1,
126      uint8x16_t* const p0, uint8x16_t* const q0, uint8x16_t* const q1,
127      uint8x16_t* const q2, uint8x16_t* const q3) {
128    const uint8x16_t row0 = LOAD_UV_8(0);
129    const uint8x16_t row1 = LOAD_UV_8(1);
130    const uint8x16_t row2 = LOAD_UV_8(2);
131    const uint8x16_t row3 = LOAD_UV_8(3);
132    const uint8x16_t row4 = LOAD_UV_8(4);
133    const uint8x16_t row5 = LOAD_UV_8(5);
134    const uint8x16_t row6 = LOAD_UV_8(6);
135    const uint8x16_t row7 = LOAD_UV_8(7);
136    const uint8x16x2_t row01 = vtrnq_u8(row0, row1);  
137    const uint8x16x2_t row23 = vtrnq_u8(row2, row3);  
138    const uint8x16x2_t row45 = vtrnq_u8(row4, row5);  
139    const uint8x16x2_t row67 = vtrnq_u8(row6, row7);  
140    const uint16x8x2_t row02 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[0]),
141                                         vreinterpretq_u16_u8(row23.val[0]));
142    const uint16x8x2_t row13 = vtrnq_u16(vreinterpretq_u16_u8(row01.val[1]),
143                                         vreinterpretq_u16_u8(row23.val[1]));
144    const uint16x8x2_t row46 = vtrnq_u16(vreinterpretq_u16_u8(row45.val[0]),
145                                         vreinterpretq_u16_u8(row67.val[0]));
146    const uint16x8x2_t row57 = vtrnq_u16(vreinterpretq_u16_u8(row45.val[1]),
147                                         vreinterpretq_u16_u8(row67.val[1]));
148    const uint32x4x2_t row04 = vtrnq_u32(vreinterpretq_u32_u16(row02.val[0]),
149                                         vreinterpretq_u32_u16(row46.val[0]));
150    const uint32x4x2_t row26 = vtrnq_u32(vreinterpretq_u32_u16(row02.val[1]),
151                                         vreinterpretq_u32_u16(row46.val[1]));
152    const uint32x4x2_t row15 = vtrnq_u32(vreinterpretq_u32_u16(row13.val[0]),
153                                         vreinterpretq_u32_u16(row57.val[0]));
154    const uint32x4x2_t row37 = vtrnq_u32(vreinterpretq_u32_u16(row13.val[1]),
155                                         vreinterpretq_u32_u16(row57.val[1]));
156    *p3 = vreinterpretq_u8_u32(row04.val[0]);
157    *p2 = vreinterpretq_u8_u32(row15.val[0]);
158    *p1 = vreinterpretq_u8_u32(row26.val[0]);
159    *p0 = vreinterpretq_u8_u32(row37.val[0]);
160    *q0 = vreinterpretq_u8_u32(row04.val[1]);
161    *q1 = vreinterpretq_u8_u32(row15.val[1]);
162    *q2 = vreinterpretq_u8_u32(row26.val[1]);
163    *q3 = vreinterpretq_u8_u32(row37.val[1]);
164  }
165  #undef LOAD_UV_8
166  #endif  
167  static WEBP_INLINE void Store2x8_NEON(const uint8x8x2_t v,
168                                        uint8_t* const dst, int stride) {
169    vst2_lane_u8(dst + 0 * stride, v, 0);
170    vst2_lane_u8(dst + 1 * stride, v, 1);
171    vst2_lane_u8(dst + 2 * stride, v, 2);
172    vst2_lane_u8(dst + 3 * stride, v, 3);
173    vst2_lane_u8(dst + 4 * stride, v, 4);
174    vst2_lane_u8(dst + 5 * stride, v, 5);
175    vst2_lane_u8(dst + 6 * stride, v, 6);
176    vst2_lane_u8(dst + 7 * stride, v, 7);
177  }
178  static WEBP_INLINE void Store2x16_NEON(const uint8x16_t p0, const uint8x16_t q0,
179                                         uint8_t* const dst, int stride) {
180    uint8x8x2_t lo, hi;
181    lo.val[0] = vget_low_u8(p0);
182    lo.val[1] = vget_low_u8(q0);
183    hi.val[0] = vget_high_u8(p0);
184    hi.val[1] = vget_high_u8(q0);
185    Store2x8_NEON(lo, dst - 1 + 0 * stride, stride);
186    Store2x8_NEON(hi, dst - 1 + 8 * stride, stride);
187  }
188  #if !defined(WORK_AROUND_GCC)
189  static WEBP_INLINE void Store4x8_NEON(const uint8x8x4_t v,
190                                        uint8_t* const dst, int stride) {
191    vst4_lane_u8(dst + 0 * stride, v, 0);
192    vst4_lane_u8(dst + 1 * stride, v, 1);
193    vst4_lane_u8(dst + 2 * stride, v, 2);
194    vst4_lane_u8(dst + 3 * stride, v, 3);
195    vst4_lane_u8(dst + 4 * stride, v, 4);
196    vst4_lane_u8(dst + 5 * stride, v, 5);
197    vst4_lane_u8(dst + 6 * stride, v, 6);
198    vst4_lane_u8(dst + 7 * stride, v, 7);
199  }
200  static WEBP_INLINE void Store4x16_NEON(const uint8x16_t p1, const uint8x16_t p0,
201                                         const uint8x16_t q0, const uint8x16_t q1,
202                                         uint8_t* const dst, int stride) {
203    uint8x8x4_t lo, hi;
204    INIT_VECTOR4(lo,
205                 vget_low_u8(p1), vget_low_u8(p0),
206                 vget_low_u8(q0), vget_low_u8(q1));
207    INIT_VECTOR4(hi,
208                 vget_high_u8(p1), vget_high_u8(p0),
209                 vget_high_u8(q0), vget_high_u8(q1));
210    Store4x8_NEON(lo, dst - 2 + 0 * stride, stride);
211    Store4x8_NEON(hi, dst - 2 + 8 * stride, stride);
212  }
213  #endif  
214  static WEBP_INLINE void Store16x2_NEON(const uint8x16_t p0, const uint8x16_t q0,
215                                         uint8_t* const dst, int stride) {
216    vst1q_u8(dst - stride, p0);
217    vst1q_u8(dst, q0);
218  }
219  static WEBP_INLINE void Store16x4_NEON(const uint8x16_t p1, const uint8x16_t p0,
220                                         const uint8x16_t q0, const uint8x16_t q1,
221                                         uint8_t* const dst, int stride) {
222    Store16x2_NEON(p1, p0, dst - stride, stride);
223    Store16x2_NEON(q0, q1, dst + stride, stride);
224  }
225  static WEBP_INLINE void Store8x2x2_NEON(const uint8x16_t p0,
226                                          const uint8x16_t q0,
227                                          uint8_t* const u, uint8_t* const v,
228                                          int stride) {
229    vst1_u8(u - stride, vget_low_u8(p0));
230    vst1_u8(u,          vget_low_u8(q0));
231    vst1_u8(v - stride, vget_high_u8(p0));
232    vst1_u8(v,          vget_high_u8(q0));
233  }
234  static WEBP_INLINE void Store8x4x2_NEON(const uint8x16_t p1,
235                                          const uint8x16_t p0,
236                                          const uint8x16_t q0,
237                                          const uint8x16_t q1,
238                                          uint8_t* const u, uint8_t* const v,
239                                          int stride) {
240    Store8x2x2_NEON(p1, p0, u - stride, v - stride, stride);
241    Store8x2x2_NEON(q0, q1, u + stride, v + stride, stride);
242  }
243  #if !defined(WORK_AROUND_GCC)
244  #define STORE6_LANE(DST, VAL0, VAL1, LANE) do {   \
245    vst3_lane_u8((DST) - 3, (VAL0), (LANE));        \
246    vst3_lane_u8((DST) + 0, (VAL1), (LANE));        \
247    (DST) += stride;                                \
248  } while (0)
249  static WEBP_INLINE void Store6x8x2_NEON(
250      const uint8x16_t p2, const uint8x16_t p1, const uint8x16_t p0,
251      const uint8x16_t q0, const uint8x16_t q1, const uint8x16_t q2,
252      uint8_t* u, uint8_t* v, int stride) {
253    uint8x8x3_t u0, u1, v0, v1;
254    INIT_VECTOR3(u0, vget_low_u8(p2), vget_low_u8(p1), vget_low_u8(p0));
255    INIT_VECTOR3(u1, vget_low_u8(q0), vget_low_u8(q1), vget_low_u8(q2));
256    INIT_VECTOR3(v0, vget_high_u8(p2), vget_high_u8(p1), vget_high_u8(p0));
257    INIT_VECTOR3(v1, vget_high_u8(q0), vget_high_u8(q1), vget_high_u8(q2));
258    STORE6_LANE(u, u0, u1, 0);
259    STORE6_LANE(u, u0, u1, 1);
260    STORE6_LANE(u, u0, u1, 2);
261    STORE6_LANE(u, u0, u1, 3);
262    STORE6_LANE(u, u0, u1, 4);
263    STORE6_LANE(u, u0, u1, 5);
264    STORE6_LANE(u, u0, u1, 6);
265    STORE6_LANE(u, u0, u1, 7);
266    STORE6_LANE(v, v0, v1, 0);
267    STORE6_LANE(v, v0, v1, 1);
268    STORE6_LANE(v, v0, v1, 2);
269    STORE6_LANE(v, v0, v1, 3);
270    STORE6_LANE(v, v0, v1, 4);
271    STORE6_LANE(v, v0, v1, 5);
272    STORE6_LANE(v, v0, v1, 6);
273    STORE6_LANE(v, v0, v1, 7);
274  }
275  #undef STORE6_LANE
276  static WEBP_INLINE void Store4x8x2_NEON(const uint8x16_t p1,
277                                          const uint8x16_t p0,
278                                          const uint8x16_t q0,
279                                          const uint8x16_t q1,
280                                          uint8_t* const u, uint8_t* const v,
281                                          int stride) {
282    uint8x8x4_t u0, v0;
283    INIT_VECTOR4(u0,
284                 vget_low_u8(p1), vget_low_u8(p0),
285                 vget_low_u8(q0), vget_low_u8(q1));
286    INIT_VECTOR4(v0,
287                 vget_high_u8(p1), vget_high_u8(p0),
288                 vget_high_u8(q0), vget_high_u8(q1));
289    vst4_lane_u8(u - 2 + 0 * stride, u0, 0);
290    vst4_lane_u8(u - 2 + 1 * stride, u0, 1);
291    vst4_lane_u8(u - 2 + 2 * stride, u0, 2);
292    vst4_lane_u8(u - 2 + 3 * stride, u0, 3);
293    vst4_lane_u8(u - 2 + 4 * stride, u0, 4);
294    vst4_lane_u8(u - 2 + 5 * stride, u0, 5);
295    vst4_lane_u8(u - 2 + 6 * stride, u0, 6);
296    vst4_lane_u8(u - 2 + 7 * stride, u0, 7);
297    vst4_lane_u8(v - 2 + 0 * stride, v0, 0);
298    vst4_lane_u8(v - 2 + 1 * stride, v0, 1);
299    vst4_lane_u8(v - 2 + 2 * stride, v0, 2);
300    vst4_lane_u8(v - 2 + 3 * stride, v0, 3);
301    vst4_lane_u8(v - 2 + 4 * stride, v0, 4);
302    vst4_lane_u8(v - 2 + 5 * stride, v0, 5);
303    vst4_lane_u8(v - 2 + 6 * stride, v0, 6);
304    vst4_lane_u8(v - 2 + 7 * stride, v0, 7);
305  }
306  #endif  
307  static WEBP_INLINE int16x8_t ConvertU8ToS16_NEON(uint8x8_t v) {
308    return vreinterpretq_s16_u16(vmovl_u8(v));
309  }
310  static WEBP_INLINE void SaturateAndStore4x4_NEON(uint8_t* const dst,
311                                                   const int16x8_t dst01,
312                                                   const int16x8_t dst23) {
313    const uint8x8_t dst01_u8 = vqmovun_s16(dst01);
314    const uint8x8_t dst23_u8 = vqmovun_s16(dst23);
315    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), vreinterpret_u32_u8(dst01_u8), 0);
316    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), vreinterpret_u32_u8(dst01_u8), 1);
317    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), vreinterpret_u32_u8(dst23_u8), 0);
318    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), vreinterpret_u32_u8(dst23_u8), 1);
319  }
320  static WEBP_INLINE void Add4x4_NEON(const int16x8_t row01,
321                                      const int16x8_t row23,
322                                      uint8_t* const dst) {
323    uint32x2_t dst01 = vdup_n_u32(0);
324    uint32x2_t dst23 = vdup_n_u32(0);
325    dst01 = vld1_lane_u32((uint32_t*)(dst + 0 * BPS), dst01, 0);
326    dst23 = vld1_lane_u32((uint32_t*)(dst + 2 * BPS), dst23, 0);
327    dst01 = vld1_lane_u32((uint32_t*)(dst + 1 * BPS), dst01, 1);
328    dst23 = vld1_lane_u32((uint32_t*)(dst + 3 * BPS), dst23, 1);
329    {
330      const int16x8_t dst01_s16 = ConvertU8ToS16_NEON(vreinterpret_u8_u32(dst01));
331      const int16x8_t dst23_s16 = ConvertU8ToS16_NEON(vreinterpret_u8_u32(dst23));
332      const int16x8_t out01 = vrsraq_n_s16(dst01_s16, row01, 3);
333      const int16x8_t out23 = vrsraq_n_s16(dst23_s16, row23, 3);
334      SaturateAndStore4x4_NEON(dst, out01, out23);
335    }
336  }
337  static uint8x16_t NeedsFilter_NEON(const uint8x16_t p1, const uint8x16_t p0,
338                                     const uint8x16_t q0, const uint8x16_t q1,
339                                     int thresh) {
340    const uint8x16_t thresh_v = vdupq_n_u8((uint8_t)thresh);
341    const uint8x16_t a_p0_q0 = vabdq_u8(p0, q0);               
342    const uint8x16_t a_p1_q1 = vabdq_u8(p1, q1);               
343    const uint8x16_t a_p0_q0_2 = vqaddq_u8(a_p0_q0, a_p0_q0);  
344    const uint8x16_t a_p1_q1_2 = vshrq_n_u8(a_p1_q1, 1);       
345    const uint8x16_t sum = vqaddq_u8(a_p0_q0_2, a_p1_q1_2);
346    const uint8x16_t mask = vcgeq_u8(thresh_v, sum);
347    return mask;
348  }
349  static int8x16_t FlipSign_NEON(const uint8x16_t v) {
350    const uint8x16_t sign_bit = vdupq_n_u8(0x80);
351    return vreinterpretq_s8_u8(veorq_u8(v, sign_bit));
352  }
353  static uint8x16_t FlipSignBack_NEON(const int8x16_t v) {
354    const int8x16_t sign_bit = vdupq_n_s8(0x80);
355    return vreinterpretq_u8_s8(veorq_s8(v, sign_bit));
356  }
357  static int8x16_t GetBaseDelta_NEON(const int8x16_t p1, const int8x16_t p0,
358                                     const int8x16_t q0, const int8x16_t q1) {
359    const int8x16_t q0_p0 = vqsubq_s8(q0, p0);      
360    const int8x16_t p1_q1 = vqsubq_s8(p1, q1);      
361    const int8x16_t s1 = vqaddq_s8(p1_q1, q0_p0);   
362    const int8x16_t s2 = vqaddq_s8(q0_p0, s1);      
363    const int8x16_t s3 = vqaddq_s8(q0_p0, s2);      
364    return s3;
365  }
366  static int8x16_t GetBaseDelta0_NEON(const int8x16_t p0, const int8x16_t q0) {
367    const int8x16_t q0_p0 = vqsubq_s8(q0, p0);      
368    const int8x16_t s1 = vqaddq_s8(q0_p0, q0_p0);   
369    const int8x16_t s2 = vqaddq_s8(q0_p0, s1);      
370    return s2;
371  }
372  static void ApplyFilter2NoFlip_NEON(const int8x16_t p0s, const int8x16_t q0s,
373                                      const int8x16_t delta,
374                                      int8x16_t* const op0,
375                                      int8x16_t* const oq0) {
376    const int8x16_t kCst3 = vdupq_n_s8(0x03);
377    const int8x16_t kCst4 = vdupq_n_s8(0x04);
378    const int8x16_t delta_p3 = vqaddq_s8(delta, kCst3);
379    const int8x16_t delta_p4 = vqaddq_s8(delta, kCst4);
380    const int8x16_t delta3 = vshrq_n_s8(delta_p3, 3);
381    const int8x16_t delta4 = vshrq_n_s8(delta_p4, 3);
382    *op0 = vqaddq_s8(p0s, delta3);
383    *oq0 = vqsubq_s8(q0s, delta4);
384  }
385  #if defined(WEBP_USE_INTRINSICS)
386  static void ApplyFilter2_NEON(const int8x16_t p0s, const int8x16_t q0s,
387                                const int8x16_t delta,
388                                uint8x16_t* const op0, uint8x16_t* const oq0) {
389    const int8x16_t kCst3 = vdupq_n_s8(0x03);
390    const int8x16_t kCst4 = vdupq_n_s8(0x04);
391    const int8x16_t delta_p3 = vqaddq_s8(delta, kCst3);
392    const int8x16_t delta_p4 = vqaddq_s8(delta, kCst4);
393    const int8x16_t delta3 = vshrq_n_s8(delta_p3, 3);
394    const int8x16_t delta4 = vshrq_n_s8(delta_p4, 3);
395    const int8x16_t sp0 = vqaddq_s8(p0s, delta3);
396    const int8x16_t sq0 = vqsubq_s8(q0s, delta4);
397    *op0 = FlipSignBack_NEON(sp0);
398    *oq0 = FlipSignBack_NEON(sq0);
399  }
400  static void DoFilter2_NEON(const uint8x16_t p1, const uint8x16_t p0,
401                             const uint8x16_t q0, const uint8x16_t q1,
402                             const uint8x16_t mask,
403                             uint8x16_t* const op0, uint8x16_t* const oq0) {
404    const int8x16_t p1s = FlipSign_NEON(p1);
405    const int8x16_t p0s = FlipSign_NEON(p0);
406    const int8x16_t q0s = FlipSign_NEON(q0);
407    const int8x16_t q1s = FlipSign_NEON(q1);
408    const int8x16_t delta0 = GetBaseDelta_NEON(p1s, p0s, q0s, q1s);
409    const int8x16_t delta1 = vandq_s8(delta0, vreinterpretq_s8_u8(mask));
410    ApplyFilter2_NEON(p0s, q0s, delta1, op0, oq0);
411  }
412  static void SimpleVFilter16_NEON(uint8_t* p, int stride, int thresh) {
413    uint8x16_t p1, p0, q0, q1, op0, oq0;
414    Load16x4_NEON(p, stride, &p1, &p0, &q0, &q1);
415    {
416      const uint8x16_t mask = NeedsFilter_NEON(p1, p0, q0, q1, thresh);
417      DoFilter2_NEON(p1, p0, q0, q1, mask, &op0, &oq0);
418    }
419    Store16x2_NEON(op0, oq0, p, stride);
420  }
421  static void SimpleHFilter16_NEON(uint8_t* p, int stride, int thresh) {
422    uint8x16_t p1, p0, q0, q1, oq0, op0;
423    Load4x16_NEON(p, stride, &p1, &p0, &q0, &q1);
424    {
425      const uint8x16_t mask = NeedsFilter_NEON(p1, p0, q0, q1, thresh);
426      DoFilter2_NEON(p1, p0, q0, q1, mask, &op0, &oq0);
427    }
428    Store2x16_NEON(op0, oq0, p, stride);
429  }
430  #else
431  #define LOAD8x4(c1, c2, c3, c4, b1, b2, stride)                                \
432    "vld4.8 {" #c1 "[0]," #c2 "[0]," #c3 "[0]," #c4 "[0]}," #b1 "," #stride "\n" \
433    "vld4.8 {" #c1 "[1]," #c2 "[1]," #c3 "[1]," #c4 "[1]}," #b2 "," #stride "\n" \
434    "vld4.8 {" #c1 "[2]," #c2 "[2]," #c3 "[2]," #c4 "[2]}," #b1 "," #stride "\n" \
435    "vld4.8 {" #c1 "[3]," #c2 "[3]," #c3 "[3]," #c4 "[3]}," #b2 "," #stride "\n" \
436    "vld4.8 {" #c1 "[4]," #c2 "[4]," #c3 "[4]," #c4 "[4]}," #b1 "," #stride "\n" \
437    "vld4.8 {" #c1 "[5]," #c2 "[5]," #c3 "[5]," #c4 "[5]}," #b2 "," #stride "\n" \
438    "vld4.8 {" #c1 "[6]," #c2 "[6]," #c3 "[6]," #c4 "[6]}," #b1 "," #stride "\n" \
439    "vld4.8 {" #c1 "[7]," #c2 "[7]," #c3 "[7]," #c4 "[7]}," #b2 "," #stride "\n"
440  #define STORE8x2(c1, c2, p, stride)                                            \
441    "vst2.8   {" #c1 "[0], " #c2 "[0]}," #p "," #stride " \n"                    \
442    "vst2.8   {" #c1 "[1], " #c2 "[1]}," #p "," #stride " \n"                    \
443    "vst2.8   {" #c1 "[2], " #c2 "[2]}," #p "," #stride " \n"                    \
444    "vst2.8   {" #c1 "[3], " #c2 "[3]}," #p "," #stride " \n"                    \
445    "vst2.8   {" #c1 "[4], " #c2 "[4]}," #p "," #stride " \n"                    \
446    "vst2.8   {" #c1 "[5], " #c2 "[5]}," #p "," #stride " \n"                    \
447    "vst2.8   {" #c1 "[6], " #c2 "[6]}," #p "," #stride " \n"                    \
448    "vst2.8   {" #c1 "[7], " #c2 "[7]}," #p "," #stride " \n"
449  #define QRegs "q0", "q1", "q2", "q3",                                          \
450                "q8", "q9", "q10", "q11", "q12", "q13", "q14", "q15"
451  #define FLIP_SIGN_BIT2(a, b, s)                                                \
452    "veor     " #a "," #a "," #s "               \n"                             \
453    "veor     " #b "," #b "," #s "               \n"                             \
454  
455  #define FLIP_SIGN_BIT4(a, b, c, d, s)                                          \
456    FLIP_SIGN_BIT2(a, b, s)                                                      \
457    FLIP_SIGN_BIT2(c, d, s)                                                      \
458  
459  #define NEEDS_FILTER(p1, p0, q0, q1, thresh, mask)                             \
460    "vabd.u8    q15," #p0 "," #q0 "         \n"  &bsol;* abs(p0 - q0) */              \
461    "vabd.u8    q14," #p1 "," #q1 "         \n"  &bsol;* abs(p1 - q1) */              \
462    "vqadd.u8   q15, q15, q15               \n"  &bsol;* abs(p0 - q0) * 2 */          \
463    "vshr.u8    q14, q14, #1                \n"  &bsol;* abs(p1 - q1) / 2 */          \
464    "vqadd.u8   q15, q15, q14     \n"  &bsol;* abs(p0 - q0) * 2 + abs(p1 - q1) / 2 */ \
465    "vdup.8     q14, " #thresh "            \n"                                  \
466    "vcge.u8   " #mask ", q14, q15          \n"  &bsol;* mask <= thresh */
467  #define GET_BASE_DELTA(p1, p0, q0, q1, o)                                      \
468    "vqsub.s8   q15," #q0 "," #p0 "         \n"  &bsol;* (q0 - p0) */                 \
469    "vqsub.s8  " #o "," #p1 "," #q1 "       \n"  &bsol;* (p1 - q1) */                 \
470    "vqadd.s8  " #o "," #o ", q15           \n"  &bsol;* (p1 - q1) + 1 * (p0 - q0) */ \
471    "vqadd.s8  " #o "," #o ", q15           \n"  &bsol;* (p1 - q1) + 2 * (p0 - q0) */ \
472    "vqadd.s8  " #o "," #o ", q15           \n"  &bsol;* (p1 - q1) + 3 * (p0 - q0) */
473  #define DO_SIMPLE_FILTER(p0, q0, fl)                                           \
474    "vmov.i8    q15, #0x03                  \n"                                  \
475    "vqadd.s8   q15, q15, " #fl "           \n"  &bsol;* filter1 = filter + 3 */      \
476    "vshr.s8    q15, q15, #3                \n"  &bsol;* filter1 >> 3 */              \
477    "vqadd.s8  " #p0 "," #p0 ", q15         \n"  &bsol;* p0 += filter1 */             \
478                                                                                 \
479    "vmov.i8    q15, #0x04                  \n"                                  \
480    "vqadd.s8   q15, q15, " #fl "           \n"  &bsol;* filter1 = filter + 4 */      \
481    "vshr.s8    q15, q15, #3                \n"  &bsol;* filter2 >> 3 */              \
482    "vqsub.s8  " #q0 "," #q0 ", q15         \n"  &bsol;* q0 -= filter2 */
483  #define DO_FILTER2(p1, p0, q0, q1, thresh)                                     \
484    NEEDS_FILTER(p1, p0, q0, q1, thresh, q9)     &bsol;* filter mask in q9 */         \
485    "vmov.i8    q10, #0x80                  \n"  &bsol;* sign bit */                  \
486    FLIP_SIGN_BIT4(p1, p0, q0, q1, q10)          &bsol;* convert to signed value */   \
487    GET_BASE_DELTA(p1, p0, q0, q1, q11)          &bsol;* get filter level  */         \
488    "vand       q9, q9, q11                 \n"  &bsol;* apply filter mask */         \
489    DO_SIMPLE_FILTER(p0, q0, q9)                 &bsol;* apply filter */              \
490    FLIP_SIGN_BIT2(p0, q0, q10)
491  static void SimpleVFilter16_NEON(uint8_t* p, int stride, int thresh) {
492    __asm__ volatile (
493      "sub        %[p], %[p], %[stride], lsl #1  \n"  
494      "vld1.u8    {q1}, [%[p]], %[stride]        \n"  
495      "vld1.u8    {q2}, [%[p]], %[stride]        \n"  
496      "vld1.u8    {q3}, [%[p]], %[stride]        \n"  
497      "vld1.u8    {q12}, [%[p]]                  \n"  
498      DO_FILTER2(q1, q2, q3, q12, %[thresh])
499      "sub        %[p], %[p], %[stride], lsl #1  \n"  
500      "vst1.u8    {q2}, [%[p]], %[stride]        \n"  
501      "vst1.u8    {q3}, [%[p]]                   \n"  
502      : [p] "+r"(p)
503      : [stride] "r"(stride), [thresh] "r"(thresh)
504      : "memory", QRegs
505    );
506  }
507  static void SimpleHFilter16_NEON(uint8_t* p, int stride, int thresh) {
508    __asm__ volatile (
509      "sub        r4, %[p], #2                   \n"  
510      "lsl        r6, %[stride], #1              \n"  
511      "add        r5, r4, %[stride]              \n"  
512      LOAD8x4(d2, d3, d4, d5, [r4], [r5], r6)
513      LOAD8x4(d24, d25, d26, d27, [r4], [r5], r6)
514      "vswp       d3, d24                        \n"  
515      "vswp       d5, d26                        \n"  
516      "vswp       q2, q12                        \n"  
517      DO_FILTER2(q1, q2, q12, q13, %[thresh])
518      "sub        %[p], %[p], #1                 \n"  
519      "vswp        d5, d24                       \n"
520      STORE8x2(d4, d5, [%[p]], %[stride])
521      STORE8x2(d24, d25, [%[p]], %[stride])
522      : [p] "+r"(p)
523      : [stride] "r"(stride), [thresh] "r"(thresh)
524      : "memory", "r4", "r5", "r6", QRegs
525    );
526  }
527  #undef LOAD8x4
528  #undef STORE8x2
529  #endif    
530  static void SimpleVFilter16i_NEON(uint8_t* p, int stride, int thresh) {
531    uint32_t k;
532    for (k = 3; k != 0; --k) {
533      p += 4 * stride;
534      SimpleVFilter16_NEON(p, stride, thresh);
535    }
536  }
537  static void SimpleHFilter16i_NEON(uint8_t* p, int stride, int thresh) {
538    uint32_t k;
539    for (k = 3; k != 0; --k) {
540      p += 4;
541      SimpleHFilter16_NEON(p, stride, thresh);
542    }
543  }
544  static uint8x16_t NeedsHev_NEON(const uint8x16_t p1, const uint8x16_t p0,
545                                  const uint8x16_t q0, const uint8x16_t q1,
546                                  int hev_thresh) {
547    const uint8x16_t hev_thresh_v = vdupq_n_u8((uint8_t)hev_thresh);
548    const uint8x16_t a_p1_p0 = vabdq_u8(p1, p0);  
549    const uint8x16_t a_q1_q0 = vabdq_u8(q1, q0);  
550    const uint8x16_t a_max = vmaxq_u8(a_p1_p0, a_q1_q0);
551    const uint8x16_t mask = vcgtq_u8(a_max, hev_thresh_v);
552    return mask;
553  }
554  static uint8x16_t NeedsFilter2_NEON(const uint8x16_t p3, const uint8x16_t p2,
555                                      const uint8x16_t p1, const uint8x16_t p0,
556                                      const uint8x16_t q0, const uint8x16_t q1,
557                                      const uint8x16_t q2, const uint8x16_t q3,
558                                      int ithresh, int thresh) {
559    const uint8x16_t ithresh_v = vdupq_n_u8((uint8_t)ithresh);
560    const uint8x16_t a_p3_p2 = vabdq_u8(p3, p2);  
561    const uint8x16_t a_p2_p1 = vabdq_u8(p2, p1);  
562    const uint8x16_t a_p1_p0 = vabdq_u8(p1, p0);  
563    const uint8x16_t a_q3_q2 = vabdq_u8(q3, q2);  
564    const uint8x16_t a_q2_q1 = vabdq_u8(q2, q1);  
565    const uint8x16_t a_q1_q0 = vabdq_u8(q1, q0);  
566    const uint8x16_t max1 = vmaxq_u8(a_p3_p2, a_p2_p1);
567    const uint8x16_t max2 = vmaxq_u8(a_p1_p0, a_q3_q2);
568    const uint8x16_t max3 = vmaxq_u8(a_q2_q1, a_q1_q0);
569    const uint8x16_t max12 = vmaxq_u8(max1, max2);
570    const uint8x16_t max123 = vmaxq_u8(max12, max3);
571    const uint8x16_t mask2 = vcgeq_u8(ithresh_v, max123);
572    const uint8x16_t mask1 = NeedsFilter_NEON(p1, p0, q0, q1, thresh);
573    const uint8x16_t mask = vandq_u8(mask1, mask2);
574    return mask;
575  }
576  static void ApplyFilter4_NEON(
577      const int8x16_t p1, const int8x16_t p0,
578      const int8x16_t q0, const int8x16_t q1,
579      const int8x16_t delta0,
580      uint8x16_t* const op1, uint8x16_t* const op0,
581      uint8x16_t* const oq0, uint8x16_t* const oq1) {
582    const int8x16_t kCst3 = vdupq_n_s8(0x03);
583    const int8x16_t kCst4 = vdupq_n_s8(0x04);
584    const int8x16_t delta1 = vqaddq_s8(delta0, kCst4);
585    const int8x16_t delta2 = vqaddq_s8(delta0, kCst3);
586    const int8x16_t a1 = vshrq_n_s8(delta1, 3);
587    const int8x16_t a2 = vshrq_n_s8(delta2, 3);
588    const int8x16_t a3 = vrshrq_n_s8(a1, 1);   
589    *op0 = FlipSignBack_NEON(vqaddq_s8(p0, a2));  
590    *oq0 = FlipSignBack_NEON(vqsubq_s8(q0, a1));  
591    *op1 = FlipSignBack_NEON(vqaddq_s8(p1, a3));  
592    *oq1 = FlipSignBack_NEON(vqsubq_s8(q1, a3));  
593  }
594  static void DoFilter4_NEON(
595      const uint8x16_t p1, const uint8x16_t p0,
596      const uint8x16_t q0, const uint8x16_t q1,
597      const uint8x16_t mask, const uint8x16_t hev_mask,
598      uint8x16_t* const op1, uint8x16_t* const op0,
599      uint8x16_t* const oq0, uint8x16_t* const oq1) {
600    const int8x16_t p1s = FlipSign_NEON(p1);
601    int8x16_t p0s = FlipSign_NEON(p0);
602    int8x16_t q0s = FlipSign_NEON(q0);
603    const int8x16_t q1s = FlipSign_NEON(q1);
604    const uint8x16_t simple_lf_mask = vandq_u8(mask, hev_mask);
605    {
606      const int8x16_t delta = GetBaseDelta_NEON(p1s, p0s, q0s, q1s);
607      const int8x16_t simple_lf_delta =
608          vandq_s8(delta, vreinterpretq_s8_u8(simple_lf_mask));
609      ApplyFilter2NoFlip_NEON(p0s, q0s, simple_lf_delta, &p0s, &q0s);
610    }
611    {
612      const int8x16_t delta0 = GetBaseDelta0_NEON(p0s, q0s);
613      const uint8x16_t complex_lf_mask = veorq_u8(simple_lf_mask, mask);
614      const int8x16_t complex_lf_delta =
615          vandq_s8(delta0, vreinterpretq_s8_u8(complex_lf_mask));
616      ApplyFilter4_NEON(p1s, p0s, q0s, q1s, complex_lf_delta, op1, op0, oq0, oq1);
617    }
618  }
619  static void ApplyFilter6_NEON(
620      const int8x16_t p2, const int8x16_t p1, const int8x16_t p0,
621      const int8x16_t q0, const int8x16_t q1, const int8x16_t q2,
622      const int8x16_t delta,
623      uint8x16_t* const op2, uint8x16_t* const op1, uint8x16_t* const op0,
624      uint8x16_t* const oq0, uint8x16_t* const oq1, uint8x16_t* const oq2) {
625    const int8x8_t delta_lo = vget_low_s8(delta);
626    const int8x8_t delta_hi = vget_high_s8(delta);
627    const int8x8_t kCst9 = vdup_n_s8(9);
628    const int16x8_t kCstm1 = vdupq_n_s16(-1);
629    const int8x8_t kCst18 = vdup_n_s8(18);
630    const int16x8_t S_lo = vmlal_s8(kCstm1, kCst9, delta_lo);  
631    const int16x8_t S_hi = vmlal_s8(kCstm1, kCst9, delta_hi);
632    const int16x8_t Z_lo = vmlal_s8(S_lo, kCst18, delta_lo);   
633    const int16x8_t Z_hi = vmlal_s8(S_hi, kCst18, delta_hi);
634    const int8x8_t a3_lo = vqrshrn_n_s16(S_lo, 7);   
635    const int8x8_t a3_hi = vqrshrn_n_s16(S_hi, 7);
636    const int8x8_t a2_lo = vqrshrn_n_s16(S_lo, 6);   
637    const int8x8_t a2_hi = vqrshrn_n_s16(S_hi, 6);
638    const int8x8_t a1_lo = vqrshrn_n_s16(Z_lo, 7);   
639    const int8x8_t a1_hi = vqrshrn_n_s16(Z_hi, 7);
640    const int8x16_t a1 = vcombine_s8(a1_lo, a1_hi);
641    const int8x16_t a2 = vcombine_s8(a2_lo, a2_hi);
642    const int8x16_t a3 = vcombine_s8(a3_lo, a3_hi);
643    *op0 = FlipSignBack_NEON(vqaddq_s8(p0, a1));  
644    *oq0 = FlipSignBack_NEON(vqsubq_s8(q0, a1));  
645    *oq1 = FlipSignBack_NEON(vqsubq_s8(q1, a2));  
646    *op1 = FlipSignBack_NEON(vqaddq_s8(p1, a2));  
647    *oq2 = FlipSignBack_NEON(vqsubq_s8(q2, a3));  
648    *op2 = FlipSignBack_NEON(vqaddq_s8(p2, a3));  
649  }
650  static void DoFilter6_NEON(
651      const uint8x16_t p2, const uint8x16_t p1, const uint8x16_t p0,
652      const uint8x16_t q0, const uint8x16_t q1, const uint8x16_t q2,
653      const uint8x16_t mask, const uint8x16_t hev_mask,
654      uint8x16_t* const op2, uint8x16_t* const op1, uint8x16_t* const op0,
655      uint8x16_t* const oq0, uint8x16_t* const oq1, uint8x16_t* const oq2) {
656    const int8x16_t p2s = FlipSign_NEON(p2);
657    const int8x16_t p1s = FlipSign_NEON(p1);
658    int8x16_t p0s = FlipSign_NEON(p0);
659    int8x16_t q0s = FlipSign_NEON(q0);
660    const int8x16_t q1s = FlipSign_NEON(q1);
661    const int8x16_t q2s = FlipSign_NEON(q2);
662    const uint8x16_t simple_lf_mask = vandq_u8(mask, hev_mask);
663    const int8x16_t delta0 = GetBaseDelta_NEON(p1s, p0s, q0s, q1s);
664    {
665      const int8x16_t simple_lf_delta =
666          vandq_s8(delta0, vreinterpretq_s8_u8(simple_lf_mask));
667      ApplyFilter2NoFlip_NEON(p0s, q0s, simple_lf_delta, &p0s, &q0s);
668    }
669    {
670      const uint8x16_t complex_lf_mask = veorq_u8(simple_lf_mask, mask);
671      const int8x16_t complex_lf_delta =
672          vandq_s8(delta0, vreinterpretq_s8_u8(complex_lf_mask));
673      ApplyFilter6_NEON(p2s, p1s, p0s, q0s, q1s, q2s, complex_lf_delta,
674                        op2, op1, op0, oq0, oq1, oq2);
675    }
676  }
677  static void VFilter16_NEON(uint8_t* p, int stride,
678                             int thresh, int ithresh, int hev_thresh) {
679    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
680    Load16x8_NEON(p, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
681    {
682      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
683                                                ithresh, thresh);
684      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
685      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
686      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
687                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
688      Store16x2_NEON(op2, op1, p - 2 * stride, stride);
689      Store16x2_NEON(op0, oq0, p + 0 * stride, stride);
690      Store16x2_NEON(oq1, oq2, p + 2 * stride, stride);
691    }
692  }
693  static void HFilter16_NEON(uint8_t* p, int stride,
694                             int thresh, int ithresh, int hev_thresh) {
695    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
696    Load8x16_NEON(p, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
697    {
698      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
699                                                ithresh, thresh);
700      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
701      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
702      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
703                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
704      Store2x16_NEON(op2, op1, p - 2, stride);
705      Store2x16_NEON(op0, oq0, p + 0, stride);
706      Store2x16_NEON(oq1, oq2, p + 2, stride);
707    }
708  }
709  static void VFilter16i_NEON(uint8_t* p, int stride,
710                              int thresh, int ithresh, int hev_thresh) {
711    uint32_t k;
712    uint8x16_t p3, p2, p1, p0;
713    Load16x4_NEON(p + 2  * stride, stride, &p3, &p2, &p1, &p0);
714    for (k = 3; k != 0; --k) {
715      uint8x16_t q0, q1, q2, q3;
716      p += 4 * stride;
717      Load16x4_NEON(p + 2  * stride, stride, &q0, &q1, &q2, &q3);
718      {
719        const uint8x16_t mask =
720            NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3, ithresh, thresh);
721        const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
722        DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &p1, &p0, &p3, &p2);
723        Store16x4_NEON(p1, p0, p3, p2, p, stride);
724        p1 = q2;
725        p0 = q3;
726      }
727    }
728  }
729  #if !defined(WORK_AROUND_GCC)
730  static void HFilter16i_NEON(uint8_t* p, int stride,
731                              int thresh, int ithresh, int hev_thresh) {
732    uint32_t k;
733    uint8x16_t p3, p2, p1, p0;
734    Load4x16_NEON(p + 2, stride, &p3, &p2, &p1, &p0);
735    for (k = 3; k != 0; --k) {
736      uint8x16_t q0, q1, q2, q3;
737      p += 4;
738      Load4x16_NEON(p + 2, stride, &q0, &q1, &q2, &q3);
739      {
740        const uint8x16_t mask =
741            NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3, ithresh, thresh);
742        const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
743        DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &p1, &p0, &p3, &p2);
744        Store4x16_NEON(p1, p0, p3, p2, p, stride);
745        p1 = q2;
746        p0 = q3;
747      }
748    }
749  }
750  #endif  
751  static void VFilter8_NEON(uint8_t* u, uint8_t* v, int stride,
752                            int thresh, int ithresh, int hev_thresh) {
753    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
754    Load8x8x2_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
755    {
756      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
757                                                ithresh, thresh);
758      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
759      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
760      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
761                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
762      Store8x2x2_NEON(op2, op1, u - 2 * stride, v - 2 * stride, stride);
763      Store8x2x2_NEON(op0, oq0, u + 0 * stride, v + 0 * stride, stride);
764      Store8x2x2_NEON(oq1, oq2, u + 2 * stride, v + 2 * stride, stride);
765    }
766  }
767  static void VFilter8i_NEON(uint8_t* u, uint8_t* v, int stride,
768                             int thresh, int ithresh, int hev_thresh) {
769    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
770    u += 4 * stride;
771    v += 4 * stride;
772    Load8x8x2_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
773    {
774      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
775                                                ithresh, thresh);
776      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
777      uint8x16_t op1, op0, oq0, oq1;
778      DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &op1, &op0, &oq0, &oq1);
779      Store8x4x2_NEON(op1, op0, oq0, oq1, u, v, stride);
780    }
781  }
782  #if !defined(WORK_AROUND_GCC)
783  static void HFilter8_NEON(uint8_t* u, uint8_t* v, int stride,
784                            int thresh, int ithresh, int hev_thresh) {
785    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
786    Load8x8x2T_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
787    {
788      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
789                                                ithresh, thresh);
790      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
791      uint8x16_t op2, op1, op0, oq0, oq1, oq2;
792      DoFilter6_NEON(p2, p1, p0, q0, q1, q2, mask, hev_mask,
793                     &op2, &op1, &op0, &oq0, &oq1, &oq2);
794      Store6x8x2_NEON(op2, op1, op0, oq0, oq1, oq2, u, v, stride);
795    }
796  }
797  static void HFilter8i_NEON(uint8_t* u, uint8_t* v, int stride,
798                             int thresh, int ithresh, int hev_thresh) {
799    uint8x16_t p3, p2, p1, p0, q0, q1, q2, q3;
800    u += 4;
801    v += 4;
802    Load8x8x2T_NEON(u, v, stride, &p3, &p2, &p1, &p0, &q0, &q1, &q2, &q3);
803    {
804      const uint8x16_t mask = NeedsFilter2_NEON(p3, p2, p1, p0, q0, q1, q2, q3,
805                                                ithresh, thresh);
806      const uint8x16_t hev_mask = NeedsHev_NEON(p1, p0, q0, q1, hev_thresh);
807      uint8x16_t op1, op0, oq0, oq1;
808      DoFilter4_NEON(p1, p0, q0, q1, mask, hev_mask, &op1, &op0, &oq0, &oq1);
809      Store4x8x2_NEON(op1, op0, oq0, oq1, u, v, stride);
810    }
811  }
812  #endif  
813  static const int16_t kC1 = 20091;
814  static const int16_t kC2 = 17734;  
815  #if defined(WEBP_USE_INTRINSICS)
816  static WEBP_INLINE void Transpose8x2_NEON(const int16x8_t in0,
817                                            const int16x8_t in1,
818                                            int16x8x2_t* const out) {
819    const int16x8x2_t tmp0 = vzipq_s16(in0, in1);   
820    *out = vzipq_s16(tmp0.val[0], tmp0.val[1]);
821  }
822  static WEBP_INLINE void TransformPass_NEON(int16x8x2_t* const rows) {
823    const int16x8_t B1 =
824        vcombine_s16(vget_high_s16(rows->val[0]), vget_high_s16(rows->val[1]));
825    const int16x8_t C0 = vsraq_n_s16(B1, vqdmulhq_n_s16(B1, kC1), 1);
826    const int16x8_t C1 = vqdmulhq_n_s16(B1, kC2);
827    const int16x4_t a = vqadd_s16(vget_low_s16(rows->val[0]),
828                                  vget_low_s16(rows->val[1]));   
829    const int16x4_t b = vqsub_s16(vget_low_s16(rows->val[0]),
830                                  vget_low_s16(rows->val[1]));   
831    const int16x4_t c = vqsub_s16(vget_low_s16(C1), vget_high_s16(C0));
832    const int16x4_t d = vqadd_s16(vget_low_s16(C0), vget_high_s16(C1));
833    const int16x8_t D0 = vcombine_s16(a, b);      
834    const int16x8_t D1 = vcombine_s16(d, c);      
835    const int16x8_t E0 = vqaddq_s16(D0, D1);      
836    const int16x8_t E_tmp = vqsubq_s16(D0, D1);   
837    const int16x8_t E1 = vcombine_s16(vget_high_s16(E_tmp), vget_low_s16(E_tmp));
838    Transpose8x2_NEON(E0, E1, rows);
839  }
840  static void TransformOne_NEON(const int16_t* in, uint8_t* dst) {
841    int16x8x2_t rows;
842    INIT_VECTOR2(rows, vld1q_s16(in + 0), vld1q_s16(in + 8));
843    TransformPass_NEON(&rows);
844    TransformPass_NEON(&rows);
845    Add4x4_NEON(rows.val[0], rows.val[1], dst);
846  }
847  #else
848  static void TransformOne_NEON(const int16_t* in, uint8_t* dst) {
849    const int kBPS = BPS;
850    const int16_t constants[4] = { kC1, kC2, 0, 0 };
851    __asm__ volatile (
852      "vld1.16         {q1, q2}, [%[in]]           \n"
853      "vld1.16         {d0}, [%[constants]]        \n"
854      "vswp            d3, d4                      \n"
855      "vqdmulh.s16     q8, q2, d0[0]               \n"
856      "vqdmulh.s16     q9, q2, d0[1]               \n"
857      "vqadd.s16       d22, d2, d3                 \n"
858      "vqsub.s16       d23, d2, d3                 \n"
859      "vshr.s16        q8, q8, #1                  \n"
860      "vqadd.s16       q8, q2, q8                  \n"
861      "vqsub.s16       d20, d18, d17               \n"
862      "vqadd.s16       d21, d19, d16               \n"
863      "vqadd.s16       d2, d22, d21                \n"
864      "vqadd.s16       d3, d23, d20                \n"
865      "vqsub.s16       d4, d23, d20                \n"
866      "vqsub.s16       d5, d22, d21                \n"
867      "vzip.16         q1, q2                      \n"
868      "vzip.16         q1, q2                      \n"
869      "vswp            d3, d4                      \n"
870      "vqdmulh.s16     q8, q2, d0[0]               \n"
871      "vqdmulh.s16     q9, q2, d0[1]               \n"
872      "vqadd.s16       d22, d2, d3                 \n"
873      "vqsub.s16       d23, d2, d3                 \n"
874      "vshr.s16        q8, q8, #1                  \n"
875      "vqadd.s16       q8, q2, q8                  \n"
876      "vqsub.s16       d20, d18, d17               \n"
877      "vqadd.s16       d21, d19, d16               \n"
878      "vqadd.s16       d2, d22, d21                \n"
879      "vqadd.s16       d3, d23, d20                \n"
880      "vqsub.s16       d4, d23, d20                \n"
881      "vqsub.s16       d5, d22, d21                \n"
882      "vld1.32         d6[0], [%[dst]], %[kBPS]    \n"
883      "vld1.32         d6[1], [%[dst]], %[kBPS]    \n"
884      "vld1.32         d7[0], [%[dst]], %[kBPS]    \n"
885      "vld1.32         d7[1], [%[dst]], %[kBPS]    \n"
886      "sub         %[dst], %[dst], %[kBPS], lsl #2 \n"
887      "vrshr.s16       d2, d2, #3                  \n"
888      "vrshr.s16       d3, d3, #3                  \n"
889      "vrshr.s16       d4, d4, #3                  \n"
890      "vrshr.s16       d5, d5, #3                  \n"
891      "vzip.16         q1, q2                      \n"
892      "vzip.16         q1, q2                      \n"
893      "vmovl.u8        q8, d6                      \n"
894      "vmovl.u8        q9, d7                      \n"
895      "vqadd.s16       q1, q1, q8                  \n"
896      "vqadd.s16       q2, q2, q9                  \n"
897      "vqmovun.s16     d0, q1                      \n"
898      "vqmovun.s16     d1, q2                      \n"
899      "vst1.32         d0[0], [%[dst]], %[kBPS]    \n"
900      "vst1.32         d0[1], [%[dst]], %[kBPS]    \n"
901      "vst1.32         d1[0], [%[dst]], %[kBPS]    \n"
902      "vst1.32         d1[1], [%[dst]]             \n"
903      : [in] "+r"(in), [dst] "+r"(dst)  &bsol;* modified registers */
904      : [kBPS] "r"(kBPS), [constants] "r"(constants)  &bsol;* constants */
905      : "memory", "q0", "q1", "q2", "q8", "q9", "q10", "q11"  &bsol;* clobbered */
906    );
907  }
908  #endif    
909  static void TransformTwo_NEON(const int16_t* in, uint8_t* dst, int do_two) {
910    TransformOne_NEON(in, dst);
911    if (do_two) {
912      TransformOne_NEON(in + 16, dst + 4);
913    }
914  }
915  static void TransformDC_NEON(const int16_t* in, uint8_t* dst) {
916    const int16x8_t DC = vdupq_n_s16(in[0]);
917    Add4x4_NEON(DC, DC, dst);
918  }
919  #define STORE_WHT(dst, col, rows) do {                  \
920    *dst = vgetq_lane_s32(rows.val[0], col); (dst) += 16; \
921    *dst = vgetq_lane_s32(rows.val[1], col); (dst) += 16; \
922    *dst = vgetq_lane_s32(rows.val[2], col); (dst) += 16; \
923    *dst = vgetq_lane_s32(rows.val[3], col); (dst) += 16; \
924  } while (0)
925  static void TransformWHT_NEON(const int16_t* in, int16_t* out) {
926    int32x4x4_t tmp;
927    {
928      const int16x4_t in00_03 = vld1_s16(in + 0);
929      const int16x4_t in04_07 = vld1_s16(in + 4);
930      const int16x4_t in08_11 = vld1_s16(in + 8);
931      const int16x4_t in12_15 = vld1_s16(in + 12);
932      const int32x4_t a0 = vaddl_s16(in00_03, in12_15);  
933      const int32x4_t a1 = vaddl_s16(in04_07, in08_11);  
934      const int32x4_t a2 = vsubl_s16(in04_07, in08_11);  
935      const int32x4_t a3 = vsubl_s16(in00_03, in12_15);  
936      tmp.val[0] = vaddq_s32(a0, a1);
937      tmp.val[1] = vaddq_s32(a3, a2);
938      tmp.val[2] = vsubq_s32(a0, a1);
939      tmp.val[3] = vsubq_s32(a3, a2);
940      tmp = Transpose4x4_NEON(tmp);
941    }
942    {
943      const int32x4_t kCst3 = vdupq_n_s32(3);
944      const int32x4_t dc = vaddq_s32(tmp.val[0], kCst3);  
945      const int32x4_t a0 = vaddq_s32(dc, tmp.val[3]);
946      const int32x4_t a1 = vaddq_s32(tmp.val[1], tmp.val[2]);
947      const int32x4_t a2 = vsubq_s32(tmp.val[1], tmp.val[2]);
948      const int32x4_t a3 = vsubq_s32(dc, tmp.val[3]);
949      tmp.val[0] = vaddq_s32(a0, a1);
950      tmp.val[1] = vaddq_s32(a3, a2);
951      tmp.val[2] = vsubq_s32(a0, a1);
952      tmp.val[3] = vsubq_s32(a3, a2);
953      tmp.val[0] = vshrq_n_s32(tmp.val[0], 3);
954      tmp.val[1] = vshrq_n_s32(tmp.val[1], 3);
955      tmp.val[2] = vshrq_n_s32(tmp.val[2], 3);
956      tmp.val[3] = vshrq_n_s32(tmp.val[3], 3);
957      STORE_WHT(out, 0, tmp);
958      STORE_WHT(out, 1, tmp);
959      STORE_WHT(out, 2, tmp);
960      STORE_WHT(out, 3, tmp);
961    }
962  }
963  #undef STORE_WHT
964  #define MUL(a, b) (((a) * (b)) >> 16)
965  static void TransformAC3_NEON(const int16_t* in, uint8_t* dst) {
966    static const int kC1_full = 20091 + (1 << 16);
967    static const int kC2_full = 35468;
968    const int16x4_t A = vld1_dup_s16(in);
969    const int16x4_t c4 = vdup_n_s16(MUL(in[4], kC2_full));
970    const int16x4_t d4 = vdup_n_s16(MUL(in[4], kC1_full));
971    const int c1 = MUL(in[1], kC2_full);
972    const int d1 = MUL(in[1], kC1_full);
973    const uint64_t cd = (uint64_t)( d1 & 0xffff) <<  0 |
974                        (uint64_t)( c1 & 0xffff) << 16 |
975                        (uint64_t)(-c1 & 0xffff) << 32 |
976                        (uint64_t)(-d1 & 0xffff) << 48;
977    const int16x4_t CD = vcreate_s16(cd);
978    const int16x4_t B = vqadd_s16(A, CD);
979    const int16x8_t m0_m1 = vcombine_s16(vqadd_s16(B, d4), vqadd_s16(B, c4));
980    const int16x8_t m2_m3 = vcombine_s16(vqsub_s16(B, c4), vqsub_s16(B, d4));
981    Add4x4_NEON(m0_m1, m2_m3, dst);
982  }
983  #undef MUL
984  static void DC4_NEON(uint8_t* dst) {    
985    const uint8x8_t A = vld1_u8(dst - BPS);  
986    const uint16x4_t p0 = vpaddl_u8(A);  
987    const uint16x4_t p1 = vpadd_u16(p0, p0);
988    const uint16x8_t L0 = vmovl_u8(vld1_u8(dst + 0 * BPS - 1));
989    const uint16x8_t L1 = vmovl_u8(vld1_u8(dst + 1 * BPS - 1));
990    const uint16x8_t L2 = vmovl_u8(vld1_u8(dst + 2 * BPS - 1));
991    const uint16x8_t L3 = vmovl_u8(vld1_u8(dst + 3 * BPS - 1));
992    const uint16x8_t s0 = vaddq_u16(L0, L1);
993    const uint16x8_t s1 = vaddq_u16(L2, L3);
994    const uint16x8_t s01 = vaddq_u16(s0, s1);
995    const uint16x8_t sum = vaddq_u16(s01, vcombine_u16(p1, p1));
996    const uint8x8_t dc0 = vrshrn_n_u16(sum, 3);  
997    const uint8x8_t dc = vdup_lane_u8(dc0, 0);
998    int i;
999    for (i = 0; i < 4; ++i) {
1000      vst1_lane_u32((uint32_t*)(dst + i * BPS), vreinterpret_u32_u8(dc), 0);
1001    }
1002  }
1003  static WEBP_INLINE void TrueMotion_NEON(uint8_t* dst, int size) {
1004    const uint8x8_t TL = vld1_dup_u8(dst - BPS - 1);  
1005    const uint8x8_t T = vld1_u8(dst - BPS);  
1006    const int16x8_t d = vreinterpretq_s16_u16(vsubl_u8(T, TL));  
1007    int y;
1008    for (y = 0; y < size; y += 4) {
1009      const int16x8_t L0 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 0 * BPS - 1));
1010      const int16x8_t L1 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 1 * BPS - 1));
1011      const int16x8_t L2 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 2 * BPS - 1));
1012      const int16x8_t L3 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 3 * BPS - 1));
1013      const int16x8_t r0 = vaddq_s16(L0, d);  
1014      const int16x8_t r1 = vaddq_s16(L1, d);
1015      const int16x8_t r2 = vaddq_s16(L2, d);
1016      const int16x8_t r3 = vaddq_s16(L3, d);
1017      const uint32x2_t r0_u32 = vreinterpret_u32_u8(vqmovun_s16(r0));
1018      const uint32x2_t r1_u32 = vreinterpret_u32_u8(vqmovun_s16(r1));
1019      const uint32x2_t r2_u32 = vreinterpret_u32_u8(vqmovun_s16(r2));
1020      const uint32x2_t r3_u32 = vreinterpret_u32_u8(vqmovun_s16(r3));
1021      if (size == 4) {
1022        vst1_lane_u32((uint32_t*)(dst + 0 * BPS), r0_u32, 0);
1023        vst1_lane_u32((uint32_t*)(dst + 1 * BPS), r1_u32, 0);
1024        vst1_lane_u32((uint32_t*)(dst + 2 * BPS), r2_u32, 0);
1025        vst1_lane_u32((uint32_t*)(dst + 3 * BPS), r3_u32, 0);
1026      } else {
1027        vst1_u32((uint32_t*)(dst + 0 * BPS), r0_u32);
1028        vst1_u32((uint32_t*)(dst + 1 * BPS), r1_u32);
1029        vst1_u32((uint32_t*)(dst + 2 * BPS), r2_u32);
1030        vst1_u32((uint32_t*)(dst + 3 * BPS), r3_u32);
1031      }
1032      dst += 4 * BPS;
1033    }
1034  }
1035  static void TM4_NEON(uint8_t* dst) { TrueMotion_NEON(dst, 4); }
1036  static void VE4_NEON(uint8_t* dst) {    
1037    const uint64x1_t A0 = vreinterpret_u64_u8(vld1_u8(dst - BPS - 1));  
1038    const uint64x1_t A1 = vshr_n_u64(A0, 8);
1039    const uint64x1_t A2 = vshr_n_u64(A0, 16);
1040    const uint8x8_t ABCDEFGH = vreinterpret_u8_u64(A0);
1041    const uint8x8_t BCDEFGH0 = vreinterpret_u8_u64(A1);
1042    const uint8x8_t CDEFGH00 = vreinterpret_u8_u64(A2);
1043    const uint8x8_t b = vhadd_u8(ABCDEFGH, CDEFGH00);
1044    const uint8x8_t avg = vrhadd_u8(b, BCDEFGH0);
1045    int i;
1046    for (i = 0; i < 4; ++i) {
1047      vst1_lane_u32((uint32_t*)(dst + i * BPS), vreinterpret_u32_u8(avg), 0);
1048    }
1049  }
1050  static void RD4_NEON(uint8_t* dst) {   
1051    const uint8x8_t XABCD_u8 = vld1_u8(dst - BPS - 1);
1052    const uint64x1_t XABCD = vreinterpret_u64_u8(XABCD_u8);
1053    const uint64x1_t ____XABC = vshl_n_u64(XABCD, 32);
1054    const uint32_t I = dst[-1 + 0 * BPS];
1055    const uint32_t J = dst[-1 + 1 * BPS];
1056    const uint32_t K = dst[-1 + 2 * BPS];
1057    const uint32_t L = dst[-1 + 3 * BPS];
1058    const uint64x1_t LKJI____ =
1059        vcreate_u64((uint64_t)L | (K << 8) | (J << 16) | (I << 24));
1060    const uint64x1_t LKJIXABC = vorr_u64(LKJI____, ____XABC);
1061    const uint8x8_t KJIXABC_ = vreinterpret_u8_u64(vshr_n_u64(LKJIXABC, 8));
1062    const uint8x8_t JIXABC__ = vreinterpret_u8_u64(vshr_n_u64(LKJIXABC, 16));
1063    const uint8_t D = vget_lane_u8(XABCD_u8, 4);
1064    const uint8x8_t JIXABCD_ = vset_lane_u8(D, JIXABC__, 6);
1065    const uint8x8_t LKJIXABC_u8 = vreinterpret_u8_u64(LKJIXABC);
1066    const uint8x8_t avg1 = vhadd_u8(JIXABCD_, LKJIXABC_u8);
1067    const uint8x8_t avg2 = vrhadd_u8(avg1, KJIXABC_);
1068    const uint64x1_t avg2_u64 = vreinterpret_u64_u8(avg2);
1069    const uint32x2_t r3 = vreinterpret_u32_u8(avg2);
1070    const uint32x2_t r2 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 8));
1071    const uint32x2_t r1 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 16));
1072    const uint32x2_t r0 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 24));
1073    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), r0, 0);
1074    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), r1, 0);
1075    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), r2, 0);
1076    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), r3, 0);
1077  }
1078  static void LD4_NEON(uint8_t* dst) {    
1079    const uint8x8_t ABCDEFGH = vld1_u8(dst - BPS + 0);
1080    const uint8x8_t BCDEFGH0 = vld1_u8(dst - BPS + 1);
1081    const uint8x8_t CDEFGH00 = vld1_u8(dst - BPS + 2);
1082    const uint8x8_t CDEFGHH0 = vset_lane_u8(dst[-BPS + 7], CDEFGH00, 6);
1083    const uint8x8_t avg1 = vhadd_u8(ABCDEFGH, CDEFGHH0);
1084    const uint8x8_t avg2 = vrhadd_u8(avg1, BCDEFGH0);
1085    const uint64x1_t avg2_u64 = vreinterpret_u64_u8(avg2);
1086    const uint32x2_t r0 = vreinterpret_u32_u8(avg2);
1087    const uint32x2_t r1 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 8));
1088    const uint32x2_t r2 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 16));
1089    const uint32x2_t r3 = vreinterpret_u32_u64(vshr_n_u64(avg2_u64, 24));
1090    vst1_lane_u32((uint32_t*)(dst + 0 * BPS), r0, 0);
1091    vst1_lane_u32((uint32_t*)(dst + 1 * BPS), r1, 0);
1092    vst1_lane_u32((uint32_t*)(dst + 2 * BPS), r2, 0);
1093    vst1_lane_u32((uint32_t*)(dst + 3 * BPS), r3, 0);
1094  }
1095  static void VE8uv_NEON(uint8_t* dst) {    
1096    const uint8x8_t top = vld1_u8(dst - BPS);
1097    int j;
1098    for (j = 0; j < 8; ++j) {
1099      vst1_u8(dst + j * BPS, top);
1100    }
1101  }
1102  static void HE8uv_NEON(uint8_t* dst) {    
1103    int j;
1104    for (j = 0; j < 8; ++j) {
1105      const uint8x8_t left = vld1_dup_u8(dst - 1);
1106      vst1_u8(dst, left);
1107      dst += BPS;
1108    }
1109  }
1110  static WEBP_INLINE void DC8_NEON(uint8_t* dst, int do_top, int do_left) {
1111    uint16x8_t sum_top;
1112    uint16x8_t sum_left;
1113    uint8x8_t dc0;
1114    if (do_top) {
1115      const uint8x8_t A = vld1_u8(dst - BPS);  
1116  #if defined(__aarch64__)
1117      const uint16x8_t B = vmovl_u8(A);
1118      const uint16_t p2 = vaddvq_u16(B);
1119      sum_top = vdupq_n_u16(p2);
1120  #else
1121      const uint16x4_t p0 = vpaddl_u8(A);  
1122      const uint16x4_t p1 = vpadd_u16(p0, p0);
1123      const uint16x4_t p2 = vpadd_u16(p1, p1);
1124      sum_top = vcombine_u16(p2, p2);
1125  #endif
1126    }
1127    if (do_left) {
1128      const uint16x8_t L0 = vmovl_u8(vld1_u8(dst + 0 * BPS - 1));
1129      const uint16x8_t L1 = vmovl_u8(vld1_u8(dst + 1 * BPS - 1));
1130      const uint16x8_t L2 = vmovl_u8(vld1_u8(dst + 2 * BPS - 1));
1131      const uint16x8_t L3 = vmovl_u8(vld1_u8(dst + 3 * BPS - 1));
1132      const uint16x8_t L4 = vmovl_u8(vld1_u8(dst + 4 * BPS - 1));
1133      const uint16x8_t L5 = vmovl_u8(vld1_u8(dst + 5 * BPS - 1));
1134      const uint16x8_t L6 = vmovl_u8(vld1_u8(dst + 6 * BPS - 1));
1135      const uint16x8_t L7 = vmovl_u8(vld1_u8(dst + 7 * BPS - 1));
1136      const uint16x8_t s0 = vaddq_u16(L0, L1);
1137      const uint16x8_t s1 = vaddq_u16(L2, L3);
1138      const uint16x8_t s2 = vaddq_u16(L4, L5);
1139      const uint16x8_t s3 = vaddq_u16(L6, L7);
1140      const uint16x8_t s01 = vaddq_u16(s0, s1);
1141      const uint16x8_t s23 = vaddq_u16(s2, s3);
1142      sum_left = vaddq_u16(s01, s23);
1143    }
1144    if (do_top && do_left) {
1145      const uint16x8_t sum = vaddq_u16(sum_left, sum_top);
1146      dc0 = vrshrn_n_u16(sum, 4);
1147    } else if (do_top) {
1148      dc0 = vrshrn_n_u16(sum_top, 3);
1149    } else if (do_left) {
1150      dc0 = vrshrn_n_u16(sum_left, 3);
1151    } else {
1152      dc0 = vdup_n_u8(0x80);
1153    }
1154    {
1155      const uint8x8_t dc = vdup_lane_u8(dc0, 0);
1156      int i;
1157      for (i = 0; i < 8; ++i) {
1158        vst1_u32((uint32_t*)(dst + i * BPS), vreinterpret_u32_u8(dc));
1159      }
1160    }
1161  }
1162  static void DC8uv_NEON(uint8_t* dst) { DC8_NEON(dst, 1, 1); }
1163  static void DC8uvNoTop_NEON(uint8_t* dst) { DC8_NEON(dst, 0, 1); }
1164  static void DC8uvNoLeft_NEON(uint8_t* dst) { DC8_NEON(dst, 1, 0); }
1165  static void DC8uvNoTopLeft_NEON(uint8_t* dst) { DC8_NEON(dst, 0, 0); }
1166  static void TM8uv_NEON(uint8_t* dst) { TrueMotion_NEON(dst, 8); }
1167  static void VE16_NEON(uint8_t* dst) {     
1168    const uint8x16_t top = vld1q_u8(dst - BPS);
1169    int j;
1170    for (j = 0; j < 16; ++j) {
1171      vst1q_u8(dst + j * BPS, top);
1172    }
1173  }
1174  static void HE16_NEON(uint8_t* dst) {     
1175    int j;
1176    for (j = 0; j < 16; ++j) {
1177      const uint8x16_t left = vld1q_dup_u8(dst - 1);
1178      vst1q_u8(dst, left);
1179      dst += BPS;
1180    }
1181  }
1182  static WEBP_INLINE void DC16_NEON(uint8_t* dst, int do_top, int do_left) {
1183    uint16x8_t sum_top;
1184    uint16x8_t sum_left;
1185    uint8x8_t dc0;
1186    if (do_top) {
1187      const uint8x16_t A = vld1q_u8(dst - BPS);  
1188      const uint16x8_t p0 = vpaddlq_u8(A);  
1189      const uint16x4_t p1 = vadd_u16(vget_low_u16(p0), vget_high_u16(p0));
1190      const uint16x4_t p2 = vpadd_u16(p1, p1);
1191      const uint16x4_t p3 = vpadd_u16(p2, p2);
1192      sum_top = vcombine_u16(p3, p3);
1193    }
1194    if (do_left) {
1195      int i;
1196      sum_left = vdupq_n_u16(0);
1197      for (i = 0; i < 16; i += 8) {
1198        const uint16x8_t L0 = vmovl_u8(vld1_u8(dst + (i + 0) * BPS - 1));
1199        const uint16x8_t L1 = vmovl_u8(vld1_u8(dst + (i + 1) * BPS - 1));
1200        const uint16x8_t L2 = vmovl_u8(vld1_u8(dst + (i + 2) * BPS - 1));
1201        const uint16x8_t L3 = vmovl_u8(vld1_u8(dst + (i + 3) * BPS - 1));
1202        const uint16x8_t L4 = vmovl_u8(vld1_u8(dst + (i + 4) * BPS - 1));
1203        const uint16x8_t L5 = vmovl_u8(vld1_u8(dst + (i + 5) * BPS - 1));
1204        const uint16x8_t L6 = vmovl_u8(vld1_u8(dst + (i + 6) * BPS - 1));
1205        const uint16x8_t L7 = vmovl_u8(vld1_u8(dst + (i + 7) * BPS - 1));
1206        const uint16x8_t s0 = vaddq_u16(L0, L1);
1207        const uint16x8_t s1 = vaddq_u16(L2, L3);
1208        const uint16x8_t s2 = vaddq_u16(L4, L5);
1209        const uint16x8_t s3 = vaddq_u16(L6, L7);
1210        const uint16x8_t s01 = vaddq_u16(s0, s1);
1211        const uint16x8_t s23 = vaddq_u16(s2, s3);
1212        const uint16x8_t sum = vaddq_u16(s01, s23);
1213        sum_left = vaddq_u16(sum_left, sum);
1214      }
1215    }
1216    if (do_top && do_left) {
1217      const uint16x8_t sum = vaddq_u16(sum_left, sum_top);
1218      dc0 = vrshrn_n_u16(sum, 5);
1219    } else if (do_top) {
1220      dc0 = vrshrn_n_u16(sum_top, 4);
1221    } else if (do_left) {
1222      dc0 = vrshrn_n_u16(sum_left, 4);
1223    } else {
1224      dc0 = vdup_n_u8(0x80);
1225    }
1226    {
1227      const uint8x16_t dc = vdupq_lane_u8(dc0, 0);
1228      int i;
1229      for (i = 0; i < 16; ++i) {
1230        vst1q_u8(dst + i * BPS, dc);
1231      }
1232    }
1233  }
1234  static void DC16TopLeft_NEON(uint8_t* dst) { DC16_NEON(dst, 1, 1); }
1235  static void DC16NoTop_NEON(uint8_t* dst) { DC16_NEON(dst, 0, 1); }
1236  static void DC16NoLeft_NEON(uint8_t* dst) { DC16_NEON(dst, 1, 0); }
1237  static void DC16NoTopLeft_NEON(uint8_t* dst) { DC16_NEON(dst, 0, 0); }
1238  static void TM16_NEON(uint8_t* dst) {
1239    const uint8x8_t TL = vld1_dup_u8(dst - BPS - 1);  
1240    const uint8x16_t T = vld1q_u8(dst - BPS);  
1241    const int16x8_t d_lo = vreinterpretq_s16_u16(vsubl_u8(vget_low_u8(T), TL));
1242    const int16x8_t d_hi = vreinterpretq_s16_u16(vsubl_u8(vget_high_u8(T), TL));
1243    int y;
1244    for (y = 0; y < 16; y += 4) {
1245      const int16x8_t L0 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 0 * BPS - 1));
1246      const int16x8_t L1 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 1 * BPS - 1));
1247      const int16x8_t L2 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 2 * BPS - 1));
1248      const int16x8_t L3 = ConvertU8ToS16_NEON(vld1_dup_u8(dst + 3 * BPS - 1));
1249      const int16x8_t r0_lo = vaddq_s16(L0, d_lo);  
1250      const int16x8_t r1_lo = vaddq_s16(L1, d_lo);
1251      const int16x8_t r2_lo = vaddq_s16(L2, d_lo);
1252      const int16x8_t r3_lo = vaddq_s16(L3, d_lo);
1253      const int16x8_t r0_hi = vaddq_s16(L0, d_hi);
1254      const int16x8_t r1_hi = vaddq_s16(L1, d_hi);
1255      const int16x8_t r2_hi = vaddq_s16(L2, d_hi);
1256      const int16x8_t r3_hi = vaddq_s16(L3, d_hi);
1257      const uint8x16_t row0 = vcombine_u8(vqmovun_s16(r0_lo), vqmovun_s16(r0_hi));
1258      const uint8x16_t row1 = vcombine_u8(vqmovun_s16(r1_lo), vqmovun_s16(r1_hi));
1259      const uint8x16_t row2 = vcombine_u8(vqmovun_s16(r2_lo), vqmovun_s16(r2_hi));
1260      const uint8x16_t row3 = vcombine_u8(vqmovun_s16(r3_lo), vqmovun_s16(r3_hi));
1261      vst1q_u8(dst + 0 * BPS, row0);
1262      vst1q_u8(dst + 1 * BPS, row1);
1263      vst1q_u8(dst + 2 * BPS, row2);
1264      vst1q_u8(dst + 3 * BPS, row3);
1265      dst += 4 * BPS;
1266    }
1267  }
1268  extern void VP8DspInitNEON(void);
<span onclick='openModal()' class='match'>1269  WEBP_TSAN_IGNORE_FUNCTION void VP8DspInitNEON(void) {
1270    VP8Transform = TransformTwo_NEON;
1271    VP8TransformAC3 = TransformAC3_NEON;
1272    VP8TransformDC = TransformDC_NEON;
1273    VP8TransformWHT = TransformWHT_NEON;
1274    VP8VFilter16 = VFilter16_NEON;
1275    VP8VFilter16i = VFilter16i_NEON;
1276    VP8HFilter16 = HFilter16_NEON;
1277  #if !defined(WORK_AROUND_GCC)
1278    VP8HFilter16i = HFilter16i_NEON;
1279  #endif
1280    VP8VFilter8 = VFilter8_NEON;
1281    VP8VFilter8i = VFilter8i_NEON;
1282  #if !defined(WORK_AROUND_GCC)
1283    VP8HFilter8 = HFilter8_NEON;
1284    VP8HFilter8i = HFilter8i_NEON;
1285  #endif
1286    VP8SimpleVFilter16 = SimpleVFilter16_NEON;
1287    VP8SimpleHFilter16 = SimpleHFilter16_NEON;
1288    VP8SimpleVFilter16i = SimpleVFilter16i_NEON;
1289    VP8SimpleHFilter16i = SimpleHFilter16i_NEON;
1290    VP8PredLuma4[0] = DC4_NEON;
1291    VP8PredLuma4[1] = TM4_NEON;
1292    VP8PredLuma4[2] = VE4_NEON;
1293    VP8PredLuma4[4] = RD4_NEON;
1294    VP8PredLuma4[6] = LD4_NEON;
1295    VP8PredLuma16[0] = DC16TopLeft_NEON;
1296    VP8PredLuma16[1] = TM16_NEON;
1297    VP8PredLuma16[2] = VE16_NEON;
1298    VP8PredLuma16[3] = HE16_NEON;
1299    VP8PredLuma16[4] = DC16NoTop_NEON;
1300    VP8PredLuma16[5] = DC16NoLeft_NEON;
1301    VP8PredLuma16[6] = DC16NoTopLeft_NEON;
1302    VP8PredChroma8[0] = DC8uv_NEON;
1303    VP8PredChroma8[1] = TM8uv_NEON;
1304    VP8PredChroma8[2] = VE8uv_NEON;
1305    VP8PredChroma8[3] = HE8uv_NEON;
1306    VP8PredChroma8[4] = DC8uvNoTop_NEON;
1307    VP8PredChroma8[5] = DC8uvNoLeft_NEON;
1308    VP8PredChroma8[6] = DC8uvNoTopLeft_NEON;
</span>1309  }
1310  #else  
1311  WEBP_DSP_INIT_STUB(VP8DspInitNEON)
1312  #endif  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from libtomcrypt-MDEwOlJlcG9zaXRvcnk3NzcwMTE=-flat-safer.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from PINRemoteImage-MDEwOlJlcG9zaXRvcnkzOTUzNzEwMw==-flat-dec_neon.c</div>
                </div>
                <div class="column column_space"><pre><code>204      {
205          a ^= *++key; b += *++key; c += *++key; d ^= *++key;
206          e ^= *++key; f += *++key; g += *++key; h ^= *++key;
207          a = EXP(a) + *++key; b = LOG(b) ^ *++key;
208          c = LOG(c) ^ *++key; d = EXP(d) + *++key;
209          e = EXP(e) + *++key; f = LOG(f) ^ *++key;
210          g = LOG(g) ^ *++key; h = EXP(h) + *++key;
211          PHT(a, b); PHT(c, d); PHT(e, f); PHT(g, h);
212          PHT(a, c); PHT(e, g); PHT(b, d); PHT(f, h);
213          PHT(a, e); PHT(b, f); PHT(c, g); PHT(d, h);
214          t = b; b = e; e = c; c = t; t = d; d = f; f = g; g = t;
</pre></code></div>
                <div class="column column_space"><pre><code>1269  WEBP_TSAN_IGNORE_FUNCTION void VP8DspInitNEON(void) {
1270    VP8Transform = TransformTwo_NEON;
1271    VP8TransformAC3 = TransformAC3_NEON;
1272    VP8TransformDC = TransformDC_NEON;
1273    VP8TransformWHT = TransformWHT_NEON;
1274    VP8VFilter16 = VFilter16_NEON;
1275    VP8VFilter16i = VFilter16i_NEON;
1276    VP8HFilter16 = HFilter16_NEON;
1277  #if !defined(WORK_AROUND_GCC)
1278    VP8HFilter16i = HFilter16i_NEON;
1279  #endif
1280    VP8VFilter8 = VFilter8_NEON;
1281    VP8VFilter8i = VFilter8i_NEON;
1282  #if !defined(WORK_AROUND_GCC)
1283    VP8HFilter8 = HFilter8_NEON;
1284    VP8HFilter8i = HFilter8i_NEON;
1285  #endif
1286    VP8SimpleVFilter16 = SimpleVFilter16_NEON;
1287    VP8SimpleHFilter16 = SimpleHFilter16_NEON;
1288    VP8SimpleVFilter16i = SimpleVFilter16i_NEON;
1289    VP8SimpleHFilter16i = SimpleHFilter16i_NEON;
1290    VP8PredLuma4[0] = DC4_NEON;
1291    VP8PredLuma4[1] = TM4_NEON;
1292    VP8PredLuma4[2] = VE4_NEON;
1293    VP8PredLuma4[4] = RD4_NEON;
1294    VP8PredLuma4[6] = LD4_NEON;
1295    VP8PredLuma16[0] = DC16TopLeft_NEON;
1296    VP8PredLuma16[1] = TM16_NEON;
1297    VP8PredLuma16[2] = VE16_NEON;
1298    VP8PredLuma16[3] = HE16_NEON;
1299    VP8PredLuma16[4] = DC16NoTop_NEON;
1300    VP8PredLuma16[5] = DC16NoLeft_NEON;
1301    VP8PredLuma16[6] = DC16NoTopLeft_NEON;
1302    VP8PredChroma8[0] = DC8uv_NEON;
1303    VP8PredChroma8[1] = TM8uv_NEON;
1304    VP8PredChroma8[2] = VE8uv_NEON;
1305    VP8PredChroma8[3] = HE8uv_NEON;
1306    VP8PredChroma8[4] = DC8uvNoTop_NEON;
1307    VP8PredChroma8[5] = DC8uvNoLeft_NEON;
1308    VP8PredChroma8[6] = DC8uvNoTopLeft_NEON;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    