
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 15, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-schema.h</h3>
            <pre><code>1  #ifndef RAPIDJSON_SCHEMA_H_
2  #define RAPIDJSON_SCHEMA_H_
3  #include &quot;document.h&quot;
4  #include &quot;pointer.h&quot;
5  #include &quot;stringbuffer.h&quot;
6  #include &quot;error/en.h&quot;
7  #include &quot;uri.h&quot;
8  #include &lt;cmath&gt; 
9  #if !defined(RAPIDJSON_SCHEMA_USE_INTERNALREGEX)
10  #define RAPIDJSON_SCHEMA_USE_INTERNALREGEX 1
11  #else
12  #define RAPIDJSON_SCHEMA_USE_INTERNALREGEX 0
13  #endif
14  #if !RAPIDJSON_SCHEMA_USE_INTERNALREGEX &amp;&amp; defined(RAPIDJSON_SCHEMA_USE_STDREGEX) &amp;&amp; (__cplusplus &gt;=201103L || (defined(_MSC_VER) &amp;&amp; _MSC_VER &gt;= 1800))
15  #define RAPIDJSON_SCHEMA_USE_STDREGEX 1
16  #else
17  #define RAPIDJSON_SCHEMA_USE_STDREGEX 0
18  #endif
19  #if RAPIDJSON_SCHEMA_USE_INTERNALREGEX
20  #include &quot;internal/regex.h&quot;
21  #elif RAPIDJSON_SCHEMA_USE_STDREGEX
22  #include &lt;regex&gt;
23  #endif
24  #if RAPIDJSON_SCHEMA_USE_INTERNALREGEX || RAPIDJSON_SCHEMA_USE_STDREGEX
25  #define RAPIDJSON_SCHEMA_HAS_REGEX 1
26  #else
27  #define RAPIDJSON_SCHEMA_HAS_REGEX 0
28  #endif
29  #ifndef RAPIDJSON_SCHEMA_VERBOSE
30  #define RAPIDJSON_SCHEMA_VERBOSE 0
31  #endif
32  #if RAPIDJSON_SCHEMA_VERBOSE
33  #include &quot;stringbuffer.h&quot;
34  #endif
35  RAPIDJSON_DIAG_PUSH
36  #if defined(__GNUC__)
37  RAPIDJSON_DIAG_OFF(effc++)
38  #endif
39  #ifdef __clang__
40  RAPIDJSON_DIAG_OFF(weak-vtables)
41  RAPIDJSON_DIAG_OFF(exit-time-destructors)
42  RAPIDJSON_DIAG_OFF(c++98-compat-pedantic)
43  RAPIDJSON_DIAG_OFF(variadic-macros)
44  #elif defined(_MSC_VER)
45  RAPIDJSON_DIAG_OFF(4512) 
46  #endif
47  RAPIDJSON_NAMESPACE_BEGIN
48  #if RAPIDJSON_SCHEMA_VERBOSE
49  namespace internal {
50  inline void PrintInvalidKeyword(const char* keyword) {
51      printf(&quot;Fail keyword: %s\n&quot;, keyword);
52  }
53  inline void PrintInvalidKeyword(const wchar_t* keyword) {
54      wprintf(L&quot;Fail keyword: %ls\n&quot;, keyword);
55  }
56  inline void PrintInvalidDocument(const char* document) {
57      printf(&quot;Fail document: %s\n\n&quot;, document);
58  }
59  inline void PrintInvalidDocument(const wchar_t* document) {
60      wprintf(L&quot;Fail document: %ls\n\n&quot;, document);
61  }
62  inline void PrintValidatorPointers(unsigned depth, const char* s, const char* d) {
63      printf(&quot;S: %*s%s\nD: %*s%s\n\n&quot;, depth * 4, &quot; &quot;, s, depth * 4, &quot; &quot;, d);
64  }
65  inline void PrintValidatorPointers(unsigned depth, const wchar_t* s, const wchar_t* d) {
66      wprintf(L&quot;S: %*ls%ls\nD: %*ls%ls\n\n&quot;, depth * 4, L&quot; &quot;, s, depth * 4, L&quot; &quot;, d);
67  }
68  } 
69  #endif 
70  #if RAPIDJSON_SCHEMA_VERBOSE
71  #define RAPIDJSON_INVALID_KEYWORD_VERBOSE(keyword) internal::PrintInvalidKeyword(keyword)
72  #else
73  #define RAPIDJSON_INVALID_KEYWORD_VERBOSE(keyword)
74  #endif
75  #define RAPIDJSON_INVALID_KEYWORD_RETURN(code)\
76  RAPIDJSON_MULTILINEMACRO_BEGIN\
77      context.invalidCode = code;\
78      context.invalidKeyword = SchemaType::GetValidateErrorKeyword(code).GetString();\
79      RAPIDJSON_INVALID_KEYWORD_VERBOSE(context.invalidKeyword);\
80      return false;\
81  RAPIDJSON_MULTILINEMACRO_END
82  #ifndef RAPIDJSON_VALIDATE_DEFAULT_FLAGS
83  #define RAPIDJSON_VALIDATE_DEFAULT_FLAGS kValidateNoFlags
84  #endif
85  enum ValidateFlag {
86      kValidateNoFlags = 0,                                       
87      kValidateContinueOnErrorFlag = 1,                           
88      kValidateDefaultFlags = RAPIDJSON_VALIDATE_DEFAULT_FLAGS    
89  };
90  template &lt;typename ValueType, typename Allocator&gt;
91  class GenericSchemaDocument;
92  namespace internal {
93  template &lt;typename SchemaDocumentType&gt;
94  class Schema;
95  class ISchemaValidator {
96  public:
97      virtual ~ISchemaValidator() {}
98      virtual bool IsValid() const = 0;
99      virtual void SetValidateFlags(unsigned flags) = 0;
100      virtual unsigned GetValidateFlags() const = 0;
101  };
102  template &lt;typename SchemaType&gt;
103  class ISchemaStateFactory {
104  public:
105      virtual ~ISchemaStateFactory() {}
106      virtual ISchemaValidator* CreateSchemaValidator(const SchemaType&amp;, const bool inheritContinueOnErrors) = 0;
107      virtual void DestroySchemaValidator(ISchemaValidator* validator) = 0;
108      virtual void* CreateHasher() = 0;
109      virtual uint64_t GetHashCode(void* hasher) = 0;
110      virtual void DestroryHasher(void* hasher) = 0;
111      virtual void* MallocState(size_t size) = 0;
112      virtual void FreeState(void* p) = 0;
113  };
114  template &lt;typename SchemaType&gt;
115  class IValidationErrorHandler {
116  public:
117      typedef typename SchemaType::Ch Ch;
118      typedef typename SchemaType::SValue SValue;
119      virtual ~IValidationErrorHandler() {}
120      virtual void NotMultipleOf(int64_t actual, const SValue&amp; expected) = 0;
121      virtual void NotMultipleOf(uint64_t actual, const SValue&amp; expected) = 0;
122      virtual void NotMultipleOf(double actual, const SValue&amp; expected) = 0;
123      virtual void AboveMaximum(int64_t actual, const SValue&amp; expected, bool exclusive) = 0;
124      virtual void AboveMaximum(uint64_t actual, const SValue&amp; expected, bool exclusive) = 0;
125      virtual void AboveMaximum(double actual, const SValue&amp; expected, bool exclusive) = 0;
126      virtual void BelowMinimum(int64_t actual, const SValue&amp; expected, bool exclusive) = 0;
127      virtual void BelowMinimum(uint64_t actual, const SValue&amp; expected, bool exclusive) = 0;
128      virtual void BelowMinimum(double actual, const SValue&amp; expected, bool exclusive) = 0;
129      virtual void TooLong(const Ch* str, SizeType length, SizeType expected) = 0;
130      virtual void TooShort(const Ch* str, SizeType length, SizeType expected) = 0;
131      virtual void DoesNotMatch(const Ch* str, SizeType length) = 0;
132      virtual void DisallowedItem(SizeType index) = 0;
133      virtual void TooFewItems(SizeType actualCount, SizeType expectedCount) = 0;
134      virtual void TooManyItems(SizeType actualCount, SizeType expectedCount) = 0;
135      virtual void DuplicateItems(SizeType index1, SizeType index2) = 0;
136      virtual void TooManyProperties(SizeType actualCount, SizeType expectedCount) = 0;
137      virtual void TooFewProperties(SizeType actualCount, SizeType expectedCount) = 0;
138      virtual void StartMissingProperties() = 0;
139      virtual void AddMissingProperty(const SValue&amp; name) = 0;
140      virtual bool EndMissingProperties() = 0;
141      virtual void PropertyViolations(ISchemaValidator** subvalidators, SizeType count) = 0;
142      virtual void DisallowedProperty(const Ch* name, SizeType length) = 0;
143      virtual void StartDependencyErrors() = 0;
144      virtual void StartMissingDependentProperties() = 0;
145      virtual void AddMissingDependentProperty(const SValue&amp; targetName) = 0;
146      virtual void EndMissingDependentProperties(const SValue&amp; sourceName) = 0;
147      virtual void AddDependencySchemaError(const SValue&amp; souceName, ISchemaValidator* subvalidator) = 0;
148      virtual bool EndDependencyErrors() = 0;
149      virtual void DisallowedValue(const ValidateErrorCode code) = 0;
150      virtual void StartDisallowedType() = 0;
151      virtual void AddExpectedType(const typename SchemaType::ValueType&amp; expectedType) = 0;
152      virtual void EndDisallowedType(const typename SchemaType::ValueType&amp; actualType) = 0;
153      virtual void NotAllOf(ISchemaValidator** subvalidators, SizeType count) = 0;
154      virtual void NoneOf(ISchemaValidator** subvalidators, SizeType count) = 0;
155      virtual void NotOneOf(ISchemaValidator** subvalidators, SizeType count, bool matched) = 0;
156      virtual void Disallowed() = 0;
157  };
158  template&lt;typename Encoding, typename Allocator&gt;
159  class Hasher {
160  public:
161      typedef typename Encoding::Ch Ch;
162      Hasher(Allocator* allocator = 0, size_t stackCapacity = kDefaultSize) : stack_(allocator, stackCapacity) {}
163      bool Null() { return WriteType(kNullType); }
164      bool Bool(bool b) { return WriteType(b ? kTrueType : kFalseType); }
165      bool Int(int i) { Number n; n.u.i = i; n.d = static_cast&lt;double&gt;(i); return WriteNumber(n); }
166      bool Uint(unsigned u) { Number n; n.u.u = u; n.d = static_cast&lt;double&gt;(u); return WriteNumber(n); }
167      bool Int64(int64_t i) { Number n; n.u.i = i; n.d = static_cast&lt;double&gt;(i); return WriteNumber(n); }
168      bool Uint64(uint64_t u) { Number n; n.u.u = u; n.d = static_cast&lt;double&gt;(u); return WriteNumber(n); }
169      bool Double(double d) {
170          Number n;
171          if (d &lt; 0) n.u.i = static_cast&lt;int64_t&gt;(d);
172          else       n.u.u = static_cast&lt;uint64_t&gt;(d);
173          n.d = d;
174          return WriteNumber(n);
175      }
176      bool RawNumber(const Ch* str, SizeType len, bool) {
177          WriteBuffer(kNumberType, str, len * sizeof(Ch));
178          return true;
179      }
180      bool String(const Ch* str, SizeType len, bool) {
181          WriteBuffer(kStringType, str, len * sizeof(Ch));
182          return true;
183      }
184      bool StartObject() { return true; }
185      bool Key(const Ch* str, SizeType len, bool copy) { return String(str, len, copy); }
186      bool EndObject(SizeType memberCount) {
187          uint64_t h = Hash(0, kObjectType);
188          uint64_t* kv = stack_.template Pop&lt;uint64_t&gt;(memberCount * 2);
189          for (SizeType i = 0; i &lt; memberCount; i++)
190              h ^= Hash(kv[i * 2], kv[i * 2 + 1]);  
191          *stack_.template Push&lt;uint64_t&gt;() = h;
192          return true;
193      }
194      bool StartArray() { return true; }
195      bool EndArray(SizeType elementCount) {
196          uint64_t h = Hash(0, kArrayType);
197          uint64_t* e = stack_.template Pop&lt;uint64_t&gt;(elementCount);
198          for (SizeType i = 0; i &lt; elementCount; i++)
199              h = Hash(h, e[i]); 
200          *stack_.template Push&lt;uint64_t&gt;() = h;
201          return true;
202      }
203      bool IsValid() const { return stack_.GetSize() == sizeof(uint64_t); }
204      uint64_t GetHashCode() const {
205          RAPIDJSON_ASSERT(IsValid());
206          return *stack_.template Top&lt;uint64_t&gt;();
207      }
208  private:
209      static const size_t kDefaultSize = 256;
210      struct Number {
211          union U {
212              uint64_t u;
213              int64_t i;
214          }u;
215          double d;
216      };
217      bool WriteType(Type type) { return WriteBuffer(type, 0, 0); }
218      bool WriteNumber(const Number&amp; n) { return WriteBuffer(kNumberType, &amp;n, sizeof(n)); }
219      bool WriteBuffer(Type type, const void* data, size_t len) {
220          uint64_t h = Hash(RAPIDJSON_UINT64_C2(0x84222325, 0xcbf29ce4), type);
221          const unsigned char* d = static_cast&lt;const unsigned char*&gt;(data);
222          for (size_t i = 0; i &lt; len; i++)
223              h = Hash(h, d[i]);
224          *stack_.template Push&lt;uint64_t&gt;() = h;
225          return true;
226      }
227      static uint64_t Hash(uint64_t h, uint64_t d) {
228          static const uint64_t kPrime = RAPIDJSON_UINT64_C2(0x00000100, 0x000001b3);
229          h ^= d;
230          h *= kPrime;
231          return h;
232      }
233      Stack&lt;Allocator&gt; stack_;
234  };
235  template &lt;typename SchemaDocumentType&gt;
236  struct SchemaValidationContext {
237      typedef Schema&lt;SchemaDocumentType&gt; SchemaType;
238      typedef ISchemaStateFactory&lt;SchemaType&gt; SchemaValidatorFactoryType;
239      typedef IValidationErrorHandler&lt;SchemaType&gt; ErrorHandlerType;
240      typedef typename SchemaType::ValueType ValueType;
241      typedef typename ValueType::Ch Ch;
242      enum PatternValidatorType {
243          kPatternValidatorOnly,
244          kPatternValidatorWithProperty,
245          kPatternValidatorWithAdditionalProperty
246      };
247      SchemaValidationContext(SchemaValidatorFactoryType&amp; f, ErrorHandlerType&amp; eh, const SchemaType* s) :
248          factory(f),
249          error_handler(eh),
250          schema(s),
251          valueSchema(),
252          invalidKeyword(),
253          invalidCode(),
254          hasher(),
255          arrayElementHashCodes(),
256          validators(),
257          validatorCount(),
258          patternPropertiesValidators(),
259          patternPropertiesValidatorCount(),
260          patternPropertiesSchemas(),
261          patternPropertiesSchemaCount(),
262          valuePatternValidatorType(kPatternValidatorOnly),
263          propertyExist(),
264          inArray(false),
265          valueUniqueness(false),
266          arrayUniqueness(false)
267      {
268      }
269      ~SchemaValidationContext() {
270          if (hasher)
271              factory.DestroryHasher(hasher);
272          if (validators) {
273              for (SizeType i = 0; i &lt; validatorCount; i++)
274                  factory.DestroySchemaValidator(validators[i]);
275              factory.FreeState(validators);
276          }
277          if (patternPropertiesValidators) {
278              for (SizeType i = 0; i &lt; patternPropertiesValidatorCount; i++)
279                  factory.DestroySchemaValidator(patternPropertiesValidators[i]);
280              factory.FreeState(patternPropertiesValidators);
281          }
282          if (patternPropertiesSchemas)
283              factory.FreeState(patternPropertiesSchemas);
284          if (propertyExist)
285              factory.FreeState(propertyExist);
286      }
287      SchemaValidatorFactoryType&amp; factory;
288      ErrorHandlerType&amp; error_handler;
289      const SchemaType* schema;
290      const SchemaType* valueSchema;
291      const Ch* invalidKeyword;
292      ValidateErrorCode invalidCode;
293      void* hasher; 
294      void* arrayElementHashCodes; 
295      ISchemaValidator** validators;
296      SizeType validatorCount;
297      ISchemaValidator** patternPropertiesValidators;
298      SizeType patternPropertiesValidatorCount;
299      const SchemaType** patternPropertiesSchemas;
300      SizeType patternPropertiesSchemaCount;
301      PatternValidatorType valuePatternValidatorType;
302      PatternValidatorType objectPatternValidatorType;
303      SizeType arrayElementIndex;
304      bool* propertyExist;
305      bool inArray;
306      bool valueUniqueness;
307      bool arrayUniqueness;
308  };
309  template &lt;typename SchemaDocumentType&gt;
310  class Schema {
311  public:
312      typedef typename SchemaDocumentType::ValueType ValueType;
313      typedef typename SchemaDocumentType::AllocatorType AllocatorType;
314      typedef typename SchemaDocumentType::PointerType PointerType;
315      typedef typename ValueType::EncodingType EncodingType;
316      typedef typename EncodingType::Ch Ch;
317      typedef SchemaValidationContext&lt;SchemaDocumentType&gt; Context;
318      typedef Schema&lt;SchemaDocumentType&gt; SchemaType;
319      typedef GenericValue&lt;EncodingType, AllocatorType&gt; SValue;
320      typedef IValidationErrorHandler&lt;Schema&gt; ErrorHandler;
321      typedef GenericUri&lt;ValueType, AllocatorType&gt; UriType;
322      friend class GenericSchemaDocument&lt;ValueType, AllocatorType&gt;;
323      Schema(SchemaDocumentType* schemaDocument, const PointerType&amp; p, const ValueType&amp; value, const ValueType&amp; document, AllocatorType* allocator, const UriType&amp; id = UriType()) :
324          allocator_(allocator),
325          uri_(schemaDocument-&gt;GetURI(), *allocator),
326          id_(id),
327          pointer_(p, allocator),
328          typeless_(schemaDocument-&gt;GetTypeless()),
329          enum_(),
330          enumCount_(),
331          not_(),
332          type_((1 &lt;&lt; kTotalSchemaType) - 1), 
333          validatorCount_(),
334          notValidatorIndex_(),
335          properties_(),
336          additionalPropertiesSchema_(),
337          patternProperties_(),
338          patternPropertyCount_(),
339          propertyCount_(),
340          minProperties_(),
341          maxProperties_(SizeType(~0)),
342          additionalProperties_(true),
343          hasDependencies_(),
344          hasRequired_(),
345          hasSchemaDependencies_(),
346          additionalItemsSchema_(),
347          itemsList_(),
348          itemsTuple_(),
349          itemsTupleCount_(),
350          minItems_(),
351          maxItems_(SizeType(~0)),
352          additionalItems_(true),
353          uniqueItems_(false),
354          pattern_(),
355          minLength_(0),
356          maxLength_(~SizeType(0)),
357          exclusiveMinimum_(false),
358          exclusiveMaximum_(false),
359          defaultValueLength_(0)
360      {
361          typedef typename ValueType::ConstValueIterator ConstValueIterator;
362          typedef typename ValueType::ConstMemberIterator ConstMemberIterator;
363          if (this != typeless_) {
364            typedef typename SchemaDocumentType::SchemaEntry SchemaEntry;
365            SchemaEntry *entry = schemaDocument-&gt;schemaMap_.template Push&lt;SchemaEntry&gt;();
366            new (entry) SchemaEntry(pointer_, this, true, allocator_);
367            schemaDocument-&gt;AddSchemaRefs(this);
368          }
369          if (!value.IsObject())
370              return;
371          if (const ValueType* v = GetMember(value, GetIdString())) {
372              if (v-&gt;IsString()) {
373                  UriType local(*v, allocator);
374                  id_ = local.Resolve(id_, allocator);
375              }
376          }
377          if (const ValueType* v = GetMember(value, GetTypeString())) {
378              type_ = 0;
379              if (v-&gt;IsString())
380                  AddType(*v);
381              else if (v-&gt;IsArray())
382                  for (ConstValueIterator itr = v-&gt;Begin(); itr != v-&gt;End(); ++itr)
383                      AddType(*itr);
384          }
385          if (const ValueType* v = GetMember(value, GetEnumString())) {
386              if (v-&gt;IsArray() &amp;&amp; v-&gt;Size() &gt; 0) {
387                  enum_ = static_cast&lt;uint64_t*&gt;(allocator_-&gt;Malloc(sizeof(uint64_t) * v-&gt;Size()));
388                  for (ConstValueIterator itr = v-&gt;Begin(); itr != v-&gt;End(); ++itr) {
389                      typedef Hasher&lt;EncodingType, MemoryPoolAllocator&lt;&gt; &gt; EnumHasherType;
390                      char buffer[256u + 24];
391                      MemoryPoolAllocator&lt;&gt; hasherAllocator(buffer, sizeof(buffer));
392                      EnumHasherType h(&amp;hasherAllocator, 256);
393                      itr-&gt;Accept(h);
394                      enum_[enumCount_++] = h.GetHashCode();
395                  }
396              }
397          }
398          if (schemaDocument) {
399              AssignIfExist(allOf_, *schemaDocument, p, value, GetAllOfString(), document);
400              AssignIfExist(anyOf_, *schemaDocument, p, value, GetAnyOfString(), document);
401              AssignIfExist(oneOf_, *schemaDocument, p, value, GetOneOfString(), document);
402              if (const ValueType* v = GetMember(value, GetNotString())) {
403                  schemaDocument-&gt;CreateSchema(&amp;not_, p.Append(GetNotString(), allocator_), *v, document, id_);
404                  notValidatorIndex_ = validatorCount_;
405                  validatorCount_++;
406              }
407          }
408          const ValueType* properties = GetMember(value, GetPropertiesString());
409          const ValueType* required = GetMember(value, GetRequiredString());
410          const ValueType* dependencies = GetMember(value, GetDependenciesString());
411          {
412              SValue allProperties(kArrayType);
413              if (properties &amp;&amp; properties-&gt;IsObject())
414                  for (ConstMemberIterator itr = properties-&gt;MemberBegin(); itr != properties-&gt;MemberEnd(); ++itr)
415                      AddUniqueElement(allProperties, itr-&gt;name);
416              if (required &amp;&amp; required-&gt;IsArray())
417                  for (ConstValueIterator itr = required-&gt;Begin(); itr != required-&gt;End(); ++itr)
418                      if (itr-&gt;IsString())
419                          AddUniqueElement(allProperties, *itr);
420              if (dependencies &amp;&amp; dependencies-&gt;IsObject())
421                  for (ConstMemberIterator itr = dependencies-&gt;MemberBegin(); itr != dependencies-&gt;MemberEnd(); ++itr) {
422                      AddUniqueElement(allProperties, itr-&gt;name);
423                      if (itr-&gt;value.IsArray())
424                          for (ConstValueIterator i = itr-&gt;value.Begin(); i != itr-&gt;value.End(); ++i)
425                              if (i-&gt;IsString())
426                                  AddUniqueElement(allProperties, *i);
427                  }
428              if (allProperties.Size() &gt; 0) {
429                  propertyCount_ = allProperties.Size();
430                  properties_ = static_cast&lt;Property*&gt;(allocator_-&gt;Malloc(sizeof(Property) * propertyCount_));
431                  for (SizeType i = 0; i &lt; propertyCount_; i++) {
432                      new (&amp;properties_[i]) Property();
433                      properties_[i].name = allProperties[i];
434                      properties_[i].schema = typeless_;
435                  }
436              }
437          }
438          if (properties &amp;&amp; properties-&gt;IsObject()) {
439              PointerType q = p.Append(GetPropertiesString(), allocator_);
440              for (ConstMemberIterator itr = properties-&gt;MemberBegin(); itr != properties-&gt;MemberEnd(); ++itr) {
441                  SizeType index;
442                  if (FindPropertyIndex(itr-&gt;name, &amp;index))
443                      schemaDocument-&gt;CreateSchema(&amp;properties_[index].schema, q.Append(itr-&gt;name, allocator_), itr-&gt;value, document, id_);
444              }
445          }
446          if (const ValueType* v = GetMember(value, GetPatternPropertiesString())) {
447              PointerType q = p.Append(GetPatternPropertiesString(), allocator_);
448              patternProperties_ = static_cast&lt;PatternProperty*&gt;(allocator_-&gt;Malloc(sizeof(PatternProperty) * v-&gt;MemberCount()));
449              patternPropertyCount_ = 0;
450              for (ConstMemberIterator itr = v-&gt;MemberBegin(); itr != v-&gt;MemberEnd(); ++itr) {
451                  new (&amp;patternProperties_[patternPropertyCount_]) PatternProperty();
452                  patternProperties_[patternPropertyCount_].pattern = CreatePattern(itr-&gt;name);
453                  schemaDocument-&gt;CreateSchema(&amp;patternProperties_[patternPropertyCount_].schema, q.Append(itr-&gt;name, allocator_), itr-&gt;value, document, id_);
454                  patternPropertyCount_++;
455              }
456          }
457          if (required &amp;&amp; required-&gt;IsArray())
458              for (ConstValueIterator itr = required-&gt;Begin(); itr != required-&gt;End(); ++itr)
459                  if (itr-&gt;IsString()) {
460                      SizeType index;
461                      if (FindPropertyIndex(*itr, &amp;index)) {
462                          properties_[index].required = true;
463                          hasRequired_ = true;
464                      }
465                  }
466          if (dependencies &amp;&amp; dependencies-&gt;IsObject()) {
467              PointerType q = p.Append(GetDependenciesString(), allocator_);
468              hasDependencies_ = true;
469              for (ConstMemberIterator itr = dependencies-&gt;MemberBegin(); itr != dependencies-&gt;MemberEnd(); ++itr) {
470                  SizeType sourceIndex;
471                  if (FindPropertyIndex(itr-&gt;name, &amp;sourceIndex)) {
472                      if (itr-&gt;value.IsArray()) {
473                          properties_[sourceIndex].dependencies = static_cast&lt;bool*&gt;(allocator_-&gt;Malloc(sizeof(bool) * propertyCount_));
474                          std::memset(properties_[sourceIndex].dependencies, 0, sizeof(bool)* propertyCount_);
475                          for (ConstValueIterator targetItr = itr-&gt;value.Begin(); targetItr != itr-&gt;value.End(); ++targetItr) {
476                              SizeType targetIndex;
477                              if (FindPropertyIndex(*targetItr, &amp;targetIndex))
478                                  properties_[sourceIndex].dependencies[targetIndex] = true;
479                          }
480                      }
481                      else if (itr-&gt;value.IsObject()) {
482                          hasSchemaDependencies_ = true;
483                          schemaDocument-&gt;CreateSchema(&amp;properties_[sourceIndex].dependenciesSchema, q.Append(itr-&gt;name, allocator_), itr-&gt;value, document, id_);
484                          properties_[sourceIndex].dependenciesValidatorIndex = validatorCount_;
485                          validatorCount_++;
486                      }
487                  }
488              }
489          }
490          if (const ValueType* v = GetMember(value, GetAdditionalPropertiesString())) {
491              if (v-&gt;IsBool())
492                  additionalProperties_ = v-&gt;GetBool();
493              else if (v-&gt;IsObject())
494                  schemaDocument-&gt;CreateSchema(&amp;additionalPropertiesSchema_, p.Append(GetAdditionalPropertiesString(), allocator_), *v, document, id_);
495          }
496          AssignIfExist(minProperties_, value, GetMinPropertiesString());
497          AssignIfExist(maxProperties_, value, GetMaxPropertiesString());
498          if (const ValueType* v = GetMember(value, GetItemsString())) {
499              PointerType q = p.Append(GetItemsString(), allocator_);
500              if (v-&gt;IsObject()) 
501                  schemaDocument-&gt;CreateSchema(&amp;itemsList_, q, *v, document, id_);
502              else if (v-&gt;IsArray()) { 
503                  itemsTuple_ = static_cast&lt;const Schema**&gt;(allocator_-&gt;Malloc(sizeof(const Schema*) * v-&gt;Size()));
504                  SizeType index = 0;
505                  for (ConstValueIterator itr = v-&gt;Begin(); itr != v-&gt;End(); ++itr, index++)
506                      schemaDocument-&gt;CreateSchema(&amp;itemsTuple_[itemsTupleCount_++], q.Append(index, allocator_), *itr, document, id_);
507              }
508          }
509          AssignIfExist(minItems_, value, GetMinItemsString());
510          AssignIfExist(maxItems_, value, GetMaxItemsString());
511          if (const ValueType* v = GetMember(value, GetAdditionalItemsString())) {
512              if (v-&gt;IsBool())
513                  additionalItems_ = v-&gt;GetBool();
514              else if (v-&gt;IsObject())
515                  schemaDocument-&gt;CreateSchema(&amp;additionalItemsSchema_, p.Append(GetAdditionalItemsString(), allocator_), *v, document, id_);
516          }
517          AssignIfExist(uniqueItems_, value, GetUniqueItemsString());
518          AssignIfExist(minLength_, value, GetMinLengthString());
519          AssignIfExist(maxLength_, value, GetMaxLengthString());
520          if (const ValueType* v = GetMember(value, GetPatternString()))
521              pattern_ = CreatePattern(*v);
522          if (const ValueType* v = GetMember(value, GetMinimumString()))
523              if (v-&gt;IsNumber())
524                  minimum_.CopyFrom(*v, *allocator_);
525          if (const ValueType* v = GetMember(value, GetMaximumString()))
526              if (v-&gt;IsNumber())
527                  maximum_.CopyFrom(*v, *allocator_);
528          AssignIfExist(exclusiveMinimum_, value, GetExclusiveMinimumString());
529          AssignIfExist(exclusiveMaximum_, value, GetExclusiveMaximumString());
530          if (const ValueType* v = GetMember(value, GetMultipleOfString()))
531              if (v-&gt;IsNumber() &amp;&amp; v-&gt;GetDouble() &gt; 0.0)
532                  multipleOf_.CopyFrom(*v, *allocator_);
533          if (const ValueType* v = GetMember(value, GetDefaultValueString()))
534              if (v-&gt;IsString())
535                  defaultValueLength_ = v-&gt;GetStringLength();
536      }
537      ~Schema() {
538          AllocatorType::Free(enum_);
539          if (properties_) {
540              for (SizeType i = 0; i &lt; propertyCount_; i++)
541                  properties_[i].~Property();
542              AllocatorType::Free(properties_);
543          }
544          if (patternProperties_) {
545              for (SizeType i = 0; i &lt; patternPropertyCount_; i++)
546                  patternProperties_[i].~PatternProperty();
547              AllocatorType::Free(patternProperties_);
548          }
549          AllocatorType::Free(itemsTuple_);
550  #if RAPIDJSON_SCHEMA_HAS_REGEX
551          if (pattern_) {
552              pattern_-&gt;~RegexType();
553              AllocatorType::Free(pattern_);
554          }
555  #endif
556      }
557      const SValue&amp; GetURI() const {
558          return uri_;
559      }
560      const UriType&amp; GetId() const {
561          return id_;
562      }
563      const PointerType&amp; GetPointer() const {
564          return pointer_;
565      }
566      bool BeginValue(Context&amp; context) const {
567          if (context.inArray) {
568              if (uniqueItems_)
569                  context.valueUniqueness = true;
570              if (itemsList_)
571                  context.valueSchema = itemsList_;
572              else if (itemsTuple_) {
573                  if (context.arrayElementIndex &lt; itemsTupleCount_)
574                      context.valueSchema = itemsTuple_[context.arrayElementIndex];
575                  else if (additionalItemsSchema_)
576                      context.valueSchema = additionalItemsSchema_;
577                  else if (additionalItems_)
578                      context.valueSchema = typeless_;
579                  else {
580                      context.error_handler.DisallowedItem(context.arrayElementIndex);
581                      context.valueSchema = typeless_;
582                      context.arrayElementIndex++;
583                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorAdditionalItems);
584                  }
585              }
586              else
587                  context.valueSchema = typeless_;
588              context.arrayElementIndex++;
589          }
590          return true;
591      }
592      RAPIDJSON_FORCEINLINE bool EndValue(Context&amp; context) const {
593          if (context.patternPropertiesValidatorCount &gt; 0) {
594              bool otherValid = false;
595              SizeType count = context.patternPropertiesValidatorCount;
596              if (context.objectPatternValidatorType != Context::kPatternValidatorOnly)
597                  otherValid = context.patternPropertiesValidators[--count]-&gt;IsValid();
598              bool patternValid = true;
599              for (SizeType i = 0; i &lt; count; i++)
600                  if (!context.patternPropertiesValidators[i]-&gt;IsValid()) {
601                      patternValid = false;
602                      break;
603                  }
604              if (context.objectPatternValidatorType == Context::kPatternValidatorOnly) {
605                  if (!patternValid) {
606                      context.error_handler.PropertyViolations(context.patternPropertiesValidators, count);
607                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorPatternProperties);
608                  }
609              }
610              else if (context.objectPatternValidatorType == Context::kPatternValidatorWithProperty) {
611                  if (!patternValid || !otherValid) {
612                      context.error_handler.PropertyViolations(context.patternPropertiesValidators, count + 1);
613                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorPatternProperties);
614                  }
615              }
616              else if (!patternValid &amp;&amp; !otherValid) { 
617                  context.error_handler.PropertyViolations(context.patternPropertiesValidators, count + 1);
618                  RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorPatternProperties);
619              }
620          }
621          if (enum_ &amp;&amp; context.hasher) {
622              const uint64_t h = context.factory.GetHashCode(context.hasher);
623              for (SizeType i = 0; i &lt; enumCount_; i++)
624                  if (enum_[i] == h)
625                      goto foundEnum;
626              context.error_handler.DisallowedValue(kValidateErrorEnum);
627              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorEnum);
628              foundEnum:;
629          }
630          if (context.validatorCount &gt; 0) {
631              if (allOf_.schemas)
632                  for (SizeType i = allOf_.begin; i &lt; allOf_.begin + allOf_.count; i++)
633                      if (!context.validators[i]-&gt;IsValid()) {
634                          context.error_handler.NotAllOf(&amp;context.validators[allOf_.begin], allOf_.count);
635                          RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorAllOf);
636                      }
637              if (anyOf_.schemas) {
638                  for (SizeType i = anyOf_.begin; i &lt; anyOf_.begin + anyOf_.count; i++)
639                      if (context.validators[i]-&gt;IsValid())
640                          goto foundAny;
641                  context.error_handler.NoneOf(&amp;context.validators[anyOf_.begin], anyOf_.count);
642                  RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorAnyOf);
643                  foundAny:;
644              }
645              if (oneOf_.schemas) {
646                  bool oneValid = false;
647                  for (SizeType i = oneOf_.begin; i &lt; oneOf_.begin + oneOf_.count; i++)
648                      if (context.validators[i]-&gt;IsValid()) {
649                          if (oneValid) {
650                              context.error_handler.NotOneOf(&amp;context.validators[oneOf_.begin], oneOf_.count, true);
651                              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorOneOfMatch);
652                          } else
653                              oneValid = true;
654                      }
655                  if (!oneValid) {
656                      context.error_handler.NotOneOf(&amp;context.validators[oneOf_.begin], oneOf_.count, false);
657                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorOneOf);
658                  }
659              }
660              if (not_ &amp;&amp; context.validators[notValidatorIndex_]-&gt;IsValid()) {
661                  context.error_handler.Disallowed();
662                  RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorNot);
663              }
664          }
665          return true;
666      }
667      bool Null(Context&amp; context) const {
668          if (!(type_ &amp; (1 &lt;&lt; kNullSchemaType))) {
669              DisallowedType(context, GetNullString());
670              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
671          }
672          return CreateParallelValidator(context);
673      }
674      bool Bool(Context&amp; context, bool) const {
675          if (!(type_ &amp; (1 &lt;&lt; kBooleanSchemaType))) {
676              DisallowedType(context, GetBooleanString());
677              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
678          }
679          return CreateParallelValidator(context);
680      }
681      bool Int(Context&amp; context, int i) const {
682          if (!CheckInt(context, i))
683              return false;
684          return CreateParallelValidator(context);
685      }
686      bool Uint(Context&amp; context, unsigned u) const {
687          if (!CheckUint(context, u))
688              return false;
689          return CreateParallelValidator(context);
690      }
691      bool Int64(Context&amp; context, int64_t i) const {
692          if (!CheckInt(context, i))
693              return false;
694          return CreateParallelValidator(context);
695      }
696      bool Uint64(Context&amp; context, uint64_t u) const {
697          if (!CheckUint(context, u))
698              return false;
699          return CreateParallelValidator(context);
700      }
701      bool Double(Context&amp; context, double d) const {
702          if (!(type_ &amp; (1 &lt;&lt; kNumberSchemaType))) {
703              DisallowedType(context, GetNumberString());
704              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
705          }
706          if (!minimum_.IsNull() &amp;&amp; !CheckDoubleMinimum(context, d))
707              return false;
708          if (!maximum_.IsNull() &amp;&amp; !CheckDoubleMaximum(context, d))
709              return false;
710          if (!multipleOf_.IsNull() &amp;&amp; !CheckDoubleMultipleOf(context, d))
711              return false;
712          return CreateParallelValidator(context);
713      }
714      bool String(Context&amp; context, const Ch* str, SizeType length, bool) const {
715          if (!(type_ &amp; (1 &lt;&lt; kStringSchemaType))) {
716              DisallowedType(context, GetStringString());
717              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
718          }
719          if (minLength_ != 0 || maxLength_ != SizeType(~0)) {
720              SizeType count;
721              if (internal::CountStringCodePoint&lt;EncodingType&gt;(str, length, &amp;count)) {
722                  if (count &lt; minLength_) {
723                      context.error_handler.TooShort(str, length, minLength_);
724                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMinLength);
725                  }
726                  if (count &gt; maxLength_) {
727                      context.error_handler.TooLong(str, length, maxLength_);
728                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMaxLength);
729                  }
730              }
731          }
732          if (pattern_ &amp;&amp; !IsPatternMatch(pattern_, str, length)) {
733              context.error_handler.DoesNotMatch(str, length);
734              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorPattern);
735          }
736          return CreateParallelValidator(context);
737      }
738      bool StartObject(Context&amp; context) const {
739          if (!(type_ &amp; (1 &lt;&lt; kObjectSchemaType))) {
740              DisallowedType(context, GetObjectString());
741              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
742          }
743          if (hasDependencies_ || hasRequired_) {
744              context.propertyExist = static_cast&lt;bool*&gt;(context.factory.MallocState(sizeof(bool) * propertyCount_));
745              std::memset(context.propertyExist, 0, sizeof(bool) * propertyCount_);
746          }
747          if (patternProperties_) { 
748              SizeType count = patternPropertyCount_ + 1; 
749              context.patternPropertiesSchemas = static_cast&lt;const SchemaType**&gt;(context.factory.MallocState(sizeof(const SchemaType*) * count));
750              context.patternPropertiesSchemaCount = 0;
751              std::memset(context.patternPropertiesSchemas, 0, sizeof(SchemaType*) * count);
752          }
753          return CreateParallelValidator(context);
754      }
755      bool Key(Context&amp; context, const Ch* str, SizeType len, bool) const {
756          if (patternProperties_) {
757              context.patternPropertiesSchemaCount = 0;
758              for (SizeType i = 0; i &lt; patternPropertyCount_; i++)
759                  if (patternProperties_[i].pattern &amp;&amp; IsPatternMatch(patternProperties_[i].pattern, str, len)) {
760                      context.patternPropertiesSchemas[context.patternPropertiesSchemaCount++] = patternProperties_[i].schema;
761                      context.valueSchema = typeless_;
762                  }
763          }
764          SizeType index  = 0;
765          if (FindPropertyIndex(ValueType(str, len).Move(), &amp;index)) {
766              if (context.patternPropertiesSchemaCount &gt; 0) {
767                  context.patternPropertiesSchemas[context.patternPropertiesSchemaCount++] = properties_[index].schema;
768                  context.valueSchema = typeless_;
769                  context.valuePatternValidatorType = Context::kPatternValidatorWithProperty;
770              }
771              else
772                  context.valueSchema = properties_[index].schema;
773              if (context.propertyExist)
774                  context.propertyExist[index] = true;
775              return true;
776          }
777          if (additionalPropertiesSchema_) {
778              if (context.patternPropertiesSchemaCount &gt; 0) {
779                  context.patternPropertiesSchemas[context.patternPropertiesSchemaCount++] = additionalPropertiesSchema_;
780                  context.valueSchema = typeless_;
781                  context.valuePatternValidatorType = Context::kPatternValidatorWithAdditionalProperty;
782              }
783              else
784                  context.valueSchema = additionalPropertiesSchema_;
785              return true;
786          }
787          else if (additionalProperties_) {
788              context.valueSchema = typeless_;
789              return true;
790          }
791          if (context.patternPropertiesSchemaCount == 0) { 
792              context.valueSchema = typeless_;
793              context.error_handler.DisallowedProperty(str, len);
794              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorAdditionalProperties);
795          }
796          return true;
797      }
798      bool EndObject(Context&amp; context, SizeType memberCount) const {
799          if (hasRequired_) {
800              context.error_handler.StartMissingProperties();
801              for (SizeType index = 0; index &lt; propertyCount_; index++)
802                  if (properties_[index].required &amp;&amp; !context.propertyExist[index])
803                      if (properties_[index].schema-&gt;defaultValueLength_ == 0 )
804                          context.error_handler.AddMissingProperty(properties_[index].name);
805              if (context.error_handler.EndMissingProperties())
806                  RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorRequired);
807          }
808          if (memberCount &lt; minProperties_) {
809              context.error_handler.TooFewProperties(memberCount, minProperties_);
810              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMinProperties);
811          }
812          if (memberCount &gt; maxProperties_) {
813              context.error_handler.TooManyProperties(memberCount, maxProperties_);
814              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMaxProperties);
815          }
816          if (hasDependencies_) {
817              context.error_handler.StartDependencyErrors();
818              for (SizeType sourceIndex = 0; sourceIndex &lt; propertyCount_; sourceIndex++) {
819                  const Property&amp; source = properties_[sourceIndex];
820                  if (context.propertyExist[sourceIndex]) {
821                      if (source.dependencies) {
822                          context.error_handler.StartMissingDependentProperties();
823                          for (SizeType targetIndex = 0; targetIndex &lt; propertyCount_; targetIndex++)
824                              if (source.dependencies[targetIndex] &amp;&amp; !context.propertyExist[targetIndex])
825                                  context.error_handler.AddMissingDependentProperty(properties_[targetIndex].name);
826                          context.error_handler.EndMissingDependentProperties(source.name);
827                      }
828                      else if (source.dependenciesSchema) {
829                          ISchemaValidator* dependenciesValidator = context.validators[source.dependenciesValidatorIndex];
830                          if (!dependenciesValidator-&gt;IsValid())
831                              context.error_handler.AddDependencySchemaError(source.name, dependenciesValidator);
832                      }
833                  }
834              }
835              if (context.error_handler.EndDependencyErrors())
836                  RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorDependencies);
837          }
838          return true;
839      }
840      bool StartArray(Context&amp; context) const {
841          context.arrayElementIndex = 0;
842          context.inArray = true;  
843          if (!(type_ &amp; (1 &lt;&lt; kArraySchemaType))) {
844              DisallowedType(context, GetArrayString());
845              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
846          }
847          return CreateParallelValidator(context);
848      }
849      bool EndArray(Context&amp; context, SizeType elementCount) const {
850          context.inArray = false;
851          if (elementCount &lt; minItems_) {
852              context.error_handler.TooFewItems(elementCount, minItems_);
853              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMinItems);
854          }
855          if (elementCount &gt; maxItems_) {
856              context.error_handler.TooManyItems(elementCount, maxItems_);
857              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMaxItems);
858          }
859          return true;
860      }
861      static const ValueType&amp; GetValidateErrorKeyword(ValidateErrorCode validateErrorCode) {
862          switch (validateErrorCode) {
863              case kValidateErrorMultipleOf:              return GetMultipleOfString();
864              case kValidateErrorMaximum:                 return GetMaximumString();
865              case kValidateErrorExclusiveMaximum:        return GetMaximumString(); 
866              case kValidateErrorMinimum:                 return GetMinimumString();
867              case kValidateErrorExclusiveMinimum:        return GetMinimumString(); 
868              case kValidateErrorMaxLength:               return GetMaxLengthString();
869              case kValidateErrorMinLength:               return GetMinLengthString();
870              case kValidateErrorPattern:                 return GetPatternString();
871              case kValidateErrorMaxItems:                return GetMaxItemsString();
872              case kValidateErrorMinItems:                return GetMinItemsString();
873              case kValidateErrorUniqueItems:             return GetUniqueItemsString();
874              case kValidateErrorAdditionalItems:         return GetAdditionalItemsString();
875              case kValidateErrorMaxProperties:           return GetMaxPropertiesString();
876              case kValidateErrorMinProperties:           return GetMinPropertiesString();
877              case kValidateErrorRequired:                return GetRequiredString();
878              case kValidateErrorAdditionalProperties:    return GetAdditionalPropertiesString();
879              case kValidateErrorPatternProperties:       return GetPatternPropertiesString();
880              case kValidateErrorDependencies:            return GetDependenciesString();
881              case kValidateErrorEnum:                    return GetEnumString();
882              case kValidateErrorType:                    return GetTypeString();
883              case kValidateErrorOneOf:                   return GetOneOfString();
884              case kValidateErrorOneOfMatch:              return GetOneOfString(); 
885              case kValidateErrorAllOf:                   return GetAllOfString();
886              case kValidateErrorAnyOf:                   return GetAnyOfString();
887              case kValidateErrorNot:                     return GetNotString();
888              default:                                    return GetNullString();
889          }
890      }
891  #define RAPIDJSON_STRING_(name, ...) \
892      static const ValueType&amp; Get##name##String() {\
893          static const Ch s[] = { __VA_ARGS__, &#x27;\0&#x27; };\
894          static const ValueType v(s, static_cast&lt;SizeType&gt;(sizeof(s) / sizeof(Ch) - 1));\
895          return v;\
896      }
897      RAPIDJSON_STRING_(Null, &#x27;n&#x27;, &#x27;u&#x27;, &#x27;l&#x27;, &#x27;l&#x27;)
898      RAPIDJSON_STRING_(Boolean, &#x27;b&#x27;, &#x27;o&#x27;, &#x27;o&#x27;, &#x27;l&#x27;, &#x27;e&#x27;, &#x27;a&#x27;, &#x27;n&#x27;)
899      RAPIDJSON_STRING_(Object, &#x27;o&#x27;, &#x27;b&#x27;, &#x27;j&#x27;, &#x27;e&#x27;, &#x27;c&#x27;, &#x27;t&#x27;)
900      RAPIDJSON_STRING_(Array, &#x27;a&#x27;, &#x27;r&#x27;, &#x27;r&#x27;, &#x27;a&#x27;, &#x27;y&#x27;)
901      RAPIDJSON_STRING_(String, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;r&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;g&#x27;)
902      RAPIDJSON_STRING_(Number, &#x27;n&#x27;, &#x27;u&#x27;, &#x27;m&#x27;, &#x27;b&#x27;, &#x27;e&#x27;, &#x27;r&#x27;)
903      RAPIDJSON_STRING_(Integer, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;g&#x27;, &#x27;e&#x27;, &#x27;r&#x27;)
904      RAPIDJSON_STRING_(Type, &#x27;t&#x27;, &#x27;y&#x27;, &#x27;p&#x27;, &#x27;e&#x27;)
905      RAPIDJSON_STRING_(Enum, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;u&#x27;, &#x27;m&#x27;)
906      RAPIDJSON_STRING_(AllOf, &#x27;a&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;O&#x27;, &#x27;f&#x27;)
907      RAPIDJSON_STRING_(AnyOf, &#x27;a&#x27;, &#x27;n&#x27;, &#x27;y&#x27;, &#x27;O&#x27;, &#x27;f&#x27;)
908      RAPIDJSON_STRING_(OneOf, &#x27;o&#x27;, &#x27;n&#x27;, &#x27;e&#x27;, &#x27;O&#x27;, &#x27;f&#x27;)
909      RAPIDJSON_STRING_(Not, &#x27;n&#x27;, &#x27;o&#x27;, &#x27;t&#x27;)
910      RAPIDJSON_STRING_(Properties, &#x27;p&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
911      RAPIDJSON_STRING_(Required, &#x27;r&#x27;, &#x27;e&#x27;, &#x27;q&#x27;, &#x27;u&#x27;, &#x27;i&#x27;, &#x27;r&#x27;, &#x27;e&#x27;, &#x27;d&#x27;)
912      RAPIDJSON_STRING_(Dependencies, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;c&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
913      RAPIDJSON_STRING_(PatternProperties, &#x27;p&#x27;, &#x27;a&#x27;, &#x27;t&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;n&#x27;, &#x27;P&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
914      RAPIDJSON_STRING_(AdditionalProperties, &#x27;a&#x27;, &#x27;d&#x27;, &#x27;d&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;n&#x27;, &#x27;a&#x27;, &#x27;l&#x27;, &#x27;P&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
915      RAPIDJSON_STRING_(MinProperties, &#x27;m&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;P&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
916      RAPIDJSON_STRING_(MaxProperties, &#x27;m&#x27;, &#x27;a&#x27;, &#x27;x&#x27;, &#x27;P&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
917      RAPIDJSON_STRING_(Items, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;s&#x27;)
918      RAPIDJSON_STRING_(MinItems, &#x27;m&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;I&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;s&#x27;)
919      RAPIDJSON_STRING_(MaxItems, &#x27;m&#x27;, &#x27;a&#x27;, &#x27;x&#x27;, &#x27;I&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;s&#x27;)
920      RAPIDJSON_STRING_(AdditionalItems, &#x27;a&#x27;, &#x27;d&#x27;, &#x27;d&#x27;, &#x27;i&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;n&#x27;, &#x27;a&#x27;, &#x27;l&#x27;, &#x27;I&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;s&#x27;)
921      RAPIDJSON_STRING_(UniqueItems, &#x27;u&#x27;, &#x27;n&#x27;, &#x27;i&#x27;, &#x27;q&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;I&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;s&#x27;)
922      RAPIDJSON_STRING_(MinLength, &#x27;m&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;L&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;g&#x27;, &#x27;t&#x27;, &#x27;h&#x27;)
923      RAPIDJSON_STRING_(MaxLength, &#x27;m&#x27;, &#x27;a&#x27;, &#x27;x&#x27;, &#x27;L&#x27;, &#x27;e&#x27;, &#x27;n&#x27;, &#x27;g&#x27;, &#x27;t&#x27;, &#x27;h&#x27;)
924      RAPIDJSON_STRING_(Pattern, &#x27;p&#x27;, &#x27;a&#x27;, &#x27;t&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;n&#x27;)
925      RAPIDJSON_STRING_(Minimum, &#x27;m&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;i&#x27;, &#x27;m&#x27;, &#x27;u&#x27;, &#x27;m&#x27;)
926      RAPIDJSON_STRING_(Maximum, &#x27;m&#x27;, &#x27;a&#x27;, &#x27;x&#x27;, &#x27;i&#x27;, &#x27;m&#x27;, &#x27;u&#x27;, &#x27;m&#x27;)
927      RAPIDJSON_STRING_(ExclusiveMinimum, &#x27;e&#x27;, &#x27;x&#x27;, &#x27;c&#x27;, &#x27;l&#x27;, &#x27;u&#x27;, &#x27;s&#x27;, &#x27;i&#x27;, &#x27;v&#x27;, &#x27;e&#x27;, &#x27;M&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;i&#x27;, &#x27;m&#x27;, &#x27;u&#x27;, &#x27;m&#x27;)
928      RAPIDJSON_STRING_(ExclusiveMaximum, &#x27;e&#x27;, &#x27;x&#x27;, &#x27;c&#x27;, &#x27;l&#x27;, &#x27;u&#x27;, &#x27;s&#x27;, &#x27;i&#x27;, &#x27;v&#x27;, &#x27;e&#x27;, &#x27;M&#x27;, &#x27;a&#x27;, &#x27;x&#x27;, &#x27;i&#x27;, &#x27;m&#x27;, &#x27;u&#x27;, &#x27;m&#x27;)
929      RAPIDJSON_STRING_(MultipleOf, &#x27;m&#x27;, &#x27;u&#x27;, &#x27;l&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;p&#x27;, &#x27;l&#x27;, &#x27;e&#x27;, &#x27;O&#x27;, &#x27;f&#x27;)
930      RAPIDJSON_STRING_(DefaultValue, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;a&#x27;, &#x27;u&#x27;, &#x27;l&#x27;, &#x27;t&#x27;)
931      RAPIDJSON_STRING_(Ref, &#x27;$&#x27;, &#x27;r&#x27;, &#x27;e&#x27;, &#x27;f&#x27;)
932      RAPIDJSON_STRING_(Id, &#x27;i&#x27;, &#x27;d&#x27;)
933      RAPIDJSON_STRING_(SchemeEnd, &#x27;:&#x27;)
934      RAPIDJSON_STRING_(AuthStart, &#x27;/&#x27;, &#x27;/&#x27;)
935      RAPIDJSON_STRING_(QueryStart, &#x27;?&#x27;)
936      RAPIDJSON_STRING_(FragStart, &#x27;#&#x27;)
937      RAPIDJSON_STRING_(Slash, &#x27;/&#x27;)
938      RAPIDJSON_STRING_(Dot, &#x27;.&#x27;)
939  #undef RAPIDJSON_STRING_
940  private:
941      enum SchemaValueType {
942          kNullSchemaType,
943          kBooleanSchemaType,
944          kObjectSchemaType,
945          kArraySchemaType,
946          kStringSchemaType,
947          kNumberSchemaType,
948          kIntegerSchemaType,
949          kTotalSchemaType
950      };
951  #if RAPIDJSON_SCHEMA_USE_INTERNALREGEX
952          typedef internal::GenericRegex&lt;EncodingType, AllocatorType&gt; RegexType;
953  #elif RAPIDJSON_SCHEMA_USE_STDREGEX
954          typedef std::basic_regex&lt;Ch&gt; RegexType;
955  #else
956          typedef char RegexType;
957  #endif
958      struct SchemaArray {
959          SchemaArray() : schemas(), count() {}
960          ~SchemaArray() { AllocatorType::Free(schemas); }
961          const SchemaType** schemas;
962          SizeType begin; 
963          SizeType count;
964      };
965      template &lt;typename V1, typename V2&gt;
966      void AddUniqueElement(V1&amp; a, const V2&amp; v) {
967          for (typename V1::ConstValueIterator itr = a.Begin(); itr != a.End(); ++itr)
968              if (*itr == v)
969                  return;
970          V1 c(v, *allocator_);
971          a.PushBack(c, *allocator_);
972      }
973      static const ValueType* GetMember(const ValueType&amp; value, const ValueType&amp; name) {
974          typename ValueType::ConstMemberIterator itr = value.FindMember(name);
975          return itr != value.MemberEnd() ? &amp;(itr-&gt;value) : 0;
976      }
977      static void AssignIfExist(bool&amp; out, const ValueType&amp; value, const ValueType&amp; name) {
978          if (const ValueType* v = GetMember(value, name))
979              if (v-&gt;IsBool())
980                  out = v-&gt;GetBool();
981      }
982      static void AssignIfExist(SizeType&amp; out, const ValueType&amp; value, const ValueType&amp; name) {
983          if (const ValueType* v = GetMember(value, name))
984              if (v-&gt;IsUint64() &amp;&amp; v-&gt;GetUint64() &lt;= SizeType(~0))
985                  out = static_cast&lt;SizeType&gt;(v-&gt;GetUint64());
986      }
987      void AssignIfExist(SchemaArray&amp; out, SchemaDocumentType&amp; schemaDocument, const PointerType&amp; p, const ValueType&amp; value, const ValueType&amp; name, const ValueType&amp; document) {
988          if (const ValueType* v = GetMember(value, name)) {
989              if (v-&gt;IsArray() &amp;&amp; v-&gt;Size() &gt; 0) {
990                  PointerType q = p.Append(name, allocator_);
991                  out.count = v-&gt;Size();
992                  out.schemas = static_cast&lt;const Schema**&gt;(allocator_-&gt;Malloc(out.count * sizeof(const Schema*)));
993                  memset(out.schemas, 0, sizeof(Schema*)* out.count);
994                  for (SizeType i = 0; i &lt; out.count; i++)
995                      schemaDocument.CreateSchema(&amp;out.schemas[i], q.Append(i, allocator_), (*v)[i], document, id_);
996                  out.begin = validatorCount_;
997                  validatorCount_ += out.count;
998              }
999          }
1000      }
1001  #if RAPIDJSON_SCHEMA_USE_INTERNALREGEX
1002      template &lt;typename ValueType&gt;
1003      RegexType* CreatePattern(const ValueType&amp; value) {
1004          if (value.IsString()) {
1005              RegexType* r = new (allocator_-&gt;Malloc(sizeof(RegexType))) RegexType(value.GetString(), allocator_);
1006              if (!r-&gt;IsValid()) {
1007                  r-&gt;~RegexType();
1008                  AllocatorType::Free(r);
1009                  r = 0;
1010              }
1011              return r;
1012          }
1013          return 0;
1014      }
1015      static bool IsPatternMatch(const RegexType* pattern, const Ch *str, SizeType) {
1016          GenericRegexSearch&lt;RegexType&gt; rs(*pattern);
1017          return rs.Search(str);
1018      }
1019  #elif RAPIDJSON_SCHEMA_USE_STDREGEX
1020      template &lt;typename ValueType&gt;
1021      RegexType* CreatePattern(const ValueType&amp; value) {
1022          if (value.IsString()) {
1023              RegexType *r = static_cast&lt;RegexType*&gt;(allocator_-&gt;Malloc(sizeof(RegexType)));
1024              try {
1025                  return new (r) RegexType(value.GetString(), std::size_t(value.GetStringLength()), std::regex_constants::ECMAScript);
1026              }
1027              catch (const std::regex_error&amp;) {
1028                  AllocatorType::Free(r);
1029              }
1030          }
1031          return 0;
1032      }
1033      static bool IsPatternMatch(const RegexType* pattern, const Ch *str, SizeType length) {
1034          std::match_results&lt;const Ch*&gt; r;
1035          return std::regex_search(str, str + length, r, *pattern);
1036      }
1037  #else
1038      template &lt;typename ValueType&gt;
1039      RegexType* CreatePattern(const ValueType&amp;) { return 0; }
1040      static bool IsPatternMatch(const RegexType*, const Ch *, SizeType) { return true; }
1041  #endif 
1042      void AddType(const ValueType&amp; type) {
1043          if      (type == GetNullString()   ) type_ |= 1 &lt;&lt; kNullSchemaType;
1044          else if (type == GetBooleanString()) type_ |= 1 &lt;&lt; kBooleanSchemaType;
1045          else if (type == GetObjectString() ) type_ |= 1 &lt;&lt; kObjectSchemaType;
1046          else if (type == GetArrayString()  ) type_ |= 1 &lt;&lt; kArraySchemaType;
1047          else if (type == GetStringString() ) type_ |= 1 &lt;&lt; kStringSchemaType;
1048          else if (type == GetIntegerString()) type_ |= 1 &lt;&lt; kIntegerSchemaType;
1049          else if (type == GetNumberString() ) type_ |= (1 &lt;&lt; kNumberSchemaType) | (1 &lt;&lt; kIntegerSchemaType);
1050      }
1051      bool CreateParallelValidator(Context&amp; context) const {
1052          if (enum_ || context.arrayUniqueness)
1053              context.hasher = context.factory.CreateHasher();
1054          if (validatorCount_) {
1055              RAPIDJSON_ASSERT(context.validators == 0);
1056              context.validators = static_cast&lt;ISchemaValidator**&gt;(context.factory.MallocState(sizeof(ISchemaValidator*) * validatorCount_));
1057              context.validatorCount = validatorCount_;
1058              if (allOf_.schemas)
1059                  CreateSchemaValidators(context, allOf_, false);
1060              if (anyOf_.schemas)
1061                  CreateSchemaValidators(context, anyOf_, false);
1062              if (oneOf_.schemas)
1063                  CreateSchemaValidators(context, oneOf_, false);
1064              if (not_)
1065                  context.validators[notValidatorIndex_] = context.factory.CreateSchemaValidator(*not_, false);
1066              if (hasSchemaDependencies_) {
1067                  for (SizeType i = 0; i &lt; propertyCount_; i++)
1068                      if (properties_[i].dependenciesSchema)
1069                          context.validators[properties_[i].dependenciesValidatorIndex] = context.factory.CreateSchemaValidator(*properties_[i].dependenciesSchema, false);
1070              }
1071          }
1072          return true;
1073      }
1074      void CreateSchemaValidators(Context&amp; context, const SchemaArray&amp; schemas, const bool inheritContinueOnErrors) const {
1075          for (SizeType i = 0; i &lt; schemas.count; i++)
1076              context.validators[schemas.begin + i] = context.factory.CreateSchemaValidator(*schemas.schemas[i], inheritContinueOnErrors);
1077      }
1078      bool FindPropertyIndex(const ValueType&amp; name, SizeType* outIndex) const {
1079          SizeType len = name.GetStringLength();
1080          const Ch* str = name.GetString();
1081          for (SizeType index = 0; index &lt; propertyCount_; index++)
1082              if (properties_[index].name.GetStringLength() == len &amp;&amp;
1083                  (std::memcmp(properties_[index].name.GetString(), str, sizeof(Ch) * len) == 0))
1084              {
1085                  *outIndex = index;
1086                  return true;
1087              }
1088          return false;
1089      }
1090      bool CheckInt(Context&amp; context, int64_t i) const {
1091          if (!(type_ &amp; ((1 &lt;&lt; kIntegerSchemaType) | (1 &lt;&lt; kNumberSchemaType)))) {
1092              DisallowedType(context, GetIntegerString());
1093              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
1094          }
1095          if (!minimum_.IsNull()) {
1096              if (minimum_.IsInt64()) {
1097                  if (exclusiveMinimum_ ? i &lt;= minimum_.GetInt64() : i &lt; minimum_.GetInt64()) {
1098                      context.error_handler.BelowMinimum(i, minimum_, exclusiveMinimum_);
1099                      RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMinimum_ ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum);
1100                  }
1101              }
1102              else if (minimum_.IsUint64()) {
1103                  context.error_handler.BelowMinimum(i, minimum_, exclusiveMinimum_);
1104                  RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMinimum_ ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum); 
1105              }
1106              else if (!CheckDoubleMinimum(context, static_cast&lt;double&gt;(i)))
1107                  return false;
1108          }
1109          if (!maximum_.IsNull()) {
1110              if (maximum_.IsInt64()) {
1111                  if (exclusiveMaximum_ ? i &gt;= maximum_.GetInt64() : i &gt; maximum_.GetInt64()) {
1112                      context.error_handler.AboveMaximum(i, maximum_, exclusiveMaximum_);
1113                      RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMaximum_ ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum);
1114                  }
1115              }
1116              else if (maximum_.IsUint64()) { }
1117              else if (!CheckDoubleMaximum(context, static_cast&lt;double&gt;(i)))
1118                  return false;
1119          }
1120          if (!multipleOf_.IsNull()) {
1121              if (multipleOf_.IsUint64()) {
1122                  if (static_cast&lt;uint64_t&gt;(i &gt;= 0 ? i : -i) % multipleOf_.GetUint64() != 0) {
1123                      context.error_handler.NotMultipleOf(i, multipleOf_);
1124                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMultipleOf);
1125                  }
1126              }
1127              else if (!CheckDoubleMultipleOf(context, static_cast&lt;double&gt;(i)))
1128                  return false;
1129          }
1130          return true;
1131      }
1132      bool CheckUint(Context&amp; context, uint64_t i) const {
1133          if (!(type_ &amp; ((1 &lt;&lt; kIntegerSchemaType) | (1 &lt;&lt; kNumberSchemaType)))) {
1134              DisallowedType(context, GetIntegerString());
1135              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorType);
1136          }
1137          if (!minimum_.IsNull()) {
1138              if (minimum_.IsUint64()) {
1139                  if (exclusiveMinimum_ ? i &lt;= minimum_.GetUint64() : i &lt; minimum_.GetUint64()) {
1140                      context.error_handler.BelowMinimum(i, minimum_, exclusiveMinimum_);
1141                      RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMinimum_ ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum);
1142                  }
1143              }
1144              else if (minimum_.IsInt64())
1145                  ; 
1146              else if (!CheckDoubleMinimum(context, static_cast&lt;double&gt;(i)))
1147                  return false;
1148          }
1149          if (!maximum_.IsNull()) {
1150              if (maximum_.IsUint64()) {
1151                  if (exclusiveMaximum_ ? i &gt;= maximum_.GetUint64() : i &gt; maximum_.GetUint64()) {
1152                      context.error_handler.AboveMaximum(i, maximum_, exclusiveMaximum_);
1153                      RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMaximum_ ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum);
1154                  }
1155              }
1156              else if (maximum_.IsInt64()) {
1157                  context.error_handler.AboveMaximum(i, maximum_, exclusiveMaximum_);
1158                  RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMaximum_ ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum); 
1159              }
1160              else if (!CheckDoubleMaximum(context, static_cast&lt;double&gt;(i)))
1161                  return false;
1162          }
1163          if (!multipleOf_.IsNull()) {
1164              if (multipleOf_.IsUint64()) {
1165                  if (i % multipleOf_.GetUint64() != 0) {
1166                      context.error_handler.NotMultipleOf(i, multipleOf_);
1167                      RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMultipleOf);
1168                  }
1169              }
1170              else if (!CheckDoubleMultipleOf(context, static_cast&lt;double&gt;(i)))
1171                  return false;
1172          }
1173          return true;
1174      }
1175      bool CheckDoubleMinimum(Context&amp; context, double d) const {
1176          if (exclusiveMinimum_ ? d &lt;= minimum_.GetDouble() : d &lt; minimum_.GetDouble()) {
1177              context.error_handler.BelowMinimum(d, minimum_, exclusiveMinimum_);
1178              RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMinimum_ ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum);
1179          }
1180          return true;
1181      }
1182      bool CheckDoubleMaximum(Context&amp; context, double d) const {
1183          if (exclusiveMaximum_ ? d &gt;= maximum_.GetDouble() : d &gt; maximum_.GetDouble()) {
1184              context.error_handler.AboveMaximum(d, maximum_, exclusiveMaximum_);
1185              RAPIDJSON_INVALID_KEYWORD_RETURN(exclusiveMaximum_ ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum);
1186          }
1187          return true;
1188      }
1189      bool CheckDoubleMultipleOf(Context&amp; context, double d) const {
1190          double a = std::abs(d), b = std::abs(multipleOf_.GetDouble());
1191          double q = std::floor(a / b);
1192          double r = a - q * b;
1193          if (r &gt; 0.0) {
1194              context.error_handler.NotMultipleOf(d, multipleOf_);
1195              RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorMultipleOf);
1196          }
1197          return true;
1198      }
1199      void DisallowedType(Context&amp; context, const ValueType&amp; actualType) const {
1200          ErrorHandler&amp; eh = context.error_handler;
1201          eh.StartDisallowedType();
1202          if (type_ &amp; (1 &lt;&lt; kNullSchemaType)) eh.AddExpectedType(GetNullString());
1203          if (type_ &amp; (1 &lt;&lt; kBooleanSchemaType)) eh.AddExpectedType(GetBooleanString());
1204          if (type_ &amp; (1 &lt;&lt; kObjectSchemaType)) eh.AddExpectedType(GetObjectString());
1205          if (type_ &amp; (1 &lt;&lt; kArraySchemaType)) eh.AddExpectedType(GetArrayString());
1206          if (type_ &amp; (1 &lt;&lt; kStringSchemaType)) eh.AddExpectedType(GetStringString());
1207          if (type_ &amp; (1 &lt;&lt; kNumberSchemaType)) eh.AddExpectedType(GetNumberString());
1208          else if (type_ &amp; (1 &lt;&lt; kIntegerSchemaType)) eh.AddExpectedType(GetIntegerString());
1209          eh.EndDisallowedType(actualType);
1210      }
1211      struct Property {
1212          Property() : schema(), dependenciesSchema(), dependenciesValidatorIndex(), dependencies(), required(false) {}
1213          ~Property() { AllocatorType::Free(dependencies); }
1214          SValue name;
1215          const SchemaType* schema;
1216          const SchemaType* dependenciesSchema;
1217          SizeType dependenciesValidatorIndex;
1218          bool* dependencies;
1219          bool required;
1220      };
1221      struct PatternProperty {
1222          PatternProperty() : schema(), pattern() {}
1223          ~PatternProperty() {
1224              if (pattern) {
1225                  pattern-&gt;~RegexType();
1226                  AllocatorType::Free(pattern);
1227              }
1228          }
1229          const SchemaType* schema;
1230          RegexType* pattern;
1231      };
1232      AllocatorType* allocator_;
1233      SValue uri_;
1234      UriType id_;
1235      PointerType pointer_;
1236      const SchemaType* typeless_;
1237      uint64_t* enum_;
1238      SizeType enumCount_;
1239      SchemaArray allOf_;
1240      SchemaArray anyOf_;
1241      SchemaArray oneOf_;
1242      const SchemaType* not_;
1243      unsigned type_; 
1244      SizeType validatorCount_;
1245      SizeType notValidatorIndex_;
1246      Property* properties_;
1247      const SchemaType* additionalPropertiesSchema_;
1248      PatternProperty* patternProperties_;
1249      SizeType patternPropertyCount_;
1250      SizeType propertyCount_;
1251      SizeType minProperties_;
1252      SizeType maxProperties_;
1253      bool additionalProperties_;
1254      bool hasDependencies_;
1255      bool hasRequired_;
1256      bool hasSchemaDependencies_;
1257      const SchemaType* additionalItemsSchema_;
1258      const SchemaType* itemsList_;
1259      const SchemaType** itemsTuple_;
1260      SizeType itemsTupleCount_;
1261      SizeType minItems_;
1262      SizeType maxItems_;
1263      bool additionalItems_;
1264      bool uniqueItems_;
1265      RegexType* pattern_;
1266      SizeType minLength_;
1267      SizeType maxLength_;
1268      SValue minimum_;
1269      SValue maximum_;
1270      SValue multipleOf_;
1271      bool exclusiveMinimum_;
1272      bool exclusiveMaximum_;
1273      SizeType defaultValueLength_;
1274  };
1275  template&lt;typename Stack, typename Ch&gt;
1276  struct TokenHelper {
1277      RAPIDJSON_FORCEINLINE static void AppendIndexToken(Stack&amp; documentStack, SizeType index) {
1278          *documentStack.template Push&lt;Ch&gt;() = &#x27;/&#x27;;
1279          char buffer[21];
1280          size_t length = static_cast&lt;size_t&gt;((sizeof(SizeType) == 4 ? u32toa(index, buffer) : u64toa(index, buffer)) - buffer);
1281          for (size_t i = 0; i &lt; length; i++)
1282              *documentStack.template Push&lt;Ch&gt;() = static_cast&lt;Ch&gt;(buffer[i]);
1283      }
1284  };
1285  template &lt;typename Stack&gt;
1286  struct TokenHelper&lt;Stack, char&gt; {
1287      RAPIDJSON_FORCEINLINE static void AppendIndexToken(Stack&amp; documentStack, SizeType index) {
1288          if (sizeof(SizeType) == 4) {
1289              char *buffer = documentStack.template Push&lt;char&gt;(1 + 10); 
1290              *buffer++ = &#x27;/&#x27;;
1291              const char* end = internal::u32toa(index, buffer);
1292               documentStack.template Pop&lt;char&gt;(static_cast&lt;size_t&gt;(10 - (end - buffer)));
1293          }
1294          else {
1295              char *buffer = documentStack.template Push&lt;char&gt;(1 + 20); 
1296              *buffer++ = &#x27;/&#x27;;
1297              const char* end = internal::u64toa(index, buffer);
1298              documentStack.template Pop&lt;char&gt;(static_cast&lt;size_t&gt;(20 - (end - buffer)));
1299          }
1300      }
1301  };
1302  } 
1303  template &lt;typename SchemaDocumentType&gt;
1304  class IGenericRemoteSchemaDocumentProvider {
1305  public:
1306      typedef typename SchemaDocumentType::Ch Ch;
1307      typedef typename SchemaDocumentType::ValueType ValueType;
1308      typedef typename SchemaDocumentType::AllocatorType AllocatorType;
1309      virtual ~IGenericRemoteSchemaDocumentProvider() {}
1310      virtual const SchemaDocumentType* GetRemoteDocument(const Ch* uri, SizeType length) = 0;
1311      virtual const SchemaDocumentType* GetRemoteDocument(GenericUri&lt;ValueType, AllocatorType&gt; uri) { return GetRemoteDocument(uri.GetBaseString(), uri.GetBaseStringLength()); }
1312  };
1313  template &lt;typename ValueT, typename Allocator = CrtAllocator&gt;
1314  class GenericSchemaDocument {
1315  public:
1316      typedef ValueT ValueType;
1317      typedef IGenericRemoteSchemaDocumentProvider&lt;GenericSchemaDocument&gt; IRemoteSchemaDocumentProviderType;
1318      typedef Allocator AllocatorType;
1319      typedef typename ValueType::EncodingType EncodingType;
1320      typedef typename EncodingType::Ch Ch;
1321      typedef internal::Schema&lt;GenericSchemaDocument&gt; SchemaType;
1322      typedef GenericPointer&lt;ValueType, Allocator&gt; PointerType;
1323      typedef GenericValue&lt;EncodingType, AllocatorType&gt; SValue;
1324      typedef GenericUri&lt;ValueType, Allocator&gt; UriType;
1325      friend class internal::Schema&lt;GenericSchemaDocument&gt;;
1326      template &lt;typename, typename, typename&gt;
1327      friend class GenericSchemaValidator;
1328      explicit GenericSchemaDocument(const ValueType&amp; document, const Ch* uri = 0, SizeType uriLength = 0,
1329          IRemoteSchemaDocumentProviderType* remoteProvider = 0, Allocator* allocator = 0,
1330          const PointerType&amp; pointer = PointerType()) :  
1331          remoteProvider_(remoteProvider),
1332          allocator_(allocator),
1333          ownAllocator_(),
1334          root_(),
1335          typeless_(),
1336          schemaMap_(allocator, kInitialSchemaMapSize),
1337          schemaRef_(allocator, kInitialSchemaRefSize)
1338      {
1339          if (!allocator_)
1340              ownAllocator_ = allocator_ = RAPIDJSON_NEW(Allocator)();
1341          Ch noUri[1] = {0};
1342          uri_.SetString(uri ? uri : noUri, uriLength, *allocator_);
1343          docId_ = UriType(uri_, allocator_);
1344          typeless_ = static_cast&lt;SchemaType*&gt;(allocator_-&gt;Malloc(sizeof(SchemaType)));
1345          new (typeless_) SchemaType(this, PointerType(), ValueType(kObjectType).Move(), ValueType(kObjectType).Move(), allocator_, docId_);
1346          root_ = typeless_;
1347          if (pointer.GetTokenCount() == 0) {
1348              CreateSchemaRecursive(&amp;root_, pointer, document, document, docId_);
1349          }
1350          else if (const ValueType* v = pointer.Get(document)) {
1351              CreateSchema(&amp;root_, pointer, *v, document, docId_);
1352          }
1353          RAPIDJSON_ASSERT(root_ != 0);
1354          schemaRef_.ShrinkToFit(); 
1355      }
1356  #if RAPIDJSON_HAS_CXX11_RVALUE_REFS
1357      GenericSchemaDocument(GenericSchemaDocument&amp;&amp; rhs) RAPIDJSON_NOEXCEPT :
1358          remoteProvider_(rhs.remoteProvider_),
1359          allocator_(rhs.allocator_),
1360          ownAllocator_(rhs.ownAllocator_),
1361          root_(rhs.root_),
1362          typeless_(rhs.typeless_),
1363          schemaMap_(std::move(rhs.schemaMap_)),
1364          schemaRef_(std::move(rhs.schemaRef_)),
1365          uri_(std::move(rhs.uri_)),
1366          docId_(rhs.docId_)
1367      {
1368          rhs.remoteProvider_ = 0;
1369          rhs.allocator_ = 0;
1370          rhs.ownAllocator_ = 0;
1371          rhs.typeless_ = 0;
1372      }
1373  #endif
1374      ~GenericSchemaDocument() {
1375          while (!schemaMap_.Empty())
1376              schemaMap_.template Pop&lt;SchemaEntry&gt;(1)-&gt;~SchemaEntry();
1377          if (typeless_) {
1378              typeless_-&gt;~SchemaType();
1379              Allocator::Free(typeless_);
1380          }
1381          RAPIDJSON_DELETE(ownAllocator_);
1382      }
1383      const SValue&amp; GetURI() const { return uri_; }
1384      const SchemaType&amp; GetRoot() const { return *root_; }
1385  private:
1386      GenericSchemaDocument(const GenericSchemaDocument&amp;);
1387      GenericSchemaDocument&amp; operator=(const GenericSchemaDocument&amp;);
1388      typedef const PointerType* SchemaRefPtr; 
1389      struct SchemaEntry {
1390          SchemaEntry(const PointerType&amp; p, SchemaType* s, bool o, Allocator* allocator) : pointer(p, allocator), schema(s), owned(o) {}
1391          ~SchemaEntry() {
1392              if (owned) {
1393                  schema-&gt;~SchemaType();
1394                  Allocator::Free(schema);
1395              }
1396          }
1397          PointerType pointer;
1398          SchemaType* schema;
1399          bool owned;
1400      };
1401      void CreateSchemaRecursive(const SchemaType** schema, const PointerType&amp; pointer, const ValueType&amp; v, const ValueType&amp; document, const UriType&amp; id) {
1402          if (v.GetType() == kObjectType) {
1403              UriType newid = UriType(CreateSchema(schema, pointer, v, document, id), allocator_);
1404              for (typename ValueType::ConstMemberIterator itr = v.MemberBegin(); itr != v.MemberEnd(); ++itr)
1405                  CreateSchemaRecursive(0, pointer.Append(itr-&gt;name, allocator_), itr-&gt;value, document, newid);
1406          }
1407          else if (v.GetType() == kArrayType)
1408              for (SizeType i = 0; i &lt; v.Size(); i++)
1409                  CreateSchemaRecursive(0, pointer.Append(i, allocator_), v[i], document, id);
1410      }
1411      const UriType&amp; CreateSchema(const SchemaType** schema, const PointerType&amp; pointer, const ValueType&amp; v, const ValueType&amp; document, const UriType&amp; id) {
1412          RAPIDJSON_ASSERT(pointer.IsValid());
1413          if (v.IsObject()) {
1414              if (const SchemaType* sc = GetSchema(pointer)) {
1415                  if (schema)
1416                      *schema = sc;
1417                  AddSchemaRefs(const_cast&lt;SchemaType*&gt;(sc));
1418              }
1419              else if (!HandleRefSchema(pointer, schema, v, document, id)) {
1420                  SchemaType* s = new (allocator_-&gt;Malloc(sizeof(SchemaType))) SchemaType(this, pointer, v, document, allocator_, id);
1421                  if (schema)
1422                      *schema = s;
1423                  return s-&gt;GetId();
1424              }
1425          }
1426          else {
1427              if (schema)
1428                  *schema = typeless_;
1429              AddSchemaRefs(typeless_);
1430          }
1431          return id;
1432      }
1433      bool HandleRefSchema(const PointerType&amp; source, const SchemaType** schema, const ValueType&amp; v, const ValueType&amp; document, const UriType&amp; id) {
1434          typename ValueType::ConstMemberIterator itr = v.FindMember(SchemaType::GetRefString());
1435          if (itr == v.MemberEnd())
1436              return false;
1437          new (schemaRef_.template Push&lt;SchemaRefPtr&gt;()) SchemaRefPtr(&amp;source);
1438          if (itr-&gt;value.IsString()) {
1439              SizeType len = itr-&gt;value.GetStringLength();
1440              if (len &gt; 0) {
1441                  UriType scopeId = UriType(id, allocator_);
1442                  UriType ref = UriType(itr-&gt;value, allocator_).Resolve(scopeId, allocator_);
1443                  PointerType basePointer = PointerType();
1444                  const ValueType *base = FindId(document, ref, basePointer, docId_, false);
1445                  if (!base) {
1446                      if (remoteProvider_) {
1447                          if (const GenericSchemaDocument* remoteDocument = remoteProvider_-&gt;GetRemoteDocument(ref)) {
1448                              const Ch* s = ref.GetFragString();
1449                              len = ref.GetFragStringLength();
1450                              if (len &lt;= 1 || s[1] == &#x27;/&#x27;) {
1451                                  const PointerType pointer(s, len, allocator_);
1452                                  if (pointer.IsValid()) {
1453                                      if (const SchemaType *sc = remoteDocument-&gt;GetSchema(pointer)) {
1454                                          if (schema)
1455                                              *schema = sc;
1456                                          AddSchemaRefs(const_cast&lt;SchemaType *&gt;(sc));
1457                                          return true;
1458                                      }
1459                                  }
1460                            } else {
1461                            }
1462                          }
1463                      }
1464                  }
1465                  else { 
1466                      const Ch* s = ref.GetFragString();
1467                      len = ref.GetFragStringLength();
1468                      if (len &lt;= 1 || s[1] == &#x27;/&#x27;) {
1469                          const PointerType relPointer(s, len, allocator_);
1470                          if (relPointer.IsValid()) {
1471                              if (const ValueType *pv = relPointer.Get(*base)) {
1472                                  PointerType pointer(basePointer);
1473                                  for (SizeType i = 0; i &lt; relPointer.GetTokenCount(); i++)
1474                                      pointer = pointer.Append(relPointer.GetTokens()[i], allocator_);
1475                                  if (pointer.IsValid() &amp;&amp; !IsCyclicRef(pointer)) {
1476                                      size_t unresolvedTokenIndex;
1477                                      scopeId = pointer.GetUri(document, docId_, &amp;unresolvedTokenIndex, allocator_);
1478                                      CreateSchema(schema, pointer, *pv, document, scopeId);
1479                                      return true;
1480                                  }
1481                              }
1482                          }
1483                      } else {
1484                          PointerType pointer = PointerType();
1485                          if (const ValueType *pv = FindId(*base, ref, pointer, UriType(ref.GetBaseString(), ref.GetBaseStringLength(), allocator_), true, basePointer)) {
1486                              if (!IsCyclicRef(pointer)) {
1487                                  size_t unresolvedTokenIndex;
1488                                  scopeId = pointer.GetUri(document, docId_, &amp;unresolvedTokenIndex, allocator_);
1489                                  CreateSchema(schema, pointer, *pv, document, scopeId);
1490                                  return true;
1491                              }
1492                          }
1493                      }
1494                  }
1495              }
1496          }
1497          if (schema)
1498              *schema = typeless_;
1499          AddSchemaRefs(typeless_);
1500          return true;
1501      }
1502      ValueType* FindId(const ValueType&amp; doc, const UriType&amp; finduri, PointerType&amp; resptr, const UriType&amp; baseuri, bool full, const PointerType&amp; here = PointerType()) const {
1503          SizeType i = 0;
1504          ValueType* resval = 0;
1505          UriType tempuri = UriType(finduri, allocator_);
1506          UriType localuri = UriType(baseuri, allocator_);
1507          if (doc.GetType() == kObjectType) {
1508              typename ValueType::ConstMemberIterator m = doc.FindMember(SchemaType::GetIdString());
1509              if (m != doc.MemberEnd() &amp;&amp; m-&gt;value.GetType() == kStringType) {
1510                  localuri = UriType(m-&gt;value, allocator_).Resolve(baseuri, allocator_);
1511              }
1512              if (localuri.Match(finduri, full)) {
1513                  resval = const_cast&lt;ValueType *&gt;(&amp;doc);
1514                  resptr = here;
1515                  return resval;
1516              }
1517              for (m = doc.MemberBegin(); m != doc.MemberEnd(); ++m) {
1518                  if (m-&gt;value.GetType() == kObjectType || m-&gt;value.GetType() == kArrayType) {
1519                      resval = FindId(m-&gt;value, finduri, resptr, localuri, full, here.Append(m-&gt;name.GetString(), m-&gt;name.GetStringLength(), allocator_));
1520                  }
1521                  if (resval) break;
1522              }
1523          } else if (doc.GetType() == kArrayType) {
1524              for (typename ValueType::ConstValueIterator v = doc.Begin(); v != doc.End(); ++v) {
1525                  if (v-&gt;GetType() == kObjectType || v-&gt;GetType() == kArrayType) {
1526                      resval = FindId(*v, finduri, resptr, localuri, full, here.Append(i, allocator_));
1527                  }
1528                  if (resval) break;
1529                  i++;
1530              }
1531          }
1532          return resval;
1533      }
1534      void AddSchemaRefs(SchemaType* schema) {
1535          while (!schemaRef_.Empty()) {
1536              SchemaRefPtr *ref = schemaRef_.template Pop&lt;SchemaRefPtr&gt;(1);
1537              SchemaEntry *entry = schemaMap_.template Push&lt;SchemaEntry&gt;();
1538              new (entry) SchemaEntry(**ref, schema, false, allocator_);
1539          }
1540      }
1541      bool IsCyclicRef(const PointerType&amp; pointer) const {
1542          for (const SchemaRefPtr* ref = schemaRef_.template Bottom&lt;SchemaRefPtr&gt;(); ref != schemaRef_.template End&lt;SchemaRefPtr&gt;(); ++ref)
1543              if (pointer == **ref)
1544                  return true;
1545          return false;
1546      }
1547      const SchemaType* GetSchema(const PointerType&amp; pointer) const {
1548          for (const SchemaEntry* target = schemaMap_.template Bottom&lt;SchemaEntry&gt;(); target != schemaMap_.template End&lt;SchemaEntry&gt;(); ++target)
1549              if (pointer == target-&gt;pointer)
1550                  return target-&gt;schema;
1551          return 0;
1552      }
1553      PointerType GetPointer(const SchemaType* schema) const {
1554          for (const SchemaEntry* target = schemaMap_.template Bottom&lt;SchemaEntry&gt;(); target != schemaMap_.template End&lt;SchemaEntry&gt;(); ++target)
1555              if (schema == target-&gt;schema)
1556                  return target-&gt;pointer;
1557          return PointerType();
1558      }
1559      const SchemaType* GetTypeless() const { return typeless_; }
1560      static const size_t kInitialSchemaMapSize = 64;
1561      static const size_t kInitialSchemaRefSize = 64;
1562      IRemoteSchemaDocumentProviderType* remoteProvider_;
1563      Allocator *allocator_;
1564      Allocator *ownAllocator_;
1565      const SchemaType* root_;                
1566      SchemaType* typeless_;
1567      internal::Stack&lt;Allocator&gt; schemaMap_;  
1568      internal::Stack&lt;Allocator&gt; schemaRef_;  
1569      SValue uri_;                            
1570      UriType docId_;
1571  };
1572  typedef GenericSchemaDocument&lt;Value&gt; SchemaDocument;
1573  typedef IGenericRemoteSchemaDocumentProvider&lt;SchemaDocument&gt; IRemoteSchemaDocumentProvider;
1574  template &lt;
1575      typename SchemaDocumentType,
1576      typename OutputHandler = BaseReaderHandler&lt;typename SchemaDocumentType::SchemaType::EncodingType&gt;,
1577      typename StateAllocator = CrtAllocator&gt;
1578  class GenericSchemaValidator :
1579      public internal::ISchemaStateFactory&lt;typename SchemaDocumentType::SchemaType&gt;,
1580      public internal::ISchemaValidator,
1581      public internal::IValidationErrorHandler&lt;typename SchemaDocumentType::SchemaType&gt; {
1582  public:
1583      typedef typename SchemaDocumentType::SchemaType SchemaType;
1584      typedef typename SchemaDocumentType::PointerType PointerType;
1585      typedef typename SchemaType::EncodingType EncodingType;
1586      typedef typename SchemaType::SValue SValue;
1587      typedef typename EncodingType::Ch Ch;
1588      typedef GenericStringRef&lt;Ch&gt; StringRefType;
1589      typedef GenericValue&lt;EncodingType, StateAllocator&gt; ValueType;
1590      GenericSchemaValidator(
1591          const SchemaDocumentType&amp; schemaDocument,
1592          StateAllocator* allocator = 0,
1593          size_t schemaStackCapacity = kDefaultSchemaStackCapacity,
1594          size_t documentStackCapacity = kDefaultDocumentStackCapacity)
1595          :
1596          schemaDocument_(&amp;schemaDocument),
1597          root_(schemaDocument.GetRoot()),
1598          stateAllocator_(allocator),
1599          ownStateAllocator_(0),
1600          schemaStack_(allocator, schemaStackCapacity),
1601          documentStack_(allocator, documentStackCapacity),
1602          outputHandler_(0),
1603          error_(kObjectType),
1604          currentError_(),
1605          missingDependents_(),
1606          valid_(true),
1607          flags_(kValidateDefaultFlags)
1608  #if RAPIDJSON_SCHEMA_VERBOSE
1609          , depth_(0)
1610  #endif
1611      {
1612      }
1613      GenericSchemaValidator(
1614          const SchemaDocumentType&amp; schemaDocument,
1615          OutputHandler&amp; outputHandler,
1616          StateAllocator* allocator = 0,
1617          size_t schemaStackCapacity = kDefaultSchemaStackCapacity,
1618          size_t documentStackCapacity = kDefaultDocumentStackCapacity)
1619          :
1620          schemaDocument_(&amp;schemaDocument),
1621          root_(schemaDocument.GetRoot()),
1622          stateAllocator_(allocator),
1623          ownStateAllocator_(0),
1624          schemaStack_(allocator, schemaStackCapacity),
1625          documentStack_(allocator, documentStackCapacity),
1626          outputHandler_(&amp;outputHandler),
1627          error_(kObjectType),
1628          currentError_(),
1629          missingDependents_(),
1630          valid_(true),
1631          flags_(kValidateDefaultFlags)
1632  #if RAPIDJSON_SCHEMA_VERBOSE
1633          , depth_(0)
1634  #endif
1635      {
1636      }
1637      ~GenericSchemaValidator() {
1638          Reset();
1639          RAPIDJSON_DELETE(ownStateAllocator_);
1640      }
1641      void Reset() {
1642          while (!schemaStack_.Empty())
1643              PopSchema();
1644          documentStack_.Clear();
1645          ResetError();
1646      }
1647      void ResetError() {
1648          error_.SetObject();
1649          currentError_.SetNull();
1650          missingDependents_.SetNull();
1651          valid_ = true;
1652      }
1653      void SetValidateFlags(unsigned flags) {
1654          flags_ = flags;
1655      }
1656      virtual unsigned GetValidateFlags() const {
1657          return flags_;
1658      }
1659      virtual bool IsValid() const {
1660          if (!valid_) return false;
1661          if (GetContinueOnErrors() &amp;&amp; !error_.ObjectEmpty()) return false;
1662          return true;
1663      }
1664      ValueType&amp; GetError() { return error_; }
1665      const ValueType&amp; GetError() const { return error_; }
1666      PointerType GetInvalidSchemaPointer() const {
1667          return schemaStack_.Empty() ? PointerType() : CurrentSchema().GetPointer();
1668      }
1669      const Ch* GetInvalidSchemaKeyword() const {
1670          if (!schemaStack_.Empty()) return CurrentContext().invalidKeyword;
1671          if (GetContinueOnErrors() &amp;&amp; !error_.ObjectEmpty()) return (const Ch*)GetErrorsString();
1672          return 0;
1673      }
1674      ValidateErrorCode GetInvalidSchemaCode() const {
1675          if (!schemaStack_.Empty()) return CurrentContext().invalidCode;
1676          if (GetContinueOnErrors() &amp;&amp; !error_.ObjectEmpty()) return kValidateErrors;
1677          return kValidateErrorNone;
1678      }
1679      PointerType GetInvalidDocumentPointer() const {
1680          if (documentStack_.Empty()) {
1681              return PointerType();
1682          }
1683          else {
1684              return PointerType(documentStack_.template Bottom&lt;Ch&gt;(), documentStack_.GetSize() / sizeof(Ch));
1685          }
1686      }
1687      void NotMultipleOf(int64_t actual, const SValue&amp; expected) {
1688          AddNumberError(kValidateErrorMultipleOf, ValueType(actual).Move(), expected);
1689      }
1690      void NotMultipleOf(uint64_t actual, const SValue&amp; expected) {
1691          AddNumberError(kValidateErrorMultipleOf, ValueType(actual).Move(), expected);
1692      }
1693      void NotMultipleOf(double actual, const SValue&amp; expected) {
1694          AddNumberError(kValidateErrorMultipleOf, ValueType(actual).Move(), expected);
1695      }
1696      void AboveMaximum(int64_t actual, const SValue&amp; expected, bool exclusive) {
1697          AddNumberError(exclusive ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum, ValueType(actual).Move(), expected,
1698              exclusive ? &amp;SchemaType::GetExclusiveMaximumString : 0);
1699      }
1700      void AboveMaximum(uint64_t actual, const SValue&amp; expected, bool exclusive) {
1701          AddNumberError(exclusive ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum, ValueType(actual).Move(), expected,
1702              exclusive ? &amp;SchemaType::GetExclusiveMaximumString : 0);
1703      }
1704      void AboveMaximum(double actual, const SValue&amp; expected, bool exclusive) {
1705          AddNumberError(exclusive ? kValidateErrorExclusiveMaximum : kValidateErrorMaximum, ValueType(actual).Move(), expected,
1706              exclusive ? &amp;SchemaType::GetExclusiveMaximumString : 0);
1707      }
1708      void BelowMinimum(int64_t actual, const SValue&amp; expected, bool exclusive) {
1709          AddNumberError(exclusive ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum, ValueType(actual).Move(), expected,
1710              exclusive ? &amp;SchemaType::GetExclusiveMinimumString : 0);
1711      }
1712      void BelowMinimum(uint64_t actual, const SValue&amp; expected, bool exclusive) {
1713          AddNumberError(exclusive ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum, ValueType(actual).Move(), expected,
1714              exclusive ? &amp;SchemaType::GetExclusiveMinimumString : 0);
1715      }
1716      void BelowMinimum(double actual, const SValue&amp; expected, bool exclusive) {
1717          AddNumberError(exclusive ? kValidateErrorExclusiveMinimum : kValidateErrorMinimum, ValueType(actual).Move(), expected,
1718              exclusive ? &amp;SchemaType::GetExclusiveMinimumString : 0);
1719      }
1720      void TooLong(const Ch* str, SizeType length, SizeType expected) {
1721          AddNumberError(kValidateErrorMaxLength,
1722              ValueType(str, length, GetStateAllocator()).Move(), SValue(expected).Move());
1723      }
1724      void TooShort(const Ch* str, SizeType length, SizeType expected) {
1725          AddNumberError(kValidateErrorMinLength,
1726              ValueType(str, length, GetStateAllocator()).Move(), SValue(expected).Move());
1727      }
1728      void DoesNotMatch(const Ch* str, SizeType length) {
1729          currentError_.SetObject();
1730          currentError_.AddMember(GetActualString(), ValueType(str, length, GetStateAllocator()).Move(), GetStateAllocator());
1731          AddCurrentError(kValidateErrorPattern);
1732      }
1733      void DisallowedItem(SizeType index) {
1734          currentError_.SetObject();
1735          currentError_.AddMember(GetDisallowedString(), ValueType(index).Move(), GetStateAllocator());
1736          AddCurrentError(kValidateErrorAdditionalItems, true);
1737      }
1738      void TooFewItems(SizeType actualCount, SizeType expectedCount) {
1739          AddNumberError(kValidateErrorMinItems,
1740              ValueType(actualCount).Move(), SValue(expectedCount).Move());
1741      }
1742      void TooManyItems(SizeType actualCount, SizeType expectedCount) {
1743          AddNumberError(kValidateErrorMaxItems,
1744              ValueType(actualCount).Move(), SValue(expectedCount).Move());
1745      }
1746      void DuplicateItems(SizeType index1, SizeType index2) {
1747          ValueType duplicates(kArrayType);
1748          duplicates.PushBack(index1, GetStateAllocator());
1749          duplicates.PushBack(index2, GetStateAllocator());
1750          currentError_.SetObject();
1751          currentError_.AddMember(GetDuplicatesString(), duplicates, GetStateAllocator());
1752          AddCurrentError(kValidateErrorUniqueItems, true);
1753      }
1754      void TooManyProperties(SizeType actualCount, SizeType expectedCount) {
1755          AddNumberError(kValidateErrorMaxProperties,
1756              ValueType(actualCount).Move(), SValue(expectedCount).Move());
1757      }
1758      void TooFewProperties(SizeType actualCount, SizeType expectedCount) {
1759          AddNumberError(kValidateErrorMinProperties,
1760              ValueType(actualCount).Move(), SValue(expectedCount).Move());
1761      }
1762      void StartMissingProperties() {
1763          currentError_.SetArray();
1764      }
1765      void AddMissingProperty(const SValue&amp; name) {
1766          currentError_.PushBack(ValueType(name, GetStateAllocator()).Move(), GetStateAllocator());
1767      }
1768      bool EndMissingProperties() {
1769          if (currentError_.Empty())
1770              return false;
1771          ValueType error(kObjectType);
1772          error.AddMember(GetMissingString(), currentError_, GetStateAllocator());
1773          currentError_ = error;
1774          AddCurrentError(kValidateErrorRequired);
1775          return true;
1776      }
1777      void PropertyViolations(ISchemaValidator** subvalidators, SizeType count) {
1778          for (SizeType i = 0; i &lt; count; ++i)
1779              MergeError(static_cast&lt;GenericSchemaValidator*&gt;(subvalidators[i])-&gt;GetError());
1780      }
1781      void DisallowedProperty(const Ch* name, SizeType length) {
1782          currentError_.SetObject();
1783          currentError_.AddMember(GetDisallowedString(), ValueType(name, length, GetStateAllocator()).Move(), GetStateAllocator());
1784          AddCurrentError(kValidateErrorAdditionalProperties, true);
1785      }
1786      void StartDependencyErrors() {
1787          currentError_.SetObject();
1788      }
1789      void StartMissingDependentProperties() {
1790          missingDependents_.SetArray();
1791      }
1792      void AddMissingDependentProperty(const SValue&amp; targetName) {
1793          missingDependents_.PushBack(ValueType(targetName, GetStateAllocator()).Move(), GetStateAllocator());
1794      }
1795      void EndMissingDependentProperties(const SValue&amp; sourceName) {
1796          if (!missingDependents_.Empty()) {
1797              ValueType error(kObjectType);
1798              ValidateErrorCode code = kValidateErrorRequired;
1799              error.AddMember(GetMissingString(), missingDependents_.Move(), GetStateAllocator());
1800              AddErrorCode(error, code);
1801              AddErrorInstanceLocation(error, false);
1802              PointerType schemaRef = GetInvalidSchemaPointer().Append(SchemaType::GetValidateErrorKeyword(kValidateErrorDependencies), &amp;GetInvalidSchemaPointer().GetAllocator());
1803              AddErrorSchemaLocation(error, schemaRef.Append(sourceName.GetString(), sourceName.GetStringLength(), &amp;GetInvalidSchemaPointer().GetAllocator()));
1804              ValueType wrapper(kObjectType);
1805              wrapper.AddMember(ValueType(SchemaType::GetValidateErrorKeyword(code), GetStateAllocator()).Move(), error, GetStateAllocator());
1806              currentError_.AddMember(ValueType(sourceName, GetStateAllocator()).Move(), wrapper, GetStateAllocator());
1807          }
1808      }
1809      void AddDependencySchemaError(const SValue&amp; sourceName, ISchemaValidator* subvalidator) {
1810          currentError_.AddMember(ValueType(sourceName, GetStateAllocator()).Move(),
1811              static_cast&lt;GenericSchemaValidator*&gt;(subvalidator)-&gt;GetError(), GetStateAllocator());
1812      }
1813      bool EndDependencyErrors() {
1814          if (currentError_.ObjectEmpty())
1815              return false;
1816          ValueType error(kObjectType);
1817          error.AddMember(GetErrorsString(), currentError_, GetStateAllocator());
1818          currentError_ = error;
1819          AddCurrentError(kValidateErrorDependencies);
1820          return true;
1821      }
1822      void DisallowedValue(const ValidateErrorCode code = kValidateErrorEnum) {
1823          currentError_.SetObject();
1824          AddCurrentError(code);
1825      }
1826      void StartDisallowedType() {
1827          currentError_.SetArray();
1828      }
1829      void AddExpectedType(const typename SchemaType::ValueType&amp; expectedType) {
1830          currentError_.PushBack(ValueType(expectedType, GetStateAllocator()).Move(), GetStateAllocator());
1831      }
1832      void EndDisallowedType(const typename SchemaType::ValueType&amp; actualType) {
1833          ValueType error(kObjectType);
1834          error.AddMember(GetExpectedString(), currentError_, GetStateAllocator());
1835          error.AddMember(GetActualString(), ValueType(actualType, GetStateAllocator()).Move(), GetStateAllocator());
1836          currentError_ = error;
1837          AddCurrentError(kValidateErrorType);
1838      }
1839      void NotAllOf(ISchemaValidator** subvalidators, SizeType count) {
1840          AddErrorArray(kValidateErrorAllOf, subvalidators, count);
1841      }
1842      void NoneOf(ISchemaValidator** subvalidators, SizeType count) {
1843          AddErrorArray(kValidateErrorAnyOf, subvalidators, count);
1844      }
1845      void NotOneOf(ISchemaValidator** subvalidators, SizeType count, bool matched = false) {
1846          AddErrorArray(matched ? kValidateErrorOneOfMatch : kValidateErrorOneOf, subvalidators, count);
1847      }
1848      void Disallowed() {
1849          currentError_.SetObject();
1850          AddCurrentError(kValidateErrorNot);
1851      }
1852  #define RAPIDJSON_STRING_(name, ...) \
1853      static const StringRefType&amp; Get##name##String() {\
1854          static const Ch s[] = { __VA_ARGS__, &#x27;\0&#x27; };\
1855          static const StringRefType v(s, static_cast&lt;SizeType&gt;(sizeof(s) / sizeof(Ch) - 1)); \
1856          return v;\
1857      }
1858      RAPIDJSON_STRING_(InstanceRef, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;a&#x27;, &#x27;n&#x27;, &#x27;c&#x27;, &#x27;e&#x27;, &#x27;R&#x27;, &#x27;e&#x27;, &#x27;f&#x27;)
1859      RAPIDJSON_STRING_(SchemaRef, &#x27;s&#x27;, &#x27;c&#x27;, &#x27;h&#x27;, &#x27;e&#x27;, &#x27;m&#x27;, &#x27;a&#x27;, &#x27;R&#x27;, &#x27;e&#x27;, &#x27;f&#x27;)
1860      RAPIDJSON_STRING_(Expected, &#x27;e&#x27;, &#x27;x&#x27;, &#x27;p&#x27;, &#x27;e&#x27;, &#x27;c&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;d&#x27;)
1861      RAPIDJSON_STRING_(Actual, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;t&#x27;, &#x27;u&#x27;, &#x27;a&#x27;, &#x27;l&#x27;)
1862      RAPIDJSON_STRING_(Disallowed, &#x27;d&#x27;, &#x27;i&#x27;, &#x27;s&#x27;, &#x27;a&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;, &#x27;w&#x27;, &#x27;e&#x27;, &#x27;d&#x27;)
1863      RAPIDJSON_STRING_(Missing, &#x27;m&#x27;, &#x27;i&#x27;, &#x27;s&#x27;, &#x27;s&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;g&#x27;)
1864      RAPIDJSON_STRING_(Errors, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;s&#x27;)
1865      RAPIDJSON_STRING_(ErrorCode, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;C&#x27;, &#x27;o&#x27;, &#x27;d&#x27;, &#x27;e&#x27;)
1866      RAPIDJSON_STRING_(ErrorMessage, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;r&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;M&#x27;, &#x27;e&#x27;, &#x27;s&#x27;, &#x27;s&#x27;, &#x27;a&#x27;, &#x27;g&#x27;, &#x27;e&#x27;)
1867      RAPIDJSON_STRING_(Duplicates, &#x27;d&#x27;, &#x27;u&#x27;, &#x27;p&#x27;, &#x27;l&#x27;, &#x27;i&#x27;, &#x27;c&#x27;, &#x27;a&#x27;, &#x27;t&#x27;, &#x27;e&#x27;, &#x27;s&#x27;)
1868  #undef RAPIDJSON_STRING_
1869  #if RAPIDJSON_SCHEMA_VERBOSE
1870  #define RAPIDJSON_SCHEMA_HANDLE_BEGIN_VERBOSE_() \
1871  RAPIDJSON_MULTILINEMACRO_BEGIN\
1872      *documentStack_.template Push&lt;Ch&gt;() = &#x27;\0&#x27;;\
1873      documentStack_.template Pop&lt;Ch&gt;(1);\
1874      internal::PrintInvalidDocument(documentStack_.template Bottom&lt;Ch&gt;());\
1875  RAPIDJSON_MULTILINEMACRO_END
1876  #else
1877  #define RAPIDJSON_SCHEMA_HANDLE_BEGIN_VERBOSE_()
1878  #endif
1879  #define RAPIDJSON_SCHEMA_HANDLE_BEGIN_(method, arg1)\
1880      if (!valid_) return false; \
1881      if ((!BeginValue() &amp;&amp; !GetContinueOnErrors()) || (!CurrentSchema().method arg1 &amp;&amp; !GetContinueOnErrors())) {\
1882          RAPIDJSON_SCHEMA_HANDLE_BEGIN_VERBOSE_();\
1883          return valid_ = false;\
1884      }
1885  #define RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(method, arg2)\
1886      for (Context* context = schemaStack_.template Bottom&lt;Context&gt;(); context != schemaStack_.template End&lt;Context&gt;(); context++) {\
1887          if (context-&gt;hasher)\
1888              static_cast&lt;HasherType*&gt;(context-&gt;hasher)-&gt;method arg2;\
1889          if (context-&gt;validators)\
1890              for (SizeType i_ = 0; i_ &lt; context-&gt;validatorCount; i_++)\
1891                  static_cast&lt;GenericSchemaValidator*&gt;(context-&gt;validators[i_])-&gt;method arg2;\
1892          if (context-&gt;patternPropertiesValidators)\
1893              for (SizeType i_ = 0; i_ &lt; context-&gt;patternPropertiesValidatorCount; i_++)\
1894                  static_cast&lt;GenericSchemaValidator*&gt;(context-&gt;patternPropertiesValidators[i_])-&gt;method arg2;\
1895      }
1896  #define RAPIDJSON_SCHEMA_HANDLE_END_(method, arg2)\
1897      valid_ = (EndValue() || GetContinueOnErrors()) &amp;&amp; (!outputHandler_ || outputHandler_-&gt;method arg2);\
1898      return valid_;
1899  #define RAPIDJSON_SCHEMA_HANDLE_VALUE_(method, arg1, arg2) \
1900      RAPIDJSON_SCHEMA_HANDLE_BEGIN_   (method, arg1);\
1901      RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(method, arg2);\
1902      RAPIDJSON_SCHEMA_HANDLE_END_     (method, arg2)
1903      bool Null()             { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Null,   (CurrentContext()), ( )); }
1904      bool Bool(bool b)       { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Bool,   (CurrentContext(), b), (b)); }
1905      bool Int(int i)         { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Int,    (CurrentContext(), i), (i)); }
1906      bool Uint(unsigned u)   { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Uint,   (CurrentContext(), u), (u)); }
1907      bool Int64(int64_t i)   { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Int64,  (CurrentContext(), i), (i)); }
1908      bool Uint64(uint64_t u) { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Uint64, (CurrentContext(), u), (u)); }
1909      bool Double(double d)   { RAPIDJSON_SCHEMA_HANDLE_VALUE_(Double, (CurrentContext(), d), (d)); }
1910      bool RawNumber(const Ch* str, SizeType length, bool copy)
1911                                      { RAPIDJSON_SCHEMA_HANDLE_VALUE_(String, (CurrentContext(), str, length, copy), (str, length, copy)); }
1912      bool String(const Ch* str, SizeType length, bool copy)
1913                                      { RAPIDJSON_SCHEMA_HANDLE_VALUE_(String, (CurrentContext(), str, length, copy), (str, length, copy)); }
1914      bool StartObject() {
1915          RAPIDJSON_SCHEMA_HANDLE_BEGIN_(StartObject, (CurrentContext()));
1916          RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(StartObject, ());
1917          return valid_ = !outputHandler_ || outputHandler_-&gt;StartObject();
1918      }
1919      bool Key(const Ch* str, SizeType len, bool copy) {
1920          if (!valid_) return false;
1921          AppendToken(str, len);
1922          if (!CurrentSchema().Key(CurrentContext(), str, len, copy) &amp;&amp; !GetContinueOnErrors()) return valid_ = false;
1923          RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(Key, (str, len, copy));
1924          return valid_ = !outputHandler_ || outputHandler_-&gt;Key(str, len, copy);
1925      }
1926      bool EndObject(SizeType memberCount) {
1927          if (!valid_) return false;
1928          RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(EndObject, (memberCount));
1929          if (!CurrentSchema().EndObject(CurrentContext(), memberCount) &amp;&amp; !GetContinueOnErrors()) return valid_ = false;
1930          RAPIDJSON_SCHEMA_HANDLE_END_(EndObject, (memberCount));
1931      }
1932      bool StartArray() {
1933          RAPIDJSON_SCHEMA_HANDLE_BEGIN_(StartArray, (CurrentContext()));
1934          RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(StartArray, ());
1935          return valid_ = !outputHandler_ || outputHandler_-&gt;StartArray();
1936      }
1937      bool EndArray(SizeType elementCount) {
1938          if (!valid_) return false;
1939          RAPIDJSON_SCHEMA_HANDLE_PARALLEL_(EndArray, (elementCount));
1940          if (!CurrentSchema().EndArray(CurrentContext(), elementCount) &amp;&amp; !GetContinueOnErrors()) return valid_ = false;
1941          RAPIDJSON_SCHEMA_HANDLE_END_(EndArray, (elementCount));
1942      }
1943  #undef RAPIDJSON_SCHEMA_HANDLE_BEGIN_VERBOSE_
1944  #undef RAPIDJSON_SCHEMA_HANDLE_BEGIN_
1945  #undef RAPIDJSON_SCHEMA_HANDLE_PARALLEL_
1946  #undef RAPIDJSON_SCHEMA_HANDLE_VALUE_
1947      virtual ISchemaValidator* CreateSchemaValidator(const SchemaType&amp; root, const bool inheritContinueOnErrors) {
1948          ISchemaValidator* sv = new (GetStateAllocator().Malloc(sizeof(GenericSchemaValidator))) GenericSchemaValidator(*schemaDocument_, root, documentStack_.template Bottom&lt;char&gt;(), documentStack_.GetSize(),
1949  #if RAPIDJSON_SCHEMA_VERBOSE
1950          depth_ + 1,
1951  #endif
1952          &amp;GetStateAllocator());
1953          sv-&gt;SetValidateFlags(inheritContinueOnErrors ? GetValidateFlags() : GetValidateFlags() &amp; ~(unsigned)kValidateContinueOnErrorFlag);
1954          return sv;
1955      }
1956      virtual void DestroySchemaValidator(ISchemaValidator* validator) {
1957          GenericSchemaValidator* v = static_cast&lt;GenericSchemaValidator*&gt;(validator);
1958          v-&gt;~GenericSchemaValidator();
1959          StateAllocator::Free(v);
1960      }
1961      virtual void* CreateHasher() {
1962          return new (GetStateAllocator().Malloc(sizeof(HasherType))) HasherType(&amp;GetStateAllocator());
1963      }
1964      virtual uint64_t GetHashCode(void* hasher) {
1965          return static_cast&lt;HasherType*&gt;(hasher)-&gt;GetHashCode();
1966      }
1967      virtual void DestroryHasher(void* hasher) {
1968          HasherType* h = static_cast&lt;HasherType*&gt;(hasher);
1969          h-&gt;~HasherType();
1970          StateAllocator::Free(h);
1971      }
1972      virtual void* MallocState(size_t size) {
1973          return GetStateAllocator().Malloc(size);
1974      }
1975      virtual void FreeState(void* p) {
1976          StateAllocator::Free(p);
1977      }
1978  private:
1979      typedef typename SchemaType::Context Context;
1980      typedef GenericValue&lt;UTF8&lt;&gt;, StateAllocator&gt; HashCodeArray;
1981      typedef internal::Hasher&lt;EncodingType, StateAllocator&gt; HasherType;
1982      GenericSchemaValidator(
1983          const SchemaDocumentType&amp; schemaDocument,
1984          const SchemaType&amp; root,
1985          const char* basePath, size_t basePathSize,
1986  #if RAPIDJSON_SCHEMA_VERBOSE
1987          unsigned depth,
1988  #endif
1989          StateAllocator* allocator = 0,
1990          size_t schemaStackCapacity = kDefaultSchemaStackCapacity,
1991          size_t documentStackCapacity = kDefaultDocumentStackCapacity)
1992          :
1993          schemaDocument_(&amp;schemaDocument),
1994          root_(root),
1995          stateAllocator_(allocator),
1996          ownStateAllocator_(0),
1997          schemaStack_(allocator, schemaStackCapacity),
1998          documentStack_(allocator, documentStackCapacity),
1999          outputHandler_(0),
2000          error_(kObjectType),
2001          currentError_(),
2002          missingDependents_(),
2003          valid_(true),
2004          flags_(kValidateDefaultFlags)
2005  #if RAPIDJSON_SCHEMA_VERBOSE
2006          , depth_(depth)
2007  #endif
2008      {
2009          if (basePath &amp;&amp; basePathSize)
2010              memcpy(documentStack_.template Push&lt;char&gt;(basePathSize), basePath, basePathSize);
2011      }
2012      StateAllocator&amp; GetStateAllocator() {
2013          if (!stateAllocator_)
2014              stateAllocator_ = ownStateAllocator_ = RAPIDJSON_NEW(StateAllocator)();
2015          return *stateAllocator_;
2016      }
2017      bool GetContinueOnErrors() const {
2018          return flags_ &amp; kValidateContinueOnErrorFlag;
2019      }
2020      bool BeginValue() {
2021          if (schemaStack_.Empty())
2022              PushSchema(root_);
2023          else {
2024              if (CurrentContext().inArray)
2025                  internal::TokenHelper&lt;internal::Stack&lt;StateAllocator&gt;, Ch&gt;::AppendIndexToken(documentStack_, CurrentContext().arrayElementIndex);
2026              if (!CurrentSchema().BeginValue(CurrentContext()) &amp;&amp; !GetContinueOnErrors())
2027                  return false;
2028              SizeType count = CurrentContext().patternPropertiesSchemaCount;
2029              const SchemaType** sa = CurrentContext().patternPropertiesSchemas;
2030              typename Context::PatternValidatorType patternValidatorType = CurrentContext().valuePatternValidatorType;
2031              bool valueUniqueness = CurrentContext().valueUniqueness;
2032              RAPIDJSON_ASSERT(CurrentContext().valueSchema);
2033              PushSchema(*CurrentContext().valueSchema);
2034              if (count &gt; 0) {
2035                  CurrentContext().objectPatternValidatorType = patternValidatorType;
2036                  ISchemaValidator**&amp; va = CurrentContext().patternPropertiesValidators;
2037                  SizeType&amp; validatorCount = CurrentContext().patternPropertiesValidatorCount;
2038                  va = static_cast&lt;ISchemaValidator**&gt;(MallocState(sizeof(ISchemaValidator*) * count));
2039                  for (SizeType i = 0; i &lt; count; i++)
2040                      va[validatorCount++] = CreateSchemaValidator(*sa[i], true);  
2041              }
2042              CurrentContext().arrayUniqueness = valueUniqueness;
2043          }
2044          return true;
2045      }
2046      bool EndValue() {
2047          if (!CurrentSchema().EndValue(CurrentContext()) &amp;&amp; !GetContinueOnErrors())
2048              return false;
2049  #if RAPIDJSON_SCHEMA_VERBOSE
2050          GenericStringBuffer&lt;EncodingType&gt; sb;
2051          schemaDocument_-&gt;GetPointer(&amp;CurrentSchema()).Stringify(sb);
2052          *documentStack_.template Push&lt;Ch&gt;() = &#x27;\0&#x27;;
2053          documentStack_.template Pop&lt;Ch&gt;(1);
2054          internal::PrintValidatorPointers(depth_, sb.GetString(), documentStack_.template Bottom&lt;Ch&gt;());
2055  #endif
2056          void* hasher = CurrentContext().hasher;
2057          uint64_t h = hasher &amp;&amp; CurrentContext().arrayUniqueness ? static_cast&lt;HasherType*&gt;(hasher)-&gt;GetHashCode() : 0;
2058          PopSchema();
2059          if (!schemaStack_.Empty()) {
2060              Context&amp; context = CurrentContext();
2061              if (hasher &amp;&amp; context.valueUniqueness) {
2062                  HashCodeArray* a = static_cast&lt;HashCodeArray*&gt;(context.arrayElementHashCodes);
2063                  if (!a)
2064                      CurrentContext().arrayElementHashCodes = a = new (GetStateAllocator().Malloc(sizeof(HashCodeArray))) HashCodeArray(kArrayType);
2065                  for (typename HashCodeArray::ConstValueIterator itr = a-&gt;Begin(); itr != a-&gt;End(); ++itr)
2066                      if (itr-&gt;GetUint64() == h) {
2067                          DuplicateItems(static_cast&lt;SizeType&gt;(itr - a-&gt;Begin()), a-&gt;Size());
2068                          if (GetContinueOnErrors()) {
2069                              a-&gt;PushBack(h, GetStateAllocator());
2070                              while (!documentStack_.Empty() &amp;&amp; *documentStack_.template Pop&lt;Ch&gt;(1) != &#x27;/&#x27;);
2071                          }
2072                          RAPIDJSON_INVALID_KEYWORD_RETURN(kValidateErrorUniqueItems);
2073                      }
2074                  a-&gt;PushBack(h, GetStateAllocator());
2075              }
2076          }
2077          while (!documentStack_.Empty() &amp;&amp; *documentStack_.template Pop&lt;Ch&gt;(1) != &#x27;/&#x27;)
2078              ;
2079          return true;
2080      }
2081      void AppendToken(const Ch* str, SizeType len) {
2082          documentStack_.template Reserve&lt;Ch&gt;(1 + len * 2); 
2083          *documentStack_.template PushUnsafe&lt;Ch&gt;() = &#x27;/&#x27;;
2084          for (SizeType i = 0; i &lt; len; i++) {
2085              if (str[i] == &#x27;~&#x27;) {
2086                  *documentStack_.template PushUnsafe&lt;Ch&gt;() = &#x27;~&#x27;;
2087                  *documentStack_.template PushUnsafe&lt;Ch&gt;() = &#x27;0&#x27;;
2088              }
2089              else if (str[i] == &#x27;/&#x27;) {
2090                  *documentStack_.template PushUnsafe&lt;Ch&gt;() = &#x27;~&#x27;;
2091                  *documentStack_.template PushUnsafe&lt;Ch&gt;() = &#x27;1&#x27;;
2092              }
2093              else
2094                  *documentStack_.template PushUnsafe&lt;Ch&gt;() = str[i];
2095          }
2096      }
2097      RAPIDJSON_FORCEINLINE void PushSchema(const SchemaType&amp; schema) { new (schemaStack_.template Push&lt;Context&gt;()) Context(*this, *this, &amp;schema); }
2098      RAPIDJSON_FORCEINLINE void PopSchema() {
2099          Context* c = schemaStack_.template Pop&lt;Context&gt;(1);
2100          if (HashCodeArray* a = static_cast&lt;HashCodeArray*&gt;(c-&gt;arrayElementHashCodes)) {
2101              a-&gt;~HashCodeArray();
2102              StateAllocator::Free(a);
2103          }
2104          c-&gt;~Context();
2105      }
2106      void AddErrorInstanceLocation(ValueType&amp; result, bool parent) {
2107          GenericStringBuffer&lt;EncodingType&gt; sb;
2108          PointerType instancePointer = GetInvalidDocumentPointer();
2109          ((parent &amp;&amp; instancePointer.GetTokenCount() &gt; 0)
2110           ? PointerType(instancePointer.GetTokens(), instancePointer.GetTokenCount() - 1)
2111           : instancePointer).StringifyUriFragment(sb);
2112          ValueType instanceRef(sb.GetString(), static_cast&lt;SizeType&gt;(sb.GetSize() / sizeof(Ch)),
2113                                GetStateAllocator());
2114          result.AddMember(GetInstanceRefString(), instanceRef, GetStateAllocator());
2115      }
2116      void AddErrorSchemaLocation(ValueType&amp; result, PointerType schema = PointerType()) {
2117          GenericStringBuffer&lt;EncodingType&gt; sb;
2118          SizeType len = CurrentSchema().GetURI().GetStringLength();
2119          if (len) memcpy(sb.Push(len), CurrentSchema().GetURI().GetString(), len * sizeof(Ch));
2120          if (schema.GetTokenCount()) schema.StringifyUriFragment(sb);
2121          else GetInvalidSchemaPointer().StringifyUriFragment(sb);
2122          ValueType schemaRef(sb.GetString(), static_cast&lt;SizeType&gt;(sb.GetSize() / sizeof(Ch)),
2123              GetStateAllocator());
2124          result.AddMember(GetSchemaRefString(), schemaRef, GetStateAllocator());
2125      }
2126      void AddErrorCode(ValueType&amp; result, const ValidateErrorCode code) {
2127          result.AddMember(GetErrorCodeString(), code, GetStateAllocator());
2128      }
2129      void AddError(ValueType&amp; keyword, ValueType&amp; error) {
2130          typename ValueType::MemberIterator member = error_.FindMember(keyword);
2131          if (member == error_.MemberEnd())
2132              error_.AddMember(keyword, error, GetStateAllocator());
2133          else {
2134              if (member-&gt;value.IsObject()) {
2135                  ValueType errors(kArrayType);
2136                  errors.PushBack(member-&gt;value, GetStateAllocator());
2137                  member-&gt;value = errors;
2138              }
2139              member-&gt;value.PushBack(error, GetStateAllocator());
2140          }
2141      }
2142      void AddCurrentError(const ValidateErrorCode code, bool parent = false) {
2143          AddErrorCode(currentError_, code);
2144          AddErrorInstanceLocation(currentError_, parent);
2145          AddErrorSchemaLocation(currentError_);
2146          AddError(ValueType(SchemaType::GetValidateErrorKeyword(code), GetStateAllocator(), false).Move(), currentError_);
2147      }
2148      void MergeError(ValueType&amp; other) {
2149          for (typename ValueType::MemberIterator it = other.MemberBegin(), end = other.MemberEnd(); it != end; ++it) {
2150              AddError(it-&gt;name, it-&gt;value);
2151          }
2152      }
2153      void AddNumberError(const ValidateErrorCode code, ValueType&amp; actual, const SValue&amp; expected,
2154          const typename SchemaType::ValueType&amp; (*exclusive)() = 0) {
2155          currentError_.SetObject();
2156          currentError_.AddMember(GetActualString(), actual, GetStateAllocator());
2157          currentError_.AddMember(GetExpectedString(), ValueType(expected, GetStateAllocator()).Move(), GetStateAllocator());
2158          if (exclusive)
2159              currentError_.AddMember(ValueType(exclusive(), GetStateAllocator()).Move(), true, GetStateAllocator());
2160          AddCurrentError(code);
2161      }
2162      void AddErrorArray(const ValidateErrorCode code,
2163          ISchemaValidator** subvalidators, SizeType count) {
2164          ValueType errors(kArrayType);
2165          for (SizeType i = 0; i &lt; count; ++i)
2166              errors.PushBack(static_cast&lt;GenericSchemaValidator*&gt;(subvalidators[i])-&gt;GetError(), GetStateAllocator());
2167          currentError_.SetObject();
2168          currentError_.AddMember(GetErrorsString(), errors, GetStateAllocator());
2169          AddCurrentError(code);
2170      }
2171      const SchemaType&amp; CurrentSchema() const { return *schemaStack_.template Top&lt;Context&gt;()-&gt;schema; }
2172      Context&amp; CurrentContext() { return *schemaStack_.template Top&lt;Context&gt;(); }
2173      const Context&amp; CurrentContext() const { return *schemaStack_.template Top&lt;Context&gt;(); }
2174      static const size_t kDefaultSchemaStackCapacity = 1024;
2175      static const size_t kDefaultDocumentStackCapacity = 256;
2176      const SchemaDocumentType* schemaDocument_;
2177      const SchemaType&amp; root_;
2178      StateAllocator* stateAllocator_;
2179      StateAllocator* ownStateAllocator_;
2180      internal::Stack&lt;StateAllocator&gt; schemaStack_;    
2181      internal::Stack&lt;StateAllocator&gt; documentStack_;  
2182      OutputHandler* outputHandler_;
<span onclick='openModal()' class='match'>2183      ValueType error_;
2184      ValueType currentError_;
2185      ValueType missingDependents_;
2186      bool valid_;
2187      unsigned flags_;
2188  #if RAPIDJSON_SCHEMA_VERBOSE
</span>2189      unsigned depth_;
2190  #endif
2191  };
2192  typedef GenericSchemaValidator&lt;SchemaDocument&gt; SchemaValidator;
2193  template &lt;
2194      unsigned parseFlags,
2195      typename InputStream,
2196      typename SourceEncoding,
2197      typename SchemaDocumentType = SchemaDocument,
2198      typename StackAllocator = CrtAllocator&gt;
2199  class SchemaValidatingReader {
2200  public:
2201      typedef typename SchemaDocumentType::PointerType PointerType;
2202      typedef typename InputStream::Ch Ch;
2203      typedef GenericValue&lt;SourceEncoding, StackAllocator&gt; ValueType;
2204      SchemaValidatingReader(InputStream&amp; is, const SchemaDocumentType&amp; sd) : is_(is), sd_(sd), invalidSchemaKeyword_(), invalidSchemaCode_(kValidateErrorNone), error_(kObjectType), isValid_(true) {}
2205      template &lt;typename Handler&gt;
2206      bool operator()(Handler&amp; handler) {
2207          GenericReader&lt;SourceEncoding, typename SchemaDocumentType::EncodingType, StackAllocator&gt; reader;
2208          GenericSchemaValidator&lt;SchemaDocumentType, Handler&gt; validator(sd_, handler);
2209          parseResult_ = reader.template Parse&lt;parseFlags&gt;(is_, validator);
2210          isValid_ = validator.IsValid();
2211          if (isValid_) {
2212              invalidSchemaPointer_ = PointerType();
2213              invalidSchemaKeyword_ = 0;
2214              invalidDocumentPointer_ = PointerType();
2215              error_.SetObject();
2216          }
2217          else {
2218              invalidSchemaPointer_ = validator.GetInvalidSchemaPointer();
2219              invalidSchemaKeyword_ = validator.GetInvalidSchemaKeyword();
2220              invalidSchemaCode_ = validator.GetInvalidSchemaCode();
2221              invalidDocumentPointer_ = validator.GetInvalidDocumentPointer();
2222              error_.CopyFrom(validator.GetError(), allocator_);
2223          }
2224          return parseResult_;
2225      }
2226      const ParseResult&amp; GetParseResult() const { return parseResult_; }
2227      bool IsValid() const { return isValid_; }
2228      const PointerType&amp; GetInvalidSchemaPointer() const { return invalidSchemaPointer_; }
2229      const Ch* GetInvalidSchemaKeyword() const { return invalidSchemaKeyword_; }
2230      const PointerType&amp; GetInvalidDocumentPointer() const { return invalidDocumentPointer_; }
2231      const ValueType&amp; GetError() const { return error_; }
2232      ValidateErrorCode GetInvalidSchemaCode() const { return invalidSchemaCode_; }
2233  private:
2234      InputStream&amp; is_;
2235      const SchemaDocumentType&amp; sd_;
2236      ParseResult parseResult_;
2237      PointerType invalidSchemaPointer_;
2238      const Ch* invalidSchemaKeyword_;
2239      PointerType invalidDocumentPointer_;
2240      ValidateErrorCode invalidSchemaCode_;
2241      StackAllocator allocator_;
2242      ValueType error_;
2243      bool isValid_;
2244  };
2245  RAPIDJSON_NAMESPACE_END
2246  RAPIDJSON_DIAG_POP
2247  #endif 
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-jemalloc.c</h3>
            <pre><code>1  #define JEMALLOC_C_
2  #include &quot;jemalloc/internal/jemalloc_preamble.h&quot;
3  #include &quot;jemalloc/internal/jemalloc_internal_includes.h&quot;
4  #include &quot;jemalloc/internal/assert.h&quot;
5  #include &quot;jemalloc/internal/atomic.h&quot;
6  #include &quot;jemalloc/internal/ctl.h&quot;
7  #include &quot;jemalloc/internal/extent_dss.h&quot;
8  #include &quot;jemalloc/internal/extent_mmap.h&quot;
9  #include &quot;jemalloc/internal/hook.h&quot;
10  #include &quot;jemalloc/internal/jemalloc_internal_types.h&quot;
11  #include &quot;jemalloc/internal/log.h&quot;
12  #include &quot;jemalloc/internal/malloc_io.h&quot;
13  #include &quot;jemalloc/internal/mutex.h&quot;
14  #include &quot;jemalloc/internal/rtree.h&quot;
15  #include &quot;jemalloc/internal/safety_check.h&quot;
16  #include &quot;jemalloc/internal/sc.h&quot;
17  #include &quot;jemalloc/internal/spin.h&quot;
18  #include &quot;jemalloc/internal/sz.h&quot;
19  #include &quot;jemalloc/internal/ticker.h&quot;
20  #include &quot;jemalloc/internal/util.h&quot;
21  const char	*je_malloc_conf
22  #ifndef _WIN32
23      JEMALLOC_ATTR(weak)
24  #endif
25      ;
26  bool	opt_abort =
27  #ifdef JEMALLOC_DEBUG
28      true
29  #else
30      false
31  #endif
32      ;
33  bool	opt_abort_conf =
34  #ifdef JEMALLOC_DEBUG
35      true
36  #else
37      false
38  #endif
39      ;
40  bool	opt_confirm_conf = false;
41  const char	*opt_junk =
42  #if (defined(JEMALLOC_DEBUG) &amp;&amp; defined(JEMALLOC_FILL))
43      &quot;true&quot;
44  #else
45      &quot;false&quot;
46  #endif
47      ;
48  bool	opt_junk_alloc =
49  #if (defined(JEMALLOC_DEBUG) &amp;&amp; defined(JEMALLOC_FILL))
50      true
51  #else
52      false
53  #endif
54      ;
55  bool	opt_junk_free =
56  #if (defined(JEMALLOC_DEBUG) &amp;&amp; defined(JEMALLOC_FILL))
57      true
58  #else
59      false
60  #endif
61      ;
62  bool	opt_utrace = false;
63  bool	opt_xmalloc = false;
64  bool	opt_zero = false;
65  unsigned	opt_narenas = 0;
66  unsigned	ncpus;
67  malloc_mutex_t arenas_lock;
68  JEMALLOC_ALIGNED(CACHELINE)
69  atomic_p_t		arenas[MALLOCX_ARENA_LIMIT];
70  static atomic_u_t	narenas_total; &amp;bsol;* Use narenas_total_*(). */
71  static arena_t		*a0; &amp;bsol;* arenas[0]. */
72  unsigned		narenas_auto;
73  unsigned		manual_arena_base;
74  typedef enum {
75  	malloc_init_uninitialized	= 3,
76  	malloc_init_a0_initialized	= 2,
77  	malloc_init_recursible		= 1,
78  	malloc_init_initialized		= 0 &amp;bsol;* Common case --&gt; jnz. */
79  } malloc_init_t;
80  static malloc_init_t	malloc_init_state = malloc_init_uninitialized;
81  bool			malloc_slow = true;
82  enum {
83  	flag_opt_junk_alloc	= (1U),
84  	flag_opt_junk_free	= (1U &lt;&lt; 1),
85  	flag_opt_zero		= (1U &lt;&lt; 2),
86  	flag_opt_utrace		= (1U &lt;&lt; 3),
87  	flag_opt_xmalloc	= (1U &lt;&lt; 4)
88  };
89  static uint8_t	malloc_slow_flags;
90  #ifdef JEMALLOC_THREADED_INIT
91  #  define NO_INITIALIZER	((unsigned long)0)
92  #  define INITIALIZER		pthread_self()
93  #  define IS_INITIALIZER	(malloc_initializer == pthread_self())
94  static pthread_t		malloc_initializer = NO_INITIALIZER;
95  #else
96  #  define NO_INITIALIZER	false
97  #  define INITIALIZER		true
98  #  define IS_INITIALIZER	malloc_initializer
99  static bool			malloc_initializer = NO_INITIALIZER;
100  #endif
101  #ifdef _WIN32
102  #if _WIN32_WINNT &gt;= 0x0600
103  static malloc_mutex_t	init_lock = SRWLOCK_INIT;
104  #else
105  static malloc_mutex_t	init_lock;
106  static bool init_lock_initialized = false;
107  JEMALLOC_ATTR(constructor)
108  static void WINAPI
109  _init_init_lock(void) {
110  	if (!init_lock_initialized) {
111  		malloc_mutex_init(&amp;init_lock, &quot;init&quot;, WITNESS_RANK_INIT,
112  		    malloc_mutex_rank_exclusive);
113  	}
114  	init_lock_initialized = true;
115  }
116  #ifdef _MSC_VER
117  #  pragma section(&quot;.CRT$XCU&quot;, read)
118  JEMALLOC_SECTION(&quot;.CRT$XCU&quot;) JEMALLOC_ATTR(used)
119  static const void (WINAPI *init_init_lock)(void) = _init_init_lock;
120  #endif
121  #endif
122  #else
123  static malloc_mutex_t	init_lock = MALLOC_MUTEX_INITIALIZER;
124  #endif
125  typedef struct {
126  	void	*p;	&amp;bsol;* Input pointer (as in realloc(p, s)). */
127  	size_t	s;	&amp;bsol;* Request size. */
128  	void	*r;	&amp;bsol;* Result pointer. */
129  } malloc_utrace_t;
130  #ifdef JEMALLOC_UTRACE
131  #  define UTRACE(a, b, c) do {						\
132  	if (unlikely(opt_utrace)) {					\
133  		int utrace_serrno = errno;				\
134  		malloc_utrace_t ut;					\
135  		ut.p = (a);						\
136  		ut.s = (b);						\
137  		ut.r = (c);						\
138  		utrace(&amp;ut, sizeof(ut));				\
139  		errno = utrace_serrno;					\
140  	}								\
141  } while (0)
142  #else
143  #  define UTRACE(a, b, c)
144  #endif
145  static bool had_conf_error = false;
146  static bool	malloc_init_hard_a0(void);
147  static bool	malloc_init_hard(void);
148  bool
149  malloc_initialized(void) {
150  	return (malloc_init_state == malloc_init_initialized);
151  }
152  JEMALLOC_ALWAYS_INLINE bool
153  malloc_init_a0(void) {
154  	if (unlikely(malloc_init_state == malloc_init_uninitialized)) {
155  		return malloc_init_hard_a0();
156  	}
157  	return false;
158  }
159  JEMALLOC_ALWAYS_INLINE bool
160  malloc_init(void) {
161  	if (unlikely(!malloc_initialized()) &amp;&amp; malloc_init_hard()) {
162  		return true;
163  	}
164  	return false;
165  }
166  static void *
167  a0ialloc(size_t size, bool zero, bool is_internal) {
168  	if (unlikely(malloc_init_a0())) {
169  		return NULL;
170  	}
171  	return iallocztm(TSDN_NULL, size, sz_size2index(size), zero, NULL,
172  	    is_internal, arena_get(TSDN_NULL, 0, true), true);
173  }
174  static void
175  a0idalloc(void *ptr, bool is_internal) {
176  	idalloctm(TSDN_NULL, ptr, NULL, NULL, is_internal, true);
177  }
178  void *
179  a0malloc(size_t size) {
180  	return a0ialloc(size, false, true);
181  }
182  void
183  a0dalloc(void *ptr) {
184  	a0idalloc(ptr, true);
185  }
186  void *
187  bootstrap_malloc(size_t size) {
188  	if (unlikely(size == 0)) {
189  		size = 1;
190  	}
191  	return a0ialloc(size, false, false);
192  }
193  void *
194  bootstrap_calloc(size_t num, size_t size) {
195  	size_t num_size;
196  	num_size = num * size;
197  	if (unlikely(num_size == 0)) {
198  		assert(num == 0 || size == 0);
199  		num_size = 1;
200  	}
201  	return a0ialloc(num_size, true, false);
202  }
203  void
204  bootstrap_free(void *ptr) {
205  	if (unlikely(ptr == NULL)) {
206  		return;
207  	}
208  	a0idalloc(ptr, false);
209  }
210  void
211  arena_set(unsigned ind, arena_t *arena) {
212  	atomic_store_p(&amp;arenas[ind], arena, ATOMIC_RELEASE);
213  }
214  static void
215  narenas_total_set(unsigned narenas) {
216  	atomic_store_u(&amp;narenas_total, narenas, ATOMIC_RELEASE);
217  }
218  static void
219  narenas_total_inc(void) {
220  	atomic_fetch_add_u(&amp;narenas_total, 1, ATOMIC_RELEASE);
221  }
222  unsigned
223  narenas_total_get(void) {
224  	return atomic_load_u(&amp;narenas_total, ATOMIC_ACQUIRE);
225  }
226  static arena_t *
227  arena_init_locked(tsdn_t *tsdn, unsigned ind, extent_hooks_t *extent_hooks) {
228  	arena_t *arena;
229  	assert(ind &lt;= narenas_total_get());
230  	if (ind &gt;= MALLOCX_ARENA_LIMIT) {
231  		return NULL;
232  	}
233  	if (ind == narenas_total_get()) {
234  		narenas_total_inc();
235  	}
236  	arena = arena_get(tsdn, ind, false);
237  	if (arena != NULL) {
238  		assert(arena_is_auto(arena));
239  		return arena;
240  	}
241  	arena = arena_new(tsdn, ind, extent_hooks);
242  	return arena;
243  }
244  static void
245  arena_new_create_background_thread(tsdn_t *tsdn, unsigned ind) {
246  	if (ind == 0) {
247  		return;
248  	}
249  	if (have_background_thread &amp;&amp; !arena_is_huge(ind)) {
250  		if (background_thread_create(tsdn_tsd(tsdn), ind)) {
251  			malloc_printf(&quot;&lt;jemalloc&gt;: error in background thread &quot;
252  				      &quot;creation for arena %u. Abort.\n&quot;, ind);
253  			abort();
254  		}
255  	}
256  }
257  arena_t *
258  arena_init(tsdn_t *tsdn, unsigned ind, extent_hooks_t *extent_hooks) {
259  	arena_t *arena;
260  	malloc_mutex_lock(tsdn, &amp;arenas_lock);
261  	arena = arena_init_locked(tsdn, ind, extent_hooks);
262  	malloc_mutex_unlock(tsdn, &amp;arenas_lock);
263  	arena_new_create_background_thread(tsdn, ind);
264  	return arena;
265  }
266  static void
267  arena_bind(tsd_t *tsd, unsigned ind, bool internal) {
268  	arena_t *arena = arena_get(tsd_tsdn(tsd), ind, false);
269  	arena_nthreads_inc(arena, internal);
270  	if (internal) {
271  		tsd_iarena_set(tsd, arena);
272  	} else {
273  		tsd_arena_set(tsd, arena);
274  		unsigned shard = atomic_fetch_add_u(&amp;arena-&gt;binshard_next, 1,
275  		    ATOMIC_RELAXED);
276  		tsd_binshards_t *bins = tsd_binshardsp_get(tsd);
277  		for (unsigned i = 0; i &lt; SC_NBINS; i++) {
278  			assert(bin_infos[i].n_shards &gt; 0 &amp;&amp;
279  			    bin_infos[i].n_shards &lt;= BIN_SHARDS_MAX);
280  			bins-&gt;binshard[i] = shard % bin_infos[i].n_shards;
281  		}
282  	}
283  }
284  void
285  arena_migrate(tsd_t *tsd, unsigned oldind, unsigned newind) {
286  	arena_t *oldarena, *newarena;
287  	oldarena = arena_get(tsd_tsdn(tsd), oldind, false);
288  	newarena = arena_get(tsd_tsdn(tsd), newind, false);
289  	arena_nthreads_dec(oldarena, false);
290  	arena_nthreads_inc(newarena, false);
291  	tsd_arena_set(tsd, newarena);
292  }
293  static void
294  arena_unbind(tsd_t *tsd, unsigned ind, bool internal) {
295  	arena_t *arena;
296  	arena = arena_get(tsd_tsdn(tsd), ind, false);
297  	arena_nthreads_dec(arena, internal);
298  	if (internal) {
299  		tsd_iarena_set(tsd, NULL);
300  	} else {
301  		tsd_arena_set(tsd, NULL);
302  	}
303  }
304  arena_tdata_t *
305  arena_tdata_get_hard(tsd_t *tsd, unsigned ind) {
306  	arena_tdata_t *tdata, *arenas_tdata_old;
307  	arena_tdata_t *arenas_tdata = tsd_arenas_tdata_get(tsd);
308  	unsigned narenas_tdata_old, i;
309  	unsigned narenas_tdata = tsd_narenas_tdata_get(tsd);
310  	unsigned narenas_actual = narenas_total_get();
311  	if (arenas_tdata != NULL &amp;&amp; narenas_tdata &lt; narenas_actual) {
312  		arenas_tdata_old = arenas_tdata;
313  		narenas_tdata_old = narenas_tdata;
314  		arenas_tdata = NULL;
315  		narenas_tdata = 0;
316  		tsd_arenas_tdata_set(tsd, arenas_tdata);
317  		tsd_narenas_tdata_set(tsd, narenas_tdata);
318  	} else {
319  		arenas_tdata_old = NULL;
320  		narenas_tdata_old = 0;
321  	}
322  	if (arenas_tdata == NULL) {
323  		bool *arenas_tdata_bypassp = tsd_arenas_tdata_bypassp_get(tsd);
324  		narenas_tdata = (ind &lt; narenas_actual) ? narenas_actual : ind+1;
325  		if (tsd_nominal(tsd) &amp;&amp; !*arenas_tdata_bypassp) {
326  			*arenas_tdata_bypassp = true;
327  			arenas_tdata = (arena_tdata_t *)a0malloc(
328  			    sizeof(arena_tdata_t) * narenas_tdata);
329  			*arenas_tdata_bypassp = false;
330  		}
331  		if (arenas_tdata == NULL) {
332  			tdata = NULL;
333  			goto label_return;
334  		}
335  		assert(tsd_nominal(tsd) &amp;&amp; !*arenas_tdata_bypassp);
336  		tsd_arenas_tdata_set(tsd, arenas_tdata);
337  		tsd_narenas_tdata_set(tsd, narenas_tdata);
338  	}
339  	for (i = 0; i &lt; narenas_actual; i++) {
340  		if (i &lt; narenas_tdata_old) {
341  			ticker_copy(&amp;arenas_tdata[i].decay_ticker,
342  			    &amp;arenas_tdata_old[i].decay_ticker);
343  		} else {
344  			ticker_init(&amp;arenas_tdata[i].decay_ticker,
345  			    DECAY_NTICKS_PER_UPDATE);
346  		}
347  	}
348  	if (narenas_tdata &gt; narenas_actual) {
349  		memset(&amp;arenas_tdata[narenas_actual], 0, sizeof(arena_tdata_t)
350  		    * (narenas_tdata - narenas_actual));
351  	}
352  	tdata = &amp;arenas_tdata[ind];
353  label_return:
354  	if (arenas_tdata_old != NULL) {
355  		a0dalloc(arenas_tdata_old);
356  	}
357  	return tdata;
358  }
359  arena_t *
360  arena_choose_hard(tsd_t *tsd, bool internal) {
361  	arena_t *ret JEMALLOC_CC_SILENCE_INIT(NULL);
362  	if (have_percpu_arena &amp;&amp; PERCPU_ARENA_ENABLED(opt_percpu_arena)) {
363  		unsigned choose = percpu_arena_choose();
364  		ret = arena_get(tsd_tsdn(tsd), choose, true);
365  		assert(ret != NULL);
366  		arena_bind(tsd, arena_ind_get(ret), false);
367  		arena_bind(tsd, arena_ind_get(ret), true);
368  		return ret;
369  	}
370  	if (narenas_auto &gt; 1) {
371  		unsigned i, j, choose[2], first_null;
372  		bool is_new_arena[2];
373  		for (j = 0; j &lt; 2; j++) {
374  			choose[j] = 0;
375  			is_new_arena[j] = false;
376  		}
377  		first_null = narenas_auto;
378  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;arenas_lock);
379  		assert(arena_get(tsd_tsdn(tsd), 0, false) != NULL);
380  		for (i = 1; i &lt; narenas_auto; i++) {
381  			if (arena_get(tsd_tsdn(tsd), i, false) != NULL) {
382  				for (j = 0; j &lt; 2; j++) {
383  					if (arena_nthreads_get(arena_get(
384  					    tsd_tsdn(tsd), i, false), !!j) &lt;
385  					    arena_nthreads_get(arena_get(
386  					    tsd_tsdn(tsd), choose[j], false),
387  					    !!j)) {
388  						choose[j] = i;
389  					}
390  				}
391  			} else if (first_null == narenas_auto) {
392  				first_null = i;
393  			}
394  		}
395  		for (j = 0; j &lt; 2; j++) {
396  			if (arena_nthreads_get(arena_get(tsd_tsdn(tsd),
397  			    choose[j], false), !!j) == 0 || first_null ==
398  			    narenas_auto) {
399  				if (!!j == internal) {
400  					ret = arena_get(tsd_tsdn(tsd),
401  					    choose[j], false);
402  				}
403  			} else {
404  				arena_t *arena;
405  				choose[j] = first_null;
406  				arena = arena_init_locked(tsd_tsdn(tsd),
407  				    choose[j],
408  				    (extent_hooks_t *)&amp;extent_hooks_default);
409  				if (arena == NULL) {
410  					malloc_mutex_unlock(tsd_tsdn(tsd),
411  					    &amp;arenas_lock);
412  					return NULL;
413  				}
414  				is_new_arena[j] = true;
415  				if (!!j == internal) {
416  					ret = arena;
417  				}
418  			}
419  			arena_bind(tsd, choose[j], !!j);
420  		}
421  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;arenas_lock);
422  		for (j = 0; j &lt; 2; j++) {
423  			if (is_new_arena[j]) {
424  				assert(choose[j] &gt; 0);
425  				arena_new_create_background_thread(
426  				    tsd_tsdn(tsd), choose[j]);
427  			}
428  		}
429  	} else {
430  		ret = arena_get(tsd_tsdn(tsd), 0, false);
431  		arena_bind(tsd, 0, false);
432  		arena_bind(tsd, 0, true);
433  	}
434  	return ret;
435  }
436  void
437  iarena_cleanup(tsd_t *tsd) {
438  	arena_t *iarena;
439  	iarena = tsd_iarena_get(tsd);
440  	if (iarena != NULL) {
441  		arena_unbind(tsd, arena_ind_get(iarena), true);
442  	}
443  }
444  void
445  arena_cleanup(tsd_t *tsd) {
446  	arena_t *arena;
447  	arena = tsd_arena_get(tsd);
448  	if (arena != NULL) {
449  		arena_unbind(tsd, arena_ind_get(arena), false);
450  	}
451  }
452  void
453  arenas_tdata_cleanup(tsd_t *tsd) {
454  	arena_tdata_t *arenas_tdata;
455  	*tsd_arenas_tdata_bypassp_get(tsd) = true;
456  	arenas_tdata = tsd_arenas_tdata_get(tsd);
457  	if (arenas_tdata != NULL) {
458  		tsd_arenas_tdata_set(tsd, NULL);
459  		a0dalloc(arenas_tdata);
460  	}
461  }
462  static void
463  stats_print_atexit(void) {
464  	if (config_stats) {
465  		tsdn_t *tsdn;
466  		unsigned narenas, i;
467  		tsdn = tsdn_fetch();
468  		for (i = 0, narenas = narenas_total_get(); i &lt; narenas; i++) {
469  			arena_t *arena = arena_get(tsdn, i, false);
470  			if (arena != NULL) {
471  				tcache_t *tcache;
472  				malloc_mutex_lock(tsdn, &amp;arena-&gt;tcache_ql_mtx);
473  				ql_foreach(tcache, &amp;arena-&gt;tcache_ql, link) {
474  					tcache_stats_merge(tsdn, tcache, arena);
475  				}
476  				malloc_mutex_unlock(tsdn,
477  				    &amp;arena-&gt;tcache_ql_mtx);
478  			}
479  		}
480  	}
481  	je_malloc_stats_print(NULL, NULL, opt_stats_print_opts);
482  }
483  JEMALLOC_ALWAYS_INLINE void
484  check_entry_exit_locking(tsdn_t *tsdn) {
485  	if (!config_debug) {
486  		return;
487  	}
488  	if (tsdn_null(tsdn)) {
489  		return;
490  	}
491  	tsd_t *tsd = tsdn_tsd(tsdn);
492  	int8_t reentrancy_level = tsd_reentrancy_level_get(tsd);
493  	if (reentrancy_level != 0) {
494  		return;
495  	}
496  	witness_assert_lockless(tsdn_witness_tsdp_get(tsdn));
497  }
498  static char *
499  jemalloc_secure_getenv(const char *name) {
500  #ifdef JEMALLOC_HAVE_SECURE_GETENV
501  	return secure_getenv(name);
502  #else
503  #  ifdef JEMALLOC_HAVE_ISSETUGID
504  	if (issetugid() != 0) {
505  		return NULL;
506  	}
507  #  endif
508  	return getenv(name);
509  #endif
510  }
511  static unsigned
512  malloc_ncpus(void) {
513  	long result;
514  #ifdef _WIN32
515  	SYSTEM_INFO si;
516  	GetSystemInfo(&amp;si);
517  	result = si.dwNumberOfProcessors;
518  #elif defined(JEMALLOC_GLIBC_MALLOC_HOOK) &amp;&amp; defined(CPU_COUNT)
519  	{
520  		cpu_set_t set;
521  		pthread_getaffinity_np(pthread_self(), sizeof(set), &amp;set);
522  		result = CPU_COUNT(&amp;set);
523  	}
524  #else
525  	result = sysconf(_SC_NPROCESSORS_ONLN);
526  #endif
527  	return ((result == -1) ? 1 : (unsigned)result);
528  }
529  static void
530  init_opt_stats_print_opts(const char *v, size_t vlen) {
531  	size_t opts_len = strlen(opt_stats_print_opts);
532  	assert(opts_len &lt;= stats_print_tot_num_options);
533  	for (size_t i = 0; i &lt; vlen; i++) {
534  		switch (v[i]) {
535  #define OPTION(o, v, d, s) case o: break;
536  			STATS_PRINT_OPTIONS
537  #undef OPTION
538  		default: continue;
539  		}
540  		if (strchr(opt_stats_print_opts, v[i]) != NULL) {
541  			continue;
542  		}
543  		opt_stats_print_opts[opts_len++] = v[i];
544  		opt_stats_print_opts[opts_len] = &#x27;\0&#x27;;
545  		assert(opts_len &lt;= stats_print_tot_num_options);
546  	}
547  	assert(opts_len == strlen(opt_stats_print_opts));
548  }
549  static bool
550  malloc_conf_multi_sizes_next(const char **slab_size_segment_cur,
551      size_t *vlen_left, size_t *slab_start, size_t *slab_end, size_t *new_size) {
552  	const char *cur = *slab_size_segment_cur;
553  	char *end;
554  	uintmax_t um;
555  	set_errno(0);
556  	um = malloc_strtoumax(cur, &amp;end, 0);
557  	if (get_errno() != 0 || *end != &#x27;-&#x27;) {
558  		return true;
559  	}
560  	*slab_start = (size_t)um;
561  	cur = end + 1;
562  	um = malloc_strtoumax(cur, &amp;end, 0);
563  	if (get_errno() != 0 || *end != &#x27;:&#x27;) {
564  		return true;
565  	}
566  	*slab_end = (size_t)um;
567  	cur = end + 1;
568  	um = malloc_strtoumax(cur, &amp;end, 0);
569  	if (get_errno() != 0) {
570  		return true;
571  	}
572  	*new_size = (size_t)um;
573  	if (*end == &#x27;|&#x27;) {
574  		end++;
575  	}
576  	*vlen_left -= end - *slab_size_segment_cur;
577  	*slab_size_segment_cur = end;
578  	return false;
579  }
580  static bool
581  malloc_conf_next(char const **opts_p, char const **k_p, size_t *klen_p,
582      char const **v_p, size_t *vlen_p) {
583  	bool accept;
584  	const char *opts = *opts_p;
585  	*k_p = opts;
586  	for (accept = false; !accept;) {
587  		switch (*opts) {
588  		case &#x27;A&#x27;: case &#x27;B&#x27;: case &#x27;C&#x27;: case &#x27;D&#x27;: case &#x27;E&#x27;: case &#x27;F&#x27;:
589  		case &#x27;G&#x27;: case &#x27;H&#x27;: case &#x27;I&#x27;: case &#x27;J&#x27;: case &#x27;K&#x27;: case &#x27;L&#x27;:
590  		case &#x27;M&#x27;: case &#x27;N&#x27;: case &#x27;O&#x27;: case &#x27;P&#x27;: case &#x27;Q&#x27;: case &#x27;R&#x27;:
591  		case &#x27;S&#x27;: case &#x27;T&#x27;: case &#x27;U&#x27;: case &#x27;V&#x27;: case &#x27;W&#x27;: case &#x27;X&#x27;:
592  		case &#x27;Y&#x27;: case &#x27;Z&#x27;:
593  		case &#x27;a&#x27;: case &#x27;b&#x27;: case &#x27;c&#x27;: case &#x27;d&#x27;: case &#x27;e&#x27;: case &#x27;f&#x27;:
594  		case &#x27;g&#x27;: case &#x27;h&#x27;: case &#x27;i&#x27;: case &#x27;j&#x27;: case &#x27;k&#x27;: case &#x27;l&#x27;:
595  		case &#x27;m&#x27;: case &#x27;n&#x27;: case &#x27;o&#x27;: case &#x27;p&#x27;: case &#x27;q&#x27;: case &#x27;r&#x27;:
596  		case &#x27;s&#x27;: case &#x27;t&#x27;: case &#x27;u&#x27;: case &#x27;v&#x27;: case &#x27;w&#x27;: case &#x27;x&#x27;:
597  		case &#x27;y&#x27;: case &#x27;z&#x27;:
598  		case &#x27;0&#x27;: case &#x27;1&#x27;: case &#x27;2&#x27;: case &#x27;3&#x27;: case &#x27;4&#x27;: case &#x27;5&#x27;:
599  		case &#x27;6&#x27;: case &#x27;7&#x27;: case &#x27;8&#x27;: case &#x27;9&#x27;:
600  		case &#x27;_&#x27;:
601  			opts++;
602  			break;
603  		case &#x27;:&#x27;:
604  			opts++;
605  			*klen_p = (uintptr_t)opts - 1 - (uintptr_t)*k_p;
606  			*v_p = opts;
607  			accept = true;
608  			break;
609  		case &#x27;\0&#x27;:
610  			if (opts != *opts_p) {
611  				malloc_write(&quot;&lt;jemalloc&gt;: Conf string ends &quot;
612  				    &quot;with key\n&quot;);
613  			}
614  			return true;
615  		default:
616  			malloc_write(&quot;&lt;jemalloc&gt;: Malformed conf string\n&quot;);
617  			return true;
618  		}
619  	}
620  	for (accept = false; !accept;) {
621  		switch (*opts) {
622  		case &#x27;,&#x27;:
623  			opts++;
624  			if (*opts == &#x27;\0&#x27;) {
625  				malloc_write(&quot;&lt;jemalloc&gt;: Conf string ends &quot;
626  				    &quot;with comma\n&quot;);
627  			}
628  			*vlen_p = (uintptr_t)opts - 1 - (uintptr_t)*v_p;
629  			accept = true;
630  			break;
631  		case &#x27;\0&#x27;:
632  			*vlen_p = (uintptr_t)opts - (uintptr_t)*v_p;
633  			accept = true;
634  			break;
635  		default:
636  			opts++;
637  			break;
638  		}
639  	}
640  	*opts_p = opts;
641  	return false;
642  }
643  static void
644  malloc_abort_invalid_conf(void) {
645  	assert(opt_abort_conf);
646  	malloc_printf(&quot;&lt;jemalloc&gt;: Abort (abort_conf:true) on invalid conf &quot;
647  	    &quot;value (see above).\n&quot;);
648  	abort();
649  }
650  static void
651  malloc_conf_error(const char *msg, const char *k, size_t klen, const char *v,
652      size_t vlen) {
653  	malloc_printf(&quot;&lt;jemalloc&gt;: %s: %.*s:%.*s\n&quot;, msg, (int)klen, k,
654  	    (int)vlen, v);
655  	const char *experimental = &quot;experimental_&quot;;
656  	if (strncmp(k, experimental, strlen(experimental)) == 0) {
657  		return;
658  	}
659  	had_conf_error = true;
660  }
661  static void
662  malloc_slow_flag_init(void) {
663  	malloc_slow_flags |= (opt_junk_alloc ? flag_opt_junk_alloc : 0)
664  	    | (opt_junk_free ? flag_opt_junk_free : 0)
665  	    | (opt_zero ? flag_opt_zero : 0)
666  	    | (opt_utrace ? flag_opt_utrace : 0)
667  	    | (opt_xmalloc ? flag_opt_xmalloc : 0);
668  	malloc_slow = (malloc_slow_flags != 0);
669  }
670  #define MALLOC_CONF_NSOURCES 4
671  static const char *
672  obtain_malloc_conf(unsigned which_source, char buf[PATH_MAX + 1]) {
673  	if (config_debug) {
674  		static unsigned read_source = 0;
675  		assert(read_source++ == which_source);
676  	}
677  	assert(which_source &lt; MALLOC_CONF_NSOURCES);
678  	const char *ret;
679  	switch (which_source) {
680  	case 0:
681  		ret = config_malloc_conf;
682  		break;
683  	case 1:
684  		if (je_malloc_conf != NULL) {
685  			ret = je_malloc_conf;
686  		} else {
687  			ret = NULL;
688  		}
689  		break;
690  	case 2: {
691  		ssize_t linklen = 0;
692  #ifndef _WIN32
693  		int saved_errno = errno;
694  		const char *linkname =
695  #  ifdef JEMALLOC_PREFIX
696  		    &quot;/etc/&quot;JEMALLOC_PREFIX&quot;malloc.conf&quot;
697  #  else
698  		    &quot;/etc/malloc.conf&quot;
699  #  endif
700  		    ;
701  #ifndef JEMALLOC_READLINKAT
702  		linklen = readlink(linkname, buf, PATH_MAX);
703  #else
704  		linklen = readlinkat(AT_FDCWD, linkname, buf, PATH_MAX);
705  #endif
706  		if (linklen == -1) {
707  			linklen = 0;
708  			set_errno(saved_errno);
709  		}
710  #endif
711  		buf[linklen] = &#x27;\0&#x27;;
712  		ret = buf;
713  		break;
714  	} case 3: {
715  		const char *envname =
716  #ifdef JEMALLOC_PREFIX
717  		    JEMALLOC_CPREFIX&quot;MALLOC_CONF&quot;
718  #else
719  		    &quot;MALLOC_CONF&quot;
720  #endif
721  		    ;
722  		if ((ret = jemalloc_secure_getenv(envname)) != NULL) {
723  		} else {
724  			ret = NULL;
725  		}
726  		break;
727  	} default:
728  		not_reached();
729  		ret = NULL;
730  	}
731  	return ret;
732  }
733  static void
734  malloc_conf_init_helper(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS],
735      bool initial_call, const char *opts_cache[MALLOC_CONF_NSOURCES],
736      char buf[PATH_MAX + 1]) {
737  	static const char *opts_explain[MALLOC_CONF_NSOURCES] = {
738  		&quot;string specified via --with-malloc-conf&quot;,
739  		&quot;string pointed to by the global variable malloc_conf&quot;,
740  		&quot;\&quot;name\&quot; of the file referenced by the symbolic link named &quot;
741  		    &quot;/etc/malloc.conf&quot;,
742  		&quot;value of the environment variable MALLOC_CONF&quot;
743  	};
744  	unsigned i;
745  	const char *opts, *k, *v;
746  	size_t klen, vlen;
747  	for (i = 0; i &lt; MALLOC_CONF_NSOURCES; i++) {
748  		if (initial_call) {
749  			opts_cache[i] = obtain_malloc_conf(i, buf);
750  		}
751  		opts = opts_cache[i];
752  		if (!initial_call &amp;&amp; opt_confirm_conf) {
753  			malloc_printf(
754  			    &quot;&lt;jemalloc&gt;: malloc_conf #%u (%s): \&quot;%s\&quot;\n&quot;,
755  			    i + 1, opts_explain[i], opts != NULL ? opts : &quot;&quot;);
756  		}
757  		if (opts == NULL) {
758  			continue;
759  		}
760  		while (*opts != &#x27;\0&#x27; &amp;&amp; !malloc_conf_next(&amp;opts, &amp;k, &amp;klen, &amp;v,
761  		    &amp;vlen)) {
762  #define CONF_ERROR(msg, k, klen, v, vlen)				\
763  			if (!initial_call) {				\
764  				malloc_conf_error(			\
765  				    msg, k, klen, v, vlen);		\
766  				cur_opt_valid = false;			\
767  			}
768  #define CONF_CONTINUE	{						\
769  				if (!initial_call &amp;&amp; opt_confirm_conf	\
770  				    &amp;&amp; cur_opt_valid) {			\
771  					malloc_printf(&quot;&lt;jemalloc&gt;: -- &quot;	\
772  					    &quot;Set conf value: %.*s:%.*s&quot;	\
773  					    &quot;\n&quot;, (int)klen, k,		\
774  					    (int)vlen, v);		\
775  				}					\
776  				continue;				\
777  			}
778  #define CONF_MATCH(n)							\
779  	(sizeof(n)-1 == klen &amp;&amp; strncmp(n, k, klen) == 0)
780  #define CONF_MATCH_VALUE(n)						\
781  	(sizeof(n)-1 == vlen &amp;&amp; strncmp(n, v, vlen) == 0)
782  #define CONF_HANDLE_BOOL(o, n)						\
783  			if (CONF_MATCH(n)) {				\
784  				if (CONF_MATCH_VALUE(&quot;true&quot;)) {		\
785  					o = true;			\
786  				} else if (CONF_MATCH_VALUE(&quot;false&quot;)) {	\
787  					o = false;			\
788  				} else {				\
789  					CONF_ERROR(&quot;Invalid conf value&quot;,\
790  					    k, klen, v, vlen);		\
791  				}					\
792  				CONF_CONTINUE;				\
793  			}
794        JEMALLOC_DIAGNOSTIC_PUSH
795        JEMALLOC_DIAGNOSTIC_IGNORE_TYPE_LIMITS
796  #define CONF_DONT_CHECK_MIN(um, min)	false
797  #define CONF_CHECK_MIN(um, min)	((um) &lt; (min))
798  #define CONF_DONT_CHECK_MAX(um, max)	false
799  #define CONF_CHECK_MAX(um, max)	((um) &gt; (max))
800  #define CONF_HANDLE_T_U(t, o, n, min, max, check_min, check_max, clip)	\
801  			if (CONF_MATCH(n)) {				\
802  				uintmax_t um;				\
803  				char *end;				\
804  									\
805  				set_errno(0);				\
806  				um = malloc_strtoumax(v, &amp;end, 0);	\
807  				if (get_errno() != 0 || (uintptr_t)end -\
808  				    (uintptr_t)v != vlen) {		\
809  					CONF_ERROR(&quot;Invalid conf value&quot;,\
810  					    k, klen, v, vlen);		\
811  				} else if (clip) {			\
812  					if (check_min(um, (t)(min))) {	\
813  						o = (t)(min);		\
814  					} else if (			\
815  					    check_max(um, (t)(max))) {	\
816  						o = (t)(max);		\
817  					} else {			\
818  						o = (t)um;		\
819  					}				\
820  				} else {				\
821  					if (check_min(um, (t)(min)) ||	\
822  					    check_max(um, (t)(max))) {	\
823  						CONF_ERROR(		\
824  						    &quot;Out-of-range &quot;	\
825  						    &quot;conf value&quot;,	\
826  						    k, klen, v, vlen);	\
827  					} else {			\
828  						o = (t)um;		\
829  					}				\
830  				}					\
831  				CONF_CONTINUE;				\
832  			}
833  #define CONF_HANDLE_UNSIGNED(o, n, min, max, check_min, check_max,	\
834      clip)								\
835  			CONF_HANDLE_T_U(unsigned, o, n, min, max,	\
836  			    check_min, check_max, clip)
837  #define CONF_HANDLE_SIZE_T(o, n, min, max, check_min, check_max, clip)	\
838  			CONF_HANDLE_T_U(size_t, o, n, min, max,		\
839  			    check_min, check_max, clip)
840  #define CONF_HANDLE_SSIZE_T(o, n, min, max)				\
841  			if (CONF_MATCH(n)) {				\
842  				long l;					\
843  				char *end;				\
844  									\
845  				set_errno(0);				\
846  				l = strtol(v, &amp;end, 0);			\
847  				if (get_errno() != 0 || (uintptr_t)end -\
848  				    (uintptr_t)v != vlen) {		\
849  					CONF_ERROR(&quot;Invalid conf value&quot;,\
850  					    k, klen, v, vlen);		\
851  				} else if (l &lt; (ssize_t)(min) || l &gt;	\
852  				    (ssize_t)(max)) {			\
853  					CONF_ERROR(			\
854  					    &quot;Out-of-range conf value&quot;,	\
855  					    k, klen, v, vlen);		\
856  				} else {				\
857  					o = l;				\
858  				}					\
859  				CONF_CONTINUE;				\
860  			}
861  #define CONF_HANDLE_CHAR_P(o, n, d)					\
862  			if (CONF_MATCH(n)) {				\
863  				size_t cpylen = (vlen &lt;=		\
864  				    sizeof(o)-1) ? vlen :		\
865  				    sizeof(o)-1;			\
866  				strncpy(o, v, cpylen);			\
867  				o[cpylen] = &#x27;\0&#x27;;			\
868  				CONF_CONTINUE;				\
869  			}
870  			bool cur_opt_valid = true;
871  			CONF_HANDLE_BOOL(opt_confirm_conf, &quot;confirm_conf&quot;)
872  			if (initial_call) {
873  				continue;
874  			}
875  			CONF_HANDLE_BOOL(opt_abort, &quot;abort&quot;)
876  			CONF_HANDLE_BOOL(opt_abort_conf, &quot;abort_conf&quot;)
877  			if (strncmp(&quot;metadata_thp&quot;, k, klen) == 0) {
878  				int i;
879  				bool match = false;
880  				for (i = 0; i &lt; metadata_thp_mode_limit; i++) {
881  					if (strncmp(metadata_thp_mode_names[i],
882  					    v, vlen) == 0) {
883  						opt_metadata_thp = i;
884  						match = true;
885  						break;
886  					}
887  				}
888  				if (!match) {
889  					CONF_ERROR(&quot;Invalid conf value&quot;,
890  					    k, klen, v, vlen);
891  				}
892  				CONF_CONTINUE;
893  			}
894  			CONF_HANDLE_BOOL(opt_retain, &quot;retain&quot;)
895  			if (strncmp(&quot;dss&quot;, k, klen) == 0) {
896  				int i;
897  				bool match = false;
898  				for (i = 0; i &lt; dss_prec_limit; i++) {
899  					if (strncmp(dss_prec_names[i], v, vlen)
900  					    == 0) {
901  						if (extent_dss_prec_set(i)) {
902  							CONF_ERROR(
903  							    &quot;Error setting dss&quot;,
904  							    k, klen, v, vlen);
905  						} else {
906  							opt_dss =
907  							    dss_prec_names[i];
908  							match = true;
909  							break;
910  						}
911  					}
912  				}
913  				if (!match) {
914  					CONF_ERROR(&quot;Invalid conf value&quot;,
915  					    k, klen, v, vlen);
916  				}
917  				CONF_CONTINUE;
918  			}
919  			CONF_HANDLE_UNSIGNED(opt_narenas, &quot;narenas&quot;, 1,
920  			    UINT_MAX, CONF_CHECK_MIN, CONF_DONT_CHECK_MAX,
921  			    false)
922  			if (CONF_MATCH(&quot;bin_shards&quot;)) {
923  				const char *bin_shards_segment_cur = v;
924  				size_t vlen_left = vlen;
925  				do {
926  					size_t size_start;
927  					size_t size_end;
928  					size_t nshards;
929  					bool err = malloc_conf_multi_sizes_next(
930  					    &amp;bin_shards_segment_cur, &amp;vlen_left,
931  					    &amp;size_start, &amp;size_end, &amp;nshards);
932  					if (err || bin_update_shard_size(
933  					    bin_shard_sizes, size_start,
934  					    size_end, nshards)) {
935  						CONF_ERROR(
936  						    &quot;Invalid settings for &quot;
937  						    &quot;bin_shards&quot;, k, klen, v,
938  						    vlen);
939  						break;
940  					}
941  				} while (vlen_left &gt; 0);
942  				CONF_CONTINUE;
943  			}
944  			CONF_HANDLE_SSIZE_T(opt_dirty_decay_ms,
945  			    &quot;dirty_decay_ms&quot;, -1, NSTIME_SEC_MAX * KQU(1000) &lt;
946  			    QU(SSIZE_MAX) ? NSTIME_SEC_MAX * KQU(1000) :
947  			    SSIZE_MAX);
948  			CONF_HANDLE_SSIZE_T(opt_muzzy_decay_ms,
949  			    &quot;muzzy_decay_ms&quot;, -1, NSTIME_SEC_MAX * KQU(1000) &lt;
950  			    QU(SSIZE_MAX) ? NSTIME_SEC_MAX * KQU(1000) :
951  			    SSIZE_MAX);
952  			CONF_HANDLE_BOOL(opt_stats_print, &quot;stats_print&quot;)
953  			if (CONF_MATCH(&quot;stats_print_opts&quot;)) {
954  				init_opt_stats_print_opts(v, vlen);
955  				CONF_CONTINUE;
956  			}
957  			if (config_fill) {
958  				if (CONF_MATCH(&quot;junk&quot;)) {
959  					if (CONF_MATCH_VALUE(&quot;true&quot;)) {
960  						opt_junk = &quot;true&quot;;
961  						opt_junk_alloc = opt_junk_free =
962  						    true;
963  					} else if (CONF_MATCH_VALUE(&quot;false&quot;)) {
964  						opt_junk = &quot;false&quot;;
965  						opt_junk_alloc = opt_junk_free =
966  						    false;
967  					} else if (CONF_MATCH_VALUE(&quot;alloc&quot;)) {
968  						opt_junk = &quot;alloc&quot;;
969  						opt_junk_alloc = true;
970  						opt_junk_free = false;
971  					} else if (CONF_MATCH_VALUE(&quot;free&quot;)) {
972  						opt_junk = &quot;free&quot;;
973  						opt_junk_alloc = false;
974  						opt_junk_free = true;
975  					} else {
976  						CONF_ERROR(
977  						    &quot;Invalid conf value&quot;,
978  						    k, klen, v, vlen);
979  					}
980  					CONF_CONTINUE;
981  				}
982  				CONF_HANDLE_BOOL(opt_zero, &quot;zero&quot;)
983  			}
984  			if (config_utrace) {
985  				CONF_HANDLE_BOOL(opt_utrace, &quot;utrace&quot;)
986  			}
987  			if (config_xmalloc) {
988  				CONF_HANDLE_BOOL(opt_xmalloc, &quot;xmalloc&quot;)
989  			}
990  			CONF_HANDLE_BOOL(opt_tcache, &quot;tcache&quot;)
991  			CONF_HANDLE_SSIZE_T(opt_lg_tcache_max, &quot;lg_tcache_max&quot;,
992  			    -1, (sizeof(size_t) &lt;&lt; 3) - 1)
993  			CONF_HANDLE_SIZE_T(opt_oversize_threshold,
994  			    &quot;oversize_threshold&quot;, 0, SC_LARGE_MAXCLASS,
995  			    CONF_DONT_CHECK_MIN, CONF_CHECK_MAX, false)
996  			CONF_HANDLE_SIZE_T(opt_lg_extent_max_active_fit,
997  			    &quot;lg_extent_max_active_fit&quot;, 0,
998  			    (sizeof(size_t) &lt;&lt; 3), CONF_DONT_CHECK_MIN,
999  			    CONF_CHECK_MAX, false)
1000  			if (strncmp(&quot;percpu_arena&quot;, k, klen) == 0) {
1001  				bool match = false;
1002  				for (int i = percpu_arena_mode_names_base; i &lt;
1003  				    percpu_arena_mode_names_limit; i++) {
1004  					if (strncmp(percpu_arena_mode_names[i],
1005  					    v, vlen) == 0) {
1006  						if (!have_percpu_arena) {
1007  							CONF_ERROR(
1008  							    &quot;No getcpu support&quot;,
1009  							    k, klen, v, vlen);
1010  						}
1011  						opt_percpu_arena = i;
1012  						match = true;
1013  						break;
1014  					}
1015  				}
1016  				if (!match) {
1017  					CONF_ERROR(&quot;Invalid conf value&quot;,
1018  					    k, klen, v, vlen);
1019  				}
1020  				CONF_CONTINUE;
1021  			}
1022  			CONF_HANDLE_BOOL(opt_background_thread,
1023  			    &quot;background_thread&quot;);
1024  			CONF_HANDLE_SIZE_T(opt_max_background_threads,
1025  					   &quot;max_background_threads&quot;, 1,
1026  					   opt_max_background_threads,
1027  					   CONF_CHECK_MIN, CONF_CHECK_MAX,
1028  					   true);
1029  			if (CONF_MATCH(&quot;slab_sizes&quot;)) {
1030  				bool err;
1031  				const char *slab_size_segment_cur = v;
1032  				size_t vlen_left = vlen;
1033  				do {
1034  					size_t slab_start;
1035  					size_t slab_end;
1036  					size_t pgs;
1037  					err = malloc_conf_multi_sizes_next(
1038  					    &amp;slab_size_segment_cur,
1039  					    &amp;vlen_left, &amp;slab_start, &amp;slab_end,
1040  					    &amp;pgs);
1041  					if (!err) {
1042  						sc_data_update_slab_size(
1043  						    sc_data, slab_start,
1044  						    slab_end, (int)pgs);
1045  					} else {
1046  						CONF_ERROR(&quot;Invalid settings &quot;
1047  						    &quot;for slab_sizes&quot;,
1048  						    k, klen, v, vlen);
1049  					}
1050  				} while (!err &amp;&amp; vlen_left &gt; 0);
1051  				CONF_CONTINUE;
1052  			}
1053  			if (config_prof) {
1054  				CONF_HANDLE_BOOL(opt_prof, &quot;prof&quot;)
1055  				CONF_HANDLE_CHAR_P(opt_prof_prefix,
1056  				    &quot;prof_prefix&quot;, &quot;jeprof&quot;)
1057  				CONF_HANDLE_BOOL(opt_prof_active, &quot;prof_active&quot;)
1058  				CONF_HANDLE_BOOL(opt_prof_thread_active_init,
1059  				    &quot;prof_thread_active_init&quot;)
1060  				CONF_HANDLE_SIZE_T(opt_lg_prof_sample,
1061  				    &quot;lg_prof_sample&quot;, 0, (sizeof(uint64_t) &lt;&lt; 3)
1062  				    - 1, CONF_DONT_CHECK_MIN, CONF_CHECK_MAX,
1063  				    true)
1064  				CONF_HANDLE_BOOL(opt_prof_accum, &quot;prof_accum&quot;)
1065  				CONF_HANDLE_SSIZE_T(opt_lg_prof_interval,
1066  				    &quot;lg_prof_interval&quot;, -1,
1067  				    (sizeof(uint64_t) &lt;&lt; 3) - 1)
1068  				CONF_HANDLE_BOOL(opt_prof_gdump, &quot;prof_gdump&quot;)
1069  				CONF_HANDLE_BOOL(opt_prof_final, &quot;prof_final&quot;)
1070  				CONF_HANDLE_BOOL(opt_prof_leak, &quot;prof_leak&quot;)
1071  				CONF_HANDLE_BOOL(opt_prof_log, &quot;prof_log&quot;)
1072  			}
1073  			if (config_log) {
1074  				if (CONF_MATCH(&quot;log&quot;)) {
1075  					size_t cpylen = (
1076  					    vlen &lt;= sizeof(log_var_names) ?
1077  					    vlen : sizeof(log_var_names) - 1);
1078  					strncpy(log_var_names, v, cpylen);
1079  					log_var_names[cpylen] = &#x27;\0&#x27;;
1080  					CONF_CONTINUE;
1081  				}
1082  			}
1083  			if (CONF_MATCH(&quot;thp&quot;)) {
1084  				bool match = false;
1085  				for (int i = 0; i &lt; thp_mode_names_limit; i++) {
1086  					if (strncmp(thp_mode_names[i],v, vlen)
1087  					    == 0) {
1088  						if (!have_madvise_huge) {
1089  							CONF_ERROR(
1090  							    &quot;No THP support&quot;,
1091  							    k, klen, v, vlen);
1092  						}
1093  						opt_thp = i;
1094  						match = true;
1095  						break;
1096  					}
1097  				}
1098  				if (!match) {
1099  					CONF_ERROR(&quot;Invalid conf value&quot;,
1100  					    k, klen, v, vlen);
1101  				}
1102  				CONF_CONTINUE;
1103  			}
1104  			CONF_ERROR(&quot;Invalid conf pair&quot;, k, klen, v, vlen);
1105  #undef CONF_ERROR
1106  #undef CONF_CONTINUE
1107  #undef CONF_MATCH
1108  #undef CONF_MATCH_VALUE
1109  #undef CONF_HANDLE_BOOL
1110  #undef CONF_DONT_CHECK_MIN
1111  #undef CONF_CHECK_MIN
1112  #undef CONF_DONT_CHECK_MAX
1113  #undef CONF_CHECK_MAX
1114  #undef CONF_HANDLE_T_U
1115  #undef CONF_HANDLE_UNSIGNED
1116  #undef CONF_HANDLE_SIZE_T
1117  #undef CONF_HANDLE_SSIZE_T
1118  #undef CONF_HANDLE_CHAR_P
1119      JEMALLOC_DIAGNOSTIC_POP
1120  		}
1121  		if (opt_abort_conf &amp;&amp; had_conf_error) {
1122  			malloc_abort_invalid_conf();
1123  		}
1124  	}
1125  	atomic_store_b(&amp;log_init_done, true, ATOMIC_RELEASE);
1126  }
1127  static void
1128  malloc_conf_init(sc_data_t *sc_data, unsigned bin_shard_sizes[SC_NBINS]) {
1129  	const char *opts_cache[MALLOC_CONF_NSOURCES] = {NULL, NULL, NULL, NULL};
1130  	char buf[PATH_MAX + 1];
1131  	malloc_conf_init_helper(NULL, NULL, true, opts_cache, buf);
1132  	malloc_conf_init_helper(sc_data, bin_shard_sizes, false, opts_cache,
1133  	    NULL);
1134  }
1135  #undef MALLOC_CONF_NSOURCES
1136  static bool
1137  malloc_init_hard_needed(void) {
1138  	if (malloc_initialized() || (IS_INITIALIZER &amp;&amp; malloc_init_state ==
1139  	    malloc_init_recursible)) {
1140  		return false;
1141  	}
1142  #ifdef JEMALLOC_THREADED_INIT
1143  	if (malloc_initializer != NO_INITIALIZER &amp;&amp; !IS_INITIALIZER) {
1144  		spin_t spinner = SPIN_INITIALIZER;
1145  		do {
1146  			malloc_mutex_unlock(TSDN_NULL, &amp;init_lock);
1147  			spin_adaptive(&amp;spinner);
1148  			malloc_mutex_lock(TSDN_NULL, &amp;init_lock);
1149  		} while (!malloc_initialized());
1150  		return false;
1151  	}
1152  #endif
1153  	return true;
1154  }
1155  static bool
1156  malloc_init_hard_a0_locked() {
1157  	malloc_initializer = INITIALIZER;
1158  	JEMALLOC_DIAGNOSTIC_PUSH
1159  	JEMALLOC_DIAGNOSTIC_IGNORE_MISSING_STRUCT_FIELD_INITIALIZERS
1160  	sc_data_t sc_data = {0};
1161  	JEMALLOC_DIAGNOSTIC_POP
1162  	sc_boot(&amp;sc_data);
1163  	unsigned bin_shard_sizes[SC_NBINS];
1164  	bin_shard_sizes_boot(bin_shard_sizes);
1165  	if (config_prof) {
1166  		prof_boot0();
1167  	}
1168  	malloc_conf_init(&amp;sc_data, bin_shard_sizes);
1169  	sz_boot(&amp;sc_data);
1170  	bin_boot(&amp;sc_data, bin_shard_sizes);
1171  	if (opt_stats_print) {
1172  		if (atexit(stats_print_atexit) != 0) {
1173  			malloc_write(&quot;&lt;jemalloc&gt;: Error in atexit()\n&quot;);
1174  			if (opt_abort) {
1175  				abort();
1176  			}
1177  		}
1178  	}
1179  	if (pages_boot()) {
1180  		return true;
1181  	}
1182  	if (base_boot(TSDN_NULL)) {
1183  		return true;
1184  	}
1185  	if (extent_boot()) {
1186  		return true;
1187  	}
1188  	if (ctl_boot()) {
1189  		return true;
1190  	}
1191  	if (config_prof) {
1192  		prof_boot1();
1193  	}
1194  	arena_boot(&amp;sc_data);
1195  	if (tcache_boot(TSDN_NULL)) {
1196  		return true;
1197  	}
1198  	if (malloc_mutex_init(&amp;arenas_lock, &quot;arenas&quot;, WITNESS_RANK_ARENAS,
1199  	    malloc_mutex_rank_exclusive)) {
1200  		return true;
1201  	}
1202  	hook_boot();
1203  	narenas_auto = 1;
1204  	manual_arena_base = narenas_auto + 1;
1205  	memset(arenas, 0, sizeof(arena_t *) * narenas_auto);
1206  	if (arena_init(TSDN_NULL, 0, (extent_hooks_t *)&amp;extent_hooks_default)
1207  	    == NULL) {
1208  		return true;
1209  	}
1210  	a0 = arena_get(TSDN_NULL, 0, false);
1211  	malloc_init_state = malloc_init_a0_initialized;
1212  	return false;
1213  }
1214  static bool
1215  malloc_init_hard_a0(void) {
1216  	bool ret;
1217  	malloc_mutex_lock(TSDN_NULL, &amp;init_lock);
1218  	ret = malloc_init_hard_a0_locked();
1219  	malloc_mutex_unlock(TSDN_NULL, &amp;init_lock);
1220  	return ret;
1221  }
1222  static bool
1223  malloc_init_hard_recursible(void) {
1224  	malloc_init_state = malloc_init_recursible;
1225  	ncpus = malloc_ncpus();
1226  #if (defined(JEMALLOC_HAVE_PTHREAD_ATFORK) &amp;&amp; !defined(JEMALLOC_MUTEX_INIT_CB) \
1227      &amp;&amp; !defined(JEMALLOC_ZONE) &amp;&amp; !defined(_WIN32) &amp;&amp; \
1228      !defined(__native_client__))
1229  	if (pthread_atfork(jemalloc_prefork, jemalloc_postfork_parent,
1230  	    jemalloc_postfork_child) != 0) {
1231  		malloc_write(&quot;&lt;jemalloc&gt;: Error in pthread_atfork()\n&quot;);
1232  		if (opt_abort) {
1233  			abort();
1234  		}
1235  		return true;
1236  	}
1237  #endif
1238  	if (background_thread_boot0()) {
1239  		return true;
1240  	}
1241  	return false;
1242  }
1243  static unsigned
1244  malloc_narenas_default(void) {
1245  	assert(ncpus &gt; 0);
1246  	if (ncpus &gt; 1) {
1247  		return ncpus &lt;&lt; 2;
1248  	} else {
1249  		return 1;
1250  	}
1251  }
1252  static percpu_arena_mode_t
1253  percpu_arena_as_initialized(percpu_arena_mode_t mode) {
1254  	assert(!malloc_initialized());
1255  	assert(mode &lt;= percpu_arena_disabled);
1256  	if (mode != percpu_arena_disabled) {
1257  		mode += percpu_arena_mode_enabled_base;
1258  	}
1259  	return mode;
1260  }
1261  static bool
1262  malloc_init_narenas(void) {
1263  	assert(ncpus &gt; 0);
1264  	if (opt_percpu_arena != percpu_arena_disabled) {
1265  		if (!have_percpu_arena || malloc_getcpu() &lt; 0) {
1266  			opt_percpu_arena = percpu_arena_disabled;
1267  			malloc_printf(&quot;&lt;jemalloc&gt;: perCPU arena getcpu() not &quot;
1268  			    &quot;available. Setting narenas to %u.\n&quot;, opt_narenas ?
1269  			    opt_narenas : malloc_narenas_default());
1270  			if (opt_abort) {
1271  				abort();
1272  			}
1273  		} else {
1274  			if (ncpus &gt;= MALLOCX_ARENA_LIMIT) {
1275  				malloc_printf(&quot;&lt;jemalloc&gt;: narenas w/ percpu&quot;
1276  				    &quot;arena beyond limit (%d)\n&quot;, ncpus);
1277  				if (opt_abort) {
1278  					abort();
1279  				}
1280  				return true;
1281  			}
1282  			if (percpu_arena_as_initialized(opt_percpu_arena) ==
1283  			    per_phycpu_arena &amp;&amp; ncpus % 2 != 0) {
1284  				malloc_printf(&quot;&lt;jemalloc&gt;: invalid &quot;
1285  				    &quot;configuration -- per physical CPU arena &quot;
1286  				    &quot;with odd number (%u) of CPUs (no hyper &quot;
1287  				    &quot;threading?).\n&quot;, ncpus);
1288  				if (opt_abort)
1289  					abort();
1290  			}
1291  			unsigned n = percpu_arena_ind_limit(
1292  			    percpu_arena_as_initialized(opt_percpu_arena));
1293  			if (opt_narenas &lt; n) {
1294  				opt_narenas = n;
1295  			}
1296  		}
1297  	}
1298  	if (opt_narenas == 0) {
1299  		opt_narenas = malloc_narenas_default();
1300  	}
1301  	assert(opt_narenas &gt; 0);
1302  	narenas_auto = opt_narenas;
1303  	if (narenas_auto &gt;= MALLOCX_ARENA_LIMIT) {
1304  		narenas_auto = MALLOCX_ARENA_LIMIT - 1;
1305  		malloc_printf(&quot;&lt;jemalloc&gt;: Reducing narenas to limit (%d)\n&quot;,
1306  		    narenas_auto);
1307  	}
1308  	narenas_total_set(narenas_auto);
1309  	if (arena_init_huge()) {
1310  		narenas_total_inc();
1311  	}
1312  	manual_arena_base = narenas_total_get();
1313  	return false;
1314  }
1315  static void
1316  malloc_init_percpu(void) {
1317  	opt_percpu_arena = percpu_arena_as_initialized(opt_percpu_arena);
1318  }
1319  static bool
1320  malloc_init_hard_finish(void) {
1321  	if (malloc_mutex_boot()) {
1322  		return true;
1323  	}
1324  	malloc_init_state = malloc_init_initialized;
1325  	malloc_slow_flag_init();
1326  	return false;
1327  }
1328  static void
1329  malloc_init_hard_cleanup(tsdn_t *tsdn, bool reentrancy_set) {
1330  	malloc_mutex_assert_owner(tsdn, &amp;init_lock);
1331  	malloc_mutex_unlock(tsdn, &amp;init_lock);
1332  	if (reentrancy_set) {
1333  		assert(!tsdn_null(tsdn));
1334  		tsd_t *tsd = tsdn_tsd(tsdn);
1335  		assert(tsd_reentrancy_level_get(tsd) &gt; 0);
1336  		post_reentrancy(tsd);
1337  	}
1338  }
1339  static bool
1340  malloc_init_hard(void) {
1341  	tsd_t *tsd;
1342  #if defined(_WIN32) &amp;&amp; _WIN32_WINNT &lt; 0x0600
1343  	_init_init_lock();
1344  #endif
1345  	malloc_mutex_lock(TSDN_NULL, &amp;init_lock);
1346  #define UNLOCK_RETURN(tsdn, ret, reentrancy)		\
1347  	malloc_init_hard_cleanup(tsdn, reentrancy);	\
1348  	return ret;
1349  	if (!malloc_init_hard_needed()) {
1350  		UNLOCK_RETURN(TSDN_NULL, false, false)
1351  	}
1352  	if (malloc_init_state != malloc_init_a0_initialized &amp;&amp;
1353  	    malloc_init_hard_a0_locked()) {
1354  		UNLOCK_RETURN(TSDN_NULL, true, false)
1355  	}
1356  	malloc_mutex_unlock(TSDN_NULL, &amp;init_lock);
1357  	tsd = malloc_tsd_boot0();
1358  	if (tsd == NULL) {
1359  		return true;
1360  	}
1361  	if (malloc_init_hard_recursible()) {
1362  		return true;
1363  	}
1364  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;init_lock);
1365  	pre_reentrancy(tsd, NULL);
1366  	if (malloc_init_narenas() || background_thread_boot1(tsd_tsdn(tsd))) {
1367  		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
1368  	}
1369  	if (config_prof &amp;&amp; prof_boot2(tsd)) {
1370  		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
1371  	}
1372  	malloc_init_percpu();
1373  	if (malloc_init_hard_finish()) {
1374  		UNLOCK_RETURN(tsd_tsdn(tsd), true, true)
1375  	}
1376  	post_reentrancy(tsd);
1377  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;init_lock);
1378  	witness_assert_lockless(witness_tsd_tsdn(
1379  	    tsd_witness_tsdp_get_unsafe(tsd)));
1380  	malloc_tsd_boot1();
1381  	tsd = tsd_fetch();
1382  	if (opt_background_thread) {
1383  		assert(have_background_thread);
1384  		background_thread_ctl_init(tsd_tsdn(tsd));
1385  		if (background_thread_create(tsd, 0)) {
1386  			return true;
1387  		}
1388  	}
1389  #undef UNLOCK_RETURN
1390  	return false;
1391  }
1392  typedef struct static_opts_s static_opts_t;
1393  struct static_opts_s {
1394  	bool may_overflow;
1395  	bool bump_empty_aligned_alloc;
1396  	bool assert_nonempty_alloc;
1397  	bool null_out_result_on_error;
1398  	bool set_errno_on_error;
1399  	size_t min_alignment;
1400  	const char *oom_string;
1401  	const char *invalid_alignment_string;
1402  	bool slow;
1403  	bool usize;
1404  };
1405  JEMALLOC_ALWAYS_INLINE void
1406  static_opts_init(static_opts_t *static_opts) {
1407  	static_opts-&gt;may_overflow = false;
1408  	static_opts-&gt;bump_empty_aligned_alloc = false;
1409  	static_opts-&gt;assert_nonempty_alloc = false;
1410  	static_opts-&gt;null_out_result_on_error = false;
1411  	static_opts-&gt;set_errno_on_error = false;
1412  	static_opts-&gt;min_alignment = 0;
1413  	static_opts-&gt;oom_string = &quot;&quot;;
1414  	static_opts-&gt;invalid_alignment_string = &quot;&quot;;
1415  	static_opts-&gt;slow = false;
1416  	static_opts-&gt;usize = false;
1417  }
1418  #define TCACHE_IND_NONE ((unsigned)-1)
1419  #define TCACHE_IND_AUTOMATIC ((unsigned)-2)
1420  #define ARENA_IND_AUTOMATIC ((unsigned)-1)
1421  typedef struct dynamic_opts_s dynamic_opts_t;
1422  struct dynamic_opts_s {
1423  	void **result;
1424  	size_t usize;
<span onclick='openModal()' class='match'>1425  	size_t num_items;
1426  	size_t item_size;
1427  	size_t alignment;
1428  	bool zero;
1429  	unsigned tcache_ind;
1430  	unsigned arena_ind;
</span>1431  };
1432  JEMALLOC_ALWAYS_INLINE void
1433  dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
1434  	dynamic_opts-&gt;result = NULL;
1435  	dynamic_opts-&gt;usize = 0;
1436  	dynamic_opts-&gt;num_items = 0;
1437  	dynamic_opts-&gt;item_size = 0;
1438  	dynamic_opts-&gt;alignment = 0;
1439  	dynamic_opts-&gt;zero = false;
1440  	dynamic_opts-&gt;tcache_ind = TCACHE_IND_AUTOMATIC;
1441  	dynamic_opts-&gt;arena_ind = ARENA_IND_AUTOMATIC;
1442  }
1443  JEMALLOC_ALWAYS_INLINE void *
1444  imalloc_no_sample(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd,
1445      size_t size, size_t usize, szind_t ind) {
1446  	tcache_t *tcache;
1447  	arena_t *arena;
1448  	if (dopts-&gt;tcache_ind == TCACHE_IND_AUTOMATIC) {
1449  		if (likely(!sopts-&gt;slow)) {
1450  			tcache = tsd_tcachep_get(tsd);
1451  			assert(tcache == tcache_get(tsd));
1452  		} else {
1453  			tcache = tcache_get(tsd);
1454  		}
1455  	} else if (dopts-&gt;tcache_ind == TCACHE_IND_NONE) {
1456  		tcache = NULL;
1457  	} else {
1458  		tcache = tcaches_get(tsd, dopts-&gt;tcache_ind);
1459  	}
1460  	if (dopts-&gt;arena_ind == ARENA_IND_AUTOMATIC) {
1461  		arena = NULL;
1462  	} else {
1463  		arena = arena_get(tsd_tsdn(tsd), dopts-&gt;arena_ind, true);
1464  	}
1465  	if (unlikely(dopts-&gt;alignment != 0)) {
1466  		return ipalloct(tsd_tsdn(tsd), usize, dopts-&gt;alignment,
1467  		    dopts-&gt;zero, tcache, arena);
1468  	}
1469  	return iallocztm(tsd_tsdn(tsd), size, ind, dopts-&gt;zero, tcache, false,
1470  	    arena, sopts-&gt;slow);
1471  }
1472  JEMALLOC_ALWAYS_INLINE void *
1473  imalloc_sample(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd,
1474      size_t usize, szind_t ind) {
1475  	void *ret;
1476  	szind_t ind_large;
1477  	size_t bumped_usize = usize;
1478  	if (usize &lt;= SC_SMALL_MAXCLASS) {
1479  		assert(((dopts-&gt;alignment == 0) ?
1480  		    sz_s2u(SC_LARGE_MINCLASS) :
1481  		    sz_sa2u(SC_LARGE_MINCLASS, dopts-&gt;alignment))
1482  			== SC_LARGE_MINCLASS);
1483  		ind_large = sz_size2index(SC_LARGE_MINCLASS);
1484  		bumped_usize = sz_s2u(SC_LARGE_MINCLASS);
1485  		ret = imalloc_no_sample(sopts, dopts, tsd, bumped_usize,
1486  		    bumped_usize, ind_large);
1487  		if (unlikely(ret == NULL)) {
1488  			return NULL;
1489  		}
1490  		arena_prof_promote(tsd_tsdn(tsd), ret, usize);
1491  	} else {
1492  		ret = imalloc_no_sample(sopts, dopts, tsd, usize, usize, ind);
1493  	}
1494  	return ret;
1495  }
1496  JEMALLOC_ALWAYS_INLINE bool
1497  compute_size_with_overflow(bool may_overflow, dynamic_opts_t *dopts,
1498      size_t *size) {
1499  	if (!may_overflow) {
1500  		assert(dopts-&gt;num_items == 1);
1501  		*size = dopts-&gt;item_size;
1502  		return false;
1503  	}
1504  	static const size_t high_bits = SIZE_T_MAX &lt;&lt; (sizeof(size_t) * 8 / 2);
1505  	*size = dopts-&gt;item_size * dopts-&gt;num_items;
1506  	if (unlikely(*size == 0)) {
1507  		return (dopts-&gt;num_items != 0 &amp;&amp; dopts-&gt;item_size != 0);
1508  	}
1509  	if (likely((high_bits &amp; (dopts-&gt;num_items | dopts-&gt;item_size)) == 0)) {
1510  		return false;
1511  	}
1512  	if (likely(*size / dopts-&gt;item_size == dopts-&gt;num_items)) {
1513  		return false;
1514  	}
1515  	return true;
1516  }
1517  JEMALLOC_ALWAYS_INLINE int
1518  imalloc_body(static_opts_t *sopts, dynamic_opts_t *dopts, tsd_t *tsd) {
1519  	void *allocation = NULL;
1520  	size_t size = 0;
1521  	szind_t ind = 0;
1522  	size_t usize = 0;
1523  	int8_t reentrancy_level;
1524  	if (unlikely(compute_size_with_overflow(sopts-&gt;may_overflow, dopts,
1525  	    &amp;size))) {
1526  		goto label_oom;
1527  	}
1528  	if (unlikely(dopts-&gt;alignment &lt; sopts-&gt;min_alignment
1529  	    || (dopts-&gt;alignment &amp; (dopts-&gt;alignment - 1)) != 0)) {
1530  		goto label_invalid_alignment;
1531  	}
1532  	if (dopts-&gt;alignment == 0) {
1533  		ind = sz_size2index(size);
1534  		if (unlikely(ind &gt;= SC_NSIZES)) {
1535  			goto label_oom;
1536  		}
1537  		if (config_stats || (config_prof &amp;&amp; opt_prof) || sopts-&gt;usize) {
1538  			usize = sz_index2size(ind);
1539  			dopts-&gt;usize = usize;
1540  			assert(usize &gt; 0 &amp;&amp; usize
1541  			    &lt;= SC_LARGE_MAXCLASS);
1542  		}
1543  	} else {
1544  		if (sopts-&gt;bump_empty_aligned_alloc) {
1545  			if (unlikely(size == 0)) {
1546  				size = 1;
1547  			}
1548  		}
1549  		usize = sz_sa2u(size, dopts-&gt;alignment);
1550  		dopts-&gt;usize = usize;
1551  		if (unlikely(usize == 0
1552  		    || usize &gt; SC_LARGE_MAXCLASS)) {
1553  			goto label_oom;
1554  		}
1555  	}
1556  	if (sopts-&gt;assert_nonempty_alloc) {
1557  		assert (size != 0);
1558  	}
1559  	check_entry_exit_locking(tsd_tsdn(tsd));
1560  	reentrancy_level = tsd_reentrancy_level_get(tsd);
1561  	if (sopts-&gt;slow &amp;&amp; unlikely(reentrancy_level &gt; 0)) {
1562  		assert(dopts-&gt;tcache_ind == TCACHE_IND_AUTOMATIC ||
1563  		    dopts-&gt;tcache_ind == TCACHE_IND_NONE);
1564  		assert(dopts-&gt;arena_ind == ARENA_IND_AUTOMATIC);
1565  		dopts-&gt;tcache_ind = TCACHE_IND_NONE;
1566  		dopts-&gt;arena_ind = 0;
1567  	}
1568  	if (config_prof &amp;&amp; opt_prof) {
1569  		prof_tctx_t *tctx = prof_alloc_prep(
1570  		    tsd, usize, prof_active_get_unlocked(), true);
1571  		alloc_ctx_t alloc_ctx;
1572  		if (likely((uintptr_t)tctx == (uintptr_t)1U)) {
1573  			alloc_ctx.slab = (usize
1574  			    &lt;= SC_SMALL_MAXCLASS);
1575  			allocation = imalloc_no_sample(
1576  			    sopts, dopts, tsd, usize, usize, ind);
1577  		} else if ((uintptr_t)tctx &gt; (uintptr_t)1U) {
1578  			allocation = imalloc_sample(
1579  			    sopts, dopts, tsd, usize, ind);
1580  			alloc_ctx.slab = false;
1581  		} else {
1582  			allocation = NULL;
1583  		}
1584  		if (unlikely(allocation == NULL)) {
1585  			prof_alloc_rollback(tsd, tctx, true);
1586  			goto label_oom;
1587  		}
1588  		prof_malloc(tsd_tsdn(tsd), allocation, usize, &amp;alloc_ctx, tctx);
1589  	} else {
1590  		allocation = imalloc_no_sample(sopts, dopts, tsd, size, usize,
1591  		    ind);
1592  		if (unlikely(allocation == NULL)) {
1593  			goto label_oom;
1594  		}
1595  	}
1596  	assert(dopts-&gt;alignment == 0
1597  	    || ((uintptr_t)allocation &amp; (dopts-&gt;alignment - 1)) == ZU(0));
1598  	if (config_stats) {
1599  		assert(usize == isalloc(tsd_tsdn(tsd), allocation));
1600  		*tsd_thread_allocatedp_get(tsd) += usize;
1601  	}
1602  	if (sopts-&gt;slow) {
1603  		UTRACE(0, size, allocation);
1604  	}
1605  	check_entry_exit_locking(tsd_tsdn(tsd));
1606  	*dopts-&gt;result = allocation;
1607  	return 0;
1608  label_oom:
1609  	if (unlikely(sopts-&gt;slow) &amp;&amp; config_xmalloc &amp;&amp; unlikely(opt_xmalloc)) {
1610  		malloc_write(sopts-&gt;oom_string);
1611  		abort();
1612  	}
1613  	if (sopts-&gt;slow) {
1614  		UTRACE(NULL, size, NULL);
1615  	}
1616  	check_entry_exit_locking(tsd_tsdn(tsd));
1617  	if (sopts-&gt;set_errno_on_error) {
1618  		set_errno(ENOMEM);
1619  	}
1620  	if (sopts-&gt;null_out_result_on_error) {
1621  		*dopts-&gt;result = NULL;
1622  	}
1623  	return ENOMEM;
1624  label_invalid_alignment:
1625  	if (config_xmalloc &amp;&amp; unlikely(opt_xmalloc)) {
1626  		malloc_write(sopts-&gt;invalid_alignment_string);
1627  		abort();
1628  	}
1629  	if (sopts-&gt;set_errno_on_error) {
1630  		set_errno(EINVAL);
1631  	}
1632  	if (sopts-&gt;slow) {
1633  		UTRACE(NULL, size, NULL);
1634  	}
1635  	check_entry_exit_locking(tsd_tsdn(tsd));
1636  	if (sopts-&gt;null_out_result_on_error) {
1637  		*dopts-&gt;result = NULL;
1638  	}
1639  	return EINVAL;
1640  }
1641  JEMALLOC_ALWAYS_INLINE bool
1642  imalloc_init_check(static_opts_t *sopts, dynamic_opts_t *dopts) {
1643  	if (unlikely(!malloc_initialized()) &amp;&amp; unlikely(malloc_init())) {
1644  		if (config_xmalloc &amp;&amp; unlikely(opt_xmalloc)) {
1645  			malloc_write(sopts-&gt;oom_string);
1646  			abort();
1647  		}
1648  		UTRACE(NULL, dopts-&gt;num_items * dopts-&gt;item_size, NULL);
1649  		set_errno(ENOMEM);
1650  		*dopts-&gt;result = NULL;
1651  		return false;
1652  	}
1653  	return true;
1654  }
1655  JEMALLOC_ALWAYS_INLINE int
1656  imalloc(static_opts_t *sopts, dynamic_opts_t *dopts) {
1657  	if (tsd_get_allocates() &amp;&amp; !imalloc_init_check(sopts, dopts)) {
1658  		return ENOMEM;
1659  	}
1660  	tsd_t *tsd = tsd_fetch();
1661  	assert(tsd);
1662  	if (likely(tsd_fast(tsd))) {
1663  		tsd_assert_fast(tsd);
1664  		sopts-&gt;slow = false;
1665  		return imalloc_body(sopts, dopts, tsd);
1666  	} else {
1667  		if (!tsd_get_allocates() &amp;&amp; !imalloc_init_check(sopts, dopts)) {
1668  			return ENOMEM;
1669  		}
1670  		sopts-&gt;slow = true;
1671  		return imalloc_body(sopts, dopts, tsd);
1672  	}
1673  }
1674  JEMALLOC_NOINLINE
1675  void *
1676  malloc_default(size_t size) {
1677  	void *ret;
1678  	static_opts_t sopts;
1679  	dynamic_opts_t dopts;
1680  	LOG(&quot;core.malloc.entry&quot;, &quot;size: %zu&quot;, size);
1681  	static_opts_init(&amp;sopts);
1682  	dynamic_opts_init(&amp;dopts);
1683  	sopts.null_out_result_on_error = true;
1684  	sopts.set_errno_on_error = true;
1685  	sopts.oom_string = &quot;&lt;jemalloc&gt;: Error in malloc(): out of memory\n&quot;;
1686  	dopts.result = &amp;ret;
1687  	dopts.num_items = 1;
1688  	dopts.item_size = size;
1689  	imalloc(&amp;sopts, &amp;dopts);
1690  	if (sopts.slow) {
1691  		uintptr_t args[3] = {size};
1692  		hook_invoke_alloc(hook_alloc_malloc, ret, (uintptr_t)ret, args);
1693  	}
1694  	LOG(&quot;core.malloc.exit&quot;, &quot;result: %p&quot;, ret);
1695  	return ret;
1696  }
1697  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
1698  void JEMALLOC_NOTHROW *
1699  JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)
1700  je_malloc(size_t size) {
1701  	LOG(&quot;core.malloc.entry&quot;, &quot;size: %zu&quot;, size);
1702  	if (tsd_get_allocates() &amp;&amp; unlikely(!malloc_initialized())) {
1703  		return malloc_default(size);
1704  	}
1705  	tsd_t *tsd = tsd_get(false);
1706  	if (unlikely(!tsd || !tsd_fast(tsd) || (size &gt; SC_LOOKUP_MAXCLASS))) {
1707  		return malloc_default(size);
1708  	}
1709  	tcache_t *tcache = tsd_tcachep_get(tsd);
1710  	if (unlikely(ticker_trytick(&amp;tcache-&gt;gc_ticker))) {
1711  		return malloc_default(size);
1712  	}
1713  	szind_t ind = sz_size2index_lookup(size);
1714  	size_t usize;
1715  	if (config_stats || config_prof) {
1716  		usize = sz_index2size(ind);
1717  	}
1718  	assert(ind &lt; SC_NBINS);
1719  	assert(size &lt;= SC_SMALL_MAXCLASS);
1720  	if (config_prof) {
1721  		int64_t bytes_until_sample = tsd_bytes_until_sample_get(tsd);
1722  		bytes_until_sample -= usize;
1723  		tsd_bytes_until_sample_set(tsd, bytes_until_sample);
1724  		if (unlikely(bytes_until_sample &lt; 0)) {
1725  			if (!prof_active) {
1726  				tsd_bytes_until_sample_set(tsd, SSIZE_MAX);
1727  			}
1728  			return malloc_default(size);
1729  		}
1730  	}
1731  	cache_bin_t *bin = tcache_small_bin_get(tcache, ind);
1732  	bool tcache_success;
1733  	void* ret = cache_bin_alloc_easy(bin, &amp;tcache_success);
1734  	if (tcache_success) {
1735  		if (config_stats) {
1736  			*tsd_thread_allocatedp_get(tsd) += usize;
1737  			bin-&gt;tstats.nrequests++;
1738  		}
1739  		if (config_prof) {
1740  			tcache-&gt;prof_accumbytes += usize;
1741  		}
1742  		LOG(&quot;core.malloc.exit&quot;, &quot;result: %p&quot;, ret);
1743  		return ret;
1744  	}
1745  	return malloc_default(size);
1746  }
1747  JEMALLOC_EXPORT int JEMALLOC_NOTHROW
1748  JEMALLOC_ATTR(nonnull(1))
1749  je_posix_memalign(void **memptr, size_t alignment, size_t size) {
1750  	int ret;
1751  	static_opts_t sopts;
1752  	dynamic_opts_t dopts;
1753  	LOG(&quot;core.posix_memalign.entry&quot;, &quot;mem ptr: %p, alignment: %zu, &quot;
1754  	    &quot;size: %zu&quot;, memptr, alignment, size);
1755  	static_opts_init(&amp;sopts);
1756  	dynamic_opts_init(&amp;dopts);
1757  	sopts.bump_empty_aligned_alloc = true;
1758  	sopts.min_alignment = sizeof(void *);
1759  	sopts.oom_string =
1760  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: out of memory\n&quot;;
1761  	sopts.invalid_alignment_string =
1762  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: invalid alignment\n&quot;;
1763  	dopts.result = memptr;
1764  	dopts.num_items = 1;
1765  	dopts.item_size = size;
1766  	dopts.alignment = alignment;
1767  	ret = imalloc(&amp;sopts, &amp;dopts);
1768  	if (sopts.slow) {
1769  		uintptr_t args[3] = {(uintptr_t)memptr, (uintptr_t)alignment,
1770  			(uintptr_t)size};
1771  		hook_invoke_alloc(hook_alloc_posix_memalign, *memptr,
1772  		    (uintptr_t)ret, args);
1773  	}
1774  	LOG(&quot;core.posix_memalign.exit&quot;, &quot;result: %d, alloc ptr: %p&quot;, ret,
1775  	    *memptr);
1776  	return ret;
1777  }
1778  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
1779  void JEMALLOC_NOTHROW *
1780  JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(2)
1781  je_aligned_alloc(size_t alignment, size_t size) {
1782  	void *ret;
1783  	static_opts_t sopts;
1784  	dynamic_opts_t dopts;
1785  	LOG(&quot;core.aligned_alloc.entry&quot;, &quot;alignment: %zu, size: %zu\n&quot;,
1786  	    alignment, size);
1787  	static_opts_init(&amp;sopts);
1788  	dynamic_opts_init(&amp;dopts);
1789  	sopts.bump_empty_aligned_alloc = true;
1790  	sopts.null_out_result_on_error = true;
1791  	sopts.set_errno_on_error = true;
1792  	sopts.min_alignment = 1;
1793  	sopts.oom_string =
1794  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: out of memory\n&quot;;
1795  	sopts.invalid_alignment_string =
1796  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: invalid alignment\n&quot;;
1797  	dopts.result = &amp;ret;
1798  	dopts.num_items = 1;
1799  	dopts.item_size = size;
1800  	dopts.alignment = alignment;
1801  	imalloc(&amp;sopts, &amp;dopts);
1802  	if (sopts.slow) {
1803  		uintptr_t args[3] = {(uintptr_t)alignment, (uintptr_t)size};
1804  		hook_invoke_alloc(hook_alloc_aligned_alloc, ret,
1805  		    (uintptr_t)ret, args);
1806  	}
1807  	LOG(&quot;core.aligned_alloc.exit&quot;, &quot;result: %p&quot;, ret);
1808  	return ret;
1809  }
1810  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
1811  void JEMALLOC_NOTHROW *
1812  JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE2(1, 2)
1813  je_calloc(size_t num, size_t size) {
1814  	void *ret;
1815  	static_opts_t sopts;
1816  	dynamic_opts_t dopts;
1817  	LOG(&quot;core.calloc.entry&quot;, &quot;num: %zu, size: %zu\n&quot;, num, size);
1818  	static_opts_init(&amp;sopts);
1819  	dynamic_opts_init(&amp;dopts);
1820  	sopts.may_overflow = true;
1821  	sopts.null_out_result_on_error = true;
1822  	sopts.set_errno_on_error = true;
1823  	sopts.oom_string = &quot;&lt;jemalloc&gt;: Error in calloc(): out of memory\n&quot;;
1824  	dopts.result = &amp;ret;
1825  	dopts.num_items = num;
1826  	dopts.item_size = size;
1827  	dopts.zero = true;
1828  	imalloc(&amp;sopts, &amp;dopts);
1829  	if (sopts.slow) {
1830  		uintptr_t args[3] = {(uintptr_t)num, (uintptr_t)size};
1831  		hook_invoke_alloc(hook_alloc_calloc, ret, (uintptr_t)ret, args);
1832  	}
1833  	LOG(&quot;core.calloc.exit&quot;, &quot;result: %p&quot;, ret);
1834  	return ret;
1835  }
1836  static void *
1837  irealloc_prof_sample(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize,
1838      prof_tctx_t *tctx, hook_ralloc_args_t *hook_args) {
1839  	void *p;
1840  	if (tctx == NULL) {
1841  		return NULL;
1842  	}
1843  	if (usize &lt;= SC_SMALL_MAXCLASS) {
1844  		p = iralloc(tsd, old_ptr, old_usize,
1845  		    SC_LARGE_MINCLASS, 0, false, hook_args);
1846  		if (p == NULL) {
1847  			return NULL;
1848  		}
1849  		arena_prof_promote(tsd_tsdn(tsd), p, usize);
1850  	} else {
1851  		p = iralloc(tsd, old_ptr, old_usize, usize, 0, false,
1852  		    hook_args);
1853  	}
1854  	return p;
1855  }
1856  JEMALLOC_ALWAYS_INLINE void *
1857  irealloc_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t usize,
1858     alloc_ctx_t *alloc_ctx, hook_ralloc_args_t *hook_args) {
1859  	void *p;
1860  	bool prof_active;
1861  	prof_tctx_t *old_tctx, *tctx;
1862  	prof_active = prof_active_get_unlocked();
1863  	old_tctx = prof_tctx_get(tsd_tsdn(tsd), old_ptr, alloc_ctx);
1864  	tctx = prof_alloc_prep(tsd, usize, prof_active, true);
1865  	if (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {
1866  		p = irealloc_prof_sample(tsd, old_ptr, old_usize, usize, tctx,
1867  		    hook_args);
1868  	} else {
1869  		p = iralloc(tsd, old_ptr, old_usize, usize, 0, false,
1870  		    hook_args);
1871  	}
1872  	if (unlikely(p == NULL)) {
1873  		prof_alloc_rollback(tsd, tctx, true);
1874  		return NULL;
1875  	}
1876  	prof_realloc(tsd, p, usize, tctx, prof_active, true, old_ptr, old_usize,
1877  	    old_tctx);
1878  	return p;
1879  }
1880  JEMALLOC_ALWAYS_INLINE void
1881  ifree(tsd_t *tsd, void *ptr, tcache_t *tcache, bool slow_path) {
1882  	if (!slow_path) {
1883  		tsd_assert_fast(tsd);
1884  	}
1885  	check_entry_exit_locking(tsd_tsdn(tsd));
1886  	if (tsd_reentrancy_level_get(tsd) != 0) {
1887  		assert(slow_path);
1888  	}
1889  	assert(ptr != NULL);
1890  	assert(malloc_initialized() || IS_INITIALIZER);
1891  	alloc_ctx_t alloc_ctx;
1892  	rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
1893  	rtree_szind_slab_read(tsd_tsdn(tsd), &amp;extents_rtree, rtree_ctx,
1894  	    (uintptr_t)ptr, true, &amp;alloc_ctx.szind, &amp;alloc_ctx.slab);
1895  	assert(alloc_ctx.szind != SC_NSIZES);
1896  	size_t usize;
1897  	if (config_prof &amp;&amp; opt_prof) {
1898  		usize = sz_index2size(alloc_ctx.szind);
1899  		prof_free(tsd, ptr, usize, &amp;alloc_ctx);
1900  	} else if (config_stats) {
1901  		usize = sz_index2size(alloc_ctx.szind);
1902  	}
1903  	if (config_stats) {
1904  		*tsd_thread_deallocatedp_get(tsd) += usize;
1905  	}
1906  	if (likely(!slow_path)) {
1907  		idalloctm(tsd_tsdn(tsd), ptr, tcache, &amp;alloc_ctx, false,
1908  		    false);
1909  	} else {
1910  		idalloctm(tsd_tsdn(tsd), ptr, tcache, &amp;alloc_ctx, false,
1911  		    true);
1912  	}
1913  }
1914  JEMALLOC_ALWAYS_INLINE void
1915  isfree(tsd_t *tsd, void *ptr, size_t usize, tcache_t *tcache, bool slow_path) {
1916  	if (!slow_path) {
1917  		tsd_assert_fast(tsd);
1918  	}
1919  	check_entry_exit_locking(tsd_tsdn(tsd));
1920  	if (tsd_reentrancy_level_get(tsd) != 0) {
1921  		assert(slow_path);
1922  	}
1923  	assert(ptr != NULL);
1924  	assert(malloc_initialized() || IS_INITIALIZER);
1925  	alloc_ctx_t alloc_ctx, *ctx;
1926  	if (!config_cache_oblivious &amp;&amp; ((uintptr_t)ptr &amp; PAGE_MASK) != 0) {
1927  		alloc_ctx.szind = sz_size2index(usize);
1928  		alloc_ctx.slab = true;
1929  		ctx = &amp;alloc_ctx;
1930  		if (config_debug) {
1931  			alloc_ctx_t dbg_ctx;
1932  			rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
1933  			rtree_szind_slab_read(tsd_tsdn(tsd), &amp;extents_rtree,
1934  			    rtree_ctx, (uintptr_t)ptr, true, &amp;dbg_ctx.szind,
1935  			    &amp;dbg_ctx.slab);
1936  			assert(dbg_ctx.szind == alloc_ctx.szind);
1937  			assert(dbg_ctx.slab == alloc_ctx.slab);
1938  		}
1939  	} else if (config_prof &amp;&amp; opt_prof) {
1940  		rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
1941  		rtree_szind_slab_read(tsd_tsdn(tsd), &amp;extents_rtree, rtree_ctx,
1942  		    (uintptr_t)ptr, true, &amp;alloc_ctx.szind, &amp;alloc_ctx.slab);
1943  		assert(alloc_ctx.szind == sz_size2index(usize));
1944  		ctx = &amp;alloc_ctx;
1945  	} else {
1946  		ctx = NULL;
1947  	}
1948  	if (config_prof &amp;&amp; opt_prof) {
1949  		prof_free(tsd, ptr, usize, ctx);
1950  	}
1951  	if (config_stats) {
1952  		*tsd_thread_deallocatedp_get(tsd) += usize;
1953  	}
1954  	if (likely(!slow_path)) {
1955  		isdalloct(tsd_tsdn(tsd), ptr, usize, tcache, ctx, false);
1956  	} else {
1957  		isdalloct(tsd_tsdn(tsd), ptr, usize, tcache, ctx, true);
1958  	}
1959  }
1960  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
1961  void JEMALLOC_NOTHROW *
1962  JEMALLOC_ALLOC_SIZE(2)
1963  je_realloc(void *ptr, size_t arg_size) {
1964  	void *ret;
1965  	tsdn_t *tsdn JEMALLOC_CC_SILENCE_INIT(NULL);
1966  	size_t usize JEMALLOC_CC_SILENCE_INIT(0);
1967  	size_t old_usize = 0;
1968  	size_t size = arg_size;
1969  	LOG(&quot;core.realloc.entry&quot;, &quot;ptr: %p, size: %zu\n&quot;, ptr, size);
1970  	if (unlikely(size == 0)) {
1971  		if (ptr != NULL) {
1972  			UTRACE(ptr, 0, 0);
1973  			tcache_t *tcache;
1974  			tsd_t *tsd = tsd_fetch();
1975  			if (tsd_reentrancy_level_get(tsd) == 0) {
1976  				tcache = tcache_get(tsd);
1977  			} else {
1978  				tcache = NULL;
1979  			}
1980  			uintptr_t args[3] = {(uintptr_t)ptr, size};
1981  			hook_invoke_dalloc(hook_dalloc_realloc, ptr, args);
1982  			ifree(tsd, ptr, tcache, true);
1983  			LOG(&quot;core.realloc.exit&quot;, &quot;result: %p&quot;, NULL);
1984  			return NULL;
1985  		}
1986  		size = 1;
1987  	}
1988  	if (likely(ptr != NULL)) {
1989  		assert(malloc_initialized() || IS_INITIALIZER);
1990  		tsd_t *tsd = tsd_fetch();
1991  		check_entry_exit_locking(tsd_tsdn(tsd));
1992  		hook_ralloc_args_t hook_args = {true, {(uintptr_t)ptr,
1993  			(uintptr_t)arg_size, 0, 0}};
1994  		alloc_ctx_t alloc_ctx;
1995  		rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
1996  		rtree_szind_slab_read(tsd_tsdn(tsd), &amp;extents_rtree, rtree_ctx,
1997  		    (uintptr_t)ptr, true, &amp;alloc_ctx.szind, &amp;alloc_ctx.slab);
1998  		assert(alloc_ctx.szind != SC_NSIZES);
1999  		old_usize = sz_index2size(alloc_ctx.szind);
2000  		assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
2001  		if (config_prof &amp;&amp; opt_prof) {
2002  			usize = sz_s2u(size);
2003  			if (unlikely(usize == 0
2004  			    || usize &gt; SC_LARGE_MAXCLASS)) {
2005  				ret = NULL;
2006  			} else {
2007  				ret = irealloc_prof(tsd, ptr, old_usize, usize,
2008  				    &amp;alloc_ctx, &amp;hook_args);
2009  			}
2010  		} else {
2011  			if (config_stats) {
2012  				usize = sz_s2u(size);
2013  			}
2014  			ret = iralloc(tsd, ptr, old_usize, size, 0, false,
2015  			    &amp;hook_args);
2016  		}
2017  		tsdn = tsd_tsdn(tsd);
2018  	} else {
2019  		static_opts_t sopts;
2020  		dynamic_opts_t dopts;
2021  		static_opts_init(&amp;sopts);
2022  		dynamic_opts_init(&amp;dopts);
2023  		sopts.null_out_result_on_error = true;
2024  		sopts.set_errno_on_error = true;
2025  		sopts.oom_string =
2026  		    &quot;&lt;jemalloc&gt;: Error in realloc(): out of memory\n&quot;;
2027  		dopts.result = &amp;ret;
2028  		dopts.num_items = 1;
2029  		dopts.item_size = size;
2030  		imalloc(&amp;sopts, &amp;dopts);
2031  		if (sopts.slow) {
2032  			uintptr_t args[3] = {(uintptr_t)ptr, arg_size};
2033  			hook_invoke_alloc(hook_alloc_realloc, ret,
2034  			    (uintptr_t)ret, args);
2035  		}
2036  		return ret;
2037  	}
2038  	if (unlikely(ret == NULL)) {
2039  		if (config_xmalloc &amp;&amp; unlikely(opt_xmalloc)) {
2040  			malloc_write(&quot;&lt;jemalloc&gt;: Error in realloc(): &quot;
2041  			    &quot;out of memory\n&quot;);
2042  			abort();
2043  		}
2044  		set_errno(ENOMEM);
2045  	}
2046  	if (config_stats &amp;&amp; likely(ret != NULL)) {
2047  		tsd_t *tsd;
2048  		assert(usize == isalloc(tsdn, ret));
2049  		tsd = tsdn_tsd(tsdn);
2050  		*tsd_thread_allocatedp_get(tsd) += usize;
2051  		*tsd_thread_deallocatedp_get(tsd) += old_usize;
2052  	}
2053  	UTRACE(ptr, size, ret);
2054  	check_entry_exit_locking(tsdn);
2055  	LOG(&quot;core.realloc.exit&quot;, &quot;result: %p&quot;, ret);
2056  	return ret;
2057  }
2058  JEMALLOC_NOINLINE
2059  void
2060  free_default(void *ptr) {
2061  	UTRACE(ptr, 0, 0);
2062  	if (likely(ptr != NULL)) {
2063  		tsd_t *tsd = tsd_fetch_min();
2064  		check_entry_exit_locking(tsd_tsdn(tsd));
2065  		tcache_t *tcache;
2066  		if (likely(tsd_fast(tsd))) {
2067  			tsd_assert_fast(tsd);
2068  			tcache = tsd_tcachep_get(tsd);
2069  			ifree(tsd, ptr, tcache, false);
2070  		} else {
2071  			if (likely(tsd_reentrancy_level_get(tsd) == 0)) {
2072  				tcache = tcache_get(tsd);
2073  			} else {
2074  				tcache = NULL;
2075  			}
2076  			uintptr_t args_raw[3] = {(uintptr_t)ptr};
2077  			hook_invoke_dalloc(hook_dalloc_free, ptr, args_raw);
2078  			ifree(tsd, ptr, tcache, true);
2079  		}
2080  		check_entry_exit_locking(tsd_tsdn(tsd));
2081  	}
2082  }
2083  JEMALLOC_ALWAYS_INLINE
2084  bool free_fastpath(void *ptr, size_t size, bool size_hint) {
2085  	tsd_t *tsd = tsd_get(false);
2086  	if (unlikely(!tsd || !tsd_fast(tsd))) {
2087  		return false;
2088  	}
2089  	tcache_t *tcache = tsd_tcachep_get(tsd);
2090  	alloc_ctx_t alloc_ctx;
2091  	if (!size_hint || config_cache_oblivious) {
2092  		rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
2093  		bool res = rtree_szind_slab_read_fast(tsd_tsdn(tsd), &amp;extents_rtree,
2094  						      rtree_ctx, (uintptr_t)ptr,
2095  						      &amp;alloc_ctx.szind, &amp;alloc_ctx.slab);
2096  		if (!res || !alloc_ctx.slab) {
2097  			return false;
2098  		}
2099  		assert(alloc_ctx.szind != SC_NSIZES);
2100  	} else {
2101  		if (size &gt; SC_LOOKUP_MAXCLASS || (((uintptr_t)ptr &amp; PAGE_MASK) == 0)) {
2102  			return false;
2103  		}
2104  		alloc_ctx.szind = sz_size2index_lookup(size);
2105  	}
2106  	if (unlikely(ticker_trytick(&amp;tcache-&gt;gc_ticker))) {
2107  		return false;
2108  	}
2109  	cache_bin_t *bin = tcache_small_bin_get(tcache, alloc_ctx.szind);
2110  	cache_bin_info_t *bin_info = &amp;tcache_bin_info[alloc_ctx.szind];
2111  	if (!cache_bin_dalloc_easy(bin, bin_info, ptr)) {
2112  		return false;
2113  	}
2114  	if (config_stats) {
2115  		size_t usize = sz_index2size(alloc_ctx.szind);
2116  		*tsd_thread_deallocatedp_get(tsd) += usize;
2117  	}
2118  	return true;
2119  }
2120  JEMALLOC_EXPORT void JEMALLOC_NOTHROW
2121  je_free(void *ptr) {
2122  	LOG(&quot;core.free.entry&quot;, &quot;ptr: %p&quot;, ptr);
2123  	if (!free_fastpath(ptr, 0, false)) {
2124  		free_default(ptr);
2125  	}
2126  	LOG(&quot;core.free.exit&quot;, &quot;&quot;);
2127  }
2128  #ifdef JEMALLOC_OVERRIDE_MEMALIGN
2129  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2130  void JEMALLOC_NOTHROW *
2131  JEMALLOC_ATTR(malloc)
2132  je_memalign(size_t alignment, size_t size) {
2133  	void *ret;
2134  	static_opts_t sopts;
2135  	dynamic_opts_t dopts;
2136  	LOG(&quot;core.memalign.entry&quot;, &quot;alignment: %zu, size: %zu\n&quot;, alignment,
2137  	    size);
2138  	static_opts_init(&amp;sopts);
2139  	dynamic_opts_init(&amp;dopts);
2140  	sopts.min_alignment = 1;
2141  	sopts.oom_string =
2142  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: out of memory\n&quot;;
2143  	sopts.invalid_alignment_string =
2144  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: invalid alignment\n&quot;;
2145  	sopts.null_out_result_on_error = true;
2146  	dopts.result = &amp;ret;
2147  	dopts.num_items = 1;
2148  	dopts.item_size = size;
2149  	dopts.alignment = alignment;
2150  	imalloc(&amp;sopts, &amp;dopts);
2151  	if (sopts.slow) {
2152  		uintptr_t args[3] = {alignment, size};
2153  		hook_invoke_alloc(hook_alloc_memalign, ret, (uintptr_t)ret,
2154  		    args);
2155  	}
2156  	LOG(&quot;core.memalign.exit&quot;, &quot;result: %p&quot;, ret);
2157  	return ret;
2158  }
2159  #endif
2160  #ifdef JEMALLOC_OVERRIDE_VALLOC
2161  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2162  void JEMALLOC_NOTHROW *
2163  JEMALLOC_ATTR(malloc)
2164  je_valloc(size_t size) {
2165  	void *ret;
2166  	static_opts_t sopts;
2167  	dynamic_opts_t dopts;
2168  	LOG(&quot;core.valloc.entry&quot;, &quot;size: %zu\n&quot;, size);
2169  	static_opts_init(&amp;sopts);
2170  	dynamic_opts_init(&amp;dopts);
2171  	sopts.null_out_result_on_error = true;
2172  	sopts.min_alignment = PAGE;
2173  	sopts.oom_string =
2174  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: out of memory\n&quot;;
2175  	sopts.invalid_alignment_string =
2176  	    &quot;&lt;jemalloc&gt;: Error allocating aligned memory: invalid alignment\n&quot;;
2177  	dopts.result = &amp;ret;
2178  	dopts.num_items = 1;
2179  	dopts.item_size = size;
2180  	dopts.alignment = PAGE;
2181  	imalloc(&amp;sopts, &amp;dopts);
2182  	if (sopts.slow) {
2183  		uintptr_t args[3] = {size};
2184  		hook_invoke_alloc(hook_alloc_valloc, ret, (uintptr_t)ret, args);
2185  	}
2186  	LOG(&quot;core.valloc.exit&quot;, &quot;result: %p\n&quot;, ret);
2187  	return ret;
2188  }
2189  #endif
2190  #if defined(JEMALLOC_IS_MALLOC) &amp;&amp; defined(JEMALLOC_GLIBC_MALLOC_HOOK)
2191  JEMALLOC_EXPORT void (*__free_hook)(void *ptr) = je_free;
2192  JEMALLOC_EXPORT void *(*__malloc_hook)(size_t size) = je_malloc;
2193  JEMALLOC_EXPORT void *(*__realloc_hook)(void *ptr, size_t size) = je_realloc;
2194  #  ifdef JEMALLOC_GLIBC_MEMALIGN_HOOK
2195  JEMALLOC_EXPORT void *(*__memalign_hook)(size_t alignment, size_t size) =
2196      je_memalign;
2197  #  endif
2198  #  ifdef CPU_COUNT
2199  #    define ALIAS(je_fn)	__attribute__((alias (#je_fn), used))
2200  #    define PREALIAS(je_fn)	ALIAS(je_fn)
2201  #    ifdef JEMALLOC_OVERRIDE___LIBC_CALLOC
2202  void *__libc_calloc(size_t n, size_t size) PREALIAS(je_calloc);
2203  #    endif
2204  #    ifdef JEMALLOC_OVERRIDE___LIBC_FREE
2205  void __libc_free(void* ptr) PREALIAS(je_free);
2206  #    endif
2207  #    ifdef JEMALLOC_OVERRIDE___LIBC_MALLOC
2208  void *__libc_malloc(size_t size) PREALIAS(je_malloc);
2209  #    endif
2210  #    ifdef JEMALLOC_OVERRIDE___LIBC_MEMALIGN
2211  void *__libc_memalign(size_t align, size_t s) PREALIAS(je_memalign);
2212  #    endif
2213  #    ifdef JEMALLOC_OVERRIDE___LIBC_REALLOC
2214  void *__libc_realloc(void* ptr, size_t size) PREALIAS(je_realloc);
2215  #    endif
2216  #    ifdef JEMALLOC_OVERRIDE___LIBC_VALLOC
2217  void *__libc_valloc(size_t size) PREALIAS(je_valloc);
2218  #    endif
2219  #    ifdef JEMALLOC_OVERRIDE___POSIX_MEMALIGN
2220  int __posix_memalign(void** r, size_t a, size_t s) PREALIAS(je_posix_memalign);
2221  #    endif
2222  #    undef PREALIAS
2223  #    undef ALIAS
2224  #  endif
2225  #endif
2226  #ifdef JEMALLOC_EXPERIMENTAL_SMALLOCX_API
2227  #define JEMALLOC_SMALLOCX_CONCAT_HELPER(x, y) x ## y
2228  #define JEMALLOC_SMALLOCX_CONCAT_HELPER2(x, y)  \
2229    JEMALLOC_SMALLOCX_CONCAT_HELPER(x, y)
2230  typedef struct {
2231  	void *ptr;
2232  	size_t size;
2233  } smallocx_return_t;
2234  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2235  smallocx_return_t JEMALLOC_NOTHROW
2236  JEMALLOC_SMALLOCX_CONCAT_HELPER2(je_smallocx_, JEMALLOC_VERSION_GID_IDENT)
2237    (size_t size, int flags) {
2238  	smallocx_return_t ret;
2239  	static_opts_t sopts;
2240  	dynamic_opts_t dopts;
2241  	LOG(&quot;core.smallocx.entry&quot;, &quot;size: %zu, flags: %d&quot;, size, flags);
2242  	static_opts_init(&amp;sopts);
2243  	dynamic_opts_init(&amp;dopts);
2244  	sopts.assert_nonempty_alloc = true;
2245  	sopts.null_out_result_on_error = true;
2246  	sopts.oom_string = &quot;&lt;jemalloc&gt;: Error in mallocx(): out of memory\n&quot;;
2247  	sopts.usize = true;
2248  	dopts.result = &amp;ret.ptr;
2249  	dopts.num_items = 1;
2250  	dopts.item_size = size;
2251  	if (unlikely(flags != 0)) {
2252  		if ((flags &amp; MALLOCX_LG_ALIGN_MASK) != 0) {
2253  			dopts.alignment = MALLOCX_ALIGN_GET_SPECIFIED(flags);
2254  		}
2255  		dopts.zero = MALLOCX_ZERO_GET(flags);
2256  		if ((flags &amp; MALLOCX_TCACHE_MASK) != 0) {
2257  			if ((flags &amp; MALLOCX_TCACHE_MASK)
2258  			    == MALLOCX_TCACHE_NONE) {
2259  				dopts.tcache_ind = TCACHE_IND_NONE;
2260  			} else {
2261  				dopts.tcache_ind = MALLOCX_TCACHE_GET(flags);
2262  			}
2263  		} else {
2264  			dopts.tcache_ind = TCACHE_IND_AUTOMATIC;
2265  		}
2266  		if ((flags &amp; MALLOCX_ARENA_MASK) != 0)
2267  			dopts.arena_ind = MALLOCX_ARENA_GET(flags);
2268  	}
2269  	imalloc(&amp;sopts, &amp;dopts);
2270  	assert(dopts.usize == je_nallocx(size, flags));
2271  	ret.size = dopts.usize;
2272  	LOG(&quot;core.smallocx.exit&quot;, &quot;result: %p, size: %zu&quot;, ret.ptr, ret.size);
2273  	return ret;
2274  }
2275  #undef JEMALLOC_SMALLOCX_CONCAT_HELPER
2276  #undef JEMALLOC_SMALLOCX_CONCAT_HELPER2
2277  #endif
2278  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2279  void JEMALLOC_NOTHROW *
2280  JEMALLOC_ATTR(malloc) JEMALLOC_ALLOC_SIZE(1)
2281  je_mallocx(size_t size, int flags) {
2282  	void *ret;
2283  	static_opts_t sopts;
2284  	dynamic_opts_t dopts;
2285  	LOG(&quot;core.mallocx.entry&quot;, &quot;size: %zu, flags: %d&quot;, size, flags);
2286  	static_opts_init(&amp;sopts);
2287  	dynamic_opts_init(&amp;dopts);
2288  	sopts.assert_nonempty_alloc = true;
2289  	sopts.null_out_result_on_error = true;
2290  	sopts.oom_string = &quot;&lt;jemalloc&gt;: Error in mallocx(): out of memory\n&quot;;
2291  	dopts.result = &amp;ret;
2292  	dopts.num_items = 1;
2293  	dopts.item_size = size;
2294  	if (unlikely(flags != 0)) {
2295  		if ((flags &amp; MALLOCX_LG_ALIGN_MASK) != 0) {
2296  			dopts.alignment = MALLOCX_ALIGN_GET_SPECIFIED(flags);
2297  		}
2298  		dopts.zero = MALLOCX_ZERO_GET(flags);
2299  		if ((flags &amp; MALLOCX_TCACHE_MASK) != 0) {
2300  			if ((flags &amp; MALLOCX_TCACHE_MASK)
2301  			    == MALLOCX_TCACHE_NONE) {
2302  				dopts.tcache_ind = TCACHE_IND_NONE;
2303  			} else {
2304  				dopts.tcache_ind = MALLOCX_TCACHE_GET(flags);
2305  			}
2306  		} else {
2307  			dopts.tcache_ind = TCACHE_IND_AUTOMATIC;
2308  		}
2309  		if ((flags &amp; MALLOCX_ARENA_MASK) != 0)
2310  			dopts.arena_ind = MALLOCX_ARENA_GET(flags);
2311  	}
2312  	imalloc(&amp;sopts, &amp;dopts);
2313  	if (sopts.slow) {
2314  		uintptr_t args[3] = {size, flags};
2315  		hook_invoke_alloc(hook_alloc_mallocx, ret, (uintptr_t)ret,
2316  		    args);
2317  	}
2318  	LOG(&quot;core.mallocx.exit&quot;, &quot;result: %p&quot;, ret);
2319  	return ret;
2320  }
2321  static void *
2322  irallocx_prof_sample(tsdn_t *tsdn, void *old_ptr, size_t old_usize,
2323      size_t usize, size_t alignment, bool zero, tcache_t *tcache, arena_t *arena,
2324      prof_tctx_t *tctx, hook_ralloc_args_t *hook_args) {
2325  	void *p;
2326  	if (tctx == NULL) {
2327  		return NULL;
2328  	}
2329  	if (usize &lt;= SC_SMALL_MAXCLASS) {
2330  		p = iralloct(tsdn, old_ptr, old_usize,
2331  		    SC_LARGE_MINCLASS, alignment, zero, tcache,
2332  		    arena, hook_args);
2333  		if (p == NULL) {
2334  			return NULL;
2335  		}
2336  		arena_prof_promote(tsdn, p, usize);
2337  	} else {
2338  		p = iralloct(tsdn, old_ptr, old_usize, usize, alignment, zero,
2339  		    tcache, arena, hook_args);
2340  	}
2341  	return p;
2342  }
2343  JEMALLOC_ALWAYS_INLINE void *
2344  irallocx_prof(tsd_t *tsd, void *old_ptr, size_t old_usize, size_t size,
2345      size_t alignment, size_t *usize, bool zero, tcache_t *tcache,
2346      arena_t *arena, alloc_ctx_t *alloc_ctx, hook_ralloc_args_t *hook_args) {
2347  	void *p;
2348  	bool prof_active;
2349  	prof_tctx_t *old_tctx, *tctx;
2350  	prof_active = prof_active_get_unlocked();
2351  	old_tctx = prof_tctx_get(tsd_tsdn(tsd), old_ptr, alloc_ctx);
2352  	tctx = prof_alloc_prep(tsd, *usize, prof_active, false);
2353  	if (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {
2354  		p = irallocx_prof_sample(tsd_tsdn(tsd), old_ptr, old_usize,
2355  		    *usize, alignment, zero, tcache, arena, tctx, hook_args);
2356  	} else {
2357  		p = iralloct(tsd_tsdn(tsd), old_ptr, old_usize, size, alignment,
2358  		    zero, tcache, arena, hook_args);
2359  	}
2360  	if (unlikely(p == NULL)) {
2361  		prof_alloc_rollback(tsd, tctx, false);
2362  		return NULL;
2363  	}
2364  	if (p == old_ptr &amp;&amp; alignment != 0) {
2365  		*usize = isalloc(tsd_tsdn(tsd), p);
2366  	}
2367  	prof_realloc(tsd, p, *usize, tctx, prof_active, false, old_ptr,
2368  	    old_usize, old_tctx);
2369  	return p;
2370  }
2371  JEMALLOC_EXPORT JEMALLOC_ALLOCATOR JEMALLOC_RESTRICT_RETURN
2372  void JEMALLOC_NOTHROW *
2373  JEMALLOC_ALLOC_SIZE(2)
2374  je_rallocx(void *ptr, size_t size, int flags) {
2375  	void *p;
2376  	tsd_t *tsd;
2377  	size_t usize;
2378  	size_t old_usize;
2379  	size_t alignment = MALLOCX_ALIGN_GET(flags);
2380  	bool zero = flags &amp; MALLOCX_ZERO;
2381  	arena_t *arena;
2382  	tcache_t *tcache;
2383  	LOG(&quot;core.rallocx.entry&quot;, &quot;ptr: %p, size: %zu, flags: %d&quot;, ptr,
2384  	    size, flags);
2385  	assert(ptr != NULL);
2386  	assert(size != 0);
2387  	assert(malloc_initialized() || IS_INITIALIZER);
2388  	tsd = tsd_fetch();
2389  	check_entry_exit_locking(tsd_tsdn(tsd));
2390  	if (unlikely((flags &amp; MALLOCX_ARENA_MASK) != 0)) {
2391  		unsigned arena_ind = MALLOCX_ARENA_GET(flags);
2392  		arena = arena_get(tsd_tsdn(tsd), arena_ind, true);
2393  		if (unlikely(arena == NULL)) {
2394  			goto label_oom;
2395  		}
2396  	} else {
2397  		arena = NULL;
2398  	}
2399  	if (unlikely((flags &amp; MALLOCX_TCACHE_MASK) != 0)) {
2400  		if ((flags &amp; MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
2401  			tcache = NULL;
2402  		} else {
2403  			tcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));
2404  		}
2405  	} else {
2406  		tcache = tcache_get(tsd);
2407  	}
2408  	alloc_ctx_t alloc_ctx;
2409  	rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
2410  	rtree_szind_slab_read(tsd_tsdn(tsd), &amp;extents_rtree, rtree_ctx,
2411  	    (uintptr_t)ptr, true, &amp;alloc_ctx.szind, &amp;alloc_ctx.slab);
2412  	assert(alloc_ctx.szind != SC_NSIZES);
2413  	old_usize = sz_index2size(alloc_ctx.szind);
2414  	assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
2415  	hook_ralloc_args_t hook_args = {false, {(uintptr_t)ptr, size, flags,
2416  		0}};
2417  	if (config_prof &amp;&amp; opt_prof) {
2418  		usize = (alignment == 0) ?
2419  		    sz_s2u(size) : sz_sa2u(size, alignment);
2420  		if (unlikely(usize == 0
2421  		    || usize &gt; SC_LARGE_MAXCLASS)) {
2422  			goto label_oom;
2423  		}
2424  		p = irallocx_prof(tsd, ptr, old_usize, size, alignment, &amp;usize,
2425  		    zero, tcache, arena, &amp;alloc_ctx, &amp;hook_args);
2426  		if (unlikely(p == NULL)) {
2427  			goto label_oom;
2428  		}
2429  	} else {
2430  		p = iralloct(tsd_tsdn(tsd), ptr, old_usize, size, alignment,
2431  		    zero, tcache, arena, &amp;hook_args);
2432  		if (unlikely(p == NULL)) {
2433  			goto label_oom;
2434  		}
2435  		if (config_stats) {
2436  			usize = isalloc(tsd_tsdn(tsd), p);
2437  		}
2438  	}
2439  	assert(alignment == 0 || ((uintptr_t)p &amp; (alignment - 1)) == ZU(0));
2440  	if (config_stats) {
2441  		*tsd_thread_allocatedp_get(tsd) += usize;
2442  		*tsd_thread_deallocatedp_get(tsd) += old_usize;
2443  	}
2444  	UTRACE(ptr, size, p);
2445  	check_entry_exit_locking(tsd_tsdn(tsd));
2446  	LOG(&quot;core.rallocx.exit&quot;, &quot;result: %p&quot;, p);
2447  	return p;
2448  label_oom:
2449  	if (config_xmalloc &amp;&amp; unlikely(opt_xmalloc)) {
2450  		malloc_write(&quot;&lt;jemalloc&gt;: Error in rallocx(): out of memory\n&quot;);
2451  		abort();
2452  	}
2453  	UTRACE(ptr, size, 0);
2454  	check_entry_exit_locking(tsd_tsdn(tsd));
2455  	LOG(&quot;core.rallocx.exit&quot;, &quot;result: %p&quot;, NULL);
2456  	return NULL;
2457  }
2458  JEMALLOC_ALWAYS_INLINE size_t
2459  ixallocx_helper(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,
2460      size_t extra, size_t alignment, bool zero) {
2461  	size_t newsize;
2462  	if (ixalloc(tsdn, ptr, old_usize, size, extra, alignment, zero,
2463  	    &amp;newsize)) {
2464  		return old_usize;
2465  	}
2466  	return newsize;
2467  }
2468  static size_t
2469  ixallocx_prof_sample(tsdn_t *tsdn, void *ptr, size_t old_usize, size_t size,
2470      size_t extra, size_t alignment, bool zero, prof_tctx_t *tctx) {
2471  	size_t usize;
2472  	if (tctx == NULL) {
2473  		return old_usize;
2474  	}
2475  	usize = ixallocx_helper(tsdn, ptr, old_usize, size, extra, alignment,
2476  	    zero);
2477  	return usize;
2478  }
2479  JEMALLOC_ALWAYS_INLINE size_t
2480  ixallocx_prof(tsd_t *tsd, void *ptr, size_t old_usize, size_t size,
2481      size_t extra, size_t alignment, bool zero, alloc_ctx_t *alloc_ctx) {
2482  	size_t usize_max, usize;
2483  	bool prof_active;
2484  	prof_tctx_t *old_tctx, *tctx;
2485  	prof_active = prof_active_get_unlocked();
2486  	old_tctx = prof_tctx_get(tsd_tsdn(tsd), ptr, alloc_ctx);
2487  	if (alignment == 0) {
2488  		usize_max = sz_s2u(size+extra);
2489  		assert(usize_max &gt; 0
2490  		    &amp;&amp; usize_max &lt;= SC_LARGE_MAXCLASS);
2491  	} else {
2492  		usize_max = sz_sa2u(size+extra, alignment);
2493  		if (unlikely(usize_max == 0
2494  		    || usize_max &gt; SC_LARGE_MAXCLASS)) {
2495  			usize_max = SC_LARGE_MAXCLASS;
2496  		}
2497  	}
2498  	tctx = prof_alloc_prep(tsd, usize_max, prof_active, false);
2499  	if (unlikely((uintptr_t)tctx != (uintptr_t)1U)) {
2500  		usize = ixallocx_prof_sample(tsd_tsdn(tsd), ptr, old_usize,
2501  		    size, extra, alignment, zero, tctx);
2502  	} else {
2503  		usize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,
2504  		    extra, alignment, zero);
2505  	}
2506  	if (usize == old_usize) {
2507  		prof_alloc_rollback(tsd, tctx, false);
2508  		return usize;
2509  	}
2510  	prof_realloc(tsd, ptr, usize, tctx, prof_active, false, ptr, old_usize,
2511  	    old_tctx);
2512  	return usize;
2513  }
2514  JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
2515  je_xallocx(void *ptr, size_t size, size_t extra, int flags) {
2516  	tsd_t *tsd;
2517  	size_t usize, old_usize;
2518  	size_t alignment = MALLOCX_ALIGN_GET(flags);
2519  	bool zero = flags &amp; MALLOCX_ZERO;
2520  	LOG(&quot;core.xallocx.entry&quot;, &quot;ptr: %p, size: %zu, extra: %zu, &quot;
2521  	    &quot;flags: %d&quot;, ptr, size, extra, flags);
2522  	assert(ptr != NULL);
2523  	assert(size != 0);
2524  	assert(SIZE_T_MAX - size &gt;= extra);
2525  	assert(malloc_initialized() || IS_INITIALIZER);
2526  	tsd = tsd_fetch();
2527  	check_entry_exit_locking(tsd_tsdn(tsd));
2528  	alloc_ctx_t alloc_ctx;
2529  	rtree_ctx_t *rtree_ctx = tsd_rtree_ctx(tsd);
2530  	rtree_szind_slab_read(tsd_tsdn(tsd), &amp;extents_rtree, rtree_ctx,
2531  	    (uintptr_t)ptr, true, &amp;alloc_ctx.szind, &amp;alloc_ctx.slab);
2532  	assert(alloc_ctx.szind != SC_NSIZES);
2533  	old_usize = sz_index2size(alloc_ctx.szind);
2534  	assert(old_usize == isalloc(tsd_tsdn(tsd), ptr));
2535  	if (unlikely(size &gt; SC_LARGE_MAXCLASS)) {
2536  		usize = old_usize;
2537  		goto label_not_resized;
2538  	}
2539  	if (unlikely(SC_LARGE_MAXCLASS - size &lt; extra)) {
2540  		extra = SC_LARGE_MAXCLASS - size;
2541  	}
2542  	if (config_prof &amp;&amp; opt_prof) {
2543  		usize = ixallocx_prof(tsd, ptr, old_usize, size, extra,
2544  		    alignment, zero, &amp;alloc_ctx);
2545  	} else {
2546  		usize = ixallocx_helper(tsd_tsdn(tsd), ptr, old_usize, size,
2547  		    extra, alignment, zero);
2548  	}
2549  	if (unlikely(usize == old_usize)) {
2550  		goto label_not_resized;
2551  	}
2552  	if (config_stats) {
2553  		*tsd_thread_allocatedp_get(tsd) += usize;
2554  		*tsd_thread_deallocatedp_get(tsd) += old_usize;
2555  	}
2556  label_not_resized:
2557  	if (unlikely(!tsd_fast(tsd))) {
2558  		uintptr_t args[4] = {(uintptr_t)ptr, size, extra, flags};
2559  		hook_invoke_expand(hook_expand_xallocx, ptr, old_usize,
2560  		    usize, (uintptr_t)usize, args);
2561  	}
2562  	UTRACE(ptr, size, ptr);
2563  	check_entry_exit_locking(tsd_tsdn(tsd));
2564  	LOG(&quot;core.xallocx.exit&quot;, &quot;result: %zu&quot;, usize);
2565  	return usize;
2566  }
2567  JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
2568  JEMALLOC_ATTR(pure)
2569  je_sallocx(const void *ptr, int flags) {
2570  	size_t usize;
2571  	tsdn_t *tsdn;
2572  	LOG(&quot;core.sallocx.entry&quot;, &quot;ptr: %p, flags: %d&quot;, ptr, flags);
2573  	assert(malloc_initialized() || IS_INITIALIZER);
2574  	assert(ptr != NULL);
2575  	tsdn = tsdn_fetch();
2576  	check_entry_exit_locking(tsdn);
2577  	if (config_debug || force_ivsalloc) {
2578  		usize = ivsalloc(tsdn, ptr);
2579  		assert(force_ivsalloc || usize != 0);
2580  	} else {
2581  		usize = isalloc(tsdn, ptr);
2582  	}
2583  	check_entry_exit_locking(tsdn);
2584  	LOG(&quot;core.sallocx.exit&quot;, &quot;result: %zu&quot;, usize);
2585  	return usize;
2586  }
2587  JEMALLOC_EXPORT void JEMALLOC_NOTHROW
2588  je_dallocx(void *ptr, int flags) {
2589  	LOG(&quot;core.dallocx.entry&quot;, &quot;ptr: %p, flags: %d&quot;, ptr, flags);
2590  	assert(ptr != NULL);
2591  	assert(malloc_initialized() || IS_INITIALIZER);
2592  	tsd_t *tsd = tsd_fetch();
2593  	bool fast = tsd_fast(tsd);
2594  	check_entry_exit_locking(tsd_tsdn(tsd));
2595  	tcache_t *tcache;
2596  	if (unlikely((flags &amp; MALLOCX_TCACHE_MASK) != 0)) {
2597  		assert(tsd_reentrancy_level_get(tsd) == 0);
2598  		if ((flags &amp; MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
2599  			tcache = NULL;
2600  		} else {
2601  			tcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));
2602  		}
2603  	} else {
2604  		if (likely(fast)) {
2605  			tcache = tsd_tcachep_get(tsd);
2606  			assert(tcache == tcache_get(tsd));
2607  		} else {
2608  			if (likely(tsd_reentrancy_level_get(tsd) == 0)) {
2609  				tcache = tcache_get(tsd);
2610  			}  else {
2611  				tcache = NULL;
2612  			}
2613  		}
2614  	}
2615  	UTRACE(ptr, 0, 0);
2616  	if (likely(fast)) {
2617  		tsd_assert_fast(tsd);
2618  		ifree(tsd, ptr, tcache, false);
2619  	} else {
2620  		uintptr_t args_raw[3] = {(uintptr_t)ptr, flags};
2621  		hook_invoke_dalloc(hook_dalloc_dallocx, ptr, args_raw);
2622  		ifree(tsd, ptr, tcache, true);
2623  	}
2624  	check_entry_exit_locking(tsd_tsdn(tsd));
2625  	LOG(&quot;core.dallocx.exit&quot;, &quot;&quot;);
2626  }
2627  JEMALLOC_ALWAYS_INLINE size_t
2628  inallocx(tsdn_t *tsdn, size_t size, int flags) {
2629  	check_entry_exit_locking(tsdn);
2630  	size_t usize;
2631  	if (likely((flags &amp; MALLOCX_LG_ALIGN_MASK) == 0)) {
2632  		usize = sz_s2u(size);
2633  	} else {
2634  		usize = sz_sa2u(size, MALLOCX_ALIGN_GET_SPECIFIED(flags));
2635  	}
2636  	check_entry_exit_locking(tsdn);
2637  	return usize;
2638  }
2639  JEMALLOC_NOINLINE void
2640  sdallocx_default(void *ptr, size_t size, int flags) {
2641  	assert(ptr != NULL);
2642  	assert(malloc_initialized() || IS_INITIALIZER);
2643  	tsd_t *tsd = tsd_fetch();
2644  	bool fast = tsd_fast(tsd);
2645  	size_t usize = inallocx(tsd_tsdn(tsd), size, flags);
2646  	assert(usize == isalloc(tsd_tsdn(tsd), ptr));
2647  	check_entry_exit_locking(tsd_tsdn(tsd));
2648  	tcache_t *tcache;
2649  	if (unlikely((flags &amp; MALLOCX_TCACHE_MASK) != 0)) {
2650  		assert(tsd_reentrancy_level_get(tsd) == 0);
2651  		if ((flags &amp; MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
2652  			tcache = NULL;
2653  		} else {
2654  			tcache = tcaches_get(tsd, MALLOCX_TCACHE_GET(flags));
2655  		}
2656  	} else {
2657  		if (likely(fast)) {
2658  			tcache = tsd_tcachep_get(tsd);
2659  			assert(tcache == tcache_get(tsd));
2660  		} else {
2661  			if (likely(tsd_reentrancy_level_get(tsd) == 0)) {
2662  				tcache = tcache_get(tsd);
2663  			} else {
2664  				tcache = NULL;
2665  			}
2666  		}
2667  	}
2668  	UTRACE(ptr, 0, 0);
2669  	if (likely(fast)) {
2670  		tsd_assert_fast(tsd);
2671  		isfree(tsd, ptr, usize, tcache, false);
2672  	} else {
2673  		uintptr_t args_raw[3] = {(uintptr_t)ptr, size, flags};
2674  		hook_invoke_dalloc(hook_dalloc_sdallocx, ptr, args_raw);
2675  		isfree(tsd, ptr, usize, tcache, true);
2676  	}
2677  	check_entry_exit_locking(tsd_tsdn(tsd));
2678  }
2679  JEMALLOC_EXPORT void JEMALLOC_NOTHROW
2680  je_sdallocx(void *ptr, size_t size, int flags) {
2681  	LOG(&quot;core.sdallocx.entry&quot;, &quot;ptr: %p, size: %zu, flags: %d&quot;, ptr,
2682  		size, flags);
2683  	if (flags !=0 || !free_fastpath(ptr, size, true)) {
2684  		sdallocx_default(ptr, size, flags);
2685  	}
2686  	LOG(&quot;core.sdallocx.exit&quot;, &quot;&quot;);
2687  }
2688  void JEMALLOC_NOTHROW
2689  je_sdallocx_noflags(void *ptr, size_t size) {
2690  	LOG(&quot;core.sdallocx.entry&quot;, &quot;ptr: %p, size: %zu, flags: 0&quot;, ptr,
2691  		size);
2692  	if (!free_fastpath(ptr, size, true)) {
2693  		sdallocx_default(ptr, size, 0);
2694  	}
2695  	LOG(&quot;core.sdallocx.exit&quot;, &quot;&quot;);
2696  }
2697  JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
2698  JEMALLOC_ATTR(pure)
2699  je_nallocx(size_t size, int flags) {
2700  	size_t usize;
2701  	tsdn_t *tsdn;
2702  	assert(size != 0);
2703  	if (unlikely(malloc_init())) {
2704  		LOG(&quot;core.nallocx.exit&quot;, &quot;result: %zu&quot;, ZU(0));
2705  		return 0;
2706  	}
2707  	tsdn = tsdn_fetch();
2708  	check_entry_exit_locking(tsdn);
2709  	usize = inallocx(tsdn, size, flags);
2710  	if (unlikely(usize &gt; SC_LARGE_MAXCLASS)) {
2711  		LOG(&quot;core.nallocx.exit&quot;, &quot;result: %zu&quot;, ZU(0));
2712  		return 0;
2713  	}
2714  	check_entry_exit_locking(tsdn);
2715  	LOG(&quot;core.nallocx.exit&quot;, &quot;result: %zu&quot;, usize);
2716  	return usize;
2717  }
2718  JEMALLOC_EXPORT int JEMALLOC_NOTHROW
2719  je_mallctl(const char *name, void *oldp, size_t *oldlenp, void *newp,
2720      size_t newlen) {
2721  	int ret;
2722  	tsd_t *tsd;
2723  	LOG(&quot;core.mallctl.entry&quot;, &quot;name: %s&quot;, name);
2724  	if (unlikely(malloc_init())) {
2725  		LOG(&quot;core.mallctl.exit&quot;, &quot;result: %d&quot;, EAGAIN);
2726  		return EAGAIN;
2727  	}
2728  	tsd = tsd_fetch();
2729  	check_entry_exit_locking(tsd_tsdn(tsd));
2730  	ret = ctl_byname(tsd, name, oldp, oldlenp, newp, newlen);
2731  	check_entry_exit_locking(tsd_tsdn(tsd));
2732  	LOG(&quot;core.mallctl.exit&quot;, &quot;result: %d&quot;, ret);
2733  	return ret;
2734  }
2735  JEMALLOC_EXPORT int JEMALLOC_NOTHROW
2736  je_mallctlnametomib(const char *name, size_t *mibp, size_t *miblenp) {
2737  	int ret;
2738  	LOG(&quot;core.mallctlnametomib.entry&quot;, &quot;name: %s&quot;, name);
2739  	if (unlikely(malloc_init())) {
2740  		LOG(&quot;core.mallctlnametomib.exit&quot;, &quot;result: %d&quot;, EAGAIN);
2741  		return EAGAIN;
2742  	}
2743  	tsd_t *tsd = tsd_fetch();
2744  	check_entry_exit_locking(tsd_tsdn(tsd));
2745  	ret = ctl_nametomib(tsd, name, mibp, miblenp);
2746  	check_entry_exit_locking(tsd_tsdn(tsd));
2747  	LOG(&quot;core.mallctlnametomib.exit&quot;, &quot;result: %d&quot;, ret);
2748  	return ret;
2749  }
2750  JEMALLOC_EXPORT int JEMALLOC_NOTHROW
2751  je_mallctlbymib(const size_t *mib, size_t miblen, void *oldp, size_t *oldlenp,
2752    void *newp, size_t newlen) {
2753  	int ret;
2754  	tsd_t *tsd;
2755  	LOG(&quot;core.mallctlbymib.entry&quot;, &quot;&quot;);
2756  	if (unlikely(malloc_init())) {
2757  		LOG(&quot;core.mallctlbymib.exit&quot;, &quot;result: %d&quot;, EAGAIN);
2758  		return EAGAIN;
2759  	}
2760  	tsd = tsd_fetch();
2761  	check_entry_exit_locking(tsd_tsdn(tsd));
2762  	ret = ctl_bymib(tsd, mib, miblen, oldp, oldlenp, newp, newlen);
2763  	check_entry_exit_locking(tsd_tsdn(tsd));
2764  	LOG(&quot;core.mallctlbymib.exit&quot;, &quot;result: %d&quot;, ret);
2765  	return ret;
2766  }
2767  JEMALLOC_EXPORT void JEMALLOC_NOTHROW
2768  je_malloc_stats_print(void (*write_cb)(void *, const char *), void *cbopaque,
2769      const char *opts) {
2770  	tsdn_t *tsdn;
2771  	LOG(&quot;core.malloc_stats_print.entry&quot;, &quot;&quot;);
2772  	tsdn = tsdn_fetch();
2773  	check_entry_exit_locking(tsdn);
2774  	stats_print(write_cb, cbopaque, opts);
2775  	check_entry_exit_locking(tsdn);
2776  	LOG(&quot;core.malloc_stats_print.exit&quot;, &quot;&quot;);
2777  }
2778  JEMALLOC_EXPORT size_t JEMALLOC_NOTHROW
2779  je_malloc_usable_size(JEMALLOC_USABLE_SIZE_CONST void *ptr) {
2780  	size_t ret;
2781  	tsdn_t *tsdn;
2782  	LOG(&quot;core.malloc_usable_size.entry&quot;, &quot;ptr: %p&quot;, ptr);
2783  	assert(malloc_initialized() || IS_INITIALIZER);
2784  	tsdn = tsdn_fetch();
2785  	check_entry_exit_locking(tsdn);
2786  	if (unlikely(ptr == NULL)) {
2787  		ret = 0;
2788  	} else {
2789  		if (config_debug || force_ivsalloc) {
2790  			ret = ivsalloc(tsdn, ptr);
2791  			assert(force_ivsalloc || ret != 0);
2792  		} else {
2793  			ret = isalloc(tsdn, ptr);
2794  		}
2795  	}
2796  	check_entry_exit_locking(tsdn);
2797  	LOG(&quot;core.malloc_usable_size.exit&quot;, &quot;result: %zu&quot;, ret);
2798  	return ret;
2799  }
2800  #ifndef JEMALLOC_JET
2801  JEMALLOC_ATTR(constructor)
2802  static void
2803  jemalloc_constructor(void) {
2804  	malloc_init();
2805  }
2806  #endif
2807  #ifndef JEMALLOC_MUTEX_INIT_CB
2808  void
2809  jemalloc_prefork(void)
2810  #else
2811  JEMALLOC_EXPORT void
2812  _malloc_prefork(void)
2813  #endif
2814  {
2815  	tsd_t *tsd;
2816  	unsigned i, j, narenas;
2817  	arena_t *arena;
2818  #ifdef JEMALLOC_MUTEX_INIT_CB
2819  	if (!malloc_initialized()) {
2820  		return;
2821  	}
2822  #endif
2823  	assert(malloc_initialized());
2824  	tsd = tsd_fetch();
2825  	narenas = narenas_total_get();
2826  	witness_prefork(tsd_witness_tsdp_get(tsd));
2827  	ctl_prefork(tsd_tsdn(tsd));
2828  	tcache_prefork(tsd_tsdn(tsd));
2829  	malloc_mutex_prefork(tsd_tsdn(tsd), &amp;arenas_lock);
2830  	if (have_background_thread) {
2831  		background_thread_prefork0(tsd_tsdn(tsd));
2832  	}
2833  	prof_prefork0(tsd_tsdn(tsd));
2834  	if (have_background_thread) {
2835  		background_thread_prefork1(tsd_tsdn(tsd));
2836  	}
2837  	for (i = 0; i &lt; 8; i++) {
2838  		for (j = 0; j &lt; narenas; j++) {
2839  			if ((arena = arena_get(tsd_tsdn(tsd), j, false)) !=
2840  			    NULL) {
2841  				switch (i) {
2842  				case 0:
2843  					arena_prefork0(tsd_tsdn(tsd), arena);
2844  					break;
2845  				case 1:
2846  					arena_prefork1(tsd_tsdn(tsd), arena);
2847  					break;
2848  				case 2:
2849  					arena_prefork2(tsd_tsdn(tsd), arena);
2850  					break;
2851  				case 3:
2852  					arena_prefork3(tsd_tsdn(tsd), arena);
2853  					break;
2854  				case 4:
2855  					arena_prefork4(tsd_tsdn(tsd), arena);
2856  					break;
2857  				case 5:
2858  					arena_prefork5(tsd_tsdn(tsd), arena);
2859  					break;
2860  				case 6:
2861  					arena_prefork6(tsd_tsdn(tsd), arena);
2862  					break;
2863  				case 7:
2864  					arena_prefork7(tsd_tsdn(tsd), arena);
2865  					break;
2866  				default: not_reached();
2867  				}
2868  			}
2869  		}
2870  	}
2871  	prof_prefork1(tsd_tsdn(tsd));
2872  	tsd_prefork(tsd);
2873  }
2874  #ifndef JEMALLOC_MUTEX_INIT_CB
2875  void
2876  jemalloc_postfork_parent(void)
2877  #else
2878  JEMALLOC_EXPORT void
2879  _malloc_postfork(void)
2880  #endif
2881  {
2882  	tsd_t *tsd;
2883  	unsigned i, narenas;
2884  #ifdef JEMALLOC_MUTEX_INIT_CB
2885  	if (!malloc_initialized()) {
2886  		return;
2887  	}
2888  #endif
2889  	assert(malloc_initialized());
2890  	tsd = tsd_fetch();
2891  	tsd_postfork_parent(tsd);
2892  	witness_postfork_parent(tsd_witness_tsdp_get(tsd));
2893  	for (i = 0, narenas = narenas_total_get(); i &lt; narenas; i++) {
2894  		arena_t *arena;
2895  		if ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL) {
2896  			arena_postfork_parent(tsd_tsdn(tsd), arena);
2897  		}
2898  	}
2899  	prof_postfork_parent(tsd_tsdn(tsd));
2900  	if (have_background_thread) {
2901  		background_thread_postfork_parent(tsd_tsdn(tsd));
2902  	}
2903  	malloc_mutex_postfork_parent(tsd_tsdn(tsd), &amp;arenas_lock);
2904  	tcache_postfork_parent(tsd_tsdn(tsd));
2905  	ctl_postfork_parent(tsd_tsdn(tsd));
2906  }
2907  void
2908  jemalloc_postfork_child(void) {
2909  	tsd_t *tsd;
2910  	unsigned i, narenas;
2911  	assert(malloc_initialized());
2912  	tsd = tsd_fetch();
2913  	tsd_postfork_child(tsd);
2914  	witness_postfork_child(tsd_witness_tsdp_get(tsd));
2915  	for (i = 0, narenas = narenas_total_get(); i &lt; narenas; i++) {
2916  		arena_t *arena;
2917  		if ((arena = arena_get(tsd_tsdn(tsd), i, false)) != NULL) {
2918  			arena_postfork_child(tsd_tsdn(tsd), arena);
2919  		}
2920  	}
2921  	prof_postfork_child(tsd_tsdn(tsd));
2922  	if (have_background_thread) {
2923  		background_thread_postfork_child(tsd_tsdn(tsd));
2924  	}
2925  	malloc_mutex_postfork_child(tsd_tsdn(tsd), &amp;arenas_lock);
2926  	tcache_postfork_child(tsd_tsdn(tsd));
2927  	ctl_postfork_child(tsd_tsdn(tsd));
2928  }
2929  JEMALLOC_EXPORT int JEMALLOC_NOTHROW
2930  je_get_defrag_hint(void* ptr, int *bin_util, int *run_util) {
2931  	assert(ptr != NULL);
2932  	return iget_defrag_hint(TSDN_NULL, ptr, bin_util, run_util);
2933  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-schema.h</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-jemalloc.c</div>
                </div>
                <div class="column column_space"><pre><code>2183      ValueType error_;
2184      ValueType currentError_;
2185      ValueType missingDependents_;
2186      bool valid_;
2187      unsigned flags_;
2188  #if RAPIDJSON_SCHEMA_VERBOSE
</pre></code></div>
                <div class="column column_space"><pre><code>1425  	size_t num_items;
1426  	size_t item_size;
1427  	size_t alignment;
1428  	bool zero;
1429  	unsigned tcache_ind;
1430  	unsigned arena_ind;
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    