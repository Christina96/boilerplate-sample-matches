
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 30, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_bmw.c</h3>
            <pre><code>1  #include <stddef.h>
2  #include <string.h>
3  #include <limits.h>
4  #ifdef __cplusplus
5  extern "C"{
6  #endif
7  #include "sph_bmw.h"
8  #if SPH_SMALL_FOOTPRINT && !defined SPH_SMALL_FOOTPRINT_BMW
9  #define SPH_SMALL_FOOTPRINT_BMW   1
10  #endif
11  #ifdef _MSC_VER
12  #pragma warning (disable: 4146)
13  #endif
14  #if !defined(__AVX2__)
15  static const sph_u32 IV224[] = {
16  	SPH_C32(0x00010203), SPH_C32(0x04050607),
17  	SPH_C32(0x08090A0B), SPH_C32(0x0C0D0E0F),
18  	SPH_C32(0x10111213), SPH_C32(0x14151617),
19  	SPH_C32(0x18191A1B), SPH_C32(0x1C1D1E1F),
20  	SPH_C32(0x20212223), SPH_C32(0x24252627),
21  	SPH_C32(0x28292A2B), SPH_C32(0x2C2D2E2F),
22  	SPH_C32(0x30313233), SPH_C32(0x34353637),
23  	SPH_C32(0x38393A3B), SPH_C32(0x3C3D3E3F)
24  };
25  static const sph_u32 IV256[] = {
26  	SPH_C32(0x40414243), SPH_C32(0x44454647),
27  	SPH_C32(0x48494A4B), SPH_C32(0x4C4D4E4F),
28  	SPH_C32(0x50515253), SPH_C32(0x54555657),
29  	SPH_C32(0x58595A5B), SPH_C32(0x5C5D5E5F),
30  	SPH_C32(0x60616263), SPH_C32(0x64656667),
31  	SPH_C32(0x68696A6B), SPH_C32(0x6C6D6E6F),
32  	SPH_C32(0x70717273), SPH_C32(0x74757677),
33  	SPH_C32(0x78797A7B), SPH_C32(0x7C7D7E7F)
34  };
35  #endif 
36  #if SPH_64
37  static const sph_u64 IV384[] = {
38  	SPH_C64(0x0001020304050607), SPH_C64(0x08090A0B0C0D0E0F),
39  	SPH_C64(0x1011121314151617), SPH_C64(0x18191A1B1C1D1E1F),
40  	SPH_C64(0x2021222324252627), SPH_C64(0x28292A2B2C2D2E2F),
41  	SPH_C64(0x3031323334353637), SPH_C64(0x38393A3B3C3D3E3F),
42  	SPH_C64(0x4041424344454647), SPH_C64(0x48494A4B4C4D4E4F),
43  	SPH_C64(0x5051525354555657), SPH_C64(0x58595A5B5C5D5E5F),
44  	SPH_C64(0x6061626364656667), SPH_C64(0x68696A6B6C6D6E6F),
45  	SPH_C64(0x7071727374757677), SPH_C64(0x78797A7B7C7D7E7F)
46  };
47  static const sph_u64 IV512[] = {
48  	SPH_C64(0x8081828384858687), SPH_C64(0x88898A8B8C8D8E8F),
49  	SPH_C64(0x9091929394959697), SPH_C64(0x98999A9B9C9D9E9F),
50  	SPH_C64(0xA0A1A2A3A4A5A6A7), SPH_C64(0xA8A9AAABACADAEAF),
51  	SPH_C64(0xB0B1B2B3B4B5B6B7), SPH_C64(0xB8B9BABBBCBDBEBF),
52  	SPH_C64(0xC0C1C2C3C4C5C6C7), SPH_C64(0xC8C9CACBCCCDCECF),
53  	SPH_C64(0xD0D1D2D3D4D5D6D7), SPH_C64(0xD8D9DADBDCDDDEDF),
54  	SPH_C64(0xE0E1E2E3E4E5E6E7), SPH_C64(0xE8E9EAEBECEDEEEF),
55  	SPH_C64(0xF0F1F2F3F4F5F6F7), SPH_C64(0xF8F9FAFBFCFDFEFF)
56  };
57  #endif
58  #define XCAT(x, y)    XCAT_(x, y)
59  #define XCAT_(x, y)   x ## y
60  #define LPAR   (
61  #define I16_16    0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15
62  #define I16_17    1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16
63  #define I16_18    2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17
64  #define I16_19    3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18
65  #define I16_20    4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19
66  #define I16_21    5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20
67  #define I16_22    6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21
68  #define I16_23    7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22
69  #define I16_24    8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23
70  #define I16_25    9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
71  #define I16_26   10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25
72  #define I16_27   11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26
73  #define I16_28   12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
74  #define I16_29   13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28
75  #define I16_30   14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29
76  #define I16_31   15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30
77  #define M16_16    0,  1,  3,  4,  7, 10, 11
78  #define M16_17    1,  2,  4,  5,  8, 11, 12
79  #define M16_18    2,  3,  5,  6,  9, 12, 13
80  #define M16_19    3,  4,  6,  7, 10, 13, 14
81  #define M16_20    4,  5,  7,  8, 11, 14, 15
82  #define M16_21    5,  6,  8,  9, 12, 15, 16
83  #define M16_22    6,  7,  9, 10, 13,  0,  1
84  #define M16_23    7,  8, 10, 11, 14,  1,  2
85  #define M16_24    8,  9, 11, 12, 15,  2,  3
86  #define M16_25    9, 10, 12, 13,  0,  3,  4
87  #define M16_26   10, 11, 13, 14,  1,  4,  5
88  #define M16_27   11, 12, 14, 15,  2,  5,  6
89  #define M16_28   12, 13, 15, 16,  3,  6,  7
90  #define M16_29   13, 14,  0,  1,  4,  7,  8
91  #define M16_30   14, 15,  1,  2,  5,  8,  9
92  #define M16_31   15, 16,  2,  3,  6,  9, 10
93  #if !defined(__AVX2__)
94  #define ss0(x)    (((x) >> 1) ^ SPH_T32((x) << 3) \
95                    ^ SPH_ROTL32(x,  4) ^ SPH_ROTL32(x, 19))
96  #define ss1(x)    (((x) >> 1) ^ SPH_T32((x) << 2) \
97                    ^ SPH_ROTL32(x,  8) ^ SPH_ROTL32(x, 23))
98  #define ss2(x)    (((x) >> 2) ^ SPH_T32((x) << 1) \
99                    ^ SPH_ROTL32(x, 12) ^ SPH_ROTL32(x, 25))
100  #define ss3(x)    (((x) >> 2) ^ SPH_T32((x) << 2) \
101                    ^ SPH_ROTL32(x, 15) ^ SPH_ROTL32(x, 29))
102  #define ss4(x)    (((x) >> 1) ^ (x))
103  #define ss5(x)    (((x) >> 2) ^ (x))
104  #define rs1(x)    SPH_ROTL32(x,  3)
105  #define rs2(x)    SPH_ROTL32(x,  7)
106  #define rs3(x)    SPH_ROTL32(x, 13)
107  #define rs4(x)    SPH_ROTL32(x, 16)
108  #define rs5(x)    SPH_ROTL32(x, 19)
109  #define rs6(x)    SPH_ROTL32(x, 23)
110  #define rs7(x)    SPH_ROTL32(x, 27)
111  #define Ks(j)   SPH_T32((sph_u32)(j) * SPH_C32(0x05555555))
112  #define add_elt_s(mf, hf, j0m, j1m, j3m, j4m, j7m, j10m, j11m, j16) \
113  	(SPH_T32(SPH_ROTL32(mf(j0m), j1m) + SPH_ROTL32(mf(j3m), j4m) \
114  		- SPH_ROTL32(mf(j10m), j11m) + Ks(j16)) ^ hf(j7m))
115  #define expand1s_inner(qf, mf, hf, i16, \
116  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
117  		i9, i10, i11, i12, i13, i14, i15, \
118  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
119  	SPH_T32(ss1(qf(i0)) + ss2(qf(i1)) + ss3(qf(i2)) + ss0(qf(i3)) \
120  		+ ss1(qf(i4)) + ss2(qf(i5)) + ss3(qf(i6)) + ss0(qf(i7)) \
121  		+ ss1(qf(i8)) + ss2(qf(i9)) + ss3(qf(i10)) + ss0(qf(i11)) \
122  		+ ss1(qf(i12)) + ss2(qf(i13)) + ss3(qf(i14)) + ss0(qf(i15)) \
123  		+ add_elt_s(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
124  #define expand1s(qf, mf, hf, i16) \
125  	expand1s_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
126  #define expand1s_(qf, mf, hf, i16, ix, iy) \
127  	expand1s_inner LPAR qf, mf, hf, i16, ix, iy)
128  #define expand2s_inner(qf, mf, hf, i16, \
129  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
130  		i9, i10, i11, i12, i13, i14, i15, \
131  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
132  	SPH_T32(qf(i0) + rs1(qf(i1)) + qf(i2) + rs2(qf(i3)) \
133  		+ qf(i4) + rs3(qf(i5)) + qf(i6) + rs4(qf(i7)) \
134  		+ qf(i8) + rs5(qf(i9)) + qf(i10) + rs6(qf(i11)) \
135  		+ qf(i12) + rs7(qf(i13)) + ss4(qf(i14)) + ss5(qf(i15)) \
136  		+ add_elt_s(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
137  #define expand2s(qf, mf, hf, i16) \
138  	expand2s_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
139  #define expand2s_(qf, mf, hf, i16, ix, iy) \
140  	expand2s_inner LPAR qf, mf, hf, i16, ix, iy)
141  #endif 
142  #if SPH_64
143  #define sb0(x)    (((x) >> 1) ^ SPH_T64((x) << 3) \
144                    ^ SPH_ROTL64(x,  4) ^ SPH_ROTL64(x, 37))
145  #define sb1(x)    (((x) >> 1) ^ SPH_T64((x) << 2) \
146                    ^ SPH_ROTL64(x, 13) ^ SPH_ROTL64(x, 43))
147  #define sb2(x)    (((x) >> 2) ^ SPH_T64((x) << 1) \
148                    ^ SPH_ROTL64(x, 19) ^ SPH_ROTL64(x, 53))
149  #define sb3(x)    (((x) >> 2) ^ SPH_T64((x) << 2) \
150                    ^ SPH_ROTL64(x, 28) ^ SPH_ROTL64(x, 59))
151  #define sb4(x)    (((x) >> 1) ^ (x))
152  #define sb5(x)    (((x) >> 2) ^ (x))
153  #define rb1(x)    SPH_ROTL64(x,  5)
154  #define rb2(x)    SPH_ROTL64(x, 11)
155  #define rb3(x)    SPH_ROTL64(x, 27)
156  #define rb4(x)    SPH_ROTL64(x, 32)
157  #define rb5(x)    SPH_ROTL64(x, 37)
158  #define rb6(x)    SPH_ROTL64(x, 43)
159  #define rb7(x)    SPH_ROTL64(x, 53)
160  #define Kb(j)   SPH_T64((sph_u64)(j) * SPH_C64(0x0555555555555555))
161  #if SPH_SMALL_FOOTPRINT_BMW
162  static const sph_u64 Kb_tab[] = {
163  	Kb(16), Kb(17), Kb(18), Kb(19), Kb(20), Kb(21), Kb(22), Kb(23),
164  	Kb(24), Kb(25), Kb(26), Kb(27), Kb(28), Kb(29), Kb(30), Kb(31)
165  };
166  #define rol_off(mf, j, off) \
167  	SPH_ROTL64(mf(((j) + (off)) & 15), (((j) + (off)) & 15) + 1)
168  #define add_elt_b(mf, hf, j) \
169  	(SPH_T64(rol_off(mf, j, 0) + rol_off(mf, j, 3) \
170  		- rol_off(mf, j, 10) + Kb_tab[j]) ^ hf(((j) + 7) & 15))
171  #define expand1b(qf, mf, hf, i) \
172  	SPH_T64(sb1(qf((i) - 16)) + sb2(qf((i) - 15)) \
173  		+ sb3(qf((i) - 14)) + sb0(qf((i) - 13)) \
174  		+ sb1(qf((i) - 12)) + sb2(qf((i) - 11)) \
175  		+ sb3(qf((i) - 10)) + sb0(qf((i) - 9)) \
176  		+ sb1(qf((i) - 8)) + sb2(qf((i) - 7)) \
177  		+ sb3(qf((i) - 6)) + sb0(qf((i) - 5)) \
178  		+ sb1(qf((i) - 4)) + sb2(qf((i) - 3)) \
179  		+ sb3(qf((i) - 2)) + sb0(qf((i) - 1)) \
180  		+ add_elt_b(mf, hf, (i) - 16))
181  #define expand2b(qf, mf, hf, i) \
182  	SPH_T64(qf((i) - 16) + rb1(qf((i) - 15)) \
183  		+ qf((i) - 14) + rb2(qf((i) - 13)) \
184  		+ qf((i) - 12) + rb3(qf((i) - 11)) \
185  		+ qf((i) - 10) + rb4(qf((i) - 9)) \
186  		+ qf((i) - 8) + rb5(qf((i) - 7)) \
187  		+ qf((i) - 6) + rb6(qf((i) - 5)) \
188  		+ qf((i) - 4) + rb7(qf((i) - 3)) \
189  		+ sb4(qf((i) - 2)) + sb5(qf((i) - 1)) \
190  		+ add_elt_b(mf, hf, (i) - 16))
191  #else
192  #define add_elt_b(mf, hf, j0m, j1m, j3m, j4m, j7m, j10m, j11m, j16) \
193  	(SPH_T64(SPH_ROTL64(mf(j0m), j1m) + SPH_ROTL64(mf(j3m), j4m) \
194  		- SPH_ROTL64(mf(j10m), j11m) + Kb(j16)) ^ hf(j7m))
195  #define expand1b_inner(qf, mf, hf, i16, \
196  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
<span onclick='openModal()' class='match'>197  		i9, i10, i11, i12, i13, i14, i15, \
198  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
199  	SPH_T64(sb1(qf(i0)) + sb2(qf(i1)) + sb3(qf(i2)) + sb0(qf(i3)) \
</span>200  		+ sb1(qf(i4)) + sb2(qf(i5)) + sb3(qf(i6)) + sb0(qf(i7)) \
201  		+ sb1(qf(i8)) + sb2(qf(i9)) + sb3(qf(i10)) + sb0(qf(i11)) \
202  		+ sb1(qf(i12)) + sb2(qf(i13)) + sb3(qf(i14)) + sb0(qf(i15)) \
203  		+ add_elt_b(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
204  #define expand1b(qf, mf, hf, i16) \
205  	expand1b_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
206  #define expand1b_(qf, mf, hf, i16, ix, iy) \
207  	expand1b_inner LPAR qf, mf, hf, i16, ix, iy)
208  #define expand2b_inner(qf, mf, hf, i16, \
209  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
210  		i9, i10, i11, i12, i13, i14, i15, \
211  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
212  	SPH_T64(qf(i0) + rb1(qf(i1)) + qf(i2) + rb2(qf(i3)) \
213  		+ qf(i4) + rb3(qf(i5)) + qf(i6) + rb4(qf(i7)) \
214  		+ qf(i8) + rb5(qf(i9)) + qf(i10) + rb6(qf(i11)) \
215  		+ qf(i12) + rb7(qf(i13)) + sb4(qf(i14)) + sb5(qf(i15)) \
216  		+ add_elt_b(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
217  #define expand2b(qf, mf, hf, i16) \
218  	expand2b_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
219  #define expand2b_(qf, mf, hf, i16, ix, iy) \
220  	expand2b_inner LPAR qf, mf, hf, i16, ix, iy)
221  #endif
222  #endif
223  #define MAKE_W(tt, i0, op01, i1, op12, i2, op23, i3, op34, i4) \
224  	tt((M(i0) ^ H(i0)) op01 (M(i1) ^ H(i1)) op12 (M(i2) ^ H(i2)) \
225  	op23 (M(i3) ^ H(i3)) op34 (M(i4) ^ H(i4)))
226  #if !defined(__AVX2__)
227  #define Ws0    MAKE_W(SPH_T32,  5, -,  7, +, 10, +, 13, +, 14)
228  #define Ws1    MAKE_W(SPH_T32,  6, -,  8, +, 11, +, 14, -, 15)
229  #define Ws2    MAKE_W(SPH_T32,  0, +,  7, +,  9, -, 12, +, 15)
230  #define Ws3    MAKE_W(SPH_T32,  0, -,  1, +,  8, -, 10, +, 13)
231  #define Ws4    MAKE_W(SPH_T32,  1, +,  2, +,  9, -, 11, -, 14)
232  #define Ws5    MAKE_W(SPH_T32,  3, -,  2, +, 10, -, 12, +, 15)
233  #define Ws6    MAKE_W(SPH_T32,  4, -,  0, -,  3, -, 11, +, 13)
234  #define Ws7    MAKE_W(SPH_T32,  1, -,  4, -,  5, -, 12, -, 14)
235  #define Ws8    MAKE_W(SPH_T32,  2, -,  5, -,  6, +, 13, -, 15)
236  #define Ws9    MAKE_W(SPH_T32,  0, -,  3, +,  6, -,  7, +, 14)
237  #define Ws10   MAKE_W(SPH_T32,  8, -,  1, -,  4, -,  7, +, 15)
238  #define Ws11   MAKE_W(SPH_T32,  8, -,  0, -,  2, -,  5, +,  9)
239  #define Ws12   MAKE_W(SPH_T32,  1, +,  3, -,  6, -,  9, +, 10)
240  #define Ws13   MAKE_W(SPH_T32,  2, +,  4, +,  7, +, 10, +, 11)
241  #define Ws14   MAKE_W(SPH_T32,  3, -,  5, +,  8, -, 11, -, 12)
242  #define Ws15   MAKE_W(SPH_T32, 12, -,  4, -,  6, -,  9, +, 13)
243  #if SPH_SMALL_FOOTPRINT_BMW
244  #define MAKE_Qas   do { \
245  		unsigned u; \
246  		sph_u32 Ws[16]; \
247  		Ws[ 0] = Ws0; \
248  		Ws[ 1] = Ws1; \
249  		Ws[ 2] = Ws2; \
250  		Ws[ 3] = Ws3; \
251  		Ws[ 4] = Ws4; \
252  		Ws[ 5] = Ws5; \
253  		Ws[ 6] = Ws6; \
254  		Ws[ 7] = Ws7; \
255  		Ws[ 8] = Ws8; \
256  		Ws[ 9] = Ws9; \
257  		Ws[10] = Ws10; \
258  		Ws[11] = Ws11; \
259  		Ws[12] = Ws12; \
260  		Ws[13] = Ws13; \
261  		Ws[14] = Ws14; \
262  		Ws[15] = Ws15; \
263  		for (u = 0; u < 15; u += 5) { \
264  			qt[u + 0] = SPH_T32(ss0(Ws[u + 0]) + H(u + 1)); \
265  			qt[u + 1] = SPH_T32(ss1(Ws[u + 1]) + H(u + 2)); \
266  			qt[u + 2] = SPH_T32(ss2(Ws[u + 2]) + H(u + 3)); \
267  			qt[u + 3] = SPH_T32(ss3(Ws[u + 3]) + H(u + 4)); \
268  			qt[u + 4] = SPH_T32(ss4(Ws[u + 4]) + H(u + 5)); \
269  		} \
270  		qt[15] = SPH_T32(ss0(Ws[15]) + H(0)); \
271  	} while (0)
272  #define MAKE_Qbs   do { \
273  		qt[16] = expand1s(Qs, M, H, 16); \
274  		qt[17] = expand1s(Qs, M, H, 17); \
275  		qt[18] = expand2s(Qs, M, H, 18); \
276  		qt[19] = expand2s(Qs, M, H, 19); \
277  		qt[20] = expand2s(Qs, M, H, 20); \
278  		qt[21] = expand2s(Qs, M, H, 21); \
279  		qt[22] = expand2s(Qs, M, H, 22); \
280  		qt[23] = expand2s(Qs, M, H, 23); \
281  		qt[24] = expand2s(Qs, M, H, 24); \
282  		qt[25] = expand2s(Qs, M, H, 25); \
283  		qt[26] = expand2s(Qs, M, H, 26); \
284  		qt[27] = expand2s(Qs, M, H, 27); \
285  		qt[28] = expand2s(Qs, M, H, 28); \
286  		qt[29] = expand2s(Qs, M, H, 29); \
287  		qt[30] = expand2s(Qs, M, H, 30); \
288  		qt[31] = expand2s(Qs, M, H, 31); \
289  	} while (0)
290  #else
291  #define MAKE_Qas   do { \
292  		qt[ 0] = SPH_T32(ss0(Ws0 ) + H( 1)); \
293  		qt[ 1] = SPH_T32(ss1(Ws1 ) + H( 2)); \
294  		qt[ 2] = SPH_T32(ss2(Ws2 ) + H( 3)); \
295  		qt[ 3] = SPH_T32(ss3(Ws3 ) + H( 4)); \
296  		qt[ 4] = SPH_T32(ss4(Ws4 ) + H( 5)); \
297  		qt[ 5] = SPH_T32(ss0(Ws5 ) + H( 6)); \
298  		qt[ 6] = SPH_T32(ss1(Ws6 ) + H( 7)); \
299  		qt[ 7] = SPH_T32(ss2(Ws7 ) + H( 8)); \
300  		qt[ 8] = SPH_T32(ss3(Ws8 ) + H( 9)); \
301  		qt[ 9] = SPH_T32(ss4(Ws9 ) + H(10)); \
302  		qt[10] = SPH_T32(ss0(Ws10) + H(11)); \
303  		qt[11] = SPH_T32(ss1(Ws11) + H(12)); \
304  		qt[12] = SPH_T32(ss2(Ws12) + H(13)); \
305  		qt[13] = SPH_T32(ss3(Ws13) + H(14)); \
306  		qt[14] = SPH_T32(ss4(Ws14) + H(15)); \
307  		qt[15] = SPH_T32(ss0(Ws15) + H( 0)); \
308  	} while (0)
309  #define MAKE_Qbs   do { \
310  		qt[16] = expand1s(Qs, M, H, 16); \
311  		qt[17] = expand1s(Qs, M, H, 17); \
312  		qt[18] = expand2s(Qs, M, H, 18); \
313  		qt[19] = expand2s(Qs, M, H, 19); \
314  		qt[20] = expand2s(Qs, M, H, 20); \
315  		qt[21] = expand2s(Qs, M, H, 21); \
316  		qt[22] = expand2s(Qs, M, H, 22); \
317  		qt[23] = expand2s(Qs, M, H, 23); \
318  		qt[24] = expand2s(Qs, M, H, 24); \
319  		qt[25] = expand2s(Qs, M, H, 25); \
320  		qt[26] = expand2s(Qs, M, H, 26); \
321  		qt[27] = expand2s(Qs, M, H, 27); \
322  		qt[28] = expand2s(Qs, M, H, 28); \
323  		qt[29] = expand2s(Qs, M, H, 29); \
324  		qt[30] = expand2s(Qs, M, H, 30); \
325  		qt[31] = expand2s(Qs, M, H, 31); \
326  	} while (0)
327  #endif
328  #define MAKE_Qs   do { \
329  		MAKE_Qas; \
330  		MAKE_Qbs; \
331  	} while (0)
332  #define Qs(j)   (qt[j])
333  #endif  
334  #if SPH_64
335  #define Wb0    MAKE_W(SPH_T64,  5, -,  7, +, 10, +, 13, +, 14)
336  #define Wb1    MAKE_W(SPH_T64,  6, -,  8, +, 11, +, 14, -, 15)
337  #define Wb2    MAKE_W(SPH_T64,  0, +,  7, +,  9, -, 12, +, 15)
338  #define Wb3    MAKE_W(SPH_T64,  0, -,  1, +,  8, -, 10, +, 13)
339  #define Wb4    MAKE_W(SPH_T64,  1, +,  2, +,  9, -, 11, -, 14)
340  #define Wb5    MAKE_W(SPH_T64,  3, -,  2, +, 10, -, 12, +, 15)
341  #define Wb6    MAKE_W(SPH_T64,  4, -,  0, -,  3, -, 11, +, 13)
342  #define Wb7    MAKE_W(SPH_T64,  1, -,  4, -,  5, -, 12, -, 14)
343  #define Wb8    MAKE_W(SPH_T64,  2, -,  5, -,  6, +, 13, -, 15)
344  #define Wb9    MAKE_W(SPH_T64,  0, -,  3, +,  6, -,  7, +, 14)
345  #define Wb10   MAKE_W(SPH_T64,  8, -,  1, -,  4, -,  7, +, 15)
346  #define Wb11   MAKE_W(SPH_T64,  8, -,  0, -,  2, -,  5, +,  9)
347  #define Wb12   MAKE_W(SPH_T64,  1, +,  3, -,  6, -,  9, +, 10)
348  #define Wb13   MAKE_W(SPH_T64,  2, +,  4, +,  7, +, 10, +, 11)
349  #define Wb14   MAKE_W(SPH_T64,  3, -,  5, +,  8, -, 11, -, 12)
350  #define Wb15   MAKE_W(SPH_T64, 12, -,  4, -,  6, -,  9, +, 13)
351  #if SPH_SMALL_FOOTPRINT_BMW
352  #define MAKE_Qab   do { \
353  		unsigned u; \
354  		sph_u64 Wb[16]; \
355  		Wb[ 0] = Wb0; \
356  		Wb[ 1] = Wb1; \
357  		Wb[ 2] = Wb2; \
358  		Wb[ 3] = Wb3; \
359  		Wb[ 4] = Wb4; \
360  		Wb[ 5] = Wb5; \
361  		Wb[ 6] = Wb6; \
362  		Wb[ 7] = Wb7; \
363  		Wb[ 8] = Wb8; \
364  		Wb[ 9] = Wb9; \
365  		Wb[10] = Wb10; \
366  		Wb[11] = Wb11; \
367  		Wb[12] = Wb12; \
368  		Wb[13] = Wb13; \
369  		Wb[14] = Wb14; \
370  		Wb[15] = Wb15; \
371  		for (u = 0; u < 15; u += 5) { \
372  			qt[u + 0] = SPH_T64(sb0(Wb[u + 0]) + H(u + 1)); \
373  			qt[u + 1] = SPH_T64(sb1(Wb[u + 1]) + H(u + 2)); \
374  			qt[u + 2] = SPH_T64(sb2(Wb[u + 2]) + H(u + 3)); \
375  			qt[u + 3] = SPH_T64(sb3(Wb[u + 3]) + H(u + 4)); \
376  			qt[u + 4] = SPH_T64(sb4(Wb[u + 4]) + H(u + 5)); \
377  		} \
378  		qt[15] = SPH_T64(sb0(Wb[15]) + H(0)); \
379  	} while (0)
380  #define MAKE_Qbb   do { \
381  		unsigned u; \
382  		for (u = 16; u < 18; u ++) \
383  			qt[u] = expand1b(Qb, M, H, u); \
384  		for (u = 18; u < 32; u ++) \
385  			qt[u] = expand2b(Qb, M, H, u); \
386  	} while (0)
387  #else
388  #define MAKE_Qab   do { \
389  		qt[ 0] = SPH_T64(sb0(Wb0 ) + H( 1)); \
390  		qt[ 1] = SPH_T64(sb1(Wb1 ) + H( 2)); \
391  		qt[ 2] = SPH_T64(sb2(Wb2 ) + H( 3)); \
392  		qt[ 3] = SPH_T64(sb3(Wb3 ) + H( 4)); \
393  		qt[ 4] = SPH_T64(sb4(Wb4 ) + H( 5)); \
394  		qt[ 5] = SPH_T64(sb0(Wb5 ) + H( 6)); \
395  		qt[ 6] = SPH_T64(sb1(Wb6 ) + H( 7)); \
396  		qt[ 7] = SPH_T64(sb2(Wb7 ) + H( 8)); \
397  		qt[ 8] = SPH_T64(sb3(Wb8 ) + H( 9)); \
398  		qt[ 9] = SPH_T64(sb4(Wb9 ) + H(10)); \
399  		qt[10] = SPH_T64(sb0(Wb10) + H(11)); \
400  		qt[11] = SPH_T64(sb1(Wb11) + H(12)); \
401  		qt[12] = SPH_T64(sb2(Wb12) + H(13)); \
402  		qt[13] = SPH_T64(sb3(Wb13) + H(14)); \
403  		qt[14] = SPH_T64(sb4(Wb14) + H(15)); \
404  		qt[15] = SPH_T64(sb0(Wb15) + H( 0)); \
405  	} while (0)
406  #define MAKE_Qbb   do { \
407  		qt[16] = expand1b(Qb, M, H, 16); \
408  		qt[17] = expand1b(Qb, M, H, 17); \
409  		qt[18] = expand2b(Qb, M, H, 18); \
410  		qt[19] = expand2b(Qb, M, H, 19); \
411  		qt[20] = expand2b(Qb, M, H, 20); \
412  		qt[21] = expand2b(Qb, M, H, 21); \
413  		qt[22] = expand2b(Qb, M, H, 22); \
414  		qt[23] = expand2b(Qb, M, H, 23); \
415  		qt[24] = expand2b(Qb, M, H, 24); \
416  		qt[25] = expand2b(Qb, M, H, 25); \
417  		qt[26] = expand2b(Qb, M, H, 26); \
418  		qt[27] = expand2b(Qb, M, H, 27); \
419  		qt[28] = expand2b(Qb, M, H, 28); \
420  		qt[29] = expand2b(Qb, M, H, 29); \
421  		qt[30] = expand2b(Qb, M, H, 30); \
422  		qt[31] = expand2b(Qb, M, H, 31); \
423  	} while (0)
424  #endif
425  #define MAKE_Qb   do { \
426  		MAKE_Qab; \
427  		MAKE_Qbb; \
428  	} while (0)
429  #define Qb(j)   (qt[j])
430  #endif
431  #define FOLD(type, mkQ, tt, rol, mf, qf, dhf)   do { \
432  		type qt[32], xl, xh; \
433  		mkQ; \
434  		xl = qf(16) ^ qf(17) ^ qf(18) ^ qf(19) \
435  			^ qf(20) ^ qf(21) ^ qf(22) ^ qf(23); \
436  		xh = xl ^ qf(24) ^ qf(25) ^ qf(26) ^ qf(27) \
437  			^ qf(28) ^ qf(29) ^ qf(30) ^ qf(31); \
438  		dhf( 0) = tt(((xh <<  5) ^ (qf(16) >>  5) ^ mf( 0)) \
439  			+ (xl ^ qf(24) ^ qf( 0))); \
440  		dhf( 1) = tt(((xh >>  7) ^ (qf(17) <<  8) ^ mf( 1)) \
441  			+ (xl ^ qf(25) ^ qf( 1))); \
442  		dhf( 2) = tt(((xh >>  5) ^ (qf(18) <<  5) ^ mf( 2)) \
443  			+ (xl ^ qf(26) ^ qf( 2))); \
444  		dhf( 3) = tt(((xh >>  1) ^ (qf(19) <<  5) ^ mf( 3)) \
445  			+ (xl ^ qf(27) ^ qf( 3))); \
446  		dhf( 4) = tt(((xh >>  3) ^ (qf(20) <<  0) ^ mf( 4)) \
447  			+ (xl ^ qf(28) ^ qf( 4))); \
448  		dhf( 5) = tt(((xh <<  6) ^ (qf(21) >>  6) ^ mf( 5)) \
449  			+ (xl ^ qf(29) ^ qf( 5))); \
450  		dhf( 6) = tt(((xh >>  4) ^ (qf(22) <<  6) ^ mf( 6)) \
451  			+ (xl ^ qf(30) ^ qf( 6))); \
452  		dhf( 7) = tt(((xh >> 11) ^ (qf(23) <<  2) ^ mf( 7)) \
453  			+ (xl ^ qf(31) ^ qf( 7))); \
454  		dhf( 8) = tt(rol(dhf(4),  9) + (xh ^ qf(24) ^ mf( 8)) \
455  			+ ((xl << 8) ^ qf(23) ^ qf( 8))); \
456  		dhf( 9) = tt(rol(dhf(5), 10) + (xh ^ qf(25) ^ mf( 9)) \
457  			+ ((xl >> 6) ^ qf(16) ^ qf( 9))); \
458  		dhf(10) = tt(rol(dhf(6), 11) + (xh ^ qf(26) ^ mf(10)) \
459  			+ ((xl << 6) ^ qf(17) ^ qf(10))); \
460  		dhf(11) = tt(rol(dhf(7), 12) + (xh ^ qf(27) ^ mf(11)) \
461  			+ ((xl << 4) ^ qf(18) ^ qf(11))); \
462  		dhf(12) = tt(rol(dhf(0), 13) + (xh ^ qf(28) ^ mf(12)) \
463  			+ ((xl >> 3) ^ qf(19) ^ qf(12))); \
464  		dhf(13) = tt(rol(dhf(1), 14) + (xh ^ qf(29) ^ mf(13)) \
465  			+ ((xl >> 4) ^ qf(20) ^ qf(13))); \
466  		dhf(14) = tt(rol(dhf(2), 15) + (xh ^ qf(30) ^ mf(14)) \
467  			+ ((xl >> 7) ^ qf(21) ^ qf(14))); \
468  		dhf(15) = tt(rol(dhf(3), 16) + (xh ^ qf(31) ^ mf(15)) \
469  			+ ((xl >> 2) ^ qf(22) ^ qf(15))); \
470  	} while (0)
471  #if SPH_64
472  #define FOLDb   FOLD(sph_u64, MAKE_Qb, SPH_T64, SPH_ROTL64, M, Qb, dH)
473  #endif
474  #if !defined(__AVX2__)
475  #define FOLDs   FOLD(sph_u32, MAKE_Qs, SPH_T32, SPH_ROTL32, M, Qs, dH)
476  static void
477  compress_small(const unsigned char *data, const sph_u32 h[16], sph_u32 dh[16])
478  {
479  #if SPH_LITTLE_FAST
480  #define M(x)    sph_dec32le_aligned(data + 4 * (x))
481  #else
482  	sph_u32 mv[16];
483  	mv[ 0] = sph_dec32le_aligned(data +  0);
484  	mv[ 1] = sph_dec32le_aligned(data +  4);
485  	mv[ 2] = sph_dec32le_aligned(data +  8);
486  	mv[ 3] = sph_dec32le_aligned(data + 12);
487  	mv[ 4] = sph_dec32le_aligned(data + 16);
488  	mv[ 5] = sph_dec32le_aligned(data + 20);
489  	mv[ 6] = sph_dec32le_aligned(data + 24);
490  	mv[ 7] = sph_dec32le_aligned(data + 28);
491  	mv[ 8] = sph_dec32le_aligned(data + 32);
492  	mv[ 9] = sph_dec32le_aligned(data + 36);
493  	mv[10] = sph_dec32le_aligned(data + 40);
494  	mv[11] = sph_dec32le_aligned(data + 44);
495  	mv[12] = sph_dec32le_aligned(data + 48);
496  	mv[13] = sph_dec32le_aligned(data + 52);
497  	mv[14] = sph_dec32le_aligned(data + 56);
498  	mv[15] = sph_dec32le_aligned(data + 60);
499  #define M(x)    (mv[x])
500  #endif
501  #define H(x)    (h[x])
502  #define dH(x)   (dh[x])
503  	FOLDs;
504  #undef M
505  #undef H
506  #undef dH
507  }
508  static const sph_u32 final_s[16] = {
509  	SPH_C32(0xaaaaaaa0), SPH_C32(0xaaaaaaa1), SPH_C32(0xaaaaaaa2),
510  	SPH_C32(0xaaaaaaa3), SPH_C32(0xaaaaaaa4), SPH_C32(0xaaaaaaa5),
511  	SPH_C32(0xaaaaaaa6), SPH_C32(0xaaaaaaa7), SPH_C32(0xaaaaaaa8),
512  	SPH_C32(0xaaaaaaa9), SPH_C32(0xaaaaaaaa), SPH_C32(0xaaaaaaab),
513  	SPH_C32(0xaaaaaaac), SPH_C32(0xaaaaaaad), SPH_C32(0xaaaaaaae),
514  	SPH_C32(0xaaaaaaaf)
515  };
516  static void
517  bmw32_init(sph_bmw_small_context *sc, const sph_u32 *iv)
518  {
519  	memcpy(sc->H, iv, sizeof sc->H);
520  	sc->ptr = 0;
521  #if SPH_64
522  	sc->bit_count = 0;
523  #else
524  	sc->bit_count_high = 0;
525  	sc->bit_count_low = 0;
526  #endif
527  }
528  static void
529  bmw32(sph_bmw_small_context *sc, const void *data, size_t len)
530  {
531  	unsigned char *buf;
532  	size_t ptr;
533  	sph_u32 htmp[16];
534  	sph_u32 *h1, *h2;
535  #if !SPH_64
536  	sph_u32 tmp;
537  #endif
538  #if SPH_64
539  	sc->bit_count += (sph_u64)len << 3;
540  #else
541  	tmp = sc->bit_count_low;
542  	sc->bit_count_low = SPH_T32(tmp + ((sph_u32)len << 3));
543  	if (sc->bit_count_low < tmp)
544  		sc->bit_count_high ++;
545  	sc->bit_count_high += len >> 29;
546  #endif
547  	buf = sc->buf;
548  	ptr = sc->ptr;
549  	h1 = sc->H;
550  	h2 = htmp;
551  	while (len > 0) {
552  		size_t clen;
553  		clen = (sizeof sc->buf) - ptr;
554  		if (clen > len)
555  			clen = len;
556  		memcpy(buf + ptr, data, clen);
557  		data = (const unsigned char *)data + clen;
558  		len -= clen;
559  		ptr += clen;
560  		if (ptr == sizeof sc->buf) {
561  			sph_u32 *ht;
562  			compress_small(buf, h1, h2);
563  			ht = h1;
564  			h1 = h2;
565  			h2 = ht;
566  			ptr = 0;
567  		}
568  	}
569  	sc->ptr = ptr;
570  	if (h1 != sc->H)
571  		memcpy(sc->H, h1, sizeof sc->H);
572  }
573  static void
574  bmw32_close(sph_bmw_small_context *sc, unsigned ub, unsigned n,
575  	void *dst, size_t out_size_w32)
576  {
577  	unsigned char *buf, *out;
578  	size_t ptr, u, v;
579  	unsigned z;
580  	sph_u32 h1[16], h2[16], *h;
581  	buf = sc->buf;
582  	ptr = sc->ptr;
583  	z = 0x80 >> n;
584  	buf[ptr ++] = ((ub & -z) | z) & 0xFF;
585  	h = sc->H;
586  	if (ptr > (sizeof sc->buf) - 8) {
587  		memset(buf + ptr, 0, (sizeof sc->buf) - ptr);
588  		compress_small(buf, h, h1);
589  		ptr = 0;
590  		h = h1;
591  	}
592  	memset(buf + ptr, 0, (sizeof sc->buf) - 8 - ptr);
593  #if SPH_64
594  	sph_enc64le_aligned(buf + (sizeof sc->buf) - 8,
595  		SPH_T64(sc->bit_count + n));
596  #else
597  	sph_enc32le_aligned(buf + (sizeof sc->buf) - 8,
598  		sc->bit_count_low + n);
599  	sph_enc32le_aligned(buf + (sizeof sc->buf) - 4,
600  		SPH_T32(sc->bit_count_high));
601  #endif
602  	compress_small(buf, h, h2);
603  	for (u = 0; u < 16; u ++)
604  		sph_enc32le_aligned(buf + 4 * u, h2[u]);
605  	compress_small(buf, final_s, h1);
606  	out = dst;
607  	for (u = 0, v = 16 - out_size_w32; u < out_size_w32; u ++, v ++)
608  		sph_enc32le(out + 4 * u, h1[v]);
609  }
610  #endif 
611  #if SPH_64
612  static void
613  compress_big(const unsigned char *data, const sph_u64 h[16], sph_u64 dh[16])
614  {
615  #if SPH_LITTLE_FAST
616  #define M(x)    sph_dec64le_aligned(data + 8 * (x))
617  #else
618  	sph_u64 mv[16];
619  	mv[ 0] = sph_dec64le_aligned(data +   0);
620  	mv[ 1] = sph_dec64le_aligned(data +   8);
621  	mv[ 2] = sph_dec64le_aligned(data +  16);
622  	mv[ 3] = sph_dec64le_aligned(data +  24);
623  	mv[ 4] = sph_dec64le_aligned(data +  32);
624  	mv[ 5] = sph_dec64le_aligned(data +  40);
625  	mv[ 6] = sph_dec64le_aligned(data +  48);
626  	mv[ 7] = sph_dec64le_aligned(data +  56);
627  	mv[ 8] = sph_dec64le_aligned(data +  64);
628  	mv[ 9] = sph_dec64le_aligned(data +  72);
629  	mv[10] = sph_dec64le_aligned(data +  80);
630  	mv[11] = sph_dec64le_aligned(data +  88);
631  	mv[12] = sph_dec64le_aligned(data +  96);
632  	mv[13] = sph_dec64le_aligned(data + 104);
633  	mv[14] = sph_dec64le_aligned(data + 112);
634  	mv[15] = sph_dec64le_aligned(data + 120);
635  #define M(x)    (mv[x])
636  #endif
637  #define H(x)    (h[x])
638  #define dH(x)   (dh[x])
639  	FOLDb;
640  #undef M
641  #undef H
642  #undef dH
643  }
644  static const sph_u64 final_b[16] = {
645  	SPH_C64(0xaaaaaaaaaaaaaaa0), SPH_C64(0xaaaaaaaaaaaaaaa1),
646  	SPH_C64(0xaaaaaaaaaaaaaaa2), SPH_C64(0xaaaaaaaaaaaaaaa3),
647  	SPH_C64(0xaaaaaaaaaaaaaaa4), SPH_C64(0xaaaaaaaaaaaaaaa5),
648  	SPH_C64(0xaaaaaaaaaaaaaaa6), SPH_C64(0xaaaaaaaaaaaaaaa7),
649  	SPH_C64(0xaaaaaaaaaaaaaaa8), SPH_C64(0xaaaaaaaaaaaaaaa9),
650  	SPH_C64(0xaaaaaaaaaaaaaaaa), SPH_C64(0xaaaaaaaaaaaaaaab),
651  	SPH_C64(0xaaaaaaaaaaaaaaac), SPH_C64(0xaaaaaaaaaaaaaaad),
652  	SPH_C64(0xaaaaaaaaaaaaaaae), SPH_C64(0xaaaaaaaaaaaaaaaf)
653  };
654  static void
655  bmw64_init(sph_bmw_big_context *sc, const sph_u64 *iv)
656  {
657  	memcpy(sc->H, iv, sizeof sc->H);
658  	sc->ptr = 0;
659  	sc->bit_count = 0;
660  }
661  static void
662  bmw64(sph_bmw_big_context *sc, const void *data, size_t len)
663  {
664  	unsigned char *buf;
665  	size_t ptr;
666  	sph_u64 htmp[16];
667  	sph_u64 *h1, *h2;
668  	sc->bit_count += (sph_u64)len << 3;
669  	buf = sc->buf;
670  	ptr = sc->ptr;
671  	h1 = sc->H;
672  	h2 = htmp;
673  	while (len > 0) {
674  		size_t clen;
675  		clen = (sizeof sc->buf) - ptr;
676  		if (clen > len)
677  			clen = len;
678  		memcpy(buf + ptr, data, clen);
679  		data = (const unsigned char *)data + clen;
680  		len -= clen;
681  		ptr += clen;
682  		if (ptr == sizeof sc->buf) {
683  			sph_u64 *ht;
684  			compress_big(buf, h1, h2);
685  			ht = h1;
686  			h1 = h2;
687  			h2 = ht;
688  			ptr = 0;
689  		}
690  	}
691  	sc->ptr = ptr;
692  	if (h1 != sc->H)
693  		memcpy(sc->H, h1, sizeof sc->H);
694  }
695  static void
696  bmw64_close(sph_bmw_big_context *sc, unsigned ub, unsigned n,
697  	void *dst, size_t out_size_w64)
698  {
699  	unsigned char *buf, *out;
700  	size_t ptr, u, v;
701  	unsigned z;
702  	sph_u64 h1[16], h2[16], *h;
703  	buf = sc->buf;
704  	ptr = sc->ptr;
705  	z = 0x80 >> n;
706  	buf[ptr ++] = ((ub & -z) | z) & 0xFF;
707  	h = sc->H;
708  	if (ptr > (sizeof sc->buf) - 8) {
709  		memset(buf + ptr, 0, (sizeof sc->buf) - ptr);
710  		compress_big(buf, h, h1);
711  		ptr = 0;
712  		h = h1;
713  	}
714  	memset(buf + ptr, 0, (sizeof sc->buf) - 8 - ptr);
715  	sph_enc64le_aligned(buf + (sizeof sc->buf) - 8,
716  		SPH_T64(sc->bit_count + n));
717  	compress_big(buf, h, h2);
718  	for (u = 0; u < 16; u ++)
719  		sph_enc64le_aligned(buf + 8 * u, h2[u]);
720  	compress_big(buf, final_b, h1);
721  	out = dst;
722  	for (u = 0, v = 16 - out_size_w64; u < out_size_w64; u ++, v ++)
723  		sph_enc64le(out + 8 * u, h1[v]);
724  }
725  #endif
726  #if !defined(__AVX2__)
727  void
728  sph_bmw224_init(void *cc)
729  {
730  	bmw32_init(cc, IV224);
731  }
732  void
733  sph_bmw224(void *cc, const void *data, size_t len)
734  {
735  	bmw32(cc, data, len);
736  }
737  void
738  sph_bmw224_close(void *cc, void *dst)
739  {
740  	sph_bmw224_addbits_and_close(cc, 0, 0, dst);
741  }
742  void
743  sph_bmw224_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
744  {
745  	bmw32_close(cc, ub, n, dst, 7);
746  }
747  void
748  sph_bmw256_init(void *cc)
749  {
750  	bmw32_init(cc, IV256);
751  }
752  void
753  sph_bmw256(void *cc, const void *data, size_t len)
754  {
755  	bmw32(cc, data, len);
756  }
757  void
758  sph_bmw256_close(void *cc, void *dst)
759  {
760  	sph_bmw256_addbits_and_close(cc, 0, 0, dst);
761  }
762  void
763  sph_bmw256_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
764  {
765  	bmw32_close(cc, ub, n, dst, 8);
766  }
767  #endif 
768  #if SPH_64
769  void
770  sph_bmw384_init(void *cc)
771  {
772  	bmw64_init(cc, IV384);
773  }
774  void
775  sph_bmw384(void *cc, const void *data, size_t len)
776  {
777  	bmw64(cc, data, len);
778  }
779  void
780  sph_bmw384_close(void *cc, void *dst)
781  {
782  	sph_bmw384_addbits_and_close(cc, 0, 0, dst);
783  }
784  void
785  sph_bmw384_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
786  {
787  	bmw64_close(cc, ub, n, dst, 6);
788  }
789  void
790  sph_bmw512_init(void *cc)
791  {
792  	bmw64_init(cc, IV512);
793  }
794  void
795  sph_bmw512(void *cc, const void *data, size_t len)
796  {
797  	bmw64(cc, data, len);
798  }
799  void
800  sph_bmw512_close(void *cc, void *dst)
801  {
802  	sph_bmw512_addbits_and_close(cc, 0, 0, dst);
803  }
804  void
805  sph_bmw512_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
806  {
807  	bmw64_close(cc, ub, n, dst, 8);
808  }
809  #endif
810  #ifdef __cplusplus
811  }
812  #endif
</code></pre>
        </div>
        <div class="column">
            <h3>xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_bmw.c</h3>
            <pre><code>1  #include <stddef.h>
2  #include <string.h>
3  #include <limits.h>
4  #ifdef __cplusplus
5  extern "C"{
6  #endif
7  #include "sph_bmw.h"
8  #if SPH_SMALL_FOOTPRINT && !defined SPH_SMALL_FOOTPRINT_BMW
9  #define SPH_SMALL_FOOTPRINT_BMW   1
10  #endif
11  #ifdef _MSC_VER
12  #pragma warning (disable: 4146)
13  #endif
14  #if !defined(__AVX2__)
15  static const sph_u32 IV224[] = {
16  	SPH_C32(0x00010203), SPH_C32(0x04050607),
17  	SPH_C32(0x08090A0B), SPH_C32(0x0C0D0E0F),
18  	SPH_C32(0x10111213), SPH_C32(0x14151617),
19  	SPH_C32(0x18191A1B), SPH_C32(0x1C1D1E1F),
20  	SPH_C32(0x20212223), SPH_C32(0x24252627),
21  	SPH_C32(0x28292A2B), SPH_C32(0x2C2D2E2F),
22  	SPH_C32(0x30313233), SPH_C32(0x34353637),
23  	SPH_C32(0x38393A3B), SPH_C32(0x3C3D3E3F)
24  };
25  static const sph_u32 IV256[] = {
26  	SPH_C32(0x40414243), SPH_C32(0x44454647),
27  	SPH_C32(0x48494A4B), SPH_C32(0x4C4D4E4F),
28  	SPH_C32(0x50515253), SPH_C32(0x54555657),
29  	SPH_C32(0x58595A5B), SPH_C32(0x5C5D5E5F),
30  	SPH_C32(0x60616263), SPH_C32(0x64656667),
31  	SPH_C32(0x68696A6B), SPH_C32(0x6C6D6E6F),
32  	SPH_C32(0x70717273), SPH_C32(0x74757677),
33  	SPH_C32(0x78797A7B), SPH_C32(0x7C7D7E7F)
34  };
35  #endif 
36  #if SPH_64
37  static const sph_u64 IV384[] = {
38  	SPH_C64(0x0001020304050607), SPH_C64(0x08090A0B0C0D0E0F),
39  	SPH_C64(0x1011121314151617), SPH_C64(0x18191A1B1C1D1E1F),
40  	SPH_C64(0x2021222324252627), SPH_C64(0x28292A2B2C2D2E2F),
41  	SPH_C64(0x3031323334353637), SPH_C64(0x38393A3B3C3D3E3F),
42  	SPH_C64(0x4041424344454647), SPH_C64(0x48494A4B4C4D4E4F),
43  	SPH_C64(0x5051525354555657), SPH_C64(0x58595A5B5C5D5E5F),
44  	SPH_C64(0x6061626364656667), SPH_C64(0x68696A6B6C6D6E6F),
45  	SPH_C64(0x7071727374757677), SPH_C64(0x78797A7B7C7D7E7F)
46  };
47  static const sph_u64 IV512[] = {
48  	SPH_C64(0x8081828384858687), SPH_C64(0x88898A8B8C8D8E8F),
49  	SPH_C64(0x9091929394959697), SPH_C64(0x98999A9B9C9D9E9F),
50  	SPH_C64(0xA0A1A2A3A4A5A6A7), SPH_C64(0xA8A9AAABACADAEAF),
51  	SPH_C64(0xB0B1B2B3B4B5B6B7), SPH_C64(0xB8B9BABBBCBDBEBF),
52  	SPH_C64(0xC0C1C2C3C4C5C6C7), SPH_C64(0xC8C9CACBCCCDCECF),
53  	SPH_C64(0xD0D1D2D3D4D5D6D7), SPH_C64(0xD8D9DADBDCDDDEDF),
54  	SPH_C64(0xE0E1E2E3E4E5E6E7), SPH_C64(0xE8E9EAEBECEDEEEF),
55  	SPH_C64(0xF0F1F2F3F4F5F6F7), SPH_C64(0xF8F9FAFBFCFDFEFF)
56  };
57  #endif
58  #define XCAT(x, y)    XCAT_(x, y)
59  #define XCAT_(x, y)   x ## y
60  #define LPAR   (
61  #define I16_16    0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15
62  #define I16_17    1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16
63  #define I16_18    2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17
64  #define I16_19    3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18
65  #define I16_20    4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19
66  #define I16_21    5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20
67  #define I16_22    6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21
68  #define I16_23    7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22
69  #define I16_24    8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23
70  #define I16_25    9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
71  #define I16_26   10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25
72  #define I16_27   11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26
73  #define I16_28   12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27
74  #define I16_29   13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28
75  #define I16_30   14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29
76  #define I16_31   15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30
77  #define M16_16    0,  1,  3,  4,  7, 10, 11
78  #define M16_17    1,  2,  4,  5,  8, 11, 12
79  #define M16_18    2,  3,  5,  6,  9, 12, 13
80  #define M16_19    3,  4,  6,  7, 10, 13, 14
81  #define M16_20    4,  5,  7,  8, 11, 14, 15
82  #define M16_21    5,  6,  8,  9, 12, 15, 16
83  #define M16_22    6,  7,  9, 10, 13,  0,  1
84  #define M16_23    7,  8, 10, 11, 14,  1,  2
85  #define M16_24    8,  9, 11, 12, 15,  2,  3
86  #define M16_25    9, 10, 12, 13,  0,  3,  4
87  #define M16_26   10, 11, 13, 14,  1,  4,  5
88  #define M16_27   11, 12, 14, 15,  2,  5,  6
89  #define M16_28   12, 13, 15, 16,  3,  6,  7
90  #define M16_29   13, 14,  0,  1,  4,  7,  8
91  #define M16_30   14, 15,  1,  2,  5,  8,  9
92  #define M16_31   15, 16,  2,  3,  6,  9, 10
93  #if !defined(__AVX2__)
94  #define ss0(x)    (((x) >> 1) ^ SPH_T32((x) << 3) \
95                    ^ SPH_ROTL32(x,  4) ^ SPH_ROTL32(x, 19))
96  #define ss1(x)    (((x) >> 1) ^ SPH_T32((x) << 2) \
97                    ^ SPH_ROTL32(x,  8) ^ SPH_ROTL32(x, 23))
98  #define ss2(x)    (((x) >> 2) ^ SPH_T32((x) << 1) \
99                    ^ SPH_ROTL32(x, 12) ^ SPH_ROTL32(x, 25))
100  #define ss3(x)    (((x) >> 2) ^ SPH_T32((x) << 2) \
101                    ^ SPH_ROTL32(x, 15) ^ SPH_ROTL32(x, 29))
102  #define ss4(x)    (((x) >> 1) ^ (x))
103  #define ss5(x)    (((x) >> 2) ^ (x))
104  #define rs1(x)    SPH_ROTL32(x,  3)
105  #define rs2(x)    SPH_ROTL32(x,  7)
106  #define rs3(x)    SPH_ROTL32(x, 13)
107  #define rs4(x)    SPH_ROTL32(x, 16)
108  #define rs5(x)    SPH_ROTL32(x, 19)
109  #define rs6(x)    SPH_ROTL32(x, 23)
110  #define rs7(x)    SPH_ROTL32(x, 27)
111  #define Ks(j)   SPH_T32((sph_u32)(j) * SPH_C32(0x05555555))
112  #define add_elt_s(mf, hf, j0m, j1m, j3m, j4m, j7m, j10m, j11m, j16) \
113  	(SPH_T32(SPH_ROTL32(mf(j0m), j1m) + SPH_ROTL32(mf(j3m), j4m) \
114  		- SPH_ROTL32(mf(j10m), j11m) + Ks(j16)) ^ hf(j7m))
115  #define expand1s_inner(qf, mf, hf, i16, \
116  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
<span onclick='openModal()' class='match'>117  		i9, i10, i11, i12, i13, i14, i15, \
118  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
119  	SPH_T32(ss1(qf(i0)) + ss2(qf(i1)) + ss3(qf(i2)) + ss0(qf(i3)) \
</span>120  		+ ss1(qf(i4)) + ss2(qf(i5)) + ss3(qf(i6)) + ss0(qf(i7)) \
121  		+ ss1(qf(i8)) + ss2(qf(i9)) + ss3(qf(i10)) + ss0(qf(i11)) \
122  		+ ss1(qf(i12)) + ss2(qf(i13)) + ss3(qf(i14)) + ss0(qf(i15)) \
123  		+ add_elt_s(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
124  #define expand1s(qf, mf, hf, i16) \
125  	expand1s_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
126  #define expand1s_(qf, mf, hf, i16, ix, iy) \
127  	expand1s_inner LPAR qf, mf, hf, i16, ix, iy)
128  #define expand2s_inner(qf, mf, hf, i16, \
129  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
130  		i9, i10, i11, i12, i13, i14, i15, \
131  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
132  	SPH_T32(qf(i0) + rs1(qf(i1)) + qf(i2) + rs2(qf(i3)) \
133  		+ qf(i4) + rs3(qf(i5)) + qf(i6) + rs4(qf(i7)) \
134  		+ qf(i8) + rs5(qf(i9)) + qf(i10) + rs6(qf(i11)) \
135  		+ qf(i12) + rs7(qf(i13)) + ss4(qf(i14)) + ss5(qf(i15)) \
136  		+ add_elt_s(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
137  #define expand2s(qf, mf, hf, i16) \
138  	expand2s_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
139  #define expand2s_(qf, mf, hf, i16, ix, iy) \
140  	expand2s_inner LPAR qf, mf, hf, i16, ix, iy)
141  #endif 
142  #if SPH_64
143  #define sb0(x)    (((x) >> 1) ^ SPH_T64((x) << 3) \
144                    ^ SPH_ROTL64(x,  4) ^ SPH_ROTL64(x, 37))
145  #define sb1(x)    (((x) >> 1) ^ SPH_T64((x) << 2) \
146                    ^ SPH_ROTL64(x, 13) ^ SPH_ROTL64(x, 43))
147  #define sb2(x)    (((x) >> 2) ^ SPH_T64((x) << 1) \
148                    ^ SPH_ROTL64(x, 19) ^ SPH_ROTL64(x, 53))
149  #define sb3(x)    (((x) >> 2) ^ SPH_T64((x) << 2) \
150                    ^ SPH_ROTL64(x, 28) ^ SPH_ROTL64(x, 59))
151  #define sb4(x)    (((x) >> 1) ^ (x))
152  #define sb5(x)    (((x) >> 2) ^ (x))
153  #define rb1(x)    SPH_ROTL64(x,  5)
154  #define rb2(x)    SPH_ROTL64(x, 11)
155  #define rb3(x)    SPH_ROTL64(x, 27)
156  #define rb4(x)    SPH_ROTL64(x, 32)
157  #define rb5(x)    SPH_ROTL64(x, 37)
158  #define rb6(x)    SPH_ROTL64(x, 43)
159  #define rb7(x)    SPH_ROTL64(x, 53)
160  #define Kb(j)   SPH_T64((sph_u64)(j) * SPH_C64(0x0555555555555555))
161  #if SPH_SMALL_FOOTPRINT_BMW
162  static const sph_u64 Kb_tab[] = {
163  	Kb(16), Kb(17), Kb(18), Kb(19), Kb(20), Kb(21), Kb(22), Kb(23),
164  	Kb(24), Kb(25), Kb(26), Kb(27), Kb(28), Kb(29), Kb(30), Kb(31)
165  };
166  #define rol_off(mf, j, off) \
167  	SPH_ROTL64(mf(((j) + (off)) & 15), (((j) + (off)) & 15) + 1)
168  #define add_elt_b(mf, hf, j) \
169  	(SPH_T64(rol_off(mf, j, 0) + rol_off(mf, j, 3) \
170  		- rol_off(mf, j, 10) + Kb_tab[j]) ^ hf(((j) + 7) & 15))
171  #define expand1b(qf, mf, hf, i) \
172  	SPH_T64(sb1(qf((i) - 16)) + sb2(qf((i) - 15)) \
173  		+ sb3(qf((i) - 14)) + sb0(qf((i) - 13)) \
174  		+ sb1(qf((i) - 12)) + sb2(qf((i) - 11)) \
175  		+ sb3(qf((i) - 10)) + sb0(qf((i) - 9)) \
176  		+ sb1(qf((i) - 8)) + sb2(qf((i) - 7)) \
177  		+ sb3(qf((i) - 6)) + sb0(qf((i) - 5)) \
178  		+ sb1(qf((i) - 4)) + sb2(qf((i) - 3)) \
179  		+ sb3(qf((i) - 2)) + sb0(qf((i) - 1)) \
180  		+ add_elt_b(mf, hf, (i) - 16))
181  #define expand2b(qf, mf, hf, i) \
182  	SPH_T64(qf((i) - 16) + rb1(qf((i) - 15)) \
183  		+ qf((i) - 14) + rb2(qf((i) - 13)) \
184  		+ qf((i) - 12) + rb3(qf((i) - 11)) \
185  		+ qf((i) - 10) + rb4(qf((i) - 9)) \
186  		+ qf((i) - 8) + rb5(qf((i) - 7)) \
187  		+ qf((i) - 6) + rb6(qf((i) - 5)) \
188  		+ qf((i) - 4) + rb7(qf((i) - 3)) \
189  		+ sb4(qf((i) - 2)) + sb5(qf((i) - 1)) \
190  		+ add_elt_b(mf, hf, (i) - 16))
191  #else
192  #define add_elt_b(mf, hf, j0m, j1m, j3m, j4m, j7m, j10m, j11m, j16) \
193  	(SPH_T64(SPH_ROTL64(mf(j0m), j1m) + SPH_ROTL64(mf(j3m), j4m) \
194  		- SPH_ROTL64(mf(j10m), j11m) + Kb(j16)) ^ hf(j7m))
195  #define expand1b_inner(qf, mf, hf, i16, \
196  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
197  		i9, i10, i11, i12, i13, i14, i15, \
198  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
199  	SPH_T64(sb1(qf(i0)) + sb2(qf(i1)) + sb3(qf(i2)) + sb0(qf(i3)) \
200  		+ sb1(qf(i4)) + sb2(qf(i5)) + sb3(qf(i6)) + sb0(qf(i7)) \
201  		+ sb1(qf(i8)) + sb2(qf(i9)) + sb3(qf(i10)) + sb0(qf(i11)) \
202  		+ sb1(qf(i12)) + sb2(qf(i13)) + sb3(qf(i14)) + sb0(qf(i15)) \
203  		+ add_elt_b(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
204  #define expand1b(qf, mf, hf, i16) \
205  	expand1b_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
206  #define expand1b_(qf, mf, hf, i16, ix, iy) \
207  	expand1b_inner LPAR qf, mf, hf, i16, ix, iy)
208  #define expand2b_inner(qf, mf, hf, i16, \
209  		i0, i1, i2, i3, i4, i5, i6, i7, i8, \
210  		i9, i10, i11, i12, i13, i14, i15, \
211  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
212  	SPH_T64(qf(i0) + rb1(qf(i1)) + qf(i2) + rb2(qf(i3)) \
213  		+ qf(i4) + rb3(qf(i5)) + qf(i6) + rb4(qf(i7)) \
214  		+ qf(i8) + rb5(qf(i9)) + qf(i10) + rb6(qf(i11)) \
215  		+ qf(i12) + rb7(qf(i13)) + sb4(qf(i14)) + sb5(qf(i15)) \
216  		+ add_elt_b(mf, hf, i0m, i1m, i3m, i4m, i7m, i10m, i11m, i16))
217  #define expand2b(qf, mf, hf, i16) \
218  	expand2b_(qf, mf, hf, i16, I16_ ## i16, M16_ ## i16)
219  #define expand2b_(qf, mf, hf, i16, ix, iy) \
220  	expand2b_inner LPAR qf, mf, hf, i16, ix, iy)
221  #endif
222  #endif
223  #define MAKE_W(tt, i0, op01, i1, op12, i2, op23, i3, op34, i4) \
224  	tt((M(i0) ^ H(i0)) op01 (M(i1) ^ H(i1)) op12 (M(i2) ^ H(i2)) \
225  	op23 (M(i3) ^ H(i3)) op34 (M(i4) ^ H(i4)))
226  #if !defined(__AVX2__)
227  #define Ws0    MAKE_W(SPH_T32,  5, -,  7, +, 10, +, 13, +, 14)
228  #define Ws1    MAKE_W(SPH_T32,  6, -,  8, +, 11, +, 14, -, 15)
229  #define Ws2    MAKE_W(SPH_T32,  0, +,  7, +,  9, -, 12, +, 15)
230  #define Ws3    MAKE_W(SPH_T32,  0, -,  1, +,  8, -, 10, +, 13)
231  #define Ws4    MAKE_W(SPH_T32,  1, +,  2, +,  9, -, 11, -, 14)
232  #define Ws5    MAKE_W(SPH_T32,  3, -,  2, +, 10, -, 12, +, 15)
233  #define Ws6    MAKE_W(SPH_T32,  4, -,  0, -,  3, -, 11, +, 13)
234  #define Ws7    MAKE_W(SPH_T32,  1, -,  4, -,  5, -, 12, -, 14)
235  #define Ws8    MAKE_W(SPH_T32,  2, -,  5, -,  6, +, 13, -, 15)
236  #define Ws9    MAKE_W(SPH_T32,  0, -,  3, +,  6, -,  7, +, 14)
237  #define Ws10   MAKE_W(SPH_T32,  8, -,  1, -,  4, -,  7, +, 15)
238  #define Ws11   MAKE_W(SPH_T32,  8, -,  0, -,  2, -,  5, +,  9)
239  #define Ws12   MAKE_W(SPH_T32,  1, +,  3, -,  6, -,  9, +, 10)
240  #define Ws13   MAKE_W(SPH_T32,  2, +,  4, +,  7, +, 10, +, 11)
241  #define Ws14   MAKE_W(SPH_T32,  3, -,  5, +,  8, -, 11, -, 12)
242  #define Ws15   MAKE_W(SPH_T32, 12, -,  4, -,  6, -,  9, +, 13)
243  #if SPH_SMALL_FOOTPRINT_BMW
244  #define MAKE_Qas   do { \
245  		unsigned u; \
246  		sph_u32 Ws[16]; \
247  		Ws[ 0] = Ws0; \
248  		Ws[ 1] = Ws1; \
249  		Ws[ 2] = Ws2; \
250  		Ws[ 3] = Ws3; \
251  		Ws[ 4] = Ws4; \
252  		Ws[ 5] = Ws5; \
253  		Ws[ 6] = Ws6; \
254  		Ws[ 7] = Ws7; \
255  		Ws[ 8] = Ws8; \
256  		Ws[ 9] = Ws9; \
257  		Ws[10] = Ws10; \
258  		Ws[11] = Ws11; \
259  		Ws[12] = Ws12; \
260  		Ws[13] = Ws13; \
261  		Ws[14] = Ws14; \
262  		Ws[15] = Ws15; \
263  		for (u = 0; u < 15; u += 5) { \
264  			qt[u + 0] = SPH_T32(ss0(Ws[u + 0]) + H(u + 1)); \
265  			qt[u + 1] = SPH_T32(ss1(Ws[u + 1]) + H(u + 2)); \
266  			qt[u + 2] = SPH_T32(ss2(Ws[u + 2]) + H(u + 3)); \
267  			qt[u + 3] = SPH_T32(ss3(Ws[u + 3]) + H(u + 4)); \
268  			qt[u + 4] = SPH_T32(ss4(Ws[u + 4]) + H(u + 5)); \
269  		} \
270  		qt[15] = SPH_T32(ss0(Ws[15]) + H(0)); \
271  	} while (0)
272  #define MAKE_Qbs   do { \
273  		qt[16] = expand1s(Qs, M, H, 16); \
274  		qt[17] = expand1s(Qs, M, H, 17); \
275  		qt[18] = expand2s(Qs, M, H, 18); \
276  		qt[19] = expand2s(Qs, M, H, 19); \
277  		qt[20] = expand2s(Qs, M, H, 20); \
278  		qt[21] = expand2s(Qs, M, H, 21); \
279  		qt[22] = expand2s(Qs, M, H, 22); \
280  		qt[23] = expand2s(Qs, M, H, 23); \
281  		qt[24] = expand2s(Qs, M, H, 24); \
282  		qt[25] = expand2s(Qs, M, H, 25); \
283  		qt[26] = expand2s(Qs, M, H, 26); \
284  		qt[27] = expand2s(Qs, M, H, 27); \
285  		qt[28] = expand2s(Qs, M, H, 28); \
286  		qt[29] = expand2s(Qs, M, H, 29); \
287  		qt[30] = expand2s(Qs, M, H, 30); \
288  		qt[31] = expand2s(Qs, M, H, 31); \
289  	} while (0)
290  #else
291  #define MAKE_Qas   do { \
292  		qt[ 0] = SPH_T32(ss0(Ws0 ) + H( 1)); \
293  		qt[ 1] = SPH_T32(ss1(Ws1 ) + H( 2)); \
294  		qt[ 2] = SPH_T32(ss2(Ws2 ) + H( 3)); \
295  		qt[ 3] = SPH_T32(ss3(Ws3 ) + H( 4)); \
296  		qt[ 4] = SPH_T32(ss4(Ws4 ) + H( 5)); \
297  		qt[ 5] = SPH_T32(ss0(Ws5 ) + H( 6)); \
298  		qt[ 6] = SPH_T32(ss1(Ws6 ) + H( 7)); \
299  		qt[ 7] = SPH_T32(ss2(Ws7 ) + H( 8)); \
300  		qt[ 8] = SPH_T32(ss3(Ws8 ) + H( 9)); \
301  		qt[ 9] = SPH_T32(ss4(Ws9 ) + H(10)); \
302  		qt[10] = SPH_T32(ss0(Ws10) + H(11)); \
303  		qt[11] = SPH_T32(ss1(Ws11) + H(12)); \
304  		qt[12] = SPH_T32(ss2(Ws12) + H(13)); \
305  		qt[13] = SPH_T32(ss3(Ws13) + H(14)); \
306  		qt[14] = SPH_T32(ss4(Ws14) + H(15)); \
307  		qt[15] = SPH_T32(ss0(Ws15) + H( 0)); \
308  	} while (0)
309  #define MAKE_Qbs   do { \
310  		qt[16] = expand1s(Qs, M, H, 16); \
311  		qt[17] = expand1s(Qs, M, H, 17); \
312  		qt[18] = expand2s(Qs, M, H, 18); \
313  		qt[19] = expand2s(Qs, M, H, 19); \
314  		qt[20] = expand2s(Qs, M, H, 20); \
315  		qt[21] = expand2s(Qs, M, H, 21); \
316  		qt[22] = expand2s(Qs, M, H, 22); \
317  		qt[23] = expand2s(Qs, M, H, 23); \
318  		qt[24] = expand2s(Qs, M, H, 24); \
319  		qt[25] = expand2s(Qs, M, H, 25); \
320  		qt[26] = expand2s(Qs, M, H, 26); \
321  		qt[27] = expand2s(Qs, M, H, 27); \
322  		qt[28] = expand2s(Qs, M, H, 28); \
323  		qt[29] = expand2s(Qs, M, H, 29); \
324  		qt[30] = expand2s(Qs, M, H, 30); \
325  		qt[31] = expand2s(Qs, M, H, 31); \
326  	} while (0)
327  #endif
328  #define MAKE_Qs   do { \
329  		MAKE_Qas; \
330  		MAKE_Qbs; \
331  	} while (0)
332  #define Qs(j)   (qt[j])
333  #endif  
334  #if SPH_64
335  #define Wb0    MAKE_W(SPH_T64,  5, -,  7, +, 10, +, 13, +, 14)
336  #define Wb1    MAKE_W(SPH_T64,  6, -,  8, +, 11, +, 14, -, 15)
337  #define Wb2    MAKE_W(SPH_T64,  0, +,  7, +,  9, -, 12, +, 15)
338  #define Wb3    MAKE_W(SPH_T64,  0, -,  1, +,  8, -, 10, +, 13)
339  #define Wb4    MAKE_W(SPH_T64,  1, +,  2, +,  9, -, 11, -, 14)
340  #define Wb5    MAKE_W(SPH_T64,  3, -,  2, +, 10, -, 12, +, 15)
341  #define Wb6    MAKE_W(SPH_T64,  4, -,  0, -,  3, -, 11, +, 13)
342  #define Wb7    MAKE_W(SPH_T64,  1, -,  4, -,  5, -, 12, -, 14)
343  #define Wb8    MAKE_W(SPH_T64,  2, -,  5, -,  6, +, 13, -, 15)
344  #define Wb9    MAKE_W(SPH_T64,  0, -,  3, +,  6, -,  7, +, 14)
345  #define Wb10   MAKE_W(SPH_T64,  8, -,  1, -,  4, -,  7, +, 15)
346  #define Wb11   MAKE_W(SPH_T64,  8, -,  0, -,  2, -,  5, +,  9)
347  #define Wb12   MAKE_W(SPH_T64,  1, +,  3, -,  6, -,  9, +, 10)
348  #define Wb13   MAKE_W(SPH_T64,  2, +,  4, +,  7, +, 10, +, 11)
349  #define Wb14   MAKE_W(SPH_T64,  3, -,  5, +,  8, -, 11, -, 12)
350  #define Wb15   MAKE_W(SPH_T64, 12, -,  4, -,  6, -,  9, +, 13)
351  #if SPH_SMALL_FOOTPRINT_BMW
352  #define MAKE_Qab   do { \
353  		unsigned u; \
354  		sph_u64 Wb[16]; \
355  		Wb[ 0] = Wb0; \
356  		Wb[ 1] = Wb1; \
357  		Wb[ 2] = Wb2; \
358  		Wb[ 3] = Wb3; \
359  		Wb[ 4] = Wb4; \
360  		Wb[ 5] = Wb5; \
361  		Wb[ 6] = Wb6; \
362  		Wb[ 7] = Wb7; \
363  		Wb[ 8] = Wb8; \
364  		Wb[ 9] = Wb9; \
365  		Wb[10] = Wb10; \
366  		Wb[11] = Wb11; \
367  		Wb[12] = Wb12; \
368  		Wb[13] = Wb13; \
369  		Wb[14] = Wb14; \
370  		Wb[15] = Wb15; \
371  		for (u = 0; u < 15; u += 5) { \
372  			qt[u + 0] = SPH_T64(sb0(Wb[u + 0]) + H(u + 1)); \
373  			qt[u + 1] = SPH_T64(sb1(Wb[u + 1]) + H(u + 2)); \
374  			qt[u + 2] = SPH_T64(sb2(Wb[u + 2]) + H(u + 3)); \
375  			qt[u + 3] = SPH_T64(sb3(Wb[u + 3]) + H(u + 4)); \
376  			qt[u + 4] = SPH_T64(sb4(Wb[u + 4]) + H(u + 5)); \
377  		} \
378  		qt[15] = SPH_T64(sb0(Wb[15]) + H(0)); \
379  	} while (0)
380  #define MAKE_Qbb   do { \
381  		unsigned u; \
382  		for (u = 16; u < 18; u ++) \
383  			qt[u] = expand1b(Qb, M, H, u); \
384  		for (u = 18; u < 32; u ++) \
385  			qt[u] = expand2b(Qb, M, H, u); \
386  	} while (0)
387  #else
388  #define MAKE_Qab   do { \
389  		qt[ 0] = SPH_T64(sb0(Wb0 ) + H( 1)); \
390  		qt[ 1] = SPH_T64(sb1(Wb1 ) + H( 2)); \
391  		qt[ 2] = SPH_T64(sb2(Wb2 ) + H( 3)); \
392  		qt[ 3] = SPH_T64(sb3(Wb3 ) + H( 4)); \
393  		qt[ 4] = SPH_T64(sb4(Wb4 ) + H( 5)); \
394  		qt[ 5] = SPH_T64(sb0(Wb5 ) + H( 6)); \
395  		qt[ 6] = SPH_T64(sb1(Wb6 ) + H( 7)); \
396  		qt[ 7] = SPH_T64(sb2(Wb7 ) + H( 8)); \
397  		qt[ 8] = SPH_T64(sb3(Wb8 ) + H( 9)); \
398  		qt[ 9] = SPH_T64(sb4(Wb9 ) + H(10)); \
399  		qt[10] = SPH_T64(sb0(Wb10) + H(11)); \
400  		qt[11] = SPH_T64(sb1(Wb11) + H(12)); \
401  		qt[12] = SPH_T64(sb2(Wb12) + H(13)); \
402  		qt[13] = SPH_T64(sb3(Wb13) + H(14)); \
403  		qt[14] = SPH_T64(sb4(Wb14) + H(15)); \
404  		qt[15] = SPH_T64(sb0(Wb15) + H( 0)); \
405  	} while (0)
406  #define MAKE_Qbb   do { \
407  		qt[16] = expand1b(Qb, M, H, 16); \
408  		qt[17] = expand1b(Qb, M, H, 17); \
409  		qt[18] = expand2b(Qb, M, H, 18); \
410  		qt[19] = expand2b(Qb, M, H, 19); \
411  		qt[20] = expand2b(Qb, M, H, 20); \
412  		qt[21] = expand2b(Qb, M, H, 21); \
413  		qt[22] = expand2b(Qb, M, H, 22); \
414  		qt[23] = expand2b(Qb, M, H, 23); \
415  		qt[24] = expand2b(Qb, M, H, 24); \
416  		qt[25] = expand2b(Qb, M, H, 25); \
417  		qt[26] = expand2b(Qb, M, H, 26); \
418  		qt[27] = expand2b(Qb, M, H, 27); \
419  		qt[28] = expand2b(Qb, M, H, 28); \
420  		qt[29] = expand2b(Qb, M, H, 29); \
421  		qt[30] = expand2b(Qb, M, H, 30); \
422  		qt[31] = expand2b(Qb, M, H, 31); \
423  	} while (0)
424  #endif
425  #define MAKE_Qb   do { \
426  		MAKE_Qab; \
427  		MAKE_Qbb; \
428  	} while (0)
429  #define Qb(j)   (qt[j])
430  #endif
431  #define FOLD(type, mkQ, tt, rol, mf, qf, dhf)   do { \
432  		type qt[32], xl, xh; \
433  		mkQ; \
434  		xl = qf(16) ^ qf(17) ^ qf(18) ^ qf(19) \
435  			^ qf(20) ^ qf(21) ^ qf(22) ^ qf(23); \
436  		xh = xl ^ qf(24) ^ qf(25) ^ qf(26) ^ qf(27) \
437  			^ qf(28) ^ qf(29) ^ qf(30) ^ qf(31); \
438  		dhf( 0) = tt(((xh <<  5) ^ (qf(16) >>  5) ^ mf( 0)) \
439  			+ (xl ^ qf(24) ^ qf( 0))); \
440  		dhf( 1) = tt(((xh >>  7) ^ (qf(17) <<  8) ^ mf( 1)) \
441  			+ (xl ^ qf(25) ^ qf( 1))); \
442  		dhf( 2) = tt(((xh >>  5) ^ (qf(18) <<  5) ^ mf( 2)) \
443  			+ (xl ^ qf(26) ^ qf( 2))); \
444  		dhf( 3) = tt(((xh >>  1) ^ (qf(19) <<  5) ^ mf( 3)) \
445  			+ (xl ^ qf(27) ^ qf( 3))); \
446  		dhf( 4) = tt(((xh >>  3) ^ (qf(20) <<  0) ^ mf( 4)) \
447  			+ (xl ^ qf(28) ^ qf( 4))); \
448  		dhf( 5) = tt(((xh <<  6) ^ (qf(21) >>  6) ^ mf( 5)) \
449  			+ (xl ^ qf(29) ^ qf( 5))); \
450  		dhf( 6) = tt(((xh >>  4) ^ (qf(22) <<  6) ^ mf( 6)) \
451  			+ (xl ^ qf(30) ^ qf( 6))); \
452  		dhf( 7) = tt(((xh >> 11) ^ (qf(23) <<  2) ^ mf( 7)) \
453  			+ (xl ^ qf(31) ^ qf( 7))); \
454  		dhf( 8) = tt(rol(dhf(4),  9) + (xh ^ qf(24) ^ mf( 8)) \
455  			+ ((xl << 8) ^ qf(23) ^ qf( 8))); \
456  		dhf( 9) = tt(rol(dhf(5), 10) + (xh ^ qf(25) ^ mf( 9)) \
457  			+ ((xl >> 6) ^ qf(16) ^ qf( 9))); \
458  		dhf(10) = tt(rol(dhf(6), 11) + (xh ^ qf(26) ^ mf(10)) \
459  			+ ((xl << 6) ^ qf(17) ^ qf(10))); \
460  		dhf(11) = tt(rol(dhf(7), 12) + (xh ^ qf(27) ^ mf(11)) \
461  			+ ((xl << 4) ^ qf(18) ^ qf(11))); \
462  		dhf(12) = tt(rol(dhf(0), 13) + (xh ^ qf(28) ^ mf(12)) \
463  			+ ((xl >> 3) ^ qf(19) ^ qf(12))); \
464  		dhf(13) = tt(rol(dhf(1), 14) + (xh ^ qf(29) ^ mf(13)) \
465  			+ ((xl >> 4) ^ qf(20) ^ qf(13))); \
466  		dhf(14) = tt(rol(dhf(2), 15) + (xh ^ qf(30) ^ mf(14)) \
467  			+ ((xl >> 7) ^ qf(21) ^ qf(14))); \
468  		dhf(15) = tt(rol(dhf(3), 16) + (xh ^ qf(31) ^ mf(15)) \
469  			+ ((xl >> 2) ^ qf(22) ^ qf(15))); \
470  	} while (0)
471  #if SPH_64
472  #define FOLDb   FOLD(sph_u64, MAKE_Qb, SPH_T64, SPH_ROTL64, M, Qb, dH)
473  #endif
474  #if !defined(__AVX2__)
475  #define FOLDs   FOLD(sph_u32, MAKE_Qs, SPH_T32, SPH_ROTL32, M, Qs, dH)
476  static void
477  compress_small(const unsigned char *data, const sph_u32 h[16], sph_u32 dh[16])
478  {
479  #if SPH_LITTLE_FAST
480  #define M(x)    sph_dec32le_aligned(data + 4 * (x))
481  #else
482  	sph_u32 mv[16];
483  	mv[ 0] = sph_dec32le_aligned(data +  0);
484  	mv[ 1] = sph_dec32le_aligned(data +  4);
485  	mv[ 2] = sph_dec32le_aligned(data +  8);
486  	mv[ 3] = sph_dec32le_aligned(data + 12);
487  	mv[ 4] = sph_dec32le_aligned(data + 16);
488  	mv[ 5] = sph_dec32le_aligned(data + 20);
489  	mv[ 6] = sph_dec32le_aligned(data + 24);
490  	mv[ 7] = sph_dec32le_aligned(data + 28);
491  	mv[ 8] = sph_dec32le_aligned(data + 32);
492  	mv[ 9] = sph_dec32le_aligned(data + 36);
493  	mv[10] = sph_dec32le_aligned(data + 40);
494  	mv[11] = sph_dec32le_aligned(data + 44);
495  	mv[12] = sph_dec32le_aligned(data + 48);
496  	mv[13] = sph_dec32le_aligned(data + 52);
497  	mv[14] = sph_dec32le_aligned(data + 56);
498  	mv[15] = sph_dec32le_aligned(data + 60);
499  #define M(x)    (mv[x])
500  #endif
501  #define H(x)    (h[x])
502  #define dH(x)   (dh[x])
503  	FOLDs;
504  #undef M
505  #undef H
506  #undef dH
507  }
508  static const sph_u32 final_s[16] = {
509  	SPH_C32(0xaaaaaaa0), SPH_C32(0xaaaaaaa1), SPH_C32(0xaaaaaaa2),
510  	SPH_C32(0xaaaaaaa3), SPH_C32(0xaaaaaaa4), SPH_C32(0xaaaaaaa5),
511  	SPH_C32(0xaaaaaaa6), SPH_C32(0xaaaaaaa7), SPH_C32(0xaaaaaaa8),
512  	SPH_C32(0xaaaaaaa9), SPH_C32(0xaaaaaaaa), SPH_C32(0xaaaaaaab),
513  	SPH_C32(0xaaaaaaac), SPH_C32(0xaaaaaaad), SPH_C32(0xaaaaaaae),
514  	SPH_C32(0xaaaaaaaf)
515  };
516  static void
517  bmw32_init(sph_bmw_small_context *sc, const sph_u32 *iv)
518  {
519  	memcpy(sc->H, iv, sizeof sc->H);
520  	sc->ptr = 0;
521  #if SPH_64
522  	sc->bit_count = 0;
523  #else
524  	sc->bit_count_high = 0;
525  	sc->bit_count_low = 0;
526  #endif
527  }
528  static void
529  bmw32(sph_bmw_small_context *sc, const void *data, size_t len)
530  {
531  	unsigned char *buf;
532  	size_t ptr;
533  	sph_u32 htmp[16];
534  	sph_u32 *h1, *h2;
535  #if !SPH_64
536  	sph_u32 tmp;
537  #endif
538  #if SPH_64
539  	sc->bit_count += (sph_u64)len << 3;
540  #else
541  	tmp = sc->bit_count_low;
542  	sc->bit_count_low = SPH_T32(tmp + ((sph_u32)len << 3));
543  	if (sc->bit_count_low < tmp)
544  		sc->bit_count_high ++;
545  	sc->bit_count_high += len >> 29;
546  #endif
547  	buf = sc->buf;
548  	ptr = sc->ptr;
549  	h1 = sc->H;
550  	h2 = htmp;
551  	while (len > 0) {
552  		size_t clen;
553  		clen = (sizeof sc->buf) - ptr;
554  		if (clen > len)
555  			clen = len;
556  		memcpy(buf + ptr, data, clen);
557  		data = (const unsigned char *)data + clen;
558  		len -= clen;
559  		ptr += clen;
560  		if (ptr == sizeof sc->buf) {
561  			sph_u32 *ht;
562  			compress_small(buf, h1, h2);
563  			ht = h1;
564  			h1 = h2;
565  			h2 = ht;
566  			ptr = 0;
567  		}
568  	}
569  	sc->ptr = ptr;
570  	if (h1 != sc->H)
571  		memcpy(sc->H, h1, sizeof sc->H);
572  }
573  static void
574  bmw32_close(sph_bmw_small_context *sc, unsigned ub, unsigned n,
575  	void *dst, size_t out_size_w32)
576  {
577  	unsigned char *buf, *out;
578  	size_t ptr, u, v;
579  	unsigned z;
580  	sph_u32 h1[16], h2[16], *h;
581  	buf = sc->buf;
582  	ptr = sc->ptr;
583  	z = 0x80 >> n;
584  	buf[ptr ++] = ((ub & -z) | z) & 0xFF;
585  	h = sc->H;
586  	if (ptr > (sizeof sc->buf) - 8) {
587  		memset(buf + ptr, 0, (sizeof sc->buf) - ptr);
588  		compress_small(buf, h, h1);
589  		ptr = 0;
590  		h = h1;
591  	}
592  	memset(buf + ptr, 0, (sizeof sc->buf) - 8 - ptr);
593  #if SPH_64
594  	sph_enc64le_aligned(buf + (sizeof sc->buf) - 8,
595  		SPH_T64(sc->bit_count + n));
596  #else
597  	sph_enc32le_aligned(buf + (sizeof sc->buf) - 8,
598  		sc->bit_count_low + n);
599  	sph_enc32le_aligned(buf + (sizeof sc->buf) - 4,
600  		SPH_T32(sc->bit_count_high));
601  #endif
602  	compress_small(buf, h, h2);
603  	for (u = 0; u < 16; u ++)
604  		sph_enc32le_aligned(buf + 4 * u, h2[u]);
605  	compress_small(buf, final_s, h1);
606  	out = dst;
607  	for (u = 0, v = 16 - out_size_w32; u < out_size_w32; u ++, v ++)
608  		sph_enc32le(out + 4 * u, h1[v]);
609  }
610  #endif 
611  #if SPH_64
612  static void
613  compress_big(const unsigned char *data, const sph_u64 h[16], sph_u64 dh[16])
614  {
615  #if SPH_LITTLE_FAST
616  #define M(x)    sph_dec64le_aligned(data + 8 * (x))
617  #else
618  	sph_u64 mv[16];
619  	mv[ 0] = sph_dec64le_aligned(data +   0);
620  	mv[ 1] = sph_dec64le_aligned(data +   8);
621  	mv[ 2] = sph_dec64le_aligned(data +  16);
622  	mv[ 3] = sph_dec64le_aligned(data +  24);
623  	mv[ 4] = sph_dec64le_aligned(data +  32);
624  	mv[ 5] = sph_dec64le_aligned(data +  40);
625  	mv[ 6] = sph_dec64le_aligned(data +  48);
626  	mv[ 7] = sph_dec64le_aligned(data +  56);
627  	mv[ 8] = sph_dec64le_aligned(data +  64);
628  	mv[ 9] = sph_dec64le_aligned(data +  72);
629  	mv[10] = sph_dec64le_aligned(data +  80);
630  	mv[11] = sph_dec64le_aligned(data +  88);
631  	mv[12] = sph_dec64le_aligned(data +  96);
632  	mv[13] = sph_dec64le_aligned(data + 104);
633  	mv[14] = sph_dec64le_aligned(data + 112);
634  	mv[15] = sph_dec64le_aligned(data + 120);
635  #define M(x)    (mv[x])
636  #endif
637  #define H(x)    (h[x])
638  #define dH(x)   (dh[x])
639  	FOLDb;
640  #undef M
641  #undef H
642  #undef dH
643  }
644  static const sph_u64 final_b[16] = {
645  	SPH_C64(0xaaaaaaaaaaaaaaa0), SPH_C64(0xaaaaaaaaaaaaaaa1),
646  	SPH_C64(0xaaaaaaaaaaaaaaa2), SPH_C64(0xaaaaaaaaaaaaaaa3),
647  	SPH_C64(0xaaaaaaaaaaaaaaa4), SPH_C64(0xaaaaaaaaaaaaaaa5),
648  	SPH_C64(0xaaaaaaaaaaaaaaa6), SPH_C64(0xaaaaaaaaaaaaaaa7),
649  	SPH_C64(0xaaaaaaaaaaaaaaa8), SPH_C64(0xaaaaaaaaaaaaaaa9),
650  	SPH_C64(0xaaaaaaaaaaaaaaaa), SPH_C64(0xaaaaaaaaaaaaaaab),
651  	SPH_C64(0xaaaaaaaaaaaaaaac), SPH_C64(0xaaaaaaaaaaaaaaad),
652  	SPH_C64(0xaaaaaaaaaaaaaaae), SPH_C64(0xaaaaaaaaaaaaaaaf)
653  };
654  static void
655  bmw64_init(sph_bmw_big_context *sc, const sph_u64 *iv)
656  {
657  	memcpy(sc->H, iv, sizeof sc->H);
658  	sc->ptr = 0;
659  	sc->bit_count = 0;
660  }
661  static void
662  bmw64(sph_bmw_big_context *sc, const void *data, size_t len)
663  {
664  	unsigned char *buf;
665  	size_t ptr;
666  	sph_u64 htmp[16];
667  	sph_u64 *h1, *h2;
668  	sc->bit_count += (sph_u64)len << 3;
669  	buf = sc->buf;
670  	ptr = sc->ptr;
671  	h1 = sc->H;
672  	h2 = htmp;
673  	while (len > 0) {
674  		size_t clen;
675  		clen = (sizeof sc->buf) - ptr;
676  		if (clen > len)
677  			clen = len;
678  		memcpy(buf + ptr, data, clen);
679  		data = (const unsigned char *)data + clen;
680  		len -= clen;
681  		ptr += clen;
682  		if (ptr == sizeof sc->buf) {
683  			sph_u64 *ht;
684  			compress_big(buf, h1, h2);
685  			ht = h1;
686  			h1 = h2;
687  			h2 = ht;
688  			ptr = 0;
689  		}
690  	}
691  	sc->ptr = ptr;
692  	if (h1 != sc->H)
693  		memcpy(sc->H, h1, sizeof sc->H);
694  }
695  static void
696  bmw64_close(sph_bmw_big_context *sc, unsigned ub, unsigned n,
697  	void *dst, size_t out_size_w64)
698  {
699  	unsigned char *buf, *out;
700  	size_t ptr, u, v;
701  	unsigned z;
702  	sph_u64 h1[16], h2[16], *h;
703  	buf = sc->buf;
704  	ptr = sc->ptr;
705  	z = 0x80 >> n;
706  	buf[ptr ++] = ((ub & -z) | z) & 0xFF;
707  	h = sc->H;
708  	if (ptr > (sizeof sc->buf) - 8) {
709  		memset(buf + ptr, 0, (sizeof sc->buf) - ptr);
710  		compress_big(buf, h, h1);
711  		ptr = 0;
712  		h = h1;
713  	}
714  	memset(buf + ptr, 0, (sizeof sc->buf) - 8 - ptr);
715  	sph_enc64le_aligned(buf + (sizeof sc->buf) - 8,
716  		SPH_T64(sc->bit_count + n));
717  	compress_big(buf, h, h2);
718  	for (u = 0; u < 16; u ++)
719  		sph_enc64le_aligned(buf + 8 * u, h2[u]);
720  	compress_big(buf, final_b, h1);
721  	out = dst;
722  	for (u = 0, v = 16 - out_size_w64; u < out_size_w64; u ++, v ++)
723  		sph_enc64le(out + 8 * u, h1[v]);
724  }
725  #endif
726  #if !defined(__AVX2__)
727  void
728  sph_bmw224_init(void *cc)
729  {
730  	bmw32_init(cc, IV224);
731  }
732  void
733  sph_bmw224(void *cc, const void *data, size_t len)
734  {
735  	bmw32(cc, data, len);
736  }
737  void
738  sph_bmw224_close(void *cc, void *dst)
739  {
740  	sph_bmw224_addbits_and_close(cc, 0, 0, dst);
741  }
742  void
743  sph_bmw224_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
744  {
745  	bmw32_close(cc, ub, n, dst, 7);
746  }
747  void
748  sph_bmw256_init(void *cc)
749  {
750  	bmw32_init(cc, IV256);
751  }
752  void
753  sph_bmw256(void *cc, const void *data, size_t len)
754  {
755  	bmw32(cc, data, len);
756  }
757  void
758  sph_bmw256_close(void *cc, void *dst)
759  {
760  	sph_bmw256_addbits_and_close(cc, 0, 0, dst);
761  }
762  void
763  sph_bmw256_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
764  {
765  	bmw32_close(cc, ub, n, dst, 8);
766  }
767  #endif 
768  #if SPH_64
769  void
770  sph_bmw384_init(void *cc)
771  {
772  	bmw64_init(cc, IV384);
773  }
774  void
775  sph_bmw384(void *cc, const void *data, size_t len)
776  {
777  	bmw64(cc, data, len);
778  }
779  void
780  sph_bmw384_close(void *cc, void *dst)
781  {
782  	sph_bmw384_addbits_and_close(cc, 0, 0, dst);
783  }
784  void
785  sph_bmw384_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
786  {
787  	bmw64_close(cc, ub, n, dst, 6);
788  }
789  void
790  sph_bmw512_init(void *cc)
791  {
792  	bmw64_init(cc, IV512);
793  }
794  void
795  sph_bmw512(void *cc, const void *data, size_t len)
796  {
797  	bmw64(cc, data, len);
798  }
799  void
800  sph_bmw512_close(void *cc, void *dst)
801  {
802  	sph_bmw512_addbits_and_close(cc, 0, 0, dst);
803  }
804  void
805  sph_bmw512_addbits_and_close(void *cc, unsigned ub, unsigned n, void *dst)
806  {
807  	bmw64_close(cc, ub, n, dst, 8);
808  }
809  #endif
810  #ifdef __cplusplus
811  }
812  #endif
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_bmw.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from xmrig-MDEwOlJlcG9zaXRvcnk4ODMyNzQwNg==-flat-sph_bmw.c</div>
                </div>
                <div class="column column_space"><pre><code>197  		i9, i10, i11, i12, i13, i14, i15, \
198  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
199  	SPH_T64(sb1(qf(i0)) + sb2(qf(i1)) + sb3(qf(i2)) + sb0(qf(i3)) \
</pre></code></div>
                <div class="column column_space"><pre><code>117  		i9, i10, i11, i12, i13, i14, i15, \
118  		i0m, i1m, i3m, i4m, i7m, i10m, i11m) \
119  	SPH_T32(ss1(qf(i0)) + ss2(qf(i1)) + ss3(qf(i2)) + ss0(qf(i3)) \
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    