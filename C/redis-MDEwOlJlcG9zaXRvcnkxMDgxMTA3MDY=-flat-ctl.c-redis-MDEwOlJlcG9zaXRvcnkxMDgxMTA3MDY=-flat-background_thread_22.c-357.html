
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 15, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-ctl.c</h3>
            <pre><code>1  #define JEMALLOC_CTL_C_
2  #include &quot;jemalloc/internal/jemalloc_preamble.h&quot;
3  #include &quot;jemalloc/internal/jemalloc_internal_includes.h&quot;
4  #include &quot;jemalloc/internal/assert.h&quot;
5  #include &quot;jemalloc/internal/ctl.h&quot;
6  #include &quot;jemalloc/internal/extent_dss.h&quot;
7  #include &quot;jemalloc/internal/extent_mmap.h&quot;
8  #include &quot;jemalloc/internal/mutex.h&quot;
9  #include &quot;jemalloc/internal/nstime.h&quot;
10  #include &quot;jemalloc/internal/sc.h&quot;
11  #include &quot;jemalloc/internal/util.h&quot;
12  static malloc_mutex_t	ctl_mtx;
13  static bool		ctl_initialized;
14  static ctl_stats_t	*ctl_stats;
15  static ctl_arenas_t	*ctl_arenas;
16  static const ctl_named_node_t *
17  ctl_named_node(const ctl_node_t *node) {
18  	return ((node-&gt;named) ? (const ctl_named_node_t *)node : NULL);
19  }
20  static const ctl_named_node_t *
21  ctl_named_children(const ctl_named_node_t *node, size_t index) {
22  	const ctl_named_node_t *children = ctl_named_node(node-&gt;children);
23  	return (children ? &amp;children[index] : NULL);
24  }
25  static const ctl_indexed_node_t *
26  ctl_indexed_node(const ctl_node_t *node) {
27  	return (!node-&gt;named ? (const ctl_indexed_node_t *)node : NULL);
28  }
29  #define CTL_PROTO(n)							\
30  static int	n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,	\
31      void *oldp, size_t *oldlenp, void *newp, size_t newlen);
32  #define INDEX_PROTO(n)							\
33  static const ctl_named_node_t	*n##_index(tsdn_t *tsdn,		\
34      const size_t *mib, size_t miblen, size_t i);
35  CTL_PROTO(version)
36  CTL_PROTO(epoch)
37  CTL_PROTO(background_thread)
38  CTL_PROTO(max_background_threads)
39  CTL_PROTO(thread_tcache_enabled)
40  CTL_PROTO(thread_tcache_flush)
41  CTL_PROTO(thread_prof_name)
42  CTL_PROTO(thread_prof_active)
43  CTL_PROTO(thread_arena)
44  CTL_PROTO(thread_allocated)
45  CTL_PROTO(thread_allocatedp)
46  CTL_PROTO(thread_deallocated)
47  CTL_PROTO(thread_deallocatedp)
48  CTL_PROTO(config_cache_oblivious)
49  CTL_PROTO(config_debug)
50  CTL_PROTO(config_fill)
51  CTL_PROTO(config_lazy_lock)
52  CTL_PROTO(config_malloc_conf)
53  CTL_PROTO(config_opt_safety_checks)
54  CTL_PROTO(config_prof)
55  CTL_PROTO(config_prof_libgcc)
56  CTL_PROTO(config_prof_libunwind)
57  CTL_PROTO(config_stats)
58  CTL_PROTO(config_utrace)
59  CTL_PROTO(config_xmalloc)
60  CTL_PROTO(opt_abort)
61  CTL_PROTO(opt_abort_conf)
62  CTL_PROTO(opt_confirm_conf)
63  CTL_PROTO(opt_metadata_thp)
64  CTL_PROTO(opt_retain)
65  CTL_PROTO(opt_dss)
66  CTL_PROTO(opt_narenas)
67  CTL_PROTO(opt_percpu_arena)
68  CTL_PROTO(opt_oversize_threshold)
69  CTL_PROTO(opt_background_thread)
70  CTL_PROTO(opt_max_background_threads)
71  CTL_PROTO(opt_dirty_decay_ms)
72  CTL_PROTO(opt_muzzy_decay_ms)
73  CTL_PROTO(opt_stats_print)
74  CTL_PROTO(opt_stats_print_opts)
75  CTL_PROTO(opt_junk)
76  CTL_PROTO(opt_zero)
77  CTL_PROTO(opt_utrace)
78  CTL_PROTO(opt_xmalloc)
79  CTL_PROTO(opt_tcache)
80  CTL_PROTO(opt_thp)
81  CTL_PROTO(opt_lg_extent_max_active_fit)
82  CTL_PROTO(opt_lg_tcache_max)
83  CTL_PROTO(opt_prof)
84  CTL_PROTO(opt_prof_prefix)
85  CTL_PROTO(opt_prof_active)
86  CTL_PROTO(opt_prof_thread_active_init)
87  CTL_PROTO(opt_lg_prof_sample)
88  CTL_PROTO(opt_lg_prof_interval)
89  CTL_PROTO(opt_prof_gdump)
90  CTL_PROTO(opt_prof_final)
91  CTL_PROTO(opt_prof_leak)
92  CTL_PROTO(opt_prof_accum)
93  CTL_PROTO(tcache_create)
94  CTL_PROTO(tcache_flush)
95  CTL_PROTO(tcache_destroy)
96  CTL_PROTO(arena_i_initialized)
97  CTL_PROTO(arena_i_decay)
98  CTL_PROTO(arena_i_purge)
99  CTL_PROTO(arena_i_reset)
100  CTL_PROTO(arena_i_destroy)
101  CTL_PROTO(arena_i_dss)
102  CTL_PROTO(arena_i_dirty_decay_ms)
103  CTL_PROTO(arena_i_muzzy_decay_ms)
104  CTL_PROTO(arena_i_extent_hooks)
105  CTL_PROTO(arena_i_retain_grow_limit)
106  INDEX_PROTO(arena_i)
107  CTL_PROTO(arenas_bin_i_size)
108  CTL_PROTO(arenas_bin_i_nregs)
109  CTL_PROTO(arenas_bin_i_slab_size)
110  CTL_PROTO(arenas_bin_i_nshards)
111  INDEX_PROTO(arenas_bin_i)
112  CTL_PROTO(arenas_lextent_i_size)
113  INDEX_PROTO(arenas_lextent_i)
114  CTL_PROTO(arenas_narenas)
115  CTL_PROTO(arenas_dirty_decay_ms)
116  CTL_PROTO(arenas_muzzy_decay_ms)
117  CTL_PROTO(arenas_quantum)
118  CTL_PROTO(arenas_page)
119  CTL_PROTO(arenas_tcache_max)
120  CTL_PROTO(arenas_nbins)
121  CTL_PROTO(arenas_nhbins)
122  CTL_PROTO(arenas_nlextents)
123  CTL_PROTO(arenas_create)
124  CTL_PROTO(arenas_lookup)
125  CTL_PROTO(prof_thread_active_init)
126  CTL_PROTO(prof_active)
127  CTL_PROTO(prof_dump)
128  CTL_PROTO(prof_gdump)
129  CTL_PROTO(prof_reset)
130  CTL_PROTO(prof_interval)
131  CTL_PROTO(lg_prof_sample)
132  CTL_PROTO(prof_log_start)
133  CTL_PROTO(prof_log_stop)
134  CTL_PROTO(stats_arenas_i_small_allocated)
135  CTL_PROTO(stats_arenas_i_small_nmalloc)
136  CTL_PROTO(stats_arenas_i_small_ndalloc)
137  CTL_PROTO(stats_arenas_i_small_nrequests)
138  CTL_PROTO(stats_arenas_i_small_nfills)
139  CTL_PROTO(stats_arenas_i_small_nflushes)
140  CTL_PROTO(stats_arenas_i_large_allocated)
141  CTL_PROTO(stats_arenas_i_large_nmalloc)
142  CTL_PROTO(stats_arenas_i_large_ndalloc)
143  CTL_PROTO(stats_arenas_i_large_nrequests)
144  CTL_PROTO(stats_arenas_i_large_nfills)
145  CTL_PROTO(stats_arenas_i_large_nflushes)
146  CTL_PROTO(stats_arenas_i_bins_j_nmalloc)
147  CTL_PROTO(stats_arenas_i_bins_j_ndalloc)
148  CTL_PROTO(stats_arenas_i_bins_j_nrequests)
149  CTL_PROTO(stats_arenas_i_bins_j_curregs)
150  CTL_PROTO(stats_arenas_i_bins_j_nfills)
151  CTL_PROTO(stats_arenas_i_bins_j_nflushes)
152  CTL_PROTO(stats_arenas_i_bins_j_nslabs)
153  CTL_PROTO(stats_arenas_i_bins_j_nreslabs)
154  CTL_PROTO(stats_arenas_i_bins_j_curslabs)
155  CTL_PROTO(stats_arenas_i_bins_j_nonfull_slabs)
156  INDEX_PROTO(stats_arenas_i_bins_j)
157  CTL_PROTO(stats_arenas_i_lextents_j_nmalloc)
158  CTL_PROTO(stats_arenas_i_lextents_j_ndalloc)
159  CTL_PROTO(stats_arenas_i_lextents_j_nrequests)
160  CTL_PROTO(stats_arenas_i_lextents_j_curlextents)
161  INDEX_PROTO(stats_arenas_i_lextents_j)
162  CTL_PROTO(stats_arenas_i_extents_j_ndirty)
163  CTL_PROTO(stats_arenas_i_extents_j_nmuzzy)
164  CTL_PROTO(stats_arenas_i_extents_j_nretained)
165  CTL_PROTO(stats_arenas_i_extents_j_dirty_bytes)
166  CTL_PROTO(stats_arenas_i_extents_j_muzzy_bytes)
167  CTL_PROTO(stats_arenas_i_extents_j_retained_bytes)
168  INDEX_PROTO(stats_arenas_i_extents_j)
169  CTL_PROTO(stats_arenas_i_nthreads)
170  CTL_PROTO(stats_arenas_i_uptime)
171  CTL_PROTO(stats_arenas_i_dss)
172  CTL_PROTO(stats_arenas_i_dirty_decay_ms)
173  CTL_PROTO(stats_arenas_i_muzzy_decay_ms)
174  CTL_PROTO(stats_arenas_i_pactive)
175  CTL_PROTO(stats_arenas_i_pdirty)
176  CTL_PROTO(stats_arenas_i_pmuzzy)
177  CTL_PROTO(stats_arenas_i_mapped)
178  CTL_PROTO(stats_arenas_i_retained)
179  CTL_PROTO(stats_arenas_i_extent_avail)
180  CTL_PROTO(stats_arenas_i_dirty_npurge)
181  CTL_PROTO(stats_arenas_i_dirty_nmadvise)
182  CTL_PROTO(stats_arenas_i_dirty_purged)
183  CTL_PROTO(stats_arenas_i_muzzy_npurge)
184  CTL_PROTO(stats_arenas_i_muzzy_nmadvise)
185  CTL_PROTO(stats_arenas_i_muzzy_purged)
186  CTL_PROTO(stats_arenas_i_base)
187  CTL_PROTO(stats_arenas_i_internal)
188  CTL_PROTO(stats_arenas_i_metadata_thp)
189  CTL_PROTO(stats_arenas_i_tcache_bytes)
190  CTL_PROTO(stats_arenas_i_resident)
191  CTL_PROTO(stats_arenas_i_abandoned_vm)
192  INDEX_PROTO(stats_arenas_i)
193  CTL_PROTO(stats_allocated)
194  CTL_PROTO(stats_active)
195  CTL_PROTO(stats_background_thread_num_threads)
196  CTL_PROTO(stats_background_thread_num_runs)
197  CTL_PROTO(stats_background_thread_run_interval)
198  CTL_PROTO(stats_metadata)
199  CTL_PROTO(stats_metadata_thp)
200  CTL_PROTO(stats_resident)
201  CTL_PROTO(stats_mapped)
202  CTL_PROTO(stats_retained)
203  CTL_PROTO(experimental_hooks_install)
204  CTL_PROTO(experimental_hooks_remove)
205  CTL_PROTO(experimental_utilization_query)
206  CTL_PROTO(experimental_utilization_batch_query)
207  CTL_PROTO(experimental_arenas_i_pactivep)
208  INDEX_PROTO(experimental_arenas_i)
209  #define MUTEX_STATS_CTL_PROTO_GEN(n)					\
210  CTL_PROTO(stats_##n##_num_ops)						\
211  CTL_PROTO(stats_##n##_num_wait)						\
212  CTL_PROTO(stats_##n##_num_spin_acq)					\
213  CTL_PROTO(stats_##n##_num_owner_switch)					\
214  CTL_PROTO(stats_##n##_total_wait_time)					\
215  CTL_PROTO(stats_##n##_max_wait_time)					\
216  CTL_PROTO(stats_##n##_max_num_thds)
217  #define OP(mtx) MUTEX_STATS_CTL_PROTO_GEN(mutexes_##mtx)
218  MUTEX_PROF_GLOBAL_MUTEXES
219  #undef OP
220  #define OP(mtx) MUTEX_STATS_CTL_PROTO_GEN(arenas_i_mutexes_##mtx)
221  MUTEX_PROF_ARENA_MUTEXES
222  #undef OP
223  MUTEX_STATS_CTL_PROTO_GEN(arenas_i_bins_j_mutex)
224  #undef MUTEX_STATS_CTL_PROTO_GEN
225  CTL_PROTO(stats_mutexes_reset)
226  #define NAME(n)	{true},	n
227  #define CHILD(t, c)							\
228  	sizeof(c##_node) / sizeof(ctl_##t##_node_t),			\
229  	(ctl_node_t *)c##_node,						\
230  	NULL
231  #define CTL(c)	0, NULL, c##_ctl
232  #define INDEX(i)	{false},	i##_index
233  static const ctl_named_node_t	thread_tcache_node[] = {
234  	{NAME(&quot;enabled&quot;),	CTL(thread_tcache_enabled)},
235  	{NAME(&quot;flush&quot;),		CTL(thread_tcache_flush)}
236  };
237  static const ctl_named_node_t	thread_prof_node[] = {
238  	{NAME(&quot;name&quot;),		CTL(thread_prof_name)},
239  	{NAME(&quot;active&quot;),	CTL(thread_prof_active)}
240  };
241  static const ctl_named_node_t	thread_node[] = {
242  	{NAME(&quot;arena&quot;),		CTL(thread_arena)},
243  	{NAME(&quot;allocated&quot;),	CTL(thread_allocated)},
244  	{NAME(&quot;allocatedp&quot;),	CTL(thread_allocatedp)},
245  	{NAME(&quot;deallocated&quot;),	CTL(thread_deallocated)},
246  	{NAME(&quot;deallocatedp&quot;),	CTL(thread_deallocatedp)},
247  	{NAME(&quot;tcache&quot;),	CHILD(named, thread_tcache)},
248  	{NAME(&quot;prof&quot;),		CHILD(named, thread_prof)}
249  };
250  static const ctl_named_node_t	config_node[] = {
251  	{NAME(&quot;cache_oblivious&quot;), CTL(config_cache_oblivious)},
252  	{NAME(&quot;debug&quot;),		CTL(config_debug)},
253  	{NAME(&quot;fill&quot;),		CTL(config_fill)},
254  	{NAME(&quot;lazy_lock&quot;),	CTL(config_lazy_lock)},
255  	{NAME(&quot;malloc_conf&quot;),	CTL(config_malloc_conf)},
256  	{NAME(&quot;opt_safety_checks&quot;),	CTL(config_opt_safety_checks)},
257  	{NAME(&quot;prof&quot;),		CTL(config_prof)},
258  	{NAME(&quot;prof_libgcc&quot;),	CTL(config_prof_libgcc)},
259  	{NAME(&quot;prof_libunwind&quot;), CTL(config_prof_libunwind)},
260  	{NAME(&quot;stats&quot;),		CTL(config_stats)},
261  	{NAME(&quot;utrace&quot;),	CTL(config_utrace)},
262  	{NAME(&quot;xmalloc&quot;),	CTL(config_xmalloc)}
263  };
264  static const ctl_named_node_t opt_node[] = {
265  	{NAME(&quot;abort&quot;),		CTL(opt_abort)},
266  	{NAME(&quot;abort_conf&quot;),	CTL(opt_abort_conf)},
267  	{NAME(&quot;confirm_conf&quot;),	CTL(opt_confirm_conf)},
268  	{NAME(&quot;metadata_thp&quot;),	CTL(opt_metadata_thp)},
269  	{NAME(&quot;retain&quot;),	CTL(opt_retain)},
270  	{NAME(&quot;dss&quot;),		CTL(opt_dss)},
271  	{NAME(&quot;narenas&quot;),	CTL(opt_narenas)},
272  	{NAME(&quot;percpu_arena&quot;),	CTL(opt_percpu_arena)},
273  	{NAME(&quot;oversize_threshold&quot;),	CTL(opt_oversize_threshold)},
274  	{NAME(&quot;background_thread&quot;),	CTL(opt_background_thread)},
275  	{NAME(&quot;max_background_threads&quot;),	CTL(opt_max_background_threads)},
276  	{NAME(&quot;dirty_decay_ms&quot;), CTL(opt_dirty_decay_ms)},
277  	{NAME(&quot;muzzy_decay_ms&quot;), CTL(opt_muzzy_decay_ms)},
278  	{NAME(&quot;stats_print&quot;),	CTL(opt_stats_print)},
279  	{NAME(&quot;stats_print_opts&quot;),	CTL(opt_stats_print_opts)},
280  	{NAME(&quot;junk&quot;),		CTL(opt_junk)},
281  	{NAME(&quot;zero&quot;),		CTL(opt_zero)},
282  	{NAME(&quot;utrace&quot;),	CTL(opt_utrace)},
283  	{NAME(&quot;xmalloc&quot;),	CTL(opt_xmalloc)},
284  	{NAME(&quot;tcache&quot;),	CTL(opt_tcache)},
285  	{NAME(&quot;thp&quot;),		CTL(opt_thp)},
286  	{NAME(&quot;lg_extent_max_active_fit&quot;), CTL(opt_lg_extent_max_active_fit)},
287  	{NAME(&quot;lg_tcache_max&quot;),	CTL(opt_lg_tcache_max)},
288  	{NAME(&quot;prof&quot;),		CTL(opt_prof)},
289  	{NAME(&quot;prof_prefix&quot;),	CTL(opt_prof_prefix)},
290  	{NAME(&quot;prof_active&quot;),	CTL(opt_prof_active)},
291  	{NAME(&quot;prof_thread_active_init&quot;), CTL(opt_prof_thread_active_init)},
292  	{NAME(&quot;lg_prof_sample&quot;), CTL(opt_lg_prof_sample)},
293  	{NAME(&quot;lg_prof_interval&quot;), CTL(opt_lg_prof_interval)},
294  	{NAME(&quot;prof_gdump&quot;),	CTL(opt_prof_gdump)},
295  	{NAME(&quot;prof_final&quot;),	CTL(opt_prof_final)},
296  	{NAME(&quot;prof_leak&quot;),	CTL(opt_prof_leak)},
297  	{NAME(&quot;prof_accum&quot;),	CTL(opt_prof_accum)}
298  };
299  static const ctl_named_node_t	tcache_node[] = {
300  	{NAME(&quot;create&quot;),	CTL(tcache_create)},
301  	{NAME(&quot;flush&quot;),		CTL(tcache_flush)},
302  	{NAME(&quot;destroy&quot;),	CTL(tcache_destroy)}
303  };
304  static const ctl_named_node_t arena_i_node[] = {
305  	{NAME(&quot;initialized&quot;),	CTL(arena_i_initialized)},
306  	{NAME(&quot;decay&quot;),		CTL(arena_i_decay)},
307  	{NAME(&quot;purge&quot;),		CTL(arena_i_purge)},
308  	{NAME(&quot;reset&quot;),		CTL(arena_i_reset)},
309  	{NAME(&quot;destroy&quot;),	CTL(arena_i_destroy)},
310  	{NAME(&quot;dss&quot;),		CTL(arena_i_dss)},
311  	{NAME(&quot;dirty_decay_ms&quot;), CTL(arena_i_dirty_decay_ms)},
312  	{NAME(&quot;muzzy_decay_ms&quot;), CTL(arena_i_muzzy_decay_ms)},
313  	{NAME(&quot;extent_hooks&quot;),	CTL(arena_i_extent_hooks)},
314  	{NAME(&quot;retain_grow_limit&quot;),	CTL(arena_i_retain_grow_limit)}
315  };
316  static const ctl_named_node_t super_arena_i_node[] = {
317  	{NAME(&quot;&quot;),		CHILD(named, arena_i)}
318  };
319  static const ctl_indexed_node_t arena_node[] = {
320  	{INDEX(arena_i)}
321  };
322  static const ctl_named_node_t arenas_bin_i_node[] = {
323  	{NAME(&quot;size&quot;),		CTL(arenas_bin_i_size)},
324  	{NAME(&quot;nregs&quot;),		CTL(arenas_bin_i_nregs)},
325  	{NAME(&quot;slab_size&quot;),	CTL(arenas_bin_i_slab_size)},
326  	{NAME(&quot;nshards&quot;),	CTL(arenas_bin_i_nshards)}
327  };
328  static const ctl_named_node_t super_arenas_bin_i_node[] = {
329  	{NAME(&quot;&quot;),		CHILD(named, arenas_bin_i)}
330  };
331  static const ctl_indexed_node_t arenas_bin_node[] = {
332  	{INDEX(arenas_bin_i)}
333  };
334  static const ctl_named_node_t arenas_lextent_i_node[] = {
335  	{NAME(&quot;size&quot;),		CTL(arenas_lextent_i_size)}
336  };
337  static const ctl_named_node_t super_arenas_lextent_i_node[] = {
338  	{NAME(&quot;&quot;),		CHILD(named, arenas_lextent_i)}
339  };
340  static const ctl_indexed_node_t arenas_lextent_node[] = {
341  	{INDEX(arenas_lextent_i)}
342  };
343  static const ctl_named_node_t arenas_node[] = {
344  	{NAME(&quot;narenas&quot;),	CTL(arenas_narenas)},
345  	{NAME(&quot;dirty_decay_ms&quot;), CTL(arenas_dirty_decay_ms)},
346  	{NAME(&quot;muzzy_decay_ms&quot;), CTL(arenas_muzzy_decay_ms)},
347  	{NAME(&quot;quantum&quot;),	CTL(arenas_quantum)},
348  	{NAME(&quot;page&quot;),		CTL(arenas_page)},
349  	{NAME(&quot;tcache_max&quot;),	CTL(arenas_tcache_max)},
350  	{NAME(&quot;nbins&quot;),		CTL(arenas_nbins)},
351  	{NAME(&quot;nhbins&quot;),	CTL(arenas_nhbins)},
352  	{NAME(&quot;bin&quot;),		CHILD(indexed, arenas_bin)},
353  	{NAME(&quot;nlextents&quot;),	CTL(arenas_nlextents)},
354  	{NAME(&quot;lextent&quot;),	CHILD(indexed, arenas_lextent)},
355  	{NAME(&quot;create&quot;),	CTL(arenas_create)},
356  	{NAME(&quot;lookup&quot;),	CTL(arenas_lookup)}
357  };
358  static const ctl_named_node_t	prof_node[] = {
359  	{NAME(&quot;thread_active_init&quot;), CTL(prof_thread_active_init)},
360  	{NAME(&quot;active&quot;),	CTL(prof_active)},
361  	{NAME(&quot;dump&quot;),		CTL(prof_dump)},
362  	{NAME(&quot;gdump&quot;),		CTL(prof_gdump)},
363  	{NAME(&quot;reset&quot;),		CTL(prof_reset)},
364  	{NAME(&quot;interval&quot;),	CTL(prof_interval)},
365  	{NAME(&quot;lg_sample&quot;),	CTL(lg_prof_sample)},
366  	{NAME(&quot;log_start&quot;),	CTL(prof_log_start)},
367  	{NAME(&quot;log_stop&quot;),	CTL(prof_log_stop)}
368  };
369  static const ctl_named_node_t stats_arenas_i_small_node[] = {
370  	{NAME(&quot;allocated&quot;),	CTL(stats_arenas_i_small_allocated)},
371  	{NAME(&quot;nmalloc&quot;),	CTL(stats_arenas_i_small_nmalloc)},
372  	{NAME(&quot;ndalloc&quot;),	CTL(stats_arenas_i_small_ndalloc)},
373  	{NAME(&quot;nrequests&quot;),	CTL(stats_arenas_i_small_nrequests)},
374  	{NAME(&quot;nfills&quot;),	CTL(stats_arenas_i_small_nfills)},
375  	{NAME(&quot;nflushes&quot;),	CTL(stats_arenas_i_small_nflushes)}
376  };
377  static const ctl_named_node_t stats_arenas_i_large_node[] = {
378  	{NAME(&quot;allocated&quot;),	CTL(stats_arenas_i_large_allocated)},
379  	{NAME(&quot;nmalloc&quot;),	CTL(stats_arenas_i_large_nmalloc)},
380  	{NAME(&quot;ndalloc&quot;),	CTL(stats_arenas_i_large_ndalloc)},
381  	{NAME(&quot;nrequests&quot;),	CTL(stats_arenas_i_large_nrequests)},
382  	{NAME(&quot;nfills&quot;),	CTL(stats_arenas_i_large_nfills)},
383  	{NAME(&quot;nflushes&quot;),	CTL(stats_arenas_i_large_nflushes)}
384  };
385  #define MUTEX_PROF_DATA_NODE(prefix)					\
386  static const ctl_named_node_t stats_##prefix##_node[] = {		\
387  	{NAME(&quot;num_ops&quot;),						\
388  	 CTL(stats_##prefix##_num_ops)},				\
389  	{NAME(&quot;num_wait&quot;),						\
390  	 CTL(stats_##prefix##_num_wait)},				\
391  	{NAME(&quot;num_spin_acq&quot;),						\
392  	 CTL(stats_##prefix##_num_spin_acq)},				\
393  	{NAME(&quot;num_owner_switch&quot;),					\
394  	 CTL(stats_##prefix##_num_owner_switch)},			\
395  	{NAME(&quot;total_wait_time&quot;),					\
396  	 CTL(stats_##prefix##_total_wait_time)},			\
397  	{NAME(&quot;max_wait_time&quot;),						\
398  	 CTL(stats_##prefix##_max_wait_time)},				\
399  	{NAME(&quot;max_num_thds&quot;),						\
400  	 CTL(stats_##prefix##_max_num_thds)}				\
401  		\
402  };
403  MUTEX_PROF_DATA_NODE(arenas_i_bins_j_mutex)
404  static const ctl_named_node_t stats_arenas_i_bins_j_node[] = {
405  	{NAME(&quot;nmalloc&quot;),	CTL(stats_arenas_i_bins_j_nmalloc)},
406  	{NAME(&quot;ndalloc&quot;),	CTL(stats_arenas_i_bins_j_ndalloc)},
407  	{NAME(&quot;nrequests&quot;),	CTL(stats_arenas_i_bins_j_nrequests)},
408  	{NAME(&quot;curregs&quot;),	CTL(stats_arenas_i_bins_j_curregs)},
409  	{NAME(&quot;nfills&quot;),	CTL(stats_arenas_i_bins_j_nfills)},
410  	{NAME(&quot;nflushes&quot;),	CTL(stats_arenas_i_bins_j_nflushes)},
411  	{NAME(&quot;nslabs&quot;),	CTL(stats_arenas_i_bins_j_nslabs)},
412  	{NAME(&quot;nreslabs&quot;),	CTL(stats_arenas_i_bins_j_nreslabs)},
413  	{NAME(&quot;curslabs&quot;),	CTL(stats_arenas_i_bins_j_curslabs)},
414  	{NAME(&quot;nonfull_slabs&quot;),	CTL(stats_arenas_i_bins_j_nonfull_slabs)},
415  	{NAME(&quot;mutex&quot;),		CHILD(named, stats_arenas_i_bins_j_mutex)}
416  };
417  static const ctl_named_node_t super_stats_arenas_i_bins_j_node[] = {
418  	{NAME(&quot;&quot;),		CHILD(named, stats_arenas_i_bins_j)}
419  };
420  static const ctl_indexed_node_t stats_arenas_i_bins_node[] = {
421  	{INDEX(stats_arenas_i_bins_j)}
422  };
423  static const ctl_named_node_t stats_arenas_i_lextents_j_node[] = {
424  	{NAME(&quot;nmalloc&quot;),	CTL(stats_arenas_i_lextents_j_nmalloc)},
425  	{NAME(&quot;ndalloc&quot;),	CTL(stats_arenas_i_lextents_j_ndalloc)},
426  	{NAME(&quot;nrequests&quot;),	CTL(stats_arenas_i_lextents_j_nrequests)},
427  	{NAME(&quot;curlextents&quot;),	CTL(stats_arenas_i_lextents_j_curlextents)}
428  };
429  static const ctl_named_node_t super_stats_arenas_i_lextents_j_node[] = {
430  	{NAME(&quot;&quot;),		CHILD(named, stats_arenas_i_lextents_j)}
431  };
432  static const ctl_indexed_node_t stats_arenas_i_lextents_node[] = {
433  	{INDEX(stats_arenas_i_lextents_j)}
434  };
435  static const ctl_named_node_t stats_arenas_i_extents_j_node[] = {
436  	{NAME(&quot;ndirty&quot;),	CTL(stats_arenas_i_extents_j_ndirty)},
437  	{NAME(&quot;nmuzzy&quot;),	CTL(stats_arenas_i_extents_j_nmuzzy)},
438  	{NAME(&quot;nretained&quot;),	CTL(stats_arenas_i_extents_j_nretained)},
439  	{NAME(&quot;dirty_bytes&quot;),	CTL(stats_arenas_i_extents_j_dirty_bytes)},
440  	{NAME(&quot;muzzy_bytes&quot;),	CTL(stats_arenas_i_extents_j_muzzy_bytes)},
441  	{NAME(&quot;retained_bytes&quot;), CTL(stats_arenas_i_extents_j_retained_bytes)}
442  };
443  static const ctl_named_node_t super_stats_arenas_i_extents_j_node[] = {
444  	{NAME(&quot;&quot;),		CHILD(named, stats_arenas_i_extents_j)}
445  };
446  static const ctl_indexed_node_t stats_arenas_i_extents_node[] = {
447  	{INDEX(stats_arenas_i_extents_j)}
448  };
449  #define OP(mtx)  MUTEX_PROF_DATA_NODE(arenas_i_mutexes_##mtx)
450  MUTEX_PROF_ARENA_MUTEXES
451  #undef OP
452  static const ctl_named_node_t stats_arenas_i_mutexes_node[] = {
453  #define OP(mtx) {NAME(#mtx), CHILD(named, stats_arenas_i_mutexes_##mtx)},
454  MUTEX_PROF_ARENA_MUTEXES
455  #undef OP
456  };
457  static const ctl_named_node_t stats_arenas_i_node[] = {
458  	{NAME(&quot;nthreads&quot;),	CTL(stats_arenas_i_nthreads)},
459  	{NAME(&quot;uptime&quot;),	CTL(stats_arenas_i_uptime)},
460  	{NAME(&quot;dss&quot;),		CTL(stats_arenas_i_dss)},
461  	{NAME(&quot;dirty_decay_ms&quot;), CTL(stats_arenas_i_dirty_decay_ms)},
462  	{NAME(&quot;muzzy_decay_ms&quot;), CTL(stats_arenas_i_muzzy_decay_ms)},
463  	{NAME(&quot;pactive&quot;),	CTL(stats_arenas_i_pactive)},
464  	{NAME(&quot;pdirty&quot;),	CTL(stats_arenas_i_pdirty)},
465  	{NAME(&quot;pmuzzy&quot;),	CTL(stats_arenas_i_pmuzzy)},
466  	{NAME(&quot;mapped&quot;),	CTL(stats_arenas_i_mapped)},
467  	{NAME(&quot;retained&quot;),	CTL(stats_arenas_i_retained)},
468  	{NAME(&quot;extent_avail&quot;),	CTL(stats_arenas_i_extent_avail)},
469  	{NAME(&quot;dirty_npurge&quot;),	CTL(stats_arenas_i_dirty_npurge)},
470  	{NAME(&quot;dirty_nmadvise&quot;), CTL(stats_arenas_i_dirty_nmadvise)},
471  	{NAME(&quot;dirty_purged&quot;),	CTL(stats_arenas_i_dirty_purged)},
472  	{NAME(&quot;muzzy_npurge&quot;),	CTL(stats_arenas_i_muzzy_npurge)},
473  	{NAME(&quot;muzzy_nmadvise&quot;), CTL(stats_arenas_i_muzzy_nmadvise)},
474  	{NAME(&quot;muzzy_purged&quot;),	CTL(stats_arenas_i_muzzy_purged)},
475  	{NAME(&quot;base&quot;),		CTL(stats_arenas_i_base)},
476  	{NAME(&quot;internal&quot;),	CTL(stats_arenas_i_internal)},
477  	{NAME(&quot;metadata_thp&quot;),	CTL(stats_arenas_i_metadata_thp)},
478  	{NAME(&quot;tcache_bytes&quot;),	CTL(stats_arenas_i_tcache_bytes)},
479  	{NAME(&quot;resident&quot;),	CTL(stats_arenas_i_resident)},
480  	{NAME(&quot;abandoned_vm&quot;),	CTL(stats_arenas_i_abandoned_vm)},
481  	{NAME(&quot;small&quot;),		CHILD(named, stats_arenas_i_small)},
482  	{NAME(&quot;large&quot;),		CHILD(named, stats_arenas_i_large)},
483  	{NAME(&quot;bins&quot;),		CHILD(indexed, stats_arenas_i_bins)},
484  	{NAME(&quot;lextents&quot;),	CHILD(indexed, stats_arenas_i_lextents)},
485  	{NAME(&quot;extents&quot;),	CHILD(indexed, stats_arenas_i_extents)},
486  	{NAME(&quot;mutexes&quot;),	CHILD(named, stats_arenas_i_mutexes)}
487  };
488  static const ctl_named_node_t super_stats_arenas_i_node[] = {
489  	{NAME(&quot;&quot;),		CHILD(named, stats_arenas_i)}
490  };
491  static const ctl_indexed_node_t stats_arenas_node[] = {
492  	{INDEX(stats_arenas_i)}
493  };
494  static const ctl_named_node_t stats_background_thread_node[] = {
495  	{NAME(&quot;num_threads&quot;),	CTL(stats_background_thread_num_threads)},
496  	{NAME(&quot;num_runs&quot;),	CTL(stats_background_thread_num_runs)},
497  	{NAME(&quot;run_interval&quot;),	CTL(stats_background_thread_run_interval)}
498  };
499  #define OP(mtx) MUTEX_PROF_DATA_NODE(mutexes_##mtx)
500  MUTEX_PROF_GLOBAL_MUTEXES
501  #undef OP
502  static const ctl_named_node_t stats_mutexes_node[] = {
503  #define OP(mtx) {NAME(#mtx), CHILD(named, stats_mutexes_##mtx)},
504  MUTEX_PROF_GLOBAL_MUTEXES
505  #undef OP
506  	{NAME(&quot;reset&quot;),		CTL(stats_mutexes_reset)}
507  };
508  #undef MUTEX_PROF_DATA_NODE
509  static const ctl_named_node_t stats_node[] = {
510  	{NAME(&quot;allocated&quot;),	CTL(stats_allocated)},
511  	{NAME(&quot;active&quot;),	CTL(stats_active)},
512  	{NAME(&quot;metadata&quot;),	CTL(stats_metadata)},
513  	{NAME(&quot;metadata_thp&quot;),	CTL(stats_metadata_thp)},
514  	{NAME(&quot;resident&quot;),	CTL(stats_resident)},
515  	{NAME(&quot;mapped&quot;),	CTL(stats_mapped)},
516  	{NAME(&quot;retained&quot;),	CTL(stats_retained)},
517  	{NAME(&quot;background_thread&quot;),
518  	 CHILD(named, stats_background_thread)},
519  	{NAME(&quot;mutexes&quot;),	CHILD(named, stats_mutexes)},
520  	{NAME(&quot;arenas&quot;),	CHILD(indexed, stats_arenas)}
521  };
522  static const ctl_named_node_t experimental_hooks_node[] = {
523  	{NAME(&quot;install&quot;),	CTL(experimental_hooks_install)},
524  	{NAME(&quot;remove&quot;),	CTL(experimental_hooks_remove)}
525  };
526  static const ctl_named_node_t experimental_utilization_node[] = {
527  	{NAME(&quot;query&quot;),		CTL(experimental_utilization_query)},
528  	{NAME(&quot;batch_query&quot;),	CTL(experimental_utilization_batch_query)}
529  };
530  static const ctl_named_node_t experimental_arenas_i_node[] = {
531  	{NAME(&quot;pactivep&quot;),	CTL(experimental_arenas_i_pactivep)}
532  };
533  static const ctl_named_node_t super_experimental_arenas_i_node[] = {
534  	{NAME(&quot;&quot;),		CHILD(named, experimental_arenas_i)}
535  };
536  static const ctl_indexed_node_t experimental_arenas_node[] = {
537  	{INDEX(experimental_arenas_i)}
538  };
539  static const ctl_named_node_t experimental_node[] = {
540  	{NAME(&quot;hooks&quot;),		CHILD(named, experimental_hooks)},
541  	{NAME(&quot;utilization&quot;),	CHILD(named, experimental_utilization)},
542  	{NAME(&quot;arenas&quot;),	CHILD(indexed, experimental_arenas)}
543  };
544  static const ctl_named_node_t	root_node[] = {
545  	{NAME(&quot;version&quot;),	CTL(version)},
546  	{NAME(&quot;epoch&quot;),		CTL(epoch)},
547  	{NAME(&quot;background_thread&quot;),	CTL(background_thread)},
548  	{NAME(&quot;max_background_threads&quot;),	CTL(max_background_threads)},
549  	{NAME(&quot;thread&quot;),	CHILD(named, thread)},
550  	{NAME(&quot;config&quot;),	CHILD(named, config)},
551  	{NAME(&quot;opt&quot;),		CHILD(named, opt)},
552  	{NAME(&quot;tcache&quot;),	CHILD(named, tcache)},
553  	{NAME(&quot;arena&quot;),		CHILD(indexed, arena)},
554  	{NAME(&quot;arenas&quot;),	CHILD(named, arenas)},
555  	{NAME(&quot;prof&quot;),		CHILD(named, prof)},
556  	{NAME(&quot;stats&quot;),		CHILD(named, stats)},
557  	{NAME(&quot;experimental&quot;),	CHILD(named, experimental)}
558  };
559  static const ctl_named_node_t super_root_node[] = {
560  	{NAME(&quot;&quot;),		CHILD(named, root)}
561  };
562  #undef NAME
563  #undef CHILD
564  #undef CTL
565  #undef INDEX
566  static void
567  ctl_accum_arena_stats_u64(arena_stats_u64_t *dst, arena_stats_u64_t *src) {
568  #ifdef JEMALLOC_ATOMIC_U64
569  	uint64_t cur_dst = atomic_load_u64(dst, ATOMIC_RELAXED);
570  	uint64_t cur_src = atomic_load_u64(src, ATOMIC_RELAXED);
571  	atomic_store_u64(dst, cur_dst + cur_src, ATOMIC_RELAXED);
572  #else
573  	*dst += *src;
574  #endif
575  }
576  static uint64_t
577  ctl_arena_stats_read_u64(arena_stats_u64_t *p) {
578  #ifdef JEMALLOC_ATOMIC_U64
579  	return atomic_load_u64(p, ATOMIC_RELAXED);
580  #else
581  	return *p;
582  #endif
583  }
584  static void
585  accum_atomic_zu(atomic_zu_t *dst, atomic_zu_t *src) {
586  	size_t cur_dst = atomic_load_zu(dst, ATOMIC_RELAXED);
587  	size_t cur_src = atomic_load_zu(src, ATOMIC_RELAXED);
588  	atomic_store_zu(dst, cur_dst + cur_src, ATOMIC_RELAXED);
589  }
590  static unsigned
591  arenas_i2a_impl(size_t i, bool compat, bool validate) {
592  	unsigned a;
593  	switch (i) {
594  	case MALLCTL_ARENAS_ALL:
595  		a = 0;
596  		break;
597  	case MALLCTL_ARENAS_DESTROYED:
598  		a = 1;
599  		break;
600  	default:
601  		if (compat &amp;&amp; i == ctl_arenas-&gt;narenas) {
602  			a = 0;
603  		} else if (validate &amp;&amp; i &gt;= ctl_arenas-&gt;narenas) {
604  			a = UINT_MAX;
605  		} else {
606  			assert(i &lt; ctl_arenas-&gt;narenas || (!validate &amp;&amp; i ==
607  			    ctl_arenas-&gt;narenas));
608  			a = (unsigned)i + 2;
609  		}
610  		break;
611  	}
612  	return a;
613  }
614  static unsigned
615  arenas_i2a(size_t i) {
616  	return arenas_i2a_impl(i, true, false);
617  }
618  static ctl_arena_t *
619  arenas_i_impl(tsd_t *tsd, size_t i, bool compat, bool init) {
620  	ctl_arena_t *ret;
621  	assert(!compat || !init);
622  	ret = ctl_arenas-&gt;arenas[arenas_i2a_impl(i, compat, false)];
623  	if (init &amp;&amp; ret == NULL) {
624  		if (config_stats) {
625  			struct container_s {
626  				ctl_arena_t		ctl_arena;
627  				ctl_arena_stats_t	astats;
628  			};
629  			struct container_s *cont =
630  			    (struct container_s *)base_alloc(tsd_tsdn(tsd),
631  			    b0get(), sizeof(struct container_s), QUANTUM);
632  			if (cont == NULL) {
633  				return NULL;
634  			}
635  			ret = &amp;cont-&gt;ctl_arena;
636  			ret-&gt;astats = &amp;cont-&gt;astats;
637  		} else {
638  			ret = (ctl_arena_t *)base_alloc(tsd_tsdn(tsd), b0get(),
639  			    sizeof(ctl_arena_t), QUANTUM);
640  			if (ret == NULL) {
641  				return NULL;
642  			}
643  		}
644  		ret-&gt;arena_ind = (unsigned)i;
645  		ctl_arenas-&gt;arenas[arenas_i2a_impl(i, compat, false)] = ret;
646  	}
647  	assert(ret == NULL || arenas_i2a(ret-&gt;arena_ind) == arenas_i2a(i));
648  	return ret;
649  }
650  static ctl_arena_t *
651  arenas_i(size_t i) {
652  	ctl_arena_t *ret = arenas_i_impl(tsd_fetch(), i, true, false);
653  	assert(ret != NULL);
654  	return ret;
655  }
656  static void
657  ctl_arena_clear(ctl_arena_t *ctl_arena) {
658  	ctl_arena-&gt;nthreads = 0;
659  	ctl_arena-&gt;dss = dss_prec_names[dss_prec_limit];
660  	ctl_arena-&gt;dirty_decay_ms = -1;
661  	ctl_arena-&gt;muzzy_decay_ms = -1;
662  	ctl_arena-&gt;pactive = 0;
663  	ctl_arena-&gt;pdirty = 0;
664  	ctl_arena-&gt;pmuzzy = 0;
665  	if (config_stats) {
666  		memset(&amp;ctl_arena-&gt;astats-&gt;astats, 0, sizeof(arena_stats_t));
667  		ctl_arena-&gt;astats-&gt;allocated_small = 0;
668  		ctl_arena-&gt;astats-&gt;nmalloc_small = 0;
669  		ctl_arena-&gt;astats-&gt;ndalloc_small = 0;
670  		ctl_arena-&gt;astats-&gt;nrequests_small = 0;
671  		ctl_arena-&gt;astats-&gt;nfills_small = 0;
672  		ctl_arena-&gt;astats-&gt;nflushes_small = 0;
673  		memset(ctl_arena-&gt;astats-&gt;bstats, 0, SC_NBINS *
674  		    sizeof(bin_stats_t));
675  		memset(ctl_arena-&gt;astats-&gt;lstats, 0, (SC_NSIZES - SC_NBINS) *
676  		    sizeof(arena_stats_large_t));
677  		memset(ctl_arena-&gt;astats-&gt;estats, 0, SC_NPSIZES *
678  		    sizeof(arena_stats_extents_t));
679  	}
680  }
681  static void
682  ctl_arena_stats_amerge(tsdn_t *tsdn, ctl_arena_t *ctl_arena, arena_t *arena) {
683  	unsigned i;
684  	if (config_stats) {
685  		arena_stats_merge(tsdn, arena, &amp;ctl_arena-&gt;nthreads,
686  		    &amp;ctl_arena-&gt;dss, &amp;ctl_arena-&gt;dirty_decay_ms,
687  		    &amp;ctl_arena-&gt;muzzy_decay_ms, &amp;ctl_arena-&gt;pactive,
688  		    &amp;ctl_arena-&gt;pdirty, &amp;ctl_arena-&gt;pmuzzy,
689  		    &amp;ctl_arena-&gt;astats-&gt;astats, ctl_arena-&gt;astats-&gt;bstats,
690  		    ctl_arena-&gt;astats-&gt;lstats, ctl_arena-&gt;astats-&gt;estats);
691  		for (i = 0; i &lt; SC_NBINS; i++) {
692  			ctl_arena-&gt;astats-&gt;allocated_small +=
693  			    ctl_arena-&gt;astats-&gt;bstats[i].curregs *
694  			    sz_index2size(i);
695  			ctl_arena-&gt;astats-&gt;nmalloc_small +=
696  			    ctl_arena-&gt;astats-&gt;bstats[i].nmalloc;
697  			ctl_arena-&gt;astats-&gt;ndalloc_small +=
698  			    ctl_arena-&gt;astats-&gt;bstats[i].ndalloc;
699  			ctl_arena-&gt;astats-&gt;nrequests_small +=
700  			    ctl_arena-&gt;astats-&gt;bstats[i].nrequests;
701  			ctl_arena-&gt;astats-&gt;nfills_small +=
702  			    ctl_arena-&gt;astats-&gt;bstats[i].nfills;
703  			ctl_arena-&gt;astats-&gt;nflushes_small +=
704  			    ctl_arena-&gt;astats-&gt;bstats[i].nflushes;
705  		}
706  	} else {
707  		arena_basic_stats_merge(tsdn, arena, &amp;ctl_arena-&gt;nthreads,
708  		    &amp;ctl_arena-&gt;dss, &amp;ctl_arena-&gt;dirty_decay_ms,
709  		    &amp;ctl_arena-&gt;muzzy_decay_ms, &amp;ctl_arena-&gt;pactive,
710  		    &amp;ctl_arena-&gt;pdirty, &amp;ctl_arena-&gt;pmuzzy);
711  	}
712  }
713  static void
714  ctl_arena_stats_sdmerge(ctl_arena_t *ctl_sdarena, ctl_arena_t *ctl_arena,
715      bool destroyed) {
716  	unsigned i;
717  	if (!destroyed) {
718  		ctl_sdarena-&gt;nthreads += ctl_arena-&gt;nthreads;
719  		ctl_sdarena-&gt;pactive += ctl_arena-&gt;pactive;
720  		ctl_sdarena-&gt;pdirty += ctl_arena-&gt;pdirty;
721  		ctl_sdarena-&gt;pmuzzy += ctl_arena-&gt;pmuzzy;
722  	} else {
723  		assert(ctl_arena-&gt;nthreads == 0);
724  		assert(ctl_arena-&gt;pactive == 0);
725  		assert(ctl_arena-&gt;pdirty == 0);
726  		assert(ctl_arena-&gt;pmuzzy == 0);
727  	}
728  	if (config_stats) {
729  		ctl_arena_stats_t *sdstats = ctl_sdarena-&gt;astats;
730  		ctl_arena_stats_t *astats = ctl_arena-&gt;astats;
731  		if (!destroyed) {
732  			accum_atomic_zu(&amp;sdstats-&gt;astats.mapped,
733  			    &amp;astats-&gt;astats.mapped);
734  			accum_atomic_zu(&amp;sdstats-&gt;astats.retained,
735  			    &amp;astats-&gt;astats.retained);
736  			accum_atomic_zu(&amp;sdstats-&gt;astats.extent_avail,
737  			    &amp;astats-&gt;astats.extent_avail);
738  		}
739  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.decay_dirty.npurge,
740  		    &amp;astats-&gt;astats.decay_dirty.npurge);
741  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.decay_dirty.nmadvise,
742  		    &amp;astats-&gt;astats.decay_dirty.nmadvise);
743  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.decay_dirty.purged,
744  		    &amp;astats-&gt;astats.decay_dirty.purged);
745  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.decay_muzzy.npurge,
746  		    &amp;astats-&gt;astats.decay_muzzy.npurge);
747  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.decay_muzzy.nmadvise,
748  		    &amp;astats-&gt;astats.decay_muzzy.nmadvise);
749  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.decay_muzzy.purged,
750  		    &amp;astats-&gt;astats.decay_muzzy.purged);
751  #define OP(mtx) malloc_mutex_prof_merge(				\
752  		    &amp;(sdstats-&gt;astats.mutex_prof_data[			\
753  		        arena_prof_mutex_##mtx]),			\
754  		    &amp;(astats-&gt;astats.mutex_prof_data[			\
755  		        arena_prof_mutex_##mtx]));
756  MUTEX_PROF_ARENA_MUTEXES
757  #undef OP
758  		if (!destroyed) {
759  			accum_atomic_zu(&amp;sdstats-&gt;astats.base,
760  			    &amp;astats-&gt;astats.base);
761  			accum_atomic_zu(&amp;sdstats-&gt;astats.internal,
762  			    &amp;astats-&gt;astats.internal);
763  			accum_atomic_zu(&amp;sdstats-&gt;astats.resident,
764  			    &amp;astats-&gt;astats.resident);
765  			accum_atomic_zu(&amp;sdstats-&gt;astats.metadata_thp,
766  			    &amp;astats-&gt;astats.metadata_thp);
767  		} else {
768  			assert(atomic_load_zu(
769  			    &amp;astats-&gt;astats.internal, ATOMIC_RELAXED) == 0);
770  		}
771  		if (!destroyed) {
772  			sdstats-&gt;allocated_small += astats-&gt;allocated_small;
773  		} else {
774  			assert(astats-&gt;allocated_small == 0);
775  		}
776  		sdstats-&gt;nmalloc_small += astats-&gt;nmalloc_small;
777  		sdstats-&gt;ndalloc_small += astats-&gt;ndalloc_small;
778  		sdstats-&gt;nrequests_small += astats-&gt;nrequests_small;
779  		sdstats-&gt;nfills_small += astats-&gt;nfills_small;
780  		sdstats-&gt;nflushes_small += astats-&gt;nflushes_small;
781  		if (!destroyed) {
782  			accum_atomic_zu(&amp;sdstats-&gt;astats.allocated_large,
783  			    &amp;astats-&gt;astats.allocated_large);
784  		} else {
785  			assert(atomic_load_zu(&amp;astats-&gt;astats.allocated_large,
786  			    ATOMIC_RELAXED) == 0);
787  		}
788  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.nmalloc_large,
789  		    &amp;astats-&gt;astats.nmalloc_large);
790  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.ndalloc_large,
791  		    &amp;astats-&gt;astats.ndalloc_large);
792  		ctl_accum_arena_stats_u64(&amp;sdstats-&gt;astats.nrequests_large,
793  		    &amp;astats-&gt;astats.nrequests_large);
794  		accum_atomic_zu(&amp;sdstats-&gt;astats.abandoned_vm,
795  		    &amp;astats-&gt;astats.abandoned_vm);
796  		accum_atomic_zu(&amp;sdstats-&gt;astats.tcache_bytes,
797  		    &amp;astats-&gt;astats.tcache_bytes);
798  		if (ctl_arena-&gt;arena_ind == 0) {
799  			sdstats-&gt;astats.uptime = astats-&gt;astats.uptime;
800  		}
801  		for (i = 0; i &lt; SC_NBINS; i++) {
802  			sdstats-&gt;bstats[i].nmalloc += astats-&gt;bstats[i].nmalloc;
803  			sdstats-&gt;bstats[i].ndalloc += astats-&gt;bstats[i].ndalloc;
804  			sdstats-&gt;bstats[i].nrequests +=
805  			    astats-&gt;bstats[i].nrequests;
806  			if (!destroyed) {
807  				sdstats-&gt;bstats[i].curregs +=
808  				    astats-&gt;bstats[i].curregs;
809  			} else {
810  				assert(astats-&gt;bstats[i].curregs == 0);
811  			}
812  			sdstats-&gt;bstats[i].nfills += astats-&gt;bstats[i].nfills;
813  			sdstats-&gt;bstats[i].nflushes +=
814  			    astats-&gt;bstats[i].nflushes;
815  			sdstats-&gt;bstats[i].nslabs += astats-&gt;bstats[i].nslabs;
816  			sdstats-&gt;bstats[i].reslabs += astats-&gt;bstats[i].reslabs;
817  			if (!destroyed) {
818  				sdstats-&gt;bstats[i].curslabs +=
819  				    astats-&gt;bstats[i].curslabs;
820  				sdstats-&gt;bstats[i].nonfull_slabs +=
821  				    astats-&gt;bstats[i].nonfull_slabs;
822  			} else {
823  				assert(astats-&gt;bstats[i].curslabs == 0);
824  				assert(astats-&gt;bstats[i].nonfull_slabs == 0);
825  			}
826  			malloc_mutex_prof_merge(&amp;sdstats-&gt;bstats[i].mutex_data,
827  			    &amp;astats-&gt;bstats[i].mutex_data);
828  		}
829  		for (i = 0; i &lt; SC_NSIZES - SC_NBINS; i++) {
830  			ctl_accum_arena_stats_u64(&amp;sdstats-&gt;lstats[i].nmalloc,
831  			    &amp;astats-&gt;lstats[i].nmalloc);
832  			ctl_accum_arena_stats_u64(&amp;sdstats-&gt;lstats[i].ndalloc,
833  			    &amp;astats-&gt;lstats[i].ndalloc);
834  			ctl_accum_arena_stats_u64(&amp;sdstats-&gt;lstats[i].nrequests,
835  			    &amp;astats-&gt;lstats[i].nrequests);
836  			if (!destroyed) {
837  				sdstats-&gt;lstats[i].curlextents +=
838  				    astats-&gt;lstats[i].curlextents;
839  			} else {
840  				assert(astats-&gt;lstats[i].curlextents == 0);
841  			}
842  		}
843  		for (i = 0; i &lt; SC_NPSIZES; i++) {
844  			accum_atomic_zu(&amp;sdstats-&gt;estats[i].ndirty,
845  			    &amp;astats-&gt;estats[i].ndirty);
846  			accum_atomic_zu(&amp;sdstats-&gt;estats[i].nmuzzy,
847  			    &amp;astats-&gt;estats[i].nmuzzy);
848  			accum_atomic_zu(&amp;sdstats-&gt;estats[i].nretained,
849  			    &amp;astats-&gt;estats[i].nretained);
850  			accum_atomic_zu(&amp;sdstats-&gt;estats[i].dirty_bytes,
851  			    &amp;astats-&gt;estats[i].dirty_bytes);
852  			accum_atomic_zu(&amp;sdstats-&gt;estats[i].muzzy_bytes,
853  			    &amp;astats-&gt;estats[i].muzzy_bytes);
854  			accum_atomic_zu(&amp;sdstats-&gt;estats[i].retained_bytes,
855  			    &amp;astats-&gt;estats[i].retained_bytes);
856  		}
857  	}
858  }
859  static void
860  ctl_arena_refresh(tsdn_t *tsdn, arena_t *arena, ctl_arena_t *ctl_sdarena,
861      unsigned i, bool destroyed) {
862  	ctl_arena_t *ctl_arena = arenas_i(i);
863  	ctl_arena_clear(ctl_arena);
864  	ctl_arena_stats_amerge(tsdn, ctl_arena, arena);
865  	ctl_arena_stats_sdmerge(ctl_sdarena, ctl_arena, destroyed);
866  }
867  static unsigned
868  ctl_arena_init(tsd_t *tsd, extent_hooks_t *extent_hooks) {
869  	unsigned arena_ind;
870  	ctl_arena_t *ctl_arena;
871  	if ((ctl_arena = ql_last(&amp;ctl_arenas-&gt;destroyed, destroyed_link)) !=
872  	    NULL) {
873  		ql_remove(&amp;ctl_arenas-&gt;destroyed, ctl_arena, destroyed_link);
874  		arena_ind = ctl_arena-&gt;arena_ind;
875  	} else {
876  		arena_ind = ctl_arenas-&gt;narenas;
877  	}
878  	if (arenas_i_impl(tsd, arena_ind, false, true) == NULL) {
879  		return UINT_MAX;
880  	}
881  	if (arena_init(tsd_tsdn(tsd), arena_ind, extent_hooks) == NULL) {
882  		return UINT_MAX;
883  	}
884  	if (arena_ind == ctl_arenas-&gt;narenas) {
885  		ctl_arenas-&gt;narenas++;
886  	}
887  	return arena_ind;
888  }
889  static void
890  ctl_background_thread_stats_read(tsdn_t *tsdn) {
891  	background_thread_stats_t *stats = &amp;ctl_stats-&gt;background_thread;
892  	if (!have_background_thread ||
893  	    background_thread_stats_read(tsdn, stats)) {
894  		memset(stats, 0, sizeof(background_thread_stats_t));
895  		nstime_init(&amp;stats-&gt;run_interval, 0);
896  	}
897  }
898  static void
899  ctl_refresh(tsdn_t *tsdn) {
900  	unsigned i;
901  	ctl_arena_t *ctl_sarena = arenas_i(MALLCTL_ARENAS_ALL);
902  	VARIABLE_ARRAY(arena_t *, tarenas, ctl_arenas-&gt;narenas);
903  	ctl_arena_clear(ctl_sarena);
904  	for (i = 0; i &lt; ctl_arenas-&gt;narenas; i++) {
905  		tarenas[i] = arena_get(tsdn, i, false);
906  	}
907  	for (i = 0; i &lt; ctl_arenas-&gt;narenas; i++) {
908  		ctl_arena_t *ctl_arena = arenas_i(i);
909  		bool initialized = (tarenas[i] != NULL);
910  		ctl_arena-&gt;initialized = initialized;
911  		if (initialized) {
912  			ctl_arena_refresh(tsdn, tarenas[i], ctl_sarena, i,
913  			    false);
914  		}
915  	}
916  	if (config_stats) {
917  		ctl_stats-&gt;allocated = ctl_sarena-&gt;astats-&gt;allocated_small +
918  		    atomic_load_zu(&amp;ctl_sarena-&gt;astats-&gt;astats.allocated_large,
919  			ATOMIC_RELAXED);
920  		ctl_stats-&gt;active = (ctl_sarena-&gt;pactive &lt;&lt; LG_PAGE);
921  		ctl_stats-&gt;metadata = atomic_load_zu(
922  		    &amp;ctl_sarena-&gt;astats-&gt;astats.base, ATOMIC_RELAXED) +
923  		    atomic_load_zu(&amp;ctl_sarena-&gt;astats-&gt;astats.internal,
924  			ATOMIC_RELAXED);
925  		ctl_stats-&gt;metadata_thp = atomic_load_zu(
926  		    &amp;ctl_sarena-&gt;astats-&gt;astats.metadata_thp, ATOMIC_RELAXED);
927  		ctl_stats-&gt;resident = atomic_load_zu(
928  		    &amp;ctl_sarena-&gt;astats-&gt;astats.resident, ATOMIC_RELAXED);
929  		ctl_stats-&gt;mapped = atomic_load_zu(
930  		    &amp;ctl_sarena-&gt;astats-&gt;astats.mapped, ATOMIC_RELAXED);
931  		ctl_stats-&gt;retained = atomic_load_zu(
932  		    &amp;ctl_sarena-&gt;astats-&gt;astats.retained, ATOMIC_RELAXED);
933  		ctl_background_thread_stats_read(tsdn);
934  #define READ_GLOBAL_MUTEX_PROF_DATA(i, mtx)				\
935      malloc_mutex_lock(tsdn, &amp;mtx);					\
936      malloc_mutex_prof_read(tsdn, &amp;ctl_stats-&gt;mutex_prof_data[i], &amp;mtx);	\
937      malloc_mutex_unlock(tsdn, &amp;mtx);
938  		if (config_prof &amp;&amp; opt_prof) {
939  			READ_GLOBAL_MUTEX_PROF_DATA(global_prof_mutex_prof,
940  			    bt2gctx_mtx);
941  		}
942  		if (have_background_thread) {
943  			READ_GLOBAL_MUTEX_PROF_DATA(
944  			    global_prof_mutex_background_thread,
945  			    background_thread_lock);
946  		} else {
947  			memset(&amp;ctl_stats-&gt;mutex_prof_data[
948  			    global_prof_mutex_background_thread], 0,
949  			    sizeof(mutex_prof_data_t));
950  		}
951  		malloc_mutex_prof_read(tsdn,
952  		    &amp;ctl_stats-&gt;mutex_prof_data[global_prof_mutex_ctl],
953  		    &amp;ctl_mtx);
954  #undef READ_GLOBAL_MUTEX_PROF_DATA
955  	}
956  	ctl_arenas-&gt;epoch++;
957  }
958  static bool
959  ctl_init(tsd_t *tsd) {
960  	bool ret;
961  	tsdn_t *tsdn = tsd_tsdn(tsd);
962  	malloc_mutex_lock(tsdn, &amp;ctl_mtx);
963  	if (!ctl_initialized) {
964  		ctl_arena_t *ctl_sarena, *ctl_darena;
965  		unsigned i;
966  		if (ctl_arenas == NULL) {
967  			ctl_arenas = (ctl_arenas_t *)base_alloc(tsdn,
968  			    b0get(), sizeof(ctl_arenas_t), QUANTUM);
969  			if (ctl_arenas == NULL) {
970  				ret = true;
971  				goto label_return;
972  			}
973  		}
974  		if (config_stats &amp;&amp; ctl_stats == NULL) {
975  			ctl_stats = (ctl_stats_t *)base_alloc(tsdn, b0get(),
976  			    sizeof(ctl_stats_t), QUANTUM);
977  			if (ctl_stats == NULL) {
978  				ret = true;
979  				goto label_return;
980  			}
981  		}
982  		if ((ctl_sarena = arenas_i_impl(tsd, MALLCTL_ARENAS_ALL, false,
983  		    true)) == NULL) {
984  			ret = true;
985  			goto label_return;
986  		}
987  		ctl_sarena-&gt;initialized = true;
988  		if ((ctl_darena = arenas_i_impl(tsd, MALLCTL_ARENAS_DESTROYED,
989  		    false, true)) == NULL) {
990  			ret = true;
991  			goto label_return;
992  		}
993  		ctl_arena_clear(ctl_darena);
994  		ctl_arenas-&gt;narenas = narenas_total_get();
995  		for (i = 0; i &lt; ctl_arenas-&gt;narenas; i++) {
996  			if (arenas_i_impl(tsd, i, false, true) == NULL) {
997  				ret = true;
998  				goto label_return;
999  			}
1000  		}
1001  		ql_new(&amp;ctl_arenas-&gt;destroyed);
1002  		ctl_refresh(tsdn);
1003  		ctl_initialized = true;
1004  	}
1005  	ret = false;
1006  label_return:
1007  	malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
1008  	return ret;
1009  }
1010  static int
1011  ctl_lookup(tsdn_t *tsdn, const char *name, ctl_node_t const **nodesp,
1012      size_t *mibp, size_t *depthp) {
1013  	int ret;
1014  	const char *elm, *tdot, *dot;
1015  	size_t elen, i, j;
1016  	const ctl_named_node_t *node;
1017  	elm = name;
1018  	dot = ((tdot = strchr(elm, &#x27;.&#x27;)) != NULL) ? tdot : strchr(elm, &#x27;\0&#x27;);
1019  	elen = (size_t)((uintptr_t)dot - (uintptr_t)elm);
1020  	if (elen == 0) {
1021  		ret = ENOENT;
1022  		goto label_return;
1023  	}
1024  	node = super_root_node;
1025  	for (i = 0; i &lt; *depthp; i++) {
1026  		assert(node);
1027  		assert(node-&gt;nchildren &gt; 0);
1028  		if (ctl_named_node(node-&gt;children) != NULL) {
1029  			const ctl_named_node_t *pnode = node;
1030  			for (j = 0; j &lt; node-&gt;nchildren; j++) {
1031  				const ctl_named_node_t *child =
1032  				    ctl_named_children(node, j);
1033  				if (strlen(child-&gt;name) == elen &amp;&amp;
1034  				    strncmp(elm, child-&gt;name, elen) == 0) {
1035  					node = child;
1036  					if (nodesp != NULL) {
1037  						nodesp[i] =
1038  						    (const ctl_node_t *)node;
1039  					}
1040  					mibp[i] = j;
1041  					break;
1042  				}
1043  			}
1044  			if (node == pnode) {
1045  				ret = ENOENT;
1046  				goto label_return;
1047  			}
1048  		} else {
1049  			uintmax_t index;
1050  			const ctl_indexed_node_t *inode;
1051  			index = malloc_strtoumax(elm, NULL, 10);
1052  			if (index == UINTMAX_MAX || index &gt; SIZE_T_MAX) {
1053  				ret = ENOENT;
1054  				goto label_return;
1055  			}
1056  			inode = ctl_indexed_node(node-&gt;children);
1057  			node = inode-&gt;index(tsdn, mibp, *depthp, (size_t)index);
1058  			if (node == NULL) {
1059  				ret = ENOENT;
1060  				goto label_return;
1061  			}
1062  			if (nodesp != NULL) {
1063  				nodesp[i] = (const ctl_node_t *)node;
1064  			}
1065  			mibp[i] = (size_t)index;
1066  		}
1067  		if (node-&gt;ctl != NULL) {
1068  			if (*dot != &#x27;\0&#x27;) {
1069  				ret = ENOENT;
1070  				goto label_return;
1071  			}
1072  			*depthp = i + 1;
1073  			break;
1074  		}
1075  		if (*dot == &#x27;\0&#x27;) {
1076  			ret = ENOENT;
1077  			goto label_return;
1078  		}
1079  		elm = &amp;dot[1];
1080  		dot = ((tdot = strchr(elm, &#x27;.&#x27;)) != NULL) ? tdot :
1081  		    strchr(elm, &#x27;\0&#x27;);
1082  		elen = (size_t)((uintptr_t)dot - (uintptr_t)elm);
1083  	}
1084  	ret = 0;
1085  label_return:
1086  	return ret;
1087  }
1088  int
1089  ctl_byname(tsd_t *tsd, const char *name, void *oldp, size_t *oldlenp,
1090      void *newp, size_t newlen) {
1091  	int ret;
1092  	size_t depth;
1093  	ctl_node_t const *nodes[CTL_MAX_DEPTH];
1094  	size_t mib[CTL_MAX_DEPTH];
1095  	const ctl_named_node_t *node;
1096  	if (!ctl_initialized &amp;&amp; ctl_init(tsd)) {
1097  		ret = EAGAIN;
1098  		goto label_return;
1099  	}
1100  	depth = CTL_MAX_DEPTH;
1101  	ret = ctl_lookup(tsd_tsdn(tsd), name, nodes, mib, &amp;depth);
1102  	if (ret != 0) {
1103  		goto label_return;
1104  	}
1105  	node = ctl_named_node(nodes[depth-1]);
1106  	if (node != NULL &amp;&amp; node-&gt;ctl) {
1107  		ret = node-&gt;ctl(tsd, mib, depth, oldp, oldlenp, newp, newlen);
1108  	} else {
1109  		ret = ENOENT;
1110  	}
1111  label_return:
1112  	return(ret);
1113  }
1114  int
1115  ctl_nametomib(tsd_t *tsd, const char *name, size_t *mibp, size_t *miblenp) {
1116  	int ret;
1117  	if (!ctl_initialized &amp;&amp; ctl_init(tsd)) {
1118  		ret = EAGAIN;
1119  		goto label_return;
1120  	}
1121  	ret = ctl_lookup(tsd_tsdn(tsd), name, NULL, mibp, miblenp);
1122  label_return:
1123  	return(ret);
1124  }
1125  int
1126  ctl_bymib(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1127      size_t *oldlenp, void *newp, size_t newlen) {
1128  	int ret;
1129  	const ctl_named_node_t *node;
1130  	size_t i;
1131  	if (!ctl_initialized &amp;&amp; ctl_init(tsd)) {
1132  		ret = EAGAIN;
1133  		goto label_return;
1134  	}
1135  	node = super_root_node;
1136  	for (i = 0; i &lt; miblen; i++) {
1137  		assert(node);
1138  		assert(node-&gt;nchildren &gt; 0);
1139  		if (ctl_named_node(node-&gt;children) != NULL) {
1140  			if (node-&gt;nchildren &lt;= mib[i]) {
1141  				ret = ENOENT;
1142  				goto label_return;
1143  			}
1144  			node = ctl_named_children(node, mib[i]);
1145  		} else {
1146  			const ctl_indexed_node_t *inode;
1147  			inode = ctl_indexed_node(node-&gt;children);
1148  			node = inode-&gt;index(tsd_tsdn(tsd), mib, miblen, mib[i]);
1149  			if (node == NULL) {
1150  				ret = ENOENT;
1151  				goto label_return;
1152  			}
1153  		}
1154  	}
1155  	if (node &amp;&amp; node-&gt;ctl) {
1156  		ret = node-&gt;ctl(tsd, mib, miblen, oldp, oldlenp, newp, newlen);
1157  	} else {
1158  		ret = ENOENT;
1159  	}
1160  label_return:
1161  	return(ret);
1162  }
1163  bool
1164  ctl_boot(void) {
1165  	if (malloc_mutex_init(&amp;ctl_mtx, &quot;ctl&quot;, WITNESS_RANK_CTL,
1166  	    malloc_mutex_rank_exclusive)) {
1167  		return true;
1168  	}
1169  	ctl_initialized = false;
1170  	return false;
1171  }
1172  void
<span onclick='openModal()' class='match'>1173  ctl_prefork(tsdn_t *tsdn) {
1174  	malloc_mutex_prefork(tsdn, &amp;ctl_mtx);
1175  }
</span>1176  void
1177  ctl_postfork_parent(tsdn_t *tsdn) {
1178  	malloc_mutex_postfork_parent(tsdn, &amp;ctl_mtx);
1179  }
1180  void
1181  ctl_postfork_child(tsdn_t *tsdn) {
1182  	malloc_mutex_postfork_child(tsdn, &amp;ctl_mtx);
1183  }
1184  #define READONLY()	do {						\
1185  	if (newp != NULL || newlen != 0) {				\
1186  		ret = EPERM;						\
1187  		goto label_return;					\
1188  	}								\
1189  } while (0)
1190  #define WRITEONLY()	do {						\
1191  	if (oldp != NULL || oldlenp != NULL) {				\
1192  		ret = EPERM;						\
1193  		goto label_return;					\
1194  	}								\
1195  } while (0)
1196  #define READ_XOR_WRITE()	do {					\
1197  	if ((oldp != NULL &amp;&amp; oldlenp != NULL) &amp;&amp; (newp != NULL ||	\
1198  	    newlen != 0)) {						\
1199  		ret = EPERM;						\
1200  		goto label_return;					\
1201  	}								\
1202  } while (0)
1203  #define READ(v, t)	do {						\
1204  	if (oldp != NULL &amp;&amp; oldlenp != NULL) {				\
1205  		if (*oldlenp != sizeof(t)) {				\
1206  			size_t	copylen = (sizeof(t) &lt;= *oldlenp)	\
1207  			    ? sizeof(t) : *oldlenp;			\
1208  			memcpy(oldp, (void *)&amp;(v), copylen);		\
1209  			ret = EINVAL;					\
1210  			goto label_return;				\
1211  		}							\
1212  		*(t *)oldp = (v);					\
1213  	}								\
1214  } while (0)
1215  #define WRITE(v, t)	do {						\
1216  	if (newp != NULL) {						\
1217  		if (newlen != sizeof(t)) {				\
1218  			ret = EINVAL;					\
1219  			goto label_return;				\
1220  		}							\
1221  		(v) = *(t *)newp;					\
1222  	}								\
1223  } while (0)
1224  #define MIB_UNSIGNED(v, i) do {						\
1225  	if (mib[i] &gt; UINT_MAX) {					\
1226  		ret = EFAULT;						\
1227  		goto label_return;					\
1228  	}								\
1229  	v = (unsigned)mib[i];						\
1230  } while (0)
1231  #define CTL_RO_CLGEN(c, l, n, v, t)					\
1232  static int								\
1233  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,	\
1234      size_t *oldlenp, void *newp, size_t newlen) {			\
1235  	int ret;							\
1236  	t oldval;							\
1237  									\
1238  	if (!(c)) {							\
1239  		return ENOENT;						\
1240  	}								\
1241  	if (l) {							\
1242  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);		\
1243  	}								\
1244  	READONLY();							\
1245  	oldval = (v);							\
1246  	READ(oldval, t);						\
1247  									\
1248  	ret = 0;							\
1249  label_return:								\
1250  	if (l) {							\
1251  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);		\
1252  	}								\
1253  	return ret;							\
1254  }
1255  #define CTL_RO_CGEN(c, n, v, t)						\
1256  static int								\
1257  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1258      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1259  	int ret;							\
1260  	t oldval;							\
1261  									\
1262  	if (!(c)) {							\
1263  		return ENOENT;						\
1264  	}								\
1265  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);			\
1266  	READONLY();							\
1267  	oldval = (v);							\
1268  	READ(oldval, t);						\
1269  									\
1270  	ret = 0;							\
1271  label_return:								\
1272  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);			\
1273  	return ret;							\
1274  }
1275  #define CTL_RO_GEN(n, v, t)						\
1276  static int								\
1277  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,	\
1278      size_t *oldlenp, void *newp, size_t newlen) {			\
1279  	int ret;							\
1280  	t oldval;							\
1281  									\
1282  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);			\
1283  	READONLY();							\
1284  	oldval = (v);							\
1285  	READ(oldval, t);						\
1286  									\
1287  	ret = 0;							\
1288  label_return:								\
1289  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);			\
1290  	return ret;							\
1291  }
1292  #define CTL_RO_NL_CGEN(c, n, v, t)					\
1293  static int								\
1294  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1295      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1296  	int ret;							\
1297  	t oldval;							\
1298  									\
1299  	if (!(c)) {							\
1300  		return ENOENT;						\
1301  	}								\
1302  	READONLY();							\
1303  	oldval = (v);							\
1304  	READ(oldval, t);						\
1305  									\
1306  	ret = 0;							\
1307  label_return:								\
1308  	return ret;							\
1309  }
1310  #define CTL_RO_NL_GEN(n, v, t)						\
1311  static int								\
1312  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1313      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1314  	int ret;							\
1315  	t oldval;							\
1316  									\
1317  	READONLY();							\
1318  	oldval = (v);							\
1319  	READ(oldval, t);						\
1320  									\
1321  	ret = 0;							\
1322  label_return:								\
1323  	return ret;							\
1324  }
1325  #define CTL_TSD_RO_NL_CGEN(c, n, m, t)					\
1326  static int								\
1327  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,	\
1328      size_t *oldlenp, void *newp, size_t newlen) {			\
1329  	int ret;							\
1330  	t oldval;							\
1331  									\
1332  	if (!(c)) {							\
1333  		return ENOENT;						\
1334  	}								\
1335  	READONLY();							\
1336  	oldval = (m(tsd));						\
1337  	READ(oldval, t);						\
1338  									\
1339  	ret = 0;							\
1340  label_return:								\
1341  	return ret;							\
1342  }
1343  #define CTL_RO_CONFIG_GEN(n, t)						\
1344  static int								\
1345  n##_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, \
1346      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {			\
1347  	int ret;							\
1348  	t oldval;							\
1349  									\
1350  	READONLY();							\
1351  	oldval = n;							\
1352  	READ(oldval, t);						\
1353  									\
1354  	ret = 0;							\
1355  label_return:								\
1356  	return ret;							\
1357  }
1358  CTL_RO_NL_GEN(version, JEMALLOC_VERSION, const char *)
1359  static int
1360  epoch_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1361      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1362  	int ret;
1363  	UNUSED uint64_t newval;
1364  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
1365  	WRITE(newval, uint64_t);
1366  	if (newp != NULL) {
1367  		ctl_refresh(tsd_tsdn(tsd));
1368  	}
1369  	READ(ctl_arenas-&gt;epoch, uint64_t);
1370  	ret = 0;
1371  label_return:
1372  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
1373  	return ret;
1374  }
1375  static int
1376  background_thread_ctl(tsd_t *tsd, const size_t *mib,
1377      size_t miblen, void *oldp, size_t *oldlenp,
1378      void *newp, size_t newlen) {
1379  	int ret;
1380  	bool oldval;
1381  	if (!have_background_thread) {
1382  		return ENOENT;
1383  	}
1384  	background_thread_ctl_init(tsd_tsdn(tsd));
1385  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
1386  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;background_thread_lock);
1387  	if (newp == NULL) {
1388  		oldval = background_thread_enabled();
1389  		READ(oldval, bool);
1390  	} else {
1391  		if (newlen != sizeof(bool)) {
1392  			ret = EINVAL;
1393  			goto label_return;
1394  		}
1395  		oldval = background_thread_enabled();
1396  		READ(oldval, bool);
1397  		bool newval = *(bool *)newp;
1398  		if (newval == oldval) {
1399  			ret = 0;
1400  			goto label_return;
1401  		}
1402  		background_thread_enabled_set(tsd_tsdn(tsd), newval);
1403  		if (newval) {
1404  			if (background_threads_enable(tsd)) {
1405  				ret = EFAULT;
1406  				goto label_return;
1407  			}
1408  		} else {
1409  			if (background_threads_disable(tsd)) {
1410  				ret = EFAULT;
1411  				goto label_return;
1412  			}
1413  		}
1414  	}
1415  	ret = 0;
1416  label_return:
1417  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;background_thread_lock);
1418  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
1419  	return ret;
1420  }
1421  static int
1422  max_background_threads_ctl(tsd_t *tsd, const size_t *mib,
1423      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1424      size_t newlen) {
1425  	int ret;
1426  	size_t oldval;
1427  	if (!have_background_thread) {
1428  		return ENOENT;
1429  	}
1430  	background_thread_ctl_init(tsd_tsdn(tsd));
1431  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
1432  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;background_thread_lock);
1433  	if (newp == NULL) {
1434  		oldval = max_background_threads;
1435  		READ(oldval, size_t);
1436  	} else {
1437  		if (newlen != sizeof(size_t)) {
1438  			ret = EINVAL;
1439  			goto label_return;
1440  		}
1441  		oldval = max_background_threads;
1442  		READ(oldval, size_t);
1443  		size_t newval = *(size_t *)newp;
1444  		if (newval == oldval) {
1445  			ret = 0;
1446  			goto label_return;
1447  		}
1448  		if (newval &gt; opt_max_background_threads) {
1449  			ret = EINVAL;
1450  			goto label_return;
1451  		}
1452  		if (background_thread_enabled()) {
1453  			background_thread_enabled_set(tsd_tsdn(tsd), false);
1454  			if (background_threads_disable(tsd)) {
1455  				ret = EFAULT;
1456  				goto label_return;
1457  			}
1458  			max_background_threads = newval;
1459  			background_thread_enabled_set(tsd_tsdn(tsd), true);
1460  			if (background_threads_enable(tsd)) {
1461  				ret = EFAULT;
1462  				goto label_return;
1463  			}
1464  		} else {
1465  			max_background_threads = newval;
1466  		}
1467  	}
1468  	ret = 0;
1469  label_return:
1470  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;background_thread_lock);
1471  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
1472  	return ret;
1473  }
1474  CTL_RO_CONFIG_GEN(config_cache_oblivious, bool)
1475  CTL_RO_CONFIG_GEN(config_debug, bool)
1476  CTL_RO_CONFIG_GEN(config_fill, bool)
1477  CTL_RO_CONFIG_GEN(config_lazy_lock, bool)
1478  CTL_RO_CONFIG_GEN(config_malloc_conf, const char *)
1479  CTL_RO_CONFIG_GEN(config_opt_safety_checks, bool)
1480  CTL_RO_CONFIG_GEN(config_prof, bool)
1481  CTL_RO_CONFIG_GEN(config_prof_libgcc, bool)
1482  CTL_RO_CONFIG_GEN(config_prof_libunwind, bool)
1483  CTL_RO_CONFIG_GEN(config_stats, bool)
1484  CTL_RO_CONFIG_GEN(config_utrace, bool)
1485  CTL_RO_CONFIG_GEN(config_xmalloc, bool)
1486  CTL_RO_NL_GEN(opt_abort, opt_abort, bool)
1487  CTL_RO_NL_GEN(opt_abort_conf, opt_abort_conf, bool)
1488  CTL_RO_NL_GEN(opt_confirm_conf, opt_confirm_conf, bool)
1489  CTL_RO_NL_GEN(opt_metadata_thp, metadata_thp_mode_names[opt_metadata_thp],
1490      const char *)
1491  CTL_RO_NL_GEN(opt_retain, opt_retain, bool)
1492  CTL_RO_NL_GEN(opt_dss, opt_dss, const char *)
1493  CTL_RO_NL_GEN(opt_narenas, opt_narenas, unsigned)
1494  CTL_RO_NL_GEN(opt_percpu_arena, percpu_arena_mode_names[opt_percpu_arena],
1495      const char *)
1496  CTL_RO_NL_GEN(opt_oversize_threshold, opt_oversize_threshold, size_t)
1497  CTL_RO_NL_GEN(opt_background_thread, opt_background_thread, bool)
1498  CTL_RO_NL_GEN(opt_max_background_threads, opt_max_background_threads, size_t)
1499  CTL_RO_NL_GEN(opt_dirty_decay_ms, opt_dirty_decay_ms, ssize_t)
1500  CTL_RO_NL_GEN(opt_muzzy_decay_ms, opt_muzzy_decay_ms, ssize_t)
1501  CTL_RO_NL_GEN(opt_stats_print, opt_stats_print, bool)
1502  CTL_RO_NL_GEN(opt_stats_print_opts, opt_stats_print_opts, const char *)
1503  CTL_RO_NL_CGEN(config_fill, opt_junk, opt_junk, const char *)
1504  CTL_RO_NL_CGEN(config_fill, opt_zero, opt_zero, bool)
1505  CTL_RO_NL_CGEN(config_utrace, opt_utrace, opt_utrace, bool)
1506  CTL_RO_NL_CGEN(config_xmalloc, opt_xmalloc, opt_xmalloc, bool)
1507  CTL_RO_NL_GEN(opt_tcache, opt_tcache, bool)
1508  CTL_RO_NL_GEN(opt_thp, thp_mode_names[opt_thp], const char *)
1509  CTL_RO_NL_GEN(opt_lg_extent_max_active_fit, opt_lg_extent_max_active_fit,
1510      size_t)
1511  CTL_RO_NL_GEN(opt_lg_tcache_max, opt_lg_tcache_max, ssize_t)
1512  CTL_RO_NL_CGEN(config_prof, opt_prof, opt_prof, bool)
1513  CTL_RO_NL_CGEN(config_prof, opt_prof_prefix, opt_prof_prefix, const char *)
1514  CTL_RO_NL_CGEN(config_prof, opt_prof_active, opt_prof_active, bool)
1515  CTL_RO_NL_CGEN(config_prof, opt_prof_thread_active_init,
1516      opt_prof_thread_active_init, bool)
1517  CTL_RO_NL_CGEN(config_prof, opt_lg_prof_sample, opt_lg_prof_sample, size_t)
1518  CTL_RO_NL_CGEN(config_prof, opt_prof_accum, opt_prof_accum, bool)
1519  CTL_RO_NL_CGEN(config_prof, opt_lg_prof_interval, opt_lg_prof_interval, ssize_t)
1520  CTL_RO_NL_CGEN(config_prof, opt_prof_gdump, opt_prof_gdump, bool)
1521  CTL_RO_NL_CGEN(config_prof, opt_prof_final, opt_prof_final, bool)
1522  CTL_RO_NL_CGEN(config_prof, opt_prof_leak, opt_prof_leak, bool)
1523  static int
1524  thread_arena_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1525      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1526  	int ret;
1527  	arena_t *oldarena;
1528  	unsigned newind, oldind;
1529  	oldarena = arena_choose(tsd, NULL);
1530  	if (oldarena == NULL) {
1531  		return EAGAIN;
1532  	}
1533  	newind = oldind = arena_ind_get(oldarena);
1534  	WRITE(newind, unsigned);
1535  	READ(oldind, unsigned);
1536  	if (newind != oldind) {
1537  		arena_t *newarena;
1538  		if (newind &gt;= narenas_total_get()) {
1539  			ret = EFAULT;
1540  			goto label_return;
1541  		}
1542  		if (have_percpu_arena &amp;&amp;
1543  		    PERCPU_ARENA_ENABLED(opt_percpu_arena)) {
1544  			if (newind &lt; percpu_arena_ind_limit(opt_percpu_arena)) {
1545  				ret = EPERM;
1546  				goto label_return;
1547  			}
1548  		}
1549  		newarena = arena_get(tsd_tsdn(tsd), newind, true);
1550  		if (newarena == NULL) {
1551  			ret = EAGAIN;
1552  			goto label_return;
1553  		}
1554  		arena_migrate(tsd, oldind, newind);
1555  		if (tcache_available(tsd)) {
1556  			tcache_arena_reassociate(tsd_tsdn(tsd),
1557  			    tsd_tcachep_get(tsd), newarena);
1558  		}
1559  	}
1560  	ret = 0;
1561  label_return:
1562  	return ret;
1563  }
1564  CTL_TSD_RO_NL_CGEN(config_stats, thread_allocated, tsd_thread_allocated_get,
1565      uint64_t)
1566  CTL_TSD_RO_NL_CGEN(config_stats, thread_allocatedp, tsd_thread_allocatedp_get,
1567      uint64_t *)
1568  CTL_TSD_RO_NL_CGEN(config_stats, thread_deallocated, tsd_thread_deallocated_get,
1569      uint64_t)
1570  CTL_TSD_RO_NL_CGEN(config_stats, thread_deallocatedp,
1571      tsd_thread_deallocatedp_get, uint64_t *)
1572  static int
1573  thread_tcache_enabled_ctl(tsd_t *tsd, const size_t *mib,
1574      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1575      size_t newlen) {
1576  	int ret;
1577  	bool oldval;
1578  	oldval = tcache_enabled_get(tsd);
1579  	if (newp != NULL) {
1580  		if (newlen != sizeof(bool)) {
1581  			ret = EINVAL;
1582  			goto label_return;
1583  		}
1584  		tcache_enabled_set(tsd, *(bool *)newp);
1585  	}
1586  	READ(oldval, bool);
1587  	ret = 0;
1588  label_return:
1589  	return ret;
1590  }
1591  static int
1592  thread_tcache_flush_ctl(tsd_t *tsd, const size_t *mib,
1593      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1594      size_t newlen) {
1595  	int ret;
1596  	if (!tcache_available(tsd)) {
1597  		ret = EFAULT;
1598  		goto label_return;
1599  	}
1600  	READONLY();
1601  	WRITEONLY();
1602  	tcache_flush(tsd);
1603  	ret = 0;
1604  label_return:
1605  	return ret;
1606  }
1607  static int
1608  thread_prof_name_ctl(tsd_t *tsd, const size_t *mib,
1609      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1610      size_t newlen) {
1611  	int ret;
1612  	if (!config_prof) {
1613  		return ENOENT;
1614  	}
1615  	READ_XOR_WRITE();
1616  	if (newp != NULL) {
1617  		if (newlen != sizeof(const char *)) {
1618  			ret = EINVAL;
1619  			goto label_return;
1620  		}
1621  		if ((ret = prof_thread_name_set(tsd, *(const char **)newp)) !=
1622  		    0) {
1623  			goto label_return;
1624  		}
1625  	} else {
1626  		const char *oldname = prof_thread_name_get(tsd);
1627  		READ(oldname, const char *);
1628  	}
1629  	ret = 0;
1630  label_return:
1631  	return ret;
1632  }
1633  static int
1634  thread_prof_active_ctl(tsd_t *tsd, const size_t *mib,
1635      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
1636      size_t newlen) {
1637  	int ret;
1638  	bool oldval;
1639  	if (!config_prof) {
1640  		return ENOENT;
1641  	}
1642  	oldval = prof_thread_active_get(tsd);
1643  	if (newp != NULL) {
1644  		if (newlen != sizeof(bool)) {
1645  			ret = EINVAL;
1646  			goto label_return;
1647  		}
1648  		if (prof_thread_active_set(tsd, *(bool *)newp)) {
1649  			ret = EAGAIN;
1650  			goto label_return;
1651  		}
1652  	}
1653  	READ(oldval, bool);
1654  	ret = 0;
1655  label_return:
1656  	return ret;
1657  }
1658  static int
1659  tcache_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1660      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1661  	int ret;
1662  	unsigned tcache_ind;
1663  	READONLY();
1664  	if (tcaches_create(tsd, &amp;tcache_ind)) {
1665  		ret = EFAULT;
1666  		goto label_return;
1667  	}
1668  	READ(tcache_ind, unsigned);
1669  	ret = 0;
1670  label_return:
1671  	return ret;
1672  }
1673  static int
1674  tcache_flush_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1675      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1676  	int ret;
1677  	unsigned tcache_ind;
1678  	WRITEONLY();
1679  	tcache_ind = UINT_MAX;
1680  	WRITE(tcache_ind, unsigned);
1681  	if (tcache_ind == UINT_MAX) {
1682  		ret = EFAULT;
1683  		goto label_return;
1684  	}
1685  	tcaches_flush(tsd, tcache_ind);
1686  	ret = 0;
1687  label_return:
1688  	return ret;
1689  }
1690  static int
1691  tcache_destroy_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1692      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1693  	int ret;
1694  	unsigned tcache_ind;
1695  	WRITEONLY();
1696  	tcache_ind = UINT_MAX;
1697  	WRITE(tcache_ind, unsigned);
1698  	if (tcache_ind == UINT_MAX) {
1699  		ret = EFAULT;
1700  		goto label_return;
1701  	}
1702  	tcaches_destroy(tsd, tcache_ind);
1703  	ret = 0;
1704  label_return:
1705  	return ret;
1706  }
1707  static int
1708  arena_i_initialized_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1709      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1710  	int ret;
1711  	tsdn_t *tsdn = tsd_tsdn(tsd);
1712  	unsigned arena_ind;
1713  	bool initialized;
1714  	READONLY();
1715  	MIB_UNSIGNED(arena_ind, 1);
1716  	malloc_mutex_lock(tsdn, &amp;ctl_mtx);
1717  	initialized = arenas_i(arena_ind)-&gt;initialized;
1718  	malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
1719  	READ(initialized, bool);
1720  	ret = 0;
1721  label_return:
1722  	return ret;
1723  }
1724  static void
1725  arena_i_decay(tsdn_t *tsdn, unsigned arena_ind, bool all) {
1726  	malloc_mutex_lock(tsdn, &amp;ctl_mtx);
1727  	{
1728  		unsigned narenas = ctl_arenas-&gt;narenas;
1729  		if (arena_ind == MALLCTL_ARENAS_ALL || arena_ind == narenas) {
1730  			unsigned i;
1731  			VARIABLE_ARRAY(arena_t *, tarenas, narenas);
1732  			for (i = 0; i &lt; narenas; i++) {
1733  				tarenas[i] = arena_get(tsdn, i, false);
1734  			}
1735  			malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
1736  			for (i = 0; i &lt; narenas; i++) {
1737  				if (tarenas[i] != NULL) {
1738  					arena_decay(tsdn, tarenas[i], false,
1739  					    all);
1740  				}
1741  			}
1742  		} else {
1743  			arena_t *tarena;
1744  			assert(arena_ind &lt; narenas);
1745  			tarena = arena_get(tsdn, arena_ind, false);
1746  			malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
1747  			if (tarena != NULL) {
1748  				arena_decay(tsdn, tarena, false, all);
1749  			}
1750  		}
1751  	}
1752  }
1753  static int
1754  arena_i_decay_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1755      size_t *oldlenp, void *newp, size_t newlen) {
1756  	int ret;
1757  	unsigned arena_ind;
1758  	READONLY();
1759  	WRITEONLY();
1760  	MIB_UNSIGNED(arena_ind, 1);
1761  	arena_i_decay(tsd_tsdn(tsd), arena_ind, false);
1762  	ret = 0;
1763  label_return:
1764  	return ret;
1765  }
1766  static int
1767  arena_i_purge_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1768      size_t *oldlenp, void *newp, size_t newlen) {
1769  	int ret;
1770  	unsigned arena_ind;
1771  	READONLY();
1772  	WRITEONLY();
1773  	MIB_UNSIGNED(arena_ind, 1);
1774  	arena_i_decay(tsd_tsdn(tsd), arena_ind, true);
1775  	ret = 0;
1776  label_return:
1777  	return ret;
1778  }
1779  static int
1780  arena_i_reset_destroy_helper(tsd_t *tsd, const size_t *mib, size_t miblen,
1781      void *oldp, size_t *oldlenp, void *newp, size_t newlen, unsigned *arena_ind,
1782      arena_t **arena) {
1783  	int ret;
1784  	READONLY();
1785  	WRITEONLY();
1786  	MIB_UNSIGNED(*arena_ind, 1);
1787  	*arena = arena_get(tsd_tsdn(tsd), *arena_ind, false);
1788  	if (*arena == NULL || arena_is_auto(*arena)) {
1789  		ret = EFAULT;
1790  		goto label_return;
1791  	}
1792  	ret = 0;
1793  label_return:
1794  	return ret;
1795  }
1796  static void
1797  arena_reset_prepare_background_thread(tsd_t *tsd, unsigned arena_ind) {
1798  	if (have_background_thread) {
1799  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;background_thread_lock);
1800  		if (background_thread_enabled()) {
1801  			background_thread_info_t *info =
1802  			    background_thread_info_get(arena_ind);
1803  			assert(info-&gt;state == background_thread_started);
1804  			malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
1805  			info-&gt;state = background_thread_paused;
1806  			malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
1807  		}
1808  	}
1809  }
1810  static void
1811  arena_reset_finish_background_thread(tsd_t *tsd, unsigned arena_ind) {
1812  	if (have_background_thread) {
1813  		if (background_thread_enabled()) {
1814  			background_thread_info_t *info =
1815  			    background_thread_info_get(arena_ind);
1816  			assert(info-&gt;state == background_thread_paused);
1817  			malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
1818  			info-&gt;state = background_thread_started;
1819  			malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
1820  		}
1821  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;background_thread_lock);
1822  	}
1823  }
1824  static int
1825  arena_i_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1826      size_t *oldlenp, void *newp, size_t newlen) {
1827  	int ret;
1828  	unsigned arena_ind;
1829  	arena_t *arena;
1830  	ret = arena_i_reset_destroy_helper(tsd, mib, miblen, oldp, oldlenp,
1831  	    newp, newlen, &amp;arena_ind, &amp;arena);
1832  	if (ret != 0) {
1833  		return ret;
1834  	}
1835  	arena_reset_prepare_background_thread(tsd, arena_ind);
1836  	arena_reset(tsd, arena);
1837  	arena_reset_finish_background_thread(tsd, arena_ind);
1838  	return ret;
1839  }
1840  static int
1841  arena_i_destroy_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1842      size_t *oldlenp, void *newp, size_t newlen) {
1843  	int ret;
1844  	unsigned arena_ind;
1845  	arena_t *arena;
1846  	ctl_arena_t *ctl_darena, *ctl_arena;
1847  	ret = arena_i_reset_destroy_helper(tsd, mib, miblen, oldp, oldlenp,
1848  	    newp, newlen, &amp;arena_ind, &amp;arena);
1849  	if (ret != 0) {
1850  		goto label_return;
1851  	}
1852  	if (arena_nthreads_get(arena, false) != 0 || arena_nthreads_get(arena,
1853  	    true) != 0) {
1854  		ret = EFAULT;
1855  		goto label_return;
1856  	}
1857  	arena_reset_prepare_background_thread(tsd, arena_ind);
1858  	arena_reset(tsd, arena);
1859  	arena_decay(tsd_tsdn(tsd), arena, false, true);
1860  	ctl_darena = arenas_i(MALLCTL_ARENAS_DESTROYED);
1861  	ctl_darena-&gt;initialized = true;
1862  	ctl_arena_refresh(tsd_tsdn(tsd), arena, ctl_darena, arena_ind, true);
1863  	arena_destroy(tsd, arena);
1864  	ctl_arena = arenas_i(arena_ind);
1865  	ctl_arena-&gt;initialized = false;
1866  	ql_elm_new(ctl_arena, destroyed_link);
1867  	ql_tail_insert(&amp;ctl_arenas-&gt;destroyed, ctl_arena, destroyed_link);
1868  	arena_reset_finish_background_thread(tsd, arena_ind);
1869  	assert(ret == 0);
1870  label_return:
1871  	return ret;
1872  }
1873  static int
1874  arena_i_dss_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
1875      size_t *oldlenp, void *newp, size_t newlen) {
1876  	int ret;
1877  	const char *dss = NULL;
1878  	unsigned arena_ind;
1879  	dss_prec_t dss_prec_old = dss_prec_limit;
1880  	dss_prec_t dss_prec = dss_prec_limit;
1881  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
1882  	WRITE(dss, const char *);
1883  	MIB_UNSIGNED(arena_ind, 1);
1884  	if (dss != NULL) {
1885  		int i;
1886  		bool match = false;
1887  		for (i = 0; i &lt; dss_prec_limit; i++) {
1888  			if (strcmp(dss_prec_names[i], dss) == 0) {
1889  				dss_prec = i;
1890  				match = true;
1891  				break;
1892  			}
1893  		}
1894  		if (!match) {
1895  			ret = EINVAL;
1896  			goto label_return;
1897  		}
1898  	}
1899  	if (arena_ind == MALLCTL_ARENAS_ALL || arena_ind ==
1900  	    ctl_arenas-&gt;narenas) {
1901  		if (dss_prec != dss_prec_limit &amp;&amp;
1902  		    extent_dss_prec_set(dss_prec)) {
1903  			ret = EFAULT;
1904  			goto label_return;
1905  		}
1906  		dss_prec_old = extent_dss_prec_get();
1907  	} else {
1908  		arena_t *arena = arena_get(tsd_tsdn(tsd), arena_ind, false);
1909  		if (arena == NULL || (dss_prec != dss_prec_limit &amp;&amp;
1910  		    arena_dss_prec_set(arena, dss_prec))) {
1911  			ret = EFAULT;
1912  			goto label_return;
1913  		}
1914  		dss_prec_old = arena_dss_prec_get(arena);
1915  	}
1916  	dss = dss_prec_names[dss_prec_old];
1917  	READ(dss, const char *);
1918  	ret = 0;
1919  label_return:
1920  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
1921  	return ret;
1922  }
1923  static int
1924  arena_i_decay_ms_ctl_impl(tsd_t *tsd, const size_t *mib, size_t miblen,
1925      void *oldp, size_t *oldlenp, void *newp, size_t newlen, bool dirty) {
1926  	int ret;
1927  	unsigned arena_ind;
1928  	arena_t *arena;
1929  	MIB_UNSIGNED(arena_ind, 1);
1930  	arena = arena_get(tsd_tsdn(tsd), arena_ind, false);
1931  	if (arena == NULL) {
1932  		ret = EFAULT;
1933  		goto label_return;
1934  	}
1935  	if (oldp != NULL &amp;&amp; oldlenp != NULL) {
1936  		size_t oldval = dirty ? arena_dirty_decay_ms_get(arena) :
1937  		    arena_muzzy_decay_ms_get(arena);
1938  		READ(oldval, ssize_t);
1939  	}
1940  	if (newp != NULL) {
1941  		if (newlen != sizeof(ssize_t)) {
1942  			ret = EINVAL;
1943  			goto label_return;
1944  		}
1945  		if (arena_is_huge(arena_ind) &amp;&amp; *(ssize_t *)newp &gt; 0) {
1946  			if (background_thread_create(tsd, arena_ind)) {
1947  				ret = EFAULT;
1948  				goto label_return;
1949  			}
1950  		}
1951  		if (dirty ? arena_dirty_decay_ms_set(tsd_tsdn(tsd), arena,
1952  		    *(ssize_t *)newp) : arena_muzzy_decay_ms_set(tsd_tsdn(tsd),
1953  		    arena, *(ssize_t *)newp)) {
1954  			ret = EFAULT;
1955  			goto label_return;
1956  		}
1957  	}
1958  	ret = 0;
1959  label_return:
1960  	return ret;
1961  }
1962  static int
1963  arena_i_dirty_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1964      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1965  	return arena_i_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
1966  	    newlen, true);
1967  }
1968  static int
1969  arena_i_muzzy_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1970      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1971  	return arena_i_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
1972  	    newlen, false);
1973  }
1974  static int
1975  arena_i_extent_hooks_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
1976      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
1977  	int ret;
1978  	unsigned arena_ind;
1979  	arena_t *arena;
1980  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
1981  	MIB_UNSIGNED(arena_ind, 1);
1982  	if (arena_ind &lt; narenas_total_get()) {
1983  		extent_hooks_t *old_extent_hooks;
1984  		arena = arena_get(tsd_tsdn(tsd), arena_ind, false);
1985  		if (arena == NULL) {
1986  			if (arena_ind &gt;= narenas_auto) {
1987  				ret = EFAULT;
1988  				goto label_return;
1989  			}
1990  			old_extent_hooks =
1991  			    (extent_hooks_t *)&amp;extent_hooks_default;
1992  			READ(old_extent_hooks, extent_hooks_t *);
1993  			if (newp != NULL) {
1994  				extent_hooks_t *new_extent_hooks
1995  				    JEMALLOC_CC_SILENCE_INIT(NULL);
1996  				WRITE(new_extent_hooks, extent_hooks_t *);
1997  				arena = arena_init(tsd_tsdn(tsd), arena_ind,
1998  				    new_extent_hooks);
1999  				if (arena == NULL) {
2000  					ret = EFAULT;
2001  					goto label_return;
2002  				}
2003  			}
2004  		} else {
2005  			if (newp != NULL) {
2006  				extent_hooks_t *new_extent_hooks
2007  				    JEMALLOC_CC_SILENCE_INIT(NULL);
2008  				WRITE(new_extent_hooks, extent_hooks_t *);
2009  				old_extent_hooks = extent_hooks_set(tsd, arena,
2010  				    new_extent_hooks);
2011  				READ(old_extent_hooks, extent_hooks_t *);
2012  			} else {
2013  				old_extent_hooks = extent_hooks_get(arena);
2014  				READ(old_extent_hooks, extent_hooks_t *);
2015  			}
2016  		}
2017  	} else {
2018  		ret = EFAULT;
2019  		goto label_return;
2020  	}
2021  	ret = 0;
2022  label_return:
2023  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
2024  	return ret;
2025  }
2026  static int
2027  arena_i_retain_grow_limit_ctl(tsd_t *tsd, const size_t *mib,
2028      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2029      size_t newlen) {
2030  	int ret;
2031  	unsigned arena_ind;
2032  	arena_t *arena;
2033  	if (!opt_retain) {
2034  		return ENOENT;
2035  	}
2036  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
2037  	MIB_UNSIGNED(arena_ind, 1);
2038  	if (arena_ind &lt; narenas_total_get() &amp;&amp; (arena =
2039  	    arena_get(tsd_tsdn(tsd), arena_ind, false)) != NULL) {
2040  		size_t old_limit, new_limit;
2041  		if (newp != NULL) {
2042  			WRITE(new_limit, size_t);
2043  		}
2044  		bool err = arena_retain_grow_limit_get_set(tsd, arena,
2045  		    &amp;old_limit, newp != NULL ? &amp;new_limit : NULL);
2046  		if (!err) {
2047  			READ(old_limit, size_t);
2048  			ret = 0;
2049  		} else {
2050  			ret = EFAULT;
2051  		}
2052  	} else {
2053  		ret = EFAULT;
2054  	}
2055  label_return:
2056  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
2057  	return ret;
2058  }
2059  static const ctl_named_node_t *
2060  arena_i_index(tsdn_t *tsdn, const size_t *mib, size_t miblen,
2061      size_t i) {
2062  	const ctl_named_node_t *ret;
2063  	malloc_mutex_lock(tsdn, &amp;ctl_mtx);
2064  	switch (i) {
2065  	case MALLCTL_ARENAS_ALL:
2066  	case MALLCTL_ARENAS_DESTROYED:
2067  		break;
2068  	default:
2069  		if (i &gt; ctl_arenas-&gt;narenas) {
2070  			ret = NULL;
2071  			goto label_return;
2072  		}
2073  		break;
2074  	}
2075  	ret = super_arena_i_node;
2076  label_return:
2077  	malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
2078  	return ret;
2079  }
2080  static int
2081  arenas_narenas_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2082      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2083  	int ret;
2084  	unsigned narenas;
2085  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
2086  	READONLY();
2087  	if (*oldlenp != sizeof(unsigned)) {
2088  		ret = EINVAL;
2089  		goto label_return;
2090  	}
2091  	narenas = ctl_arenas-&gt;narenas;
2092  	READ(narenas, unsigned);
2093  	ret = 0;
2094  label_return:
2095  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
2096  	return ret;
2097  }
2098  static int
2099  arenas_decay_ms_ctl_impl(tsd_t *tsd, const size_t *mib,
2100      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2101      size_t newlen, bool dirty) {
2102  	int ret;
2103  	if (oldp != NULL &amp;&amp; oldlenp != NULL) {
2104  		size_t oldval = (dirty ? arena_dirty_decay_ms_default_get() :
2105  		    arena_muzzy_decay_ms_default_get());
2106  		READ(oldval, ssize_t);
2107  	}
2108  	if (newp != NULL) {
2109  		if (newlen != sizeof(ssize_t)) {
2110  			ret = EINVAL;
2111  			goto label_return;
2112  		}
2113  		if (dirty ? arena_dirty_decay_ms_default_set(*(ssize_t *)newp)
2114  		    : arena_muzzy_decay_ms_default_set(*(ssize_t *)newp)) {
2115  			ret = EFAULT;
2116  			goto label_return;
2117  		}
2118  	}
2119  	ret = 0;
2120  label_return:
2121  	return ret;
2122  }
2123  static int
2124  arenas_dirty_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2125      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2126  	return arenas_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
2127  	    newlen, true);
2128  }
2129  static int
2130  arenas_muzzy_decay_ms_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2131      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2132  	return arenas_decay_ms_ctl_impl(tsd, mib, miblen, oldp, oldlenp, newp,
2133  	    newlen, false);
2134  }
2135  CTL_RO_NL_GEN(arenas_quantum, QUANTUM, size_t)
2136  CTL_RO_NL_GEN(arenas_page, PAGE, size_t)
2137  CTL_RO_NL_GEN(arenas_tcache_max, tcache_maxclass, size_t)
2138  CTL_RO_NL_GEN(arenas_nbins, SC_NBINS, unsigned)
2139  CTL_RO_NL_GEN(arenas_nhbins, nhbins, unsigned)
2140  CTL_RO_NL_GEN(arenas_bin_i_size, bin_infos[mib[2]].reg_size, size_t)
2141  CTL_RO_NL_GEN(arenas_bin_i_nregs, bin_infos[mib[2]].nregs, uint32_t)
2142  CTL_RO_NL_GEN(arenas_bin_i_slab_size, bin_infos[mib[2]].slab_size, size_t)
2143  CTL_RO_NL_GEN(arenas_bin_i_nshards, bin_infos[mib[2]].n_shards, uint32_t)
2144  static const ctl_named_node_t *
2145  arenas_bin_i_index(tsdn_t *tsdn, const size_t *mib,
2146      size_t miblen, size_t i) {
2147  	if (i &gt; SC_NBINS) {
2148  		return NULL;
2149  	}
2150  	return super_arenas_bin_i_node;
2151  }
2152  CTL_RO_NL_GEN(arenas_nlextents, SC_NSIZES - SC_NBINS, unsigned)
2153  CTL_RO_NL_GEN(arenas_lextent_i_size, sz_index2size(SC_NBINS+(szind_t)mib[2]),
2154      size_t)
2155  static const ctl_named_node_t *
2156  arenas_lextent_i_index(tsdn_t *tsdn, const size_t *mib,
2157      size_t miblen, size_t i) {
2158  	if (i &gt; SC_NSIZES - SC_NBINS) {
2159  		return NULL;
2160  	}
2161  	return super_arenas_lextent_i_node;
2162  }
2163  static int
2164  arenas_create_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2165      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2166  	int ret;
2167  	extent_hooks_t *extent_hooks;
2168  	unsigned arena_ind;
2169  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
2170  	extent_hooks = (extent_hooks_t *)&amp;extent_hooks_default;
2171  	WRITE(extent_hooks, extent_hooks_t *);
2172  	if ((arena_ind = ctl_arena_init(tsd, extent_hooks)) == UINT_MAX) {
2173  		ret = EAGAIN;
2174  		goto label_return;
2175  	}
2176  	READ(arena_ind, unsigned);
2177  	ret = 0;
2178  label_return:
2179  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
2180  	return ret;
2181  }
2182  static int
2183  arenas_lookup_ctl(tsd_t *tsd, const size_t *mib,
2184      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2185      size_t newlen) {
2186  	int ret;
2187  	unsigned arena_ind;
2188  	void *ptr;
2189  	extent_t *extent;
2190  	arena_t *arena;
2191  	ptr = NULL;
2192  	ret = EINVAL;
2193  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
2194  	WRITE(ptr, void *);
2195  	extent = iealloc(tsd_tsdn(tsd), ptr);
2196  	if (extent == NULL)
2197  		goto label_return;
2198  	arena = extent_arena_get(extent);
2199  	if (arena == NULL)
2200  		goto label_return;
2201  	arena_ind = arena_ind_get(arena);
2202  	READ(arena_ind, unsigned);
2203  	ret = 0;
2204  label_return:
2205  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
2206  	return ret;
2207  }
2208  static int
2209  prof_thread_active_init_ctl(tsd_t *tsd, const size_t *mib,
2210      size_t miblen, void *oldp, size_t *oldlenp, void *newp,
2211      size_t newlen) {
2212  	int ret;
2213  	bool oldval;
2214  	if (!config_prof) {
2215  		return ENOENT;
2216  	}
2217  	if (newp != NULL) {
2218  		if (newlen != sizeof(bool)) {
2219  			ret = EINVAL;
2220  			goto label_return;
2221  		}
2222  		oldval = prof_thread_active_init_set(tsd_tsdn(tsd),
2223  		    *(bool *)newp);
2224  	} else {
2225  		oldval = prof_thread_active_init_get(tsd_tsdn(tsd));
2226  	}
2227  	READ(oldval, bool);
2228  	ret = 0;
2229  label_return:
2230  	return ret;
2231  }
2232  static int
2233  prof_active_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2234      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2235  	int ret;
2236  	bool oldval;
2237  	if (!config_prof) {
2238  		return ENOENT;
2239  	}
2240  	if (newp != NULL) {
2241  		if (newlen != sizeof(bool)) {
2242  			ret = EINVAL;
2243  			goto label_return;
2244  		}
2245  		oldval = prof_active_set(tsd_tsdn(tsd), *(bool *)newp);
2246  	} else {
2247  		oldval = prof_active_get(tsd_tsdn(tsd));
2248  	}
2249  	READ(oldval, bool);
2250  	ret = 0;
2251  label_return:
2252  	return ret;
2253  }
2254  static int
2255  prof_dump_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2256      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2257  	int ret;
2258  	const char *filename = NULL;
2259  	if (!config_prof) {
2260  		return ENOENT;
2261  	}
2262  	WRITEONLY();
2263  	WRITE(filename, const char *);
2264  	if (prof_mdump(tsd, filename)) {
2265  		ret = EFAULT;
2266  		goto label_return;
2267  	}
2268  	ret = 0;
2269  label_return:
2270  	return ret;
2271  }
2272  static int
2273  prof_gdump_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2274      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2275  	int ret;
2276  	bool oldval;
2277  	if (!config_prof) {
2278  		return ENOENT;
2279  	}
2280  	if (newp != NULL) {
2281  		if (newlen != sizeof(bool)) {
2282  			ret = EINVAL;
2283  			goto label_return;
2284  		}
2285  		oldval = prof_gdump_set(tsd_tsdn(tsd), *(bool *)newp);
2286  	} else {
2287  		oldval = prof_gdump_get(tsd_tsdn(tsd));
2288  	}
2289  	READ(oldval, bool);
2290  	ret = 0;
2291  label_return:
2292  	return ret;
2293  }
2294  static int
2295  prof_reset_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2296      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2297  	int ret;
2298  	size_t lg_sample = lg_prof_sample;
2299  	if (!config_prof) {
2300  		return ENOENT;
2301  	}
2302  	WRITEONLY();
2303  	WRITE(lg_sample, size_t);
2304  	if (lg_sample &gt;= (sizeof(uint64_t) &lt;&lt; 3)) {
2305  		lg_sample = (sizeof(uint64_t) &lt;&lt; 3) - 1;
2306  	}
2307  	prof_reset(tsd, lg_sample);
2308  	ret = 0;
2309  label_return:
2310  	return ret;
2311  }
2312  CTL_RO_NL_CGEN(config_prof, prof_interval, prof_interval, uint64_t)
2313  CTL_RO_NL_CGEN(config_prof, lg_prof_sample, lg_prof_sample, size_t)
2314  static int
2315  prof_log_start_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
2316      size_t *oldlenp, void *newp, size_t newlen) {
2317  	int ret;
2318  	const char *filename = NULL;
2319  	if (!config_prof) {
2320  		return ENOENT;
2321  	}
2322  	WRITEONLY();
2323  	WRITE(filename, const char *);
2324  	if (prof_log_start(tsd_tsdn(tsd), filename)) {
2325  		ret = EFAULT;
2326  		goto label_return;
2327  	}
2328  	ret = 0;
2329  label_return:
2330  	return ret;
2331  }
2332  static int
2333  prof_log_stop_ctl(tsd_t *tsd, const size_t *mib, size_t miblen, void *oldp,
2334      size_t *oldlenp, void *newp, size_t newlen) {
2335  	if (!config_prof) {
2336  		return ENOENT;
2337  	}
2338  	if (prof_log_stop(tsd_tsdn(tsd))) {
2339  		return EFAULT;
2340  	}
2341  	return 0;
2342  }
2343  CTL_RO_CGEN(config_stats, stats_allocated, ctl_stats-&gt;allocated, size_t)
2344  CTL_RO_CGEN(config_stats, stats_active, ctl_stats-&gt;active, size_t)
2345  CTL_RO_CGEN(config_stats, stats_metadata, ctl_stats-&gt;metadata, size_t)
2346  CTL_RO_CGEN(config_stats, stats_metadata_thp, ctl_stats-&gt;metadata_thp, size_t)
2347  CTL_RO_CGEN(config_stats, stats_resident, ctl_stats-&gt;resident, size_t)
2348  CTL_RO_CGEN(config_stats, stats_mapped, ctl_stats-&gt;mapped, size_t)
2349  CTL_RO_CGEN(config_stats, stats_retained, ctl_stats-&gt;retained, size_t)
2350  CTL_RO_CGEN(config_stats, stats_background_thread_num_threads,
2351      ctl_stats-&gt;background_thread.num_threads, size_t)
2352  CTL_RO_CGEN(config_stats, stats_background_thread_num_runs,
2353      ctl_stats-&gt;background_thread.num_runs, uint64_t)
2354  CTL_RO_CGEN(config_stats, stats_background_thread_run_interval,
2355      nstime_ns(&amp;ctl_stats-&gt;background_thread.run_interval), uint64_t)
2356  CTL_RO_GEN(stats_arenas_i_dss, arenas_i(mib[2])-&gt;dss, const char *)
2357  CTL_RO_GEN(stats_arenas_i_dirty_decay_ms, arenas_i(mib[2])-&gt;dirty_decay_ms,
2358      ssize_t)
2359  CTL_RO_GEN(stats_arenas_i_muzzy_decay_ms, arenas_i(mib[2])-&gt;muzzy_decay_ms,
2360      ssize_t)
2361  CTL_RO_GEN(stats_arenas_i_nthreads, arenas_i(mib[2])-&gt;nthreads, unsigned)
2362  CTL_RO_GEN(stats_arenas_i_uptime,
2363      nstime_ns(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.uptime), uint64_t)
2364  CTL_RO_GEN(stats_arenas_i_pactive, arenas_i(mib[2])-&gt;pactive, size_t)
2365  CTL_RO_GEN(stats_arenas_i_pdirty, arenas_i(mib[2])-&gt;pdirty, size_t)
2366  CTL_RO_GEN(stats_arenas_i_pmuzzy, arenas_i(mib[2])-&gt;pmuzzy, size_t)
2367  CTL_RO_CGEN(config_stats, stats_arenas_i_mapped,
2368      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.mapped, ATOMIC_RELAXED),
2369      size_t)
2370  CTL_RO_CGEN(config_stats, stats_arenas_i_retained,
2371      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.retained, ATOMIC_RELAXED),
2372      size_t)
2373  CTL_RO_CGEN(config_stats, stats_arenas_i_extent_avail,
2374      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.extent_avail,
2375          ATOMIC_RELAXED),
2376      size_t)
2377  CTL_RO_CGEN(config_stats, stats_arenas_i_dirty_npurge,
2378      ctl_arena_stats_read_u64(
2379      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.decay_dirty.npurge), uint64_t)
2380  CTL_RO_CGEN(config_stats, stats_arenas_i_dirty_nmadvise,
2381      ctl_arena_stats_read_u64(
2382      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.decay_dirty.nmadvise), uint64_t)
2383  CTL_RO_CGEN(config_stats, stats_arenas_i_dirty_purged,
2384      ctl_arena_stats_read_u64(
2385      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.decay_dirty.purged), uint64_t)
2386  CTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_npurge,
2387      ctl_arena_stats_read_u64(
2388      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.decay_muzzy.npurge), uint64_t)
2389  CTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_nmadvise,
2390      ctl_arena_stats_read_u64(
2391      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.decay_muzzy.nmadvise), uint64_t)
2392  CTL_RO_CGEN(config_stats, stats_arenas_i_muzzy_purged,
2393      ctl_arena_stats_read_u64(
2394      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.decay_muzzy.purged), uint64_t)
2395  CTL_RO_CGEN(config_stats, stats_arenas_i_base,
2396      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.base, ATOMIC_RELAXED),
2397      size_t)
2398  CTL_RO_CGEN(config_stats, stats_arenas_i_internal,
2399      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.internal, ATOMIC_RELAXED),
2400      size_t)
2401  CTL_RO_CGEN(config_stats, stats_arenas_i_metadata_thp,
2402      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.metadata_thp,
2403      ATOMIC_RELAXED), size_t)
2404  CTL_RO_CGEN(config_stats, stats_arenas_i_tcache_bytes,
2405      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.tcache_bytes,
2406      ATOMIC_RELAXED), size_t)
2407  CTL_RO_CGEN(config_stats, stats_arenas_i_resident,
2408      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.resident, ATOMIC_RELAXED),
2409      size_t)
2410  CTL_RO_CGEN(config_stats, stats_arenas_i_abandoned_vm,
2411      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.abandoned_vm,
2412      ATOMIC_RELAXED), size_t)
2413  CTL_RO_CGEN(config_stats, stats_arenas_i_small_allocated,
2414      arenas_i(mib[2])-&gt;astats-&gt;allocated_small, size_t)
2415  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nmalloc,
2416      arenas_i(mib[2])-&gt;astats-&gt;nmalloc_small, uint64_t)
2417  CTL_RO_CGEN(config_stats, stats_arenas_i_small_ndalloc,
2418      arenas_i(mib[2])-&gt;astats-&gt;ndalloc_small, uint64_t)
2419  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nrequests,
2420      arenas_i(mib[2])-&gt;astats-&gt;nrequests_small, uint64_t)
2421  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nfills,
2422      arenas_i(mib[2])-&gt;astats-&gt;nfills_small, uint64_t)
2423  CTL_RO_CGEN(config_stats, stats_arenas_i_small_nflushes,
2424      arenas_i(mib[2])-&gt;astats-&gt;nflushes_small, uint64_t)
2425  CTL_RO_CGEN(config_stats, stats_arenas_i_large_allocated,
2426      atomic_load_zu(&amp;arenas_i(mib[2])-&gt;astats-&gt;astats.allocated_large,
2427      ATOMIC_RELAXED), size_t)
2428  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nmalloc,
2429      ctl_arena_stats_read_u64(
2430      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.nmalloc_large), uint64_t)
2431  CTL_RO_CGEN(config_stats, stats_arenas_i_large_ndalloc,
2432      ctl_arena_stats_read_u64(
2433      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.ndalloc_large), uint64_t)
2434  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nrequests,
2435      ctl_arena_stats_read_u64(
2436      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.nrequests_large), uint64_t)
2437  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nfills,
2438      ctl_arena_stats_read_u64(
2439      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.nmalloc_large), uint64_t)
2440  CTL_RO_CGEN(config_stats, stats_arenas_i_large_nflushes,
2441      ctl_arena_stats_read_u64(
2442      &amp;arenas_i(mib[2])-&gt;astats-&gt;astats.nflushes_large), uint64_t)
2443  #define RO_MUTEX_CTL_GEN(n, l)						\
2444  CTL_RO_CGEN(config_stats, stats_##n##_num_ops,				\
2445      l.n_lock_ops, uint64_t)						\
2446  CTL_RO_CGEN(config_stats, stats_##n##_num_wait,				\
2447      l.n_wait_times, uint64_t)						\
2448  CTL_RO_CGEN(config_stats, stats_##n##_num_spin_acq,			\
2449      l.n_spin_acquired, uint64_t)					\
2450  CTL_RO_CGEN(config_stats, stats_##n##_num_owner_switch,			\
2451      l.n_owner_switches, uint64_t) 					\
2452  CTL_RO_CGEN(config_stats, stats_##n##_total_wait_time,			\
2453      nstime_ns(&amp;l.tot_wait_time), uint64_t)				\
2454  CTL_RO_CGEN(config_stats, stats_##n##_max_wait_time,			\
2455      nstime_ns(&amp;l.max_wait_time), uint64_t)				\
2456  CTL_RO_CGEN(config_stats, stats_##n##_max_num_thds,			\
2457      l.max_n_thds, uint32_t)
2458  #define OP(mtx)								\
2459      RO_MUTEX_CTL_GEN(mutexes_##mtx,					\
2460          ctl_stats-&gt;mutex_prof_data[global_prof_mutex_##mtx])
2461  MUTEX_PROF_GLOBAL_MUTEXES
2462  #undef OP
2463  #define OP(mtx) RO_MUTEX_CTL_GEN(arenas_i_mutexes_##mtx,		\
2464      arenas_i(mib[2])-&gt;astats-&gt;astats.mutex_prof_data[arena_prof_mutex_##mtx])
2465  MUTEX_PROF_ARENA_MUTEXES
2466  #undef OP
2467  RO_MUTEX_CTL_GEN(arenas_i_bins_j_mutex,
2468      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].mutex_data)
2469  #undef RO_MUTEX_CTL_GEN
2470  static int
2471  stats_mutexes_reset_ctl(tsd_t *tsd, const size_t *mib,
2472      size_t miblen, void *oldp, size_t *oldlenp,
2473      void *newp, size_t newlen) {
2474  	if (!config_stats) {
2475  		return ENOENT;
2476  	}
2477  	tsdn_t *tsdn = tsd_tsdn(tsd);
2478  #define MUTEX_PROF_RESET(mtx)						\
2479      malloc_mutex_lock(tsdn, &amp;mtx);					\
2480      malloc_mutex_prof_data_reset(tsdn, &amp;mtx);				\
2481      malloc_mutex_unlock(tsdn, &amp;mtx);
2482  	MUTEX_PROF_RESET(ctl_mtx);
2483  	if (have_background_thread) {
2484  		MUTEX_PROF_RESET(background_thread_lock);
2485  	}
2486  	if (config_prof &amp;&amp; opt_prof) {
2487  		MUTEX_PROF_RESET(bt2gctx_mtx);
2488  	}
2489  	unsigned n = narenas_total_get();
2490  	for (unsigned i = 0; i &lt; n; i++) {
2491  		arena_t *arena = arena_get(tsdn, i, false);
2492  		if (!arena) {
2493  			continue;
2494  		}
2495  		MUTEX_PROF_RESET(arena-&gt;large_mtx);
2496  		MUTEX_PROF_RESET(arena-&gt;extent_avail_mtx);
2497  		MUTEX_PROF_RESET(arena-&gt;extents_dirty.mtx);
2498  		MUTEX_PROF_RESET(arena-&gt;extents_muzzy.mtx);
2499  		MUTEX_PROF_RESET(arena-&gt;extents_retained.mtx);
2500  		MUTEX_PROF_RESET(arena-&gt;decay_dirty.mtx);
2501  		MUTEX_PROF_RESET(arena-&gt;decay_muzzy.mtx);
2502  		MUTEX_PROF_RESET(arena-&gt;tcache_ql_mtx);
2503  		MUTEX_PROF_RESET(arena-&gt;base-&gt;mtx);
2504  		for (szind_t i = 0; i &lt; SC_NBINS; i++) {
2505  			for (unsigned j = 0; j &lt; bin_infos[i].n_shards; j++) {
2506  				bin_t *bin = &amp;arena-&gt;bins[i].bin_shards[j];
2507  				MUTEX_PROF_RESET(bin-&gt;lock);
2508  			}
2509  		}
2510  	}
2511  #undef MUTEX_PROF_RESET
2512  	return 0;
2513  }
2514  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nmalloc,
2515      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].nmalloc, uint64_t)
2516  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_ndalloc,
2517      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].ndalloc, uint64_t)
2518  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nrequests,
2519      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].nrequests, uint64_t)
2520  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_curregs,
2521      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].curregs, size_t)
2522  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nfills,
2523      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].nfills, uint64_t)
2524  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nflushes,
2525      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].nflushes, uint64_t)
2526  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nslabs,
2527      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].nslabs, uint64_t)
2528  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nreslabs,
2529      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].reslabs, uint64_t)
2530  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_curslabs,
2531      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].curslabs, size_t)
2532  CTL_RO_CGEN(config_stats, stats_arenas_i_bins_j_nonfull_slabs,
2533      arenas_i(mib[2])-&gt;astats-&gt;bstats[mib[4]].nonfull_slabs, size_t)
2534  static const ctl_named_node_t *
2535  stats_arenas_i_bins_j_index(tsdn_t *tsdn, const size_t *mib,
2536      size_t miblen, size_t j) {
2537  	if (j &gt; SC_NBINS) {
2538  		return NULL;
2539  	}
2540  	return super_stats_arenas_i_bins_j_node;
2541  }
2542  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_nmalloc,
2543      ctl_arena_stats_read_u64(
2544      &amp;arenas_i(mib[2])-&gt;astats-&gt;lstats[mib[4]].nmalloc), uint64_t)
2545  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_ndalloc,
2546      ctl_arena_stats_read_u64(
2547      &amp;arenas_i(mib[2])-&gt;astats-&gt;lstats[mib[4]].ndalloc), uint64_t)
2548  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_nrequests,
2549      ctl_arena_stats_read_u64(
2550      &amp;arenas_i(mib[2])-&gt;astats-&gt;lstats[mib[4]].nrequests), uint64_t)
2551  CTL_RO_CGEN(config_stats, stats_arenas_i_lextents_j_curlextents,
2552      arenas_i(mib[2])-&gt;astats-&gt;lstats[mib[4]].curlextents, size_t)
2553  static const ctl_named_node_t *
2554  stats_arenas_i_lextents_j_index(tsdn_t *tsdn, const size_t *mib,
2555      size_t miblen, size_t j) {
2556  	if (j &gt; SC_NSIZES - SC_NBINS) {
2557  		return NULL;
2558  	}
2559  	return super_stats_arenas_i_lextents_j_node;
2560  }
2561  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_ndirty,
2562      atomic_load_zu(
2563          &amp;arenas_i(mib[2])-&gt;astats-&gt;estats[mib[4]].ndirty,
2564  	ATOMIC_RELAXED), size_t);
2565  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_nmuzzy,
2566      atomic_load_zu(
2567          &amp;arenas_i(mib[2])-&gt;astats-&gt;estats[mib[4]].nmuzzy,
2568  	ATOMIC_RELAXED), size_t);
2569  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_nretained,
2570      atomic_load_zu(
2571          &amp;arenas_i(mib[2])-&gt;astats-&gt;estats[mib[4]].nretained,
2572  	ATOMIC_RELAXED), size_t);
2573  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_dirty_bytes,
2574      atomic_load_zu(
2575          &amp;arenas_i(mib[2])-&gt;astats-&gt;estats[mib[4]].dirty_bytes,
2576  	ATOMIC_RELAXED), size_t);
2577  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_muzzy_bytes,
2578      atomic_load_zu(
2579          &amp;arenas_i(mib[2])-&gt;astats-&gt;estats[mib[4]].muzzy_bytes,
2580  	ATOMIC_RELAXED), size_t);
2581  CTL_RO_CGEN(config_stats, stats_arenas_i_extents_j_retained_bytes,
2582      atomic_load_zu(
2583          &amp;arenas_i(mib[2])-&gt;astats-&gt;estats[mib[4]].retained_bytes,
2584  	ATOMIC_RELAXED), size_t);
2585  static const ctl_named_node_t *
2586  stats_arenas_i_extents_j_index(tsdn_t *tsdn, const size_t *mib,
2587      size_t miblen, size_t j) {
2588  	if (j &gt;= SC_NPSIZES) {
2589  		return NULL;
2590  	}
2591  	return super_stats_arenas_i_extents_j_node;
2592  }
2593  static bool
2594  ctl_arenas_i_verify(size_t i) {
2595  	size_t a = arenas_i2a_impl(i, true, true);
2596  	if (a == UINT_MAX || !ctl_arenas-&gt;arenas[a]-&gt;initialized) {
2597  		return true;
2598  	}
2599  	return false;
2600  }
2601  static const ctl_named_node_t *
2602  stats_arenas_i_index(tsdn_t *tsdn, const size_t *mib,
2603      size_t miblen, size_t i) {
2604  	const ctl_named_node_t *ret;
2605  	malloc_mutex_lock(tsdn, &amp;ctl_mtx);
2606  	if (ctl_arenas_i_verify(i)) {
2607  		ret = NULL;
2608  		goto label_return;
2609  	}
2610  	ret = super_stats_arenas_i_node;
2611  label_return:
2612  	malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
2613  	return ret;
2614  }
2615  static int
2616  experimental_hooks_install_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2617      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2618  	int ret;
2619  	if (oldp == NULL || oldlenp == NULL|| newp == NULL) {
2620  		ret = EINVAL;
2621  		goto label_return;
2622  	}
2623  	hooks_t hooks;
2624  	WRITE(hooks, hooks_t);
2625  	void *handle = hook_install(tsd_tsdn(tsd), &amp;hooks);
2626  	if (handle == NULL) {
2627  		ret = EAGAIN;
2628  		goto label_return;
2629  	}
2630  	READ(handle, void *);
2631  	ret = 0;
2632  label_return:
2633  	return ret;
2634  }
2635  static int
2636  experimental_hooks_remove_ctl(tsd_t *tsd, const size_t *mib, size_t miblen,
2637      void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2638  	int ret;
2639  	WRITEONLY();
2640  	void *handle = NULL;
2641  	WRITE(handle, void *);
2642  	if (handle == NULL) {
2643  		ret = EINVAL;
2644  		goto label_return;
2645  	}
2646  	hook_remove(tsd_tsdn(tsd), handle);
2647  	ret = 0;
2648  label_return:
2649  	return ret;
2650  }
2651  static int
2652  experimental_utilization_query_ctl(tsd_t *tsd, const size_t *mib,
2653      size_t miblen, void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2654  	int ret;
2655  	assert(sizeof(extent_util_stats_verbose_t)
2656  	    == sizeof(void *) + sizeof(size_t) * 5);
2657  	if (oldp == NULL || oldlenp == NULL
2658  	    || *oldlenp != sizeof(extent_util_stats_verbose_t)
2659  	    || newp == NULL) {
2660  		ret = EINVAL;
2661  		goto label_return;
2662  	}
2663  	void *ptr = NULL;
2664  	WRITE(ptr, void *);
2665  	extent_util_stats_verbose_t *util_stats
2666  	    = (extent_util_stats_verbose_t *)oldp;
2667  	extent_util_stats_verbose_get(tsd_tsdn(tsd), ptr,
2668  	    &amp;util_stats-&gt;nfree, &amp;util_stats-&gt;nregs, &amp;util_stats-&gt;size,
2669  	    &amp;util_stats-&gt;bin_nfree, &amp;util_stats-&gt;bin_nregs,
2670  	    &amp;util_stats-&gt;slabcur_addr);
2671  	ret = 0;
2672  label_return:
2673  	return ret;
2674  }
2675  static int
2676  experimental_utilization_batch_query_ctl(tsd_t *tsd, const size_t *mib,
2677      size_t miblen, void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2678  	int ret;
2679  	assert(sizeof(extent_util_stats_t) == sizeof(size_t) * 3);
2680  	const size_t len = newlen / sizeof(const void *);
2681  	if (oldp == NULL || oldlenp == NULL || newp == NULL || newlen == 0
2682  	    || newlen != len * sizeof(const void *)
2683  	    || *oldlenp != len * sizeof(extent_util_stats_t)) {
2684  		ret = EINVAL;
2685  		goto label_return;
2686  	}
2687  	void **ptrs = (void **)newp;
2688  	extent_util_stats_t *util_stats = (extent_util_stats_t *)oldp;
2689  	size_t i;
2690  	for (i = 0; i &lt; len; ++i) {
2691  		extent_util_stats_get(tsd_tsdn(tsd), ptrs[i],
2692  		    &amp;util_stats[i].nfree, &amp;util_stats[i].nregs,
2693  		    &amp;util_stats[i].size);
2694  	}
2695  	ret = 0;
2696  label_return:
2697  	return ret;
2698  }
2699  static const ctl_named_node_t *
2700  experimental_arenas_i_index(tsdn_t *tsdn, const size_t *mib,
2701      size_t miblen, size_t i) {
2702  	const ctl_named_node_t *ret;
2703  	malloc_mutex_lock(tsdn, &amp;ctl_mtx);
2704  	if (ctl_arenas_i_verify(i)) {
2705  		ret = NULL;
2706  		goto label_return;
2707  	}
2708  	ret = super_experimental_arenas_i_node;
2709  label_return:
2710  	malloc_mutex_unlock(tsdn, &amp;ctl_mtx);
2711  	return ret;
2712  }
2713  static int
2714  experimental_arenas_i_pactivep_ctl(tsd_t *tsd, const size_t *mib,
2715      size_t miblen, void *oldp, size_t *oldlenp, void *newp, size_t newlen) {
2716  	if (!config_stats) {
2717  		return ENOENT;
2718  	}
2719  	if (oldp == NULL || oldlenp == NULL || *oldlenp != sizeof(size_t *)) {
2720  		return EINVAL;
2721  	}
2722  	unsigned arena_ind;
2723  	arena_t *arena;
2724  	int ret;
2725  	size_t *pactivep;
2726  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;ctl_mtx);
2727  	READONLY();
2728  	MIB_UNSIGNED(arena_ind, 2);
2729  	if (arena_ind &lt; narenas_total_get() &amp;&amp; (arena =
2730  	    arena_get(tsd_tsdn(tsd), arena_ind, false)) != NULL) {
2731  #if defined(JEMALLOC_GCC_ATOMIC_ATOMICS) ||				\
2732      defined(JEMALLOC_GCC_SYNC_ATOMICS) || defined(_MSC_VER)
2733  		pactivep = (size_t *)&amp;(arena-&gt;nactive.repr);
2734  		READ(pactivep, size_t *);
2735  		ret = 0;
2736  #else
2737  		ret = EFAULT;
2738  #endif
2739  	} else {
2740  		ret = EFAULT;
2741  	}
2742  label_return:
2743  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;ctl_mtx);
2744  	return ret;
2745  }
</code></pre>
        </div>
        <div class="column">
            <h3>redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-background_thread_22.c</h3>
            <pre><code>1  #define JEMALLOC_BACKGROUND_THREAD_C_
2  #include &quot;jemalloc/internal/jemalloc_preamble.h&quot;
3  #include &quot;jemalloc/internal/jemalloc_internal_includes.h&quot;
4  #include &quot;jemalloc/internal/assert.h&quot;
5  JEMALLOC_DIAGNOSTIC_DISABLE_SPURIOUS
6  #define BACKGROUND_THREAD_DEFAULT false
7  bool opt_background_thread = BACKGROUND_THREAD_DEFAULT;
8  size_t opt_max_background_threads = MAX_BACKGROUND_THREAD_LIMIT + 1;
9  malloc_mutex_t background_thread_lock;
10  atomic_b_t background_thread_enabled_state;
11  size_t n_background_threads;
12  size_t max_background_threads;
13  background_thread_info_t *background_thread_info;
14  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
15  static int (*pthread_create_fptr)(pthread_t *__restrict, const pthread_attr_t *,
16      void *(*)(void *), void *__restrict);
17  static void
18  pthread_create_wrapper_init(void) {
19  #ifdef JEMALLOC_LAZY_LOCK
20  	if (!isthreaded) {
21  		isthreaded = true;
22  	}
23  #endif
24  }
25  int
26  pthread_create_wrapper(pthread_t *__restrict thread, const pthread_attr_t *attr,
27      void *(*start_routine)(void *), void *__restrict arg) {
28  	pthread_create_wrapper_init();
29  	return pthread_create_fptr(thread, attr, start_routine, arg);
30  }
31  #endif &amp;bsol;* JEMALLOC_PTHREAD_CREATE_WRAPPER */
32  #ifndef JEMALLOC_BACKGROUND_THREAD
33  #define NOT_REACHED { not_reached(); }
34  bool background_thread_create(tsd_t *tsd, unsigned arena_ind) NOT_REACHED
35  bool background_threads_enable(tsd_t *tsd) NOT_REACHED
36  bool background_threads_disable(tsd_t *tsd) NOT_REACHED
37  void background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,
38      arena_decay_t *decay, size_t npages_new) NOT_REACHED
39  void background_thread_prefork0(tsdn_t *tsdn) NOT_REACHED
40  void background_thread_prefork1(tsdn_t *tsdn) NOT_REACHED
41  void background_thread_postfork_parent(tsdn_t *tsdn) NOT_REACHED
42  void background_thread_postfork_child(tsdn_t *tsdn) NOT_REACHED
43  bool background_thread_stats_read(tsdn_t *tsdn,
44      background_thread_stats_t *stats) NOT_REACHED
45  void background_thread_ctl_init(tsdn_t *tsdn) NOT_REACHED
46  #undef NOT_REACHED
47  #else
48  static bool background_thread_enabled_at_fork;
49  static void
50  background_thread_info_init(tsdn_t *tsdn, background_thread_info_t *info) {
51  	background_thread_wakeup_time_set(tsdn, info, 0);
52  	info-&gt;npages_to_purge_new = 0;
53  	if (config_stats) {
54  		info-&gt;tot_n_runs = 0;
55  		nstime_init(&amp;info-&gt;tot_sleep_time, 0);
56  	}
57  }
58  static inline bool
59  set_current_thread_affinity(int cpu) {
60  #if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
61  	cpu_set_t cpuset;
62  	CPU_ZERO(&amp;cpuset);
63  	CPU_SET(cpu, &amp;cpuset);
64  	int ret = sched_setaffinity(0, sizeof(cpu_set_t), &amp;cpuset);
65  	return (ret != 0);
66  #else
67  	return false;
68  #endif
69  }
70  #define BACKGROUND_THREAD_NPAGES_THRESHOLD UINT64_C(1024)
71  #define BILLION UINT64_C(1000000000)
72  #define BACKGROUND_THREAD_MIN_INTERVAL_NS (BILLION / 10)
73  static inline size_t
74  decay_npurge_after_interval(arena_decay_t *decay, size_t interval) {
75  	size_t i;
76  	uint64_t sum = 0;
77  	for (i = 0; i &lt; interval; i++) {
78  		sum += decay-&gt;backlog[i] * h_steps[i];
79  	}
80  	for (; i &lt; SMOOTHSTEP_NSTEPS; i++) {
81  		sum += decay-&gt;backlog[i] * (h_steps[i] - h_steps[i - interval]);
82  	}
83  	return (size_t)(sum &gt;&gt; SMOOTHSTEP_BFP);
84  }
85  static uint64_t
86  arena_decay_compute_purge_interval_impl(tsdn_t *tsdn, arena_decay_t *decay,
87      extents_t *extents) {
88  	if (malloc_mutex_trylock(tsdn, &amp;decay-&gt;mtx)) {
89  		return BACKGROUND_THREAD_MIN_INTERVAL_NS;
90  	}
91  	uint64_t interval;
92  	ssize_t decay_time = atomic_load_zd(&amp;decay-&gt;time_ms, ATOMIC_RELAXED);
93  	if (decay_time &lt;= 0) {
94  		interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
95  		goto label_done;
96  	}
97  	uint64_t decay_interval_ns = nstime_ns(&amp;decay-&gt;interval);
98  	assert(decay_interval_ns &gt; 0);
99  	size_t npages = extents_npages_get(extents);
100  	if (npages == 0) {
101  		unsigned i;
102  		for (i = 0; i &lt; SMOOTHSTEP_NSTEPS; i++) {
103  			if (decay-&gt;backlog[i] &gt; 0) {
104  				break;
105  			}
106  		}
107  		if (i == SMOOTHSTEP_NSTEPS) {
108  			interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
109  			goto label_done;
110  		}
111  	}
112  	if (npages &lt;= BACKGROUND_THREAD_NPAGES_THRESHOLD) {
113  		interval = decay_interval_ns * SMOOTHSTEP_NSTEPS;
114  		goto label_done;
115  	}
116  	size_t lb = BACKGROUND_THREAD_MIN_INTERVAL_NS / decay_interval_ns;
117  	size_t ub = SMOOTHSTEP_NSTEPS;
118  	lb = (lb &lt; 2) ? 2 : lb;
119  	if ((decay_interval_ns * ub &lt;= BACKGROUND_THREAD_MIN_INTERVAL_NS) ||
120  	    (lb + 2 &gt; ub)) {
121  		interval = BACKGROUND_THREAD_MIN_INTERVAL_NS;
122  		goto label_done;
123  	}
124  	assert(lb + 2 &lt;= ub);
125  	size_t npurge_lb, npurge_ub;
126  	npurge_lb = decay_npurge_after_interval(decay, lb);
127  	if (npurge_lb &gt; BACKGROUND_THREAD_NPAGES_THRESHOLD) {
128  		interval = decay_interval_ns * lb;
129  		goto label_done;
130  	}
131  	npurge_ub = decay_npurge_after_interval(decay, ub);
132  	if (npurge_ub &lt; BACKGROUND_THREAD_NPAGES_THRESHOLD) {
133  		interval = decay_interval_ns * ub;
134  		goto label_done;
135  	}
136  	unsigned n_search = 0;
137  	size_t target, npurge;
138  	while ((npurge_lb + BACKGROUND_THREAD_NPAGES_THRESHOLD &lt; npurge_ub)
139  	    &amp;&amp; (lb + 2 &lt; ub)) {
140  		target = (lb + ub) / 2;
141  		npurge = decay_npurge_after_interval(decay, target);
142  		if (npurge &gt; BACKGROUND_THREAD_NPAGES_THRESHOLD) {
143  			ub = target;
144  			npurge_ub = npurge;
145  		} else {
146  			lb = target;
147  			npurge_lb = npurge;
148  		}
149  		assert(n_search++ &lt; lg_floor(SMOOTHSTEP_NSTEPS) + 1);
150  	}
151  	interval = decay_interval_ns * (ub + lb) / 2;
152  label_done:
153  	interval = (interval &lt; BACKGROUND_THREAD_MIN_INTERVAL_NS) ?
154  	    BACKGROUND_THREAD_MIN_INTERVAL_NS : interval;
155  	malloc_mutex_unlock(tsdn, &amp;decay-&gt;mtx);
156  	return interval;
157  }
158  static uint64_t
159  arena_decay_compute_purge_interval(tsdn_t *tsdn, arena_t *arena) {
160  	uint64_t i1, i2;
161  	i1 = arena_decay_compute_purge_interval_impl(tsdn, &amp;arena-&gt;decay_dirty,
162  	    &amp;arena-&gt;extents_dirty);
163  	if (i1 == BACKGROUND_THREAD_MIN_INTERVAL_NS) {
164  		return i1;
165  	}
166  	i2 = arena_decay_compute_purge_interval_impl(tsdn, &amp;arena-&gt;decay_muzzy,
167  	    &amp;arena-&gt;extents_muzzy);
168  	return i1 &lt; i2 ? i1 : i2;
169  }
170  static void
171  background_thread_sleep(tsdn_t *tsdn, background_thread_info_t *info,
172      uint64_t interval) {
173  	if (config_stats) {
174  		info-&gt;tot_n_runs++;
175  	}
176  	info-&gt;npages_to_purge_new = 0;
177  	struct timeval tv;
178  	gettimeofday(&amp;tv, NULL);
179  	nstime_t before_sleep;
180  	nstime_init2(&amp;before_sleep, tv.tv_sec, tv.tv_usec * 1000);
181  	int ret;
182  	if (interval == BACKGROUND_THREAD_INDEFINITE_SLEEP) {
183  		assert(background_thread_indefinite_sleep(info));
184  		ret = pthread_cond_wait(&amp;info-&gt;cond, &amp;info-&gt;mtx.lock);
185  		assert(ret == 0);
186  	} else {
187  		assert(interval &gt;= BACKGROUND_THREAD_MIN_INTERVAL_NS &amp;&amp;
188  		    interval &lt;= BACKGROUND_THREAD_INDEFINITE_SLEEP);
189  		nstime_t next_wakeup;
190  		nstime_init(&amp;next_wakeup, 0);
191  		nstime_update(&amp;next_wakeup);
192  		nstime_iadd(&amp;next_wakeup, interval);
193  		assert(nstime_ns(&amp;next_wakeup) &lt;
194  		    BACKGROUND_THREAD_INDEFINITE_SLEEP);
195  		background_thread_wakeup_time_set(tsdn, info,
196  		    nstime_ns(&amp;next_wakeup));
197  		nstime_t ts_wakeup;
198  		nstime_copy(&amp;ts_wakeup, &amp;before_sleep);
199  		nstime_iadd(&amp;ts_wakeup, interval);
200  		struct timespec ts;
201  		ts.tv_sec = (size_t)nstime_sec(&amp;ts_wakeup);
202  		ts.tv_nsec = (size_t)nstime_nsec(&amp;ts_wakeup);
203  		assert(!background_thread_indefinite_sleep(info));
204  		ret = pthread_cond_timedwait(&amp;info-&gt;cond, &amp;info-&gt;mtx.lock, &amp;ts);
205  		assert(ret == ETIMEDOUT || ret == 0);
206  		background_thread_wakeup_time_set(tsdn, info,
207  		    BACKGROUND_THREAD_INDEFINITE_SLEEP);
208  	}
209  	if (config_stats) {
210  		gettimeofday(&amp;tv, NULL);
211  		nstime_t after_sleep;
212  		nstime_init2(&amp;after_sleep, tv.tv_sec, tv.tv_usec * 1000);
213  		if (nstime_compare(&amp;after_sleep, &amp;before_sleep) &gt; 0) {
214  			nstime_subtract(&amp;after_sleep, &amp;before_sleep);
215  			nstime_add(&amp;info-&gt;tot_sleep_time, &amp;after_sleep);
216  		}
217  	}
218  }
219  static bool
220  background_thread_pause_check(tsdn_t *tsdn, background_thread_info_t *info) {
221  	if (unlikely(info-&gt;state == background_thread_paused)) {
222  		malloc_mutex_unlock(tsdn, &amp;info-&gt;mtx);
223  		malloc_mutex_lock(tsdn, &amp;background_thread_lock);
224  		malloc_mutex_unlock(tsdn, &amp;background_thread_lock);
225  		malloc_mutex_lock(tsdn, &amp;info-&gt;mtx);
226  		return true;
227  	}
228  	return false;
229  }
230  static inline void
231  background_work_sleep_once(tsdn_t *tsdn, background_thread_info_t *info, unsigned ind) {
232  	uint64_t min_interval = BACKGROUND_THREAD_INDEFINITE_SLEEP;
233  	unsigned narenas = narenas_total_get();
234  	for (unsigned i = ind; i &lt; narenas; i += max_background_threads) {
235  		arena_t *arena = arena_get(tsdn, i, false);
236  		if (!arena) {
237  			continue;
238  		}
239  		arena_decay(tsdn, arena, true, false);
240  		if (min_interval == BACKGROUND_THREAD_MIN_INTERVAL_NS) {
241  			continue;
242  		}
243  		uint64_t interval = arena_decay_compute_purge_interval(tsdn,
244  		    arena);
245  		assert(interval &gt;= BACKGROUND_THREAD_MIN_INTERVAL_NS);
246  		if (min_interval &gt; interval) {
247  			min_interval = interval;
248  		}
249  	}
250  	background_thread_sleep(tsdn, info, min_interval);
251  }
252  static bool
253  background_threads_disable_single(tsd_t *tsd, background_thread_info_t *info) {
254  	if (info == &amp;background_thread_info[0]) {
255  		malloc_mutex_assert_owner(tsd_tsdn(tsd),
256  		    &amp;background_thread_lock);
257  	} else {
258  		malloc_mutex_assert_not_owner(tsd_tsdn(tsd),
259  		    &amp;background_thread_lock);
260  	}
261  	pre_reentrancy(tsd, NULL);
262  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
263  	bool has_thread;
264  	assert(info-&gt;state != background_thread_paused);
265  	if (info-&gt;state == background_thread_started) {
266  		has_thread = true;
267  		info-&gt;state = background_thread_stopped;
268  		pthread_cond_signal(&amp;info-&gt;cond);
269  	} else {
270  		has_thread = false;
271  	}
272  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
273  	if (!has_thread) {
274  		post_reentrancy(tsd);
275  		return false;
276  	}
277  	void *ret;
278  	if (pthread_join(info-&gt;thread, &amp;ret)) {
279  		post_reentrancy(tsd);
280  		return true;
281  	}
282  	assert(ret == NULL);
283  	n_background_threads--;
284  	post_reentrancy(tsd);
285  	return false;
286  }
287  static void *background_thread_entry(void *ind_arg);
288  static int
289  background_thread_create_signals_masked(pthread_t *thread,
290      const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg) {
291  	sigset_t set;
292  	sigfillset(&amp;set);
293  	sigset_t oldset;
294  	int mask_err = pthread_sigmask(SIG_SETMASK, &amp;set, &amp;oldset);
295  	if (mask_err != 0) {
296  		return mask_err;
297  	}
298  	int create_err = pthread_create_wrapper(thread, attr, start_routine,
299  	    arg);
300  	int restore_err = pthread_sigmask(SIG_SETMASK, &amp;oldset, NULL);
301  	if (restore_err != 0) {
302  		malloc_printf(&quot;&lt;jemalloc&gt;: background thread creation &quot;
303  		    &quot;failed (%d), and signal mask restoration failed &quot;
304  		    &quot;(%d)\n&quot;, create_err, restore_err);
305  		if (opt_abort) {
306  			abort();
307  		}
308  	}
309  	return create_err;
310  }
311  static bool
312  check_background_thread_creation(tsd_t *tsd, unsigned *n_created,
313      bool *created_threads) {
314  	bool ret = false;
315  	if (likely(*n_created == n_background_threads)) {
316  		return ret;
317  	}
318  	tsdn_t *tsdn = tsd_tsdn(tsd);
319  	malloc_mutex_unlock(tsdn, &amp;background_thread_info[0].mtx);
320  	for (unsigned i = 1; i &lt; max_background_threads; i++) {
321  		if (created_threads[i]) {
322  			continue;
323  		}
324  		background_thread_info_t *info = &amp;background_thread_info[i];
325  		malloc_mutex_lock(tsdn, &amp;info-&gt;mtx);
326  		bool create = (info-&gt;state == background_thread_started);
327  		malloc_mutex_unlock(tsdn, &amp;info-&gt;mtx);
328  		if (!create) {
329  			continue;
330  		}
331  		pre_reentrancy(tsd, NULL);
332  		int err = background_thread_create_signals_masked(&amp;info-&gt;thread,
333  		    NULL, background_thread_entry, (void *)(uintptr_t)i);
334  		post_reentrancy(tsd);
335  		if (err == 0) {
336  			(*n_created)++;
337  			created_threads[i] = true;
338  		} else {
339  			malloc_printf(&quot;&lt;jemalloc&gt;: background thread &quot;
340  			    &quot;creation failed (%d)\n&quot;, err);
341  			if (opt_abort) {
342  				abort();
343  			}
344  		}
345  		ret = true;
346  		break;
347  	}
348  	malloc_mutex_lock(tsdn, &amp;background_thread_info[0].mtx);
349  	return ret;
350  }
351  static void
352  background_thread0_work(tsd_t *tsd) {
353  	VARIABLE_ARRAY(bool, created_threads, max_background_threads);
354  	unsigned i;
355  	for (i = 1; i &lt; max_background_threads; i++) {
356  		created_threads[i] = false;
357  	}
358  	unsigned n_created = 1;
359  	while (background_thread_info[0].state != background_thread_stopped) {
360  		if (background_thread_pause_check(tsd_tsdn(tsd),
361  		    &amp;background_thread_info[0])) {
362  			continue;
363  		}
364  		if (check_background_thread_creation(tsd, &amp;n_created,
365  		    (bool *)&amp;created_threads)) {
366  			continue;
367  		}
368  		background_work_sleep_once(tsd_tsdn(tsd),
369  		    &amp;background_thread_info[0], 0);
370  	}
371  	assert(!background_thread_enabled());
372  	for (i = 1; i &lt; max_background_threads; i++) {
373  		background_thread_info_t *info = &amp;background_thread_info[i];
374  		assert(info-&gt;state != background_thread_paused);
375  		if (created_threads[i]) {
376  			background_threads_disable_single(tsd, info);
377  		} else {
378  			malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
379  			if (info-&gt;state != background_thread_stopped) {
380  				assert(info-&gt;state ==
381  				    background_thread_started);
382  				n_background_threads--;
383  				info-&gt;state = background_thread_stopped;
384  			}
385  			malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
386  		}
387  	}
388  	background_thread_info[0].state = background_thread_stopped;
389  	assert(n_background_threads == 1);
390  }
391  static void
392  background_work(tsd_t *tsd, unsigned ind) {
393  	background_thread_info_t *info = &amp;background_thread_info[ind];
394  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
395  	background_thread_wakeup_time_set(tsd_tsdn(tsd), info,
396  	    BACKGROUND_THREAD_INDEFINITE_SLEEP);
397  	if (ind == 0) {
398  		background_thread0_work(tsd);
399  	} else {
400  		while (info-&gt;state != background_thread_stopped) {
401  			if (background_thread_pause_check(tsd_tsdn(tsd),
402  			    info)) {
403  				continue;
404  			}
405  			background_work_sleep_once(tsd_tsdn(tsd), info, ind);
406  		}
407  	}
408  	assert(info-&gt;state == background_thread_stopped);
409  	background_thread_wakeup_time_set(tsd_tsdn(tsd), info, 0);
410  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
411  }
412  static void *
413  background_thread_entry(void *ind_arg) {
414  	unsigned thread_ind = (unsigned)(uintptr_t)ind_arg;
415  	assert(thread_ind &lt; max_background_threads);
416  #ifdef JEMALLOC_HAVE_PTHREAD_SETNAME_NP
417  	pthread_setname_np(pthread_self(), &quot;jemalloc_bg_thd&quot;);
418  #elif defined(__FreeBSD__)
419  	pthread_set_name_np(pthread_self(), &quot;jemalloc_bg_thd&quot;);
420  #endif
421  	if (opt_percpu_arena != percpu_arena_disabled) {
422  		set_current_thread_affinity((int)thread_ind);
423  	}
424  	background_work(tsd_internal_fetch(), thread_ind);
425  	assert(pthread_equal(pthread_self(),
426  	    background_thread_info[thread_ind].thread));
427  	return NULL;
428  }
429  static void
430  background_thread_init(tsd_t *tsd, background_thread_info_t *info) {
431  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;background_thread_lock);
432  	info-&gt;state = background_thread_started;
433  	background_thread_info_init(tsd_tsdn(tsd), info);
434  	n_background_threads++;
435  }
436  static bool
437  background_thread_create_locked(tsd_t *tsd, unsigned arena_ind) {
438  	assert(have_background_thread);
439  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;background_thread_lock);
440  	size_t thread_ind = arena_ind % max_background_threads;
441  	background_thread_info_t *info = &amp;background_thread_info[thread_ind];
442  	bool need_new_thread;
443  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
444  	need_new_thread = background_thread_enabled() &amp;&amp;
445  	    (info-&gt;state == background_thread_stopped);
446  	if (need_new_thread) {
447  		background_thread_init(tsd, info);
448  	}
449  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
450  	if (!need_new_thread) {
451  		return false;
452  	}
453  	if (arena_ind != 0) {
454  		background_thread_info_t *t0 = &amp;background_thread_info[0];
455  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;t0-&gt;mtx);
456  		assert(t0-&gt;state == background_thread_started);
457  		pthread_cond_signal(&amp;t0-&gt;cond);
458  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;t0-&gt;mtx);
459  		return false;
460  	}
461  	pre_reentrancy(tsd, NULL);
462  	int err = background_thread_create_signals_masked(&amp;info-&gt;thread, NULL,
463  	    background_thread_entry, (void *)thread_ind);
464  	post_reentrancy(tsd);
465  	if (err != 0) {
466  		malloc_printf(&quot;&lt;jemalloc&gt;: arena 0 background thread creation &quot;
467  		    &quot;failed (%d)\n&quot;, err);
468  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
469  		info-&gt;state = background_thread_stopped;
470  		n_background_threads--;
471  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
472  		return true;
473  	}
474  	return false;
475  }
476  bool
477  background_thread_create(tsd_t *tsd, unsigned arena_ind) {
478  	assert(have_background_thread);
479  	bool ret;
480  	malloc_mutex_lock(tsd_tsdn(tsd), &amp;background_thread_lock);
481  	ret = background_thread_create_locked(tsd, arena_ind);
482  	malloc_mutex_unlock(tsd_tsdn(tsd), &amp;background_thread_lock);
483  	return ret;
484  }
485  bool
486  background_threads_enable(tsd_t *tsd) {
487  	assert(n_background_threads == 0);
488  	assert(background_thread_enabled());
489  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;background_thread_lock);
490  	VARIABLE_ARRAY(bool, marked, max_background_threads);
491  	unsigned i, nmarked;
492  	for (i = 0; i &lt; max_background_threads; i++) {
493  		marked[i] = false;
494  	}
495  	nmarked = 0;
496  	marked[0] = true;
497  	unsigned n = narenas_total_get();
498  	for (i = 1; i &lt; n; i++) {
499  		if (marked[i % max_background_threads] ||
500  		    arena_get(tsd_tsdn(tsd), i, false) == NULL) {
501  			continue;
502  		}
503  		background_thread_info_t *info = &amp;background_thread_info[
504  		    i % max_background_threads];
505  		malloc_mutex_lock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
506  		assert(info-&gt;state == background_thread_stopped);
507  		background_thread_init(tsd, info);
508  		malloc_mutex_unlock(tsd_tsdn(tsd), &amp;info-&gt;mtx);
509  		marked[i % max_background_threads] = true;
510  		if (++nmarked == max_background_threads) {
511  			break;
512  		}
513  	}
514  	return background_thread_create_locked(tsd, 0);
515  }
516  bool
517  background_threads_disable(tsd_t *tsd) {
518  	assert(!background_thread_enabled());
519  	malloc_mutex_assert_owner(tsd_tsdn(tsd), &amp;background_thread_lock);
520  	if (background_threads_disable_single(tsd,
521  	    &amp;background_thread_info[0])) {
522  		return true;
523  	}
524  	assert(n_background_threads == 0);
525  	return false;
526  }
527  void
528  background_thread_interval_check(tsdn_t *tsdn, arena_t *arena,
529      arena_decay_t *decay, size_t npages_new) {
530  	background_thread_info_t *info = arena_background_thread_info_get(
531  	    arena);
532  	if (malloc_mutex_trylock(tsdn, &amp;info-&gt;mtx)) {
533  		return;
534  	}
535  	if (info-&gt;state != background_thread_started) {
536  		goto label_done;
537  	}
538  	if (malloc_mutex_trylock(tsdn, &amp;decay-&gt;mtx)) {
539  		goto label_done;
540  	}
541  	ssize_t decay_time = atomic_load_zd(&amp;decay-&gt;time_ms, ATOMIC_RELAXED);
542  	if (decay_time &lt;= 0) {
543  		goto label_done_unlock2;
544  	}
545  	uint64_t decay_interval_ns = nstime_ns(&amp;decay-&gt;interval);
546  	assert(decay_interval_ns &gt; 0);
547  	nstime_t diff;
548  	nstime_init(&amp;diff, background_thread_wakeup_time_get(info));
549  	if (nstime_compare(&amp;diff, &amp;decay-&gt;epoch) &lt;= 0) {
550  		goto label_done_unlock2;
551  	}
552  	nstime_subtract(&amp;diff, &amp;decay-&gt;epoch);
553  	if (nstime_ns(&amp;diff) &lt; BACKGROUND_THREAD_MIN_INTERVAL_NS) {
554  		goto label_done_unlock2;
555  	}
556  	if (npages_new &gt; 0) {
557  		size_t n_epoch = (size_t)(nstime_ns(&amp;diff) / decay_interval_ns);
558  		uint64_t npurge_new;
559  		if (n_epoch &gt;= SMOOTHSTEP_NSTEPS) {
560  			npurge_new = npages_new;
561  		} else {
562  			uint64_t h_steps_max = h_steps[SMOOTHSTEP_NSTEPS - 1];
563  			assert(h_steps_max &gt;=
564  			    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);
565  			npurge_new = npages_new * (h_steps_max -
566  			    h_steps[SMOOTHSTEP_NSTEPS - 1 - n_epoch]);
567  			npurge_new &gt;&gt;= SMOOTHSTEP_BFP;
568  		}
569  		info-&gt;npages_to_purge_new += npurge_new;
570  	}
571  	bool should_signal;
572  	if (info-&gt;npages_to_purge_new &gt; BACKGROUND_THREAD_NPAGES_THRESHOLD) {
573  		should_signal = true;
574  	} else if (unlikely(background_thread_indefinite_sleep(info)) &amp;&amp;
575  	    (extents_npages_get(&amp;arena-&gt;extents_dirty) &gt; 0 ||
576  	    extents_npages_get(&amp;arena-&gt;extents_muzzy) &gt; 0 ||
577  	    info-&gt;npages_to_purge_new &gt; 0)) {
578  		should_signal = true;
579  	} else {
580  		should_signal = false;
581  	}
582  	if (should_signal) {
583  		info-&gt;npages_to_purge_new = 0;
584  		pthread_cond_signal(&amp;info-&gt;cond);
585  	}
586  label_done_unlock2:
587  	malloc_mutex_unlock(tsdn, &amp;decay-&gt;mtx);
588  label_done:
589  	malloc_mutex_unlock(tsdn, &amp;info-&gt;mtx);
590  }
591  void
592  background_thread_prefork0(tsdn_t *tsdn) {
593  	malloc_mutex_prefork(tsdn, &amp;background_thread_lock);
594  	background_thread_enabled_at_fork = background_thread_enabled();
595  }
596  void
597  background_thread_prefork1(tsdn_t *tsdn) {
598  	for (unsigned i = 0; i &lt; max_background_threads; i++) {
599  		malloc_mutex_prefork(tsdn, &amp;background_thread_info[i].mtx);
600  	}
601  }
602  void
603  background_thread_postfork_parent(tsdn_t *tsdn) {
604  	for (unsigned i = 0; i &lt; max_background_threads; i++) {
605  		malloc_mutex_postfork_parent(tsdn,
606  		    &amp;background_thread_info[i].mtx);
607  	}
608  	malloc_mutex_postfork_parent(tsdn, &amp;background_thread_lock);
609  }
610  void
611  background_thread_postfork_child(tsdn_t *tsdn) {
612  	for (unsigned i = 0; i &lt; max_background_threads; i++) {
613  		malloc_mutex_postfork_child(tsdn,
614  		    &amp;background_thread_info[i].mtx);
615  	}
616  	malloc_mutex_postfork_child(tsdn, &amp;background_thread_lock);
617  	if (!background_thread_enabled_at_fork) {
618  		return;
619  	}
620  	malloc_mutex_lock(tsdn, &amp;background_thread_lock);
621  	n_background_threads = 0;
622  	background_thread_enabled_set(tsdn, false);
623  	for (unsigned i = 0; i &lt; max_background_threads; i++) {
624  		background_thread_info_t *info = &amp;background_thread_info[i];
625  		malloc_mutex_lock(tsdn, &amp;info-&gt;mtx);
626  		info-&gt;state = background_thread_stopped;
627  		int ret = pthread_cond_init(&amp;info-&gt;cond, NULL);
628  		assert(ret == 0);
629  		background_thread_info_init(tsdn, info);
630  		malloc_mutex_unlock(tsdn, &amp;info-&gt;mtx);
631  	}
632  	malloc_mutex_unlock(tsdn, &amp;background_thread_lock);
633  }
634  bool
635  background_thread_stats_read(tsdn_t *tsdn, background_thread_stats_t *stats) {
636  	assert(config_stats);
637  	malloc_mutex_lock(tsdn, &amp;background_thread_lock);
638  	if (!background_thread_enabled()) {
639  		malloc_mutex_unlock(tsdn, &amp;background_thread_lock);
640  		return true;
641  	}
642  	stats-&gt;num_threads = n_background_threads;
643  	uint64_t num_runs = 0;
644  	nstime_init(&amp;stats-&gt;run_interval, 0);
645  	for (unsigned i = 0; i &lt; max_background_threads; i++) {
646  		background_thread_info_t *info = &amp;background_thread_info[i];
647  		if (malloc_mutex_trylock(tsdn, &amp;info-&gt;mtx)) {
648  			continue;
649  		}
650  		if (info-&gt;state != background_thread_stopped) {
651  			num_runs += info-&gt;tot_n_runs;
652  			nstime_add(&amp;stats-&gt;run_interval, &amp;info-&gt;tot_sleep_time);
653  		}
654  		malloc_mutex_unlock(tsdn, &amp;info-&gt;mtx);
655  	}
656  	stats-&gt;num_runs = num_runs;
657  	if (num_runs &gt; 0) {
658  		nstime_idivide(&amp;stats-&gt;run_interval, num_runs);
659  	}
660  	malloc_mutex_unlock(tsdn, &amp;background_thread_lock);
661  	return false;
662  }
663  #undef BACKGROUND_THREAD_NPAGES_THRESHOLD
664  #undef BILLION
665  #undef BACKGROUND_THREAD_MIN_INTERVAL_NS
666  #ifdef JEMALLOC_HAVE_DLSYM
667  #include &lt;dlfcn.h&gt;
668  #endif
669  static bool
670  pthread_create_fptr_init(void) {
671  	if (pthread_create_fptr != NULL) {
672  		return false;
673  	}
674  #ifdef JEMALLOC_HAVE_DLSYM
675  	pthread_create_fptr = dlsym(RTLD_NEXT, &quot;pthread_create&quot;);
676  #else
677  	pthread_create_fptr = NULL;
678  #endif
679  	if (pthread_create_fptr == NULL) {
680  		if (config_lazy_lock) {
681  			malloc_write(&quot;&lt;jemalloc&gt;: Error in dlsym(RTLD_NEXT, &quot;
682  			    &quot;\&quot;pthread_create\&quot;)\n&quot;);
683  			abort();
684  		} else {
685  			pthread_create_fptr = pthread_create;
686  		}
687  	}
688  	return false;
689  }
690  void
<span onclick='openModal()' class='match'>691  background_thread_ctl_init(tsdn_t *tsdn) {
692  	malloc_mutex_assert_not_owner(tsdn, &amp;background_thread_lock);
693  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
</span>694  	pthread_create_fptr_init();
695  	pthread_create_wrapper_init();
696  #endif
697  }
698  #endif &amp;bsol;* defined(JEMALLOC_BACKGROUND_THREAD) */
699  bool
700  background_thread_boot0(void) {
701  	if (!have_background_thread &amp;&amp; opt_background_thread) {
702  		malloc_printf(&quot;&lt;jemalloc&gt;: option background_thread currently &quot;
703  		    &quot;supports pthread only\n&quot;);
704  		return true;
705  	}
706  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
707  	if ((config_lazy_lock || opt_background_thread) &amp;&amp;
708  	    pthread_create_fptr_init()) {
709  		return true;
710  	}
711  #endif
712  	return false;
713  }
714  bool
715  background_thread_boot1(tsdn_t *tsdn) {
716  #ifdef JEMALLOC_BACKGROUND_THREAD
717  	assert(have_background_thread);
718  	assert(narenas_total_get() &gt; 0);
719  	if (opt_max_background_threads &gt; MAX_BACKGROUND_THREAD_LIMIT) {
720  		opt_max_background_threads = DEFAULT_NUM_BACKGROUND_THREAD;
721  	}
722  	max_background_threads = opt_max_background_threads;
723  	background_thread_enabled_set(tsdn, opt_background_thread);
724  	if (malloc_mutex_init(&amp;background_thread_lock,
725  	    &quot;background_thread_global&quot;,
726  	    WITNESS_RANK_BACKGROUND_THREAD_GLOBAL,
727  	    malloc_mutex_rank_exclusive)) {
728  		return true;
729  	}
730  	background_thread_info = (background_thread_info_t *)base_alloc(tsdn,
731  	    b0get(), opt_max_background_threads *
732  	    sizeof(background_thread_info_t), CACHELINE);
733  	if (background_thread_info == NULL) {
734  		return true;
735  	}
736  	for (unsigned i = 0; i &lt; max_background_threads; i++) {
737  		background_thread_info_t *info = &amp;background_thread_info[i];
738  		if (malloc_mutex_init(&amp;info-&gt;mtx, &quot;background_thread&quot;,
739  		    WITNESS_RANK_BACKGROUND_THREAD,
740  		    malloc_mutex_address_ordered)) {
741  			return true;
742  		}
743  		if (pthread_cond_init(&amp;info-&gt;cond, NULL)) {
744  			return true;
745  		}
746  		malloc_mutex_lock(tsdn, &amp;info-&gt;mtx);
747  		info-&gt;state = background_thread_stopped;
748  		background_thread_info_init(tsdn, info);
749  		malloc_mutex_unlock(tsdn, &amp;info-&gt;mtx);
750  	}
751  #endif
752  	return false;
753  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-ctl.c</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from redis-MDEwOlJlcG9zaXRvcnkxMDgxMTA3MDY=-flat-background_thread_22.c</div>
                </div>
                <div class="column column_space"><pre><code>1173  ctl_prefork(tsdn_t *tsdn) {
1174  	malloc_mutex_prefork(tsdn, &amp;ctl_mtx);
1175  }
</pre></code></div>
                <div class="column column_space"><pre><code>691  background_thread_ctl_init(tsdn_t *tsdn) {
692  	malloc_mutex_assert_not_owner(tsdn, &amp;background_thread_lock);
693  #ifdef JEMALLOC_PTHREAD_CREATE_WRAPPER
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    