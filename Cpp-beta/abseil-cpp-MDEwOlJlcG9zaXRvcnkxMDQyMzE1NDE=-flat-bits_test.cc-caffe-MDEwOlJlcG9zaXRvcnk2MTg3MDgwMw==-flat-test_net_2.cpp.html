
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 4.597701149425287%, Tokens: 9, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>abseil-cpp-MDEwOlJlcG9zaXRvcnkxMDQyMzE1NDE=-flat-bits_test.cc</h3>
            <pre><code>1  #include "absl/numeric/bits.h"
2  #include <limits>
3  #include "gmock/gmock.h"
4  #include "gtest/gtest.h"
5  #include "absl/random/random.h"
6  namespace absl {
7  ABSL_NAMESPACE_BEGIN
8  namespace {
9  TEST(Rotate, Left) {
10    static_assert(rotl(uint8_t{0x12}, 0) == uint8_t{0x12}, "");
11    static_assert(rotl(uint16_t{0x1234}, 0) == uint16_t{0x1234}, "");
12    static_assert(rotl(uint32_t{0x12345678UL}, 0) == uint32_t{0x12345678UL}, "");
13    static_assert(rotl(uint64_t{0x12345678ABCDEF01ULL}, 0) ==
14                      uint64_t{0x12345678ABCDEF01ULL},
15                  "");
16    EXPECT_EQ(rotl(uint8_t{0x12}, 0), uint8_t{0x12});
17    EXPECT_EQ(rotl(uint16_t{0x1234}, 0), uint16_t{0x1234});
18    EXPECT_EQ(rotl(uint32_t{0x12345678UL}, 0), uint32_t{0x12345678UL});
19    EXPECT_EQ(rotl(uint64_t{0x12345678ABCDEF01ULL}, 0),
20              uint64_t{0x12345678ABCDEF01ULL});
21    EXPECT_EQ(rotl(uint8_t{0x12}, 8), uint8_t{0x12});
22    EXPECT_EQ(rotl(uint16_t{0x1234}, 16), uint16_t{0x1234});
23    EXPECT_EQ(rotl(uint32_t{0x12345678UL}, 32), uint32_t{0x12345678UL});
24    EXPECT_EQ(rotl(uint64_t{0x12345678ABCDEF01ULL}, 64),
25              uint64_t{0x12345678ABCDEF01ULL});
26    EXPECT_EQ(rotl(uint8_t{0x12}, -8), uint8_t{0x12});
27    EXPECT_EQ(rotl(uint16_t{0x1234}, -16), uint16_t{0x1234});
28    EXPECT_EQ(rotl(uint32_t{0x12345678UL}, -32), uint32_t{0x12345678UL});
29    EXPECT_EQ(rotl(uint64_t{0x12345678ABCDEF01ULL}, -64),
30              uint64_t{0x12345678ABCDEF01ULL});
31    EXPECT_EQ(rotl(uint8_t{0x12}, 4), uint8_t{0x21});
32    EXPECT_EQ(rotl(uint16_t{0x1234}, 4), uint16_t{0x2341});
33    EXPECT_EQ(rotl(uint32_t{0x12345678UL}, 4), uint32_t{0x23456781UL});
34    EXPECT_EQ(rotl(uint64_t{0x12345678ABCDEF01ULL}, 4),
35              uint64_t{0x2345678ABCDEF011ULL});
36    EXPECT_EQ(rotl(uint8_t{0x12}, -4), uint8_t{0x21});
37    EXPECT_EQ(rotl(uint16_t{0x1234}, -4), uint16_t{0x4123});
38    EXPECT_EQ(rotl(uint32_t{0x12345678UL}, -4), uint32_t{0x81234567UL});
39    EXPECT_EQ(rotl(uint64_t{0x12345678ABCDEF01ULL}, -4),
40              uint64_t{0x112345678ABCDEF0ULL});
41  }
42  TEST(Rotate, Right) {
43    static_assert(rotr(uint8_t{0x12}, 0) == uint8_t{0x12}, "");
44    static_assert(rotr(uint16_t{0x1234}, 0) == uint16_t{0x1234}, "");
45    static_assert(rotr(uint32_t{0x12345678UL}, 0) == uint32_t{0x12345678UL}, "");
46    static_assert(rotr(uint64_t{0x12345678ABCDEF01ULL}, 0) ==
47                      uint64_t{0x12345678ABCDEF01ULL},
48                  "");
49    EXPECT_EQ(rotr(uint8_t{0x12}, 0), uint8_t{0x12});
50    EXPECT_EQ(rotr(uint16_t{0x1234}, 0), uint16_t{0x1234});
51    EXPECT_EQ(rotr(uint32_t{0x12345678UL}, 0), uint32_t{0x12345678UL});
52    EXPECT_EQ(rotr(uint64_t{0x12345678ABCDEF01ULL}, 0),
53              uint64_t{0x12345678ABCDEF01ULL});
54    EXPECT_EQ(rotr(uint8_t{0x12}, 8), uint8_t{0x12});
55    EXPECT_EQ(rotr(uint16_t{0x1234}, 16), uint16_t{0x1234});
56    EXPECT_EQ(rotr(uint32_t{0x12345678UL}, 32), uint32_t{0x12345678UL});
57    EXPECT_EQ(rotr(uint64_t{0x12345678ABCDEF01ULL}, 64),
58              uint64_t{0x12345678ABCDEF01ULL});
59    EXPECT_EQ(rotr(uint8_t{0x12}, -8), uint8_t{0x12});
60    EXPECT_EQ(rotr(uint16_t{0x1234}, -16), uint16_t{0x1234});
61    EXPECT_EQ(rotr(uint32_t{0x12345678UL}, -32), uint32_t{0x12345678UL});
62    EXPECT_EQ(rotr(uint64_t{0x12345678ABCDEF01ULL}, -64),
63              uint64_t{0x12345678ABCDEF01ULL});
64    EXPECT_EQ(rotr(uint8_t{0x12}, 4), uint8_t{0x21});
65    EXPECT_EQ(rotr(uint16_t{0x1234}, 4), uint16_t{0x4123});
66    EXPECT_EQ(rotr(uint32_t{0x12345678UL}, 4), uint32_t{0x81234567UL});
67    EXPECT_EQ(rotr(uint64_t{0x12345678ABCDEF01ULL}, 4),
68              uint64_t{0x112345678ABCDEF0ULL});
69    EXPECT_EQ(rotr(uint8_t{0x12}, -4), uint8_t{0x21});
70    EXPECT_EQ(rotr(uint16_t{0x1234}, -4), uint16_t{0x2341});
71    EXPECT_EQ(rotr(uint32_t{0x12345678UL}, -4), uint32_t{0x23456781UL});
72    EXPECT_EQ(rotr(uint64_t{0x12345678ABCDEF01ULL}, -4),
73              uint64_t{0x2345678ABCDEF011ULL});
74  }
75  TEST(Rotate, Symmetry) {
76    absl::BitGen rng;
77    constexpr int kTrials = 100;
78    for (int i = 0; i < kTrials; ++i) {
79      uint8_t value = absl::Uniform(rng, std::numeric_limits<uint8_t>::min(),
80                                    std::numeric_limits<uint8_t>::max());
81      int shift = absl::Uniform(rng, -2 * std::numeric_limits<uint8_t>::digits,
82                                2 * std::numeric_limits<uint8_t>::digits);
83      EXPECT_EQ(rotl(value, shift), rotr(value, -shift));
84    }
85    for (int i = 0; i < kTrials; ++i) {
86      uint16_t value = absl::Uniform(rng, std::numeric_limits<uint16_t>::min(),
87                                     std::numeric_limits<uint16_t>::max());
88      int shift = absl::Uniform(rng, -2 * std::numeric_limits<uint16_t>::digits,
89                                2 * std::numeric_limits<uint16_t>::digits);
90      EXPECT_EQ(rotl(value, shift), rotr(value, -shift));
91    }
92    for (int i = 0; i < kTrials; ++i) {
93      uint32_t value = absl::Uniform(rng, std::numeric_limits<uint32_t>::min(),
94                                     std::numeric_limits<uint32_t>::max());
95      int shift = absl::Uniform(rng, -2 * std::numeric_limits<uint32_t>::digits,
96                                2 * std::numeric_limits<uint32_t>::digits);
97      EXPECT_EQ(rotl(value, shift), rotr(value, -shift));
98    }
99    for (int i = 0; i < kTrials; ++i) {
100      uint64_t value = absl::Uniform(rng, std::numeric_limits<uint64_t>::min(),
101                                     std::numeric_limits<uint64_t>::max());
102      int shift = absl::Uniform(rng, -2 * std::numeric_limits<uint64_t>::digits,
103                                2 * std::numeric_limits<uint64_t>::digits);
104      EXPECT_EQ(rotl(value, shift), rotr(value, -shift));
105    }
106  }
107  TEST(Counting, LeadingZeroes) {
108  #if ABSL_INTERNAL_HAS_CONSTEXPR_CLZ
109    static_assert(countl_zero(uint8_t{}) == 8, "");
110    static_assert(countl_zero(static_cast<uint8_t>(-1)) == 0, "");
111    static_assert(countl_zero(uint16_t{}) == 16, "");
112    static_assert(countl_zero(static_cast<uint16_t>(-1)) == 0, "");
113    static_assert(countl_zero(uint32_t{}) == 32, "");
114    static_assert(countl_zero(~uint32_t{}) == 0, "");
115    static_assert(countl_zero(uint64_t{}) == 64, "");
116    static_assert(countl_zero(~uint64_t{}) == 0, "");
117  #endif
118    EXPECT_EQ(countl_zero(uint8_t{}), 8);
119    EXPECT_EQ(countl_zero(static_cast<uint8_t>(-1)), 0);
120    EXPECT_EQ(countl_zero(uint16_t{}), 16);
121    EXPECT_EQ(countl_zero(static_cast<uint16_t>(-1)), 0);
122    EXPECT_EQ(countl_zero(uint32_t{}), 32);
123    EXPECT_EQ(countl_zero(~uint32_t{}), 0);
124    EXPECT_EQ(countl_zero(uint64_t{}), 64);
125    EXPECT_EQ(countl_zero(~uint64_t{}), 0);
126    for (int i = 0; i < 8; i++) {
127      EXPECT_EQ(countl_zero(static_cast<uint8_t>(1u << i)), 7 - i);
128    }
129    for (int i = 0; i < 16; i++) {
130      EXPECT_EQ(countl_zero(static_cast<uint16_t>(1u << i)), 15 - i);
131    }
132    for (int i = 0; i < 32; i++) {
133      EXPECT_EQ(countl_zero(uint32_t{1} << i), 31 - i);
134    }
135    for (int i = 0; i < 64; i++) {
136      EXPECT_EQ(countl_zero(uint64_t{1} << i), 63 - i);
137    }
138  }
139  TEST(Counting, LeadingOnes) {
140  #if ABSL_INTERNAL_HAS_CONSTEXPR_CLZ
141    static_assert(countl_one(uint8_t{}) == 0, "");
142    static_assert(countl_one(static_cast<uint8_t>(-1)) == 8, "");
143    static_assert(countl_one(uint16_t{}) == 0, "");
144    static_assert(countl_one(static_cast<uint16_t>(-1)) == 16, "");
145    static_assert(countl_one(uint32_t{}) == 0, "");
146    static_assert(countl_one(~uint32_t{}) == 32, "");
147    static_assert(countl_one(uint64_t{}) == 0, "");
148    static_assert(countl_one(~uint64_t{}) == 64, "");
149  #endif
150    EXPECT_EQ(countl_one(uint8_t{}), 0);
151    EXPECT_EQ(countl_one(static_cast<uint8_t>(-1)), 8);
152    EXPECT_EQ(countl_one(uint16_t{}), 0);
153    EXPECT_EQ(countl_one(static_cast<uint16_t>(-1)), 16);
154    EXPECT_EQ(countl_one(uint32_t{}), 0);
155    EXPECT_EQ(countl_one(~uint32_t{}), 32);
156    EXPECT_EQ(countl_one(uint64_t{}), 0);
157    EXPECT_EQ(countl_one(~uint64_t{}), 64);
158  }
159  TEST(Counting, TrailingZeroes) {
160  #if ABSL_INTERNAL_HAS_CONSTEXPR_CTZ
161    static_assert(countr_zero(uint8_t{}) == 8, "");
162    static_assert(countr_zero(static_cast<uint8_t>(-1)) == 0, "");
163    static_assert(countr_zero(uint16_t{}) == 16, "");
164    static_assert(countr_zero(static_cast<uint16_t>(-1)) == 0, "");
165    static_assert(countr_zero(uint32_t{}) == 32, "");
166    static_assert(countr_zero(~uint32_t{}) == 0, "");
167    static_assert(countr_zero(uint64_t{}) == 64, "");
168    static_assert(countr_zero(~uint64_t{}) == 0, "");
169  #endif
170    EXPECT_EQ(countr_zero(uint8_t{}), 8);
171    EXPECT_EQ(countr_zero(static_cast<uint8_t>(-1)), 0);
172    EXPECT_EQ(countr_zero(uint16_t{}), 16);
173    EXPECT_EQ(countr_zero(static_cast<uint16_t>(-1)), 0);
174    EXPECT_EQ(countr_zero(uint32_t{}), 32);
175    EXPECT_EQ(countr_zero(~uint32_t{}), 0);
176    EXPECT_EQ(countr_zero(uint64_t{}), 64);
177    EXPECT_EQ(countr_zero(~uint64_t{}), 0);
178  }
179  TEST(Counting, TrailingOnes) {
180  #if ABSL_INTERNAL_HAS_CONSTEXPR_CTZ
181    static_assert(countr_one(uint8_t{}) == 0, "");
182    static_assert(countr_one(static_cast<uint8_t>(-1)) == 8, "");
183    static_assert(countr_one(uint16_t{}) == 0, "");
184    static_assert(countr_one(static_cast<uint16_t>(-1)) == 16, "");
185    static_assert(countr_one(uint32_t{}) == 0, "");
186    static_assert(countr_one(~uint32_t{}) == 32, "");
187    static_assert(countr_one(uint64_t{}) == 0, "");
188    static_assert(countr_one(~uint64_t{}) == 64, "");
189  #endif
190    EXPECT_EQ(countr_one(uint8_t{}), 0);
191    EXPECT_EQ(countr_one(static_cast<uint8_t>(-1)), 8);
192    EXPECT_EQ(countr_one(uint16_t{}), 0);
193    EXPECT_EQ(countr_one(static_cast<uint16_t>(-1)), 16);
194    EXPECT_EQ(countr_one(uint32_t{}), 0);
195    EXPECT_EQ(countr_one(~uint32_t{}), 32);
196    EXPECT_EQ(countr_one(uint64_t{}), 0);
197    EXPECT_EQ(countr_one(~uint64_t{}), 64);
198  }
199  TEST(Counting, Popcount) {
200  #if ABSL_INTERNAL_HAS_CONSTEXPR_POPCOUNT
201    static_assert(popcount(uint8_t{}) == 0, "");
202    static_assert(popcount(uint8_t{1}) == 1, "");
203    static_assert(popcount(static_cast<uint8_t>(-1)) == 8, "");
204    static_assert(popcount(uint16_t{}) == 0, "");
205    static_assert(popcount(uint16_t{1}) == 1, "");
206    static_assert(popcount(static_cast<uint16_t>(-1)) == 16, "");
207    static_assert(popcount(uint32_t{}) == 0, "");
208    static_assert(popcount(uint32_t{1}) == 1, "");
209    static_assert(popcount(~uint32_t{}) == 32, "");
210    static_assert(popcount(uint64_t{}) == 0, "");
211    static_assert(popcount(uint64_t{1}) == 1, "");
212    static_assert(popcount(~uint64_t{}) == 64, "");
213  #endif  
214    EXPECT_EQ(popcount(uint8_t{}), 0);
215    EXPECT_EQ(popcount(uint8_t{1}), 1);
216    EXPECT_EQ(popcount(static_cast<uint8_t>(-1)), 8);
217    EXPECT_EQ(popcount(uint16_t{}), 0);
218    EXPECT_EQ(popcount(uint16_t{1}), 1);
219    EXPECT_EQ(popcount(static_cast<uint16_t>(-1)), 16);
220    EXPECT_EQ(popcount(uint32_t{}), 0);
221    EXPECT_EQ(popcount(uint32_t{1}), 1);
222    EXPECT_EQ(popcount(~uint32_t{}), 32);
223    EXPECT_EQ(popcount(uint64_t{}), 0);
224    EXPECT_EQ(popcount(uint64_t{1}), 1);
225    EXPECT_EQ(popcount(~uint64_t{}), 64);
226    for (int i = 0; i < 8; i++) {
227      EXPECT_EQ(popcount(static_cast<uint8_t>(uint8_t{1} << i)), 1);
228      EXPECT_EQ(popcount(static_cast<uint8_t>(static_cast<uint8_t>(-1) ^
229                                              (uint8_t{1} << i))),
230                7);
231    }
232    for (int i = 0; i < 16; i++) {
233      EXPECT_EQ(popcount(static_cast<uint16_t>(uint16_t{1} << i)), 1);
234      EXPECT_EQ(popcount(static_cast<uint16_t>(static_cast<uint16_t>(-1) ^
235                                               (uint16_t{1} << i))),
236                15);
237    }
238    for (int i = 0; i < 32; i++) {
239      EXPECT_EQ(popcount(uint32_t{1} << i), 1);
240      EXPECT_EQ(popcount(static_cast<uint32_t>(-1) ^ (uint32_t{1} << i)), 31);
241    }
242    for (int i = 0; i < 64; i++) {
243      EXPECT_EQ(popcount(uint64_t{1} << i), 1);
244      EXPECT_EQ(popcount(static_cast<uint64_t>(-1) ^ (uint64_t{1} << i)), 63);
245    }
246  }
247  template <typename T>
248  struct PopcountInput {
249    T value = 0;
250    int expected = 0;
251  };
252  template <typename T>
253  PopcountInput<T> GeneratePopcountInput(absl::BitGen& gen) {
254    PopcountInput<T> ret;
255    for (int i = 0; i < std::numeric_limits<T>::digits; i++) {
256      bool coin = absl::Bernoulli(gen, 0.2);
257      if (coin) {
258        ret.value |= T{1} << i;
259        ret.expected++;
260      }
261    }
262    return ret;
263  }
264  TEST(Counting, PopcountFuzz) {
265    absl::BitGen rng;
266    constexpr int kTrials = 100;
267    for (int i = 0; i < kTrials; ++i) {
268      auto input = GeneratePopcountInput<uint8_t>(rng);
269      EXPECT_EQ(popcount(input.value), input.expected);
270    }
271    for (int i = 0; i < kTrials; ++i) {
272      auto input = GeneratePopcountInput<uint16_t>(rng);
273      EXPECT_EQ(popcount(input.value), input.expected);
274    }
275    for (int i = 0; i < kTrials; ++i) {
276      auto input = GeneratePopcountInput<uint32_t>(rng);
277      EXPECT_EQ(popcount(input.value), input.expected);
278    }
279    for (int i = 0; i < kTrials; ++i) {
<span onclick='openModal()' class='match'>280      auto input = GeneratePopcountInput<uint64_t>(rng);
281      EXPECT_EQ(popcount(input.value), input.expected);
282    }
283  }
284  TEST(IntegralPowersOfTwo, SingleBit) {
285    EXPECT_FALSE(has_single_bit(uint8_t{}));
</span>286    EXPECT_FALSE(has_single_bit(static_cast<uint8_t>(-1)));
287    EXPECT_FALSE(has_single_bit(uint16_t{}));
288    EXPECT_FALSE(has_single_bit(static_cast<uint16_t>(-1)));
289    EXPECT_FALSE(has_single_bit(uint32_t{}));
290    EXPECT_FALSE(has_single_bit(~uint32_t{}));
291    EXPECT_FALSE(has_single_bit(uint64_t{}));
292    EXPECT_FALSE(has_single_bit(~uint64_t{}));
293    static_assert(!has_single_bit(0u), "");
294    static_assert(has_single_bit(1u), "");
295    static_assert(has_single_bit(2u), "");
296    static_assert(!has_single_bit(3u), "");
297    static_assert(has_single_bit(4u), "");
298    static_assert(!has_single_bit(1337u), "");
299    static_assert(has_single_bit(65536u), "");
300    static_assert(has_single_bit(uint32_t{1} << 30), "");
301    static_assert(has_single_bit(uint64_t{1} << 42), "");
302    EXPECT_FALSE(has_single_bit(0u));
303    EXPECT_TRUE(has_single_bit(1u));
304    EXPECT_TRUE(has_single_bit(2u));
305    EXPECT_FALSE(has_single_bit(3u));
306    EXPECT_TRUE(has_single_bit(4u));
307    EXPECT_FALSE(has_single_bit(1337u));
308    EXPECT_TRUE(has_single_bit(65536u));
309    EXPECT_TRUE(has_single_bit(uint32_t{1} << 30));
310    EXPECT_TRUE(has_single_bit(uint64_t{1} << 42));
311    EXPECT_TRUE(has_single_bit(
312        static_cast<uint8_t>(std::numeric_limits<uint8_t>::max() / 2 + 1)));
313    EXPECT_TRUE(has_single_bit(
314        static_cast<uint16_t>(std::numeric_limits<uint16_t>::max() / 2 + 1)));
315    EXPECT_TRUE(has_single_bit(
316        static_cast<uint32_t>(std::numeric_limits<uint32_t>::max() / 2 + 1)));
317    EXPECT_TRUE(has_single_bit(
318        static_cast<uint64_t>(std::numeric_limits<uint64_t>::max() / 2 + 1)));
319  }
320  template <typename T, T arg, T = bit_ceil(arg)>
321  bool IsBitCeilConstantExpression(int) {
322    return true;
323  }
324  template <typename T, T arg>
325  bool IsBitCeilConstantExpression(char) {
326    return false;
327  }
328  TEST(IntegralPowersOfTwo, Ceiling) {
329  #if ABSL_INTERNAL_HAS_CONSTEXPR_CLZ
330    static_assert(bit_ceil(0u) == 1, "");
331    static_assert(bit_ceil(1u) == 1, "");
332    static_assert(bit_ceil(2u) == 2, "");
333    static_assert(bit_ceil(3u) == 4, "");
334    static_assert(bit_ceil(4u) == 4, "");
335    static_assert(bit_ceil(1337u) == 2048, "");
336    static_assert(bit_ceil(65536u) == 65536, "");
337    static_assert(bit_ceil(65536u - 1337u) == 65536, "");
338    static_assert(bit_ceil(uint32_t{0x80000000}) == uint32_t{0x80000000}, "");
339    static_assert(bit_ceil(uint64_t{0x40000000000}) == uint64_t{0x40000000000},
340                  "");
341    static_assert(
342        bit_ceil(uint64_t{0x8000000000000000}) == uint64_t{0x8000000000000000},
343        "");
344    EXPECT_TRUE((IsBitCeilConstantExpression<uint8_t, uint8_t{0x0}>(0)));
345    EXPECT_TRUE((IsBitCeilConstantExpression<uint8_t, uint8_t{0x80}>(0)));
346    EXPECT_FALSE((IsBitCeilConstantExpression<uint8_t, uint8_t{0x81}>(0)));
347    EXPECT_FALSE((IsBitCeilConstantExpression<uint8_t, uint8_t{0xff}>(0)));
348    EXPECT_TRUE((IsBitCeilConstantExpression<uint16_t, uint16_t{0x0}>(0)));
349    EXPECT_TRUE((IsBitCeilConstantExpression<uint16_t, uint16_t{0x8000}>(0)));
350    EXPECT_FALSE((IsBitCeilConstantExpression<uint16_t, uint16_t{0x8001}>(0)));
351    EXPECT_FALSE((IsBitCeilConstantExpression<uint16_t, uint16_t{0xffff}>(0)));
352    EXPECT_TRUE((IsBitCeilConstantExpression<uint32_t, uint32_t{0x0}>(0)));
353    EXPECT_TRUE((IsBitCeilConstantExpression<uint32_t, uint32_t{0x80000000}>(0)));
354    EXPECT_FALSE(
355        (IsBitCeilConstantExpression<uint32_t, uint32_t{0x80000001}>(0)));
356    EXPECT_FALSE(
357        (IsBitCeilConstantExpression<uint32_t, uint32_t{0xffffffff}>(0)));
358    EXPECT_TRUE((IsBitCeilConstantExpression<uint64_t, uint64_t{0x0}>(0)));
359    EXPECT_TRUE(
360        (IsBitCeilConstantExpression<uint64_t, uint64_t{0x8000000000000000}>(0)));
361    EXPECT_FALSE(
362        (IsBitCeilConstantExpression<uint64_t, uint64_t{0x8000000000000001}>(0)));
363    EXPECT_FALSE(
364        (IsBitCeilConstantExpression<uint64_t, uint64_t{0xffffffffffffffff}>(0)));
365  #endif
366    EXPECT_EQ(bit_ceil(0u), 1);
367    EXPECT_EQ(bit_ceil(1u), 1);
368    EXPECT_EQ(bit_ceil(2u), 2);
369    EXPECT_EQ(bit_ceil(3u), 4);
370    EXPECT_EQ(bit_ceil(4u), 4);
371    EXPECT_EQ(bit_ceil(1337u), 2048);
372    EXPECT_EQ(bit_ceil(65536u), 65536);
373    EXPECT_EQ(bit_ceil(65536u - 1337u), 65536);
374    EXPECT_EQ(bit_ceil(uint64_t{0x40000000000}), uint64_t{0x40000000000});
375  }
376  TEST(IntegralPowersOfTwo, Floor) {
377  #if ABSL_INTERNAL_HAS_CONSTEXPR_CLZ
378    static_assert(bit_floor(0u) == 0, "");
379    static_assert(bit_floor(1u) == 1, "");
380    static_assert(bit_floor(2u) == 2, "");
381    static_assert(bit_floor(3u) == 2, "");
382    static_assert(bit_floor(4u) == 4, "");
383    static_assert(bit_floor(1337u) == 1024, "");
384    static_assert(bit_floor(65536u) == 65536, "");
385    static_assert(bit_floor(65536u - 1337u) == 32768, "");
386    static_assert(bit_floor(uint64_t{0x40000000000}) == uint64_t{0x40000000000},
387                  "");
388  #endif
389    EXPECT_EQ(bit_floor(0u), 0);
390    EXPECT_EQ(bit_floor(1u), 1);
391    EXPECT_EQ(bit_floor(2u), 2);
392    EXPECT_EQ(bit_floor(3u), 2);
393    EXPECT_EQ(bit_floor(4u), 4);
394    EXPECT_EQ(bit_floor(1337u), 1024);
395    EXPECT_EQ(bit_floor(65536u), 65536);
396    EXPECT_EQ(bit_floor(65536u - 1337u), 32768);
397    EXPECT_EQ(bit_floor(uint64_t{0x40000000000}), uint64_t{0x40000000000});
398    for (int i = 0; i < 8; i++) {
399      uint8_t input = uint8_t{1} << i;
400      EXPECT_EQ(bit_floor(input), input);
401      if (i > 0) {
402        EXPECT_EQ(bit_floor(static_cast<uint8_t>(input + 1)), input);
403      }
404    }
405    for (int i = 0; i < 16; i++) {
406      uint16_t input = uint16_t{1} << i;
407      EXPECT_EQ(bit_floor(input), input);
408      if (i > 0) {
409        EXPECT_EQ(bit_floor(static_cast<uint16_t>(input + 1)), input);
410      }
411    }
412    for (int i = 0; i < 32; i++) {
413      uint32_t input = uint32_t{1} << i;
414      EXPECT_EQ(bit_floor(input), input);
415      if (i > 0) {
416        EXPECT_EQ(bit_floor(input + 1), input);
417      }
418    }
419    for (int i = 0; i < 64; i++) {
420      uint64_t input = uint64_t{1} << i;
421      EXPECT_EQ(bit_floor(input), input);
422      if (i > 0) {
423        EXPECT_EQ(bit_floor(input + 1), input);
424      }
425    }
426  }
427  TEST(IntegralPowersOfTwo, Width) {
428  #if ABSL_INTERNAL_HAS_CONSTEXPR_CLZ
429    static_assert(bit_width(uint8_t{}) == 0, "");
430    static_assert(bit_width(uint8_t{1}) == 1, "");
431    static_assert(bit_width(uint8_t{3}) == 2, "");
432    static_assert(bit_width(static_cast<uint8_t>(-1)) == 8, "");
433    static_assert(bit_width(uint16_t{}) == 0, "");
434    static_assert(bit_width(uint16_t{1}) == 1, "");
435    static_assert(bit_width(uint16_t{3}) == 2, "");
436    static_assert(bit_width(static_cast<uint16_t>(-1)) == 16, "");
437    static_assert(bit_width(uint32_t{}) == 0, "");
438    static_assert(bit_width(uint32_t{1}) == 1, "");
439    static_assert(bit_width(uint32_t{3}) == 2, "");
440    static_assert(bit_width(~uint32_t{}) == 32, "");
441    static_assert(bit_width(uint64_t{}) == 0, "");
442    static_assert(bit_width(uint64_t{1}) == 1, "");
443    static_assert(bit_width(uint64_t{3}) == 2, "");
444    static_assert(bit_width(~uint64_t{}) == 64, "");
445  #endif
446    EXPECT_EQ(bit_width(uint8_t{}), 0);
447    EXPECT_EQ(bit_width(uint8_t{1}), 1);
448    EXPECT_EQ(bit_width(uint8_t{3}), 2);
449    EXPECT_EQ(bit_width(static_cast<uint8_t>(-1)), 8);
450    EXPECT_EQ(bit_width(uint16_t{}), 0);
451    EXPECT_EQ(bit_width(uint16_t{1}), 1);
452    EXPECT_EQ(bit_width(uint16_t{3}), 2);
453    EXPECT_EQ(bit_width(static_cast<uint16_t>(-1)), 16);
454    EXPECT_EQ(bit_width(uint32_t{}), 0);
455    EXPECT_EQ(bit_width(uint32_t{1}), 1);
456    EXPECT_EQ(bit_width(uint32_t{3}), 2);
457    EXPECT_EQ(bit_width(~uint32_t{}), 32);
458    EXPECT_EQ(bit_width(uint64_t{}), 0);
459    EXPECT_EQ(bit_width(uint64_t{1}), 1);
460    EXPECT_EQ(bit_width(uint64_t{3}), 2);
461    EXPECT_EQ(bit_width(~uint64_t{}), 64);
462    for (int i = 0; i < 8; i++) {
463      EXPECT_EQ(bit_width(static_cast<uint8_t>(uint8_t{1} << i)), i + 1);
464    }
465    for (int i = 0; i < 16; i++) {
466      EXPECT_EQ(bit_width(static_cast<uint16_t>(uint16_t{1} << i)), i + 1);
467    }
468    for (int i = 0; i < 32; i++) {
469      EXPECT_EQ(bit_width(uint32_t{1} << i), i + 1);
470    }
471    for (int i = 0; i < 64; i++) {
472      EXPECT_EQ(bit_width(uint64_t{1} << i), i + 1);
473    }
474  }
475  #if defined(__GNUC__)
476  static_assert(ABSL_INTERNAL_HAS_CONSTEXPR_POPCOUNT,
477                "popcount should be constexpr");
478  static_assert(ABSL_INTERNAL_HAS_CONSTEXPR_CLZ, "clz should be constexpr");
479  static_assert(ABSL_INTERNAL_HAS_CONSTEXPR_CTZ, "ctz should be constexpr");
480  #endif
481  }  
482  ABSL_NAMESPACE_END
483  }  
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</h3>
            <pre><code>1  #include <string>
2  #include <utility>
3  #include <vector>
4  #include "google/protobuf/text_format.h"
5  #include "gtest/gtest.h"
6  #include "caffe/common.hpp"
7  #include "caffe/filler.hpp"
8  #include "caffe/net.hpp"
9  #include "caffe/util/io.hpp"
10  #include "caffe/util/math_functions.hpp"
11  #include "caffe/test/test_caffe_main.hpp"
12  #include "caffe/test/test_gradient_check_util.hpp"
13  namespace caffe {
14  template <typename ParentType>
15  class ParentTest : public ParentType {
16    typedef typename ParentType::Dtype Dtype;
17   protected:
18    ParentTest() : seed_(1701) {}
19    virtual void InitNetFromProtoString(const string& proto) {
20      NetParameter param;
21      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &param));
22      net_.reset(new Net<Dtype>(param));
23    }
24    virtual void InitNetFromProtoFileWithState(const string& proto,
25        Phase phase = caffe::TRAIN, const int level = 0,
26        const vector<string>* stages = NULL) {
27      NetParameter param;
28      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &param));
29      string param_file;
30      MakeTempFilename(&param_file);
31      WriteProtoToTextFile(param, param_file);
32      net_.reset(new Net<Dtype>(param_file, phase, level, stages));
33    }
34    virtual void CopyNetBlobs(const bool copy_diff,
35        vector<shared_ptr<Blob<Dtype> > >* blobs_copy) {
36      CHECK(net_);
37      const vector<shared_ptr<Blob<Dtype> > >& net_blobs = net_->blobs();
38      blobs_copy->clear();
39      blobs_copy->resize(net_blobs.size());
40      const bool kReshape = true;
41      for (int i = 0; i < net_blobs.size(); ++i) {
42        (*blobs_copy)[i].reset(new Blob<Dtype>());
43        (*blobs_copy)[i]->CopyFrom(*net_blobs[i], copy_diff, kReshape);
44      }
45    }
46    virtual void CopyNetParams(const bool copy_diff,
47        vector<shared_ptr<Blob<Dtype> > >* params_copy) {
48      CHECK(net_);
49      const vector<shared_ptr<Blob<Dtype> > >& net_params = net_->params();
50      params_copy->clear();
51      params_copy->resize(net_params.size());
52      const bool kReshape = true;
53      for (int i = 0; i < net_params.size(); ++i) {
54        (*params_copy)[i].reset(new Blob<Dtype>());
55        (*params_copy)[i]->CopyFrom(*net_params[i], copy_diff, kReshape);
56      }
57    }
58    virtual void InitTinyNet(const bool force_backward = false,
59                             const bool accuracy_layer = false) {
60      string proto =
61          "name: 'TinyTestNetwork' "
62          "layer { "
63          "  name: 'data' "
64          "  type: 'DummyData' "
65          "  dummy_data_param { "
66          "    shape { "
67          "      dim: 5 "
68          "      dim: 2 "
69          "      dim: 3 "
70          "      dim: 4 "
71          "    } "
72          "    data_filler { "
73          "      type: 'gaussian' "
74          "      std: 0.01 "
75          "    } "
76          "    shape { "
77          "      dim: 5 "
78          "    } "
79          "    data_filler { "
80          "      type: 'constant' "
81          "      value: 0 "
82          "    } "
83          "  } "
84          "  top: 'data' "
85          "  top: 'label' "
86          "} "
87          "layer { "
88          "  name: 'innerproduct' "
89          "  type: 'InnerProduct' "
90          "  inner_product_param { "
91          "    num_output: 1000 "
92          "    weight_filler { "
93          "      type: 'gaussian' "
94          "      std: 0.01 "
95          "    } "
96          "    bias_filler { "
97          "      type: 'constant' "
98          "      value: 0 "
99          "    } "
100          "  } "
101          "  param { "
102          "    lr_mult: 1 "
103          "    decay_mult: 1 "
104          "  } "
105          "  param { "
106          "    lr_mult: 2 "
107          "    decay_mult: 0 "
108          "  } "
109          "  bottom: 'data' "
110          "  top: 'innerproduct' "
111          "} "
112          "layer { "
113          "  name: 'loss' "
114          "  type: 'SoftmaxWithLoss' "
115          "  bottom: 'innerproduct' "
116          "  bottom: 'label' "
117          "  top: 'top_loss' "
118          "} ";
119      if (accuracy_layer) {
120        proto +=
121            "layer { "
122            "  name: 'loss' "
123            "  type: 'Accuracy' "
124            "  bottom: 'innerproduct' "
125            "  bottom: 'label' "
126            "  top: 'accuracy' "
127            "} ";
128      }
129      if (force_backward) {
130        proto += "force_backward: true ";
131      }
132      InitNetFromProtoString(proto);
133    }
134    virtual void InitTinyNetEuclidean(const bool force_backward = false) {
135      string proto =
136          "name: 'TinyTestEuclidLossNetwork' "
137          "layer { "
138          "  name: 'data' "
139          "  type: 'DummyData' "
140          "  dummy_data_param { "
141          "    num: 5 "
142          "    channels: 2 "
143          "    height: 3 "
144          "    width: 4 "
145          "    num: 5 "
146          "    channels: 1 "
147          "    height: 1 "
148          "    width: 1 "
149          "    data_filler { "
150          "      type: 'gaussian' "
151          "      std: 0.01 "
152          "    } "
153          "  } "
154          "  top: 'data' "
155          "  top: 'label' "
156          "} "
157          "layer { "
158          "  name: 'innerproduct' "
159          "  type: 'InnerProduct' "
160          "  inner_product_param { "
161          "    num_output: 1 "
162          "    weight_filler { "
163          "      type: 'gaussian' "
164          "      std: 0.01 "
165          "    } "
166          "    bias_filler { "
167          "      type: 'constant' "
168          "      value: 0 "
169          "    } "
170          "  } "
171          "  param { "
172          "    lr_mult: 1 "
173          "    decay_mult: 1 "
174          "  } "
175          "  param { "
176          "    lr_mult: 2 "
177          "    decay_mult: 0 "
178          "  } "
179          "  bottom: 'data' "
180          "  top: 'innerproduct' "
181          "} "
182          "layer { "
183          "  name: 'loss' "
184          "  type: 'EuclideanLoss' "
185          "  bottom: 'innerproduct' "
186          "  bottom: 'label' "
187          "} ";
188      if (force_backward) {
189        proto += "force_backward: true ";
190      }
191      InitNetFromProtoString(proto);
192    }
193    virtual void InitTrickyNet(Dtype* loss_weight = NULL) {
194      ostringstream loss_weight_stream;
195      if (loss_weight) {
196        loss_weight_stream << "  loss_weight: " << *loss_weight << " ";
197      }
198      const string& proto =
199          "name: 'TrickyTestNetwork' "
200          "layer { "
201          "  name: 'data' "
202          "  type: 'DummyData' "
203          "  dummy_data_param { "
204          "    num: 5 "
205          "    channels: 2 "
206          "    height: 3 "
207          "    width: 4 "
208          "    num: 5 "
209          "    channels: 1 "
210          "    height: 1 "
211          "    width: 1 "
212          "    data_filler { "
213          "      type: 'gaussian' "
214          "      std: 0.01 "
215          "    } "
216          "  } "
217          "  top: 'data' "
218          "  top: 'label' "
219          "} "
220          "layer { "
221          "  name: 'innerproduct' "
222          "  type: 'InnerProduct' "
223          "  inner_product_param { "
224          "    num_output: 1000 "
225          "    weight_filler { "
226          "      type: 'gaussian' "
227          "      std: 0.01 "
228          "    } "
229          "    bias_filler { "
230          "      type: 'constant' "
231          "      value: 0 "
232          "    } "
233          "  } "
234          "  param { "
235          "    lr_mult: 1 "
236          "    decay_mult: 1 "
237          "  } "
238          "  param { "
239          "    lr_mult: 2 "
240          "    decay_mult: 0 "
241          "  } "
242          "  bottom: 'data' "
243          "  top: 'transformed_data' "
244          "} "
245          "layer { "
246          "  name: 'innerproduct' "
247          "  type: 'InnerProduct' "
248          "  inner_product_param { "
249          "    num_output: 1 "
250          "    weight_filler { "
251          "      type: 'gaussian' "
252          "      std: 0.01 "
253          "    } "
254          "    bias_filler { "
255          "      type: 'constant' "
256          "      value: 0 "
257          "    } "
258          "  } "
259          "  param { "
260          "    lr_mult: 1 "
261          "    decay_mult: 1 "
262          "  } "
263          "  param { "
264          "    lr_mult: 2 "
265          "    decay_mult: 0 "
266          "  } "
267          "  bottom: 'label' "
268          "  top: 'transformed_label' "
269          "} "
270          "layer { "
271          "  name: 'loss' "
272          "  type: 'SoftmaxWithLoss' " +
273          loss_weight_stream.str() +
274          "  bottom: 'transformed_data' "
275          "  bottom: 'transformed_label' "
276          "} ";
277      InitNetFromProtoString(proto);
278    }
279    virtual void InitUnsharedWeightsNet(const Dtype* loss_weight = NULL,
280        const Dtype* midnet_loss_weight = NULL,
281        const bool force_backward = false, const bool bias_term = false,
282        const Dtype blobs_lr_w1 = 1, const Dtype blobs_lr_b1 = 2,
283        const Dtype blobs_lr_w2 = 1, const Dtype blobs_lr_b2 = 2) {
284      string bias_str = bias_term ? "true ":"false ";
285      ostringstream proto;
286      proto << "name: 'UnsharedWeightsNetwork' ";
287      if (force_backward) {
288        proto << "force_backward: true ";
289      }
290      proto <<
291          "layer { "
292          "  name: 'data' "
293          "  type: 'DummyData' "
294          "  dummy_data_param { "
295          "    num: 5 "
296          "    channels: 2 "
297          "    height: 3 "
298          "    width: 4 "
299          "    data_filler { "
300          "      type: 'gaussian' "
301          "      std: 0.01 "
302          "    } "
303          "  } "
304          "  top: 'data' "
305          "} "
306          "layer { "
307          "  name: 'innerproduct1' "
308          "  type: 'InnerProduct' "
309          "  inner_product_param { "
310          "    num_output: 10 "
311          "    bias_term: " << bias_str <<
312          "    weight_filler { "
313          "      type: 'gaussian' "
314          "      std: 10 "
315          "    } "
316          "  } "
317          "  param { "
318          "    name: 'unsharedweights1' "
319          "    lr_mult: " << blobs_lr_w1 <<
320          "  } ";
321      if (bias_term) {
322        proto << "  param { lr_mult: " << blobs_lr_b1 << " } ";
323      }
324      proto <<
325          "  bottom: 'data' "
326          "  top: 'innerproduct1' ";
327      if (midnet_loss_weight) {
328        proto << "  loss_weight: " << *midnet_loss_weight << " ";
329      }
330      proto <<
331          "} "
332          "layer { "
333          "  name: 'innerproduct2' "
334          "  type: 'InnerProduct' "
335          "  inner_product_param { "
336          "    num_output: 10 "
337          "    bias_term: " << bias_str <<
338          "    weight_filler { "
339          "      type: 'gaussian' "
340          "      std: 10 "
341          "    } "
342          "  } "
343          "  param { "
344          "    name: 'unsharedweights2' "
345          "    lr_mult: " << blobs_lr_w2 <<
346          "  } ";
347      if (bias_term) {
348        proto << "  param { lr_mult: " << blobs_lr_b2 << " } ";
349      }
350      proto <<
351          "  bottom: 'data' "
352          "  top: 'innerproduct2' "
353          "} "
354          "layer { "
355          "  name: 'loss' "
356          "  type: 'EuclideanLoss' ";
357      if (loss_weight) {
358        proto << "  loss_weight: " << *loss_weight << " ";
359      }
360      proto <<
361          "  bottom: 'innerproduct1' "
362          "  bottom: 'innerproduct2' "
363          "} ";
364      InitNetFromProtoString(proto.str());
365    }
366    virtual void InitSharedWeightsNet() {
367      const string& proto =
368          "name: 'SharedWeightsNetwork' "
369          "layer { "
370          "  name: 'data' "
371          "  type: 'DummyData' "
372          "  dummy_data_param { "
373          "    num: 5 "
374          "    channels: 2 "
375          "    height: 3 "
376          "    width: 4 "
377          "    data_filler { "
378          "      type: 'gaussian' "
379          "      std: 0.01 "
380          "    } "
381          "  } "
382          "  top: 'data' "
383          "} "
384          "layer { "
385          "  name: 'innerproduct1' "
386          "  type: 'InnerProduct' "
387          "  inner_product_param { "
388          "    num_output: 10 "
389          "    bias_term: false "
390          "    weight_filler { "
391          "      type: 'gaussian' "
392          "      std: 10 "
393          "    } "
394          "  } "
395          "  param { name: 'sharedweights' } "
396          "  bottom: 'data' "
397          "  top: 'innerproduct1' "
398          "} "
399          "layer { "
400          "  name: 'innerproduct2' "
401          "  type: 'InnerProduct' "
402          "  inner_product_param { "
403          "    num_output: 10 "
404          "    bias_term: false "
405          "    weight_filler { "
406          "      type: 'gaussian' "
407          "      std: 10 "
408          "    } "
409          "  } "
410          "  param { name: 'sharedweights' } "
411          "  bottom: 'data' "
412          "  top: 'innerproduct2' "
413          "} "
414          "layer { "
415          "  name: 'loss' "
416          "  type: 'EuclideanLoss' "
417          "  bottom: 'innerproduct1' "
418          "  bottom: 'innerproduct2' "
419          "} ";
420      InitNetFromProtoString(proto);
421    }
422    virtual void InitDiffDataUnsharedWeightsNet() {
423      const string& proto =
424          "name: 'DiffDataUnsharedWeightsNetwork' "
425          "layer { "
426          "  name: 'data' "
427          "  type: 'DummyData' "
428          "  dummy_data_param { "
429          "    num: 10 "
430          "    channels: 10 "
431          "    height: 1 "
432          "    width: 1 "
433          "    num: 10 "
434          "    channels: 10 "
435          "    height: 1 "
436          "    width: 1 "
437          "    data_filler { "
438          "      type: 'gaussian' "
439          "      std: 10 "
440          "    } "
441          "  } "
442          "  top: 'data1' "
443          "  top: 'data2' "
444          "} "
445          "layer { "
446          "  name: 'innerproduct1' "
447          "  type: 'InnerProduct' "
448          "  inner_product_param { "
449          "    num_output: 10 "
450          "    bias_term: false "
451          "    weight_filler { "
452          "      type: 'constant' "
453          "      value: 0.5 "
454          "    } "
455          "  } "
456          "  param { name: 'unsharedweights1' } "
457          "  bottom: 'data1' "
458          "  top: 'innerproduct1' "
459          "} "
460          "layer { "
461          "  name: 'innerproduct2' "
462          "  type: 'InnerProduct' "
463          "  inner_product_param { "
464          "    num_output: 10 "
465          "    bias_term: false "
466          "    weight_filler { "
467          "      type: 'constant' "
468          "      value: 0.5 "
469          "    } "
470          "  } "
471          "  param { name: 'unsharedweights2' } "
472          "  bottom: 'innerproduct1' "
473          "  top: 'innerproduct2' "
474          "} "
475          "layer { "
476          "  name: 'loss' "
477          "  type: 'EuclideanLoss' "
478          "  bottom: 'data2' "
479          "  bottom: 'innerproduct2' "
480          "} ";
481      InitNetFromProtoString(proto);
482    }
483    virtual void InitDiffDataSharedWeightsNet() {
484      const string& proto =
485          "name: 'DiffDataSharedWeightsNetwork' "
486          "layer { "
487          "  name: 'data' "
488          "  type: 'DummyData' "
489          "  dummy_data_param { "
490          "    num: 10 "
491          "    channels: 10 "
492          "    height: 1 "
493          "    width: 1 "
494          "    num: 10 "
495          "    channels: 10 "
496          "    height: 1 "
497          "    width: 1 "
498          "    data_filler { "
499          "      type: 'gaussian' "
500          "      std: 10 "
501          "    } "
502          "  } "
503          "  top: 'data1' "
504          "  top: 'data2' "
505          "} "
506          "layer { "
507          "  name: 'innerproduct1' "
508          "  type: 'InnerProduct' "
509          "  inner_product_param { "
510          "    num_output: 10 "
511          "    bias_term: false "
512          "    weight_filler { "
513          "      type: 'constant' "
514          "      value: 0.5 "
515          "    } "
516          "  } "
517          "  param { name: 'sharedweights' } "
518          "  bottom: 'data1' "
519          "  top: 'innerproduct1' "
520          "} "
521          "layer { "
522          "  name: 'innerproduct2' "
523          "  type: 'InnerProduct' "
524          "  inner_product_param { "
525          "    num_output: 10 "
526          "    bias_term: false "
527          "    weight_filler { "
528          "      type: 'constant' "
529          "      value: 0.5 "
530          "    } "
531          "  } "
532          "  param { name: 'sharedweights' } "
533          "  bottom: 'innerproduct1' "
534          "  top: 'innerproduct2' "
535          "} "
536          "layer { "
537          "  name: 'loss' "
538          "  type: 'EuclideanLoss' "
539          "  bottom: 'data2' "
540          "  bottom: 'innerproduct2' "
541          "} ";
542      InitNetFromProtoString(proto);
543    }
544    virtual void InitReshapableNet() {
545      const string& proto =
546          "name: 'ReshapableNetwork' "
547          "layer { "
548          "  name: 'data' "
549          "  type: 'Input' "
550          "  top: 'data' "
551          "  input_param { "
552          "  shape: { dim: 1 dim: 3 dim: 100 dim: 100 } "
553          "  } "
554          "} "
555          "layer { "
556          "  name: 'conv1' "
557          "  type: 'Convolution' "
558          "  bottom: 'data' "
559          "  top: 'conv1' "
560          "  convolution_param { "
561          "    num_output: 5 "
562          "    kernel_size: 3 "
563          "    stride: 2 "
564          "    weight_filler { "
565          "      type: 'gaussian' "
566          "      std: 0.01 "
567          "    } "
568          "    bias_filler { "
569          "      type: 'constant' "
570          "      value: 0.2 "
571          "    } "
572          "  } "
573          "} "
574          "layer { "
575          "  name: 'relu1' "
576          "  type: 'ReLU' "
577          "  bottom: 'conv1' "
578          "  top: 'conv1' "
579          "} "
580          "layer { "
581          "  name: 'pool1' "
582          "  type: 'Pooling' "
583          "  bottom: 'conv1' "
584          "  top: 'pool1' "
585          "  pooling_param { "
586          "    pool: MAX "
587          "    kernel_size: 2 "
588          "    stride: 2 "
589          "  } "
590          "} "
591          "layer { "
592          "  name: 'norm1' "
593          "  type: 'LRN' "
594          "  bottom: 'pool1' "
595          "  top: 'norm1' "
596          "  lrn_param { "
597          "    local_size: 3 "
598          "  } "
599          "} "
600          "layer { "
601          "  name: 'softmax' "
602          "  type: 'Softmax' "
603          "  bottom: 'norm1' "
604          "  top: 'softmax' "
605          "} ";
606      InitNetFromProtoString(proto);
607    }
608    virtual void InitSkipPropNet(bool test_skip_true) {
609      string proto =
610        "name: 'SkipPropTestNetwork' "
611        "layer { "
612        "  name: 'data' "
613        "  type: 'DummyData' "
614        "  dummy_data_param { "
615        "    shape { "
616        "      dim: 5 "
617        "      dim: 2 "
618        "      dim: 3 "
619        "      dim: 4 "
620        "    } "
621        "    data_filler { "
622        "      type: 'gaussian' "
623        "      std: 0.01 "
624        "    } "
625        "    shape { "
626        "      dim: 5 "
627        "    } "
628        "    data_filler { "
629        "      type: 'constant' "
630        "      value: 0 "
631        "    } "
632        "  } "
633        "  top: 'data' "
634        "  top: 'label' "
635        "} "
636        "layer { "
637        "  name: 'silence' "
638        "  bottom: 'label' "
639        "  type: 'Silence' "
640        "} "
641        "layer { "
642        "  name: 'innerproduct' "
643        "  type: 'InnerProduct' "
644        "  inner_product_param { "
645        "    num_output: 1 "
646        "    weight_filler { "
647        "      type: 'gaussian' "
648        "      std: 0.01 "
649        "    } "
650        "    bias_filler { "
651        "      type: 'constant' "
652        "      value: 0 "
653        "    } "
654        "  } "
655        "  param { "
656        "    lr_mult: 1 "
657        "    decay_mult: 1 "
658        "  } "
659        "  param { "
660        "    lr_mult: 2 "
661        "    decay_mult: 0 "
662        "  } "
663        "  bottom: 'data' "
664        "  top: 'innerproduct' "
665        "} "
666        "layer { "
667        "  name: 'ip_fake_labels' "
668        "  type: 'InnerProduct' "
669        "  inner_product_param { "
670        "    num_output: 1 "
671        "    weight_filler { "
672        "      type: 'gaussian' "
673        "      std: 0.01 "
674        "    } "
675        "    bias_filler { "
676        "      type: 'constant' "
677        "      value: 0 "
678        "    } "
679        "  } "
680        "  bottom: 'data' "
681        "  top: 'fake_labels' "
682        "} "
683        "layer { "
684        "  name: 'argmax' "
685        "  bottom: 'fake_labels' "
686        "  top: 'label_argmax' "
687        "  type: 'ArgMax' "
688        "} "
689        "layer { "
690        "  name: 'loss' "
691        "  bottom: 'innerproduct' "
692        "  bottom: 'label_argmax' ";
693      if (test_skip_true)
694        proto += "  propagate_down: true "
695                 "  propagate_down: false ";
696      else
697        proto += "  propagate_down: true "
698                 "  propagate_down: true ";
699      proto +=
700        "  top: 'cross_entropy_loss' "
701        "  type: 'SigmoidCrossEntropyLoss' "
702        "  loss_weight: 0.1 "
703        "} ";
704      InitNetFromProtoString(proto);
705    }
706    virtual void InitForcePropNet(bool test_force_true) {
707      string proto =
708        "name: 'ForcePropTestNetwork' "
709        "layer { "
710        "  name: 'data' "
711        "  type: 'DummyData' "
712        "  dummy_data_param { "
713        "    shape { "
714        "      dim: 5 "
715        "      dim: 2 "
716        "      dim: 3 "
717        "      dim: 4 "
718        "    } "
719        "    data_filler { "
720        "      type: 'gaussian' "
721        "      std: 0.01 "
722        "    } "
723        "    shape { "
724        "      dim: 5 "
725        "    } "
726        "    data_filler { "
727        "      type: 'constant' "
728        "      value: 0 "
729        "    } "
730        "  } "
731        "  top: 'data' "
732        "  top: 'label' "
733        "} "
734        "layer { "
735        "  name: 'innerproduct' "
736        "  type: 'InnerProduct' "
737        "  inner_product_param { "
738        "    num_output: 1 "
739        "    weight_filler { "
740        "      type: 'gaussian' "
741        "      std: 0.01 "
742        "    } "
743        "  } "
744        "  bottom: 'data' "
745        "  top: 'innerproduct' ";
746      if (test_force_true) {
747        proto += "  propagate_down: true ";
748      }
749      proto +=
750        "} "
751        "layer { "
752        "  name: 'loss' "
753        "  bottom: 'innerproduct' "
754        "  bottom: 'label' "
755        "  top: 'cross_entropy_loss' "
756        "  type: 'SigmoidCrossEntropyLoss' "
757        "} ";
758      InitNetFromProtoString(proto);
759    }
760    virtual void InitAllInOneNet(Phase phase = caffe::TRAIN,
761        const int level = 0, const vector<string>* stages = NULL) {
762      string proto =
763        "name: 'All-in-one Network'"
764        "layer { "
765        "  name: 'train-data' "
766        "  type: 'DummyData' "
767        "  top: 'data' "
768        "  top: 'label' "
769        "  dummy_data_param { "
770        "    shape { dim: 1 dim: 10 } "
771        "    shape { dim: 1 dim: 1 } "
772        "  } "
773        "  include { phase: TRAIN stage: 'train' } "
774        "} "
775        "layer { "
776        "  name: 'val-data' "
777        "  type: 'DummyData' "
778        "  top: 'data' "
779        "  top: 'label' "
780        "  dummy_data_param { "
781        "    shape { dim: 1 dim: 10 } "
782        "    shape { dim: 1 dim: 1 } "
783        "  } "
784        "  include { phase: TEST stage: 'val' } "
785        "} "
786        "layer { "
787        "  name: 'deploy-data' "
788        "  type: 'Input' "
789        "  top: 'data' "
790        "  input_param { "
791        "    shape { dim: 1 dim: 10 } "
792        "  } "
793        "  include { phase: TEST stage: 'deploy' } "
794        "} "
795        "layer { "
796        "  name: 'ip' "
797        "  type: 'InnerProduct' "
798        "  bottom: 'data' "
799        "  top: 'ip' "
800        "  inner_product_param { "
801        "    num_output: 2 "
802        "  } "
803        "} "
804        "layer { "
805        "  name: 'loss' "
806        "  type: 'SoftmaxWithLoss' "
807        "  bottom: 'ip' "
808        "  bottom: 'label' "
809        "  top: 'loss' "
810        "  include { phase: TRAIN stage: 'train' } "
811        "  include { phase: TEST stage: 'val' } "
812        "} ";
813      InitNetFromProtoFileWithState(proto, phase, level, stages);
814    }
815    int seed_;
816    shared_ptr<Net<Dtype> > net_;
817  };
818  template <typename TypeParam>
819  class NetTest : public ParentTest<MultiDeviceTest<TypeParam>> {
820  };
821  template <typename TypeParam>
822  class NetTestCPU : public ParentTest<CPUDeviceTest<TypeParam>> {
823  };
824  #ifdef USE_MKLDNN_AS_DEFAULT_ENGINE
825  TYPED_TEST_CASE(NetTest, MKLDNNTestDtypesAndDevices);
826  #else
827  TYPED_TEST_CASE(NetTest, TestDtypesAndDevices);
828  #endif
829  TYPED_TEST_CASE(NetTestCPU, TestDtypes);
830  TYPED_TEST(NetTest, TestHasBlob) {
831    this->InitTinyNet();
832    EXPECT_TRUE(this->net_->has_blob("data"));
833    EXPECT_TRUE(this->net_->has_blob("label"));
834    EXPECT_TRUE(this->net_->has_blob("innerproduct"));
835    EXPECT_FALSE(this->net_->has_blob("loss"));
836    EXPECT_TRUE(this->net_->has_blob("top_loss"));
837  }
838  TYPED_TEST(NetTest, TestGetBlob) {
839    this->InitTinyNet();
840    EXPECT_EQ(this->net_->blob_by_name("data"), this->net_->blobs()[0]);
841    EXPECT_EQ(this->net_->blob_by_name("label"), this->net_->blobs()[1]);
842    EXPECT_EQ(this->net_->blob_by_name("innerproduct"), this->net_->blobs()[2]);
843    EXPECT_FALSE(this->net_->blob_by_name("loss"));
844    EXPECT_EQ(this->net_->blob_by_name("top_loss"), this->net_->blobs()[3]);
845  }
846  TYPED_TEST(NetTest, TestHasLayer) {
847    this->InitTinyNet();
848    EXPECT_TRUE(this->net_->has_layer("data"));
849    EXPECT_TRUE(this->net_->has_layer("innerproduct"));
850    EXPECT_TRUE(this->net_->has_layer("loss"));
851    EXPECT_FALSE(this->net_->has_layer("label"));
852  }
853  TYPED_TEST(NetTest, TestGetLayerByName) {
854    this->InitTinyNet();
855    EXPECT_EQ(this->net_->layer_by_name("data"), this->net_->layers()[0]);
856    EXPECT_EQ(this->net_->layer_by_name("innerproduct"), this->net_->layers()[1]);
857    EXPECT_EQ(this->net_->layer_by_name("loss"), this->net_->layers()[2]);
858    EXPECT_FALSE(this->net_->layer_by_name("label"));
859  }
860  TYPED_TEST(NetTest, TestBottomNeedBackward) {
861    this->InitTinyNet();
862    const vector<vector<bool> >& bottom_need_backward =
863        this->net_->bottom_need_backward();
864    EXPECT_EQ(3, bottom_need_backward.size());
865    EXPECT_EQ(0, bottom_need_backward[0].size());
866    EXPECT_EQ(1, bottom_need_backward[1].size());
867    EXPECT_EQ(false, bottom_need_backward[1][0]);
868    EXPECT_EQ(2, bottom_need_backward[2].size());
869    EXPECT_EQ(true, bottom_need_backward[2][0]);
870    EXPECT_EQ(false, bottom_need_backward[2][1]);
871  }
872  TYPED_TEST(NetTest, TestBottomNeedBackwardForce) {
873    const bool force_backward = true;
874    this->InitTinyNet(force_backward);
875    const vector<vector<bool> >& bottom_need_backward =
876        this->net_->bottom_need_backward();
877    EXPECT_EQ(3, bottom_need_backward.size());
878    EXPECT_EQ(0, bottom_need_backward[0].size());
879    EXPECT_EQ(1, bottom_need_backward[1].size());
880    EXPECT_EQ(true, bottom_need_backward[1][0]);
881    EXPECT_EQ(2, bottom_need_backward[2].size());
882    EXPECT_EQ(true, bottom_need_backward[2][0]);
883    EXPECT_EQ(false, bottom_need_backward[2][1]);
884  }
885  TYPED_TEST(NetTest, TestBottomNeedBackwardEuclideanForce) {
886    const bool force_backward = true;
887    this->InitTinyNetEuclidean(force_backward);
888    const vector<vector<bool> >& bottom_need_backward =
889        this->net_->bottom_need_backward();
890    EXPECT_EQ(3, bottom_need_backward.size());
891    EXPECT_EQ(0, bottom_need_backward[0].size());
892    EXPECT_EQ(1, bottom_need_backward[1].size());
893    EXPECT_EQ(true, bottom_need_backward[1][0]);
894    EXPECT_EQ(2, bottom_need_backward[2].size());
895    EXPECT_EQ(true, bottom_need_backward[2][0]);
896    EXPECT_EQ(true, bottom_need_backward[2][1]);
897  }
898  TYPED_TEST(NetTest, TestBottomNeedBackwardTricky) {
899    this->InitTrickyNet();
900    const vector<vector<bool> >& bottom_need_backward =
901        this->net_->bottom_need_backward();
902    EXPECT_EQ(4, bottom_need_backward.size());
903    EXPECT_EQ(0, bottom_need_backward[0].size());
904    EXPECT_EQ(1, bottom_need_backward[1].size());
905    EXPECT_EQ(false, bottom_need_backward[1][0]);
906    EXPECT_EQ(1, bottom_need_backward[2].size());
907    EXPECT_EQ(false, bottom_need_backward[2][0]);
908    EXPECT_EQ(2, bottom_need_backward[3].size());
909    EXPECT_EQ(true, bottom_need_backward[3][0]);
910    EXPECT_EQ(true, bottom_need_backward[3][1]);
911  }
912  TYPED_TEST(NetTest, TestLossWeight) {
913    typedef typename TypeParam::Dtype Dtype;
914    vector<Blob<Dtype>*> bottom;
915    Caffe::set_random_seed(this->seed_);
916    const bool kForceBackward = true;
917    this->InitUnsharedWeightsNet(NULL, NULL, kForceBackward);
918    const Dtype loss = this->net_->ForwardBackward();
919    const bool kCopyDiff = true;
920    vector<shared_ptr<Blob<Dtype> > > blob_grads;
921    this->CopyNetBlobs(kCopyDiff, &blob_grads);
922    vector<shared_ptr<Blob<Dtype> > > param_grads;
923    this->CopyNetParams(kCopyDiff, &param_grads);
924    const Dtype kMinLossAbsValue = 1e-2;
925    ASSERT_GE(fabs(loss), kMinLossAbsValue);
926    const Dtype kErrorMargin = 1e-4;
927    const int kNumLossWeights = 6;
928    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
929    for (int i = 0; i < kNumLossWeights; ++i) {
930      Caffe::set_random_seed(this->seed_);
931      this->InitUnsharedWeightsNet(&kLossWeights[i], NULL, kForceBackward);
932      const Dtype weighted_loss = this->net_->ForwardBackward();
933      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
934      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
935          << "loss weight = " << kLossWeights[i];
936      const vector<shared_ptr<Blob<Dtype> > >& weighted_blobs =
937          this->net_->blobs();
938      ASSERT_EQ(blob_grads.size(), weighted_blobs.size());
939      for (int j = 0; j < blob_grads.size(); ++j) {
940        ASSERT_EQ(blob_grads[j]->count(), weighted_blobs[j]->count());
941        for (int k = 0; k < blob_grads[j]->count(); ++k) {
942          EXPECT_NEAR(blob_grads[j]->cpu_diff()[k] * kLossWeights[i],
943                      weighted_blobs[j]->cpu_diff()[k], error_margin);
944        }
945      }
946      const vector<shared_ptr<Blob<Dtype> > >& weighted_params =
947          this->net_->params();
948      ASSERT_EQ(param_grads.size(), weighted_params.size());
949      for (int j = 0; j < param_grads.size(); ++j) {
950        ASSERT_EQ(param_grads[j]->count(), weighted_params[j]->count());
951        for (int k = 0; k < param_grads[j]->count(); ++k) {
952          EXPECT_NEAR(param_grads[j]->cpu_diff()[k] * kLossWeights[i],
953                      weighted_params[j]->cpu_diff()[k], error_margin);
954        }
955      }
956    }
957  }
958  TYPED_TEST(NetTest, TestLossWeightMidNet) {
959    typedef typename TypeParam::Dtype Dtype;
960    Caffe::set_random_seed(this->seed_);
961    const bool kForceBackward = true;
962    Dtype loss_weight = 0;
963    Dtype midnet_loss_weight = 1;
964    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
965                                 kForceBackward);
966    const Dtype loss = this->net_->ForwardBackward();
967    const bool kCopyDiff = true;
968    const bool kReshape = true;
969    Blob<Dtype> data_grad;
970    data_grad.CopyFrom(*this->net_->blob_by_name("data"), kCopyDiff, kReshape);
971    const Dtype kMinLossAbsValue = 1e-2;
972    ASSERT_GE(fabs(loss), kMinLossAbsValue);
973    const Dtype kErrorMargin = 1e-4;
974    const int kNumLossWeights = 6;
975    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
976    for (int i = 0; i < kNumLossWeights; ++i) {
977      Caffe::set_random_seed(this->seed_);
978      this->InitUnsharedWeightsNet(&loss_weight, &kLossWeights[i],
979                                   kForceBackward);
980      const Dtype weighted_loss = this->net_->ForwardBackward();
981      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
982      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
983          << "loss weight = " << kLossWeights[i];
984      const shared_ptr<Blob<Dtype> >& weighted_blob =
985          this->net_->blob_by_name("data");
986      ASSERT_EQ(data_grad.count(), weighted_blob->count());
987      for (int j = 0; j < data_grad.count(); ++j) {
988        EXPECT_NEAR(data_grad.cpu_diff()[j] * kLossWeights[i],
989                    weighted_blob->cpu_diff()[j], error_margin);
990      }
991    }
992  }
993  TYPED_TEST(NetTest, TestComboLossWeight) {
994    typedef typename TypeParam::Dtype Dtype;
995    Dtype loss_weight;
996    Dtype midnet_loss_weight;
997    const bool kForceBackward = true;
998    const Dtype kErrorMargin = 1e-4;
999    loss_weight = 1;
1000    midnet_loss_weight = 1;
1001    Caffe::set_random_seed(this->seed_);
1002    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1003                                 kForceBackward);
1004    const Dtype loss = this->net_->ForwardBackward();
1005    const bool kCopyDiff = true;
1006    vector<shared_ptr<Blob<Dtype> > > blob_grads;
1007    this->CopyNetBlobs(kCopyDiff, &blob_grads);
1008    vector<shared_ptr<Blob<Dtype> > > param_grads;
1009    this->CopyNetParams(kCopyDiff, &param_grads);
1010    loss_weight = 2;
1011    midnet_loss_weight = 1;
1012    Caffe::set_random_seed(this->seed_);
1013    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1014                                 kForceBackward);
1015    const Dtype loss_main_2 = this->net_->ForwardBackward();
1016    vector<shared_ptr<Blob<Dtype> > > blob_grads_loss_2;
1017    this->CopyNetBlobs(kCopyDiff, &blob_grads_loss_2);
1018    vector<shared_ptr<Blob<Dtype> > > param_grads_loss_2;
1019    this->CopyNetParams(kCopyDiff, &param_grads_loss_2);
1020    loss_weight = 3;
1021    midnet_loss_weight = 1;
1022    Caffe::set_random_seed(this->seed_);
1023    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1024                                 kForceBackward);
1025    const Dtype loss_main_3 = this->net_->ForwardBackward();
1026    const vector<shared_ptr<Blob<Dtype> > >& blob_grads_loss_3 =
1027        this->net_->blobs();
1028    ASSERT_EQ(blob_grads.size(), blob_grads_loss_3.size());
1029    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_loss_3.size());
1030    for (int j = 0; j < blob_grads.size(); ++j) {
1031      const string& blob_name = this->net_->blob_names()[j];
1032      bool grad_should_change = true;
1033      if (blob_name == "innerproduct1_innerproduct1_0_split_0") {
1034        grad_should_change = false;
1035      }
1036      ASSERT_EQ(blob_grads[j]->count(), blob_grads_loss_3[j]->count());
1037      ASSERT_EQ(blob_grads_loss_2[j]->count(), blob_grads_loss_3[j]->count());
1038      for (int k = 0; k < blob_grads[j]->count(); ++k) {
1039        const Dtype grad_diff_2 = blob_grads_loss_2[j]->cpu_diff()[k] -
1040                                      blob_grads[j]->cpu_diff()[k];
1041        const Dtype grad_diff_3 = blob_grads_loss_3[j]->cpu_diff()[k] -
1042                                      blob_grads[j]->cpu_diff()[k];
1043        if (grad_should_change) {
1044          const Dtype kMinGradDiffAbsValue = 1e-4;
1045          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) << blob_name;
1046          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) << blob_name;
1047        } else {
1048          EXPECT_EQ(0, grad_diff_2) << blob_name;
1049          EXPECT_EQ(0, grad_diff_3) << blob_name;
1050        }
1051      }
1052    }
1053    loss_weight = 1;
1054    midnet_loss_weight = 2;
1055    Caffe::set_random_seed(this->seed_);
1056    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1057                                 kForceBackward);
1058    const Dtype loss_midnet_2 = this->net_->ForwardBackward();
1059    this->CopyNetBlobs(kCopyDiff, &blob_grads_loss_2);
1060    this->CopyNetParams(kCopyDiff, &param_grads_loss_2);
1061    loss_weight = 1;
1062    midnet_loss_weight = 3;
1063    Caffe::set_random_seed(this->seed_);
1064    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1065                                 kForceBackward);
1066    const Dtype loss_midnet_3 = this->net_->ForwardBackward();
1067    const vector<shared_ptr<Blob<Dtype> > >& blob_grads_midnet_loss_3 =
1068        this->net_->blobs();
1069    ASSERT_EQ(blob_grads.size(), blob_grads_midnet_loss_3.size());
1070    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_midnet_loss_3.size());
1071    const vector<string>& blob_names = this->net_->blob_names();
1072    for (int j = 0; j < blob_grads.size(); ++j) {
1073      const string& blob_name = blob_names[j];
1074      bool grad_should_change = false;
1075      if (blob_name == "innerproduct1" ||
1076          blob_name == "innerproduct1_innerproduct1_0_split_0" ||
1077          blob_name == "data_data_0_split_0" || blob_name == "data") {
1078        grad_should_change = true;
1079      }
1080      ASSERT_EQ(blob_grads[j]->count(), blob_grads_midnet_loss_3[j]->count());
1081      ASSERT_EQ(blob_grads[j]->count(), blob_grads_loss_2[j]->count());
1082      for (int k = 0; k < blob_grads[j]->count(); ++k) {
1083        const Dtype grad_diff_2 = blob_grads_loss_2[j]->cpu_diff()[k] -
1084                                      blob_grads[j]->cpu_diff()[k];
1085        const Dtype grad_diff_3 = blob_grads_midnet_loss_3[j]->cpu_diff()[k] -
1086                                      blob_grads[j]->cpu_diff()[k];
1087        if (grad_should_change) {
1088          const Dtype kMinGradDiffAbsValue = 1e-4;
1089          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) << blob_name;
1090          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) << blob_name;
1091        } else {
1092          EXPECT_EQ(0, grad_diff_2) << blob_name;
1093          EXPECT_EQ(0, grad_diff_3) << blob_name;
1094        }
1095      }
1096    }
1097    const Dtype kMinLossDiffAbsValue = 1e-4;
1098    Dtype loss_diff_2 = loss_main_2 - loss;
1099    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1100    Dtype loss_diff_3 = loss_main_3 - loss;
1101    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1102    loss_diff_2 = loss_midnet_2 - loss;
1103    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1104    loss_diff_3 = loss_midnet_3 - loss;
1105    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1106  }
1107  TYPED_TEST(NetTest, TestBackwardWithAccuracyLayer) {
1108    const bool kForceBackward = false;
1109    const bool kAccuracyLayer = true;
1110    this->InitTinyNet(kForceBackward, kAccuracyLayer);
1111    EXPECT_TRUE(this->net_->has_blob("accuracy"));
1112    this->net_->ForwardBackward();
1113  }
1114  TYPED_TEST(NetTest, TestUnsharedWeightsDataNet) {
1115    typedef typename TypeParam::Dtype Dtype;
1116    this->InitUnsharedWeightsNet();
1117    Dtype loss;
1118    this->net_->Forward(&loss);
1119    EXPECT_GT(loss, 0);
1120  }
1121  TYPED_TEST(NetTest, TestSharedWeightsDataNet) {
1122    typedef typename TypeParam::Dtype Dtype;
1123    this->InitSharedWeightsNet();
1124    Dtype loss;
1125    this->net_->Forward(&loss);
1126    EXPECT_FLOAT_EQ(loss, 0);
1127  }
1128  TYPED_TEST(NetTest, TestUnsharedWeightsDiffNet) {
1129    typedef typename TypeParam::Dtype Dtype;
1130    this->InitUnsharedWeightsNet();
1131    Net<Dtype>* net = this->net_.get();
1132    net->Forward();
1133    net->Backward();
1134    Layer<Dtype>* ip1_layer = net->layer_by_name("innerproduct1").get();
1135    Layer<Dtype>* ip2_layer = net->layer_by_name("innerproduct2").get();
1136    const int count = ip1_layer->blobs()[0]->count();
1137    const Dtype* grad1 = ip1_layer->blobs()[0]->cpu_diff();
1138    const Dtype* grad2 = ip2_layer->blobs()[0]->cpu_diff();
<span onclick='openModal()' class='match'>1139    for (int i = 0; i < count; ++i) {
1140      EXPECT_GT(fabs(grad1[i]), 0);
1141      EXPECT_FLOAT_EQ(-1 * grad1[i], grad2[i]);
1142    }
1143  }
1144  TYPED_TEST(NetTest, TestSharedWeightsDiffNet) {
1145    typedef typename TypeParam::Dtype Dtype;
1146    this->InitSharedWeightsNet();
1147    Net<Dtype>* net = this->net_.get();
</span>1148    Dtype loss;
1149    net->Forward(&loss);
1150    net->Backward();
1151    EXPECT_FLOAT_EQ(loss, 0);
1152    Layer<Dtype>* ip1_layer = net->layer_by_name("innerproduct1").get();
1153    Layer<Dtype>* ip2_layer = net->layer_by_name("innerproduct2").get();
1154    const int count = ip1_layer->blobs()[0]->count();
1155    const Dtype* grad1 = ip1_layer->blobs()[0]->cpu_diff();
1156    const Dtype* grad2 = ip2_layer->blobs()[0]->cpu_diff();
1157    for (int i = 0; i < count; ++i) {
1158      EXPECT_FLOAT_EQ(0, grad1[i]);
1159      EXPECT_FLOAT_EQ(0, grad2[i]);
1160    }
1161  }
1162  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1163  TYPED_TEST(NetTest, TestSharedWeightsUpdate) {
1164    typedef typename TypeParam::Dtype Dtype;
1165    Caffe::set_random_seed(this->seed_);
1166    this->InitDiffDataSharedWeightsNet();
1167    EXPECT_EQ(this->net_->layer_names()[1], "innerproduct1");
1168    EXPECT_EQ(this->net_->layer_names()[2], "innerproduct2");
1169    Blob<Dtype>* ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1170    Blob<Dtype>* ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1171    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1172    EXPECT_EQ(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1173    this->net_->Forward();
1174    this->net_->Backward();
1175    Blob<Dtype> shared_params;
1176    const bool reshape = true;
1177    const bool copy_diff = false;
1178    shared_params.CopyFrom(*ip1_weights, copy_diff, reshape);
1179    shared_params.CopyFrom(*ip1_weights, !copy_diff, reshape);
1180    const int count = ip1_weights->count();
1181    for (int i = 0; i < count; ++i) {
1182      EXPECT_NE(0, ip1_weights->cpu_diff()[i]);
1183    }
1184    caffe_axpy(count, Dtype(-1), shared_params.cpu_diff(),
1185               shared_params.mutable_cpu_data());
1186    const Dtype* expected_updated_params = shared_params.cpu_data();
1187    this->net_->Update();
1188    const Dtype* actual_updated_params = ip1_weights->cpu_data();
1189    for (int i = 0; i < count; ++i) {
1190      EXPECT_EQ(expected_updated_params[i], actual_updated_params[i]);
1191    }
1192    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1193    Caffe::set_random_seed(this->seed_);
1194    this->InitDiffDataUnsharedWeightsNet();
1195    EXPECT_EQ(this->net_->layer_names()[1], "innerproduct1");
1196    EXPECT_EQ(this->net_->layer_names()[2], "innerproduct2");
1197    ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1198    ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1199    EXPECT_NE(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1200    EXPECT_NE(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1201    this->net_->Forward();
1202    this->net_->Backward();
1203    Blob<Dtype> unshared_params1;
1204    unshared_params1.CopyFrom(*ip1_weights, copy_diff, reshape);
1205    unshared_params1.CopyFrom(*ip1_weights, !copy_diff, reshape);
1206    Blob<Dtype> unshared_params2;
1207    unshared_params2.CopyFrom(*ip2_weights, copy_diff, reshape);
1208    unshared_params2.CopyFrom(*ip2_weights, !copy_diff, reshape);
1209    for (int i = 0; i < count; ++i) {
1210      EXPECT_NE(0, ip1_weights->cpu_diff()[i]);
1211      EXPECT_NE(0, ip2_weights->cpu_diff()[i]);
1212      EXPECT_NE(ip1_weights->cpu_diff()[i], ip2_weights->cpu_diff()[i]);
1213      EXPECT_FLOAT_EQ(ip1_weights->cpu_diff()[i] + ip2_weights->cpu_diff()[i],
1214                      shared_params.cpu_diff()[i]);
1215    }
1216    caffe_axpy(count, Dtype(-1), ip1_weights->cpu_diff(),
1217               unshared_params1.mutable_cpu_data());
1218    caffe_axpy(count, Dtype(-1), ip2_weights->cpu_diff(),
1219               unshared_params2.mutable_cpu_data());
1220    const Dtype* expected_updated_params1 = unshared_params1.cpu_data();
1221    const Dtype* expected_updated_params2 = unshared_params2.cpu_data();
1222    this->net_->Update();
1223    const Dtype* actual_updated_params1 = ip1_weights->cpu_data();
1224    const Dtype* actual_updated_params2 = ip2_weights->cpu_data();
1225    for (int i = 0; i < count; ++i) {
1226      EXPECT_EQ(expected_updated_params1[i], actual_updated_params1[i]);
1227      EXPECT_EQ(expected_updated_params2[i], actual_updated_params2[i]);
1228      EXPECT_NE(actual_updated_params1[i], actual_updated_params2[i]);
1229      EXPECT_NE(expected_updated_params, expected_updated_params1);
1230    }
1231  }
1232  #endif
1233  TYPED_TEST(NetTest, TestSharedWeightsResume) {
1234    typedef typename TypeParam::Dtype Dtype;
1235    Caffe::set_random_seed(this->seed_);
1236    this->InitDiffDataSharedWeightsNet();
1237    EXPECT_EQ(this->net_->layer_names()[1], "innerproduct1");
1238    EXPECT_EQ(this->net_->layer_names()[2], "innerproduct2");
1239    Blob<Dtype>* ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1240    Blob<Dtype>* ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1241    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1242    EXPECT_EQ(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1243    this->net_->ForwardBackward();
1244    this->net_->Update();
1245    Blob<Dtype> shared_params;
1246    const bool kReshape = true;
1247    const bool kCopyDiff = false;
1248    shared_params.CopyFrom(*ip1_weights, kCopyDiff, kReshape);
1249    const int count = ip1_weights->count();
1250    NetParameter net_param;
1251    this->net_->ToProto(&net_param);
1252    Caffe::set_random_seed(this->seed_);
1253    this->InitDiffDataSharedWeightsNet();
1254    this->net_->CopyTrainedLayersFrom(net_param);
1255    ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1256    ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1257    ASSERT_FALSE(NULL == ip1_weights);
1258    ASSERT_FALSE(NULL == ip2_weights);
1259    EXPECT_NE(ip1_weights, ip2_weights);
1260    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1261    EXPECT_EQ(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1262    for (int i = 0; i < count; ++i) {
1263      EXPECT_FLOAT_EQ(shared_params.cpu_data()[i], ip1_weights->cpu_data()[i]);
1264    }
1265  }
1266  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1267  TYPED_TEST(NetTest, TestParamPropagateDown) {
1268    typedef typename TypeParam::Dtype Dtype;
1269    const bool kBiasTerm = true, kForceBackward = false;
1270    const Dtype* kLossWeight1 = NULL;
1271    const Dtype* kLossWeight2 = NULL;
1272    Caffe::set_random_seed(this->seed_);
1273    Dtype blobs_lr_w1 = 1, blobs_lr_w2 = 1, blobs_lr_b1 = 2, blobs_lr_b2 = 2;
1274    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1275        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1276    this->net_->Forward();
1277    this->net_->Backward();
1278    const vector<shared_ptr<Blob<Dtype> > >& params = this->net_->params();
1279    const int num_params = params.size();
1280    ASSERT_EQ(4, num_params);
1281    const Dtype kNonZeroTestMin = 1e-3;
1282    vector<Dtype> param_asums(params.size());
1283    for (int i = 0; i < num_params; ++i) {
1284      const Dtype param_asum =
1285         caffe_cpu_asum(params[i]->count(), params[i]->cpu_diff());
1286      param_asums[i] = param_asum;
1287      EXPECT_GT(param_asum, kNonZeroTestMin);
1288    }
1289    Caffe::set_random_seed(this->seed_);
1290    blobs_lr_w1 *= 2, blobs_lr_w2 *= 2, blobs_lr_b1 *= 2, blobs_lr_b2 *= 2;
1291    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1292        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1293    this->net_->Forward();
1294    this->net_->Backward();
1295    const vector<shared_ptr<Blob<Dtype> > >& params2 = this->net_->params();
1296    ASSERT_EQ(num_params, params2.size());
1297    for (int i = 0; i < num_params; ++i) {
1298      const Dtype param_asum =
1299         caffe_cpu_asum(params2[i]->count(), params2[i]->cpu_diff());
1300      EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1301    }
1302    Caffe::set_random_seed(this->seed_);
1303    blobs_lr_w1 = 1, blobs_lr_w2 = 0, blobs_lr_b1 = 0, blobs_lr_b2 = 1;
1304    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1305        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1306    this->net_->Forward();
1307    this->net_->Backward();
1308    const vector<shared_ptr<Blob<Dtype> > >& params3 = this->net_->params();
1309    ASSERT_EQ(num_params, params3.size());
1310    for (int i = 0; i < num_params; ++i) {
1311      const Dtype param_asum =
1312         caffe_cpu_asum(params3[i]->count(), params3[i]->cpu_diff());
1313      if (i == 1 || i == 2) {
1314        EXPECT_FLOAT_EQ(0, param_asum);
1315      } else {
1316        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1317      }
1318    }
1319    Caffe::set_random_seed(this->seed_);
1320    blobs_lr_w1 = 0, blobs_lr_w2 = 1, blobs_lr_b1 = 1, blobs_lr_b2 = 0;
1321    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1322        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1323    this->net_->Forward();
1324    this->net_->Backward();
1325    const vector<shared_ptr<Blob<Dtype> > >& params4 = this->net_->params();
1326    ASSERT_EQ(num_params, params4.size());
1327    for (int i = 0; i < num_params; ++i) {
1328      const Dtype param_asum =
1329         caffe_cpu_asum(params4[i]->count(), params4[i]->cpu_diff());
1330      if (i == 0 || i == 3) {
1331        EXPECT_FLOAT_EQ(0, param_asum);
1332      } else {
1333        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1334      }
1335    }
1336  }
1337  #endif
1338  TYPED_TEST(NetTest, TestFromTo) {
1339    typedef typename TypeParam::Dtype Dtype;
1340    this->InitTinyNet();
1341    Blob<Dtype> data;
1342    data.ReshapeLike(*this->net_->blob_by_name("data"));
1343    this->net_->Forward();
1344    this->net_->Backward();
1345    data.CopyFrom(*this->net_->blob_by_name("data"), true, true);
1346    const Dtype *loss_ptr = this->net_->output_blobs()[0]->cpu_data();
1347    Dtype loss = *loss_ptr;
1348    for (int i = 1; i < this->net_->layers().size(); ++i) {
1349      this->net_->ForwardFromTo(1, 1);
1350      if (i < this->net_->layers().size() - 1) {
1351        this->net_->ForwardFrom(i + 1);
1352      }
1353      EXPECT_EQ(loss, *loss_ptr);
1354    }
1355    for (int i = 1; i < this->net_->layers().size(); ++i) {
1356      this->net_->BackwardTo(i);
1357      this->net_->BackwardFrom(i - 1);
1358      for (int j = 0; j < data.count(); ++j) {
1359        EXPECT_EQ(data.cpu_diff()[j],
1360            this->net_->blob_by_name("data")->cpu_diff()[j]);
1361      }
1362    }
1363  }
1364  class FilterNetTest : public ::testing::Test {
1365   protected:
1366    void RunFilterNetTest(
1367        const string& input_param_string, const string& filtered_param_string) {
1368      NetParameter input_param;
1369      CHECK(google::protobuf::TextFormat::ParseFromString(
1370          input_param_string, &input_param));
1371      NetParameter expected_filtered_param;
1372      CHECK(google::protobuf::TextFormat::ParseFromString(
1373          filtered_param_string, &expected_filtered_param));
1374      NetParameter actual_filtered_param;
1375      Net<float>::FilterNet(input_param, &actual_filtered_param);
1376      EXPECT_EQ(expected_filtered_param.DebugString(),
1377          actual_filtered_param.DebugString());
1378      NetParameter double_filtered_param;
1379      Net<float>::FilterNet(actual_filtered_param, &double_filtered_param);
1380      EXPECT_EQ(actual_filtered_param.DebugString(),
1381         double_filtered_param.DebugString());
1382    }
1383  };
1384  TEST_F(FilterNetTest, TestNoFilter) {
1385    const string& input_proto =
1386        "name: 'TestNetwork' "
1387        "layer { "
1388        "  name: 'data' "
1389        "  type: 'Data' "
1390        "  top: 'data' "
1391        "  top: 'label' "
1392        "} "
1393        "layer { "
1394        "  name: 'innerprod' "
1395        "  type: 'InnerProduct' "
1396        "  bottom: 'data' "
1397        "  top: 'innerprod' "
1398        "} "
1399        "layer { "
1400        "  name: 'loss' "
1401        "  type: 'SoftmaxWithLoss' "
1402        "  bottom: 'innerprod' "
1403        "  bottom: 'label' "
1404        "} ";
1405    this->RunFilterNetTest(input_proto, input_proto);
1406  }
1407  TEST_F(FilterNetTest, TestFilterLeNetTrainTest) {
1408    const string& input_proto =
1409        "name: 'LeNet' "
1410        "layer { "
1411        "  name: 'mnist' "
1412        "  type: 'Data' "
1413        "  top: 'data' "
1414        "  top: 'label' "
1415        "  data_param { "
1416        "    source: 'mnist-train-leveldb' "
1417        "    batch_size: 64 "
1418        "  } "
1419        "  transform_param { "
1420        "    scale: 0.00390625 "
1421        "  } "
1422        "  include: { phase: TRAIN } "
1423        "} "
1424        "layer { "
1425        "  name: 'mnist' "
1426        "  type: 'Data' "
1427        "  top: 'data' "
1428        "  top: 'label' "
1429        "  data_param { "
1430        "    source: 'mnist-test-leveldb' "
1431        "    batch_size: 100 "
1432        "  } "
1433        "  transform_param { "
1434        "    scale: 0.00390625 "
1435        "  } "
1436        "  include: { phase: TEST } "
1437        "} "
1438        "layer { "
1439        "  name: 'conv1' "
1440        "  type: 'Convolution' "
1441        "  bottom: 'data' "
1442        "  top: 'conv1' "
1443        "  param { "
1444        "    lr_mult: 1 "
1445        "  } "
1446        "  param { "
1447        "    lr_mult: 2 "
1448        "  } "
1449        "  convolution_param { "
1450        "    num_output: 20 "
1451        "    kernel_size: 5 "
1452        "    stride: 1 "
1453        "    weight_filler { "
1454        "      type: 'xavier' "
1455        "    } "
1456        "    bias_filler { "
1457        "      type: 'constant' "
1458        "    } "
1459        "  } "
1460        "} "
1461        "layer { "
1462        "  name: 'ip1' "
1463        "  type: 'InnerProduct' "
1464        "  bottom: 'conv1' "
1465        "  top: 'ip1' "
1466        "  param { "
1467        "    lr_mult: 1 "
1468        "  } "
1469        "  param { "
1470        "    lr_mult: 2 "
1471        "  } "
1472        "  inner_product_param { "
1473        "    num_output: 10 "
1474        "    weight_filler { "
1475        "      type: 'xavier' "
1476        "    } "
1477        "    bias_filler { "
1478        "      type: 'constant' "
1479        "    } "
1480        "  } "
1481        "} "
1482        "layer { "
1483        "  name: 'accuracy' "
1484        "  type: 'Accuracy' "
1485        "  bottom: 'ip1' "
1486        "  bottom: 'label' "
1487        "  top: 'accuracy' "
1488        "  include: { phase: TEST } "
1489        "} "
1490        "layer { "
1491        "  name: 'loss' "
1492        "  type: 'SoftmaxWithLoss' "
1493        "  bottom: 'ip2' "
1494        "  bottom: 'label' "
1495        "  top: 'loss' "
1496        "} ";
1497    const string input_proto_train = "state: { phase: TRAIN } " + input_proto;
1498    const string input_proto_test = "state: { phase: TEST } " + input_proto;
1499    const string output_proto_train =
1500        "name: 'LeNet' "
1501        "layer { "
1502        "  name: 'mnist' "
1503        "  type: 'Data' "
1504        "  top: 'data' "
1505        "  top: 'label' "
1506        "  data_param { "
1507        "    source: 'mnist-train-leveldb' "
1508        "    batch_size: 64 "
1509        "  } "
1510        "  transform_param { "
1511        "    scale: 0.00390625 "
1512        "  } "
1513        "  include: { phase: TRAIN } "
1514        "} "
1515        "layer { "
1516        "  name: 'conv1' "
1517        "  type: 'Convolution' "
1518        "  bottom: 'data' "
1519        "  top: 'conv1' "
1520        "  param { "
1521        "    lr_mult: 1 "
1522        "  } "
1523        "  param { "
1524        "    lr_mult: 2 "
1525        "  } "
1526        "  convolution_param { "
1527        "    num_output: 20 "
1528        "    kernel_size: 5 "
1529        "    stride: 1 "
1530        "    weight_filler { "
1531        "      type: 'xavier' "
1532        "    } "
1533        "    bias_filler { "
1534        "      type: 'constant' "
1535        "    } "
1536        "  } "
1537        "} "
1538        "layer { "
1539        "  name: 'ip1' "
1540        "  type: 'InnerProduct' "
1541        "  bottom: 'conv1' "
1542        "  top: 'ip1' "
1543        "  param { "
1544        "    lr_mult: 1 "
1545        "  } "
1546        "  param { "
1547        "    lr_mult: 2 "
1548        "  } "
1549        "  inner_product_param { "
1550        "    num_output: 10 "
1551        "    weight_filler { "
1552        "      type: 'xavier' "
1553        "    } "
1554        "    bias_filler { "
1555        "      type: 'constant' "
1556        "    } "
1557        "  } "
1558        "} "
1559        "layer { "
1560        "  name: 'loss' "
1561        "  type: 'SoftmaxWithLoss' "
1562        "  bottom: 'ip2' "
1563        "  bottom: 'label' "
1564        "  top: 'loss' "
1565        "} ";
1566    const string& output_proto_test =
1567        "name: 'LeNet' "
1568        "layer { "
1569        "  name: 'mnist' "
1570        "  type: 'Data' "
1571        "  top: 'data' "
1572        "  top: 'label' "
1573        "  data_param { "
1574        "    source: 'mnist-test-leveldb' "
1575        "    batch_size: 100 "
1576        "  } "
1577        "  transform_param { "
1578        "    scale: 0.00390625 "
1579        "  } "
1580        "  include: { phase: TEST } "
1581        "} "
1582        "layer { "
1583        "  name: 'conv1' "
1584        "  type: 'Convolution' "
1585        "  bottom: 'data' "
1586        "  top: 'conv1' "
1587        "  param { "
1588        "    lr_mult: 1 "
1589        "  } "
1590        "  param { "
1591        "    lr_mult: 2 "
1592        "  } "
1593        "  convolution_param { "
1594        "    num_output: 20 "
1595        "    kernel_size: 5 "
1596        "    stride: 1 "
1597        "    weight_filler { "
1598        "      type: 'xavier' "
1599        "    } "
1600        "    bias_filler { "
1601        "      type: 'constant' "
1602        "    } "
1603        "  } "
1604        "} "
1605        "layer { "
1606        "  name: 'ip1' "
1607        "  type: 'InnerProduct' "
1608        "  bottom: 'conv1' "
1609        "  top: 'ip1' "
1610        "  param { "
1611        "    lr_mult: 1 "
1612        "  } "
1613        "  param { "
1614        "    lr_mult: 2 "
1615        "  } "
1616        "  inner_product_param { "
1617        "    num_output: 10 "
1618        "    weight_filler { "
1619        "      type: 'xavier' "
1620        "    } "
1621        "    bias_filler { "
1622        "      type: 'constant' "
1623        "    } "
1624        "  } "
1625        "} "
1626        "layer { "
1627        "  name: 'accuracy' "
1628        "  type: 'Accuracy' "
1629        "  bottom: 'ip1' "
1630        "  bottom: 'label' "
1631        "  top: 'accuracy' "
1632        "  include: { phase: TEST } "
1633        "} "
1634        "layer { "
1635        "  name: 'loss' "
1636        "  type: 'SoftmaxWithLoss' "
1637        "  bottom: 'ip2' "
1638        "  bottom: 'label' "
1639        "  top: 'loss' "
1640        "} ";
1641    const string output_proto_train_explicit =
1642        output_proto_train + " state: { phase: TRAIN } ";
1643    const string output_proto_test_explicit =
1644        output_proto_test + " state: { phase: TEST } ";
1645    this->RunFilterNetTest(input_proto_train, output_proto_train_explicit);
1646    this->RunFilterNetTest(input_proto_test, output_proto_test_explicit);
1647  }
1648  TEST_F(FilterNetTest, TestFilterOutByStage) {
1649    const string& input_proto =
1650        "name: 'TestNetwork' "
1651        "layer { "
1652        "  name: 'data' "
1653        "  type: 'Data' "
1654        "  top: 'data' "
1655        "  top: 'label' "
1656        "  include: { stage: 'mystage' } "
1657        "} "
1658        "layer { "
1659        "  name: 'innerprod' "
1660        "  type: 'InnerProduct' "
1661        "  bottom: 'data' "
1662        "  top: 'innerprod' "
1663        "} "
1664        "layer { "
1665        "  name: 'loss' "
1666        "  type: 'SoftmaxWithLoss' "
1667        "  bottom: 'innerprod' "
1668        "  bottom: 'label' "
1669        "} ";
1670    const string& output_proto =
1671        "name: 'TestNetwork' "
1672        "layer { "
1673        "  name: 'innerprod' "
1674        "  type: 'InnerProduct' "
1675        "  bottom: 'data' "
1676        "  top: 'innerprod' "
1677        "} "
1678        "layer { "
1679        "  name: 'loss' "
1680        "  type: 'SoftmaxWithLoss' "
1681        "  bottom: 'innerprod' "
1682        "  bottom: 'label' "
1683        "} ";
1684    this->RunFilterNetTest(input_proto, output_proto);
1685  }
1686  TEST_F(FilterNetTest, TestFilterOutByStage2) {
1687    const string& input_proto =
1688        "name: 'TestNetwork' "
1689        "layer { "
1690        "  name: 'data' "
1691        "  type: 'Data' "
1692        "  top: 'data' "
1693        "  top: 'label' "
1694        "} "
1695        "layer { "
1696        "  name: 'innerprod' "
1697        "  type: 'InnerProduct' "
1698        "  bottom: 'data' "
1699        "  top: 'innerprod' "
1700        "  include: { stage: 'mystage' } "
1701        "} "
1702        "layer { "
1703        "  name: 'loss' "
1704        "  type: 'SoftmaxWithLoss' "
1705        "  bottom: 'innerprod' "
1706        "  bottom: 'label' "
1707        "} ";
1708    const string& output_proto =
1709        "name: 'TestNetwork' "
1710        "layer { "
1711        "  name: 'data' "
1712        "  type: 'Data' "
1713        "  top: 'data' "
1714        "  top: 'label' "
1715        "} "
1716        "layer { "
1717        "  name: 'loss' "
1718        "  type: 'SoftmaxWithLoss' "
1719        "  bottom: 'innerprod' "
1720        "  bottom: 'label' "
1721        "} ";
1722    this->RunFilterNetTest(input_proto, output_proto);
1723  }
1724  TEST_F(FilterNetTest, TestFilterInByStage) {
1725    const string& input_proto =
1726        "state: { stage: 'mystage' } "
1727        "name: 'TestNetwork' "
1728        "layer { "
1729        "  name: 'data' "
1730        "  type: 'Data' "
1731        "  top: 'data' "
1732        "  top: 'label' "
1733        "} "
1734        "layer { "
1735        "  name: 'innerprod' "
1736        "  type: 'InnerProduct' "
1737        "  bottom: 'data' "
1738        "  top: 'innerprod' "
1739        "  include: { stage: 'mystage' } "
1740        "} "
1741        "layer { "
1742        "  name: 'loss' "
1743        "  type: 'SoftmaxWithLoss' "
1744        "  bottom: 'innerprod' "
1745        "  bottom: 'label' "
1746        "} ";
1747    this->RunFilterNetTest(input_proto, input_proto);
1748  }
1749  TEST_F(FilterNetTest, TestFilterInByStage2) {
1750    const string& input_proto =
1751        "name: 'TestNetwork' "
1752        "layer { "
1753        "  name: 'data' "
1754        "  type: 'Data' "
1755        "  top: 'data' "
1756        "  top: 'label' "
1757        "} "
1758        "layer { "
1759        "  name: 'innerprod' "
1760        "  type: 'InnerProduct' "
1761        "  bottom: 'data' "
1762        "  top: 'innerprod' "
1763        "  exclude: { stage: 'mystage' } "
1764        "} "
1765        "layer { "
1766        "  name: 'loss' "
1767        "  type: 'SoftmaxWithLoss' "
1768        "  bottom: 'innerprod' "
1769        "  bottom: 'label' "
1770        "} ";
1771    this->RunFilterNetTest(input_proto, input_proto);
1772  }
1773  TEST_F(FilterNetTest, TestFilterOutByMultipleStage) {
1774    const string& input_proto =
1775        "state: { stage: 'mystage' } "
1776        "name: 'TestNetwork' "
1777        "layer { "
1778        "  name: 'data' "
1779        "  type: 'Data' "
1780        "  top: 'data' "
1781        "  top: 'label' "
1782        "} "
1783        "layer { "
1784        "  name: 'innerprod' "
1785        "  type: 'InnerProduct' "
1786        "  bottom: 'data' "
1787        "  top: 'innerprod' "
1788        "  include: { stage: 'mystage' stage: 'myotherstage' } "
1789        "} "
1790        "layer { "
1791        "  name: 'loss' "
1792        "  type: 'SoftmaxWithLoss' "
1793        "  bottom: 'innerprod' "
1794        "  bottom: 'label' "
1795        "  include: { stage: 'mystage' } "
1796        "} ";
1797    const string& output_proto =
1798        "state: { stage: 'mystage' } "
1799        "name: 'TestNetwork' "
1800        "layer { "
1801        "  name: 'data' "
1802        "  type: 'Data' "
1803        "  top: 'data' "
1804        "  top: 'label' "
1805        "} "
1806        "layer { "
1807        "  name: 'loss' "
1808        "  type: 'SoftmaxWithLoss' "
1809        "  bottom: 'innerprod' "
1810        "  bottom: 'label' "
1811        "  include: { stage: 'mystage' } "
1812        "} ";
1813    this->RunFilterNetTest(input_proto, output_proto);
1814  }
1815  TEST_F(FilterNetTest, TestFilterInByMultipleStage) {
1816    const string& input_proto =
1817        "state: { stage: 'mystage' } "
1818        "name: 'TestNetwork' "
1819        "layer { "
1820        "  name: 'data' "
1821        "  type: 'Data' "
1822        "  top: 'data' "
1823        "  top: 'label' "
1824        "} "
1825        "layer { "
1826        "  name: 'innerprod' "
1827        "  type: 'InnerProduct' "
1828        "  bottom: 'data' "
1829        "  top: 'innerprod' "
1830        "  include: { stage: 'myotherstage' } "
1831        "  include: { stage: 'mystage' } "
1832        "} "
1833        "layer { "
1834        "  name: 'loss' "
1835        "  type: 'SoftmaxWithLoss' "
1836        "  bottom: 'innerprod' "
1837        "  bottom: 'label' "
1838        "  include: { stage: 'mystage' } "
1839        "} ";
1840    this->RunFilterNetTest(input_proto, input_proto);
1841  }
1842  TEST_F(FilterNetTest, TestFilterInByMultipleStage2) {
1843    const string& input_proto =
1844        "state: { stage: 'mystage' stage: 'myotherstage' } "
1845        "name: 'TestNetwork' "
1846        "layer { "
1847        "  name: 'data' "
1848        "  type: 'Data' "
1849        "  top: 'data' "
1850        "  top: 'label' "
1851        "} "
1852        "layer { "
1853        "  name: 'innerprod' "
1854        "  type: 'InnerProduct' "
1855        "  bottom: 'data' "
1856        "  top: 'innerprod' "
1857        "  include: { stage: 'mystage' stage: 'myotherstage' } "
1858        "} "
1859        "layer { "
1860        "  name: 'loss' "
1861        "  type: 'SoftmaxWithLoss' "
1862        "  bottom: 'innerprod' "
1863        "  bottom: 'label' "
1864        "  include: { stage: 'mystage' } "
1865        "} ";
1866    this->RunFilterNetTest(input_proto, input_proto);
1867  }
1868  TEST_F(FilterNetTest, TestFilterInByNotStage) {
1869    const string& input_proto =
1870        "state: { stage: 'mystage' } "
1871        "name: 'TestNetwork' "
1872        "layer { "
1873        "  name: 'data' "
1874        "  type: 'Data' "
1875        "  top: 'data' "
1876        "  top: 'label' "
1877        "} "
1878        "layer { "
1879        "  name: 'innerprod' "
1880        "  type: 'InnerProduct' "
1881        "  bottom: 'data' "
1882        "  top: 'innerprod' "
1883        "  include: { not_stage: 'myotherstage' } "
1884        "} "
1885        "layer { "
1886        "  name: 'loss' "
1887        "  type: 'SoftmaxWithLoss' "
1888        "  bottom: 'innerprod' "
1889        "  bottom: 'label' "
1890        "  include: { not_stage: 'myotherstage' } "
1891        "} ";
1892    this->RunFilterNetTest(input_proto, input_proto);
1893  }
1894  TEST_F(FilterNetTest, TestFilterOutByNotStage) {
1895    const string& input_proto =
1896        "state: { stage: 'mystage' } "
1897        "name: 'TestNetwork' "
1898        "layer { "
1899        "  name: 'data' "
1900        "  type: 'Data' "
1901        "  top: 'data' "
1902        "  top: 'label' "
1903        "} "
1904        "layer { "
1905        "  name: 'innerprod' "
1906        "  type: 'InnerProduct' "
1907        "  bottom: 'data' "
1908        "  top: 'innerprod' "
1909        "  include: { not_stage: 'mystage' } "
1910        "} "
1911        "layer { "
1912        "  name: 'loss' "
1913        "  type: 'SoftmaxWithLoss' "
1914        "  bottom: 'innerprod' "
1915        "  bottom: 'label' "
1916        "  include: { not_stage: 'mystage' } "
1917        "} ";
1918    const string& output_proto =
1919        "state: { stage: 'mystage' } "
1920        "name: 'TestNetwork' "
1921        "layer { "
1922        "  name: 'data' "
1923        "  type: 'Data' "
1924        "  top: 'data' "
1925        "  top: 'label' "
1926        "} ";
1927    this->RunFilterNetTest(input_proto, output_proto);
1928  }
1929  TEST_F(FilterNetTest, TestFilterOutByMinLevel) {
1930    const string& input_proto =
1931        "name: 'TestNetwork' "
1932        "layer { "
1933        "  name: 'data' "
1934        "  type: 'Data' "
1935        "  top: 'data' "
1936        "  top: 'label' "
1937        "} "
1938        "layer { "
1939        "  name: 'innerprod' "
1940        "  type: 'InnerProduct' "
1941        "  bottom: 'data' "
1942        "  top: 'innerprod' "
1943        "  include: { min_level: 3 } "
1944        "} "
1945        "layer { "
1946        "  name: 'loss' "
1947        "  type: 'SoftmaxWithLoss' "
1948        "  bottom: 'innerprod' "
1949        "  bottom: 'label' "
1950        "} ";
1951    const string& output_proto =
1952        "name: 'TestNetwork' "
1953        "layer { "
1954        "  name: 'data' "
1955        "  type: 'Data' "
1956        "  top: 'data' "
1957        "  top: 'label' "
1958        "} "
1959        "layer { "
1960        "  name: 'loss' "
1961        "  type: 'SoftmaxWithLoss' "
1962        "  bottom: 'innerprod' "
1963        "  bottom: 'label' "
1964        "} ";
1965    this->RunFilterNetTest(input_proto, output_proto);
1966  }
1967  TEST_F(FilterNetTest, TestFilterOutByMaxLevel) {
1968    const string& input_proto =
1969        "name: 'TestNetwork' "
1970        "layer { "
1971        "  name: 'data' "
1972        "  type: 'Data' "
1973        "  top: 'data' "
1974        "  top: 'label' "
1975        "} "
1976        "layer { "
1977        "  name: 'innerprod' "
1978        "  type: 'InnerProduct' "
1979        "  bottom: 'data' "
1980        "  top: 'innerprod' "
1981        "  include: { max_level: -3 } "
1982        "} "
1983        "layer { "
1984        "  name: 'loss' "
1985        "  type: 'SoftmaxWithLoss' "
1986        "  bottom: 'innerprod' "
1987        "  bottom: 'label' "
1988        "} ";
1989    const string& output_proto =
1990        "name: 'TestNetwork' "
1991        "layer { "
1992        "  name: 'data' "
1993        "  type: 'Data' "
1994        "  top: 'data' "
1995        "  top: 'label' "
1996        "} "
1997        "layer { "
1998        "  name: 'loss' "
1999        "  type: 'SoftmaxWithLoss' "
2000        "  bottom: 'innerprod' "
2001        "  bottom: 'label' "
2002        "} ";
2003    this->RunFilterNetTest(input_proto, output_proto);
2004  }
2005  TEST_F(FilterNetTest, TestFilterInByMinLevel) {
2006    const string& input_proto =
2007        "name: 'TestNetwork' "
2008        "layer { "
2009        "  name: 'data' "
2010        "  type: 'Data' "
2011        "  top: 'data' "
2012        "  top: 'label' "
2013        "} "
2014        "layer { "
2015        "  name: 'innerprod' "
2016        "  type: 'InnerProduct' "
2017        "  bottom: 'data' "
2018        "  top: 'innerprod' "
2019        "  include: { min_level: 0 } "
2020        "} "
2021        "layer { "
2022        "  name: 'loss' "
2023        "  type: 'SoftmaxWithLoss' "
2024        "  bottom: 'innerprod' "
2025        "  bottom: 'label' "
2026        "} ";
2027    this->RunFilterNetTest(input_proto, input_proto);
2028  }
2029  TEST_F(FilterNetTest, TestFilterInByMinLevel2) {
2030    const string& input_proto =
2031        "state: { level: 7 } "
2032        "name: 'TestNetwork' "
2033        "layer { "
2034        "  name: 'data' "
2035        "  type: 'Data' "
2036        "  top: 'data' "
2037        "  top: 'label' "
2038        "} "
2039        "layer { "
2040        "  name: 'innerprod' "
2041        "  type: 'InnerProduct' "
2042        "  bottom: 'data' "
2043        "  top: 'innerprod' "
2044        "  include: { min_level: 3 } "
2045        "} "
2046        "layer { "
2047        "  name: 'loss' "
2048        "  type: 'SoftmaxWithLoss' "
2049        "  bottom: 'innerprod' "
2050        "  bottom: 'label' "
2051        "} ";
2052    this->RunFilterNetTest(input_proto, input_proto);
2053  }
2054  TEST_F(FilterNetTest, TestFilterInByMaxLevel) {
2055    const string& input_proto =
2056        "name: 'TestNetwork' "
2057        "layer { "
2058        "  name: 'data' "
2059        "  type: 'Data' "
2060        "  top: 'data' "
2061        "  top: 'label' "
2062        "} "
2063        "layer { "
2064        "  name: 'innerprod' "
2065        "  type: 'InnerProduct' "
2066        "  bottom: 'data' "
2067        "  top: 'innerprod' "
2068        "  include: { max_level: 0 } "
2069        "} "
2070        "layer { "
2071        "  name: 'loss' "
2072        "  type: 'SoftmaxWithLoss' "
2073        "  bottom: 'innerprod' "
2074        "  bottom: 'label' "
2075        "} ";
2076    this->RunFilterNetTest(input_proto, input_proto);
2077  }
2078  TEST_F(FilterNetTest, TestFilterInByMaxLevel2) {
2079    const string& input_proto =
2080        "state: { level: -7 } "
2081        "name: 'TestNetwork' "
2082        "layer { "
2083        "  name: 'data' "
2084        "  type: 'Data' "
2085        "  top: 'data' "
2086        "  top: 'label' "
2087        "} "
2088        "layer { "
2089        "  name: 'innerprod' "
2090        "  type: 'InnerProduct' "
2091        "  bottom: 'data' "
2092        "  top: 'innerprod' "
2093        "  include: { max_level: -3 } "
2094        "} "
2095        "layer { "
2096        "  name: 'loss' "
2097        "  type: 'SoftmaxWithLoss' "
2098        "  bottom: 'innerprod' "
2099        "  bottom: 'label' "
2100        "} ";
2101    this->RunFilterNetTest(input_proto, input_proto);
2102  }
2103  TEST_F(FilterNetTest, TestFilterInOutByIncludeMultiRule) {
2104    const string& input_proto =
2105        "name: 'TestNetwork' "
2106        "layer { "
2107        "  name: 'data' "
2108        "  type: 'Data' "
2109        "  top: 'data' "
2110        "  top: 'label' "
2111        "} "
2112        "layer { "
2113        "  name: 'innerprod' "
2114        "  type: 'InnerProduct' "
2115        "  bottom: 'data' "
2116        "  top: 'innerprod' "
2117        "  include: { min_level: 2  phase: TRAIN } "
2118        "} "
2119        "layer { "
2120        "  name: 'loss' "
2121        "  type: 'SoftmaxWithLoss' "
2122        "  bottom: 'innerprod' "
2123        "  bottom: 'label' "
2124        "  include: { min_level: 2  phase: TEST } "
2125        "} ";
2126    const string& input_proto_train =
2127        "state: { level: 4  phase: TRAIN } " + input_proto;
2128    const string& input_proto_test =
2129        "state: { level: 4  phase: TEST } " + input_proto;
2130    const string& output_proto_train =
2131        "state: { level: 4  phase: TRAIN } "
2132        "name: 'TestNetwork' "
2133        "layer { "
2134        "  name: 'data' "
2135        "  type: 'Data' "
2136        "  top: 'data' "
2137        "  top: 'label' "
2138        "} "
2139        "layer { "
2140        "  name: 'innerprod' "
2141        "  type: 'InnerProduct' "
2142        "  bottom: 'data' "
2143        "  top: 'innerprod' "
2144        "  include: { min_level: 2  phase: TRAIN } "
2145        "} ";
2146    const string& output_proto_test =
2147        "state: { level: 4  phase: TEST } "
2148        "name: 'TestNetwork' "
2149        "layer { "
2150        "  name: 'data' "
2151        "  type: 'Data' "
2152        "  top: 'data' "
2153        "  top: 'label' "
2154        "} "
2155        "layer { "
2156        "  name: 'loss' "
2157        "  type: 'SoftmaxWithLoss' "
2158        "  bottom: 'innerprod' "
2159        "  bottom: 'label' "
2160        "  include: { min_level: 2  phase: TEST } "
2161        "} ";
2162    this->RunFilterNetTest(input_proto_train, output_proto_train);
2163    this->RunFilterNetTest(input_proto_test, output_proto_test);
2164  }
2165  TEST_F(FilterNetTest, TestFilterInByIncludeMultiRule) {
2166    const string& input_proto =
2167        "name: 'TestNetwork' "
2168        "layer { "
2169        "  name: 'data' "
2170        "  type: 'Data' "
2171        "  top: 'data' "
2172        "  top: 'label' "
2173        "} "
2174        "layer { "
2175        "  name: 'innerprod' "
2176        "  type: 'InnerProduct' "
2177        "  bottom: 'data' "
2178        "  top: 'innerprod' "
2179        "  include: { min_level: 2  phase: TRAIN } "
2180        "  include: { phase: TEST } "
2181        "} "
2182        "layer { "
2183        "  name: 'loss' "
2184        "  type: 'SoftmaxWithLoss' "
2185        "  bottom: 'innerprod' "
2186        "  bottom: 'label' "
2187        "  include: { min_level: 2  phase: TEST } "
2188        "  include: { phase: TRAIN } "
2189        "} ";
2190    const string& input_proto_train =
2191        "state: { level: 2  phase: TRAIN } " + input_proto;
2192    const string& input_proto_test =
2193        "state: { level: 2  phase: TEST } " + input_proto;
2194    this->RunFilterNetTest(input_proto_train, input_proto_train);
2195    this->RunFilterNetTest(input_proto_test, input_proto_test);
2196  }
2197  TEST_F(FilterNetTest, TestFilterInOutByExcludeMultiRule) {
2198    const string& input_proto =
2199        "name: 'TestNetwork' "
2200        "layer { "
2201        "  name: 'data' "
2202        "  type: 'Data' "
2203        "  top: 'data' "
2204        "  top: 'label' "
2205        "} "
2206        "layer { "
2207        "  name: 'innerprod' "
2208        "  type: 'InnerProduct' "
2209        "  bottom: 'data' "
2210        "  top: 'innerprod' "
2211        "  exclude: { min_level: 2  phase: TRAIN } "
2212        "} "
2213        "layer { "
2214        "  name: 'loss' "
2215        "  type: 'SoftmaxWithLoss' "
2216        "  bottom: 'innerprod' "
2217        "  bottom: 'label' "
2218        "  exclude: { min_level: 2  phase: TEST } "
2219        "} ";
2220    const string& input_proto_train =
2221        "state: { level: 4  phase: TRAIN } " + input_proto;
2222    const string& input_proto_test =
2223        "state: { level: 4  phase: TEST } " + input_proto;
2224    const string& output_proto_train =
2225        "state: { level: 4  phase: TRAIN } "
2226        "name: 'TestNetwork' "
2227        "layer { "
2228        "  name: 'data' "
2229        "  type: 'Data' "
2230        "  top: 'data' "
2231        "  top: 'label' "
2232        "} "
2233        "layer { "
2234        "  name: 'loss' "
2235        "  type: 'SoftmaxWithLoss' "
2236        "  bottom: 'innerprod' "
2237        "  bottom: 'label' "
2238        "  exclude: { min_level: 2  phase: TEST } "
2239        "} ";
2240    const string& output_proto_test =
2241        "state: { level: 4  phase: TEST } "
2242        "name: 'TestNetwork' "
2243        "layer { "
2244        "  name: 'data' "
2245        "  type: 'Data' "
2246        "  top: 'data' "
2247        "  top: 'label' "
2248        "} "
2249        "layer { "
2250        "  name: 'innerprod' "
2251        "  type: 'InnerProduct' "
2252        "  bottom: 'data' "
2253        "  top: 'innerprod' "
2254        "  exclude: { min_level: 2  phase: TRAIN } "
2255        "} ";
2256    this->RunFilterNetTest(input_proto_train, output_proto_train);
2257    this->RunFilterNetTest(input_proto_test, output_proto_test);
2258  }
2259  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
2260  TYPED_TEST(NetTest, TestReshape) {
2261    typedef typename TypeParam::Dtype Dtype;
2262    Caffe::set_random_seed(this->seed_);
2263    Caffe::set_mode(Caffe::CPU);
2264    FillerParameter filler_param;
2265    filler_param.set_std(1);
2266    GaussianFiller<Dtype> filler(filler_param);
2267    Blob<Dtype> blob1(2, 3, 12, 10);
2268    Blob<Dtype> blob2(4, 3, 9, 11);
2269    ASSERT_LT(blob1.count(), blob2.count());
2270    filler.Fill(&blob1);
2271    filler.Fill(&blob2);
2272    this->InitReshapableNet();
2273    shared_ptr<Blob<Dtype> > input_blob = this->net_->blob_by_name("data");
2274    Blob<Dtype>* output_blob = this->net_->output_blobs()[0];
2275    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2276        blob1.width());
2277    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2278    this->net_->Forward();
2279    this->net_->Backward();
2280    Blob<Dtype> output1(output_blob->num(), output_blob->channels(),
2281        output_blob->height(), output_blob->width());
2282    caffe_copy(output1.count(), output_blob->cpu_data(),
2283        output1.mutable_cpu_data());
2284    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2285        blob2.width());
2286    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2287    this->net_->Forward();
2288    this->net_->Backward();
2289    Blob<Dtype> output2(output_blob->num(), output_blob->channels(),
2290        output_blob->height(), output_blob->width());
2291    caffe_copy(output2.count(), output_blob->cpu_data(),
2292        output2.mutable_cpu_data());
2293    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2294        blob1.width());
2295    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2296    this->net_->Forward();
2297    this->net_->Backward();
2298    for (int i = 0; i < output1.count(); ++i) {
2299      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob->cpu_data() + i));
2300    }
2301    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2302        blob2.width());
2303    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2304    this->net_->Forward();
2305    this->net_->Backward();
2306    for (int i = 0; i < output2.count(); ++i) {
2307      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob->cpu_data() + i));
2308    }
2309    EXPECT_EQ(output1.num(), blob1.num());
2310    EXPECT_EQ(output2.num(), blob2.num());
2311    bool same_spatial_shape = true;
2312    const int kFirstSpatialAxis = 2;
2313    for (int i = kFirstSpatialAxis; i < output1.num_axes(); ++i) {
2314      if (output1.shape(i) != output2.shape(i)) {
2315        same_spatial_shape = false;
2316        break;
2317      }
2318    }
2319    EXPECT_FALSE(same_spatial_shape);
2320  }
2321  #endif
2322  #ifdef MKL2017_SUPPORTED
2323  TYPED_TEST(NetTestCPU, TestForwardReshapeForward) {
2324    typedef TypeParam Dtype;
2325    const string& proto =
2326        "name: 'TestNetwork' "
2327        " layer {"
2328        "   top: 'data'"
2329        "   top: 'label'"
2330        "   name: 'data'"
2331        "   type: 'DummyData'"
2332        "   dummy_data_param {"
2333        "     shape: { dim: 32 dim: 3 dim: 227 dim: 227 }"
2334        "     data_filler {"
2335        "       type: 'constant'"
2336        "       value: 0.01"
2337        "     }"
2338        "   }"
2339        "   transform_param {"
2340        "     mirror: true"
2341        "     crop_size: 224"
2342        "     mean_value: 104"
2343        "     mean_value: 117"
2344        "     mean_value: 123"
2345        "   }"
2346        " }"
2347        " layer {"
2348        "  bottom: 'data'"
2349        "   top: 'conv'"
2350        "   name: 'conv1'"
2351        "   type: 'Convolution'"
2352        "   param {"
2353        "     lr_mult: 1"
2354        "     decay_mult: 1"
2355        "   }"
2356        "   convolution_param {"
2357        "     "
2358        "     num_output: 64"
2359        "     engine: MKL2017 "
2360        "     pad: 3"
2361        "     kernel_size: 7"
2362        "     stride: 2"
2363        "     weight_filler {"
2364        "       type: 'xavier'"
2365        "     }"
2366        "     bias_term: false"
2367        "   }"
2368        " }"
2369        " layer {"
2370        "   bottom: 'conv'"
2371        "   top: 'relu1'"
2372        "   name: 'relu1'"
2373        "   type: 'ReLU'"
2374        "   relu_param {"
2375        "     engine: MKL2017 "
2376        "     "
2377        "   }"
2378        " }"
2379        " layer {"
2380        "   bottom: 'conv'"
2381        "   top: 'relu2'"
2382        "   name: 'relu2'"
2383        "   type: 'ReLU'"
2384        "   relu_param {"
2385        "     engine: MKL2017 "
2386        "     "
2387        "   }"
2388        " }"
2389        " layer {"
2390        "   bottom: 'relu1'"
2391        "   bottom: 'relu2'"
2392        "   top: 'concat'"
2393        "   name: 'concat'"
2394        "   type: 'Concat'"
2395        "   concat_param {"
2396        "     engine: MKL2017 "
2397        "     "
2398        "   }"
2399        " } "
2400        " layer {"
2401        "   bottom: 'concat'"
2402        "   top: 'lrn'"
2403        "   name: 'LRN'"
2404        "   type: 'LRN'"
2405        "   lrn_param {"
2406        "     engine: MKL2017 "
2407        "     local_size: 5"
2408        "     alpha: 0.0001"
2409        "     beta: 0.75"
2410        "   }"
2411        " }"
2412        " layer {"
2413        "   bottom: 'lrn'"
2414        "   top: 'pooling'"
2415        "   name: 'Pooling'"
2416        "   type: 'Pooling'"
2417        "   pooling_param {"
2418        "     engine: MKL2017 "
2419        "     kernel_size: 5"
2420        "     stride: 2"
2421        "     pool: MAX"
2422        "   }"
2423        " }"
2424        " layer {"
2425        "   bottom: 'pooling'"
2426        "   top: 'bn'"
2427        "   name: 'BatchNorm'"
2428        "   type: 'BatchNorm'"
2429        "   batch_norm_param {"
2430        "     engine: MKL2017 "
2431        "   }"
2432        " }";
2433      this->InitNetFromProtoString(proto);
2434      this->net_->Forward();
2435      shared_ptr<Blob<Dtype> > input_blob = this->net_->blob_by_name("data");
2436      input_blob->Reshape(1, 3, 1280, 720);
2437      this->net_->Forward();
2438  }
2439  #if 0
2440  TYPED_TEST(NetTest, TestTotalForwardReshape) {
2441    typedef typename TypeParam::Dtype Dtype;
2442    Caffe::set_random_seed(this->seed_);
2443    Caffe::set_mode(Caffe::CPU);
2444    FillerParameter filler_param;
2445    filler_param.set_std(1);
2446    GaussianFiller<Dtype> filler(filler_param);
2447    Blob<Dtype> blob1(2, 3, 12, 10);
2448    Blob<Dtype> blob2(4, 3, 9, 11);
2449    ASSERT_LT(blob1.count(), blob2.count());
2450    filler.Fill(&blob1);
2451    filler.Fill(&blob2);
2452    const string& proto =
2453        "name: 'TestNetwork' "
2454        " layer {"
2455        "   top: 'data'"
2456        "   top: 'label'"
2457        "   name: 'data'"
2458        "   type: 'DummyData'"
2459        "   dummy_data_param {"
2460        "     shape: { dim: 3 dim: 3 dim: 13 dim: 11 }"
2461        "     data_filler {"
2462        "       type: 'constant'"
2463        "       value: 0.01"
2464        "     }"
2465        "   }"
2466        "   transform_param {"
2467        "     mirror: true"
2468        "     crop_size: 224"
2469        "     mean_value: 104"
2470        "     mean_value: 117"
2471        "     mean_value: 123"
2472        "   }"
2473        " }"
2474        " layer {"
2475        "  bottom: 'data'"
2476        "   top: 'conv'"
2477        "   name: 'conv1'"
2478        "   type: 'Convolution'"
2479        "   param {"
2480        "     lr_mult: 1"
2481        "     decay_mult: 1"
2482        "   }"
2483        "   convolution_param {"
2484        "     "
2485        "     num_output: 64"
2486        "     engine: MKL2017 "
2487        "     pad: 3"
2488        "     kernel_size: 7"
2489        "     stride: 2"
2490        "     weight_filler {"
2491        "       type: 'xavier'"
2492        "     }"
2493        "     bias_term: false"
2494        "   }"
2495        " }"
2496        " layer {"
2497        "   bottom: 'conv'"
2498        "   top: 'relu1'"
2499        "   name: 'relu1'"
2500        "   type: 'ReLU'"
2501        "   relu_param {"
2502        "     engine: MKL2017 "
2503        "     "
2504        "   }"
2505        " }"
2506        " layer {"
2507        "   bottom: 'conv'"
2508        "   top: 'relu2'"
2509        "   name: 'relu2'"
2510        "   type: 'ReLU'"
2511        "   relu_param {"
2512        "     engine: MKL2017 "
2513        "     "
2514        "   }"
2515        " }"
2516        " layer {"
2517        "   bottom: 'relu1'"
2518        "   bottom: 'relu2'"
2519        "   top: 'concat'"
2520        "   name: 'concat'"
2521        "   type: 'Concat'"
2522        "   concat_param {"
2523        "     engine: MKL2017 "
2524        "     "
2525        "   }"
2526        " } "
2527        " layer {"
2528        "   bottom: 'concat'"
2529        "   top: 'lrn'"
2530        "   name: 'LRN'"
2531        "   type: 'LRN'"
2532        "   lrn_param {"
2533        "     engine: MKL2017 "
2534        "     local_size: 5"
2535        "     alpha: 0.0001"
2536        "     beta: 0.75"
2537        "   }"
2538        " }"
2539        " layer {"
2540        "   bottom: 'lrn'"
2541        "   top: 'pooling'"
2542        "   name: 'Pooling'"
2543        "   type: 'Pooling'"
2544        "   pooling_param {"
2545        "     engine: MKL2017 "
2546        "     kernel_size: 5"
2547        "     stride: 2"
2548        "     pool: MAX"
2549        "   }"
2550        " }"
2551        " layer {"
2552        "   bottom: 'pooling'"
2553        "   top: 'bn'"
2554        "   name: 'BatchNorm'"
2555        "   type: 'BatchNorm'"
2556        "   batch_norm_param {"
2557        "     engine: MKL2017 "
2558        "   }"
2559        " }";
2560      this->InitNetFromProtoString(proto);
2561    shared_ptr<Blob<Dtype> > input_blob = this->net_->blob_by_name("data");
2562    Blob<Dtype>* output_blob = this->net_->output_blobs()[0];
2563    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2564        blob1.width());
2565    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2566    this->net_->Forward();
2567    this->net_->Backward();
2568    Blob<Dtype> output1(output_blob->num(), output_blob->channels(),
2569        output_blob->height(), output_blob->width());
2570    caffe_copy(output1.count(), output_blob->cpu_data(),
2571        output1.mutable_cpu_data());
2572    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2573        blob2.width());
2574    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2575    this->net_->Forward();
2576    this->net_->Backward();
2577    Blob<Dtype> output2(output_blob->num(), output_blob->channels(),
2578        output_blob->height(), output_blob->width());
2579    caffe_copy(output2.count(), output_blob->cpu_data(),
2580        output2.mutable_cpu_data());
2581    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2582        blob1.width());
2583    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2584    this->net_->Forward();
2585    this->net_->Backward();
2586    for (int i = 0; i < output1.count(); ++i) {
2587      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob->cpu_data() + i));
2588    }
2589    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2590        blob2.width());
2591    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2592    this->net_->Forward();
2593    this->net_->Backward();
2594    for (int i = 0; i < output2.count(); ++i) {
2595      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob->cpu_data() + i));
2596    }
2597    EXPECT_EQ(output1.num(), blob1.num());
2598    EXPECT_EQ(output2.num(), blob2.num());
2599    bool same_spatial_shape = true;
2600    const int kFirstSpatialAxis = 2;
2601    for (int i = kFirstSpatialAxis; i < output1.num_axes(); ++i) {
2602      if (output1.shape(i) != output2.shape(i)) {
2603        same_spatial_shape = false;
2604        break;
2605      }
2606    }
2607    EXPECT_FALSE(same_spatial_shape);
2608  }
2609  #endif
2610  #endif
2611  TYPED_TEST(NetTest, TestSkipPropagateDown) {
2612    this->InitSkipPropNet(false);
2613    vector<bool> vec_layer_need_backward = this->net_->layer_need_backward();
2614    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2615      string layer_name = this->net_->layer_names()[layer_id];
2616      if (layer_name == "loss") {
2617        bool need_back = this->net_->bottom_need_backward()[layer_id][1];
2618        EXPECT_TRUE(need_back) << "bottom_need_backward should be True";
2619      }
2620      if (layer_name.find("data") != std::string::npos ||
2621            layer_name == "silence") {
2622        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2623            << "layer_need_backward for " << layer_name << " should be False";
2624      } else {
2625        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2626            << "layer_need_backward for " << layer_name << " should be True";
2627      }
2628    }
2629    this->InitSkipPropNet(true);
2630    vec_layer_need_backward.clear();
2631    vec_layer_need_backward = this->net_->layer_need_backward();
2632    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2633      string layer_name = this->net_->layer_names()[layer_id];
2634      if (layer_name == "loss") {
2635        bool need_back = this->net_->bottom_need_backward()[layer_id][1];
2636        EXPECT_FALSE(need_back) << "bottom_need_backward should be False";
2637      }
2638      if (layer_name == "innerproduct" || layer_name == "loss") {
2639        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2640            << "layer_need_backward for " << layer_name << " should be True";
2641      } else {
2642        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2643            << "layer_need_backward for " << layer_name << " should be False";
2644      }
2645    }
2646  }
2647  TYPED_TEST(NetTest, TestForcePropagateDown) {
2648    this->InitForcePropNet(false);
2649    vector<bool> layer_need_backward = this->net_->layer_need_backward();
2650    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2651      const string& layer_name = this->net_->layer_names()[layer_id];
2652      const vector<bool> need_backward =
2653          this->net_->bottom_need_backward()[layer_id];
2654      if (layer_name == "data") {
2655        ASSERT_EQ(need_backward.size(), 0);
2656        EXPECT_FALSE(layer_need_backward[layer_id]);
2657      } else if (layer_name == "innerproduct") {
2658        ASSERT_EQ(need_backward.size(), 1);
2659        EXPECT_FALSE(need_backward[0]);  
2660        EXPECT_TRUE(layer_need_backward[layer_id]);
2661      } else if (layer_name == "loss") {
2662        ASSERT_EQ(need_backward.size(), 2);
2663        EXPECT_TRUE(need_backward[0]);   
2664        EXPECT_FALSE(need_backward[1]);  
2665        EXPECT_TRUE(layer_need_backward[layer_id]);
2666      } else {
2667        LOG(FATAL) << "Unknown layer: " << layer_name;
2668      }
2669    }
2670    this->InitForcePropNet(true);
2671    layer_need_backward = this->net_->layer_need_backward();
2672    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2673      const string& layer_name = this->net_->layer_names()[layer_id];
2674      const vector<bool> need_backward =
2675          this->net_->bottom_need_backward()[layer_id];
2676      if (layer_name == "data") {
2677        ASSERT_EQ(need_backward.size(), 0);
2678        EXPECT_FALSE(layer_need_backward[layer_id]);
2679      } else if (layer_name == "innerproduct") {
2680        ASSERT_EQ(need_backward.size(), 1);
2681        EXPECT_TRUE(need_backward[0]);  
2682        EXPECT_TRUE(layer_need_backward[layer_id]);
2683      } else if (layer_name == "loss") {
2684        ASSERT_EQ(need_backward.size(), 2);
2685        EXPECT_TRUE(need_backward[0]);   
2686        EXPECT_FALSE(need_backward[1]);  
2687        EXPECT_TRUE(layer_need_backward[layer_id]);
2688      } else {
2689        LOG(FATAL) << "Unknown layer: " << layer_name;
2690      }
2691    }
2692  }
2693  TYPED_TEST(NetTest, TestAllInOneNetTrain) {
2694    vector<string> stages;
2695    stages.push_back("train");
2696    this->InitAllInOneNet(caffe::TRAIN, 0, &stages);
2697    bool found_data = false;
2698    bool found_loss = false;
2699    for (int i = 0; i < this->net_->layers().size(); ++i) {
2700      const string& layer_name = this->net_->layer_names()[i];
2701      if (layer_name == "train-data") {
2702        found_data = true;
2703      } else if (layer_name == "loss") {
2704        found_loss = true;
2705      } else {
2706        ASSERT_NE(layer_name, "val-data");
2707        ASSERT_NE(layer_name, "deploy-data");
2708      }
2709    }
2710    ASSERT_TRUE(found_data);
2711    ASSERT_TRUE(found_loss);
2712  }
2713  TYPED_TEST(NetTest, TestAllInOneNetVal) {
2714    vector<string> stages;
2715    stages.push_back("val");
2716    this->InitAllInOneNet(caffe::TEST, 0, &stages);
2717    bool found_data = false;
2718    bool found_loss = false;
2719    for (int i = 0; i < this->net_->layers().size(); ++i) {
2720      const string& layer_name = this->net_->layer_names()[i];
2721      if (layer_name == "val-data") {
2722        found_data = true;
2723      } else if (layer_name == "loss") {
2724        found_loss = true;
2725      } else {
2726        ASSERT_NE(layer_name, "train-data");
2727        ASSERT_NE(layer_name, "deploy-data");
2728      }
2729    }
2730    ASSERT_TRUE(found_data);
2731    ASSERT_TRUE(found_loss);
2732  }
2733  TYPED_TEST(NetTest, TestAllInOneNetDeploy) {
2734    vector<string> stages;
2735    stages.push_back("deploy");
2736    this->InitAllInOneNet(caffe::TEST, 0, &stages);
2737    bool found_data = false;
2738    for (int i = 0; i < this->net_->layers().size(); ++i) {
2739      const string& layer_name = this->net_->layer_names()[i];
2740      if (layer_name == "deploy-data") {
2741        found_data = true;
2742      } else {
2743        ASSERT_NE(layer_name, "train-data");
2744        ASSERT_NE(layer_name, "val-data");
2745        ASSERT_NE(layer_name, "loss");
2746      }
2747    }
2748    ASSERT_TRUE(found_data);
2749  }
2750  class CompileNetTest : public ::testing::Test {
2751   protected:
2752    void RunCompilerNetTest(
2753        const string& input_param_string, const string& compiled_param_string) {
2754      NetParameter input_param;
2755      CHECK(google::protobuf::TextFormat::ParseFromString(
2756          input_param_string, &input_param));
2757      NetParameter expected_compiled_param;
2758      CHECK(google::protobuf::TextFormat::ParseFromString(
2759          compiled_param_string, &expected_compiled_param));
2760      NetParameter actual_compiled_param;
2761      Net<float>::CompileNet(input_param, &actual_compiled_param);
2762      actual_compiled_param.mutable_compile_net_state()->Clear();
2763      expected_compiled_param.mutable_compile_net_state()->Clear();
2764      string expect_net_string = expected_compiled_param.DebugString();
2765      string actual_net_string = actual_compiled_param.DebugString();
2766      EXPECT_EQ(expect_net_string,
2767          actual_net_string);
2768      NetParameter double_compiled_param;
2769      Net<float>::CompileNet(actual_compiled_param, &double_compiled_param);
2770      double_compiled_param.mutable_compile_net_state()->Clear();
2771      string double_net_string = double_compiled_param.DebugString();
2772      EXPECT_EQ(actual_net_string,
2773         double_net_string);
2774    }
2775  };
2776  #ifndef DISABLE_BN_FOLDING
2777  TEST_F(CompileNetTest, TestRemoveBatchNorm1) {
2778    const string& input_proto = 
2779        "name: 'TestNetwork' "
2780        "layer { "
2781        "  name: 'data' "
2782        "  type: 'Data' "
2783        "  top: 'data' "
2784        "  top: 'label' "
2785        "} "
2786        "layer { "
2787        "  bottom: 'data' "
2788        "  name: 'conv' "
2789        "  top: 'conv' "
2790        "  type: 'Convolution' "
2791        "} "
2792        "layer { "
2793        "  bottom: 'conv' "
2794        "  name: 'bn' "
2795        "  top: 'conv' "
2796        "  type: 'BatchNorm' "
2797        "} "
2798        "layer { "
2799        "  name: 'loss' "
2800        "  type: 'SoftmaxWithLoss' "
2801        "  bottom: 'conv' "
2802        "  bottom: 'label' "
2803        "} ";
2804    const string& output_proto =
2805        "name: 'TestNetwork' "
2806        "layer { "
2807        "  name: 'data' "
2808        "  type: 'Data' "
2809        "  top: 'data' "
2810        "  top: 'label' "
2811        "} "
2812        "layer { "
2813        "  bottom: 'data' "
2814        "  name: 'conv' "
2815        "  top: 'conv' "
2816        "  type: 'Convolution' "
2817        "} "
2818        "layer { "
2819        "  name: 'loss' "
2820        "  type: 'SoftmaxWithLoss' "
2821        "  bottom: 'conv' "
2822        "  bottom: 'label' "
2823        "} ";
2824    this->RunCompilerNetTest(input_proto, output_proto);
2825  }
2826  TEST_F(CompileNetTest, TestRemoveBatchNorm2) {
2827    const string& input_proto = 
2828        "name: 'TestNetwork' "
2829        "layer { "
2830        "  name: 'data' "
2831        "  type: 'Data' "
2832        "  top: 'data' "
2833        "  top: 'label' "
2834        "} "
2835        "layer { "
2836        "  bottom: 'data' "
2837        "  name: 'fc1' "
2838        "  top: 'fc1' "
2839        "  type: 'InnerProduct' "
2840        "} "
2841        "layer { "
2842        "  bottom: 'fc1' "
2843        "  name: 'bn' "
2844        "  top: 'bn' "
2845        "  type: 'BatchNorm' "
2846        "} "
2847        "layer { "
2848        "  name: 'loss' "
2849        "  type: 'SoftmaxWithLoss' "
2850        "  bottom: 'bn' "
2851        "  bottom: 'label' "
2852        "} ";
2853    const string& output_proto =
2854        "name: 'TestNetwork' "
2855        "layer { "
2856        "  name: 'data' "
2857        "  type: 'Data' "
2858        "  top: 'data' "
2859        "  top: 'label' "
2860        "} "
2861        "layer { "
2862        "  bottom: 'data' "
2863        "  name: 'fc1' "
2864        "  top: 'fc1' "
2865        "  type: 'InnerProduct' "
2866        "} "
2867        "layer { "
2868        "  bottom: 'fc1' "
2869        "  name: 'bn' "
2870        "  top: 'bn' "
2871        "  type: 'BatchNorm' "
2872        "} "
2873        "layer { "
2874        "  name: 'loss' "
2875        "  type: 'SoftmaxWithLoss' "
2876        "  bottom: 'bn' "
2877        "  bottom: 'label' "
2878        "} ";
2879    this->RunCompilerNetTest(input_proto, output_proto);
2880  }
2881  TEST_F(CompileNetTest, TestRemoveBatchNorm3) {
2882    const string& input_proto = 
2883        "name: 'TestNetwork' "
2884        "layer { "
2885        "  name: 'data' "
2886        "  type: 'Data' "
2887        "  top: 'data' "
2888        "  top: 'label' "
2889        "} "
2890        "layer { "
2891        "  bottom: 'data' "
2892        "  name: 'conv' "
2893        "  top: 'conv' "
2894        "  type: 'Convolution' "
2895        "} "
2896        "layer { "
2897        "  bottom: 'conv' "
2898        "  name: 'bn' "
2899        "  top: 'conv' "
2900        "  type: 'BatchNorm' "
2901  	  "  batch_norm_param { "
2902  	  "    use_global_stats: false"
2903  	  "  }"
2904        "} "
2905        "layer { "
2906        "  name: 'loss' "
2907        "  type: 'SoftmaxWithLoss' "
2908        "  bottom: 'conv' "
2909        "  bottom: 'label' "
2910        "} ";
2911    const string& output_proto =
2912        "name: 'TestNetwork' "
2913        "layer { "
2914        "  name: 'data' "
2915        "  type: 'Data' "
2916        "  top: 'data' "
2917        "  top: 'label' "
2918        "} "
2919        "layer { "
2920        "  bottom: 'data' "
2921        "  name: 'conv' "
2922        "  top: 'conv' "
2923        "  type: 'Convolution' "
2924        "} "
2925        "layer { "
2926        "  bottom: 'conv' "
2927        "  name: 'bn' "
2928        "  top: 'conv' "
2929        "  type: 'BatchNorm' "
2930  	  "  batch_norm_param { "
2931  	  "    use_global_stats: false"
2932  	  "  }"
2933        "} "
2934        "layer { "
2935        "  name: 'loss' "
2936        "  type: 'SoftmaxWithLoss' "
2937        "  bottom: 'conv' "
2938        "  bottom: 'label' "
2939        "} ";
2940    this->RunCompilerNetTest(input_proto, output_proto);
2941  }
2942  TEST_F(CompileNetTest, TestRemoveBatchNorm4) {
2943    const string& input_proto = 
2944        "name: 'TestNetwork' "
2945        "layer { "
2946        "  name: 'data' "
2947        "  type: 'Data' "
2948        "  top: 'data' "
2949        "  top: 'label' "
2950        "} "
2951        "layer { "
2952        "  bottom: 'data' "
2953        "  name: 'conv' "
2954        "  top: 'conv' "
2955        "  type: 'Convolution' "
2956        "} "
2957        "layer { "
2958        "  bottom: 'conv' "
2959        "  name: 'bn' "
2960        "  top: 'conv' "
2961        "  type: 'BatchNorm' "
2962  	  "  batch_norm_param { "
2963  	  "    use_global_stats: true"
2964  	  "  }"
2965        "} "
2966        "layer { "
2967        "  name: 'loss' "
2968        "  type: 'SoftmaxWithLoss' "
2969        "  bottom: 'conv' "
2970        "  bottom: 'label' "
2971        "} ";
2972    const string& output_proto =
2973        "name: 'TestNetwork' "
2974        "layer { "
2975        "  name: 'data' "
2976        "  type: 'Data' "
2977        "  top: 'data' "
2978        "  top: 'label' "
2979        "} "
2980        "layer { "
2981        "  bottom: 'data' "
2982        "  name: 'conv' "
2983        "  top: 'conv' "
2984        "  type: 'Convolution' "
2985        "} "
2986        "layer { "
2987        "  name: 'loss' "
2988        "  type: 'SoftmaxWithLoss' "
2989        "  bottom: 'conv' "
2990        "  bottom: 'label' "
2991        "} ";
2992    this->RunCompilerNetTest(input_proto, output_proto);
2993  }
2994  #endif
2995  #ifdef MKL2017_SUPPORTED
2996  TEST_F(CompileNetTest, TestCompileNetBatchNorm) {
2997    const string& input_proto =
2998        "name: 'TestNetwork' "
2999        "layer { "
3000        "  name: 'data' "
3001        "  type: 'Data' "
3002        "  top: 'data' "
3003        "  top: 'label' "
3004        "} "
3005        "layer { "
3006        "  bottom: 'data' "
3007        "  name: 'bn' "
3008        "  top: 'bn' "
3009        "  type: 'BatchNorm' "
3010        "  batch_norm_param { "
3011        "   engine: MKL2017 "
3012        "  } "
3013        "} "
3014        "layer { "
3015        " bottom: 'bn' "
3016        " top: 'sc' "
3017        " name: 'sc' "
3018        " type: 'Scale' "
3019        " scale_param { "
3020        "   bias_term: true "
3021        " }"
3022        "}"
3023        "layer { "
3024        "  name: 'loss' "
3025        "  type: 'SoftmaxWithLoss' "
3026        "  bottom: 'sc' "
3027        "  bottom: 'label' "
3028        "} ";
3029    const string& output_proto =
3030        "name: 'TestNetwork' "
3031        "layer { "
3032        "  name: 'data' "
3033        "  type: 'Data' "
3034        "  top: 'data' "
3035        "  top: 'label' "
3036        "} "
3037        "layer { "
3038        "  bottom: 'data' "
3039        "  name: 'bn' "
3040        "  top: 'sc' "
3041        "  type: 'BatchNorm' "
3042        "  batch_norm_param { "
3043        "   engine: MKL2017 "
3044        "   bias_term: true "
3045        "  } "
3046        "} "
3047        "layer { "
3048        "  name: 'loss' "
3049        "  type: 'SoftmaxWithLoss' "
3050        "  bottom: 'sc' "
3051        "  bottom: 'label' "
3052        "} ";
3053    this->RunCompilerNetTest(input_proto, output_proto);
3054  }
3055  TEST_F(CompileNetTest, TestCompileNetBatchNormInPlace) {
3056    const string& input_proto =
3057        "name: 'TestNetwork' "
3058        "layer { "
3059        "  name: 'data' "
3060        "  type: 'Data' "
3061        "  top: 'data' "
3062        "  top: 'label' "
3063        "} "
3064        "layer { "
3065        "  bottom: 'data' "
3066        "  name: 'bn' "
3067        "  top: 'data' "
3068        "  type: 'BatchNorm' "
3069        "  batch_norm_param { "
3070        "   engine: MKL2017 "
3071        "  } "
3072        "} "
3073        "layer { "
3074        " bottom: 'data' "
3075        " top: 'data' "
3076        " name: 'sc' "
3077        " type: 'Scale' "
3078        " scale_param { "
3079        "   bias_term: true "
3080        " }"
3081        "}"
3082        "layer { "
3083        " bottom: 'data' "
3084        " top: 'data' "
3085        " name: 'relu' "
3086        " type: 'ReLU' "
3087        " relu_param { "
3088        "  engine: MKL2017 "
3089        " } "
3090        "}"
3091        "layer { "
3092        "  name: 'loss' "
3093        "  type: 'SoftmaxWithLoss' "
3094        "  bottom: 'data' "
3095        "  bottom: 'label' "
3096        "} ";
3097    const string& output_proto =
3098        "name: 'TestNetwork' "
3099        "layer { "
3100        "  name: 'data' "
3101        "  type: 'Data' "
3102        "  top: 'data' "
3103        "  top: 'label' "
3104        "} "
3105        "layer { "
3106        "  bottom: 'data' "
3107        "  name: 'bn' "
3108        "  top: 'data_x' "
3109        "  type: 'BatchNorm' "
3110        "  batch_norm_param { "
3111        "   engine: MKL2017 "
3112        "   bias_term: true "
3113        "  } "
3114        "} "
3115        "layer { "
3116        " bottom: 'data_x' "
3117        " top: 'data_x' "
3118        " name: 'relu' "
3119        " type: 'ReLU' "
3120        " relu_param { "
3121        "  engine: MKL2017 "
3122        " } "
3123        "}"
3124        "layer { "
3125        "  name: 'loss' "
3126        "  type: 'SoftmaxWithLoss' "
3127        "  bottom: 'data_x' "
3128        "  bottom: 'label' "
3129        "} ";
3130    this->RunCompilerNetTest(input_proto, output_proto);
3131  }
3132  #endif
3133  #if defined(MKL2017_SUPPORTED) && defined(MKLDNN_SUPPORTED)
3134  TEST_F(CompileNetTest, TestCompileNetBatchNormConvolution) {
3135    const string& input_proto =
3136        "name: 'TestNetwork' "
3137        "layer { "
3138        "  name: 'data' "
3139        "  type: 'Data' "
3140        "  top: 'data' "
3141        "  top: 'label' "
3142        "} "
3143        "layer { "
3144        "  bottom: 'data' "
3145        "  name: 'bn' "
3146        "  top: 'bn' "
3147        "  type: 'BatchNorm' "
3148        "  batch_norm_param { "
3149        "   engine: MKL2017 "
3150        "  } "
3151        "} "
3152        "layer { "
3153        " bottom: 'bn' "
3154        " top: 'conv' "
3155        " name: 'sc' "
3156        " type: 'Scale' "
3157        " scale_param { "
3158        "   bias_term: true "
3159        " }"
3160        "}"
3161        "layer { "
3162        "  bottom: 'conv' "
3163        "  name: 'conv' "
3164        "  top: 'relu' "
3165        "  type: 'Convolution' "
3166        "  convolution_param { "
3167        "   engine: MKLDNN "
3168        "  } "
3169        "} "
3170        "layer { "
3171        " bottom: 'relu' "
3172        " top: 'relu' "
3173        " name: 'relu' "
3174        " type: 'ReLU' "
3175        " relu_param { "
3176        "  engine: MKLDNN "
3177        " } "
3178        "}"
3179        "layer { "
3180        "  name: 'loss' "
3181        "  type: 'SoftmaxWithLoss' "
3182        "  bottom: 'relu' "
3183        "  bottom: 'label' "
3184        "} ";
3185    const string& output_proto =
3186        "name: 'TestNetwork' "
3187        "layer { "
3188        "  name: 'data' "
3189        "  type: 'Data' "
3190        "  top: 'data' "
3191        "  top: 'label' "
3192        "} "
3193        "layer { "
3194        "  bottom: 'data' "
3195        "  name: 'bn' "
3196        "  top: 'conv' "
3197        "  type: 'BatchNorm' "
3198        "  batch_norm_param { "
3199        "   engine: MKL2017 "
3200        "   bias_term: true "
3201        "  } "
3202        "} "
3203        "layer { "
3204        "  bottom: 'conv' "
3205        "  name: 'conv' "
3206        "  top: 'relu' "
3207        "  type: 'Convolution' "
3208        "  convolution_param { "
3209        "   engine: MKLDNN "
3210        "   relu: true "
3211        "negative_slope: 0"
3212        "  } "
3213        "} "
3214        "layer { "
3215        "  name: 'loss' "
3216        "  type: 'SoftmaxWithLoss' "
3217        "  bottom: 'relu' "
3218        "  bottom: 'label' "
3219        "} ";
3220    this->RunCompilerNetTest(input_proto, output_proto);
3221  }
3222  #endif
3223  #ifndef DISABLE_CONV_SUM_FUSION
3224  TEST_F(CompileNetTest, TestCompileNetConvEltReluFusionMKLDNN) {
3225    const string& input_proto =
3226        "name: 'TestNetwork' "
3227        "layer { "
3228        "  name: 'data' "
3229        "  type: 'Data' "
3230        "  top: 'data' "
3231        "  top: 'label' "
3232        "} "
3233        "layer { "
3234        "  bottom: 'data' "
3235        "  name: 'conv1' "
3236        "  top: 'conv1' "
3237        "  type: 'Convolution' "
3238        "  convolution_param { "
3239        "   engine: MKLDNN "
3240        "  } "
3241        "} "
3242        "layer { "
3243        "  bottom: 'data' "
3244        "  name: 'conv2' "
3245        "  top: 'conv2' "
3246        "  type: 'Convolution' "
3247        "  convolution_param { "
3248        "   engine: MKLDNN "
3249        "  } "
3250        "} "
3251        "layer { "
3252        "  bottom: 'conv1' "
3253        "  name: 'conv3' "
3254        "  top: 'conv3' "
3255        "  type: 'Convolution' "
3256        "  convolution_param { "
3257        "   engine: MKLDNN "
3258        "  } "
3259        "} "
3260        "layer { "
3261        "  bottom: 'conv2' "
3262        "  bottom: 'conv3' "
3263        "  name: 'conv4' "
3264        "  top: 'relu' "
3265        "  type: 'Eltwise' "
3266        "} "
3267        "layer { "
3268        " bottom: 'relu' "
3269        " top: 'relu' "
3270        " name: 'relu' "
3271        " type: 'ReLU' "
3272        "}"
3273        "layer { "
3274        "  name: 'loss' "
3275        "  type: 'SoftmaxWithLoss' "
3276        "  bottom: 'relu' "
3277        "  bottom: 'label' "
3278        "} ";
3279    const string& output_proto =
3280        "name: 'TestNetwork' "
3281        "layer { "
3282        "  name: 'data' "
3283        "  type: 'Data' "
3284        "  top: 'data' "
3285        "  top: 'label' "
3286        "} "
3287        "layer { "
3288        "  bottom: 'data' "
3289        "  name: 'conv1' "
3290        "  top: 'conv1' "
3291        "  type: 'Convolution' "
3292        "  convolution_param { "
3293        "   engine: MKLDNN "
3294        "  } "
3295        "} "
3296        "layer { "
3297        "  bottom: 'data' "
3298        "  name: 'conv2' "
3299        "  top: 'conv2' "
3300        "  type: 'Convolution' "
3301        "  convolution_param { "
3302        "   engine: MKLDNN "
3303        "  } "
3304        "} "
3305        "layer { "
3306        "  bottom: 'conv1' "
3307        "  bottom: 'conv2' "
3308        "  name: 'conv3' "
3309        "  top: 'relu' "
3310        "  type: 'Convolution' "
3311        "  convolution_param { "
3312        "   engine: MKLDNN "
3313        "   relu: true "
3314        "   fusion_type: SUM_FUSION "
3315        "  } "
3316        "} "
3317        "layer { "
3318        "  name: 'loss' "
3319        "  type: 'SoftmaxWithLoss' "
3320        "  bottom: 'relu' "
3321        "  bottom: 'label' "
3322        "} ";
3323   const string input_proto_test = "state: { phase: TEST } engine: 'MKLDNN'" + input_proto;
3324   const string output_proto_test = "state: { phase: TEST } engine: 'MKLDNN'" + output_proto;
3325   this->RunCompilerNetTest(input_proto_test, output_proto_test);
3326  }
3327  #endif
3328  #ifdef MKLDNN_SUPPORTED
3329  TEST_F(CompileNetTest, TestCompileNetBatchNormMKLDNN) {
3330      const string& input_proto =
3331        "name: 'TestNetwork' "
3332        "layer { "
3333        "  name: 'data' "
3334        "  type: 'Data' "
3335        "  top: 'data' "
3336        "  top: 'label' "
3337        "} "
3338        "layer { "
3339        "  bottom: 'data' "
3340        "  name: 'bn' "
3341        "  top: 'bn' "
3342        "  type: 'BatchNorm' "
3343        "  batch_norm_param { "
3344        "   engine: MKLDNN "
3345        "  } "
3346        "} "
3347        "layer { "
3348        " bottom: 'bn' "
3349        " top: 'sc' "
3350        " name: 'sc' "
3351        " type: 'Scale' "
3352        " scale_param { "
3353        "   bias_term: true "
3354        " }"
3355        "}"
3356        "layer { "
3357        "  name: 'loss' "
3358        "  type: 'SoftmaxWithLoss' "
3359        "  bottom: 'sc' "
3360        "  bottom: 'label' "
3361        "} ";
3362    const string& output_proto =
3363        "name: 'TestNetwork' "
3364        "layer { "
3365        "  name: 'data' "
3366        "  type: 'Data' "
3367        "  top: 'data' "
3368        "  top: 'label' "
3369        "} "
3370        "layer { "
3371        "  bottom: 'data' "
3372        "  name: 'bn' "
3373        "  top: 'sc' "
3374        "  type: 'BatchNorm' "
3375        "  batch_norm_param { "
3376        "   engine: MKLDNN "
3377        "   bias_term: true "
3378        "  } "
3379        "} "
3380        "layer { "
3381        "  name: 'loss' "
3382        "  type: 'SoftmaxWithLoss' "
3383        "  bottom: 'sc' "
3384        "  bottom: 'label' "
3385        "} ";
3386    this->RunCompilerNetTest(input_proto, output_proto);
3387  }
3388  TEST_F(CompileNetTest, TestCompileNetConvolution) {
3389    const string& input_proto =
3390        "name: 'TestNetwork' "
3391        "layer { "
3392        "  name: 'data' "
3393        "  type: 'Data' "
3394        "  top: 'data' "
3395        "  top: 'label' "
3396        "} "
3397        "layer { "
3398        "  bottom: 'data' "
3399        "  name: 'conv' "
3400        "  top: 'relu' "
3401        "  type: 'Convolution' "
3402        "  convolution_param { "
3403        "   engine: MKLDNN "
3404        "  } "
3405        "} "
3406        "layer { "
3407        " bottom: 'relu' "
3408        " top: 'relu' "
3409        " name: 'relu' "
3410        " type: 'ReLU' "
3411        " relu_param { "
3412        "  engine: MKLDNN "
3413        " } "
3414        "}"
3415        "layer { "
3416        "  name: 'loss' "
3417        "  type: 'SoftmaxWithLoss' "
3418        "  bottom: 'relu' "
3419        "  bottom: 'label' "
3420        "} ";
3421    const string& output_proto =
3422        "name: 'TestNetwork' "
3423        "layer { "
3424        "  name: 'data' "
3425        "  type: 'Data' "
3426        "  top: 'data' "
3427        "  top: 'label' "
3428        "} "
3429        "layer { "
3430        "  bottom: 'data' "
3431        "  name: 'conv' "
3432        "  top: 'relu' "
3433        "  type: 'Convolution' "
3434        "  convolution_param { "
3435        "   engine: MKLDNN "
3436        "   relu: true "
3437        "negative_slope: 0"
3438        "  } "
3439        "} "
3440        "layer { "
3441        "  name: 'loss' "
3442        "  type: 'SoftmaxWithLoss' "
3443        "  bottom: 'relu' "
3444        "  bottom: 'label' "
3445        "} ";
3446    this->RunCompilerNetTest(input_proto, output_proto);
3447  }
3448  TEST_F(CompileNetTest, TestCompileNetLayerParamEngineConvolution) {
3449    const string& input_proto =
3450        "name: 'TestNetwork' "
3451        "layer { "
3452        "  name: 'data' "
3453        "  type: 'Data' "
3454        "  top: 'data' "
3455        "  top: 'label' "
3456        "} "
3457        "layer { "
3458        "  bottom: 'data' "
3459        "  name: 'conv' "
3460        "  top: 'relu' "
3461        "  type: 'Convolution' "
3462        "  engine: 'MKLDNN:CPU' "
3463        "  convolution_param { "
3464        "  } "
3465        "} "
3466        "layer { "
3467        " bottom: 'relu' "
3468        " top: 'relu' "
3469        " name: 'relu' "
3470        " type: 'ReLU' "
3471        " engine: 'MKLDNN:CPU' "
3472        " relu_param { "
3473        " } "
3474        "}"
3475        "layer { "
3476        "  name: 'loss' "
3477        "  type: 'SoftmaxWithLoss' "
3478        "  bottom: 'relu' "
3479        "  bottom: 'label' "
3480        "} ";
3481    const string& output_proto =
3482        "name: 'TestNetwork' "
3483        "layer { "
3484        "  name: 'data' "
3485        "  type: 'Data' "
3486        "  top: 'data' "
3487        "  top: 'label' "
3488        "} "
3489        "layer { "
3490        "  bottom: 'data' "
3491        "  name: 'conv' "
3492        "  top: 'relu' "
3493        "  type: 'Convolution' "
3494        "  engine: 'MKLDNN:CPU' "
3495        "  convolution_param { "
3496        "   relu: true "
3497        "negative_slope: 0"
3498        "  } "
3499        "} "
3500        "layer { "
3501        "  name: 'loss' "
3502        "  type: 'SoftmaxWithLoss' "
3503        "  bottom: 'relu' "
3504        "  bottom: 'label' "
3505        "} ";
3506    this->RunCompilerNetTest(input_proto, output_proto);
3507  }
3508  TEST_F(CompileNetTest, TestNoCompileNetLayerParamEngineConvolution) {
3509    const string& input_proto =
3510        "name: 'TestNetwork' "
3511        "layer { "
3512        "  name: 'data' "
3513        "  type: 'Data' "
3514        "  top: 'data' "
3515        "  top: 'label' "
3516        "} "
3517        "layer { "
3518        "  bottom: 'data' "
3519        "  name: 'conv' "
3520        "  top: 'relu' "
3521        "  type: 'Convolution' "
3522        "  engine: 'MKLDNN:DLA,CPU' "
3523        "  convolution_param { "
3524        "  } "
3525        "} "
3526        "layer { "
3527        " bottom: 'relu' "
3528        " top: 'relu' "
3529        " name: 'relu' "
3530        " type: 'ReLU' "
3531        "  engine: 'MKLDNN:DLA,CPU' "
3532        " relu_param { "
3533        " } "
3534        "}"
3535        "layer { "
3536        "  name: 'loss' "
3537        "  type: 'SoftmaxWithLoss' "
3538        "  bottom: 'relu' "
3539        "  bottom: 'label' "
3540        "} ";
3541    this->RunCompilerNetTest(input_proto, input_proto);
3542  }
3543  #endif
3544  TEST_F(CompileNetTest, TestNoCompileNet) {
3545    const string& input_proto=
3546        "name: 'TestNetwork' "
3547        "layer { "
3548        "  name: 'data' "
3549        "  type: 'Data' "
3550        "  top: 'data' "
3551        "  top: 'label' "
3552        "} "
3553        "layer { "
3554        "  bottom: 'data' "
3555        "  name: 'bn' "
3556        "  top: 'bn' "
3557        "  type: 'BatchNorm' "
3558        "  batch_norm_param { "
3559        "   engine: CAFFE "
3560        "  } "
3561        "} "
3562        "layer { "
3563        " bottom: 'bn' "
3564        " top: 'sc' "
3565        " name: 'sc' "
3566        " type: 'Scale' "
3567        " scale_param { "
3568        "   bias_term: true "
3569        " }"
3570        "}"
3571        "layer { "
3572        "  name: 'loss' "
3573        "  type: 'SoftmaxWithLoss' "
3574        "  bottom: 'sc' "
3575        "  bottom: 'label' "
3576        "} ";
3577    this->RunCompilerNetTest(input_proto, input_proto);
3578  }
3579  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from abseil-cpp-MDEwOlJlcG9zaXRvcnkxMDQyMzE1NDE=-flat-bits_test.cc</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</div>
                </div>
                <div class="column column_space"><pre><code>280      auto input = GeneratePopcountInput<uint64_t>(rng);
281      EXPECT_EQ(popcount(input.value), input.expected);
282    }
283  }
284  TEST(IntegralPowersOfTwo, SingleBit) {
285    EXPECT_FALSE(has_single_bit(uint8_t{}));
</pre></code></div>
                <div class="column column_space"><pre><code>1139    for (int i = 0; i < count; ++i) {
1140      EXPECT_GT(fabs(grad1[i]), 0);
1141      EXPECT_FLOAT_EQ(-1 * grad1[i], grad2[i]);
1142    }
1143  }
1144  TYPED_TEST(NetTest, TestSharedWeightsDiffNet) {
1145    typedef typename TypeParam::Dtype Dtype;
1146    this->InitSharedWeightsNet();
1147    Net<Dtype>* net = this->net_.get();
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    