
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 11.96187972395662%, Tokens: 9, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkldnn_convolution_layer.cpp</h3>
            <pre><code>1  #ifdef MKLDNN_SUPPORTED
2  #include <vector>
3  #include "gtest/gtest.h"
4  #include "caffe/blob.hpp"
5  #include "caffe/common.hpp"
6  #include "caffe/filler.hpp"
7  #include "caffe/layers/mkldnn_layers.hpp"
8  #include "caffe/test/test_caffe_main.hpp"
9  #include "caffe/test/test_gradient_check_util.hpp"
10  namespace caffe {
11  template <typename Dtype>
12  void caffe_conv(const Blob<Dtype>* in, ConvolutionParameter* conv_param,
13      const vector<shared_ptr<Blob<Dtype> > >& weights,
14      Blob<Dtype>* out) {
15    const bool has_depth = (out->num_axes() == 5);
16    if (!has_depth) { CHECK_EQ(4, out->num_axes()); }
17    int kernel_h, kernel_w;
18    if (conv_param->has_kernel_h() || conv_param->has_kernel_w()) {
19      kernel_h = conv_param->kernel_h();
20      kernel_w = conv_param->kernel_w();
21    } else {
22      kernel_h = kernel_w = conv_param->kernel_size(0);
23    }
24    int pad_h, pad_w;
25    if (conv_param->has_pad_h() || conv_param->has_pad_w()) {
26      pad_h = conv_param->pad_h();
27      pad_w = conv_param->pad_w();
28    } else {
29      pad_h = pad_w = conv_param->pad_size() ? conv_param->pad(0) : 0;
30    }
31    int stride_h, stride_w;
32    if (conv_param->has_stride_h() || conv_param->has_stride_w()) {
33      stride_h = conv_param->stride_h();
34      stride_w = conv_param->stride_w();
35    } else {
36      stride_h = stride_w = conv_param->stride_size() ? conv_param->stride(0) : 1;
37    }
38    int dilation_h, dilation_w;
39    dilation_h = dilation_w = conv_param->dilation_size() ?
40                              conv_param->dilation(0) : 1;
41    int kernel_d, pad_d, stride_d, dilation_d;
42    if (has_depth) {
43      kernel_d = kernel_h;
44      stride_d = stride_h;
45      pad_d = pad_h;
46      dilation_d = dilation_h;
47    } else {
48      kernel_d = stride_d = dilation_d = 1;
49      pad_d = 0;
50    }
51    int groups = conv_param->group();
52    int o_g = out->shape(1) / groups;
53    int k_g = in->shape(1) / groups;
54    int o_head, k_head;
55    vector<int> weight_offset(4 + has_depth);
56    vector<int> in_offset(4 + has_depth);
57    vector<int> out_offset(4 + has_depth);
58    Dtype* out_data = out->mutable_cpu_data();
59    for (int n = 0; n < out->shape(0); n++) {
60      for (int g = 0; g < groups; g++) {
61        o_head = o_g * g;
62        k_head = k_g * g;
63        for (int o = 0; o < o_g; o++) {
64          for (int k = 0; k < k_g; k++) {
65            for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
66              for (int y = 0; y < out->shape(2 + has_depth); y++) {
67                for (int x = 0; x < out->shape(3 + has_depth); x++) {
68                  for (int r = 0; r < kernel_d; r++) {
69                    for (int p = 0; p < kernel_h; p++) {
70                      for (int q = 0; q < kernel_w; q++) {
71                        int in_z = z * stride_d - pad_d + r * dilation_d;
72                        int in_y = y * stride_h - pad_h + p * dilation_h;
73                        int in_x = x * stride_w - pad_w + q * dilation_w;
74                        if (in_z >= 0 && in_z < (has_depth ? in->shape(2) : 1)
75                            && in_y >= 0 && in_y < in->shape(2 + has_depth)
76                            && in_x >= 0 && in_x < in->shape(3 + has_depth)) {
77                          weight_offset[0] = o + o_head;
78                          weight_offset[1] = k;
79                          if (has_depth) { weight_offset[2] = r; }
80                          weight_offset[2 + has_depth] = p;
81                          weight_offset[3 + has_depth] = q;
82                          in_offset[0] = n;
83                          in_offset[1] = k + k_head;
84                          if (has_depth) { in_offset[2] = in_z; }
85                          in_offset[2 + has_depth] = in_y;
86                          in_offset[3 + has_depth] = in_x;
87                          out_offset[0] = n;
88                          out_offset[1] = o + o_head;
89                          if (has_depth) { out_offset[2] = z; }
90                          out_offset[2 + has_depth] = y;
91                          out_offset[3 + has_depth] = x;
92                          out_data[out->offset(out_offset)] +=
93                              in->data_at(in_offset)
94                              * weights[0]->data_at(weight_offset);
95                        }
96                      }
97                    }
98                  }
99                }
100              }
101            }
102          }
103        }
104      }
105    }
106    if (conv_param->bias_term()) {
107      const Dtype* bias_data = weights[1]->cpu_data();
108      for (int n = 0; n < out->shape(0); n++) {
109        for (int o = 0; o < out->shape(1); o++) {
110          for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
111            for (int y = 0; y < out->shape(2 + has_depth); y++) {
112              for (int x = 0; x < out->shape(3 + has_depth); x++) {
113                out_offset[0] = n;
114                out_offset[1] = o;
115                if (has_depth) { out_offset[2] = z; }
116                out_offset[2 + has_depth] = y;
117                out_offset[3 + has_depth] = x;
118                out_data[out->offset(out_offset)] += bias_data[o];
119              }
120            }
121          }
122        }
123      }
124    }
125    if (conv_param->relu()){
126      for (int n = 0; n < out->shape(0); n++) {
127        for (int o = 0; o < out->shape(1); o++) {
128          for (int z = 0; z < (has_depth ? out->shape(2) : 1); z++) {
129            for (int y = 0; y < out->shape(2 + has_depth); y++) {
130              for (int x = 0; x < out->shape(3 + has_depth); x++) {
131                out_offset[0] = n;
132                out_offset[1] = o;
133                if (has_depth) { out_offset[2] = z; }
134                out_offset[2 + has_depth] = y;
135                out_offset[3 + has_depth] = x;
136                if(out_data[out->offset(out_offset)] < 0) out_data[out->offset(out_offset)] = 0;
137              }
138            }
139          }
140        }
141      }
142    }
143  }
144  template void caffe_conv(const Blob<float>* in,
145      ConvolutionParameter* conv_param,
146      const vector<shared_ptr<Blob<float> > >& weights,
147      Blob<float>* out);
148  template void caffe_conv(const Blob<double>* in,
149      ConvolutionParameter* conv_param,
150      const vector<shared_ptr<Blob<double> > >& weights,
151      Blob<double>* out);
152  template <typename TypeParam>
153  class MKLDNNConvolutionLayerTest : public MultiDeviceTest<TypeParam> {
154    typedef typename TypeParam::Dtype Dtype;
155  #define MB 2
156  #define IC 8
157  #define OC 8
158  #define IH 5
159  #define IW 5
160  #define OH 5
161  #define OW 5
162  #define KH 3
163  #define KW 3
164  #define CS 1
165  #define GR 2
166  #define PD 1
167   protected:
168    MKLDNNConvolutionLayerTest()
169        : blob_bottom_(new Blob<Dtype>(MB, IC, IH, IW)),
170          blob_bottom_2_(new Blob<Dtype>(MB, IC, IH, IW)),
171          blob_top_(new Blob<Dtype>()),
172          blob_top_2_(new Blob<Dtype>()) {}
173    virtual void SetUp() {
174      FillerParameter filler_param;
175      filler_param.set_value(1.);
176      GaussianFiller<Dtype> filler(filler_param);
177      filler.Fill(this->blob_bottom_);
178      filler.Fill(this->blob_bottom_2_);
179      blob_bottom_vec_.push_back(blob_bottom_);
180      blob_top_vec_.push_back(blob_top_);
181    }
182    virtual ~MKLDNNConvolutionLayerTest() {
183      delete blob_bottom_;
184      delete blob_bottom_2_;
185      delete blob_top_;
186      delete blob_top_2_;
187    }
188    virtual Blob<Dtype>* MakeReferenceTop(Blob<Dtype>* top) {
189      this->ref_blob_top_.reset(new Blob<Dtype>());
190      this->ref_blob_top_->ReshapeLike(*top);
191      return this->ref_blob_top_.get();
192    }
193    Blob<Dtype>* const blob_bottom_;
194    Blob<Dtype>* const blob_bottom_2_;
195    Blob<Dtype>* const blob_top_;
196    Blob<Dtype>* const blob_top_2_;
197    shared_ptr<Blob<Dtype> > ref_blob_top_;
198    vector<Blob<Dtype>*> blob_bottom_vec_;
199    vector<Blob<Dtype>*> blob_top_vec_;
200  };
201  typedef ::testing::Types<CPUDevice<float>
202                          > TestDtypesCPU;
203  TYPED_TEST_CASE(MKLDNNConvolutionLayerTest, TestDtypesCPU);
204  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSetupMKLDNN) {
205    typedef typename TypeParam::Dtype Dtype;
206    LayerParameter layer_param;
207    ConvolutionParameter* convolution_param =
208        layer_param.mutable_convolution_param();
209    convolution_param->add_kernel_size(KH);
210    convolution_param->add_stride(CS);
211    convolution_param->set_num_output(OC);
212    convolution_param->add_pad(PD);
213    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
214    this->blob_top_vec_.push_back(this->blob_top_2_);
215    shared_ptr<Layer<Dtype> > layer(
216        new MKLDNNConvolutionLayer<Dtype>(layer_param));
217    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
218    EXPECT_EQ(this->blob_top_->num(), MB);
219    EXPECT_EQ(this->blob_top_->channels(), OC);
220    EXPECT_EQ(this->blob_top_->height(), OH);
221    EXPECT_EQ(this->blob_top_->width(), OW);
222    EXPECT_EQ(this->blob_top_2_->num(), MB);
223    EXPECT_EQ(this->blob_top_2_->channels(), OC );
224    EXPECT_EQ(this->blob_top_2_->height(), OH);
225    EXPECT_EQ(this->blob_top_2_->width(), OW);
226    convolution_param->set_num_output(OC);
227    convolution_param->set_group(GR);
228    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
229    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
230    EXPECT_EQ(this->blob_top_->num(), MB);
231    EXPECT_EQ(this->blob_top_->channels(), OC);
232    EXPECT_EQ(this->blob_top_->height(), OH);
233    EXPECT_EQ(this->blob_top_->width(), OW);
234    EXPECT_EQ(this->blob_top_2_->num(), MB);
235    EXPECT_EQ(this->blob_top_2_->channels(), OC);
236    EXPECT_EQ(this->blob_top_2_->height(), OH);
237    EXPECT_EQ(this->blob_top_2_->width(), OW);
238  }
239  #if 0
240  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSetupMKLDNNWithRectangeKernelStridePad) {
241    typedef typename TypeParam::Dtype Dtype;
242    LayerParameter layer_param;
243    ConvolutionParameter* convolution_param =
244        layer_param.mutable_convolution_param();
245    convolution_param->set_kernel_h(4);
246    convolution_param->set_kernel_w(1);
247    convolution_param->set_stride_h(3);
248    convolution_param->set_stride_w(1);
249    convolution_param->set_num_output(OC);
250    convolution_param->set_pad_h(2);
251    convolution_param->set_pad_w(1);
252    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
253    this->blob_top_vec_.push_back(this->blob_top_2_);
254    shared_ptr<MKLDNNConvolutionLayer<Dtype> > layer(
255        new MKLDNNConvolutionLayer<Dtype>(layer_param));
256    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
257    EXPECT_EQ(convolution_param->kernel_h(), 4);
258    EXPECT_EQ(layer->GetKernelHeight(), 4);
259    EXPECT_EQ(convolution_param->kernel_w(), 1);
260    EXPECT_EQ(layer->GetKernelWidth(), 1);
261    EXPECT_EQ(convolution_param->stride_h(), 3);
262    EXPECT_EQ(layer->GetStrideHeight(), 3);
263    EXPECT_EQ(convolution_param->stride_w(), 1);
264    EXPECT_EQ(layer->GetStrideWidth(), 1);
265    EXPECT_EQ(convolution_param->pad_h(), 2);
266    EXPECT_EQ(layer->GetPadHeight(), 2);
267    EXPECT_EQ(convolution_param->pad_w(), 1);
268    EXPECT_EQ(layer->GetPadWidth(), 1);
269    convolution_param->set_num_output(OC);
270    convolution_param->set_group(GR);
271    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
272    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
273    EXPECT_EQ(convolution_param->kernel_h(), 4);
274    EXPECT_EQ(layer->GetKernelHeight(), 4);
275    EXPECT_EQ(convolution_param->kernel_w(), 1);
276    EXPECT_EQ(layer->GetKernelWidth(), 1);
277    EXPECT_EQ(convolution_param->stride_h(), 3);
278    EXPECT_EQ(layer->GetStrideHeight(), 3);
279    EXPECT_EQ(convolution_param->stride_w(), 1);
280    EXPECT_EQ(layer->GetStrideWidth(), 1);
281    EXPECT_EQ(convolution_param->pad_h(), 2);
282    EXPECT_EQ(layer->GetPadHeight(), 2);
283    EXPECT_EQ(convolution_param->pad_w(), 1);
284    EXPECT_EQ(layer->GetPadWidth(), 1);
285  }
286  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionMKLDNN) {
287    typedef typename TypeParam::Dtype Dtype;
288    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
289    this->blob_top_vec_.push_back(this->blob_top_2_);
290    LayerParameter layer_param;
291    ConvolutionParameter* convolution_param =
292        layer_param.mutable_convolution_param();
293    convolution_param->add_kernel_size(KH);
294    convolution_param->add_stride(CS);
295    convolution_param->set_num_output(OC);
296    convolution_param->add_pad(PD);
297    convolution_param->mutable_weight_filler()->set_type("gaussian");
298    convolution_param->mutable_bias_filler()->set_type("constant");
299    convolution_param->mutable_bias_filler()->set_value(0.1);
300    shared_ptr<Layer<Dtype> > layer(
301        new MKLDNNConvolutionLayer<Dtype>(layer_param));
302    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
303    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
304    const Dtype* top_data;
305    const Dtype* ref_top_data;
306    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
307        this->MakeReferenceTop(this->blob_top_));
308    top_data = this->blob_top_->cpu_data();
309    ref_top_data = this->ref_blob_top_->cpu_data();
310    for (int i = 0; i < this->blob_top_->count(); ++i) {
311      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
312    }
313  #if 0   
314    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
315        this->MakeReferenceTop(this->blob_top_2_));
316    top_data = this->blob_top_2_->cpu_data();
317    ref_top_data = this->ref_blob_top_->cpu_data();
318    for (int i = 0; i < this->blob_top_->count(); ++i) {
319      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
320    }
321  #endif
322  }
323  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionReLUMKLDNN) {
324    typedef typename TypeParam::Dtype Dtype;
325    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
326    this->blob_top_vec_.push_back(this->blob_top_2_);
327    LayerParameter layer_param;
328    ConvolutionParameter* convolution_param =
329        layer_param.mutable_convolution_param();
330    convolution_param->add_kernel_size(3);
331    convolution_param->add_stride(2);
332    convolution_param->set_num_output(OC);
333    convolution_param->set_relu(true);
334    convolution_param->mutable_weight_filler()->set_type("gaussian");
335    convolution_param->mutable_bias_filler()->set_type("constant");
336    convolution_param->mutable_bias_filler()->set_value(0.1);
337    shared_ptr<Layer<Dtype> > layer(
338        new MKLDNNConvolutionLayer<Dtype>(layer_param));
339    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
340    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
341    const Dtype* top_data;
342    const Dtype* ref_top_data;
343    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
344        this->MakeReferenceTop(this->blob_top_));
345    top_data = this->blob_top_->cpu_data();
346    ref_top_data = this->ref_blob_top_->cpu_data();
347    for (int i = 0; i < this->blob_top_->count(); ++i) {
348      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
349    }
350  }
351  TYPED_TEST(MKLDNNConvolutionLayerTest, TestDilatedConvolutionMKLDNN) {
352    typedef typename TypeParam::Dtype Dtype;
353    vector<int> bottom_shape;
354    bottom_shape.push_back(2);
355    bottom_shape.push_back(3);
356    bottom_shape.push_back(8);
357    bottom_shape.push_back(7);
358    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
359    this->blob_top_vec_.push_back(this->blob_top_2_);
360    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
361      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
362    }
363    LayerParameter layer_param;
364    ConvolutionParameter* convolution_param =
365        layer_param.mutable_convolution_param();
366    convolution_param->add_kernel_size(3);
367    convolution_param->add_dilation(2);
368    convolution_param->set_num_output(4);
369    convolution_param->mutable_weight_filler()->set_type("gaussian");
370    convolution_param->mutable_bias_filler()->set_type("constant");
371    convolution_param->mutable_bias_filler()->set_value(0.1);
372    shared_ptr<Layer<Dtype> > layer(
373        new MKLDNNConvolutionLayer<Dtype>(layer_param));
374    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
375    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
376    const Dtype* top_data;
377    const Dtype* ref_top_data;
378    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
379               this->MakeReferenceTop(this->blob_top_));
380    top_data = this->blob_top_->cpu_data();
381    ref_top_data = this->ref_blob_top_->cpu_data();
382    for (int i = 0; i < this->blob_top_->count(); ++i) {
383      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
384    }
385  #if 0   
386    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
387               this->MakeReferenceTop(this->blob_top_2_));
388    top_data = this->blob_top_2_->cpu_data();
389    ref_top_data = this->ref_blob_top_->cpu_data();
390    for (int i = 0; i < this->blob_top_->count(); ++i) {
391      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
392    }
393  #endif
394  }
395  #endif
396  #if 0
397  TYPED_TEST(MKLDNNConvolutionLayerTest, Test0DConvolutionMKLDNN) {
398    typedef typename TypeParam::Dtype Dtype;
399    LayerParameter layer_param;
400    ConvolutionParameter* convolution_param =
401        layer_param.mutable_convolution_param();
402    const int kNumOutput = 3;
403    convolution_param->set_num_output(kNumOutput);
404    convolution_param->set_axis(3);
405    convolution_param->mutable_weight_filler()->set_type("gaussian");
406    convolution_param->mutable_bias_filler()->set_type("gaussian");
407    shared_ptr<Layer<Dtype> > layer(
408        new MKLDNNConvolutionLayer<Dtype>(layer_param));
409    vector<int> top_shape = this->blob_bottom_->shape();
410    top_shape[3] = kNumOutput;
411    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
412    EXPECT_EQ(top_shape, this->blob_top_->shape());
413    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
414    vector<int> weight_offset(2);
415    const Blob<Dtype>* weight = layer->blobs()[0].get();
416    const Blob<Dtype>* bias = layer->blobs()[1].get();
417    const int num = this->blob_top_->count(3);
418    const int dim = this->blob_top_->shape(3);
419    const int bottom_dim = this->blob_bottom_->shape(3);
420    for (int n = 0; n < num; ++n) {
421      for (int d = 0; d < dim; ++d) {
422        weight_offset[0] = d;
423        Dtype value = bias->cpu_data()[d];
424        for (int bottom_d = 0; bottom_d < bottom_dim; ++bottom_d) {
425          weight_offset[1] = bottom_d;
426          value += weight->data_at(weight_offset) *
427                   this->blob_bottom_->cpu_data()[n * bottom_dim + bottom_d];
428        }
429        EXPECT_NEAR(value, this->blob_top_->cpu_data()[n * dim + d], 1e-4);
430      }
431    }
432  }
433  #endif
434  #if 0
435  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimple3DConvolution) {
436    typedef typename TypeParam::Dtype Dtype;
437    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
438    this->blob_top_vec_.push_back(this->blob_top_2_);
439    vector<int> bottom_shape(5);
440    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
441    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
442    bottom_shape[2] = 5;
443    bottom_shape[3] = this->blob_bottom_vec_[0]->shape(2);
444    bottom_shape[4] = this->blob_bottom_vec_[0]->shape(3);
445    FillerParameter filler_param;
446    GaussianFiller<Dtype> filler(filler_param);
447    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
448      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
449      filler.Fill(this->blob_bottom_vec_[i]);
450    }
451    LayerParameter layer_param;
452    ConvolutionParameter* convolution_param =
453        layer_param.mutable_convolution_param();
454    convolution_param->add_kernel_size(3);
455    convolution_param->add_stride(2);
456    convolution_param->set_num_output(4);
457    convolution_param->mutable_weight_filler()->set_type("gaussian");
458    convolution_param->mutable_bias_filler()->set_type("gaussian");
459    shared_ptr<Layer<Dtype> > layer(
460        new MKLDNNConvolutionLayer<Dtype>(layer_param));
461    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
462    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
463    const Dtype* top_data;
464    const Dtype* ref_top_data;
465    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
466        this->MakeReferenceTop(this->blob_top_));
467    top_data = this->blob_top_->cpu_data();
468    ref_top_data = this->ref_blob_top_->cpu_data();
469    for (int i = 0; i < this->blob_top_->count(); ++i) {
470      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
471    }
472  #if 0   
473    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
474        this->MakeReferenceTop(this->blob_top_2_));
475    top_data = this->blob_top_2_->cpu_data();
476    ref_top_data = this->ref_blob_top_->cpu_data();
477    for (int i = 0; i < this->blob_top_->count(); ++i) {
478      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
479    }
480  #endif
481  }
482  #endif
483  #if 0
484  TYPED_TEST(MKLDNNConvolutionLayerTest, TestDilated3DConvolution) {
485    typedef typename TypeParam::Dtype Dtype;
486    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
487    this->blob_top_vec_.push_back(this->blob_top_2_);
488    vector<int> bottom_shape(5);
489    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
490    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
491    bottom_shape[2] = 6;
492    bottom_shape[3] = 7;
493    bottom_shape[4] = 8;
494    FillerParameter filler_param;
495    GaussianFiller<Dtype> filler(filler_param);
496    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
497      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
498      filler.Fill(this->blob_bottom_vec_[i]);
499    }
500    LayerParameter layer_param;
501    ConvolutionParameter* convolution_param =
502        layer_param.mutable_convolution_param();
503    convolution_param->add_kernel_size(3);
504    convolution_param->add_dilation(2);
505    convolution_param->set_num_output(4);
506    convolution_param->mutable_weight_filler()->set_type("gaussian");
507    convolution_param->mutable_bias_filler()->set_type("gaussian");
508    shared_ptr<Layer<Dtype> > layer(
509        new MKLDNNConvolutionLayer<Dtype>(layer_param));
510    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
511    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
512    const Dtype* top_data;
513    const Dtype* ref_top_data;
514    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
515               this->MakeReferenceTop(this->blob_top_));
516    top_data = this->blob_top_->cpu_data();
517    ref_top_data = this->ref_blob_top_->cpu_data();
518    for (int i = 0; i < this->blob_top_->count(); ++i) {
519      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
520    }
521    caffe_conv(this->blob_bottom_2_, convolution_param, layer->blobs(),
522               this->MakeReferenceTop(this->blob_top_2_));
523    top_data = this->blob_top_2_->cpu_data();
524    ref_top_data = this->ref_blob_top_->cpu_data();
525    for (int i = 0; i < this->blob_top_->count(); ++i) {
526      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
527    }
528  }
529  #endif
530  TYPED_TEST(MKLDNNConvolutionLayerTest, Test1x1Convolution) {
531    typedef typename TypeParam::Dtype Dtype;
532    LayerParameter layer_param;
533    ConvolutionParameter* convolution_param =
534        layer_param.mutable_convolution_param();
535    convolution_param->add_kernel_size(1);
536    convolution_param->add_stride(1);
537    convolution_param->set_num_output(OC);
538    convolution_param->mutable_weight_filler()->set_type("gaussian");
539    convolution_param->mutable_bias_filler()->set_type("constant");
540    convolution_param->mutable_bias_filler()->set_value(0.1);
541    shared_ptr<Layer<Dtype> > layer(
542        new MKLDNNConvolutionLayer<Dtype>(layer_param));
543    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
544    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
545    const Dtype* top_data;
546    const Dtype* ref_top_data;
547    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
548        this->MakeReferenceTop(this->blob_top_));
549    top_data = this->blob_top_->cpu_data();
550    ref_top_data = this->ref_blob_top_->cpu_data();
551    for (int i = 0; i < this->blob_top_->count(); ++i) {
552      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
553    }
554  }
555  TYPED_TEST(MKLDNNConvolutionLayerTest, Test1x1ConvolutionReLU) {
556    typedef typename TypeParam::Dtype Dtype;
557    LayerParameter layer_param;
558    ConvolutionParameter* convolution_param =
559        layer_param.mutable_convolution_param();
560    convolution_param->add_kernel_size(1);
561    convolution_param->add_stride(1);
562    convolution_param->set_num_output(OC);
563    convolution_param->set_relu(true);
564    convolution_param->mutable_weight_filler()->set_type("gaussian");
565    convolution_param->mutable_bias_filler()->set_type("constant");
566    convolution_param->mutable_bias_filler()->set_value(0.1);
567    shared_ptr<Layer<Dtype> > layer(
568        new MKLDNNConvolutionLayer<Dtype>(layer_param));
569    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
570    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
571    const Dtype* top_data;
572    const Dtype* ref_top_data;
573    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
574        this->MakeReferenceTop(this->blob_top_));
575    top_data = this->blob_top_->cpu_data();
576    ref_top_data = this->ref_blob_top_->cpu_data();
577    for (int i = 0; i < this->blob_top_->count(); ++i) {
578      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
579    }
580  }
581  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionGroup) {
582    typedef typename TypeParam::Dtype Dtype;
583    LayerParameter layer_param;
584    ConvolutionParameter* convolution_param =
585        layer_param.mutable_convolution_param();
586    convolution_param->add_kernel_size(KH);
587    convolution_param->add_stride(CS);
588    convolution_param->set_num_output(OC);
589    convolution_param->set_group(GR);
590    convolution_param->add_pad(PD);
591    convolution_param->mutable_weight_filler()->set_type("gaussian");
592    convolution_param->mutable_bias_filler()->set_type("constant");
593    convolution_param->mutable_bias_filler()->set_value(0.1);
594    shared_ptr<Layer<Dtype> > layer(
595        new MKLDNNConvolutionLayer<Dtype>(layer_param));
596    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
597    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
598    const Dtype* top_data;
599    const Dtype* ref_top_data;
600    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
601        this->MakeReferenceTop(this->blob_top_));
602    top_data = this->blob_top_->cpu_data();
603    ref_top_data = this->ref_blob_top_->cpu_data();
604    for (int i = 0; i < this->blob_top_->count(); ++i) {
605      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
606    }
607  }
608  #if 0
609  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSimpleConvolutionReLUGroup) {
610    typedef typename TypeParam::Dtype Dtype;
611    LayerParameter layer_param;
612    ConvolutionParameter* convolution_param =
613        layer_param.mutable_convolution_param();
614    convolution_param->add_kernel_size(3);
615    convolution_param->add_stride(2);
616    convolution_param->set_num_output(OC);
617    convolution_param->set_relu(true);
618    convolution_param->set_group(GR);
619    convolution_param->mutable_weight_filler()->set_type("gaussian");
620    convolution_param->mutable_bias_filler()->set_type("constant");
621    convolution_param->mutable_bias_filler()->set_value(0.1);
622    shared_ptr<Layer<Dtype> > layer(
623        new MKLDNNConvolutionLayer<Dtype>(layer_param));
624    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
625    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
626    const Dtype* top_data;
627    const Dtype* ref_top_data;
628    caffe_conv(this->blob_bottom_, convolution_param, layer->blobs(),
629        this->MakeReferenceTop(this->blob_top_));
630    top_data = this->blob_top_->cpu_data();
631    ref_top_data = this->ref_blob_top_->cpu_data();
632    for (int i = 0; i < this->blob_top_->count(); ++i) {
633      EXPECT_NEAR(top_data[i], ref_top_data[i], 1e-4);
634    }
635  }
636  TYPED_TEST(MKLDNNConvolutionLayerTest, TestSobelConvolution) {
637    typedef typename TypeParam::Dtype Dtype;
638    shared_ptr<GaussianFiller<Dtype> > filler;
639    FillerParameter filler_param;
640    filler_param.set_value(1.);
641    filler.reset(new GaussianFiller<Dtype>(filler_param));
642    filler->Fill(this->blob_bottom_);
643    this->blob_bottom_2_->CopyFrom(*this->blob_bottom_);
644    LayerParameter layer_param;
645    ConvolutionParameter* convolution_param =
646        layer_param.mutable_convolution_param();
647    convolution_param->add_kernel_size(3);
648    convolution_param->add_stride(2);
649    convolution_param->set_num_output(1);
650    convolution_param->set_bias_term(false);
651    shared_ptr<Layer<Dtype> > layer(
652        new MKLDNNConvolutionLayer<Dtype>(layer_param));
653    layer->blobs().resize(1);
654    layer->blobs()[0].reset(new Blob<Dtype>(1, 3, 3, 3));
655    Dtype* weights = layer->blobs()[0]->mutable_cpu_data();
656    for (int c = 0; c < 3; ++c) {
657      int i = c * 9;  
658      weights[i +  0] = -1;
659      weights[i +  1] =  0;
660      weights[i +  2] =  1;
661      weights[i +  3] = -2;
662      weights[i +  4] =  0;
663      weights[i +  5] =  2;
664      weights[i +  6] = -1;
665      weights[i +  7] =  0;
666      weights[i +  8] =  1;
667    }
668    layer->SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
669    layer->Forward(this->blob_bottom_vec_, this->blob_top_vec_);
670    vector<Blob<Dtype>*> sep_blob_bottom_vec;
671    vector<Blob<Dtype>*> sep_blob_top_vec;
672    shared_ptr<Blob<Dtype> > blob_sep(new Blob<Dtype>());
673    sep_blob_bottom_vec.push_back(this->blob_bottom_2_);
674    sep_blob_top_vec.push_back(this->blob_top_2_);
675    convolution_param->clear_kernel_size();
676    convolution_param->clear_stride();
677    convolution_param->set_kernel_h(3);
678    convolution_param->set_kernel_w(1);
679    convolution_param->set_stride_h(2);
680    convolution_param->set_stride_w(1);
681    convolution_param->set_num_output(1);
682    convolution_param->set_bias_term(false);
683    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
684    layer->blobs().resize(1);
685    layer->blobs()[0].reset(new Blob<Dtype>(1, 3, 3, 1));
686    Dtype* weights_1 = layer->blobs()[0]->mutable_cpu_data();
687    for (int c = 0; c < 3; ++c) {
688      int i = c * 3;  
689      weights_1[i +  0] = 1;
690      weights_1[i +  1] = 2;
691      weights_1[i +  2] = 1;
692    }
693    layer->SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
694    layer->Forward(sep_blob_bottom_vec, sep_blob_top_vec);
695    blob_sep->CopyFrom(*this->blob_top_2_, false, true);
696    sep_blob_bottom_vec.clear();
697    sep_blob_bottom_vec.push_back(blob_sep.get());
698    convolution_param->set_kernel_h(1);
699    convolution_param->set_kernel_w(3);
700    convolution_param->set_stride_h(1);
701    convolution_param->set_stride_w(2);
702    convolution_param->set_num_output(1);
703    convolution_param->set_bias_term(false);
704    layer.reset(new MKLDNNConvolutionLayer<Dtype>(layer_param));
705    layer->blobs().resize(1);
706    layer->blobs()[0].reset(new Blob<Dtype>(1, 1, 1, 3));
707    Dtype* weights_2 = layer->blobs()[0]->mutable_cpu_data();
708    weights_2[0] = -1;
709    weights_2[1] =  0;
710    weights_2[2] =  1;
711    layer->SetUp(sep_blob_bottom_vec, sep_blob_top_vec);
712    layer->Forward(sep_blob_bottom_vec, sep_blob_top_vec);
<span onclick='openModal()' class='match'>713    const Dtype* top_data = this->blob_top_->cpu_data();
714    const Dtype* sep_top_data = this->blob_top_2_->cpu_data();
715    for (int i = 0; i < this->blob_top_->count(); ++i) {
</span>716      EXPECT_NEAR(top_data[i], sep_top_data[i], 1e-4);
717    }
718  }
719  #endif
720  #if 0
721  TYPED_TEST(MKLDNNConvolutionLayerTest, TestNDAgainst2D) {
722    typedef typename TypeParam::Dtype Dtype;
723    const int kernel_h = 11;
724    const int kernel_w = 13;
725    vector<int> bottom_shape(4);
726    bottom_shape[0] = 15;
727    bottom_shape[1] = 18;
728    bottom_shape[2] = kernel_h * 2;
729    bottom_shape[3] = kernel_w * 2;
730    FillerParameter filler_param;
731    GaussianFiller<Dtype> filler(filler_param);
732    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
733      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
734      filler.Fill(this->blob_bottom_vec_[i]);
735    }
736    LayerParameter layer_param;
737    ConvolutionParameter* convolution_param =
738        layer_param.mutable_convolution_param();
739    convolution_param->set_num_output(12);
740    convolution_param->set_bias_term(false);
741    convolution_param->set_group(6);
742    convolution_param->set_kernel_h(kernel_h);
743    convolution_param->set_kernel_w(kernel_w);
744    convolution_param->mutable_weight_filler()->set_type("gaussian");
745    Blob<Dtype> weights;
746    Blob<Dtype> top_diff;
747    bool copy_diff;
748    bool reshape;
749    {
750      MKLDNNConvolutionLayer<Dtype> layer(layer_param);
751      layer.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
752      top_diff.ReshapeLike(*this->blob_top_);
753      filler.Fill(&top_diff);
754      ASSERT_EQ(1, layer.blobs().size());
755      copy_diff = false; reshape = true;
756      weights.CopyFrom(*layer.blobs()[0], copy_diff, reshape);
757    }
758    vector<bool> propagate_down(1, true);
759    Blob<Dtype> result_2d;
760    Blob<Dtype> backward_result_2d;
761    Blob<Dtype> backward_weight_result_2d;
762    {
763      caffe_set(this->blob_top_->count(), Dtype(0),
764                this->blob_top_->mutable_cpu_data());
765      caffe_set(this->blob_bottom_->count(), Dtype(0),
766                this->blob_bottom_->mutable_cpu_diff());
767      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
768      convolution_param->set_force_nd_im2col(false);
769      MKLDNNConvolutionLayer<Dtype> layer_2d(layer_param);
770      layer_2d.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
771      ASSERT_EQ(1, layer_2d.blobs().size());
772      copy_diff = false; reshape = false;
773      layer_2d.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
774      layer_2d.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
775      copy_diff = false; reshape = true;
776      result_2d.CopyFrom(*this->blob_top_, copy_diff, reshape);
777      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
778      caffe_copy(top_diff.count(), top_diff.cpu_data(),
779                 this->blob_top_->mutable_cpu_diff());
780      layer_2d.Backward(this->blob_top_vec_, propagate_down,
781                        this->blob_bottom_vec_);
782      copy_diff = true; reshape = true;
783      backward_result_2d.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
784      backward_weight_result_2d.CopyFrom(weights, copy_diff, reshape);
785    }
786    Blob<Dtype> result_nd;
787    Blob<Dtype> backward_result_nd;
788    Blob<Dtype> backward_weight_result_nd;
789    {
790      caffe_set(this->blob_top_->count(), Dtype(0),
791                this->blob_top_->mutable_cpu_data());
792      caffe_set(this->blob_bottom_->count(), Dtype(0),
793                this->blob_bottom_->mutable_cpu_diff());
794      caffe_set(weights.count(), Dtype(0), weights.mutable_cpu_diff());
795      convolution_param->set_force_nd_im2col(true);
796      MKLDNNConvolutionLayer<Dtype> layer_nd(layer_param);
797      layer_nd.SetUp(this->blob_bottom_vec_, this->blob_top_vec_);
798      ASSERT_EQ(1, layer_nd.blobs().size());
799      copy_diff = false; reshape = false;
800      layer_nd.blobs()[0]->CopyFrom(weights, copy_diff, reshape);
801      layer_nd.Forward(this->blob_bottom_vec_, this->blob_top_vec_);
802      copy_diff = false; reshape = true;
803      result_nd.CopyFrom(*this->blob_top_, copy_diff, reshape);
804      ASSERT_EQ(this->blob_top_->shape(), top_diff.shape());
805      caffe_copy(top_diff.count(), top_diff.cpu_data(),
806                 this->blob_top_->mutable_cpu_diff());
807      layer_nd.Backward(this->blob_top_vec_, propagate_down,
808                        this->blob_bottom_vec_);
809      copy_diff = true; reshape = true;
810      backward_result_nd.CopyFrom(*this->blob_bottom_, copy_diff, reshape);
811      backward_weight_result_nd.CopyFrom(weights, copy_diff, reshape);
812    }
813    ASSERT_EQ(result_nd.count(), result_2d.count());
814    for (int i = 0; i < result_2d.count(); ++i)  {
815      EXPECT_EQ(result_2d.cpu_data()[i], result_nd.cpu_data()[i]);
816    }
817    ASSERT_EQ(backward_result_nd.count(), backward_result_2d.count());
818    for (int i = 0; i < backward_result_2d.count(); ++i) {
819      EXPECT_EQ(backward_result_2d.cpu_diff()[i],
820                backward_result_nd.cpu_diff()[i]);
821    }
822    ASSERT_EQ(backward_weight_result_nd.count(),
823              backward_weight_result_2d.count());
824    for (int i = 0; i < backward_weight_result_2d.count(); ++i) {
825      EXPECT_EQ(backward_weight_result_2d.cpu_diff()[i],
826                backward_weight_result_nd.cpu_diff()[i]);
827    }
828  }
829  #endif
830  TYPED_TEST(MKLDNNConvolutionLayerTest, DISABLED_TestGradient) {
831    typedef typename TypeParam::Dtype Dtype;
832    LayerParameter layer_param;
833    ConvolutionParameter* convolution_param =
834        layer_param.mutable_convolution_param();
835    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
836    this->blob_top_vec_.push_back(this->blob_top_2_);
837    convolution_param->add_kernel_size(KH);
838    convolution_param->add_stride(CS);
839    convolution_param->set_num_output(OC);
840    convolution_param->add_pad(PD);
841    convolution_param->mutable_weight_filler()->set_type("gaussian");
842    convolution_param->mutable_bias_filler()->set_type("gaussian");
843    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
844    GradientChecker<Dtype> checker(1e-2, 1e-3);
845    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
846        this->blob_top_vec_);
847  }
848  #if 0
849  TYPED_TEST(MKLDNNConvolutionLayerTest, TestDilatedGradient) {
850    typedef typename TypeParam::Dtype Dtype;
851    LayerParameter layer_param;
852    ConvolutionParameter* convolution_param =
853        layer_param.mutable_convolution_param();
854    vector<int> bottom_shape;
855    bottom_shape.push_back(2);
856    bottom_shape.push_back(3);
857    bottom_shape.push_back(5);
858    bottom_shape.push_back(6);
859    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
860      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
861    }
862    convolution_param->add_kernel_size(3);
863    convolution_param->add_dilation(2);
864    convolution_param->set_num_output(2);
865    convolution_param->mutable_weight_filler()->set_type("gaussian");
866    convolution_param->mutable_bias_filler()->set_type("gaussian");
867    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
868    GradientChecker<Dtype> checker(1e-2, 1e-3);
869    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
870                                    this->blob_top_vec_);
871  }
872  #endif
873  #if 0
874  TYPED_TEST(MKLDNNConvolutionLayerTest, TestGradient3D) {
875    typedef typename TypeParam::Dtype Dtype;
876    LayerParameter layer_param;
877    ConvolutionParameter* convolution_param =
878        layer_param.mutable_convolution_param();
879    vector<int> bottom_shape(5);
880    bottom_shape[0] = this->blob_bottom_vec_[0]->shape(0);
881    bottom_shape[1] = this->blob_bottom_vec_[0]->shape(1);
882    bottom_shape[2] = 5;
883    bottom_shape[3] = this->blob_bottom_vec_[0]->shape(2);
884    bottom_shape[4] = this->blob_bottom_vec_[0]->shape(3);
885    FillerParameter filler_param;
886    GaussianFiller<Dtype> filler(filler_param);
887    for (int i = 0; i < this->blob_bottom_vec_.size(); ++i) {
888      this->blob_bottom_vec_[i]->Reshape(bottom_shape);
889      filler.Fill(this->blob_bottom_vec_[i]);
890    }
891    convolution_param->add_kernel_size(3);
892    convolution_param->add_stride(2);
893    convolution_param->set_num_output(2);
894    convolution_param->mutable_weight_filler()->set_type("gaussian");
895    convolution_param->mutable_bias_filler()->set_type("gaussian");
896    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
897    GradientChecker<Dtype> checker(1e-2, 1e-3);
898    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
899        this->blob_top_vec_);
900  }
901  TYPED_TEST(MKLDNNConvolutionLayerTest, Test1x1Gradient) {
902    typedef typename TypeParam::Dtype Dtype;
903    LayerParameter layer_param;
904    ConvolutionParameter* convolution_param =
905        layer_param.mutable_convolution_param();
906    this->blob_bottom_vec_.push_back(this->blob_bottom_2_);
907    this->blob_top_vec_.push_back(this->blob_top_2_);
908    convolution_param->add_kernel_size(1);
909    convolution_param->add_stride(1);
910    convolution_param->set_num_output(2);
911    convolution_param->mutable_weight_filler()->set_type("gaussian");
912    convolution_param->mutable_bias_filler()->set_type("gaussian");
913    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
914    GradientChecker<Dtype> checker(1e-2, 1e-3);
915    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
916        this->blob_top_vec_);
917  }
918  #endif
919  TYPED_TEST(MKLDNNConvolutionLayerTest, TestGradientGroup) {
920    typedef typename TypeParam::Dtype Dtype;
921    LayerParameter layer_param;
922    ConvolutionParameter* convolution_param =
923        layer_param.mutable_convolution_param();
924    convolution_param->add_kernel_size(3);
925    convolution_param->add_stride(2);
926    convolution_param->set_num_output(2);
927    convolution_param->set_group(GR);
928    convolution_param->mutable_weight_filler()->set_type("gaussian");
929    convolution_param->mutable_bias_filler()->set_type("gaussian");
930    MKLDNNConvolutionLayer<Dtype> layer(layer_param);
931    GradientChecker<Dtype> checker(1e-2, 1e-3);
932    checker.CheckGradientExhaustive(&layer, this->blob_bottom_vec_,
933        this->blob_top_vec_);
934  }
935  }  
936  #endif  
</code></pre>
        </div>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-sch.cpp</h3>
            <pre><code>1  #include "sch.h"
2  TSchTask::TSchTask(
3   const int& _TaskId, const TVec<TStrV>& _DimObjVV,
4   const TSecTm& _StartTm, const TSecTm& _EndTm, const bool& _Forced):
5    TaskId(_TaskId), DimObjVV(_DimObjVV),
6    StartTm(_StartTm), EndTm(_EndTm), Forced(_Forced){
7    IAssert(StartTm<EndTm);
8  }
9  TSchTask::TSchTask(
10   const int& _TaskId, const TStrV& DimObjV,
11   const TSecTm& _StartTm, const TSecTm& _EndTm, const bool& _Forced):
12    TaskId(_TaskId), DimObjVV(),
13    StartTm(_StartTm), EndTm(_EndTm), Forced(_Forced){
14    IAssert(StartTm<EndTm);
15    for (int DimObjN=0; DimObjN<DimObjV.Len(); DimObjN++){
16      DimObjVV.Add(); DimObjVV.Last().Add(DimObjV[DimObjN]);
17    }
18  }
19  TStr TSchTask::GetAllDimObjStr(const int& DimN) const {
20    TChA ChA;
21    for (int ObjN=0; ObjN<GetDimObjs(DimN); ObjN++){
22      if (ObjN>0){ChA+=", ";}
23      ChA+=GetDimObj(DimN, ObjN);
24    }
25    return ChA;
26  }
27  TStr TSchTask::GetStr() const {
28    TChA ChA;
29    if (Forced){ChA+="F:";}
30    for (int DimN=0; DimN<GetDims(); DimN++){
31      if (DimN>0){ChA+=' ';}
32      ChA+='[';
33      for (int ObjN=0; ObjN<GetDimObjs(DimN); ObjN++){
34        if (ObjN>0){ChA+=' ';}
35        ChA+=DimObjVV[DimN][ObjN];
36      }
37      ChA+=']';
38    }
39    return ChA;
40  }
41  bool TSchTask::IsCons(const PSchTask& Task1, const PSchTask& Task2){
42    IAssert((!Task1->IsForced())&&(!Task2->IsForced()));
43    return
44     (Task1->GetEndTm()<=Task2->GetStartTm())||
45     (Task2->GetEndTm()<=Task1->GetStartTm());
46  }
47  PSchTask TSchTask::LoadTxt(TILx& Lx){
48    PSchTask SchTask=PSchTask(new TSchTask());
49    Lx.GetVar("SchTask", true, true);
50    SchTask->TaskId=Lx.GetVarInt("TaskId");
51    Lx.GetVarStrVV("DimObjVV", SchTask->DimObjVV);
52    SchTask->StartTm=Lx.GetVarSecTm("StartTm");
53    SchTask->EndTm=Lx.GetVarSecTm("EndTm");
54    SchTask->Forced=Lx.GetVarBool("Forced");
55    Lx.GetVarEnd(true, true);
56    return SchTask;
57  }
58  void TSchTask::SaveTxt(TOLx& Lx) const {
59    Lx.PutVar("SchTask", true, true);
60    Lx.PutVarInt("TaskId", TaskId);
61    Lx.PutVarStrVV("DimObjVV", DimObjVV);
62    Lx.PutVarSecTm("StartTm", StartTm);
63    Lx.PutVarSecTm("EndTm", EndTm);
64    Lx.PutVarBool("Forced", Forced);
65    Lx.PutVarEnd(true, true);
66  }
67  void TSchObj::AddTask(const PSchTask& Task, const bool& OverlapCheckP){
68    IAssert(IsTaskOk(Task, OverlapCheckP));
69    if (Task->IsForced()){
70      FrcTaskV.Add(Task);
71    } else {
72      TTmTaskKd TmTaskKd(Task->GetStartTm(), Task);
73      int TaskN=TmTaskKdV.AddSorted(TmTaskKd);
74      if (0<TaskN){
75        PSchTask PrevTask=TmTaskKdV[TaskN-1].Dat;
76        if (OverlapCheckP){IAssert(TSchTask::IsCons(PrevTask, Task));}
77      }
78      if (TaskN+1<TmTaskKdV.Len()){
79        PSchTask NextTask=TmTaskKdV[TaskN+1].Dat;
80        if (OverlapCheckP){IAssert(TSchTask::IsCons(Task, NextTask));}
81      }
82    }
83  }
84  void TSchObj::DelTask(const PSchTask& Task){
85    if (Task->IsForced()){
86      int FrcTaskN=0;
87      while ((FrcTaskN<FrcTaskV.Len())&&
88       (FrcTaskV[FrcTaskN]->GetTaskId()!=Task->GetTaskId())){FrcTaskN++;}
89      IAssert(FrcTaskN<FrcTaskV.Len());
90      FrcTaskV.Del(FrcTaskN);
91    } else {
92      TTmTaskKd TmTaskKd(Task->GetStartTm(), Task);
93      int TaskN=TmTaskKdV.SearchBin(TmTaskKd);
94      IAssert(TaskN!=-1);
95      IAssert(Task->GetTaskId()==TmTaskKdV[TaskN].Dat->GetTaskId());
96      TmTaskKdV.Del(TaskN);
97    }
98  }
99  void TSchObj::GetTaskV(TSchTaskV& TaskV){
100    TaskV.Clr();
101    for (int TaskN=0; TaskN<GetTasks(); TaskN++){
102      TaskV.Add(GetTask(TaskN));}
103  }
104  int TSchObj::GetTasks() const {
105    return TmTaskKdV.Len()+FrcTaskV.Len();
106  }
107  PSchTask TSchObj::GetTask(const int& TaskN) const {
108    if (TaskN<TmTaskKdV.Len()){
109      return TmTaskKdV[TaskN].Dat;
110    } else {
111      return FrcTaskV[TaskN-TmTaskKdV.Len()];
112    }
113  }
114  bool TSchObj::IsRegTaskAtTm(const TSecTm& Tm, PSchTask& Task) const {
115    TTmTaskKd TmTaskKd(Tm);
116    int InsTaskN; int TaskN=TmTaskKdV.SearchBin(TmTaskKd, InsTaskN);
117    if (TaskN==-1){
118      if (InsTaskN==0){
119        Task=NULL; return false;
120      } else
121      if (TmTaskKdV[InsTaskN-1].Dat->IsTmIn(Tm)){
122        Task=TmTaskKdV[InsTaskN-1].Dat; return true;
123      } else {
124        Task=NULL; return false;
125      }
126    } else {
127      Task=TmTaskKdV[TaskN].Dat; return true;
128    }
129  }
130  bool TSchObj::IsRegTaskInTm(
131   const TSecTm& MnTm, const TSecTm& MxTm, PSchTask& Task) const {
132    TTmTaskKd StartTmTaskKd(MnTm);
133    TTmTaskKd EndTmTaskKd(MxTm);
134    int StartInsTaskN; int EndInsTaskN;
135    int StartTaskN=TmTaskKdV.SearchBin(StartTmTaskKd, StartInsTaskN);
136    int EndTaskN=TmTaskKdV.SearchBin(EndTmTaskKd, EndInsTaskN);
137    if ((StartTaskN==-1)&&(EndTaskN==-1)){
138      if (StartInsTaskN==EndInsTaskN){
139        if (StartInsTaskN==0){
140          Task=NULL; return false;
141        } else
142        if (TmTaskKdV[StartInsTaskN-1].Dat->IsTmIn(MnTm)){
143          Task=TmTaskKdV[StartInsTaskN-1].Dat; return true;
144        } else {
145          Task=NULL; return false;
146        }
147      } else {
148        Task=TmTaskKdV[StartInsTaskN].Dat; return true;
149      }
150    } else {
151      if (StartTaskN!=-1){IsRegTaskAtTm(MnTm, Task);}
152      else if (EndTaskN!=-1){IsRegTaskAtTm(MxTm, Task);}
153      else {Fail;}
154      IAssert(!Task.Empty());
155      return true;
156    }
157  }
158  bool TSchObj::IsFrcTaskInTm(
159   const TSecTm& MnTm, const TSecTm& MxTm, PSchTask& Task) const {
160    for (int FrcTaskN=0; FrcTaskN<FrcTaskV.Len(); FrcTaskN++){
161      if (FrcTaskV[FrcTaskN]->IsTmInts(MnTm, MxTm)){
162        Task=FrcTaskV[FrcTaskN]; return true;
163      }
164    }
165    return false;
166  }
167  bool TSchObj::IsTaskInTm(
168   const TSecTm& MnTm, const TSecTm& MxTm, PSchTask& Task) const {
169    return IsRegTaskInTm(MnTm, MxTm, Task) || IsFrcTaskInTm(MnTm, MxTm, Task);
170  }
171  PSchObj TSchObj::LoadTxt(TILx& Lx){
172    PSchObj SchObj=PSchObj(new TSchObj());
173    Lx.GetVar("SchObj", true, true);
174    SchObj->Nm=Lx.GetVarStr("Nm");
175    Lx.GetVar("TmTaskKdV", true, true);
176    while (!Lx.PeekVarEnd(true, true)){
177      PSchTask Task=TSchTask::LoadTxt(Lx);
178      SchObj->TmTaskKdV.Add(TTmTaskKd(Task->GetStartTm(), Task));
179    }
180    Lx.GetVarEnd(true, true);
181    Lx.GetVar("FrcTaskV", true, true);
182    while (!Lx.PeekVarEnd(true, true)){
183      PSchTask Task=TSchTask::LoadTxt(Lx);
184      SchObj->FrcTaskV.Add(Task);
185    }
186    Lx.GetVarEnd(true, true);
187    Lx.GetVarEnd(true, true);
188    return SchObj;
189  }
190  void TSchObj::SaveTxt(TOLx& Lx) const {
191    Lx.PutVar("SchObj", true, true);
192    Lx.PutVarStr("Nm", Nm);
193    Lx.PutVar("TmTaskKdV", true, true);
194    for (int TaskN=0; TaskN<TmTaskKdV.Len(); TaskN++){
195      TmTaskKdV[TaskN].Dat->SaveTxt(Lx);}
196    Lx.PutVarEnd(true, true);
197    Lx.PutVar("FrcTaskV", true, true);
198    for (int FrcTaskN=0; FrcTaskN<FrcTaskV.Len(); FrcTaskN++){
199      FrcTaskV[FrcTaskN]->SaveTxt(Lx);}
200    Lx.PutVarEnd(true, true);
201    Lx.PutVarEnd(true, true);
202  }
203  void TSchDim::AddObj(const TStr& ObjNm){
204    IAssert((!IsObj(ObjNm))&&(!IsCObj(ObjNm)));
205    PSchObj Obj=PSchObj(new TSchObj(ObjNm));
206    NmToObjH.AddDat(ObjNm, Obj);
207    NmToObjH.GetKeyId(ObjNm);
208  }
209  TStrV TSchDim::GetObjNmV() const {
210    TStrV ObjNmV(NmToObjH.Len(), 0);
211    int NmToObjP=NmToObjH.FFirstKeyId();
212    while (NmToObjH.FNextKeyId(NmToObjP)){
213      ObjNmV.Add(NmToObjH[NmToObjP]->GetNm());}
214    ObjNmV.Sort();
215    return ObjNmV;
216  }
217  int TSchDim::AddCObj(const TStr& CObjNm, const TStrV& ObjNmV){
218    IAssert((!IsObj(CObjNm))&&(!IsCObj(CObjNm)));
219    for (int ObjNmN=0; ObjNmN<ObjNmV.Len(); ObjNmN++){
220      IAssert(IsObj(ObjNmV[ObjNmN]));}
221    CObjNmToObjNmVH.AddDat(CObjNm, ObjNmV);
222    return NmToObjH.GetKeyId(CObjNm);
223  }
224  void TSchDim::GetCObjFromDesc(
225   const TStr& DescStr, TStr& CObjNm, TStrV& ObjNmV) const {
226    if (DescStr.IsChIn(':')){
227      TStr ObjNmVStr; DescStr.SplitOnCh(CObjNm, ':', ObjNmVStr);
228      CObjNm=CObjNm.GetTrunc(); ObjNmV.Clr();
229      TChA ObjNm;
230      for (int ChN=0; ChN<=ObjNmVStr.Len(); ChN++){
231        if ((ChN==ObjNmVStr.Len())||(ObjNmVStr[ChN]==',')){
232          ObjNm.Trunc();
233          if (!ObjNm.Empty()){ObjNmV.Add(ObjNm);}
234          ObjNm.Clr();
235        } else {
236          ObjNm+=ObjNmVStr[ChN];
237        }
238      }
239      ObjNmV.Sort();
240    } else {
241      CObjNm=DescStr.GetTrunc(); ObjNmV.Clr();
242    }
243  }
244  TStrV TSchDim::GetAllObjDescV() const {
245    TStrV DescStrV;
246    DescStrV.AddV(GetObjNmV());
247    TStrV CObjNmV=GetCObjNmV();
248    for (int CObjNmN=0; CObjNmN<CObjNmV.Len(); CObjNmN++){
249      TStrV ObjNmV=GetCObj_ObjNmV(CObjNmV[CObjNmN]);
250      TChA DescChA; DescChA+=CObjNmV[CObjNmN]; DescChA+=':';
251      for (int ObjNmN=0; ObjNmN<ObjNmV.Len(); ObjNmN++){
252        if (ObjNmN>0){DescChA+=", ";} DescChA+=ObjNmV[ObjNmN];}
253      DescStrV.Add(DescChA);
254    }
255    DescStrV.Sort();
256    return DescStrV;
257  }
258  void TSchDim::AddTask(const PSchTask& Task){
259    for (int DimObjN=0; DimObjN<Task->GetDimObjs(DimN); DimObjN++){
260      PSchObj Obj=GetObj(Task->GetDimObj(DimN, DimObjN));
261      Obj->AddTask(Task, OverlapCheckP);
262    }
263  }
264  void TSchDim::DelTask(const PSchTask& Task){
265    for (int DimObjN=0; DimObjN<Task->GetDimObjs(DimN); DimObjN++){
266      PSchObj Obj=GetObj(Task->GetDimObj(DimN, DimObjN));
267      Obj->DelTask(Task);
268    }
269  }
270  bool TSchDim::IsTaskOk(const PSchTask& Task, PSchTask& IcTask) const {
271    for (int DimObjN=0; DimObjN<Task->GetDimObjs(DimN); DimObjN++){
272      PSchObj Obj=GetObj(Task->GetDimObj(DimN, DimObjN));
273      if (!Obj->IsTaskOk(Task, IcTask, OverlapCheckP)){return false;}
274    }
275    return true;
276  }
277  bool TSchDim::IsTaskInTm(const TStr& ObjNm,
278   const TSecTm& MnTm, const TSecTm& MxTm, PSchTask& Task) const {
279    PSchObj Obj=GetObj(ObjNm);
280    return Obj->IsTaskInTm(MnTm, MxTm, Task);
281  }
282  PSchDim TSchDim::LoadTxt(TILx& Lx){
283    PSchDim SchDim=PSchDim(new TSchDim());
284    Lx.GetVar("SchDim", true, true);
285    SchDim->DimN=Lx.GetVarInt("DimN");
286    SchDim->Nm=Lx.GetVarStr("Nm");
287    SchDim->OverlapCheckP=Lx.GetVarBool("OverlapCheckP");
288    Lx.GetVar("NmToObjH", true, true);
289    int NmToObjH_Ports=Lx.GetVarInt("Ports");
290    SchDim->NmToObjH=TNmToObjH(NmToObjH_Ports);
291    while (!Lx.PeekVarEnd(true, true)){
292      PSchObj Obj=TSchObj::LoadTxt(Lx);
293      SchDim->NmToObjH.AddDat(Obj->GetNm(), Obj);
294    }
295    Lx.GetVarEnd(true, true);
296    Lx.GetVar("CObjNmToObjNmVH", true, true);
297    int CObjNmToObjNmVH_Ports=Lx.GetVarInt("Ports");
298    SchDim->CObjNmToObjNmVH=TStrStrVH(CObjNmToObjNmVH_Ports);
299    while (!Lx.PeekVarEnd(true, true)){
300      Lx.GetVar("CObj", true, true);
301      TStr CObjNm=Lx.GetVarStr("CObjNm");
302      TStrV ObjNmV; Lx.GetVarStrV("ObjNmV", ObjNmV);
303      SchDim->CObjNmToObjNmVH.AddDat(CObjNm, ObjNmV);
304      Lx.GetVarEnd(true, true);
305    }
306    Lx.GetVarEnd(true, true);
307    Lx.GetVarEnd(true, true);
308    return SchDim;
309  }
310  void TSchDim::SaveTxt(TOLx& Lx) const {
311    Lx.PutVar("SchDim", true, true);
312    Lx.PutVarInt("DimN", DimN);
313    Lx.PutVarStr("Nm", Nm);
314    Lx.PutVarBool("OverlapCheckP", OverlapCheckP);
315    Lx.PutVar("NmToObjH", true, true);
316    int NmToObjP=NmToObjH.FFirstKeyId();
317    Lx.PutVarInt("Ports", NmToObjH.GetPorts());
318    while (NmToObjH.FNextKeyId(NmToObjP)){
319      NmToObjH[NmToObjP]->SaveTxt(Lx);}
320    Lx.PutVarEnd(true, true);
321    Lx.PutVar("CObjNmToObjNmVH", true, true);
322    int CObjNmToObjNmVP=CObjNmToObjNmVH.FFirstKeyId();
323    Lx.PutVarInt("Ports", CObjNmToObjNmVH.GetPorts());
324    while (CObjNmToObjNmVH.FNextKeyId(CObjNmToObjNmVP)){
325      Lx.PutVar("CObj", true, true);
326      Lx.PutVarStr("CObjNm", CObjNmToObjNmVH.GetKey(CObjNmToObjNmVP));
327      Lx.PutVarStrV("ObjNmV", CObjNmToObjNmVH[CObjNmToObjNmVP]);
328      Lx.PutVarEnd(true, true);
329    }
330    Lx.PutVarEnd(true, true);
331    Lx.PutVarEnd(true, true);
332  }
333  PSchRep TSchRep::New(
334   const PSch& Sch, const TStr& DimNm, const TStr& ObjNm, const int& ResMins,
335   const bool& AllTmP, const TSecTm& StartDtTm, const TSecTm& EndDtTm){
336    PSchRep SchRep=PSchRep(new TSchRep());
337    TSecTm EndDayTm=TSecTm::GetZeroTm().AddDays(1);
338    {TSecTm CurDayTm=TSecTm::GetZeroTm();
339    while (CurDayTm<EndDayTm){
340      SchRep->DayTmV.Add(CurDayTm); CurDayTm.AddMins(ResMins);}}
341    TSecTm NrStartDtTm; TSecTm NrEndDtTm;
342    if (AllTmP){
343      NrStartDtTm=Sch->GetCal()->GetStartTm();
344      NrEndDtTm=Sch->GetCal()->GetEndTm();
345    } else {
346      NrStartDtTm=StartDtTm; NrEndDtTm=EndDtTm;
347    }
348    if (NrStartDtTm>NrEndDtTm){NrEndDtTm=NrStartDtTm;}
349    if (NrStartDtTm<Sch->GetCal()->GetStartTm()){
350      NrStartDtTm=Sch->GetCal()->GetStartTm();}
351    if (NrEndDtTm>Sch->GetCal()->GetEndTm()){
352      NrEndDtTm=Sch->GetCal()->GetEndTm();}
353    TSecTm CurDtTm=NrStartDtTm;
354    PSchTask PrevTask; int TaskN=0-1; int TaskSegN=-1;
355    while (CurDtTm<NrEndDtTm){
356      SchRep->DtTmV.Add(CurDtTm);
357      SchRep->TaskNmVV.Add();
358      TSecTm StartTaskTm=CurDtTm;
359      TSecTm EndTaskTm=TSecTm(StartTaskTm).AddMins(ResMins);
360      for (int DayTmN=0; DayTmN<SchRep->DayTmV.Len(); DayTmN++){
361        PSchTask Task;
362        if (Sch->IsTaskInTm(DimNm, ObjNm, StartTaskTm, EndTaskTm-1, Task)){
363          if (PrevTask==Task){TaskSegN++;} else {TaskN++; TaskSegN=1;}
364          TStr TaskStr=Task->GetStr()+" ("+TInt::GetStr(TaskSegN)+")";
365          SchRep->TaskNmVV.Last().Add(TaskStr);
366        } else {
367          SchRep->TaskNmVV.Last().Add();
368        }
369        PrevTask=Task;
370        StartTaskTm.AddMins(ResMins); EndTaskTm.AddMins(ResMins);
371      }
372      CurDtTm.AddDays(1);
373    }
374    return SchRep;
375  }
376  void TSchRep::SaveHtml(const PSOut& SOut) const {
377    int CurDtTmN=0;
378    while (CurDtTmN<GetDtTms()){
379      int MnDtTmN=CurDtTmN;
380      while ((CurDtTmN<GetDtTms())&&
381       (GetDtTm(CurDtTmN).GetDayOfWeekN()!=TTmInfo::SunN)){CurDtTmN++;}
382      int MxDtTmN=CurDtTmN;
383      CurDtTmN++;
384      SOut->PutStr("<table border=1 cellpadding=3>\n");
385      SOut->PutStr("<tr>");
386      SOut->PutStr("<td>");
387      SOut->PutStr("Time");
388      SOut->PutStr("</td>");
389      for (int DtTmN=MnDtTmN; DtTmN<MxDtTmN; DtTmN++){
390        SOut->PutStr("<td>");
391        SOut->PutStr(GetDtTmStr(DtTmN));
392        SOut->PutStr("</td>");
393      }
394      SOut->PutStr("</tr>"); SOut->PutLn();
395      for (int DayTmN=0; DayTmN<GetDayTms(); DayTmN++){
396        SOut->PutStr("</tr>");
397        SOut->PutStr("<td>");
398        SOut->PutStr(GetDayTmStr(DayTmN));
399        SOut->PutStr("</td>");
400        for (int DtTmN=MnDtTmN; DtTmN<MxDtTmN; DtTmN++){
401          SOut->PutStr("<td>");
402          if (GetTaskStr(DtTmN, DayTmN).Empty()){
403            SOut->PutStr("&nbsp;");
404          } else {
405            SOut->PutStr(GetTaskStr(DtTmN, DayTmN));
406          }
407          SOut->PutStr("</td>");
408        }
409        SOut->PutStr("</tr>"); SOut->PutLn();
410      }
411      SOut->PutStr("</table>\n");
412    }
413  }
414  void TSchRep::SaveHtml(const TStr& FNm) const {
415    PSOut SOut=TFOut::New(FNm);
416    SOut->PutStr("<html>\n");
417    SOut->PutStr("<head><title>Urnik</title></head>\n");
418    SOut->PutStr("<body>\n");
419    SaveHtml(SOut);
420    SOut->PutStr("</body>\n");
421    SOut->PutStr("</html>\n");
422  }
423  void TSchRep::SaveTxt(const PSOut& SOut) const {
424    SOut->PutStr("Time");
425    for (int DtTmN=0; DtTmN<GetDtTms(); DtTmN++){
426      SOut->PutCh(TabCh); SOut->PutStr(GetDtTmStr(DtTmN));}
427    SOut->PutLn();
428    for (int DayTmN=0; DayTmN<GetDayTms(); DayTmN++){
429      SOut->PutStr(GetDayTmStr(DayTmN));
430      for (int DtTmN=0; DtTmN<GetDtTms(); DtTmN++){
431        SOut->PutCh(TabCh); SOut->PutStr(GetTaskStr(DtTmN, DayTmN));}
432      SOut->PutLn();
433    }
434  }
435  void TSchRep::SaveTxt(const TStr& FNm) const {
436    PSOut SOut=TFOut::New(FNm);
437    SaveTxt(SOut);
438  }
439  void TSchRep::SaveStat(const TStr& FNm, const PSch& Sch){
440    PSOut SOut=TFOut::New(FNm);
441    TStr StartTmStr=Sch->GetCal()->GetStartTm().GetDtStr(lSi);
442    TStr EndTmStr=Sch->GetCal()->GetEndTm().GetDtStr(lSi);
443    SOut->PutStr("Zacetek: "); SOut->PutStr(StartTmStr); SOut->PutLn();
444    SOut->PutStr("Konec: "); SOut->PutStr(EndTmStr); SOut->PutLn();
445    for (int DimN=0; DimN<Sch->GetDims(); DimN++){
446      SOut->PutStr("----------------------"); SOut->PutLn();
<span onclick='openModal()' class='match'>447      SOut->PutStr(TStr("Dimenzija: ")+Sch->GetDimNm(DimN)); SOut->PutLn();
448      TStrV ObjNmV=Sch->GetObjNmV(DimN);
449      for (int ObjNmN=0; ObjNmN<ObjNmV.Len(); ObjNmN++){
</span>450        TStr ObjNm=ObjNmV[ObjNmN];
451        SOut->PutStr(ObjNm+": ");
452        TSchTaskV TaskV; Sch->GetObjTaskV(DimN, ObjNm, TaskV);
453        int ObjSecs=0;
454        for (int TaskN=0; TaskN<TaskV.Len(); TaskN++){
455          ObjSecs+=TaskV[TaskN]->GetTmSecs();}
456        SOut->PutStr(TInt::GetStr(ObjSecs/3600)+"h ");
457        SOut->PutStr(TInt::GetStr((ObjSecs%3600)/60)+"m ");
458        SOut->PutStr(TInt::GetStr((ObjSecs%3600*60)/60)+"s");
459        SOut->PutLn();
460      }
461    }
462  }
463  void TSchCal::PutScope(const TSchScope& _Scope,
464   const TSecTm& _StartTm, const TSecTm& _EndTm){
465    Scope=_Scope; StartTm=_StartTm; EndTm=_EndTm;
466    switch (Scope){
467      case ssUnlim: IAssert((!StartTm.IsDef())&&(!EndTm.IsDef())); break;
468      case ssLim: IAssert((StartTm.IsDef())&&(EndTm.IsDef())&&(StartTm<=EndTm)); break;
469      case ssWeek:
470        IAssert((!StartTm.IsDef())&&(!EndTm.IsDef()));
471        StartTm=EndTm=TSecTm::GetZeroWeekTm(); EndTm.AddDays(7); break;
472      default: Fail;
473    }
474  }
475  void TSchCal::AddBadDay(const TSecTm& DtTm, const TStr& DescStr){
476    TSecTm NrDtTm=TSecTm::GetDtTm(DtTm);
477    int BadDayN;
478    if (BadDay_DtTmDescStrKdV.IsIn(NrDtTm, BadDayN)){
479      BadDay_DtTmDescStrKdV.Del(BadDayN);}
480    BadDay_DtTmDescStrKdV.AddSorted(TSecTmStrKd(DtTm, DescStr));
481  }
482  void TSchCal::DelBadDay(const TSecTm& DtTm){
483    TSecTm NrDtTm=TSecTm::GetDtTm(DtTm);
484    int BadDayN;
485    if (BadDay_DtTmDescStrKdV.IsIn(NrDtTm, BadDayN)){
486      BadDay_DtTmDescStrKdV.Del(BadDayN);}
487  }
488  bool TSchCal::IsBadDay(const TSecTm& DtTm) const {
489    TSecTm NrDtTm=TSecTm::GetDtTm(DtTm);
490    return BadDay_DtTmDescStrKdV.IsIn(NrDtTm);
491  }
492  PSchCal TSchCal::LoadTxt(TILx& Lx){
493    PSchCal SchCal=TSchCal::New();
494    Lx.GetVar("SchCal", true, true);
495    SchCal->Scope=TSchScope(Lx.GetVarInt("Scope"));
496    SchCal->StartTm=Lx.GetVarSecTm("StartTm");
497    SchCal->EndTm=Lx.GetVarSecTm("EndTm");
498    Lx.GetVar("BadDay_DtTmDescStrKdV", true, true);
499    while (!Lx.PeekVarEnd(true, true)){
500      TSecTm DtTm=Lx.GetVarSecTm("DtTm");
501      TStr DescStr=Lx.GetVarStr("DescStr");
502      SchCal->BadDay_DtTmDescStrKdV.Add(TSecTmStrKd(DtTm, DescStr));
503    }
504    Lx.GetVarEnd(true, true);
505    Lx.GetVarEnd(true, true);
506    return SchCal;
507  }
508  void TSchCal::SaveTxt(TOLx& Lx) const {
509    Lx.PutVar("SchCal", true, true);
510    Lx.PutVarInt("Scope", int(Scope));
511    Lx.PutVarSecTm("StartTm", StartTm);
512    Lx.PutVarSecTm("EndTm", EndTm);
513    Lx.PutVar("BadDay_DtTmDescStrKdV", true, true);
514    for (int BadDayN=0; BadDayN<BadDay_DtTmDescStrKdV.Len(); BadDayN++){
515      Lx.PutVarSecTm("DtTm", BadDay_DtTmDescStrKdV[BadDayN].Key);
516      Lx.PutVarStr("DescStr", BadDay_DtTmDescStrKdV[BadDayN].Dat);
517    }
518    Lx.PutVarEnd(true, true);
519    Lx.PutVarEnd(true, true);
520  }
521  TSch::TSch(const TStr& _Nm, const int& Dims,
522   const int& ExpObjs, const int& ExpTasks):
523    Nm(_Nm), Cal(TSchCal::New()),
524    DimV(Dims), LastTaskId(0), CpbTask(),
525    IdToTaskH(ExpTasks), NmToPatH(10){
526    for (int DimN=0; DimN<Dims; DimN++){
527      DimV[DimN]=PSchDim(new TSchDim(DimN, ExpObjs));}
528  }
529  TStrV TSch::GetDimNmV() const {
530    TStrV DimNmV(GetDims(), 0);
531    for (int DimN=0; DimN<GetDims(); DimN++){
532      DimNmV.Add(GetDimNm(DimN));}
533    return DimNmV;
534  }
535  int TSch::GetDimN(const TStr& DimNm) const {
536    for (int DimN=0; DimN<GetDims(); DimN++){
537      if (GetDimNm(DimN)==DimNm){return DimN;}}
538    return -1;
539  }
540  void TSch::DelObj(const int& DimN, const TStr& ObjNm){
541    TSchTaskV TaskV; GetObjTaskV(DimN, ObjNm, TaskV);
542    for (int TaskN=0; TaskN<TaskV.Len(); TaskN++){
543      DelTask(TaskV[TaskN]->GetTaskId());}
544    DimV[DimN]->DelObj(ObjNm);
545    SyncPatVObj();
546  }
547  bool TSch::IsOkForAddTask(TStr& MsgStr) const {
548    if (GetDims()==0){MsgStr="Dimenzije niso definirane."; return false;}
549    for (int DimN=0; DimN<GetDims(); DimN++){
550      if (GetObjs(DimN)==0){
551        MsgStr="Dimenzija nima objektov."; return false;}
552    }
553    MsgStr="Ok."; return true;
554  }
555  int TSch::AddTask(const PSchTask& Task){
556    IAssert(Task->GetDims()==GetDims());
557    IdToTaskH.AddDat(Task->GetTaskId(), Task);
558    for (int DimN=0; DimN<GetDims(); DimN++){
559      DimV[DimN]->AddTask(Task);}
560    return Task->GetTaskId();
561  }
562  int TSch::AddTask(
563   const TVec<TStrV>& DimObjVV,
564   const TSecTm& StartTm, const TSecTm& EndTm, const bool& Forced){
565    int TaskId=GetNewTaskId();
566    PSchTask Task=PSchTask(new TSchTask(TaskId, DimObjVV, StartTm, EndTm, Forced));
567    return AddTask(Task);
568  }
569  int TSch::AddTask(
570   const TStr& DimObj1, const TStr& DimObj2, const TStr& DimObj3,
571   const TSecTm& StartTm, const TSecTm& EndTm, const bool& Forced){
572    TVec<TStrV> DimObjVV(3, 0);
573    DimObjVV.Add(); DimObjVV.Last().Add(DimObj1);
574    DimObjVV.Add(); DimObjVV.Last().Add(DimObj2);
575    DimObjVV.Add(); DimObjVV.Last().Add(DimObj3);
576    return AddTask(DimObjVV, StartTm, EndTm, Forced);
577  }
578  int TSch::AddTask(
579   const TStrV& DimObjV,
580   const TSecTm& StartTm, const TSecTm& EndTm, const bool& Forced){
581    TVec<TStrV> DimObjVV(3, 0);
582    DimObjVV.Add(); DimObjVV.Last().Add(DimObjV[0]);
583    DimObjVV.Add(); DimObjVV.Last().Add(DimObjV[1]);
584    DimObjVV.Add(); DimObjVV.Last().Add(DimObjV[2]);
585    return AddTask(DimObjVV, StartTm, EndTm, Forced);
586  }
587  int TSch::AddTask(
588   const TStrV& DimObjV1, const TStrV& DimObjV2, const TStrV& DimObjV3,
589   const TSecTm& StartTm, const TSecTm& EndTm, const bool& Forced){
590    TVec<TStrV> DimObjVV(3, 0);
591    DimObjVV.Add(DimObjV1);
592    DimObjVV.Add(DimObjV2);
593    DimObjVV.Add(DimObjV3);
594    return AddTask(DimObjVV, StartTm, EndTm, Forced);
595  }
596  void TSch::DelTask(const int& TaskId){
597    PSchTask Task=IdToTaskH.GetDat(TaskId);
598    for (int DimN=0; DimN<GetDims(); DimN++){
599      DimV[DimN]->DelTask(Task);}
600    IdToTaskH.DelKey(TaskId);
601  }
602  void TSch::GetDayTaskV(const TSecTm& DtTm, TIntV& TaskIdV) const {
603    TaskIdV.Clr();
604    int IdToTaskP=IdToTaskH.FFirstKeyId();
605    while (IdToTaskH.FNextKeyId(IdToTaskP)){
606      PSchTask Task=IdToTaskH[IdToTaskP];
607      TSecTm TaskDtTm=TSecTm::GetDtTm(Task->GetStartTm());
608      if (TaskDtTm==DtTm){
609        TaskIdV.Add(Task->GetTaskId());}
610    }
611  }
612  bool TSch::IsTask(const int& TaskId, PSchTask& Task) const {
613    int IdToTaskP;
614    if (IdToTaskH.IsKey(TaskId, IdToTaskP)){
615      Task=IdToTaskH[IdToTaskP]; return true;}
616    else {return false;}
617  }
618  bool TSch::IsTaskOk(const PSchTask& Task, TSchTaskV& DimIcTaskV) const {
619    DimIcTaskV.Gen(GetDims(), GetDims());
620    bool Ok=true;
621    for (int DimN=0; DimN<GetDims(); DimN++){
622      PSchTask IcTask;
623      if (!DimV[DimN]->IsTaskOk(Task, IcTask)){
624        DimIcTaskV[DimN]=IcTask; Ok=false;
625      }
626    }
627    return Ok;
628  }
629  bool TSch::IsTaskSim(const PSchTask& Task, PSchTask& SimTask) const {
630    TVec<TStrV> DimObjVV; Task->GetDimObjVV(DimObjVV);
631    int IdToTaskP=IdToTaskH.FFirstKeyId();
632    while (IdToTaskH.FNextKeyId(IdToTaskP)){
633      SimTask=IdToTaskH[IdToTaskP];
634      if (
635       (Task->GetStartTm()==SimTask->GetStartTm())&&
636       (Task->GetEndTm()==SimTask->GetEndTm())&&
637       (Task->IsForced()==SimTask->IsForced())){
638        TVec<TStrV> SimDimObjVV; SimTask->GetDimObjVV(SimDimObjVV);
639        if (DimObjVV==SimDimObjVV){return true;}
640      }
641    }
642    return false;
643  }
644  PSch TSch::AddPat(const TStr& PatNm){
645    IAssert(!IsPat(PatNm));
646    PSch Pat=New(PatNm, GetDims());
647    Pat->PutNm(PatNm);
648    Pat->GetCal()->PutScope(ssWeek);
649    for (int DimN=0; DimN<GetDims(); DimN++){
650      Pat->PutDimNm(DimN, GetDimNm(DimN));
651      Pat->AddObjV(DimN, GetObjNmV(DimN));
652      TStrV CObjNmV=GetCObjNmV(DimN);
653      for (int CObjNmN=0; CObjNmN<CObjNmV.Len(); CObjNmN++){
654        TStr CObjNm=CObjNmV[CObjNmN];
655        TStrV ObjNmV=GetCObj_ObjNmV(DimN, CObjNm);
656        Pat->AddCObj(DimN, CObjNm, ObjNmV);
657      }
658    }
659    NmToPatH.AddDat(PatNm, Pat);
660    return Pat;
661  }
662  void TSch::SyncPatObj(const TStr& PatNm) const {
663    PSch Pat=GetPat(PatNm);
664    for (int DimN=0; DimN<GetDims(); DimN++){
665      {TStrV ObjNmV=GetObjNmV(DimN);
666      for (int ObjNmN=0; ObjNmN<ObjNmV.Len(); ObjNmN++){
667        if (!Pat->IsObj(DimN, ObjNmV[ObjNmN])){
668          Pat->AddObj(DimN, ObjNmV[ObjNmN]);}
669      }}
670      {TStrV CObjNmV=GetCObjNmV(DimN);
671      for (int CObjNmN=0; CObjNmN<CObjNmV.Len(); CObjNmN++){
672        TStrV ObjNmV=GetCObj_ObjNmV(DimN, CObjNmV[CObjNmN]);
673        if (!Pat->IsCObj(DimN, CObjNmV[CObjNmN])){
674          Pat->AddCObj(DimN, CObjNmV[CObjNmN], ObjNmV);
675        } else {
676          TStrV PatObjNmV=Pat->GetCObj_ObjNmV(DimN, CObjNmV[CObjNmN]);
677          if (ObjNmV!=PatObjNmV){
678            Pat->DelCObj(DimN, CObjNmV[CObjNmN]);
679            Pat->AddCObj(DimN, CObjNmV[CObjNmN], ObjNmV);
680          }
681        }
682      }}
683      {TStrV PatObjNmV=Pat->GetObjNmV(DimN);
684      for (int PatObjNmN=0; PatObjNmN<PatObjNmV.Len(); PatObjNmN++){
685        if (!IsObj(DimN, PatObjNmV[PatObjNmN])){
686          Pat->DelObj(DimN, PatObjNmV[PatObjNmN]);}
687      }}
688      {TStrV PatCObjNmV=Pat->GetCObjNmV(DimN);
689      for (int PatCObjNmN=0; PatCObjNmN<PatCObjNmV.Len(); PatCObjNmN++){
690        TStrV PatObjNmV=Pat->GetCObj_ObjNmV(DimN, PatCObjNmV[PatCObjNmN]);
691        if (!IsCObj(DimN, PatCObjNmV[PatCObjNmN])){
692          Pat->DelCObj(DimN, PatCObjNmV[PatCObjNmN]);
693        }
694      }}
695    }
696  }
697  void TSch::SyncPatVObj() const {
698    TStrV PatNmV=GetPatNmV();
699    for (int PatNmN=0; PatNmN<PatNmV.Len(); PatNmN++){
700      SyncPatObj(PatNmV[PatNmN]);}
701  }
702  void TSch::ApplyPatAdd(
703   const PSch& Pat, const TSecTm& StartDtTm, const TSecTm& EndDtTm,
704   const bool& TestOnlyP, TStrV& LogStrV){
705    LogStrV.Clr();
706    LogStrV.Add(TStr("Adding pattern: ")+Pat->GetNm());
707    LogStrV.Add(TStr("Start date: ")+StartDtTm.GetDtStr());
708    LogStrV.Add(TStr("End date: ")+EndDtTm.GetDtStr());
709    LogStrV.Add(TStr("Test only: ")+TBool::GetYesNoStr(TestOnlyP));
710    LogStrV.Add("------------------------");
711    TSecTm CurSchDtTm=TSecTm::GetDtTm(StartDtTm);
712    if (CurSchDtTm<GetCal()->GetStartTm()){CurSchDtTm=GetCal()->GetStartTm();}
713    TSecTm CurPatDtTm=TSecTm::GetDtTm(Pat->GetCal()->GetStartTm());
714    while (CurPatDtTm.GetDayOfWeekN()!=CurSchDtTm.GetDayOfWeekN()){
715      CurPatDtTm.AddDays(1);}
716    while ((CurSchDtTm<GetCal()->GetStartTm())&&(CurSchDtTm<=EndDtTm)){
717      TIntV PatTaskIdV; Pat->GetDayTaskV(CurPatDtTm, PatTaskIdV);
718      for (int PatTaskIdN=0; PatTaskIdN<PatTaskIdV.Len(); PatTaskIdN++){
719        PSchTask PatTask=Pat->GetTask(PatTaskIdV[PatTaskIdN]);
720        TSecTm SchTaskStartTm=
721         CurSchDtTm+PatTask->GetStartTm()-TSecTm::GetDtTm(PatTask->GetStartTm());
722        TSecTm SchTaskEndTm=
723         CurSchDtTm+PatTask->GetEndTm()-TSecTm::GetDtTm(PatTask->GetEndTm());
724        int SchTaskId=GetNewTaskId();
725        TVec<TStrV> SchTaskDimObjVV; PatTask->GetDimObjVV(SchTaskDimObjVV);
726        bool SchTaskForced=PatTask->IsForced();
727        PSchTask SchTask=PSchTask(new TSchTask(SchTaskId,
728         SchTaskDimObjVV, SchTaskStartTm, SchTaskEndTm, SchTaskForced));
729        if (IsTaskOk(SchTask)){
730          if (!TestOnlyP){
731            AddTask(SchTask);} 
732          TChA LogChA;
733          LogChA+="Add ";
734          LogChA+='['; LogChA+=SchTaskStartTm.GetStr(); LogChA+=']';
735          LogChA+=" - ";
736          LogChA+='['; LogChA+=SchTaskEndTm.GetStr(); LogChA+=']'; LogChA+="  ";
737          LogChA+=SchTask->GetStr();
738          LogStrV.Add(LogChA);
739        }
740      }
741      CurSchDtTm.AddDays(1);
742      CurPatDtTm.AddDays(1);
743      if (CurPatDtTm>=Pat->GetCal()->GetEndTm()){
744        CurPatDtTm=Pat->GetCal()->GetStartTm();}
745    }
746  }
747  void TSch::ApplyPatDel(
748   const PSch& Pat, const TSecTm& StartDtTm, const TSecTm& EndDtTm,
749   const bool& TestOnlyP, TStrV& LogStrV){
750    LogStrV.Clr();
751    LogStrV.Add(TStr("Deleting pattern: ")+Pat->GetNm());
752    LogStrV.Add(TStr("Start date: ")+StartDtTm.GetDtStr());
753    LogStrV.Add(TStr("End date: ")+EndDtTm.GetDtStr());
754    LogStrV.Add(TStr("Test only: ")+TBool::GetYesNoStr(TestOnlyP));
755    LogStrV.Add("------------------------");
756    TSecTm CurSchDtTm=TSecTm::GetDtTm(StartDtTm);
757    if (CurSchDtTm<GetCal()->GetStartTm()){CurSchDtTm=GetCal()->GetStartTm();}
758    TSecTm CurPatDtTm=TSecTm::GetDtTm(Pat->GetCal()->GetStartTm());
759    while (CurPatDtTm.GetDayOfWeekN()!=CurSchDtTm.GetDayOfWeekN()){
760      CurPatDtTm.AddDays(1);}
761    while ((CurSchDtTm<GetCal()->GetEndTm())&&(CurSchDtTm<=EndDtTm)){
762      TIntV PatTaskIdV; Pat->GetDayTaskV(CurPatDtTm, PatTaskIdV);
763      for (int PatTaskIdN=0; PatTaskIdN<PatTaskIdV.Len(); PatTaskIdN++){
764        PSchTask PatTask=Pat->GetTask(PatTaskIdV[PatTaskIdN]);
765        TSecTm SchTaskStartTm=
766         CurSchDtTm+PatTask->GetStartTm()-TSecTm::GetDtTm(PatTask->GetStartTm());
767        TSecTm SchTaskEndTm=
768         CurSchDtTm+PatTask->GetEndTm()-TSecTm::GetDtTm(PatTask->GetEndTm());
769        int SchTaskId=GetNewTaskId();
770        TVec<TStrV> SchTaskDimObjVV; PatTask->GetDimObjVV(SchTaskDimObjVV);
771        bool SchTaskForced=PatTask->IsForced();
772        PSchTask SchTask=PSchTask(new TSchTask(SchTaskId,
773         SchTaskDimObjVV, SchTaskStartTm, SchTaskEndTm, SchTaskForced));
774        PSchTask SimSchTask;
775        if (IsTaskSim(SchTask, SimSchTask)){
776          if (!TestOnlyP){
777            DelTask(SimSchTask->GetTaskId()); 
778          }
779          TChA LogChA;
780          LogChA+="Del ";
781          LogChA+='['; LogChA+=SchTaskStartTm.GetStr(); LogChA+=']';
782          LogChA+=" - ";
783          LogChA+='['; LogChA+=SchTaskEndTm.GetStr(); LogChA+=']'; LogChA+="  ";
784          LogChA+=SchTask->GetStr();
785          LogStrV.Add(LogChA);
786        }
787      }
788      CurSchDtTm.AddDays(1);
789      CurPatDtTm.AddDays(1);
790      if (CurPatDtTm>=Pat->GetCal()->GetEndTm()){
791        CurPatDtTm=Pat->GetCal()->GetStartTm();}
792    }
793  }
794  PSch TSch::LoadTxt(TILx& Lx){
795    PSch Sch=PSch(new TSch());
796    Lx.GetVar("Sch", true, true);
797    Sch->Nm=Lx.GetVarStr("Nm");
798    Sch->Cal=TSchCal::LoadTxt(Lx);
799    Lx.GetVar("DimV", true, true);
800    while (!Lx.PeekVarEnd(true, true)){
801      Sch->DimV.Add(TSchDim::LoadTxt(Lx));}
802    Lx.GetVarEnd(true, true);
803    Sch->LastTaskId=Lx.GetVarInt("LastTaskId");
804    {Lx.GetVar("IdToTaskH", true, true);
805    int Ports=Lx.GetVarInt("Ports");
806    Sch->IdToTaskH=TIdToTaskH(Ports);
807    while (!Lx.PeekVarEnd(true, true)){
808      PSchTask Task=TSchTask::LoadTxt(Lx);
809      Sch->IdToTaskH.AddDat(Task->GetTaskId(), Task);
810    }
811    Lx.GetVarEnd(true, true);}
812    {Lx.GetVar("NmToPatH", true, true);
813    int Ports=Lx.GetVarInt("Ports");
814    Sch->NmToPatH=TNmToPatH(Ports);
815    while (!Lx.PeekVarEnd(true, true)){
816      PSch Pat=TSch::LoadTxt(Lx);
817      Sch->NmToPatH.AddDat(Pat->GetNm(), Pat);
818    }
819    Lx.GetVarEnd(true, true);}
820    Lx.GetVarEnd(true, true);
821    return Sch;
822  }
823  PSch TSch::LoadTxt(const TStr& FNm){
824    PSIn SIn=PSIn(new TFIn(FNm));
825    TILx Lx(SIn, TFSet()|iloRetEoln|iloSigNum|iloCsSens);
826    return LoadTxt(Lx);
827  }
828  void TSch::SaveTxt(TOLx& Lx) const {
829    Lx.PutVar("Sch", true, true);
830    Lx.PutVarStr("Nm", Nm);
831    Cal->SaveTxt(Lx);
832    Lx.PutVar("DimV", true, true);
833    for (int DimN=0; DimN<DimV.Len(); DimN++){
834      DimV[DimN]->SaveTxt(Lx);}
835    Lx.PutVarEnd(true, true);
836    Lx.PutVarInt("LastTaskId", LastTaskId);
837    Lx.PutVar("IdToTaskH", true, true);
838    Lx.PutVarInt("Ports", IdToTaskH.GetPorts());
839    int IdToTaskP=IdToTaskH.FFirstKeyId();
840    while (IdToTaskH.FNextKeyId(IdToTaskP)){
841      IdToTaskH[IdToTaskP]->SaveTxt(Lx);}
842    Lx.PutVarEnd(true, true);
843    Lx.PutVar("NmToPatH", true, true);
844    Lx.PutVarInt("Ports", NmToPatH.GetPorts());
845    int NmToPatP=NmToPatH.FFirstKeyId();
846    while (NmToPatH.FNextKeyId(NmToPatP)){
847      NmToPatH[NmToPatP]->SaveTxt(Lx);}
848    Lx.PutVarEnd(true, true);
849    Lx.PutVarEnd(true, true);
850  }
851  void TSch::SaveTxt(const TStr& FNm) const {
852    PSOut SOut=PSOut(new TFOut(FNm));
853    TOLx Lx(SOut, TFSet()|oloFrcEoln|oloSigNum|oloCsSens|oloVarIndent);
854    SaveTxt(Lx);
855  }
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_mkldnn_convolution_layer.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-sch.cpp</div>
                </div>
                <div class="column column_space"><pre><code>713    const Dtype* top_data = this->blob_top_->cpu_data();
714    const Dtype* sep_top_data = this->blob_top_2_->cpu_data();
715    for (int i = 0; i < this->blob_top_->count(); ++i) {
</pre></code></div>
                <div class="column column_space"><pre><code>447      SOut->PutStr(TStr("Dimenzija: ")+Sch->GetDimNm(DimN)); SOut->PutLn();
448      TStrV ObjNmV=Sch->GetObjNmV(DimN);
449      for (int ObjNmN=0; ObjNmN<ObjNmV.Len(); ObjNmN++){
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    