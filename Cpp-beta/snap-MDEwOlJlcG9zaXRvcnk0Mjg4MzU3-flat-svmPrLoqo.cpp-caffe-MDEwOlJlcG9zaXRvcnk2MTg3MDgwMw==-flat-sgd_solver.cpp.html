
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 8.34326579261025%, Tokens: 9, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-svmPrLoqo.cpp</h3>
            <pre><code>1  #define PREDICTOR 1
2  #define CORRECTOR 2
3  void TPrLoqo::nrerror(char error_text[]) {
4      printf("ERROR: terminating optimizer - %s\n", error_text);
5  }
6  void TPrLoqo::choldc(double a[], int n, double p[]) {
7      int i, j, k;
8      double sum;
9      for (i = 0; i < n; i++){
10          for (j = i; j < n; j++) {
11              sum=a[n*i + j];
12              for (k=i-1; k>=0; k--) sum -= a[n*i + k]*a[n*j + k];
13              if (i == j) {
14                  if (sum <= 0.0) {
15                      nrerror("choldc failed, matrix not positive definite");
16                      sum = 0.0;
17                  }
18                  p[i]=sqrt(sum);
19              } else a[n*j + i] = sum/p[i];
20          }
21      }
22  }
<span onclick='openModal()' class='match'>23  void TPrLoqo::cholsb(double a[], int n, double p[], double b[], double x[]) {
24      int i, k;
25      double sum;
26      for (i=0; i<n; i++) {
</span>27          sum=b[i];
28          for (k=i-1; k>=0; k--) sum -= a[n*i + k]*x[k];
29          x[i]=sum/p[i];
30      }
31      for (i=n-1; i>=0; i--) {
32          sum=x[i];
33          for (k=i+1; k<n; k++) sum -= a[n*k + i]*x[k];
34          x[i]=sum/p[i];
35      }
36  }
37  void TPrLoqo::chol_forward(double a[], int n, double p[], double b[], double x[])
38  {
39      int i, k;
40      double sum;
41      for (i=0; i<n; i++) {
42          sum=b[i];
43          for (k=i-1; k>=0; k--) sum -= a[n*i + k]*x[k];
44          x[i]=sum/p[i];
45      }
46  }
47  void TPrLoqo::chol_backward(double a[], int n, double p[], double b[], double x[])
48  {
49      int i, k;
50      double sum;
51      for (i=n-1; i>=0; i--) {
52          sum=b[i];
53          for (k=i+1; k<n; k++) sum -= a[n*k + i]*x[k];
54          x[i]=sum/p[i];
55      }
56  }
57  void TPrLoqo::solve_reduced(int n, int m, double h_x[], double h_y[], 
58                              double a[], double x_x[], double x_y[],
59                              double c_x[], double c_y[],
60                              double workspace[], int step)
61  {
62      int i,j,k;
63      double *p_x;
64      double *p_y;
65      double *t_a;
66      double *t_c;
67      double *t_y;
68      p_x = workspace;		&bsol;* together n + m + n*m + n + m = n*(m+2)+2*m */
69      p_y = p_x + n;
70      t_a = p_y + m;
71      t_c = t_a + n*m;
72      t_y = t_c + n;
73      if (step == PREDICTOR) {
74          choldc(h_x, n, p_x);	&bsol;* do cholesky decomposition */
75          for (i=0; i<m; i++)         &bsol;* forward pass for A' */
76              chol_forward(h_x, n, p_x, a+i*n, t_a+i*n);
77          for (i=0; i<m; i++)         &bsol;* compute (h_y + a h_x^-1A') */
78              for (j=i; j<m; j++)
79                  for (k=0; k<n; k++) 
80                      h_y[m*i + j] += t_a[n*j + k] * t_a[n*i + k];
81          choldc(h_y, m, p_y);	&bsol;* and cholesky decomposition */
82      }
83      chol_forward(h_x, n, p_x, c_x, t_c);
84      for (i=0; i<m; i++) {		&bsol;* and solve for x_y */
85          t_y[i] = c_y[i];
86          for (j=0; j<n; j++)
87              t_y[i] += t_a[i*n + j] * t_c[j];
88      }
89      cholsb(h_y, m, p_y, t_y, x_y);
90      for (i=0; i<n; i++) {		&bsol;* finally solve for x_x */
91          t_c[i] = -t_c[i];
92          for (j=0; j<m; j++)
93              t_c[i] += t_a[j*n + i] * x_y[j];
94      }
95      chol_backward(h_x, n, p_x, t_c, x_x);
96  }
97  void TPrLoqo::matrix_vector(int n, double m[], double x[], double y[])
98  {
99      int i, j;
100      for (i=0; i<n; i++) {
101          y[i] = m[(n+1) * i] * x[i];
102          for (j=0; j<i; j++)
103              y[i] += m[i + n*j] * x[j];
104          for (j=i+1; j<n; j++) 
105              y[i] += m[n*i + j] * x[j]; 
106      }
107  }
108  int TPrLoqo::pr_loqo(int n, int m, double c[], double h_x[], double a[], double b[],
109                       double l[], double u[], double primal[], double dual[], 
110                       int verb, double sigfig_max, int counter_max, 
111                       double margin, double bound, int restart)
112  {
113      double *workspace;
114      double *diag_h_x;
115      double *h_y;
116      double *c_x;
117      double *c_y;
118      double *h_dot_x;
119      double *rho;
120      double *nu;
121      double *tau;
122      double *sigma;
123      double *gamma_z;
124      double *gamma_s;  
125      double *hat_nu;
126      double *hat_tau;
127      double *delta_x;
128      double *delta_y;
129      double *delta_s;
130      double *delta_z;
131      double *delta_g;
132      double *delta_t;
133      double *d;
134      double *x;
135      double *y;
136      double *g;
137      double *z;
138      double *s;
139      double *t;  
140      double b_plus_1;
141      double c_plus_1;
142      double x_h_x;
143      double primal_inf;
144      double dual_inf;
145      double sigfig;
146      double primal_obj, dual_obj;
147      double mu;
148      double alfa;
149      int counter = 0;
150      int status = STILL_RUNNING;
151      int i,j;
152      workspace = (double *)malloc((n*(m+2)+2*m)*sizeof(double));
153      diag_h_x  = (double *)malloc(n*sizeof(double));
154      h_y       = (double *)malloc(m*m*sizeof(double));
155      c_x       = (double *)malloc(n*sizeof(double));
156      c_y       = (double *)malloc(m*sizeof(double));
157      h_dot_x   = (double *)malloc(n*sizeof(double));
158      rho       = (double *)malloc(m*sizeof(double));
159      nu        = (double *)malloc(n*sizeof(double));
160      tau       = (double *)malloc(n*sizeof(double));
161      sigma     = (double *)malloc(n*sizeof(double));
162      gamma_z   = (double *)malloc(n*sizeof(double));
163      gamma_s   = (double *)malloc(n*sizeof(double));
164      hat_nu    = (double *)malloc(n*sizeof(double));
165      hat_tau   = (double *)malloc(n*sizeof(double));
166      delta_x   = (double *)malloc(n*sizeof(double));
167      delta_y   = (double *)malloc(m*sizeof(double));
168      delta_s   = (double *)malloc(n*sizeof(double));
169      delta_z   = (double *)malloc(n*sizeof(double));
170      delta_g   = (double *)malloc(n*sizeof(double));
171      delta_t   = (double *)malloc(n*sizeof(double));
172      d         = (double *)malloc(n*sizeof(double));
173      x = primal;			&bsol;* n */
174      g = x + n;			&bsol;* n */
175      t = g + n;			&bsol;* n */
176      y = dual;			&bsol;* m */
177      z = y + m;			&bsol;* n */
178      s = z + n;			&bsol;* n */
179      b_plus_1 = 1;
180      c_plus_1 = 0;
181      for (i=0; i<n; i++) c_plus_1 += c[i];
182      for (i=0; i<n; i++) diag_h_x[i] = h_x[(n+1)*i]; 
183      if (restart == 1) {
184          for (i=0; i<n; i++) {	&bsol;* compute g, t for primal feasibility */
185              g[i] = mx(fabs(x[i] - l[i]), bound);
186              t[i] = mx(fabs(u[i] - x[i]), bound); 
187          }
188          matrix_vector(n, h_x, x, h_dot_x); &bsol;* h_dot_x = h_x * x */
189          for (i=0; i<n; i++) {	&bsol;* sigma is a dummy variable to calculate z, s */
190              sigma[i] = c[i] + h_dot_x[i];
191              for (j=0; j<m; j++)
192                  sigma[i] -= a[n*j + i] * y[j];
193              if (sigma[i] > 0) {
194                  s[i] = bound;
195                  z[i] = sigma[i] + bound;
196              }
197              else {
198                  s[i] = bound - sigma[i];
199                  z[i] = bound;
200              }
201          }
202      }
203      else {			&bsol;* use default start settings */
204          for (i=0; i<m; i++)
205              for (j=i; j<m; j++)
206                  h_y[i*m + j] = (i==j) ? 1 : 0;
207          for (i=0; i<n; i++) {
208              c_x[i] = c[i];
209              h_x[(n+1)*i] += 1;
210          }
211          for (i=0; i<m; i++)
212              c_y[i] = b[i];
213          solve_reduced(n, m, h_x, h_y, a, x, y, c_x, c_y, workspace,
214              PREDICTOR);
215          for (i=0; i<n; i++) {
216              g[i] = mx(fabs(x[i] - l[i]), bound);
217              z[i] = mx(fabs(x[i]), bound);
218              t[i] = mx(fabs(u[i] - x[i]), bound); 
219              s[i] = mx(fabs(x[i]), bound); 
220          }
221      }
222      for (i=0, mu=0; i<n; i++)
223          mu += z[i] * g[i] + s[i] * t[i];
224      mu = mu / (2*n);
225      if (verb >= STATUS) {
226          printf("counter | pri_inf  | dual_inf  | pri_obj   | dual_obj  | ");
227          printf("sigfig | alpha  | nu \n");
228          printf("-------------------------------------------------------");
229          printf("---------------------------\n");
230      }
231      while (status == STILL_RUNNING) {
232          for (i=0; i<n; i++) 
233              h_x[(n+1) * i] = diag_h_x[i];
234          matrix_vector(n, h_x, x, h_dot_x); &bsol;* compute h_dot_x = h_x * x */
235          for (i=0; i<m; i++) {
236              rho[i] = b[i];
237              for (j=0; j<n; j++)
238                  rho[i] -= a[n*i + j] * x[j];
239          }
240          for (i=0; i<n; i++) {
241              nu[i] = l[i] - x[i] + g[i];
242              tau[i] = u[i] - x[i] - t[i];
243              sigma[i] = c[i] - z[i] + s[i] + h_dot_x[i];
244              for (j=0; j<m; j++)
245                  sigma[i] -= a[n*j + i] * y[j];
246              gamma_z[i] = - z[i];
247              gamma_s[i] = - s[i];
248          }
249          x_h_x = 0;
250          primal_inf = 0;
251          dual_inf = 0;
252          for (i=0; i<n; i++) {
253              x_h_x += h_dot_x[i] * x[i];
254              primal_inf += sqr(tau[i]);
255              primal_inf += sqr(nu[i]);
256              dual_inf += sqr(sigma[i]);
257          }
258          for (i=0; i<m; i++) 
259              primal_inf += sqr(rho[i]);
260          primal_inf = sqrt(primal_inf)/b_plus_1;
261          dual_inf = sqrt(dual_inf)/c_plus_1;
262          primal_obj = 0.5 * x_h_x;
263          dual_obj = -0.5 * x_h_x;
264          for (i=0; i<n; i++) {
265              primal_obj += c[i] * x[i];
266              dual_obj += l[i] * z[i] - u[i] * s[i];
267          }
268          for (i=0; i<m; i++)
269              dual_obj += b[i] * y[i];
270          sigfig = log10(fabs(primal_obj) + 1) -
271              log10(fabs(primal_obj - dual_obj));
272          sigfig = mx(sigfig, 0);
273          if (counter > counter_max) status = ITERATION_LIMIT;
274          if (sigfig  > sigfig_max)  status = OPTIMAL_SOLUTION;
275          if (primal_inf > 10e100)   status = PRIMAL_INFEASIBLE;
276          if (dual_inf > 10e100)     status = DUAL_INFEASIBLE;
277          if ((primal_inf > 10e100) & (dual_inf > 10e100)) status = PRIMAL_AND_DUAL_INFEASIBLE;
278          if (fabs(primal_obj) > 10e100) status = PRIMAL_UNBOUNDED;
279          if (fabs(dual_obj) > 10e100) status = DUAL_UNBOUNDED;
280          if ((verb >= FLOOD) | ((verb == STATUS) & (status != 0)))
281              printf("%7i | %.2e | %.2e | % .2e | % .2e | %6.3f | %.4f | %.2e\n",
282              counter, primal_inf, dual_inf, primal_obj, dual_obj,
283              sigfig, alfa, mu);
284          counter++;
285          if (status == 0) {		&bsol;* we may keep on going, otherwise
286                                  it'll cost one loop extra plus a
287                                  messed up main diagonal of h_x */
288              for (i=0; i<n; i++) {
289                  hat_nu[i] = nu[i] + g[i] * gamma_z[i] / z[i];
290                  hat_tau[i] = tau[i] - t[i] * gamma_s[i] / s[i];
291                  d[i] = z[i] / g[i] + s[i] / t[i];
292              }
293              for (i=0; i<n; i++) {
294                  h_x[(n+1)*i] = diag_h_x[i] + d[i];
295                  c_x[i] = sigma[i] - z[i] * hat_nu[i] / g[i] - 
296                      s[i] * hat_tau[i] / t[i];
297              }
298              for (i=0; i<m; i++) {
299                  c_y[i] = rho[i];
300                  for (j=i; j<m; j++) 
301                      h_y[m*i + j] = 0;
302              }
303              solve_reduced(n, m, h_x, h_y, a, delta_x, delta_y, c_x, c_y, workspace,
304                  PREDICTOR);
305              for (i=0; i<n; i++) {
306                  delta_s[i] = s[i] * (delta_x[i] - hat_tau[i]) / t[i];
307                  delta_z[i] = z[i] * (hat_nu[i] - delta_x[i]) / g[i];
308                  delta_g[i] = g[i] * (gamma_z[i] - delta_z[i]) / z[i];
309                  delta_t[i] = t[i] * (gamma_s[i] - delta_s[i]) / s[i];
310                  gamma_z[i] = mu / g[i] - z[i] - delta_z[i] * delta_g[i] / g[i];
311                  gamma_s[i] = mu / t[i] - s[i] - delta_s[i] * delta_t[i] / t[i];
312                  hat_nu[i] = nu[i] + g[i] * gamma_z[i] / z[i];
313                  hat_tau[i] = tau[i] - t[i] * gamma_s[i] / s[i];
314                  c_x[i] = sigma[i] - z[i] * hat_nu[i] / g[i] - s[i] * hat_tau[i] / t[i];
315              }
316              for (i=0; i<m; i++) {	&bsol;* comput c_y and rho */
317                  c_y[i] = rho[i];
318                  for (j=i; j<m; j++)
319                      h_y[m*i + j] = 0;
320              }
321              solve_reduced(n, m, h_x, h_y, a, delta_x, delta_y, c_x, c_y, workspace,
322                  CORRECTOR);
323              for (i=0; i<n; i++) {
324                  delta_s[i] = s[i] * (delta_x[i] - hat_tau[i]) / t[i];
325                  delta_z[i] = z[i] * (hat_nu[i] - delta_x[i]) / g[i];
326                  delta_g[i] = g[i] * (gamma_z[i] - delta_z[i]) / z[i];
327                  delta_t[i] = t[i] * (gamma_s[i] - delta_s[i]) / s[i];
328              }
329              alfa = -1;
330              for (i=0; i<n; i++) {
331                  alfa = mn(alfa, delta_g[i]/g[i]);
332                  alfa = mn(alfa, delta_t[i]/t[i]);
333                  alfa = mn(alfa, delta_s[i]/s[i]);
334                  alfa = mn(alfa, delta_z[i]/z[i]);
335              }
336              alfa = (margin - 1) / alfa;
337              for (i=0, mu=0; i<n; i++)
338                  mu += z[i] * g[i] + s[i] * t[i];
339              mu = mu / (2*n);
340              mu = mu * sqr((alfa - 1) / (alfa + 10));
341              for (i=0; i<n; i++) {
342                  x[i] += alfa * delta_x[i];
343                  g[i] += alfa * delta_g[i];
344                  t[i] += alfa * delta_t[i];
345                  z[i] += alfa * delta_z[i];
346                  s[i] += alfa * delta_s[i];
347              }
348              for (i=0; i<m; i++) 
349                  y[i] += alfa * delta_y[i];
350          }
351      }
352      if ((status == 1) && (verb >= STATUS)) {
353          printf("----------------------------------------------------------------------------------\n");
354          printf("optimization converged\n");
355      }
356      free(workspace);
357      free(diag_h_x);
358      free(h_y);
359      free(c_x);
360      free(c_y);
361      free(h_dot_x);
362      free(rho);
363      free(nu);
364      free(tau);
365      free(sigma);
366      free(gamma_z);
367      free(gamma_s);
368      free(hat_nu);
369      free(hat_tau);
370      free(delta_x);
371      free(delta_y);
372      free(delta_s);
373      free(delta_z);
374      free(delta_g);
375      free(delta_t);
376      free(d);
377      return status;
378  }
</code></pre>
        </div>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-sgd_solver.cpp</h3>
            <pre><code>1  #include <string>
2  #include <vector>
3  #include "caffe/sgd_solvers.hpp"
4  #include "caffe/util/hdf5.hpp"
5  #include "caffe/util/io.hpp"
6  #include "caffe/util/upgrade_proto.hpp"
7  namespace caffe {
8  #ifdef CAFFE_PER_LAYER_TIMINGS
9  #define LAYER_UPDATE_TIMING_START(index) do { \
10    if (this->net()->phase() == TRAIN) { \
11      this->net()->update_start_time_per_layer[index] = this->net()->timer.Duration(); \
12    } \
13  }while(0)
14  #define LAYER_UPDATE_TIMING_STOP(index) do { \
15    if (this->net()->phase() == TRAIN) { \
16      this->net()->update_stop_time_per_layer[index] = this->net()->timer.Duration(); \
17      this->net()->update_time_per_layer[index] += (this->net()->update_stop_time_per_layer[index] - this->net()->update_start_time_per_layer[index]); \
18    } \
19  }while(0)
20  #else
21  #define LAYER_UPDATE_TIMING_START(index)
22  #define LAYER_UPDATE_TIMING_STOP(index)
23  #endif
24  template <typename Dtype>
25  Dtype SGDSolver<Dtype>::GetWarmUpLR(int cur_iter, int warmup_iter, Dtype warmup_start_lr) {
26    if (cur_iter < 0) {
27      cur_iter = 0;
28    }
29    return (cur_iter * this->param_.base_lr() +
30            (warmup_iter - cur_iter) * warmup_start_lr) / warmup_iter;
31  }
32  template <typename Dtype>
33  Dtype SGDSolver<Dtype>::GetLearningRate() {
34    Dtype rate;
35    const string& lr_policy = this->param_.lr_policy();
36    if (this->param_.warmup_iter() > 0 &&
37        this->iter_ < this->param_.warmup_iter()) {
38      rate = GetWarmUpLR(this->iter_, this->param_.warmup_iter(),
39                         this->param_.warmup_start_lr());
40    } else if (lr_policy == "fixed") {
41      rate = this->param_.base_lr();
42    } else if (lr_policy == "step") {
43      this->current_step_ = this->iter_ / this->param_.stepsize();
44      rate = this->param_.base_lr() *
45          pow(this->param_.gamma(), this->current_step_);
46    } else if (lr_policy == "exp") {
47      rate = this->param_.base_lr() * pow(this->param_.gamma(), this->iter_);
48    } else if (lr_policy == "inv") {
49      rate = this->param_.base_lr() *
50          pow(Dtype(1) + this->param_.gamma() * this->iter_,
51              - this->param_.power());
52    } else if (lr_policy == "multistep") {
53      if (this->current_step_ < this->param_.stepvalue_size() &&
54            this->iter_ >= this->param_.stepvalue(this->current_step_)) {
55        this->current_step_++;
56        LOG(INFO) << "MultiStep Status: Iteration " <<
57        this->iter_ << ", step = " << this->current_step_;
58      }
59      rate = this->param_.base_lr() *
60          pow(this->param_.gamma(), this->current_step_);
61    } else if (lr_policy == "poly") {
62      rate = this->param_.base_lr() * pow(Dtype(1.) -
63          (Dtype(this->iter_) / Dtype(this->param_.max_iter())),
64          this->param_.power());
65    } else if (lr_policy == "sigmoid") {
66      rate = this->param_.base_lr() * (Dtype(1.) /
67          (Dtype(1.) + exp(-this->param_.gamma() * (Dtype(this->iter_) -
68            Dtype(this->param_.stepsize())))));
69    } else if (lr_policy == "plateau") {
70      if (this->smoothed_loss_ < this->minimum_loss_) {
71        this->minimum_loss_ = this->smoothed_loss_;
72        this->iter_last_event_ = this->iter_;
73      }
74      if (this->current_step_ < this->param_.plateau_winsize_size()) {
75        int iter_next_update = this->iter_last_event_
76              + this->param_.plateau_winsize(this->current_step_);
77        if (this->iter_ >= iter_next_update) {
78          this->current_step_++;
79          this->iter_last_event_ = this->iter_;
80          LOG(INFO) << "Plateau Status: Iteration " << this->iter_
81                    << ", step = " << this->current_step_;
82        }
83      }
84      if (this->param_.display() && this->iter_ % this->param_.display() == 0
85          && this->iter_last_event_ > (this->iter_ - this->param_.display())) {
86        LOG(INFO) << "Plateau Status: Iteration " << this->iter_
87                  << ", current minimum_loss = " << this->minimum_loss_;
88      }
89      rate = this->param_.base_lr() *
90          pow(this->param_.gamma(), this->current_step_);
91    } else if (lr_policy == "multifixed") {
92        CHECK_EQ(this->param_.stageiter_size(), this->param_.stagelr_size());
93        int num_stages = this->param_.stagelr_size();
94        int stage = 0;
95        for (; stage < num_stages; ++stage) {
96            if (this->iter_ <= this->param_.stageiter(stage))
97                break;
98        }
99        stage = (stage == num_stages) ? stage - 1 : stage;
100        rate = this->param_.stagelr(stage);
101    } else {
102      LOG(FATAL) << "Unknown learning rate policy: " << lr_policy;
103    }
104    return rate;
105  }
106  template <typename Dtype>
107  void SGDSolver<Dtype>::PreSolve() {
108    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
109    history_.clear();
110    update_.clear();
111    temp_.clear();
112    for (int i = 0; i < net_params.size(); ++i) {
113      const vector<int>& shape = net_params[i]->shape();
114      history_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
115      update_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
116      temp_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(shape)));
117    }
118    this->minimum_loss_ = std::numeric_limits<float>::max();
119  }
120  template <typename Dtype>
121  void SGDSolver<Dtype>::ClipGradients() {
122    const Dtype clip_gradients = this->param_.clip_gradients();
123    if (clip_gradients < 0) { return; }
124    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
125    Dtype sumsq_diff = 0;
126    for (int i = 0; i < net_params.size(); ++i) {
127      sumsq_diff += net_params[i]->sumsq_diff();
128    }
129    const Dtype l2norm_diff = std::sqrt(sumsq_diff);
130    if (l2norm_diff > clip_gradients) {
131      Dtype scale_factor = clip_gradients / l2norm_diff;
132      LOG(INFO) << "Gradient clipping: scaling down gradients (L2 norm "
133          << l2norm_diff << " > " << clip_gradients << ") "
134          << "by scale factor " << scale_factor;
135      for (int i = 0; i < net_params.size(); ++i) {
136        net_params[i]->scale_diff(scale_factor);
137      }
138    }
139  }
140  template <typename Dtype>
141  void SGDSolver<Dtype>::PrintLearningRate() {
142    CHECK(Caffe::root_solver());
143    Dtype rate = GetLearningRate();
144    if (this->param_.display() && this->iter_ % this->param_.display() == 0) {
145      LOG(INFO) << "Iteration " << this->iter_ << ", lr = " << rate;
146    }
147  }
148  template <typename Dtype>
149  void SGDSolver<Dtype>::ApplyUpdate() {
150    PrintLearningRate();
151    ClipGradients();
152  #ifdef CAFFE_PER_LAYER_TIMINGS
153  #ifdef USE_MLSL
154    CHECK(mn::is_multinode() == false);
155  #endif
156    for (int i=0; i<this->net_->layers().size(); i++) {
157      const std::vector<int> param_ids = this->net_->get_layer_learnable_param_ids(i);
158      LAYER_UPDATE_TIMING_START(i);
159      for (int param_id = 0; param_id < param_ids.size(); ++param_id) {
160        ApplyUpdate(param_ids[param_id]);
161      }
162      LAYER_UPDATE_TIMING_STOP(i);
163    }
164  #else
165    for (int param_id = 0; param_id < this->net_->learnable_params().size();
166         ++param_id) {
167      ApplyUpdate(param_id);
168    }
169  #endif
170  }
171  template <typename Dtype>
172  void SGDSolver<Dtype>::ApplyUpdate(int param_id) {
173    CHECK(Caffe::root_solver());
174    Dtype rate = GetLearningRate();
175    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: raw delwt:");
176    if (this->net_->params_lr()[param_id] == 0) {
177      return;
178    }
179  #ifdef ENABLE_SGD_FUSION
180    if ((Caffe::mode() == Caffe::CPU) && (this->type() == string("SGD")))
181    {
182      SGDFusion(param_id, rate);
183      return;
184    }
185  #endif &bsol;* ENABLE_SGD_FUSION */
186    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
187    bool need_sync_data_back_to_prv = false;
188    bool need_sync_diff_back_to_prv = false;
189    if (net_params[param_id]->prv_data()
190        && (net_params[param_id]->prv_data_count()
191            != net_params[param_id]->count())) {
192      need_sync_data_back_to_prv = true;
193    }
194    if (net_params[param_id]->prv_diff()
195        && (net_params[param_id]->prv_diff_count()
196            != net_params[param_id]->count())) {
197      need_sync_diff_back_to_prv = true;
198    }
199    Normalize(param_id);
200    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Normalize:");
201    if (strcmp(this->type(), "SGD")) {
202      Regularize(param_id);
203      LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
204    }
205    ComputeUpdateValue(param_id, rate);
206    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: wtinc:");
207    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight before update:");
208    this->net_->learnable_params()[param_id]->Update();
209    if (need_sync_diff_back_to_prv) {
210      net_params[param_id]->mutable_prv_diff();
211    }
212    if (need_sync_data_back_to_prv) {
213      net_params[param_id]->mutable_prv_data();
214    }
215    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], data, param_id, "ApplyUpdate: weight after update:");
216  }
217  #ifdef ENABLE_SGD_FUSION
218  template <typename Dtype>
219  void axpy_axpby_copy(size_t count, const Dtype decay, const Dtype* net_params_data, Dtype *net_params_diff,
220                       const Dtype rate, const Dtype momentum, Dtype* history_data);
221  template <>
<span onclick='openModal()' class='match'>222  void axpy_axpby_copy<float>(size_t count, const float decay, const float* net_params_data, float *net_params_diff,
223                              const float rate, const float momentum, float* history_data)
224  {
225  #ifdef _OPENMP
226  #pragma omp parallel for simd schedule(static)
227  #endif  
228    for (size_t i = 0; i < count; ++i) {
</span>229      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
230      net_params_diff[i] = history_data[i];
231    }
232  }
233  template <>
234  void axpy_axpby_copy<double>(size_t count, const double decay, const double* net_params_data, double *net_params_diff,
235                               const double rate, const double momentum, double* history_data)
236  {
237  #ifdef _OPENMP
238  #pragma omp parallel for simd schedule(static)
239  #endif  
240    for (size_t i = 0; i < count; ++i) {
241      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
242      net_params_diff[i] = history_data[i];
243    }
244  }
245  template <typename Dtype>
246  void axpy_axpby_copy_axpy(size_t count, const Dtype decay, Dtype* net_params_data, Dtype *net_params_diff,
247                       const Dtype rate, const Dtype momentum, Dtype* history_data, const Dtype update_param);
248  template <>
249  void axpy_axpby_copy_axpy<float>(size_t count, const float decay, float* net_params_data, float *net_params_diff,
250                              const float rate, const float momentum, float* history_data, const float update_param)
251  {
252  #ifdef _OPENMP
253  #pragma omp parallel for simd schedule(static)
254  #endif  
255    for (size_t i = 0; i < count; ++i) {
256      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
257      net_params_data[i] = update_param * history_data[i] + net_params_data[i];
258    }
259  }
260  template <>
261  void axpy_axpby_copy_axpy<double>(size_t count, const double decay, double* net_params_data, double *net_params_diff,
262                               const double rate, const double momentum, double* history_data, const double update_param)
263  {
264  #ifdef _OPENMP
265  #pragma omp parallel for simd schedule(static)
266  #endif  
267    for (size_t i = 0; i < count; ++i) {
268      history_data[i] = rate * (decay * net_params_data[i] + net_params_diff[i]) + momentum * history_data[i];
269      net_params_data[i] = update_param * history_data[i] + net_params_data[i];
270    }
271  }
272  template <typename Dtype>
273  void SGDSolver<Dtype>::SGDFusion(int param_id, Dtype rate) {
274    bool skip_Normalize_stage_flag = false;
275    if (this->param_.iter_size() == 1) { skip_Normalize_stage_flag = true; }
276    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
277    const vector<float>& net_params_weight_decay =
278      this->net_->params_weight_decay();
279    Dtype weight_decay = this->param_.weight_decay();
280    string regularization_type = this->param_.regularization_type();
281    Dtype local_decay = weight_decay * net_params_weight_decay[param_id];
282    Dtype momentum = this->param_.momentum();
283    bool prv_diff_condition_flag = false;
284    bool need_sync_data_back_to_prv = false;
285    bool need_sync_diff_back_to_prv = false;
286    if (net_params[param_id]->prv_diff()
287      && (net_params[param_id]->prv_diff_count()
288      == net_params[param_id]->count())) {
289        prv_diff_condition_flag = true;
290    }
291    if (net_params[param_id]->prv_diff()
292      && (net_params[param_id]->prv_diff_count()
293      != net_params[param_id]->count())) {
294        need_sync_diff_back_to_prv = true;
295    }
296    if (net_params[param_id]->prv_data()
297      && (net_params[param_id]->prv_data_count()
298      != net_params[param_id]->count())) {
299        need_sync_data_back_to_prv = true;
300    }
301    if (skip_Normalize_stage_flag == false)
302    {
303      const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();
304      if (prv_diff_condition_flag) {
305        caffe_scal(net_params[param_id]->prv_diff_count(), accum_normalization,
306          net_params[param_id]->mutable_prv_diff());
307      }
308      else {
309        caffe_scal(net_params[param_id]->count(), accum_normalization,
310          net_params[param_id]->mutable_cpu_diff());
311      }
312    }
313    Dtype local_rate = rate * GetLocalRate(param_id);
314    bool is_separate_ComputeUpdateValue_Update = true;
315    if (local_decay) {
316      if (regularization_type == "L2") {
317        if (net_params[param_id]->prv_data() && net_params[param_id]->prv_diff()
318          && (net_params[param_id]->prv_data_count()
319          == net_params[param_id]->count()) &&
320              net_params[param_id]->get_prv_data_descriptor()->layout_compare(
321              net_params[param_id]->get_prv_diff_descriptor())) {
322            if (prv_diff_condition_flag) {
323              axpy_axpby_copy_axpy(net_params[param_id]->prv_data_count(), local_decay,
324                                  net_params[param_id]->mutable_prv_data(), net_params[param_id]->mutable_prv_diff(),
325                                  local_rate, momentum, history_[param_id]->mutable_cpu_data(), Dtype(-1));
326              is_separate_ComputeUpdateValue_Update = false;
327            }
328        } else {
329          if (!prv_diff_condition_flag)
330          {
331            axpy_axpby_copy_axpy(net_params[param_id]->count(), local_decay,
332                                  net_params[param_id]->mutable_cpu_data(), net_params[param_id]->mutable_cpu_diff(),
333                                  local_rate, momentum, history_[param_id]->mutable_cpu_data(), Dtype(-1));
334            is_separate_ComputeUpdateValue_Update = false;
335          }
336        }
337      } else if (regularization_type == "L1") {
338        caffe_cpu_sign(net_params[param_id]->count(),
339                        net_params[param_id]->cpu_data(),
340                        temp_[param_id]->mutable_cpu_data());
341        axpy_axpby_copy(net_params[param_id]->count(), local_decay,
342                                  temp_[param_id]->cpu_data(), net_params[param_id]->mutable_cpu_diff(),
343                                  local_rate, momentum, history_[param_id]->mutable_cpu_data());
344        is_separate_ComputeUpdateValue_Update = false;
345        net_params[param_id]->Update();
346      } else {
347        LOG(FATAL) << "Unknown regularization type: " << regularization_type;
348      }
349    }
350    if (is_separate_ComputeUpdateValue_Update == true)
351    {
352      if (prv_diff_condition_flag) {
353        caffe_cpu_axpby(net_params[param_id]->prv_diff_count(), local_rate,
354                        net_params[param_id]->prv_diff(), momentum,
355                        history_[param_id]->mutable_cpu_data());
356        caffe_copy(net_params[param_id]->count(),
357                    history_[param_id]->cpu_data(),
358                    net_params[param_id]->mutable_prv_diff());
359      } else {
360        caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
361                        net_params[param_id]->cpu_diff(), momentum,
362                        history_[param_id]->mutable_cpu_data());
363        caffe_copy(net_params[param_id]->count(),
364                    history_[param_id]->cpu_data(),
365                    net_params[param_id]->mutable_cpu_diff());
366      }
367      net_params[param_id]->Update();
368    }
369    if (need_sync_data_back_to_prv) {
370      net_params[param_id]->mutable_prv_data();
371    }
372    if (need_sync_diff_back_to_prv) {
373      net_params[param_id]->mutable_prv_diff();
374    }
375  }
376  #endif &bsol;* ENABLE_SGD_FUSION */
377  template <typename Dtype>
378  void SGDSolver<Dtype>::Normalize(int param_id) {
379    if (this->param_.iter_size() == 1) { 
380      return;
381    }
382    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
383    const Dtype accum_normalization = Dtype(1.) / this->param_.iter_size();
384    switch (Caffe::mode()) {
385    case Caffe::CPU: {
386      if (net_params[param_id]->prv_diff()
387          && (net_params[param_id]->prv_diff_count()
388              == net_params[param_id]->count())) {
389          caffe_scal(net_params[param_id]->prv_diff_count(), accum_normalization,
390              net_params[param_id]->mutable_prv_diff());
391      }
392      else {
393          caffe_scal(net_params[param_id]->count(), accum_normalization,
394              net_params[param_id]->mutable_cpu_diff());
395      }
396      break;
397    }
398    case Caffe::GPU: {
399  #ifndef CPU_ONLY
400      caffe_gpu_scal(net_params[param_id]->count(), accum_normalization,
401          net_params[param_id]->mutable_gpu_diff());
402  #else
403      NO_GPU;
404  #endif
405      break;
406    }
407    default:
408      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
409    }
410  }
411  template <typename Dtype>
412  void SGDSolver<Dtype>::Regularize(int param_id) {
413    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
414    const vector<float>& net_params_weight_decay =
415        this->net_->params_weight_decay();
416    Dtype weight_decay = this->param_.weight_decay();
417    string regularization_type = this->param_.regularization_type();
418    Dtype local_decay = weight_decay * net_params_weight_decay[param_id];
419    switch (Caffe::mode()) {
420    case Caffe::CPU: {
421      if (local_decay) {
422        if (regularization_type == "L2") {
423          if (net_params[param_id]->prv_data() && net_params[param_id]->prv_diff()
424               && (net_params[param_id]->prv_data_count()
425                   == net_params[param_id]->count()) &&
426              net_params[param_id]->get_prv_data_descriptor()->layout_compare(
427              net_params[param_id]->get_prv_diff_descriptor())) {
428            caffe_axpy(net_params[param_id]->prv_data_count(),
429                       local_decay,
430                       net_params[param_id]->prv_data(),
431                       net_params[param_id]->mutable_prv_diff());
432          } else {
433            caffe_axpy(net_params[param_id]->count(),
434                local_decay,
435                net_params[param_id]->cpu_data(),
436                net_params[param_id]->mutable_cpu_diff());
437          }
438        } else if (regularization_type == "L1") {
439          caffe_cpu_sign(net_params[param_id]->count(),
440              net_params[param_id]->cpu_data(),
441              temp_[param_id]->mutable_cpu_data());
442          caffe_axpy(net_params[param_id]->count(),
443              local_decay,
444              temp_[param_id]->cpu_data(),
445              net_params[param_id]->mutable_cpu_diff());
446        } else {
447          LOG(FATAL) << "Unknown regularization type: " << regularization_type;
448        }
449      }
450      break;
451    }
452    case Caffe::GPU: {
453  #ifndef CPU_ONLY
454      if (local_decay) {
455        if (regularization_type == "L2") {
456          caffe_gpu_axpy(net_params[param_id]->count(),
457              local_decay,
458              net_params[param_id]->gpu_data(),
459              net_params[param_id]->mutable_gpu_diff());
460        } else if (regularization_type == "L1") {
461          caffe_gpu_sign(net_params[param_id]->count(),
462              net_params[param_id]->gpu_data(),
463              temp_[param_id]->mutable_gpu_data());
464          caffe_gpu_axpy(net_params[param_id]->count(),
465              local_decay,
466              temp_[param_id]->gpu_data(),
467              net_params[param_id]->mutable_gpu_diff());
468        } else {
469          LOG(FATAL) << "Unknown regularization type: " << regularization_type;
470        }
471      }
472  #else
473      NO_GPU;
474  #endif
475      break;
476    }
477    default:
478      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
479    }
480  }
481  #ifndef CPU_ONLY
482  template <typename Dtype>
483  void sgd_update_gpu(int N, Dtype* g, Dtype* h, Dtype momentum,
484      Dtype local_rate);
485  #endif
486  template <typename Dtype>
487  void SGDSolver<Dtype>::ComputeUpdateValue(int param_id, Dtype rate) {
488    const vector<Blob<Dtype>*>& net_params = this->net_->learnable_params();
489    Dtype momentum = this->param_.momentum();
490    Dtype local_rate = rate * GetLocalRate(param_id);
491    Regularize(param_id);
492    LOG_PARAM_BLOB(this->net_->learnable_params()[param_id], diff, param_id, "ApplyUpdate: delwt after Regularize:");
493    if (this->param_.warmup_iter() > 0 &&
494        this->iter_ < this->param_.warmup_iter()) {
495      Dtype prev_rate = GetWarmUpLR(this->iter_ - 1, this->param_.warmup_iter(),
496                                    this->param_.warmup_start_lr());
497      momentum = momentum * (rate / prev_rate);
498    }
499    switch (Caffe::mode()) {
500    case Caffe::CPU: {
501      if (net_params[param_id]->prv_diff()
502          && (net_params[param_id]->prv_diff_count()
503              == net_params[param_id]->count())) {
504        caffe_cpu_axpby(net_params[param_id]->prv_diff_count(), local_rate,
505                        net_params[param_id]->prv_diff(), momentum,
506                        history_[param_id]->mutable_cpu_data());
507        caffe_copy(net_params[param_id]->count(),
508                   history_[param_id]->cpu_data(),
509                   net_params[param_id]->mutable_prv_diff());
510      } else {
511        caffe_cpu_axpby(net_params[param_id]->count(), local_rate,
512                       net_params[param_id]->cpu_diff(), momentum,
513                       history_[param_id]->mutable_cpu_data());
514        caffe_copy(net_params[param_id]->count(),
515                   history_[param_id]->cpu_data(),
516                   net_params[param_id]->mutable_cpu_diff());
517      }
518      break;
519    }
520    case Caffe::GPU: {
521  #ifndef CPU_ONLY
522      sgd_update_gpu(net_params[param_id]->count(),
523          net_params[param_id]->mutable_gpu_diff(),
524          history_[param_id]->mutable_gpu_data(),
525          momentum, local_rate);
526  #else
527      NO_GPU;
528  #endif
529      break;
530    }
531    default:
532      LOG(FATAL) << "Unknown caffe mode: " << Caffe::mode();
533    }
534  }
535  template <typename Dtype>
536  Dtype SGDSolver<Dtype>::GetLocalRate(int param_id) const {
537    const vector<float>& net_params_lr = this->net_->params_lr();
538    float local_lr = net_params_lr[param_id];
539    if (this->param_.local_lr_auto()) {
540      Blob<Dtype>* param = this->net_->learnable_params()[param_id];
541      const float w_norm = std::sqrt(param->sumsq_data());
542      const float wgrad_norm = std::sqrt(param->sumsq_diff());
543      const float gw_ratio = this->param_.local_gw_ratio();
544      float rate = 1.F;
545      float weight_decay = this->param_.weight_decay();
546      if (w_norm > 0.F && wgrad_norm > 0.F) {
547        rate = gw_ratio * w_norm / (wgrad_norm + weight_decay * w_norm);
548      }
549      if (local_lr > 0.F) {
550        local_lr = rate;
551      }
552  #ifdef DEBUG
553      if (Caffe::root_solver()
554          && this->param_.display()
555          && (this->iter_ % this->param_.display() == 0)) {
556        const int layer_id = this->net_->param_layer_indices(param_id).first;
557        const string& layer_name = this->net_->layer_names()[layer_id];
558        const int blob_id = this->net_->param_layer_indices(param_id).second;
559        LOG(INFO) << layer_name << "." << blob_id << " lr=" << local_lr
560          << ".\t  w=" << w_norm << "\t  dw=" << wgrad_norm;
561      }
562  #endif
563    }
564    return local_lr;
565  }
566  template <typename Dtype>
567  void SGDSolver<Dtype>::SnapshotSolverState(const string& model_filename) {
568    switch (this->param_.snapshot_format()) {
569      case caffe::SolverParameter_SnapshotFormat_BINARYPROTO:
570        SnapshotSolverStateToBinaryProto(model_filename);
571        break;
572      case caffe::SolverParameter_SnapshotFormat_HDF5:
573        SnapshotSolverStateToHDF5(model_filename);
574        break;
575      default:
576        LOG(FATAL) << "Unsupported snapshot format.";
577    }
578  }
579  template <typename Dtype>
580  void SGDSolver<Dtype>::SnapshotSolverStateToBinaryProto(
581      const string& model_filename) {
582  #ifdef USE_MLSL
583    if (mn::is_root()) {
584  #endif
585    SolverState state;
586    state.set_iter(this->iter_);
587    state.set_learned_net(model_filename);
588    state.set_current_step(this->current_step_);
589    state.set_iter_last_event(this->iter_last_event_);
590    state.set_minimum_loss(this->minimum_loss_);
591    state.clear_history();
592    for (int i = 0; i < history_.size(); ++i) {
593      BlobProto* history_blob = state.add_history();
594      history_[i]->ToProto(history_blob);
595    }
596    string snapshot_filename = Solver<Dtype>::SnapshotFilename(".solverstate");
597    LOG(INFO)
598      << "Snapshotting solver state to binary proto file " << snapshot_filename;
599    WriteProtoToBinaryFile(state, snapshot_filename.c_str());
600  #ifdef USE_MLSL
601    }
602  #endif
603  }
604  template <typename Dtype>
605  void SGDSolver<Dtype>::SnapshotSolverStateToHDF5(
606      const string& model_filename) {
607  #ifdef USE_MLSL
608    if (mn::is_root()) {
609  #endif
610    string snapshot_filename =
611        Solver<Dtype>::SnapshotFilename(".solverstate.h5");
612    LOG(INFO) << "Snapshotting solver state to HDF5 file " << snapshot_filename;
613    hid_t file_hid = H5Fcreate(snapshot_filename.c_str(), H5F_ACC_TRUNC,
614        H5P_DEFAULT, H5P_DEFAULT);
615    CHECK_GE(file_hid, 0)
616        << "Couldn't open " << snapshot_filename << " to save solver state.";
617    hdf5_save_int(file_hid, "iter", this->iter_);
618    hdf5_save_string(file_hid, "learned_net", model_filename);
619    hdf5_save_int(file_hid, "current_step", this->current_step_);
620    hdf5_save_int(file_hid, "iter_last_event", this->iter_last_event_);
621    hdf5_save_float<Dtype>(file_hid, "minimum_loss", this->minimum_loss_);
622    hid_t history_hid = H5Gcreate2(file_hid, "history", H5P_DEFAULT, H5P_DEFAULT,
623        H5P_DEFAULT);
624    CHECK_GE(history_hid, 0)
625        << "Error saving solver state to " << snapshot_filename << ".";
626    for (int i = 0; i < history_.size(); ++i) {
627      ostringstream oss;
628      oss << i;
629      hdf5_save_nd_dataset<Dtype>(history_hid, oss.str(), *history_[i]);
630    }
631    H5Gclose(history_hid);
632    H5Fclose(file_hid);
633  #ifdef USE_MLSL
634    }
635  #endif
636  }
637  template <typename Dtype>
638  void SGDSolver<Dtype>::RestoreSolverStateFromBinaryProto(
639      const string& state_file) {
640    SolverState state;
641    ReadProtoFromBinaryFile(state_file, &state);
642    this->iter_ = state.iter();
643    if (state.has_learned_net()) {
644      NetParameter net_param;
645      ReadNetParamsFromBinaryFileOrDie(state.learned_net().c_str(), &net_param);
646      this->net_->CopyTrainedLayersFrom(net_param);
647    }
648    this->current_step_ = state.current_step();
649    this->iter_last_event_ = state.iter_last_event();
650    this->minimum_loss_ = state.minimum_loss();
651    CHECK_EQ(state.history_size(), history_.size())
652        << "Incorrect length of history blobs.";
653    LOG(INFO) << "SGDSolver: restoring history";
654    for (int i = 0; i < history_.size(); ++i) {
655      history_[i]->FromProto(state.history(i));
656    }
657  }
658  template <typename Dtype>
659  void SGDSolver<Dtype>::RestoreSolverStateFromHDF5(const string& state_file) {
660    hid_t file_hid = H5Fopen(state_file.c_str(), H5F_ACC_RDONLY, H5P_DEFAULT);
661    CHECK_GE(file_hid, 0) << "Couldn't open solver state file " << state_file;
662    this->iter_ = hdf5_load_int(file_hid, "iter");
663    if (H5LTfind_dataset(file_hid, "learned_net")) {
664      string learned_net = hdf5_load_string(file_hid, "learned_net");
665      this->net_->CopyTrainedLayersFrom(learned_net);
666    }
667    this->current_step_ = hdf5_load_int(file_hid, "current_step");
668    this->iter_last_event_ = hdf5_load_int(file_hid, "iter_last_event");
669    this->minimum_loss_ = hdf5_load_float<Dtype>(file_hid, "minimum_loss");
670    hid_t history_hid = H5Gopen2(file_hid, "history", H5P_DEFAULT);
671    CHECK_GE(history_hid, 0) << "Error reading history from " << state_file;
672    int state_history_size = hdf5_get_num_links(history_hid);
673    CHECK_EQ(state_history_size, history_.size())
674        << "Incorrect length of history blobs.";
675    for (int i = 0; i < history_.size(); ++i) {
676      ostringstream oss;
677      oss << i;
678      hdf5_load_nd_dataset<Dtype>(history_hid, oss.str().c_str(), 0,
679                                  kMaxBlobAxes, history_[i].get());
680    }
681    H5Gclose(history_hid);
682    H5Fclose(file_hid);
683  }
684  INSTANTIATE_CLASS(SGDSolver);
685  REGISTER_SOLVER_CLASS(SGD);
686  }  
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-svmPrLoqo.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-sgd_solver.cpp</div>
                </div>
                <div class="column column_space"><pre><code>23  void TPrLoqo::cholsb(double a[], int n, double p[], double b[], double x[]) {
24      int i, k;
25      double sum;
26      for (i=0; i<n; i++) {
</pre></code></div>
                <div class="column column_space"><pre><code>222  void axpy_axpby_copy<float>(size_t count, const float decay, const float* net_params_data, float *net_params_diff,
223                              const float rate, const float momentum, float* history_data)
224  {
225  #ifdef _OPENMP
226  #pragma omp parallel for simd schedule(static)
227  #endif  
228    for (size_t i = 0; i < count; ++i) {
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    