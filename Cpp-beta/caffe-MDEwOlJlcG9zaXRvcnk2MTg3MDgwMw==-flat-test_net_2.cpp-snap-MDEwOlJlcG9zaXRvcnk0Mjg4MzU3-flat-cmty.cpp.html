
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 17.87109375%, Tokens: 9, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</h3>
            <pre><code>1  #include <string>
2  #include <utility>
3  #include <vector>
4  #include "google/protobuf/text_format.h"
5  #include "gtest/gtest.h"
6  #include "caffe/common.hpp"
7  #include "caffe/filler.hpp"
8  #include "caffe/net.hpp"
9  #include "caffe/util/io.hpp"
10  #include "caffe/util/math_functions.hpp"
11  #include "caffe/test/test_caffe_main.hpp"
12  #include "caffe/test/test_gradient_check_util.hpp"
13  namespace caffe {
14  template <typename ParentType>
15  class ParentTest : public ParentType {
16    typedef typename ParentType::Dtype Dtype;
17   protected:
18    ParentTest() : seed_(1701) {}
19    virtual void InitNetFromProtoString(const string& proto) {
20      NetParameter param;
21      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &param));
22      net_.reset(new Net<Dtype>(param));
23    }
24    virtual void InitNetFromProtoFileWithState(const string& proto,
25        Phase phase = caffe::TRAIN, const int level = 0,
26        const vector<string>* stages = NULL) {
27      NetParameter param;
28      CHECK(google::protobuf::TextFormat::ParseFromString(proto, &param));
29      string param_file;
30      MakeTempFilename(&param_file);
31      WriteProtoToTextFile(param, param_file);
32      net_.reset(new Net<Dtype>(param_file, phase, level, stages));
33    }
34    virtual void CopyNetBlobs(const bool copy_diff,
35        vector<shared_ptr<Blob<Dtype> > >* blobs_copy) {
36      CHECK(net_);
37      const vector<shared_ptr<Blob<Dtype> > >& net_blobs = net_->blobs();
38      blobs_copy->clear();
39      blobs_copy->resize(net_blobs.size());
40      const bool kReshape = true;
41      for (int i = 0; i < net_blobs.size(); ++i) {
42        (*blobs_copy)[i].reset(new Blob<Dtype>());
43        (*blobs_copy)[i]->CopyFrom(*net_blobs[i], copy_diff, kReshape);
44      }
45    }
46    virtual void CopyNetParams(const bool copy_diff,
47        vector<shared_ptr<Blob<Dtype> > >* params_copy) {
48      CHECK(net_);
49      const vector<shared_ptr<Blob<Dtype> > >& net_params = net_->params();
50      params_copy->clear();
51      params_copy->resize(net_params.size());
52      const bool kReshape = true;
53      for (int i = 0; i < net_params.size(); ++i) {
54        (*params_copy)[i].reset(new Blob<Dtype>());
55        (*params_copy)[i]->CopyFrom(*net_params[i], copy_diff, kReshape);
56      }
57    }
58    virtual void InitTinyNet(const bool force_backward = false,
59                             const bool accuracy_layer = false) {
60      string proto =
61          "name: 'TinyTestNetwork' "
62          "layer { "
63          "  name: 'data' "
64          "  type: 'DummyData' "
65          "  dummy_data_param { "
66          "    shape { "
67          "      dim: 5 "
68          "      dim: 2 "
69          "      dim: 3 "
70          "      dim: 4 "
71          "    } "
72          "    data_filler { "
73          "      type: 'gaussian' "
74          "      std: 0.01 "
75          "    } "
76          "    shape { "
77          "      dim: 5 "
78          "    } "
79          "    data_filler { "
80          "      type: 'constant' "
81          "      value: 0 "
82          "    } "
83          "  } "
84          "  top: 'data' "
85          "  top: 'label' "
86          "} "
87          "layer { "
88          "  name: 'innerproduct' "
89          "  type: 'InnerProduct' "
90          "  inner_product_param { "
91          "    num_output: 1000 "
92          "    weight_filler { "
93          "      type: 'gaussian' "
94          "      std: 0.01 "
95          "    } "
96          "    bias_filler { "
97          "      type: 'constant' "
98          "      value: 0 "
99          "    } "
100          "  } "
101          "  param { "
102          "    lr_mult: 1 "
103          "    decay_mult: 1 "
104          "  } "
105          "  param { "
106          "    lr_mult: 2 "
107          "    decay_mult: 0 "
108          "  } "
109          "  bottom: 'data' "
110          "  top: 'innerproduct' "
111          "} "
112          "layer { "
113          "  name: 'loss' "
114          "  type: 'SoftmaxWithLoss' "
115          "  bottom: 'innerproduct' "
116          "  bottom: 'label' "
117          "  top: 'top_loss' "
118          "} ";
119      if (accuracy_layer) {
120        proto +=
121            "layer { "
122            "  name: 'loss' "
123            "  type: 'Accuracy' "
124            "  bottom: 'innerproduct' "
125            "  bottom: 'label' "
126            "  top: 'accuracy' "
127            "} ";
128      }
129      if (force_backward) {
130        proto += "force_backward: true ";
131      }
132      InitNetFromProtoString(proto);
133    }
134    virtual void InitTinyNetEuclidean(const bool force_backward = false) {
135      string proto =
136          "name: 'TinyTestEuclidLossNetwork' "
137          "layer { "
138          "  name: 'data' "
139          "  type: 'DummyData' "
140          "  dummy_data_param { "
141          "    num: 5 "
142          "    channels: 2 "
143          "    height: 3 "
144          "    width: 4 "
145          "    num: 5 "
146          "    channels: 1 "
147          "    height: 1 "
148          "    width: 1 "
149          "    data_filler { "
150          "      type: 'gaussian' "
151          "      std: 0.01 "
152          "    } "
153          "  } "
154          "  top: 'data' "
155          "  top: 'label' "
156          "} "
157          "layer { "
158          "  name: 'innerproduct' "
159          "  type: 'InnerProduct' "
160          "  inner_product_param { "
161          "    num_output: 1 "
162          "    weight_filler { "
163          "      type: 'gaussian' "
164          "      std: 0.01 "
165          "    } "
166          "    bias_filler { "
167          "      type: 'constant' "
168          "      value: 0 "
169          "    } "
170          "  } "
171          "  param { "
172          "    lr_mult: 1 "
173          "    decay_mult: 1 "
174          "  } "
175          "  param { "
176          "    lr_mult: 2 "
177          "    decay_mult: 0 "
178          "  } "
179          "  bottom: 'data' "
180          "  top: 'innerproduct' "
181          "} "
182          "layer { "
183          "  name: 'loss' "
184          "  type: 'EuclideanLoss' "
185          "  bottom: 'innerproduct' "
186          "  bottom: 'label' "
187          "} ";
188      if (force_backward) {
189        proto += "force_backward: true ";
190      }
191      InitNetFromProtoString(proto);
192    }
193    virtual void InitTrickyNet(Dtype* loss_weight = NULL) {
194      ostringstream loss_weight_stream;
195      if (loss_weight) {
196        loss_weight_stream << "  loss_weight: " << *loss_weight << " ";
197      }
198      const string& proto =
199          "name: 'TrickyTestNetwork' "
200          "layer { "
201          "  name: 'data' "
202          "  type: 'DummyData' "
203          "  dummy_data_param { "
204          "    num: 5 "
205          "    channels: 2 "
206          "    height: 3 "
207          "    width: 4 "
208          "    num: 5 "
209          "    channels: 1 "
210          "    height: 1 "
211          "    width: 1 "
212          "    data_filler { "
213          "      type: 'gaussian' "
214          "      std: 0.01 "
215          "    } "
216          "  } "
217          "  top: 'data' "
218          "  top: 'label' "
219          "} "
220          "layer { "
221          "  name: 'innerproduct' "
222          "  type: 'InnerProduct' "
223          "  inner_product_param { "
224          "    num_output: 1000 "
225          "    weight_filler { "
226          "      type: 'gaussian' "
227          "      std: 0.01 "
228          "    } "
229          "    bias_filler { "
230          "      type: 'constant' "
231          "      value: 0 "
232          "    } "
233          "  } "
234          "  param { "
235          "    lr_mult: 1 "
236          "    decay_mult: 1 "
237          "  } "
238          "  param { "
239          "    lr_mult: 2 "
240          "    decay_mult: 0 "
241          "  } "
242          "  bottom: 'data' "
243          "  top: 'transformed_data' "
244          "} "
245          "layer { "
246          "  name: 'innerproduct' "
247          "  type: 'InnerProduct' "
248          "  inner_product_param { "
249          "    num_output: 1 "
250          "    weight_filler { "
251          "      type: 'gaussian' "
252          "      std: 0.01 "
253          "    } "
254          "    bias_filler { "
255          "      type: 'constant' "
256          "      value: 0 "
257          "    } "
258          "  } "
259          "  param { "
260          "    lr_mult: 1 "
261          "    decay_mult: 1 "
262          "  } "
263          "  param { "
264          "    lr_mult: 2 "
265          "    decay_mult: 0 "
266          "  } "
267          "  bottom: 'label' "
268          "  top: 'transformed_label' "
269          "} "
270          "layer { "
271          "  name: 'loss' "
272          "  type: 'SoftmaxWithLoss' " +
273          loss_weight_stream.str() +
274          "  bottom: 'transformed_data' "
275          "  bottom: 'transformed_label' "
276          "} ";
277      InitNetFromProtoString(proto);
278    }
279    virtual void InitUnsharedWeightsNet(const Dtype* loss_weight = NULL,
280        const Dtype* midnet_loss_weight = NULL,
281        const bool force_backward = false, const bool bias_term = false,
282        const Dtype blobs_lr_w1 = 1, const Dtype blobs_lr_b1 = 2,
283        const Dtype blobs_lr_w2 = 1, const Dtype blobs_lr_b2 = 2) {
284      string bias_str = bias_term ? "true ":"false ";
285      ostringstream proto;
286      proto << "name: 'UnsharedWeightsNetwork' ";
287      if (force_backward) {
288        proto << "force_backward: true ";
289      }
290      proto <<
291          "layer { "
292          "  name: 'data' "
293          "  type: 'DummyData' "
294          "  dummy_data_param { "
295          "    num: 5 "
296          "    channels: 2 "
297          "    height: 3 "
298          "    width: 4 "
299          "    data_filler { "
300          "      type: 'gaussian' "
301          "      std: 0.01 "
302          "    } "
303          "  } "
304          "  top: 'data' "
305          "} "
306          "layer { "
307          "  name: 'innerproduct1' "
308          "  type: 'InnerProduct' "
309          "  inner_product_param { "
310          "    num_output: 10 "
311          "    bias_term: " << bias_str <<
312          "    weight_filler { "
313          "      type: 'gaussian' "
314          "      std: 10 "
315          "    } "
316          "  } "
317          "  param { "
318          "    name: 'unsharedweights1' "
319          "    lr_mult: " << blobs_lr_w1 <<
320          "  } ";
321      if (bias_term) {
322        proto << "  param { lr_mult: " << blobs_lr_b1 << " } ";
323      }
324      proto <<
325          "  bottom: 'data' "
326          "  top: 'innerproduct1' ";
327      if (midnet_loss_weight) {
328        proto << "  loss_weight: " << *midnet_loss_weight << " ";
329      }
330      proto <<
331          "} "
332          "layer { "
333          "  name: 'innerproduct2' "
334          "  type: 'InnerProduct' "
335          "  inner_product_param { "
336          "    num_output: 10 "
337          "    bias_term: " << bias_str <<
338          "    weight_filler { "
339          "      type: 'gaussian' "
340          "      std: 10 "
341          "    } "
342          "  } "
343          "  param { "
344          "    name: 'unsharedweights2' "
345          "    lr_mult: " << blobs_lr_w2 <<
346          "  } ";
347      if (bias_term) {
348        proto << "  param { lr_mult: " << blobs_lr_b2 << " } ";
349      }
350      proto <<
351          "  bottom: 'data' "
352          "  top: 'innerproduct2' "
353          "} "
354          "layer { "
355          "  name: 'loss' "
356          "  type: 'EuclideanLoss' ";
357      if (loss_weight) {
358        proto << "  loss_weight: " << *loss_weight << " ";
359      }
360      proto <<
361          "  bottom: 'innerproduct1' "
362          "  bottom: 'innerproduct2' "
363          "} ";
364      InitNetFromProtoString(proto.str());
365    }
366    virtual void InitSharedWeightsNet() {
367      const string& proto =
368          "name: 'SharedWeightsNetwork' "
369          "layer { "
370          "  name: 'data' "
371          "  type: 'DummyData' "
372          "  dummy_data_param { "
373          "    num: 5 "
374          "    channels: 2 "
375          "    height: 3 "
376          "    width: 4 "
377          "    data_filler { "
378          "      type: 'gaussian' "
379          "      std: 0.01 "
380          "    } "
381          "  } "
382          "  top: 'data' "
383          "} "
384          "layer { "
385          "  name: 'innerproduct1' "
386          "  type: 'InnerProduct' "
387          "  inner_product_param { "
388          "    num_output: 10 "
389          "    bias_term: false "
390          "    weight_filler { "
391          "      type: 'gaussian' "
392          "      std: 10 "
393          "    } "
394          "  } "
395          "  param { name: 'sharedweights' } "
396          "  bottom: 'data' "
397          "  top: 'innerproduct1' "
398          "} "
399          "layer { "
400          "  name: 'innerproduct2' "
401          "  type: 'InnerProduct' "
402          "  inner_product_param { "
403          "    num_output: 10 "
404          "    bias_term: false "
405          "    weight_filler { "
406          "      type: 'gaussian' "
407          "      std: 10 "
408          "    } "
409          "  } "
410          "  param { name: 'sharedweights' } "
411          "  bottom: 'data' "
412          "  top: 'innerproduct2' "
413          "} "
414          "layer { "
415          "  name: 'loss' "
416          "  type: 'EuclideanLoss' "
417          "  bottom: 'innerproduct1' "
418          "  bottom: 'innerproduct2' "
419          "} ";
420      InitNetFromProtoString(proto);
421    }
422    virtual void InitDiffDataUnsharedWeightsNet() {
423      const string& proto =
424          "name: 'DiffDataUnsharedWeightsNetwork' "
425          "layer { "
426          "  name: 'data' "
427          "  type: 'DummyData' "
428          "  dummy_data_param { "
429          "    num: 10 "
430          "    channels: 10 "
431          "    height: 1 "
432          "    width: 1 "
433          "    num: 10 "
434          "    channels: 10 "
435          "    height: 1 "
436          "    width: 1 "
437          "    data_filler { "
438          "      type: 'gaussian' "
439          "      std: 10 "
440          "    } "
441          "  } "
442          "  top: 'data1' "
443          "  top: 'data2' "
444          "} "
445          "layer { "
446          "  name: 'innerproduct1' "
447          "  type: 'InnerProduct' "
448          "  inner_product_param { "
449          "    num_output: 10 "
450          "    bias_term: false "
451          "    weight_filler { "
452          "      type: 'constant' "
453          "      value: 0.5 "
454          "    } "
455          "  } "
456          "  param { name: 'unsharedweights1' } "
457          "  bottom: 'data1' "
458          "  top: 'innerproduct1' "
459          "} "
460          "layer { "
461          "  name: 'innerproduct2' "
462          "  type: 'InnerProduct' "
463          "  inner_product_param { "
464          "    num_output: 10 "
465          "    bias_term: false "
466          "    weight_filler { "
467          "      type: 'constant' "
468          "      value: 0.5 "
469          "    } "
470          "  } "
471          "  param { name: 'unsharedweights2' } "
472          "  bottom: 'innerproduct1' "
473          "  top: 'innerproduct2' "
474          "} "
475          "layer { "
476          "  name: 'loss' "
477          "  type: 'EuclideanLoss' "
478          "  bottom: 'data2' "
479          "  bottom: 'innerproduct2' "
480          "} ";
481      InitNetFromProtoString(proto);
482    }
483    virtual void InitDiffDataSharedWeightsNet() {
484      const string& proto =
485          "name: 'DiffDataSharedWeightsNetwork' "
486          "layer { "
487          "  name: 'data' "
488          "  type: 'DummyData' "
489          "  dummy_data_param { "
490          "    num: 10 "
491          "    channels: 10 "
492          "    height: 1 "
493          "    width: 1 "
494          "    num: 10 "
495          "    channels: 10 "
496          "    height: 1 "
497          "    width: 1 "
498          "    data_filler { "
499          "      type: 'gaussian' "
500          "      std: 10 "
501          "    } "
502          "  } "
503          "  top: 'data1' "
504          "  top: 'data2' "
505          "} "
506          "layer { "
507          "  name: 'innerproduct1' "
508          "  type: 'InnerProduct' "
509          "  inner_product_param { "
510          "    num_output: 10 "
511          "    bias_term: false "
512          "    weight_filler { "
513          "      type: 'constant' "
514          "      value: 0.5 "
515          "    } "
516          "  } "
517          "  param { name: 'sharedweights' } "
518          "  bottom: 'data1' "
519          "  top: 'innerproduct1' "
520          "} "
521          "layer { "
522          "  name: 'innerproduct2' "
523          "  type: 'InnerProduct' "
524          "  inner_product_param { "
525          "    num_output: 10 "
526          "    bias_term: false "
527          "    weight_filler { "
528          "      type: 'constant' "
529          "      value: 0.5 "
530          "    } "
531          "  } "
532          "  param { name: 'sharedweights' } "
533          "  bottom: 'innerproduct1' "
534          "  top: 'innerproduct2' "
535          "} "
536          "layer { "
537          "  name: 'loss' "
538          "  type: 'EuclideanLoss' "
539          "  bottom: 'data2' "
540          "  bottom: 'innerproduct2' "
541          "} ";
542      InitNetFromProtoString(proto);
543    }
544    virtual void InitReshapableNet() {
545      const string& proto =
546          "name: 'ReshapableNetwork' "
547          "layer { "
548          "  name: 'data' "
549          "  type: 'Input' "
550          "  top: 'data' "
551          "  input_param { "
552          "  shape: { dim: 1 dim: 3 dim: 100 dim: 100 } "
553          "  } "
554          "} "
555          "layer { "
556          "  name: 'conv1' "
557          "  type: 'Convolution' "
558          "  bottom: 'data' "
559          "  top: 'conv1' "
560          "  convolution_param { "
561          "    num_output: 5 "
562          "    kernel_size: 3 "
563          "    stride: 2 "
564          "    weight_filler { "
565          "      type: 'gaussian' "
566          "      std: 0.01 "
567          "    } "
568          "    bias_filler { "
569          "      type: 'constant' "
570          "      value: 0.2 "
571          "    } "
572          "  } "
573          "} "
574          "layer { "
575          "  name: 'relu1' "
576          "  type: 'ReLU' "
577          "  bottom: 'conv1' "
578          "  top: 'conv1' "
579          "} "
580          "layer { "
581          "  name: 'pool1' "
582          "  type: 'Pooling' "
583          "  bottom: 'conv1' "
584          "  top: 'pool1' "
585          "  pooling_param { "
586          "    pool: MAX "
587          "    kernel_size: 2 "
588          "    stride: 2 "
589          "  } "
590          "} "
591          "layer { "
592          "  name: 'norm1' "
593          "  type: 'LRN' "
594          "  bottom: 'pool1' "
595          "  top: 'norm1' "
596          "  lrn_param { "
597          "    local_size: 3 "
598          "  } "
599          "} "
600          "layer { "
601          "  name: 'softmax' "
602          "  type: 'Softmax' "
603          "  bottom: 'norm1' "
604          "  top: 'softmax' "
605          "} ";
606      InitNetFromProtoString(proto);
607    }
608    virtual void InitSkipPropNet(bool test_skip_true) {
609      string proto =
610        "name: 'SkipPropTestNetwork' "
611        "layer { "
612        "  name: 'data' "
613        "  type: 'DummyData' "
614        "  dummy_data_param { "
615        "    shape { "
616        "      dim: 5 "
617        "      dim: 2 "
618        "      dim: 3 "
619        "      dim: 4 "
620        "    } "
621        "    data_filler { "
622        "      type: 'gaussian' "
623        "      std: 0.01 "
624        "    } "
625        "    shape { "
626        "      dim: 5 "
627        "    } "
628        "    data_filler { "
629        "      type: 'constant' "
630        "      value: 0 "
631        "    } "
632        "  } "
633        "  top: 'data' "
634        "  top: 'label' "
635        "} "
636        "layer { "
637        "  name: 'silence' "
638        "  bottom: 'label' "
639        "  type: 'Silence' "
640        "} "
641        "layer { "
642        "  name: 'innerproduct' "
643        "  type: 'InnerProduct' "
644        "  inner_product_param { "
645        "    num_output: 1 "
646        "    weight_filler { "
647        "      type: 'gaussian' "
648        "      std: 0.01 "
649        "    } "
650        "    bias_filler { "
651        "      type: 'constant' "
652        "      value: 0 "
653        "    } "
654        "  } "
655        "  param { "
656        "    lr_mult: 1 "
657        "    decay_mult: 1 "
658        "  } "
659        "  param { "
660        "    lr_mult: 2 "
661        "    decay_mult: 0 "
662        "  } "
663        "  bottom: 'data' "
664        "  top: 'innerproduct' "
665        "} "
666        "layer { "
667        "  name: 'ip_fake_labels' "
668        "  type: 'InnerProduct' "
669        "  inner_product_param { "
670        "    num_output: 1 "
671        "    weight_filler { "
672        "      type: 'gaussian' "
673        "      std: 0.01 "
674        "    } "
675        "    bias_filler { "
676        "      type: 'constant' "
677        "      value: 0 "
678        "    } "
679        "  } "
680        "  bottom: 'data' "
681        "  top: 'fake_labels' "
682        "} "
683        "layer { "
684        "  name: 'argmax' "
685        "  bottom: 'fake_labels' "
686        "  top: 'label_argmax' "
687        "  type: 'ArgMax' "
688        "} "
689        "layer { "
690        "  name: 'loss' "
691        "  bottom: 'innerproduct' "
692        "  bottom: 'label_argmax' ";
693      if (test_skip_true)
694        proto += "  propagate_down: true "
695                 "  propagate_down: false ";
696      else
697        proto += "  propagate_down: true "
698                 "  propagate_down: true ";
699      proto +=
700        "  top: 'cross_entropy_loss' "
701        "  type: 'SigmoidCrossEntropyLoss' "
702        "  loss_weight: 0.1 "
703        "} ";
704      InitNetFromProtoString(proto);
705    }
706    virtual void InitForcePropNet(bool test_force_true) {
707      string proto =
708        "name: 'ForcePropTestNetwork' "
709        "layer { "
710        "  name: 'data' "
711        "  type: 'DummyData' "
712        "  dummy_data_param { "
713        "    shape { "
714        "      dim: 5 "
715        "      dim: 2 "
716        "      dim: 3 "
717        "      dim: 4 "
718        "    } "
719        "    data_filler { "
720        "      type: 'gaussian' "
721        "      std: 0.01 "
722        "    } "
723        "    shape { "
724        "      dim: 5 "
725        "    } "
726        "    data_filler { "
727        "      type: 'constant' "
728        "      value: 0 "
729        "    } "
730        "  } "
731        "  top: 'data' "
732        "  top: 'label' "
733        "} "
734        "layer { "
735        "  name: 'innerproduct' "
736        "  type: 'InnerProduct' "
737        "  inner_product_param { "
738        "    num_output: 1 "
739        "    weight_filler { "
740        "      type: 'gaussian' "
741        "      std: 0.01 "
742        "    } "
743        "  } "
744        "  bottom: 'data' "
745        "  top: 'innerproduct' ";
746      if (test_force_true) {
747        proto += "  propagate_down: true ";
748      }
749      proto +=
750        "} "
751        "layer { "
752        "  name: 'loss' "
753        "  bottom: 'innerproduct' "
754        "  bottom: 'label' "
755        "  top: 'cross_entropy_loss' "
756        "  type: 'SigmoidCrossEntropyLoss' "
757        "} ";
758      InitNetFromProtoString(proto);
759    }
760    virtual void InitAllInOneNet(Phase phase = caffe::TRAIN,
761        const int level = 0, const vector<string>* stages = NULL) {
762      string proto =
763        "name: 'All-in-one Network'"
764        "layer { "
765        "  name: 'train-data' "
766        "  type: 'DummyData' "
767        "  top: 'data' "
768        "  top: 'label' "
769        "  dummy_data_param { "
770        "    shape { dim: 1 dim: 10 } "
771        "    shape { dim: 1 dim: 1 } "
772        "  } "
773        "  include { phase: TRAIN stage: 'train' } "
774        "} "
775        "layer { "
776        "  name: 'val-data' "
777        "  type: 'DummyData' "
778        "  top: 'data' "
779        "  top: 'label' "
780        "  dummy_data_param { "
781        "    shape { dim: 1 dim: 10 } "
782        "    shape { dim: 1 dim: 1 } "
783        "  } "
784        "  include { phase: TEST stage: 'val' } "
785        "} "
786        "layer { "
787        "  name: 'deploy-data' "
788        "  type: 'Input' "
789        "  top: 'data' "
790        "  input_param { "
791        "    shape { dim: 1 dim: 10 } "
792        "  } "
793        "  include { phase: TEST stage: 'deploy' } "
794        "} "
795        "layer { "
796        "  name: 'ip' "
797        "  type: 'InnerProduct' "
798        "  bottom: 'data' "
799        "  top: 'ip' "
800        "  inner_product_param { "
801        "    num_output: 2 "
802        "  } "
803        "} "
804        "layer { "
805        "  name: 'loss' "
806        "  type: 'SoftmaxWithLoss' "
807        "  bottom: 'ip' "
808        "  bottom: 'label' "
809        "  top: 'loss' "
810        "  include { phase: TRAIN stage: 'train' } "
811        "  include { phase: TEST stage: 'val' } "
812        "} ";
813      InitNetFromProtoFileWithState(proto, phase, level, stages);
814    }
815    int seed_;
816    shared_ptr<Net<Dtype> > net_;
817  };
818  template <typename TypeParam>
819  class NetTest : public ParentTest<MultiDeviceTest<TypeParam>> {
820  };
821  template <typename TypeParam>
822  class NetTestCPU : public ParentTest<CPUDeviceTest<TypeParam>> {
823  };
824  #ifdef USE_MKLDNN_AS_DEFAULT_ENGINE
825  TYPED_TEST_CASE(NetTest, MKLDNNTestDtypesAndDevices);
826  #else
827  TYPED_TEST_CASE(NetTest, TestDtypesAndDevices);
828  #endif
829  TYPED_TEST_CASE(NetTestCPU, TestDtypes);
830  TYPED_TEST(NetTest, TestHasBlob) {
831    this->InitTinyNet();
832    EXPECT_TRUE(this->net_->has_blob("data"));
833    EXPECT_TRUE(this->net_->has_blob("label"));
834    EXPECT_TRUE(this->net_->has_blob("innerproduct"));
835    EXPECT_FALSE(this->net_->has_blob("loss"));
836    EXPECT_TRUE(this->net_->has_blob("top_loss"));
837  }
838  TYPED_TEST(NetTest, TestGetBlob) {
839    this->InitTinyNet();
840    EXPECT_EQ(this->net_->blob_by_name("data"), this->net_->blobs()[0]);
841    EXPECT_EQ(this->net_->blob_by_name("label"), this->net_->blobs()[1]);
842    EXPECT_EQ(this->net_->blob_by_name("innerproduct"), this->net_->blobs()[2]);
843    EXPECT_FALSE(this->net_->blob_by_name("loss"));
844    EXPECT_EQ(this->net_->blob_by_name("top_loss"), this->net_->blobs()[3]);
845  }
846  TYPED_TEST(NetTest, TestHasLayer) {
847    this->InitTinyNet();
848    EXPECT_TRUE(this->net_->has_layer("data"));
849    EXPECT_TRUE(this->net_->has_layer("innerproduct"));
850    EXPECT_TRUE(this->net_->has_layer("loss"));
851    EXPECT_FALSE(this->net_->has_layer("label"));
852  }
853  TYPED_TEST(NetTest, TestGetLayerByName) {
854    this->InitTinyNet();
855    EXPECT_EQ(this->net_->layer_by_name("data"), this->net_->layers()[0]);
856    EXPECT_EQ(this->net_->layer_by_name("innerproduct"), this->net_->layers()[1]);
857    EXPECT_EQ(this->net_->layer_by_name("loss"), this->net_->layers()[2]);
858    EXPECT_FALSE(this->net_->layer_by_name("label"));
859  }
860  TYPED_TEST(NetTest, TestBottomNeedBackward) {
861    this->InitTinyNet();
862    const vector<vector<bool> >& bottom_need_backward =
863        this->net_->bottom_need_backward();
864    EXPECT_EQ(3, bottom_need_backward.size());
865    EXPECT_EQ(0, bottom_need_backward[0].size());
866    EXPECT_EQ(1, bottom_need_backward[1].size());
867    EXPECT_EQ(false, bottom_need_backward[1][0]);
868    EXPECT_EQ(2, bottom_need_backward[2].size());
869    EXPECT_EQ(true, bottom_need_backward[2][0]);
870    EXPECT_EQ(false, bottom_need_backward[2][1]);
871  }
872  TYPED_TEST(NetTest, TestBottomNeedBackwardForce) {
873    const bool force_backward = true;
874    this->InitTinyNet(force_backward);
875    const vector<vector<bool> >& bottom_need_backward =
876        this->net_->bottom_need_backward();
877    EXPECT_EQ(3, bottom_need_backward.size());
878    EXPECT_EQ(0, bottom_need_backward[0].size());
879    EXPECT_EQ(1, bottom_need_backward[1].size());
880    EXPECT_EQ(true, bottom_need_backward[1][0]);
881    EXPECT_EQ(2, bottom_need_backward[2].size());
882    EXPECT_EQ(true, bottom_need_backward[2][0]);
883    EXPECT_EQ(false, bottom_need_backward[2][1]);
884  }
885  TYPED_TEST(NetTest, TestBottomNeedBackwardEuclideanForce) {
886    const bool force_backward = true;
887    this->InitTinyNetEuclidean(force_backward);
888    const vector<vector<bool> >& bottom_need_backward =
889        this->net_->bottom_need_backward();
890    EXPECT_EQ(3, bottom_need_backward.size());
891    EXPECT_EQ(0, bottom_need_backward[0].size());
892    EXPECT_EQ(1, bottom_need_backward[1].size());
893    EXPECT_EQ(true, bottom_need_backward[1][0]);
894    EXPECT_EQ(2, bottom_need_backward[2].size());
895    EXPECT_EQ(true, bottom_need_backward[2][0]);
896    EXPECT_EQ(true, bottom_need_backward[2][1]);
897  }
898  TYPED_TEST(NetTest, TestBottomNeedBackwardTricky) {
899    this->InitTrickyNet();
900    const vector<vector<bool> >& bottom_need_backward =
901        this->net_->bottom_need_backward();
902    EXPECT_EQ(4, bottom_need_backward.size());
903    EXPECT_EQ(0, bottom_need_backward[0].size());
904    EXPECT_EQ(1, bottom_need_backward[1].size());
905    EXPECT_EQ(false, bottom_need_backward[1][0]);
906    EXPECT_EQ(1, bottom_need_backward[2].size());
907    EXPECT_EQ(false, bottom_need_backward[2][0]);
908    EXPECT_EQ(2, bottom_need_backward[3].size());
909    EXPECT_EQ(true, bottom_need_backward[3][0]);
910    EXPECT_EQ(true, bottom_need_backward[3][1]);
911  }
912  TYPED_TEST(NetTest, TestLossWeight) {
913    typedef typename TypeParam::Dtype Dtype;
914    vector<Blob<Dtype>*> bottom;
915    Caffe::set_random_seed(this->seed_);
916    const bool kForceBackward = true;
917    this->InitUnsharedWeightsNet(NULL, NULL, kForceBackward);
918    const Dtype loss = this->net_->ForwardBackward();
919    const bool kCopyDiff = true;
920    vector<shared_ptr<Blob<Dtype> > > blob_grads;
921    this->CopyNetBlobs(kCopyDiff, &blob_grads);
922    vector<shared_ptr<Blob<Dtype> > > param_grads;
923    this->CopyNetParams(kCopyDiff, &param_grads);
924    const Dtype kMinLossAbsValue = 1e-2;
925    ASSERT_GE(fabs(loss), kMinLossAbsValue);
926    const Dtype kErrorMargin = 1e-4;
927    const int kNumLossWeights = 6;
928    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
929    for (int i = 0; i < kNumLossWeights; ++i) {
930      Caffe::set_random_seed(this->seed_);
931      this->InitUnsharedWeightsNet(&kLossWeights[i], NULL, kForceBackward);
932      const Dtype weighted_loss = this->net_->ForwardBackward();
933      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
934      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
935          << "loss weight = " << kLossWeights[i];
936      const vector<shared_ptr<Blob<Dtype> > >& weighted_blobs =
937          this->net_->blobs();
938      ASSERT_EQ(blob_grads.size(), weighted_blobs.size());
939      for (int j = 0; j < blob_grads.size(); ++j) {
940        ASSERT_EQ(blob_grads[j]->count(), weighted_blobs[j]->count());
941        for (int k = 0; k < blob_grads[j]->count(); ++k) {
942          EXPECT_NEAR(blob_grads[j]->cpu_diff()[k] * kLossWeights[i],
943                      weighted_blobs[j]->cpu_diff()[k], error_margin);
944        }
945      }
946      const vector<shared_ptr<Blob<Dtype> > >& weighted_params =
947          this->net_->params();
948      ASSERT_EQ(param_grads.size(), weighted_params.size());
949      for (int j = 0; j < param_grads.size(); ++j) {
950        ASSERT_EQ(param_grads[j]->count(), weighted_params[j]->count());
951        for (int k = 0; k < param_grads[j]->count(); ++k) {
952          EXPECT_NEAR(param_grads[j]->cpu_diff()[k] * kLossWeights[i],
953                      weighted_params[j]->cpu_diff()[k], error_margin);
954        }
955      }
956    }
957  }
958  TYPED_TEST(NetTest, TestLossWeightMidNet) {
959    typedef typename TypeParam::Dtype Dtype;
960    Caffe::set_random_seed(this->seed_);
961    const bool kForceBackward = true;
962    Dtype loss_weight = 0;
963    Dtype midnet_loss_weight = 1;
964    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
965                                 kForceBackward);
966    const Dtype loss = this->net_->ForwardBackward();
967    const bool kCopyDiff = true;
968    const bool kReshape = true;
969    Blob<Dtype> data_grad;
970    data_grad.CopyFrom(*this->net_->blob_by_name("data"), kCopyDiff, kReshape);
971    const Dtype kMinLossAbsValue = 1e-2;
972    ASSERT_GE(fabs(loss), kMinLossAbsValue);
973    const Dtype kErrorMargin = 1e-4;
974    const int kNumLossWeights = 6;
975    Dtype kLossWeights[kNumLossWeights] = {2, 0, 1, -1, -2.5, 3.7};
976    for (int i = 0; i < kNumLossWeights; ++i) {
977      Caffe::set_random_seed(this->seed_);
978      this->InitUnsharedWeightsNet(&loss_weight, &kLossWeights[i],
979                                   kForceBackward);
980      const Dtype weighted_loss = this->net_->ForwardBackward();
981      const Dtype error_margin = kErrorMargin * fabs(kLossWeights[i]);
982      EXPECT_NEAR(loss * kLossWeights[i], weighted_loss, error_margin)
983          << "loss weight = " << kLossWeights[i];
984      const shared_ptr<Blob<Dtype> >& weighted_blob =
985          this->net_->blob_by_name("data");
986      ASSERT_EQ(data_grad.count(), weighted_blob->count());
987      for (int j = 0; j < data_grad.count(); ++j) {
988        EXPECT_NEAR(data_grad.cpu_diff()[j] * kLossWeights[i],
989                    weighted_blob->cpu_diff()[j], error_margin);
990      }
991    }
992  }
993  TYPED_TEST(NetTest, TestComboLossWeight) {
994    typedef typename TypeParam::Dtype Dtype;
995    Dtype loss_weight;
996    Dtype midnet_loss_weight;
997    const bool kForceBackward = true;
998    const Dtype kErrorMargin = 1e-4;
999    loss_weight = 1;
1000    midnet_loss_weight = 1;
1001    Caffe::set_random_seed(this->seed_);
1002    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1003                                 kForceBackward);
1004    const Dtype loss = this->net_->ForwardBackward();
1005    const bool kCopyDiff = true;
1006    vector<shared_ptr<Blob<Dtype> > > blob_grads;
1007    this->CopyNetBlobs(kCopyDiff, &blob_grads);
1008    vector<shared_ptr<Blob<Dtype> > > param_grads;
1009    this->CopyNetParams(kCopyDiff, &param_grads);
1010    loss_weight = 2;
1011    midnet_loss_weight = 1;
1012    Caffe::set_random_seed(this->seed_);
1013    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1014                                 kForceBackward);
1015    const Dtype loss_main_2 = this->net_->ForwardBackward();
1016    vector<shared_ptr<Blob<Dtype> > > blob_grads_loss_2;
1017    this->CopyNetBlobs(kCopyDiff, &blob_grads_loss_2);
1018    vector<shared_ptr<Blob<Dtype> > > param_grads_loss_2;
1019    this->CopyNetParams(kCopyDiff, &param_grads_loss_2);
1020    loss_weight = 3;
1021    midnet_loss_weight = 1;
1022    Caffe::set_random_seed(this->seed_);
1023    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1024                                 kForceBackward);
1025    const Dtype loss_main_3 = this->net_->ForwardBackward();
1026    const vector<shared_ptr<Blob<Dtype> > >& blob_grads_loss_3 =
1027        this->net_->blobs();
1028    ASSERT_EQ(blob_grads.size(), blob_grads_loss_3.size());
1029    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_loss_3.size());
1030    for (int j = 0; j < blob_grads.size(); ++j) {
1031      const string& blob_name = this->net_->blob_names()[j];
1032      bool grad_should_change = true;
1033      if (blob_name == "innerproduct1_innerproduct1_0_split_0") {
1034        grad_should_change = false;
1035      }
1036      ASSERT_EQ(blob_grads[j]->count(), blob_grads_loss_3[j]->count());
1037      ASSERT_EQ(blob_grads_loss_2[j]->count(), blob_grads_loss_3[j]->count());
1038      for (int k = 0; k < blob_grads[j]->count(); ++k) {
1039        const Dtype grad_diff_2 = blob_grads_loss_2[j]->cpu_diff()[k] -
1040                                      blob_grads[j]->cpu_diff()[k];
1041        const Dtype grad_diff_3 = blob_grads_loss_3[j]->cpu_diff()[k] -
1042                                      blob_grads[j]->cpu_diff()[k];
1043        if (grad_should_change) {
1044          const Dtype kMinGradDiffAbsValue = 1e-4;
1045          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) << blob_name;
1046          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) << blob_name;
1047        } else {
1048          EXPECT_EQ(0, grad_diff_2) << blob_name;
1049          EXPECT_EQ(0, grad_diff_3) << blob_name;
1050        }
1051      }
1052    }
1053    loss_weight = 1;
1054    midnet_loss_weight = 2;
1055    Caffe::set_random_seed(this->seed_);
1056    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1057                                 kForceBackward);
1058    const Dtype loss_midnet_2 = this->net_->ForwardBackward();
1059    this->CopyNetBlobs(kCopyDiff, &blob_grads_loss_2);
1060    this->CopyNetParams(kCopyDiff, &param_grads_loss_2);
1061    loss_weight = 1;
1062    midnet_loss_weight = 3;
1063    Caffe::set_random_seed(this->seed_);
1064    this->InitUnsharedWeightsNet(&loss_weight, &midnet_loss_weight,
1065                                 kForceBackward);
1066    const Dtype loss_midnet_3 = this->net_->ForwardBackward();
1067    const vector<shared_ptr<Blob<Dtype> > >& blob_grads_midnet_loss_3 =
1068        this->net_->blobs();
1069    ASSERT_EQ(blob_grads.size(), blob_grads_midnet_loss_3.size());
1070    ASSERT_EQ(blob_grads_loss_2.size(), blob_grads_midnet_loss_3.size());
1071    const vector<string>& blob_names = this->net_->blob_names();
1072    for (int j = 0; j < blob_grads.size(); ++j) {
1073      const string& blob_name = blob_names[j];
1074      bool grad_should_change = false;
1075      if (blob_name == "innerproduct1" ||
1076          blob_name == "innerproduct1_innerproduct1_0_split_0" ||
1077          blob_name == "data_data_0_split_0" || blob_name == "data") {
1078        grad_should_change = true;
1079      }
1080      ASSERT_EQ(blob_grads[j]->count(), blob_grads_midnet_loss_3[j]->count());
1081      ASSERT_EQ(blob_grads[j]->count(), blob_grads_loss_2[j]->count());
1082      for (int k = 0; k < blob_grads[j]->count(); ++k) {
1083        const Dtype grad_diff_2 = blob_grads_loss_2[j]->cpu_diff()[k] -
1084                                      blob_grads[j]->cpu_diff()[k];
1085        const Dtype grad_diff_3 = blob_grads_midnet_loss_3[j]->cpu_diff()[k] -
1086                                      blob_grads[j]->cpu_diff()[k];
1087        if (grad_should_change) {
1088          const Dtype kMinGradDiffAbsValue = 1e-4;
1089          EXPECT_GT(fabs(grad_diff_2), kMinGradDiffAbsValue) << blob_name;
1090          EXPECT_NEAR(2 * grad_diff_2, grad_diff_3, kErrorMargin) << blob_name;
1091        } else {
1092          EXPECT_EQ(0, grad_diff_2) << blob_name;
1093          EXPECT_EQ(0, grad_diff_3) << blob_name;
1094        }
1095      }
1096    }
1097    const Dtype kMinLossDiffAbsValue = 1e-4;
1098    Dtype loss_diff_2 = loss_main_2 - loss;
1099    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1100    Dtype loss_diff_3 = loss_main_3 - loss;
1101    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1102    loss_diff_2 = loss_midnet_2 - loss;
1103    EXPECT_GT(fabs(loss_diff_2), kMinLossDiffAbsValue);
1104    loss_diff_3 = loss_midnet_3 - loss;
1105    EXPECT_NEAR(2 * loss_diff_2, loss_diff_3, kErrorMargin);
1106  }
1107  TYPED_TEST(NetTest, TestBackwardWithAccuracyLayer) {
1108    const bool kForceBackward = false;
1109    const bool kAccuracyLayer = true;
1110    this->InitTinyNet(kForceBackward, kAccuracyLayer);
1111    EXPECT_TRUE(this->net_->has_blob("accuracy"));
1112    this->net_->ForwardBackward();
1113  }
1114  TYPED_TEST(NetTest, TestUnsharedWeightsDataNet) {
1115    typedef typename TypeParam::Dtype Dtype;
1116    this->InitUnsharedWeightsNet();
1117    Dtype loss;
1118    this->net_->Forward(&loss);
1119    EXPECT_GT(loss, 0);
1120  }
1121  TYPED_TEST(NetTest, TestSharedWeightsDataNet) {
1122    typedef typename TypeParam::Dtype Dtype;
1123    this->InitSharedWeightsNet();
1124    Dtype loss;
1125    this->net_->Forward(&loss);
1126    EXPECT_FLOAT_EQ(loss, 0);
1127  }
1128  TYPED_TEST(NetTest, TestUnsharedWeightsDiffNet) {
1129    typedef typename TypeParam::Dtype Dtype;
1130    this->InitUnsharedWeightsNet();
1131    Net<Dtype>* net = this->net_.get();
1132    net->Forward();
1133    net->Backward();
1134    Layer<Dtype>* ip1_layer = net->layer_by_name("innerproduct1").get();
1135    Layer<Dtype>* ip2_layer = net->layer_by_name("innerproduct2").get();
1136    const int count = ip1_layer->blobs()[0]->count();
1137    const Dtype* grad1 = ip1_layer->blobs()[0]->cpu_diff();
1138    const Dtype* grad2 = ip2_layer->blobs()[0]->cpu_diff();
1139    for (int i = 0; i < count; ++i) {
1140      EXPECT_GT(fabs(grad1[i]), 0);
1141      EXPECT_FLOAT_EQ(-1 * grad1[i], grad2[i]);
1142    }
1143  }
1144  TYPED_TEST(NetTest, TestSharedWeightsDiffNet) {
1145    typedef typename TypeParam::Dtype Dtype;
1146    this->InitSharedWeightsNet();
1147    Net<Dtype>* net = this->net_.get();
1148    Dtype loss;
1149    net->Forward(&loss);
1150    net->Backward();
1151    EXPECT_FLOAT_EQ(loss, 0);
1152    Layer<Dtype>* ip1_layer = net->layer_by_name("innerproduct1").get();
1153    Layer<Dtype>* ip2_layer = net->layer_by_name("innerproduct2").get();
1154    const int count = ip1_layer->blobs()[0]->count();
1155    const Dtype* grad1 = ip1_layer->blobs()[0]->cpu_diff();
1156    const Dtype* grad2 = ip2_layer->blobs()[0]->cpu_diff();
1157    for (int i = 0; i < count; ++i) {
1158      EXPECT_FLOAT_EQ(0, grad1[i]);
1159      EXPECT_FLOAT_EQ(0, grad2[i]);
1160    }
1161  }
1162  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1163  TYPED_TEST(NetTest, TestSharedWeightsUpdate) {
1164    typedef typename TypeParam::Dtype Dtype;
1165    Caffe::set_random_seed(this->seed_);
1166    this->InitDiffDataSharedWeightsNet();
1167    EXPECT_EQ(this->net_->layer_names()[1], "innerproduct1");
1168    EXPECT_EQ(this->net_->layer_names()[2], "innerproduct2");
1169    Blob<Dtype>* ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1170    Blob<Dtype>* ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1171    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1172    EXPECT_EQ(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1173    this->net_->Forward();
1174    this->net_->Backward();
1175    Blob<Dtype> shared_params;
1176    const bool reshape = true;
1177    const bool copy_diff = false;
1178    shared_params.CopyFrom(*ip1_weights, copy_diff, reshape);
1179    shared_params.CopyFrom(*ip1_weights, !copy_diff, reshape);
1180    const int count = ip1_weights->count();
1181    for (int i = 0; i < count; ++i) {
1182      EXPECT_NE(0, ip1_weights->cpu_diff()[i]);
1183    }
1184    caffe_axpy(count, Dtype(-1), shared_params.cpu_diff(),
1185               shared_params.mutable_cpu_data());
1186    const Dtype* expected_updated_params = shared_params.cpu_data();
1187    this->net_->Update();
1188    const Dtype* actual_updated_params = ip1_weights->cpu_data();
1189    for (int i = 0; i < count; ++i) {
1190      EXPECT_EQ(expected_updated_params[i], actual_updated_params[i]);
1191    }
1192    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1193    Caffe::set_random_seed(this->seed_);
1194    this->InitDiffDataUnsharedWeightsNet();
1195    EXPECT_EQ(this->net_->layer_names()[1], "innerproduct1");
1196    EXPECT_EQ(this->net_->layer_names()[2], "innerproduct2");
1197    ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1198    ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1199    EXPECT_NE(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1200    EXPECT_NE(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1201    this->net_->Forward();
1202    this->net_->Backward();
1203    Blob<Dtype> unshared_params1;
1204    unshared_params1.CopyFrom(*ip1_weights, copy_diff, reshape);
1205    unshared_params1.CopyFrom(*ip1_weights, !copy_diff, reshape);
1206    Blob<Dtype> unshared_params2;
1207    unshared_params2.CopyFrom(*ip2_weights, copy_diff, reshape);
1208    unshared_params2.CopyFrom(*ip2_weights, !copy_diff, reshape);
1209    for (int i = 0; i < count; ++i) {
1210      EXPECT_NE(0, ip1_weights->cpu_diff()[i]);
1211      EXPECT_NE(0, ip2_weights->cpu_diff()[i]);
1212      EXPECT_NE(ip1_weights->cpu_diff()[i], ip2_weights->cpu_diff()[i]);
1213      EXPECT_FLOAT_EQ(ip1_weights->cpu_diff()[i] + ip2_weights->cpu_diff()[i],
1214                      shared_params.cpu_diff()[i]);
1215    }
1216    caffe_axpy(count, Dtype(-1), ip1_weights->cpu_diff(),
1217               unshared_params1.mutable_cpu_data());
1218    caffe_axpy(count, Dtype(-1), ip2_weights->cpu_diff(),
1219               unshared_params2.mutable_cpu_data());
1220    const Dtype* expected_updated_params1 = unshared_params1.cpu_data();
1221    const Dtype* expected_updated_params2 = unshared_params2.cpu_data();
1222    this->net_->Update();
1223    const Dtype* actual_updated_params1 = ip1_weights->cpu_data();
1224    const Dtype* actual_updated_params2 = ip2_weights->cpu_data();
1225    for (int i = 0; i < count; ++i) {
1226      EXPECT_EQ(expected_updated_params1[i], actual_updated_params1[i]);
1227      EXPECT_EQ(expected_updated_params2[i], actual_updated_params2[i]);
1228      EXPECT_NE(actual_updated_params1[i], actual_updated_params2[i]);
1229      EXPECT_NE(expected_updated_params, expected_updated_params1);
1230    }
1231  }
1232  #endif
1233  TYPED_TEST(NetTest, TestSharedWeightsResume) {
1234    typedef typename TypeParam::Dtype Dtype;
1235    Caffe::set_random_seed(this->seed_);
1236    this->InitDiffDataSharedWeightsNet();
1237    EXPECT_EQ(this->net_->layer_names()[1], "innerproduct1");
1238    EXPECT_EQ(this->net_->layer_names()[2], "innerproduct2");
1239    Blob<Dtype>* ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1240    Blob<Dtype>* ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1241    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1242    EXPECT_EQ(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1243    this->net_->ForwardBackward();
1244    this->net_->Update();
1245    Blob<Dtype> shared_params;
1246    const bool kReshape = true;
1247    const bool kCopyDiff = false;
1248    shared_params.CopyFrom(*ip1_weights, kCopyDiff, kReshape);
1249    const int count = ip1_weights->count();
1250    NetParameter net_param;
1251    this->net_->ToProto(&net_param);
1252    Caffe::set_random_seed(this->seed_);
1253    this->InitDiffDataSharedWeightsNet();
1254    this->net_->CopyTrainedLayersFrom(net_param);
1255    ip1_weights = this->net_->layers()[1]->blobs()[0].get();
1256    ip2_weights = this->net_->layers()[2]->blobs()[0].get();
1257    ASSERT_FALSE(NULL == ip1_weights);
1258    ASSERT_FALSE(NULL == ip2_weights);
1259    EXPECT_NE(ip1_weights, ip2_weights);
1260    EXPECT_EQ(ip1_weights->cpu_data(), ip2_weights->cpu_data());
1261    EXPECT_EQ(ip1_weights->cpu_diff(), ip2_weights->cpu_diff());
1262    for (int i = 0; i < count; ++i) {
1263      EXPECT_FLOAT_EQ(shared_params.cpu_data()[i], ip1_weights->cpu_data()[i]);
1264    }
1265  }
1266  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
1267  TYPED_TEST(NetTest, TestParamPropagateDown) {
1268    typedef typename TypeParam::Dtype Dtype;
1269    const bool kBiasTerm = true, kForceBackward = false;
1270    const Dtype* kLossWeight1 = NULL;
1271    const Dtype* kLossWeight2 = NULL;
1272    Caffe::set_random_seed(this->seed_);
1273    Dtype blobs_lr_w1 = 1, blobs_lr_w2 = 1, blobs_lr_b1 = 2, blobs_lr_b2 = 2;
1274    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1275        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1276    this->net_->Forward();
1277    this->net_->Backward();
1278    const vector<shared_ptr<Blob<Dtype> > >& params = this->net_->params();
1279    const int num_params = params.size();
1280    ASSERT_EQ(4, num_params);
1281    const Dtype kNonZeroTestMin = 1e-3;
1282    vector<Dtype> param_asums(params.size());
1283    for (int i = 0; i < num_params; ++i) {
1284      const Dtype param_asum =
1285         caffe_cpu_asum(params[i]->count(), params[i]->cpu_diff());
1286      param_asums[i] = param_asum;
1287      EXPECT_GT(param_asum, kNonZeroTestMin);
1288    }
1289    Caffe::set_random_seed(this->seed_);
1290    blobs_lr_w1 *= 2, blobs_lr_w2 *= 2, blobs_lr_b1 *= 2, blobs_lr_b2 *= 2;
1291    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1292        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1293    this->net_->Forward();
1294    this->net_->Backward();
1295    const vector<shared_ptr<Blob<Dtype> > >& params2 = this->net_->params();
1296    ASSERT_EQ(num_params, params2.size());
1297    for (int i = 0; i < num_params; ++i) {
1298      const Dtype param_asum =
1299         caffe_cpu_asum(params2[i]->count(), params2[i]->cpu_diff());
1300      EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1301    }
1302    Caffe::set_random_seed(this->seed_);
1303    blobs_lr_w1 = 1, blobs_lr_w2 = 0, blobs_lr_b1 = 0, blobs_lr_b2 = 1;
1304    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1305        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1306    this->net_->Forward();
1307    this->net_->Backward();
1308    const vector<shared_ptr<Blob<Dtype> > >& params3 = this->net_->params();
1309    ASSERT_EQ(num_params, params3.size());
1310    for (int i = 0; i < num_params; ++i) {
1311      const Dtype param_asum =
1312         caffe_cpu_asum(params3[i]->count(), params3[i]->cpu_diff());
1313      if (i == 1 || i == 2) {
1314        EXPECT_FLOAT_EQ(0, param_asum);
1315      } else {
1316        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1317      }
1318    }
1319    Caffe::set_random_seed(this->seed_);
1320    blobs_lr_w1 = 0, blobs_lr_w2 = 1, blobs_lr_b1 = 1, blobs_lr_b2 = 0;
1321    this->InitUnsharedWeightsNet(kLossWeight1, kLossWeight2, kForceBackward,
1322        kBiasTerm, blobs_lr_w1, blobs_lr_w2, blobs_lr_b1, blobs_lr_b2);
1323    this->net_->Forward();
1324    this->net_->Backward();
1325    const vector<shared_ptr<Blob<Dtype> > >& params4 = this->net_->params();
1326    ASSERT_EQ(num_params, params4.size());
1327    for (int i = 0; i < num_params; ++i) {
1328      const Dtype param_asum =
1329         caffe_cpu_asum(params4[i]->count(), params4[i]->cpu_diff());
1330      if (i == 0 || i == 3) {
1331        EXPECT_FLOAT_EQ(0, param_asum);
1332      } else {
1333        EXPECT_FLOAT_EQ(param_asum, param_asums[i]);
1334      }
1335    }
1336  }
1337  #endif
1338  TYPED_TEST(NetTest, TestFromTo) {
1339    typedef typename TypeParam::Dtype Dtype;
1340    this->InitTinyNet();
1341    Blob<Dtype> data;
1342    data.ReshapeLike(*this->net_->blob_by_name("data"));
1343    this->net_->Forward();
1344    this->net_->Backward();
1345    data.CopyFrom(*this->net_->blob_by_name("data"), true, true);
1346    const Dtype *loss_ptr = this->net_->output_blobs()[0]->cpu_data();
1347    Dtype loss = *loss_ptr;
1348    for (int i = 1; i < this->net_->layers().size(); ++i) {
1349      this->net_->ForwardFromTo(1, 1);
1350      if (i < this->net_->layers().size() - 1) {
1351        this->net_->ForwardFrom(i + 1);
1352      }
1353      EXPECT_EQ(loss, *loss_ptr);
1354    }
1355    for (int i = 1; i < this->net_->layers().size(); ++i) {
1356      this->net_->BackwardTo(i);
1357      this->net_->BackwardFrom(i - 1);
1358      for (int j = 0; j < data.count(); ++j) {
1359        EXPECT_EQ(data.cpu_diff()[j],
1360            this->net_->blob_by_name("data")->cpu_diff()[j]);
1361      }
1362    }
1363  }
1364  class FilterNetTest : public ::testing::Test {
1365   protected:
1366    void RunFilterNetTest(
1367        const string& input_param_string, const string& filtered_param_string) {
1368      NetParameter input_param;
1369      CHECK(google::protobuf::TextFormat::ParseFromString(
1370          input_param_string, &input_param));
1371      NetParameter expected_filtered_param;
1372      CHECK(google::protobuf::TextFormat::ParseFromString(
1373          filtered_param_string, &expected_filtered_param));
1374      NetParameter actual_filtered_param;
1375      Net<float>::FilterNet(input_param, &actual_filtered_param);
1376      EXPECT_EQ(expected_filtered_param.DebugString(),
1377          actual_filtered_param.DebugString());
1378      NetParameter double_filtered_param;
1379      Net<float>::FilterNet(actual_filtered_param, &double_filtered_param);
1380      EXPECT_EQ(actual_filtered_param.DebugString(),
1381         double_filtered_param.DebugString());
1382    }
1383  };
1384  TEST_F(FilterNetTest, TestNoFilter) {
1385    const string& input_proto =
1386        "name: 'TestNetwork' "
1387        "layer { "
1388        "  name: 'data' "
1389        "  type: 'Data' "
1390        "  top: 'data' "
1391        "  top: 'label' "
1392        "} "
1393        "layer { "
1394        "  name: 'innerprod' "
1395        "  type: 'InnerProduct' "
1396        "  bottom: 'data' "
1397        "  top: 'innerprod' "
1398        "} "
1399        "layer { "
1400        "  name: 'loss' "
1401        "  type: 'SoftmaxWithLoss' "
1402        "  bottom: 'innerprod' "
1403        "  bottom: 'label' "
1404        "} ";
1405    this->RunFilterNetTest(input_proto, input_proto);
1406  }
1407  TEST_F(FilterNetTest, TestFilterLeNetTrainTest) {
1408    const string& input_proto =
1409        "name: 'LeNet' "
1410        "layer { "
1411        "  name: 'mnist' "
1412        "  type: 'Data' "
1413        "  top: 'data' "
1414        "  top: 'label' "
1415        "  data_param { "
1416        "    source: 'mnist-train-leveldb' "
1417        "    batch_size: 64 "
1418        "  } "
1419        "  transform_param { "
1420        "    scale: 0.00390625 "
1421        "  } "
1422        "  include: { phase: TRAIN } "
1423        "} "
1424        "layer { "
1425        "  name: 'mnist' "
1426        "  type: 'Data' "
1427        "  top: 'data' "
1428        "  top: 'label' "
1429        "  data_param { "
1430        "    source: 'mnist-test-leveldb' "
1431        "    batch_size: 100 "
1432        "  } "
1433        "  transform_param { "
1434        "    scale: 0.00390625 "
1435        "  } "
1436        "  include: { phase: TEST } "
1437        "} "
1438        "layer { "
1439        "  name: 'conv1' "
1440        "  type: 'Convolution' "
1441        "  bottom: 'data' "
1442        "  top: 'conv1' "
1443        "  param { "
1444        "    lr_mult: 1 "
1445        "  } "
1446        "  param { "
1447        "    lr_mult: 2 "
1448        "  } "
1449        "  convolution_param { "
1450        "    num_output: 20 "
1451        "    kernel_size: 5 "
1452        "    stride: 1 "
1453        "    weight_filler { "
1454        "      type: 'xavier' "
1455        "    } "
1456        "    bias_filler { "
1457        "      type: 'constant' "
1458        "    } "
1459        "  } "
1460        "} "
1461        "layer { "
1462        "  name: 'ip1' "
1463        "  type: 'InnerProduct' "
1464        "  bottom: 'conv1' "
1465        "  top: 'ip1' "
1466        "  param { "
1467        "    lr_mult: 1 "
1468        "  } "
1469        "  param { "
1470        "    lr_mult: 2 "
1471        "  } "
1472        "  inner_product_param { "
1473        "    num_output: 10 "
1474        "    weight_filler { "
1475        "      type: 'xavier' "
1476        "    } "
1477        "    bias_filler { "
1478        "      type: 'constant' "
1479        "    } "
1480        "  } "
1481        "} "
1482        "layer { "
1483        "  name: 'accuracy' "
1484        "  type: 'Accuracy' "
1485        "  bottom: 'ip1' "
1486        "  bottom: 'label' "
1487        "  top: 'accuracy' "
1488        "  include: { phase: TEST } "
1489        "} "
1490        "layer { "
1491        "  name: 'loss' "
1492        "  type: 'SoftmaxWithLoss' "
1493        "  bottom: 'ip2' "
1494        "  bottom: 'label' "
1495        "  top: 'loss' "
1496        "} ";
1497    const string input_proto_train = "state: { phase: TRAIN } " + input_proto;
1498    const string input_proto_test = "state: { phase: TEST } " + input_proto;
1499    const string output_proto_train =
1500        "name: 'LeNet' "
1501        "layer { "
1502        "  name: 'mnist' "
1503        "  type: 'Data' "
1504        "  top: 'data' "
1505        "  top: 'label' "
1506        "  data_param { "
1507        "    source: 'mnist-train-leveldb' "
1508        "    batch_size: 64 "
1509        "  } "
1510        "  transform_param { "
1511        "    scale: 0.00390625 "
1512        "  } "
1513        "  include: { phase: TRAIN } "
1514        "} "
1515        "layer { "
1516        "  name: 'conv1' "
1517        "  type: 'Convolution' "
1518        "  bottom: 'data' "
1519        "  top: 'conv1' "
1520        "  param { "
1521        "    lr_mult: 1 "
1522        "  } "
1523        "  param { "
1524        "    lr_mult: 2 "
1525        "  } "
1526        "  convolution_param { "
1527        "    num_output: 20 "
1528        "    kernel_size: 5 "
1529        "    stride: 1 "
1530        "    weight_filler { "
1531        "      type: 'xavier' "
1532        "    } "
1533        "    bias_filler { "
1534        "      type: 'constant' "
1535        "    } "
1536        "  } "
1537        "} "
1538        "layer { "
1539        "  name: 'ip1' "
1540        "  type: 'InnerProduct' "
1541        "  bottom: 'conv1' "
1542        "  top: 'ip1' "
1543        "  param { "
1544        "    lr_mult: 1 "
1545        "  } "
1546        "  param { "
1547        "    lr_mult: 2 "
1548        "  } "
1549        "  inner_product_param { "
1550        "    num_output: 10 "
1551        "    weight_filler { "
1552        "      type: 'xavier' "
1553        "    } "
1554        "    bias_filler { "
1555        "      type: 'constant' "
1556        "    } "
1557        "  } "
1558        "} "
1559        "layer { "
1560        "  name: 'loss' "
1561        "  type: 'SoftmaxWithLoss' "
1562        "  bottom: 'ip2' "
1563        "  bottom: 'label' "
1564        "  top: 'loss' "
1565        "} ";
1566    const string& output_proto_test =
1567        "name: 'LeNet' "
1568        "layer { "
1569        "  name: 'mnist' "
1570        "  type: 'Data' "
1571        "  top: 'data' "
1572        "  top: 'label' "
1573        "  data_param { "
1574        "    source: 'mnist-test-leveldb' "
1575        "    batch_size: 100 "
1576        "  } "
1577        "  transform_param { "
1578        "    scale: 0.00390625 "
1579        "  } "
1580        "  include: { phase: TEST } "
1581        "} "
1582        "layer { "
1583        "  name: 'conv1' "
1584        "  type: 'Convolution' "
1585        "  bottom: 'data' "
1586        "  top: 'conv1' "
1587        "  param { "
1588        "    lr_mult: 1 "
1589        "  } "
1590        "  param { "
1591        "    lr_mult: 2 "
1592        "  } "
1593        "  convolution_param { "
1594        "    num_output: 20 "
1595        "    kernel_size: 5 "
1596        "    stride: 1 "
1597        "    weight_filler { "
1598        "      type: 'xavier' "
1599        "    } "
1600        "    bias_filler { "
1601        "      type: 'constant' "
1602        "    } "
1603        "  } "
1604        "} "
1605        "layer { "
1606        "  name: 'ip1' "
1607        "  type: 'InnerProduct' "
1608        "  bottom: 'conv1' "
1609        "  top: 'ip1' "
1610        "  param { "
1611        "    lr_mult: 1 "
1612        "  } "
1613        "  param { "
1614        "    lr_mult: 2 "
1615        "  } "
1616        "  inner_product_param { "
1617        "    num_output: 10 "
1618        "    weight_filler { "
1619        "      type: 'xavier' "
1620        "    } "
1621        "    bias_filler { "
1622        "      type: 'constant' "
1623        "    } "
1624        "  } "
1625        "} "
1626        "layer { "
1627        "  name: 'accuracy' "
1628        "  type: 'Accuracy' "
1629        "  bottom: 'ip1' "
1630        "  bottom: 'label' "
1631        "  top: 'accuracy' "
1632        "  include: { phase: TEST } "
1633        "} "
1634        "layer { "
1635        "  name: 'loss' "
1636        "  type: 'SoftmaxWithLoss' "
1637        "  bottom: 'ip2' "
1638        "  bottom: 'label' "
1639        "  top: 'loss' "
1640        "} ";
1641    const string output_proto_train_explicit =
1642        output_proto_train + " state: { phase: TRAIN } ";
1643    const string output_proto_test_explicit =
1644        output_proto_test + " state: { phase: TEST } ";
1645    this->RunFilterNetTest(input_proto_train, output_proto_train_explicit);
1646    this->RunFilterNetTest(input_proto_test, output_proto_test_explicit);
1647  }
1648  TEST_F(FilterNetTest, TestFilterOutByStage) {
1649    const string& input_proto =
1650        "name: 'TestNetwork' "
1651        "layer { "
1652        "  name: 'data' "
1653        "  type: 'Data' "
1654        "  top: 'data' "
1655        "  top: 'label' "
1656        "  include: { stage: 'mystage' } "
1657        "} "
1658        "layer { "
1659        "  name: 'innerprod' "
1660        "  type: 'InnerProduct' "
1661        "  bottom: 'data' "
1662        "  top: 'innerprod' "
1663        "} "
1664        "layer { "
1665        "  name: 'loss' "
1666        "  type: 'SoftmaxWithLoss' "
1667        "  bottom: 'innerprod' "
1668        "  bottom: 'label' "
1669        "} ";
1670    const string& output_proto =
1671        "name: 'TestNetwork' "
1672        "layer { "
1673        "  name: 'innerprod' "
1674        "  type: 'InnerProduct' "
1675        "  bottom: 'data' "
1676        "  top: 'innerprod' "
1677        "} "
1678        "layer { "
1679        "  name: 'loss' "
1680        "  type: 'SoftmaxWithLoss' "
1681        "  bottom: 'innerprod' "
1682        "  bottom: 'label' "
1683        "} ";
1684    this->RunFilterNetTest(input_proto, output_proto);
1685  }
1686  TEST_F(FilterNetTest, TestFilterOutByStage2) {
1687    const string& input_proto =
1688        "name: 'TestNetwork' "
1689        "layer { "
1690        "  name: 'data' "
1691        "  type: 'Data' "
1692        "  top: 'data' "
1693        "  top: 'label' "
1694        "} "
1695        "layer { "
1696        "  name: 'innerprod' "
1697        "  type: 'InnerProduct' "
1698        "  bottom: 'data' "
1699        "  top: 'innerprod' "
1700        "  include: { stage: 'mystage' } "
1701        "} "
1702        "layer { "
1703        "  name: 'loss' "
1704        "  type: 'SoftmaxWithLoss' "
1705        "  bottom: 'innerprod' "
1706        "  bottom: 'label' "
1707        "} ";
1708    const string& output_proto =
1709        "name: 'TestNetwork' "
1710        "layer { "
1711        "  name: 'data' "
1712        "  type: 'Data' "
1713        "  top: 'data' "
1714        "  top: 'label' "
1715        "} "
1716        "layer { "
1717        "  name: 'loss' "
1718        "  type: 'SoftmaxWithLoss' "
1719        "  bottom: 'innerprod' "
1720        "  bottom: 'label' "
1721        "} ";
1722    this->RunFilterNetTest(input_proto, output_proto);
1723  }
1724  TEST_F(FilterNetTest, TestFilterInByStage) {
1725    const string& input_proto =
1726        "state: { stage: 'mystage' } "
1727        "name: 'TestNetwork' "
1728        "layer { "
1729        "  name: 'data' "
1730        "  type: 'Data' "
1731        "  top: 'data' "
1732        "  top: 'label' "
1733        "} "
1734        "layer { "
1735        "  name: 'innerprod' "
1736        "  type: 'InnerProduct' "
1737        "  bottom: 'data' "
1738        "  top: 'innerprod' "
1739        "  include: { stage: 'mystage' } "
1740        "} "
1741        "layer { "
1742        "  name: 'loss' "
1743        "  type: 'SoftmaxWithLoss' "
1744        "  bottom: 'innerprod' "
1745        "  bottom: 'label' "
1746        "} ";
1747    this->RunFilterNetTest(input_proto, input_proto);
1748  }
1749  TEST_F(FilterNetTest, TestFilterInByStage2) {
1750    const string& input_proto =
1751        "name: 'TestNetwork' "
1752        "layer { "
1753        "  name: 'data' "
1754        "  type: 'Data' "
1755        "  top: 'data' "
1756        "  top: 'label' "
1757        "} "
1758        "layer { "
1759        "  name: 'innerprod' "
1760        "  type: 'InnerProduct' "
1761        "  bottom: 'data' "
1762        "  top: 'innerprod' "
1763        "  exclude: { stage: 'mystage' } "
1764        "} "
1765        "layer { "
1766        "  name: 'loss' "
1767        "  type: 'SoftmaxWithLoss' "
1768        "  bottom: 'innerprod' "
1769        "  bottom: 'label' "
1770        "} ";
1771    this->RunFilterNetTest(input_proto, input_proto);
1772  }
1773  TEST_F(FilterNetTest, TestFilterOutByMultipleStage) {
1774    const string& input_proto =
1775        "state: { stage: 'mystage' } "
1776        "name: 'TestNetwork' "
1777        "layer { "
1778        "  name: 'data' "
1779        "  type: 'Data' "
1780        "  top: 'data' "
1781        "  top: 'label' "
1782        "} "
1783        "layer { "
1784        "  name: 'innerprod' "
1785        "  type: 'InnerProduct' "
1786        "  bottom: 'data' "
1787        "  top: 'innerprod' "
1788        "  include: { stage: 'mystage' stage: 'myotherstage' } "
1789        "} "
1790        "layer { "
1791        "  name: 'loss' "
1792        "  type: 'SoftmaxWithLoss' "
1793        "  bottom: 'innerprod' "
1794        "  bottom: 'label' "
1795        "  include: { stage: 'mystage' } "
1796        "} ";
1797    const string& output_proto =
1798        "state: { stage: 'mystage' } "
1799        "name: 'TestNetwork' "
1800        "layer { "
1801        "  name: 'data' "
1802        "  type: 'Data' "
1803        "  top: 'data' "
1804        "  top: 'label' "
1805        "} "
1806        "layer { "
1807        "  name: 'loss' "
1808        "  type: 'SoftmaxWithLoss' "
1809        "  bottom: 'innerprod' "
1810        "  bottom: 'label' "
1811        "  include: { stage: 'mystage' } "
1812        "} ";
1813    this->RunFilterNetTest(input_proto, output_proto);
1814  }
1815  TEST_F(FilterNetTest, TestFilterInByMultipleStage) {
1816    const string& input_proto =
1817        "state: { stage: 'mystage' } "
1818        "name: 'TestNetwork' "
1819        "layer { "
1820        "  name: 'data' "
1821        "  type: 'Data' "
1822        "  top: 'data' "
1823        "  top: 'label' "
1824        "} "
1825        "layer { "
1826        "  name: 'innerprod' "
1827        "  type: 'InnerProduct' "
1828        "  bottom: 'data' "
1829        "  top: 'innerprod' "
1830        "  include: { stage: 'myotherstage' } "
1831        "  include: { stage: 'mystage' } "
1832        "} "
1833        "layer { "
1834        "  name: 'loss' "
1835        "  type: 'SoftmaxWithLoss' "
1836        "  bottom: 'innerprod' "
1837        "  bottom: 'label' "
1838        "  include: { stage: 'mystage' } "
1839        "} ";
1840    this->RunFilterNetTest(input_proto, input_proto);
1841  }
1842  TEST_F(FilterNetTest, TestFilterInByMultipleStage2) {
1843    const string& input_proto =
1844        "state: { stage: 'mystage' stage: 'myotherstage' } "
1845        "name: 'TestNetwork' "
1846        "layer { "
1847        "  name: 'data' "
1848        "  type: 'Data' "
1849        "  top: 'data' "
1850        "  top: 'label' "
1851        "} "
1852        "layer { "
1853        "  name: 'innerprod' "
1854        "  type: 'InnerProduct' "
1855        "  bottom: 'data' "
1856        "  top: 'innerprod' "
1857        "  include: { stage: 'mystage' stage: 'myotherstage' } "
1858        "} "
1859        "layer { "
1860        "  name: 'loss' "
1861        "  type: 'SoftmaxWithLoss' "
1862        "  bottom: 'innerprod' "
1863        "  bottom: 'label' "
1864        "  include: { stage: 'mystage' } "
1865        "} ";
1866    this->RunFilterNetTest(input_proto, input_proto);
1867  }
1868  TEST_F(FilterNetTest, TestFilterInByNotStage) {
1869    const string& input_proto =
1870        "state: { stage: 'mystage' } "
1871        "name: 'TestNetwork' "
1872        "layer { "
1873        "  name: 'data' "
1874        "  type: 'Data' "
1875        "  top: 'data' "
1876        "  top: 'label' "
1877        "} "
1878        "layer { "
1879        "  name: 'innerprod' "
1880        "  type: 'InnerProduct' "
1881        "  bottom: 'data' "
1882        "  top: 'innerprod' "
1883        "  include: { not_stage: 'myotherstage' } "
1884        "} "
1885        "layer { "
1886        "  name: 'loss' "
1887        "  type: 'SoftmaxWithLoss' "
1888        "  bottom: 'innerprod' "
1889        "  bottom: 'label' "
1890        "  include: { not_stage: 'myotherstage' } "
1891        "} ";
1892    this->RunFilterNetTest(input_proto, input_proto);
1893  }
1894  TEST_F(FilterNetTest, TestFilterOutByNotStage) {
1895    const string& input_proto =
1896        "state: { stage: 'mystage' } "
1897        "name: 'TestNetwork' "
1898        "layer { "
1899        "  name: 'data' "
1900        "  type: 'Data' "
1901        "  top: 'data' "
1902        "  top: 'label' "
1903        "} "
1904        "layer { "
1905        "  name: 'innerprod' "
1906        "  type: 'InnerProduct' "
1907        "  bottom: 'data' "
1908        "  top: 'innerprod' "
1909        "  include: { not_stage: 'mystage' } "
1910        "} "
1911        "layer { "
1912        "  name: 'loss' "
1913        "  type: 'SoftmaxWithLoss' "
1914        "  bottom: 'innerprod' "
1915        "  bottom: 'label' "
1916        "  include: { not_stage: 'mystage' } "
1917        "} ";
1918    const string& output_proto =
1919        "state: { stage: 'mystage' } "
1920        "name: 'TestNetwork' "
1921        "layer { "
1922        "  name: 'data' "
1923        "  type: 'Data' "
1924        "  top: 'data' "
1925        "  top: 'label' "
1926        "} ";
1927    this->RunFilterNetTest(input_proto, output_proto);
1928  }
1929  TEST_F(FilterNetTest, TestFilterOutByMinLevel) {
1930    const string& input_proto =
1931        "name: 'TestNetwork' "
1932        "layer { "
1933        "  name: 'data' "
1934        "  type: 'Data' "
1935        "  top: 'data' "
1936        "  top: 'label' "
1937        "} "
1938        "layer { "
1939        "  name: 'innerprod' "
1940        "  type: 'InnerProduct' "
1941        "  bottom: 'data' "
1942        "  top: 'innerprod' "
1943        "  include: { min_level: 3 } "
1944        "} "
1945        "layer { "
1946        "  name: 'loss' "
1947        "  type: 'SoftmaxWithLoss' "
1948        "  bottom: 'innerprod' "
1949        "  bottom: 'label' "
1950        "} ";
1951    const string& output_proto =
1952        "name: 'TestNetwork' "
1953        "layer { "
1954        "  name: 'data' "
1955        "  type: 'Data' "
1956        "  top: 'data' "
1957        "  top: 'label' "
1958        "} "
1959        "layer { "
1960        "  name: 'loss' "
1961        "  type: 'SoftmaxWithLoss' "
1962        "  bottom: 'innerprod' "
1963        "  bottom: 'label' "
1964        "} ";
1965    this->RunFilterNetTest(input_proto, output_proto);
1966  }
1967  TEST_F(FilterNetTest, TestFilterOutByMaxLevel) {
1968    const string& input_proto =
1969        "name: 'TestNetwork' "
1970        "layer { "
1971        "  name: 'data' "
1972        "  type: 'Data' "
1973        "  top: 'data' "
1974        "  top: 'label' "
1975        "} "
1976        "layer { "
1977        "  name: 'innerprod' "
1978        "  type: 'InnerProduct' "
1979        "  bottom: 'data' "
1980        "  top: 'innerprod' "
1981        "  include: { max_level: -3 } "
1982        "} "
1983        "layer { "
1984        "  name: 'loss' "
1985        "  type: 'SoftmaxWithLoss' "
1986        "  bottom: 'innerprod' "
1987        "  bottom: 'label' "
1988        "} ";
1989    const string& output_proto =
1990        "name: 'TestNetwork' "
1991        "layer { "
1992        "  name: 'data' "
1993        "  type: 'Data' "
1994        "  top: 'data' "
1995        "  top: 'label' "
1996        "} "
1997        "layer { "
1998        "  name: 'loss' "
1999        "  type: 'SoftmaxWithLoss' "
2000        "  bottom: 'innerprod' "
2001        "  bottom: 'label' "
2002        "} ";
2003    this->RunFilterNetTest(input_proto, output_proto);
2004  }
2005  TEST_F(FilterNetTest, TestFilterInByMinLevel) {
2006    const string& input_proto =
2007        "name: 'TestNetwork' "
2008        "layer { "
2009        "  name: 'data' "
2010        "  type: 'Data' "
2011        "  top: 'data' "
2012        "  top: 'label' "
2013        "} "
2014        "layer { "
2015        "  name: 'innerprod' "
2016        "  type: 'InnerProduct' "
2017        "  bottom: 'data' "
2018        "  top: 'innerprod' "
2019        "  include: { min_level: 0 } "
2020        "} "
2021        "layer { "
2022        "  name: 'loss' "
2023        "  type: 'SoftmaxWithLoss' "
2024        "  bottom: 'innerprod' "
2025        "  bottom: 'label' "
2026        "} ";
2027    this->RunFilterNetTest(input_proto, input_proto);
2028  }
2029  TEST_F(FilterNetTest, TestFilterInByMinLevel2) {
2030    const string& input_proto =
2031        "state: { level: 7 } "
2032        "name: 'TestNetwork' "
2033        "layer { "
2034        "  name: 'data' "
2035        "  type: 'Data' "
2036        "  top: 'data' "
2037        "  top: 'label' "
2038        "} "
2039        "layer { "
2040        "  name: 'innerprod' "
2041        "  type: 'InnerProduct' "
2042        "  bottom: 'data' "
2043        "  top: 'innerprod' "
2044        "  include: { min_level: 3 } "
2045        "} "
2046        "layer { "
2047        "  name: 'loss' "
2048        "  type: 'SoftmaxWithLoss' "
2049        "  bottom: 'innerprod' "
2050        "  bottom: 'label' "
2051        "} ";
2052    this->RunFilterNetTest(input_proto, input_proto);
2053  }
2054  TEST_F(FilterNetTest, TestFilterInByMaxLevel) {
2055    const string& input_proto =
2056        "name: 'TestNetwork' "
2057        "layer { "
2058        "  name: 'data' "
2059        "  type: 'Data' "
2060        "  top: 'data' "
2061        "  top: 'label' "
2062        "} "
2063        "layer { "
2064        "  name: 'innerprod' "
2065        "  type: 'InnerProduct' "
2066        "  bottom: 'data' "
2067        "  top: 'innerprod' "
2068        "  include: { max_level: 0 } "
2069        "} "
2070        "layer { "
2071        "  name: 'loss' "
2072        "  type: 'SoftmaxWithLoss' "
2073        "  bottom: 'innerprod' "
2074        "  bottom: 'label' "
2075        "} ";
2076    this->RunFilterNetTest(input_proto, input_proto);
2077  }
2078  TEST_F(FilterNetTest, TestFilterInByMaxLevel2) {
2079    const string& input_proto =
2080        "state: { level: -7 } "
2081        "name: 'TestNetwork' "
2082        "layer { "
2083        "  name: 'data' "
2084        "  type: 'Data' "
2085        "  top: 'data' "
2086        "  top: 'label' "
2087        "} "
2088        "layer { "
2089        "  name: 'innerprod' "
2090        "  type: 'InnerProduct' "
2091        "  bottom: 'data' "
2092        "  top: 'innerprod' "
2093        "  include: { max_level: -3 } "
2094        "} "
2095        "layer { "
2096        "  name: 'loss' "
2097        "  type: 'SoftmaxWithLoss' "
2098        "  bottom: 'innerprod' "
2099        "  bottom: 'label' "
2100        "} ";
2101    this->RunFilterNetTest(input_proto, input_proto);
2102  }
2103  TEST_F(FilterNetTest, TestFilterInOutByIncludeMultiRule) {
2104    const string& input_proto =
2105        "name: 'TestNetwork' "
2106        "layer { "
2107        "  name: 'data' "
2108        "  type: 'Data' "
2109        "  top: 'data' "
2110        "  top: 'label' "
2111        "} "
2112        "layer { "
2113        "  name: 'innerprod' "
2114        "  type: 'InnerProduct' "
2115        "  bottom: 'data' "
2116        "  top: 'innerprod' "
2117        "  include: { min_level: 2  phase: TRAIN } "
2118        "} "
2119        "layer { "
2120        "  name: 'loss' "
2121        "  type: 'SoftmaxWithLoss' "
2122        "  bottom: 'innerprod' "
2123        "  bottom: 'label' "
2124        "  include: { min_level: 2  phase: TEST } "
2125        "} ";
2126    const string& input_proto_train =
2127        "state: { level: 4  phase: TRAIN } " + input_proto;
2128    const string& input_proto_test =
2129        "state: { level: 4  phase: TEST } " + input_proto;
2130    const string& output_proto_train =
2131        "state: { level: 4  phase: TRAIN } "
2132        "name: 'TestNetwork' "
2133        "layer { "
2134        "  name: 'data' "
2135        "  type: 'Data' "
2136        "  top: 'data' "
2137        "  top: 'label' "
2138        "} "
2139        "layer { "
2140        "  name: 'innerprod' "
2141        "  type: 'InnerProduct' "
2142        "  bottom: 'data' "
2143        "  top: 'innerprod' "
2144        "  include: { min_level: 2  phase: TRAIN } "
2145        "} ";
2146    const string& output_proto_test =
2147        "state: { level: 4  phase: TEST } "
2148        "name: 'TestNetwork' "
2149        "layer { "
2150        "  name: 'data' "
2151        "  type: 'Data' "
2152        "  top: 'data' "
2153        "  top: 'label' "
2154        "} "
2155        "layer { "
2156        "  name: 'loss' "
2157        "  type: 'SoftmaxWithLoss' "
2158        "  bottom: 'innerprod' "
2159        "  bottom: 'label' "
2160        "  include: { min_level: 2  phase: TEST } "
2161        "} ";
2162    this->RunFilterNetTest(input_proto_train, output_proto_train);
2163    this->RunFilterNetTest(input_proto_test, output_proto_test);
2164  }
2165  TEST_F(FilterNetTest, TestFilterInByIncludeMultiRule) {
2166    const string& input_proto =
2167        "name: 'TestNetwork' "
2168        "layer { "
2169        "  name: 'data' "
2170        "  type: 'Data' "
2171        "  top: 'data' "
2172        "  top: 'label' "
2173        "} "
2174        "layer { "
2175        "  name: 'innerprod' "
2176        "  type: 'InnerProduct' "
2177        "  bottom: 'data' "
2178        "  top: 'innerprod' "
2179        "  include: { min_level: 2  phase: TRAIN } "
2180        "  include: { phase: TEST } "
2181        "} "
2182        "layer { "
2183        "  name: 'loss' "
2184        "  type: 'SoftmaxWithLoss' "
2185        "  bottom: 'innerprod' "
2186        "  bottom: 'label' "
2187        "  include: { min_level: 2  phase: TEST } "
2188        "  include: { phase: TRAIN } "
2189        "} ";
2190    const string& input_proto_train =
2191        "state: { level: 2  phase: TRAIN } " + input_proto;
2192    const string& input_proto_test =
2193        "state: { level: 2  phase: TEST } " + input_proto;
2194    this->RunFilterNetTest(input_proto_train, input_proto_train);
2195    this->RunFilterNetTest(input_proto_test, input_proto_test);
2196  }
2197  TEST_F(FilterNetTest, TestFilterInOutByExcludeMultiRule) {
2198    const string& input_proto =
2199        "name: 'TestNetwork' "
2200        "layer { "
2201        "  name: 'data' "
2202        "  type: 'Data' "
2203        "  top: 'data' "
2204        "  top: 'label' "
2205        "} "
2206        "layer { "
2207        "  name: 'innerprod' "
2208        "  type: 'InnerProduct' "
2209        "  bottom: 'data' "
2210        "  top: 'innerprod' "
2211        "  exclude: { min_level: 2  phase: TRAIN } "
2212        "} "
2213        "layer { "
2214        "  name: 'loss' "
2215        "  type: 'SoftmaxWithLoss' "
2216        "  bottom: 'innerprod' "
2217        "  bottom: 'label' "
2218        "  exclude: { min_level: 2  phase: TEST } "
2219        "} ";
2220    const string& input_proto_train =
2221        "state: { level: 4  phase: TRAIN } " + input_proto;
2222    const string& input_proto_test =
2223        "state: { level: 4  phase: TEST } " + input_proto;
2224    const string& output_proto_train =
2225        "state: { level: 4  phase: TRAIN } "
2226        "name: 'TestNetwork' "
2227        "layer { "
2228        "  name: 'data' "
2229        "  type: 'Data' "
2230        "  top: 'data' "
2231        "  top: 'label' "
2232        "} "
2233        "layer { "
2234        "  name: 'loss' "
2235        "  type: 'SoftmaxWithLoss' "
2236        "  bottom: 'innerprod' "
2237        "  bottom: 'label' "
2238        "  exclude: { min_level: 2  phase: TEST } "
2239        "} ";
2240    const string& output_proto_test =
2241        "state: { level: 4  phase: TEST } "
2242        "name: 'TestNetwork' "
2243        "layer { "
2244        "  name: 'data' "
2245        "  type: 'Data' "
2246        "  top: 'data' "
2247        "  top: 'label' "
2248        "} "
2249        "layer { "
2250        "  name: 'innerprod' "
2251        "  type: 'InnerProduct' "
2252        "  bottom: 'data' "
2253        "  top: 'innerprod' "
2254        "  exclude: { min_level: 2  phase: TRAIN } "
2255        "} ";
2256    this->RunFilterNetTest(input_proto_train, output_proto_train);
2257    this->RunFilterNetTest(input_proto_test, output_proto_test);
2258  }
2259  #ifndef USE_MKLDNN_AS_DEFAULT_ENGINE
2260  TYPED_TEST(NetTest, TestReshape) {
2261    typedef typename TypeParam::Dtype Dtype;
2262    Caffe::set_random_seed(this->seed_);
2263    Caffe::set_mode(Caffe::CPU);
2264    FillerParameter filler_param;
2265    filler_param.set_std(1);
2266    GaussianFiller<Dtype> filler(filler_param);
2267    Blob<Dtype> blob1(2, 3, 12, 10);
2268    Blob<Dtype> blob2(4, 3, 9, 11);
2269    ASSERT_LT(blob1.count(), blob2.count());
2270    filler.Fill(&blob1);
2271    filler.Fill(&blob2);
2272    this->InitReshapableNet();
2273    shared_ptr<Blob<Dtype> > input_blob = this->net_->blob_by_name("data");
2274    Blob<Dtype>* output_blob = this->net_->output_blobs()[0];
2275    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2276        blob1.width());
2277    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2278    this->net_->Forward();
2279    this->net_->Backward();
2280    Blob<Dtype> output1(output_blob->num(), output_blob->channels(),
2281        output_blob->height(), output_blob->width());
2282    caffe_copy(output1.count(), output_blob->cpu_data(),
2283        output1.mutable_cpu_data());
2284    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2285        blob2.width());
2286    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2287    this->net_->Forward();
2288    this->net_->Backward();
2289    Blob<Dtype> output2(output_blob->num(), output_blob->channels(),
2290        output_blob->height(), output_blob->width());
2291    caffe_copy(output2.count(), output_blob->cpu_data(),
2292        output2.mutable_cpu_data());
2293    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2294        blob1.width());
2295    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2296    this->net_->Forward();
2297    this->net_->Backward();
2298    for (int i = 0; i < output1.count(); ++i) {
2299      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob->cpu_data() + i));
2300    }
2301    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2302        blob2.width());
2303    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2304    this->net_->Forward();
2305    this->net_->Backward();
2306    for (int i = 0; i < output2.count(); ++i) {
2307      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob->cpu_data() + i));
2308    }
2309    EXPECT_EQ(output1.num(), blob1.num());
2310    EXPECT_EQ(output2.num(), blob2.num());
2311    bool same_spatial_shape = true;
2312    const int kFirstSpatialAxis = 2;
2313    for (int i = kFirstSpatialAxis; i < output1.num_axes(); ++i) {
2314      if (output1.shape(i) != output2.shape(i)) {
2315        same_spatial_shape = false;
2316        break;
2317      }
2318    }
2319    EXPECT_FALSE(same_spatial_shape);
2320  }
2321  #endif
2322  #ifdef MKL2017_SUPPORTED
2323  TYPED_TEST(NetTestCPU, TestForwardReshapeForward) {
2324    typedef TypeParam Dtype;
2325    const string& proto =
2326        "name: 'TestNetwork' "
2327        " layer {"
2328        "   top: 'data'"
2329        "   top: 'label'"
2330        "   name: 'data'"
2331        "   type: 'DummyData'"
2332        "   dummy_data_param {"
2333        "     shape: { dim: 32 dim: 3 dim: 227 dim: 227 }"
2334        "     data_filler {"
2335        "       type: 'constant'"
2336        "       value: 0.01"
2337        "     }"
2338        "   }"
2339        "   transform_param {"
2340        "     mirror: true"
2341        "     crop_size: 224"
2342        "     mean_value: 104"
2343        "     mean_value: 117"
2344        "     mean_value: 123"
2345        "   }"
2346        " }"
2347        " layer {"
2348        "  bottom: 'data'"
2349        "   top: 'conv'"
2350        "   name: 'conv1'"
2351        "   type: 'Convolution'"
2352        "   param {"
2353        "     lr_mult: 1"
2354        "     decay_mult: 1"
2355        "   }"
2356        "   convolution_param {"
2357        "     "
2358        "     num_output: 64"
2359        "     engine: MKL2017 "
2360        "     pad: 3"
2361        "     kernel_size: 7"
2362        "     stride: 2"
2363        "     weight_filler {"
2364        "       type: 'xavier'"
2365        "     }"
2366        "     bias_term: false"
2367        "   }"
2368        " }"
2369        " layer {"
2370        "   bottom: 'conv'"
2371        "   top: 'relu1'"
2372        "   name: 'relu1'"
2373        "   type: 'ReLU'"
2374        "   relu_param {"
2375        "     engine: MKL2017 "
2376        "     "
2377        "   }"
2378        " }"
2379        " layer {"
2380        "   bottom: 'conv'"
2381        "   top: 'relu2'"
2382        "   name: 'relu2'"
2383        "   type: 'ReLU'"
2384        "   relu_param {"
2385        "     engine: MKL2017 "
2386        "     "
2387        "   }"
2388        " }"
2389        " layer {"
2390        "   bottom: 'relu1'"
2391        "   bottom: 'relu2'"
2392        "   top: 'concat'"
2393        "   name: 'concat'"
2394        "   type: 'Concat'"
2395        "   concat_param {"
2396        "     engine: MKL2017 "
2397        "     "
2398        "   }"
2399        " } "
2400        " layer {"
2401        "   bottom: 'concat'"
2402        "   top: 'lrn'"
2403        "   name: 'LRN'"
2404        "   type: 'LRN'"
2405        "   lrn_param {"
2406        "     engine: MKL2017 "
2407        "     local_size: 5"
2408        "     alpha: 0.0001"
2409        "     beta: 0.75"
2410        "   }"
2411        " }"
2412        " layer {"
2413        "   bottom: 'lrn'"
2414        "   top: 'pooling'"
2415        "   name: 'Pooling'"
2416        "   type: 'Pooling'"
2417        "   pooling_param {"
2418        "     engine: MKL2017 "
2419        "     kernel_size: 5"
2420        "     stride: 2"
2421        "     pool: MAX"
2422        "   }"
2423        " }"
2424        " layer {"
2425        "   bottom: 'pooling'"
2426        "   top: 'bn'"
2427        "   name: 'BatchNorm'"
2428        "   type: 'BatchNorm'"
2429        "   batch_norm_param {"
2430        "     engine: MKL2017 "
2431        "   }"
2432        " }";
2433      this->InitNetFromProtoString(proto);
2434      this->net_->Forward();
2435      shared_ptr<Blob<Dtype> > input_blob = this->net_->blob_by_name("data");
2436      input_blob->Reshape(1, 3, 1280, 720);
2437      this->net_->Forward();
2438  }
2439  #if 0
2440  TYPED_TEST(NetTest, TestTotalForwardReshape) {
2441    typedef typename TypeParam::Dtype Dtype;
2442    Caffe::set_random_seed(this->seed_);
2443    Caffe::set_mode(Caffe::CPU);
2444    FillerParameter filler_param;
2445    filler_param.set_std(1);
2446    GaussianFiller<Dtype> filler(filler_param);
2447    Blob<Dtype> blob1(2, 3, 12, 10);
2448    Blob<Dtype> blob2(4, 3, 9, 11);
2449    ASSERT_LT(blob1.count(), blob2.count());
2450    filler.Fill(&blob1);
2451    filler.Fill(&blob2);
2452    const string& proto =
2453        "name: 'TestNetwork' "
2454        " layer {"
2455        "   top: 'data'"
2456        "   top: 'label'"
2457        "   name: 'data'"
2458        "   type: 'DummyData'"
2459        "   dummy_data_param {"
2460        "     shape: { dim: 3 dim: 3 dim: 13 dim: 11 }"
2461        "     data_filler {"
2462        "       type: 'constant'"
2463        "       value: 0.01"
2464        "     }"
2465        "   }"
2466        "   transform_param {"
2467        "     mirror: true"
2468        "     crop_size: 224"
2469        "     mean_value: 104"
2470        "     mean_value: 117"
2471        "     mean_value: 123"
2472        "   }"
2473        " }"
2474        " layer {"
2475        "  bottom: 'data'"
2476        "   top: 'conv'"
2477        "   name: 'conv1'"
2478        "   type: 'Convolution'"
2479        "   param {"
2480        "     lr_mult: 1"
2481        "     decay_mult: 1"
2482        "   }"
2483        "   convolution_param {"
2484        "     "
2485        "     num_output: 64"
2486        "     engine: MKL2017 "
2487        "     pad: 3"
2488        "     kernel_size: 7"
2489        "     stride: 2"
2490        "     weight_filler {"
2491        "       type: 'xavier'"
2492        "     }"
2493        "     bias_term: false"
2494        "   }"
2495        " }"
2496        " layer {"
2497        "   bottom: 'conv'"
2498        "   top: 'relu1'"
2499        "   name: 'relu1'"
2500        "   type: 'ReLU'"
2501        "   relu_param {"
2502        "     engine: MKL2017 "
2503        "     "
2504        "   }"
2505        " }"
2506        " layer {"
2507        "   bottom: 'conv'"
2508        "   top: 'relu2'"
2509        "   name: 'relu2'"
2510        "   type: 'ReLU'"
2511        "   relu_param {"
2512        "     engine: MKL2017 "
2513        "     "
2514        "   }"
2515        " }"
2516        " layer {"
2517        "   bottom: 'relu1'"
2518        "   bottom: 'relu2'"
2519        "   top: 'concat'"
2520        "   name: 'concat'"
2521        "   type: 'Concat'"
2522        "   concat_param {"
2523        "     engine: MKL2017 "
2524        "     "
2525        "   }"
2526        " } "
2527        " layer {"
2528        "   bottom: 'concat'"
2529        "   top: 'lrn'"
2530        "   name: 'LRN'"
2531        "   type: 'LRN'"
2532        "   lrn_param {"
2533        "     engine: MKL2017 "
2534        "     local_size: 5"
2535        "     alpha: 0.0001"
2536        "     beta: 0.75"
2537        "   }"
2538        " }"
2539        " layer {"
2540        "   bottom: 'lrn'"
2541        "   top: 'pooling'"
2542        "   name: 'Pooling'"
2543        "   type: 'Pooling'"
2544        "   pooling_param {"
2545        "     engine: MKL2017 "
2546        "     kernel_size: 5"
2547        "     stride: 2"
2548        "     pool: MAX"
2549        "   }"
2550        " }"
2551        " layer {"
2552        "   bottom: 'pooling'"
2553        "   top: 'bn'"
2554        "   name: 'BatchNorm'"
2555        "   type: 'BatchNorm'"
2556        "   batch_norm_param {"
2557        "     engine: MKL2017 "
2558        "   }"
2559        " }";
2560      this->InitNetFromProtoString(proto);
2561    shared_ptr<Blob<Dtype> > input_blob = this->net_->blob_by_name("data");
2562    Blob<Dtype>* output_blob = this->net_->output_blobs()[0];
2563    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2564        blob1.width());
2565    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2566    this->net_->Forward();
2567    this->net_->Backward();
2568    Blob<Dtype> output1(output_blob->num(), output_blob->channels(),
2569        output_blob->height(), output_blob->width());
2570    caffe_copy(output1.count(), output_blob->cpu_data(),
2571        output1.mutable_cpu_data());
2572    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2573        blob2.width());
2574    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2575    this->net_->Forward();
2576    this->net_->Backward();
2577    Blob<Dtype> output2(output_blob->num(), output_blob->channels(),
2578        output_blob->height(), output_blob->width());
2579    caffe_copy(output2.count(), output_blob->cpu_data(),
2580        output2.mutable_cpu_data());
2581    input_blob->Reshape(blob1.num(), blob1.channels(), blob1.height(),
2582        blob1.width());
2583    caffe_copy(blob1.count(), blob1.cpu_data(), input_blob->mutable_cpu_data());
2584    this->net_->Forward();
2585    this->net_->Backward();
2586    for (int i = 0; i < output1.count(); ++i) {
2587      EXPECT_FLOAT_EQ(*(output1.cpu_data() + i), *(output_blob->cpu_data() + i));
2588    }
2589    input_blob->Reshape(blob2.num(), blob2.channels(), blob2.height(),
2590        blob2.width());
2591    caffe_copy(blob2.count(), blob2.cpu_data(), input_blob->mutable_cpu_data());
2592    this->net_->Forward();
2593    this->net_->Backward();
2594    for (int i = 0; i < output2.count(); ++i) {
2595      EXPECT_FLOAT_EQ(*(output2.cpu_data() + i), *(output_blob->cpu_data() + i));
2596    }
2597    EXPECT_EQ(output1.num(), blob1.num());
2598    EXPECT_EQ(output2.num(), blob2.num());
2599    bool same_spatial_shape = true;
2600    const int kFirstSpatialAxis = 2;
2601    for (int i = kFirstSpatialAxis; i < output1.num_axes(); ++i) {
2602      if (output1.shape(i) != output2.shape(i)) {
2603        same_spatial_shape = false;
2604        break;
2605      }
2606    }
2607    EXPECT_FALSE(same_spatial_shape);
2608  }
2609  #endif
2610  #endif
2611  TYPED_TEST(NetTest, TestSkipPropagateDown) {
2612    this->InitSkipPropNet(false);
2613    vector<bool> vec_layer_need_backward = this->net_->layer_need_backward();
2614    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2615      string layer_name = this->net_->layer_names()[layer_id];
2616      if (layer_name == "loss") {
2617        bool need_back = this->net_->bottom_need_backward()[layer_id][1];
2618        EXPECT_TRUE(need_back) << "bottom_need_backward should be True";
2619      }
2620      if (layer_name.find("data") != std::string::npos ||
2621            layer_name == "silence") {
2622        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2623            << "layer_need_backward for " << layer_name << " should be False";
2624      } else {
2625        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2626            << "layer_need_backward for " << layer_name << " should be True";
2627      }
2628    }
2629    this->InitSkipPropNet(true);
2630    vec_layer_need_backward.clear();
2631    vec_layer_need_backward = this->net_->layer_need_backward();
2632    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2633      string layer_name = this->net_->layer_names()[layer_id];
2634      if (layer_name == "loss") {
2635        bool need_back = this->net_->bottom_need_backward()[layer_id][1];
2636        EXPECT_FALSE(need_back) << "bottom_need_backward should be False";
2637      }
2638      if (layer_name == "innerproduct" || layer_name == "loss") {
2639        EXPECT_TRUE(vec_layer_need_backward[layer_id])
2640            << "layer_need_backward for " << layer_name << " should be True";
2641      } else {
2642        EXPECT_FALSE(vec_layer_need_backward[layer_id])
2643            << "layer_need_backward for " << layer_name << " should be False";
2644      }
2645    }
2646  }
2647  TYPED_TEST(NetTest, TestForcePropagateDown) {
2648    this->InitForcePropNet(false);
2649    vector<bool> layer_need_backward = this->net_->layer_need_backward();
2650    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2651      const string& layer_name = this->net_->layer_names()[layer_id];
2652      const vector<bool> need_backward =
2653          this->net_->bottom_need_backward()[layer_id];
2654      if (layer_name == "data") {
2655        ASSERT_EQ(need_backward.size(), 0);
2656        EXPECT_FALSE(layer_need_backward[layer_id]);
2657      } else if (layer_name == "innerproduct") {
2658        ASSERT_EQ(need_backward.size(), 1);
2659        EXPECT_FALSE(need_backward[0]);  
2660        EXPECT_TRUE(layer_need_backward[layer_id]);
2661      } else if (layer_name == "loss") {
2662        ASSERT_EQ(need_backward.size(), 2);
2663        EXPECT_TRUE(need_backward[0]);   
2664        EXPECT_FALSE(need_backward[1]);  
2665        EXPECT_TRUE(layer_need_backward[layer_id]);
2666      } else {
2667        LOG(FATAL) << "Unknown layer: " << layer_name;
2668      }
2669    }
2670    this->InitForcePropNet(true);
2671    layer_need_backward = this->net_->layer_need_backward();
2672    for (int layer_id = 0; layer_id < this->net_->layers().size(); ++layer_id) {
2673      const string& layer_name = this->net_->layer_names()[layer_id];
2674      const vector<bool> need_backward =
2675          this->net_->bottom_need_backward()[layer_id];
2676      if (layer_name == "data") {
2677        ASSERT_EQ(need_backward.size(), 0);
2678        EXPECT_FALSE(layer_need_backward[layer_id]);
2679      } else if (layer_name == "innerproduct") {
2680        ASSERT_EQ(need_backward.size(), 1);
2681        EXPECT_TRUE(need_backward[0]);  
2682        EXPECT_TRUE(layer_need_backward[layer_id]);
2683      } else if (layer_name == "loss") {
2684        ASSERT_EQ(need_backward.size(), 2);
2685        EXPECT_TRUE(need_backward[0]);   
2686        EXPECT_FALSE(need_backward[1]);  
2687        EXPECT_TRUE(layer_need_backward[layer_id]);
2688      } else {
2689        LOG(FATAL) << "Unknown layer: " << layer_name;
2690      }
2691    }
2692  }
2693  TYPED_TEST(NetTest, TestAllInOneNetTrain) {
2694    vector<string> stages;
2695    stages.push_back("train");
2696    this->InitAllInOneNet(caffe::TRAIN, 0, &stages);
2697    bool found_data = false;
2698    bool found_loss = false;
2699    for (int i = 0; i < this->net_->layers().size(); ++i) {
2700      const string& layer_name = this->net_->layer_names()[i];
2701      if (layer_name == "train-data") {
2702        found_data = true;
2703      } else if (layer_name == "loss") {
2704        found_loss = true;
2705      } else {
2706        ASSERT_NE(layer_name, "val-data");
2707        ASSERT_NE(layer_name, "deploy-data");
2708      }
2709    }
2710    ASSERT_TRUE(found_data);
2711    ASSERT_TRUE(found_loss);
2712  }
2713  TYPED_TEST(NetTest, TestAllInOneNetVal) {
2714    vector<string> stages;
2715    stages.push_back("val");
2716    this->InitAllInOneNet(caffe::TEST, 0, &stages);
2717    bool found_data = false;
2718    bool found_loss = false;
<span onclick='openModal()' class='match'>2719    for (int i = 0; i < this->net_->layers().size(); ++i) {
2720      const string& layer_name = this->net_->layer_names()[i];
</span>2721      if (layer_name == "val-data") {
2722        found_data = true;
2723      } else if (layer_name == "loss") {
2724        found_loss = true;
2725      } else {
2726        ASSERT_NE(layer_name, "train-data");
2727        ASSERT_NE(layer_name, "deploy-data");
2728      }
2729    }
2730    ASSERT_TRUE(found_data);
2731    ASSERT_TRUE(found_loss);
2732  }
2733  TYPED_TEST(NetTest, TestAllInOneNetDeploy) {
2734    vector<string> stages;
2735    stages.push_back("deploy");
2736    this->InitAllInOneNet(caffe::TEST, 0, &stages);
2737    bool found_data = false;
2738    for (int i = 0; i < this->net_->layers().size(); ++i) {
2739      const string& layer_name = this->net_->layer_names()[i];
2740      if (layer_name == "deploy-data") {
2741        found_data = true;
2742      } else {
2743        ASSERT_NE(layer_name, "train-data");
2744        ASSERT_NE(layer_name, "val-data");
2745        ASSERT_NE(layer_name, "loss");
2746      }
2747    }
2748    ASSERT_TRUE(found_data);
2749  }
2750  class CompileNetTest : public ::testing::Test {
2751   protected:
2752    void RunCompilerNetTest(
2753        const string& input_param_string, const string& compiled_param_string) {
2754      NetParameter input_param;
2755      CHECK(google::protobuf::TextFormat::ParseFromString(
2756          input_param_string, &input_param));
2757      NetParameter expected_compiled_param;
2758      CHECK(google::protobuf::TextFormat::ParseFromString(
2759          compiled_param_string, &expected_compiled_param));
2760      NetParameter actual_compiled_param;
2761      Net<float>::CompileNet(input_param, &actual_compiled_param);
2762      actual_compiled_param.mutable_compile_net_state()->Clear();
2763      expected_compiled_param.mutable_compile_net_state()->Clear();
2764      string expect_net_string = expected_compiled_param.DebugString();
2765      string actual_net_string = actual_compiled_param.DebugString();
2766      EXPECT_EQ(expect_net_string,
2767          actual_net_string);
2768      NetParameter double_compiled_param;
2769      Net<float>::CompileNet(actual_compiled_param, &double_compiled_param);
2770      double_compiled_param.mutable_compile_net_state()->Clear();
2771      string double_net_string = double_compiled_param.DebugString();
2772      EXPECT_EQ(actual_net_string,
2773         double_net_string);
2774    }
2775  };
2776  #ifndef DISABLE_BN_FOLDING
2777  TEST_F(CompileNetTest, TestRemoveBatchNorm1) {
2778    const string& input_proto = 
2779        "name: 'TestNetwork' "
2780        "layer { "
2781        "  name: 'data' "
2782        "  type: 'Data' "
2783        "  top: 'data' "
2784        "  top: 'label' "
2785        "} "
2786        "layer { "
2787        "  bottom: 'data' "
2788        "  name: 'conv' "
2789        "  top: 'conv' "
2790        "  type: 'Convolution' "
2791        "} "
2792        "layer { "
2793        "  bottom: 'conv' "
2794        "  name: 'bn' "
2795        "  top: 'conv' "
2796        "  type: 'BatchNorm' "
2797        "} "
2798        "layer { "
2799        "  name: 'loss' "
2800        "  type: 'SoftmaxWithLoss' "
2801        "  bottom: 'conv' "
2802        "  bottom: 'label' "
2803        "} ";
2804    const string& output_proto =
2805        "name: 'TestNetwork' "
2806        "layer { "
2807        "  name: 'data' "
2808        "  type: 'Data' "
2809        "  top: 'data' "
2810        "  top: 'label' "
2811        "} "
2812        "layer { "
2813        "  bottom: 'data' "
2814        "  name: 'conv' "
2815        "  top: 'conv' "
2816        "  type: 'Convolution' "
2817        "} "
2818        "layer { "
2819        "  name: 'loss' "
2820        "  type: 'SoftmaxWithLoss' "
2821        "  bottom: 'conv' "
2822        "  bottom: 'label' "
2823        "} ";
2824    this->RunCompilerNetTest(input_proto, output_proto);
2825  }
2826  TEST_F(CompileNetTest, TestRemoveBatchNorm2) {
2827    const string& input_proto = 
2828        "name: 'TestNetwork' "
2829        "layer { "
2830        "  name: 'data' "
2831        "  type: 'Data' "
2832        "  top: 'data' "
2833        "  top: 'label' "
2834        "} "
2835        "layer { "
2836        "  bottom: 'data' "
2837        "  name: 'fc1' "
2838        "  top: 'fc1' "
2839        "  type: 'InnerProduct' "
2840        "} "
2841        "layer { "
2842        "  bottom: 'fc1' "
2843        "  name: 'bn' "
2844        "  top: 'bn' "
2845        "  type: 'BatchNorm' "
2846        "} "
2847        "layer { "
2848        "  name: 'loss' "
2849        "  type: 'SoftmaxWithLoss' "
2850        "  bottom: 'bn' "
2851        "  bottom: 'label' "
2852        "} ";
2853    const string& output_proto =
2854        "name: 'TestNetwork' "
2855        "layer { "
2856        "  name: 'data' "
2857        "  type: 'Data' "
2858        "  top: 'data' "
2859        "  top: 'label' "
2860        "} "
2861        "layer { "
2862        "  bottom: 'data' "
2863        "  name: 'fc1' "
2864        "  top: 'fc1' "
2865        "  type: 'InnerProduct' "
2866        "} "
2867        "layer { "
2868        "  bottom: 'fc1' "
2869        "  name: 'bn' "
2870        "  top: 'bn' "
2871        "  type: 'BatchNorm' "
2872        "} "
2873        "layer { "
2874        "  name: 'loss' "
2875        "  type: 'SoftmaxWithLoss' "
2876        "  bottom: 'bn' "
2877        "  bottom: 'label' "
2878        "} ";
2879    this->RunCompilerNetTest(input_proto, output_proto);
2880  }
2881  TEST_F(CompileNetTest, TestRemoveBatchNorm3) {
2882    const string& input_proto = 
2883        "name: 'TestNetwork' "
2884        "layer { "
2885        "  name: 'data' "
2886        "  type: 'Data' "
2887        "  top: 'data' "
2888        "  top: 'label' "
2889        "} "
2890        "layer { "
2891        "  bottom: 'data' "
2892        "  name: 'conv' "
2893        "  top: 'conv' "
2894        "  type: 'Convolution' "
2895        "} "
2896        "layer { "
2897        "  bottom: 'conv' "
2898        "  name: 'bn' "
2899        "  top: 'conv' "
2900        "  type: 'BatchNorm' "
2901  	  "  batch_norm_param { "
2902  	  "    use_global_stats: false"
2903  	  "  }"
2904        "} "
2905        "layer { "
2906        "  name: 'loss' "
2907        "  type: 'SoftmaxWithLoss' "
2908        "  bottom: 'conv' "
2909        "  bottom: 'label' "
2910        "} ";
2911    const string& output_proto =
2912        "name: 'TestNetwork' "
2913        "layer { "
2914        "  name: 'data' "
2915        "  type: 'Data' "
2916        "  top: 'data' "
2917        "  top: 'label' "
2918        "} "
2919        "layer { "
2920        "  bottom: 'data' "
2921        "  name: 'conv' "
2922        "  top: 'conv' "
2923        "  type: 'Convolution' "
2924        "} "
2925        "layer { "
2926        "  bottom: 'conv' "
2927        "  name: 'bn' "
2928        "  top: 'conv' "
2929        "  type: 'BatchNorm' "
2930  	  "  batch_norm_param { "
2931  	  "    use_global_stats: false"
2932  	  "  }"
2933        "} "
2934        "layer { "
2935        "  name: 'loss' "
2936        "  type: 'SoftmaxWithLoss' "
2937        "  bottom: 'conv' "
2938        "  bottom: 'label' "
2939        "} ";
2940    this->RunCompilerNetTest(input_proto, output_proto);
2941  }
2942  TEST_F(CompileNetTest, TestRemoveBatchNorm4) {
2943    const string& input_proto = 
2944        "name: 'TestNetwork' "
2945        "layer { "
2946        "  name: 'data' "
2947        "  type: 'Data' "
2948        "  top: 'data' "
2949        "  top: 'label' "
2950        "} "
2951        "layer { "
2952        "  bottom: 'data' "
2953        "  name: 'conv' "
2954        "  top: 'conv' "
2955        "  type: 'Convolution' "
2956        "} "
2957        "layer { "
2958        "  bottom: 'conv' "
2959        "  name: 'bn' "
2960        "  top: 'conv' "
2961        "  type: 'BatchNorm' "
2962  	  "  batch_norm_param { "
2963  	  "    use_global_stats: true"
2964  	  "  }"
2965        "} "
2966        "layer { "
2967        "  name: 'loss' "
2968        "  type: 'SoftmaxWithLoss' "
2969        "  bottom: 'conv' "
2970        "  bottom: 'label' "
2971        "} ";
2972    const string& output_proto =
2973        "name: 'TestNetwork' "
2974        "layer { "
2975        "  name: 'data' "
2976        "  type: 'Data' "
2977        "  top: 'data' "
2978        "  top: 'label' "
2979        "} "
2980        "layer { "
2981        "  bottom: 'data' "
2982        "  name: 'conv' "
2983        "  top: 'conv' "
2984        "  type: 'Convolution' "
2985        "} "
2986        "layer { "
2987        "  name: 'loss' "
2988        "  type: 'SoftmaxWithLoss' "
2989        "  bottom: 'conv' "
2990        "  bottom: 'label' "
2991        "} ";
2992    this->RunCompilerNetTest(input_proto, output_proto);
2993  }
2994  #endif
2995  #ifdef MKL2017_SUPPORTED
2996  TEST_F(CompileNetTest, TestCompileNetBatchNorm) {
2997    const string& input_proto =
2998        "name: 'TestNetwork' "
2999        "layer { "
3000        "  name: 'data' "
3001        "  type: 'Data' "
3002        "  top: 'data' "
3003        "  top: 'label' "
3004        "} "
3005        "layer { "
3006        "  bottom: 'data' "
3007        "  name: 'bn' "
3008        "  top: 'bn' "
3009        "  type: 'BatchNorm' "
3010        "  batch_norm_param { "
3011        "   engine: MKL2017 "
3012        "  } "
3013        "} "
3014        "layer { "
3015        " bottom: 'bn' "
3016        " top: 'sc' "
3017        " name: 'sc' "
3018        " type: 'Scale' "
3019        " scale_param { "
3020        "   bias_term: true "
3021        " }"
3022        "}"
3023        "layer { "
3024        "  name: 'loss' "
3025        "  type: 'SoftmaxWithLoss' "
3026        "  bottom: 'sc' "
3027        "  bottom: 'label' "
3028        "} ";
3029    const string& output_proto =
3030        "name: 'TestNetwork' "
3031        "layer { "
3032        "  name: 'data' "
3033        "  type: 'Data' "
3034        "  top: 'data' "
3035        "  top: 'label' "
3036        "} "
3037        "layer { "
3038        "  bottom: 'data' "
3039        "  name: 'bn' "
3040        "  top: 'sc' "
3041        "  type: 'BatchNorm' "
3042        "  batch_norm_param { "
3043        "   engine: MKL2017 "
3044        "   bias_term: true "
3045        "  } "
3046        "} "
3047        "layer { "
3048        "  name: 'loss' "
3049        "  type: 'SoftmaxWithLoss' "
3050        "  bottom: 'sc' "
3051        "  bottom: 'label' "
3052        "} ";
3053    this->RunCompilerNetTest(input_proto, output_proto);
3054  }
3055  TEST_F(CompileNetTest, TestCompileNetBatchNormInPlace) {
3056    const string& input_proto =
3057        "name: 'TestNetwork' "
3058        "layer { "
3059        "  name: 'data' "
3060        "  type: 'Data' "
3061        "  top: 'data' "
3062        "  top: 'label' "
3063        "} "
3064        "layer { "
3065        "  bottom: 'data' "
3066        "  name: 'bn' "
3067        "  top: 'data' "
3068        "  type: 'BatchNorm' "
3069        "  batch_norm_param { "
3070        "   engine: MKL2017 "
3071        "  } "
3072        "} "
3073        "layer { "
3074        " bottom: 'data' "
3075        " top: 'data' "
3076        " name: 'sc' "
3077        " type: 'Scale' "
3078        " scale_param { "
3079        "   bias_term: true "
3080        " }"
3081        "}"
3082        "layer { "
3083        " bottom: 'data' "
3084        " top: 'data' "
3085        " name: 'relu' "
3086        " type: 'ReLU' "
3087        " relu_param { "
3088        "  engine: MKL2017 "
3089        " } "
3090        "}"
3091        "layer { "
3092        "  name: 'loss' "
3093        "  type: 'SoftmaxWithLoss' "
3094        "  bottom: 'data' "
3095        "  bottom: 'label' "
3096        "} ";
3097    const string& output_proto =
3098        "name: 'TestNetwork' "
3099        "layer { "
3100        "  name: 'data' "
3101        "  type: 'Data' "
3102        "  top: 'data' "
3103        "  top: 'label' "
3104        "} "
3105        "layer { "
3106        "  bottom: 'data' "
3107        "  name: 'bn' "
3108        "  top: 'data_x' "
3109        "  type: 'BatchNorm' "
3110        "  batch_norm_param { "
3111        "   engine: MKL2017 "
3112        "   bias_term: true "
3113        "  } "
3114        "} "
3115        "layer { "
3116        " bottom: 'data_x' "
3117        " top: 'data_x' "
3118        " name: 'relu' "
3119        " type: 'ReLU' "
3120        " relu_param { "
3121        "  engine: MKL2017 "
3122        " } "
3123        "}"
3124        "layer { "
3125        "  name: 'loss' "
3126        "  type: 'SoftmaxWithLoss' "
3127        "  bottom: 'data_x' "
3128        "  bottom: 'label' "
3129        "} ";
3130    this->RunCompilerNetTest(input_proto, output_proto);
3131  }
3132  #endif
3133  #if defined(MKL2017_SUPPORTED) && defined(MKLDNN_SUPPORTED)
3134  TEST_F(CompileNetTest, TestCompileNetBatchNormConvolution) {
3135    const string& input_proto =
3136        "name: 'TestNetwork' "
3137        "layer { "
3138        "  name: 'data' "
3139        "  type: 'Data' "
3140        "  top: 'data' "
3141        "  top: 'label' "
3142        "} "
3143        "layer { "
3144        "  bottom: 'data' "
3145        "  name: 'bn' "
3146        "  top: 'bn' "
3147        "  type: 'BatchNorm' "
3148        "  batch_norm_param { "
3149        "   engine: MKL2017 "
3150        "  } "
3151        "} "
3152        "layer { "
3153        " bottom: 'bn' "
3154        " top: 'conv' "
3155        " name: 'sc' "
3156        " type: 'Scale' "
3157        " scale_param { "
3158        "   bias_term: true "
3159        " }"
3160        "}"
3161        "layer { "
3162        "  bottom: 'conv' "
3163        "  name: 'conv' "
3164        "  top: 'relu' "
3165        "  type: 'Convolution' "
3166        "  convolution_param { "
3167        "   engine: MKLDNN "
3168        "  } "
3169        "} "
3170        "layer { "
3171        " bottom: 'relu' "
3172        " top: 'relu' "
3173        " name: 'relu' "
3174        " type: 'ReLU' "
3175        " relu_param { "
3176        "  engine: MKLDNN "
3177        " } "
3178        "}"
3179        "layer { "
3180        "  name: 'loss' "
3181        "  type: 'SoftmaxWithLoss' "
3182        "  bottom: 'relu' "
3183        "  bottom: 'label' "
3184        "} ";
3185    const string& output_proto =
3186        "name: 'TestNetwork' "
3187        "layer { "
3188        "  name: 'data' "
3189        "  type: 'Data' "
3190        "  top: 'data' "
3191        "  top: 'label' "
3192        "} "
3193        "layer { "
3194        "  bottom: 'data' "
3195        "  name: 'bn' "
3196        "  top: 'conv' "
3197        "  type: 'BatchNorm' "
3198        "  batch_norm_param { "
3199        "   engine: MKL2017 "
3200        "   bias_term: true "
3201        "  } "
3202        "} "
3203        "layer { "
3204        "  bottom: 'conv' "
3205        "  name: 'conv' "
3206        "  top: 'relu' "
3207        "  type: 'Convolution' "
3208        "  convolution_param { "
3209        "   engine: MKLDNN "
3210        "   relu: true "
3211        "negative_slope: 0"
3212        "  } "
3213        "} "
3214        "layer { "
3215        "  name: 'loss' "
3216        "  type: 'SoftmaxWithLoss' "
3217        "  bottom: 'relu' "
3218        "  bottom: 'label' "
3219        "} ";
3220    this->RunCompilerNetTest(input_proto, output_proto);
3221  }
3222  #endif
3223  #ifndef DISABLE_CONV_SUM_FUSION
3224  TEST_F(CompileNetTest, TestCompileNetConvEltReluFusionMKLDNN) {
3225    const string& input_proto =
3226        "name: 'TestNetwork' "
3227        "layer { "
3228        "  name: 'data' "
3229        "  type: 'Data' "
3230        "  top: 'data' "
3231        "  top: 'label' "
3232        "} "
3233        "layer { "
3234        "  bottom: 'data' "
3235        "  name: 'conv1' "
3236        "  top: 'conv1' "
3237        "  type: 'Convolution' "
3238        "  convolution_param { "
3239        "   engine: MKLDNN "
3240        "  } "
3241        "} "
3242        "layer { "
3243        "  bottom: 'data' "
3244        "  name: 'conv2' "
3245        "  top: 'conv2' "
3246        "  type: 'Convolution' "
3247        "  convolution_param { "
3248        "   engine: MKLDNN "
3249        "  } "
3250        "} "
3251        "layer { "
3252        "  bottom: 'conv1' "
3253        "  name: 'conv3' "
3254        "  top: 'conv3' "
3255        "  type: 'Convolution' "
3256        "  convolution_param { "
3257        "   engine: MKLDNN "
3258        "  } "
3259        "} "
3260        "layer { "
3261        "  bottom: 'conv2' "
3262        "  bottom: 'conv3' "
3263        "  name: 'conv4' "
3264        "  top: 'relu' "
3265        "  type: 'Eltwise' "
3266        "} "
3267        "layer { "
3268        " bottom: 'relu' "
3269        " top: 'relu' "
3270        " name: 'relu' "
3271        " type: 'ReLU' "
3272        "}"
3273        "layer { "
3274        "  name: 'loss' "
3275        "  type: 'SoftmaxWithLoss' "
3276        "  bottom: 'relu' "
3277        "  bottom: 'label' "
3278        "} ";
3279    const string& output_proto =
3280        "name: 'TestNetwork' "
3281        "layer { "
3282        "  name: 'data' "
3283        "  type: 'Data' "
3284        "  top: 'data' "
3285        "  top: 'label' "
3286        "} "
3287        "layer { "
3288        "  bottom: 'data' "
3289        "  name: 'conv1' "
3290        "  top: 'conv1' "
3291        "  type: 'Convolution' "
3292        "  convolution_param { "
3293        "   engine: MKLDNN "
3294        "  } "
3295        "} "
3296        "layer { "
3297        "  bottom: 'data' "
3298        "  name: 'conv2' "
3299        "  top: 'conv2' "
3300        "  type: 'Convolution' "
3301        "  convolution_param { "
3302        "   engine: MKLDNN "
3303        "  } "
3304        "} "
3305        "layer { "
3306        "  bottom: 'conv1' "
3307        "  bottom: 'conv2' "
3308        "  name: 'conv3' "
3309        "  top: 'relu' "
3310        "  type: 'Convolution' "
3311        "  convolution_param { "
3312        "   engine: MKLDNN "
3313        "   relu: true "
3314        "   fusion_type: SUM_FUSION "
3315        "  } "
3316        "} "
3317        "layer { "
3318        "  name: 'loss' "
3319        "  type: 'SoftmaxWithLoss' "
3320        "  bottom: 'relu' "
3321        "  bottom: 'label' "
3322        "} ";
3323   const string input_proto_test = "state: { phase: TEST } engine: 'MKLDNN'" + input_proto;
3324   const string output_proto_test = "state: { phase: TEST } engine: 'MKLDNN'" + output_proto;
3325   this->RunCompilerNetTest(input_proto_test, output_proto_test);
3326  }
3327  #endif
3328  #ifdef MKLDNN_SUPPORTED
3329  TEST_F(CompileNetTest, TestCompileNetBatchNormMKLDNN) {
3330      const string& input_proto =
3331        "name: 'TestNetwork' "
3332        "layer { "
3333        "  name: 'data' "
3334        "  type: 'Data' "
3335        "  top: 'data' "
3336        "  top: 'label' "
3337        "} "
3338        "layer { "
3339        "  bottom: 'data' "
3340        "  name: 'bn' "
3341        "  top: 'bn' "
3342        "  type: 'BatchNorm' "
3343        "  batch_norm_param { "
3344        "   engine: MKLDNN "
3345        "  } "
3346        "} "
3347        "layer { "
3348        " bottom: 'bn' "
3349        " top: 'sc' "
3350        " name: 'sc' "
3351        " type: 'Scale' "
3352        " scale_param { "
3353        "   bias_term: true "
3354        " }"
3355        "}"
3356        "layer { "
3357        "  name: 'loss' "
3358        "  type: 'SoftmaxWithLoss' "
3359        "  bottom: 'sc' "
3360        "  bottom: 'label' "
3361        "} ";
3362    const string& output_proto =
3363        "name: 'TestNetwork' "
3364        "layer { "
3365        "  name: 'data' "
3366        "  type: 'Data' "
3367        "  top: 'data' "
3368        "  top: 'label' "
3369        "} "
3370        "layer { "
3371        "  bottom: 'data' "
3372        "  name: 'bn' "
3373        "  top: 'sc' "
3374        "  type: 'BatchNorm' "
3375        "  batch_norm_param { "
3376        "   engine: MKLDNN "
3377        "   bias_term: true "
3378        "  } "
3379        "} "
3380        "layer { "
3381        "  name: 'loss' "
3382        "  type: 'SoftmaxWithLoss' "
3383        "  bottom: 'sc' "
3384        "  bottom: 'label' "
3385        "} ";
3386    this->RunCompilerNetTest(input_proto, output_proto);
3387  }
3388  TEST_F(CompileNetTest, TestCompileNetConvolution) {
3389    const string& input_proto =
3390        "name: 'TestNetwork' "
3391        "layer { "
3392        "  name: 'data' "
3393        "  type: 'Data' "
3394        "  top: 'data' "
3395        "  top: 'label' "
3396        "} "
3397        "layer { "
3398        "  bottom: 'data' "
3399        "  name: 'conv' "
3400        "  top: 'relu' "
3401        "  type: 'Convolution' "
3402        "  convolution_param { "
3403        "   engine: MKLDNN "
3404        "  } "
3405        "} "
3406        "layer { "
3407        " bottom: 'relu' "
3408        " top: 'relu' "
3409        " name: 'relu' "
3410        " type: 'ReLU' "
3411        " relu_param { "
3412        "  engine: MKLDNN "
3413        " } "
3414        "}"
3415        "layer { "
3416        "  name: 'loss' "
3417        "  type: 'SoftmaxWithLoss' "
3418        "  bottom: 'relu' "
3419        "  bottom: 'label' "
3420        "} ";
3421    const string& output_proto =
3422        "name: 'TestNetwork' "
3423        "layer { "
3424        "  name: 'data' "
3425        "  type: 'Data' "
3426        "  top: 'data' "
3427        "  top: 'label' "
3428        "} "
3429        "layer { "
3430        "  bottom: 'data' "
3431        "  name: 'conv' "
3432        "  top: 'relu' "
3433        "  type: 'Convolution' "
3434        "  convolution_param { "
3435        "   engine: MKLDNN "
3436        "   relu: true "
3437        "negative_slope: 0"
3438        "  } "
3439        "} "
3440        "layer { "
3441        "  name: 'loss' "
3442        "  type: 'SoftmaxWithLoss' "
3443        "  bottom: 'relu' "
3444        "  bottom: 'label' "
3445        "} ";
3446    this->RunCompilerNetTest(input_proto, output_proto);
3447  }
3448  TEST_F(CompileNetTest, TestCompileNetLayerParamEngineConvolution) {
3449    const string& input_proto =
3450        "name: 'TestNetwork' "
3451        "layer { "
3452        "  name: 'data' "
3453        "  type: 'Data' "
3454        "  top: 'data' "
3455        "  top: 'label' "
3456        "} "
3457        "layer { "
3458        "  bottom: 'data' "
3459        "  name: 'conv' "
3460        "  top: 'relu' "
3461        "  type: 'Convolution' "
3462        "  engine: 'MKLDNN:CPU' "
3463        "  convolution_param { "
3464        "  } "
3465        "} "
3466        "layer { "
3467        " bottom: 'relu' "
3468        " top: 'relu' "
3469        " name: 'relu' "
3470        " type: 'ReLU' "
3471        " engine: 'MKLDNN:CPU' "
3472        " relu_param { "
3473        " } "
3474        "}"
3475        "layer { "
3476        "  name: 'loss' "
3477        "  type: 'SoftmaxWithLoss' "
3478        "  bottom: 'relu' "
3479        "  bottom: 'label' "
3480        "} ";
3481    const string& output_proto =
3482        "name: 'TestNetwork' "
3483        "layer { "
3484        "  name: 'data' "
3485        "  type: 'Data' "
3486        "  top: 'data' "
3487        "  top: 'label' "
3488        "} "
3489        "layer { "
3490        "  bottom: 'data' "
3491        "  name: 'conv' "
3492        "  top: 'relu' "
3493        "  type: 'Convolution' "
3494        "  engine: 'MKLDNN:CPU' "
3495        "  convolution_param { "
3496        "   relu: true "
3497        "negative_slope: 0"
3498        "  } "
3499        "} "
3500        "layer { "
3501        "  name: 'loss' "
3502        "  type: 'SoftmaxWithLoss' "
3503        "  bottom: 'relu' "
3504        "  bottom: 'label' "
3505        "} ";
3506    this->RunCompilerNetTest(input_proto, output_proto);
3507  }
3508  TEST_F(CompileNetTest, TestNoCompileNetLayerParamEngineConvolution) {
3509    const string& input_proto =
3510        "name: 'TestNetwork' "
3511        "layer { "
3512        "  name: 'data' "
3513        "  type: 'Data' "
3514        "  top: 'data' "
3515        "  top: 'label' "
3516        "} "
3517        "layer { "
3518        "  bottom: 'data' "
3519        "  name: 'conv' "
3520        "  top: 'relu' "
3521        "  type: 'Convolution' "
3522        "  engine: 'MKLDNN:DLA,CPU' "
3523        "  convolution_param { "
3524        "  } "
3525        "} "
3526        "layer { "
3527        " bottom: 'relu' "
3528        " top: 'relu' "
3529        " name: 'relu' "
3530        " type: 'ReLU' "
3531        "  engine: 'MKLDNN:DLA,CPU' "
3532        " relu_param { "
3533        " } "
3534        "}"
3535        "layer { "
3536        "  name: 'loss' "
3537        "  type: 'SoftmaxWithLoss' "
3538        "  bottom: 'relu' "
3539        "  bottom: 'label' "
3540        "} ";
3541    this->RunCompilerNetTest(input_proto, input_proto);
3542  }
3543  #endif
3544  TEST_F(CompileNetTest, TestNoCompileNet) {
3545    const string& input_proto=
3546        "name: 'TestNetwork' "
3547        "layer { "
3548        "  name: 'data' "
3549        "  type: 'Data' "
3550        "  top: 'data' "
3551        "  top: 'label' "
3552        "} "
3553        "layer { "
3554        "  bottom: 'data' "
3555        "  name: 'bn' "
3556        "  top: 'bn' "
3557        "  type: 'BatchNorm' "
3558        "  batch_norm_param { "
3559        "   engine: CAFFE "
3560        "  } "
3561        "} "
3562        "layer { "
3563        " bottom: 'bn' "
3564        " top: 'sc' "
3565        " name: 'sc' "
3566        " type: 'Scale' "
3567        " scale_param { "
3568        "   bias_term: true "
3569        " }"
3570        "}"
3571        "layer { "
3572        "  name: 'loss' "
3573        "  type: 'SoftmaxWithLoss' "
3574        "  bottom: 'sc' "
3575        "  bottom: 'label' "
3576        "} ";
3577    this->RunCompilerNetTest(input_proto, input_proto);
3578  }
3579  }  
</code></pre>
        </div>
        <div class="column">
            <h3>snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-cmty.cpp</h3>
            <pre><code>1  namespace TSnap {
2  namespace TSnapDetail {
3  void CmtyGirvanNewmanStep(PUNGraph& Graph, TIntV& Cmty1, TIntV& Cmty2) {
4    TIntPrFltH BtwEH;
5    TBreathFS<PUNGraph> BFS(Graph);
6    Cmty1.Clr(false);  Cmty2.Clr(false);
7    while (true) {
8      TSnap::GetBetweennessCentr(Graph, BtwEH);
9      BtwEH.SortByDat(false);
10      if (BtwEH.Empty()) { return; }
11      const int NId1 = BtwEH.GetKey(0).Val1;
12      const int NId2 = BtwEH.GetKey(0).Val2;
13      Graph->DelEdge(NId1, NId2);
14      BFS.DoBfs(NId1, true, false, NId2, TInt::Mx);
15      if (BFS.GetHops(NId1, NId2) == -1) { 
16        TSnap::GetNodeWcc(Graph, NId1, Cmty1);
17        TSnap::GetNodeWcc(Graph, NId2, Cmty2);
18        return;
19      }
20    }
21  }
22  double _GirvanNewmanGetModularity(const PUNGraph& G, const TIntH& OutDegH, const int& OrigEdges, TCnComV& CnComV) {
23    TSnap::GetWccs(G, CnComV); 
24    double Mod = 0;
25    for (int c = 0; c < CnComV.Len(); c++) {
26      const TIntV& NIdV = CnComV[c]();
27      double EIn = 0, EEIn = 0;
28      for (int i = 0; i < NIdV.Len(); i++) {
29        TUNGraph::TNodeI NI = G->GetNI(NIdV[i]);
30        EIn += NI.GetOutDeg();
31        EEIn += OutDegH.GetDat(NIdV[i]);
32      }
33      Mod += (EIn-EEIn*EEIn / (2.0*OrigEdges));
34    }
35    if (Mod == 0) { return 0; }
36    else { return Mod / (2.0*OrigEdges); }
37  }
38  void MapEquationNew2Modules(PUNGraph& Graph, TIntH& Module, TIntFltH& Qi, int a, int b) {
39    float InModule = 0.0, OutModule = 0.0, Val;
40    int Mds[2] = { a, b };
41    for (int i = 0; i<2; i++) {
42      InModule = 0.0, OutModule = 0.0;
43      if (Qi.IsKey(Mds[i])) {
44        int CentralModule = Mds[i];
45        TIntV newM;
46        for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++) {
47          if (Module.GetDat(NI.GetId()) == CentralModule)
48            newM.Add(NI.GetId());
49          }
50        for (int j = 0; j<newM.Len(); j++) {
51          for (int k = 0; k<Graph->GetNI(newM[j]).GetDeg(); k++) {
52            int ids = Graph->GetNI(newM[j]).GetId();
53            int idd = Graph->GetNI(newM[j]).GetNbrNId(k);
54            int ms = Module.GetDat(ids);
55            int md = Module.GetDat(idd);
56            if (ms == md) {
57              InModule += 1.0;
58            } else {
59              OutModule += 1.0;
60            }
61          }
62        }
63        if (InModule >1) InModule = InModule / 2;
64        Val = 0.0;
65        if (InModule + OutModule > 0) {
66          Val = OutModule / (InModule + OutModule);
67        }
68        Qi.AddDat(Mds[i], Val);
69      } else {
70        Qi.AddDat(Mds[i], 0.0);
71      }
72    }
73  }
74  double Equation(TIntFltH& PAlpha, double& SumPAlphaLogPAlpha, TIntFltH& Qi){
75    double SumPAlpha = 1.0, SumQi = 0.0, SumQiLogQi = 0.0;
76    double SumQiSumPAlphaLogQiSumPAlpha = 0.0, logqi = 0.0, qi = 0.0;
77    for (int i = 0; i<Qi.Len(); i++) {
78      SumQi += Qi[i];
79      qi = Qi[i];
80      if (qi != 0) {
81        logqi = log(qi);
82      } else {
83        logqi = 0;
84      }
85      SumQiLogQi += Qi[i] * logqi;
86      SumQiSumPAlphaLogQiSumPAlpha += (Qi[i] + SumPAlpha)*log(Qi[i] + SumPAlpha);
87    }
88    return (SumQi*log(SumQi) - 2 * SumQiLogQi - SumPAlphaLogPAlpha +
89            SumQiSumPAlphaLogQiSumPAlpha);
90  }
91  bool edgeIntersect(PNGraph& graph, TIntV& a, TIntV& b) {
92    for (int i = 0; i<a.Len(); i++) {
93      for (int j = 0; j<b.Len(); j++) {
94        if (graph->IsEdge(a[i], b[j]))
95          return true;
96      }
97    }
98    return false;
99  }
100  int vectorIntersect(TIntV& a, TIntV& b) {
101    int count = 0;
102    for (int i = 0; i<a.Len(); i++) {
103      for (int j = 0; j<b.Len(); j++) {
104        if (a[i] == b[j])
105          count++;
106      }
107    }
108    return count;
109  }
110  bool inComp(PNGraph& g1, PNGraph& Graph, TIntH& inCompCount, int id, int neigh) {
111    bool out = true;
112    int inCompN = 0;
113    int inComp = 0;
114    if (g1->IsNode(id) && g1->IsNode(neigh)) {
115      int deg = g1->GetNI(id).GetDeg();
116      int neighDeg = g1->GetNI(neigh).GetDeg();
117      if (inCompCount.IsKey(id)) {
118        inComp = inCompCount.GetDat(id);
119      }
120      if (inCompCount.IsKey(neigh)) {
121        inCompN = inCompCount.GetDat(neigh);
122      }
123      if (inCompN < neighDeg && inComp < deg && (!g1->IsNode(neigh) || Graph->GetNI(neigh).GetDeg() - neighDeg == 0)) {
124        inCompCount.AddDat(neigh, ++inCompN);
125        inCompCount.AddDat(id, ++inComp);
126        out = true;
127      } else {
128        out = false;
129      }
130    }
131    return out;
132  }
133  void transitiveTransform(TIntV& a, TIntV& b) {
134    for (int i = 0; i < a.Len(); i++) {
135      bool diff = false;
136      for (int j = 0; j < b.Len(); j++) {
137        if (a[i] == a[j]) {
138          diff = true;
139          break;
140        }
141      }
142      if (!diff) {
143        b.Add(a[i]);
144        break;
145      }
146    }
147  }
148  bool chekIfCrossing(TIntV& a, TIntH& t, int f, int l, int TP) {
149    bool after = false;
150    bool before = false;
151    for (int i = 0; i < a.Len(); i++) {
152      if (t.GetDat(a[i]) < TP)
153        before = true;
154      if (t.GetDat(a[i]) > TP)
155        after = true;
156    }
157    if (TP == f)
158      before = true;
159    if (TP == l)
160      after = true;
161    return (after && before);
162  }
163  double InfomapOnlineIncrement(PUNGraph& Graph, int n1, int n2, TIntFltH& PAlpha, double& SumPAlphaLogPAlpha, TIntFltH& Qi, TIntH& Module, int& Br) {
164    bool n1new = false;
165    bool n2new = false;
166    if (!Graph->IsNode(n1)){
167      Graph->AddNode(n1);
168      n1new = true;
169    }
170    if (!Graph->IsNode(n2)) {
171      Graph->AddNode(n2);
172      n2new = true;
173    }
174    Graph->AddEdge(n1, n2);
175    int e = Graph->GetEdges();
176    double oldAlphaN1 = 0.0;
177    double oldAlphaN2 = 0.0;
178    if (!n1new)
179      oldAlphaN1 = PAlpha.GetDat(n1);
180    if (!n2new)
181      oldAlphaN2 = PAlpha.GetDat(n2);
182    TUNGraph::TNodeI node = Graph->GetNI(n1);
183    int nodeDeg = node.GetDeg();
184    float d = ((float)nodeDeg / (float)(2 * e));
185    PAlpha.AddDat(n1, d);
186    SumPAlphaLogPAlpha = SumPAlphaLogPAlpha - oldAlphaN1 + d*log(d);
187    if (n1new) {
188      Module.AddDat(n1, Br);
189      Qi.AddDat(Br, 1.0);
190      Br++;
191    }
192    node = Graph->GetNI(n2);
193    nodeDeg = node.GetDeg();
194    d = ((float)nodeDeg / (float)(2 * e));
195    PAlpha.AddDat(n2, d);
196    SumPAlphaLogPAlpha = SumPAlphaLogPAlpha - oldAlphaN2 + d*log(d);
197    if (n2new) {
198      Module.AddDat(n2, Br);
199      Qi.AddDat(Br, 1.0);
200      Br++;
201    }
202    double MinCodeLength = TSnapDetail::Equation(PAlpha, SumPAlphaLogPAlpha, Qi);
203    double PrevIterationCodeLength = 0.0;
204    do {
205      PrevIterationCodeLength = MinCodeLength;
206      int id[2] = { n1, n2 };
207      for (int k = 0; k<2; k++) {
208        for (int i = 0; i<Graph->GetNI(id[k]).GetDeg(); i++) {
209          int OldModule = Module.GetDat(id[k]);
210          int NewModule = Module.GetDat(Graph->GetNI(id[k]).GetNbrNId(i));
211          Module.AddDat(id[k], NewModule);
212          TSnapDetail::MapEquationNew2Modules(Graph, Module, Qi, OldModule, NewModule);
213          double NewCodeLength = TSnapDetail::Equation(PAlpha, SumPAlphaLogPAlpha, Qi);
214          if (NewCodeLength<MinCodeLength) {
215            MinCodeLength = NewCodeLength;
216            OldModule = NewModule;
217          }
218          else {
219            Module.AddDat(id[k], OldModule);
220          }
221        }
222      }
223    } while (MinCodeLength<PrevIterationCodeLength);
224    return MinCodeLength;
225  }
226  } 
227  double CommunityGirvanNewman(PUNGraph& Graph, TCnComV& CmtyV) {
228    PUNGraph LocalGraph = TSnap::ConvertGraph<PUNGraph>(Graph, false);
229    TIntH OutDegH;
230    const int NEdges = LocalGraph->GetEdges();
231    for (TUNGraph::TNodeI NI = LocalGraph->BegNI(); NI < LocalGraph->EndNI(); NI++) {
232      OutDegH.AddDat(NI.GetId(), NI.GetOutDeg());
233    }
234    double BestQ = -1; 
235    TCnComV CurCmtyV;
236    CmtyV.Clr();
237    TIntV Cmty1, Cmty2;
238    while (true) {
239      TSnapDetail::CmtyGirvanNewmanStep(LocalGraph, Cmty1, Cmty2);
240      const double Q = TSnapDetail::_GirvanNewmanGetModularity(LocalGraph, OutDegH, NEdges, CurCmtyV);
241      if (Q > BestQ) {
242        BestQ = Q; 
243        CmtyV.Swap(CurCmtyV);
244      }
245      if (Cmty1.Len() == 0 || Cmty2.Len() == 0) { break; }
246    }
247    return BestQ;
248  }
249  double Infomap(PUNGraph& Graph, TCnComV& CmtyV){
250    TIntFltH PAlpha; 
251    TIntH Module; 
252    TIntFltH Qi; 
253    double SumPAlphaLogPAlpha = 0.0;
254    int Br = 0;
255    const int e = Graph->GetEdges();
256    for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++) {
257      int nodeId = NI.GetId();
258      int nodeDeg = NI.GetDeg();
259      float d = ((float)nodeDeg / (float)(2 * e));
260      PAlpha.AddDat(nodeId, d);
261      SumPAlphaLogPAlpha += d*log(d);
262      Module.AddDat(nodeId, Br);
263      Qi.AddDat(Br, 1.0);
264      Br += 1;
265    }
266    double MinCodeLength = TSnapDetail::Equation(PAlpha, SumPAlphaLogPAlpha, Qi);
267    double NewCodeLength, PrevIterationCodeLength = 0.0;
268    int OldModule, NewModule;
269    TIntV nodes;
270    for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++)
271      nodes.Add(NI.GetId());
272    do {
273      PrevIterationCodeLength = MinCodeLength;
274      TRnd rnd;
275      rnd.Randomize();
276      nodes.Shuffle(rnd);
277      for (int ndcounter = 0; ndcounter<nodes.Len(); ndcounter++) {
278        MinCodeLength = TSnapDetail::Equation(PAlpha, SumPAlphaLogPAlpha, Qi);
279        int nodeId = nodes[ndcounter];
280        TUNGraph::TNodeI NI = Graph->GetNI(nodeId);
281        for (int i = 0; i<NI.GetDeg(); i++) {
282          OldModule = Module.GetDat(nodeId);
283          NewModule = Module.GetDat(NI.GetNbrNId(i));
284          if (OldModule != NewModule){
285            Module.AddDat(nodeId, NewModule);
286            TSnapDetail::MapEquationNew2Modules(Graph, Module, Qi, OldModule, NewModule);
287            NewCodeLength = TSnapDetail::Equation(PAlpha, SumPAlphaLogPAlpha, Qi);
288            if (NewCodeLength<MinCodeLength) {
289              MinCodeLength = NewCodeLength;
290              OldModule = NewModule;
291            }
292            else {
293              Module.AddDat(nodeId, OldModule);
294            }
295          }
296        }
297      }
298    } while (MinCodeLength<PrevIterationCodeLength);
299    Module.SortByDat(true);
300    int Mod = -1;
301    for (int i = 0; i<Module.Len(); i++) {
302      if (Module[i]>Mod){
303        Mod = Module[i];
304        TCnCom t;
305        for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++){
306          if (Module.GetDat(NI.GetId()) == Mod)
307            t.Add(NI.GetId());
308        }
309        CmtyV.Add(t);
310      }
311    }
312    return MinCodeLength;
313  }
314  double InfomapOnline(PUNGraph& Graph, int n1, int n2, TIntFltH& PAlpha, double& SumPAlphaLogPAlpha, TIntFltH& Qi, TIntH& Module, int& Br, TCnComV& CmtyV) {
315    double MinCodeLength = TSnapDetail::InfomapOnlineIncrement(Graph, n1, n2, PAlpha, SumPAlphaLogPAlpha, Qi, Module, Br);
316    Module.SortByDat(true);
317    int Mod = -1;
318    for (int i = 0; i<Module.Len(); i++) {
319      if (Module[i]>Mod){
320        Mod = Module[i];
321        TCnCom t;
322        for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++){
323          if (Module.GetDat(NI.GetId()) == Mod)
324            t.Add(NI.GetId());
325        }
326        CmtyV.Add(t);
327      }
328    }
329    return MinCodeLength;
330  }
331  void CmtyEvolutionFileBatchV(TStr InFNm, TIntIntVH& sizesContV, TIntIntVH& cContV, TIntIntVH& edges, double alpha, double beta, int CmtyAlg) {
332    TIntIntHH sizesCont;
333    TIntIntHH cCont;
334    CmtyEvolutionFileBatch(InFNm, sizesCont, cCont, edges, alpha, beta, CmtyAlg);
335    TIntV uniqueId;
336    for (int i = 0; i < cCont.Len(); i++){
337      for (THashKeyDatI<TInt, TInt> it = cCont[i].BegI(); !it.IsEnd(); it++){
338        if (!uniqueId.IsIn(it.GetKey()))
339          uniqueId.Add(it.GetKey());
340      }
341    }
342    for (int j = 0; j<uniqueId.Len(); j++)
343    {
344      TIntV cV;
345      for (int i = 0; i<cCont.Len(); i++)
346      {
347        if (cCont[i].IsKey(uniqueId[j]))
348          cV.Add(cCont[i].GetDat(uniqueId[j]));
349        else
350          cV.Add(-1);
351      }
352      cContV.AddDat(uniqueId[j], cV);
353    }
354    TIntV uniqueC;
355    for (int i = 0; i < sizesCont.Len(); i++){
356      for (THashKeyDatI<TInt, TInt> it = sizesCont[i].BegI(); !it.IsEnd(); it++){
357        if (!uniqueC.IsIn(it.GetKey()))
358          uniqueC.Add(it.GetKey());
359      }
360    }
361    for (int j = 0; j<uniqueC.Len(); j++)
362    {
363      TIntV cV;
364      for (int i = 0; i<sizesCont.Len(); i++)
365      {
366        if (sizesCont[i].IsKey(uniqueC[j]))
367          cV.Add(sizesCont[i].GetDat(uniqueC[j]));
368        else
369          cV.Add(0);
370      }
371      sizesContV.AddDat(uniqueC[j], cV);
372    }
373  }
374  void CmtyEvolutionFileBatch(TStr InFNm, TIntIntHH& sizesCont, TIntIntHH& cCont, TIntIntVH& edges, double alpha, double beta, int CmtyAlg) {
375    int br = 0;
376    TIntIntH prev;
377    TIntH prev_sizes;
378    TSsParser Ss(InFNm, ssfWhiteSep, true, false, true);
379    Ss.Next();
380    TStr Marker;
381    int SrcNId, DstNId; 
382    TIntIntVH edges_;
383    while (!Ss.Eof()) {
384      Marker = Ss.GetLnStr();
385      if (Marker.GetCh(0) == '#'){
386        Ss.Next();
387        PUNGraph Graph = PUNGraph::TObj::New();
388        do{
389          if (!Ss.GetInt(0, SrcNId) || !Ss.GetInt(1, DstNId)) {
390            if (!Ss.Eof()){
391              Ss.Next();
392              if (!Ss.Eof())
393                Marker = Ss.GetLnStr();
394            }
395            continue;
396          }
397          if (!Graph->IsNode(SrcNId)) { Graph->AddNode(SrcNId); }
398          if (!Graph->IsNode(DstNId)) { Graph->AddNode(DstNId); }
399          Graph->AddEdge(SrcNId, DstNId);
400          Ss.Next();
401          if (!Ss.Eof())
402            Marker = Ss.GetLnStr();
403        } while (Marker.GetCh(0) != '#' && !Ss.Eof());
404        if (Graph->GetNodes()>0) {
405          TSnap::DelSelfEdges(Graph);
406          TCnComV CmtyV;
407          TStr CmtyAlgStr;
408          if (CmtyAlg == 1) {
409            CmtyAlgStr = "Girvan-Newman";
410          }
411          else if (CmtyAlg == 2) {
412            CmtyAlgStr = "Clauset-Newman-Moore";
413          }
414          else if (CmtyAlg == 3) {
415            CmtyAlgStr = "Infomap";
416          }
417          else { Fail; }
418          TIntIntHH distCont;
419          if (br == 0) {
420            prev.Clr();
421            for (int c = 0; c < CmtyV.Len(); c++) {
422              for (int i = 0; i < CmtyV[c].Len(); i++){
423                prev.AddDat(CmtyV[c][i].Val, c);
424              }
425              prev_sizes.AddDat(c, CmtyV[c].Len());
426            }
427          }
428          else {
429            TIntH dist;
430            TIntH map;
431            int first_new_c_id = -1;
432            for (THashKeyDatI<TInt, TInt> it = prev_sizes.BegI(); !it.IsEnd(); it++)
433              if (it.GetKey() > first_new_c_id)
434                first_new_c_id = it.GetKey();
435            if (CmtyV.Len() - 1>first_new_c_id)
436              first_new_c_id = CmtyV.Len() - 1;
437            first_new_c_id++;
438            for (int c = 0; c < CmtyV.Len(); c++) {
439              TIntV stat;
440              TIntFltH statH1;
441              TIntFltH statH2;
442              for (THashKeyDatI<TInt, TInt> it = prev_sizes.BegI(); !it.IsEnd(); it++)
443                dist.AddDat(it.GetKey(), 0);
444              dist.AddDat(-1, 0);
445              for (int i = 0; i < CmtyV[c].Len(); i++) {
446                int id = CmtyV[c][i].Val;
447                int prev_comm = -1;
448                if (prev.IsKey(id))
449                  prev_comm = prev.GetDat(CmtyV[c][i].Val);
450                stat.Add(prev_comm);
451                int pre_val = dist.GetDat(prev_comm);
452                dist.AddDat(prev_comm, pre_val + 1);
453              }
454              double sumstat2 = 0;
455              for (THashKeyDatI<TInt, TInt> it = dist.BegI(); !it.IsEnd(); it++) {
456                int k = it.GetKey();
457                int d = it.GetDat();
458                if (d > 0){
459                  if (prev_sizes.IsKey(it.GetKey())){
460                    double stat1_ = (double)d / (double)prev_sizes.GetDat(k);
461                    statH1.AddDat(k, stat1_);
462                  }
463                  double stat2_ = (double)d / (double)CmtyV[c].Len();
464                  statH2.AddDat(k, stat2_);
465                  sumstat2 += stat2_;
466                  TIntV edge;
467                  edge.Add(k);
468                  edge.Add(c);
469                  edge.Add(d);
470                  edge.Add(br - 1);
471                  edge.Add(br);
472                  edges_.AddDat(edges_.Len() + 1, edge);
473                }
474                if (sumstat2 > 0.98) break;
475              }
476              int n_of_c_greater_than_half = 0;
477              int id_of_c_greater_than_half = -1;
478              TIntV ids_of_c_greater_than_half;
479              for (THashKeyDatI<TInt, TFlt> it = statH1.BegI(); !it.IsEnd(); it++){
480                if (it.GetDat()>alpha){
481                  id_of_c_greater_than_half = it.GetKey();
482                  ids_of_c_greater_than_half.Add(it.GetKey());
483                  n_of_c_greater_than_half++;
484                }
485              }
486              if (n_of_c_greater_than_half == 1){
487                map.AddDat(c, id_of_c_greater_than_half);
488              }
489              else{
490                int h2part_id = -2;
491                for (int i = 0; i<ids_of_c_greater_than_half.Len(); i++){
492                  double H2 = statH2.GetDat(ids_of_c_greater_than_half[i]);
493                  if (H2>beta){
494                    h2part_id = ids_of_c_greater_than_half[i];
495                  }
496                }
497                if (h2part_id != -2)
498                  map.AddDat(c, h2part_id);
499                else{
500                  map.AddDat(c, first_new_c_id);
501                  first_new_c_id++;
502                }
503              }
504              distCont.AddDat(c, dist);
505            }
506            prev.Clr();
507            prev_sizes.Clr();
508            for (int c = 0; c < CmtyV.Len(); c++){
509              for (int i = 0; i < CmtyV[c].Len(); i++){
510                prev.AddDat(CmtyV[c][i].Val, map.GetDat(c));
511              }
512              prev_sizes.AddDat(map.GetDat(c), CmtyV[c].Len());
513            }
514            for (THashKeyDatI<TInt, TIntV> it = edges_.BegI(); !it.IsEnd(); it++){
515              TIntV edgesV;
516              int a = it.GetDat()[0];
517              int b = it.GetDat()[1];
518              int v = it.GetDat()[2];
519              int d = it.GetDat()[3];
520              int e = it.GetDat()[4];
521              edgesV.Add(map.GetDat(b));
522              edgesV.Add(a);
523              edgesV.Add(v);
524              edgesV.Add(d);
525              edgesV.Add(e);
526              if (a != -1)
527                edges.AddDat(edges.Len(), edgesV);
528            }
529            edges_.Clr();
530          }
531          sizesCont.AddDat(br, prev_sizes);
532          cCont.AddDat(br, prev);
533          br++;
534        }
535      }
536      else Ss.Next();
537    }
538  }
539  void CmtyEvolutionJson(TStr& Json, TIntIntVH& sizesContV, TIntIntVH& cContV, TIntIntVH& edges){
540    Json.InsStr(Json.Len(), "{\n\"edges\":[\n");
541    TInt br = 0;
542    for (THashKeyDatI<TInt, TIntV> it = edges.BegI(); !it.IsEnd(); it++)
543    {
544      TInt n1 = it.GetDat()[1];
545      TInt n2 = it.GetDat()[0];
546      TInt w = it.GetDat()[2];
547      TInt t0 = it.GetDat()[3];
548      TInt t1 = it.GetDat()[4];
549      if (br>0)
550        Json.InsStr(Json.Len(), ",");
551      Json.InsStr(Json.Len(), "{\"n1\":"); Json.InsStr(Json.Len(), n1.GetStr());
552      Json.InsStr(Json.Len(), ", \"n2\":"); Json.InsStr(Json.Len(), n2.GetStr());
553      Json.InsStr(Json.Len(), ", \"w\":"); Json.InsStr(Json.Len(), w.GetStr());
554      Json.InsStr(Json.Len(), ", \"t0\":"); Json.InsStr(Json.Len(), t0.GetStr());
555      Json.InsStr(Json.Len(), ", \"t1\":"); Json.InsStr(Json.Len(), t1.GetStr());
556      Json.InsStr(Json.Len(), " }\n");
557      br++;
558    }
559    Json.InsStr(Json.Len(), "],\n\"communities\":[\n");
560    br = 0;
561    for (int i = 0; i < sizesContV[0].Len(); i++)
562    {
563      for (THashKeyDatI<TInt, TIntV> it = sizesContV.BegI(); !it.IsEnd(); it++)
564      {
565        TInt id = it.GetKey();
566        TInt size = it.GetDat()[i];
567        TInt j = i;
568        if (size > 0) {
569          if (br>0)
570            Json.InsStr(Json.Len(), ",");
571          TInt size = it.GetDat()[i];
572          Json.InsStr(Json.Len(), "{\"id\":"); Json.InsStr(Json.Len(), id.GetStr());
573          Json.InsStr(Json.Len(), ", \"size\":"); Json.InsStr(Json.Len(), size.GetStr());
574          Json.InsStr(Json.Len(), ", \"t\":"); Json.InsStr(Json.Len(), j.GetStr());
575          Json.InsStr(Json.Len(), " }\n");
576          br++;
577        }
578      }
579    }
580    Json.InsStr(Json.Len(), "]\n}");
581  }
582  TStr CmtyTest(TStr InFNm, int CmtyAlg){
583    TIntIntVH sizesContV;
584    TIntIntVH cContV;
585    TIntIntVH edges;
586    double alpha = 0.5;
587    double beta = 0.75;
588    CmtyEvolutionFileBatchV(InFNm, sizesContV, cContV, edges, alpha, beta, CmtyAlg);
589    TStr out;
590    CmtyEvolutionJson(out, sizesContV, cContV, edges);
591    return out;
592  }
593  void ReebSimplify(PNGraph& Graph, TIntH& t, int e, PNGraph& gFinal, TIntH& tFinal, bool collapse) {
594    TIntIntVH components;
595    TIntIntVH ct;
596    int newId = 0; 
597    int first = 429496729;
598    int last = -1;
599    TIntV timePoints;
600    for (THashKeyDatI<TInt, TInt> it = t.BegI(); !it.IsEnd(); it++) {
601      if (it.GetDat()<first)
602        first = it.GetDat();
603      if (it.GetDat()>last)
604        last = it.GetDat();
605    }
606    for (THashKeyDatI<TInt, TInt> it = t.BegI(); !it.IsEnd(); it++) {
607      if (it.GetDat() - (e / 2) >= first)
608        timePoints.Add(it.GetDat() - (e / 2) &bsol;*- 0.1*/);
609      timePoints.Add(it.GetDat());
610      if (it.GetDat() + (e / 2) <= last)
611        timePoints.Add(it.GetDat() + (e / 2) &bsol;*+ 0.1*/);
612    }
613    for (int i = 0; i<timePoints.Len(); i++) {
614      int focusTimePoint = timePoints[i];
615      TIntV fnodes; 
616      for (THashKeyDatI<TInt, TInt> it = t.BegI(); !it.IsEnd(); it++) {
617        if ((it.GetDat() <= focusTimePoint + (e / 2)) && (it.GetDat() >= focusTimePoint - (e / 2)))
618          fnodes.Add(it.GetKey());
619      }
620      PNGraph g1 = TNGraph::New();
621      for (int i = 0; i<fnodes.Len(); i++) {
622        if (!g1->IsNode(fnodes[i]))
623          g1->AddNode(fnodes[i]);
624        for (int j = 0; j<Graph->GetNI(fnodes[i]).GetInDeg(); j++) {
625          int NeighId = Graph->GetNI(fnodes[i]).GetInNId(j);
626          if (t.GetDat(NeighId)<focusTimePoint - (e / 2)) {
627          }
628          else {
629            if (!g1->IsNode(NeighId))
630              g1->AddNode(NeighId);
631            g1->AddEdge(NeighId, fnodes[i]);
632          }
633        }
634        for (int j = 0; j<Graph->GetNI(fnodes[i]).GetOutDeg(); j++) {
635          int NeighId = Graph->GetNI(fnodes[i]).GetOutNId(j);
636          if (t.GetDat(NeighId)>focusTimePoint + (e / 2)) {
637          }
638          else {
639            if (!g1->IsNode(NeighId))
640              g1->AddNode(NeighId);
641            g1->AddEdge(fnodes[i], NeighId);
642          }
643        }
644      }
645      TCnComV CnComV;
646      GetWccs(g1, CnComV);
647      TIntV communitiesAtT;
648      for (int cc = 0; cc < CnComV.Len(); cc++) {
649        components.AddDat(newId, CnComV[cc].NIdV);
650        communitiesAtT.Add(newId);
651        newId++;
652      }
653      if (CnComV.Len() > 0)
654        ct.AddDat(focusTimePoint, communitiesAtT);
655    } 
656    THashKeyDatI<TInt, TIntV> it = ct.BegI();
657    THashKeyDatI<TInt, TIntV> prelast = ct.EndI()--;
658    prelast--;
659    while (it < prelast) {
660      TIntV cms0;
661      TIntV cms1;
662      int focusTimePoint;
663      int focusTimePoint1;
664      focusTimePoint = it.GetKey();
665      cms0 = it.GetDat();
666      it++;
667      focusTimePoint1 = it.GetKey();
668      cms1 = it.GetDat();
669      if (cms0.Len()>0 && cms1.Len() > 0) {
670        for (int i = 0; i < cms0.Len(); i++) {
671          for (int j = 0; j < cms1.Len(); j++) {
672            TIntV ids0 = components.GetDat(cms0[i]);
673            TIntV ids1 = components.GetDat(cms1[j]);
674            if (ids0.IntrsLen(ids1) > 0 || TSnapDetail::edgeIntersect(Graph, ids0, ids1)) {
675              if (!gFinal->IsNode(cms0[i])) {
676                gFinal->AddNode(cms0[i]);
677                tFinal.AddDat(cms0[i], focusTimePoint);
678              }
679              if (!gFinal->IsNode(cms1[j])) {
680                gFinal->AddNode(cms1[j]);
681                tFinal.AddDat(cms1[j], focusTimePoint1);
682              }
683              gFinal->AddEdge(cms0[i], cms1[j]);
684            }
685          }
686        }
687      }
688    }
689    if (collapse) {
690      for (TNGraph::TNodeI NI = gFinal->BegNI(); NI < gFinal->EndNI(); NI++) {
691        if (NI.GetInDeg() == 1 && NI.GetOutDeg() == 1)
692          if (gFinal->GetNI(NI.GetInNId(0)).GetOutDeg() == 1 && gFinal->GetNI(NI.GetOutNId(0)).GetInDeg() == 1)
693          {
694          gFinal->AddEdge(NI.GetInNId(0), NI.GetOutNId(0));
695          gFinal->DelEdge(NI.GetInNId(0), NI.GetId());
696          tFinal.DelKey(NI.GetId());
697          gFinal->DelNode(NI.GetId());
698          }
699      }
700    }
701  }
702  void ReebRefine(PNGraph& Graph, TIntH& t, int e, PNGraph& gFinal, TIntH& tFinal, bool collapse) {
703    TIntIntVH components;
704    TIntIntVH ct;
705    int newId = 0; 
706    int first = 429496729;
707    int last = -1;
708    TIntV timePoints;
709    for (THashKeyDatI<TInt, TInt> it = t.BegI(); !it.IsEnd(); it++) {
710      if (it.GetDat() < first)
711        first = it.GetDat();
712      if (it.GetDat() > last)
713        last = it.GetDat();
714    }
715    for (THashKeyDatI<TInt, TInt> it = t.BegI(); !it.IsEnd(); it++) {
716      if (it.GetDat() - (e / 2) >= first)
717        timePoints.Add(it.GetDat() - (e / 2) &bsol;*- 0.1*/);
718      timePoints.Add(it.GetDat());
719      if (it.GetDat() + (e / 2) <= last)
720        timePoints.Add(it.GetDat() + (e / 2) &bsol;*+ 0.1*/);
721    }
722    TIntV timePointsUnique;
723    int prevtp = -1;
724    for (int i = 0; i < timePoints.Len(); i++){
725      if (timePoints[i] > prevtp)
726        timePointsUnique.Add(timePoints[i]);
727      prevtp = timePoints[i];
728    }
729    timePoints.Clr();
730    timePoints = timePointsUnique;
731    for (int i = 0; i < timePoints.Len(); i++) {
732      int focusTimePoint = timePoints[i];
733      TIntV fnodes; 
734      for (THashKeyDatI<TInt, TInt> it = t.BegI(); !it.IsEnd(); it++) {
735        if ((it.GetDat() <= focusTimePoint + (e / 2)) && (it.GetDat() >= focusTimePoint - (e / 2)))
736          fnodes.Add(it.GetKey());
737      }
738      PNGraph g1 = TNGraph::New();
739      for (int i = 0; i < fnodes.Len(); i++) {
740        if (!g1->IsNode(fnodes[i]))
741          g1->AddNode(fnodes[i]);
<span onclick='openModal()' class='match'>742        for (int j = 0; j < Graph->GetNI(fnodes[i]).GetInDeg(); j++) {
743          int NeighId = Graph->GetNI(fnodes[i]).GetInNId(j);
</span>744          if (t.GetDat(NeighId) < focusTimePoint - (e / 2)) {
745          }
746          else {
747            if (!g1->IsNode(NeighId))
748              g1->AddNode(NeighId);
749            g1->AddEdge(NeighId, fnodes[i]);
750          }
751        }
752        for (int j = 0; j < Graph->GetNI(fnodes[i]).GetOutDeg(); j++) {
753          int NeighId = Graph->GetNI(fnodes[i]).GetOutNId(j);
754          if (t.GetDat(NeighId) > focusTimePoint + (e / 2)) {
755          }
756          else {
757            if (!g1->IsNode(NeighId))
758              g1->AddNode(NeighId);
759            g1->AddEdge(fnodes[i], NeighId);
760          }
761        }
762      }
763      TIntH inCompCount;
764      TIntIntVH comps;
765      int compBr = 0;
766      TIntH nn_nodes;
767      int FTP = focusTimePoint;
768      TIntH TEdges;
769      for (TNGraph::TNodeI NI = g1->BegNI(); NI < g1->EndNI(); NI++) {
770        int FTPNode = NI.GetId();
771        TNGraph::TNodeI GNI = Graph->GetNI(FTPNode);
772        int FI, FO, RI, RO, I, O;
773        RI = NI.GetInDeg();
774        RO = NI.GetOutDeg();
775        FI = Graph->GetNI(FTPNode).GetInDeg() - RI;
776        FO = Graph->GetNI(FTPNode).GetOutDeg() - RO;
777        if (focusTimePoint + (e / 2) == t.GetDat(NI.GetId())) { 
778          RO = FO = 0;
779        }
780        if (focusTimePoint - (e / 2) == t.GetDat(NI.GetId())) { 
781          RI = FI = 0;
782        }
783        I = RI + FI;
784        O = RO + FO;
785        int temp = 0;
786        if (TEdges.IsKey(FTP))
787          temp = TEdges.GetDat(FTP);
788        TEdges.AddDat(FTP, O + temp);
789        if (I > 1 && O > 1) {
790          int nn = I;
791          if (O > I)
792            nn = O;
793          TIntV nds;
794          nds.Add(FTPNode);
795          for (int i = 0; i < I; i++) {
796            nds.Add(GNI.GetInNId(i));
797          }
798          for (int i = 0; i < O; i++) {
799            nds.Add(GNI.GetOutNId(i));
800          }
801          for (int j = 0; j < nn; j++) {
802            nn_nodes.AddDat(compBr);
803            comps.AddDat(compBr, nds);
804            compBr++;
805          }
806        }
807        else if (I == 1 && O > 1) {
808          for (int i = 0; i < O; i++) {
809            TIntV nds;
810            nds.Add(FTPNode);
811            nds.Add(GNI.GetInNId(0));
812            nds.Add(GNI.GetOutNId(i));
813            comps.AddDat(compBr, nds);
814            compBr++;
815          }
816        }
817        else if (I > 1 && O == 1) {
818          for (int i = 0; i < I; i++) {
819            TIntV nds;
820            nds.Add(FTPNode);
821            nds.Add(GNI.GetOutNId(0));
822            nds.Add(GNI.GetInNId(i));
823            comps.AddDat(compBr, nds);
824            compBr++;
825          }
826        }
827        else if (I == 0 && O > 1) {
828          for (int i = 0; i < O; i++) {
829            TIntV nds;
830            nds.Add(FTPNode);
831            nds.Add(GNI.GetOutNId(i));
832            comps.AddDat(compBr, nds);
833            compBr++;
834          }
835        }
836        else if (I > 1 && O == 0) {
837          for (int i = 0; i < I; i++) {
838            TIntV nds;
839            nds.Add(FTPNode);
840            nds.Add(GNI.GetInNId(i));
841            comps.AddDat(compBr, nds);
842            compBr++;
843          }
844        }
845        else if (I == 1 && O == 1) {
846          TIntV nds;
847          nds.Add(FTPNode);
848          nds.Add(GNI.GetOutNId(0));
849          nds.Add(GNI.GetInNId(0));
850          comps.AddDat(compBr, nds);
851          compBr++;
852        }
853        else if (I == 0 && O == 1) {
854          TIntV nds;
855          nds.Add(FTPNode);
856          nds.Add(GNI.GetOutNId(0));
857          comps.AddDat(compBr, nds);
858          compBr++;
859        }
860        else if (I == 1 && O == 0) {
861          TIntV nds;
862          nds.Add(FTPNode);
863          nds.Add(GNI.GetInNId(0));
864          comps.AddDat(compBr, nds);
865          compBr++;
866        }
867      } 
868      TIntIntVH elements;
869      TIntH banned;
870      for (int cc0 = 0; cc0 < comps.Len(); cc0++) {
871        for (int cc1 = cc0; cc1 < comps.Len(); cc1++) {
872          int smaller = comps[cc0].Len();
873          int smaller_id = cc0;
874          if (cc0 != cc1) {
875            if (comps[cc1].Len() < smaller) {
876              smaller = comps[cc1].Len();
877              smaller_id = cc1;
878            }
879            int vi = TSnapDetail::vectorIntersect(comps[cc0], comps[cc1]);
880            if (vi == smaller && !nn_nodes.IsKey(smaller_id)){
881              banned.AddDat(smaller_id);
882            }
883          }
884        }
885      }
886      for (int cc0 = 0; cc0 < comps.Len(); cc0++) {
887        if (!banned.IsKey(cc0) &bsol;*&& TSnapDetail::chekIfCrossing(comps[cc0], t, first, last, max_out_tp)*/)
888          elements.AddDat(cc0, comps[cc0]);
889      }
890      TIntV communitiesAtT;
891      for (int cc = 0; cc < elements.Len(); cc++) {
892        components.AddDat(newId, elements[cc]);
893      communitiesAtT.Add(newId);
894      newId++;
895      }
896      if (elements.Len() > 0)
897        ct.AddDat(focusTimePoint, communitiesAtT);
898    } 
899    THashKeyDatI<TInt, TIntV> it = ct.BegI();
900    THashKeyDatI<TInt, TIntV> prelast = ct.EndI()--;
901    prelast--;
902    while (it < prelast) {
903      TIntV cms0;
904      TIntV cms1;
905      int focusTimePoint;
906      int focusTimePoint1;
907      focusTimePoint = it.GetKey();
908      cms0 = it.GetDat();
909      it++;
910      focusTimePoint1 = it.GetKey();
911      cms1 = it.GetDat();
912      if (cms0.Len() > 0 && cms1.Len() > 0) {
913        for (int i = 0; i < cms0.Len(); i++) {
914          for (int j = 0; j < cms1.Len(); j++) {
915            TIntV ids0 = components.GetDat(cms0[i]);
916            TIntV ids1 = components.GetDat(cms1[j]);
917            int smaller = ids0.Len();
918            if (ids1.Len() < smaller)
919              smaller = ids1.Len();
920            if (TSnapDetail::vectorIntersect(ids0, ids1) == smaller || (smaller > 2 && TSnapDetail::vectorIntersect(ids0, ids1) == (smaller -1 ))) {
921              if (!gFinal->IsNode(cms0[i])) {
922                gFinal->AddNode(cms0[i]);
923                tFinal.AddDat(cms0[i], focusTimePoint);
924              }
925              if (!gFinal->IsNode(cms1[j])) {
926                gFinal->AddNode(cms1[j]);
927                tFinal.AddDat(cms1[j], focusTimePoint1);
928              }
929              gFinal->AddEdge(cms0[i], cms1[j]);
930            }
931          }
932        }
933      }
934    }
935    if (collapse) {
936      for (TNGraph::TNodeI NI = gFinal->BegNI(); NI < gFinal->EndNI(); NI++) {
937        if (NI.GetInDeg() == 1 && NI.GetOutDeg() == 1)
938          if (gFinal->GetNI(NI.GetInNId(0)).GetOutDeg() == 1 && gFinal->GetNI(NI.GetOutNId(0)).GetInDeg() == 1)
939          {
940          gFinal->AddEdge(NI.GetInNId(0), NI.GetOutNId(0));
941          gFinal->DelEdge(NI.GetInNId(0), NI.GetId());
942          tFinal.DelKey(NI.GetId());
943          gFinal->DelNode(NI.GetId());
944          }
945      }
946    }
947  }
948  namespace TSnapDetail {
949  class TCNMQMatrix {
950  private:
951    struct TCmtyDat {
952      double DegFrac;
953      TIntFltH NIdQH;
954      int MxQId;
955      TCmtyDat() : MxQId(-1) { }
956      TCmtyDat(const double& NodeDegFrac, const int& OutDeg) : 
957        DegFrac(NodeDegFrac), NIdQH(OutDeg), MxQId(-1) { }
958      void AddQ(const int& NId, const double& Q) {
959        NIdQH.AddDat(NId, Q);
960        if (MxQId == -1 || NIdQH[MxQId]<Q) { MxQId = NIdQH.GetKeyId(NId); }
961      }
962      void UpdateMaxQ() {
963        MxQId = -1;
964        for (int i = -1; NIdQH.FNextKeyId(i);) {
965          if (MxQId == -1 || NIdQH[MxQId]< NIdQH[i]) { MxQId = i; }
966        }
967      }
968      void DelLink(const int& K) {
969        const int NId = GetMxQNId();
970        NIdQH.DelKey(K); if (NId == K) { UpdateMaxQ(); }
971      }
972      int GetMxQNId() const { return NIdQH.GetKey(MxQId); }
973      double GetMxQ() const { return NIdQH[MxQId]; }
974    };
975  private:
976    THash<TInt, TCmtyDat> CmtyQH;
977    THeap<TFltIntIntTr> MxQHeap;
978    TUnionFind CmtyIdUF;
979    double Q;
980  public:
981    TCNMQMatrix(const PUNGraph& Graph) : CmtyQH(Graph->GetNodes()), 
982      MxQHeap(Graph->GetNodes()), CmtyIdUF(Graph->GetNodes()) {
983      Init(Graph);
984    }
985    void Init(const PUNGraph& Graph) {
986      const double M = 0.5 / Graph->GetEdges(); 
987      Q = 0.0;
988      for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++) {
989        CmtyIdUF.Add(NI.GetId());
990        const int OutDeg = NI.GetOutDeg();
991        if (OutDeg == 0) { continue; }
992        TCmtyDat& Dat = CmtyQH.AddDat(NI.GetId(), TCmtyDat(M * OutDeg, OutDeg));
993        for (int e = 0; e < NI.GetOutDeg(); e++) {
994          const int DstNId = NI.GetOutNId(e);
995          const double DstMod = 2 * M * (1.0 - OutDeg * Graph->GetNI(DstNId).GetOutDeg() * M);
996          Dat.AddQ(DstNId, DstMod);
997        }
998        Q += -1.0*TMath::Sqr(OutDeg*M);
999        if (NI.GetId() < Dat.GetMxQNId()) {
1000          MxQHeap.Add(TFltIntIntTr(Dat.GetMxQ(), NI.GetId(), Dat.GetMxQNId()));
1001        }
1002      }
1003      MxQHeap.MakeHeap();
1004    }
1005    TFltIntIntTr FindMxQEdge() {
1006      while (true) {
1007        if (MxQHeap.Empty()) { break; }
1008        const TFltIntIntTr TopQ = MxQHeap.PopHeap();
1009        if (!CmtyQH.IsKey(TopQ.Val2) || !CmtyQH.IsKey(TopQ.Val3)) { continue; }
1010        if (TopQ.Val1 != CmtyQH.GetDat(TopQ.Val2).GetMxQ() && TopQ.Val1 != CmtyQH.GetDat(TopQ.Val3).GetMxQ()) { continue; }
1011        return TopQ;
1012      }
1013      return TFltIntIntTr(-1, -1, -1);
1014    }
1015    bool MergeBestQ() {
1016      const TFltIntIntTr TopQ = FindMxQEdge();
1017      if (TopQ.Val1 <= 0.0) { return false; }
1018      const int I = TopQ.Val3;
1019      const int J = TopQ.Val2;
1020      CmtyIdUF.Union(I, J); 
1021      Q += TopQ.Val1;
1022      TCmtyDat& DatJ = CmtyQH.GetDat(J);
1023      { TCmtyDat& DatI = CmtyQH.GetDat(I);
1024      DatI.DelLink(J);  DatJ.DelLink(I);
1025      for (int i = -1; DatJ.NIdQH.FNextKeyId(i); ) {
1026        const int K = DatJ.NIdQH.GetKey(i);
1027        TCmtyDat& DatK = CmtyQH.GetDat(K);
1028        double NewQ = DatJ.NIdQH[i];
1029        if (DatI.NIdQH.IsKey(K)) { NewQ = NewQ + DatI.NIdQH.GetDat(K);  DatK.DelLink(I); }     
1030        else { NewQ = NewQ - 2 * DatI.DegFrac*DatK.DegFrac; }  
1031        DatJ.AddQ(K, NewQ);
1032        DatK.AddQ(J, NewQ);
1033        MxQHeap.PushHeap(TFltIntIntTr(NewQ, TMath::Mn(J, K), TMath::Mx(J, K)));
1034      }
1035      for (int i = -1; DatI.NIdQH.FNextKeyId(i); ) {
1036        const int K = DatI.NIdQH.GetKey(i);
1037        if (!DatJ.NIdQH.IsKey(K)) { 
1038          TCmtyDat& DatK = CmtyQH.GetDat(K);
1039          const double NewQ = DatI.NIdQH[i] - 2 * DatJ.DegFrac*DatK.DegFrac; 
1040          DatJ.AddQ(K, NewQ);
1041          DatK.DelLink(I);
1042          DatK.AddQ(J, NewQ);
1043          MxQHeap.PushHeap(TFltIntIntTr(NewQ, TMath::Mn(J, K), TMath::Mx(J, K)));
1044        }
1045      }
1046      DatJ.DegFrac += DatI.DegFrac; }
1047      if (DatJ.NIdQH.Empty()) { CmtyQH.DelKey(J); } 
1048      CmtyQH.DelKey(I);
1049      return true;
1050    }
1051    static double CmtyCMN(const PUNGraph& Graph, TCnComV& CmtyV) {
1052      TCNMQMatrix QMatrix(Graph);
1053      while (QMatrix.MergeBestQ()) {}
1054      THash<TInt, TIntV> IdCmtyH;
1055      for (TUNGraph::TNodeI NI = Graph->BegNI(); NI < Graph->EndNI(); NI++) {
1056        IdCmtyH.AddDat(QMatrix.CmtyIdUF.Find(NI.GetId())).Add(NI.GetId());
1057      }
1058      CmtyV.Gen(IdCmtyH.Len());
1059      for (int j = 0; j < IdCmtyH.Len(); j++) {
1060        CmtyV[j].NIdV.Swap(IdCmtyH[j]);
1061      }
1062      return QMatrix.Q;
1063    }
1064  };
1065  } 
1066  double CommunityCNM(const PUNGraph& Graph, TCnComV& CmtyV) {
1067    return TSnapDetail::TCNMQMatrix::CmtyCMN(Graph, CmtyV);
1068  }
1069  }; 
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from caffe-MDEwOlJlcG9zaXRvcnk2MTg3MDgwMw==-flat-test_net_2.cpp</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from snap-MDEwOlJlcG9zaXRvcnk0Mjg4MzU3-flat-cmty.cpp</div>
                </div>
                <div class="column column_space"><pre><code>2719    for (int i = 0; i < this->net_->layers().size(); ++i) {
2720      const string& layer_name = this->net_->layer_names()[i];
</pre></code></div>
                <div class="column column_space"><pre><code>742        for (int j = 0; j < Graph->GetNI(fnodes[i]).GetInDeg(); j++) {
743          int NeighId = Graph->GetNI(fnodes[i]).GetInNId(j);
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    