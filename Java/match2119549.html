<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for AbstractBytesReferenceTestCase.java & IndexShard.java</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for AbstractBytesReferenceTestCase.java & IndexShard.java
      </h3>
      <h1 align="center">
        10.4%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>AbstractBytesReferenceTestCase.java (23.773266%)<TH>IndexShard.java (6.671415%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#0',2,'match2119549-1.html#0',3)" NAME="0">(22-49)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#0',2,'match2119549-1.html#0',3)" NAME="0">(137-165)</A><TD ALIGN=center><FONT COLOR="#ff0000">25</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#1',2,'match2119549-1.html#1',3)" NAME="1">(219-225)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#1',2,'match2119549-1.html#1',3)" NAME="1">(1588-1598)</A><TD ALIGN=center><FONT COLOR="#840000">13</FONT>
<TR><TD BGCOLOR="#980517"><FONT COLOR="#980517">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#2',2,'match2119549-1.html#2',3)" NAME="2">(55-60)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#2',2,'match2119549-1.html#2',3)" NAME="2">(2232-2244)</A><TD ALIGN=center><FONT COLOR="#840000">13</FONT>
<TR><TD BGCOLOR="#53858b"><FONT COLOR="#53858b">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#3',2,'match2119549-1.html#3',3)" NAME="3">(514-521)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#3',2,'match2119549-1.html#3',3)" NAME="3">(1447-1455)</A><TD ALIGN=center><FONT COLOR="#7a0000">12</FONT>
<TR><TD BGCOLOR="#6cc417"><FONT COLOR="#6cc417">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#4',2,'match2119549-1.html#4',3)" NAME="4">(292-297)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#4',2,'match2119549-1.html#4',3)" NAME="4">(781-793)</A><TD ALIGN=center><FONT COLOR="#7a0000">12</FONT>
<TR><TD BGCOLOR="#151b8d"><FONT COLOR="#151b8d">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#5',2,'match2119549-1.html#5',3)" NAME="5">(550-555)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#5',2,'match2119549-1.html#5',3)" NAME="5">(2152-2163)</A><TD ALIGN=center><FONT COLOR="#700000">11</FONT>
<TR><TD BGCOLOR="#8c8774"><FONT COLOR="#8c8774">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#6',2,'match2119549-1.html#6',3)" NAME="6">(187-196)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#6',2,'match2119549-1.html#6',3)" NAME="6">(2396-2400)</A><TD ALIGN=center><FONT COLOR="#700000">11</FONT>
<TR><TD BGCOLOR="#38a4a5"><FONT COLOR="#38a4a5">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#7',2,'match2119549-1.html#7',3)" NAME="7">(602-606)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#7',2,'match2119549-1.html#7',3)" NAME="7">(1988-1992)</A><TD ALIGN=center><FONT COLOR="#660000">10</FONT>
<TR><TD BGCOLOR="#c58917"><FONT COLOR="#c58917">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#8',2,'match2119549-1.html#8',3)" NAME="8">(379-384)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#8',2,'match2119549-1.html#8',3)" NAME="8">(1520-1524)</A><TD ALIGN=center><FONT COLOR="#660000">10</FONT>
<TR><TD BGCOLOR="#83a33a"><FONT COLOR="#83a33a">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#9',2,'match2119549-1.html#9',3)" NAME="9">(135-140)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#9',2,'match2119549-1.html#9',3)" NAME="9">(1897-1901)</A><TD ALIGN=center><FONT COLOR="#660000">10</FONT>
<TR><TD BGCOLOR="#ad5910"><FONT COLOR="#ad5910">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#10',2,'match2119549-1.html#10',3)" NAME="10">(101-107)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#10',2,'match2119549-1.html#10',3)" NAME="10">(2302-2304)</A><TD ALIGN=center><FONT COLOR="#660000">10</FONT>
<TR><TD BGCOLOR="#b041ff"><FONT COLOR="#b041ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#11',2,'match2119549-1.html#11',3)" NAME="11">(625-631)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#11',2,'match2119549-1.html#11',3)" NAME="11">(1776-1787)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#571b7e"><FONT COLOR="#571b7e">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#12',2,'match2119549-1.html#12',3)" NAME="12">(616-620)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#12',2,'match2119549-1.html#12',3)" NAME="12">(1613-1618)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#3b9c9c"><FONT COLOR="#3b9c9c">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#13',2,'match2119549-1.html#13',3)" NAME="13">(578-582)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#13',2,'match2119549-1.html#13',3)" NAME="13">(1369-1383)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#842dce"><FONT COLOR="#842dce">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#14',2,'match2119549-1.html#14',3)" NAME="14">(574-578)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#14',2,'match2119549-1.html#14',3)" NAME="14">(796-803)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#f52887"><FONT COLOR="#f52887">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#15',2,'match2119549-1.html#15',3)" NAME="15">(565-570)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#15',2,'match2119549-1.html#15',3)" NAME="15">(277-279)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#2981b2"><FONT COLOR="#2981b2">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#16',2,'match2119549-1.html#16',3)" NAME="16">(542-545)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#16',2,'match2119549-1.html#16',3)" NAME="16">(1020-1029)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#3090c7"><FONT COLOR="#3090c7">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#17',2,'match2119549-1.html#17',3)" NAME="17">(536-541)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#17',2,'match2119549-1.html#17',3)" NAME="17">(2462-2465)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#800517"><FONT COLOR="#800517">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#18',2,'match2119549-1.html#18',3)" NAME="18">(484-491)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#18',2,'match2119549-1.html#18',3)" NAME="18">(2895-2901)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#f62817"><FONT COLOR="#f62817">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#19',2,'match2119549-1.html#19',3)" NAME="19">(451-457)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#19',2,'match2119549-1.html#19',3)" NAME="19">(2669-2673)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#4e9258"><FONT COLOR="#4e9258">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#20',2,'match2119549-1.html#20',3)" NAME="20">(309-314)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#20',2,'match2119549-1.html#20',3)" NAME="20">(1484-1490)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#947010"><FONT COLOR="#947010">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#21',2,'match2119549-1.html#21',3)" NAME="21">(275-280)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#21',2,'match2119549-1.html#21',3)" NAME="21">(1056-1070)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#4cc417"><FONT COLOR="#4cc417">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#22',2,'match2119549-1.html#22',3)" NAME="22">(270-275)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#22',2,'match2119549-1.html#22',3)" NAME="22">(754-757)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#f660ab"><FONT COLOR="#f660ab">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#23',2,'match2119549-1.html#23',3)" NAME="23">(263-268)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#23',2,'match2119549-1.html#23',3)" NAME="23">(770-778)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#79764d"><FONT COLOR="#79764d">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#24',2,'match2119549-1.html#24',3)" NAME="24">(238-246)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#24',2,'match2119549-1.html#24',3)" NAME="24">(2135-2142)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#5eac10"><FONT COLOR="#5eac10">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#25',2,'match2119549-1.html#25',3)" NAME="25">(144-149)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#25',2,'match2119549-1.html#25',3)" NAME="25">(1039-1048)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
<TR><TD BGCOLOR="#68818b"><FONT COLOR="#68818b">-</FONT><TD><A HREF="javascript:ZweiFrames('match2119549-0.html#26',2,'match2119549-1.html#26',3)" NAME="26">(114-123)<TD><A HREF="javascript:ZweiFrames('match2119549-0.html#26',2,'match2119549-1.html#26',3)" NAME="26">(2483-2486)</A><TD ALIGN=center><FONT COLOR="#5b0000">9</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>AbstractBytesReferenceTestCase.java</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the &quot;License&quot;); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
<A NAME="0"></A>
package org.elasticsearch.common.bytes;

<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match2119549-1.html#0',3,'match2119549-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.BytesRefBuilder;
import org.apache.lucene.util.BytesRefIterator;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.elasticsearch.common.io.stream.ReleasableBytesStreamOutput;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.util.BigArrays;
import org.elasticsearch.common.util.ByteArray;
import org.elasticsearch.common.util.PageCacheRecycler;
import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
import org.elasticsearch.test.ESTestCase;

import java.io.EOFException;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import java.nio.IntBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public abstract class AbstractBytesReferenceTestCase extends ESTestCase {

    protected static final int PAGE_SIZE = PageCacheRecycler.BYTE_PAGE_SIZE;
    protected final BigArrays bigarrays = new BigArrays(null, new NoneCircuitBreakerService(), CircuitBreaker.REQUEST)</B></FONT>;

    public void testGet() throws IOException {
<A NAME="2"></A>        int length = randomIntBetween(1, PAGE_SIZE * 3);
        BytesReference pbr = newBytesReference(length);

        <FONT color="#980517"><A HREF="javascript:ZweiFrames('match2119549-1.html#2',3,'match2119549-top.html#2',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>int sliceOffset = randomIntBetween(0, length / 2);
        int sliceLength = Math.max(1, length - sliceOffset - 1);
        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
        assertEquals(pbr.get(sliceOffset), slice.get(0));
        assertEquals(pbr.get(sliceOffset + sliceLength - 1), slice.get(sliceLength - 1));
        final int probes = randomIntBetween(20, 100)</B></FONT>;
        BytesReference copy = new BytesArray(pbr.toBytesRef(), true);
        for (int i = 0; i &lt; probes; i++) {
            int index = randomIntBetween(0, copy.length() - 1);
            assertEquals(pbr.get(index), copy.get(index));
            index = randomIntBetween(sliceOffset, sliceOffset + sliceLength - 1);
            assertEquals(pbr.get(index), slice.get(index - sliceOffset));
        }
    }

    public void testLength() throws IOException {
        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomInt(PAGE_SIZE * 3)};

        for (int i = 0; i &lt; sizes.length; i++) {
            BytesReference pbr = newBytesReference(sizes[i]);
            assertEquals(sizes[i], pbr.length());
        }
    }

    public void testSlice() throws IOException {
        for (int length : new int[] {0, 1, randomIntBetween(2, PAGE_SIZE), randomIntBetween(PAGE_SIZE + 1, 3 * PAGE_SIZE)}) {
            BytesReference pbr = newBytesReference(length);
            int sliceOffset = randomIntBetween(0, length / 2);
            int sliceLength = Math.max(0, length - sliceOffset - 1);
            BytesReference slice = pbr.slice(sliceOffset, sliceLength);
            assertEquals(sliceLength, slice.length());
            for (int i = 0; i &lt; sliceLength; i++) {
                assertEquals(pbr.get(i+sliceOffset), slice.get(i));
            }
            BytesRef singlePageOrNull = getSinglePageOrNull(slice);
            if (singlePageOrNull != null) {
                // we can't assert the offset since if the length is smaller than the refercence
                // the offset can be anywhere
                assertEquals(sliceLength, singlePageOrNull.length);
            }
        }
    }

<A NAME="10"></A>    public void testStreamInput() throws IOException {
        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
        BytesReference pbr = newBytesReference(length);
        StreamInput si = <FONT color="#ad5910"><A HREF="javascript:ZweiFrames('match2119549-1.html#10',3,'match2119549-top.html#10',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>pbr.streamInput();
        assertNotNull(si);

        // read single bytes one by one
        assertEquals(pbr.get(0), si.readByte());
        assertEquals(pbr.get(1), si.readByte());
        assertEquals(pbr.get</B></FONT>(2), si.readByte());

        // reset the stream for bulk reading
        si.reset();
<A NAME="26"></A>
        // buffer for bulk reads
        byte[] origBuf = new byte[length];
        <FONT color="#68818b"><A HREF="javascript:ZweiFrames('match2119549-1.html#26',3,'match2119549-top.html#26',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>random().nextBytes(origBuf);
        byte[] targetBuf = Arrays.copyOf(origBuf, origBuf.length);

        // bulk-read 0 bytes: must not modify buffer
        si.readBytes(targetBuf, 0, 0);
        assertEquals(origBuf[0], targetBuf[0]);
        si.reset();

        // read a few few bytes as ints
        int bytesToRead = randomIntBetween</B></FONT>(1, length / 2);
        for (int i = 0; i &lt; bytesToRead; i++) {
            int b = si.read();
            assertEquals(pbr.get(i) &amp; 0xff, b);
        }
        si.reset();

        // bulk-read all
        si.readFully(targetBuf);
<A NAME="9"></A>        assertArrayEquals(BytesReference.toBytes(pbr), targetBuf);

        // continuing to read should now fail with EOFException
        <FONT color="#83a33a"><A HREF="javascript:ZweiFrames('match2119549-1.html#9',3,'match2119549-top.html#9',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>try {
            si.readByte();
            fail(&quot;expected EOF&quot;);
        } catch (EOFException | IndexOutOfBoundsException eof) {
            // yay
        }</B></FONT>
<A NAME="25"></A>
        // try to read more than the stream contains
        si.reset();
        <FONT color="#5eac10"><A HREF="javascript:ZweiFrames('match2119549-1.html#25',3,'match2119549-top.html#25',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>expectThrows(IndexOutOfBoundsException.class, () -&gt;
            si.readBytes(targetBuf, 0, length * 2));
    }

    public void testStreamInputMarkAndReset() throws IOException {
        int length = randomIntBetween(10, scaledRandomIntBetween</B></FONT>(PAGE_SIZE * 2, PAGE_SIZE * 20));
        BytesReference pbr = newBytesReference(length);
        StreamInput si = pbr.streamInput();
        assertNotNull(si);

        StreamInput wrap = StreamInput.wrap(BytesReference.toBytes(pbr));
        while(wrap.available() &gt; 0) {
            if (rarely()) {
                wrap.mark(Integer.MAX_VALUE);
                si.mark(Integer.MAX_VALUE);
            } else if (rarely()) {
                wrap.reset();
                si.reset();
            }
            assertEquals(si.readByte(), wrap.readByte());
            assertEquals(si.available(), wrap.available());
        }
    }

    public void testStreamInputBulkReadWithOffset() throws IOException {
        final int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
        BytesReference pbr = newBytesReference(length);
        StreamInput si = pbr.streamInput();
        assertNotNull(si);

        // read a bunch of single bytes one by one
        int offset = randomIntBetween(1, length / 2);
        for (int i = 0; i &lt; offset; i++) {
            assertEquals(si.available(), length - i);
            assertEquals(pbr.get(i), si.readByte());
        }

        // now do NOT reset the stream - keep the stream's offset!

        // buffer to compare remaining bytes against bulk read
<A NAME="6"></A>        byte[] pbrBytesWithOffset = Arrays.copyOfRange(BytesReference.toBytes(pbr), offset, length);
        // randomized target buffer to ensure no stale slots
        byte[] targetBytes = new byte[pbrBytesWithOffset.length];
        <FONT color="#8c8774"><A HREF="javascript:ZweiFrames('match2119549-1.html#6',3,'match2119549-top.html#6',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>random().nextBytes(targetBytes);

        // bulk-read all
        si.readFully(targetBytes);
        assertArrayEquals(pbrBytesWithOffset, targetBytes);
        assertEquals(si.available(), 0);
    }

    public void testRandomReads() throws IOException {
        int length = randomIntBetween</B></FONT>(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
        BytesReference pbr = newBytesReference(length);
        StreamInput streamInput = pbr.streamInput();
        BytesRefBuilder target = new BytesRefBuilder();
        while (target.length() &lt; pbr.length()) {
            switch (randomIntBetween(0, 10)) {
                case 6:
                case 5:
                    target.append(new BytesRef(new byte[]{streamInput.readByte()}));
                    break;
                case 4:
                case 3:
                    BytesRef bytesRef = streamInput.readBytesRef(scaledRandomIntBetween(1, pbr.length() - target.length()));
                    target.append(bytesRef);
                    break;
                default:
                    byte[] buffer = new byte[scaledRandomIntBetween(1, pbr.length() - target.length())];
                    int offset = scaledRandomIntBetween(0, buffer.length - 1);
                    int read = streamInput.read(buffer, offset, buffer.length - offset);
                    target.append(new BytesRef(buffer, offset, read));
<A NAME="1"></A>                    break;
            }
        }
        assertEquals(<FONT color="#f63526"><A HREF="javascript:ZweiFrames('match2119549-1.html#1',3,'match2119549-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>pbr.length(), target.length());
        BytesRef targetBytes = target.get();
        assertArrayEquals(BytesReference.toBytes(pbr), Arrays.copyOfRange(targetBytes.bytes, targetBytes.offset, targetBytes.length));
    }

    public void testSliceStreamInput() throws IOException {
        int length = randomIntBetween</B></FONT>(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
        BytesReference pbr = newBytesReference(length);

        // test stream input over slice (upper half of original)
        int sliceOffset = randomIntBetween(1, length / 2);
        int sliceLength = length - sliceOffset;
        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
        StreamInput sliceInput = slice.streamInput();
        assertEquals(sliceInput.available(), sliceLength);

<A NAME="24"></A>        // single reads
        assertEquals(slice.get(0), sliceInput.readByte());
        assertEquals(slice.get(1), sliceInput.readByte());
        <FONT color="#79764d"><A HREF="javascript:ZweiFrames('match2119549-1.html#24',3,'match2119549-top.html#24',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>assertEquals(slice.get(2), sliceInput.readByte());
        assertEquals(sliceInput.available(), sliceLength - 3);

        // reset the slice stream for bulk reading
        sliceInput.reset();
        assertEquals(sliceInput.available(), sliceLength);

        // bulk read
        byte[] sliceBytes = new byte[sliceLength]</B></FONT>;
        sliceInput.readFully(sliceBytes);
        assertEquals(sliceInput.available(), 0);

        // compare slice content with upper half of original
        byte[] pbrSliceBytes = Arrays.copyOfRange(BytesReference.toBytes(pbr), sliceOffset, length);
        assertArrayEquals(pbrSliceBytes, sliceBytes);

        // compare slice bytes with bytes read from slice via streamInput :D
        byte[] sliceToBytes = BytesReference.toBytes(slice);
        assertEquals(sliceBytes.length, sliceToBytes.length);
        assertArrayEquals(sliceBytes, sliceToBytes);

        sliceInput.reset();
<A NAME="23"></A>        assertEquals(sliceInput.available(), sliceLength);
        byte[] buffer = new byte[sliceLength + scaledRandomIntBetween(1, 100)];
        int offset = scaledRandomIntBetween(0, Math.max(1, buffer.length - sliceLength - 1));
        int read = <FONT color="#f660ab"><A HREF="javascript:ZweiFrames('match2119549-1.html#23',3,'match2119549-top.html#23',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>sliceInput.read(buffer, offset, sliceLength / 2);
        assertEquals(sliceInput.available(), sliceLength - read);
        sliceInput.read(buffer, offset + read, sliceLength - read);
        assertArrayEquals(sliceBytes, Arrays.copyOfRange(buffer, offset, offset + sliceLength));
<A NAME="22"></A>        assertEquals(sliceInput.available(), 0);
    }</B></FONT>

    public void testWriteToOutputStream() throws IOException <FONT color="#4cc417"><A HREF="javascript:ZweiFrames('match2119549-1.html#22',3,'match2119549-top.html#22',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>{
        int length = randomIntBetween(10, PAGE_SIZE * 4);
<A NAME="21"></A>        BytesReference pbr = newBytesReference(length);
        BytesStreamOutput out = new BytesStreamOutput();
        pbr.writeTo(out);
        assertEquals</B></FONT>(pbr.length(), <FONT color="#947010"><A HREF="javascript:ZweiFrames('match2119549-1.html#21',3,'match2119549-top.html#21',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>out.size());
        assertArrayEquals(BytesReference.toBytes(pbr), BytesReference.toBytes(out.bytes()));
        out.close();
    }

    public void testInp</B></FONT>utStreamSkip() throws IOException {
        int length = randomIntBetween(10, scaledRandomIntBetween(PAGE_SIZE * 2, PAGE_SIZE * 20));
        BytesReference pbr = newBytesReference(length);
        final int iters = randomIntBetween(5, 50);
        for (int i = 0; i &lt; iters; i++) {
            try (StreamInput input = pbr.streamInput()) {
                final int offset = randomIntBetween(0, length-1);
                assertEquals(offset, input.skip(offset));
                assertEquals(pbr.get(offset), input.readByte());
<A NAME="4"></A>                if (offset == length - 1) {
                    continue; // no more bytes to retrieve!
                }
                final int nextOffset = <FONT color="#6cc417"><A HREF="javascript:ZweiFrames('match2119549-1.html#4',3,'match2119549-top.html#4',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>randomIntBetween(offset, length-2);
                assertEquals(nextOffset - offset, input.skip(nextOffset - offset));
                assertEquals(pbr.get(nextOffset+1), input.readByte()); // +1 for the one byte we read above
                assertEquals(length - (nextOffset+2), input.skip(Long.MAX_VALUE));
                assertEquals(0, input.skip(randomIntBetween(0, Integer.MAX_VALUE)));
            }</B></FONT>
        }
    }

    public void testSliceWriteToOutputStream() throws IOException {
        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 5));
        BytesReference pbr = newBytesReference(length);
        int sliceOffset = randomIntBetween(1, length / 2);
        int sliceLength = length - sliceOffset;
<A NAME="20"></A>        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
        BytesStreamOutput sliceOut = new BytesStreamOutput(sliceLength);
        slice.writeTo(sliceOut);
        assertEquals(slice.length(), <FONT color="#4e9258"><A HREF="javascript:ZweiFrames('match2119549-1.html#20',3,'match2119549-top.html#20',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>sliceOut.size());
        assertArrayEquals(BytesReference.toBytes(slice), BytesReference.toBytes(sliceOut.bytes()));
        sliceOut.close();
    }

    public void</B></FONT> testToBytes() throws IOException {
        int[] sizes = {0, randomInt(PAGE_SIZE), PAGE_SIZE, randomIntBetween(2, PAGE_SIZE * randomIntBetween(2, 5))};
        for (int i = 0; i &lt; sizes.length; i++) {
            BytesReference pbr = newBytesReference(sizes[i]);
            byte[] bytes = BytesReference.toBytes(pbr);
            assertEquals(sizes[i], bytes.length);
            for (int j = 0; j  &lt; bytes.length; j++) {
                assertEquals(bytes[j], pbr.get(j));
            }
        }
    }

    public void testToBytesRefSharedPage() throws IOException {
        int length = randomIntBetween(10, PAGE_SIZE);
        BytesReference pbr = newBytesReference(length);
        BytesArray ba = new BytesArray(pbr.toBytesRef());
        BytesArray ba2 = new BytesArray(pbr.toBytesRef());
        assertNotNull(ba);
        assertNotNull(ba2);
        assertEquals(pbr.length(), ba.length());
        assertEquals(ba.length(), ba2.length());
        // single-page optimization
        assertSame(ba.array(), ba2.array());
    }

    public void testToBytesRefMaterializedPages() throws IOException {
        // we need a length != (n * pagesize) to avoid page sharing at boundaries
        int length = 0;
        while ((length % PAGE_SIZE) == 0) {
            length = randomIntBetween(PAGE_SIZE, PAGE_SIZE * randomIntBetween(2, 5));
        }
        BytesReference pbr = newBytesReference(length);
        BytesArray ba = new BytesArray(pbr.toBytesRef());
        BytesArray ba2 = new BytesArray(pbr.toBytesRef());
        assertNotNull(ba);
        assertNotNull(ba2);
        assertEquals(pbr.length(), ba.length());
        assertEquals(ba.length(), ba2.length());
    }

    public void testCopyBytesRefSharesBytes() throws IOException {
        // small PBR which would normally share the first page
        int length = randomIntBetween(10, PAGE_SIZE);
        BytesReference pbr = newBytesReference(length);
        BytesArray ba = new BytesArray(pbr.toBytesRef(), true);
        BytesArray ba2 = new BytesArray(pbr.toBytesRef(), true);
        assertNotNull(ba);
        assertNotSame(ba, ba2);
        assertNotSame(ba.array(), ba2.array());
    }

    public void testSliceCopyBytesRef() throws IOException {
        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
        BytesReference pbr = newBytesReference(length);
        int sliceOffset = randomIntBetween(0, pbr.length());
        int sliceLength = randomIntBetween(0, pbr.length() - sliceOffset);
        BytesReference slice = pbr.slice(sliceOffset, sliceLength);

        BytesArray ba1 = new BytesArray(slice.toBytesRef(), true);
        BytesArray ba2 = new BytesArray(slice.toBytesRef(), true);
        assertNotNull(ba1);
        assertNotNull(ba2);
<A NAME="8"></A>        assertNotSame(ba1.array(), ba2.array());
        assertArrayEquals(BytesReference.toBytes(slice), ba1.array());
        assertArrayEquals(BytesReference.toBytes(slice), ba2.array());
        assertArrayEquals(<FONT color="#c58917"><A HREF="javascript:ZweiFrames('match2119549-1.html#8',3,'match2119549-top.html#8',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>ba1.array(), ba2.array());
    }

    public void testEmptyToBytesRefIterator() throws IOException {
        BytesReference pbr = newBytesReference(0);
        assertNull(pbr.iterator().next</B></FONT>());
    }

    public void testIterator() throws IOException {
        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
        BytesReference pbr = newBytesReference(length);
        BytesRefIterator iterator = pbr.iterator();
        BytesRef ref;
        BytesRefBuilder builder = new BytesRefBuilder();
        while((ref = iterator.next()) != null) {
            builder.append(ref);
        }
        assertArrayEquals(BytesReference.toBytes(pbr), BytesRef.deepCopyOf(builder.toBytesRef()).bytes);
    }

    public void testSliceIterator() throws IOException {
        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
        BytesReference pbr = newBytesReference(length);
        int sliceOffset = randomIntBetween(0, pbr.length());
        int sliceLength = randomIntBetween(0, pbr.length() - sliceOffset);
        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
        BytesRefIterator iterator = slice.iterator();
        BytesRef ref = null;
        BytesRefBuilder builder = new BytesRefBuilder();
        while((ref = iterator.next()) != null) {
            builder.append(ref);
        }
        assertArrayEquals(BytesReference.toBytes(slice), BytesRef.deepCopyOf(builder.toBytesRef()).bytes);
    }

    public void testIteratorRandom() throws IOException {
        int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
        BytesReference pbr = newBytesReference(length);
        if (randomBoolean()) {
            int sliceOffset = randomIntBetween(0, pbr.length());
            int sliceLength = randomIntBetween(0, pbr.length() - sliceOffset);
            pbr = pbr.slice(sliceOffset, sliceLength);
        }

        if (randomBoolean()) {
            pbr = new BytesArray(pbr.toBytesRef());
        }
        BytesRefIterator iterator = pbr.iterator();
        BytesRef ref = null;
        BytesRefBuilder builder = new BytesRefBuilder();
        while((ref = iterator.next()) != null) {
            builder.append(ref);
        }
        assertArrayEquals(BytesReference.toBytes(pbr), BytesRef.deepCopyOf(builder.toBytesRef()).bytes);
    }

    public void testArrayOffset() throws IOException {
        int length = randomInt(PAGE_SIZE * randomIntBetween(2, 5));
        BytesReference pbr = newBytesReference(length);
        BytesRef singlePageOrNull = getSinglePageOrNull(pbr);
        if (singlePageOrNull != null) {
            assertEquals(0, singlePageOrNull.offset);
        }
    }

    public void testSliceArrayOffset() throws IOException {
        int length = randomIntBetween(1, PAGE_SIZE * randomIntBetween(2, 5));
        BytesReference pbr = newBytesReferenceWithOffsetOfZero(length);
        int sliceOffset = randomIntBetween(0, pbr.length() - 1); // an offset to the end would be len 0
<A NAME="19"></A>        int sliceLength = randomIntBetween(1, pbr.length() - sliceOffset);
        BytesReference slice = pbr.slice(sliceOffset, sliceLength);
        BytesRef singlePageOrNull = getSinglePageOrNull(slice);
        if (singlePageOrNull != null) <FONT color="#f62817"><A HREF="javascript:ZweiFrames('match2119549-1.html#19',3,'match2119549-top.html#19',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>{
            if (getSinglePageOrNull(pbr) == null) {
                // original reference has pages
                assertEquals(sliceOffset % PAGE_SIZE, singlePageOrNull.offset);
            } else {
                // orig ref has no pages ie. BytesArray
                assertEquals</B></FONT>(sliceOffset, singlePageOrNull.offset);
            }
        }
    }

    public void testToUtf8() throws IOException {
        // test empty
        BytesReference pbr = newBytesReference(0);
        assertEquals(&quot;&quot;, pbr.utf8ToString());
        // TODO: good way to test?
    }

    public void testToBytesRef() throws IOException {
        int length = randomIntBetween(0, PAGE_SIZE);
        BytesReference pbr = newBytesReference(length);
        BytesRef ref = pbr.toBytesRef();
        assertNotNull(ref);
        assertEquals(pbr.length(), ref.length);
    }

    public void testSliceToBytesRef() throws IOException {
        int length = randomIntBetween(0, PAGE_SIZE);
        BytesReference pbr = newBytesReferenceWithOffsetOfZero(length);
        // get a BytesRef from a slice
<A NAME="18"></A>        int sliceOffset = randomIntBetween(0, pbr.length());
        int sliceLength = randomIntBetween(0, pbr.length() - sliceOffset);

        BytesRef sliceRef = <FONT color="#800517"><A HREF="javascript:ZweiFrames('match2119549-1.html#18',3,'match2119549-top.html#18',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>pbr.slice(sliceOffset, sliceLength).toBytesRef();

        if (sliceLength == 0 &amp;&amp; sliceOffset != sliceRef.offset) {
            // some impls optimize this to an empty instance then the offset will be 0
            assertEquals(0, sliceRef.offset);
        } else {
            // note that these are only true if we have &lt;= than a page, otherwise offset/length are shifted
            assertEquals</B></FONT>(sliceOffset, sliceRef.offset);
        }
        assertEquals(sliceLength, sliceRef.length);
    }

    public void testHashCode() throws IOException {
        // empty content must have hash 1 (JDK compat)
        BytesReference pbr = newBytesReference(0);
        assertEquals(Arrays.hashCode(BytesRef.EMPTY_BYTES), pbr.hashCode());

        // test with content
        pbr = newBytesReference(randomIntBetween(0, PAGE_SIZE * randomIntBetween(2, 5)));
        int jdkHash = Arrays.hashCode(BytesReference.toBytes(pbr));
        int pbrHash = pbr.hashCode();
        assertEquals(jdkHash, pbrHash);

        // test hashes of slices
        int sliceFrom = randomIntBetween(0, pbr.length());
        int sliceLength = randomIntBetween(0, pbr.length() - sliceFrom);
        BytesReference slice = pbr.slice(sliceFrom, sliceLength);
<A NAME="3"></A>        int sliceJdkHash = Arrays.hashCode(BytesReference.toBytes(slice));
        int sliceHash = slice.hashCode();
        assertEquals(sliceJdkHash, sliceHash);
    <FONT color="#53858b"><A HREF="javascript:ZweiFrames('match2119549-1.html#3',3,'match2119549-top.html#3',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    public void testEquals() throws IOException {
        BytesReference bytesReference = newBytesReference(randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5)));
        BytesReference copy = bytesReference.slice(0, bytesReference.length());

        // get refs &amp; compare
        assertEquals</B></FONT>(copy, bytesReference);
        int sliceFrom = randomIntBetween(0, bytesReference.length());
        int sliceLength = randomIntBetween(0, bytesReference.length() - sliceFrom);
        assertEquals(copy.slice(sliceFrom, sliceLength), bytesReference.slice(sliceFrom, sliceLength));

        BytesRef bytesRef = BytesRef.deepCopyOf(copy.toBytesRef());
        assertEquals(new BytesArray(bytesRef), copy);

        int offsetToFlip = randomIntBetween(0, bytesRef.length - 1);
        int value = ~Byte.toUnsignedInt(bytesRef.bytes[bytesRef.offset+offsetToFlip]);
        bytesRef.bytes[bytesRef.offset+offsetToFlip] = (byte)value;
        assertNotEquals(new BytesArray(bytesRef), copy);
<A NAME="17"></A>    }

    public void testSliceEquals() {
        <FONT color="#3090c7"><A HREF="javascript:ZweiFrames('match2119549-1.html#17',3,'match2119549-top.html#17',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>int length = randomIntBetween(100, PAGE_SIZE * randomIntBetween(2, 5));
        ByteArray ba1 = bigarrays.newByteArray(length, false);
        BytesReference pbr = new PagedBytesReference(ba1, length);
<A NAME="16"></A>
        // test equality of slices
        int sliceFrom = randomIntBetween</B></FONT>(0, pbr.length());
        int sliceLength = <FONT color="#2981b2"><A HREF="javascript:ZweiFrames('match2119549-1.html#16',3,'match2119549-top.html#16',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>randomIntBetween(0, pbr.length() - sliceFrom);
        BytesReference slice1 = pbr.slice(sliceFrom, sliceLength);
        BytesReference slice2 = pbr.slice(sliceFrom, sliceLength);
        assertArrayEquals(BytesReference.toBytes(slice1), BytesReference.toBytes</B></FONT>(slice2));

<A NAME="5"></A>        // test a slice with same offset but different length,
        // unless randomized testing gave us a 0-length slice.
        if (sliceLength &gt; 0) {
            BytesReference slice3 = <FONT color="#151b8d"><A HREF="javascript:ZweiFrames('match2119549-1.html#5',3,'match2119549-top.html#5',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>pbr.slice(sliceFrom, sliceLength / 2);
            assertFalse(Arrays.equals(BytesReference.toBytes(slice1), BytesReference.toBytes(slice3)));
        }
    }

    protected abstract BytesReference newBytesReference(int length</B></FONT>) throws IOException;

    protected abstract BytesReference newBytesReferenceWithOffsetOfZero(int length) throws IOException;

    public void testCompareTo() throws IOException {
        final int iters = randomIntBetween(5, 10);
        for (int i = 0; i &lt; iters; i++) {
<A NAME="15"></A>            int length = randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
            BytesReference bytesReference = newBytesReference(length);
            assertTrue(bytesReference.compareTo(new BytesArray(&quot;&quot;)) &gt; 0);
            assertTrue(<FONT color="#f52887"><A HREF="javascript:ZweiFrames('match2119549-1.html#15',3,'match2119549-top.html#15',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>new BytesArray(&quot;&quot;).compareTo(bytesReference) &lt; 0);


            assertEquals(0, bytesReference.compareTo(bytesReference));
            int sliceFrom = randomIntBetween(0, bytesReference.length());
            int sliceLength = randomIntBetween(0, bytesReference.length</B></FONT>() - sliceFrom);
<A NAME="14"></A>            BytesReference slice = bytesReference.slice(sliceFrom, sliceLength);

            assertEquals(bytesReference.toBytesRef().compareTo(slice.toBytesRef()),
                new BytesArray(<FONT color="#842dce"><A HREF="javascript:ZweiFrames('match2119549-1.html#14',3,'match2119549-top.html#14',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>bytesReference.toBytesRef(), true).compareTo(new BytesArray(slice.toBytesRef(), true)));
<A NAME="13"></A>
            assertEquals(bytesReference.toBytesRef().compareTo(slice.toBytesRef()),
                bytesReference.compareTo(slice));
            assertEquals</B></FONT>(<FONT color="#3b9c9c"><A HREF="javascript:ZweiFrames('match2119549-1.html#13',3,'match2119549-top.html#13',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>slice.toBytesRef().compareTo(bytesReference.toBytesRef()),
                slice.compareTo(bytesReference));

            assertEquals(0, slice.compareTo(new BytesArray(slice.toBytesRef())));
            assertEquals(0, new BytesArray(slice.toBytesRef()).compareTo</B></FONT>(slice));

            final int crazyLength = length + randomIntBetween(10, PAGE_SIZE * randomIntBetween(2, 8));
            ReleasableBytesStreamOutput crazyStream = new ReleasableBytesStreamOutput(length, bigarrays);
            final int offset = randomIntBetween(0, crazyLength - length);
            for (int j = 0; j &lt; offset; j++) {
                crazyStream.writeByte((byte) random().nextInt(1 &lt;&lt; 8));
            }
            bytesReference.writeTo(crazyStream);
            for (int j = crazyStream.size(); j &lt; crazyLength; j++) {
                crazyStream.writeByte((byte) random().nextInt(1 &lt;&lt; 8));
            }
            BytesReference crazyReference = crazyStream.bytes();

            assertFalse(crazyReference.compareTo(bytesReference) == 0);
            assertEquals(0, crazyReference.slice(offset, length).compareTo(
                bytesReference));
<A NAME="7"></A>            assertEquals(0, bytesReference.compareTo(
                crazyReference.slice(offset, length)));
        }
    <FONT color="#38a4a5"><A HREF="javascript:ZweiFrames('match2119549-1.html#7',3,'match2119549-top.html#7',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    public static BytesRef getSinglePageOrNull(BytesReference ref) throws IOException {
        if (ref.length() &gt; 0) {
            BytesRefIterator iterator = ref.iterator</B></FONT>();
            BytesRef next = iterator.next();
            BytesRef retVal = next.clone();
            if (iterator.next() == null) {
                return retVal;
            }
        } else {
<A NAME="12"></A>            return new BytesRef();
        }
        return null;
    <FONT color="#571b7e"><A HREF="javascript:ZweiFrames('match2119549-1.html#12',3,'match2119549-top.html#12',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    public static int getNumPages(BytesReference ref) throws IOException {
        int num = 0;
        if (ref.length() &gt; 0) {</B></FONT>
            BytesRefIterator iterator = ref.iterator();
<A NAME="11"></A>            while(iterator.next() != null) {
                num++;
            }
        <FONT color="#b041ff"><A HREF="javascript:ZweiFrames('match2119549-1.html#11',3,'match2119549-top.html#11',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}
        return num;
    }


    public void testBasicEquals() {
        final int len = randomIntBetween</B></FONT>(0, randomBoolean() ? 10: 100000);
        final int offset1 = randomInt(5);
        final byte[] array1 = new byte[offset1 + len + randomInt(5)];
        random().nextBytes(array1);
        final int offset2 = randomInt(offset1);
        final byte[] array2 = Arrays.copyOfRange(array1, offset1 - offset2, array1.length);

        final BytesArray b1 = new BytesArray(array1, offset1, len);
        final BytesArray b2 = new BytesArray(array2, offset2, len);
        assertEquals(b1, b2);
        assertEquals(Arrays.hashCode(BytesReference.toBytes(b1)), b1.hashCode());
        assertEquals(Arrays.hashCode(BytesReference.toBytes(b2)), b2.hashCode());

        // test same instance
        assertEquals(b1, b1);
        assertEquals(b2, b2);

        if (len &gt; 0) {
            // test different length
            BytesArray differentLen = new BytesArray(array1, offset1, randomInt(len - 1));
            assertNotEquals(b1, differentLen);

            // test changed bytes
            array1[offset1 + randomInt(len - 1)] += 13;
            assertNotEquals(b1, b2);
        }
    }

    public void testGetInt() throws IOException {
        final int count = randomIntBetween(1, 10);
        final BytesReference bytesReference = newBytesReference(count * Integer.BYTES);
        final BytesRef bytesRef = bytesReference.toBytesRef();
        final IntBuffer intBuffer =
            ByteBuffer.wrap(bytesRef.bytes, bytesRef.offset, bytesRef.length).order(ByteOrder.BIG_ENDIAN).asIntBuffer();
        for (int i = 0; i &lt; count; ++i) {
            assertEquals(intBuffer.get(i), bytesReference.getInt(i * Integer.BYTES));
        }
    }

    public void testIndexOf() throws IOException {
        final int size = randomIntBetween(0, 100);
        final BytesReference bytesReference = newBytesReference(size);
        final Map&lt;Byte, List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;();
        for (int i = 0; i &lt; size; ++i) {
            final byte value = bytesReference.get(i);
            map.computeIfAbsent(value, v -&gt; new ArrayList&lt;&gt;()).add(i);
        }
        map.forEach((value, positions) -&gt; {
            for (int i = 0; i &lt; positions.size(); i++) {
                final int pos = positions.get(i);
                final int from = i == 0 ? randomIntBetween(0, pos) : positions.get(i - 1) + 1;
                assertEquals(bytesReference.indexOf(value, from), pos);
            }
        });
        final byte missing = randomValueOtherThanMany(map::containsKey, ESTestCase::randomByte);
        assertEquals(-1, bytesReference.indexOf(missing, randomIntBetween(0, Math.max(0, size - 1))));
    }
}
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>IndexShard.java</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the &quot;License&quot;); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.elasticsearch.index.shard;

import static org.elasticsearch.index.seqno.SequenceNumbers.UNASSIGNED_SEQ_NO;

import java.io.Closeable;
import java.io.IOException;
import java.io.PrintStream;
import java.nio.channels.ClosedByInterruptException;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.Collections;
import java.util.EnumSet;
import java.util.HashSet;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import java.util.Set;
import java.util.concurrent.CopyOnWriteArrayList;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;
import java.util.function.BiConsumer;
import java.util.function.Consumer;
import java.util.function.Function;
import java.util.function.LongSupplier;
import java.util.stream.Collectors;
import java.util.stream.StreamSupport;

import javax.annotation.Nullable;

import com.carrotsearch.hppc.ObjectLongMap;

import org.apache.logging.log4j.Logger;
import org.apache.logging.log4j.message.ParameterizedMessage;
import org.apache.lucene.index.CheckIndex;
import org.apache.lucene.index.IndexCommit;
import org.apache.lucene.index.SegmentInfos;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.QueryCachingPolicy;
import org.apache.lucene.search.ReferenceManager;
import org.apache.lucene.search.UsageTrackingQueryCachingPolicy;
import org.apache.lucene.store.AlreadyClosedException;
import org.apache.lucene.util.SetOnce;
import org.apache.lucene.util.ThreadInterruptedException;
import org.elasticsearch.Assertions;
import org.elasticsearch.ElasticsearchException;
import org.elasticsearch.ExceptionsHelper;
import org.elasticsearch.Version;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.ActionRunnable;
import org.elasticsearch.action.admin.indices.flush.FlushRequest;
import org.elasticsearch.action.admin.indices.forcemerge.ForceMergeRequest;
import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeRequest;
import org.elasticsearch.action.support.replication.ReplicationResponse;
import org.elasticsearch.cluster.metadata.IndexMetadata;
import org.elasticsearch.cluster.metadata.MappingMetadata;
import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
import org.elasticsearch.cluster.routing.RecoverySource;
import org.elasticsearch.cluster.routing.RecoverySource.SnapshotRecoverySource;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.CheckedConsumer;
import org.elasticsearch.common.CheckedRunnable;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.elasticsearch.common.lease.Releasable;
import org.elasticsearch.common.lucene.Lucene;
import org.elasticsearch.common.metrics.CounterMetric;
import org.elasticsearch.common.metrics.MeanMetric;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.unit.ByteSizeValue;
import org.elasticsearch.common.util.BigArrays;
import org.elasticsearch.common.util.concurrent.AbstractRunnable;
import org.elasticsearch.common.util.concurrent.AsyncIOProcessor;
import org.elasticsearch.common.util.concurrent.RunOnce;
import org.elasticsearch.common.xcontent.XContentHelper;
import org.elasticsearch.gateway.WriteStateException;
import org.elasticsearch.index.Index;
import org.elasticsearch.index.IndexModule;
import org.elasticsearch.index.IndexNotFoundException;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.IndexSettings;
import org.elasticsearch.index.VersionType;
import org.elasticsearch.index.cache.IndexCache;
import org.elasticsearch.index.codec.CodecService;
import org.elasticsearch.index.engine.CommitStats;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.engine.EngineConfig;
import org.elasticsearch.index.engine.EngineException;
import org.elasticsearch.index.engine.EngineFactory;
import org.elasticsearch.index.engine.ReadOnlyEngine;
import org.elasticsearch.index.engine.RefreshFailedEngineException;
import org.elasticsearch.index.engine.SafeCommitInfo;
import org.elasticsearch.index.engine.Segment;
import org.elasticsearch.index.mapper.DocumentMapper;
import org.elasticsearch.index.mapper.IdFieldMapper;
import org.elasticsearch.index.mapper.MapperService;
import org.elasticsearch.index.mapper.Mapping;
import org.elasticsearch.index.mapper.ParsedDocument;
import org.elasticsearch.index.mapper.RootObjectMapper;
import org.elasticsearch.index.mapper.SourceToParse;
import org.elasticsearch.index.mapper.Uid;
import org.elasticsearch.index.recovery.RecoveryStats;
import org.elasticsearch.index.seqno.ReplicationTracker;
import org.elasticsearch.index.seqno.RetentionLease;
import org.elasticsearch.index.seqno.RetentionLeaseStats;
import org.elasticsearch.index.seqno.RetentionLeaseSyncer;
import org.elasticsearch.index.seqno.RetentionLeases;
import org.elasticsearch.index.seqno.SeqNoStats;
import org.elasticsearch.index.seqno.SequenceNumbers;
<A NAME="0"></A>import org.elasticsearch.index.shard.PrimaryReplicaSyncer.ResyncTask;
import org.elasticsearch.index.store.Store;
import org.elasticsearch.index.store.Store.MetadataSnapshot;
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#0',2,'match2119549-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>import org.elasticsearch.index.store.StoreFileMetadata;
import org.elasticsearch.index.store.StoreStats;
import org.elasticsearch.index.translog.Translog;
import org.elasticsearch.index.translog.TranslogConfig;
import org.elasticsearch.index.translog.TranslogStats;
import org.elasticsearch.indices.IndexingMemoryController;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.indices.breaker.CircuitBreakerService;
import org.elasticsearch.indices.cluster.IndicesClusterStateService;
import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;
import org.elasticsearch.indices.recovery.RecoveryFailedException;
import org.elasticsearch.indices.recovery.RecoveryState;
import org.elasticsearch.indices.recovery.RecoveryTarget;
import org.elasticsearch.repositories.RepositoriesService;
import org.elasticsearch.repositories.Repository;
import org.elasticsearch.rest.RestStatus;
import org.elasticsearch.threadpool.ThreadPool;

import io.crate.common.Booleans;
import io.crate.common.collections.Tuple;
import io.crate.common.io.IOUtils;
import io.crate.common.unit.TimeValue;
import io.crate.exceptions.Exceptions;

public class IndexShard extends AbstractIndexShardComponent implements IndicesClusterStateService.Shard {

    public static final long RETAIN_ALL = -1;

    private final ThreadPool threadPool</B></FONT>;
    private final MapperService mapperService;
    private final IndexCache indexCache;
    private final Store store;
    private final Object mutex = new Object();
    private final String checkIndexOnStartup;
    private final CodecService codecService;
    private final TranslogConfig translogConfig;
    private final IndexEventListener indexEventListener;
    private final QueryCachingPolicy cachingPolicy;
    // Package visible for testing
    final CircuitBreakerService circuitBreakerService;

    private final GlobalCheckpointListeners globalCheckpointListeners;
    private final ReplicationTracker replicationTracker;

    protected volatile ShardRouting shardRouting;
    protected volatile IndexShardState state;
    // ensure happens-before relation between addRefreshListener() and postRecovery()
    private final Object postRecoveryMutex = new Object();
    protected volatile long pendingPrimaryTerm; // see JavaDocs for getPendingPrimaryTerm
    private final Object engineMutex = new Object(); // lock ordering: engineMutex -&gt; mutex
    private final AtomicReference&lt;Engine&gt; currentEngineReference = new AtomicReference&lt;&gt;();
    final EngineFactory engineFactory;

    private final IndexingOperationListener indexingOperationListeners;
    private final Runnable globalCheckpointSyncer;
    private final RetentionLeaseSyncer retentionLeaseSyncer;

    Runnable getGlobalCheckpointSyncer() {
        return globalCheckpointSyncer;
    }

    public RetentionLeaseSyncer getRetentionLeaseSyncer() {
        return retentionLeaseSyncer;
    }

    @Nullable
    private volatile RecoveryState recoveryState;

    private final RecoveryStats recoveryStats = new RecoveryStats();
    private final MeanMetric refreshMetric = new MeanMetric();
    private final MeanMetric flushMetric = new MeanMetric();
    private final CounterMetric periodicFlushMetric = new CounterMetric();

    private final ShardEventListener shardEventListener = new ShardEventListener();

    private final ShardPath path;

    private final IndexShardOperationPermits indexShardOperationPermits;

    private static final EnumSet&lt;IndexShardState&gt; READ_ALLOWED_STATES = EnumSet.of(IndexShardState.STARTED, IndexShardState.POST_RECOVERY);

    // for primaries, we only allow to write when actually started (so the cluster has decided we started)
    // in case we have a relocation of a primary, we also allow to write after phase 2 completed, where the shard may be
    // in state RECOVERING or POST_RECOVERY.
    // for replicas, replication is also allowed while recovering, since we index also during recovery to replicas and rely on version checks to make sure its consistent
    // a relocated shard can also be target of a replication if the relocation target has not been marked as active yet and is syncing it's changes back to the relocation source
    private static final EnumSet&lt;IndexShardState&gt; WRITE_ALLOWED_STATES = EnumSet.of(IndexShardState.RECOVERING, IndexShardState.POST_RECOVERY, IndexShardState.STARTED);

    /**
     * True if this shard is still indexing (recently) and false if we've been idle for long enough (as periodically checked by {@link
     * IndexingMemoryController}).
     */
    private final AtomicBoolean active = new AtomicBoolean();
    /**
     * Allows for the registration of listeners that are called when a change becomes visible for search.
     */
    private final RefreshListeners refreshListeners;

    private final AtomicLong lastSearcherAccess = new AtomicLong();
    private final AtomicReference&lt;Translog.Location&gt; pendingRefreshLocation = new AtomicReference&lt;&gt;();
    private final RefreshPendingLocationListener refreshPendingLocationListener;
    private volatile boolean useRetentionLeasesInPeerRecovery;

    public IndexShard(
            ShardRouting shardRouting,
            IndexSettings indexSettings,
            ShardPath path,
            Store store,
            IndexCache indexCache,
            MapperService mapperService,
            @Nullable EngineFactory engineFactory,
            IndexEventListener indexEventListener,
            ThreadPool threadPool,
            BigArrays bigArrays,
            List&lt;IndexingOperationListener&gt; listeners,
            Runnable globalCheckpointSyncer,
            RetentionLeaseSyncer retentionLeaseSyncer,
            CircuitBreakerService circuitBreakerService) throws IOException {
        super(shardRouting.shardId(), indexSettings);
        assert shardRouting.initializing();
        this.shardRouting = shardRouting;
        final Settings settings = indexSettings.getSettings();
        this.codecService = new CodecService(mapperService, logger);
        Objects.requireNonNull(store, &quot;Store must be provided to the index shard&quot;);
        this.engineFactory = Objects.requireNonNull(engineFactory);
        this.store = store;
        this.indexEventListener = indexEventListener;
        this.threadPool = threadPool;
        this.mapperService = mapperService;
        this.indexCache = indexCache;
        this.indexingOperationListeners = new IndexingOperationListener.CompositeListener(listeners, logger);
        this.globalCheckpointSyncer = globalCheckpointSyncer;
        this.retentionLeaseSyncer = retentionLeaseSyncer;
        state = IndexShardState.CREATED;
        this.path = path;
        this.circuitBreakerService = circuitBreakerService;
        /* create engine config */
<A NAME="15"></A>        logger.debug(&quot;state: [CREATED]&quot;);

        this.checkIndexOnStartup = indexSettings.getValue(IndexSettings.INDEX_CHECK_ON_STARTUP);
        this.translogConfig = <FONT color="#f52887"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#15',2,'match2119549-top.html#15',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>new TranslogConfig(shardId, shardPath().resolveTranslog(), indexSettings, bigArrays);
        final String aId = shardRouting.allocationId().getId();
        final long primaryTerm = indexSettings.getIndexMetadata</B></FONT>().primaryTerm(shardId.id());
        this.pendingPrimaryTerm = primaryTerm;
        this.globalCheckpointListeners = new GlobalCheckpointListeners(
            shardId,
            threadPool.executor(ThreadPool.Names.LISTENER),
            threadPool.scheduler(),
            logger
        );
        this.replicationTracker = new ReplicationTracker(
            shardId,
            aId,
            indexSettings,
            primaryTerm,
            UNASSIGNED_SEQ_NO,
            globalCheckpointListeners::globalCheckpointUpdated,
            threadPool::absoluteTimeInMillis,
            (retentionLeases, listener) -&gt; retentionLeaseSyncer.sync(shardId, aId, getPendingPrimaryTerm(), retentionLeases, listener),
            this::getSafeCommitInfo);

        // the query cache is a node-level thing, however we want the most popular filters
        // to be computed on a per-shard basis
        if (IndexModule.INDEX_QUERY_CACHE_EVERYTHING_SETTING.get(settings)) {
            cachingPolicy = new QueryCachingPolicy() {

                @Override
                public void onUse(Query query) {

                }

                @Override
                public boolean shouldCache(Query query) {
                    return true;
                }
            };
        } else {
            cachingPolicy = new UsageTrackingQueryCachingPolicy();
        }
        indexShardOperationPermits = new IndexShardOperationPermits(shardId, logger, threadPool);
        refreshListeners = buildRefreshListeners();
        lastSearcherAccess.set(threadPool.relativeTimeInMillis());
        persistMetadata(path, indexSettings, shardRouting, null, logger);
        this.useRetentionLeasesInPeerRecovery = replicationTracker.hasAllPeerRecoveryRetentionLeases();
        this.refreshPendingLocationListener = new RefreshPendingLocationListener();
    }

    public ThreadPool getThreadPool() {
        return this.threadPool;
    }

    public Store store() {
        return this.store;
    }

    public MapperService mapperService() {
        return mapperService;
    }

    /**
     * USE THIS METHOD WITH CARE!
     * Returns the primary term the index shard is supposed to be on. In case of primary promotion or when a replica learns about
     * a new term due to a new primary, the term that's exposed here will not be the term that the shard internally uses to assign
     * to operations. The shard will auto-correct its internal operation term, but this might take time.
     * See {@link IndexMetadata#primaryTerm(int)}
     */
    public long getPendingPrimaryTerm() {
        return this.pendingPrimaryTerm;
    }

    /** Returns the primary term that is currently being used to assign to operations */
    public long getOperationPrimaryTerm() {
        return replicationTracker.getOperationPrimaryTerm();
    }

    /**
     * Returns the latest cluster routing entry received with this shard.
     */
    @Override
    public ShardRouting routingEntry() {
        return this.shardRouting;
    }

    @Override
    public void updateShardState(final ShardRouting newRouting,
                                 final long newPrimaryTerm,
                                 final BiConsumer&lt;IndexShard, ActionListener&lt;ResyncTask&gt;&gt; primaryReplicaSyncer,
                                 final long applyingClusterStateVersion,
                                 final Set&lt;String&gt; inSyncAllocationIds,
                                 final IndexShardRoutingTable routingTable) throws IOException {
        final ShardRouting currentRouting;
        synchronized (mutex) {
            currentRouting = this.shardRouting;
            assert currentRouting != null : &quot;shardRouting must not be null&quot;;

            if (!newRouting.shardId().equals(shardId())) {
                throw new IllegalArgumentException(&quot;Trying to set a routing entry with shardId &quot; + newRouting.shardId() + &quot; on a shard with shardId &quot; + shardId());
            }
            if (newRouting.isSameAllocation(currentRouting) == false) {
                throw new IllegalArgumentException(&quot;Trying to set a routing entry with a different allocation. Current &quot; + currentRouting + &quot;, new &quot; + newRouting);
            }
            if (currentRouting.primary() &amp;&amp; newRouting.primary() == false) {
                throw new IllegalArgumentException(&quot;illegal state: trying to move shard from primary mode to replica mode. Current &quot;
                    + currentRouting + &quot;, new &quot; + newRouting);
            }

            if (newRouting.primary()) {
                replicationTracker.updateFromMaster(applyingClusterStateVersion, inSyncAllocationIds, routingTable);
            }

            if (state == IndexShardState.POST_RECOVERY &amp;&amp; newRouting.active()) {
                assert currentRouting.active() == false : &quot;we are in POST_RECOVERY, but our shard routing is active &quot; + currentRouting;

                assert currentRouting.isRelocationTarget() == false || currentRouting.primary() == false ||
                       replicationTracker.isPrimaryMode() :
                    &quot;a primary relocation is completed by the master, but primary mode is not active &quot; + currentRouting;

                changeState(IndexShardState.STARTED, &quot;global state is [&quot; + newRouting.state() + &quot;]&quot;);
            } else if (currentRouting.primary() &amp;&amp; currentRouting.relocating() &amp;&amp; replicationTracker.isRelocated() &amp;&amp;
                (newRouting.relocating() == false || newRouting.equalsIgnoringMetadata(currentRouting) == false)) {
                // if the shard is not in primary mode anymore (after primary relocation) we have to fail when any changes in shard routing occur (e.g. due to recovery
                // failure / cancellation). The reason is that at the moment we cannot safely reactivate primary mode without risking two
                // active primaries.
                throw new IndexShardRelocatedException(shardId(), &quot;Shard is marked as relocated, cannot safely move to state &quot; + newRouting.state());
            }
            assert newRouting.active() == false || state == IndexShardState.STARTED || state == IndexShardState.CLOSED :
                &quot;routing is active, but local shard state isn't. routing: &quot; + newRouting + &quot;, local state: &quot; + state;
            persistMetadata(path, indexSettings, newRouting, currentRouting, logger);
            final CountDownLatch shardStateUpdated = new CountDownLatch(1);

            if (newRouting.primary()) {
                if (newPrimaryTerm == pendingPrimaryTerm) {
                    if (currentRouting.initializing() &amp;&amp; newRouting.active()) {
                        if (currentRouting.isRelocationTarget() == false) {
                            // the master started a recovering primary, activate primary mode.
                            replicationTracker.activatePrimaryMode(getLocalCheckpoint());
                        }
                    }
                } else {
                    assert currentRouting.primary() == false : &quot;term is only increased as part of primary promotion&quot;;
                    /* Note that due to cluster state batching an initializing primary shard term can failed and re-assigned
                     * in one state causing it's term to be incremented. Note that if both current shard state and new
                     * shard state are initializing, we could replace the current shard and reinitialize it. It is however
                     * possible that this shard is being started. This can happen if:
                     * 1) Shard is post recovery and sends shard started to the master
                     * 2) Node gets disconnected and rejoins
                     * 3) Master assigns the shard back to the node
                     * 4) Master processes the shard started and starts the shard
                     * 5) The node process the cluster state where the shard is both started and primary term is incremented.
                     *
                     * We could fail the shard in that case, but this will cause it to be removed from the insync allocations list
                     * potentially preventing re-allocation.
                     */
                    assert newRouting.initializing() == false :
                        &quot;a started primary shard should never update its term; &quot;
                            + &quot;shard &quot; + newRouting + &quot;, &quot;
                            + &quot;current term [&quot; + pendingPrimaryTerm + &quot;], &quot;
                            + &quot;new term [&quot; + newPrimaryTerm + &quot;]&quot;;
                    assert newPrimaryTerm &gt; pendingPrimaryTerm :
                        &quot;primary terms can only go up; current term [&quot; + pendingPrimaryTerm + &quot;], new term [&quot; + newPrimaryTerm + &quot;]&quot;;
                    /*
                     * Before this call returns, we are guaranteed that all future operations are delayed and so this happens before we
                     * increment the primary term. The latch is needed to ensure that we do not unblock operations before the primary term is
                     * incremented.
                     */
                    // to prevent primary relocation handoff while resync is not completed
                    boolean resyncStarted = primaryReplicaResyncInProgress.compareAndSet(false, true);
                    if (resyncStarted == false) {
                        throw new IllegalStateException(&quot;cannot start resync while it's already in progress&quot;);
                    }
                    bumpPrimaryTerm(newPrimaryTerm,
                        () -&gt; {
                            shardStateUpdated.await();
                            assert pendingPrimaryTerm == newPrimaryTerm :
                                &quot;shard term changed on primary. expected [&quot; + newPrimaryTerm + &quot;] but was [&quot; + pendingPrimaryTerm + &quot;]&quot; +
                                &quot;, current routing: &quot; + currentRouting + &quot;, new routing: &quot; + newRouting;
                            assert getOperationPrimaryTerm() == newPrimaryTerm;
                            try {
                                replicationTracker.activatePrimaryMode(getLocalCheckpoint());
                                ensurePeerRecoveryRetentionLeasesExist();
                                /*
                                 * If this shard was serving as a replica shard when another shard was promoted to primary then
                                 * its Lucene index was reset during the primary term transition. In particular, the Lucene index
                                 * on this shard was reset to the global checkpoint and the operations above the local checkpoint
                                 * were reverted. If the other shard that was promoted to primary subsequently fails before the
                                 * primary/replica re-sync completes successfully and we are now being promoted, we have to restore
                                 * the reverted operations on this shard by replaying the translog to avoid losing acknowledged writes.
                                 */
                                final Engine engine = getEngine();
                                engine.restoreLocalHistoryFromTranslog((resettingEngine, snapshot) -&gt;
                                    runTranslogRecovery(resettingEngine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -&gt; {}));
                                if (indexSettings.getIndexVersionCreated().onOrBefore(Version.V_3_0_1)) {
                                    // an index that was created before sequence numbers were introduced may contain operations in its
                                    // translog that do not have a sequence numbers. We want to make sure those operations will never
                                    // be replayed as part of peer recovery to avoid an arbitrary mixture of operations with seq# (due
                                    // to active indexing) and operations without a seq# coming from the translog. We therefore flush
                                    // to create a lucene commit point to an empty translog file.
                                    engine.flush(false, true);
                                }
                                /* Rolling the translog generation is not strictly needed here (as we will never have collisions between
                                 * sequence numbers in a translog generation in a new primary as it takes the last known sequence number
                                 * as a starting point), but it simplifies reasoning about the relationship between primary terms and
                                 * translog generations.
                                 */
                                engine.rollTranslogGeneration();
                                engine.fillSeqNoGaps(newPrimaryTerm);
                                replicationTracker.updateLocalCheckpoint(currentRouting.allocationId().getId(), getLocalCheckpoint());
                                primaryReplicaSyncer.accept(this, new ActionListener&lt;ResyncTask&gt;() {
                                    @Override
                                    public void onResponse(ResyncTask resyncTask) {
                                        logger.info(&quot;primary-replica resync completed with {} operations&quot;,
                                            resyncTask.getResyncedOperations());
                                        boolean resyncCompleted = primaryReplicaResyncInProgress.compareAndSet(true, false);
                                        assert resyncCompleted : &quot;primary-replica resync finished but was not started&quot;;
                                    }

                                    @Override
                                    public void onFailure(Exception e) {
                                        boolean resyncCompleted = primaryReplicaResyncInProgress.compareAndSet(true, false);
                                        assert resyncCompleted : &quot;primary-replica resync finished but was not started&quot;;
                                        if (state == IndexShardState.CLOSED) {
                                            // ignore, shutting down
                                        } else {
                                            failShard(&quot;exception during primary-replica resync&quot;, e);
                                        }
                                    }
                                });
                            } catch (final AlreadyClosedException e) {
                                // okay, the index was deleted
                            }
                        }, null);
                }
            }
            // set this last, once we finished updating all internal state.
            this.shardRouting = newRouting;

            assert this.shardRouting.primary() == false ||
                this.shardRouting.started() == false || // note that we use started and not active to avoid relocating shards
                this.indexShardOperationPermits.isBlocked() || // if permits are blocked, we are still transitioning
                this.replicationTracker.isPrimaryMode()
                : &quot;a started primary with non-pending operation term must be in primary mode &quot; + this.shardRouting;
            shardStateUpdated.countDown();
        }
        if (currentRouting.active() == false &amp;&amp; newRouting.active()) {
            indexEventListener.afterIndexShardStarted(this);
        }
        if (newRouting.equals(currentRouting) == false) {
            indexEventListener.shardRoutingChanged(this, currentRouting, newRouting);
        }

        if (indexSettings.isSoftDeleteEnabled() &amp;&amp; useRetentionLeasesInPeerRecovery == false) {
            final RetentionLeases retentionLeases = replicationTracker.getRetentionLeases();
            final Set&lt;ShardRouting&gt; shardRoutings = new HashSet&lt;&gt;(routingTable.getShards());
            shardRoutings.addAll(routingTable.assignedShards()); // include relocation targets
            if (shardRoutings.stream().allMatch(
                shr -&gt; shr.assignedToNode() &amp;&amp; retentionLeases.contains(ReplicationTracker.getPeerRecoveryRetentionLeaseId(shr)))) {
                useRetentionLeasesInPeerRecovery = true;
                turnOffTranslogRetention();
            }
        }
    }

    /**
     * Marks the shard as recovering based on a recovery state, fails with exception is recovering is not allowed to be set.
     */
    public IndexShardState markAsRecovering(String reason, RecoveryState recoveryState) throws IndexShardStartedException,
        IndexShardRelocatedException, IndexShardRecoveringException, IndexShardClosedException {
        synchronized (mutex) {
            if (state == IndexShardState.CLOSED) {
                throw new IndexShardClosedException(shardId);
            }
            if (state == IndexShardState.STARTED) {
                throw new IndexShardStartedException(shardId);
            }
            if (state == IndexShardState.RECOVERING) {
                throw new IndexShardRecoveringException(shardId);
            }
            if (state == IndexShardState.POST_RECOVERY) {
                throw new IndexShardRecoveringException(shardId);
            }
            this.recoveryState = recoveryState;
            return changeState(IndexShardState.RECOVERING, reason);
        }
    }

    private final AtomicBoolean primaryReplicaResyncInProgress = new AtomicBoolean();

    /**
     * Completes the relocation. Operations are blocked and current operations are drained before changing state to relocated. The provided
     * {@link Runnable} is executed after all operations are successfully blocked.
     *
     * @param consumer a {@link Runnable} that is executed after operations are blocked
     * @throws IllegalIndexShardStateException if the shard is not relocating due to concurrent cancellation
     * @throws IllegalStateException           if the relocation target is no longer part of the replication group
     * @throws InterruptedException            if blocking operations is interrupted
     */
    public void relocated(final String targetAllocationId,
                          final Consumer&lt;ReplicationTracker.PrimaryContext&gt; consumer) throws IllegalIndexShardStateException, IllegalStateException, InterruptedException {
        assert shardRouting.primary() : &quot;only primaries can be marked as relocated: &quot; + shardRouting;
        try (Releasable forceRefreshes = refreshListeners.forceRefreshes()) {
            indexShardOperationPermits.blockOperations(30, TimeUnit.MINUTES, () -&gt; {
                forceRefreshes.close();

                // no shard operation permits are being held here, move state from started to relocated
                assert indexShardOperationPermits.getActiveOperationsCount() == OPERATIONS_BLOCKED :
                        &quot;in-flight operations in progress while moving shard state to relocated&quot;;
                /*
                 * We should not invoke the runnable under the mutex as the expected implementation is to handoff the primary context via a
                 * network operation. Doing this under the mutex can implicitly block the cluster state update thread on network operations.
                 */
                verifyRelocatingState();
                final ReplicationTracker.PrimaryContext primaryContext = replicationTracker.startRelocationHandoff(targetAllocationId);
                try {
                    consumer.accept(primaryContext);
                    synchronized (mutex) {
                        verifyRelocatingState();
                        replicationTracker.completeRelocationHandoff(); // make changes to primaryMode and relocated flag only under mutex
                    }
                } catch (final Exception e) {
                    try {
                        replicationTracker.abortRelocationHandoff();
                    } catch (final Exception inner) {
                        e.addSuppressed(inner);
                    }
                    throw e;
                }
            });
        } catch (TimeoutException e) {
            logger.warn(&quot;timed out waiting for relocation hand-off to complete&quot;);
            // This is really bad as ongoing replication operations are preventing this shard from completing relocation hand-off.
            // Fail primary relocation source and target shards.
            failShard(&quot;timed out waiting for relocation hand-off to complete&quot;, null);
            throw new IndexShardClosedException(shardId(), &quot;timed out waiting for relocation hand-off to complete&quot;);
        }
    }

    private void verifyRelocatingState() {
        if (state != IndexShardState.STARTED) {
            throw new IndexShardNotStartedException(shardId, state);
        }
        /*
         * If the master cancelled recovery, the target will be removed and the recovery will be cancelled. However, it is still possible
         * that we concurrently end up here and therefore have to protect that we do not mark the shard as relocated when its shard routing
         * says otherwise.
         */

        if (shardRouting.relocating() == false) {
            throw new IllegalIndexShardStateException(shardId, IndexShardState.STARTED,
                &quot;: shard is no longer relocating &quot; + shardRouting);
        }

        if (primaryReplicaResyncInProgress.get()) {
            throw new IllegalIndexShardStateException(shardId, IndexShardState.STARTED,
                &quot;: primary relocation is forbidden while primary-replica resync is in progress &quot; + shardRouting);
        }
    }

    @Override
    public IndexShardState state() {
        return state;
    }

    /**
     * Changes the state of the current shard
     *
     * @param newState the new shard state
     * @param reason   the reason for the state change
     * @return the previous shard state
     */
    private IndexShardState changeState(IndexShardState newState, String reason) {
        assert Thread.holdsLock(mutex);
        logger.debug(&quot;state: [{}]-&gt;[{}], reason [{}]&quot;, state, newState, reason);
        IndexShardState previousState = state;
        state = newState;
        this.indexEventListener.indexShardStateChanged(this, previousState, newState, reason);
        return previousState;
    }

    public Engine.IndexResult applyIndexOperationOnPrimary(long version,
                                                           VersionType versionType,
                                                           SourceToParse sourceToParse,
                                                           long ifSeqNo,
                                                           long ifPrimaryTerm,
                                                           long autoGeneratedTimestamp,
                                                           boolean isRetry) throws IOException {
        assert versionType.validateVersionForWrites(version);
        return applyIndexOperation(
            getEngine(),
            UNASSIGNED_SEQ_NO,
            getOperationPrimaryTerm(),
            version,
            versionType,
            ifSeqNo,
            ifPrimaryTerm,
            autoGeneratedTimestamp,
            isRetry,
            Engine.Operation.Origin.PRIMARY,
            sourceToParse
        );
    }

    public Engine.IndexResult applyIndexOperationOnReplica(long seqNo,
                                                           long opPrimaryTerm,
                                                           long version,
                                                           long autoGeneratedTimeStamp,
                                                           boolean isRetry,
                                                           SourceToParse sourceToParse) throws IOException {
        return applyIndexOperation(
            getEngine(),
            seqNo,
            opPrimaryTerm,
            version,
            null,
            UNASSIGNED_SEQ_NO,
            0,
            autoGeneratedTimeStamp,
            isRetry,
            Engine.Operation.Origin.REPLICA,
            sourceToParse
        );
    }

    private Engine.IndexResult applyIndexOperation(Engine engine,
                                                   long seqNo,
                                                   long opPrimaryTerm,
                                                   long version,
                                                   @Nullable VersionType versionType,
                                                   long ifSeqNo,
                                                   long ifPrimaryTerm,
                                                   long autoGeneratedTimeStamp,
                                                   boolean isRetry,
                                                   Engine.Operation.Origin origin,
                                                   SourceToParse sourceToParse) throws IOException {
        assert opPrimaryTerm &lt;= getOperationPrimaryTerm()
                : &quot;op term [ &quot; + opPrimaryTerm + &quot; ] &gt; shard term [&quot; + getOperationPrimaryTerm() + &quot;]&quot;;
        ensureWriteAllowed(origin);
        Engine.Index operation;
        try {
            operation = prepareIndex(
                mapperService.documentMapper(),
                sourceToParse,
                seqNo,
                opPrimaryTerm,
                version,
                versionType,
                origin,
                autoGeneratedTimeStamp,
                isRetry,
                ifSeqNo,
                ifPrimaryTerm
            );
            Mapping update = operation.parsedDoc().dynamicMappingsUpdate();
            if (update != null) {
                return new Engine.IndexResult(update);
            }
        } catch (Exception e) {
            // We treat any exception during parsing and or mapping update as a document level failure
            // with the exception side effects of closing the shard. Since we don't have the shard, we
            // can not raise an exception that may block any replication of previous operations to the
            // replicas
            verifyNotClosed(e);
            return new Engine.IndexResult(e, version, opPrimaryTerm, seqNo);
        }

        return index(engine, operation);
    }

    public static Engine.Index prepareIndex(DocumentMapper docMapper,
                                            SourceToParse source,
                                            long seqNo,
                                            long primaryTerm,
                                            long version,
                                            VersionType versionType,
                                            Engine.Operation.Origin origin,
<A NAME="22"></A>                                            long autoGeneratedIdTimestamp,
                                            boolean isRetry,
                                            long ifSeqNo,
                                            long ifPrimaryTerm) <FONT color="#4cc417"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#22',2,'match2119549-top.html#22',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>{
        long startTime = System.nanoTime();
        ParsedDocument doc = docMapper.parse(source);
        Term uid = new Term(IdFieldMapper.NAME, Uid.encodeId(doc.id</B></FONT>()));
        return new Engine.Index(uid, doc, seqNo, primaryTerm, version, versionType, origin, startTime, autoGeneratedIdTimestamp, isRetry,
                                ifSeqNo, ifPrimaryTerm);
    }

    private Engine.IndexResult index(Engine engine, Engine.Index index) throws IOException {
        active.set(true);
        final Engine.IndexResult result;
        index = indexingOperationListeners.preIndex(shardId, index);
        boolean traceEnabled = logger.isTraceEnabled();
<A NAME="23"></A>        try {
            if (traceEnabled) {
                // don't use index.source().utf8ToString() here source might not be valid UTF-8
                <FONT color="#f660ab"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#23',2,'match2119549-top.html#23',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>logger.trace(
                    &quot;index [{}] seq# [{}] allocation-id [{}] primaryTerm [{}] operationPrimaryTerm [{}] origin [{}]&quot;,
                    index.id(),
                    index.seqNo(),
                    routingEntry().allocationId(),
                    index.primaryTerm(),
                    getOperationPrimaryTerm(),
                    index.origin());
<A NAME="4"></A>            }</B></FONT>
            result = engine.index(index);
            if (traceEnabled) {
                <FONT color="#6cc417"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#4',2,'match2119549-top.html#4',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>logger.trace(
                    &quot;index-done [{}] seq# [{}] allocation-id [{}] primaryTerm [{}] operationPrimaryTerm [{}] origin [{}] &quot; +
                    &quot;result-seq# [{}] result-term [{}] failure [{}]&quot;,
                    index.id(),
                    index.seqNo(),
                    routingEntry().allocationId(),
                    index.primaryTerm(),
                    getOperationPrimaryTerm(),
                    index.origin(),
                    result.getSeqNo(),
                    result.getTerm(),
                    result.getFailure());
<A NAME="14"></A>            }</B></FONT>
        } catch (Exception e) {
            if (traceEnabled) {
                <FONT color="#842dce"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#14',2,'match2119549-top.html#14',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>logger.trace(new ParameterizedMessage(
                    &quot;index-fail [{}] seq# [{}] allocation-id [{}] primaryTerm [{}] operationPrimaryTerm [{}] origin [{}]&quot;,
                    index.id(),
                    index.seqNo(),
                    routingEntry().allocationId(),
                    index.primaryTerm(),
                    getOperationPrimaryTerm(),
                    index.origin</B></FONT>()
                ), e);
            }
            indexingOperationListeners.postIndex(shardId, index, e);
            throw e;
        }
        indexingOperationListeners.postIndex(shardId, index, result);
        return result;
    }

    public Engine.NoOpResult markSeqNoAsNoop(long seqNo, long opPrimaryTerm, String reason) throws IOException {
        return markSeqNoAsNoop(getEngine(), seqNo, opPrimaryTerm, reason, Engine.Operation.Origin.REPLICA);
    }

    private Engine.NoOpResult markSeqNoAsNoop(Engine engine, long seqNo, long opPrimaryTerm, String reason,
                                              Engine.Operation.Origin origin) throws IOException {
        assert opPrimaryTerm &lt;= getOperationPrimaryTerm()
                : &quot;op term [ &quot; + opPrimaryTerm + &quot; ] &gt; shard term [&quot; + getOperationPrimaryTerm() + &quot;]&quot;;
        long startTime = System.nanoTime();
        ensureWriteAllowed(origin);
        final Engine.NoOp noOp = new Engine.NoOp(seqNo, opPrimaryTerm, origin, startTime, reason);
        return noOp(engine, noOp);
    }

    private Engine.NoOpResult noOp(Engine engine, Engine.NoOp noOp) throws IOException {
        active.set(true);
        if (logger.isTraceEnabled()) {
            logger.trace(&quot;noop (seq# [{}])&quot;, noOp.seqNo());
        }
        return engine.noOp(noOp);
    }

    public Engine.IndexResult getFailedIndexResult(Exception e, long version) {
        return new Engine.IndexResult(e, version);
    }

    public Engine.DeleteResult applyDeleteOperationOnPrimary(long version,
                                                             String id,
                                                             VersionType versionType,
                                                             long ifSeqNo,
                                                             long ifPrimaryTerm) throws IOException {
        return applyDeleteOperation(
            getEngine(),
            UNASSIGNED_SEQ_NO,
            getOperationPrimaryTerm(),
            version,
            id,
            versionType,
            ifSeqNo,
            ifPrimaryTerm,
            Engine.Operation.Origin.PRIMARY
        );
    }

    public Engine.DeleteResult applyDeleteOperationOnReplica(long seqNo,
                                                             long opPrimaryTerm,
                                                             long version,
                                                             String id) throws IOException {
        return applyDeleteOperation(
            getEngine(),
            seqNo,
            opPrimaryTerm,
            version,
            id,
            null,
            UNASSIGNED_SEQ_NO,
            0,
            Engine.Operation.Origin.REPLICA
        );
    }

    private Engine.DeleteResult applyDeleteOperation(Engine engine,
                                                     long seqNo,
                                                     long opPrimaryTerm,
                                                     long version,
                                                     String id,
                                                     @Nullable VersionType versionType,
                                                     long ifSeqNo,
                                                     long ifPrimaryTerm,
                                                     Engine.Operation.Origin origin) throws IOException {
        assert opPrimaryTerm &lt;= getOperationPrimaryTerm()
                : &quot;op term [ &quot; + opPrimaryTerm + &quot; ] &gt; shard term [&quot; + getOperationPrimaryTerm() + &quot;]&quot;;
        ensureWriteAllowed(origin);
        final Term uid = new Term(IdFieldMapper.NAME, Uid.encodeId(id));
        final Engine.Delete delete = prepareDelete(
            id,
            uid,
            seqNo,
            opPrimaryTerm,
            version,
            versionType,
            origin,
            ifSeqNo,
            ifPrimaryTerm
        );
        return delete(engine, delete);
    }

    private Engine.Delete prepareDelete(String id,
                                        Term uid,
                                        long seqNo,
                                        long primaryTerm,
                                        long version,
                                        VersionType versionType,
                                        Engine.Operation.Origin origin,
                                        long ifSeqNo,
                                        long ifPrimaryTerm) {
        long startTime = System.nanoTime();
        return new Engine.Delete(
            id,
            uid,
            seqNo,
            primaryTerm,
            version,
            versionType,
            origin,
            startTime,
            ifSeqNo,
            ifPrimaryTerm
        );
    }

    private Engine.DeleteResult delete(Engine engine, Engine.Delete delete) throws IOException {
        active.set(true);
        final Engine.DeleteResult result;
        delete = indexingOperationListeners.preDelete(shardId, delete);
        try {
            if (logger.isTraceEnabled()) {
                logger.trace(&quot;delete [{}] (seq no [{}])&quot;, delete.uid().text(), delete.seqNo());
            }
            result = engine.delete(delete);
        } catch (Exception e) {
            indexingOperationListeners.postDelete(shardId, delete, e);
            throw e;
        }
        indexingOperationListeners.postDelete(shardId, delete, result);
        return result;
    }

    public Engine.GetResult get(Engine.Get get) {
        readAllowed();
        return getEngine().get(get, this::acquireSearcher);
    }

    /**
     * Writes all indexing changes to disk and opens a new searcher reflecting all changes.  This can throw {@link AlreadyClosedException}.
     */
    public void refresh(String source) {
        verifyNotClosed();
        if (logger.isTraceEnabled()) {
            logger.trace(&quot;refresh with source [{}]&quot;, source);
        }
        getEngine().refresh(source);
    }

    /**
     * Returns how many bytes we are currently moving from heap to disk
     */
    public long getWritingBytes() {
        Engine engine = getEngineOrNull();
        if (engine == null) {
            return 0;
        }
        return engine.getWritingBytes();
    }

    public DocsStats docStats() {
        readAllowed();
        return getEngine().docStats();
    }

    /**
     * @return {@link CommitStats}
     * @throws AlreadyClosedException if shard is closed
     */
    public CommitStats commitStats() {
        return getEngine().commitStats();
    }

    /**
     * @return {@link SeqNoStats}
     * @throws AlreadyClosedException if shard is closed
     */
    public SeqNoStats seqNoStats() {
        return getEngine().getSeqNoStats(replicationTracker.getGlobalCheckpoint());
    }

    public TranslogStats translogStats() {
        return getEngine().getTranslogStats();
    }

    public StoreStats storeStats() {
        try {
            final RecoveryState recoveryState = this.recoveryState;
            final long bytesStillToRecover = recoveryState == null ? -1L : recoveryState.getIndex().bytesStillToRecover();
            return store.stats(bytesStillToRecover == -1 ? StoreStats.UNKNOWN_RESERVED_BYTES : bytesStillToRecover);
        } catch (IOException e) {
            failShard(&quot;Failing shard because of exception during storeStats&quot;, e);
            throw new ElasticsearchException(&quot;io exception while building 'store stats'&quot;, e);
        }
    }

    public Engine.SyncedFlushResult syncFlush(String syncId, Engine.CommitId expectedCommitId) {
        verifyNotClosed();
        logger.trace(&quot;trying to sync flush. sync id [{}]. expected commit id [{}]]&quot;, syncId, expectedCommitId);
        return getEngine().syncFlush(syncId, expectedCommitId);
    }

    /**
     * Executes the given flush request against the engine.
     *
     * @param request the flush request
     * @return the commit ID
     */
<A NAME="16"></A>    public Engine.CommitId flush(FlushRequest request) {
        final boolean waitIfOngoing = request.waitIfOngoing();
        final boolean force = request.force();
        <FONT color="#2981b2"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#16',2,'match2119549-top.html#16',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>logger.trace(&quot;flush with {}&quot;, request);
        /*
         * We allow flushes while recovery since we allow operations to happen while recovering and we want to keep the translog under
         * control (up to deletes, which we do not GC). Yet, we do not use flush internally to clear deletes and flush the index writer
         * since we use Engine#writeIndexingBuffer for this now.
         */
        verifyNotClosed();
        final long time = System.nanoTime();
        final Engine.CommitId commitId = getEngine().flush(force, waitIfOngoing);
        flushMetric.inc(System.nanoTime</B></FONT>() - time);
        return commitId;
    }

    /**
     * checks and removes translog files that no longer need to be retained. See
     * {@link org.elasticsearch.index.translog.TranslogDeletionPolicy} for details
<A NAME="25"></A>     */
    public void trimTranslog() {
        verifyNotClosed();
        final Engine engine = <FONT color="#5eac10"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#25',2,'match2119549-top.html#25',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>getEngine();
        engine.trimUnreferencedTranslogFiles();
    }

    /**
     * Rolls the tranlog generation and cleans unneeded.
     */
    public void rollTranslogGeneration() {
        final Engine engine = getEngine();
        engine.rollTranslogGeneration</B></FONT>();
    }

    public void forceMerge(ForceMergeRequest forceMerge) throws IOException {
        verifyActive();
<A NAME="21"></A>        if (logger.isTraceEnabled()) {
            logger.trace(&quot;force merge with {}&quot;, forceMerge);
        }
        Engine engine = <FONT color="#947010"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#21',2,'match2119549-top.html#21',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>getEngine();
        engine.forceMerge(
            forceMerge.flush(),
            forceMerge.maxNumSegments(),
            forceMerge.onlyExpungeDeletes(),
            false,
            false,
            forceMerge.forceMergeUUID()
        );
    }

    /**
     * Upgrades the shard to the current version of Lucene and returns the minimum segment version
     */
    public </B></FONT>org.apache.lucene.util.Version upgrade(UpgradeRequest upgrade) throws IOException {
        verifyActive();
        if (logger.isTraceEnabled()) {
            logger.trace(&quot;upgrade with {}&quot;, upgrade);
        }
        org.apache.lucene.util.Version previousVersion = minimumCompatibleVersion();
        // we just want to upgrade the segments, not actually forge merge to a single segment
        final Engine engine = getEngine();
        engine.forceMerge(
            true,  // we need to flush at the end to make sure the upgrade is durable
            Integer.MAX_VALUE, // we just want to upgrade the segments, not actually optimize to a single segment
            false,
            true,
            upgrade.upgradeOnlyAncientSegments(),
            null
        );

        org.apache.lucene.util.Version version = minimumCompatibleVersion();
        if (logger.isTraceEnabled()) {
            logger.trace(&quot;upgraded segments for {} from version {} to version {}&quot;, shardId, previousVersion, version);
        }

        return version;
    }

    public org.apache.lucene.util.Version minimumCompatibleVersion() {
        org.apache.lucene.util.Version luceneVersion = null;
        for (Segment segment : getEngine().segments(false)) {
            if (luceneVersion == null || luceneVersion.onOrAfter(segment.getVersion())) {
                luceneVersion = segment.getVersion();
            }
        }
        return luceneVersion == null ? indexSettings.getIndexVersionCreated().luceneVersion : luceneVersion;
    }

    /**
     * Creates a new {@link IndexCommit} snapshot from the currently running engine. All resources referenced by this
     * commit won't be freed until the commit / snapshot is closed.
     *
     * @param flushFirst &lt;code&gt;true&lt;/code&gt; if the index should first be flushed to disk / a low level lucene commit should be executed
     */
    public Engine.IndexCommitRef acquireLastIndexCommit(boolean flushFirst) throws EngineException {
        final IndexShardState state = this.state; // one time volatile read
        // we allow snapshot on closed index shard, since we want to do one after we close the shard and before we close the engine
        if (state == IndexShardState.STARTED || state == IndexShardState.CLOSED) {
            return getEngine().acquireLastIndexCommit(flushFirst);
        } else {
            throw new IllegalIndexShardStateException(shardId, state, &quot;snapshot is not allowed&quot;);
        }
    }

    /**
     * Snapshots the most recent safe index commit from the currently running engine.
     * All index files referenced by this index commit won't be freed until the commit/snapshot is closed.
     */
    public Engine.IndexCommitRef acquireSafeIndexCommit() throws EngineException {
        final IndexShardState state = this.state; // one time volatile read
        // we allow snapshot on closed index shard, since we want to do one after we close the shard and before we close the engine
        if (state == IndexShardState.STARTED || state == IndexShardState.CLOSED) {
            return getEngine().acquireSafeIndexCommit();
        } else {
            throw new IllegalIndexShardStateException(shardId, state, &quot;snapshot is not allowed&quot;);
        }
    }

    /**
     * gets a {@link Store.MetadataSnapshot} for the current directory. This method is safe to call in all lifecycle of the index shard,
     * without having to worry about the current state of the engine and concurrent flushes.
     *
     * @throws org.apache.lucene.index.IndexNotFoundException     if no index is found in the current directory
     * @throws org.apache.lucene.index.CorruptIndexException      if the lucene index is corrupted. This can be caused by a checksum
     *                                                            mismatch or an unexpected exception when opening the index reading the
     *                                                            segments file.
     * @throws org.apache.lucene.index.IndexFormatTooOldException if the lucene index is too old to be opened.
     * @throws org.apache.lucene.index.IndexFormatTooNewException if the lucene index is too new to be opened.
     * @throws java.io.FileNotFoundException                      if one or more files referenced by a commit are not present.
     * @throws java.nio.file.NoSuchFileException                  if one or more files referenced by a commit are not present.
     */
    public Store.MetadataSnapshot snapshotStoreMetadata() throws IOException {
        assert Thread.holdsLock(mutex) == false : &quot;snapshotting store metadata under mutex&quot;;
        Engine.IndexCommitRef indexCommit = null;
        store.incRef();
        try {
            synchronized (engineMutex) {
                // if the engine is not running, we can access the store directly, but we need to make sure no one starts
                // the engine on us. If the engine is running, we can get a snapshot via the deletion policy of the engine.
                final Engine engine = getEngineOrNull();
                if (engine != null) {
                    indexCommit = engine.acquireLastIndexCommit(false);
                }
                if (indexCommit == null) {
                    return store.getMetadata(null, true);
                }
            }
            return store.getMetadata(indexCommit.getIndexCommit());
        } finally {
            store.decRef();
            IOUtils.close(indexCommit);
        }
    }

    /**
     * Fails the shard and marks the shard store as corrupted if
     * &lt;code&gt;e&lt;/code&gt; is caused by index corruption
     */
    public void failShard(String reason, @Nullable Exception e) {
        // fail the engine. This will cause this shard to also be removed from the node's index service.
        getEngine().failEngine(reason, e);
    }

    public Engine.Searcher acquireSearcher(String source) {
        return acquireSearcher(source, Engine.SearcherScope.EXTERNAL);
    }

    private void markSearcherAccessed() {
        lastSearcherAccess.lazySet(threadPool.relativeTimeInMillis());
    }

    private Engine.Searcher acquireSearcher(String source, Engine.SearcherScope scope) {
        readAllowed();
        markSearcherAccessed();
        return getEngine().acquireSearcher(source, scope);
    }

    public void close(String reason, boolean flushEngine) throws IOException {
        synchronized (engineMutex) {
            try {
                synchronized (mutex) {
                    changeState(IndexShardState.CLOSED, reason);
                }
            } finally {
                final Engine engine = this.currentEngineReference.getAndSet(null);
                try {
                    if (engine != null &amp;&amp; flushEngine) {
                        engine.flushAndClose();
                    }
                } finally {
                    // playing safe here and close the engine even if the above succeeds - close can be called multiple times
                    // Also closing refreshListeners to prevent us from accumulating any more listeners
                    IOUtils.close(engine, globalCheckpointListeners, refreshListeners);
                    indexShardOperationPermits.close();
                }
            }
        }
    }

    public void postRecovery(String reason) throws IndexShardStartedException, IndexShardRelocatedException, IndexShardClosedException {
        synchronized (postRecoveryMutex) {
            // we need to refresh again to expose all operations that were index until now. Otherwise
            // we may not expose operations that were indexed with a refresh listener that was immediately
            // responded to in addRefreshListener. The refresh must happen under the same mutex used in addRefreshListener
            // and before moving this shard to POST_RECOVERY state (i.e., allow to read from this shard).
            getEngine().refresh(&quot;post_recovery&quot;);
            synchronized (mutex) {
                if (state == IndexShardState.CLOSED) {
                    throw new IndexShardClosedException(shardId);
                }
                if (state == IndexShardState.STARTED) {
                    throw new IndexShardStartedException(shardId);
                }
                recoveryState.setStage(RecoveryState.Stage.DONE);
                changeState(IndexShardState.POST_RECOVERY, reason);
            }
        }
    }

    /**
     * called before starting to copy index files over
     */
    public void prepareForIndexRecovery() {
        if (state != IndexShardState.RECOVERING) {
            throw new IndexShardNotRecoveringException(shardId, state);
        }
        recoveryState.setStage(RecoveryState.Stage.INDEX);
        assert currentEngineReference.get() == null;
    }

    /**
     * A best effort to bring up this shard to the global checkpoint using the local translog before performing a peer recovery.
     *
     * @return a sequence number that an operation-based peer recovery can start with.
     * This is the first operation after the local checkpoint of the safe commit if exists.
     */
    public long recoverLocallyUpToGlobalCheckpoint() {
        assert Thread.holdsLock(mutex) == false : &quot;recover locally under mutex&quot;;
        if (state != IndexShardState.RECOVERING) {
            throw new IndexShardNotRecoveringException(shardId, state);
        }
        assert recoveryState.getStage() == RecoveryState.Stage.INDEX : &quot;unexpected recovery stage [&quot; + recoveryState.getStage() + &quot;]&quot;;
        assert routingEntry().recoverySource().getType() == RecoverySource.Type.PEER : &quot;not a peer recovery [&quot; + routingEntry() + &quot;]&quot;;
        final Optional&lt;SequenceNumbers.CommitInfo&gt; safeCommit;
        final long globalCheckpoint;
        try {
            final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY);
            globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID);
            safeCommit = store.findSafeIndexCommit(globalCheckpoint);
        } catch (org.apache.lucene.index.IndexNotFoundException e) {
            logger.trace(&quot;skip local recovery as no index commit found&quot;);
            return UNASSIGNED_SEQ_NO;
        } catch (Exception e) {
            logger.debug(&quot;skip local recovery as failed to find the safe commit&quot;, e);
            return UNASSIGNED_SEQ_NO;
        }
        try {
            maybeCheckIndex(); // check index here and won't do it again if ops-based recovery occurs
            recoveryState.setStage(RecoveryState.Stage.TRANSLOG);
            if (safeCommit.isPresent() == false) {
                assert globalCheckpoint == UNASSIGNED_SEQ_NO || indexSettings.getIndexVersionCreated().before(Version.V_3_2_0) :
                    &quot;global checkpoint [&quot; + globalCheckpoint + &quot;] [ created version [&quot; + indexSettings.getIndexVersionCreated() + &quot;]&quot;;
                logger.trace(&quot;skip local recovery as no safe commit found&quot;);
                return UNASSIGNED_SEQ_NO;
            }
            assert safeCommit.get().localCheckpoint &lt;= globalCheckpoint : safeCommit.get().localCheckpoint + &quot; &gt; &quot; + globalCheckpoint;
            if (safeCommit.get().localCheckpoint == globalCheckpoint) {
                logger.trace(&quot;skip local recovery as the safe commit is up to date; safe commit {} global checkpoint {}&quot;,
                    safeCommit.get(), globalCheckpoint);
                recoveryState.getTranslog().totalLocal(0);
                return globalCheckpoint + 1;
            }
            if (indexSettings.getIndexMetadata().getState() == IndexMetadata.State.CLOSE ||
                IndexMetadata.INDEX_BLOCKS_WRITE_SETTING.get(indexSettings.getSettings())) {
                logger.trace(&quot;skip local recovery as the index was closed or not allowed to write; safe commit {} global checkpoint {}&quot;,
                    safeCommit.get(), globalCheckpoint);
                recoveryState.getTranslog().totalLocal(0);
                return safeCommit.get().localCheckpoint + 1;
            }
            try {
                final Engine.TranslogRecoveryRunner translogRecoveryRunner = (engine, snapshot) -&gt; {
                    recoveryState.getTranslog().totalLocal(snapshot.totalOperations());
                    final int recoveredOps = runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY,
                        recoveryState.getTranslog()::incrementRecoveredOperations);
                    recoveryState.getTranslog().totalLocal(recoveredOps); // adjust the total local to reflect the actual count
                    return recoveredOps;
                };
                innerOpenEngineAndTranslog(() -&gt; globalCheckpoint);
                getEngine().recoverFromTranslog(translogRecoveryRunner, globalCheckpoint);
                logger.trace(&quot;shard locally recovered up to {}&quot;, getEngine().getSeqNoStats(globalCheckpoint));
            } finally {
                synchronized (engineMutex) {
                    IOUtils.close(currentEngineReference.getAndSet(null));
                }
            }
        } catch (Exception e) {
            logger.debug(new ParameterizedMessage(&quot;failed to recover shard locally up to global checkpoint {}&quot;, globalCheckpoint), e);
            return UNASSIGNED_SEQ_NO;
        }
        try {
            // we need to find the safe commit again as we should have created a new one during the local recovery
            final Optional&lt;SequenceNumbers.CommitInfo&gt; newSafeCommit = store.findSafeIndexCommit(globalCheckpoint);
            assert newSafeCommit.isPresent() : &quot;no safe commit found after local recovery&quot;;
            return newSafeCommit.get().localCheckpoint + 1;
        } catch (Exception e) {
            logger.debug(new ParameterizedMessage(
                &quot;failed to find the safe commit after recovering shard locally up to global checkpoint {}&quot;, globalCheckpoint), e);
            return UNASSIGNED_SEQ_NO;
        }
    }

    public void trimOperationOfPreviousPrimaryTerms(long aboveSeqNo) {
        getEngine().trimOperationsFromTranslog(getOperationPrimaryTerm(), aboveSeqNo);
    }

    /**
     * Returns the maximum auto_id_timestamp of all append-only requests have been processed by this shard or the auto_id_timestamp received
     * from the primary via {@link #updateMaxUnsafeAutoIdTimestamp(long)} at the beginning of a peer-recovery or a primary-replica resync.
     *
     * @see #updateMaxUnsafeAutoIdTimestamp(long)
     */
    public long getMaxSeenAutoIdTimestamp() {
        return getEngine().getMaxSeenAutoIdTimestamp();
    }

    /**
     * Since operations stored in soft-deletes do not have max_auto_id_timestamp, the primary has to propagate its max_auto_id_timestamp
     * (via {@link #getMaxSeenAutoIdTimestamp()} of all processed append-only requests to replicas at the beginning of a peer-recovery
     * or a primary-replica resync to force a replica to disable optimization for all append-only requests which are replicated via
     * replication while its retry variants are replicated via recovery without auto_id_timestamp.
     * &lt;p&gt;
     * Without this force-update, a replica can generate duplicate documents (for the same id) if it first receives
     * a retry append-only (without timestamp) via recovery, then an original append-only (with timestamp) via replication.
     */
    public void updateMaxUnsafeAutoIdTimestamp(long maxSeenAutoIdTimestampFromPrimary) {
        getEngine().updateMaxUnsafeAutoIdTimestamp(maxSeenAutoIdTimestampFromPrimary);
    }

    public Engine.Result applyTranslogOperation(Translog.Operation operation, Engine.Operation.Origin origin) throws IOException {
        return applyTranslogOperation(getEngine(), operation, origin);
    }

    private Engine.Result applyTranslogOperation(Engine engine, Translog.Operation operation,
                                                 Engine.Operation.Origin origin) throws IOException {
        // If a translog op is replayed on the primary (eg. ccr), we need to use external instead of null for its version type.
        final VersionType versionType = (origin == Engine.Operation.Origin.PRIMARY) ? VersionType.EXTERNAL : null;
        final Engine.Result result;
        switch (operation.opType()) {
            case INDEX:
<A NAME="13"></A>                final Translog.Index index = (Translog.Index) operation;
                // we set canHaveDuplicates to true all the time such that we de-optimze the translog case and ensure that all
                // autoGeneratedID docs that are coming from the primary are updated correctly.
                result = <FONT color="#3b9c9c"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#13',2,'match2119549-top.html#13',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>applyIndexOperation(
                    engine,
                    index.seqNo(),
                    index.primaryTerm(),
                    index.version(),
                    versionType,
                    UNASSIGNED_SEQ_NO,
                    0,
                    index.getAutoGeneratedIdTimestamp(),
                    true,
                    origin,
                    new SourceToParse(
                        shardId.getIndexName(),
                        index.id(),
                        index.source</B></FONT>(),
                        XContentHelper.xContentType(index.source()), index.routing())
                );
                break;
            case DELETE:
                final Translog.Delete delete = (Translog.Delete) operation;
                result = applyDeleteOperation(
                    engine,
                    delete.seqNo(),
                    delete.primaryTerm(),
                    delete.version(),
                    delete.id(),
                    versionType,
                    UNASSIGNED_SEQ_NO,
                    0,
                    origin
                );
                break;
            case NO_OP:
                final Translog.NoOp noOp = (Translog.NoOp) operation;
                result = markSeqNoAsNoop(engine, noOp.seqNo(), noOp.primaryTerm(), noOp.reason(), origin);
                break;
            default:
                throw new IllegalStateException(&quot;No operation defined for [&quot; + operation + &quot;]&quot;);
        }
        return result;
    }

    /**
     * Replays translog operations from the provided translog {@code snapshot} to the current engine using the given {@code origin}.
     * The callback {@code onOperationRecovered} is notified after each translog operation is replayed successfully.
     */
    int runTranslogRecovery(Engine engine, Translog.Snapshot snapshot, Engine.Operation.Origin origin,
                            Runnable onOperationRecovered) throws IOException {
        int opsRecovered = 0;
        Translog.Operation operation;
        while ((operation = snapshot.next()) != null) {
            try {
                logger.trace(&quot;[translog] recover op {}&quot;, operation);
                Engine.Result result = applyTranslogOperation(engine, operation, origin);
                switch (result.getResultType()) {
                    case FAILURE:
                        throw result.getFailure();
                    case MAPPING_UPDATE_REQUIRED:
                        throw new IllegalArgumentException(&quot;unexpected mapping update: &quot; + result.getRequiredMappingUpdate());
                    case SUCCESS:
                        break;
                    default:
                        throw new AssertionError(&quot;Unknown result type [&quot; + result.getResultType() + &quot;]&quot;);
                }

                opsRecovered++;
                onOperationRecovered.run();
            } catch (Exception e) {
                // TODO: Don't enable this leniency unless users explicitly opt-in
                if (origin == Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY &amp;&amp; ExceptionsHelper.status(e) == RestStatus.BAD_REQUEST) {
                    // mainly for MapperParsingException and Failure to detect xcontent
                    logger.info(&quot;ignoring recovery of a corrupt translog entry&quot;, e);
                } else {
                    throw Exceptions.toRuntimeException(e);
                }
<A NAME="3"></A>            }
        }
        return opsRecovered;
    <FONT color="#53858b"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#3',2,'match2119549-top.html#3',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    private void loadGlobalCheckpointToReplicationTracker() throws IOException {
        // we have to set it before we open an engine and recover from the translog because
        // acquiring a snapshot from the translog causes a sync which causes the global checkpoint to be pulled in,
        // and an engine can be forced to close in ctor which also causes the global checkpoint to be pulled in.
        final String translogUUID = store.readLastCommittedSegmentsInfo().getUserData().get(Translog.TRANSLOG_UUID_KEY);
        final long globalCheckpoint = Translog.readGlobalCheckpoint(translogConfig.getTranslogPath(), translogUUID);
        replicationTracker.updateGlobalCheckpointOnReplica</B></FONT>(globalCheckpoint, &quot;read from translog checkpoint&quot;);
    }

    /**
     * opens the engine on top of the existing lucene engine and translog.
     * Operations from the translog will be replayed to bring lucene up to date.
     **/
    public void openEngineAndRecoverFromTranslog() throws IOException {
        assert recoveryState.getStage() == RecoveryState.Stage.INDEX : &quot;unexpected recovery stage [&quot; + recoveryState.getStage() + &quot;]&quot;;
        maybeCheckIndex();
        recoveryState.setStage(RecoveryState.Stage.TRANSLOG);
        final RecoveryState.Translog translogRecoveryStats = recoveryState.getTranslog();
        final Engine.TranslogRecoveryRunner translogRecoveryRunner = (engine, snapshot) -&gt; {
            translogRecoveryStats.totalOperations(snapshot.totalOperations());
            translogRecoveryStats.totalOperationsOnStart(snapshot.totalOperations());
            return runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY,
                translogRecoveryStats::incrementRecoveredOperations);
        };
        loadGlobalCheckpointToReplicationTracker();
        innerOpenEngineAndTranslog(replicationTracker);
        getEngine().recoverFromTranslog(translogRecoveryRunner, Long.MAX_VALUE);
    }

    /**
     * Opens the engine on top of the existing lucene engine and translog.
     * The translog is kept but its operations won't be replayed.
<A NAME="20"></A>     */
    public void openEngineAndSkipTranslogRecovery() throws IOException {
        assert routingEntry().recoverySource().getType() == RecoverySource.Type.PEER : &quot;not a peer recovery [&quot; + routingEntry() + &quot;]&quot;;
        assert <FONT color="#4e9258"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#20',2,'match2119549-top.html#20',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>recoveryState.getStage() == RecoveryState.Stage.TRANSLOG : &quot;unexpected recovery stage [&quot; + recoveryState.getStage() + &quot;]&quot;;
        loadGlobalCheckpointToReplicationTracker();
        innerOpenEngineAndTranslog(replicationTracker);
        getEngine().skipTranslogRecovery();
    }

    private void innerOpenEngi</B></FONT>neAndTranslog(LongSupplier globalCheckpointSupplier) throws IOException {
        assert Thread.holdsLock(mutex) == false : &quot;opening engine under mutex&quot;;
        if (state != IndexShardState.RECOVERING) {
            throw new IndexShardNotRecoveringException(shardId, state);
        }
        final EngineConfig config = newEngineConfig(globalCheckpointSupplier);

        // we disable deletes since we allow for operations to be executed against the shard while recovering
        // but we need to make sure we don't loose deletes until we are done recovering
        config.setEnableGcDeletes(false);
        updateRetentionLeasesOnReplica(loadRetentionLeases());
        assert recoveryState.getRecoverySource().expectEmptyRetentionLeases() == false || getRetentionLeases().leases().isEmpty()
            : &quot;expected empty set of retention leases with recovery source [&quot; + recoveryState.getRecoverySource()
            + &quot;] but got &quot; + getRetentionLeases();

        synchronized (engineMutex) {
            assert currentEngineReference.get() == null : &quot;engine is running&quot;;
            verifyNotClosed();
            // we must create a new engine under mutex (see IndexShard#snapshotStoreMetadata).
            final Engine newEngine = engineFactory.newReadWriteEngine(config);
            onNewEngine(newEngine);
            currentEngineReference.set(newEngine);
            // We set active because we are now writing operations to the engine; this way,
            // if we go idle after some time and become inactive, we still give sync'd flush a chance to run.
            active.set(true);
        }
        // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during
<A NAME="8"></A>        // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.
        onSettingsChanged();
        assert assertSequenceNumbersInCommit();
        assert <FONT color="#c58917"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#8',2,'match2119549-top.html#8',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>recoveryState.getStage() == RecoveryState.Stage.TRANSLOG : &quot;TRANSLOG stage expected but was: &quot; + recoveryState.getStage();
    }

    private boolean assertSequenceNumbersInCommit() throws IOException {
        final Map&lt;String, String&gt; userData = SegmentInfos.readLatestCommit(store.directory</B></FONT>()).getUserData();
        assert userData.containsKey(SequenceNumbers.LOCAL_CHECKPOINT_KEY) : &quot;commit point doesn't contains a local checkpoint&quot;;
        assert userData.containsKey(SequenceNumbers.MAX_SEQ_NO) : &quot;commit point doesn't contains a maximum sequence number&quot;;
        assert userData.containsKey(Engine.HISTORY_UUID_KEY) : &quot;commit point doesn't contains a history uuid&quot;;
        assert userData.get(Engine.HISTORY_UUID_KEY).equals(getHistoryUUID()) : &quot;commit point history uuid [&quot;
            + userData.get(Engine.HISTORY_UUID_KEY) + &quot;] is different than engine [&quot; + getHistoryUUID() + &quot;]&quot;;
        // as of 5.5.0, the engine stores the maxUnsafeAutoIdTimestamp in the commit point.
        // This should have baked into the commit by the primary we recover from, regardless of the index age.
        assert userData.containsKey(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID) :
            &quot;opening index which was created post 5.5.0 but &quot; + Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID
                + &quot; is not found in commit&quot;;
        return true;
    }

    private void onNewEngine(Engine newEngine) {
        assert Thread.holdsLock(engineMutex);
        refreshListeners.setCurrentRefreshLocationSupplier(newEngine::getTranslogLastWriteLocation);
    }

    /**
     * called if recovery has to be restarted after network error / delay **
     */
    public void performRecoveryRestart() throws IOException {
        assert Thread.holdsLock(mutex) == false : &quot;restart recovery under mutex&quot;;
        synchronized (engineMutex) {
            assert refreshListeners.pendingCount() == 0 : &quot;we can't restart with pending listeners&quot;;
            IOUtils.close(currentEngineReference.getAndSet(null));
            resetRecoveryStage();
        }
    }

    /**
     * If a file-based recovery occurs, a recovery target calls this method to reset the recovery stage.
     */
    public void resetRecoveryStage() {
        assert routingEntry().recoverySource().getType() == RecoverySource.Type.PEER : &quot;not a peer recovery [&quot; + routingEntry() + &quot;]&quot;;
        assert currentEngineReference.get() == null;
        if (state != IndexShardState.RECOVERING) {
            throw new IndexShardNotRecoveringException(shardId, state);
        }
        recoveryState().setStage(RecoveryState.Stage.INIT);
    }

    /**
     * returns stats about ongoing recoveries, both source and target
     */
    public RecoveryStats recoveryStats() {
        return recoveryStats;
    }

    /**
     * Returns the current {@link RecoveryState} if this shard is recovering or has been recovering.
     * Returns null if the recovery has not yet started or shard was not recovered (created via an API).
     */
    @Override
    public RecoveryState recoveryState() {
        return this.recoveryState;
    }

    /**
     * perform the last stages of recovery once all translog operations are done.
<A NAME="1"></A>     * note that you should still call {@link #postRecovery(String)}.
     */
    public void finalizeRecovery() {
        <FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#1',2,'match2119549-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>recoveryState().setStage(RecoveryState.Stage.FINALIZE);
        Engine engine = getEngine();
        engine.refresh(&quot;recovery_finalization&quot;);
        engine.config().setEnableGcDeletes(true);
    }

    /**
     * Returns {@code true} if this shard can ignore a recovery attempt made to it (since the already doing/done it)
     */
    public boolean ignoreRecoveryAttempt() {
        IndexShardState state = state</B></FONT>(); // one time volatile read
        return state == IndexShardState.POST_RECOVERY || state == IndexShardState.RECOVERING || state == IndexShardState.STARTED ||
            state == IndexShardState.CLOSED;
    }

    public void readAllowed() throws IllegalIndexShardStateException {
        IndexShardState state = this.state; // one time volatile read
        if (READ_ALLOWED_STATES.contains(state) == false) {
            throw new IllegalIndexShardStateException(shardId, state, &quot;operations only allowed when shard state is one of &quot; + READ_ALLOWED_STATES.toString());
        }
    }

<A NAME="12"></A>    /** returns true if the {@link IndexShardState} allows reading */
    public boolean isReadAllowed() {
        return READ_ALLOWED_STATES.contains(state);
    <FONT color="#571b7e"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#12',2,'match2119549-top.html#12',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    private void ensureWriteAllowed(Engine.Operation.Origin origin) throws IllegalIndexShardStateException {
        IndexShardState state = this.state; // one time volatile read

        if (origin.isRecovery()) {</B></FONT>
            if (state != IndexShardState.RECOVERING) {
                throw new IllegalIndexShardStateException(shardId, state, &quot;operation only allowed when recovering, origin [&quot; + origin + &quot;]&quot;);
            }
        } else {
            if (origin == Engine.Operation.Origin.PRIMARY) {
                assert assertPrimaryMode();
            } else if (origin == Engine.Operation.Origin.REPLICA) {
                assert assertReplicationTarget();
            } else {
                assert origin == Engine.Operation.Origin.LOCAL_RESET;
                assert getActiveOperationsCount() == OPERATIONS_BLOCKED
                    : &quot;locally resetting without blocking operations, active operations are [&quot; + getActiveOperations() + &quot;]&quot;;
            }
            if (WRITE_ALLOWED_STATES.contains(state) == false) {
                throw new IllegalIndexShardStateException(shardId, state, &quot;operation only allowed when shard state is one of &quot; + WRITE_ALLOWED_STATES + &quot;, origin [&quot; + origin + &quot;]&quot;);
            }
        }
    }

    private boolean assertPrimaryMode() {
        assert shardRouting.primary() &amp;&amp; replicationTracker.isPrimaryMode() : &quot;shard &quot; + shardRouting + &quot; is not a primary shard in primary mode&quot;;
        return true;
    }

    private boolean assertReplicationTarget() {
        assert replicationTracker.isPrimaryMode() == false : &quot;shard &quot; + shardRouting + &quot; in primary mode cannot be a replication target&quot;;
        return true;
    }

    private void verifyNotClosed() throws IllegalIndexShardStateException {
        verifyNotClosed(null);
    }

    private void verifyNotClosed(Exception suppressed) throws IllegalIndexShardStateException {
        IndexShardState state = this.state; // one time volatile read
        if (state == IndexShardState.CLOSED) {
            final IllegalIndexShardStateException exc = new IndexShardClosedException(shardId, &quot;operation only allowed when not closed&quot;);
            if (suppressed != null) {
                exc.addSuppressed(suppressed);
            }
            throw exc;
        }
    }

    protected final void verifyActive() throws IllegalIndexShardStateException {
        IndexShardState state = this.state; // one time volatile read
        if (state != IndexShardState.STARTED) {
            throw new IllegalIndexShardStateException(shardId, state, &quot;operation only allowed when shard is active&quot;);
        }
    }

    /**
     * Returns number of heap bytes used by the indexing buffer for this shard, or 0 if the shard is closed
     */
    public long getIndexBufferRAMBytesUsed() {
        Engine engine = getEngineOrNull();
        if (engine == null) {
            return 0;
        }
        try {
            return engine.getIndexBufferRAMBytesUsed();
        } catch (AlreadyClosedException ex) {
            return 0;
        }
    }

    public void addShardFailureCallback(Consumer&lt;ShardFailure&gt; onShardFailure) {
        this.shardEventListener.delegates.add(onShardFailure);
    }

    /**
     * Called by {@link IndexingMemoryController} to check whether more than {@code inactiveTimeNS} has passed since the last
     * indexing operation, and notify listeners that we are now inactive so e.g. sync'd flush can happen.
     */
    public void checkIdle(long inactiveTimeNS) {
        Engine engineOrNull = getEngineOrNull();
        if (engineOrNull != null &amp;&amp; System.nanoTime() - engineOrNull.getLastWriteNanos() &gt;= inactiveTimeNS) {
            boolean wasActive = active.getAndSet(false);
            if (wasActive) {
                logger.debug(&quot;shard is now inactive&quot;);
                try {
                    indexEventListener.onShardInactive(this);
                } catch (Exception e) {
                    logger.warn(&quot;failed to notify index event listener&quot;, e);
                }
            }
        }
    }

    public boolean isActive() {
        return active.get();
    }

    public ShardPath shardPath() {
        return path;
    }

    public void recoverFromLocalShards(Consumer&lt;MappingMetadata&gt; mappingUpdateConsumer,
                                       List&lt;IndexShard&gt; localShards,
                                       ActionListener&lt;Boolean&gt; listener) throws IOException {
        assert shardRouting.primary() : &quot;recover from local shards only makes sense if the shard is a primary shard&quot;;
        assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.LOCAL_SHARDS : &quot;invalid recovery type: &quot; + recoveryState.getRecoverySource();
        final List&lt;LocalShardSnapshot&gt; snapshots = new ArrayList&lt;&gt;();
        final ActionListener&lt;Boolean&gt; recoveryListener = ActionListener.runBefore(listener, () -&gt; IOUtils.close(snapshots));
        boolean success = false;
        try {
            for (IndexShard shard : localShards) {
                snapshots.add(new LocalShardSnapshot(shard));
            }
            // we are the first primary, recover from the gateway
            // if its post api allocation, the index should exists
            assert shardRouting.primary() : &quot;recover from local shards only makes sense if the shard is a primary shard&quot;;
            StoreRecovery storeRecovery = new StoreRecovery(shardId, logger);
            storeRecovery.recoverFromLocalShards(mappingUpdateConsumer, this, snapshots, recoveryListener);
            success = true;
        } finally {
            if (success == false) {
                IOUtils.close(snapshots);
            }
        }
    }

    public void recoverFromStore(ActionListener&lt;Boolean&gt; listener) {
        // we are the first primary, recover from the gateway
        // if its post api allocation, the index should exists
        assert shardRouting.primary() : &quot;recover from store only makes sense if the shard is a primary shard&quot;;
        assert shardRouting.initializing() : &quot;can only start recovery on initializing shard&quot;;
        StoreRecovery storeRecovery = new StoreRecovery(shardId, logger);
        storeRecovery.recoverFromStore(this, listener);
    }

    public void restoreFromRepository(Repository repository, ActionListener&lt;Boolean&gt; listener) {
        try {
            assert shardRouting.primary() : &quot;recover from store only makes sense if the shard is a primary shard&quot;;
            assert recoveryState.getRecoverySource().getType() == RecoverySource.Type.SNAPSHOT : &quot;invalid recovery type: &quot; +
                recoveryState.getRecoverySource();
            StoreRecovery storeRecovery = new StoreRecovery(shardId, logger);
            storeRecovery.recoverFromRepository(this, repository, listener);
        } catch (Exception e) {
            listener.onFailure(e);
        }
    }

    /**
     * Tests whether or not the engine should be flushed periodically.
     * This test is based on the current size of the translog compared to the configured flush threshold size.
     *
     * @return {@code true} if the engine should be flushed
     */
    boolean shouldPeriodicallyFlush() {
        final Engine engine = getEngineOrNull();
        if (engine != null) {
            try {
                return engine.shouldPeriodicallyFlush();
<A NAME="11"></A>            } catch (final AlreadyClosedException e) {
                // we are already closed, no need to flush or roll
            }
        <FONT color="#b041ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#11',2,'match2119549-top.html#11',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}
        return false;
    }

    /**
     * Tests whether or not the translog generation should be rolled to a new generation. This test is based on the size of the current
     * generation compared to the configured generation threshold size.
     *
     * @return {@code true} if the current generation should be rolled to a new generation
     */
    boolean shouldRollTranslogGeneration() {
        final Engine engine = getEngineOrNull</B></FONT>();
        if (engine != null) {
            try {
                return engine.shouldRollTranslogGeneration();
            } catch (final AlreadyClosedException e) {
                // we are already closed, no need to flush or roll
            }
        }
        return false;
    }

    public void onSettingsChanged() {
        Engine engineOrNull = getEngineOrNull();
        if (engineOrNull != null) {
            final boolean disableTranslogRetention = indexSettings.isSoftDeleteEnabled() &amp;&amp; useRetentionLeasesInPeerRecovery;
            engineOrNull.onSettingsChanged(
                disableTranslogRetention ? TimeValue.MINUS_ONE : indexSettings.getTranslogRetentionAge(),
                disableTranslogRetention ? new ByteSizeValue(-1) : indexSettings.getTranslogRetentionSize(),
                indexSettings.getSoftDeleteRetentionOperations()
            );
        }
    }

    private void turnOffTranslogRetention() {
        logger.debug(&quot;turn off the translog retention for the replication group {} &quot; +
            &quot;as it starts using retention leases exclusively in peer recoveries&quot;, shardId);
        // Off to the generic threadPool as pruning the delete tombstones can be expensive.
        threadPool.generic().execute(new AbstractRunnable() {
            @Override
            public void onFailure(Exception e) {
                if (state != IndexShardState.CLOSED) {
                    logger.warn(&quot;failed to turn off translog retention&quot;, e);
                }
            }

            @Override
            protected void doRun() {
                onSettingsChanged();
                trimTranslog();
            }
        });
    }

    /**
     * Acquires a lock on the translog files and Lucene soft-deleted documents to prevent them from being trimmed
     */
    public Closeable acquireHistoryRetentionLock(Engine.HistorySource source) {
        return getEngine().acquireHistoryRetentionLock(source);
    }

    /**
     * Returns the estimated number of history operations whose seq# at least the provided seq# in this shard.
     */
    public int estimateNumberOfHistoryOperations(String reason, Engine.HistorySource source, long startingSeqNo) throws IOException {
        return getEngine().estimateNumberOfHistoryOperations(reason, source, mapperService, startingSeqNo);
    }

    /**
     * Creates a new history snapshot for reading operations since the provided starting seqno (inclusive).
     * The returned snapshot can be retrieved from either Lucene index or translog files.
     */
    public Translog.Snapshot getHistoryOperations(String reason, Engine.HistorySource source, long startingSeqNo) throws IOException {
        return getEngine().readHistoryOperations(reason, source, mapperService, startingSeqNo);
    }

    /**
     * Checks if we have a completed history of operations since the given starting seqno (inclusive).
     * This method should be called after acquiring the retention lock; See {@link #acquireHistoryRetentionLock(Engine.HistorySource)}
     */
    public boolean hasCompleteHistoryOperations(String reason, Engine.HistorySource source, long startingSeqNo) throws IOException {
        return getEngine().hasCompleteOperationHistory(reason, source, mapperService, startingSeqNo);
    }

    /**
     * Gets the minimum retained sequence number for this engine.
     *
     * @return the minimum retained sequence number
     */
    public long getMinRetainedSeqNo() {
        return getEngine().getMinRetainedSeqNo();
    }

    /**
     * Creates a new changes snapshot for reading operations whose seq_no are between {@code fromSeqNo}(inclusive)
     * and {@code toSeqNo}(inclusive). The caller has to close the returned snapshot after finishing the reading.
     *
     * @param source            the source of the request
     * @param fromSeqNo         the from seq_no (inclusive) to read
     * @param toSeqNo           the to seq_no (inclusive) to read
     * @param requiredFullRange if {@code true} then {@link Translog.Snapshot#next()} will throw {@link IllegalStateException}
     *                          if any operation between {@code fromSeqNo} and {@code toSeqNo} is missing.
     *                          This parameter should be only enabled when the entire requesting range is below the global checkpoint.
     */
    public Translog.Snapshot newChangesSnapshot(String source, long fromSeqNo, long toSeqNo, boolean requiredFullRange) throws IOException {
        return getEngine().newChangesSnapshot(source, mapperService, fromSeqNo, toSeqNo, requiredFullRange);
    }

    public List&lt;Segment&gt; segments(boolean verbose) {
        return getEngine().segments(verbose);
    }

    public String getHistoryUUID() {
        return getEngine().getHistoryUUID();
    }

    public IndexEventListener getIndexEventListener() {
        return indexEventListener;
<A NAME="9"></A>    }

    public void activateThrottling() {
        <FONT color="#83a33a"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#9',2,'match2119549-top.html#9',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>try {
            getEngine().activateThrottling();
        } catch (AlreadyClosedException ex) {
            // ignore
        }</B></FONT>
    }

    public void deactivateThrottling() {
        try {
            getEngine().deactivateThrottling();
        } catch (AlreadyClosedException ex) {
            // ignore
        }
    }

    private void handleRefreshException(Exception e) {
        if (e instanceof AlreadyClosedException) {
            // ignore
        } else if (e instanceof RefreshFailedEngineException) {
            RefreshFailedEngineException rfee = (RefreshFailedEngineException) e;
            if (rfee.getCause() instanceof InterruptedException) {
                // ignore, we are being shutdown
            } else if (rfee.getCause() instanceof ClosedByInterruptException) {
                // ignore, we are being shutdown
            } else if (rfee.getCause() instanceof ThreadInterruptedException) {
                // ignore, we are being shutdown
            } else {
                if (state != IndexShardState.CLOSED) {
                    logger.warn(&quot;Failed to perform engine refresh&quot;, e);
                }
            }
        } else {
            if (state != IndexShardState.CLOSED) {
                logger.warn(&quot;Failed to perform engine refresh&quot;, e);
            }
        }
    }

    /**
     * Called when our shard is using too much heap and should move buffered indexed/deleted documents to disk.
     */
    public void writeIndexingBuffer() {
        try {
            Engine engine = getEngine();
            engine.writeIndexingBuffer();
        } catch (Exception e) {
            handleRefreshException(e);
        }
    }

    /**
     * Notifies the service to update the local checkpoint for the shard with the provided allocation ID. See
     * {@link ReplicationTracker#updateLocalCheckpoint(String, long)} for
     * details.
     *
     * @param allocationId the allocation ID of the shard to update the local checkpoint for
     * @param checkpoint   the local checkpoint for the shard
     */
    public void updateLocalCheckpointForShard(final String allocationId, final long checkpoint) {
        assert assertPrimaryMode();
        verifyNotClosed();
        replicationTracker.updateLocalCheckpoint(allocationId, checkpoint);
    }

    /**
     * Update the local knowledge of the persisted global checkpoint for the specified allocation ID.
     *
     * @param allocationId     the allocation ID to update the global checkpoint for
     * @param globalCheckpoint the global checkpoint
     */
    public void updateGlobalCheckpointForShard(final String allocationId, final long globalCheckpoint) {
        assert assertPrimaryMode();
        verifyNotClosed();
        replicationTracker.updateGlobalCheckpointForShard(allocationId, globalCheckpoint);
    }

    /**
     * Add a global checkpoint listener. If the global checkpoint is equal to or above the global checkpoint the listener is waiting for,
     * then the listener will be notified immediately via an executor (so possibly not on the current thread). If the specified timeout
     * elapses before the listener is notified, the listener will be notified with an {@link TimeoutException}. A caller may pass null to
     * specify no timeout.
     *
     * @param waitingForGlobalCheckpoint the global checkpoint the listener is waiting for
     * @param listener                   the listener
     * @param timeout                    the timeout
     */
    public void addGlobalCheckpointListener(
            final long waitingForGlobalCheckpoint,
<A NAME="7"></A>            final GlobalCheckpointListeners.GlobalCheckpointListener listener,
            final TimeValue timeout) {
        this.globalCheckpointListeners.add(waitingForGlobalCheckpoint, listener, timeout);
    <FONT color="#38a4a5"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#7',2,'match2119549-top.html#7',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    private void ensureSoftDeletesEnabled(String feature) {
        if (indexSettings.isSoftDeleteEnabled() == false) {
            String message = feature + &quot; requires soft deletes but &quot; + indexSettings.getIndex</B></FONT>() + &quot; does not have soft deletes enabled&quot;;
            assert false : message;
            throw new IllegalStateException(message);
        }
    }

    /**
     * Get all non-expired retention leases tracked on this shard.
     *
     * @return the retention leases
     */
    public RetentionLeases getRetentionLeases() {
        return getRetentionLeases(false).v2();
    }

    /**
     * If the expire leases parameter is false, gets all retention leases tracked on this shard and otherwise first calculates
     * expiration of existing retention leases, and then gets all non-expired retention leases tracked on this shard. Note that only the
     * primary shard calculates which leases are expired, and if any have expired, syncs the retention leases to any replicas. If the
     * expire leases parameter is true, this replication tracker must be in primary mode.
     *
     * @return a tuple indicating whether or not any retention leases were expired, and the non-expired retention leases
     */
    public Tuple&lt;Boolean, RetentionLeases&gt; getRetentionLeases(final boolean expireLeases) {
        assert expireLeases == false || assertPrimaryMode();
        verifyNotClosed();
        return replicationTracker.getRetentionLeases(expireLeases);
    }

    public RetentionLeaseStats getRetentionLeaseStats() {
        verifyNotClosed();
        return new RetentionLeaseStats(getRetentionLeases());
    }

    /**
     * Adds a new retention lease.
     *
     * @param id                      the identifier of the retention lease
     * @param retainingSequenceNumber the retaining sequence number
     * @param source                  the source of the retention lease
     * @param listener                the callback when the retention lease is successfully added and synced to replicas
     * @return the new retention lease
     * @throws IllegalArgumentException if the specified retention lease already exists
     */
    public RetentionLease addRetentionLease(
            final String id,
            final long retainingSequenceNumber,
            final String source,
            final ActionListener&lt;ReplicationResponse&gt; listener) {
        Objects.requireNonNull(listener);
        assert assertPrimaryMode();
        verifyNotClosed();
        ensureSoftDeletesEnabled(&quot;retention leases&quot;);
        try (Closeable ignore = acquireHistoryRetentionLock(Engine.HistorySource.INDEX)) {
            final long actualRetainingSequenceNumber =
                retainingSequenceNumber == RETAIN_ALL ? getMinRetainedSeqNo() : retainingSequenceNumber;
            return replicationTracker.addRetentionLease(id, actualRetainingSequenceNumber, source, listener);
        } catch (final IOException e) {
            throw new AssertionError(e);
        }
    }

    /**
     * Renews an existing retention lease.
     *
     * @param id                      the identifier of the retention lease
     * @param retainingSequenceNumber the retaining sequence number
     * @param source                  the source of the retention lease
     * @return the renewed retention lease
     * @throws IllegalArgumentException if the specified retention lease does not exist
     */
    public RetentionLease renewRetentionLease(final String id, final long retainingSequenceNumber, final String source) {
        assert assertPrimaryMode();
        verifyNotClosed();
        ensureSoftDeletesEnabled(&quot;retention leases&quot;);
        try (Closeable ignore = acquireHistoryRetentionLock(Engine.HistorySource.INDEX)) {
            final long actualRetainingSequenceNumber =
                    retainingSequenceNumber == RETAIN_ALL ? getMinRetainedSeqNo() : retainingSequenceNumber;
            return replicationTracker.renewRetentionLease(id, actualRetainingSequenceNumber, source);
        } catch (final IOException e) {
            throw new AssertionError(e);
        }
    }

    /**
     * Removes an existing retention lease.
     *
     * @param id       the identifier of the retention lease
     * @param listener the callback when the retention lease is successfully removed and synced to replicas
     */
    public void removeRetentionLease(final String id, final ActionListener&lt;ReplicationResponse&gt; listener) {
        Objects.requireNonNull(listener);
        assert assertPrimaryMode();
        verifyNotClosed();
        ensureSoftDeletesEnabled(&quot;retention leases&quot;);
        replicationTracker.removeRetentionLease(id, listener);
    }

    /**
     * Updates retention leases on a replica.
     *
     * @param retentionLeases the retention leases
     */
    public void updateRetentionLeasesOnReplica(final RetentionLeases retentionLeases) {
        assert assertReplicationTarget();
        verifyNotClosed();
        replicationTracker.updateRetentionLeasesOnReplica(retentionLeases);
    }

    /**
     * Loads the latest retention leases from their dedicated state file.
     *
     * @return the retention leases
     * @throws IOException if an I/O exception occurs reading the retention leases
     */
    public RetentionLeases loadRetentionLeases() throws IOException {
        verifyNotClosed();
        return replicationTracker.loadRetentionLeases(path.getShardStatePath());
    }

    /**
     * Persists the current retention leases to their dedicated state file.
     *
     * @throws WriteStateException if an exception occurs writing the state file
     */
    public void persistRetentionLeases() throws WriteStateException {
        verifyNotClosed();
        replicationTracker.persistRetentionLeases(path.getShardStatePath());
    }

    public boolean assertRetentionLeasesPersisted() throws IOException {
        return replicationTracker.assertRetentionLeasesPersisted(path.getShardStatePath());
    }

    /**
     * Syncs the current retention leases to all replicas.
     */
    public void syncRetentionLeases() {
        assert assertPrimaryMode();
        verifyNotClosed();
<A NAME="24"></A>        replicationTracker.renewPeerRecoveryRetentionLeases();
        final Tuple&lt;Boolean, RetentionLeases&gt; retentionLeases = getRetentionLeases(true);
        if (retentionLeases.v1()) {
            <FONT color="#79764d"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#24',2,'match2119549-top.html#24',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>logger.trace(&quot;syncing retention leases [{}] after expiration check&quot;, retentionLeases.v2());
            retentionLeaseSyncer.sync(
                shardId,
                shardRouting.allocationId().getId(),
                getPendingPrimaryTerm(),
                retentionLeases.v2(),
                ActionListener.wrap(
                    r -&gt; {},</B></FONT>
                    e -&gt; logger.warn(
                        new ParameterizedMessage(
                            &quot;failed to sync retention leases [{}] after expiration check&quot;, retentionLeases),
                        e
                    )
                )
<A NAME="5"></A>            );
        } else {
            logger.trace(&quot;background syncing retention leases [{}] after expiration check&quot;, retentionLeases.v2());
            <FONT color="#151b8d"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#5',2,'match2119549-top.html#5',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>retentionLeaseSyncer.backgroundSync(
                shardId, shardRouting.allocationId().getId(), getPendingPrimaryTerm(), retentionLeases.v2());
        }
    }

    /**
     * Called when the recovery process for a shard has opened the engine on the target shard. Ensures that the right data structures
     * have been set up locally to track local checkpoint information for the shard and that the shard is added to the replication group.
     *
     * @param allocationId  the allocation ID of the shard for which recovery was initiated
     */
    public void initiateTracking(final String allocationId</B></FONT>) {
        assert assertPrimaryMode();
        replicationTracker.initiateTracking(allocationId);
    }

    /**
     * Marks the shard with the provided allocation ID as in-sync with the primary shard. See
     * {@link ReplicationTracker#markAllocationIdAsInSync(String, long)}
     * for additional details.
     *
     * @param allocationId    the allocation ID of the shard to mark as in-sync
     * @param localCheckpoint the current local checkpoint on the shard
     */
    public void markAllocationIdAsInSync(final String allocationId, final long localCheckpoint) throws InterruptedException {
        assert assertPrimaryMode();
        replicationTracker.markAllocationIdAsInSync(allocationId, localCheckpoint);
    }

    /**
     * Returns the persisted local checkpoint for the shard.
     *
     * @return the local checkpoint
     */
    public long getLocalCheckpoint() {
        return getEngine().getPersistedLocalCheckpoint();
    }

    /**
     * Returns the global checkpoint for the shard.
     *
     * @return the global checkpoint
     */
    public long getLastKnownGlobalCheckpoint() {
        return replicationTracker.getGlobalCheckpoint();
    }

    /**
     * Returns the latest global checkpoint value that has been persisted in the underlying storage (i.e. translog's checkpoint)
     */
    public long getLastSyncedGlobalCheckpoint() {
        return getEngine().getLastSyncedGlobalCheckpoint();
    }

    /**
     * Get the local knowledge of the global checkpoints for all in-sync allocation IDs.
     *
     * @return a map from allocation ID to the local knowledge of the global checkpoint for that allocation ID
     */
    public ObjectLongMap&lt;String&gt; getInSyncGlobalCheckpoints() {
        assert assertPrimaryMode();
        verifyNotClosed();
        return replicationTracker.getInSyncGlobalCheckpoints();
    }

    /**
     * Syncs the global checkpoint to the replicas if the global checkpoint on at least one replica is behind the global checkpoint on the
     * primary.
     */
    public void maybeSyncGlobalCheckpoint(final String reason) {
        verifyNotClosed();
        assert shardRouting.primary() : &quot;only call maybeSyncGlobalCheckpoint on primary shard&quot;;
        if (replicationTracker.isPrimaryMode() == false) {
            return;
        }
        assert assertPrimaryMode();
        // only sync if there are no operations in flight, or when using async durability
<A NAME="2"></A>        final SeqNoStats stats = getEngine().getSeqNoStats(replicationTracker.getGlobalCheckpoint());
        final boolean asyncDurability = indexSettings().getTranslogDurability() == Translog.Durability.ASYNC;
        if (stats.getMaxSeqNo() == stats.getGlobalCheckpoint() || asyncDurability) {
            <FONT color="#980517"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#2',2,'match2119549-top.html#2',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>final ObjectLongMap&lt;String&gt; globalCheckpoints = getInSyncGlobalCheckpoints();
            final long globalCheckpoint = replicationTracker.getGlobalCheckpoint();
            // async durability means that the local checkpoint might lag (as it is only advanced on fsync)
            // periodically ask for the newest local checkpoint by syncing the global checkpoint, so that ultimately the global
            // checkpoint can be synced. Also take into account that a shard might be pending sync, which means that it isn't
            // in the in-sync set just yet but might be blocked on waiting for its persisted local checkpoint to catch up to
            // the global checkpoint.
            final boolean syncNeeded =
                (asyncDurability &amp;&amp; (stats.getGlobalCheckpoint() &lt; stats.getMaxSeqNo() || replicationTracker.pendingInSync()))
                    // check if the persisted global checkpoint
                    || StreamSupport
                            .stream(globalCheckpoints.values().spliterator(), false)
                            .anyMatch(v -&gt; v.value </B></FONT>&lt; globalCheckpoint);
            // only sync if index is not closed and there is a shard lagging the primary
            if (syncNeeded &amp;&amp; indexSettings.getIndexMetadata().getState() == IndexMetadata.State.OPEN) {
                logger.trace(&quot;syncing global checkpoint for [{}]&quot;, reason);
                globalCheckpointSyncer.run();
            }
        }
    }

    /**
     * Returns the current replication group for the shard.
     *
     * @return the replication group
     */
    public ReplicationGroup getReplicationGroup() {
        assert assertPrimaryMode();
        verifyNotClosed();
        return replicationTracker.getReplicationGroup();
    }

    /**
     * Updates the global checkpoint on a replica shard after it has been updated by the primary.
     *
     * @param globalCheckpoint the global checkpoint
     * @param reason           the reason the global checkpoint was updated
     */
    public void updateGlobalCheckpointOnReplica(final long globalCheckpoint, final String reason) {
        assert assertReplicationTarget();
        final long localCheckpoint = getLocalCheckpoint();
        if (globalCheckpoint &gt; localCheckpoint) {
            /*
             * This can happen during recovery when the shard has started its engine but recovery is not finalized and is receiving global
             * checkpoint updates. However, since this shard is not yet contributing to calculating the global checkpoint, it can be the
             * case that the global checkpoint update from the primary is ahead of the local checkpoint on this shard. In this case, we
             * ignore the global checkpoint update. This can happen if we are in the translog stage of recovery. Prior to this, the engine
             * is not opened and this shard will not receive global checkpoint updates, and after this the shard will be contributing to
             * calculations of the global checkpoint. However, we can not assert that we are in the translog stage of recovery here as
             * while the global checkpoint update may have emanated from the primary when we were in that state, we could subsequently move
             * to recovery finalization, or even finished recovery before the update arrives here.
             */
            assert state() != IndexShardState.POST_RECOVERY &amp;&amp; state() != IndexShardState.STARTED :
                &quot;supposedly in-sync shard copy received a global checkpoint [&quot; + globalCheckpoint + &quot;] &quot; +
                    &quot;that is higher than its local checkpoint [&quot; + localCheckpoint + &quot;]&quot;;
            return;
        }
        replicationTracker.updateGlobalCheckpointOnReplica(globalCheckpoint, reason);
    }

    /**
     * Updates the known allocation IDs and the local checkpoints for the corresponding allocations from a primary relocation source.
     *
     * @param primaryContext the sequence number context
     */
    public void activateWithPrimaryContext(final ReplicationTracker.PrimaryContext primaryContext) {
        assert shardRouting.primary() &amp;&amp; shardRouting.isRelocationTarget() :
<A NAME="10"></A>            &quot;only primary relocation target can update allocation IDs from primary context: &quot; + shardRouting;
        assert primaryContext.getCheckpointStates().containsKey(routingEntry().allocationId().getId()) :
            &quot;primary context [&quot; + primaryContext + &quot;] does not contain relocation target [&quot; + routingEntry() + &quot;]&quot;;
        assert <FONT color="#ad5910"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#10',2,'match2119549-top.html#10',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>getLocalCheckpoint() == primaryContext.getCheckpointStates().get(routingEntry().allocationId().getId())
            .getLocalCheckpoint() || indexSettings().getTranslogDurability() == Translog.Durability.ASYNC :
            &quot;local checkpoint [&quot; + getLocalCheckpoint</B></FONT>() + &quot;] does not match checkpoint from primary context [&quot; + primaryContext + &quot;]&quot;;
        synchronized (mutex) {
            replicationTracker.activateWithPrimaryContext(primaryContext); // make changes to primaryMode flag only under mutex
        }
        ensurePeerRecoveryRetentionLeasesExist();
    }

    private void ensurePeerRecoveryRetentionLeasesExist() {
        threadPool.generic().execute(() -&gt; replicationTracker.createMissingPeerRecoveryRetentionLeases(ActionListener.wrap(
            r -&gt; logger.trace(&quot;created missing peer recovery retention leases&quot;),
            e -&gt; logger.debug(&quot;failed creating missing peer recovery retention leases&quot;, e))));
    }

    /**
     * Check if there are any recoveries pending in-sync.
     *
     * @return {@code true} if there is at least one shard pending in-sync, otherwise false
     */
    public boolean pendingInSync() {
        assert assertPrimaryMode();
        return replicationTracker.pendingInSync();
    }

    public void maybeCheckIndex() {
        recoveryState.setStage(RecoveryState.Stage.VERIFY_INDEX);
        if (Booleans.isTrue(checkIndexOnStartup) || &quot;checksum&quot;.equals(checkIndexOnStartup)) {
            try {
                checkIndex();
            } catch (IOException ex) {
                throw new RecoveryFailedException(recoveryState, &quot;check index failed&quot;, ex);
            }
        }
    }

    void checkIndex() throws IOException {
        if (store.tryIncRef()) {
            try {
                doCheckIndex();
            } catch (IOException e) {
                store.markStoreCorrupted(e);
                throw e;
            } finally {
                store.decRef();
            }
        }
    }

    private void doCheckIndex() throws IOException {
        final long timeNS = System.nanoTime();
        if (!Lucene.indexExists(store.directory())) {
            return;
        }
        BytesStreamOutput os = new BytesStreamOutput();
        PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name());

        if (&quot;checksum&quot;.equals(checkIndexOnStartup)) {
            // physical verification only: verify all checksums for the latest commit
            IOException corrupt = null;
            MetadataSnapshot metadata = snapshotStoreMetadata();
            for (Map.Entry&lt;String, StoreFileMetadata&gt; entry : metadata.asMap().entrySet()) {
                try {
                    Store.checkIntegrity(entry.getValue(), store.directory());
                    out.println(&quot;checksum passed: &quot; + entry.getKey());
                } catch (IOException exc) {
                    out.println(&quot;checksum failed: &quot; + entry.getKey());
                    exc.printStackTrace(out);
                    corrupt = exc;
                }
            }
            out.flush();
            if (corrupt != null) {
                logger.warn(&quot;check index [failure]\n{}&quot;, os.bytes().utf8ToString());
                throw corrupt;
            }
        } else {
            // full checkindex
            final CheckIndex.Status status = store.checkIndex(out);
            out.flush();
            if (!status.clean) {
                if (state == IndexShardState.CLOSED) {
                    // ignore if closed....
                    return;
                }
                logger.warn(&quot;check index [failure]\n{}&quot;, os.bytes().utf8ToString());
                throw new IOException(&quot;index check failure&quot;);
            }
        }

        if (logger.isDebugEnabled()) {
<A NAME="6"></A>            logger.debug(&quot;check index [success]\n{}&quot;, os.bytes().utf8ToString());
        }

        <FONT color="#8c8774"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#6',2,'match2119549-top.html#6',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>recoveryState.getVerifyIndex().checkIndexTime(Math.max(0, TimeValue.nsecToMSec(System.nanoTime() - timeNS)));
    }

    Engine getEngine() {
        Engine engine = getEngineOrNull</B></FONT>();
        if (engine == null) {
            throw new AlreadyClosedException(&quot;engine is closed&quot;);
        }
        return engine;
    }

    /**
     * NOTE: returns null if engine is not yet started (e.g. recovery phase 1, copying over index files, is still running), or if engine is
     * closed.
     */
    protected Engine getEngineOrNull() {
        return this.currentEngineReference.get();
    }

    public void startRecovery(RecoveryState recoveryState,
                              PeerRecoveryTargetService recoveryTargetService,
                              PeerRecoveryTargetService.RecoveryListener recoveryListener,
                              RepositoriesService repositoriesService,
                              Consumer&lt;MappingMetadata&gt; mappingUpdateConsumer,
                              IndicesService indicesService) {
        // TODO: Create a proper object to encapsulate the recovery context
        // all of the current methods here follow a pattern of:
        // resolve context which isn't really dependent on the local shards and then async
        // call some external method with this pointer.
        // with a proper recovery context object we can simply change this to:
        // startRecovery(RecoveryState recoveryState, ShardRecoverySource source ) {
        //     markAsRecovery(&quot;from &quot; + source.getShortDescription(), recoveryState);
        //     threadPool.generic().execute()  {
        //           onFailure () { listener.failure() };
        //           doRun() {
        //                if (source.recover(this)) {
        //                  recoveryListener.onRecoveryDone(recoveryState);
        //                }
        //           }
        //     }}
        // }
        assert recoveryState.getRecoverySource().equals(shardRouting.recoverySource());
        switch (recoveryState.getRecoverySource().getType()) {
            case EMPTY_STORE:
            case EXISTING_STORE:
                executeRecovery(&quot;from store&quot;, recoveryState, recoveryListener, this::recoverFromStore);
                break;
            case PEER:
                try {
                    markAsRecovering(&quot;from &quot; + recoveryState.getSourceNode(), recoveryState);
                    recoveryTargetService.startRecovery(this, recoveryState.getSourceNode(), recoveryListener);
                } catch (Exception e) {
                    failShard(&quot;corrupted preexisting index&quot;, e);
                    recoveryListener.onRecoveryFailure(recoveryState, new RecoveryFailedException(recoveryState, null, e), true);
                }
                break;
            case SNAPSHOT:
                final String repo = ((SnapshotRecoverySource) recoveryState.getRecoverySource()).snapshot().getRepository();
                executeRecovery(
                    &quot;from snapshot&quot;,
                    recoveryState,
                    recoveryListener,
                    l -&gt; restoreFromRepository(repositoriesService.repository(repo), l)
<A NAME="17"></A>                );
                break;
            case LOCAL_SHARDS:
                <FONT color="#3090c7"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#17',2,'match2119549-top.html#17',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>final IndexMetadata indexMetadata = indexSettings().getIndexMetadata();
                final Index resizeSourceIndex = indexMetadata.getResizeSourceIndex();
                final List&lt;IndexShard&gt; startedShards = new ArrayList&lt;&gt;();
                final IndexService sourceIndexService = indicesService.indexService</B></FONT>(resizeSourceIndex);
                final Set&lt;ShardId&gt; requiredShards;
                final int numShards;
                if (sourceIndexService != null) {
                    requiredShards = IndexMetadata.selectRecoverFromShards(shardId().id(),
                                                                           sourceIndexService.getMetadata(), indexMetadata.getNumberOfShards());
                    for (IndexShard shard : sourceIndexService) {
                        if (shard.state() == IndexShardState.STARTED &amp;&amp; requiredShards.contains(shard.shardId())) {
                            startedShards.add(shard);
                        }
                    }
                    numShards = requiredShards.size();
                } else {
                    numShards = -1;
                    requiredShards = Collections.emptySet();
<A NAME="26"></A>                }

                if (numShards == startedShards.size()) {
                    assert <FONT color="#68818b"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#26',2,'match2119549-top.html#26',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>requiredShards.isEmpty() == false;
                    executeRecovery(&quot;from local shards&quot;, recoveryState, recoveryListener,
                        l -&gt; recoverFromLocalShards(mappingUpdateConsumer,
                            startedShards.stream().filter((s) -&gt; requiredShards.contains</B></FONT>(s.shardId())).collect(Collectors.toList()), l));
                } else {
                    final RuntimeException e;
                    if (numShards == -1) {
                        e = new IndexNotFoundException(resizeSourceIndex);
                    } else {
                        e = new IllegalStateException(&quot;not all required shards of index &quot; + resizeSourceIndex
                            + &quot; are started yet, expected &quot; + numShards + &quot; found &quot; + startedShards.size() + &quot; can't recover shard &quot;
                            + shardId());
                    }
                    throw e;
                }
                break;
            default:
                throw new IllegalArgumentException(&quot;Unknown recovery source &quot; + recoveryState.getRecoverySource());
        }
    }

    private void executeRecovery(String reason,
                                 RecoveryState recoveryState,
                                 PeerRecoveryTargetService.RecoveryListener recoveryListener,
                                 CheckedConsumer&lt;ActionListener&lt;Boolean&gt;, Exception&gt; action) {
        markAsRecovering(reason, recoveryState); // mark the shard as recovering on the cluster state thread
        threadPool.generic().execute(ActionRunnable.wrap(ActionListener.wrap(
            r -&gt; {
                if (r) {
                    recoveryListener.onRecoveryDone(recoveryState);
                }
            },
            e -&gt; recoveryListener.onRecoveryFailure(recoveryState, new RecoveryFailedException(recoveryState, null, e), true)), action));
    }

    /**
     * Returns whether the shard is a relocated primary, i.e. not in charge anymore of replicating changes (see {@link ReplicationTracker}).
     */
    public boolean isRelocatedPrimary() {
        assert shardRouting.primary() : &quot;only call isRelocatedPrimary on primary shard&quot;;
        return replicationTracker.isRelocated();
    }

    public RetentionLease addPeerRecoveryRetentionLease(String nodeId, long globalCheckpoint,
                                                        ActionListener&lt;ReplicationResponse&gt; listener) {
        assert assertPrimaryMode();
        // only needed for BWC reasons involving rolling upgrades from versions that do not support PRRLs:
        assert indexSettings.getIndexVersionCreated().before(Version.V_4_3_0) || indexSettings.isSoftDeleteEnabled() == false;
        return replicationTracker.addPeerRecoveryRetentionLease(nodeId, globalCheckpoint, listener);
    }

    public RetentionLease cloneLocalPeerRecoveryRetentionLease(String nodeId, ActionListener&lt;ReplicationResponse&gt; listener) {
        assert assertPrimaryMode();
        return replicationTracker.cloneLocalPeerRecoveryRetentionLease(nodeId, listener);
    }

    public void removePeerRecoveryRetentionLease(String nodeId, ActionListener&lt;ReplicationResponse&gt; listener) {
        assert assertPrimaryMode();
        replicationTracker.removePeerRecoveryRetentionLease(nodeId, listener);
    }

    /**
     * Returns a list of retention leases for peer recovery installed in this shard copy.
     */
    public List&lt;RetentionLease&gt; getPeerRecoveryRetentionLeases() {
        return replicationTracker.getPeerRecoveryRetentionLeases();
    }

    public boolean useRetentionLeasesInPeerRecovery() {
        return useRetentionLeasesInPeerRecovery;
    }

    private SafeCommitInfo getSafeCommitInfo() {
        final Engine engine = getEngineOrNull();
        return engine == null ? SafeCommitInfo.EMPTY : engine.getSafeCommitInfo();
    }

    class ShardEventListener implements Engine.EventListener {
        private final CopyOnWriteArrayList&lt;Consumer&lt;ShardFailure&gt;&gt; delegates = new CopyOnWriteArrayList&lt;&gt;();

        // called by the current engine
        @Override
        public void onFailedEngine(String reason, @Nullable Exception failure) {
            final ShardFailure shardFailure = new ShardFailure(shardRouting, reason, failure);
            for (Consumer&lt;ShardFailure&gt; listener : delegates) {
                try {
                    listener.accept(shardFailure);
                } catch (Exception inner) {
                    inner.addSuppressed(failure);
                    logger.warn(&quot;exception while notifying engine failure&quot;, inner);
                }
            }
        }
    }

    private static void persistMetadata(
            final ShardPath shardPath,
            final IndexSettings indexSettings,
            final ShardRouting newRouting,
            final @Nullable ShardRouting currentRouting,
            final Logger logger) throws IOException {
        assert newRouting != null : &quot;newRouting must not be null&quot;;

        // only persist metadata if routing information that is persisted in shard state metadata actually changed
        final ShardId shardId = newRouting.shardId();
        if (currentRouting == null
            || currentRouting.primary() != newRouting.primary()
            || currentRouting.allocationId().equals(newRouting.allocationId()) == false) {
            assert currentRouting == null || currentRouting.isSameAllocation(newRouting);
            final String writeReason;
            if (currentRouting == null) {
                writeReason = &quot;initial state with allocation id [&quot; + newRouting.allocationId() + &quot;]&quot;;
            } else {
                writeReason = &quot;routing changed from &quot; + currentRouting + &quot; to &quot; + newRouting;
            }
            logger.trace(&quot;{} writing shard state, reason [{}]&quot;, shardId, writeReason);
            final ShardStateMetadata newShardStateMetadata =
                    new ShardStateMetadata(newRouting.primary(), indexSettings.getUUID(), newRouting.allocationId());
            ShardStateMetadata.FORMAT.write(newShardStateMetadata, shardPath.getShardStatePath());
        } else {
            logger.trace(&quot;{} skip writing shard state, has been written before&quot;, shardId);
        }
    }

    private EngineConfig newEngineConfig(LongSupplier globalCheckpointSupplier) {
        return new EngineConfig(
            shardId,
            shardRouting.allocationId().getId(),
            threadPool,
            indexSettings,
            store,
            indexSettings.getMergePolicy(),
            mapperService == null ? null : mapperService.indexAnalyzer(),
            codecService,
            shardEventListener,
            indexCache == null ? null : indexCache.query(),
            cachingPolicy,
            translogConfig,
            IndexingMemoryController.SHARD_INACTIVE_TIME_SETTING.get(indexSettings.getSettings()),
            List.of(refreshListeners, refreshPendingLocationListener),
            Collections.singletonList(new RefreshMetricUpdater(refreshMetric)),
            circuitBreakerService,
            globalCheckpointSupplier,
            replicationTracker::getRetentionLeases,
            this::getOperationPrimaryTerm,
            tombstoneDocSupplier()
        );
    }

    /**
     * Acquire a primary operation permit whenever the shard is ready for indexing. If a permit is directly available, the provided
     * ActionListener will be called on the calling thread. During relocation hand-off, permit acquisition can be delayed. The provided
     * ActionListener will then be called using the provided executor.
     *
     * @param debugInfo an extra information that can be useful when tracing an unreleased permit. When assertions are enabled
     *                  the tracing will capture the supplied object's {@link Object#toString()} value. Otherwise the object
     *                  isn't used
     */
    public void acquirePrimaryOperationPermit(ActionListener&lt;Releasable&gt; onPermitAcquired, String executorOnDelay, Object debugInfo) {
        verifyNotClosed();
        assert shardRouting.primary() : &quot;acquirePrimaryOperationPermit should only be called on primary shard: &quot; + shardRouting;

        indexShardOperationPermits.acquire(wrapPrimaryOperationPermitListener(onPermitAcquired), executorOnDelay, false, debugInfo);
    }

    /**
     * Acquire all primary operation permits. Once all permits are acquired, the provided ActionListener is called.
     * It is the responsibility of the caller to close the {@link Releasable}.
     */
    public void acquireAllPrimaryOperationsPermits(final ActionListener&lt;Releasable&gt; onPermitAcquired, final TimeValue timeout) {
        verifyNotClosed();
        assert shardRouting.primary() : &quot;acquireAllPrimaryOperationsPermits should only be called on primary shard: &quot; + shardRouting;

        asyncBlockOperations(wrapPrimaryOperationPermitListener(onPermitAcquired), timeout.duration(), timeout.timeUnit());
    }

    /**
     * Wraps the action to run on a primary after acquiring permit. This wrapping is used to check if the shard is in primary mode before
     * executing the action.
     *
     * @param listener the listener to wrap
     * @return the wrapped listener
     */
<A NAME="19"></A>    private ActionListener&lt;Releasable&gt; wrapPrimaryOperationPermitListener(final ActionListener&lt;Releasable&gt; listener) {
        return ActionListener.delegateFailure(
            listener,
            (l, r) -&gt; <FONT color="#f62817"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#19',2,'match2119549-top.html#19',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>{
                if (replicationTracker.isPrimaryMode()) {
                    l.onResponse(r);
                } else {
                    r.close</B></FONT>();
                    l.onFailure(new ShardNotInPrimaryModeException(shardId, state));
                }
            }
        );
    }

    private void asyncBlockOperations(ActionListener&lt;Releasable&gt; onPermitAcquired, long timeout, TimeUnit timeUnit) {
        final Releasable forceRefreshes = refreshListeners.forceRefreshes();
        final ActionListener&lt;Releasable&gt; wrappedListener = ActionListener.wrap(
            r -&gt; {
                forceRefreshes.close();
                onPermitAcquired.onResponse(r);
            },
            e -&gt; {
                forceRefreshes.close();
                onPermitAcquired.onFailure(e);
            }
        );
        try {
            indexShardOperationPermits.asyncBlockOperations(wrappedListener, timeout, timeUnit);
        } catch (Exception e) {
            forceRefreshes.close();
            throw e;
        }
    }

    /**
     * Runs the specified runnable under a permit and otherwise calling back the specified failure callback. This method is really a
     * convenience for {@link #acquirePrimaryOperationPermit(ActionListener, String, Object)} where the listener equates to
     * try-with-resources closing the releasable after executing the runnable on successfully acquiring the permit, an otherwise calling
     * back the failure callback.
     *
     * @param runnable the runnable to execute under permit
     * @param onFailure the callback on failure
     * @param executorOnDelay the executor to execute the runnable on if permit acquisition is blocked
     * @param debugInfo debug info
     */
    public void runUnderPrimaryPermit(
            final Runnable runnable,
            final Consumer&lt;Exception&gt; onFailure,
            final String executorOnDelay,
            final Object debugInfo) {
        verifyNotClosed();
        assert shardRouting.primary() : &quot;runUnderPrimaryPermit should only be called on primary shard but was &quot; + shardRouting;
        final ActionListener&lt;Releasable&gt; onPermitAcquired = ActionListener.wrap(
            releasable -&gt; {
                try (Releasable ignore = releasable) {
                    runnable.run();
                }
            },
            onFailure);
        acquirePrimaryOperationPermit(onPermitAcquired, executorOnDelay, debugInfo);
    }

    private &lt;E extends Exception&gt; void bumpPrimaryTerm(long newPrimaryTerm,
                                                       final CheckedRunnable&lt;E&gt; onBlocked,
                                                       @Nullable ActionListener&lt;Releasable&gt; combineWithAction) {
        assert Thread.holdsLock(mutex);
        assert newPrimaryTerm &gt; pendingPrimaryTerm || (newPrimaryTerm &gt;= pendingPrimaryTerm &amp;&amp; combineWithAction != null);
        assert getOperationPrimaryTerm() &lt;= pendingPrimaryTerm;
        final CountDownLatch termUpdated = new CountDownLatch(1);
        asyncBlockOperations(new ActionListener&lt;Releasable&gt;() {
            @Override
            public void onFailure(final Exception e) {
                try {
                    innerFail(e);
                } finally {
                    if (combineWithAction != null) {
                        combineWithAction.onFailure(e);
                    }
                }
            }

            private void innerFail(final Exception e) {
                try {
                    failShard(&quot;exception during primary term transition&quot;, e);
                } catch (AlreadyClosedException ace) {
                    // ignore, shard is already closed
                }
            }

            @Override
            public void onResponse(final Releasable releasable) {
                final RunOnce releaseOnce = new RunOnce(releasable::close);
                try {
                    assert getOperationPrimaryTerm() &lt;= pendingPrimaryTerm;
                    termUpdated.await();
                    // indexShardOperationPermits doesn't guarantee that async submissions are executed
                    // in the order submitted. We need to guard against another term bump
                    if (getOperationPrimaryTerm() &lt; newPrimaryTerm) {
                        replicationTracker.setOperationPrimaryTerm(newPrimaryTerm);
                        onBlocked.run();
                    }
                } catch (final Exception e) {
                    if (combineWithAction == null) {
                        // otherwise leave it to combineWithAction to release the permit
                        releaseOnce.run();
                    }
                    innerFail(e);
                } finally {
                    if (combineWithAction != null) {
                        combineWithAction.onResponse(releasable);
                    } else {
                        releaseOnce.run();
                    }
                }
            }
        }, 30, TimeUnit.MINUTES);
        pendingPrimaryTerm = newPrimaryTerm;
        termUpdated.countDown();
    }

    /**
     * Acquire a replica operation permit whenever the shard is ready for indexing (see
     * {@link #acquirePrimaryOperationPermit(ActionListener, String, Object)}). If the given primary term is lower than then one in
     * {@link #shardRouting}, the {@link ActionListener#onFailure(Exception)} method of the provided listener is invoked with an
     * {@link IllegalStateException}. If permit acquisition is delayed, the listener will be invoked on the executor with the specified
     * name.
     *
     * @param opPrimaryTerm              the operation primary term
     * @param globalCheckpoint           the global checkpoint associated with the request
     * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates (index operations overwrite Lucene) or deletes captured on the primary
     *                                   after this replication request was executed on it (see {@link #getMaxSeqNoOfUpdatesOrDeletes()}
     * @param onPermitAcquired           the listener for permit acquisition
     * @param executorOnDelay            the name of the executor to invoke the listener on if permit acquisition is delayed
     * @param debugInfo                  an extra information that can be useful when tracing an unreleased permit. When assertions are
     *                                   enabled the tracing will capture the supplied object's {@link Object#toString()} value.
     *                                   Otherwise the object isn't used
     */
    public void acquireReplicaOperationPermit(final long opPrimaryTerm,
                                              final long globalCheckpoint,
                                              final long maxSeqNoOfUpdatesOrDeletes,
                                              final ActionListener&lt;Releasable&gt; onPermitAcquired,
                                              final String executorOnDelay,
                                              final Object debugInfo) {
        innerAcquireReplicaOperationPermit(opPrimaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, onPermitAcquired, false,
            (listener) -&gt; indexShardOperationPermits.acquire(listener, executorOnDelay, true, debugInfo));
    }

    /**
     * Acquire all replica operation permits whenever the shard is ready for indexing (see
     * {@link #acquireAllPrimaryOperationsPermits(ActionListener, TimeValue)}. If the given primary term is lower than then one in
     * {@link #shardRouting}, the {@link ActionListener#onFailure(Exception)} method of the provided listener is invoked with an
     * {@link IllegalStateException}.
     *
     * @param opPrimaryTerm              the operation primary term
     * @param globalCheckpoint           the global checkpoint associated with the request
     * @param maxSeqNoOfUpdatesOrDeletes the max seq_no of updates (index operations overwrite Lucene) or deletes captured on the primary
     *                                   after this replication request was executed on it (see {@link #getMaxSeqNoOfUpdatesOrDeletes()}
     * @param onPermitAcquired           the listener for permit acquisition
     * @param timeout                    the maximum time to wait for the in-flight operations block
     */
    public void acquireAllReplicaOperationsPermits(final long opPrimaryTerm,
                                                   final long globalCheckpoint,
                                                   final long maxSeqNoOfUpdatesOrDeletes,
                                                   final ActionListener&lt;Releasable&gt; onPermitAcquired,
                                                   final TimeValue timeout) {
        innerAcquireReplicaOperationPermit(
            opPrimaryTerm,
            globalCheckpoint,
            maxSeqNoOfUpdatesOrDeletes,
            onPermitAcquired,
            true,
            (listener) -&gt; asyncBlockOperations(listener, timeout.duration(), timeout.timeUnit())
        );
    }

    private void innerAcquireReplicaOperationPermit(final long opPrimaryTerm,
                                                    final long globalCheckpoint,
                                                    final long maxSeqNoOfUpdatesOrDeletes,
                                                    final ActionListener&lt;Releasable&gt; onPermitAcquired,
                                                    final boolean allowCombineOperationWithPrimaryTermUpdate,
                                                    final Consumer&lt;ActionListener&lt;Releasable&gt;&gt; operationExecutor) {
        verifyNotClosed();
        // This listener is used for the execution of the operation. If the operation requires all the permits for its
        // execution and the primary term must be updated first, we can combine the operation execution with the
        // primary term update. Since indexShardOperationPermits doesn't guarantee that async submissions are executed
        // in the order submitted, combining both operations ensure that the term is updated before the operation is
        // executed. It also has the side effect of acquiring all the permits one time instead of two.
        final ActionListener&lt;Releasable&gt; operationListener = ActionListener.delegateFailure(
            onPermitAcquired,
            (delegatedListener, releasable) -&gt; {
                if (opPrimaryTerm &lt; getOperationPrimaryTerm()) {
                    releasable.close();
                    final String message = String.format(
                        Locale.ROOT,
                        &quot;%s operation primary term [%d] is too old (current [%d])&quot;,
                        shardId,
                        opPrimaryTerm,
                        getOperationPrimaryTerm());
                    delegatedListener.onFailure(new IllegalStateException(message));
                } else {
                    assert assertReplicationTarget();
                    try {
                        updateGlobalCheckpointOnReplica(globalCheckpoint, &quot;operation&quot;);
                        advanceMaxSeqNoOfUpdatesOrDeletes(maxSeqNoOfUpdatesOrDeletes);
                    } catch (Exception e) {
                        releasable.close();
                        delegatedListener.onFailure(e);
                        return;
                    }
                    delegatedListener.onResponse(releasable);
                }
            }
        );
        if (requirePrimaryTermUpdate(opPrimaryTerm, allowCombineOperationWithPrimaryTermUpdate)) {
            synchronized (mutex) {
                if (requirePrimaryTermUpdate(opPrimaryTerm, allowCombineOperationWithPrimaryTermUpdate)) {
                    final IndexShardState shardState = state();
                    // only roll translog and update primary term if shard has made it past recovery
                    // Having a new primary term here means that the old primary failed and that there is a new primary, which again
                    // means that the master will fail this shard as all initializing shards are failed when a primary is selected
                    // We abort early here to prevent an ongoing recovery from the failed primary to mess with the global / local checkpoint
                    if (shardState != IndexShardState.POST_RECOVERY &amp;&amp;
                        shardState != IndexShardState.STARTED) {
                        throw new IndexShardNotStartedException(shardId, shardState);
                    }

<A NAME="18"></A>                    bumpPrimaryTerm(opPrimaryTerm, () -&gt; {
                        updateGlobalCheckpointOnReplica(globalCheckpoint, &quot;primary term transition&quot;);
                        final long currentGlobalCheckpoint = getLastKnownGlobalCheckpoint();
                        final long maxSeqNo = <FONT color="#800517"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match2119549-0.html#18',2,'match2119549-top.html#18',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>seqNoStats().getMaxSeqNo();
                        logger.info(&quot;detected new primary with primary term [{}], global checkpoint [{}], max_seq_no [{}]&quot;,
                            opPrimaryTerm, currentGlobalCheckpoint, maxSeqNo);
                        if (currentGlobalCheckpoint &lt; maxSeqNo) {
                            resetEngineToGlobalCheckpoint();
                        } else {
                            getEngine().rollTranslogGeneration</B></FONT>();
                        }
                    }, allowCombineOperationWithPrimaryTermUpdate ? operationListener : null);

                    if (allowCombineOperationWithPrimaryTermUpdate) {
                        logger.debug(&quot;operation execution has been combined with primary term update&quot;);
                        return;
                    }
                }
            }
        }
        assert opPrimaryTerm &lt;= pendingPrimaryTerm
            : &quot;operation primary term [&quot; + opPrimaryTerm + &quot;] should be at most [&quot; + pendingPrimaryTerm + &quot;]&quot;;
        operationExecutor.accept(operationListener);
    }

    private boolean requirePrimaryTermUpdate(final long opPrimaryTerm, final boolean allPermits) {
        return (opPrimaryTerm &gt; pendingPrimaryTerm) || (allPermits &amp;&amp; opPrimaryTerm &gt; getOperationPrimaryTerm());
    }

    public static final int OPERATIONS_BLOCKED = -1;

    /**
     * Obtain the active operation count, or {@link IndexShard#OPERATIONS_BLOCKED} if all permits are held (even if there are
     * outstanding operations in flight).
     *
     * @return the active operation count, or {@link IndexShard#OPERATIONS_BLOCKED} when all permits are held.
     */
    public int getActiveOperationsCount() {
        return indexShardOperationPermits.getActiveOperationsCount();
    }

    /**
     * @return a list of describing each permit that wasn't released yet. The description consist of the debugInfo supplied
     *         when the permit was acquired plus a stack traces that was captured when the permit was request.
     */
    public List&lt;String&gt; getActiveOperations() {
        return indexShardOperationPermits.getActiveOperations();
    }

    private final AsyncIOProcessor&lt;Translog.Location&gt; translogSyncProcessor = new AsyncIOProcessor&lt;Translog.Location&gt;(logger, 1024) {
        @Override
        protected void write(List&lt;Tuple&lt;Translog.Location, Consumer&lt;Exception&gt;&gt;&gt; candidates) throws IOException {
            try {
                getEngine().ensureTranslogSynced(candidates.stream().map(Tuple::v1));
            } catch (AlreadyClosedException ex) {
                // that's fine since we already synced everything on engine close - this also is conform with the methods
                // documentation
            } catch (IOException ex) { // if this fails we are in deep shit - fail the request
                logger.debug(&quot;failed to sync translog&quot;, ex);
                throw ex;
            }
        }
    };

    /**
     * Syncs the given location with the underlying storage unless already synced. This method might return immediately without
     * actually fsyncing the location until the sync listener is called. Yet, unless there is already another thread fsyncing
     * the transaction log the caller thread will be hijacked to run the fsync for all pending fsync operations.
     * This method allows indexing threads to continue indexing without blocking on fsync calls. We ensure that there is only
     * one thread blocking on the sync an all others can continue indexing.
     * NOTE: if the syncListener throws an exception when it's processed the exception will only be logged. Users should make sure that the
     * listener handles all exception cases internally.
     */
    public final void sync(Translog.Location location, Consumer&lt;Exception&gt; syncListener) {
        verifyNotClosed();
        translogSyncProcessor.put(location, syncListener);
    }

    public void sync() throws IOException {
        verifyNotClosed();
        getEngine().syncTranslog();
    }

    /**
     * Checks if the underlying storage sync is required.
     */
    public boolean isSyncNeeded() {
        return getEngine().isTranslogSyncNeeded();
    }

    /**
     * Returns the current translog durability mode
     */
    public Translog.Durability getTranslogDurability() {
        return indexSettings.getTranslogDurability();
    }

    // we can not protect with a lock since we &quot;release&quot; on a different thread
    private final AtomicBoolean flushOrRollRunning = new AtomicBoolean();

    /**
     * Schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. The operation will be
     * executed asynchronously on the flush thread pool.
     */
    public void afterWriteOperation() {
        if (shouldPeriodicallyFlush() || shouldRollTranslogGeneration()) {
            if (flushOrRollRunning.compareAndSet(false, true)) {
                /*
                 * We have to check again since otherwise there is a race when a thread passes the first check next to another thread which
                 * performs the operation quickly enough to  finish before the current thread could flip the flag. In that situation, we
                 * have an extra operation.
                 *
                 * Additionally, a flush implicitly executes a translog generation roll so if we execute a flush then we do not need to
                 * check if we should roll the translog generation.
                 */
                if (shouldPeriodicallyFlush()) {
                    logger.debug(&quot;submitting async flush request&quot;);
                    final AbstractRunnable flush = new AbstractRunnable() {
                        @Override
                        public void onFailure(final Exception e) {
                            if (state != IndexShardState.CLOSED) {
                                logger.warn(&quot;failed to flush index&quot;, e);
                            }
                        }

                        @Override
                        protected void doRun() throws IOException {
                            flush(new FlushRequest());
                            periodicFlushMetric.inc();
                        }

                        @Override
                        public void onAfter() {
                            flushOrRollRunning.compareAndSet(true, false);
                            afterWriteOperation();
                        }
                    };
                    threadPool.executor(ThreadPool.Names.FLUSH).execute(flush);
                } else if (shouldRollTranslogGeneration()) {
                    logger.debug(&quot;submitting async roll translog generation request&quot;);
                    final AbstractRunnable roll = new AbstractRunnable() {
                        @Override
                        public void onFailure(final Exception e) {
                            if (state != IndexShardState.CLOSED) {
                                logger.warn(&quot;failed to roll translog generation&quot;, e);
                            }
                        }

                        @Override
                        protected void doRun() throws Exception {
                            rollTranslogGeneration();
                        }

                        @Override
                        public void onAfter() {
                            flushOrRollRunning.compareAndSet(true, false);
                            afterWriteOperation();
                        }
                    };
                    threadPool.executor(ThreadPool.Names.FLUSH).execute(roll);
                } else {
                    flushOrRollRunning.compareAndSet(true, false);
                }
            }
        }
    }

    /**
     * Build {@linkplain RefreshListeners} for this shard.
     */
    private RefreshListeners buildRefreshListeners() {
        return new RefreshListeners(
            indexSettings::getMaxRefreshListeners,
            () -&gt; refresh(&quot;too_many_listeners&quot;),
            threadPool.executor(ThreadPool.Names.LISTENER)::execute,
            logger
        );
    }

    /**
     * Simple struct encapsulating a shard failure
     *
     * @see IndexShard#addShardFailureCallback(Consumer)
     */
    public static final class ShardFailure {
        public final ShardRouting routing;
        public final String reason;
        @Nullable
        public final Exception cause;

        public ShardFailure(ShardRouting routing, String reason, @Nullable Exception cause) {
            this.routing = routing;
            this.reason = reason;
            this.cause = cause;
        }
    }

    EngineFactory getEngineFactory() {
        return engineFactory;
    }

    // for tests
    ReplicationTracker getReplicationTracker() {
        return replicationTracker;
    }

    /**
     * Executes a scheduled refresh if necessary.
     *
     * @return &lt;code&gt;true&lt;/code&gt; iff the engine got refreshed otherwise &lt;code&gt;false&lt;/code&gt;
     */
    public boolean scheduledRefresh() {
        verifyNotClosed();
        boolean listenerNeedsRefresh = refreshListeners.refreshNeeded();
        if (isReadAllowed() &amp;&amp; (listenerNeedsRefresh || getEngine().refreshNeeded())) {
            if (listenerNeedsRefresh == false // if we have a listener that is waiting for a refresh we need to force it
                &amp;&amp; isSearchIdle()
                &amp;&amp; indexSettings.isExplicitRefresh() == false
                &amp;&amp; active.get()) { // it must be active otherwise we might not free up segment memory once the shard became inactive
                // lets skip this refresh since we are search idle and
                // don't necessarily need to refresh. the next searcher access will register a refreshListener and that will
                // cause the next schedule to refresh.
                final Engine engine = getEngine();
                engine.maybePruneDeletes(); // try to prune the deletes in the engine if we accumulated some
                setRefreshPending(engine);
                return false;
            } else {
                if (logger.isTraceEnabled()) {
                    logger.trace(&quot;refresh with source [schedule]&quot;);
                }
                return getEngine().maybeRefresh(&quot;schedule&quot;);
            }
        }
        final Engine engine = getEngine();
        engine.maybePruneDeletes(); // try to prune the deletes in the engine if we accumulated some
        return false;
    }

    /**
     * Returns true if this shards is search idle
     */
    public final boolean isSearchIdle() {
        return (threadPool.relativeTimeInMillis() - lastSearcherAccess.get()) &gt;= indexSettings.getSearchIdleAfter().getMillis();
    }

    /**
     * Returns the last timestamp the searcher was accessed. This is a relative timestamp in milliseconds.
     */
    final long getLastSearcherAccess() {
        return lastSearcherAccess.get();
    }

    /**
     * Returns true if this shard has some scheduled refresh that is pending because of search-idle.
     */
    public final boolean hasRefreshPending() {
        return pendingRefreshLocation.get() != null;
    }

    private void setRefreshPending(Engine engine) {
        final Translog.Location lastWriteLocation = engine.getTranslogLastWriteLocation();
        pendingRefreshLocation.updateAndGet(curr -&gt; {
            if (curr == null || curr.compareTo(lastWriteLocation) &lt;= 0) {
                return lastWriteLocation;
            } else {
                return curr;
            }
        });
    }

    private class RefreshPendingLocationListener implements ReferenceManager.RefreshListener {
        Translog.Location lastWriteLocation;

        @Override
        public void beforeRefresh() {
            try {
                lastWriteLocation = getEngine().getTranslogLastWriteLocation();
            } catch (AlreadyClosedException exc) {
                // shard is closed - no location is fine
                lastWriteLocation = null;
            }
        }

        @Override
        public void afterRefresh(boolean didRefresh) {
            if (didRefresh &amp;&amp; lastWriteLocation != null) {
                pendingRefreshLocation.updateAndGet(pendingLocation -&gt; {
                    if (pendingLocation == null || pendingLocation.compareTo(lastWriteLocation) &lt;= 0) {
                        return null;
                    } else {
                        return pendingLocation;
                    }
                });
            }
        }
    }

    /**
     * Registers the given listener and invokes it once the shard is active again and all
     * pending refresh translog location has been refreshed. If there is no pending refresh location registered the listener will be
     * invoked immediately.
     * @param listener the listener to invoke once the pending refresh location is visible. The listener will be called with
     *                 &lt;code&gt;true&lt;/code&gt; if the listener was registered to wait for a refresh.
     */
    public final void awaitShardSearchActive(Consumer&lt;Boolean&gt; listener) {
        markSearcherAccessed(); // move the shard into non-search idle
        final Translog.Location location = pendingRefreshLocation.get();
        if (location != null) {
            addRefreshListener(location, (b) -&gt; {
                pendingRefreshLocation.compareAndSet(location, null);
                listener.accept(true);
            });
        } else {
            listener.accept(false);
        }
    }

    /**
     * Add a listener for refreshes.
     *
     * @param location the location to listen for
     * @param listener for the refresh. Called with true if registering the listener ran it out of slots and forced a refresh. Called with
     *        false otherwise.
     */
    public void addRefreshListener(Translog.Location location, Consumer&lt;Boolean&gt; listener) {
        final boolean readAllowed;
        if (isReadAllowed()) {
            readAllowed = true;
        } else {
            // check again under postRecoveryMutex. this is important to create a happens before relationship
            // between the switch to POST_RECOVERY + associated refresh. Otherwise we may respond
            // to a listener before a refresh actually happened that contained that operation.
            synchronized (postRecoveryMutex) {
                readAllowed = isReadAllowed();
            }
        }
        if (readAllowed) {
            refreshListeners.addOrNotify(location, listener);
        } else {
            // we're not yet ready fo ready for reads, just ignore refresh cycles
            listener.accept(false);
        }
    }

    private static class RefreshMetricUpdater implements ReferenceManager.RefreshListener {

        private final MeanMetric refreshMetric;
        private long currentRefreshStartTime;
        private Thread callingThread = null;

        private RefreshMetricUpdater(MeanMetric refreshMetric) {
            this.refreshMetric = refreshMetric;
        }

        @Override
        public void beforeRefresh() throws IOException {
            if (Assertions.ENABLED) {
                assert callingThread == null : &quot;beforeRefresh was called by &quot; + callingThread.getName() +
                    &quot; without a corresponding call to afterRefresh&quot;;
                callingThread = Thread.currentThread();
            }
            currentRefreshStartTime = System.nanoTime();
        }

        @Override
        public void afterRefresh(boolean didRefresh) throws IOException {
            if (Assertions.ENABLED) {
                assert callingThread != null : &quot;afterRefresh called but not beforeRefresh&quot;;
                assert callingThread == Thread.currentThread() : &quot;beforeRefreshed called by a different thread. current [&quot;
                    + Thread.currentThread().getName() + &quot;], thread that called beforeRefresh [&quot; + callingThread.getName() + &quot;]&quot;;
                callingThread = null;
            }
            refreshMetric.inc(System.nanoTime() - currentRefreshStartTime);
        }
    }

    private EngineConfig.TombstoneDocSupplier tombstoneDocSupplier() {
        final RootObjectMapper.Builder noopRootMapper = new RootObjectMapper.Builder(&quot;default&quot;);
        final DocumentMapper noopDocumentMapper = mapperService == null
            ? null
            : new DocumentMapper.Builder(noopRootMapper, mapperService).build(mapperService);
        return new EngineConfig.TombstoneDocSupplier() {

            @Override
            public ParsedDocument newDeleteTombstoneDoc(String id) {
                return mapperService.documentMapper().createDeleteTombstoneDoc(shardId.getIndexName(), id);
            }

            @Override
            public ParsedDocument newNoopTombstoneDoc(String reason) {
                return noopDocumentMapper.createNoopTombstoneDoc(shardId.getIndexName(), reason);
            }
        };
    }

    /**
     * Rollback the current engine to the safe commit, then replay local translog up to the global checkpoint.
     */
    void resetEngineToGlobalCheckpoint() throws IOException {
        assert Thread.holdsLock(mutex) == false : &quot;resetting engine under mutex&quot;;
        assert getActiveOperationsCount() == OPERATIONS_BLOCKED
            : &quot;resetting engine without blocking operations; active operations are [&quot; + getActiveOperations() + ']';
        sync(); // persist the global checkpoint to disk
        final SeqNoStats seqNoStats = seqNoStats();
        final TranslogStats translogStats = translogStats();
        // flush to make sure the latest commit, which will be opened by the read-only engine, includes all operations.
        flush(new FlushRequest().waitIfOngoing(true));

        SetOnce&lt;Engine&gt; newEngineReference = new SetOnce&lt;&gt;();
        final long globalCheckpoint = getLastKnownGlobalCheckpoint();
        assert globalCheckpoint == getLastSyncedGlobalCheckpoint();
        synchronized (engineMutex) {
            verifyNotClosed();
            // we must create both new read-only engine and new read-write engine under engineMutex to ensure snapshotStoreMetadata,
            // acquireXXXCommit and close works.
            final Engine readOnlyEngine =
                new ReadOnlyEngine(newEngineConfig(replicationTracker), seqNoStats, translogStats, false, Function.identity()) {
                    @Override
                    public IndexCommitRef acquireLastIndexCommit(boolean flushFirst) {
                        synchronized (engineMutex) {
                            if (newEngineReference.get() == null) {
                                throw new AlreadyClosedException(&quot;engine was closed&quot;);
                            }
                            // ignore flushFirst since we flushed above and we do not want to interfere with ongoing translog replay
                            return newEngineReference.get().acquireLastIndexCommit(false);
                        }
                    }

                    @Override
                    public IndexCommitRef acquireSafeIndexCommit() {
                        synchronized (engineMutex) {
                            if (newEngineReference.get() == null) {
                                throw new AlreadyClosedException(&quot;engine was closed&quot;);
                            }
                            return newEngineReference.get().acquireSafeIndexCommit();
                        }
                    }

                    @Override
                    public void close() throws IOException {
                        assert Thread.holdsLock(engineMutex);

                        Engine newEngine = newEngineReference.get();
                        if (newEngine == currentEngineReference.get()) {
                            // we successfully installed the new engine so do not close it.
                            newEngine = null;
                        }
                        IOUtils.close(super::close, newEngine);
                    }
                };
            IOUtils.close(currentEngineReference.getAndSet(readOnlyEngine));
            newEngineReference.set(engineFactory.newReadWriteEngine(newEngineConfig(replicationTracker)));
            onNewEngine(newEngineReference.get());
        }
        final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -&gt; runTranslogRecovery(
            engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -&gt; {
                // TODO: add a dedicate recovery stats for the reset translog
            });
        newEngineReference.get().recoverFromTranslog(translogRunner, globalCheckpoint);
        newEngineReference.get().refresh(&quot;reset_engine&quot;);
        synchronized (engineMutex) {
            verifyNotClosed();
            IOUtils.close(currentEngineReference.getAndSet(newEngineReference.get()));
            // We set active because we are now writing operations to the engine; this way,
            // if we go idle after some time and become inactive, we still give sync'd flush a chance to run.
            active.set(true);
        }
        // time elapses after the engine is created above (pulling the config settings) until we set the engine reference, during
        // which settings changes could possibly have happened, so here we forcefully push any config changes to the new engine.
        onSettingsChanged();
    }

    /**
     * Returns the maximum sequence number of either update or delete operations have been processed in this shard
     * or the sequence number from {@link #advanceMaxSeqNoOfUpdatesOrDeletes(long)}. An index request is considered
     * as an update operation if it overwrites the existing documents in Lucene index with the same document id.
     * &lt;p&gt;
     * The primary captures this value after executes a replication request, then transfers it to a replica before
     * executing that replication request on a replica.
     */
    public long getMaxSeqNoOfUpdatesOrDeletes() {
        return getEngine().getMaxSeqNoOfUpdatesOrDeletes();
    }

    /**
     * A replica calls this method to advance the max_seq_no_of_updates marker of its engine to at least the max_seq_no_of_updates
     * value (piggybacked in a replication request) that it receives from its primary before executing that replication request.
     * The receiving value is at least as high as the max_seq_no_of_updates on the primary was when any of the operations of that
     * replication request were processed on it.
     * &lt;p&gt;
     * A replica shard also calls this method to bootstrap the max_seq_no_of_updates marker with the value that it received from
     * the primary in peer-recovery, before it replays remote translog operations from the primary. The receiving value is at least
     * as high as the max_seq_no_of_updates on the primary was when any of these operations were processed on it.
     * &lt;p&gt;
     * These transfers guarantee that every index/delete operation when executing on a replica engine will observe this marker a value
     * which is at least the value of the max_seq_no_of_updates marker on the primary after that operation was executed on the primary.
     *
     * @see #acquireReplicaOperationPermit(long, long, long, ActionListener, String, Object)
     * @see RecoveryTarget#indexTranslogOperations(List, int, long, long, RetentionLeases, long, ActionListener)
     */
    public void advanceMaxSeqNoOfUpdatesOrDeletes(long seqNo) {
        getEngine().advanceMaxSeqNoOfUpdatesOrDeletes(seqNo);
    }

    /**
     * Performs the pre-closing checks on the {@link IndexShard}.
     *
     * @throws IllegalStateException if the sanity checks failed
     */
    public void verifyShardBeforeIndexClosing() throws IllegalStateException {
        getEngine().verifyEngineBeforeIndexClosing();
    }

    public MeanMetric getFlushMetric() {
        return flushMetric;
    }

    public long periodicFlushCount() {
        return periodicFlushMetric.count();
    }
}
</PRE>
</div>
  </div>
</body>
</html>
