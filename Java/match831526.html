<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for MetadataToASTNodeResolverTest.java & RetentionLeaseIT.java</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for MetadataToASTNodeResolverTest.java & RetentionLeaseIT.java
      </h3>
      <h1 align="center">
        17.7%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>MetadataToASTNodeResolverTest.java (64.28571%)<TH>RetentionLeaseIT.java (10.29552%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#0',2,'match831526-1.html#0',3)" NAME="0">(24-37)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#0',2,'match831526-1.html#0',3)" NAME="0">(56-69)</A><TD ALIGN=center><FONT COLOR="#ff0000">12</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#1',2,'match831526-1.html#1',3)" NAME="1">(38-71)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#1',2,'match831526-1.html#1',3)" NAME="1">(188-194)</A><TD ALIGN=center><FONT COLOR="#e90000">11</FONT>
<TR><TD BGCOLOR="#980517"><FONT COLOR="#980517">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#2',2,'match831526-1.html#2',3)" NAME="2">(271-288)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#2',2,'match831526-1.html#2',3)" NAME="2">(564-572)</A><TD ALIGN=center><FONT COLOR="#d40000">10</FONT>
<TR><TD BGCOLOR="#53858b"><FONT COLOR="#53858b">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#3',2,'match831526-1.html#3',3)" NAME="3">(223-238)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#3',2,'match831526-1.html#3',3)" NAME="3">(518-526)</A><TD ALIGN=center><FONT COLOR="#d40000">10</FONT>
<TR><TD BGCOLOR="#6cc417"><FONT COLOR="#6cc417">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#4',2,'match831526-1.html#4',3)" NAME="4">(171-188)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#4',2,'match831526-1.html#4',3)" NAME="4">(404-412)</A><TD ALIGN=center><FONT COLOR="#d40000">10</FONT>
<TR><TD BGCOLOR="#151b8d"><FONT COLOR="#151b8d">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#5',2,'match831526-1.html#5',3)" NAME="5">(120-125)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#5',2,'match831526-1.html#5',3)" NAME="5">(74-80)</A><TD ALIGN=center><FONT COLOR="#d40000">10</FONT>
<TR><TD BGCOLOR="#8c8774"><FONT COLOR="#8c8774">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#6',2,'match831526-1.html#6',3)" NAME="6">(506-511)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#6',2,'match831526-1.html#6',3)" NAME="6">(145-148)</A><TD ALIGN=center><FONT COLOR="#bf0000">9</FONT>
<TR><TD BGCOLOR="#38a4a5"><FONT COLOR="#38a4a5">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#7',2,'match831526-1.html#7',3)" NAME="7">(496-501)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#7',2,'match831526-1.html#7',3)" NAME="7">(95-98)</A><TD ALIGN=center><FONT COLOR="#bf0000">9</FONT>
<TR><TD BGCOLOR="#c58917"><FONT COLOR="#c58917">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#8',2,'match831526-1.html#8',3)" NAME="8">(446-450)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#8',2,'match831526-1.html#8',3)" NAME="8">(321-326)</A><TD ALIGN=center><FONT COLOR="#bf0000">9</FONT>
<TR><TD BGCOLOR="#83a33a"><FONT COLOR="#83a33a">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#9',2,'match831526-1.html#9',3)" NAME="9">(398-402)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#9',2,'match831526-1.html#9',3)" NAME="9">(263-268)</A><TD ALIGN=center><FONT COLOR="#bf0000">9</FONT>
<TR><TD BGCOLOR="#ad5910"><FONT COLOR="#ad5910">-</FONT><TD><A HREF="javascript:ZweiFrames('match831526-0.html#10',2,'match831526-1.html#10',3)" NAME="10">(322-327)<TD><A HREF="javascript:ZweiFrames('match831526-0.html#10',2,'match831526-1.html#10',3)" NAME="10">(127-132)</A><TD ALIGN=center><FONT COLOR="#bf0000">9</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>MetadataToASTNodeResolverTest.java</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
/*
 * Licensed to Crate.io GmbH (&quot;Crate&quot;) under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */
<A NAME="0"></A>
package io.crate.analyze;

<FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match831526-1.html#0',3,'match831526-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>import io.crate.metadata.doc.DocTableInfo;
import io.crate.sql.SqlFormatter;
import io.crate.sql.tree.CreateTable;
import io.crate.test.integration.CrateDummyClusterServiceUnitTest;
import io.crate.testing.SQLExecutor;

import org.hamcrest.Matchers;
import org.junit.Test;

public class MetadataToASTNodeResolverTest extends CrateDummyClusterServiceUnitTest {

<A NAME="1"></A>    @Override
    protected boolean enableWarningsCheck() {
        return</B></FONT> false;
    <FONT color="#f63526"><A HREF="javascript:ZweiFrames('match831526-1.html#1',3,'match831526-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    @Test
    public void testBuildCreateTableColumns() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable(&quot;create table doc.test (&quot; +
                      &quot; bools boolean,&quot; +
                      &quot; bytes byte,&quot; +
                      &quot; strings string,&quot; +
                      &quot; shorts short,&quot; +
                      &quot; floats float,&quot; +
                      &quot; doubles double,&quot; +
                      &quot; ints integer,&quot; +
                      &quot; longs long,&quot; +
                      &quot; timestamp timestamp with time zone,&quot; +
                      &quot; ip_addr ip,&quot; +
                      &quot; arr_simple array(string),&quot; +
                      &quot; arr_geo_point array(geo_point),&quot; +
                      &quot; arr_obj array(object(strict) as (&quot; +
                      &quot;  col_1 long,&quot; +
                      &quot;  col_2 string&quot; +
                      &quot; )),&quot; +
                      &quot; obj object as (&quot; +
                      &quot;  col_1 long,&quot; +
                      &quot;  col_2 string&quot; +
                      &quot; )&quot; +
                      &quot;) &quot; +
                      &quot;clustered into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all',&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo</B></FONT>(&quot;doc.test&quot;);

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;doc\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;bools\&quot; BOOLEAN,\n&quot; +
                     &quot;   \&quot;bytes\&quot; CHAR,\n&quot; +
                     &quot;   \&quot;strings\&quot; TEXT,\n&quot; +
                     &quot;   \&quot;shorts\&quot; SMALLINT,\n&quot; +
                     &quot;   \&quot;floats\&quot; REAL,\n&quot; +
                     &quot;   \&quot;doubles\&quot; DOUBLE PRECISION,\n&quot; +
                     &quot;   \&quot;ints\&quot; INTEGER,\n&quot; +
                     &quot;   \&quot;longs\&quot; BIGINT,\n&quot; +
                     &quot;   \&quot;timestamp\&quot; TIMESTAMP WITH TIME ZONE,\n&quot; +
                     &quot;   \&quot;ip_addr\&quot; IP,\n&quot; +
                     &quot;   \&quot;arr_simple\&quot; ARRAY(TEXT),\n&quot; +
                     &quot;   \&quot;arr_geo_point\&quot; ARRAY(GEO_POINT),\n&quot; +
                     &quot;   \&quot;arr_obj\&quot; ARRAY(OBJECT(STRICT) AS (\n&quot; +
                     &quot;      \&quot;col_1\&quot; BIGINT,\n&quot; +
                     &quot;      \&quot;col_2\&quot; TEXT\n&quot; +
                     &quot;   )),\n&quot; +
                     &quot;   \&quot;obj\&quot; OBJECT(DYNAMIC) AS (\n&quot; +
                     &quot;      \&quot;col_1\&quot; BIGINT,\n&quot; +
                     &quot;      \&quot;col_2\&quot; TEXT\n&quot; +
                     &quot;   )\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED INTO 5 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
<A NAME="5"></A>                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
            <FONT color="#151b8d"><A HREF="javascript:ZweiFrames('match831526-1.html#5',3,'match831526-top.html#5',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>SqlFormatter.formatSql(node));
    }

    @Test
    public void testBuildCreateTablePrimaryKey() throws Exception {
        SQLExecutor e = SQLExecutor.builder</B></FONT>(clusterService)
            .addTable(&quot;create table myschema.test (&quot; +
                      &quot; pk_col_one long,&quot; +
                      &quot; pk_col_two long,&quot; +
                      &quot; primary key (pk_col_one, pk_col_two)&quot; +
                      &quot;) &quot; +
                      &quot;clustered into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all',&quot; +
                      &quot; column_policy = 'strict',&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;myschema.test&quot;);

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;myschema\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;pk_col_one\&quot; BIGINT,\n&quot; +
                     &quot;   \&quot;pk_col_two\&quot; BIGINT,\n&quot; +
                     &quot;   PRIMARY KEY (\&quot;pk_col_one\&quot;, \&quot;pk_col_two\&quot;)\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED INTO 5 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
<A NAME="4"></A>                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
            SqlFormatter.formatSql(node));
    <FONT color="#6cc417"><A HREF="javascript:ZweiFrames('match831526-1.html#4',3,'match831526-top.html#4',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    @Test
    public void testBuildCreateTableNotNull() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable(&quot;create table myschema.test (&quot; +
                      &quot; col_a string,&quot; +
                      &quot; col_b string not null index using fulltext,&quot; +
                      &quot; primary key (col_a)&quot; +
                      &quot;) &quot; +
                      &quot;clustered into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all',&quot; +
                      &quot; column_policy = 'strict',&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;myschema.test&quot;)</B></FONT>;

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;myschema\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;col_a\&quot; TEXT,\n&quot; +
                     &quot;   \&quot;col_b\&quot; TEXT NOT NULL INDEX USING FULLTEXT WITH (\n&quot; +
                     &quot;      analyzer = 'standard'\n&quot; +
                     &quot;   ),\n&quot; +
                     &quot;   PRIMARY KEY (\&quot;col_a\&quot;)\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED BY (\&quot;col_a\&quot;) INTO 5 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
<A NAME="3"></A>                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
            SqlFormatter.formatSql(node));
    <FONT color="#53858b"><A HREF="javascript:ZweiFrames('match831526-1.html#3',3,'match831526-top.html#3',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    @Test
    public void testBuildCreateTableCheckConstraints() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable(&quot;create table doc.test (&quot; +
                      &quot; floats float constraint test_floats_check check (floats != -1),&quot; +
                      &quot; shorts short,&quot; +
                      &quot; constraint test_shorts_check check (shorts &gt;= 0)&quot; +
                      &quot;) &quot; +
                      &quot;clustered into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all'&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;doc.test&quot;)</B></FONT>;

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;doc\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;floats\&quot; REAL,\n&quot; +
                     &quot;   \&quot;shorts\&quot; SMALLINT,\n&quot; +
                     &quot;   CONSTRAINT test_floats_check CHECK(\&quot;floats\&quot; &lt;&gt; - 1),\n&quot; +
                     &quot;   CONSTRAINT test_shorts_check CHECK(\&quot;shorts\&quot; &gt;= 0)\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED INTO 5 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
<A NAME="2"></A>                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
                     SqlFormatter.formatSql(node));
    <FONT color="#980517"><A HREF="javascript:ZweiFrames('match831526-1.html#2',3,'match831526-top.html#2',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    @Test
    public void testBuildCreateTableClusteredByPartitionedBy() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addPartitionedTable(&quot;create table myschema.test (&quot; +
                      &quot; id long,&quot; +
                      &quot; partition_column string,&quot; +
                      &quot; cluster_column string&quot; +
                      &quot;) &quot; +
                      &quot;partitioned by (partition_column) &quot; +
                      &quot;clustered by (cluster_column) into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all',&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;myschema.test&quot;)</B></FONT>;

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;myschema\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;id\&quot; BIGINT,\n&quot; +
                     &quot;   \&quot;partition_column\&quot; TEXT,\n&quot; +
                     &quot;   \&quot;cluster_column\&quot; TEXT\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED BY (\&quot;cluster_column\&quot;) INTO 5 SHARDS\n&quot; +
                     &quot;PARTITIONED BY (\&quot;partition_column\&quot;)\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
<A NAME="10"></A>                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
            SqlFormatter.formatSql(node));
    <FONT color="#ad5910"><A HREF="javascript:ZweiFrames('match831526-1.html#10',3,'match831526-top.html#10',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}


    @Test
    public void testBuildCreateTableIndexes() throws Exception {
        SQLExecutor e = SQLExecutor.builder</B></FONT>(clusterService)
            .addTable(&quot;create table myschema.test (&quot; +
                      &quot; id long,&quot; +
                      &quot; col_a string,&quot; +
                      &quot; col_b string index using fulltext,&quot; +
                      &quot; col_c string index off,&quot; +
                      &quot; col_d object as (&quot; +
                      &quot;  a string&quot; +
                      &quot; ),&quot; +
                      &quot; index col_a_col_b_ft using fulltext (col_a, col_b) with (&quot; +
                      &quot;  analyzer= 'english'&quot; +
                      &quot; ),&quot; +
                      &quot; index col_d_a_ft using fulltext (col_d['a']) with (&quot; +
                      &quot;  analyzer= 'custom_analyzer'&quot; +
                      &quot; ),&quot; +
                      &quot; index col_a_col_b_plain using plain (col_a, col_b)&quot; +
                      &quot;) &quot; +
                      &quot;clustered into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all',&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;myschema.test&quot;);

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;myschema\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;id\&quot; BIGINT,\n&quot; +
                     &quot;   \&quot;col_a\&quot; TEXT,\n&quot; +
                     &quot;   \&quot;col_b\&quot; TEXT INDEX USING FULLTEXT WITH (\n&quot; +
                     &quot;      analyzer = 'standard'\n&quot; +
                     &quot;   ),\n&quot; +
                     &quot;   \&quot;col_c\&quot; TEXT INDEX OFF,\n&quot; +
                     &quot;   \&quot;col_d\&quot; OBJECT(DYNAMIC) AS (\n&quot; +
                     &quot;      \&quot;a\&quot; TEXT\n&quot; +
                     &quot;   ),\n&quot; +
                     &quot;   INDEX \&quot;col_a_col_b_ft\&quot; USING FULLTEXT (\&quot;col_a\&quot;, \&quot;col_b\&quot;) WITH (\n&quot; +
                     &quot;      analyzer = 'english'\n&quot; +
                     &quot;   ),\n&quot; +
                     &quot;   INDEX \&quot;col_d_a_ft\&quot; USING FULLTEXT (\&quot;col_d\&quot;['a']) WITH (\n&quot; +
                     &quot;      analyzer = 'custom_analyzer'\n&quot; +
                     &quot;   ),\n&quot; +
                     &quot;   INDEX \&quot;col_a_col_b_plain\&quot; USING FULLTEXT (\&quot;col_a\&quot;, \&quot;col_b\&quot;) WITH (\n&quot; +
                     &quot;      analyzer = 'keyword'\n&quot; +
                     &quot;   )\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED INTO 5 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
<A NAME="9"></A>                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
            SqlFormatter.formatSql(node));
    <FONT color="#83a33a"><A HREF="javascript:ZweiFrames('match831526-1.html#9',3,'match831526-top.html#9',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    @Test
    public void testBuildCreateTableStorageDefinitions() throws Exception {
        SQLExecutor e = SQLExecutor.builder</B></FONT>(clusterService)
            .addTable(&quot;create table myschema.test (&quot; +
                      &quot; s string storage with (columnstore =false)&quot; +
                      &quot;) &quot; +
                      &quot;clustered into 5 shards &quot; +
                      &quot;with (&quot; +
                      &quot; number_of_replicas = '0-all',&quot; +
                      &quot; column_policy = 'strict',&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;myschema.test&quot;);

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;myschema\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;s\&quot; TEXT STORAGE WITH (\n&quot; +
                     &quot;      columnstore = false\n&quot; +
                     &quot;   )\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED INTO 5 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-all',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
<A NAME="8"></A>                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
            SqlFormatter.formatSql(node));
    <FONT color="#c58917"><A HREF="javascript:ZweiFrames('match831526-1.html#8',3,'match831526-top.html#8',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>}

    @Test
    public void testBuildCreateTableColumnDefaultClause() throws Exception {
        SQLExecutor e = SQLExecutor.builder</B></FONT>(clusterService)
            .addTable(&quot;CREATE TABLE test (&quot; +
                      &quot;   col1 TEXT,&quot; +
                      &quot;   col2 INTEGER DEFAULT 1 + 1,&quot; +
                      &quot;   col3 TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP(3),&quot; +
                      &quot;   col4 TIMESTAMP WITHOUT TIME ZONE DEFAULT CURRENT_TIMESTAMP(3)&quot; +
                      &quot;) with (&quot; +
                      &quot; \&quot;merge.scheduler.max_thread_count\&quot; = 1&quot; +
                      &quot;)&quot;)
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo(&quot;test&quot;);
        CreateTable&lt;?&gt; node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals(&quot;CREATE TABLE IF NOT EXISTS \&quot;doc\&quot;.\&quot;test\&quot; (\n&quot; +
                     &quot;   \&quot;col1\&quot; TEXT,\n&quot; +
                     &quot;   \&quot;col2\&quot; INTEGER DEFAULT 2,\n&quot; +
                     &quot;   \&quot;col3\&quot; TIMESTAMP WITH TIME ZONE DEFAULT current_timestamp(3),\n&quot; +
                     &quot;   \&quot;col4\&quot; TIMESTAMP WITHOUT TIME ZONE DEFAULT _cast(current_timestamp(3), 'timestamp without time zone')\n&quot; +
                     &quot;)\n&quot; +
                     &quot;CLUSTERED INTO 4 SHARDS\n&quot; +
                     &quot;WITH (\n&quot; +
                     &quot;   \&quot;allocation.max_retries\&quot; = 5,\n&quot; +
                     &quot;   \&quot;blocks.metadata\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.read_only_allow_delete\&quot; = false,\n&quot; +
                     &quot;   \&quot;blocks.write\&quot; = false,\n&quot; +
                     &quot;   codec = 'default',\n&quot; +
                     &quot;   column_policy = 'strict',\n&quot; +
                     &quot;   \&quot;mapping.total_fields.limit\&quot; = 1000,\n&quot; +
                     &quot;   max_ngram_diff = 1,\n&quot; +
                     &quot;   max_shingle_diff = 3,\n&quot; +
                     &quot;   \&quot;merge.scheduler.max_thread_count\&quot; = 1,\n&quot; +
                     &quot;   number_of_replicas = '0-1',\n&quot; +
                     &quot;   \&quot;routing.allocation.enable\&quot; = 'all',\n&quot; +
                     &quot;   \&quot;routing.allocation.total_shards_per_node\&quot; = -1,\n&quot; +
                     &quot;   \&quot;store.type\&quot; = 'fs',\n&quot; +
                     &quot;   \&quot;translog.durability\&quot; = 'REQUEST',\n&quot; +
                     &quot;   \&quot;translog.flush_threshold_size\&quot; = 536870912,\n&quot; +
                     &quot;   \&quot;translog.sync_interval\&quot; = 5000,\n&quot; +
                     &quot;   \&quot;unassigned.node_left.delayed_timeout\&quot; = 60000,\n&quot; +
                     &quot;   \&quot;write.wait_for_active_shards\&quot; = '1'\n&quot; +
                     &quot;)&quot;,
                     SqlFormatter.formatSql(node));
<A NAME="7"></A>    }

    @Test
    public void test_varchar_with_length_limit_is_printed_as_varchar_with_length_in_show_create_table() throws Exception <FONT color="#38a4a5"><A HREF="javascript:ZweiFrames('match831526-1.html#7',3,'match831526-top.html#7',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>{
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable(&quot;create table tbl (name varchar(10))&quot;)
            .build();
        DocTableInfo table = e.resolveTableInfo(&quot;tbl&quot;);
        CreateTable&lt;?&gt; node = MetadataToASTNodeResolver.resolveCreateTable</B></FONT>(table);
        assertThat(SqlFormatter.formatSql(node), Matchers.containsString(&quot;\&quot;name\&quot; VARCHAR(10)&quot;));
<A NAME="6"></A>    }

    @Test
    public void test_bit_string_length_is_shown_in_show_create_table_output() throws Exception <FONT color="#8c8774"><A HREF="javascript:ZweiFrames('match831526-1.html#6',3,'match831526-top.html#6',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>{
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable(&quot;create table tbl (xs bit(8))&quot;)
            .build();
        DocTableInfo table = e.resolveTableInfo(&quot;tbl&quot;);
        CreateTable&lt;?&gt; node = MetadataToASTNodeResolver.resolveCreateTable</B></FONT>(table);
        assertThat(SqlFormatter.formatSql(node), Matchers.containsString(&quot;\&quot;xs\&quot; BIT(8)&quot;));
    }
}
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>RetentionLeaseIT.java</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the &quot;License&quot;); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.elasticsearch.index.seqno;

import static org.hamcrest.Matchers.anyOf;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.empty;
import static org.hamcrest.Matchers.equalTo;

import java.io.Closeable;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicReference;
import java.util.function.BiConsumer;
import java.util.function.Consumer;

import org.elasticsearch.ElasticsearchException;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.support.replication.ReplicationResponse;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;
<A NAME="0"></A>import org.elasticsearch.plugins.Plugin;
import org.elasticsearch.test.transport.MockTransportService;
import org.elasticsearch.threadpool.ThreadPool;
<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#0',2,'match831526-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>import org.elasticsearch.transport.AbstractSimpleTransportTestCase;
import org.elasticsearch.transport.TransportService;
import org.junit.After;
import org.junit.Test;

import io.crate.common.collections.Lists2;
import io.crate.common.unit.TimeValue;
import io.crate.integrationtests.SQLIntegrationTestCase;

public class RetentionLeaseIT extends SQLIntegrationTestCase  {

    @Override
    protected Collection&lt;Class&lt;? extends Plugin&gt;&gt; nodePlugins() {
        return</B></FONT> Lists2.concat(super.nodePlugins(), MockTransportService.TestPlugin.class);
    }
<A NAME="5"></A>
    @After
    public void resetSettings() {
        <FONT color="#151b8d"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#5',2,'match831526-top.html#5',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>execute(&quot;reset global \&quot;indices.recovery.retry_delay_network\&quot;&quot;);
    }

    @Test
    public void testRetentionLeasesSyncedOnAdd() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</B></FONT>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        execute(
            &quot;create table doc.tbl (x int) clustered into 1 shards &quot; +
            &quot;with (number_of_replicas = ?, \&quot;soft_deletes.enabled\&quot; = true)&quot;,
            new Object[] { numberOfReplicas }
        );
        ensureGreen(&quot;tbl&quot;);
        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
            .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
<A NAME="7"></A>        // we will add multiple retention leases and expect to see them synced to all replicas
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new HashMap&lt;&gt;();
        for (int i = 0; i &lt; length; i++) <FONT color="#38a4a5"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#7',2,'match831526-top.html#7',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>{
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength</B></FONT>(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
            // simulate a peer recovery which locks the soft deletes policy on the primary
            final Closeable retentionLock = randomBoolean() ? primary.acquireHistoryRetentionLock(Engine.HistorySource.INDEX) : () -&gt; {};
            currentRetentionLeases.put(id, primary.addRetentionLease(id, retainingSequenceNumber, source, listener));
            latch.await();
            retentionLock.close();

            // check retention leases have been written on the primary
            assertThat(currentRetentionLeases,
                equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases())));

            // check current retention leases have been synced to all replicas
            for (final ShardRouting replicaShard : clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).replicaShards()) {
                final String replicaShardNodeId = replicaShard.currentNodeId();
                final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                final IndexShard replica = internalCluster()
                    .getInstance(IndicesService.class, replicaShardNodeName)
                    .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
                final Map&lt;String, RetentionLease&gt; retentionLeasesOnReplica =
                    RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases());
                assertThat(retentionLeasesOnReplica, equalTo(currentRetentionLeases));

                // check retention leases have been written on the replica
                assertThat(currentRetentionLeases,
<A NAME="10"></A>                    equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.loadRetentionLeases())));
            }
        }
    <FONT color="#ad5910"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#10',2,'match831526-top.html#10',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testRetentionLeaseSyncedOnRemove() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</B></FONT>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);

        execute(&quot;create table doc.tbl (x int) clustered into 1 shards with (number_of_replicas = ?)&quot;,
                new Object[]{numberOfReplicas});

        ensureGreen(&quot;tbl&quot;);
        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
<A NAME="6"></A>            .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new LinkedHashMap&lt;&gt;();
        for (int i = 0; i &lt; length; i++) <FONT color="#8c8774"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#6',2,'match831526-top.html#6',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>{
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength</B></FONT>(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = countDownLatchListener(latch);
            // simulate a peer recovery which locks the soft deletes policy on the primary
            final Closeable retentionLock = randomBoolean() ? primary.acquireHistoryRetentionLock(Engine.HistorySource.INDEX) : () -&gt; {};
            currentRetentionLeases.put(id, primary.addRetentionLease(id, retainingSequenceNumber, source, listener));
            latch.await();
            retentionLock.close();
        }

        for (int i = 0; i &lt; length; i++) {
            final String id = randomFrom(currentRetentionLeases.keySet());
            final CountDownLatch latch = new CountDownLatch(1);
            primary.removeRetentionLease(id, countDownLatchListener(latch));
            // simulate a peer recovery which locks the soft deletes policy on the primary
            final Closeable retentionLock = randomBoolean() ? primary.acquireHistoryRetentionLock(Engine.HistorySource.INDEX) : () -&gt; {};
            currentRetentionLeases.remove(id);
            latch.await();
            retentionLock.close();

            // check retention leases have been written on the primary
            assertThat(currentRetentionLeases,
                       equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases())));

            // check current retention leases have been synced to all replicas
            for (final ShardRouting replicaShard : clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).replicaShards()) {
                final String replicaShardNodeId = replicaShard.currentNodeId();
                final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                final IndexShard replica = internalCluster()
                    .getInstance(IndicesService.class, replicaShardNodeName)
                    .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
                final Map&lt;String, RetentionLease&gt; retentionLeasesOnReplica =
                    RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases());
                assertThat(retentionLeasesOnReplica, equalTo(currentRetentionLeases));

                // check retention leases have been written on the replica
                assertThat(currentRetentionLeases,
<A NAME="1"></A>                           equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.loadRetentionLeases())));
            }
        }
    <FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#1',2,'match831526-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testRetentionLeasesSyncOnExpiration() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        final long estimatedTimeIntervalMillis = ThreadPool.ESTIMATED_TIME_INTERVAL_SETTING.get(Settings.EMPTY).millis</B></FONT>();
        final TimeValue retentionLeaseTimeToLive =
                TimeValue.timeValueMillis(randomLongBetween(estimatedTimeIntervalMillis, 2 * estimatedTimeIntervalMillis));
        execute(
            &quot;create table doc.tbl (x int) clustered into 1 shards &quot; +
            &quot;with (&quot; +
            &quot;   number_of_replicas = ?, &quot; +
            &quot;   \&quot;soft_deletes.enabled\&quot; = true, &quot; +
            &quot;   \&quot;soft_deletes.retention_lease.sync_interval\&quot; = ?)&quot;,
            new Object[] {
                numberOfReplicas,
                retentionLeaseTimeToLive.getStringRep()
            }
        );
        ensureGreen(&quot;tbl&quot;);
        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
                .getInstance(IndicesService.class, primaryShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
        // we will add multiple retention leases, wait for some to expire, and assert a consistent view between the primary and the replicas
        final int length = randomIntBetween(1, 8);
        for (int i = 0; i &lt; length; i++) {
            // update the index for retention leases to live a long time
            execute(&quot;alter table doc.tbl reset (\&quot;soft_deletes.retention_lease.period\&quot;)&quot;);

            final String id = randomAlphaOfLength(8);
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
            final RetentionLease currentRetentionLease = primary.addRetentionLease(id, retainingSequenceNumber, source, listener);
            final long now = System.nanoTime();
            latch.await();

            // check current retention leases have been synced to all replicas
            for (final ShardRouting replicaShard : clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).replicaShards()) {
                final String replicaShardNodeId = replicaShard.currentNodeId();
                final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                final IndexShard replica = internalCluster()
                        .getInstance(IndicesService.class, replicaShardNodeName)
                        .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
                assertThat(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases()).values(),
                    anyOf(empty(), contains(currentRetentionLease)));
            }

            // update the index for retention leases to short a long time, to force expiration
            execute(&quot;alter table doc.tbl set (\&quot;soft_deletes.retention_lease.period\&quot; = ?)&quot;, new Object[] { retentionLeaseTimeToLive.getStringRep() });

            // sleep long enough that the current retention lease has expired
            final long later = System.nanoTime();
            Thread.sleep(Math.max(0, retentionLeaseTimeToLive.millis() - TimeUnit.NANOSECONDS.toMillis(later - now)));
            assertBusy(() -&gt; assertThat(
                RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.getRetentionLeases()).entrySet(), empty()));

            // now that all retention leases are expired should have been synced to all replicas
            assertBusy(() -&gt; {
                for (final ShardRouting replicaShard : clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).replicaShards()) {
                    final String replicaShardNodeId = replicaShard.currentNodeId();
                    final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                    final IndexShard replica = internalCluster()
                        .getInstance(IndicesService.class, replicaShardNodeName)
                        .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));

                    assertThat(
                        RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases()).entrySet(), empty());
<A NAME="9"></A>                }
            });
        }
    <FONT color="#83a33a"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#9',2,'match831526-top.html#9',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testBackgroundRetentionLeaseSync() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</B></FONT>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        execute(
            &quot;create table doc.tbl (x int) clustered into 1 shards &quot; +
            &quot;with (&quot; +
            &quot;   number_of_replicas = ?, &quot; +
            &quot;   \&quot;soft_deletes.retention_lease.sync_interval\&quot; = '1s')&quot;,
            new Object[] {
                numberOfReplicas,
            }
        );

        ensureGreen(&quot;tbl&quot;);
        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
                .getInstance(IndicesService.class, primaryShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
        // we will add multiple retention leases and expect to see them synced to all replicas
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new LinkedHashMap&lt;&gt;(length);
        final List&lt;String&gt; ids = new ArrayList&lt;&gt;(length);
        for (int i = 0; i &lt; length; i++) {
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            ids.add(id);
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength(8);
            final CountDownLatch latch = new CountDownLatch(1);
            // put a new lease
            currentRetentionLeases.put(
                    id,
                    primary.addRetentionLease(id, retainingSequenceNumber, source, ActionListener.wrap(latch::countDown)));
            latch.await();
            // now renew all existing leases; we expect to see these synced to the replicas
            for (int j = 0; j &lt;= i; j++) {
                currentRetentionLeases.put(
                        ids.get(j),
                        primary.renewRetentionLease(
                                ids.get(j),
                                randomLongBetween(currentRetentionLeases.get(ids.get(j)).retainingSequenceNumber(), Long.MAX_VALUE),
                                source));
            }
            assertBusy(() -&gt; {
                // check all retention leases have been synced to all replicas
                for (final ShardRouting replicaShard : clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).replicaShards()) {
                    final String replicaShardNodeId = replicaShard.currentNodeId();
                    final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                    final IndexShard replica = internalCluster()
                            .getInstance(IndicesService.class, replicaShardNodeName)
                            .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
                    assertThat(replica.getRetentionLeases(), equalTo(primary.getRetentionLeases()));
<A NAME="8"></A>                }
            });
        }
    <FONT color="#c58917"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#8',2,'match831526-top.html#8',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testRetentionLeasesSyncOnRecovery() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</B></FONT>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        /*
         * We effectively disable the background sync to ensure that the retention leases are not synced in the background so that the only
         * source of retention leases on the replicas would be from recovery.
         */
        execute(
            &quot;create table doc.tbl (x int) clustered into 1 shards &quot; +
            &quot;with (&quot; +
            &quot;   number_of_replicas = 0, &quot; +
            &quot;   \&quot;soft_deletes.enabled\&quot; = true, &quot; +
            &quot;   \&quot;soft_deletes.retention_lease.sync_interval\&quot; = ?)&quot;,
            new Object[] {
                TimeValue.timeValueHours(24).getStringRep()
            }
        );
        allowNodes(&quot;tbl&quot;, 1);
        ensureYellow(&quot;tbl&quot;);
        execute(&quot;alter table doc.tbl set (number_of_replicas = ?)&quot;, new Object[] { numberOfReplicas });

        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
            .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new HashMap&lt;&gt;();
        for (int i = 0; i &lt; length; i++) {
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
            currentRetentionLeases.put(id, primary.addRetentionLease(id, retainingSequenceNumber, source, listener));
            latch.await();
        }

        // Cause some recoveries to fail to ensure that retention leases are handled properly when retrying a recovery
        //
        execute(&quot;set global persistent \&quot;indices.recovery.retry_delay_network\&quot; = '100ms'&quot;);
        final Semaphore recoveriesToDisrupt = new Semaphore(scaledRandomIntBetween(0, 4));
        final MockTransportService primaryTransportService
            = (MockTransportService) internalCluster().getInstance(TransportService.class, primaryShardNodeName);
        primaryTransportService.addSendBehavior((connection, requestId, action, request, options) -&gt; {
            if (action.equals(PeerRecoveryTargetService.Actions.FINALIZE) &amp;&amp; recoveriesToDisrupt.tryAcquire()) {
                if (randomBoolean()) {
                    // return a ConnectTransportException to the START_RECOVERY action
                    final TransportService replicaTransportService
                        = internalCluster().getInstance(TransportService.class, connection.getNode().getName());
                    final DiscoveryNode primaryNode = primaryTransportService.getLocalNode();
                    replicaTransportService.disconnectFromNode(primaryNode);
                    AbstractSimpleTransportTestCase.connectToNode(replicaTransportService, primaryNode);
                } else {
                    // return an exception to the FINALIZE action
                    throw new ElasticsearchException(&quot;failing recovery for test purposes&quot;);
                }
            }
            connection.sendRequest(requestId, action, request, options);
        });

        // now allow the replicas to be allocated and wait for recovery to finalize
        allowNodes(&quot;tbl&quot;, 1 + numberOfReplicas);
        ensureGreen(&quot;tbl&quot;);

        // check current retention leases have been synced to all replicas
        for (final ShardRouting replicaShard : clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).replicaShards()) {
            final String replicaShardNodeId = replicaShard.currentNodeId();
            final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
            final IndexShard replica = internalCluster()
                .getInstance(IndicesService.class, replicaShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));
            final Map&lt;String, RetentionLease&gt; retentionLeasesOnReplica
                = RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases());
            assertThat(retentionLeasesOnReplica, equalTo(currentRetentionLeases));

            // check retention leases have been written on the replica; see RecoveryTarget#finalizeRecovery
<A NAME="4"></A>            assertThat(currentRetentionLeases,
                equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.loadRetentionLeases())));
        }
    <FONT color="#6cc417"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#4',2,'match831526-top.html#4',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testCanAddRetentionLeaseUnderBlock() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runUnderBlockTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) </B></FONT>-&gt; {
                    final String nextId = randomValueOtherThan(idForInitialRetentionLease, () -&gt; randomAlphaOfLength(8));
                    final long nextRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    primary.addRetentionLease(nextId, nextRetainingSequenceNumber, nextSource, listener);
                },
                primary -&gt; {});
    }

    @Test
    public void testCanRenewRetentionLeaseUnderBlock() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        final long initialRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
        final AtomicReference&lt;RetentionLease&gt; retentionLease = new AtomicReference&lt;&gt;();
        runUnderBlockTest(
                idForInitialRetentionLease,
                initialRetainingSequenceNumber,
                (primary, listener) -&gt; {
                    final long nextRetainingSequenceNumber = randomLongBetween(initialRetainingSequenceNumber, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    retentionLease.set(primary.renewRetentionLease(idForInitialRetentionLease, nextRetainingSequenceNumber, nextSource));
                    listener.onResponse(new ReplicationResponse());
                },
                primary -&gt; {
                    try {
                        /*
                         * If the background renew was able to execute, then the retention leases were persisted to disk. There is no other
                         * way for the current retention leases to end up written to disk so we assume that if they are written to disk, it
                         * implies that the background sync was able to execute under a block.
                         */
                        assertBusy(() -&gt; assertThat(
                            RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases()).values(),
                            contains(retentionLease.get())));
                    } catch (final Exception e) {
                        fail(e.toString());
                    }
                });

    }

    public void testCanRemoveRetentionLeasesUnderBlock() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runUnderBlockTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) -&gt; primary.removeRetentionLease(idForInitialRetentionLease, listener),
                indexShard -&gt; {});
    }

    private void runUnderBlockTest(
            final String idForInitialRetentionLease,
            final long initialRetainingSequenceNumber,
            final BiConsumer&lt;IndexShard, ActionListener&lt;ReplicationResponse&gt;&gt; primaryConsumer,
            final Consumer&lt;IndexShard&gt; afterSync) throws InterruptedException {
        execute(
            &quot;create table doc.tbl (x int) clustered into 1 shards &quot; +
            &quot;with (&quot; +
            &quot;   number_of_replicas = 0, &quot; +
            &quot;   \&quot;soft_deletes.enabled\&quot; = true, &quot; +
            &quot;   \&quot;soft_deletes.retention_lease.sync_interval\&quot; = '1s' &quot; +
            &quot;)&quot;
        );
        ensureGreen(&quot;tbl&quot;);

        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
            .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));

        final String source = randomAlphaOfLength(8);
        final CountDownLatch latch = new CountDownLatch(1);
        final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
        primary.addRetentionLease(idForInitialRetentionLease, initialRetainingSequenceNumber, source, listener);
        latch.await();

        final String block = randomFrom(&quot;read_only&quot;, &quot;read_only_allow_delete&quot;, &quot;read&quot;, &quot;write&quot;, &quot;metadata&quot;);

        execute(&quot;alter table doc.tbl set (\&quot;blocks.&quot; + block + &quot;\&quot; = true)&quot;);
        try {
            final CountDownLatch actionLatch = new CountDownLatch(1);
            final AtomicBoolean success = new AtomicBoolean();

            primaryConsumer.accept(
                primary,
                new ActionListener&lt;ReplicationResponse&gt;() {

                    @Override
                    public void onResponse(final ReplicationResponse replicationResponse) {
                        success.set(true);
                        actionLatch.countDown();
                    }

                    @Override
                    public void onFailure(final Exception e) {
                        fail(e.toString());
                    }

                }
            );
            actionLatch.await();
            assertTrue(success.get());
            afterSync.accept(primary);
<A NAME="3"></A>        } finally {
            execute(&quot;alter table doc.tbl reset (\&quot;blocks.&quot; + block + &quot;\&quot;)&quot;);
        }
    <FONT color="#53858b"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#3',2,'match831526-top.html#3',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testCanAddRetentionLeaseWithoutWaitingForShards() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runWaitForShardsTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) </B></FONT>-&gt; {
                    final String nextId = randomValueOtherThan(idForInitialRetentionLease, () -&gt; randomAlphaOfLength(8));
                    final long nextRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    primary.addRetentionLease(nextId, nextRetainingSequenceNumber, nextSource, listener);
                },
                primary -&gt; {});
    }

    @Test
    public void testCanRenewRetentionLeaseWithoutWaitingForShards() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        final long initialRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
        final AtomicReference&lt;RetentionLease&gt; retentionLease = new AtomicReference&lt;&gt;();
        runWaitForShardsTest(
                idForInitialRetentionLease,
                initialRetainingSequenceNumber,
                (primary, listener) -&gt; {
                    final long nextRetainingSequenceNumber = randomLongBetween(initialRetainingSequenceNumber, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    retentionLease.set(primary.renewRetentionLease(idForInitialRetentionLease, nextRetainingSequenceNumber, nextSource));
                    listener.onResponse(new ReplicationResponse());
                },
                primary -&gt; {
                    try {
                        /*
                         * If the background renew was able to execute, then the retention leases were persisted to disk. There is no other
                         * way for the current retention leases to end up written to disk so we assume that if they are written to disk, it
                         * implies that the background sync was able to execute despite wait for shards being set on the index.
                         */
                        assertBusy(() -&gt; assertThat(
                            RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases()).values(),
                            contains(retentionLease.get())));
                    } catch (final Exception e) {
                        fail(e.toString());
<A NAME="2"></A>                    }
                });

    <FONT color="#980517"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match831526-0.html#2',2,'match831526-top.html#2',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>}

    @Test
    public void testCanRemoveRetentionLeasesWithoutWaitingForShards() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runWaitForShardsTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) </B></FONT>-&gt; primary.removeRetentionLease(idForInitialRetentionLease, listener),
                primary -&gt; {});
    }

    private void runWaitForShardsTest(
            final String idForInitialRetentionLease,
            final long initialRetainingSequenceNumber,
            final BiConsumer&lt;IndexShard, ActionListener&lt;ReplicationResponse&gt;&gt; primaryConsumer,
            final Consumer&lt;IndexShard&gt; afterSync) throws InterruptedException {
        final int numDataNodes = internalCluster().numDataNodes();
        execute(
            &quot;create table doc.tbl (x int) clustered into 1 shards &quot; +
            &quot;with (&quot; +
            &quot;   number_of_replicas = ?, &quot; +
            &quot;   \&quot;soft_deletes.enabled\&quot; = true,&quot; +
            &quot;   \&quot;soft_deletes.retention_lease.sync_interval\&quot; = ?)&quot;,
            new Object[] {
                numDataNodes == 1 ? 0 : numDataNodes - 1,
                TimeValue.timeValueSeconds(1).getStringRep()
            }
        );
        ensureYellowAndNoInitializingShards(&quot;tbl&quot;);
        assertFalse(client().admin().cluster().prepareHealth(&quot;tbl&quot;).setWaitForActiveShards(numDataNodes).get().isTimedOut());

        final String primaryShardNodeId = clusterService().state().routingTable().index(&quot;tbl&quot;).shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
                .getInstance(IndicesService.class, primaryShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex(&quot;tbl&quot;), 0));

        final String source = randomAlphaOfLength(8);
        final CountDownLatch latch = new CountDownLatch(1);
        final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
        primary.addRetentionLease(idForInitialRetentionLease, initialRetainingSequenceNumber, source, listener);
        latch.await();

        final String waitForActiveValue = randomBoolean() ? &quot;all&quot; : Integer.toString(numDataNodes);

        execute(&quot;alter table doc.tbl set (\&quot;write.wait_for_active_shards\&quot; = ?)&quot;, new Object[] { waitForActiveValue });
        final CountDownLatch actionLatch = new CountDownLatch(1);
        final AtomicBoolean success = new AtomicBoolean();

        primaryConsumer.accept(
                primary,
                new ActionListener&lt;ReplicationResponse&gt;() {

                    @Override
                    public void onResponse(final ReplicationResponse replicationResponse) {
                        success.set(true);
                        actionLatch.countDown();
                    }

                    @Override
                    public void onFailure(final Exception e) {
                        fail(e.toString());
                    }

                });
        actionLatch.await();
        assertTrue(success.get());
        afterSync.accept(primary);
    }

    private static void failWithException(Exception e) {
        throw new AssertionError(&quot;unexpected&quot;, e);
    }

    private static ActionListener&lt;ReplicationResponse&gt; countDownLatchListener(CountDownLatch latch) {
        return ActionListener.wrap(r -&gt; latch.countDown(), RetentionLeaseIT::failWithException);
    }

}
</PRE>
</div>
  </div>
</body>
</html>
