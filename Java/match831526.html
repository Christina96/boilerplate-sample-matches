<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html><head><title>Matches for MetadataToASTNodeResolverTest.java &amp; RetentionLeaseIT.java</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for MetadataToASTNodeResolverTest.java &amp; RetentionLeaseIT.java
      </h3>
<h1 align="center">
        17.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>MetadataToASTNodeResolverTest.java (64.28571%)<th>RetentionLeaseIT.java (10.29552%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(24-37)<td><a href="#" name="0">(56-69)</a><td align="center"><font color="#ff0000">12</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(38-71)<td><a href="#" name="1">(188-194)</a><td align="center"><font color="#e90000">11</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(271-288)<td><a href="#" name="2">(564-572)</a><td align="center"><font color="#d40000">10</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(223-238)<td><a href="#" name="3">(518-526)</a><td align="center"><font color="#d40000">10</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(171-188)<td><a href="#" name="4">(404-412)</a><td align="center"><font color="#d40000">10</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(120-125)<td><a href="#" name="5">(74-80)</a><td align="center"><font color="#d40000">10</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(506-511)<td><a href="#" name="6">(145-148)</a><td align="center"><font color="#bf0000">9</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(496-501)<td><a href="#" name="7">(95-98)</a><td align="center"><font color="#bf0000">9</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(446-450)<td><a href="#" name="8">(321-326)</a><td align="center"><font color="#bf0000">9</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(398-402)<td><a href="#" name="9">(263-268)</a><td align="center"><font color="#bf0000">9</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(322-327)<td><a href="#" name="10">(127-132)</a><td align="center"><font color="#bf0000">9</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>MetadataToASTNodeResolverTest.java</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
/*
 * Licensed to Crate.io GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */
<a name="0"></a>
package io.crate.analyze;

<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import io.crate.metadata.doc.DocTableInfo;
import io.crate.sql.SqlFormatter;
import io.crate.sql.tree.CreateTable;
import io.crate.test.integration.CrateDummyClusterServiceUnitTest;
import io.crate.testing.SQLExecutor;

import org.hamcrest.Matchers;
import org.junit.Test;

public class MetadataToASTNodeResolverTest extends CrateDummyClusterServiceUnitTest {

<a name="1"></a>    @Override
    protected boolean enableWarningsCheck() {
        return</b></font> false;
    <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}

    @Test
    public void testBuildCreateTableColumns() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable("create table doc.test (" +
                      " bools boolean," +
                      " bytes byte," +
                      " strings string," +
                      " shorts short," +
                      " floats float," +
                      " doubles double," +
                      " ints integer," +
                      " longs long," +
                      " timestamp timestamp with time zone," +
                      " ip_addr ip," +
                      " arr_simple array(string)," +
                      " arr_geo_point array(geo_point)," +
                      " arr_obj array(object(strict) as (" +
                      "  col_1 long," +
                      "  col_2 string" +
                      " ))," +
                      " obj object as (" +
                      "  col_1 long," +
                      "  col_2 string" +
                      " )" +
                      ") " +
                      "clustered into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'," +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo</b></font>("doc.test");

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"doc\".\"test\" (\n" +
                     "   \"bools\" BOOLEAN,\n" +
                     "   \"bytes\" CHAR,\n" +
                     "   \"strings\" TEXT,\n" +
                     "   \"shorts\" SMALLINT,\n" +
                     "   \"floats\" REAL,\n" +
                     "   \"doubles\" DOUBLE PRECISION,\n" +
                     "   \"ints\" INTEGER,\n" +
                     "   \"longs\" BIGINT,\n" +
                     "   \"timestamp\" TIMESTAMP WITH TIME ZONE,\n" +
                     "   \"ip_addr\" IP,\n" +
                     "   \"arr_simple\" ARRAY(TEXT),\n" +
                     "   \"arr_geo_point\" ARRAY(GEO_POINT),\n" +
                     "   \"arr_obj\" ARRAY(OBJECT(STRICT) AS (\n" +
                     "      \"col_1\" BIGINT,\n" +
                     "      \"col_2\" TEXT\n" +
                     "   )),\n" +
                     "   \"obj\" OBJECT(DYNAMIC) AS (\n" +
                     "      \"col_1\" BIGINT,\n" +
                     "      \"col_2\" TEXT\n" +
                     "   )\n" +
                     ")\n" +
                     "CLUSTERED INTO 5 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
<a name="5"></a>                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
            <font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>SqlFormatter.formatSql(node));
    }

    @Test
    public void testBuildCreateTablePrimaryKey() throws Exception {
        SQLExecutor e = SQLExecutor.builder</b></font>(clusterService)
            .addTable("create table myschema.test (" +
                      " pk_col_one long," +
                      " pk_col_two long," +
                      " primary key (pk_col_one, pk_col_two)" +
                      ") " +
                      "clustered into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'," +
                      " column_policy = 'strict'," +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("myschema.test");

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                     "   \"pk_col_one\" BIGINT,\n" +
                     "   \"pk_col_two\" BIGINT,\n" +
                     "   PRIMARY KEY (\"pk_col_one\", \"pk_col_two\")\n" +
                     ")\n" +
                     "CLUSTERED INTO 5 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
<a name="4"></a>                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
            SqlFormatter.formatSql(node));
    <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}

    @Test
    public void testBuildCreateTableNotNull() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable("create table myschema.test (" +
                      " col_a string," +
                      " col_b string not null index using fulltext," +
                      " primary key (col_a)" +
                      ") " +
                      "clustered into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'," +
                      " column_policy = 'strict'," +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("myschema.test")</b></font>;

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                     "   \"col_a\" TEXT,\n" +
                     "   \"col_b\" TEXT NOT NULL INDEX USING FULLTEXT WITH (\n" +
                     "      analyzer = 'standard'\n" +
                     "   ),\n" +
                     "   PRIMARY KEY (\"col_a\")\n" +
                     ")\n" +
                     "CLUSTERED BY (\"col_a\") INTO 5 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
<a name="3"></a>                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
            SqlFormatter.formatSql(node));
    <font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}

    @Test
    public void testBuildCreateTableCheckConstraints() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable("create table doc.test (" +
                      " floats float constraint test_floats_check check (floats != -1)," +
                      " shorts short," +
                      " constraint test_shorts_check check (shorts &gt;= 0)" +
                      ") " +
                      "clustered into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("doc.test")</b></font>;

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"doc\".\"test\" (\n" +
                     "   \"floats\" REAL,\n" +
                     "   \"shorts\" SMALLINT,\n" +
                     "   CONSTRAINT test_floats_check CHECK(\"floats\" &lt;&gt; - 1),\n" +
                     "   CONSTRAINT test_shorts_check CHECK(\"shorts\" &gt;= 0)\n" +
                     ")\n" +
                     "CLUSTERED INTO 5 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
<a name="2"></a>                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
                     SqlFormatter.formatSql(node));
    <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}

    @Test
    public void testBuildCreateTableClusteredByPartitionedBy() throws Exception {
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addPartitionedTable("create table myschema.test (" +
                      " id long," +
                      " partition_column string," +
                      " cluster_column string" +
                      ") " +
                      "partitioned by (partition_column) " +
                      "clustered by (cluster_column) into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'," +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("myschema.test")</b></font>;

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                     "   \"id\" BIGINT,\n" +
                     "   \"partition_column\" TEXT,\n" +
                     "   \"cluster_column\" TEXT\n" +
                     ")\n" +
                     "CLUSTERED BY (\"cluster_column\") INTO 5 SHARDS\n" +
                     "PARTITIONED BY (\"partition_column\")\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
<a name="10"></a>                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
            SqlFormatter.formatSql(node));
    <font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}


    @Test
    public void testBuildCreateTableIndexes() throws Exception {
        SQLExecutor e = SQLExecutor.builder</b></font>(clusterService)
            .addTable("create table myschema.test (" +
                      " id long," +
                      " col_a string," +
                      " col_b string index using fulltext," +
                      " col_c string index off," +
                      " col_d object as (" +
                      "  a string" +
                      " )," +
                      " index col_a_col_b_ft using fulltext (col_a, col_b) with (" +
                      "  analyzer= 'english'" +
                      " )," +
                      " index col_d_a_ft using fulltext (col_d['a']) with (" +
                      "  analyzer= 'custom_analyzer'" +
                      " )," +
                      " index col_a_col_b_plain using plain (col_a, col_b)" +
                      ") " +
                      "clustered into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'," +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("myschema.test");

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                     "   \"id\" BIGINT,\n" +
                     "   \"col_a\" TEXT,\n" +
                     "   \"col_b\" TEXT INDEX USING FULLTEXT WITH (\n" +
                     "      analyzer = 'standard'\n" +
                     "   ),\n" +
                     "   \"col_c\" TEXT INDEX OFF,\n" +
                     "   \"col_d\" OBJECT(DYNAMIC) AS (\n" +
                     "      \"a\" TEXT\n" +
                     "   ),\n" +
                     "   INDEX \"col_a_col_b_ft\" USING FULLTEXT (\"col_a\", \"col_b\") WITH (\n" +
                     "      analyzer = 'english'\n" +
                     "   ),\n" +
                     "   INDEX \"col_d_a_ft\" USING FULLTEXT (\"col_d\"['a']) WITH (\n" +
                     "      analyzer = 'custom_analyzer'\n" +
                     "   ),\n" +
                     "   INDEX \"col_a_col_b_plain\" USING FULLTEXT (\"col_a\", \"col_b\") WITH (\n" +
                     "      analyzer = 'keyword'\n" +
                     "   )\n" +
                     ")\n" +
                     "CLUSTERED INTO 5 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
<a name="9"></a>                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
            SqlFormatter.formatSql(node));
    <font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}

    @Test
    public void testBuildCreateTableStorageDefinitions() throws Exception {
        SQLExecutor e = SQLExecutor.builder</b></font>(clusterService)
            .addTable("create table myschema.test (" +
                      " s string storage with (columnstore =false)" +
                      ") " +
                      "clustered into 5 shards " +
                      "with (" +
                      " number_of_replicas = '0-all'," +
                      " column_policy = 'strict'," +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("myschema.test");

        CreateTable node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                     "   \"s\" TEXT STORAGE WITH (\n" +
                     "      columnstore = false\n" +
                     "   )\n" +
                     ")\n" +
                     "CLUSTERED INTO 5 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-all',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
<a name="8"></a>                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
            SqlFormatter.formatSql(node));
    <font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>}

    @Test
    public void testBuildCreateTableColumnDefaultClause() throws Exception {
        SQLExecutor e = SQLExecutor.builder</b></font>(clusterService)
            .addTable("CREATE TABLE test (" +
                      "   col1 TEXT," +
                      "   col2 INTEGER DEFAULT 1 + 1," +
                      "   col3 TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP(3)," +
                      "   col4 TIMESTAMP WITHOUT TIME ZONE DEFAULT CURRENT_TIMESTAMP(3)" +
                      ") with (" +
                      " \"merge.scheduler.max_thread_count\" = 1" +
                      ")")
            .build();
        DocTableInfo tableInfo = e.resolveTableInfo("test");
        CreateTable&lt;?&gt; node = MetadataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"doc\".\"test\" (\n" +
                     "   \"col1\" TEXT,\n" +
                     "   \"col2\" INTEGER DEFAULT 2,\n" +
                     "   \"col3\" TIMESTAMP WITH TIME ZONE DEFAULT current_timestamp(3),\n" +
                     "   \"col4\" TIMESTAMP WITHOUT TIME ZONE DEFAULT _cast(current_timestamp(3), 'timestamp without time zone')\n" +
                     ")\n" +
                     "CLUSTERED INTO 4 SHARDS\n" +
                     "WITH (\n" +
                     "   \"allocation.max_retries\" = 5,\n" +
                     "   \"blocks.metadata\" = false,\n" +
                     "   \"blocks.read\" = false,\n" +
                     "   \"blocks.read_only\" = false,\n" +
                     "   \"blocks.read_only_allow_delete\" = false,\n" +
                     "   \"blocks.write\" = false,\n" +
                     "   codec = 'default',\n" +
                     "   column_policy = 'strict',\n" +
                     "   \"mapping.total_fields.limit\" = 1000,\n" +
                     "   max_ngram_diff = 1,\n" +
                     "   max_shingle_diff = 3,\n" +
                     "   \"merge.scheduler.max_thread_count\" = 1,\n" +
                     "   number_of_replicas = '0-1',\n" +
                     "   \"routing.allocation.enable\" = 'all',\n" +
                     "   \"routing.allocation.total_shards_per_node\" = -1,\n" +
                     "   \"store.type\" = 'fs',\n" +
                     "   \"translog.durability\" = 'REQUEST',\n" +
                     "   \"translog.flush_threshold_size\" = 536870912,\n" +
                     "   \"translog.sync_interval\" = 5000,\n" +
                     "   \"unassigned.node_left.delayed_timeout\" = 60000,\n" +
                     "   \"write.wait_for_active_shards\" = '1'\n" +
                     ")",
                     SqlFormatter.formatSql(node));
<a name="7"></a>    }

    @Test
    public void test_varchar_with_length_limit_is_printed_as_varchar_with_length_in_show_create_table() throws Exception <font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>{
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable("create table tbl (name varchar(10))")
            .build();
        DocTableInfo table = e.resolveTableInfo("tbl");
        CreateTable&lt;?&gt; node = MetadataToASTNodeResolver.resolveCreateTable</b></font>(table);
        assertThat(SqlFormatter.formatSql(node), Matchers.containsString("\"name\" VARCHAR(10)"));
<a name="6"></a>    }

    @Test
    public void test_bit_string_length_is_shown_in_show_create_table_output() throws Exception <font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>{
        SQLExecutor e = SQLExecutor.builder(clusterService)
            .addTable("create table tbl (xs bit(8))")
            .build();
        DocTableInfo table = e.resolveTableInfo("tbl");
        CreateTable&lt;?&gt; node = MetadataToASTNodeResolver.resolveCreateTable</b></font>(table);
        assertThat(SqlFormatter.formatSql(node), Matchers.containsString("\"xs\" BIT(8)"));
    }
}
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>RetentionLeaseIT.java</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.elasticsearch.index.seqno;

import static org.hamcrest.Matchers.anyOf;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.empty;
import static org.hamcrest.Matchers.equalTo;

import java.io.Closeable;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicReference;
import java.util.function.BiConsumer;
import java.util.function.Consumer;

import org.elasticsearch.ElasticsearchException;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.support.replication.ReplicationResponse;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;
<a name="0"></a>import org.elasticsearch.plugins.Plugin;
import org.elasticsearch.test.transport.MockTransportService;
import org.elasticsearch.threadpool.ThreadPool;
<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import org.elasticsearch.transport.AbstractSimpleTransportTestCase;
import org.elasticsearch.transport.TransportService;
import org.junit.After;
import org.junit.Test;

import io.crate.common.collections.Lists2;
import io.crate.common.unit.TimeValue;
import io.crate.integrationtests.SQLIntegrationTestCase;

public class RetentionLeaseIT extends SQLIntegrationTestCase  {

    @Override
    protected Collection&lt;Class&lt;? extends Plugin&gt;&gt; nodePlugins() {
        return</b></font> Lists2.concat(super.nodePlugins(), MockTransportService.TestPlugin.class);
    }
<a name="5"></a>
    @After
    public void resetSettings() {
        <font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>execute("reset global \"indices.recovery.retry_delay_network\"");
    }

    @Test
    public void testRetentionLeasesSyncedOnAdd() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</b></font>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        execute(
            "create table doc.tbl (x int) clustered into 1 shards " +
            "with (number_of_replicas = ?, \"soft_deletes.enabled\" = true)",
            new Object[] { numberOfReplicas }
        );
        ensureGreen("tbl");
        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
            .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
<a name="7"></a>        // we will add multiple retention leases and expect to see them synced to all replicas
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new HashMap&lt;&gt;();
        for (int i = 0; i &lt; length; i++) <font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>{
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength</b></font>(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
            // simulate a peer recovery which locks the soft deletes policy on the primary
            final Closeable retentionLock = randomBoolean() ? primary.acquireHistoryRetentionLock(Engine.HistorySource.INDEX) : () -&gt; {};
            currentRetentionLeases.put(id, primary.addRetentionLease(id, retainingSequenceNumber, source, listener));
            latch.await();
            retentionLock.close();

            // check retention leases have been written on the primary
            assertThat(currentRetentionLeases,
                equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases())));

            // check current retention leases have been synced to all replicas
            for (final ShardRouting replicaShard : clusterService().state().routingTable().index("tbl").shard(0).replicaShards()) {
                final String replicaShardNodeId = replicaShard.currentNodeId();
                final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                final IndexShard replica = internalCluster()
                    .getInstance(IndicesService.class, replicaShardNodeName)
                    .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
                final Map&lt;String, RetentionLease&gt; retentionLeasesOnReplica =
                    RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases());
                assertThat(retentionLeasesOnReplica, equalTo(currentRetentionLeases));

                // check retention leases have been written on the replica
                assertThat(currentRetentionLeases,
<a name="10"></a>                    equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.loadRetentionLeases())));
            }
        }
    <font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testRetentionLeaseSyncedOnRemove() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</b></font>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);

        execute("create table doc.tbl (x int) clustered into 1 shards with (number_of_replicas = ?)",
                new Object[]{numberOfReplicas});

        ensureGreen("tbl");
        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
<a name="6"></a>            .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new LinkedHashMap&lt;&gt;();
        for (int i = 0; i &lt; length; i++) <font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>{
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength</b></font>(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = countDownLatchListener(latch);
            // simulate a peer recovery which locks the soft deletes policy on the primary
            final Closeable retentionLock = randomBoolean() ? primary.acquireHistoryRetentionLock(Engine.HistorySource.INDEX) : () -&gt; {};
            currentRetentionLeases.put(id, primary.addRetentionLease(id, retainingSequenceNumber, source, listener));
            latch.await();
            retentionLock.close();
        }

        for (int i = 0; i &lt; length; i++) {
            final String id = randomFrom(currentRetentionLeases.keySet());
            final CountDownLatch latch = new CountDownLatch(1);
            primary.removeRetentionLease(id, countDownLatchListener(latch));
            // simulate a peer recovery which locks the soft deletes policy on the primary
            final Closeable retentionLock = randomBoolean() ? primary.acquireHistoryRetentionLock(Engine.HistorySource.INDEX) : () -&gt; {};
            currentRetentionLeases.remove(id);
            latch.await();
            retentionLock.close();

            // check retention leases have been written on the primary
            assertThat(currentRetentionLeases,
                       equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases())));

            // check current retention leases have been synced to all replicas
            for (final ShardRouting replicaShard : clusterService().state().routingTable().index("tbl").shard(0).replicaShards()) {
                final String replicaShardNodeId = replicaShard.currentNodeId();
                final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                final IndexShard replica = internalCluster()
                    .getInstance(IndicesService.class, replicaShardNodeName)
                    .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
                final Map&lt;String, RetentionLease&gt; retentionLeasesOnReplica =
                    RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases());
                assertThat(retentionLeasesOnReplica, equalTo(currentRetentionLeases));

                // check retention leases have been written on the replica
                assertThat(currentRetentionLeases,
<a name="1"></a>                           equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.loadRetentionLeases())));
            }
        }
    <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testRetentionLeasesSyncOnExpiration() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        final long estimatedTimeIntervalMillis = ThreadPool.ESTIMATED_TIME_INTERVAL_SETTING.get(Settings.EMPTY).millis</b></font>();
        final TimeValue retentionLeaseTimeToLive =
                TimeValue.timeValueMillis(randomLongBetween(estimatedTimeIntervalMillis, 2 * estimatedTimeIntervalMillis));
        execute(
            "create table doc.tbl (x int) clustered into 1 shards " +
            "with (" +
            "   number_of_replicas = ?, " +
            "   \"soft_deletes.enabled\" = true, " +
            "   \"soft_deletes.retention_lease.sync_interval\" = ?)",
            new Object[] {
                numberOfReplicas,
                retentionLeaseTimeToLive.getStringRep()
            }
        );
        ensureGreen("tbl");
        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
                .getInstance(IndicesService.class, primaryShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
        // we will add multiple retention leases, wait for some to expire, and assert a consistent view between the primary and the replicas
        final int length = randomIntBetween(1, 8);
        for (int i = 0; i &lt; length; i++) {
            // update the index for retention leases to live a long time
            execute("alter table doc.tbl reset (\"soft_deletes.retention_lease.period\")");

            final String id = randomAlphaOfLength(8);
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
            final RetentionLease currentRetentionLease = primary.addRetentionLease(id, retainingSequenceNumber, source, listener);
            final long now = System.nanoTime();
            latch.await();

            // check current retention leases have been synced to all replicas
            for (final ShardRouting replicaShard : clusterService().state().routingTable().index("tbl").shard(0).replicaShards()) {
                final String replicaShardNodeId = replicaShard.currentNodeId();
                final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                final IndexShard replica = internalCluster()
                        .getInstance(IndicesService.class, replicaShardNodeName)
                        .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
                assertThat(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases()).values(),
                    anyOf(empty(), contains(currentRetentionLease)));
            }

            // update the index for retention leases to short a long time, to force expiration
            execute("alter table doc.tbl set (\"soft_deletes.retention_lease.period\" = ?)", new Object[] { retentionLeaseTimeToLive.getStringRep() });

            // sleep long enough that the current retention lease has expired
            final long later = System.nanoTime();
            Thread.sleep(Math.max(0, retentionLeaseTimeToLive.millis() - TimeUnit.NANOSECONDS.toMillis(later - now)));
            assertBusy(() -&gt; assertThat(
                RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.getRetentionLeases()).entrySet(), empty()));

            // now that all retention leases are expired should have been synced to all replicas
            assertBusy(() -&gt; {
                for (final ShardRouting replicaShard : clusterService().state().routingTable().index("tbl").shard(0).replicaShards()) {
                    final String replicaShardNodeId = replicaShard.currentNodeId();
                    final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                    final IndexShard replica = internalCluster()
                        .getInstance(IndicesService.class, replicaShardNodeName)
                        .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));

                    assertThat(
                        RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases()).entrySet(), empty());
<a name="9"></a>                }
            });
        }
    <font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testBackgroundRetentionLeaseSync() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</b></font>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        execute(
            "create table doc.tbl (x int) clustered into 1 shards " +
            "with (" +
            "   number_of_replicas = ?, " +
            "   \"soft_deletes.retention_lease.sync_interval\" = '1s')",
            new Object[] {
                numberOfReplicas,
            }
        );

        ensureGreen("tbl");
        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
                .getInstance(IndicesService.class, primaryShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
        // we will add multiple retention leases and expect to see them synced to all replicas
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new LinkedHashMap&lt;&gt;(length);
        final List&lt;String&gt; ids = new ArrayList&lt;&gt;(length);
        for (int i = 0; i &lt; length; i++) {
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            ids.add(id);
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength(8);
            final CountDownLatch latch = new CountDownLatch(1);
            // put a new lease
            currentRetentionLeases.put(
                    id,
                    primary.addRetentionLease(id, retainingSequenceNumber, source, ActionListener.wrap(latch::countDown)));
            latch.await();
            // now renew all existing leases; we expect to see these synced to the replicas
            for (int j = 0; j &lt;= i; j++) {
                currentRetentionLeases.put(
                        ids.get(j),
                        primary.renewRetentionLease(
                                ids.get(j),
                                randomLongBetween(currentRetentionLeases.get(ids.get(j)).retainingSequenceNumber(), Long.MAX_VALUE),
                                source));
            }
            assertBusy(() -&gt; {
                // check all retention leases have been synced to all replicas
                for (final ShardRouting replicaShard : clusterService().state().routingTable().index("tbl").shard(0).replicaShards()) {
                    final String replicaShardNodeId = replicaShard.currentNodeId();
                    final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
                    final IndexShard replica = internalCluster()
                            .getInstance(IndicesService.class, replicaShardNodeName)
                            .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
                    assertThat(replica.getRetentionLeases(), equalTo(primary.getRetentionLeases()));
<a name="8"></a>                }
            });
        }
    <font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testRetentionLeasesSyncOnRecovery() throws Exception {
        final int numberOfReplicas = 2 - scaledRandomIntBetween(0, 2);
        internalCluster</b></font>().ensureAtLeastNumDataNodes(1 + numberOfReplicas);
        /*
         * We effectively disable the background sync to ensure that the retention leases are not synced in the background so that the only
         * source of retention leases on the replicas would be from recovery.
         */
        execute(
            "create table doc.tbl (x int) clustered into 1 shards " +
            "with (" +
            "   number_of_replicas = 0, " +
            "   \"soft_deletes.enabled\" = true, " +
            "   \"soft_deletes.retention_lease.sync_interval\" = ?)",
            new Object[] {
                TimeValue.timeValueHours(24).getStringRep()
            }
        );
        allowNodes("tbl", 1);
        ensureYellow("tbl");
        execute("alter table doc.tbl set (number_of_replicas = ?)", new Object[] { numberOfReplicas });

        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
            .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
        final int length = randomIntBetween(1, 8);
        final Map&lt;String, RetentionLease&gt; currentRetentionLeases = new HashMap&lt;&gt;();
        for (int i = 0; i &lt; length; i++) {
            final String id = randomValueOtherThanMany(currentRetentionLeases.keySet()::contains, () -&gt; randomAlphaOfLength(8));
            final long retainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
            final String source = randomAlphaOfLength(8);
            final CountDownLatch latch = new CountDownLatch(1);
            final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
            currentRetentionLeases.put(id, primary.addRetentionLease(id, retainingSequenceNumber, source, listener));
            latch.await();
        }

        // Cause some recoveries to fail to ensure that retention leases are handled properly when retrying a recovery
        //
        execute("set global persistent \"indices.recovery.retry_delay_network\" = '100ms'");
        final Semaphore recoveriesToDisrupt = new Semaphore(scaledRandomIntBetween(0, 4));
        final MockTransportService primaryTransportService
            = (MockTransportService) internalCluster().getInstance(TransportService.class, primaryShardNodeName);
        primaryTransportService.addSendBehavior((connection, requestId, action, request, options) -&gt; {
            if (action.equals(PeerRecoveryTargetService.Actions.FINALIZE) &amp;&amp; recoveriesToDisrupt.tryAcquire()) {
                if (randomBoolean()) {
                    // return a ConnectTransportException to the START_RECOVERY action
                    final TransportService replicaTransportService
                        = internalCluster().getInstance(TransportService.class, connection.getNode().getName());
                    final DiscoveryNode primaryNode = primaryTransportService.getLocalNode();
                    replicaTransportService.disconnectFromNode(primaryNode);
                    AbstractSimpleTransportTestCase.connectToNode(replicaTransportService, primaryNode);
                } else {
                    // return an exception to the FINALIZE action
                    throw new ElasticsearchException("failing recovery for test purposes");
                }
            }
            connection.sendRequest(requestId, action, request, options);
        });

        // now allow the replicas to be allocated and wait for recovery to finalize
        allowNodes("tbl", 1 + numberOfReplicas);
        ensureGreen("tbl");

        // check current retention leases have been synced to all replicas
        for (final ShardRouting replicaShard : clusterService().state().routingTable().index("tbl").shard(0).replicaShards()) {
            final String replicaShardNodeId = replicaShard.currentNodeId();
            final String replicaShardNodeName = clusterService().state().nodes().get(replicaShardNodeId).getName();
            final IndexShard replica = internalCluster()
                .getInstance(IndicesService.class, replicaShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));
            final Map&lt;String, RetentionLease&gt; retentionLeasesOnReplica
                = RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.getRetentionLeases());
            assertThat(retentionLeasesOnReplica, equalTo(currentRetentionLeases));

            // check retention leases have been written on the replica; see RecoveryTarget#finalizeRecovery
<a name="4"></a>            assertThat(currentRetentionLeases,
                equalTo(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(replica.loadRetentionLeases())));
        }
    <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testCanAddRetentionLeaseUnderBlock() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runUnderBlockTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) </b></font>-&gt; {
                    final String nextId = randomValueOtherThan(idForInitialRetentionLease, () -&gt; randomAlphaOfLength(8));
                    final long nextRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    primary.addRetentionLease(nextId, nextRetainingSequenceNumber, nextSource, listener);
                },
                primary -&gt; {});
    }

    @Test
    public void testCanRenewRetentionLeaseUnderBlock() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        final long initialRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
        final AtomicReference&lt;RetentionLease&gt; retentionLease = new AtomicReference&lt;&gt;();
        runUnderBlockTest(
                idForInitialRetentionLease,
                initialRetainingSequenceNumber,
                (primary, listener) -&gt; {
                    final long nextRetainingSequenceNumber = randomLongBetween(initialRetainingSequenceNumber, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    retentionLease.set(primary.renewRetentionLease(idForInitialRetentionLease, nextRetainingSequenceNumber, nextSource));
                    listener.onResponse(new ReplicationResponse());
                },
                primary -&gt; {
                    try {
                        /*
                         * If the background renew was able to execute, then the retention leases were persisted to disk. There is no other
                         * way for the current retention leases to end up written to disk so we assume that if they are written to disk, it
                         * implies that the background sync was able to execute under a block.
                         */
                        assertBusy(() -&gt; assertThat(
                            RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases()).values(),
                            contains(retentionLease.get())));
                    } catch (final Exception e) {
                        fail(e.toString());
                    }
                });

    }

    public void testCanRemoveRetentionLeasesUnderBlock() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runUnderBlockTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) -&gt; primary.removeRetentionLease(idForInitialRetentionLease, listener),
                indexShard -&gt; {});
    }

    private void runUnderBlockTest(
            final String idForInitialRetentionLease,
            final long initialRetainingSequenceNumber,
            final BiConsumer&lt;IndexShard, ActionListener&lt;ReplicationResponse&gt;&gt; primaryConsumer,
            final Consumer&lt;IndexShard&gt; afterSync) throws InterruptedException {
        execute(
            "create table doc.tbl (x int) clustered into 1 shards " +
            "with (" +
            "   number_of_replicas = 0, " +
            "   \"soft_deletes.enabled\" = true, " +
            "   \"soft_deletes.retention_lease.sync_interval\" = '1s' " +
            ")"
        );
        ensureGreen("tbl");

        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
            .getInstance(IndicesService.class, primaryShardNodeName)
            .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));

        final String source = randomAlphaOfLength(8);
        final CountDownLatch latch = new CountDownLatch(1);
        final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
        primary.addRetentionLease(idForInitialRetentionLease, initialRetainingSequenceNumber, source, listener);
        latch.await();

        final String block = randomFrom("read_only", "read_only_allow_delete", "read", "write", "metadata");

        execute("alter table doc.tbl set (\"blocks." + block + "\" = true)");
        try {
            final CountDownLatch actionLatch = new CountDownLatch(1);
            final AtomicBoolean success = new AtomicBoolean();

            primaryConsumer.accept(
                primary,
                new ActionListener&lt;ReplicationResponse&gt;() {

                    @Override
                    public void onResponse(final ReplicationResponse replicationResponse) {
                        success.set(true);
                        actionLatch.countDown();
                    }

                    @Override
                    public void onFailure(final Exception e) {
                        fail(e.toString());
                    }

                }
            );
            actionLatch.await();
            assertTrue(success.get());
            afterSync.accept(primary);
<a name="3"></a>        } finally {
            execute("alter table doc.tbl reset (\"blocks." + block + "\")");
        }
    <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testCanAddRetentionLeaseWithoutWaitingForShards() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runWaitForShardsTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) </b></font>-&gt; {
                    final String nextId = randomValueOtherThan(idForInitialRetentionLease, () -&gt; randomAlphaOfLength(8));
                    final long nextRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    primary.addRetentionLease(nextId, nextRetainingSequenceNumber, nextSource, listener);
                },
                primary -&gt; {});
    }

    @Test
    public void testCanRenewRetentionLeaseWithoutWaitingForShards() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        final long initialRetainingSequenceNumber = randomLongBetween(0, Long.MAX_VALUE);
        final AtomicReference&lt;RetentionLease&gt; retentionLease = new AtomicReference&lt;&gt;();
        runWaitForShardsTest(
                idForInitialRetentionLease,
                initialRetainingSequenceNumber,
                (primary, listener) -&gt; {
                    final long nextRetainingSequenceNumber = randomLongBetween(initialRetainingSequenceNumber, Long.MAX_VALUE);
                    final String nextSource = randomAlphaOfLength(8);
                    retentionLease.set(primary.renewRetentionLease(idForInitialRetentionLease, nextRetainingSequenceNumber, nextSource));
                    listener.onResponse(new ReplicationResponse());
                },
                primary -&gt; {
                    try {
                        /*
                         * If the background renew was able to execute, then the retention leases were persisted to disk. There is no other
                         * way for the current retention leases to end up written to disk so we assume that if they are written to disk, it
                         * implies that the background sync was able to execute despite wait for shards being set on the index.
                         */
                        assertBusy(() -&gt; assertThat(
                            RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(primary.loadRetentionLeases()).values(),
                            contains(retentionLease.get())));
                    } catch (final Exception e) {
                        fail(e.toString());
<a name="2"></a>                    }
                });

    <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>}

    @Test
    public void testCanRemoveRetentionLeasesWithoutWaitingForShards() throws InterruptedException {
        final String idForInitialRetentionLease = randomAlphaOfLength(8);
        runWaitForShardsTest(
                idForInitialRetentionLease,
                randomLongBetween(0, Long.MAX_VALUE),
                (primary, listener) </b></font>-&gt; primary.removeRetentionLease(idForInitialRetentionLease, listener),
                primary -&gt; {});
    }

    private void runWaitForShardsTest(
            final String idForInitialRetentionLease,
            final long initialRetainingSequenceNumber,
            final BiConsumer&lt;IndexShard, ActionListener&lt;ReplicationResponse&gt;&gt; primaryConsumer,
            final Consumer&lt;IndexShard&gt; afterSync) throws InterruptedException {
        final int numDataNodes = internalCluster().numDataNodes();
        execute(
            "create table doc.tbl (x int) clustered into 1 shards " +
            "with (" +
            "   number_of_replicas = ?, " +
            "   \"soft_deletes.enabled\" = true," +
            "   \"soft_deletes.retention_lease.sync_interval\" = ?)",
            new Object[] {
                numDataNodes == 1 ? 0 : numDataNodes - 1,
                TimeValue.timeValueSeconds(1).getStringRep()
            }
        );
        ensureYellowAndNoInitializingShards("tbl");
        assertFalse(client().admin().cluster().prepareHealth("tbl").setWaitForActiveShards(numDataNodes).get().isTimedOut());

        final String primaryShardNodeId = clusterService().state().routingTable().index("tbl").shard(0).primaryShard().currentNodeId();
        final String primaryShardNodeName = clusterService().state().nodes().get(primaryShardNodeId).getName();
        final IndexShard primary = internalCluster()
                .getInstance(IndicesService.class, primaryShardNodeName)
                .getShardOrNull(new ShardId(resolveIndex("tbl"), 0));

        final String source = randomAlphaOfLength(8);
        final CountDownLatch latch = new CountDownLatch(1);
        final ActionListener&lt;ReplicationResponse&gt; listener = ActionListener.wrap(r -&gt; latch.countDown(), e -&gt; fail(e.toString()));
        primary.addRetentionLease(idForInitialRetentionLease, initialRetainingSequenceNumber, source, listener);
        latch.await();

        final String waitForActiveValue = randomBoolean() ? "all" : Integer.toString(numDataNodes);

        execute("alter table doc.tbl set (\"write.wait_for_active_shards\" = ?)", new Object[] { waitForActiveValue });
        final CountDownLatch actionLatch = new CountDownLatch(1);
        final AtomicBoolean success = new AtomicBoolean();

        primaryConsumer.accept(
                primary,
                new ActionListener&lt;ReplicationResponse&gt;() {

                    @Override
                    public void onResponse(final ReplicationResponse replicationResponse) {
                        success.set(true);
                        actionLatch.countDown();
                    }

                    @Override
                    public void onFailure(final Exception e) {
                        fail(e.toString());
                    }

                });
        actionLatch.await();
        assertTrue(success.get());
        afterSync.accept(primary);
    }

    private static void failWithException(Exception e) {
        throw new AssertionError("unexpected", e);
    }

    private static ActionListener&lt;ReplicationResponse&gt; countDownLatchListener(CountDownLatch latch) {
        return ActionListener.wrap(r -&gt; latch.countDown(), RetentionLeaseIT::failWithException);
    }

}
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerHTML.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
