<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for rpmbuild_pkgbuild.py &amp; vmware.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for rpmbuild_pkgbuild.py &amp; vmware.py
      </h3>
<h1 align="center">
        0.8%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>rpmbuild_pkgbuild.py (3.276699%)<th>vmware.py (0.45592704%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(13-27)<td><a href="#" name="0">(116-130)</a><td align="center"><font color="#ff0000">14</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(84-87)<td><a href="#" name="1">(4101-4108)</a><td align="center"><font color="#ec0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>rpmbuild_pkgbuild.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import errno
2 import functools
3 import logging
4 import os
5 import re
6 import shutil
7 import tempfile
8 import time
9 import traceback
10 import urllib.parse
11 import salt.utils.files
12 import salt.utils.path
13 import salt.utils.user
14 import</b></font> salt.utils.vt
15 from salt.exceptions import CommandExecutionError, SaltInvocationError
16 HAS_LIBS = False
17 try:
18     import gnupg  # pylint: disable=unused-import
19     import salt.modules.gpg
20     HAS_LIBS = True
21 except ImportError:
22     pass
23 log = logging.getLogger(__name__)
24 __virtualname__ = "pkgbuild"
25 def __virtual__():
26     missing_util = False
27     utils_reqd = ["gpg", "rpm", "rpmbuild", "mock", "createrepo"]
28     for named_util in utils_reqd:
29         if not salt.utils.path.which(named_util):
30             missing_util = True
31             break
32     if HAS_LIBS and not missing_util:
33         if __grains__.get("os_family", False) in ("RedHat", "Suse"):
34             return __virtualname__
35         else:
36             return "rpmbuild"
37     else:
38         return (
39             False,
40             "The rpmbuild module could not be loaded: requires python-gnupg, "
41             "gpg, rpm, rpmbuild, mock and createrepo utilities to be installed",
42         )
43 def _create_rpmmacros(runas="root"):
44     home = os.path.expanduser("~" + runas)
45     rpmbuilddir = os.path.join(home, "rpmbuild")
46     if not os.path.isdir(rpmbuilddir):
47         __salt__["file.makedirs_perms"](name=rpmbuilddir, user=runas, group="mock")
48     mockdir = os.path.join(home, "mock")
49     if not os.path.isdir(mockdir):
50 <a name="1"></a>        __salt__["file.makedirs_perms"](name=mockdir, user=runas, group="mock")
51     rpmmacros = os.path.join(home, ".rpmmacros")
52     with salt.utils.files<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.fopen(rpmmacros, "w") as afile:
53         afile.write(salt.utils.stringutils.to_str("%_topdir {}\n".format(rpmbuilddir)))
54         afile.write("%signature gpg\n")
55         afile.</b></font>write("%_source_filedigest_algorithm 8\n")
56         afile.write("%_binary_filedigest_algorithm 8\n")
57         afile.write("%_gpg_name packaging@saltstack.com\n")
58 def _mk_tree(runas="root"):
59     basedir = tempfile.mkdtemp()
60     paths = ["BUILD", "RPMS", "SOURCES", "SPECS", "SRPMS"]
61     for path in paths:
62         full = os.path.join(basedir, path)
63         __salt__["file.makedirs_perms"](name=full, user=runas, group="mock")
64     return basedir
65 def _get_spec(tree_base, spec, template, saltenv="base"):
66     spec_tgt = os.path.basename(spec)
67     dest = os.path.join(tree_base, "SPECS", spec_tgt)
68     return __salt__["cp.get_url"](spec, dest, saltenv=saltenv)
69 def _get_src(tree_base, source, saltenv="base", runas="root"):
70     parsed = urllib.parse.urlparse(source)
71     sbase = os.path.basename(source)
72     dest = os.path.join(tree_base, "SOURCES", sbase)
73     if parsed.scheme:
74         lsrc = __salt__["cp.get_url"](source, dest, saltenv=saltenv)
75     else:
76         shutil.copy(source, dest)
77     __salt__["file.chown"](path=dest, user=runas, group="mock")
78 def _get_distset(tgt):
79     tgtattrs = tgt.split("-")
80     if tgtattrs[0] == "amzn2":
81         distset = '--define "dist .{}"'.format(tgtattrs[0])
82     elif tgtattrs[1] in ["6", "7", "8"]:
83         distset = '--define "dist .el{}"'.format(tgtattrs[1])
84     else:
85         distset = ""
86     return distset
87 def _get_deps(deps, tree_base, saltenv="base"):
88     deps_list = ""
89     if deps is None:
90         return deps_list
91     if not isinstance(deps, list):
92         raise SaltInvocationError(
93             "'deps' must be a Python list or comma-separated string"
94         )
95     for deprpm in deps:
96         parsed = urllib.parse._urlparse(deprpm)
97         depbase = os.path.basename(deprpm)
98         dest = os.path.join(tree_base, depbase)
99         if parsed.scheme:
100             __salt__["cp.get_url"](deprpm, dest, saltenv=saltenv)
101         else:
102             shutil.copy(deprpm, dest)
103         deps_list += " {}".format(dest)
104     return deps_list
105 def _check_repo_gpg_phrase_utils():
106     util_name = "/usr/libexec/gpg-preset-passphrase"
107     if __salt__["file.file_exists"](util_name):
108         return True
109     else:
110         raise CommandExecutionError(
111             "utility '{}' needs to be installed".format(util_name)
112         )
113 def _get_gpg_key_resources(keyid, env, use_passphrase, gnupghome, runas):
114     local_keygrip_to_use = None
115     local_key_fingerprint = None
116     local_keyid = None
117     local_uids = None
118     define_gpg_name = ""
119     phrase = ""
120     retrc = 0
121     use_gpg_agent = False
122     if (
123         __grains__.get("os_family") == "RedHat"
124         and __grains__.get("osmajorrelease") &gt;= 8
125     ):
126         use_gpg_agent = True
127     if keyid is not None:
128         pkg_pub_key_file = "{}/{}".format(
129             gnupghome, __salt__["pillar.get"]("gpg_pkg_pub_keyname", None)
130         )
131         pkg_priv_key_file = "{}/{}".format(
132             gnupghome, __salt__["pillar.get"]("gpg_pkg_priv_keyname", None)
133         )
134         if pkg_pub_key_file is None or pkg_priv_key_file is None:
135             raise SaltInvocationError(
136                 "Pillar data should contain Public and Private keys associated with"
137                 " 'keyid'"
138             )
139         try:
140             __salt__["gpg.import_key"](
141                 user=runas, filename=pkg_pub_key_file, gnupghome=gnupghome
142             )
143             __salt__["gpg.import_key"](
144                 user=runas, filename=pkg_priv_key_file, gnupghome=gnupghome
145             )
146         except SaltInvocationError:
147             raise SaltInvocationError(
148                 "Public and Private key files associated with Pillar data and 'keyid' "
149                 "{} could not be found".format(keyid)
150             )
151         local_keys = __salt__["gpg.list_keys"](user=runas, gnupghome=gnupghome)
152         for gpg_key in local_keys:
153             if keyid == gpg_key["keyid"][8:]:
154                 local_uids = gpg_key["uids"]
155                 local_keyid = gpg_key["keyid"]
156                 if use_gpg_agent:
157                     local_keygrip_to_use = gpg_key["fingerprint"]
158                     local_key_fingerprint = gpg_key["fingerprint"]
159                 break
160         if use_gpg_agent:
161             cmd = "gpg --with-keygrip --list-secret-keys"
162             local_keys2_keygrip = __salt__["cmd.run"](cmd, runas=runas, env=env)
163             local_keys2 = iter(local_keys2_keygrip.splitlines())
164             try:
165                 for line in local_keys2:
166                     if line.startswith("sec"):
167                         line_fingerprint = next(local_keys2).lstrip().rstrip()
168                         if local_key_fingerprint == line_fingerprint:
169                             lkeygrip = next(local_keys2).split("=")
170                             local_keygrip_to_use = lkeygrip[1].lstrip().rstrip()
171                             break
172             except StopIteration:
173                 raise SaltInvocationError(
174                     "unable to find keygrip associated with fingerprint '{}' for keyid"
175                     " '{}'".format(local_key_fingerprint, local_keyid)
176                 )
177         if local_keyid is None:
178             raise SaltInvocationError(
179                 "The key ID '{}' was not found in GnuPG keyring at '{}'".format(
180                     keyid, gnupghome
181                 )
182             )
183         if use_passphrase:
184             phrase = __salt__["pillar.get"]("gpg_passphrase")
185             if use_gpg_agent:
186                 _check_repo_gpg_phrase_utils()
187                 cmd = (
188                     "/usr/libexec/gpg-preset-passphrase --verbose --preset "
189                     '--passphrase "{}" {}'.format(phrase, local_keygrip_to_use)
190                 )
191                 retrc = __salt__["cmd.retcode"](cmd, runas=runas, env=env)
192                 if retrc != 0:
193                     raise SaltInvocationError(
194                         "Failed to preset passphrase, error {1}, "
195                         "check logs for further details".format(retrc)
196                     )
197         if local_uids:
198             define_gpg_name = (
199                 "--define='%_signature gpg' --define='%_gpg_name {}'".format(
200                     local_uids[0]
201                 )
202             )
203         cmd = "rpm --import {}".format(pkg_pub_key_file)
204         retrc = __salt__["cmd.retcode"](cmd, runas=runas, use_vt=True)
205         if retrc != 0:
206             raise SaltInvocationError(
207                 "Failed to import public key from file {} with return "
208                 "error {}, check logs for further details".format(
209                     pkg_pub_key_file, retrc
210                 )
211             )
212     return (use_gpg_agent, local_keyid, define_gpg_name, phrase)
213 def _sign_file(runas, define_gpg_name, phrase, abs_file, timeout):
214     SIGN_PROMPT_RE = re.compile(r"Enter pass phrase: ", re.M)
215     interval = 0.5
216     number_retries = timeout / interval
217     times_looped = 0
218     error_msg = "Failed to sign file {}".format(abs_file)
219     cmd = "rpm {} --addsign {}".format(define_gpg_name, abs_file)
220     preexec_fn = functools.partial(salt.utils.user.chugid_and_umask, runas, None)
221     try:
222         stdout, stderr = None, None
223         proc = salt.utils.vt.Terminal(
224             cmd,
225             shell=True,
226             preexec_fn=preexec_fn,
227             stream_stdout=True,
228             stream_stderr=True,
229         )
230         while proc.has_unread_data:
231             stdout, stderr = proc.recv()
232             if stdout and SIGN_PROMPT_RE.search(stdout):
233                 proc.sendline(phrase)
234             else:
235                 times_looped += 1
236             if times_looped &gt; number_retries:
237                 raise SaltInvocationError(
238                     "Attemping to sign file {} failed, timed out after {} seconds".format(
239                         abs_file, int(times_looped * interval)
240                     )
241                 )
242             time.sleep(interval)
243         proc_exitstatus = proc.exitstatus
244         if proc_exitstatus != 0:
245             raise SaltInvocationError(
246                 "Signing file {} failed with proc.status {}".format(
247                     abs_file, proc_exitstatus
248                 )
249             )
250     except salt.utils.vt.TerminalException as err:
251         trace = traceback.format_exc()
252         log.error(error_msg, err, trace)
253     finally:
254         proc.close(terminate=True, kill=True)
255 def _sign_files_with_gpg_agent(runas, local_keyid, abs_file, repodir, env, timeout):
256     cmd = "rpmsign --verbose  --key-id={} --addsign {}".format(local_keyid, abs_file)
257     retrc = __salt__["cmd.retcode"](cmd, runas=runas, cwd=repodir, use_vt=True, env=env)
258     if retrc != 0:
259         raise SaltInvocationError(
260             "Signing encountered errors for command '{}', "
261             "return error {}, check logs for further details".format(cmd, retrc)
262         )
263 def make_src_pkg(
264     dest_dir, spec, sources, env=None, template=None, saltenv="base", runas="root"
265 ):
266     _create_rpmmacros(runas)
267     tree_base = _mk_tree(runas)
268     spec_path = _get_spec(tree_base, spec, template, saltenv)
269     __salt__["file.chown"](path=spec_path, user=runas, group="mock")
270     __salt__["file.chown"](path=tree_base, user=runas, group="mock")
271     if isinstance(sources, str):
272         sources = sources.split(",")
273     for src in sources:
274         _get_src(tree_base, src, saltenv, runas)
275     cmd = 'rpmbuild --verbose --define "_topdir {}" -bs --define "dist .el6" {}'.format(
276         tree_base, spec_path
277     )
278     retrc = __salt__["cmd.retcode"](cmd, runas=runas)
279     if retrc != 0:
280         raise SaltInvocationError(
281             "Make source package for destination directory {}, spec {}, sources {},"
282             " failed with return error {}, check logs for further details".format(
283                 dest_dir, spec, sources, retrc
284             )
285         )
286     srpms = os.path.join(tree_base, "SRPMS")
287     ret = []
288     if not os.path.isdir(dest_dir):
289         __salt__["file.makedirs_perms"](name=dest_dir, user=runas, group="mock")
290     for fn_ in os.listdir(srpms):
291         full = os.path.join(srpms, fn_)
292         tgt = os.path.join(dest_dir, fn_)
293         shutil.copy(full, tgt)
294         ret.append(tgt)
295     return ret
296 def build(
297     runas,
298     tgt,
299     dest_dir,
300     spec,
301     sources,
302     deps,
303     env,
304     template,
305     saltenv="base",
306     log_dir="/var/log/salt/pkgbuild",
307 ):
308     ret = {}
309     try:
310         __salt__["file.chown"](path=dest_dir, user=runas, group="mock")
311     except OSError as exc:
312         if exc.errno != errno.EEXIST:
313             raise
314     srpm_dir = os.path.join(dest_dir, "SRPMS")
315     srpm_build_dir = tempfile.mkdtemp()
316     try:
317         srpms = make_src_pkg(
318             srpm_build_dir, spec, sources, env, template, saltenv, runas
319         )
320     except Exception as exc:  # pylint: disable=broad-except
321         shutil.rmtree(srpm_build_dir)
322         log.error("Failed to make src package")
323         return ret
324     distset = _get_distset(tgt)
325     noclean = ""
326     deps_dir = tempfile.mkdtemp()
327     deps_list = _get_deps(deps, deps_dir, saltenv)
328     retrc = 0
329     for srpm in srpms:
330         dbase = os.path.dirname(srpm)
331         results_dir = tempfile.mkdtemp()
332         try:
333             __salt__["file.chown"](path=dbase, user=runas, group="mock")
334             __salt__["file.chown"](path=results_dir, user=runas, group="mock")
335             cmd = "mock --root={} --resultdir={} --init".format(tgt, results_dir)
336             retrc |= __salt__["cmd.retcode"](cmd, runas=runas)
337             if deps_list and not deps_list.isspace():
338                 cmd = "mock --root={} --resultdir={} --install {} {}".format(
339                     tgt, results_dir, deps_list, noclean
340                 )
341                 retrc |= __salt__["cmd.retcode"](cmd, runas=runas)
342                 noclean += " --no-clean"
343             cmd = "mock --root={} --resultdir={} {} {} {}".format(
344                 tgt, results_dir, distset, noclean, srpm
345             )
346             retrc |= __salt__["cmd.retcode"](cmd, runas=runas)
347             cmdlist = [
348                 "rpm",
349                 "-qp",
350                 "--queryformat",
351                 "{0}/%{{name}}/%{{version}}-%{{release}}".format(log_dir),
352                 srpm,
353             ]
354             log_dest = __salt__["cmd.run_stdout"](cmdlist, python_shell=False)
355             for filename in os.listdir(results_dir):
356                 full = os.path.join(results_dir, filename)
357                 if filename.endswith("src.rpm"):
358                     sdest = os.path.join(srpm_dir, filename)
359                     try:
360                         __salt__["file.makedirs_perms"](
361                             name=srpm_dir, user=runas, group="mock"
362                         )
363                     except OSError as exc:
364                         if exc.errno != errno.EEXIST:
365                             raise
366                     shutil.copy(full, sdest)
367                     ret.setdefault("Source Packages", []).append(sdest)
368                 elif filename.endswith(".rpm"):
369                     bdist = os.path.join(dest_dir, filename)
370                     shutil.copy(full, bdist)
371                     ret.setdefault("Packages", []).append(bdist)
372                 else:
373                     log_file = os.path.join(log_dest, filename)
374                     try:
375                         __salt__["file.makedirs_perms"](
376                             name=log_dest, user=runas, group="mock"
377                         )
378                     except OSError as exc:
379                         if exc.errno != errno.EEXIST:
380                             raise
381                     shutil.copy(full, log_file)
382                     ret.setdefault("Log Files", []).append(log_file)
383         except Exception as exc:  # pylint: disable=broad-except
384             log.error("Error building from %s: %s", srpm, exc)
385         finally:
386             shutil.rmtree(results_dir)
387     if retrc != 0:
388         raise SaltInvocationError(
389             "Building packages for destination directory {}, spec {}, sources {},"
390             " failed with return error {}, check logs for further details".format(
391                 dest_dir, spec, sources, retrc
392             )
393         )
394     shutil.rmtree(deps_dir)
395     shutil.rmtree(srpm_build_dir)
396     return ret
397 def make_repo(
398     repodir,
399     keyid=None,
400     env=None,
401     use_passphrase=False,
402     gnupghome="/etc/salt/gpgkeys",
403     runas="root",
404     timeout=15.0,
405 ):
406     home = os.path.expanduser("~" + runas)
407     rpmmacros = os.path.join(home, ".rpmmacros")
408     if not os.path.exists(rpmmacros):
409         _create_rpmmacros(runas)
410     if gnupghome and env is None:
411         env = {}
412         env["GNUPGHOME"] = gnupghome
413     use_gpg_agent, local_keyid, define_gpg_name, phrase = _get_gpg_key_resources(
414         keyid, env, use_passphrase, gnupghome, runas
415     )
416     for fileused in os.listdir(repodir):
417         if fileused.endswith(".rpm"):
418             abs_file = os.path.join(repodir, fileused)
419             if use_gpg_agent:
420                 _sign_files_with_gpg_agent(
421                     runas, local_keyid, abs_file, repodir, env, timeout
422                 )
423             else:
424                 _sign_file(runas, define_gpg_name, phrase, abs_file, timeout)
425     cmd = "createrepo --update {}".format(repodir)
426     retrc = __salt__["cmd.run_all"](cmd, runas=runas)
427     return retrc
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>vmware.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import logging
2 import os.path
3 import pprint
4 import re
5 import subprocess
6 import time
7 from random import randint
8 import salt.config as config
9 import salt.utils.cloud
10 import salt.utils.network
11 import salt.utils.stringutils
12 import salt.utils.vmware
13 import salt.utils.xmlutil
14 from</b></font> salt.exceptions import SaltCloudSystemExit
15 try:
16     from pyVmomi import vim  # pylint: disable=no-name-in-module
17     HAS_PYVMOMI = True
18 except ImportError:
19     HAS_PYVMOMI = False
20 try:
21     from requests.packages.urllib3 import (
22         disable_warnings,
23     )  # pylint: disable=no-name-in-module
24     disable_warnings()
25 except ImportError:
26     pass
27 ESX_5_5_NAME_PORTION = "VMware ESXi 5.5"
28 SAFE_ESX_5_5_CONTROLLER_KEY_INDEX = 200
29 FLATTEN_DISK_FULL_CLONE = "moveAllDiskBackingsAndDisallowSharing"
30 COPY_ALL_DISKS_FULL_CLONE = "moveAllDiskBackingsAndAllowSharing"
31 CURRENT_STATE_LINKED_CLONE = "moveChildMostDiskBacking"
32 QUICK_LINKED_CLONE = "createNewChildDiskBacking"
33 IP_RE = r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
34 log = logging.getLogger(__name__)
35 __virtualname__ = "vmware"
36 def __virtual__():
37     if get_configured_provider() is False:
38         return False
39     if get_dependencies() is False:
40         return False
41     return __virtualname__
42 def _get_active_provider_name():
43     try:
44         return __active_provider_name__.value()
45     except AttributeError:
46         return __active_provider_name__
47 def get_configured_provider():
48     return config.is_provider_configured(
49         __opts__,
50         _get_active_provider_name() or __virtualname__,
51         (
52             "url",
53             "user",
54             "password",
55         ),
56     )
57 def get_dependencies():
58     deps = {
59         "pyVmomi": HAS_PYVMOMI,
60     }
61     return config.check_driver_dependencies(__virtualname__, deps)
62 def script(vm_):
63     script_name = config.get_cloud_config_value("script", vm_, __opts__)
64     if not script_name:
65         script_name = "bootstrap-salt"
66     return salt.utils.cloud.os_script(
67         script_name,
68         vm_,
69         __opts__,
70         salt.utils.cloud.salt_config_to_yaml(
71             salt.utils.cloud.minion_config(__opts__, vm_)
72         ),
73     )
74 def _str_to_bool(var):
75     if isinstance(var, bool):
76         return var
77     if isinstance(var, str):
78         return True if var.lower() == "true" else False
79     return None
80 def _get_si():
81     url = config.get_cloud_config_value(
82         "url", get_configured_provider(), __opts__, search_global=False
83     )
84     username = config.get_cloud_config_value(
85         "user", get_configured_provider(), __opts__, search_global=False
86     )
87     password = config.get_cloud_config_value(
88         "password", get_configured_provider(), __opts__, search_global=False
89     )
90     protocol = config.get_cloud_config_value(
91         "protocol",
92         get_configured_provider(),
93         __opts__,
94         search_global=False,
95         default="https",
96     )
97     port = config.get_cloud_config_value(
98         "port", get_configured_provider(), __opts__, search_global=False, default=443
99     )
100     verify_ssl = config.get_cloud_config_value(
101         "verify_ssl",
102         get_configured_provider(),
103         __opts__,
104         search_global=False,
105         default=True,
106     )
107     return salt.utils.vmware.get_service_instance(
108         url, username, password, protocol=protocol, port=port, verify_ssl=verify_ssl
109     )
110 def _edit_existing_hard_disk_helper(disk, size_kb=None, size_gb=None, mode=None):
111     if size_kb or size_gb:
112         disk.capacityInKB = size_kb if size_kb else int(size_gb * 1024.0 * 1024.0)
113     if mode:
114         disk.backing.diskMode = mode
115     disk_spec = vim.vm.device.VirtualDeviceSpec()
116     disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
117     disk_spec.device = disk
118     return disk_spec
119 def _add_new_hard_disk_helper(
120     disk_label,
121     size_gb,
122     unit_number,
123     controller_key=1000,
124     thin_provision=False,
125     eagerly_scrub=False,
126     datastore=None,
127     vm_name=None,
128 ):
129     random_key = randint(-2099, -2000)
130     size_kb = int(size_gb * 1024.0 * 1024.0)
131     disk_spec = vim.vm.device.VirtualDeviceSpec()
132     disk_spec.fileOperation = "create"
133     disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
134     disk_spec.device = vim.vm.device.VirtualDisk()
135     disk_spec.device.key = random_key
136     disk_spec.device.deviceInfo = vim.Description()
137     disk_spec.device.deviceInfo.label = disk_label
138     disk_spec.device.deviceInfo.summary = "{} GB".format(size_gb)
139     disk_spec.device.backing = vim.vm.device.VirtualDisk.FlatVer2BackingInfo()
140     disk_spec.device.backing.thinProvisioned = thin_provision
141     disk_spec.device.backing.eagerlyScrub = eagerly_scrub
142     disk_spec.device.backing.diskMode = "persistent"
143     if datastore:
144         datastore_ref = salt.utils.vmware.get_mor_using_container_view(
145             _get_si(), vim.Datastore, datastore
146         )
147         if not datastore_ref:
148             datastore_cluster_ref = salt.utils.vmware.get_mor_using_container_view(
149                 _get_si(), vim.StoragePod, datastore
150             )
151             if not datastore_cluster_ref:
152                 raise SaltCloudSystemExit(
153                     "Specified datastore/datastore cluster ({}) for disk ({}) does not"
154                     " exist".format(datastore, disk_label)
155                 )
156             datastore_list = salt.utils.vmware.get_datastores(
157                 _get_si(), datastore_cluster_ref, get_all_datastores=True
158             )
159             datastore_free_space = 0
160             for ds_ref in datastore_list:
161                 log.trace(
162                     "Found datastore (%s) with free space (%s) in datastore "
163                     "cluster (%s)",
164                     ds_ref.name,
165                     ds_ref.summary.freeSpace,
166                     datastore,
167                 )
168                 if (
169                     ds_ref.summary.accessible
170                     and ds_ref.summary.freeSpace &gt; datastore_free_space
171                 ):
172                     datastore_free_space = ds_ref.summary.freeSpace
173                     datastore_ref = ds_ref
174             if not datastore_ref:
175                 raise SaltCloudSystemExit(
176                     "Specified datastore cluster ({}) for disk ({}) does not have any"
177                     " accessible datastores available".format(datastore, disk_label)
178                 )
179         datastore_path = "[" + str(datastore_ref.name) + "] " + vm_name
180         disk_spec.device.backing.fileName = datastore_path + "/" + disk_label + ".vmdk"
181         disk_spec.device.backing.datastore = datastore_ref
182         log.trace(
183             "Using datastore (%s) for disk (%s), vm_name (%s)",
184             datastore_ref.name,
185             disk_label,
186             vm_name,
187         )
188     disk_spec.device.controllerKey = controller_key
189     disk_spec.device.unitNumber = unit_number
190     disk_spec.device.capacityInKB = size_kb
191     return disk_spec
192 def _edit_existing_network_adapter(
193     network_adapter, new_network_name, adapter_type, switch_type, container_ref=None
194 ):
195     adapter_type.strip().lower()
196     switch_type.strip().lower()
197     if adapter_type in ["vmxnet", "vmxnet2", "vmxnet3", "e1000", "e1000e"]:
198         edited_network_adapter = salt.utils.vmware.get_network_adapter_type(
199             adapter_type
200         )
201         if isinstance(network_adapter, type(edited_network_adapter)):
202             edited_network_adapter = network_adapter
203         else:
204             log.debug(
205                 "Changing type of '%s' from '%s' to '%s'",
206                 network_adapter.deviceInfo.label,
207                 type(network_adapter).__name__.rsplit(".", 1)[1][7:].lower(),
208                 adapter_type,
209             )
210     else:
211         if adapter_type:
212             log.error(
213                 "Cannot change type of '%s' to '%s'. Not changing type",
214                 network_adapter.deviceInfo.label,
215                 adapter_type,
216             )
217         edited_network_adapter = network_adapter
218     if switch_type == "standard":
219         network_ref = salt.utils.vmware.get_mor_by_property(
220             _get_si(), vim.Network, new_network_name, container_ref=container_ref
221         )
222         edited_network_adapter.backing = (
223             vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()
224         )
225         edited_network_adapter.backing.deviceName = new_network_name
226         edited_network_adapter.backing.network = network_ref
227     elif switch_type == "distributed":
228         network_ref = salt.utils.vmware.get_mor_by_property(
229             _get_si(),
230             vim.dvs.DistributedVirtualPortgroup,
231             new_network_name,
232             container_ref=container_ref,
233         )
234         dvs_port_connection = vim.dvs.PortConnection(
235             portgroupKey=network_ref.key,
236             switchUuid=network_ref.config.distributedVirtualSwitch.uuid,
237         )
238         edited_network_adapter.backing = (
239             vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()
240         )
241         edited_network_adapter.backing.port = dvs_port_connection
242     else:
243         if not switch_type:
244             err_msg = (
245                 "The switch type to be used by '{}' has not been specified".format(
246                     network_adapter.deviceInfo.label
247                 )
248             )
249         else:
250             err_msg = "Cannot create '{}'. Invalid/unsupported switch type '{}'".format(
251                 network_adapter.deviceInfo.label, switch_type
252             )
253         raise SaltCloudSystemExit(err_msg)
254     edited_network_adapter.key = network_adapter.key
255     edited_network_adapter.deviceInfo = network_adapter.deviceInfo
256     edited_network_adapter.deviceInfo.summary = new_network_name
257     edited_network_adapter.connectable = network_adapter.connectable
258     edited_network_adapter.slotInfo = network_adapter.slotInfo
259     edited_network_adapter.controllerKey = network_adapter.controllerKey
260     edited_network_adapter.unitNumber = network_adapter.unitNumber
261     edited_network_adapter.addressType = network_adapter.addressType
262     edited_network_adapter.macAddress = network_adapter.macAddress
263     edited_network_adapter.wakeOnLanEnabled = network_adapter.wakeOnLanEnabled
264     network_spec = vim.vm.device.VirtualDeviceSpec()
265     network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
266     network_spec.device = edited_network_adapter
267     return network_spec
268 def _add_new_network_adapter_helper(
269     network_adapter_label,
270     network_name,
271     adapter_type,
272     switch_type,
273     mac,
274     container_ref=None,
275 ):
276     random_key = randint(-4099, -4000)
277     adapter_type.strip().lower()
278     switch_type.strip().lower()
279     network_spec = vim.vm.device.VirtualDeviceSpec()
280     if adapter_type in ["vmxnet", "vmxnet2", "vmxnet3", "e1000", "e1000e"]:
281         network_spec.device = salt.utils.vmware.get_network_adapter_type(adapter_type)
282     else:
283         if not adapter_type:
284             log.debug(
285                 "The type of '%s' has not been specified. "
286                 "Creating default type 'vmxnet3'",
287                 network_adapter_label,
288             )
289         else:
290             log.error(
291                 "Cannot create network adapter of type '%s'. "
292                 "Creating '%s' of default type 'vmxnet3'",
293                 adapter_type,
294                 network_adapter_label,
295             )
296         network_spec.device = vim.vm.device.VirtualVmxnet3()
297     network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
298     if switch_type == "standard":
299         network_spec.device.backing = (
300             vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()
301         )
302         network_spec.device.backing.deviceName = network_name
303         network_spec.device.backing.network = salt.utils.vmware.get_mor_by_property(
304             _get_si(), vim.Network, network_name, container_ref=container_ref
305         )
306     elif switch_type == "distributed":
307         network_ref = salt.utils.vmware.get_mor_by_property(
308             _get_si(),
309             vim.dvs.DistributedVirtualPortgroup,
310             network_name,
311             container_ref=container_ref,
312         )
313         dvs_port_connection = vim.dvs.PortConnection(
314             portgroupKey=network_ref.key,
315             switchUuid=network_ref.config.distributedVirtualSwitch.uuid,
316         )
317         network_spec.device.backing = (
318             vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()
319         )
320         network_spec.device.backing.port = dvs_port_connection
321     else:
322         if not switch_type:
323             err_msg = (
324                 "The switch type to be used by '{}' has not been specified".format(
325                     network_adapter_label
326                 )
327             )
328         else:
329             err_msg = "Cannot create '{}'. Invalid/unsupported switch type '{}'".format(
330                 network_adapter_label, switch_type
331             )
332         raise SaltCloudSystemExit(err_msg)
333     if mac != "":
334         network_spec.device.addressType = "assigned"
335         network_spec.device.macAddress = mac
336     network_spec.device.key = random_key
337     network_spec.device.deviceInfo = vim.Description()
338     network_spec.device.deviceInfo.label = network_adapter_label
339     network_spec.device.deviceInfo.summary = network_name
340     network_spec.device.wakeOnLanEnabled = True
341     network_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
342     network_spec.device.connectable.startConnected = True
343     network_spec.device.connectable.allowGuestControl = True
344     return network_spec
345 def _edit_existing_scsi_controller(scsi_controller, bus_sharing):
346     scsi_controller.sharedBus = bus_sharing
347     scsi_spec = vim.vm.device.VirtualDeviceSpec()
348     scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
349     scsi_spec.device = scsi_controller
350     return scsi_spec
351 def _add_new_scsi_controller_helper(scsi_controller_label, properties, bus_number):
352     random_key = randint(-1050, -1000)
353     adapter_type = properties["type"].strip().lower() if "type" in properties else None
354     bus_sharing = (
355         properties["bus_sharing"].strip().lower()
356         if "bus_sharing" in properties
357         else None
358     )
359     scsi_spec = vim.vm.device.VirtualDeviceSpec()
360     if adapter_type == "lsilogic":
361         summary = "LSI Logic"
362         scsi_spec.device = vim.vm.device.VirtualLsiLogicController()
363     elif adapter_type == "lsilogic_sas":
364         summary = "LSI Logic Sas"
365         scsi_spec.device = vim.vm.device.VirtualLsiLogicSASController()
366     elif adapter_type == "paravirtual":
367         summary = "VMware paravirtual SCSI"
368         scsi_spec.device = vim.vm.device.ParaVirtualSCSIController()
369     else:
370         if not adapter_type:
371             err_msg = "The type of '{}' has not been specified".format(
372                 scsi_controller_label
373             )
374         else:
375             err_msg = "Cannot create '{}'. Invalid/unsupported type '{}'".format(
376                 scsi_controller_label, adapter_type
377             )
378         raise SaltCloudSystemExit(err_msg)
379     scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
380     scsi_spec.device.key = random_key
381     scsi_spec.device.busNumber = bus_number
382     scsi_spec.device.deviceInfo = vim.Description()
383     scsi_spec.device.deviceInfo.label = scsi_controller_label
384     scsi_spec.device.deviceInfo.summary = summary
385     if bus_sharing == "virtual":
386         scsi_spec.device.sharedBus = (
387             vim.vm.device.VirtualSCSIController.Sharing.virtualSharing
388         )
389     elif bus_sharing == "physical":
390         scsi_spec.device.sharedBus = (
391             vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
392         )
393     else:
394         scsi_spec.device.sharedBus = (
395             vim.vm.device.VirtualSCSIController.Sharing.noSharing
396         )
397     return scsi_spec
398 def _add_new_ide_controller_helper(ide_controller_label, controller_key, bus_number):
399     if controller_key is None:
400         controller_key = randint(-200, 250)
401     ide_spec = vim.vm.device.VirtualDeviceSpec()
402     ide_spec.device = vim.vm.device.VirtualIDEController()
403     ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
404     ide_spec.device.key = controller_key
405     ide_spec.device.busNumber = bus_number
406     ide_spec.device.deviceInfo = vim.Description()
407     ide_spec.device.deviceInfo.label = ide_controller_label
408     ide_spec.device.deviceInfo.summary = ide_controller_label
409     return ide_spec
410 def _set_cd_or_dvd_backing_type(drive, device_type, mode, iso_path):
411     if device_type == "datastore_iso_file":
412         drive.backing = vim.vm.device.VirtualCdrom.IsoBackingInfo()
413         drive.backing.fileName = iso_path
414         datastore = iso_path.partition("[")[-1].rpartition("]")[0]
415         datastore_ref = salt.utils.vmware.get_mor_by_property(
416             _get_si(), vim.Datastore, datastore
417         )
418         if datastore_ref:
419             drive.backing.datastore = datastore_ref
420         drive.deviceInfo.summary = "ISO {}".format(iso_path)
421     elif device_type == "client_device":
422         if mode == "passthrough":
423             drive.backing = vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo()
424             drive.deviceInfo.summary = "Remote Device"
425         elif mode == "atapi":
426             drive.backing = vim.vm.device.VirtualCdrom.RemoteAtapiBackingInfo()
427             drive.deviceInfo.summary = "Remote ATAPI"
428     return drive
429 def _edit_existing_cd_or_dvd_drive(drive, device_type, mode, iso_path):
430     device_type.strip().lower()
431     mode.strip().lower()
432     drive_spec = vim.vm.device.VirtualDeviceSpec()
433     drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
434     drive_spec.device = _set_cd_or_dvd_backing_type(drive, device_type, mode, iso_path)
435     return drive_spec
436 def _add_new_cd_or_dvd_drive_helper(
437     drive_label, controller_key, device_type, mode, iso_path
438 ):
439     random_key = randint(-3025, -3000)
440     device_type.strip().lower()
441     mode.strip().lower()
442     drive_spec = vim.vm.device.VirtualDeviceSpec()
443     drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
444     drive_spec.device = vim.vm.device.VirtualCdrom()
445     drive_spec.device.deviceInfo = vim.Description()
446     if device_type in ["datastore_iso_file", "client_device"]:
447         drive_spec.device = _set_cd_or_dvd_backing_type(
448             drive_spec.device, device_type, mode, iso_path
449         )
450     else:
451         if not device_type:
452             log.debug(
453                 "The 'device_type' of '%s' has not been specified. "
454                 "Creating default type 'client_device'",
455                 drive_label,
456             )
457         else:
458             log.error(
459                 "Cannot create CD/DVD drive of type '%s'. "
460                 "Creating '%s' of default type 'client_device'",
461                 device_type,
462                 drive_label,
463             )
464         drive_spec.device.backing = (
465             vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo()
466         )
467         drive_spec.device.deviceInfo.summary = "Remote Device"
468     drive_spec.device.key = random_key
469     drive_spec.device.deviceInfo.label = drive_label
470     drive_spec.device.controllerKey = controller_key
471     drive_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
472     drive_spec.device.connectable.startConnected = True
473     drive_spec.device.connectable.allowGuestControl = True
474     return drive_spec
475 def _set_network_adapter_mapping(adapter_specs):
476     adapter_mapping = vim.vm.customization.AdapterMapping()
477     adapter_mapping.adapter = vim.vm.customization.IPSettings()
478     if "domain" in list(adapter_specs.keys()):
479         domain = adapter_specs["domain"]
480         adapter_mapping.adapter.dnsDomain = domain
481     if "gateway" in list(adapter_specs.keys()):
482         gateway = adapter_specs["gateway"]
483         adapter_mapping.adapter.gateway = gateway
484     if "ip" in list(adapter_specs.keys()):
485         ip = str(adapter_specs["ip"])
486         subnet_mask = str(adapter_specs["subnet_mask"])
487         adapter_mapping.adapter.ip = vim.vm.customization.FixedIp(ipAddress=ip)
488         adapter_mapping.adapter.subnetMask = subnet_mask
489     else:
490         adapter_mapping.adapter.ip = vim.vm.customization.DhcpIpGenerator()
491     return adapter_mapping
492 def _get_mode_spec(device, mode, disk_spec):
493     if device.backing.diskMode != mode:
494         if not disk_spec:
495             disk_spec = _edit_existing_hard_disk_helper(disk=device, mode=mode)
496         else:
497             disk_spec.device.backing.diskMode = mode
498     return disk_spec
499 def _get_size_spec(device, size_gb=None, size_kb=None):
500     if size_kb is None and size_gb is not None:
501         size_kb = int(size_gb * 1024.0 * 1024.0)
502     disk_spec = (
503         _edit_existing_hard_disk_helper(disk=device, size_kb=size_kb)
504         if device.capacityInKB &lt; size_kb
505         else None
506     )
507     return disk_spec
508 def _iter_disk_unit_number(unit_number):
509     unit_number += 1
510     if unit_number == 7:
511         unit_number += 1
512     return unit_number
513 def _manage_devices(devices, vm=None, container_ref=None, new_vm_name=None):
514     unit_number = 0
515     bus_number = 0
516     device_specs = []
517     existing_disks_label = []
518     existing_scsi_controllers_label = []
519     existing_ide_controllers_label = []
520     existing_network_adapters_label = []
521     existing_cd_drives_label = []
522     ide_controllers = {}
523     nics_map = []
524     cloning_from_vm = vm is not None
525     if cloning_from_vm:
526         for device in vm.config.hardware.device:
527             if isinstance(device, vim.vm.device.VirtualDisk):
528                 if "disk" in list(devices.keys()):
529                     unit_number = _iter_disk_unit_number(unit_number)
530                     existing_disks_label.append(device.deviceInfo.label)
531                     if device.deviceInfo.label in list(devices["disk"].keys()):
532                         disk_spec = None
533                         if "size" in devices["disk"][device.deviceInfo.label]:
534                             size_gb = float(
535                                 devices["disk"][device.deviceInfo.label]["size"]
536                             )
537                             size_kb = int(size_gb * 1024.0 * 1024.0)
538                         else:
539                             size_kb = device.capacityInKB
540                             size_gb = size_kb / (1024.0 * 1024.0)
541                             log.debug(
542                                 "Virtual disk size for '%s' was not "
543                                 "specified in the cloud profile or map file. "
544                                 "Using existing virtual disk size of '%sGB'",
545                                 device.deviceInfo.label,
546                                 size_gb,
547                             )
548                         if device.capacityInKB &gt; size_kb:
549                             raise SaltCloudSystemExit(
550                                 "The specified disk size '{}GB' for '{}' is "
551                                 "smaller than the disk image size '{}GB'. It must "
552                                 "be equal to or greater than the disk image".format(
553                                     float(
554                                         devices["disk"][device.deviceInfo.label]["size"]
555                                     ),
556                                     device.deviceInfo.label,
557                                     float(device.capacityInKB / (1024.0 * 1024.0)),
558                                 )
559                             )
560                         else:
561                             disk_spec = _get_size_spec(device=device, size_kb=size_kb)
562                         if "mode" in devices["disk"][device.deviceInfo.label]:
563                             if devices["disk"][device.deviceInfo.label]["mode"] in [
564                                 "independent_persistent",
565                                 "independent_nonpersistent",
566                                 "dependent",
567                             ]:
568                                 mode = devices["disk"][device.deviceInfo.label]["mode"]
569                                 disk_spec = _get_mode_spec(device, mode, disk_spec)
570                             else:
571                                 raise SaltCloudSystemExit(
572                                     "Invalid disk backing mode specified!"
573                                 )
574                         if disk_spec is not None:
575                             device_specs.append(disk_spec)
576             elif isinstance(
577                 device.backing,
578                 (
579                     vim.vm.device.VirtualEthernetCard.NetworkBackingInfo,
580                     vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo,
581                 ),
582             ):
583                 if "network" in list(devices.keys()):
584                     existing_network_adapters_label.append(device.deviceInfo.label)
585                     if device.deviceInfo.label in list(devices["network"].keys()):
586                         network_name = devices["network"][device.deviceInfo.label][
587                             "name"
588                         ]
589                         adapter_type = (
590                             devices["network"][device.deviceInfo.label]["adapter_type"]
591                             if "adapter_type"
592                             in devices["network"][device.deviceInfo.label]
593                             else ""
594                         )
595                         switch_type = (
596                             devices["network"][device.deviceInfo.label]["switch_type"]
597                             if "switch_type"
598                             in devices["network"][device.deviceInfo.label]
599                             else ""
600                         )
601                         network_spec = _edit_existing_network_adapter(
602                             device,
603                             network_name,
604                             adapter_type,
605                             switch_type,
606                             container_ref,
607                         )
608                         adapter_mapping = _set_network_adapter_mapping(
609                             devices["network"][device.deviceInfo.label]
610                         )
611                         device_specs.append(network_spec)
612                         nics_map.append(adapter_mapping)
613             elif hasattr(device, "scsiCtlrUnitNumber"):
614                 if "scsi" in list(devices.keys()):
615                     bus_number += 1
616                     existing_scsi_controllers_label.append(device.deviceInfo.label)
617                     if device.deviceInfo.label in list(devices["scsi"].keys()):
618                         scsi_controller_properties = devices["scsi"][
619                             device.deviceInfo.label
620                         ]
621                         bus_sharing = (
622                             scsi_controller_properties["bus_sharing"].strip().lower()
623                             if "bus_sharing" in scsi_controller_properties
624                             else None
625                         )
626                         if bus_sharing and bus_sharing in ["virtual", "physical", "no"]:
627                             bus_sharing = "{}Sharing".format(bus_sharing)
628                             if bus_sharing != device.sharedBus:
629                                 scsi_spec = _edit_existing_scsi_controller(
630                                     device, bus_sharing
631                                 )
632                                 device_specs.append(scsi_spec)
633             elif isinstance(device, vim.vm.device.VirtualCdrom):
634                 if "cd" in list(devices.keys()):
635                     existing_cd_drives_label.append(device.deviceInfo.label)
636                     if device.deviceInfo.label in list(devices["cd"].keys()):
637                         device_type = (
638                             devices["cd"][device.deviceInfo.label]["device_type"]
639                             if "device_type" in devices["cd"][device.deviceInfo.label]
640                             else ""
641                         )
642                         mode = (
643                             devices["cd"][device.deviceInfo.label]["mode"]
644                             if "mode" in devices["cd"][device.deviceInfo.label]
645                             else ""
646                         )
647                         iso_path = (
648                             devices["cd"][device.deviceInfo.label]["iso_path"]
649                             if "iso_path" in devices["cd"][device.deviceInfo.label]
650                             else ""
651                         )
652                         cd_drive_spec = _edit_existing_cd_or_dvd_drive(
653                             device, device_type, mode, iso_path
654                         )
655                         device_specs.append(cd_drive_spec)
656             elif isinstance(device, vim.vm.device.VirtualIDEController):
657                 ide_controllers[device.key] = len(device.device)
658     if "network" in list(devices.keys()):
659         network_adapters_to_create = list(
660             set(devices["network"].keys()) - set(existing_network_adapters_label)
661         )
662         network_adapters_to_create.sort()
663         if network_adapters_to_create:
664             log.debug("Networks adapters to create: %s", network_adapters_to_create)
665         for network_adapter_label in network_adapters_to_create:
666             network_name = devices["network"][network_adapter_label]["name"]
667             adapter_type = (
668                 devices["network"][network_adapter_label]["adapter_type"]
669                 if "adapter_type" in devices["network"][network_adapter_label]
670                 else ""
671             )
672             switch_type = (
673                 devices["network"][network_adapter_label]["switch_type"]
674                 if "switch_type" in devices["network"][network_adapter_label]
675                 else ""
676             )
677             mac = (
678                 devices["network"][network_adapter_label]["mac"]
679                 if "mac" in devices["network"][network_adapter_label]
680                 else ""
681             )
682             network_spec = _add_new_network_adapter_helper(
683                 network_adapter_label,
684                 network_name,
685                 adapter_type,
686                 switch_type,
687                 mac,
688                 container_ref,
689             )
690             adapter_mapping = _set_network_adapter_mapping(
691                 devices["network"][network_adapter_label]
692             )
693             device_specs.append(network_spec)
694             nics_map.append(adapter_mapping)
695     if "scsi" in list(devices.keys()):
696         scsi_controllers_to_create = list(
697             set(devices["scsi"].keys()) - set(existing_scsi_controllers_label)
698         )
699         scsi_controllers_to_create.sort()
700         if scsi_controllers_to_create:
701             log.debug("SCSI controllers to create: %s", scsi_controllers_to_create)
702         for scsi_controller_label in scsi_controllers_to_create:
703             scsi_controller_properties = devices["scsi"][scsi_controller_label]
704             scsi_spec = _add_new_scsi_controller_helper(
705                 scsi_controller_label, scsi_controller_properties, bus_number
706             )
707             device_specs.append(scsi_spec)
708             bus_number += 1
709     if "ide" in list(devices.keys()):
710         ide_controllers_to_create = list(
711             set(devices["ide"].keys()) - set(existing_ide_controllers_label)
712         )
713         ide_controllers_to_create.sort()
714         if ide_controllers_to_create:
715             log.debug("IDE controllers to create: %s", ide_controllers_to_create)
716         vcenter_name = get_vcenter_version(call="function")
717         controller_index = (
718             SAFE_ESX_5_5_CONTROLLER_KEY_INDEX
719             if ESX_5_5_NAME_PORTION in vcenter_name
720             else None
721         )
722         for ide_controller_label in ide_controllers_to_create:
723             ide_spec = _add_new_ide_controller_helper(
724                 ide_controller_label, controller_index, bus_number
725             )
726             device_specs.append(ide_spec)
727             bus_number += 1
728             if controller_index is not None:
729                 controller_index += 1
730     if "disk" in list(devices.keys()):
731         disks_to_create = list(set(devices["disk"].keys()) - set(existing_disks_label))
732         disks_to_create.sort()
733         if disks_to_create:
734             log.debug("Hard disks to create: %s", disks_to_create)
735         for disk_label in disks_to_create:
736             size_gb = float(devices["disk"][disk_label]["size"])
737             thin_provision = (
738                 bool(devices["disk"][disk_label]["thin_provision"])
739                 if "thin_provision" in devices["disk"][disk_label]
740                 else False
741             )
742             eagerly_scrub = (
743                 bool(devices["disk"][disk_label]["eagerly_scrub"])
744                 if "eagerly_scrub" in devices["disk"][disk_label]
745                 else False
746             )
747             datastore = devices["disk"][disk_label].get("datastore", None)
748             disk_spec = _add_new_hard_disk_helper(
749                 disk_label,
750                 size_gb,
751                 unit_number,
752                 thin_provision=thin_provision,
753                 eagerly_scrub=eagerly_scrub,
754                 datastore=datastore,
755                 vm_name=new_vm_name,
756             )
757             if "controller" in devices["disk"][disk_label]:
758                 for spec in device_specs:
759                     if (
760                         spec.device.deviceInfo.label
761                         == devices["disk"][disk_label]["controller"]
762                     ):
763                         disk_spec.device.controllerKey = spec.device.key
764                         break
765             device_specs.append(disk_spec)
766             unit_number = _iter_disk_unit_number(unit_number)
767     if "cd" in list(devices.keys()):
768         cd_drives_to_create = list(
769             set(devices["cd"].keys()) - set(existing_cd_drives_label)
770         )
771         cd_drives_to_create.sort()
772         if cd_drives_to_create:
773             log.debug("CD/DVD drives to create: %s", cd_drives_to_create)
774         for cd_drive_label in cd_drives_to_create:
775             device_type = (
776                 devices["cd"][cd_drive_label]["device_type"]
777                 if "device_type" in devices["cd"][cd_drive_label]
778                 else ""
779             )
780             mode = (
781                 devices["cd"][cd_drive_label]["mode"]
782                 if "mode" in devices["cd"][cd_drive_label]
783                 else ""
784             )
785             iso_path = (
786                 devices["cd"][cd_drive_label]["iso_path"]
787                 if "iso_path" in devices["cd"][cd_drive_label]
788                 else ""
789             )
790             controller_key = None
791             if "controller" in devices["cd"][cd_drive_label]:
792                 for spec in device_specs:
793                     if (
794                         spec.device.deviceInfo.label
795                         == devices["cd"][cd_drive_label]["controller"]
796                     ):
797                         controller_key = spec.device.key
798                         ide_controllers[controller_key] = 0
799                         break
800             else:
801                 for ide_controller_key, num_devices in ide_controllers.items():
802                     if num_devices &lt; 2:
803                         controller_key = ide_controller_key
804                         break
805             if not controller_key:
806                 log.error(
807                     "No more available controllers for '%s'. "
808                     "All IDE controllers are currently in use",
809                     cd_drive_label,
810                 )
811             else:
812                 cd_drive_spec = _add_new_cd_or_dvd_drive_helper(
813                     cd_drive_label, controller_key, device_type, mode, iso_path
814                 )
815                 device_specs.append(cd_drive_spec)
816                 ide_controllers[controller_key] += 1
817     ret = {"device_specs": device_specs, "nics_map": nics_map}
818     return ret
819 def _wait_for_vmware_tools(vm_ref, max_wait):
820     time_counter = 0
821     starttime = time.time()
822     while time_counter &lt; max_wait:
823         if time_counter % 5 == 0:
824             log.info(
825                 "[ %s ] Waiting for VMware tools to be running [%s s]",
826                 vm_ref.name,
827                 time_counter,
828             )
829         if str(vm_ref.summary.guest.toolsRunningStatus) == "guestToolsRunning":
830             log.info(
831                 "[ %s ] Successfully got VMware tools running on the guest in "
832                 "%s seconds",
833                 vm_ref.name,
834                 time_counter,
835             )
836             return True
837         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
838         time_counter += 1
839     log.warning(
840         "[ %s ] Timeout Reached. VMware tools still not running after waiting "
841         "for %s seconds",
842         vm_ref.name,
843         max_wait,
844     )
845     return False
846 def _valid_ip(ip_address):
847     octets = ip_address.split(".")
848     if len(octets) != 4:
849         return False
850     for i, octet in enumerate(octets):
851         try:
852             octets[i] = int(octet)
853         except ValueError:
854             return False
855     first_octet, second_octet, third_octet, fourth_octet = octets
856     if first_octet &lt; 1 or first_octet &gt; 223 or first_octet == 127:
857         return False
858     if first_octet == 169 and second_octet == 254:
859         return False
860     for octet in (second_octet, third_octet, fourth_octet):
861         if (octet &lt; 0) or (octet &gt; 255):
862             return False
863     return True
864 def _wait_for_ip(vm_ref, max_wait):
865     max_wait_vmware_tools = max_wait
866     max_wait_ip = max_wait
867     vmware_tools_status = _wait_for_vmware_tools(vm_ref, max_wait_vmware_tools)
868     if not vmware_tools_status:
869         vm_name = vm_ref.summary.config.name
870         resolved_ips = salt.utils.network.host_to_ips(vm_name)
871         log.debug(
872             "Timeout waiting for VMware tools. The name %s resolved to %s",
873             vm_name,
874             resolved_ips,
875         )
876         if isinstance(resolved_ips, list) and resolved_ips:
877             return resolved_ips[0]
878         return False
879     time_counter = 0
880     starttime = time.time()
881     while time_counter &lt; max_wait_ip:
882         if time_counter % 5 == 0:
883             log.info(
884                 "[ %s ] Waiting to retrieve IPv4 information [%s s]",
885                 vm_ref.name,
886                 time_counter,
887             )
888         if vm_ref.summary.guest.ipAddress and _valid_ip(vm_ref.summary.guest.ipAddress):
889             log.info(
890                 "[ %s ] Successfully retrieved IPv4 information in %s seconds",
891                 vm_ref.name,
892                 time_counter,
893             )
894             return vm_ref.summary.guest.ipAddress
895         for net in vm_ref.guest.net:
896             if net.ipConfig.ipAddress:
897                 for current_ip in net.ipConfig.ipAddress:
898                     if _valid_ip(current_ip.ipAddress):
899                         log.info(
900                             "[ %s ] Successfully retrieved IPv4 information "
901                             "in %s seconds",
902                             vm_ref.name,
903                             time_counter,
904                         )
905                         return current_ip.ipAddress
906         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
907         time_counter += 1
908     log.warning(
909         "[ %s ] Timeout Reached. Unable to retrieve IPv4 information after "
910         "waiting for %s seconds",
911         vm_ref.name,
912         max_wait_ip,
913     )
914     return False
915 def _wait_for_host(host_ref, task_type, sleep_seconds=5, log_level="debug"):
916     time_counter = 0
917     starttime = time.time()
918     while host_ref.runtime.connectionState != "notResponding":
919         if time_counter % sleep_seconds == 0:
920             log.log(
921                 logging.INFO if log_level == "info" else logging.DEBUG,
922                 "[ %s ] Waiting for host %s to finish [%s s]",
923                 host_ref.name,
924                 task_type,
925                 time_counter,
926             )
927         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
928         time_counter += 1
929     while host_ref.runtime.connectionState != "connected":
930         if time_counter % sleep_seconds == 0:
931             log.log(
932                 logging.INFO if log_level == "info" else logging.DEBUG,
933                 "[ %s ] Waiting for host %s to finish [%s s]",
934                 host_ref.name,
935                 task_type,
936                 time_counter,
937             )
938         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
939         time_counter += 1
940     if host_ref.runtime.connectionState == "connected":
941         log.log(
942             logging.INFO if log_level == "info" else logging.DEBUG,
943             "[ %s ] Successfully completed host %s in %s seconds",
944             host_ref.name,
945             task_type,
946             time_counter,
947         )
948     else:
949         log.error("Could not connect back to the host system")
950 def _format_instance_info_select(vm, selection):
951     def defaultto(machine, section, default="N/A"):
952         return default if section not in machine else machine[section]
953     vm_select_info = {}
954     if "id" in selection:
955         vm_select_info["id"] = vm["name"]
956     if "image" in selection:
957         vm_select_info["image"] = "{} (Detected)".format(
958             defaultto(vm, "config.guestFullName")
959         )
960     if "size" in selection:
961         cpu = defaultto(vm, "config.hardware.numCPU")
962         ram = "{} MB".format(defaultto(vm, "config.hardware.memoryMB"))
963         vm_select_info["size"] = "cpu: {}\nram: {}".format(cpu, ram)
964         vm_select_info["size_dict"] = {
965             "cpu": cpu,
966             "memory": ram,
967         }
968     if "state" in selection:
969         vm_select_info["state"] = str(defaultto(vm, "summary.runtime.powerState"))
970     if "guest_id" in selection:
971         vm_select_info["guest_id"] = defaultto(vm, "config.guestId")
972     if "hostname" in selection:
973         vm_select_info["hostname"] = vm["object"].guest.hostName
974     if "path" in selection:
975         vm_select_info["path"] = defaultto(vm, "config.files.vmPathName")
976     if "tools_status" in selection:
977         vm_select_info["tools_status"] = str(defaultto(vm, "guest.toolsStatus"))
978     if "private_ips" in selection or "networks" in selection:
979         network_full_info = {}
980         ip_addresses = []
981         if "guest.net" in vm:
982             for net in vm["guest.net"]:
983                 network_full_info[net.network] = {
984                     "connected": net.connected,
985                     "ip_addresses": net.ipAddress,
986                     "mac_address": net.macAddress,
987                 }
988                 ip_addresses.extend(net.ipAddress)
989         if "private_ips" in selection:
990             vm_select_info["private_ips"] = ip_addresses
991         if "networks" in selection:
992             vm_select_info["networks"] = network_full_info
993     if any(x in ["devices", "mac_address", "mac_addresses"] for x in selection):
994         device_full_info = {}
995         device_mac_addresses = []
996         if "config.hardware.device" in vm:
997             for device in vm["config.hardware.device"]:
998                 device_full_info[device.deviceInfo.label] = {}
999                 if "devices" in selection:
1000                     device_full_info[device.deviceInfo.label]["key"] = (device.key,)
1001                     device_full_info[device.deviceInfo.label]["label"] = (
1002                         device.deviceInfo.label,
1003                     )
1004                     device_full_info[device.deviceInfo.label]["summary"] = (
1005                         device.deviceInfo.summary,
1006                     )
1007                     device_full_info[device.deviceInfo.label]["type"] = type(
1008                         device
1009                     ).__name__.rsplit(".", 1)[1]
1010                     if device.unitNumber:
1011                         device_full_info[device.deviceInfo.label][
1012                             "unitNumber"
1013                         ] = device.unitNumber
1014                     if hasattr(device, "connectable") and device.connectable:
1015                         device_full_info[device.deviceInfo.label][
1016                             "startConnected"
1017                         ] = device.connectable.startConnected
1018                         device_full_info[device.deviceInfo.label][
1019                             "allowGuestControl"
1020                         ] = device.connectable.allowGuestControl
1021                         device_full_info[device.deviceInfo.label][
1022                             "connected"
1023                         ] = device.connectable.connected
1024                         device_full_info[device.deviceInfo.label][
1025                             "status"
1026                         ] = device.connectable.status
1027                     if hasattr(device, "controllerKey") and device.controllerKey:
1028                         device_full_info[device.deviceInfo.label][
1029                             "controllerKey"
1030                         ] = device.controllerKey
1031                     if hasattr(device, "addressType"):
1032                         device_full_info[device.deviceInfo.label][
1033                             "addressType"
1034                         ] = device.addressType
1035                     if hasattr(device, "busNumber"):
1036                         device_full_info[device.deviceInfo.label][
1037                             "busNumber"
1038                         ] = device.busNumber
1039                     if hasattr(device, "device"):
1040                         device_full_info[device.deviceInfo.label][
1041                             "deviceKeys"
1042                         ] = device.device
1043                     if hasattr(device, "videoRamSizeInKB"):
1044                         device_full_info[device.deviceInfo.label][
1045                             "videoRamSizeInKB"
1046                         ] = device.videoRamSizeInKB
1047                     if isinstance(device, vim.vm.device.VirtualDisk):
1048                         device_full_info[device.deviceInfo.label][
1049                             "capacityInKB"
1050                         ] = device.capacityInKB
1051                         device_full_info[device.deviceInfo.label][
1052                             "diskMode"
1053                         ] = device.backing.diskMode
1054                         device_full_info[device.deviceInfo.label][
1055                             "fileName"
1056                         ] = device.backing.fileName
1057                 if hasattr(device, "macAddress"):
1058                     device_full_info[device.deviceInfo.label][
1059                         "macAddress"
1060                     ] = device.macAddress
1061                     device_mac_addresses.append(device.macAddress)
1062         if "devices" in selection:
1063             vm_select_info["devices"] = device_full_info
1064         if "mac_address" in selection or "mac_addresses" in selection:
1065             vm_select_info["mac_addresses"] = device_mac_addresses
1066     if "storage" in selection:
1067         storage_full_info = {
1068             "committed": int(vm["summary.storage.committed"])
1069             if "summary.storage.committed" in vm
1070             else "N/A",
1071             "uncommitted": int(vm["summary.storage.uncommitted"])
1072             if "summary.storage.uncommitted" in vm
1073             else "N/A",
1074             "unshared": int(vm["summary.storage.unshared"])
1075             if "summary.storage.unshared" in vm
1076             else "N/A",
1077         }
1078         vm_select_info["storage"] = storage_full_info
1079     if "files" in selection:
1080         file_full_info = {}
1081         if "layoutEx.file" in vm:
1082             for filename in vm["layoutEx.file"]:
1083                 file_full_info[filename.key] = {
1084                     "key": filename.key,
1085                     "name": filename.name,
1086                     "size": filename.size,
1087                     "type": filename.type,
1088                 }
1089         vm_select_info["files"] = file_full_info
1090     return vm_select_info
1091 def _format_instance_info(vm):
1092     device_full_info = {}
1093     device_mac_addresses = []
1094     if "config.hardware.device" in vm:
1095         for device in vm["config.hardware.device"]:
1096             device_full_info[device.deviceInfo.label] = {
1097                 "key": device.key,
1098                 "label": device.deviceInfo.label,
1099                 "summary": device.deviceInfo.summary,
1100                 "type": type(device).__name__.rsplit(".", 1)[1],
1101             }
1102             if device.unitNumber:
1103                 device_full_info[device.deviceInfo.label][
1104                     "unitNumber"
1105                 ] = device.unitNumber
1106             if hasattr(device, "connectable") and device.connectable:
1107                 device_full_info[device.deviceInfo.label][
1108                     "startConnected"
1109                 ] = device.connectable.startConnected
1110                 device_full_info[device.deviceInfo.label][
1111                     "allowGuestControl"
1112                 ] = device.connectable.allowGuestControl
1113                 device_full_info[device.deviceInfo.label][
1114                     "connected"
1115                 ] = device.connectable.connected
1116                 device_full_info[device.deviceInfo.label][
1117                     "status"
1118                 ] = device.connectable.status
1119             if hasattr(device, "controllerKey") and device.controllerKey:
1120                 device_full_info[device.deviceInfo.label][
1121                     "controllerKey"
1122                 ] = device.controllerKey
1123             if hasattr(device, "addressType"):
1124                 device_full_info[device.deviceInfo.label][
1125                     "addressType"
1126                 ] = device.addressType
1127             if hasattr(device, "macAddress"):
1128                 device_full_info[device.deviceInfo.label][
1129                     "macAddress"
1130                 ] = device.macAddress
1131                 device_mac_addresses.append(device.macAddress)
1132             if hasattr(device, "busNumber"):
1133                 device_full_info[device.deviceInfo.label][
1134                     "busNumber"
1135                 ] = device.busNumber
1136             if hasattr(device, "device"):
1137                 device_full_info[device.deviceInfo.label]["deviceKeys"] = device.device
1138             if hasattr(device, "videoRamSizeInKB"):
1139                 device_full_info[device.deviceInfo.label][
1140                     "videoRamSizeInKB"
1141                 ] = device.videoRamSizeInKB
1142             if isinstance(device, vim.vm.device.VirtualDisk):
1143                 device_full_info[device.deviceInfo.label][
1144                     "capacityInKB"
1145                 ] = device.capacityInKB
1146                 device_full_info[device.deviceInfo.label][
1147                     "diskMode"
1148                 ] = device.backing.diskMode
1149                 device_full_info[device.deviceInfo.label][
1150                     "fileName"
1151                 ] = device.backing.fileName
1152     storage_full_info = {
1153         "committed": int(vm["summary.storage.committed"])
1154         if "summary.storage.committed" in vm
1155         else "N/A",
1156         "uncommitted": int(vm["summary.storage.uncommitted"])
1157         if "summary.storage.uncommitted" in vm
1158         else "N/A",
1159         "unshared": int(vm["summary.storage.unshared"])
1160         if "summary.storage.unshared" in vm
1161         else "N/A",
1162     }
1163     file_full_info = {}
1164     if "layoutEx.file" in vm:
1165         for filename in vm["layoutEx.file"]:
1166             file_full_info[filename.key] = {
1167                 "key": filename.key,
1168                 "name": filename.name,
1169                 "size": filename.size,
1170                 "type": filename.type,
1171             }
1172     network_full_info = {}
1173     ip_addresses = []
1174     if "guest.net" in vm:
1175         for net in vm["guest.net"]:
1176             network_full_info[net.network] = {
1177                 "connected": net.connected,
1178                 "ip_addresses": net.ipAddress,
1179                 "mac_address": net.macAddress,
1180             }
1181             ip_addresses.extend(net.ipAddress)
1182     cpu = vm["config.hardware.numCPU"] if "config.hardware.numCPU" in vm else "N/A"
1183     ram = (
1184         "{} MB".format(vm["config.hardware.memoryMB"])
1185         if "config.hardware.memoryMB" in vm
1186         else "N/A"
1187     )
1188     vm_full_info = {
1189         "id": str(vm["name"]),
1190         "image": "{} (Detected)".format(vm["config.guestFullName"])
1191         if "config.guestFullName" in vm
1192         else "N/A",
1193         "size": "cpu: {}\nram: {}".format(cpu, ram),
1194         "size_dict": {"cpu": cpu, "memory": ram},
1195         "state": str(vm["summary.runtime.powerState"])
1196         if "summary.runtime.powerState" in vm
1197         else "N/A",
1198         "private_ips": ip_addresses,
1199         "public_ips": [],
1200         "devices": device_full_info,
1201         "storage": storage_full_info,
1202         "files": file_full_info,
1203         "guest_id": str(vm["config.guestId"]) if "config.guestId" in vm else "N/A",
1204         "hostname": str(vm["object"].guest.hostName),
1205         "mac_addresses": device_mac_addresses,
1206         "networks": network_full_info,
1207         "path": str(vm["config.files.vmPathName"])
1208         if "config.files.vmPathName" in vm
1209         else "N/A",
1210         "tools_status": str(vm["guest.toolsStatus"])
1211         if "guest.toolsStatus" in vm
1212         else "N/A",
1213     }
1214     return vm_full_info
1215 def _get_snapshots(snapshot_list, current_snapshot=None, parent_snapshot_path=""):
1216     snapshots = {}
1217     for snapshot in snapshot_list:
1218         snapshot_path = "{}/{}".format(parent_snapshot_path, snapshot.name)
1219         snapshots[snapshot_path] = {
1220             "name": snapshot.name,
1221             "description": snapshot.description,
1222             "created": str(snapshot.createTime).split(".")[0],
1223             "state": snapshot.state,
1224             "path": snapshot_path,
1225         }
1226         if current_snapshot and current_snapshot == snapshot.snapshot:
1227             return snapshots[snapshot_path]
1228         if snapshot.childSnapshotList:
1229             ret = _get_snapshots(
1230                 snapshot.childSnapshotList, current_snapshot, snapshot_path
1231             )
1232             if current_snapshot:
1233                 return ret
1234             snapshots.update(ret)
1235     return snapshots
1236 def _get_snapshot_ref_helper(base_snapshot, snapshot_name):
1237     if base_snapshot.name == snapshot_name:
1238         return base_snapshot
1239     for snapshot in base_snapshot.childSnapshotList:
1240         snapshot_ref = _get_snapshot_ref_helper(snapshot, snapshot_name)
1241         if snapshot_ref is not None:
1242             return snapshot_ref
1243     return None
1244 def _get_snapshot_ref_by_name(vm_ref, snapshot_name):
1245     snapshot_ref = None
1246     try:
1247         for root_snapshot in vm_ref.snapshot.rootSnapshotList:
1248             snapshot_ref = _get_snapshot_ref_helper(root_snapshot, snapshot_name)
1249             if snapshot_ref is not None:
1250                 break
1251     except (IndexError, AttributeError):
1252         snapshot_ref = None
1253     return snapshot_ref
1254 def _upg_tools_helper(vm, reboot=False):
1255     if vm.config.template:
1256         status = "VMware tools cannot be updated on a template"
1257     elif vm.guest.toolsStatus == "toolsOk":
1258         status = "VMware tools is already up to date"
1259     elif vm.summary.runtime.powerState != "poweredOn":
1260         status = "VM must be powered on to upgrade tools"
1261     elif vm.guest.toolsStatus in ["toolsNotRunning", "toolsNotInstalled"]:
1262         status = "VMware tools is either not running or not installed"
1263     elif vm.guest.toolsStatus == "toolsOld":
1264         log.info("Upgrading VMware tools on %s", vm.name)
1265         try:
1266             if vm.guest.guestFamily == "windowsGuest" and not reboot:
1267                 log.info("Reboot suppressed on %s", vm.name)
1268                 task = vm.UpgradeTools('/S /v"/qn REBOOT=R"')
1269             elif vm.guest.guestFamily in ["linuxGuest", "windowsGuest"]:
1270                 task = vm.UpgradeTools()
1271             else:
1272                 return "Only Linux and Windows guests are currently supported"
1273             salt.utils.vmware.wait_for_task(
1274                 task, vm.name, "tools upgrade", sleep_seconds=5, log_level="info"
1275             )
1276         except Exception as exc:  # pylint: disable=broad-except
1277             log.error(
1278                 "Error while upgrading VMware tools on VM %s: %s",
1279                 vm.name,
1280                 exc,
1281                 exc_info_on_loglevel=logging.DEBUG,
1282             )
1283             return "VMware tools upgrade failed"
1284         status = "VMware tools upgrade succeeded"
1285     else:
1286         status = "VMWare tools could not be upgraded"
1287     return status
1288 def _get_hba_type(hba_type):
1289     if hba_type == "parallel":
1290         return vim.host.ParallelScsiHba
1291     elif hba_type == "block":
1292         return vim.host.BlockHba
1293     elif hba_type == "iscsi":
1294         return vim.host.InternetScsiHba
1295     elif hba_type == "fibre":
1296         return vim.host.FibreChannelHba
1297     raise ValueError("Unknown Host Bus Adapter Type")
1298 def test_vcenter_connection(kwargs=None, call=None):
1299     if call != "function":
1300         raise SaltCloudSystemExit(
1301             "The test_vcenter_connection function must be called with -f or --function."
1302         )
1303     try:
1304         _get_si()
1305     except Exception as exc:  # pylint: disable=broad-except
1306         return "failed to connect: {}".format(exc)
1307     return "connection successful"
1308 def get_vcenter_version(kwargs=None, call=None):
1309     if call != "function":
1310         raise SaltCloudSystemExit(
1311             "The get_vcenter_version function must be called with -f or --function."
1312         )
1313     inv = salt.utils.vmware.get_inventory(_get_si())
1314     return inv.about.fullName
1315 def list_datacenters(kwargs=None, call=None):
1316     if call != "function":
1317         raise SaltCloudSystemExit(
1318             "The list_datacenters function must be called with -f or --function."
1319         )
1320     return {"Datacenters": salt.utils.vmware.list_datacenters(_get_si())}
1321 def list_portgroups(kwargs=None, call=None):
1322     if call != "function":
1323         raise SaltCloudSystemExit(
1324             "The list_portgroups function must be called with -f or --function."
1325         )
1326     return {"Portgroups": salt.utils.vmware.list_portgroups(_get_si())}
1327 def list_clusters(kwargs=None, call=None):
1328     if call != "function":
1329         raise SaltCloudSystemExit(
1330             "The list_clusters function must be called with -f or --function."
1331         )
1332     return {"Clusters": salt.utils.vmware.list_clusters(_get_si())}
1333 def list_datastore_clusters(kwargs=None, call=None):
1334     if call != "function":
1335         raise SaltCloudSystemExit(
1336             "The list_datastore_clusters function must be called with -f or --function."
1337         )
1338     return {"Datastore Clusters": salt.utils.vmware.list_datastore_clusters(_get_si())}
1339 def list_datastores(kwargs=None, call=None):
1340     if call != "function":
1341         raise SaltCloudSystemExit(
1342             "The list_datastores function must be called with -f or --function."
1343         )
1344     return {"Datastores": salt.utils.vmware.list_datastores(_get_si())}
1345 def list_hosts(kwargs=None, call=None):
1346     if call != "function":
1347         raise SaltCloudSystemExit(
1348             "The list_hosts function must be called with -f or --function."
1349         )
1350     return {"Hosts": salt.utils.vmware.list_hosts(_get_si())}
1351 def list_resourcepools(kwargs=None, call=None):
1352     if call != "function":
1353         raise SaltCloudSystemExit(
1354             "The list_resourcepools function must be called with -f or --function."
1355         )
1356     return {"Resource Pools": salt.utils.vmware.list_resourcepools(_get_si())}
1357 def list_networks(kwargs=None, call=None):
1358     if call != "function":
1359         raise SaltCloudSystemExit(
1360             "The list_networks function must be called with -f or --function."
1361         )
1362     return {"Networks": salt.utils.vmware.list_networks(_get_si())}
1363 def list_nodes_min(kwargs=None, call=None):
1364     if call == "action":
1365         raise SaltCloudSystemExit(
1366             "The list_nodes_min function must be called with -f or --function."
1367         )
1368     ret = {}
1369     vm_properties = ["name"]
1370     vm_list = salt.utils.vmware.get_mors_with_properties(
1371         _get_si(), vim.VirtualMachine, vm_properties
1372     )
1373     for vm in vm_list:
1374         ret[vm["name"]] = {"state": "Running", "id": vm["name"]}
1375     return ret
1376 def list_nodes(kwargs=None, call=None):
1377     if call == "action":
1378         raise SaltCloudSystemExit(
1379             "The list_nodes function must be called with -f or --function."
1380         )
1381     ret = {}
1382     vm_properties = [
1383         "name",
1384         "guest.ipAddress",
1385         "config.guestFullName",
1386         "config.hardware.numCPU",
1387         "config.hardware.memoryMB",
1388         "summary.runtime.powerState",
1389     ]
1390     vm_list = salt.utils.vmware.get_mors_with_properties(
1391         _get_si(), vim.VirtualMachine, vm_properties
1392     )
1393     for vm in vm_list:
1394         cpu = vm["config.hardware.numCPU"] if "config.hardware.numCPU" in vm else "N/A"
1395         ram = (
1396             "{} MB".format(vm["config.hardware.memoryMB"])
1397             if "config.hardware.memoryMB" in vm
1398             else "N/A"
1399         )
1400         vm_info = {
1401             "id": vm["name"],
1402             "image": "{} (Detected)".format(vm["config.guestFullName"])
1403             if "config.guestFullName" in vm
1404             else "N/A",
1405             "size": "cpu: {}\nram: {}".format(cpu, ram),
1406             "size_dict": {"cpu": cpu, "memory": ram},
1407             "state": str(vm["summary.runtime.powerState"])
1408             if "summary.runtime.powerState" in vm
1409             else "N/A",
1410             "private_ips": [vm["guest.ipAddress"]] if "guest.ipAddress" in vm else [],
1411             "public_ips": [],
1412         }
1413         ret[vm_info["id"]] = vm_info
1414     return ret
1415 def list_nodes_full(kwargs=None, call=None):
1416     if call == "action":
1417         raise SaltCloudSystemExit(
1418             "The list_nodes_full function must be called with -f or --function."
1419         )
1420     ret = {}
1421     vm_properties = [
1422         "config.hardware.device",
1423         "summary.storage.committed",
1424         "summary.storage.uncommitted",
1425         "summary.storage.unshared",
1426         "layoutEx.file",
1427         "config.guestFullName",
1428         "config.guestId",
1429         "guest.net",
1430         "config.hardware.memoryMB",
1431         "name",
1432         "config.hardware.numCPU",
1433         "config.files.vmPathName",
1434         "summary.runtime.powerState",
1435         "guest.toolsStatus",
1436     ]
1437     vm_list = salt.utils.vmware.get_mors_with_properties(
1438         _get_si(), vim.VirtualMachine, vm_properties
1439     )
1440     for vm in vm_list:
1441         ret[vm["name"]] = _format_instance_info(vm)
1442     return ret
1443 def list_nodes_select(call=None):
1444     if call == "action":
1445         raise SaltCloudSystemExit(
1446             "The list_nodes_select function must be called with -f or --function."
1447         )
1448     ret = {}
1449     vm_properties = []
1450     selection = __opts__.get("query.selection")
1451     if not selection:
1452         raise SaltCloudSystemExit("query.selection not found in /etc/salt/cloud")
1453     if "id" in selection:
1454         vm_properties.append("name")
1455     if "image" in selection:
1456         vm_properties.append("config.guestFullName")
1457     if "size" in selection:
1458         vm_properties.extend(["config.hardware.numCPU", "config.hardware.memoryMB"])
1459     if "state" in selection:
1460         vm_properties.append("summary.runtime.powerState")
1461     if "private_ips" in selection or "networks" in selection:
1462         vm_properties.append("guest.net")
1463     if (
1464         "devices" in selection
1465         or "mac_address" in selection
1466         or "mac_addresses" in selection
1467     ):
1468         vm_properties.append("config.hardware.device")
1469     if "storage" in selection:
1470         vm_properties.extend(
1471             [
1472                 "config.hardware.device",
1473                 "summary.storage.committed",
1474                 "summary.storage.uncommitted",
1475                 "summary.storage.unshared",
1476             ]
1477         )
1478     if "files" in selection:
1479         vm_properties.append("layoutEx.file")
1480     if "guest_id" in selection:
1481         vm_properties.append("config.guestId")
1482     if "hostname" in selection:
1483         vm_properties.append("guest.hostName")
1484     if "path" in selection:
1485         vm_properties.append("config.files.vmPathName")
1486     if "tools_status" in selection:
1487         vm_properties.append("guest.toolsStatus")
1488     if not vm_properties:
1489         return {}
1490     elif "name" not in vm_properties:
1491         vm_properties.append("name")
1492     vm_list = salt.utils.vmware.get_mors_with_properties(
1493         _get_si(), vim.VirtualMachine, vm_properties
1494     )
1495     for vm in vm_list:
1496         ret[vm["name"]] = _format_instance_info_select(vm, selection)
1497     return ret
1498 def show_instance(name, call=None):
1499     if call != "action":
1500         raise SaltCloudSystemExit(
1501             "The show_instance action must be called with -a or --action."
1502         )
1503     vm_properties = [
1504         "config.hardware.device",
1505         "summary.storage.committed",
1506         "summary.storage.uncommitted",
1507         "summary.storage.unshared",
1508         "layoutEx.file",
1509         "config.guestFullName",
1510         "config.guestId",
1511         "guest.net",
1512         "config.hardware.memoryMB",
1513         "name",
1514         "config.hardware.numCPU",
1515         "config.files.vmPathName",
1516         "summary.runtime.powerState",
1517         "guest.toolsStatus",
1518     ]
1519     vm_list = salt.utils.vmware.get_mors_with_properties(
1520         _get_si(), vim.VirtualMachine, vm_properties
1521     )
1522     for vm in vm_list:
1523         if vm["name"] == name:
1524             return _format_instance_info(vm)
1525     return {}
1526 def avail_images(call=None):
1527     if call == "action":
1528         raise SaltCloudSystemExit(
1529             "The avail_images function must be called with "
1530             "-f or --function, or with the --list-images option."
1531         )
1532     templates = {}
1533     vm_properties = [
1534         "name",
1535         "config.template",
1536         "config.guestFullName",
1537         "config.hardware.numCPU",
1538         "config.hardware.memoryMB",
1539     ]
1540     vm_list = salt.utils.vmware.get_mors_with_properties(
1541         _get_si(), vim.VirtualMachine, vm_properties
1542     )
1543     for vm in vm_list:
1544         if "config.template" in vm and vm["config.template"]:
1545             templates[vm["name"]] = {
1546                 "name": vm["name"],
1547                 "guest_fullname": vm["config.guestFullName"]
1548                 if "config.guestFullName" in vm
1549                 else "N/A",
1550                 "cpus": vm["config.hardware.numCPU"]
1551                 if "config.hardware.numCPU" in vm
1552                 else "N/A",
1553                 "ram": vm["config.hardware.memoryMB"]
1554                 if "config.hardware.memoryMB" in vm
1555                 else "N/A",
1556             }
1557     return templates
1558 def avail_locations(call=None):
1559     if call == "action":
1560         raise SaltCloudSystemExit(
1561             "The avail_locations function must be called with "
1562             "-f or --function, or with the --list-locations option."
1563         )
1564     return list_datacenters(call="function")
1565 def avail_sizes(call=None):
1566     if call == "action":
1567         raise SaltCloudSystemExit(
1568             "The avail_sizes function must be called with "
1569             "-f or --function, or with the --list-sizes option."
1570         )
1571     log.warning(
1572         "Because sizes are built into templates with VMware, there are no sizes "
1573         "to return."
1574     )
1575     return {}
1576 def list_templates(kwargs=None, call=None):
1577     if call != "function":
1578         raise SaltCloudSystemExit(
1579             "The list_templates function must be called with -f or --function."
1580         )
1581     return {"Templates": avail_images(call="function")}
1582 def list_folders(kwargs=None, call=None):
1583     if call != "function":
1584         raise SaltCloudSystemExit(
1585             "The list_folders function must be called with -f or --function."
1586         )
1587     return {"Folders": salt.utils.vmware.list_folders(_get_si())}
1588 def list_snapshots(kwargs=None, call=None):
1589     if call != "function":
1590         raise SaltCloudSystemExit(
1591             "The list_snapshots function must be called with -f or --function."
1592         )
1593     ret = {}
1594     vm_properties = ["name", "rootSnapshot", "snapshot"]
1595     vm_list = salt.utils.vmware.get_mors_with_properties(
1596         _get_si(), vim.VirtualMachine, vm_properties
1597     )
1598     for vm in vm_list:
1599         if vm["rootSnapshot"]:
1600             if kwargs and kwargs.get("name") == vm["name"]:
1601                 return {vm["name"]: _get_snapshots(vm["snapshot"].rootSnapshotList)}
1602             else:
1603                 ret[vm["name"]] = _get_snapshots(vm["snapshot"].rootSnapshotList)
1604         else:
1605             if kwargs and kwargs.get("name") == vm["name"]:
1606                 return {}
1607     return ret
1608 def start(name, call=None):
1609     if call != "action":
1610         raise SaltCloudSystemExit(
1611             "The start action must be called with -a or --action."
1612         )
1613     vm_properties = ["name", "summary.runtime.powerState"]
1614     vm_list = salt.utils.vmware.get_mors_with_properties(
1615         _get_si(), vim.VirtualMachine, vm_properties
1616     )
1617     for vm in vm_list:
1618         if vm["name"] == name:
1619             if vm["summary.runtime.powerState"] == "poweredOn":
1620                 ret = "already powered on"
1621                 log.info("VM %s %s", name, ret)
1622                 return ret
1623             try:
1624                 log.info("Starting VM %s", name)
1625                 task = vm["object"].PowerOn()
1626                 salt.utils.vmware.wait_for_task(task, name, "power on")
1627             except Exception as exc:  # pylint: disable=broad-except
1628                 log.error(
1629                     "Error while powering on VM %s: %s",
1630                     name,
1631                     exc,
1632                     exc_info_on_loglevel=logging.DEBUG,
1633                 )
1634                 return "failed to power on"
1635     return "powered on"
1636 def stop(name, soft=False, call=None):
1637     if call != "action":
1638         raise SaltCloudSystemExit("The stop action must be called with -a or --action.")
1639     vm_properties = ["name", "summary.runtime.powerState"]
1640     vm_list = salt.utils.vmware.get_mors_with_properties(
1641         _get_si(), vim.VirtualMachine, vm_properties
1642     )
1643     for vm in vm_list:
1644         if vm["name"] == name:
1645             if vm["summary.runtime.powerState"] == "poweredOff":
1646                 ret = "already powered off"
1647                 log.info("VM %s %s", name, ret)
1648                 return ret
1649             try:
1650                 log.info("Stopping VM %s", name)
1651                 if soft:
1652                     vm["object"].ShutdownGuest()
1653                 else:
1654                     task = vm["object"].PowerOff()
1655                     salt.utils.vmware.wait_for_task(task, name, "power off")
1656             except Exception as exc:  # pylint: disable=broad-except
1657                 log.error(
1658                     "Error while powering off VM %s: %s",
1659                     name,
1660                     exc,
1661                     exc_info_on_loglevel=logging.DEBUG,
1662                 )
1663                 return "failed to power off"
1664     return "powered off"
1665 def suspend(name, call=None):
1666     if call != "action":
1667         raise SaltCloudSystemExit(
1668             "The suspend action must be called with -a or --action."
1669         )
1670     vm_properties = ["name", "summary.runtime.powerState"]
1671     vm_list = salt.utils.vmware.get_mors_with_properties(
1672         _get_si(), vim.VirtualMachine, vm_properties
1673     )
1674     for vm in vm_list:
1675         if vm["name"] == name:
1676             if vm["summary.runtime.powerState"] == "poweredOff":
1677                 ret = "cannot suspend in powered off state"
1678                 log.info("VM %s %s", name, ret)
1679                 return ret
1680             elif vm["summary.runtime.powerState"] == "suspended":
1681                 ret = "already suspended"
1682                 log.info("VM %s %s", name, ret)
1683                 return ret
1684             try:
1685                 log.info("Suspending VM %s", name)
1686                 task = vm["object"].Suspend()
1687                 salt.utils.vmware.wait_for_task(task, name, "suspend")
1688             except Exception as exc:  # pylint: disable=broad-except
1689                 log.error(
1690                     "Error while suspending VM %s: %s",
1691                     name,
1692                     exc,
1693                     exc_info_on_loglevel=logging.DEBUG,
1694                 )
1695                 return "failed to suspend"
1696     return "suspended"
1697 def reset(name, soft=False, call=None):
1698     if call != "action":
1699         raise SaltCloudSystemExit(
1700             "The reset action must be called with -a or --action."
1701         )
1702     vm_properties = ["name", "summary.runtime.powerState"]
1703     vm_list = salt.utils.vmware.get_mors_with_properties(
1704         _get_si(), vim.VirtualMachine, vm_properties
1705     )
1706     for vm in vm_list:
1707         if vm["name"] == name:
1708             if (
1709                 vm["summary.runtime.powerState"] == "suspended"
1710                 or vm["summary.runtime.powerState"] == "poweredOff"
1711             ):
1712                 ret = "cannot reset in suspended/powered off state"
1713                 log.info("VM %s %s", name, ret)
1714                 return ret
1715             try:
1716                 log.info("Resetting VM %s", name)
1717                 if soft:
1718                     vm["object"].RebootGuest()
1719                 else:
1720                     task = vm["object"].ResetVM_Task()
1721                     salt.utils.vmware.wait_for_task(task, name, "reset")
1722             except Exception as exc:  # pylint: disable=broad-except
1723                 log.error(
1724                     "Error while resetting VM %s: %s",
1725                     name,
1726                     exc,
1727                     exc_info_on_loglevel=logging.DEBUG,
1728                 )
1729                 return "failed to reset"
1730     return "reset"
1731 def terminate(name, call=None):
1732     if call != "action":
1733         raise SaltCloudSystemExit(
1734             "The terminate action must be called with -a or --action."
1735         )
1736     vm_properties = ["name", "summary.runtime.powerState"]
1737     vm_list = salt.utils.vmware.get_mors_with_properties(
1738         _get_si(), vim.VirtualMachine, vm_properties
1739     )
1740     for vm in vm_list:
1741         if vm["name"] == name:
1742             if vm["summary.runtime.powerState"] == "poweredOff":
1743                 ret = "already powered off"
1744                 log.info("VM %s %s", name, ret)
1745                 return ret
1746             try:
1747                 log.info("Terminating VM %s", name)
1748                 vm["object"].Terminate()
1749             except Exception as exc:  # pylint: disable=broad-except
1750                 log.error(
1751                     "Error while terminating VM %s: %s",
1752                     name,
1753                     exc,
1754                     exc_info_on_loglevel=logging.DEBUG,
1755                 )
1756                 return "failed to terminate"
1757     return "terminated"
1758 def destroy(name, call=None):
1759     if call == "function":
1760         raise SaltCloudSystemExit(
1761             "The destroy action must be called with -d, --destroy, -a or --action."
1762         )
1763     __utils__["cloud.fire_event"](
1764         "event",
1765         "destroying instance",
1766         "salt/cloud/{}/destroying".format(name),
1767         args={"name": name},
1768         sock_dir=__opts__["sock_dir"],
1769         transport=__opts__["transport"],
1770     )
1771     vm_properties = ["name", "summary.runtime.powerState"]
1772     vm_list = salt.utils.vmware.get_mors_with_properties(
1773         _get_si(), vim.VirtualMachine, vm_properties
1774     )
1775     for vm in vm_list:
1776         if vm["name"] == name:
1777             if vm["summary.runtime.powerState"] != "poweredOff":
1778                 try:
1779                     log.info("Powering Off VM %s", name)
1780                     task = vm["object"].PowerOff()
1781                     salt.utils.vmware.wait_for_task(task, name, "power off")
1782                 except Exception as exc:  # pylint: disable=broad-except
1783                     log.error(
1784                         "Error while powering off VM %s: %s",
1785                         name,
1786                         exc,
1787                         exc_info_on_loglevel=logging.DEBUG,
1788                     )
1789                     return "failed to destroy"
1790             try:
1791                 log.info("Destroying VM %s", name)
1792                 task = vm["object"].Destroy_Task()
1793                 salt.utils.vmware.wait_for_task(task, name, "destroy")
1794             except Exception as exc:  # pylint: disable=broad-except
1795                 log.error(
1796                     "Error while destroying VM %s: %s",
1797                     name,
1798                     exc,
1799                     exc_info_on_loglevel=logging.DEBUG,
1800                 )
1801                 return "failed to destroy"
1802     __utils__["cloud.fire_event"](
1803         "event",
1804         "destroyed instance",
1805         "salt/cloud/{}/destroyed".format(name),
1806         args={"name": name},
1807         sock_dir=__opts__["sock_dir"],
1808         transport=__opts__["transport"],
1809     )
1810     if __opts__.get("update_cachedir", False) is True:
1811         __utils__["cloud.delete_minion_cachedir"](
1812             name, _get_active_provider_name().split(":")[0], __opts__
1813         )
1814     return True
1815 def create(vm_):
1816     try:
1817         if (
1818             vm_["profile"]
1819             and config.is_profile_configured(
1820                 __opts__,
1821                 _get_active_provider_name() or "vmware",
1822                 vm_["profile"],
1823                 vm_=vm_,
1824             )
1825             is False
1826         ):
1827             return False
1828     except AttributeError:
1829         pass
1830     __utils__["cloud.fire_event"](
1831         "event",
1832         "starting create",
1833         "salt/cloud/{}/creating".format(vm_["name"]),
1834         args=__utils__["cloud.filter_event"](
1835             "creating", vm_, ["name", "profile", "provider", "driver"]
1836         ),
1837         sock_dir=__opts__["sock_dir"],
1838         transport=__opts__["transport"],
1839     )
1840     vm_name = config.get_cloud_config_value("name", vm_, __opts__, default=None)
1841     folder = config.get_cloud_config_value("folder", vm_, __opts__, default=None)
1842     datacenter = config.get_cloud_config_value(
1843         "datacenter", vm_, __opts__, default=None
1844     )
1845     resourcepool = config.get_cloud_config_value(
1846         "resourcepool", vm_, __opts__, default=None
1847     )
1848     cluster = config.get_cloud_config_value("cluster", vm_, __opts__, default=None)
1849     datastore = config.get_cloud_config_value("datastore", vm_, __opts__, default=None)
1850     host = config.get_cloud_config_value("host", vm_, __opts__, default=None)
1851     template = config.get_cloud_config_value("template", vm_, __opts__, default=False)
1852     num_cpus = config.get_cloud_config_value("num_cpus", vm_, __opts__, default=None)
1853     cores_per_socket = config.get_cloud_config_value(
1854         "cores_per_socket", vm_, __opts__, default=None
1855     )
1856     instant_clone = config.get_cloud_config_value(
1857         "instant_clone", vm_, __opts__, default=False
1858     )
1859     memory = config.get_cloud_config_value("memory", vm_, __opts__, default=None)
1860     devices = config.get_cloud_config_value("devices", vm_, __opts__, default=None)
1861     extra_config = config.get_cloud_config_value(
1862         "extra_config", vm_, __opts__, default=None
1863     )
1864     annotation = config.get_cloud_config_value(
1865         "annotation", vm_, __opts__, default=None
1866     )
1867     power = config.get_cloud_config_value("power_on", vm_, __opts__, default=True)
1868     key_filename = config.get_cloud_config_value(
1869         "private_key", vm_, __opts__, search_global=False, default=None
1870     )
1871     deploy = config.get_cloud_config_value(
1872         "deploy", vm_, __opts__, search_global=True, default=True
1873     )
1874     wait_for_ip_timeout = config.get_cloud_config_value(
1875         "wait_for_ip_timeout", vm_, __opts__, default=20 * 60
1876     )
1877     domain = config.get_cloud_config_value(
1878         "domain", vm_, __opts__, search_global=False, default="local"
1879     )
1880     hardware_version = config.get_cloud_config_value(
1881         "hardware_version", vm_, __opts__, search_global=False, default=None
1882     )
1883     guest_id = config.get_cloud_config_value(
1884         "image", vm_, __opts__, search_global=False, default=None
1885     )
1886     customization = config.get_cloud_config_value(
1887         "customization", vm_, __opts__, search_global=False, default=True
1888     )
1889     customization_spec = config.get_cloud_config_value(
1890         "customization_spec", vm_, __opts__, search_global=False, default=None
1891     )
1892     win_password = config.get_cloud_config_value(
1893         "win_password", vm_, __opts__, search_global=False, default=None
1894     )
1895     win_organization_name = config.get_cloud_config_value(
1896         "win_organization_name",
1897         vm_,
1898         __opts__,
1899         search_global=False,
1900         default="Organization",
1901     )
1902     plain_text = config.get_cloud_config_value(
1903         "plain_text", vm_, __opts__, search_global=False, default=False
1904     )
1905     win_user_fullname = config.get_cloud_config_value(
1906         "win_user_fullname", vm_, __opts__, search_global=False, default="Windows User"
1907     )
1908     win_run_once = config.get_cloud_config_value(
1909         "win_run_once", vm_, __opts__, search_global=False, default=None
1910     )
1911     cpu_hot_add = config.get_cloud_config_value(
1912         "cpu_hot_add", vm_, __opts__, search_global=False, default=None
1913     )
1914     cpu_hot_remove = config.get_cloud_config_value(
1915         "cpu_hot_remove", vm_, __opts__, search_global=False, default=None
1916     )
1917     mem_hot_add = config.get_cloud_config_value(
1918         "mem_hot_add", vm_, __opts__, search_global=False, default=None
1919     )
1920     nested_hv = config.get_cloud_config_value(
1921         "nested_hv", vm_, __opts__, search_global=False, default=None
1922     )
1923     vpmc = config.get_cloud_config_value(
1924         "vpmc", vm_, __opts__, search_global=False, default=None
1925     )
1926     si = _get_si()
1927     container_ref = None
1928     if datacenter:
1929         datacenter_ref = salt.utils.vmware.get_mor_by_property(
1930             _get_si(), vim.Datacenter, datacenter
1931         )
1932         container_ref = datacenter_ref if datacenter_ref else None
1933     if "clonefrom" in vm_:
1934         if datacenter:
1935             datacenter_ref = salt.utils.vmware.get_mor_by_property(
1936                 si, vim.Datacenter, datacenter
1937             )
1938             container_ref = datacenter_ref if datacenter_ref else None
1939         object_ref = salt.utils.vmware.get_mor_by_property(
1940             si, vim.VirtualMachine, vm_["clonefrom"], container_ref=container_ref
1941         )
1942         if object_ref:
1943             clone_type = "template" if object_ref.config.template else "vm"
1944         else:
1945             raise SaltCloudSystemExit(
1946                 "The VM/template that you have specified under clonefrom does not"
1947                 " exist."
1948             )
1949     else:
1950         clone_type = None
1951         object_ref = None
1952     if resourcepool:
1953         resourcepool_ref = salt.utils.vmware.get_mor_by_property(
1954             si, vim.ResourcePool, resourcepool, container_ref=container_ref
1955         )
1956         if not resourcepool_ref:
1957             log.error("Specified resource pool: '%s' does not exist", resourcepool)
1958             if not clone_type or clone_type == "template":
1959                 raise SaltCloudSystemExit(
1960                     "You must specify a resource pool that exists."
1961                 )
1962     elif cluster:
1963         cluster_ref = salt.utils.vmware.get_mor_by_property(
1964             si, vim.ClusterComputeResource, cluster, container_ref=container_ref
1965         )
1966         if not cluster_ref:
1967             log.error("Specified cluster: '%s' does not exist", cluster)
1968             if not clone_type or clone_type == "template":
1969                 raise SaltCloudSystemExit("You must specify a cluster that exists.")
1970         else:
1971             resourcepool_ref = cluster_ref.resourcePool
1972     elif clone_type == "template":
1973         raise SaltCloudSystemExit(
1974             "You must either specify a cluster or a resource pool when cloning from a"
1975             " template."
1976         )
1977     elif not clone_type:
1978         raise SaltCloudSystemExit(
1979             "You must either specify a cluster or a resource pool when creating."
1980         )
1981     else:
1982         log.debug("Using resource pool used by the %s %s", clone_type, vm_["clonefrom"])
1983     if folder:
1984         folder_parts = folder.split("/")
1985         search_reference = container_ref
1986         for folder_part in folder_parts:
1987             if folder_part:
1988                 folder_ref = salt.utils.vmware.get_mor_by_property(
1989                     si, vim.Folder, folder_part, container_ref=search_reference
1990                 )
1991                 search_reference = folder_ref
1992         if not folder_ref:
1993             log.error("Specified folder: '%s' does not exist", folder)
1994             log.debug(
1995                 "Using folder in which %s %s is present", clone_type, vm_["clonefrom"]
1996             )
1997             folder_ref = object_ref.parent
1998     elif datacenter:
1999         if not datacenter_ref:
2000             log.error("Specified datacenter: '%s' does not exist", datacenter)
2001             log.debug(
2002                 "Using datacenter folder in which %s %s is present",
2003                 clone_type,
2004                 vm_["clonefrom"],
2005             )
2006             folder_ref = object_ref.parent
2007         else:
2008             folder_ref = datacenter_ref.vmFolder
2009     elif not clone_type:
2010         raise SaltCloudSystemExit(
2011             "You must either specify a folder or a datacenter when creating not"
2012             " cloning."
2013         )
2014     else:
2015         log.debug(
2016             "Using folder in which %s %s is present", clone_type, vm_["clonefrom"]
2017         )
2018         folder_ref = object_ref.parent
2019     if "clonefrom" in vm_:
2020         reloc_spec = vim.vm.RelocateSpec()
2021         if (resourcepool and resourcepool_ref) or (cluster and cluster_ref):
2022             reloc_spec.pool = resourcepool_ref
2023         if datastore:
2024             datastore_ref = salt.utils.vmware.get_mor_by_property(
2025                 si, vim.Datastore, datastore, container_ref=container_ref
2026             )
2027             if datastore_ref:
2028                 reloc_spec.datastore = datastore_ref
2029             else:
2030                 datastore_cluster_ref = salt.utils.vmware.get_mor_by_property(
2031                     si, vim.StoragePod, datastore, container_ref=container_ref
2032                 )
2033                 if not datastore_cluster_ref:
2034                     log.error(
2035                         "Specified datastore/datastore cluster: '%s' does not exist",
2036                         datastore,
2037                     )
2038                     log.debug(
2039                         "Using datastore used by the %s %s",
2040                         clone_type,
2041                         vm_["clonefrom"],
2042                     )
2043         else:
2044             log.debug("No datastore/datastore cluster specified")
2045             log.debug("Using datastore used by the %s %s", clone_type, vm_["clonefrom"])
2046         if host:
2047             host_ref = salt.utils.vmware.get_mor_by_property(
2048                 si, vim.HostSystem, host, container_ref=container_ref
2049             )
2050             if host_ref:
2051                 reloc_spec.host = host_ref
2052             else:
2053                 log.error("Specified host: '%s' does not exist", host)
2054         if instant_clone:
2055             instant_clone_spec = vim.vm.InstantCloneSpec()
2056             instant_clone_spec.name = vm_name
2057             instant_clone_spec.location = reloc_spec
2058             event_kwargs = vm_.copy()
2059             if event_kwargs.get("password"):
2060                 del event_kwargs["password"]
2061             try:
2062                 __utils__["cloud.fire_event"](
2063                     "event",
2064                     "requesting instance",
2065                     "salt/cloud/{}/requesting".format(vm_["name"]),
2066                     args=__utils__["cloud.filter_event"](
2067                         "requesting", event_kwargs, list(event_kwargs)
2068                     ),
2069                     sock_dir=__opts__["sock_dir"],
2070                     transport=__opts__["transport"],
2071                 )
2072                 log.info(
2073                     "Creating %s from %s(%s)", vm_["name"], clone_type, vm_["clonefrom"]
2074                 )
2075                 if datastore and not datastore_ref and datastore_cluster_ref:
2076                     pod_spec = vim.storageDrs.PodSelectionSpec(
2077                         storagePod=datastore_cluster_ref
2078                     )
2079                     storage_spec = vim.storageDrs.StoragePlacementSpec(
2080                         type="clone",
2081                         vm=object_ref,
2082                         podSelectionSpec=pod_spec,
2083                         cloneName=vm_name,
2084                         folder=folder_ref,
2085                     )
2086                     recommended_datastores = (
2087                         si.content.storageResourceManager.RecommendDatastores(
2088                             storageSpec=storage_spec
2089                         )
2090                     )
2091                     task = si.content.storageResourceManager.ApplyStorageDrsRecommendation_Task(
2092                         recommended_datastores.recommendations[0].key
2093                     )
2094                     salt.utils.vmware.wait_for_task(
2095                         task, vm_name, "apply storage DRS recommendations", 5, "info"
2096                     )
2097                 else:
2098                     task = object_ref.InstantClone_Task(spec=instant_clone_spec)
2099                     salt.utils.vmware.wait_for_task(
2100                         task, vm_name, "Instantclone", 5, "info"
2101                     )
2102             except Exception as exc:  # pylint: disable=broad-except
2103                 err_msg = "Error Instant cloning {}: {}".format(vm_["name"], exc)
2104                 log.error(
2105                     err_msg,
2106                     exc_info_on_loglevel=logging.DEBUG,
2107                 )
2108                 return {"Error": err_msg}
2109             new_vm_ref = salt.utils.vmware.get_mor_by_property(
2110                 si, vim.VirtualMachine, vm_name, container_ref=container_ref
2111             )
2112             out = None
2113             if not template and power:
2114                 ip = _wait_for_ip(new_vm_ref, wait_for_ip_timeout)
2115                 if ip:
2116                     log.info("[ %s ] IPv4 is: %s", vm_name, ip)
2117                     if deploy:
2118                         vm_["key_filename"] = key_filename
2119                         if "ssh_host" not in vm_:
2120                             vm_["ssh_host"] = ip
2121                         log.info("[ %s ] Deploying to %s", vm_name, vm_["ssh_host"])
2122                         out = __utils__["cloud.bootstrap"](vm_, __opts__)
2123             data = show_instance(vm_name, call="action")
2124             if deploy and isinstance(out, dict):
2125                 data["deploy_kwargs"] = out.get("deploy_kwargs", {})
2126             __utils__["cloud.fire_event"](
2127                 "event",
2128                 "created instance",
2129                 "salt/cloud/{}/created".format(vm_["name"]),
2130                 args=__utils__["cloud.filter_event"](
2131                     "created", vm_, ["name", "profile", "provider", "driver"]
2132                 ),
2133                 sock_dir=__opts__["sock_dir"],
2134                 transport=__opts__["transport"],
2135             )
2136             return {"Instant Clone created successfully": data}
2137     else:
2138         if not datastore:
2139             raise SaltCloudSystemExit(
2140                 "You must specify a datastore when creating not cloning."
2141             )
2142         else:
2143             datastore_ref = salt.utils.vmware.get_mor_by_property(
2144                 si, vim.Datastore, datastore
2145             )
2146             if not datastore_ref:
2147                 raise SaltCloudSystemExit(
2148                     "Specified datastore: '{}' does not exist".format(datastore)
2149                 )
2150         if host:
2151             host_ref = salt.utils.vmware.get_mor_by_property(
2152                 _get_si(), vim.HostSystem, host, container_ref=container_ref
2153             )
2154             if not host_ref:
2155                 log.error("Specified host: '%s' does not exist", host)
2156     config_spec = vim.vm.ConfigSpec()
2157     if hardware_version and object_ref is not None:
2158         hardware_version = "vmx-{:02}".format(hardware_version)
2159         if hardware_version != object_ref.config.version:
2160             log.debug(
2161                 "Scheduling hardware version upgrade from %s to %s",
2162                 object_ref.config.version,
2163                 hardware_version,
2164             )
2165             scheduled_hardware_upgrade = vim.vm.ScheduledHardwareUpgradeInfo()
2166             scheduled_hardware_upgrade.upgradePolicy = "always"
2167             scheduled_hardware_upgrade.versionKey = hardware_version
2168             config_spec.scheduledHardwareUpgradeInfo = scheduled_hardware_upgrade
2169         else:
2170             log.debug("Virtual hardware version already set to %s", hardware_version)
2171     if num_cpus:
2172         log.debug("Setting cpu to: %s", num_cpus)
2173         config_spec.numCPUs = int(num_cpus)
2174     if cores_per_socket:
2175         log.debug("Setting cores per socket to: %s", cores_per_socket)
2176         config_spec.numCoresPerSocket = int(cores_per_socket)
2177     if memory:
2178         try:
2179             memory_num, memory_unit = re.findall(r"[^\W\d_]+|\d+.\d+|\d+", memory)
2180             if memory_unit.lower() == "mb":
2181                 memory_mb = int(memory_num)
2182             elif memory_unit.lower() == "gb":
2183                 memory_mb = int(float(memory_num) * 1024.0)
2184             else:
2185                 err_msg = "Invalid memory type specified: '{}'".format(memory_unit)
2186                 log.error(err_msg)
2187                 return {"Error": err_msg}
2188         except (TypeError, ValueError):
2189             memory_mb = int(memory)
2190         log.debug("Setting memory to: %s MB", memory_mb)
2191         config_spec.memoryMB = memory_mb
2192     if devices:
2193         specs = _manage_devices(
2194             devices, vm=object_ref, container_ref=container_ref, new_vm_name=vm_name
2195         )
2196         config_spec.deviceChange = specs["device_specs"]
2197     if cpu_hot_add and hasattr(config_spec, "cpuHotAddEnabled"):
2198         config_spec.cpuHotAddEnabled = bool(cpu_hot_add)
2199     if cpu_hot_remove and hasattr(config_spec, "cpuHotRemoveEnabled"):
2200         config_spec.cpuHotRemoveEnabled = bool(cpu_hot_remove)
2201     if mem_hot_add and hasattr(config_spec, "memoryHotAddEnabled"):
2202         config_spec.memoryHotAddEnabled = bool(mem_hot_add)
2203     if nested_hv and hasattr(config_spec, "nestedHVEnabled"):
2204         config_spec.nestedHVEnabled = bool(nested_hv)
2205     if vpmc and hasattr(config_spec, "vPMCEnabled"):
2206         config_spec.vPMCEnabled = bool(vpmc)
2207     if extra_config:
2208         for key, value in extra_config.items():
2209             option = vim.option.OptionValue(key=key, value=value)
2210             config_spec.extraConfig.append(option)
2211     if annotation:
2212         config_spec.annotation = str(annotation)
2213     if "clonefrom" in vm_:
2214         clone_spec = handle_snapshot(config_spec, object_ref, reloc_spec, template, vm_)
2215         if not clone_spec:
2216             clone_spec = build_clonespec(config_spec, object_ref, reloc_spec, template)
2217         if customization and customization_spec:
2218             customization_spec = salt.utils.vmware.get_customizationspec_ref(
2219                 si=si, customization_spec_name=customization_spec
2220             )
2221             clone_spec.customization = customization_spec.spec
2222         elif customization and (devices and "network" in list(devices.keys())):
2223             global_ip = vim.vm.customization.GlobalIPSettings()
2224             if "dns_servers" in list(vm_.keys()):
2225                 global_ip.dnsServerList = vm_["dns_servers"]
2226             if "domain" in list(vm_.keys()):
2227                 global_ip.dnsSuffixList = vm_["domain"]
2228             non_hostname_chars = re.compile(r"[^\w-]")
2229             if re.search(non_hostname_chars, vm_name):
2230                 host_name = re.split(non_hostname_chars, vm_name, maxsplit=1)[0]
2231                 domain_name = re.split(non_hostname_chars, vm_name, maxsplit=1)[-1]
2232             else:
2233                 host_name = vm_name
2234                 domain_name = domain
2235             if "Windows" not in object_ref.config.guestFullName:
2236                 identity = vim.vm.customization.LinuxPrep()
2237                 identity.hostName = vim.vm.customization.FixedName(name=host_name)
2238                 identity.domain = domain_name
2239             else:
2240                 identity = vim.vm.customization.Sysprep()
2241                 identity.guiUnattended = vim.vm.customization.GuiUnattended()
2242                 identity.guiUnattended.autoLogon = True
2243                 identity.guiUnattended.autoLogonCount = 1
2244                 identity.guiUnattended.password = vim.vm.customization.Password()
2245                 identity.guiUnattended.password.value = win_password
2246                 identity.guiUnattended.password.plainText = plain_text
2247                 if win_run_once:
2248                     identity.guiRunOnce = vim.vm.customization.GuiRunOnce()
2249                     identity.guiRunOnce.commandList = win_run_once
2250                 identity.userData = vim.vm.customization.UserData()
2251                 identity.userData.fullName = win_user_fullname
2252                 identity.userData.orgName = win_organization_name
2253                 identity.userData.computerName = vim.vm.customization.FixedName()
2254                 identity.userData.computerName.name = host_name
2255                 identity.identification = vim.vm.customization.Identification()
2256             custom_spec = vim.vm.customization.Specification(
2257                 globalIPSettings=global_ip,
2258                 identity=identity,
2259                 nicSettingMap=specs["nics_map"],
2260             )
2261             clone_spec.customization = custom_spec
2262         if not template:
2263             clone_spec.powerOn = power
2264         log.debug("clone_spec set to:\n%s", pprint.pformat(clone_spec))
2265     else:
2266         config_spec.name = vm_name
2267         config_spec.files = vim.vm.FileInfo()
2268         config_spec.files.vmPathName = "[{0}] {1}/{1}.vmx".format(datastore, vm_name)
2269         config_spec.guestId = guest_id
2270         log.debug("config_spec set to:\n%s", pprint.pformat(config_spec))
2271     event_kwargs = vm_.copy()
2272     if event_kwargs.get("password"):
2273         del event_kwargs["password"]
2274     try:
2275         __utils__["cloud.fire_event"](
2276             "event",
2277             "requesting instance",
2278             "salt/cloud/{}/requesting".format(vm_["name"]),
2279             args=__utils__["cloud.filter_event"](
2280                 "requesting", event_kwargs, list(event_kwargs)
2281             ),
2282             sock_dir=__opts__["sock_dir"],
2283             transport=__opts__["transport"],
2284         )
2285         if "clonefrom" in vm_:
2286             log.info(
2287                 "Creating %s from %s(%s)", vm_["name"], clone_type, vm_["clonefrom"]
2288             )
2289             if datastore and not datastore_ref and datastore_cluster_ref:
2290                 pod_spec = vim.storageDrs.PodSelectionSpec(
2291                     storagePod=datastore_cluster_ref
2292                 )
2293                 storage_spec = vim.storageDrs.StoragePlacementSpec(
2294                     type="clone",
2295                     vm=object_ref,
2296                     podSelectionSpec=pod_spec,
2297                     cloneSpec=clone_spec,
2298                     cloneName=vm_name,
2299                     folder=folder_ref,
2300                 )
2301                 recommended_datastores = (
2302                     si.content.storageResourceManager.RecommendDatastores(
2303                         storageSpec=storage_spec
2304                     )
2305                 )
2306                 task = si.content.storageResourceManager.ApplyStorageDrsRecommendation_Task(
2307                     recommended_datastores.recommendations[0].key
2308                 )
2309                 salt.utils.vmware.wait_for_task(
2310                     task, vm_name, "apply storage DRS recommendations", 5, "info"
2311                 )
2312             else:
2313                 task = object_ref.Clone(folder_ref, vm_name, clone_spec)
2314                 salt.utils.vmware.wait_for_task(task, vm_name, "clone", 5, "info")
2315         else:
2316             log.info("Creating %s", vm_["name"])
2317             if host:
2318                 task = folder_ref.CreateVM_Task(config_spec, resourcepool_ref, host_ref)
2319             else:
2320                 task = folder_ref.CreateVM_Task(config_spec, resourcepool_ref)
2321             salt.utils.vmware.wait_for_task(task, vm_name, "create", 15, "info")
2322     except Exception as exc:  # pylint: disable=broad-except
2323         err_msg = "Error creating {}: {}".format(vm_["name"], exc)
2324         log.error(
2325             err_msg,
2326             exc_info_on_loglevel=logging.DEBUG,
2327         )
2328         return {"Error": err_msg}
2329     new_vm_ref = salt.utils.vmware.get_mor_by_property(
2330         si, vim.VirtualMachine, vm_name, container_ref=container_ref
2331     )
2332     try:
2333         if not clone_type and power:
2334             task = new_vm_ref.PowerOn()
2335             salt.utils.vmware.wait_for_task(task, vm_name, "power", 5, "info")
2336     except Exception as exc:  # pylint: disable=broad-except
2337         log.info("Powering on the VM threw this exception. Ignoring.")
2338         log.info(exc)
2339     out = None
2340     if not template and power:
2341         ip = _wait_for_ip(new_vm_ref, wait_for_ip_timeout)
2342         if ip:
2343             log.info("[ %s ] IPv4 is: %s", vm_name, ip)
2344             if deploy:
2345                 vm_["key_filename"] = key_filename
2346                 if "ssh_host" not in vm_:
2347                     vm_["ssh_host"] = ip
2348                 log.info("[ %s ] Deploying to %s", vm_name, vm_["ssh_host"])
2349                 out = __utils__["cloud.bootstrap"](vm_, __opts__)
2350     data = show_instance(vm_name, call="action")
2351     if deploy and isinstance(out, dict):
2352         data["deploy_kwargs"] = out.get("deploy_kwargs", {})
2353     __utils__["cloud.fire_event"](
2354         "event",
2355         "created instance",
2356         "salt/cloud/{}/created".format(vm_["name"]),
2357         args=__utils__["cloud.filter_event"](
2358             "created", vm_, ["name", "profile", "provider", "driver"]
2359         ),
2360         sock_dir=__opts__["sock_dir"],
2361         transport=__opts__["transport"],
2362     )
2363     return data
2364 def handle_snapshot(config_spec, object_ref, reloc_spec, template, vm_):
2365     if "snapshot" not in vm_:
2366         return None
2367     allowed_types = [
2368         FLATTEN_DISK_FULL_CLONE,
2369         COPY_ALL_DISKS_FULL_CLONE,
2370         CURRENT_STATE_LINKED_CLONE,
2371         QUICK_LINKED_CLONE,
2372     ]
2373     clone_spec = get_clonespec_for_valid_snapshot(
2374         config_spec, object_ref, reloc_spec, template, vm_
2375     )
2376     if not clone_spec:
2377         raise SaltCloudSystemExit(
2378             "Invalid disk move type specified supported types are {}".format(
2379                 " ".join(allowed_types)
2380             )
2381         )
2382     return clone_spec
2383 def get_clonespec_for_valid_snapshot(
2384     config_spec, object_ref, reloc_spec, template, vm_
2385 ):
2386     moving = True
2387     if QUICK_LINKED_CLONE == vm_["snapshot"]["disk_move_type"]:
2388         reloc_spec.diskMoveType = QUICK_LINKED_CLONE
2389     elif CURRENT_STATE_LINKED_CLONE == vm_["snapshot"]["disk_move_type"]:
2390         reloc_spec.diskMoveType = CURRENT_STATE_LINKED_CLONE
2391     elif COPY_ALL_DISKS_FULL_CLONE == vm_["snapshot"]["disk_move_type"]:
2392         reloc_spec.diskMoveType = COPY_ALL_DISKS_FULL_CLONE
2393     elif FLATTEN_DISK_FULL_CLONE == vm_["snapshot"]["disk_move_type"]:
2394         reloc_spec.diskMoveType = FLATTEN_DISK_FULL_CLONE
2395     else:
2396         moving = False
2397     if moving:
2398         return build_clonespec(config_spec, object_ref, reloc_spec, template)
2399     return None
2400 def build_clonespec(config_spec, object_ref, reloc_spec, template):
2401     if reloc_spec.diskMoveType == QUICK_LINKED_CLONE:
2402         return vim.vm.CloneSpec(
2403             template=template,
2404             location=reloc_spec,
2405             config=config_spec,
2406             snapshot=object_ref.snapshot.currentSnapshot,
2407         )
2408     return vim.vm.CloneSpec(template=template, location=reloc_spec, config=config_spec)
2409 def create_datacenter(kwargs=None, call=None):
2410     if call != "function":
2411         raise SaltCloudSystemExit(
2412             "The create_datacenter function must be called with -f or --function."
2413         )
2414     datacenter_name = kwargs.get("name") if kwargs and "name" in kwargs else None
2415     if not datacenter_name:
2416         raise SaltCloudSystemExit(
2417             "You must specify name of the new datacenter to be created."
2418         )
2419     if not datacenter_name or len(datacenter_name) &gt;= 80:
2420         raise SaltCloudSystemExit(
2421             "The datacenter name must be a non empty string of less than 80 characters."
2422         )
2423     si = _get_si()
2424     datacenter_ref = salt.utils.vmware.get_mor_by_property(
2425         si, vim.Datacenter, datacenter_name
2426     )
2427     if datacenter_ref:
2428         return {datacenter_name: "datacenter already exists"}
2429     folder = si.content.rootFolder
2430     if isinstance(folder, vim.Folder):
2431         try:
2432             folder.CreateDatacenter(name=datacenter_name)
2433         except Exception as exc:  # pylint: disable=broad-except
2434             log.error(
2435                 "Error creating datacenter %s: %s",
2436                 datacenter_name,
2437                 exc,
2438                 exc_info_on_loglevel=logging.DEBUG,
2439             )
2440             return False
2441         log.debug("Created datacenter %s", datacenter_name)
2442         return {datacenter_name: "created"}
2443     return False
2444 def create_cluster(kwargs=None, call=None):
2445     if call != "function":
2446         raise SaltCloudSystemExit(
2447             "The create_cluster function must be called with -f or --function."
2448         )
2449     cluster_name = kwargs.get("name") if kwargs and "name" in kwargs else None
2450     datacenter = kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
2451     if not cluster_name:
2452         raise SaltCloudSystemExit(
2453             "You must specify name of the new cluster to be created."
2454         )
2455     if not datacenter:
2456         raise SaltCloudSystemExit(
2457             "You must specify name of the datacenter where the cluster should be"
2458             " created."
2459         )
2460     si = _get_si()
2461     if not isinstance(datacenter, vim.Datacenter):
2462         datacenter = salt.utils.vmware.get_mor_by_property(
2463             si, vim.Datacenter, datacenter
2464         )
2465         if not datacenter:
2466             raise SaltCloudSystemExit("The specified datacenter does not exist.")
2467     cluster_ref = salt.utils.vmware.get_mor_by_property(
2468         si, vim.ClusterComputeResource, cluster_name
2469     )
2470     if cluster_ref:
2471         return {cluster_name: "cluster already exists"}
2472     cluster_spec = vim.cluster.ConfigSpecEx()
2473     folder = datacenter.hostFolder
2474     if isinstance(folder, vim.Folder):
2475         try:
2476             folder.CreateClusterEx(name=cluster_name, spec=cluster_spec)
2477         except Exception as exc:  # pylint: disable=broad-except
2478             log.error(
2479                 "Error creating cluster %s: %s",
2480                 cluster_name,
2481                 exc,
2482                 exc_info_on_loglevel=logging.DEBUG,
2483             )
2484             return False
2485         log.debug(
2486             "Created cluster %s under datacenter %s", cluster_name, datacenter.name
2487         )
2488         return {cluster_name: "created"}
2489     return False
2490 def rescan_hba(kwargs=None, call=None):
2491     if call != "function":
2492         raise SaltCloudSystemExit(
2493             "The rescan_hba function must be called with -f or --function."
2494         )
2495     hba = kwargs.get("hba") if kwargs and "hba" in kwargs else None
2496     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
2497     if not host_name:
2498         raise SaltCloudSystemExit("You must specify name of the host system.")
2499     host_ref = salt.utils.vmware.get_mor_by_property(
2500         _get_si(), vim.HostSystem, host_name
2501     )
2502     try:
2503         if hba:
2504             log.info("Rescanning HBA %s on host %s", hba, host_name)
2505             host_ref.configManager.storageSystem.RescanHba(hba)
2506             ret = "rescanned HBA {}".format(hba)
2507         else:
2508             log.info("Rescanning all HBAs on host %s", host_name)
2509             host_ref.configManager.storageSystem.RescanAllHba()
2510             ret = "rescanned all HBAs"
2511     except Exception as exc:  # pylint: disable=broad-except
2512         log.error(
2513             "Error while rescaning HBA on host %s: %s",
2514             host_name,
2515             exc,
2516             exc_info_on_loglevel=logging.DEBUG,
2517         )
2518         return {host_name: "failed to rescan HBA"}
2519     return {host_name: ret}
2520 def upgrade_tools_all(call=None):
2521     if call != "function":
2522         raise SaltCloudSystemExit(
2523             "The upgrade_tools_all function must be called with -f or --function."
2524         )
2525     ret = {}
2526     vm_properties = ["name"]
2527     vm_list = salt.utils.vmware.get_mors_with_properties(
2528         _get_si(), vim.VirtualMachine, vm_properties
2529     )
2530     for vm in vm_list:
2531         ret[vm["name"]] = _upg_tools_helper(vm["object"])
2532     return ret
2533 def upgrade_tools(name, reboot=False, call=None):
2534     if call != "action":
2535         raise SaltCloudSystemExit(
2536             "The upgrade_tools action must be called with -a or --action."
2537         )
2538     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2539     return _upg_tools_helper(vm_ref, reboot)
2540 def list_hosts_by_cluster(kwargs=None, call=None):
2541     if call != "function":
2542         raise SaltCloudSystemExit(
2543             "The list_hosts_by_cluster function must be called with -f or --function."
2544         )
2545     ret = {}
2546     cluster_name = kwargs.get("cluster") if kwargs and "cluster" in kwargs else None
2547     cluster_properties = ["name"]
2548     cluster_list = salt.utils.vmware.get_mors_with_properties(
2549         _get_si(), vim.ClusterComputeResource, cluster_properties
2550     )
2551     for cluster in cluster_list:
2552         ret[cluster["name"]] = []
2553         for host in cluster["object"].host:
2554             if isinstance(host, vim.HostSystem):
2555                 ret[cluster["name"]].append(host.name)
2556         if cluster_name and cluster_name == cluster["name"]:
2557             return {"Hosts by Cluster": {cluster_name: ret[cluster_name]}}
2558     return {"Hosts by Cluster": ret}
2559 def list_clusters_by_datacenter(kwargs=None, call=None):
2560     if call != "function":
2561         raise SaltCloudSystemExit(
2562             "The list_clusters_by_datacenter function must be called with "
2563             "-f or --function."
2564         )
2565     ret = {}
2566     datacenter_name = (
2567         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
2568     )
2569     datacenter_properties = ["name"]
2570     datacenter_list = salt.utils.vmware.get_mors_with_properties(
2571         _get_si(), vim.Datacenter, datacenter_properties
2572     )
2573     for datacenter in datacenter_list:
2574         ret[datacenter["name"]] = []
2575         for cluster in datacenter["object"].hostFolder.childEntity:
2576             if isinstance(cluster, vim.ClusterComputeResource):
2577                 ret[datacenter["name"]].append(cluster.name)
2578         if datacenter_name and datacenter_name == datacenter["name"]:
2579             return {"Clusters by Datacenter": {datacenter_name: ret[datacenter_name]}}
2580     return {"Clusters by Datacenter": ret}
2581 def list_hosts_by_datacenter(kwargs=None, call=None):
2582     if call != "function":
2583         raise SaltCloudSystemExit(
2584             "The list_hosts_by_datacenter function must be called with "
2585             "-f or --function."
2586         )
2587     ret = {}
2588     datacenter_name = (
2589         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
2590     )
2591     datacenter_properties = ["name"]
2592     datacenter_list = salt.utils.vmware.get_mors_with_properties(
2593         _get_si(), vim.Datacenter, datacenter_properties
2594     )
2595     for datacenter in datacenter_list:
2596         ret[datacenter["name"]] = []
2597         for cluster in datacenter["object"].hostFolder.childEntity:
2598             if isinstance(cluster, vim.ClusterComputeResource):
2599                 for host in cluster.host:
2600                     if isinstance(host, vim.HostSystem):
2601                         ret[datacenter["name"]].append(host.name)
2602         if datacenter_name and datacenter_name == datacenter["name"]:
2603             return {"Hosts by Datacenter": {datacenter_name: ret[datacenter_name]}}
2604     return {"Hosts by Datacenter": ret}
2605 def list_hbas(kwargs=None, call=None):
2606     if call != "function":
2607         raise SaltCloudSystemExit(
2608             "The list_hbas function must be called with -f or --function."
2609         )
2610     ret = {}
2611     hba_type = kwargs.get("type").lower() if kwargs and "type" in kwargs else None
2612     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
2613     host_properties = ["name", "config.storageDevice.hostBusAdapter"]
2614     if hba_type and hba_type not in ["parallel", "block", "iscsi", "fibre"]:
2615         raise SaltCloudSystemExit(
2616             "Specified hba type {} currently not supported.".format(hba_type)
2617         )
2618     host_list = salt.utils.vmware.get_mors_with_properties(
2619         _get_si(), vim.HostSystem, host_properties
2620     )
2621     for host in host_list:
2622         ret[host["name"]] = {}
2623         for hba in host["config.storageDevice.hostBusAdapter"]:
2624             hba_spec = {
2625                 "driver": hba.driver,
2626                 "status": hba.status,
2627                 "type": type(hba).__name__.rsplit(".", 1)[1],
2628             }
2629             if hba_type:
2630                 if isinstance(hba, _get_hba_type(hba_type)):
2631                     if hba.model in ret[host["name"]]:
2632                         ret[host["name"]][hba.model][hba.device] = hba_spec
2633                     else:
2634                         ret[host["name"]][hba.model] = {hba.device: hba_spec}
2635             else:
2636                 if hba.model in ret[host["name"]]:
2637                     ret[host["name"]][hba.model][hba.device] = hba_spec
2638                 else:
2639                     ret[host["name"]][hba.model] = {hba.device: hba_spec}
2640         if host["name"] == host_name:
2641             return {"HBAs by Host": {host_name: ret[host_name]}}
2642     return {"HBAs by Host": ret}
2643 def list_dvs(kwargs=None, call=None):
2644     if call != "function":
2645         raise SaltCloudSystemExit(
2646             "The list_dvs function must be called with -f or --function."
2647         )
2648     return {"Distributed Virtual Switches": salt.utils.vmware.list_dvs(_get_si())}
2649 def list_vapps(kwargs=None, call=None):
2650     if call != "function":
2651         raise SaltCloudSystemExit(
2652             "The list_vapps function must be called with -f or --function."
2653         )
2654     return {"vApps": salt.utils.vmware.list_vapps(_get_si())}
2655 def enter_maintenance_mode(kwargs=None, call=None):
2656     if call != "function":
2657         raise SaltCloudSystemExit(
2658             "The enter_maintenance_mode function must be called with -f or --function."
2659         )
2660     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
2661     host_ref = salt.utils.vmware.get_mor_by_property(
2662         _get_si(), vim.HostSystem, host_name
2663     )
2664     if not host_name or not host_ref:
2665         raise SaltCloudSystemExit("You must specify a valid name of the host system.")
2666     if host_ref.runtime.inMaintenanceMode:
2667         return {host_name: "already in maintenance mode"}
2668     try:
2669         task = host_ref.EnterMaintenanceMode(timeout=0, evacuatePoweredOffVms=True)
2670         salt.utils.vmware.wait_for_task(task, host_name, "enter maintenance mode")
2671     except Exception as exc:  # pylint: disable=broad-except
2672         log.error(
2673             "Error while moving host system %s in maintenance mode: %s",
2674             host_name,
2675             exc,
2676             exc_info_on_loglevel=logging.DEBUG,
2677         )
2678         return {host_name: "failed to enter maintenance mode"}
2679     return {host_name: "entered maintenance mode"}
2680 def exit_maintenance_mode(kwargs=None, call=None):
2681     if call != "function":
2682         raise SaltCloudSystemExit(
2683             "The exit_maintenance_mode function must be called with -f or --function."
2684         )
2685     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
2686     host_ref = salt.utils.vmware.get_mor_by_property(
2687         _get_si(), vim.HostSystem, host_name
2688     )
2689     if not host_name or not host_ref:
2690         raise SaltCloudSystemExit("You must specify a valid name of the host system.")
2691     if not host_ref.runtime.inMaintenanceMode:
2692         return {host_name: "already not in maintenance mode"}
2693     try:
2694         task = host_ref.ExitMaintenanceMode(timeout=0)
2695         salt.utils.vmware.wait_for_task(task, host_name, "exit maintenance mode")
2696     except Exception as exc:  # pylint: disable=broad-except
2697         log.error(
2698             "Error while moving host system %s out of maintenance mode: %s",
2699             host_name,
2700             exc,
2701             exc_info_on_loglevel=logging.DEBUG,
2702         )
2703         return {host_name: "failed to exit maintenance mode"}
2704     return {host_name: "exited maintenance mode"}
2705 def create_folder(kwargs=None, call=None):
2706     if call != "function":
2707         raise SaltCloudSystemExit(
2708             "The create_folder function must be called with -f or --function."
2709         )
2710     si = _get_si()
2711     folder_path = kwargs.get("path") if kwargs and "path" in kwargs else None
2712     if not folder_path:
2713         raise SaltCloudSystemExit("You must specify a non empty folder path.")
2714     folder_refs = []
2715     inventory_path = "/"
2716     path_exists = True
2717     for index, folder_name in enumerate(
2718         os.path.normpath(folder_path.strip("/")).split("/")
2719     ):
2720         inventory_path = os.path.join(inventory_path, folder_name)
2721         folder_ref = si.content.searchIndex.FindByInventoryPath(
2722             inventoryPath=inventory_path
2723         )
2724         if isinstance(folder_ref, vim.Folder):
2725             log.debug("Path %s/ exists in the inventory", inventory_path)
2726             folder_refs.append(folder_ref)
2727         elif isinstance(folder_ref, vim.Datacenter):
2728             log.debug("Path %s/ exists in the inventory", inventory_path)
2729             folder_refs.append(folder_ref)
2730         else:
2731 <a name="1"></a>            path_exists = False
2732             if not folder_refs:
2733                 log<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.debug(
2734                     "Creating folder %s under rootFolder in the inventory", folder_name
2735                 )
2736                 folder_refs.append(si.content.rootFolder.CreateFolder(folder_name))
2737             else:
2738                 log.debug("Creating path %s/ in the inventory", inventory_path)
2739                 folder_refs.append(folder_refs[</b></font>index - 1].CreateFolder(folder_name))
2740     if path_exists:
2741         return {inventory_path: "specified path already exists"}
2742     return {inventory_path: "created the specified path"}
2743 def create_snapshot(name, kwargs=None, call=None):
2744     if call != "action":
2745         raise SaltCloudSystemExit(
2746             "The create_snapshot action must be called with -a or --action."
2747         )
2748     if kwargs is None:
2749         kwargs = {}
2750     snapshot_name = (
2751         kwargs.get("snapshot_name") if kwargs and "snapshot_name" in kwargs else None
2752     )
2753     if not snapshot_name:
2754         raise SaltCloudSystemExit(
2755             "You must specify snapshot name for the snapshot to be created."
2756         )
2757     memdump = _str_to_bool(kwargs.get("memdump", True))
2758     quiesce = _str_to_bool(kwargs.get("quiesce", False))
2759     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2760     if vm_ref.summary.runtime.powerState != "poweredOn":
2761         log.debug(
2762             "VM %s is not powered on. Setting both memdump and quiesce to False", name
2763         )
2764         memdump = False
2765         quiesce = False
2766     if memdump and quiesce:
2767         log.warning(
2768             "You can only set either memdump or quiesce to True. Setting quiesce=False"
2769         )
2770         quiesce = False
2771     desc = kwargs.get("description") if "description" in kwargs else ""
2772     try:
2773         task = vm_ref.CreateSnapshot(snapshot_name, desc, memdump, quiesce)
2774         salt.utils.vmware.wait_for_task(task, name, "create snapshot", 5, "info")
2775     except Exception as exc:  # pylint: disable=broad-except
2776         log.error(
2777             "Error while creating snapshot of %s: %s",
2778             name,
2779             exc,
2780             exc_info_on_loglevel=logging.DEBUG,
2781         )
2782         return "failed to create snapshot"
2783     return {
2784         "Snapshot created successfully": _get_snapshots(
2785             vm_ref.snapshot.rootSnapshotList, vm_ref.snapshot.currentSnapshot
2786         )
2787     }
2788 def revert_to_snapshot(name, kwargs=None, call=None):
2789     if call != "action":
2790         raise SaltCloudSystemExit(
2791             "The revert_to_snapshot action must be called with -a or --action."
2792         )
2793     if kwargs is None:
2794         kwargs = {}
2795     snapshot_name = (
2796         kwargs.get("snapshot_name") if kwargs and "snapshot_name" in kwargs else None
2797     )
2798     suppress_power_on = _str_to_bool(kwargs.get("power_off", False))
2799     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2800     if not vm_ref.rootSnapshot:
2801         log.error("VM %s does not contain any current snapshots", name)
2802         return "revert failed"
2803     msg = "reverted to current snapshot"
2804     try:
2805         if snapshot_name is None:
2806             log.debug("Reverting VM %s to current snapshot", name)
2807             task = vm_ref.RevertToCurrentSnapshot(suppressPowerOn=suppress_power_on)
2808         else:
2809             log.debug("Reverting VM %s to snapshot %s", name, snapshot_name)
2810             msg = "reverted to snapshot {}".format(snapshot_name)
2811             snapshot_ref = _get_snapshot_ref_by_name(vm_ref, snapshot_name)
2812             if snapshot_ref is None:
2813                 return "specified snapshot '{}' does not exist".format(snapshot_name)
2814             task = snapshot_ref.snapshot.Revert(suppressPowerOn=suppress_power_on)
2815         salt.utils.vmware.wait_for_task(task, name, "revert to snapshot", 5, "info")
2816     except Exception as exc:  # pylint: disable=broad-except
2817         log.error(
2818             "Error while reverting VM %s to snapshot: %s",
2819             name,
2820             exc,
2821             exc_info_on_loglevel=logging.DEBUG,
2822         )
2823         return "revert failed"
2824     return msg
2825 def remove_snapshot(name, kwargs=None, call=None):
2826     if call != "action":
2827         raise SaltCloudSystemExit(
2828             "The create_snapshot action must be called with -a or --action."
2829         )
2830     if kwargs is None:
2831         kwargs = {}
2832     snapshot_name = (
2833         kwargs.get("snapshot_name") if kwargs and "snapshot_name" in kwargs else None
2834     )
2835     remove_children = _str_to_bool(kwargs.get("remove_children", False))
2836     if not snapshot_name:
2837         raise SaltCloudSystemExit(
2838             "You must specify snapshot name for the snapshot to be deleted."
2839         )
2840     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2841     if not _get_snapshot_ref_by_name(vm_ref, snapshot_name):
2842         raise SaltCloudSystemExit(
2843             "ould not find the snapshot with the specified name."
2844         )
2845     try:
2846         snap_obj = _get_snapshot_ref_by_name(vm_ref, snapshot_name).snapshot
2847         task = snap_obj.RemoveSnapshot_Task(remove_children)
2848         salt.utils.vmware.wait_for_task(task, name, "remove snapshot", 5, "info")
2849     except Exception as exc:  # pylint: disable=broad-except
2850         log.error(
2851             "Error while removing snapshot of %s: %s",
2852             name,
2853             exc,
2854             exc_info_on_loglevel=logging.DEBUG,
2855         )
2856         return "failed to remove snapshot"
2857     if vm_ref.snapshot:
2858         return {
2859             "Snapshot removed successfully": _get_snapshots(
2860                 vm_ref.snapshot.rootSnapshotList, vm_ref.snapshot.currentSnapshot
2861             )
2862         }
2863     return "Snapshots removed successfully"
2864 def remove_all_snapshots(name, kwargs=None, call=None):
2865     if call != "action":
2866         raise SaltCloudSystemExit(
2867             "The remove_all_snapshots action must be called with -a or --action."
2868         )
2869     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2870     try:
2871         task = vm_ref.RemoveAllSnapshots()
2872         salt.utils.vmware.wait_for_task(task, name, "remove snapshots", 5, "info")
2873     except Exception as exc:  # pylint: disable=broad-except
2874         log.error(
2875             "Error while removing snapshots on VM %s: %s",
2876             name,
2877             exc,
2878             exc_info_on_loglevel=logging.DEBUG,
2879         )
2880         return "Failed to remove snapshots"
2881     return "Removed all snapshots"
2882 def convert_to_template(name, kwargs=None, call=None):
2883     if call != "action":
2884         raise SaltCloudSystemExit(
2885             "The convert_to_template action must be called with -a or --action."
2886         )
2887     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2888     if vm_ref.config.template:
2889         raise SaltCloudSystemExit("{} already a template".format(name))
2890     try:
2891         vm_ref.MarkAsTemplate()
2892     except Exception as exc:  # pylint: disable=broad-except
2893         log.error(
2894             "Error while converting VM to template %s: %s",
2895             name,
2896             exc,
2897             exc_info_on_loglevel=logging.DEBUG,
2898         )
2899         return "failed to convert to teamplate"
2900     return "{} converted to template".format(name)
2901 def add_host(kwargs=None, call=None):
2902     if call != "function":
2903         raise SaltCloudSystemExit(
2904             "The add_host function must be called with -f or --function."
2905         )
2906     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
2907     cluster_name = kwargs.get("cluster") if kwargs and "cluster" in kwargs else None
2908     datacenter_name = (
2909         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
2910     )
2911     host_user = config.get_cloud_config_value(
2912         "esxi_host_user", get_configured_provider(), __opts__, search_global=False
2913     )
2914     host_password = config.get_cloud_config_value(
2915         "esxi_host_password", get_configured_provider(), __opts__, search_global=False
2916     )
2917     host_ssl_thumbprint = config.get_cloud_config_value(
2918         "esxi_host_ssl_thumbprint",
2919         get_configured_provider(),
2920         __opts__,
2921         search_global=False,
2922     )
2923     if not host_user:
2924         raise SaltCloudSystemExit(
2925             "You must specify the ESXi host username in your providers config."
2926         )
2927     if not host_password:
2928         raise SaltCloudSystemExit(
2929             "You must specify the ESXi host password in your providers config."
2930         )
2931     if not host_name:
2932         raise SaltCloudSystemExit(
2933             "You must specify either the IP or DNS name of the host system."
2934         )
2935     if (cluster_name and datacenter_name) or not (cluster_name or datacenter_name):
2936         raise SaltCloudSystemExit(
2937             "You must specify either the cluster name or the datacenter name."
2938         )
2939     si = _get_si()
2940     if cluster_name:
2941         cluster_ref = salt.utils.vmware.get_mor_by_property(
2942             si, vim.ClusterComputeResource, cluster_name
2943         )
2944         if not cluster_ref:
2945             raise SaltCloudSystemExit("Specified cluster does not exist.")
2946     if datacenter_name:
2947         datacenter_ref = salt.utils.vmware.get_mor_by_property(
2948             si, vim.Datacenter, datacenter_name
2949         )
2950         if not datacenter_ref:
2951             raise SaltCloudSystemExit("Specified datacenter does not exist.")
2952     spec = vim.host.ConnectSpec(
2953         hostName=host_name,
2954         userName=host_user,
2955         password=host_password,
2956     )
2957     if host_ssl_thumbprint:
2958         spec.sslThumbprint = host_ssl_thumbprint
2959     else:
2960         log.warning("SSL thumbprint has not been specified in provider configuration")
2961         try:
2962             log.debug("Trying to get the SSL thumbprint directly from the host system")
2963             p1 = subprocess.Popen(
2964                 ("echo", "-n"), stdout=subprocess.PIPE, stderr=subprocess.PIPE
2965             )
2966             p2 = subprocess.Popen(
2967                 ("openssl", "s_client", "-connect", "{}:443".format(host_name)),
2968                 stdin=p1.stdout,
2969                 stdout=subprocess.PIPE,
2970                 stderr=subprocess.PIPE,
2971             )
2972             p3 = subprocess.Popen(
2973                 ("openssl", "x509", "-noout", "-fingerprint", "-sha1"),
2974                 stdin=p2.stdout,
2975                 stdout=subprocess.PIPE,
2976                 stderr=subprocess.PIPE,
2977             )
2978             out = salt.utils.stringutils.to_str(p3.stdout.read())
2979             ssl_thumbprint = out.split("=")[-1].strip()
2980             log.debug(
2981                 "SSL thumbprint received from the host system: %s", ssl_thumbprint
2982             )
2983             spec.sslThumbprint = ssl_thumbprint
2984         except Exception as exc:  # pylint: disable=broad-except
2985             log.error(
2986                 "Error while trying to get SSL thumbprint of host %s: %s",
2987                 host_name,
2988                 exc,
2989                 exc_info_on_loglevel=logging.DEBUG,
2990             )
2991             return {host_name: "failed to add host"}
2992     try:
2993         if cluster_name:
2994             task = cluster_ref.AddHost(spec=spec, asConnected=True)
2995             ret = "added host system to cluster {}".format(cluster_name)
2996         if datacenter_name:
2997             task = datacenter_ref.hostFolder.AddStandaloneHost(
2998                 spec=spec, addConnected=True
2999             )
3000             ret = "added host system to datacenter {}".format(datacenter_name)
3001         salt.utils.vmware.wait_for_task(task, host_name, "add host system", 5, "info")
3002     except Exception as exc:  # pylint: disable=broad-except
3003         if isinstance(exc, vim.fault.SSLVerifyFault):
3004             log.error("Authenticity of the host's SSL certificate is not verified")
3005             log.info(
3006                 "Try again after setting the esxi_host_ssl_thumbprint "
3007                 "to %s in provider configuration",
3008                 spec.sslThumbprint,
3009             )
3010         log.error(
3011             "Error while adding host %s: %s",
3012             host_name,
3013             exc,
3014             exc_info_on_loglevel=logging.DEBUG,
3015         )
3016         return {host_name: "failed to add host"}
3017     return {host_name: ret}
3018 def remove_host(kwargs=None, call=None):
3019     if call != "function":
3020         raise SaltCloudSystemExit(
3021             "The remove_host function must be called with -f or --function."
3022         )
3023     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3024     if not host_name:
3025         raise SaltCloudSystemExit("You must specify name of the host system.")
3026     si = _get_si()
3027     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3028     if not host_ref:
3029         raise SaltCloudSystemExit("Specified host system does not exist.")
3030     try:
3031         if isinstance(host_ref.parent, vim.ClusterComputeResource):
3032             task = host_ref.Destroy_Task()
3033         else:
3034             task = host_ref.parent.Destroy_Task()
3035         salt.utils.vmware.wait_for_task(
3036             task, host_name, "remove host", log_level="info"
3037         )
3038     except Exception as exc:  # pylint: disable=broad-except
3039         log.error(
3040             "Error while removing host %s: %s",
3041             host_name,
3042             exc,
3043             exc_info_on_loglevel=logging.DEBUG,
3044         )
3045         return {host_name: "failed to remove host"}
3046     return {host_name: "removed host from vcenter"}
3047 def connect_host(kwargs=None, call=None):
3048     if call != "function":
3049         raise SaltCloudSystemExit(
3050             "The connect_host function must be called with -f or --function."
3051         )
3052     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3053     if not host_name:
3054         raise SaltCloudSystemExit("You must specify name of the host system.")
3055     si = _get_si()
3056     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3057     if not host_ref:
3058         raise SaltCloudSystemExit("Specified host system does not exist.")
3059     if host_ref.runtime.connectionState == "connected":
3060         return {host_name: "host system already connected"}
3061     try:
3062         task = host_ref.ReconnectHost_Task()
3063         salt.utils.vmware.wait_for_task(task, host_name, "connect host", 5, "info")
3064     except Exception as exc:  # pylint: disable=broad-except
3065         log.error(
3066             "Error while connecting host %s: %s",
3067             host_name,
3068             exc,
3069             exc_info_on_loglevel=logging.DEBUG,
3070         )
3071         return {host_name: "failed to connect host"}
3072     return {host_name: "connected host"}
3073 def disconnect_host(kwargs=None, call=None):
3074     if call != "function":
3075         raise SaltCloudSystemExit(
3076             "The disconnect_host function must be called with -f or --function."
3077         )
3078     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3079     if not host_name:
3080         raise SaltCloudSystemExit("You must specify name of the host system.")
3081     si = _get_si()
3082     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3083     if not host_ref:
3084         raise SaltCloudSystemExit("Specified host system does not exist.")
3085     if host_ref.runtime.connectionState == "disconnected":
3086         return {host_name: "host system already disconnected"}
3087     try:
3088         task = host_ref.DisconnectHost_Task()
3089         salt.utils.vmware.wait_for_task(
3090             task, host_name, "disconnect host", log_level="info"
3091         )
3092     except Exception as exc:  # pylint: disable=broad-except
3093         log.error(
3094             "Error while disconnecting host %s: %s",
3095             host_name,
3096             exc,
3097             exc_info_on_loglevel=logging.DEBUG,
3098         )
3099         return {host_name: "failed to disconnect host"}
3100     return {host_name: "disconnected host"}
3101 def reboot_host(kwargs=None, call=None):
3102     if call != "function":
3103         raise SaltCloudSystemExit(
3104             "The reboot_host function must be called with -f or --function."
3105         )
3106     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3107     force = _str_to_bool(kwargs.get("force")) if kwargs and "force" in kwargs else False
3108     if not host_name:
3109         raise SaltCloudSystemExit("You must specify name of the host system.")
3110     si = _get_si()
3111     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3112     if not host_ref:
3113         raise SaltCloudSystemExit("Specified host system does not exist.")
3114     if host_ref.runtime.connectionState == "notResponding":
3115         raise SaltCloudSystemExit(
3116             "Specified host system cannot be rebooted in it's current state (not"
3117             " responding)."
3118         )
3119     if not host_ref.capability.rebootSupported:
3120         raise SaltCloudSystemExit("Specified host system does not support reboot.")
3121     if not host_ref.runtime.inMaintenanceMode and not force:
3122         raise SaltCloudSystemExit(
3123             "Specified host system is not in maintenance mode. Specify force=True to"
3124             " force reboot even if there are virtual machines running or other"
3125             " operations in progress."
3126         )
3127     try:
3128         host_ref.RebootHost_Task(force)
3129         _wait_for_host(host_ref, "reboot", 10, "info")
3130     except Exception as exc:  # pylint: disable=broad-except
3131         log.error(
3132             "Error while rebooting host %s: %s",
3133             host_name,
3134             exc,
3135             exc_info_on_loglevel=logging.DEBUG,
3136         )
3137         return {host_name: "failed to reboot host"}
3138     return {host_name: "rebooted host"}
3139 def create_datastore_cluster(kwargs=None, call=None):
3140     if call != "function":
3141         raise SaltCloudSystemExit(
3142             "The create_datastore_cluster function must be called with "
3143             "-f or --function."
3144         )
3145     datastore_cluster_name = kwargs.get("name") if kwargs and "name" in kwargs else None
3146     datacenter_name = (
3147         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
3148     )
3149     if not datastore_cluster_name:
3150         raise SaltCloudSystemExit(
3151             "You must specify name of the new datastore cluster to be created."
3152         )
3153     if not datastore_cluster_name or len(datastore_cluster_name) &gt;= 80:
3154         raise SaltCloudSystemExit(
3155             "The datastore cluster name must be a non empty string of less than 80"
3156             " characters."
3157         )
3158     if not datacenter_name:
3159         raise SaltCloudSystemExit(
3160             "You must specify name of the datacenter where the datastore cluster should"
3161             " be created."
3162         )
3163     si = _get_si()
3164     datastore_cluster_ref = salt.utils.vmware.get_mor_by_property(
3165         si, vim.StoragePod, datastore_cluster_name
3166     )
3167     if datastore_cluster_ref:
3168         return {datastore_cluster_name: "datastore cluster already exists"}
3169     datacenter_ref = salt.utils.vmware.get_mor_by_property(
3170         si, vim.Datacenter, datacenter_name
3171     )
3172     if not datacenter_ref:
3173         raise SaltCloudSystemExit("The specified datacenter does not exist.")
3174     try:
3175         datacenter_ref.datastoreFolder.CreateStoragePod(name=datastore_cluster_name)
3176     except Exception as exc:  # pylint: disable=broad-except
3177         log.error(
3178             "Error creating datastore cluster %s: %s",
3179             datastore_cluster_name,
3180             exc,
3181             exc_info_on_loglevel=logging.DEBUG,
3182         )
3183         return False
3184     return {datastore_cluster_name: "created"}
3185 def shutdown_host(kwargs=None, call=None):
3186     if call != "function":
3187         raise SaltCloudSystemExit(
3188             "The shutdown_host function must be called with -f or --function."
3189         )
3190     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3191     force = _str_to_bool(kwargs.get("force")) if kwargs and "force" in kwargs else False
3192     if not host_name:
3193         raise SaltCloudSystemExit("You must specify name of the host system.")
3194     si = _get_si()
3195     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3196     if not host_ref:
3197         raise SaltCloudSystemExit("Specified host system does not exist.")
3198     if host_ref.runtime.connectionState == "notResponding":
3199         raise SaltCloudSystemExit(
3200             "Specified host system cannot be shut down in it's current state (not"
3201             " responding)."
3202         )
3203     if not host_ref.capability.rebootSupported:
3204         raise SaltCloudSystemExit("Specified host system does not support shutdown.")
3205     if not host_ref.runtime.inMaintenanceMode and not force:
3206         raise SaltCloudSystemExit(
3207             "Specified host system is not in maintenance mode. Specify force=True to"
3208             " force reboot even if there are virtual machines running or other"
3209             " operations in progress."
3210         )
3211     try:
3212         host_ref.ShutdownHost_Task(force)
3213     except Exception as exc:  # pylint: disable=broad-except
3214         log.error(
3215             "Error while shutting down host %s: %s",
3216             host_name,
3217             exc,
3218             exc_info_on_loglevel=logging.DEBUG,
3219         )
3220         return {host_name: "failed to shut down host"}
3221     return {host_name: "shut down host"}
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
