<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for rpmbuild_pkgbuild.py &amp; vmware.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for rpmbuild_pkgbuild.py &amp; vmware.py
      </h3>
<h1 align="center">
        0.8%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>rpmbuild_pkgbuild.py (3.276699%)<th>vmware.py (0.45592704%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(13-27)<td><a href="#" name="0">(116-130)</a><td align="center"><font color="#ff0000">14</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(84-87)<td><a href="#" name="1">(4101-4108)</a><td align="center"><font color="#ec0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>rpmbuild_pkgbuild.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 """
2 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import errno
3 import functools
4 import logging
5 import os
6 import re
7 import shutil
8 import tempfile
9 import time
10 import traceback
11 import urllib.parse
12 import salt.utils.files
13 import salt.utils.path
14 import salt.utils.user
15 import</b></font> salt.utils.vt
16 from salt.exceptions import CommandExecutionError, SaltInvocationError
17 HAS_LIBS = False
18 try:
19     import gnupg  # pylint: disable=unused-import
20     import salt.modules.gpg
21     HAS_LIBS = True
22 except ImportError:
23     pass
24 log = logging.getLogger(__name__)
25 __virtualname__ = "pkgbuild"
26 def __virtual__():
27     """
28     Confirm this module is on a RPM based system, and has required utilities
29     """
30     missing_util = False
31     utils_reqd = ["gpg", "rpm", "rpmbuild", "mock", "createrepo"]
32     for named_util in utils_reqd:
33         if not salt.utils.path.which(named_util):
34             missing_util = True
35             break
36     if HAS_LIBS and not missing_util:
37         if __grains__.get("os_family", False) in ("RedHat", "Suse"):
38             return __virtualname__
39         else:
40             return "rpmbuild"
41     else:
42         return (
43             False,
44             "The rpmbuild module could not be loaded: requires python-gnupg, "
45             "gpg, rpm, rpmbuild, mock and createrepo utilities to be installed",
46         )
47 def _create_rpmmacros(runas="root"):
48     """
49     Create the .rpmmacros file in user's home directory
50     """
51     home = os.path.expanduser("~" + runas)
52     rpmbuilddir = os.path.join(home, "rpmbuild")
53     if not os.path.isdir(rpmbuilddir):
54         __salt__["file.makedirs_perms"](name=rpmbuilddir, user=runas, group="mock")
55     mockdir = os.path.join(home, "mock")
56     if not os.path.isdir(mockdir):
57     rpmmacros = os.path.join(home, ".rpmmacros")
58     with salt.utils.files<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.fopen(rpmmacros, "w") as afile:
59         afile.write(salt.utils.stringutils.to_str("%_topdir {}\n".format(rpmbuilddir)))
60         afile.write("%signature gpg\n")
61         afile.</b></font>write("%_source_filedigest_algorithm 8\n")
62         afile.write("%_binary_filedigest_algorithm 8\n")
63         afile.write("%_gpg_name packaging@saltstack.com\n")
64 def _mk_tree(runas="root"):
65     """
66     Create the rpm build tree
67     """
68     basedir = tempfile.mkdtemp()
69     paths = ["BUILD", "RPMS", "SOURCES", "SPECS", "SRPMS"]
70     for path in paths:
71         full = os.path.join(basedir, path)
72         __salt__["file.makedirs_perms"](name=full, user=runas, group="mock")
73     return basedir
74 def _get_spec(tree_base, spec, template, saltenv="base"):
75     """
76     Get the spec file and place it in the SPECS dir
77     """
78     spec_tgt = os.path.basename(spec)
79     dest = os.path.join(tree_base, "SPECS", spec_tgt)
80     return __salt__["cp.get_url"](spec, dest, saltenv=saltenv)
81 def _get_src(tree_base, source, saltenv="base", runas="root"):
82     """
83     Get the named sources and place them into the tree_base
84     """
85     parsed = urllib.parse.urlparse(source)
86     sbase = os.path.basename(source)
87     dest = os.path.join(tree_base, "SOURCES", sbase)
88     if parsed.scheme:
89         lsrc = __salt__["cp.get_url"](source, dest, saltenv=saltenv)
90     else:
91         shutil.copy(source, dest)
92     __salt__["file.chown"](path=dest, user=runas, group="mock")
93 def _get_distset(tgt):
94     """
95     Get the distribution string for use with rpmbuild and mock
96     """
97     tgtattrs = tgt.split("-")
98     if tgtattrs[0] == "amzn2":
99         distset = '--define "dist .{}"'.format(tgtattrs[0])
100     elif tgtattrs[1] in ["6", "7", "8"]:
101         distset = '--define "dist .el{}"'.format(tgtattrs[1])
102     else:
103         distset = ""
104     return distset
105 def _get_deps(deps, tree_base, saltenv="base"):
106     """
107     Get include string for list of dependent rpms to build package
108     """
109     deps_list = ""
110     if deps is None:
111         return deps_list
112     if not isinstance(deps, list):
113         raise SaltInvocationError(
114             "'deps' must be a Python list or comma-separated string"
115         )
116     for deprpm in deps:
117         parsed = urllib.parse._urlparse(deprpm)
118         depbase = os.path.basename(deprpm)
119         dest = os.path.join(tree_base, depbase)
120         if parsed.scheme:
121             __salt__["cp.get_url"](deprpm, dest, saltenv=saltenv)
122         else:
123             shutil.copy(deprpm, dest)
124         deps_list += " {}".format(dest)
125     return deps_list
126 def _check_repo_gpg_phrase_utils():
127     """
128     Check for /usr/libexec/gpg-preset-passphrase is installed
129     """
130     util_name = "/usr/libexec/gpg-preset-passphrase"
131     if __salt__["file.file_exists"](util_name):
132         return True
133     else:
134         raise CommandExecutionError(
135             "utility '{}' needs to be installed".format(util_name)
136         )
137 def _get_gpg_key_resources(keyid, env, use_passphrase, gnupghome, runas):
138     """
139     Obtain gpg key resource infomation to sign repo files with
140     keyid
141         Optional Key ID to use in signing packages and repository.
142         Utilizes Public and Private keys associated with keyid which have
143         been loaded into the minion's Pillar data.
144     env
145         A dictionary of environment variables to be utilized in creating the
146         repository.
147     use_passphrase : False
148         Use a passphrase with the signing key presented in ``keyid``.
149         Passphrase is received from Pillar data which could be passed on the
150         command line with ``pillar`` parameter.
151     gnupghome : /etc/salt/gpgkeys
152         Location where GPG related files are stored, used with ``keyid``.
153     runas : root
154         User to create the repository as, and optionally sign packages.
155         .. note::
156             Ensure the user has correct permissions to any files and
157             directories which are to be utilized.
158     Returns:
159         tuple
160             use_gpg_agent       True | False, Redhat 8 now makes use of a gpg-agent similar ot Debian
161             local_keyid         key id to use in signing
162             define_gpg_name     string containing definition to use with addsign (use_gpg_agent False)
163             phrase              pass phrase (may not be used)
164     """
165     local_keygrip_to_use = None
166     local_key_fingerprint = None
167     local_keyid = None
168     local_uids = None
169     define_gpg_name = ""
170     phrase = ""
171     retrc = 0
172     use_gpg_agent = False
173     if (
174         __grains__.get("os_family") == "RedHat"
175         and __grains__.get("osmajorrelease") &gt;= 8
176     ):
177         use_gpg_agent = True
178     if keyid is not None:
179         pkg_pub_key_file = "{}/{}".format(
180             gnupghome, __salt__["pillar.get"]("gpg_pkg_pub_keyname", None)
181         )
182         pkg_priv_key_file = "{}/{}".format(
183             gnupghome, __salt__["pillar.get"]("gpg_pkg_priv_keyname", None)
184         )
185         if pkg_pub_key_file is None or pkg_priv_key_file is None:
186             raise SaltInvocationError(
187                 "Pillar data should contain Public and Private keys associated with"
188                 " 'keyid'"
189             )
190         try:
191             __salt__["gpg.import_key"](
192                 user=runas, filename=pkg_pub_key_file, gnupghome=gnupghome
193             )
194             __salt__["gpg.import_key"](
195                 user=runas, filename=pkg_priv_key_file, gnupghome=gnupghome
196             )
197         except SaltInvocationError:
198             raise SaltInvocationError(
199                 "Public and Private key files associated with Pillar data and 'keyid' "
200                 "{} could not be found".format(keyid)
201             )
202         local_keys = __salt__["gpg.list_keys"](user=runas, gnupghome=gnupghome)
203         for gpg_key in local_keys:
204             if keyid == gpg_key["keyid"][8:]:
205                 local_uids = gpg_key["uids"]
206                 local_keyid = gpg_key["keyid"]
207                 if use_gpg_agent:
208                     local_keygrip_to_use = gpg_key["fingerprint"]
209                     local_key_fingerprint = gpg_key["fingerprint"]
210                 break
211         if use_gpg_agent:
212             cmd = "gpg --with-keygrip --list-secret-keys"
213             local_keys2_keygrip = __salt__["cmd.run"](cmd, runas=runas, env=env)
214             local_keys2 = iter(local_keys2_keygrip.splitlines())
215             try:
216                 for line in local_keys2:
217                     if line.startswith("sec"):
218                         line_fingerprint = next(local_keys2).lstrip().rstrip()
219                         if local_key_fingerprint == line_fingerprint:
220                             lkeygrip = next(local_keys2).split("=")
221                             local_keygrip_to_use = lkeygrip[1].lstrip().rstrip()
222                             break
223             except StopIteration:
224                 raise SaltInvocationError(
225                     "unable to find keygrip associated with fingerprint '{}' for keyid"
226                     " '{}'".format(local_key_fingerprint, local_keyid)
227                 )
228         if local_keyid is None:
229             raise SaltInvocationError(
230                 "The key ID '{}' was not found in GnuPG keyring at '{}'".format(
231                     keyid, gnupghome
232                 )
233             )
234         if use_passphrase:
235             phrase = __salt__["pillar.get"]("gpg_passphrase")
236             if use_gpg_agent:
237                 _check_repo_gpg_phrase_utils()
238                 cmd = (
239                     "/usr/libexec/gpg-preset-passphrase --verbose --preset "
240                     '--passphrase "{}" {}'.format(phrase, local_keygrip_to_use)
241                 )
242                 retrc = __salt__["cmd.retcode"](cmd, runas=runas, env=env)
243                 if retrc != 0:
244                     raise SaltInvocationError(
245                         "Failed to preset passphrase, error {1}, "
246                         "check logs for further details".format(retrc)
247                     )
248         if local_uids:
249             define_gpg_name = (
250                 "--define='%_signature gpg' --define='%_gpg_name {}'".format(
251                     local_uids[0]
252                 )
253             )
254         cmd = "rpm --import {}".format(pkg_pub_key_file)
255         retrc = __salt__["cmd.retcode"](cmd, runas=runas, use_vt=True)
256         if retrc != 0:
257             raise SaltInvocationError(
258                 "Failed to import public key from file {} with return "
259                 "error {}, check logs for further details".format(
260                     pkg_pub_key_file, retrc
261                 )
262             )
263     return (use_gpg_agent, local_keyid, define_gpg_name, phrase)
264 def _sign_file(runas, define_gpg_name, phrase, abs_file, timeout):
265     """
266     Sign file with provided key and definition
267     """
268     SIGN_PROMPT_RE = re.compile(r"Enter pass phrase: ", re.M)
269     interval = 0.5
270     number_retries = timeout / interval
271     times_looped = 0
272     error_msg = "Failed to sign file {}".format(abs_file)
273     cmd = "rpm {} --addsign {}".format(define_gpg_name, abs_file)
274     preexec_fn = functools.partial(salt.utils.user.chugid_and_umask, runas, None)
275     try:
276         stdout, stderr = None, None
277         proc = salt.utils.vt.Terminal(
278             cmd,
279             shell=True,
280             preexec_fn=preexec_fn,
281             stream_stdout=True,
282             stream_stderr=True,
283         )
284         while proc.has_unread_data:
285             stdout, stderr = proc.recv()
286             if stdout and SIGN_PROMPT_RE.search(stdout):
287                 proc.sendline(phrase)
288             else:
289                 times_looped += 1
290             if times_looped &gt; number_retries:
291                 raise SaltInvocationError(
292                     "Attemping to sign file {} failed, timed out after {} seconds".format(
293                         abs_file, int(times_looped * interval)
294                     )
295                 )
296             time.sleep(interval)
297         proc_exitstatus = proc.exitstatus
298         if proc_exitstatus != 0:
299             raise SaltInvocationError(
300                 "Signing file {} failed with proc.status {}".format(
301                     abs_file, proc_exitstatus
302                 )
303             )
304     except salt.utils.vt.TerminalException as err:
305         trace = traceback.format_exc()
306         log.error(error_msg, err, trace)
307     finally:
308         proc.close(terminate=True, kill=True)
309 def _sign_files_with_gpg_agent(runas, local_keyid, abs_file, repodir, env, timeout):
310     """
311     Sign file with provided key utilizing gpg-agent
312     """
313     cmd = "rpmsign --verbose  --key-id={} --addsign {}".format(local_keyid, abs_file)
314     retrc = __salt__["cmd.retcode"](cmd, runas=runas, cwd=repodir, use_vt=True, env=env)
315     if retrc != 0:
316         raise SaltInvocationError(
317             "Signing encountered errors for command '{}', "
318             "return error {}, check logs for further details".format(cmd, retrc)
319         )
320 def make_src_pkg(
321     dest_dir, spec, sources, env=None, template=None, saltenv="base", runas="root"
322 ):
323     """
324     Create a source rpm from the given spec file and sources
325     CLI Example:
326     .. code-block:: bash
327         salt '*' pkgbuild.make_src_pkg /var/www/html/
328                 https://raw.githubusercontent.com/saltstack/libnacl/master/pkg/rpm/python-libnacl.spec
329                 https://pypi.python.org/packages/source/l/libnacl/libnacl-1.3.5.tar.gz
330     This example command should build the libnacl SOURCE package and place it in
331     /var/www/html/ on the minion
332     .. versionchanged:: 2017.7.0
333     dest_dir
334         The directory on the minion to place the built package(s)
335     spec
336         The location of the spec file (used for rpms)
337     sources
338         The list of package sources
339     env
340         A dictionary of environment variables to be set prior to execution.
341     template
342         Run the spec file through a templating engine
343         Optional argument, allows for no templating engine used to be
344         if none is desired.
345     saltenv
346         The saltenv to use for files downloaded from the salt filesever
347     runas
348         The user to run the build process as
349         .. versionadded:: 2018.3.3
350     .. note::
351         using SHA256 as digest and minimum level dist el6
352     """
353     _create_rpmmacros(runas)
354     tree_base = _mk_tree(runas)
355     spec_path = _get_spec(tree_base, spec, template, saltenv)
356     __salt__["file.chown"](path=spec_path, user=runas, group="mock")
357     __salt__["file.chown"](path=tree_base, user=runas, group="mock")
358     if isinstance(sources, str):
359         sources = sources.split(",")
360     for src in sources:
361         _get_src(tree_base, src, saltenv, runas)
362     cmd = 'rpmbuild --verbose --define "_topdir {}" -bs --define "dist .el6" {}'.format(
363         tree_base, spec_path
364     )
365     retrc = __salt__["cmd.retcode"](cmd, runas=runas)
366     if retrc != 0:
367         raise SaltInvocationError(
368             "Make source package for destination directory {}, spec {}, sources {},"
369             " failed with return error {}, check logs for further details".format(
370                 dest_dir, spec, sources, retrc
371             )
372         )
373     srpms = os.path.join(tree_base, "SRPMS")
374     ret = []
375     if not os.path.isdir(dest_dir):
376         __salt__["file.makedirs_perms"](name=dest_dir, user=runas, group="mock")
377     for fn_ in os.listdir(srpms):
378         full = os.path.join(srpms, fn_)
379         tgt = os.path.join(dest_dir, fn_)
380         shutil.copy(full, tgt)
381         ret.append(tgt)
382     return ret
383 def build(
384     runas,
385     tgt,
386     dest_dir,
387     spec,
388     sources,
389     deps,
390     env,
391     template,
392     saltenv="base",
393     log_dir="/var/log/salt/pkgbuild",
394 ):
395     """
396     Given the package destination directory, the spec file source and package
397     sources, use mock to safely build the rpm defined in the spec file
398     CLI Example:
399     .. code-block:: bash
400         salt '*' pkgbuild.build mock epel-7-x86_64 /var/www/html
401                     https://raw.githubusercontent.com/saltstack/libnacl/master/pkg/rpm/python-libnacl.spec
402                     https://pypi.python.org/packages/source/l/libnacl/libnacl-1.3.5.tar.gz
403     This example command should build the libnacl package for rhel 7 using user
404     mock and place it in /var/www/html/ on the minion
405     """
406     ret = {}
407     try:
408         __salt__["file.chown"](path=dest_dir, user=runas, group="mock")
409     except OSError as exc:
410         if exc.errno != errno.EEXIST:
411             raise
412     srpm_dir = os.path.join(dest_dir, "SRPMS")
413     srpm_build_dir = tempfile.mkdtemp()
414     try:
415         srpms = make_src_pkg(
416             srpm_build_dir, spec, sources, env, template, saltenv, runas
417         )
418     except Exception as exc:  # pylint: disable=broad-except
419         shutil.rmtree(srpm_build_dir)
420         log.error("Failed to make src package")
421         return ret
422     distset = _get_distset(tgt)
423     noclean = ""
424     deps_dir = tempfile.mkdtemp()
425     deps_list = _get_deps(deps, deps_dir, saltenv)
426     retrc = 0
427     for srpm in srpms:
428         dbase = os.path.dirname(srpm)
429         results_dir = tempfile.mkdtemp()
430         try:
431             __salt__["file.chown"](path=dbase, user=runas, group="mock")
432             __salt__["file.chown"](path=results_dir, user=runas, group="mock")
433             cmd = "mock --root={} --resultdir={} --init".format(tgt, results_dir)
434             retrc |= __salt__["cmd.retcode"](cmd, runas=runas)
435             if deps_list and not deps_list.isspace():
436                 cmd = "mock --root={} --resultdir={} --install {} {}".format(
437                     tgt, results_dir, deps_list, noclean
438                 )
439                 retrc |= __salt__["cmd.retcode"](cmd, runas=runas)
440                 noclean += " --no-clean"
441             cmd = "mock --root={} --resultdir={} {} {} {}".format(
442                 tgt, results_dir, distset, noclean, srpm
443             )
444             retrc |= __salt__["cmd.retcode"](cmd, runas=runas)
445             cmdlist = [
446                 "rpm",
447                 "-qp",
448                 "--queryformat",
449                 "{0}/%{{name}}/%{{version}}-%{{release}}".format(log_dir),
450                 srpm,
451             ]
452             log_dest = __salt__["cmd.run_stdout"](cmdlist, python_shell=False)
453             for filename in os.listdir(results_dir):
454                 full = os.path.join(results_dir, filename)
455                 if filename.endswith("src.rpm"):
456                     sdest = os.path.join(srpm_dir, filename)
457                     try:
458                         __salt__["file.makedirs_perms"](
459                             name=srpm_dir, user=runas, group="mock"
460                         )
461                     except OSError as exc:
462                         if exc.errno != errno.EEXIST:
463                             raise
464                     shutil.copy(full, sdest)
465                     ret.setdefault("Source Packages", []).append(sdest)
466                 elif filename.endswith(".rpm"):
467                     bdist = os.path.join(dest_dir, filename)
468                     shutil.copy(full, bdist)
469                     ret.setdefault("Packages", []).append(bdist)
470                 else:
471                     log_file = os.path.join(log_dest, filename)
472                     try:
473                         __salt__["file.makedirs_perms"](
474                             name=log_dest, user=runas, group="mock"
475                         )
476                     except OSError as exc:
477                         if exc.errno != errno.EEXIST:
478                             raise
479                     shutil.copy(full, log_file)
480                     ret.setdefault("Log Files", []).append(log_file)
481         except Exception as exc:  # pylint: disable=broad-except
482             log.error("Error building from %s: %s", srpm, exc)
483         finally:
484             shutil.rmtree(results_dir)
485     if retrc != 0:
486         raise SaltInvocationError(
487             "Building packages for destination directory {}, spec {}, sources {},"
488             " failed with return error {}, check logs for further details".format(
489                 dest_dir, spec, sources, retrc
490             )
491         )
492     shutil.rmtree(deps_dir)
493     shutil.rmtree(srpm_build_dir)
494     return ret
495 def make_repo(
496     repodir,
497     keyid=None,
498     env=None,
499     use_passphrase=False,
500     gnupghome="/etc/salt/gpgkeys",
501     runas="root",
502     timeout=15.0,
503 ):
504     """
505     Make a package repository and optionally sign packages present
506     Given the repodir, create a ``yum`` repository out of the rpms therein
507     and optionally sign it and packages present, the name is directory to
508     turn into a repo. This state is best used with onchanges linked to
509     your package building states.
510     repodir
511         The directory to find packages that will be in the repository.
512     keyid
513         .. versionchanged:: 2016.3.0
514         Optional Key ID to use in signing packages and repository.
515         Utilizes Public and Private keys associated with keyid which have
516         been loaded into the minion's Pillar data.
517         For example, contents from a Pillar data file with named Public
518         and Private keys as follows:
519         .. code-block:: yaml
520             gpg_pkg_priv_key: |
521               -----BEGIN PGP PRIVATE KEY BLOCK-----
522               Version: GnuPG v1
523               lQO+BFciIfQBCADAPCtzx7I5Rl32escCMZsPzaEKWe7bIX1em4KCKkBoX47IG54b
524               w82PCE8Y1jF/9Uk2m3RKVWp3YcLlc7Ap3gj6VO4ysvVz28UbnhPxsIkOlf2cq8qc
525               .
526               .
527               Ebe+8JCQTwqSXPRTzXmy/b5WXDeM79CkLWvuGpXFor76D+ECMRPv/rawukEcNptn
528               R5OmgHqvydEnO4pWbn8JzQO9YX/Us0SMHBVzLC8eIi5ZIopzalvX
529               =JvW8
530               -----END PGP PRIVATE KEY BLOCK-----
531             gpg_pkg_priv_keyname: gpg_pkg_key.pem
532             gpg_pkg_pub_key: |
533               -----BEGIN PGP PUBLIC KEY BLOCK-----
534               Version: GnuPG v1
535               mQENBFciIfQBCADAPCtzx7I5Rl32escCMZsPzaEKWe7bIX1em4KCKkBoX47IG54b
536               w82PCE8Y1jF/9Uk2m3RKVWp3YcLlc7Ap3gj6VO4ysvVz28UbnhPxsIkOlf2cq8qc
537               .
538               .
539               bYP7t5iwJmQzRMyFInYRt77wkJBPCpJc9FPNebL9vlZcN4zv0KQta+4alcWivvoP
540               4QIxE+/+trC6QRw2m2dHk6aAeq/J0Sc7ilZufwnNA71hf9SzRIwcFXMsLx4iLlki
541               inNqW9c=
542               =s1CX
543               -----END PGP PUBLIC KEY BLOCK-----
544             gpg_pkg_pub_keyname: gpg_pkg_key.pub
545     env
546         .. versionchanged:: 2016.3.0
547         A dictionary of environment variables to be utilized in creating the
548         repository.
549         .. note::
550             This parameter is not used for making ``yum`` repositories.
551     use_passphrase : False
552         .. versionadded:: 2016.3.0
553         Use a passphrase with the signing key presented in ``keyid``.
554         Passphrase is received from Pillar data which could be passed on the
555         command line with ``pillar`` parameter.
556         .. code-block:: bash
557             pillar='{ "gpg_passphrase" : "my_passphrase" }'
558         .. versionadded:: 3001.1
559         RHEL 8 and above leverages gpg-agent and gpg-preset-passphrase for
560         caching keys, etc.
561     gnupghome : /etc/salt/gpgkeys
562         .. versionadded:: 2016.3.0
563         Location where GPG related files are stored, used with ``keyid``.
564     runas : root
565         .. versionadded:: 2016.3.0
566         User to create the repository as, and optionally sign packages.
567         .. note::
568             Ensure the user has correct permissions to any files and
569             directories which are to be utilized.
570     timeout : 15.0
571         .. versionadded:: 2016.3.4
572         Timeout in seconds to wait for the prompt for inputting the passphrase.
573     CLI Example:
574     .. code-block:: bash
575         salt '*' pkgbuild.make_repo /var/www/html/
576     """
577     home = os.path.expanduser("~" + runas)
578     rpmmacros = os.path.join(home, ".rpmmacros")
579     if not os.path.exists(rpmmacros):
580         _create_rpmmacros(runas)
581     if gnupghome and env is None:
582         env = {}
583         env["GNUPGHOME"] = gnupghome
584     use_gpg_agent, local_keyid, define_gpg_name, phrase = _get_gpg_key_resources(
585         keyid, env, use_passphrase, gnupghome, runas
586     )
587     for fileused in os.listdir(repodir):
588         if fileused.endswith(".rpm"):
589             abs_file = os.path.join(repodir, fileused)
590             if use_gpg_agent:
591                 _sign_files_with_gpg_agent(
592                     runas, local_keyid, abs_file, repodir, env, timeout
593                 )
594             else:
595                 _sign_file(runas, define_gpg_name, phrase, abs_file, timeout)
596     cmd = "createrepo --update {}".format(repodir)
597     retrc = __salt__["cmd.run_all"](cmd, runas=runas)
598     return retrc
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>vmware.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 """
2 VMware Cloud Module
3 ===================
4 .. versionadded:: 2015.5.4
5 The VMware cloud module allows you to manage VMware ESX, ESXi, and vCenter.
6 See :ref:`Getting started with VMware &lt;cloud-getting-started-vmware&gt;` to get started.
7 :codeauthor: Nitin Madhok &lt;nmadhok@g.clemson.edu&gt;
8 Dependencies
9 ============
10 - pyVmomi Python Module
11 pyVmomi
12 -------
13 PyVmomi can be installed via pip:
14 .. code-block:: bash
15     pip install pyVmomi
16 .. note::
17     Version 6.0 of pyVmomi has some problems with SSL error handling on certain
18     versions of Python. If using version 6.0 of pyVmomi, Python 2.6,
19     Python 2.7.9, or newer must be present. This is due to an upstream dependency
20     in pyVmomi 6.0 that is not supported in Python versions 2.7 to 2.7.8. If the
21     version of Python is not in the supported range, you will need to install an
22     earlier version of pyVmomi. See `Issue #29537`_ for more information.
23 .. _Issue #29537: https://github.com/saltstack/salt/issues/29537
24 Based on the note above, to install an earlier version of pyVmomi than the
25 version currently listed in PyPi, run the following:
26 .. code-block:: bash
27     pip install pyVmomi==5.5.0.2014.1.1
28 The 5.5.0.2014.1.1 is a known stable version that this original VMware cloud
29 driver was developed against.
30 .. note::
31     Ensure python pyVmomi module is installed by running following one-liner
32     check. The output should be 0.
33     .. code-block:: bash
34        python -c "import pyVmomi" ; echo $?
35 Configuration
36 =============
37 To use this module, set up the vCenter or ESX/ESXi URL, username and password in the
38 cloud configuration at
39 ``/etc/salt/cloud.providers`` or ``/etc/salt/cloud.providers.d/vmware.conf``:
40 .. code-block:: yaml
41     my-vmware-config:
42       driver: vmware
43       user: 'DOMAIN\\user'
44       password: 'verybadpass'
45       url: '10.20.30.40'
46     vcenter01:
47       driver: vmware
48       user: 'DOMAIN\\user'
49       password: 'verybadpass'
50       url: 'vcenter01.domain.com'
51       protocol: 'https'
52       port: 443
53     vcenter02:
54       driver: vmware
55       user: 'DOMAIN\\user'
56       password: 'verybadpass'
57       url: 'vcenter02.domain.com'
58       protocol: 'http'
59       port: 80
60     esx01:
61       driver: vmware
62       user: 'admin'
63       password: 'verybadpass'
64       url: 'esx01.domain.com'
65 .. note::
66     Optionally, ``protocol`` and ``port`` can be specified if the vCenter
67     server is not using the defaults. Default is ``protocol: https`` and
68     ``port: 443``.
69 .. note::
70     .. versionchanged:: 2015.8.0
71     The ``provider`` parameter in cloud provider configuration was renamed to ``driver``.
72     This change was made to avoid confusion with the ``provider`` parameter that is
73     used in cloud profile configuration. Cloud provider configuration now uses ``driver``
74     to refer to the salt-cloud driver that provides the underlying functionality to
75     connect to a cloud provider, while cloud profile configuration continues to use
76     ``provider`` to refer to the cloud provider configuration that you define.
77 To test the connection for ``my-vmware-config`` specified in the cloud
78 """
79 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import logging
80 import os.path
81 import pprint
82 import re
83 import subprocess
84 import time
85 from random import randint
86 import salt.config as config
87 import salt.utils.cloud
88 import salt.utils.network
89 import salt.utils.stringutils
90 import salt.utils.vmware
91 import salt.utils.xmlutil
92 from</b></font> salt.exceptions import SaltCloudSystemExit
93 try:
94     from pyVmomi import vim  # pylint: disable=no-name-in-module
95     HAS_PYVMOMI = True
96 except ImportError:
97     HAS_PYVMOMI = False
98 try:
99     from requests.packages.urllib3 import (
100         disable_warnings,
101     )  # pylint: disable=no-name-in-module
102     disable_warnings()
103 except ImportError:
104     pass
105 ESX_5_5_NAME_PORTION = "VMware ESXi 5.5"
106 SAFE_ESX_5_5_CONTROLLER_KEY_INDEX = 200
107 FLATTEN_DISK_FULL_CLONE = "moveAllDiskBackingsAndDisallowSharing"
108 COPY_ALL_DISKS_FULL_CLONE = "moveAllDiskBackingsAndAllowSharing"
109 CURRENT_STATE_LINKED_CLONE = "moveChildMostDiskBacking"
110 QUICK_LINKED_CLONE = "createNewChildDiskBacking"
111 IP_RE = r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
112 log = logging.getLogger(__name__)
113 __virtualname__ = "vmware"
114 def __virtual__():
115     """
116     Check for VMware configuration and if required libs are available.
117     """
118     if get_configured_provider() is False:
119         return False
120     if get_dependencies() is False:
121         return False
122     return __virtualname__
123 def _get_active_provider_name():
124     try:
125         return __active_provider_name__.value()
126     except AttributeError:
127         return __active_provider_name__
128 def get_configured_provider():
129     """
130     Return the first configured instance.
131     """
132     return config.is_provider_configured(
133         __opts__,
134         _get_active_provider_name() or __virtualname__,
135         (
136             "url",
137             "user",
138             "password",
139         ),
140     )
141 def get_dependencies():
142     """
143     Warn if dependencies aren't met.
144     """
145     deps = {
146         "pyVmomi": HAS_PYVMOMI,
147     }
148     return config.check_driver_dependencies(__virtualname__, deps)
149 def script(vm_):
150     """
151     Return the script deployment object
152     """
153     script_name = config.get_cloud_config_value("script", vm_, __opts__)
154     if not script_name:
155         script_name = "bootstrap-salt"
156     return salt.utils.cloud.os_script(
157         script_name,
158         vm_,
159         __opts__,
160         salt.utils.cloud.salt_config_to_yaml(
161             salt.utils.cloud.minion_config(__opts__, vm_)
162         ),
163     )
164 def _str_to_bool(var):
165     if isinstance(var, bool):
166         return var
167     if isinstance(var, str):
168         return True if var.lower() == "true" else False
169     return None
170 def _get_si():
171     """
172     Authenticate with vCenter server and return service instance object.
173     """
174     url = config.get_cloud_config_value(
175         "url", get_configured_provider(), __opts__, search_global=False
176     )
177     username = config.get_cloud_config_value(
178         "user", get_configured_provider(), __opts__, search_global=False
179     )
180     password = config.get_cloud_config_value(
181         "password", get_configured_provider(), __opts__, search_global=False
182     )
183     protocol = config.get_cloud_config_value(
184         "protocol",
185         get_configured_provider(),
186         __opts__,
187         search_global=False,
188         default="https",
189     )
190     port = config.get_cloud_config_value(
191         "port", get_configured_provider(), __opts__, search_global=False, default=443
192     )
193     verify_ssl = config.get_cloud_config_value(
194         "verify_ssl",
195         get_configured_provider(),
196         __opts__,
197         search_global=False,
198         default=True,
199     )
200     return salt.utils.vmware.get_service_instance(
201         url, username, password, protocol=protocol, port=port, verify_ssl=verify_ssl
202     )
203 def _edit_existing_hard_disk_helper(disk, size_kb=None, size_gb=None, mode=None):
204     if size_kb or size_gb:
205         disk.capacityInKB = size_kb if size_kb else int(size_gb * 1024.0 * 1024.0)
206     if mode:
207         disk.backing.diskMode = mode
208     disk_spec = vim.vm.device.VirtualDeviceSpec()
209     disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
210     disk_spec.device = disk
211     return disk_spec
212 def _add_new_hard_disk_helper(
213     disk_label,
214     size_gb,
215     unit_number,
216     controller_key=1000,
217     thin_provision=False,
218     eagerly_scrub=False,
219     datastore=None,
220     vm_name=None,
221 ):
222     random_key = randint(-2099, -2000)
223     size_kb = int(size_gb * 1024.0 * 1024.0)
224     disk_spec = vim.vm.device.VirtualDeviceSpec()
225     disk_spec.fileOperation = "create"
226     disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
227     disk_spec.device = vim.vm.device.VirtualDisk()
228     disk_spec.device.key = random_key
229     disk_spec.device.deviceInfo = vim.Description()
230     disk_spec.device.deviceInfo.label = disk_label
231     disk_spec.device.deviceInfo.summary = "{} GB".format(size_gb)
232     disk_spec.device.backing = vim.vm.device.VirtualDisk.FlatVer2BackingInfo()
233     disk_spec.device.backing.thinProvisioned = thin_provision
234     disk_spec.device.backing.eagerlyScrub = eagerly_scrub
235     disk_spec.device.backing.diskMode = "persistent"
236     if datastore:
237         datastore_ref = salt.utils.vmware.get_mor_using_container_view(
238             _get_si(), vim.Datastore, datastore
239         )
240         if not datastore_ref:
241             datastore_cluster_ref = salt.utils.vmware.get_mor_using_container_view(
242                 _get_si(), vim.StoragePod, datastore
243             )
244             if not datastore_cluster_ref:
245                 raise SaltCloudSystemExit(
246                     "Specified datastore/datastore cluster ({}) for disk ({}) does not"
247                     " exist".format(datastore, disk_label)
248                 )
249             datastore_list = salt.utils.vmware.get_datastores(
250                 _get_si(), datastore_cluster_ref, get_all_datastores=True
251             )
252             datastore_free_space = 0
253             for ds_ref in datastore_list:
254                 log.trace(
255                     "Found datastore (%s) with free space (%s) in datastore "
256                     "cluster (%s)",
257                     ds_ref.name,
258                     ds_ref.summary.freeSpace,
259                     datastore,
260                 )
261                 if (
262                     ds_ref.summary.accessible
263                     and ds_ref.summary.freeSpace &gt; datastore_free_space
264                 ):
265                     datastore_free_space = ds_ref.summary.freeSpace
266                     datastore_ref = ds_ref
267             if not datastore_ref:
268                 raise SaltCloudSystemExit(
269                     "Specified datastore cluster ({}) for disk ({}) does not have any"
270                     " accessible datastores available".format(datastore, disk_label)
271                 )
272         datastore_path = "[" + str(datastore_ref.name) + "] " + vm_name
273         disk_spec.device.backing.fileName = datastore_path + "/" + disk_label + ".vmdk"
274         disk_spec.device.backing.datastore = datastore_ref
275         log.trace(
276             "Using datastore (%s) for disk (%s), vm_name (%s)",
277             datastore_ref.name,
278             disk_label,
279             vm_name,
280         )
281     disk_spec.device.controllerKey = controller_key
282     disk_spec.device.unitNumber = unit_number
283     disk_spec.device.capacityInKB = size_kb
284     return disk_spec
285 def _edit_existing_network_adapter(
286     network_adapter, new_network_name, adapter_type, switch_type, container_ref=None
287 ):
288     adapter_type.strip().lower()
289     switch_type.strip().lower()
290     if adapter_type in ["vmxnet", "vmxnet2", "vmxnet3", "e1000", "e1000e"]:
291         edited_network_adapter = salt.utils.vmware.get_network_adapter_type(
292             adapter_type
293         )
294         if isinstance(network_adapter, type(edited_network_adapter)):
295             edited_network_adapter = network_adapter
296         else:
297             log.debug(
298                 "Changing type of '%s' from '%s' to '%s'",
299                 network_adapter.deviceInfo.label,
300                 type(network_adapter).__name__.rsplit(".", 1)[1][7:].lower(),
301                 adapter_type,
302             )
303     else:
304         if adapter_type:
305             log.error(
306                 "Cannot change type of '%s' to '%s'. Not changing type",
307                 network_adapter.deviceInfo.label,
308                 adapter_type,
309             )
310         edited_network_adapter = network_adapter
311     if switch_type == "standard":
312         network_ref = salt.utils.vmware.get_mor_by_property(
313             _get_si(), vim.Network, new_network_name, container_ref=container_ref
314         )
315         edited_network_adapter.backing = (
316             vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()
317         )
318         edited_network_adapter.backing.deviceName = new_network_name
319         edited_network_adapter.backing.network = network_ref
320     elif switch_type == "distributed":
321         network_ref = salt.utils.vmware.get_mor_by_property(
322             _get_si(),
323             vim.dvs.DistributedVirtualPortgroup,
324             new_network_name,
325             container_ref=container_ref,
326         )
327         dvs_port_connection = vim.dvs.PortConnection(
328             portgroupKey=network_ref.key,
329             switchUuid=network_ref.config.distributedVirtualSwitch.uuid,
330         )
331         edited_network_adapter.backing = (
332             vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()
333         )
334         edited_network_adapter.backing.port = dvs_port_connection
335     else:
336         if not switch_type:
337             err_msg = (
338                 "The switch type to be used by '{}' has not been specified".format(
339                     network_adapter.deviceInfo.label
340                 )
341             )
342         else:
343             err_msg = "Cannot create '{}'. Invalid/unsupported switch type '{}'".format(
344                 network_adapter.deviceInfo.label, switch_type
345             )
346         raise SaltCloudSystemExit(err_msg)
347     edited_network_adapter.key = network_adapter.key
348     edited_network_adapter.deviceInfo = network_adapter.deviceInfo
349     edited_network_adapter.deviceInfo.summary = new_network_name
350     edited_network_adapter.connectable = network_adapter.connectable
351     edited_network_adapter.slotInfo = network_adapter.slotInfo
352     edited_network_adapter.controllerKey = network_adapter.controllerKey
353     edited_network_adapter.unitNumber = network_adapter.unitNumber
354     edited_network_adapter.addressType = network_adapter.addressType
355     edited_network_adapter.macAddress = network_adapter.macAddress
356     edited_network_adapter.wakeOnLanEnabled = network_adapter.wakeOnLanEnabled
357     network_spec = vim.vm.device.VirtualDeviceSpec()
358     network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
359     network_spec.device = edited_network_adapter
360     return network_spec
361 def _add_new_network_adapter_helper(
362     network_adapter_label,
363     network_name,
364     adapter_type,
365     switch_type,
366     mac,
367     container_ref=None,
368 ):
369     random_key = randint(-4099, -4000)
370     adapter_type.strip().lower()
371     switch_type.strip().lower()
372     network_spec = vim.vm.device.VirtualDeviceSpec()
373     if adapter_type in ["vmxnet", "vmxnet2", "vmxnet3", "e1000", "e1000e"]:
374         network_spec.device = salt.utils.vmware.get_network_adapter_type(adapter_type)
375     else:
376         if not adapter_type:
377             log.debug(
378                 "The type of '%s' has not been specified. "
379                 "Creating default type 'vmxnet3'",
380                 network_adapter_label,
381             )
382         else:
383             log.error(
384                 "Cannot create network adapter of type '%s'. "
385                 "Creating '%s' of default type 'vmxnet3'",
386                 adapter_type,
387                 network_adapter_label,
388             )
389         network_spec.device = vim.vm.device.VirtualVmxnet3()
390     network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
391     if switch_type == "standard":
392         network_spec.device.backing = (
393             vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()
394         )
395         network_spec.device.backing.deviceName = network_name
396         network_spec.device.backing.network = salt.utils.vmware.get_mor_by_property(
397             _get_si(), vim.Network, network_name, container_ref=container_ref
398         )
399     elif switch_type == "distributed":
400         network_ref = salt.utils.vmware.get_mor_by_property(
401             _get_si(),
402             vim.dvs.DistributedVirtualPortgroup,
403             network_name,
404             container_ref=container_ref,
405         )
406         dvs_port_connection = vim.dvs.PortConnection(
407             portgroupKey=network_ref.key,
408             switchUuid=network_ref.config.distributedVirtualSwitch.uuid,
409         )
410         network_spec.device.backing = (
411             vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()
412         )
413         network_spec.device.backing.port = dvs_port_connection
414     else:
415         if not switch_type:
416             err_msg = (
417                 "The switch type to be used by '{}' has not been specified".format(
418                     network_adapter_label
419                 )
420             )
421         else:
422             err_msg = "Cannot create '{}'. Invalid/unsupported switch type '{}'".format(
423                 network_adapter_label, switch_type
424             )
425         raise SaltCloudSystemExit(err_msg)
426     if mac != "":
427         network_spec.device.addressType = "assigned"
428         network_spec.device.macAddress = mac
429     network_spec.device.key = random_key
430     network_spec.device.deviceInfo = vim.Description()
431     network_spec.device.deviceInfo.label = network_adapter_label
432     network_spec.device.deviceInfo.summary = network_name
433     network_spec.device.wakeOnLanEnabled = True
434     network_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
435     network_spec.device.connectable.startConnected = True
436     network_spec.device.connectable.allowGuestControl = True
437     return network_spec
438 def _edit_existing_scsi_controller(scsi_controller, bus_sharing):
439     scsi_controller.sharedBus = bus_sharing
440     scsi_spec = vim.vm.device.VirtualDeviceSpec()
441     scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
442     scsi_spec.device = scsi_controller
443     return scsi_spec
444 def _add_new_scsi_controller_helper(scsi_controller_label, properties, bus_number):
445     random_key = randint(-1050, -1000)
446     adapter_type = properties["type"].strip().lower() if "type" in properties else None
447     bus_sharing = (
448         properties["bus_sharing"].strip().lower()
449         if "bus_sharing" in properties
450         else None
451     )
452     scsi_spec = vim.vm.device.VirtualDeviceSpec()
453     if adapter_type == "lsilogic":
454         summary = "LSI Logic"
455         scsi_spec.device = vim.vm.device.VirtualLsiLogicController()
456     elif adapter_type == "lsilogic_sas":
457         summary = "LSI Logic Sas"
458         scsi_spec.device = vim.vm.device.VirtualLsiLogicSASController()
459     elif adapter_type == "paravirtual":
460         summary = "VMware paravirtual SCSI"
461         scsi_spec.device = vim.vm.device.ParaVirtualSCSIController()
462     else:
463         if not adapter_type:
464             err_msg = "The type of '{}' has not been specified".format(
465                 scsi_controller_label
466             )
467         else:
468             err_msg = "Cannot create '{}'. Invalid/unsupported type '{}'".format(
469                 scsi_controller_label, adapter_type
470             )
471         raise SaltCloudSystemExit(err_msg)
472     scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
473     scsi_spec.device.key = random_key
474     scsi_spec.device.busNumber = bus_number
475     scsi_spec.device.deviceInfo = vim.Description()
476     scsi_spec.device.deviceInfo.label = scsi_controller_label
477     scsi_spec.device.deviceInfo.summary = summary
478     if bus_sharing == "virtual":
479         scsi_spec.device.sharedBus = (
480             vim.vm.device.VirtualSCSIController.Sharing.virtualSharing
481         )
482     elif bus_sharing == "physical":
483         scsi_spec.device.sharedBus = (
484             vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
485         )
486     else:
487         scsi_spec.device.sharedBus = (
488             vim.vm.device.VirtualSCSIController.Sharing.noSharing
489         )
490     return scsi_spec
491 def _add_new_ide_controller_helper(ide_controller_label, controller_key, bus_number):
492     """
493     Helper function for adding new IDE controllers
494     .. versionadded:: 2016.3.0
495     Args:
496       ide_controller_label: label of the IDE controller
497       controller_key: if not None, the controller key to use; otherwise it is randomly generated
498       bus_number: bus number
499     Returns: created device spec for an IDE controller
500     """
501     if controller_key is None:
502         controller_key = randint(-200, 250)
503     ide_spec = vim.vm.device.VirtualDeviceSpec()
504     ide_spec.device = vim.vm.device.VirtualIDEController()
505     ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
506     ide_spec.device.key = controller_key
507     ide_spec.device.busNumber = bus_number
508     ide_spec.device.deviceInfo = vim.Description()
509     ide_spec.device.deviceInfo.label = ide_controller_label
510     ide_spec.device.deviceInfo.summary = ide_controller_label
511     return ide_spec
512 def _set_cd_or_dvd_backing_type(drive, device_type, mode, iso_path):
513     if device_type == "datastore_iso_file":
514         drive.backing = vim.vm.device.VirtualCdrom.IsoBackingInfo()
515         drive.backing.fileName = iso_path
516         datastore = iso_path.partition("[")[-1].rpartition("]")[0]
517         datastore_ref = salt.utils.vmware.get_mor_by_property(
518             _get_si(), vim.Datastore, datastore
519         )
520         if datastore_ref:
521             drive.backing.datastore = datastore_ref
522         drive.deviceInfo.summary = "ISO {}".format(iso_path)
523     elif device_type == "client_device":
524         if mode == "passthrough":
525             drive.backing = vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo()
526             drive.deviceInfo.summary = "Remote Device"
527         elif mode == "atapi":
528             drive.backing = vim.vm.device.VirtualCdrom.RemoteAtapiBackingInfo()
529             drive.deviceInfo.summary = "Remote ATAPI"
530     return drive
531 def _edit_existing_cd_or_dvd_drive(drive, device_type, mode, iso_path):
532     device_type.strip().lower()
533     mode.strip().lower()
534     drive_spec = vim.vm.device.VirtualDeviceSpec()
535     drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
536     drive_spec.device = _set_cd_or_dvd_backing_type(drive, device_type, mode, iso_path)
537     return drive_spec
538 def _add_new_cd_or_dvd_drive_helper(
539     drive_label, controller_key, device_type, mode, iso_path
540 ):
541     random_key = randint(-3025, -3000)
542     device_type.strip().lower()
543     mode.strip().lower()
544     drive_spec = vim.vm.device.VirtualDeviceSpec()
545     drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
546     drive_spec.device = vim.vm.device.VirtualCdrom()
547     drive_spec.device.deviceInfo = vim.Description()
548     if device_type in ["datastore_iso_file", "client_device"]:
549         drive_spec.device = _set_cd_or_dvd_backing_type(
550             drive_spec.device, device_type, mode, iso_path
551         )
552     else:
553         if not device_type:
554             log.debug(
555                 "The 'device_type' of '%s' has not been specified. "
556                 "Creating default type 'client_device'",
557                 drive_label,
558             )
559         else:
560             log.error(
561                 "Cannot create CD/DVD drive of type '%s'. "
562                 "Creating '%s' of default type 'client_device'",
563                 device_type,
564                 drive_label,
565             )
566         drive_spec.device.backing = (
567             vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo()
568         )
569         drive_spec.device.deviceInfo.summary = "Remote Device"
570     drive_spec.device.key = random_key
571     drive_spec.device.deviceInfo.label = drive_label
572     drive_spec.device.controllerKey = controller_key
573     drive_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
574     drive_spec.device.connectable.startConnected = True
575     drive_spec.device.connectable.allowGuestControl = True
576     return drive_spec
577 def _set_network_adapter_mapping(adapter_specs):
578     adapter_mapping = vim.vm.customization.AdapterMapping()
579     adapter_mapping.adapter = vim.vm.customization.IPSettings()
580     if "domain" in list(adapter_specs.keys()):
581         domain = adapter_specs["domain"]
582         adapter_mapping.adapter.dnsDomain = domain
583     if "gateway" in list(adapter_specs.keys()):
584         gateway = adapter_specs["gateway"]
585         adapter_mapping.adapter.gateway = gateway
586     if "ip" in list(adapter_specs.keys()):
587         ip = str(adapter_specs["ip"])
588         subnet_mask = str(adapter_specs["subnet_mask"])
589         adapter_mapping.adapter.ip = vim.vm.customization.FixedIp(ipAddress=ip)
590         adapter_mapping.adapter.subnetMask = subnet_mask
591     else:
592         adapter_mapping.adapter.ip = vim.vm.customization.DhcpIpGenerator()
593     return adapter_mapping
594 def _get_mode_spec(device, mode, disk_spec):
595     if device.backing.diskMode != mode:
596         if not disk_spec:
597             disk_spec = _edit_existing_hard_disk_helper(disk=device, mode=mode)
598         else:
599             disk_spec.device.backing.diskMode = mode
600     return disk_spec
601 def _get_size_spec(device, size_gb=None, size_kb=None):
602     if size_kb is None and size_gb is not None:
603         size_kb = int(size_gb * 1024.0 * 1024.0)
604     disk_spec = (
605         _edit_existing_hard_disk_helper(disk=device, size_kb=size_kb)
606         if device.capacityInKB &lt; size_kb
607         else None
608     )
609     return disk_spec
610 def _iter_disk_unit_number(unit_number):
611     """
612     Apparently vmware reserves ID 7 for SCSI controllers, so we cannot specify
613     hard drives for 7.
614     Skip 7 to make sure.
615     """
616     unit_number += 1
617     if unit_number == 7:
618         unit_number += 1
619     return unit_number
620 def _manage_devices(devices, vm=None, container_ref=None, new_vm_name=None):
621     unit_number = 0
622     bus_number = 0
623     device_specs = []
624     existing_disks_label = []
625     existing_scsi_controllers_label = []
626     existing_ide_controllers_label = []
627     existing_network_adapters_label = []
628     existing_cd_drives_label = []
629     ide_controllers = {}
630     nics_map = []
631     cloning_from_vm = vm is not None
632     if cloning_from_vm:
633         for device in vm.config.hardware.device:
634             if isinstance(device, vim.vm.device.VirtualDisk):
635                 if "disk" in list(devices.keys()):
636                     unit_number = _iter_disk_unit_number(unit_number)
637                     existing_disks_label.append(device.deviceInfo.label)
638                     if device.deviceInfo.label in list(devices["disk"].keys()):
639                         disk_spec = None
640                         if "size" in devices["disk"][device.deviceInfo.label]:
641                             size_gb = float(
642                                 devices["disk"][device.deviceInfo.label]["size"]
643                             )
644                             size_kb = int(size_gb * 1024.0 * 1024.0)
645                         else:
646                             size_kb = device.capacityInKB
647                             size_gb = size_kb / (1024.0 * 1024.0)
648                             log.debug(
649                                 "Virtual disk size for '%s' was not "
650                                 "specified in the cloud profile or map file. "
651                                 "Using existing virtual disk size of '%sGB'",
652                                 device.deviceInfo.label,
653                                 size_gb,
654                             )
655                         if device.capacityInKB &gt; size_kb:
656                             raise SaltCloudSystemExit(
657                                 "The specified disk size '{}GB' for '{}' is "
658                                 "smaller than the disk image size '{}GB'. It must "
659                                 "be equal to or greater than the disk image".format(
660                                     float(
661                                         devices["disk"][device.deviceInfo.label]["size"]
662                                     ),
663                                     device.deviceInfo.label,
664                                     float(device.capacityInKB / (1024.0 * 1024.0)),
665                                 )
666                             )
667                         else:
668                             disk_spec = _get_size_spec(device=device, size_kb=size_kb)
669                         if "mode" in devices["disk"][device.deviceInfo.label]:
670                             if devices["disk"][device.deviceInfo.label]["mode"] in [
671                                 "independent_persistent",
672                                 "independent_nonpersistent",
673                                 "dependent",
674                             ]:
675                                 mode = devices["disk"][device.deviceInfo.label]["mode"]
676                                 disk_spec = _get_mode_spec(device, mode, disk_spec)
677                             else:
678                                 raise SaltCloudSystemExit(
679                                     "Invalid disk backing mode specified!"
680                                 )
681                         if disk_spec is not None:
682                             device_specs.append(disk_spec)
683             elif isinstance(
684                 device.backing,
685                 (
686                     vim.vm.device.VirtualEthernetCard.NetworkBackingInfo,
687                     vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo,
688                 ),
689             ):
690                 if "network" in list(devices.keys()):
691                     existing_network_adapters_label.append(device.deviceInfo.label)
692                     if device.deviceInfo.label in list(devices["network"].keys()):
693                         network_name = devices["network"][device.deviceInfo.label][
694                             "name"
695                         ]
696                         adapter_type = (
697                             devices["network"][device.deviceInfo.label]["adapter_type"]
698                             if "adapter_type"
699                             in devices["network"][device.deviceInfo.label]
700                             else ""
701                         )
702                         switch_type = (
703                             devices["network"][device.deviceInfo.label]["switch_type"]
704                             if "switch_type"
705                             in devices["network"][device.deviceInfo.label]
706                             else ""
707                         )
708                         network_spec = _edit_existing_network_adapter(
709                             device,
710                             network_name,
711                             adapter_type,
712                             switch_type,
713                             container_ref,
714                         )
715                         adapter_mapping = _set_network_adapter_mapping(
716                             devices["network"][device.deviceInfo.label]
717                         )
718                         device_specs.append(network_spec)
719                         nics_map.append(adapter_mapping)
720             elif hasattr(device, "scsiCtlrUnitNumber"):
721                 if "scsi" in list(devices.keys()):
722                     bus_number += 1
723                     existing_scsi_controllers_label.append(device.deviceInfo.label)
724                     if device.deviceInfo.label in list(devices["scsi"].keys()):
725                         scsi_controller_properties = devices["scsi"][
726                             device.deviceInfo.label
727                         ]
728                         bus_sharing = (
729                             scsi_controller_properties["bus_sharing"].strip().lower()
730                             if "bus_sharing" in scsi_controller_properties
731                             else None
732                         )
733                         if bus_sharing and bus_sharing in ["virtual", "physical", "no"]:
734                             bus_sharing = "{}Sharing".format(bus_sharing)
735                             if bus_sharing != device.sharedBus:
736                                 scsi_spec = _edit_existing_scsi_controller(
737                                     device, bus_sharing
738                                 )
739                                 device_specs.append(scsi_spec)
740             elif isinstance(device, vim.vm.device.VirtualCdrom):
741                 if "cd" in list(devices.keys()):
742                     existing_cd_drives_label.append(device.deviceInfo.label)
743                     if device.deviceInfo.label in list(devices["cd"].keys()):
744                         device_type = (
745                             devices["cd"][device.deviceInfo.label]["device_type"]
746                             if "device_type" in devices["cd"][device.deviceInfo.label]
747                             else ""
748                         )
749                         mode = (
750                             devices["cd"][device.deviceInfo.label]["mode"]
751                             if "mode" in devices["cd"][device.deviceInfo.label]
752                             else ""
753                         )
754                         iso_path = (
755                             devices["cd"][device.deviceInfo.label]["iso_path"]
756                             if "iso_path" in devices["cd"][device.deviceInfo.label]
757                             else ""
758                         )
759                         cd_drive_spec = _edit_existing_cd_or_dvd_drive(
760                             device, device_type, mode, iso_path
761                         )
762                         device_specs.append(cd_drive_spec)
763             elif isinstance(device, vim.vm.device.VirtualIDEController):
764                 ide_controllers[device.key] = len(device.device)
765     if "network" in list(devices.keys()):
766         network_adapters_to_create = list(
767             set(devices["network"].keys()) - set(existing_network_adapters_label)
768         )
769         network_adapters_to_create.sort()
770         if network_adapters_to_create:
771             log.debug("Networks adapters to create: %s", network_adapters_to_create)
772         for network_adapter_label in network_adapters_to_create:
773             network_name = devices["network"][network_adapter_label]["name"]
774             adapter_type = (
775                 devices["network"][network_adapter_label]["adapter_type"]
776                 if "adapter_type" in devices["network"][network_adapter_label]
777                 else ""
778             )
779             switch_type = (
780                 devices["network"][network_adapter_label]["switch_type"]
781                 if "switch_type" in devices["network"][network_adapter_label]
782                 else ""
783             )
784             mac = (
785                 devices["network"][network_adapter_label]["mac"]
786                 if "mac" in devices["network"][network_adapter_label]
787                 else ""
788             )
789             network_spec = _add_new_network_adapter_helper(
790                 network_adapter_label,
791                 network_name,
792                 adapter_type,
793                 switch_type,
794                 mac,
795                 container_ref,
796             )
797             adapter_mapping = _set_network_adapter_mapping(
798                 devices["network"][network_adapter_label]
799             )
800             device_specs.append(network_spec)
801             nics_map.append(adapter_mapping)
802     if "scsi" in list(devices.keys()):
803         scsi_controllers_to_create = list(
804             set(devices["scsi"].keys()) - set(existing_scsi_controllers_label)
805         )
806         scsi_controllers_to_create.sort()
807         if scsi_controllers_to_create:
808             log.debug("SCSI controllers to create: %s", scsi_controllers_to_create)
809         for scsi_controller_label in scsi_controllers_to_create:
810             scsi_controller_properties = devices["scsi"][scsi_controller_label]
811             scsi_spec = _add_new_scsi_controller_helper(
812                 scsi_controller_label, scsi_controller_properties, bus_number
813             )
814             device_specs.append(scsi_spec)
815             bus_number += 1
816     if "ide" in list(devices.keys()):
817         ide_controllers_to_create = list(
818             set(devices["ide"].keys()) - set(existing_ide_controllers_label)
819         )
820         ide_controllers_to_create.sort()
821         if ide_controllers_to_create:
822             log.debug("IDE controllers to create: %s", ide_controllers_to_create)
823         vcenter_name = get_vcenter_version(call="function")
824         controller_index = (
825             SAFE_ESX_5_5_CONTROLLER_KEY_INDEX
826             if ESX_5_5_NAME_PORTION in vcenter_name
827             else None
828         )
829         for ide_controller_label in ide_controllers_to_create:
830             ide_spec = _add_new_ide_controller_helper(
831                 ide_controller_label, controller_index, bus_number
832             )
833             device_specs.append(ide_spec)
834             bus_number += 1
835             if controller_index is not None:
836                 controller_index += 1
837     if "disk" in list(devices.keys()):
838         disks_to_create = list(set(devices["disk"].keys()) - set(existing_disks_label))
839         disks_to_create.sort()
840         if disks_to_create:
841             log.debug("Hard disks to create: %s", disks_to_create)
842         for disk_label in disks_to_create:
843             size_gb = float(devices["disk"][disk_label]["size"])
844             thin_provision = (
845                 bool(devices["disk"][disk_label]["thin_provision"])
846                 if "thin_provision" in devices["disk"][disk_label]
847                 else False
848             )
849             eagerly_scrub = (
850                 bool(devices["disk"][disk_label]["eagerly_scrub"])
851                 if "eagerly_scrub" in devices["disk"][disk_label]
852                 else False
853             )
854             datastore = devices["disk"][disk_label].get("datastore", None)
855             disk_spec = _add_new_hard_disk_helper(
856                 disk_label,
857                 size_gb,
858                 unit_number,
859                 thin_provision=thin_provision,
860                 eagerly_scrub=eagerly_scrub,
861                 datastore=datastore,
862                 vm_name=new_vm_name,
863             )
864             if "controller" in devices["disk"][disk_label]:
865                 for spec in device_specs:
866                     if (
867                         spec.device.deviceInfo.label
868                         == devices["disk"][disk_label]["controller"]
869                     ):
870                         disk_spec.device.controllerKey = spec.device.key
871                         break
872             device_specs.append(disk_spec)
873             unit_number = _iter_disk_unit_number(unit_number)
874     if "cd" in list(devices.keys()):
875         cd_drives_to_create = list(
876             set(devices["cd"].keys()) - set(existing_cd_drives_label)
877         )
878         cd_drives_to_create.sort()
879         if cd_drives_to_create:
880             log.debug("CD/DVD drives to create: %s", cd_drives_to_create)
881         for cd_drive_label in cd_drives_to_create:
882             device_type = (
883                 devices["cd"][cd_drive_label]["device_type"]
884                 if "device_type" in devices["cd"][cd_drive_label]
885                 else ""
886             )
887             mode = (
888                 devices["cd"][cd_drive_label]["mode"]
889                 if "mode" in devices["cd"][cd_drive_label]
890                 else ""
891             )
892             iso_path = (
893                 devices["cd"][cd_drive_label]["iso_path"]
894                 if "iso_path" in devices["cd"][cd_drive_label]
895                 else ""
896             )
897             controller_key = None
898             if "controller" in devices["cd"][cd_drive_label]:
899                 for spec in device_specs:
900                     if (
901                         spec.device.deviceInfo.label
902                         == devices["cd"][cd_drive_label]["controller"]
903                     ):
904                         controller_key = spec.device.key
905                         ide_controllers[controller_key] = 0
906                         break
907             else:
908                 for ide_controller_key, num_devices in ide_controllers.items():
909                     if num_devices &lt; 2:
910                         controller_key = ide_controller_key
911                         break
912             if not controller_key:
913                 log.error(
914                     "No more available controllers for '%s'. "
915                     "All IDE controllers are currently in use",
916                     cd_drive_label,
917                 )
918             else:
919                 cd_drive_spec = _add_new_cd_or_dvd_drive_helper(
920                     cd_drive_label, controller_key, device_type, mode, iso_path
921                 )
922                 device_specs.append(cd_drive_spec)
923                 ide_controllers[controller_key] += 1
924     ret = {"device_specs": device_specs, "nics_map": nics_map}
925     return ret
926 def _wait_for_vmware_tools(vm_ref, max_wait):
927     time_counter = 0
928     starttime = time.time()
929     while time_counter &lt; max_wait:
930         if time_counter % 5 == 0:
931             log.info(
932                 "[ %s ] Waiting for VMware tools to be running [%s s]",
933                 vm_ref.name,
934                 time_counter,
935             )
936         if str(vm_ref.summary.guest.toolsRunningStatus) == "guestToolsRunning":
937             log.info(
938                 "[ %s ] Successfully got VMware tools running on the guest in "
939                 "%s seconds",
940                 vm_ref.name,
941                 time_counter,
942             )
943             return True
944         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
945         time_counter += 1
946     log.warning(
947         "[ %s ] Timeout Reached. VMware tools still not running after waiting "
948         "for %s seconds",
949         vm_ref.name,
950         max_wait,
951     )
952     return False
953 def _valid_ip(ip_address):
954     """
955     Check if the IP address is valid
956     Return either True or False
957     """
958     octets = ip_address.split(".")
959     if len(octets) != 4:
960         return False
961     for i, octet in enumerate(octets):
962         try:
963             octets[i] = int(octet)
964         except ValueError:
965             return False
966     first_octet, second_octet, third_octet, fourth_octet = octets
967     if first_octet &lt; 1 or first_octet &gt; 223 or first_octet == 127:
968         return False
969     if first_octet == 169 and second_octet == 254:
970         return False
971     for octet in (second_octet, third_octet, fourth_octet):
972         if (octet &lt; 0) or (octet &gt; 255):
973             return False
974     return True
975 def _wait_for_ip(vm_ref, max_wait):
976     max_wait_vmware_tools = max_wait
977     max_wait_ip = max_wait
978     vmware_tools_status = _wait_for_vmware_tools(vm_ref, max_wait_vmware_tools)
979     if not vmware_tools_status:
980         vm_name = vm_ref.summary.config.name
981         resolved_ips = salt.utils.network.host_to_ips(vm_name)
982         log.debug(
983             "Timeout waiting for VMware tools. The name %s resolved to %s",
984             vm_name,
985             resolved_ips,
986         )
987         if isinstance(resolved_ips, list) and resolved_ips:
988             return resolved_ips[0]
989         return False
990     time_counter = 0
991     starttime = time.time()
992     while time_counter &lt; max_wait_ip:
993         if time_counter % 5 == 0:
994             log.info(
995                 "[ %s ] Waiting to retrieve IPv4 information [%s s]",
996                 vm_ref.name,
997                 time_counter,
998             )
999         if vm_ref.summary.guest.ipAddress and _valid_ip(vm_ref.summary.guest.ipAddress):
1000             log.info(
1001                 "[ %s ] Successfully retrieved IPv4 information in %s seconds",
1002                 vm_ref.name,
1003                 time_counter,
1004             )
1005             return vm_ref.summary.guest.ipAddress
1006         for net in vm_ref.guest.net:
1007             if net.ipConfig.ipAddress:
1008                 for current_ip in net.ipConfig.ipAddress:
1009                     if _valid_ip(current_ip.ipAddress):
1010                         log.info(
1011                             "[ %s ] Successfully retrieved IPv4 information "
1012                             "in %s seconds",
1013                             vm_ref.name,
1014                             time_counter,
1015                         )
1016                         return current_ip.ipAddress
1017         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
1018         time_counter += 1
1019     log.warning(
1020         "[ %s ] Timeout Reached. Unable to retrieve IPv4 information after "
1021         "waiting for %s seconds",
1022         vm_ref.name,
1023         max_wait_ip,
1024     )
1025     return False
1026 def _wait_for_host(host_ref, task_type, sleep_seconds=5, log_level="debug"):
1027     time_counter = 0
1028     starttime = time.time()
1029     while host_ref.runtime.connectionState != "notResponding":
1030         if time_counter % sleep_seconds == 0:
1031             log.log(
1032                 logging.INFO if log_level == "info" else logging.DEBUG,
1033                 "[ %s ] Waiting for host %s to finish [%s s]",
1034                 host_ref.name,
1035                 task_type,
1036                 time_counter,
1037             )
1038         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
1039         time_counter += 1
1040     while host_ref.runtime.connectionState != "connected":
1041         if time_counter % sleep_seconds == 0:
1042             log.log(
1043                 logging.INFO if log_level == "info" else logging.DEBUG,
1044                 "[ %s ] Waiting for host %s to finish [%s s]",
1045                 host_ref.name,
1046                 task_type,
1047                 time_counter,
1048             )
1049         time.sleep(1.0 - ((time.time() - starttime) % 1.0))
1050         time_counter += 1
1051     if host_ref.runtime.connectionState == "connected":
1052         log.log(
1053             logging.INFO if log_level == "info" else logging.DEBUG,
1054             "[ %s ] Successfully completed host %s in %s seconds",
1055             host_ref.name,
1056             task_type,
1057             time_counter,
1058         )
1059     else:
1060         log.error("Could not connect back to the host system")
1061 def _format_instance_info_select(vm, selection):
1062     def defaultto(machine, section, default="N/A"):
1063         """
1064         Return either a named value from a VirtualMachineConfig or a
1065         default string "N/A".
1066         """
1067         return default if section not in machine else machine[section]
1068     vm_select_info = {}
1069     if "id" in selection:
1070         vm_select_info["id"] = vm["name"]
1071     if "image" in selection:
1072         vm_select_info["image"] = "{} (Detected)".format(
1073             defaultto(vm, "config.guestFullName")
1074         )
1075     if "size" in selection:
1076         cpu = defaultto(vm, "config.hardware.numCPU")
1077         ram = "{} MB".format(defaultto(vm, "config.hardware.memoryMB"))
1078         vm_select_info["size"] = "cpu: {}\nram: {}".format(cpu, ram)
1079         vm_select_info["size_dict"] = {
1080             "cpu": cpu,
1081             "memory": ram,
1082         }
1083     if "state" in selection:
1084         vm_select_info["state"] = str(defaultto(vm, "summary.runtime.powerState"))
1085     if "guest_id" in selection:
1086         vm_select_info["guest_id"] = defaultto(vm, "config.guestId")
1087     if "hostname" in selection:
1088         vm_select_info["hostname"] = vm["object"].guest.hostName
1089     if "path" in selection:
1090         vm_select_info["path"] = defaultto(vm, "config.files.vmPathName")
1091     if "tools_status" in selection:
1092         vm_select_info["tools_status"] = str(defaultto(vm, "guest.toolsStatus"))
1093     if "private_ips" in selection or "networks" in selection:
1094         network_full_info = {}
1095         ip_addresses = []
1096         if "guest.net" in vm:
1097             for net in vm["guest.net"]:
1098                 network_full_info[net.network] = {
1099                     "connected": net.connected,
1100                     "ip_addresses": net.ipAddress,
1101                     "mac_address": net.macAddress,
1102                 }
1103                 ip_addresses.extend(net.ipAddress)
1104         if "private_ips" in selection:
1105             vm_select_info["private_ips"] = ip_addresses
1106         if "networks" in selection:
1107             vm_select_info["networks"] = network_full_info
1108     if any(x in ["devices", "mac_address", "mac_addresses"] for x in selection):
1109         device_full_info = {}
1110         device_mac_addresses = []
1111         if "config.hardware.device" in vm:
1112             for device in vm["config.hardware.device"]:
1113                 device_full_info[device.deviceInfo.label] = {}
1114                 if "devices" in selection:
1115                     device_full_info[device.deviceInfo.label]["key"] = (device.key,)
1116                     device_full_info[device.deviceInfo.label]["label"] = (
1117                         device.deviceInfo.label,
1118                     )
1119                     device_full_info[device.deviceInfo.label]["summary"] = (
1120                         device.deviceInfo.summary,
1121                     )
1122                     device_full_info[device.deviceInfo.label]["type"] = type(
1123                         device
1124                     ).__name__.rsplit(".", 1)[1]
1125                     if device.unitNumber:
1126                         device_full_info[device.deviceInfo.label][
1127                             "unitNumber"
1128                         ] = device.unitNumber
1129                     if hasattr(device, "connectable") and device.connectable:
1130                         device_full_info[device.deviceInfo.label][
1131                             "startConnected"
1132                         ] = device.connectable.startConnected
1133                         device_full_info[device.deviceInfo.label][
1134                             "allowGuestControl"
1135                         ] = device.connectable.allowGuestControl
1136                         device_full_info[device.deviceInfo.label][
1137                             "connected"
1138                         ] = device.connectable.connected
1139                         device_full_info[device.deviceInfo.label][
1140                             "status"
1141                         ] = device.connectable.status
1142                     if hasattr(device, "controllerKey") and device.controllerKey:
1143                         device_full_info[device.deviceInfo.label][
1144                             "controllerKey"
1145                         ] = device.controllerKey
1146                     if hasattr(device, "addressType"):
1147                         device_full_info[device.deviceInfo.label][
1148                             "addressType"
1149                         ] = device.addressType
1150                     if hasattr(device, "busNumber"):
1151                         device_full_info[device.deviceInfo.label][
1152                             "busNumber"
1153                         ] = device.busNumber
1154                     if hasattr(device, "device"):
1155                         device_full_info[device.deviceInfo.label][
1156                             "deviceKeys"
1157                         ] = device.device
1158                     if hasattr(device, "videoRamSizeInKB"):
1159                         device_full_info[device.deviceInfo.label][
1160                             "videoRamSizeInKB"
1161                         ] = device.videoRamSizeInKB
1162                     if isinstance(device, vim.vm.device.VirtualDisk):
1163                         device_full_info[device.deviceInfo.label][
1164                             "capacityInKB"
1165                         ] = device.capacityInKB
1166                         device_full_info[device.deviceInfo.label][
1167                             "diskMode"
1168                         ] = device.backing.diskMode
1169                         device_full_info[device.deviceInfo.label][
1170                             "fileName"
1171                         ] = device.backing.fileName
1172                 if hasattr(device, "macAddress"):
1173                     device_full_info[device.deviceInfo.label][
1174                         "macAddress"
1175                     ] = device.macAddress
1176                     device_mac_addresses.append(device.macAddress)
1177         if "devices" in selection:
1178             vm_select_info["devices"] = device_full_info
1179         if "mac_address" in selection or "mac_addresses" in selection:
1180             vm_select_info["mac_addresses"] = device_mac_addresses
1181     if "storage" in selection:
1182         storage_full_info = {
1183             "committed": int(vm["summary.storage.committed"])
1184             if "summary.storage.committed" in vm
1185             else "N/A",
1186             "uncommitted": int(vm["summary.storage.uncommitted"])
1187             if "summary.storage.uncommitted" in vm
1188             else "N/A",
1189             "unshared": int(vm["summary.storage.unshared"])
1190             if "summary.storage.unshared" in vm
1191             else "N/A",
1192         }
1193         vm_select_info["storage"] = storage_full_info
1194     if "files" in selection:
1195         file_full_info = {}
1196         if "layoutEx.file" in vm:
1197             for filename in vm["layoutEx.file"]:
1198                 file_full_info[filename.key] = {
1199                     "key": filename.key,
1200                     "name": filename.name,
1201                     "size": filename.size,
1202                     "type": filename.type,
1203                 }
1204         vm_select_info["files"] = file_full_info
1205     return vm_select_info
1206 def _format_instance_info(vm):
1207     device_full_info = {}
1208     device_mac_addresses = []
1209     if "config.hardware.device" in vm:
1210         for device in vm["config.hardware.device"]:
1211             device_full_info[device.deviceInfo.label] = {
1212                 "key": device.key,
1213                 "label": device.deviceInfo.label,
1214                 "summary": device.deviceInfo.summary,
1215                 "type": type(device).__name__.rsplit(".", 1)[1],
1216             }
1217             if device.unitNumber:
1218                 device_full_info[device.deviceInfo.label][
1219                     "unitNumber"
1220                 ] = device.unitNumber
1221             if hasattr(device, "connectable") and device.connectable:
1222                 device_full_info[device.deviceInfo.label][
1223                     "startConnected"
1224                 ] = device.connectable.startConnected
1225                 device_full_info[device.deviceInfo.label][
1226                     "allowGuestControl"
1227                 ] = device.connectable.allowGuestControl
1228                 device_full_info[device.deviceInfo.label][
1229                     "connected"
1230                 ] = device.connectable.connected
1231                 device_full_info[device.deviceInfo.label][
1232                     "status"
1233                 ] = device.connectable.status
1234             if hasattr(device, "controllerKey") and device.controllerKey:
1235                 device_full_info[device.deviceInfo.label][
1236                     "controllerKey"
1237                 ] = device.controllerKey
1238             if hasattr(device, "addressType"):
1239                 device_full_info[device.deviceInfo.label][
1240                     "addressType"
1241                 ] = device.addressType
1242             if hasattr(device, "macAddress"):
1243                 device_full_info[device.deviceInfo.label][
1244                     "macAddress"
1245                 ] = device.macAddress
1246                 device_mac_addresses.append(device.macAddress)
1247             if hasattr(device, "busNumber"):
1248                 device_full_info[device.deviceInfo.label][
1249                     "busNumber"
1250                 ] = device.busNumber
1251             if hasattr(device, "device"):
1252                 device_full_info[device.deviceInfo.label]["deviceKeys"] = device.device
1253             if hasattr(device, "videoRamSizeInKB"):
1254                 device_full_info[device.deviceInfo.label][
1255                     "videoRamSizeInKB"
1256                 ] = device.videoRamSizeInKB
1257             if isinstance(device, vim.vm.device.VirtualDisk):
1258                 device_full_info[device.deviceInfo.label][
1259                     "capacityInKB"
1260                 ] = device.capacityInKB
1261                 device_full_info[device.deviceInfo.label][
1262                     "diskMode"
1263                 ] = device.backing.diskMode
1264                 device_full_info[device.deviceInfo.label][
1265                     "fileName"
1266                 ] = device.backing.fileName
1267     storage_full_info = {
1268         "committed": int(vm["summary.storage.committed"])
1269         if "summary.storage.committed" in vm
1270         else "N/A",
1271         "uncommitted": int(vm["summary.storage.uncommitted"])
1272         if "summary.storage.uncommitted" in vm
1273         else "N/A",
1274         "unshared": int(vm["summary.storage.unshared"])
1275         if "summary.storage.unshared" in vm
1276         else "N/A",
1277     }
1278     file_full_info = {}
1279     if "layoutEx.file" in vm:
1280         for filename in vm["layoutEx.file"]:
1281             file_full_info[filename.key] = {
1282                 "key": filename.key,
1283                 "name": filename.name,
1284                 "size": filename.size,
1285                 "type": filename.type,
1286             }
1287     network_full_info = {}
1288     ip_addresses = []
1289     if "guest.net" in vm:
1290         for net in vm["guest.net"]:
1291             network_full_info[net.network] = {
1292                 "connected": net.connected,
1293                 "ip_addresses": net.ipAddress,
1294                 "mac_address": net.macAddress,
1295             }
1296             ip_addresses.extend(net.ipAddress)
1297     cpu = vm["config.hardware.numCPU"] if "config.hardware.numCPU" in vm else "N/A"
1298     ram = (
1299         "{} MB".format(vm["config.hardware.memoryMB"])
1300         if "config.hardware.memoryMB" in vm
1301         else "N/A"
1302     )
1303     vm_full_info = {
1304         "id": str(vm["name"]),
1305         "image": "{} (Detected)".format(vm["config.guestFullName"])
1306         if "config.guestFullName" in vm
1307         else "N/A",
1308         "size": "cpu: {}\nram: {}".format(cpu, ram),
1309         "size_dict": {"cpu": cpu, "memory": ram},
1310         "state": str(vm["summary.runtime.powerState"])
1311         if "summary.runtime.powerState" in vm
1312         else "N/A",
1313         "private_ips": ip_addresses,
1314         "public_ips": [],
1315         "devices": device_full_info,
1316         "storage": storage_full_info,
1317         "files": file_full_info,
1318         "guest_id": str(vm["config.guestId"]) if "config.guestId" in vm else "N/A",
1319         "hostname": str(vm["object"].guest.hostName),
1320         "mac_addresses": device_mac_addresses,
1321         "networks": network_full_info,
1322         "path": str(vm["config.files.vmPathName"])
1323         if "config.files.vmPathName" in vm
1324         else "N/A",
1325         "tools_status": str(vm["guest.toolsStatus"])
1326         if "guest.toolsStatus" in vm
1327         else "N/A",
1328     }
1329     return vm_full_info
1330 def _get_snapshots(snapshot_list, current_snapshot=None, parent_snapshot_path=""):
1331     snapshots = {}
1332     for snapshot in snapshot_list:
1333         snapshot_path = "{}/{}".format(parent_snapshot_path, snapshot.name)
1334         snapshots[snapshot_path] = {
1335             "name": snapshot.name,
1336             "description": snapshot.description,
1337             "created": str(snapshot.createTime).split(".")[0],
1338             "state": snapshot.state,
1339             "path": snapshot_path,
1340         }
1341         if current_snapshot and current_snapshot == snapshot.snapshot:
1342             return snapshots[snapshot_path]
1343         if snapshot.childSnapshotList:
1344             ret = _get_snapshots(
1345                 snapshot.childSnapshotList, current_snapshot, snapshot_path
1346             )
1347             if current_snapshot:
1348                 return ret
1349             snapshots.update(ret)
1350     return snapshots
1351 def _get_snapshot_ref_helper(base_snapshot, snapshot_name):
1352     if base_snapshot.name == snapshot_name:
1353         return base_snapshot
1354     for snapshot in base_snapshot.childSnapshotList:
1355         snapshot_ref = _get_snapshot_ref_helper(snapshot, snapshot_name)
1356         if snapshot_ref is not None:
1357             return snapshot_ref
1358     return None
1359 def _get_snapshot_ref_by_name(vm_ref, snapshot_name):
1360     snapshot_ref = None
1361     try:
1362         for root_snapshot in vm_ref.snapshot.rootSnapshotList:
1363             snapshot_ref = _get_snapshot_ref_helper(root_snapshot, snapshot_name)
1364             if snapshot_ref is not None:
1365                 break
1366     except (IndexError, AttributeError):
1367         snapshot_ref = None
1368     return snapshot_ref
1369 def _upg_tools_helper(vm, reboot=False):
1370     if vm.config.template:
1371         status = "VMware tools cannot be updated on a template"
1372     elif vm.guest.toolsStatus == "toolsOk":
1373         status = "VMware tools is already up to date"
1374     elif vm.summary.runtime.powerState != "poweredOn":
1375         status = "VM must be powered on to upgrade tools"
1376     elif vm.guest.toolsStatus in ["toolsNotRunning", "toolsNotInstalled"]:
1377         status = "VMware tools is either not running or not installed"
1378     elif vm.guest.toolsStatus == "toolsOld":
1379         log.info("Upgrading VMware tools on %s", vm.name)
1380         try:
1381             if vm.guest.guestFamily == "windowsGuest" and not reboot:
1382                 log.info("Reboot suppressed on %s", vm.name)
1383                 task = vm.UpgradeTools('/S /v"/qn REBOOT=R"')
1384             elif vm.guest.guestFamily in ["linuxGuest", "windowsGuest"]:
1385                 task = vm.UpgradeTools()
1386             else:
1387                 return "Only Linux and Windows guests are currently supported"
1388             salt.utils.vmware.wait_for_task(
1389                 task, vm.name, "tools upgrade", sleep_seconds=5, log_level="info"
1390             )
1391         except Exception as exc:  # pylint: disable=broad-except
1392             log.error(
1393                 "Error while upgrading VMware tools on VM %s: %s",
1394                 vm.name,
1395                 exc,
1396                 exc_info_on_loglevel=logging.DEBUG,
1397             )
1398             return "VMware tools upgrade failed"
1399         status = "VMware tools upgrade succeeded"
1400     else:
1401         status = "VMWare tools could not be upgraded"
1402     return status
1403 def _get_hba_type(hba_type):
1404     """
1405     Convert a string representation of a HostHostBusAdapter into an
1406     object reference.
1407     """
1408     if hba_type == "parallel":
1409         return vim.host.ParallelScsiHba
1410     elif hba_type == "block":
1411         return vim.host.BlockHba
1412     elif hba_type == "iscsi":
1413         return vim.host.InternetScsiHba
1414     elif hba_type == "fibre":
1415         return vim.host.FibreChannelHba
1416     raise ValueError("Unknown Host Bus Adapter Type")
1417 def test_vcenter_connection(kwargs=None, call=None):
1418     """
1419     Test if the connection can be made to the vCenter server using
1420     the specified credentials inside ``/etc/salt/cloud.providers``
1421     or ``/etc/salt/cloud.providers.d/vmware.conf``
1422     CLI Example:
1423     .. code-block:: bash
1424         salt-cloud -f test_vcenter_connection my-vmware-config
1425     """
1426     if call != "function":
1427         raise SaltCloudSystemExit(
1428             "The test_vcenter_connection function must be called with -f or --function."
1429         )
1430     try:
1431         _get_si()
1432     except Exception as exc:  # pylint: disable=broad-except
1433         return "failed to connect: {}".format(exc)
1434     return "connection successful"
1435 def get_vcenter_version(kwargs=None, call=None):
1436     """
1437     Show the vCenter Server version with build number.
1438     CLI Example:
1439     .. code-block:: bash
1440         salt-cloud -f get_vcenter_version my-vmware-config
1441     """
1442     if call != "function":
1443         raise SaltCloudSystemExit(
1444             "The get_vcenter_version function must be called with -f or --function."
1445         )
1446     inv = salt.utils.vmware.get_inventory(_get_si())
1447     return inv.about.fullName
1448 def list_datacenters(kwargs=None, call=None):
1449     """
1450     List all the data centers for this VMware environment
1451     CLI Example:
1452     .. code-block:: bash
1453         salt-cloud -f list_datacenters my-vmware-config
1454     """
1455     if call != "function":
1456         raise SaltCloudSystemExit(
1457             "The list_datacenters function must be called with -f or --function."
1458         )
1459     return {"Datacenters": salt.utils.vmware.list_datacenters(_get_si())}
1460 def list_portgroups(kwargs=None, call=None):
1461     """
1462     List all the distributed virtual portgroups for this VMware environment
1463     CLI Example:
1464     .. code-block:: bash
1465         salt-cloud -f list_portgroups my-vmware-config
1466     """
1467     if call != "function":
1468         raise SaltCloudSystemExit(
1469             "The list_portgroups function must be called with -f or --function."
1470         )
1471     return {"Portgroups": salt.utils.vmware.list_portgroups(_get_si())}
1472 def list_clusters(kwargs=None, call=None):
1473     """
1474     List all the clusters for this VMware environment
1475     CLI Example:
1476     .. code-block:: bash
1477         salt-cloud -f list_clusters my-vmware-config
1478     """
1479     if call != "function":
1480         raise SaltCloudSystemExit(
1481             "The list_clusters function must be called with -f or --function."
1482         )
1483     return {"Clusters": salt.utils.vmware.list_clusters(_get_si())}
1484 def list_datastore_clusters(kwargs=None, call=None):
1485     """
1486     List all the datastore clusters for this VMware environment
1487     CLI Example:
1488     .. code-block:: bash
1489         salt-cloud -f list_datastore_clusters my-vmware-config
1490     """
1491     if call != "function":
1492         raise SaltCloudSystemExit(
1493             "The list_datastore_clusters function must be called with -f or --function."
1494         )
1495     return {"Datastore Clusters": salt.utils.vmware.list_datastore_clusters(_get_si())}
1496 def list_datastores(kwargs=None, call=None):
1497     """
1498     List all the datastores for this VMware environment
1499     CLI Example:
1500     .. code-block:: bash
1501         salt-cloud -f list_datastores my-vmware-config
1502     """
1503     if call != "function":
1504         raise SaltCloudSystemExit(
1505             "The list_datastores function must be called with -f or --function."
1506         )
1507     return {"Datastores": salt.utils.vmware.list_datastores(_get_si())}
1508 def list_hosts(kwargs=None, call=None):
1509     """
1510     List all the hosts for this VMware environment
1511     CLI Example:
1512     .. code-block:: bash
1513         salt-cloud -f list_hosts my-vmware-config
1514     """
1515     if call != "function":
1516         raise SaltCloudSystemExit(
1517             "The list_hosts function must be called with -f or --function."
1518         )
1519     return {"Hosts": salt.utils.vmware.list_hosts(_get_si())}
1520 def list_resourcepools(kwargs=None, call=None):
1521     """
1522     List all the resource pools for this VMware environment
1523     CLI Example:
1524     .. code-block:: bash
1525         salt-cloud -f list_resourcepools my-vmware-config
1526     """
1527     if call != "function":
1528         raise SaltCloudSystemExit(
1529             "The list_resourcepools function must be called with -f or --function."
1530         )
1531     return {"Resource Pools": salt.utils.vmware.list_resourcepools(_get_si())}
1532 def list_networks(kwargs=None, call=None):
1533     """
1534     List all the standard networks for this VMware environment
1535     CLI Example:
1536     .. code-block:: bash
1537         salt-cloud -f list_networks my-vmware-config
1538     """
1539     if call != "function":
1540         raise SaltCloudSystemExit(
1541             "The list_networks function must be called with -f or --function."
1542         )
1543     return {"Networks": salt.utils.vmware.list_networks(_get_si())}
1544 def list_nodes_min(kwargs=None, call=None):
1545     """
1546     Return a list of all VMs and templates that are on the specified provider, with no details
1547     CLI Example:
1548     .. code-block:: bash
1549         salt-cloud -f list_nodes_min my-vmware-config
1550     """
1551     if call == "action":
1552         raise SaltCloudSystemExit(
1553             "The list_nodes_min function must be called with -f or --function."
1554         )
1555     ret = {}
1556     vm_properties = ["name"]
1557     vm_list = salt.utils.vmware.get_mors_with_properties(
1558         _get_si(), vim.VirtualMachine, vm_properties
1559     )
1560     for vm in vm_list:
1561         ret[vm["name"]] = {"state": "Running", "id": vm["name"]}
1562     return ret
1563 def list_nodes(kwargs=None, call=None):
1564     """
1565     Return a list of all VMs and templates that are on the specified provider, with basic fields
1566     CLI Example:
1567     .. code-block:: bash
1568         salt-cloud -f list_nodes my-vmware-config
1569     To return a list of all VMs and templates present on ALL configured providers, with basic
1570     fields:
1571     CLI Example:
1572     .. code-block:: bash
1573         salt-cloud -Q
1574     """
1575     if call == "action":
1576         raise SaltCloudSystemExit(
1577             "The list_nodes function must be called with -f or --function."
1578         )
1579     ret = {}
1580     vm_properties = [
1581         "name",
1582         "guest.ipAddress",
1583         "config.guestFullName",
1584         "config.hardware.numCPU",
1585         "config.hardware.memoryMB",
1586         "summary.runtime.powerState",
1587     ]
1588     vm_list = salt.utils.vmware.get_mors_with_properties(
1589         _get_si(), vim.VirtualMachine, vm_properties
1590     )
1591     for vm in vm_list:
1592         cpu = vm["config.hardware.numCPU"] if "config.hardware.numCPU" in vm else "N/A"
1593         ram = (
1594             "{} MB".format(vm["config.hardware.memoryMB"])
1595             if "config.hardware.memoryMB" in vm
1596             else "N/A"
1597         )
1598         vm_info = {
1599             "id": vm["name"],
1600             "image": "{} (Detected)".format(vm["config.guestFullName"])
1601             if "config.guestFullName" in vm
1602             else "N/A",
1603             "size": "cpu: {}\nram: {}".format(cpu, ram),
1604             "size_dict": {"cpu": cpu, "memory": ram},
1605             "state": str(vm["summary.runtime.powerState"])
1606             if "summary.runtime.powerState" in vm
1607             else "N/A",
1608             "private_ips": [vm["guest.ipAddress"]] if "guest.ipAddress" in vm else [],
1609             "public_ips": [],
1610         }
1611         ret[vm_info["id"]] = vm_info
1612     return ret
1613 def list_nodes_full(kwargs=None, call=None):
1614     """
1615     Return a list of all VMs and templates that are on the specified provider, with full details
1616     CLI Example:
1617     .. code-block:: bash
1618         salt-cloud -f list_nodes_full my-vmware-config
1619     To return a list of all VMs and templates present on ALL configured providers, with full
1620     details:
1621     CLI Example:
1622     .. code-block:: bash
1623         salt-cloud -F
1624     """
1625     if call == "action":
1626         raise SaltCloudSystemExit(
1627             "The list_nodes_full function must be called with -f or --function."
1628         )
1629     ret = {}
1630     vm_properties = [
1631         "config.hardware.device",
1632         "summary.storage.committed",
1633         "summary.storage.uncommitted",
1634         "summary.storage.unshared",
1635         "layoutEx.file",
1636         "config.guestFullName",
1637         "config.guestId",
1638         "guest.net",
1639         "config.hardware.memoryMB",
1640         "name",
1641         "config.hardware.numCPU",
1642         "config.files.vmPathName",
1643         "summary.runtime.powerState",
1644         "guest.toolsStatus",
1645     ]
1646     vm_list = salt.utils.vmware.get_mors_with_properties(
1647         _get_si(), vim.VirtualMachine, vm_properties
1648     )
1649     for vm in vm_list:
1650         ret[vm["name"]] = _format_instance_info(vm)
1651     return ret
1652 def list_nodes_select(call=None):
1653     """
1654     Return a list of all VMs and templates that are on the specified provider, with fields
1655     specified under ``query.selection`` in ``/etc/salt/cloud``
1656     CLI Example:
1657     .. code-block:: bash
1658         salt-cloud -f list_nodes_select my-vmware-config
1659     To return a list of all VMs and templates present on ALL configured providers, with
1660     fields specified under ``query.selection`` in ``/etc/salt/cloud``:
1661     CLI Example:
1662     .. code-block:: bash
1663         salt-cloud -S
1664     """
1665     if call == "action":
1666         raise SaltCloudSystemExit(
1667             "The list_nodes_select function must be called with -f or --function."
1668         )
1669     ret = {}
1670     vm_properties = []
1671     selection = __opts__.get("query.selection")
1672     if not selection:
1673         raise SaltCloudSystemExit("query.selection not found in /etc/salt/cloud")
1674     if "id" in selection:
1675         vm_properties.append("name")
1676     if "image" in selection:
1677         vm_properties.append("config.guestFullName")
1678     if "size" in selection:
1679         vm_properties.extend(["config.hardware.numCPU", "config.hardware.memoryMB"])
1680     if "state" in selection:
1681         vm_properties.append("summary.runtime.powerState")
1682     if "private_ips" in selection or "networks" in selection:
1683         vm_properties.append("guest.net")
1684     if (
1685         "devices" in selection
1686         or "mac_address" in selection
1687         or "mac_addresses" in selection
1688     ):
1689         vm_properties.append("config.hardware.device")
1690     if "storage" in selection:
1691         vm_properties.extend(
1692             [
1693                 "config.hardware.device",
1694                 "summary.storage.committed",
1695                 "summary.storage.uncommitted",
1696                 "summary.storage.unshared",
1697             ]
1698         )
1699     if "files" in selection:
1700         vm_properties.append("layoutEx.file")
1701     if "guest_id" in selection:
1702         vm_properties.append("config.guestId")
1703     if "hostname" in selection:
1704         vm_properties.append("guest.hostName")
1705     if "path" in selection:
1706         vm_properties.append("config.files.vmPathName")
1707     if "tools_status" in selection:
1708         vm_properties.append("guest.toolsStatus")
1709     if not vm_properties:
1710         return {}
1711     elif "name" not in vm_properties:
1712         vm_properties.append("name")
1713     vm_list = salt.utils.vmware.get_mors_with_properties(
1714         _get_si(), vim.VirtualMachine, vm_properties
1715     )
1716     for vm in vm_list:
1717         ret[vm["name"]] = _format_instance_info_select(vm, selection)
1718     return ret
1719 def show_instance(name, call=None):
1720     """
1721     List all available details of the specified VM
1722     CLI Example:
1723     .. code-block:: bash
1724         salt-cloud -a show_instance vmname
1725     """
1726     if call != "action":
1727         raise SaltCloudSystemExit(
1728             "The show_instance action must be called with -a or --action."
1729         )
1730     vm_properties = [
1731         "config.hardware.device",
1732         "summary.storage.committed",
1733         "summary.storage.uncommitted",
1734         "summary.storage.unshared",
1735         "layoutEx.file",
1736         "config.guestFullName",
1737         "config.guestId",
1738         "guest.net",
1739         "config.hardware.memoryMB",
1740         "name",
1741         "config.hardware.numCPU",
1742         "config.files.vmPathName",
1743         "summary.runtime.powerState",
1744         "guest.toolsStatus",
1745     ]
1746     vm_list = salt.utils.vmware.get_mors_with_properties(
1747         _get_si(), vim.VirtualMachine, vm_properties
1748     )
1749     for vm in vm_list:
1750         if vm["name"] == name:
1751             return _format_instance_info(vm)
1752     return {}
1753 def avail_images(call=None):
1754     """
1755     Return a list of all the templates present in this VMware environment with basic
1756     details
1757     CLI Example:
1758     .. code-block:: bash
1759         salt-cloud --list-images my-vmware-config
1760     """
1761     if call == "action":
1762         raise SaltCloudSystemExit(
1763             "The avail_images function must be called with "
1764             "-f or --function, or with the --list-images option."
1765         )
1766     templates = {}
1767     vm_properties = [
1768         "name",
1769         "config.template",
1770         "config.guestFullName",
1771         "config.hardware.numCPU",
1772         "config.hardware.memoryMB",
1773     ]
1774     vm_list = salt.utils.vmware.get_mors_with_properties(
1775         _get_si(), vim.VirtualMachine, vm_properties
1776     )
1777     for vm in vm_list:
1778         if "config.template" in vm and vm["config.template"]:
1779             templates[vm["name"]] = {
1780                 "name": vm["name"],
1781                 "guest_fullname": vm["config.guestFullName"]
1782                 if "config.guestFullName" in vm
1783                 else "N/A",
1784                 "cpus": vm["config.hardware.numCPU"]
1785                 if "config.hardware.numCPU" in vm
1786                 else "N/A",
1787                 "ram": vm["config.hardware.memoryMB"]
1788                 if "config.hardware.memoryMB" in vm
1789                 else "N/A",
1790             }
1791     return templates
1792 def avail_locations(call=None):
1793     """
1794     Return a list of all the available locations/datacenters in this VMware environment
1795     CLI Example:
1796     .. code-block:: bash
1797         salt-cloud --list-locations my-vmware-config
1798     """
1799     if call == "action":
1800         raise SaltCloudSystemExit(
1801             "The avail_locations function must be called with "
1802             "-f or --function, or with the --list-locations option."
1803         )
1804     return list_datacenters(call="function")
1805 def avail_sizes(call=None):
1806     """
1807     Return a list of all the available sizes in this VMware environment.
1808     CLI Example:
1809     .. code-block:: bash
1810         salt-cloud --list-sizes my-vmware-config
1811     .. note::
1812         Since sizes are built into templates, this function will return
1813         an empty dictionary.
1814     """
1815     if call == "action":
1816         raise SaltCloudSystemExit(
1817             "The avail_sizes function must be called with "
1818             "-f or --function, or with the --list-sizes option."
1819         )
1820     log.warning(
1821         "Because sizes are built into templates with VMware, there are no sizes "
1822         "to return."
1823     )
1824     return {}
1825 def list_templates(kwargs=None, call=None):
1826     """
1827     List all the templates present in this VMware environment
1828     CLI Example:
1829     .. code-block:: bash
1830         salt-cloud -f list_templates my-vmware-config
1831     """
1832     if call != "function":
1833         raise SaltCloudSystemExit(
1834             "The list_templates function must be called with -f or --function."
1835         )
1836     return {"Templates": avail_images(call="function")}
1837 def list_folders(kwargs=None, call=None):
1838     """
1839     List all the folders for this VMware environment
1840     CLI Example:
1841     .. code-block:: bash
1842         salt-cloud -f list_folders my-vmware-config
1843     """
1844     if call != "function":
1845         raise SaltCloudSystemExit(
1846             "The list_folders function must be called with -f or --function."
1847         )
1848     return {"Folders": salt.utils.vmware.list_folders(_get_si())}
1849 def list_snapshots(kwargs=None, call=None):
1850     """
1851     List snapshots either for all VMs and templates or for a specific VM/template
1852     in this VMware environment
1853     To list snapshots for all VMs and templates:
1854     CLI Example:
1855     .. code-block:: bash
1856         salt-cloud -f list_snapshots my-vmware-config
1857     To list snapshots for a specific VM/template:
1858     CLI Example:
1859     .. code-block:: bash
1860         salt-cloud -f list_snapshots my-vmware-config name="vmname"
1861     """
1862     if call != "function":
1863         raise SaltCloudSystemExit(
1864             "The list_snapshots function must be called with -f or --function."
1865         )
1866     ret = {}
1867     vm_properties = ["name", "rootSnapshot", "snapshot"]
1868     vm_list = salt.utils.vmware.get_mors_with_properties(
1869         _get_si(), vim.VirtualMachine, vm_properties
1870     )
1871     for vm in vm_list:
1872         if vm["rootSnapshot"]:
1873             if kwargs and kwargs.get("name") == vm["name"]:
1874                 return {vm["name"]: _get_snapshots(vm["snapshot"].rootSnapshotList)}
1875             else:
1876                 ret[vm["name"]] = _get_snapshots(vm["snapshot"].rootSnapshotList)
1877         else:
1878             if kwargs and kwargs.get("name") == vm["name"]:
1879                 return {}
1880     return ret
1881 def start(name, call=None):
1882     """
1883     To start/power on a VM using its name
1884     CLI Example:
1885     .. code-block:: bash
1886         salt-cloud -a start vmname
1887     """
1888     if call != "action":
1889         raise SaltCloudSystemExit(
1890             "The start action must be called with -a or --action."
1891         )
1892     vm_properties = ["name", "summary.runtime.powerState"]
1893     vm_list = salt.utils.vmware.get_mors_with_properties(
1894         _get_si(), vim.VirtualMachine, vm_properties
1895     )
1896     for vm in vm_list:
1897         if vm["name"] == name:
1898             if vm["summary.runtime.powerState"] == "poweredOn":
1899                 ret = "already powered on"
1900                 log.info("VM %s %s", name, ret)
1901                 return ret
1902             try:
1903                 log.info("Starting VM %s", name)
1904                 task = vm["object"].PowerOn()
1905                 salt.utils.vmware.wait_for_task(task, name, "power on")
1906             except Exception as exc:  # pylint: disable=broad-except
1907                 log.error(
1908                     "Error while powering on VM %s: %s",
1909                     name,
1910                     exc,
1911                     exc_info_on_loglevel=logging.DEBUG,
1912                 )
1913                 return "failed to power on"
1914     return "powered on"
1915 def stop(name, soft=False, call=None):
1916     """
1917     To stop/power off a VM using its name
1918     .. note::
1919         If ``soft=True`` then issues a command to the guest operating system
1920         asking it to perform a clean shutdown of all services.
1921         Default is soft=False
1922         For ``soft=True`` vmtools should be installed on guest system.
1923     CLI Example:
1924     .. code-block:: bash
1925         salt-cloud -a stop vmname
1926         salt-cloud -a stop vmname soft=True
1927     """
1928     if call != "action":
1929         raise SaltCloudSystemExit("The stop action must be called with -a or --action.")
1930     vm_properties = ["name", "summary.runtime.powerState"]
1931     vm_list = salt.utils.vmware.get_mors_with_properties(
1932         _get_si(), vim.VirtualMachine, vm_properties
1933     )
1934     for vm in vm_list:
1935         if vm["name"] == name:
1936             if vm["summary.runtime.powerState"] == "poweredOff":
1937                 ret = "already powered off"
1938                 log.info("VM %s %s", name, ret)
1939                 return ret
1940             try:
1941                 log.info("Stopping VM %s", name)
1942                 if soft:
1943                     vm["object"].ShutdownGuest()
1944                 else:
1945                     task = vm["object"].PowerOff()
1946                     salt.utils.vmware.wait_for_task(task, name, "power off")
1947             except Exception as exc:  # pylint: disable=broad-except
1948                 log.error(
1949                     "Error while powering off VM %s: %s",
1950                     name,
1951                     exc,
1952                     exc_info_on_loglevel=logging.DEBUG,
1953                 )
1954                 return "failed to power off"
1955     return "powered off"
1956 def suspend(name, call=None):
1957     """
1958     To suspend a VM using its name
1959     CLI Example:
1960     .. code-block:: bash
1961         salt-cloud -a suspend vmname
1962     """
1963     if call != "action":
1964         raise SaltCloudSystemExit(
1965             "The suspend action must be called with -a or --action."
1966         )
1967     vm_properties = ["name", "summary.runtime.powerState"]
1968     vm_list = salt.utils.vmware.get_mors_with_properties(
1969         _get_si(), vim.VirtualMachine, vm_properties
1970     )
1971     for vm in vm_list:
1972         if vm["name"] == name:
1973             if vm["summary.runtime.powerState"] == "poweredOff":
1974                 ret = "cannot suspend in powered off state"
1975                 log.info("VM %s %s", name, ret)
1976                 return ret
1977             elif vm["summary.runtime.powerState"] == "suspended":
1978                 ret = "already suspended"
1979                 log.info("VM %s %s", name, ret)
1980                 return ret
1981             try:
1982                 log.info("Suspending VM %s", name)
1983                 task = vm["object"].Suspend()
1984                 salt.utils.vmware.wait_for_task(task, name, "suspend")
1985             except Exception as exc:  # pylint: disable=broad-except
1986                 log.error(
1987                     "Error while suspending VM %s: %s",
1988                     name,
1989                     exc,
1990                     exc_info_on_loglevel=logging.DEBUG,
1991                 )
1992                 return "failed to suspend"
1993     return "suspended"
1994 def reset(name, soft=False, call=None):
1995     """
1996     To reset a VM using its name
1997     .. note::
1998         If ``soft=True`` then issues a command to the guest operating system
1999         asking it to perform a reboot. Otherwise hypervisor will terminate VM and start it again.
2000         Default is soft=False
2001         For ``soft=True`` vmtools should be installed on guest system.
2002     CLI Example:
2003     .. code-block:: bash
2004         salt-cloud -a reset vmname
2005         salt-cloud -a reset vmname soft=True
2006     """
2007     if call != "action":
2008         raise SaltCloudSystemExit(
2009             "The reset action must be called with -a or --action."
2010         )
2011     vm_properties = ["name", "summary.runtime.powerState"]
2012     vm_list = salt.utils.vmware.get_mors_with_properties(
2013         _get_si(), vim.VirtualMachine, vm_properties
2014     )
2015     for vm in vm_list:
2016         if vm["name"] == name:
2017             if (
2018                 vm["summary.runtime.powerState"] == "suspended"
2019                 or vm["summary.runtime.powerState"] == "poweredOff"
2020             ):
2021                 ret = "cannot reset in suspended/powered off state"
2022                 log.info("VM %s %s", name, ret)
2023                 return ret
2024             try:
2025                 log.info("Resetting VM %s", name)
2026                 if soft:
2027                     vm["object"].RebootGuest()
2028                 else:
2029                     task = vm["object"].ResetVM_Task()
2030                     salt.utils.vmware.wait_for_task(task, name, "reset")
2031             except Exception as exc:  # pylint: disable=broad-except
2032                 log.error(
2033                     "Error while resetting VM %s: %s",
2034                     name,
2035                     exc,
2036                     exc_info_on_loglevel=logging.DEBUG,
2037                 )
2038                 return "failed to reset"
2039     return "reset"
2040 def terminate(name, call=None):
2041     """
2042     To do an immediate power off of a VM using its name. A ``SIGKILL``
2043     is issued to the vmx process of the VM
2044     CLI Example:
2045     .. code-block:: bash
2046         salt-cloud -a terminate vmname
2047     """
2048     if call != "action":
2049         raise SaltCloudSystemExit(
2050             "The terminate action must be called with -a or --action."
2051         )
2052     vm_properties = ["name", "summary.runtime.powerState"]
2053     vm_list = salt.utils.vmware.get_mors_with_properties(
2054         _get_si(), vim.VirtualMachine, vm_properties
2055     )
2056     for vm in vm_list:
2057         if vm["name"] == name:
2058             if vm["summary.runtime.powerState"] == "poweredOff":
2059                 ret = "already powered off"
2060                 log.info("VM %s %s", name, ret)
2061                 return ret
2062             try:
2063                 log.info("Terminating VM %s", name)
2064                 vm["object"].Terminate()
2065             except Exception as exc:  # pylint: disable=broad-except
2066                 log.error(
2067                     "Error while terminating VM %s: %s",
2068                     name,
2069                     exc,
2070                     exc_info_on_loglevel=logging.DEBUG,
2071                 )
2072                 return "failed to terminate"
2073     return "terminated"
2074 def destroy(name, call=None):
2075     """
2076     To destroy a VM from the VMware environment
2077     CLI Example:
2078     .. code-block:: bash
2079         salt-cloud -d vmname
2080         salt-cloud --destroy vmname
2081         salt-cloud -a destroy vmname
2082     """
2083     if call == "function":
2084         raise SaltCloudSystemExit(
2085             "The destroy action must be called with -d, --destroy, -a or --action."
2086         )
2087     __utils__["cloud.fire_event"](
2088         "event",
2089         "destroying instance",
2090         "salt/cloud/{}/destroying".format(name),
2091         args={"name": name},
2092         sock_dir=__opts__["sock_dir"],
2093         transport=__opts__["transport"],
2094     )
2095     vm_properties = ["name", "summary.runtime.powerState"]
2096     vm_list = salt.utils.vmware.get_mors_with_properties(
2097         _get_si(), vim.VirtualMachine, vm_properties
2098     )
2099     for vm in vm_list:
2100         if vm["name"] == name:
2101             if vm["summary.runtime.powerState"] != "poweredOff":
2102                 try:
2103                     log.info("Powering Off VM %s", name)
2104                     task = vm["object"].PowerOff()
2105                     salt.utils.vmware.wait_for_task(task, name, "power off")
2106                 except Exception as exc:  # pylint: disable=broad-except
2107                     log.error(
2108                         "Error while powering off VM %s: %s",
2109                         name,
2110                         exc,
2111                         exc_info_on_loglevel=logging.DEBUG,
2112                     )
2113                     return "failed to destroy"
2114             try:
2115                 log.info("Destroying VM %s", name)
2116                 task = vm["object"].Destroy_Task()
2117                 salt.utils.vmware.wait_for_task(task, name, "destroy")
2118             except Exception as exc:  # pylint: disable=broad-except
2119                 log.error(
2120                     "Error while destroying VM %s: %s",
2121                     name,
2122                     exc,
2123                     exc_info_on_loglevel=logging.DEBUG,
2124                 )
2125                 return "failed to destroy"
2126     __utils__["cloud.fire_event"](
2127         "event",
2128         "destroyed instance",
2129         "salt/cloud/{}/destroyed".format(name),
2130         args={"name": name},
2131         sock_dir=__opts__["sock_dir"],
2132         transport=__opts__["transport"],
2133     )
2134     if __opts__.get("update_cachedir", False) is True:
2135         __utils__["cloud.delete_minion_cachedir"](
2136             name, _get_active_provider_name().split(":")[0], __opts__
2137         )
2138     return True
2139 def create(vm_):
2140     """
2141     To create a single VM in the VMware environment.
2142     Sample profile and arguments that can be specified in it can be found
2143     :ref:`here. &lt;vmware-cloud-profile&gt;`
2144     CLI Example:
2145     .. code-block:: bash
2146         salt-cloud -p vmware-centos6.5 vmname
2147     """
2148     try:
2149         if (
2150             vm_["profile"]
2151             and config.is_profile_configured(
2152                 __opts__,
2153                 _get_active_provider_name() or "vmware",
2154                 vm_["profile"],
2155                 vm_=vm_,
2156             )
2157             is False
2158         ):
2159             return False
2160     except AttributeError:
2161         pass
2162     __utils__["cloud.fire_event"](
2163         "event",
2164         "starting create",
2165         "salt/cloud/{}/creating".format(vm_["name"]),
2166         args=__utils__["cloud.filter_event"](
2167             "creating", vm_, ["name", "profile", "provider", "driver"]
2168         ),
2169         sock_dir=__opts__["sock_dir"],
2170         transport=__opts__["transport"],
2171     )
2172     vm_name = config.get_cloud_config_value("name", vm_, __opts__, default=None)
2173     folder = config.get_cloud_config_value("folder", vm_, __opts__, default=None)
2174     datacenter = config.get_cloud_config_value(
2175         "datacenter", vm_, __opts__, default=None
2176     )
2177     resourcepool = config.get_cloud_config_value(
2178         "resourcepool", vm_, __opts__, default=None
2179     )
2180     cluster = config.get_cloud_config_value("cluster", vm_, __opts__, default=None)
2181     datastore = config.get_cloud_config_value("datastore", vm_, __opts__, default=None)
2182     host = config.get_cloud_config_value("host", vm_, __opts__, default=None)
2183     template = config.get_cloud_config_value("template", vm_, __opts__, default=False)
2184     num_cpus = config.get_cloud_config_value("num_cpus", vm_, __opts__, default=None)
2185     cores_per_socket = config.get_cloud_config_value(
2186         "cores_per_socket", vm_, __opts__, default=None
2187     )
2188     instant_clone = config.get_cloud_config_value(
2189         "instant_clone", vm_, __opts__, default=False
2190     )
2191     memory = config.get_cloud_config_value("memory", vm_, __opts__, default=None)
2192     devices = config.get_cloud_config_value("devices", vm_, __opts__, default=None)
2193     extra_config = config.get_cloud_config_value(
2194         "extra_config", vm_, __opts__, default=None
2195     )
2196     annotation = config.get_cloud_config_value(
2197         "annotation", vm_, __opts__, default=None
2198     )
2199     power = config.get_cloud_config_value("power_on", vm_, __opts__, default=True)
2200     key_filename = config.get_cloud_config_value(
2201         "private_key", vm_, __opts__, search_global=False, default=None
2202     )
2203     deploy = config.get_cloud_config_value(
2204         "deploy", vm_, __opts__, search_global=True, default=True
2205     )
2206     wait_for_ip_timeout = config.get_cloud_config_value(
2207         "wait_for_ip_timeout", vm_, __opts__, default=20 * 60
2208     )
2209     domain = config.get_cloud_config_value(
2210         "domain", vm_, __opts__, search_global=False, default="local"
2211     )
2212     hardware_version = config.get_cloud_config_value(
2213         "hardware_version", vm_, __opts__, search_global=False, default=None
2214     )
2215     guest_id = config.get_cloud_config_value(
2216         "image", vm_, __opts__, search_global=False, default=None
2217     )
2218     customization = config.get_cloud_config_value(
2219         "customization", vm_, __opts__, search_global=False, default=True
2220     )
2221     customization_spec = config.get_cloud_config_value(
2222         "customization_spec", vm_, __opts__, search_global=False, default=None
2223     )
2224     win_password = config.get_cloud_config_value(
2225         "win_password", vm_, __opts__, search_global=False, default=None
2226     )
2227     win_organization_name = config.get_cloud_config_value(
2228         "win_organization_name",
2229         vm_,
2230         __opts__,
2231         search_global=False,
2232         default="Organization",
2233     )
2234     plain_text = config.get_cloud_config_value(
2235         "plain_text", vm_, __opts__, search_global=False, default=False
2236     )
2237     win_user_fullname = config.get_cloud_config_value(
2238         "win_user_fullname", vm_, __opts__, search_global=False, default="Windows User"
2239     )
2240     win_run_once = config.get_cloud_config_value(
2241         "win_run_once", vm_, __opts__, search_global=False, default=None
2242     )
2243     cpu_hot_add = config.get_cloud_config_value(
2244         "cpu_hot_add", vm_, __opts__, search_global=False, default=None
2245     )
2246     cpu_hot_remove = config.get_cloud_config_value(
2247         "cpu_hot_remove", vm_, __opts__, search_global=False, default=None
2248     )
2249     mem_hot_add = config.get_cloud_config_value(
2250         "mem_hot_add", vm_, __opts__, search_global=False, default=None
2251     )
2252     nested_hv = config.get_cloud_config_value(
2253         "nested_hv", vm_, __opts__, search_global=False, default=None
2254     )
2255     vpmc = config.get_cloud_config_value(
2256         "vpmc", vm_, __opts__, search_global=False, default=None
2257     )
2258     si = _get_si()
2259     container_ref = None
2260     if datacenter:
2261         datacenter_ref = salt.utils.vmware.get_mor_by_property(
2262             _get_si(), vim.Datacenter, datacenter
2263         )
2264         container_ref = datacenter_ref if datacenter_ref else None
2265     if "clonefrom" in vm_:
2266         if datacenter:
2267             datacenter_ref = salt.utils.vmware.get_mor_by_property(
2268                 si, vim.Datacenter, datacenter
2269             )
2270             container_ref = datacenter_ref if datacenter_ref else None
2271         object_ref = salt.utils.vmware.get_mor_by_property(
2272             si, vim.VirtualMachine, vm_["clonefrom"], container_ref=container_ref
2273         )
2274         if object_ref:
2275             clone_type = "template" if object_ref.config.template else "vm"
2276         else:
2277             raise SaltCloudSystemExit(
2278                 "The VM/template that you have specified under clonefrom does not"
2279                 " exist."
2280             )
2281     else:
2282         clone_type = None
2283         object_ref = None
2284     if resourcepool:
2285         resourcepool_ref = salt.utils.vmware.get_mor_by_property(
2286             si, vim.ResourcePool, resourcepool, container_ref=container_ref
2287         )
2288         if not resourcepool_ref:
2289             log.error("Specified resource pool: '%s' does not exist", resourcepool)
2290             if not clone_type or clone_type == "template":
2291                 raise SaltCloudSystemExit(
2292                     "You must specify a resource pool that exists."
2293                 )
2294     elif cluster:
2295         cluster_ref = salt.utils.vmware.get_mor_by_property(
2296             si, vim.ClusterComputeResource, cluster, container_ref=container_ref
2297         )
2298         if not cluster_ref:
2299             log.error("Specified cluster: '%s' does not exist", cluster)
2300             if not clone_type or clone_type == "template":
2301                 raise SaltCloudSystemExit("You must specify a cluster that exists.")
2302         else:
2303             resourcepool_ref = cluster_ref.resourcePool
2304     elif clone_type == "template":
2305         raise SaltCloudSystemExit(
2306             "You must either specify a cluster or a resource pool when cloning from a"
2307             " template."
2308         )
2309     elif not clone_type:
2310         raise SaltCloudSystemExit(
2311             "You must either specify a cluster or a resource pool when creating."
2312         )
2313     else:
2314         log.debug("Using resource pool used by the %s %s", clone_type, vm_["clonefrom"])
2315     if folder:
2316         folder_parts = folder.split("/")
2317         search_reference = container_ref
2318         for folder_part in folder_parts:
2319             if folder_part:
2320                 folder_ref = salt.utils.vmware.get_mor_by_property(
2321                     si, vim.Folder, folder_part, container_ref=search_reference
2322                 )
2323                 search_reference = folder_ref
2324         if not folder_ref:
2325             log.error("Specified folder: '%s' does not exist", folder)
2326             log.debug(
2327                 "Using folder in which %s %s is present", clone_type, vm_["clonefrom"]
2328             )
2329             folder_ref = object_ref.parent
2330     elif datacenter:
2331         if not datacenter_ref:
2332             log.error("Specified datacenter: '%s' does not exist", datacenter)
2333             log.debug(
2334                 "Using datacenter folder in which %s %s is present",
2335                 clone_type,
2336                 vm_["clonefrom"],
2337             )
2338             folder_ref = object_ref.parent
2339         else:
2340             folder_ref = datacenter_ref.vmFolder
2341     elif not clone_type:
2342         raise SaltCloudSystemExit(
2343             "You must either specify a folder or a datacenter when creating not"
2344             " cloning."
2345         )
2346     else:
2347         log.debug(
2348             "Using folder in which %s %s is present", clone_type, vm_["clonefrom"]
2349         )
2350         folder_ref = object_ref.parent
2351     if "clonefrom" in vm_:
2352         reloc_spec = vim.vm.RelocateSpec()
2353         if (resourcepool and resourcepool_ref) or (cluster and cluster_ref):
2354             reloc_spec.pool = resourcepool_ref
2355         if datastore:
2356             datastore_ref = salt.utils.vmware.get_mor_by_property(
2357                 si, vim.Datastore, datastore, container_ref=container_ref
2358             )
2359             if datastore_ref:
2360                 reloc_spec.datastore = datastore_ref
2361             else:
2362                 datastore_cluster_ref = salt.utils.vmware.get_mor_by_property(
2363                     si, vim.StoragePod, datastore, container_ref=container_ref
2364                 )
2365                 if not datastore_cluster_ref:
2366                     log.error(
2367                         "Specified datastore/datastore cluster: '%s' does not exist",
2368                         datastore,
2369                     )
2370                     log.debug(
2371                         "Using datastore used by the %s %s",
2372                         clone_type,
2373                         vm_["clonefrom"],
2374                     )
2375         else:
2376             log.debug("No datastore/datastore cluster specified")
2377             log.debug("Using datastore used by the %s %s", clone_type, vm_["clonefrom"])
2378         if host:
2379             host_ref = salt.utils.vmware.get_mor_by_property(
2380                 si, vim.HostSystem, host, container_ref=container_ref
2381             )
2382             if host_ref:
2383                 reloc_spec.host = host_ref
2384             else:
2385                 log.error("Specified host: '%s' does not exist", host)
2386         if instant_clone:
2387             instant_clone_spec = vim.vm.InstantCloneSpec()
2388             instant_clone_spec.name = vm_name
2389             instant_clone_spec.location = reloc_spec
2390             event_kwargs = vm_.copy()
2391             if event_kwargs.get("password"):
2392                 del event_kwargs["password"]
2393             try:
2394                 __utils__["cloud.fire_event"](
2395                     "event",
2396                     "requesting instance",
2397                     "salt/cloud/{}/requesting".format(vm_["name"]),
2398                     args=__utils__["cloud.filter_event"](
2399                         "requesting", event_kwargs, list(event_kwargs)
2400                     ),
2401                     sock_dir=__opts__["sock_dir"],
2402                     transport=__opts__["transport"],
2403                 )
2404                 log.info(
2405                     "Creating %s from %s(%s)", vm_["name"], clone_type, vm_["clonefrom"]
2406                 )
2407                 if datastore and not datastore_ref and datastore_cluster_ref:
2408                     pod_spec = vim.storageDrs.PodSelectionSpec(
2409                         storagePod=datastore_cluster_ref
2410                     )
2411                     storage_spec = vim.storageDrs.StoragePlacementSpec(
2412                         type="clone",
2413                         vm=object_ref,
2414                         podSelectionSpec=pod_spec,
2415                         cloneName=vm_name,
2416                         folder=folder_ref,
2417                     )
2418                     recommended_datastores = (
2419                         si.content.storageResourceManager.RecommendDatastores(
2420                             storageSpec=storage_spec
2421                         )
2422                     )
2423                     task = si.content.storageResourceManager.ApplyStorageDrsRecommendation_Task(
2424                         recommended_datastores.recommendations[0].key
2425                     )
2426                     salt.utils.vmware.wait_for_task(
2427                         task, vm_name, "apply storage DRS recommendations", 5, "info"
2428                     )
2429                 else:
2430                     task = object_ref.InstantClone_Task(spec=instant_clone_spec)
2431                     salt.utils.vmware.wait_for_task(
2432                         task, vm_name, "Instantclone", 5, "info"
2433                     )
2434             except Exception as exc:  # pylint: disable=broad-except
2435                 err_msg = "Error Instant cloning {}: {}".format(vm_["name"], exc)
2436                 log.error(
2437                     err_msg,
2438                     exc_info_on_loglevel=logging.DEBUG,
2439                 )
2440                 return {"Error": err_msg}
2441             new_vm_ref = salt.utils.vmware.get_mor_by_property(
2442                 si, vim.VirtualMachine, vm_name, container_ref=container_ref
2443             )
2444             out = None
2445             if not template and power:
2446                 ip = _wait_for_ip(new_vm_ref, wait_for_ip_timeout)
2447                 if ip:
2448                     log.info("[ %s ] IPv4 is: %s", vm_name, ip)
2449                     if deploy:
2450                         vm_["key_filename"] = key_filename
2451                         if "ssh_host" not in vm_:
2452                             vm_["ssh_host"] = ip
2453                         log.info("[ %s ] Deploying to %s", vm_name, vm_["ssh_host"])
2454                         out = __utils__["cloud.bootstrap"](vm_, __opts__)
2455             data = show_instance(vm_name, call="action")
2456             if deploy and isinstance(out, dict):
2457                 data["deploy_kwargs"] = out.get("deploy_kwargs", {})
2458             __utils__["cloud.fire_event"](
2459                 "event",
2460                 "created instance",
2461                 "salt/cloud/{}/created".format(vm_["name"]),
2462                 args=__utils__["cloud.filter_event"](
2463                     "created", vm_, ["name", "profile", "provider", "driver"]
2464                 ),
2465                 sock_dir=__opts__["sock_dir"],
2466                 transport=__opts__["transport"],
2467             )
2468             return {"Instant Clone created successfully": data}
2469     else:
2470         if not datastore:
2471             raise SaltCloudSystemExit(
2472                 "You must specify a datastore when creating not cloning."
2473             )
2474         else:
2475             datastore_ref = salt.utils.vmware.get_mor_by_property(
2476                 si, vim.Datastore, datastore
2477             )
2478             if not datastore_ref:
2479                 raise SaltCloudSystemExit(
2480                     "Specified datastore: '{}' does not exist".format(datastore)
2481                 )
2482         if host:
2483             host_ref = salt.utils.vmware.get_mor_by_property(
2484                 _get_si(), vim.HostSystem, host, container_ref=container_ref
2485             )
2486             if not host_ref:
2487                 log.error("Specified host: '%s' does not exist", host)
2488     config_spec = vim.vm.ConfigSpec()
2489     if hardware_version and object_ref is not None:
2490         hardware_version = "vmx-{:02}".format(hardware_version)
2491         if hardware_version != object_ref.config.version:
2492             log.debug(
2493                 "Scheduling hardware version upgrade from %s to %s",
2494                 object_ref.config.version,
2495                 hardware_version,
2496             )
2497             scheduled_hardware_upgrade = vim.vm.ScheduledHardwareUpgradeInfo()
2498             scheduled_hardware_upgrade.upgradePolicy = "always"
2499             scheduled_hardware_upgrade.versionKey = hardware_version
2500             config_spec.scheduledHardwareUpgradeInfo = scheduled_hardware_upgrade
2501         else:
2502             log.debug("Virtual hardware version already set to %s", hardware_version)
2503     if num_cpus:
2504         log.debug("Setting cpu to: %s", num_cpus)
2505         config_spec.numCPUs = int(num_cpus)
2506     if cores_per_socket:
2507         log.debug("Setting cores per socket to: %s", cores_per_socket)
2508         config_spec.numCoresPerSocket = int(cores_per_socket)
2509     if memory:
2510         try:
2511             memory_num, memory_unit = re.findall(r"[^\W\d_]+|\d+.\d+|\d+", memory)
2512             if memory_unit.lower() == "mb":
2513                 memory_mb = int(memory_num)
2514             elif memory_unit.lower() == "gb":
2515                 memory_mb = int(float(memory_num) * 1024.0)
2516             else:
2517                 err_msg = "Invalid memory type specified: '{}'".format(memory_unit)
2518                 log.error(err_msg)
2519                 return {"Error": err_msg}
2520         except (TypeError, ValueError):
2521             memory_mb = int(memory)
2522         log.debug("Setting memory to: %s MB", memory_mb)
2523         config_spec.memoryMB = memory_mb
2524     if devices:
2525         specs = _manage_devices(
2526             devices, vm=object_ref, container_ref=container_ref, new_vm_name=vm_name
2527         )
2528         config_spec.deviceChange = specs["device_specs"]
2529     if cpu_hot_add and hasattr(config_spec, "cpuHotAddEnabled"):
2530         config_spec.cpuHotAddEnabled = bool(cpu_hot_add)
2531     if cpu_hot_remove and hasattr(config_spec, "cpuHotRemoveEnabled"):
2532         config_spec.cpuHotRemoveEnabled = bool(cpu_hot_remove)
2533     if mem_hot_add and hasattr(config_spec, "memoryHotAddEnabled"):
2534         config_spec.memoryHotAddEnabled = bool(mem_hot_add)
2535     if nested_hv and hasattr(config_spec, "nestedHVEnabled"):
2536         config_spec.nestedHVEnabled = bool(nested_hv)
2537     if vpmc and hasattr(config_spec, "vPMCEnabled"):
2538         config_spec.vPMCEnabled = bool(vpmc)
2539     if extra_config:
2540         for key, value in extra_config.items():
2541             option = vim.option.OptionValue(key=key, value=value)
2542             config_spec.extraConfig.append(option)
2543     if annotation:
2544         config_spec.annotation = str(annotation)
2545     if "clonefrom" in vm_:
2546         clone_spec = handle_snapshot(config_spec, object_ref, reloc_spec, template, vm_)
2547         if not clone_spec:
2548             clone_spec = build_clonespec(config_spec, object_ref, reloc_spec, template)
2549         if customization and customization_spec:
2550             customization_spec = salt.utils.vmware.get_customizationspec_ref(
2551                 si=si, customization_spec_name=customization_spec
2552             )
2553             clone_spec.customization = customization_spec.spec
2554         elif customization and (devices and "network" in list(devices.keys())):
2555             global_ip = vim.vm.customization.GlobalIPSettings()
2556             if "dns_servers" in list(vm_.keys()):
2557                 global_ip.dnsServerList = vm_["dns_servers"]
2558             if "domain" in list(vm_.keys()):
2559                 global_ip.dnsSuffixList = vm_["domain"]
2560             non_hostname_chars = re.compile(r"[^\w-]")
2561             if re.search(non_hostname_chars, vm_name):
2562                 host_name = re.split(non_hostname_chars, vm_name, maxsplit=1)[0]
2563                 domain_name = re.split(non_hostname_chars, vm_name, maxsplit=1)[-1]
2564             else:
2565                 host_name = vm_name
2566                 domain_name = domain
2567             if "Windows" not in object_ref.config.guestFullName:
2568                 identity = vim.vm.customization.LinuxPrep()
2569                 identity.hostName = vim.vm.customization.FixedName(name=host_name)
2570                 identity.domain = domain_name
2571             else:
2572                 identity = vim.vm.customization.Sysprep()
2573                 identity.guiUnattended = vim.vm.customization.GuiUnattended()
2574                 identity.guiUnattended.autoLogon = True
2575                 identity.guiUnattended.autoLogonCount = 1
2576                 identity.guiUnattended.password = vim.vm.customization.Password()
2577                 identity.guiUnattended.password.value = win_password
2578                 identity.guiUnattended.password.plainText = plain_text
2579                 if win_run_once:
2580                     identity.guiRunOnce = vim.vm.customization.GuiRunOnce()
2581                     identity.guiRunOnce.commandList = win_run_once
2582                 identity.userData = vim.vm.customization.UserData()
2583                 identity.userData.fullName = win_user_fullname
2584                 identity.userData.orgName = win_organization_name
2585                 identity.userData.computerName = vim.vm.customization.FixedName()
2586                 identity.userData.computerName.name = host_name
2587                 identity.identification = vim.vm.customization.Identification()
2588             custom_spec = vim.vm.customization.Specification(
2589                 globalIPSettings=global_ip,
2590                 identity=identity,
2591                 nicSettingMap=specs["nics_map"],
2592             )
2593             clone_spec.customization = custom_spec
2594         if not template:
2595             clone_spec.powerOn = power
2596         log.debug("clone_spec set to:\n%s", pprint.pformat(clone_spec))
2597     else:
2598         config_spec.name = vm_name
2599         config_spec.files = vim.vm.FileInfo()
2600         config_spec.files.vmPathName = "[{0}] {1}/{1}.vmx".format(datastore, vm_name)
2601         config_spec.guestId = guest_id
2602         log.debug("config_spec set to:\n%s", pprint.pformat(config_spec))
2603     event_kwargs = vm_.copy()
2604     if event_kwargs.get("password"):
2605         del event_kwargs["password"]
2606     try:
2607         __utils__["cloud.fire_event"](
2608             "event",
2609             "requesting instance",
2610             "salt/cloud/{}/requesting".format(vm_["name"]),
2611             args=__utils__["cloud.filter_event"](
2612                 "requesting", event_kwargs, list(event_kwargs)
2613             ),
2614             sock_dir=__opts__["sock_dir"],
2615             transport=__opts__["transport"],
2616         )
2617         if "clonefrom" in vm_:
2618             log.info(
2619                 "Creating %s from %s(%s)", vm_["name"], clone_type, vm_["clonefrom"]
2620             )
2621             if datastore and not datastore_ref and datastore_cluster_ref:
2622                 pod_spec = vim.storageDrs.PodSelectionSpec(
2623                     storagePod=datastore_cluster_ref
2624                 )
2625                 storage_spec = vim.storageDrs.StoragePlacementSpec(
2626                     type="clone",
2627                     vm=object_ref,
2628                     podSelectionSpec=pod_spec,
2629                     cloneSpec=clone_spec,
2630                     cloneName=vm_name,
2631                     folder=folder_ref,
2632                 )
2633                 recommended_datastores = (
2634                     si.content.storageResourceManager.RecommendDatastores(
2635                         storageSpec=storage_spec
2636                     )
2637                 )
2638                 task = si.content.storageResourceManager.ApplyStorageDrsRecommendation_Task(
2639                     recommended_datastores.recommendations[0].key
2640                 )
2641                 salt.utils.vmware.wait_for_task(
2642                     task, vm_name, "apply storage DRS recommendations", 5, "info"
2643                 )
2644             else:
2645                 task = object_ref.Clone(folder_ref, vm_name, clone_spec)
2646                 salt.utils.vmware.wait_for_task(task, vm_name, "clone", 5, "info")
2647         else:
2648             log.info("Creating %s", vm_["name"])
2649             if host:
2650                 task = folder_ref.CreateVM_Task(config_spec, resourcepool_ref, host_ref)
2651             else:
2652                 task = folder_ref.CreateVM_Task(config_spec, resourcepool_ref)
2653             salt.utils.vmware.wait_for_task(task, vm_name, "create", 15, "info")
2654     except Exception as exc:  # pylint: disable=broad-except
2655         err_msg = "Error creating {}: {}".format(vm_["name"], exc)
2656         log.error(
2657             err_msg,
2658             exc_info_on_loglevel=logging.DEBUG,
2659         )
2660         return {"Error": err_msg}
2661     new_vm_ref = salt.utils.vmware.get_mor_by_property(
2662         si, vim.VirtualMachine, vm_name, container_ref=container_ref
2663     )
2664     try:
2665         if not clone_type and power:
2666             task = new_vm_ref.PowerOn()
2667             salt.utils.vmware.wait_for_task(task, vm_name, "power", 5, "info")
2668     except Exception as exc:  # pylint: disable=broad-except
2669         log.info("Powering on the VM threw this exception. Ignoring.")
2670         log.info(exc)
2671     out = None
2672     if not template and power:
2673         ip = _wait_for_ip(new_vm_ref, wait_for_ip_timeout)
2674         if ip:
2675             log.info("[ %s ] IPv4 is: %s", vm_name, ip)
2676             if deploy:
2677                 vm_["key_filename"] = key_filename
2678                 if "ssh_host" not in vm_:
2679                     vm_["ssh_host"] = ip
2680                 log.info("[ %s ] Deploying to %s", vm_name, vm_["ssh_host"])
2681                 out = __utils__["cloud.bootstrap"](vm_, __opts__)
2682     data = show_instance(vm_name, call="action")
2683     if deploy and isinstance(out, dict):
2684         data["deploy_kwargs"] = out.get("deploy_kwargs", {})
2685     __utils__["cloud.fire_event"](
2686         "event",
2687         "created instance",
2688         "salt/cloud/{}/created".format(vm_["name"]),
2689         args=__utils__["cloud.filter_event"](
2690             "created", vm_, ["name", "profile", "provider", "driver"]
2691         ),
2692         sock_dir=__opts__["sock_dir"],
2693         transport=__opts__["transport"],
2694     )
2695     return data
2696 def handle_snapshot(config_spec, object_ref, reloc_spec, template, vm_):
2697     """
2698     Returns a clone spec for cloning from shapshots
2699     :rtype vim.vm.CloneSpec
2700     """
2701     if "snapshot" not in vm_:
2702         return None
2703     allowed_types = [
2704         FLATTEN_DISK_FULL_CLONE,
2705         COPY_ALL_DISKS_FULL_CLONE,
2706         CURRENT_STATE_LINKED_CLONE,
2707         QUICK_LINKED_CLONE,
2708     ]
2709     clone_spec = get_clonespec_for_valid_snapshot(
2710         config_spec, object_ref, reloc_spec, template, vm_
2711     )
2712     if not clone_spec:
2713         raise SaltCloudSystemExit(
2714             "Invalid disk move type specified supported types are {}".format(
2715                 " ".join(allowed_types)
2716             )
2717         )
2718     return clone_spec
2719 def get_clonespec_for_valid_snapshot(
2720     config_spec, object_ref, reloc_spec, template, vm_
2721 ):
2722     """
2723     return clonespec only if values are valid
2724     """
2725     moving = True
2726     if QUICK_LINKED_CLONE == vm_["snapshot"]["disk_move_type"]:
2727         reloc_spec.diskMoveType = QUICK_LINKED_CLONE
2728     elif CURRENT_STATE_LINKED_CLONE == vm_["snapshot"]["disk_move_type"]:
2729         reloc_spec.diskMoveType = CURRENT_STATE_LINKED_CLONE
2730     elif COPY_ALL_DISKS_FULL_CLONE == vm_["snapshot"]["disk_move_type"]:
2731         reloc_spec.diskMoveType = COPY_ALL_DISKS_FULL_CLONE
2732     elif FLATTEN_DISK_FULL_CLONE == vm_["snapshot"]["disk_move_type"]:
2733         reloc_spec.diskMoveType = FLATTEN_DISK_FULL_CLONE
2734     else:
2735         moving = False
2736     if moving:
2737         return build_clonespec(config_spec, object_ref, reloc_spec, template)
2738     return None
2739 def build_clonespec(config_spec, object_ref, reloc_spec, template):
2740     """
2741     Returns the clone spec
2742     """
2743     if reloc_spec.diskMoveType == QUICK_LINKED_CLONE:
2744         return vim.vm.CloneSpec(
2745             template=template,
2746             location=reloc_spec,
2747             config=config_spec,
2748             snapshot=object_ref.snapshot.currentSnapshot,
2749         )
2750     return vim.vm.CloneSpec(template=template, location=reloc_spec, config=config_spec)
2751 def create_datacenter(kwargs=None, call=None):
2752     """
2753     Create a new data center in this VMware environment
2754     CLI Example:
2755     .. code-block:: bash
2756         salt-cloud -f create_datacenter my-vmware-config name="MyNewDatacenter"
2757     """
2758     if call != "function":
2759         raise SaltCloudSystemExit(
2760             "The create_datacenter function must be called with -f or --function."
2761         )
2762     datacenter_name = kwargs.get("name") if kwargs and "name" in kwargs else None
2763     if not datacenter_name:
2764         raise SaltCloudSystemExit(
2765             "You must specify name of the new datacenter to be created."
2766         )
2767     if not datacenter_name or len(datacenter_name) &gt;= 80:
2768         raise SaltCloudSystemExit(
2769             "The datacenter name must be a non empty string of less than 80 characters."
2770         )
2771     si = _get_si()
2772     datacenter_ref = salt.utils.vmware.get_mor_by_property(
2773         si, vim.Datacenter, datacenter_name
2774     )
2775     if datacenter_ref:
2776         return {datacenter_name: "datacenter already exists"}
2777     folder = si.content.rootFolder
2778     if isinstance(folder, vim.Folder):
2779         try:
2780             folder.CreateDatacenter(name=datacenter_name)
2781         except Exception as exc:  # pylint: disable=broad-except
2782             log.error(
2783                 "Error creating datacenter %s: %s",
2784                 datacenter_name,
2785                 exc,
2786                 exc_info_on_loglevel=logging.DEBUG,
2787             )
2788             return False
2789         log.debug("Created datacenter %s", datacenter_name)
2790         return {datacenter_name: "created"}
2791     return False
2792 def create_cluster(kwargs=None, call=None):
2793     """
2794     Create a new cluster under the specified datacenter in this VMware environment
2795     CLI Example:
2796     .. code-block:: bash
2797         salt-cloud -f create_cluster my-vmware-config name="myNewCluster" datacenter="datacenterName"
2798     """
2799     if call != "function":
2800         raise SaltCloudSystemExit(
2801             "The create_cluster function must be called with -f or --function."
2802         )
2803     cluster_name = kwargs.get("name") if kwargs and "name" in kwargs else None
2804     datacenter = kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
2805     if not cluster_name:
2806         raise SaltCloudSystemExit(
2807             "You must specify name of the new cluster to be created."
2808         )
2809     if not datacenter:
2810         raise SaltCloudSystemExit(
2811             "You must specify name of the datacenter where the cluster should be"
2812             " created."
2813         )
2814     si = _get_si()
2815     if not isinstance(datacenter, vim.Datacenter):
2816         datacenter = salt.utils.vmware.get_mor_by_property(
2817             si, vim.Datacenter, datacenter
2818         )
2819         if not datacenter:
2820             raise SaltCloudSystemExit("The specified datacenter does not exist.")
2821     cluster_ref = salt.utils.vmware.get_mor_by_property(
2822         si, vim.ClusterComputeResource, cluster_name
2823     )
2824     if cluster_ref:
2825         return {cluster_name: "cluster already exists"}
2826     cluster_spec = vim.cluster.ConfigSpecEx()
2827     folder = datacenter.hostFolder
2828     if isinstance(folder, vim.Folder):
2829         try:
2830             folder.CreateClusterEx(name=cluster_name, spec=cluster_spec)
2831         except Exception as exc:  # pylint: disable=broad-except
2832             log.error(
2833                 "Error creating cluster %s: %s",
2834                 cluster_name,
2835                 exc,
2836                 exc_info_on_loglevel=logging.DEBUG,
2837             )
2838             return False
2839         log.debug(
2840             "Created cluster %s under datacenter %s", cluster_name, datacenter.name
2841         )
2842         return {cluster_name: "created"}
2843     return False
2844 def rescan_hba(kwargs=None, call=None):
2845     """
2846     To rescan a specified HBA or all the HBAs on the Host System
2847     CLI Example:
2848     .. code-block:: bash
2849         salt-cloud -f rescan_hba my-vmware-config host="hostSystemName"
2850         salt-cloud -f rescan_hba my-vmware-config hba="hbaDeviceName" host="hostSystemName"
2851     """
2852     if call != "function":
2853         raise SaltCloudSystemExit(
2854             "The rescan_hba function must be called with -f or --function."
2855         )
2856     hba = kwargs.get("hba") if kwargs and "hba" in kwargs else None
2857     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
2858     if not host_name:
2859         raise SaltCloudSystemExit("You must specify name of the host system.")
2860     host_ref = salt.utils.vmware.get_mor_by_property(
2861         _get_si(), vim.HostSystem, host_name
2862     )
2863     try:
2864         if hba:
2865             log.info("Rescanning HBA %s on host %s", hba, host_name)
2866             host_ref.configManager.storageSystem.RescanHba(hba)
2867             ret = "rescanned HBA {}".format(hba)
2868         else:
2869             log.info("Rescanning all HBAs on host %s", host_name)
2870             host_ref.configManager.storageSystem.RescanAllHba()
2871             ret = "rescanned all HBAs"
2872     except Exception as exc:  # pylint: disable=broad-except
2873         log.error(
2874             "Error while rescaning HBA on host %s: %s",
2875             host_name,
2876             exc,
2877             exc_info_on_loglevel=logging.DEBUG,
2878         )
2879         return {host_name: "failed to rescan HBA"}
2880     return {host_name: ret}
2881 def upgrade_tools_all(call=None):
2882     """
2883     To upgrade VMware Tools on all virtual machines present in
2884     the specified provider
2885     .. note::
2886         If the virtual machine is running Windows OS, this function
2887         will attempt to suppress the automatic reboot caused by a
2888         VMware Tools upgrade.
2889     CLI Example:
2890     .. code-block:: bash
2891         salt-cloud -f upgrade_tools_all my-vmware-config
2892     """
2893     if call != "function":
2894         raise SaltCloudSystemExit(
2895             "The upgrade_tools_all function must be called with -f or --function."
2896         )
2897     ret = {}
2898     vm_properties = ["name"]
2899     vm_list = salt.utils.vmware.get_mors_with_properties(
2900         _get_si(), vim.VirtualMachine, vm_properties
2901     )
2902     for vm in vm_list:
2903         ret[vm["name"]] = _upg_tools_helper(vm["object"])
2904     return ret
2905 def upgrade_tools(name, reboot=False, call=None):
2906     """
2907     To upgrade VMware Tools on a specified virtual machine.
2908     .. note::
2909         If the virtual machine is running Windows OS, use ``reboot=True``
2910         to reboot the virtual machine after VMware tools upgrade. Default
2911         is ``reboot=False``
2912     CLI Example:
2913     .. code-block:: bash
2914         salt-cloud -a upgrade_tools vmname
2915         salt-cloud -a upgrade_tools vmname reboot=True
2916     """
2917     if call != "action":
2918         raise SaltCloudSystemExit(
2919             "The upgrade_tools action must be called with -a or --action."
2920         )
2921     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
2922     return _upg_tools_helper(vm_ref, reboot)
2923 def list_hosts_by_cluster(kwargs=None, call=None):
2924     """
2925     List hosts for each cluster; or hosts for a specified cluster in
2926     this VMware environment
2927     To list hosts for each cluster:
2928     CLI Example:
2929     .. code-block:: bash
2930         salt-cloud -f list_hosts_by_cluster my-vmware-config
2931     To list hosts for a specified cluster:
2932     CLI Example:
2933     .. code-block:: bash
2934         salt-cloud -f list_hosts_by_cluster my-vmware-config cluster="clusterName"
2935     """
2936     if call != "function":
2937         raise SaltCloudSystemExit(
2938             "The list_hosts_by_cluster function must be called with -f or --function."
2939         )
2940     ret = {}
2941     cluster_name = kwargs.get("cluster") if kwargs and "cluster" in kwargs else None
2942     cluster_properties = ["name"]
2943     cluster_list = salt.utils.vmware.get_mors_with_properties(
2944         _get_si(), vim.ClusterComputeResource, cluster_properties
2945     )
2946     for cluster in cluster_list:
2947         ret[cluster["name"]] = []
2948         for host in cluster["object"].host:
2949             if isinstance(host, vim.HostSystem):
2950                 ret[cluster["name"]].append(host.name)
2951         if cluster_name and cluster_name == cluster["name"]:
2952             return {"Hosts by Cluster": {cluster_name: ret[cluster_name]}}
2953     return {"Hosts by Cluster": ret}
2954 def list_clusters_by_datacenter(kwargs=None, call=None):
2955     """
2956     List clusters for each datacenter; or clusters for a specified datacenter in
2957     this VMware environment
2958     To list clusters for each datacenter:
2959     CLI Example:
2960     .. code-block:: bash
2961         salt-cloud -f list_clusters_by_datacenter my-vmware-config
2962     To list clusters for a specified datacenter:
2963     CLI Example:
2964     .. code-block:: bash
2965         salt-cloud -f list_clusters_by_datacenter my-vmware-config datacenter="datacenterName"
2966     """
2967     if call != "function":
2968         raise SaltCloudSystemExit(
2969             "The list_clusters_by_datacenter function must be called with "
2970             "-f or --function."
2971         )
2972     ret = {}
2973     datacenter_name = (
2974         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
2975     )
2976     datacenter_properties = ["name"]
2977     datacenter_list = salt.utils.vmware.get_mors_with_properties(
2978         _get_si(), vim.Datacenter, datacenter_properties
2979     )
2980     for datacenter in datacenter_list:
2981         ret[datacenter["name"]] = []
2982         for cluster in datacenter["object"].hostFolder.childEntity:
2983             if isinstance(cluster, vim.ClusterComputeResource):
2984                 ret[datacenter["name"]].append(cluster.name)
2985         if datacenter_name and datacenter_name == datacenter["name"]:
2986             return {"Clusters by Datacenter": {datacenter_name: ret[datacenter_name]}}
2987     return {"Clusters by Datacenter": ret}
2988 def list_hosts_by_datacenter(kwargs=None, call=None):
2989     """
2990     List hosts for each datacenter; or hosts for a specified datacenter in
2991     this VMware environment
2992     To list hosts for each datacenter:
2993     CLI Example:
2994     .. code-block:: bash
2995         salt-cloud -f list_hosts_by_datacenter my-vmware-config
2996     To list hosts for a specified datacenter:
2997     CLI Example:
2998     .. code-block:: bash
2999         salt-cloud -f list_hosts_by_datacenter my-vmware-config datacenter="datacenterName"
3000     """
3001     if call != "function":
3002         raise SaltCloudSystemExit(
3003             "The list_hosts_by_datacenter function must be called with "
3004             "-f or --function."
3005         )
3006     ret = {}
3007     datacenter_name = (
3008         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
3009     )
3010     datacenter_properties = ["name"]
3011     datacenter_list = salt.utils.vmware.get_mors_with_properties(
3012         _get_si(), vim.Datacenter, datacenter_properties
3013     )
3014     for datacenter in datacenter_list:
3015         ret[datacenter["name"]] = []
3016         for cluster in datacenter["object"].hostFolder.childEntity:
3017             if isinstance(cluster, vim.ClusterComputeResource):
3018                 for host in cluster.host:
3019                     if isinstance(host, vim.HostSystem):
3020                         ret[datacenter["name"]].append(host.name)
3021         if datacenter_name and datacenter_name == datacenter["name"]:
3022             return {"Hosts by Datacenter": {datacenter_name: ret[datacenter_name]}}
3023     return {"Hosts by Datacenter": ret}
3024 def list_hbas(kwargs=None, call=None):
3025     """
3026     List all HBAs for each host system; or all HBAs for a specified host
3027     system; or HBAs of specified type for each host system; or HBAs of
3028     specified type for a specified host system in this VMware environment
3029     .. note::
3030         You can specify type as either ``parallel``, ``iscsi``, ``block``
3031         or ``fibre``.
3032     To list all HBAs for each host system:
3033     CLI Example:
3034     .. code-block:: bash
3035         salt-cloud -f list_hbas my-vmware-config
3036     To list all HBAs for a specified host system:
3037     CLI Example:
3038     .. code-block:: bash
3039         salt-cloud -f list_hbas my-vmware-config host="hostSystemName"
3040     To list HBAs of specified type for each host system:
3041     CLI Example:
3042     .. code-block:: bash
3043         salt-cloud -f list_hbas my-vmware-config type="HBAType"
3044     To list HBAs of specified type for a specified host system:
3045     CLI Example:
3046     .. code-block:: bash
3047         salt-cloud -f list_hbas my-vmware-config host="hostSystemName" type="HBAtype"
3048     """
3049     if call != "function":
3050         raise SaltCloudSystemExit(
3051             "The list_hbas function must be called with -f or --function."
3052         )
3053     ret = {}
3054     hba_type = kwargs.get("type").lower() if kwargs and "type" in kwargs else None
3055     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3056     host_properties = ["name", "config.storageDevice.hostBusAdapter"]
3057     if hba_type and hba_type not in ["parallel", "block", "iscsi", "fibre"]:
3058         raise SaltCloudSystemExit(
3059             "Specified hba type {} currently not supported.".format(hba_type)
3060         )
3061     host_list = salt.utils.vmware.get_mors_with_properties(
3062         _get_si(), vim.HostSystem, host_properties
3063     )
3064     for host in host_list:
3065         ret[host["name"]] = {}
3066         for hba in host["config.storageDevice.hostBusAdapter"]:
3067             hba_spec = {
3068                 "driver": hba.driver,
3069                 "status": hba.status,
3070                 "type": type(hba).__name__.rsplit(".", 1)[1],
3071             }
3072             if hba_type:
3073                 if isinstance(hba, _get_hba_type(hba_type)):
3074                     if hba.model in ret[host["name"]]:
3075                         ret[host["name"]][hba.model][hba.device] = hba_spec
3076                     else:
3077                         ret[host["name"]][hba.model] = {hba.device: hba_spec}
3078             else:
3079                 if hba.model in ret[host["name"]]:
3080                     ret[host["name"]][hba.model][hba.device] = hba_spec
3081                 else:
3082                     ret[host["name"]][hba.model] = {hba.device: hba_spec}
3083         if host["name"] == host_name:
3084             return {"HBAs by Host": {host_name: ret[host_name]}}
3085     return {"HBAs by Host": ret}
3086 def list_dvs(kwargs=None, call=None):
3087     """
3088     List all the distributed virtual switches for this VMware environment
3089     CLI Example:
3090     .. code-block:: bash
3091         salt-cloud -f list_dvs my-vmware-config
3092     """
3093     if call != "function":
3094         raise SaltCloudSystemExit(
3095             "The list_dvs function must be called with -f or --function."
3096         )
3097     return {"Distributed Virtual Switches": salt.utils.vmware.list_dvs(_get_si())}
3098 def list_vapps(kwargs=None, call=None):
3099     """
3100     List all the vApps for this VMware environment
3101     CLI Example:
3102     .. code-block:: bash
3103         salt-cloud -f list_vapps my-vmware-config
3104     """
3105     if call != "function":
3106         raise SaltCloudSystemExit(
3107             "The list_vapps function must be called with -f or --function."
3108         )
3109     return {"vApps": salt.utils.vmware.list_vapps(_get_si())}
3110 def enter_maintenance_mode(kwargs=None, call=None):
3111     """
3112     To put the specified host system in maintenance mode in this VMware environment
3113     CLI Example:
3114     .. code-block:: bash
3115         salt-cloud -f enter_maintenance_mode my-vmware-config host="myHostSystemName"
3116     """
3117     if call != "function":
3118         raise SaltCloudSystemExit(
3119             "The enter_maintenance_mode function must be called with -f or --function."
3120         )
3121     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3122     host_ref = salt.utils.vmware.get_mor_by_property(
3123         _get_si(), vim.HostSystem, host_name
3124     )
3125     if not host_name or not host_ref:
3126         raise SaltCloudSystemExit("You must specify a valid name of the host system.")
3127     if host_ref.runtime.inMaintenanceMode:
3128         return {host_name: "already in maintenance mode"}
3129     try:
3130         task = host_ref.EnterMaintenanceMode(timeout=0, evacuatePoweredOffVms=True)
3131         salt.utils.vmware.wait_for_task(task, host_name, "enter maintenance mode")
3132     except Exception as exc:  # pylint: disable=broad-except
3133         log.error(
3134             "Error while moving host system %s in maintenance mode: %s",
3135             host_name,
3136             exc,
3137             exc_info_on_loglevel=logging.DEBUG,
3138         )
3139         return {host_name: "failed to enter maintenance mode"}
3140     return {host_name: "entered maintenance mode"}
3141 def exit_maintenance_mode(kwargs=None, call=None):
3142     """
3143     To take the specified host system out of maintenance mode in this VMware environment
3144     CLI Example:
3145     .. code-block:: bash
3146         salt-cloud -f exit_maintenance_mode my-vmware-config host="myHostSystemName"
3147     """
3148     if call != "function":
3149         raise SaltCloudSystemExit(
3150             "The exit_maintenance_mode function must be called with -f or --function."
3151         )
3152     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3153     host_ref = salt.utils.vmware.get_mor_by_property(
3154         _get_si(), vim.HostSystem, host_name
3155     )
3156     if not host_name or not host_ref:
3157         raise SaltCloudSystemExit("You must specify a valid name of the host system.")
3158     if not host_ref.runtime.inMaintenanceMode:
3159         return {host_name: "already not in maintenance mode"}
3160     try:
3161         task = host_ref.ExitMaintenanceMode(timeout=0)
3162         salt.utils.vmware.wait_for_task(task, host_name, "exit maintenance mode")
3163     except Exception as exc:  # pylint: disable=broad-except
3164         log.error(
3165             "Error while moving host system %s out of maintenance mode: %s",
3166             host_name,
3167             exc,
3168             exc_info_on_loglevel=logging.DEBUG,
3169         )
3170         return {host_name: "failed to exit maintenance mode"}
3171     return {host_name: "exited maintenance mode"}
3172 def create_folder(kwargs=None, call=None):
3173     """
3174     Create the specified folder path in this VMware environment
3175     .. note::
3176         To create a Host and Cluster Folder under a Datacenter, specify
3177         ``path="/yourDatacenterName/host/yourFolderName"``
3178         To create a Network Folder under a Datacenter, specify
3179         ``path="/yourDatacenterName/network/yourFolderName"``
3180         To create a Storage Folder under a Datacenter, specify
3181         ``path="/yourDatacenterName/datastore/yourFolderName"``
3182         To create a VM and Template Folder under a Datacenter, specify
3183         ``path="/yourDatacenterName/vm/yourFolderName"``
3184     CLI Example:
3185     .. code-block:: bash
3186         salt-cloud -f create_folder my-vmware-config path="/Local/a/b/c"
3187         salt-cloud -f create_folder my-vmware-config path="/MyDatacenter/vm/MyVMFolder"
3188         salt-cloud -f create_folder my-vmware-config path="/MyDatacenter/host/MyHostFolder"
3189         salt-cloud -f create_folder my-vmware-config path="/MyDatacenter/network/MyNetworkFolder"
3190         salt-cloud -f create_folder my-vmware-config path="/MyDatacenter/storage/MyStorageFolder"
3191     """
3192     if call != "function":
3193         raise SaltCloudSystemExit(
3194             "The create_folder function must be called with -f or --function."
3195         )
3196     si = _get_si()
3197     folder_path = kwargs.get("path") if kwargs and "path" in kwargs else None
3198     if not folder_path:
3199         raise SaltCloudSystemExit("You must specify a non empty folder path.")
3200     folder_refs = []
3201     inventory_path = "/"
3202     path_exists = True
3203     for index, folder_name in enumerate(
3204         os.path.normpath(folder_path.strip("/")).split("/")
3205     ):
3206         inventory_path = os.path.join(inventory_path, folder_name)
3207         folder_ref = si.content.searchIndex.FindByInventoryPath(
3208             inventoryPath=inventory_path
3209         )
3210         if isinstance(folder_ref, vim.Folder):
3211             log.debug("Path %s/ exists in the inventory", inventory_path)
3212             folder_refs.append(folder_ref)
3213         elif isinstance(folder_ref, vim.Datacenter):
3214             log.debug("Path %s/ exists in the inventory", inventory_path)
3215             folder_refs.append(folder_ref)
3216         else:
3217             if not folder_refs:
3218                 log<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.debug(
3219                     "Creating folder %s under rootFolder in the inventory", folder_name
3220                 )
3221                 folder_refs.append(si.content.rootFolder.CreateFolder(folder_name))
3222             else:
3223                 log.debug("Creating path %s/ in the inventory", inventory_path)
3224                 folder_refs.append(folder_refs[</b></font>index - 1].CreateFolder(folder_name))
3225     if path_exists:
3226         return {inventory_path: "specified path already exists"}
3227     return {inventory_path: "created the specified path"}
3228 def create_snapshot(name, kwargs=None, call=None):
3229     """
3230     Create a snapshot of the specified virtual machine in this VMware
3231     environment
3232     .. note::
3233         If the VM is powered on, the internal state of the VM (memory
3234         dump) is included in the snapshot by default which will also set
3235         the power state of the snapshot to "powered on". You can set
3236         ``memdump=False`` to override this. This field is ignored if
3237         the virtual machine is powered off or if the VM does not support
3238         snapshots with memory dumps. Default is ``memdump=True``
3239     .. note::
3240         If the VM is powered on when the snapshot is taken, VMware Tools
3241         can be used to quiesce the file system in the virtual machine by
3242         setting ``quiesce=True``. This field is ignored if the virtual
3243         machine is powered off; if VMware Tools are not available or if
3244         ``memdump=True``. Default is ``quiesce=False``
3245     CLI Example:
3246     .. code-block:: bash
3247         salt-cloud -a create_snapshot vmname snapshot_name="mySnapshot"
3248         salt-cloud -a create_snapshot vmname snapshot_name="mySnapshot" [description="My snapshot"] [memdump=False] [quiesce=True]
3249     """
3250     if call != "action":
3251         raise SaltCloudSystemExit(
3252             "The create_snapshot action must be called with -a or --action."
3253         )
3254     if kwargs is None:
3255         kwargs = {}
3256     snapshot_name = (
3257         kwargs.get("snapshot_name") if kwargs and "snapshot_name" in kwargs else None
3258     )
3259     if not snapshot_name:
3260         raise SaltCloudSystemExit(
3261             "You must specify snapshot name for the snapshot to be created."
3262         )
3263     memdump = _str_to_bool(kwargs.get("memdump", True))
3264     quiesce = _str_to_bool(kwargs.get("quiesce", False))
3265     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
3266     if vm_ref.summary.runtime.powerState != "poweredOn":
3267         log.debug(
3268             "VM %s is not powered on. Setting both memdump and quiesce to False", name
3269         )
3270         memdump = False
3271         quiesce = False
3272     if memdump and quiesce:
3273         log.warning(
3274             "You can only set either memdump or quiesce to True. Setting quiesce=False"
3275         )
3276         quiesce = False
3277     desc = kwargs.get("description") if "description" in kwargs else ""
3278     try:
3279         task = vm_ref.CreateSnapshot(snapshot_name, desc, memdump, quiesce)
3280         salt.utils.vmware.wait_for_task(task, name, "create snapshot", 5, "info")
3281     except Exception as exc:  # pylint: disable=broad-except
3282         log.error(
3283             "Error while creating snapshot of %s: %s",
3284             name,
3285             exc,
3286             exc_info_on_loglevel=logging.DEBUG,
3287         )
3288         return "failed to create snapshot"
3289     return {
3290         "Snapshot created successfully": _get_snapshots(
3291             vm_ref.snapshot.rootSnapshotList, vm_ref.snapshot.currentSnapshot
3292         )
3293     }
3294 def revert_to_snapshot(name, kwargs=None, call=None):
3295     """
3296     Revert virtual machine to its current snapshot. If no snapshot
3297     exists, the state of the virtual machine remains unchanged
3298     .. note::
3299         The virtual machine will be powered on if the power state of
3300         the snapshot when it was created was set to "Powered On". Set
3301         ``power_off=True`` so that the virtual machine stays powered
3302         off regardless of the power state of the snapshot when it was
3303         created. Default is ``power_off=False``.
3304         If the power state of the snapshot when it was created was
3305         "Powered On" and if ``power_off=True``, the VM will be put in
3306         suspended state after it has been reverted to the snapshot.
3307     CLI Example:
3308     .. code-block:: bash
3309         salt-cloud -a revert_to_snapshot vmame [power_off=True]
3310         salt-cloud -a revert_to_snapshot vmame snapshot_name="selectedSnapshot" [power_off=True]
3311     """
3312     if call != "action":
3313         raise SaltCloudSystemExit(
3314             "The revert_to_snapshot action must be called with -a or --action."
3315         )
3316     if kwargs is None:
3317         kwargs = {}
3318     snapshot_name = (
3319         kwargs.get("snapshot_name") if kwargs and "snapshot_name" in kwargs else None
3320     )
3321     suppress_power_on = _str_to_bool(kwargs.get("power_off", False))
3322     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
3323     if not vm_ref.rootSnapshot:
3324         log.error("VM %s does not contain any current snapshots", name)
3325         return "revert failed"
3326     msg = "reverted to current snapshot"
3327     try:
3328         if snapshot_name is None:
3329             log.debug("Reverting VM %s to current snapshot", name)
3330             task = vm_ref.RevertToCurrentSnapshot(suppressPowerOn=suppress_power_on)
3331         else:
3332             log.debug("Reverting VM %s to snapshot %s", name, snapshot_name)
3333             msg = "reverted to snapshot {}".format(snapshot_name)
3334             snapshot_ref = _get_snapshot_ref_by_name(vm_ref, snapshot_name)
3335             if snapshot_ref is None:
3336                 return "specified snapshot '{}' does not exist".format(snapshot_name)
3337             task = snapshot_ref.snapshot.Revert(suppressPowerOn=suppress_power_on)
3338         salt.utils.vmware.wait_for_task(task, name, "revert to snapshot", 5, "info")
3339     except Exception as exc:  # pylint: disable=broad-except
3340         log.error(
3341             "Error while reverting VM %s to snapshot: %s",
3342             name,
3343             exc,
3344             exc_info_on_loglevel=logging.DEBUG,
3345         )
3346         return "revert failed"
3347     return msg
3348 def remove_snapshot(name, kwargs=None, call=None):
3349     """
3350     Remove a snapshot of the specified virtual machine in this VMware environment
3351     CLI Example:
3352     .. code-block:: bash
3353         salt-cloud -a remove_snapshot vmname snapshot_name="mySnapshot"
3354         salt-cloud -a remove_snapshot vmname snapshot_name="mySnapshot" [remove_children="True"]
3355     """
3356     if call != "action":
3357         raise SaltCloudSystemExit(
3358             "The create_snapshot action must be called with -a or --action."
3359         )
3360     if kwargs is None:
3361         kwargs = {}
3362     snapshot_name = (
3363         kwargs.get("snapshot_name") if kwargs and "snapshot_name" in kwargs else None
3364     )
3365     remove_children = _str_to_bool(kwargs.get("remove_children", False))
3366     if not snapshot_name:
3367         raise SaltCloudSystemExit(
3368             "You must specify snapshot name for the snapshot to be deleted."
3369         )
3370     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
3371     if not _get_snapshot_ref_by_name(vm_ref, snapshot_name):
3372         raise SaltCloudSystemExit(
3373             "ould not find the snapshot with the specified name."
3374         )
3375     try:
3376         snap_obj = _get_snapshot_ref_by_name(vm_ref, snapshot_name).snapshot
3377         task = snap_obj.RemoveSnapshot_Task(remove_children)
3378         salt.utils.vmware.wait_for_task(task, name, "remove snapshot", 5, "info")
3379     except Exception as exc:  # pylint: disable=broad-except
3380         log.error(
3381             "Error while removing snapshot of %s: %s",
3382             name,
3383             exc,
3384             exc_info_on_loglevel=logging.DEBUG,
3385         )
3386         return "failed to remove snapshot"
3387     if vm_ref.snapshot:
3388         return {
3389             "Snapshot removed successfully": _get_snapshots(
3390                 vm_ref.snapshot.rootSnapshotList, vm_ref.snapshot.currentSnapshot
3391             )
3392         }
3393     return "Snapshots removed successfully"
3394 def remove_all_snapshots(name, kwargs=None, call=None):
3395     """
3396     Remove all the snapshots present for the specified virtual machine.
3397     .. note::
3398         All the snapshots higher up in the hierarchy of the current snapshot tree
3399         are consolidated and their virtual disks are merged. To override this
3400         behavior and only remove all snapshots, set ``merge_snapshots=False``.
3401         Default is ``merge_snapshots=True``
3402     CLI Example:
3403     .. code-block:: bash
3404         salt-cloud -a remove_all_snapshots vmname [merge_snapshots=False]
3405     """
3406     if call != "action":
3407         raise SaltCloudSystemExit(
3408             "The remove_all_snapshots action must be called with -a or --action."
3409         )
3410     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
3411     try:
3412         task = vm_ref.RemoveAllSnapshots()
3413         salt.utils.vmware.wait_for_task(task, name, "remove snapshots", 5, "info")
3414     except Exception as exc:  # pylint: disable=broad-except
3415         log.error(
3416             "Error while removing snapshots on VM %s: %s",
3417             name,
3418             exc,
3419             exc_info_on_loglevel=logging.DEBUG,
3420         )
3421         return "Failed to remove snapshots"
3422     return "Removed all snapshots"
3423 def convert_to_template(name, kwargs=None, call=None):
3424     """
3425     Convert the specified virtual machine to template.
3426     CLI Example:
3427     .. code-block:: bash
3428         salt-cloud -a convert_to_template vmname
3429     """
3430     if call != "action":
3431         raise SaltCloudSystemExit(
3432             "The convert_to_template action must be called with -a or --action."
3433         )
3434     vm_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.VirtualMachine, name)
3435     if vm_ref.config.template:
3436         raise SaltCloudSystemExit("{} already a template".format(name))
3437     try:
3438         vm_ref.MarkAsTemplate()
3439     except Exception as exc:  # pylint: disable=broad-except
3440         log.error(
3441             "Error while converting VM to template %s: %s",
3442             name,
3443             exc,
3444             exc_info_on_loglevel=logging.DEBUG,
3445         )
3446         return "failed to convert to teamplate"
3447     return "{} converted to template".format(name)
3448 def add_host(kwargs=None, call=None):
3449     """
3450     Add a host system to the specified cluster or datacenter in this VMware environment
3451     .. note::
3452         To use this function, you need to specify ``esxi_host_user`` and
3453         ``esxi_host_password`` under your provider configuration set up at
3454         ``/etc/salt/cloud.providers`` or ``/etc/salt/cloud.providers.d/vmware.conf``:
3455         .. code-block:: yaml
3456             vcenter01:
3457               driver: vmware
3458               user: 'DOMAIN\\user'
3459               password: 'verybadpass'
3460               url: 'vcenter01.domain.com'
3461               esxi_host_user: 'root'
3462               esxi_host_password: 'myhostpassword'
3463               esxi_host_ssl_thumbprint: '12:A3:45:B6:CD:7E:F8:90:A1:BC:23:45:D6:78:9E:FA:01:2B:34:CD'
3464         The SSL thumbprint of the host system can be optionally specified by setting
3465         ``esxi_host_ssl_thumbprint`` under your provider configuration. To get the SSL
3466         thumbprint of the host system, execute the following command from a remote
3467         server:
3468         .. code-block:: bash
3469             echo -n | openssl s_client -connect &lt;YOUR-HOSTSYSTEM-DNS/IP&gt;:443 2&gt;/dev/null | openssl x509 -noout -fingerprint -sha1
3470     CLI Example:
3471     .. code-block:: bash
3472         salt-cloud -f add_host my-vmware-config host="myHostSystemName" cluster="myClusterName"
3473         salt-cloud -f add_host my-vmware-config host="myHostSystemName" datacenter="myDatacenterName"
3474     """
3475     if call != "function":
3476         raise SaltCloudSystemExit(
3477             "The add_host function must be called with -f or --function."
3478         )
3479     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3480     cluster_name = kwargs.get("cluster") if kwargs and "cluster" in kwargs else None
3481     datacenter_name = (
3482         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
3483     )
3484     host_user = config.get_cloud_config_value(
3485         "esxi_host_user", get_configured_provider(), __opts__, search_global=False
3486     )
3487     host_password = config.get_cloud_config_value(
3488         "esxi_host_password", get_configured_provider(), __opts__, search_global=False
3489     )
3490     host_ssl_thumbprint = config.get_cloud_config_value(
3491         "esxi_host_ssl_thumbprint",
3492         get_configured_provider(),
3493         __opts__,
3494         search_global=False,
3495     )
3496     if not host_user:
3497         raise SaltCloudSystemExit(
3498             "You must specify the ESXi host username in your providers config."
3499         )
3500     if not host_password:
3501         raise SaltCloudSystemExit(
3502             "You must specify the ESXi host password in your providers config."
3503         )
3504     if not host_name:
3505         raise SaltCloudSystemExit(
3506             "You must specify either the IP or DNS name of the host system."
3507         )
3508     if (cluster_name and datacenter_name) or not (cluster_name or datacenter_name):
3509         raise SaltCloudSystemExit(
3510             "You must specify either the cluster name or the datacenter name."
3511         )
3512     si = _get_si()
3513     if cluster_name:
3514         cluster_ref = salt.utils.vmware.get_mor_by_property(
3515             si, vim.ClusterComputeResource, cluster_name
3516         )
3517         if not cluster_ref:
3518             raise SaltCloudSystemExit("Specified cluster does not exist.")
3519     if datacenter_name:
3520         datacenter_ref = salt.utils.vmware.get_mor_by_property(
3521             si, vim.Datacenter, datacenter_name
3522         )
3523         if not datacenter_ref:
3524             raise SaltCloudSystemExit("Specified datacenter does not exist.")
3525     spec = vim.host.ConnectSpec(
3526         hostName=host_name,
3527         userName=host_user,
3528         password=host_password,
3529     )
3530     if host_ssl_thumbprint:
3531         spec.sslThumbprint = host_ssl_thumbprint
3532     else:
3533         log.warning("SSL thumbprint has not been specified in provider configuration")
3534         try:
3535             log.debug("Trying to get the SSL thumbprint directly from the host system")
3536             p1 = subprocess.Popen(
3537                 ("echo", "-n"), stdout=subprocess.PIPE, stderr=subprocess.PIPE
3538             )
3539             p2 = subprocess.Popen(
3540                 ("openssl", "s_client", "-connect", "{}:443".format(host_name)),
3541                 stdin=p1.stdout,
3542                 stdout=subprocess.PIPE,
3543                 stderr=subprocess.PIPE,
3544             )
3545             p3 = subprocess.Popen(
3546                 ("openssl", "x509", "-noout", "-fingerprint", "-sha1"),
3547                 stdin=p2.stdout,
3548                 stdout=subprocess.PIPE,
3549                 stderr=subprocess.PIPE,
3550             )
3551             out = salt.utils.stringutils.to_str(p3.stdout.read())
3552             ssl_thumbprint = out.split("=")[-1].strip()
3553             log.debug(
3554                 "SSL thumbprint received from the host system: %s", ssl_thumbprint
3555             )
3556             spec.sslThumbprint = ssl_thumbprint
3557         except Exception as exc:  # pylint: disable=broad-except
3558             log.error(
3559                 "Error while trying to get SSL thumbprint of host %s: %s",
3560                 host_name,
3561                 exc,
3562                 exc_info_on_loglevel=logging.DEBUG,
3563             )
3564             return {host_name: "failed to add host"}
3565     try:
3566         if cluster_name:
3567             task = cluster_ref.AddHost(spec=spec, asConnected=True)
3568             ret = "added host system to cluster {}".format(cluster_name)
3569         if datacenter_name:
3570             task = datacenter_ref.hostFolder.AddStandaloneHost(
3571                 spec=spec, addConnected=True
3572             )
3573             ret = "added host system to datacenter {}".format(datacenter_name)
3574         salt.utils.vmware.wait_for_task(task, host_name, "add host system", 5, "info")
3575     except Exception as exc:  # pylint: disable=broad-except
3576         if isinstance(exc, vim.fault.SSLVerifyFault):
3577             log.error("Authenticity of the host's SSL certificate is not verified")
3578             log.info(
3579                 "Try again after setting the esxi_host_ssl_thumbprint "
3580                 "to %s in provider configuration",
3581                 spec.sslThumbprint,
3582             )
3583         log.error(
3584             "Error while adding host %s: %s",
3585             host_name,
3586             exc,
3587             exc_info_on_loglevel=logging.DEBUG,
3588         )
3589         return {host_name: "failed to add host"}
3590     return {host_name: ret}
3591 def remove_host(kwargs=None, call=None):
3592     """
3593     Remove the specified host system from this VMware environment
3594     CLI Example:
3595     .. code-block:: bash
3596         salt-cloud -f remove_host my-vmware-config host="myHostSystemName"
3597     """
3598     if call != "function":
3599         raise SaltCloudSystemExit(
3600             "The remove_host function must be called with -f or --function."
3601         )
3602     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3603     if not host_name:
3604         raise SaltCloudSystemExit("You must specify name of the host system.")
3605     si = _get_si()
3606     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3607     if not host_ref:
3608         raise SaltCloudSystemExit("Specified host system does not exist.")
3609     try:
3610         if isinstance(host_ref.parent, vim.ClusterComputeResource):
3611             task = host_ref.Destroy_Task()
3612         else:
3613             task = host_ref.parent.Destroy_Task()
3614         salt.utils.vmware.wait_for_task(
3615             task, host_name, "remove host", log_level="info"
3616         )
3617     except Exception as exc:  # pylint: disable=broad-except
3618         log.error(
3619             "Error while removing host %s: %s",
3620             host_name,
3621             exc,
3622             exc_info_on_loglevel=logging.DEBUG,
3623         )
3624         return {host_name: "failed to remove host"}
3625     return {host_name: "removed host from vcenter"}
3626 def connect_host(kwargs=None, call=None):
3627     """
3628     Connect the specified host system in this VMware environment
3629     CLI Example:
3630     .. code-block:: bash
3631         salt-cloud -f connect_host my-vmware-config host="myHostSystemName"
3632     """
3633     if call != "function":
3634         raise SaltCloudSystemExit(
3635             "The connect_host function must be called with -f or --function."
3636         )
3637     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3638     if not host_name:
3639         raise SaltCloudSystemExit("You must specify name of the host system.")
3640     si = _get_si()
3641     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3642     if not host_ref:
3643         raise SaltCloudSystemExit("Specified host system does not exist.")
3644     if host_ref.runtime.connectionState == "connected":
3645         return {host_name: "host system already connected"}
3646     try:
3647         task = host_ref.ReconnectHost_Task()
3648         salt.utils.vmware.wait_for_task(task, host_name, "connect host", 5, "info")
3649     except Exception as exc:  # pylint: disable=broad-except
3650         log.error(
3651             "Error while connecting host %s: %s",
3652             host_name,
3653             exc,
3654             exc_info_on_loglevel=logging.DEBUG,
3655         )
3656         return {host_name: "failed to connect host"}
3657     return {host_name: "connected host"}
3658 def disconnect_host(kwargs=None, call=None):
3659     """
3660     Disconnect the specified host system in this VMware environment
3661     CLI Example:
3662     .. code-block:: bash
3663         salt-cloud -f disconnect_host my-vmware-config host="myHostSystemName"
3664     """
3665     if call != "function":
3666         raise SaltCloudSystemExit(
3667             "The disconnect_host function must be called with -f or --function."
3668         )
3669     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3670     if not host_name:
3671         raise SaltCloudSystemExit("You must specify name of the host system.")
3672     si = _get_si()
3673     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3674     if not host_ref:
3675         raise SaltCloudSystemExit("Specified host system does not exist.")
3676     if host_ref.runtime.connectionState == "disconnected":
3677         return {host_name: "host system already disconnected"}
3678     try:
3679         task = host_ref.DisconnectHost_Task()
3680         salt.utils.vmware.wait_for_task(
3681             task, host_name, "disconnect host", log_level="info"
3682         )
3683     except Exception as exc:  # pylint: disable=broad-except
3684         log.error(
3685             "Error while disconnecting host %s: %s",
3686             host_name,
3687             exc,
3688             exc_info_on_loglevel=logging.DEBUG,
3689         )
3690         return {host_name: "failed to disconnect host"}
3691     return {host_name: "disconnected host"}
3692 def reboot_host(kwargs=None, call=None):
3693     """
3694     Reboot the specified host system in this VMware environment
3695     .. note::
3696         If the host system is not in maintenance mode, it will not be rebooted. If you
3697         want to reboot the host system regardless of whether it is in maintenance mode,
3698         set ``force=True``. Default is ``force=False``.
3699     CLI Example:
3700     .. code-block:: bash
3701         salt-cloud -f reboot_host my-vmware-config host="myHostSystemName" [force=True]
3702     """
3703     if call != "function":
3704         raise SaltCloudSystemExit(
3705             "The reboot_host function must be called with -f or --function."
3706         )
3707     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3708     force = _str_to_bool(kwargs.get("force")) if kwargs and "force" in kwargs else False
3709     if not host_name:
3710         raise SaltCloudSystemExit("You must specify name of the host system.")
3711     si = _get_si()
3712     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3713     if not host_ref:
3714         raise SaltCloudSystemExit("Specified host system does not exist.")
3715     if host_ref.runtime.connectionState == "notResponding":
3716         raise SaltCloudSystemExit(
3717             "Specified host system cannot be rebooted in it's current state (not"
3718             " responding)."
3719         )
3720     if not host_ref.capability.rebootSupported:
3721         raise SaltCloudSystemExit("Specified host system does not support reboot.")
3722     if not host_ref.runtime.inMaintenanceMode and not force:
3723         raise SaltCloudSystemExit(
3724             "Specified host system is not in maintenance mode. Specify force=True to"
3725             " force reboot even if there are virtual machines running or other"
3726             " operations in progress."
3727         )
3728     try:
3729         host_ref.RebootHost_Task(force)
3730         _wait_for_host(host_ref, "reboot", 10, "info")
3731     except Exception as exc:  # pylint: disable=broad-except
3732         log.error(
3733             "Error while rebooting host %s: %s",
3734             host_name,
3735             exc,
3736             exc_info_on_loglevel=logging.DEBUG,
3737         )
3738         return {host_name: "failed to reboot host"}
3739     return {host_name: "rebooted host"}
3740 def create_datastore_cluster(kwargs=None, call=None):
3741     """
3742     Create a new datastore cluster for the specified datacenter in this VMware environment
3743     CLI Example:
3744     .. code-block:: bash
3745         salt-cloud -f create_datastore_cluster my-vmware-config name="datastoreClusterName" datacenter="datacenterName"
3746     """
3747     if call != "function":
3748         raise SaltCloudSystemExit(
3749             "The create_datastore_cluster function must be called with "
3750             "-f or --function."
3751         )
3752     datastore_cluster_name = kwargs.get("name") if kwargs and "name" in kwargs else None
3753     datacenter_name = (
3754         kwargs.get("datacenter") if kwargs and "datacenter" in kwargs else None
3755     )
3756     if not datastore_cluster_name:
3757         raise SaltCloudSystemExit(
3758             "You must specify name of the new datastore cluster to be created."
3759         )
3760     if not datastore_cluster_name or len(datastore_cluster_name) &gt;= 80:
3761         raise SaltCloudSystemExit(
3762             "The datastore cluster name must be a non empty string of less than 80"
3763             " characters."
3764         )
3765     if not datacenter_name:
3766         raise SaltCloudSystemExit(
3767             "You must specify name of the datacenter where the datastore cluster should"
3768             " be created."
3769         )
3770     si = _get_si()
3771     datastore_cluster_ref = salt.utils.vmware.get_mor_by_property(
3772         si, vim.StoragePod, datastore_cluster_name
3773     )
3774     if datastore_cluster_ref:
3775         return {datastore_cluster_name: "datastore cluster already exists"}
3776     datacenter_ref = salt.utils.vmware.get_mor_by_property(
3777         si, vim.Datacenter, datacenter_name
3778     )
3779     if not datacenter_ref:
3780         raise SaltCloudSystemExit("The specified datacenter does not exist.")
3781     try:
3782         datacenter_ref.datastoreFolder.CreateStoragePod(name=datastore_cluster_name)
3783     except Exception as exc:  # pylint: disable=broad-except
3784         log.error(
3785             "Error creating datastore cluster %s: %s",
3786             datastore_cluster_name,
3787             exc,
3788             exc_info_on_loglevel=logging.DEBUG,
3789         )
3790         return False
3791     return {datastore_cluster_name: "created"}
3792 def shutdown_host(kwargs=None, call=None):
3793     """
3794     Shut down the specified host system in this VMware environment
3795     .. note::
3796         If the host system is not in maintenance mode, it will not be shut down. If you
3797         want to shut down the host system regardless of whether it is in maintenance mode,
3798         set ``force=True``. Default is ``force=False``.
3799     CLI Example:
3800     .. code-block:: bash
3801         salt-cloud -f shutdown_host my-vmware-config host="myHostSystemName" [force=True]
3802     """
3803     if call != "function":
3804         raise SaltCloudSystemExit(
3805             "The shutdown_host function must be called with -f or --function."
3806         )
3807     host_name = kwargs.get("host") if kwargs and "host" in kwargs else None
3808     force = _str_to_bool(kwargs.get("force")) if kwargs and "force" in kwargs else False
3809     if not host_name:
3810         raise SaltCloudSystemExit("You must specify name of the host system.")
3811     si = _get_si()
3812     host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)
3813     if not host_ref:
3814         raise SaltCloudSystemExit("Specified host system does not exist.")
3815     if host_ref.runtime.connectionState == "notResponding":
3816         raise SaltCloudSystemExit(
3817             "Specified host system cannot be shut down in it's current state (not"
3818             " responding)."
3819         )
3820     if not host_ref.capability.rebootSupported:
3821         raise SaltCloudSystemExit("Specified host system does not support shutdown.")
3822     if not host_ref.runtime.inMaintenanceMode and not force:
3823         raise SaltCloudSystemExit(
3824             "Specified host system is not in maintenance mode. Specify force=True to"
3825             " force reboot even if there are virtual machines running or other"
3826             " operations in progress."
3827         )
3828     try:
3829         host_ref.ShutdownHost_Task(force)
3830     except Exception as exc:  # pylint: disable=broad-except
3831         log.error(
3832             "Error while shutting down host %s: %s",
3833             host_name,
3834             exc,
3835             exc_info_on_loglevel=logging.DEBUG,
3836         )
3837         return {host_name: "failed to shut down host"}
3838     return {host_name: "shut down host"}
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
