<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for labelarray.py &amp; hdf5_daily_bars.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for labelarray.py &amp; hdf5_daily_bars.py
      </h3>
<h1 align="center">
        1.4%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>labelarray.py (1.5706806%)<th>hdf5_daily_bars.py (1.4101057%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(4-17)<td><a href="#" name="0">(101-119)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>labelarray.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <a name="0"></a>"""
2 An ndarray subclass for working with arrays of strings.
3 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>from functools import partial, total_ordering
4 from operator import eq, ne
5 import re
6 import numpy as np
7 from numpy import ndarray
8 import pandas as pd
9 from toolz import compose
10 from zipline.utils.compat import unicode
11 from zipline.utils.functional import instance
12 from zipline.utils.preprocess import preprocess
13 from zipline.utils.sentinel import sentinel
14 from</b></font> zipline.utils.input_validation import (
15     coerce,
16     expect_kinds,
17     expect_types,
18     optional,
19 )
20 from zipline.utils.numpy_utils import (
21     bool_dtype,
22     unsigned_int_dtype_with_size_in_bytes,
23     is_object,
24     object_dtype,
25 )
26 from zipline.utils.pandas_utils import ignore_pandas_nan_categorical_warning
27 from ._factorize import (
28     factorize_strings,
29     factorize_strings_known_categories,
30     smallest_uint_that_can_hold,
31 )
32 def compare_arrays(left, right):
33     "Eq check with a short-circuit for identical objects."
34     return (
35         left is right
36         or ((left.shape == right.shape) and (left == right).all())
37     )
38 def _make_unsupported_method(name):
39     def method(*args, **kwargs):
40         raise NotImplementedError(
41             "Method %s is not supported on LabelArrays." % name
42         )
43     method.__name__ = name
44     method.__doc__ = "Unsupported LabelArray Method: %s" % name
45     return method
46 class MissingValueMismatch(ValueError):
47     def __init__(self, left, right):
48         super(MissingValueMismatch, self).__init__(
49             "LabelArray missing_values don't match:"
50             " left={}, right={}".format(left, right)
51         )
52 class CategoryMismatch(ValueError):
53     def __init__(self, left, right):
54         (mismatches,) = np.where(left != right)
55         assert len(mismatches), "Not actually a mismatch!"
56         super(CategoryMismatch, self).__init__(
57             "LabelArray categories don't match:\n"
58             "Mismatched Indices: {mismatches}\n"
59             "Left: {left}\n"
60             "Right: {right}".format(
61                 mismatches=mismatches,
62                 left=left[mismatches],
63                 right=right[mismatches],
64             )
65         )
66 _NotPassed = sentinel('_NotPassed')
67 class LabelArray(ndarray):
68     SUPPORTED_SCALAR_TYPES = (bytes, unicode, type(None))
69     SUPPORTED_NON_NONE_SCALAR_TYPES = (bytes, unicode)
70     @preprocess(
71         values=coerce(list, partial(np.asarray, dtype=object)),
72         categories=coerce((list, np.ndarray, set), list),
73     )
74     @expect_types(
75         values=np.ndarray,
76         missing_value=SUPPORTED_SCALAR_TYPES,
77         categories=optional(list),
78     )
79     @expect_kinds(values=("O", "S", "U"))
80     def __new__(cls,
81                 values,
82                 missing_value,
83                 categories=None,
84                 sort=True):
85         if not is_object(values):
86             values = values.astype(object)
87         if values.flags.f_contiguous:
88             ravel_order = 'F'
89         else:
90             ravel_order = 'C'
91         if categories is None:
92             codes, categories, reverse_categories = factorize_strings(
93                 values.ravel(ravel_order),
94                 missing_value=missing_value,
95                 sort=sort,
96             )
97         else:
98             codes, categories, reverse_categories = (
99                 factorize_strings_known_categories(
100                     values.ravel(ravel_order),
101                     categories=categories,
102                     missing_value=missing_value,
103                     sort=sort,
104                 )
105             )
106         categories.setflags(write=False)
107         return cls.from_codes_and_metadata(
108             codes=codes.reshape(values.shape, order=ravel_order),
109             categories=categories,
110             reverse_categories=reverse_categories,
111             missing_value=missing_value,
112         )
113     @classmethod
114     def from_codes_and_metadata(cls,
115                                 codes,
116                                 categories,
117                                 reverse_categories,
118                                 missing_value):
119         ret = codes.view(type=cls, dtype=np.void)
120         ret._categories = categories
121         ret._reverse_categories = reverse_categories
122         ret._missing_value = missing_value
123         return ret
124     @classmethod
125     def from_categorical(cls, categorical, missing_value=None):
126         return LabelArray(
127             categorical,
128             missing_value,
129             categorical.categories,
130         )
131     @property
132     def categories(self):
133         return self._categories
134     @property
135     def reverse_categories(self):
136         return self._reverse_categories
137     @property
138     def missing_value(self):
139         return self._missing_value
140     @property
141     def missing_value_code(self):
142         return self.reverse_categories[self.missing_value]
143     def has_label(self, value):
144         return value in self.reverse_categories
145     def __array_finalize__(self, obj):
146         if obj is None:
147             raise TypeError(
148                 "Direct construction of LabelArrays is not supported."
149             )
150         self._categories = getattr(obj, 'categories', None)
151         self._reverse_categories = getattr(obj, 'reverse_categories', None)
152         self._missing_value = getattr(obj, 'missing_value', None)
153     def as_int_array(self):
154         return self.view(
155             type=ndarray,
156             dtype=unsigned_int_dtype_with_size_in_bytes(self.itemsize),
157         )
158     def as_string_array(self):
159         return self.categories[self.as_int_array()]
160     def as_categorical(self):
161         if len(self.shape) &gt; 1:
162             raise ValueError("Can't convert a 2D array to a categorical.")
163         with ignore_pandas_nan_categorical_warning():
164             return pd.Categorical.from_codes(
165                 self.as_int_array(),
166                 self.categories.copy(),
167                 ordered=False,
168             )
169     def as_categorical_frame(self, index, columns, name=None):
170         if len(self.shape) != 2:
171             raise ValueError(
172                 "Can't convert a non-2D LabelArray into a DataFrame."
173             )
174         expected_shape = (len(index), len(columns))
175         if expected_shape != self.shape:
176             raise ValueError(
177                 "Can't construct a DataFrame with provided indices:\n\n"
178                 "LabelArray shape is {actual}, but index and columns imply "
179                 "that shape should be {expected}.".format(
180                     actual=self.shape,
181                     expected=expected_shape,
182                 )
183             )
184         return pd.Series(
185             index=pd.MultiIndex.from_product([index, columns]),
186             data=self.ravel().as_categorical(),
187             name=name,
188         ).unstack()
189     def __setitem__(self, indexer, value):
190         self_categories = self.categories
191         if isinstance(value, self.SUPPORTED_SCALAR_TYPES):
192             value_code = self.reverse_categories.get(value, None)
193             if value_code is None:
194                 raise ValueError("%r is not in LabelArray categories." % value)
195             self.as_int_array()[indexer] = value_code
196         elif isinstance(value, LabelArray):
197             value_categories = value.categories
198             if compare_arrays(self_categories, value_categories):
199                 return super(LabelArray, self).__setitem__(indexer, value)
200             elif (self.missing_value == value.missing_value and
201                   set(value.categories) &lt;= set(self.categories)):
202                 rhs = LabelArray.from_codes_and_metadata(
203                     *factorize_strings_known_categories(
204                         value.as_string_array().ravel(),
205                         list(self.categories),
206                         self.missing_value,
207                         False,
208                     ),
209                     missing_value=self.missing_value
210                 ).reshape(value.shape)
211                 super(LabelArray, self).__setitem__(indexer, rhs)
212             else:
213                 raise CategoryMismatch(self_categories, value_categories)
214         else:
215             raise NotImplementedError(
216                 "Setting into a LabelArray with a value of "
217                 "type {type} is not yet supported.".format(
218                     type=type(value).__name__,
219                 ),
220             )
221     def set_scalar(self, indexer, value):
222         try:
223             value_code = self.reverse_categories[value]
224         except KeyError:
225             raise ValueError("%r is not in LabelArray categories." % value)
226         self.as_int_array()[indexer] = value_code
227     def __setslice__(self, i, j, sequence):
228         self.__setitem__(slice(i, j), sequence)
229     def __getitem__(self, indexer):
230         result = super(LabelArray, self).__getitem__(indexer)
231         if result.ndim:
232             return result
233         index = result.view(
234             unsigned_int_dtype_with_size_in_bytes(self.itemsize),
235         )
236         return self.categories[index]
237     def is_missing(self):
238         return (
239             self.as_int_array() == self.reverse_categories[self.missing_value]
240         )
241     def not_missing(self):
242         return (
243             self.as_int_array() != self.reverse_categories[self.missing_value]
244         )
245     def _equality_check(op):
246         def method(self, other):
247             if isinstance(other, LabelArray):
248                 self_mv = self.missing_value
249                 other_mv = other.missing_value
250                 if self_mv != other_mv:
251                     raise MissingValueMismatch(self_mv, other_mv)
252                 self_categories = self.categories
253                 other_categories = other.categories
254                 if not compare_arrays(self_categories, other_categories):
255                     raise CategoryMismatch(self_categories, other_categories)
256                 return (
257                     op(self.as_int_array(), other.as_int_array())
258                     &amp; self.not_missing()
259                     &amp; other.not_missing()
260                 )
261             elif isinstance(other, ndarray):
262                 return op(self.as_string_array(), other) &amp; self.not_missing()
263             elif isinstance(other, self.SUPPORTED_SCALAR_TYPES):
264                 i = self._reverse_categories.get(other, -1)
265                 return op(self.as_int_array(), i) &amp; self.not_missing()
266             return op(super(LabelArray, self), other)
267         return method
268     __eq__ = _equality_check(eq)
269     __ne__ = _equality_check(ne)
270     del _equality_check
271     def view(self, dtype=_NotPassed, type=_NotPassed):
272         if type is _NotPassed and dtype not in (_NotPassed, self.dtype):
273             raise TypeError("Can't view LabelArray as another dtype.")
274         kwargs = {}
275         if dtype is not _NotPassed:
276             kwargs['dtype'] = dtype
277         if type is not _NotPassed:
278             kwargs['type'] = type
279         return super(LabelArray, self).view(**kwargs)
280     def astype(self,
281                dtype,
282                order='K',
283                casting='unsafe',
284                subok=True,
285                copy=True):
286         if dtype == self.dtype:
287             if not subok:
288                 array = self.view(type=np.ndarray)
289             else:
290                 array = self
291             if copy:
292                 return array.copy()
293             return array
294         if dtype == object_dtype:
295             return self.as_string_array()
296         if dtype.kind == 'S':
297             return self.as_string_array().astype(
298                 dtype,
299                 order=order,
300                 casting=casting,
301                 subok=subok,
302                 copy=copy,
303             )
304         raise TypeError(
305             '%s can only be converted into object, string, or void,'
306             ' got: %r' % (
307                 type(self).__name__,
308                 dtype,
309             ),
310         )
311     SUPPORTED_NDARRAY_METHODS = frozenset([
312         'astype',
313         'base',
314         'compress',
315         'copy',
316         'data',
317         'diagonal',
318         'dtype',
319         'flat',
320         'flatten',
321         'item',
322         'itemset',
323         'itemsize',
324         'nbytes',
325         'ndim',
326         'ravel',
327         'repeat',
328         'reshape',
329         'resize',
330         'setflags',
331         'shape',
332         'size',
333         'squeeze',
334         'strides',
335         'swapaxes',
336         'take',
337         'trace',
338         'transpose',
339         'view'
340     ])
341     PUBLIC_NDARRAY_METHODS = frozenset([
342         s for s in dir(ndarray) if not s.startswith('_')
343     ])
344     locals().update(
345         {
346             method: _make_unsupported_method(method)
347             for method in PUBLIC_NDARRAY_METHODS - SUPPORTED_NDARRAY_METHODS
348         }
349     )
350     def __repr__(self):
351         repr_lines = repr(self.as_string_array()).splitlines()
352         repr_lines[0] = repr_lines[0].replace('array(', 'LabelArray(', 1)
353         repr_lines[-1] = repr_lines[-1].rsplit(',', 1)[0] + ')'
354         return '\n     '.join(repr_lines)
355     def empty_like(self, shape):
356         return type(self).from_codes_and_metadata(
357             codes=np.full(
358                 shape,
359                 self.reverse_categories[self.missing_value],
360                 dtype=unsigned_int_dtype_with_size_in_bytes(self.itemsize),
361             ),
362             categories=self.categories,
363             reverse_categories=self.reverse_categories,
364             missing_value=self.missing_value,
365         )
366     def map_predicate(self, f):
367         if self.missing_value is None:
368             def f_to_use(x):
369                 return False if x is None else f(x)
370         else:
371             f_to_use = f
372         results = np.vectorize(f_to_use, otypes=[bool_dtype])(self.categories)
373         results[self.reverse_categories[self.missing_value]] = False
374         return results[self.as_int_array()]
375     def map(self, f):
376         if self.missing_value is None:
377             allowed_outtypes = self.SUPPORTED_SCALAR_TYPES
378         else:
379             allowed_outtypes = self.SUPPORTED_NON_NONE_SCALAR_TYPES
380         def f_to_use(x,
381                      missing_value=self.missing_value,
382                      otypes=allowed_outtypes):
383             if x == missing_value:
384                 return _sortable_sentinel
385             ret = f(x)
386             if not isinstance(ret, otypes):
387                 raise TypeError(
388                     "LabelArray.map expected function {f} to return a string"
389                     " or None, but got {type} instead.\n"
390                     "Value was {value}.".format(
391                         f=f.__name__,
392                         type=type(ret).__name__,
393                         value=ret,
394                     )
395                 )
396             if ret == missing_value:
397                 return _sortable_sentinel
398             return ret
399         new_categories_with_duplicates = (
400             np.vectorize(f_to_use, otypes=[object])(self.categories)
401         )
402         new_categories, bloated_inverse_index = np.unique(
403             new_categories_with_duplicates,
404             return_inverse=True
405         )
406         if new_categories[0] is _sortable_sentinel:
407             new_categories[0] = self.missing_value
408         reverse_index = bloated_inverse_index.astype(
409             smallest_uint_that_can_hold(len(new_categories))
410         )
411         new_codes = np.take(reverse_index, self.as_int_array())
412         return self.from_codes_and_metadata(
413             new_codes,
414             new_categories,
415             dict(zip(new_categories, range(len(new_categories)))),
416             missing_value=self.missing_value,
417         )
418     def startswith(self, prefix):
419         return self.map_predicate(lambda elem: elem.startswith(prefix))
420     def endswith(self, suffix):
421         return self.map_predicate(lambda elem: elem.endswith(suffix))
422     def has_substring(self, substring):
423         return self.map_predicate(lambda elem: substring in elem)
424     @preprocess(pattern=coerce(from_=(bytes, unicode), to=re.compile))
425     def matches(self, pattern):
426         return self.map_predicate(compose(bool, pattern.match))
427     @preprocess(container=coerce((list, tuple, np.ndarray), set))
428     def element_of(self, container):
429         return self.map_predicate(container.__contains__)
430 @instance  # This makes _sortable_sentinel a singleton instance.
431 @total_ordering
432 class _sortable_sentinel(object):
433     def __eq__(self, other):
434         return self is other
435     def __lt__(self, other):
436         return True
437 @expect_types(trues=LabelArray, falses=LabelArray)
438 def labelarray_where(cond, trues, falses):
439     if trues.missing_value != falses.missing_value:
440         raise ValueError(
441             "Can't compute where on arrays with different missing values."
442         )
443     strs = np.where(cond, trues.as_string_array(), falses.as_string_array())
444     return LabelArray(strs, missing_value=trues.missing_value)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>hdf5_daily_bars.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from functools import partial
2 import h5py
3 import logbook
4 import numpy as np
5 import pandas as pd
6 from six import iteritems, raise_from, viewkeys
7 from six.moves import reduce
8 from zipline.data.bar_reader import (
9     NoDataAfterDate,
10     NoDataBeforeDate,
11     NoDataForSid,
12     NoDataOnDate,
13 )
14 from zipline.data.session_bars import CurrencyAwareSessionBarReader
15 from zipline.utils.memoize import lazyval
16 from zipline.utils.numpy_utils import bytes_array_to_native_str_object_array
17 from</b></font> zipline.utils.pandas_utils import check_indexes_all_same
18 log = logbook.Logger('HDF5DailyBars')
19 VERSION = 0
20 DATA = 'data'
21 INDEX = 'index'
22 LIFETIMES = 'lifetimes'
23 CURRENCY = 'currency'
24 CODE = 'code'
25 SCALING_FACTOR = 'scaling_factor'
26 OPEN = 'open'
27 HIGH = 'high'
28 LOW = 'low'
29 CLOSE = 'close'
30 VOLUME = 'volume'
31 FIELDS = (OPEN, HIGH, LOW, CLOSE, VOLUME)
32 DAY = 'day'
33 SID = 'sid'
34 START_DATE = 'start_date'
35 END_DATE = 'end_date'
36 MISSING_CURRENCY = 'XXX'
37 DEFAULT_SCALING_FACTORS = {
38     OPEN: 1000,
39     HIGH: 1000,
40     LOW: 1000,
41     CLOSE: 1000,
42     VOLUME: 1,
43 }
44 def coerce_to_uint32(a, scaling_factor):
45     return (a * scaling_factor).round().astype('uint32')
46 def days_and_sids_for_frames(frames):
47     if not frames:
48         days = np.array([], dtype='datetime64[ns]')
49         sids = np.array([], dtype='int64')
50         return days, sids
51     check_indexes_all_same(
52         [frame.index for frame in frames],
53         message='Frames have mismatched days.',
54     )
55     check_indexes_all_same(
56         [frame.columns for frame in frames],
57         message='Frames have mismatched sids.',
58     )
59     return frames[0].index.values, frames[0].columns.values
60 class HDF5DailyBarWriter(object):
61     def __init__(self, filename, date_chunk_size):
62         self._filename = filename
63         self._date_chunk_size = date_chunk_size
64     def h5_file(self, mode):
65         return h5py.File(self._filename, mode)
66     def write(self,
67               country_code,
68               frames,
69               currency_codes=None,
70               scaling_factors=None):
71         if scaling_factors is None:
72             scaling_factors = DEFAULT_SCALING_FACTORS
73         days, sids = days_and_sids_for_frames(list(frames.values()))
74         if currency_codes is None:
75             currency_codes = pd.Series(index=sids, data=MISSING_CURRENCY)
76         check_sids_arrays_match(
77             sids,
78             currency_codes.index.values,
79             message="currency_codes sids do not match data sids:",
80         )
81         start_date_ixs, end_date_ixs = compute_asset_lifetimes(frames)
82         if len(sids):
83             chunks = (len(sids), min(self._date_chunk_size, len(days)))
84         else:
85             chunks = None
86         with self.h5_file(mode='a') as h5_file:
87             h5_file.attrs['version'] = VERSION
88             country_group = h5_file.create_group(country_code)
89             self._write_index_group(country_group, days, sids)
90             self._write_lifetimes_group(
91                 country_group,
92                 start_date_ixs,
93                 end_date_ixs,
94             )
95             self._write_currency_group(country_group, currency_codes)
96             self._write_data_group(
97                 country_group,
98                 frames,
99                 scaling_factors,
100                 chunks,
101             )
102     def write_from_sid_df_pairs(self,
103                                 country_code,
104                                 data,
105                                 currency_codes=None,
106                                 scaling_factors=None):
107         data = list(data)
108         if not data:
109             empty_frame = pd.DataFrame(
110                 data=None,
111                 index=np.array([], dtype='datetime64[ns]'),
112                 columns=np.array([], dtype='int64'),
113             )
114             return self.write(
115                 country_code,
116                 {f: empty_frame.copy() for f in FIELDS},
117                 scaling_factors,
118             )
119         sids, frames = zip(*data)
120         ohlcv_frame = pd.concat(frames)
121         sid_ix = np.repeat(sids, [len(f) for f in frames])
122         ohlcv_frame.set_index(sid_ix, append=True, inplace=True)
123         frames = {
124             field: ohlcv_frame[field].unstack()
125             for field in FIELDS
126         }
127         return self.write(
128             country_code=country_code,
129             frames=frames,
130             scaling_factors=scaling_factors,
131             currency_codes=currency_codes
132         )
133     def _write_index_group(self, country_group, days, sids):
134         index_group = country_group.create_group(INDEX)
135         self._log_writing_dataset(index_group)
136         index_group.create_dataset(SID, data=sids)
137         index_group.create_dataset(DAY, data=days.astype(np.int64))
138     def _write_lifetimes_group(self,
139                                country_group,
140                                start_date_ixs,
141                                end_date_ixs):
142         lifetimes_group = country_group.create_group(LIFETIMES)
143         self._log_writing_dataset(lifetimes_group)
144         lifetimes_group.create_dataset(START_DATE, data=start_date_ixs)
145         lifetimes_group.create_dataset(END_DATE, data=end_date_ixs)
146     def _write_currency_group(self, country_group, currencies):
147         currency_group = country_group.create_group(CURRENCY)
148         self._log_writing_dataset(currency_group)
149         currency_group.create_dataset(
150             CODE,
151             data=currencies.values.astype(dtype='S3'),
152         )
153     def _write_data_group(self,
154                           country_group,
155                           frames,
156                           scaling_factors,
157                           chunks):
158         data_group = country_group.create_group(DATA)
159         self._log_writing_dataset(data_group)
160         for field in FIELDS:
161             frame = frames[field]
162             frame.sort_index(inplace=True)
163             frame.sort_index(axis='columns', inplace=True)
164             data = coerce_to_uint32(
165                 frame.T.fillna(0).values,
166                 scaling_factors[field],
167             )
168             dataset = data_group.create_dataset(
169                 field,
170                 compression='lzf',
171                 shuffle=True,
172                 data=data,
173                 chunks=chunks,
174             )
175             self._log_writing_dataset(dataset)
176             dataset.attrs[SCALING_FACTOR] = scaling_factors[field]
177             log.debug(
178                 'Writing dataset {} to file {}',
179                 dataset.name, self._filename
180             )
181     def _log_writing_dataset(self, dataset):
182         log.debug("Writing {} to file {}", dataset.name, self._filename)
183 def compute_asset_lifetimes(frames):
184     is_null_matrix = np.logical_and.reduce(
185         [frames[field].isnull().values for field in FIELDS],
186     )
187     if not is_null_matrix.size:
188         empty = np.array([], dtype='int64')
189         return empty, empty.copy()
190     start_date_ixs = is_null_matrix.argmin(axis=0)
191     end_offsets = is_null_matrix[::-1].argmin(axis=0)
192     end_date_ixs = is_null_matrix.shape[0] - end_offsets - 1
193     return start_date_ixs, end_date_ixs
194 def convert_price_with_scaling_factor(a, scaling_factor):
195     conversion_factor = (1.0 / scaling_factor)
196     zeroes = (a == 0)
197     return np.where(zeroes, np.nan, a.astype('float64')) * conversion_factor
198 class HDF5DailyBarReader(CurrencyAwareSessionBarReader):
199     def __init__(self, country_group):
200         self._country_group = country_group
201         self._postprocessors = {
202             OPEN: partial(convert_price_with_scaling_factor,
203                           scaling_factor=self._read_scaling_factor(OPEN)),
204             HIGH: partial(convert_price_with_scaling_factor,
205                           scaling_factor=self._read_scaling_factor(HIGH)),
206             LOW: partial(convert_price_with_scaling_factor,
207                          scaling_factor=self._read_scaling_factor(LOW)),
208             CLOSE: partial(convert_price_with_scaling_factor,
209                            scaling_factor=self._read_scaling_factor(CLOSE)),
210             VOLUME: lambda a: a,
211         }
212     @classmethod
213     def from_file(cls, h5_file, country_code):
214         if h5_file.attrs['version'] != VERSION:
215             raise ValueError(
216                 'mismatched version: file is of version %s, expected %s' % (
217                     h5_file.attrs['version'],
218                     VERSION,
219                 ),
220             )
221         return cls(h5_file[country_code])
222     @classmethod
223     def from_path(cls, path, country_code):
224         return cls.from_file(h5py.File(path), country_code)
225     def _read_scaling_factor(self, field):
226         return self._country_group[DATA][field].attrs[SCALING_FACTOR]
227     def load_raw_arrays(self,
228                         columns,
229                         start_date,
230                         end_date,
231                         assets):
232         self._validate_timestamp(start_date)
233         self._validate_timestamp(end_date)
234         start = start_date.asm8
235         end = end_date.asm8
236         date_slice = self._compute_date_range_slice(start, end)
237         n_dates = date_slice.stop - date_slice.start
238         full_buf = np.zeros((len(self.sids) + 1, n_dates), dtype=np.uint32)
239         mutable_buf = full_buf[:-1]
240         sid_selector = self._make_sid_selector(assets)
241         out = []
242         for column in columns:
243             mutable_buf.fill(0)
244             dataset = self._country_group[DATA][column]
245             dataset.read_direct(
246                 mutable_buf,
247                 np.s_[:, date_slice],
248             )
249             out.append(self._postprocessors[column](full_buf[sid_selector].T))
250         return out
251     def _make_sid_selector(self, assets):
252         assets = np.array(assets)
253         sid_selector = self.sids.searchsorted(assets)
254         unknown = np.in1d(assets, self.sids, invert=True)
255         sid_selector[unknown] = -1
256         return sid_selector
257     def _compute_date_range_slice(self, start_date, end_date):
258         start_ix = self.dates.searchsorted(start_date)
259         end_ix = self.dates.searchsorted(end_date, side='right')
260         return slice(start_ix, end_ix)
261     def _validate_assets(self, assets):
262         missing_sids = np.setdiff1d(assets, self.sids)
263         if len(missing_sids):
264             raise NoDataForSid(
265                 'Assets not contained in daily pricing file: {}'.format(
266                     missing_sids
267                 )
268             )
269     def _validate_timestamp(self, ts):
270         if ts.asm8 not in self.dates:
271             raise NoDataOnDate(ts)
272     @lazyval
273     def dates(self):
274         return self._country_group[INDEX][DAY][:].astype('datetime64[ns]')
275     @lazyval
276     def sids(self):
277         return self._country_group[INDEX][SID][:].astype('int64', copy=False)
278     @lazyval
279     def asset_start_dates(self):
280         return self.dates[self._country_group[LIFETIMES][START_DATE][:]]
281     @lazyval
282     def asset_end_dates(self):
283         return self.dates[self._country_group[LIFETIMES][END_DATE][:]]
284     @lazyval
285     def _currency_codes(self):
286         bytes_array = self._country_group[CURRENCY][CODE][:]
287         return bytes_array_to_native_str_object_array(bytes_array)
288     def currency_codes(self, sids):
289         ixs = self.sids.searchsorted(sids, side='left')
290         result = self._currency_codes[ixs]
291         not_found = (self.sids[ixs] != sids)
292         result[not_found] = None
293         return result
294     @property
295     def last_available_dt(self):
296         return pd.Timestamp(self.dates[-1], tz='UTC')
297     @property
298     def trading_calendar(self):
299         raise NotImplementedError(
300             'HDF5 pricing does not yet support trading calendars.'
301         )
302     @property
303     def first_trading_day(self):
304         return pd.Timestamp(self.dates[0], tz='UTC')
305     @lazyval
306     def sessions(self):
307         return pd.to_datetime(self.dates, utc=True)
308     def get_value(self, sid, dt, field):
309         self._validate_assets([sid])
310         self._validate_timestamp(dt)
311         sid_ix = self.sids.searchsorted(sid)
312         dt_ix = self.dates.searchsorted(dt.asm8)
313         value = self._postprocessors[field](
314             self._country_group[DATA][field][sid_ix, dt_ix]
315         )
316         if np.isnan(value):
317             if dt.asm8 &lt; self.asset_start_dates[sid_ix]:
318                 raise NoDataBeforeDate()
319             if dt.asm8 &gt; self.asset_end_dates[sid_ix]:
320                 raise NoDataAfterDate()
321         return value
322     def get_last_traded_dt(self, asset, dt):
323         sid_ix = self.sids.searchsorted(asset.sid)
324         dt_limit_ix = self.dates.searchsorted(dt.asm8, side='right')
325         nonzero_volume_ixs = np.ravel(
326             np.nonzero(self._country_group[DATA][VOLUME][sid_ix, :dt_limit_ix])
327         )
328         if len(nonzero_volume_ixs) == 0:
329             return pd.NaT
330         return pd.Timestamp(self.dates[nonzero_volume_ixs][-1], tz='UTC')
331 class MultiCountryDailyBarReader(CurrencyAwareSessionBarReader):
332     def __init__(self, readers):
333         self._readers = readers
334         self._country_map = pd.concat([
335             pd.Series(index=reader.sids, data=country_code)
336             for country_code, reader in iteritems(readers)
337         ])
338     @classmethod
339     def from_file(cls, h5_file):
340         return cls({
341             country: HDF5DailyBarReader.from_file(h5_file, country)
342             for country in h5_file.keys()
343         })
344     @classmethod
345     def from_path(cls, path):
346         return cls.from_file(h5py.File(path))
347     @property
348     def countries(self):
349         return viewkeys(self._readers)
350     def _country_code_for_assets(self, assets):
351         country_codes = self._country_map.get(assets)
352         if country_codes is not None:
353             unique_country_codes = country_codes.dropna().unique()
354             num_countries = len(unique_country_codes)
355         else:
356             num_countries = 0
357         if num_countries == 0:
358             raise ValueError('At least one valid asset id is required.')
359         elif num_countries &gt; 1:
360             raise NotImplementedError(
361                 (
362                     'Assets were requested from multiple countries ({}),'
363                     ' but multi-country reads are not yet supported.'
364                 ).format(list(unique_country_codes))
365             )
366         return np.asscalar(unique_country_codes)
367     def load_raw_arrays(self,
368                         columns,
369                         start_date,
370                         end_date,
371                         assets):
372         country_code = self._country_code_for_assets(assets)
373         return self._readers[country_code].load_raw_arrays(
374             columns,
375             start_date,
376             end_date,
377             assets,
378         )
379     @property
380     def last_available_dt(self):
381         return max(
382             reader.last_available_dt for reader in self._readers.values()
383         )
384     @property
385     def trading_calendar(self):
386         raise NotImplementedError(
387             'HDF5 pricing does not yet support trading calendars.'
388         )
389     @property
390     def first_trading_day(self):
391         return min(
392             reader.first_trading_day for reader in self._readers.values()
393         )
394     @property
395     def sessions(self):
396         return pd.to_datetime(
397             reduce(
398                 np.union1d,
399                 (reader.dates for reader in self._readers.values()),
400             ),
401             utc=True,
402         )
403     def get_value(self, sid, dt, field):
404         try:
405             country_code = self._country_code_for_assets([sid])
406         except ValueError as exc:
407             raise_from(
408                 NoDataForSid(
409                     'Asset not contained in daily pricing file: {}'.format(sid)
410                 ),
411                 exc
412             )
413         return self._readers[country_code].get_value(sid, dt, field)
414     def get_last_traded_dt(self, asset, dt):
415         country_code = self._country_code_for_assets([asset.sid])
416         return self._readers[country_code].get_last_traded_dt(asset, dt)
417     def currency_codes(self, sids):
418         country_code = self._country_code_for_assets(sids)
419         return self._readers[country_code].currency_codes(sids)
420 def check_sids_arrays_match(left, right, message):
421     if len(left) != len(right):
422         raise ValueError(
423             "{}:\nlen(left) ({}) != len(right) ({})".format(
424                 message, len(left), len(right)
425             )
426         )
427     diff = (left != right)
428     if diff.any():
429         (bad_locs,) = np.where(diff)
430         raise ValueError(
431             "{}:\n Indices with differences: {}".format(message, bad_locs)
432         )
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
