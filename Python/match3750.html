<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for labelarray.py &amp; hdf5_daily_bars.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for labelarray.py &amp; hdf5_daily_bars.py
      </h3>
<h1 align="center">
        1.4%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>labelarray.py (1.5706806%)<th>hdf5_daily_bars.py (1.4101057%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(4-17)<td><a href="#" name="0">(101-119)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>labelarray.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
An ndarray subclass for working with arrays of strings.
"""
<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>from functools import partial, total_ordering
from operator import eq, ne
import re
import numpy as np
from numpy import ndarray
import pandas as pd
from toolz import compose
from zipline.utils.compat import unicode
from zipline.utils.functional import instance
from zipline.utils.preprocess import preprocess
from zipline.utils.sentinel import sentinel
from</b></font> zipline.utils.input_validation import (
    coerce,
    expect_kinds,
    expect_types,
    optional,
)
from zipline.utils.numpy_utils import (
    bool_dtype,
    unsigned_int_dtype_with_size_in_bytes,
    is_object,
    object_dtype,
)
from zipline.utils.pandas_utils import ignore_pandas_nan_categorical_warning
from ._factorize import (
    factorize_strings,
    factorize_strings_known_categories,
    smallest_uint_that_can_hold,
)
def compare_arrays(left, right):
    "Eq check with a short-circuit for identical objects."
    return (
        left is right
        or ((left.shape == right.shape) and (left == right).all())
    )
def _make_unsupported_method(name):
    def method(*args, **kwargs):
        raise NotImplementedError(
            "Method %s is not supported on LabelArrays." % name
        )
    method.__name__ = name
    method.__doc__ = "Unsupported LabelArray Method: %s" % name
    return method
class MissingValueMismatch(ValueError):
    """
    Error raised on attempt to perform operations between LabelArrays with
    mismatched missing_values.
    """
    def __init__(self, left, right):
        super(MissingValueMismatch, self).__init__(
            "LabelArray missing_values don't match:"
            " left={}, right={}".format(left, right)
        )
class CategoryMismatch(ValueError):
    """
    Error raised on attempt to perform operations between LabelArrays with
    mismatched category arrays.
    """
    def __init__(self, left, right):
        (mismatches,) = np.where(left != right)
        assert len(mismatches), "Not actually a mismatch!"
        super(CategoryMismatch, self).__init__(
            "LabelArray categories don't match:\n"
            "Mismatched Indices: {mismatches}\n"
            "Left: {left}\n"
            "Right: {right}".format(
                mismatches=mismatches,
                left=left[mismatches],
                right=right[mismatches],
            )
        )
_NotPassed = sentinel('_NotPassed')
class LabelArray(ndarray):
    """
    An ndarray subclass for working with arrays of strings.
    Factorizes the input array into integers, but overloads equality on strings
    to check against the factor label.
    Parameters
    ----------
    values : array-like
        Array of values that can be passed to np.asarray with dtype=object.
    missing_value : str
        Scalar value to treat as 'missing' for operations on ``self``.
    categories : list[str], optional
        List of values to use as categories.  If not supplied, categories will
        be inferred as the unique set of entries in ``values``.
    sort : bool, optional
        Whether to sort categories.  If sort is False and categories is
        supplied, they are left in the order provided.  If sort is False and
        categories is None, categories will be constructed in a random order.
    Attributes
    ----------
    categories : ndarray[str]
        An array containing the unique labels of self.
    reverse_categories : dict[str -&gt; int]
        Reverse lookup table for ``categories``. Stores the index in
        ``categories`` at which each entry each unique entry is found.
    missing_value : str or None
        A sentinel missing value with NaN semantics for comparisons.
    Notes
    -----
    Consumers should be cautious when passing instances of LabelArray to numpy
    functions.  We attempt to disallow as many meaningless operations as
    possible, but since a LabelArray is just an ndarray of ints with some
    additional metadata, many numpy functions (for example, trigonometric) will
    happily accept a LabelArray and treat its values as though they were
    integers.
    In a future change, we may be able to disallow more numerical operations by
    creating a wrapper dtype which doesn't register an implementation for most
    numpy ufuncs. Until that change is made, consumers of LabelArray should
    assume that it is undefined behavior to pass a LabelArray to any numpy
    ufunc that operates on semantically-numerical data.
    See Also
    --------
    https://docs.scipy.org/doc/numpy-1.11.0/user/basics.subclassing.html
    """
    SUPPORTED_SCALAR_TYPES = (bytes, unicode, type(None))
    SUPPORTED_NON_NONE_SCALAR_TYPES = (bytes, unicode)
    @preprocess(
        values=coerce(list, partial(np.asarray, dtype=object)),
        categories=coerce((list, np.ndarray, set), list),
    )
    @expect_types(
        values=np.ndarray,
        missing_value=SUPPORTED_SCALAR_TYPES,
        categories=optional(list),
    )
    @expect_kinds(values=("O", "S", "U"))
    def __new__(cls,
                values,
                missing_value,
                categories=None,
                sort=True):
        if not is_object(values):
            values = values.astype(object)
        if values.flags.f_contiguous:
            ravel_order = 'F'
        else:
            ravel_order = 'C'
        if categories is None:
            codes, categories, reverse_categories = factorize_strings(
                values.ravel(ravel_order),
                missing_value=missing_value,
                sort=sort,
            )
        else:
            codes, categories, reverse_categories = (
                factorize_strings_known_categories(
                    values.ravel(ravel_order),
                    categories=categories,
                    missing_value=missing_value,
                    sort=sort,
                )
            )
        categories.setflags(write=False)
        return cls.from_codes_and_metadata(
            codes=codes.reshape(values.shape, order=ravel_order),
            categories=categories,
            reverse_categories=reverse_categories,
            missing_value=missing_value,
        )
    @classmethod
    def from_codes_and_metadata(cls,
                                codes,
                                categories,
                                reverse_categories,
                                missing_value):
        """
        Rehydrate a LabelArray from the codes and metadata.
        Parameters
        ----------
        codes : np.ndarray[integral]
            The codes for the label array.
        categories : np.ndarray[object]
            The unique string categories.
        reverse_categories : dict[str, int]
            The mapping from category to its code-index.
        missing_value : any
            The value used to represent missing data.
        """
        ret = codes.view(type=cls, dtype=np.void)
        ret._categories = categories
        ret._reverse_categories = reverse_categories
        ret._missing_value = missing_value
        return ret
    @classmethod
    def from_categorical(cls, categorical, missing_value=None):
        """
        Create a LabelArray from a pandas categorical.
        Parameters
        ----------
        categorical : pd.Categorical
            The categorical object to convert.
        missing_value : bytes, unicode, or None, optional
            The missing value to use for this LabelArray.
        Returns
        -------
        la : LabelArray
            The LabelArray representation of this categorical.
        """
        return LabelArray(
            categorical,
            missing_value,
            categorical.categories,
        )
    @property
    def categories(self):
        return self._categories
    @property
    def reverse_categories(self):
        return self._reverse_categories
    @property
    def missing_value(self):
        return self._missing_value
    @property
    def missing_value_code(self):
        return self.reverse_categories[self.missing_value]
    def has_label(self, value):
        return value in self.reverse_categories
    def __array_finalize__(self, obj):
        """
        Called by Numpy after array construction.
        There are three cases where this can happen:
        1. Someone tries to directly construct a new array by doing::
            &gt;&gt;&gt; ndarray.__new__(LabelArray, ...)  # doctest: +SKIP
           In this case, obj will be None.  We treat this as an error case and
           fail.
        2. Someone (most likely our own __new__) does::
           &gt;&gt;&gt; other_array.view(type=LabelArray)  # doctest: +SKIP
           In this case, `self` will be the new LabelArray instance, and
           ``obj` will be the array on which ``view`` is being called.
           The caller of ``obj.view`` is responsible for setting category
           metadata on ``self`` after we exit.
        3. Someone creates a new LabelArray by slicing an existing one.
           In this case, ``obj`` will be the original LabelArray.  We're
           responsible for copying over the parent array's category metadata.
        """
        if obj is None:
            raise TypeError(
                "Direct construction of LabelArrays is not supported."
            )
        self._categories = getattr(obj, 'categories', None)
        self._reverse_categories = getattr(obj, 'reverse_categories', None)
        self._missing_value = getattr(obj, 'missing_value', None)
    def as_int_array(self):
        """
        Convert self into a regular ndarray of ints.
        This is an O(1) operation. It does not copy the underlying data.
        """
        return self.view(
            type=ndarray,
            dtype=unsigned_int_dtype_with_size_in_bytes(self.itemsize),
        )
    def as_string_array(self):
        """
        Convert self back into an array of strings.
        This is an O(N) operation.
        """
        return self.categories[self.as_int_array()]
    def as_categorical(self):
        """
        Coerce self into a pandas categorical.
        This is only defined on 1D arrays, since that's all pandas supports.
        """
        if len(self.shape) &gt; 1:
            raise ValueError("Can't convert a 2D array to a categorical.")
        with ignore_pandas_nan_categorical_warning():
            return pd.Categorical.from_codes(
                self.as_int_array(),
                self.categories.copy(),
                ordered=False,
            )
    def as_categorical_frame(self, index, columns, name=None):
        """
        Coerce self into a pandas DataFrame of Categoricals.
        """
        if len(self.shape) != 2:
            raise ValueError(
                "Can't convert a non-2D LabelArray into a DataFrame."
            )
        expected_shape = (len(index), len(columns))
        if expected_shape != self.shape:
            raise ValueError(
                "Can't construct a DataFrame with provided indices:\n\n"
                "LabelArray shape is {actual}, but index and columns imply "
                "that shape should be {expected}.".format(
                    actual=self.shape,
                    expected=expected_shape,
                )
            )
        return pd.Series(
            index=pd.MultiIndex.from_product([index, columns]),
            data=self.ravel().as_categorical(),
            name=name,
        ).unstack()
    def __setitem__(self, indexer, value):
        self_categories = self.categories
        if isinstance(value, self.SUPPORTED_SCALAR_TYPES):
            value_code = self.reverse_categories.get(value, None)
            if value_code is None:
                raise ValueError("%r is not in LabelArray categories." % value)
            self.as_int_array()[indexer] = value_code
        elif isinstance(value, LabelArray):
            value_categories = value.categories
            if compare_arrays(self_categories, value_categories):
                return super(LabelArray, self).__setitem__(indexer, value)
            elif (self.missing_value == value.missing_value and
                  set(value.categories) &lt;= set(self.categories)):
                rhs = LabelArray.from_codes_and_metadata(
                    *factorize_strings_known_categories(
                        value.as_string_array().ravel(),
                        list(self.categories),
                        self.missing_value,
                        False,
                    ),
                    missing_value=self.missing_value
                ).reshape(value.shape)
                super(LabelArray, self).__setitem__(indexer, rhs)
            else:
                raise CategoryMismatch(self_categories, value_categories)
        else:
            raise NotImplementedError(
                "Setting into a LabelArray with a value of "
                "type {type} is not yet supported.".format(
                    type=type(value).__name__,
                ),
            )
    def set_scalar(self, indexer, value):
        """
        Set scalar value into the array.
        Parameters
        ----------
        indexer : any
            The indexer to set the value at.
        value : str
            The value to assign at the given locations.
        Raises
        ------
        ValueError
            Raised when ``value`` is not a value element of this this label
            array.
        """
        try:
            value_code = self.reverse_categories[value]
        except KeyError:
            raise ValueError("%r is not in LabelArray categories." % value)
        self.as_int_array()[indexer] = value_code
    def __setslice__(self, i, j, sequence):
        """
        This method was deprecated in Python 2.0. It predates slice objects,
        but Python 2.7.11 still uses it if you implement it, which ndarray
        does.  In newer Pythons, __setitem__ is always called, but we need to
        manuallly forward in py2.
        """
        self.__setitem__(slice(i, j), sequence)
    def __getitem__(self, indexer):
        result = super(LabelArray, self).__getitem__(indexer)
        if result.ndim:
            return result
        index = result.view(
            unsigned_int_dtype_with_size_in_bytes(self.itemsize),
        )
        return self.categories[index]
    def is_missing(self):
        """
        Like isnan, but checks for locations where we store missing values.
        """
        return (
            self.as_int_array() == self.reverse_categories[self.missing_value]
        )
    def not_missing(self):
        """
        Like ~isnan, but checks for locations where we store missing values.
        """
        return (
            self.as_int_array() != self.reverse_categories[self.missing_value]
        )
    def _equality_check(op):
        """
        Shared code for __eq__ and __ne__, parameterized on the actual
        comparison operator to use.
        """
        def method(self, other):
            if isinstance(other, LabelArray):
                self_mv = self.missing_value
                other_mv = other.missing_value
                if self_mv != other_mv:
                    raise MissingValueMismatch(self_mv, other_mv)
                self_categories = self.categories
                other_categories = other.categories
                if not compare_arrays(self_categories, other_categories):
                    raise CategoryMismatch(self_categories, other_categories)
                return (
                    op(self.as_int_array(), other.as_int_array())
                    &amp; self.not_missing()
                    &amp; other.not_missing()
                )
            elif isinstance(other, ndarray):
                return op(self.as_string_array(), other) &amp; self.not_missing()
            elif isinstance(other, self.SUPPORTED_SCALAR_TYPES):
                i = self._reverse_categories.get(other, -1)
                return op(self.as_int_array(), i) &amp; self.not_missing()
            return op(super(LabelArray, self), other)
        return method
    __eq__ = _equality_check(eq)
    __ne__ = _equality_check(ne)
    del _equality_check
    def view(self, dtype=_NotPassed, type=_NotPassed):
        if type is _NotPassed and dtype not in (_NotPassed, self.dtype):
            raise TypeError("Can't view LabelArray as another dtype.")
        kwargs = {}
        if dtype is not _NotPassed:
            kwargs['dtype'] = dtype
        if type is not _NotPassed:
            kwargs['type'] = type
        return super(LabelArray, self).view(**kwargs)
    def astype(self,
               dtype,
               order='K',
               casting='unsafe',
               subok=True,
               copy=True):
        if dtype == self.dtype:
            if not subok:
                array = self.view(type=np.ndarray)
            else:
                array = self
            if copy:
                return array.copy()
            return array
        if dtype == object_dtype:
            return self.as_string_array()
        if dtype.kind == 'S':
            return self.as_string_array().astype(
                dtype,
                order=order,
                casting=casting,
                subok=subok,
                copy=copy,
            )
        raise TypeError(
            '%s can only be converted into object, string, or void,'
            ' got: %r' % (
                type(self).__name__,
                dtype,
            ),
        )
    SUPPORTED_NDARRAY_METHODS = frozenset([
        'astype',
        'base',
        'compress',
        'copy',
        'data',
        'diagonal',
        'dtype',
        'flat',
        'flatten',
        'item',
        'itemset',
        'itemsize',
        'nbytes',
        'ndim',
        'ravel',
        'repeat',
        'reshape',
        'resize',
        'setflags',
        'shape',
        'size',
        'squeeze',
        'strides',
        'swapaxes',
        'take',
        'trace',
        'transpose',
        'view'
    ])
    PUBLIC_NDARRAY_METHODS = frozenset([
        s for s in dir(ndarray) if not s.startswith('_')
    ])
    locals().update(
        {
            method: _make_unsupported_method(method)
            for method in PUBLIC_NDARRAY_METHODS - SUPPORTED_NDARRAY_METHODS
        }
    )
    def __repr__(self):
        repr_lines = repr(self.as_string_array()).splitlines()
        repr_lines[0] = repr_lines[0].replace('array(', 'LabelArray(', 1)
        repr_lines[-1] = repr_lines[-1].rsplit(',', 1)[0] + ')'
        return '\n     '.join(repr_lines)
    def empty_like(self, shape):
        """
        Make an empty LabelArray with the same categories as ``self``, filled
        with ``self.missing_value``.
        """
        return type(self).from_codes_and_metadata(
            codes=np.full(
                shape,
                self.reverse_categories[self.missing_value],
                dtype=unsigned_int_dtype_with_size_in_bytes(self.itemsize),
            ),
            categories=self.categories,
            reverse_categories=self.reverse_categories,
            missing_value=self.missing_value,
        )
    def map_predicate(self, f):
        """
        Map a function from str -&gt; bool element-wise over ``self``.
        ``f`` will be applied exactly once to each non-missing unique value in
        ``self``. Missing values will always return False.
        """
        if self.missing_value is None:
            def f_to_use(x):
                return False if x is None else f(x)
        else:
            f_to_use = f
        results = np.vectorize(f_to_use, otypes=[bool_dtype])(self.categories)
        results[self.reverse_categories[self.missing_value]] = False
        return results[self.as_int_array()]
    def map(self, f):
        """
        Map a function from str -&gt; str element-wise over ``self``.
        ``f`` will be applied exactly once to each non-missing unique value in
        ``self``. Missing values will always map to ``self.missing_value``.
        """
        if self.missing_value is None:
            allowed_outtypes = self.SUPPORTED_SCALAR_TYPES
        else:
            allowed_outtypes = self.SUPPORTED_NON_NONE_SCALAR_TYPES
        def f_to_use(x,
                     missing_value=self.missing_value,
                     otypes=allowed_outtypes):
            if x == missing_value:
                return _sortable_sentinel
            ret = f(x)
            if not isinstance(ret, otypes):
                raise TypeError(
                    "LabelArray.map expected function {f} to return a string"
                    " or None, but got {type} instead.\n"
                    "Value was {value}.".format(
                        f=f.__name__,
                        type=type(ret).__name__,
                        value=ret,
                    )
                )
            if ret == missing_value:
                return _sortable_sentinel
            return ret
        new_categories_with_duplicates = (
            np.vectorize(f_to_use, otypes=[object])(self.categories)
        )
        new_categories, bloated_inverse_index = np.unique(
            new_categories_with_duplicates,
            return_inverse=True
        )
        if new_categories[0] is _sortable_sentinel:
            new_categories[0] = self.missing_value
        reverse_index = bloated_inverse_index.astype(
            smallest_uint_that_can_hold(len(new_categories))
        )
        new_codes = np.take(reverse_index, self.as_int_array())
        return self.from_codes_and_metadata(
            new_codes,
            new_categories,
            dict(zip(new_categories, range(len(new_categories)))),
            missing_value=self.missing_value,
        )
    def startswith(self, prefix):
        """
        Element-wise startswith.
        Parameters
        ----------
        prefix : str
        Returns
        -------
        matches : np.ndarray[bool]
            An array with the same shape as self indicating whether each
            element of self started with ``prefix``.
        """
        return self.map_predicate(lambda elem: elem.startswith(prefix))
    def endswith(self, suffix):
        """
        Elementwise endswith.
        Parameters
        ----------
        suffix : str
        Returns
        -------
        matches : np.ndarray[bool]
            An array with the same shape as self indicating whether each
            element of self ended with ``suffix``
        """
        return self.map_predicate(lambda elem: elem.endswith(suffix))
    def has_substring(self, substring):
        """
        Elementwise contains.
        Parameters
        ----------
        substring : str
        Returns
        -------
        matches : np.ndarray[bool]
            An array with the same shape as self indicating whether each
            element of self ended with ``suffix``.
        """
        return self.map_predicate(lambda elem: substring in elem)
    @preprocess(pattern=coerce(from_=(bytes, unicode), to=re.compile))
    def matches(self, pattern):
        """
        Elementwise regex match.
        Parameters
        ----------
        pattern : str or compiled regex
        Returns
        -------
        matches : np.ndarray[bool]
            An array with the same shape as self indicating whether each
            element of self was matched by ``pattern``.
        """
        return self.map_predicate(compose(bool, pattern.match))
    @preprocess(container=coerce((list, tuple, np.ndarray), set))
    def element_of(self, container):
        """
        Check if each element of self is an of ``container``.
        Parameters
        ----------
        container : object
            An object implementing a __contains__ to call on each element of
            ``self``.
        Returns
        -------
        is_contained : np.ndarray[bool]
            An array with the same shape as self indicating whether each
            element of self was an element of ``container``.
        """
        return self.map_predicate(container.__contains__)
@instance  # This makes _sortable_sentinel a singleton instance.
@total_ordering
class _sortable_sentinel(object):
    """Dummy object that sorts before any other python object.
    """
    def __eq__(self, other):
        return self is other
    def __lt__(self, other):
        return True
@expect_types(trues=LabelArray, falses=LabelArray)
def labelarray_where(cond, trues, falses):
    """LabelArray-aware implementation of np.where.
    """
    if trues.missing_value != falses.missing_value:
        raise ValueError(
            "Can't compute where on arrays with different missing values."
        )
    strs = np.where(cond, trues.as_string_array(), falses.as_string_array())
    return LabelArray(strs, missing_value=trues.missing_value)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>hdf5_daily_bars.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
"""
HDF5 Pricing File Format
------------------------
At the top level, the file is keyed by country (to support regional
files containing multiple countries).
Within each country, there are 4 subgroups:
``/data``
^^^^^^^^^
Each field (OHLCV) is stored in a dataset as a 2D array, with a row per
sid and a column per session. This differs from the more standard
orientation of dates x sids, because it allows each compressed block to
contain contiguous values for the same sid, which allows for better
compression.
.. code-block:: none
   /data
     /open
     /high
     /low
     /close
     /volume
``/index``
^^^^^^^^^^
Contains two datasets, the index of sids (aligned to the rows of the
OHLCV 2D arrays) and index of sessions (aligned to the columns of the
OHLCV 2D arrays) to use for lookups.
.. code-block:: none
   /index
     /sid
     /day
``/lifetimes``
^^^^^^^^^^^^^^
Contains two datasets, start_date and end_date, defining the lifetime
for each asset, aligned to the sids index.
.. code-block:: none
   /lifetimes
     /start_date
     /end_date
``/currency``
^^^^^^^^^^^^^
Contains a single dataset, ``code``, aligned to the sids index, which contains
the listing currency of each sid.
Example
^^^^^^^
Sample layout of the full file with multiple countries.
.. code-block:: none
   |- /US
   |  |- /data
   |  |  |- /open
   |  |  |- /high
   |  |  |- /low
   |  |  |- /close
   |  |  |- /volume
   |  |
   |  |- /index
   |  |  |- /sid
   |  |  |- /day
   |  |
   |  |- /lifetimes
   |  |  |- /start_date
   |  |  |- /end_date
   |  |
   |  |- /currency
   |     |- /code
   |
   |- /CA
      |- /data
      |  |- /open
      |  |- /high
      |  |- /low
      |  |- /close
      |  |- /volume
      |
      |- /index
      |  |- /sid
      |  |- /day
      |
      |- /lifetimes
      |  |- /start_date
      |  |- /end_date
      |
      |- /currency
         |- /code
<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from functools import partial
import h5py
import logbook
import numpy as np
import pandas as pd
from six import iteritems, raise_from, viewkeys
from six.moves import reduce
from zipline.data.bar_reader import (
    NoDataAfterDate,
    NoDataBeforeDate,
    NoDataForSid,
    NoDataOnDate,
)
from zipline.data.session_bars import CurrencyAwareSessionBarReader
from zipline.utils.memoize import lazyval
from zipline.utils.numpy_utils import bytes_array_to_native_str_object_array
from</b></font> zipline.utils.pandas_utils import check_indexes_all_same
log = logbook.Logger('HDF5DailyBars')
VERSION = 0
DATA = 'data'
INDEX = 'index'
LIFETIMES = 'lifetimes'
CURRENCY = 'currency'
CODE = 'code'
SCALING_FACTOR = 'scaling_factor'
OPEN = 'open'
HIGH = 'high'
LOW = 'low'
CLOSE = 'close'
VOLUME = 'volume'
FIELDS = (OPEN, HIGH, LOW, CLOSE, VOLUME)
DAY = 'day'
SID = 'sid'
START_DATE = 'start_date'
END_DATE = 'end_date'
MISSING_CURRENCY = 'XXX'
DEFAULT_SCALING_FACTORS = {
    OPEN: 1000,
    HIGH: 1000,
    LOW: 1000,
    CLOSE: 1000,
    VOLUME: 1,
}
def coerce_to_uint32(a, scaling_factor):
    """
    Returns a copy of the array as uint32, applying a scaling factor to
    maintain precision if supplied.
    """
    return (a * scaling_factor).round().astype('uint32')
def days_and_sids_for_frames(frames):
    """
    Returns the date index and sid columns shared by a list of dataframes,
    ensuring they all match.
    Parameters
    ----------
    frames : list[pd.DataFrame]
        A list of dataframes indexed by day, with a column per sid.
    Returns
    -------
    days : np.array[datetime64[ns]]
        The days in these dataframes.
    sids : np.array[int64]
        The sids in these dataframes.
    Raises
    ------
    ValueError
        If the dataframes passed are not all indexed by the same days
        and sids.
    """
    if not frames:
        days = np.array([], dtype='datetime64[ns]')
        sids = np.array([], dtype='int64')
        return days, sids
    check_indexes_all_same(
        [frame.index for frame in frames],
        message='Frames have mismatched days.',
    )
    check_indexes_all_same(
        [frame.columns for frame in frames],
        message='Frames have mismatched sids.',
    )
    return frames[0].index.values, frames[0].columns.values
class HDF5DailyBarWriter(object):
    """
    Class capable of writing daily OHLCV data to disk in a format that
    can be read efficiently by HDF5DailyBarReader.
    Parameters
    ----------
    filename : str
        The location at which we should write our output.
    date_chunk_size : int
        The number of days per chunk in the HDF5 file. If this is
        greater than the number of days in the data, the chunksize will
        match the actual number of days.
    See Also
    --------
    zipline.data.hdf5_daily_bars.HDF5DailyBarReader
    """
    def __init__(self, filename, date_chunk_size):
        self._filename = filename
        self._date_chunk_size = date_chunk_size
    def h5_file(self, mode):
        return h5py.File(self._filename, mode)
    def write(self,
              country_code,
              frames,
              currency_codes=None,
              scaling_factors=None):
        """
        Write the OHLCV data for one country to the HDF5 file.
        Parameters
        ----------
        country_code : str
            The ISO 3166 alpha-2 country code for this country.
        frames : dict[str, pd.DataFrame]
            A dict mapping each OHLCV field to a dataframe with a row
            for each date and a column for each sid. The dataframes need
            to have the same index and columns.
        currency_codes : pd.Series, optional
            Series mapping sids to 3-digit currency code values for those sids'
            listing currencies. If not passed, missing currencies will be
            written.
        scaling_factors : dict[str, float], optional
            A dict mapping each OHLCV field to a scaling factor, which
            is applied (as a multiplier) to the values of field to
            efficiently store them as uint32, while maintaining desired
            precision. These factors are written to the file as metadata,
            which is consumed by the reader to adjust back to the original
            float values. Default is None, in which case
            DEFAULT_SCALING_FACTORS is used.
        """
        if scaling_factors is None:
            scaling_factors = DEFAULT_SCALING_FACTORS
        days, sids = days_and_sids_for_frames(list(frames.values()))
        if currency_codes is None:
            currency_codes = pd.Series(index=sids, data=MISSING_CURRENCY)
        check_sids_arrays_match(
            sids,
            currency_codes.index.values,
            message="currency_codes sids do not match data sids:",
        )
        start_date_ixs, end_date_ixs = compute_asset_lifetimes(frames)
        if len(sids):
            chunks = (len(sids), min(self._date_chunk_size, len(days)))
        else:
            chunks = None
        with self.h5_file(mode='a') as h5_file:
            h5_file.attrs['version'] = VERSION
            country_group = h5_file.create_group(country_code)
            self._write_index_group(country_group, days, sids)
            self._write_lifetimes_group(
                country_group,
                start_date_ixs,
                end_date_ixs,
            )
            self._write_currency_group(country_group, currency_codes)
            self._write_data_group(
                country_group,
                frames,
                scaling_factors,
                chunks,
            )
    def write_from_sid_df_pairs(self,
                                country_code,
                                data,
                                currency_codes=None,
                                scaling_factors=None):
        """
        Parameters
        ----------
        country_code : str
            The ISO 3166 alpha-2 country code for this country.
        data : iterable[tuple[int, pandas.DataFrame]]
            The data chunks to write. Each chunk should be a tuple of
            sid and the data for that asset.
        currency_codes : pd.Series, optional
            Series mapping sids to 3-digit currency code values for those sids'
            listing currencies. If not passed, missing currencies will be
            written.
        scaling_factors : dict[str, float], optional
            A dict mapping each OHLCV field to a scaling factor, which
            is applied (as a multiplier) to the values of field to
            efficiently store them as uint32, while maintaining desired
            precision. These factors are written to the file as metadata,
            which is consumed by the reader to adjust back to the original
            float values. Default is None, in which case
            DEFAULT_SCALING_FACTORS is used.
        """
        data = list(data)
        if not data:
            empty_frame = pd.DataFrame(
                data=None,
                index=np.array([], dtype='datetime64[ns]'),
                columns=np.array([], dtype='int64'),
            )
            return self.write(
                country_code,
                {f: empty_frame.copy() for f in FIELDS},
                scaling_factors,
            )
        sids, frames = zip(*data)
        ohlcv_frame = pd.concat(frames)
        sid_ix = np.repeat(sids, [len(f) for f in frames])
        ohlcv_frame.set_index(sid_ix, append=True, inplace=True)
        frames = {
            field: ohlcv_frame[field].unstack()
            for field in FIELDS
        }
        return self.write(
            country_code=country_code,
            frames=frames,
            scaling_factors=scaling_factors,
            currency_codes=currency_codes
        )
    def _write_index_group(self, country_group, days, sids):
        """Write /country/index.
        """
        index_group = country_group.create_group(INDEX)
        self._log_writing_dataset(index_group)
        index_group.create_dataset(SID, data=sids)
        index_group.create_dataset(DAY, data=days.astype(np.int64))
    def _write_lifetimes_group(self,
                               country_group,
                               start_date_ixs,
                               end_date_ixs):
        """Write /country/lifetimes
        """
        lifetimes_group = country_group.create_group(LIFETIMES)
        self._log_writing_dataset(lifetimes_group)
        lifetimes_group.create_dataset(START_DATE, data=start_date_ixs)
        lifetimes_group.create_dataset(END_DATE, data=end_date_ixs)
    def _write_currency_group(self, country_group, currencies):
        """Write /country/currency
        """
        currency_group = country_group.create_group(CURRENCY)
        self._log_writing_dataset(currency_group)
        currency_group.create_dataset(
            CODE,
            data=currencies.values.astype(dtype='S3'),
        )
    def _write_data_group(self,
                          country_group,
                          frames,
                          scaling_factors,
                          chunks):
        """Write /country/data
        """
        data_group = country_group.create_group(DATA)
        self._log_writing_dataset(data_group)
        for field in FIELDS:
            frame = frames[field]
            frame.sort_index(inplace=True)
            frame.sort_index(axis='columns', inplace=True)
            data = coerce_to_uint32(
                frame.T.fillna(0).values,
                scaling_factors[field],
            )
            dataset = data_group.create_dataset(
                field,
                compression='lzf',
                shuffle=True,
                data=data,
                chunks=chunks,
            )
            self._log_writing_dataset(dataset)
            dataset.attrs[SCALING_FACTOR] = scaling_factors[field]
            log.debug(
                'Writing dataset {} to file {}',
                dataset.name, self._filename
            )
    def _log_writing_dataset(self, dataset):
        log.debug("Writing {} to file {}", dataset.name, self._filename)
def compute_asset_lifetimes(frames):
    """
    Parameters
    ----------
    frames : dict[str, pd.DataFrame]
        A dict mapping each OHLCV field to a dataframe with a row for
        each date and a column for each sid, as passed to write().
    Returns
    -------
    start_date_ixs : np.array[int64]
        The index of the first date with non-nan values, for each sid.
    end_date_ixs : np.array[int64]
        The index of the last date with non-nan values, for each sid.
    """
    is_null_matrix = np.logical_and.reduce(
        [frames[field].isnull().values for field in FIELDS],
    )
    if not is_null_matrix.size:
        empty = np.array([], dtype='int64')
        return empty, empty.copy()
    start_date_ixs = is_null_matrix.argmin(axis=0)
    end_offsets = is_null_matrix[::-1].argmin(axis=0)
    end_date_ixs = is_null_matrix.shape[0] - end_offsets - 1
    return start_date_ixs, end_date_ixs
def convert_price_with_scaling_factor(a, scaling_factor):
    conversion_factor = (1.0 / scaling_factor)
    zeroes = (a == 0)
    return np.where(zeroes, np.nan, a.astype('float64')) * conversion_factor
class HDF5DailyBarReader(CurrencyAwareSessionBarReader):
    """
    Parameters
    ---------
    country_group : h5py.Group
        The group for a single country in an HDF5 daily pricing file.
    """
    def __init__(self, country_group):
        self._country_group = country_group
        self._postprocessors = {
            OPEN: partial(convert_price_with_scaling_factor,
                          scaling_factor=self._read_scaling_factor(OPEN)),
            HIGH: partial(convert_price_with_scaling_factor,
                          scaling_factor=self._read_scaling_factor(HIGH)),
            LOW: partial(convert_price_with_scaling_factor,
                         scaling_factor=self._read_scaling_factor(LOW)),
            CLOSE: partial(convert_price_with_scaling_factor,
                           scaling_factor=self._read_scaling_factor(CLOSE)),
            VOLUME: lambda a: a,
        }
    @classmethod
    def from_file(cls, h5_file, country_code):
        """
        Construct from an h5py.File and a country code.
        Parameters
        ----------
        h5_file : h5py.File
            An HDF5 daily pricing file.
        country_code : str
            The ISO 3166 alpha-2 country code for the country to read.
        """
        if h5_file.attrs['version'] != VERSION:
            raise ValueError(
                'mismatched version: file is of version %s, expected %s' % (
                    h5_file.attrs['version'],
                    VERSION,
                ),
            )
        return cls(h5_file[country_code])
    @classmethod
    def from_path(cls, path, country_code):
        """
        Construct from a file path and a country code.
        Parameters
        ----------
        path : str
            The path to an HDF5 daily pricing file.
        country_code : str
            The ISO 3166 alpha-2 country code for the country to read.
        """
        return cls.from_file(h5py.File(path), country_code)
    def _read_scaling_factor(self, field):
        return self._country_group[DATA][field].attrs[SCALING_FACTOR]
    def load_raw_arrays(self,
                        columns,
                        start_date,
                        end_date,
                        assets):
        """
        Parameters
        ----------
        columns : list of str
           'open', 'high', 'low', 'close', or 'volume'
        start_date: Timestamp
           Beginning of the window range.
        end_date: Timestamp
           End of the window range.
        assets : list of int
           The asset identifiers in the window.
        Returns
        -------
        list of np.ndarray
            A list with an entry per field of ndarrays with shape
            (minutes in range, sids) with a dtype of float64, containing the
            values for the respective field over start and end dt range.
        """
        self._validate_timestamp(start_date)
        self._validate_timestamp(end_date)
        start = start_date.asm8
        end = end_date.asm8
        date_slice = self._compute_date_range_slice(start, end)
        n_dates = date_slice.stop - date_slice.start
        full_buf = np.zeros((len(self.sids) + 1, n_dates), dtype=np.uint32)
        mutable_buf = full_buf[:-1]
        sid_selector = self._make_sid_selector(assets)
        out = []
        for column in columns:
            mutable_buf.fill(0)
            dataset = self._country_group[DATA][column]
            dataset.read_direct(
                mutable_buf,
                np.s_[:, date_slice],
            )
            out.append(self._postprocessors[column](full_buf[sid_selector].T))
        return out
    def _make_sid_selector(self, assets):
        """
        Build an indexer mapping ``self.sids`` to ``assets``.
        Parameters
        ----------
        assets : list[int]
            List of assets requested by a caller of ``load_raw_arrays``.
        Returns
        -------
        index : np.array[int64]
            Index array containing the index in ``self.sids`` for each location
            in ``assets``. Entries in ``assets`` for which we don't have a sid
            will contain -1. It is caller's responsibility to handle these
            values correctly.
        """
        assets = np.array(assets)
        sid_selector = self.sids.searchsorted(assets)
        unknown = np.in1d(assets, self.sids, invert=True)
        sid_selector[unknown] = -1
        return sid_selector
    def _compute_date_range_slice(self, start_date, end_date):
        start_ix = self.dates.searchsorted(start_date)
        end_ix = self.dates.searchsorted(end_date, side='right')
        return slice(start_ix, end_ix)
    def _validate_assets(self, assets):
        """Validate that asset identifiers are contained in the daily bars.
        Parameters
        ----------
        assets : array-like[int]
           The asset identifiers to validate.
        Raises
        ------
        NoDataForSid
            If one or more of the provided asset identifiers are not
            contained in the daily bars.
        """
        missing_sids = np.setdiff1d(assets, self.sids)
        if len(missing_sids):
            raise NoDataForSid(
                'Assets not contained in daily pricing file: {}'.format(
                    missing_sids
                )
            )
    def _validate_timestamp(self, ts):
        if ts.asm8 not in self.dates:
            raise NoDataOnDate(ts)
    @lazyval
    def dates(self):
        return self._country_group[INDEX][DAY][:].astype('datetime64[ns]')
    @lazyval
    def sids(self):
        return self._country_group[INDEX][SID][:].astype('int64', copy=False)
    @lazyval
    def asset_start_dates(self):
        return self.dates[self._country_group[LIFETIMES][START_DATE][:]]
    @lazyval
    def asset_end_dates(self):
        return self.dates[self._country_group[LIFETIMES][END_DATE][:]]
    @lazyval
    def _currency_codes(self):
        bytes_array = self._country_group[CURRENCY][CODE][:]
        return bytes_array_to_native_str_object_array(bytes_array)
    def currency_codes(self, sids):
        """Get currencies in which prices are quoted for the requested sids.
        Parameters
        ----------
        sids : np.array[int64]
            Array of sids for which currencies are needed.
        Returns
        -------
        currency_codes : np.array[object]
            Array of currency codes for listing currencies of ``sids``.
        """
        ixs = self.sids.searchsorted(sids, side='left')
        result = self._currency_codes[ixs]
        not_found = (self.sids[ixs] != sids)
        result[not_found] = None
        return result
    @property
    def last_available_dt(self):
        """
        Returns
        -------
        dt : pd.Timestamp
            The last session for which the reader can provide data.
        """
        return pd.Timestamp(self.dates[-1], tz='UTC')
    @property
    def trading_calendar(self):
        """
        Returns the zipline.utils.calendar.trading_calendar used to read
        the data.  Can be None (if the writer didn't specify it).
        """
        raise NotImplementedError(
            'HDF5 pricing does not yet support trading calendars.'
        )
    @property
    def first_trading_day(self):
        """
        Returns
        -------
        dt : pd.Timestamp
            The first trading day (session) for which the reader can provide
            data.
        """
        return pd.Timestamp(self.dates[0], tz='UTC')
    @lazyval
    def sessions(self):
        """
        Returns
        -------
        sessions : DatetimeIndex
           All session labels (unioning the range for all assets) which the
           reader can provide.
        """
        return pd.to_datetime(self.dates, utc=True)
    def get_value(self, sid, dt, field):
        """
        Retrieve the value at the given coordinates.
        Parameters
        ----------
        sid : int
            The asset identifier.
        dt : pd.Timestamp
            The timestamp for the desired data point.
        field : string
            The OHLVC name for the desired data point.
        Returns
        -------
        value : float|int
            The value at the given coordinates, ``float`` for OHLC, ``int``
            for 'volume'.
        Raises
        ------
        NoDataOnDate
            If the given dt is not a valid market minute (in minute mode) or
            session (in daily mode) according to this reader's tradingcalendar.
        """
        self._validate_assets([sid])
        self._validate_timestamp(dt)
        sid_ix = self.sids.searchsorted(sid)
        dt_ix = self.dates.searchsorted(dt.asm8)
        value = self._postprocessors[field](
            self._country_group[DATA][field][sid_ix, dt_ix]
        )
        if np.isnan(value):
            if dt.asm8 &lt; self.asset_start_dates[sid_ix]:
                raise NoDataBeforeDate()
            if dt.asm8 &gt; self.asset_end_dates[sid_ix]:
                raise NoDataAfterDate()
        return value
    def get_last_traded_dt(self, asset, dt):
        """
        Get the latest day on or before ``dt`` in which ``asset`` traded.
        If there are no trades on or before ``dt``, returns ``pd.NaT``.
        Parameters
        ----------
        asset : zipline.asset.Asset
            The asset for which to get the last traded day.
        dt : pd.Timestamp
            The dt at which to start searching for the last traded day.
        Returns
        -------
        last_traded : pd.Timestamp
            The day of the last trade for the given asset, using the
            input dt as a vantage point.
        """
        sid_ix = self.sids.searchsorted(asset.sid)
        dt_limit_ix = self.dates.searchsorted(dt.asm8, side='right')
        nonzero_volume_ixs = np.ravel(
            np.nonzero(self._country_group[DATA][VOLUME][sid_ix, :dt_limit_ix])
        )
        if len(nonzero_volume_ixs) == 0:
            return pd.NaT
        return pd.Timestamp(self.dates[nonzero_volume_ixs][-1], tz='UTC')
class MultiCountryDailyBarReader(CurrencyAwareSessionBarReader):
    """
    Parameters
    ---------
    readers : dict[str -&gt; SessionBarReader]
        A dict mapping country codes to SessionBarReader instances to
        service each country.
    """
    def __init__(self, readers):
        self._readers = readers
        self._country_map = pd.concat([
            pd.Series(index=reader.sids, data=country_code)
            for country_code, reader in iteritems(readers)
        ])
    @classmethod
    def from_file(cls, h5_file):
        """
        Construct from an h5py.File.
        Parameters
        ----------
        h5_file : h5py.File
            An HDF5 daily pricing file.
        """
        return cls({
            country: HDF5DailyBarReader.from_file(h5_file, country)
            for country in h5_file.keys()
        })
    @classmethod
    def from_path(cls, path):
        """
        Construct from a file path.
        Parameters
        ----------
        path : str
            Path to an HDF5 daily pricing file.
        """
        return cls.from_file(h5py.File(path))
    @property
    def countries(self):
        """A set-like object of the country codes supplied by this reader.
        """
        return viewkeys(self._readers)
    def _country_code_for_assets(self, assets):
        country_codes = self._country_map.get(assets)
        if country_codes is not None:
            unique_country_codes = country_codes.dropna().unique()
            num_countries = len(unique_country_codes)
        else:
            num_countries = 0
        if num_countries == 0:
            raise ValueError('At least one valid asset id is required.')
        elif num_countries &gt; 1:
            raise NotImplementedError(
                (
                    'Assets were requested from multiple countries ({}),'
                    ' but multi-country reads are not yet supported.'
                ).format(list(unique_country_codes))
            )
        return np.asscalar(unique_country_codes)
    def load_raw_arrays(self,
                        columns,
                        start_date,
                        end_date,
                        assets):
        """
        Parameters
        ----------
        columns : list of str
           'open', 'high', 'low', 'close', or 'volume'
        start_date: Timestamp
           Beginning of the window range.
        end_date: Timestamp
           End of the window range.
        assets : list of int
           The asset identifiers in the window.
        Returns
        -------
        list of np.ndarray
            A list with an entry per field of ndarrays with shape
            (minutes in range, sids) with a dtype of float64, containing the
            values for the respective field over start and end dt range.
        """
        country_code = self._country_code_for_assets(assets)
        return self._readers[country_code].load_raw_arrays(
            columns,
            start_date,
            end_date,
            assets,
        )
    @property
    def last_available_dt(self):
        """
        Returns
        -------
        dt : pd.Timestamp
            The last session for which the reader can provide data.
        """
        return max(
            reader.last_available_dt for reader in self._readers.values()
        )
    @property
    def trading_calendar(self):
        """
        Returns the zipline.utils.calendar.trading_calendar used to read
        the data.  Can be None (if the writer didn't specify it).
        """
        raise NotImplementedError(
            'HDF5 pricing does not yet support trading calendars.'
        )
    @property
    def first_trading_day(self):
        """
        Returns
        -------
        dt : pd.Timestamp
            The first trading day (session) for which the reader can provide
            data.
        """
        return min(
            reader.first_trading_day for reader in self._readers.values()
        )
    @property
    def sessions(self):
        """
        Returns
        -------
        sessions : DatetimeIndex
           All session labels (unioning the range for all assets) which the
           reader can provide.
        """
        return pd.to_datetime(
            reduce(
                np.union1d,
                (reader.dates for reader in self._readers.values()),
            ),
            utc=True,
        )
    def get_value(self, sid, dt, field):
        """
        Retrieve the value at the given coordinates.
        Parameters
        ----------
        sid : int
            The asset identifier.
        dt : pd.Timestamp
            The timestamp for the desired data point.
        field : string
            The OHLVC name for the desired data point.
        Returns
        -------
        value : float|int
            The value at the given coordinates, ``float`` for OHLC, ``int``
            for 'volume'.
        Raises
        ------
        NoDataOnDate
            If the given dt is not a valid market minute (in minute mode) or
            session (in daily mode) according to this reader's tradingcalendar.
        NoDataForSid
            If the given sid is not valid.
        """
        try:
            country_code = self._country_code_for_assets([sid])
        except ValueError as exc:
            raise_from(
                NoDataForSid(
                    'Asset not contained in daily pricing file: {}'.format(sid)
                ),
                exc
            )
        return self._readers[country_code].get_value(sid, dt, field)
    def get_last_traded_dt(self, asset, dt):
        """
        Get the latest day on or before ``dt`` in which ``asset`` traded.
        If there are no trades on or before ``dt``, returns ``pd.NaT``.
        Parameters
        ----------
        asset : zipline.asset.Asset
            The asset for which to get the last traded day.
        dt : pd.Timestamp
            The dt at which to start searching for the last traded day.
        Returns
        -------
        last_traded : pd.Timestamp
            The day of the last trade for the given asset, using the
            input dt as a vantage point.
        """
        country_code = self._country_code_for_assets([asset.sid])
        return self._readers[country_code].get_last_traded_dt(asset, dt)
    def currency_codes(self, sids):
        """Get currencies in which prices are quoted for the requested sids.
        Assumes that a sid's prices are always quoted in a single currency.
        Parameters
        ----------
        sids : np.array[int64]
            Array of sids for which currencies are needed.
        Returns
        -------
        currency_codes : np.array[S3]
            Array of currency codes for listing currencies of ``sids``.
        """
        country_code = self._country_code_for_assets(sids)
        return self._readers[country_code].currency_codes(sids)
def check_sids_arrays_match(left, right, message):
    """Check that two 1d arrays of sids are equal
    """
    if len(left) != len(right):
        raise ValueError(
            "{}:\nlen(left) ({}) != len(right) ({})".format(
                message, len(left), len(right)
            )
        )
    diff = (left != right)
    if diff.any():
        (bad_locs,) = np.where(diff)
        raise ValueError(
            "{}:\n Indices with differences: {}".format(message, bad_locs)
        )
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
