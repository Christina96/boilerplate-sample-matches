
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 4.036326942482341%, Tokens: 9, <button onclick='openModal()' class='match'></button></h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-input_pipeline.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import abc
6  import sys
7  import six
8  import tensorflow as tf
9  from tensorflow.contrib.slim.python.slim.data import tfexample_decoder
10  from seq2seq.configurable import Configurable
11  from seq2seq.data import split_tokens_decoder, parallel_data_provider
12  from seq2seq.data.sequence_example_decoder import TFSEquenceExampleDecoder
13  def make_input_pipeline_from_def(def_dict, mode, **kwargs):
14    if not "class" in def_dict:
15      raise ValueError("Input Pipeline definition must have a class property.")
16    class_ = def_dict["class"]
17    if not hasattr(sys.modules[__name__], class_):
18      raise ValueError("Invalid Input Pipeline class: {}".format(class_))
19    pipeline_class = getattr(sys.modules[__name__], class_)
20    params = {}
21    if "params" in def_dict:
22      params.update(def_dict["params"])
23    params.update(kwargs)
24    return pipeline_class(params=params, mode=mode)
25  @six.add_metaclass(abc.ABCMeta)
26  class InputPipeline(Configurable):
27    def __init__(self, params, mode):
28      Configurable.__init__(self, params, mode)
29    @staticmethod
30    def default_params():
31      return {
32          "shuffle": True,
33          "num_epochs": None,
34      }
35    def make_data_provider(self, **kwargs):
36      raise NotImplementedError("Not implemented.")
37    @property
38    def feature_keys(self):
39      return set()
40    @property
41    def label_keys(self):
42      return set()
43    @staticmethod
44    def read_from_data_provider(data_provider):
45      item_values = data_provider.get(list(data_provider.list_items()))
46      items_dict = dict(zip(data_provider.list_items(), item_values))
47      return items_dict
48  class ParallelTextInputPipeline(InputPipeline):
49    @staticmethod
50    def default_params():
51      params = InputPipeline.default_params()
52      params.update({
53          "source_files": [],
54          "target_files": [],
55          "source_delimiter": " ",
56          "target_delimiter": " ",
57      })
58      return params
59    def make_data_provider(self, **kwargs):
60      decoder_source = split_tokens_decoder.SplitTokensDecoder(
61          tokens_feature_name="source_tokens",
62          length_feature_name="source_len",
<span onclick='openModal()' class='match'>63          append_token="SEQUENCE_END",
64          delimiter=self.params["source_delimiter"])
65      dataset_source = tf.contrib.slim.dataset.Dataset(
</span>66          data_sources=self.params["source_files"],
67          reader=tf.TextLineReader,
68          decoder=decoder_source,
69          num_samples=None,
70          items_to_descriptions={})
71      dataset_target = None
72      if len(self.params["target_files"]) > 0:
73        decoder_target = split_tokens_decoder.SplitTokensDecoder(
74            tokens_feature_name="target_tokens",
75            length_feature_name="target_len",
76            prepend_token="SEQUENCE_START",
77            append_token="SEQUENCE_END",
78            delimiter=self.params["target_delimiter"])
79        dataset_target = tf.contrib.slim.dataset.Dataset(
80            data_sources=self.params["target_files"],
81            reader=tf.TextLineReader,
82            decoder=decoder_target,
83            num_samples=None,
84            items_to_descriptions={})
85      return parallel_data_provider.ParallelDataProvider(
86          dataset1=dataset_source,
87          dataset2=dataset_target,
88          shuffle=self.params["shuffle"],
89          num_epochs=self.params["num_epochs"],
90          **kwargs)
91    @property
92    def feature_keys(self):
93      return set(["source_tokens", "source_len"])
94    @property
95    def label_keys(self):
96      return set(["target_tokens", "target_len"])
97  class TFRecordInputPipeline(InputPipeline):
98    @staticmethod
99    def default_params():
100      params = InputPipeline.default_params()
101      params.update({
102          "files": [],
103          "source_field": "source",
104          "target_field": "target",
105          "source_delimiter": " ",
106          "target_delimiter": " ",
107      })
108      return params
109    def make_data_provider(self, **kwargs):
110      splitter_source = split_tokens_decoder.SplitTokensDecoder(
111          tokens_feature_name="source_tokens",
112          length_feature_name="source_len",
113          append_token="SEQUENCE_END",
114          delimiter=self.params["source_delimiter"])
115      splitter_target = split_tokens_decoder.SplitTokensDecoder(
116          tokens_feature_name="target_tokens",
117          length_feature_name="target_len",
118          prepend_token="SEQUENCE_START",
119          append_token="SEQUENCE_END",
120          delimiter=self.params["target_delimiter"])
121      keys_to_features = {
122          self.params["source_field"]: tf.FixedLenFeature((), tf.string),
123          self.params["target_field"]: tf.FixedLenFeature(
124              (), tf.string, default_value="")
125      }
126      items_to_handlers = {}
127      items_to_handlers["source_tokens"] = tfexample_decoder.ItemHandlerCallback(
128          keys=[self.params["source_field"]],
129          func=lambda dict: splitter_source.decode(
130              dict[self.params["source_field"]], ["source_tokens"])[0])
131      items_to_handlers["source_len"] = tfexample_decoder.ItemHandlerCallback(
132          keys=[self.params["source_field"]],
133          func=lambda dict: splitter_source.decode(
134              dict[self.params["source_field"]], ["source_len"])[0])
135      items_to_handlers["target_tokens"] = tfexample_decoder.ItemHandlerCallback(
136          keys=[self.params["target_field"]],
137          func=lambda dict: splitter_target.decode(
138              dict[self.params["target_field"]], ["target_tokens"])[0])
139      items_to_handlers["target_len"] = tfexample_decoder.ItemHandlerCallback(
140          keys=[self.params["target_field"]],
141          func=lambda dict: splitter_target.decode(
142              dict[self.params["target_field"]], ["target_len"])[0])
143      decoder = tfexample_decoder.TFExampleDecoder(keys_to_features,
144                                                   items_to_handlers)
145      dataset = tf.contrib.slim.dataset.Dataset(
146          data_sources=self.params["files"],
147          reader=tf.TFRecordReader,
148          decoder=decoder,
149          num_samples=None,
150          items_to_descriptions={})
151      return tf.contrib.slim.dataset_data_provider.DatasetDataProvider(
152          dataset=dataset,
153          shuffle=self.params["shuffle"],
154          num_epochs=self.params["num_epochs"],
155          **kwargs)
156    @property
157    def feature_keys(self):
158      return set(["source_tokens", "source_len"])
159    @property
160    def label_keys(self):
161      return set(["target_tokens", "target_len"])
162  class ImageCaptioningInputPipeline(InputPipeline):
163    @staticmethod
164    def default_params():
165      params = InputPipeline.default_params()
166      params.update({
167          "files": [],
168          "image_field": "image/data",
169          "image_format": "jpg",
170          "caption_ids_field": "image/caption_ids",
171          "caption_tokens_field": "image/caption",
172      })
173      return params
174    def make_data_provider(self, **kwargs):
175      context_keys_to_features = {
176          self.params["image_field"]: tf.FixedLenFeature(
177              [], dtype=tf.string),
178          "image/format": tf.FixedLenFeature(
179              [], dtype=tf.string, default_value=self.params["image_format"]),
180      }
181      sequence_keys_to_features = {
182          self.params["caption_ids_field"]: tf.FixedLenSequenceFeature(
183              [], dtype=tf.int64),
184          self.params["caption_tokens_field"]: tf.FixedLenSequenceFeature(
185              [], dtype=tf.string)
186      }
187      items_to_handlers = {
188          "image": tfexample_decoder.Image(
189              image_key=self.params["image_field"],
190              format_key="image/format",
191              channels=3),
192          "target_ids":
193          tfexample_decoder.Tensor(self.params["caption_ids_field"]),
194          "target_tokens":
195          tfexample_decoder.Tensor(self.params["caption_tokens_field"]),
196          "target_len": tfexample_decoder.ItemHandlerCallback(
197              keys=[self.params["caption_tokens_field"]],
198              func=lambda x: tf.size(x[self.params["caption_tokens_field"]]))
199      }
200      decoder = TFSEquenceExampleDecoder(
201          context_keys_to_features, sequence_keys_to_features, items_to_handlers)
202      dataset = tf.contrib.slim.dataset.Dataset(
203          data_sources=self.params["files"],
204          reader=tf.TFRecordReader,
205          decoder=decoder,
206          num_samples=None,
207          items_to_descriptions={})
208      return tf.contrib.slim.dataset_data_provider.DatasetDataProvider(
209          dataset=dataset,
210          shuffle=self.params["shuffle"],
211          num_epochs=self.params["num_epochs"],
212          **kwargs)
213    @property
214    def feature_keys(self):
215      return set(["image"])
216    @property
217    def label_keys(self):
218      return set(["target_tokens", "target_ids", "target_len"])
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</h3>
            <pre><code>1  import torch
2  from utils import timer
3  from data import cfg
4  @torch.jit.script
5  def point_form(boxes):
6      return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin
7                       boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax
8  @torch.jit.script
9  def center_size(boxes):
10      return torch.cat(( (boxes[:, 2:] + boxes[:, :2])/2,     # cx, cy
11                          boxes[:, 2:] - boxes[:, :2]  ), 1)  # w, h
12  @torch.jit.script
13  def intersect(box_a, box_b):
14      n = box_a.size(0)
15      A = box_a.size(1)
16      B = box_b.size(1)
17      max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),
18                         box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))
19      min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),
20                         box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))
21      return torch.clamp(max_xy - min_xy, min=0).prod(3)  # inter
22  def jaccard(box_a, box_b, iscrowd:bool=False):
23      use_batch = True
24      if box_a.dim() == 2:
25          use_batch = False
26          box_a = box_a[None, ...]
27          box_b = box_b[None, ...]
28      inter = intersect(box_a, box_b)
29      area_a = ((box_a[:, :, 2]-box_a[:, :, 0]) *
30                (box_a[:, :, 3]-box_a[:, :, 1])).unsqueeze(2).expand_as(inter)  # [A,B]
31      area_b = ((box_b[:, :, 2]-box_b[:, :, 0]) *
32                (box_b[:, :, 3]-box_b[:, :, 1])).unsqueeze(1).expand_as(inter)  # [A,B]
33      union = area_a + area_b - inter
34      out = inter / area_a if iscrowd else inter / union
35      return out if use_batch else out.squeeze(0)
36  def elemwise_box_iou(box_a, box_b):
37      max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])
38      min_xy = torch.max(box_a[:, :2], box_b[:, :2])
<span onclick='openModal()' class='match'>39      inter = torch.clamp((max_xy - min_xy), min=0)
40      inter = inter[:, 0] * inter[:, 1]
41      area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])
</span>42      area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])
43      union = area_a + area_b - inter
44      union = torch.clamp(union, min=0.1)
45      return torch.clamp(inter / union, max=1)
46  def mask_iou(masks_a, masks_b, iscrowd=False):
47      masks_a = masks_a.view(masks_a.size(0), -1)
48      masks_b = masks_b.view(masks_b.size(0), -1)
49      intersection = masks_a @ masks_b.t()
50      area_a = masks_a.sum(dim=1).unsqueeze(1)
51      area_b = masks_b.sum(dim=1).unsqueeze(0)
52      return intersection / (area_a + area_b - intersection) if not iscrowd else intersection / area_a
53  def elemwise_mask_iou(masks_a, masks_b):
54      masks_a = masks_a.view(-1, masks_a.size(-1))
55      masks_b = masks_b.view(-1, masks_b.size(-1))
56      intersection = (masks_a * masks_b).sum(dim=0)
57      area_a = masks_a.sum(dim=0)
58      area_b = masks_b.sum(dim=0)
59      return torch.clamp(intersection / torch.clamp(area_a + area_b - intersection, min=0.1), max=1)
60  def change(gt, priors):
61      num_priors = priors.size(0)
62      num_gt     = gt.size(0)
63      gt_w = (gt[:, 2] - gt[:, 0])[:, None].expand(num_gt, num_priors)
64      gt_h = (gt[:, 3] - gt[:, 1])[:, None].expand(num_gt, num_priors)
65      gt_mat =     gt[:, None, :].expand(num_gt, num_priors, 4)
66      pr_mat = priors[None, :, :].expand(num_gt, num_priors, 4)
67      diff = gt_mat - pr_mat
68      diff[:, :, 0] /= gt_w
69      diff[:, :, 2] /= gt_w
70      diff[:, :, 1] /= gt_h
71      diff[:, :, 3] /= gt_h
72      return -torch.sqrt( (diff ** 2).sum(dim=2) )
73  def match(pos_thresh, neg_thresh, truths, priors, labels, crowd_boxes, loc_t, conf_t, idx_t, idx, loc_data):
74      decoded_priors = decode(loc_data, priors, cfg.use_yolo_regressors) if cfg.use_prediction_matching else point_form(priors)
75      overlaps = jaccard(truths, decoded_priors) if not cfg.use_change_matching else change(truths, decoded_priors)
76      best_truth_overlap, best_truth_idx = overlaps.max(0)
77      for _ in range(overlaps.size(0)):
78          best_prior_overlap, best_prior_idx = overlaps.max(1)
79          j = best_prior_overlap.max(0)[1]
80          i = best_prior_idx[j]
81          overlaps[:, i] = -1
82          overlaps[j, :] = -1
83          best_truth_overlap[i] = 2
84          best_truth_idx[i] = j
85      matches = truths[best_truth_idx]            # Shape: [num_priors,4]
86      conf = labels[best_truth_idx] + 1           # Shape: [num_priors]
87      conf[best_truth_overlap < pos_thresh] = -1  # label as neutral
88      conf[best_truth_overlap < neg_thresh] =  0  # label as background
89      if crowd_boxes is not None and cfg.crowd_iou_threshold < 1:
90          crowd_overlaps = jaccard(decoded_priors, crowd_boxes, iscrowd=True)
91          best_crowd_overlap, best_crowd_idx = crowd_overlaps.max(1)
92          conf[(conf <= 0) & (best_crowd_overlap > cfg.crowd_iou_threshold)] = -1
93      loc = encode(matches, priors, cfg.use_yolo_regressors)
94      loc_t[idx]  = loc    # [num_priors,4] encoded offsets to learn
95      conf_t[idx] = conf   # [num_priors] top class label for each prior
96      idx_t[idx]  = best_truth_idx # [num_priors] indices for lookup
97  @torch.jit.script
98  def encode(matched, priors, use_yolo_regressors:bool=False):
99      if use_yolo_regressors:
100          boxes = center_size(matched)
101          loc = torch.cat((
102              boxes[:, :2] - priors[:, :2],
103              torch.log(boxes[:, 2:] / priors[:, 2:])
104          ), 1)
105      else:
106          variances = [0.1, 0.2]
107          g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]
108          g_cxcy /= (variances[0] * priors[:, 2:])
109          g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]
110          g_wh = torch.log(g_wh) / variances[1]
111          loc = torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]
112      return loc
113  @torch.jit.script
114  def decode(loc, priors, use_yolo_regressors:bool=False):
115      if use_yolo_regressors:
116          boxes = torch.cat((
117              loc[:, :2] + priors[:, :2],
118              priors[:, 2:] * torch.exp(loc[:, 2:])
119          ), 1)
120          boxes = point_form(boxes)
121      else:
122          variances = [0.1, 0.2]
123          boxes = torch.cat((
124              priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],
125              priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)
126          boxes[:, :2] -= boxes[:, 2:] / 2
127          boxes[:, 2:] += boxes[:, :2]
128      return boxes
129  def log_sum_exp(x):
130      x_max = x.data.max()
131      return torch.log(torch.sum(torch.exp(x-x_max), 1)) + x_max
132  @torch.jit.script
133  def sanitize_coordinates(_x1, _x2, img_size:int, padding:int=0, cast:bool=True):
134      _x1 = _x1 * img_size
135      _x2 = _x2 * img_size
136      if cast:
137          _x1 = _x1.long()
138          _x2 = _x2.long()
139      x1 = torch.min(_x1, _x2)
140      x2 = torch.max(_x1, _x2)
141      x1 = torch.clamp(x1-padding, min=0)
142      x2 = torch.clamp(x2+padding, max=img_size)
143      return x1, x2
144  @torch.jit.script
145  def crop(masks, boxes, padding:int=1):
146      h, w, n = masks.size()
147      x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding, cast=False)
148      y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding, cast=False)
149      rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)
150      cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)
151      masks_left  = rows >= x1.view(1, 1, -1)
152      masks_right = rows <  x2.view(1, 1, -1)
153      masks_up    = cols >= y1.view(1, 1, -1)
154      masks_down  = cols <  y2.view(1, 1, -1)
155      crop_mask = masks_left * masks_right * masks_up * masks_down
156      return masks * crop_mask.float()
157  def index2d(src, idx):
158      offs = torch.arange(idx.size(0), device=idx.device)[:, None].expand_as(idx)
159      idx  = idx + offs * idx.size(1)
160      return src.view(-1)[idx.view(-1)].view(idx.size())
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-input_pipeline.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</div>
                </div>
                <div class="column column_space"><pre><code>63          append_token="SEQUENCE_END",
64          delimiter=self.params["source_delimiter"])
65      dataset_source = tf.contrib.slim.dataset.Dataset(
</pre></code></div>
                <div class="column column_space"><pre><code>39      inter = torch.clamp((max_xy - min_xy), min=0)
40      inter = inter[:, 0] * inter[:, 1]
41      area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    