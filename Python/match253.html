<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_opt_5.py &amp; test_blas_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_opt_5.py &amp; test_blas_1.py
      </h3>
<h1 align="center">
        9.6%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_opt_5.py (15.069445%)<th>test_blas_1.py (7.112422%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(907-956)<td><a href="#" name="0">(1919-1930)</a><td align="center"><font color="#ff0000">48</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(976-1022)<td><a href="#" name="1">(1989-1999)</a><td align="center"><font color="#d90000">41</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(673-678)<td><a href="#" name="2">(1866-1875)</a><td align="center"><font color="#6f0000">21</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1-23)<td><a href="#" name="3">(1-27)</a><td align="center"><font color="#640000">19</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(655-659)<td><a href="#" name="4">(1939-1946)</a><td align="center"><font color="#5f0000">18</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(709-716)<td><a href="#" name="5">(69-73)</a><td align="center"><font color="#5a0000">17</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(638-646)<td><a href="#" name="6">(727-738)</a><td align="center"><font color="#5a0000">17</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(175-181)<td><a href="#" name="7">(2245-2249)</a><td align="center"><font color="#5a0000">17</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(1097-1112)<td><a href="#" name="8">(424-426)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(211-214)<td><a href="#" name="9">(1229-1232)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(122-125)<td><a href="#" name="10">(286-290)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(78-85)<td><a href="#" name="11">(2001-2009)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(582-583)<td><a href="#" name="12">(575-578)</a><td align="center"><font color="#4a0000">14</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(60-64)<td><a href="#" name="13">(1409-1415)</a><td align="center"><font color="#4a0000">14</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(614-619)<td><a href="#" name="14">(742-747)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(482-484)<td><a href="#" name="15">(76-78)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(454-464)<td><a href="#" name="16">(2180-2186)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(387-394)<td><a href="#" name="17">(2125-2131)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(762-765)<td><a href="#" name="18">(673-676)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(607-612)<td><a href="#" name="19">(1179-1184)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(599-603)<td><a href="#" name="20">(1099-1105)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(579-581)<td><a href="#" name="21">(1524-1528)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(496-499)<td><a href="#" name="22">(1022-1029)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(307-312)<td><a href="#" name="23">(2250-2255)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(298-303)<td><a href="#" name="24">(168-173)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(49-50)<td><a href="#" name="25">(2284-2286)</a><td align="center"><font color="#3f0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_opt_5.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <a name="3"></a><font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>from __future__ import absolute_import, print_function, division
2 from nose.tools import assert_raises
3 import numpy as np
4 import theano
5 from theano import tensor
6 import theano.tensor.slinalg as slinalg
7 from theano.tests.breakpoint import PdbBreakpoint
8 from theano.tests import unittest_tools as utt, test_ifelse
9 from theano.tensor.tests import test_basic
10 from theano.gof.opt import check_stack_trace
11 import theano.gpuarray
12 from .. import basic_ops
13 from ..type import GpuArrayType, gpuarray_shared_constructor, get_context
14 from ..basic_ops import (
15     GpuAlloc, GpuAllocEmpty, GpuReshape, GpuFromHost, HostFromGpu, host_from_gpu)
16 from ..blas import GpuGemm
17 from ..elemwise import (
18     GpuCAReduceCuda, GpuCAReduceCPY, GpuElemwise, Elemwise, max_inputs_to_GpuElemwise)
19 from ..dnn import GpuDnnReduction
20 from ..subtensor import GpuSubtensor
21 from</b></font> ..linalg import GpuCusolverSolve, cusolver_available, GpuCholesky
22 from .config import mode_with_gpu, mode_without_gpu, test_ctx_name, SkipTest
23 import unittest
24 from theano.tensor.nnet import abstract_conv
25 from theano.gpuarray import dnn, blas, opt
26 def _check_stack_trace(thing):
27     def _ops_to_check(op):
28         if not isinstance(op, theano.gof.Op):
29             op = op.op  # assume it is an apply node
30         return not isinstance(op, (theano.compile.ops.Shape_i,
31                                    theano.compile.ops.Shape,
32                                    theano.compile.ops.DeepCopyOp,
33                                    theano.tensor.opt.MakeVector,
34                                    theano.tensor.subtensor.Subtensor,
35                                    theano.tensor.elemwise.Elemwise,
36                                    theano.ifelse.IfElse,
37                                    GpuFromHost, HostFromGpu,
38                                    ))
39     return check_stack_trace(thing, ops_to_check=_ops_to_check,
40                              bug_print="ignore")
41 <a name="25"></a>
42 def test_local_assert():
43     x = theano<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.tensor.fmatrix()
44     a = theano.tensor.opt.assert_op(x, theano.tensor.eq(x, 0).</b></font>any())
45     f = theano.function([x], a, mode=mode_with_gpu)
46     topo = f.maker.fgraph.toposort()
47     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
48     assert len(a_op) == 1
49     assert isinstance(a_op[0].inputs[0].type, GpuArrayType)
50 <a name="13"></a>
51 def test_local_remove_all_assert():
52     x = theano.tensor.fmatrix()
53     a = theano.tensor.opt.assert_op(x, theano.tensor<font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.eq(x, 0).any())
54     f = theano.function([x], a, mode=mode_with_gpu.excluding('unsafe'))
55     topo = f.maker.fgraph.</b></font>toposort()
56     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
57     assert len(a_op) == 1
58     f = theano.function([x], a, mode=mode_with_gpu.including('unsafe'))
59     topo = f.maker.fgraph.toposort()
60     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
61     assert len(a_op) == 0
62 <a name="11"></a>    f = theano.function([x], a, mode=mode_with_gpu.excluding('unsafe'))
63     topo = f.maker.fgraph.toposort()
64     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
65     assert len(<font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>a_op) == 1
66 def test_local_gpu_contiguous_gpu_contiguous():
67     a = tensor.fmatrix()
68     o1 = basic_ops.gpu_contiguous(a)
69     o2 = basic_ops.gpu_contiguous(o1)
70     f1 = theano.function([a], o1, mode=</b></font>mode_with_gpu)
71     f2 = theano.function([a], o2, mode=mode_with_gpu)
72     assert 1 == len([node for node in f1.maker.fgraph.toposort()
73                      if isinstance(node.op, basic_ops.GpuContiguous)])
74     assert 1 == len([node for node in f2.maker.fgraph.toposort()
75                      if isinstance(node.op, basic_ops.GpuContiguous)])
76     assert _check_stack_trace(f1)
77     assert _check_stack_trace(f2)
78 def test_local_gpu_contiguous():
79     a = tensor.fmatrix()
80     o = tensor.extra_ops.cpu_contiguous(a)
81     f = theano.function([a], o, mode=mode_with_gpu)
82     assert 1 == len([node for node in f.maker.fgraph.toposort()
83                      if isinstance(node.op, basic_ops.GpuContiguous)])
84     f([[2.]])
85     assert _check_stack_trace(f)
86 def test_flatten():
87     m = theano.tensor.fmatrix()
88     f = theano.function([m], m.flatten(), mode=mode_with_gpu)
89     val = np.random.rand(10, 11).astype("float32")
90     res = f(val)
91     utt.assert_allclose(res, val.flatten())
92     assert res.shape == val.flatten().shape
93     assert GpuReshape in [type(node.op)
94                           for node in f.maker.fgraph.toposort()]
95     val = np.random.rand(10, 11).astype("float32")
96     res = f(val)
97     utt.assert_allclose(res, val.flatten())
98     assert res.shape == val.flatten().shape
99     assert GpuReshape in [type(node.op)
100 <a name="10"></a>                          for node in f.maker.fgraph.toposort()]
101     assert _check_stack_trace(f)
102     f = theano<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.function([m], m.flatten(ndim=2),
103                         mode=mode_with_gpu.excluding("local_useless_reshape"))
104     val = np.random.rand(10, 11).astype("float32")
105     res =</b></font> f(val)
106     utt.assert_allclose(res, val)
107     assert res.shape == val.shape
108     assert GpuReshape in [type(node.op)
109                           for node in f.maker.fgraph.toposort()]
110     assert _check_stack_trace(f)
111     m = theano.tensor.tensor3()
112     f = theano.function([m], m.flatten(ndim=2), mode=mode_with_gpu)
113     val = np.random.rand(10, 11, 12).astype("float32")
114     res = f(val)
115     utt.assert_allclose(res, val.reshape(10, -1))
116     assert res.shape == val.reshape(10, -1).shape
117     assert GpuReshape in [type(node.op)
118                           for node in f.maker.fgraph.toposort()]
119     assert _check_stack_trace(f)
120 def test_reduce():
121     kind = get_context(test_ctx_name).kind
122     for method, param in [('sum', dict(acc_dtype='float32')),
123                           ('prod', dict(acc_dtype='float32')),
124                           ('max', {}), ('min', {})]:
125         m = theano.tensor.fmatrix()
126         f = theano.function([m], getattr(m, method)(axis=0,
127                                                     **param),
128                             mode=mode_with_gpu)
129         val = np.random.rand(10, 11).astype("float32")
130         res = f(val)
131         utt.assert_allclose(res, getattr(val, method)(axis=0))
132         assert res.shape == (11,)
133         topo = f.maker.fgraph.toposort()
134         ops = [type(node.op) for node in topo]
135         if kind == b'opencl' and method in ["max", "min"]:
136             assert not(GpuCAReduceCuda in ops or
137                        GpuCAReduceCPY in ops or
138                        GpuDnnReduction in ops)
139         else:
140             assert (GpuCAReduceCuda in ops or
141                     GpuCAReduceCPY in ops or
142                     GpuDnnReduction in ops)
143 <a name="7"></a>
144 def test_local_gpualloc_memset_0():
145     i = theano.tensor.iscalar()
146     z <font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.zeros((1,), dtype='float32')
147     o = np.ones((1,), dtype='float32')
148     ones = np.ones((2,), dtype='float32')
149     a = tensor.</b></font>alloc(z, i)
150     f = theano.function([i], a, mode=mode_with_gpu)
151     topo = f.maker.fgraph.toposort()
152     assert len(topo) == 1
153     assert isinstance(topo[0].op, theano.tensor.Alloc)
154     assert (np.asarray(f(6)) == 0).all()
155     assert _check_stack_trace(f)
156     a = tensor.alloc(z, i)
157     f = theano.function([i], a.cumsum(), mode=mode_with_gpu)
158     topo = f.maker.fgraph.toposort()
159     assert len(topo) == 3
160     assert isinstance(topo[0].op, GpuAlloc)
161     assert (np.asarray(f(6)) == 0).all()
162     assert _check_stack_trace(f)
163     a = GpuAlloc(test_ctx_name)(z, i)
164     f = theano.function([i], a, mode=mode_with_gpu)
165     topo = f.maker.fgraph.toposort()
166     assert len(topo) == 1
167     assert isinstance(topo[0].op, GpuAlloc) and topo[0].op.memset_0
168     assert (np.asarray(f(6)) == 0).all()
169     assert _check_stack_trace(f)
170     a = GpuAlloc(test_ctx_name)(o, i)
171     f = theano.function([i], a, mode=mode_with_gpu)
172     topo <font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= f.maker.fgraph.toposort()
173     assert len(topo) == 1
174     assert isinstance(topo[0].op, GpuAlloc)
175     assert not topo[0].op.</b></font>memset_0
176     assert (np.asarray(f(6)) == 1).all()
177     assert _check_stack_trace(f)
178     a = GpuAlloc(test_ctx_name)(ones, i)
179     f = theano.function([i], a, mode=mode_with_gpu)
180     topo = f.maker.fgraph.toposort()
181     assert len(topo) == 1
182     assert isinstance(topo[0].op, GpuAlloc)
183     assert not topo[0].op.memset_0
184     assert (np.asarray(f(2)) == 1).all()
185     assert _check_stack_trace(f)
186 def test_local_gpualloc_empty():
187     i = theano.tensor.iscalar()
188     ii = theano.tensor.iscalar()
189     a = tensor.AllocEmpty('float32')(i)
190     f = theano.function([i], a, mode=mode_with_gpu)
191     topo = f.maker.fgraph.toposort()
192     assert len(topo) == 1
193     assert isinstance(topo[0].op, theano.tensor.AllocEmpty)
194     assert f(3).shape == (3,)
195     assert _check_stack_trace(f)
196     a = tensor.AllocEmpty('float32')(i)
197     f = theano.function([i], a.cumsum(), mode=mode_with_gpu)
198     topo = f.maker.fgraph.toposort()
199     assert len(topo) == 3
200     assert isinstance(topo[0].op, GpuAllocEmpty)
201     assert f(3).shape == (3,)
202     assert _check_stack_trace(f)
203     a = tensor.AllocEmpty('float32')(i, ii)
204     f = theano.function([i, ii], a.cumsum(axis=0), mode=mode_with_gpu)
205     topo = f.maker.fgraph.toposort()
206     assert len(topo) == 3
207     assert isinstance(topo[0].op, GpuAllocEmpty)
208     assert f(3, 4).shape == (3, 4)
209     assert _check_stack_trace(f)
210 def test_rebroadcast():
211     d = np.random.rand(10, 10).astype('float32')
212     v = theano.tensor.fmatrix()
213     up = tensor.unbroadcast(v.sum().dimshuffle('x', 'x'), 0, 1)
214     f = theano.function([v], [up], mode=mode_with_gpu)
215     f(d)
216     topo = f.maker.fgraph.toposort()
217     rebrs = [node for node in topo if isinstance(node.op, tensor.Rebroadcast)]
218     assert len(rebrs) == 1
219     rebr = rebrs[0]
220     assert isinstance(rebr.inputs[0].type, GpuArrayType)
221     assert isinstance(rebr.outputs[0].type, GpuArrayType)
222     assert _check_stack_trace(f)
223 class TestSpecifyShape(test_basic.TestSpecifyShape):
224     mode = mode_with_gpu
225     input_type = GpuArrayType
226 class test_gpu_ifelse(test_ifelse.test_ifelse):
227     mode = mode_with_gpu
228     @staticmethod
229     def cast_output(v):
230         return basic_ops.as_gpuarray_variable(v, test_ctx_name)
231 <a name="24"></a>    shared = staticmethod(gpuarray_shared_constructor)
232     def get_ifelse(self, n):
233         return theano.ifelse.IfElse(n, gpu=True, as_view=<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>True)
234     def test_lifter_with_inputs_of_graph(self):
235         x = tensor.vector()
236         cond = tensor.iscalar()
237         f = theano.function([x</b></font>, cond],
238 <a name="23"></a>                            theano.ifelse.ifelse(cond, x.mean(), x.sum()),
239                             mode=mode_with_gpu)
240         assert f(np.float32([1, 2, 3]), 0) == 6
241         assert _check_stack_trace<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(f)
242         x = tensor.vector()
243         cond = tensor.scalar()
244         f = theano.function([x, cond],
245                             theano.</b></font>ifelse.ifelse(cond, x.mean(), x.sum()),
246                             mode=mode_with_gpu)
247         assert f(np.float32([1, 2, 3]), 0) == 6
248         assert _check_stack_trace(f)
249     def test_lifter_with_shared_var(self):
250         x = tensor.lscalar('x')
251         y = gpuarray_shared_constructor(np.asarray(1, dtype='float32'),
252                                         target=test_ctx_name)
253         z = tensor.constant(2.)
254         a = theano.ifelse.ifelse(x, y, z)
255         with theano.change_flags(on_opt_error='raise'):
256             theano.function([x], [a], mode=mode_with_gpu)
257 def test_print_op():
258     b = tensor.fmatrix()
259     f = theano.function([b], theano.printing.Print()(b) * 2,
260                         mode=mode_with_gpu)
261     topo = f.maker.fgraph.toposort()
262     assert isinstance(topo[0].op, GpuFromHost)
263     assert isinstance(topo[1].op, theano.printing.Print)
264     assert isinstance(topo[2].op, GpuElemwise)
265     assert topo[3].op == host_from_gpu
266     assert _check_stack_trace(f)
267     f(np.random.random((5, 5)).astype('float32'))
268 def test_pdbbreakpoint_op():
269     b = tensor.fmatrix()
270     condition = tensor.gt(b.sum(), 0)
271     b_monitored = PdbBreakpoint(name='TestBreakpoint')(condition, b)
272     output = b_monitored ** 2
273     f = theano.function([b], output, mode=mode_with_gpu)
274     topo = f.maker.fgraph.toposort()
275     assert isinstance(topo[-2].op, GpuElemwise)
276     assert topo[-1].op == host_from_gpu
277     assert _check_stack_trace(f)
278 def test_local_gpu_elemwise_careduce():
279     mode_with_gpu_no_cudnn = mode_with_gpu.excluding('cudnn')
280     x = theano.tensor.matrix()
281     def fn_sum_square(x, axis):
282         return (x * x).sum(axis=axis)
283     def fn_sum_abs(x, axis):
284         return abs(x).sum(axis=axis)
285     def fn_max_abs(x, axis):
286         return abs(x).max(axis=axis)
287     for fn, pre_scalar_op in ((fn_sum_square, theano.scalar.sqr),
288                               (fn_sum_abs, theano.scalar.abs_),
289                               (fn_max_abs, theano.scalar.abs_)):
290         for axis in (None, 0, 1):
291             o = fn(x, axis)
292             f = theano.function([x], o, mode=mode_with_gpu_no_cudnn)
293             topo = f.maker.fgraph.toposort()
294             assert len(topo) == 3
295             assert isinstance(topo[1].op, GpuCAReduceCuda)
296 <a name="17"></a>            assert topo[1].op.pre_scalar_op == pre_scalar_op
297             assert _check_stack_trace(f)
298             data = np.random.rand(3, 4).astype(theano.config.floatX)
299             utt.assert_allclose(fn(data, axis), f(da<font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>ta))
300 def test_local_lift_dot22scalar():
301     x = tensor.matrix()
302     y = tensor.matrix()
303     a = tensor.scalar()
304     o = tensor.</b></font>blas.Dot22Scalar()(x, y, a)
305     f_cpu = theano.function([x, y, a], o)
306     f_gpu = theano.function([x, y, a], o, mode=mode_with_gpu)
307     assert not any(isinstance(n.op, tensor.blas.Dot22Scalar)
308                    for n in f_gpu.maker.fgraph.apply_nodes)
309     assert any(isinstance(n.op, GpuGemm)
310                for n in f_gpu.maker.fgraph.apply_nodes)
311     x_val = np.random.random((2, 3)).astype(theano.config.floatX)
312     y_val = np.random.random((3, 4)).astype(theano.config.floatX)
313     a_val = 0.5
314     utt.assert_allclose(f_cpu(x_val, y_val, a_val), f_gpu(x_val, y_val, a_val))
315     assert _check_stack_trace(f_gpu)
316 def test_local_gpu_subtensor():
317     t = tensor._shared(np.zeros(20, "float32"))
318     f = theano.function([], t[3:4], mode=mode_with_gpu)
319     topo = f.maker.fgraph.toposort()
320     assert any([type(node.op) is tensor.Subtensor for node in topo])
321     assert not any([isinstance(node.op, GpuSubtensor) for node in topo])
322     assert _check_stack_trace(f)
323     t = tensor.fmatrix()
324     f = theano.function([t], t[3:4], mode=mode_with_gpu)
325     topo = f.maker.fgraph.toposort()
326     assert any([type(node.op) is tensor.Subtensor for node in topo])
327     assert not any([isinstance(node.op, GpuSubtensor) for node in topo])
328     assert _check_stack_trace(f)
329     t = tensor.fmatrix()
330     f = theano.function([t], [t[3:4], t + 1], mode=mode_with_gpu)
331     topo = f.maker.fgraph.toposort()
332     assert not any([type(node.op) is tensor.Subtensor for node in topo])
333     assert any([isinstance(node.op, GpuSubtensor) for node in topo])
334     assert _check_stack_trace(f)
335     t = tensor.fmatrix()
336     f = theano.function([t], [t[3:4], t + 1, t], mode=mode_with_gpu)
337     topo = f.maker.fgraph.toposort()
338     assert not any([type(node.op) is tensor.Subtensor for node in topo])
339     assert any([isinstance(node.op, GpuSubtensor) for node in topo])
340     assert _check_stack_trace(f)
341     t = tensor._shared(np.zeros(20, "float32"))
342     f = theano.function([], t[3:4] + 1, mode=mode_with_gpu)
343     topo = f.maker.fgraph.toposort()
344     assert any([type(node.op) is tensor.Subtensor for node in topo])
345     assert not any([isinstance(node.op, GpuSubtensor) for node in topo])
346     assert any([isinstance(node.op, tensor.Elemwise) for node in topo])
347     assert _check_stack_trace(<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>f)
348 def test_local_gpu_elemwise():
349     a = tensor.bmatrix()
350     b = tensor.fmatrix()
351     c = tensor.fmatrix()
352     a_v = (np</b></font>.random.rand(4, 5) * 10).astype("int8")
353     b_v = (np.random.rand(4, 5) * 10).astype("float32")
354     c_v = (np.random.rand(4, 5) * 10).astype("float32")
355     f = theano.function([a, b, c], a + b + c, mode=mode_with_gpu)
356     topo = f.maker.fgraph.toposort()
357     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
358     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
359     utt.assert_allclose(f(a_v, b_v, c_v), a_v + b_v + c_v)
360     assert _check_stack_trace(f)
361 <a name="15"></a>    a_s = theano.scalar.int8()
362     b_s = theano.scalar.float32()
363     c_s = theano.scalar.float32()
364     out_s <font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.scalar.Composite([a_s, b_s, c_s], [a_s + b_s + c_s])
365     out_op = tensor.Elemwise(out_s)
366     f = theano.function([a, b, c], out_op(</b></font>a, b, c), mode=mode_with_gpu)
367     topo = f.maker.fgraph.toposort()
368     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
369     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
370     utt.assert_allclose(f(a_v, b_v, c_v), a_v + b_v + c_v)
371     assert _check_stack_trace(f)
372     return  # Not yet implemeted
373 <a name="22"></a>    a_s = theano.scalar.float32()
374     a = tensor.fmatrix()
375     from theano.scalar.basic import identity
376     out_s = theano.scalar.Composite([<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>a_s, b_s, c_s],
377                                     [identity(a_s), identity(c_s), identity(b_s)])
378     outs_op = tensor.Elemwise(out_s)
379     f = theano.function([a</b></font>, b, c], outs_op(a, b, c), mode=mode_with_gpu)
380     topo = f.maker.fgraph.toposort()
381     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
382     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
383     out = f(a_v, b_v, c_v)
384     utt.assert_allclose(out[0], a_v)
385     utt.assert_allclose(out[1], c_v)
386     utt.assert_allclose(out[2], b_v)
387     assert _check_stack_trace(f)
388     out_s = theano.scalar.Composite([a_s, b_s, c_s], [a_s + b_s, a_s * b_s])
389     outs_op = tensor.Elemwise(out_s)
390     f = theano.function([a, b, c], outs_op(a, b, c), mode=mode_with_gpu)
391     topo = f.maker.fgraph.toposort()
392     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
393     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
394     out = f(a_v, b_v, c_v)
395     utt.assert_allclose(out[0], a_v + b_v)
396     utt.assert_allclose(out[1], a_v * c_v)
397     assert _check_stack_trace(f)
398     c = gpuarray_shared_constructor(np.asarray(c_v, dtype='float32'))
399     f = theano.function([a, b], outs_op(a[::2], b[::2], c[::2]),
400                         mode=mode_with_gpu)
401     out = f(a_v, b_v)
402     utt.assert_allclose(out[0], a_v[::2] + b_v[::2])
403     utt.assert_allclose(out[1], a_v[::2] * c_v[::2])
404     assert _check_stack_trace(f)
405 def test_many_arg_elemwise():
406     rng = np.random.RandomState([1, 2, 3])
407     nb_of_inputs_overflows = []
408     for num_args in [64]:
409         for op_to_test in [theano.tensor.add, theano.tensor.mul]:
410             for nb_dim in [2, 8]:
411                 shapes = [rng.randint(1, 5) for i in range(nb_dim)]
412                 args = [np.cast['float32'](rng.randn(*shapes))
413                         for arg in range(0, num_args)]
414                 symb_args = [theano.tensor.TensorType('float32',
415                                                       (False,) * nb_dim)()
416                              for arg in range(0, num_args)]
417                 outputs = []
418                 for mode in [mode_with_gpu, mode_without_gpu]:
419                     output = op_to_test(*symb_args)
420                     f = theano.function(symb_args, output, mode=mode)
421                     outputs.append(f(*args))
422                     if mode is mode_with_gpu:
423                         nb_of_inputs_overflows.append(
424                             max_inputs_to_GpuElemwise(output.owner) - num_args)
425                         nodelst = [node for node in f.maker.fgraph.apply_nodes]
426                         assert any(isinstance(node.op, GpuElemwise)
427                                    for node in nodelst)
428                         assert not any(isinstance(node.op, Elemwise)
429                                        for node in nodelst
430                                        if not isinstance(node.op, GpuElemwise))
431                 results_gpu, results_cpu = outputs
432                 utt.assert_allclose(results_gpu, results_cpu)
433     assert any(overflow &gt;= 0 for overflow in nb_of_inputs_overflows)
434     assert any(overflow &lt; 0 for overflow in nb_of_inputs_overflows)
435 def test_not_useless_scalar_gpuelemwise():
436 <a name="12"></a>    with theano<font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.change_flags(warn_float64='ignore'):
437         X = tensor.fmatrix()
438         x = np.random.randn(32, 32).astype(</b></font>np.float32)
439         m1 = theano.shared<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(np.random.randn(32, 32).astype(np.float32))
440         loss = (X - tensor.dot(X, m1)).norm(L=</b></font>2)
441         lr = theano.shared(np.asarray(.001, dtype=np.float32))
442         grad = tensor.grad(loss, m1)
443         train = theano.function(inputs=[X], updates=[(m1, m1 - lr * grad)],
444                                 mode=mode_with_gpu)
445         train(x)
446         topo = train.maker.fgraph.toposort()
447         gemms = [app for app in topo if isinstance(app.op, GpuGemm)]
448         assert len(gemms) == 2
449         assert isinstance(gemms[1].inputs[1].owner.op, tensor.Elemwise)
450 <a name="20"></a>def test_local_lift_abstractconv_gpu_shape():
451     prev = theano.config.on_opt_error
452     try:
453         theano.config.on_opt_error <font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= 'raise'
454         s = tensor.ivector()
455         a = tensor.ftensor4()
456         b = tensor.ftensor4()
457         c = tensor.</b></font>nnet.abstract_conv.AbstractConv2d_gradWeights()(a, b, s)
458 <a name="19"></a>        f = theano.function([s, a, b], c, mode=mode_with_gpu)
459         assert _check_stack_trace(f)
460     finally:
461         theano.config.on_opt_error =<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b> prev
462 def test_local_assert_no_cpu_op():
463 <a name="14"></a>    rng = np.random.RandomState(utt.fetch_seed())
464     m = rng.uniform(-1, 1, (10</b></font>, 10)).astype("float32")
465     ms = gpuarray_shared_constructor(m, name="m_shared")
466     out = theano.tensor<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.tanh(ms).dot(ms.T)
467     mode_local_assert = mode_with_gpu.including("assert_no_cpu_op")
468     mode_local_assert = mode_local_assert.excluding("local_gpua_elemwise")
469     old = theano.</b></font>config.assert_no_cpu_op
470     old2 = theano.config.on_opt_error
471     try:
472         theano.config.assert_no_cpu_op = 'raise'
473         theano.config.on_opt_error = 'ignore'
474         assert_raises(AssertionError, theano.function,
475                       [], out, mode=mode_local_assert)
476     finally:
477         theano.config.assert_no_cpu_op = old
478         theano.config.on_opt_error = old2
479     try:
480         theano.config.assert_no_cpu_op = 'ignore'
481 <a name="6"></a>        f = theano.function([], out, mode=mode_local_assert)
482         assert _check_stack_trace(f)
483     finally:
484         theano.config.assert_no_cpu_op =<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b> old
485 def test_no_complex():
486     width_var = tensor.cscalar()
487     freq_var = tensor.fscalar()
488     signal_var = tensor.fscalar()
489     stft_out = tensor.exp(width_var * freq_var) * signal_var
490     f = theano.function(</b></font>[width_var, freq_var, signal_var], stft_out,
491                         mode=mode_with_gpu)
492     assert _check_stack_trace(f)
493 @utt.assertFailure_fast
494 <a name="4"></a>def test_local_lift_solve():
495     if not cusolver_available or not slinalg.imported_scipy:
496         raise SkipTest('No cuSolver or SciPy')
497     A <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= tensor.fmatrix()
498     b = tensor.fmatrix()
499     o = slinalg.solve(A, b)
500     f_cpu = theano.function([A, b], o, mode_without_gpu)
501     f_gpu = theano.function([A, b], o, mode=</b></font>mode_with_gpu)
502     assert not any(isinstance(n.op, slinalg.Solve)
503                    for n in f_gpu.maker.fgraph.apply_nodes)
504     assert any(isinstance(n.op, GpuCusolverSolve) and n.op.inplace
505                for n in f_gpu.maker.fgraph.apply_nodes)
506     A_val = np.random.uniform(-0.4, 0.4, (5, 5)).astype("float32")
507     b_val = np.random.uniform(-0.4, 0.4, (5, 3)).astype("float32")
508     utt.assert_allclose(f_cpu(A_val, b_val), f_gpu(A_val, b_val))
509     assert _check_stack_trace(f_gpu)
510 <a name="2"></a>def test_gpu_solve_not_inplace():
511     if not cusolver_available or not slinalg.imported_scipy:
512         raise SkipTest('No cuSolver or Scipy')
513     A <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= tensor.fmatrix()
514     b = tensor.fmatrix()
515     s = slinalg.solve(A, b)
516     o = tensor.dot(A, s)
517     f_cpu = theano.function([A, b], o, mode_without_gpu)
518     f_gpu = theano.function([A, b], o, mode=</b></font>mode_with_gpu)
519     count_not_inplace = len([n.op for n in f_gpu.maker.fgraph.apply_nodes
520                              if isinstance(n.op, GpuCusolverSolve) and not n.op.inplace])
521     assert count_not_inplace == 1, count_not_inplace
522     A_val = np.random.uniform(-0.4, 0.4, (5, 5)).astype("float32")
523     b_val = np.random.uniform(-0.4, 0.4, (5, 3)).astype("float32")
524     utt.assert_allclose(f_cpu(A_val, b_val), f_gpu(A_val, b_val))
525 @utt.assertFailure_fast
526 def test_local_lift_cholesky():
527     if not cusolver_available or not slinalg.imported_scipy:
528         raise SkipTest('No cuSolver or Scipy')
529     A = tensor.fmatrix()
530     o = slinalg.cholesky(A)
531     f_cpu = theano.function([A], o, mode=mode_without_gpu)
532     f_gpu = theano.function([A], o, mode=mode_with_gpu)
533     assert not any(isinstance(n.op, slinalg.Cholesky)
534                    for n in f_gpu.maker.fgraph.apply_nodes)
535     assert any(isinstance(n.op, GpuCholesky) and n.op.inplace
536                for n in f_gpu.maker.fgraph.apply_nodes)
537     M_val = np.random.normal(size=(3, 3)).astype("float32")
538     A_val = M_val.dot(M_val.T)
539     utt.assert_allclose(f_cpu(A_val), f_gpu(A_val))
540 <a name="5"></a>def test_gpu_cholesky_not_inplace():
541     if not cusolver_available or not slinalg.imported_scipy:
542         raise SkipTest('No cuSolver or SciPy')
543     A <font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= tensor.fmatrix()
544     A_squared = A**2
545     B = slinalg.cholesky(A_squared)
546     D = B + A_squared
547     f_cpu = theano.function([A], D, mode=mode_without_gpu)
548     f_gpu = theano.function([A], D, mode=mode_with_gpu)
549     count_cholesky_not_inplace =</b></font> len([n.op for n in f_gpu.maker.fgraph.apply_nodes
550                                       if isinstance(n.op, GpuCholesky) and not n.op.inplace])
551     assert count_cholesky_not_inplace == 1, count_cholesky_not_inplace
552     M_val = np.random.normal(size=(3, 3)).astype("float32")
553     A_val = M_val.dot(M_val.T)
554     utt.assert_allclose(f_cpu(A_val), f_gpu(A_val))
555 def test_local_gpua_advanced_incsubtensor():
556     target = tensor.ftensor4()
557     y = target.dimshuffle(1, 0, 2, 3).flatten(ndim=1)
558     w = tensor.ones_like(y)
559     w = tensor.set_subtensor(w[tensor.eq(y, 1.0).nonzero()], 100)
560     w = tensor.set_subtensor(w[tensor.eq(y, -1.0).nonzero()], 0)
561     f = theano.function([target], w)
562     assert _check_stack_trace(f)
563 def test_batched_dot_lifter():
564     rng = np.random.RandomState(utt.fetch_seed())
565     def randX(*args):
566         return rng.rand(*args).astype(theano.config.floatX)
567     cases = [
568         (randX(3, 5, 7), randX(3, 7)),
569         (randX(3, 5), randX(3, 5, 7)),
570         (randX(3, 5), randX(3, 5)),
571         (rng.rand(3, 5, 7).astype('float32'), randX(3, 7, 9)),
572         (rng.rand(3, 5, 7).astype('float64'), randX(3, 7, 9))]
573     for x_val, y_val in cases:
574         x = tensor.TensorType(broadcastable=[s == 1 for s in x_val.shape],
575                               dtype=x_val.dtype)('x')
576         y = tensor.TensorType(broadcastable=[s == 1 for s in y_val.shape],
577                               dtype=y_val.dtype)('y')
578         z = tensor.batched_dot(x, y)
579         f = theano.function([x, y], z, mode=mode_with_gpu)
580         f(x_val, y_val)
581         assert check_stack_trace(f, ops_to_check='all')
582 <a name="18"></a>
583 def test_crossentropycategorical1hot_lifter():
584     rng = np.random<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.RandomState(utt.fetch_seed())
585     x = tensor.matrix()
586     y = tensor.lvector()
587     z = tensor.</b></font>nnet.crossentropy_categorical_1hot(x, y)
588     gx = theano.grad(z.mean(), x)
589     f = theano.function([x, y], [z, gx], mode=mode_with_gpu)
590     assert not any(isinstance(n.op, (tensor.nnet.CrossentropyCategorical1Hot,
591                                      tensor.nnet.CrossentropyCategorical1HotGrad))
592                    for n in f.maker.fgraph.apply_nodes)
593     f(rng.uniform(0.1, 0.9, (13, 5)).astype(theano.config.floatX),
594       rng.randint(5, size=(13,)))
595 class Conv_opt_test(unittest.TestCase):
596     def optimizer_2d(self, input_shapes, direction, include_tags, exclude_tags,
597                      op, border_mode='valid', subsample=(1, 1),
598                      filter_dilation=(1, 1), num_groups=1, unshared=False,
599                      optimiser=None):
600         inp1 = theano.shared(np.random.random(input_shapes[0]).astype(theano.config.floatX))
601         inp2 = theano.shared(np.random.random(input_shapes[1]).astype(theano.config.floatX))
602         if op is None:
603             inp1 = basic_ops.as_gpuarray_variable(inp1, test_ctx_name)
604             inp2 = basic_ops.as_gpuarray_variable(inp2, test_ctx_name)
605         if(direction == 0):
606             conv_op = abstract_conv.AbstractConv2d(input_shapes[0],
607                                                    input_shapes[1],
608                                                    border_mode=border_mode,
609                                                    subsample=subsample,
610                                                    filter_dilation=filter_dilation,
611                                                    num_groups=num_groups,
612                                                    unshared=unshared)(inp1, inp2)
613         if(direction == 1):
614             conv_op = abstract_conv.AbstractConv2d_gradWeights(imshp=input_shapes[0],
615                                                                kshp=input_shapes[2],
616                                                                border_mode=border_mode,
617                                                                subsample=subsample,
618                                                                filter_dilation=filter_dilation,
619                                                                num_groups=num_groups,
620                                                                unshared=unshared)(inp1,
621                                                                                   inp2,
622                                                                                   input_shapes[2][-2:])
623         if(direction == 2):
624             conv_op = abstract_conv.AbstractConv2d_gradInputs(imshp=input_shapes[2],
625                                                               kshp=input_shapes[1],
626                                                               border_mode=border_mode,
627                                                               subsample=subsample,
628                                                               filter_dilation=filter_dilation,
629                                                               num_groups=num_groups,
630                                                               unshared=unshared)(inp2,
631                                                                                  inp1,
632                                                                                  input_shapes[2][-2:])
633         theano.config.metaopt.optimizer_including = include_tags
634         theano.config.metaopt.optimizer_excluding = exclude_tags
635         mode = mode_with_gpu.including('conv_meta').excluding('conv_dnn').excluding('conv_gemm')
636         if op is None:
637             assert optimiser.transform(conv_op.owner) is None
638         else:
639             ref_func = theano.function([], conv_op, mode=mode_with_gpu)
640             with theano.change_flags(mode=mode):
641                 conv_func = theano.function([], conv_op, mode=mode)
642             assert any([isinstance(node.op, op)
643                         for node in conv_func.maker.fgraph.toposort()])
644             utt.assert_allclose(conv_func(), ref_func())
645     def optimizer_3d(self, input_shapes, direction, include_tags, exclude_tags,
646                      op, border_mode='valid', subsample=(1, 1, 1),
647                      filter_dilation=(1, 1, 1), num_groups=1, optimiser=None):
648         inp1 = theano.shared(np.random.random(input_shapes[0]).astype(theano.config.floatX))
649         inp2 = theano.shared(np.random.random(input_shapes[1]).astype(theano.config.floatX))
650         if op is None:
651             inp1 = basic_ops.as_gpuarray_variable(inp1, None)
652             inp2 = basic_ops.as_gpuarray_variable(inp2, None)
653         if(direction == 0):
654             conv_op = abstract_conv.AbstractConv3d(input_shapes[0],
655                                                    input_shapes[1],
656                                                    border_mode=border_mode,
657                                                    subsample=subsample,
658                                                    filter_dilation=filter_dilation,
659                                                    num_groups=num_groups)(inp1, inp2)
660         if(direction == 1):
661             conv_op = abstract_conv.AbstractConv3d_gradWeights(input_shapes[0],
662                                                                input_shapes[2],
663                                                                border_mode=border_mode,
664                                                                subsample=subsample,
665                                                                filter_dilation=filter_dilation,
666                                                                num_groups=num_groups)(inp1,
667                                                                                       inp2,
668                                                                                       input_shapes[2][-3:])
669         if(direction == 2):
670             conv_op = abstract_conv.AbstractConv3d_gradInputs(input_shapes[2],
671                                                               input_shapes[1],
672                                                               border_mode=border_mode,
673                                                               subsample=subsample,
674                                                               filter_dilation=filter_dilation,
675                                                               num_groups=num_groups)(inp2,
676                                                                                      inp1,
677                                                                                      input_shapes[2][-3:])
678         theano.config.metaopt.optimizer_including = include_tags
679         theano.config.metaopt.optimizer_excluding = exclude_tags
680         mode = mode_with_gpu.including('conv_meta').excluding('conv_dnn').excluding('conv_gemm')
681         if op is None:
682             assert optimiser.transform(conv_op.owner) is None
683             return
684         elif op != 'conv3d2d':
685             with theano.change_flags(mode=mode):
686                 conv_func = theano.function([], conv_op, mode=mode)
687             assert any([isinstance(node.op, op)
688                        for node in conv_func.maker.fgraph.toposort()])
689         else:
690             with theano.change_flags(mode=mode):
691                 conv_func = theano.function(
692                     [], conv_op,
693                     mode=mode_with_gpu.including('conv_meta'))
694         ref_func = theano.function([], conv_op, mode=mode_with_gpu)
695         utt.assert_allclose(conv_func(), ref_func())
696     def test_optimizers_2d(self):
697         if theano.config.cxx == "":
698             raise SkipTest("Need a c compiler.")
699         imshp2d = [(2, 3, 5, 5), (2, 2, 5, 7), (2, 1, 3, 3)]
700         kshp2d = [(4, 3, 3, 3), (3, 2, 3, 5), (4, 1, 1, 1)]
701         tshp2d = [(2, 4, 3, 3), (2, 3, 3, 3), (2, 4, 3, 3)]
702 <a name="0"></a>
703         for imshp, kshp, tshp in zip(imshp2d, kshp2d, tshp2d):
704             self<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.optimizer_2d([imshp, kshp, tshp], 0,
705                               '',
706                               'conv_dnn:alternative',
707                               blas.GpuCorrMM)
708             self.optimizer_2d([imshp, kshp, tshp], 0,
709                               'alternative',
710                               'conv_dnn:default',
711                               blas.GpuCorrMM_gradWeights)
712             self.optimizer_2d([imshp, kshp, tshp], 0,
713                               '',
714                               'conv_gemm:alternative',
715                               dnn.GpuDnnConv)
716             self.optimizer_2d([imshp, kshp, tshp], 0,
717                               'alternative',
718                               'conv_gemm:default',
719                               dnn.GpuDnnConvGradW)
720             self.optimizer_2d([imshp, tshp, kshp], 1,
721                               '',
722                               'conv_dnn:alternative',
723                               blas.GpuCorrMM_gradWeights)
724             self.optimizer_2d([imshp, tshp, kshp], 1,
725                               'alternative',
726                               'conv_dnn:default',
727                               blas.GpuCorrMM)
728             self.optimizer_2d([imshp, tshp, kshp], 1,
729                               '',
730                               'conv_gemm:alternative',
731                               dnn.GpuDnnConvGradW)
732             self.optimizer_2d([imshp, tshp, kshp], 1,
733                               'alternative',
734                               'conv_gemm:default',
735                               dnn.GpuDnnConv)
736             self.optimizer_2d([tshp, kshp, imshp], 2,
737                               '',
738                               'conv_dnn:alternative',
739                               blas.GpuCorrMM_gradInputs)
740             self.optimizer_2d([tshp, kshp, imshp], 2,
741                               'alternative',
742                               'conv_dnn:default',
743                               blas.GpuCorrMM)
744             self.optimizer_2d([tshp, kshp, imshp], 2,
745                               '',
746                               'conv_gemm:alternative',
747                               dnn.GpuDnnConvGradI)
748             self.optimizer_2d([tshp, kshp, imshp], 2,
749                               'alternative',
750                               'conv_gemm:default',
751                               dnn.</b></font>GpuDnnConv)
752     def test_optimizers_3d(self):
753         if theano.config.cxx == "":
754             raise SkipTest("Need a c compiler.")
755         imshp3d = [(2, 3, 5, 5, 5), (2, 2, 5, 7, 5), (2, 1, 3, 3, 3)]
756         kshp3d = [(4, 3, 3, 3, 3), (3, 2, 3, 5, 3), (4, 1, 1, 1, 1)]
757         tshp3d = [(2, 4, 3, 3, 3), (2, 3, 3, 3, 3), (2, 4, 3, 3, 3)]
758         for imshp, kshp, tshp in zip(imshp3d, kshp3d, tshp3d):
759             self.optimizer_3d([imshp, kshp, tshp], 0,
760                               '',
761                               'conv_dnn:alternative:conv3d2d',
762                               blas.GpuCorr3dMM)
763             self.optimizer_3d([imshp, kshp, tshp], 0,
764 <a name="1"></a>                              'alternative',
765                               'conv_dnn:default:conv3d2d',
766                               blas.GpuCorr3dMM_gradWeights)
767             self.optimizer_3d([<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>imshp, kshp, tshp], 0,
768                               'conv3d2d',
769                               'default',
770                               'conv3d2d')
771             self.optimizer_3d([imshp, kshp, tshp], 0,
772                               'alternative',
773                               'conv_gemm:default:conv3d2d',
774                               dnn.GpuDnnConvGradW)
775             self.optimizer_3d([imshp, kshp, tshp], 0,
776                               '',
777                               'conv_gemm:alternative:conv3d2d',
778                               dnn.GpuDnnConv)
779             self.optimizer_3d([imshp, tshp, kshp], 1,
780                               '',
781                               'conv_dnn:alternative',
782                               blas.GpuCorr3dMM_gradWeights)
783             self.optimizer_3d([imshp, tshp, kshp], 1,
784                               'alternative',
785                               'conv_dnn:default',
786                               blas.GpuCorr3dMM)
787             self.optimizer_3d([imshp, tshp, kshp], 1,
788                               'alternative',
789                               'conv_gemm:default',
790                               dnn.GpuDnnConv)
791             self.optimizer_3d([imshp, tshp, kshp], 1,
792                               '',
793                               'conv_gemm:alternative',
794                               dnn.GpuDnnConvGradW)
795             self.optimizer_3d([tshp, kshp, imshp], 2,
796                               '',
797                               'conv_dnn:alternative',
798                               blas.GpuCorr3dMM_gradInputs)
799             self.optimizer_3d([tshp, kshp, imshp], 2,
800                               'alternative',
801                               'conv_dnn:default',
802                               blas.GpuCorr3dMM)
803             self.optimizer_3d([tshp, kshp, imshp], 2,
804                               'alternative',
805                               'conv_gemm:default',
806                               dnn.GpuDnnConv)
807             self.optimizer_3d([tshp, kshp, imshp], 2,
808                               '',
809                               'conv_gemm:alternative',
810                               dnn.</b></font>GpuDnnConvGradI)
811     def test_optimizers_non_default(self):
812         if theano.config.cxx == "":
813             raise SkipTest("Need a c compiler.")
814         imshp2d = [(2, 3, 5, 5), (4, 2, 5, 5)]
815         kshp2d = [(4, 3, 3, 3), (3, 2, 3, 3)]
816         filter_dilation = [(1, 1), (2, 2)]
817         for imshp, kshp, fdil in zip(imshp2d, kshp2d, filter_dilation):
818             self.optimizer_2d([imshp, kshp], 0,
819                               '',
820                               'conv_dnn:alternative',
821                               blas.GpuCorrMM,
822                               border_mode='full',
823                               filter_dilation=fdil)
824             self.optimizer_2d([imshp, kshp], 0,
825                               'alternative',
826                               'conv_dnn:default',
827                               blas.GpuCorrMM_gradInputs,
828                               border_mode='full',
829                               filter_dilation=fdil)
830             self.optimizer_2d([imshp, kshp], 0,
831                               '',
832                               'conv_gemm:alternative',
833                               dnn.GpuDnnConv,
834                               border_mode='full',
835                               filter_dilation=fdil)
836             self.optimizer_2d([imshp, kshp], 0,
837                               'alternative',
838                               'conv_gemm:default',
839                               dnn.GpuDnnConvGradI,
840                               border_mode='full',
841                               filter_dilation=fdil)
842         imshp3d = [(2, 3, 5, 5, 5), (4, 2, 5, 5, 5)]
843         kshp3d = [(4, 3, 3, 3, 3), (3, 2, 3, 3, 3)]
844         filter_dilation = [(1, 1, 1), (2, 2, 2)]
845         for imshp, kshp, fdil in zip(imshp3d, kshp3d, filter_dilation):
846             self.optimizer_3d([imshp, kshp], 0,
847                               '',
848                               'conv_dnn:alternative:conv3d2d',
849                               blas.GpuCorr3dMM,
850                               border_mode='full',
851                               filter_dilation=fdil)
852             self.optimizer_3d([imshp, kshp], 0,
853                               'alternative',
854                               'conv_dnn:default:conv3d2d',
855                               blas.GpuCorr3dMM_gradInputs,
856                               border_mode='full',
857                               filter_dilation=fdil)
858             self.optimizer_3d([imshp, kshp], 0,
859                               '',
860                               'conv_gemm:alternative:conv3d2d',
861                               dnn.GpuDnnConv,
862                               border_mode='full',
863                               filter_dilation=fdil)
864             self.optimizer_3d([imshp, kshp], 0,
865                               'alternative',
866                               'conv_gemm:default:conv3d2d',
867                               dnn.GpuDnnConvGradI,
868                               border_mode='full',
869                               filter_dilation=fdil)
870         imshp2d = [(2, 6, 5, 5), (2, 4, 5, 5)]
871         kshp2d = [(3, 2, 3, 3), (2, 2, 3, 3)]
872         tshp2d = [(2, 3, 3, 3), (2, 2, 3, 3)]
873         num_groups = [3, 2]
874         for imshp, kshp, tshp, groups in zip(imshp2d, kshp2d, tshp2d, num_groups):
875             self.optimizer_2d([imshp, kshp, tshp], 0,
876 <a name="8"></a>                              '',
877                               'conv_dnn:alternative',
878                               blas.GpuCorrMM,
879                               num_groups<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=groups)
880             self.optimizer_2d([imshp, kshp, tshp], 0,
881                               '',
882                               'conv_gemm:alternative',
883                               dnn.GpuDnnConv,
884                               num_groups=groups)
885             self.optimizer_2d([imshp, tshp, kshp], 1,
886                               '',
887                               'conv_dnn:alternative',
888                               blas.GpuCorrMM_gradWeights,
889                               num_groups=groups)
890             self.optimizer_2d([imshp, tshp, kshp], 1,
891                               '',
892                               'conv_gemm:alternative',
893                               dnn.</b></font>GpuDnnConvGradW,
894                               num_groups=groups)
895             self.optimizer_2d([tshp, kshp, imshp], 2,
896                               '',
897                               'conv_dnn:alternative',
898                               blas.GpuCorrMM_gradInputs,
899                               num_groups=groups)
900             self.optimizer_2d([tshp, kshp, imshp], 2,
901                               '',
902                               'conv_gemm:alternative',
903                               dnn.GpuDnnConvGradI,
904                               num_groups=groups)
905         imshp2d = [(2, 2, 4, 4), (3, 2, 5, 3)]
906         kshp2d = [(2, 2, 2, 2, 3, 3), (2, 3, 1, 2, 3, 3)]
907         tshp2d = [(2, 2, 2, 2), (3, 2, 3, 1)]
908         for imshp, kshp, tshp, groups in zip(imshp2d, kshp2d, tshp2d, num_groups):
909             self.optimizer_2d([imshp, kshp, tshp], 0,
910                               '',
911                               'alternative',
912                               blas.GpuCorrMM,
913                               unshared=True)
914             self.optimizer_2d([imshp, tshp, kshp], 1,
915                               '',
916                               'alternative',
917                               blas.GpuCorrMM_gradWeights,
918                               unshared=True)
919             self.optimizer_2d([tshp, kshp, imshp], 2,
920                               '',
921                               'alternative',
922                               blas.GpuCorrMM_gradInputs,
923                               unshared=True)
924         imshp3d = [(2, 6, 5, 5, 5), (2, 4, 5, 5, 5)]
925         kshp3d = [(3, 2, 3, 3, 3), (2, 2, 3, 3, 3)]
926         tshp3d = [(2, 3, 3, 3, 3), (2, 2, 3, 3, 3)]
927         num_groups = [3, 2]
928         for imshp, kshp, tshp, groups in zip(imshp3d, kshp3d, tshp3d, num_groups):
929             self.optimizer_3d([imshp, kshp, tshp], 0,
930                               '',
931                               'conv_dnn:alternative:conv3d2d',
932                               blas.GpuCorr3dMM,
933                               num_groups=groups)
934             self.optimizer_3d([imshp, kshp, tshp], 0,
935                               '',
936                               'conv_gemm:alternative:conv3d2d',
937                               dnn.GpuDnnConv,
938                               num_groups=groups)
939             self.optimizer_3d([imshp, tshp, kshp], 1,
940                               '',
941                               'conv_dnn:alternative:conv3d2d',
942                               blas.GpuCorr3dMM_gradWeights,
943                               num_groups=groups)
944             self.optimizer_3d([imshp, tshp, kshp], 1,
945                               '',
946                               'conv_gemm:alternative:conv3d2d',
947                               dnn.GpuDnnConvGradW,
948                               num_groups=groups)
949             self.optimizer_3d([tshp, kshp, imshp], 2,
950                               '',
951                               'conv_dnn:alternative:conv3d2d',
952                               blas.GpuCorr3dMM_gradInputs,
953                               num_groups=groups)
954             self.optimizer_3d([tshp, kshp, imshp], 2,
955                               '',
956                               'conv_gemm:alternative:conv3d2d',
957                               dnn.GpuDnnConvGradI,
958                               num_groups=groups)
959     def test_returns_none_2d(self):
960         if theano.config.cxx == "":
961             raise SkipTest("Need a c compiler.")
962         imshp = (2, 3, 5, 5)
963         kshp = (4, 3, 3, 3)
964         tshp = (2, 4, 3, 3)
965         conv_direction = [0, 1, 2]
966         optimisers = [[opt.local_abstractconv_gemm_alt,
967                        opt.local_abstractconv_cudnn_alt],
968                       [opt.local_abstractconv_gemm_gradweights_alt,
969                        opt.local_abstractconv_cudnn_alt],
970                       [opt.local_abstractconv_gradinputs_gemm_alt,
971                        opt.local_abstractconv_cudnn_alt]]
972         for opt_direction, direction in zip(optimisers, conv_direction):
973             for optimiser in opt_direction:
974                 self.optimizer_2d([imshp, kshp, tshp],
975                                   direction,
976                                   '',
977                                   '',
978                                   None,
979                                   subsample=(2, 2),
980                                   optimiser=optimiser)
981         for opt_direction, direction in zip(optimisers, conv_direction):
982             for optimiser in opt_direction:
983                 self.optimizer_2d([imshp, kshp, tshp],
984                                   direction,
985                                   '',
986                                   '',
987                                   None,
988                                   num_groups=3,
989                                   optimiser=optimiser)
990         for opt_direction, direction in zip(optimisers, conv_direction):
991             for optimiser in opt_direction:
992                 self.optimizer_2d([imshp, kshp, tshp],
993                                   direction,
994                                   '',
995                                   '',
996                                   None,
997                                   border_mode='half',
998                                   optimiser=optimiser)
999         for optimiser in optimisers[1]:
1000             self.optimizer_2d([imshp, kshp, tshp],
1001                               1,
1002                               '',
1003                               '',
1004                               None,
1005                               filter_dilation=(2, 2),
1006                               optimiser=optimiser)
1007         imshp = (2, 2, 4, 4)
1008         kshp = (2, 2, 2, 2, 3, 3)
1009         tshp = (2, 2, 2, 2)
1010         shape_perms = [[imshp, kshp, tshp],
1011                        [imshp, tshp, kshp],
1012                        [tshp, kshp, imshp]]
1013         for opt_direction, direction, perms in zip(optimisers, conv_direction,
1014                                                    shape_perms):
1015             for optimiser in opt_direction:
1016                 self.optimizer_2d(perms,
1017                                   direction,
1018                                   '',
1019                                   '',
1020                                   None,
1021                                   unshared=True,
1022                                   optimiser=optimiser)
1023     def test_returns_none_3d(self):
1024         if theano.config.cxx == "":
1025             raise SkipTest("Need a c compiler.")
1026         imshp = (2, 3, 5, 5, 5)
1027         kshp = (4, 3, 3, 3, 3)
1028         tshp = (2, 4, 3, 3, 3)
1029         conv_direction = [0, 1, 2]
1030         optimisers = [[opt.local_abstractconv3d_alt,
1031                        opt.local_abstractconv3d_cudnn_alt],
1032                       [opt.local_abstractconv3d_gemm_gradweights_alt,
1033                        opt.local_abstractconv3d_cudnn_alt],
1034                       [opt.local_abstractconv3d_gradinputs_gemm_alt,
1035                        opt.local_abstractconv3d_cudnn_alt]]
1036         for opt_direction, direction in zip(optimisers, conv_direction):
1037             for optimiser in opt_direction:
1038                 self.optimizer_3d([imshp, kshp, tshp],
1039                                   direction,
1040                                   '',
1041                                   '',
1042                                   None,
1043                                   subsample=(2, 2, 2),
1044                                   optimiser=optimiser)
1045         for opt_direction, direction in zip(optimisers, conv_direction):
1046             for optimiser in opt_direction:
1047                 self.optimizer_3d([imshp, kshp, tshp],
1048                                   direction,
1049                                   '',
1050                                   '',
1051                                   None,
1052                                   num_groups=3,
1053                                   optimiser=optimiser)
1054         for opt_direction, direction in zip(optimisers, conv_direction):
1055             for optimiser in opt_direction:
1056                 self.optimizer_3d([imshp, kshp, tshp],
1057                                   direction,
1058                                   '',
1059                                   '',
1060                                   None,
1061                                   border_mode='half',
1062                                   optimiser=optimiser)
1063         for optimiser in optimisers[1]:
1064             self.optimizer_3d([imshp, kshp, tshp],
1065                               1,
1066                               '',
1067                               '',
1068                               None,
1069                               filter_dilation=(2, 2, 2),
1070                               optimiser=optimiser)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_blas_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <a name="3"></a><font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
2 from copy import copy
3 from itertools import product as itertools_product
4 from unittest import TestCase
5 import numpy as np
6 from numpy import (arange, array, common_type, complex64, complex128, float32,
7                    float64, newaxis, shape, transpose, zeros)
8 from numpy.testing import assert_array_almost_equal
9 from itertools import product
10 from six.moves import xrange
11 import theano
12 import theano.tensor as T
13 from theano import tensor, In, shared, config
14 from theano.compat import exc_message
15 from theano.printing import pp
16 from theano.tensor.blas import (_dot22, _dot22scalar, res_is_a, _as_scalar,
17                                 _is_real_matrix, _gemm_canonicalize,
18                                 _factor_canonicalized, Gemm, Gemv,
19                                 gemm_inplace, gemm_no_inplace,
20                                 InconsistencyError, Ger, ger, ger_destructive)
21 from theano.tests import unittest_tools
22 from .test_basic import (as_tensor_variable, inplace_func,
23                          compile, inplace)
24 import theano.tensor.blas_scipy
25 from</b></font> theano.tests.unittest_tools import attr
26 if config.mode == 'FAST_COMPILE':
27     mode_not_fast_compile = 'FAST_RUN'
28 else:
29     mode_not_fast_compile = config.mode
30 mode_blas_opt = theano.compile.get_default_mode().including(
31     'BlasOpt', 'specialize', 'InplaceBlasOpt')
32 mode_blas_opt = mode_blas_opt.excluding('c_blas')
33 def test_dot_eq():
34     assert T.Dot() == T.Dot()
35 def sharedX(x, name):
36     return theano.shared(np.asarray(x, config.floatX), name=name)
37 class t_gemm(TestCase):
38     def setUp(self):
39         unittest_tools.seed_rng()
40         Gemm.debug = False
41     @staticmethod
42     def _gemm(z, a, x, y, b):
43         assert a.shape == ()
44         assert b.shape == ()
45         return b * z + a * np.dot(x, y)
46     @staticmethod
47     def rand(*args):
48         return np.random.rand(*args)
49 <a name="5"></a>
50     def cmp(self, z_, a_, x_, y_, b_):
51         for dtype in ['float32', 'float64', 'complex64', 'complex128']:
52             z <font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.asarray(z_, dtype=dtype)
53             a = np.asarray(a_, dtype=dtype)
54             x = np.asarray(x_, dtype=dtype)
55             y = np.asarray(y_, dtype=dtype)
56 <a name="15"></a>            b =</b></font> np.asarray(b_, dtype=dtype)
57             def cmp_linker(z, a, x, y, b, l):
58                 z, a, x, y, b <font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [np.asarray(p) for p in (z, a, x, y, b)]
59                 z_orig = z.copy()
60                 tz, ta, tx, ty, tb = [as_tensor_variable(p).type(</b></font>)
61                                       for p in (z, a, x, y, b)]
62                 f = inplace_func([tz, ta, tx, ty, tb],
63                                  gemm_inplace(tz, ta, tx, ty, tb),
64                                  mode=compile.Mode(optimizer=None, linker=l))
65                 f(z, a, x, y, b)
66                 z_after = self._gemm(z_orig, a, x, y, b)
67                 unittest_tools.assert_allclose(z_after, z)
68                 if a == 0.0 and b == 1.0:
69                     return
70                 elif z_orig.size == 0:
71                     self.assertTrue(z.size == 0)
72                 else:
73                     self.assertFalse(np.all(z_orig == z))
74             cmp_linker(copy(z), a, x, y, b, 'c|py')
75             cmp_linker(copy(z), a, x, y, b, 'py')
76             if (not dtype.startswith("complex") and theano.config.cxx):
77                 cmp_linker(copy(z), a, x, y, b, 'c')
78     def test0a(self):
79         Gemm.debug = True
80         try:
81             gemm_no_inplace([1.], 1., [1.], [1.], 1.)
82         except TypeError as e:
83             if exc_message(e) is Gemm.E_rank:
84                 return
85         self.fail()
86     def test0(self):
87         try:
88             self.cmp(1., 0., 1.0, 1.0, 1.0)
89         except TypeError as e:
90             if exc_message(e) is Gemm.E_rank:
91                 return
92         self.fail()
93     def test2(self):
94         try:
95             self.cmp(2., 1.0, [3, 2, 1.], [[1], [2], [3.]], 1.0)
96         except TypeError as e:
97             self.assertTrue(exc_message(e) == Gemm.E_rank)
98             return
99         self.fail()
100     def test4(self):
101         self.cmp(self.rand(3, 4), 1.0, self.rand(3, 5), self.rand(5, 4), 0.0)
102     def test5(self):
103         self.cmp(self.rand(3, 4), 1.0,
104                  self.rand(3, 5), self.rand(5, 4), 1.0)
105     def test6(self):
106         self.cmp(self.rand(3, 4), 1.0,
107                  self.rand(3, 5), self.rand(5, 4), -1.0)
108     def test7(self):
109         self.cmp(self.rand(3, 4), 0.0,
110                  self.rand(3, 5), self.rand(5, 4), 0.0)
111     def test8(self):
112         self.cmp(self.rand(3, 4), 0.0,
113                  self.rand(3, 5), self.rand(5, 4), 0.6)
114     def test9(self):
115         self.cmp(self.rand(3, 4), 0.0,
116                  self.rand(3, 5), self.rand(5, 4), -1.0)
117     def test10(self):
118         self.cmp(self.rand(3, 4), -1.0, self.rand(3, 5), self.rand(5, 4), 0.0)
119     def test11(self):
120         self.cmp(self.rand(3, 4), -1.0,
121                  self.rand(3, 5), self.rand(5, 4), 1.0)
122     def test12(self):
123         self.cmp(self.rand(3, 4), -1.0,
124                  self.rand(3, 5), self.rand(5, 4), -1.0)
125     def test_shape_0(self):
126         self.cmp(self.rand(0, 4), -1.0, self.rand(0, 5), self.rand(5, 4), -1.0)
127 <a name="24"></a>        self.cmp(self.rand(3, 0), -1.0, self.rand(3, 5), self.rand(5, 0), -1.0)
128         self.cmp(self.rand(3, 4), -1.0, self.rand(3, 0), self.rand(0, 4), -1.0)
129         self.cmp(self.rand(0, 0), -1.0, self.rand(0, 5), self.rand(5, 0), -1.0)
130         self.cmp(self.rand(0, 0), -1.0, self.rand(0, 0), self.rand(<font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 0), -1.0)
131     def test_factorised_scalar(self):
132         a = T.matrix()
133         b = T.matrix()
134         s = theano.shared(np.</b></font>zeros((5, 5)).astype(config.floatX))
135         lr1 = T.constant(0.01).astype(config.floatX)
136         lr2 = T.constant(2).astype(config.floatX)
137         l2_reg = T.constant(0.0001).astype(config.floatX)
138         f = theano.function([a, b], updates=[(s, lr1 * T.dot(a, b) +
139                             l2_reg * lr2 * s)],
140                             mode=mode_not_fast_compile).maker.fgraph.toposort()
141         assert len(f) == 1
142         assert f[0].op == gemm_inplace
143         f = theano.function([a, b], updates=[(s, lr1 * (T.dot(a, b) -
144                                                         l2_reg * s))],
145                             mode=mode_not_fast_compile).maker.fgraph.toposort()
146         assert len(f) == 1
147         assert f[0].op == gemm_inplace
148         f = theano.function([a, b],
149                             updates=[(s, s - lr1 * (s * .0002 + T.dot(a, b)))],
150                             mode=mode_not_fast_compile).maker.fgraph.toposort()
151         assert len(f) == 1
152         assert f[0].op == gemm_inplace
153     def test_destroy_map0(self):
154         Z = as_tensor_variable(self.rand(2, 2))
155         try:
156             gemm_inplace(Z, 1.0, Z, Z, 1.0)
157         except InconsistencyError as e:
158             if exc_message(e) == Gemm.E_z_uniq:
159                 return
160         self.fail()
161     def test_destroy_map1(self):
162         Z = as_tensor_variable(self.rand(2, 2))
163         A = as_tensor_variable(self.rand(2, 2))
164         try:
165             gemm_inplace(Z, 1.0, A, inplace.transpose_inplace(Z), 1.0)
166         except InconsistencyError as e:
167             if exc_message(e) == Gemm.E_z_uniq:
168                 return
169         self.fail()
170     def test_destroy_map2(self):
171         Z = as_tensor_variable(self.rand(2, 2))
172         A = as_tensor_variable(self.rand(2, 2))
173         try:
174             gemm_inplace(Z, 1.0, inplace.transpose_inplace(Z), A, 1.0)
175         except InconsistencyError as e:
176             if exc_message(e) == Gemm.E_z_uniq:
177                 return
178         self.fail()
179     def test_destroy_map3(self):
180         Z = as_tensor_variable(self.rand(2, 2))
181         A = as_tensor_variable(self.rand(2, 2))
182         try:
183             gemm_inplace(Z, 1.0, Z, A, 1.0)
184         except InconsistencyError as e:
185             if exc_message(e) == Gemm.E_z_uniq:
186                 return
187         self.fail()
188     def test_destroy_map4(self):
189         Z = shared(self.rand(2, 2), name='Z')
190         A = shared(self.rand(2, 2), name='A')
191         one = T.constant(1.0).astype(Z.dtype)
192         f = inplace_func([], gemm_inplace(Z, one, A, A, one))
193         f()
194         f = inplace_func([], gemm_inplace(Z, one, A, A.T, one))
195         f()
196     def test_transposes(self):
197         A = self.rand(4, 5)[:, :4]
198         B = self.rand(4, 5)[:, :4]
199         C = self.rand(4, 5)[:, :4]
200         def t(z, x, y, a=1.0, b=0.0, l='c|py', dt='float64'):
201             z, a, x, y, b = [theano._asarray(p, dtype=dt)
202                              for p in (z, a, x, y, b)]
203             z_after = self._gemm(z, a, x, y, b)
204             tz, ta, tx, ty, tb = [shared(p) for p in (z, a, x, y, b)]
205             f = inplace_func([], gemm_inplace(tz, ta, tx, ty, tb),
206                              mode=compile.Mode(optimizer=None, linker=l))
207             f()
208             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
209 <a name="10"></a>            f()
210             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
211             f()
212             unittest_tools<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assert_allclose(z_after, tz.get_value(borrow=True))
213             y_T = ty.get_value(borrow=True).T
214             ty.set_value(tx.get_value(borrow=</b></font>True).T, borrow=True)
215             tx.set_value(y_T, borrow=True)
216             f()
217             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True).T)
218         t(C, A, B)
219         t(C.T, A, B)
220         t(C, A.T, B, dt='float32')
221         t(C, A, B.T)
222         t(C.T, A.T, B)
223         t(C, A.T, B.T, dt='float32')
224         t(C.T, A, B.T)
225         t(C.T, A.T, B.T, dt='float32')
226         t(C, A[:, :2], B[:2, :])
227         t(C.T, A[:, :2], B[:2, :], dt='float32')
228         t(C, A[:2, :].T, B[:2, :])
229         t(C.T, A[:2, :].T, B[:2, :], dt='float32')
230         t(C, A[:2, :].T, B[:, :2].T)
231         t(C.T, A[:2, :].T, B[:, :2].T)
232         try:
233             t(C.T, A[:2, :], B[:, :2].T)
234         except ValueError as e:
235             if exc_message(e).find('aligned') &gt;= 0:
236                 return
237         self.fail()
238     def test_non_contiguous(self):
239         A = self.rand(4, 4, 3)
240         B = self.rand(4, 4, 3)
241         C = self.rand(4, 4, 3)
242         def t(z, x, y, a=1.0, b=0.0, l='c|py', dt='float64'):
243             z, a, x, y, b = [theano._asarray(p, dtype=dt)
244                              for p in (z, a, x, y, b)]
245             z_orig = z.copy()
246             z_after = np.zeros_like(z_orig)
247             for i in xrange(3):
248                 z_after[:, :, i] = self._gemm(z[:, :, i], a,
249                                               x[:, :, i], y[:, :, i], b)
250             tz, ta, tx, ty, tb = [shared(p) for p in (z, a, x, y, b)]
251             for i in xrange(3):
252                 f_i = inplace_func([],
253                                    gemm_inplace(tz[:, :, i],
254                                    ta, tx[:, :, i], ty[:, :, i], tb),
255                                    mode=compile.Mode(optimizer=None, linker=l))
256                 for j in xrange(3):
257                     z_i = f_i()
258                     z = tz.get_value(borrow=True, return_internal_type=True)
259                     z[:, :, i] = z_i
260                     unittest_tools.assert_allclose(z_after[:, :, i],
261                                                    tz.get_value(borrow=True)[:, :, i])
262                 tz_i = gemm_no_inplace(tz[:, :, i], ta, tx[
263                     :, :, i], ty[:, :, i], tb)
264                 g_i = theano.function(
265                     [], tz_i, updates=[(tz, T.set_subtensor(tz[:, :, i],
266                                                             tz_i))],
267                     mode=compile.Mode(optimizer=None, linker=l))
268                 for j in xrange(3):
269                     g_i()
270                     unittest_tools.assert_allclose(z_after[:, :, i],
271                                                    tz.get_value(borrow=True)[:, :, i])
272         t(C, A, B)
273         t(C.transpose((1, 0, 2)), A, B)
274         t(C, A.transpose((1, 0, 2)), B, dt='float32')
275         t(C, A, B.transpose((1, 0, 2)))
276         t(C.transpose((1, 0, 2)), A.transpose((1, 0, 2)), B)
277         t(C, A.transpose((1, 0, 2)), B.transpose((1, 0, 2)), dt='float32')
278         t(C.transpose((1, 0, 2)), A, B.transpose((1, 0, 2)))
279         t(C.transpose((1, 0, 2)), A.transpose((1, 0, 2)), B.transpose((
280             1, 0, 2)), dt='float32')
281 class TestGemmNoFlags(object):
282     gemm = gemm_no_inplace
283     M = 4
284     N = 5
285     K = 6
286     slice_step = 3
287     def setUp(self):
288         unittest_tools.seed_rng()
289     def get_variable(self, V, to_transpose, to_slice):
290         if to_transpose:
291             V = V.T
292         if to_slice:
293             V = V[::self.slice_step]
294         return V
295     def get_function(self, dtype,
296                      transpose_A=False, transpose_B=False, transpose_C=False,
297                      slice_A=False, slice_B=False, slice_C=False):
298         alpha = theano.tensor.scalar(dtype=dtype, name='alpha')
299         beta = theano.tensor.scalar(dtype=dtype, name='beta')
300         A = theano.tensor.matrix(dtype=dtype, name='A')
301         B = theano.tensor.matrix(dtype=dtype, name='B')
302         C = theano.tensor.matrix(dtype=dtype, name='C')
303         A1 = self.get_variable(A, transpose_A, slice_A)
304         B1 = self.get_variable(B, transpose_B, slice_B)
305         C1 = self.get_variable(C, transpose_C, slice_C)
306         return theano.function([alpha, A, B, beta, C], self.gemm(C1, alpha, A1, B1, beta))
307     def generate_value(self, dtype, width, height, to_transpose, to_slice):
308         if to_slice:
309             if to_transpose:
310                 shape = (height, width * self.slice_step)
311             else:
312                 shape = (width * self.slice_step, height)
313         else:
314             if to_transpose:
315                 shape = (height, width)
316             else:
317                 shape = (width, height)
318         return np.random.random(shape).astype(dtype)
319 <a name="8"></a>    def get_data(self, dtype, alpha, beta,
320                  transpose_A=False, transpose_B=False, transpose_C=False,
321                  slice_A=False, slice_B=False, slice_C=False):
322         A <font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= self.generate_value(dtype, self.M, self.N, transpose_A, slice_A)
323         B = self.generate_value(dtype, self.N, self.K, transpose_B, slice_B)
324         C = self.generate_value(dtype, self.M, self.</b></font>K, transpose_C, slice_C)
325         return (alpha, A, B, beta, C)
326     def get_value(self, V, to_transpose, to_slice):
327         if to_transpose:
328             V = V.T
329         if to_slice:
330             V = V[::self.slice_step]
331         return V
332     def compute_ref(self, alpha, A, B, beta, C,
333                     transpose_A, transpose_B, transpose_C,
334                     slice_A, slice_B, slice_C):
335         A = self.get_value(A, transpose_A, slice_A)
336         B = self.get_value(B, transpose_B, slice_B)
337         C = self.get_value(C, transpose_C, slice_C)
338         return alpha * np.dot(A, B) + beta * C
339     @theano.change_flags({'blas.ldflags': ''})
340     def run_gemm(self, dtype, ALPHA, BETA,
341                  transpose_A, transpose_B, transpose_C,
342                  slice_A, slice_B, slice_C):
343         f = self.get_function(dtype, transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)
344         values = self.get_data(dtype, ALPHA, BETA, transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)
345         assert any(isinstance(node.op, Gemm) for node in f.maker.fgraph.apply_nodes)
346         z_val = f(*values)
347         assert z_val.dtype == dtype
348         assert tuple(z_val.shape) == (self.M, self.K)
349         ref_val = self.compute_ref(*(values + (transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)))
350         unittest_tools.assert_allclose(ref_val, z_val)
351     def test_gemm(self):
352         dtypes = ('float32', 'float64')
353         scalars = (0, 1, -2)
354         booleans = (False, True)
355         iterables = [dtypes] + ([scalars] * 2) + ([booleans] * 6)
356         for dtype, alpha, beta, tA, tB, tC, sA, sB, sC in product(*iterables):
357             yield (self.run_gemm, dtype, alpha, beta, tA, tB, tC, sA, sB, sC)
358 def test_res_is_a():
359     X, Y, Z, a, b = XYZab()
360     assert not res_is_a(a, T.sqrt)
361     assert not res_is_a(a + a, T.sqrt)
362     assert res_is_a(T.sqrt(a + a), T.sqrt)
363 class t_as_scalar(TestCase):
364     def test0(self):
365         a = T.constant(2.5)
366         b = T.constant(np.asarray([[[0.5]]]))
367         b2 = b.dimshuffle()
368         assert b2.ndim == 0
369         d_a = T.DimShuffle([], [])(a)
370         d_b = T.DimShuffle([True, True, True], [0, 2, 1])(b)
371         d_a2 = T.DimShuffle([], ['x', 'x', 'x'])(a)
372         self.assertTrue(_as_scalar(a) == a)
373         self.assertTrue(_as_scalar(b) != b)
374         self.assertTrue(_as_scalar(d_a) != d_a)
375         self.assertTrue(_as_scalar(d_b) != d_b)
376         self.assertTrue(_as_scalar(d_a2) != d_a2)
377     def test1(self):
378         a = T.constant(np.ones(5))
379         self.assertTrue(_as_scalar(a) is None)
380         self.assertTrue(_as_scalar(T.DimShuffle([False], [0, 'x'])(a)) is None)
381     def test2(self):
382         a = T.dscalar()
383         d_a = T.DimShuffle([], [])(a)
384         d_a2 = T.DimShuffle([], ['x', 'x'])(a)
385         self.assertTrue(_as_scalar(a) is a)
386         self.assertTrue(_as_scalar(d_a) is a)
387         self.assertTrue(_as_scalar(d_a2) is a)
388     def test3(self):
389         a = T.matrix()
390         self.assertTrue(_as_scalar(a) is None)
391         self.assertTrue(_as_scalar(T.DimShuffle([False, False],
392                                                 [0, 'x', 1])(a)) is None)
393 class T_real_matrix(TestCase):
394     def test0(self):
395         self.assertTrue(_is_real_matrix(T.DimShuffle([False, False],
396                                                      [1, 0])(T.matrix())))
397         self.assertTrue(not _is_real_matrix(T.DimShuffle([False],
398                                                          ['x', 0])
399                                             (T.dvector())))
400 def fail(msg):
401     print('FAIL', msg)
402     assert False
403 def XYZab():
404     return T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
405 class Failure(Exception):
406     pass
407 def just_gemm(i, o, ishapes=[(4, 3), (3, 5), (4, 5), (), ()],
408               max_graphlen=0, expected_nb_gemm=1):
409     try:
410         f = inplace_func(
411             [In(ii, mutable=True, allow_downcast=True) for ii in i],
412             o,
413             mode='FAST_RUN',
414             on_unused_input='ignore')
415         nb_gemm = 0
416         for node in f.maker.fgraph.apply_nodes:
417             if isinstance(node.op, T.Dot):
418                 raise Failure('dot not changed to gemm_inplace in graph')
419             if node.op == _dot22:
420                 raise Failure('_dot22 not changed to gemm_inplace in graph')
421             if node.op == gemm_inplace:
422                 nb_gemm += 1
423         assert nb_gemm == expected_nb_gemm, (nb_gemm, expected_nb_gemm)
424         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
425                          allow_input_downcast=True, on_unused_input='ignore')
426         for node in g.maker.fgraph.apply_nodes:
427             if node.op == gemm_inplace:
428                 raise Exception('gemm_inplace in original graph')
429         graphlen = len(f.maker.fgraph.toposort())
430         if max_graphlen and (graphlen &lt;= max_graphlen):
431 <a name="12"></a>            assert False, 'graphlen=%i&gt;%i' % (graphlen, max_graphlen)
432         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
433         r0 = f<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(*[np.asarray(rng.randn(*sh), config.floatX)
434                  for sh in ishapes])
435         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
436         r1 =</b></font> g(*[np.asarray(rng.randn(*sh), config.floatX)
437                  for sh in ishapes])
438         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
439         eps = 1.0e-8
440         if config.floatX == 'float32':
441             eps = 1.0e-6
442         if max_abs_err &gt; eps:
443             raise Failure('GEMM is computing the wrong output. max_rel_err =',
444                           max_abs_err)
445     except Failure:
446         for node in f.maker.fgraph.toposort():
447             print('GRAPH', node)
448         raise
449 @unittest_tools.assertFailure_fast
450 def test_gemm_opt0():
451     X, Y, Z, a, b = XYZab()
452     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a + Z * b])
453     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) + b * Z])
454     just_gemm([X, Y, Z, a, b], [b * Z + a * T.dot(X, Y)])
455     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a - Z * b])
456     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) - b * Z])
457     just_gemm([X, Y, Z, a, b], [b * Z - a * T.dot(X, Y)])
458     just_gemm([X, Y, Z, a, b], [b * Z.T - a * T.dot(Y.T, X.T)])
459     just_gemm([X, Y, Z, a, b], [b * Z.T + a * b * T.dot(X, Y).T])
460     just_gemm([X, Y, Z, a, b], [b * Z + a * T.dot(X, Y).T],
461               ishapes=[(5, 3), (3, 4), (4, 5), (), ()])
462     just_gemm([X, Y, Z, a, b], [(b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
463     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y)])
464     just_gemm([X, Y, Z, a, b], [Z * b + T.dot(X, Y)])
465     just_gemm([X, Y, Z, a, b], [Z + a * b * a * T.dot(X, Y)])
466     just_gemm([X, Y, Z, a, b], [(b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
467     just_gemm([X, Y, Z, a, b], [Z - T.dot(X, Y)])
468     just_gemm([X, Y, Z, a, b], [Z * b - T.dot(X, Y)])
469     just_gemm([X, Y, Z, a, b], [Z - a * b * a * T.dot(X, Y)])
470 @unittest_tools.assertFailure_fast
471 def test_gemm_opt_double_gemm():
472     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
473     R, S, c = T.matrix(), T.matrix(), T.scalar()
474     just_gemm([X, Y, Z, a, b, R, S, c],
475               [Z * c + a * T.dot(X, Y) + b * T.dot(R, S).T],
476               ishapes=[(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()],
477               expected_nb_gemm=2)
478     ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()]
479     i = [X, Y, Z, a, b, R, S, c]
480     o = [(a * T.dot(X, Y) +
481          gemm_inplace(Z, b, S.T, R.T, T.constant(1.0).astype(config.floatX)))]
482     try:
483         f = inplace_func([In(ii, mutable=True) for ii in i], o,
484                          mode='FAST_RUN', on_unused_input='ignore')
485         for node in f.maker.fgraph.apply_nodes:
486             if isinstance(node.op, T.Dot):
487                 raise Failure('dot in graph')
488             if node.op == _dot22:
489                 raise Failure('_dot22 in graph')
490         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
491                          on_unused_input='ignore')
492         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
493         r0 = f(*[np.asarray(rng.randn(*sh), config.floatX)
494                  for sh in ishapes])
495         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
496         r1 = g(*[np.asarray(rng.randn(*sh), config.floatX)
497                  for sh in ishapes])
498         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
499         eps = 1.0e-8
500         if config.floatX == 'float32':
501             eps = 1.0e-6
502         if max_abs_err &gt; eps:
503             raise Failure(
504                 'GEMM is computing the wrong output. max_rel_err =',
505                 max_abs_err)
506     except Failure:
507         for node in f.maker.fgraph.toposort():
508             print('GRAPH', node)
509         raise
510 <a name="18"></a>def test_gemm_canonicalize():
511     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
512         'a'), T.scalar('b')
513     c, d = T<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.scalar('c'), T.scalar('d')
514     u = T.row('u')
515     v = T.vector('v')
516     w = T.</b></font>col('w')
517     can = []
518     _gemm_canonicalize(X + Y + Z, 1.0, can, 0)
519     assert can == [(1.0, X), (1.0, Y), (1.0, Z)]
520     can = []
521     _gemm_canonicalize(X + Y + u, 1.0, can, 0)
522     assert can == [(1.0, X), (1.0, Y), (1.0, u)], can
523     can = []
524     _gemm_canonicalize(X + Y + v, 1.0, can, 0)
525     assert can[:2] == [(1.0, X), (1.0, Y)]
526     assert isinstance(can[2], tuple)
527     assert len(can[2]) == 2
528     assert can[2][0] == 1.0
529     assert can[2][1].owner
530     assert isinstance(can[2][1].owner.op, T.DimShuffle)
531     assert can[2][1].owner.inputs == [v]
532     can = []
533     _gemm_canonicalize(X + Y + w, 1.0, can, 0)
534     assert can == [(1.0, X), (1.0, Y), (1.0, w)], can
535     can = []
536     _gemm_canonicalize(a * X + Y - b * Z * c, 1.0, can, 0)
537     assert can[0] == (a, X)
538     assert can[1] == (1.0, Y)
539     assert can[2][0].owner.op == T.mul
540     assert can[2][0].owner.inputs[0].owner.op == T.neg
541     assert can[2][0].owner.inputs[0].owner.inputs[0] == c
542     assert can[2][0].owner.inputs[1] == b
543     can = []
544     _gemm_canonicalize((-d) * X - (a * X + Y - b * Z * c), 1.0, can, 0)
545     assert can[0][0].owner.op == T.neg
546     assert can[0][0].owner.inputs[0] == d
547     assert can[0][1] == X
548     assert can[1][0].owner.op == T.neg
549     assert can[1][0].owner.inputs[0] == a
550     assert can[2] == (-1.0, Y)
551     assert can[3][0].owner.op == T.mul
552     assert can[3][0].owner.inputs == [c, b]
553 def test_gemm_factor():
554 <a name="6"></a>    X, Y = T.matrix('X'), T.matrix('Y')
555     assert [(1.0, X), (1.0, Y)] == _factor_canonicalized([(1.0, X), (1.0, Y)])
556     assert [(2.0, X)] == _factor_canonicalized([(1.0, X), (1<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.0, X)])
557 def test_upcasting_scalar_nogemm():
558     v = T.fmatrix('v')
559     w = T.fmatrix('w')
560     t = T.fmatrix('t')
561     alpha = T.dscalar('a')
562     rval = T.dot(</b></font>w, v) * alpha + t
563 <a name="14"></a>
564     f = theano.function([w, v, t, alpha], rval)
565     t = f.maker.fgraph.toposort()
566     assert np<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.sum([isinstance(n.op, Gemm) for n in t]) == 0
567     v = T.fmatrix('v')
568     w = T.fmatrix('w')
569     t = T.</b></font>fmatrix('t')
570     alpha = T.cscalar('a')
571     on_opt_error = config.on_opt_error
572     try:
573         config.on_opt_error = 'raise'
574         rval = T.dot(w, v) * alpha + t
575         f = theano.function([w, v, t, alpha], rval)
576     finally:
577         config.on_opt_error = on_opt_error
578     t = f.maker.fgraph.toposort()
579     assert np.sum([isinstance(n.op, Gemm) for n in t]) == 0
580 def test_gemm_nested():
581     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
582         'a'), T.scalar('b')
583     R, S, U, c, d = T.matrix('R'), T.matrix('S'), T.matrix('U'), T.scalar(
584         'c'), T.scalar('d')
585     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
586               [a * Z - b * (c * T.dot(X, Y) + d * Z)],
587               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
588                        (2, 4), (), (), (), ()],
589               max_graphlen=1)
590     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
591               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * Z)],
592               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
593                        (2, 4), (), (), (), ()],
594               max_graphlen=1)
595     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
596               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * U)],
597               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
598                        (2, 4), (), (), (), ()],
599               max_graphlen=3)
600 def test_gemm_opt_wishlist():
601     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
602     just_gemm([X, Y, Z, a, b],
603               [(b * b) * Z * a + (a * a) * T.dot(X, Y) + b * T.dot(X, Y)])
604     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y) + T.dot(X, Y)])
605 def test_gemm_with_vector():
606     X, Y, Z, a, b = XYZab()
607     v = T.vector()
608     def my_just_gemm(o):
609         i = [X, Y, Z, a, b, v]
610         ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, )]
611         just_gemm(i, o, ishapes=ishapes)
612     my_just_gemm([v + T.dot(X, Y) * a + Z * b])
613     my_just_gemm([v + a * T.dot(X, Y) + b * Z])
614     my_just_gemm([v + b * Z + a * T.dot(X, Y)])
615     my_just_gemm([v + T.dot(X, Y) * a - Z * b])
616     my_just_gemm([v + a * T.dot(X, Y) - b * Z])
617     my_just_gemm([v + b * Z - a * T.dot(X, Y)])
618     my_just_gemm([v + (b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
619     my_just_gemm([v + Z + T.dot(X, Y)])
620     my_just_gemm([v + Z * b + T.dot(X, Y)])
621     my_just_gemm([v + Z + a * b * a * T.dot(X, Y)])
622     my_just_gemm([v + (b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
623     my_just_gemm([Z - T.dot(X, Y) + v])
624     my_just_gemm([Z * b - T.dot(X, Y) + v])
625     my_just_gemm([Z - a * b * a * T.dot(X, Y) + v])
626 def test_gemm_opt_vector_stuff():
627     X, Y, a = T.matrix(), T.matrix(), T.scalar()
628     u, v = T.vector(), T.vector()
629     f = inplace_func([a, u, v], a + T.dot(u, v), mode='FAST_RUN')
630     if gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]:
631         raise Failure('gemm_inplace in graph')
632     f = inplace_func([a, u, X, Y], a * u + T.dot(X, Y), mode='FAST_RUN')
633     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
634         raise Failure('gemm_inplace in graph')
635 def test_gemm_unrolled():
636     batch_size = 100
637     rep_size = 40
638     rng = np.random.RandomState([1, 2, 3])
639     for num_rounds in range(1, 10):
640         W = sharedX(rng.randn(rep_size, rep_size), name='W')
641         V = sharedX(np.zeros((batch_size, rep_size)), name='V')
642         H = sharedX(np.zeros((batch_size, rep_size)), name='H')
643         G = sharedX(np.zeros((batch_size, rep_size)), name='G')
644         cur_V = V
645         cur_H = H
646         def update_V(cur_H):
647             return T.nnet.sigmoid(T.dot(cur_H, W.T))
648         def update_H(cur_V):
649             return T.nnet.sigmoid(T.dot(cur_V, W) + T.dot(G, W.T))
650         for i in xrange(num_rounds):
651             cur_V = update_V(cur_H)
652             cur_H = update_H(cur_V)
653         unrolled_theano = theano.function([], updates=[(V, cur_V), (H, cur_H)],
654                                           name='unrolled_theano')
655         nb_dot = sum([1 for node in unrolled_theano.maker.fgraph.toposort()
656                       if isinstance(node.op, (theano.tensor.Dot,
657                                               theano.tensor.blas.Dot22,
658                                               theano.tensor.blas.Gemm))])
659         assert nb_dot == num_rounds * 2 + 1, nb_dot
660         unrolled_theano()
661 def test_inplace0():
662     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
663         'a'), T.scalar('b')
664     R, S, c = T.matrix('R'), T.matrix('S'), T.scalar('c')
665     f = inplace_func([Z, b, R, S],
666                      [Z * (Z + b * T.dot(R, S).T)], mode='FAST_RUN')
667     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
668         print(pp(f.maker.fgraph.outputs[0]))
669         raise Failure('gemm_inplace in graph')
670     assert gemm_no_inplace in [n.op for n in f.maker.fgraph.apply_nodes]
671     f = inplace_func([X, Y, Z, a, b, R, S, c],
672                      [Z * (c * Z + a * T.dot(X, Y) + b * T.dot(R, S).T)],
673                      mode='FAST_RUN')
674     if (gemm_inplace not in [n.op for n in f.maker.fgraph.apply_nodes]):
675         theano.printing.debugprint(f)
676         raise Failure('no gemm_inplace in graph')
677 def test_inplace1():
678     X, Y, Z, a, b = XYZab()
679     f = inplace_func([X, Y, Z],
680                      [Z + Z + T.dot(X, Y)], mode='FAST_RUN')
681     assert [n.op for n in f.maker.fgraph.apply_nodes] == [gemm_no_inplace]
682 def test_dot22():
683     for dtype1 in ['float32', 'float64', 'complex64', 'complex128']:
684         a = T.matrix(dtype=dtype1)
685         for dtype2 in ['float32', 'float64', 'complex64', 'complex128']:
686             b = T.matrix(dtype=dtype2)
687             f = theano.function([a, b], T.dot(a, b), mode=mode_blas_opt)
688             topo = f.maker.fgraph.toposort()
689             if dtype1 == dtype2:
690                 assert _dot22 in [x.op for x in topo], (dtype1, dtype2)
691             else:
692                 check = [isinstance(x.op, T.Dot) for x in topo]
693                 assert any(check), (dtype1, dtype2)
694             rng = np.random.RandomState(unittest_tools.fetch_seed())
695             def cmp(a_shp, b_shp):
696                 av = rng.uniform(size=a_shp).astype(dtype1)
697                 bv = rng.uniform(size=b_shp).astype(dtype2)
698                 f(av, bv)
699             cmp((3, 4), (4, 5))
700             cmp((0, 4), (4, 5))
701             cmp((3, 0), (0, 5))
702             cmp((3, 4), (4, 0))
703             cmp((0, 4), (4, 0))
704             cmp((0, 0), (0, 0))
705 @attr('slow')
706 def test_dot22scalar():
707     rng = np.random.RandomState(unittest_tools.fetch_seed())
708     for dtype1 in ['complex64', 'complex128']:
709         a = T.matrix('a', dtype=dtype1)
710         for dtype2 in ['complex64', 'complex128']:
711             b = T.matrix('b', dtype=dtype2)
712             for dtype3 in ['complex64', 'complex128']:
713                 c = T.matrix('c', dtype=dtype3)
714                 for dtype4 in ['complex64', 'complex128']:
715                     cst = theano.tensor.basic.constant(.2, dtype=dtype4)
716                     cst2 = theano.tensor.basic.constant(.1, dtype=dtype4)
717                     def check_dot22scalar(func, len_topo_scalar=-1):
718                         topo = func.maker.fgraph.toposort()
719                         ops = [x.op for x in topo]
720                         dtype4_upcast = theano.scalar.upcast(dtype4, dtype1,
721                                                              dtype2)
722                         if dtype1 == dtype2 == dtype3 == dtype4_upcast:
723                             if len_topo_scalar &gt; 0:
724                                 assert len(topo) == len_topo_scalar
725                             assert _dot22scalar in ops, (dtype1, dtype2,
726                                                          dtype3, dtype4)
727                         elif dtype1 == dtype2 == dtype4_upcast:
728                             if not (len_topo_scalar &gt; 0):
729                                 assert len(topo) == len_topo_scalar
730                                 assert _dot22scalar in ops, (dtype1, dtype2,
731                                                              dtype3, dtype4)
732                             else:
733                                 assert _dot22scalar in ops or _dot22 in ops, (
734                                     dtype1, dtype2, dtype3, dtype4)
735                         elif dtype1 == dtype2:
736                             assert _dot22 in ops, (dtype1, dtype2,
737                                                    dtype3, dtype4)
738                         else:
739                             check = [isinstance(o, T.Dot) for o in ops]
740                             assert any(check), (dtype1, dtype2, dtype3, dtype4)
741                     def cmp(a_shp, b_shp, c_shp, sqr_shp=(5, 5)):
742                         av = rng.uniform(size=a_shp).astype(dtype1)
743                         bv = rng.uniform(size=b_shp).astype(dtype2)
744                         cv = rng.uniform(size=c_shp).astype(dtype3)
745                         sv = rng.uniform(size=sqr_shp).astype(dtype1)
746                         if False:
747                             f = theano.function([a, b], cst * T.dot(a, b),
748                                                 mode=mode_blas_opt)
749                             f.maker.fgraph.toposort()
750                             check_dot22scalar(f, 1)
751                             f(av, bv)
752                         if True:
753                             f = theano.function([a, b, c],
754                                                 cst * c * T.dot(a, b),
755                                                 mode=mode_blas_opt)
756                             f.maker.fgraph.toposort()
757                             check_dot22scalar(f, 2)
758                             f(av, bv, cv)
759 <a name="22"></a>                        f = theano.function([a, b, c],
760                                             c * cst * T.dot(a, b),
761                                             mode=mode_blas_opt)
762                         f.maker<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.fgraph.toposort()
763                         check_dot22scalar(f, 2)
764                         f(av, bv, cv)
765                         m2 = mode_blas_opt.including('canonicalize')
766                         f = theano.function([a</b></font>, b, c],
767                                             cst2 * c * cst * T.dot(a, b),
768                                             mode=m2)
769                         f.maker.fgraph.toposort()
770                         check_dot22scalar(f, 2)
771                         f(av, bv, cv)
772                         if dtype1 == dtype2 == dtype3:
773                             f = theano.function([a, b, c],
774                                                 c * cst * a * T.dot(a, b),
775                                                 mode=m2)
776                             f.maker.fgraph.toposort()
777                             check_dot22scalar(f, 2)
778                             f(sv, sv, sv)
779                             f = theano.function([a, b, c],
780                                                 cst * c * a * T.dot(a, b),
781                                                 mode=mode_blas_opt)
782                             f.maker.fgraph.toposort()
783                             f(sv, sv, sv)
784                             f = theano.function([a, b, c],
785                                                 c * a * cst * T.dot(a, b),
786                                                 mode=m2)
787                             f.maker.fgraph.toposort()
788                             check_dot22scalar(f, 2)
789                             f(sv, sv, sv)
790                     cmp((3, 4), (4, 5), (3, 5))
791                     cmp((0, 4), (4, 5), (0, 5))
792                     cmp((3, 0), (0, 5), (3, 5))
793                     cmp((3, 4), (4, 0), (3, 0), (0, 0))
794                     cmp((0, 4), (4, 0), (0, 0))
795                     cmp((0, 0), (0, 0), (0, 0))
796 def test_dot22scalar_cast():
797     A = T.dmatrix()
798     for scalar_int_type in T.int_dtypes:
799         y = T.scalar(dtype=scalar_int_type)
800         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
801         assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
802     A = T.fmatrix()
803     for scalar_int_type in T.int_dtypes:
804         y = T.scalar(dtype=scalar_int_type)
805         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
806         if scalar_int_type in ['int32', 'int64']:
807             assert _dot22 in [x.op for x in f.maker.fgraph.toposort()]
808         else:
809             assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
810 def test_local_dot22_to_dot22scalar():
811     A = T.dmatrix()
812 <a name="20"></a>    mode = theano.compile.mode.get_default_mode()
813     opt = theano.tensor.opt.in2out(
814         theano.tensor.blas.local_dot22_to_dot22scalar)
815     mode = mode.__class__(optimizer<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=opt)
816     x = T.dscalar()
817     y = T.dscalar()
818     z = T.dscalar()
819     m = T.</b></font>dmatrix()
820     r = T.drow()
821     for idx, node in enumerate([
822         T.mul(_dot22(A, A), x),
823         T.mul(_dot22(A, A), x, y),
824         T.mul(_dot22(A, A), x, r),
825         T.mul(_dot22(A, A), m, x),
826         T.mul(_dot22(A, A), x, m),
827         T.mul(_dot22(A, A), x, (m * y)),
828         T.mul(_dot22(A, A), (m * y), x),
829         T.mul(_dot22(A, A), x, (r * y)),
830         T.mul(_dot22(A, A), (r * y), x),
831         T.mul(_dot22(A, A), (x * y), (m * x)),
832         T.mul(_dot22(A, A), (r * y), (y * x)),
833         T.mul(_dot22(A, A), (m * y), m),
834         T.mul(_dot22(A, A), m, (m * y)),
835         T.mul(_dot22(A, A), (r * y), (m * x)),
836         T.mul(_dot22(A, A), (m * y * z), m),
837         T.mul(_dot22(A, A), m, (m * y * z)),
838         T.mul(_dot22(A, A), T.mul(m, y, z), m),
839         T.mul(_dot22(A, A), m, T.mul(m, y, z)),
840         T.mul(_dot22(A, A), (r * m), (m * x)),
841     ]):
842         node2 = theano.tensor.blas.local_dot22_to_dot22scalar.transform(
843             node.owner)
844         assert node2
845         f = theano.function([x, y, z, m, r, A], node,
846                             mode=mode, on_unused_input='ignore')
847         f(.1, .2, .3, [[1, 2], [3, 4]], [[5, 6]], [[7, 8], [9, 10]])
848 def test_dot_w_self():
849     A = shared(value=np.ones((2, 2)))
850     B = T.matrix()
851     p = T.dot(A, A) * B
852     grad = T.grad(T.mean(p), A)
853     f = theano.function([B], p, updates=[(A, A - grad)])
854     f(np.asarray([[0, 1], [2, 3]], dtype=config.floatX))
855 class TestGemv(TestCase, unittest_tools.TestOptimizationMixin):
856     def test_dot_vv(self):
857         rng = np.random.RandomState(unittest_tools.fetch_seed())
858         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
859         w = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
860         f = theano.function([], theano.dot(v, w), mode=mode_blas_opt)
861         self.assertFunctionContains0(f, T.dot)
862 <a name="19"></a>        self.assertFunctionContains1(f, Gemv(True))
863         assert np.allclose(f(), np.dot(v.get_value(), w.get_value(<font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>)))
864     def test_dot_vm(self):
865         rng = np.random.RandomState(unittest_tools.fetch_seed())
866         v = theano.shared(np.</b></font>array(rng.uniform(size=(2,)), dtype='float32'))
867         m = theano.shared(np.array(rng.uniform(size=(2, 3)), dtype='float32'))
868         f = theano.function([], theano.dot(v, m), mode=mode_blas_opt)
869         self.assertFunctionContains0(f, T.dot)
870         self.assertFunctionContains1(f, Gemv(True))
871         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
872         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
873         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
874     def test_dot_mv(self):
875         rng = np.random.RandomState(unittest_tools.fetch_seed())
876         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
877         m = theano.shared(np.array(rng.uniform(size=(3, 2)), dtype='float32'))
878         f = theano.function([], theano.dot(m, v), mode=mode_blas_opt)
879         self.assertFunctionContains0(f, T.dot)
880         self.assertFunctionContains1(f, Gemv(True))
881         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
882         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
883         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
884     @staticmethod
885     def t_gemv1(m_shp):
886         rng = np.random.RandomState(unittest_tools.fetch_seed())
887         v1 = theano.shared(np.array(rng.uniform(size=(m_shp[1],)),
888                            dtype='float32'))
889         v2_orig = np.array(rng.uniform(size=(m_shp[0],)), dtype='float32')
890         v2 = theano.shared(v2_orig)
891         m = theano.shared(np.array(rng.uniform(size=m_shp), dtype='float32'))
892         f = theano.function([], v2 + theano.dot(m, v1), mode=mode_blas_opt)
893 <a name="9"></a>
894         assert np.allclose(f(), np.dot(m.get_value(), v1.get_value()) + v2_orig)
895         topo <font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= f.maker.fgraph.toposort()
896         assert len(topo) == 1
897         assert isinstance(topo[0].op, Gemv)
898         assert topo[0].op.</b></font>inplace is False
899         g = theano.function([], [], updates=[(v2, v2 + theano.dot(m, v1))],
900                             mode=mode_blas_opt)
901         g()
902         assert np.allclose(v2.get_value(), np.dot(m.get_value(),
903                            v1.get_value()) + v2_orig)
904         topo = g.maker.fgraph.toposort()
905         assert len(topo) == 1
906         assert isinstance(topo[0].op, Gemv)
907         if config.mode != 'FAST_COMPILE':
908             assert topo[0].op.inplace is True
909         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
910                     borrow=True)
911         v2.set_value(v2_orig)
912         assert np.allclose(f(),
913                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
914         g()
915         assert np.allclose(v2.get_value(),
916                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
917     @attr('slow')
918     def test_gemv1(self):
919         self.t_gemv1((3, 2))
920         self.t_gemv1((0, 2))
921         self.t_gemv1((3, 0))
922         self.t_gemv1((0, 0))
923     def test_gemv2(self):
924         rng = np.random.RandomState(unittest_tools.fetch_seed())
925         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
926                            dtype='float32'))
927         v2_orig = np.array(rng.uniform(size=(3,)), dtype='float32')
928         v2 = theano.shared(v2_orig)
929         m = theano.shared(np.array(rng.uniform(size=(2, 3)),
930                           dtype='float32'))
931         f = theano.function([], v2 + theano.dot(v1, m), mode=mode_blas_opt)
932         assert np.allclose(f(),
933                            np.dot(v1.get_value(), m.get_value()) +
934                            v2.get_value())
935         topo = f.maker.fgraph.toposort()
936         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
937         assert topo[-1].op.inplace is False
938         g = theano.function([], [], updates=[(v2, v2 + theano.dot(v1, m))],
939                             mode=mode_blas_opt)
940         g()
941         assert np.allclose(v2.get_value(),
942                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
943         topo = g.maker.fgraph.toposort()
944         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
945         if config.mode != 'FAST_COMPILE':
946             assert topo[-1].op.inplace is True
947         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
948                     borrow=True)
949         v2.set_value(v2_orig)
950         assert np.allclose(f(),
951                            np.dot(v1.get_value(), m.get_value()) +
952                            v2.get_value())
953         g()
954         assert np.allclose(v2.get_value(),
955                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
956     def test_gemv_broadcast(self):
957         rng = np.random.RandomState(unittest_tools.fetch_seed())
958         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
959                                     dtype='float32'))
960         v2_orig = np.array(rng.uniform(size=(1,)), dtype='float32')
961         v2 = theano.shared(v2_orig)
962         m = theano.shared(np.array(rng.uniform(size=(1, 2)),
963                                    dtype='float32'),
964                           broadcastable=(True, False))
965         o = theano.dot(m, v1)
966         f = theano.function([], o + v2, mode=mode_blas_opt)
967         assert np.allclose(
968             f(),
969             np.dot(m.get_value(), v1.get_value()) + v2.get_value())
970         topo = f.maker.fgraph.toposort()
971         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
972         o = theano.tensor.blas.gemv_no_inplace(v2, 0.5, m, v1, 0.25)
973         f = theano.function([], o, mode=mode_blas_opt)
974         assert np.allclose(
975             f(),
976             0.5 * np.dot(m.get_value(), v1.get_value()) + 0.25 * v2.get_value())
977         topo = f.maker.fgraph.toposort()
978         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
979     def test_gemv_dimensions(self):
980         A = T.matrix('A')
981         x, y = T.vectors('x', 'y')
982         alpha = theano.shared(theano._asarray(1.0, dtype=config.floatX),
983                               name='alpha')
984         beta = theano.shared(theano._asarray(1.0, dtype=config.floatX),
985                              name='beta')
986         z = beta * y + alpha * T.dot(A, x)
987         f = theano.function([A, x, y], z)
988         A_val = np.ones((5, 3), dtype=config.floatX)
989         ones_3 = np.ones(3, dtype=config.floatX)
990         ones_4 = np.ones(4, dtype=config.floatX)
991         ones_5 = np.ones(5, dtype=config.floatX)
992         ones_6 = np.ones(6, dtype=config.floatX)
993         f(A_val, ones_3, ones_5)
994         f(A_val[::-1, ::-1], ones_3, ones_5)
995         self.assertRaises(ValueError, f, A_val, ones_4, ones_5)
996         self.assertRaises(ValueError, f, A_val, ones_3, ones_6)
997         self.assertRaises(ValueError, f, A_val, ones_4, ones_6)
998 def matrixmultiply(a, b):
999     if len(b.shape) == 1:
1000         b_is_vector = True
1001         b = b[:, newaxis]
1002     else:
1003         b_is_vector = False
1004     assert a.shape[1] == b.shape[0]
1005     c = zeros((a.shape[0], b.shape[1]), common_type(a, b))
1006     for i in xrange(a.shape[0]):
1007         for j in xrange(b.shape[1]):
1008             s = 0
1009             for k in xrange(a.shape[1]):
1010                 s += a[i, k] * b[k, j]
1011             c[i, j] = s
1012     if b_is_vector:
1013         c = c.reshape((a.shape[0],))
1014     return c
1015 class BaseGemv(object):
1016     mode = mode_blas_opt  # can be overridden with self.mode
1017     shared = staticmethod(theano.shared)
1018     def get_data(self, x_stride=1, y_stride=1):
1019         rng = np.random.RandomState(unittest_tools.fetch_seed())
1020         mult = array(1, dtype=self.dtype)
1021         if self.dtype in [complex64, complex128]:
1022             mult = array(1 + 1j, dtype=self.dtype)
1023         alpha = array(1., dtype=self.dtype) * mult
1024         beta = array(1., dtype=self.dtype) * mult
1025         a = rng.randn(3, 3).astype(self.dtype) * mult
1026         x = arange(shape(a)[0] * x_stride, dtype=self.dtype) * mult
1027         y = arange(shape(a)[1] * y_stride, dtype=self.dtype) * mult
1028         return alpha, beta, a, x, y
1029 <a name="13"></a>    def test_simple(self):
1030         alpha, beta, a, x, y = [self.shared(value)
1031                                 for value in self.get_data()]
1032         desired_oy = alpha.get_value() * matrixmultiply(a.get_value(), x.get_value()) + beta<font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get_value() * y.get_value()
1033         oy = alpha * T.dot(a, x) + beta * y
1034         oy_func = theano.function([], oy, mode=self.mode)
1035         oy_func.maker.</b></font>fgraph.toposort()
1036         self.assertFunctionContains1(oy_func, self.gemv)
1037         oy_val = oy_func()
1038         assert_array_almost_equal(desired_oy, oy_val)
1039     def test_default_beta_y(self):
1040         vs = self.get_data()
1041         alpha_v, beta_v, a_v, x_v, y_v = vs
1042         a = self.shared(a_v)
1043         x = self.shared(x_v)
1044         desired_oy = matrixmultiply(a_v, x_v)
1045         oy = T.dot(a, x)
1046         oy_func = theano.function([], oy, mode=self.mode)
1047         self.assertFunctionContains1(oy_func, self.gemv_inplace)
1048         oy_v = oy_func()
1049         assert_array_almost_equal(desired_oy, oy_v)
1050     def test_simple_transpose(self):
1051         vs = self.get_data()
1052         alpha_v, beta_v, a_v, x_v, y_v = vs
1053         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1054         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1055                                               x_v) + beta_v * y_v
1056         oy = alpha * T.dot(a.T, x) + beta * y
1057         oy_func = theano.function([], oy, mode=self.mode)
1058         self.assertFunctionContains1(oy_func, self.gemv)
1059         oy_v = oy_func()
1060         assert_array_almost_equal(desired_oy, oy_v)
1061     def test_x_stride(self):
1062         vs = self.get_data(x_stride=2)
1063         alpha_v, beta_v, a_v, x_v, y_v = vs
1064         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1065         desired_oy = alpha_v * matrixmultiply(a_v, x_v[::2]) + beta_v * y_v
1066         oy = alpha * T.dot(a, x[::2]) + beta * y
1067         oy_func = theano.function([], oy, mode=self.mode)
1068         self.assertFunctionContains1(oy_func, self.gemv)
1069         oy_v = oy_func()
1070         assert_array_almost_equal(desired_oy, oy_v)
1071     def test_x_stride_transpose(self):
1072         vs = self.get_data(x_stride=2)
1073         alpha_v, beta_v, a_v, x_v, y_v = vs
1074         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1075         desired_oy = alpha_v * matrixmultiply(transpose(a_v), x_v[::2]) + \
1076             beta_v * y_v
1077         oy = alpha * T.dot(a.T, x[::2]) + beta * y
1078         oy_func = theano.function([], oy, mode=self.mode)
1079         self.assertFunctionContains1(oy_func, self.gemv)
1080         oy_v = oy_func()
1081         assert_array_almost_equal(desired_oy, oy_v)
1082     def test_y_stride(self):
1083         vs = self.get_data(y_stride=2)
1084         alpha_v, beta_v, a_v, x_v, y_v = vs
1085         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1086         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v[::2]
1087         oy = alpha * T.dot(a, x) + beta * y[::2]
1088         oy_func = theano.function([], oy, mode=self.mode)
1089         self.assertFunctionContains1(oy_func, self.gemv)
1090         oy_v = oy_func()
1091         assert_array_almost_equal(desired_oy, oy_v)
1092     def test_y_stride_transpose(self):
1093         vs = self.get_data(y_stride=2)
1094         alpha_v, beta_v, a_v, x_v, y_v = vs
1095         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1096         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1097                                               x_v) + beta_v * y_v[::2]
1098         oy = alpha * T.dot(a.T, x) + beta * y[::2]
1099         oy_func = theano.function([], oy, mode=self.mode)
1100         self.assertFunctionContains1(oy_func, self.gemv)
1101         oy_v = oy_func()
1102 <a name="21"></a>        assert_array_almost_equal(desired_oy, oy_v)
1103     def test_a_strides(self):
1104         vs = self<font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get_data()
1105         alpha_v, beta_v, a_v, x_v, y_v = vs
1106         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1107         a_v = a_v[::-1, ::-1]
1108         a.set_value(a.get_value(</b></font>borrow=True,
1109                                 return_internal_type=True)[::-1, ::-1],
1110                     borrow=True)
1111         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v
1112         oy = alpha * T.dot(a, x) + beta * y
1113         oy_func = theano.function([], oy, mode=self.mode)
1114         self.assertFunctionContains1(oy_func, self.gemv)
1115         oy_v = oy_func()
1116         assert_array_almost_equal(desired_oy, oy_v)
1117     def test_a_strides_transpose(self):
1118         vs = self.get_data()
1119         alpha_v, beta_v, a_v, x_v, y_v = vs
1120         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1121         a_v = a_v[::-1, ::-1]
1122         a.set_value(a.get_value(borrow=True,
1123                                 return_internal_type=True)[::-1, ::-1],
1124                     borrow=True)
1125         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1126                                               x_v) + beta_v * y_v
1127         oy = alpha * T.dot(a.T, x) + beta * y
1128         oy_func = theano.function([], oy, mode=self.mode)
1129         self.assertFunctionContains1(oy_func, self.gemv)
1130         oy_v = oy_func()
1131         assert_array_almost_equal(desired_oy, oy_v)
1132     def test_upcasting_scalar_nogemv(self):
1133         vs = self.get_data()
1134         alpha_v, beta_v, a_v, x_v, y_v = vs
1135         alpha_v = alpha_v.astype("float64")
1136         a_v = a_v.astype("float32")
1137         x_v = x_v.astype("float32")
1138         y_v = y_v.astype("float32")
1139         alpha = T.dscalar('alpha')
1140         a = self.shared(a_v)
1141         x = self.shared(x_v)
1142         y = self.shared(y_v)
1143         rval = T.dot(a, x) * alpha + y
1144         f = theano.function([alpha], rval, mode=self.mode)
1145         n_gemvs = 0
1146         for node in f.maker.fgraph.toposort():
1147             if node.op == self.gemv_inplace:
1148                 n_gemvs += 1
1149                 assert node.outputs[0].dtype == 'float32'
1150         assert n_gemvs == 1, n_gemvs
1151         self.assertFunctionContains1(f, self.gemv_inplace)
1152         f(alpha_v)
1153 class TestSgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1154     dtype = float32
1155     gemv = theano.tensor.blas.gemv_no_inplace
1156     gemv_inplace = theano.tensor.blas.gemv_inplace
1157 class TestDgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1158     dtype = float64
1159     gemv = theano.tensor.blas.gemv_no_inplace
1160     gemv_inplace = theano.tensor.blas.gemv_inplace
1161 class TestGer_make_node(TestCase):
1162     def setUp(self):
1163         self.iv = T.tensor(dtype='int32', broadcastable=(False,))
1164         self.fv = T.tensor(dtype='float32', broadcastable=(False,))
1165         self.fv1 = T.tensor(dtype='float32', broadcastable=(True,))
1166         self.dv = T.tensor(dtype='float64', broadcastable=(False,))
1167         self.dv1 = T.tensor(dtype='float64', broadcastable=(True,))
1168         self.cv = T.tensor(dtype='complex64', broadcastable=(False,))
1169         self.zv = T.tensor(dtype='complex128', broadcastable=(False,))
1170         self.fv_2 = T.tensor(dtype='float32', broadcastable=(False,))
1171         self.fv1_2 = T.tensor(dtype='float32', broadcastable=(True,))
1172         self.dv_2 = T.tensor(dtype='float64', broadcastable=(False,))
1173         self.dv1_2 = T.tensor(dtype='float64', broadcastable=(True,))
1174         self.cv_2 = T.tensor(dtype='complex64', broadcastable=(False,))
1175         self.zv_2 = T.tensor(dtype='complex128', broadcastable=(False,))
1176         self.fm = T.fmatrix()
1177         self.dm = T.dmatrix()
1178         self.cm = T.cmatrix()
1179         self.zm = T.zmatrix()
1180         self.fa = T.fscalar()
1181         self.da = T.dscalar()
1182         self.ca = T.cscalar()
1183         self.za = T.zscalar()
1184     def test_works_on_all_valid_dtypes(self):
1185         self.assertEqual(self.fm.type,
1186                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1187         self.assertEqual(self.fm.type,
1188                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1189         self.assertEqual(self.fm.type,
1190                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1191         self.assertEqual(self.fm.type,
1192                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1193     def test_fails_on_invalid_dtypes(self):
1194         self.assertRaises(TypeError,
1195                           ger, T.imatrix(), T.iscalar(), T.ivector(),
1196                           T.ivector())
1197     def test_fails_for_nonscalar_alpha(self):
1198         self.assertRaises(TypeError,
1199                           ger, self.fm, self.fm, self.fv, self.fv_2)
1200         self.assertRaises(TypeError,
1201                           ger, self.fm, self.fv1, self.fv, self.fv_2)
1202         self.assertEqual(self.fm.type,
1203                          ger(self.fm, self.fv1.dimshuffle(), self.fv,
1204                              self.fv_2).type)
1205     def test_fails_for_nonmatrix_A(self):
1206         self.assertRaises(TypeError,
1207                           ger, self.fv, self.fa, self.fv, self.fv_2)
1208     def test_fails_for_nonvector_x_or_y(self):
1209         self.assertRaises(TypeError,
1210                           ger, self.fm, self.fa,
1211                           self.fv.dimshuffle('x', 0), self.fv_2)
1212         self.assertRaises(TypeError,
1213                           ger, self.fm, self.fa,
1214                           self.fv, self.fv_2.dimshuffle('x', 0))
1215     def test_fails_for_mixed_dtypes(self):
1216         self.assertRaises(TypeError, ger, self.dm, self.fa, self.fv, self.fv_2)
1217         self.assertRaises(TypeError, ger, self.fm, self.da, self.fv, self.fv_2)
1218         self.assertRaises(TypeError, ger, self.fm, self.fa, self.dv, self.fv_2)
1219         self.assertRaises(TypeError, ger, self.fm, self.fa, self.fv, self.dv_2)
1220         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.dv_2)
1221         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.zv_2)
1222 class TestGer_OpContract(TestCase, unittest_tools.T_OpContractMixin):
1223     def setUp(self):
1224         self.ops = [ger, ger_destructive]
1225     def clone(self, op):
1226         return Ger(op.destructive)
1227 class TestGer(TestCase, unittest_tools.TestOptimizationMixin):
1228     shared = staticmethod(theano.shared)
1229     def setUp(self):
1230         self.mode = theano.compile.get_default_mode().including('fast_run')
1231         self.mode = self.mode.excluding('c_blas', 'scipy_blas')
1232         dtype = self.dtype = 'float64'  # optimization isn't dtype-dependent
1233         self.A = T.tensor(dtype=dtype, broadcastable=(False, False))
1234         self.a = T.tensor(dtype=dtype, broadcastable=())
1235         self.x = T.tensor(dtype=dtype, broadcastable=(False,))
1236         self.y = T.tensor(dtype=dtype, broadcastable=(False,))
1237         self.ger = ger
1238         self.ger_destructive = ger_destructive
1239         self.gemm = gemm_no_inplace
1240     def function(self, inputs, outputs, updates=None):
1241         if updates is None:
1242             updates = []
1243         return theano.function(inputs, outputs, self.mode, updates=updates)
1244     def b(self, bval):
1245         return T.as_tensor_variable(np.asarray(bval, dtype=self.dtype))
1246     def test_b_0_triggers_ger(self):
1247         assert T.blas.local_gemm_to_ger.transform(
1248             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1249                             self.y.dimshuffle('x', 0), self.b(0)).owner)
1250     def test_b_1_triggers_ger(self):
1251         assert T.blas.local_gemm_to_ger.transform(
1252             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1253                             self.y.dimshuffle('x', 0), self.b(1)).owner)
1254     def test_b_other_does_not_triggers_ger(self):
1255         assert not T.blas.local_gemm_to_ger.transform(
1256             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1257                             self.y.dimshuffle('x', 0), self.b(1.5)).owner)
1258     def test_b_nonconst_does_not_triggers_ger(self):
1259         assert not T.blas.local_gemm_to_ger.transform(
1260             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1261                             self.y.dimshuffle('x', 0), self.a).owner)
1262     def test_outer(self):
1263         f = self.function([self.x, self.y], T.outer(self.x, self.y))
1264         self.assertFunctionContains(f, self.ger_destructive)
1265         f(np.random.rand(5).astype(self.dtype),
1266           np.random.rand(4).astype(self.dtype))
1267     def test_A_plus_outer(self):
1268         f = self.function([self.A, self.x, self.y],
1269                           self.A + T.outer(self.x, self.y))
1270         self.assertFunctionContains(f, self.ger)
1271         f(np.random.rand(5, 4).astype(self.dtype),
1272           np.random.rand(5).astype(self.dtype),
1273           np.random.rand(4).astype(self.dtype))
1274         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1275           np.random.rand(5).astype(self.dtype),
1276           np.random.rand(4).astype(self.dtype))
1277     def test_A_plus_scaled_outer(self):
1278         f = self.function([self.A, self.x, self.y],
1279                           self.A + 0.1 * T.outer(self.x, self.y))
1280         self.assertFunctionContains(f, self.ger)
1281         f(np.random.rand(5, 4).astype(self.dtype),
1282           np.random.rand(5).astype(self.dtype),
1283           np.random.rand(4).astype(self.dtype))
1284         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1285           np.random.rand(5).astype(self.dtype),
1286           np.random.rand(4).astype(self.dtype))
1287     def test_scaled_A_plus_scaled_outer(self):
1288         f = self.function([self.A, self.x, self.y],
1289                           np.asarray(0.2, self.dtype) * self.A +
1290                           np.asarray(0.1, self.dtype) * T.outer(
1291                           self.x, self.y))
1292         self.assertFunctionContains(f, self.gemm)
1293         f(np.random.rand(5, 4).astype(self.dtype),
1294           np.random.rand(5).astype(self.dtype),
1295           np.random.rand(4).astype(self.dtype))
1296         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1297           np.random.rand(5).astype(self.dtype),
1298           np.random.rand(4).astype(self.dtype))
1299     def given_dtype(self, dtype, M, N):
1300         f = self.function([self.A, self.x, self.y],
1301                           self.A + 0.1 * T.outer(self.x, self.y))
1302         self.assertFunctionContains(f, self.ger)
1303         f(np.random.rand(M, N).astype(self.dtype),
1304           np.random.rand(M).astype(self.dtype),
1305           np.random.rand(N).astype(self.dtype))
1306         f(np.random.rand(M, N).astype(self.dtype)[::-1, ::-1],
1307           np.random.rand(M).astype(self.dtype),
1308           np.random.rand(N).astype(self.dtype))
1309     def test_f32_0_0(self):
1310         return self.given_dtype('float32', 0, 0)
1311     def test_f32_1_0(self):
1312         return self.given_dtype('float32', 1, 0)
1313     def test_f32_0_1(self):
1314         return self.given_dtype('float32', 0, 1)
1315     def test_f32_1_1(self):
1316         return self.given_dtype('float32', 1, 1)
1317     def test_f32_4_4(self):
1318         return self.given_dtype('float32', 4, 4)
1319     def test_f32_7_1(self):
1320         return self.given_dtype('float32', 7, 1)
1321     def test_f32_1_2(self):
1322         return self.given_dtype('float32', 1, 2)
1323     def test_f64_4_5(self):
1324         return self.given_dtype('float64', 4, 5)
1325     def test_c64_7_1(self):
1326         return self.given_dtype('complex64', 7, 1)
1327     def test_c128_1_9(self):
1328         return self.given_dtype('complex128', 1, 9)
1329     def test_inplace(self):
1330         A = self.shared(np.random.rand(4, 5).astype(self.dtype))
1331         f = self.function([self.x, self.y], [],
1332                           updates=[(A, A + T.constant(0.1, dtype=self.dtype) *
1333                                    T.outer(self.x, self.y))])
1334         self.assertFunctionContains(f, self.ger_destructive)
1335         f(np.random.rand(4).astype(self.dtype),
1336           np.random.rand(5).astype(self.dtype))
1337         A.set_value(
1338             A.get_value(borrow=True, return_internal_type=True)[::-1, ::-1],
1339             borrow=True)
1340         f(np.random.rand(4).astype(self.dtype),
1341           np.random.rand(5).astype(self.dtype))
1342 class TestBlasStrides(TestCase):
1343     dtype = 'float64'
1344     shared = staticmethod(tensor._shared)
1345     mode = theano.compile.get_default_mode()
1346     mode = mode.including('fast_run').excluding('gpu', 'c_blas', 'scipy_blas')
1347     rng = np.random.RandomState(seed=unittest_tools.fetch_seed())
1348     def rand(self, *shape):
1349         return theano._asarray(self.rng.rand(*shape), dtype=self.dtype)
1350 <a name="2"></a>    def cmp_dot22(self, b_shp, c_shp):
1351         av = np.zeros((0, 0), dtype=self.dtype)
1352         bv = self.rand(*b_shp)
1353         cv <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= self.rand(*c_shp)
1354         a = self.shared(av, 'a')
1355         b = self.shared(bv, 'b')
1356         c = self.shared(cv, 'c')
1357         b_t = self.shared(bv.T, 'b.T')
1358         c_t = self.shared(cv.T, 'c.T')
1359         b_dev =</b></font> b.get_value(borrow=False, return_internal_type=True)
1360         c_dev = c.get_value(borrow=False, return_internal_type=True)
1361         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1362         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1363         f_nn = theano.function([], [], updates=[(a, tensor.dot(b, c))],
1364                                mode=self.mode)
1365         f_nt = theano.function([], [], updates=[(a, tensor.dot(b, c_t.T))],
1366                                mode=self.mode)
1367         f_tn = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c))],
1368                                mode=self.mode)
1369         f_tt = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c_t.T))],
1370                                mode=self.mode)
1371         for step_signs in itertools_product((-1, 1), repeat=4):
1372             for step in (1, 2):
1373                 b_step1, b_step2, c_step1, c_step2 = (s * step
1374                                                       for s in step_signs)
1375                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1376                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1377                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1378                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1379                 a_n = np.dot(bv[::b_step1, ::b_step2],
1380                              cv[::c_step1, ::c_step2])
1381                 f_nn()
1382                 assert np.allclose(a.get_value(), a_n)
1383                 f_nt()
1384                 assert np.allclose(a.get_value(), a_n)
1385                 f_tn()
1386                 assert np.allclose(a.get_value(), a_n)
1387                 f_tt()
1388 <a name="0"></a>                assert np.allclose(a.get_value(), a_n)
1389     def test_dot22(self):
1390         self<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.cmp_dot22((3, 4), (4, 5))
1391         self.cmp_dot22((1, 4), (4, 5))
1392         self.cmp_dot22((3, 4), (4, 1))
1393         self.cmp_dot22((3, 1), (1, 1))
1394         self.cmp_dot22((1, 4), (4, 1))
1395         self.cmp_dot22((3, 1), (1, 5))
1396         self.cmp_dot22((0, 4), (4, 5))
1397         self.cmp_dot22((0, 4), (4, 1))
1398         self.cmp_dot22((0, 1), (1, 5))
1399         self.cmp_dot22((3, 4), (4, 0))
1400         self.cmp_dot22((3, 0), (0, 5))
1401         self.cmp_dot22((0, 4), (4</b></font>, 0))
1402         self.cmp_dot22((0, 0), (0, 0))
1403     def cmp_dot22scalar(self, b_shp, c_shp):
1404         av = np.zeros((0, 0), dtype=self.dtype)
1405         bv = self.rand(*b_shp)
1406 <a name="4"></a>        cv = self.rand(*c_shp)
1407         l = np.float32(0.2)
1408         a <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= self.shared(av, 'a')
1409         b = self.shared(bv, 'b')
1410         c = self.shared(cv, 'c')
1411         b_t = self.shared(bv.T, 'b.T')
1412         c_t = self.shared(cv.T, 'c.T')
1413         b_dev =</b></font> b.get_value(borrow=False, return_internal_type=True)
1414         c_dev = c.get_value(borrow=False, return_internal_type=True)
1415         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1416         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1417         f_nn = theano.function([], [], updates=[(a, l * tensor.dot(b, c))],
1418                                mode=self.mode)
1419         f_nt = theano.function([], [], updates=[(a, l * tensor.dot(b, c_t.T))],
1420                                mode=self.mode)
1421         f_tn = theano.function([], [], updates=[(a, l * tensor.dot(b_t.T, c))],
1422                                mode=self.mode)
1423         f_tt = theano.function([], [],
1424                                updates=[(a, l * tensor.dot(b_t.T, c_t.T))],
1425                                mode=self.mode)
1426         for step_signs in itertools_product((-1, 1), repeat=4):
1427             for step in (1, 2):
1428                 b_step1, b_step2, c_step1, c_step2 = (s * step
1429                                                       for s in step_signs)
1430                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1431                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1432                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1433                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1434                 a_n = l * np.dot(bv[::b_step1, ::b_step2],
1435                                  cv[::c_step1, ::c_step2])
1436                 f_nn()
1437                 assert np.allclose(a.get_value(), a_n)
1438                 f_nt()
1439                 assert np.allclose(a.get_value(), a_n)
1440                 f_tn()
1441                 assert np.allclose(a.get_value(), a_n)
1442                 f_tt()
1443 <a name="1"></a>                assert np.allclose(a.get_value(), a_n)
1444     def test_dot22scalar(self):
1445         self.cmp_dot22scalar((3, 4), (<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>4, 5))
1446         self.cmp_dot22scalar((1, 4), (4, 5))
1447         self.cmp_dot22scalar((3, 4), (4, 1))
1448         self.cmp_dot22scalar((3, 1), (1, 1))
1449         self.cmp_dot22scalar((1, 4), (4, 1))
1450         self.cmp_dot22scalar((3, 1), (1, 5))
1451         self.cmp_dot22scalar((0, 4), (4, 5))
1452         self.cmp_dot22scalar((0, 4), (4, 1))
1453         self.cmp_dot22scalar((0, 1), (1, 5))
1454 <a name="11"></a>        self.cmp_dot22scalar((3, 4), (4, 0))
1455         self.cmp_dot22scalar((3, 0), (0</b></font>, 5))
1456         self.cmp_dot22scalar((0, 4), (4, 0))
1457         self.cmp_dot22scalar((0, 0), (0<font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0))
1458     def cmp_gemm(self, a_shp, b_shp, c_shp):
1459         av = self.rand(*a_shp)
1460         bv = self.rand(*b_shp)
1461         cv = self.rand(*c_shp)
1462         l = np.float32(0.2)
1463         a =</b></font> self.shared(av, 'a')
1464         b = self.shared(bv, 'b')
1465         c = self.shared(cv, 'c')
1466         a_t = self.shared(av.T, 'a.T')
1467         b_t = self.shared(bv.T, 'b.T')
1468         c_t = self.shared(cv.T, 'c.T')
1469         a_dev = a.get_value(borrow=False, return_internal_type=True)
1470         b_dev = b.get_value(borrow=False, return_internal_type=True)
1471         c_dev = c.get_value(borrow=False, return_internal_type=True)
1472         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1473         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1474         f_nnn = theano.function(
1475             [], [],
1476             updates=[(a, (l * a + tensor.dot(b, c)))],
1477             mode=self.mode)
1478         f_nnt = theano.function(
1479             [], [],
1480             updates=[(a, (l * a + tensor.dot(b, c_t.T)))],
1481             mode=self.mode)
1482         f_ntn = theano.function(
1483             [], [],
1484             updates=[(a, (l * a + tensor.dot(b_t.T, c)))],
1485             mode=self.mode)
1486         f_ntt = theano.function(
1487             [], [],
1488             updates=[(a, (l * a + tensor.dot(b_t.T, c_t.T)))],
1489             mode=self.mode)
1490         f_tnn = theano.function(
1491             [], [],
1492             updates=[(a_t, (l * a_t + tensor.dot(b, c).T))],
1493             mode=self.mode)
1494         f_tnt = theano.function(
1495             [], [],
1496             updates=[(a_t, (l * a_t + tensor.dot(b, c_t.T).T))],
1497             mode=self.mode)
1498         f_ttn = theano.function(
1499             [], [],
1500             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c).T))],
1501             mode=self.mode)
1502         f_ttt = theano.function(
1503             [], [],
1504             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c_t.T).T))],
1505             mode=self.mode)
1506         for step_signs in itertools_product((-1, 1), repeat=6):
1507             for step in (1, 2):
1508                 a_step1, a_step2, b_step1, b_step2, c_step1, c_step2 = \
1509                     (s * step for s in step_signs)
1510                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1511                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1512                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1513                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1514                 a_n = (l * av[::a_step1, ::a_step2] +
1515                        np.dot(bv[::b_step1, ::b_step2],
1516                               cv[::c_step1, ::c_step2]))
1517                 at_n = (l * av[::a_step1, ::a_step2].T +
1518                         np.dot(bv[::b_step1, ::b_step2],
1519                                cv[::c_step1, ::c_step2]).T)
1520                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1521                 f_nnn()
1522                 assert np.allclose(a.get_value(), a_n)
1523                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1524                 f_nnt()
1525                 assert np.allclose(a.get_value(), a_n)
1526                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1527                 f_ntn()
1528                 assert np.allclose(a.get_value(), a_n)
1529                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1530                 f_ntt()
1531                 assert np.allclose(a.get_value(), a_n)
1532                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1533                               borrow=True)
1534                 f_tnn()
1535                 assert np.allclose(a_t.get_value(), at_n)
1536                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1537                               borrow=True)
1538                 f_tnt()
1539                 assert np.allclose(a_t.get_value(), at_n)
1540                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1541                               borrow=True)
1542                 f_ttn()
1543                 assert np.allclose(a_t.get_value(), at_n)
1544                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1545                               borrow=True)
1546                 f_ttt()
1547                 assert np.allclose(a_t.get_value(), at_n)
1548     def test_gemm(self):
1549         self.cmp_gemm((3, 5), (3, 4), (4, 5))
1550         self.cmp_gemm((1, 5), (1, 4), (4, 5))
1551         self.cmp_gemm((3, 1), (3, 4), (4, 1))
1552         self.cmp_gemm((3, 1), (3, 1), (1, 1))
1553         self.cmp_gemm((1, 1), (1, 4), (4, 1))
1554         self.cmp_gemm((3, 5), (3, 1), (1, 5))
1555         self.cmp_gemm((0, 5), (0, 4), (4, 5))
1556         self.cmp_gemm((0, 1), (0, 4), (4, 1))
1557         self.cmp_gemm((0, 5), (0, 1), (1, 5))
1558 <a name="17"></a>        self.cmp_gemm((3, 0), (3, 4), (4, 0))
1559         self.cmp_gemm((3, 5), (3, 0), (0, 5))
1560         self.cmp_gemm((0, 0), (0, 4), (4, 0))
1561         self.cmp_gemm((0, 0), (0, 0), (0<font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0))
1562     def cmp_gemv(self, a_shp, b_shp, c_shp):
1563         av = self.rand(a_shp)
1564         bv = self.rand(*b_shp)
1565         cv = self.rand(c_shp)
1566         l = np.</b></font>float32(0.2)
1567         a = self.shared(av, 'a')
1568         b = self.shared(bv, 'b')
1569         c = self.shared(cv, 'c')
1570         b_t = self.shared(bv.T, 'b.T')
1571         a_dev = a.get_value(borrow=False, return_internal_type=True)
1572         b_dev = b.get_value(borrow=False, return_internal_type=True)
1573         c_dev = c.get_value(borrow=False, return_internal_type=True)
1574         f_n = theano.function([], [], updates=[(a, (a + l * tensor.dot(b, c)))],
1575                               mode=self.mode)
1576         f_t = theano.function([], [],
1577                               updates=[(a, (a + l * tensor.dot(b_t.T, c)))],
1578                               mode=self.mode)
1579         for step_signs in itertools_product((1, -1), repeat=4):
1580             for step in (1, 2):
1581                 a_step, b_step1, b_step2, c_step = (s * step
1582                                                     for s in step_signs)
1583                 a.set_value(a_dev.copy()[::a_step], borrow=True)
1584                 b.set_value(b_dev.copy()[::b_step1, ::b_step2],
1585                             borrow=True)
1586                 b_t.set_value(transpose(b_dev.copy())[::b_step2, ::b_step1],
1587                               borrow=True)
1588                 c.set_value(c_dev.copy()[::c_step], borrow=True)
1589                 a_n = (av[::a_step] +
1590                        l * np.dot(bv[::b_step1, ::b_step2],
1591                                   cv[::c_step]))
1592                 f_n()
1593                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
1594                 a.set_value(a_dev.copy()[::a_step], borrow=True)
1595                 f_t()
1596                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
1597     def test_gemv(self):
1598         self.cmp_gemv(3, (3, 5), 5)
1599         self.cmp_gemv(1, (1, 5), 5)
1600         self.cmp_gemv(3, (3, 1), 1)
1601         self.cmp_gemv(0, (0, 5), 5)
1602 <a name="16"></a>        self.cmp_gemv(3, (3, 0), 0)
1603         self.cmp_gemv(0, (0, 1), 1)
1604         self.cmp_gemv(1, (1, 0), 0)
1605         self.cmp_gemv(0, (0<font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0), 0)
1606     def cmp_ger(self, a_shp, b_shp, c_shp):
1607         av = self.rand(*a_shp)
1608         bv = self.rand(b_shp)
1609         cv = self.rand(c_shp)
1610         l = np.</b></font>float32(0.2)
1611         a = self.shared(av, 'a')
1612         b = self.shared(bv, 'b')
1613         c = self.shared(cv, 'c')
1614         a_t = self.shared(av.T, 'a.T')
1615         a_dev = a.get_value(borrow=False, return_internal_type=True)
1616         b_dev = b.get_value(borrow=False, return_internal_type=True)
1617         c_dev = c.get_value(borrow=False, return_internal_type=True)
1618         f_n = theano.function(
1619             [], [],
1620             updates=[(a, (a + l * tensor.outer(b, c)))],
1621             mode=self.mode)
1622         f_t = theano.function(
1623             [], [],
1624             updates=[(a_t, (a_t + l * tensor.outer(b, c).T))],
1625             mode=self.mode)
1626         for step_signs in itertools_product((1, -1), repeat=4):
1627             for step in (1, 2):
1628                 a_step1, a_step2, b_step, c_step = (s * step
1629                                                     for s in step_signs)
1630                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1631                 a_t.set_value(transpose(a_dev.copy())[::a_step1, ::a_step2],
1632                               borrow=True)
1633                 b.set_value(b_dev.copy()[::b_step], borrow=True)
1634                 c.set_value(c_dev.copy()[::c_step], borrow=True)
1635                 f_n()
1636                 n_n = (av[::a_step1, ::a_step2] +
1637                        l * np.outer(bv[::b_step], cv[::c_step]))
1638                 assert np.allclose(a.get_value(), n_n), (a.get_value(), n_n)
1639                 f_t()
1640                 n_t = (av.T[::a_step1, ::a_step2] +
1641                        l * np.outer(bv[::b_step], cv[::c_step]).T)
1642                 assert np.allclose(a_t.get_value(), n_t), (a_t.get_value(), n_t)
1643     def test_ger_strides(self):
1644         self.cmp_ger((3, 5), 3, 5)
1645         self.cmp_ger((1, 5), 1, 5)
1646         self.cmp_ger((3, 1), 3, 1)
1647         self.cmp_ger((0, 5), 0, 5)
1648         self.cmp_ger((3, 0), 3, 0)
1649         self.cmp_ger((0, 1), 0, 1)
1650         self.cmp_ger((1, 0), 1, 0)
1651         self.cmp_ger((0, 0), 0, 0)
1652     def test_gemm_non_contiguous(self):
1653         aval = np.ones((6, 2))
1654 <a name="7"></a>        bval = np.ones((2, 7))
1655         cval = np.arange(7) + np.arange(0, .6, .1)[:, np.newaxis]
1656         a <font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.shared(aval[:3], borrow=True)
1657         b = theano.shared(bval[:, :5], borrow=True)
1658 <a name="23"></a>        c = theano.shared(cval[:3, :5], borrow=True)
1659         s = theano.</b></font>tensor.scalar()
1660         upd_c = s * c + theano.tensor.dot<font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(a, b)
1661         f = theano.function([s], [], updates={c: upd_c})
1662         f(0)
1663         ref_output = np.ones((3, 5)) * 2
1664         unittest_tools.</b></font>assert_allclose(c.get_value(), ref_output)
1665 class test_infer_shape(unittest_tools.InferShapeTester):
1666     def test_dot22(self):
1667         x, y = T.matrices('xy')
1668         self._compile_and_check(
1669             [x, y], [T.blas._dot22(x, y)],
1670             [np.random.random((2, 3)).astype(config.floatX),
1671              np.random.random((3, 4)).astype(config.floatX)],
1672             T.blas.Dot22)
1673     def test_dot22scalar(self):
1674         x, y = T.matrices('xy')
1675         a = T.scalar('a')
1676         self._compile_and_check(
1677             [x, y, a], [T.blas._dot22scalar(x, y, a)],
1678             [np.random.random((2, 3)).astype(config.floatX),
1679              np.random.random((3, 4)).astype(config.floatX),
1680              np.asarray(0.5, dtype=config.floatX)],
1681             T.blas.Dot22Scalar)
1682     def test_gemm(self):
1683         x, y, z = T.matrices('xyz')
1684         a = T.scalar('a')
1685         b = T.scalar('b')
1686 <a name="25"></a>        self._compile_and_check(
1687             [x, y, a, z, b], [T.blas.gemm(z, a, x, y, b)],
1688             [np.random.random((2, 3)).astype(config.floatX),
1689              np.random.random((3, 4)).astype(config<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.floatX),
1690              np.asarray(0.5, dtype=config.floatX),
1691              np.random.random((2, 4)).astype(config.</b></font>floatX),
1692              np.asarray(0.5, dtype=config.floatX)],
1693             T.blas.Gemm)
1694     def test_gemv(self):
1695         A = T.matrix('A')
1696         x, y = T.vectors('xy')
1697         a = T.scalar('a')
1698         b = T.scalar('b')
1699         self._compile_and_check(
1700             [y, a, A, x, b], [T.blas.gemv(y, a, A, x, b)],
1701             [np.random.random((2,)).astype(config.floatX),
1702              np.asarray(0.5, dtype=config.floatX),
1703              np.random.random((2, 3)).astype(config.floatX),
1704              np.random.random((3,)).astype(config.floatX),
1705              np.asarray(0.5, dtype=config.floatX)],
1706             T.blas.Gemv)
1707     def test_ger(self):
1708         A = T.matrix('A')
1709         x, y = T.vectors('xy')
1710         a = T.scalar('a')
1711         self._compile_and_check(
1712             [A, a, x, y], [T.blas.ger(A, a, x, y)],
1713             [np.random.random((2, 3)).astype(config.floatX),
1714              np.asarray(0.5, dtype=config.floatX),
1715              np.random.random((2,)).astype(config.floatX),
1716              np.random.random((3,)).astype(config.floatX)],
1717             T.blas.Ger)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
