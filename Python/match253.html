<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_opt_5.py &amp; test_blas_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_opt_5.py &amp; test_blas_1.py
      </h3>
<h1 align="center">
        9.6%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_opt_5.py (15.069445%)<th>test_blas_1.py (7.112422%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(907-956)<td><a href="#" name="0">(1919-1930)</a><td align="center"><font color="#ff0000">48</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(976-1022)<td><a href="#" name="1">(1989-1999)</a><td align="center"><font color="#d90000">41</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(673-678)<td><a href="#" name="2">(1866-1875)</a><td align="center"><font color="#6f0000">21</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1-23)<td><a href="#" name="3">(1-27)</a><td align="center"><font color="#640000">19</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(655-659)<td><a href="#" name="4">(1939-1946)</a><td align="center"><font color="#5f0000">18</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(709-716)<td><a href="#" name="5">(69-73)</a><td align="center"><font color="#5a0000">17</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(638-646)<td><a href="#" name="6">(727-738)</a><td align="center"><font color="#5a0000">17</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(175-181)<td><a href="#" name="7">(2245-2249)</a><td align="center"><font color="#5a0000">17</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(1097-1112)<td><a href="#" name="8">(424-426)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(211-214)<td><a href="#" name="9">(1229-1232)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(122-125)<td><a href="#" name="10">(286-290)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(78-85)<td><a href="#" name="11">(2001-2009)</a><td align="center"><font color="#4f0000">15</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(582-583)<td><a href="#" name="12">(575-578)</a><td align="center"><font color="#4a0000">14</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(60-64)<td><a href="#" name="13">(1409-1415)</a><td align="center"><font color="#4a0000">14</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(614-619)<td><a href="#" name="14">(742-747)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(482-484)<td><a href="#" name="15">(76-78)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(454-464)<td><a href="#" name="16">(2180-2186)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(387-394)<td><a href="#" name="17">(2125-2131)</a><td align="center"><font color="#450000">13</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(762-765)<td><a href="#" name="18">(673-676)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(607-612)<td><a href="#" name="19">(1179-1184)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(599-603)<td><a href="#" name="20">(1099-1105)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(579-581)<td><a href="#" name="21">(1524-1528)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(496-499)<td><a href="#" name="22">(1022-1029)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(307-312)<td><a href="#" name="23">(2250-2255)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(298-303)<td><a href="#" name="24">(168-173)</a><td align="center"><font color="#3f0000">12</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(49-50)<td><a href="#" name="25">(2284-2286)</a><td align="center"><font color="#3f0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_opt_5.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from nose.tools import assert_raises
2 import numpy as np
3 import theano
4 from theano import tensor
5 import theano.tensor.slinalg as slinalg
6 from theano.tests.breakpoint import PdbBreakpoint
7 from theano.tests import unittest_tools as utt, test_ifelse
8 from theano.tensor.tests import test_basic
9 from theano.gof.opt import check_stack_trace
10 import theano.gpuarray
11 from .. import basic_ops
12 from ..type import GpuArrayType, gpuarray_shared_constructor, get_context
13 from ..basic_ops import (
14     GpuAlloc, GpuAllocEmpty, GpuReshape, GpuFromHost, HostFromGpu, host_from_gpu)
15 from ..blas import GpuGemm
16 from ..elemwise import (
17     GpuCAReduceCuda, GpuCAReduceCPY, GpuElemwise, Elemwise, max_inputs_to_GpuElemwise)
18 from ..dnn import GpuDnnReduction
19 from ..subtensor import GpuSubtensor
20 from</b></font> ..linalg import GpuCusolverSolve, cusolver_available, GpuCholesky
21 from .config import mode_with_gpu, mode_without_gpu, test_ctx_name, SkipTest
22 import unittest
23 from theano.tensor.nnet import abstract_conv
24 from theano.gpuarray import dnn, blas, opt
25 def _check_stack_trace(thing):
26     def _ops_to_check(op):
27         if not isinstance(op, theano.gof.Op):
28             op = op.op  # assume it is an apply node
29         return not isinstance(op, (theano.compile.ops.Shape_i,
30                                    theano.compile.ops.Shape,
31                                    theano.compile.ops.DeepCopyOp,
32                                    theano.tensor.opt.MakeVector,
33                                    theano.tensor.subtensor.Subtensor,
34                                    theano.tensor.elemwise.Elemwise,
35                                    theano.ifelse.IfElse,
36                                    GpuFromHost, HostFromGpu,
37                                    ))
38     return check_stack_trace(thing, ops_to_check=_ops_to_check,
39                              bug_print="ignore")
40 def test_local_assert():
41     x = theano<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.tensor.fmatrix()
42     a = theano.tensor.opt.assert_op(x, theano.tensor.eq(x, 0).</b></font>any())
43     f = theano.function([x], a, mode=mode_with_gpu)
44     topo = f.maker.fgraph.toposort()
45     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
46     assert len(a_op) == 1
47     assert isinstance(a_op[0].inputs[0].type, GpuArrayType)
48 def test_local_remove_all_assert():
49     x = theano.tensor.fmatrix()
50     a = theano.tensor.opt.assert_op(x, theano.tensor<font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.eq(x, 0).any())
51     f = theano.function([x], a, mode=mode_with_gpu.excluding('unsafe'))
52     topo = f.maker.fgraph.</b></font>toposort()
53     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
54     assert len(a_op) == 1
55     f = theano.function([x], a, mode=mode_with_gpu.including('unsafe'))
56     topo = f.maker.fgraph.toposort()
57     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
58     assert len(a_op) == 0
59     topo = f.maker.fgraph.toposort()
60     a_op = [n for n in topo if isinstance(n.op, theano.tensor.opt.Assert)]
61     assert len(<font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>a_op) == 1
62 def test_local_gpu_contiguous_gpu_contiguous():
63     a = tensor.fmatrix()
64     o1 = basic_ops.gpu_contiguous(a)
65     o2 = basic_ops.gpu_contiguous(o1)
66     f1 = theano.function([a], o1, mode=</b></font>mode_with_gpu)
67     f2 = theano.function([a], o2, mode=mode_with_gpu)
68     assert 1 == len([node for node in f1.maker.fgraph.toposort()
69                      if isinstance(node.op, basic_ops.GpuContiguous)])
70     assert 1 == len([node for node in f2.maker.fgraph.toposort()
71                      if isinstance(node.op, basic_ops.GpuContiguous)])
72     assert _check_stack_trace(f1)
73     assert _check_stack_trace(f2)
74 def test_local_gpu_contiguous():
75     a = tensor.fmatrix()
76     o = tensor.extra_ops.cpu_contiguous(a)
77     f = theano.function([a], o, mode=mode_with_gpu)
78     assert 1 == len([node for node in f.maker.fgraph.toposort()
79                      if isinstance(node.op, basic_ops.GpuContiguous)])
80     f([[2.]])
81     assert _check_stack_trace(f)
82 def test_flatten():
83     m = theano.tensor.fmatrix()
84     f = theano.function([m], m.flatten(), mode=mode_with_gpu)
85     val = np.random.rand(10, 11).astype("float32")
86     res = f(val)
87     utt.assert_allclose(res, val.flatten())
88     assert res.shape == val.flatten().shape
89     assert GpuReshape in [type(node.op)
90                           for node in f.maker.fgraph.toposort()]
91     val = np.random.rand(10, 11).astype("float32")
92     res = f(val)
93     utt.assert_allclose(res, val.flatten())
94     assert res.shape == val.flatten().shape
95     assert GpuReshape in [type(node.op)
96     assert _check_stack_trace(f)
97     f = theano<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.function([m], m.flatten(ndim=2),
98                         mode=mode_with_gpu.excluding("local_useless_reshape"))
99     val = np.random.rand(10, 11).astype("float32")
100     res =</b></font> f(val)
101     utt.assert_allclose(res, val)
102     assert res.shape == val.shape
103     assert GpuReshape in [type(node.op)
104                           for node in f.maker.fgraph.toposort()]
105     assert _check_stack_trace(f)
106     m = theano.tensor.tensor3()
107     f = theano.function([m], m.flatten(ndim=2), mode=mode_with_gpu)
108     val = np.random.rand(10, 11, 12).astype("float32")
109     res = f(val)
110     utt.assert_allclose(res, val.reshape(10, -1))
111     assert res.shape == val.reshape(10, -1).shape
112     assert GpuReshape in [type(node.op)
113                           for node in f.maker.fgraph.toposort()]
114     assert _check_stack_trace(f)
115 def test_reduce():
116     kind = get_context(test_ctx_name).kind
117     for method, param in [('sum', dict(acc_dtype='float32')),
118                           ('prod', dict(acc_dtype='float32')),
119                           ('max', {}), ('min', {})]:
120         m = theano.tensor.fmatrix()
121         f = theano.function([m], getattr(m, method)(axis=0,
122                                                     **param),
123                             mode=mode_with_gpu)
124         val = np.random.rand(10, 11).astype("float32")
125         res = f(val)
126         utt.assert_allclose(res, getattr(val, method)(axis=0))
127         assert res.shape == (11,)
128         topo = f.maker.fgraph.toposort()
129         ops = [type(node.op) for node in topo]
130         if kind == b'opencl' and method in ["max", "min"]:
131             assert not(GpuCAReduceCuda in ops or
132                        GpuCAReduceCPY in ops or
133                        GpuDnnReduction in ops)
134         else:
135             assert (GpuCAReduceCuda in ops or
136                     GpuCAReduceCPY in ops or
137                     GpuDnnReduction in ops)
138 def test_local_gpualloc_memset_0():
139     i = theano.tensor.iscalar()
140     z <font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.zeros((1,), dtype='float32')
141     o = np.ones((1,), dtype='float32')
142     ones = np.ones((2,), dtype='float32')
143     a = tensor.</b></font>alloc(z, i)
144     f = theano.function([i], a, mode=mode_with_gpu)
145     topo = f.maker.fgraph.toposort()
146     assert len(topo) == 1
147     assert isinstance(topo[0].op, theano.tensor.Alloc)
148     assert (np.asarray(f(6)) == 0).all()
149     assert _check_stack_trace(f)
150     a = tensor.alloc(z, i)
151     f = theano.function([i], a.cumsum(), mode=mode_with_gpu)
152     topo = f.maker.fgraph.toposort()
153     assert len(topo) == 3
154     assert isinstance(topo[0].op, GpuAlloc)
155     assert (np.asarray(f(6)) == 0).all()
156     assert _check_stack_trace(f)
157     a = GpuAlloc(test_ctx_name)(z, i)
158     f = theano.function([i], a, mode=mode_with_gpu)
159     topo = f.maker.fgraph.toposort()
160     assert len(topo) == 1
161     assert isinstance(topo[0].op, GpuAlloc) and topo[0].op.memset_0
162     assert (np.asarray(f(6)) == 0).all()
163     assert _check_stack_trace(f)
164     a = GpuAlloc(test_ctx_name)(o, i)
165     f = theano.function([i], a, mode=mode_with_gpu)
166     topo <font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= f.maker.fgraph.toposort()
167     assert len(topo) == 1
168     assert isinstance(topo[0].op, GpuAlloc)
169     assert not topo[0].op.</b></font>memset_0
170     assert (np.asarray(f(6)) == 1).all()
171     assert _check_stack_trace(f)
172     a = GpuAlloc(test_ctx_name)(ones, i)
173     f = theano.function([i], a, mode=mode_with_gpu)
174     topo = f.maker.fgraph.toposort()
175     assert len(topo) == 1
176     assert isinstance(topo[0].op, GpuAlloc)
177     assert not topo[0].op.memset_0
178     assert (np.asarray(f(2)) == 1).all()
179     assert _check_stack_trace(f)
180 def test_local_gpualloc_empty():
181     i = theano.tensor.iscalar()
182     ii = theano.tensor.iscalar()
183     a = tensor.AllocEmpty('float32')(i)
184     f = theano.function([i], a, mode=mode_with_gpu)
185     topo = f.maker.fgraph.toposort()
186     assert len(topo) == 1
187     assert isinstance(topo[0].op, theano.tensor.AllocEmpty)
188     assert f(3).shape == (3,)
189     assert _check_stack_trace(f)
190     a = tensor.AllocEmpty('float32')(i)
191     f = theano.function([i], a.cumsum(), mode=mode_with_gpu)
192     topo = f.maker.fgraph.toposort()
193     assert len(topo) == 3
194     assert isinstance(topo[0].op, GpuAllocEmpty)
195     assert f(3).shape == (3,)
196     assert _check_stack_trace(f)
197     a = tensor.AllocEmpty('float32')(i, ii)
198     f = theano.function([i, ii], a.cumsum(axis=0), mode=mode_with_gpu)
199     topo = f.maker.fgraph.toposort()
200     assert len(topo) == 3
201     assert isinstance(topo[0].op, GpuAllocEmpty)
202     assert f(3, 4).shape == (3, 4)
203     assert _check_stack_trace(f)
204 def test_rebroadcast():
205     d = np.random.rand(10, 10).astype('float32')
206     v = theano.tensor.fmatrix()
207     up = tensor.unbroadcast(v.sum().dimshuffle('x', 'x'), 0, 1)
208     f = theano.function([v], [up], mode=mode_with_gpu)
209     f(d)
210     topo = f.maker.fgraph.toposort()
211     rebrs = [node for node in topo if isinstance(node.op, tensor.Rebroadcast)]
212     assert len(rebrs) == 1
213     rebr = rebrs[0]
214     assert isinstance(rebr.inputs[0].type, GpuArrayType)
215     assert isinstance(rebr.outputs[0].type, GpuArrayType)
216     assert _check_stack_trace(f)
217 class TestSpecifyShape(test_basic.TestSpecifyShape):
218     mode = mode_with_gpu
219     input_type = GpuArrayType
220 class test_gpu_ifelse(test_ifelse.test_ifelse):
221     mode = mode_with_gpu
222     @staticmethod
223     def cast_output(v):
224         return basic_ops.as_gpuarray_variable(v, test_ctx_name)
225     def get_ifelse(self, n):
226         return theano.ifelse.IfElse(n, gpu=True, as_view=<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>True)
227     def test_lifter_with_inputs_of_graph(self):
228         x = tensor.vector()
229         cond = tensor.iscalar()
230         f = theano.function([x</b></font>, cond],
231                             mode=mode_with_gpu)
232         assert f(np.float32([1, 2, 3]), 0) == 6
233         assert _check_stack_trace<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(f)
234         x = tensor.vector()
235         cond = tensor.scalar()
236         f = theano.function([x, cond],
237                             theano.</b></font>ifelse.ifelse(cond, x.mean(), x.sum()),
238                             mode=mode_with_gpu)
239         assert f(np.float32([1, 2, 3]), 0) == 6
240         assert _check_stack_trace(f)
241     def test_lifter_with_shared_var(self):
242         x = tensor.lscalar('x')
243         y = gpuarray_shared_constructor(np.asarray(1, dtype='float32'),
244                                         target=test_ctx_name)
245         z = tensor.constant(2.)
246         a = theano.ifelse.ifelse(x, y, z)
247         with theano.change_flags(on_opt_error='raise'):
248             theano.function([x], [a], mode=mode_with_gpu)
249 def test_print_op():
250     b = tensor.fmatrix()
251     f = theano.function([b], theano.printing.Print()(b) * 2,
252                         mode=mode_with_gpu)
253     topo = f.maker.fgraph.toposort()
254     assert isinstance(topo[0].op, GpuFromHost)
255     assert isinstance(topo[1].op, theano.printing.Print)
256     assert isinstance(topo[2].op, GpuElemwise)
257     assert topo[3].op == host_from_gpu
258     assert _check_stack_trace(f)
259     f(np.random.random((5, 5)).astype('float32'))
260 def test_pdbbreakpoint_op():
261     b = tensor.fmatrix()
262     condition = tensor.gt(b.sum(), 0)
263     b_monitored = PdbBreakpoint(name='TestBreakpoint')(condition, b)
264     output = b_monitored ** 2
265     f = theano.function([b], output, mode=mode_with_gpu)
266     topo = f.maker.fgraph.toposort()
267     assert isinstance(topo[-2].op, GpuElemwise)
268     assert topo[-1].op == host_from_gpu
269     assert _check_stack_trace(f)
270 def test_local_gpu_elemwise_careduce():
271     mode_with_gpu_no_cudnn = mode_with_gpu.excluding('cudnn')
272     x = theano.tensor.matrix()
273     def fn_sum_square(x, axis):
274         return (x * x).sum(axis=axis)
275     def fn_sum_abs(x, axis):
276         return abs(x).sum(axis=axis)
277     def fn_max_abs(x, axis):
278         return abs(x).max(axis=axis)
279     for fn, pre_scalar_op in ((fn_sum_square, theano.scalar.sqr),
280                               (fn_sum_abs, theano.scalar.abs_),
281                               (fn_max_abs, theano.scalar.abs_)):
282         for axis in (None, 0, 1):
283             o = fn(x, axis)
284             f = theano.function([x], o, mode=mode_with_gpu_no_cudnn)
285             topo = f.maker.fgraph.toposort()
286             assert len(topo) == 3
287             assert isinstance(topo[1].op, GpuCAReduceCuda)
288             assert _check_stack_trace(f)
289             data = np.random.rand(3, 4).astype(theano.config.floatX)
290             utt.assert_allclose(fn(data, axis), f(da<font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>ta))
291 def test_local_lift_dot22scalar():
292     x = tensor.matrix()
293     y = tensor.matrix()
294     a = tensor.scalar()
295     o = tensor.</b></font>blas.Dot22Scalar()(x, y, a)
296     f_cpu = theano.function([x, y, a], o)
297     f_gpu = theano.function([x, y, a], o, mode=mode_with_gpu)
298     assert not any(isinstance(n.op, tensor.blas.Dot22Scalar)
299                    for n in f_gpu.maker.fgraph.apply_nodes)
300     assert any(isinstance(n.op, GpuGemm)
301                for n in f_gpu.maker.fgraph.apply_nodes)
302     x_val = np.random.random((2, 3)).astype(theano.config.floatX)
303     y_val = np.random.random((3, 4)).astype(theano.config.floatX)
304     a_val = 0.5
305     utt.assert_allclose(f_cpu(x_val, y_val, a_val), f_gpu(x_val, y_val, a_val))
306     assert _check_stack_trace(f_gpu)
307 def test_local_gpu_subtensor():
308     t = tensor._shared(np.zeros(20, "float32"))
309     f = theano.function([], t[3:4], mode=mode_with_gpu)
310     topo = f.maker.fgraph.toposort()
311     assert any([type(node.op) is tensor.Subtensor for node in topo])
312     assert not any([isinstance(node.op, GpuSubtensor) for node in topo])
313     assert _check_stack_trace(f)
314     t = tensor.fmatrix()
315     f = theano.function([t], t[3:4], mode=mode_with_gpu)
316     topo = f.maker.fgraph.toposort()
317     assert any([type(node.op) is tensor.Subtensor for node in topo])
318     assert not any([isinstance(node.op, GpuSubtensor) for node in topo])
319     assert _check_stack_trace(f)
320     t = tensor.fmatrix()
321     f = theano.function([t], [t[3:4], t + 1], mode=mode_with_gpu)
322     topo = f.maker.fgraph.toposort()
323     assert not any([type(node.op) is tensor.Subtensor for node in topo])
324     assert any([isinstance(node.op, GpuSubtensor) for node in topo])
325     assert _check_stack_trace(f)
326     t = tensor.fmatrix()
327     f = theano.function([t], [t[3:4], t + 1, t], mode=mode_with_gpu)
328     topo = f.maker.fgraph.toposort()
329     assert not any([type(node.op) is tensor.Subtensor for node in topo])
330     assert any([isinstance(node.op, GpuSubtensor) for node in topo])
331     assert _check_stack_trace(f)
332     t = tensor._shared(np.zeros(20, "float32"))
333     f = theano.function([], t[3:4] + 1, mode=mode_with_gpu)
334     topo = f.maker.fgraph.toposort()
335     assert any([type(node.op) is tensor.Subtensor for node in topo])
336     assert not any([isinstance(node.op, GpuSubtensor) for node in topo])
337     assert any([isinstance(node.op, tensor.Elemwise) for node in topo])
338     assert _check_stack_trace(<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>f)
339 def test_local_gpu_elemwise():
340     a = tensor.bmatrix()
341     b = tensor.fmatrix()
342     c = tensor.fmatrix()
343     a_v = (np</b></font>.random.rand(4, 5) * 10).astype("int8")
344     b_v = (np.random.rand(4, 5) * 10).astype("float32")
345     c_v = (np.random.rand(4, 5) * 10).astype("float32")
346     f = theano.function([a, b, c], a + b + c, mode=mode_with_gpu)
347     topo = f.maker.fgraph.toposort()
348     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
349     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
350     utt.assert_allclose(f(a_v, b_v, c_v), a_v + b_v + c_v)
351     assert _check_stack_trace(f)
352     b_s = theano.scalar.float32()
353     c_s = theano.scalar.float32()
354     out_s <font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.scalar.Composite([a_s, b_s, c_s], [a_s + b_s + c_s])
355     out_op = tensor.Elemwise(out_s)
356     f = theano.function([a, b, c], out_op(</b></font>a, b, c), mode=mode_with_gpu)
357     topo = f.maker.fgraph.toposort()
358     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
359     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
360     utt.assert_allclose(f(a_v, b_v, c_v), a_v + b_v + c_v)
361     assert _check_stack_trace(f)
362     return  # Not yet implemeted
363     a = tensor.fmatrix()
364     from theano.scalar.basic import identity
365     out_s = theano.scalar.Composite([<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>a_s, b_s, c_s],
366                                     [identity(a_s), identity(c_s), identity(b_s)])
367     outs_op = tensor.Elemwise(out_s)
368     f = theano.function([a</b></font>, b, c], outs_op(a, b, c), mode=mode_with_gpu)
369     topo = f.maker.fgraph.toposort()
370     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
371     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
372     out = f(a_v, b_v, c_v)
373     utt.assert_allclose(out[0], a_v)
374     utt.assert_allclose(out[1], c_v)
375     utt.assert_allclose(out[2], b_v)
376     assert _check_stack_trace(f)
377     out_s = theano.scalar.Composite([a_s, b_s, c_s], [a_s + b_s, a_s * b_s])
378     outs_op = tensor.Elemwise(out_s)
379     f = theano.function([a, b, c], outs_op(a, b, c), mode=mode_with_gpu)
380     topo = f.maker.fgraph.toposort()
381     assert sum(isinstance(node.op, GpuElemwise) for node in topo) == 1
382     assert sum(type(node.op) == tensor.Elemwise for node in topo) == 0
383     out = f(a_v, b_v, c_v)
384     utt.assert_allclose(out[0], a_v + b_v)
385     utt.assert_allclose(out[1], a_v * c_v)
386     assert _check_stack_trace(f)
387     c = gpuarray_shared_constructor(np.asarray(c_v, dtype='float32'))
388     f = theano.function([a, b], outs_op(a[::2], b[::2], c[::2]),
389                         mode=mode_with_gpu)
390     out = f(a_v, b_v)
391     utt.assert_allclose(out[0], a_v[::2] + b_v[::2])
392     utt.assert_allclose(out[1], a_v[::2] * c_v[::2])
393     assert _check_stack_trace(f)
394 def test_many_arg_elemwise():
395     rng = np.random.RandomState([1, 2, 3])
396     nb_of_inputs_overflows = []
397     for num_args in [64]:
398         for op_to_test in [theano.tensor.add, theano.tensor.mul]:
399             for nb_dim in [2, 8]:
400                 shapes = [rng.randint(1, 5) for i in range(nb_dim)]
401                 args = [np.cast['float32'](rng.randn(*shapes))
402                         for arg in range(0, num_args)]
403                 symb_args = [theano.tensor.TensorType('float32',
404                                                       (False,) * nb_dim)()
405                              for arg in range(0, num_args)]
406                 outputs = []
407                 for mode in [mode_with_gpu, mode_without_gpu]:
408                     output = op_to_test(*symb_args)
409                     f = theano.function(symb_args, output, mode=mode)
410                     outputs.append(f(*args))
411                     if mode is mode_with_gpu:
412                         nb_of_inputs_overflows.append(
413                             max_inputs_to_GpuElemwise(output.owner) - num_args)
414                         nodelst = [node for node in f.maker.fgraph.apply_nodes]
415                         assert any(isinstance(node.op, GpuElemwise)
416                                    for node in nodelst)
417                         assert not any(isinstance(node.op, Elemwise)
418                                        for node in nodelst
419                                        if not isinstance(node.op, GpuElemwise))
420                 results_gpu, results_cpu = outputs
421                 utt.assert_allclose(results_gpu, results_cpu)
422     assert any(overflow &gt;= 0 for overflow in nb_of_inputs_overflows)
423     assert any(overflow &lt; 0 for overflow in nb_of_inputs_overflows)
424 def test_not_useless_scalar_gpuelemwise():
425         X = tensor.fmatrix()
426         x = np.random.randn(32, 32).astype(</b></font>np.float32)
427         m1 = theano.shared<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(np.random.randn(32, 32).astype(np.float32))
428         loss = (X - tensor.dot(X, m1)).norm(L=</b></font>2)
429         lr = theano.shared(np.asarray(.001, dtype=np.float32))
430         grad = tensor.grad(loss, m1)
431         train = theano.function(inputs=[X], updates=[(m1, m1 - lr * grad)],
432                                 mode=mode_with_gpu)
433         train(x)
434         topo = train.maker.fgraph.toposort()
435         gemms = [app for app in topo if isinstance(app.op, GpuGemm)]
436         assert len(gemms) == 2
437         assert isinstance(gemms[1].inputs[1].owner.op, tensor.Elemwise)
438     prev = theano.config.on_opt_error
439     try:
440         theano.config.on_opt_error <font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= 'raise'
441         s = tensor.ivector()
442         a = tensor.ftensor4()
443         b = tensor.ftensor4()
444         c = tensor.</b></font>nnet.abstract_conv.AbstractConv2d_gradWeights()(a, b, s)
445         assert _check_stack_trace(f)
446     finally:
447         theano.config.on_opt_error =<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b> prev
448 def test_local_assert_no_cpu_op():
449     m = rng.uniform(-1, 1, (10</b></font>, 10)).astype("float32")
450     ms = gpuarray_shared_constructor(m, name="m_shared")
451     out = theano.tensor<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.tanh(ms).dot(ms.T)
452     mode_local_assert = mode_with_gpu.including("assert_no_cpu_op")
453     mode_local_assert = mode_local_assert.excluding("local_gpua_elemwise")
454     old = theano.</b></font>config.assert_no_cpu_op
455     old2 = theano.config.on_opt_error
456     try:
457         theano.config.assert_no_cpu_op = 'raise'
458         theano.config.on_opt_error = 'ignore'
459         assert_raises(AssertionError, theano.function,
460                       [], out, mode=mode_local_assert)
461     finally:
462         theano.config.assert_no_cpu_op = old
463         theano.config.on_opt_error = old2
464     try:
465         theano.config.assert_no_cpu_op = 'ignore'
466         assert _check_stack_trace(f)
467     finally:
468         theano.config.assert_no_cpu_op =<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b> old
469 def test_no_complex():
470     width_var = tensor.cscalar()
471     freq_var = tensor.fscalar()
472     signal_var = tensor.fscalar()
473     stft_out = tensor.exp(width_var * freq_var) * signal_var
474     f = theano.function(</b></font>[width_var, freq_var, signal_var], stft_out,
475                         mode=mode_with_gpu)
476     assert _check_stack_trace(f)
477 @utt.assertFailure_fast
478     if not cusolver_available or not slinalg.imported_scipy:
479         raise SkipTest('No cuSolver or SciPy')
480     A <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= tensor.fmatrix()
481     b = tensor.fmatrix()
482     o = slinalg.solve(A, b)
483     f_cpu = theano.function([A, b], o, mode_without_gpu)
484     f_gpu = theano.function([A, b], o, mode=</b></font>mode_with_gpu)
485     assert not any(isinstance(n.op, slinalg.Solve)
486                    for n in f_gpu.maker.fgraph.apply_nodes)
487     assert any(isinstance(n.op, GpuCusolverSolve) and n.op.inplace
488                for n in f_gpu.maker.fgraph.apply_nodes)
489     A_val = np.random.uniform(-0.4, 0.4, (5, 5)).astype("float32")
490     b_val = np.random.uniform(-0.4, 0.4, (5, 3)).astype("float32")
491     utt.assert_allclose(f_cpu(A_val, b_val), f_gpu(A_val, b_val))
492     assert _check_stack_trace(f_gpu)
493     if not cusolver_available or not slinalg.imported_scipy:
494         raise SkipTest('No cuSolver or Scipy')
495     A <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= tensor.fmatrix()
496     b = tensor.fmatrix()
497     s = slinalg.solve(A, b)
498     o = tensor.dot(A, s)
499     f_cpu = theano.function([A, b], o, mode_without_gpu)
500     f_gpu = theano.function([A, b], o, mode=</b></font>mode_with_gpu)
501     count_not_inplace = len([n.op for n in f_gpu.maker.fgraph.apply_nodes
502                              if isinstance(n.op, GpuCusolverSolve) and not n.op.inplace])
503     assert count_not_inplace == 1, count_not_inplace
504     A_val = np.random.uniform(-0.4, 0.4, (5, 5)).astype("float32")
505     b_val = np.random.uniform(-0.4, 0.4, (5, 3)).astype("float32")
506     utt.assert_allclose(f_cpu(A_val, b_val), f_gpu(A_val, b_val))
507 @utt.assertFailure_fast
508 def test_local_lift_cholesky():
509     if not cusolver_available or not slinalg.imported_scipy:
510         raise SkipTest('No cuSolver or Scipy')
511     A = tensor.fmatrix()
512     o = slinalg.cholesky(A)
513     f_cpu = theano.function([A], o, mode=mode_without_gpu)
514     f_gpu = theano.function([A], o, mode=mode_with_gpu)
515     assert not any(isinstance(n.op, slinalg.Cholesky)
516                    for n in f_gpu.maker.fgraph.apply_nodes)
517     assert any(isinstance(n.op, GpuCholesky) and n.op.inplace
518                for n in f_gpu.maker.fgraph.apply_nodes)
519     M_val = np.random.normal(size=(3, 3)).astype("float32")
520     A_val = M_val.dot(M_val.T)
521     utt.assert_allclose(f_cpu(A_val), f_gpu(A_val))
522     if not cusolver_available or not slinalg.imported_scipy:
523         raise SkipTest('No cuSolver or SciPy')
524     A <font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= tensor.fmatrix()
525     A_squared = A**2
526     B = slinalg.cholesky(A_squared)
527     D = B + A_squared
528     f_cpu = theano.function([A], D, mode=mode_without_gpu)
529     f_gpu = theano.function([A], D, mode=mode_with_gpu)
530     count_cholesky_not_inplace =</b></font> len([n.op for n in f_gpu.maker.fgraph.apply_nodes
531                                       if isinstance(n.op, GpuCholesky) and not n.op.inplace])
532     assert count_cholesky_not_inplace == 1, count_cholesky_not_inplace
533     M_val = np.random.normal(size=(3, 3)).astype("float32")
534     A_val = M_val.dot(M_val.T)
535     utt.assert_allclose(f_cpu(A_val), f_gpu(A_val))
536 def test_local_gpua_advanced_incsubtensor():
537     target = tensor.ftensor4()
538     y = target.dimshuffle(1, 0, 2, 3).flatten(ndim=1)
539     w = tensor.ones_like(y)
540     w = tensor.set_subtensor(w[tensor.eq(y, 1.0).nonzero()], 100)
541     w = tensor.set_subtensor(w[tensor.eq(y, -1.0).nonzero()], 0)
542     f = theano.function([target], w)
543     assert _check_stack_trace(f)
544 def test_batched_dot_lifter():
545     rng = np.random.RandomState(utt.fetch_seed())
546     def randX(*args):
547         return rng.rand(*args).astype(theano.config.floatX)
548     cases = [
549         (randX(3, 5, 7), randX(3, 7)),
550         (randX(3, 5), randX(3, 5, 7)),
551         (randX(3, 5), randX(3, 5)),
552         (rng.rand(3, 5, 7).astype('float32'), randX(3, 7, 9)),
553         (rng.rand(3, 5, 7).astype('float64'), randX(3, 7, 9))]
554     for x_val, y_val in cases:
555         x = tensor.TensorType(broadcastable=[s == 1 for s in x_val.shape],
556                               dtype=x_val.dtype)('x')
557         y = tensor.TensorType(broadcastable=[s == 1 for s in y_val.shape],
558                               dtype=y_val.dtype)('y')
559         z = tensor.batched_dot(x, y)
560         f = theano.function([x, y], z, mode=mode_with_gpu)
561         f(x_val, y_val)
562         assert check_stack_trace(f, ops_to_check='all')
563 def test_crossentropycategorical1hot_lifter():
564     rng = np.random<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.RandomState(utt.fetch_seed())
565     x = tensor.matrix()
566     y = tensor.lvector()
567     z = tensor.</b></font>nnet.crossentropy_categorical_1hot(x, y)
568     gx = theano.grad(z.mean(), x)
569     f = theano.function([x, y], [z, gx], mode=mode_with_gpu)
570     assert not any(isinstance(n.op, (tensor.nnet.CrossentropyCategorical1Hot,
571                                      tensor.nnet.CrossentropyCategorical1HotGrad))
572                    for n in f.maker.fgraph.apply_nodes)
573     f(rng.uniform(0.1, 0.9, (13, 5)).astype(theano.config.floatX),
574       rng.randint(5, size=(13,)))
575 class Conv_opt_test(unittest.TestCase):
576     def optimizer_2d(self, input_shapes, direction, include_tags, exclude_tags,
577                      op, border_mode='valid', subsample=(1, 1),
578                      filter_dilation=(1, 1), num_groups=1, unshared=False,
579                      optimiser=None):
580         inp1 = theano.shared(np.random.random(input_shapes[0]).astype(theano.config.floatX))
581         inp2 = theano.shared(np.random.random(input_shapes[1]).astype(theano.config.floatX))
582         if op is None:
583             inp1 = basic_ops.as_gpuarray_variable(inp1, test_ctx_name)
584             inp2 = basic_ops.as_gpuarray_variable(inp2, test_ctx_name)
585         if(direction == 0):
586             conv_op = abstract_conv.AbstractConv2d(input_shapes[0],
587                                                    input_shapes[1],
588                                                    border_mode=border_mode,
589                                                    subsample=subsample,
590                                                    filter_dilation=filter_dilation,
591                                                    num_groups=num_groups,
592                                                    unshared=unshared)(inp1, inp2)
593         if(direction == 1):
594             conv_op = abstract_conv.AbstractConv2d_gradWeights(imshp=input_shapes[0],
595                                                                kshp=input_shapes[2],
596                                                                border_mode=border_mode,
597                                                                subsample=subsample,
598                                                                filter_dilation=filter_dilation,
599                                                                num_groups=num_groups,
600                                                                unshared=unshared)(inp1,
601                                                                                   inp2,
602                                                                                   input_shapes[2][-2:])
603         if(direction == 2):
604             conv_op = abstract_conv.AbstractConv2d_gradInputs(imshp=input_shapes[2],
605                                                               kshp=input_shapes[1],
606                                                               border_mode=border_mode,
607                                                               subsample=subsample,
608                                                               filter_dilation=filter_dilation,
609                                                               num_groups=num_groups,
610                                                               unshared=unshared)(inp2,
611                                                                                  inp1,
612                                                                                  input_shapes[2][-2:])
613         theano.config.metaopt.optimizer_including = include_tags
614         theano.config.metaopt.optimizer_excluding = exclude_tags
615         mode = mode_with_gpu.including('conv_meta').excluding('conv_dnn').excluding('conv_gemm')
616         if op is None:
617             assert optimiser.transform(conv_op.owner) is None
618         else:
619             ref_func = theano.function([], conv_op, mode=mode_with_gpu)
620             with theano.change_flags(mode=mode):
621                 conv_func = theano.function([], conv_op, mode=mode)
622             assert any([isinstance(node.op, op)
623                         for node in conv_func.maker.fgraph.toposort()])
624             utt.assert_allclose(conv_func(), ref_func())
625     def optimizer_3d(self, input_shapes, direction, include_tags, exclude_tags,
626                      op, border_mode='valid', subsample=(1, 1, 1),
627                      filter_dilation=(1, 1, 1), num_groups=1, optimiser=None):
628         inp1 = theano.shared(np.random.random(input_shapes[0]).astype(theano.config.floatX))
629         inp2 = theano.shared(np.random.random(input_shapes[1]).astype(theano.config.floatX))
630         if op is None:
631             inp1 = basic_ops.as_gpuarray_variable(inp1, None)
632             inp2 = basic_ops.as_gpuarray_variable(inp2, None)
633         if(direction == 0):
634             conv_op = abstract_conv.AbstractConv3d(input_shapes[0],
635                                                    input_shapes[1],
636                                                    border_mode=border_mode,
637                                                    subsample=subsample,
638                                                    filter_dilation=filter_dilation,
639                                                    num_groups=num_groups)(inp1, inp2)
640         if(direction == 1):
641             conv_op = abstract_conv.AbstractConv3d_gradWeights(input_shapes[0],
642                                                                input_shapes[2],
643                                                                border_mode=border_mode,
644                                                                subsample=subsample,
645                                                                filter_dilation=filter_dilation,
646                                                                num_groups=num_groups)(inp1,
647                                                                                       inp2,
648                                                                                       input_shapes[2][-3:])
649         if(direction == 2):
650             conv_op = abstract_conv.AbstractConv3d_gradInputs(input_shapes[2],
651                                                               input_shapes[1],
652                                                               border_mode=border_mode,
653                                                               subsample=subsample,
654                                                               filter_dilation=filter_dilation,
655                                                               num_groups=num_groups)(inp2,
656                                                                                      inp1,
657                                                                                      input_shapes[2][-3:])
658         theano.config.metaopt.optimizer_including = include_tags
659         theano.config.metaopt.optimizer_excluding = exclude_tags
660         mode = mode_with_gpu.including('conv_meta').excluding('conv_dnn').excluding('conv_gemm')
661         if op is None:
662             assert optimiser.transform(conv_op.owner) is None
663             return
664         elif op != 'conv3d2d':
665             with theano.change_flags(mode=mode):
666                 conv_func = theano.function([], conv_op, mode=mode)
667             assert any([isinstance(node.op, op)
668                        for node in conv_func.maker.fgraph.toposort()])
669         else:
670             with theano.change_flags(mode=mode):
671                 conv_func = theano.function(
672                     [], conv_op,
673                     mode=mode_with_gpu.including('conv_meta'))
674         ref_func = theano.function([], conv_op, mode=mode_with_gpu)
675         utt.assert_allclose(conv_func(), ref_func())
676     def test_optimizers_2d(self):
677         if theano.config.cxx == "":
678             raise SkipTest("Need a c compiler.")
679         imshp2d = [(2, 3, 5, 5), (2, 2, 5, 7), (2, 1, 3, 3)]
680         kshp2d = [(4, 3, 3, 3), (3, 2, 3, 5), (4, 1, 1, 1)]
681         tshp2d = [(2, 4, 3, 3), (2, 3, 3, 3), (2, 4, 3, 3)]
682         for imshp, kshp, tshp in zip(imshp2d, kshp2d, tshp2d):
683             self<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.optimizer_2d([imshp, kshp, tshp], 0,
684                               '',
685                               'conv_dnn:alternative',
686                               blas.GpuCorrMM)
687             self.optimizer_2d([imshp, kshp, tshp], 0,
688                               'alternative',
689                               'conv_dnn:default',
690                               blas.GpuCorrMM_gradWeights)
691             self.optimizer_2d([imshp, kshp, tshp], 0,
692                               '',
693                               'conv_gemm:alternative',
694                               dnn.GpuDnnConv)
695             self.optimizer_2d([imshp, kshp, tshp], 0,
696                               'alternative',
697                               'conv_gemm:default',
698                               dnn.GpuDnnConvGradW)
699             self.optimizer_2d([imshp, tshp, kshp], 1,
700                               '',
701                               'conv_dnn:alternative',
702                               blas.GpuCorrMM_gradWeights)
703             self.optimizer_2d([imshp, tshp, kshp], 1,
704                               'alternative',
705                               'conv_dnn:default',
706                               blas.GpuCorrMM)
707             self.optimizer_2d([imshp, tshp, kshp], 1,
708                               '',
709                               'conv_gemm:alternative',
710                               dnn.GpuDnnConvGradW)
711             self.optimizer_2d([imshp, tshp, kshp], 1,
712                               'alternative',
713                               'conv_gemm:default',
714                               dnn.GpuDnnConv)
715             self.optimizer_2d([tshp, kshp, imshp], 2,
716                               '',
717                               'conv_dnn:alternative',
718                               blas.GpuCorrMM_gradInputs)
719             self.optimizer_2d([tshp, kshp, imshp], 2,
720                               'alternative',
721                               'conv_dnn:default',
722                               blas.GpuCorrMM)
723             self.optimizer_2d([tshp, kshp, imshp], 2,
724                               '',
725                               'conv_gemm:alternative',
726                               dnn.GpuDnnConvGradI)
727             self.optimizer_2d([tshp, kshp, imshp], 2,
728                               'alternative',
729                               'conv_gemm:default',
730                               dnn.</b></font>GpuDnnConv)
731     def test_optimizers_3d(self):
732         if theano.config.cxx == "":
733             raise SkipTest("Need a c compiler.")
734         imshp3d = [(2, 3, 5, 5, 5), (2, 2, 5, 7, 5), (2, 1, 3, 3, 3)]
735         kshp3d = [(4, 3, 3, 3, 3), (3, 2, 3, 5, 3), (4, 1, 1, 1, 1)]
736         tshp3d = [(2, 4, 3, 3, 3), (2, 3, 3, 3, 3), (2, 4, 3, 3, 3)]
737         for imshp, kshp, tshp in zip(imshp3d, kshp3d, tshp3d):
738             self.optimizer_3d([imshp, kshp, tshp], 0,
739                               '',
740                               'conv_dnn:alternative:conv3d2d',
741                               blas.GpuCorr3dMM)
742             self.optimizer_3d([imshp, kshp, tshp], 0,
743                               'conv_dnn:default:conv3d2d',
744                               blas.GpuCorr3dMM_gradWeights)
745             self.optimizer_3d([<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>imshp, kshp, tshp], 0,
746                               'conv3d2d',
747                               'default',
748                               'conv3d2d')
749             self.optimizer_3d([imshp, kshp, tshp], 0,
750                               'alternative',
751                               'conv_gemm:default:conv3d2d',
752                               dnn.GpuDnnConvGradW)
753             self.optimizer_3d([imshp, kshp, tshp], 0,
754                               '',
755                               'conv_gemm:alternative:conv3d2d',
756                               dnn.GpuDnnConv)
757             self.optimizer_3d([imshp, tshp, kshp], 1,
758                               '',
759                               'conv_dnn:alternative',
760                               blas.GpuCorr3dMM_gradWeights)
761             self.optimizer_3d([imshp, tshp, kshp], 1,
762                               'alternative',
763                               'conv_dnn:default',
764                               blas.GpuCorr3dMM)
765             self.optimizer_3d([imshp, tshp, kshp], 1,
766                               'alternative',
767                               'conv_gemm:default',
768                               dnn.GpuDnnConv)
769             self.optimizer_3d([imshp, tshp, kshp], 1,
770                               '',
771                               'conv_gemm:alternative',
772                               dnn.GpuDnnConvGradW)
773             self.optimizer_3d([tshp, kshp, imshp], 2,
774                               '',
775                               'conv_dnn:alternative',
776                               blas.GpuCorr3dMM_gradInputs)
777             self.optimizer_3d([tshp, kshp, imshp], 2,
778                               'alternative',
779                               'conv_dnn:default',
780                               blas.GpuCorr3dMM)
781             self.optimizer_3d([tshp, kshp, imshp], 2,
782                               'alternative',
783                               'conv_gemm:default',
784                               dnn.GpuDnnConv)
785             self.optimizer_3d([tshp, kshp, imshp], 2,
786                               '',
787                               'conv_gemm:alternative',
788                               dnn.</b></font>GpuDnnConvGradI)
789     def test_optimizers_non_default(self):
790         if theano.config.cxx == "":
791             raise SkipTest("Need a c compiler.")
792         imshp2d = [(2, 3, 5, 5), (4, 2, 5, 5)]
793         kshp2d = [(4, 3, 3, 3), (3, 2, 3, 3)]
794         filter_dilation = [(1, 1), (2, 2)]
795         for imshp, kshp, fdil in zip(imshp2d, kshp2d, filter_dilation):
796             self.optimizer_2d([imshp, kshp], 0,
797                               '',
798                               'conv_dnn:alternative',
799                               blas.GpuCorrMM,
800                               border_mode='full',
801                               filter_dilation=fdil)
802             self.optimizer_2d([imshp, kshp], 0,
803                               'alternative',
804                               'conv_dnn:default',
805                               blas.GpuCorrMM_gradInputs,
806                               border_mode='full',
807                               filter_dilation=fdil)
808             self.optimizer_2d([imshp, kshp], 0,
809                               '',
810                               'conv_gemm:alternative',
811                               dnn.GpuDnnConv,
812                               border_mode='full',
813                               filter_dilation=fdil)
814             self.optimizer_2d([imshp, kshp], 0,
815                               'alternative',
816                               'conv_gemm:default',
817                               dnn.GpuDnnConvGradI,
818                               border_mode='full',
819                               filter_dilation=fdil)
820         imshp3d = [(2, 3, 5, 5, 5), (4, 2, 5, 5, 5)]
821         kshp3d = [(4, 3, 3, 3, 3), (3, 2, 3, 3, 3)]
822         filter_dilation = [(1, 1, 1), (2, 2, 2)]
823         for imshp, kshp, fdil in zip(imshp3d, kshp3d, filter_dilation):
824             self.optimizer_3d([imshp, kshp], 0,
825                               '',
826                               'conv_dnn:alternative:conv3d2d',
827                               blas.GpuCorr3dMM,
828                               border_mode='full',
829                               filter_dilation=fdil)
830             self.optimizer_3d([imshp, kshp], 0,
831                               'alternative',
832                               'conv_dnn:default:conv3d2d',
833                               blas.GpuCorr3dMM_gradInputs,
834                               border_mode='full',
835                               filter_dilation=fdil)
836             self.optimizer_3d([imshp, kshp], 0,
837                               '',
838                               'conv_gemm:alternative:conv3d2d',
839                               dnn.GpuDnnConv,
840                               border_mode='full',
841                               filter_dilation=fdil)
842             self.optimizer_3d([imshp, kshp], 0,
843                               'alternative',
844                               'conv_gemm:default:conv3d2d',
845                               dnn.GpuDnnConvGradI,
846                               border_mode='full',
847                               filter_dilation=fdil)
848         imshp2d = [(2, 6, 5, 5), (2, 4, 5, 5)]
849         kshp2d = [(3, 2, 3, 3), (2, 2, 3, 3)]
850         tshp2d = [(2, 3, 3, 3), (2, 2, 3, 3)]
851         num_groups = [3, 2]
852         for imshp, kshp, tshp, groups in zip(imshp2d, kshp2d, tshp2d, num_groups):
853             self.optimizer_2d([imshp, kshp, tshp], 0,
854                               'conv_dnn:alternative',
855                               blas.GpuCorrMM,
856                               num_groups<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=groups)
857             self.optimizer_2d([imshp, kshp, tshp], 0,
858                               '',
859                               'conv_gemm:alternative',
860                               dnn.GpuDnnConv,
861                               num_groups=groups)
862             self.optimizer_2d([imshp, tshp, kshp], 1,
863                               '',
864                               'conv_dnn:alternative',
865                               blas.GpuCorrMM_gradWeights,
866                               num_groups=groups)
867             self.optimizer_2d([imshp, tshp, kshp], 1,
868                               '',
869                               'conv_gemm:alternative',
870                               dnn.</b></font>GpuDnnConvGradW,
871                               num_groups=groups)
872             self.optimizer_2d([tshp, kshp, imshp], 2,
873                               '',
874                               'conv_dnn:alternative',
875                               blas.GpuCorrMM_gradInputs,
876                               num_groups=groups)
877             self.optimizer_2d([tshp, kshp, imshp], 2,
878                               '',
879                               'conv_gemm:alternative',
880                               dnn.GpuDnnConvGradI,
881                               num_groups=groups)
882         imshp2d = [(2, 2, 4, 4), (3, 2, 5, 3)]
883         kshp2d = [(2, 2, 2, 2, 3, 3), (2, 3, 1, 2, 3, 3)]
884         tshp2d = [(2, 2, 2, 2), (3, 2, 3, 1)]
885         for imshp, kshp, tshp, groups in zip(imshp2d, kshp2d, tshp2d, num_groups):
886             self.optimizer_2d([imshp, kshp, tshp], 0,
887                               '',
888                               'alternative',
889                               blas.GpuCorrMM,
890                               unshared=True)
891             self.optimizer_2d([imshp, tshp, kshp], 1,
892                               '',
893                               'alternative',
894                               blas.GpuCorrMM_gradWeights,
895                               unshared=True)
896             self.optimizer_2d([tshp, kshp, imshp], 2,
897                               '',
898                               'alternative',
899                               blas.GpuCorrMM_gradInputs,
900                               unshared=True)
901         imshp3d = [(2, 6, 5, 5, 5), (2, 4, 5, 5, 5)]
902         kshp3d = [(3, 2, 3, 3, 3), (2, 2, 3, 3, 3)]
903         tshp3d = [(2, 3, 3, 3, 3), (2, 2, 3, 3, 3)]
904         num_groups = [3, 2]
905         for imshp, kshp, tshp, groups in zip(imshp3d, kshp3d, tshp3d, num_groups):
906             self.optimizer_3d([imshp, kshp, tshp], 0,
907                               '',
908                               'conv_dnn:alternative:conv3d2d',
909                               blas.GpuCorr3dMM,
910                               num_groups=groups)
911             self.optimizer_3d([imshp, kshp, tshp], 0,
912                               '',
913                               'conv_gemm:alternative:conv3d2d',
914                               dnn.GpuDnnConv,
915                               num_groups=groups)
916             self.optimizer_3d([imshp, tshp, kshp], 1,
917                               '',
918                               'conv_dnn:alternative:conv3d2d',
919                               blas.GpuCorr3dMM_gradWeights,
920                               num_groups=groups)
921             self.optimizer_3d([imshp, tshp, kshp], 1,
922                               '',
923                               'conv_gemm:alternative:conv3d2d',
924                               dnn.GpuDnnConvGradW,
925                               num_groups=groups)
926             self.optimizer_3d([tshp, kshp, imshp], 2,
927                               '',
928                               'conv_dnn:alternative:conv3d2d',
929                               blas.GpuCorr3dMM_gradInputs,
930                               num_groups=groups)
931             self.optimizer_3d([tshp, kshp, imshp], 2,
932                               '',
933                               'conv_gemm:alternative:conv3d2d',
934                               dnn.GpuDnnConvGradI,
935                               num_groups=groups)
936     def test_returns_none_2d(self):
937         if theano.config.cxx == "":
938             raise SkipTest("Need a c compiler.")
939         imshp = (2, 3, 5, 5)
940         kshp = (4, 3, 3, 3)
941         tshp = (2, 4, 3, 3)
942         conv_direction = [0, 1, 2]
943         optimisers = [[opt.local_abstractconv_gemm_alt,
944                        opt.local_abstractconv_cudnn_alt],
945                       [opt.local_abstractconv_gemm_gradweights_alt,
946                        opt.local_abstractconv_cudnn_alt],
947                       [opt.local_abstractconv_gradinputs_gemm_alt,
948                        opt.local_abstractconv_cudnn_alt]]
949         for opt_direction, direction in zip(optimisers, conv_direction):
950             for optimiser in opt_direction:
951                 self.optimizer_2d([imshp, kshp, tshp],
952                                   direction,
953                                   '',
954                                   '',
955                                   None,
956                                   subsample=(2, 2),
957                                   optimiser=optimiser)
958         for opt_direction, direction in zip(optimisers, conv_direction):
959             for optimiser in opt_direction:
960                 self.optimizer_2d([imshp, kshp, tshp],
961                                   direction,
962                                   '',
963                                   '',
964                                   None,
965                                   num_groups=3,
966                                   optimiser=optimiser)
967         for opt_direction, direction in zip(optimisers, conv_direction):
968             for optimiser in opt_direction:
969                 self.optimizer_2d([imshp, kshp, tshp],
970                                   direction,
971                                   '',
972                                   '',
973                                   None,
974                                   border_mode='half',
975                                   optimiser=optimiser)
976         for optimiser in optimisers[1]:
977             self.optimizer_2d([imshp, kshp, tshp],
978                               1,
979                               '',
980                               '',
981                               None,
982                               filter_dilation=(2, 2),
983                               optimiser=optimiser)
984         imshp = (2, 2, 4, 4)
985         kshp = (2, 2, 2, 2, 3, 3)
986         tshp = (2, 2, 2, 2)
987         shape_perms = [[imshp, kshp, tshp],
988                        [imshp, tshp, kshp],
989                        [tshp, kshp, imshp]]
990         for opt_direction, direction, perms in zip(optimisers, conv_direction,
991                                                    shape_perms):
992             for optimiser in opt_direction:
993                 self.optimizer_2d(perms,
994                                   direction,
995                                   '',
996                                   '',
997                                   None,
998                                   unshared=True,
999                                   optimiser=optimiser)
1000     def test_returns_none_3d(self):
1001         if theano.config.cxx == "":
1002             raise SkipTest("Need a c compiler.")
1003         imshp = (2, 3, 5, 5, 5)
1004         kshp = (4, 3, 3, 3, 3)
1005         tshp = (2, 4, 3, 3, 3)
1006         conv_direction = [0, 1, 2]
1007         optimisers = [[opt.local_abstractconv3d_alt,
1008                        opt.local_abstractconv3d_cudnn_alt],
1009                       [opt.local_abstractconv3d_gemm_gradweights_alt,
1010                        opt.local_abstractconv3d_cudnn_alt],
1011                       [opt.local_abstractconv3d_gradinputs_gemm_alt,
1012                        opt.local_abstractconv3d_cudnn_alt]]
1013         for opt_direction, direction in zip(optimisers, conv_direction):
1014             for optimiser in opt_direction:
1015                 self.optimizer_3d([imshp, kshp, tshp],
1016                                   direction,
1017                                   '',
1018                                   '',
1019                                   None,
1020                                   subsample=(2, 2, 2),
1021                                   optimiser=optimiser)
1022         for opt_direction, direction in zip(optimisers, conv_direction):
1023             for optimiser in opt_direction:
1024                 self.optimizer_3d([imshp, kshp, tshp],
1025                                   direction,
1026                                   '',
1027                                   '',
1028                                   None,
1029                                   num_groups=3,
1030                                   optimiser=optimiser)
1031         for opt_direction, direction in zip(optimisers, conv_direction):
1032             for optimiser in opt_direction:
1033                 self.optimizer_3d([imshp, kshp, tshp],
1034                                   direction,
1035                                   '',
1036                                   '',
1037                                   None,
1038                                   border_mode='half',
1039                                   optimiser=optimiser)
1040         for optimiser in optimisers[1]:
1041             self.optimizer_3d([imshp, kshp, tshp],
1042                               1,
1043                               '',
1044                               '',
1045                               None,
1046                               filter_dilation=(2, 2, 2),
1047                               optimiser=optimiser)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_blas_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from copy import copy
2 from itertools import product as itertools_product
3 from unittest import TestCase
4 import numpy as np
5 from numpy import (arange, array, common_type, complex64, complex128, float32,
6                    float64, newaxis, shape, transpose, zeros)
7 from numpy.testing import assert_array_almost_equal
8 from itertools import product
9 from six.moves import xrange
10 import theano
11 import theano.tensor as T
12 from theano import tensor, In, shared, config
13 from theano.compat import exc_message
14 from theano.printing import pp
15 from theano.tensor.blas import (_dot22, _dot22scalar, res_is_a, _as_scalar,
16                                 _is_real_matrix, _gemm_canonicalize,
17                                 _factor_canonicalized, Gemm, Gemv,
18                                 gemm_inplace, gemm_no_inplace,
19                                 InconsistencyError, Ger, ger, ger_destructive)
20 from theano.tests import unittest_tools
21 from .test_basic import (as_tensor_variable, inplace_func,
22                          compile, inplace)
23 import theano.tensor.blas_scipy
24 from</b></font> theano.tests.unittest_tools import attr
25 if config.mode == 'FAST_COMPILE':
26     mode_not_fast_compile = 'FAST_RUN'
27 else:
28     mode_not_fast_compile = config.mode
29 mode_blas_opt = theano.compile.get_default_mode().including(
30     'BlasOpt', 'specialize', 'InplaceBlasOpt')
31 mode_blas_opt = mode_blas_opt.excluding('c_blas')
32 def test_dot_eq():
33     assert T.Dot() == T.Dot()
34 def sharedX(x, name):
35     return theano.shared(np.asarray(x, config.floatX), name=name)
36 class t_gemm(TestCase):
37     def setUp(self):
38         unittest_tools.seed_rng()
39         Gemm.debug = False
40     @staticmethod
41     def _gemm(z, a, x, y, b):
42         assert a.shape == ()
43         assert b.shape == ()
44         return b * z + a * np.dot(x, y)
45     @staticmethod
46     def rand(*args):
47         return np.random.rand(*args)
48     def cmp(self, z_, a_, x_, y_, b_):
49         for dtype in ['float32', 'float64', 'complex64', 'complex128']:
50             z <font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.asarray(z_, dtype=dtype)
51             a = np.asarray(a_, dtype=dtype)
52             x = np.asarray(x_, dtype=dtype)
53             y = np.asarray(y_, dtype=dtype)
54             def cmp_linker(z, a, x, y, b, l):
55                 z, a, x, y, b <font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [np.asarray(p) for p in (z, a, x, y, b)]
56                 z_orig = z.copy()
57                 tz, ta, tx, ty, tb = [as_tensor_variable(p).type(</b></font>)
58                                       for p in (z, a, x, y, b)]
59                 f = inplace_func([tz, ta, tx, ty, tb],
60                                  gemm_inplace(tz, ta, tx, ty, tb),
61                                  mode=compile.Mode(optimizer=None, linker=l))
62                 f(z, a, x, y, b)
63                 z_after = self._gemm(z_orig, a, x, y, b)
64                 unittest_tools.assert_allclose(z_after, z)
65                 if a == 0.0 and b == 1.0:
66                     return
67                 elif z_orig.size == 0:
68                     self.assertTrue(z.size == 0)
69                 else:
70                     self.assertFalse(np.all(z_orig == z))
71             cmp_linker(copy(z), a, x, y, b, 'c|py')
72             cmp_linker(copy(z), a, x, y, b, 'py')
73             if (not dtype.startswith("complex") and theano.config.cxx):
74                 cmp_linker(copy(z), a, x, y, b, 'c')
75     def test0a(self):
76         Gemm.debug = True
77         try:
78             gemm_no_inplace([1.], 1., [1.], [1.], 1.)
79         except TypeError as e:
80             if exc_message(e) is Gemm.E_rank:
81                 return
82         self.fail()
83     def test0(self):
84         try:
85             self.cmp(1., 0., 1.0, 1.0, 1.0)
86         except TypeError as e:
87             if exc_message(e) is Gemm.E_rank:
88                 return
89         self.fail()
90     def test2(self):
91         try:
92             self.cmp(2., 1.0, [3, 2, 1.], [[1], [2], [3.]], 1.0)
93         except TypeError as e:
94             self.assertTrue(exc_message(e) == Gemm.E_rank)
95             return
96         self.fail()
97     def test4(self):
98         self.cmp(self.rand(3, 4), 1.0, self.rand(3, 5), self.rand(5, 4), 0.0)
99     def test5(self):
100         self.cmp(self.rand(3, 4), 1.0,
101                  self.rand(3, 5), self.rand(5, 4), 1.0)
102     def test6(self):
103         self.cmp(self.rand(3, 4), 1.0,
104                  self.rand(3, 5), self.rand(5, 4), -1.0)
105     def test7(self):
106         self.cmp(self.rand(3, 4), 0.0,
107                  self.rand(3, 5), self.rand(5, 4), 0.0)
108     def test8(self):
109         self.cmp(self.rand(3, 4), 0.0,
110                  self.rand(3, 5), self.rand(5, 4), 0.6)
111     def test9(self):
112         self.cmp(self.rand(3, 4), 0.0,
113                  self.rand(3, 5), self.rand(5, 4), -1.0)
114     def test10(self):
115         self.cmp(self.rand(3, 4), -1.0, self.rand(3, 5), self.rand(5, 4), 0.0)
116     def test11(self):
117         self.cmp(self.rand(3, 4), -1.0,
118                  self.rand(3, 5), self.rand(5, 4), 1.0)
119     def test12(self):
120         self.cmp(self.rand(3, 4), -1.0,
121                  self.rand(3, 5), self.rand(5, 4), -1.0)
122     def test_shape_0(self):
123         self.cmp(self.rand(0, 4), -1.0, self.rand(0, 5), self.rand(5, 4), -1.0)
124         self.cmp(self.rand(3, 4), -1.0, self.rand(3, 0), self.rand(0, 4), -1.0)
125         self.cmp(self.rand(0, 0), -1.0, self.rand(0, 5), self.rand(5, 0), -1.0)
126         self.cmp(self.rand(0, 0), -1.0, self.rand(0, 0), self.rand(<font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 0), -1.0)
127     def test_factorised_scalar(self):
128         a = T.matrix()
129         b = T.matrix()
130         s = theano.shared(np.</b></font>zeros((5, 5)).astype(config.floatX))
131         lr1 = T.constant(0.01).astype(config.floatX)
132         lr2 = T.constant(2).astype(config.floatX)
133         l2_reg = T.constant(0.0001).astype(config.floatX)
134         f = theano.function([a, b], updates=[(s, lr1 * T.dot(a, b) +
135                             l2_reg * lr2 * s)],
136                             mode=mode_not_fast_compile).maker.fgraph.toposort()
137         assert len(f) == 1
138         assert f[0].op == gemm_inplace
139         f = theano.function([a, b], updates=[(s, lr1 * (T.dot(a, b) -
140                                                         l2_reg * s))],
141                             mode=mode_not_fast_compile).maker.fgraph.toposort()
142         assert len(f) == 1
143         assert f[0].op == gemm_inplace
144         f = theano.function([a, b],
145                             updates=[(s, s - lr1 * (s * .0002 + T.dot(a, b)))],
146                             mode=mode_not_fast_compile).maker.fgraph.toposort()
147         assert len(f) == 1
148         assert f[0].op == gemm_inplace
149     def test_destroy_map0(self):
150         Z = as_tensor_variable(self.rand(2, 2))
151         try:
152             gemm_inplace(Z, 1.0, Z, Z, 1.0)
153         except InconsistencyError as e:
154             if exc_message(e) == Gemm.E_z_uniq:
155                 return
156         self.fail()
157     def test_destroy_map1(self):
158         Z = as_tensor_variable(self.rand(2, 2))
159         A = as_tensor_variable(self.rand(2, 2))
160         try:
161             gemm_inplace(Z, 1.0, A, inplace.transpose_inplace(Z), 1.0)
162         except InconsistencyError as e:
163             if exc_message(e) == Gemm.E_z_uniq:
164                 return
165         self.fail()
166     def test_destroy_map2(self):
167         Z = as_tensor_variable(self.rand(2, 2))
168         A = as_tensor_variable(self.rand(2, 2))
169         try:
170             gemm_inplace(Z, 1.0, inplace.transpose_inplace(Z), A, 1.0)
171         except InconsistencyError as e:
172             if exc_message(e) == Gemm.E_z_uniq:
173                 return
174         self.fail()
175     def test_destroy_map3(self):
176         Z = as_tensor_variable(self.rand(2, 2))
177         A = as_tensor_variable(self.rand(2, 2))
178         try:
179             gemm_inplace(Z, 1.0, Z, A, 1.0)
180         except InconsistencyError as e:
181             if exc_message(e) == Gemm.E_z_uniq:
182                 return
183         self.fail()
184     def test_destroy_map4(self):
185         Z = shared(self.rand(2, 2), name='Z')
186         A = shared(self.rand(2, 2), name='A')
187         one = T.constant(1.0).astype(Z.dtype)
188         f = inplace_func([], gemm_inplace(Z, one, A, A, one))
189         f()
190         f = inplace_func([], gemm_inplace(Z, one, A, A.T, one))
191         f()
192     def test_transposes(self):
193         A = self.rand(4, 5)[:, :4]
194         B = self.rand(4, 5)[:, :4]
195         C = self.rand(4, 5)[:, :4]
196         def t(z, x, y, a=1.0, b=0.0, l='c|py', dt='float64'):
197             z, a, x, y, b = [theano._asarray(p, dtype=dt)
198                              for p in (z, a, x, y, b)]
199             z_after = self._gemm(z, a, x, y, b)
200             tz, ta, tx, ty, tb = [shared(p) for p in (z, a, x, y, b)]
201             f = inplace_func([], gemm_inplace(tz, ta, tx, ty, tb),
202                              mode=compile.Mode(optimizer=None, linker=l))
203             f()
204             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
205             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True))
206             f()
207             unittest_tools<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.assert_allclose(z_after, tz.get_value(borrow=True))
208             y_T = ty.get_value(borrow=True).T
209             ty.set_value(tx.get_value(borrow=</b></font>True).T, borrow=True)
210             tx.set_value(y_T, borrow=True)
211             f()
212             unittest_tools.assert_allclose(z_after, tz.get_value(borrow=True).T)
213         t(C, A, B)
214         t(C.T, A, B)
215         t(C, A.T, B, dt='float32')
216         t(C, A, B.T)
217         t(C.T, A.T, B)
218         t(C, A.T, B.T, dt='float32')
219         t(C.T, A, B.T)
220         t(C.T, A.T, B.T, dt='float32')
221         t(C, A[:, :2], B[:2, :])
222         t(C.T, A[:, :2], B[:2, :], dt='float32')
223         t(C, A[:2, :].T, B[:2, :])
224         t(C.T, A[:2, :].T, B[:2, :], dt='float32')
225         t(C, A[:2, :].T, B[:, :2].T)
226         t(C.T, A[:2, :].T, B[:, :2].T)
227         try:
228             t(C.T, A[:2, :], B[:, :2].T)
229         except ValueError as e:
230             if exc_message(e).find('aligned') &gt;= 0:
231                 return
232         self.fail()
233     def test_non_contiguous(self):
234         A = self.rand(4, 4, 3)
235         B = self.rand(4, 4, 3)
236         C = self.rand(4, 4, 3)
237         def t(z, x, y, a=1.0, b=0.0, l='c|py', dt='float64'):
238             z, a, x, y, b = [theano._asarray(p, dtype=dt)
239                              for p in (z, a, x, y, b)]
240             z_orig = z.copy()
241             z_after = np.zeros_like(z_orig)
242             for i in xrange(3):
243                 z_after[:, :, i] = self._gemm(z[:, :, i], a,
244                                               x[:, :, i], y[:, :, i], b)
245             tz, ta, tx, ty, tb = [shared(p) for p in (z, a, x, y, b)]
246             for i in xrange(3):
247                 f_i = inplace_func([],
248                                    gemm_inplace(tz[:, :, i],
249                                    ta, tx[:, :, i], ty[:, :, i], tb),
250                                    mode=compile.Mode(optimizer=None, linker=l))
251                 for j in xrange(3):
252                     z_i = f_i()
253                     z = tz.get_value(borrow=True, return_internal_type=True)
254                     z[:, :, i] = z_i
255                     unittest_tools.assert_allclose(z_after[:, :, i],
256                                                    tz.get_value(borrow=True)[:, :, i])
257                 tz_i = gemm_no_inplace(tz[:, :, i], ta, tx[
258                     :, :, i], ty[:, :, i], tb)
259                 g_i = theano.function(
260                     [], tz_i, updates=[(tz, T.set_subtensor(tz[:, :, i],
261                                                             tz_i))],
262                     mode=compile.Mode(optimizer=None, linker=l))
263                 for j in xrange(3):
264                     g_i()
265                     unittest_tools.assert_allclose(z_after[:, :, i],
266                                                    tz.get_value(borrow=True)[:, :, i])
267         t(C, A, B)
268         t(C.transpose((1, 0, 2)), A, B)
269         t(C, A.transpose((1, 0, 2)), B, dt='float32')
270         t(C, A, B.transpose((1, 0, 2)))
271         t(C.transpose((1, 0, 2)), A.transpose((1, 0, 2)), B)
272         t(C, A.transpose((1, 0, 2)), B.transpose((1, 0, 2)), dt='float32')
273         t(C.transpose((1, 0, 2)), A, B.transpose((1, 0, 2)))
274         t(C.transpose((1, 0, 2)), A.transpose((1, 0, 2)), B.transpose((
275             1, 0, 2)), dt='float32')
276 class TestGemmNoFlags(object):
277     gemm = gemm_no_inplace
278     M = 4
279     N = 5
280     K = 6
281     slice_step = 3
282     def setUp(self):
283         unittest_tools.seed_rng()
284     def get_variable(self, V, to_transpose, to_slice):
285         if to_transpose:
286             V = V.T
287         if to_slice:
288             V = V[::self.slice_step]
289         return V
290     def get_function(self, dtype,
291                      transpose_A=False, transpose_B=False, transpose_C=False,
292                      slice_A=False, slice_B=False, slice_C=False):
293         alpha = theano.tensor.scalar(dtype=dtype, name='alpha')
294         beta = theano.tensor.scalar(dtype=dtype, name='beta')
295         A = theano.tensor.matrix(dtype=dtype, name='A')
296         B = theano.tensor.matrix(dtype=dtype, name='B')
297         C = theano.tensor.matrix(dtype=dtype, name='C')
298         A1 = self.get_variable(A, transpose_A, slice_A)
299         B1 = self.get_variable(B, transpose_B, slice_B)
300         C1 = self.get_variable(C, transpose_C, slice_C)
301         return theano.function([alpha, A, B, beta, C], self.gemm(C1, alpha, A1, B1, beta))
302     def generate_value(self, dtype, width, height, to_transpose, to_slice):
303         if to_slice:
304             if to_transpose:
305                 shape = (height, width * self.slice_step)
306             else:
307                 shape = (width * self.slice_step, height)
308         else:
309             if to_transpose:
310                 shape = (height, width)
311             else:
312                 shape = (width, height)
313         return np.random.random(shape).astype(dtype)
314                  transpose_A=False, transpose_B=False, transpose_C=False,
315                  slice_A=False, slice_B=False, slice_C=False):
316         A <font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= self.generate_value(dtype, self.M, self.N, transpose_A, slice_A)
317         B = self.generate_value(dtype, self.N, self.K, transpose_B, slice_B)
318         C = self.generate_value(dtype, self.M, self.</b></font>K, transpose_C, slice_C)
319         return (alpha, A, B, beta, C)
320     def get_value(self, V, to_transpose, to_slice):
321         if to_transpose:
322             V = V.T
323         if to_slice:
324             V = V[::self.slice_step]
325         return V
326     def compute_ref(self, alpha, A, B, beta, C,
327                     transpose_A, transpose_B, transpose_C,
328                     slice_A, slice_B, slice_C):
329         A = self.get_value(A, transpose_A, slice_A)
330         B = self.get_value(B, transpose_B, slice_B)
331         C = self.get_value(C, transpose_C, slice_C)
332         return alpha * np.dot(A, B) + beta * C
333     @theano.change_flags({'blas.ldflags': ''})
334     def run_gemm(self, dtype, ALPHA, BETA,
335                  transpose_A, transpose_B, transpose_C,
336                  slice_A, slice_B, slice_C):
337         f = self.get_function(dtype, transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)
338         values = self.get_data(dtype, ALPHA, BETA, transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)
339         assert any(isinstance(node.op, Gemm) for node in f.maker.fgraph.apply_nodes)
340         z_val = f(*values)
341         assert z_val.dtype == dtype
342         assert tuple(z_val.shape) == (self.M, self.K)
343         ref_val = self.compute_ref(*(values + (transpose_A, transpose_B, transpose_C, slice_A, slice_B, slice_C)))
344         unittest_tools.assert_allclose(ref_val, z_val)
345     def test_gemm(self):
346         dtypes = ('float32', 'float64')
347         scalars = (0, 1, -2)
348         booleans = (False, True)
349         iterables = [dtypes] + ([scalars] * 2) + ([booleans] * 6)
350         for dtype, alpha, beta, tA, tB, tC, sA, sB, sC in product(*iterables):
351             yield (self.run_gemm, dtype, alpha, beta, tA, tB, tC, sA, sB, sC)
352 def test_res_is_a():
353     X, Y, Z, a, b = XYZab()
354     assert not res_is_a(a, T.sqrt)
355     assert not res_is_a(a + a, T.sqrt)
356     assert res_is_a(T.sqrt(a + a), T.sqrt)
357 class t_as_scalar(TestCase):
358     def test0(self):
359         a = T.constant(2.5)
360         b = T.constant(np.asarray([[[0.5]]]))
361         b2 = b.dimshuffle()
362         assert b2.ndim == 0
363         d_a = T.DimShuffle([], [])(a)
364         d_b = T.DimShuffle([True, True, True], [0, 2, 1])(b)
365         d_a2 = T.DimShuffle([], ['x', 'x', 'x'])(a)
366         self.assertTrue(_as_scalar(a) == a)
367         self.assertTrue(_as_scalar(b) != b)
368         self.assertTrue(_as_scalar(d_a) != d_a)
369         self.assertTrue(_as_scalar(d_b) != d_b)
370         self.assertTrue(_as_scalar(d_a2) != d_a2)
371     def test1(self):
372         a = T.constant(np.ones(5))
373         self.assertTrue(_as_scalar(a) is None)
374         self.assertTrue(_as_scalar(T.DimShuffle([False], [0, 'x'])(a)) is None)
375     def test2(self):
376         a = T.dscalar()
377         d_a = T.DimShuffle([], [])(a)
378         d_a2 = T.DimShuffle([], ['x', 'x'])(a)
379         self.assertTrue(_as_scalar(a) is a)
380         self.assertTrue(_as_scalar(d_a) is a)
381         self.assertTrue(_as_scalar(d_a2) is a)
382     def test3(self):
383         a = T.matrix()
384         self.assertTrue(_as_scalar(a) is None)
385         self.assertTrue(_as_scalar(T.DimShuffle([False, False],
386                                                 [0, 'x', 1])(a)) is None)
387 class T_real_matrix(TestCase):
388     def test0(self):
389         self.assertTrue(_is_real_matrix(T.DimShuffle([False, False],
390                                                      [1, 0])(T.matrix())))
391         self.assertTrue(not _is_real_matrix(T.DimShuffle([False],
392                                                          ['x', 0])
393                                             (T.dvector())))
394 def fail(msg):
395     print('FAIL', msg)
396     assert False
397 def XYZab():
398     return T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
399 class Failure(Exception):
400     pass
401 def just_gemm(i, o, ishapes=[(4, 3), (3, 5), (4, 5), (), ()],
402               max_graphlen=0, expected_nb_gemm=1):
403     try:
404         f = inplace_func(
405             [In(ii, mutable=True, allow_downcast=True) for ii in i],
406             o,
407             mode='FAST_RUN',
408             on_unused_input='ignore')
409         nb_gemm = 0
410         for node in f.maker.fgraph.apply_nodes:
411             if isinstance(node.op, T.Dot):
412                 raise Failure('dot not changed to gemm_inplace in graph')
413             if node.op == _dot22:
414                 raise Failure('_dot22 not changed to gemm_inplace in graph')
415             if node.op == gemm_inplace:
416                 nb_gemm += 1
417         assert nb_gemm == expected_nb_gemm, (nb_gemm, expected_nb_gemm)
418         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
419                          allow_input_downcast=True, on_unused_input='ignore')
420         for node in g.maker.fgraph.apply_nodes:
421             if node.op == gemm_inplace:
422                 raise Exception('gemm_inplace in original graph')
423         graphlen = len(f.maker.fgraph.toposort())
424         if max_graphlen and (graphlen &lt;= max_graphlen):
425         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
426         r0 = f<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(*[np.asarray(rng.randn(*sh), config.floatX)
427                  for sh in ishapes])
428         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
429         r1 =</b></font> g(*[np.asarray(rng.randn(*sh), config.floatX)
430                  for sh in ishapes])
431         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
432         eps = 1.0e-8
433         if config.floatX == 'float32':
434             eps = 1.0e-6
435         if max_abs_err &gt; eps:
436             raise Failure('GEMM is computing the wrong output. max_rel_err =',
437                           max_abs_err)
438     except Failure:
439         for node in f.maker.fgraph.toposort():
440             print('GRAPH', node)
441         raise
442 @unittest_tools.assertFailure_fast
443 def test_gemm_opt0():
444     X, Y, Z, a, b = XYZab()
445     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a + Z * b])
446     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) + b * Z])
447     just_gemm([X, Y, Z, a, b], [b * Z + a * T.dot(X, Y)])
448     just_gemm([X, Y, Z, a, b], [T.dot(X, Y) * a - Z * b])
449     just_gemm([X, Y, Z, a, b], [a * T.dot(X, Y) - b * Z])
450     just_gemm([X, Y, Z, a, b], [b * Z - a * T.dot(X, Y)])
451     just_gemm([X, Y, Z, a, b], [b * Z.T - a * T.dot(Y.T, X.T)])
452     just_gemm([X, Y, Z, a, b], [b * Z.T + a * b * T.dot(X, Y).T])
453     just_gemm([X, Y, Z, a, b], [b * Z + a * T.dot(X, Y).T],
454               ishapes=[(5, 3), (3, 4), (4, 5), (), ()])
455     just_gemm([X, Y, Z, a, b], [(b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
456     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y)])
457     just_gemm([X, Y, Z, a, b], [Z * b + T.dot(X, Y)])
458     just_gemm([X, Y, Z, a, b], [Z + a * b * a * T.dot(X, Y)])
459     just_gemm([X, Y, Z, a, b], [(b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
460     just_gemm([X, Y, Z, a, b], [Z - T.dot(X, Y)])
461     just_gemm([X, Y, Z, a, b], [Z * b - T.dot(X, Y)])
462     just_gemm([X, Y, Z, a, b], [Z - a * b * a * T.dot(X, Y)])
463 @unittest_tools.assertFailure_fast
464 def test_gemm_opt_double_gemm():
465     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
466     R, S, c = T.matrix(), T.matrix(), T.scalar()
467     just_gemm([X, Y, Z, a, b, R, S, c],
468               [Z * c + a * T.dot(X, Y) + b * T.dot(R, S).T],
469               ishapes=[(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()],
470               expected_nb_gemm=2)
471     ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, 9), (9, 4), ()]
472     i = [X, Y, Z, a, b, R, S, c]
473     o = [(a * T.dot(X, Y) +
474          gemm_inplace(Z, b, S.T, R.T, T.constant(1.0).astype(config.floatX)))]
475     try:
476         f = inplace_func([In(ii, mutable=True) for ii in i], o,
477                          mode='FAST_RUN', on_unused_input='ignore')
478         for node in f.maker.fgraph.apply_nodes:
479             if isinstance(node.op, T.Dot):
480                 raise Failure('dot in graph')
481             if node.op == _dot22:
482                 raise Failure('_dot22 in graph')
483         g = inplace_func(i, o, mode=compile.Mode(linker='py', optimizer=None),
484                          on_unused_input='ignore')
485         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
486         r0 = f(*[np.asarray(rng.randn(*sh), config.floatX)
487                  for sh in ishapes])
488         rng = np.random.RandomState(unittest_tools.fetch_seed(234))
489         r1 = g(*[np.asarray(rng.randn(*sh), config.floatX)
490                  for sh in ishapes])
491         max_abs_err = np.max(np.abs(r0[0] - r1[0]))
492         eps = 1.0e-8
493         if config.floatX == 'float32':
494             eps = 1.0e-6
495         if max_abs_err &gt; eps:
496             raise Failure(
497                 'GEMM is computing the wrong output. max_rel_err =',
498                 max_abs_err)
499     except Failure:
500         for node in f.maker.fgraph.toposort():
501             print('GRAPH', node)
502         raise
503     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
504         'a'), T.scalar('b')
505     c, d = T<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.scalar('c'), T.scalar('d')
506     u = T.row('u')
507     v = T.vector('v')
508     w = T.</b></font>col('w')
509     can = []
510     _gemm_canonicalize(X + Y + Z, 1.0, can, 0)
511     assert can == [(1.0, X), (1.0, Y), (1.0, Z)]
512     can = []
513     _gemm_canonicalize(X + Y + u, 1.0, can, 0)
514     assert can == [(1.0, X), (1.0, Y), (1.0, u)], can
515     can = []
516     _gemm_canonicalize(X + Y + v, 1.0, can, 0)
517     assert can[:2] == [(1.0, X), (1.0, Y)]
518     assert isinstance(can[2], tuple)
519     assert len(can[2]) == 2
520     assert can[2][0] == 1.0
521     assert can[2][1].owner
522     assert isinstance(can[2][1].owner.op, T.DimShuffle)
523     assert can[2][1].owner.inputs == [v]
524     can = []
525     _gemm_canonicalize(X + Y + w, 1.0, can, 0)
526     assert can == [(1.0, X), (1.0, Y), (1.0, w)], can
527     can = []
528     _gemm_canonicalize(a * X + Y - b * Z * c, 1.0, can, 0)
529     assert can[0] == (a, X)
530     assert can[1] == (1.0, Y)
531     assert can[2][0].owner.op == T.mul
532     assert can[2][0].owner.inputs[0].owner.op == T.neg
533     assert can[2][0].owner.inputs[0].owner.inputs[0] == c
534     assert can[2][0].owner.inputs[1] == b
535     can = []
536     _gemm_canonicalize((-d) * X - (a * X + Y - b * Z * c), 1.0, can, 0)
537     assert can[0][0].owner.op == T.neg
538     assert can[0][0].owner.inputs[0] == d
539     assert can[0][1] == X
540     assert can[1][0].owner.op == T.neg
541     assert can[1][0].owner.inputs[0] == a
542     assert can[2] == (-1.0, Y)
543     assert can[3][0].owner.op == T.mul
544     assert can[3][0].owner.inputs == [c, b]
545 def test_gemm_factor():
546     assert [(1.0, X), (1.0, Y)] == _factor_canonicalized([(1.0, X), (1.0, Y)])
547     assert [(2.0, X)] == _factor_canonicalized([(1.0, X), (1<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.0, X)])
548 def test_upcasting_scalar_nogemm():
549     v = T.fmatrix('v')
550     w = T.fmatrix('w')
551     t = T.fmatrix('t')
552     alpha = T.dscalar('a')
553     rval = T.dot(</b></font>w, v) * alpha + t
554     f = theano.function([w, v, t, alpha], rval)
555     t = f.maker.fgraph.toposort()
556     assert np<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.sum([isinstance(n.op, Gemm) for n in t]) == 0
557     v = T.fmatrix('v')
558     w = T.fmatrix('w')
559     t = T.</b></font>fmatrix('t')
560     alpha = T.cscalar('a')
561     on_opt_error = config.on_opt_error
562     try:
563         config.on_opt_error = 'raise'
564         rval = T.dot(w, v) * alpha + t
565         f = theano.function([w, v, t, alpha], rval)
566     finally:
567         config.on_opt_error = on_opt_error
568     t = f.maker.fgraph.toposort()
569     assert np.sum([isinstance(n.op, Gemm) for n in t]) == 0
570 def test_gemm_nested():
571     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
572         'a'), T.scalar('b')
573     R, S, U, c, d = T.matrix('R'), T.matrix('S'), T.matrix('U'), T.scalar(
574         'c'), T.scalar('d')
575     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
576               [a * Z - b * (c * T.dot(X, Y) + d * Z)],
577               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
578                        (2, 4), (), (), (), ()],
579               max_graphlen=1)
580     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
581               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * Z)],
582               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
583                        (2, 4), (), (), (), ()],
584               max_graphlen=1)
585     just_gemm([X, Y, Z, R, S, U, a, b, c, d],
586               [a * Z - b * (c * T.dot(X, Y) + d * Z + c * U)],
587               ishapes=[(2, 3), (3, 4), (2, 4), (2, 3), (3, 4),
588                        (2, 4), (), (), (), ()],
589               max_graphlen=3)
590 def test_gemm_opt_wishlist():
591     X, Y, Z, a, b = T.matrix(), T.matrix(), T.matrix(), T.scalar(), T.scalar()
592     just_gemm([X, Y, Z, a, b],
593               [(b * b) * Z * a + (a * a) * T.dot(X, Y) + b * T.dot(X, Y)])
594     just_gemm([X, Y, Z, a, b], [Z + T.dot(X, Y) + T.dot(X, Y)])
595 def test_gemm_with_vector():
596     X, Y, Z, a, b = XYZab()
597     v = T.vector()
598     def my_just_gemm(o):
599         i = [X, Y, Z, a, b, v]
600         ishapes = [(4, 3), (3, 5), (4, 5), (), (), (5, )]
601         just_gemm(i, o, ishapes=ishapes)
602     my_just_gemm([v + T.dot(X, Y) * a + Z * b])
603     my_just_gemm([v + a * T.dot(X, Y) + b * Z])
604     my_just_gemm([v + b * Z + a * T.dot(X, Y)])
605     my_just_gemm([v + T.dot(X, Y) * a - Z * b])
606     my_just_gemm([v + a * T.dot(X, Y) - b * Z])
607     my_just_gemm([v + b * Z - a * T.dot(X, Y)])
608     my_just_gemm([v + (b * b) * Z * a + (a * a) * T.dot(X, Y) * b])
609     my_just_gemm([v + Z + T.dot(X, Y)])
610     my_just_gemm([v + Z * b + T.dot(X, Y)])
611     my_just_gemm([v + Z + a * b * a * T.dot(X, Y)])
612     my_just_gemm([v + (b * b) * Z * a - (a * a) * T.dot(X, Y) * b])
613     my_just_gemm([Z - T.dot(X, Y) + v])
614     my_just_gemm([Z * b - T.dot(X, Y) + v])
615     my_just_gemm([Z - a * b * a * T.dot(X, Y) + v])
616 def test_gemm_opt_vector_stuff():
617     X, Y, a = T.matrix(), T.matrix(), T.scalar()
618     u, v = T.vector(), T.vector()
619     f = inplace_func([a, u, v], a + T.dot(u, v), mode='FAST_RUN')
620     if gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]:
621         raise Failure('gemm_inplace in graph')
622     f = inplace_func([a, u, X, Y], a * u + T.dot(X, Y), mode='FAST_RUN')
623     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
624         raise Failure('gemm_inplace in graph')
625 def test_gemm_unrolled():
626     batch_size = 100
627     rep_size = 40
628     rng = np.random.RandomState([1, 2, 3])
629     for num_rounds in range(1, 10):
630         W = sharedX(rng.randn(rep_size, rep_size), name='W')
631         V = sharedX(np.zeros((batch_size, rep_size)), name='V')
632         H = sharedX(np.zeros((batch_size, rep_size)), name='H')
633         G = sharedX(np.zeros((batch_size, rep_size)), name='G')
634         cur_V = V
635         cur_H = H
636         def update_V(cur_H):
637             return T.nnet.sigmoid(T.dot(cur_H, W.T))
638         def update_H(cur_V):
639             return T.nnet.sigmoid(T.dot(cur_V, W) + T.dot(G, W.T))
640         for i in xrange(num_rounds):
641             cur_V = update_V(cur_H)
642             cur_H = update_H(cur_V)
643         unrolled_theano = theano.function([], updates=[(V, cur_V), (H, cur_H)],
644                                           name='unrolled_theano')
645         nb_dot = sum([1 for node in unrolled_theano.maker.fgraph.toposort()
646                       if isinstance(node.op, (theano.tensor.Dot,
647                                               theano.tensor.blas.Dot22,
648                                               theano.tensor.blas.Gemm))])
649         assert nb_dot == num_rounds * 2 + 1, nb_dot
650         unrolled_theano()
651 def test_inplace0():
652     X, Y, Z, a, b = T.matrix('X'), T.matrix('Y'), T.matrix('Z'), T.scalar(
653         'a'), T.scalar('b')
654     R, S, c = T.matrix('R'), T.matrix('S'), T.scalar('c')
655     f = inplace_func([Z, b, R, S],
656                      [Z * (Z + b * T.dot(R, S).T)], mode='FAST_RUN')
657     if (gemm_inplace in [n.op for n in f.maker.fgraph.apply_nodes]):
658         print(pp(f.maker.fgraph.outputs[0]))
659         raise Failure('gemm_inplace in graph')
660     assert gemm_no_inplace in [n.op for n in f.maker.fgraph.apply_nodes]
661     f = inplace_func([X, Y, Z, a, b, R, S, c],
662                      [Z * (c * Z + a * T.dot(X, Y) + b * T.dot(R, S).T)],
663                      mode='FAST_RUN')
664     if (gemm_inplace not in [n.op for n in f.maker.fgraph.apply_nodes]):
665         theano.printing.debugprint(f)
666         raise Failure('no gemm_inplace in graph')
667 def test_inplace1():
668     X, Y, Z, a, b = XYZab()
669     f = inplace_func([X, Y, Z],
670                      [Z + Z + T.dot(X, Y)], mode='FAST_RUN')
671     assert [n.op for n in f.maker.fgraph.apply_nodes] == [gemm_no_inplace]
672 def test_dot22():
673     for dtype1 in ['float32', 'float64', 'complex64', 'complex128']:
674         a = T.matrix(dtype=dtype1)
675         for dtype2 in ['float32', 'float64', 'complex64', 'complex128']:
676             b = T.matrix(dtype=dtype2)
677             f = theano.function([a, b], T.dot(a, b), mode=mode_blas_opt)
678             topo = f.maker.fgraph.toposort()
679             if dtype1 == dtype2:
680                 assert _dot22 in [x.op for x in topo], (dtype1, dtype2)
681             else:
682                 check = [isinstance(x.op, T.Dot) for x in topo]
683                 assert any(check), (dtype1, dtype2)
684             rng = np.random.RandomState(unittest_tools.fetch_seed())
685             def cmp(a_shp, b_shp):
686                 av = rng.uniform(size=a_shp).astype(dtype1)
687                 bv = rng.uniform(size=b_shp).astype(dtype2)
688                 f(av, bv)
689             cmp((3, 4), (4, 5))
690             cmp((0, 4), (4, 5))
691             cmp((3, 0), (0, 5))
692             cmp((3, 4), (4, 0))
693             cmp((0, 4), (4, 0))
694             cmp((0, 0), (0, 0))
695 @attr('slow')
696 def test_dot22scalar():
697     rng = np.random.RandomState(unittest_tools.fetch_seed())
698     for dtype1 in ['complex64', 'complex128']:
699         a = T.matrix('a', dtype=dtype1)
700         for dtype2 in ['complex64', 'complex128']:
701             b = T.matrix('b', dtype=dtype2)
702             for dtype3 in ['complex64', 'complex128']:
703                 c = T.matrix('c', dtype=dtype3)
704                 for dtype4 in ['complex64', 'complex128']:
705                     cst = theano.tensor.basic.constant(.2, dtype=dtype4)
706                     cst2 = theano.tensor.basic.constant(.1, dtype=dtype4)
707                     def check_dot22scalar(func, len_topo_scalar=-1):
708                         topo = func.maker.fgraph.toposort()
709                         ops = [x.op for x in topo]
710                         dtype4_upcast = theano.scalar.upcast(dtype4, dtype1,
711                                                              dtype2)
712                         if dtype1 == dtype2 == dtype3 == dtype4_upcast:
713                             if len_topo_scalar &gt; 0:
714                                 assert len(topo) == len_topo_scalar
715                             assert _dot22scalar in ops, (dtype1, dtype2,
716                                                          dtype3, dtype4)
717                         elif dtype1 == dtype2 == dtype4_upcast:
718                             if not (len_topo_scalar &gt; 0):
719                                 assert len(topo) == len_topo_scalar
720                                 assert _dot22scalar in ops, (dtype1, dtype2,
721                                                              dtype3, dtype4)
722                             else:
723                                 assert _dot22scalar in ops or _dot22 in ops, (
724                                     dtype1, dtype2, dtype3, dtype4)
725                         elif dtype1 == dtype2:
726                             assert _dot22 in ops, (dtype1, dtype2,
727                                                    dtype3, dtype4)
728                         else:
729                             check = [isinstance(o, T.Dot) for o in ops]
730                             assert any(check), (dtype1, dtype2, dtype3, dtype4)
731                     def cmp(a_shp, b_shp, c_shp, sqr_shp=(5, 5)):
732                         av = rng.uniform(size=a_shp).astype(dtype1)
733                         bv = rng.uniform(size=b_shp).astype(dtype2)
734                         cv = rng.uniform(size=c_shp).astype(dtype3)
735                         sv = rng.uniform(size=sqr_shp).astype(dtype1)
736                         if False:
737                             f = theano.function([a, b], cst * T.dot(a, b),
738                                                 mode=mode_blas_opt)
739                             f.maker.fgraph.toposort()
740                             check_dot22scalar(f, 1)
741                             f(av, bv)
742                         if True:
743                             f = theano.function([a, b, c],
744                                                 cst * c * T.dot(a, b),
745                                                 mode=mode_blas_opt)
746                             f.maker.fgraph.toposort()
747                             check_dot22scalar(f, 2)
748                             f(av, bv, cv)
749                                             c * cst * T.dot(a, b),
750                                             mode=mode_blas_opt)
751                         f.maker<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.fgraph.toposort()
752                         check_dot22scalar(f, 2)
753                         f(av, bv, cv)
754                         m2 = mode_blas_opt.including('canonicalize')
755                         f = theano.function([a</b></font>, b, c],
756                                             cst2 * c * cst * T.dot(a, b),
757                                             mode=m2)
758                         f.maker.fgraph.toposort()
759                         check_dot22scalar(f, 2)
760                         f(av, bv, cv)
761                         if dtype1 == dtype2 == dtype3:
762                             f = theano.function([a, b, c],
763                                                 c * cst * a * T.dot(a, b),
764                                                 mode=m2)
765                             f.maker.fgraph.toposort()
766                             check_dot22scalar(f, 2)
767                             f(sv, sv, sv)
768                             f = theano.function([a, b, c],
769                                                 cst * c * a * T.dot(a, b),
770                                                 mode=mode_blas_opt)
771                             f.maker.fgraph.toposort()
772                             f(sv, sv, sv)
773                             f = theano.function([a, b, c],
774                                                 c * a * cst * T.dot(a, b),
775                                                 mode=m2)
776                             f.maker.fgraph.toposort()
777                             check_dot22scalar(f, 2)
778                             f(sv, sv, sv)
779                     cmp((3, 4), (4, 5), (3, 5))
780                     cmp((0, 4), (4, 5), (0, 5))
781                     cmp((3, 0), (0, 5), (3, 5))
782                     cmp((3, 4), (4, 0), (3, 0), (0, 0))
783                     cmp((0, 4), (4, 0), (0, 0))
784                     cmp((0, 0), (0, 0), (0, 0))
785 def test_dot22scalar_cast():
786     A = T.dmatrix()
787     for scalar_int_type in T.int_dtypes:
788         y = T.scalar(dtype=scalar_int_type)
789         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
790         assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
791     A = T.fmatrix()
792     for scalar_int_type in T.int_dtypes:
793         y = T.scalar(dtype=scalar_int_type)
794         f = theano.function([A, y], T.dot(A, A) * y, mode=mode_blas_opt)
795         if scalar_int_type in ['int32', 'int64']:
796             assert _dot22 in [x.op for x in f.maker.fgraph.toposort()]
797         else:
798             assert _dot22scalar in [x.op for x in f.maker.fgraph.toposort()]
799 def test_local_dot22_to_dot22scalar():
800     A = T.dmatrix()
801     opt = theano.tensor.opt.in2out(
802         theano.tensor.blas.local_dot22_to_dot22scalar)
803     mode = mode.__class__(optimizer<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=opt)
804     x = T.dscalar()
805     y = T.dscalar()
806     z = T.dscalar()
807     m = T.</b></font>dmatrix()
808     r = T.drow()
809     for idx, node in enumerate([
810         T.mul(_dot22(A, A), x),
811         T.mul(_dot22(A, A), x, y),
812         T.mul(_dot22(A, A), x, r),
813         T.mul(_dot22(A, A), m, x),
814         T.mul(_dot22(A, A), x, m),
815         T.mul(_dot22(A, A), x, (m * y)),
816         T.mul(_dot22(A, A), (m * y), x),
817         T.mul(_dot22(A, A), x, (r * y)),
818         T.mul(_dot22(A, A), (r * y), x),
819         T.mul(_dot22(A, A), (x * y), (m * x)),
820         T.mul(_dot22(A, A), (r * y), (y * x)),
821         T.mul(_dot22(A, A), (m * y), m),
822         T.mul(_dot22(A, A), m, (m * y)),
823         T.mul(_dot22(A, A), (r * y), (m * x)),
824         T.mul(_dot22(A, A), (m * y * z), m),
825         T.mul(_dot22(A, A), m, (m * y * z)),
826         T.mul(_dot22(A, A), T.mul(m, y, z), m),
827         T.mul(_dot22(A, A), m, T.mul(m, y, z)),
828         T.mul(_dot22(A, A), (r * m), (m * x)),
829     ]):
830         node2 = theano.tensor.blas.local_dot22_to_dot22scalar.transform(
831             node.owner)
832         assert node2
833         f = theano.function([x, y, z, m, r, A], node,
834                             mode=mode, on_unused_input='ignore')
835         f(.1, .2, .3, [[1, 2], [3, 4]], [[5, 6]], [[7, 8], [9, 10]])
836 def test_dot_w_self():
837     A = shared(value=np.ones((2, 2)))
838     B = T.matrix()
839     p = T.dot(A, A) * B
840     grad = T.grad(T.mean(p), A)
841     f = theano.function([B], p, updates=[(A, A - grad)])
842     f(np.asarray([[0, 1], [2, 3]], dtype=config.floatX))
843 class TestGemv(TestCase, unittest_tools.TestOptimizationMixin):
844     def test_dot_vv(self):
845         rng = np.random.RandomState(unittest_tools.fetch_seed())
846         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
847         w = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
848         f = theano.function([], theano.dot(v, w), mode=mode_blas_opt)
849         self.assertFunctionContains0(f, T.dot)
850         assert np.allclose(f(), np.dot(v.get_value(), w.get_value(<font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>)))
851     def test_dot_vm(self):
852         rng = np.random.RandomState(unittest_tools.fetch_seed())
853         v = theano.shared(np.</b></font>array(rng.uniform(size=(2,)), dtype='float32'))
854         m = theano.shared(np.array(rng.uniform(size=(2, 3)), dtype='float32'))
855         f = theano.function([], theano.dot(v, m), mode=mode_blas_opt)
856         self.assertFunctionContains0(f, T.dot)
857         self.assertFunctionContains1(f, Gemv(True))
858         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
859         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
860         assert np.allclose(f(), np.dot(v.get_value(), m.get_value()))
861     def test_dot_mv(self):
862         rng = np.random.RandomState(unittest_tools.fetch_seed())
863         v = theano.shared(np.array(rng.uniform(size=(2,)), dtype='float32'))
864         m = theano.shared(np.array(rng.uniform(size=(3, 2)), dtype='float32'))
865         f = theano.function([], theano.dot(m, v), mode=mode_blas_opt)
866         self.assertFunctionContains0(f, T.dot)
867         self.assertFunctionContains1(f, Gemv(True))
868         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
869         m.set_value(m.get_value(borrow=True)[::-1, ::-1], borrow=True)
870         assert np.allclose(f(), np.dot(m.get_value(), v.get_value()))
871     @staticmethod
872     def t_gemv1(m_shp):
873         rng = np.random.RandomState(unittest_tools.fetch_seed())
874         v1 = theano.shared(np.array(rng.uniform(size=(m_shp[1],)),
875                            dtype='float32'))
876         v2_orig = np.array(rng.uniform(size=(m_shp[0],)), dtype='float32')
877         v2 = theano.shared(v2_orig)
878         m = theano.shared(np.array(rng.uniform(size=m_shp), dtype='float32'))
879         f = theano.function([], v2 + theano.dot(m, v1), mode=mode_blas_opt)
880         assert np.allclose(f(), np.dot(m.get_value(), v1.get_value()) + v2_orig)
881         topo <font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= f.maker.fgraph.toposort()
882         assert len(topo) == 1
883         assert isinstance(topo[0].op, Gemv)
884         assert topo[0].op.</b></font>inplace is False
885         g = theano.function([], [], updates=[(v2, v2 + theano.dot(m, v1))],
886                             mode=mode_blas_opt)
887         g()
888         assert np.allclose(v2.get_value(), np.dot(m.get_value(),
889                            v1.get_value()) + v2_orig)
890         topo = g.maker.fgraph.toposort()
891         assert len(topo) == 1
892         assert isinstance(topo[0].op, Gemv)
893         if config.mode != 'FAST_COMPILE':
894             assert topo[0].op.inplace is True
895         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
896                     borrow=True)
897         v2.set_value(v2_orig)
898         assert np.allclose(f(),
899                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
900         g()
901         assert np.allclose(v2.get_value(),
902                            np.dot(m.get_value(), v1.get_value()) + v2_orig)
903     @attr('slow')
904     def test_gemv1(self):
905         self.t_gemv1((3, 2))
906         self.t_gemv1((0, 2))
907         self.t_gemv1((3, 0))
908         self.t_gemv1((0, 0))
909     def test_gemv2(self):
910         rng = np.random.RandomState(unittest_tools.fetch_seed())
911         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
912                            dtype='float32'))
913         v2_orig = np.array(rng.uniform(size=(3,)), dtype='float32')
914         v2 = theano.shared(v2_orig)
915         m = theano.shared(np.array(rng.uniform(size=(2, 3)),
916                           dtype='float32'))
917         f = theano.function([], v2 + theano.dot(v1, m), mode=mode_blas_opt)
918         assert np.allclose(f(),
919                            np.dot(v1.get_value(), m.get_value()) +
920                            v2.get_value())
921         topo = f.maker.fgraph.toposort()
922         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
923         assert topo[-1].op.inplace is False
924         g = theano.function([], [], updates=[(v2, v2 + theano.dot(v1, m))],
925                             mode=mode_blas_opt)
926         g()
927         assert np.allclose(v2.get_value(),
928                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
929         topo = g.maker.fgraph.toposort()
930         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
931         if config.mode != 'FAST_COMPILE':
932             assert topo[-1].op.inplace is True
933         m.set_value(m.get_value(borrow=True)[::-1, ::-1],
934                     borrow=True)
935         v2.set_value(v2_orig)
936         assert np.allclose(f(),
937                            np.dot(v1.get_value(), m.get_value()) +
938                            v2.get_value())
939         g()
940         assert np.allclose(v2.get_value(),
941                            np.dot(v1.get_value(), m.get_value()) + v2_orig)
942     def test_gemv_broadcast(self):
943         rng = np.random.RandomState(unittest_tools.fetch_seed())
944         v1 = theano.shared(np.array(rng.uniform(size=(2,)),
945                                     dtype='float32'))
946         v2_orig = np.array(rng.uniform(size=(1,)), dtype='float32')
947         v2 = theano.shared(v2_orig)
948         m = theano.shared(np.array(rng.uniform(size=(1, 2)),
949                                    dtype='float32'),
950                           broadcastable=(True, False))
951         o = theano.dot(m, v1)
952         f = theano.function([], o + v2, mode=mode_blas_opt)
953         assert np.allclose(
954             f(),
955             np.dot(m.get_value(), v1.get_value()) + v2.get_value())
956         topo = f.maker.fgraph.toposort()
957         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
958         o = theano.tensor.blas.gemv_no_inplace(v2, 0.5, m, v1, 0.25)
959         f = theano.function([], o, mode=mode_blas_opt)
960         assert np.allclose(
961             f(),
962             0.5 * np.dot(m.get_value(), v1.get_value()) + 0.25 * v2.get_value())
963         topo = f.maker.fgraph.toposort()
964         assert sum(isinstance(node.op, Gemv) for node in topo) == 1
965     def test_gemv_dimensions(self):
966         A = T.matrix('A')
967         x, y = T.vectors('x', 'y')
968         alpha = theano.shared(theano._asarray(1.0, dtype=config.floatX),
969                               name='alpha')
970         beta = theano.shared(theano._asarray(1.0, dtype=config.floatX),
971                              name='beta')
972         z = beta * y + alpha * T.dot(A, x)
973         f = theano.function([A, x, y], z)
974         A_val = np.ones((5, 3), dtype=config.floatX)
975         ones_3 = np.ones(3, dtype=config.floatX)
976         ones_4 = np.ones(4, dtype=config.floatX)
977         ones_5 = np.ones(5, dtype=config.floatX)
978         ones_6 = np.ones(6, dtype=config.floatX)
979         f(A_val, ones_3, ones_5)
980         f(A_val[::-1, ::-1], ones_3, ones_5)
981         self.assertRaises(ValueError, f, A_val, ones_4, ones_5)
982         self.assertRaises(ValueError, f, A_val, ones_3, ones_6)
983         self.assertRaises(ValueError, f, A_val, ones_4, ones_6)
984 def matrixmultiply(a, b):
985     if len(b.shape) == 1:
986         b_is_vector = True
987         b = b[:, newaxis]
988     else:
989         b_is_vector = False
990     assert a.shape[1] == b.shape[0]
991     c = zeros((a.shape[0], b.shape[1]), common_type(a, b))
992     for i in xrange(a.shape[0]):
993         for j in xrange(b.shape[1]):
994             s = 0
995             for k in xrange(a.shape[1]):
996                 s += a[i, k] * b[k, j]
997             c[i, j] = s
998     if b_is_vector:
999         c = c.reshape((a.shape[0],))
1000     return c
1001 class BaseGemv(object):
1002     mode = mode_blas_opt  # can be overridden with self.mode
1003     shared = staticmethod(theano.shared)
1004     def get_data(self, x_stride=1, y_stride=1):
1005         rng = np.random.RandomState(unittest_tools.fetch_seed())
1006         mult = array(1, dtype=self.dtype)
1007         if self.dtype in [complex64, complex128]:
1008             mult = array(1 + 1j, dtype=self.dtype)
1009         alpha = array(1., dtype=self.dtype) * mult
1010         beta = array(1., dtype=self.dtype) * mult
1011         a = rng.randn(3, 3).astype(self.dtype) * mult
1012         x = arange(shape(a)[0] * x_stride, dtype=self.dtype) * mult
1013         y = arange(shape(a)[1] * y_stride, dtype=self.dtype) * mult
1014         return alpha, beta, a, x, y
1015         alpha, beta, a, x, y = [self.shared(value)
1016                                 for value in self.get_data()]
1017         desired_oy = alpha.get_value() * matrixmultiply(a.get_value(), x.get_value()) + beta<font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get_value() * y.get_value()
1018         oy = alpha * T.dot(a, x) + beta * y
1019         oy_func = theano.function([], oy, mode=self.mode)
1020         oy_func.maker.</b></font>fgraph.toposort()
1021         self.assertFunctionContains1(oy_func, self.gemv)
1022         oy_val = oy_func()
1023         assert_array_almost_equal(desired_oy, oy_val)
1024     def test_default_beta_y(self):
1025         vs = self.get_data()
1026         alpha_v, beta_v, a_v, x_v, y_v = vs
1027         a = self.shared(a_v)
1028         x = self.shared(x_v)
1029         desired_oy = matrixmultiply(a_v, x_v)
1030         oy = T.dot(a, x)
1031         oy_func = theano.function([], oy, mode=self.mode)
1032         self.assertFunctionContains1(oy_func, self.gemv_inplace)
1033         oy_v = oy_func()
1034         assert_array_almost_equal(desired_oy, oy_v)
1035     def test_simple_transpose(self):
1036         vs = self.get_data()
1037         alpha_v, beta_v, a_v, x_v, y_v = vs
1038         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1039         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1040                                               x_v) + beta_v * y_v
1041         oy = alpha * T.dot(a.T, x) + beta * y
1042         oy_func = theano.function([], oy, mode=self.mode)
1043         self.assertFunctionContains1(oy_func, self.gemv)
1044         oy_v = oy_func()
1045         assert_array_almost_equal(desired_oy, oy_v)
1046     def test_x_stride(self):
1047         vs = self.get_data(x_stride=2)
1048         alpha_v, beta_v, a_v, x_v, y_v = vs
1049         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1050         desired_oy = alpha_v * matrixmultiply(a_v, x_v[::2]) + beta_v * y_v
1051         oy = alpha * T.dot(a, x[::2]) + beta * y
1052         oy_func = theano.function([], oy, mode=self.mode)
1053         self.assertFunctionContains1(oy_func, self.gemv)
1054         oy_v = oy_func()
1055         assert_array_almost_equal(desired_oy, oy_v)
1056     def test_x_stride_transpose(self):
1057         vs = self.get_data(x_stride=2)
1058         alpha_v, beta_v, a_v, x_v, y_v = vs
1059         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1060         desired_oy = alpha_v * matrixmultiply(transpose(a_v), x_v[::2]) + \
1061             beta_v * y_v
1062         oy = alpha * T.dot(a.T, x[::2]) + beta * y
1063         oy_func = theano.function([], oy, mode=self.mode)
1064         self.assertFunctionContains1(oy_func, self.gemv)
1065         oy_v = oy_func()
1066         assert_array_almost_equal(desired_oy, oy_v)
1067     def test_y_stride(self):
1068         vs = self.get_data(y_stride=2)
1069         alpha_v, beta_v, a_v, x_v, y_v = vs
1070         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1071         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v[::2]
1072         oy = alpha * T.dot(a, x) + beta * y[::2]
1073         oy_func = theano.function([], oy, mode=self.mode)
1074         self.assertFunctionContains1(oy_func, self.gemv)
1075         oy_v = oy_func()
1076         assert_array_almost_equal(desired_oy, oy_v)
1077     def test_y_stride_transpose(self):
1078         vs = self.get_data(y_stride=2)
1079         alpha_v, beta_v, a_v, x_v, y_v = vs
1080         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1081         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1082                                               x_v) + beta_v * y_v[::2]
1083         oy = alpha * T.dot(a.T, x) + beta * y[::2]
1084         oy_func = theano.function([], oy, mode=self.mode)
1085         self.assertFunctionContains1(oy_func, self.gemv)
1086         oy_v = oy_func()
1087     def test_a_strides(self):
1088         vs = self<font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get_data()
1089         alpha_v, beta_v, a_v, x_v, y_v = vs
1090         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1091         a_v = a_v[::-1, ::-1]
1092         a.set_value(a.get_value(</b></font>borrow=True,
1093                                 return_internal_type=True)[::-1, ::-1],
1094                     borrow=True)
1095         desired_oy = alpha_v * matrixmultiply(a_v, x_v) + beta_v * y_v
1096         oy = alpha * T.dot(a, x) + beta * y
1097         oy_func = theano.function([], oy, mode=self.mode)
1098         self.assertFunctionContains1(oy_func, self.gemv)
1099         oy_v = oy_func()
1100         assert_array_almost_equal(desired_oy, oy_v)
1101     def test_a_strides_transpose(self):
1102         vs = self.get_data()
1103         alpha_v, beta_v, a_v, x_v, y_v = vs
1104         alpha, beta, a, x, y = [self.shared(v) for v in vs]
1105         a_v = a_v[::-1, ::-1]
1106         a.set_value(a.get_value(borrow=True,
1107                                 return_internal_type=True)[::-1, ::-1],
1108                     borrow=True)
1109         desired_oy = alpha_v * matrixmultiply(transpose(a_v),
1110                                               x_v) + beta_v * y_v
1111         oy = alpha * T.dot(a.T, x) + beta * y
1112         oy_func = theano.function([], oy, mode=self.mode)
1113         self.assertFunctionContains1(oy_func, self.gemv)
1114         oy_v = oy_func()
1115         assert_array_almost_equal(desired_oy, oy_v)
1116     def test_upcasting_scalar_nogemv(self):
1117         vs = self.get_data()
1118         alpha_v, beta_v, a_v, x_v, y_v = vs
1119         alpha_v = alpha_v.astype("float64")
1120         a_v = a_v.astype("float32")
1121         x_v = x_v.astype("float32")
1122         y_v = y_v.astype("float32")
1123         alpha = T.dscalar('alpha')
1124         a = self.shared(a_v)
1125         x = self.shared(x_v)
1126         y = self.shared(y_v)
1127         rval = T.dot(a, x) * alpha + y
1128         f = theano.function([alpha], rval, mode=self.mode)
1129         n_gemvs = 0
1130         for node in f.maker.fgraph.toposort():
1131             if node.op == self.gemv_inplace:
1132                 n_gemvs += 1
1133                 assert node.outputs[0].dtype == 'float32'
1134         assert n_gemvs == 1, n_gemvs
1135         self.assertFunctionContains1(f, self.gemv_inplace)
1136         f(alpha_v)
1137 class TestSgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1138     dtype = float32
1139     gemv = theano.tensor.blas.gemv_no_inplace
1140     gemv_inplace = theano.tensor.blas.gemv_inplace
1141 class TestDgemv(TestCase, BaseGemv, unittest_tools.TestOptimizationMixin):
1142     dtype = float64
1143     gemv = theano.tensor.blas.gemv_no_inplace
1144     gemv_inplace = theano.tensor.blas.gemv_inplace
1145 class TestGer_make_node(TestCase):
1146     def setUp(self):
1147         self.iv = T.tensor(dtype='int32', broadcastable=(False,))
1148         self.fv = T.tensor(dtype='float32', broadcastable=(False,))
1149         self.fv1 = T.tensor(dtype='float32', broadcastable=(True,))
1150         self.dv = T.tensor(dtype='float64', broadcastable=(False,))
1151         self.dv1 = T.tensor(dtype='float64', broadcastable=(True,))
1152         self.cv = T.tensor(dtype='complex64', broadcastable=(False,))
1153         self.zv = T.tensor(dtype='complex128', broadcastable=(False,))
1154         self.fv_2 = T.tensor(dtype='float32', broadcastable=(False,))
1155         self.fv1_2 = T.tensor(dtype='float32', broadcastable=(True,))
1156         self.dv_2 = T.tensor(dtype='float64', broadcastable=(False,))
1157         self.dv1_2 = T.tensor(dtype='float64', broadcastable=(True,))
1158         self.cv_2 = T.tensor(dtype='complex64', broadcastable=(False,))
1159         self.zv_2 = T.tensor(dtype='complex128', broadcastable=(False,))
1160         self.fm = T.fmatrix()
1161         self.dm = T.dmatrix()
1162         self.cm = T.cmatrix()
1163         self.zm = T.zmatrix()
1164         self.fa = T.fscalar()
1165         self.da = T.dscalar()
1166         self.ca = T.cscalar()
1167         self.za = T.zscalar()
1168     def test_works_on_all_valid_dtypes(self):
1169         self.assertEqual(self.fm.type,
1170                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1171         self.assertEqual(self.fm.type,
1172                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1173         self.assertEqual(self.fm.type,
1174                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1175         self.assertEqual(self.fm.type,
1176                          ger(self.fm, self.fa, self.fv, self.fv_2).type)
1177     def test_fails_on_invalid_dtypes(self):
1178         self.assertRaises(TypeError,
1179                           ger, T.imatrix(), T.iscalar(), T.ivector(),
1180                           T.ivector())
1181     def test_fails_for_nonscalar_alpha(self):
1182         self.assertRaises(TypeError,
1183                           ger, self.fm, self.fm, self.fv, self.fv_2)
1184         self.assertRaises(TypeError,
1185                           ger, self.fm, self.fv1, self.fv, self.fv_2)
1186         self.assertEqual(self.fm.type,
1187                          ger(self.fm, self.fv1.dimshuffle(), self.fv,
1188                              self.fv_2).type)
1189     def test_fails_for_nonmatrix_A(self):
1190         self.assertRaises(TypeError,
1191                           ger, self.fv, self.fa, self.fv, self.fv_2)
1192     def test_fails_for_nonvector_x_or_y(self):
1193         self.assertRaises(TypeError,
1194                           ger, self.fm, self.fa,
1195                           self.fv.dimshuffle('x', 0), self.fv_2)
1196         self.assertRaises(TypeError,
1197                           ger, self.fm, self.fa,
1198                           self.fv, self.fv_2.dimshuffle('x', 0))
1199     def test_fails_for_mixed_dtypes(self):
1200         self.assertRaises(TypeError, ger, self.dm, self.fa, self.fv, self.fv_2)
1201         self.assertRaises(TypeError, ger, self.fm, self.da, self.fv, self.fv_2)
1202         self.assertRaises(TypeError, ger, self.fm, self.fa, self.dv, self.fv_2)
1203         self.assertRaises(TypeError, ger, self.fm, self.fa, self.fv, self.dv_2)
1204         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.dv_2)
1205         self.assertRaises(TypeError, ger, self.cm, self.fa, self.fv, self.zv_2)
1206 class TestGer_OpContract(TestCase, unittest_tools.T_OpContractMixin):
1207     def setUp(self):
1208         self.ops = [ger, ger_destructive]
1209     def clone(self, op):
1210         return Ger(op.destructive)
1211 class TestGer(TestCase, unittest_tools.TestOptimizationMixin):
1212     shared = staticmethod(theano.shared)
1213     def setUp(self):
1214         self.mode = theano.compile.get_default_mode().including('fast_run')
1215         self.mode = self.mode.excluding('c_blas', 'scipy_blas')
1216         dtype = self.dtype = 'float64'  # optimization isn't dtype-dependent
1217         self.A = T.tensor(dtype=dtype, broadcastable=(False, False))
1218         self.a = T.tensor(dtype=dtype, broadcastable=())
1219         self.x = T.tensor(dtype=dtype, broadcastable=(False,))
1220         self.y = T.tensor(dtype=dtype, broadcastable=(False,))
1221         self.ger = ger
1222         self.ger_destructive = ger_destructive
1223         self.gemm = gemm_no_inplace
1224     def function(self, inputs, outputs, updates=None):
1225         if updates is None:
1226             updates = []
1227         return theano.function(inputs, outputs, self.mode, updates=updates)
1228     def b(self, bval):
1229         return T.as_tensor_variable(np.asarray(bval, dtype=self.dtype))
1230     def test_b_0_triggers_ger(self):
1231         assert T.blas.local_gemm_to_ger.transform(
1232             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1233                             self.y.dimshuffle('x', 0), self.b(0)).owner)
1234     def test_b_1_triggers_ger(self):
1235         assert T.blas.local_gemm_to_ger.transform(
1236             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1237                             self.y.dimshuffle('x', 0), self.b(1)).owner)
1238     def test_b_other_does_not_triggers_ger(self):
1239         assert not T.blas.local_gemm_to_ger.transform(
1240             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1241                             self.y.dimshuffle('x', 0), self.b(1.5)).owner)
1242     def test_b_nonconst_does_not_triggers_ger(self):
1243         assert not T.blas.local_gemm_to_ger.transform(
1244             gemm_no_inplace(self.A, self.a, self.x.dimshuffle(0, 'x'),
1245                             self.y.dimshuffle('x', 0), self.a).owner)
1246     def test_outer(self):
1247         f = self.function([self.x, self.y], T.outer(self.x, self.y))
1248         self.assertFunctionContains(f, self.ger_destructive)
1249         f(np.random.rand(5).astype(self.dtype),
1250           np.random.rand(4).astype(self.dtype))
1251     def test_A_plus_outer(self):
1252         f = self.function([self.A, self.x, self.y],
1253                           self.A + T.outer(self.x, self.y))
1254         self.assertFunctionContains(f, self.ger)
1255         f(np.random.rand(5, 4).astype(self.dtype),
1256           np.random.rand(5).astype(self.dtype),
1257           np.random.rand(4).astype(self.dtype))
1258         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1259           np.random.rand(5).astype(self.dtype),
1260           np.random.rand(4).astype(self.dtype))
1261     def test_A_plus_scaled_outer(self):
1262         f = self.function([self.A, self.x, self.y],
1263                           self.A + 0.1 * T.outer(self.x, self.y))
1264         self.assertFunctionContains(f, self.ger)
1265         f(np.random.rand(5, 4).astype(self.dtype),
1266           np.random.rand(5).astype(self.dtype),
1267           np.random.rand(4).astype(self.dtype))
1268         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1269           np.random.rand(5).astype(self.dtype),
1270           np.random.rand(4).astype(self.dtype))
1271     def test_scaled_A_plus_scaled_outer(self):
1272         f = self.function([self.A, self.x, self.y],
1273                           np.asarray(0.2, self.dtype) * self.A +
1274                           np.asarray(0.1, self.dtype) * T.outer(
1275                           self.x, self.y))
1276         self.assertFunctionContains(f, self.gemm)
1277         f(np.random.rand(5, 4).astype(self.dtype),
1278           np.random.rand(5).astype(self.dtype),
1279           np.random.rand(4).astype(self.dtype))
1280         f(np.random.rand(5, 4).astype(self.dtype)[::-1, ::-1],
1281           np.random.rand(5).astype(self.dtype),
1282           np.random.rand(4).astype(self.dtype))
1283     def given_dtype(self, dtype, M, N):
1284         f = self.function([self.A, self.x, self.y],
1285                           self.A + 0.1 * T.outer(self.x, self.y))
1286         self.assertFunctionContains(f, self.ger)
1287         f(np.random.rand(M, N).astype(self.dtype),
1288           np.random.rand(M).astype(self.dtype),
1289           np.random.rand(N).astype(self.dtype))
1290         f(np.random.rand(M, N).astype(self.dtype)[::-1, ::-1],
1291           np.random.rand(M).astype(self.dtype),
1292           np.random.rand(N).astype(self.dtype))
1293     def test_f32_0_0(self):
1294         return self.given_dtype('float32', 0, 0)
1295     def test_f32_1_0(self):
1296         return self.given_dtype('float32', 1, 0)
1297     def test_f32_0_1(self):
1298         return self.given_dtype('float32', 0, 1)
1299     def test_f32_1_1(self):
1300         return self.given_dtype('float32', 1, 1)
1301     def test_f32_4_4(self):
1302         return self.given_dtype('float32', 4, 4)
1303     def test_f32_7_1(self):
1304         return self.given_dtype('float32', 7, 1)
1305     def test_f32_1_2(self):
1306         return self.given_dtype('float32', 1, 2)
1307     def test_f64_4_5(self):
1308         return self.given_dtype('float64', 4, 5)
1309     def test_c64_7_1(self):
1310         return self.given_dtype('complex64', 7, 1)
1311     def test_c128_1_9(self):
1312         return self.given_dtype('complex128', 1, 9)
1313     def test_inplace(self):
1314         A = self.shared(np.random.rand(4, 5).astype(self.dtype))
1315         f = self.function([self.x, self.y], [],
1316                           updates=[(A, A + T.constant(0.1, dtype=self.dtype) *
1317                                    T.outer(self.x, self.y))])
1318         self.assertFunctionContains(f, self.ger_destructive)
1319         f(np.random.rand(4).astype(self.dtype),
1320           np.random.rand(5).astype(self.dtype))
1321         A.set_value(
1322             A.get_value(borrow=True, return_internal_type=True)[::-1, ::-1],
1323             borrow=True)
1324         f(np.random.rand(4).astype(self.dtype),
1325           np.random.rand(5).astype(self.dtype))
1326 class TestBlasStrides(TestCase):
1327     dtype = 'float64'
1328     shared = staticmethod(tensor._shared)
1329     mode = theano.compile.get_default_mode()
1330     mode = mode.including('fast_run').excluding('gpu', 'c_blas', 'scipy_blas')
1331     rng = np.random.RandomState(seed=unittest_tools.fetch_seed())
1332     def rand(self, *shape):
1333         return theano._asarray(self.rng.rand(*shape), dtype=self.dtype)
1334         av = np.zeros((0, 0), dtype=self.dtype)
1335         bv = self.rand(*b_shp)
1336         cv <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= self.rand(*c_shp)
1337         a = self.shared(av, 'a')
1338         b = self.shared(bv, 'b')
1339         c = self.shared(cv, 'c')
1340         b_t = self.shared(bv.T, 'b.T')
1341         c_t = self.shared(cv.T, 'c.T')
1342         b_dev =</b></font> b.get_value(borrow=False, return_internal_type=True)
1343         c_dev = c.get_value(borrow=False, return_internal_type=True)
1344         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1345         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1346         f_nn = theano.function([], [], updates=[(a, tensor.dot(b, c))],
1347                                mode=self.mode)
1348         f_nt = theano.function([], [], updates=[(a, tensor.dot(b, c_t.T))],
1349                                mode=self.mode)
1350         f_tn = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c))],
1351                                mode=self.mode)
1352         f_tt = theano.function([], [], updates=[(a, tensor.dot(b_t.T, c_t.T))],
1353                                mode=self.mode)
1354         for step_signs in itertools_product((-1, 1), repeat=4):
1355             for step in (1, 2):
1356                 b_step1, b_step2, c_step1, c_step2 = (s * step
1357                                                       for s in step_signs)
1358                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1359                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1360                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1361                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1362                 a_n = np.dot(bv[::b_step1, ::b_step2],
1363                              cv[::c_step1, ::c_step2])
1364                 f_nn()
1365                 assert np.allclose(a.get_value(), a_n)
1366                 f_nt()
1367                 assert np.allclose(a.get_value(), a_n)
1368                 f_tn()
1369                 assert np.allclose(a.get_value(), a_n)
1370                 f_tt()
1371     def test_dot22(self):
1372         self<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.cmp_dot22((3, 4), (4, 5))
1373         self.cmp_dot22((1, 4), (4, 5))
1374         self.cmp_dot22((3, 4), (4, 1))
1375         self.cmp_dot22((3, 1), (1, 1))
1376         self.cmp_dot22((1, 4), (4, 1))
1377         self.cmp_dot22((3, 1), (1, 5))
1378         self.cmp_dot22((0, 4), (4, 5))
1379         self.cmp_dot22((0, 4), (4, 1))
1380         self.cmp_dot22((0, 1), (1, 5))
1381         self.cmp_dot22((3, 4), (4, 0))
1382         self.cmp_dot22((3, 0), (0, 5))
1383         self.cmp_dot22((0, 4), (4</b></font>, 0))
1384         self.cmp_dot22((0, 0), (0, 0))
1385     def cmp_dot22scalar(self, b_shp, c_shp):
1386         av = np.zeros((0, 0), dtype=self.dtype)
1387         bv = self.rand(*b_shp)
1388         l = np.float32(0.2)
1389         a <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= self.shared(av, 'a')
1390         b = self.shared(bv, 'b')
1391         c = self.shared(cv, 'c')
1392         b_t = self.shared(bv.T, 'b.T')
1393         c_t = self.shared(cv.T, 'c.T')
1394         b_dev =</b></font> b.get_value(borrow=False, return_internal_type=True)
1395         c_dev = c.get_value(borrow=False, return_internal_type=True)
1396         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1397         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1398         f_nn = theano.function([], [], updates=[(a, l * tensor.dot(b, c))],
1399                                mode=self.mode)
1400         f_nt = theano.function([], [], updates=[(a, l * tensor.dot(b, c_t.T))],
1401                                mode=self.mode)
1402         f_tn = theano.function([], [], updates=[(a, l * tensor.dot(b_t.T, c))],
1403                                mode=self.mode)
1404         f_tt = theano.function([], [],
1405                                updates=[(a, l * tensor.dot(b_t.T, c_t.T))],
1406                                mode=self.mode)
1407         for step_signs in itertools_product((-1, 1), repeat=4):
1408             for step in (1, 2):
1409                 b_step1, b_step2, c_step1, c_step2 = (s * step
1410                                                       for s in step_signs)
1411                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1412                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1413                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1414                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1415                 a_n = l * np.dot(bv[::b_step1, ::b_step2],
1416                                  cv[::c_step1, ::c_step2])
1417                 f_nn()
1418                 assert np.allclose(a.get_value(), a_n)
1419                 f_nt()
1420                 assert np.allclose(a.get_value(), a_n)
1421                 f_tn()
1422                 assert np.allclose(a.get_value(), a_n)
1423                 f_tt()
1424     def test_dot22scalar(self):
1425         self.cmp_dot22scalar((3, 4), (<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>4, 5))
1426         self.cmp_dot22scalar((1, 4), (4, 5))
1427         self.cmp_dot22scalar((3, 4), (4, 1))
1428         self.cmp_dot22scalar((3, 1), (1, 1))
1429         self.cmp_dot22scalar((1, 4), (4, 1))
1430         self.cmp_dot22scalar((3, 1), (1, 5))
1431         self.cmp_dot22scalar((0, 4), (4, 5))
1432         self.cmp_dot22scalar((0, 4), (4, 1))
1433         self.cmp_dot22scalar((0, 1), (1, 5))
1434         self.cmp_dot22scalar((3, 0), (0</b></font>, 5))
1435         self.cmp_dot22scalar((0, 4), (4, 0))
1436         self.cmp_dot22scalar((0, 0), (0<font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0))
1437     def cmp_gemm(self, a_shp, b_shp, c_shp):
1438         av = self.rand(*a_shp)
1439         bv = self.rand(*b_shp)
1440         cv = self.rand(*c_shp)
1441         l = np.float32(0.2)
1442         a =</b></font> self.shared(av, 'a')
1443         b = self.shared(bv, 'b')
1444         c = self.shared(cv, 'c')
1445         a_t = self.shared(av.T, 'a.T')
1446         b_t = self.shared(bv.T, 'b.T')
1447         c_t = self.shared(cv.T, 'c.T')
1448         a_dev = a.get_value(borrow=False, return_internal_type=True)
1449         b_dev = b.get_value(borrow=False, return_internal_type=True)
1450         c_dev = c.get_value(borrow=False, return_internal_type=True)
1451         bt_dev = b_t.get_value(borrow=False, return_internal_type=True)
1452         ct_dev = c_t.get_value(borrow=False, return_internal_type=True)
1453         f_nnn = theano.function(
1454             [], [],
1455             updates=[(a, (l * a + tensor.dot(b, c)))],
1456             mode=self.mode)
1457         f_nnt = theano.function(
1458             [], [],
1459             updates=[(a, (l * a + tensor.dot(b, c_t.T)))],
1460             mode=self.mode)
1461         f_ntn = theano.function(
1462             [], [],
1463             updates=[(a, (l * a + tensor.dot(b_t.T, c)))],
1464             mode=self.mode)
1465         f_ntt = theano.function(
1466             [], [],
1467             updates=[(a, (l * a + tensor.dot(b_t.T, c_t.T)))],
1468             mode=self.mode)
1469         f_tnn = theano.function(
1470             [], [],
1471             updates=[(a_t, (l * a_t + tensor.dot(b, c).T))],
1472             mode=self.mode)
1473         f_tnt = theano.function(
1474             [], [],
1475             updates=[(a_t, (l * a_t + tensor.dot(b, c_t.T).T))],
1476             mode=self.mode)
1477         f_ttn = theano.function(
1478             [], [],
1479             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c).T))],
1480             mode=self.mode)
1481         f_ttt = theano.function(
1482             [], [],
1483             updates=[(a_t, (l * a_t + tensor.dot(b_t.T, c_t.T).T))],
1484             mode=self.mode)
1485         for step_signs in itertools_product((-1, 1), repeat=6):
1486             for step in (1, 2):
1487                 a_step1, a_step2, b_step1, b_step2, c_step1, c_step2 = \
1488                     (s * step for s in step_signs)
1489                 b.set_value(b_dev.copy()[::b_step1, ::b_step2], borrow=True)
1490                 c.set_value(c_dev.copy()[::c_step1, ::c_step2], borrow=True)
1491                 b_t.set_value(bt_dev.copy()[::b_step2, ::b_step1], borrow=True)
1492                 c_t.set_value(ct_dev.copy()[::c_step2, ::c_step1], borrow=True)
1493                 a_n = (l * av[::a_step1, ::a_step2] +
1494                        np.dot(bv[::b_step1, ::b_step2],
1495                               cv[::c_step1, ::c_step2]))
1496                 at_n = (l * av[::a_step1, ::a_step2].T +
1497                         np.dot(bv[::b_step1, ::b_step2],
1498                                cv[::c_step1, ::c_step2]).T)
1499                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1500                 f_nnn()
1501                 assert np.allclose(a.get_value(), a_n)
1502                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1503                 f_nnt()
1504                 assert np.allclose(a.get_value(), a_n)
1505                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1506                 f_ntn()
1507                 assert np.allclose(a.get_value(), a_n)
1508                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1509                 f_ntt()
1510                 assert np.allclose(a.get_value(), a_n)
1511                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1512                               borrow=True)
1513                 f_tnn()
1514                 assert np.allclose(a_t.get_value(), at_n)
1515                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1516                               borrow=True)
1517                 f_tnt()
1518                 assert np.allclose(a_t.get_value(), at_n)
1519                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1520                               borrow=True)
1521                 f_ttn()
1522                 assert np.allclose(a_t.get_value(), at_n)
1523                 a_t.set_value(transpose(a_dev.copy())[::a_step2, ::a_step1],
1524                               borrow=True)
1525                 f_ttt()
1526                 assert np.allclose(a_t.get_value(), at_n)
1527     def test_gemm(self):
1528         self.cmp_gemm((3, 5), (3, 4), (4, 5))
1529         self.cmp_gemm((1, 5), (1, 4), (4, 5))
1530         self.cmp_gemm((3, 1), (3, 4), (4, 1))
1531         self.cmp_gemm((3, 1), (3, 1), (1, 1))
1532         self.cmp_gemm((1, 1), (1, 4), (4, 1))
1533         self.cmp_gemm((3, 5), (3, 1), (1, 5))
1534         self.cmp_gemm((0, 5), (0, 4), (4, 5))
1535         self.cmp_gemm((0, 1), (0, 4), (4, 1))
1536         self.cmp_gemm((0, 5), (0, 1), (1, 5))
1537         self.cmp_gemm((3, 5), (3, 0), (0, 5))
1538         self.cmp_gemm((0, 0), (0, 4), (4, 0))
1539         self.cmp_gemm((0, 0), (0, 0), (0<font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0))
1540     def cmp_gemv(self, a_shp, b_shp, c_shp):
1541         av = self.rand(a_shp)
1542         bv = self.rand(*b_shp)
1543         cv = self.rand(c_shp)
1544         l = np.</b></font>float32(0.2)
1545         a = self.shared(av, 'a')
1546         b = self.shared(bv, 'b')
1547         c = self.shared(cv, 'c')
1548         b_t = self.shared(bv.T, 'b.T')
1549         a_dev = a.get_value(borrow=False, return_internal_type=True)
1550         b_dev = b.get_value(borrow=False, return_internal_type=True)
1551         c_dev = c.get_value(borrow=False, return_internal_type=True)
1552         f_n = theano.function([], [], updates=[(a, (a + l * tensor.dot(b, c)))],
1553                               mode=self.mode)
1554         f_t = theano.function([], [],
1555                               updates=[(a, (a + l * tensor.dot(b_t.T, c)))],
1556                               mode=self.mode)
1557         for step_signs in itertools_product((1, -1), repeat=4):
1558             for step in (1, 2):
1559                 a_step, b_step1, b_step2, c_step = (s * step
1560                                                     for s in step_signs)
1561                 a.set_value(a_dev.copy()[::a_step], borrow=True)
1562                 b.set_value(b_dev.copy()[::b_step1, ::b_step2],
1563                             borrow=True)
1564                 b_t.set_value(transpose(b_dev.copy())[::b_step2, ::b_step1],
1565                               borrow=True)
1566                 c.set_value(c_dev.copy()[::c_step], borrow=True)
1567                 a_n = (av[::a_step] +
1568                        l * np.dot(bv[::b_step1, ::b_step2],
1569                                   cv[::c_step]))
1570                 f_n()
1571                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
1572                 a.set_value(a_dev.copy()[::a_step], borrow=True)
1573                 f_t()
1574                 assert np.allclose(a.get_value(), a_n), (a.get_value(), a_n)
1575     def test_gemv(self):
1576         self.cmp_gemv(3, (3, 5), 5)
1577         self.cmp_gemv(1, (1, 5), 5)
1578         self.cmp_gemv(3, (3, 1), 1)
1579         self.cmp_gemv(0, (0, 5), 5)
1580         self.cmp_gemv(0, (0, 1), 1)
1581         self.cmp_gemv(1, (1, 0), 0)
1582         self.cmp_gemv(0, (0<font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>, 0), 0)
1583     def cmp_ger(self, a_shp, b_shp, c_shp):
1584         av = self.rand(*a_shp)
1585         bv = self.rand(b_shp)
1586         cv = self.rand(c_shp)
1587         l = np.</b></font>float32(0.2)
1588         a = self.shared(av, 'a')
1589         b = self.shared(bv, 'b')
1590         c = self.shared(cv, 'c')
1591         a_t = self.shared(av.T, 'a.T')
1592         a_dev = a.get_value(borrow=False, return_internal_type=True)
1593         b_dev = b.get_value(borrow=False, return_internal_type=True)
1594         c_dev = c.get_value(borrow=False, return_internal_type=True)
1595         f_n = theano.function(
1596             [], [],
1597             updates=[(a, (a + l * tensor.outer(b, c)))],
1598             mode=self.mode)
1599         f_t = theano.function(
1600             [], [],
1601             updates=[(a_t, (a_t + l * tensor.outer(b, c).T))],
1602             mode=self.mode)
1603         for step_signs in itertools_product((1, -1), repeat=4):
1604             for step in (1, 2):
1605                 a_step1, a_step2, b_step, c_step = (s * step
1606                                                     for s in step_signs)
1607                 a.set_value(a_dev.copy()[::a_step1, ::a_step2], borrow=True)
1608                 a_t.set_value(transpose(a_dev.copy())[::a_step1, ::a_step2],
1609                               borrow=True)
1610                 b.set_value(b_dev.copy()[::b_step], borrow=True)
1611                 c.set_value(c_dev.copy()[::c_step], borrow=True)
1612                 f_n()
1613                 n_n = (av[::a_step1, ::a_step2] +
1614                        l * np.outer(bv[::b_step], cv[::c_step]))
1615                 assert np.allclose(a.get_value(), n_n), (a.get_value(), n_n)
1616                 f_t()
1617                 n_t = (av.T[::a_step1, ::a_step2] +
1618                        l * np.outer(bv[::b_step], cv[::c_step]).T)
1619                 assert np.allclose(a_t.get_value(), n_t), (a_t.get_value(), n_t)
1620     def test_ger_strides(self):
1621         self.cmp_ger((3, 5), 3, 5)
1622         self.cmp_ger((1, 5), 1, 5)
1623         self.cmp_ger((3, 1), 3, 1)
1624         self.cmp_ger((0, 5), 0, 5)
1625         self.cmp_ger((3, 0), 3, 0)
1626         self.cmp_ger((0, 1), 0, 1)
1627         self.cmp_ger((1, 0), 1, 0)
1628         self.cmp_ger((0, 0), 0, 0)
1629     def test_gemm_non_contiguous(self):
1630         aval = np.ones((6, 2))
1631         cval = np.arange(7) + np.arange(0, .6, .1)[:, np.newaxis]
1632         a <font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.shared(aval[:3], borrow=True)
1633         b = theano.shared(bval[:, :5], borrow=True)
1634         s = theano.</b></font>tensor.scalar()
1635         upd_c = s * c + theano.tensor.dot<font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(a, b)
1636         f = theano.function([s], [], updates={c: upd_c})
1637         f(0)
1638         ref_output = np.ones((3, 5)) * 2
1639         unittest_tools.</b></font>assert_allclose(c.get_value(), ref_output)
1640 class test_infer_shape(unittest_tools.InferShapeTester):
1641     def test_dot22(self):
1642         x, y = T.matrices('xy')
1643         self._compile_and_check(
1644             [x, y], [T.blas._dot22(x, y)],
1645             [np.random.random((2, 3)).astype(config.floatX),
1646              np.random.random((3, 4)).astype(config.floatX)],
1647             T.blas.Dot22)
1648     def test_dot22scalar(self):
1649         x, y = T.matrices('xy')
1650         a = T.scalar('a')
1651         self._compile_and_check(
1652             [x, y, a], [T.blas._dot22scalar(x, y, a)],
1653             [np.random.random((2, 3)).astype(config.floatX),
1654              np.random.random((3, 4)).astype(config.floatX),
1655              np.asarray(0.5, dtype=config.floatX)],
1656             T.blas.Dot22Scalar)
1657     def test_gemm(self):
1658         x, y, z = T.matrices('xyz')
1659         a = T.scalar('a')
1660         b = T.scalar('b')
1661             [x, y, a, z, b], [T.blas.gemm(z, a, x, y, b)],
1662             [np.random.random((2, 3)).astype(config.floatX),
1663              np.random.random((3, 4)).astype(config<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.floatX),
1664              np.asarray(0.5, dtype=config.floatX),
1665              np.random.random((2, 4)).astype(config.</b></font>floatX),
1666              np.asarray(0.5, dtype=config.floatX)],
1667             T.blas.Gemm)
1668     def test_gemv(self):
1669         A = T.matrix('A')
1670         x, y = T.vectors('xy')
1671         a = T.scalar('a')
1672         b = T.scalar('b')
1673         self._compile_and_check(
1674             [y, a, A, x, b], [T.blas.gemv(y, a, A, x, b)],
1675             [np.random.random((2,)).astype(config.floatX),
1676              np.asarray(0.5, dtype=config.floatX),
1677              np.random.random((2, 3)).astype(config.floatX),
1678              np.random.random((3,)).astype(config.floatX),
1679              np.asarray(0.5, dtype=config.floatX)],
1680             T.blas.Gemv)
1681     def test_ger(self):
1682         A = T.matrix('A')
1683         x, y = T.vectors('xy')
1684         a = T.scalar('a')
1685         self._compile_and_check(
1686             [A, a, x, y], [T.blas.ger(A, a, x, y)],
1687             [np.random.random((2, 3)).astype(config.floatX),
1688              np.asarray(0.5, dtype=config.floatX),
1689              np.random.random((2,)).astype(config.floatX),
1690              np.random.random((3,)).astype(config.floatX)],
1691             T.blas.Ger)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
