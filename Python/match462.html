<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_abstract_conv.py &amp; check_dnn_conv.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_abstract_conv.py &amp; check_dnn_conv.py
      </h3>
<h1 align="center">
        7.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_abstract_conv.py (5.62056%)<th>check_dnn_conv.py (12.413475%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(377-389)<td><a href="#" name="0">(538-552)</a><td align="center"><font color="#ff0000">23</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1678-1679)<td><a href="#" name="1">(932-937)</a><td align="center"><font color="#f30000">22</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(7-34)<td><a href="#" name="2">(16-44)</a><td align="center"><font color="#d20000">19</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1313-1325)<td><a href="#" name="3">(961-965)</a><td align="center"><font color="#c70000">18</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(1009-1013)<td><a href="#" name="4">(198-202)</a><td align="center"><font color="#c70000">18</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(765-771)<td><a href="#" name="5">(555-557)</a><td align="center"><font color="#b10000">16</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1362-1372)<td><a href="#" name="6">(965-968)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(813-817)<td><a href="#" name="7">(231-237)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(701-706)<td><a href="#" name="8">(664-671)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(442-445)<td><a href="#" name="9">(822-824)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(923-928)<td><a href="#" name="10">(608-612)</a><td align="center"><font color="#9b0000">14</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(595-600)<td><a href="#" name="11">(223-228)</a><td align="center"><font color="#9b0000">14</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(1675-1676)<td><a href="#" name="12">(117-120)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1507-1511)<td><a href="#" name="13">(914-918)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(794-799)<td><a href="#" name="14">(145-147)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(505-507)<td><a href="#" name="15">(862-864)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(401-411)<td><a href="#" name="16">(804-808)</a><td align="center"><font color="#900000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_abstract_conv.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import unittest
3 import numpy as np
4 from nose.tools import assert_raises, assert_true
5 <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import theano
6 from theano import tensor
7 from theano import change_flags
8 from theano.gof.opt import check_stack_trace
9 from theano.tests import unittest_tools as utt
10 from theano.tensor.nnet import (corr, corr3d, conv2d_transpose,
11                                 abstract_conv as conv)
12 from theano.tensor.nnet.abstract_conv import (get_conv_output_shape,
13                                               get_conv_gradweights_shape,
14                                               get_conv_gradinputs_shape,
15                                               check_conv_gradinputs_shape,
16                                               assert_conv_shape,
17                                               assert_shape)
18 from theano.tensor.nnet.abstract_conv import AbstractConv2d
19 from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradInputs
20 from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradWeights
21 from theano.tensor.nnet.abstract_conv import bilinear_kernel_1D
22 from theano.tensor.nnet.abstract_conv import bilinear_kernel_2D
23 from theano.tensor.nnet.abstract_conv import bilinear_upsampling
24 from theano.tensor.nnet.abstract_conv import separable_conv2d, separable_conv3d
25 from theano.tensor.nnet.abstract_conv import causal_conv1d
26 from theano.tensor.nnet.corr import (CorrMM, CorrMM_gradWeights,
27                                      CorrMM_gradInputs)
28 from theano.tensor.nnet.corr3d import (Corr3dMM, Corr3dMM_gradWeights,
29                                        Corr3dMM_gradInputs)
30 def conv2d_corr(inputs, filters, border_mode=</b></font>"valid",
31                 subsample=(1, 1), conv_mode='conv',
32                 filter_dilation=(1, 1)):
33     if conv_mode == 'conv':
34         filters = filters[:, :, ::-1, ::-1]
35     return corr.CorrMM(border_mode,
36                        subsample,
37                        filter_dilation)(inputs, filters)
38 def conv2d_corr_gw(inputs, topgrad, filters_shape,
39                    border_mode="valid", subsample=(1, 1),
40                    conv_mode='conv', filter_dilation=(1, 1)):
41     rval = corr.CorrMM_gradWeights(border_mode,
42                                    subsample,
43                                    filter_dilation)(inputs, topgrad,
44                                                     filters_shape[2:])
45     if conv_mode == 'conv':
46         rval = rval[:, :, ::-1, ::-1]
47     return rval
48 def conv2d_corr_gi(filters, topgrad, inputs_shape,
49                    border_mode="valid", subsample=(1, 1),
50                    conv_mode='conv', filter_dilation=(1, 1)):
51     if conv_mode == 'conv':
52         filters = filters[:, :, ::-1, ::-1]
53     return corr.CorrMM_gradInputs(border_mode,
54                                   subsample,
55                                   filter_dilation)(filters,
56                                                    topgrad,
57                                                    inputs_shape[2:])
58 def conv3d_corr(inputs, filters, border_mode="valid",
59                 subsample=(1, 1, 1), conv_mode='conv',
60                 filter_dilation=(1, 1, 1)):
61     if conv_mode == 'conv':
62         filters = filters[:, :, ::-1, ::-1, ::-1]
63     return corr3d.Corr3dMM(border_mode,
64                            subsample,
65                            filter_dilation)(inputs, filters)
66 def conv3d_corr_gw(inputs, topgrad, filters_shape,
67                    border_mode="valid", subsample=(1, 1, 1),
68                    conv_mode='conv', filter_dilation=(1, 1, 1)):
69     rval = corr3d.Corr3dMM_gradWeights(border_mode,
70                                        subsample,
71                                        filter_dilation)(inputs, topgrad,
72                                                         filters_shape[2:])
73     if conv_mode == 'conv':
74         rval = rval[:, :, ::-1, ::-1, ::-1]
75     return rval
76 def conv3d_corr_gi(filters, topgrad, inputs_shape,
77                    border_mode="valid", subsample=(1, 1, 1),
78                    conv_mode='conv', filter_dilation=(1, 1, 1)):
79     if conv_mode == 'conv':
80         filters = filters[:, :, ::-1, ::-1, ::-1]
81     return corr3d.Corr3dMM_gradInputs(border_mode,
82                                       subsample,
83                                       filter_dilation)(filters,
84                                                        topgrad,
85                                                        inputs_shape[2:])
86 class TestGetConvOutShape(unittest.TestCase):
87     def test_basic(self):
88         image_shape, kernel_shape = (3, 2, 12, 9), (4, 2, 5, 6)
89         sub_sample = (1, 2)
90         filter_dilation = (2, 1)
91         test1_params = get_conv_output_shape(
92             image_shape, kernel_shape, 'valid', sub_sample, filter_dilation)
93         test2_params = get_conv_output_shape(
94             image_shape, kernel_shape, 'half', sub_sample, filter_dilation)
95         test3_params = get_conv_output_shape(
96             image_shape, kernel_shape, 'full', sub_sample, filter_dilation)
97         test4_params = get_conv_output_shape(
98             image_shape, kernel_shape, (1, 2), sub_sample, filter_dilation)
99         self.assertTrue(test1_params == (3, 4, 4, 2))
100         self.assertTrue(test2_params == (3, 4, 12, 5))
101         self.assertTrue(test3_params == (3, 4, 20, 7))
102         self.assertTrue(test4_params == (3, 4, 6, 4))
103     def test_basic_3d(self):
104         image_shape, kernel_shape = (3, 2, 12, 9, 7), (4, 2, 5, 6, 4)
105         sub_sample = (1, 2, 1)
106         filter_dilation = (2, 1, 1)
107         test1_params = get_conv_output_shape(
108             image_shape, kernel_shape, 'valid', sub_sample, filter_dilation)
109         test2_params = get_conv_output_shape(
110             image_shape, kernel_shape, 'half', sub_sample, filter_dilation)
111         test3_params = get_conv_output_shape(
112             image_shape, kernel_shape, 'full', sub_sample, filter_dilation)
113         test4_params = get_conv_output_shape(
114             image_shape, kernel_shape, (1, 2, 3), sub_sample, filter_dilation)
115         self.assertTrue(test1_params == (3, 4, 4, 2, 4))
116         self.assertTrue(test2_params == (3, 4, 12, 5, 8))
117         self.assertTrue(test3_params == (3, 4, 20, 7, 10))
118         self.assertTrue(test4_params == (3, 4, 6, 4, 10))
119 class TestConvGradInputsShape(unittest.TestCase):
120     def test_check_shape(self):
121         for i in range(1, 20):
122             for k in range(1, 10):
123                 for b in ('valid', 'half', 'full', (0, 2)):
124                     for s in (1, 2, 3):
125                         for d in (1, 2, 3):
126                             image_shape = (59, 61, i, i)
127                             kernel_shape = (67, 61, k, k)
128                             computed_shape = get_conv_output_shape(
129                                 image_shape, kernel_shape, b, (s, s), (d, d))
130                             self.assertTrue(check_conv_gradinputs_shape(
131                                 image_shape, kernel_shape, computed_shape, b, (s, s), (d, d)))
132                             trial_shape = (None, None, computed_shape[2], None)
133                             self.assertTrue(check_conv_gradinputs_shape(
134                                 image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))
135                             trial_shape = (1, 1, computed_shape[2], computed_shape[3])
136                             self.assertFalse(check_conv_gradinputs_shape(
137                                 image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))
138                             for o in (-3, -2, -1, 1, 2, 3):
139                                 trial_shape = (computed_shape[0], computed_shape[1],
140                                                computed_shape[2] + o, computed_shape[3] + o)
141                                 self.assertFalse(check_conv_gradinputs_shape(
142                                     image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))
143     def test_get_shape(self):
144         for i in range(1, 20):
145             for k in range(1, 10):
146                 for b in ('valid', 'half', 'full', (0, 2)):
147                     for d in (1, 2, 3):
148                         image_shape = (59, 61, i, i)
149                         kernel_shape = (67, 61, k, k)
150                         output_shape = get_conv_output_shape(
151                             image_shape, kernel_shape, b, (1, 1), (d, d))
152                         computed_image_shape = get_conv_gradinputs_shape(
153                             kernel_shape, output_shape, b, (1, 1), (d, d))
154                         self.assertEqual(computed_image_shape, image_shape)
155                         computed_image_shape = get_conv_gradinputs_shape(
156                             kernel_shape, output_shape, b, (2, 3), (d, d))
157                         image_shape_with_None = image_shape[:2] + (None, None)
158                         self.assertEqual(computed_image_shape, image_shape_with_None)
159                         computed_kernel_shape = get_conv_gradweights_shape(
160                             image_shape, output_shape, b, (1, 1), (d, d))
161                         if b == 'half':
162                             kernel_shape_with_None = kernel_shape[:2] + (None, None)
163                             self.assertEqual(computed_kernel_shape, kernel_shape_with_None)
164                         else:
165                             self.assertEqual(computed_kernel_shape, kernel_shape)
166                         computed_kernel_shape = get_conv_gradweights_shape(
167                             kernel_shape, output_shape, b, (2, 3), (d, d))
168                         kernel_shape_with_None = kernel_shape[:2] + (None, None)
169                         self.assertEqual(computed_kernel_shape, kernel_shape_with_None)
170 class TestAssertConvShape(unittest.TestCase):
171     def test_basic(self):
172         shape = tuple(tensor.iscalar() for i in range(4))
173         f = theano.function(shape, assert_conv_shape(shape))
174         self.assertEqual([1, 2, 3, 4], f(1, 2, 3, 4))
175         self.assertEqual([0, 0, 1, 1], f(0, 0, 1, 1))
176         assert_raises(AssertionError, f, 3, 3, 3, 0)
177         assert_raises(AssertionError, f, 3, 3, 0, 3)
178         assert_raises(AssertionError, f, 3, 3, -1, 3)
179         assert_raises(AssertionError, f, 3, -1, 3, 3)
180         assert_raises(AssertionError, f, -1, 3, 3, 3)
181 class TestAssertShape(unittest.TestCase):
182     @change_flags([("conv.assert_shape", True)])
183     def test_basic(self):
184         x = tensor.tensor4()
185         s1 = tensor.iscalar()
186         s2 = tensor.iscalar()
187         expected_shape = [None, s1, s2, None]
188         f = theano.function([x, s1, s2], assert_shape(x, expected_shape))
189         v = np.zeros((3, 5, 7, 11), dtype='float32')
190         self.assertEqual(0, np.sum(f(v, 5, 7)))
191         assert_raises(AssertionError, f, v, 5, 0)
192         assert_raises(AssertionError, f, v, 5, 9)
193         assert_raises(AssertionError, f, v, 0, 7)
194         assert_raises(AssertionError, f, v, 7, 7)
195     @change_flags([("conv.assert_shape", True)])
196     def test_shape_check_conv2d(self):
197         input = tensor.tensor4()
198         filters = tensor.tensor4()
199         out = conv.conv2d(input, filters,
200                           input_shape=(3, 5, 7, 11),
201                           filter_shape=(7, 5, 3, 3))
202         f = theano.function([input, filters], out)
203         assert_raises(AssertionError, f,
204                       np.zeros((3, 5, 9, 11), dtype='float32'),
205                       np.zeros((7, 5, 3, 3), dtype='float32'))
206         assert_raises(AssertionError, f,
207                       np.zeros((3, 5, 7, 11), dtype='float32'),
208                       np.zeros((7, 5, 2, 2), dtype='float32'))
209     @change_flags([("conv.assert_shape", True)])
210     def test_shape_check_conv3d(self):
211         if theano.config.cxx == "":
212             raise SkipTest("test needs cxx")
213         input = tensor.tensor5()
214         filters = tensor.tensor5()
215         out = conv.conv3d(input, filters,
216                           input_shape=(3, 5, 7, 11, 13),
217                           filter_shape=(7, 5, 3, 3, 3))
218         f = theano.function([input, filters], out)
219         assert_raises(AssertionError, f,
220                       np.zeros((3, 5, 9, 11, 13), dtype='float32'),
221                       np.zeros((7, 5, 3, 3, 3), dtype='float32'))
222         assert_raises(AssertionError, f,
223                       np.zeros((3, 5, 7, 11, 13), dtype='float32'),
224                       np.zeros((7, 5, 2, 2, 2), dtype='float32'))
225     @change_flags([("conv.assert_shape", True)])
226     def test_shape_check_conv2d_grad_wrt_inputs(self):
227         output_grad = tensor.tensor4()
228         filters = tensor.tensor4()
229         out = conv.conv2d_grad_wrt_inputs(output_grad, filters,
230                                           input_shape=(None, None, 7, 11),
231                                           filter_shape=(7, 5, 3, 3))
232         f = theano.function([output_grad, filters], out)
233         assert_raises(AssertionError, f,
234                       np.zeros((3, 6, 5, 9), dtype='float32'),
235                       np.zeros((7, 6, 3, 3), dtype='float32'))
236     @change_flags([("conv.assert_shape", True)])
237     def test_shape_check_conv3d_grad_wrt_inputs(self):
238         if theano.config.cxx == "":
239             raise SkipTest("test needs cxx")
240         output_grad = tensor.tensor5()
241         filters = tensor.tensor5()
242         out = conv.conv3d_grad_wrt_inputs(output_grad, filters,
243                                           input_shape=(None, None, 7, 11, 13),
244                                           filter_shape=(7, 5, 3, 3, 3))
245         f = theano.function([output_grad, filters], out)
246         assert_raises(AssertionError, f,
247                       np.zeros((3, 6, 5, 9, 11), dtype='float32'),
248                       np.zeros((7, 6, 3, 3, 3), dtype='float32'))
249     @change_flags([("conv.assert_shape", True)])
250     def test_shape_check_conv2d_grad_wrt_weights(self):
251         input = tensor.tensor4()
252         output_grad = tensor.tensor4()
253         out = conv.conv2d_grad_wrt_weights(input, output_grad,
254                                            filter_shape=(None, None, 3, 3),
255                                            input_shape=(3, 5, 7, 11))
256         f = theano.function([input, output_grad], out)
257         assert_raises(AssertionError, f,
258                       np.zeros((3, 6, 7, 11), dtype='float32'),
259                       np.zeros((3, 7, 5, 9), dtype='float32'))
260     @change_flags([("conv.assert_shape", True)])
261     def test_shape_check_conv3d_grad_wrt_weights(self):
262         if theano.config.cxx == "":
263             raise SkipTest("test needs cxx")
264         input = tensor.tensor5()
265         output_grad = tensor.tensor5()
266         out = conv.conv3d_grad_wrt_weights(input, output_grad,
267                                            filter_shape=(None, None, 3, 3, 3),
268                                            input_shape=(3, 5, 7, 11, 13))
269         f = theano.function([input, output_grad], out)
270         assert_raises(AssertionError, f,
271                       np.zeros((3, 6, 7, 11, 13), dtype='float32'),
272                       np.zeros((3, 7, 5, 9, 11), dtype='float32'))
273 class BaseTestConv(object):
274     def get_output_shape(self, inputs_shape, filters_shape,
275                          subsample, border_mode, filter_dilation):
276         dil_filters = tuple((s - 1) * d + 1 for s, d in zip(filters_shape[2:],
277                                                             filter_dilation))
278         if border_mode == "valid":
279             border_mode = (0,) * (len(inputs_shape) - 2)
280         if border_mode == "half":
281             border_mode = tuple(d // 2 for d in dil_filters)
282         if border_mode == "full":
283             border_mode = tuple(d - 1 for d in dil_filters)
284         batch_size = inputs_shape[0]
285         num_filters = filters_shape[0]
286         return ((batch_size, num_filters,) +
287                 tuple(None if i is None or k is None
288                       else ((i + 2 * pad - ((k - 1) * fd + 1)) // d + 1)
289                       for i, k, d, pad, fd in zip(inputs_shape[2:],
290                                                   filters_shape[2:],
291                                                   subsample, border_mode,
292                                                   filter_dilation)))
293     def run_fwd(self, inputs_shape, filters_shape,
294                 conv_fn, conv_op, ref,
295                 subsample=None, verify_grad=True, mode=None,
296                 border_mode='valid', filter_flip=True,
297                 provide_shape=False, target_op=None,
298                 check_trace=False, filter_dilation=None):
299         if subsample is None:
300             subsample = (1,) * (len(inputs_shape) - 2)
301             filter_dilation = (1,) * (len(inputs_shape) - 2)
302         inputs_val <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(inputs_shape).astype('float32')
303         filters_val = np.random.random(filters_shape).astype('float32')
304         inputs_val /= 10
305         filters_val /= 10
306         inputs = self.shared(inputs_val)
307         filters = self.shared(filters_val)
308         if provide_shape:
309             imshp = inputs_shape
310             kshp =</b></font> filters_shape
311         else:
312             imshp = None
313             kshp = None
314         if filter_flip:
315             conv_mode = 'conv'
316         else:
317             conv_mode = 'cross'
318                     border_mode=border_mode,
319                     subsample=subsample,
320                     conv_mode<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=conv_mode,
321                     filter_dilation=filter_dilation)
322         c = conv_fn(inputs, filters,
323                     border_mode=border_mode,
324                     subsample=subsample,
325                     filter_flip=filter_flip,
326                     input_shape=imshp,
327                     filter_shape=kshp,
328                     filter_dilation=filter_dilation)
329         f_ref = theano.function(</b></font>[], c_ref, mode='FAST_RUN')
330         f = theano.function([], c, mode=mode)
331         if target_op is not None:
332             assert any([isinstance(n.op, target_op) for n
333                         in f.maker.fgraph.toposort()])
334             if check_trace:
335                 assert_true(check_stack_trace(f, ops_to_check=target_op))
336         res_ref = np.array(f_ref())
337         res = np.array(f())
338         utt.assert_allclose(res_ref, res)
339         if verify_grad and inputs_val.size &gt; 0 and filters_val.size &gt; 0 and res.size &gt; 0:
340             utt.verify_grad(conv_op(border_mode=border_mode,
341                                     imshp=imshp, kshp=kshp,
342                                     subsample=subsample,
343                                     filter_dilation=filter_dilation),
344                             [inputs_val, filters_val],
345                             mode=mode)
346     def run_gradweight(self, inputs_shape, filters_shape, output_shape,
347                        gradWeights_fn, ref, subsample=None,
348                        filter_flip=True, verify_grad=True, mode=None,
349                        border_mode='valid', provide_shape=False,
350                        target_op=None, check_trace=False,
351                        filter_dilation=None):
352         if subsample is None:
353             subsample = (1,) * (len(inputs_shape) - 2)
354             filter_dilation = (1,) * (len(inputs_shape) - 2)
355         inputs_val <font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(inputs_shape).astype('float32')
356         output_val = np.random.random(output_shape).astype('float32')
357         inputs = self.shared(</b></font>inputs_val)
358         output = self.shared(output_val)
359         if provide_shape:
360             imshp = inputs_shape
361             kshp = filters_shape
362         else:
363             imshp = None
364             kshp = None
365         if filter_flip:
366             conv_mode = 'conv'
367         else:
368             conv_mode = 'cross'
369         c = gradWeights_fn(border_mode=border_mode,
370                            filter_flip=filter_flip,
371                            subsample=subsample,
372                            imshp=imshp, kshp=kshp,
373                            filter_dilation=filter_dilation)
374         c = c(inputs, output, filters_shape[2:])
375         c_ref = ref(inputs, output,
376                     filters_shape,
377                     border_mode=border_mode,
378                     subsample=subsample,
379                     conv_mode=conv_mode,
380                     filter_dilation=filter_dilation)
381         f = theano.function([], c, mode=mode)
382         f_ref = theano.function([], c_ref, mode='FAST_RUN')
383         if target_op is not None:
384             assert any([isinstance(n.op, target_op) for n
385                         in f.maker.fgraph.toposort()])
386             if check_trace:
387                 assert_true(check_stack_trace(f, ops_to_check=target_op))
388         res_ref = np.array(f_ref())
389         res = np.array(f())
390         utt.assert_allclose(res_ref, res)
391         def abstract_conv_gradweight(inputs_val, output_val):
392             conv_op = gradWeights_fn(border_mode=border_mode,
393                                      subsample=subsample,
394                                      filter_dilation=filter_dilation)
395             return conv_op(inputs_val, output_val, filters_shape[2:])
396         if verify_grad and inputs_val.size &gt; 0 and output_val.size &gt; 0 and res.size &gt; 0:
397             utt.verify_grad(abstract_conv_gradweight,
398                             [inputs_val, output_val],
399                             mode=mode, eps=1)
400     def run_gradinput(self, inputs_shape, filters_shape, output_shape,
401                       gradInputs_fn, ref,
402                       subsample=None, filter_flip=True,
403                       verify_grad=True, mode=None, border_mode='valid',
404                       provide_shape=False, target_op=None,
405                       check_trace=False, filter_dilation=None):
406         if subsample is None:
407             subsample = (1,) * (len(inputs_shape) - 2)
408             filter_dilation = (1,) * (len(inputs_shape) - 2)
409         output_val <font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(output_shape).astype('float32')
410         filters_val = np.random.random(filters_shape).astype('float32')
411         output =</b></font> self.shared(output_val)
412         filters = self.shared(filters_val)
413         if provide_shape:
414             imshp = inputs_shape
415             kshp = filters_shape
416         else:
417             imshp = None
418             kshp = None
419         if filter_flip:
420             conv_mode = 'conv'
421         else:
422             conv_mode = 'cross'
423         c = gradInputs_fn(border_mode=border_mode,
424                           subsample=subsample,
425                           filter_flip=filter_flip,
426                           imshp=imshp, kshp=kshp,
427                           filter_dilation=filter_dilation)
428         c = c(filters, output, inputs_shape[2:])
429         f = theano.function([], c, mode=mode)
430         if ref is not None:
431             c_ref = ref(filters, output, inputs_shape,
432                         border_mode=border_mode, subsample=subsample,
433                         conv_mode=conv_mode, filter_dilation=filter_dilation)
434             f_ref = theano.function([], c_ref, mode='FAST_RUN')
435         if target_op is not None:
436             assert any([isinstance(n.op, target_op) for n
437                         in f.maker.fgraph.toposort()])
438             if check_trace:
439                 assert_true(check_stack_trace(f, ops_to_check=target_op))
440         res = np.array(f())
441         if ref is not None:
442             res_ref = np.array(f_ref())
443             utt.assert_allclose(res_ref, res)
444         def abstract_conv_gradinputs(filters_val, output_val):
445             conv_op = gradInputs_fn(border_mode=border_mode,
446                                     subsample=subsample,
447                                     filter_dilation=filter_dilation)
448             return conv_op(filters_val, output_val, inputs_shape[2:])
449         if verify_grad and filters_val.size &gt; 0 and output_val.size &gt; 0 and res.size &gt; 0:
450             utt.verify_grad(abstract_conv_gradinputs,
451                             [filters_val, output_val],
452                             mode=mode, eps=1)
453     def test_all(self):
454         if type(self) is BaseTestConv:
455             raise SkipTest("base class")
456         ds = self.default_subsamples
457         db = self.default_border_mode
458         dflip = self.default_filter_flip
459         dprovide_shape = self.default_provide_shape
460         for (i, f) in zip(self.inputs_shapes, self.filters_shapes):
461             for provide_shape in self.provide_shape:
462                 yield (self.tcase, i, f, ds, db, dflip, provide_shape)
463             if min(i) &gt; 0 and min(f) &gt; 0:
464                 for fd in self.filters_dilations:
465                     for s in self.subsamples:
466                         for b in self.border_modes:
467                             yield (self.tcase, i, f, s, b, dflip,
468                                    dprovide_shape, fd)
469                 for flip in self.filter_flip:
470                     yield (self.tcase, i, f, ds, db, flip, dprovide_shape)
471 class BaseTestConv2d(BaseTestConv):
472     @classmethod
473     def setup_class(cls):
474         cls.inputs_shapes = [(8, 1, 6, 6), (8, 1, 8, 8), (2, 1, 7, 7),
475                              (6, 1, 10, 11), (2, 1, 6, 5), (1, 5, 9, 9),
476                              (0, 1, 6, 6), (1, 0, 6, 6), (1, 1, 6, 6)]
477         cls.filters_shapes = [(5, 1, 2, 2), (4, 1, 3, 3), (2, 1, 3, 3),
478                               (1, 1, 2, 3), (4, 1, 1, 3), (4, 5, 3, 2),
479                               (1, 1, 2, 2), (1, 0, 2, 2), (0, 1, 2, 2)]
480         cls.subsamples = [(1, 1), (2, 2), (2, 4)]
481         cls.default_subsamples = (1, 1)
482         cls.default_filters_dilations = (1, 1)
483         cls.border_modes = ["valid", "half", "full", (0, 0), (1, 1), (5, 5), (5, 2)]
484         cls.default_border_mode <font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (0, 0)
485         cls.filter_flip = [True, False]
486         cls.default_filter_flip = True
487         cls.provide_shape = [True, False]
488         cls.default_provide_shape = True
489         cls.shared =</b></font> staticmethod(theano.compile.shared)
490     def test_gradinput_arbitrary_output_shapes(self):
491         input_shape = (2, 1, 7, 7)
492         filter_shape = (2, 1, 3, 3)
493         for output_shape in [(2, 2, 8, 8), (2, 2, 9, 9), (2, 2, 12, 12)]:
494             for border_mode in ["valid", "half", "full"]:
495                 computed_shape = get_conv_output_shape(
496                     input_shape, filter_shape, border_mode, self.default_subsamples, self.default_filters_dilations)
497                 if tuple(computed_shape) == output_shape:
498                     yield (self.tcase_gi,
499                            input_shape,
500                            filter_shape,
501                            output_shape,
502                            self.default_subsamples,
503                            border_mode,
504                            True,
505                            True,
506                            self.default_filters_dilations,
507                            False)
508                 else:
509                     yield (self.tcase_gi,
510                            input_shape,
511                            filter_shape,
512                            output_shape,
513                            self.default_subsamples,
514                            border_mode,
515                            True,
516                            True,
517                            self.default_filters_dilations,
518                            True)
519     def test_gradinput_impossible_output_shapes(self):
520         def run_for_output_offsets(image_shape, kernel_shape, s, border_mode, d):
521             for o in (-3, -1, 1, 2):
522                 output_shape = (1, 1, computed_shape[2] + o, computed_shape[3] + o)
523                 self.tcase_gi(image_shape, kernel_shape, output_shape,
524                               (s, s), border_mode, True, True, (d, d), True)
525         for (i, k) in ((1, 1), (1, 2), (2, 1), (4, 2), (4, 3), (7, 3), (9, 5)):
526             for border_mode in ('valid', 'half', 'full', (0, 2)):
527                 for (s, d) in ((1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (1, 3)):
528                     image_shape = (1, 1, i, i)
529                     kernel_shape = (1, 1, k, k)
530                     computed_shape = get_conv_output_shape(
531                         image_shape, kernel_shape, border_mode, (s, s), (d, d))
532                     yield (run_for_output_offsets,
533                            image_shape, kernel_shape, s, border_mode, d)
534     def run_fwd(self, inputs_shape, filters_shape,
535                 conv_fn=conv.conv2d, conv_op=conv.AbstractConv2d,
536                 ref=conv2d_corr, **kwargs):
537         super(BaseTestConv2d, self).run_fwd(
538             inputs_shape=inputs_shape,
539             filters_shape=filters_shape,
540             conv_fn=conv_fn,
541             conv_op=conv_op,
542             ref=ref, **kwargs)
543     def run_gradweight(self, inputs_shape, filters_shape, output_shape,
544                        gradWeights_fn=conv.AbstractConv2d_gradWeights,
545                        ref=conv2d_corr_gw, **kwargs):
546         super(BaseTestConv2d, self).run_gradweight(
547             inputs_shape=inputs_shape,
548             filters_shape=filters_shape,
549             output_shape=output_shape,
550             gradWeights_fn=gradWeights_fn,
551             ref=ref, **kwargs)
552     def run_gradinput(self, inputs_shape, filters_shape, output_shape,
553                       gradInputs_fn=conv.AbstractConv2d_gradInputs,
554                       ref=conv2d_corr_gi, **kwargs):
555         super(BaseTestConv2d, self).run_gradinput(
556             inputs_shape=inputs_shape,
557             filters_shape=filters_shape,
558             output_shape=output_shape,
559             gradInputs_fn=gradInputs_fn,
560             ref=ref, **kwargs)
561 class TestCorrConv2d(BaseTestConv2d):
562     @classmethod
563     def setup_class(cls):
564         BaseTestConv2d.setup_class()
565     def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1)):
566         o = self.get_output_shape(i, f, s, b, fd)
567                 theano.config.mode == "FAST_COMPILE"):
568             raise SkipTest("Need blas to test conv2d")
569         self.run_fwd<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f, subsample=s,
570                      verify_grad=True, provide_shape=provide_shape,
571                      border_mode=b, filter_flip=flip,
572                      target_op=CorrMM, check_trace=True,
573                      filter_dilation=fd)
574         self.run_gradweight(inputs_shape=i, filters_shape=</b></font>f,
575                             output_shape=o, subsample=s, verify_grad=True,
576                             provide_shape=provide_shape, border_mode=b,
577                             filter_flip=flip, target_op=CorrMM_gradWeights,
578                             check_trace=True, filter_dilation=fd)
579         self.run_gradinput(inputs_shape=i, filters_shape=f,
580                            output_shape=o, subsample=s, verify_grad=True,
581                            provide_shape=provide_shape, border_mode=b,
582                            filter_flip=flip, target_op=CorrMM_gradInputs,
583                            check_trace=True, filter_dilation=fd)
584     def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1), expect_error=False):
585         if (not theano.config.cxx or
586                 theano.config.mode == "FAST_COMPILE"):
587             raise SkipTest("Need blas to test conv2d")
588         if not expect_error:
589             self.run_gradinput(inputs_shape=i, filters_shape=f,
590                                output_shape=o, subsample=s, verify_grad=True,
591                                provide_shape=provide_shape, border_mode=b,
592                                filter_flip=flip, target_op=CorrMM_gradInputs,
593                                check_trace=True, filter_dilation=fd)
594         else:
595             assert_raises(ValueError,
596                           self.run_gradinput,
597                           inputs_shape=i, filters_shape=f,
598                           output_shape=o, subsample=s, verify_grad=False,
599                           provide_shape=provide_shape, border_mode=b,
600                           filter_flip=flip, target_op=CorrMM_gradInputs,
601                           ref=None, check_trace=True, filter_dilation=fd)
602 class TestAbstractConvNoOptim(BaseTestConv2d):
603     @classmethod
604     def setup_class(cls):
605         BaseTestConv2d.setup_class()
606         cls.inputs_shapes = [(8, 1, 6, 6)]
607         cls.filters_shapes = [(5, 1, 2, 2)]
608         cls.subsamples = [(1, 1), (2, 2)]
609         cls.filters_dilations = [(1, 1), (1, 2), (2, 1)]
610         cls.border_modes = ["valid", "half", "full"]
611         cls.filter_flip = [True]
612         cls.provide_shape = [False]
613         if not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
614             raise SkipTest("SciPy needed")
615     def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1)):
616         o = self.get_output_shape(i, f, s, b, fd)
617         mode = theano.Mode(optimizer=None)
618         if not theano.config.cxx:
619             raise SkipTest("Need cxx to test conv2d")
620         self.run_fwd(inputs_shape=i, filters_shape=f, subsample=s,
621                      verify_grad=True, provide_shape=provide_shape,
622                      target_op=None, check_trace=True,
623                      filter_dilation=fd, mode=mode)
624         self.run_gradweight<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f,
625                             output_shape=o, subsample=s, verify_grad=True,
626                             provide_shape=provide_shape, border_mode=b,
627                             filter_flip=flip, target_op=None,
628                             check_trace=True, filter_dilation=fd,
629                             mode=mode)
630         self.run_gradinput(inputs_shape=</b></font>i, filters_shape=f,
631                            output_shape=o, subsample=s, verify_grad=True,
632                            provide_shape=provide_shape, border_mode=b,
633                            filter_flip=flip, target_op=None,
634                            check_trace=True, filter_dilation=fd,
635                            mode=mode)
636     def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1), expect_error=False):
637         if not theano.config.cxx:
638             raise SkipTest("Need cxx to test conv2d")
639         mode = theano.Mode(optimizer=None)
640         if not expect_error:
641             self.run_gradinput(inputs_shape=i, filters_shape=f,
642                                output_shape=o, subsample=s, verify_grad=True,
643                                provide_shape=provide_shape, border_mode=b,
644                                filter_flip=flip, target_op=None,
645                                check_trace=True, filter_dilation=fd,
646                                mode=mode)
647             assert_raises(ValueError,
648                           self.run_gradinput,
649                           inputs_shape<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=i, filters_shape=f,
650                           output_shape=o, subsample=s, verify_grad=False,
651                           provide_shape=provide_shape, border_mode=b,
652                           filter_flip=flip, target_op=None,
653                           check_trace=True, filter_dilation=fd,
654                           ref=None, mode=</b></font>mode)
655 class BaseTestConv3d(BaseTestConv):
656     @classmethod
657     def setup_class(cls):
658         cls.inputs_shapes = [(2, 1, 5, 5, 5), (1, 2, 7, 5, 6),
659                              (0, 1, 5, 5, 5), (1, 0, 5, 5, 5), (1, 1, 5, 5, 5)]
660         cls.filters_shapes = [(2, 1, 2, 2, 2), (1, 2, 2, 1, 3),
661                               (1, 1, 2, 2, 2), (1, 0, 2, 2, 2), (0, 1, 2, 2, 2)]
662         cls.default_subsamples = (1, 1, 1)
663         cls.filters_dilations = [(1, 1, 1), (1, 2, 1), (2, 1, 2)]
664         cls<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.default_filters_dilations = (1, 1, 1)
665         cls.border_modes = ["valid", "half", "full", (0, 0, 0), (2, 2, 3)]
666         cls.default_border_mode = (0, 0, 0)
667         cls.filter_flip = [True, False]
668         cls.</b></font>default_filter_flip = True
669         cls.provide_shape = [True, False]
670         cls.default_provide_shape = True
671         cls.shared = staticmethod(theano.compile.shared)
672     def test_gradinput_arbitrary_output_shapes(self):
673         input_shape = (2, 1, 7, 7, 7)
674         filter_shape = (1, 1, 3, 3, 3)
675         for output_shape in [(2, 1, 8, 8, 8), (2, 1, 9, 9, 9), (2, 1, 12, 12, 12)]:
676             for border_mode in ["valid", "half", "full"]:
677                 computed_shape = get_conv_output_shape(
678                     input_shape, filter_shape, border_mode, self.default_subsamples, self.default_filters_dilations)
679                 if tuple(computed_shape) == output_shape:
680                     yield (self.tcase_gi,
681                            input_shape,
682                            filter_shape,
683                            output_shape,
684                            self.default_subsamples,
685                            border_mode,
686                            True,
687                            True,
688                            self.default_filters_dilations,
689                            False)
690                 else:
691                     yield (self.tcase_gi,
692                            input_shape,
693                            filter_shape,
694                            output_shape,
695                            self.default_subsamples,
696                            border_mode,
697                            True,
698                            True,
699                            self.default_filters_dilations,
700                            True)
701     def test_gradinput_impossible_output_shapes(self):
702         def run_for_output_offsets(image_shape, kernel_shape, s, border_mode, d):
703             for o in (-3, -1, 1, 2):
704                 output_shape = (1, 1, computed_shape[2] + o,
705                                 computed_shape[3] + o, computed_shape[4] + o)
706                 self.tcase_gi(image_shape, kernel_shape, output_shape,
707                               (s, s), border_mode, True, True, (d, d), True)
708         for (i, k) in ((1, 1), (1, 2), (2, 1), (4, 2), (4, 3), (7, 3), (9, 5)):
709             for border_mode in ('valid', 'half', 'full', (0, 2, 1)):
710                 for (s, d) in ((1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (1, 3)):
711                     image_shape = (1, 1, i, i, i)
712                     kernel_shape = (1, 1, k, k, k)
713                     computed_shape = get_conv_output_shape(
714                         image_shape, kernel_shape, border_mode, (s, s, s), (d, d, d))
715                     yield (run_for_output_offsets,
716                            image_shape, kernel_shape, s, border_mode, d)
717     def run_fwd(self, inputs_shape, filters_shape,
718                 conv_fn=conv.conv3d, conv_op=conv.AbstractConv3d,
719                 ref=conv3d_corr, **kwargs):
720         super(BaseTestConv3d, self).run_fwd(
721             inputs_shape=inputs_shape,
722             filters_shape=filters_shape,
723             conv_fn=conv_fn,
724             conv_op=conv_op,
725             ref=ref, **kwargs)
726     def run_gradweight(self, inputs_shape, filters_shape, output_shape,
727                        gradWeights_fn=conv.AbstractConv3d_gradWeights,
728                        ref=conv3d_corr_gw, **kwargs):
729         super(BaseTestConv3d, self).run_gradweight(
730             inputs_shape=inputs_shape,
731             filters_shape=filters_shape,
732             output_shape=output_shape,
733             gradWeights_fn=gradWeights_fn,
734             ref=ref, **kwargs)
735     def run_gradinput(self, inputs_shape, filters_shape, output_shape,
736                       gradInputs_fn=conv.AbstractConv3d_gradInputs,
737                       ref=conv3d_corr_gi, **kwargs):
738         super(BaseTestConv3d, self).run_gradinput(
739             inputs_shape=inputs_shape,
740             filters_shape=filters_shape,
741             output_shape=output_shape,
742             gradInputs_fn=gradInputs_fn,
743             ref=ref, **kwargs)
744 class TestCorrConv3d(BaseTestConv3d):
745     @classmethod
746     def setup_class(cls):
747         BaseTestConv3d.setup_class()
748     def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1, 1)):
749         o = self.get_output_shape(i, f, s, b, fd)
750                 theano.config.mode == "FAST_COMPILE"):
751             raise SkipTest("Need blas to test conv3d")
752         self.run_fwd<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f, subsample=s,
753                      verify_grad=True, provide_shape=provide_shape,
754                      border_mode=b, filter_flip=flip,
755                      target_op=Corr3dMM, check_trace=True,
756                      filter_dilation=fd)
757         self.run_gradweight(inputs_shape=</b></font>i, filters_shape=f,
758                             output_shape=o, subsample=s, verify_grad=True,
759                             provide_shape=provide_shape, border_mode=b,
760                             filter_flip=flip, target_op=Corr3dMM_gradWeights,
761                             check_trace=True, filter_dilation=fd)
762         self.run_gradinput(inputs_shape=i, filters_shape=f,
763                            output_shape=o, subsample=s, verify_grad=True,
764                            provide_shape=provide_shape, border_mode=b,
765                            filter_flip=flip, target_op=Corr3dMM_gradInputs,
766                            check_trace=True, filter_dilation=fd)
767     def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1, 1), expect_error=False):
768         if (not theano.config.cxx or
769                 theano.config.mode == "FAST_COMPILE"):
770             raise SkipTest("Need blas to test conv3d")
771         if not expect_error:
772             self.run_gradinput(inputs_shape=i, filters_shape=f,
773                                output_shape=o, subsample=s, verify_grad=True,
774                                provide_shape=provide_shape, border_mode=b,
775                                filter_flip=flip, target_op=Corr3dMM_gradInputs,
776                                check_trace=True, filter_dilation=fd)
777         else:
778             assert_raises(ValueError,
779                           self.run_gradinput,
780                           inputs_shape=i, filters_shape=f,
781                           output_shape=o, subsample=s, verify_grad=False,
782                           provide_shape=provide_shape, border_mode=b,
783                           filter_flip=flip, target_op=Corr3dMM_gradInputs,
784                           ref=None, check_trace=True, filter_dilation=fd)
785 def test_constant_shapes():
786     dummy_t4 = tensor.ftensor4()
787     alloc_dummy_t4 = tensor.zeros((3, 5, 7, 11), dtype='float32')
788     dummy_shape = tensor.lvector()
789     dummy_one_shape = tensor.ones(4, dtype='int64')
790     constant_vec_shape = tensor.constant([3, 5, 7, 11])
791     tuple_shape = (3, 5, 7, 11)
792     list_shape = list(tuple_shape)
793     constant_list_shape = [tensor.constant(i, dtype='int64')
794                            for i in tuple_shape]
795     constant_tuple_shape = tuple(constant_list_shape)
796     bad_shapes = (
797         dummy_shape,
798         dummy_one_shape,
799         dummy_t4.shape,
800         alloc_dummy_t4.shape,
801         constant_vec_shape,
802     )
803     good_shapes = (
804         constant_list_shape,
805         constant_tuple_shape,
806         tuple_shape,
807         list_shape
808     )
809     ops_to_test = (
810         AbstractConv2d,
811         AbstractConv2d_gradInputs,
812         AbstractConv2d_gradWeights
813     )
814     for op in ops_to_test:
815         for shp in bad_shapes:
816             assert_raises(ValueError, op, imshp=shp)
817             assert_raises(ValueError, op, kshp=shp)
818         for shp in good_shapes:
819             op(imshp=shp)
820             op(kshp=shp)
821 class TestConvTypes(unittest.TestCase):
822     def setUp(self):
823         self<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.input = tensor.ftensor4()
824         self.filters = tensor.ftensor4()
825         self.topgrad = tensor.ftensor4()
826         self.constant_tensor = np.zeros((3, 5, 7, 11), dtype=</b></font>'float32')
827     def test_grad_types(self):
828         input = self.input
829         filters = self.filters
830         topgrad = self.topgrad
831         out_shape = tensor.lvector()
832         output = conv.conv2d(input, filters)
833         grad_input, grad_filters = theano.grad(output.sum(),
834                                                wrt=(input, filters))
835         assert grad_input.type == input.type, (
836             grad_input, grad_input.type, input, input.type)
837         assert grad_filters.type == filters.type, (
838             grad_filters, grad_filters.type, filters, filters.type)
839         grad_filters = conv.AbstractConv2d_gradWeights()(
840             input, topgrad, out_shape)
841         grad_input, grad_topgrad = theano.grad(grad_filters.sum(),
842                                                wrt=(input, topgrad))
843         assert grad_input.type == input.type, (
844             grad_input, grad_input.type, input, input.type)
845         assert grad_topgrad.type == topgrad.type, (
846             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
847         grad_input = conv.AbstractConv2d_gradInputs()(
848             filters, topgrad, out_shape)
849         grad_filters, grad_topgrad = theano.grad(grad_input.sum(),
850                                                  wrt=(filters, topgrad))
851         assert grad_filters.type == filters.type, (
852             grad_filters, grad_filters.type, filters, filters.type)
853         assert grad_topgrad.type == topgrad.type, (
854             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
855     def test_constant_input(self):
856         input = self.input
857         filters = self.filters
858         topgrad = self.topgrad
859         constant_tensor = self.constant_tensor
860         out_shape = tensor.lvector()
861         output = conv.conv2d(constant_tensor, filters)
862         grad_filters = theano.grad(output.sum(), wrt=filters)
863         assert grad_filters.type == filters.type, (
864             grad_filters, grad_filters.type, filters, filters.type)
865         output = conv.conv2d(input, constant_tensor)
866         grad_input = theano.grad(output.sum(), wrt=input)
867         assert grad_input.type == input.type, (
868             grad_input, grad_input.type, input, input.type)
869         grad_filters = conv.AbstractConv2d_gradWeights()(
870             constant_tensor, topgrad, out_shape)
871         grad_topgrad = theano.grad(grad_filters.sum(), wrt=topgrad)
872         assert grad_topgrad.type == topgrad.type, (
873             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
874         grad_filters = conv.AbstractConv2d_gradWeights()(
875             input, constant_tensor, out_shape)
876         grad_input = theano.grad(grad_filters.sum(), wrt=input)
877         assert grad_input.type == input.type, (
878             grad_input, grad_input.type, input, input.type)
879         grad_input = conv.AbstractConv2d_gradInputs()(
880             constant_tensor, topgrad, out_shape)
881         grad_topgrad = theano.grad(grad_input.sum(), wrt=topgrad)
882         assert grad_topgrad.type == topgrad.type, (
883             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
884         grad_input = conv.AbstractConv2d_gradInputs()(
885             filters, constant_tensor, out_shape)
886         grad_filters = theano.grad(grad_input.sum(), wrt=filters)
887         assert grad_filters.type == filters.type, (
888             grad_filters, grad_filters.type, filters, filters.type)
889 class TestBilinearUpsampling(unittest.TestCase):
890     compile_mode = theano.compile.mode.get_default_mode()
891     if theano.config.mode == "FAST_COMPILE":
892         compile_mode = compile_mode.excluding("conv_gemm")
893         compile_mode = compile_mode.excluding('AbstractConvCheck')
894     elif not theano.config.cxx:
895         compile_mode = compile_mode.excluding('AbstractConvCheck')
896     def numerical_kernel_1D(self, ratio):
897         return np.array(list(range(1, ratio + 1)) +
898                         list(range(ratio - 1, 0, -1)))
899     def numerical_kernel_2D(self, ratio):
900         return np.array([i * j for i in self.numerical_kernel_1D(ratio) for j
901                          in self.numerical_kernel_1D(ratio)]).\
902             reshape(2 * ratio - 1, 2 * ratio - 1)
903     def test_bilinear_kernel_2D(self):
904         for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
905             kernel = bilinear_kernel_2D(ratio=ratio, normalize=False)
906             f = theano.function([], kernel)
907             kernel_2D = self.numerical_kernel_2D(ratio)
908             utt.assert_allclose(kernel_2D, f())
909             kernel = bilinear_kernel_2D(ratio=ratio, normalize=True)
910             f = theano.function([], kernel)
911             kernel_2D = kernel_2D / float(ratio**2)
912             utt.assert_allclose(kernel_2D, f())
913     def test_bilinear_kernel_1D(self):
914         rat = tensor.iscalar()
915         kernel_ten = bilinear_kernel_1D(ratio=rat, normalize=False)
916         f_ten = theano.function([rat], kernel_ten)
917         kernel_ten_norm = bilinear_kernel_1D(ratio=rat, normalize=True)
918         f_ten_norm = theano.function([rat], kernel_ten_norm)
919         for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
920             kernel = bilinear_kernel_1D(ratio=ratio, normalize=False)
921             f = theano.function([], kernel)
922             kernel_1D = self.numerical_kernel_1D(ratio)
923             utt.assert_allclose(kernel_1D, f())
924             utt.assert_allclose(kernel_1D, f_ten(ratio))
925             kernel = bilinear_kernel_1D(ratio=ratio, normalize=True)
926             f = theano.function([], kernel)
927             kernel_1D = kernel_1D / float(ratio)
928             utt.assert_allclose(kernel_1D, f())
929             utt.assert_allclose(kernel_1D, f_ten_norm(ratio))
930     def numerical_upsampling_multiplier(self, ratio):
931         kern = np.arange(ratio + 1)
932         return kern, kern.shape[0]
933     def get_upsampled_twobytwo_mat(self, two_by_two, ratio):
934         kern, shp = self.numerical_upsampling_multiplier(ratio)
935         up_1D = two_by_two[:, :, :, :1] * kern[::-1] + \
936             two_by_two[:, :, :, 1:] * kern
937         up_2D = up_1D[:, :, :1, :] * kern[::-1][:, np.newaxis] + \
938             up_1D[:, :, 1:, :] * kern[:, np.newaxis]
939         num_concat = (ratio - 1) // 2
940         for i in range(num_concat):
941             up_2D = np.concatenate([up_2D[:, :, :1, :], up_2D], axis=2)
942             up_2D = np.concatenate([up_2D, up_2D[:, :, -1:, :]], axis=2)
943             up_2D = np.concatenate([up_2D[:, :, :, :1], up_2D], axis=3)
944             up_2D = np.concatenate([up_2D, up_2D[:, :, :, -1:]], axis=3)
945         if ratio % 2 == 0:
946             up_2D = np.concatenate([up_2D, up_2D[:, :, -1:, :]], axis=2)
947             up_2D = np.concatenate([up_2D, up_2D[:, :, :, -1:]], axis=3)
948         return up_2D / float(ratio)**2
949     def test_bilinear_upsampling_1D(self):
950         input_x = np.array([[[[1, 2], [3, 4]]]], dtype=theano.config.floatX)
951         for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
952             bilin_mat = bilinear_upsampling(input=input_x, ratio=ratio,
953                                             batch_size=1, num_input_channels=1,
954                                             use_1D_kernel=True)
955             f = theano.function([], bilin_mat, mode=self.compile_mode)
956             up_mat_2d = self.get_upsampled_twobytwo_mat(input_x, ratio)
957             utt.assert_allclose(f(), up_mat_2d, rtol=1e-06)
958     def test_bilinear_upsampling_reshaping(self):
959         input_x = np.array([[[[1, 2], [3, 4]]]], dtype=theano.config.floatX)
960         for ratio in [2, 3]:
961             for use_1D_kernel in [True, False]:
962                 bilin_mat = bilinear_upsampling(input=input_x, ratio=ratio,
963                                                 batch_size=None,
964                                                 num_input_channels=None,
965                                                 use_1D_kernel=use_1D_kernel)
966                 f = theano.function([], bilin_mat, mode=self.compile_mode)
967                 up_mat_2d = self.get_upsampled_twobytwo_mat(input_x, ratio)
968                 utt.assert_allclose(f(), up_mat_2d, rtol=1e-06)
969     def test_compare_1D_and_2D_upsampling_values(self):
970         input_x = np.random.rand(5, 4, 6, 7).astype(theano.config.floatX)
971         mat_1D = bilinear_upsampling(input=input_x, ratio=5,
972                                      batch_size=5, num_input_channels=4,
973                                      use_1D_kernel=True)
974         mat_2D = bilinear_upsampling(input=input_x, ratio=5,
975                                      batch_size=5, num_input_channels=4,
976                                      use_1D_kernel=False)
977         f_1D = theano.function([], mat_1D, mode=self.compile_mode)
978         f_2D = theano.function([], mat_2D, mode=self.compile_mode)
979         utt.assert_allclose(f_1D(), f_2D(), rtol=1e-06)
980         input_x = np.random.rand(12, 11, 10, 7).astype(theano.config.floatX)
981         mat_1D = bilinear_upsampling(input=input_x, ratio=8,
982                                      batch_size=12, num_input_channels=11,
983                                      use_1D_kernel=True)
984         mat_2D = bilinear_upsampling(input=input_x, ratio=8,
985                                      batch_size=12, num_input_channels=11,
986                                      use_1D_kernel=False)
987         f_1D = theano.function([], mat_1D, mode=self.compile_mode)
988         f_2D = theano.function([], mat_2D, mode=self.compile_mode)
989         utt.assert_allclose(f_1D(), f_2D(), rtol=1e-06)
990     def test_fractional_bilinear_upsampling(self):
991         input_x = np.array([[[1, 2], [3, 4]],
992                             [[5, 6], [7, 8]],
993                             [[9, 10], [11, 12]]],
994                            ndmin=4).astype(theano.config.floatX)
995         up_x = bilinear_upsampling(input=input_x,
996                                    use_1D_kernel=False)
997         num_up_x = np.array(
998             [<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[1., 1.2, 1.8, 2.],
999               [1.28571429, 1.48571429, 2.08571429, 2.28571429],
1000               [2.42857143, 2.62857143, 3.22857143, 3.42857143],
1001               [3., 3.2, 3.8, 4.]],
1002              [[5., 5.2, 5.8, 6.],
1003               [5.28571429, 5.48571429, 6.08571429, 6.28571429],
1004               [6.42857143, 6.62857143, 7.22857143, 7.42857143],
1005               [7., 7.2, 7.8, 8.]],
1006              [[9., 9.2, 9.8, 10.],
1007               [9.28571429, 9.48571429, 10.08571429, 10.28571429],
1008               [10.42857143, 10.62857143, 11.22857143, 11.42857143],
1009               [11., 11.2, 11.8, 12.]]]]
1010             ).</b></font>astype(theano.config.floatX)
1011         f_up_x = theano.function([], up_x, mode=self.compile_mode)
1012         utt.assert_allclose(f_up_x(), num_up_x, rtol=1e-6)
1013     def test_fractional_bilinear_upsampling_shape(self):
1014         x = np.random.rand(1, 1, 200, 200).astype(theano.config.floatX)
1015         resize = (24, 20)
1016         z = bilinear_upsampling(tensor.as_tensor_variable(x), frac_ratio=resize, use_1D_kernel=False)
1017         out = theano.function([], z.shape, mode='FAST_RUN')()
1018         utt.assert_allclose(out, (1, 1, 240, 240))
1019 class TestConv2dTranspose(unittest.TestCase):
1020     mode = None
1021     def test_interface(self):
1022         if theano.config.cxx == "":
1023             raise SkipTest("test needs cxx")
1024         mode = self.mode
1025         if theano.config.mode == "FAST_COMPILE":
1026             mode = theano.compile.get_mode(
1027                 mode).excluding("conv_gemm").excluding("AbstractConvCheck")
1028         output = theano.function(
1029             inputs=[],
1030             outputs=conv2d_transpose(input=tensor.ones((2, 2, 4, 4)),
1031                                      filters=tensor.ones((2, 1, 4, 4)),
1032                                      output_shape=(2, 1, 10, 10),
1033             mode=mode)()
1034         expected_output = np.array(
1035             [<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
1036                [2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
1037                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1038                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1039                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1040                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1041                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1042                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1043                [2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
1044                [2, 2, 4, 4, 4, 4, 4, 4, 2, 2]]]] * 2)
1045         np.testing.</b></font>assert_equal(output, expected_output)
1046 class TestConv2dGrads(unittest.TestCase):
1047     def setUp(self):
1048         if (not theano.config.cxx or
1049                 theano.config.mode == "FAST_COMPILE"):
1050             raise SkipTest("Need blas to test conv2d")
1051         self.random_stream = np.random.RandomState(utt.fetch_seed())
1052         self.inputs_shapes = [(8, 1, 12, 12), (1, 1, 5, 5), (1, 1, 5, 6), (1, 1, 6, 6)]
1053         self.filters_shapes = [(5, 1, 2, 2), (1, 1, 3, 3)]
1054         self.subsamples = [(1, 1), (2, 2)]
1055         self.border_modes = ["valid", "full"]
1056         self.filter_flip = [True, False]
1057         self.output_grad = theano.tensor.tensor4()
1058         self.output_grad_wrt = theano.tensor.tensor4()
1059         self.x = theano.tensor.tensor4('x', theano.config.floatX)  # inputs
1060         self.w = theano.tensor.tensor4('w', theano.config.floatX)  # filter weights
1061     def test_conv2d_grad_wrt_inputs(self):
1062         for (in_shape, fltr_shape) in zip(self.inputs_shapes, self.filters_shapes):
1063             for bm in self.border_modes:
1064                 for ss in self.subsamples:
1065                     for ff in self.filter_flip:
1066                         input_val = self.random_stream.random_sample(in_shape).astype(theano.config.floatX)
1067                         filter_val = self.random_stream.random_sample(fltr_shape).astype(theano.config.floatX)
1068                         out_grad_shape = theano.tensor.nnet.abstract_conv.get_conv_output_shape(image_shape=in_shape,
1069                                                                                                 kernel_shape=fltr_shape,
1070                                                                                                 border_mode=bm,
1071                                                                                                 subsample=ss)
1072                         out_grad_val = self.random_stream.random_sample(out_grad_shape).astype(theano.config.floatX)
1073                         conv_out = theano.tensor.nnet.conv2d(self.x,
1074                                                              filters=self.w,
1075                                                              border_mode=bm,
1076                                                              subsample=ss,
1077                                                              input_shape=in_shape,
1078                                                              filter_shape=fltr_shape,
1079                                                              filter_flip=ff
1080                                                              )
1081                         conv_grad = theano.grad(conv_out.sum(), wrt=self.x, known_grads={conv_out: self.output_grad})
1082                         f_old = theano.function([self.x, self.w, self.output_grad], conv_grad)
1083                         conv_wrt_i_out = theano.tensor.nnet.abstract_conv.conv2d_grad_wrt_inputs(output_grad=self.output_grad_wrt,
1084                                                                                                  filters=self.w,
1085                                                                                                  border_mode=bm,
1086                                                                                                  subsample=ss,
1087                                                                                                  input_shape=in_shape,
1088                                                                                                  filter_shape=fltr_shape,
1089                                                                                                  filter_flip=ff
1090                                                                                                  )
1091                         f_new = theano.function([self.w, self.output_grad_wrt], conv_wrt_i_out)
1092                         utt.assert_allclose(f_new(filter_val, out_grad_val), f_old(input_val, filter_val, out_grad_val))
1093     def test_conv2d_grad_wrt_weights(self):
1094         for (in_shape, fltr_shape) in zip(self.inputs_shapes, self.filters_shapes):
1095             for bm in self.border_modes:
1096                 for ss in self.subsamples:
1097                     for ff in self.filter_flip:
1098                         input_val = self.random_stream.random_sample(in_shape).astype(theano.config.floatX)
1099                         filter_val = self.random_stream.random_sample(fltr_shape).astype(theano.config.floatX)
1100                         out_grad_shape = theano.tensor.nnet.abstract_conv.get_conv_output_shape(image_shape=in_shape,
1101                                                                                                 kernel_shape=fltr_shape,
1102                                                                                                 border_mode=bm,
1103                                                                                                 subsample=ss)
1104                         out_grad_val = self.random_stream.random_sample(out_grad_shape).astype(theano.config.floatX)
1105                         conv_out = theano.tensor.nnet.conv2d(self.x,
1106                                                              filters=self.w,
1107                                                              border_mode=bm,
1108                                                              subsample=ss,
1109                                                              input_shape=in_shape,
1110                                                              filter_shape=fltr_shape,
1111                                                              filter_flip=ff
1112                                                              )
1113                         conv_grad = theano.grad(conv_out.sum(), wrt=self.w, known_grads={conv_out: self.output_grad})
1114                         f_old = theano.function([self.x, self.w, self.output_grad], conv_grad)
1115                         conv_wrt_w_out = theano.tensor.nnet.abstract_conv.conv2d_grad_wrt_weights(self.x,
1116                                                                                                   output_grad=self.output_grad_wrt,
1117                                                                                                   border_mode=bm,
1118                                                                                                   subsample=ss,
1119                                                                                                   input_shape=in_shape,
1120                                                                                                   filter_shape=fltr_shape,
1121                                                                                                   filter_flip=ff
1122                                                                                                   )
1123                         f_new = theano.function([self.x, self.output_grad_wrt], conv_wrt_w_out)
1124                         utt.assert_allclose(f_new(input_val, out_grad_val), f_old(input_val, filter_val, out_grad_val))
1125 class Grouped_conv_noOptim(unittest.TestCase):
1126     conv = theano.tensor.nnet.abstract_conv.AbstractConv2d
1127     conv_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1128     conv_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1129     conv_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
1130     conv_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1131     conv_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1132     mode = theano.Mode(optimizer=None)
1133     is_dnn = False
1134     def setUp(self):
1135         self.num_groups = [3, 2, 4, 4]
1136         self.border_mode = 'valid'
1137         self.subsample = (1, 1)
1138         self.img_shape = [(5, 6, 5, 5), (4, 4, 7, 5), (3, 8, 5, 3), (2, 4, 7, 7)]
1139         self.kern_shape = [(6, 2, 3, 3), (6, 2, 5, 3), (4, 2, 3, 3), (4, 1, 3, 5)]
1140         self.top_shape = [(5, 6, 3, 3), (4, 6, 3, 3), (3, 4, 3, 1), (2, 4, 5, 3)]
1141         self.filter_dilation = (1, 1)
1142         self.ref_mode = 'FAST_RUN'
1143         self.convdim = 2
1144         self.corr_fwd = conv2d_corr
1145         self.corr_gradw = conv2d_corr_gw
1146         self.corr_gradi = conv2d_corr_gi
1147         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1148             raise SkipTest("CorrMM needs cxx and SciPy")
1149     def test_fwd(self):
1150         if self.convdim == 2:
1151             img_sym <font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.tensor4('img')
1152             kern_sym = theano.tensor.tensor4('kern')
1153         else:
1154             img_sym = theano.tensor.tensor5('img')
1155             kern_sym =</b></font> theano.tensor.tensor5('kern')
1156         for imshp, kshp, groups in zip(self.img_shape, self.kern_shape, self.num_groups):
1157             img = np.random.random(imshp).astype(theano.config.floatX)
1158             kern = np.random.random(kshp).astype(theano.config.floatX)
1159             split_imgs = np.split(img, groups, axis=1)
1160             split_kern = np.split(kern, groups, axis=0)
1161             grouped_conv_op = self.conv(border_mode=self.border_mode,
1162                                         subsample=self.subsample,
1163                                         filter_dilation=self.filter_dilation,
1164                                         num_groups=groups)
1165             grouped_conv_output = grouped_conv_op(img_sym, kern_sym)
1166             grouped_func = theano.function([img_sym, kern_sym], grouped_conv_output, mode=self.mode)
1167             assert any([isinstance(node.op, self.conv_op)
1168                        for node in grouped_func.maker.fgraph.toposort()])
1169             grouped_output = grouped_func(img, kern)
1170             ref_conv_op = self.corr_fwd(img_sym,
1171                                         kern_sym,
1172                                         border_mode=self.border_mode,
1173                                         subsample=self.subsample,
1174                                         filter_dilation=self.filter_dilation)
1175             ref_func = theano.function([img_sym, kern_sym], ref_conv_op,
1176                                        mode=self.ref_mode)
1177             ref_concat_output = [ref_func(img_arr, kern_arr)
1178                                  for img_arr, kern_arr in zip(split_imgs, split_kern)]
1179             ref_concat_output = np.concatenate(ref_concat_output, axis=1)
1180             utt.assert_allclose(grouped_output, ref_concat_output)
1181             utt.verify_grad(grouped_conv_op,
1182                             [img, kern],
1183                             mode=self.mode,
1184                             eps=1)
1185     def test_gradweights(self):
1186         if self.convdim == 2:
1187             img_sym = theano.tensor.tensor4('img')
1188             top_sym = theano.tensor.tensor4('kern')
1189         else:
1190             img_sym = theano.tensor.tensor5('img')
1191             top_sym = theano.tensor.tensor5('kern')
1192         for imshp, kshp, tshp, groups in zip(self.img_shape, self.kern_shape, self.top_shape, self.num_groups):
1193             img = np.random.random(imshp).astype(theano.config.floatX)
1194             top = np.random.random(tshp).astype(theano.config.floatX)
1195             split_imgs = np.split(img, groups, axis=1)
1196             split_top = np.split(top, groups, axis=1)
1197             grouped_convgrad_op = self.conv_gradw(border_mode=self.border_mode,
1198                                                   subsample=self.subsample,
1199                                                   filter_dilation=self.filter_dilation,
1200                                                   num_groups=groups)
1201             grouped_conv_output = grouped_convgrad_op(img_sym,
1202                                                       top_sym,
1203                                                       tensor.as_tensor_variable(
1204                                                           kshp[-self.convdim:]))
1205             grouped_func = theano.function([img_sym, top_sym], grouped_conv_output, mode=self.mode)
1206             assert any([isinstance(node.op, self.conv_gradw_op)
1207                        for node in grouped_func.maker.fgraph.toposort()])
1208             grouped_output = grouped_func(img, top)
1209             ref_conv_op = self.corr_gradw(img_sym,
1210                                           top_sym,
1211                                           kshp,
1212                                           border_mode=self.border_mode,
1213                                           subsample=self.subsample,
1214                                           filter_dilation=self.filter_dilation)
1215             ref_func = theano.function([img_sym, top_sym], ref_conv_op,
1216                                        mode=self.ref_mode)
1217             ref_concat_output = [ref_func(img_arr, top_arr)
1218                                  for img_arr, top_arr in zip(split_imgs, split_top)]
1219             ref_concat_output = np.concatenate(ref_concat_output, axis=0)
1220             utt.assert_allclose(grouped_output, ref_concat_output)
1221             def conv_gradweight(inputs_val, output_val):
1222                 return grouped_convgrad_op(inputs_val, output_val,
1223                                            tensor.as_tensor_variable(
1224                                                kshp[-self.convdim:]))
1225             utt.verify_grad(conv_gradweight,
1226                             [img, top],
1227                             mode=self.mode, eps=1)
1228     def test_gradinputs(self):
1229         if self.convdim == 2:
1230             kern_sym = theano.tensor.tensor4('kern')
1231             top_sym = theano.tensor.tensor4('top')
1232         else:
1233             kern_sym = theano.tensor.tensor5('kern')
1234             top_sym = theano.tensor.tensor5('top')
1235         for imshp, kshp, tshp, groups in zip(self.img_shape, self.kern_shape, self.top_shape, self.num_groups):
1236             kern = np.random.random(kshp).astype(theano.config.floatX)
1237             top = np.random.random(tshp).astype(theano.config.floatX)
1238             split_kerns = np.split(kern, groups, axis=0)
1239             split_top = np.split(top, groups, axis=1)
1240             grouped_convgrad_op = self.conv_gradi(border_mode=self.border_mode,
1241                                                   subsample=self.subsample,
1242                                                   filter_dilation=self.filter_dilation,
1243                                                   num_groups=groups)
1244             grouped_conv_output = grouped_convgrad_op(kern_sym,
1245                                                       top_sym,
1246                                                       tensor.as_tensor_variable(
1247                                                           imshp[-self.convdim:]))
1248             grouped_func = theano.function([kern_sym, top_sym], grouped_conv_output, mode=self.mode)
1249             assert any([isinstance(node.op, self.conv_gradi_op)
1250                        for node in grouped_func.maker.fgraph.toposort()])
1251             grouped_output = grouped_func(kern, top)
1252             ref_conv_op = self.corr_gradi(kern_sym,
1253                                           top_sym,
1254                                           imshp,
1255                                           border_mode=self.border_mode,
1256                                           subsample=self.subsample,
1257                                           filter_dilation=self.filter_dilation)
1258             ref_func = theano.function([kern_sym, top_sym], ref_conv_op,
1259                                        mode=self.ref_mode)
1260             ref_concat_output = [ref_func(kern_arr, top_arr)
1261                                  for kern_arr, top_arr in zip(split_kerns, split_top)]
1262             ref_concat_output = np.concatenate(ref_concat_output, axis=1)
1263             utt.assert_allclose(grouped_output, ref_concat_output)
1264             def conv_gradinputs(filters_val, output_val):
1265                 return grouped_convgrad_op(filters_val, output_val,
1266                                            tensor.as_tensor_variable(
1267                                                imshp[-self.convdim:]))
1268             utt.verify_grad(conv_gradinputs,
1269                             [kern, top],
1270                             mode=self.mode, eps=1)
1271 class Grouped_conv3d_noOptim(Grouped_conv_noOptim):
1272     conv = theano.tensor.nnet.abstract_conv.AbstractConv3d
1273     conv_gradw = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradWeights
1274     conv_gradi = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradInputs
1275     conv_op = theano.tensor.nnet.abstract_conv.AbstractConv3d
1276     conv_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradWeights
1277     conv_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradInputs
1278     mode = theano.Mode(optimizer=None)
1279     def setUp(self):
1280         self.num_groups = [3, 2, 4, 4]
1281         self.border_mode = 'valid'
1282         self.subsample = (1, 1, 1)
1283         self.img_shape = [(2, 6, 5, 5, 5), (1, 4, 7, 5, 7), (1, 8, 5, 3, 5), (2, 4, 7, 7, 7)]
1284         self.kern_shape = [(3, 2, 3, 3, 3), (6, 2, 5, 3, 5), (4, 2, 3, 3, 3), (4, 1, 3, 5, 3)]
1285         self.top_shape = [(2, 3, 3, 3, 3), (1, 6, 3, 3, 3), (1, 4, 3, 1, 3), (2, 4, 5, 3, 5)]
1286         self.filter_dilation = (1, 1, 1)
1287         self.ref_mode = 'FAST_RUN'
1288         self.convdim = 3
1289         self.corr_fwd = conv3d_corr
1290         self.corr_gradw = conv3d_corr_gw
1291         self.corr_gradi = conv3d_corr_gi
1292         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1293             raise SkipTest("CorrMM needs cxx")
1294 class Separable_conv(unittest.TestCase):
1295     def setUp(self):
1296                             [[3, 3, 1, 2, 6], [6, 5, 4, 3, 1], [3, 4, 5, 2, 3], [6</b></font>, 4, 1, 3, 4], [2, 3, 4, 2, 5]]]]).astype(theano.config.floatX)
1297         self.depthwise_filter = np.array([<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[3, 2, 1], [5, 3, 2], [6, 4, 2]]], [[[5, 5, 2], [3, 7, 4], [3, 5, 4]]],
1298                                           [[[7, 4, 7], [5, 3, 3], [1, 3, 1]]], [[[4, 4, 4], [2, 4, 6], [0, 0, 7]]]]).</b></font>astype(theano.config.floatX)
1299         self.pointwise_filter = np.array([[[[4]], [[1]], [[3]], [[5]]], [[[2]], [[1]], [[2]], [[8]]]]).astype(theano.config.floatX)
1300         self.precomp_output_valid = np.array([[[[1385, 1333, 1339], [1382, 1243, 1291], [1303, 1120, 1228]],
1301                                                [[1532, 1410, 1259], [1522, 1346, 1314], [1379, 1192, 1286]]]]).astype(theano.config.floatX)
1302         self.precomp_output_full = np.array([[[[140, 266, 343, 206, 59],
1303                                               [395, 697, 979, 585, 245],
1304                                               [429, 863, 1385, 919, 453],
1305                                               [243, 499, 864, 627, 371],
1306                                               [90, 183, 291, 254, 202]],
1307                                              [[149, 289, 359, 213, 58],
1308                                               [400, 750, 1076, 662, 266],
1309                                               [387, 854, 1532, 1091, 540],
1310                                               [174, 411, 971, 786, 518],
1311                                               [51, 110, 286, 299, 298]]]]).astype(theano.config.floatX)
1312     def test_interface2d(self):
1313         if theano.config.cxx == "":
1314             raise SkipTest("test needs cxx")
1315         x_sym = theano.tensor.tensor4('x')
1316         dfilter_sym = theano.tensor.tensor4('d')
1317         pfilter_sym = theano.tensor.tensor4('p')
1318         sep_op = separable_conv2d(x_sym, dfilter_sym, pfilter_sym, self.x.shape[1])
1319         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1320         top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
1321         utt.assert_allclose(top, self.precomp_output_valid)
1322         top = fun(self.x[:, :, :3, :], self.depthwise_filter, self.pointwise_filter)
1323         utt.assert_allclose(top, self.precomp_output_valid[:, :, :1, :])
1324         sep_op = separable_conv2d(x_sym,
1325                                   dfilter_sym,
1326                                   pfilter_sym,
1327                                   self.x.shape[1],
1328                                   input_shape=self.x.shape,
1329                                   depthwise_filter_shape=self.depthwise_filter.shape,
1330                                   pointwise_filter_shape=self.pointwise_filter.shape)
1331         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1332         top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
1333         utt.assert_allclose(top, self.precomp_output_valid)
1334         sep_op = separable_conv2d(x_sym,
1335                                   dfilter_sym,
1336                                   pfilter_sym,
1337                                   self.x.shape[1],
1338                                   subsample=(2, 2))
1339         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1340         top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
1341         utt.assert_allclose(top, np.delete(np.delete(self.precomp_output_valid, 1, axis=3), 1, axis=2))
1342         sep_op = separable_conv2d(x_sym, dfilter_sym, pfilter_sym, self.x.shape[1], border_mode='full')
1343         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1344         top = fun(self.x[:, :, :3, :3], self.depthwise_filter, self.pointwise_filter)
1345         utt.assert_allclose(top, self.precomp_output_full)
1346     def test_interface3d(self):
1347         if theano.config.cxx == "":
1348             raise SkipTest("test needs cxx")
1349         x = np.tile(np.expand_dims(self.x, axis=2), (1, 1, 5, 1, 1))
1350         depthwise_filter = np.tile(np.expand_dims(self.depthwise_filter, axis=2), (1, 1, 3, 1, 1))
1351         pointwise_filter = np.expand_dims(self.pointwise_filter, axis=2)
1352         precomp_output = np.tile(np.expand_dims(self.precomp_output_valid, axis=2), (1, 1, 3, 1, 1)) * 3
1353         x_sym = theano.tensor.tensor5('x')
1354         dfilter_sym = theano.tensor.tensor5('d')
1355         pfilter_sym = theano.tensor.tensor5('p')
1356         sep_op = separable_conv3d(x_sym, dfilter_sym, pfilter_sym, x.shape[1])
1357         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1358         top = fun(x, depthwise_filter, pointwise_filter)
1359         utt.assert_allclose(top, precomp_output)
1360         top = fun(x[:, :, :3, :, :3], depthwise_filter, pointwise_filter)
1361         utt.assert_allclose(top, precomp_output[:, :, :1, :, :1])
1362         sep_op = separable_conv3d(x_sym,
1363                                   dfilter_sym,
1364                                   pfilter_sym,
1365                                   x.shape[1],
1366                                   input_shape=x.shape,
1367                                   depthwise_filter_shape=depthwise_filter.shape,
1368                                   pointwise_filter_shape=pointwise_filter.shape)
1369         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1370         top = fun(x, depthwise_filter, pointwise_filter)
1371         utt.assert_allclose(top, precomp_output)
1372         sep_op = separable_conv3d(x_sym,
1373                                   dfilter_sym,
1374                                   pfilter_sym,
1375                                   x.shape[1],
1376                                   subsample=(2, 2, 2))
1377         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1378         top = fun(x, depthwise_filter, pointwise_filter)
1379         utt.assert_allclose(top, np.delete(np.delete(
1380             np.delete(precomp_output, 1, axis=4), 1, axis=3), 1, axis=2))
1381         precomp_output = np.tile(np.expand_dims(self.precomp_output_full, axis=2),
1382                                  (1, 1, 5, 1, 1)) * np.array([[[[[1]], [[2]], [[3]], [[2]], [[1]]]]])
1383         sep_op = separable_conv3d(x_sym, dfilter_sym, pfilter_sym, x.shape[1], border_mode='full')
1384         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1385         top = fun(x[:, :, :3, :3, :3], depthwise_filter, pointwise_filter)
1386         utt.assert_allclose(top, precomp_output)
1387 class TestUnsharedConv(unittest.TestCase):
1388     conv2d = theano.tensor.nnet.abstract_conv.AbstractConv2d
1389     conv2d_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1390     conv2d_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1391     conv2d_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
1392     conv2d_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1393     conv2d_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1394     mode = theano.compile.mode.Mode(optimizer='None')
1395     def setUp(self):
1396         self.img_shape = [(2, 2, 4, 4), (3, 2, 4, 2), (3, 3, 5, 3), (3, 4, 4, 4)]
1397         self.kern_shape = [(2, 2, 2, 2, 3, 3), (2, 4, 2, 2, 4, 2), (3, 2, 1, 1, 3, 3), (4, 3, 3, 2, 4, 2)]
1398         self.topgrad_shape = [(2, 2, 2, 2), (3, 2, 4, 2), (3, 3, 2, 1), (3, 4, 3, 3)]
1399         self.border_mode = ['valid', 'full', 'valid', 'full']
1400         self.subsample = [(1, 1), (2, 2), (2, 1), (3, 2)]
1401         self.filter_dilation = (1, 1)
1402         self.num_groups = [1, 1, 3, 2]
1403         self.verify_flags = [True] * 4
1404         self.ref_mode = 'FAST_RUN'
1405         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1406             raise SkipTest("CorrMM needs cxx or SciPy")
1407     def test_fwd(self):
1408         tensor6 = theano.tensor.TensorType(theano.config.floatX, (False,) * 6)
1409         img_sym = theano.tensor.tensor4('img')
1410         kern_sym = tensor6('kern')
1411         ref_kern_sym = theano.tensor.tensor4('ref_kern')
1412         for imshp, kshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.border_mode,
1413                                                           self.subsample, self.num_groups, self.verify_flags):
1414             img = np.random.random(imshp).astype(theano.config.floatX)
1415             kern = np.random.random(kshp).astype(theano.config.floatX)
1416             unshared_conv_op = self.conv2d(border_mode=mode, subsample=sub,
1417                                            filter_dilation=self.filter_dilation,
1418                                            num_groups=groups, unshared=True)
1419             unshared_out_sym = unshared_conv_op(img_sym, kern_sym)
1420             unshared_func = theano.function([img_sym, kern_sym], unshared_out_sym, mode=self.mode)
1421             assert any([isinstance(node.op, self.conv2d_op)
1422                         for node in unshared_func.maker.fgraph.toposort()])
1423             unshared_output = unshared_func(img, kern)
1424             single_kshp = kshp[:1] + kshp[3:]
1425             ref_conv_op = self.conv2d(border_mode=mode, subsample=sub,
1426                                       filter_dilation=self.filter_dilation,
1427                                       num_groups=groups, unshared=False)
1428             ref_out_sym = ref_conv_op(img_sym, ref_kern_sym)
1429             ref_func = theano.function([img_sym, ref_kern_sym], ref_out_sym, mode=self.mode)
1430             for i in range(0, kshp[1]):
1431                 for j in range(0, kshp[2]):
1432                     single_kern = kern[:, i, j, ...].reshape(single_kshp)
1433                     ref_val = ref_func(img, single_kern)
1434                     utt.assert_allclose(ref_val[:, :, i, j], unshared_output[:, :, i, j])
1435             if verify:
1436                 utt.verify_grad(unshared_conv_op, [img, kern], mode=self.mode, eps=1)
1437     def test_gradweight(self):
1438         img_sym = theano.tensor.tensor4('img')
1439         top_sym = theano.tensor.tensor4('top')
1440         for imshp, kshp, topshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.topgrad_shape,
1441                                                                   self.border_mode, self.subsample, self.num_groups,
1442                                                                   self.verify_flags):
1443             img = np.random.random(imshp).astype(theano.config.floatX)
1444             top = np.random.random(topshp).astype(theano.config.floatX)
1445             unshared_conv_op = self.conv2d_gradw(border_mode=mode, subsample=sub,
1446                                                  filter_dilation=self.filter_dilation,
1447                                                  num_groups=groups, unshared=True)
1448             unshared_out_sym = unshared_conv_op(img_sym, top_sym, tensor.as_tensor_variable(kshp[-2:]))
1449             unshared_func = theano.function([img_sym, top_sym], unshared_out_sym, mode=self.mode)
1450             assert any([isinstance(node.op, self.conv2d_gradw_op)
1451                         for node in unshared_func.maker.fgraph.toposort()])
1452             unshared_output = unshared_func(img, top)
1453             single_kshp = kshp[:1] + kshp[3:]
1454             ref_conv_op = self.conv2d_gradw(border_mode=mode, subsample=sub,
1455                                             filter_dilation=self.filter_dilation,
1456                                             num_groups=groups, unshared=False)
1457             ref_out_sym = ref_conv_op(img_sym, top_sym, tensor.as_tensor_variable(single_kshp[-2:]))
1458             ref_func = theano.function([img_sym, top_sym], ref_out_sym, mode=self.mode)
1459             for i in range(0, topshp[2]):
1460                 for j in range(0, topshp[3]):
1461                     top_single = np.zeros_like(top)
1462                     top_single[:, :, i, j] = top[:, :, i, j]
1463                     ref_output = ref_func(img, top_single)
1464                     utt.assert_allclose(unshared_output[:, i, j, ...], ref_output)
1465             def conv_gradweight(inputs_val, output_val):
1466                 return unshared_conv_op(inputs_val, output_val, tensor.as_tensor_variable(kshp[-2:]))
1467             if verify:
1468                 utt.verify_grad(conv_gradweight, [img, top], mode=self.mode, eps=1)
1469     def test_gradinput(self):
1470         tensor6 = theano.tensor.TensorType(theano.config.floatX, (False,) * 6)
1471         kern_sym = tensor6('kern')
1472         top_sym = theano.tensor.tensor4('top')
1473         ref_kern_sym = theano.tensor.tensor4('ref_kern')
1474         for imshp, kshp, topshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.topgrad_shape,
1475                                                                   self.border_mode, self.subsample, self.num_groups,
1476                                                                   self.verify_flags):
1477             single_kshp = kshp[:1] + kshp[3:]
1478             kern = np.random.random(kshp).astype(theano.config.floatX)
1479             top = np.random.random(topshp).astype(theano.config.floatX)
1480             unshared_conv_op = self.conv2d_gradi(border_mode=mode, subsample=sub,
1481                                                  filter_dilation=self.filter_dilation,
1482                                                  num_groups=groups, unshared=True)
1483             unshared_out_sym = unshared_conv_op(kern_sym, top_sym, tensor.as_tensor_variable(imshp[-2:]))
1484             unshared_func = theano.function([kern_sym, top_sym], unshared_out_sym, mode=self.mode)
1485             assert any([isinstance(node.op, self.conv2d_gradi_op)
1486                         for node in unshared_func.maker.fgraph.toposort()])
1487             unshared_output = unshared_func(kern, top)
1488             ref_conv_op = self.conv2d_gradi(border_mode=mode, subsample=sub,
1489                                             filter_dilation=self.filter_dilation,
1490                                             num_groups=groups, unshared=False)
1491             ref_out_sym = ref_conv_op(ref_kern_sym, top_sym, tensor.as_tensor_variable(imshp[-2:]))
1492             ref_func = theano.function([ref_kern_sym, top_sym], ref_out_sym, mode=self.mode)
1493             ref_output = np.zeros(imshp)
1494             for i in range(0, topshp[2]):
1495                 for j in range(0, topshp[3]):
1496                     single_kern = kern[:, i, j, ...].reshape(single_kshp)
1497                     top_single = np.zeros_like(top)
1498                     top_single[:, :, i, j] = top[:, :, i, j]
1499                     ref_output += ref_func(single_kern, top_single)
1500             utt.assert_allclose(ref_output, unshared_output)
1501             def conv_gradinputs(filters_val, output_val):
1502                 return unshared_conv_op(filters_val, output_val, tensor.as_tensor_variable(imshp[-2:]))
1503             if verify:
1504                 utt.verify_grad(conv_gradinputs, [kern, top], mode=self.mode, eps=1)
1505 class TestAsymmetricPadding(unittest.TestCase):
1506     conv2d = theano.tensor.nnet.abstract_conv.AbstractConv2d
1507     conv2d_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1508     conv2d_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1509     conv2d_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
1510     conv2d_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1511     conv2d_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1512     mode = theano.compile.mode.Mode(optimizer='None')
1513     img_shape = [(2, 2, 4, 4), (3, 2, 4, 2), (3, 3, 5, 3)]
1514     kern_shape = [(4, 2, 2, 2), (2, 2, 4, 2), (2, 3, 3, 3)]
1515     topgrad_shape = [(2, 4, 6, 6), (3, 2, 3, 4), (3, 2, 6, 1)]
1516     border_mode = [((1, 2), (2, 1)), ((1, 1), (0, 3)), ((2, 1), (0, 0))]
1517     def test_fwd(self):
1518         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1519             raise SkipTest("SciPy and cxx needed")
1520         img_sym = theano.tensor.tensor4('img')
1521         kern_sym = theano.tensor.tensor4('kern')
1522         for imshp, kshp, pad in zip(self.img_shape, self.kern_shape, self.border_mode):
1523             img = np.random.random(imshp).astype(theano.config.floatX)
1524             kern = np.random.random(kshp).astype(theano.config.floatX)
1525             asymmetric_conv_op = self.conv2d(border_mode=pad, subsample=(1, 1),
1526                                              filter_dilation=(1, 1))
1527             asymmetric_out_sym = asymmetric_conv_op(img_sym, kern_sym)
1528             asymmetric_func = theano.function([img_sym, kern_sym], asymmetric_out_sym, mode=self.mode)
1529             assert any([isinstance(node.op, self.conv2d_op)
1530                         for node in asymmetric_func.maker.fgraph.toposort()])
1531             asymmetric_output = asymmetric_func(img, kern)
1532             ref_conv_op = self.conv2d(border_mode="valid", subsample=(1, 1),
1533                                       filter_dilation=(1, 1))
1534             ref_out_sym = ref_conv_op(img_sym, kern_sym)
1535             ref_func = theano.function([img_sym, kern_sym], ref_out_sym, mode=self.mode)
1536             exp_imshp = (imshp[0], imshp[1],
1537                          imshp[2] + pad[0][0] + pad[0][1],
1538                          imshp[3] + pad[1][0] + pad[1][1])
1539             exp_img = np.zeros(exp_imshp, dtype=theano.config.floatX)
1540             exp_img[:, :, pad[0][0]:imshp[2] + pad[0][0],
1541                     pad[1][0]:imshp[3] + pad[1][0]] = img
1542             ref_output = ref_func(exp_img, kern)
1543             utt.assert_allclose(asymmetric_output, ref_output)
1544             utt.verify_grad(asymmetric_conv_op, [img, kern], mode=self.mode, eps=1)
1545     def test_gradweight(self):
1546         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1547             raise SkipTest("SciPy and cxx needed")
1548         img_sym = theano.tensor.tensor4('img')
1549         top_sym = theano.tensor.tensor4('top')
1550         for imshp, kshp, topshp, pad in zip(self.img_shape, self.kern_shape, self.topgrad_shape, self.border_mode):
1551             img = np.random.random(imshp).astype(theano.config.floatX)
1552             top = np.random.random(topshp).astype(theano.config.floatX)
1553             asymmetric_conv_op = self.conv2d_gradw(border_mode=pad, subsample=(1, 1),
1554                                                    filter_dilation=(1, 1))
1555             asymmetric_out_sym = asymmetric_conv_op(img_sym, top_sym, kshp[-2:])
1556             asymmetric_func = theano.function([img_sym, top_sym], asymmetric_out_sym, mode=self.mode)
1557             assert any([isinstance(node.op, self.conv2d_gradw_op)
1558                         for node in asymmetric_func.maker.fgraph.toposort()])
1559             asymmetric_output = asymmetric_func(img, top)
1560             ref_conv_op = self.conv2d_gradw(border_mode="valid", subsample=(1, 1),
1561                                             filter_dilation=(1, 1))
1562             ref_out_sym = ref_conv_op(img_sym, top_sym, kshp[-2:])
1563             ref_func = theano.function([img_sym, top_sym], ref_out_sym, mode=self.mode)
1564             exp_imshp = (imshp[0], imshp[1],
1565                          imshp[2] + pad[0][0] + pad[0][1],
1566                          imshp[3] + pad[1][0] + pad[1][1])
1567             exp_img = np.zeros(exp_imshp, dtype=theano.config.floatX)
1568             exp_img[:, :, pad[0][0]:imshp[2] + pad[0][0],
1569                     pad[1][0]:imshp[3] + pad[1][0]] = img
1570             ref_output = ref_func(exp_img, top)
1571             utt.assert_allclose(asymmetric_output, ref_output)
1572             def conv_gradweight(inputs_val, output_val):
1573                 return asymmetric_conv_op(inputs_val, output_val, tensor.as_tensor_variable(kshp[-2:]))
1574             utt.verify_grad(conv_gradweight, [img, top], mode=self.mode, eps=1)
1575     def test_gradinput(self):
1576         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1577             raise SkipTest("test needs cxx and SciPy")
1578         kern_sym = theano.tensor.tensor4('kern')
1579         top_sym = theano.tensor.tensor4('top')
1580         for imshp, kshp, topshp, pad in zip(self.img_shape, self.kern_shape, self.topgrad_shape, self.border_mode):
1581             kern = np.random.random(kshp).astype(theano.config.floatX)
1582             top = np.random.random(topshp).astype(theano.config.floatX)
1583             asymmetric_conv_op = self.conv2d_gradi(border_mode=pad, subsample=(1, 1),
1584                                                    filter_dilation=(1, 1))
1585             asymmetric_out_sym = asymmetric_conv_op(kern_sym, top_sym, imshp[-2:])
1586             asymmetric_func = theano.function([kern_sym, top_sym], asymmetric_out_sym, mode=self.mode)
1587             assert any([isinstance(node.op, self.conv2d_gradi_op)
1588                         for node in asymmetric_func.maker.fgraph.toposort()])
1589             asymmetric_output = asymmetric_func(kern, top)
1590             ref_conv_op = self.conv2d_gradi(border_mode="valid", subsample=(1, 1),
1591                                             filter_dilation=(1, 1))
1592             exp_imshp = [imshp[2] + pad[0][0] + pad[0][1],
1593                          imshp[3] + pad[1][0] + pad[1][1]]
1594             ref_out_sym = ref_conv_op(kern_sym, top_sym, exp_imshp)
1595             ref_func = theano.function([kern_sym, top_sym], ref_out_sym, mode=self.mode)
1596             ref_output = ref_func(kern, top)
1597             ref_output = ref_output[:, :, pad[0][0]:imshp[2] + pad[0][0],
1598                                     pad[1][0]:imshp[3] + pad[1][0]]
1599             utt.assert_allclose(asymmetric_output, ref_output)
1600             def conv_gradinputs(filters_val, output_val):
1601                 return asymmetric_conv_op(filters_val, output_val, tensor.as_tensor_variable(imshp[-2:]))
1602             utt.verify_grad(conv_gradinputs, [kern, top], mode=self.mode, eps=1)
1603 class TestCausalConv(unittest.TestCase):
1604     mode = theano.compile.mode.Mode(optimizer='None')
1605     img = np.array([[[2, 4, 9, 5, 8], [0, 0, 4, 0, 5]],
1606                     [[2, 5, 8, 5, 5], [1, 3, 0, 7, 9]],
1607                     [[7, 0, 7, 1, 0], [0, 1, 4, 7, 2]]]).astype(theano.config.floatX)
1608     kern = np.array([[[5, 3, 1], [3, 1, 0]],
1609                      [[6, 4, 9], [2, 2, 7]]]).astype(theano.config.floatX)
1610     dilation = 2
1611     precomp_top = np.array([[[10, 20, 63, 37, 88], [12, 24, 70, 46, 120]],
1612                             [[13, 34, 47, 64, 78], [14, 36, 58, 70, 105]],
1613                             [[35, 3, 68, 27, 38], [42, 2, 78, 22, 103]]]).astype(theano.config.floatX)
1614     def test_interface(self):
1615         img_sym = theano.tensor.tensor3('img')
1616         kern_sym = theano.tensor.tensor3('kern')
1617         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1618             raise SkipTest("SciPy and cxx needed")
1619         sym_out = causal_conv1d(img_sym, kern_sym, self.kern.shape, filter_dilation=self.dilation)
1620         causal_func = theano.function([img_sym, kern_sym], sym_out, mode=self.mode)
1621         output = causal_func(self.img, self.kern)
1622         utt.assert_allclose(output, self.precomp_top)
1623         def causal_conv_fn(inputs_val, filters_val):
1624             return causal_conv1d(inputs_val, filters_val, self.kern.shape, filter_dilation=1)
1625         utt.verify_grad(causal_conv_fn, [self.img, self.kern], mode=self.mode, eps=1)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>check_dnn_conv.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
2 import math
3 import sys
4 from itertools import product, chain
5 import nose
6 import numpy as np
7 from nose.plugins.skip import SkipTest
8 import theano
9 import theano.tests.unittest_tools as utt
10 from theano.compat import ifilter
11 from theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME
12 from theano.gpuarray import cudnn_defs
13 from theano.gpuarray.dnn import (GpuDnnConv, GpuDnnConvGradW, GpuDnnConvGradI, version,
14                                  _dnn_conv as dnn_conv, _dnn_gradinput as dnn_gradinput,
15                                  _dnn_gradweight as dnn_gradweight)
16 from theano.gpuarray.tests.config import mode_with_gpu, ref_cast
17 from theano.tensor.nnet.abstract_conv import get_conv_output_shape, assert_conv_shape
18 from theano.tensor.nnet.corr import CorrMM, CorrMM_gradInputs, CorrMM_gradWeights
19 from theano.tensor.nnet.corr3d import Corr3dMM, Corr3dMM_gradInputs, Corr3dMM_gradWeights
20 def check_dtype_config_support(dtype, precision):
21     inputs =</b></font> theano.shared(np.zeros((1, 1, 2, 2), dtype=dtype))
22     filters = theano.shared(np.zeros((1, 1, 2, 2), dtype=dtype))
23     conv = dnn_conv(inputs, filters, precision=precision, algo='small')
24     f = theano.function([], conv, mode=mode_with_gpu)
25     try:
26         f()
27     except RuntimeError as e:
28         assert 'CUDNN_STATUS_ARCH_MISMATCH' in e.message
29         return False
30     return True
31 cudnn = cudnn_defs.get_definitions(version(raises=False))
32 class ConvCase:
33     FWD, GRADINPUT, GRADWEIGHT = 0, 1, 2
34     def __init__(self, type,
35                  inputs_shape, filters_shape,
36                  algo=None, dtype=None, precision=None,
37                  subsample=None, dilation=None, border_mode='valid',
38                  conv_mode='conv', alpha=1, beta=0,
39                  should_fail=False):
40         assert type in (ConvCase.FWD, ConvCase.GRADINPUT, ConvCase.GRADWEIGHT)
41         assert len(inputs_shape) == len(filters_shape) in (4, 5)
42         ndim = len(inputs_shape) - 2
43         if dtype is None:
44             dtype = theano.config.floatX
45         if precision is None:
46             precision = theano.config.floatX
47         if subsample is None:
48             subsample = (1,) * ndim
49         if dilation is None:
50             dilation = (1,) * ndim
51         assert dtype in ('float16', 'float32', 'float64')
52         assert precision in ('float16', 'float32', 'float64')
53         assert len(subsample) == len(dilation) == ndim
54         assert (border_mode in ('valid', 'full', 'half') or
55                 (isinstance(border_mode, (list, tuple)) and len(border_mode) == ndim))
56         assert conv_mode in ('conv', 'cross')
57         assert alpha != 0
58         self.type = type
59         self.ndim = ndim
60         self.algo = algo
61         self.inputs_shape = inputs_shape
62         self.filters_shape = filters_shape
63         self.dtype = dtype
64         self.precision = precision
65         self.subsample = subsample
66         self.dilation = dilation
67         self.border_mode = border_mode
68         self.conv_mode = conv_mode
69         self.alpha = alpha
70         self.beta = beta
71         self.should_fail = bool(should_fail)
72     def is_fwd(self):
73         return self.type == ConvCase.FWD
74     def is_bwd_filter(self):
75         return self.type == ConvCase.GRADWEIGHT
76     def is_bwd_data(self):
77     def get_case(self):
78         return (<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>self.algo, self.dtype, self.precision,
79                 (self.inputs_shape, self.filters_shape,
80                  self.subsample, self.dilation, self.border_mode,
81                  self.conv_mode, self.alpha, self.</b></font>beta))
82     @staticmethod
83     def fwd(*args, **kwargs):
84         return ConvCase(ConvCase.FWD, *args, **kwargs)
85     @staticmethod
86     def bwd_filter(*args, **kwargs):
87         return ConvCase(ConvCase.GRADWEIGHT, *args, **kwargs)
88     @staticmethod
89     def bwd_data(*args, **kwargs):
90         return ConvCase(ConvCase.GRADINPUT, *args, **kwargs)
91 class ConvCaseGenerator:
92     def _as_tuple_of_tuples(self, iterable):
93     def __init__(self, ndim,
94                  alpha<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=2, beta=-3, batch_size=2, input_channels=3, inputs_sizes=None, output_channels=2,
95                  filters_sizes=None, subsamples=None, dilations=None, borders=None,
96                  with_border_valid=True, with_border_half=True, with_border_full=</b></font>True):
97         self.ndim = int(ndim)
98         self.alpha = float(alpha)
99         self.beta = float(beta)
100         self.batch_size = int(batch_size)
101         self.input_channels = int(input_channels)
102         self.output_channels = int(output_channels)
103         assert self.ndim in (2, 3)
104         assert self.alpha != 0
105         assert self.batch_size &gt; 0
106         assert self.input_channels &gt; 0
107         assert self.output_channels &gt; 0
108         if inputs_sizes is None:
109             inputs_sizes = ((5,) * self.ndim,
110                             (300, 5) + (2,) * (self.ndim - 2))
111         if filters_sizes is None:
112             filters_sizes = ((4,) * self.ndim,
113                              (40, 4) + (2,) * (self.ndim - 2))
114         if borders is None:
115             borders = ((1,) * self.ndim,
116                        tuple(range(1, self.ndim + 1)))
117         if subsamples is None:
118             subsamples = ((1,) * self.ndim,
119                           tuple(range(1, self.ndim + 1)))
120         if dilations is None:
121             dilations = ((1,) * self.ndim,)
122             if cudnn.version &gt;= 6:
123                 dilations += (tuple(range(1, self.ndim + 1)),)
124         for sequence_list in (inputs_sizes, filters_sizes, borders, subsamples, dilations):
125             assert (isinstance(sequence_list, (tuple, list)) and
126                     all(isinstance(sequence, (tuple, list)) and len(sequence) == self.ndim
127                         for sequence in sequence_list)), (self.ndim, sequence_list)
128         self.auto_borders = tuple()
129         if with_border_valid:
130             self.auto_borders += ('valid',)
131         if with_border_half:
132             self.auto_borders += ('half',)
133             self.auto_borders += ('full',)
134         self<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.inputs_sizes = self._as_tuple_of_tuples(inputs_sizes)
135         self.filters_sizes = self._as_tuple_of_tuples(filters_sizes)
136         self.borders = self._as_tuple_of_tuples(borders)
137         self.subsamples = self._as_tuple_of_tuples(subsamples)
138         self.dilations =</b></font> self._as_tuple_of_tuples(dilations)
139     @staticmethod
140     def get_if_valid_conv_output_shape(case_tuple):
141         out_shp = get_conv_output_shape(case_tuple[0],  # input shape
142                                         case_tuple[1],  # filter shape
143                                         case_tuple[4],  # border mode
144                                         case_tuple[2],  # subsample
145                                         case_tuple[3])  # dilation
146         try:
147             return assert_conv_shape(out_shp)
148         except ValueError:
149             return False
150     def get_cases(self, filter=None):
151         all_batch_sizes <font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= (self.batch_size,)
152         all_input_channels = (self.input_channels,)
153         all_input_sizes = self.inputs_sizes
154         all_output_channels = (self.output_channels,)
155         all_filter_sizes = self.filters_sizes
156         all_dilations = self.dilations
157         all_border_modes = self.auto_borders + self.borders
158         all_conv_modes = (<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'conv', 'cross')
159         all_alphas = (self.alpha,)
160         all_betas = (0,) if self.beta == 0 else (0, self.beta)
161         all_input_shapes = ((bs, ic) + ins
162                             for bs in all_batch_sizes for ic in all_input_channels for ins in all_input_sizes)
163         all_filter_shapes = ((oc</b></font>, ic) + fis
164                              for oc in all_output_channels for ic in all_input_channels for fis in all_filter_sizes)
165         if callable(filter):
166             def local_filter(case_tuple):
167                 return ConvCaseGenerator.get_if_valid_conv_output_shape(case_tuple) and filter(case_tuple)
168         else:
169             local_filter = ConvCaseGenerator.get_if_valid_conv_output_shape
170         return ifilter(local_filter,
171                        product(all_input_shapes, all_filter_shapes, all_subsamples, all_dilations,
172                                all_border_modes, all_conv_modes, all_alphas, all_betas))
173 class ConvCaseGeneratorChain:
174     def __init__(self, *conv_case_generators):
175         assert all(isinstance(g, ConvCaseGenerator) for g in conv_case_generators)
176         self.generators = conv_case_generators
177     def get_cases(self, filter=None):
178         return chain(*[generator.get_cases(filter) for generator in self.generators])
179 class CuDNNV51ConvCaseGenerator(object):
180     NONE = 'none'
181     FFT = 'fft'
182     FFT_TILING = 'fft_tiling'
183     WINOGRAD = 'winograd'
184     WINOGRAD_NON_FUSED = 'winograd_non_fused'
185     def _dilations(self, ndim):
186         return [(1,) * ndim]
187     def _fwd_fft(self, ndim):
188         inputs_sizes = [(10,) * ndim,
189                         (240, 5) + (2,) * (ndim - 2)]
190         filters_sizes = [tuple(range(9, 9 - ndim, -1))]
191         subsamples = [(1,) * ndim]
192         return ConvCaseGenerator(ndim=ndim,
193                                  inputs_sizes=inputs_sizes,
194                                  filters_sizes=filters_sizes,
195                                  subsamples=subsamples,
196                                  dilations=self._dilations(ndim))
197     def _fwd_fft_tiling(self, ndim, dtype, precision):
198         if ndim == 2:
199             filters_sizes = [(32, 5)]
200         if ndim == 3:
201             filters_sizes = [(16, 5, 5)]
202         subsamples = [(1,) * ndim]
203         return ConvCaseGenerator(ndim=ndim,
204                                  filters_sizes=filters_sizes,
205                                  subsamples=subsamples,
206                                  dilations=self._dilations(ndim))
207     def _fwd_winograd(self, ndim):
208         filters_sizes = [(3,) * ndim]
209         subsamples = [(1,) * ndim]
210         return ConvCaseGenerator(ndim=ndim,
211                                  filters_sizes=filters_sizes,
212                                  subsamples=subsamples,
213                                  dilations=self._dilations(ndim))
214     def _fwd_winograd_non_fused(self, ndim, dtype, precision):
215         filters_sizes = [(3,) * ndim]
216         if not (dtype == precision == 'float16'):
217             filters_sizes += [(5,) * ndim]
218         subsamples = [(1,) * ndim]
219         return ConvCaseGenerator(ndim=ndim,
220                                  filters_sizes=filters_sizes,
221                                  subsamples=subsamples,
222                                  dilations=self._dilations(ndim))
223     def _gw_fft(self, ndim):
224         return self._fwd_fft(ndim)
225     def _gw_winograd_non_fused(self, ndim, dtype, precision):
226         return self._fwd_winograd_non_fused(ndim, dtype, precision)
227     def _gi_fft(self, ndim):
228         return self._fwd_fft(ndim)
229     def _gi_fft_tiling(self, ndim, dtype, precision):
230         return self._fwd_fft_tiling(ndim, dtype, precision)
231     def _gi_winograd(self, ndim):
232         return self._fwd_winograd(ndim)
233     def _gi_winograd_non_fused(self, ndim, dtype, precision):
234         return self._fwd_winograd_non_fused(ndim, dtype, precision)
235     def _fwd_runtime(self, ndim, dtype, precision):
236         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
237     def _gw_runtime(self, ndim, dtype, precision):
238         return self._fwd_runtime(ndim, dtype, precision)
239     def _gi_runtime(self, ndim, dtype, precision):
240         return self._fwd_runtime(ndim, dtype, precision)
241     def fwd(self, algo, ndim, dtype, precision):
242         if algo == self.FFT:
243             return self._fwd_fft(ndim)
244         if algo == self.FFT_TILING:
245             return self._fwd_fft_tiling(ndim, dtype, precision)
246         if algo == self.WINOGRAD:
247             return self._fwd_winograd(ndim)
248         if algo == self.WINOGRAD_NON_FUSED:
249             return self._fwd_winograd_non_fused(ndim, dtype, precision)
250         if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
251             return self._fwd_runtime(ndim, dtype, precision)
252         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
253     def gw(self, algo, ndim, dtype, precision):
254         if algo == self.FFT:
255             return self._gw_fft(ndim)
256         if algo == self.WINOGRAD_NON_FUSED:
257             return self._gw_winograd_non_fused(ndim, dtype, precision)
258         if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
259             return self._gw_runtime(ndim, dtype, precision)
260         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
261     def gi(self, algo, ndim, dtype, precision):
262         if algo == self.FFT:
263             return self._gi_fft(ndim)
264         if algo == self.FFT_TILING:
265             return self._gi_fft_tiling(ndim, dtype, precision)
266         if algo == self.WINOGRAD:
267             return self._gi_winograd(ndim)
268         if algo == self.WINOGRAD_NON_FUSED:
269             return self._gi_winograd_non_fused(ndim, dtype, precision)
270         if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
271             return self._gi_runtime(ndim, dtype, precision)
272         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
273 class CuDNNV6ConvCaseGenerator(CuDNNV51ConvCaseGenerator):
274     def _fwd_none(self, ndim):
275         return ConvCaseGenerator(ndim=ndim)
276     def _fwd_fft_tiling(self, ndim, dtype, precision):
277         if ndim == 2:
278             subsamples = [(1, 1)]
279             generators = []
280             if (dtype, precision) != ('float64', 'float64'):
281                 filters_sizes = [(32, 5), (10, 10)]
282                 borders = [(1, 1), (6, 4)]
283                 generators += [ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim), subsamples=subsamples,
284                                                  filters_sizes=filters_sizes, borders=borders)]
285             filters_sizes = [(256, 1), (5, 1)]
286             borders = [(1, 0), (2, 0)]
287             generators += [ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim), subsamples=subsamples,
288                                              filters_sizes=filters_sizes, borders=borders)]
289             return ConvCaseGeneratorChain(*generators)
290         if ndim == 3:
291             return super(CuDNNV6ConvCaseGenerator, self)._fwd_fft_tiling(ndim, dtype, precision)
292     def _gw_none(self, ndim):
293         return self._fwd_none(ndim)
294     def _gw_fft_tiling(self, ndim):
295         inputs_sizes = [(247, 1), (20, 1)]
296         filters_sizes = [(3, 1), (10, 1)]
297         subsamples = [(1,) * ndim]
298         borders = [(1, 0), (2, 0)]
299         return ConvCaseGenerator(ndim=ndim,
300                                  inputs_sizes=inputs_sizes,
301                                  filters_sizes=filters_sizes,
302                                  subsamples=subsamples,
303                                  borders=borders,
304                                  dilations=self._dilations(ndim))
305     def _gi_none(self, ndim):
306         return self._fwd_none(ndim)
307     def _fwd_runtime(self, ndim, dtype, precision):
308         if ndim == 2 and dtype == precision == 'float16':
309             return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
310         return super(CuDNNV6ConvCaseGenerator, self)._fwd_runtime(ndim, dtype, precision)
311     def _gw_runtime(self, ndim, dtype, precision):
312         if ndim == 2 and dtype == precision == 'float16':
313             return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
314         return super(CuDNNV6ConvCaseGenerator, self)._gw_runtime(ndim, dtype, precision)
315     def _gi_runtime(self, ndim, dtype, precision):
316         if ndim == 2 and dtype == precision == 'float16':
317             return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
318         return super(CuDNNV6ConvCaseGenerator, self)._gi_runtime(ndim, dtype, precision)
319     def fwd(self, algo, ndim, dtype, precision):
320         if algo == self.NONE:
321             return self._fwd_none(ndim)
322         return super(CuDNNV6ConvCaseGenerator, self).fwd(algo, ndim, dtype, precision)
323     def gw(self, algo, ndim, dtype, precision):
324         if algo == self.NONE:
325             return self._gw_none(ndim)
326         if algo == self.FFT_TILING:
327             return self._gw_fft_tiling(ndim)
328         return super(CuDNNV6ConvCaseGenerator, self).gw(algo, ndim, dtype, precision)
329     def gi(self, algo, ndim, dtype, precision):
330         if algo == self.NONE:
331             return self._gi_none(ndim)
332         return super(CuDNNV6ConvCaseGenerator, self).gi(algo, ndim, dtype, precision)
333 cudnn_conv_case_generator = CuDNNV51ConvCaseGenerator() if cudnn.version &lt; 6 else CuDNNV6ConvCaseGenerator()
334 class BaseTestDnnConv(object):
335     ndim = 2
336     fwd_algorithms = None
337     bwd_filter_algorithms = None
338     bwd_data_algorithms = None
339     cpu_conv_class = None
340     cpu_gradinput_class = None
341     cpu_gradweight_class = None
342     special_cases = []  # List of special ConvCases.
343     runtime_shapes = []  # Tuple of tuples with format: n_times, (inputs_shape, filters_shape)
344     def _next_ten_exponent(self, val):
345         ten_exponent = 1
346         while val // 10 &gt; 0:
347             ten_exponent += 1
348             val //= 10
349         return ten_exponent
350     def scale_numpy_arrays_inplace(self, A, B, alpha):
351         scale_factor = 1
352         if alpha != 1:
353             scale_factor *= alpha
354         max_a = math.floor(abs(A.max()))
355         max_b = math.floor(abs(B.max()))
356         if max_a or max_b:
357             m_a = self._next_ten_exponent(max_a)
358             m_b = self._next_ten_exponent(max_b)
359             max_m = max(m_a, m_b)
360             scale_factor *= 10 ** max_m
361         if scale_factor != 1:
362             A /= scale_factor
363             B /= scale_factor
364     def get_atol_rtol(self, algo, dtype, precision):
365         if dtype == 'float16':
366             return (5e-2, 5e-2)
367         if algo == 'winograd_non_fused' and dtype == precision == 'float32':
368             return (1e-4, 1e-4)
369         return None, None
370     def __init__(self):
371         utt.seed_rng(1234)
372         self.dtype_configs = cudnn.get_supported_dtype_configs(check_dtype_config_support)
373     def array_like_conv_output(self, inputs_shape, filters_shape, border_mode, subsample, dilation, dtype):
374         out_shp = get_conv_output_shape(inputs_shape, filters_shape, border_mode, subsample, dilation)
375         out_shp = assert_conv_shape(out_shp)
376         return np.random.random(out_shp).astype(dtype)
377         inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters
378         inputs_val <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
379         filters_val = np.random.random(filters_shape).astype(dtype)
380         inputs_val /= 10
381         filters_val /= 10
382         inputs = theano.shared(inputs_val)
383         filters = theano.shared(filters_val)
384         if beta == 0:
385             out = None
386         else:
387             out /= 10
388         conv = dnn_conv<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(img=inputs, kerns=filters, alpha=alpha, beta=beta, out=out, border_mode=border_mode,
389                         subsample=subsample, dilation=dilation, conv_mode=conv_mode, algo=algo, precision=precision)
390         f = theano.function([], conv, mode=</b></font>mode_with_gpu)
391         if conv_mode == 'conv':
392             if inputs.ndim == 5:
393                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
394             else:
395                 flipped_filters = filters[:, :, ::-1, ::-1]
396         else:
397             flipped_filters = filters
398         conv_ref = self.cpu_conv_class(border_mode=border_mode,
399                                        subsample=subsample,
400                                        filter_dilation=dilation)(ref_cast(inputs), flipped_filters)
401         f_ref = theano.function([], conv_ref, mode="FAST_RUN")
402         res_ref = f_ref()
403         res = np.asarray(f())
404         if algo in cudnn.deterministic_fwd_algorithms:
405             utt.assert_allclose(res, np.asarray(f()))
406         atol, rtol = self.get_atol_rtol(algo, dtype, precision)
407         if beta == 0:
408             cpu_res = alpha * res_ref
409         else:
410             cpu_res = alpha * res_ref + beta * out
411         self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
412         utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)
413     def run_conv_gradinput(self, algo, dtype, precision, parameters):
414         inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters
415         if beta == 0:
416             inputs_val = None
417         else:
418             inputs_val = np.random.random(inputs_shape).astype(dtype)
419             inputs_val /= 10
420         filters_val = np.random.random(filters_shape).astype(dtype)
421         topgrad_val = self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)
422         filters_val /= 10
423         topgrad_val /= 10
424         filters = theano.shared(filters_val)
425         grad_i = dnn_gradinput<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(filters, topgrad, inputs_shape, alpha=alpha, beta=beta, out=inputs_val,
426                                border_mode=border_mode, subsample=subsample, dilation=dilation, conv_mode=conv_mode,
427                                algo=algo, precision=precision)
428         f = theano.function([], grad_i, mode=</b></font>mode_with_gpu)
429         if conv_mode == 'conv':
430             if filters.ndim == 5:
431                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
432             else:
433                 flipped_filters = filters[:, :, ::-1, ::-1]
434         else:
435             flipped_filters = filters
436         grad_i_ref = self.cpu_gradinput_class(border_mode=border_mode,
437                                               subsample=subsample,
438                                               filter_dilation=dilation
439                                               )(ref_cast(flipped_filters), ref_cast(topgrad), inputs_shape[2:])
440         f_ref = theano.function([], grad_i_ref, mode="FAST_RUN")
441         res_ref = f_ref()
442         res = np.asarray(f())
443         if algo in cudnn.deterministic_bwd_data_algorithms:
444             utt.assert_allclose(res, np.asarray(f()))
445         atol, rtol = self.get_atol_rtol(algo, dtype, precision)
446         if beta == 0:
447             cpu_res = alpha * res_ref
448         else:
449             cpu_res = alpha * res_ref + beta * inputs_val
450         self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
451         utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)
452     def run_conv_gradweight(self, algo, dtype, precision, parameters):
453         inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters
454         inputs_val = np.random.random(inputs_shape).astype(dtype)
455         if beta == 0:
456             filters_val = None
457         else:
458             filters_val = np.random.random(filters_shape).astype(dtype)
459             filters_val /= 10
460         topgrad_val = self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)
461         inputs_val /= 10
462         topgrad_val /= 10
463         inputs = theano.shared(inputs_val)
464         grad_w = dnn_gradweight<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(inputs, topgrad, filters_shape, alpha=alpha, beta=beta, out=filters_val,
465                                 border_mode=border_mode, subsample=subsample, dilation=dilation, conv_mode=conv_mode,
466                                 algo=algo, precision=precision)
467         f = theano.function([], grad_w, mode=mode_with_gpu)
468         grad_w_ref =</b></font> self.cpu_gradweight_class(border_mode=border_mode,
469                                                subsample=subsample,
470                                                filter_dilation=dilation)(ref_cast(inputs), ref_cast(topgrad),
471                                                                          filters_shape[2:])
472         if conv_mode == 'conv':
473             if inputs.ndim == 5:
474                 grad_w_ref = grad_w_ref[:, :, ::-1, ::-1, ::-1]
475             else:
476                 grad_w_ref = grad_w_ref[:, :, ::-1, ::-1]
477         f_ref = theano.function([], grad_w_ref, mode="FAST_RUN")
478         res_ref = f_ref()
479         res = np.asarray(f())
480         if algo in cudnn.deterministic_bwd_filter_algorithms:
481             utt.assert_allclose(res, np.asarray(f()))
482         atol, rtol = self.get_atol_rtol(algo, dtype, precision)
483         if beta == 0:
484             cpu_res = alpha * res_ref
485         else:
486             cpu_res = alpha * res_ref + beta * filters_val
487         self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
488         utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)
489     def should_fail(self, function, *args):
490         try:
491             print('(should fail)', file=sys.stderr, end=' ')
492             function(*args)
493         except Exception:
494             pass
495         else:
496             raise AssertionError('Should fail', callable.__name__, *args)
497     def should_fail_fwd(self, *args):
498         self.should_fail(self.run_conv_fwd, *args)
499     def should_fail_gradinput(self, *args):
500         self.should_fail(self.run_conv_gradinput, *args)
501     def should_fail_gradweight(self, *args):
502         self.should_fail(self.run_conv_gradweight, *args)
503     def get_expected_tcount(self):
504         return (sum(1 for t in self.test_fwd()) +
505                 sum(1 for t in self.test_gradweight()) +
506                 sum(1 for t in self.test_gradinput()) +
507                 sum(1 for t in self.test_fwd_runtime_algorithms()) +
508                 sum(1 for t in self.test_gradweight_runtime_algorithms()) +
509                 sum(1 for t in self.test_gradinput_runtime_algorithms()))
510     def test_fwd(self):
511         for dtype, precision in self.dtype_configs:
512             algos = [algo for algo in self.fwd_algorithms
513                      if cudnn.fwd_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
514             for algo in algos:
515                 for parameters in cudnn_conv_case_generator.fwd(algo, self.ndim, dtype, precision).get_cases():
516                     yield (self.run_conv_fwd, algo, dtype, precision, parameters)
517             if algos:
518                 for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
519                     for parameters in cudnn_conv_case_generator.fwd(algo, self.ndim, dtype, precision).get_cases():
520                         yield (self.run_conv_fwd, algo, dtype, precision, parameters)
521         for dnn_case in self.special_cases:
522             if dnn_case.is_fwd():
523                 if dnn_case.should_fail:
524                     yield (self.should_fail_fwd,) + dnn_case.get_case()
525                 else:
526                     yield (self.run_conv_fwd,) + dnn_case.get_case()
527     def test_gradinput(self):
528         for dtype, precision in self.dtype_configs:
529             algos = [algo for algo in self.bwd_data_algorithms
530                      if cudnn.bwd_data_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
531             for algo in algos:
532                 for parameters in cudnn_conv_case_generator.gi(algo, self.ndim, dtype, precision).get_cases():
533                     yield (self.run_conv_gradinput, algo, dtype, precision, parameters)
534             if algos:
535                 for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
536                     for parameters in cudnn_conv_case_generator.gi(algo, self.ndim, dtype, precision).get_cases():
537                         yield (self.run_conv_gradinput, algo, dtype, precision, parameters)
538         for dnn_case in self.special_cases:
539             if dnn_case.is_bwd_data():
540                 if dnn_case.should_fail:
541                     yield (self.should_fail_gradinput,) + dnn_case.get_case()
542                 else:
543                     yield (self.run_conv_gradinput,) + dnn_case.get_case()
544     def test_gradweight(self):
545         for dtype, precision in self.dtype_configs:
546             algos = [algo for algo in self.bwd_filter_algorithms
547                      if cudnn.bwd_filter_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
548             for algo in algos:
549                 for parameters in cudnn_conv_case_generator.gw(algo, self.ndim, dtype, precision).get_cases():
550                     yield (self.run_conv_gradweight, algo, dtype, precision, parameters)
551             if algos:
552                 for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
553                     for parameters in cudnn_conv_case_generator.gw(algo, self.ndim, dtype, precision).get_cases():
554                         yield (self.run_conv_gradweight, algo, dtype, precision, parameters)
555         for dnn_case in self.special_cases:
556             if dnn_case.is_bwd_filter():
557                 if dnn_case.should_fail:
558                     yield (self.should_fail_gradweight,) + dnn_case.get_case()
559                 else:
560                     yield (self.run_conv_gradweight,) + dnn_case.get_case()
561     def test_fwd_runtime_algorithms(self):
562         dtype = 'float32'
563         unit_shape = (1,) * self.ndim
564         _broadcastable = [False] * (2 + self.ndim)
565         def run_fwd_runtime_algorithm(algo):
566             inputs = theano.tensor.TensorType(dtype, _broadcastable)()
567             lower_inputs <font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= inputs / 10
568             lower_filters = filters / 10
569             conv = dnn_conv(img=lower_inputs, kerns=lower_filters, algo=algo, precision=dtype,
570                             subsample=unit_shape, dilation=unit_shape)
571             f = theano.function(</b></font>[inputs, filters], conv, mode=mode_with_gpu)
572             if self.ndim == 3:
573                 flipped_filters = lower_filters[:, :, ::-1, ::-1, ::-1]
574             else:
575                 flipped_filters = lower_filters[:, :, ::-1, ::-1]
576             conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(lower_inputs), flipped_filters)
577             f_ref = theano.function([inputs, filters], conv_ref, mode='FAST_RUN')
578             runtime_shapes = self.runtime_shapes
579             if algo in ('time_once', 'guess_once'):
580                 runtime_shapes = [list(runtime_shapes[0])]
581                 runtime_shapes[0][0] = 5
582                 print('Shapes:', inputs_shape, filters_shape)
583                 for i in range(ntimes):
584                     inputs_val <font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
585                     filters_val = np.random.random(filters_shape).astype(dtype)
586                     gpu_res = np.asarray(</b></font>f(inputs_val, filters_val))
587                     cpu_res = f_ref(inputs_val, filters_val)
588                     self.scale_numpy_arrays_inplace(cpu_res, gpu_res, 1)
589                     utt.assert_allclose(cpu_res, gpu_res)
590         for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
591             yield (run_fwd_runtime_algorithm, algo)
592     def test_gradinput_runtime_algorithms(self):
593         dtype = 'float32'
594         unit_shape = (1,) * self.ndim
595         _broadcastable = [False] * (2 + self.ndim)
596         def run_gradinput_runtime_algorithm(algo):
597             theano.config.dnn.conv.algo_bwd_data = algo
598             inputs = theano.tensor.TensorType(dtype, _broadcastable)()
599             filters = theano.tensor.TensorType(dtype, _broadcastable)()
600             conv = dnn_conv(img=inputs, kerns=filters, algo=algo, precision=dtype,
601                             subsample=unit_shape, dilation=unit_shape)
602             grad_i = theano.tensor.grad(conv.sum(), [inputs])
603             f = theano.function([inputs, filters], grad_i, mode=mode_with_gpu)
604             assert 1 == len([node for node in f.maker.fgraph.apply_nodes if isinstance(node.op, GpuDnnConvGradI)])
605             assert not any(isinstance(node.op, GpuDnnConv) for node in f.maker.fgraph.apply_nodes)
606             assert not any(isinstance(node.op, GpuDnnConvGradW) for node in f.maker.fgraph.apply_nodes)
607             if self.ndim == 3:
608                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
609             else:
610                 flipped_filters = filters[:, :, ::-1, ::-1]
611             conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(inputs), flipped_filters)
612             grad_i_ref = theano.tensor.grad(conv_ref.sum(), [inputs])
613             f_ref = theano.function([inputs, filters], grad_i_ref, mode='FAST_RUN')
614             runtime_shapes = self.runtime_shapes
615             if algo in ('time_once', 'guess_once'):
616                 runtime_shapes = [list(runtime_shapes[0])]
617                 runtime_shapes[0][0] = 5
618                 print('Shapes:', inputs_shape, filters_shape)
619                 for i in range(ntimes):
620                     inputs_val <font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
621                     filters_val = np.random.random(filters_shape).astype(dtype)
622                     gpu_res =</b></font> f(inputs_val, filters_val)
623                     cpu_res = f_ref(inputs_val, filters_val)
624                     utt.assert_allclose(cpu_res, np.asarray(gpu_res))
625         for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
626             yield (run_gradinput_runtime_algorithm, algo)
627     def test_gradweight_runtime_algorithms(self):
628         dtype = 'float32'
629         unit_shape = (1,) * self.ndim
630         _broadcastable = [False] * (2 + self.ndim)
631         def run_gradweight_runtime_algorithm(algo):
632             theano.config.dnn.conv.algo_bwd_filter = algo
633             inputs = theano.tensor.TensorType(dtype, _broadcastable)()
634             filters = theano.tensor.TensorType(dtype, _broadcastable)()
635             conv = dnn_conv(img=inputs, kerns=filters, algo=algo, precision=dtype,
636                             subsample=unit_shape, dilation=unit_shape)
637             grad_w = theano.tensor.grad(conv.sum(), [filters])
638             f = theano.function([inputs, filters], grad_w, mode=mode_with_gpu)
639             assert 1 == len([node for node in f.maker.fgraph.apply_nodes if isinstance(node.op, GpuDnnConvGradW)])
640             assert not any(isinstance(node.op, GpuDnnConv) for node in f.maker.fgraph.apply_nodes)
641             assert not any(isinstance(node.op, GpuDnnConvGradI) for node in f.maker.fgraph.apply_nodes)
642             if self.ndim == 3:
643                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
644             else:
645                 flipped_filters = filters[:, :, ::-1, ::-1]
646             conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(inputs), flipped_filters)
647             grad_w_ref = theano.tensor.grad(conv_ref.sum(), [filters])
648             f_ref = theano.function([inputs, filters], grad_w_ref, mode='FAST_RUN')
649             runtime_shapes = self.runtime_shapes
650             if algo in ('time_once', 'guess_once'):
651                 runtime_shapes = [list(runtime_shapes[0])]
652                 runtime_shapes[0][0] = 5
653             for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
654                 print('Shapes:', inputs_shape, filters_shape)
655                 for i in range(ntimes):
656                     inputs_val = np.random.random(inputs_shape).astype(dtype)
657                     filters_val = np.random.random(filters_shape).astype(dtype)
658                     gpu_res = f(inputs_val, filters_val)
659                     cpu_res = f_ref(inputs_val, filters_val)
660                     utt.assert_allclose(cpu_res, np.asarray(gpu_res))
661         for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
662             yield (run_gradweight_runtime_algorithm, algo)
663     ndim = 2
664     fwd_algorithms <font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= cudnn.cudnnConvolutionFwdAlgo_t.get_aliases()
665     bwd_filter_algorithms = cudnn.cudnnConvolutionBwdFilterAlgo_t.get_aliases()
666     bwd_data_algorithms = cudnn.cudnnConvolutionBwdDataAlgo_t.get_aliases()
667     cpu_conv_class =</b></font> CorrMM
668     cpu_gradinput_class = CorrMM_gradInputs
669     cpu_gradweight_class = CorrMM_gradWeights
670     special_cases = [ConvCase.bwd_filter(algo='deterministic', dtype='float32', precision='float32',
671                                          inputs_shape=(1, 1, 541211, 10), filters_shape=(50, 1, 3, 10),
672                                          border_mode=(1, 0), should_fail=(cudnn.version &lt;= 6)),
673                      ConvCase.fwd(algo='small', dtype='float32', precision='float32',
674                                   inputs_shape=(65536, 2, 2, 2), filters_shape=(1, 2, 2, 2)),
675                      ConvCase.fwd(algo='small', dtype='float32', precision='float32',
676     runtime_shapes = [
677         <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, [(2, 3, 10, 9), (5, 3, 7, 7)]),
678         (1, [(1, 1, 100, 200), (1, 1, 50, 200)]),
679         (1, [(4, 2, 20, 20), (2, 2, 20, 19)]),
680         (3, [(2, 3, 10, 9), (5, 3, 7, 7)]),  # cache should be used
681         (1, [(2, 2, 50, 50), (5, 2, 25, 31)]),
682         (1</b></font>, [(1, 1, 100, 200), (1, 1, 50, 200)]),  # cache should be used
683         (1, [(4, 2, 20, 20), (2, 2, 20, 19)]),  # cache should be used
684         (1, [(1, 2, 3, 4), (6, 2, 2, 1)])
685     ]
686 class TestDnnConv3D(BaseTestDnnConv):
687     ndim = 3
688     fwd_algorithms = cudnn.conv3d_fwd_algorithms
689     bwd_filter_algorithms = cudnn.conv3d_bwd_filter_algorithms
690     bwd_data_algorithms = cudnn.conv3d_bwd_data_algorithms
691     cpu_conv_class = Corr3dMM
692     cpu_gradinput_class = Corr3dMM_gradInputs
693     cpu_gradweight_class = Corr3dMM_gradWeights
694     special_cases = [ConvCase.fwd(algo='small', dtype='float32', precision='float32',
695                                   inputs_shape=(65536, 2, 2, 2, 2), filters_shape=(1, 2, 2, 2, 2)),
696                      ConvCase.fwd(algo='small', dtype='float32', precision='float32',
697     runtime_shapes = [
698         <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, [(2, 3, 5, 10, 9), (5, 3, 4, 7, 7)]),
699         (1, [(4, 2, 20, 20, 20), (2, 2, 20, 19, 18)]),
700         (3, [(2, 3, 5, 10, 9), (5, 3, 4, 7, 7)]),  # cache should be used
701         (1</b></font>, [<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(2, 2, 50, 50, 5), (5, 2, 25, 31, 4)]),
702         (1, [(1, 1, 5, 100, 200), (1, 1, 4, 50, 200)]),  # cache should be used
703         (1, [(4, 2, 20, 20, 20), (2, 2, 20, 19, 18)]),  # cache should be used
704         (1, [(1, 2, 3, 4, 5), (6</b></font>, 2, 3, 2, 1)])
705     ]
706 def test_true_half_config_support():
707     if not check_dtype_config_support('float16', 'float16'):
708         raise SkipTest('FWD: TRUE_HALF_CONFIG not supported on this GPU.')
709 class CheckDnn:
710     @staticmethod
711     def dtype_config_to_str(dtype_config):
712         dtype, precision = dtype_config
713         if dtype == precision == 'float16':
714             return 'TRUE_HALF_CONFIG'
715         if dtype == 'float16' and precision == 'float32':
716             return 'PSEUDO_HALF_CONFIG'
717         if dtype == precision == 'float32':
718             return 'FLOAT_CONFIG'
719         if dtype == precision == 'float64':
720             return 'DOUBLE_CONFIG'
721         raise ValueError('unknown data type configuration', dtype_config)
722     @staticmethod
723     def print_infos(count_tests=True):
724         test_2d = TestDnnConv2D()
725         test_3d = TestDnnConv3D()
726         print()
727         print('Available data type configurations:',
728               ', '.join(CheckDnn.dtype_config_to_str(d)
729                         for d in cudnn.get_supported_dtype_configs(check_dtype_config_support)))
730         print()
731         print('2D algorithms:')
732         print('FWD        :', ', '.join(test_2d.fwd_algorithms))
733         print('BWD FILTER :', ', '.join(test_2d.bwd_filter_algorithms))
734         print('BWD DATA   :', ', '.join(test_2d.bwd_data_algorithms))
735         print()
736         print('3D algorithms:')
737         print('FWD        :', ', '.join(test_3d.fwd_algorithms))
738         print('BWD FILTER :', ', '.join(test_3d.bwd_filter_algorithms))
739         print('BWD DATA   :', ', '.join(test_3d.bwd_data_algorithms))
740         print()
741         if count_tests:
742             count_tests_2d = test_2d.get_expected_tcount()
743             count_tests_3d = test_3d.get_expected_tcount()
744             print(count_tests_2d, 'conv2D test cases.')
745             print(count_tests_3d, 'conv3D test cases.')
746             print('1 supplementary test.')
747             print(count_tests_2d + count_tests_3d + 1, 'total conv tests.')
748             print()
749     @staticmethod
750     def print_tests():
751         for test in (TestDnnConv2D(), TestDnnConv3D()):
752             for tcase in test.test_fwd():
753                 print(tcase[0].__name__, *tcase[1:])
754             for tcase in test.test_gradinput():
755                 print(tcase[0].__name__, *tcase[1:])
756             for tcase in test.test_gradweight():
757                 print(tcase[0].__name__, *tcase[1:])
758             for tcase in test.test_fwd_runtime_algorithms():
759                 print(tcase[0].__name__, *tcase[1:])
760             for tcase in test.test_gradinput_runtime_algorithms():
761                 print(tcase[0].__name__, *tcase[1:])
762             for tcase in test.test_gradweight_runtime_algorithms():
763                 print(tcase[0].__name__, *tcase[1:])
764         print(test_true_half_config_support.__name__)
765 if __name__ == '__main__':
766     args = sys.argv[1:]
767     if len(args) == 1 and args[0] in ('infos', 'list'):
768         if args[0] == 'infos':
769             CheckDnn.print_infos()
770         if args[0] == 'list':
771             CheckDnn.print_tests()
772     else:
773         module_name = sys.modules[__name__].__file__
774         if len(args) == 0:
775             args = ['--verbose', '--nocapture']
776         argv = [sys.argv[0], module_name] + args
777         CheckDnn.print_infos()
778         nose.main(argv=argv)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
