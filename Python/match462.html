<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html><head><title>Matches for test_abstract_conv.py &amp; check_dnn_conv.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_abstract_conv.py &amp; check_dnn_conv.py
      </h3>
<h1 align="center">
        7.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_abstract_conv.py (5.62056%)<th>check_dnn_conv.py (12.413475%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(377-389)<td><a href="#" name="0">(538-552)</a><td align="center"><font color="#ff0000">23</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1678-1679)<td><a href="#" name="1">(932-937)</a><td align="center"><font color="#f30000">22</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(7-34)<td><a href="#" name="2">(16-44)</a><td align="center"><font color="#d20000">19</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1313-1325)<td><a href="#" name="3">(961-965)</a><td align="center"><font color="#c70000">18</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(1009-1013)<td><a href="#" name="4">(198-202)</a><td align="center"><font color="#c70000">18</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(765-771)<td><a href="#" name="5">(555-557)</a><td align="center"><font color="#b10000">16</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1362-1372)<td><a href="#" name="6">(965-968)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(813-817)<td><a href="#" name="7">(231-237)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(701-706)<td><a href="#" name="8">(664-671)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(442-445)<td><a href="#" name="9">(822-824)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(923-928)<td><a href="#" name="10">(608-612)</a><td align="center"><font color="#9b0000">14</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(595-600)<td><a href="#" name="11">(223-228)</a><td align="center"><font color="#9b0000">14</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(1675-1676)<td><a href="#" name="12">(117-120)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1507-1511)<td><a href="#" name="13">(914-918)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(794-799)<td><a href="#" name="14">(145-147)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(505-507)<td><a href="#" name="15">(862-864)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(401-411)<td><a href="#" name="16">(804-808)</a><td align="center"><font color="#900000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_abstract_conv.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
from __future__ import absolute_import, print_function, division
import unittest
import numpy as np
<a name="2"></a>from nose.plugins.skip import SkipTest
from nose.tools import assert_raises, assert_true

<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import theano
from theano import tensor
from theano import change_flags
from theano.gof.opt import check_stack_trace
from theano.tests import unittest_tools as utt
from theano.tensor.nnet import (corr, corr3d, conv2d_transpose,
                                abstract_conv as conv)
from theano.tensor.nnet.abstract_conv import (get_conv_output_shape,
                                              get_conv_gradweights_shape,
                                              get_conv_gradinputs_shape,
                                              check_conv_gradinputs_shape,
                                              assert_conv_shape,
                                              assert_shape)
from theano.tensor.nnet.abstract_conv import AbstractConv2d
from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradInputs
from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradWeights
from theano.tensor.nnet.abstract_conv import bilinear_kernel_1D
from theano.tensor.nnet.abstract_conv import bilinear_kernel_2D
from theano.tensor.nnet.abstract_conv import bilinear_upsampling
from theano.tensor.nnet.abstract_conv import separable_conv2d, separable_conv3d
from theano.tensor.nnet.abstract_conv import causal_conv1d
from theano.tensor.nnet.corr import (CorrMM, CorrMM_gradWeights,
                                     CorrMM_gradInputs)
from theano.tensor.nnet.corr3d import (Corr3dMM, Corr3dMM_gradWeights,
                                       Corr3dMM_gradInputs)


def conv2d_corr(inputs, filters, border_mode=</b></font>"valid",
                subsample=(1, 1), conv_mode='conv',
                filter_dilation=(1, 1)):
    if conv_mode == 'conv':
        filters = filters[:, :, ::-1, ::-1]
    return corr.CorrMM(border_mode,
                       subsample,
                       filter_dilation)(inputs, filters)


def conv2d_corr_gw(inputs, topgrad, filters_shape,
                   border_mode="valid", subsample=(1, 1),
                   conv_mode='conv', filter_dilation=(1, 1)):
    rval = corr.CorrMM_gradWeights(border_mode,
                                   subsample,
                                   filter_dilation)(inputs, topgrad,
                                                    filters_shape[2:])
    if conv_mode == 'conv':
        rval = rval[:, :, ::-1, ::-1]
    return rval


def conv2d_corr_gi(filters, topgrad, inputs_shape,
                   border_mode="valid", subsample=(1, 1),
                   conv_mode='conv', filter_dilation=(1, 1)):
    if conv_mode == 'conv':
        filters = filters[:, :, ::-1, ::-1]
    return corr.CorrMM_gradInputs(border_mode,
                                  subsample,
                                  filter_dilation)(filters,
                                                   topgrad,
                                                   inputs_shape[2:])


def conv3d_corr(inputs, filters, border_mode="valid",
                subsample=(1, 1, 1), conv_mode='conv',
                filter_dilation=(1, 1, 1)):
    if conv_mode == 'conv':
        filters = filters[:, :, ::-1, ::-1, ::-1]
    return corr3d.Corr3dMM(border_mode,
                           subsample,
                           filter_dilation)(inputs, filters)


def conv3d_corr_gw(inputs, topgrad, filters_shape,
                   border_mode="valid", subsample=(1, 1, 1),
                   conv_mode='conv', filter_dilation=(1, 1, 1)):
    rval = corr3d.Corr3dMM_gradWeights(border_mode,
                                       subsample,
                                       filter_dilation)(inputs, topgrad,
                                                        filters_shape[2:])
    if conv_mode == 'conv':
        rval = rval[:, :, ::-1, ::-1, ::-1]
    return rval


def conv3d_corr_gi(filters, topgrad, inputs_shape,
                   border_mode="valid", subsample=(1, 1, 1),
                   conv_mode='conv', filter_dilation=(1, 1, 1)):
    if conv_mode == 'conv':
        filters = filters[:, :, ::-1, ::-1, ::-1]
    return corr3d.Corr3dMM_gradInputs(border_mode,
                                      subsample,
                                      filter_dilation)(filters,
                                                       topgrad,
                                                       inputs_shape[2:])


class TestGetConvOutShape(unittest.TestCase):
    def test_basic(self):
        image_shape, kernel_shape = (3, 2, 12, 9), (4, 2, 5, 6)
        sub_sample = (1, 2)
        filter_dilation = (2, 1)
        test1_params = get_conv_output_shape(
            image_shape, kernel_shape, 'valid', sub_sample, filter_dilation)
        test2_params = get_conv_output_shape(
            image_shape, kernel_shape, 'half', sub_sample, filter_dilation)
        test3_params = get_conv_output_shape(
            image_shape, kernel_shape, 'full', sub_sample, filter_dilation)
        test4_params = get_conv_output_shape(
            image_shape, kernel_shape, (1, 2), sub_sample, filter_dilation)

        self.assertTrue(test1_params == (3, 4, 4, 2))
        self.assertTrue(test2_params == (3, 4, 12, 5))
        self.assertTrue(test3_params == (3, 4, 20, 7))
        self.assertTrue(test4_params == (3, 4, 6, 4))

    def test_basic_3d(self):
        image_shape, kernel_shape = (3, 2, 12, 9, 7), (4, 2, 5, 6, 4)
        sub_sample = (1, 2, 1)
        filter_dilation = (2, 1, 1)
        test1_params = get_conv_output_shape(
            image_shape, kernel_shape, 'valid', sub_sample, filter_dilation)
        test2_params = get_conv_output_shape(
            image_shape, kernel_shape, 'half', sub_sample, filter_dilation)
        test3_params = get_conv_output_shape(
            image_shape, kernel_shape, 'full', sub_sample, filter_dilation)
        test4_params = get_conv_output_shape(
            image_shape, kernel_shape, (1, 2, 3), sub_sample, filter_dilation)

        self.assertTrue(test1_params == (3, 4, 4, 2, 4))
        self.assertTrue(test2_params == (3, 4, 12, 5, 8))
        self.assertTrue(test3_params == (3, 4, 20, 7, 10))
        self.assertTrue(test4_params == (3, 4, 6, 4, 10))


class TestConvGradInputsShape(unittest.TestCase):
    def test_check_shape(self):
        for i in range(1, 20):
            for k in range(1, 10):
                for b in ('valid', 'half', 'full', (0, 2)):
                    for s in (1, 2, 3):
                        for d in (1, 2, 3):
                            image_shape = (59, 61, i, i)
                            kernel_shape = (67, 61, k, k)

                            # compute the output that these inputs and parameters would produce
                            computed_shape = get_conv_output_shape(
                                image_shape, kernel_shape, b, (s, s), (d, d))
                            # this should be accepted
                            self.assertTrue(check_conv_gradinputs_shape(
                                image_shape, kernel_shape, computed_shape, b, (s, s), (d, d)))

                            # one or more None should also be accepted
                            trial_shape = (None, None, computed_shape[2], None)
                            self.assertTrue(check_conv_gradinputs_shape(
                                image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))

                            # the batch size and number of filters are important
                            trial_shape = (1, 1, computed_shape[2], computed_shape[3])
                            self.assertFalse(check_conv_gradinputs_shape(
                                image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))

                            # outputs that are too large or too small should be rejected
                            for o in (-3, -2, -1, 1, 2, 3):
                                trial_shape = (computed_shape[0], computed_shape[1],
                                               computed_shape[2] + o, computed_shape[3] + o)
                                self.assertFalse(check_conv_gradinputs_shape(
                                    image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))

    def test_get_shape(self):
        for i in range(1, 20):
            for k in range(1, 10):
                for b in ('valid', 'half', 'full', (0, 2)):
                    for d in (1, 2, 3):
                        image_shape = (59, 61, i, i)
                        kernel_shape = (67, 61, k, k)

                        # compute the output that these inputs and parameters would produce
                        output_shape = get_conv_output_shape(
                            image_shape, kernel_shape, b, (1, 1), (d, d))

                        # compute the image_shape given this output_shape
                        computed_image_shape = get_conv_gradinputs_shape(
                            kernel_shape, output_shape, b, (1, 1), (d, d))
                        self.assertEqual(computed_image_shape, image_shape)

                        # if subsample &gt; 1, the shape should be None
                        computed_image_shape = get_conv_gradinputs_shape(
                            kernel_shape, output_shape, b, (2, 3), (d, d))
                        image_shape_with_None = image_shape[:2] + (None, None)
                        self.assertEqual(computed_image_shape, image_shape_with_None)

                        # compute the kernel_shape given this output_shape
                        computed_kernel_shape = get_conv_gradweights_shape(
                            image_shape, output_shape, b, (1, 1), (d, d))

                        # if border_mode == 'half', the shape should be None
                        if b == 'half':
                            kernel_shape_with_None = kernel_shape[:2] + (None, None)
                            self.assertEqual(computed_kernel_shape, kernel_shape_with_None)
                        else:
                            self.assertEqual(computed_kernel_shape, kernel_shape)

                        # if subsample &gt; 1, the shape should be None
                        computed_kernel_shape = get_conv_gradweights_shape(
                            kernel_shape, output_shape, b, (2, 3), (d, d))
                        kernel_shape_with_None = kernel_shape[:2] + (None, None)
                        self.assertEqual(computed_kernel_shape, kernel_shape_with_None)


class TestAssertConvShape(unittest.TestCase):
    def test_basic(self):
        shape = tuple(tensor.iscalar() for i in range(4))
        f = theano.function(shape, assert_conv_shape(shape))

        self.assertEqual([1, 2, 3, 4], f(1, 2, 3, 4))
        self.assertEqual([0, 0, 1, 1], f(0, 0, 1, 1))
        assert_raises(AssertionError, f, 3, 3, 3, 0)
        assert_raises(AssertionError, f, 3, 3, 0, 3)
        assert_raises(AssertionError, f, 3, 3, -1, 3)
        assert_raises(AssertionError, f, 3, -1, 3, 3)
        assert_raises(AssertionError, f, -1, 3, 3, 3)


class TestAssertShape(unittest.TestCase):
    @change_flags([("conv.assert_shape", True)])
    def test_basic(self):
        x = tensor.tensor4()
        s1 = tensor.iscalar()
        s2 = tensor.iscalar()
        expected_shape = [None, s1, s2, None]
        f = theano.function([x, s1, s2], assert_shape(x, expected_shape))

        v = np.zeros((3, 5, 7, 11), dtype='float32')
        self.assertEqual(0, np.sum(f(v, 5, 7)))

        assert_raises(AssertionError, f, v, 5, 0)
        assert_raises(AssertionError, f, v, 5, 9)
        assert_raises(AssertionError, f, v, 0, 7)
        assert_raises(AssertionError, f, v, 7, 7)

    @change_flags([("conv.assert_shape", True)])
    def test_shape_check_conv2d(self):
        input = tensor.tensor4()
        filters = tensor.tensor4()

        out = conv.conv2d(input, filters,
                          input_shape=(3, 5, 7, 11),
                          filter_shape=(7, 5, 3, 3))
        f = theano.function([input, filters], out)
        # mismatched input_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 5, 9, 11), dtype='float32'),
                      np.zeros((7, 5, 3, 3), dtype='float32'))
        # mismatched filter_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 5, 7, 11), dtype='float32'),
                      np.zeros((7, 5, 2, 2), dtype='float32'))

    @change_flags([("conv.assert_shape", True)])
    def test_shape_check_conv3d(self):
        if theano.config.cxx == "":
            raise SkipTest("test needs cxx")
        input = tensor.tensor5()
        filters = tensor.tensor5()

        out = conv.conv3d(input, filters,
                          input_shape=(3, 5, 7, 11, 13),
                          filter_shape=(7, 5, 3, 3, 3))
        f = theano.function([input, filters], out)
        # mismatched input_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 5, 9, 11, 13), dtype='float32'),
                      np.zeros((7, 5, 3, 3, 3), dtype='float32'))
        # mismatched filter_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 5, 7, 11, 13), dtype='float32'),
                      np.zeros((7, 5, 2, 2, 2), dtype='float32'))

    @change_flags([("conv.assert_shape", True)])
    def test_shape_check_conv2d_grad_wrt_inputs(self):
        output_grad = tensor.tensor4()
        filters = tensor.tensor4()

        out = conv.conv2d_grad_wrt_inputs(output_grad, filters,
                                          input_shape=(None, None, 7, 11),
                                          filter_shape=(7, 5, 3, 3))
        f = theano.function([output_grad, filters], out)
        # mismatched filter_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 6, 5, 9), dtype='float32'),
                      np.zeros((7, 6, 3, 3), dtype='float32'))

    @change_flags([("conv.assert_shape", True)])
    def test_shape_check_conv3d_grad_wrt_inputs(self):
        if theano.config.cxx == "":
            raise SkipTest("test needs cxx")
        output_grad = tensor.tensor5()
        filters = tensor.tensor5()

        out = conv.conv3d_grad_wrt_inputs(output_grad, filters,
                                          input_shape=(None, None, 7, 11, 13),
                                          filter_shape=(7, 5, 3, 3, 3))
        f = theano.function([output_grad, filters], out)
        # mismatched filter_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 6, 5, 9, 11), dtype='float32'),
                      np.zeros((7, 6, 3, 3, 3), dtype='float32'))

    @change_flags([("conv.assert_shape", True)])
    def test_shape_check_conv2d_grad_wrt_weights(self):
        input = tensor.tensor4()
        output_grad = tensor.tensor4()

        out = conv.conv2d_grad_wrt_weights(input, output_grad,
                                           filter_shape=(None, None, 3, 3),
                                           input_shape=(3, 5, 7, 11))
        f = theano.function([input, output_grad], out)
        # mismatched filter_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 6, 7, 11), dtype='float32'),
                      np.zeros((3, 7, 5, 9), dtype='float32'))

    @change_flags([("conv.assert_shape", True)])
    def test_shape_check_conv3d_grad_wrt_weights(self):
        if theano.config.cxx == "":
            raise SkipTest("test needs cxx")
        input = tensor.tensor5()
        output_grad = tensor.tensor5()

        out = conv.conv3d_grad_wrt_weights(input, output_grad,
                                           filter_shape=(None, None, 3, 3, 3),
                                           input_shape=(3, 5, 7, 11, 13))
        f = theano.function([input, output_grad], out)
        # mismatched filter_shape
        assert_raises(AssertionError, f,
                      np.zeros((3, 6, 7, 11, 13), dtype='float32'),
                      np.zeros((3, 7, 5, 9, 11), dtype='float32'))


class BaseTestConv(object):
    def get_output_shape(self, inputs_shape, filters_shape,
                         subsample, border_mode, filter_dilation):
        dil_filters = tuple((s - 1) * d + 1 for s, d in zip(filters_shape[2:],
                                                            filter_dilation))
        if border_mode == "valid":
            border_mode = (0,) * (len(inputs_shape) - 2)
        if border_mode == "half":
            border_mode = tuple(d // 2 for d in dil_filters)
        if border_mode == "full":
            border_mode = tuple(d - 1 for d in dil_filters)
        batch_size = inputs_shape[0]
        num_filters = filters_shape[0]
        return ((batch_size, num_filters,) +
                tuple(None if i is None or k is None
                      else ((i + 2 * pad - ((k - 1) * fd + 1)) // d + 1)
                      for i, k, d, pad, fd in zip(inputs_shape[2:],
                                                  filters_shape[2:],
                                                  subsample, border_mode,
                                                  filter_dilation)))

    def run_fwd(self, inputs_shape, filters_shape,
                conv_fn, conv_op, ref,
                subsample=None, verify_grad=True, mode=None,
                border_mode='valid', filter_flip=True,
                provide_shape=False, target_op=None,
                check_trace=False, filter_dilation=None):
        if subsample is None:
            subsample = (1,) * (len(inputs_shape) - 2)
<a name="0"></a>        if filter_dilation is None:
            filter_dilation = (1,) * (len(inputs_shape) - 2)

        inputs_val <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(inputs_shape).astype('float32')
        filters_val = np.random.random(filters_shape).astype('float32')

        # scale down values to prevent rounding errors
        inputs_val /= 10
        filters_val /= 10

        inputs = self.shared(inputs_val)
        filters = self.shared(filters_val)

        if provide_shape:
            imshp = inputs_shape
            kshp =</b></font> filters_shape
        else:
            imshp = None
            kshp = None
        if filter_flip:
            conv_mode = 'conv'
        else:
            conv_mode = 'cross'

<a name="16"></a>        c_ref = ref(inputs, filters,
                    border_mode=border_mode,
                    subsample=subsample,
                    conv_mode<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=conv_mode,
                    filter_dilation=filter_dilation)
        c = conv_fn(inputs, filters,
                    border_mode=border_mode,
                    subsample=subsample,
                    filter_flip=filter_flip,
                    input_shape=imshp,
                    filter_shape=kshp,
                    filter_dilation=filter_dilation)

        f_ref = theano.function(</b></font>[], c_ref, mode='FAST_RUN')
        f = theano.function([], c, mode=mode)

        if target_op is not None:
            assert any([isinstance(n.op, target_op) for n
                        in f.maker.fgraph.toposort()])
            if check_trace:
                assert_true(check_stack_trace(f, ops_to_check=target_op))

        res_ref = np.array(f_ref())
        res = np.array(f())
        utt.assert_allclose(res_ref, res)
        if verify_grad and inputs_val.size &gt; 0 and filters_val.size &gt; 0 and res.size &gt; 0:
            utt.verify_grad(conv_op(border_mode=border_mode,
                                    imshp=imshp, kshp=kshp,
                                    subsample=subsample,
                                    filter_dilation=filter_dilation),
                            [inputs_val, filters_val],
                            mode=mode)

    def run_gradweight(self, inputs_shape, filters_shape, output_shape,
                       gradWeights_fn, ref, subsample=None,
                       filter_flip=True, verify_grad=True, mode=None,
                       border_mode='valid', provide_shape=False,
                       target_op=None, check_trace=False,
                       filter_dilation=None):
        if subsample is None:
            subsample = (1,) * (len(inputs_shape) - 2)
<a name="9"></a>        if filter_dilation is None:
            filter_dilation = (1,) * (len(inputs_shape) - 2)

        inputs_val <font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(inputs_shape).astype('float32')
        output_val = np.random.random(output_shape).astype('float32')

        inputs = self.shared(</b></font>inputs_val)
        output = self.shared(output_val)

        if provide_shape:
            imshp = inputs_shape
            kshp = filters_shape
        else:
            imshp = None
            kshp = None
        if filter_flip:
            conv_mode = 'conv'
        else:
            conv_mode = 'cross'
        c = gradWeights_fn(border_mode=border_mode,
                           filter_flip=filter_flip,
                           subsample=subsample,
                           imshp=imshp, kshp=kshp,
                           filter_dilation=filter_dilation)
        c = c(inputs, output, filters_shape[2:])
        c_ref = ref(inputs, output,
                    filters_shape,
                    border_mode=border_mode,
                    subsample=subsample,
                    conv_mode=conv_mode,
                    filter_dilation=filter_dilation)
        f = theano.function([], c, mode=mode)
        f_ref = theano.function([], c_ref, mode='FAST_RUN')

        if target_op is not None:
            assert any([isinstance(n.op, target_op) for n
                        in f.maker.fgraph.toposort()])
            if check_trace:
                assert_true(check_stack_trace(f, ops_to_check=target_op))

        res_ref = np.array(f_ref())
        res = np.array(f())
        utt.assert_allclose(res_ref, res)

        def abstract_conv_gradweight(inputs_val, output_val):
            conv_op = gradWeights_fn(border_mode=border_mode,
                                     subsample=subsample,
                                     filter_dilation=filter_dilation)
            return conv_op(inputs_val, output_val, filters_shape[2:])

        if verify_grad and inputs_val.size &gt; 0 and output_val.size &gt; 0 and res.size &gt; 0:
            utt.verify_grad(abstract_conv_gradweight,
                            [inputs_val, output_val],
                            mode=mode, eps=1)

    def run_gradinput(self, inputs_shape, filters_shape, output_shape,
                      gradInputs_fn, ref,
                      subsample=None, filter_flip=True,
                      verify_grad=True, mode=None, border_mode='valid',
                      provide_shape=False, target_op=None,
                      check_trace=False, filter_dilation=None):
        if subsample is None:
            subsample = (1,) * (len(inputs_shape) - 2)
<a name="15"></a>        if filter_dilation is None:
            filter_dilation = (1,) * (len(inputs_shape) - 2)

        output_val <font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(output_shape).astype('float32')
        filters_val = np.random.random(filters_shape).astype('float32')
        output =</b></font> self.shared(output_val)
        filters = self.shared(filters_val)

        if provide_shape:
            imshp = inputs_shape
            kshp = filters_shape
        else:
            imshp = None
            kshp = None
        if filter_flip:
            conv_mode = 'conv'
        else:
            conv_mode = 'cross'
        c = gradInputs_fn(border_mode=border_mode,
                          subsample=subsample,
                          filter_flip=filter_flip,
                          imshp=imshp, kshp=kshp,
                          filter_dilation=filter_dilation)
        c = c(filters, output, inputs_shape[2:])
        f = theano.function([], c, mode=mode)

        # ref is set to None for the inconsistent-shape tests.
        # The reference function also raises an exception, which would
        # mask the exception generated by the target implementation.
        if ref is not None:
            c_ref = ref(filters, output, inputs_shape,
                        border_mode=border_mode, subsample=subsample,
                        conv_mode=conv_mode, filter_dilation=filter_dilation)
            f_ref = theano.function([], c_ref, mode='FAST_RUN')

        if target_op is not None:
            assert any([isinstance(n.op, target_op) for n
                        in f.maker.fgraph.toposort()])
            if check_trace:
                assert_true(check_stack_trace(f, ops_to_check=target_op))

        res = np.array(f())

        if ref is not None:
            res_ref = np.array(f_ref())
            utt.assert_allclose(res_ref, res)

        def abstract_conv_gradinputs(filters_val, output_val):
            conv_op = gradInputs_fn(border_mode=border_mode,
                                    subsample=subsample,
                                    filter_dilation=filter_dilation)
            return conv_op(filters_val, output_val, inputs_shape[2:])

        if verify_grad and filters_val.size &gt; 0 and output_val.size &gt; 0 and res.size &gt; 0:
            utt.verify_grad(abstract_conv_gradinputs,
                            [filters_val, output_val],
                            mode=mode, eps=1)

    def test_all(self):
        if type(self) is BaseTestConv:
            raise SkipTest("base class")
        ds = self.default_subsamples
        db = self.default_border_mode
        dflip = self.default_filter_flip
        dprovide_shape = self.default_provide_shape
        for (i, f) in zip(self.inputs_shapes, self.filters_shapes):
            for provide_shape in self.provide_shape:
                yield (self.tcase, i, f, ds, db, dflip, provide_shape)
            if min(i) &gt; 0 and min(f) &gt; 0:
                for fd in self.filters_dilations:
                    for s in self.subsamples:
                        for b in self.border_modes:
                            yield (self.tcase, i, f, s, b, dflip,
                                   dprovide_shape, fd)
                for flip in self.filter_flip:
                    yield (self.tcase, i, f, ds, db, flip, dprovide_shape)


class BaseTestConv2d(BaseTestConv):
    @classmethod
    def setup_class(cls):
        # This tests can run even when theano.config.blas.ldflags is empty.
        cls.inputs_shapes = [(8, 1, 6, 6), (8, 1, 8, 8), (2, 1, 7, 7),
                             (6, 1, 10, 11), (2, 1, 6, 5), (1, 5, 9, 9),
                             (0, 1, 6, 6), (1, 0, 6, 6), (1, 1, 6, 6)]
        cls.filters_shapes = [(5, 1, 2, 2), (4, 1, 3, 3), (2, 1, 3, 3),
                              (1, 1, 2, 3), (4, 1, 1, 3), (4, 5, 3, 2),
                              (1, 1, 2, 2), (1, 0, 2, 2), (0, 1, 2, 2)]
        cls.subsamples = [(1, 1), (2, 2), (2, 4)]
        cls.default_subsamples = (1, 1)
<a name="11"></a>        cls.filters_dilations = [(1, 1), (1, 2), (2, 1)]
        cls.default_filters_dilations = (1, 1)
        cls.border_modes = ["valid", "half", "full", (0, 0), (1, 1), (5, 5), (5, 2)]
        cls.default_border_mode <font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (0, 0)
        cls.filter_flip = [True, False]
        cls.default_filter_flip = True
        cls.provide_shape = [True, False]
        cls.default_provide_shape = True
        cls.shared =</b></font> staticmethod(theano.compile.shared)

    def test_gradinput_arbitrary_output_shapes(self):
        # this computes the grad wrt inputs for an output shape
        # that the forward convolution would not produce
        input_shape = (2, 1, 7, 7)
        filter_shape = (2, 1, 3, 3)
        for output_shape in [(2, 2, 8, 8), (2, 2, 9, 9), (2, 2, 12, 12)]:
            for border_mode in ["valid", "half", "full"]:
                computed_shape = get_conv_output_shape(
                    input_shape, filter_shape, border_mode, self.default_subsamples, self.default_filters_dilations)
                # is this a valid combination?
                if tuple(computed_shape) == output_shape:
                    yield (self.tcase_gi,
                           input_shape,
                           filter_shape,
                           output_shape,
                           self.default_subsamples,
                           border_mode,
                           True,
                           True,
                           self.default_filters_dilations,
                           False)
                else:
                    # expect an error
                    yield (self.tcase_gi,
                           input_shape,
                           filter_shape,
                           output_shape,
                           self.default_subsamples,
                           border_mode,
                           True,
                           True,
                           self.default_filters_dilations,
                           True)

    def test_gradinput_impossible_output_shapes(self):
        def run_for_output_offsets(image_shape, kernel_shape, s, border_mode, d):
            # outputs that are too large or too small should be rejected
            for o in (-3, -1, 1, 2):
                output_shape = (1, 1, computed_shape[2] + o, computed_shape[3] + o)
                # expect an error
                self.tcase_gi(image_shape, kernel_shape, output_shape,
                              (s, s), border_mode, True, True, (d, d), True)

        for (i, k) in ((1, 1), (1, 2), (2, 1), (4, 2), (4, 3), (7, 3), (9, 5)):
            for border_mode in ('valid', 'half', 'full', (0, 2)):
                for (s, d) in ((1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (1, 3)):
                    image_shape = (1, 1, i, i)
                    kernel_shape = (1, 1, k, k)

                    # compute the output that these inputs and parameters would produce
                    computed_shape = get_conv_output_shape(
                        image_shape, kernel_shape, border_mode, (s, s), (d, d))

                    yield (run_for_output_offsets,
                           image_shape, kernel_shape, s, border_mode, d)

    def run_fwd(self, inputs_shape, filters_shape,
                conv_fn=conv.conv2d, conv_op=conv.AbstractConv2d,
                ref=conv2d_corr, **kwargs):
        super(BaseTestConv2d, self).run_fwd(
            inputs_shape=inputs_shape,
            filters_shape=filters_shape,
            conv_fn=conv_fn,
            conv_op=conv_op,
            ref=ref, **kwargs)

    def run_gradweight(self, inputs_shape, filters_shape, output_shape,
                       gradWeights_fn=conv.AbstractConv2d_gradWeights,
                       ref=conv2d_corr_gw, **kwargs):
        super(BaseTestConv2d, self).run_gradweight(
            inputs_shape=inputs_shape,
            filters_shape=filters_shape,
            output_shape=output_shape,
            gradWeights_fn=gradWeights_fn,
            ref=ref, **kwargs)

    def run_gradinput(self, inputs_shape, filters_shape, output_shape,
                      gradInputs_fn=conv.AbstractConv2d_gradInputs,
                      ref=conv2d_corr_gi, **kwargs):
        super(BaseTestConv2d, self).run_gradinput(
            inputs_shape=inputs_shape,
            filters_shape=filters_shape,
            output_shape=output_shape,
            gradInputs_fn=gradInputs_fn,
            ref=ref, **kwargs)


class TestCorrConv2d(BaseTestConv2d):
    @classmethod
    def setup_class(cls):
        # This tests can run even when theano.config.blas.ldflags is empty.
        BaseTestConv2d.setup_class()

    def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1)):
        o = self.get_output_shape(i, f, s, b, fd)
        # This tests can run even when theano.config.blas.ldflags is empty.
<a name="8"></a>        if (not theano.config.cxx or
                theano.config.mode == "FAST_COMPILE"):
            raise SkipTest("Need blas to test conv2d")
        self.run_fwd<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f, subsample=s,
                     verify_grad=True, provide_shape=provide_shape,
                     border_mode=b, filter_flip=flip,
                     target_op=CorrMM, check_trace=True,
                     filter_dilation=fd)
        self.run_gradweight(inputs_shape=i, filters_shape=</b></font>f,
                            output_shape=o, subsample=s, verify_grad=True,
                            provide_shape=provide_shape, border_mode=b,
                            filter_flip=flip, target_op=CorrMM_gradWeights,
                            check_trace=True, filter_dilation=fd)
        self.run_gradinput(inputs_shape=i, filters_shape=f,
                           output_shape=o, subsample=s, verify_grad=True,
                           provide_shape=provide_shape, border_mode=b,
                           filter_flip=flip, target_op=CorrMM_gradInputs,
                           check_trace=True, filter_dilation=fd)

    def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1), expect_error=False):
        # This tests can run even when theano.config.blas.ldflags is empty.
        if (not theano.config.cxx or
                theano.config.mode == "FAST_COMPILE"):
            raise SkipTest("Need blas to test conv2d")
        if not expect_error:
            self.run_gradinput(inputs_shape=i, filters_shape=f,
                               output_shape=o, subsample=s, verify_grad=True,
                               provide_shape=provide_shape, border_mode=b,
                               filter_flip=flip, target_op=CorrMM_gradInputs,
                               check_trace=True, filter_dilation=fd)
        else:
            assert_raises(ValueError,
                          self.run_gradinput,
                          inputs_shape=i, filters_shape=f,
                          output_shape=o, subsample=s, verify_grad=False,
                          provide_shape=provide_shape, border_mode=b,
                          filter_flip=flip, target_op=CorrMM_gradInputs,
                          ref=None, check_trace=True, filter_dilation=fd)


class TestAbstractConvNoOptim(BaseTestConv2d):
    @classmethod
    def setup_class(cls):
        # This tests can run even when theano.config.blas.ldflags is empty.
        BaseTestConv2d.setup_class()
        cls.inputs_shapes = [(8, 1, 6, 6)]
        cls.filters_shapes = [(5, 1, 2, 2)]
        cls.subsamples = [(1, 1), (2, 2)]
        cls.filters_dilations = [(1, 1), (1, 2), (2, 1)]
        cls.border_modes = ["valid", "half", "full"]
        cls.filter_flip = [True]
        cls.provide_shape = [False]
        if not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("SciPy needed")

    def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1)):
        o = self.get_output_shape(i, f, s, b, fd)
        mode = theano.Mode(optimizer=None)

        if not theano.config.cxx:
            raise SkipTest("Need cxx to test conv2d")

        self.run_fwd(inputs_shape=i, filters_shape=f, subsample=s,
                     verify_grad=True, provide_shape=provide_shape,
<a name="5"></a>                     border_mode=b, filter_flip=flip,
                     target_op=None, check_trace=True,
                     filter_dilation=fd, mode=mode)
        self.run_gradweight<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f,
                            output_shape=o, subsample=s, verify_grad=True,
                            provide_shape=provide_shape, border_mode=b,
                            filter_flip=flip, target_op=None,
                            check_trace=True, filter_dilation=fd,
                            mode=mode)
        self.run_gradinput(inputs_shape=</b></font>i, filters_shape=f,
                           output_shape=o, subsample=s, verify_grad=True,
                           provide_shape=provide_shape, border_mode=b,
                           filter_flip=flip, target_op=None,
                           check_trace=True, filter_dilation=fd,
                           mode=mode)

    def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1), expect_error=False):

        if not theano.config.cxx:
            raise SkipTest("Need cxx to test conv2d")

        mode = theano.Mode(optimizer=None)
        if not expect_error:
            self.run_gradinput(inputs_shape=i, filters_shape=f,
                               output_shape=o, subsample=s, verify_grad=True,
                               provide_shape=provide_shape, border_mode=b,
                               filter_flip=flip, target_op=None,
                               check_trace=True, filter_dilation=fd,
                               mode=mode)
<a name="14"></a>        else:
            assert_raises(ValueError,
                          self.run_gradinput,
                          inputs_shape<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=i, filters_shape=f,
                          output_shape=o, subsample=s, verify_grad=False,
                          provide_shape=provide_shape, border_mode=b,
                          filter_flip=flip, target_op=None,
                          check_trace=True, filter_dilation=fd,
                          ref=None, mode=</b></font>mode)


class BaseTestConv3d(BaseTestConv):
    @classmethod
    def setup_class(cls):
        # This tests can run even when theano.config.blas.ldflags is empty.
        cls.inputs_shapes = [(2, 1, 5, 5, 5), (1, 2, 7, 5, 6),
                             (0, 1, 5, 5, 5), (1, 0, 5, 5, 5), (1, 1, 5, 5, 5)]
        cls.filters_shapes = [(2, 1, 2, 2, 2), (1, 2, 2, 1, 3),
                              (1, 1, 2, 2, 2), (1, 0, 2, 2, 2), (0, 1, 2, 2, 2)]
<a name="7"></a>        cls.subsamples = [(1, 1, 1), (2, 2, 2), (1, 2, 3)]
        cls.default_subsamples = (1, 1, 1)
        cls.filters_dilations = [(1, 1, 1), (1, 2, 1), (2, 1, 2)]
        cls<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.default_filters_dilations = (1, 1, 1)
        cls.border_modes = ["valid", "half", "full", (0, 0, 0), (2, 2, 3)]
        cls.default_border_mode = (0, 0, 0)
        cls.filter_flip = [True, False]
        cls.</b></font>default_filter_flip = True
        cls.provide_shape = [True, False]
        cls.default_provide_shape = True
        cls.shared = staticmethod(theano.compile.shared)

    def test_gradinput_arbitrary_output_shapes(self):
        # this computes the grad wrt inputs for an output shape
        # that the forward convolution would not produce
        input_shape = (2, 1, 7, 7, 7)
        filter_shape = (1, 1, 3, 3, 3)
        for output_shape in [(2, 1, 8, 8, 8), (2, 1, 9, 9, 9), (2, 1, 12, 12, 12)]:
            for border_mode in ["valid", "half", "full"]:
                # compute the output that these inputs and parameters would produce
                computed_shape = get_conv_output_shape(
                    input_shape, filter_shape, border_mode, self.default_subsamples, self.default_filters_dilations)
                # is this a valid combination?
                if tuple(computed_shape) == output_shape:
                    yield (self.tcase_gi,
                           input_shape,
                           filter_shape,
                           output_shape,
                           self.default_subsamples,
                           border_mode,
                           True,
                           True,
                           self.default_filters_dilations,
                           False)
                else:
                    # expect an error
                    yield (self.tcase_gi,
                           input_shape,
                           filter_shape,
                           output_shape,
                           self.default_subsamples,
                           border_mode,
                           True,
                           True,
                           self.default_filters_dilations,
                           True)

    def test_gradinput_impossible_output_shapes(self):
        def run_for_output_offsets(image_shape, kernel_shape, s, border_mode, d):
            # outputs that are too large or too small should be rejected
            for o in (-3, -1, 1, 2):
                output_shape = (1, 1, computed_shape[2] + o,
                                computed_shape[3] + o, computed_shape[4] + o)
                # expect an error
                self.tcase_gi(image_shape, kernel_shape, output_shape,
                              (s, s), border_mode, True, True, (d, d), True)

        for (i, k) in ((1, 1), (1, 2), (2, 1), (4, 2), (4, 3), (7, 3), (9, 5)):
            for border_mode in ('valid', 'half', 'full', (0, 2, 1)):
                for (s, d) in ((1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (1, 3)):
                    image_shape = (1, 1, i, i, i)
                    kernel_shape = (1, 1, k, k, k)

                    # compute the output that these inputs and parameters would produce
                    computed_shape = get_conv_output_shape(
                        image_shape, kernel_shape, border_mode, (s, s, s), (d, d, d))

                    yield (run_for_output_offsets,
                           image_shape, kernel_shape, s, border_mode, d)

    def run_fwd(self, inputs_shape, filters_shape,
                conv_fn=conv.conv3d, conv_op=conv.AbstractConv3d,
                ref=conv3d_corr, **kwargs):
        super(BaseTestConv3d, self).run_fwd(
            inputs_shape=inputs_shape,
            filters_shape=filters_shape,
            conv_fn=conv_fn,
            conv_op=conv_op,
            ref=ref, **kwargs)

    def run_gradweight(self, inputs_shape, filters_shape, output_shape,
                       gradWeights_fn=conv.AbstractConv3d_gradWeights,
                       ref=conv3d_corr_gw, **kwargs):
        super(BaseTestConv3d, self).run_gradweight(
            inputs_shape=inputs_shape,
            filters_shape=filters_shape,
            output_shape=output_shape,
            gradWeights_fn=gradWeights_fn,
            ref=ref, **kwargs)

    def run_gradinput(self, inputs_shape, filters_shape, output_shape,
                      gradInputs_fn=conv.AbstractConv3d_gradInputs,
                      ref=conv3d_corr_gi, **kwargs):
        super(BaseTestConv3d, self).run_gradinput(
            inputs_shape=inputs_shape,
            filters_shape=filters_shape,
            output_shape=output_shape,
            gradInputs_fn=gradInputs_fn,
            ref=ref, **kwargs)


class TestCorrConv3d(BaseTestConv3d):
    @classmethod
    def setup_class(cls):
        # This tests can run even when theano.config.blas.ldflags is empty.
        BaseTestConv3d.setup_class()

    def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1, 1)):
        o = self.get_output_shape(i, f, s, b, fd)
        # This test can run even when theano.config.blas.ldflags is empty.
<a name="10"></a>        if (not theano.config.cxx or
                theano.config.mode == "FAST_COMPILE"):
            raise SkipTest("Need blas to test conv3d")
        self.run_fwd<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f, subsample=s,
                     verify_grad=True, provide_shape=provide_shape,
                     border_mode=b, filter_flip=flip,
                     target_op=Corr3dMM, check_trace=True,
                     filter_dilation=fd)
        self.run_gradweight(inputs_shape=</b></font>i, filters_shape=f,
                            output_shape=o, subsample=s, verify_grad=True,
                            provide_shape=provide_shape, border_mode=b,
                            filter_flip=flip, target_op=Corr3dMM_gradWeights,
                            check_trace=True, filter_dilation=fd)
        self.run_gradinput(inputs_shape=i, filters_shape=f,
                           output_shape=o, subsample=s, verify_grad=True,
                           provide_shape=provide_shape, border_mode=b,
                           filter_flip=flip, target_op=Corr3dMM_gradInputs,
                           check_trace=True, filter_dilation=fd)

    def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1, 1), expect_error=False):
        # This test can run even when theano.config.blas.ldflags is empty.
        if (not theano.config.cxx or
                theano.config.mode == "FAST_COMPILE"):
            raise SkipTest("Need blas to test conv3d")
        if not expect_error:
            self.run_gradinput(inputs_shape=i, filters_shape=f,
                               output_shape=o, subsample=s, verify_grad=True,
                               provide_shape=provide_shape, border_mode=b,
                               filter_flip=flip, target_op=Corr3dMM_gradInputs,
                               check_trace=True, filter_dilation=fd)
        else:
            assert_raises(ValueError,
                          self.run_gradinput,
                          inputs_shape=i, filters_shape=f,
                          output_shape=o, subsample=s, verify_grad=False,
                          provide_shape=provide_shape, border_mode=b,
                          filter_flip=flip, target_op=Corr3dMM_gradInputs,
                          ref=None, check_trace=True, filter_dilation=fd)


def test_constant_shapes():
    # Check that the `imshp` and `kshp` parameters of the AbstractConv Ops
    # are rejected if not constant or None
    dummy_t4 = tensor.ftensor4()
    alloc_dummy_t4 = tensor.zeros((3, 5, 7, 11), dtype='float32')

    dummy_shape = tensor.lvector()
    dummy_one_shape = tensor.ones(4, dtype='int64')
    constant_vec_shape = tensor.constant([3, 5, 7, 11])

    tuple_shape = (3, 5, 7, 11)
    list_shape = list(tuple_shape)
    constant_list_shape = [tensor.constant(i, dtype='int64')
                           for i in tuple_shape]
    constant_tuple_shape = tuple(constant_list_shape)

    bad_shapes = (
        dummy_shape,
        dummy_one_shape,
        dummy_t4.shape,
        alloc_dummy_t4.shape,
        constant_vec_shape,
    )

    good_shapes = (
        constant_list_shape,
        constant_tuple_shape,
        tuple_shape,
        list_shape
    )

    ops_to_test = (
        AbstractConv2d,
        AbstractConv2d_gradInputs,
        AbstractConv2d_gradWeights
    )

    for op in ops_to_test:
        for shp in bad_shapes:
            assert_raises(ValueError, op, imshp=shp)
            assert_raises(ValueError, op, kshp=shp)

        for shp in good_shapes:
            op(imshp=shp)
            op(kshp=shp)

<a name="4"></a>
class TestConvTypes(unittest.TestCase):
    def setUp(self):
        self<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.input = tensor.ftensor4()
        self.filters = tensor.ftensor4()
        self.topgrad = tensor.ftensor4()

        self.constant_tensor = np.zeros((3, 5, 7, 11), dtype=</b></font>'float32')

    def test_grad_types(self):
        # This function simply tests the behaviour of the AbstractConv
        # Ops, not their optimizations
        input = self.input
        filters = self.filters
        topgrad = self.topgrad

        out_shape = tensor.lvector()

        output = conv.conv2d(input, filters)
        grad_input, grad_filters = theano.grad(output.sum(),
                                               wrt=(input, filters))
        assert grad_input.type == input.type, (
            grad_input, grad_input.type, input, input.type)
        assert grad_filters.type == filters.type, (
            grad_filters, grad_filters.type, filters, filters.type)

        grad_filters = conv.AbstractConv2d_gradWeights()(
            input, topgrad, out_shape)
        grad_input, grad_topgrad = theano.grad(grad_filters.sum(),
                                               wrt=(input, topgrad))

        assert grad_input.type == input.type, (
            grad_input, grad_input.type, input, input.type)
        assert grad_topgrad.type == topgrad.type, (
            grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)

        grad_input = conv.AbstractConv2d_gradInputs()(
            filters, topgrad, out_shape)
        grad_filters, grad_topgrad = theano.grad(grad_input.sum(),
                                                 wrt=(filters, topgrad))

        assert grad_filters.type == filters.type, (
            grad_filters, grad_filters.type, filters, filters.type)
        assert grad_topgrad.type == topgrad.type, (
            grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)

    def test_constant_input(self):
        # Check the AbstractConv Ops for constant inputs
        input = self.input
        filters = self.filters
        topgrad = self.topgrad
        constant_tensor = self.constant_tensor
        out_shape = tensor.lvector()

        # Check the forward Op
        output = conv.conv2d(constant_tensor, filters)
        grad_filters = theano.grad(output.sum(), wrt=filters)
        assert grad_filters.type == filters.type, (
            grad_filters, grad_filters.type, filters, filters.type)

        output = conv.conv2d(input, constant_tensor)
        grad_input = theano.grad(output.sum(), wrt=input)
        assert grad_input.type == input.type, (
            grad_input, grad_input.type, input, input.type)

        # Check grad wrt weights
        grad_filters = conv.AbstractConv2d_gradWeights()(
            constant_tensor, topgrad, out_shape)
        grad_topgrad = theano.grad(grad_filters.sum(), wrt=topgrad)
        assert grad_topgrad.type == topgrad.type, (
            grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)

        grad_filters = conv.AbstractConv2d_gradWeights()(
            input, constant_tensor, out_shape)
        grad_input = theano.grad(grad_filters.sum(), wrt=input)
        assert grad_input.type == input.type, (
            grad_input, grad_input.type, input, input.type)

        # Check grad wrt inputs
        grad_input = conv.AbstractConv2d_gradInputs()(
            constant_tensor, topgrad, out_shape)
        grad_topgrad = theano.grad(grad_input.sum(), wrt=topgrad)
        assert grad_topgrad.type == topgrad.type, (
            grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)

        grad_input = conv.AbstractConv2d_gradInputs()(
            filters, constant_tensor, out_shape)
        grad_filters = theano.grad(grad_input.sum(), wrt=filters)
        assert grad_filters.type == filters.type, (
            grad_filters, grad_filters.type, filters, filters.type)


class TestBilinearUpsampling(unittest.TestCase):
    # If theano.config.blas.ldflags is empty, Theano will use
    # a NumPy C implementation of [sd]gemm_.
    compile_mode = theano.compile.mode.get_default_mode()
    if theano.config.mode == "FAST_COMPILE":
        compile_mode = compile_mode.excluding("conv_gemm")
        compile_mode = compile_mode.excluding('AbstractConvCheck')
    elif not theano.config.cxx:
        compile_mode = compile_mode.excluding('AbstractConvCheck')

    def numerical_kernel_1D(self, ratio):
        """
        Gets numerical 1D kernel for bilinear upsampling
        """
        return np.array(list(range(1, ratio + 1)) +
                        list(range(ratio - 1, 0, -1)))

    def numerical_kernel_2D(self, ratio):
        """
        Gets numerical 2D kernel for bilinear upsampling
        """
        return np.array([i * j for i in self.numerical_kernel_1D(ratio) for j
                         in self.numerical_kernel_1D(ratio)]).\
            reshape(2 * ratio - 1, 2 * ratio - 1)

    def test_bilinear_kernel_2D(self):
        # Test 2D kernels used in bilinear upsampling
        #
        # This method tests the correctness of the
        # 2D kernel values used in bilinear upsampling
        # for some upsampling ratios.

        for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
            # getting the un-normalized kernel
            kernel = bilinear_kernel_2D(ratio=ratio, normalize=False)
            f = theano.function([], kernel)
            kernel_2D = self.numerical_kernel_2D(ratio)
            utt.assert_allclose(kernel_2D, f())

            # getting the normalized kernel
            kernel = bilinear_kernel_2D(ratio=ratio, normalize=True)
            f = theano.function([], kernel)
            kernel_2D = kernel_2D / float(ratio**2)
            utt.assert_allclose(kernel_2D, f())

    def test_bilinear_kernel_1D(self):
        # Test 1D kernels used in bilinear upsampling
        #
        # This method tests the correctness of the
        # 1D kernel values used in bilinear upsampling
        # for some upsampling ratios.

        rat = tensor.iscalar()
        kernel_ten = bilinear_kernel_1D(ratio=rat, normalize=False)
        f_ten = theano.function([rat], kernel_ten)

        kernel_ten_norm = bilinear_kernel_1D(ratio=rat, normalize=True)
        f_ten_norm = theano.function([rat], kernel_ten_norm)

        for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
            # getting the un-normalized kernel
            kernel = bilinear_kernel_1D(ratio=ratio, normalize=False)
            f = theano.function([], kernel)
            kernel_1D = self.numerical_kernel_1D(ratio)
            utt.assert_allclose(kernel_1D, f())
            utt.assert_allclose(kernel_1D, f_ten(ratio))

            # getting the normalized kernel
            kernel = bilinear_kernel_1D(ratio=ratio, normalize=True)
            f = theano.function([], kernel)
            kernel_1D = kernel_1D / float(ratio)
            utt.assert_allclose(kernel_1D, f())
            utt.assert_allclose(kernel_1D, f_ten_norm(ratio))

    def numerical_upsampling_multiplier(self, ratio):
        """
        Compute upsampling multiplier

        This method computes the multipliers of an array
        that will be upsampled using bilinear interpolation.

        Parameters
        ----------
        ratio: int
            the ratio by which the array will be upsampled.

        Returns
        -------
        1D numpy array
            The multipliers that can be used in bilinear interpolation
            to upsample an array.

        int
            The size of the multipliers array
        """
        kern = np.arange(ratio + 1)
        return kern, kern.shape[0]

    def get_upsampled_twobytwo_mat(self, two_by_two, ratio):
        """
        Upsample 4D array with two rows and two columns

        This method gets a 4D numpy array with two rows and two columns
        and computes its upsampled array by using bilinear interpolation

        Parameters
        ----------
        two_by_two: numpy 4D array
            The array that will be upsampled by bilinear interpolation.
            Array is of shape (batch size, num channels, 2, 2)

        ratio: int
            The ratio by which two_by_two's last
            two dimensions (row and col) will be upsampled.

        Returns
        -------
        4D numpy array
            The array upsampled by using bilinear interpolation. Array
            is of shape (batch size, num channels, 2*ratio, 2*ratio).
        """
        kern, shp = self.numerical_upsampling_multiplier(ratio)
        up_1D = two_by_two[:, :, :, :1] * kern[::-1] + \
            two_by_two[:, :, :, 1:] * kern
        up_2D = up_1D[:, :, :1, :] * kern[::-1][:, np.newaxis] + \
            up_1D[:, :, 1:, :] * kern[:, np.newaxis]
        num_concat = (ratio - 1) // 2
        for i in range(num_concat):
            up_2D = np.concatenate([up_2D[:, :, :1, :], up_2D], axis=2)
            up_2D = np.concatenate([up_2D, up_2D[:, :, -1:, :]], axis=2)
            up_2D = np.concatenate([up_2D[:, :, :, :1], up_2D], axis=3)
            up_2D = np.concatenate([up_2D, up_2D[:, :, :, -1:]], axis=3)
        if ratio % 2 == 0:
            up_2D = np.concatenate([up_2D, up_2D[:, :, -1:, :]], axis=2)
            up_2D = np.concatenate([up_2D, up_2D[:, :, :, -1:]], axis=3)
        return up_2D / float(ratio)**2

    def test_bilinear_upsampling_1D(self):
        # Test bilinear upsampling using 1D kernels
        #
        # This method tests the bilinear_upsampling method
        # when using 1D kernels for some upsampling ratios.

        # upsampling for a ratio of two
        input_x = np.array([[[[1, 2], [3, 4]]]], dtype=theano.config.floatX)

        for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
            bilin_mat = bilinear_upsampling(input=input_x, ratio=ratio,
                                            batch_size=1, num_input_channels=1,
                                            use_1D_kernel=True)
            f = theano.function([], bilin_mat, mode=self.compile_mode)
            up_mat_2d = self.get_upsampled_twobytwo_mat(input_x, ratio)
            utt.assert_allclose(f(), up_mat_2d, rtol=1e-06)

    def test_bilinear_upsampling_reshaping(self):
        # Test bilinear upsampling without giving shape information
        #
        # This method tests the bilinear_upsampling method
        # without giving batch_size and num_input_channels

        # upsampling for a ratio of two
        input_x = np.array([[[[1, 2], [3, 4]]]], dtype=theano.config.floatX)

        for ratio in [2, 3]:
            for use_1D_kernel in [True, False]:
                bilin_mat = bilinear_upsampling(input=input_x, ratio=ratio,
                                                batch_size=None,
                                                num_input_channels=None,
                                                use_1D_kernel=use_1D_kernel)
                f = theano.function([], bilin_mat, mode=self.compile_mode)
                up_mat_2d = self.get_upsampled_twobytwo_mat(input_x, ratio)
                utt.assert_allclose(f(), up_mat_2d, rtol=1e-06)

    def test_compare_1D_and_2D_upsampling_values(self):
        # Compare 1D and 2D upsampling
        #
        # This method verifies the bilinear upsampling done by using
        # 1D and 2D kernels will generate the same result.

        # checking upsampling with ratio 5
        input_x = np.random.rand(5, 4, 6, 7).astype(theano.config.floatX)
        mat_1D = bilinear_upsampling(input=input_x, ratio=5,
                                     batch_size=5, num_input_channels=4,
                                     use_1D_kernel=True)
        mat_2D = bilinear_upsampling(input=input_x, ratio=5,
                                     batch_size=5, num_input_channels=4,
                                     use_1D_kernel=False)
        f_1D = theano.function([], mat_1D, mode=self.compile_mode)
        f_2D = theano.function([], mat_2D, mode=self.compile_mode)
        utt.assert_allclose(f_1D(), f_2D(), rtol=1e-06)

        # checking upsampling with ratio 8
        input_x = np.random.rand(12, 11, 10, 7).astype(theano.config.floatX)
        mat_1D = bilinear_upsampling(input=input_x, ratio=8,
                                     batch_size=12, num_input_channels=11,
                                     use_1D_kernel=True)
        mat_2D = bilinear_upsampling(input=input_x, ratio=8,
                                     batch_size=12, num_input_channels=11,
                                     use_1D_kernel=False)
        f_1D = theano.function([], mat_1D, mode=self.compile_mode)
        f_2D = theano.function([], mat_2D, mode=self.compile_mode)
        utt.assert_allclose(f_1D(), f_2D(), rtol=1e-06)

    def test_fractional_bilinear_upsampling(self):
        """Test bilinear upsampling with nonsimilar fractional
        row and col ratios
        """
        input_x = np.array([[[1, 2], [3, 4]],
                            [[5, 6], [7, 8]],
                            [[9, 10], [11, 12]]],
                           ndmin=4).astype(theano.config.floatX)
        up_x = bilinear_upsampling(input=input_x,
<a name="3"></a>                                   frac_ratio=((7, 4), (5, 3)),
                                   use_1D_kernel=False)
        num_up_x = np.array(
            [<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[1., 1.2, 1.8, 2.],
              [1.28571429, 1.48571429, 2.08571429, 2.28571429],
              [2.42857143, 2.62857143, 3.22857143, 3.42857143],
              [3., 3.2, 3.8, 4.]],
             [[5., 5.2, 5.8, 6.],
              [5.28571429, 5.48571429, 6.08571429, 6.28571429],
              [6.42857143, 6.62857143, 7.22857143, 7.42857143],
              [7., 7.2, 7.8, 8.]],
             [[9., 9.2, 9.8, 10.],
              [9.28571429, 9.48571429, 10.08571429, 10.28571429],
              [10.42857143, 10.62857143, 11.22857143, 11.42857143],
              [11., 11.2, 11.8, 12.]]]]
            ).</b></font>astype(theano.config.floatX)
        f_up_x = theano.function([], up_x, mode=self.compile_mode)
        utt.assert_allclose(f_up_x(), num_up_x, rtol=1e-6)

    def test_fractional_bilinear_upsampling_shape(self):
        x = np.random.rand(1, 1, 200, 200).astype(theano.config.floatX)
        resize = (24, 20)
        z = bilinear_upsampling(tensor.as_tensor_variable(x), frac_ratio=resize, use_1D_kernel=False)
        out = theano.function([], z.shape, mode='FAST_RUN')()
        utt.assert_allclose(out, (1, 1, 240, 240))


class TestConv2dTranspose(unittest.TestCase):
    mode = None

    def test_interface(self):
        # Test conv2d_transpose wrapper.
        #
        # This method tests that the order of the filter's
        # axes expected by the function produces the correct
        # output shape.
        if theano.config.cxx == "":
            raise SkipTest("test needs cxx")

        mode = self.mode
        if theano.config.mode == "FAST_COMPILE":
            mode = theano.compile.get_mode(
                mode).excluding("conv_gemm").excluding("AbstractConvCheck")

        output = theano.function(
            inputs=[],
            outputs=conv2d_transpose(input=tensor.ones((2, 2, 4, 4)),
                                     filters=tensor.ones((2, 1, 4, 4)),
                                     output_shape=(2, 1, 10, 10),
<a name="6"></a>                                     input_dilation=(2, 2)),
            mode=mode)()
        expected_output = np.array(
            [<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
               [2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
               [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
               [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
               [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
               [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
               [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
               [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
               [2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
               [2, 2, 4, 4, 4, 4, 4, 4, 2, 2]]]] * 2)
        np.testing.</b></font>assert_equal(output, expected_output)


class TestConv2dGrads(unittest.TestCase):

    def setUp(self):

        if (not theano.config.cxx or
                theano.config.mode == "FAST_COMPILE"):
            raise SkipTest("Need blas to test conv2d")

        self.random_stream = np.random.RandomState(utt.fetch_seed())

        self.inputs_shapes = [(8, 1, 12, 12), (1, 1, 5, 5), (1, 1, 5, 6), (1, 1, 6, 6)]
        self.filters_shapes = [(5, 1, 2, 2), (1, 1, 3, 3)]

        self.subsamples = [(1, 1), (2, 2)]
        self.border_modes = ["valid", "full"]
        self.filter_flip = [True, False]

        self.output_grad = theano.tensor.tensor4()
        self.output_grad_wrt = theano.tensor.tensor4()

        self.x = theano.tensor.tensor4('x', theano.config.floatX)  # inputs
        self.w = theano.tensor.tensor4('w', theano.config.floatX)  # filter weights

    def test_conv2d_grad_wrt_inputs(self):
        # Compares calculated abstract grads wrt inputs with the fwd grads
        # This method checks the outputs of conv2_grad_wrt_inputs against
        # the outputs of T.nnet.conv forward grads to make sure the
        # results are the same.

        for (in_shape, fltr_shape) in zip(self.inputs_shapes, self.filters_shapes):
            for bm in self.border_modes:
                for ss in self.subsamples:
                    for ff in self.filter_flip:
                        input_val = self.random_stream.random_sample(in_shape).astype(theano.config.floatX)
                        filter_val = self.random_stream.random_sample(fltr_shape).astype(theano.config.floatX)
                        out_grad_shape = theano.tensor.nnet.abstract_conv.get_conv_output_shape(image_shape=in_shape,
                                                                                                kernel_shape=fltr_shape,
                                                                                                border_mode=bm,
                                                                                                subsample=ss)
                        out_grad_val = self.random_stream.random_sample(out_grad_shape).astype(theano.config.floatX)
                        conv_out = theano.tensor.nnet.conv2d(self.x,
                                                             filters=self.w,
                                                             border_mode=bm,
                                                             subsample=ss,
                                                             input_shape=in_shape,
                                                             filter_shape=fltr_shape,
                                                             filter_flip=ff
                                                             )
                        conv_grad = theano.grad(conv_out.sum(), wrt=self.x, known_grads={conv_out: self.output_grad})
                        f_old = theano.function([self.x, self.w, self.output_grad], conv_grad)

                        conv_wrt_i_out = theano.tensor.nnet.abstract_conv.conv2d_grad_wrt_inputs(output_grad=self.output_grad_wrt,
                                                                                                 filters=self.w,
                                                                                                 border_mode=bm,
                                                                                                 subsample=ss,
                                                                                                 input_shape=in_shape,
                                                                                                 filter_shape=fltr_shape,
                                                                                                 filter_flip=ff
                                                                                                 )
                        f_new = theano.function([self.w, self.output_grad_wrt], conv_wrt_i_out)

                        # check that they're equal
                        utt.assert_allclose(f_new(filter_val, out_grad_val), f_old(input_val, filter_val, out_grad_val))

    def test_conv2d_grad_wrt_weights(self):
        # Compares calculated abstract grads wrt weights with the fwd grads
        # This method checks the outputs of conv2_grad_wrt_weights against
        # the outputs of T.nnet.conv forward grads to make sure the
        # results are the same.

        for (in_shape, fltr_shape) in zip(self.inputs_shapes, self.filters_shapes):
            for bm in self.border_modes:
                for ss in self.subsamples:
                    for ff in self.filter_flip:
                        input_val = self.random_stream.random_sample(in_shape).astype(theano.config.floatX)
                        filter_val = self.random_stream.random_sample(fltr_shape).astype(theano.config.floatX)
                        out_grad_shape = theano.tensor.nnet.abstract_conv.get_conv_output_shape(image_shape=in_shape,
                                                                                                kernel_shape=fltr_shape,
                                                                                                border_mode=bm,
                                                                                                subsample=ss)
                        out_grad_val = self.random_stream.random_sample(out_grad_shape).astype(theano.config.floatX)
                        conv_out = theano.tensor.nnet.conv2d(self.x,
                                                             filters=self.w,
                                                             border_mode=bm,
                                                             subsample=ss,
                                                             input_shape=in_shape,
                                                             filter_shape=fltr_shape,
                                                             filter_flip=ff
                                                             )
                        conv_grad = theano.grad(conv_out.sum(), wrt=self.w, known_grads={conv_out: self.output_grad})
                        f_old = theano.function([self.x, self.w, self.output_grad], conv_grad)

                        conv_wrt_w_out = theano.tensor.nnet.abstract_conv.conv2d_grad_wrt_weights(self.x,
                                                                                                  output_grad=self.output_grad_wrt,
                                                                                                  border_mode=bm,
                                                                                                  subsample=ss,
                                                                                                  input_shape=in_shape,
                                                                                                  filter_shape=fltr_shape,
                                                                                                  filter_flip=ff
                                                                                                  )
                        f_new = theano.function([self.x, self.output_grad_wrt], conv_wrt_w_out)
                        utt.assert_allclose(f_new(input_val, out_grad_val), f_old(input_val, filter_val, out_grad_val))


class Grouped_conv_noOptim(unittest.TestCase):
    conv = theano.tensor.nnet.abstract_conv.AbstractConv2d
    conv_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
    conv_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
    conv_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
    conv_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
    conv_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
    mode = theano.Mode(optimizer=None)
    is_dnn = False

    def setUp(self):
        self.num_groups = [3, 2, 4, 4]
        self.border_mode = 'valid'
        self.subsample = (1, 1)
        self.img_shape = [(5, 6, 5, 5), (4, 4, 7, 5), (3, 8, 5, 3), (2, 4, 7, 7)]
        self.kern_shape = [(6, 2, 3, 3), (6, 2, 5, 3), (4, 2, 3, 3), (4, 1, 3, 5)]
        self.top_shape = [(5, 6, 3, 3), (4, 6, 3, 3), (3, 4, 3, 1), (2, 4, 5, 3)]
        self.filter_dilation = (1, 1)
        self.ref_mode = 'FAST_RUN'
        self.convdim = 2
        self.corr_fwd = conv2d_corr
        self.corr_gradw = conv2d_corr_gw
        self.corr_gradi = conv2d_corr_gi
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("CorrMM needs cxx and SciPy")
<a name="13"></a>
    def test_fwd(self):
        if self.convdim == 2:
            img_sym <font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.tensor4('img')
            kern_sym = theano.tensor.tensor4('kern')
        else:
            img_sym = theano.tensor.tensor5('img')
            kern_sym =</b></font> theano.tensor.tensor5('kern')

        for imshp, kshp, groups in zip(self.img_shape, self.kern_shape, self.num_groups):
            img = np.random.random(imshp).astype(theano.config.floatX)
            kern = np.random.random(kshp).astype(theano.config.floatX)
            split_imgs = np.split(img, groups, axis=1)
            split_kern = np.split(kern, groups, axis=0)

            grouped_conv_op = self.conv(border_mode=self.border_mode,
                                        subsample=self.subsample,
                                        filter_dilation=self.filter_dilation,
                                        num_groups=groups)
            grouped_conv_output = grouped_conv_op(img_sym, kern_sym)

            grouped_func = theano.function([img_sym, kern_sym], grouped_conv_output, mode=self.mode)
            assert any([isinstance(node.op, self.conv_op)
                       for node in grouped_func.maker.fgraph.toposort()])
            grouped_output = grouped_func(img, kern)

            ref_conv_op = self.corr_fwd(img_sym,
                                        kern_sym,
                                        border_mode=self.border_mode,
                                        subsample=self.subsample,
                                        filter_dilation=self.filter_dilation)
            ref_func = theano.function([img_sym, kern_sym], ref_conv_op,
                                       mode=self.ref_mode)
            ref_concat_output = [ref_func(img_arr, kern_arr)
                                 for img_arr, kern_arr in zip(split_imgs, split_kern)]
            ref_concat_output = np.concatenate(ref_concat_output, axis=1)

            utt.assert_allclose(grouped_output, ref_concat_output)

            utt.verify_grad(grouped_conv_op,
                            [img, kern],
                            mode=self.mode,
                            eps=1)

    def test_gradweights(self):
        if self.convdim == 2:
            img_sym = theano.tensor.tensor4('img')
            top_sym = theano.tensor.tensor4('kern')
        else:
            img_sym = theano.tensor.tensor5('img')
            top_sym = theano.tensor.tensor5('kern')
        for imshp, kshp, tshp, groups in zip(self.img_shape, self.kern_shape, self.top_shape, self.num_groups):
            img = np.random.random(imshp).astype(theano.config.floatX)
            top = np.random.random(tshp).astype(theano.config.floatX)
            split_imgs = np.split(img, groups, axis=1)
            split_top = np.split(top, groups, axis=1)

            grouped_convgrad_op = self.conv_gradw(border_mode=self.border_mode,
                                                  subsample=self.subsample,
                                                  filter_dilation=self.filter_dilation,
                                                  num_groups=groups)
            grouped_conv_output = grouped_convgrad_op(img_sym,
                                                      top_sym,
                                                      tensor.as_tensor_variable(
                                                          kshp[-self.convdim:]))
            grouped_func = theano.function([img_sym, top_sym], grouped_conv_output, mode=self.mode)
            assert any([isinstance(node.op, self.conv_gradw_op)
                       for node in grouped_func.maker.fgraph.toposort()])
            grouped_output = grouped_func(img, top)

            ref_conv_op = self.corr_gradw(img_sym,
                                          top_sym,
                                          kshp,
                                          border_mode=self.border_mode,
                                          subsample=self.subsample,
                                          filter_dilation=self.filter_dilation)
            ref_func = theano.function([img_sym, top_sym], ref_conv_op,
                                       mode=self.ref_mode)
            ref_concat_output = [ref_func(img_arr, top_arr)
                                 for img_arr, top_arr in zip(split_imgs, split_top)]
            ref_concat_output = np.concatenate(ref_concat_output, axis=0)

            utt.assert_allclose(grouped_output, ref_concat_output)

            def conv_gradweight(inputs_val, output_val):
                return grouped_convgrad_op(inputs_val, output_val,
                                           tensor.as_tensor_variable(
                                               kshp[-self.convdim:]))

            utt.verify_grad(conv_gradweight,
                            [img, top],
                            mode=self.mode, eps=1)

    def test_gradinputs(self):
        if self.convdim == 2:
            kern_sym = theano.tensor.tensor4('kern')
            top_sym = theano.tensor.tensor4('top')
        else:
            kern_sym = theano.tensor.tensor5('kern')
            top_sym = theano.tensor.tensor5('top')
        for imshp, kshp, tshp, groups in zip(self.img_shape, self.kern_shape, self.top_shape, self.num_groups):
            kern = np.random.random(kshp).astype(theano.config.floatX)
            top = np.random.random(tshp).astype(theano.config.floatX)
            split_kerns = np.split(kern, groups, axis=0)
            split_top = np.split(top, groups, axis=1)

            grouped_convgrad_op = self.conv_gradi(border_mode=self.border_mode,
                                                  subsample=self.subsample,
                                                  filter_dilation=self.filter_dilation,
                                                  num_groups=groups)
            grouped_conv_output = grouped_convgrad_op(kern_sym,
                                                      top_sym,
                                                      tensor.as_tensor_variable(
                                                          imshp[-self.convdim:]))
            grouped_func = theano.function([kern_sym, top_sym], grouped_conv_output, mode=self.mode)
            assert any([isinstance(node.op, self.conv_gradi_op)
                       for node in grouped_func.maker.fgraph.toposort()])
            grouped_output = grouped_func(kern, top)

            ref_conv_op = self.corr_gradi(kern_sym,
                                          top_sym,
                                          imshp,
                                          border_mode=self.border_mode,
                                          subsample=self.subsample,
                                          filter_dilation=self.filter_dilation)
            ref_func = theano.function([kern_sym, top_sym], ref_conv_op,
                                       mode=self.ref_mode)
            ref_concat_output = [ref_func(kern_arr, top_arr)
                                 for kern_arr, top_arr in zip(split_kerns, split_top)]
            ref_concat_output = np.concatenate(ref_concat_output, axis=1)

            utt.assert_allclose(grouped_output, ref_concat_output)

            def conv_gradinputs(filters_val, output_val):
                return grouped_convgrad_op(filters_val, output_val,
                                           tensor.as_tensor_variable(
                                               imshp[-self.convdim:]))

            utt.verify_grad(conv_gradinputs,
                            [kern, top],
                            mode=self.mode, eps=1)


class Grouped_conv3d_noOptim(Grouped_conv_noOptim):
    conv = theano.tensor.nnet.abstract_conv.AbstractConv3d
    conv_gradw = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradWeights
    conv_gradi = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradInputs
    conv_op = theano.tensor.nnet.abstract_conv.AbstractConv3d
    conv_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradWeights
    conv_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradInputs
    mode = theano.Mode(optimizer=None)

    def setUp(self):
        self.num_groups = [3, 2, 4, 4]
        self.border_mode = 'valid'
        self.subsample = (1, 1, 1)
        self.img_shape = [(2, 6, 5, 5, 5), (1, 4, 7, 5, 7), (1, 8, 5, 3, 5), (2, 4, 7, 7, 7)]
        self.kern_shape = [(3, 2, 3, 3, 3), (6, 2, 5, 3, 5), (4, 2, 3, 3, 3), (4, 1, 3, 5, 3)]
        self.top_shape = [(2, 3, 3, 3, 3), (1, 6, 3, 3, 3), (1, 4, 3, 1, 3), (2, 4, 5, 3, 5)]
        self.filter_dilation = (1, 1, 1)
        self.ref_mode = 'FAST_RUN'
        self.convdim = 3
        self.corr_fwd = conv3d_corr
        self.corr_gradw = conv3d_corr_gw
        self.corr_gradi = conv3d_corr_gi
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("CorrMM needs cxx")

<a name="12"></a>
class Separable_conv(unittest.TestCase):
    def setUp(self):
<a name="1"></a>        self.x = np.array([<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[1, 2, 3, 4, 5], [3, 2, 1, 4, 5], [3, 3, 1, 3, 6], [5, 3, 2, 1, 1], [4, 7, 1, 2, 1]],
                            [[3, 3, 1, 2, 6], [6, 5, 4, 3, 1], [3, 4, 5, 2, 3], [6</b></font>, 4, 1, 3, 4], [2, 3, 4, 2, 5]]]]).astype(theano.config.floatX)

        self.depthwise_filter = np.array([<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[3, 2, 1], [5, 3, 2], [6, 4, 2]]], [[[5, 5, 2], [3, 7, 4], [3, 5, 4]]],
                                          [[[7, 4, 7], [5, 3, 3], [1, 3, 1]]], [[[4, 4, 4], [2, 4, 6], [0, 0, 7]]]]).</b></font>astype(theano.config.floatX)

        self.pointwise_filter = np.array([[[[4]], [[1]], [[3]], [[5]]], [[[2]], [[1]], [[2]], [[8]]]]).astype(theano.config.floatX)

        self.precomp_output_valid = np.array([[[[1385, 1333, 1339], [1382, 1243, 1291], [1303, 1120, 1228]],
                                               [[1532, 1410, 1259], [1522, 1346, 1314], [1379, 1192, 1286]]]]).astype(theano.config.floatX)

        self.precomp_output_full = np.array([[[[140, 266, 343, 206, 59],
                                              [395, 697, 979, 585, 245],
                                              [429, 863, 1385, 919, 453],
                                              [243, 499, 864, 627, 371],
                                              [90, 183, 291, 254, 202]],

                                             [[149, 289, 359, 213, 58],
                                              [400, 750, 1076, 662, 266],
                                              [387, 854, 1532, 1091, 540],
                                              [174, 411, 971, 786, 518],
                                              [51, 110, 286, 299, 298]]]]).astype(theano.config.floatX)

    def test_interface2d(self):
        if theano.config.cxx == "":
            raise SkipTest("test needs cxx")
        x_sym = theano.tensor.tensor4('x')
        dfilter_sym = theano.tensor.tensor4('d')
        pfilter_sym = theano.tensor.tensor4('p')

        sep_op = separable_conv2d(x_sym, dfilter_sym, pfilter_sym, self.x.shape[1])
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')

        # test for square matrix
        top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
        utt.assert_allclose(top, self.precomp_output_valid)

        # test for non-square matrix
        top = fun(self.x[:, :, :3, :], self.depthwise_filter, self.pointwise_filter)
        utt.assert_allclose(top, self.precomp_output_valid[:, :, :1, :])

        # test if it infers shape
        sep_op = separable_conv2d(x_sym,
                                  dfilter_sym,
                                  pfilter_sym,
                                  self.x.shape[1],
                                  input_shape=self.x.shape,
                                  depthwise_filter_shape=self.depthwise_filter.shape,
                                  pointwise_filter_shape=self.pointwise_filter.shape)
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
        top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
        utt.assert_allclose(top, self.precomp_output_valid)

        # test non-default subsample
        sep_op = separable_conv2d(x_sym,
                                  dfilter_sym,
                                  pfilter_sym,
                                  self.x.shape[1],
                                  subsample=(2, 2))
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
        top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
        utt.assert_allclose(top, np.delete(np.delete(self.precomp_output_valid, 1, axis=3), 1, axis=2))

        # test non-default border_mode
        sep_op = separable_conv2d(x_sym, dfilter_sym, pfilter_sym, self.x.shape[1], border_mode='full')
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
        top = fun(self.x[:, :, :3, :3], self.depthwise_filter, self.pointwise_filter)
        utt.assert_allclose(top, self.precomp_output_full)

    def test_interface3d(self):
        if theano.config.cxx == "":
            raise SkipTest("test needs cxx")
        # Expand the filter along the depth
        x = np.tile(np.expand_dims(self.x, axis=2), (1, 1, 5, 1, 1))
        depthwise_filter = np.tile(np.expand_dims(self.depthwise_filter, axis=2), (1, 1, 3, 1, 1))
        pointwise_filter = np.expand_dims(self.pointwise_filter, axis=2)
        precomp_output = np.tile(np.expand_dims(self.precomp_output_valid, axis=2), (1, 1, 3, 1, 1)) * 3

        x_sym = theano.tensor.tensor5('x')
        dfilter_sym = theano.tensor.tensor5('d')
        pfilter_sym = theano.tensor.tensor5('p')

        sep_op = separable_conv3d(x_sym, dfilter_sym, pfilter_sym, x.shape[1])
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')

        # test for square matrix
        top = fun(x, depthwise_filter, pointwise_filter)
        utt.assert_allclose(top, precomp_output)
        # test for non-square matrix
        top = fun(x[:, :, :3, :, :3], depthwise_filter, pointwise_filter)
        utt.assert_allclose(top, precomp_output[:, :, :1, :, :1])
        # test if it infers shape
        sep_op = separable_conv3d(x_sym,
                                  dfilter_sym,
                                  pfilter_sym,
                                  x.shape[1],
                                  input_shape=x.shape,
                                  depthwise_filter_shape=depthwise_filter.shape,
                                  pointwise_filter_shape=pointwise_filter.shape)
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
        top = fun(x, depthwise_filter, pointwise_filter)
        utt.assert_allclose(top, precomp_output)

        # test non-default subsample
        sep_op = separable_conv3d(x_sym,
                                  dfilter_sym,
                                  pfilter_sym,
                                  x.shape[1],
                                  subsample=(2, 2, 2))
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
        top = fun(x, depthwise_filter, pointwise_filter)
        utt.assert_allclose(top, np.delete(np.delete(
            np.delete(precomp_output, 1, axis=4), 1, axis=3), 1, axis=2))
        # test non-default border_mode
        precomp_output = np.tile(np.expand_dims(self.precomp_output_full, axis=2),
                                 (1, 1, 5, 1, 1)) * np.array([[[[[1]], [[2]], [[3]], [[2]], [[1]]]]])

        sep_op = separable_conv3d(x_sym, dfilter_sym, pfilter_sym, x.shape[1], border_mode='full')
        fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
        top = fun(x[:, :, :3, :3, :3], depthwise_filter, pointwise_filter)
        utt.assert_allclose(top, precomp_output)


class TestUnsharedConv(unittest.TestCase):
    conv2d = theano.tensor.nnet.abstract_conv.AbstractConv2d
    conv2d_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
    conv2d_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
    conv2d_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
    conv2d_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
    conv2d_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs

    mode = theano.compile.mode.Mode(optimizer='None')

    def setUp(self):
        self.img_shape = [(2, 2, 4, 4), (3, 2, 4, 2), (3, 3, 5, 3), (3, 4, 4, 4)]
        self.kern_shape = [(2, 2, 2, 2, 3, 3), (2, 4, 2, 2, 4, 2), (3, 2, 1, 1, 3, 3), (4, 3, 3, 2, 4, 2)]
        self.topgrad_shape = [(2, 2, 2, 2), (3, 2, 4, 2), (3, 3, 2, 1), (3, 4, 3, 3)]
        self.border_mode = ['valid', 'full', 'valid', 'full']
        self.subsample = [(1, 1), (2, 2), (2, 1), (3, 2)]
        self.filter_dilation = (1, 1)
        self.num_groups = [1, 1, 3, 2]

        # self.verify_flags = np.random.choice([True, False], 4, [0.5, 0.5])
        # Above line can be used instead if speed is a concern
        self.verify_flags = [True] * 4

        self.ref_mode = 'FAST_RUN'
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("CorrMM needs cxx or SciPy")

    def test_fwd(self):
        tensor6 = theano.tensor.TensorType(theano.config.floatX, (False,) * 6)
        img_sym = theano.tensor.tensor4('img')
        kern_sym = tensor6('kern')
        ref_kern_sym = theano.tensor.tensor4('ref_kern')

        for imshp, kshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.border_mode,
                                                          self.subsample, self.num_groups, self.verify_flags):
            img = np.random.random(imshp).astype(theano.config.floatX)
            kern = np.random.random(kshp).astype(theano.config.floatX)

            unshared_conv_op = self.conv2d(border_mode=mode, subsample=sub,
                                           filter_dilation=self.filter_dilation,
                                           num_groups=groups, unshared=True)
            unshared_out_sym = unshared_conv_op(img_sym, kern_sym)
            unshared_func = theano.function([img_sym, kern_sym], unshared_out_sym, mode=self.mode)
            assert any([isinstance(node.op, self.conv2d_op)
                        for node in unshared_func.maker.fgraph.toposort()])
            unshared_output = unshared_func(img, kern)

            single_kshp = kshp[:1] + kshp[3:]

            ref_conv_op = self.conv2d(border_mode=mode, subsample=sub,
                                      filter_dilation=self.filter_dilation,
                                      num_groups=groups, unshared=False)
            ref_out_sym = ref_conv_op(img_sym, ref_kern_sym)
            ref_func = theano.function([img_sym, ref_kern_sym], ref_out_sym, mode=self.mode)

            for i in range(0, kshp[1]):
                for j in range(0, kshp[2]):
                    single_kern = kern[:, i, j, ...].reshape(single_kshp)
                    ref_val = ref_func(img, single_kern)
                    utt.assert_allclose(ref_val[:, :, i, j], unshared_output[:, :, i, j])

            if verify:
                utt.verify_grad(unshared_conv_op, [img, kern], mode=self.mode, eps=1)

    def test_gradweight(self):
        img_sym = theano.tensor.tensor4('img')
        top_sym = theano.tensor.tensor4('top')

        for imshp, kshp, topshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.topgrad_shape,
                                                                  self.border_mode, self.subsample, self.num_groups,
                                                                  self.verify_flags):
            img = np.random.random(imshp).astype(theano.config.floatX)
            top = np.random.random(topshp).astype(theano.config.floatX)

            unshared_conv_op = self.conv2d_gradw(border_mode=mode, subsample=sub,
                                                 filter_dilation=self.filter_dilation,
                                                 num_groups=groups, unshared=True)
            unshared_out_sym = unshared_conv_op(img_sym, top_sym, tensor.as_tensor_variable(kshp[-2:]))
            unshared_func = theano.function([img_sym, top_sym], unshared_out_sym, mode=self.mode)
            assert any([isinstance(node.op, self.conv2d_gradw_op)
                        for node in unshared_func.maker.fgraph.toposort()])
            unshared_output = unshared_func(img, top)

            single_kshp = kshp[:1] + kshp[3:]

            ref_conv_op = self.conv2d_gradw(border_mode=mode, subsample=sub,
                                            filter_dilation=self.filter_dilation,
                                            num_groups=groups, unshared=False)
            ref_out_sym = ref_conv_op(img_sym, top_sym, tensor.as_tensor_variable(single_kshp[-2:]))
            ref_func = theano.function([img_sym, top_sym], ref_out_sym, mode=self.mode)

            for i in range(0, topshp[2]):
                for j in range(0, topshp[3]):
                    top_single = np.zeros_like(top)
                    top_single[:, :, i, j] = top[:, :, i, j]
                    ref_output = ref_func(img, top_single)
                    utt.assert_allclose(unshared_output[:, i, j, ...], ref_output)

            def conv_gradweight(inputs_val, output_val):
                return unshared_conv_op(inputs_val, output_val, tensor.as_tensor_variable(kshp[-2:]))

            if verify:
                utt.verify_grad(conv_gradweight, [img, top], mode=self.mode, eps=1)

    def test_gradinput(self):
        tensor6 = theano.tensor.TensorType(theano.config.floatX, (False,) * 6)
        kern_sym = tensor6('kern')
        top_sym = theano.tensor.tensor4('top')
        ref_kern_sym = theano.tensor.tensor4('ref_kern')

        for imshp, kshp, topshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.topgrad_shape,
                                                                  self.border_mode, self.subsample, self.num_groups,
                                                                  self.verify_flags):
            single_kshp = kshp[:1] + kshp[3:]

            kern = np.random.random(kshp).astype(theano.config.floatX)
            top = np.random.random(topshp).astype(theano.config.floatX)

            unshared_conv_op = self.conv2d_gradi(border_mode=mode, subsample=sub,
                                                 filter_dilation=self.filter_dilation,
                                                 num_groups=groups, unshared=True)
            unshared_out_sym = unshared_conv_op(kern_sym, top_sym, tensor.as_tensor_variable(imshp[-2:]))
            unshared_func = theano.function([kern_sym, top_sym], unshared_out_sym, mode=self.mode)
            assert any([isinstance(node.op, self.conv2d_gradi_op)
                        for node in unshared_func.maker.fgraph.toposort()])
            unshared_output = unshared_func(kern, top)

            ref_conv_op = self.conv2d_gradi(border_mode=mode, subsample=sub,
                                            filter_dilation=self.filter_dilation,
                                            num_groups=groups, unshared=False)
            ref_out_sym = ref_conv_op(ref_kern_sym, top_sym, tensor.as_tensor_variable(imshp[-2:]))
            ref_func = theano.function([ref_kern_sym, top_sym], ref_out_sym, mode=self.mode)

            ref_output = np.zeros(imshp)

            for i in range(0, topshp[2]):
                for j in range(0, topshp[3]):
                    single_kern = kern[:, i, j, ...].reshape(single_kshp)
                    top_single = np.zeros_like(top)
                    top_single[:, :, i, j] = top[:, :, i, j]
                    ref_output += ref_func(single_kern, top_single)

            utt.assert_allclose(ref_output, unshared_output)

            def conv_gradinputs(filters_val, output_val):
                return unshared_conv_op(filters_val, output_val, tensor.as_tensor_variable(imshp[-2:]))

            if verify:
                utt.verify_grad(conv_gradinputs, [kern, top], mode=self.mode, eps=1)


class TestAsymmetricPadding(unittest.TestCase):
    conv2d = theano.tensor.nnet.abstract_conv.AbstractConv2d
    conv2d_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
    conv2d_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
    conv2d_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
    conv2d_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
    conv2d_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs

    mode = theano.compile.mode.Mode(optimizer='None')

    img_shape = [(2, 2, 4, 4), (3, 2, 4, 2), (3, 3, 5, 3)]
    kern_shape = [(4, 2, 2, 2), (2, 2, 4, 2), (2, 3, 3, 3)]
    topgrad_shape = [(2, 4, 6, 6), (3, 2, 3, 4), (3, 2, 6, 1)]
    border_mode = [((1, 2), (2, 1)), ((1, 1), (0, 3)), ((2, 1), (0, 0))]

    def test_fwd(self):
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("SciPy and cxx needed")
        img_sym = theano.tensor.tensor4('img')
        kern_sym = theano.tensor.tensor4('kern')

        for imshp, kshp, pad in zip(self.img_shape, self.kern_shape, self.border_mode):
            img = np.random.random(imshp).astype(theano.config.floatX)
            kern = np.random.random(kshp).astype(theano.config.floatX)

            asymmetric_conv_op = self.conv2d(border_mode=pad, subsample=(1, 1),
                                             filter_dilation=(1, 1))
            asymmetric_out_sym = asymmetric_conv_op(img_sym, kern_sym)
            asymmetric_func = theano.function([img_sym, kern_sym], asymmetric_out_sym, mode=self.mode)
            assert any([isinstance(node.op, self.conv2d_op)
                        for node in asymmetric_func.maker.fgraph.toposort()])
            asymmetric_output = asymmetric_func(img, kern)

            ref_conv_op = self.conv2d(border_mode="valid", subsample=(1, 1),
                                      filter_dilation=(1, 1))
            ref_out_sym = ref_conv_op(img_sym, kern_sym)
            ref_func = theano.function([img_sym, kern_sym], ref_out_sym, mode=self.mode)

            exp_imshp = (imshp[0], imshp[1],
                         imshp[2] + pad[0][0] + pad[0][1],
                         imshp[3] + pad[1][0] + pad[1][1])

            exp_img = np.zeros(exp_imshp, dtype=theano.config.floatX)
            exp_img[:, :, pad[0][0]:imshp[2] + pad[0][0],
                    pad[1][0]:imshp[3] + pad[1][0]] = img
            ref_output = ref_func(exp_img, kern)

            utt.assert_allclose(asymmetric_output, ref_output)

            utt.verify_grad(asymmetric_conv_op, [img, kern], mode=self.mode, eps=1)

    def test_gradweight(self):
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("SciPy and cxx needed")

        img_sym = theano.tensor.tensor4('img')
        top_sym = theano.tensor.tensor4('top')

        for imshp, kshp, topshp, pad in zip(self.img_shape, self.kern_shape, self.topgrad_shape, self.border_mode):
            img = np.random.random(imshp).astype(theano.config.floatX)
            top = np.random.random(topshp).astype(theano.config.floatX)

            asymmetric_conv_op = self.conv2d_gradw(border_mode=pad, subsample=(1, 1),
                                                   filter_dilation=(1, 1))
            asymmetric_out_sym = asymmetric_conv_op(img_sym, top_sym, kshp[-2:])
            asymmetric_func = theano.function([img_sym, top_sym], asymmetric_out_sym, mode=self.mode)
            assert any([isinstance(node.op, self.conv2d_gradw_op)
                        for node in asymmetric_func.maker.fgraph.toposort()])
            asymmetric_output = asymmetric_func(img, top)

            ref_conv_op = self.conv2d_gradw(border_mode="valid", subsample=(1, 1),
                                            filter_dilation=(1, 1))
            ref_out_sym = ref_conv_op(img_sym, top_sym, kshp[-2:])
            ref_func = theano.function([img_sym, top_sym], ref_out_sym, mode=self.mode)

            exp_imshp = (imshp[0], imshp[1],
                         imshp[2] + pad[0][0] + pad[0][1],
                         imshp[3] + pad[1][0] + pad[1][1])

            exp_img = np.zeros(exp_imshp, dtype=theano.config.floatX)
            exp_img[:, :, pad[0][0]:imshp[2] + pad[0][0],
                    pad[1][0]:imshp[3] + pad[1][0]] = img
            ref_output = ref_func(exp_img, top)

            utt.assert_allclose(asymmetric_output, ref_output)

            def conv_gradweight(inputs_val, output_val):
                return asymmetric_conv_op(inputs_val, output_val, tensor.as_tensor_variable(kshp[-2:]))

            utt.verify_grad(conv_gradweight, [img, top], mode=self.mode, eps=1)

    def test_gradinput(self):
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("test needs cxx and SciPy")
        kern_sym = theano.tensor.tensor4('kern')
        top_sym = theano.tensor.tensor4('top')

        for imshp, kshp, topshp, pad in zip(self.img_shape, self.kern_shape, self.topgrad_shape, self.border_mode):
            kern = np.random.random(kshp).astype(theano.config.floatX)
            top = np.random.random(topshp).astype(theano.config.floatX)

            asymmetric_conv_op = self.conv2d_gradi(border_mode=pad, subsample=(1, 1),
                                                   filter_dilation=(1, 1))
            asymmetric_out_sym = asymmetric_conv_op(kern_sym, top_sym, imshp[-2:])
            asymmetric_func = theano.function([kern_sym, top_sym], asymmetric_out_sym, mode=self.mode)
            assert any([isinstance(node.op, self.conv2d_gradi_op)
                        for node in asymmetric_func.maker.fgraph.toposort()])
            asymmetric_output = asymmetric_func(kern, top)

            ref_conv_op = self.conv2d_gradi(border_mode="valid", subsample=(1, 1),
                                            filter_dilation=(1, 1))
            exp_imshp = [imshp[2] + pad[0][0] + pad[0][1],
                         imshp[3] + pad[1][0] + pad[1][1]]
            ref_out_sym = ref_conv_op(kern_sym, top_sym, exp_imshp)
            ref_func = theano.function([kern_sym, top_sym], ref_out_sym, mode=self.mode)

            ref_output = ref_func(kern, top)

            ref_output = ref_output[:, :, pad[0][0]:imshp[2] + pad[0][0],
                                    pad[1][0]:imshp[3] + pad[1][0]]

            utt.assert_allclose(asymmetric_output, ref_output)

            def conv_gradinputs(filters_val, output_val):
                return asymmetric_conv_op(filters_val, output_val, tensor.as_tensor_variable(imshp[-2:]))

            utt.verify_grad(conv_gradinputs, [kern, top], mode=self.mode, eps=1)


class TestCausalConv(unittest.TestCase):
    mode = theano.compile.mode.Mode(optimizer='None')

    img = np.array([[[2, 4, 9, 5, 8], [0, 0, 4, 0, 5]],
                    [[2, 5, 8, 5, 5], [1, 3, 0, 7, 9]],
                    [[7, 0, 7, 1, 0], [0, 1, 4, 7, 2]]]).astype(theano.config.floatX)
    kern = np.array([[[5, 3, 1], [3, 1, 0]],
                     [[6, 4, 9], [2, 2, 7]]]).astype(theano.config.floatX)
    dilation = 2
    precomp_top = np.array([[[10, 20, 63, 37, 88], [12, 24, 70, 46, 120]],
                            [[13, 34, 47, 64, 78], [14, 36, 58, 70, 105]],
                            [[35, 3, 68, 27, 38], [42, 2, 78, 22, 103]]]).astype(theano.config.floatX)

    def test_interface(self):
        img_sym = theano.tensor.tensor3('img')
        kern_sym = theano.tensor.tensor3('kern')
        if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
            raise SkipTest("SciPy and cxx needed")
        sym_out = causal_conv1d(img_sym, kern_sym, self.kern.shape, filter_dilation=self.dilation)

        causal_func = theano.function([img_sym, kern_sym], sym_out, mode=self.mode)

        output = causal_func(self.img, self.kern)

        utt.assert_allclose(output, self.precomp_top)

        def causal_conv_fn(inputs_val, filters_val):
            return causal_conv1d(inputs_val, filters_val, self.kern.shape, filter_dilation=1)

        utt.verify_grad(causal_conv_fn, [self.img, self.kern], mode=self.mode, eps=1)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>check_dnn_conv.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
#!/usr/bin/env python

# Without args, this script executes all its tests like `nosetests -vs`
# python check_dnn_conv.py

# If there is only one arg `infos`, this script prints some infos about
# supported algorithms and data type configurations for current GPU and cuDNN version.
# python check_dnn_conv.py infos

# If there is only one arg `list`, this script prints all test cases without running them.
# python check_dnn_conv.py list

<a name="2"></a># Else, any arg will be directly passed to nosetests.
# python check_dnn_conv.py -xvs  # nosetests: verbose mode, capture output, exit at first error.

<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division

import math
import sys
from itertools import product, chain

import nose
import numpy as np
from nose.plugins.skip import SkipTest

import theano
import theano.tests.unittest_tools as utt
from theano.compat import ifilter
from theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME
from theano.gpuarray import cudnn_defs
from theano.gpuarray.dnn import (GpuDnnConv, GpuDnnConvGradW, GpuDnnConvGradI, version,
                                 _dnn_conv as dnn_conv, _dnn_gradinput as dnn_gradinput,
                                 _dnn_gradweight as dnn_gradweight)
from theano.gpuarray.tests.config import mode_with_gpu, ref_cast
from theano.tensor.nnet.abstract_conv import get_conv_output_shape, assert_conv_shape
from theano.tensor.nnet.corr import CorrMM, CorrMM_gradInputs, CorrMM_gradWeights
from theano.tensor.nnet.corr3d import Corr3dMM, Corr3dMM_gradInputs, Corr3dMM_gradWeights


def check_dtype_config_support(dtype, precision):
    # We use FWD 2D to check it.
    # Based on documentation, algo small (CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)
    # should support all configurations, for both v5.1, v6 and v7.
    inputs =</b></font> theano.shared(np.zeros((1, 1, 2, 2), dtype=dtype))
    filters = theano.shared(np.zeros((1, 1, 2, 2), dtype=dtype))
    conv = dnn_conv(inputs, filters, precision=precision, algo='small')
    f = theano.function([], conv, mode=mode_with_gpu)
    try:
        f()
    except RuntimeError as e:
        assert 'CUDNN_STATUS_ARCH_MISMATCH' in e.message
        return False
    return True


cudnn = cudnn_defs.get_definitions(version(raises=False))


class ConvCase:
    """
    Helper class to describe a special test case quickly.
    This handles only 2D and 3D cases.
    """

    FWD, GRADINPUT, GRADWEIGHT = 0, 1, 2

    def __init__(self, type,
                 inputs_shape, filters_shape,
                 algo=None, dtype=None, precision=None,
                 subsample=None, dilation=None, border_mode='valid',
                 conv_mode='conv', alpha=1, beta=0,
                 should_fail=False):
        assert type in (ConvCase.FWD, ConvCase.GRADINPUT, ConvCase.GRADWEIGHT)
        assert len(inputs_shape) == len(filters_shape) in (4, 5)
        ndim = len(inputs_shape) - 2
        if dtype is None:
            dtype = theano.config.floatX
        if precision is None:
            precision = theano.config.floatX
        if subsample is None:
            subsample = (1,) * ndim
        if dilation is None:
            dilation = (1,) * ndim
        assert dtype in ('float16', 'float32', 'float64')
        assert precision in ('float16', 'float32', 'float64')
        assert len(subsample) == len(dilation) == ndim
        assert (border_mode in ('valid', 'full', 'half') or
                (isinstance(border_mode, (list, tuple)) and len(border_mode) == ndim))
        assert conv_mode in ('conv', 'cross')
        assert alpha != 0

        self.type = type
        self.ndim = ndim
        self.algo = algo
        self.inputs_shape = inputs_shape
        self.filters_shape = filters_shape
        self.dtype = dtype
        self.precision = precision
        self.subsample = subsample
        self.dilation = dilation
        self.border_mode = border_mode
        self.conv_mode = conv_mode
        self.alpha = alpha
        self.beta = beta
        self.should_fail = bool(should_fail)

    def is_fwd(self):
        return self.type == ConvCase.FWD

    def is_bwd_filter(self):
        return self.type == ConvCase.GRADWEIGHT

    def is_bwd_data(self):
<a name="12"></a>        return self.type == ConvCase.GRADINPUT

    def get_case(self):
        return (<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>self.algo, self.dtype, self.precision,
                (self.inputs_shape, self.filters_shape,
                 self.subsample, self.dilation, self.border_mode,
                 self.conv_mode, self.alpha, self.</b></font>beta))

    @staticmethod
    def fwd(*args, **kwargs):
        return ConvCase(ConvCase.FWD, *args, **kwargs)

    @staticmethod
    def bwd_filter(*args, **kwargs):
        return ConvCase(ConvCase.GRADWEIGHT, *args, **kwargs)

    @staticmethod
    def bwd_data(*args, **kwargs):
        return ConvCase(ConvCase.GRADINPUT, *args, **kwargs)


class ConvCaseGenerator:
    """
    Main class used to generate test cases.
    This handles only 2D and 3D cases.
    """

    def _as_tuple_of_tuples(self, iterable):
<a name="14"></a>        return tuple(tuple(sequence) for sequence in iterable)

    def __init__(self, ndim,
                 alpha<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=2, beta=-3, batch_size=2, input_channels=3, inputs_sizes=None, output_channels=2,
                 filters_sizes=None, subsamples=None, dilations=None, borders=None,
                 with_border_valid=True, with_border_half=True, with_border_full=</b></font>True):
        self.ndim = int(ndim)
        self.alpha = float(alpha)
        self.beta = float(beta)
        self.batch_size = int(batch_size)
        self.input_channels = int(input_channels)
        self.output_channels = int(output_channels)

        assert self.ndim in (2, 3)
        assert self.alpha != 0
        assert self.batch_size &gt; 0
        assert self.input_channels &gt; 0
        assert self.output_channels &gt; 0

        # NB: it is quite arbitrary to choose default values for inputs sizes and filters sizes.
        # Here, we just put some values that may generate errors in some cases, but that should be OK for other cases.
        # For instance, input size 300 is &gt; 256, that is a limit for certain algorithms (cf. documentation).
        # Filter size 40 is &gt; 32 and &gt; 16, that are limits for certain algorithms (cf. documentation).
        # We should either manually specify sizes, or give an appropriate filter to this generator
        # before testing values (see `self.get_cases()`).

        if inputs_sizes is None:
            inputs_sizes = ((5,) * self.ndim,
                            (300, 5) + (2,) * (self.ndim - 2))
        if filters_sizes is None:
            filters_sizes = ((4,) * self.ndim,
                             (40, 4) + (2,) * (self.ndim - 2))
        if borders is None:
            borders = ((1,) * self.ndim,
                       tuple(range(1, self.ndim + 1)))
        if subsamples is None:
            subsamples = ((1,) * self.ndim,
                          tuple(range(1, self.ndim + 1)))
        if dilations is None:
            dilations = ((1,) * self.ndim,)
            if cudnn.version &gt;= 6:
                dilations += (tuple(range(1, self.ndim + 1)),)

        for sequence_list in (inputs_sizes, filters_sizes, borders, subsamples, dilations):
            assert (isinstance(sequence_list, (tuple, list)) and
                    all(isinstance(sequence, (tuple, list)) and len(sequence) == self.ndim
                        for sequence in sequence_list)), (self.ndim, sequence_list)

        self.auto_borders = tuple()
        if with_border_valid:
            self.auto_borders += ('valid',)
        if with_border_half:
            self.auto_borders += ('half',)
<a name="4"></a>        if with_border_full:
            self.auto_borders += ('full',)

        self<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.inputs_sizes = self._as_tuple_of_tuples(inputs_sizes)
        self.filters_sizes = self._as_tuple_of_tuples(filters_sizes)
        self.borders = self._as_tuple_of_tuples(borders)
        self.subsamples = self._as_tuple_of_tuples(subsamples)
        self.dilations =</b></font> self._as_tuple_of_tuples(dilations)

    @staticmethod
    def get_if_valid_conv_output_shape(case_tuple):
        # Filter function to keep only cases that produce valid convolution output shapes.
        out_shp = get_conv_output_shape(case_tuple[0],  # input shape
                                        case_tuple[1],  # filter shape
                                        case_tuple[4],  # border mode
                                        case_tuple[2],  # subsample
                                        case_tuple[3])  # dilation
        try:
            return assert_conv_shape(out_shp)
        except ValueError:
            return False

    def get_cases(self, filter=None):
        # Generate an iterator of tuples with format:
        # (input shape, filter shape, subsample, dilation, border mode, convolution mode, alpha, beta)
<a name="11"></a>        # filter may be a callable that gets one tuple (with format specified above) and returns
        # a boolean, so that tuple is kept only if filter(tuple) is True.

        all_batch_sizes <font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= (self.batch_size,)
        all_input_channels = (self.input_channels,)
        all_input_sizes = self.inputs_sizes
        all_output_channels = (self.output_channels,)
        all_filter_sizes = self.filters_sizes
<a name="7"></a>        all_subsamples =</b></font> self.subsamples
        all_dilations = self.dilations
        all_border_modes = self.auto_borders + self.borders
        all_conv_modes = (<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'conv', 'cross')
        all_alphas = (self.alpha,)
        all_betas = (0,) if self.beta == 0 else (0, self.beta)

        all_input_shapes = ((bs, ic) + ins
                            for bs in all_batch_sizes for ic in all_input_channels for ins in all_input_sizes)
        all_filter_shapes = ((oc</b></font>, ic) + fis
                             for oc in all_output_channels for ic in all_input_channels for fis in all_filter_sizes)
        if callable(filter):
            def local_filter(case_tuple):
                return ConvCaseGenerator.get_if_valid_conv_output_shape(case_tuple) and filter(case_tuple)
        else:
            local_filter = ConvCaseGenerator.get_if_valid_conv_output_shape
        return ifilter(local_filter,
                       product(all_input_shapes, all_filter_shapes, all_subsamples, all_dilations,
                               all_border_modes, all_conv_modes, all_alphas, all_betas))


class ConvCaseGeneratorChain:
    """
    Helper class to concatenate many conv case generators.
    """

    def __init__(self, *conv_case_generators):
        assert all(isinstance(g, ConvCaseGenerator) for g in conv_case_generators)
        self.generators = conv_case_generators

    def get_cases(self, filter=None):
        return chain(*[generator.get_cases(filter) for generator in self.generators])


class CuDNNV51ConvCaseGenerator(object):
    """
    Helper class to generate specific test cases for every algorithm supported by cuDNN V5.1.
    Same class exists for cuDNN V6.0 (see below).
    This should help avoid test cases that are intended to fail according to cuDNN documentation.
    """
    NONE = 'none'
    FFT = 'fft'
    FFT_TILING = 'fft_tiling'
    WINOGRAD = 'winograd'
    WINOGRAD_NON_FUSED = 'winograd_non_fused'

    # Protected interface.

    def _dilations(self, ndim):
        return [(1,) * ndim]

    def _fwd_fft(self, ndim):
        inputs_sizes = [(10,) * ndim,
                        (240, 5) + (2,) * (ndim - 2)]
        filters_sizes = [tuple(range(9, 9 - ndim, -1))]
        subsamples = [(1,) * ndim]
        return ConvCaseGenerator(ndim=ndim,
                                 inputs_sizes=inputs_sizes,
                                 filters_sizes=filters_sizes,
                                 subsamples=subsamples,
                                 dilations=self._dilations(ndim))

    def _fwd_fft_tiling(self, ndim, dtype, precision):
        if ndim == 2:
            filters_sizes = [(32, 5)]
        if ndim == 3:
            filters_sizes = [(16, 5, 5)]
        subsamples = [(1,) * ndim]
        return ConvCaseGenerator(ndim=ndim,
                                 filters_sizes=filters_sizes,
                                 subsamples=subsamples,
                                 dilations=self._dilations(ndim))

    def _fwd_winograd(self, ndim):
        filters_sizes = [(3,) * ndim]
        subsamples = [(1,) * ndim]
        return ConvCaseGenerator(ndim=ndim,
                                 filters_sizes=filters_sizes,
                                 subsamples=subsamples,
                                 dilations=self._dilations(ndim))

    def _fwd_winograd_non_fused(self, ndim, dtype, precision):
        filters_sizes = [(3,) * ndim]
        if not (dtype == precision == 'float16'):
            filters_sizes += [(5,) * ndim]
        subsamples = [(1,) * ndim]
        return ConvCaseGenerator(ndim=ndim,
                                 filters_sizes=filters_sizes,
                                 subsamples=subsamples,
                                 dilations=self._dilations(ndim))

    def _gw_fft(self, ndim):
        return self._fwd_fft(ndim)

    def _gw_winograd_non_fused(self, ndim, dtype, precision):
        return self._fwd_winograd_non_fused(ndim, dtype, precision)

    def _gi_fft(self, ndim):
        return self._fwd_fft(ndim)

    def _gi_fft_tiling(self, ndim, dtype, precision):
        return self._fwd_fft_tiling(ndim, dtype, precision)

    def _gi_winograd(self, ndim):
        return self._fwd_winograd(ndim)

    def _gi_winograd_non_fused(self, ndim, dtype, precision):
        return self._fwd_winograd_non_fused(ndim, dtype, precision)

    def _fwd_runtime(self, ndim, dtype, precision):
        return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))

    def _gw_runtime(self, ndim, dtype, precision):
        return self._fwd_runtime(ndim, dtype, precision)

    def _gi_runtime(self, ndim, dtype, precision):
        return self._fwd_runtime(ndim, dtype, precision)

    # Public interface.

    def fwd(self, algo, ndim, dtype, precision):
        if algo == self.FFT:
            return self._fwd_fft(ndim)
        if algo == self.FFT_TILING:
            return self._fwd_fft_tiling(ndim, dtype, precision)
        if algo == self.WINOGRAD:
            return self._fwd_winograd(ndim)
        if algo == self.WINOGRAD_NON_FUSED:
            return self._fwd_winograd_non_fused(ndim, dtype, precision)
        if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            return self._fwd_runtime(ndim, dtype, precision)
        return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))

    def gw(self, algo, ndim, dtype, precision):
        if algo == self.FFT:
            return self._gw_fft(ndim)
        if algo == self.WINOGRAD_NON_FUSED:
            return self._gw_winograd_non_fused(ndim, dtype, precision)
        if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            return self._gw_runtime(ndim, dtype, precision)
        return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))

    def gi(self, algo, ndim, dtype, precision):
        if algo == self.FFT:
            return self._gi_fft(ndim)
        if algo == self.FFT_TILING:
            return self._gi_fft_tiling(ndim, dtype, precision)
        if algo == self.WINOGRAD:
            return self._gi_winograd(ndim)
        if algo == self.WINOGRAD_NON_FUSED:
            return self._gi_winograd_non_fused(ndim, dtype, precision)
        if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            return self._gi_runtime(ndim, dtype, precision)
        return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))


class CuDNNV6ConvCaseGenerator(CuDNNV51ConvCaseGenerator):
    def _fwd_none(self, ndim):
        # All dilations allowed.
        return ConvCaseGenerator(ndim=ndim)

    def _fwd_fft_tiling(self, ndim, dtype, precision):
        if ndim == 2:
            subsamples = [(1, 1)]
            # wDesc's filter height must be greater than convDesc's zero-padding height
            # wDesc's filter width must be greater than convDesc's zero-padding width
            generators = []
            if (dtype, precision) != ('float64', 'float64'):
                # Filter sizes with every dimension != 1 is not supported for DOUBLE_CONFIG.
                filters_sizes = [(32, 5), (10, 10)]
                borders = [(1, 1), (6, 4)]
                generators += [ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim), subsamples=subsamples,
                                                 filters_sizes=filters_sizes, borders=borders)]
            filters_sizes = [(256, 1), (5, 1)]
            borders = [(1, 0), (2, 0)]
            generators += [ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim), subsamples=subsamples,
                                             filters_sizes=filters_sizes, borders=borders)]
            return ConvCaseGeneratorChain(*generators)
        if ndim == 3:
            return super(CuDNNV6ConvCaseGenerator, self)._fwd_fft_tiling(ndim, dtype, precision)

    def _gw_none(self, ndim):
        return self._fwd_none(ndim)

    def _gw_fft_tiling(self, ndim):
        inputs_sizes = [(247, 1), (20, 1)]
        filters_sizes = [(3, 1), (10, 1)]
        subsamples = [(1,) * ndim]
        borders = [(1, 0), (2, 0)]
        return ConvCaseGenerator(ndim=ndim,
                                 inputs_sizes=inputs_sizes,
                                 filters_sizes=filters_sizes,
                                 subsamples=subsamples,
                                 borders=borders,
                                 dilations=self._dilations(ndim))

    def _gi_none(self, ndim):
        return self._fwd_none(ndim)

    def _fwd_runtime(self, ndim, dtype, precision):
        if ndim == 2 and dtype == precision == 'float16':
            return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
        return super(CuDNNV6ConvCaseGenerator, self)._fwd_runtime(ndim, dtype, precision)

    def _gw_runtime(self, ndim, dtype, precision):
        if ndim == 2 and dtype == precision == 'float16':
            return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
        return super(CuDNNV6ConvCaseGenerator, self)._gw_runtime(ndim, dtype, precision)

    def _gi_runtime(self, ndim, dtype, precision):
        if ndim == 2 and dtype == precision == 'float16':
            return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
        return super(CuDNNV6ConvCaseGenerator, self)._gi_runtime(ndim, dtype, precision)

    def fwd(self, algo, ndim, dtype, precision):
        if algo == self.NONE:
            return self._fwd_none(ndim)
        return super(CuDNNV6ConvCaseGenerator, self).fwd(algo, ndim, dtype, precision)

    def gw(self, algo, ndim, dtype, precision):
        if algo == self.NONE:
            return self._gw_none(ndim)
        if algo == self.FFT_TILING:
            return self._gw_fft_tiling(ndim)
        return super(CuDNNV6ConvCaseGenerator, self).gw(algo, ndim, dtype, precision)

    def gi(self, algo, ndim, dtype, precision):
        if algo == self.NONE:
            return self._gi_none(ndim)
        return super(CuDNNV6ConvCaseGenerator, self).gi(algo, ndim, dtype, precision)


cudnn_conv_case_generator = CuDNNV51ConvCaseGenerator() if cudnn.version &lt; 6 else CuDNNV6ConvCaseGenerator()


class BaseTestDnnConv(object):
    """
    Base class for exhaustive tests. Use its subclasses
    to run actual tests.
    """

    # Abstract attributes.

    ndim = 2

    fwd_algorithms = None
    bwd_filter_algorithms = None
    bwd_data_algorithms = None

    cpu_conv_class = None
    cpu_gradinput_class = None
    cpu_gradweight_class = None

    special_cases = []  # List of special ConvCases.

    runtime_shapes = []  # Tuple of tuples with format: n_times, (inputs_shape, filters_shape)

    # Utility methods.

    def _next_ten_exponent(self, val):
        # Return exponent for the next ten power that follows val.
        # val should be a positive integer.
        # Examples:
        # for 0 to 9, returns 1 (=&gt; 10**1 == 10)
        # for 10 to 99, returns 2 (=&gt; 10**2 == 100)
        ten_exponent = 1
        while val // 10 &gt; 0:
            ten_exponent += 1
            val //= 10
        return ten_exponent

    def scale_numpy_arrays_inplace(self, A, B, alpha):
        scale_factor = 1
        # Scale down simultaneously A and B if alpha is not 1.
        if alpha != 1:
            scale_factor *= alpha
        # Normalize A and B simultaneously so that any values in these tensors are in interval [0, 1)
        max_a = math.floor(abs(A.max()))
        max_b = math.floor(abs(B.max()))
        if max_a or max_b:
            m_a = self._next_ten_exponent(max_a)
            m_b = self._next_ten_exponent(max_b)
            max_m = max(m_a, m_b)
            scale_factor *= 10 ** max_m
        if scale_factor != 1:
            A /= scale_factor
            B /= scale_factor

    def get_atol_rtol(self, algo, dtype, precision):
        if dtype == 'float16':
            # Raise tolerance for float16
            return (5e-2, 5e-2)
        if algo == 'winograd_non_fused' and dtype == precision == 'float32':
            # Raise tolerance for winograd_non_fused in FLOAT_CONFIG.
            return (1e-4, 1e-4)
        return None, None

    def __init__(self):
        utt.seed_rng(1234)
        self.dtype_configs = cudnn.get_supported_dtype_configs(check_dtype_config_support)

    def array_like_conv_output(self, inputs_shape, filters_shape, border_mode, subsample, dilation, dtype):
        # Return a random array with inferred convolution output shape.
        out_shp = get_conv_output_shape(inputs_shape, filters_shape, border_mode, subsample, dilation)
        out_shp = assert_conv_shape(out_shp)
        return np.random.random(out_shp).astype(dtype)

<a name="0"></a>    def run_conv_fwd(self, algo, dtype, precision, parameters):
        inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters

        inputs_val <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
        filters_val = np.random.random(filters_shape).astype(dtype)

        # Scale down the input values to prevent very large absolute errors
        # due to float rounding
        inputs_val /= 10
        filters_val /= 10

        inputs = theano.shared(inputs_val)
        filters = theano.shared(filters_val)

        if beta == 0:
            out = None
        else:
<a name="5"></a>            out =</b></font> self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)
            out /= 10
        # Compile a theano function for the cuDNN implementation
        conv = dnn_conv<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(img=inputs, kerns=filters, alpha=alpha, beta=beta, out=out, border_mode=border_mode,
                        subsample=subsample, dilation=dilation, conv_mode=conv_mode, algo=algo, precision=precision)
        f = theano.function([], conv, mode=</b></font>mode_with_gpu)

        # If conv_mode is 'conv' the reference implementation should use
        # filters flipped according to the width, height and time axis
        if conv_mode == 'conv':
            if inputs.ndim == 5:
                flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
            else:
                flipped_filters = filters[:, :, ::-1, ::-1]
        else:
            flipped_filters = filters

        # Compile a theano function for the reference implementation
        conv_ref = self.cpu_conv_class(border_mode=border_mode,
                                       subsample=subsample,
                                       filter_dilation=dilation)(ref_cast(inputs), flipped_filters)
        f_ref = theano.function([], conv_ref, mode="FAST_RUN")

        # Compare the results of the two implementations
        res_ref = f_ref()
        res = np.asarray(f())
        if algo in cudnn.deterministic_fwd_algorithms:
            utt.assert_allclose(res, np.asarray(f()))

        atol, rtol = self.get_atol_rtol(algo, dtype, precision)
        if beta == 0:
            cpu_res = alpha * res_ref
        else:
            cpu_res = alpha * res_ref + beta * out
        self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
        utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)

    def run_conv_gradinput(self, algo, dtype, precision, parameters):
        inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters

        if beta == 0:
            inputs_val = None
        else:
            inputs_val = np.random.random(inputs_shape).astype(dtype)
            inputs_val /= 10
        filters_val = np.random.random(filters_shape).astype(dtype)
        topgrad_val = self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)

        # Scale down the input values to prevent absolute errors in utt.assert_allclose.
        filters_val /= 10
        topgrad_val /= 10

        filters = theano.shared(filters_val)
<a name="10"></a>        topgrad = theano.shared(topgrad_val)

        # Compile a theano function for the cuDNN implementation
        grad_i = dnn_gradinput<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(filters, topgrad, inputs_shape, alpha=alpha, beta=beta, out=inputs_val,
                               border_mode=border_mode, subsample=subsample, dilation=dilation, conv_mode=conv_mode,
                               algo=algo, precision=precision)

        f = theano.function([], grad_i, mode=</b></font>mode_with_gpu)

        # If conv_mode is 'conv' the reference implementation should use
        # filters flipped according to the width, height and time axis
        if conv_mode == 'conv':
            if filters.ndim == 5:
                flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
            else:
                flipped_filters = filters[:, :, ::-1, ::-1]
        else:
            flipped_filters = filters

        # Compile a theano function for the reference implementation
        grad_i_ref = self.cpu_gradinput_class(border_mode=border_mode,
                                              subsample=subsample,
                                              filter_dilation=dilation
                                              )(ref_cast(flipped_filters), ref_cast(topgrad), inputs_shape[2:])
        f_ref = theano.function([], grad_i_ref, mode="FAST_RUN")

        # Compare the results of the two implementations
        res_ref = f_ref()
        res = np.asarray(f())
        if algo in cudnn.deterministic_bwd_data_algorithms:
            utt.assert_allclose(res, np.asarray(f()))

        atol, rtol = self.get_atol_rtol(algo, dtype, precision)
        if beta == 0:
            cpu_res = alpha * res_ref
        else:
            cpu_res = alpha * res_ref + beta * inputs_val
        self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
        utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)

    def run_conv_gradweight(self, algo, dtype, precision, parameters):
        inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters

        inputs_val = np.random.random(inputs_shape).astype(dtype)
        if beta == 0:
            filters_val = None
        else:
            filters_val = np.random.random(filters_shape).astype(dtype)
            filters_val /= 10
        topgrad_val = self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)

        # Scale down the input values to prevent absolute errors in utt.assert_allclose.
        inputs_val /= 10
        topgrad_val /= 10

        inputs = theano.shared(inputs_val)
<a name="8"></a>        topgrad = theano.shared(topgrad_val)

        # Compile a theano function for the cuDNN implementation
        grad_w = dnn_gradweight<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(inputs, topgrad, filters_shape, alpha=alpha, beta=beta, out=filters_val,
                                border_mode=border_mode, subsample=subsample, dilation=dilation, conv_mode=conv_mode,
                                algo=algo, precision=precision)

        f = theano.function([], grad_w, mode=mode_with_gpu)

        # Compile a theano function for the reference implementation
        grad_w_ref =</b></font> self.cpu_gradweight_class(border_mode=border_mode,
                                               subsample=subsample,
                                               filter_dilation=dilation)(ref_cast(inputs), ref_cast(topgrad),
                                                                         filters_shape[2:])
        if conv_mode == 'conv':
            if inputs.ndim == 5:
                grad_w_ref = grad_w_ref[:, :, ::-1, ::-1, ::-1]
            else:
                grad_w_ref = grad_w_ref[:, :, ::-1, ::-1]
        f_ref = theano.function([], grad_w_ref, mode="FAST_RUN")

        # Compare the results of the two implementations
        res_ref = f_ref()
        res = np.asarray(f())
        if algo in cudnn.deterministic_bwd_filter_algorithms:
            utt.assert_allclose(res, np.asarray(f()))

        atol, rtol = self.get_atol_rtol(algo, dtype, precision)
        if beta == 0:
            cpu_res = alpha * res_ref
        else:
            cpu_res = alpha * res_ref + beta * filters_val
        self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
        utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)

    def should_fail(self, function, *args):
        try:
            print('(should fail)', file=sys.stderr, end=' ')
            function(*args)
        except Exception:
            pass
        else:
            raise AssertionError('Should fail', callable.__name__, *args)

    def should_fail_fwd(self, *args):
        self.should_fail(self.run_conv_fwd, *args)

    def should_fail_gradinput(self, *args):
        self.should_fail(self.run_conv_gradinput, *args)

    def should_fail_gradweight(self, *args):
        self.should_fail(self.run_conv_gradweight, *args)

    def get_expected_tcount(self):
        """
        Utility function to get expected test count
        without actually run nosetests.
        """
        return (sum(1 for t in self.test_fwd()) +
                sum(1 for t in self.test_gradweight()) +
                sum(1 for t in self.test_gradinput()) +
                sum(1 for t in self.test_fwd_runtime_algorithms()) +
                sum(1 for t in self.test_gradweight_runtime_algorithms()) +
                sum(1 for t in self.test_gradinput_runtime_algorithms()))

    # Iterable test methods.

    def test_fwd(self):
        for dtype, precision in self.dtype_configs:
            algos = [algo for algo in self.fwd_algorithms
                     if cudnn.fwd_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
            for algo in algos:
                for parameters in cudnn_conv_case_generator.fwd(algo, self.ndim, dtype, precision).get_cases():
                    yield (self.run_conv_fwd, algo, dtype, precision, parameters)
            if algos:
                # Some algorithms support current data type configuration for current ndim.
                # So, an algorithm could be chosen at runtime.
                for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
                    for parameters in cudnn_conv_case_generator.fwd(algo, self.ndim, dtype, precision).get_cases():
                        yield (self.run_conv_fwd, algo, dtype, precision, parameters)
        for dnn_case in self.special_cases:
            if dnn_case.is_fwd():
                if dnn_case.should_fail:
                    yield (self.should_fail_fwd,) + dnn_case.get_case()
                else:
                    yield (self.run_conv_fwd,) + dnn_case.get_case()

    def test_gradinput(self):
        for dtype, precision in self.dtype_configs:
            algos = [algo for algo in self.bwd_data_algorithms
                     if cudnn.bwd_data_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
            for algo in algos:
                for parameters in cudnn_conv_case_generator.gi(algo, self.ndim, dtype, precision).get_cases():
                    yield (self.run_conv_gradinput, algo, dtype, precision, parameters)
            if algos:
                # Some algorithms support current data type configuration for current ndim.
                # So, an algorithm could be chosen at runtime.
                for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
                    for parameters in cudnn_conv_case_generator.gi(algo, self.ndim, dtype, precision).get_cases():
                        yield (self.run_conv_gradinput, algo, dtype, precision, parameters)
        for dnn_case in self.special_cases:
            if dnn_case.is_bwd_data():
                if dnn_case.should_fail:
                    yield (self.should_fail_gradinput,) + dnn_case.get_case()
                else:
                    yield (self.run_conv_gradinput,) + dnn_case.get_case()

    def test_gradweight(self):
        for dtype, precision in self.dtype_configs:
            algos = [algo for algo in self.bwd_filter_algorithms
                     if cudnn.bwd_filter_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
            for algo in algos:
                for parameters in cudnn_conv_case_generator.gw(algo, self.ndim, dtype, precision).get_cases():
                    yield (self.run_conv_gradweight, algo, dtype, precision, parameters)
            if algos:
                # Some algorithms support current data type configuration for current ndim.
                # So, an algorithm could be chosen at runtime.
                for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
                    for parameters in cudnn_conv_case_generator.gw(algo, self.ndim, dtype, precision).get_cases():
                        yield (self.run_conv_gradweight, algo, dtype, precision, parameters)
        for dnn_case in self.special_cases:
            if dnn_case.is_bwd_filter():
                if dnn_case.should_fail:
                    yield (self.should_fail_gradweight,) + dnn_case.get_case()
                else:
                    yield (self.run_conv_gradweight,) + dnn_case.get_case()

    # The 3 following tests are intended to be run with theano flag `cmodule.debug=True`.
    # The output message should then be analyzed to check if runtime algorithms are
    # reused, reloaded from cache or updated, depending on what we expect from
    # dnn_fwd/dnn_gi/dnn_gw current codes. I currently don't know a better way
    # to efficiently test implemented cuDNN convolution caches.

    def test_fwd_runtime_algorithms(self):
        dtype = 'float32'
        unit_shape = (1,) * self.ndim
        _broadcastable = [False] * (2 + self.ndim)

        def run_fwd_runtime_algorithm(algo):
            inputs = theano.tensor.TensorType(dtype, _broadcastable)()
<a name="16"></a>            filters = theano.tensor.TensorType(dtype, _broadcastable)()
            # Scale down the input values to prevent very large absolute errors
            # due to float rounding
            lower_inputs <font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= inputs / 10
            lower_filters = filters / 10
            conv = dnn_conv(img=lower_inputs, kerns=lower_filters, algo=algo, precision=dtype,
                            subsample=unit_shape, dilation=unit_shape)
            f = theano.function(</b></font>[inputs, filters], conv, mode=mode_with_gpu)
            if self.ndim == 3:
                flipped_filters = lower_filters[:, :, ::-1, ::-1, ::-1]
            else:
                flipped_filters = lower_filters[:, :, ::-1, ::-1]
            conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(lower_inputs), flipped_filters)
            f_ref = theano.function([inputs, filters], conv_ref, mode='FAST_RUN')
            runtime_shapes = self.runtime_shapes
            if algo in ('time_once', 'guess_once'):
                runtime_shapes = [list(runtime_shapes[0])]
                runtime_shapes[0][0] = 5
<a name="9"></a>            for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
                print('Shapes:', inputs_shape, filters_shape)
                for i in range(ntimes):
                    inputs_val <font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
                    filters_val = np.random.random(filters_shape).astype(dtype)
                    gpu_res = np.asarray(</b></font>f(inputs_val, filters_val))
                    cpu_res = f_ref(inputs_val, filters_val)
                    self.scale_numpy_arrays_inplace(cpu_res, gpu_res, 1)
                    utt.assert_allclose(cpu_res, gpu_res)

        for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            yield (run_fwd_runtime_algorithm, algo)

    def test_gradinput_runtime_algorithms(self):
        dtype = 'float32'
        unit_shape = (1,) * self.ndim
        _broadcastable = [False] * (2 + self.ndim)

        def run_gradinput_runtime_algorithm(algo):
            theano.config.dnn.conv.algo_bwd_data = algo
            inputs = theano.tensor.TensorType(dtype, _broadcastable)()
            filters = theano.tensor.TensorType(dtype, _broadcastable)()
            conv = dnn_conv(img=inputs, kerns=filters, algo=algo, precision=dtype,
                            subsample=unit_shape, dilation=unit_shape)
            grad_i = theano.tensor.grad(conv.sum(), [inputs])
            f = theano.function([inputs, filters], grad_i, mode=mode_with_gpu)
            assert 1 == len([node for node in f.maker.fgraph.apply_nodes if isinstance(node.op, GpuDnnConvGradI)])
            assert not any(isinstance(node.op, GpuDnnConv) for node in f.maker.fgraph.apply_nodes)
            assert not any(isinstance(node.op, GpuDnnConvGradW) for node in f.maker.fgraph.apply_nodes)
            if self.ndim == 3:
                flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
            else:
                flipped_filters = filters[:, :, ::-1, ::-1]
            conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(inputs), flipped_filters)
            grad_i_ref = theano.tensor.grad(conv_ref.sum(), [inputs])
            f_ref = theano.function([inputs, filters], grad_i_ref, mode='FAST_RUN')
            runtime_shapes = self.runtime_shapes
            if algo in ('time_once', 'guess_once'):
                runtime_shapes = [list(runtime_shapes[0])]
                runtime_shapes[0][0] = 5
<a name="15"></a>            for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
                print('Shapes:', inputs_shape, filters_shape)
                for i in range(ntimes):
                    inputs_val <font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
                    filters_val = np.random.random(filters_shape).astype(dtype)
                    gpu_res =</b></font> f(inputs_val, filters_val)
                    cpu_res = f_ref(inputs_val, filters_val)
                    utt.assert_allclose(cpu_res, np.asarray(gpu_res))

        for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            yield (run_gradinput_runtime_algorithm, algo)

    def test_gradweight_runtime_algorithms(self):
        dtype = 'float32'
        unit_shape = (1,) * self.ndim
        _broadcastable = [False] * (2 + self.ndim)

        def run_gradweight_runtime_algorithm(algo):
            theano.config.dnn.conv.algo_bwd_filter = algo
            inputs = theano.tensor.TensorType(dtype, _broadcastable)()
            filters = theano.tensor.TensorType(dtype, _broadcastable)()
            conv = dnn_conv(img=inputs, kerns=filters, algo=algo, precision=dtype,
                            subsample=unit_shape, dilation=unit_shape)
            grad_w = theano.tensor.grad(conv.sum(), [filters])
            f = theano.function([inputs, filters], grad_w, mode=mode_with_gpu)
            assert 1 == len([node for node in f.maker.fgraph.apply_nodes if isinstance(node.op, GpuDnnConvGradW)])
            assert not any(isinstance(node.op, GpuDnnConv) for node in f.maker.fgraph.apply_nodes)
            assert not any(isinstance(node.op, GpuDnnConvGradI) for node in f.maker.fgraph.apply_nodes)
            if self.ndim == 3:
                flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
            else:
                flipped_filters = filters[:, :, ::-1, ::-1]
            conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(inputs), flipped_filters)
            grad_w_ref = theano.tensor.grad(conv_ref.sum(), [filters])
            f_ref = theano.function([inputs, filters], grad_w_ref, mode='FAST_RUN')
            runtime_shapes = self.runtime_shapes
            if algo in ('time_once', 'guess_once'):
                runtime_shapes = [list(runtime_shapes[0])]
                runtime_shapes[0][0] = 5
            for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
                print('Shapes:', inputs_shape, filters_shape)
                for i in range(ntimes):
                    inputs_val = np.random.random(inputs_shape).astype(dtype)
                    filters_val = np.random.random(filters_shape).astype(dtype)
                    gpu_res = f(inputs_val, filters_val)
                    cpu_res = f_ref(inputs_val, filters_val)
                    utt.assert_allclose(cpu_res, np.asarray(gpu_res))

        for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
            yield (run_gradweight_runtime_algorithm, algo)


<a name="13"></a>class TestDnnConv2D(BaseTestDnnConv):
    ndim = 2

    fwd_algorithms <font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= cudnn.cudnnConvolutionFwdAlgo_t.get_aliases()
    bwd_filter_algorithms = cudnn.cudnnConvolutionBwdFilterAlgo_t.get_aliases()
    bwd_data_algorithms = cudnn.cudnnConvolutionBwdDataAlgo_t.get_aliases()

    cpu_conv_class =</b></font> CorrMM
    cpu_gradinput_class = CorrMM_gradInputs
    cpu_gradweight_class = CorrMM_gradWeights

    special_cases = [ConvCase.bwd_filter(algo='deterministic', dtype='float32', precision='float32',
                                         inputs_shape=(1, 1, 541211, 10), filters_shape=(50, 1, 3, 10),
                                         border_mode=(1, 0), should_fail=(cudnn.version &lt;= 6)),
                     ConvCase.fwd(algo='small', dtype='float32', precision='float32',
                                  inputs_shape=(65536, 2, 2, 2), filters_shape=(1, 2, 2, 2)),
                     # NB: Due to current workaround (see dnn_fwd.c), this test won't fail for cuDNN &lt; v6100.
                     ConvCase.fwd(algo='small', dtype='float32', precision='float32',
<a name="1"></a>                                  inputs_shape=(65537, 2, 2, 2), filters_shape=(1, 2, 2, 2))]

    runtime_shapes = [
        <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, [(2, 3, 10, 9), (5, 3, 7, 7)]),
        (1, [(1, 1, 100, 200), (1, 1, 50, 200)]),
        (1, [(4, 2, 20, 20), (2, 2, 20, 19)]),
        (3, [(2, 3, 10, 9), (5, 3, 7, 7)]),  # cache should be used
        (1, [(2, 2, 50, 50), (5, 2, 25, 31)]),
        (1</b></font>, [(1, 1, 100, 200), (1, 1, 50, 200)]),  # cache should be used
        (1, [(4, 2, 20, 20), (2, 2, 20, 19)]),  # cache should be used
        (1, [(1, 2, 3, 4), (6, 2, 2, 1)])
    ]


class TestDnnConv3D(BaseTestDnnConv):
    ndim = 3

    fwd_algorithms = cudnn.conv3d_fwd_algorithms
    bwd_filter_algorithms = cudnn.conv3d_bwd_filter_algorithms
    bwd_data_algorithms = cudnn.conv3d_bwd_data_algorithms

    cpu_conv_class = Corr3dMM
    cpu_gradinput_class = Corr3dMM_gradInputs
    cpu_gradweight_class = Corr3dMM_gradWeights

    special_cases = [ConvCase.fwd(algo='small', dtype='float32', precision='float32',
                                  inputs_shape=(65536, 2, 2, 2, 2), filters_shape=(1, 2, 2, 2, 2)),
                     # NB: Due to current workaround (see dnn_fwd.c), this test won't fail for cuDNN &lt; v6100.
                     ConvCase.fwd(algo='small', dtype='float32', precision='float32',
<a name="3"></a>                                  inputs_shape=(65537, 2, 2, 2, 2), filters_shape=(1, 2, 2, 2, 2))]

    runtime_shapes = [
        <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, [(2, 3, 5, 10, 9), (5, 3, 4, 7, 7)]),
<a name="6"></a>        (1, [(1, 1, 5, 100, 200), (1, 1, 4, 50, 200)]),
        (1, [(4, 2, 20, 20, 20), (2, 2, 20, 19, 18)]),
        (3, [(2, 3, 5, 10, 9), (5, 3, 4, 7, 7)]),  # cache should be used
        (1</b></font>, [<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(2, 2, 50, 50, 5), (5, 2, 25, 31, 4)]),
        (1, [(1, 1, 5, 100, 200), (1, 1, 4, 50, 200)]),  # cache should be used
        (1, [(4, 2, 20, 20, 20), (2, 2, 20, 19, 18)]),  # cache should be used
        (1, [(1, 2, 3, 4, 5), (6</b></font>, 2, 3, 2, 1)])
    ]


def test_true_half_config_support():
    # For cuDNN V5.1 and V6.0:
    # "TRUE_HALF_CONFIG is only supported on architectures with true fp16 support (compute capability 5.3 and 6.0)"
    if not check_dtype_config_support('float16', 'float16'):
        raise SkipTest('FWD: TRUE_HALF_CONFIG not supported on this GPU.')


class CheckDnn:
    """
    Utility functions for scripting and infos printing.
    """

    @staticmethod
    def dtype_config_to_str(dtype_config):
        dtype, precision = dtype_config
        if dtype == precision == 'float16':
            return 'TRUE_HALF_CONFIG'
        if dtype == 'float16' and precision == 'float32':
            return 'PSEUDO_HALF_CONFIG'
        if dtype == precision == 'float32':
            return 'FLOAT_CONFIG'
        if dtype == precision == 'float64':
            return 'DOUBLE_CONFIG'
        raise ValueError('unknown data type configuration', dtype_config)

    @staticmethod
    def print_infos(count_tests=True):
        # Print infos about tests and cuDNN supported algorithms and configurations.
        test_2d = TestDnnConv2D()
        test_3d = TestDnnConv3D()
        print()
        print('Available data type configurations:',
              ', '.join(CheckDnn.dtype_config_to_str(d)
                        for d in cudnn.get_supported_dtype_configs(check_dtype_config_support)))
        print()
        print('2D algorithms:')
        print('FWD        :', ', '.join(test_2d.fwd_algorithms))
        print('BWD FILTER :', ', '.join(test_2d.bwd_filter_algorithms))
        print('BWD DATA   :', ', '.join(test_2d.bwd_data_algorithms))
        print()
        print('3D algorithms:')
        print('FWD        :', ', '.join(test_3d.fwd_algorithms))
        print('BWD FILTER :', ', '.join(test_3d.bwd_filter_algorithms))
        print('BWD DATA   :', ', '.join(test_3d.bwd_data_algorithms))
        print()
        if count_tests:
            count_tests_2d = test_2d.get_expected_tcount()
            count_tests_3d = test_3d.get_expected_tcount()
            print(count_tests_2d, 'conv2D test cases.')
            print(count_tests_3d, 'conv3D test cases.')
            print('1 supplementary test.')
            print(count_tests_2d + count_tests_3d + 1, 'total conv tests.')
            print()

    @staticmethod
    def print_tests():
        # Print test cases without running them.
        for test in (TestDnnConv2D(), TestDnnConv3D()):
            for tcase in test.test_fwd():
                print(tcase[0].__name__, *tcase[1:])
            for tcase in test.test_gradinput():
                print(tcase[0].__name__, *tcase[1:])
            for tcase in test.test_gradweight():
                print(tcase[0].__name__, *tcase[1:])
            for tcase in test.test_fwd_runtime_algorithms():
                print(tcase[0].__name__, *tcase[1:])
            for tcase in test.test_gradinput_runtime_algorithms():
                print(tcase[0].__name__, *tcase[1:])
            for tcase in test.test_gradweight_runtime_algorithms():
                print(tcase[0].__name__, *tcase[1:])
        print(test_true_half_config_support.__name__)


if __name__ == '__main__':

    args = sys.argv[1:]
    if len(args) == 1 and args[0] in ('infos', 'list'):
        if args[0] == 'infos':
            CheckDnn.print_infos()
        if args[0] == 'list':
            CheckDnn.print_tests()
    else:
        # We run all tests with nosetests.
        module_name = sys.modules[__name__].__file__
        if len(args) == 0:
            # No args given: run nosetests -vs
            args = ['--verbose', '--nocapture']
        # Else, use given args.
        argv = [sys.argv[0], module_name] + args

        CheckDnn.print_infos()
        nose.main(argv=argv)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerHTML.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
