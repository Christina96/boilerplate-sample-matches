<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_abstract_conv.py &amp; check_dnn_conv.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_abstract_conv.py &amp; check_dnn_conv.py
      </h3>
<h1 align="center">
        7.7%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_abstract_conv.py (5.62056%)<th>check_dnn_conv.py (12.413475%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(377-389)<td><a href="#" name="0">(538-552)</a><td align="center"><font color="#ff0000">23</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1678-1679)<td><a href="#" name="1">(932-937)</a><td align="center"><font color="#f30000">22</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(7-34)<td><a href="#" name="2">(16-44)</a><td align="center"><font color="#d20000">19</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1313-1325)<td><a href="#" name="3">(961-965)</a><td align="center"><font color="#c70000">18</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(1009-1013)<td><a href="#" name="4">(198-202)</a><td align="center"><font color="#c70000">18</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(765-771)<td><a href="#" name="5">(555-557)</a><td align="center"><font color="#b10000">16</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1362-1372)<td><a href="#" name="6">(965-968)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(813-817)<td><a href="#" name="7">(231-237)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(701-706)<td><a href="#" name="8">(664-671)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(442-445)<td><a href="#" name="9">(822-824)</a><td align="center"><font color="#a60000">15</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(923-928)<td><a href="#" name="10">(608-612)</a><td align="center"><font color="#9b0000">14</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(595-600)<td><a href="#" name="11">(223-228)</a><td align="center"><font color="#9b0000">14</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(1675-1676)<td><a href="#" name="12">(117-120)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1507-1511)<td><a href="#" name="13">(914-918)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(794-799)<td><a href="#" name="14">(145-147)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(505-507)<td><a href="#" name="15">(862-864)</a><td align="center"><font color="#900000">13</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(401-411)<td><a href="#" name="16">(804-808)</a><td align="center"><font color="#900000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_abstract_conv.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import unittest
3 import numpy as np
4 <a name="2"></a>from nose.plugins.skip import SkipTest
5 from nose.tools import assert_raises, assert_true
6 <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import theano
7 from theano import tensor
8 from theano import change_flags
9 from theano.gof.opt import check_stack_trace
10 from theano.tests import unittest_tools as utt
11 from theano.tensor.nnet import (corr, corr3d, conv2d_transpose,
12                                 abstract_conv as conv)
13 from theano.tensor.nnet.abstract_conv import (get_conv_output_shape,
14                                               get_conv_gradweights_shape,
15                                               get_conv_gradinputs_shape,
16                                               check_conv_gradinputs_shape,
17                                               assert_conv_shape,
18                                               assert_shape)
19 from theano.tensor.nnet.abstract_conv import AbstractConv2d
20 from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradInputs
21 from theano.tensor.nnet.abstract_conv import AbstractConv2d_gradWeights
22 from theano.tensor.nnet.abstract_conv import bilinear_kernel_1D
23 from theano.tensor.nnet.abstract_conv import bilinear_kernel_2D
24 from theano.tensor.nnet.abstract_conv import bilinear_upsampling
25 from theano.tensor.nnet.abstract_conv import separable_conv2d, separable_conv3d
26 from theano.tensor.nnet.abstract_conv import causal_conv1d
27 from theano.tensor.nnet.corr import (CorrMM, CorrMM_gradWeights,
28                                      CorrMM_gradInputs)
29 from theano.tensor.nnet.corr3d import (Corr3dMM, Corr3dMM_gradWeights,
30                                        Corr3dMM_gradInputs)
31 def conv2d_corr(inputs, filters, border_mode=</b></font>"valid",
32                 subsample=(1, 1), conv_mode='conv',
33                 filter_dilation=(1, 1)):
34     if conv_mode == 'conv':
35         filters = filters[:, :, ::-1, ::-1]
36     return corr.CorrMM(border_mode,
37                        subsample,
38                        filter_dilation)(inputs, filters)
39 def conv2d_corr_gw(inputs, topgrad, filters_shape,
40                    border_mode="valid", subsample=(1, 1),
41                    conv_mode='conv', filter_dilation=(1, 1)):
42     rval = corr.CorrMM_gradWeights(border_mode,
43                                    subsample,
44                                    filter_dilation)(inputs, topgrad,
45                                                     filters_shape[2:])
46     if conv_mode == 'conv':
47         rval = rval[:, :, ::-1, ::-1]
48     return rval
49 def conv2d_corr_gi(filters, topgrad, inputs_shape,
50                    border_mode="valid", subsample=(1, 1),
51                    conv_mode='conv', filter_dilation=(1, 1)):
52     if conv_mode == 'conv':
53         filters = filters[:, :, ::-1, ::-1]
54     return corr.CorrMM_gradInputs(border_mode,
55                                   subsample,
56                                   filter_dilation)(filters,
57                                                    topgrad,
58                                                    inputs_shape[2:])
59 def conv3d_corr(inputs, filters, border_mode="valid",
60                 subsample=(1, 1, 1), conv_mode='conv',
61                 filter_dilation=(1, 1, 1)):
62     if conv_mode == 'conv':
63         filters = filters[:, :, ::-1, ::-1, ::-1]
64     return corr3d.Corr3dMM(border_mode,
65                            subsample,
66                            filter_dilation)(inputs, filters)
67 def conv3d_corr_gw(inputs, topgrad, filters_shape,
68                    border_mode="valid", subsample=(1, 1, 1),
69                    conv_mode='conv', filter_dilation=(1, 1, 1)):
70     rval = corr3d.Corr3dMM_gradWeights(border_mode,
71                                        subsample,
72                                        filter_dilation)(inputs, topgrad,
73                                                         filters_shape[2:])
74     if conv_mode == 'conv':
75         rval = rval[:, :, ::-1, ::-1, ::-1]
76     return rval
77 def conv3d_corr_gi(filters, topgrad, inputs_shape,
78                    border_mode="valid", subsample=(1, 1, 1),
79                    conv_mode='conv', filter_dilation=(1, 1, 1)):
80     if conv_mode == 'conv':
81         filters = filters[:, :, ::-1, ::-1, ::-1]
82     return corr3d.Corr3dMM_gradInputs(border_mode,
83                                       subsample,
84                                       filter_dilation)(filters,
85                                                        topgrad,
86                                                        inputs_shape[2:])
87 class TestGetConvOutShape(unittest.TestCase):
88     def test_basic(self):
89         image_shape, kernel_shape = (3, 2, 12, 9), (4, 2, 5, 6)
90         sub_sample = (1, 2)
91         filter_dilation = (2, 1)
92         test1_params = get_conv_output_shape(
93             image_shape, kernel_shape, 'valid', sub_sample, filter_dilation)
94         test2_params = get_conv_output_shape(
95             image_shape, kernel_shape, 'half', sub_sample, filter_dilation)
96         test3_params = get_conv_output_shape(
97             image_shape, kernel_shape, 'full', sub_sample, filter_dilation)
98         test4_params = get_conv_output_shape(
99             image_shape, kernel_shape, (1, 2), sub_sample, filter_dilation)
100         self.assertTrue(test1_params == (3, 4, 4, 2))
101         self.assertTrue(test2_params == (3, 4, 12, 5))
102         self.assertTrue(test3_params == (3, 4, 20, 7))
103         self.assertTrue(test4_params == (3, 4, 6, 4))
104     def test_basic_3d(self):
105         image_shape, kernel_shape = (3, 2, 12, 9, 7), (4, 2, 5, 6, 4)
106         sub_sample = (1, 2, 1)
107         filter_dilation = (2, 1, 1)
108         test1_params = get_conv_output_shape(
109             image_shape, kernel_shape, 'valid', sub_sample, filter_dilation)
110         test2_params = get_conv_output_shape(
111             image_shape, kernel_shape, 'half', sub_sample, filter_dilation)
112         test3_params = get_conv_output_shape(
113             image_shape, kernel_shape, 'full', sub_sample, filter_dilation)
114         test4_params = get_conv_output_shape(
115             image_shape, kernel_shape, (1, 2, 3), sub_sample, filter_dilation)
116         self.assertTrue(test1_params == (3, 4, 4, 2, 4))
117         self.assertTrue(test2_params == (3, 4, 12, 5, 8))
118         self.assertTrue(test3_params == (3, 4, 20, 7, 10))
119         self.assertTrue(test4_params == (3, 4, 6, 4, 10))
120 class TestConvGradInputsShape(unittest.TestCase):
121     def test_check_shape(self):
122         for i in range(1, 20):
123             for k in range(1, 10):
124                 for b in ('valid', 'half', 'full', (0, 2)):
125                     for s in (1, 2, 3):
126                         for d in (1, 2, 3):
127                             image_shape = (59, 61, i, i)
128                             kernel_shape = (67, 61, k, k)
129                             computed_shape = get_conv_output_shape(
130                                 image_shape, kernel_shape, b, (s, s), (d, d))
131                             self.assertTrue(check_conv_gradinputs_shape(
132                                 image_shape, kernel_shape, computed_shape, b, (s, s), (d, d)))
133                             trial_shape = (None, None, computed_shape[2], None)
134                             self.assertTrue(check_conv_gradinputs_shape(
135                                 image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))
136                             trial_shape = (1, 1, computed_shape[2], computed_shape[3])
137                             self.assertFalse(check_conv_gradinputs_shape(
138                                 image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))
139                             for o in (-3, -2, -1, 1, 2, 3):
140                                 trial_shape = (computed_shape[0], computed_shape[1],
141                                                computed_shape[2] + o, computed_shape[3] + o)
142                                 self.assertFalse(check_conv_gradinputs_shape(
143                                     image_shape, kernel_shape, trial_shape, b, (s, s), (d, d)))
144     def test_get_shape(self):
145         for i in range(1, 20):
146             for k in range(1, 10):
147                 for b in ('valid', 'half', 'full', (0, 2)):
148                     for d in (1, 2, 3):
149                         image_shape = (59, 61, i, i)
150                         kernel_shape = (67, 61, k, k)
151                         output_shape = get_conv_output_shape(
152                             image_shape, kernel_shape, b, (1, 1), (d, d))
153                         computed_image_shape = get_conv_gradinputs_shape(
154                             kernel_shape, output_shape, b, (1, 1), (d, d))
155                         self.assertEqual(computed_image_shape, image_shape)
156                         computed_image_shape = get_conv_gradinputs_shape(
157                             kernel_shape, output_shape, b, (2, 3), (d, d))
158                         image_shape_with_None = image_shape[:2] + (None, None)
159                         self.assertEqual(computed_image_shape, image_shape_with_None)
160                         computed_kernel_shape = get_conv_gradweights_shape(
161                             image_shape, output_shape, b, (1, 1), (d, d))
162                         if b == 'half':
163                             kernel_shape_with_None = kernel_shape[:2] + (None, None)
164                             self.assertEqual(computed_kernel_shape, kernel_shape_with_None)
165                         else:
166                             self.assertEqual(computed_kernel_shape, kernel_shape)
167                         computed_kernel_shape = get_conv_gradweights_shape(
168                             kernel_shape, output_shape, b, (2, 3), (d, d))
169                         kernel_shape_with_None = kernel_shape[:2] + (None, None)
170                         self.assertEqual(computed_kernel_shape, kernel_shape_with_None)
171 class TestAssertConvShape(unittest.TestCase):
172     def test_basic(self):
173         shape = tuple(tensor.iscalar() for i in range(4))
174         f = theano.function(shape, assert_conv_shape(shape))
175         self.assertEqual([1, 2, 3, 4], f(1, 2, 3, 4))
176         self.assertEqual([0, 0, 1, 1], f(0, 0, 1, 1))
177         assert_raises(AssertionError, f, 3, 3, 3, 0)
178         assert_raises(AssertionError, f, 3, 3, 0, 3)
179         assert_raises(AssertionError, f, 3, 3, -1, 3)
180         assert_raises(AssertionError, f, 3, -1, 3, 3)
181         assert_raises(AssertionError, f, -1, 3, 3, 3)
182 class TestAssertShape(unittest.TestCase):
183     @change_flags([("conv.assert_shape", True)])
184     def test_basic(self):
185         x = tensor.tensor4()
186         s1 = tensor.iscalar()
187         s2 = tensor.iscalar()
188         expected_shape = [None, s1, s2, None]
189         f = theano.function([x, s1, s2], assert_shape(x, expected_shape))
190         v = np.zeros((3, 5, 7, 11), dtype='float32')
191         self.assertEqual(0, np.sum(f(v, 5, 7)))
192         assert_raises(AssertionError, f, v, 5, 0)
193         assert_raises(AssertionError, f, v, 5, 9)
194         assert_raises(AssertionError, f, v, 0, 7)
195         assert_raises(AssertionError, f, v, 7, 7)
196     @change_flags([("conv.assert_shape", True)])
197     def test_shape_check_conv2d(self):
198         input = tensor.tensor4()
199         filters = tensor.tensor4()
200         out = conv.conv2d(input, filters,
201                           input_shape=(3, 5, 7, 11),
202                           filter_shape=(7, 5, 3, 3))
203         f = theano.function([input, filters], out)
204         assert_raises(AssertionError, f,
205                       np.zeros((3, 5, 9, 11), dtype='float32'),
206                       np.zeros((7, 5, 3, 3), dtype='float32'))
207         assert_raises(AssertionError, f,
208                       np.zeros((3, 5, 7, 11), dtype='float32'),
209                       np.zeros((7, 5, 2, 2), dtype='float32'))
210     @change_flags([("conv.assert_shape", True)])
211     def test_shape_check_conv3d(self):
212         if theano.config.cxx == "":
213             raise SkipTest("test needs cxx")
214         input = tensor.tensor5()
215         filters = tensor.tensor5()
216         out = conv.conv3d(input, filters,
217                           input_shape=(3, 5, 7, 11, 13),
218                           filter_shape=(7, 5, 3, 3, 3))
219         f = theano.function([input, filters], out)
220         assert_raises(AssertionError, f,
221                       np.zeros((3, 5, 9, 11, 13), dtype='float32'),
222                       np.zeros((7, 5, 3, 3, 3), dtype='float32'))
223         assert_raises(AssertionError, f,
224                       np.zeros((3, 5, 7, 11, 13), dtype='float32'),
225                       np.zeros((7, 5, 2, 2, 2), dtype='float32'))
226     @change_flags([("conv.assert_shape", True)])
227     def test_shape_check_conv2d_grad_wrt_inputs(self):
228         output_grad = tensor.tensor4()
229         filters = tensor.tensor4()
230         out = conv.conv2d_grad_wrt_inputs(output_grad, filters,
231                                           input_shape=(None, None, 7, 11),
232                                           filter_shape=(7, 5, 3, 3))
233         f = theano.function([output_grad, filters], out)
234         assert_raises(AssertionError, f,
235                       np.zeros((3, 6, 5, 9), dtype='float32'),
236                       np.zeros((7, 6, 3, 3), dtype='float32'))
237     @change_flags([("conv.assert_shape", True)])
238     def test_shape_check_conv3d_grad_wrt_inputs(self):
239         if theano.config.cxx == "":
240             raise SkipTest("test needs cxx")
241         output_grad = tensor.tensor5()
242         filters = tensor.tensor5()
243         out = conv.conv3d_grad_wrt_inputs(output_grad, filters,
244                                           input_shape=(None, None, 7, 11, 13),
245                                           filter_shape=(7, 5, 3, 3, 3))
246         f = theano.function([output_grad, filters], out)
247         assert_raises(AssertionError, f,
248                       np.zeros((3, 6, 5, 9, 11), dtype='float32'),
249                       np.zeros((7, 6, 3, 3, 3), dtype='float32'))
250     @change_flags([("conv.assert_shape", True)])
251     def test_shape_check_conv2d_grad_wrt_weights(self):
252         input = tensor.tensor4()
253         output_grad = tensor.tensor4()
254         out = conv.conv2d_grad_wrt_weights(input, output_grad,
255                                            filter_shape=(None, None, 3, 3),
256                                            input_shape=(3, 5, 7, 11))
257         f = theano.function([input, output_grad], out)
258         assert_raises(AssertionError, f,
259                       np.zeros((3, 6, 7, 11), dtype='float32'),
260                       np.zeros((3, 7, 5, 9), dtype='float32'))
261     @change_flags([("conv.assert_shape", True)])
262     def test_shape_check_conv3d_grad_wrt_weights(self):
263         if theano.config.cxx == "":
264             raise SkipTest("test needs cxx")
265         input = tensor.tensor5()
266         output_grad = tensor.tensor5()
267         out = conv.conv3d_grad_wrt_weights(input, output_grad,
268                                            filter_shape=(None, None, 3, 3, 3),
269                                            input_shape=(3, 5, 7, 11, 13))
270         f = theano.function([input, output_grad], out)
271         assert_raises(AssertionError, f,
272                       np.zeros((3, 6, 7, 11, 13), dtype='float32'),
273                       np.zeros((3, 7, 5, 9, 11), dtype='float32'))
274 class BaseTestConv(object):
275     def get_output_shape(self, inputs_shape, filters_shape,
276                          subsample, border_mode, filter_dilation):
277         dil_filters = tuple((s - 1) * d + 1 for s, d in zip(filters_shape[2:],
278                                                             filter_dilation))
279         if border_mode == "valid":
280             border_mode = (0,) * (len(inputs_shape) - 2)
281         if border_mode == "half":
282             border_mode = tuple(d // 2 for d in dil_filters)
283         if border_mode == "full":
284             border_mode = tuple(d - 1 for d in dil_filters)
285         batch_size = inputs_shape[0]
286         num_filters = filters_shape[0]
287         return ((batch_size, num_filters,) +
288                 tuple(None if i is None or k is None
289                       else ((i + 2 * pad - ((k - 1) * fd + 1)) // d + 1)
290                       for i, k, d, pad, fd in zip(inputs_shape[2:],
291                                                   filters_shape[2:],
292                                                   subsample, border_mode,
293                                                   filter_dilation)))
294     def run_fwd(self, inputs_shape, filters_shape,
295                 conv_fn, conv_op, ref,
296                 subsample=None, verify_grad=True, mode=None,
297                 border_mode='valid', filter_flip=True,
298                 provide_shape=False, target_op=None,
299                 check_trace=False, filter_dilation=None):
300         if subsample is None:
301             subsample = (1,) * (len(inputs_shape) - 2)
302 <a name="0"></a>        if filter_dilation is None:
303             filter_dilation = (1,) * (len(inputs_shape) - 2)
304         inputs_val <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(inputs_shape).astype('float32')
305         filters_val = np.random.random(filters_shape).astype('float32')
306         inputs_val /= 10
307         filters_val /= 10
308         inputs = self.shared(inputs_val)
309         filters = self.shared(filters_val)
310         if provide_shape:
311             imshp = inputs_shape
312             kshp =</b></font> filters_shape
313         else:
314             imshp = None
315             kshp = None
316         if filter_flip:
317             conv_mode = 'conv'
318         else:
319             conv_mode = 'cross'
320 <a name="16"></a>        c_ref = ref(inputs, filters,
321                     border_mode=border_mode,
322                     subsample=subsample,
323                     conv_mode<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=conv_mode,
324                     filter_dilation=filter_dilation)
325         c = conv_fn(inputs, filters,
326                     border_mode=border_mode,
327                     subsample=subsample,
328                     filter_flip=filter_flip,
329                     input_shape=imshp,
330                     filter_shape=kshp,
331                     filter_dilation=filter_dilation)
332         f_ref = theano.function(</b></font>[], c_ref, mode='FAST_RUN')
333         f = theano.function([], c, mode=mode)
334         if target_op is not None:
335             assert any([isinstance(n.op, target_op) for n
336                         in f.maker.fgraph.toposort()])
337             if check_trace:
338                 assert_true(check_stack_trace(f, ops_to_check=target_op))
339         res_ref = np.array(f_ref())
340         res = np.array(f())
341         utt.assert_allclose(res_ref, res)
342         if verify_grad and inputs_val.size &gt; 0 and filters_val.size &gt; 0 and res.size &gt; 0:
343             utt.verify_grad(conv_op(border_mode=border_mode,
344                                     imshp=imshp, kshp=kshp,
345                                     subsample=subsample,
346                                     filter_dilation=filter_dilation),
347                             [inputs_val, filters_val],
348                             mode=mode)
349     def run_gradweight(self, inputs_shape, filters_shape, output_shape,
350                        gradWeights_fn, ref, subsample=None,
351                        filter_flip=True, verify_grad=True, mode=None,
352                        border_mode='valid', provide_shape=False,
353                        target_op=None, check_trace=False,
354                        filter_dilation=None):
355         if subsample is None:
356             subsample = (1,) * (len(inputs_shape) - 2)
357 <a name="9"></a>        if filter_dilation is None:
358             filter_dilation = (1,) * (len(inputs_shape) - 2)
359         inputs_val <font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(inputs_shape).astype('float32')
360         output_val = np.random.random(output_shape).astype('float32')
361         inputs = self.shared(</b></font>inputs_val)
362         output = self.shared(output_val)
363         if provide_shape:
364             imshp = inputs_shape
365             kshp = filters_shape
366         else:
367             imshp = None
368             kshp = None
369         if filter_flip:
370             conv_mode = 'conv'
371         else:
372             conv_mode = 'cross'
373         c = gradWeights_fn(border_mode=border_mode,
374                            filter_flip=filter_flip,
375                            subsample=subsample,
376                            imshp=imshp, kshp=kshp,
377                            filter_dilation=filter_dilation)
378         c = c(inputs, output, filters_shape[2:])
379         c_ref = ref(inputs, output,
380                     filters_shape,
381                     border_mode=border_mode,
382                     subsample=subsample,
383                     conv_mode=conv_mode,
384                     filter_dilation=filter_dilation)
385         f = theano.function([], c, mode=mode)
386         f_ref = theano.function([], c_ref, mode='FAST_RUN')
387         if target_op is not None:
388             assert any([isinstance(n.op, target_op) for n
389                         in f.maker.fgraph.toposort()])
390             if check_trace:
391                 assert_true(check_stack_trace(f, ops_to_check=target_op))
392         res_ref = np.array(f_ref())
393         res = np.array(f())
394         utt.assert_allclose(res_ref, res)
395         def abstract_conv_gradweight(inputs_val, output_val):
396             conv_op = gradWeights_fn(border_mode=border_mode,
397                                      subsample=subsample,
398                                      filter_dilation=filter_dilation)
399             return conv_op(inputs_val, output_val, filters_shape[2:])
400         if verify_grad and inputs_val.size &gt; 0 and output_val.size &gt; 0 and res.size &gt; 0:
401             utt.verify_grad(abstract_conv_gradweight,
402                             [inputs_val, output_val],
403                             mode=mode, eps=1)
404     def run_gradinput(self, inputs_shape, filters_shape, output_shape,
405                       gradInputs_fn, ref,
406                       subsample=None, filter_flip=True,
407                       verify_grad=True, mode=None, border_mode='valid',
408                       provide_shape=False, target_op=None,
409                       check_trace=False, filter_dilation=None):
410         if subsample is None:
411             subsample = (1,) * (len(inputs_shape) - 2)
412 <a name="15"></a>        if filter_dilation is None:
413             filter_dilation = (1,) * (len(inputs_shape) - 2)
414         output_val <font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.random.random(output_shape).astype('float32')
415         filters_val = np.random.random(filters_shape).astype('float32')
416         output =</b></font> self.shared(output_val)
417         filters = self.shared(filters_val)
418         if provide_shape:
419             imshp = inputs_shape
420             kshp = filters_shape
421         else:
422             imshp = None
423             kshp = None
424         if filter_flip:
425             conv_mode = 'conv'
426         else:
427             conv_mode = 'cross'
428         c = gradInputs_fn(border_mode=border_mode,
429                           subsample=subsample,
430                           filter_flip=filter_flip,
431                           imshp=imshp, kshp=kshp,
432                           filter_dilation=filter_dilation)
433         c = c(filters, output, inputs_shape[2:])
434         f = theano.function([], c, mode=mode)
435         if ref is not None:
436             c_ref = ref(filters, output, inputs_shape,
437                         border_mode=border_mode, subsample=subsample,
438                         conv_mode=conv_mode, filter_dilation=filter_dilation)
439             f_ref = theano.function([], c_ref, mode='FAST_RUN')
440         if target_op is not None:
441             assert any([isinstance(n.op, target_op) for n
442                         in f.maker.fgraph.toposort()])
443             if check_trace:
444                 assert_true(check_stack_trace(f, ops_to_check=target_op))
445         res = np.array(f())
446         if ref is not None:
447             res_ref = np.array(f_ref())
448             utt.assert_allclose(res_ref, res)
449         def abstract_conv_gradinputs(filters_val, output_val):
450             conv_op = gradInputs_fn(border_mode=border_mode,
451                                     subsample=subsample,
452                                     filter_dilation=filter_dilation)
453             return conv_op(filters_val, output_val, inputs_shape[2:])
454         if verify_grad and filters_val.size &gt; 0 and output_val.size &gt; 0 and res.size &gt; 0:
455             utt.verify_grad(abstract_conv_gradinputs,
456                             [filters_val, output_val],
457                             mode=mode, eps=1)
458     def test_all(self):
459         if type(self) is BaseTestConv:
460             raise SkipTest("base class")
461         ds = self.default_subsamples
462         db = self.default_border_mode
463         dflip = self.default_filter_flip
464         dprovide_shape = self.default_provide_shape
465         for (i, f) in zip(self.inputs_shapes, self.filters_shapes):
466             for provide_shape in self.provide_shape:
467                 yield (self.tcase, i, f, ds, db, dflip, provide_shape)
468             if min(i) &gt; 0 and min(f) &gt; 0:
469                 for fd in self.filters_dilations:
470                     for s in self.subsamples:
471                         for b in self.border_modes:
472                             yield (self.tcase, i, f, s, b, dflip,
473                                    dprovide_shape, fd)
474                 for flip in self.filter_flip:
475                     yield (self.tcase, i, f, ds, db, flip, dprovide_shape)
476 class BaseTestConv2d(BaseTestConv):
477     @classmethod
478     def setup_class(cls):
479         cls.inputs_shapes = [(8, 1, 6, 6), (8, 1, 8, 8), (2, 1, 7, 7),
480                              (6, 1, 10, 11), (2, 1, 6, 5), (1, 5, 9, 9),
481                              (0, 1, 6, 6), (1, 0, 6, 6), (1, 1, 6, 6)]
482         cls.filters_shapes = [(5, 1, 2, 2), (4, 1, 3, 3), (2, 1, 3, 3),
483                               (1, 1, 2, 3), (4, 1, 1, 3), (4, 5, 3, 2),
484                               (1, 1, 2, 2), (1, 0, 2, 2), (0, 1, 2, 2)]
485         cls.subsamples = [(1, 1), (2, 2), (2, 4)]
486         cls.default_subsamples = (1, 1)
487 <a name="11"></a>        cls.filters_dilations = [(1, 1), (1, 2), (2, 1)]
488         cls.default_filters_dilations = (1, 1)
489         cls.border_modes = ["valid", "half", "full", (0, 0), (1, 1), (5, 5), (5, 2)]
490         cls.default_border_mode <font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= (0, 0)
491         cls.filter_flip = [True, False]
492         cls.default_filter_flip = True
493         cls.provide_shape = [True, False]
494         cls.default_provide_shape = True
495         cls.shared =</b></font> staticmethod(theano.compile.shared)
496     def test_gradinput_arbitrary_output_shapes(self):
497         input_shape = (2, 1, 7, 7)
498         filter_shape = (2, 1, 3, 3)
499         for output_shape in [(2, 2, 8, 8), (2, 2, 9, 9), (2, 2, 12, 12)]:
500             for border_mode in ["valid", "half", "full"]:
501                 computed_shape = get_conv_output_shape(
502                     input_shape, filter_shape, border_mode, self.default_subsamples, self.default_filters_dilations)
503                 if tuple(computed_shape) == output_shape:
504                     yield (self.tcase_gi,
505                            input_shape,
506                            filter_shape,
507                            output_shape,
508                            self.default_subsamples,
509                            border_mode,
510                            True,
511                            True,
512                            self.default_filters_dilations,
513                            False)
514                 else:
515                     yield (self.tcase_gi,
516                            input_shape,
517                            filter_shape,
518                            output_shape,
519                            self.default_subsamples,
520                            border_mode,
521                            True,
522                            True,
523                            self.default_filters_dilations,
524                            True)
525     def test_gradinput_impossible_output_shapes(self):
526         def run_for_output_offsets(image_shape, kernel_shape, s, border_mode, d):
527             for o in (-3, -1, 1, 2):
528                 output_shape = (1, 1, computed_shape[2] + o, computed_shape[3] + o)
529                 self.tcase_gi(image_shape, kernel_shape, output_shape,
530                               (s, s), border_mode, True, True, (d, d), True)
531         for (i, k) in ((1, 1), (1, 2), (2, 1), (4, 2), (4, 3), (7, 3), (9, 5)):
532             for border_mode in ('valid', 'half', 'full', (0, 2)):
533                 for (s, d) in ((1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (1, 3)):
534                     image_shape = (1, 1, i, i)
535                     kernel_shape = (1, 1, k, k)
536                     computed_shape = get_conv_output_shape(
537                         image_shape, kernel_shape, border_mode, (s, s), (d, d))
538                     yield (run_for_output_offsets,
539                            image_shape, kernel_shape, s, border_mode, d)
540     def run_fwd(self, inputs_shape, filters_shape,
541                 conv_fn=conv.conv2d, conv_op=conv.AbstractConv2d,
542                 ref=conv2d_corr, **kwargs):
543         super(BaseTestConv2d, self).run_fwd(
544             inputs_shape=inputs_shape,
545             filters_shape=filters_shape,
546             conv_fn=conv_fn,
547             conv_op=conv_op,
548             ref=ref, **kwargs)
549     def run_gradweight(self, inputs_shape, filters_shape, output_shape,
550                        gradWeights_fn=conv.AbstractConv2d_gradWeights,
551                        ref=conv2d_corr_gw, **kwargs):
552         super(BaseTestConv2d, self).run_gradweight(
553             inputs_shape=inputs_shape,
554             filters_shape=filters_shape,
555             output_shape=output_shape,
556             gradWeights_fn=gradWeights_fn,
557             ref=ref, **kwargs)
558     def run_gradinput(self, inputs_shape, filters_shape, output_shape,
559                       gradInputs_fn=conv.AbstractConv2d_gradInputs,
560                       ref=conv2d_corr_gi, **kwargs):
561         super(BaseTestConv2d, self).run_gradinput(
562             inputs_shape=inputs_shape,
563             filters_shape=filters_shape,
564             output_shape=output_shape,
565             gradInputs_fn=gradInputs_fn,
566             ref=ref, **kwargs)
567 class TestCorrConv2d(BaseTestConv2d):
568     @classmethod
569     def setup_class(cls):
570         BaseTestConv2d.setup_class()
571     def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1)):
572         o = self.get_output_shape(i, f, s, b, fd)
573 <a name="8"></a>        if (not theano.config.cxx or
574                 theano.config.mode == "FAST_COMPILE"):
575             raise SkipTest("Need blas to test conv2d")
576         self.run_fwd<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f, subsample=s,
577                      verify_grad=True, provide_shape=provide_shape,
578                      border_mode=b, filter_flip=flip,
579                      target_op=CorrMM, check_trace=True,
580                      filter_dilation=fd)
581         self.run_gradweight(inputs_shape=i, filters_shape=</b></font>f,
582                             output_shape=o, subsample=s, verify_grad=True,
583                             provide_shape=provide_shape, border_mode=b,
584                             filter_flip=flip, target_op=CorrMM_gradWeights,
585                             check_trace=True, filter_dilation=fd)
586         self.run_gradinput(inputs_shape=i, filters_shape=f,
587                            output_shape=o, subsample=s, verify_grad=True,
588                            provide_shape=provide_shape, border_mode=b,
589                            filter_flip=flip, target_op=CorrMM_gradInputs,
590                            check_trace=True, filter_dilation=fd)
591     def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1), expect_error=False):
592         if (not theano.config.cxx or
593                 theano.config.mode == "FAST_COMPILE"):
594             raise SkipTest("Need blas to test conv2d")
595         if not expect_error:
596             self.run_gradinput(inputs_shape=i, filters_shape=f,
597                                output_shape=o, subsample=s, verify_grad=True,
598                                provide_shape=provide_shape, border_mode=b,
599                                filter_flip=flip, target_op=CorrMM_gradInputs,
600                                check_trace=True, filter_dilation=fd)
601         else:
602             assert_raises(ValueError,
603                           self.run_gradinput,
604                           inputs_shape=i, filters_shape=f,
605                           output_shape=o, subsample=s, verify_grad=False,
606                           provide_shape=provide_shape, border_mode=b,
607                           filter_flip=flip, target_op=CorrMM_gradInputs,
608                           ref=None, check_trace=True, filter_dilation=fd)
609 class TestAbstractConvNoOptim(BaseTestConv2d):
610     @classmethod
611     def setup_class(cls):
612         BaseTestConv2d.setup_class()
613         cls.inputs_shapes = [(8, 1, 6, 6)]
614         cls.filters_shapes = [(5, 1, 2, 2)]
615         cls.subsamples = [(1, 1), (2, 2)]
616         cls.filters_dilations = [(1, 1), (1, 2), (2, 1)]
617         cls.border_modes = ["valid", "half", "full"]
618         cls.filter_flip = [True]
619         cls.provide_shape = [False]
620         if not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
621             raise SkipTest("SciPy needed")
622     def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1)):
623         o = self.get_output_shape(i, f, s, b, fd)
624         mode = theano.Mode(optimizer=None)
625         if not theano.config.cxx:
626             raise SkipTest("Need cxx to test conv2d")
627         self.run_fwd(inputs_shape=i, filters_shape=f, subsample=s,
628                      verify_grad=True, provide_shape=provide_shape,
629 <a name="5"></a>                     border_mode=b, filter_flip=flip,
630                      target_op=None, check_trace=True,
631                      filter_dilation=fd, mode=mode)
632         self.run_gradweight<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f,
633                             output_shape=o, subsample=s, verify_grad=True,
634                             provide_shape=provide_shape, border_mode=b,
635                             filter_flip=flip, target_op=None,
636                             check_trace=True, filter_dilation=fd,
637                             mode=mode)
638         self.run_gradinput(inputs_shape=</b></font>i, filters_shape=f,
639                            output_shape=o, subsample=s, verify_grad=True,
640                            provide_shape=provide_shape, border_mode=b,
641                            filter_flip=flip, target_op=None,
642                            check_trace=True, filter_dilation=fd,
643                            mode=mode)
644     def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1), expect_error=False):
645         if not theano.config.cxx:
646             raise SkipTest("Need cxx to test conv2d")
647         mode = theano.Mode(optimizer=None)
648         if not expect_error:
649             self.run_gradinput(inputs_shape=i, filters_shape=f,
650                                output_shape=o, subsample=s, verify_grad=True,
651                                provide_shape=provide_shape, border_mode=b,
652                                filter_flip=flip, target_op=None,
653                                check_trace=True, filter_dilation=fd,
654                                mode=mode)
655 <a name="14"></a>        else:
656             assert_raises(ValueError,
657                           self.run_gradinput,
658                           inputs_shape<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=i, filters_shape=f,
659                           output_shape=o, subsample=s, verify_grad=False,
660                           provide_shape=provide_shape, border_mode=b,
661                           filter_flip=flip, target_op=None,
662                           check_trace=True, filter_dilation=fd,
663                           ref=None, mode=</b></font>mode)
664 class BaseTestConv3d(BaseTestConv):
665     @classmethod
666     def setup_class(cls):
667         cls.inputs_shapes = [(2, 1, 5, 5, 5), (1, 2, 7, 5, 6),
668                              (0, 1, 5, 5, 5), (1, 0, 5, 5, 5), (1, 1, 5, 5, 5)]
669         cls.filters_shapes = [(2, 1, 2, 2, 2), (1, 2, 2, 1, 3),
670                               (1, 1, 2, 2, 2), (1, 0, 2, 2, 2), (0, 1, 2, 2, 2)]
671 <a name="7"></a>        cls.subsamples = [(1, 1, 1), (2, 2, 2), (1, 2, 3)]
672         cls.default_subsamples = (1, 1, 1)
673         cls.filters_dilations = [(1, 1, 1), (1, 2, 1), (2, 1, 2)]
674         cls<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.default_filters_dilations = (1, 1, 1)
675         cls.border_modes = ["valid", "half", "full", (0, 0, 0), (2, 2, 3)]
676         cls.default_border_mode = (0, 0, 0)
677         cls.filter_flip = [True, False]
678         cls.</b></font>default_filter_flip = True
679         cls.provide_shape = [True, False]
680         cls.default_provide_shape = True
681         cls.shared = staticmethod(theano.compile.shared)
682     def test_gradinput_arbitrary_output_shapes(self):
683         input_shape = (2, 1, 7, 7, 7)
684         filter_shape = (1, 1, 3, 3, 3)
685         for output_shape in [(2, 1, 8, 8, 8), (2, 1, 9, 9, 9), (2, 1, 12, 12, 12)]:
686             for border_mode in ["valid", "half", "full"]:
687                 computed_shape = get_conv_output_shape(
688                     input_shape, filter_shape, border_mode, self.default_subsamples, self.default_filters_dilations)
689                 if tuple(computed_shape) == output_shape:
690                     yield (self.tcase_gi,
691                            input_shape,
692                            filter_shape,
693                            output_shape,
694                            self.default_subsamples,
695                            border_mode,
696                            True,
697                            True,
698                            self.default_filters_dilations,
699                            False)
700                 else:
701                     yield (self.tcase_gi,
702                            input_shape,
703                            filter_shape,
704                            output_shape,
705                            self.default_subsamples,
706                            border_mode,
707                            True,
708                            True,
709                            self.default_filters_dilations,
710                            True)
711     def test_gradinput_impossible_output_shapes(self):
712         def run_for_output_offsets(image_shape, kernel_shape, s, border_mode, d):
713             for o in (-3, -1, 1, 2):
714                 output_shape = (1, 1, computed_shape[2] + o,
715                                 computed_shape[3] + o, computed_shape[4] + o)
716                 self.tcase_gi(image_shape, kernel_shape, output_shape,
717                               (s, s), border_mode, True, True, (d, d), True)
718         for (i, k) in ((1, 1), (1, 2), (2, 1), (4, 2), (4, 3), (7, 3), (9, 5)):
719             for border_mode in ('valid', 'half', 'full', (0, 2, 1)):
720                 for (s, d) in ((1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (1, 3)):
721                     image_shape = (1, 1, i, i, i)
722                     kernel_shape = (1, 1, k, k, k)
723                     computed_shape = get_conv_output_shape(
724                         image_shape, kernel_shape, border_mode, (s, s, s), (d, d, d))
725                     yield (run_for_output_offsets,
726                            image_shape, kernel_shape, s, border_mode, d)
727     def run_fwd(self, inputs_shape, filters_shape,
728                 conv_fn=conv.conv3d, conv_op=conv.AbstractConv3d,
729                 ref=conv3d_corr, **kwargs):
730         super(BaseTestConv3d, self).run_fwd(
731             inputs_shape=inputs_shape,
732             filters_shape=filters_shape,
733             conv_fn=conv_fn,
734             conv_op=conv_op,
735             ref=ref, **kwargs)
736     def run_gradweight(self, inputs_shape, filters_shape, output_shape,
737                        gradWeights_fn=conv.AbstractConv3d_gradWeights,
738                        ref=conv3d_corr_gw, **kwargs):
739         super(BaseTestConv3d, self).run_gradweight(
740             inputs_shape=inputs_shape,
741             filters_shape=filters_shape,
742             output_shape=output_shape,
743             gradWeights_fn=gradWeights_fn,
744             ref=ref, **kwargs)
745     def run_gradinput(self, inputs_shape, filters_shape, output_shape,
746                       gradInputs_fn=conv.AbstractConv3d_gradInputs,
747                       ref=conv3d_corr_gi, **kwargs):
748         super(BaseTestConv3d, self).run_gradinput(
749             inputs_shape=inputs_shape,
750             filters_shape=filters_shape,
751             output_shape=output_shape,
752             gradInputs_fn=gradInputs_fn,
753             ref=ref, **kwargs)
754 class TestCorrConv3d(BaseTestConv3d):
755     @classmethod
756     def setup_class(cls):
757         BaseTestConv3d.setup_class()
758     def tcase(self, i, f, s, b, flip, provide_shape, fd=(1, 1, 1)):
759         o = self.get_output_shape(i, f, s, b, fd)
760 <a name="10"></a>        if (not theano.config.cxx or
761                 theano.config.mode == "FAST_COMPILE"):
762             raise SkipTest("Need blas to test conv3d")
763         self.run_fwd<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(inputs_shape=i, filters_shape=f, subsample=s,
764                      verify_grad=True, provide_shape=provide_shape,
765                      border_mode=b, filter_flip=flip,
766                      target_op=Corr3dMM, check_trace=True,
767                      filter_dilation=fd)
768         self.run_gradweight(inputs_shape=</b></font>i, filters_shape=f,
769                             output_shape=o, subsample=s, verify_grad=True,
770                             provide_shape=provide_shape, border_mode=b,
771                             filter_flip=flip, target_op=Corr3dMM_gradWeights,
772                             check_trace=True, filter_dilation=fd)
773         self.run_gradinput(inputs_shape=i, filters_shape=f,
774                            output_shape=o, subsample=s, verify_grad=True,
775                            provide_shape=provide_shape, border_mode=b,
776                            filter_flip=flip, target_op=Corr3dMM_gradInputs,
777                            check_trace=True, filter_dilation=fd)
778     def tcase_gi(self, i, f, o, s, b, flip, provide_shape, fd=(1, 1, 1), expect_error=False):
779         if (not theano.config.cxx or
780                 theano.config.mode == "FAST_COMPILE"):
781             raise SkipTest("Need blas to test conv3d")
782         if not expect_error:
783             self.run_gradinput(inputs_shape=i, filters_shape=f,
784                                output_shape=o, subsample=s, verify_grad=True,
785                                provide_shape=provide_shape, border_mode=b,
786                                filter_flip=flip, target_op=Corr3dMM_gradInputs,
787                                check_trace=True, filter_dilation=fd)
788         else:
789             assert_raises(ValueError,
790                           self.run_gradinput,
791                           inputs_shape=i, filters_shape=f,
792                           output_shape=o, subsample=s, verify_grad=False,
793                           provide_shape=provide_shape, border_mode=b,
794                           filter_flip=flip, target_op=Corr3dMM_gradInputs,
795                           ref=None, check_trace=True, filter_dilation=fd)
796 def test_constant_shapes():
797     dummy_t4 = tensor.ftensor4()
798     alloc_dummy_t4 = tensor.zeros((3, 5, 7, 11), dtype='float32')
799     dummy_shape = tensor.lvector()
800     dummy_one_shape = tensor.ones(4, dtype='int64')
801     constant_vec_shape = tensor.constant([3, 5, 7, 11])
802     tuple_shape = (3, 5, 7, 11)
803     list_shape = list(tuple_shape)
804     constant_list_shape = [tensor.constant(i, dtype='int64')
805                            for i in tuple_shape]
806     constant_tuple_shape = tuple(constant_list_shape)
807     bad_shapes = (
808         dummy_shape,
809         dummy_one_shape,
810         dummy_t4.shape,
811         alloc_dummy_t4.shape,
812         constant_vec_shape,
813     )
814     good_shapes = (
815         constant_list_shape,
816         constant_tuple_shape,
817         tuple_shape,
818         list_shape
819     )
820     ops_to_test = (
821         AbstractConv2d,
822         AbstractConv2d_gradInputs,
823         AbstractConv2d_gradWeights
824     )
825     for op in ops_to_test:
826         for shp in bad_shapes:
827             assert_raises(ValueError, op, imshp=shp)
828             assert_raises(ValueError, op, kshp=shp)
829         for shp in good_shapes:
830             op(imshp=shp)
831             op(kshp=shp)
832 <a name="4"></a>
833 class TestConvTypes(unittest.TestCase):
834     def setUp(self):
835         self<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.input = tensor.ftensor4()
836         self.filters = tensor.ftensor4()
837         self.topgrad = tensor.ftensor4()
838         self.constant_tensor = np.zeros((3, 5, 7, 11), dtype=</b></font>'float32')
839     def test_grad_types(self):
840         input = self.input
841         filters = self.filters
842         topgrad = self.topgrad
843         out_shape = tensor.lvector()
844         output = conv.conv2d(input, filters)
845         grad_input, grad_filters = theano.grad(output.sum(),
846                                                wrt=(input, filters))
847         assert grad_input.type == input.type, (
848             grad_input, grad_input.type, input, input.type)
849         assert grad_filters.type == filters.type, (
850             grad_filters, grad_filters.type, filters, filters.type)
851         grad_filters = conv.AbstractConv2d_gradWeights()(
852             input, topgrad, out_shape)
853         grad_input, grad_topgrad = theano.grad(grad_filters.sum(),
854                                                wrt=(input, topgrad))
855         assert grad_input.type == input.type, (
856             grad_input, grad_input.type, input, input.type)
857         assert grad_topgrad.type == topgrad.type, (
858             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
859         grad_input = conv.AbstractConv2d_gradInputs()(
860             filters, topgrad, out_shape)
861         grad_filters, grad_topgrad = theano.grad(grad_input.sum(),
862                                                  wrt=(filters, topgrad))
863         assert grad_filters.type == filters.type, (
864             grad_filters, grad_filters.type, filters, filters.type)
865         assert grad_topgrad.type == topgrad.type, (
866             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
867     def test_constant_input(self):
868         input = self.input
869         filters = self.filters
870         topgrad = self.topgrad
871         constant_tensor = self.constant_tensor
872         out_shape = tensor.lvector()
873         output = conv.conv2d(constant_tensor, filters)
874         grad_filters = theano.grad(output.sum(), wrt=filters)
875         assert grad_filters.type == filters.type, (
876             grad_filters, grad_filters.type, filters, filters.type)
877         output = conv.conv2d(input, constant_tensor)
878         grad_input = theano.grad(output.sum(), wrt=input)
879         assert grad_input.type == input.type, (
880             grad_input, grad_input.type, input, input.type)
881         grad_filters = conv.AbstractConv2d_gradWeights()(
882             constant_tensor, topgrad, out_shape)
883         grad_topgrad = theano.grad(grad_filters.sum(), wrt=topgrad)
884         assert grad_topgrad.type == topgrad.type, (
885             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
886         grad_filters = conv.AbstractConv2d_gradWeights()(
887             input, constant_tensor, out_shape)
888         grad_input = theano.grad(grad_filters.sum(), wrt=input)
889         assert grad_input.type == input.type, (
890             grad_input, grad_input.type, input, input.type)
891         grad_input = conv.AbstractConv2d_gradInputs()(
892             constant_tensor, topgrad, out_shape)
893         grad_topgrad = theano.grad(grad_input.sum(), wrt=topgrad)
894         assert grad_topgrad.type == topgrad.type, (
895             grad_topgrad, grad_topgrad.type, topgrad, topgrad.type)
896         grad_input = conv.AbstractConv2d_gradInputs()(
897             filters, constant_tensor, out_shape)
898         grad_filters = theano.grad(grad_input.sum(), wrt=filters)
899         assert grad_filters.type == filters.type, (
900             grad_filters, grad_filters.type, filters, filters.type)
901 class TestBilinearUpsampling(unittest.TestCase):
902     compile_mode = theano.compile.mode.get_default_mode()
903     if theano.config.mode == "FAST_COMPILE":
904         compile_mode = compile_mode.excluding("conv_gemm")
905         compile_mode = compile_mode.excluding('AbstractConvCheck')
906     elif not theano.config.cxx:
907         compile_mode = compile_mode.excluding('AbstractConvCheck')
908     def numerical_kernel_1D(self, ratio):
909         return np.array(list(range(1, ratio + 1)) +
910                         list(range(ratio - 1, 0, -1)))
911     def numerical_kernel_2D(self, ratio):
912         return np.array([i * j for i in self.numerical_kernel_1D(ratio) for j
913                          in self.numerical_kernel_1D(ratio)]).\
914             reshape(2 * ratio - 1, 2 * ratio - 1)
915     def test_bilinear_kernel_2D(self):
916         for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
917             kernel = bilinear_kernel_2D(ratio=ratio, normalize=False)
918             f = theano.function([], kernel)
919             kernel_2D = self.numerical_kernel_2D(ratio)
920             utt.assert_allclose(kernel_2D, f())
921             kernel = bilinear_kernel_2D(ratio=ratio, normalize=True)
922             f = theano.function([], kernel)
923             kernel_2D = kernel_2D / float(ratio**2)
924             utt.assert_allclose(kernel_2D, f())
925     def test_bilinear_kernel_1D(self):
926         rat = tensor.iscalar()
927         kernel_ten = bilinear_kernel_1D(ratio=rat, normalize=False)
928         f_ten = theano.function([rat], kernel_ten)
929         kernel_ten_norm = bilinear_kernel_1D(ratio=rat, normalize=True)
930         f_ten_norm = theano.function([rat], kernel_ten_norm)
931         for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
932             kernel = bilinear_kernel_1D(ratio=ratio, normalize=False)
933             f = theano.function([], kernel)
934             kernel_1D = self.numerical_kernel_1D(ratio)
935             utt.assert_allclose(kernel_1D, f())
936             utt.assert_allclose(kernel_1D, f_ten(ratio))
937             kernel = bilinear_kernel_1D(ratio=ratio, normalize=True)
938             f = theano.function([], kernel)
939             kernel_1D = kernel_1D / float(ratio)
940             utt.assert_allclose(kernel_1D, f())
941             utt.assert_allclose(kernel_1D, f_ten_norm(ratio))
942     def numerical_upsampling_multiplier(self, ratio):
943         kern = np.arange(ratio + 1)
944         return kern, kern.shape[0]
945     def get_upsampled_twobytwo_mat(self, two_by_two, ratio):
946         kern, shp = self.numerical_upsampling_multiplier(ratio)
947         up_1D = two_by_two[:, :, :, :1] * kern[::-1] + \
948             two_by_two[:, :, :, 1:] * kern
949         up_2D = up_1D[:, :, :1, :] * kern[::-1][:, np.newaxis] + \
950             up_1D[:, :, 1:, :] * kern[:, np.newaxis]
951         num_concat = (ratio - 1) // 2
952         for i in range(num_concat):
953             up_2D = np.concatenate([up_2D[:, :, :1, :], up_2D], axis=2)
954             up_2D = np.concatenate([up_2D, up_2D[:, :, -1:, :]], axis=2)
955             up_2D = np.concatenate([up_2D[:, :, :, :1], up_2D], axis=3)
956             up_2D = np.concatenate([up_2D, up_2D[:, :, :, -1:]], axis=3)
957         if ratio % 2 == 0:
958             up_2D = np.concatenate([up_2D, up_2D[:, :, -1:, :]], axis=2)
959             up_2D = np.concatenate([up_2D, up_2D[:, :, :, -1:]], axis=3)
960         return up_2D / float(ratio)**2
961     def test_bilinear_upsampling_1D(self):
962         input_x = np.array([[[[1, 2], [3, 4]]]], dtype=theano.config.floatX)
963         for ratio in [2, 3, 4, 5, 6, 7, 8, 9]:
964             bilin_mat = bilinear_upsampling(input=input_x, ratio=ratio,
965                                             batch_size=1, num_input_channels=1,
966                                             use_1D_kernel=True)
967             f = theano.function([], bilin_mat, mode=self.compile_mode)
968             up_mat_2d = self.get_upsampled_twobytwo_mat(input_x, ratio)
969             utt.assert_allclose(f(), up_mat_2d, rtol=1e-06)
970     def test_bilinear_upsampling_reshaping(self):
971         input_x = np.array([[[[1, 2], [3, 4]]]], dtype=theano.config.floatX)
972         for ratio in [2, 3]:
973             for use_1D_kernel in [True, False]:
974                 bilin_mat = bilinear_upsampling(input=input_x, ratio=ratio,
975                                                 batch_size=None,
976                                                 num_input_channels=None,
977                                                 use_1D_kernel=use_1D_kernel)
978                 f = theano.function([], bilin_mat, mode=self.compile_mode)
979                 up_mat_2d = self.get_upsampled_twobytwo_mat(input_x, ratio)
980                 utt.assert_allclose(f(), up_mat_2d, rtol=1e-06)
981     def test_compare_1D_and_2D_upsampling_values(self):
982         input_x = np.random.rand(5, 4, 6, 7).astype(theano.config.floatX)
983         mat_1D = bilinear_upsampling(input=input_x, ratio=5,
984                                      batch_size=5, num_input_channels=4,
985                                      use_1D_kernel=True)
986         mat_2D = bilinear_upsampling(input=input_x, ratio=5,
987                                      batch_size=5, num_input_channels=4,
988                                      use_1D_kernel=False)
989         f_1D = theano.function([], mat_1D, mode=self.compile_mode)
990         f_2D = theano.function([], mat_2D, mode=self.compile_mode)
991         utt.assert_allclose(f_1D(), f_2D(), rtol=1e-06)
992         input_x = np.random.rand(12, 11, 10, 7).astype(theano.config.floatX)
993         mat_1D = bilinear_upsampling(input=input_x, ratio=8,
994                                      batch_size=12, num_input_channels=11,
995                                      use_1D_kernel=True)
996         mat_2D = bilinear_upsampling(input=input_x, ratio=8,
997                                      batch_size=12, num_input_channels=11,
998                                      use_1D_kernel=False)
999         f_1D = theano.function([], mat_1D, mode=self.compile_mode)
1000         f_2D = theano.function([], mat_2D, mode=self.compile_mode)
1001         utt.assert_allclose(f_1D(), f_2D(), rtol=1e-06)
1002     def test_fractional_bilinear_upsampling(self):
1003         input_x = np.array([[[1, 2], [3, 4]],
1004                             [[5, 6], [7, 8]],
1005                             [[9, 10], [11, 12]]],
1006                            ndmin=4).astype(theano.config.floatX)
1007         up_x = bilinear_upsampling(input=input_x,
1008 <a name="3"></a>                                   frac_ratio=((7, 4), (5, 3)),
1009                                    use_1D_kernel=False)
1010         num_up_x = np.array(
1011             [<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[1., 1.2, 1.8, 2.],
1012               [1.28571429, 1.48571429, 2.08571429, 2.28571429],
1013               [2.42857143, 2.62857143, 3.22857143, 3.42857143],
1014               [3., 3.2, 3.8, 4.]],
1015              [[5., 5.2, 5.8, 6.],
1016               [5.28571429, 5.48571429, 6.08571429, 6.28571429],
1017               [6.42857143, 6.62857143, 7.22857143, 7.42857143],
1018               [7., 7.2, 7.8, 8.]],
1019              [[9., 9.2, 9.8, 10.],
1020               [9.28571429, 9.48571429, 10.08571429, 10.28571429],
1021               [10.42857143, 10.62857143, 11.22857143, 11.42857143],
1022               [11., 11.2, 11.8, 12.]]]]
1023             ).</b></font>astype(theano.config.floatX)
1024         f_up_x = theano.function([], up_x, mode=self.compile_mode)
1025         utt.assert_allclose(f_up_x(), num_up_x, rtol=1e-6)
1026     def test_fractional_bilinear_upsampling_shape(self):
1027         x = np.random.rand(1, 1, 200, 200).astype(theano.config.floatX)
1028         resize = (24, 20)
1029         z = bilinear_upsampling(tensor.as_tensor_variable(x), frac_ratio=resize, use_1D_kernel=False)
1030         out = theano.function([], z.shape, mode='FAST_RUN')()
1031         utt.assert_allclose(out, (1, 1, 240, 240))
1032 class TestConv2dTranspose(unittest.TestCase):
1033     mode = None
1034     def test_interface(self):
1035         if theano.config.cxx == "":
1036             raise SkipTest("test needs cxx")
1037         mode = self.mode
1038         if theano.config.mode == "FAST_COMPILE":
1039             mode = theano.compile.get_mode(
1040                 mode).excluding("conv_gemm").excluding("AbstractConvCheck")
1041         output = theano.function(
1042             inputs=[],
1043             outputs=conv2d_transpose(input=tensor.ones((2, 2, 4, 4)),
1044                                      filters=tensor.ones((2, 1, 4, 4)),
1045                                      output_shape=(2, 1, 10, 10),
1046 <a name="6"></a>                                     input_dilation=(2, 2)),
1047             mode=mode)()
1048         expected_output = np.array(
1049             [<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
1050                [2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
1051                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1052                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1053                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1054                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1055                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1056                [4, 4, 8, 8, 8, 8, 8, 8, 4, 4],
1057                [2, 2, 4, 4, 4, 4, 4, 4, 2, 2],
1058                [2, 2, 4, 4, 4, 4, 4, 4, 2, 2]]]] * 2)
1059         np.testing.</b></font>assert_equal(output, expected_output)
1060 class TestConv2dGrads(unittest.TestCase):
1061     def setUp(self):
1062         if (not theano.config.cxx or
1063                 theano.config.mode == "FAST_COMPILE"):
1064             raise SkipTest("Need blas to test conv2d")
1065         self.random_stream = np.random.RandomState(utt.fetch_seed())
1066         self.inputs_shapes = [(8, 1, 12, 12), (1, 1, 5, 5), (1, 1, 5, 6), (1, 1, 6, 6)]
1067         self.filters_shapes = [(5, 1, 2, 2), (1, 1, 3, 3)]
1068         self.subsamples = [(1, 1), (2, 2)]
1069         self.border_modes = ["valid", "full"]
1070         self.filter_flip = [True, False]
1071         self.output_grad = theano.tensor.tensor4()
1072         self.output_grad_wrt = theano.tensor.tensor4()
1073         self.x = theano.tensor.tensor4('x', theano.config.floatX)  # inputs
1074         self.w = theano.tensor.tensor4('w', theano.config.floatX)  # filter weights
1075     def test_conv2d_grad_wrt_inputs(self):
1076         for (in_shape, fltr_shape) in zip(self.inputs_shapes, self.filters_shapes):
1077             for bm in self.border_modes:
1078                 for ss in self.subsamples:
1079                     for ff in self.filter_flip:
1080                         input_val = self.random_stream.random_sample(in_shape).astype(theano.config.floatX)
1081                         filter_val = self.random_stream.random_sample(fltr_shape).astype(theano.config.floatX)
1082                         out_grad_shape = theano.tensor.nnet.abstract_conv.get_conv_output_shape(image_shape=in_shape,
1083                                                                                                 kernel_shape=fltr_shape,
1084                                                                                                 border_mode=bm,
1085                                                                                                 subsample=ss)
1086                         out_grad_val = self.random_stream.random_sample(out_grad_shape).astype(theano.config.floatX)
1087                         conv_out = theano.tensor.nnet.conv2d(self.x,
1088                                                              filters=self.w,
1089                                                              border_mode=bm,
1090                                                              subsample=ss,
1091                                                              input_shape=in_shape,
1092                                                              filter_shape=fltr_shape,
1093                                                              filter_flip=ff
1094                                                              )
1095                         conv_grad = theano.grad(conv_out.sum(), wrt=self.x, known_grads={conv_out: self.output_grad})
1096                         f_old = theano.function([self.x, self.w, self.output_grad], conv_grad)
1097                         conv_wrt_i_out = theano.tensor.nnet.abstract_conv.conv2d_grad_wrt_inputs(output_grad=self.output_grad_wrt,
1098                                                                                                  filters=self.w,
1099                                                                                                  border_mode=bm,
1100                                                                                                  subsample=ss,
1101                                                                                                  input_shape=in_shape,
1102                                                                                                  filter_shape=fltr_shape,
1103                                                                                                  filter_flip=ff
1104                                                                                                  )
1105                         f_new = theano.function([self.w, self.output_grad_wrt], conv_wrt_i_out)
1106                         utt.assert_allclose(f_new(filter_val, out_grad_val), f_old(input_val, filter_val, out_grad_val))
1107     def test_conv2d_grad_wrt_weights(self):
1108         for (in_shape, fltr_shape) in zip(self.inputs_shapes, self.filters_shapes):
1109             for bm in self.border_modes:
1110                 for ss in self.subsamples:
1111                     for ff in self.filter_flip:
1112                         input_val = self.random_stream.random_sample(in_shape).astype(theano.config.floatX)
1113                         filter_val = self.random_stream.random_sample(fltr_shape).astype(theano.config.floatX)
1114                         out_grad_shape = theano.tensor.nnet.abstract_conv.get_conv_output_shape(image_shape=in_shape,
1115                                                                                                 kernel_shape=fltr_shape,
1116                                                                                                 border_mode=bm,
1117                                                                                                 subsample=ss)
1118                         out_grad_val = self.random_stream.random_sample(out_grad_shape).astype(theano.config.floatX)
1119                         conv_out = theano.tensor.nnet.conv2d(self.x,
1120                                                              filters=self.w,
1121                                                              border_mode=bm,
1122                                                              subsample=ss,
1123                                                              input_shape=in_shape,
1124                                                              filter_shape=fltr_shape,
1125                                                              filter_flip=ff
1126                                                              )
1127                         conv_grad = theano.grad(conv_out.sum(), wrt=self.w, known_grads={conv_out: self.output_grad})
1128                         f_old = theano.function([self.x, self.w, self.output_grad], conv_grad)
1129                         conv_wrt_w_out = theano.tensor.nnet.abstract_conv.conv2d_grad_wrt_weights(self.x,
1130                                                                                                   output_grad=self.output_grad_wrt,
1131                                                                                                   border_mode=bm,
1132                                                                                                   subsample=ss,
1133                                                                                                   input_shape=in_shape,
1134                                                                                                   filter_shape=fltr_shape,
1135                                                                                                   filter_flip=ff
1136                                                                                                   )
1137                         f_new = theano.function([self.x, self.output_grad_wrt], conv_wrt_w_out)
1138                         utt.assert_allclose(f_new(input_val, out_grad_val), f_old(input_val, filter_val, out_grad_val))
1139 class Grouped_conv_noOptim(unittest.TestCase):
1140     conv = theano.tensor.nnet.abstract_conv.AbstractConv2d
1141     conv_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1142     conv_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1143     conv_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
1144     conv_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1145     conv_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1146     mode = theano.Mode(optimizer=None)
1147     is_dnn = False
1148     def setUp(self):
1149         self.num_groups = [3, 2, 4, 4]
1150         self.border_mode = 'valid'
1151         self.subsample = (1, 1)
1152         self.img_shape = [(5, 6, 5, 5), (4, 4, 7, 5), (3, 8, 5, 3), (2, 4, 7, 7)]
1153         self.kern_shape = [(6, 2, 3, 3), (6, 2, 5, 3), (4, 2, 3, 3), (4, 1, 3, 5)]
1154         self.top_shape = [(5, 6, 3, 3), (4, 6, 3, 3), (3, 4, 3, 1), (2, 4, 5, 3)]
1155         self.filter_dilation = (1, 1)
1156         self.ref_mode = 'FAST_RUN'
1157         self.convdim = 2
1158         self.corr_fwd = conv2d_corr
1159         self.corr_gradw = conv2d_corr_gw
1160         self.corr_gradi = conv2d_corr_gi
1161         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1162             raise SkipTest("CorrMM needs cxx and SciPy")
1163 <a name="13"></a>
1164     def test_fwd(self):
1165         if self.convdim == 2:
1166             img_sym <font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= theano.tensor.tensor4('img')
1167             kern_sym = theano.tensor.tensor4('kern')
1168         else:
1169             img_sym = theano.tensor.tensor5('img')
1170             kern_sym =</b></font> theano.tensor.tensor5('kern')
1171         for imshp, kshp, groups in zip(self.img_shape, self.kern_shape, self.num_groups):
1172             img = np.random.random(imshp).astype(theano.config.floatX)
1173             kern = np.random.random(kshp).astype(theano.config.floatX)
1174             split_imgs = np.split(img, groups, axis=1)
1175             split_kern = np.split(kern, groups, axis=0)
1176             grouped_conv_op = self.conv(border_mode=self.border_mode,
1177                                         subsample=self.subsample,
1178                                         filter_dilation=self.filter_dilation,
1179                                         num_groups=groups)
1180             grouped_conv_output = grouped_conv_op(img_sym, kern_sym)
1181             grouped_func = theano.function([img_sym, kern_sym], grouped_conv_output, mode=self.mode)
1182             assert any([isinstance(node.op, self.conv_op)
1183                        for node in grouped_func.maker.fgraph.toposort()])
1184             grouped_output = grouped_func(img, kern)
1185             ref_conv_op = self.corr_fwd(img_sym,
1186                                         kern_sym,
1187                                         border_mode=self.border_mode,
1188                                         subsample=self.subsample,
1189                                         filter_dilation=self.filter_dilation)
1190             ref_func = theano.function([img_sym, kern_sym], ref_conv_op,
1191                                        mode=self.ref_mode)
1192             ref_concat_output = [ref_func(img_arr, kern_arr)
1193                                  for img_arr, kern_arr in zip(split_imgs, split_kern)]
1194             ref_concat_output = np.concatenate(ref_concat_output, axis=1)
1195             utt.assert_allclose(grouped_output, ref_concat_output)
1196             utt.verify_grad(grouped_conv_op,
1197                             [img, kern],
1198                             mode=self.mode,
1199                             eps=1)
1200     def test_gradweights(self):
1201         if self.convdim == 2:
1202             img_sym = theano.tensor.tensor4('img')
1203             top_sym = theano.tensor.tensor4('kern')
1204         else:
1205             img_sym = theano.tensor.tensor5('img')
1206             top_sym = theano.tensor.tensor5('kern')
1207         for imshp, kshp, tshp, groups in zip(self.img_shape, self.kern_shape, self.top_shape, self.num_groups):
1208             img = np.random.random(imshp).astype(theano.config.floatX)
1209             top = np.random.random(tshp).astype(theano.config.floatX)
1210             split_imgs = np.split(img, groups, axis=1)
1211             split_top = np.split(top, groups, axis=1)
1212             grouped_convgrad_op = self.conv_gradw(border_mode=self.border_mode,
1213                                                   subsample=self.subsample,
1214                                                   filter_dilation=self.filter_dilation,
1215                                                   num_groups=groups)
1216             grouped_conv_output = grouped_convgrad_op(img_sym,
1217                                                       top_sym,
1218                                                       tensor.as_tensor_variable(
1219                                                           kshp[-self.convdim:]))
1220             grouped_func = theano.function([img_sym, top_sym], grouped_conv_output, mode=self.mode)
1221             assert any([isinstance(node.op, self.conv_gradw_op)
1222                        for node in grouped_func.maker.fgraph.toposort()])
1223             grouped_output = grouped_func(img, top)
1224             ref_conv_op = self.corr_gradw(img_sym,
1225                                           top_sym,
1226                                           kshp,
1227                                           border_mode=self.border_mode,
1228                                           subsample=self.subsample,
1229                                           filter_dilation=self.filter_dilation)
1230             ref_func = theano.function([img_sym, top_sym], ref_conv_op,
1231                                        mode=self.ref_mode)
1232             ref_concat_output = [ref_func(img_arr, top_arr)
1233                                  for img_arr, top_arr in zip(split_imgs, split_top)]
1234             ref_concat_output = np.concatenate(ref_concat_output, axis=0)
1235             utt.assert_allclose(grouped_output, ref_concat_output)
1236             def conv_gradweight(inputs_val, output_val):
1237                 return grouped_convgrad_op(inputs_val, output_val,
1238                                            tensor.as_tensor_variable(
1239                                                kshp[-self.convdim:]))
1240             utt.verify_grad(conv_gradweight,
1241                             [img, top],
1242                             mode=self.mode, eps=1)
1243     def test_gradinputs(self):
1244         if self.convdim == 2:
1245             kern_sym = theano.tensor.tensor4('kern')
1246             top_sym = theano.tensor.tensor4('top')
1247         else:
1248             kern_sym = theano.tensor.tensor5('kern')
1249             top_sym = theano.tensor.tensor5('top')
1250         for imshp, kshp, tshp, groups in zip(self.img_shape, self.kern_shape, self.top_shape, self.num_groups):
1251             kern = np.random.random(kshp).astype(theano.config.floatX)
1252             top = np.random.random(tshp).astype(theano.config.floatX)
1253             split_kerns = np.split(kern, groups, axis=0)
1254             split_top = np.split(top, groups, axis=1)
1255             grouped_convgrad_op = self.conv_gradi(border_mode=self.border_mode,
1256                                                   subsample=self.subsample,
1257                                                   filter_dilation=self.filter_dilation,
1258                                                   num_groups=groups)
1259             grouped_conv_output = grouped_convgrad_op(kern_sym,
1260                                                       top_sym,
1261                                                       tensor.as_tensor_variable(
1262                                                           imshp[-self.convdim:]))
1263             grouped_func = theano.function([kern_sym, top_sym], grouped_conv_output, mode=self.mode)
1264             assert any([isinstance(node.op, self.conv_gradi_op)
1265                        for node in grouped_func.maker.fgraph.toposort()])
1266             grouped_output = grouped_func(kern, top)
1267             ref_conv_op = self.corr_gradi(kern_sym,
1268                                           top_sym,
1269                                           imshp,
1270                                           border_mode=self.border_mode,
1271                                           subsample=self.subsample,
1272                                           filter_dilation=self.filter_dilation)
1273             ref_func = theano.function([kern_sym, top_sym], ref_conv_op,
1274                                        mode=self.ref_mode)
1275             ref_concat_output = [ref_func(kern_arr, top_arr)
1276                                  for kern_arr, top_arr in zip(split_kerns, split_top)]
1277             ref_concat_output = np.concatenate(ref_concat_output, axis=1)
1278             utt.assert_allclose(grouped_output, ref_concat_output)
1279             def conv_gradinputs(filters_val, output_val):
1280                 return grouped_convgrad_op(filters_val, output_val,
1281                                            tensor.as_tensor_variable(
1282                                                imshp[-self.convdim:]))
1283             utt.verify_grad(conv_gradinputs,
1284                             [kern, top],
1285                             mode=self.mode, eps=1)
1286 class Grouped_conv3d_noOptim(Grouped_conv_noOptim):
1287     conv = theano.tensor.nnet.abstract_conv.AbstractConv3d
1288     conv_gradw = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradWeights
1289     conv_gradi = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradInputs
1290     conv_op = theano.tensor.nnet.abstract_conv.AbstractConv3d
1291     conv_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradWeights
1292     conv_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv3d_gradInputs
1293     mode = theano.Mode(optimizer=None)
1294     def setUp(self):
1295         self.num_groups = [3, 2, 4, 4]
1296         self.border_mode = 'valid'
1297         self.subsample = (1, 1, 1)
1298         self.img_shape = [(2, 6, 5, 5, 5), (1, 4, 7, 5, 7), (1, 8, 5, 3, 5), (2, 4, 7, 7, 7)]
1299         self.kern_shape = [(3, 2, 3, 3, 3), (6, 2, 5, 3, 5), (4, 2, 3, 3, 3), (4, 1, 3, 5, 3)]
1300         self.top_shape = [(2, 3, 3, 3, 3), (1, 6, 3, 3, 3), (1, 4, 3, 1, 3), (2, 4, 5, 3, 5)]
1301         self.filter_dilation = (1, 1, 1)
1302         self.ref_mode = 'FAST_RUN'
1303         self.convdim = 3
1304         self.corr_fwd = conv3d_corr
1305         self.corr_gradw = conv3d_corr_gw
1306         self.corr_gradi = conv3d_corr_gi
1307         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1308             raise SkipTest("CorrMM needs cxx")
1309 <a name="12"></a>
1310 class Separable_conv(unittest.TestCase):
1311     def setUp(self):
1312 <a name="1"></a>        self.x = np.array([<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[1, 2, 3, 4, 5], [3, 2, 1, 4, 5], [3, 3, 1, 3, 6], [5, 3, 2, 1, 1], [4, 7, 1, 2, 1]],
1313                             [[3, 3, 1, 2, 6], [6, 5, 4, 3, 1], [3, 4, 5, 2, 3], [6</b></font>, 4, 1, 3, 4], [2, 3, 4, 2, 5]]]]).astype(theano.config.floatX)
1314         self.depthwise_filter = np.array([<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[[[3, 2, 1], [5, 3, 2], [6, 4, 2]]], [[[5, 5, 2], [3, 7, 4], [3, 5, 4]]],
1315                                           [[[7, 4, 7], [5, 3, 3], [1, 3, 1]]], [[[4, 4, 4], [2, 4, 6], [0, 0, 7]]]]).</b></font>astype(theano.config.floatX)
1316         self.pointwise_filter = np.array([[[[4]], [[1]], [[3]], [[5]]], [[[2]], [[1]], [[2]], [[8]]]]).astype(theano.config.floatX)
1317         self.precomp_output_valid = np.array([[[[1385, 1333, 1339], [1382, 1243, 1291], [1303, 1120, 1228]],
1318                                                [[1532, 1410, 1259], [1522, 1346, 1314], [1379, 1192, 1286]]]]).astype(theano.config.floatX)
1319         self.precomp_output_full = np.array([[[[140, 266, 343, 206, 59],
1320                                               [395, 697, 979, 585, 245],
1321                                               [429, 863, 1385, 919, 453],
1322                                               [243, 499, 864, 627, 371],
1323                                               [90, 183, 291, 254, 202]],
1324                                              [[149, 289, 359, 213, 58],
1325                                               [400, 750, 1076, 662, 266],
1326                                               [387, 854, 1532, 1091, 540],
1327                                               [174, 411, 971, 786, 518],
1328                                               [51, 110, 286, 299, 298]]]]).astype(theano.config.floatX)
1329     def test_interface2d(self):
1330         if theano.config.cxx == "":
1331             raise SkipTest("test needs cxx")
1332         x_sym = theano.tensor.tensor4('x')
1333         dfilter_sym = theano.tensor.tensor4('d')
1334         pfilter_sym = theano.tensor.tensor4('p')
1335         sep_op = separable_conv2d(x_sym, dfilter_sym, pfilter_sym, self.x.shape[1])
1336         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1337         top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
1338         utt.assert_allclose(top, self.precomp_output_valid)
1339         top = fun(self.x[:, :, :3, :], self.depthwise_filter, self.pointwise_filter)
1340         utt.assert_allclose(top, self.precomp_output_valid[:, :, :1, :])
1341         sep_op = separable_conv2d(x_sym,
1342                                   dfilter_sym,
1343                                   pfilter_sym,
1344                                   self.x.shape[1],
1345                                   input_shape=self.x.shape,
1346                                   depthwise_filter_shape=self.depthwise_filter.shape,
1347                                   pointwise_filter_shape=self.pointwise_filter.shape)
1348         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1349         top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
1350         utt.assert_allclose(top, self.precomp_output_valid)
1351         sep_op = separable_conv2d(x_sym,
1352                                   dfilter_sym,
1353                                   pfilter_sym,
1354                                   self.x.shape[1],
1355                                   subsample=(2, 2))
1356         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1357         top = fun(self.x, self.depthwise_filter, self.pointwise_filter)
1358         utt.assert_allclose(top, np.delete(np.delete(self.precomp_output_valid, 1, axis=3), 1, axis=2))
1359         sep_op = separable_conv2d(x_sym, dfilter_sym, pfilter_sym, self.x.shape[1], border_mode='full')
1360         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1361         top = fun(self.x[:, :, :3, :3], self.depthwise_filter, self.pointwise_filter)
1362         utt.assert_allclose(top, self.precomp_output_full)
1363     def test_interface3d(self):
1364         if theano.config.cxx == "":
1365             raise SkipTest("test needs cxx")
1366         x = np.tile(np.expand_dims(self.x, axis=2), (1, 1, 5, 1, 1))
1367         depthwise_filter = np.tile(np.expand_dims(self.depthwise_filter, axis=2), (1, 1, 3, 1, 1))
1368         pointwise_filter = np.expand_dims(self.pointwise_filter, axis=2)
1369         precomp_output = np.tile(np.expand_dims(self.precomp_output_valid, axis=2), (1, 1, 3, 1, 1)) * 3
1370         x_sym = theano.tensor.tensor5('x')
1371         dfilter_sym = theano.tensor.tensor5('d')
1372         pfilter_sym = theano.tensor.tensor5('p')
1373         sep_op = separable_conv3d(x_sym, dfilter_sym, pfilter_sym, x.shape[1])
1374         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1375         top = fun(x, depthwise_filter, pointwise_filter)
1376         utt.assert_allclose(top, precomp_output)
1377         top = fun(x[:, :, :3, :, :3], depthwise_filter, pointwise_filter)
1378         utt.assert_allclose(top, precomp_output[:, :, :1, :, :1])
1379         sep_op = separable_conv3d(x_sym,
1380                                   dfilter_sym,
1381                                   pfilter_sym,
1382                                   x.shape[1],
1383                                   input_shape=x.shape,
1384                                   depthwise_filter_shape=depthwise_filter.shape,
1385                                   pointwise_filter_shape=pointwise_filter.shape)
1386         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1387         top = fun(x, depthwise_filter, pointwise_filter)
1388         utt.assert_allclose(top, precomp_output)
1389         sep_op = separable_conv3d(x_sym,
1390                                   dfilter_sym,
1391                                   pfilter_sym,
1392                                   x.shape[1],
1393                                   subsample=(2, 2, 2))
1394         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1395         top = fun(x, depthwise_filter, pointwise_filter)
1396         utt.assert_allclose(top, np.delete(np.delete(
1397             np.delete(precomp_output, 1, axis=4), 1, axis=3), 1, axis=2))
1398         precomp_output = np.tile(np.expand_dims(self.precomp_output_full, axis=2),
1399                                  (1, 1, 5, 1, 1)) * np.array([[[[[1]], [[2]], [[3]], [[2]], [[1]]]]])
1400         sep_op = separable_conv3d(x_sym, dfilter_sym, pfilter_sym, x.shape[1], border_mode='full')
1401         fun = theano.function([x_sym, dfilter_sym, pfilter_sym], sep_op, mode='FAST_RUN')
1402         top = fun(x[:, :, :3, :3, :3], depthwise_filter, pointwise_filter)
1403         utt.assert_allclose(top, precomp_output)
1404 class TestUnsharedConv(unittest.TestCase):
1405     conv2d = theano.tensor.nnet.abstract_conv.AbstractConv2d
1406     conv2d_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1407     conv2d_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1408     conv2d_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
1409     conv2d_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1410     conv2d_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1411     mode = theano.compile.mode.Mode(optimizer='None')
1412     def setUp(self):
1413         self.img_shape = [(2, 2, 4, 4), (3, 2, 4, 2), (3, 3, 5, 3), (3, 4, 4, 4)]
1414         self.kern_shape = [(2, 2, 2, 2, 3, 3), (2, 4, 2, 2, 4, 2), (3, 2, 1, 1, 3, 3), (4, 3, 3, 2, 4, 2)]
1415         self.topgrad_shape = [(2, 2, 2, 2), (3, 2, 4, 2), (3, 3, 2, 1), (3, 4, 3, 3)]
1416         self.border_mode = ['valid', 'full', 'valid', 'full']
1417         self.subsample = [(1, 1), (2, 2), (2, 1), (3, 2)]
1418         self.filter_dilation = (1, 1)
1419         self.num_groups = [1, 1, 3, 2]
1420         self.verify_flags = [True] * 4
1421         self.ref_mode = 'FAST_RUN'
1422         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1423             raise SkipTest("CorrMM needs cxx or SciPy")
1424     def test_fwd(self):
1425         tensor6 = theano.tensor.TensorType(theano.config.floatX, (False,) * 6)
1426         img_sym = theano.tensor.tensor4('img')
1427         kern_sym = tensor6('kern')
1428         ref_kern_sym = theano.tensor.tensor4('ref_kern')
1429         for imshp, kshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.border_mode,
1430                                                           self.subsample, self.num_groups, self.verify_flags):
1431             img = np.random.random(imshp).astype(theano.config.floatX)
1432             kern = np.random.random(kshp).astype(theano.config.floatX)
1433             unshared_conv_op = self.conv2d(border_mode=mode, subsample=sub,
1434                                            filter_dilation=self.filter_dilation,
1435                                            num_groups=groups, unshared=True)
1436             unshared_out_sym = unshared_conv_op(img_sym, kern_sym)
1437             unshared_func = theano.function([img_sym, kern_sym], unshared_out_sym, mode=self.mode)
1438             assert any([isinstance(node.op, self.conv2d_op)
1439                         for node in unshared_func.maker.fgraph.toposort()])
1440             unshared_output = unshared_func(img, kern)
1441             single_kshp = kshp[:1] + kshp[3:]
1442             ref_conv_op = self.conv2d(border_mode=mode, subsample=sub,
1443                                       filter_dilation=self.filter_dilation,
1444                                       num_groups=groups, unshared=False)
1445             ref_out_sym = ref_conv_op(img_sym, ref_kern_sym)
1446             ref_func = theano.function([img_sym, ref_kern_sym], ref_out_sym, mode=self.mode)
1447             for i in range(0, kshp[1]):
1448                 for j in range(0, kshp[2]):
1449                     single_kern = kern[:, i, j, ...].reshape(single_kshp)
1450                     ref_val = ref_func(img, single_kern)
1451                     utt.assert_allclose(ref_val[:, :, i, j], unshared_output[:, :, i, j])
1452             if verify:
1453                 utt.verify_grad(unshared_conv_op, [img, kern], mode=self.mode, eps=1)
1454     def test_gradweight(self):
1455         img_sym = theano.tensor.tensor4('img')
1456         top_sym = theano.tensor.tensor4('top')
1457         for imshp, kshp, topshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.topgrad_shape,
1458                                                                   self.border_mode, self.subsample, self.num_groups,
1459                                                                   self.verify_flags):
1460             img = np.random.random(imshp).astype(theano.config.floatX)
1461             top = np.random.random(topshp).astype(theano.config.floatX)
1462             unshared_conv_op = self.conv2d_gradw(border_mode=mode, subsample=sub,
1463                                                  filter_dilation=self.filter_dilation,
1464                                                  num_groups=groups, unshared=True)
1465             unshared_out_sym = unshared_conv_op(img_sym, top_sym, tensor.as_tensor_variable(kshp[-2:]))
1466             unshared_func = theano.function([img_sym, top_sym], unshared_out_sym, mode=self.mode)
1467             assert any([isinstance(node.op, self.conv2d_gradw_op)
1468                         for node in unshared_func.maker.fgraph.toposort()])
1469             unshared_output = unshared_func(img, top)
1470             single_kshp = kshp[:1] + kshp[3:]
1471             ref_conv_op = self.conv2d_gradw(border_mode=mode, subsample=sub,
1472                                             filter_dilation=self.filter_dilation,
1473                                             num_groups=groups, unshared=False)
1474             ref_out_sym = ref_conv_op(img_sym, top_sym, tensor.as_tensor_variable(single_kshp[-2:]))
1475             ref_func = theano.function([img_sym, top_sym], ref_out_sym, mode=self.mode)
1476             for i in range(0, topshp[2]):
1477                 for j in range(0, topshp[3]):
1478                     top_single = np.zeros_like(top)
1479                     top_single[:, :, i, j] = top[:, :, i, j]
1480                     ref_output = ref_func(img, top_single)
1481                     utt.assert_allclose(unshared_output[:, i, j, ...], ref_output)
1482             def conv_gradweight(inputs_val, output_val):
1483                 return unshared_conv_op(inputs_val, output_val, tensor.as_tensor_variable(kshp[-2:]))
1484             if verify:
1485                 utt.verify_grad(conv_gradweight, [img, top], mode=self.mode, eps=1)
1486     def test_gradinput(self):
1487         tensor6 = theano.tensor.TensorType(theano.config.floatX, (False,) * 6)
1488         kern_sym = tensor6('kern')
1489         top_sym = theano.tensor.tensor4('top')
1490         ref_kern_sym = theano.tensor.tensor4('ref_kern')
1491         for imshp, kshp, topshp, mode, sub, groups, verify in zip(self.img_shape, self.kern_shape, self.topgrad_shape,
1492                                                                   self.border_mode, self.subsample, self.num_groups,
1493                                                                   self.verify_flags):
1494             single_kshp = kshp[:1] + kshp[3:]
1495             kern = np.random.random(kshp).astype(theano.config.floatX)
1496             top = np.random.random(topshp).astype(theano.config.floatX)
1497             unshared_conv_op = self.conv2d_gradi(border_mode=mode, subsample=sub,
1498                                                  filter_dilation=self.filter_dilation,
1499                                                  num_groups=groups, unshared=True)
1500             unshared_out_sym = unshared_conv_op(kern_sym, top_sym, tensor.as_tensor_variable(imshp[-2:]))
1501             unshared_func = theano.function([kern_sym, top_sym], unshared_out_sym, mode=self.mode)
1502             assert any([isinstance(node.op, self.conv2d_gradi_op)
1503                         for node in unshared_func.maker.fgraph.toposort()])
1504             unshared_output = unshared_func(kern, top)
1505             ref_conv_op = self.conv2d_gradi(border_mode=mode, subsample=sub,
1506                                             filter_dilation=self.filter_dilation,
1507                                             num_groups=groups, unshared=False)
1508             ref_out_sym = ref_conv_op(ref_kern_sym, top_sym, tensor.as_tensor_variable(imshp[-2:]))
1509             ref_func = theano.function([ref_kern_sym, top_sym], ref_out_sym, mode=self.mode)
1510             ref_output = np.zeros(imshp)
1511             for i in range(0, topshp[2]):
1512                 for j in range(0, topshp[3]):
1513                     single_kern = kern[:, i, j, ...].reshape(single_kshp)
1514                     top_single = np.zeros_like(top)
1515                     top_single[:, :, i, j] = top[:, :, i, j]
1516                     ref_output += ref_func(single_kern, top_single)
1517             utt.assert_allclose(ref_output, unshared_output)
1518             def conv_gradinputs(filters_val, output_val):
1519                 return unshared_conv_op(filters_val, output_val, tensor.as_tensor_variable(imshp[-2:]))
1520             if verify:
1521                 utt.verify_grad(conv_gradinputs, [kern, top], mode=self.mode, eps=1)
1522 class TestAsymmetricPadding(unittest.TestCase):
1523     conv2d = theano.tensor.nnet.abstract_conv.AbstractConv2d
1524     conv2d_gradw = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1525     conv2d_gradi = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1526     conv2d_op = theano.tensor.nnet.abstract_conv.AbstractConv2d
1527     conv2d_gradw_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradWeights
1528     conv2d_gradi_op = theano.tensor.nnet.abstract_conv.AbstractConv2d_gradInputs
1529     mode = theano.compile.mode.Mode(optimizer='None')
1530     img_shape = [(2, 2, 4, 4), (3, 2, 4, 2), (3, 3, 5, 3)]
1531     kern_shape = [(4, 2, 2, 2), (2, 2, 4, 2), (2, 3, 3, 3)]
1532     topgrad_shape = [(2, 4, 6, 6), (3, 2, 3, 4), (3, 2, 6, 1)]
1533     border_mode = [((1, 2), (2, 1)), ((1, 1), (0, 3)), ((2, 1), (0, 0))]
1534     def test_fwd(self):
1535         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1536             raise SkipTest("SciPy and cxx needed")
1537         img_sym = theano.tensor.tensor4('img')
1538         kern_sym = theano.tensor.tensor4('kern')
1539         for imshp, kshp, pad in zip(self.img_shape, self.kern_shape, self.border_mode):
1540             img = np.random.random(imshp).astype(theano.config.floatX)
1541             kern = np.random.random(kshp).astype(theano.config.floatX)
1542             asymmetric_conv_op = self.conv2d(border_mode=pad, subsample=(1, 1),
1543                                              filter_dilation=(1, 1))
1544             asymmetric_out_sym = asymmetric_conv_op(img_sym, kern_sym)
1545             asymmetric_func = theano.function([img_sym, kern_sym], asymmetric_out_sym, mode=self.mode)
1546             assert any([isinstance(node.op, self.conv2d_op)
1547                         for node in asymmetric_func.maker.fgraph.toposort()])
1548             asymmetric_output = asymmetric_func(img, kern)
1549             ref_conv_op = self.conv2d(border_mode="valid", subsample=(1, 1),
1550                                       filter_dilation=(1, 1))
1551             ref_out_sym = ref_conv_op(img_sym, kern_sym)
1552             ref_func = theano.function([img_sym, kern_sym], ref_out_sym, mode=self.mode)
1553             exp_imshp = (imshp[0], imshp[1],
1554                          imshp[2] + pad[0][0] + pad[0][1],
1555                          imshp[3] + pad[1][0] + pad[1][1])
1556             exp_img = np.zeros(exp_imshp, dtype=theano.config.floatX)
1557             exp_img[:, :, pad[0][0]:imshp[2] + pad[0][0],
1558                     pad[1][0]:imshp[3] + pad[1][0]] = img
1559             ref_output = ref_func(exp_img, kern)
1560             utt.assert_allclose(asymmetric_output, ref_output)
1561             utt.verify_grad(asymmetric_conv_op, [img, kern], mode=self.mode, eps=1)
1562     def test_gradweight(self):
1563         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1564             raise SkipTest("SciPy and cxx needed")
1565         img_sym = theano.tensor.tensor4('img')
1566         top_sym = theano.tensor.tensor4('top')
1567         for imshp, kshp, topshp, pad in zip(self.img_shape, self.kern_shape, self.topgrad_shape, self.border_mode):
1568             img = np.random.random(imshp).astype(theano.config.floatX)
1569             top = np.random.random(topshp).astype(theano.config.floatX)
1570             asymmetric_conv_op = self.conv2d_gradw(border_mode=pad, subsample=(1, 1),
1571                                                    filter_dilation=(1, 1))
1572             asymmetric_out_sym = asymmetric_conv_op(img_sym, top_sym, kshp[-2:])
1573             asymmetric_func = theano.function([img_sym, top_sym], asymmetric_out_sym, mode=self.mode)
1574             assert any([isinstance(node.op, self.conv2d_gradw_op)
1575                         for node in asymmetric_func.maker.fgraph.toposort()])
1576             asymmetric_output = asymmetric_func(img, top)
1577             ref_conv_op = self.conv2d_gradw(border_mode="valid", subsample=(1, 1),
1578                                             filter_dilation=(1, 1))
1579             ref_out_sym = ref_conv_op(img_sym, top_sym, kshp[-2:])
1580             ref_func = theano.function([img_sym, top_sym], ref_out_sym, mode=self.mode)
1581             exp_imshp = (imshp[0], imshp[1],
1582                          imshp[2] + pad[0][0] + pad[0][1],
1583                          imshp[3] + pad[1][0] + pad[1][1])
1584             exp_img = np.zeros(exp_imshp, dtype=theano.config.floatX)
1585             exp_img[:, :, pad[0][0]:imshp[2] + pad[0][0],
1586                     pad[1][0]:imshp[3] + pad[1][0]] = img
1587             ref_output = ref_func(exp_img, top)
1588             utt.assert_allclose(asymmetric_output, ref_output)
1589             def conv_gradweight(inputs_val, output_val):
1590                 return asymmetric_conv_op(inputs_val, output_val, tensor.as_tensor_variable(kshp[-2:]))
1591             utt.verify_grad(conv_gradweight, [img, top], mode=self.mode, eps=1)
1592     def test_gradinput(self):
1593         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1594             raise SkipTest("test needs cxx and SciPy")
1595         kern_sym = theano.tensor.tensor4('kern')
1596         top_sym = theano.tensor.tensor4('top')
1597         for imshp, kshp, topshp, pad in zip(self.img_shape, self.kern_shape, self.topgrad_shape, self.border_mode):
1598             kern = np.random.random(kshp).astype(theano.config.floatX)
1599             top = np.random.random(topshp).astype(theano.config.floatX)
1600             asymmetric_conv_op = self.conv2d_gradi(border_mode=pad, subsample=(1, 1),
1601                                                    filter_dilation=(1, 1))
1602             asymmetric_out_sym = asymmetric_conv_op(kern_sym, top_sym, imshp[-2:])
1603             asymmetric_func = theano.function([kern_sym, top_sym], asymmetric_out_sym, mode=self.mode)
1604             assert any([isinstance(node.op, self.conv2d_gradi_op)
1605                         for node in asymmetric_func.maker.fgraph.toposort()])
1606             asymmetric_output = asymmetric_func(kern, top)
1607             ref_conv_op = self.conv2d_gradi(border_mode="valid", subsample=(1, 1),
1608                                             filter_dilation=(1, 1))
1609             exp_imshp = [imshp[2] + pad[0][0] + pad[0][1],
1610                          imshp[3] + pad[1][0] + pad[1][1]]
1611             ref_out_sym = ref_conv_op(kern_sym, top_sym, exp_imshp)
1612             ref_func = theano.function([kern_sym, top_sym], ref_out_sym, mode=self.mode)
1613             ref_output = ref_func(kern, top)
1614             ref_output = ref_output[:, :, pad[0][0]:imshp[2] + pad[0][0],
1615                                     pad[1][0]:imshp[3] + pad[1][0]]
1616             utt.assert_allclose(asymmetric_output, ref_output)
1617             def conv_gradinputs(filters_val, output_val):
1618                 return asymmetric_conv_op(filters_val, output_val, tensor.as_tensor_variable(imshp[-2:]))
1619             utt.verify_grad(conv_gradinputs, [kern, top], mode=self.mode, eps=1)
1620 class TestCausalConv(unittest.TestCase):
1621     mode = theano.compile.mode.Mode(optimizer='None')
1622     img = np.array([[[2, 4, 9, 5, 8], [0, 0, 4, 0, 5]],
1623                     [[2, 5, 8, 5, 5], [1, 3, 0, 7, 9]],
1624                     [[7, 0, 7, 1, 0], [0, 1, 4, 7, 2]]]).astype(theano.config.floatX)
1625     kern = np.array([[[5, 3, 1], [3, 1, 0]],
1626                      [[6, 4, 9], [2, 2, 7]]]).astype(theano.config.floatX)
1627     dilation = 2
1628     precomp_top = np.array([[[10, 20, 63, 37, 88], [12, 24, 70, 46, 120]],
1629                             [[13, 34, 47, 64, 78], [14, 36, 58, 70, 105]],
1630                             [[35, 3, 68, 27, 38], [42, 2, 78, 22, 103]]]).astype(theano.config.floatX)
1631     def test_interface(self):
1632         img_sym = theano.tensor.tensor3('img')
1633         kern_sym = theano.tensor.tensor3('kern')
1634         if theano.config.cxx == "" or not theano.tensor.nnet.abstract_conv.imported_scipy_signal:
1635             raise SkipTest("SciPy and cxx needed")
1636         sym_out = causal_conv1d(img_sym, kern_sym, self.kern.shape, filter_dilation=self.dilation)
1637         causal_func = theano.function([img_sym, kern_sym], sym_out, mode=self.mode)
1638         output = causal_func(self.img, self.kern)
1639         utt.assert_allclose(output, self.precomp_top)
1640         def causal_conv_fn(inputs_val, filters_val):
1641             return causal_conv1d(inputs_val, filters_val, self.kern.shape, filter_dilation=1)
1642         utt.verify_grad(causal_conv_fn, [self.img, self.kern], mode=self.mode, eps=1)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>check_dnn_conv.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
2 import math
3 import sys
4 from itertools import product, chain
5 import nose
6 import numpy as np
7 from nose.plugins.skip import SkipTest
8 import theano
9 import theano.tests.unittest_tools as utt
10 from theano.compat import ifilter
11 from theano.configdefaults import SUPPORTED_DNN_CONV_ALGO_RUNTIME
12 from theano.gpuarray import cudnn_defs
13 from theano.gpuarray.dnn import (GpuDnnConv, GpuDnnConvGradW, GpuDnnConvGradI, version,
14                                  _dnn_conv as dnn_conv, _dnn_gradinput as dnn_gradinput,
15                                  _dnn_gradweight as dnn_gradweight)
16 from theano.gpuarray.tests.config import mode_with_gpu, ref_cast
17 from theano.tensor.nnet.abstract_conv import get_conv_output_shape, assert_conv_shape
18 from theano.tensor.nnet.corr import CorrMM, CorrMM_gradInputs, CorrMM_gradWeights
19 from theano.tensor.nnet.corr3d import Corr3dMM, Corr3dMM_gradInputs, Corr3dMM_gradWeights
20 def check_dtype_config_support(dtype, precision):
21     inputs =</b></font> theano.shared(np.zeros((1, 1, 2, 2), dtype=dtype))
22     filters = theano.shared(np.zeros((1, 1, 2, 2), dtype=dtype))
23     conv = dnn_conv(inputs, filters, precision=precision, algo='small')
24     f = theano.function([], conv, mode=mode_with_gpu)
25     try:
26         f()
27     except RuntimeError as e:
28         assert 'CUDNN_STATUS_ARCH_MISMATCH' in e.message
29         return False
30     return True
31 cudnn = cudnn_defs.get_definitions(version(raises=False))
32 class ConvCase:
33     FWD, GRADINPUT, GRADWEIGHT = 0, 1, 2
34     def __init__(self, type,
35                  inputs_shape, filters_shape,
36                  algo=None, dtype=None, precision=None,
37                  subsample=None, dilation=None, border_mode='valid',
38                  conv_mode='conv', alpha=1, beta=0,
39                  should_fail=False):
40         assert type in (ConvCase.FWD, ConvCase.GRADINPUT, ConvCase.GRADWEIGHT)
41         assert len(inputs_shape) == len(filters_shape) in (4, 5)
42         ndim = len(inputs_shape) - 2
43         if dtype is None:
44             dtype = theano.config.floatX
45         if precision is None:
46             precision = theano.config.floatX
47         if subsample is None:
48             subsample = (1,) * ndim
49         if dilation is None:
50             dilation = (1,) * ndim
51         assert dtype in ('float16', 'float32', 'float64')
52         assert precision in ('float16', 'float32', 'float64')
53         assert len(subsample) == len(dilation) == ndim
54         assert (border_mode in ('valid', 'full', 'half') or
55                 (isinstance(border_mode, (list, tuple)) and len(border_mode) == ndim))
56         assert conv_mode in ('conv', 'cross')
57         assert alpha != 0
58         self.type = type
59         self.ndim = ndim
60         self.algo = algo
61         self.inputs_shape = inputs_shape
62         self.filters_shape = filters_shape
63         self.dtype = dtype
64         self.precision = precision
65         self.subsample = subsample
66         self.dilation = dilation
67         self.border_mode = border_mode
68         self.conv_mode = conv_mode
69         self.alpha = alpha
70         self.beta = beta
71         self.should_fail = bool(should_fail)
72     def is_fwd(self):
73         return self.type == ConvCase.FWD
74     def is_bwd_filter(self):
75         return self.type == ConvCase.GRADWEIGHT
76     def is_bwd_data(self):
77 <a name="12"></a>        return self.type == ConvCase.GRADINPUT
78     def get_case(self):
79         return (<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>self.algo, self.dtype, self.precision,
80                 (self.inputs_shape, self.filters_shape,
81                  self.subsample, self.dilation, self.border_mode,
82                  self.conv_mode, self.alpha, self.</b></font>beta))
83     @staticmethod
84     def fwd(*args, **kwargs):
85         return ConvCase(ConvCase.FWD, *args, **kwargs)
86     @staticmethod
87     def bwd_filter(*args, **kwargs):
88         return ConvCase(ConvCase.GRADWEIGHT, *args, **kwargs)
89     @staticmethod
90     def bwd_data(*args, **kwargs):
91         return ConvCase(ConvCase.GRADINPUT, *args, **kwargs)
92 class ConvCaseGenerator:
93     def _as_tuple_of_tuples(self, iterable):
94 <a name="14"></a>        return tuple(tuple(sequence) for sequence in iterable)
95     def __init__(self, ndim,
96                  alpha<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=2, beta=-3, batch_size=2, input_channels=3, inputs_sizes=None, output_channels=2,
97                  filters_sizes=None, subsamples=None, dilations=None, borders=None,
98                  with_border_valid=True, with_border_half=True, with_border_full=</b></font>True):
99         self.ndim = int(ndim)
100         self.alpha = float(alpha)
101         self.beta = float(beta)
102         self.batch_size = int(batch_size)
103         self.input_channels = int(input_channels)
104         self.output_channels = int(output_channels)
105         assert self.ndim in (2, 3)
106         assert self.alpha != 0
107         assert self.batch_size &gt; 0
108         assert self.input_channels &gt; 0
109         assert self.output_channels &gt; 0
110         if inputs_sizes is None:
111             inputs_sizes = ((5,) * self.ndim,
112                             (300, 5) + (2,) * (self.ndim - 2))
113         if filters_sizes is None:
114             filters_sizes = ((4,) * self.ndim,
115                              (40, 4) + (2,) * (self.ndim - 2))
116         if borders is None:
117             borders = ((1,) * self.ndim,
118                        tuple(range(1, self.ndim + 1)))
119         if subsamples is None:
120             subsamples = ((1,) * self.ndim,
121                           tuple(range(1, self.ndim + 1)))
122         if dilations is None:
123             dilations = ((1,) * self.ndim,)
124             if cudnn.version &gt;= 6:
125                 dilations += (tuple(range(1, self.ndim + 1)),)
126         for sequence_list in (inputs_sizes, filters_sizes, borders, subsamples, dilations):
127             assert (isinstance(sequence_list, (tuple, list)) and
128                     all(isinstance(sequence, (tuple, list)) and len(sequence) == self.ndim
129                         for sequence in sequence_list)), (self.ndim, sequence_list)
130         self.auto_borders = tuple()
131         if with_border_valid:
132             self.auto_borders += ('valid',)
133         if with_border_half:
134             self.auto_borders += ('half',)
135 <a name="4"></a>        if with_border_full:
136             self.auto_borders += ('full',)
137         self<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.inputs_sizes = self._as_tuple_of_tuples(inputs_sizes)
138         self.filters_sizes = self._as_tuple_of_tuples(filters_sizes)
139         self.borders = self._as_tuple_of_tuples(borders)
140         self.subsamples = self._as_tuple_of_tuples(subsamples)
141         self.dilations =</b></font> self._as_tuple_of_tuples(dilations)
142     @staticmethod
143     def get_if_valid_conv_output_shape(case_tuple):
144         out_shp = get_conv_output_shape(case_tuple[0],  # input shape
145                                         case_tuple[1],  # filter shape
146                                         case_tuple[4],  # border mode
147                                         case_tuple[2],  # subsample
148                                         case_tuple[3])  # dilation
149         try:
150             return assert_conv_shape(out_shp)
151         except ValueError:
152             return False
153     def get_cases(self, filter=None):
154         all_batch_sizes <font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= (self.batch_size,)
155         all_input_channels = (self.input_channels,)
156         all_input_sizes = self.inputs_sizes
157         all_output_channels = (self.output_channels,)
158         all_filter_sizes = self.filters_sizes
159 <a name="7"></a>        all_subsamples =</b></font> self.subsamples
160         all_dilations = self.dilations
161         all_border_modes = self.auto_borders + self.borders
162         all_conv_modes = (<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'conv', 'cross')
163         all_alphas = (self.alpha,)
164         all_betas = (0,) if self.beta == 0 else (0, self.beta)
165         all_input_shapes = ((bs, ic) + ins
166                             for bs in all_batch_sizes for ic in all_input_channels for ins in all_input_sizes)
167         all_filter_shapes = ((oc</b></font>, ic) + fis
168                              for oc in all_output_channels for ic in all_input_channels for fis in all_filter_sizes)
169         if callable(filter):
170             def local_filter(case_tuple):
171                 return ConvCaseGenerator.get_if_valid_conv_output_shape(case_tuple) and filter(case_tuple)
172         else:
173             local_filter = ConvCaseGenerator.get_if_valid_conv_output_shape
174         return ifilter(local_filter,
175                        product(all_input_shapes, all_filter_shapes, all_subsamples, all_dilations,
176                                all_border_modes, all_conv_modes, all_alphas, all_betas))
177 class ConvCaseGeneratorChain:
178     def __init__(self, *conv_case_generators):
179         assert all(isinstance(g, ConvCaseGenerator) for g in conv_case_generators)
180         self.generators = conv_case_generators
181     def get_cases(self, filter=None):
182         return chain(*[generator.get_cases(filter) for generator in self.generators])
183 class CuDNNV51ConvCaseGenerator(object):
184     NONE = 'none'
185     FFT = 'fft'
186     FFT_TILING = 'fft_tiling'
187     WINOGRAD = 'winograd'
188     WINOGRAD_NON_FUSED = 'winograd_non_fused'
189     def _dilations(self, ndim):
190         return [(1,) * ndim]
191     def _fwd_fft(self, ndim):
192         inputs_sizes = [(10,) * ndim,
193                         (240, 5) + (2,) * (ndim - 2)]
194         filters_sizes = [tuple(range(9, 9 - ndim, -1))]
195         subsamples = [(1,) * ndim]
196         return ConvCaseGenerator(ndim=ndim,
197                                  inputs_sizes=inputs_sizes,
198                                  filters_sizes=filters_sizes,
199                                  subsamples=subsamples,
200                                  dilations=self._dilations(ndim))
201     def _fwd_fft_tiling(self, ndim, dtype, precision):
202         if ndim == 2:
203             filters_sizes = [(32, 5)]
204         if ndim == 3:
205             filters_sizes = [(16, 5, 5)]
206         subsamples = [(1,) * ndim]
207         return ConvCaseGenerator(ndim=ndim,
208                                  filters_sizes=filters_sizes,
209                                  subsamples=subsamples,
210                                  dilations=self._dilations(ndim))
211     def _fwd_winograd(self, ndim):
212         filters_sizes = [(3,) * ndim]
213         subsamples = [(1,) * ndim]
214         return ConvCaseGenerator(ndim=ndim,
215                                  filters_sizes=filters_sizes,
216                                  subsamples=subsamples,
217                                  dilations=self._dilations(ndim))
218     def _fwd_winograd_non_fused(self, ndim, dtype, precision):
219         filters_sizes = [(3,) * ndim]
220         if not (dtype == precision == 'float16'):
221             filters_sizes += [(5,) * ndim]
222         subsamples = [(1,) * ndim]
223         return ConvCaseGenerator(ndim=ndim,
224                                  filters_sizes=filters_sizes,
225                                  subsamples=subsamples,
226                                  dilations=self._dilations(ndim))
227     def _gw_fft(self, ndim):
228         return self._fwd_fft(ndim)
229     def _gw_winograd_non_fused(self, ndim, dtype, precision):
230         return self._fwd_winograd_non_fused(ndim, dtype, precision)
231     def _gi_fft(self, ndim):
232         return self._fwd_fft(ndim)
233     def _gi_fft_tiling(self, ndim, dtype, precision):
234         return self._fwd_fft_tiling(ndim, dtype, precision)
235     def _gi_winograd(self, ndim):
236         return self._fwd_winograd(ndim)
237     def _gi_winograd_non_fused(self, ndim, dtype, precision):
238         return self._fwd_winograd_non_fused(ndim, dtype, precision)
239     def _fwd_runtime(self, ndim, dtype, precision):
240         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
241     def _gw_runtime(self, ndim, dtype, precision):
242         return self._fwd_runtime(ndim, dtype, precision)
243     def _gi_runtime(self, ndim, dtype, precision):
244         return self._fwd_runtime(ndim, dtype, precision)
245     def fwd(self, algo, ndim, dtype, precision):
246         if algo == self.FFT:
247             return self._fwd_fft(ndim)
248         if algo == self.FFT_TILING:
249             return self._fwd_fft_tiling(ndim, dtype, precision)
250         if algo == self.WINOGRAD:
251             return self._fwd_winograd(ndim)
252         if algo == self.WINOGRAD_NON_FUSED:
253             return self._fwd_winograd_non_fused(ndim, dtype, precision)
254         if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
255             return self._fwd_runtime(ndim, dtype, precision)
256         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
257     def gw(self, algo, ndim, dtype, precision):
258         if algo == self.FFT:
259             return self._gw_fft(ndim)
260         if algo == self.WINOGRAD_NON_FUSED:
261             return self._gw_winograd_non_fused(ndim, dtype, precision)
262         if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
263             return self._gw_runtime(ndim, dtype, precision)
264         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
265     def gi(self, algo, ndim, dtype, precision):
266         if algo == self.FFT:
267             return self._gi_fft(ndim)
268         if algo == self.FFT_TILING:
269             return self._gi_fft_tiling(ndim, dtype, precision)
270         if algo == self.WINOGRAD:
271             return self._gi_winograd(ndim)
272         if algo == self.WINOGRAD_NON_FUSED:
273             return self._gi_winograd_non_fused(ndim, dtype, precision)
274         if algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
275             return self._gi_runtime(ndim, dtype, precision)
276         return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
277 class CuDNNV6ConvCaseGenerator(CuDNNV51ConvCaseGenerator):
278     def _fwd_none(self, ndim):
279         return ConvCaseGenerator(ndim=ndim)
280     def _fwd_fft_tiling(self, ndim, dtype, precision):
281         if ndim == 2:
282             subsamples = [(1, 1)]
283             generators = []
284             if (dtype, precision) != ('float64', 'float64'):
285                 filters_sizes = [(32, 5), (10, 10)]
286                 borders = [(1, 1), (6, 4)]
287                 generators += [ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim), subsamples=subsamples,
288                                                  filters_sizes=filters_sizes, borders=borders)]
289             filters_sizes = [(256, 1), (5, 1)]
290             borders = [(1, 0), (2, 0)]
291             generators += [ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim), subsamples=subsamples,
292                                              filters_sizes=filters_sizes, borders=borders)]
293             return ConvCaseGeneratorChain(*generators)
294         if ndim == 3:
295             return super(CuDNNV6ConvCaseGenerator, self)._fwd_fft_tiling(ndim, dtype, precision)
296     def _gw_none(self, ndim):
297         return self._fwd_none(ndim)
298     def _gw_fft_tiling(self, ndim):
299         inputs_sizes = [(247, 1), (20, 1)]
300         filters_sizes = [(3, 1), (10, 1)]
301         subsamples = [(1,) * ndim]
302         borders = [(1, 0), (2, 0)]
303         return ConvCaseGenerator(ndim=ndim,
304                                  inputs_sizes=inputs_sizes,
305                                  filters_sizes=filters_sizes,
306                                  subsamples=subsamples,
307                                  borders=borders,
308                                  dilations=self._dilations(ndim))
309     def _gi_none(self, ndim):
310         return self._fwd_none(ndim)
311     def _fwd_runtime(self, ndim, dtype, precision):
312         if ndim == 2 and dtype == precision == 'float16':
313             return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
314         return super(CuDNNV6ConvCaseGenerator, self)._fwd_runtime(ndim, dtype, precision)
315     def _gw_runtime(self, ndim, dtype, precision):
316         if ndim == 2 and dtype == precision == 'float16':
317             return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
318         return super(CuDNNV6ConvCaseGenerator, self)._gw_runtime(ndim, dtype, precision)
319     def _gi_runtime(self, ndim, dtype, precision):
320         if ndim == 2 and dtype == precision == 'float16':
321             return ConvCaseGenerator(ndim=ndim, dilations=self._dilations(ndim))
322         return super(CuDNNV6ConvCaseGenerator, self)._gi_runtime(ndim, dtype, precision)
323     def fwd(self, algo, ndim, dtype, precision):
324         if algo == self.NONE:
325             return self._fwd_none(ndim)
326         return super(CuDNNV6ConvCaseGenerator, self).fwd(algo, ndim, dtype, precision)
327     def gw(self, algo, ndim, dtype, precision):
328         if algo == self.NONE:
329             return self._gw_none(ndim)
330         if algo == self.FFT_TILING:
331             return self._gw_fft_tiling(ndim)
332         return super(CuDNNV6ConvCaseGenerator, self).gw(algo, ndim, dtype, precision)
333     def gi(self, algo, ndim, dtype, precision):
334         if algo == self.NONE:
335             return self._gi_none(ndim)
336         return super(CuDNNV6ConvCaseGenerator, self).gi(algo, ndim, dtype, precision)
337 cudnn_conv_case_generator = CuDNNV51ConvCaseGenerator() if cudnn.version &lt; 6 else CuDNNV6ConvCaseGenerator()
338 class BaseTestDnnConv(object):
339     ndim = 2
340     fwd_algorithms = None
341     bwd_filter_algorithms = None
342     bwd_data_algorithms = None
343     cpu_conv_class = None
344     cpu_gradinput_class = None
345     cpu_gradweight_class = None
346     special_cases = []  # List of special ConvCases.
347     runtime_shapes = []  # Tuple of tuples with format: n_times, (inputs_shape, filters_shape)
348     def _next_ten_exponent(self, val):
349         ten_exponent = 1
350         while val // 10 &gt; 0:
351             ten_exponent += 1
352             val //= 10
353         return ten_exponent
354     def scale_numpy_arrays_inplace(self, A, B, alpha):
355         scale_factor = 1
356         if alpha != 1:
357             scale_factor *= alpha
358         max_a = math.floor(abs(A.max()))
359         max_b = math.floor(abs(B.max()))
360         if max_a or max_b:
361             m_a = self._next_ten_exponent(max_a)
362             m_b = self._next_ten_exponent(max_b)
363             max_m = max(m_a, m_b)
364             scale_factor *= 10 ** max_m
365         if scale_factor != 1:
366             A /= scale_factor
367             B /= scale_factor
368     def get_atol_rtol(self, algo, dtype, precision):
369         if dtype == 'float16':
370             return (5e-2, 5e-2)
371         if algo == 'winograd_non_fused' and dtype == precision == 'float32':
372             return (1e-4, 1e-4)
373         return None, None
374     def __init__(self):
375         utt.seed_rng(1234)
376         self.dtype_configs = cudnn.get_supported_dtype_configs(check_dtype_config_support)
377     def array_like_conv_output(self, inputs_shape, filters_shape, border_mode, subsample, dilation, dtype):
378         out_shp = get_conv_output_shape(inputs_shape, filters_shape, border_mode, subsample, dilation)
379         out_shp = assert_conv_shape(out_shp)
380         return np.random.random(out_shp).astype(dtype)
381 <a name="0"></a>    def run_conv_fwd(self, algo, dtype, precision, parameters):
382         inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters
383         inputs_val <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
384         filters_val = np.random.random(filters_shape).astype(dtype)
385         inputs_val /= 10
386         filters_val /= 10
387         inputs = theano.shared(inputs_val)
388         filters = theano.shared(filters_val)
389         if beta == 0:
390             out = None
391         else:
392 <a name="5"></a>            out =</b></font> self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)
393             out /= 10
394         conv = dnn_conv<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(img=inputs, kerns=filters, alpha=alpha, beta=beta, out=out, border_mode=border_mode,
395                         subsample=subsample, dilation=dilation, conv_mode=conv_mode, algo=algo, precision=precision)
396         f = theano.function([], conv, mode=</b></font>mode_with_gpu)
397         if conv_mode == 'conv':
398             if inputs.ndim == 5:
399                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
400             else:
401                 flipped_filters = filters[:, :, ::-1, ::-1]
402         else:
403             flipped_filters = filters
404         conv_ref = self.cpu_conv_class(border_mode=border_mode,
405                                        subsample=subsample,
406                                        filter_dilation=dilation)(ref_cast(inputs), flipped_filters)
407         f_ref = theano.function([], conv_ref, mode="FAST_RUN")
408         res_ref = f_ref()
409         res = np.asarray(f())
410         if algo in cudnn.deterministic_fwd_algorithms:
411             utt.assert_allclose(res, np.asarray(f()))
412         atol, rtol = self.get_atol_rtol(algo, dtype, precision)
413         if beta == 0:
414             cpu_res = alpha * res_ref
415         else:
416             cpu_res = alpha * res_ref + beta * out
417         self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
418         utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)
419     def run_conv_gradinput(self, algo, dtype, precision, parameters):
420         inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters
421         if beta == 0:
422             inputs_val = None
423         else:
424             inputs_val = np.random.random(inputs_shape).astype(dtype)
425             inputs_val /= 10
426         filters_val = np.random.random(filters_shape).astype(dtype)
427         topgrad_val = self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)
428         filters_val /= 10
429         topgrad_val /= 10
430         filters = theano.shared(filters_val)
431 <a name="10"></a>        topgrad = theano.shared(topgrad_val)
432         grad_i = dnn_gradinput<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(filters, topgrad, inputs_shape, alpha=alpha, beta=beta, out=inputs_val,
433                                border_mode=border_mode, subsample=subsample, dilation=dilation, conv_mode=conv_mode,
434                                algo=algo, precision=precision)
435         f = theano.function([], grad_i, mode=</b></font>mode_with_gpu)
436         if conv_mode == 'conv':
437             if filters.ndim == 5:
438                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
439             else:
440                 flipped_filters = filters[:, :, ::-1, ::-1]
441         else:
442             flipped_filters = filters
443         grad_i_ref = self.cpu_gradinput_class(border_mode=border_mode,
444                                               subsample=subsample,
445                                               filter_dilation=dilation
446                                               )(ref_cast(flipped_filters), ref_cast(topgrad), inputs_shape[2:])
447         f_ref = theano.function([], grad_i_ref, mode="FAST_RUN")
448         res_ref = f_ref()
449         res = np.asarray(f())
450         if algo in cudnn.deterministic_bwd_data_algorithms:
451             utt.assert_allclose(res, np.asarray(f()))
452         atol, rtol = self.get_atol_rtol(algo, dtype, precision)
453         if beta == 0:
454             cpu_res = alpha * res_ref
455         else:
456             cpu_res = alpha * res_ref + beta * inputs_val
457         self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
458         utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)
459     def run_conv_gradweight(self, algo, dtype, precision, parameters):
460         inputs_shape, filters_shape, subsample, dilation, border_mode, conv_mode, alpha, beta = parameters
461         inputs_val = np.random.random(inputs_shape).astype(dtype)
462         if beta == 0:
463             filters_val = None
464         else:
465             filters_val = np.random.random(filters_shape).astype(dtype)
466             filters_val /= 10
467         topgrad_val = self.array_like_conv_output(inputs_shape, filters_shape, border_mode, subsample, dilation, dtype)
468         inputs_val /= 10
469         topgrad_val /= 10
470         inputs = theano.shared(inputs_val)
471 <a name="8"></a>        topgrad = theano.shared(topgrad_val)
472         grad_w = dnn_gradweight<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(inputs, topgrad, filters_shape, alpha=alpha, beta=beta, out=filters_val,
473                                 border_mode=border_mode, subsample=subsample, dilation=dilation, conv_mode=conv_mode,
474                                 algo=algo, precision=precision)
475         f = theano.function([], grad_w, mode=mode_with_gpu)
476         grad_w_ref =</b></font> self.cpu_gradweight_class(border_mode=border_mode,
477                                                subsample=subsample,
478                                                filter_dilation=dilation)(ref_cast(inputs), ref_cast(topgrad),
479                                                                          filters_shape[2:])
480         if conv_mode == 'conv':
481             if inputs.ndim == 5:
482                 grad_w_ref = grad_w_ref[:, :, ::-1, ::-1, ::-1]
483             else:
484                 grad_w_ref = grad_w_ref[:, :, ::-1, ::-1]
485         f_ref = theano.function([], grad_w_ref, mode="FAST_RUN")
486         res_ref = f_ref()
487         res = np.asarray(f())
488         if algo in cudnn.deterministic_bwd_filter_algorithms:
489             utt.assert_allclose(res, np.asarray(f()))
490         atol, rtol = self.get_atol_rtol(algo, dtype, precision)
491         if beta == 0:
492             cpu_res = alpha * res_ref
493         else:
494             cpu_res = alpha * res_ref + beta * filters_val
495         self.scale_numpy_arrays_inplace(cpu_res, res, alpha)
496         utt.assert_allclose(cpu_res, res, rtol=rtol, atol=atol)
497     def should_fail(self, function, *args):
498         try:
499             print('(should fail)', file=sys.stderr, end=' ')
500             function(*args)
501         except Exception:
502             pass
503         else:
504             raise AssertionError('Should fail', callable.__name__, *args)
505     def should_fail_fwd(self, *args):
506         self.should_fail(self.run_conv_fwd, *args)
507     def should_fail_gradinput(self, *args):
508         self.should_fail(self.run_conv_gradinput, *args)
509     def should_fail_gradweight(self, *args):
510         self.should_fail(self.run_conv_gradweight, *args)
511     def get_expected_tcount(self):
512         return (sum(1 for t in self.test_fwd()) +
513                 sum(1 for t in self.test_gradweight()) +
514                 sum(1 for t in self.test_gradinput()) +
515                 sum(1 for t in self.test_fwd_runtime_algorithms()) +
516                 sum(1 for t in self.test_gradweight_runtime_algorithms()) +
517                 sum(1 for t in self.test_gradinput_runtime_algorithms()))
518     def test_fwd(self):
519         for dtype, precision in self.dtype_configs:
520             algos = [algo for algo in self.fwd_algorithms
521                      if cudnn.fwd_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
522             for algo in algos:
523                 for parameters in cudnn_conv_case_generator.fwd(algo, self.ndim, dtype, precision).get_cases():
524                     yield (self.run_conv_fwd, algo, dtype, precision, parameters)
525             if algos:
526                 for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
527                     for parameters in cudnn_conv_case_generator.fwd(algo, self.ndim, dtype, precision).get_cases():
528                         yield (self.run_conv_fwd, algo, dtype, precision, parameters)
529         for dnn_case in self.special_cases:
530             if dnn_case.is_fwd():
531                 if dnn_case.should_fail:
532                     yield (self.should_fail_fwd,) + dnn_case.get_case()
533                 else:
534                     yield (self.run_conv_fwd,) + dnn_case.get_case()
535     def test_gradinput(self):
536         for dtype, precision in self.dtype_configs:
537             algos = [algo for algo in self.bwd_data_algorithms
538                      if cudnn.bwd_data_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
539             for algo in algos:
540                 for parameters in cudnn_conv_case_generator.gi(algo, self.ndim, dtype, precision).get_cases():
541                     yield (self.run_conv_gradinput, algo, dtype, precision, parameters)
542             if algos:
543                 for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
544                     for parameters in cudnn_conv_case_generator.gi(algo, self.ndim, dtype, precision).get_cases():
545                         yield (self.run_conv_gradinput, algo, dtype, precision, parameters)
546         for dnn_case in self.special_cases:
547             if dnn_case.is_bwd_data():
548                 if dnn_case.should_fail:
549                     yield (self.should_fail_gradinput,) + dnn_case.get_case()
550                 else:
551                     yield (self.run_conv_gradinput,) + dnn_case.get_case()
552     def test_gradweight(self):
553         for dtype, precision in self.dtype_configs:
554             algos = [algo for algo in self.bwd_filter_algorithms
555                      if cudnn.bwd_filter_algo_supports_dtype_config(algo, dtype, precision, self.ndim)]
556             for algo in algos:
557                 for parameters in cudnn_conv_case_generator.gw(algo, self.ndim, dtype, precision).get_cases():
558                     yield (self.run_conv_gradweight, algo, dtype, precision, parameters)
559             if algos:
560                 for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
561                     for parameters in cudnn_conv_case_generator.gw(algo, self.ndim, dtype, precision).get_cases():
562                         yield (self.run_conv_gradweight, algo, dtype, precision, parameters)
563         for dnn_case in self.special_cases:
564             if dnn_case.is_bwd_filter():
565                 if dnn_case.should_fail:
566                     yield (self.should_fail_gradweight,) + dnn_case.get_case()
567                 else:
568                     yield (self.run_conv_gradweight,) + dnn_case.get_case()
569     def test_fwd_runtime_algorithms(self):
570         dtype = 'float32'
571         unit_shape = (1,) * self.ndim
572         _broadcastable = [False] * (2 + self.ndim)
573         def run_fwd_runtime_algorithm(algo):
574             inputs = theano.tensor.TensorType(dtype, _broadcastable)()
575 <a name="16"></a>            filters = theano.tensor.TensorType(dtype, _broadcastable)()
576             lower_inputs <font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= inputs / 10
577             lower_filters = filters / 10
578             conv = dnn_conv(img=lower_inputs, kerns=lower_filters, algo=algo, precision=dtype,
579                             subsample=unit_shape, dilation=unit_shape)
580             f = theano.function(</b></font>[inputs, filters], conv, mode=mode_with_gpu)
581             if self.ndim == 3:
582                 flipped_filters = lower_filters[:, :, ::-1, ::-1, ::-1]
583             else:
584                 flipped_filters = lower_filters[:, :, ::-1, ::-1]
585             conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(lower_inputs), flipped_filters)
586             f_ref = theano.function([inputs, filters], conv_ref, mode='FAST_RUN')
587             runtime_shapes = self.runtime_shapes
588             if algo in ('time_once', 'guess_once'):
589                 runtime_shapes = [list(runtime_shapes[0])]
590                 runtime_shapes[0][0] = 5
591 <a name="9"></a>            for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
592                 print('Shapes:', inputs_shape, filters_shape)
593                 for i in range(ntimes):
594                     inputs_val <font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
595                     filters_val = np.random.random(filters_shape).astype(dtype)
596                     gpu_res = np.asarray(</b></font>f(inputs_val, filters_val))
597                     cpu_res = f_ref(inputs_val, filters_val)
598                     self.scale_numpy_arrays_inplace(cpu_res, gpu_res, 1)
599                     utt.assert_allclose(cpu_res, gpu_res)
600         for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
601             yield (run_fwd_runtime_algorithm, algo)
602     def test_gradinput_runtime_algorithms(self):
603         dtype = 'float32'
604         unit_shape = (1,) * self.ndim
605         _broadcastable = [False] * (2 + self.ndim)
606         def run_gradinput_runtime_algorithm(algo):
607             theano.config.dnn.conv.algo_bwd_data = algo
608             inputs = theano.tensor.TensorType(dtype, _broadcastable)()
609             filters = theano.tensor.TensorType(dtype, _broadcastable)()
610             conv = dnn_conv(img=inputs, kerns=filters, algo=algo, precision=dtype,
611                             subsample=unit_shape, dilation=unit_shape)
612             grad_i = theano.tensor.grad(conv.sum(), [inputs])
613             f = theano.function([inputs, filters], grad_i, mode=mode_with_gpu)
614             assert 1 == len([node for node in f.maker.fgraph.apply_nodes if isinstance(node.op, GpuDnnConvGradI)])
615             assert not any(isinstance(node.op, GpuDnnConv) for node in f.maker.fgraph.apply_nodes)
616             assert not any(isinstance(node.op, GpuDnnConvGradW) for node in f.maker.fgraph.apply_nodes)
617             if self.ndim == 3:
618                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
619             else:
620                 flipped_filters = filters[:, :, ::-1, ::-1]
621             conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(inputs), flipped_filters)
622             grad_i_ref = theano.tensor.grad(conv_ref.sum(), [inputs])
623             f_ref = theano.function([inputs, filters], grad_i_ref, mode='FAST_RUN')
624             runtime_shapes = self.runtime_shapes
625             if algo in ('time_once', 'guess_once'):
626                 runtime_shapes = [list(runtime_shapes[0])]
627                 runtime_shapes[0][0] = 5
628 <a name="15"></a>            for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
629                 print('Shapes:', inputs_shape, filters_shape)
630                 for i in range(ntimes):
631                     inputs_val <font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= np.random.random(inputs_shape).astype(dtype)
632                     filters_val = np.random.random(filters_shape).astype(dtype)
633                     gpu_res =</b></font> f(inputs_val, filters_val)
634                     cpu_res = f_ref(inputs_val, filters_val)
635                     utt.assert_allclose(cpu_res, np.asarray(gpu_res))
636         for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
637             yield (run_gradinput_runtime_algorithm, algo)
638     def test_gradweight_runtime_algorithms(self):
639         dtype = 'float32'
640         unit_shape = (1,) * self.ndim
641         _broadcastable = [False] * (2 + self.ndim)
642         def run_gradweight_runtime_algorithm(algo):
643             theano.config.dnn.conv.algo_bwd_filter = algo
644             inputs = theano.tensor.TensorType(dtype, _broadcastable)()
645             filters = theano.tensor.TensorType(dtype, _broadcastable)()
646             conv = dnn_conv(img=inputs, kerns=filters, algo=algo, precision=dtype,
647                             subsample=unit_shape, dilation=unit_shape)
648             grad_w = theano.tensor.grad(conv.sum(), [filters])
649             f = theano.function([inputs, filters], grad_w, mode=mode_with_gpu)
650             assert 1 == len([node for node in f.maker.fgraph.apply_nodes if isinstance(node.op, GpuDnnConvGradW)])
651             assert not any(isinstance(node.op, GpuDnnConv) for node in f.maker.fgraph.apply_nodes)
652             assert not any(isinstance(node.op, GpuDnnConvGradI) for node in f.maker.fgraph.apply_nodes)
653             if self.ndim == 3:
654                 flipped_filters = filters[:, :, ::-1, ::-1, ::-1]
655             else:
656                 flipped_filters = filters[:, :, ::-1, ::-1]
657             conv_ref = self.cpu_conv_class(subsample=unit_shape)(ref_cast(inputs), flipped_filters)
658             grad_w_ref = theano.tensor.grad(conv_ref.sum(), [filters])
659             f_ref = theano.function([inputs, filters], grad_w_ref, mode='FAST_RUN')
660             runtime_shapes = self.runtime_shapes
661             if algo in ('time_once', 'guess_once'):
662                 runtime_shapes = [list(runtime_shapes[0])]
663                 runtime_shapes[0][0] = 5
664             for ntimes, (inputs_shape, filters_shape) in runtime_shapes:
665                 print('Shapes:', inputs_shape, filters_shape)
666                 for i in range(ntimes):
667                     inputs_val = np.random.random(inputs_shape).astype(dtype)
668                     filters_val = np.random.random(filters_shape).astype(dtype)
669                     gpu_res = f(inputs_val, filters_val)
670                     cpu_res = f_ref(inputs_val, filters_val)
671                     utt.assert_allclose(cpu_res, np.asarray(gpu_res))
672         for algo in SUPPORTED_DNN_CONV_ALGO_RUNTIME:
673             yield (run_gradweight_runtime_algorithm, algo)
674 <a name="13"></a>class TestDnnConv2D(BaseTestDnnConv):
675     ndim = 2
676     fwd_algorithms <font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= cudnn.cudnnConvolutionFwdAlgo_t.get_aliases()
677     bwd_filter_algorithms = cudnn.cudnnConvolutionBwdFilterAlgo_t.get_aliases()
678     bwd_data_algorithms = cudnn.cudnnConvolutionBwdDataAlgo_t.get_aliases()
679     cpu_conv_class =</b></font> CorrMM
680     cpu_gradinput_class = CorrMM_gradInputs
681     cpu_gradweight_class = CorrMM_gradWeights
682     special_cases = [ConvCase.bwd_filter(algo='deterministic', dtype='float32', precision='float32',
683                                          inputs_shape=(1, 1, 541211, 10), filters_shape=(50, 1, 3, 10),
684                                          border_mode=(1, 0), should_fail=(cudnn.version &lt;= 6)),
685                      ConvCase.fwd(algo='small', dtype='float32', precision='float32',
686                                   inputs_shape=(65536, 2, 2, 2), filters_shape=(1, 2, 2, 2)),
687                      ConvCase.fwd(algo='small', dtype='float32', precision='float32',
688 <a name="1"></a>                                  inputs_shape=(65537, 2, 2, 2), filters_shape=(1, 2, 2, 2))]
689     runtime_shapes = [
690         <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, [(2, 3, 10, 9), (5, 3, 7, 7)]),
691         (1, [(1, 1, 100, 200), (1, 1, 50, 200)]),
692         (1, [(4, 2, 20, 20), (2, 2, 20, 19)]),
693         (3, [(2, 3, 10, 9), (5, 3, 7, 7)]),  # cache should be used
694         (1, [(2, 2, 50, 50), (5, 2, 25, 31)]),
695         (1</b></font>, [(1, 1, 100, 200), (1, 1, 50, 200)]),  # cache should be used
696         (1, [(4, 2, 20, 20), (2, 2, 20, 19)]),  # cache should be used
697         (1, [(1, 2, 3, 4), (6, 2, 2, 1)])
698     ]
699 class TestDnnConv3D(BaseTestDnnConv):
700     ndim = 3
701     fwd_algorithms = cudnn.conv3d_fwd_algorithms
702     bwd_filter_algorithms = cudnn.conv3d_bwd_filter_algorithms
703     bwd_data_algorithms = cudnn.conv3d_bwd_data_algorithms
704     cpu_conv_class = Corr3dMM
705     cpu_gradinput_class = Corr3dMM_gradInputs
706     cpu_gradweight_class = Corr3dMM_gradWeights
707     special_cases = [ConvCase.fwd(algo='small', dtype='float32', precision='float32',
708                                   inputs_shape=(65536, 2, 2, 2, 2), filters_shape=(1, 2, 2, 2, 2)),
709                      ConvCase.fwd(algo='small', dtype='float32', precision='float32',
710 <a name="3"></a>                                  inputs_shape=(65537, 2, 2, 2, 2), filters_shape=(1, 2, 2, 2, 2))]
711     runtime_shapes = [
712         <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(3, [(2, 3, 5, 10, 9), (5, 3, 4, 7, 7)]),
713 <a name="6"></a>        (1, [(1, 1, 5, 100, 200), (1, 1, 4, 50, 200)]),
714         (1, [(4, 2, 20, 20, 20), (2, 2, 20, 19, 18)]),
715         (3, [(2, 3, 5, 10, 9), (5, 3, 4, 7, 7)]),  # cache should be used
716         (1</b></font>, [<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(2, 2, 50, 50, 5), (5, 2, 25, 31, 4)]),
717         (1, [(1, 1, 5, 100, 200), (1, 1, 4, 50, 200)]),  # cache should be used
718         (1, [(4, 2, 20, 20, 20), (2, 2, 20, 19, 18)]),  # cache should be used
719         (1, [(1, 2, 3, 4, 5), (6</b></font>, 2, 3, 2, 1)])
720     ]
721 def test_true_half_config_support():
722     if not check_dtype_config_support('float16', 'float16'):
723         raise SkipTest('FWD: TRUE_HALF_CONFIG not supported on this GPU.')
724 class CheckDnn:
725     @staticmethod
726     def dtype_config_to_str(dtype_config):
727         dtype, precision = dtype_config
728         if dtype == precision == 'float16':
729             return 'TRUE_HALF_CONFIG'
730         if dtype == 'float16' and precision == 'float32':
731             return 'PSEUDO_HALF_CONFIG'
732         if dtype == precision == 'float32':
733             return 'FLOAT_CONFIG'
734         if dtype == precision == 'float64':
735             return 'DOUBLE_CONFIG'
736         raise ValueError('unknown data type configuration', dtype_config)
737     @staticmethod
738     def print_infos(count_tests=True):
739         test_2d = TestDnnConv2D()
740         test_3d = TestDnnConv3D()
741         print()
742         print('Available data type configurations:',
743               ', '.join(CheckDnn.dtype_config_to_str(d)
744                         for d in cudnn.get_supported_dtype_configs(check_dtype_config_support)))
745         print()
746         print('2D algorithms:')
747         print('FWD        :', ', '.join(test_2d.fwd_algorithms))
748         print('BWD FILTER :', ', '.join(test_2d.bwd_filter_algorithms))
749         print('BWD DATA   :', ', '.join(test_2d.bwd_data_algorithms))
750         print()
751         print('3D algorithms:')
752         print('FWD        :', ', '.join(test_3d.fwd_algorithms))
753         print('BWD FILTER :', ', '.join(test_3d.bwd_filter_algorithms))
754         print('BWD DATA   :', ', '.join(test_3d.bwd_data_algorithms))
755         print()
756         if count_tests:
757             count_tests_2d = test_2d.get_expected_tcount()
758             count_tests_3d = test_3d.get_expected_tcount()
759             print(count_tests_2d, 'conv2D test cases.')
760             print(count_tests_3d, 'conv3D test cases.')
761             print('1 supplementary test.')
762             print(count_tests_2d + count_tests_3d + 1, 'total conv tests.')
763             print()
764     @staticmethod
765     def print_tests():
766         for test in (TestDnnConv2D(), TestDnnConv3D()):
767             for tcase in test.test_fwd():
768                 print(tcase[0].__name__, *tcase[1:])
769             for tcase in test.test_gradinput():
770                 print(tcase[0].__name__, *tcase[1:])
771             for tcase in test.test_gradweight():
772                 print(tcase[0].__name__, *tcase[1:])
773             for tcase in test.test_fwd_runtime_algorithms():
774                 print(tcase[0].__name__, *tcase[1:])
775             for tcase in test.test_gradinput_runtime_algorithms():
776                 print(tcase[0].__name__, *tcase[1:])
777             for tcase in test.test_gradweight_runtime_algorithms():
778                 print(tcase[0].__name__, *tcase[1:])
779         print(test_true_half_config_support.__name__)
780 if __name__ == '__main__':
781     args = sys.argv[1:]
782     if len(args) == 1 and args[0] in ('infos', 'list'):
783         if args[0] == 'infos':
784             CheckDnn.print_infos()
785         if args[0] == 'list':
786             CheckDnn.print_tests()
787     else:
788         module_name = sys.modules[__name__].__file__
789         if len(args) == 0:
790             args = ['--verbose', '--nocapture']
791         argv = [sys.argv[0], module_name] + args
792         CheckDnn.print_infos()
793         nose.main(argv=argv)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
