<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML><HEAD><TITLE>Matches for test_algorithm.py & test_quarters_estimates.py</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
</HEAD>
<body>
  <div style="align-items: center; display: flex; justify-content: space-around;">
    <div>
      <h3 align="center">
Matches for test_algorithm.py & test_quarters_estimates.py
      </h3>
      <h1 align="center">
        12.3%
      </h1>
      <center>
        <a href="index.html" target="_top">
          INDEX
        </a>
        <span>-</span>
        <a href="help-en.html" target="_top">
          HELP
        </a>
      </center>
    </div>
    <div>
<TABLE BORDER="1" CELLSPACING="0" BGCOLOR="#d0d0d0">
<TR><TH><TH>test_algorithm.py (10.85359%)<TH>test_quarters_estimates.py (14.4%)<TH>Tokens
<TR><TD BGCOLOR="#0000ff"><FONT COLOR="#0000ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#0',2,'match48-1.html#0',3)" NAME="0">(2095-2131)<TD><A HREF="javascript:ZweiFrames('match48-0.html#0',2,'match48-1.html#0',3)" NAME="0">(2608-2621)</A><TD ALIGN=center><FONT COLOR="#ff0000">26</FONT>
<TR><TD BGCOLOR="#f63526"><FONT COLOR="#f63526">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#1',2,'match48-1.html#1',3)" NAME="1">(15-40)<TD><A HREF="javascript:ZweiFrames('match48-0.html#1',2,'match48-1.html#1',3)" NAME="1">(1-49)</A><TD ALIGN=center><FONT COLOR="#e10000">23</FONT>
<TR><TD BGCOLOR="#980517"><FONT COLOR="#980517">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#2',2,'match48-1.html#2',3)" NAME="2">(2275-2296)<TD><A HREF="javascript:ZweiFrames('match48-0.html#2',2,'match48-1.html#2',3)" NAME="2">(2646-2658)</A><TD ALIGN=center><FONT COLOR="#d70000">22</FONT>
<TR><TD BGCOLOR="#53858b"><FONT COLOR="#53858b">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#3',2,'match48-1.html#3',3)" NAME="3">(1159-1174)<TD><A HREF="javascript:ZweiFrames('match48-0.html#3',2,'match48-1.html#3',3)" NAME="3">(1900-1908)</A><TD ALIGN=center><FONT COLOR="#cd0000">21</FONT>
<TR><TD BGCOLOR="#6cc417"><FONT COLOR="#6cc417">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#4',2,'match48-1.html#4',3)" NAME="4">(803-807)<TD><A HREF="javascript:ZweiFrames('match48-0.html#4',2,'match48-1.html#4',3)" NAME="4">(1241-1251)</A><TD ALIGN=center><FONT COLOR="#ba0000">19</FONT>
<TR><TD BGCOLOR="#151b8d"><FONT COLOR="#151b8d">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#5',2,'match48-1.html#5',3)" NAME="5">(2160-2169)<TD><A HREF="javascript:ZweiFrames('match48-0.html#5',2,'match48-1.html#5',3)" NAME="5">(1787-1793)</A><TD ALIGN=center><FONT COLOR="#b00000">18</FONT>
<TR><TD BGCOLOR="#8c8774"><FONT COLOR="#8c8774">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#6',2,'match48-1.html#6',3)" NAME="6">(1384-1402)<TD><A HREF="javascript:ZweiFrames('match48-0.html#6',2,'match48-1.html#6',3)" NAME="6">(1867-1873)</A><TD ALIGN=center><FONT COLOR="#b00000">18</FONT>
<TR><TD BGCOLOR="#38a4a5"><FONT COLOR="#38a4a5">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#7',2,'match48-1.html#7',3)" NAME="7">(205-219)<TD><A HREF="javascript:ZweiFrames('match48-0.html#7',2,'match48-1.html#7',3)" NAME="7">(1049-1054)</A><TD ALIGN=center><FONT COLOR="#b00000">18</FONT>
<TR><TD BGCOLOR="#c58917"><FONT COLOR="#c58917">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#8',2,'match48-1.html#8',3)" NAME="8">(1324-1329)<TD><A HREF="javascript:ZweiFrames('match48-0.html#8',2,'match48-1.html#8',3)" NAME="8">(1729-1735)</A><TD ALIGN=center><FONT COLOR="#a60000">17</FONT>
<TR><TD BGCOLOR="#83a33a"><FONT COLOR="#83a33a">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#9',2,'match48-1.html#9',3)" NAME="9">(1174-1179)<TD><A HREF="javascript:ZweiFrames('match48-0.html#9',2,'match48-1.html#9',3)" NAME="9">(2213-2218)</A><TD ALIGN=center><FONT COLOR="#a60000">17</FONT>
<TR><TD BGCOLOR="#ad5910"><FONT COLOR="#ad5910">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#10',2,'match48-1.html#10',3)" NAME="10">(778-791)<TD><A HREF="javascript:ZweiFrames('match48-0.html#10',2,'match48-1.html#10',3)" NAME="10">(1668-1674)</A><TD ALIGN=center><FONT COLOR="#a60000">17</FONT>
<TR><TD BGCOLOR="#b041ff"><FONT COLOR="#b041ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#11',2,'match48-1.html#11',3)" NAME="11">(2325-2332)<TD><A HREF="javascript:ZweiFrames('match48-0.html#11',2,'match48-1.html#11',3)" NAME="11">(1799-1804)</A><TD ALIGN=center><FONT COLOR="#9c0000">16</FONT>
<TR><TD BGCOLOR="#571b7e"><FONT COLOR="#571b7e">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#12',2,'match48-1.html#12',3)" NAME="12">(1522-1531)<TD><A HREF="javascript:ZweiFrames('match48-0.html#12',2,'match48-1.html#12',3)" NAME="12">(1880-1886)</A><TD ALIGN=center><FONT COLOR="#9c0000">16</FONT>
<TR><TD BGCOLOR="#3b9c9c"><FONT COLOR="#3b9c9c">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#13',2,'match48-1.html#13',3)" NAME="13">(1213-1218)<TD><A HREF="javascript:ZweiFrames('match48-0.html#13',2,'match48-1.html#13',3)" NAME="13">(1346-1352)</A><TD ALIGN=center><FONT COLOR="#9c0000">16</FONT>
<TR><TD BGCOLOR="#842dce"><FONT COLOR="#842dce">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#14',2,'match48-1.html#14',3)" NAME="14">(2190-2195)<TD><A HREF="javascript:ZweiFrames('match48-0.html#14',2,'match48-1.html#14',3)" NAME="14">(1909-1913)</A><TD ALIGN=center><FONT COLOR="#930000">15</FONT>
<TR><TD BGCOLOR="#f52887"><FONT COLOR="#f52887">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#15',2,'match48-1.html#15',3)" NAME="15">(634-638)<TD><A HREF="javascript:ZweiFrames('match48-0.html#15',2,'match48-1.html#15',3)" NAME="15">(1810-1815)</A><TD ALIGN=center><FONT COLOR="#930000">15</FONT>
<TR><TD BGCOLOR="#2981b2"><FONT COLOR="#2981b2">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#16',2,'match48-1.html#16',3)" NAME="16">(4174-4186)<TD><A HREF="javascript:ZweiFrames('match48-0.html#16',2,'match48-1.html#16',3)" NAME="16">(1466-1478)</A><TD ALIGN=center><FONT COLOR="#890000">14</FONT>
<TR><TD BGCOLOR="#3090c7"><FONT COLOR="#3090c7">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#17',2,'match48-1.html#17',3)" NAME="17">(3172-3180)<TD><A HREF="javascript:ZweiFrames('match48-0.html#17',2,'match48-1.html#17',3)" NAME="17">(744-750)</A><TD ALIGN=center><FONT COLOR="#890000">14</FONT>
<TR><TD BGCOLOR="#800517"><FONT COLOR="#800517">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#18',2,'match48-1.html#18',3)" NAME="18">(1143-1147)<TD><A HREF="javascript:ZweiFrames('match48-0.html#18',2,'match48-1.html#18',3)" NAME="18">(1776-1781)</A><TD ALIGN=center><FONT COLOR="#890000">14</FONT>
<TR><TD BGCOLOR="#f62817"><FONT COLOR="#f62817">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#19',2,'match48-1.html#19',3)" NAME="19">(935-953)<TD><A HREF="javascript:ZweiFrames('match48-0.html#19',2,'match48-1.html#19',3)" NAME="19">(1975-1978)</A><TD ALIGN=center><FONT COLOR="#890000">14</FONT>
<TR><TD BGCOLOR="#4e9258"><FONT COLOR="#4e9258">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#20',2,'match48-1.html#20',3)" NAME="20">(718-721)<TD><A HREF="javascript:ZweiFrames('match48-0.html#20',2,'match48-1.html#20',3)" NAME="20">(969-975)</A><TD ALIGN=center><FONT COLOR="#890000">14</FONT>
<TR><TD BGCOLOR="#947010"><FONT COLOR="#947010">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#21',2,'match48-1.html#21',3)" NAME="21">(3602-3607)<TD><A HREF="javascript:ZweiFrames('match48-0.html#21',2,'match48-1.html#21',3)" NAME="21">(1204-1210)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#4cc417"><FONT COLOR="#4cc417">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#22',2,'match48-1.html#22',3)" NAME="22">(2555-2562)<TD><A HREF="javascript:ZweiFrames('match48-0.html#22',2,'match48-1.html#22',3)" NAME="22">(2491-2498)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#f660ab"><FONT COLOR="#f660ab">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#23',2,'match48-1.html#23',3)" NAME="23">(2513-2517)<TD><A HREF="javascript:ZweiFrames('match48-0.html#23',2,'match48-1.html#23',3)" NAME="23">(2530-2536)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#79764d"><FONT COLOR="#79764d">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#24',2,'match48-1.html#24',3)" NAME="24">(2495-2499)<TD><A HREF="javascript:ZweiFrames('match48-0.html#24',2,'match48-1.html#24',3)" NAME="24">(2477-2483)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#5eac10"><FONT COLOR="#5eac10">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#25',2,'match48-1.html#25',3)" NAME="25">(1557-1588)<TD><A HREF="javascript:ZweiFrames('match48-0.html#25',2,'match48-1.html#25',3)" NAME="25">(250-252)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#68818b"><FONT COLOR="#68818b">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#26',2,'match48-1.html#26',3)" NAME="26">(1209-1213)<TD><A HREF="javascript:ZweiFrames('match48-0.html#26',2,'match48-1.html#26',3)" NAME="26">(609-615)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#e77471"><FONT COLOR="#e77471">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#27',2,'match48-1.html#27',3)" NAME="27">(686-691)<TD><A HREF="javascript:ZweiFrames('match48-0.html#27',2,'match48-1.html#27',3)" NAME="27">(925-930)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#717d7d"><FONT COLOR="#717d7d">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#28',2,'match48-1.html#28',3)" NAME="28">(164-168)<TD><A HREF="javascript:ZweiFrames('match48-0.html#28',2,'match48-1.html#28',3)" NAME="28">(1625-1629)</A><TD ALIGN=center><FONT COLOR="#7f0000">13</FONT>
<TR><TD BGCOLOR="#af7a82"><FONT COLOR="#af7a82">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#29',2,'match48-1.html#29',3)" NAME="29">(4282-4296)<TD><A HREF="javascript:ZweiFrames('match48-0.html#29',2,'match48-1.html#29',3)" NAME="29">(1194-1197)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#ae694a"><FONT COLOR="#ae694a">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#30',2,'match48-1.html#30',3)" NAME="30">(4149-4157)<TD><A HREF="javascript:ZweiFrames('match48-0.html#30',2,'match48-1.html#30',3)" NAME="30">(2427-2430)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#3ea99f"><FONT COLOR="#3ea99f">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#31',2,'match48-1.html#31',3)" NAME="31">(3037-3046)<TD><A HREF="javascript:ZweiFrames('match48-0.html#31',2,'match48-1.html#31',3)" NAME="31">(100-107)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#5b8daf"><FONT COLOR="#5b8daf">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#32',2,'match48-1.html#32',3)" NAME="32">(2773-2775)<TD><A HREF="javascript:ZweiFrames('match48-0.html#32',2,'match48-1.html#32',3)" NAME="32">(1424-1437)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#736aff"><FONT COLOR="#736aff">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#33',2,'match48-1.html#33',3)" NAME="33">(2435-2438)<TD><A HREF="javascript:ZweiFrames('match48-0.html#33',2,'match48-1.html#33',3)" NAME="33">(775-781)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#827d6b"><FONT COLOR="#827d6b">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#34',2,'match48-1.html#34',3)" NAME="34">(2360-2364)<TD><A HREF="javascript:ZweiFrames('match48-0.html#34',2,'match48-1.html#34',3)" NAME="34">(1586-1589)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#41a317"><FONT COLOR="#41a317">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#35',2,'match48-1.html#35',3)" NAME="35">(1403-1420)<TD><A HREF="javascript:ZweiFrames('match48-0.html#35',2,'match48-1.html#35',3)" NAME="35">(1575-1579)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#ff00ff"><FONT COLOR="#ff00ff">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#36',2,'match48-1.html#36',3)" NAME="36">(1329-1333)<TD><A HREF="javascript:ZweiFrames('match48-0.html#36',2,'match48-1.html#36',3)" NAME="36">(2231-2232)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
<TR><TD BGCOLOR="#810541"><FONT COLOR="#810541">-</FONT><TD><A HREF="javascript:ZweiFrames('match48-0.html#37',2,'match48-1.html#37',3)" NAME="37">(1220-1221)<TD><A HREF="javascript:ZweiFrames('match48-0.html#37',2,'match48-1.html#37',3)" NAME="37">(2139-2142)</A><TD ALIGN=center><FONT COLOR="#750000">12</FONT>
</TABLE>
    </div>
  </div>
  <hr>
  <div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_algorithm.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
#
# Copyright 2018 Quantopian, Inc.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<A NAME="1"></A># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
<FONT color="#f63526"><A HREF="javascript:ZweiFrames('match48-1.html#1',3,'match48-top.html#1',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>import warnings
import datetime
from datetime import timedelta
from functools import partial
from textwrap import dedent
from copy import deepcopy

import logbook
import toolz
from logbook import TestHandler, WARNING
from nose_parameterized import parameterized
from six import iteritems, itervalues, string_types
from six.moves import range
from testfixtures import TempDirectory

import numpy as np
import pandas as pd
import pytz
from pandas.core.common import PerformanceWarning
from trading_calendars import get_calendar, register_calendar

import zipline.api
from zipline.api import FixedSlippage
from zipline.assets import Equity, Future, Asset
from zipline.assets.continuous_futures import ContinuousFuture
from</B></FONT> zipline.assets.synthetic import (
    make_jagged_equity_info,
    make_simple_equity_info,
)
from zipline.errors import (
    AccountControlViolation,
    CannotOrderDelistedAsset,
    IncompatibleSlippageModel,
    RegisterTradingControlPostInit,
    ScheduleFunctionInvalidCalendar,
    SetCancelPolicyPostInit,
    SymbolNotFound,
    TradingControlViolation,
    UnsupportedCancelPolicy,
    UnsupportedDatetimeFormat,
    ZeroCapitalError
)

from zipline.finance.commission import PerShare, PerTrade
from zipline.finance.execution import LimitOrder
from zipline.finance.order import ORDER_STATUS
from zipline.finance.trading import SimulationParameters
from zipline.finance.asset_restrictions import (
    Restriction,
    HistoricalRestrictions,
    StaticRestrictions,
    RESTRICTION_STATES,
)
from zipline.finance.controls import AssetDateBounds
from zipline.testing import (
    FakeDataPortal,
    create_daily_df_for_asset,
    create_data_portal_from_trade_history,
    create_minute_df_for_asset,
    make_test_handler,
    make_trade_data_for_asset_info,
    parameter_space,
    str_to_seconds,
    to_utc,
)
from zipline.testing import RecordBatchBlotter
import zipline.testing.fixtures as zf
from zipline.test_algorithms import (
    access_account_in_init,
    access_portfolio_in_init,
    api_algo,
    api_get_environment_algo,
    api_symbol_algo,
    handle_data_api,
    handle_data_noop,
    initialize_api,
    initialize_noop,
    noop_algo,
    record_float_magic,
    record_variables,
    call_with_kwargs,
    call_without_kwargs,
    call_with_bad_kwargs_current,
    call_with_bad_kwargs_history,
    bad_type_history_assets,
    bad_type_history_fields,
    bad_type_history_bar_count,
    bad_type_history_frequency,
    bad_type_history_assets_kwarg_list,
    bad_type_current_assets,
    bad_type_current_fields,
    bad_type_can_trade_assets,
    bad_type_is_stale_assets,
    bad_type_history_assets_kwarg,
    bad_type_history_fields_kwarg,
    bad_type_history_bar_count_kwarg,
    bad_type_history_frequency_kwarg,
    bad_type_current_assets_kwarg,
    bad_type_current_fields_kwarg,
    call_with_bad_kwargs_get_open_orders,
    call_with_good_kwargs_get_open_orders,
    call_with_no_kwargs_get_open_orders,
    empty_positions,
    no_handle_data,
)
from zipline.testing.predicates import assert_equal
from zipline.utils.api_support import ZiplineAPI
from zipline.utils.context_tricks import CallbackManager, nop_context
from zipline.utils.events import (
    date_rules,
    time_rules,
    Always,
    ComposedRule,
    Never,
    OncePerDay,
)
import zipline.utils.factory as factory

# Because test cases appear to reuse some resources.


_multiprocess_can_split_ = False


class TestRecord(zf.WithMakeAlgo, zf.ZiplineTestCase):
    ASSET_FINDER_EQUITY_SIDS = (133,)
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False

    def test_record_incr(self):

        def initialize(self):
            self.incr = 0

        def handle_data(self, data):
            self.incr += 1
            self.record(incr=self.incr)
            name = 'name'
            self.record(name, self.incr)
            zipline.api.record(name, self.incr, 'name2', 2, name3=self.incr)

        output = self.run_algorithm(
            initialize=initialize,
            handle_data=handle_data,
        )

<A NAME="28"></A>        np.testing.assert_array_equal(output['incr'].values,
                                      range(1, len(output) + 1))
        np.testing.assert_array_equal(output['name'].values,
                                      range(1, len<FONT color="#717d7d"><A HREF="javascript:ZweiFrames('match48-1.html#28',3,'match48-top.html#28',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>(output) + 1))
        np.testing.assert_array_equal(output['name2'].values,
                                      [2] * len(output))
        np.testing.assert_array_equal(output['name3'].values,
                                      range(</B></FONT>1, len(output) + 1))


class TestMiscellaneousAPI(zf.WithMakeAlgo, zf.ZiplineTestCase):

    START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
    END_DATE = pd.Timestamp('2006-01-04', tz='UTC')
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    sids = 1, 2

    # FIXME: Pass a benchmark source instead of this.
    BENCHMARK_SID = None

    @classmethod
    def make_equity_info(cls):
        return pd.concat((
            make_simple_equity_info(cls.sids, '2002-02-1', '2007-01-01'),
            pd.DataFrame.from_dict(
                {3: {'symbol': 'PLAY',
                     'start_date': '2002-01-01',
                     'end_date': '2004-01-01',
                     'exchange': 'TEST'},
                 4: {'symbol': 'PLAY',
                     'start_date': '2005-01-01',
                     'end_date': '2006-01-01',
                     'exchange': 'TEST'}},
                orient='index',
            ),
        ))

    @classmethod
    def make_futures_info(cls):
        return pd.DataFrame.from_dict(
            {
<A NAME="7"></A>                5: {
                    'symbol': 'CLG06',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2005-12-01', tz<FONT color="#38a4a5"><A HREF="javascript:ZweiFrames('match48-1.html#7',3,'match48-top.html#7',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>='UTC'),
                    'notice_date': pd.Timestamp('2005-12-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-01-20', tz='UTC'),
                    'exchange': 'TEST'
                },
                6: {
                    'root_symbol': 'CL',
                    'symbol': 'CLK06',
                    'start_date': pd.Timestamp('2005-12-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2006-03-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-04-20', tz='UTC'),
                    'exchange': 'TEST',
                },
                7: {
                    'symbol'</B></FONT>: 'CLQ06',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2005-12-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2006-06-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-07-20', tz='UTC'),
                    'exchange': 'TEST',
                },
                8: {
                    'symbol': 'CLX06',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2006-02-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2006-09-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-10-20', tz='UTC'),
                    'exchange': 'TEST',
                }
            },
            orient='index',
        )

    def test_cancel_policy_outside_init(self):
        code = &quot;&quot;&quot;
from zipline.api import cancel_policy, set_cancel_policy

def initialize(algo):
    pass

def handle_data(algo, data):
    set_cancel_policy(cancel_policy.NeverCancel())
&quot;&quot;&quot;
        algo = self.make_algo(script=code)
        with self.assertRaises(SetCancelPolicyPostInit):
            algo.run()

    def test_cancel_policy_invalid_param(self):
        code = &quot;&quot;&quot;
from zipline.api import set_cancel_policy

def initialize(algo):
    set_cancel_policy(&quot;foo&quot;)

def handle_data(algo, data):
    pass
&quot;&quot;&quot;
        algo = self.make_algo(script=code)
        with self.assertRaises(UnsupportedCancelPolicy):
            algo.run()

    def test_zipline_api_resolves_dynamically(self):
        # Make a dummy algo.
        algo = self.make_algo(
            initialize=lambda context: None,
            handle_data=lambda context, data: None,
        )

        # Verify that api methods get resolved dynamically by patching them out
        # and then calling them
        for method in algo.all_api_methods():
            name = method.__name__
            sentinel = object()

            def fake_method(*args, **kwargs):
                return sentinel
            setattr(algo, name, fake_method)
            with ZiplineAPI(algo):
                self.assertIs(sentinel, getattr(zipline.api, name)())

    def test_sid_datetime(self):
        algo_text = &quot;&quot;&quot;
from zipline.api import sid, get_datetime

def initialize(context):
    pass

def handle_data(context, data):
    aapl_dt = data.current(sid(1), &quot;last_traded&quot;)
    assert_equal(aapl_dt, get_datetime())
&quot;&quot;&quot;
        self.run_algorithm(
            script=algo_text,
            namespace={'assert_equal': self.assertEqual},
        )

    def test_datetime_bad_params(self):
        algo_text = &quot;&quot;&quot;
from zipline.api import get_datetime
from pytz import timezone

def initialize(context):
    pass

def handle_data(context, data):
    get_datetime(timezone)
&quot;&quot;&quot;
        algo = self.make_algo(script=algo_text)
        with self.assertRaises(TypeError):
            algo.run()

    @parameterized.expand([
        (-1000, 'invalid_base'),
        (0, 'invalid_base'),
    ])
    def test_invalid_capital_base(self, cap_base, name):
        &quot;&quot;&quot;
        Test that the appropriate error is being raised and orders aren't
        filled for algos with capital base &lt;= 0
        &quot;&quot;&quot;
        algo_text = &quot;&quot;&quot;
def initialize(context):
    pass

def handle_data(context, data):
    order(sid(24), 1000)
        &quot;&quot;&quot;
        sim_params = SimulationParameters(
            start_session=pd.Timestamp(&quot;2006-01-03&quot;, tz='UTC'),
            end_session=pd.Timestamp(&quot;2006-01-06&quot;, tz='UTC'),
            capital_base=cap_base,
            data_frequency=&quot;minute&quot;,
            trading_calendar=self.trading_calendar
        )

        with self.assertRaises(ZeroCapitalError) as exc:
            # make_algo will trace to TradingAlgorithm,
            # where the exception will be raised
            self.make_algo(script=algo_text, sim_params=sim_params)
        # Make sure the correct error was raised
        error = exc.exception
        self.assertEqual(str(error),
                         'initial capital base must be greater than zero')

    def test_get_environment(self):
        expected_env = {
            'arena': 'backtest',
            'data_frequency': 'minute',
            'start': pd.Timestamp('2006-01-03 14:31:00+0000', tz='utc'),
            'end': pd.Timestamp('2006-01-04 21:00:00+0000', tz='utc'),
            'capital_base': 100000.0,
            'platform': 'zipline'
        }

        def initialize(algo):
            self.assertEqual('zipline', algo.get_environment())
            self.assertEqual(expected_env, algo.get_environment('*'))

        def handle_data(algo, data):
            pass

        self.run_algorithm(initialize=initialize, handle_data=handle_data)

    def test_get_open_orders(self):
        def initialize(algo):
            algo.minute = 0

        def handle_data(algo, data):
            if algo.minute == 0:

                # Should be filled by the next minute
                algo.order(algo.sid(1), 1)

                # Won't be filled because the price is too low.
                algo.order(
                    algo.sid(2), 1, style=LimitOrder(0.01, asset=algo.sid(2))
                )
                algo.order(
                    algo.sid(2), 1, style=LimitOrder(0.01, asset=algo.sid(2))
                )
                algo.order(
                    algo.sid(2), 1, style=LimitOrder(0.01, asset=algo.sid(2))
                )

                all_orders = algo.get_open_orders()
                self.assertEqual(list(all_orders.keys()), [1, 2])

                self.assertEqual(all_orders[1], algo.get_open_orders(1))
                self.assertEqual(len(all_orders[1]), 1)

                self.assertEqual(all_orders[2], algo.get_open_orders(2))
                self.assertEqual(len(all_orders[2]), 3)

            if algo.minute == 1:
                # First order should have filled.
                # Second order should still be open.
                all_orders = algo.get_open_orders()
                self.assertEqual(list(all_orders.keys()), [2])

                self.assertEqual([], algo.get_open_orders(1))

                orders_2 = algo.get_open_orders(2)
                self.assertEqual(all_orders[2], orders_2)
                self.assertEqual(len(all_orders[2]), 3)

                for order_ in orders_2:
                    algo.cancel_order(order_)

                all_orders = algo.get_open_orders()
                self.assertEqual(all_orders, {})

            algo.minute += 1

        self.run_algorithm(initialize=initialize, handle_data=handle_data)

    def test_schedule_function_custom_cal(self):
        # run a simulation on the CMES cal, and schedule a function
        # using the NYSE cal
        algotext = &quot;&quot;&quot;
from zipline.api import (
    schedule_function, get_datetime, time_rules, date_rules, calendars,
)

def initialize(context):
    schedule_function(
        func=log_nyse_open,
        date_rule=date_rules.every_day(),
        time_rule=time_rules.market_open(),
        calendar=calendars.US_EQUITIES,
    )

    schedule_function(
        func=log_nyse_close,
        date_rule=date_rules.every_day(),
        time_rule=time_rules.market_close(),
        calendar=calendars.US_EQUITIES,
    )

    context.nyse_opens = []
    context.nyse_closes = []

def log_nyse_open(context, data):
    context.nyse_opens.append(get_datetime())

def log_nyse_close(context, data):
    context.nyse_closes.append(get_datetime())
        &quot;&quot;&quot;

        algo = self.make_algo(
            script=algotext,
            sim_params=self.make_simparams(
                trading_calendar=get_calendar(&quot;CMES&quot;),
            )
        )
        algo.run()

        nyse = get_calendar(&quot;NYSE&quot;)

        for minute in algo.nyse_opens:
            # each minute should be a nyse session open
            session_label = nyse.minute_to_session_label(minute)
            session_open = nyse.session_open(session_label)
            self.assertEqual(session_open, minute)

        for minute in algo.nyse_closes:
            # each minute should be a minute before a nyse session close
            session_label = nyse.minute_to_session_label(minute)
            session_close = nyse.session_close(session_label)
            self.assertEqual(session_close - timedelta(minutes=1), minute)

        # Test that passing an invalid calendar parameter raises an error.
        erroring_algotext = dedent(
            &quot;&quot;&quot;
            from zipline.api import schedule_function
            from trading_calendars import get_calendar

            def initialize(context):
                schedule_function(func=my_func, calendar=get_calendar('XNYS'))

            def my_func(context, data):
                pass
            &quot;&quot;&quot;
        )

        algo = self.make_algo(
            script=erroring_algotext,
            sim_params=self.make_simparams(
                trading_calendar=get_calendar(&quot;CMES&quot;),
            ),
        )

        with self.assertRaises(ScheduleFunctionInvalidCalendar):
            algo.run()

    def test_schedule_function(self):
        us_eastern = pytz.timezone('US/Eastern')

        def incrementer(algo, data):
            algo.func_called += 1
            curdt = algo.get_datetime().tz_convert(pytz.utc)
            self.assertEqual(
                curdt,
                us_eastern.localize(
                    datetime.datetime.combine(
                        curdt.date(),
                        datetime.time(9, 31)
                    ),
                ),
            )

        def initialize(algo):
            algo.func_called = 0
            algo.days = 1
            algo.date = None
            algo.schedule_function(
                func=incrementer,
                date_rule=date_rules.every_day(),
                time_rule=time_rules.market_open(),
            )

        def handle_data(algo, data):
            if not algo.date:
                algo.date = algo.get_datetime().date()

            if algo.date &lt; algo.get_datetime().date():
                algo.days += 1
                algo.date = algo.get_datetime().date()

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
        )
        algo.run()

        self.assertEqual(algo.func_called, algo.days)

    def test_event_context(self):
        expected_data = []
        collected_data_pre = []
        collected_data_post = []
        function_stack = []

        def pre(data):
            function_stack.append(pre)
            collected_data_pre.append(data)

        def post(data):
            function_stack.append(post)
            collected_data_post.append(data)

        def initialize(context):
            context.add_event(Always(), f)
            context.add_event(Always(), g)

        def handle_data(context, data):
            function_stack.append(handle_data)
            expected_data.append(data)

        def f(context, data):
            function_stack.append(f)

        def g(context, data):
            function_stack.append(g)

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            create_event_context=CallbackManager(pre, post),
        )
        algo.run()

        self.assertEqual(len(expected_data), 780)
        self.assertEqual(collected_data_pre, expected_data)
        self.assertEqual(collected_data_post, expected_data)

        self.assertEqual(
            len(function_stack),
            3900,
            'Incorrect number of functions called: %s != 3900' %
            len(function_stack),
        )
        expected_functions = [pre, handle_data, f, g, post] * 97530
        for n, (f, g) in enumerate(zip(function_stack, expected_functions)):
            self.assertEqual(
                f,
                g,
                'function at position %d was incorrect, expected %s but got %s'
                % (n, g.__name__, f.__name__),
            )

    @parameterized.expand([
        ('daily',),
        ('minute'),
    ])
    def test_schedule_function_rule_creation(self, mode):
        def nop(*args, **kwargs):
            return None

        self.sim_params.data_frequency = mode
        algo = self.make_algo(
            initialize=nop,
            handle_data=nop,
            sim_params=self.sim_params,
        )

        # Schedule something for NOT Always.
        # Compose two rules to ensure calendar is set properly.
        algo.schedule_function(nop, time_rule=Never() &amp; Always())

        event_rule = algo.event_manager._events[1].rule
        self.assertIsInstance(event_rule, OncePerDay)
        self.assertEqual(event_rule.cal, algo.trading_calendar)

        inner_rule = event_rule.rule
        self.assertIsInstance(inner_rule, ComposedRule)
        self.assertEqual(inner_rule.cal, algo.trading_calendar)

        first = inner_rule.first
        second = inner_rule.second
        composer = inner_rule.composer

        self.assertIsInstance(first, Always)
        self.assertEqual(first.cal, algo.trading_calendar)
        self.assertEqual(second.cal, algo.trading_calendar)

        if mode == 'daily':
<A NAME="15"></A>            self.assertIsInstance(second, Always)
        else:
            self.assertIsInstance(second, ComposedRule)
            self<FONT color="#f52887"><A HREF="javascript:ZweiFrames('match48-1.html#15',3,'match48-top.html#15',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.assertIsInstance(second.first, Never)
            self.assertEqual(second.first.cal, algo.trading_calendar)

            self.assertIsInstance(second.second, Always)
            self.assertEqual(second.second.</B></FONT>cal, algo.trading_calendar)

        self.assertIs(composer, ComposedRule.lazy_and)

    def test_asset_lookup(self):
        algo = self.make_algo()

        # this date doesn't matter
        start_session = pd.Timestamp(&quot;2000-01-01&quot;, tz=&quot;UTC&quot;)

        # Test before either PLAY existed
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2001-12-01', tz='UTC')
        )
        with self.assertRaises(SymbolNotFound):
            algo.symbol('PLAY')
        with self.assertRaises(SymbolNotFound):
            algo.symbols('PLAY')

        # Test when first PLAY exists
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2002-12-01', tz='UTC')
        )
        list_result = algo.symbols('PLAY')
        self.assertEqual(3, list_result[0])

        # Test after first PLAY ends
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2004-12-01', tz='UTC')
        )
        self.assertEqual(3, algo.symbol('PLAY'))

        # Test after second PLAY begins
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2005-12-01', tz='UTC')
        )
        self.assertEqual(4, algo.symbol('PLAY'))

        # Test after second PLAY ends
        algo.sim_params = algo.sim_params.create_new(
            start_session,
<A NAME="27"></A>            pd.Timestamp('2006-12-01', tz='UTC')
        )
        self.assertEqual(4, algo.symbol('PLAY'))
        list_result = algo<FONT color="#e77471"><A HREF="javascript:ZweiFrames('match48-1.html#27',3,'match48-top.html#27',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.symbols('PLAY')
        self.assertEqual(4, list_result[0])

        # Test lookup SID
        self.assertIsInstance(algo.sid(3), Equity)
        self.assertIsInstance(algo.sid(</B></FONT>4), Equity)

        # Supplying a non-string argument to symbol()
        # should result in a TypeError.
        with self.assertRaises(TypeError):
            algo.symbol(1)

        with self.assertRaises(TypeError):
            algo.symbol((1,))

        with self.assertRaises(TypeError):
            algo.symbol({1})

        with self.assertRaises(TypeError):
            algo.symbol([1])

        with self.assertRaises(TypeError):
            algo.symbol({'foo': 'bar'})

    def test_future_symbol(self):
        &quot;&quot;&quot; Tests the future_symbol API function.
        &quot;&quot;&quot;
        algo = self.make_algo()
        algo.datetime = pd.Timestamp('2006-12-01', tz='UTC')
<A NAME="20"></A>
        # Check that we get the correct fields for the CLG06 symbol
        cl = algo.future_symbol('CLG06')
        self<FONT color="#4e9258"><A HREF="javascript:ZweiFrames('match48-1.html#20',3,'match48-top.html#20',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.assertEqual(cl.sid, 5)
        self.assertEqual(cl.symbol, 'CLG06')
        self.assertEqual(cl.root_symbol, 'CL')
        self.assertEqual(cl.start_date, pd.Timestamp(</B></FONT>'2005-12-01', tz='UTC'))
        self.assertEqual(cl.notice_date, pd.Timestamp('2005-12-20', tz='UTC'))
        self.assertEqual(cl.expiration_date,
                         pd.Timestamp('2006-01-20', tz='UTC'))

        with self.assertRaises(SymbolNotFound):
            algo.future_symbol('')

        with self.assertRaises(SymbolNotFound):
            algo.future_symbol('PLAY')

        with self.assertRaises(SymbolNotFound):
            algo.future_symbol('FOOBAR')

        # Supplying a non-string argument to future_symbol()
        # should result in a TypeError.
        with self.assertRaises(TypeError):
            algo.future_symbol(1)

        with self.assertRaises(TypeError):
            algo.future_symbol((1,))

        with self.assertRaises(TypeError):
            algo.future_symbol({1})

        with self.assertRaises(TypeError):
            algo.future_symbol([1])

        with self.assertRaises(TypeError):
            algo.future_symbol({'foo': 'bar'})


class TestSetSymbolLookupDate(zf.WithMakeAlgo, zf.ZiplineTestCase):
    #     January 2006
    # Su Mo Tu We Th Fr Sa
    #  1  2  3  4  5  6  7
    #  8  9 10 11 12 13 14
    # 15 16 17 18 19 20 21
    # 22 23 24 25 26 27 28
    # 29 30 31
    START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
    END_DATE = pd.Timestamp('2006-01-06', tz='UTC')
    SIM_PARAMS_START_DATE = pd.Timestamp('2006-01-04', tz='UTC')
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    BENCHMARK_SID = 3

    @classmethod
    def make_equity_info(cls):
        dates = pd.date_range(cls.START_DATE, cls.END_DATE)
        assert len(dates) == 4, &quot;Expected four dates.&quot;

        # Two assets with the same ticker, ending on days[1] and days[3], plus
        # a benchmark that spans the whole period.
<A NAME="10"></A>        cls.sids = [1, 2, 3]
        cls.asset_starts = [dates[0], dates[2]]
        cls.asset_ends = [dates[1], dates[3]]
        return pd<FONT color="#ad5910"><A HREF="javascript:ZweiFrames('match48-1.html#10',3,'match48-top.html#10',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.DataFrame.from_records([
            {'symbol': 'DUP',
             'start_date': cls.asset_starts[0],
             'end_date': cls.asset_ends[0],
             'exchange': 'TEST',
             'asset_name': 'FIRST'},
            {'symbol': 'DUP',
             'start_date': cls.asset_starts[1],
             'end_date': cls.asset_ends[1],
             'exchange': 'TEST',
             'asset_name': 'SECOND'},
            {'symbol': 'BENCH',
             'start_date': cls.START_DATE,
             'end_date': cls.</B></FONT>END_DATE,
             'exchange': 'TEST',
             'asset_name': 'BENCHMARK'},
        ], index=cls.sids)

    def test_set_symbol_lookup_date(self):
        &quot;&quot;&quot;
        Test the set_symbol_lookup_date API method.
        &quot;&quot;&quot;
<A NAME="4"></A>        set_symbol_lookup_date = zipline.api.set_symbol_lookup_date

        def initialize(context):
            set_symbol_lookup_date(self<FONT color="#6cc417"><A HREF="javascript:ZweiFrames('match48-1.html#4',3,'match48-top.html#4',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.asset_ends[0])
            self.assertEqual(zipline.api.symbol('DUP').sid, self.sids[0])

            set_symbol_lookup_date(self.asset_ends[1])
            self.assertEqual(zipline.api.symbol('DUP').</B></FONT>sid, self.sids[1])

            with self.assertRaises(UnsupportedDatetimeFormat):
                set_symbol_lookup_date('foobar')

        self.run_algorithm(initialize=initialize)


class TestPositions(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-01-06', tz='utc')
    SIM_PARAMS_CAPITAL_BASE = 1000

    ASSET_FINDER_EQUITY_SIDS = (1, 133)

    SIM_PARAMS_DATA_FREQUENCY = 'daily'

    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        frame = pd.DataFrame(
            {
                'open': [90, 95, 100, 105],
                'high': [90, 95, 100, 105],
                'low': [90, 95, 100, 105],
                'close': [90, 95, 100, 105],
                'volume': 100,
            },
            index=cls.equity_daily_bar_days,
        )
        return ((sid, frame) for sid in sids)

    @classmethod
    def make_futures_info(cls):
        return pd.DataFrame.from_dict(
            {
                1000: {
                    'symbol': 'CLF06',
                    'root_symbol': 'CL',
                    'start_date': cls.START_DATE,
                    'end_date': cls.END_DATE,
                    'auto_close_date': cls.END_DATE + cls.trading_calendar.day,
                    'exchange': 'CMES',
                    'multiplier': 100,
                },
            },
            orient='index',
        )

    @classmethod
    def make_future_minute_bar_data(cls):
        trading_calendar = cls.trading_calendars[Future]

        sids = cls.asset_finder.futures_sids
        minutes = trading_calendar.minutes_for_sessions_in_range(
            cls.future_minute_bar_days[0],
            cls.future_minute_bar_days[-1],
        )
        frame = pd.DataFrame(
            {
                'open': 2.0,
                'high': 2.0,
                'low': 2.0,
                'close': 2.0,
                'volume': 100,
            },
            index=minutes,
        )
        return ((sid, frame) for sid in sids)

    def test_portfolio_exited_position(self):
        # This test ensures ensures that 'phantom' positions do not appear in
        # context.portfolio.positions in the case that a position has been
        # entered and fully exited.

        def initialize(context, sids):
            context.ordered = False
            context.exited = False
            context.sids = sids

        def handle_data(context, data):
            if not context.ordered:
                for s in context.sids:
                    context.order(context.sid(s), 1)
                context.ordered = True

            if not context.exited:
                amounts = [pos.amount for pos
                           in itervalues(context.portfolio.positions)]

                if (
                    len(amounts) &gt; 0 and
                    all([(amount == 1) for amount in amounts])
                ):
                    for stock in context.portfolio.positions:
                        context.order(context.sid(stock), -1)
                    context.exited = True

            # Should be 0 when all positions are exited.
            context.record(num_positions=len(context.portfolio.positions))

        result = self.run_algorithm(
            initialize=initialize,
            handle_data=handle_data,
            sids=self.ASSET_FINDER_EQUITY_SIDS,
        )

        expected_position_count = [
            0,  # Before entering the first position
            2,  # After entering, exiting on this date
            0,  # After exiting
            0,
        ]
        for i, expected in enumerate(expected_position_count):
            self.assertEqual(result.ix[i]['num_positions'], expected)

    def test_noop_orders(self):
        asset = self.asset_finder.retrieve_asset(1)

        # Algorithm that tries to buy with extremely low stops/limits and tries
        # to sell with extremely high versions of same. Should not end up with
        # any positions for reasonable data.
        def handle_data(algo, data):

            ########
            # Buys #
<A NAME="19"></A>            ########

            # Buy with low limit, shouldn't trigger.
            algo.order(asset, 100, limit_price<FONT color="#f62817"><A HREF="javascript:ZweiFrames('match48-1.html#19',3,'match48-top.html#19',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>=1)

            # But with high stop, shouldn't trigger
            algo.order(asset, 100, stop_price=10000000)

            # Buy with high limit (should trigger) but also high stop (should
            # prevent trigger).
            algo.order(asset, 100, limit_price=10000000, stop_price=10000000)

            # Buy with low stop (should trigger), but also low limit (should
            # prevent trigger).
            algo.order(asset, 100, limit_price=1, stop_price=1)

            #########
            # Sells #
            #########

            # Sell with high limit, shouldn't trigger.
            algo.order(</B></FONT>asset, -100, limit_price=1000000)

            # Sell with low stop, shouldn't trigger.
            algo.order(asset, -100, stop_price=1)

            # Sell with low limit (should trigger), but also high stop (should
            # prevent trigger).
            algo.order(asset, -100, limit_price=1000000, stop_price=1000000)

            # Sell with low limit (should trigger), but also low stop (should
            # prevent trigger).
            algo.order(asset, -100, limit_price=1, stop_price=1)

            ###################
            # Rounding Checks #
            ###################
            algo.order(asset, 100, limit_price=.00000001)
            algo.order(asset, -100, stop_price=.00000001)

        daily_stats = self.run_algorithm(handle_data=handle_data)

        # Verify that positions are empty for all dates.
        empty_positions = daily_stats.positions.map(lambda x: len(x) == 0)
        self.assertTrue(empty_positions.all())

    def test_position_weights(self):
        sids = (1, 133, 1000)
        equity_1, equity_133, future_1000 = \
            self.asset_finder.retrieve_all(sids)

        def initialize(algo, sids_and_amounts, *args, **kwargs):
            algo.ordered = False
            algo.sids_and_amounts = sids_and_amounts
            algo.set_commission(
                us_equities=PerTrade(0), us_futures=PerTrade(0),
            )
            algo.set_slippage(
                us_equities=FixedSlippage(0),
                us_futures=FixedSlippage(0),
            )

        def handle_data(algo, data):
            if not algo.ordered:
                for s, amount in algo.sids_and_amounts:
                    algo.order(algo.sid(s), amount)
                algo.ordered = True

            algo.record(
                position_weights=algo.portfolio.current_portfolio_weights,
            )

        daily_stats = self.run_algorithm(
            sids_and_amounts=zip(sids, [2, -1, 1]),
            initialize=initialize,
            handle_data=handle_data,
        )

        expected_position_weights = [
            # No positions held on the first day.
            pd.Series({}),
            # Each equity's position value is its price times the number of
            # shares held. In this example, we hold a long position in 2 shares
            # of equity_1 so its weight is (95.0 * 2) = 190.0 divided by the
            # total portfolio value. The total portfolio value is the sum of
            # cash ($905.00) plus the value of all equity positions.
            #
            # For a futures contract, its weight is the unit price times number
            # of shares held times the multiplier. For future_1000, this is
            # (2.0 * 1 * 100) = 200.0 divided by total portfolio value.
            pd.Series({
                equity_1: 190.0 / (190.0 - 95.0 + 905.0),
                equity_133: -95.0 / (190.0 - 95.0 + 905.0),
                future_1000: 200.0 / (190.0 - 95.0 + 905.0),
            }),
            pd.Series({
                equity_1: 200.0 / (200.0 - 100.0 + 905.0),
                equity_133: -100.0 / (200.0 - 100.0 + 905.0),
                future_1000: 200.0 / (200.0 - 100.0 + 905.0),
            }),
            pd.Series({
                equity_1: 210.0 / (210.0 - 105.0 + 905.0),
                equity_133: -105.0 / (210.0 - 105.0 + 905.0),
                future_1000: 200.0 / (210.0 - 105.0 + 905.0),
            }),
        ]

        for i, expected in enumerate(expected_position_weights):
            assert_equal(daily_stats.iloc[i]['position_weights'], expected)


class TestBeforeTradingStart(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-06', tz='utc')
    END_DATE = pd.Timestamp('2016-01-07', tz='utc')
    SIM_PARAMS_CAPITAL_BASE = 10000
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    EQUITY_DAILY_BAR_LOOKBACK_DAYS = EQUITY_MINUTE_BAR_LOOKBACK_DAYS = 1

    DATA_PORTAL_FIRST_TRADING_DAY = pd.Timestamp(&quot;2016-01-05&quot;, tz='UTC')
    EQUITY_MINUTE_BAR_START_DATE = pd.Timestamp(&quot;2016-01-05&quot;, tz='UTC')
    FUTURE_MINUTE_BAR_START_DATE = pd.Timestamp(&quot;2016-01-05&quot;, tz='UTC')

    data_start = ASSET_FINDER_EQUITY_START_DATE = pd.Timestamp(
        '2016-01-05',
        tz='utc',
    )

    SPLIT_ASSET_SID = 3
    ASSET_FINDER_EQUITY_SIDS = 1, 2, SPLIT_ASSET_SID

    @classmethod
    def make_equity_minute_bar_data(cls):
        asset_minutes = \
            cls.trading_calendar.minutes_in_range(
                cls.data_start,
                cls.END_DATE,
            )
        minutes_count = len(asset_minutes)
        minutes_arr = np.arange(minutes_count) + 1
        split_data = pd.DataFrame(
            {
                'open': minutes_arr + 1,
                'high': minutes_arr + 2,
                'low': minutes_arr - 1,
                'close': minutes_arr,
                'volume': 100 * minutes_arr,
            },
            index=asset_minutes,
        )
        split_data.iloc[780:] = split_data.iloc[780:] / 2.0
        for sid in (1, 8554):
            yield sid, create_minute_df_for_asset(
                cls.trading_calendar,
                cls.data_start,
                cls.END_DATE,
            )

        yield 2, create_minute_df_for_asset(
            cls.trading_calendar,
            cls.data_start,
            cls.END_DATE,
            50,
        )
        yield cls.SPLIT_ASSET_SID, split_data

    @classmethod
    def make_splits_data(cls):
        return pd.DataFrame.from_records([
            {
                'effective_date': str_to_seconds('2016-01-07'),
                'ratio': 0.5,
                'sid': cls.SPLIT_ASSET_SID,
            }
        ])

    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        for sid in sids:
            yield sid, create_daily_df_for_asset(
                cls.trading_calendar,
                cls.data_start,
                cls.END_DATE,
            )

    def test_data_in_bts_minute(self):
        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import record, sid
        def initialize(context):
            context.history_values = []

        def before_trading_start(context, data):
            record(the_price1=data.current(sid(1), &quot;price&quot;))
            record(the_high1=data.current(sid(1), &quot;high&quot;))
            record(the_price2=data.current(sid(2), &quot;price&quot;))
            record(the_high2=data.current(sid(2), &quot;high&quot;))

            context.history_values.append(data.history(
                [sid(1), sid(2)],
                [&quot;price&quot;, &quot;high&quot;],
                60,
                &quot;1m&quot;
            ))

        def handle_data(context, data):
            pass
        &quot;&quot;&quot;)

        algo = self.make_algo(script=algo_code)
<A NAME="18"></A>        results = algo.run()

        # fetching data at midnight gets us the previous market minute's data
        self<FONT color="#800517"><A HREF="javascript:ZweiFrames('match48-1.html#18',3,'match48-top.html#18',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.assertEqual(390, results.iloc[0].the_price1)
        self.assertEqual(392, results.iloc[0].the_high1)

        # make sure that price is ffilled, but not other fields
        self.assertEqual(350, results.iloc[</B></FONT>0].the_price2)
        self.assertTrue(np.isnan(results.iloc[0].the_high2))

        # 10-minute history

        # asset1 day1 price should be 331-390
        np.testing.assert_array_equal(
            range(331, 391), algo.history_values[0][&quot;price&quot;][1]
        )
<A NAME="3"></A>
        # asset1 day1 high should be 333-392
        np.testing.assert_array_equal(
            range(333, 393), algo.history_values[0][&quot;high&quot;]<FONT color="#53858b"><A HREF="javascript:ZweiFrames('match48-1.html#3',3,'match48-top.html#3',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>[1]
        )

        # asset2 day1 price should be 19 300s, then 40 350s
        np.testing.assert_array_equal(
            [300] * 19, algo.history_values[0][&quot;price&quot;][2][0:19]
        )

        np.testing.assert_array_equal(
            [350] * 40, algo.history_values[0][&quot;price&quot;][2][20:]
        )

<A NAME="9"></A>        # asset2 day1 high should be all NaNs except for the 19th item
        # = 2016-01-05 20:20:00+00:00
        np.testing.assert_array_equal(
            np.</B></FONT>full<FONT color="#83a33a"><A HREF="javascript:ZweiFrames('match48-1.html#9',3,'match48-top.html#9',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>(19, np.nan), algo.history_values[0][&quot;high&quot;][2][0:19]
        )

        self.assertEqual(352, algo.history_values[0][&quot;high&quot;][2][19])

        np.testing.assert_array_equal(</B></FONT>
            np.full(40, np.nan), algo.history_values[0][&quot;high&quot;][2][20:]
        )

    def test_data_in_bts_daily(self):
        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import record, sid
        def initialize(context):
            context.history_values = []

        def before_trading_start(context, data):
            record(the_price1=data.current(sid(1), &quot;price&quot;))
            record(the_high1=data.current(sid(1), &quot;high&quot;))
            record(the_price2=data.current(sid(2), &quot;price&quot;))
            record(the_high2=data.current(sid(2), &quot;high&quot;))

            context.history_values.append(data.history(
                [sid(1), sid(2)],
                [&quot;price&quot;, &quot;high&quot;],
                1,
                &quot;1d&quot;,
            ))

        def handle_data(context, data):
            pass
        &quot;&quot;&quot;)

<A NAME="26"></A>        algo = self.make_algo(script=algo_code)
        results = algo.run()

        self<FONT color="#68818b"><A HREF="javascript:ZweiFrames('match48-1.html#26',3,'match48-top.html#26',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.assertEqual(392, results.the_high1[0])
<A NAME="13"></A>        self.assertEqual(390, results.the_price1[0])

        # nan because asset2 only trades every 50 minutes
        self.assertTrue(np.isnan(results.</B></FONT>the_high2<FONT color="#3b9c9c"><A HREF="javascript:ZweiFrames('match48-1.html#13',3,'match48-top.html#13',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>[0]))

        self.assertTrue(350, results.the_price2[0])

<A NAME="37"></A>        self.assertEqual(392, algo.history_values[0][&quot;high&quot;][1][0])
        self.assertEqual(390, algo.history_values[</B></FONT>0][&quot;price&quot;][1][0])

        self.assertEqual(352, algo<FONT color="#810541"><A HREF="javascript:ZweiFrames('match48-1.html#37',3,'match48-top.html#37',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.history_values[0][&quot;high&quot;][2][0])
        self.assertEqual(350, algo.history_values[0][&quot;price&quot;][2][</B></FONT>0])

    def test_portfolio_bts(self):
        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import order, sid, record

        def initialize(context):
            context.ordered = False
            context.hd_portfolio = context.portfolio

        def before_trading_start(context, data):
            bts_portfolio = context.portfolio

            # Assert that the portfolio in BTS is the same as the last
            # portfolio in handle_data
            assert (context.hd_portfolio == bts_portfolio)
            record(pos_value=bts_portfolio.positions_value)

        def handle_data(context, data):
            if not context.ordered:
                order(sid(1), 1)
                context.ordered = True
            context.hd_portfolio = context.portfolio
        &quot;&quot;&quot;)

        algo = self.make_algo(script=algo_code)
        results = algo.run()

        # Asset starts with price 1 on 1/05 and increases by 1 every minute.
        # Simulation starts on 1/06, where the price in bts is 390, and
        # positions_value is 0. On 1/07, price is 780, and after buying one
        # share on the first bar of 1/06, positions_value is 780
        self.assertEqual(results.pos_value.iloc[0], 0)
        self.assertEqual(results.pos_value.iloc[1], 780)

    def test_account_bts(self):
        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import order, sid, record, set_slippage, slippage

        def initialize(context):
            context.ordered = False
            context.hd_account = context.account
            set_slippage(slippage.VolumeShareSlippage())

        def before_trading_start(context, data):
            bts_account = context.account

            # Assert that the account in BTS is the same as the last account
            # in handle_data
            assert (context.hd_account == bts_account)
            record(port_value=context.account.equity_with_loan)

        def handle_data(context, data):
            if not context.ordered:
                order(sid(1), 1)
                context.ordered = True
            context.hd_account = context.account
        &quot;&quot;&quot;)

        algo = self.make_algo(script=algo_code)
        results = algo.run()

        # Starting portfolio value is 10000. Order for the asset fills on the
        # second bar of 1/06, where the price is 391, and costs the default
        # commission of 0. On 1/07, the price is 780, and the increase in
        # portfolio value is 780-392-0
        self.assertEqual(results.port_value.iloc[0], 10000)
        self.assertAlmostEqual(results.port_value.iloc[1],
                               10000 + 780 - 392 - 0,
                               places=2)

    def test_portfolio_bts_with_overnight_split(self):
        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import order, sid, record

        def initialize(context):
            context.ordered = False
            context.hd_portfolio = context.portfolio

        def before_trading_start(context, data):
            bts_portfolio = context.portfolio
            # Assert that the portfolio in BTS is the same as the last
            # portfolio in handle_data, except for the positions
            for k in bts_portfolio.__dict__:
                if k != 'positions':
                    assert (context.hd_portfolio.__dict__[k]
                            == bts_portfolio.__dict__[k])
            record(pos_value=bts_portfolio.positions_value)
            record(pos_amount=bts_portfolio.positions[sid(3)].amount)
            record(
                last_sale_price=bts_portfolio.positions[sid(3)].last_sale_price
            )

        def handle_data(context, data):
            if not context.ordered:
                order(sid(3), 1)
                context.ordered = True
            context.hd_portfolio = context.portfolio
        &quot;&quot;&quot;)

<A NAME="8"></A>        results = self.run_algorithm(script=algo_code)

        # On 1/07, positions value should by 780, same as without split
        self.assertEqual(results<FONT color="#c58917"><A HREF="javascript:ZweiFrames('match48-1.html#8',3,'match48-top.html#8',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.pos_value.iloc[0], 0)
        self.assertEqual(results.pos_value.iloc[1], 780)
<A NAME="36"></A>
        # On 1/07, after applying the split, 1 share becomes 2
        self.assertEqual(results.pos_amount.iloc[0], 0)
        self.assertEqual(results.pos_amount.</B></FONT>iloc<FONT color="#ff00ff"><A HREF="javascript:ZweiFrames('match48-1.html#36',3,'match48-top.html#36',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>[1], 2)

        # On 1/07, after applying the split, last sale price is halved
        self.assertEqual(results.last_sale_price.iloc[0], 0)
        self.assertEqual(results.last_sale_price.iloc[</B></FONT>1], 390)

    def test_account_bts_with_overnight_split(self):
        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import order, sid, record, set_slippage, slippage

        def initialize(context):
            context.ordered = False
            context.hd_account = context.account
            set_slippage(slippage.VolumeShareSlippage())


        def before_trading_start(context, data):
            bts_account = context.account
            # Assert that the account in BTS is the same as the last account
            # in handle_data
            assert (context.hd_account == bts_account)
            record(port_value=bts_account.equity_with_loan)

        def handle_data(context, data):
            if not context.ordered:
                order(sid(1), 1)
                context.ordered = True
            context.hd_account = context.account
        &quot;&quot;&quot;)

        results = self.run_algorithm(script=algo_code)

        # On 1/07, portfolio value is the same as without split
        self.assertEqual(results.port_value.iloc[0], 10000)
        self.assertAlmostEqual(results.port_value.iloc[1],
                               10000 + 780 - 392 - 0, places=2)


class TestAlgoScript(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-12-31', tz='utc')
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    EQUITY_DAILY_BAR_LOOKBACK_DAYS = 5  # max history window length

    STRING_TYPE_NAMES = [s.__name__ for s in string_types]
    STRING_TYPE_NAMES_STRING = ', '.join(STRING_TYPE_NAMES)
    ASSET_TYPE_NAME = Asset.__name__
    CONTINUOUS_FUTURE_NAME = ContinuousFuture.__name__
    ASSET_OR_STRING_TYPE_NAMES = ', '.join([ASSET_TYPE_NAME] +
                                           STRING_TYPE_NAMES)
    ASSET_OR_STRING_OR_CF_TYPE_NAMES = ', '.join([ASSET_TYPE_NAME,
<A NAME="6"></A>                                                  CONTINUOUS_FUTURE_NAME] +
                                                 STRING_TYPE_NAMES)
    ARG_TYPE_TEST_CASES = (
        <FONT color="#8c8774"><A HREF="javascript:ZweiFrames('match48-1.html#6',3,'match48-top.html#6',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>('history__assets', (bad_type_history_assets,
                             ASSET_OR_STRING_OR_CF_TYPE_NAMES,
                             True)),
        ('history__fields', (bad_type_history_fields,
                             STRING_TYPE_NAMES_STRING,
                             True)),
        ('history__bar_count', (bad_type_history_bar_count, 'int', False)),
        ('history__frequency', (bad_type_history_frequency,
                                STRING_TYPE_NAMES_STRING,
                                False)),
        ('current__assets', (bad_type_current_assets,
                             ASSET_OR_STRING_OR_CF_TYPE_NAMES,
                             True)),
        ('current__fields', (bad_type_current_fields,
                             STRING_TYPE_NAMES_STRING,
                             True)),
<A NAME="35"></A>        ('is_stale__assets', (bad_type_is_stale_assets, 'Asset', True)),
        ('can_trade__assets', (bad_type_can_trade_assets, 'Asset', True)),
        ('history_kwarg__assets'</B></FONT>,
         (<FONT color="#41a317"><A HREF="javascript:ZweiFrames('match48-1.html#35',3,'match48-top.html#35',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>bad_type_history_assets_kwarg,
          ASSET_OR_STRING_OR_CF_TYPE_NAMES,
          True)),
        ('history_kwarg_bad_list__assets',
         (bad_type_history_assets_kwarg_list,
          ASSET_OR_STRING_OR_CF_TYPE_NAMES,
          True)),
        ('history_kwarg__fields',
         (bad_type_history_fields_kwarg, STRING_TYPE_NAMES_STRING, True)),
        ('history_kwarg__bar_count',
         (bad_type_history_bar_count_kwarg, 'int', False)),
        ('history_kwarg__frequency',
         (bad_type_history_frequency_kwarg, STRING_TYPE_NAMES_STRING, False)),
        ('current_kwarg__assets',
         (bad_type_current_assets_kwarg,
          ASSET_OR_STRING_OR_CF_TYPE_NAMES,
          True)),
        ('current_kwarg__fields'</B></FONT>,
         (bad_type_current_fields_kwarg, STRING_TYPE_NAMES_STRING, True)),
    )

    sids = 0, 1, 3, 133

    # FIXME: Pass a benchmark explicitly here.
    BENCHMARK_SID = None

    @classmethod
    def make_equity_info(cls):
        register_calendar(&quot;TEST&quot;, get_calendar(&quot;NYSE&quot;), force=True)

        data = make_simple_equity_info(
            cls.sids,
            cls.START_DATE,
            cls.END_DATE,
        )
        data.loc[3, 'symbol'] = 'TEST'
        return data

    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        cal = cls.trading_calendars[Equity]
        sessions = cal.sessions_in_range(cls.START_DATE, cls.END_DATE)
        frame = pd.DataFrame({
            'close': 10., 'high': 10.5, 'low': 9.5, 'open': 10., 'volume': 100,
        }, index=sessions)

        for sid in sids:
            yield sid, frame

    def test_noop(self):
        self.run_algorithm(
            initialize=initialize_noop,
            handle_data=handle_data_noop,
        )

    def test_noop_string(self):
        self.run_algorithm(script=noop_algo)

    def test_no_handle_data(self):
        self.run_algorithm(script=no_handle_data)

    def test_api_calls(self):
        self.run_algorithm(
            initialize=initialize_api,
            handle_data=handle_data_api,
        )

    def test_api_calls_string(self):
        self.run_algorithm(script=api_algo)

    def test_api_get_environment(self):
        platform = 'zipline'
        algo = self.make_algo(
            script=api_get_environment_algo,
            platform=platform,
        )
        algo.run()
        self.assertEqual(algo.environment, platform)

    def test_api_symbol(self):
        self.run_algorithm(script=api_symbol_algo)

    def test_fixed_slippage(self):
        # verify order -&gt; transaction -&gt; portfolio position.
        # --------------
        test_algo = self.make_algo(
            script=&quot;&quot;&quot;
from zipline.api import (slippage,
                         commission,
                         set_slippage,
                         set_commission,
                         order,
                         record,
                         sid)

def initialize(context):
    model = slippage.FixedSlippage(spread=0.10)
    set_slippage(model)
    set_commission(commission.PerTrade(100.00))
    context.count = 1
    context.incr = 0

def handle_data(context, data):
    if context.incr &lt; context.count:
        order(sid(0), -1000)
    record(price=data.current(sid(0), &quot;price&quot;))

    context.incr += 1&quot;&quot;&quot;,
        )
        results = test_algo.run()

        # flatten the list of txns
        all_txns = [val for sublist in results[&quot;transactions&quot;].tolist()
                    for val in sublist]

        self.assertEqual(len(all_txns), 1)
<A NAME="12"></A>        txn = all_txns[0]

        expected_spread = 0.05
        expected_price = test_algo<FONT color="#571b7e"><A HREF="javascript:ZweiFrames('match48-1.html#12',3,'match48-top.html#12',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.recorded_vars[&quot;price&quot;] - expected_spread

        self.assertEqual(expected_price, txn['price'])

        # make sure that the $100 commission was applied to our cash
        # the txn was for -1000 shares at 9.95, means -9.95k.  our capital_used
        # for that day was therefore 9.95k, but after the $100 commission,
        # it should be 9.85k.
        self.assertEqual(9850, results.capital_used[1])
        self.assertEqual(100, results[&quot;orders&quot;].iloc[1][0][</B></FONT>&quot;commission&quot;])

    @parameterized.expand(
        [
            ('no_minimum_commission', 0,),
            ('default_minimum_commission', 0,),
            ('alternate_minimum_commission', 2,),
        ]
    )
    def test_volshare_slippage(self, name, minimum_commission):
        tempdir = TempDirectory()
        try:
            if name == &quot;default_minimum_commission&quot;:
                commission_line = &quot;set_commission(commission.PerShare(0.02))&quot;
            else:
                commission_line = \
                    &quot;set_commission(commission.PerShare(0.02, &quot; \
                    &quot;min_trade_cost={0}))&quot;.format(minimum_commission)

            # verify order -&gt; transaction -&gt; portfolio position.
            # --------------
            # XXX: This is the last remaining consumer of
            #      create_daily_trade_source.
<A NAME="25"></A>            trades = factory.create_daily_trade_source(
                [0], self.sim_params, self.asset_finder, self.trading_calendar
            )
            data_portal = create_data_portal_from_trade_history<FONT color="#5eac10"><A HREF="javascript:ZweiFrames('match48-1.html#25',3,'match48-top.html#25',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>(
                self.asset_finder, self.trading_calendar, tempdir,
                self.sim_params, {0: trades}
            )
            test_algo = self.make_algo(
                data_portal=data_portal,
                script=&quot;&quot;&quot;
from zipline.api import *

def initialize(context):
    model = slippage.VolumeShareSlippage(
                            volume_limit=.3,
                            price_impact=0.05
                       )
    set_slippage(model)
    {0}

    context.count = 2
    context.incr = 0

def handle_data(context, data):
    if context.incr &lt; context.count:
        # order small lots to be sure the
        # order will fill in a single transaction
        order(sid(0), 5000)
    record(price=data.current(sid(0), &quot;price&quot;))
    record(volume=data.current(sid(0), &quot;volume&quot;))
    record(incr=context.incr)
    context.incr += 1
    &quot;&quot;&quot;.format(commission_line),
            )
            results =</B></FONT> test_algo.run()

            all_txns = [
                val for sublist in results[&quot;transactions&quot;].tolist()
                for val in sublist]

            self.assertEqual(len(all_txns), 67)
            # all_orders are all the incremental versions of the
            # orders as each new fill comes in.
            all_orders = list(toolz.concat(results['orders']))

            if minimum_commission == 0:
                # for each incremental version of each order, the commission
                # should be its filled amount * 0.02
                for order_ in all_orders:
                    self.assertAlmostEqual(
                        order_[&quot;filled&quot;] * 0.02,
                        order_[&quot;commission&quot;]
                    )
            else:
                # the commission should be at least the min_trade_cost
                for order_ in all_orders:
                    if order_[&quot;filled&quot;] &gt; 0:
                        self.assertAlmostEqual(
                            max(order_[&quot;filled&quot;] * 0.02, minimum_commission),
                            order_[&quot;commission&quot;]
                        )
                    else:
                        self.assertEqual(0, order_[&quot;commission&quot;])
        finally:
            tempdir.cleanup()

    def test_incorrectly_set_futures_slippage_model(self):
        code = dedent(
            &quot;&quot;&quot;
            from zipline.api import set_slippage, slippage

            class MySlippage(slippage.FutureSlippageModel):
                def process_order(self, data, order):
                    return data.current(order.asset, 'price'), order.amount

            def initialize(context):
                set_slippage(MySlippage())
            &quot;&quot;&quot;
        )
        test_algo = self.make_algo(script=code)
        with self.assertRaises(IncompatibleSlippageModel):
            # Passing a futures slippage model as the first argument, which is
            # for setting equity models, should fail.
            test_algo.run()

    def test_algo_record_vars(self):
        test_algo = self.make_algo(script=record_variables)
        results = test_algo.run()

        for i in range(1, 252):
            self.assertEqual(results.iloc[i-1][&quot;incr&quot;], i)

    def test_algo_record_nan(self):
        test_algo = self.make_algo(script=record_float_magic % 'nan')
        results = test_algo.run()
        for i in range(1, 252):
            self.assertTrue(np.isnan(results.iloc[i-1][&quot;data&quot;]))

    def test_batch_market_order_matches_multiple_manual_orders(self):
        share_counts = pd.Series([50, 100])

        multi_blotter = RecordBatchBlotter()
        multi_test_algo = self.make_algo(
            script=dedent(&quot;&quot;&quot;\
                from collections import OrderedDict
                from six import iteritems

                from zipline.api import sid, order


                def initialize(context):
                    context.assets = [sid(0), sid(3)]
                    context.placed = False

                def handle_data(context, data):
                    if not context.placed:
                        it = zip(context.assets, {share_counts})
                        for asset, shares in it:
                            order(asset, shares)

                        context.placed = True

            &quot;&quot;&quot;).format(share_counts=list(share_counts)),
            blotter=multi_blotter,
        )
        multi_stats = multi_test_algo.run()
        self.assertFalse(multi_blotter.order_batch_called)

        batch_blotter = RecordBatchBlotter()
        batch_test_algo = self.make_algo(
            script=dedent(&quot;&quot;&quot;\
                import pandas as pd

                from zipline.api import sid, batch_market_order


                def initialize(context):
                    context.assets = [sid(0), sid(3)]
                    context.placed = False

                def handle_data(context, data):
                    if not context.placed:
                        orders = batch_market_order(pd.Series(
                            index=context.assets, data={share_counts}
                        ))
                        assert len(orders) == 2, \
                            &quot;len(orders) was %s but expected 2&quot; % len(orders)
                        for o in orders:
                            assert o is not None, &quot;An order is None&quot;

                        context.placed = True

            &quot;&quot;&quot;).format(share_counts=list(share_counts)),
            blotter=batch_blotter,
        )
        batch_stats = batch_test_algo.run()
        self.assertTrue(batch_blotter.order_batch_called)

        for stats in (multi_stats, batch_stats):
            stats.orders = stats.orders.apply(
                lambda orders: [toolz.dissoc(o, 'id') for o in orders]
            )
            stats.transactions = stats.transactions.apply(
                lambda txns: [toolz.dissoc(txn, 'order_id') for txn in txns]
            )
        assert_equal(multi_stats, batch_stats)

    def test_batch_market_order_filters_null_orders(self):
        share_counts = [50, 0]

        batch_blotter = RecordBatchBlotter()
        batch_test_algo = self.make_algo(
            script=dedent(&quot;&quot;&quot;\
                import pandas as pd

                from zipline.api import sid, batch_market_order

                def initialize(context):
                    context.assets = [sid(0), sid(3)]
                    context.placed = False

                def handle_data(context, data):
                    if not context.placed:
                        orders = batch_market_order(pd.Series(
                            index=context.assets, data={share_counts}
                        ))
                        assert len(orders) == 1, \
                            &quot;len(orders) was %s but expected 1&quot; % len(orders)
                        for o in orders:
                            assert o is not None, &quot;An order is None&quot;

                        context.placed = True

            &quot;&quot;&quot;).format(share_counts=share_counts),
            blotter=batch_blotter,
        )
        batch_test_algo.run()
        self.assertTrue(batch_blotter.order_batch_called)

    def test_order_dead_asset(self):
        # after asset 0 is dead
        params = SimulationParameters(
            start_session=pd.Timestamp(&quot;2007-01-03&quot;, tz='UTC'),
            end_session=pd.Timestamp(&quot;2007-01-05&quot;, tz='UTC'),
            trading_calendar=self.trading_calendar,
        )

        # order method shouldn't blow up
        self.run_algorithm(
            script=&quot;&quot;&quot;
from zipline.api import order, sid

def initialize(context):
    pass

def handle_data(context, data):
    order(sid(0), 10)
        &quot;&quot;&quot;,
        )

        # order_value and order_percent should blow up
        for order_str in [&quot;order_value&quot;, &quot;order_percent&quot;]:
            test_algo = self.make_algo(
                script=&quot;&quot;&quot;
from zipline.api import order_percent, order_value, sid

def initialize(context):
    pass

def handle_data(context, data):
    {0}(sid(0), 10)
        &quot;&quot;&quot;.format(order_str),
                sim_params=params,
            )

        with self.assertRaises(CannotOrderDelistedAsset):
            test_algo.run()

    def test_portfolio_in_init(self):
        &quot;&quot;&quot;
        Test that accessing portfolio in init doesn't break.
        &quot;&quot;&quot;
        self.run_algorithm(script=access_portfolio_in_init)

    def test_account_in_init(self):
        &quot;&quot;&quot;
        Test that accessing account in init doesn't break.
        &quot;&quot;&quot;
        self.run_algorithm(script=access_account_in_init)

    def test_without_kwargs(self):
        &quot;&quot;&quot;
        Test that api methods on the data object can be called with positional
        arguments.
        &quot;&quot;&quot;
        params = SimulationParameters(
            start_session=pd.Timestamp(&quot;2006-01-10&quot;, tz='UTC'),
            end_session=pd.Timestamp(&quot;2006-01-11&quot;, tz='UTC'),
            trading_calendar=self.trading_calendar,
        )
        self.run_algorithm(sim_params=params, script=call_without_kwargs)

    def test_good_kwargs(self):
        &quot;&quot;&quot;
        Test that api methods on the data object can be called with keyword
        arguments.
        &quot;&quot;&quot;
        params = SimulationParameters(
            start_session=pd.Timestamp(&quot;2006-01-10&quot;, tz='UTC'),
            end_session=pd.Timestamp(&quot;2006-01-11&quot;, tz='UTC'),
            trading_calendar=self.trading_calendar,
        )
        self.run_algorithm(script=call_with_kwargs, sim_params=params)

    @parameterized.expand([('history', call_with_bad_kwargs_history),
                           ('current', call_with_bad_kwargs_current)])
    def test_bad_kwargs(self, name, algo_text):
        &quot;&quot;&quot;
        Test that api methods on the data object called with bad kwargs return
        a meaningful TypeError that we create, rather than an unhelpful cython
        error
        &quot;&quot;&quot;
        algo = self.make_algo(script=algo_text)
        with self.assertRaises(TypeError) as cm:
            algo.run()

        self.assertEqual(&quot;%s() got an unexpected keyword argument 'blahblah'&quot;
                         % name, cm.exception.args[0])

    @parameterized.expand(ARG_TYPE_TEST_CASES)
    def test_arg_types(self, name, inputs):

        keyword = name.split('__')[1]

        algo = self.make_algo(script=inputs[0])
        with self.assertRaises(TypeError) as cm:
            algo.run()

        expected = &quot;Expected %s argument to be of type %s%s&quot; % (
            keyword,
            'or iterable of type ' if inputs[2] else '',
            inputs[1]
        )

        self.assertEqual(expected, cm.exception.args[0])

    def test_empty_asset_list_to_history(self):
        params = SimulationParameters(
            start_session=pd.Timestamp(&quot;2006-01-10&quot;, tz='UTC'),
            end_session=pd.Timestamp(&quot;2006-01-11&quot;, tz='UTC'),
            trading_calendar=self.trading_calendar,
        )

        self.run_algorithm(
            script=dedent(&quot;&quot;&quot;
                def initialize(context):
                    pass

                def handle_data(context, data):
                    data.history([], &quot;price&quot;, 5, '1d')
                &quot;&quot;&quot;),
            sim_params=params,
        )

    @parameterized.expand(
        [('bad_kwargs', call_with_bad_kwargs_get_open_orders),
         ('good_kwargs', call_with_good_kwargs_get_open_orders),
         ('no_kwargs', call_with_no_kwargs_get_open_orders)]
    )
    def test_get_open_orders_kwargs(self, name, script):
        algo = self.make_algo(script=script)
        if name == 'bad_kwargs':
            with self.assertRaises(TypeError) as cm:
                algo.run()
                self.assertEqual('Keyword argument `sid` is no longer '
                                 'supported for get_open_orders. Use `asset` '
                                 'instead.', cm.exception.args[0])
        else:
            algo.run()

    def test_empty_positions(self):
        &quot;&quot;&quot;
        Test that when we try context.portfolio.positions[stock] on a stock
        for which we have no positions, we return a Position with values 0
        (but more importantly, we don't crash) and don't save this Position
        to the user-facing dictionary PositionTracker._positions_store
        &quot;&quot;&quot;
        results = self.run_algorithm(script=empty_positions)
        num_positions = results.num_positions
        amounts = results.amounts
        self.assertTrue(all(num_positions == 0))
        self.assertTrue(all(amounts == 0))

    def test_schedule_function_time_rule_positionally_misplaced(self):
        &quot;&quot;&quot;
        Test that when a user specifies a time rule for the date_rule argument,
        but no rule in the time_rule argument
        (e.g. schedule_function(func, &lt;time_rule&gt;)), we assume that means
        assign a time rule but no date rule
        &quot;&quot;&quot;

        sim_params = factory.create_simulation_parameters(
            start=pd.Timestamp('2006-01-12', tz='UTC'),
            end=pd.Timestamp('2006-01-13', tz='UTC'),
            data_frequency='minute'
        )

        algocode = dedent(&quot;&quot;&quot;
        from zipline.api import time_rules, schedule_function

        def do_at_open(context, data):
            context.done_at_open.append(context.get_datetime())

        def do_at_close(context, data):
            context.done_at_close.append(context.get_datetime())

        def initialize(context):
            context.done_at_open = []
            context.done_at_close = []
            schedule_function(do_at_open, time_rules.market_open())
            schedule_function(do_at_close, time_rules.market_close())

        def handle_data(algo, data):
            pass
        &quot;&quot;&quot;)

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter(&quot;ignore&quot;, PerformanceWarning)

            algo = self.make_algo(script=algocode, sim_params=sim_params)
            algo.run()

            self.assertEqual(len(w), 2)

            for i, warning in enumerate(w):
                self.assertIsInstance(warning.message, UserWarning)
                self.assertEqual(
                    warning.message.args[0],
                    'Got a time rule for the second positional argument '
                    'date_rule. You should use keyword argument '
                    'time_rule= when calling schedule_function without '
                    'specifying a date_rule'
                )
                # The warnings come from line 13 and 14 in the algocode
                self.assertEqual(warning.lineno, 13 + i)

        self.assertEqual(
            algo.done_at_open,
            [pd.Timestamp('2006-01-12 14:31:00', tz='UTC'),
             pd.Timestamp('2006-01-13 14:31:00', tz='UTC')]
        )

        self.assertEqual(
            algo.done_at_close,
            [pd.Timestamp('2006-01-12 20:59:00', tz='UTC'),
             pd.Timestamp('2006-01-13 20:59:00', tz='UTC')]
        )


class TestCapitalChanges(zf.WithMakeAlgo, zf.ZiplineTestCase):

    START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
    END_DATE = pd.Timestamp('2006-01-09', tz='UTC')

    # XXX: This suite only has daily data for sid 0 and only has minutely data
    #      for sid 1.
    sids = ASSET_FINDER_EQUITY_SIDS = (0, 1)
    DAILY_SID = 0
    MINUTELY_SID = 1

    # FIXME: Pass a benchmark source explicitly here.
    BENCHMARK_SID = None

    @classmethod
    def make_equity_minute_bar_data(cls):
        minutes = cls.trading_calendar.minutes_in_range(
            cls.START_DATE,
            cls.END_DATE,
        )
        closes = np.arange(100, 100 + len(minutes), 1)
        opens = closes
        highs = closes + 5
        lows = closes - 5

        frame = pd.DataFrame(
            index=minutes,
            data={
                'open': opens,
                'high': highs,
                'low': lows,
                'close': closes,
                'volume': 10000,
            },
        )

        yield cls.MINUTELY_SID, frame

    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        days = cls.trading_calendar.sessions_in_range(
            cls.START_DATE,
            cls.END_DATE,
        )

        closes = np.arange(10.0, 10.0 + len(days), 1.0)
        opens = closes
        highs = closes + 0.5
        lows = closes - 0.5

        frame = pd.DataFrame(
            index=days,
            data={
                'open': opens,
                'high': highs,
                'low': lows,
                'close': closes,
                'volume': 10000,
            },
        )

        yield cls.DAILY_SID, frame

    @parameterized.expand([
        ('target', 151000.0), ('delta', 50000.0)
    ])
    def test_capital_changes_daily_mode(self, change_type, value):
        capital_changes = {
            pd.Timestamp('2006-01-06', tz='UTC'):
                {'type': change_type, 'value': value}
        }

        algocode = &quot;&quot;&quot;
from zipline.api import set_slippage, set_commission, slippage, commission, \
    schedule_function, time_rules, order, sid

def initialize(context):
    set_slippage(slippage.FixedSlippage(spread=0))
    set_commission(commission.PerShare(0, 0))
    schedule_function(order_stuff, time_rule=time_rules.market_open())

def order_stuff(context, data):
    order(sid(0), 1000)
&quot;&quot;&quot;
        algo = self.make_algo(
            script=algocode,
            capital_changes=capital_changes,
            sim_params=SimulationParameters(
                start_session=self.START_DATE,
                end_session=self.END_DATE,
                trading_calendar=self.nyse_calendar,
            )
        )

        # We call get_generator rather than `run()` here because we care about
        # the raw capital change packets.
        gen = algo.get_generator()
        results = list(gen)

        cumulative_perf = \
            [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
        daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
        capital_change_packets = \
            [r['capital_change'] for r in results if 'capital_change' in r]

        self.assertEqual(len(capital_change_packets), 1)
        self.assertEqual(
            capital_change_packets[0],
            {'date': pd.Timestamp('2006-01-06', tz='UTC'),
             'type': 'cash',
             'target': 151000.0 if change_type == 'target' else None,
             'delta': 50000.0})

        # 1/03: price = 10, place orders
        # 1/04: orders execute at price = 11, place orders
        # 1/05: orders execute at price = 12, place orders
        # 1/06: +50000 capital change,
        #       orders execute at price = 13, place orders
        # 1/09: orders execute at price = 14, place orders
<A NAME="0"></A>
        expected_daily = {}

        expected_capital_changes <FONT color="#0000ff"><A HREF="javascript:ZweiFrames('match48-1.html#0',3,'match48-top.html#0',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>= np.array([
            0.0, 0.0, 0.0, 50000.0, 0.0
        ])

        # Day 1, no transaction. Day 2, we transact, but the price of our stock
        # does not change. Day 3, we start getting returns
        expected_daily['returns'] = np.array([
            0.0,
            0.0,
            # 1000 shares * gain of 1
            (100000.0 + 1000.0) / 100000.0 - 1.0,
            # 2000 shares * gain of 1, capital change of +50000
            (151000.0 + 2000.0) / 151000.0 - 1.0,
            # 3000 shares * gain of 1
            (153000.0 + 3000.0) / 153000.0 - 1.0,
        ])

        expected_daily['pnl'] = np.array([
            0.0,
            0.0,
            1000.00,  # 1000 shares * gain of 1
            2000.00,  # 2000 shares * gain of 1
            3000.00,  # 3000 shares * gain of 1
        ])

        expected_daily['capital_used'] = np.array([
            0.0,
            -11000.0,  # 1000 shares at price = 11
            -12000.0,  # 1000 shares at price = 12
            -13000.0,  # 1000 shares at price = 13
            -14000.0,  # 1000 shares at price = 14
        ])

        expected_daily['ending_cash'] = \
            np.array([100000.0] * 5) + \
            np.cumsum(expected_capital_changes) + \
            np.</B></FONT>cumsum(expected_daily['capital_used'])

        expected_daily['starting_cash'] = \
            expected_daily['ending_cash'] - \
            expected_daily['capital_used']

        expected_daily['starting_value'] = np.array([
            0.0,
            0.0,
            11000.0,  # 1000 shares at price = 11
            24000.0,  # 2000 shares at price = 12
            39000.0,  # 3000 shares at price = 13
        ])

        expected_daily['ending_value'] = \
            expected_daily['starting_value'] + \
            expected_daily['pnl'] - \
            expected_daily['capital_used']

        expected_daily['portfolio_value'] = \
            expected_daily['ending_value'] + \
            expected_daily['ending_cash']

        stats = [
            'returns', 'pnl', 'capital_used', 'starting_cash', 'ending_cash',
            'starting_value', 'ending_value', 'portfolio_value'
<A NAME="5"></A>        ]

        expected_cumulative = {
            'returns': np.cumprod(expected_daily<FONT color="#151b8d"><A HREF="javascript:ZweiFrames('match48-1.html#5',3,'match48-top.html#5',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>['returns'] + 1) - 1,
            'pnl': np.cumsum(expected_daily['pnl']),
            'capital_used': np.cumsum(expected_daily['capital_used']),
            'starting_cash':
                np.repeat(expected_daily['starting_cash'][0:1], 5),
            'ending_cash': expected_daily['ending_cash'],
            'starting_value':
                np.repeat(expected_daily['starting_value'][0:1], 5),
            'ending_value': expected_daily['ending_value'],
            'portfolio_value': expected_daily[</B></FONT>'portfolio_value'],
        }

        for stat in stats:
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in daily_perf]),
                expected_daily[stat],
                err_msg='daily ' + stat,
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in cumulative_perf]),
                expected_cumulative[stat],
                err_msg='cumulative ' + stat,
            )

        self.assertEqual(
            algo.capital_change_deltas,
            {pd.Timestamp('2006-01-06', tz='UTC'): 50000.0}
<A NAME="14"></A>        )

    @parameterized.expand([
        <FONT color="#842dce"><A HREF="javascript:ZweiFrames('match48-1.html#14',3,'match48-top.html#14',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>('interday_target', [('2006-01-04', 2388.0)]),
        ('interday_delta', [('2006-01-04', 1000.0)]),
        ('intraday_target', [('2006-01-04 17:00', 2184.0),
                             ('2006-01-04 18:00', 2804.0)]),
        ('intraday_delta', [('2006-01-04 17:00', 500.0),
                            ('2006-01-04 18:00'</B></FONT>, 500.0)]),
    ])
    def test_capital_changes_minute_mode_daily_emission(self, change, values):
        change_loc, change_type = change.split('_')

        sim_params = SimulationParameters(
            start_session=pd.Timestamp('2006-01-03', tz='UTC'),
            end_session=pd.Timestamp('2006-01-05', tz='UTC'),
            data_frequency='minute',
            capital_base=1000.0,
            trading_calendar=self.nyse_calendar,
        )

        capital_changes = {
            pd.Timestamp(datestr, tz='UTC'): {
                'type': change_type,
                'value': value
            }
            for datestr, value in values
        }

        algocode = &quot;&quot;&quot;
from zipline.api import set_slippage, set_commission, slippage, commission, \
    schedule_function, time_rules, order, sid

def initialize(context):
    set_slippage(slippage.FixedSlippage(spread=0))
    set_commission(commission.PerShare(0, 0))
    schedule_function(order_stuff, time_rule=time_rules.market_open())

def order_stuff(context, data):
    order(sid(1), 1)
&quot;&quot;&quot;

        algo = self.make_algo(
            script=algocode,
            sim_params=sim_params,
            capital_changes=capital_changes
        )

        gen = algo.get_generator()
        results = list(gen)

        cumulative_perf = \
            [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
        daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
        capital_change_packets = \
            [r['capital_change'] for r in results if 'capital_change' in r]

        self.assertEqual(len(capital_change_packets), len(capital_changes))
        expected = [
            {'date': pd.Timestamp(val[0], tz='UTC'),
             'type': 'cash',
             'target': val[1] if change_type == 'target' else None,
             'delta': 1000.0 if len(values) == 1 else 500.0}
            for val in values]
        self.assertEqual(capital_change_packets, expected)

        # 1/03: place orders at price = 100, execute at 101
        # 1/04: place orders at price = 490, execute at 491,
        #       +500 capital change at 17:00 and 18:00 (intraday)
        #       or +1000 at 00:00 (interday),
        # 1/05: place orders at price = 880, execute at 881

        expected_daily = {}

        expected_capital_changes = np.array([0.0, 1000.0, 0.0])

        if change_loc == 'intraday':
            # Fills at 491, +500 capital change comes at 638 (17:00) and
            # 698 (18:00), ends day at 879
            day2_return = (
                (1388.0 + 149.0 + 147.0) / 1388.0 *
                (2184.0 + 60.0 + 60.0) / 2184.0 *
                (2804.0 + 181.0 + 181.0) / 2804.0 - 1.0
            )
        else:
<A NAME="2"></A>            # Fills at 491, ends day at 879, capital change +1000
            day2_return = (2388.0 + 390.0 + 388.0) / 2388.0 - 1

        expected_daily<FONT color="#980517"><A HREF="javascript:ZweiFrames('match48-1.html#2',3,'match48-top.html#2',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>['returns'] = np.array([
            # Fills at 101, ends day at 489
            (1000.0 + 489 - 101) / 1000.0 - 1.0,
            day2_return,
            # Fills at 881, ends day at 1269
            (3166.0 + 390.0 + 390.0 + 388.0) / 3166.0 - 1.0,
        ])

        expected_daily['pnl'] = np.array([
            388.0,
            390.0 + 388.0,
            390.0 + 390.0 + 388.0,
        ])

        expected_daily['capital_used'] = np.array([
            -101.0, -491.0, -881.0
        ])

        expected_daily['ending_cash'] = \
            np.array([1000.0] * 3) + \
            np.cumsum(expected_capital_changes) + \
            np.</B></FONT>cumsum(expected_daily['capital_used'])

        expected_daily['starting_cash'] = \
            expected_daily['ending_cash'] - \
            expected_daily['capital_used']

        if change_loc == 'intraday':
            # Capital changes come after day start
            expected_daily['starting_cash'] -= expected_capital_changes

        expected_daily['starting_value'] = np.array([
            0.0, 489.0, 879.0 * 2
        ])

        expected_daily['ending_value'] = \
            expected_daily['starting_value'] + \
            expected_daily['pnl'] - \
            expected_daily['capital_used']

        expected_daily['portfolio_value'] = \
            expected_daily['ending_value'] + \
            expected_daily['ending_cash']

        stats = [
            'returns', 'pnl', 'capital_used', 'starting_cash', 'ending_cash',
            'starting_value', 'ending_value', 'portfolio_value'
<A NAME="11"></A>        ]

        expected_cumulative = {
            'returns': np.cumprod(expected_daily<FONT color="#b041ff"><A HREF="javascript:ZweiFrames('match48-1.html#11',3,'match48-top.html#11',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>['returns'] + 1) - 1,
            'pnl': np.cumsum(expected_daily['pnl']),
            'capital_used': np.cumsum(expected_daily['capital_used']),
            'starting_cash':
                np.repeat(expected_daily['starting_cash'][0:1], 3),
            'ending_cash': expected_daily['ending_cash'],
            'starting_value':
                np.repeat(expected_daily['starting_value'][</B></FONT>0:1], 3),
            'ending_value': expected_daily['ending_value'],
            'portfolio_value': expected_daily['portfolio_value'],
        }

        for stat in stats:
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in daily_perf]),
                expected_daily[stat]
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in cumulative_perf]),
                expected_cumulative[stat]
            )

        if change_loc == 'interday':
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04', tz='UTC'): 1000.0}
            )
        else:
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04 17:00', tz='UTC'): 500.0,
                 pd.Timestamp('2006-01-04 18:00', tz='UTC'): 500.0}
<A NAME="34"></A>            )

    @parameterized.expand([
        <FONT color="#827d6b"><A HREF="javascript:ZweiFrames('match48-1.html#34',3,'match48-top.html#34',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>('interday_target', [('2006-01-04', 2388.0)]),
        ('interday_delta', [('2006-01-04', 1000.0)]),
        ('intraday_target', [('2006-01-04 17:00', 2184.0),
                             ('2006-01-04 18:00', 2804.0)]),
        ('intraday_delta'</B></FONT>, [('2006-01-04 17:00', 500.0),
                            ('2006-01-04 18:00', 500.0)]),
    ])
    def test_capital_changes_minute_mode_minute_emission(self, change, values):
        change_loc, change_type = change.split('_')

        sim_params = SimulationParameters(
            start_session=pd.Timestamp('2006-01-03', tz='UTC'),
            end_session=pd.Timestamp('2006-01-05', tz='UTC'),
            data_frequency='minute',
            emission_rate='minute',
            capital_base=1000.0,
            trading_calendar=self.nyse_calendar,
        )

        capital_changes = {pd.Timestamp(val[0], tz='UTC'): {
            'type': change_type, 'value': val[1]} for val in values}

        algocode = &quot;&quot;&quot;
from zipline.api import set_slippage, set_commission, slippage, commission, \
    schedule_function, time_rules, order, sid

def initialize(context):
    set_slippage(slippage.FixedSlippage(spread=0))
    set_commission(commission.PerShare(0, 0))
    schedule_function(order_stuff, time_rule=time_rules.market_open())

def order_stuff(context, data):
    order(sid(1), 1)
&quot;&quot;&quot;

        algo = self.make_algo(
            script=algocode,
            sim_params=sim_params,
            capital_changes=capital_changes
        )

        gen = algo.get_generator()
        results = list(gen)

        cumulative_perf = \
            [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
        minute_perf = [r['minute_perf'] for r in results if 'minute_perf' in r]
        daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
        capital_change_packets = \
            [r['capital_change'] for r in results if 'capital_change' in r]

        self.assertEqual(len(capital_change_packets), len(capital_changes))
        expected = [
            {'date': pd.Timestamp(val[0], tz='UTC'),
             'type': 'cash',
             'target': val[1] if change_type == 'target' else None,
             'delta': 1000.0 if len(values) == 1 else 500.0}
            for val in values]
        self.assertEqual(capital_change_packets, expected)

        # 1/03: place orders at price = 100, execute at 101
        # 1/04: place orders at price = 490, execute at 491,
        #       +500 capital change at 17:00 and 18:00 (intraday)
        #       or +1000 at 00:00 (interday),
        # 1/05: place orders at price = 880, execute at 881

        # Minute perfs are cumulative for the day
        expected_minute = {}

        capital_changes_after_start = np.array([0.0] * 1170)
        if change_loc == 'intraday':
            capital_changes_after_start[539:599] = 500.0
<A NAME="33"></A>            capital_changes_after_start[599:780] = 1000.0

        expected_minute['pnl'] = np.array([0.0] * 1170)
        expected_minute<FONT color="#736aff"><A HREF="javascript:ZweiFrames('match48-1.html#33',3,'match48-top.html#33',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>['pnl'][:2] = 0.0
        expected_minute['pnl'][2:392] = 1.0
        expected_minute['pnl'][392:782] = 2.0
        expected_minute['pnl'][782:] =</B></FONT> 3.0
        for start, end in ((0, 390), (390, 780), (780, 1170)):
            expected_minute['pnl'][start:end] = \
                np.cumsum(expected_minute['pnl'][start:end])

        expected_minute['capital_used'] = np.concatenate((
            [0.0] * 1, [-101.0] * 389,
            [0.0] * 1, [-491.0] * 389,
            [0.0] * 1, [-881.0] * 389,
        ))

        # +1000 capital changes comes before the day start if interday
        day2adj = 0.0 if change_loc == 'intraday' else 1000.0

        expected_minute['starting_cash'] = np.concatenate((
            [1000.0] * 390,
            # 101 spent on 1/03
            [1000.0 - 101.0 + day2adj] * 390,
            # 101 spent on 1/03, 491 on 1/04, +1000 capital change on 1/04
            [1000.0 - 101.0 - 491.0 + 1000] * 390
        ))

        expected_minute['ending_cash'] = \
            expected_minute['starting_cash'] + \
            expected_minute['capital_used'] + \
            capital_changes_after_start

        expected_minute['starting_value'] = np.concatenate((
            [0.0] * 390,
            [489.0] * 390,
            [879.0 * 2] * 390
        ))

        expected_minute['ending_value'] = \
            expected_minute['starting_value'] + \
            expected_minute['pnl'] - \
            expected_minute['capital_used']

        expected_minute['portfolio_value'] = \
            expected_minute['ending_value'] + \
            expected_minute['ending_cash']

        expected_minute['returns'] = \
            expected_minute['pnl'] / \
            (expected_minute['starting_value'] +
             expected_minute['starting_cash'])

        # If the change is interday, we can just calculate the returns from
        # the pnl, starting_value and starting_cash. If the change is intraday,
        # the returns after the change have to be calculated from two
        # subperiods
        if change_loc == 'intraday':
            # The last packet (at 1/04 16:59) before the first capital change
            prev_subperiod_return = expected_minute['returns'][538]
<A NAME="24"></A>
            # From 1/04 17:00 to 17:59
            cur_subperiod_pnl = \
                expected_minute['pnl']<FONT color="#79764d"><A HREF="javascript:ZweiFrames('match48-1.html#24',3,'match48-top.html#24',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>[539:599] - expected_minute['pnl'][538]
            cur_subperiod_starting_value = \
                np.array([expected_minute['ending_value'][538]] * 60)
            cur_subperiod_starting_cash = \
                np.array([expected_minute['ending_cash'][</B></FONT>538] + 500] * 60)

            cur_subperiod_returns = cur_subperiod_pnl / \
                (cur_subperiod_starting_value + cur_subperiod_starting_cash)
            expected_minute['returns'][539:599] = \
                (cur_subperiod_returns + 1.0) * \
                (prev_subperiod_return + 1.0) - \
                1.0

            # The last packet (at 1/04 17:59) before the second capital change
            prev_subperiod_return = expected_minute['returns'][598]
<A NAME="23"></A>
            # From 1/04 18:00 to 21:00
            cur_subperiod_pnl = \
                expected_minute['pnl']<FONT color="#f660ab"><A HREF="javascript:ZweiFrames('match48-1.html#23',3,'match48-top.html#23',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>[599:780] - expected_minute['pnl'][598]
            cur_subperiod_starting_value = \
                np.array([expected_minute['ending_value'][598]] * 181)
            cur_subperiod_starting_cash = \
                np.array([expected_minute['ending_cash'][</B></FONT>598] + 500] * 181)

            cur_subperiod_returns = cur_subperiod_pnl / \
                (cur_subperiod_starting_value + cur_subperiod_starting_cash)
            expected_minute['returns'][599:780] = \
                (cur_subperiod_returns + 1.0) * \
                (prev_subperiod_return + 1.0) - \
                1.0

        # The last minute packet of each day
        expected_daily = {
            k: np.array([v[389], v[779], v[1169]])
            for k, v in iteritems(expected_minute)
        }

        stats = [
            'pnl', 'capital_used', 'starting_cash', 'ending_cash',
            'starting_value', 'ending_value', 'portfolio_value', 'returns'
        ]

        expected_cumulative = deepcopy(expected_minute)

        # &quot;Add&quot; daily return from 1/03 to minute returns on 1/04 and 1/05
        # &quot;Add&quot; daily return from 1/04 to minute returns on 1/05
        expected_cumulative['returns'][390:] = \
            (expected_cumulative['returns'][390:] + 1) * \
            (expected_daily['returns'][0] + 1) - 1
        expected_cumulative['returns'][780:] = \
            (expected_cumulative['returns'][780:] + 1) * \
            (expected_daily['returns'][1] + 1) - 1

        # Add daily pnl/capital_used from 1/03 to 1/04 and 1/05
        # Add daily pnl/capital_used from 1/04 to 1/05
        expected_cumulative['pnl'][390:] += expected_daily['pnl'][0]
        expected_cumulative['pnl'][780:] += expected_daily['pnl'][1]
<A NAME="22"></A>        expected_cumulative['capital_used'][390:] += \
            expected_daily['capital_used'][0]
        expected_cumulative['capital_used'][780:] += \
            expected_daily['capital_used']<FONT color="#4cc417"><A HREF="javascript:ZweiFrames('match48-1.html#22',3,'match48-top.html#22',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>[1]

        # starting_cash, starting_value are same as those of the first daily
        # packet
        expected_cumulative['starting_cash'] = \
            np.repeat(expected_daily['starting_cash'][0:1], 1170)
        expected_cumulative['starting_value'] = \
            np.repeat(expected_daily['starting_value'][</B></FONT>0:1], 1170)

        # extra cumulative packet per day from the daily packet
        for stat in stats:
            for i in (390, 781, 1172):
                expected_cumulative[stat] = np.insert(
                    expected_cumulative[stat],
                    i,
                    expected_cumulative[stat][i-1]
                )

        for stat in stats:
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in minute_perf]),
                expected_minute[stat]
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in daily_perf]),
                expected_daily[stat]
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in cumulative_perf]),
                expected_cumulative[stat]
            )

        if change_loc == 'interday':
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04', tz='UTC'): 1000.0}
            )
        else:
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04 17:00', tz='UTC'): 500.0,
                 pd.Timestamp('2006-01-04 18:00', tz='UTC'): 500.0}
            )


class TestGetDatetime(zf.WithMakeAlgo, zf.ZiplineTestCase):
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    START_DATE = to_utc('2014-01-02 9:31')
    END_DATE = to_utc('2014-01-03 9:31')

    ASSET_FINDER_EQUITY_SIDS = 0, 1

    # FIXME: Pass a benchmark source explicitly here.
    BENCHMARK_SID = None

    @parameterized.expand(
        [
            ('default', None,),
            ('utc', 'UTC',),
            ('us_east', 'US/Eastern',),
        ]
    )
    def test_get_datetime(self, name, tz):
        algo = dedent(
            &quot;&quot;&quot;
            import pandas as pd
            from zipline.api import get_datetime

            def initialize(context):
                context.tz = {tz} or 'UTC'
                context.first_bar = True

            def handle_data(context, data):
                dt = get_datetime({tz})
                if dt.tz.zone != context.tz:
                    raise ValueError(&quot;Mismatched Zone&quot;)

                if context.first_bar:
                    if dt.tz_convert(&quot;US/Eastern&quot;).hour != 9:
                        raise ValueError(&quot;Mismatched Hour&quot;)
                    elif dt.tz_convert(&quot;US/Eastern&quot;).minute != 31:
                        raise ValueError(&quot;Mismatched Minute&quot;)

                    context.first_bar = False
            &quot;&quot;&quot;.format(tz=repr(tz))
        )

        algo = self.make_algo(script=algo)
        algo.run()
        self.assertFalse(algo.first_bar)


class TestTradingControls(zf.WithMakeAlgo,
                          zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-01-06', tz='utc')

    sid = 133
    sids = ASSET_FINDER_EQUITY_SIDS = 133, 134

    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = True

    @classmethod
    def init_class_fixtures(cls):
        super(TestTradingControls, cls).init_class_fixtures()
        cls.asset = cls.asset_finder.retrieve_asset(cls.sid)
        cls.another_asset = cls.asset_finder.retrieve_asset(134)

    def _check_algo(self,
                    algo,
                    expected_order_count,
                    expected_exc):

        with self.assertRaises(expected_exc) if expected_exc else nop_context:
            algo.run()
        self.assertEqual(algo.order_count, expected_order_count)

    def check_algo_succeeds(self, algo, order_count=4):
        # Default for order_count assumes one order per handle_data call.
        self._check_algo(algo, order_count, None)

    def check_algo_fails(self, algo, order_count):
        self._check_algo(algo,
                         order_count,
                         TradingControlViolation)

    def test_set_max_position_size(self):

        def initialize(self, asset, max_shares, max_notional):
            self.set_slippage(FixedSlippage())
            self.order_count = 0
            self.set_max_position_size(asset=asset,
                                       max_shares=max_shares,
                                       max_notional=max_notional)

        # Buy one share four times.  Should be fine.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 1)
            algo.order_count += 1

        algo = self.make_algo(
            asset=self.asset,
            max_shares=10,
            max_notional=500.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_succeeds(algo)

        # Buy three shares four times.  Should bail on the fourth before it's
        # placed.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 3)
            algo.order_count += 1

        algo = self.make_algo(
            asset=self.asset,
            max_shares=10,
            max_notional=500.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 3)

        # Buy three shares four times. Should bail due to max_notional on the
        # third attempt.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 3)
            algo.order_count += 1

        algo = self.make_algo(
            asset=self.asset,
            max_shares=10,
            max_notional=67.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 2)

        # Set the trading control to a different sid, then BUY ALL THE THINGS!.
        # Should continue normally.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1

        algo = self.make_algo(
            asset=self.another_asset,
            max_shares=10,
            max_notional=67.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_succeeds(algo)

        # Set the trading control sid to None, then BUY ALL THE THINGS!. Should
        # fail because setting sid to None makes the control apply to all sids.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1

        algo = self.make_algo(
            max_shares=10,
            max_notional=61.0,
            asset=None,
            initialize=initialize,
            handle_data=handle_data,
        )

        self.check_algo_fails(algo, 0)

    def test_set_asset_restrictions(self):

        def initialize(algo, sid, restrictions, on_error):
            algo.order_count = 0
<A NAME="32"></A>            algo.set_asset_restrictions(restrictions, on_error)

        def handle_data(algo, data):
            algo.could_trade = data<FONT color="#5b8daf"><A HREF="javascript:ZweiFrames('match48-1.html#32',3,'match48-top.html#32',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.can_trade(algo.sid(self.sid))
            algo.order(algo.sid(self.sid), 100)
            algo.order_count +=</B></FONT> 1

        # Set HistoricalRestrictions for one sid for the entire simulation,
        # and fail.
        rlm = HistoricalRestrictions([
            Restriction(
                self.sid,
                self.sim_params.start_session,
                RESTRICTION_STATES.FROZEN)
        ])
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='fail',
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade)

        # Set StaticRestrictions for one sid and fail.
        rlm = StaticRestrictions([self.sid])
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='fail',
            initialize=initialize,
            handle_data=handle_data,
        )

        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade)

        # just log an error on the violation if we choose not to fail.
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='log',
            initialize=initialize,
            handle_data=handle_data,
        )
        with make_test_handler(self) as log_catcher:
            self.check_algo_succeeds(algo)
        logs = [r.message for r in log_catcher.records]
        self.assertIn(&quot;Order for 100 shares of Equity(133 [A]) at &quot;
                      &quot;2006-01-03 21:00:00+00:00 violates trading constraint &quot;
                      &quot;RestrictedListOrder({})&quot;, logs)
        self.assertFalse(algo.could_trade)

        # set the restricted list to exclude the sid, and succeed
        rlm = HistoricalRestrictions([
            Restriction(
                sid,
                self.sim_params.start_session,
                RESTRICTION_STATES.FROZEN) for sid in [134, 135, 136]
        ])
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='fail',
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_succeeds(algo)
        self.assertTrue(algo.could_trade)

    @parameterized.expand([
        ('order_first_restricted_sid', 0),
        ('order_second_restricted_sid', 1)
    ])
    def test_set_multiple_asset_restrictions(self, name, to_order_idx):

        def initialize(algo, restrictions1, restrictions2, on_error):
            algo.order_count = 0
            algo.set_asset_restrictions(restrictions1, on_error)
            algo.set_asset_restrictions(restrictions2, on_error)

        def handle_data(algo, data):
            algo.could_trade1 = data.can_trade(algo.sid(self.sids[0]))
            algo.could_trade2 = data.can_trade(algo.sid(self.sids[1]))
            algo.order(algo.sid(self.sids[to_order_idx]), 100)
            algo.order_count += 1

        rl1 = StaticRestrictions([self.sids[0]])
        rl2 = StaticRestrictions([self.sids[1]])
        algo = self.make_algo(
            restrictions1=rl1,
            restrictions2=rl2,
            initialize=initialize,
            handle_data=handle_data,
            on_error='fail',
        )
        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade1)
        self.assertFalse(algo.could_trade2)

    def test_set_do_not_order_list(self):

        def initialize(self, restricted_list):
            self.order_count = 0
            self.set_do_not_order_list(restricted_list, on_error='fail')

        def handle_data(algo, data):
            algo.could_trade = data.can_trade(algo.sid(self.sid))
            algo.order(algo.sid(self.sid), 100)
            algo.order_count += 1

        rlm = [self.sid]
        algo = self.make_algo(
            restricted_list=rlm,
            initialize=initialize,
            handle_data=handle_data,
        )

        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade)

    def test_set_max_order_size(self):

        def initialize(algo, asset, max_shares, max_notional):
            algo.order_count = 0
            algo.set_max_order_size(asset=asset,
                                    max_shares=max_shares,
                                    max_notional=max_notional)

        # Buy one share.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 1)
            algo.order_count += 1

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.asset,
            max_shares=10,
            max_notional=500.0,
        )
        self.check_algo_succeeds(algo)

        # Buy 1, then 2, then 3, then 4 shares.  Bail on the last attempt
        # because we exceed shares.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), algo.order_count + 1)
            algo.order_count += 1

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.asset,
            max_shares=3,
            max_notional=500.0,
        )
        self.check_algo_fails(algo, 3)

        # Buy 1, then 2, then 3, then 4 shares.  Bail on the last attempt
        # because we exceed notional.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), algo.order_count + 1)
            algo.order_count += 1

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.asset,
            max_shares=10,
            max_notional=40.0,
        )
        self.check_algo_fails(algo, 3)

        # Set the trading control to a different sid, then BUY ALL THE THINGS!.
        # Should continue normally.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.another_asset,
            max_shares=1,
            max_notional=1.0,
        )
        self.check_algo_succeeds(algo)

        # Set the trading control sid to None, then BUY ALL THE THINGS!.
        # Should fail because not specifying a sid makes the trading control
        # apply to all sids.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=None,
            max_shares=1,
            max_notional=1.0,
        )
        self.check_algo_fails(algo, 0)

    def test_set_max_order_count(self):

        def initialize(algo, count):
            algo.order_count = 0
            algo.set_max_order_count(count)

        def handle_data(algo, data):
            for i in range(5):
                algo.order(self.asset, 1)
                algo.order_count += 1

        algo = self.make_algo(
            count=3,
            initialize=initialize,
            handle_data=handle_data,
        )
        with self.assertRaises(TradingControlViolation):
            algo.run()

        self.assertEqual(algo.order_count, 3)

    def test_set_max_order_count_minutely(self):
        sim_params = self.make_simparams(data_frequency='minute')

        def initialize(algo, max_orders_per_day):
            algo.minute_count = 0
            algo.order_count = 0
            algo.set_max_order_count(max_orders_per_day)

        # Order 5 times twice in a single day, and set a max order count of
        # 9. The last order of the second batch should fail.
        def handle_data(algo, data):
            if algo.minute_count == 0 or algo.minute_count == 100:
                for i in range(5):
                    algo.order(self.asset, 1)
                    algo.order_count += 1

            algo.minute_count += 1

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_orders_per_day=9,
            sim_params=sim_params,
        )

        with self.assertRaises(TradingControlViolation):
            algo.run()

        self.assertEqual(algo.order_count, 9)

        # Set a limit of 5 orders per day, and order 5 times in the first
        # minute of each day. This should succeed because the counter gets
        # reset each day.
        def handle_data(algo, data):
            if (algo.minute_count % 390) == 0:
                for i in range(5):
                    algo.order(self.asset, 1)
                    algo.order_count += 1
<A NAME="31"></A>
            algo.minute_count += 1

        algo <FONT color="#3ea99f"><A HREF="javascript:ZweiFrames('match48-1.html#31',3,'match48-top.html#31',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>= self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_orders_per_day=5,
            sim_params=sim_params,
        )
        algo.run()

        # 5 orders per day times 4 days.
        self.assertEqual(algo.</B></FONT>order_count, 20)

    def test_long_only(self):
        def initialize(algo):
            algo.order_count = 0
            algo.set_long_only()

        # Sell immediately -&gt; fail immediately.
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), -1)
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_fails(algo, 0)

        # Buy on even days, sell on odd days.  Never takes a short position, so
        # should succeed.
        def handle_data(algo, data):
            if (algo.order_count % 2) == 0:
                algo.order(algo.sid(self.sid), 1)
            else:
                algo.order(algo.sid(self.sid), -1)
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_succeeds(algo)

        # Buy on first three days, then sell off holdings.  Should succeed.
        def handle_data(algo, data):
            amounts = [1, 1, 1, -3]
            algo.order(algo.sid(self.sid), amounts[algo.order_count])
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_succeeds(algo)

        # Buy on first three days, then sell off holdings plus an extra share.
        # Should fail on the last sale.
        def handle_data(algo, data):
            amounts = [1, 1, 1, -4]
            algo.order(algo.sid(self.sid), amounts[algo.order_count])
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_fails(algo, 3)

    def test_register_post_init(self):

        def initialize(algo):
            algo.initialized = True

        def handle_data(algo, data):
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_max_position_size(self.sid, 1, 1)
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_max_order_size(self.sid, 1, 1)
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_max_order_count(1)
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_long_only()

        self.run_algorithm(initialize=initialize, handle_data=handle_data)


class TestAssetDateBounds(zf.WithMakeAlgo, zf.ZiplineTestCase):

    START_DATE = pd.Timestamp('2014-01-02', tz='UTC')
    END_DATE = pd.Timestamp('2014-01-03', tz='UTC')
    SIM_PARAMS_START_DATE = END_DATE  # Only run for one day.

    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False

    BENCHMARK_SID = 3

    @classmethod
    def make_equity_info(cls):
        T = partial(pd.Timestamp, tz='UTC')
        return pd.DataFrame.from_records([
            {'sid': 1,
             'symbol': 'OLD',
             'start_date': T('1990'),
             'end_date': T('1991'),
             'exchange': 'TEST'},
            {'sid': 2,
             'symbol': 'NEW',
             'start_date': T('2017'),
             'end_date': T('2018'),
             'exchange': 'TEST'},
            {'sid': 3,
             'symbol': 'GOOD',
             'start_date': cls.START_DATE,
             'end_date': cls.END_DATE,
             'exchange': 'TEST'},
        ])

    def test_asset_date_bounds(self):
        def initialize(algo):
            algo.ran = False
            algo.register_trading_control(AssetDateBounds(on_error='fail'))

        def handle_data(algo, data):
            # This should work because sid 3 is valid during the algo lifetime.
            algo.order(algo.sid(3), 1)

            # Sid already expired.
            with self.assertRaises(TradingControlViolation):
                algo.order(algo.sid(1), 1)

            # Sid doesn't exist yet.
            with self.assertRaises(TradingControlViolation):
                algo.order(algo.sid(2), 1)

            algo.ran = True

        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        algo.run()
        self.assertTrue(algo.ran)


class TestAccountControls(zf.WithMakeAlgo,
                          zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-01-06', tz='utc')

    sidint, = ASSET_FINDER_EQUITY_SIDS = (133,)
    BENCHMARK_SID = None
<A NAME="17"></A>    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False

    <FONT color="#3090c7"><A HREF="javascript:ZweiFrames('match48-1.html#17',3,'match48-top.html#17',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>@classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        frame = pd.DataFrame(data={
            'close': [10., 10., 11., 11.],
            'open': [10., 10., 11., 11.],
            'low': [9.5, 9.5, 10.45, 10.45],
            'high': [10.5, 10.5, 11.55, 11.55],
            'volume': [100, 100, 100, 300],
        }, index=cls.</B></FONT>equity_daily_bar_days)
        yield cls.sidint, frame

    def _check_algo(self, algo, expected_exc):
        with self.assertRaises(expected_exc) if expected_exc else nop_context:
            algo.run()

    def check_algo_succeeds(self, algo):
        # Default for order_count assumes one order per handle_data call.
        self._check_algo(algo, None)

    def check_algo_fails(self, algo):
        self._check_algo(algo, AccountControlViolation)

    def test_set_max_leverage(self):

        def initialize(algo, max_leverage):
            algo.set_max_leverage(max_leverage=max_leverage)

        def handle_data(algo, data):
            algo.order(algo.sid(self.sidint), 1)
            algo.record(latest_time=algo.get_datetime())

        # Set max leverage to 0 so buying one share fails.
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_leverage=0,
        )
        self.check_algo_fails(algo)
        self.assertEqual(
            algo.recorded_vars['latest_time'],
            pd.Timestamp('2006-01-04 21:00:00', tz='UTC'),
        )

        # Set max leverage to 1 so buying one share passes
        def handle_data(algo, data):
            algo.order(algo.sid(self.sidint), 1)

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_leverage=1,
        )
        self.check_algo_succeeds(algo)

    def test_set_min_leverage(self):
        def initialize(algo, min_leverage, grace_period):
            algo.set_min_leverage(
                min_leverage=min_leverage, grace_period=grace_period
            )

        def handle_data(algo, data):
            algo.order_target_percent(algo.sid(self.sidint), .5)
            algo.record(latest_time=algo.get_datetime())

        # Helper for not having to pass init/handle_data at each callsite.
        def make_algo(min_leverage, grace_period):
            return self.make_algo(
                initialize=initialize,
                handle_data=handle_data,
                min_leverage=min_leverage,
                grace_period=grace_period,
            )

        # Set min leverage to 1.
        # The algorithm will succeed because it doesn't run for more
        # than 10 days.
        offset = pd.Timedelta('10 days')
        algo = make_algo(min_leverage=1, grace_period=offset)
        self.check_algo_succeeds(algo)

        # The algorithm will fail because it doesn't reach a min leverage of 1
        # after 1 day.
        offset = pd.Timedelta('1 days')
        algo = make_algo(min_leverage=1, grace_period=offset)
        self.check_algo_fails(algo)
        self.assertEqual(
            algo.recorded_vars['latest_time'],
            pd.Timestamp('2006-01-04 21:00:00', tz='UTC'),
        )

        # Increase the offset to 2 days, and the algorithm fails a day later
        offset = pd.Timedelta('2 days')
        algo = make_algo(min_leverage=1, grace_period=offset)
        self.check_algo_fails(algo)
        self.assertEqual(
            algo.recorded_vars['latest_time'],
            pd.Timestamp('2006-01-05 21:00:00', tz='UTC'),
        )

        # Set the min_leverage to .0001 and the algorithm succeeds.
        algo = make_algo(min_leverage=.0001, grace_period=offset)
        self.check_algo_succeeds(algo)


class TestFuturesAlgo(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-06', tz='utc')
    END_DATE = pd.Timestamp('2016-01-07', tz='utc')
    FUTURE_MINUTE_BAR_START_DATE = pd.Timestamp('2016-01-05', tz='UTC')

    SIM_PARAMS_DATA_FREQUENCY = 'minute'

    TRADING_CALENDAR_STRS = ('us_futures',)
    TRADING_CALENDAR_PRIMARY_CAL = 'us_futures'
    BENCHMARK_SID = None

    @classmethod
    def make_futures_info(cls):
        return pd.DataFrame.from_dict(
            {
                1: {
                    'symbol': 'CLG16',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2015-12-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2016-01-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2016-02-19', tz='UTC'),
                    'auto_close_date': pd.Timestamp('2016-01-18', tz='UTC'),
                    'exchange': 'TEST',
                },
            },
            orient='index',
        )

    def test_futures_history(self):
        algo_code = dedent(
            &quot;&quot;&quot;
            from datetime import time
            from zipline.api import (
                date_rules,
                get_datetime,
                schedule_function,
                sid,
                time_rules,
            )

            def initialize(context):
                context.history_values = []

                schedule_function(
                    make_history_call,
                    date_rules.every_day(),
                    time_rules.market_open(),
                )

                schedule_function(
                    check_market_close_time,
                    date_rules.every_day(),
                    time_rules.market_close(),
                )

            def make_history_call(context, data):
                # Ensure that the market open is 6:31am US/Eastern.
                open_time = get_datetime().tz_convert('US/Eastern').time()
                assert open_time == time(6, 31)
                context.history_values.append(
                    data.history(sid(1), 'close', 5, '1m'),
                )

            def check_market_close_time(context, data):
                # Ensure that this function is called at 4:59pm US/Eastern.
                # By default, `market_close()` uses an offset of 1 minute.
                close_time = get_datetime().tz_convert('US/Eastern').time()
                assert close_time == time(16, 59)
            &quot;&quot;&quot;
        )

        algo = self.make_algo(
            script=algo_code,
            trading_calendar=get_calendar('us_futures'),
        )
        algo.run()

        # Assert that we were able to retrieve history data for minutes outside
        # of the 6:31am US/Eastern to 5:00pm US/Eastern futures open times.
        np.testing.assert_array_equal(
            algo.history_values[0].index,
            pd.date_range(
                '2016-01-06 6:27',
                '2016-01-06 6:31',
                freq='min',
                tz='US/Eastern',
            ),
        )
        np.testing.assert_array_equal(
            algo.history_values[1].index,
            pd.date_range(
                '2016-01-07 6:27',
                '2016-01-07 6:31',
                freq='min',
                tz='US/Eastern',
            ),
        )

        # Expected prices here are given by the range values created by the
        # default `make_future_minute_bar_data` method.
        np.testing.assert_array_equal(
            algo.history_values[0].values, list(map(float, range(2196, 2201))),
        )
        np.testing.assert_array_equal(
            algo.history_values[1].values, list(map(float, range(3636, 3641))),
        )

    @staticmethod
    def algo_with_slippage(slippage_model):
        return dedent(
            &quot;&quot;&quot;
            from zipline.api import (
                commission,
                order,
                set_commission,
                set_slippage,
                sid,
                slippage,
                get_datetime,
            )

            def initialize(context):
                commission_model = commission.PerFutureTrade(0)
                set_commission(us_futures=commission_model)
                slippage_model = slippage.{model}
                set_slippage(us_futures=slippage_model)
                context.ordered = False

            def handle_data(context, data):
                if not context.ordered:
                    order(sid(1), 10)
                    context.ordered = True
                    context.order_price = data.current(sid(1), 'price')
            &quot;&quot;&quot;
        ).format(model=slippage_model)

    def test_fixed_future_slippage(self):
        algo_code = self.algo_with_slippage('FixedSlippage(spread=0.10)')
        algo = self.make_algo(
            script=algo_code,
            trading_calendar=get_calendar('us_futures'),
        )
        results = algo.run()

        # Flatten the list of transactions.
        all_txns = [
            val for sublist in results['transactions'].tolist()
            for val in sublist
        ]

        self.assertEqual(len(all_txns), 1)
        txn = all_txns[0]

        # Add 1 to the expected price because the order does not fill until the
        # bar after the price is recorded.
        expected_spread = 0.05
        expected_price = (algo.order_price + 1) + expected_spread

        self.assertEqual(txn['price'], expected_price)
        self.assertEqual(results['orders'][0][0]['commission'], 0.0)

    def test_volume_contract_slippage(self):
        algo_code = self.algo_with_slippage(
            'VolumeShareSlippage(volume_limit=0.05, price_impact=0.1)',
        )
        algo = self.make_algo(
            script=algo_code,
            trading_calendar=get_calendar('us_futures'),
        )
        results = algo.run()

        # There should be no commissions.
        self.assertEqual(results['orders'][0][0]['commission'], 0.0)

        # Flatten the list of transactions.
        all_txns = [
            val for sublist in results['transactions'].tolist()
            for val in sublist
        ]

        # With a volume limit of 0.05, and a total volume of 100 contracts
        # traded per minute, we should require 2 transactions to order 10
        # contracts.
        self.assertEqual(len(all_txns), 2)

        for i, txn in enumerate(all_txns):
            # Add 1 to the order price because the order does not fill until
            # the bar after the price is recorded.
            order_price = algo.order_price + i + 1
            expected_impact = order_price * 0.1 * (0.05 ** 2)
            expected_price = order_price + expected_impact
            self.assertEqual(txn['price'], expected_price)


class TestAnalyzeAPIMethod(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-05', tz='utc')
    END_DATE = pd.Timestamp('2016-01-05', tz='utc')
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False

    def test_analyze_called(self):
        self.perf_ref = None

        def initialize(context):
            pass

        def handle_data(context, data):
            pass

        def analyze(context, perf):
            self.perf_ref = perf

        algo = self.make_algo(
            initialize=initialize, handle_data=handle_data, analyze=analyze,
        )
        results = algo.run()
        self.assertIs(results, self.perf_ref)


class TestOrderCancelation(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-05', tz='utc')
    END_DATE = pd.Timestamp('2016-01-07', tz='utc')

    ASSET_FINDER_EQUITY_SIDS = (1,)
    ASSET_FINDER_EQUITY_SYMBOLS = ('ASSET1',)
    BENCHMARK_SID = None

    code = dedent(
        &quot;&quot;&quot;
        from zipline.api import (
            sid, order, set_slippage, slippage, VolumeShareSlippage,
            set_cancel_policy, cancel_policy, EODCancel
        )


        def initialize(context):
            set_slippage(
                slippage.VolumeShareSlippage(
                    volume_limit=1,
                    price_impact=0
                )
            )

            {0}
            context.ordered = False


        def handle_data(context, data):
            if not context.ordered:
                order(sid(1), {1})
                context.ordered = True
        &quot;&quot;&quot;,
    )

    @classmethod
    def make_equity_minute_bar_data(cls):
        asset_minutes = \
            cls.trading_calendar.minutes_for_sessions_in_range(
                cls.START_DATE,
                cls.END_DATE,
            )

        minutes_count = len(asset_minutes)
        minutes_arr = np.arange(1, 1 + minutes_count)

        # normal test data, but volume is pinned at 1 share per minute
        yield 1, pd.DataFrame(
            {
                'open': minutes_arr + 1,
                'high': minutes_arr + 2,
                'low': minutes_arr - 1,
                'close': minutes_arr,
                'volume': np.full(minutes_count, 1.0),
            },
            index=asset_minutes,
        )

    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        yield 1, pd.DataFrame(
            {
                'open': np.full(3, 1, dtype=np.float64),
                'high': np.full(3, 1, dtype=np.float64),
                'low': np.full(3, 1, dtype=np.float64),
                'close': np.full(3, 1, dtype=np.float64),
                'volume': np.full(3, 1, dtype=np.float64),
            },
            index=cls.equity_daily_bar_days,
        )

    def prep_algo(self,
                  cancelation_string,
                  data_frequency=&quot;minute&quot;,
                  amount=1000,
                  minute_emission=False):
        code = self.code.format(cancelation_string, amount)
        return self.make_algo(
            script=code,
            sim_params=self.make_simparams(
                data_frequency=data_frequency,
                emission_rate='minute' if minute_emission else 'daily',
            )
        )

    @parameter_space(
        direction=[1, -1],
        minute_emission=[True, False],
    )
    def test_eod_order_cancel_minute(self, direction, minute_emission):
        &quot;&quot;&quot;
        Test that EOD order cancel works in minute mode for both shorts and
        longs, and both daily emission and minute emission
        &quot;&quot;&quot;
        # order 1000 shares of asset1.  the volume is only 1 share per bar,
        # so the order should be cancelled at the end of the day.
        algo = self.prep_algo(
            &quot;set_cancel_policy(cancel_policy.EODCancel())&quot;,
            amount=np.copysign(1000, direction),
            minute_emission=minute_emission
        )

        log_catcher = TestHandler()
        with log_catcher:
<A NAME="21"></A>            results = algo.run()

            for daily_positions in results.positions:
                self.assertEqual(1, len<FONT color="#947010"><A HREF="javascript:ZweiFrames('match48-1.html#21',3,'match48-top.html#21',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>(daily_positions))
                self.assertEqual(
                    np.copysign(389, direction),
                    daily_positions[0][&quot;amount&quot;],
                )
                self.assertEqual(1, results.positions[0][0][</B></FONT>&quot;sid&quot;])

            # should be an order on day1, but no more orders afterwards
            np.testing.assert_array_equal([1, 0, 0],
                                          list(map(len, results.orders)))

            # should be 389 txns on day 1, but no more afterwards
            np.testing.assert_array_equal([389, 0, 0],
                                          list(map(len, results.transactions)))

            the_order = results.orders[0][0]

            self.assertEqual(ORDER_STATUS.CANCELLED, the_order[&quot;status&quot;])
            self.assertEqual(np.copysign(389, direction), the_order[&quot;filled&quot;])

            warnings = [record for record in log_catcher.records if
                        record.level == WARNING]

            self.assertEqual(1, len(warnings))

            if direction == 1:
                self.assertEqual(
                    &quot;Your order for 1000 shares of ASSET1 has been partially &quot;
                    &quot;filled. 389 shares were successfully purchased. &quot;
                    &quot;611 shares were not filled by the end of day and &quot;
                    &quot;were canceled.&quot;,
                    str(warnings[0].message)
                )
            elif direction == -1:
                self.assertEqual(
                    &quot;Your order for -1000 shares of ASSET1 has been partially &quot;
                    &quot;filled. 389 shares were successfully sold. &quot;
                    &quot;611 shares were not filled by the end of day and &quot;
                    &quot;were canceled.&quot;,
                    str(warnings[0].message)
                )

    def test_default_cancelation_policy(self):
        algo = self.prep_algo(&quot;&quot;)

        log_catcher = TestHandler()
        with log_catcher:
            results = algo.run()

            # order stays open throughout simulation
            np.testing.assert_array_equal([1, 1, 1],
                                          list(map(len, results.orders)))

            # one txn per minute.  389 the first day (since no order until the
            # end of the first minute).  390 on the second day.  221 on the
            # the last day, sum = 1000.
            np.testing.assert_array_equal([389, 390, 221],
                                          list(map(len, results.transactions)))

            self.assertFalse(log_catcher.has_warnings)

    def test_eod_order_cancel_daily(self):
        # in daily mode, EODCancel does nothing.
        algo = self.prep_algo(
            &quot;set_cancel_policy(cancel_policy.EODCancel())&quot;,
            &quot;daily&quot;
        )

        log_catcher = TestHandler()
        with log_catcher:
            results = algo.run()

            # order stays open throughout simulation
            np.testing.assert_array_equal([1, 1, 1],
                                          list(map(len, results.orders)))

            # one txn per day
            np.testing.assert_array_equal([0, 1, 1],
                                          list(map(len, results.transactions)))

            self.assertFalse(log_catcher.has_warnings)


class TestDailyEquityAutoClose(zf.WithMakeAlgo, zf.ZiplineTestCase):
    &quot;&quot;&quot;
    Tests if delisted equities are properly removed from a portfolio holding
    positions in said equities.
    &quot;&quot;&quot;
    #     January 2015
    # Su Mo Tu We Th Fr Sa
    #              1  2  3
    #  4  5  6  7  8  9 10
    # 11 12 13 14 15 16 17
    # 18 19 20 21 22 23 24
    # 25 26 27 28 29 30 31
    START_DATE = pd.Timestamp('2015-01-05', tz='UTC')
    END_DATE = pd.Timestamp('2015-01-13', tz='UTC')

    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    BENCHMARK_SID = None

    @classmethod
    def init_class_fixtures(cls):
        super(TestDailyEquityAutoClose, cls).init_class_fixtures()
        cls.assets = (
            cls.asset_finder.retrieve_all(cls.asset_finder.equities_sids)
        )

    @classmethod
    def make_equity_info(cls):
        cls.test_days = cls.trading_calendar.sessions_in_range(
            cls.START_DATE, cls.END_DATE,
        )
        assert len(cls.test_days) == 7, &quot;Number of days in test changed!&quot;
        cls.first_asset_expiration = cls.test_days[2]

        # Assets start on start date and delist every two days:
        #
        #     start_date   end_date auto_close_date
        #   0 2015-01-05 2015-01-07      2015-01-09
        #   1 2015-01-05 2015-01-09      2015-01-13
        #   2 2015-01-05 2015-01-13      2015-01-15
        cls.asset_info = make_jagged_equity_info(
            num_assets=3,
            start_date=cls.test_days[0],
            first_end=cls.first_asset_expiration,
            frequency=cls.trading_calendar.day,
            periods_between_ends=2,
            auto_close_delta=2 * cls.trading_calendar.day,
        )
        return cls.asset_info

    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        cls.daily_data = make_trade_data_for_asset_info(
            dates=cls.test_days,
            asset_info=cls.asset_info,
            price_start=10,
            price_step_by_sid=10,
            price_step_by_date=1,
            volume_start=100,
            volume_step_by_sid=100,
            volume_step_by_date=10,
        )
        return cls.daily_data.items()

    def daily_prices_on_tick(self, row):
        return [
            trades.iloc[row].close for trades in itervalues(self.daily_data)
        ]

    def final_daily_price(self, asset):
        return self.daily_data[asset.sid].loc[asset.end_date].close

    def default_initialize(self):
        &quot;&quot;&quot;
        Initialize function shared between test algos.
        &quot;&quot;&quot;
        def initialize(context):
            context.ordered = False
            context.set_commission(PerShare(0, 0))
            context.set_slippage(FixedSlippage(spread=0))
            context.num_positions = []
            context.cash = []

        return initialize

    def default_handle_data(self, assets, order_size):
        &quot;&quot;&quot;
        Handle data function shared between test algos.
        &quot;&quot;&quot;
        def handle_data(context, data):
            if not context.ordered:
                for asset in assets:
                    context.order(asset, order_size)
                context.ordered = True

            context.cash.append(context.portfolio.cash)
            context.num_positions.append(len(context.portfolio.positions))

        return handle_data

    @parameter_space(
        order_size=[10, -10],
        capital_base=[1, 100000],
        __fail_fast=True,
    )
    def test_daily_delisted_equities(self,
                                     order_size,
                                     capital_base):
        &quot;&quot;&quot;
        Make sure that after an equity gets delisted, our portfolio holds the
        correct number of equities and correct amount of cash.
        &quot;&quot;&quot;
        assets = self.assets
        final_prices = {
            asset.sid: self.final_daily_price(asset)
            for asset in assets
        }

        # Prices at which we expect our orders to be filled.
        initial_fill_prices = self.daily_prices_on_tick(1)
        cost_basis = sum(initial_fill_prices) * order_size

        # Last known prices of assets that will be auto-closed.
        fp0 = final_prices[0]
        fp1 = final_prices[1]

        algo = self.make_algo(
            initialize=self.default_initialize(),
            handle_data=self.default_handle_data(assets, order_size),
            sim_params=self.make_simparams(
                capital_base=capital_base,
                data_frequency='daily',
            ),
        )
        output = algo.run()

        initial_cash = capital_base
        after_fills = initial_cash - cost_basis
        after_first_auto_close = after_fills + fp0 * (order_size)
        after_second_auto_close = after_first_auto_close + fp1 * (order_size)

        # Day 1: Order 10 shares of each equity; there are 3 equities.
        # Day 2: Order goes through at the day 2 price of each equity.
        # Day 3: End date of Equity 0.
        # Day 4: Nothing happens.
        # Day 5: End date of Equity 1. Auto close of equity 0.
        #        Add cash == (fp0 * size).
        # Day 6: Nothing happens.
        # Day 7: End date of Equity 2 and auto-close date of Equity 1.
        #        Add cash equal to (fp1 * size).
        expected_cash = [
            initial_cash,
            after_fills,
            after_fills,
            after_fills,
            after_first_auto_close,
            after_first_auto_close,
            after_second_auto_close,
        ]
        expected_num_positions = [0, 3, 3, 3, 2, 2, 1]

        # Check expected cash.
        self.assertEqual(expected_cash, list(output['ending_cash']))

        # The cash recorded by the algo should be behind by a day from the
        # computed ending cash.
        expected_cash.insert(3, after_fills)
        self.assertEqual(algo.cash, expected_cash[:-1])

        # Check expected long/short counts.
        # We have longs if order_size &gt; 0.
        # We have shorts if order_size &lt; 0.
        if order_size &gt; 0:
            self.assertEqual(
                expected_num_positions,
                list(output['longs_count']),
            )
            self.assertEqual(
                [0] * len(self.test_days),
                list(output['shorts_count']),
            )
        else:
            self.assertEqual(
                expected_num_positions,
                list(output['shorts_count']),
            )
            self.assertEqual(
                [0] * len(self.test_days),
                list(output['longs_count']),
            )

        # The number of positions recorded by the algo should be behind by a
        # day from the computed long/short counts.
        expected_num_positions.insert(3, 3)
        self.assertEqual(algo.num_positions, expected_num_positions[:-1])

        # Check expected transactions.
        # We should have a transaction of order_size shares per sid.
        transactions = output['transactions']
        initial_fills = transactions.iloc[1]
        self.assertEqual(len(initial_fills), len(assets))

        last_minute_of_session = \
            self.trading_calendar.session_close(self.test_days[1])

        for asset, txn in zip(assets, initial_fills):
            self.assertDictContainsSubset(
                {
                    'amount': order_size,
                    'commission': None,
                    'dt': last_minute_of_session,
                    'price': initial_fill_prices[asset],
                    'sid': asset,
                },
                txn,
            )
            # This will be a UUID.
            self.assertIsInstance(txn['order_id'], str)

        def transactions_for_date(date):
            return transactions.iloc[self.test_days.get_loc(date)]

        # We should have exactly one auto-close transaction on the close date
        # of asset 0.
        (first_auto_close_transaction,) = transactions_for_date(
            assets[0].auto_close_date
        )
        self.assertEqual(
            first_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': self.trading_calendar.session_close(
                    assets[0].auto_close_date,
                ),
                'price': fp0,
                'sid': assets[0],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )

        (second_auto_close_transaction,) = transactions_for_date(
            assets[1].auto_close_date
        )
        self.assertEqual(
            second_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': self.trading_calendar.session_close(
                    assets[1].auto_close_date,
                ),
                'price': fp1,
                'sid': assets[1],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )

    def test_cancel_open_orders(self):
        &quot;&quot;&quot;
        Test that any open orders for an equity that gets delisted are
        canceled.  Unless an equity is auto closed, any open orders for that
        equity will persist indefinitely.
        &quot;&quot;&quot;
        assets = self.assets
        first_asset_end_date = assets[0].end_date
        first_asset_auto_close_date = assets[0].auto_close_date

        def initialize(context):
            pass

        def handle_data(context, data):
            # The only order we place in this test should never be filled.
            assert (
                context.portfolio.cash == context.portfolio.starting_cash
            )

            today_session = self.trading_calendar.minute_to_session_label(
                context.get_datetime()
            )
            day_after_auto_close = self.trading_calendar.next_session_label(
                first_asset_auto_close_date,
            )

            if today_session == first_asset_end_date:
                # Equity 0 will no longer exist tomorrow, so this order will
                # never be filled.
                assert len(context.get_open_orders()) == 0
                context.order(context.sid(0), 10)
                assert len(context.get_open_orders()) == 1
            elif today_session == first_asset_auto_close_date:
                # We do not cancel open orders until the end of the auto close
                # date, so our open order should still exist at this point.
                assert len(context.get_open_orders()) == 1
            elif today_session == day_after_auto_close:
                assert len(context.get_open_orders()) == 0

        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            sim_params=self.make_simparams(
                data_frequency='daily',
            ),
        )
        results = algo.run()

        orders = results['orders']

        def orders_for_date(date):
            return orders.iloc[self.test_days.get_loc(date)]

        original_open_orders = orders_for_date(first_asset_end_date)
        assert len(original_open_orders) == 1

        last_close_for_asset = \
            algo.trading_calendar.session_close(first_asset_end_date)

        self.assertDictContainsSubset(
            {
                'amount': 10,
                'commission': 0.0,
                'created': last_close_for_asset,
                'dt': last_close_for_asset,
                'sid': assets[0],
                'status': ORDER_STATUS.OPEN,
                'filled': 0,
            },
            original_open_orders[0],
        )

        orders_after_auto_close = orders_for_date(first_asset_auto_close_date)
        assert len(orders_after_auto_close) == 1
        self.assertDictContainsSubset(
            {
                'amount': 10,
                'commission': 0.0,
                'created': last_close_for_asset,
                'dt': algo.trading_calendar.session_close(
                    first_asset_auto_close_date,
                ),
                'sid': assets[0],
                'status': ORDER_STATUS.CANCELLED,
                'filled': 0,
            },
            orders_after_auto_close[0],
        )


# NOTE: This suite is almost the same as TestDailyEquityAutoClose, except it
# uses minutely data instead of daily data, and the auto_close_date for
# equities is one day after their end_date instead of two.
class TestMinutelyEquityAutoClose(zf.WithMakeAlgo,
                                  zf.ZiplineTestCase):
    #     January 2015
    # Su Mo Tu We Th Fr Sa
    #              1  2  3
    #  4  5  6  7  8  9 10
    # 11 12 13 14 15 16 17
    # 18 19 20 21 22 23 24
    # 25 26 27 28 29 30 31
    START_DATE = pd.Timestamp('2015-01-05', tz='UTC')
    END_DATE = pd.Timestamp('2015-01-13', tz='UTC')

    BENCHMARK_SID = None

    @classmethod
    def init_class_fixtures(cls):
        super(TestMinutelyEquityAutoClose, cls).init_class_fixtures()
        cls.assets = (
            cls.asset_finder.retrieve_all(cls.asset_finder.equities_sids)
        )

    @classmethod
    def make_equity_info(cls):
        cls.test_days = cls.trading_calendar.sessions_in_range(
            cls.START_DATE, cls.END_DATE,
        )
        cls.test_minutes = cls.trading_calendar.minutes_for_sessions_in_range(
            cls.START_DATE, cls.END_DATE,
        )
        cls.first_asset_expiration = cls.test_days[2]

        # Assets start on start date and delist every two days:
        #
        #     start_date   end_date auto_close_date
        #   0 2015-01-05 2015-01-07      2015-01-09
        #   1 2015-01-05 2015-01-09      2015-01-13
        #   2 2015-01-05 2015-01-13      2015-01-15
        cls.asset_info = make_jagged_equity_info(
            num_assets=3,
            start_date=cls.test_days[0],
            first_end=cls.first_asset_expiration,
            frequency=cls.trading_calendar.day,
            periods_between_ends=2,
            auto_close_delta=1 * cls.trading_calendar.day,
        )
        return cls.asset_info

    # XXX: This test suite uses inconsistent data for minutely and daily bars.
    @classmethod
    def make_equity_minute_bar_data(cls):
        cls.minute_data = make_trade_data_for_asset_info(
            dates=cls.test_minutes,
            asset_info=cls.asset_info,
            price_start=10,
            price_step_by_sid=10,
            price_step_by_date=1,
            volume_start=100,
            volume_step_by_sid=100,
            volume_step_by_date=10,
        )
        return cls.minute_data.items()

    def minute_prices_on_tick(self, row):
        return [
            trades.iloc[row].close for trades in itervalues(self.minute_data)
        ]

    def final_minute_price(self, asset):
        return self.minute_data[asset.sid].loc[
            self.trading_calendar.session_close(asset.end_date)
        ].close

    def default_initialize(self):
        &quot;&quot;&quot;
        Initialize function shared between test algos.
        &quot;&quot;&quot;
        def initialize(context):
            context.ordered = False
            context.set_commission(PerShare(0, 0))
            context.set_slippage(FixedSlippage(spread=0))
            context.num_positions = []
            context.cash = []

        return initialize

    def default_handle_data(self, assets, order_size):
        &quot;&quot;&quot;
        Handle data function shared between test algos.
        &quot;&quot;&quot;
        def handle_data(context, data):
            if not context.ordered:
                for asset in assets:
                    context.order(asset, order_size)
                context.ordered = True

            context.cash.append(context.portfolio.cash)
            context.num_positions.append(len(context.portfolio.positions))

        return handle_data

    def test_minutely_delisted_equities(self):
        assets = self.assets
        final_prices = {
            asset.sid: self.final_minute_price(asset)
            for asset in assets
        }
        backtest_minutes = self.minute_data[0].index.tolist()

        order_size = 10

<A NAME="30"></A>        capital_base = 100000
        algo = self.make_algo(
            initialize=self.default_initialize(),
            handle_data=self.default_handle_data<FONT color="#ae694a"><A HREF="javascript:ZweiFrames('match48-1.html#30',3,'match48-top.html#30',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>(assets, order_size),
            sim_params=self.make_simparams(
                capital_base=capital_base,
                data_frequency='minute',
            )
        )

        output = algo.run()
        initial_fill_prices = self.minute_prices_on_tick(</B></FONT>1)
        cost_basis = sum(initial_fill_prices) * order_size

        # Last known prices of assets that will be auto-closed.
        fp0 = final_prices[0]
        fp1 = final_prices[1]

        initial_cash = capital_base
        after_fills = initial_cash - cost_basis
        after_first_auto_close = after_fills + fp0 * (order_size)
        after_second_auto_close = after_first_auto_close + fp1 * (order_size)

        expected_cash = [initial_cash]
        expected_position_counts = [0]
<A NAME="16"></A>
        # We have the rest of the first sim day, plus the second, third and
        # fourth days' worth of minutes with cash spent.
        expected_cash<FONT color="#2981b2"><A HREF="javascript:ZweiFrames('match48-1.html#16',3,'match48-top.html#16',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.extend([after_fills] * (389 + 390 + 390 + 390))
        expected_position_counts.extend([3] * (389 + 390 + 390 + 390))

        # We then have two days with the cash refunded from asset 0.
        expected_cash.extend([after_first_auto_close] * (390 + 390))
        expected_position_counts.extend([2] * (390 + 390))

        # We then have one day with cash refunded from asset 1.
        expected_cash.extend([after_second_auto_close] * 390)
        expected_position_counts.extend([1] * 390)

        # Check list lengths first to avoid expensive comparison
        self.assertEqual(</B></FONT>len(algo.cash), len(expected_cash))
        # TODO find more efficient way to compare these lists
        self.assertEqual(algo.cash, expected_cash)
        self.assertEqual(
            list(output['ending_cash']),
            [
                after_fills,
                after_fills,
                after_fills,
                after_first_auto_close,
                after_first_auto_close,
                after_second_auto_close,
                after_second_auto_close,
            ],
        )

        self.assertEqual(algo.num_positions, expected_position_counts)
        self.assertEqual(
            list(output['longs_count']),
            [3, 3, 3, 2, 2, 1, 1],
        )

        # Check expected transactions.
        # We should have a transaction of order_size shares per sid.
        transactions = output['transactions']

        # Note that the transactions appear on the first day rather than the
        # second in minute mode, because the fills happen on the second tick of
        # the backtest, which is still on the first day in minute mode.
        initial_fills = transactions.iloc[0]
        self.assertEqual(len(initial_fills), len(assets))
        for asset, txn in zip(assets, initial_fills):
            self.assertDictContainsSubset(
                {
                    'amount': order_size,
                    'commission': None,
                    'dt': backtest_minutes[1],
                    'price': initial_fill_prices[asset],
                    'sid': asset,
                },
                txn,
            )
            # This will be a UUID.
            self.assertIsInstance(txn['order_id'], str)

        def transactions_for_date(date):
            return transactions.iloc[self.test_days.get_loc(date)]

        # We should have exactly one auto-close transaction on the close date
        # of asset 0.
        (first_auto_close_transaction,) = transactions_for_date(
            assets[0].auto_close_date
        )
        self.assertEqual(
            first_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': algo.trading_calendar.session_close(
                    assets[0].auto_close_date,
                ),
                'price': fp0,
                'sid': assets[0],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )

        (second_auto_close_transaction,) = transactions_for_date(
            assets[1].auto_close_date
        )
        self.assertEqual(
            second_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': algo.trading_calendar.session_close(
                    assets[1].auto_close_date,
                ),
                'price': fp1,
                'sid': assets[1],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )


class TestOrderAfterDelist(zf.WithMakeAlgo, zf.ZiplineTestCase):
    start = pd.Timestamp('2016-01-05', tz='utc')
    day_1 = pd.Timestamp('2016-01-06', tz='utc')
    day_4 = pd.Timestamp('2016-01-11', tz='utc')
    end = pd.Timestamp('2016-01-15', tz='utc')

    # FIXME: Pass a benchmark source here.
    BENCHMARK_SID = None
<A NAME="29"></A>
    @classmethod
    def make_equity_info(cls):
        return pd<FONT color="#af7a82"><A HREF="javascript:ZweiFrames('match48-1.html#29',3,'match48-top.html#29',1)"><IMG SRC="forward.gif" ALT="other" BORDER="0" ALIGN="right"></A><B>.DataFrame.from_dict(
            {
                # Asset whose auto close date is after its end date.
                1: {
                    'start_date': cls.start,
                    'end_date': cls.day_1,
                    'auto_close_date': cls.day_4,
                    'symbol': &quot;ASSET1&quot;,
                    'exchange': &quot;TEST&quot;,
                },
                # Asset whose auto close date is before its end date.
                2: {
                    'start_date': cls.start,
                    'end_date': cls.day_4,
                    'auto_close_date': cls.</B></FONT>day_1,
                    'symbol': 'ASSET2',
                    'exchange': 'TEST',
                },
            },
            orient='index',
        )

    # XXX: This suite doesn't use the data in its DataPortal; it uses a
    # FakeDataPortal with different mock data.
    def init_instance_fixtures(self):
        super(TestOrderAfterDelist, self).init_instance_fixtures()
        self.data_portal = FakeDataPortal(self.asset_finder)

    @parameterized.expand([
        ('auto_close_after_end_date', 1),
        ('auto_close_before_end_date', 2),
    ])
    def test_order_in_quiet_period(self, name, sid):
        asset = self.asset_finder.retrieve_asset(sid)

        algo_code = dedent(&quot;&quot;&quot;
        from zipline.api import (
            sid,
            order,
            order_value,
            order_percent,
            order_target,
            order_target_percent,
            order_target_value
        )

        def initialize(context):
            pass

        def handle_data(context, data):
            order(sid({sid}), 1)
            order_value(sid({sid}), 100)
            order_percent(sid({sid}), 0.5)
            order_target(sid({sid}), 50)
            order_target_percent(sid({sid}), 0.5)
            order_target_value(sid({sid}), 50)
        &quot;&quot;&quot;).format(sid=sid)

        # run algo from 1/6 to 1/7
        algo = self.make_algo(
            script=algo_code,
            sim_params=SimulationParameters(
                start_session=pd.Timestamp(&quot;2016-01-06&quot;, tz='UTC'),
                end_session=pd.Timestamp(&quot;2016-01-07&quot;, tz='UTC'),
                trading_calendar=self.trading_calendar,
                data_frequency=&quot;minute&quot;
            )
        )
        with make_test_handler(self) as log_catcher:
            algo.run()

            warnings = [r for r in log_catcher.records
                        if r.level == logbook.WARNING]

            # one warning per order on the second day
            self.assertEqual(6 * 390, len(warnings))

            for w in warnings:
                expected_message = (
                    'Cannot place order for ASSET{sid}, as it has de-listed. '
                    'Any existing positions for this asset will be liquidated '
                    'on {date}.'.format(sid=sid, date=asset.auto_close_date)
                )
                self.assertEqual(expected_message, w.message)


class AlgoInputValidationTestCase(zf.WithMakeAlgo,
                                  zf.ZiplineTestCase):

    def test_reject_passing_both_api_methods_and_script(self):
        script = dedent(
            &quot;&quot;&quot;
            def initialize(context):
                pass

            def handle_data(context, data):
                pass

            def before_trading_start(context, data):
                pass

            def analyze(context, results):
                pass
            &quot;&quot;&quot;
        )
        for method in ('initialize',
                       'handle_data',
                       'before_trading_start',
                       'analyze'):

            with self.assertRaises(ValueError):
                self.make_algo(
                    script=script,
                    **{method: lambda *args, **kwargs: None}
                )
</PRE>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_quarters_estimates.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<HR>
<PRE>
<A NAME="1"></A><FONT color="#f63526"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#1',2,'match48-top.html#1',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>from __future__ import division

from datetime import timedelta
from functools import partial

import blaze as bz
import itertools
from nose.tools import assert_true
from nose_parameterized import parameterized
import numpy as np
from numpy.testing import assert_array_equal, assert_almost_equal
import pandas as pd
from toolz import merge

from zipline.pipeline import SimplePipelineEngine, Pipeline, CustomFactor
from zipline.pipeline.common import (
    EVENT_DATE_FIELD_NAME,
    FISCAL_QUARTER_FIELD_NAME,
    FISCAL_YEAR_FIELD_NAME,
    SID_FIELD_NAME,
    TS_FIELD_NAME,
)
from zipline.pipeline.data import DataSet
from zipline.pipeline.data import Column
from zipline.pipeline.domain import EquitySessionDomain
from zipline.pipeline.loaders.blaze.estimates import (
    BlazeNextEstimatesLoader,
    BlazeNextSplitAdjustedEstimatesLoader,
    BlazePreviousEstimatesLoader,
    BlazePreviousSplitAdjustedEstimatesLoader,
)
from zipline.pipeline.loaders.earnings_estimates import (
    INVALID_NUM_QTRS_MESSAGE,
    NextEarningsEstimatesLoader,
    NextSplitAdjustedEarningsEstimatesLoader,
    normalize_quarters,
    PreviousEarningsEstimatesLoader,
    PreviousSplitAdjustedEarningsEstimatesLoader,
    split_normalized_quarters,
)
from zipline.testing.fixtures import (
    WithAdjustmentReader,
    WithTradingSessions,
    ZiplineTestCase,
)
from zipline.testing.predicates import assert_equal, assert_raises_regex
from zipline.testing.predicates import assert_frame_equal
from zipline.utils.numpy_utils import datetime64ns_dtype
from</B></FONT> zipline.utils.numpy_utils import float64_dtype


class Estimates(DataSet):
    event_date = Column(dtype=datetime64ns_dtype)
    fiscal_quarter = Column(dtype=float64_dtype)
    fiscal_year = Column(dtype=float64_dtype)
    estimate = Column(dtype=float64_dtype)


class MultipleColumnsEstimates(DataSet):
    event_date = Column(dtype=datetime64ns_dtype)
    fiscal_quarter = Column(dtype=float64_dtype)
    fiscal_year = Column(dtype=float64_dtype)
    estimate1 = Column(dtype=float64_dtype)
    estimate2 = Column(dtype=float64_dtype)


def QuartersEstimates(announcements_out):
    class QtrEstimates(Estimates):
        num_announcements = announcements_out
        name = Estimates
    return QtrEstimates


def MultipleColumnsQuartersEstimates(announcements_out):
    class QtrEstimates(MultipleColumnsEstimates):
        num_announcements = announcements_out
        name = Estimates
    return QtrEstimates


def QuartersEstimatesNoNumQuartersAttr(num_qtr):
    class QtrEstimates(Estimates):
        name = Estimates
    return QtrEstimates


def create_expected_df_for_factor_compute(start_date,
                                          sids,
                                          tuples,
                                          end_date):
    &quot;&quot;&quot;
    Given a list of tuples of new data we get for each sid on each critical
    date (when information changes), create a DataFrame that fills that
    data through a date range ending at `end_date`.
    &quot;&quot;&quot;
    df = pd.DataFrame(tuples,
<A NAME="31"></A>                      columns=[SID_FIELD_NAME,
                               'estimate',
                               'knowledge_date'])
    df <FONT color="#3ea99f"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#31',2,'match48-top.html#31',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>= df.pivot_table(columns=SID_FIELD_NAME,
                        values='estimate',
                        index='knowledge_date')
    df = df.reindex(
        pd.date_range(start_date, end_date)
    )
    # Index name is lost during reindex.
    df.</B></FONT>index = df.index.rename('knowledge_date')
    df['at_date'] = end_date.tz_localize('utc')
    df = df.set_index(['at_date', df.index.tz_localize('utc')]).ffill()
    new_sids = set(sids) - set(df.columns)
    df = df.reindex(columns=df.columns.union(new_sids))
    return df


class WithEstimates(WithTradingSessions, WithAdjustmentReader):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing cls.loader and cls.events as class
    level fixtures.


    Methods
    -------
    make_loader(events, columns) -&gt; PipelineLoader
        Method which returns the loader to be used throughout tests.

        events : pd.DataFrame
            The raw events to be used as input to the pipeline loader.
        columns : dict[str -&gt; str]
            The dictionary mapping the names of BoundColumns to the
            associated column name in the events DataFrame.
    make_columns() -&gt; dict[BoundColumn -&gt; str]
       Method which returns a dictionary of BoundColumns mapped to the
       associated column names in the raw data.
    &quot;&quot;&quot;

    # Short window defined in order for test to run faster.
    START_DATE = pd.Timestamp('2014-12-28')
    END_DATE = pd.Timestamp('2015-02-04')

    @classmethod
    def make_loader(cls, events, columns):
        raise NotImplementedError('make_loader')

    @classmethod
    def make_events(cls):
        raise NotImplementedError('make_events')

    @classmethod
    def get_sids(cls):
        return cls.events[SID_FIELD_NAME].unique()

    @classmethod
    def make_columns(cls):
        return {
            Estimates.event_date: 'event_date',
            Estimates.fiscal_quarter: 'fiscal_quarter',
            Estimates.fiscal_year: 'fiscal_year',
            Estimates.estimate: 'estimate'
        }

    def make_engine(self, loader=None):
        if loader is None:
            loader = self.loader

        return SimplePipelineEngine(
            lambda x: loader,
            self.asset_finder,
            default_domain=EquitySessionDomain(
                self.trading_days, self.ASSET_FINDER_COUNTRY_CODE,
            ),
        )

    @classmethod
    def init_class_fixtures(cls):
        cls.events = cls.make_events()
        cls.ASSET_FINDER_EQUITY_SIDS = cls.get_sids()
        cls.ASSET_FINDER_EQUITY_SYMBOLS = [
            's' + str(n) for n in cls.ASSET_FINDER_EQUITY_SIDS
        ]
        # We need to instantiate certain constants needed by supers of
        # `WithEstimates` before we call their `init_class_fixtures`.
        super(WithEstimates, cls).init_class_fixtures()
        cls.columns = cls.make_columns()
        # Some tests require `WithAdjustmentReader` to be set up by the time we
        # make the loader.
        cls.loader = cls.make_loader(cls.events, {column.name: val for
                                                  column, val in
                                                  cls.columns.items()})


class WithOneDayPipeline(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing cls.events as a class level fixture and
    defining a test for all inheritors to use.

    Attributes
    ----------
    events : pd.DataFrame
        A simple DataFrame with columns needed for estimates and a single sid
        and no other data.

    Tests
    ------
    test_wrong_num_announcements_passed()
        Tests that loading with an incorrect quarter number raises an error.
    test_no_num_announcements_attr()
        Tests that the loader throws an AssertionError if the dataset being
        loaded has no `num_announcements` attribute.
    &quot;&quot;&quot;

    @classmethod
    def make_columns(cls):
        return {
            MultipleColumnsEstimates.event_date: 'event_date',
            MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',
            MultipleColumnsEstimates.fiscal_year: 'fiscal_year',
            MultipleColumnsEstimates.estimate1: 'estimate1',
            MultipleColumnsEstimates.estimate2: 'estimate2'
        }

    @classmethod
    def make_events(cls):
        return pd.DataFrame({
            SID_FIELD_NAME: [0] * 2,
            TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),
                            pd.Timestamp('2015-01-06')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),
                                    pd.Timestamp('2015-01-20')],
            'estimate1': [1., 2.],
            'estimate2': [3., 4.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: [2015, 2015]
        })

    @classmethod
    def make_expected_out(cls):
        raise NotImplementedError('make_expected_out')

    @classmethod
    def init_class_fixtures(cls):
        super(WithOneDayPipeline, cls).init_class_fixtures()
        cls.sid0 = cls.asset_finder.retrieve_asset(0)
        cls.expected_out = cls.make_expected_out()

    def test_load_one_day(self):
        # We want to test multiple columns
<A NAME="25"></A>        dataset = MultipleColumnsQuartersEstimates(1)
        engine = self.make_engine()
        results = engine.run_pipeline(
            Pipeline<FONT color="#5eac10"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#25',2,'match48-top.html#25',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>({c.name: c.latest for c in dataset.columns}),
            start_date=pd.Timestamp('2015-01-15', tz='utc'),
            end_date=pd.Timestamp('2015-01-15', tz=</B></FONT>'utc'),
        )
        assert_frame_equal(results, self.expected_out)


class PreviousWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):
    &quot;&quot;&quot;
    Tests that previous quarter loader correctly breaks if an incorrect
    number of quarters is passed.
    &quot;&quot;&quot;
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)

    @classmethod
    def make_expected_out(cls):
        return pd.DataFrame(
            {
                EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-10'),
                'estimate1': 1.,
                'estimate2': 3.,
                FISCAL_QUARTER_FIELD_NAME: 1.,
                FISCAL_YEAR_FIELD_NAME: 2015.,
            },
            index=pd.MultiIndex.from_tuples(
                ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)
            )
        )


class NextWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):
    &quot;&quot;&quot;
    Tests that next quarter loader correctly breaks if an incorrect
    number of quarters is passed.
    &quot;&quot;&quot;
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)

    @classmethod
    def make_expected_out(cls):
        return pd.DataFrame(
            {
                EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-20'),
                'estimate1': 2.,
                'estimate2': 4.,
                FISCAL_QUARTER_FIELD_NAME: 2.,
                FISCAL_YEAR_FIELD_NAME: 2015.,
            },
            index=pd.MultiIndex.from_tuples(
                ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)
            )
        )


dummy_df = pd.DataFrame({SID_FIELD_NAME: 0},
                        columns=[SID_FIELD_NAME,
                                 TS_FIELD_NAME,
                                 EVENT_DATE_FIELD_NAME,
                                 FISCAL_QUARTER_FIELD_NAME,
                                 FISCAL_YEAR_FIELD_NAME,
                                 'estimate'],
                        index=[0])


class WithWrongLoaderDefinition(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing cls.events as a class level fixture and
    defining a test for all inheritors to use.

    Attributes
    ----------
    events : pd.DataFrame
        A simple DataFrame with columns needed for estimates and a single sid
        and no other data.

    Tests
    ------
    test_wrong_num_announcements_passed()
        Tests that loading with an incorrect quarter number raises an error.
    test_no_num_announcements_attr()
        Tests that the loader throws an AssertionError if the dataset being
        loaded has no `num_announcements` attribute.
    &quot;&quot;&quot;

    @classmethod
    def make_events(cls):
        return dummy_df

    def test_wrong_num_announcements_passed(self):
        bad_dataset1 = QuartersEstimates(-1)
        bad_dataset2 = QuartersEstimates(-2)
        good_dataset = QuartersEstimates(1)
        engine = self.make_engine()
        columns = {c.name + str(dataset.num_announcements): c.latest
                   for dataset in (bad_dataset1,
                                   bad_dataset2,
                                   good_dataset)
                   for c in dataset.columns}
        p = Pipeline(columns)

        with self.assertRaises(ValueError) as e:
            engine.run_pipeline(
                p,
                start_date=self.trading_days[0],
                end_date=self.trading_days[-1],
            )
            assert_raises_regex(e, INVALID_NUM_QTRS_MESSAGE % &quot;-1,-2&quot;)

    def test_no_num_announcements_attr(self):
        dataset = QuartersEstimatesNoNumQuartersAttr(1)
        engine = self.make_engine()
        p = Pipeline({c.name: c.latest for c in dataset.columns})

        with self.assertRaises(AttributeError):
            engine.run_pipeline(
                p,
                start_date=self.trading_days[0],
                end_date=self.trading_days[-1],
            )


class PreviousWithWrongNumQuarters(WithWrongLoaderDefinition,
                                   ZiplineTestCase):
    &quot;&quot;&quot;
    Tests that previous quarter loader correctly breaks if an incorrect
    number of quarters is passed.
    &quot;&quot;&quot;
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)


class NextWithWrongNumQuarters(WithWrongLoaderDefinition,
                               ZiplineTestCase):
    &quot;&quot;&quot;
    Tests that next quarter loader correctly breaks if an incorrect
    number of quarters is passed.
    &quot;&quot;&quot;
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)


options = [&quot;split_adjustments_loader&quot;,
           &quot;split_adjusted_column_names&quot;,
           &quot;split_adjusted_asof&quot;]


class WrongSplitsLoaderDefinition(WithEstimates, ZiplineTestCase):
    &quot;&quot;&quot;
    Test class that tests that loaders break correctly when incorrectly
    instantiated.

    Tests
    -----
    test_extra_splits_columns_passed(SplitAdjustedEstimatesLoader)
        A test that checks that the loader correctly breaks when an
        unexpected column is passed in the list of split-adjusted columns.
    &quot;&quot;&quot;
    @classmethod
    def init_class_fixtures(cls):
        super(WithEstimates, cls).init_class_fixtures()

    @parameterized.expand(itertools.product(
        (NextSplitAdjustedEarningsEstimatesLoader,
         PreviousSplitAdjustedEarningsEstimatesLoader),
    ))
    def test_extra_splits_columns_passed(self, loader):
        columns = {
            Estimates.event_date: 'event_date',
            Estimates.fiscal_quarter: 'fiscal_quarter',
            Estimates.fiscal_year: 'fiscal_year',
            Estimates.estimate: 'estimate'
        }

        with self.assertRaises(ValueError):
            loader(dummy_df,
                   {column.name: val for column, val in
                    columns.items()},
                   split_adjustments_loader=self.adjustment_reader,
                   split_adjusted_column_names=[&quot;estimate&quot;, &quot;extra_col&quot;],
                   split_adjusted_asof=pd.Timestamp(&quot;2015-01-01&quot;))


class WithEstimatesTimeZero(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing cls.events as a class level fixture and
    defining a test for all inheritors to use.

    Attributes
    ----------
    cls.events : pd.DataFrame
        Generated dynamically in order to test inter-leavings of estimates and
        event dates for multiple quarters to make sure that we select the
        right immediate 'next' or 'previous' quarter relative to each date -
        i.e., the right 'time zero' on the timeline. We care about selecting
        the right 'time zero' because we use that to calculate which quarter's
        data needs to be returned for each day.

    Methods
    -------
    get_expected_estimate(q1_knowledge,
                          q2_knowledge,
                          comparable_date) -&gt; pd.DataFrame
        Retrieves the expected estimate given the latest knowledge about each
        quarter and the date on which the estimate is being requested. If
        there is no expected estimate, returns an empty DataFrame.

    Tests
    ------
    test_estimates()
        Tests that we get the right 'time zero' value on each day for each
        sid and for each column.
    &quot;&quot;&quot;
    # Shorter date range for performance
    END_DATE = pd.Timestamp('2015-01-28')

    q1_knowledge_dates = [pd.Timestamp('2015-01-01'),
                          pd.Timestamp('2015-01-04'),
                          pd.Timestamp('2015-01-07'),
                          pd.Timestamp('2015-01-11')]
    q2_knowledge_dates = [pd.Timestamp('2015-01-14'),
                          pd.Timestamp('2015-01-17'),
                          pd.Timestamp('2015-01-20'),
                          pd.Timestamp('2015-01-23')]
    # We want to model the possibility of an estimate predicting a release date
    # that doesn't match the actual release. This could be done by dynamically
    # generating more combinations with different release dates, but that
    # significantly increases the amount of time it takes to run the tests.
    # These hard-coded cases are sufficient to know that we can update our
    # beliefs when we get new information.
    q1_release_dates = [pd.Timestamp('2015-01-13'),
                        pd.Timestamp('2015-01-14')]  # One day late
    q2_release_dates = [pd.Timestamp('2015-01-25'),  # One day early
                        pd.Timestamp('2015-01-26')]

    @classmethod
    def make_events(cls):
        &quot;&quot;&quot;
        In order to determine which estimate we care about for a particular
        sid, we need to look at all estimates that we have for that sid and
        their associated event dates.

        We define q1 &lt; q2, and thus event1 &lt; event2 since event1 occurs
        during q1 and event2 occurs during q2 and we assume that there can
        only be 1 event per quarter. We assume that there can be multiple
        estimates per quarter leading up to the event. We assume that estimates
        will not surpass the relevant event date. We will look at 2 estimates
        for an event before the event occurs, since that is the simplest
        scenario that covers the interesting edge cases:
            - estimate values changing
            - a release date changing
            - estimates for different quarters interleaving

        Thus, we generate all possible inter-leavings of 2 estimates per
        quarter-event where estimate1 &lt; estimate2 and all estimates are &lt; the
        relevant event and assign each of these inter-leavings to a
        different sid.
        &quot;&quot;&quot;

        sid_estimates = []
        sid_releases = []
        # We want all permutations of 2 knowledge dates per quarter.
        it = enumerate(
            itertools.permutations(cls.q1_knowledge_dates +
                                   cls.q2_knowledge_dates,
                                   4)
        )
        for sid, (q1e1, q1e2, q2e1, q2e2) in it:
            # We're assuming that estimates must come before the relevant
            # release.
            if (q1e1 &lt; q1e2 and
                    q2e1 &lt; q2e2 and
                    # All estimates are &lt; Q2's event, so just constrain Q1
                    # estimates.
                    q1e1 &lt; cls.q1_release_dates[0] and
                    q1e2 &lt; cls.q1_release_dates[0]):
                sid_estimates.append(cls.create_estimates_df(q1e1,
                                                             q1e2,
                                                             q2e1,
                                                             q2e2,
                                                             sid))
                sid_releases.append(cls.create_releases_df(sid))
        return pd.concat(sid_estimates +
                         sid_releases).reset_index(drop=True)

    @classmethod
    def get_sids(cls):
        sids = cls.events[SID_FIELD_NAME].unique()
        # Tack on an extra sid to make sure that sids with no data are
        # included but have all-null columns.
        return list(sids) + [max(sids) + 1]

    @classmethod
    def create_releases_df(cls, sid):
        # Final release dates never change. The quarters have very tight date
        # ranges in order to reduce the number of dates we need to iterate
        # through when testing.
        return pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-13'),
                            pd.Timestamp('2015-01-26')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-13'),
                                    pd.Timestamp('2015-01-26')],
            'estimate': [0.5, 0.8],
            FISCAL_QUARTER_FIELD_NAME: [1.0, 2.0],
            FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0],
            SID_FIELD_NAME: sid
        })

    @classmethod
    def create_estimates_df(cls,
                            q1e1,
                            q1e2,
                            q2e1,
                            q2e2,
                            sid):
        return pd.DataFrame({
            EVENT_DATE_FIELD_NAME: cls.q1_release_dates + cls.q2_release_dates,
            'estimate': [.1, .2, .3, .4],
            FISCAL_QUARTER_FIELD_NAME: [1.0, 1.0, 2.0, 2.0],
            FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0, 2015.0, 2015.0],
            TS_FIELD_NAME: [q1e1, q1e2, q2e1, q2e2],
            SID_FIELD_NAME: sid,
        })

    def get_expected_estimate(self,
                              q1_knowledge,
                              q2_knowledge,
                              comparable_date):
        return pd.DataFrame()

    def test_estimates(self):
        dataset = QuartersEstimates(1)
        engine = self.make_engine()
        results = engine.run_pipeline(
            Pipeline({c.name: c.latest for c in dataset.columns}),
            start_date=self.trading_days[1],
            end_date=self.trading_days[-2],
        )
        for sid in self.ASSET_FINDER_EQUITY_SIDS:
            sid_estimates = results.xs(sid, level=1)
            # Separate assertion for all-null DataFrame to avoid setting
            # column dtypes on `all_expected`.
            if sid == max(self.ASSET_FINDER_EQUITY_SIDS):
                assert_true(sid_estimates.isnull().all().all())
            else:
                ts_sorted_estimates = self.events[
                    self.events[SID_FIELD_NAME] == sid
                ].sort_values(TS_FIELD_NAME)
                q1_knowledge = ts_sorted_estimates[
                    ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 1
                ]
                q2_knowledge = ts_sorted_estimates[
<A NAME="26"></A>                    ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 2
                ]
                all_expected = pd.concat(
                    [self<FONT color="#68818b"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#26',2,'match48-top.html#26',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.get_expected_estimate(
                        q1_knowledge[q1_knowledge[TS_FIELD_NAME] &lt;=
                                     date.tz_localize(None)],
                        q2_knowledge[q2_knowledge[TS_FIELD_NAME] &lt;=
                                     date.tz_localize(None)],
                        date.tz_localize(None),
                    ).</B></FONT>set_index([[date]]) for date in sid_estimates.index],
                    axis=0)
                assert_equal(all_expected[sid_estimates.columns],
                             sid_estimates)


class NextEstimate(WithEstimatesTimeZero, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)

    def get_expected_estimate(self,
                              q1_knowledge,
                              q2_knowledge,
                              comparable_date):
        # If our latest knowledge of q1 is that the release is
        # happening on this simulation date or later, then that's
        # the estimate we want to use.
        if (not q1_knowledge.empty and
            q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &gt;=
                comparable_date):
            return q1_knowledge.iloc[-1:]
        # If q1 has already happened or we don't know about it
        # yet and our latest knowledge indicates that q2 hasn't
        # happened yet, then that's the estimate we want to use.
        elif (not q2_knowledge.empty and
              q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &gt;=
                comparable_date):
            return q2_knowledge.iloc[-1:]
        return pd.DataFrame(columns=q1_knowledge.columns,
                            index=[comparable_date])


class BlazeNextEstimateLoaderTestCase(NextEstimate):
    &quot;&quot;&quot;
    Run the same tests as EventsLoaderTestCase, but using a BlazeEventsLoader.
    &quot;&quot;&quot;

    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(
            bz.data(events),
            columns,
        )


class PreviousEstimate(WithEstimatesTimeZero, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)

    def get_expected_estimate(self,
                              q1_knowledge,
                              q2_knowledge,
                              comparable_date):

        # The expected estimate will be for q2 if the last thing
        # we've seen is that the release date already happened.
        # Otherwise, it'll be for q1, as long as the release date
        # for q1 has already happened.
        if (not q2_knowledge.empty and
            q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &lt;=
                comparable_date):
            return q2_knowledge.iloc[-1:]
        elif (not q1_knowledge.empty and
              q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &lt;=
                comparable_date):
            return q1_knowledge.iloc[-1:]
        return pd.DataFrame(columns=q1_knowledge.columns,
                            index=[comparable_date])


class BlazePreviousEstimateLoaderTestCase(PreviousEstimate):
    &quot;&quot;&quot;
    Run the same tests as EventsLoaderTestCase, but using a BlazeEventsLoader.
    &quot;&quot;&quot;

    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(
            bz.data(events),
            columns,
        )


class WithEstimateMultipleQuarters(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing cls.events, cls.make_expected_out as
    class-level fixtures and self.test_multiple_qtrs_requested as a test.

    Attributes
    ----------
    events : pd.DataFrame
        Simple DataFrame with estimates for 2 quarters for a single sid.

    Methods
    -------
    make_expected_out() --&gt; pd.DataFrame
        Returns the DataFrame that is expected as a result of running a
        Pipeline where estimates are requested for multiple quarters out.
    fill_expected_out(expected)
        Fills the expected DataFrame with data.

    Tests
    ------
    test_multiple_qtrs_requested()
        Runs a Pipeline that calculate which estimates for multiple quarters
        out and checks that the returned columns contain data for the correct
        number of quarters out.
    &quot;&quot;&quot;

    @classmethod
    def make_events(cls):
        return pd.DataFrame({
            SID_FIELD_NAME: [0] * 2,
            TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),
                            pd.Timestamp('2015-01-06')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),
                                    pd.Timestamp('2015-01-20')],
            'estimate': [1., 2.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: [2015, 2015]
        })

    @classmethod
    def init_class_fixtures(cls):
<A NAME="17"></A>        super(WithEstimateMultipleQuarters, cls).init_class_fixtures()
        cls.expected_out = cls.make_expected_out()

    <FONT color="#3090c7"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#17',2,'match48-top.html#17',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>@classmethod
    def make_expected_out(cls):
        expected = pd.DataFrame(columns=[cls.columns[col] + '1'
                                         for col in cls.columns] +
                                        [cls.columns[col] + '2'
                                         for col in cls.columns],
                                index=cls.</B></FONT>trading_days)

        for (col, raw_name), suffix in itertools.product(
            cls.columns.items(), ('1', '2')
        ):
            expected_name = raw_name + suffix
            if col.dtype == datetime64ns_dtype:
                expected[expected_name] = pd.to_datetime(
                    expected[expected_name]
                )
            else:
                expected[expected_name] = expected[
                    expected_name
                ].astype(col.dtype)
        cls.fill_expected_out(expected)
        return expected.reindex(cls.trading_days)

    def test_multiple_qtrs_requested(self):
        dataset1 = QuartersEstimates(1)
        dataset2 = QuartersEstimates(2)
        engine = self.make_engine()

<A NAME="33"></A>        results = engine.run_pipeline(
            Pipeline(
                merge([{c.name + '1': c.latest for c in dataset1.columns},
                       {c.name + '2': c<FONT color="#736aff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#33',2,'match48-top.html#33',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.latest for c in dataset2.columns}])
            ),
            start_date=self.trading_days[0],
            end_date=self.trading_days[-1],
        )
        q1_columns = [col.name + '1' for col in self.columns]
        q2_columns =</B></FONT> [col.name + '2' for col in self.columns]

        # We now expect a column for 1 quarter out and a column for 2
        # quarters out for each of the dataset columns.
        assert_equal(sorted(np.array(q1_columns + q2_columns)),
                     sorted(results.columns.values))
        assert_equal(self.expected_out.sort_index(axis=1),
                     results.xs(0, level=1).sort_index(axis=1))


class NextEstimateMultipleQuarters(
    WithEstimateMultipleQuarters, ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)

    @classmethod
    def fill_expected_out(cls, expected):
        # Fill columns for 1 Q out
        for raw_name in cls.columns.values():
            expected.loc[
                pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-11'),
                raw_name + '1'
            ] = cls.events[raw_name].iloc[0]
            expected.loc[
                pd.Timestamp('2015-01-11'):pd.Timestamp('2015-01-20'),
                raw_name + '1'
            ] = cls.events[raw_name].iloc[1]

        # Fill columns for 2 Q out
        # We only have an estimate and event date for 2 quarters out before
        # Q1's event happens; after Q1's event, we know 1 Q out but not 2 Qs
        # out.
        for col_name in ['estimate', 'event_date']:
            expected.loc[
                pd.Timestamp('2015-01-06'):pd.Timestamp('2015-01-10'),
                col_name + '2'
            ] = cls.events[col_name].iloc[1]
        # But we know what FQ and FY we'd need in both Q1 and Q2
        # because we know which FQ is next and can calculate from there
        expected.loc[
            pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-09'),
            FISCAL_QUARTER_FIELD_NAME + '2'
        ] = 2
        expected.loc[
            pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20'),
            FISCAL_QUARTER_FIELD_NAME + '2'
        ] = 3
        expected.loc[
            pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-20'),
            FISCAL_YEAR_FIELD_NAME + '2'
        ] = 2015

        return expected


class BlazeNextEstimateMultipleQuarters(NextEstimateMultipleQuarters):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(
            bz.data(events),
            columns,
        )


class PreviousEstimateMultipleQuarters(
    WithEstimateMultipleQuarters,
    ZiplineTestCase
):

    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)

    @classmethod
    def fill_expected_out(cls, expected):
        # Fill columns for 1 Q out
        for raw_name in cls.columns.values():
            expected[raw_name + '1'].loc[
                pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-19')
            ] = cls.events[raw_name].iloc[0]
            expected[raw_name + '1'].loc[
                pd.Timestamp('2015-01-20'):
            ] = cls.events[raw_name].iloc[1]

        # Fill columns for 2 Q out
        for col_name in ['estimate', 'event_date']:
            expected[col_name + '2'].loc[
                pd.Timestamp('2015-01-20'):
            ] = cls.events[col_name].iloc[0]
        expected[
            FISCAL_QUARTER_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 4
        expected[
            FISCAL_YEAR_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 2014
        expected[
            FISCAL_QUARTER_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-20'):] = 1
        expected[
            FISCAL_YEAR_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-20'):] = 2015
        return expected


class BlazePreviousEstimateMultipleQuarters(PreviousEstimateMultipleQuarters):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(
            bz.data(events),
            columns,
        )


class WithVaryingNumEstimates(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing fixtures and a test to ensure that we
    have the correct overwrites when the event date changes. We want to make
    sure that if we have a quarter with an event date that gets pushed back,
    we don't start overwriting for the next quarter early. Likewise,
    if we have a quarter with an event date that gets pushed forward, we want
    to make sure that we start applying adjustments at the appropriate, earlier
    date, rather than the later date.

    Methods
    -------
    assert_compute()
        Defines how to determine that results computed for the `SomeFactor`
        factor are correct.

    Tests
    -----
    test_windows_with_varying_num_estimates()
        Tests that we create the correct overwrites from 2015-01-13 to
        2015-01-14 regardless of how event dates were updated for each
        quarter for each sid.
    &quot;&quot;&quot;

    @classmethod
    def make_events(cls):
<A NAME="27"></A>        return pd.DataFrame({
            SID_FIELD_NAME: [0] * 3 + [1] * 3,
            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                            pd<FONT color="#e77471"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#27',2,'match48-top.html#27',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.Timestamp('2015-01-12'),
                            pd.Timestamp('2015-01-13')] * 2,
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-12'),
                                    pd.Timestamp('2015-01-13'),
                                    pd.Timestamp('2015-01-20'),
                                    pd.Timestamp(</B></FONT>'2015-01-13'),
                                    pd.Timestamp('2015-01-12'),
                                    pd.Timestamp('2015-01-20')],
            'estimate': [11., 12., 21.] * 2,
            FISCAL_QUARTER_FIELD_NAME: [1, 1, 2] * 2,
            FISCAL_YEAR_FIELD_NAME: [2015] * 6
        })

    @classmethod
    def assert_compute(cls, estimate, today):
        raise NotImplementedError('assert_compute')

    def test_windows_with_varying_num_estimates(self):
        dataset = QuartersEstimates(1)
        assert_compute = self.assert_compute

        class SomeFactor(CustomFactor):
            inputs = [dataset.estimate]
            window_length = 3

            def compute(self, today, assets, out, estimate):
                assert_compute(estimate, today)

        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est': SomeFactor()}),
            start_date=pd.Timestamp('2015-01-13', tz='utc'),
            # last event date we have
            end_date=pd.Timestamp('2015-01-14', tz='utc'),
        )


class PreviousVaryingNumEstimates(
    WithVaryingNumEstimates,
    ZiplineTestCase
):
<A NAME="20"></A>    def assert_compute(self, estimate, today):
        if today == pd.Timestamp('2015-01-13', tz='utc'):
            assert_array_equal(estimate[:, 0],
                               np.array([np.NaN, np<FONT color="#4e9258"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#20',2,'match48-top.html#20',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.NaN, 12]))
            assert_array_equal(estimate[:, 1],
                               np.array([np.NaN, 12, 12]))
        else:
            assert_array_equal(estimate[:, 0],
                               np.array([np.NaN, 12, 12]))
            assert_array_equal(</B></FONT>estimate[:, 1],
                               np.array([12, 12, 12]))

    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)


class BlazePreviousVaryingNumEstimates(PreviousVaryingNumEstimates):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(
            bz.data(events),
            columns,
        )


class NextVaryingNumEstimates(
    WithVaryingNumEstimates,
    ZiplineTestCase
):

    def assert_compute(self, estimate, today):
        if today == pd.Timestamp('2015-01-13', tz='utc'):
            assert_array_equal(estimate[:, 0],
                               np.array([11, 12, 12]))
            assert_array_equal(estimate[:, 1],
                               np.array([np.NaN, np.NaN, 21]))
        else:
            assert_array_equal(estimate[:, 0],
                               np.array([np.NaN, 21, 21]))
            assert_array_equal(estimate[:, 1],
                               np.array([np.NaN, 21, 21]))

    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)


class BlazeNextVaryingNumEstimates(NextVaryingNumEstimates):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(
            bz.data(events),
            columns,
        )


class WithEstimateWindows(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing fixures and a test to test running a
    Pipeline with an estimates loader over differently-sized windows.

    Attributes
    ----------
    events : pd.DataFrame
        DataFrame with estimates for 2 quarters for 2 sids.
    window_test_start_date : pd.Timestamp
        The date from which the window should start.
    timelines : dict[int -&gt; pd.DataFrame]
        A dictionary mapping to the number of quarters out to
        snapshots of how the data should look on each date in the date range.

    Methods
    -------
    make_expected_timelines() -&gt; dict[int -&gt; pd.DataFrame]
        Creates a dictionary of expected data. See `timelines`, above.

    Tests
    -----
    test_estimate_windows_at_quarter_boundaries()
<A NAME="7"></A>        Tests that we overwrite values with the correct quarter's estimate at
        the correct dates when we have a factor that asks for a window of data.
    &quot;&quot;&quot;
    END_DATE <FONT color="#38a4a5"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#7',2,'match48-top.html#7',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>= pd.Timestamp('2015-02-10')
    window_test_start_date = pd.Timestamp('2015-01-05')
    critical_dates = [pd.Timestamp('2015-01-09', tz='utc'),
                      pd.Timestamp('2015-01-15', tz='utc'),
                      pd.Timestamp('2015-01-20', tz='utc'),
                      pd.</B></FONT>Timestamp('2015-01-26', tz='utc'),
                      pd.Timestamp('2015-02-05', tz='utc'),
                      pd.Timestamp('2015-02-10', tz='utc')]
    # Starting date, number of announcements out.
    window_test_cases = list(itertools.product(critical_dates, (1, 2)))

    @classmethod
    def make_events(cls):
        # Typical case: 2 consecutive quarters.
        sid_0_timeline = pd.DataFrame({
            TS_FIELD_NAME: [cls.window_test_start_date,
                            pd.Timestamp('2015-01-20'),
                            pd.Timestamp('2015-01-12'),
                            pd.Timestamp('2015-02-10'),
                            # We want a case where we get info for a later
                            # quarter before the current quarter is over but
                            # after the split_asof_date to make sure that
                            # we choose the correct date to overwrite until.
                            pd.Timestamp('2015-01-18')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-02-10'),
                 pd.Timestamp('2015-02-10'),
                 pd.Timestamp('2015-04-01')],
            'estimate': [100., 101.] + [200., 201.] + [400],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2 + [4],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 0,
        })

        # We want a case where we skip a quarter. We never find out about Q2.
        sid_10_timeline = pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-12'),
                            pd.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-15')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-22'), pd.Timestamp('2015-01-22'),
                 pd.Timestamp('2015-02-05'), pd.Timestamp('2015-02-05')],
            'estimate': [110., 111.] + [310., 311.],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [3] * 2,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 10
        })

        # We want to make sure we have correct overwrites when sid quarter
        # boundaries collide. This sid's quarter boundaries collide with sid 0.
        sid_20_timeline = pd.DataFrame({
            TS_FIELD_NAME: [cls.window_test_start_date,
                            pd.Timestamp('2015-01-07'),
                            cls.window_test_start_date,
                            pd.Timestamp('2015-01-17')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-02-10'),
                 pd.Timestamp('2015-02-10')],
            'estimate': [120., 121.] + [220., 221.],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 20
        })
        concatted = pd.concat([sid_0_timeline,
                               sid_10_timeline,
                               sid_20_timeline]).reset_index()
        np.random.seed(0)
        return concatted.reindex(np.random.permutation(concatted.index))

    @classmethod
    def get_sids(cls):
        sids = sorted(cls.events[SID_FIELD_NAME].unique())
        # Add extra sids between sids in our data. We want to test that we
        # apply adjustments to the correct sids.
        return [sid for i in range(len(sids) - 1)
                for sid in range(sids[i], sids[i+1])] + [sids[-1]]

    @classmethod
    def make_expected_timelines(cls):
        return {}

    @classmethod
    def init_class_fixtures(cls):
        super(WithEstimateWindows, cls).init_class_fixtures()
        cls.create_expected_df_for_factor_compute = partial(
            create_expected_df_for_factor_compute,
            cls.window_test_start_date,
            cls.get_sids()
        )
        cls.timelines = cls.make_expected_timelines()

    @parameterized.expand(window_test_cases)
    def test_estimate_windows_at_quarter_boundaries(self,
                                                    start_date,
                                                    num_announcements_out):
        dataset = QuartersEstimates(num_announcements_out)
        trading_days = self.trading_days
        timelines = self.timelines
        # The window length should be from the starting index back to the first
        # date on which we got data. The goal is to ensure that as we
        # progress through the timeline, all data we got, starting from that
        # first date, is correctly overwritten.
        window_len = (
            self.trading_days.get_loc(start_date) -
            self.trading_days.get_loc(self.window_test_start_date) + 1
        )

        class SomeFactor(CustomFactor):
            inputs = [dataset.estimate]
            window_length = window_len

            def compute(self, today, assets, out, estimate):
                today_idx = trading_days.get_loc(today)
                today_timeline = timelines[
                    num_announcements_out
                ].loc[today].reindex(
                    trading_days[:today_idx + 1]
                ).values
                timeline_start_idx = (len(today_timeline) - window_len)
                assert_almost_equal(estimate,
                                    today_timeline[timeline_start_idx:])

        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est': SomeFactor()}),
            start_date=start_date,
            # last event date we have
            end_date=pd.Timestamp('2015-02-10', tz='utc'),
        )


class PreviousEstimateWindows(WithEstimateWindows, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)

    @classmethod
<A NAME="29"></A>    def make_expected_timelines(cls):
        oneq_previous = pd.concat([
            pd.concat([
                <FONT color="#af7a82"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#29',2,'match48-top.html#29',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>cls.create_expected_df_for_factor_compute([
                    (0, np.NaN, cls.window_test_start_date),
                    (10, np.NaN, cls.window_test_start_date),
                    (20, np.</B></FONT>NaN, cls.window_test_start_date)
                ], end_date)
                for end_date in pd.date_range('2015-01-09', '2015-01-19')
            ]),
<A NAME="21"></A>            cls.create_expected_df_for_factor_compute(
                [(0, 101, pd.Timestamp('2015-01-20')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 121, pd.Timestamp<FONT color="#947010"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#21',2,'match48-top.html#21',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>('2015-01-20'))],
                pd.Timestamp('2015-01-20')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 101, pd.Timestamp('2015-01-20')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20</B></FONT>, 121, pd.Timestamp('2015-01-20'))],
                pd.Timestamp('2015-01-21')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp('2015-01-20')),
                     (10, 111, pd.Timestamp('2015-01-22')),
                     (20, 121, pd.Timestamp('2015-01-20'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-22', '2015-02-04')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp('2015-01-20')),
                     (10, 311, pd.Timestamp('2015-02-05')),
                     (20, 121, pd.Timestamp('2015-01-20'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-05', '2015-02-09')
                ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 201, pd.Timestamp('2015-02-10')),
                 (10, 311, pd.Timestamp('2015-02-05')),
                 (20, 221, pd.Timestamp('2015-02-10'))],
                pd.Timestamp('2015-02-10')
            ),
        ])

        twoq_previous = pd.concat(
<A NAME="4"></A>            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np<FONT color="#6cc417"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#4',2,'match48-top.html#4',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-09', '2015-02-09')] +
            # We never get estimates for S1 for 2Q ago because once Q3
            # becomes our previous quarter, 2Q ago would be Q2, and we have
            # no data on it.
            [cls.create_expected_df_for_factor_compute(
                [(0, 101, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, pd.Timestamp('2015-02-05')),
                 (20, 121, pd.Timestamp('2015-02-10'))],
                pd.</B></FONT>Timestamp('2015-02-10')
            )]
        )
        return {
            1: oneq_previous,
            2: twoq_previous
        }


class BlazePreviousEstimateWindows(PreviousEstimateWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(bz.data(events), columns)


class NextEstimateWindows(WithEstimateWindows, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)

    @classmethod
    def make_expected_timelines(cls):
        oneq_next = pd.concat([
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07'))],
                pd.Timestamp('2015-01-09')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 100, cls.window_test_start_date),
                     (10, 110, pd.Timestamp('2015-01-09')),
                     (10, 111, pd.Timestamp('2015-01-12')),
                     (20, 120, cls.window_test_start_date),
                     (20, 121, pd.Timestamp('2015-01-07'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-12', '2015-01-19')
            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (0, 101, pd.Timestamp('2015-01-20')),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07'))],
                pd.Timestamp('2015-01-20')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200, pd.Timestamp('2015-01-12')),
                     (10, 110, pd.Timestamp('2015-01-09')),
                     (10, 111, pd.Timestamp('2015-01-12')),
                     (20, 220, cls.window_test_start_date),
                     (20, 221, pd.Timestamp('2015-01-17'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-21', '2015-01-22')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200, pd.Timestamp('2015-01-12')),
                     (10, 310, pd.Timestamp('2015-01-09')),
                     (10, 311, pd.Timestamp('2015-01-15')),
                     (20, 220, cls.window_test_start_date),
                     (20, 221, pd.Timestamp('2015-01-17'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-23', '2015-02-05')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200, pd.Timestamp('2015-01-12')),
                     (10, np.NaN, cls.window_test_start_date),
                     (20, 220, cls.window_test_start_date),
                     (20, 221, pd.Timestamp('2015-01-17'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-06', '2015-02-09')
            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (0, 201, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.window_test_start_date),
                 (20, 221, pd.Timestamp('2015-01-17'))],
                pd.Timestamp('2015-02-10')
            )
        ])

        twoq_next = pd.concat(
            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
<A NAME="13"></A>                 (20, 220, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-09', '2015-01-11')] +
            [<FONT color="#3b9c9c"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#13',2,'match48-top.html#13',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>cls.create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-12', '2015-01-16')] +
            [cls.</B></FONT>create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.window_test_start_date),
                 (20, 221, pd.Timestamp('2015-01-17'))],
                pd.Timestamp('2015-01-20')
            )] +
            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]
        )

        return {
            1: oneq_next,
            2: twoq_next
        }


class BlazeNextEstimateWindows(NextEstimateWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(bz.data(events), columns)


class WithSplitAdjustedWindows(WithEstimateWindows):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing fixures and a test to test running a
    Pipeline with an estimates loader over differently-sized windows and with
    split adjustments.
    &quot;&quot;&quot;

    split_adjusted_asof_date = pd.Timestamp('2015-01-14')

    @classmethod
    def make_events(cls):
        # Add an extra sid that has a release before the split-asof-date in
        # order to test that we're reversing splits correctly in the previous
        # case (without an overwrite) and in the next case (with an overwrite).
        sid_30 = pd.DataFrame({
            TS_FIELD_NAME: [cls.window_test_start_date,
                            pd.Timestamp('2015-01-09'),
                            # For Q2, we want it to start early enough
                            # that we can have several adjustments before
                            # the end of the first quarter so that we
                            # can test un-adjusting &amp; readjusting with an
                            # overwrite.
                            cls.window_test_start_date,
                            # We want the Q2 event date to be enough past
                            # the split-asof-date that we can have
                            # several splits and can make sure that they
                            # are applied correctly.
                            pd.Timestamp('2015-01-20')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-09'),
                 pd.Timestamp('2015-01-09'),
                 pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-01-20')],
            'estimate': [130., 131., 230., 231.],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 30
        })

        # An extra sid to test no splits before the split-adjusted-asof-date.
        # We want an event before and after the split-adjusted-asof-date &amp;
        # timestamps for data points also before and after
<A NAME="32"></A>        # split-adjsuted-asof-date (but also before the split dates, so that
        # we can test that splits actually get applied at the correct times).
        sid_40 = pd.DataFrame({
            TS_FIELD_NAME: [pd<FONT color="#5b8daf"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#32',2,'match48-top.html#32',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-15')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                                    pd.Timestamp('2015-02-10')],
            'estimate': [140., 240.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 40
        })

        # An extra sid to test all splits before the
        # split-adjusted-asof-date. All timestamps should be before that date
        # so that we have cases where we un-apply and re-apply splits.
        sid_50 =</B></FONT> pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-12')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                                    pd.Timestamp('2015-02-10')],
            'estimate': [150., 250.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 50
        })

        return pd.concat([
            # Slightly hacky, but want to make sure we're using the same
            # events as WithEstimateWindows.
            cls.__base__.make_events(),
            sid_30,
            sid_40,
            sid_50,
        ])

    @classmethod
    def make_splits_data(cls):
        # For sid 0, we want to apply a series of splits before and after the
        #  split-adjusted-asof-date we well as between quarters (for the
        # previous case, where we won't see any values until after the event
        # happens).
<A NAME="16"></A>        sid_0_splits = pd.DataFrame({
            SID_FIELD_NAME: 0,
            'ratio': (-1., 2., 3., 4., 5., 6., 7., 100),
            'effective_date': (pd<FONT color="#2981b2"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#16',2,'match48-top.html#16',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.Timestamp('2014-01-01'),  # Filter out
                               # Split before Q1 event &amp; after first estimate
                               pd.Timestamp('2015-01-07'),
                               # Split before Q1 event
                               pd.Timestamp('2015-01-09'),
                               # Split before Q1 event
                               pd.Timestamp('2015-01-13'),
                               # Split before Q1 event
                               pd.Timestamp('2015-01-15'),
                               # Split before Q1 event
                               pd.Timestamp('2015-01-18'),
                               # Split after Q1 event and before Q2 event
                               pd.Timestamp(</B></FONT>'2015-01-30'),
                               # Filter out - this is after our date index
                               pd.Timestamp('2016-01-01'))
        })

        sid_10_splits = pd.DataFrame({
            SID_FIELD_NAME: 10,
            'ratio': (.2, .3),
            'effective_date': (
                # We want a split before the first estimate and before the
                # split-adjusted-asof-date but within our calendar index so
                # that we can test that the split is NEVER applied.
                pd.Timestamp('2015-01-07'),
                # Apply a single split before Q1 event.
                pd.Timestamp('2015-01-20')),
        })

        # We want a sid with split dates that collide with another sid (0) to
        # make sure splits are correctly applied for both sids.
        sid_20_splits = pd.DataFrame({
            SID_FIELD_NAME: 20,
            'ratio': (.4, .5, .6, .7, .8, .9,),
            'effective_date': (
                pd.Timestamp('2015-01-07'),
                pd.Timestamp('2015-01-09'),
                pd.Timestamp('2015-01-13'),
                pd.Timestamp('2015-01-15'),
                pd.Timestamp('2015-01-18'),
                pd.Timestamp('2015-01-30')),
        })

        # This sid has event dates that are shifted back so that we can test
        # cases where an event occurs before the split-asof-date.
        sid_30_splits = pd.DataFrame({
            SID_FIELD_NAME: 30,
            'ratio': (8, 9, 10, 11, 12),
            'effective_date': (
                # Split before the event and before the
                # split-asof-date.
                pd.Timestamp('2015-01-07'),
                # Split on date of event but before the
                # split-asof-date.
                pd.Timestamp('2015-01-09'),
                # Split after the event, but before the
                # split-asof-date.
                pd.Timestamp('2015-01-13'),
                pd.Timestamp('2015-01-15'),
                pd.Timestamp('2015-01-18')),
        })

        # No splits for a sid before the split-adjusted-asof-date.
        sid_40_splits = pd.DataFrame({
            SID_FIELD_NAME: 40,
            'ratio': (13, 14),
            'effective_date': (
                pd.Timestamp('2015-01-20'),
                pd.Timestamp('2015-01-22')
            )
        })

        # No splits for a sid after the split-adjusted-asof-date.
        sid_50_splits = pd.DataFrame({
            SID_FIELD_NAME: 50,
            'ratio': (15, 16),
            'effective_date': (
                pd.Timestamp('2015-01-13'),
                pd.Timestamp('2015-01-14')
            )
        })

        return pd.concat([
            sid_0_splits,
            sid_10_splits,
            sid_20_splits,
            sid_30_splits,
            sid_40_splits,
            sid_50_splits,
        ])


class PreviousWithSplitAdjustedWindows(WithSplitAdjustedWindows,
                                       ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )

    @classmethod
    def make_expected_timelines(cls):
<A NAME="35"></A>        oneq_previous = pd.concat([
            pd.concat([
                cls.create_expected_df_for_factor_compute([
                    <FONT color="#41a317"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#35',2,'match48-top.html#35',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>(0, np.NaN, cls.window_test_start_date),
                    (10, np.NaN, cls.window_test_start_date),
                    (20, np.NaN, cls.window_test_start_date),
                    # Undo all adjustments that haven't happened yet.
                    (30, 131*1/10, pd.</B></FONT>Timestamp('2015-01-09')),
                    (40, 140., pd.Timestamp('2015-01-09')),
                    (50, 150 * 1 / 15 * 1 / 16, pd.Timestamp('2015-01-09')),
                ], end_date)
<A NAME="34"></A>                for end_date in pd.date_range('2015-01-09', '2015-01-12')
            ]),
            cls.create_expected_df_for_factor_compute([
                <FONT color="#827d6b"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#34',2,'match48-top.html#34',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>(0, np.NaN, cls.window_test_start_date),
                (10, np.NaN, cls.window_test_start_date),
                (20, np.NaN, cls.window_test_start_date),
                (30, 131, pd.</B></FONT>Timestamp('2015-01-09')),
                (40, 140., pd.Timestamp('2015-01-09')),
                (50, 150. * 1 / 16, pd.Timestamp('2015-01-09')),
            ], pd.Timestamp('2015-01-13')),
            cls.create_expected_df_for_factor_compute([
                (0, np.NaN, cls.window_test_start_date),
                (10, np.NaN, cls.window_test_start_date),
                (20, np.NaN, cls.window_test_start_date),
                (30, 131, pd.Timestamp('2015-01-09')),
                (40, 140., pd.Timestamp('2015-01-09')),
                (50, 150., pd.Timestamp('2015-01-09'))
            ], pd.Timestamp('2015-01-14')),
            pd.concat([
                cls.create_expected_df_for_factor_compute([
                    (0, np.NaN, cls.window_test_start_date),
                    (10, np.NaN, cls.window_test_start_date),
                    (20, np.NaN, cls.window_test_start_date),
                    (30, 131*11, pd.Timestamp('2015-01-09')),
                    (40, 140., pd.Timestamp('2015-01-09')),
                    (50, 150., pd.Timestamp('2015-01-09')),
                ], end_date)
                for end_date in pd.date_range('2015-01-15', '2015-01-16')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp('2015-01-20')),
                     (10, np.NaN, cls.window_test_start_date),
                     (20, 121*.7*.8, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13, pd.Timestamp('2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-20', '2015-01-21')
<A NAME="28"></A>            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp<FONT color="#717d7d"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#28',2,'match48-top.html#28',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>('2015-01-20')),
                     (10, 111*.3, pd.Timestamp('2015-01-22')),
                     (20, 121*.7*.8, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13*14, pd.Timestamp(</B></FONT>'2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-22', '2015-01-29')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101*7, pd.Timestamp('2015-01-20')),
                     (10, 111*.3, pd.Timestamp('2015-01-22')),
                     (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-30', '2015-02-04')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101*7, pd.Timestamp('2015-01-20')),
                     (10, 311*.3, pd.Timestamp('2015-02-05')),
                     (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-05', '2015-02-09')
                ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 201, pd.Timestamp('2015-02-10')),
                 (10, 311*.3, pd.Timestamp('2015-02-05')),
                 (20, 221*.8*.9, pd.Timestamp('2015-02-10')),
                 (30, 231, pd.Timestamp('2015-01-20')),
                 (40, 240.*13*14, pd.Timestamp('2015-02-10')),
                 (50, 250., pd.Timestamp('2015-02-10'))],
                pd.Timestamp('2015-02-10')
            ),
<A NAME="10"></A>        ])

        twoq_previous = pd.concat(
            [<FONT color="#ad5910"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#10',2,'match48-top.html#10',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.</B></FONT>date_range('2015-01-09', '2015-01-19')] +
            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date),
                 (30, 131*11*12, pd.Timestamp('2015-01-20'))],
                end_date
            ) for end_date in pd.date_range('2015-01-20', '2015-02-09')] +
            # We never get estimates for S1 for 2Q ago because once Q3
            # becomes our previous quarter, 2Q ago would be Q2, and we have
            # no data on it.
            [cls.create_expected_df_for_factor_compute(
                [(0, 101*7, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, pd.Timestamp('2015-02-05')),
                 (20, 121*.7*.8*.9, pd.Timestamp('2015-02-10')),
                 (30, 131*11*12, pd.Timestamp('2015-01-20')),
                 (40, 140. * 13 * 14, pd.Timestamp('2015-02-10')),
                 (50, 150., pd.Timestamp('2015-02-10'))],
                pd.Timestamp('2015-02-10')
            )]
        )
        return {
            1: oneq_previous,
            2: twoq_previous
        }


class BlazePreviousWithSplitAdjustedWindows(PreviousWithSplitAdjustedWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )


class NextWithSplitAdjustedWindows(WithSplitAdjustedWindows, ZiplineTestCase):

    @classmethod
    def make_loader(cls, events, columns):
        return NextSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )

    @classmethod
<A NAME="8"></A>    def make_expected_timelines(cls):
        oneq_next = pd.concat([
            cls.create_expected_df_for_factor_compute(
                [(<FONT color="#c58917"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#8',2,'match48-top.html#8',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>0, 100*1/4, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (20, 120*5/3, cls.window_test_start_date),
                 (20, 121*5/3, pd.Timestamp('2015-01-07')),
                 (30, 130*1/10, cls.window_test_start_date),
                 (30, 131*1/10, pd.Timestamp('2015-01-09')),
                 (40, 140, pd.</B></FONT>Timestamp('2015-01-09')),
                 (50, 150.*1/15*1/16, pd.Timestamp('2015-01-09'))],
                pd.Timestamp('2015-01-09')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 100*1/4, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120*5/3, cls.window_test_start_date),
                 (20, 121*5/3, pd.Timestamp('2015-01-07')),
                 (30, 230*1/10, cls.window_test_start_date),
                 (40, np.NaN, pd.Timestamp('2015-01-10')),
                 (50, 250.*1/15*1/16, pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-12')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07')),
                 (30, 230, cls.window_test_start_date),
                 (40, np.NaN, pd.Timestamp('2015-01-10')),
                 (50, 250.*1/16, pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-13')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07')),
                 (30, 230, cls.window_test_start_date),
                 (40, np.NaN, pd.Timestamp('2015-01-10')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-14')
            ),
            pd.concat([
<A NAME="18"></A>                cls.create_expected_df_for_factor_compute(
                    [(0, 100*5, cls.window_test_start_date),
                     (10, 110, pd.Timestamp('2015-01-09')),
                     (10, 111, pd<FONT color="#800517"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#18',2,'match48-top.html#18',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.Timestamp('2015-01-12')),
                     (20, 120*.7, cls.window_test_start_date),
                     (20, 121*.7, pd.Timestamp('2015-01-07')),
                     (30, 230*11, cls.window_test_start_date),
                     (40, 240, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.</B></FONT>Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-15', '2015-01-16')
<A NAME="5"></A>            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 100*5*6, cls.window_test_start_date),
                 (<FONT color="#151b8d"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#5',2,'match48-top.html#5',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>0, 101, pd.Timestamp('2015-01-20')),
                 (10, 110*.3, pd.Timestamp('2015-01-09')),
                 (10, 111*.3, pd.Timestamp('2015-01-12')),
                 (20, 120*.7*.8, cls.window_test_start_date),
                 (20, 121*.7*.8, pd.Timestamp('2015-01-07')),
                 (30, 230*11*12, cls.window_test_start_date),
                 (30, 231, pd.</B></FONT>Timestamp('2015-01-20')),
                 (40, 240*13, pd.Timestamp('2015-01-15')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
<A NAME="11"></A>                pd.Timestamp('2015-01-20')
            ),
            cls.create_expected_df_for_factor_compute(
                [(<FONT color="#b041ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#11',2,'match48-top.html#11',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),
                 (10, 110 * .3, pd.Timestamp('2015-01-09')),
                 (10, 111 * .3, pd.Timestamp('2015-01-12')),
                 (20, 220 * .7 * .8, cls.window_test_start_date),
                 (20, 221 * .8, pd.Timestamp('2015-01-17')),
                 (40, 240 * 13, pd.</B></FONT>Timestamp('2015-01-15')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-21')
<A NAME="15"></A>            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),
                 (10, 110 * .3, pd<FONT color="#f52887"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#15',2,'match48-top.html#15',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.Timestamp('2015-01-09')),
                 (10, 111 * .3, pd.Timestamp('2015-01-12')),
                 (20, 220 * .7 * .8, cls.window_test_start_date),
                 (20, 221 * .8, pd.Timestamp('2015-01-17')),
                 (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                 (50, 250., pd.</B></FONT>Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-22')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200*5*6, pd.Timestamp('2015-01-12')),
                     (10, 310*.3, pd.Timestamp('2015-01-09')),
                     (10, 311*.3, pd.Timestamp('2015-01-15')),
                     (20, 220*.7*.8, cls.window_test_start_date),
                     (20, 221*.8, pd.Timestamp('2015-01-17')),
                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-23', '2015-01-29')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
                     (10, 310*.3, pd.Timestamp('2015-01-09')),
                     (10, 311*.3, pd.Timestamp('2015-01-15')),
                     (20, 220*.7*.8*.9, cls.window_test_start_date),
                     (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-30', '2015-02-05')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
                     (10, np.NaN, cls.window_test_start_date),
                     (20, 220*.7*.8*.9, cls.window_test_start_date),
                     (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-06', '2015-02-09')
            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
                 (0, 201, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*.7*.8*.9, cls.window_test_start_date),
                 (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
                 (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-02-10')
            )
        ])
<A NAME="6"></A>
        twoq_next = pd.concat(
            [cls.create_expected_df_for_factor_compute(
                [<FONT color="#8c8774"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#6',2,'match48-top.html#6',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*5/3, cls.window_test_start_date),
                 (30, 230*1/10, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date),
                 (50, np.NaN, cls.window_test_start_date)],
                pd.</B></FONT>Timestamp('2015-01-09')
            )] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 200*1/4, pd.Timestamp('2015-01-12')),
<A NAME="12"></A>                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*5/3, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np<FONT color="#571b7e"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#12',2,'match48-top.html#12',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.NaN, cls.window_test_start_date)],
                pd.Timestamp('2015-01-12')
            )] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.</B></FONT>window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-13', '2015-01-14')] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 200*5, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*.7, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date)],
<A NAME="3"></A>                end_date
            ) for end_date in pd.date_range('2015-01-15', '2015-01-16')] +
            [cls.create_expected_df_for_factor_compute(
                [<FONT color="#53858b"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#3',2,'match48-top.html#3',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>(0, 200*5*6, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*.7*.8, cls.window_test_start_date),
                 (20, 221*.8, pd.Timestamp('2015-01-17')),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date)],
<A NAME="14"></A>                pd.Timestamp('2015-01-20')
            )] +
            [cls</B></FONT>.create_expected_df_for_factor_compute(
                [<FONT color="#842dce"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#14',2,'match48-top.html#14',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.</B></FONT>NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]
        )

        return {
            1: oneq_next,
            2: twoq_next
        }


class BlazeNextWithSplitAdjustedWindows(NextWithSplitAdjustedWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )


class WithSplitAdjustedMultipleEstimateColumns(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin for having multiple estimate columns that are
    split-adjusted to make sure that adjustments are applied correctly.

    Attributes
    ----------
    test_start_date : pd.Timestamp
        The start date of the test.
    test_end_date : pd.Timestamp
        The start date of the test.
    split_adjusted_asof : pd.Timestamp
        The split-adjusted-asof-date of the data used in the test, to be used
        to create all loaders of test classes that subclass this mixin.

    Methods
    -------
    make_expected_timelines_1q_out -&gt; dict[pd.Timestamp -&gt; dict[str -&gt;
        np.array]]
        The expected array of results for each date of the date range for
        each column. Only for 1 quarter out.

    make_expected_timelines_2q_out -&gt; dict[pd.Timestamp -&gt; dict[str -&gt;
        np.array]]
        The expected array of results for each date of the date range. For 2
        quarters out, so only for the column that is requested to be loaded
        with 2 quarters out.

    Tests
    -----
    test_adjustments_with_multiple_adjusted_columns
        Tests that if you have multiple columns, we still split-adjust
        correctly.

    test_multiple_datasets_different_num_announcements
        Tests that if you have multiple datasets that ask for a different
<A NAME="19"></A>        number of quarters out, and each asks for a different estimates column,
        we still split-adjust correctly.
    &quot;&quot;&quot;
    END_DATE <FONT color="#f62817"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#19',2,'match48-top.html#19',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>= pd.Timestamp('2015-02-10')
    test_start_date = pd.Timestamp('2015-01-06', tz='utc')
    test_end_date = pd.Timestamp('2015-01-12', tz='utc')
    split_adjusted_asof = pd.Timestamp(</B></FONT>'2015-01-08')

    @classmethod
    def make_columns(cls):
        return {
            MultipleColumnsEstimates.event_date: 'event_date',
            MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',
            MultipleColumnsEstimates.fiscal_year: 'fiscal_year',
            MultipleColumnsEstimates.estimate1: 'estimate1',
            MultipleColumnsEstimates.estimate2: 'estimate2'
        }

    @classmethod
    def make_events(cls):
        sid_0_events = pd.DataFrame({
            # We only want a stale KD here so that adjustments
            # will be applied.
            TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),
                            pd.Timestamp('2015-01-05')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-09'),
                 pd.Timestamp('2015-01-12')],
            'estimate1': [1100., 1200.],
            'estimate2': [2100., 2200.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 0,
        })

        # This is just an extra sid to make sure that we apply adjustments
        # correctly for multiple columns when we have multiple sids.
        sid_1_events = pd.DataFrame({
            # We only want a stale KD here so that adjustments
            # will be applied.
            TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),
                            pd.Timestamp('2015-01-05')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-08'),
                 pd.Timestamp('2015-01-11')],
            'estimate1': [1110., 1210.],
            'estimate2': [2110., 2210.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 1,
        })
        return pd.concat([sid_0_events, sid_1_events])

    @classmethod
    def make_splits_data(cls):
        sid_0_splits = pd.DataFrame({
            SID_FIELD_NAME: 0,
            'ratio': (.3, 3.),
            'effective_date': (pd.Timestamp('2015-01-07'),
                               pd.Timestamp('2015-01-09')),
        })

        sid_1_splits = pd.DataFrame({
            SID_FIELD_NAME: 1,
            'ratio': (.4, 4.),
            'effective_date': (pd.Timestamp('2015-01-07'),
                               pd.Timestamp('2015-01-09')),
        })

        return pd.concat([sid_0_splits, sid_1_splits])

    @classmethod
    def make_expected_timelines_1q_out(cls):
        return {}

    @classmethod
    def make_expected_timelines_2q_out(cls):
        return {}

    @classmethod
    def init_class_fixtures(cls):
        super(
            WithSplitAdjustedMultipleEstimateColumns, cls
        ).init_class_fixtures()
        cls.timelines_1q_out = cls.make_expected_timelines_1q_out()
        cls.timelines_2q_out = cls.make_expected_timelines_2q_out()

    def test_adjustments_with_multiple_adjusted_columns(self):
        dataset = MultipleColumnsQuartersEstimates(1)
        timelines = self.timelines_1q_out
        window_len = 3

        class SomeFactor(CustomFactor):
            inputs = [dataset.estimate1, dataset.estimate2]
            window_length = window_len

            def compute(self, today, assets, out, estimate1, estimate2):
                assert_almost_equal(estimate1, timelines[today]['estimate1'])
                assert_almost_equal(estimate2, timelines[today]['estimate2'])

        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est': SomeFactor()}),
            start_date=self.test_start_date,
            # last event date we have
            end_date=self.test_end_date,
        )

    def test_multiple_datasets_different_num_announcements(self):
        dataset1 = MultipleColumnsQuartersEstimates(1)
        dataset2 = MultipleColumnsQuartersEstimates(2)
        timelines_1q_out = self.timelines_1q_out
        timelines_2q_out = self.timelines_2q_out
        window_len = 3

        class SomeFactor1(CustomFactor):
            inputs = [dataset1.estimate1]
            window_length = window_len

            def compute(self, today, assets, out, estimate1):
                assert_almost_equal(
                    estimate1, timelines_1q_out[today]['estimate1']
                )

        class SomeFactor2(CustomFactor):
            inputs = [dataset2.estimate2]
            window_length = window_len

            def compute(self, today, assets, out, estimate2):
                assert_almost_equal(
                    estimate2, timelines_2q_out[today]['estimate2']
                )

        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est1': SomeFactor1(), 'est2': SomeFactor2()}),
            start_date=self.test_start_date,
            # last event date we have
            end_date=self.test_end_date,
        )


class PreviousWithSplitAdjustedMultipleEstimateColumns(
    WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )

    @classmethod
    def make_expected_timelines_1q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] * 3),
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-07', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] * 3),
<A NAME="37"></A>                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate1': np.array([[np<FONT color="#810541"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#37',2,'match48-top.html#37',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.NaN, np.NaN]] * 2 +
                                      [[np.NaN, 1110.]]),
                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[</B></FONT>np.NaN, 2110.]])
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] +
                                      [[np.NaN, 1110. * 4]] +
                                      [[1100 * 3., 1110. * 4]]),
                'estimate2': np.array([[np.NaN, np.NaN]] +
                                      [[np.NaN, 2110. * 4]] +
                                      [[2100 * 3., 2110. * 4]])
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[1200 * 3., 1210. * 4]]),
                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[2200 * 3., 2210. * 4]])
            }
        }

    @classmethod
    def make_expected_timelines_2q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-07', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[2100 * 3., 2110. * 4]])
            }
        }


class BlazePreviousWithMultipleEstimateColumns(
    PreviousWithSplitAdjustedMultipleEstimateColumns
):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )


class NextWithSplitAdjustedMultipleEstimateColumns(
    WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return NextSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )

    @classmethod
<A NAME="9"></A>    def make_expected_timelines_1q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate1': np.array<FONT color="#83a33a"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#9',2,'match48-top.html#9',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>([[np.NaN, np.NaN]] +
                                      [[1100. * 1/.3, 1110. * 1/.4]] * 2),
                'estimate2': np.array([[np.NaN, np.NaN]] +
                                      [[2100. * 1/.3, 2110. * 1/.4]] * 2),
            },
            pd.Timestamp(</B></FONT>'2015-01-07', tz='utc'): {
                'estimate1': np.array([[1100., 1110.]] * 3),
                'estimate2': np.array([[2100., 2110.]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate1': np.array([[1100., 1110.]] * 3),
                'estimate2': np.array([[2100., 2110.]] * 3)
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate1': np.array([[1100 * 3., 1210. * 4]] * 3),
<A NAME="36"></A>                'estimate2': np.array([[2100 * 3., 2210. * 4]] * 3)
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                <FONT color="#ff00ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#36',2,'match48-top.html#36',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>'estimate1': np.array([[1200 * 3., np.NaN]] * 3),
                'estimate2': np.array([[2200 * 3., np.</B></FONT>NaN]] * 3)
            }
        }

    @classmethod
    def make_expected_timelines_2q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] +
                                      [[2200 * 1/.3, 2210. * 1/.4]] * 2)
            },
            pd.Timestamp('2015-01-07', tz='utc'): {
                'estimate2': np.array([[2200., 2210.]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate2': np.array([[2200, 2210.]] * 3)
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate2': np.array([[2200 * 3., np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            }
        }


class BlazeNextWithMultipleEstimateColumns(
    NextWithSplitAdjustedMultipleEstimateColumns
):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )


class WithAdjustmentBoundaries(WithEstimates):
    &quot;&quot;&quot;
    ZiplineTestCase mixin providing class-level attributes, methods,
    and a test to make sure that when the split-adjusted-asof-date is not
    strictly within the date index, we can still apply adjustments correctly.

    Attributes
    ----------
    split_adjusted_before_start : pd.Timestamp
        A split-adjusted-asof-date before the start date of the test.
    split_adjusted_after_end : pd.Timestamp
        A split-adjusted-asof-date before the end date of the test.
    split_adjusted_asof_dates : list of tuples of pd.Timestamp
        All the split-adjusted-asof-dates over which we want to parameterize
        the test.

    Methods
    -------
    make_expected_out -&gt; dict[pd.Timestamp -&gt; pd.DataFrame]
        A dictionary of the expected output of the pipeline at each of the
        dates of interest.
    &quot;&quot;&quot;
    START_DATE = pd.Timestamp('2015-01-04')
    # We want to run the pipeline starting from `START_DATE`, but the
    # pipeline results will start from the next day, which is
    # `test_start_date`.
    test_start_date = pd.Timestamp('2015-01-05')
    END_DATE = test_end_date = pd.Timestamp('2015-01-12')
    split_adjusted_before_start = (
        test_start_date - timedelta(days=1)
    )
    split_adjusted_after_end = (
        test_end_date + timedelta(days=1)
    )
    # Must parametrize over this because there can only be 1 such date for
    # each set of data.
    split_adjusted_asof_dates = [(test_start_date,),
                                 (test_end_date,),
                                 (split_adjusted_before_start,),
                                 (split_adjusted_after_end,)]

    @classmethod
    def init_class_fixtures(cls):
        super(WithAdjustmentBoundaries, cls).init_class_fixtures()
        cls.s0 = cls.asset_finder.retrieve_asset(0)
        cls.s1 = cls.asset_finder.retrieve_asset(1)
        cls.s2 = cls.asset_finder.retrieve_asset(2)
        cls.s3 = cls.asset_finder.retrieve_asset(3)
        cls.s4 = cls.asset_finder.retrieve_asset(4)
        cls.expected = cls.make_expected_out()

    @classmethod
    def make_events(cls):
        # We can create a sid for each configuration of dates for KDs, events,
        # and splits. For this test we don't care about overwrites so we only
        # test 1 quarter.
        sid_0_timeline = pd.DataFrame({
            # KD on first date of index
            TS_FIELD_NAME: cls.test_start_date,
            EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-09'),
            'estimate': 10.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 0,
        }, index=[0])

        sid_1_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_start_date,
            # event date on first date of index
            EVENT_DATE_FIELD_NAME: cls.test_start_date,
            'estimate': 11.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 1,
        }, index=[0])

        sid_2_timeline = pd.DataFrame({
            # KD on first date of index
            TS_FIELD_NAME: cls.test_end_date,
            EVENT_DATE_FIELD_NAME: cls.test_end_date + timedelta(days=1),
            'estimate': 12.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 2,
        }, index=[0])

        sid_3_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),
            EVENT_DATE_FIELD_NAME: cls.test_end_date,
            'estimate': 13.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 3,
        }, index=[0])

        # KD and event date don't fall on date index boundaries
        sid_4_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),
            EVENT_DATE_FIELD_NAME: cls.test_end_date - timedelta(days=1),
            'estimate': 14.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 4,
        }, index=[0])

        return pd.concat([sid_0_timeline,
                          sid_1_timeline,
                          sid_2_timeline,
                          sid_3_timeline,
                          sid_4_timeline])

    @classmethod
    def make_splits_data(cls):
        # Here we want splits that collide
        sid_0_splits = pd.DataFrame({
            SID_FIELD_NAME: 0,
            'ratio': .10,
            'effective_date': cls.test_start_date,
        }, index=[0])

        sid_1_splits = pd.DataFrame({
            SID_FIELD_NAME: 1,
            'ratio': .11,
            'effective_date': cls.test_start_date,
        }, index=[0])

        sid_2_splits = pd.DataFrame({
            SID_FIELD_NAME: 2,
            'ratio': .12,
            'effective_date': cls.test_end_date,
        }, index=[0])

        sid_3_splits = pd.DataFrame({
            SID_FIELD_NAME: 3,
            'ratio': .13,
            'effective_date': cls.test_end_date,
        }, index=[0])

        # We want 2 splits here - at the starting boundary and at the end
        # boundary - while there is no collision with KD/event date for the
        # sid.
        sid_4_splits = pd.DataFrame({
            SID_FIELD_NAME: 4,
            'ratio': (.14, .15),
            'effective_date': (cls.test_start_date, cls.test_end_date),
        })

        return pd.concat([sid_0_splits,
                          sid_1_splits,
                          sid_2_splits,
                          sid_3_splits,
                          sid_4_splits])
<A NAME="30"></A>
    @parameterized.expand(split_adjusted_asof_dates)
    def test_boundaries(self, split_date):
        dataset = QuartersEstimates<FONT color="#ae694a"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#30',2,'match48-top.html#30',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>(1)
        loader = self.loader(split_adjusted_asof=split_date)
        engine = engine = self.make_engine(loader)
        result = engine.run_pipeline(</B></FONT>
            Pipeline({'estimate': dataset.estimate.latest}),
            start_date=self.trading_days[0],
            # last event date we have
            end_date=self.trading_days[-1],
        )
        expected = self.expected[split_date]
        assert_frame_equal(result, expected, check_names=False)

    @classmethod
    def make_expected_out(cls):
        return {}


class PreviousWithAdjustmentBoundaries(WithAdjustmentBoundaries,
                                       ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(PreviousSplitAdjustedEarningsEstimatesLoader,
                       events,
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])

    @classmethod
    def make_expected_out(cls):
        split_adjusted_at_start_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': np.NaN,
            }, index=pd.date_range(
                cls.test_start_date,
                pd.Timestamp('2015-01-08'),
                tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10.,
            }, index=pd.date_range(
                pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
<A NAME="24"></A>            }, index=pd.date_range(cls.test_start_date, cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                <FONT color="#79764d"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#24',2,'match48-top.html#24',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>SID_FIELD_NAME: cls.s2,
                'estimate': np.NaN
            }, index=pd.date_range(cls.test_start_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.</B></FONT>s3,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'
            )),
<A NAME="22"></A>            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13. * .13
            }, index=pd.date_range(cls<FONT color="#4cc417"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#22',2,'match48-top.html#22',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.</B></FONT>test_end_date - timedelta(2), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14. * .15
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1), cls.test_end_date, tz='utc'
            )),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(
            SID_FIELD_NAME, dropna=False)

        split_adjusted_at_end_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': np.NaN,
            }, index=pd.date_range(
                cls.test_start_date, pd.Timestamp('2015-01-08'), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10.,
            }, index=pd.date_range(
                pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
            }, index=pd.date_range(cls.test_start_date,
<A NAME="23"></A>                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                <FONT color="#f660ab"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#23',2,'match48-top.html#23',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>SID_FIELD_NAME: cls.s2,
                'estimate': np.NaN
            }, index=pd.date_range(cls.test_start_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.</B></FONT>s3,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13.
            }, index=pd.date_range(cls.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.test_end_date - timedelta(2), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14.
            }, index=pd.date_range(cls.test_end_date - timedelta(1),
                                   cls.test_end_date,
                                   tz='utc')),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(SID_FIELD_NAME,
                                                            dropna=False)

        split_adjusted_before_start_boundary = split_adjusted_at_start_boundary
        split_adjusted_after_end_boundary = split_adjusted_at_end_boundary

        return {cls.test_start_date:
                split_adjusted_at_start_boundary,
                cls.split_adjusted_before_start:
                split_adjusted_before_start_boundary,
                cls.test_end_date:
                split_adjusted_at_end_boundary,
                cls.split_adjusted_after_end:
                split_adjusted_after_end_boundary}


class BlazePreviousWithAdjustmentBoundaries(PreviousWithAdjustmentBoundaries):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(BlazePreviousSplitAdjustedEstimatesLoader,
                       bz.data(events),
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])


class NextWithAdjustmentBoundaries(WithAdjustmentBoundaries,
                                   ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(NextSplitAdjustedEarningsEstimatesLoader,
                       events,
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])

    @classmethod
    def make_expected_out(cls):
        split_adjusted_at_start_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10,
            }, index=pd.date_range(
                cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'
            )),
<A NAME="0"></A>            pd.DataFrame({
                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
            }, index<FONT color="#0000ff"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#0',2,'match48-top.html#0',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>=pd.date_range(cls.test_start_date,
                                   cls.test_start_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s2,
                'estimate': 12.,
            }, index=pd.date_range(cls.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13. * .13,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1), cls.</B></FONT>test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14.,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1),
                cls.test_end_date - timedelta(1),
                tz='utc'
            )),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(
            SID_FIELD_NAME, dropna=False)

        split_adjusted_at_end_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10,
            }, index=pd.date_range(
                cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'
            )),
            pd.DataFrame({
<A NAME="2"></A>                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
            }, index=pd.date_range(cls.test_start_date,
                                   cls<FONT color="#980517"><div style="position:absolute;left:0"><A HREF="javascript:ZweiFrames('match48-0.html#2',2,'match48-top.html#2',1)"><IMG SRC="back.gif" ALT="other" BORDER="0" ALIGN="left"></A></div><B>.test_start_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s2,
                'estimate': 12.,
            }, index=pd.date_range(cls.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13.,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1), cls.</B></FONT>test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14.,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1),
                cls.test_end_date - timedelta(1),
                tz='utc'
            )),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(
            SID_FIELD_NAME, dropna=False)

        split_adjusted_before_start_boundary = split_adjusted_at_start_boundary
        split_adjusted_after_end_boundary = split_adjusted_at_end_boundary

        return {cls.test_start_date:
                split_adjusted_at_start_boundary,
                cls.split_adjusted_before_start:
                split_adjusted_before_start_boundary,
                cls.test_end_date:
                split_adjusted_at_end_boundary,
                cls.split_adjusted_after_end:
                split_adjusted_after_end_boundary}


class BlazeNextWithAdjustmentBoundaries(NextWithAdjustmentBoundaries):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(BlazeNextSplitAdjustedEstimatesLoader,
                       bz.data(events),
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])


class QuarterShiftTestCase(ZiplineTestCase):
    &quot;&quot;&quot;
    This tests, in isolation, quarter calculation logic for shifting quarters
    backwards/forwards from a starting point.
    &quot;&quot;&quot;
    def test_quarter_normalization(self):
        input_yrs = pd.Series(range(2011, 2015), dtype=np.int64)
        input_qtrs = pd.Series(range(1, 5), dtype=np.int64)
        result_years, result_quarters = split_normalized_quarters(
            normalize_quarters(input_yrs, input_qtrs)
        )
        # Can't use assert_series_equal here with check_names=False
        # because that still fails due to name differences.
        assert_equal(input_yrs, result_years)
        assert_equal(input_qtrs, result_quarters)
</PRE>
</div>
  </div>
</body>
</html>
