<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_algorithm.py &amp; test_quarters_estimates.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_algorithm.py &amp; test_quarters_estimates.py
      </h3>
<h1 align="center">
        12.3%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_algorithm.py (10.85359%)<th>test_quarters_estimates.py (14.4%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(2095-2131)<td><a href="#" name="0">(2608-2621)</a><td align="center"><font color="#ff0000">26</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(15-40)<td><a href="#" name="1">(1-49)</a><td align="center"><font color="#e10000">23</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(2275-2296)<td><a href="#" name="2">(2646-2658)</a><td align="center"><font color="#d70000">22</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1159-1174)<td><a href="#" name="3">(1900-1908)</a><td align="center"><font color="#cd0000">21</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(803-807)<td><a href="#" name="4">(1241-1251)</a><td align="center"><font color="#ba0000">19</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(2160-2169)<td><a href="#" name="5">(1787-1793)</a><td align="center"><font color="#b00000">18</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1384-1402)<td><a href="#" name="6">(1867-1873)</a><td align="center"><font color="#b00000">18</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(205-219)<td><a href="#" name="7">(1049-1054)</a><td align="center"><font color="#b00000">18</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(1324-1329)<td><a href="#" name="8">(1729-1735)</a><td align="center"><font color="#a60000">17</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(1174-1179)<td><a href="#" name="9">(2213-2218)</a><td align="center"><font color="#a60000">17</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(778-791)<td><a href="#" name="10">(1668-1674)</a><td align="center"><font color="#a60000">17</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(2325-2332)<td><a href="#" name="11">(1799-1804)</a><td align="center"><font color="#9c0000">16</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(1522-1531)<td><a href="#" name="12">(1880-1886)</a><td align="center"><font color="#9c0000">16</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1213-1218)<td><a href="#" name="13">(1346-1352)</a><td align="center"><font color="#9c0000">16</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(2190-2195)<td><a href="#" name="14">(1909-1913)</a><td align="center"><font color="#930000">15</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(634-638)<td><a href="#" name="15">(1810-1815)</a><td align="center"><font color="#930000">15</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(4174-4186)<td><a href="#" name="16">(1466-1478)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(3172-3180)<td><a href="#" name="17">(744-750)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(1143-1147)<td><a href="#" name="18">(1776-1781)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(935-953)<td><a href="#" name="19">(1975-1978)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(718-721)<td><a href="#" name="20">(969-975)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(3602-3607)<td><a href="#" name="21">(1204-1210)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(2555-2562)<td><a href="#" name="22">(2491-2498)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(2513-2517)<td><a href="#" name="23">(2530-2536)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(2495-2499)<td><a href="#" name="24">(2477-2483)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(1557-1588)<td><a href="#" name="25">(250-252)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#68818b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#68818b"><font color="#68818b">-</font><td><a href="#" name="26">(1209-1213)<td><a href="#" name="26">(609-615)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#e77471")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#e77471"><font color="#e77471">-</font><td><a href="#" name="27">(686-691)<td><a href="#" name="27">(925-930)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#717d7d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#717d7d"><font color="#717d7d">-</font><td><a href="#" name="28">(164-168)<td><a href="#" name="28">(1625-1629)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#af7a82")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#af7a82"><font color="#af7a82">-</font><td><a href="#" name="29">(4282-4296)<td><a href="#" name="29">(1194-1197)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#ae694a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ae694a"><font color="#ae694a">-</font><td><a href="#" name="30">(4149-4157)<td><a href="#" name="30">(2427-2430)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#3ea99f")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3ea99f"><font color="#3ea99f">-</font><td><a href="#" name="31">(3037-3046)<td><a href="#" name="31">(100-107)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#5b8daf")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5b8daf"><font color="#5b8daf">-</font><td><a href="#" name="32">(2773-2775)<td><a href="#" name="32">(1424-1437)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#736aff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#736aff"><font color="#736aff">-</font><td><a href="#" name="33">(2435-2438)<td><a href="#" name="33">(775-781)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#827d6b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#827d6b"><font color="#827d6b">-</font><td><a href="#" name="34">(2360-2364)<td><a href="#" name="34">(1586-1589)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#41a317")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#41a317"><font color="#41a317">-</font><td><a href="#" name="35">(1403-1420)<td><a href="#" name="35">(1575-1579)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#ff00ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ff00ff"><font color="#ff00ff">-</font><td><a href="#" name="36">(1329-1333)<td><a href="#" name="36">(2231-2232)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#810541")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#810541"><font color="#810541">-</font><td><a href="#" name="37">(1220-1221)<td><a href="#" name="37">(2139-2142)</a><td align="center"><font color="#750000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_algorithm.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import warnings
import datetime
from datetime import timedelta
from functools import partial
from textwrap import dedent
from copy import deepcopy
import logbook
import toolz
from logbook import TestHandler, WARNING
from nose_parameterized import parameterized
from six import iteritems, itervalues, string_types
from six.moves import range
from testfixtures import TempDirectory
import numpy as np
import pandas as pd
import pytz
from pandas.core.common import PerformanceWarning
from trading_calendars import get_calendar, register_calendar
import zipline.api
from zipline.api import FixedSlippage
from zipline.assets import Equity, Future, Asset
from zipline.assets.continuous_futures import ContinuousFuture
from</b></font> zipline.assets.synthetic import (
    make_jagged_equity_info,
    make_simple_equity_info,
)
from zipline.errors import (
    AccountControlViolation,
    CannotOrderDelistedAsset,
    IncompatibleSlippageModel,
    RegisterTradingControlPostInit,
    ScheduleFunctionInvalidCalendar,
    SetCancelPolicyPostInit,
    SymbolNotFound,
    TradingControlViolation,
    UnsupportedCancelPolicy,
    UnsupportedDatetimeFormat,
    ZeroCapitalError
)
from zipline.finance.commission import PerShare, PerTrade
from zipline.finance.execution import LimitOrder
from zipline.finance.order import ORDER_STATUS
from zipline.finance.trading import SimulationParameters
from zipline.finance.asset_restrictions import (
    Restriction,
    HistoricalRestrictions,
    StaticRestrictions,
    RESTRICTION_STATES,
)
from zipline.finance.controls import AssetDateBounds
from zipline.testing import (
    FakeDataPortal,
    create_daily_df_for_asset,
    create_data_portal_from_trade_history,
    create_minute_df_for_asset,
    make_test_handler,
    make_trade_data_for_asset_info,
    parameter_space,
    str_to_seconds,
    to_utc,
)
from zipline.testing import RecordBatchBlotter
import zipline.testing.fixtures as zf
from zipline.test_algorithms import (
    access_account_in_init,
    access_portfolio_in_init,
    api_algo,
    api_get_environment_algo,
    api_symbol_algo,
    handle_data_api,
    handle_data_noop,
    initialize_api,
    initialize_noop,
    noop_algo,
    record_float_magic,
    record_variables,
    call_with_kwargs,
    call_without_kwargs,
    call_with_bad_kwargs_current,
    call_with_bad_kwargs_history,
    bad_type_history_assets,
    bad_type_history_fields,
    bad_type_history_bar_count,
    bad_type_history_frequency,
    bad_type_history_assets_kwarg_list,
    bad_type_current_assets,
    bad_type_current_fields,
    bad_type_can_trade_assets,
    bad_type_is_stale_assets,
    bad_type_history_assets_kwarg,
    bad_type_history_fields_kwarg,
    bad_type_history_bar_count_kwarg,
    bad_type_history_frequency_kwarg,
    bad_type_current_assets_kwarg,
    bad_type_current_fields_kwarg,
    call_with_bad_kwargs_get_open_orders,
    call_with_good_kwargs_get_open_orders,
    call_with_no_kwargs_get_open_orders,
    empty_positions,
    no_handle_data,
)
from zipline.testing.predicates import assert_equal
from zipline.utils.api_support import ZiplineAPI
from zipline.utils.context_tricks import CallbackManager, nop_context
from zipline.utils.events import (
    date_rules,
    time_rules,
    Always,
    ComposedRule,
    Never,
    OncePerDay,
)
import zipline.utils.factory as factory
_multiprocess_can_split_ = False
class TestRecord(zf.WithMakeAlgo, zf.ZiplineTestCase):
    ASSET_FINDER_EQUITY_SIDS = (133,)
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    def test_record_incr(self):
        def initialize(self):
            self.incr = 0
        def handle_data(self, data):
            self.incr += 1
            self.record(incr=self.incr)
            name = 'name'
            self.record(name, self.incr)
            zipline.api.record(name, self.incr, 'name2', 2, name3=self.incr)
        output = self.run_algorithm(
            initialize=initialize,
            handle_data=handle_data,
        )
<a name="28"></a>        np.testing.assert_array_equal(output['incr'].values,
                                      range(1, len(output) + 1))
        np.testing.assert_array_equal(output['name'].values,
                                      range(1, len<font color="#717d7d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(output) + 1))
        np.testing.assert_array_equal(output['name2'].values,
                                      [2] * len(output))
        np.testing.assert_array_equal(output['name3'].values,
                                      range(</b></font>1, len(output) + 1))
class TestMiscellaneousAPI(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
    END_DATE = pd.Timestamp('2006-01-04', tz='UTC')
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    sids = 1, 2
    BENCHMARK_SID = None
    @classmethod
    def make_equity_info(cls):
        return pd.concat((
            make_simple_equity_info(cls.sids, '2002-02-1', '2007-01-01'),
            pd.DataFrame.from_dict(
                {3: {'symbol': 'PLAY',
                     'start_date': '2002-01-01',
                     'end_date': '2004-01-01',
                     'exchange': 'TEST'},
                 4: {'symbol': 'PLAY',
                     'start_date': '2005-01-01',
                     'end_date': '2006-01-01',
                     'exchange': 'TEST'}},
                orient='index',
            ),
        ))
    @classmethod
    def make_futures_info(cls):
        return pd.DataFrame.from_dict(
            {
<a name="7"></a>                5: {
                    'symbol': 'CLG06',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2005-12-01', tz<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>='UTC'),
                    'notice_date': pd.Timestamp('2005-12-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-01-20', tz='UTC'),
                    'exchange': 'TEST'
                },
                6: {
                    'root_symbol': 'CL',
                    'symbol': 'CLK06',
                    'start_date': pd.Timestamp('2005-12-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2006-03-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-04-20', tz='UTC'),
                    'exchange': 'TEST',
                },
                7: {
                    'symbol'</b></font>: 'CLQ06',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2005-12-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2006-06-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-07-20', tz='UTC'),
                    'exchange': 'TEST',
                },
                8: {
                    'symbol': 'CLX06',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2006-02-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2006-09-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2006-10-20', tz='UTC'),
                    'exchange': 'TEST',
                }
            },
            orient='index',
        )
    def test_cancel_policy_outside_init(self):
        code = """
from zipline.api import cancel_policy, set_cancel_policy
def initialize(algo):
    pass
def handle_data(algo, data):
    set_cancel_policy(cancel_policy.NeverCancel())
        algo = self.make_algo(script=code)
        with self.assertRaises(UnsupportedCancelPolicy):
            algo.run()
    def test_zipline_api_resolves_dynamically(self):
        algo = self.make_algo(
            initialize=lambda context: None,
            handle_data=lambda context, data: None,
        )
        for method in algo.all_api_methods():
            name = method.__name__
            sentinel = object()
            def fake_method(*args, **kwargs):
                return sentinel
            setattr(algo, name, fake_method)
            with ZiplineAPI(algo):
                self.assertIs(sentinel, getattr(zipline.api, name)())
    def test_sid_datetime(self):
        algo_text = """
from zipline.api import sid, get_datetime
def initialize(context):
    pass
def handle_data(context, data):
    aapl_dt = data.current(sid(1), "last_traded")
    assert_equal(aapl_dt, get_datetime())
        algo = self.make_algo(script=algo_text)
        with self.assertRaises(TypeError):
            algo.run()
    @parameterized.expand([
        (-1000, 'invalid_base'),
        (0, 'invalid_base'),
    ])
    def test_invalid_capital_base(self, cap_base, name):
        algo_text = """
def initialize(context):
    pass
def handle_data(context, data):
    order(sid(24), 1000)
        algo = self.make_algo(
            script=algotext,
            sim_params=self.make_simparams(
                trading_calendar=get_calendar("CMES"),
            )
        )
        algo.run()
        nyse = get_calendar("NYSE")
        for minute in algo.nyse_opens:
            session_label = nyse.minute_to_session_label(minute)
            session_open = nyse.session_open(session_label)
            self.assertEqual(session_open, minute)
        for minute in algo.nyse_closes:
            session_label = nyse.minute_to_session_label(minute)
            session_close = nyse.session_close(session_label)
            self.assertEqual(session_close - timedelta(minutes=1), minute)
        erroring_algotext = dedent(
        )
        algo = self.make_algo(
            script=erroring_algotext,
            sim_params=self.make_simparams(
                trading_calendar=get_calendar("CMES"),
            ),
        )
        with self.assertRaises(ScheduleFunctionInvalidCalendar):
            algo.run()
    def test_schedule_function(self):
        us_eastern = pytz.timezone('US/Eastern')
        def incrementer(algo, data):
            algo.func_called += 1
            curdt = algo.get_datetime().tz_convert(pytz.utc)
            self.assertEqual(
                curdt,
                us_eastern.localize(
                    datetime.datetime.combine(
                        curdt.date(),
                        datetime.time(9, 31)
                    ),
                ),
            )
        def initialize(algo):
            algo.func_called = 0
            algo.days = 1
            algo.date = None
            algo.schedule_function(
                func=incrementer,
                date_rule=date_rules.every_day(),
                time_rule=time_rules.market_open(),
            )
        def handle_data(algo, data):
            if not algo.date:
                algo.date = algo.get_datetime().date()
            if algo.date &lt; algo.get_datetime().date():
                algo.days += 1
                algo.date = algo.get_datetime().date()
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
        )
        algo.run()
        self.assertEqual(algo.func_called, algo.days)
    def test_event_context(self):
        expected_data = []
        collected_data_pre = []
        collected_data_post = []
        function_stack = []
        def pre(data):
            function_stack.append(pre)
            collected_data_pre.append(data)
        def post(data):
            function_stack.append(post)
            collected_data_post.append(data)
        def initialize(context):
            context.add_event(Always(), f)
            context.add_event(Always(), g)
        def handle_data(context, data):
            function_stack.append(handle_data)
            expected_data.append(data)
        def f(context, data):
            function_stack.append(f)
        def g(context, data):
            function_stack.append(g)
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            create_event_context=CallbackManager(pre, post),
        )
        algo.run()
        self.assertEqual(len(expected_data), 780)
        self.assertEqual(collected_data_pre, expected_data)
        self.assertEqual(collected_data_post, expected_data)
        self.assertEqual(
            len(function_stack),
            3900,
            'Incorrect number of functions called: %s != 3900' %
            len(function_stack),
        )
        expected_functions = [pre, handle_data, f, g, post] * 97530
        for n, (f, g) in enumerate(zip(function_stack, expected_functions)):
            self.assertEqual(
                f,
                g,
                'function at position %d was incorrect, expected %s but got %s'
                % (n, g.__name__, f.__name__),
            )
    @parameterized.expand([
        ('daily',),
        ('minute'),
    ])
    def test_schedule_function_rule_creation(self, mode):
        def nop(*args, **kwargs):
            return None
        self.sim_params.data_frequency = mode
        algo = self.make_algo(
            initialize=nop,
            handle_data=nop,
            sim_params=self.sim_params,
        )
        algo.schedule_function(nop, time_rule=Never() &amp; Always())
        event_rule = algo.event_manager._events[1].rule
        self.assertIsInstance(event_rule, OncePerDay)
        self.assertEqual(event_rule.cal, algo.trading_calendar)
        inner_rule = event_rule.rule
        self.assertIsInstance(inner_rule, ComposedRule)
        self.assertEqual(inner_rule.cal, algo.trading_calendar)
        first = inner_rule.first
        second = inner_rule.second
        composer = inner_rule.composer
        self.assertIsInstance(first, Always)
        self.assertEqual(first.cal, algo.trading_calendar)
        self.assertEqual(second.cal, algo.trading_calendar)
        if mode == 'daily':
<a name="15"></a>            self.assertIsInstance(second, Always)
        else:
            self.assertIsInstance(second, ComposedRule)
            self<font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertIsInstance(second.first, Never)
            self.assertEqual(second.first.cal, algo.trading_calendar)
            self.assertIsInstance(second.second, Always)
            self.assertEqual(second.second.</b></font>cal, algo.trading_calendar)
        self.assertIs(composer, ComposedRule.lazy_and)
    def test_asset_lookup(self):
        algo = self.make_algo()
        start_session = pd.Timestamp("2000-01-01", tz="UTC")
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2001-12-01', tz='UTC')
        )
        with self.assertRaises(SymbolNotFound):
            algo.symbol('PLAY')
        with self.assertRaises(SymbolNotFound):
            algo.symbols('PLAY')
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2002-12-01', tz='UTC')
        )
        list_result = algo.symbols('PLAY')
        self.assertEqual(3, list_result[0])
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2004-12-01', tz='UTC')
        )
        self.assertEqual(3, algo.symbol('PLAY'))
        algo.sim_params = algo.sim_params.create_new(
            start_session,
            pd.Timestamp('2005-12-01', tz='UTC')
        )
        self.assertEqual(4, algo.symbol('PLAY'))
        algo.sim_params = algo.sim_params.create_new(
            start_session,
<a name="27"></a>            pd.Timestamp('2006-12-01', tz='UTC')
        )
        self.assertEqual(4, algo.symbol('PLAY'))
        list_result = algo<font color="#e77471"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.symbols('PLAY')
        self.assertEqual(4, list_result[0])
        self.assertIsInstance(algo.sid(3), Equity)
        self.assertIsInstance(algo.sid(</b></font>4), Equity)
        with self.assertRaises(TypeError):
            algo.symbol(1)
        with self.assertRaises(TypeError):
            algo.symbol((1,))
        with self.assertRaises(TypeError):
            algo.symbol({1})
        with self.assertRaises(TypeError):
            algo.symbol([1])
        with self.assertRaises(TypeError):
            algo.symbol({'foo': 'bar'})
    def test_future_symbol(self):
        algo = self.make_algo()
        algo.datetime = pd.Timestamp('2006-12-01', tz='UTC')
<a name="20"></a>
        cl = algo.future_symbol('CLG06')
        self<font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertEqual(cl.sid, 5)
        self.assertEqual(cl.symbol, 'CLG06')
        self.assertEqual(cl.root_symbol, 'CL')
        self.assertEqual(cl.start_date, pd.Timestamp(</b></font>'2005-12-01', tz='UTC'))
        self.assertEqual(cl.notice_date, pd.Timestamp('2005-12-20', tz='UTC'))
        self.assertEqual(cl.expiration_date,
                         pd.Timestamp('2006-01-20', tz='UTC'))
        with self.assertRaises(SymbolNotFound):
            algo.future_symbol('')
        with self.assertRaises(SymbolNotFound):
            algo.future_symbol('PLAY')
        with self.assertRaises(SymbolNotFound):
            algo.future_symbol('FOOBAR')
        with self.assertRaises(TypeError):
            algo.future_symbol(1)
        with self.assertRaises(TypeError):
            algo.future_symbol((1,))
        with self.assertRaises(TypeError):
            algo.future_symbol({1})
        with self.assertRaises(TypeError):
            algo.future_symbol([1])
        with self.assertRaises(TypeError):
            algo.future_symbol({'foo': 'bar'})
class TestSetSymbolLookupDate(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
    END_DATE = pd.Timestamp('2006-01-06', tz='UTC')
    SIM_PARAMS_START_DATE = pd.Timestamp('2006-01-04', tz='UTC')
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    BENCHMARK_SID = 3
    @classmethod
    def make_equity_info(cls):
        dates = pd.date_range(cls.START_DATE, cls.END_DATE)
        assert len(dates) == 4, "Expected four dates."
<a name="10"></a>        cls.sids = [1, 2, 3]
        cls.asset_starts = [dates[0], dates[2]]
        cls.asset_ends = [dates[1], dates[3]]
        return pd<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.DataFrame.from_records([
            {'symbol': 'DUP',
             'start_date': cls.asset_starts[0],
             'end_date': cls.asset_ends[0],
             'exchange': 'TEST',
             'asset_name': 'FIRST'},
            {'symbol': 'DUP',
             'start_date': cls.asset_starts[1],
             'end_date': cls.asset_ends[1],
             'exchange': 'TEST',
             'asset_name': 'SECOND'},
            {'symbol': 'BENCH',
             'start_date': cls.START_DATE,
             'end_date': cls.</b></font>END_DATE,
             'exchange': 'TEST',
             'asset_name': 'BENCHMARK'},
        ], index=cls.sids)
    def test_set_symbol_lookup_date(self):
<a name="4"></a>        set_symbol_lookup_date = zipline.api.set_symbol_lookup_date
        def initialize(context):
            set_symbol_lookup_date(self<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.asset_ends[0])
            self.assertEqual(zipline.api.symbol('DUP').sid, self.sids[0])
            set_symbol_lookup_date(self.asset_ends[1])
            self.assertEqual(zipline.api.symbol('DUP').</b></font>sid, self.sids[1])
            with self.assertRaises(UnsupportedDatetimeFormat):
                set_symbol_lookup_date('foobar')
        self.run_algorithm(initialize=initialize)
class TestPositions(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-01-06', tz='utc')
    SIM_PARAMS_CAPITAL_BASE = 1000
    ASSET_FINDER_EQUITY_SIDS = (1, 133)
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        frame = pd.DataFrame(
            {
                'open': [90, 95, 100, 105],
                'high': [90, 95, 100, 105],
                'low': [90, 95, 100, 105],
                'close': [90, 95, 100, 105],
                'volume': 100,
            },
            index=cls.equity_daily_bar_days,
        )
        return ((sid, frame) for sid in sids)
    @classmethod
    def make_futures_info(cls):
        return pd.DataFrame.from_dict(
            {
                1000: {
                    'symbol': 'CLF06',
                    'root_symbol': 'CL',
                    'start_date': cls.START_DATE,
                    'end_date': cls.END_DATE,
                    'auto_close_date': cls.END_DATE + cls.trading_calendar.day,
                    'exchange': 'CMES',
                    'multiplier': 100,
                },
            },
            orient='index',
        )
    @classmethod
    def make_future_minute_bar_data(cls):
        trading_calendar = cls.trading_calendars[Future]
        sids = cls.asset_finder.futures_sids
        minutes = trading_calendar.minutes_for_sessions_in_range(
            cls.future_minute_bar_days[0],
            cls.future_minute_bar_days[-1],
        )
        frame = pd.DataFrame(
            {
                'open': 2.0,
                'high': 2.0,
                'low': 2.0,
                'close': 2.0,
                'volume': 100,
            },
            index=minutes,
        )
        return ((sid, frame) for sid in sids)
    def test_portfolio_exited_position(self):
        def initialize(context, sids):
            context.ordered = False
            context.exited = False
            context.sids = sids
        def handle_data(context, data):
            if not context.ordered:
                for s in context.sids:
                    context.order(context.sid(s), 1)
                context.ordered = True
            if not context.exited:
                amounts = [pos.amount for pos
                           in itervalues(context.portfolio.positions)]
                if (
                    len(amounts) &gt; 0 and
                    all([(amount == 1) for amount in amounts])
                ):
                    for stock in context.portfolio.positions:
                        context.order(context.sid(stock), -1)
                    context.exited = True
            context.record(num_positions=len(context.portfolio.positions))
        result = self.run_algorithm(
            initialize=initialize,
            handle_data=handle_data,
            sids=self.ASSET_FINDER_EQUITY_SIDS,
        )
        expected_position_count = [
            0,  # Before entering the first position
            2,  # After entering, exiting on this date
            0,  # After exiting
            0,
        ]
        for i, expected in enumerate(expected_position_count):
            self.assertEqual(result.ix[i]['num_positions'], expected)
    def test_noop_orders(self):
        asset = self.asset_finder.retrieve_asset(1)
        def handle_data(algo, data):
            algo.order(asset, 100, limit_price<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=1)
            algo.order(asset, 100, stop_price=10000000)
            algo.order(asset, 100, limit_price=10000000, stop_price=10000000)
            algo.order(asset, 100, limit_price=1, stop_price=1)
            algo.order(</b></font>asset, -100, limit_price=1000000)
            algo.order(asset, -100, stop_price=1)
            algo.order(asset, -100, limit_price=1000000, stop_price=1000000)
            algo.order(asset, -100, limit_price=1, stop_price=1)
            algo.order(asset, 100, limit_price=.00000001)
            algo.order(asset, -100, stop_price=.00000001)
        daily_stats = self.run_algorithm(handle_data=handle_data)
        empty_positions = daily_stats.positions.map(lambda x: len(x) == 0)
        self.assertTrue(empty_positions.all())
    def test_position_weights(self):
        sids = (1, 133, 1000)
        equity_1, equity_133, future_1000 = \
            self.asset_finder.retrieve_all(sids)
        def initialize(algo, sids_and_amounts, *args, **kwargs):
            algo.ordered = False
            algo.sids_and_amounts = sids_and_amounts
            algo.set_commission(
                us_equities=PerTrade(0), us_futures=PerTrade(0),
            )
            algo.set_slippage(
                us_equities=FixedSlippage(0),
                us_futures=FixedSlippage(0),
            )
        def handle_data(algo, data):
            if not algo.ordered:
                for s, amount in algo.sids_and_amounts:
                    algo.order(algo.sid(s), amount)
                algo.ordered = True
            algo.record(
                position_weights=algo.portfolio.current_portfolio_weights,
            )
        daily_stats = self.run_algorithm(
            sids_and_amounts=zip(sids, [2, -1, 1]),
            initialize=initialize,
            handle_data=handle_data,
        )
        expected_position_weights = [
            pd.Series({}),
            pd.Series({
                equity_1: 190.0 / (190.0 - 95.0 + 905.0),
                equity_133: -95.0 / (190.0 - 95.0 + 905.0),
                future_1000: 200.0 / (190.0 - 95.0 + 905.0),
            }),
            pd.Series({
                equity_1: 200.0 / (200.0 - 100.0 + 905.0),
                equity_133: -100.0 / (200.0 - 100.0 + 905.0),
                future_1000: 200.0 / (200.0 - 100.0 + 905.0),
            }),
            pd.Series({
                equity_1: 210.0 / (210.0 - 105.0 + 905.0),
                equity_133: -105.0 / (210.0 - 105.0 + 905.0),
                future_1000: 200.0 / (210.0 - 105.0 + 905.0),
            }),
        ]
        for i, expected in enumerate(expected_position_weights):
            assert_equal(daily_stats.iloc[i]['position_weights'], expected)
class TestBeforeTradingStart(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-06', tz='utc')
    END_DATE = pd.Timestamp('2016-01-07', tz='utc')
    SIM_PARAMS_CAPITAL_BASE = 10000
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    EQUITY_DAILY_BAR_LOOKBACK_DAYS = EQUITY_MINUTE_BAR_LOOKBACK_DAYS = 1
    DATA_PORTAL_FIRST_TRADING_DAY = pd.Timestamp("2016-01-05", tz='UTC')
    EQUITY_MINUTE_BAR_START_DATE = pd.Timestamp("2016-01-05", tz='UTC')
    FUTURE_MINUTE_BAR_START_DATE = pd.Timestamp("2016-01-05", tz='UTC')
    data_start = ASSET_FINDER_EQUITY_START_DATE = pd.Timestamp(
        '2016-01-05',
        tz='utc',
    )
    SPLIT_ASSET_SID = 3
    ASSET_FINDER_EQUITY_SIDS = 1, 2, SPLIT_ASSET_SID
    @classmethod
    def make_equity_minute_bar_data(cls):
        asset_minutes = \
            cls.trading_calendar.minutes_in_range(
                cls.data_start,
                cls.END_DATE,
            )
        minutes_count = len(asset_minutes)
        minutes_arr = np.arange(minutes_count) + 1
        split_data = pd.DataFrame(
            {
                'open': minutes_arr + 1,
                'high': minutes_arr + 2,
                'low': minutes_arr - 1,
                'close': minutes_arr,
                'volume': 100 * minutes_arr,
            },
            index=asset_minutes,
        )
        split_data.iloc[780:] = split_data.iloc[780:] / 2.0
        for sid in (1, 8554):
            yield sid, create_minute_df_for_asset(
                cls.trading_calendar,
                cls.data_start,
                cls.END_DATE,
            )
        yield 2, create_minute_df_for_asset(
            cls.trading_calendar,
            cls.data_start,
            cls.END_DATE,
            50,
        )
        yield cls.SPLIT_ASSET_SID, split_data
    @classmethod
    def make_splits_data(cls):
        return pd.DataFrame.from_records([
            {
                'effective_date': str_to_seconds('2016-01-07'),
                'ratio': 0.5,
                'sid': cls.SPLIT_ASSET_SID,
            }
        ])
    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        for sid in sids:
            yield sid, create_daily_df_for_asset(
                cls.trading_calendar,
                cls.data_start,
                cls.END_DATE,
            )
    def test_data_in_bts_minute(self):
        algo_code = dedent("""
        from zipline.api import record, sid
        def initialize(context):
            context.history_values = []
        def before_trading_start(context, data):
            record(the_price1=data.current(sid(1), "price"))
            record(the_high1=data.current(sid(1), "high"))
            record(the_price2=data.current(sid(2), "price"))
            record(the_high2=data.current(sid(2), "high"))
            context.history_values.append(data.history(
                [sid(1), sid(2)],
                ["price", "high"],
                60,
                "1m"
            ))
        def handle_data(context, data):
            pass
        self<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertEqual(390, results.iloc[0].the_price1)
        self.assertEqual(392, results.iloc[0].the_high1)
        self.assertEqual(350, results.iloc[</b></font>0].the_price2)
        self.assertTrue(np.isnan(results.iloc[0].the_high2))
        np.testing.assert_array_equal(
            range(331, 391), algo.history_values[0]["price"][1]
        )
<a name="3"></a>
        np.testing.assert_array_equal(
            range(333, 393), algo.history_values[0]["high"]<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[1]
        )
        np.testing.assert_array_equal(
            [300] * 19, algo.history_values[0]["price"][2][0:19]
        )
        np.testing.assert_array_equal(
            [350] * 40, algo.history_values[0]["price"][2][20:]
        )
        np.testing.assert_array_equal(
            np.</b></font>full<font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(19, np.nan), algo.history_values[0]["high"][2][0:19]
        )
        self.assertEqual(352, algo.history_values[0]["high"][2][19])
        np.testing.assert_array_equal(</b></font>
            np.full(40, np.nan), algo.history_values[0]["high"][2][20:]
        )
    def test_data_in_bts_daily(self):
        algo_code = dedent("""
        from zipline.api import record, sid
        def initialize(context):
            context.history_values = []
        def before_trading_start(context, data):
            record(the_price1=data.current(sid(1), "price"))
            record(the_high1=data.current(sid(1), "high"))
            record(the_price2=data.current(sid(2), "price"))
            record(the_high2=data.current(sid(2), "high"))
            context.history_values.append(data.history(
                [sid(1), sid(2)],
                ["price", "high"],
                1,
                "1d",
            ))
        def handle_data(context, data):
            pass
        self<font color="#68818b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertEqual(392, results.the_high1[0])
<a name="13"></a>        self.assertEqual(390, results.the_price1[0])
        self.assertTrue(np.isnan(results.</b></font>the_high2<font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[0]))
        self.assertTrue(350, results.the_price2[0])
<a name="37"></a>        self.assertEqual(392, algo.history_values[0]["high"][1][0])
        self.assertEqual(390, algo.history_values[</b></font>0]["price"][1][0])
        self.assertEqual(352, algo<font color="#810541"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.history_values[0]["high"][2][0])
        self.assertEqual(350, algo.history_values[0]["price"][2][</b></font>0])
    def test_portfolio_bts(self):
        algo_code = dedent("""
        from zipline.api import order, sid, record
        def initialize(context):
            context.ordered = False
            context.hd_portfolio = context.portfolio
        def before_trading_start(context, data):
            bts_portfolio = context.portfolio
            assert (context.hd_portfolio == bts_portfolio)
            record(pos_value=bts_portfolio.positions_value)
        def handle_data(context, data):
            if not context.ordered:
                order(sid(1), 1)
                context.ordered = True
            context.hd_portfolio = context.portfolio
        algo = self.make_algo(script=algo_code)
        results = algo.run()
        self.assertEqual(results.port_value.iloc[0], 10000)
        self.assertAlmostEqual(results.port_value.iloc[1],
                               10000 + 780 - 392 - 0,
                               places=2)
    def test_portfolio_bts_with_overnight_split(self):
        algo_code = dedent("""
        from zipline.api import order, sid, record
        def initialize(context):
            context.ordered = False
            context.hd_portfolio = context.portfolio
        def before_trading_start(context, data):
            bts_portfolio = context.portfolio
            for k in bts_portfolio.__dict__:
                if k != 'positions':
                    assert (context.hd_portfolio.__dict__[k]
                            == bts_portfolio.__dict__[k])
            record(pos_value=bts_portfolio.positions_value)
            record(pos_amount=bts_portfolio.positions[sid(3)].amount)
            record(
                last_sale_price=bts_portfolio.positions[sid(3)].last_sale_price
            )
        def handle_data(context, data):
            if not context.ordered:
                order(sid(3), 1)
                context.ordered = True
            context.hd_portfolio = context.portfolio
        self.assertEqual(results<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.pos_value.iloc[0], 0)
        self.assertEqual(results.pos_value.iloc[1], 780)
<a name="36"></a>
        self.assertEqual(results.pos_amount.iloc[0], 0)
        self.assertEqual(results.pos_amount.</b></font>iloc<font color="#ff00ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[1], 2)
        self.assertEqual(results.last_sale_price.iloc[0], 0)
        self.assertEqual(results.last_sale_price.iloc[</b></font>1], 390)
    def test_account_bts_with_overnight_split(self):
        algo_code = dedent("""
        from zipline.api import order, sid, record, set_slippage, slippage
        def initialize(context):
            context.ordered = False
            context.hd_account = context.account
            set_slippage(slippage.VolumeShareSlippage())
        def before_trading_start(context, data):
            bts_account = context.account
            assert (context.hd_account == bts_account)
            record(port_value=bts_account.equity_with_loan)
        def handle_data(context, data):
            if not context.ordered:
                order(sid(1), 1)
                context.ordered = True
            context.hd_account = context.account
        <font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('history__assets', (bad_type_history_assets,
                             ASSET_OR_STRING_OR_CF_TYPE_NAMES,
                             True)),
        ('history__fields', (bad_type_history_fields,
                             STRING_TYPE_NAMES_STRING,
                             True)),
        ('history__bar_count', (bad_type_history_bar_count, 'int', False)),
        ('history__frequency', (bad_type_history_frequency,
                                STRING_TYPE_NAMES_STRING,
                                False)),
        ('current__assets', (bad_type_current_assets,
                             ASSET_OR_STRING_OR_CF_TYPE_NAMES,
                             True)),
        ('current__fields', (bad_type_current_fields,
                             STRING_TYPE_NAMES_STRING,
                             True)),
<a name="35"></a>        ('is_stale__assets', (bad_type_is_stale_assets, 'Asset', True)),
        ('can_trade__assets', (bad_type_can_trade_assets, 'Asset', True)),
        ('history_kwarg__assets'</b></font>,
         (<font color="#41a317"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>bad_type_history_assets_kwarg,
          ASSET_OR_STRING_OR_CF_TYPE_NAMES,
          True)),
        ('history_kwarg_bad_list__assets',
         (bad_type_history_assets_kwarg_list,
          ASSET_OR_STRING_OR_CF_TYPE_NAMES,
          True)),
        ('history_kwarg__fields',
         (bad_type_history_fields_kwarg, STRING_TYPE_NAMES_STRING, True)),
        ('history_kwarg__bar_count',
         (bad_type_history_bar_count_kwarg, 'int', False)),
        ('history_kwarg__frequency',
         (bad_type_history_frequency_kwarg, STRING_TYPE_NAMES_STRING, False)),
        ('current_kwarg__assets',
         (bad_type_current_assets_kwarg,
          ASSET_OR_STRING_OR_CF_TYPE_NAMES,
          True)),
        ('current_kwarg__fields'</b></font>,
         (bad_type_current_fields_kwarg, STRING_TYPE_NAMES_STRING, True)),
    )
    sids = 0, 1, 3, 133
    BENCHMARK_SID = None
    @classmethod
    def make_equity_info(cls):
        register_calendar("TEST", get_calendar("NYSE"), force=True)
        data = make_simple_equity_info(
            cls.sids,
            cls.START_DATE,
            cls.END_DATE,
        )
        data.loc[3, 'symbol'] = 'TEST'
        return data
    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        cal = cls.trading_calendars[Equity]
        sessions = cal.sessions_in_range(cls.START_DATE, cls.END_DATE)
        frame = pd.DataFrame({
            'close': 10., 'high': 10.5, 'low': 9.5, 'open': 10., 'volume': 100,
        }, index=sessions)
        for sid in sids:
            yield sid, frame
    def test_noop(self):
        self.run_algorithm(
            initialize=initialize_noop,
            handle_data=handle_data_noop,
        )
    def test_noop_string(self):
        self.run_algorithm(script=noop_algo)
    def test_no_handle_data(self):
        self.run_algorithm(script=no_handle_data)
    def test_api_calls(self):
        self.run_algorithm(
            initialize=initialize_api,
            handle_data=handle_data_api,
        )
    def test_api_calls_string(self):
        self.run_algorithm(script=api_algo)
    def test_api_get_environment(self):
        platform = 'zipline'
        algo = self.make_algo(
            script=api_get_environment_algo,
            platform=platform,
        )
        algo.run()
        self.assertEqual(algo.environment, platform)
    def test_api_symbol(self):
        self.run_algorithm(script=api_symbol_algo)
    def test_fixed_slippage(self):
        test_algo = self.make_algo(
            script="""
from zipline.api import (slippage,
                         commission,
                         set_slippage,
                         set_commission,
                         order,
                         record,
                         sid)
def initialize(context):
    model = slippage.FixedSlippage(spread=0.10)
    set_slippage(model)
    set_commission(commission.PerTrade(100.00))
    context.count = 1
    context.incr = 0
def handle_data(context, data):
    if context.incr &lt; context.count:
        order(sid(0), -1000)
    record(price=data.current(sid(0), "price"))
    context.incr += 1""",
        )
        results = test_algo.run()
        all_txns = [val for sublist in results["transactions"].tolist()
                    for val in sublist]
        self.assertEqual(len(all_txns), 1)
<a name="12"></a>        txn = all_txns[0]
        expected_spread = 0.05
        expected_price = test_algo<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.recorded_vars["price"] - expected_spread
        self.assertEqual(expected_price, txn['price'])
        self.assertEqual(9850, results.capital_used[1])
        self.assertEqual(100, results["orders"].iloc[1][0][</b></font>"commission"])
    @parameterized.expand(
        [
            ('no_minimum_commission', 0,),
            ('default_minimum_commission', 0,),
            ('alternate_minimum_commission', 2,),
        ]
    )
    def test_volshare_slippage(self, name, minimum_commission):
        tempdir = TempDirectory()
        try:
            if name == "default_minimum_commission":
                commission_line = "set_commission(commission.PerShare(0.02))"
            else:
                commission_line = \
                    "set_commission(commission.PerShare(0.02, " \
                    "min_trade_cost={0}))".format(minimum_commission)
<a name="25"></a>            trades = factory.create_daily_trade_source(
                [0], self.sim_params, self.asset_finder, self.trading_calendar
            )
            data_portal = create_data_portal_from_trade_history<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(
                self.asset_finder, self.trading_calendar, tempdir,
                self.sim_params, {0: trades}
            )
            test_algo = self.make_algo(
                data_portal=data_portal,
                script="""
from zipline.api import *
def initialize(context):
    model = slippage.VolumeShareSlippage(
                            volume_limit=.3,
                            price_impact=0.05
                       )
    set_slippage(model)
    {0}
    context.count = 2
    context.incr = 0
def handle_data(context, data):
    if context.incr &lt; context.count:
        order(sid(0), 5000)
    record(price=data.current(sid(0), "price"))
    record(volume=data.current(sid(0), "volume"))
    record(incr=context.incr)
    context.incr += 1
            results =</b></font> test_algo.run()
            all_txns = [
                val for sublist in results["transactions"].tolist()
                for val in sublist]
            self.assertEqual(len(all_txns), 67)
            all_orders = list(toolz.concat(results['orders']))
            if minimum_commission == 0:
                for order_ in all_orders:
                    self.assertAlmostEqual(
                        order_["filled"] * 0.02,
                        order_["commission"]
                    )
            else:
                for order_ in all_orders:
                    if order_["filled"] &gt; 0:
                        self.assertAlmostEqual(
                            max(order_["filled"] * 0.02, minimum_commission),
                            order_["commission"]
                        )
                    else:
                        self.assertEqual(0, order_["commission"])
        finally:
            tempdir.cleanup()
    def test_incorrectly_set_futures_slippage_model(self):
        code = dedent(
        )
        test_algo = self.make_algo(script=code)
        with self.assertRaises(IncompatibleSlippageModel):
            test_algo.run()
    def test_algo_record_vars(self):
        test_algo = self.make_algo(script=record_variables)
        results = test_algo.run()
        for i in range(1, 252):
            self.assertEqual(results.iloc[i-1]["incr"], i)
    def test_algo_record_nan(self):
        test_algo = self.make_algo(script=record_float_magic % 'nan')
        results = test_algo.run()
        for i in range(1, 252):
            self.assertTrue(np.isnan(results.iloc[i-1]["data"]))
    def test_batch_market_order_matches_multiple_manual_orders(self):
        share_counts = pd.Series([50, 100])
        multi_blotter = RecordBatchBlotter()
        multi_test_algo = self.make_algo(
            script=dedent("""\
                from collections import OrderedDict
                from six import iteritems
                from zipline.api import sid, order
                def initialize(context):
                    context.assets = [sid(0), sid(3)]
                    context.placed = False
                def handle_data(context, data):
                    if not context.placed:
                        it = zip(context.assets, {share_counts})
                        for asset, shares in it:
                            order(asset, shares)
                        context.placed = True
            blotter=batch_blotter,
        )
        batch_stats = batch_test_algo.run()
        self.assertTrue(batch_blotter.order_batch_called)
        for stats in (multi_stats, batch_stats):
            stats.orders = stats.orders.apply(
                lambda orders: [toolz.dissoc(o, 'id') for o in orders]
            )
            stats.transactions = stats.transactions.apply(
                lambda txns: [toolz.dissoc(txn, 'order_id') for txn in txns]
            )
        assert_equal(multi_stats, batch_stats)
    def test_batch_market_order_filters_null_orders(self):
        share_counts = [50, 0]
        batch_blotter = RecordBatchBlotter()
        batch_test_algo = self.make_algo(
            script=dedent("""\
                import pandas as pd
                from zipline.api import sid, batch_market_order
                def initialize(context):
                    context.assets = [sid(0), sid(3)]
                    context.placed = False
                def handle_data(context, data):
                    if not context.placed:
                        orders = batch_market_order(pd.Series(
                            index=context.assets, data={share_counts}
                        ))
                        assert len(orders) == 1, \
                            "len(orders) was %s but expected 1" % len(orders)
                        for o in orders:
                            assert o is not None, "An order is None"
                        context.placed = True
        )
        for order_str in ["order_value", "order_percent"]:
            test_algo = self.make_algo(
                script="""
from zipline.api import order_percent, order_value, sid
def initialize(context):
    pass
def handle_data(context, data):
    {0}(sid(0), 10)
        Test that accessing portfolio in init doesn't break.
        Test that accessing account in init doesn't break.
        Test that api methods on the data object can be called with positional
        arguments.
        Test that api methods on the data object can be called with keyword
        arguments.
        Test that api methods on the data object called with bad kwargs return
        a meaningful TypeError that we create, rather than an unhelpful cython
        error
            sim_params=params,
        )
    @parameterized.expand(
        [('bad_kwargs', call_with_bad_kwargs_get_open_orders),
         ('good_kwargs', call_with_good_kwargs_get_open_orders),
         ('no_kwargs', call_with_no_kwargs_get_open_orders)]
    )
    def test_get_open_orders_kwargs(self, name, script):
        algo = self.make_algo(script=script)
        if name == 'bad_kwargs':
            with self.assertRaises(TypeError) as cm:
                algo.run()
                self.assertEqual('Keyword argument `sid` is no longer '
                                 'supported for get_open_orders. Use `asset` '
                                 'instead.', cm.exception.args[0])
        else:
            algo.run()
    def test_empty_positions(self):
        results = self.run_algorithm(script=empty_positions)
        num_positions = results.num_positions
        amounts = results.amounts
        self.assertTrue(all(num_positions == 0))
        self.assertTrue(all(amounts == 0))
    def test_schedule_function_time_rule_positionally_misplaced(self):
        sim_params = factory.create_simulation_parameters(
            start=pd.Timestamp('2006-01-12', tz='UTC'),
            end=pd.Timestamp('2006-01-13', tz='UTC'),
            data_frequency='minute'
        )
        algocode = dedent("""
        from zipline.api import time_rules, schedule_function
        def do_at_open(context, data):
            context.done_at_open.append(context.get_datetime())
        def do_at_close(context, data):
            context.done_at_close.append(context.get_datetime())
        def initialize(context):
            context.done_at_open = []
            context.done_at_close = []
            schedule_function(do_at_open, time_rules.market_open())
            schedule_function(do_at_close, time_rules.market_close())
        def handle_data(algo, data):
            pass
        algo = self.make_algo(
            script=algocode,
            capital_changes=capital_changes,
            sim_params=SimulationParameters(
                start_session=self.START_DATE,
                end_session=self.END_DATE,
                trading_calendar=self.nyse_calendar,
            )
        )
        gen = algo.get_generator()
        results = list(gen)
        cumulative_perf = \
            [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
        daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
        capital_change_packets = \
            [r['capital_change'] for r in results if 'capital_change' in r]
        self.assertEqual(len(capital_change_packets), 1)
        self.assertEqual(
            capital_change_packets[0],
            {'date': pd.Timestamp('2006-01-06', tz='UTC'),
             'type': 'cash',
             'target': 151000.0 if change_type == 'target' else None,
             'delta': 50000.0})
<a name="0"></a>
        expected_daily = {}
        expected_capital_changes <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.array([
            0.0, 0.0, 0.0, 50000.0, 0.0
        ])
        expected_daily['returns'] = np.array([
            0.0,
            0.0,
            (100000.0 + 1000.0) / 100000.0 - 1.0,
            (151000.0 + 2000.0) / 151000.0 - 1.0,
            (153000.0 + 3000.0) / 153000.0 - 1.0,
        ])
        expected_daily['pnl'] = np.array([
            0.0,
            0.0,
            1000.00,  # 1000 shares * gain of 1
            2000.00,  # 2000 shares * gain of 1
            3000.00,  # 3000 shares * gain of 1
        ])
        expected_daily['capital_used'] = np.array([
            0.0,
            -11000.0,  # 1000 shares at price = 11
            -12000.0,  # 1000 shares at price = 12
            -13000.0,  # 1000 shares at price = 13
            -14000.0,  # 1000 shares at price = 14
        ])
        expected_daily['ending_cash'] = \
            np.array([100000.0] * 5) + \
            np.cumsum(expected_capital_changes) + \
            np.</b></font>cumsum(expected_daily['capital_used'])
        expected_daily['starting_cash'] = \
            expected_daily['ending_cash'] - \
            expected_daily['capital_used']
        expected_daily['starting_value'] = np.array([
            0.0,
            0.0,
            11000.0,  # 1000 shares at price = 11
            24000.0,  # 2000 shares at price = 12
            39000.0,  # 3000 shares at price = 13
        ])
        expected_daily['ending_value'] = \
            expected_daily['starting_value'] + \
            expected_daily['pnl'] - \
            expected_daily['capital_used']
        expected_daily['portfolio_value'] = \
            expected_daily['ending_value'] + \
            expected_daily['ending_cash']
        stats = [
            'returns', 'pnl', 'capital_used', 'starting_cash', 'ending_cash',
            'starting_value', 'ending_value', 'portfolio_value'
<a name="5"></a>        ]
        expected_cumulative = {
            'returns': np.cumprod(expected_daily<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['returns'] + 1) - 1,
            'pnl': np.cumsum(expected_daily['pnl']),
            'capital_used': np.cumsum(expected_daily['capital_used']),
            'starting_cash':
                np.repeat(expected_daily['starting_cash'][0:1], 5),
            'ending_cash': expected_daily['ending_cash'],
            'starting_value':
                np.repeat(expected_daily['starting_value'][0:1], 5),
            'ending_value': expected_daily['ending_value'],
            'portfolio_value': expected_daily[</b></font>'portfolio_value'],
        }
        for stat in stats:
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in daily_perf]),
                expected_daily[stat],
                err_msg='daily ' + stat,
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in cumulative_perf]),
                expected_cumulative[stat],
                err_msg='cumulative ' + stat,
            )
        self.assertEqual(
            algo.capital_change_deltas,
            {pd.Timestamp('2006-01-06', tz='UTC'): 50000.0}
<a name="14"></a>        )
    @parameterized.expand([
        <font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('interday_target', [('2006-01-04', 2388.0)]),
        ('interday_delta', [('2006-01-04', 1000.0)]),
        ('intraday_target', [('2006-01-04 17:00', 2184.0),
                             ('2006-01-04 18:00', 2804.0)]),
        ('intraday_delta', [('2006-01-04 17:00', 500.0),
                            ('2006-01-04 18:00'</b></font>, 500.0)]),
    ])
    def test_capital_changes_minute_mode_daily_emission(self, change, values):
        change_loc, change_type = change.split('_')
        sim_params = SimulationParameters(
            start_session=pd.Timestamp('2006-01-03', tz='UTC'),
            end_session=pd.Timestamp('2006-01-05', tz='UTC'),
            data_frequency='minute',
            capital_base=1000.0,
            trading_calendar=self.nyse_calendar,
        )
        capital_changes = {
            pd.Timestamp(datestr, tz='UTC'): {
                'type': change_type,
                'value': value
            }
            for datestr, value in values
        }
        algocode = """
from zipline.api import set_slippage, set_commission, slippage, commission, \
    schedule_function, time_rules, order, sid
def initialize(context):
    set_slippage(slippage.FixedSlippage(spread=0))
    set_commission(commission.PerShare(0, 0))
    schedule_function(order_stuff, time_rule=time_rules.market_open())
def order_stuff(context, data):
    order(sid(1), 1)
        expected_daily<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['returns'] = np.array([
            (1000.0 + 489 - 101) / 1000.0 - 1.0,
            day2_return,
            (3166.0 + 390.0 + 390.0 + 388.0) / 3166.0 - 1.0,
        ])
        expected_daily['pnl'] = np.array([
            388.0,
            390.0 + 388.0,
            390.0 + 390.0 + 388.0,
        ])
        expected_daily['capital_used'] = np.array([
            -101.0, -491.0, -881.0
        ])
        expected_daily['ending_cash'] = \
            np.array([1000.0] * 3) + \
            np.cumsum(expected_capital_changes) + \
            np.</b></font>cumsum(expected_daily['capital_used'])
        expected_daily['starting_cash'] = \
            expected_daily['ending_cash'] - \
            expected_daily['capital_used']
        if change_loc == 'intraday':
            expected_daily['starting_cash'] -= expected_capital_changes
        expected_daily['starting_value'] = np.array([
            0.0, 489.0, 879.0 * 2
        ])
        expected_daily['ending_value'] = \
            expected_daily['starting_value'] + \
            expected_daily['pnl'] - \
            expected_daily['capital_used']
        expected_daily['portfolio_value'] = \
            expected_daily['ending_value'] + \
            expected_daily['ending_cash']
        stats = [
            'returns', 'pnl', 'capital_used', 'starting_cash', 'ending_cash',
            'starting_value', 'ending_value', 'portfolio_value'
<a name="11"></a>        ]
        expected_cumulative = {
            'returns': np.cumprod(expected_daily<font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['returns'] + 1) - 1,
            'pnl': np.cumsum(expected_daily['pnl']),
            'capital_used': np.cumsum(expected_daily['capital_used']),
            'starting_cash':
                np.repeat(expected_daily['starting_cash'][0:1], 3),
            'ending_cash': expected_daily['ending_cash'],
            'starting_value':
                np.repeat(expected_daily['starting_value'][</b></font>0:1], 3),
            'ending_value': expected_daily['ending_value'],
            'portfolio_value': expected_daily['portfolio_value'],
        }
        for stat in stats:
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in daily_perf]),
                expected_daily[stat]
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in cumulative_perf]),
                expected_cumulative[stat]
            )
        if change_loc == 'interday':
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04', tz='UTC'): 1000.0}
            )
        else:
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04 17:00', tz='UTC'): 500.0,
                 pd.Timestamp('2006-01-04 18:00', tz='UTC'): 500.0}
<a name="34"></a>            )
    @parameterized.expand([
        <font color="#827d6b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('interday_target', [('2006-01-04', 2388.0)]),
        ('interday_delta', [('2006-01-04', 1000.0)]),
        ('intraday_target', [('2006-01-04 17:00', 2184.0),
                             ('2006-01-04 18:00', 2804.0)]),
        ('intraday_delta'</b></font>, [('2006-01-04 17:00', 500.0),
                            ('2006-01-04 18:00', 500.0)]),
    ])
    def test_capital_changes_minute_mode_minute_emission(self, change, values):
        change_loc, change_type = change.split('_')
        sim_params = SimulationParameters(
            start_session=pd.Timestamp('2006-01-03', tz='UTC'),
            end_session=pd.Timestamp('2006-01-05', tz='UTC'),
            data_frequency='minute',
            emission_rate='minute',
            capital_base=1000.0,
            trading_calendar=self.nyse_calendar,
        )
        capital_changes = {pd.Timestamp(val[0], tz='UTC'): {
            'type': change_type, 'value': val[1]} for val in values}
        algocode = """
from zipline.api import set_slippage, set_commission, slippage, commission, \
    schedule_function, time_rules, order, sid
def initialize(context):
    set_slippage(slippage.FixedSlippage(spread=0))
    set_commission(commission.PerShare(0, 0))
    schedule_function(order_stuff, time_rule=time_rules.market_open())
def order_stuff(context, data):
    order(sid(1), 1)
        expected_minute<font color="#736aff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['pnl'][:2] = 0.0
        expected_minute['pnl'][2:392] = 1.0
        expected_minute['pnl'][392:782] = 2.0
        expected_minute['pnl'][782:] =</b></font> 3.0
        for start, end in ((0, 390), (390, 780), (780, 1170)):
            expected_minute['pnl'][start:end] = \
                np.cumsum(expected_minute['pnl'][start:end])
        expected_minute['capital_used'] = np.concatenate((
            [0.0] * 1, [-101.0] * 389,
            [0.0] * 1, [-491.0] * 389,
            [0.0] * 1, [-881.0] * 389,
        ))
        day2adj = 0.0 if change_loc == 'intraday' else 1000.0
        expected_minute['starting_cash'] = np.concatenate((
            [1000.0] * 390,
            [1000.0 - 101.0 + day2adj] * 390,
            [1000.0 - 101.0 - 491.0 + 1000] * 390
        ))
        expected_minute['ending_cash'] = \
            expected_minute['starting_cash'] + \
            expected_minute['capital_used'] + \
            capital_changes_after_start
        expected_minute['starting_value'] = np.concatenate((
            [0.0] * 390,
            [489.0] * 390,
            [879.0 * 2] * 390
        ))
        expected_minute['ending_value'] = \
            expected_minute['starting_value'] + \
            expected_minute['pnl'] - \
            expected_minute['capital_used']
        expected_minute['portfolio_value'] = \
            expected_minute['ending_value'] + \
            expected_minute['ending_cash']
        expected_minute['returns'] = \
            expected_minute['pnl'] / \
            (expected_minute['starting_value'] +
             expected_minute['starting_cash'])
        if change_loc == 'intraday':
            prev_subperiod_return = expected_minute['returns'][538]
<a name="24"></a>
            cur_subperiod_pnl = \
                expected_minute['pnl']<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[539:599] - expected_minute['pnl'][538]
            cur_subperiod_starting_value = \
                np.array([expected_minute['ending_value'][538]] * 60)
            cur_subperiod_starting_cash = \
                np.array([expected_minute['ending_cash'][</b></font>538] + 500] * 60)
            cur_subperiod_returns = cur_subperiod_pnl / \
                (cur_subperiod_starting_value + cur_subperiod_starting_cash)
            expected_minute['returns'][539:599] = \
                (cur_subperiod_returns + 1.0) * \
                (prev_subperiod_return + 1.0) - \
                1.0
            prev_subperiod_return = expected_minute['returns'][598]
<a name="23"></a>
            cur_subperiod_pnl = \
                expected_minute['pnl']<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[599:780] - expected_minute['pnl'][598]
            cur_subperiod_starting_value = \
                np.array([expected_minute['ending_value'][598]] * 181)
            cur_subperiod_starting_cash = \
                np.array([expected_minute['ending_cash'][</b></font>598] + 500] * 181)
            cur_subperiod_returns = cur_subperiod_pnl / \
                (cur_subperiod_starting_value + cur_subperiod_starting_cash)
            expected_minute['returns'][599:780] = \
                (cur_subperiod_returns + 1.0) * \
                (prev_subperiod_return + 1.0) - \
                1.0
        expected_daily = {
            k: np.array([v[389], v[779], v[1169]])
            for k, v in iteritems(expected_minute)
        }
        stats = [
            'pnl', 'capital_used', 'starting_cash', 'ending_cash',
            'starting_value', 'ending_value', 'portfolio_value', 'returns'
        ]
        expected_cumulative = deepcopy(expected_minute)
        expected_cumulative['returns'][390:] = \
            (expected_cumulative['returns'][390:] + 1) * \
            (expected_daily['returns'][0] + 1) - 1
        expected_cumulative['returns'][780:] = \
            (expected_cumulative['returns'][780:] + 1) * \
            (expected_daily['returns'][1] + 1) - 1
        expected_cumulative['pnl'][390:] += expected_daily['pnl'][0]
        expected_cumulative['pnl'][780:] += expected_daily['pnl'][1]
<a name="22"></a>        expected_cumulative['capital_used'][390:] += \
            expected_daily['capital_used'][0]
        expected_cumulative['capital_used'][780:] += \
            expected_daily['capital_used']<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[1]
        expected_cumulative['starting_cash'] = \
            np.repeat(expected_daily['starting_cash'][0:1], 1170)
        expected_cumulative['starting_value'] = \
            np.repeat(expected_daily['starting_value'][</b></font>0:1], 1170)
        for stat in stats:
            for i in (390, 781, 1172):
                expected_cumulative[stat] = np.insert(
                    expected_cumulative[stat],
                    i,
                    expected_cumulative[stat][i-1]
                )
        for stat in stats:
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in minute_perf]),
                expected_minute[stat]
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in daily_perf]),
                expected_daily[stat]
            )
            np.testing.assert_array_almost_equal(
                np.array([perf[stat] for perf in cumulative_perf]),
                expected_cumulative[stat]
            )
        if change_loc == 'interday':
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04', tz='UTC'): 1000.0}
            )
        else:
            self.assertEqual(
                algo.capital_change_deltas,
                {pd.Timestamp('2006-01-04 17:00', tz='UTC'): 500.0,
                 pd.Timestamp('2006-01-04 18:00', tz='UTC'): 500.0}
            )
class TestGetDatetime(zf.WithMakeAlgo, zf.ZiplineTestCase):
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    START_DATE = to_utc('2014-01-02 9:31')
    END_DATE = to_utc('2014-01-03 9:31')
    ASSET_FINDER_EQUITY_SIDS = 0, 1
    BENCHMARK_SID = None
    @parameterized.expand(
        [
            ('default', None,),
            ('utc', 'UTC',),
            ('us_east', 'US/Eastern',),
        ]
    )
    def test_get_datetime(self, name, tz):
        algo = dedent(
        )
        algo = self.make_algo(script=algo)
        algo.run()
        self.assertFalse(algo.first_bar)
class TestTradingControls(zf.WithMakeAlgo,
                          zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-01-06', tz='utc')
    sid = 133
    sids = ASSET_FINDER_EQUITY_SIDS = 133, 134
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = True
    @classmethod
    def init_class_fixtures(cls):
        super(TestTradingControls, cls).init_class_fixtures()
        cls.asset = cls.asset_finder.retrieve_asset(cls.sid)
        cls.another_asset = cls.asset_finder.retrieve_asset(134)
    def _check_algo(self,
                    algo,
                    expected_order_count,
                    expected_exc):
        with self.assertRaises(expected_exc) if expected_exc else nop_context:
            algo.run()
        self.assertEqual(algo.order_count, expected_order_count)
    def check_algo_succeeds(self, algo, order_count=4):
        self._check_algo(algo, order_count, None)
    def check_algo_fails(self, algo, order_count):
        self._check_algo(algo,
                         order_count,
                         TradingControlViolation)
    def test_set_max_position_size(self):
        def initialize(self, asset, max_shares, max_notional):
            self.set_slippage(FixedSlippage())
            self.order_count = 0
            self.set_max_position_size(asset=asset,
                                       max_shares=max_shares,
                                       max_notional=max_notional)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 1)
            algo.order_count += 1
        algo = self.make_algo(
            asset=self.asset,
            max_shares=10,
            max_notional=500.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_succeeds(algo)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 3)
            algo.order_count += 1
        algo = self.make_algo(
            asset=self.asset,
            max_shares=10,
            max_notional=500.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 3)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 3)
            algo.order_count += 1
        algo = self.make_algo(
            asset=self.asset,
            max_shares=10,
            max_notional=67.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 2)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1
        algo = self.make_algo(
            asset=self.another_asset,
            max_shares=10,
            max_notional=67.0,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_succeeds(algo)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1
        algo = self.make_algo(
            max_shares=10,
            max_notional=61.0,
            asset=None,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 0)
    def test_set_asset_restrictions(self):
        def initialize(algo, sid, restrictions, on_error):
            algo.order_count = 0
<a name="32"></a>            algo.set_asset_restrictions(restrictions, on_error)
        def handle_data(algo, data):
            algo.could_trade = data<font color="#5b8daf"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.can_trade(algo.sid(self.sid))
            algo.order(algo.sid(self.sid), 100)
            algo.order_count +=</b></font> 1
        rlm = HistoricalRestrictions([
            Restriction(
                self.sid,
                self.sim_params.start_session,
                RESTRICTION_STATES.FROZEN)
        ])
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='fail',
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade)
        rlm = StaticRestrictions([self.sid])
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='fail',
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade)
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='log',
            initialize=initialize,
            handle_data=handle_data,
        )
        with make_test_handler(self) as log_catcher:
            self.check_algo_succeeds(algo)
        logs = [r.message for r in log_catcher.records]
        self.assertIn("Order for 100 shares of Equity(133 [A]) at "
                      "2006-01-03 21:00:00+00:00 violates trading constraint "
                      "RestrictedListOrder({})", logs)
        self.assertFalse(algo.could_trade)
        rlm = HistoricalRestrictions([
            Restriction(
                sid,
                self.sim_params.start_session,
                RESTRICTION_STATES.FROZEN) for sid in [134, 135, 136]
        ])
        algo = self.make_algo(
            sid=self.sid,
            restrictions=rlm,
            on_error='fail',
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_succeeds(algo)
        self.assertTrue(algo.could_trade)
    @parameterized.expand([
        ('order_first_restricted_sid', 0),
        ('order_second_restricted_sid', 1)
    ])
    def test_set_multiple_asset_restrictions(self, name, to_order_idx):
        def initialize(algo, restrictions1, restrictions2, on_error):
            algo.order_count = 0
            algo.set_asset_restrictions(restrictions1, on_error)
            algo.set_asset_restrictions(restrictions2, on_error)
        def handle_data(algo, data):
            algo.could_trade1 = data.can_trade(algo.sid(self.sids[0]))
            algo.could_trade2 = data.can_trade(algo.sid(self.sids[1]))
            algo.order(algo.sid(self.sids[to_order_idx]), 100)
            algo.order_count += 1
        rl1 = StaticRestrictions([self.sids[0]])
        rl2 = StaticRestrictions([self.sids[1]])
        algo = self.make_algo(
            restrictions1=rl1,
            restrictions2=rl2,
            initialize=initialize,
            handle_data=handle_data,
            on_error='fail',
        )
        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade1)
        self.assertFalse(algo.could_trade2)
    def test_set_do_not_order_list(self):
        def initialize(self, restricted_list):
            self.order_count = 0
            self.set_do_not_order_list(restricted_list, on_error='fail')
        def handle_data(algo, data):
            algo.could_trade = data.can_trade(algo.sid(self.sid))
            algo.order(algo.sid(self.sid), 100)
            algo.order_count += 1
        rlm = [self.sid]
        algo = self.make_algo(
            restricted_list=rlm,
            initialize=initialize,
            handle_data=handle_data,
        )
        self.check_algo_fails(algo, 0)
        self.assertFalse(algo.could_trade)
    def test_set_max_order_size(self):
        def initialize(algo, asset, max_shares, max_notional):
            algo.order_count = 0
            algo.set_max_order_size(asset=asset,
                                    max_shares=max_shares,
                                    max_notional=max_notional)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 1)
            algo.order_count += 1
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.asset,
            max_shares=10,
            max_notional=500.0,
        )
        self.check_algo_succeeds(algo)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), algo.order_count + 1)
            algo.order_count += 1
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.asset,
            max_shares=3,
            max_notional=500.0,
        )
        self.check_algo_fails(algo, 3)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), algo.order_count + 1)
            algo.order_count += 1
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.asset,
            max_shares=10,
            max_notional=40.0,
        )
        self.check_algo_fails(algo, 3)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=self.another_asset,
            max_shares=1,
            max_notional=1.0,
        )
        self.check_algo_succeeds(algo)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), 10000)
            algo.order_count += 1
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            asset=None,
            max_shares=1,
            max_notional=1.0,
        )
        self.check_algo_fails(algo, 0)
    def test_set_max_order_count(self):
        def initialize(algo, count):
            algo.order_count = 0
            algo.set_max_order_count(count)
        def handle_data(algo, data):
            for i in range(5):
                algo.order(self.asset, 1)
                algo.order_count += 1
        algo = self.make_algo(
            count=3,
            initialize=initialize,
            handle_data=handle_data,
        )
        with self.assertRaises(TradingControlViolation):
            algo.run()
        self.assertEqual(algo.order_count, 3)
    def test_set_max_order_count_minutely(self):
        sim_params = self.make_simparams(data_frequency='minute')
        def initialize(algo, max_orders_per_day):
            algo.minute_count = 0
            algo.order_count = 0
            algo.set_max_order_count(max_orders_per_day)
        def handle_data(algo, data):
            if algo.minute_count == 0 or algo.minute_count == 100:
                for i in range(5):
                    algo.order(self.asset, 1)
                    algo.order_count += 1
            algo.minute_count += 1
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_orders_per_day=9,
            sim_params=sim_params,
        )
        with self.assertRaises(TradingControlViolation):
            algo.run()
        self.assertEqual(algo.order_count, 9)
        def handle_data(algo, data):
            if (algo.minute_count % 390) == 0:
                for i in range(5):
                    algo.order(self.asset, 1)
                    algo.order_count += 1
<a name="31"></a>
            algo.minute_count += 1
        algo <font color="#3ea99f"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_orders_per_day=5,
            sim_params=sim_params,
        )
        algo.run()
        self.assertEqual(algo.</b></font>order_count, 20)
    def test_long_only(self):
        def initialize(algo):
            algo.order_count = 0
            algo.set_long_only()
        def handle_data(algo, data):
            algo.order(algo.sid(self.sid), -1)
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_fails(algo, 0)
        def handle_data(algo, data):
            if (algo.order_count % 2) == 0:
                algo.order(algo.sid(self.sid), 1)
            else:
                algo.order(algo.sid(self.sid), -1)
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_succeeds(algo)
        def handle_data(algo, data):
            amounts = [1, 1, 1, -3]
            algo.order(algo.sid(self.sid), amounts[algo.order_count])
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_succeeds(algo)
        def handle_data(algo, data):
            amounts = [1, 1, 1, -4]
            algo.order(algo.sid(self.sid), amounts[algo.order_count])
            algo.order_count += 1
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        self.check_algo_fails(algo, 3)
    def test_register_post_init(self):
        def initialize(algo):
            algo.initialized = True
        def handle_data(algo, data):
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_max_position_size(self.sid, 1, 1)
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_max_order_size(self.sid, 1, 1)
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_max_order_count(1)
            with self.assertRaises(RegisterTradingControlPostInit):
                algo.set_long_only()
        self.run_algorithm(initialize=initialize, handle_data=handle_data)
class TestAssetDateBounds(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2014-01-02', tz='UTC')
    END_DATE = pd.Timestamp('2014-01-03', tz='UTC')
    SIM_PARAMS_START_DATE = END_DATE  # Only run for one day.
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    BENCHMARK_SID = 3
    @classmethod
    def make_equity_info(cls):
        T = partial(pd.Timestamp, tz='UTC')
        return pd.DataFrame.from_records([
            {'sid': 1,
             'symbol': 'OLD',
             'start_date': T('1990'),
             'end_date': T('1991'),
             'exchange': 'TEST'},
            {'sid': 2,
             'symbol': 'NEW',
             'start_date': T('2017'),
             'end_date': T('2018'),
             'exchange': 'TEST'},
            {'sid': 3,
             'symbol': 'GOOD',
             'start_date': cls.START_DATE,
             'end_date': cls.END_DATE,
             'exchange': 'TEST'},
        ])
    def test_asset_date_bounds(self):
        def initialize(algo):
            algo.ran = False
            algo.register_trading_control(AssetDateBounds(on_error='fail'))
        def handle_data(algo, data):
            algo.order(algo.sid(3), 1)
            with self.assertRaises(TradingControlViolation):
                algo.order(algo.sid(1), 1)
            with self.assertRaises(TradingControlViolation):
                algo.order(algo.sid(2), 1)
            algo.ran = True
        algo = self.make_algo(initialize=initialize, handle_data=handle_data)
        algo.run()
        self.assertTrue(algo.ran)
class TestAccountControls(zf.WithMakeAlgo,
                          zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2006-01-03', tz='utc')
    END_DATE = pd.Timestamp('2006-01-06', tz='utc')
    sidint, = ASSET_FINDER_EQUITY_SIDS = (133,)
    BENCHMARK_SID = None
<a name="17"></a>    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    <font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>@classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        frame = pd.DataFrame(data={
            'close': [10., 10., 11., 11.],
            'open': [10., 10., 11., 11.],
            'low': [9.5, 9.5, 10.45, 10.45],
            'high': [10.5, 10.5, 11.55, 11.55],
            'volume': [100, 100, 100, 300],
        }, index=cls.</b></font>equity_daily_bar_days)
        yield cls.sidint, frame
    def _check_algo(self, algo, expected_exc):
        with self.assertRaises(expected_exc) if expected_exc else nop_context:
            algo.run()
    def check_algo_succeeds(self, algo):
        self._check_algo(algo, None)
    def check_algo_fails(self, algo):
        self._check_algo(algo, AccountControlViolation)
    def test_set_max_leverage(self):
        def initialize(algo, max_leverage):
            algo.set_max_leverage(max_leverage=max_leverage)
        def handle_data(algo, data):
            algo.order(algo.sid(self.sidint), 1)
            algo.record(latest_time=algo.get_datetime())
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_leverage=0,
        )
        self.check_algo_fails(algo)
        self.assertEqual(
            algo.recorded_vars['latest_time'],
            pd.Timestamp('2006-01-04 21:00:00', tz='UTC'),
        )
        def handle_data(algo, data):
            algo.order(algo.sid(self.sidint), 1)
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            max_leverage=1,
        )
        self.check_algo_succeeds(algo)
    def test_set_min_leverage(self):
        def initialize(algo, min_leverage, grace_period):
            algo.set_min_leverage(
                min_leverage=min_leverage, grace_period=grace_period
            )
        def handle_data(algo, data):
            algo.order_target_percent(algo.sid(self.sidint), .5)
            algo.record(latest_time=algo.get_datetime())
        def make_algo(min_leverage, grace_period):
            return self.make_algo(
                initialize=initialize,
                handle_data=handle_data,
                min_leverage=min_leverage,
                grace_period=grace_period,
            )
        offset = pd.Timedelta('10 days')
        algo = make_algo(min_leverage=1, grace_period=offset)
        self.check_algo_succeeds(algo)
        offset = pd.Timedelta('1 days')
        algo = make_algo(min_leverage=1, grace_period=offset)
        self.check_algo_fails(algo)
        self.assertEqual(
            algo.recorded_vars['latest_time'],
            pd.Timestamp('2006-01-04 21:00:00', tz='UTC'),
        )
        offset = pd.Timedelta('2 days')
        algo = make_algo(min_leverage=1, grace_period=offset)
        self.check_algo_fails(algo)
        self.assertEqual(
            algo.recorded_vars['latest_time'],
            pd.Timestamp('2006-01-05 21:00:00', tz='UTC'),
        )
        algo = make_algo(min_leverage=.0001, grace_period=offset)
        self.check_algo_succeeds(algo)
class TestFuturesAlgo(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-06', tz='utc')
    END_DATE = pd.Timestamp('2016-01-07', tz='utc')
    FUTURE_MINUTE_BAR_START_DATE = pd.Timestamp('2016-01-05', tz='UTC')
    SIM_PARAMS_DATA_FREQUENCY = 'minute'
    TRADING_CALENDAR_STRS = ('us_futures',)
    TRADING_CALENDAR_PRIMARY_CAL = 'us_futures'
    BENCHMARK_SID = None
    @classmethod
    def make_futures_info(cls):
        return pd.DataFrame.from_dict(
            {
                1: {
                    'symbol': 'CLG16',
                    'root_symbol': 'CL',
                    'start_date': pd.Timestamp('2015-12-01', tz='UTC'),
                    'notice_date': pd.Timestamp('2016-01-20', tz='UTC'),
                    'expiration_date': pd.Timestamp('2016-02-19', tz='UTC'),
                    'auto_close_date': pd.Timestamp('2016-01-18', tz='UTC'),
                    'exchange': 'TEST',
                },
            },
            orient='index',
        )
    def test_futures_history(self):
        algo_code = dedent(
        )
        algo = self.make_algo(
            script=algo_code,
            trading_calendar=get_calendar('us_futures'),
        )
        algo.run()
        np.testing.assert_array_equal(
            algo.history_values[0].index,
            pd.date_range(
                '2016-01-06 6:27',
                '2016-01-06 6:31',
                freq='min',
                tz='US/Eastern',
            ),
        )
        np.testing.assert_array_equal(
            algo.history_values[1].index,
            pd.date_range(
                '2016-01-07 6:27',
                '2016-01-07 6:31',
                freq='min',
                tz='US/Eastern',
            ),
        )
        np.testing.assert_array_equal(
            algo.history_values[0].values, list(map(float, range(2196, 2201))),
        )
        np.testing.assert_array_equal(
            algo.history_values[1].values, list(map(float, range(3636, 3641))),
        )
    @staticmethod
    def algo_with_slippage(slippage_model):
        return dedent(
        ).format(model=slippage_model)
    def test_fixed_future_slippage(self):
        algo_code = self.algo_with_slippage('FixedSlippage(spread=0.10)')
        algo = self.make_algo(
            script=algo_code,
            trading_calendar=get_calendar('us_futures'),
        )
        results = algo.run()
        all_txns = [
            val for sublist in results['transactions'].tolist()
            for val in sublist
        ]
        self.assertEqual(len(all_txns), 1)
        txn = all_txns[0]
        expected_spread = 0.05
        expected_price = (algo.order_price + 1) + expected_spread
        self.assertEqual(txn['price'], expected_price)
        self.assertEqual(results['orders'][0][0]['commission'], 0.0)
    def test_volume_contract_slippage(self):
        algo_code = self.algo_with_slippage(
            'VolumeShareSlippage(volume_limit=0.05, price_impact=0.1)',
        )
        algo = self.make_algo(
            script=algo_code,
            trading_calendar=get_calendar('us_futures'),
        )
        results = algo.run()
        self.assertEqual(results['orders'][0][0]['commission'], 0.0)
        all_txns = [
            val for sublist in results['transactions'].tolist()
            for val in sublist
        ]
        self.assertEqual(len(all_txns), 2)
        for i, txn in enumerate(all_txns):
            order_price = algo.order_price + i + 1
            expected_impact = order_price * 0.1 * (0.05 ** 2)
            expected_price = order_price + expected_impact
            self.assertEqual(txn['price'], expected_price)
class TestAnalyzeAPIMethod(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-05', tz='utc')
    END_DATE = pd.Timestamp('2016-01-05', tz='utc')
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    def test_analyze_called(self):
        self.perf_ref = None
        def initialize(context):
            pass
        def handle_data(context, data):
            pass
        def analyze(context, perf):
            self.perf_ref = perf
        algo = self.make_algo(
            initialize=initialize, handle_data=handle_data, analyze=analyze,
        )
        results = algo.run()
        self.assertIs(results, self.perf_ref)
class TestOrderCancelation(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2016-01-05', tz='utc')
    END_DATE = pd.Timestamp('2016-01-07', tz='utc')
    ASSET_FINDER_EQUITY_SIDS = (1,)
    ASSET_FINDER_EQUITY_SYMBOLS = ('ASSET1',)
    BENCHMARK_SID = None
    code = dedent(
    )
    @classmethod
    def make_equity_minute_bar_data(cls):
        asset_minutes = \
            cls.trading_calendar.minutes_for_sessions_in_range(
                cls.START_DATE,
                cls.END_DATE,
            )
        minutes_count = len(asset_minutes)
        minutes_arr = np.arange(1, 1 + minutes_count)
        yield 1, pd.DataFrame(
            {
                'open': minutes_arr + 1,
                'high': minutes_arr + 2,
                'low': minutes_arr - 1,
                'close': minutes_arr,
                'volume': np.full(minutes_count, 1.0),
            },
            index=asset_minutes,
        )
    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        yield 1, pd.DataFrame(
            {
                'open': np.full(3, 1, dtype=np.float64),
                'high': np.full(3, 1, dtype=np.float64),
                'low': np.full(3, 1, dtype=np.float64),
                'close': np.full(3, 1, dtype=np.float64),
                'volume': np.full(3, 1, dtype=np.float64),
            },
            index=cls.equity_daily_bar_days,
        )
    def prep_algo(self,
                  cancelation_string,
                  data_frequency="minute",
                  amount=1000,
                  minute_emission=False):
        code = self.code.format(cancelation_string, amount)
        return self.make_algo(
            script=code,
            sim_params=self.make_simparams(
                data_frequency=data_frequency,
                emission_rate='minute' if minute_emission else 'daily',
            )
        )
    @parameter_space(
        direction=[1, -1],
        minute_emission=[True, False],
    )
    def test_eod_order_cancel_minute(self, direction, minute_emission):
        algo = self.prep_algo(
            "set_cancel_policy(cancel_policy.EODCancel())",
            amount=np.copysign(1000, direction),
            minute_emission=minute_emission
        )
        log_catcher = TestHandler()
        with log_catcher:
<a name="21"></a>            results = algo.run()
            for daily_positions in results.positions:
                self.assertEqual(1, len<font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(daily_positions))
                self.assertEqual(
                    np.copysign(389, direction),
                    daily_positions[0]["amount"],
                )
                self.assertEqual(1, results.positions[0][0][</b></font>"sid"])
            np.testing.assert_array_equal([1, 0, 0],
                                          list(map(len, results.orders)))
            np.testing.assert_array_equal([389, 0, 0],
                                          list(map(len, results.transactions)))
            the_order = results.orders[0][0]
            self.assertEqual(ORDER_STATUS.CANCELLED, the_order["status"])
            self.assertEqual(np.copysign(389, direction), the_order["filled"])
            warnings = [record for record in log_catcher.records if
                        record.level == WARNING]
            self.assertEqual(1, len(warnings))
            if direction == 1:
                self.assertEqual(
                    "Your order for 1000 shares of ASSET1 has been partially "
                    "filled. 389 shares were successfully purchased. "
                    "611 shares were not filled by the end of day and "
                    "were canceled.",
                    str(warnings[0].message)
                )
            elif direction == -1:
                self.assertEqual(
                    "Your order for -1000 shares of ASSET1 has been partially "
                    "filled. 389 shares were successfully sold. "
                    "611 shares were not filled by the end of day and "
                    "were canceled.",
                    str(warnings[0].message)
                )
    def test_default_cancelation_policy(self):
        algo = self.prep_algo("")
        log_catcher = TestHandler()
        with log_catcher:
            results = algo.run()
            np.testing.assert_array_equal([1, 1, 1],
                                          list(map(len, results.orders)))
            np.testing.assert_array_equal([389, 390, 221],
                                          list(map(len, results.transactions)))
            self.assertFalse(log_catcher.has_warnings)
    def test_eod_order_cancel_daily(self):
        algo = self.prep_algo(
            "set_cancel_policy(cancel_policy.EODCancel())",
            "daily"
        )
        log_catcher = TestHandler()
        with log_catcher:
            results = algo.run()
            np.testing.assert_array_equal([1, 1, 1],
                                          list(map(len, results.orders)))
            np.testing.assert_array_equal([0, 1, 1],
                                          list(map(len, results.transactions)))
            self.assertFalse(log_catcher.has_warnings)
class TestDailyEquityAutoClose(zf.WithMakeAlgo, zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2015-01-05', tz='UTC')
    END_DATE = pd.Timestamp('2015-01-13', tz='UTC')
    SIM_PARAMS_DATA_FREQUENCY = 'daily'
    DATA_PORTAL_USE_MINUTE_DATA = False
    BENCHMARK_SID = None
    @classmethod
    def init_class_fixtures(cls):
        super(TestDailyEquityAutoClose, cls).init_class_fixtures()
        cls.assets = (
            cls.asset_finder.retrieve_all(cls.asset_finder.equities_sids)
        )
    @classmethod
    def make_equity_info(cls):
        cls.test_days = cls.trading_calendar.sessions_in_range(
            cls.START_DATE, cls.END_DATE,
        )
        assert len(cls.test_days) == 7, "Number of days in test changed!"
        cls.first_asset_expiration = cls.test_days[2]
        cls.asset_info = make_jagged_equity_info(
            num_assets=3,
            start_date=cls.test_days[0],
            first_end=cls.first_asset_expiration,
            frequency=cls.trading_calendar.day,
            periods_between_ends=2,
            auto_close_delta=2 * cls.trading_calendar.day,
        )
        return cls.asset_info
    @classmethod
    def make_equity_daily_bar_data(cls, country_code, sids):
        cls.daily_data = make_trade_data_for_asset_info(
            dates=cls.test_days,
            asset_info=cls.asset_info,
            price_start=10,
            price_step_by_sid=10,
            price_step_by_date=1,
            volume_start=100,
            volume_step_by_sid=100,
            volume_step_by_date=10,
        )
        return cls.daily_data.items()
    def daily_prices_on_tick(self, row):
        return [
            trades.iloc[row].close for trades in itervalues(self.daily_data)
        ]
    def final_daily_price(self, asset):
        return self.daily_data[asset.sid].loc[asset.end_date].close
    def default_initialize(self):
        def initialize(context):
            context.ordered = False
            context.set_commission(PerShare(0, 0))
            context.set_slippage(FixedSlippage(spread=0))
            context.num_positions = []
            context.cash = []
        return initialize
    def default_handle_data(self, assets, order_size):
        def handle_data(context, data):
            if not context.ordered:
                for asset in assets:
                    context.order(asset, order_size)
                context.ordered = True
            context.cash.append(context.portfolio.cash)
            context.num_positions.append(len(context.portfolio.positions))
        return handle_data
    @parameter_space(
        order_size=[10, -10],
        capital_base=[1, 100000],
        __fail_fast=True,
    )
    def test_daily_delisted_equities(self,
                                     order_size,
                                     capital_base):
        assets = self.assets
        final_prices = {
            asset.sid: self.final_daily_price(asset)
            for asset in assets
        }
        initial_fill_prices = self.daily_prices_on_tick(1)
        cost_basis = sum(initial_fill_prices) * order_size
        fp0 = final_prices[0]
        fp1 = final_prices[1]
        algo = self.make_algo(
            initialize=self.default_initialize(),
            handle_data=self.default_handle_data(assets, order_size),
            sim_params=self.make_simparams(
                capital_base=capital_base,
                data_frequency='daily',
            ),
        )
        output = algo.run()
        initial_cash = capital_base
        after_fills = initial_cash - cost_basis
        after_first_auto_close = after_fills + fp0 * (order_size)
        after_second_auto_close = after_first_auto_close + fp1 * (order_size)
        expected_cash = [
            initial_cash,
            after_fills,
            after_fills,
            after_fills,
            after_first_auto_close,
            after_first_auto_close,
            after_second_auto_close,
        ]
        expected_num_positions = [0, 3, 3, 3, 2, 2, 1]
        self.assertEqual(expected_cash, list(output['ending_cash']))
        expected_cash.insert(3, after_fills)
        self.assertEqual(algo.cash, expected_cash[:-1])
        if order_size &gt; 0:
            self.assertEqual(
                expected_num_positions,
                list(output['longs_count']),
            )
            self.assertEqual(
                [0] * len(self.test_days),
                list(output['shorts_count']),
            )
        else:
            self.assertEqual(
                expected_num_positions,
                list(output['shorts_count']),
            )
            self.assertEqual(
                [0] * len(self.test_days),
                list(output['longs_count']),
            )
        expected_num_positions.insert(3, 3)
        self.assertEqual(algo.num_positions, expected_num_positions[:-1])
        transactions = output['transactions']
        initial_fills = transactions.iloc[1]
        self.assertEqual(len(initial_fills), len(assets))
        last_minute_of_session = \
            self.trading_calendar.session_close(self.test_days[1])
        for asset, txn in zip(assets, initial_fills):
            self.assertDictContainsSubset(
                {
                    'amount': order_size,
                    'commission': None,
                    'dt': last_minute_of_session,
                    'price': initial_fill_prices[asset],
                    'sid': asset,
                },
                txn,
            )
            self.assertIsInstance(txn['order_id'], str)
        def transactions_for_date(date):
            return transactions.iloc[self.test_days.get_loc(date)]
        (first_auto_close_transaction,) = transactions_for_date(
            assets[0].auto_close_date
        )
        self.assertEqual(
            first_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': self.trading_calendar.session_close(
                    assets[0].auto_close_date,
                ),
                'price': fp0,
                'sid': assets[0],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )
        (second_auto_close_transaction,) = transactions_for_date(
            assets[1].auto_close_date
        )
        self.assertEqual(
            second_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': self.trading_calendar.session_close(
                    assets[1].auto_close_date,
                ),
                'price': fp1,
                'sid': assets[1],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )
    def test_cancel_open_orders(self):
        assets = self.assets
        first_asset_end_date = assets[0].end_date
        first_asset_auto_close_date = assets[0].auto_close_date
        def initialize(context):
            pass
        def handle_data(context, data):
            assert (
                context.portfolio.cash == context.portfolio.starting_cash
            )
            today_session = self.trading_calendar.minute_to_session_label(
                context.get_datetime()
            )
            day_after_auto_close = self.trading_calendar.next_session_label(
                first_asset_auto_close_date,
            )
            if today_session == first_asset_end_date:
                assert len(context.get_open_orders()) == 0
                context.order(context.sid(0), 10)
                assert len(context.get_open_orders()) == 1
            elif today_session == first_asset_auto_close_date:
                assert len(context.get_open_orders()) == 1
            elif today_session == day_after_auto_close:
                assert len(context.get_open_orders()) == 0
        algo = self.make_algo(
            initialize=initialize,
            handle_data=handle_data,
            sim_params=self.make_simparams(
                data_frequency='daily',
            ),
        )
        results = algo.run()
        orders = results['orders']
        def orders_for_date(date):
            return orders.iloc[self.test_days.get_loc(date)]
        original_open_orders = orders_for_date(first_asset_end_date)
        assert len(original_open_orders) == 1
        last_close_for_asset = \
            algo.trading_calendar.session_close(first_asset_end_date)
        self.assertDictContainsSubset(
            {
                'amount': 10,
                'commission': 0.0,
                'created': last_close_for_asset,
                'dt': last_close_for_asset,
                'sid': assets[0],
                'status': ORDER_STATUS.OPEN,
                'filled': 0,
            },
            original_open_orders[0],
        )
        orders_after_auto_close = orders_for_date(first_asset_auto_close_date)
        assert len(orders_after_auto_close) == 1
        self.assertDictContainsSubset(
            {
                'amount': 10,
                'commission': 0.0,
                'created': last_close_for_asset,
                'dt': algo.trading_calendar.session_close(
                    first_asset_auto_close_date,
                ),
                'sid': assets[0],
                'status': ORDER_STATUS.CANCELLED,
                'filled': 0,
            },
            orders_after_auto_close[0],
        )
class TestMinutelyEquityAutoClose(zf.WithMakeAlgo,
                                  zf.ZiplineTestCase):
    START_DATE = pd.Timestamp('2015-01-05', tz='UTC')
    END_DATE = pd.Timestamp('2015-01-13', tz='UTC')
    BENCHMARK_SID = None
    @classmethod
    def init_class_fixtures(cls):
        super(TestMinutelyEquityAutoClose, cls).init_class_fixtures()
        cls.assets = (
            cls.asset_finder.retrieve_all(cls.asset_finder.equities_sids)
        )
    @classmethod
    def make_equity_info(cls):
        cls.test_days = cls.trading_calendar.sessions_in_range(
            cls.START_DATE, cls.END_DATE,
        )
        cls.test_minutes = cls.trading_calendar.minutes_for_sessions_in_range(
            cls.START_DATE, cls.END_DATE,
        )
        cls.first_asset_expiration = cls.test_days[2]
        cls.asset_info = make_jagged_equity_info(
            num_assets=3,
            start_date=cls.test_days[0],
            first_end=cls.first_asset_expiration,
            frequency=cls.trading_calendar.day,
            periods_between_ends=2,
            auto_close_delta=1 * cls.trading_calendar.day,
        )
        return cls.asset_info
    @classmethod
    def make_equity_minute_bar_data(cls):
        cls.minute_data = make_trade_data_for_asset_info(
            dates=cls.test_minutes,
            asset_info=cls.asset_info,
            price_start=10,
            price_step_by_sid=10,
            price_step_by_date=1,
            volume_start=100,
            volume_step_by_sid=100,
            volume_step_by_date=10,
        )
        return cls.minute_data.items()
    def minute_prices_on_tick(self, row):
        return [
            trades.iloc[row].close for trades in itervalues(self.minute_data)
        ]
    def final_minute_price(self, asset):
        return self.minute_data[asset.sid].loc[
            self.trading_calendar.session_close(asset.end_date)
        ].close
    def default_initialize(self):
        def initialize(context):
            context.ordered = False
            context.set_commission(PerShare(0, 0))
            context.set_slippage(FixedSlippage(spread=0))
            context.num_positions = []
            context.cash = []
        return initialize
    def default_handle_data(self, assets, order_size):
        def handle_data(context, data):
            if not context.ordered:
                for asset in assets:
                    context.order(asset, order_size)
                context.ordered = True
            context.cash.append(context.portfolio.cash)
            context.num_positions.append(len(context.portfolio.positions))
        return handle_data
    def test_minutely_delisted_equities(self):
        assets = self.assets
        final_prices = {
            asset.sid: self.final_minute_price(asset)
            for asset in assets
        }
        backtest_minutes = self.minute_data[0].index.tolist()
        order_size = 10
<a name="30"></a>        capital_base = 100000
        algo = self.make_algo(
            initialize=self.default_initialize(),
            handle_data=self.default_handle_data<font color="#ae694a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(assets, order_size),
            sim_params=self.make_simparams(
                capital_base=capital_base,
                data_frequency='minute',
            )
        )
        output = algo.run()
        initial_fill_prices = self.minute_prices_on_tick(</b></font>1)
        cost_basis = sum(initial_fill_prices) * order_size
        fp0 = final_prices[0]
        fp1 = final_prices[1]
        initial_cash = capital_base
        after_fills = initial_cash - cost_basis
        after_first_auto_close = after_fills + fp0 * (order_size)
        after_second_auto_close = after_first_auto_close + fp1 * (order_size)
        expected_cash = [initial_cash]
        expected_position_counts = [0]
<a name="16"></a>
        expected_cash<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.extend([after_fills] * (389 + 390 + 390 + 390))
        expected_position_counts.extend([3] * (389 + 390 + 390 + 390))
        expected_cash.extend([after_first_auto_close] * (390 + 390))
        expected_position_counts.extend([2] * (390 + 390))
        expected_cash.extend([after_second_auto_close] * 390)
        expected_position_counts.extend([1] * 390)
        self.assertEqual(</b></font>len(algo.cash), len(expected_cash))
        self.assertEqual(algo.cash, expected_cash)
        self.assertEqual(
            list(output['ending_cash']),
            [
                after_fills,
                after_fills,
                after_fills,
                after_first_auto_close,
                after_first_auto_close,
                after_second_auto_close,
                after_second_auto_close,
            ],
        )
        self.assertEqual(algo.num_positions, expected_position_counts)
        self.assertEqual(
            list(output['longs_count']),
            [3, 3, 3, 2, 2, 1, 1],
        )
        transactions = output['transactions']
        initial_fills = transactions.iloc[0]
        self.assertEqual(len(initial_fills), len(assets))
        for asset, txn in zip(assets, initial_fills):
            self.assertDictContainsSubset(
                {
                    'amount': order_size,
                    'commission': None,
                    'dt': backtest_minutes[1],
                    'price': initial_fill_prices[asset],
                    'sid': asset,
                },
                txn,
            )
            self.assertIsInstance(txn['order_id'], str)
        def transactions_for_date(date):
            return transactions.iloc[self.test_days.get_loc(date)]
        (first_auto_close_transaction,) = transactions_for_date(
            assets[0].auto_close_date
        )
        self.assertEqual(
            first_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': algo.trading_calendar.session_close(
                    assets[0].auto_close_date,
                ),
                'price': fp0,
                'sid': assets[0],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )
        (second_auto_close_transaction,) = transactions_for_date(
            assets[1].auto_close_date
        )
        self.assertEqual(
            second_auto_close_transaction,
            {
                'amount': -order_size,
                'commission': None,
                'dt': algo.trading_calendar.session_close(
                    assets[1].auto_close_date,
                ),
                'price': fp1,
                'sid': assets[1],
                'order_id': None,  # Auto-close txns emit Nones for order_id.
            },
        )
class TestOrderAfterDelist(zf.WithMakeAlgo, zf.ZiplineTestCase):
    start = pd.Timestamp('2016-01-05', tz='utc')
    day_1 = pd.Timestamp('2016-01-06', tz='utc')
    day_4 = pd.Timestamp('2016-01-11', tz='utc')
    end = pd.Timestamp('2016-01-15', tz='utc')
    BENCHMARK_SID = None
<a name="29"></a>
    @classmethod
    def make_equity_info(cls):
        return pd<font color="#af7a82"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.DataFrame.from_dict(
            {
                1: {
                    'start_date': cls.start,
                    'end_date': cls.day_1,
                    'auto_close_date': cls.day_4,
                    'symbol': "ASSET1",
                    'exchange': "TEST",
                },
                2: {
                    'start_date': cls.start,
                    'end_date': cls.day_4,
                    'auto_close_date': cls.</b></font>day_1,
                    'symbol': 'ASSET2',
                    'exchange': 'TEST',
                },
            },
            orient='index',
        )
    def init_instance_fixtures(self):
        super(TestOrderAfterDelist, self).init_instance_fixtures()
        self.data_portal = FakeDataPortal(self.asset_finder)
    @parameterized.expand([
        ('auto_close_after_end_date', 1),
        ('auto_close_before_end_date', 2),
    ])
    def test_order_in_quiet_period(self, name, sid):
        asset = self.asset_finder.retrieve_asset(sid)
        algo_code = dedent("""
        from zipline.api import (
            sid,
            order,
            order_value,
            order_percent,
            order_target,
            order_target_percent,
            order_target_value
        )
        def initialize(context):
            pass
        def handle_data(context, data):
            order(sid({sid}), 1)
            order_value(sid({sid}), 100)
            order_percent(sid({sid}), 0.5)
            order_target(sid({sid}), 50)
            order_target_percent(sid({sid}), 0.5)
            order_target_value(sid({sid}), 50)
            def initialize(context):
                pass
            def handle_data(context, data):
                pass
            def before_trading_start(context, data):
                pass
            def analyze(context, results):
                pass
<a name="1"></a><font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import division
from datetime import timedelta
from functools import partial
import blaze as bz
import itertools
from nose.tools import assert_true
from nose_parameterized import parameterized
import numpy as np
from numpy.testing import assert_array_equal, assert_almost_equal
import pandas as pd
from toolz import merge
from zipline.pipeline import SimplePipelineEngine, Pipeline, CustomFactor
from zipline.pipeline.common import (
    EVENT_DATE_FIELD_NAME,
    FISCAL_QUARTER_FIELD_NAME,
    FISCAL_YEAR_FIELD_NAME,
    SID_FIELD_NAME,
    TS_FIELD_NAME,
)
from zipline.pipeline.data import DataSet
from zipline.pipeline.data import Column
from zipline.pipeline.domain import EquitySessionDomain
from zipline.pipeline.loaders.blaze.estimates import (
    BlazeNextEstimatesLoader,
    BlazeNextSplitAdjustedEstimatesLoader,
    BlazePreviousEstimatesLoader,
    BlazePreviousSplitAdjustedEstimatesLoader,
)
from zipline.pipeline.loaders.earnings_estimates import (
    INVALID_NUM_QTRS_MESSAGE,
    NextEarningsEstimatesLoader,
    NextSplitAdjustedEarningsEstimatesLoader,
    normalize_quarters,
    PreviousEarningsEstimatesLoader,
    PreviousSplitAdjustedEarningsEstimatesLoader,
    split_normalized_quarters,
)
from zipline.testing.fixtures import (
    WithAdjustmentReader,
    WithTradingSessions,
    ZiplineTestCase,
)
from zipline.testing.predicates import assert_equal, assert_raises_regex
from zipline.testing.predicates import assert_frame_equal
from zipline.utils.numpy_utils import datetime64ns_dtype
from</b></font> zipline.utils.numpy_utils import float64_dtype
class Estimates(DataSet):
    event_date = Column(dtype=datetime64ns_dtype)
    fiscal_quarter = Column(dtype=float64_dtype)
    fiscal_year = Column(dtype=float64_dtype)
    estimate = Column(dtype=float64_dtype)
class MultipleColumnsEstimates(DataSet):
    event_date = Column(dtype=datetime64ns_dtype)
    fiscal_quarter = Column(dtype=float64_dtype)
    fiscal_year = Column(dtype=float64_dtype)
    estimate1 = Column(dtype=float64_dtype)
    estimate2 = Column(dtype=float64_dtype)
def QuartersEstimates(announcements_out):
    class QtrEstimates(Estimates):
        num_announcements = announcements_out
        name = Estimates
    return QtrEstimates
def MultipleColumnsQuartersEstimates(announcements_out):
    class QtrEstimates(MultipleColumnsEstimates):
        num_announcements = announcements_out
        name = Estimates
    return QtrEstimates
def QuartersEstimatesNoNumQuartersAttr(num_qtr):
    class QtrEstimates(Estimates):
        name = Estimates
    return QtrEstimates
def create_expected_df_for_factor_compute(start_date,
                                          sids,
                                          tuples,
                                          end_date):
    df = pd.DataFrame(tuples,
<a name="31"></a>                      columns=[SID_FIELD_NAME,
                               'estimate',
                               'knowledge_date'])
    df <font color="#3ea99f"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= df.pivot_table(columns=SID_FIELD_NAME,
                        values='estimate',
                        index='knowledge_date')
    df = df.reindex(
        pd.date_range(start_date, end_date)
    )
    df.</b></font>index = df.index.rename('knowledge_date')
    df['at_date'] = end_date.tz_localize('utc')
    df = df.set_index(['at_date', df.index.tz_localize('utc')]).ffill()
    new_sids = set(sids) - set(df.columns)
    df = df.reindex(columns=df.columns.union(new_sids))
    return df
class WithEstimates(WithTradingSessions, WithAdjustmentReader):
    START_DATE = pd.Timestamp('2014-12-28')
    END_DATE = pd.Timestamp('2015-02-04')
    @classmethod
    def make_loader(cls, events, columns):
        raise NotImplementedError('make_loader')
    @classmethod
    def make_events(cls):
        raise NotImplementedError('make_events')
    @classmethod
    def get_sids(cls):
        return cls.events[SID_FIELD_NAME].unique()
    @classmethod
    def make_columns(cls):
        return {
            Estimates.event_date: 'event_date',
            Estimates.fiscal_quarter: 'fiscal_quarter',
            Estimates.fiscal_year: 'fiscal_year',
            Estimates.estimate: 'estimate'
        }
    def make_engine(self, loader=None):
        if loader is None:
            loader = self.loader
        return SimplePipelineEngine(
            lambda x: loader,
            self.asset_finder,
            default_domain=EquitySessionDomain(
                self.trading_days, self.ASSET_FINDER_COUNTRY_CODE,
            ),
        )
    @classmethod
    def init_class_fixtures(cls):
        cls.events = cls.make_events()
        cls.ASSET_FINDER_EQUITY_SIDS = cls.get_sids()
        cls.ASSET_FINDER_EQUITY_SYMBOLS = [
            's' + str(n) for n in cls.ASSET_FINDER_EQUITY_SIDS
        ]
        super(WithEstimates, cls).init_class_fixtures()
        cls.columns = cls.make_columns()
        cls.loader = cls.make_loader(cls.events, {column.name: val for
                                                  column, val in
                                                  cls.columns.items()})
class WithOneDayPipeline(WithEstimates):
    @classmethod
    def make_columns(cls):
        return {
            MultipleColumnsEstimates.event_date: 'event_date',
            MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',
            MultipleColumnsEstimates.fiscal_year: 'fiscal_year',
            MultipleColumnsEstimates.estimate1: 'estimate1',
            MultipleColumnsEstimates.estimate2: 'estimate2'
        }
    @classmethod
    def make_events(cls):
        return pd.DataFrame({
            SID_FIELD_NAME: [0] * 2,
            TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),
                            pd.Timestamp('2015-01-06')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),
                                    pd.Timestamp('2015-01-20')],
            'estimate1': [1., 2.],
            'estimate2': [3., 4.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: [2015, 2015]
        })
    @classmethod
    def make_expected_out(cls):
        raise NotImplementedError('make_expected_out')
    @classmethod
    def init_class_fixtures(cls):
        super(WithOneDayPipeline, cls).init_class_fixtures()
        cls.sid0 = cls.asset_finder.retrieve_asset(0)
        cls.expected_out = cls.make_expected_out()
    def test_load_one_day(self):
<a name="25"></a>        dataset = MultipleColumnsQuartersEstimates(1)
        engine = self.make_engine()
        results = engine.run_pipeline(
            Pipeline<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>({c.name: c.latest for c in dataset.columns}),
            start_date=pd.Timestamp('2015-01-15', tz='utc'),
            end_date=pd.Timestamp('2015-01-15', tz=</b></font>'utc'),
        )
        assert_frame_equal(results, self.expected_out)
class PreviousWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)
    @classmethod
    def make_expected_out(cls):
        return pd.DataFrame(
            {
                EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-10'),
                'estimate1': 1.,
                'estimate2': 3.,
                FISCAL_QUARTER_FIELD_NAME: 1.,
                FISCAL_YEAR_FIELD_NAME: 2015.,
            },
            index=pd.MultiIndex.from_tuples(
                ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)
            )
        )
class NextWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)
    @classmethod
    def make_expected_out(cls):
        return pd.DataFrame(
            {
                EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-20'),
                'estimate1': 2.,
                'estimate2': 4.,
                FISCAL_QUARTER_FIELD_NAME: 2.,
                FISCAL_YEAR_FIELD_NAME: 2015.,
            },
            index=pd.MultiIndex.from_tuples(
                ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)
            )
        )
dummy_df = pd.DataFrame({SID_FIELD_NAME: 0},
                        columns=[SID_FIELD_NAME,
                                 TS_FIELD_NAME,
                                 EVENT_DATE_FIELD_NAME,
                                 FISCAL_QUARTER_FIELD_NAME,
                                 FISCAL_YEAR_FIELD_NAME,
                                 'estimate'],
                        index=[0])
class WithWrongLoaderDefinition(WithEstimates):
    @classmethod
    def make_events(cls):
        return dummy_df
    def test_wrong_num_announcements_passed(self):
        bad_dataset1 = QuartersEstimates(-1)
        bad_dataset2 = QuartersEstimates(-2)
        good_dataset = QuartersEstimates(1)
        engine = self.make_engine()
        columns = {c.name + str(dataset.num_announcements): c.latest
                   for dataset in (bad_dataset1,
                                   bad_dataset2,
                                   good_dataset)
                   for c in dataset.columns}
        p = Pipeline(columns)
        with self.assertRaises(ValueError) as e:
            engine.run_pipeline(
                p,
                start_date=self.trading_days[0],
                end_date=self.trading_days[-1],
            )
            assert_raises_regex(e, INVALID_NUM_QTRS_MESSAGE % "-1,-2")
    def test_no_num_announcements_attr(self):
        dataset = QuartersEstimatesNoNumQuartersAttr(1)
        engine = self.make_engine()
        p = Pipeline({c.name: c.latest for c in dataset.columns})
        with self.assertRaises(AttributeError):
            engine.run_pipeline(
                p,
                start_date=self.trading_days[0],
                end_date=self.trading_days[-1],
            )
class PreviousWithWrongNumQuarters(WithWrongLoaderDefinition,
                                   ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)
class NextWithWrongNumQuarters(WithWrongLoaderDefinition,
                               ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)
options = ["split_adjustments_loader",
           "split_adjusted_column_names",
           "split_adjusted_asof"]
class WrongSplitsLoaderDefinition(WithEstimates, ZiplineTestCase):
    @classmethod
    def init_class_fixtures(cls):
        super(WithEstimates, cls).init_class_fixtures()
    @parameterized.expand(itertools.product(
        (NextSplitAdjustedEarningsEstimatesLoader,
         PreviousSplitAdjustedEarningsEstimatesLoader),
    ))
    def test_extra_splits_columns_passed(self, loader):
        columns = {
            Estimates.event_date: 'event_date',
            Estimates.fiscal_quarter: 'fiscal_quarter',
            Estimates.fiscal_year: 'fiscal_year',
            Estimates.estimate: 'estimate'
        }
        with self.assertRaises(ValueError):
            loader(dummy_df,
                   {column.name: val for column, val in
                    columns.items()},
                   split_adjustments_loader=self.adjustment_reader,
                   split_adjusted_column_names=["estimate", "extra_col"],
                   split_adjusted_asof=pd.Timestamp("2015-01-01"))
class WithEstimatesTimeZero(WithEstimates):
    END_DATE = pd.Timestamp('2015-01-28')
    q1_knowledge_dates = [pd.Timestamp('2015-01-01'),
                          pd.Timestamp('2015-01-04'),
                          pd.Timestamp('2015-01-07'),
                          pd.Timestamp('2015-01-11')]
    q2_knowledge_dates = [pd.Timestamp('2015-01-14'),
                          pd.Timestamp('2015-01-17'),
                          pd.Timestamp('2015-01-20'),
                          pd.Timestamp('2015-01-23')]
    q1_release_dates = [pd.Timestamp('2015-01-13'),
                        pd.Timestamp('2015-01-14')]  # One day late
    q2_release_dates = [pd.Timestamp('2015-01-25'),  # One day early
                        pd.Timestamp('2015-01-26')]
    @classmethod
    def make_events(cls):
        sid_estimates = []
        sid_releases = []
        it = enumerate(
            itertools.permutations(cls.q1_knowledge_dates +
                                   cls.q2_knowledge_dates,
                                   4)
        )
        for sid, (q1e1, q1e2, q2e1, q2e2) in it:
            if (q1e1 &lt; q1e2 and
                    q2e1 &lt; q2e2 and
                    q1e1 &lt; cls.q1_release_dates[0] and
                    q1e2 &lt; cls.q1_release_dates[0]):
                sid_estimates.append(cls.create_estimates_df(q1e1,
                                                             q1e2,
                                                             q2e1,
                                                             q2e2,
                                                             sid))
                sid_releases.append(cls.create_releases_df(sid))
        return pd.concat(sid_estimates +
                         sid_releases).reset_index(drop=True)
    @classmethod
    def get_sids(cls):
        sids = cls.events[SID_FIELD_NAME].unique()
        return list(sids) + [max(sids) + 1]
    @classmethod
    def create_releases_df(cls, sid):
        return pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-13'),
                            pd.Timestamp('2015-01-26')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-13'),
                                    pd.Timestamp('2015-01-26')],
            'estimate': [0.5, 0.8],
            FISCAL_QUARTER_FIELD_NAME: [1.0, 2.0],
            FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0],
            SID_FIELD_NAME: sid
        })
    @classmethod
    def create_estimates_df(cls,
                            q1e1,
                            q1e2,
                            q2e1,
                            q2e2,
                            sid):
        return pd.DataFrame({
            EVENT_DATE_FIELD_NAME: cls.q1_release_dates + cls.q2_release_dates,
            'estimate': [.1, .2, .3, .4],
            FISCAL_QUARTER_FIELD_NAME: [1.0, 1.0, 2.0, 2.0],
            FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0, 2015.0, 2015.0],
            TS_FIELD_NAME: [q1e1, q1e2, q2e1, q2e2],
            SID_FIELD_NAME: sid,
        })
    def get_expected_estimate(self,
                              q1_knowledge,
                              q2_knowledge,
                              comparable_date):
        return pd.DataFrame()
    def test_estimates(self):
        dataset = QuartersEstimates(1)
        engine = self.make_engine()
        results = engine.run_pipeline(
            Pipeline({c.name: c.latest for c in dataset.columns}),
            start_date=self.trading_days[1],
            end_date=self.trading_days[-2],
        )
        for sid in self.ASSET_FINDER_EQUITY_SIDS:
            sid_estimates = results.xs(sid, level=1)
            if sid == max(self.ASSET_FINDER_EQUITY_SIDS):
                assert_true(sid_estimates.isnull().all().all())
            else:
                ts_sorted_estimates = self.events[
                    self.events[SID_FIELD_NAME] == sid
                ].sort_values(TS_FIELD_NAME)
                q1_knowledge = ts_sorted_estimates[
                    ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 1
                ]
                q2_knowledge = ts_sorted_estimates[
<a name="26"></a>                    ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 2
                ]
                all_expected = pd.concat(
                    [self<font color="#68818b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get_expected_estimate(
                        q1_knowledge[q1_knowledge[TS_FIELD_NAME] &lt;=
                                     date.tz_localize(None)],
                        q2_knowledge[q2_knowledge[TS_FIELD_NAME] &lt;=
                                     date.tz_localize(None)],
                        date.tz_localize(None),
                    ).</b></font>set_index([[date]]) for date in sid_estimates.index],
                    axis=0)
                assert_equal(all_expected[sid_estimates.columns],
                             sid_estimates)
class NextEstimate(WithEstimatesTimeZero, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)
    def get_expected_estimate(self,
                              q1_knowledge,
                              q2_knowledge,
                              comparable_date):
        if (not q1_knowledge.empty and
            q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &gt;=
                comparable_date):
            return q1_knowledge.iloc[-1:]
        elif (not q2_knowledge.empty and
              q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &gt;=
                comparable_date):
            return q2_knowledge.iloc[-1:]
        return pd.DataFrame(columns=q1_knowledge.columns,
                            index=[comparable_date])
class BlazeNextEstimateLoaderTestCase(NextEstimate):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(
            bz.data(events),
            columns,
        )
class PreviousEstimate(WithEstimatesTimeZero, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)
    def get_expected_estimate(self,
                              q1_knowledge,
                              q2_knowledge,
                              comparable_date):
        if (not q2_knowledge.empty and
            q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &lt;=
                comparable_date):
            return q2_knowledge.iloc[-1:]
        elif (not q1_knowledge.empty and
              q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &lt;=
                comparable_date):
            return q1_knowledge.iloc[-1:]
        return pd.DataFrame(columns=q1_knowledge.columns,
                            index=[comparable_date])
class BlazePreviousEstimateLoaderTestCase(PreviousEstimate):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(
            bz.data(events),
            columns,
        )
class WithEstimateMultipleQuarters(WithEstimates):
    @classmethod
    def make_events(cls):
        return pd.DataFrame({
            SID_FIELD_NAME: [0] * 2,
            TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),
                            pd.Timestamp('2015-01-06')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),
                                    pd.Timestamp('2015-01-20')],
            'estimate': [1., 2.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: [2015, 2015]
        })
    @classmethod
    def init_class_fixtures(cls):
<a name="17"></a>        super(WithEstimateMultipleQuarters, cls).init_class_fixtures()
        cls.expected_out = cls.make_expected_out()
    <font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>@classmethod
    def make_expected_out(cls):
        expected = pd.DataFrame(columns=[cls.columns[col] + '1'
                                         for col in cls.columns] +
                                        [cls.columns[col] + '2'
                                         for col in cls.columns],
                                index=cls.</b></font>trading_days)
        for (col, raw_name), suffix in itertools.product(
            cls.columns.items(), ('1', '2')
        ):
            expected_name = raw_name + suffix
            if col.dtype == datetime64ns_dtype:
                expected[expected_name] = pd.to_datetime(
                    expected[expected_name]
                )
            else:
                expected[expected_name] = expected[
                    expected_name
                ].astype(col.dtype)
        cls.fill_expected_out(expected)
        return expected.reindex(cls.trading_days)
    def test_multiple_qtrs_requested(self):
        dataset1 = QuartersEstimates(1)
        dataset2 = QuartersEstimates(2)
        engine = self.make_engine()
<a name="33"></a>        results = engine.run_pipeline(
            Pipeline(
                merge([{c.name + '1': c.latest for c in dataset1.columns},
                       {c.name + '2': c<font color="#736aff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.latest for c in dataset2.columns}])
            ),
            start_date=self.trading_days[0],
            end_date=self.trading_days[-1],
        )
        q1_columns = [col.name + '1' for col in self.columns]
        q2_columns =</b></font> [col.name + '2' for col in self.columns]
        assert_equal(sorted(np.array(q1_columns + q2_columns)),
                     sorted(results.columns.values))
        assert_equal(self.expected_out.sort_index(axis=1),
                     results.xs(0, level=1).sort_index(axis=1))
class NextEstimateMultipleQuarters(
    WithEstimateMultipleQuarters, ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)
    @classmethod
    def fill_expected_out(cls, expected):
        for raw_name in cls.columns.values():
            expected.loc[
                pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-11'),
                raw_name + '1'
            ] = cls.events[raw_name].iloc[0]
            expected.loc[
                pd.Timestamp('2015-01-11'):pd.Timestamp('2015-01-20'),
                raw_name + '1'
            ] = cls.events[raw_name].iloc[1]
        for col_name in ['estimate', 'event_date']:
            expected.loc[
                pd.Timestamp('2015-01-06'):pd.Timestamp('2015-01-10'),
                col_name + '2'
            ] = cls.events[col_name].iloc[1]
        expected.loc[
            pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-09'),
            FISCAL_QUARTER_FIELD_NAME + '2'
        ] = 2
        expected.loc[
            pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20'),
            FISCAL_QUARTER_FIELD_NAME + '2'
        ] = 3
        expected.loc[
            pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-20'),
            FISCAL_YEAR_FIELD_NAME + '2'
        ] = 2015
        return expected
class BlazeNextEstimateMultipleQuarters(NextEstimateMultipleQuarters):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(
            bz.data(events),
            columns,
        )
class PreviousEstimateMultipleQuarters(
    WithEstimateMultipleQuarters,
    ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)
    @classmethod
    def fill_expected_out(cls, expected):
        for raw_name in cls.columns.values():
            expected[raw_name + '1'].loc[
                pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-19')
            ] = cls.events[raw_name].iloc[0]
            expected[raw_name + '1'].loc[
                pd.Timestamp('2015-01-20'):
            ] = cls.events[raw_name].iloc[1]
        for col_name in ['estimate', 'event_date']:
            expected[col_name + '2'].loc[
                pd.Timestamp('2015-01-20'):
            ] = cls.events[col_name].iloc[0]
        expected[
            FISCAL_QUARTER_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 4
        expected[
            FISCAL_YEAR_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 2014
        expected[
            FISCAL_QUARTER_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-20'):] = 1
        expected[
            FISCAL_YEAR_FIELD_NAME + '2'
        ].loc[pd.Timestamp('2015-01-20'):] = 2015
        return expected
class BlazePreviousEstimateMultipleQuarters(PreviousEstimateMultipleQuarters):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(
            bz.data(events),
            columns,
        )
class WithVaryingNumEstimates(WithEstimates):
    @classmethod
    def make_events(cls):
<a name="27"></a>        return pd.DataFrame({
            SID_FIELD_NAME: [0] * 3 + [1] * 3,
            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                            pd<font color="#e77471"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-12'),
                            pd.Timestamp('2015-01-13')] * 2,
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-12'),
                                    pd.Timestamp('2015-01-13'),
                                    pd.Timestamp('2015-01-20'),
                                    pd.Timestamp(</b></font>'2015-01-13'),
                                    pd.Timestamp('2015-01-12'),
                                    pd.Timestamp('2015-01-20')],
            'estimate': [11., 12., 21.] * 2,
            FISCAL_QUARTER_FIELD_NAME: [1, 1, 2] * 2,
            FISCAL_YEAR_FIELD_NAME: [2015] * 6
        })
    @classmethod
    def assert_compute(cls, estimate, today):
        raise NotImplementedError('assert_compute')
    def test_windows_with_varying_num_estimates(self):
        dataset = QuartersEstimates(1)
        assert_compute = self.assert_compute
        class SomeFactor(CustomFactor):
            inputs = [dataset.estimate]
            window_length = 3
            def compute(self, today, assets, out, estimate):
                assert_compute(estimate, today)
        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est': SomeFactor()}),
            start_date=pd.Timestamp('2015-01-13', tz='utc'),
            end_date=pd.Timestamp('2015-01-14', tz='utc'),
        )
class PreviousVaryingNumEstimates(
    WithVaryingNumEstimates,
    ZiplineTestCase
):
<a name="20"></a>    def assert_compute(self, estimate, today):
        if today == pd.Timestamp('2015-01-13', tz='utc'):
            assert_array_equal(estimate[:, 0],
                               np.array([np.NaN, np<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, 12]))
            assert_array_equal(estimate[:, 1],
                               np.array([np.NaN, 12, 12]))
        else:
            assert_array_equal(estimate[:, 0],
                               np.array([np.NaN, 12, 12]))
            assert_array_equal(</b></font>estimate[:, 1],
                               np.array([12, 12, 12]))
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)
class BlazePreviousVaryingNumEstimates(PreviousVaryingNumEstimates):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(
            bz.data(events),
            columns,
        )
class NextVaryingNumEstimates(
    WithVaryingNumEstimates,
    ZiplineTestCase
):
    def assert_compute(self, estimate, today):
        if today == pd.Timestamp('2015-01-13', tz='utc'):
            assert_array_equal(estimate[:, 0],
                               np.array([11, 12, 12]))
            assert_array_equal(estimate[:, 1],
                               np.array([np.NaN, np.NaN, 21]))
        else:
            assert_array_equal(estimate[:, 0],
                               np.array([np.NaN, 21, 21]))
            assert_array_equal(estimate[:, 1],
                               np.array([np.NaN, 21, 21]))
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)
class BlazeNextVaryingNumEstimates(NextVaryingNumEstimates):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(
            bz.data(events),
            columns,
        )
class WithEstimateWindows(WithEstimates):
    END_DATE <font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= pd.Timestamp('2015-02-10')
    window_test_start_date = pd.Timestamp('2015-01-05')
    critical_dates = [pd.Timestamp('2015-01-09', tz='utc'),
                      pd.Timestamp('2015-01-15', tz='utc'),
                      pd.Timestamp('2015-01-20', tz='utc'),
                      pd.</b></font>Timestamp('2015-01-26', tz='utc'),
                      pd.Timestamp('2015-02-05', tz='utc'),
                      pd.Timestamp('2015-02-10', tz='utc')]
    window_test_cases = list(itertools.product(critical_dates, (1, 2)))
    @classmethod
    def make_events(cls):
        sid_0_timeline = pd.DataFrame({
            TS_FIELD_NAME: [cls.window_test_start_date,
                            pd.Timestamp('2015-01-20'),
                            pd.Timestamp('2015-01-12'),
                            pd.Timestamp('2015-02-10'),
                            pd.Timestamp('2015-01-18')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-02-10'),
                 pd.Timestamp('2015-02-10'),
                 pd.Timestamp('2015-04-01')],
            'estimate': [100., 101.] + [200., 201.] + [400],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2 + [4],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 0,
        })
        sid_10_timeline = pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-12'),
                            pd.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-15')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-22'), pd.Timestamp('2015-01-22'),
                 pd.Timestamp('2015-02-05'), pd.Timestamp('2015-02-05')],
            'estimate': [110., 111.] + [310., 311.],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [3] * 2,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 10
        })
        sid_20_timeline = pd.DataFrame({
            TS_FIELD_NAME: [cls.window_test_start_date,
                            pd.Timestamp('2015-01-07'),
                            cls.window_test_start_date,
                            pd.Timestamp('2015-01-17')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-02-10'),
                 pd.Timestamp('2015-02-10')],
            'estimate': [120., 121.] + [220., 221.],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 20
        })
        concatted = pd.concat([sid_0_timeline,
                               sid_10_timeline,
                               sid_20_timeline]).reset_index()
        np.random.seed(0)
        return concatted.reindex(np.random.permutation(concatted.index))
    @classmethod
    def get_sids(cls):
        sids = sorted(cls.events[SID_FIELD_NAME].unique())
        return [sid for i in range(len(sids) - 1)
                for sid in range(sids[i], sids[i+1])] + [sids[-1]]
    @classmethod
    def make_expected_timelines(cls):
        return {}
    @classmethod
    def init_class_fixtures(cls):
        super(WithEstimateWindows, cls).init_class_fixtures()
        cls.create_expected_df_for_factor_compute = partial(
            create_expected_df_for_factor_compute,
            cls.window_test_start_date,
            cls.get_sids()
        )
        cls.timelines = cls.make_expected_timelines()
    @parameterized.expand(window_test_cases)
    def test_estimate_windows_at_quarter_boundaries(self,
                                                    start_date,
                                                    num_announcements_out):
        dataset = QuartersEstimates(num_announcements_out)
        trading_days = self.trading_days
        timelines = self.timelines
        window_len = (
            self.trading_days.get_loc(start_date) -
            self.trading_days.get_loc(self.window_test_start_date) + 1
        )
        class SomeFactor(CustomFactor):
            inputs = [dataset.estimate]
            window_length = window_len
            def compute(self, today, assets, out, estimate):
                today_idx = trading_days.get_loc(today)
                today_timeline = timelines[
                    num_announcements_out
                ].loc[today].reindex(
                    trading_days[:today_idx + 1]
                ).values
                timeline_start_idx = (len(today_timeline) - window_len)
                assert_almost_equal(estimate,
                                    today_timeline[timeline_start_idx:])
        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est': SomeFactor()}),
            start_date=start_date,
            end_date=pd.Timestamp('2015-02-10', tz='utc'),
        )
class PreviousEstimateWindows(WithEstimateWindows, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousEarningsEstimatesLoader(events, columns)
    @classmethod
<a name="29"></a>    def make_expected_timelines(cls):
        oneq_previous = pd.concat([
            pd.concat([
                <font color="#af7a82"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>cls.create_expected_df_for_factor_compute([
                    (0, np.NaN, cls.window_test_start_date),
                    (10, np.NaN, cls.window_test_start_date),
                    (20, np.</b></font>NaN, cls.window_test_start_date)
                ], end_date)
                for end_date in pd.date_range('2015-01-09', '2015-01-19')
            ]),
<a name="21"></a>            cls.create_expected_df_for_factor_compute(
                [(0, 101, pd.Timestamp('2015-01-20')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 121, pd.Timestamp<font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>('2015-01-20'))],
                pd.Timestamp('2015-01-20')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 101, pd.Timestamp('2015-01-20')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20</b></font>, 121, pd.Timestamp('2015-01-20'))],
                pd.Timestamp('2015-01-21')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp('2015-01-20')),
                     (10, 111, pd.Timestamp('2015-01-22')),
                     (20, 121, pd.Timestamp('2015-01-20'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-22', '2015-02-04')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp('2015-01-20')),
                     (10, 311, pd.Timestamp('2015-02-05')),
                     (20, 121, pd.Timestamp('2015-01-20'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-05', '2015-02-09')
                ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 201, pd.Timestamp('2015-02-10')),
                 (10, 311, pd.Timestamp('2015-02-05')),
                 (20, 221, pd.Timestamp('2015-02-10'))],
                pd.Timestamp('2015-02-10')
            ),
        ])
        twoq_previous = pd.concat(
<a name="4"></a>            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-09', '2015-02-09')] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 101, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, pd.Timestamp('2015-02-05')),
                 (20, 121, pd.Timestamp('2015-02-10'))],
                pd.</b></font>Timestamp('2015-02-10')
            )]
        )
        return {
            1: oneq_previous,
            2: twoq_previous
        }
class BlazePreviousEstimateWindows(PreviousEstimateWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousEstimatesLoader(bz.data(events), columns)
class NextEstimateWindows(WithEstimateWindows, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextEarningsEstimatesLoader(events, columns)
    @classmethod
    def make_expected_timelines(cls):
        oneq_next = pd.concat([
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07'))],
                pd.Timestamp('2015-01-09')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 100, cls.window_test_start_date),
                     (10, 110, pd.Timestamp('2015-01-09')),
                     (10, 111, pd.Timestamp('2015-01-12')),
                     (20, 120, cls.window_test_start_date),
                     (20, 121, pd.Timestamp('2015-01-07'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-12', '2015-01-19')
            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (0, 101, pd.Timestamp('2015-01-20')),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07'))],
                pd.Timestamp('2015-01-20')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200, pd.Timestamp('2015-01-12')),
                     (10, 110, pd.Timestamp('2015-01-09')),
                     (10, 111, pd.Timestamp('2015-01-12')),
                     (20, 220, cls.window_test_start_date),
                     (20, 221, pd.Timestamp('2015-01-17'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-21', '2015-01-22')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200, pd.Timestamp('2015-01-12')),
                     (10, 310, pd.Timestamp('2015-01-09')),
                     (10, 311, pd.Timestamp('2015-01-15')),
                     (20, 220, cls.window_test_start_date),
                     (20, 221, pd.Timestamp('2015-01-17'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-23', '2015-02-05')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200, pd.Timestamp('2015-01-12')),
                     (10, np.NaN, cls.window_test_start_date),
                     (20, 220, cls.window_test_start_date),
                     (20, 221, pd.Timestamp('2015-01-17'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-06', '2015-02-09')
            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (0, 201, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.window_test_start_date),
                 (20, 221, pd.Timestamp('2015-01-17'))],
                pd.Timestamp('2015-02-10')
            )
        ])
        twoq_next = pd.concat(
            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
<a name="13"></a>                 (20, 220, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-09', '2015-01-11')] +
            [<font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>cls.create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-12', '2015-01-16')] +
            [cls.</b></font>create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.window_test_start_date),
                 (20, 221, pd.Timestamp('2015-01-17'))],
                pd.Timestamp('2015-01-20')
            )] +
            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]
        )
        return {
            1: oneq_next,
            2: twoq_next
        }
class BlazeNextEstimateWindows(NextEstimateWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextEstimatesLoader(bz.data(events), columns)
class WithSplitAdjustedWindows(WithEstimateWindows):
    split_adjusted_asof_date = pd.Timestamp('2015-01-14')
    @classmethod
    def make_events(cls):
        sid_30 = pd.DataFrame({
            TS_FIELD_NAME: [cls.window_test_start_date,
                            pd.Timestamp('2015-01-09'),
                            cls.window_test_start_date,
                            pd.Timestamp('2015-01-20')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-09'),
                 pd.Timestamp('2015-01-09'),
                 pd.Timestamp('2015-01-20'),
                 pd.Timestamp('2015-01-20')],
            'estimate': [130., 131., 230., 231.],
            FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 30
        })
        sid_40 = pd.DataFrame({
            TS_FIELD_NAME: [pd<font color="#5b8daf"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-15')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                                    pd.Timestamp('2015-02-10')],
            'estimate': [140., 240.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 40
        })
        sid_50 =</b></font> pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                            pd.Timestamp('2015-01-12')],
            EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),
                                    pd.Timestamp('2015-02-10')],
            'estimate': [150., 250.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 50
        })
        return pd.concat([
            cls.__base__.make_events(),
            sid_30,
            sid_40,
            sid_50,
        ])
    @classmethod
    def make_splits_data(cls):
<a name="16"></a>        sid_0_splits = pd.DataFrame({
            SID_FIELD_NAME: 0,
            'ratio': (-1., 2., 3., 4., 5., 6., 7., 100),
            'effective_date': (pd<font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2014-01-01'),  # Filter out
                               pd.Timestamp('2015-01-07'),
                               pd.Timestamp('2015-01-09'),
                               pd.Timestamp('2015-01-13'),
                               pd.Timestamp('2015-01-15'),
                               pd.Timestamp('2015-01-18'),
                               pd.Timestamp(</b></font>'2015-01-30'),
                               pd.Timestamp('2016-01-01'))
        })
        sid_10_splits = pd.DataFrame({
            SID_FIELD_NAME: 10,
            'ratio': (.2, .3),
            'effective_date': (
                pd.Timestamp('2015-01-07'),
                pd.Timestamp('2015-01-20')),
        })
        sid_20_splits = pd.DataFrame({
            SID_FIELD_NAME: 20,
            'ratio': (.4, .5, .6, .7, .8, .9,),
            'effective_date': (
                pd.Timestamp('2015-01-07'),
                pd.Timestamp('2015-01-09'),
                pd.Timestamp('2015-01-13'),
                pd.Timestamp('2015-01-15'),
                pd.Timestamp('2015-01-18'),
                pd.Timestamp('2015-01-30')),
        })
        sid_30_splits = pd.DataFrame({
            SID_FIELD_NAME: 30,
            'ratio': (8, 9, 10, 11, 12),
            'effective_date': (
                pd.Timestamp('2015-01-07'),
                pd.Timestamp('2015-01-09'),
                pd.Timestamp('2015-01-13'),
                pd.Timestamp('2015-01-15'),
                pd.Timestamp('2015-01-18')),
        })
        sid_40_splits = pd.DataFrame({
            SID_FIELD_NAME: 40,
            'ratio': (13, 14),
            'effective_date': (
                pd.Timestamp('2015-01-20'),
                pd.Timestamp('2015-01-22')
            )
        })
        sid_50_splits = pd.DataFrame({
            SID_FIELD_NAME: 50,
            'ratio': (15, 16),
            'effective_date': (
                pd.Timestamp('2015-01-13'),
                pd.Timestamp('2015-01-14')
            )
        })
        return pd.concat([
            sid_0_splits,
            sid_10_splits,
            sid_20_splits,
            sid_30_splits,
            sid_40_splits,
            sid_50_splits,
        ])
class PreviousWithSplitAdjustedWindows(WithSplitAdjustedWindows,
                                       ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )
    @classmethod
    def make_expected_timelines(cls):
<a name="35"></a>        oneq_previous = pd.concat([
            pd.concat([
                cls.create_expected_df_for_factor_compute([
                    <font color="#41a317"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
                    (10, np.NaN, cls.window_test_start_date),
                    (20, np.NaN, cls.window_test_start_date),
                    (30, 131*1/10, pd.</b></font>Timestamp('2015-01-09')),
                    (40, 140., pd.Timestamp('2015-01-09')),
                    (50, 150 * 1 / 15 * 1 / 16, pd.Timestamp('2015-01-09')),
                ], end_date)
<a name="34"></a>                for end_date in pd.date_range('2015-01-09', '2015-01-12')
            ]),
            cls.create_expected_df_for_factor_compute([
                <font color="#827d6b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
                (10, np.NaN, cls.window_test_start_date),
                (20, np.NaN, cls.window_test_start_date),
                (30, 131, pd.</b></font>Timestamp('2015-01-09')),
                (40, 140., pd.Timestamp('2015-01-09')),
                (50, 150. * 1 / 16, pd.Timestamp('2015-01-09')),
            ], pd.Timestamp('2015-01-13')),
            cls.create_expected_df_for_factor_compute([
                (0, np.NaN, cls.window_test_start_date),
                (10, np.NaN, cls.window_test_start_date),
                (20, np.NaN, cls.window_test_start_date),
                (30, 131, pd.Timestamp('2015-01-09')),
                (40, 140., pd.Timestamp('2015-01-09')),
                (50, 150., pd.Timestamp('2015-01-09'))
            ], pd.Timestamp('2015-01-14')),
            pd.concat([
                cls.create_expected_df_for_factor_compute([
                    (0, np.NaN, cls.window_test_start_date),
                    (10, np.NaN, cls.window_test_start_date),
                    (20, np.NaN, cls.window_test_start_date),
                    (30, 131*11, pd.Timestamp('2015-01-09')),
                    (40, 140., pd.Timestamp('2015-01-09')),
                    (50, 150., pd.Timestamp('2015-01-09')),
                ], end_date)
                for end_date in pd.date_range('2015-01-15', '2015-01-16')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp('2015-01-20')),
                     (10, np.NaN, cls.window_test_start_date),
                     (20, 121*.7*.8, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13, pd.Timestamp('2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-20', '2015-01-21')
<a name="28"></a>            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101, pd.Timestamp<font color="#717d7d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>('2015-01-20')),
                     (10, 111*.3, pd.Timestamp('2015-01-22')),
                     (20, 121*.7*.8, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13*14, pd.Timestamp(</b></font>'2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-22', '2015-01-29')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101*7, pd.Timestamp('2015-01-20')),
                     (10, 111*.3, pd.Timestamp('2015-01-22')),
                     (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-30', '2015-02-04')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 101*7, pd.Timestamp('2015-01-20')),
                     (10, 311*.3, pd.Timestamp('2015-02-05')),
                     (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),
                     (30, 231, pd.Timestamp('2015-01-20')),
                     (40, 140.*13*14, pd.Timestamp('2015-01-09')),
                     (50, 150., pd.Timestamp('2015-01-09'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-05', '2015-02-09')
                ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 201, pd.Timestamp('2015-02-10')),
                 (10, 311*.3, pd.Timestamp('2015-02-05')),
                 (20, 221*.8*.9, pd.Timestamp('2015-02-10')),
                 (30, 231, pd.Timestamp('2015-01-20')),
                 (40, 240.*13*14, pd.Timestamp('2015-02-10')),
                 (50, 250., pd.Timestamp('2015-02-10'))],
                pd.Timestamp('2015-02-10')
            ),
<a name="10"></a>        ])
        twoq_previous = pd.concat(
            [<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.</b></font>date_range('2015-01-09', '2015-01-19')] +
            [cls.create_expected_df_for_factor_compute(
                [(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date),
                 (30, 131*11*12, pd.Timestamp('2015-01-20'))],
                end_date
            ) for end_date in pd.date_range('2015-01-20', '2015-02-09')] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 101*7, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, pd.Timestamp('2015-02-05')),
                 (20, 121*.7*.8*.9, pd.Timestamp('2015-02-10')),
                 (30, 131*11*12, pd.Timestamp('2015-01-20')),
                 (40, 140. * 13 * 14, pd.Timestamp('2015-02-10')),
                 (50, 150., pd.Timestamp('2015-02-10'))],
                pd.Timestamp('2015-02-10')
            )]
        )
        return {
            1: oneq_previous,
            2: twoq_previous
        }
class BlazePreviousWithSplitAdjustedWindows(PreviousWithSplitAdjustedWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )
class NextWithSplitAdjustedWindows(WithSplitAdjustedWindows, ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return NextSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )
    @classmethod
<a name="8"></a>    def make_expected_timelines(cls):
        oneq_next = pd.concat([
            cls.create_expected_df_for_factor_compute(
                [(<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 100*1/4, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (20, 120*5/3, cls.window_test_start_date),
                 (20, 121*5/3, pd.Timestamp('2015-01-07')),
                 (30, 130*1/10, cls.window_test_start_date),
                 (30, 131*1/10, pd.Timestamp('2015-01-09')),
                 (40, 140, pd.</b></font>Timestamp('2015-01-09')),
                 (50, 150.*1/15*1/16, pd.Timestamp('2015-01-09'))],
                pd.Timestamp('2015-01-09')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 100*1/4, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120*5/3, cls.window_test_start_date),
                 (20, 121*5/3, pd.Timestamp('2015-01-07')),
                 (30, 230*1/10, cls.window_test_start_date),
                 (40, np.NaN, pd.Timestamp('2015-01-10')),
                 (50, 250.*1/15*1/16, pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-12')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07')),
                 (30, 230, cls.window_test_start_date),
                 (40, np.NaN, pd.Timestamp('2015-01-10')),
                 (50, 250.*1/16, pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-13')
            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 100, cls.window_test_start_date),
                 (10, 110, pd.Timestamp('2015-01-09')),
                 (10, 111, pd.Timestamp('2015-01-12')),
                 (20, 120, cls.window_test_start_date),
                 (20, 121, pd.Timestamp('2015-01-07')),
                 (30, 230, cls.window_test_start_date),
                 (40, np.NaN, pd.Timestamp('2015-01-10')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-14')
            ),
            pd.concat([
<a name="18"></a>                cls.create_expected_df_for_factor_compute(
                    [(0, 100*5, cls.window_test_start_date),
                     (10, 110, pd.Timestamp('2015-01-09')),
                     (10, 111, pd<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-12')),
                     (20, 120*.7, cls.window_test_start_date),
                     (20, 121*.7, pd.Timestamp('2015-01-07')),
                     (30, 230*11, cls.window_test_start_date),
                     (40, 240, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.</b></font>Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-15', '2015-01-16')
<a name="5"></a>            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 100*5*6, cls.window_test_start_date),
                 (<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 101, pd.Timestamp('2015-01-20')),
                 (10, 110*.3, pd.Timestamp('2015-01-09')),
                 (10, 111*.3, pd.Timestamp('2015-01-12')),
                 (20, 120*.7*.8, cls.window_test_start_date),
                 (20, 121*.7*.8, pd.Timestamp('2015-01-07')),
                 (30, 230*11*12, cls.window_test_start_date),
                 (30, 231, pd.</b></font>Timestamp('2015-01-20')),
                 (40, 240*13, pd.Timestamp('2015-01-15')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
<a name="11"></a>                pd.Timestamp('2015-01-20')
            ),
            cls.create_expected_df_for_factor_compute(
                [(<font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),
                 (10, 110 * .3, pd.Timestamp('2015-01-09')),
                 (10, 111 * .3, pd.Timestamp('2015-01-12')),
                 (20, 220 * .7 * .8, cls.window_test_start_date),
                 (20, 221 * .8, pd.Timestamp('2015-01-17')),
                 (40, 240 * 13, pd.</b></font>Timestamp('2015-01-15')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-21')
<a name="15"></a>            ),
            cls.create_expected_df_for_factor_compute(
                [(0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),
                 (10, 110 * .3, pd<font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-09')),
                 (10, 111 * .3, pd.Timestamp('2015-01-12')),
                 (20, 220 * .7 * .8, cls.window_test_start_date),
                 (20, 221 * .8, pd.Timestamp('2015-01-17')),
                 (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                 (50, 250., pd.</b></font>Timestamp('2015-01-12'))],
                pd.Timestamp('2015-01-22')
            ),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200*5*6, pd.Timestamp('2015-01-12')),
                     (10, 310*.3, pd.Timestamp('2015-01-09')),
                     (10, 311*.3, pd.Timestamp('2015-01-15')),
                     (20, 220*.7*.8, cls.window_test_start_date),
                     (20, 221*.8, pd.Timestamp('2015-01-17')),
                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-23', '2015-01-29')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
                     (10, 310*.3, pd.Timestamp('2015-01-09')),
                     (10, 311*.3, pd.Timestamp('2015-01-15')),
                     (20, 220*.7*.8*.9, cls.window_test_start_date),
                     (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-01-30', '2015-02-05')
            ]),
            pd.concat([
                cls.create_expected_df_for_factor_compute(
                    [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
                     (10, np.NaN, cls.window_test_start_date),
                     (20, 220*.7*.8*.9, cls.window_test_start_date),
                     (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
                     (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                     (50, 250., pd.Timestamp('2015-01-12'))],
                    end_date
                ) for end_date in pd.date_range('2015-02-06', '2015-02-09')
            ]),
            cls.create_expected_df_for_factor_compute(
                [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
                 (0, 201, pd.Timestamp('2015-02-10')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*.7*.8*.9, cls.window_test_start_date),
                 (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
                 (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
                 (50, 250., pd.Timestamp('2015-01-12'))],
                pd.Timestamp('2015-02-10')
            )
        ])
<a name="6"></a>
        twoq_next = pd.concat(
            [cls.create_expected_df_for_factor_compute(
                [<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*5/3, cls.window_test_start_date),
                 (30, 230*1/10, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date),
                 (50, np.NaN, cls.window_test_start_date)],
                pd.</b></font>Timestamp('2015-01-09')
            )] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 200*1/4, pd.Timestamp('2015-01-12')),
<a name="12"></a>                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*5/3, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, cls.window_test_start_date)],
                pd.Timestamp('2015-01-12')
            )] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 200, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220, cls.</b></font>window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-13', '2015-01-14')] +
            [cls.create_expected_df_for_factor_compute(
                [(0, 200*5, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*.7, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date)],
<a name="3"></a>                end_date
            ) for end_date in pd.date_range('2015-01-15', '2015-01-16')] +
            [cls.create_expected_df_for_factor_compute(
                [<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, 200*5*6, pd.Timestamp('2015-01-12')),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, 220*.7*.8, cls.window_test_start_date),
                 (20, 221*.8, pd.Timestamp('2015-01-17')),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.NaN, cls.window_test_start_date)],
<a name="14"></a>                pd.Timestamp('2015-01-20')
            )] +
            [cls</b></font>.create_expected_df_for_factor_compute(
                [<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
                 (10, np.NaN, cls.window_test_start_date),
                 (20, np.NaN, cls.window_test_start_date),
                 (30, np.NaN, cls.window_test_start_date),
                 (40, np.</b></font>NaN, cls.window_test_start_date)],
                end_date
            ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]
        )
        return {
            1: oneq_next,
            2: twoq_next
        }
class BlazeNextWithSplitAdjustedWindows(NextWithSplitAdjustedWindows):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate'],
            split_adjusted_asof=cls.split_adjusted_asof_date,
        )
class WithSplitAdjustedMultipleEstimateColumns(WithEstimates):
    END_DATE <font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= pd.Timestamp('2015-02-10')
    test_start_date = pd.Timestamp('2015-01-06', tz='utc')
    test_end_date = pd.Timestamp('2015-01-12', tz='utc')
    split_adjusted_asof = pd.Timestamp(</b></font>'2015-01-08')
    @classmethod
    def make_columns(cls):
        return {
            MultipleColumnsEstimates.event_date: 'event_date',
            MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',
            MultipleColumnsEstimates.fiscal_year: 'fiscal_year',
            MultipleColumnsEstimates.estimate1: 'estimate1',
            MultipleColumnsEstimates.estimate2: 'estimate2'
        }
    @classmethod
    def make_events(cls):
        sid_0_events = pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),
                            pd.Timestamp('2015-01-05')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-09'),
                 pd.Timestamp('2015-01-12')],
            'estimate1': [1100., 1200.],
            'estimate2': [2100., 2200.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 0,
        })
        sid_1_events = pd.DataFrame({
            TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),
                            pd.Timestamp('2015-01-05')],
            EVENT_DATE_FIELD_NAME:
                [pd.Timestamp('2015-01-08'),
                 pd.Timestamp('2015-01-11')],
            'estimate1': [1110., 1210.],
            'estimate2': [2110., 2210.],
            FISCAL_QUARTER_FIELD_NAME: [1, 2],
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 1,
        })
        return pd.concat([sid_0_events, sid_1_events])
    @classmethod
    def make_splits_data(cls):
        sid_0_splits = pd.DataFrame({
            SID_FIELD_NAME: 0,
            'ratio': (.3, 3.),
            'effective_date': (pd.Timestamp('2015-01-07'),
                               pd.Timestamp('2015-01-09')),
        })
        sid_1_splits = pd.DataFrame({
            SID_FIELD_NAME: 1,
            'ratio': (.4, 4.),
            'effective_date': (pd.Timestamp('2015-01-07'),
                               pd.Timestamp('2015-01-09')),
        })
        return pd.concat([sid_0_splits, sid_1_splits])
    @classmethod
    def make_expected_timelines_1q_out(cls):
        return {}
    @classmethod
    def make_expected_timelines_2q_out(cls):
        return {}
    @classmethod
    def init_class_fixtures(cls):
        super(
            WithSplitAdjustedMultipleEstimateColumns, cls
        ).init_class_fixtures()
        cls.timelines_1q_out = cls.make_expected_timelines_1q_out()
        cls.timelines_2q_out = cls.make_expected_timelines_2q_out()
    def test_adjustments_with_multiple_adjusted_columns(self):
        dataset = MultipleColumnsQuartersEstimates(1)
        timelines = self.timelines_1q_out
        window_len = 3
        class SomeFactor(CustomFactor):
            inputs = [dataset.estimate1, dataset.estimate2]
            window_length = window_len
            def compute(self, today, assets, out, estimate1, estimate2):
                assert_almost_equal(estimate1, timelines[today]['estimate1'])
                assert_almost_equal(estimate2, timelines[today]['estimate2'])
        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est': SomeFactor()}),
            start_date=self.test_start_date,
            end_date=self.test_end_date,
        )
    def test_multiple_datasets_different_num_announcements(self):
        dataset1 = MultipleColumnsQuartersEstimates(1)
        dataset2 = MultipleColumnsQuartersEstimates(2)
        timelines_1q_out = self.timelines_1q_out
        timelines_2q_out = self.timelines_2q_out
        window_len = 3
        class SomeFactor1(CustomFactor):
            inputs = [dataset1.estimate1]
            window_length = window_len
            def compute(self, today, assets, out, estimate1):
                assert_almost_equal(
                    estimate1, timelines_1q_out[today]['estimate1']
                )
        class SomeFactor2(CustomFactor):
            inputs = [dataset2.estimate2]
            window_length = window_len
            def compute(self, today, assets, out, estimate2):
                assert_almost_equal(
                    estimate2, timelines_2q_out[today]['estimate2']
                )
        engine = self.make_engine()
        engine.run_pipeline(
            Pipeline({'est1': SomeFactor1(), 'est2': SomeFactor2()}),
            start_date=self.test_start_date,
            end_date=self.test_end_date,
        )
class PreviousWithSplitAdjustedMultipleEstimateColumns(
    WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return PreviousSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )
    @classmethod
    def make_expected_timelines_1q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] * 3),
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-07', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] * 3),
<a name="37"></a>                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate1': np.array([[np<font color="#810541"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, np.NaN]] * 2 +
                                      [[np.NaN, 1110.]]),
                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[</b></font>np.NaN, 2110.]])
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] +
                                      [[np.NaN, 1110. * 4]] +
                                      [[1100 * 3., 1110. * 4]]),
                'estimate2': np.array([[np.NaN, np.NaN]] +
                                      [[np.NaN, 2110. * 4]] +
                                      [[2100 * 3., 2110. * 4]])
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                'estimate1': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[1200 * 3., 1210. * 4]]),
                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[2200 * 3., 2210. * 4]])
            }
        }
    @classmethod
    def make_expected_timelines_2q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-07', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
                                      [[2100 * 3., 2110. * 4]])
            }
        }
class BlazePreviousWithMultipleEstimateColumns(
    PreviousWithSplitAdjustedMultipleEstimateColumns
):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazePreviousSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )
class NextWithSplitAdjustedMultipleEstimateColumns(
    WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase
):
    @classmethod
    def make_loader(cls, events, columns):
        return NextSplitAdjustedEarningsEstimatesLoader(
            events,
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )
    @classmethod
<a name="9"></a>    def make_expected_timelines_1q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate1': np.array<font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>([[np.NaN, np.NaN]] +
                                      [[1100. * 1/.3, 1110. * 1/.4]] * 2),
                'estimate2': np.array([[np.NaN, np.NaN]] +
                                      [[2100. * 1/.3, 2110. * 1/.4]] * 2),
            },
            pd.Timestamp(</b></font>'2015-01-07', tz='utc'): {
                'estimate1': np.array([[1100., 1110.]] * 3),
                'estimate2': np.array([[2100., 2110.]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate1': np.array([[1100., 1110.]] * 3),
                'estimate2': np.array([[2100., 2110.]] * 3)
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate1': np.array([[1100 * 3., 1210. * 4]] * 3),
<a name="36"></a>                'estimate2': np.array([[2100 * 3., 2210. * 4]] * 3)
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                <font color="#ff00ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'estimate1': np.array([[1200 * 3., np.NaN]] * 3),
                'estimate2': np.array([[2200 * 3., np.</b></font>NaN]] * 3)
            }
        }
    @classmethod
    def make_expected_timelines_2q_out(cls):
        return {
            pd.Timestamp('2015-01-06', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] +
                                      [[2200 * 1/.3, 2210. * 1/.4]] * 2)
            },
            pd.Timestamp('2015-01-07', tz='utc'): {
                'estimate2': np.array([[2200., 2210.]] * 3)
            },
            pd.Timestamp('2015-01-08', tz='utc'): {
                'estimate2': np.array([[2200, 2210.]] * 3)
            },
            pd.Timestamp('2015-01-09', tz='utc'): {
                'estimate2': np.array([[2200 * 3., np.NaN]] * 3)
            },
            pd.Timestamp('2015-01-12', tz='utc'): {
                'estimate2': np.array([[np.NaN, np.NaN]] * 3)
            }
        }
class BlazeNextWithMultipleEstimateColumns(
    NextWithSplitAdjustedMultipleEstimateColumns
):
    @classmethod
    def make_loader(cls, events, columns):
        return BlazeNextSplitAdjustedEstimatesLoader(
            bz.data(events),
            columns,
            split_adjustments_loader=cls.adjustment_reader,
            split_adjusted_column_names=['estimate1', 'estimate2'],
            split_adjusted_asof=cls.split_adjusted_asof,
        )
class WithAdjustmentBoundaries(WithEstimates):
    START_DATE = pd.Timestamp('2015-01-04')
    test_start_date = pd.Timestamp('2015-01-05')
    END_DATE = test_end_date = pd.Timestamp('2015-01-12')
    split_adjusted_before_start = (
        test_start_date - timedelta(days=1)
    )
    split_adjusted_after_end = (
        test_end_date + timedelta(days=1)
    )
    split_adjusted_asof_dates = [(test_start_date,),
                                 (test_end_date,),
                                 (split_adjusted_before_start,),
                                 (split_adjusted_after_end,)]
    @classmethod
    def init_class_fixtures(cls):
        super(WithAdjustmentBoundaries, cls).init_class_fixtures()
        cls.s0 = cls.asset_finder.retrieve_asset(0)
        cls.s1 = cls.asset_finder.retrieve_asset(1)
        cls.s2 = cls.asset_finder.retrieve_asset(2)
        cls.s3 = cls.asset_finder.retrieve_asset(3)
        cls.s4 = cls.asset_finder.retrieve_asset(4)
        cls.expected = cls.make_expected_out()
    @classmethod
    def make_events(cls):
        sid_0_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_start_date,
            EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-09'),
            'estimate': 10.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 0,
        }, index=[0])
        sid_1_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_start_date,
            EVENT_DATE_FIELD_NAME: cls.test_start_date,
            'estimate': 11.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 1,
        }, index=[0])
        sid_2_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_end_date,
            EVENT_DATE_FIELD_NAME: cls.test_end_date + timedelta(days=1),
            'estimate': 12.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 2,
        }, index=[0])
        sid_3_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),
            EVENT_DATE_FIELD_NAME: cls.test_end_date,
            'estimate': 13.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 3,
        }, index=[0])
        sid_4_timeline = pd.DataFrame({
            TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),
            EVENT_DATE_FIELD_NAME: cls.test_end_date - timedelta(days=1),
            'estimate': 14.,
            FISCAL_QUARTER_FIELD_NAME: 1,
            FISCAL_YEAR_FIELD_NAME: 2015,
            SID_FIELD_NAME: 4,
        }, index=[0])
        return pd.concat([sid_0_timeline,
                          sid_1_timeline,
                          sid_2_timeline,
                          sid_3_timeline,
                          sid_4_timeline])
    @classmethod
    def make_splits_data(cls):
        sid_0_splits = pd.DataFrame({
            SID_FIELD_NAME: 0,
            'ratio': .10,
            'effective_date': cls.test_start_date,
        }, index=[0])
        sid_1_splits = pd.DataFrame({
            SID_FIELD_NAME: 1,
            'ratio': .11,
            'effective_date': cls.test_start_date,
        }, index=[0])
        sid_2_splits = pd.DataFrame({
            SID_FIELD_NAME: 2,
            'ratio': .12,
            'effective_date': cls.test_end_date,
        }, index=[0])
        sid_3_splits = pd.DataFrame({
            SID_FIELD_NAME: 3,
            'ratio': .13,
            'effective_date': cls.test_end_date,
        }, index=[0])
        sid_4_splits = pd.DataFrame({
            SID_FIELD_NAME: 4,
            'ratio': (.14, .15),
            'effective_date': (cls.test_start_date, cls.test_end_date),
        })
        return pd.concat([sid_0_splits,
                          sid_1_splits,
                          sid_2_splits,
                          sid_3_splits,
                          sid_4_splits])
<a name="30"></a>
    @parameterized.expand(split_adjusted_asof_dates)
    def test_boundaries(self, split_date):
        dataset = QuartersEstimates<font color="#ae694a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(1)
        loader = self.loader(split_adjusted_asof=split_date)
        engine = engine = self.make_engine(loader)
        result = engine.run_pipeline(</b></font>
            Pipeline({'estimate': dataset.estimate.latest}),
            start_date=self.trading_days[0],
            end_date=self.trading_days[-1],
        )
        expected = self.expected[split_date]
        assert_frame_equal(result, expected, check_names=False)
    @classmethod
    def make_expected_out(cls):
        return {}
class PreviousWithAdjustmentBoundaries(WithAdjustmentBoundaries,
                                       ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(PreviousSplitAdjustedEarningsEstimatesLoader,
                       events,
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])
    @classmethod
    def make_expected_out(cls):
        split_adjusted_at_start_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': np.NaN,
            }, index=pd.date_range(
                cls.test_start_date,
                pd.Timestamp('2015-01-08'),
                tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10.,
            }, index=pd.date_range(
                pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
<a name="24"></a>            }, index=pd.date_range(cls.test_start_date, cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                <font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>SID_FIELD_NAME: cls.s2,
                'estimate': np.NaN
            }, index=pd.date_range(cls.test_start_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.</b></font>s3,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'
            )),
<a name="22"></a>            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13. * .13
            }, index=pd.date_range(cls<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.</b></font>test_end_date - timedelta(2), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14. * .15
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1), cls.test_end_date, tz='utc'
            )),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(
            SID_FIELD_NAME, dropna=False)
        split_adjusted_at_end_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': np.NaN,
            }, index=pd.date_range(
                cls.test_start_date, pd.Timestamp('2015-01-08'), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10.,
            }, index=pd.date_range(
                pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
            }, index=pd.date_range(cls.test_start_date,
<a name="23"></a>                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                <font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>SID_FIELD_NAME: cls.s2,
                'estimate': np.NaN
            }, index=pd.date_range(cls.test_start_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.</b></font>s3,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13.
            }, index=pd.date_range(cls.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': np.NaN
            }, index=pd.date_range(
                cls.test_start_date, cls.test_end_date - timedelta(2), tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14.
            }, index=pd.date_range(cls.test_end_date - timedelta(1),
                                   cls.test_end_date,
                                   tz='utc')),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(SID_FIELD_NAME,
                                                            dropna=False)
        split_adjusted_before_start_boundary = split_adjusted_at_start_boundary
        split_adjusted_after_end_boundary = split_adjusted_at_end_boundary
        return {cls.test_start_date:
                split_adjusted_at_start_boundary,
                cls.split_adjusted_before_start:
                split_adjusted_before_start_boundary,
                cls.test_end_date:
                split_adjusted_at_end_boundary,
                cls.split_adjusted_after_end:
                split_adjusted_after_end_boundary}
class BlazePreviousWithAdjustmentBoundaries(PreviousWithAdjustmentBoundaries):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(BlazePreviousSplitAdjustedEstimatesLoader,
                       bz.data(events),
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])
class NextWithAdjustmentBoundaries(WithAdjustmentBoundaries,
                                   ZiplineTestCase):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(NextSplitAdjustedEarningsEstimatesLoader,
                       events,
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])
    @classmethod
    def make_expected_out(cls):
        split_adjusted_at_start_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10,
            }, index=pd.date_range(
                cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'
            )),
<a name="0"></a>            pd.DataFrame({
                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
            }, index<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=pd.date_range(cls.test_start_date,
                                   cls.test_start_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s2,
                'estimate': 12.,
            }, index=pd.date_range(cls.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13. * .13,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1), cls.</b></font>test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14.,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1),
                cls.test_end_date - timedelta(1),
                tz='utc'
            )),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(
            SID_FIELD_NAME, dropna=False)
        split_adjusted_at_end_boundary = pd.concat([
            pd.DataFrame({
                SID_FIELD_NAME: cls.s0,
                'estimate': 10,
            }, index=pd.date_range(
                cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'
            )),
            pd.DataFrame({
<a name="2"></a>                SID_FIELD_NAME: cls.s1,
                'estimate': 11.,
            }, index=pd.date_range(cls.test_start_date,
                                   cls<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.test_start_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s2,
                'estimate': 12.,
            }, index=pd.date_range(cls.test_end_date,
                                   cls.test_end_date,
                                   tz='utc')),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s3,
                'estimate': 13.,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1), cls.</b></font>test_end_date, tz='utc'
            )),
            pd.DataFrame({
                SID_FIELD_NAME: cls.s4,
                'estimate': 14.,
            }, index=pd.date_range(
                cls.test_end_date - timedelta(1),
                cls.test_end_date - timedelta(1),
                tz='utc'
            )),
        ]).set_index(SID_FIELD_NAME, append=True).unstack(
            SID_FIELD_NAME).reindex(cls.trading_days).stack(
            SID_FIELD_NAME, dropna=False)
        split_adjusted_before_start_boundary = split_adjusted_at_start_boundary
        split_adjusted_after_end_boundary = split_adjusted_at_end_boundary
        return {cls.test_start_date:
                split_adjusted_at_start_boundary,
                cls.split_adjusted_before_start:
                split_adjusted_before_start_boundary,
                cls.test_end_date:
                split_adjusted_at_end_boundary,
                cls.split_adjusted_after_end:
                split_adjusted_after_end_boundary}
class BlazeNextWithAdjustmentBoundaries(NextWithAdjustmentBoundaries):
    @classmethod
    def make_loader(cls, events, columns):
        return partial(BlazeNextSplitAdjustedEstimatesLoader,
                       bz.data(events),
                       columns,
                       split_adjustments_loader=cls.adjustment_reader,
                       split_adjusted_column_names=['estimate'])
class QuarterShiftTestCase(ZiplineTestCase):
    def test_quarter_normalization(self):
        input_yrs = pd.Series(range(2011, 2015), dtype=np.int64)
        input_qtrs = pd.Series(range(1, 5), dtype=np.int64)
        result_years, result_quarters = split_normalized_quarters(
            normalize_quarters(input_yrs, input_qtrs)
        )
        assert_equal(input_yrs, result_years)
        assert_equal(input_qtrs, result_quarters)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
