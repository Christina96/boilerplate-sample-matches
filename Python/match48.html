<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_algorithm.py &amp; test_quarters_estimates.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_algorithm.py &amp; test_quarters_estimates.py
      </h3>
<h1 align="center">
        12.3%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_algorithm.py (10.85359%)<th>test_quarters_estimates.py (14.4%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(2095-2131)<td><a href="#" name="0">(2608-2621)</a><td align="center"><font color="#ff0000">26</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(15-40)<td><a href="#" name="1">(1-49)</a><td align="center"><font color="#e10000">23</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(2275-2296)<td><a href="#" name="2">(2646-2658)</a><td align="center"><font color="#d70000">22</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(1159-1174)<td><a href="#" name="3">(1900-1908)</a><td align="center"><font color="#cd0000">21</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(803-807)<td><a href="#" name="4">(1241-1251)</a><td align="center"><font color="#ba0000">19</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(2160-2169)<td><a href="#" name="5">(1787-1793)</a><td align="center"><font color="#b00000">18</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(1384-1402)<td><a href="#" name="6">(1867-1873)</a><td align="center"><font color="#b00000">18</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(205-219)<td><a href="#" name="7">(1049-1054)</a><td align="center"><font color="#b00000">18</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(1324-1329)<td><a href="#" name="8">(1729-1735)</a><td align="center"><font color="#a60000">17</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(1174-1179)<td><a href="#" name="9">(2213-2218)</a><td align="center"><font color="#a60000">17</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(778-791)<td><a href="#" name="10">(1668-1674)</a><td align="center"><font color="#a60000">17</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(2325-2332)<td><a href="#" name="11">(1799-1804)</a><td align="center"><font color="#9c0000">16</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(1522-1531)<td><a href="#" name="12">(1880-1886)</a><td align="center"><font color="#9c0000">16</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(1213-1218)<td><a href="#" name="13">(1346-1352)</a><td align="center"><font color="#9c0000">16</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(2190-2195)<td><a href="#" name="14">(1909-1913)</a><td align="center"><font color="#930000">15</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(634-638)<td><a href="#" name="15">(1810-1815)</a><td align="center"><font color="#930000">15</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(4174-4186)<td><a href="#" name="16">(1466-1478)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(3172-3180)<td><a href="#" name="17">(744-750)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(1143-1147)<td><a href="#" name="18">(1776-1781)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(935-953)<td><a href="#" name="19">(1975-1978)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(718-721)<td><a href="#" name="20">(969-975)</a><td align="center"><font color="#890000">14</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(3602-3607)<td><a href="#" name="21">(1204-1210)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(2555-2562)<td><a href="#" name="22">(2491-2498)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#f660ab")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f660ab"><font color="#f660ab">-</font><td><a href="#" name="23">(2513-2517)<td><a href="#" name="23">(2530-2536)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#79764d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#79764d"><font color="#79764d">-</font><td><a href="#" name="24">(2495-2499)<td><a href="#" name="24">(2477-2483)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#5eac10")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5eac10"><font color="#5eac10">-</font><td><a href="#" name="25">(1557-1588)<td><a href="#" name="25">(250-252)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#68818b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#68818b"><font color="#68818b">-</font><td><a href="#" name="26">(1209-1213)<td><a href="#" name="26">(609-615)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#e77471")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#e77471"><font color="#e77471">-</font><td><a href="#" name="27">(686-691)<td><a href="#" name="27">(925-930)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#717d7d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#717d7d"><font color="#717d7d">-</font><td><a href="#" name="28">(164-168)<td><a href="#" name="28">(1625-1629)</a><td align="center"><font color="#7f0000">13</font>
<tr onclick='openModal("#af7a82")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#af7a82"><font color="#af7a82">-</font><td><a href="#" name="29">(4282-4296)<td><a href="#" name="29">(1194-1197)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#ae694a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ae694a"><font color="#ae694a">-</font><td><a href="#" name="30">(4149-4157)<td><a href="#" name="30">(2427-2430)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#3ea99f")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3ea99f"><font color="#3ea99f">-</font><td><a href="#" name="31">(3037-3046)<td><a href="#" name="31">(100-107)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#5b8daf")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#5b8daf"><font color="#5b8daf">-</font><td><a href="#" name="32">(2773-2775)<td><a href="#" name="32">(1424-1437)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#736aff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#736aff"><font color="#736aff">-</font><td><a href="#" name="33">(2435-2438)<td><a href="#" name="33">(775-781)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#827d6b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#827d6b"><font color="#827d6b">-</font><td><a href="#" name="34">(2360-2364)<td><a href="#" name="34">(1586-1589)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#41a317")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#41a317"><font color="#41a317">-</font><td><a href="#" name="35">(1403-1420)<td><a href="#" name="35">(1575-1579)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#ff00ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ff00ff"><font color="#ff00ff">-</font><td><a href="#" name="36">(1329-1333)<td><a href="#" name="36">(2231-2232)</a><td align="center"><font color="#750000">12</font>
<tr onclick='openModal("#810541")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#810541"><font color="#810541">-</font><td><a href="#" name="37">(1220-1221)<td><a href="#" name="37">(2139-2142)</a><td align="center"><font color="#750000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_algorithm.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import warnings
2 import datetime
3 from datetime import timedelta
4 from functools import partial
5 from textwrap import dedent
6 from copy import deepcopy
7 import logbook
8 import toolz
9 from logbook import TestHandler, WARNING
10 from nose_parameterized import parameterized
11 from six import iteritems, itervalues, string_types
12 from six.moves import range
13 from testfixtures import TempDirectory
14 import numpy as np
15 import pandas as pd
16 import pytz
17 from pandas.core.common import PerformanceWarning
18 from trading_calendars import get_calendar, register_calendar
19 import zipline.api
20 from zipline.api import FixedSlippage
21 from zipline.assets import Equity, Future, Asset
22 from zipline.assets.continuous_futures import ContinuousFuture
23 from</b></font> zipline.assets.synthetic import (
24     make_jagged_equity_info,
25     make_simple_equity_info,
26 )
27 from zipline.errors import (
28     AccountControlViolation,
29     CannotOrderDelistedAsset,
30     IncompatibleSlippageModel,
31     RegisterTradingControlPostInit,
32     ScheduleFunctionInvalidCalendar,
33     SetCancelPolicyPostInit,
34     SymbolNotFound,
35     TradingControlViolation,
36     UnsupportedCancelPolicy,
37     UnsupportedDatetimeFormat,
38     ZeroCapitalError
39 )
40 from zipline.finance.commission import PerShare, PerTrade
41 from zipline.finance.execution import LimitOrder
42 from zipline.finance.order import ORDER_STATUS
43 from zipline.finance.trading import SimulationParameters
44 from zipline.finance.asset_restrictions import (
45     Restriction,
46     HistoricalRestrictions,
47     StaticRestrictions,
48     RESTRICTION_STATES,
49 )
50 from zipline.finance.controls import AssetDateBounds
51 from zipline.testing import (
52     FakeDataPortal,
53     create_daily_df_for_asset,
54     create_data_portal_from_trade_history,
55     create_minute_df_for_asset,
56     make_test_handler,
57     make_trade_data_for_asset_info,
58     parameter_space,
59     str_to_seconds,
60     to_utc,
61 )
62 from zipline.testing import RecordBatchBlotter
63 import zipline.testing.fixtures as zf
64 from zipline.test_algorithms import (
65     access_account_in_init,
66     access_portfolio_in_init,
67     api_algo,
68     api_get_environment_algo,
69     api_symbol_algo,
70     handle_data_api,
71     handle_data_noop,
72     initialize_api,
73     initialize_noop,
74     noop_algo,
75     record_float_magic,
76     record_variables,
77     call_with_kwargs,
78     call_without_kwargs,
79     call_with_bad_kwargs_current,
80     call_with_bad_kwargs_history,
81     bad_type_history_assets,
82     bad_type_history_fields,
83     bad_type_history_bar_count,
84     bad_type_history_frequency,
85     bad_type_history_assets_kwarg_list,
86     bad_type_current_assets,
87     bad_type_current_fields,
88     bad_type_can_trade_assets,
89     bad_type_is_stale_assets,
90     bad_type_history_assets_kwarg,
91     bad_type_history_fields_kwarg,
92     bad_type_history_bar_count_kwarg,
93     bad_type_history_frequency_kwarg,
94     bad_type_current_assets_kwarg,
95     bad_type_current_fields_kwarg,
96     call_with_bad_kwargs_get_open_orders,
97     call_with_good_kwargs_get_open_orders,
98     call_with_no_kwargs_get_open_orders,
99     empty_positions,
100     no_handle_data,
101 )
102 from zipline.testing.predicates import assert_equal
103 from zipline.utils.api_support import ZiplineAPI
104 from zipline.utils.context_tricks import CallbackManager, nop_context
105 from zipline.utils.events import (
106     date_rules,
107     time_rules,
108     Always,
109     ComposedRule,
110     Never,
111     OncePerDay,
112 )
113 import zipline.utils.factory as factory
114 _multiprocess_can_split_ = False
115 class TestRecord(zf.WithMakeAlgo, zf.ZiplineTestCase):
116     ASSET_FINDER_EQUITY_SIDS = (133,)
117     SIM_PARAMS_DATA_FREQUENCY = 'daily'
118     DATA_PORTAL_USE_MINUTE_DATA = False
119     def test_record_incr(self):
120         def initialize(self):
121             self.incr = 0
122         def handle_data(self, data):
123             self.incr += 1
124             self.record(incr=self.incr)
125             name = 'name'
126             self.record(name, self.incr)
127             zipline.api.record(name, self.incr, 'name2', 2, name3=self.incr)
128         output = self.run_algorithm(
129             initialize=initialize,
130             handle_data=handle_data,
131         )
132                                       range(1, len(output) + 1))
133         np.testing.assert_array_equal(output['name'].values,
134                                       range(1, len<font color="#717d7d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(output) + 1))
135         np.testing.assert_array_equal(output['name2'].values,
136                                       [2] * len(output))
137         np.testing.assert_array_equal(output['name3'].values,
138                                       range(</b></font>1, len(output) + 1))
139 class TestMiscellaneousAPI(zf.WithMakeAlgo, zf.ZiplineTestCase):
140     START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
141     END_DATE = pd.Timestamp('2006-01-04', tz='UTC')
142     SIM_PARAMS_DATA_FREQUENCY = 'minute'
143     sids = 1, 2
144     BENCHMARK_SID = None
145     @classmethod
146     def make_equity_info(cls):
147         return pd.concat((
148             make_simple_equity_info(cls.sids, '2002-02-1', '2007-01-01'),
149             pd.DataFrame.from_dict(
150                 {3: {'symbol': 'PLAY',
151                      'start_date': '2002-01-01',
152                      'end_date': '2004-01-01',
153                      'exchange': 'TEST'},
154                  4: {'symbol': 'PLAY',
155                      'start_date': '2005-01-01',
156                      'end_date': '2006-01-01',
157                      'exchange': 'TEST'}},
158                 orient='index',
159             ),
160         ))
161     @classmethod
162     def make_futures_info(cls):
163         return pd.DataFrame.from_dict(
164             {
165                     'symbol': 'CLG06',
166                     'root_symbol': 'CL',
167                     'start_date': pd.Timestamp('2005-12-01', tz<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>='UTC'),
168                     'notice_date': pd.Timestamp('2005-12-20', tz='UTC'),
169                     'expiration_date': pd.Timestamp('2006-01-20', tz='UTC'),
170                     'exchange': 'TEST'
171                 },
172                 6: {
173                     'root_symbol': 'CL',
174                     'symbol': 'CLK06',
175                     'start_date': pd.Timestamp('2005-12-01', tz='UTC'),
176                     'notice_date': pd.Timestamp('2006-03-20', tz='UTC'),
177                     'expiration_date': pd.Timestamp('2006-04-20', tz='UTC'),
178                     'exchange': 'TEST',
179                 },
180                 7: {
181                     'symbol'</b></font>: 'CLQ06',
182                     'root_symbol': 'CL',
183                     'start_date': pd.Timestamp('2005-12-01', tz='UTC'),
184                     'notice_date': pd.Timestamp('2006-06-20', tz='UTC'),
185                     'expiration_date': pd.Timestamp('2006-07-20', tz='UTC'),
186                     'exchange': 'TEST',
187                 },
188                 8: {
189                     'symbol': 'CLX06',
190                     'root_symbol': 'CL',
191                     'start_date': pd.Timestamp('2006-02-01', tz='UTC'),
192                     'notice_date': pd.Timestamp('2006-09-20', tz='UTC'),
193                     'expiration_date': pd.Timestamp('2006-10-20', tz='UTC'),
194                     'exchange': 'TEST',
195                 }
196             },
197             orient='index',
198         )
199     def test_cancel_policy_outside_init(self):
200         code = """
201 from zipline.api import cancel_policy, set_cancel_policy
202 def initialize(algo):
203     pass
204 def handle_data(algo, data):
205     set_cancel_policy(cancel_policy.NeverCancel())
206         algo = self.make_algo(script=code)
207         with self.assertRaises(UnsupportedCancelPolicy):
208             algo.run()
209     def test_zipline_api_resolves_dynamically(self):
210         algo = self.make_algo(
211             initialize=lambda context: None,
212             handle_data=lambda context, data: None,
213         )
214         for method in algo.all_api_methods():
215             name = method.__name__
216             sentinel = object()
217             def fake_method(*args, **kwargs):
218                 return sentinel
219             setattr(algo, name, fake_method)
220             with ZiplineAPI(algo):
221                 self.assertIs(sentinel, getattr(zipline.api, name)())
222     def test_sid_datetime(self):
223         algo_text = """
224 from zipline.api import sid, get_datetime
225 def initialize(context):
226     pass
227 def handle_data(context, data):
228     aapl_dt = data.current(sid(1), "last_traded")
229     assert_equal(aapl_dt, get_datetime())
230         algo = self.make_algo(script=algo_text)
231         with self.assertRaises(TypeError):
232             algo.run()
233     @parameterized.expand([
234         (-1000, 'invalid_base'),
235         (0, 'invalid_base'),
236     ])
237     def test_invalid_capital_base(self, cap_base, name):
238         algo_text = """
239 def initialize(context):
240     pass
241 def handle_data(context, data):
242     order(sid(24), 1000)
243         algo = self.make_algo(
244             script=algotext,
245             sim_params=self.make_simparams(
246                 trading_calendar=get_calendar("CMES"),
247             )
248         )
249         algo.run()
250         nyse = get_calendar("NYSE")
251         for minute in algo.nyse_opens:
252             session_label = nyse.minute_to_session_label(minute)
253             session_open = nyse.session_open(session_label)
254             self.assertEqual(session_open, minute)
255         for minute in algo.nyse_closes:
256             session_label = nyse.minute_to_session_label(minute)
257             session_close = nyse.session_close(session_label)
258             self.assertEqual(session_close - timedelta(minutes=1), minute)
259         erroring_algotext = dedent(
260         )
261         algo = self.make_algo(
262             script=erroring_algotext,
263             sim_params=self.make_simparams(
264                 trading_calendar=get_calendar("CMES"),
265             ),
266         )
267         with self.assertRaises(ScheduleFunctionInvalidCalendar):
268             algo.run()
269     def test_schedule_function(self):
270         us_eastern = pytz.timezone('US/Eastern')
271         def incrementer(algo, data):
272             algo.func_called += 1
273             curdt = algo.get_datetime().tz_convert(pytz.utc)
274             self.assertEqual(
275                 curdt,
276                 us_eastern.localize(
277                     datetime.datetime.combine(
278                         curdt.date(),
279                         datetime.time(9, 31)
280                     ),
281                 ),
282             )
283         def initialize(algo):
284             algo.func_called = 0
285             algo.days = 1
286             algo.date = None
287             algo.schedule_function(
288                 func=incrementer,
289                 date_rule=date_rules.every_day(),
290                 time_rule=time_rules.market_open(),
291             )
292         def handle_data(algo, data):
293             if not algo.date:
294                 algo.date = algo.get_datetime().date()
295             if algo.date &lt; algo.get_datetime().date():
296                 algo.days += 1
297                 algo.date = algo.get_datetime().date()
298         algo = self.make_algo(
299             initialize=initialize,
300             handle_data=handle_data,
301         )
302         algo.run()
303         self.assertEqual(algo.func_called, algo.days)
304     def test_event_context(self):
305         expected_data = []
306         collected_data_pre = []
307         collected_data_post = []
308         function_stack = []
309         def pre(data):
310             function_stack.append(pre)
311             collected_data_pre.append(data)
312         def post(data):
313             function_stack.append(post)
314             collected_data_post.append(data)
315         def initialize(context):
316             context.add_event(Always(), f)
317             context.add_event(Always(), g)
318         def handle_data(context, data):
319             function_stack.append(handle_data)
320             expected_data.append(data)
321         def f(context, data):
322             function_stack.append(f)
323         def g(context, data):
324             function_stack.append(g)
325         algo = self.make_algo(
326             initialize=initialize,
327             handle_data=handle_data,
328             create_event_context=CallbackManager(pre, post),
329         )
330         algo.run()
331         self.assertEqual(len(expected_data), 780)
332         self.assertEqual(collected_data_pre, expected_data)
333         self.assertEqual(collected_data_post, expected_data)
334         self.assertEqual(
335             len(function_stack),
336             3900,
337             'Incorrect number of functions called: %s != 3900' %
338             len(function_stack),
339         )
340         expected_functions = [pre, handle_data, f, g, post] * 97530
341         for n, (f, g) in enumerate(zip(function_stack, expected_functions)):
342             self.assertEqual(
343                 f,
344                 g,
345                 'function at position %d was incorrect, expected %s but got %s'
346                 % (n, g.__name__, f.__name__),
347             )
348     @parameterized.expand([
349         ('daily',),
350         ('minute'),
351     ])
352     def test_schedule_function_rule_creation(self, mode):
353         def nop(*args, **kwargs):
354             return None
355         self.sim_params.data_frequency = mode
356         algo = self.make_algo(
357             initialize=nop,
358             handle_data=nop,
359             sim_params=self.sim_params,
360         )
361         algo.schedule_function(nop, time_rule=Never() &amp; Always())
362         event_rule = algo.event_manager._events[1].rule
363         self.assertIsInstance(event_rule, OncePerDay)
364         self.assertEqual(event_rule.cal, algo.trading_calendar)
365         inner_rule = event_rule.rule
366         self.assertIsInstance(inner_rule, ComposedRule)
367         self.assertEqual(inner_rule.cal, algo.trading_calendar)
368         first = inner_rule.first
369         second = inner_rule.second
370         composer = inner_rule.composer
371         self.assertIsInstance(first, Always)
372         self.assertEqual(first.cal, algo.trading_calendar)
373         self.assertEqual(second.cal, algo.trading_calendar)
374         if mode == 'daily':
375         else:
376             self.assertIsInstance(second, ComposedRule)
377             self<font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertIsInstance(second.first, Never)
378             self.assertEqual(second.first.cal, algo.trading_calendar)
379             self.assertIsInstance(second.second, Always)
380             self.assertEqual(second.second.</b></font>cal, algo.trading_calendar)
381         self.assertIs(composer, ComposedRule.lazy_and)
382     def test_asset_lookup(self):
383         algo = self.make_algo()
384         start_session = pd.Timestamp("2000-01-01", tz="UTC")
385         algo.sim_params = algo.sim_params.create_new(
386             start_session,
387             pd.Timestamp('2001-12-01', tz='UTC')
388         )
389         with self.assertRaises(SymbolNotFound):
390             algo.symbol('PLAY')
391         with self.assertRaises(SymbolNotFound):
392             algo.symbols('PLAY')
393         algo.sim_params = algo.sim_params.create_new(
394             start_session,
395             pd.Timestamp('2002-12-01', tz='UTC')
396         )
397         list_result = algo.symbols('PLAY')
398         self.assertEqual(3, list_result[0])
399         algo.sim_params = algo.sim_params.create_new(
400             start_session,
401             pd.Timestamp('2004-12-01', tz='UTC')
402         )
403         self.assertEqual(3, algo.symbol('PLAY'))
404         algo.sim_params = algo.sim_params.create_new(
405             start_session,
406             pd.Timestamp('2005-12-01', tz='UTC')
407         )
408         self.assertEqual(4, algo.symbol('PLAY'))
409         algo.sim_params = algo.sim_params.create_new(
410             start_session,
411         )
412         self.assertEqual(4, algo.symbol('PLAY'))
413         list_result = algo<font color="#e77471"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.symbols('PLAY')
414         self.assertEqual(4, list_result[0])
415         self.assertIsInstance(algo.sid(3), Equity)
416         self.assertIsInstance(algo.sid(</b></font>4), Equity)
417         with self.assertRaises(TypeError):
418             algo.symbol(1)
419         with self.assertRaises(TypeError):
420             algo.symbol((1,))
421         with self.assertRaises(TypeError):
422             algo.symbol({1})
423         with self.assertRaises(TypeError):
424             algo.symbol([1])
425         with self.assertRaises(TypeError):
426             algo.symbol({'foo': 'bar'})
427     def test_future_symbol(self):
428         algo = self.make_algo()
429         algo.datetime = pd.Timestamp('2006-12-01', tz='UTC')
430         cl = algo.future_symbol('CLG06')
431         self<font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertEqual(cl.sid, 5)
432         self.assertEqual(cl.symbol, 'CLG06')
433         self.assertEqual(cl.root_symbol, 'CL')
434         self.assertEqual(cl.start_date, pd.Timestamp(</b></font>'2005-12-01', tz='UTC'))
435         self.assertEqual(cl.notice_date, pd.Timestamp('2005-12-20', tz='UTC'))
436         self.assertEqual(cl.expiration_date,
437                          pd.Timestamp('2006-01-20', tz='UTC'))
438         with self.assertRaises(SymbolNotFound):
439             algo.future_symbol('')
440         with self.assertRaises(SymbolNotFound):
441             algo.future_symbol('PLAY')
442         with self.assertRaises(SymbolNotFound):
443             algo.future_symbol('FOOBAR')
444         with self.assertRaises(TypeError):
445             algo.future_symbol(1)
446         with self.assertRaises(TypeError):
447             algo.future_symbol((1,))
448         with self.assertRaises(TypeError):
449             algo.future_symbol({1})
450         with self.assertRaises(TypeError):
451             algo.future_symbol([1])
452         with self.assertRaises(TypeError):
453             algo.future_symbol({'foo': 'bar'})
454 class TestSetSymbolLookupDate(zf.WithMakeAlgo, zf.ZiplineTestCase):
455     START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
456     END_DATE = pd.Timestamp('2006-01-06', tz='UTC')
457     SIM_PARAMS_START_DATE = pd.Timestamp('2006-01-04', tz='UTC')
458     SIM_PARAMS_DATA_FREQUENCY = 'daily'
459     DATA_PORTAL_USE_MINUTE_DATA = False
460     BENCHMARK_SID = 3
461     @classmethod
462     def make_equity_info(cls):
463         dates = pd.date_range(cls.START_DATE, cls.END_DATE)
464         assert len(dates) == 4, "Expected four dates."
465         cls.asset_starts = [dates[0], dates[2]]
466         cls.asset_ends = [dates[1], dates[3]]
467         return pd<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.DataFrame.from_records([
468             {'symbol': 'DUP',
469              'start_date': cls.asset_starts[0],
470              'end_date': cls.asset_ends[0],
471              'exchange': 'TEST',
472              'asset_name': 'FIRST'},
473             {'symbol': 'DUP',
474              'start_date': cls.asset_starts[1],
475              'end_date': cls.asset_ends[1],
476              'exchange': 'TEST',
477              'asset_name': 'SECOND'},
478             {'symbol': 'BENCH',
479              'start_date': cls.START_DATE,
480              'end_date': cls.</b></font>END_DATE,
481              'exchange': 'TEST',
482              'asset_name': 'BENCHMARK'},
483         ], index=cls.sids)
484     def test_set_symbol_lookup_date(self):
485         def initialize(context):
486             set_symbol_lookup_date(self<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.asset_ends[0])
487             self.assertEqual(zipline.api.symbol('DUP').sid, self.sids[0])
488             set_symbol_lookup_date(self.asset_ends[1])
489             self.assertEqual(zipline.api.symbol('DUP').</b></font>sid, self.sids[1])
490             with self.assertRaises(UnsupportedDatetimeFormat):
491                 set_symbol_lookup_date('foobar')
492         self.run_algorithm(initialize=initialize)
493 class TestPositions(zf.WithMakeAlgo, zf.ZiplineTestCase):
494     START_DATE = pd.Timestamp('2006-01-03', tz='utc')
495     END_DATE = pd.Timestamp('2006-01-06', tz='utc')
496     SIM_PARAMS_CAPITAL_BASE = 1000
497     ASSET_FINDER_EQUITY_SIDS = (1, 133)
498     SIM_PARAMS_DATA_FREQUENCY = 'daily'
499     @classmethod
500     def make_equity_daily_bar_data(cls, country_code, sids):
501         frame = pd.DataFrame(
502             {
503                 'open': [90, 95, 100, 105],
504                 'high': [90, 95, 100, 105],
505                 'low': [90, 95, 100, 105],
506                 'close': [90, 95, 100, 105],
507                 'volume': 100,
508             },
509             index=cls.equity_daily_bar_days,
510         )
511         return ((sid, frame) for sid in sids)
512     @classmethod
513     def make_futures_info(cls):
514         return pd.DataFrame.from_dict(
515             {
516                 1000: {
517                     'symbol': 'CLF06',
518                     'root_symbol': 'CL',
519                     'start_date': cls.START_DATE,
520                     'end_date': cls.END_DATE,
521                     'auto_close_date': cls.END_DATE + cls.trading_calendar.day,
522                     'exchange': 'CMES',
523                     'multiplier': 100,
524                 },
525             },
526             orient='index',
527         )
528     @classmethod
529     def make_future_minute_bar_data(cls):
530         trading_calendar = cls.trading_calendars[Future]
531         sids = cls.asset_finder.futures_sids
532         minutes = trading_calendar.minutes_for_sessions_in_range(
533             cls.future_minute_bar_days[0],
534             cls.future_minute_bar_days[-1],
535         )
536         frame = pd.DataFrame(
537             {
538                 'open': 2.0,
539                 'high': 2.0,
540                 'low': 2.0,
541                 'close': 2.0,
542                 'volume': 100,
543             },
544             index=minutes,
545         )
546         return ((sid, frame) for sid in sids)
547     def test_portfolio_exited_position(self):
548         def initialize(context, sids):
549             context.ordered = False
550             context.exited = False
551             context.sids = sids
552         def handle_data(context, data):
553             if not context.ordered:
554                 for s in context.sids:
555                     context.order(context.sid(s), 1)
556                 context.ordered = True
557             if not context.exited:
558                 amounts = [pos.amount for pos
559                            in itervalues(context.portfolio.positions)]
560                 if (
561                     len(amounts) &gt; 0 and
562                     all([(amount == 1) for amount in amounts])
563                 ):
564                     for stock in context.portfolio.positions:
565                         context.order(context.sid(stock), -1)
566                     context.exited = True
567             context.record(num_positions=len(context.portfolio.positions))
568         result = self.run_algorithm(
569             initialize=initialize,
570             handle_data=handle_data,
571             sids=self.ASSET_FINDER_EQUITY_SIDS,
572         )
573         expected_position_count = [
574             0,  # Before entering the first position
575             2,  # After entering, exiting on this date
576             0,  # After exiting
577             0,
578         ]
579         for i, expected in enumerate(expected_position_count):
580             self.assertEqual(result.ix[i]['num_positions'], expected)
581     def test_noop_orders(self):
582         asset = self.asset_finder.retrieve_asset(1)
583         def handle_data(algo, data):
584             algo.order(asset, 100, limit_price<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=1)
585             algo.order(asset, 100, stop_price=10000000)
586             algo.order(asset, 100, limit_price=10000000, stop_price=10000000)
587             algo.order(asset, 100, limit_price=1, stop_price=1)
588             algo.order(</b></font>asset, -100, limit_price=1000000)
589             algo.order(asset, -100, stop_price=1)
590             algo.order(asset, -100, limit_price=1000000, stop_price=1000000)
591             algo.order(asset, -100, limit_price=1, stop_price=1)
592             algo.order(asset, 100, limit_price=.00000001)
593             algo.order(asset, -100, stop_price=.00000001)
594         daily_stats = self.run_algorithm(handle_data=handle_data)
595         empty_positions = daily_stats.positions.map(lambda x: len(x) == 0)
596         self.assertTrue(empty_positions.all())
597     def test_position_weights(self):
598         sids = (1, 133, 1000)
599         equity_1, equity_133, future_1000 = \
600             self.asset_finder.retrieve_all(sids)
601         def initialize(algo, sids_and_amounts, *args, **kwargs):
602             algo.ordered = False
603             algo.sids_and_amounts = sids_and_amounts
604             algo.set_commission(
605                 us_equities=PerTrade(0), us_futures=PerTrade(0),
606             )
607             algo.set_slippage(
608                 us_equities=FixedSlippage(0),
609                 us_futures=FixedSlippage(0),
610             )
611         def handle_data(algo, data):
612             if not algo.ordered:
613                 for s, amount in algo.sids_and_amounts:
614                     algo.order(algo.sid(s), amount)
615                 algo.ordered = True
616             algo.record(
617                 position_weights=algo.portfolio.current_portfolio_weights,
618             )
619         daily_stats = self.run_algorithm(
620             sids_and_amounts=zip(sids, [2, -1, 1]),
621             initialize=initialize,
622             handle_data=handle_data,
623         )
624         expected_position_weights = [
625             pd.Series({}),
626             pd.Series({
627                 equity_1: 190.0 / (190.0 - 95.0 + 905.0),
628                 equity_133: -95.0 / (190.0 - 95.0 + 905.0),
629                 future_1000: 200.0 / (190.0 - 95.0 + 905.0),
630             }),
631             pd.Series({
632                 equity_1: 200.0 / (200.0 - 100.0 + 905.0),
633                 equity_133: -100.0 / (200.0 - 100.0 + 905.0),
634                 future_1000: 200.0 / (200.0 - 100.0 + 905.0),
635             }),
636             pd.Series({
637                 equity_1: 210.0 / (210.0 - 105.0 + 905.0),
638                 equity_133: -105.0 / (210.0 - 105.0 + 905.0),
639                 future_1000: 200.0 / (210.0 - 105.0 + 905.0),
640             }),
641         ]
642         for i, expected in enumerate(expected_position_weights):
643             assert_equal(daily_stats.iloc[i]['position_weights'], expected)
644 class TestBeforeTradingStart(zf.WithMakeAlgo, zf.ZiplineTestCase):
645     START_DATE = pd.Timestamp('2016-01-06', tz='utc')
646     END_DATE = pd.Timestamp('2016-01-07', tz='utc')
647     SIM_PARAMS_CAPITAL_BASE = 10000
648     SIM_PARAMS_DATA_FREQUENCY = 'minute'
649     EQUITY_DAILY_BAR_LOOKBACK_DAYS = EQUITY_MINUTE_BAR_LOOKBACK_DAYS = 1
650     DATA_PORTAL_FIRST_TRADING_DAY = pd.Timestamp("2016-01-05", tz='UTC')
651     EQUITY_MINUTE_BAR_START_DATE = pd.Timestamp("2016-01-05", tz='UTC')
652     FUTURE_MINUTE_BAR_START_DATE = pd.Timestamp("2016-01-05", tz='UTC')
653     data_start = ASSET_FINDER_EQUITY_START_DATE = pd.Timestamp(
654         '2016-01-05',
655         tz='utc',
656     )
657     SPLIT_ASSET_SID = 3
658     ASSET_FINDER_EQUITY_SIDS = 1, 2, SPLIT_ASSET_SID
659     @classmethod
660     def make_equity_minute_bar_data(cls):
661         asset_minutes = \
662             cls.trading_calendar.minutes_in_range(
663                 cls.data_start,
664                 cls.END_DATE,
665             )
666         minutes_count = len(asset_minutes)
667         minutes_arr = np.arange(minutes_count) + 1
668         split_data = pd.DataFrame(
669             {
670                 'open': minutes_arr + 1,
671                 'high': minutes_arr + 2,
672                 'low': minutes_arr - 1,
673                 'close': minutes_arr,
674                 'volume': 100 * minutes_arr,
675             },
676             index=asset_minutes,
677         )
678         split_data.iloc[780:] = split_data.iloc[780:] / 2.0
679         for sid in (1, 8554):
680             yield sid, create_minute_df_for_asset(
681                 cls.trading_calendar,
682                 cls.data_start,
683                 cls.END_DATE,
684             )
685         yield 2, create_minute_df_for_asset(
686             cls.trading_calendar,
687             cls.data_start,
688             cls.END_DATE,
689             50,
690         )
691         yield cls.SPLIT_ASSET_SID, split_data
692     @classmethod
693     def make_splits_data(cls):
694         return pd.DataFrame.from_records([
695             {
696                 'effective_date': str_to_seconds('2016-01-07'),
697                 'ratio': 0.5,
698                 'sid': cls.SPLIT_ASSET_SID,
699             }
700         ])
701     @classmethod
702     def make_equity_daily_bar_data(cls, country_code, sids):
703         for sid in sids:
704             yield sid, create_daily_df_for_asset(
705                 cls.trading_calendar,
706                 cls.data_start,
707                 cls.END_DATE,
708             )
709     def test_data_in_bts_minute(self):
710         algo_code = dedent("""
711         from zipline.api import record, sid
712         def initialize(context):
713             context.history_values = []
714         def before_trading_start(context, data):
715             record(the_price1=data.current(sid(1), "price"))
716             record(the_high1=data.current(sid(1), "high"))
717             record(the_price2=data.current(sid(2), "price"))
718             record(the_high2=data.current(sid(2), "high"))
719             context.history_values.append(data.history(
720                 [sid(1), sid(2)],
721                 ["price", "high"],
722                 60,
723                 "1m"
724             ))
725         def handle_data(context, data):
726             pass
727         """)
728         self<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertEqual(390, results.iloc[0].the_price1)
729         self.assertEqual(392, results.iloc[0].the_high1)
730         self.assertEqual(350, results.iloc[</b></font>0].the_price2)
731         self.assertTrue(np.isnan(results.iloc[0].the_high2))
732         np.testing.assert_array_equal(
733             range(331, 391), algo.history_values[0]["price"][1]
734         )
735         np.testing.assert_array_equal(
736             range(333, 393), algo.history_values[0]["high"]<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[1]
737         )
738         np.testing.assert_array_equal(
739             [300] * 19, algo.history_values[0]["price"][2][0:19]
740         )
741         np.testing.assert_array_equal(
742             [350] * 40, algo.history_values[0]["price"][2][20:]
743         )
744         np.testing.assert_array_equal(
745             np.</b></font>full<font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(19, np.nan), algo.history_values[0]["high"][2][0:19]
746         )
747         self.assertEqual(352, algo.history_values[0]["high"][2][19])
748         np.testing.assert_array_equal(</b></font>
749             np.full(40, np.nan), algo.history_values[0]["high"][2][20:]
750         )
751     def test_data_in_bts_daily(self):
752         algo_code = dedent("""
753         from zipline.api import record, sid
754         def initialize(context):
755             context.history_values = []
756         def before_trading_start(context, data):
757             record(the_price1=data.current(sid(1), "price"))
758             record(the_high1=data.current(sid(1), "high"))
759             record(the_price2=data.current(sid(2), "price"))
760             record(the_high2=data.current(sid(2), "high"))
761             context.history_values.append(data.history(
762                 [sid(1), sid(2)],
763                 ["price", "high"],
764                 1,
765                 "1d",
766             ))
767         def handle_data(context, data):
768             pass
769         """)
770         results = algo.run()
771         self<font color="#68818b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.assertEqual(392, results.the_high1[0])
772         self.assertTrue(np.isnan(results.</b></font>the_high2<font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[0]))
773         self.assertTrue(350, results.the_price2[0])
774         self.assertEqual(390, algo.history_values[</b></font>0]["price"][1][0])
775         self.assertEqual(352, algo<font color="#810541"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.history_values[0]["high"][2][0])
776         self.assertEqual(350, algo.history_values[0]["price"][2][</b></font>0])
777     def test_portfolio_bts(self):
778         algo_code = dedent("""
779         from zipline.api import order, sid, record
780         def initialize(context):
781             context.ordered = False
782             context.hd_portfolio = context.portfolio
783         def before_trading_start(context, data):
784             bts_portfolio = context.portfolio
785             assert (context.hd_portfolio == bts_portfolio)
786             record(pos_value=bts_portfolio.positions_value)
787         def handle_data(context, data):
788             if not context.ordered:
789                 order(sid(1), 1)
790                 context.ordered = True
791             context.hd_portfolio = context.portfolio
792         """)
793         algo = self.make_algo(script=algo_code)
794         results = algo.run()
795         self.assertEqual(results.pos_value.iloc[0], 0)
796         self.assertEqual(results.pos_value.iloc[1], 780)
797     def test_account_bts(self):
798         algo_code = dedent("""
799         from zipline.api import order, sid, record, set_slippage, slippage
800         def initialize(context):
801             context.ordered = False
802             context.hd_account = context.account
803             set_slippage(slippage.VolumeShareSlippage())
804         def before_trading_start(context, data):
805             bts_account = context.account
806             assert (context.hd_account == bts_account)
807             record(port_value=context.account.equity_with_loan)
808         def handle_data(context, data):
809             if not context.ordered:
810                 order(sid(1), 1)
811                 context.ordered = True
812             context.hd_account = context.account
813         """)
814         algo = self.make_algo(script=algo_code)
815         results = algo.run()
816         self.assertEqual(results.port_value.iloc[0], 10000)
817         self.assertAlmostEqual(results.port_value.iloc[1],
818                                10000 + 780 - 392 - 0,
819                                places=2)
820     def test_portfolio_bts_with_overnight_split(self):
821         algo_code = dedent("""
822         from zipline.api import order, sid, record
823         def initialize(context):
824             context.ordered = False
825             context.hd_portfolio = context.portfolio
826         def before_trading_start(context, data):
827             bts_portfolio = context.portfolio
828             for k in bts_portfolio.__dict__:
829                 if k != 'positions':
830                     assert (context.hd_portfolio.__dict__[k]
831                             == bts_portfolio.__dict__[k])
832             record(pos_value=bts_portfolio.positions_value)
833             record(pos_amount=bts_portfolio.positions[sid(3)].amount)
834             record(
835                 last_sale_price=bts_portfolio.positions[sid(3)].last_sale_price
836             )
837         def handle_data(context, data):
838             if not context.ordered:
839                 order(sid(3), 1)
840                 context.ordered = True
841             context.hd_portfolio = context.portfolio
842         """)
843         self.assertEqual(results<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.pos_value.iloc[0], 0)
844         self.assertEqual(results.pos_value.iloc[1], 780)
845         self.assertEqual(results.pos_amount.iloc[0], 0)
846         self.assertEqual(results.pos_amount.</b></font>iloc<font color="#ff00ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[1], 2)
847         self.assertEqual(results.last_sale_price.iloc[0], 0)
848         self.assertEqual(results.last_sale_price.iloc[</b></font>1], 390)
849     def test_account_bts_with_overnight_split(self):
850         algo_code = dedent("""
851         from zipline.api import order, sid, record, set_slippage, slippage
852         def initialize(context):
853             context.ordered = False
854             context.hd_account = context.account
855             set_slippage(slippage.VolumeShareSlippage())
856         def before_trading_start(context, data):
857             bts_account = context.account
858             assert (context.hd_account == bts_account)
859             record(port_value=bts_account.equity_with_loan)
860         def handle_data(context, data):
861             if not context.ordered:
862                 order(sid(1), 1)
863                 context.ordered = True
864             context.hd_account = context.account
865         """)
866         results = self.run_algorithm(script=algo_code)
867         self.assertEqual(results.port_value.iloc[0], 10000)
868         self.assertAlmostEqual(results.port_value.iloc[1],
869                                10000 + 780 - 392 - 0, places=2)
870 class TestAlgoScript(zf.WithMakeAlgo, zf.ZiplineTestCase):
871     START_DATE = pd.Timestamp('2006-01-03', tz='utc')
872     END_DATE = pd.Timestamp('2006-12-31', tz='utc')
873     SIM_PARAMS_DATA_FREQUENCY = 'daily'
874     DATA_PORTAL_USE_MINUTE_DATA = False
875     EQUITY_DAILY_BAR_LOOKBACK_DAYS = 5  # max history window length
876     STRING_TYPE_NAMES = [s.__name__ for s in string_types]
877     STRING_TYPE_NAMES_STRING = ', '.join(STRING_TYPE_NAMES)
878     ASSET_TYPE_NAME = Asset.__name__
879     CONTINUOUS_FUTURE_NAME = ContinuousFuture.__name__
880     ASSET_OR_STRING_TYPE_NAMES = ', '.join([ASSET_TYPE_NAME] +
881                                            STRING_TYPE_NAMES)
882     ASSET_OR_STRING_OR_CF_TYPE_NAMES = ', '.join([ASSET_TYPE_NAME,
883                                                  STRING_TYPE_NAMES)
884     ARG_TYPE_TEST_CASES = (
885         <font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('history__assets', (bad_type_history_assets,
886                              ASSET_OR_STRING_OR_CF_TYPE_NAMES,
887                              True)),
888         ('history__fields', (bad_type_history_fields,
889                              STRING_TYPE_NAMES_STRING,
890                              True)),
891         ('history__bar_count', (bad_type_history_bar_count, 'int', False)),
892         ('history__frequency', (bad_type_history_frequency,
893                                 STRING_TYPE_NAMES_STRING,
894                                 False)),
895         ('current__assets', (bad_type_current_assets,
896                              ASSET_OR_STRING_OR_CF_TYPE_NAMES,
897                              True)),
898         ('current__fields', (bad_type_current_fields,
899                              STRING_TYPE_NAMES_STRING,
900                              True)),
901         ('can_trade__assets', (bad_type_can_trade_assets, 'Asset', True)),
902         ('history_kwarg__assets'</b></font>,
903          (<font color="#41a317"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>bad_type_history_assets_kwarg,
904           ASSET_OR_STRING_OR_CF_TYPE_NAMES,
905           True)),
906         ('history_kwarg_bad_list__assets',
907          (bad_type_history_assets_kwarg_list,
908           ASSET_OR_STRING_OR_CF_TYPE_NAMES,
909           True)),
910         ('history_kwarg__fields',
911          (bad_type_history_fields_kwarg, STRING_TYPE_NAMES_STRING, True)),
912         ('history_kwarg__bar_count',
913          (bad_type_history_bar_count_kwarg, 'int', False)),
914         ('history_kwarg__frequency',
915          (bad_type_history_frequency_kwarg, STRING_TYPE_NAMES_STRING, False)),
916         ('current_kwarg__assets',
917          (bad_type_current_assets_kwarg,
918           ASSET_OR_STRING_OR_CF_TYPE_NAMES,
919           True)),
920         ('current_kwarg__fields'</b></font>,
921          (bad_type_current_fields_kwarg, STRING_TYPE_NAMES_STRING, True)),
922     )
923     sids = 0, 1, 3, 133
924     BENCHMARK_SID = None
925     @classmethod
926     def make_equity_info(cls):
927         register_calendar("TEST", get_calendar("NYSE"), force=True)
928         data = make_simple_equity_info(
929             cls.sids,
930             cls.START_DATE,
931             cls.END_DATE,
932         )
933         data.loc[3, 'symbol'] = 'TEST'
934         return data
935     @classmethod
936     def make_equity_daily_bar_data(cls, country_code, sids):
937         cal = cls.trading_calendars[Equity]
938         sessions = cal.sessions_in_range(cls.START_DATE, cls.END_DATE)
939         frame = pd.DataFrame({
940             'close': 10., 'high': 10.5, 'low': 9.5, 'open': 10., 'volume': 100,
941         }, index=sessions)
942         for sid in sids:
943             yield sid, frame
944     def test_noop(self):
945         self.run_algorithm(
946             initialize=initialize_noop,
947             handle_data=handle_data_noop,
948         )
949     def test_noop_string(self):
950         self.run_algorithm(script=noop_algo)
951     def test_no_handle_data(self):
952         self.run_algorithm(script=no_handle_data)
953     def test_api_calls(self):
954         self.run_algorithm(
955             initialize=initialize_api,
956             handle_data=handle_data_api,
957         )
958     def test_api_calls_string(self):
959         self.run_algorithm(script=api_algo)
960     def test_api_get_environment(self):
961         platform = 'zipline'
962         algo = self.make_algo(
963             script=api_get_environment_algo,
964             platform=platform,
965         )
966         algo.run()
967         self.assertEqual(algo.environment, platform)
968     def test_api_symbol(self):
969         self.run_algorithm(script=api_symbol_algo)
970     def test_fixed_slippage(self):
971         test_algo = self.make_algo(
972             script="""
973 from zipline.api import (slippage,
974                          commission,
975                          set_slippage,
976                          set_commission,
977                          order,
978                          record,
979                          sid)
980 def initialize(context):
981     model = slippage.FixedSlippage(spread=0.10)
982     set_slippage(model)
983     set_commission(commission.PerTrade(100.00))
984     context.count = 1
985     context.incr = 0
986 def handle_data(context, data):
987     if context.incr &lt; context.count:
988         order(sid(0), -1000)
989     record(price=data.current(sid(0), "price"))
990     context.incr += 1""",
991         )
992         results = test_algo.run()
993         all_txns = [val for sublist in results["transactions"].tolist()
994                     for val in sublist]
995         self.assertEqual(len(all_txns), 1)
996         expected_spread = 0.05
997         expected_price = test_algo<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.recorded_vars["price"] - expected_spread
998         self.assertEqual(expected_price, txn['price'])
999         self.assertEqual(9850, results.capital_used[1])
1000         self.assertEqual(100, results["orders"].iloc[1][0][</b></font>"commission"])
1001     @parameterized.expand(
1002         [
1003             ('no_minimum_commission', 0,),
1004             ('default_minimum_commission', 0,),
1005             ('alternate_minimum_commission', 2,),
1006         ]
1007     )
1008     def test_volshare_slippage(self, name, minimum_commission):
1009         tempdir = TempDirectory()
1010         try:
1011             if name == "default_minimum_commission":
1012                 commission_line = "set_commission(commission.PerShare(0.02))"
1013             else:
1014                 commission_line = \
1015                     "set_commission(commission.PerShare(0.02, " \
1016                     "min_trade_cost={0}))".format(minimum_commission)
1017                 [0], self.sim_params, self.asset_finder, self.trading_calendar
1018             )
1019             data_portal = create_data_portal_from_trade_history<font color="#5eac10"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(
1020                 self.asset_finder, self.trading_calendar, tempdir,
1021                 self.sim_params, {0: trades}
1022             )
1023             test_algo = self.make_algo(
1024                 data_portal=data_portal,
1025                 script="""
1026 from zipline.api import *
1027 def initialize(context):
1028     model = slippage.VolumeShareSlippage(
1029                             volume_limit=.3,
1030                             price_impact=0.05
1031                        )
1032     set_slippage(model)
1033     {0}
1034     context.count = 2
1035     context.incr = 0
1036 def handle_data(context, data):
1037     if context.incr &lt; context.count:
1038         order(sid(0), 5000)
1039     record(price=data.current(sid(0), "price"))
1040     record(volume=data.current(sid(0), "volume"))
1041     record(incr=context.incr)
1042     context.incr += 1
1043     """.format(commission_line),
1044             )
1045             results =</b></font> test_algo.run()
1046             all_txns = [
1047                 val for sublist in results["transactions"].tolist()
1048                 for val in sublist]
1049             self.assertEqual(len(all_txns), 67)
1050             all_orders = list(toolz.concat(results['orders']))
1051             if minimum_commission == 0:
1052                 for order_ in all_orders:
1053                     self.assertAlmostEqual(
1054                         order_["filled"] * 0.02,
1055                         order_["commission"]
1056                     )
1057             else:
1058                 for order_ in all_orders:
1059                     if order_["filled"] &gt; 0:
1060                         self.assertAlmostEqual(
1061                             max(order_["filled"] * 0.02, minimum_commission),
1062                             order_["commission"]
1063                         )
1064                     else:
1065                         self.assertEqual(0, order_["commission"])
1066         finally:
1067             tempdir.cleanup()
1068     def test_incorrectly_set_futures_slippage_model(self):
1069         code = dedent(
1070             """
1071             from zipline.api import set_slippage, slippage
1072             class MySlippage(slippage.FutureSlippageModel):
1073                 def process_order(self, data, order):
1074                     return data.current(order.asset, 'price'), order.amount
1075             def initialize(context):
1076                 set_slippage(MySlippage())
1077             """
1078         )
1079         test_algo = self.make_algo(script=code)
1080         with self.assertRaises(IncompatibleSlippageModel):
1081             test_algo.run()
1082     def test_algo_record_vars(self):
1083         test_algo = self.make_algo(script=record_variables)
1084         results = test_algo.run()
1085         for i in range(1, 252):
1086             self.assertEqual(results.iloc[i-1]["incr"], i)
1087     def test_algo_record_nan(self):
1088         test_algo = self.make_algo(script=record_float_magic % 'nan')
1089         results = test_algo.run()
1090         for i in range(1, 252):
1091             self.assertTrue(np.isnan(results.iloc[i-1]["data"]))
1092     def test_batch_market_order_matches_multiple_manual_orders(self):
1093         share_counts = pd.Series([50, 100])
1094         multi_blotter = RecordBatchBlotter()
1095         multi_test_algo = self.make_algo(
1096             script=dedent("""\
1097                 from collections import OrderedDict
1098                 from six import iteritems
1099                 from zipline.api import sid, order
1100                 def initialize(context):
1101                     context.assets = [sid(0), sid(3)]
1102                     context.placed = False
1103                 def handle_data(context, data):
1104                     if not context.placed:
1105                         it = zip(context.assets, {share_counts})
1106                         for asset, shares in it:
1107                             order(asset, shares)
1108                         context.placed = True
1109             """).format(share_counts=list(share_counts)),
1110             blotter=multi_blotter,
1111         )
1112         multi_stats = multi_test_algo.run()
1113         self.assertFalse(multi_blotter.order_batch_called)
1114         batch_blotter = RecordBatchBlotter()
1115         batch_test_algo = self.make_algo(
1116             script=dedent("""\
1117                 import pandas as pd
1118                 from zipline.api import sid, batch_market_order
1119                 def initialize(context):
1120                     context.assets = [sid(0), sid(3)]
1121                     context.placed = False
1122                 def handle_data(context, data):
1123                     if not context.placed:
1124                         orders = batch_market_order(pd.Series(
1125                             index=context.assets, data={share_counts}
1126                         ))
1127                         assert len(orders) == 2, \
1128                             "len(orders) was %s but expected 2" % len(orders)
1129                         for o in orders:
1130                             assert o is not None, "An order is None"
1131                         context.placed = True
1132             """).format(share_counts=list(share_counts)),
1133             blotter=batch_blotter,
1134         )
1135         batch_stats = batch_test_algo.run()
1136         self.assertTrue(batch_blotter.order_batch_called)
1137         for stats in (multi_stats, batch_stats):
1138             stats.orders = stats.orders.apply(
1139                 lambda orders: [toolz.dissoc(o, 'id') for o in orders]
1140             )
1141             stats.transactions = stats.transactions.apply(
1142                 lambda txns: [toolz.dissoc(txn, 'order_id') for txn in txns]
1143             )
1144         assert_equal(multi_stats, batch_stats)
1145     def test_batch_market_order_filters_null_orders(self):
1146         share_counts = [50, 0]
1147         batch_blotter = RecordBatchBlotter()
1148         batch_test_algo = self.make_algo(
1149             script=dedent("""\
1150                 import pandas as pd
1151                 from zipline.api import sid, batch_market_order
1152                 def initialize(context):
1153                     context.assets = [sid(0), sid(3)]
1154                     context.placed = False
1155                 def handle_data(context, data):
1156                     if not context.placed:
1157                         orders = batch_market_order(pd.Series(
1158                             index=context.assets, data={share_counts}
1159                         ))
1160                         assert len(orders) == 1, \
1161                             "len(orders) was %s but expected 1" % len(orders)
1162                         for o in orders:
1163                             assert o is not None, "An order is None"
1164                         context.placed = True
1165             """).format(share_counts=share_counts),
1166             blotter=batch_blotter,
1167         )
1168         batch_test_algo.run()
1169         self.assertTrue(batch_blotter.order_batch_called)
1170     def test_order_dead_asset(self):
1171         params = SimulationParameters(
1172             start_session=pd.Timestamp("2007-01-03", tz='UTC'),
1173             end_session=pd.Timestamp("2007-01-05", tz='UTC'),
1174             trading_calendar=self.trading_calendar,
1175         )
1176         self.run_algorithm(
1177             script="""
1178 from zipline.api import order, sid
1179 def initialize(context):
1180     pass
1181 def handle_data(context, data):
1182     order(sid(0), 10)
1183         """,
1184         )
1185         for order_str in ["order_value", "order_percent"]:
1186             test_algo = self.make_algo(
1187                 script="""
1188 from zipline.api import order_percent, order_value, sid
1189 def initialize(context):
1190     pass
1191 def handle_data(context, data):
1192     {0}(sid(0), 10)
1193         """.format(order_str),
1194                 sim_params=params,
1195             )
1196         with self.assertRaises(CannotOrderDelistedAsset):
1197             test_algo.run()
1198     def test_portfolio_in_init(self):
1199         """
1200         Test that accessing portfolio in init doesn't break.
1201         """
1202         self.run_algorithm(script=access_portfolio_in_init)
1203     def test_account_in_init(self):
1204         """
1205         Test that accessing account in init doesn't break.
1206         """
1207         self.run_algorithm(script=access_account_in_init)
1208     def test_without_kwargs(self):
1209         """
1210         Test that api methods on the data object can be called with positional
1211         arguments.
1212         """
1213         params = SimulationParameters(
1214             start_session=pd.Timestamp("2006-01-10", tz='UTC'),
1215             end_session=pd.Timestamp("2006-01-11", tz='UTC'),
1216             trading_calendar=self.trading_calendar,
1217         )
1218         self.run_algorithm(sim_params=params, script=call_without_kwargs)
1219     def test_good_kwargs(self):
1220         """
1221         Test that api methods on the data object can be called with keyword
1222         arguments.
1223         """
1224         params = SimulationParameters(
1225             start_session=pd.Timestamp("2006-01-10", tz='UTC'),
1226             end_session=pd.Timestamp("2006-01-11", tz='UTC'),
1227             trading_calendar=self.trading_calendar,
1228         )
1229         self.run_algorithm(script=call_with_kwargs, sim_params=params)
1230     @parameterized.expand([('history', call_with_bad_kwargs_history),
1231                            ('current', call_with_bad_kwargs_current)])
1232     def test_bad_kwargs(self, name, algo_text):
1233         """
1234         Test that api methods on the data object called with bad kwargs return
1235         a meaningful TypeError that we create, rather than an unhelpful cython
1236         error
1237         """
1238         algo = self.make_algo(script=algo_text)
1239         with self.assertRaises(TypeError) as cm:
1240             algo.run()
1241         self.assertEqual("%s() got an unexpected keyword argument 'blahblah'"
1242                          % name, cm.exception.args[0])
1243     @parameterized.expand(ARG_TYPE_TEST_CASES)
1244     def test_arg_types(self, name, inputs):
1245         keyword = name.split('__')[1]
1246         algo = self.make_algo(script=inputs[0])
1247         with self.assertRaises(TypeError) as cm:
1248             algo.run()
1249         expected = "Expected %s argument to be of type %s%s" % (
1250             keyword,
1251             'or iterable of type ' if inputs[2] else '',
1252             inputs[1]
1253         )
1254         self.assertEqual(expected, cm.exception.args[0])
1255     def test_empty_asset_list_to_history(self):
1256         params = SimulationParameters(
1257             start_session=pd.Timestamp("2006-01-10", tz='UTC'),
1258             end_session=pd.Timestamp("2006-01-11", tz='UTC'),
1259             trading_calendar=self.trading_calendar,
1260         )
1261         self.run_algorithm(
1262             script=dedent("""
1263                 def initialize(context):
1264                     pass
1265                 def handle_data(context, data):
1266                     data.history([], "price", 5, '1d')
1267                 """),
1268             sim_params=params,
1269         )
1270     @parameterized.expand(
1271         [('bad_kwargs', call_with_bad_kwargs_get_open_orders),
1272          ('good_kwargs', call_with_good_kwargs_get_open_orders),
1273          ('no_kwargs', call_with_no_kwargs_get_open_orders)]
1274     )
1275     def test_get_open_orders_kwargs(self, name, script):
1276         algo = self.make_algo(script=script)
1277         if name == 'bad_kwargs':
1278             with self.assertRaises(TypeError) as cm:
1279                 algo.run()
1280                 self.assertEqual('Keyword argument `sid` is no longer '
1281                                  'supported for get_open_orders. Use `asset` '
1282                                  'instead.', cm.exception.args[0])
1283         else:
1284             algo.run()
1285     def test_empty_positions(self):
1286         """
1287         Test that when we try context.portfolio.positions[stock] on a stock
1288         for which we have no positions, we return a Position with values 0
1289         (but more importantly, we don't crash) and don't save this Position
1290         to the user-facing dictionary PositionTracker._positions_store
1291         """
1292         results = self.run_algorithm(script=empty_positions)
1293         num_positions = results.num_positions
1294         amounts = results.amounts
1295         self.assertTrue(all(num_positions == 0))
1296         self.assertTrue(all(amounts == 0))
1297     def test_schedule_function_time_rule_positionally_misplaced(self):
1298         """
1299         Test that when a user specifies a time rule for the date_rule argument,
1300         but no rule in the time_rule argument
1301         (e.g. schedule_function(func, &lt;time_rule&gt;)), we assume that means
1302         assign a time rule but no date rule
1303         """
1304         sim_params = factory.create_simulation_parameters(
1305             start=pd.Timestamp('2006-01-12', tz='UTC'),
1306             end=pd.Timestamp('2006-01-13', tz='UTC'),
1307             data_frequency='minute'
1308         )
1309         algocode = dedent("""
1310         from zipline.api import time_rules, schedule_function
1311         def do_at_open(context, data):
1312             context.done_at_open.append(context.get_datetime())
1313         def do_at_close(context, data):
1314             context.done_at_close.append(context.get_datetime())
1315         def initialize(context):
1316             context.done_at_open = []
1317             context.done_at_close = []
1318             schedule_function(do_at_open, time_rules.market_open())
1319             schedule_function(do_at_close, time_rules.market_close())
1320         def handle_data(algo, data):
1321             pass
1322         """)
1323         with warnings.catch_warnings(record=True) as w:
1324             warnings.simplefilter("ignore", PerformanceWarning)
1325             algo = self.make_algo(script=algocode, sim_params=sim_params)
1326             algo.run()
1327             self.assertEqual(len(w), 2)
1328             for i, warning in enumerate(w):
1329                 self.assertIsInstance(warning.message, UserWarning)
1330                 self.assertEqual(
1331                     warning.message.args[0],
1332                     'Got a time rule for the second positional argument '
1333                     'date_rule. You should use keyword argument '
1334                     'time_rule= when calling schedule_function without '
1335                     'specifying a date_rule'
1336                 )
1337                 self.assertEqual(warning.lineno, 13 + i)
1338         self.assertEqual(
1339             algo.done_at_open,
1340             [pd.Timestamp('2006-01-12 14:31:00', tz='UTC'),
1341              pd.Timestamp('2006-01-13 14:31:00', tz='UTC')]
1342         )
1343         self.assertEqual(
1344             algo.done_at_close,
1345             [pd.Timestamp('2006-01-12 20:59:00', tz='UTC'),
1346              pd.Timestamp('2006-01-13 20:59:00', tz='UTC')]
1347         )
1348 class TestCapitalChanges(zf.WithMakeAlgo, zf.ZiplineTestCase):
1349     START_DATE = pd.Timestamp('2006-01-03', tz='UTC')
1350     END_DATE = pd.Timestamp('2006-01-09', tz='UTC')
1351     sids = ASSET_FINDER_EQUITY_SIDS = (0, 1)
1352     DAILY_SID = 0
1353     MINUTELY_SID = 1
1354     BENCHMARK_SID = None
1355     @classmethod
1356     def make_equity_minute_bar_data(cls):
1357         minutes = cls.trading_calendar.minutes_in_range(
1358             cls.START_DATE,
1359             cls.END_DATE,
1360         )
1361         closes = np.arange(100, 100 + len(minutes), 1)
1362         opens = closes
1363         highs = closes + 5
1364         lows = closes - 5
1365         frame = pd.DataFrame(
1366             index=minutes,
1367             data={
1368                 'open': opens,
1369                 'high': highs,
1370                 'low': lows,
1371                 'close': closes,
1372                 'volume': 10000,
1373             },
1374         )
1375         yield cls.MINUTELY_SID, frame
1376     @classmethod
1377     def make_equity_daily_bar_data(cls, country_code, sids):
1378         days = cls.trading_calendar.sessions_in_range(
1379             cls.START_DATE,
1380             cls.END_DATE,
1381         )
1382         closes = np.arange(10.0, 10.0 + len(days), 1.0)
1383         opens = closes
1384         highs = closes + 0.5
1385         lows = closes - 0.5
1386         frame = pd.DataFrame(
1387             index=days,
1388             data={
1389                 'open': opens,
1390                 'high': highs,
1391                 'low': lows,
1392                 'close': closes,
1393                 'volume': 10000,
1394             },
1395         )
1396         yield cls.DAILY_SID, frame
1397     @parameterized.expand([
1398         ('target', 151000.0), ('delta', 50000.0)
1399     ])
1400     def test_capital_changes_daily_mode(self, change_type, value):
1401         capital_changes = {
1402             pd.Timestamp('2006-01-06', tz='UTC'):
1403                 {'type': change_type, 'value': value}
1404         }
1405         algocode = """
1406 from zipline.api import set_slippage, set_commission, slippage, commission, \
1407     schedule_function, time_rules, order, sid
1408 def initialize(context):
1409     set_slippage(slippage.FixedSlippage(spread=0))
1410     set_commission(commission.PerShare(0, 0))
1411     schedule_function(order_stuff, time_rule=time_rules.market_open())
1412 def order_stuff(context, data):
1413     order(sid(0), 1000)
1414 """
1415         algo = self.make_algo(
1416             script=algocode,
1417             capital_changes=capital_changes,
1418             sim_params=SimulationParameters(
1419                 start_session=self.START_DATE,
1420                 end_session=self.END_DATE,
1421                 trading_calendar=self.nyse_calendar,
1422             )
1423         )
1424         gen = algo.get_generator()
1425         results = list(gen)
1426         cumulative_perf = \
1427             [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
1428         daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
1429         capital_change_packets = \
1430             [r['capital_change'] for r in results if 'capital_change' in r]
1431         self.assertEqual(len(capital_change_packets), 1)
1432         self.assertEqual(
1433             capital_change_packets[0],
1434             {'date': pd.Timestamp('2006-01-06', tz='UTC'),
1435              'type': 'cash',
1436              'target': 151000.0 if change_type == 'target' else None,
1437              'delta': 50000.0})
1438         expected_daily = {}
1439         expected_capital_changes <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= np.array([
1440             0.0, 0.0, 0.0, 50000.0, 0.0
1441         ])
1442         expected_daily['returns'] = np.array([
1443             0.0,
1444             0.0,
1445             (100000.0 + 1000.0) / 100000.0 - 1.0,
1446             (151000.0 + 2000.0) / 151000.0 - 1.0,
1447             (153000.0 + 3000.0) / 153000.0 - 1.0,
1448         ])
1449         expected_daily['pnl'] = np.array([
1450             0.0,
1451             0.0,
1452             1000.00,  # 1000 shares * gain of 1
1453             2000.00,  # 2000 shares * gain of 1
1454             3000.00,  # 3000 shares * gain of 1
1455         ])
1456         expected_daily['capital_used'] = np.array([
1457             0.0,
1458             -11000.0,  # 1000 shares at price = 11
1459             -12000.0,  # 1000 shares at price = 12
1460             -13000.0,  # 1000 shares at price = 13
1461             -14000.0,  # 1000 shares at price = 14
1462         ])
1463         expected_daily['ending_cash'] = \
1464             np.array([100000.0] * 5) + \
1465             np.cumsum(expected_capital_changes) + \
1466             np.</b></font>cumsum(expected_daily['capital_used'])
1467         expected_daily['starting_cash'] = \
1468             expected_daily['ending_cash'] - \
1469             expected_daily['capital_used']
1470         expected_daily['starting_value'] = np.array([
1471             0.0,
1472             0.0,
1473             11000.0,  # 1000 shares at price = 11
1474             24000.0,  # 2000 shares at price = 12
1475             39000.0,  # 3000 shares at price = 13
1476         ])
1477         expected_daily['ending_value'] = \
1478             expected_daily['starting_value'] + \
1479             expected_daily['pnl'] - \
1480             expected_daily['capital_used']
1481         expected_daily['portfolio_value'] = \
1482             expected_daily['ending_value'] + \
1483             expected_daily['ending_cash']
1484         stats = [
1485             'returns', 'pnl', 'capital_used', 'starting_cash', 'ending_cash',
1486             'starting_value', 'ending_value', 'portfolio_value'
1487         expected_cumulative = {
1488             'returns': np.cumprod(expected_daily<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['returns'] + 1) - 1,
1489             'pnl': np.cumsum(expected_daily['pnl']),
1490             'capital_used': np.cumsum(expected_daily['capital_used']),
1491             'starting_cash':
1492                 np.repeat(expected_daily['starting_cash'][0:1], 5),
1493             'ending_cash': expected_daily['ending_cash'],
1494             'starting_value':
1495                 np.repeat(expected_daily['starting_value'][0:1], 5),
1496             'ending_value': expected_daily['ending_value'],
1497             'portfolio_value': expected_daily[</b></font>'portfolio_value'],
1498         }
1499         for stat in stats:
1500             np.testing.assert_array_almost_equal(
1501                 np.array([perf[stat] for perf in daily_perf]),
1502                 expected_daily[stat],
1503                 err_msg='daily ' + stat,
1504             )
1505             np.testing.assert_array_almost_equal(
1506                 np.array([perf[stat] for perf in cumulative_perf]),
1507                 expected_cumulative[stat],
1508                 err_msg='cumulative ' + stat,
1509             )
1510         self.assertEqual(
1511             algo.capital_change_deltas,
1512             {pd.Timestamp('2006-01-06', tz='UTC'): 50000.0}
1513     @parameterized.expand([
1514         <font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('interday_target', [('2006-01-04', 2388.0)]),
1515         ('interday_delta', [('2006-01-04', 1000.0)]),
1516         ('intraday_target', [('2006-01-04 17:00', 2184.0),
1517                              ('2006-01-04 18:00', 2804.0)]),
1518         ('intraday_delta', [('2006-01-04 17:00', 500.0),
1519                             ('2006-01-04 18:00'</b></font>, 500.0)]),
1520     ])
1521     def test_capital_changes_minute_mode_daily_emission(self, change, values):
1522         change_loc, change_type = change.split('_')
1523         sim_params = SimulationParameters(
1524             start_session=pd.Timestamp('2006-01-03', tz='UTC'),
1525             end_session=pd.Timestamp('2006-01-05', tz='UTC'),
1526             data_frequency='minute',
1527             capital_base=1000.0,
1528             trading_calendar=self.nyse_calendar,
1529         )
1530         capital_changes = {
1531             pd.Timestamp(datestr, tz='UTC'): {
1532                 'type': change_type,
1533                 'value': value
1534             }
1535             for datestr, value in values
1536         }
1537         algocode = """
1538 from zipline.api import set_slippage, set_commission, slippage, commission, \
1539     schedule_function, time_rules, order, sid
1540 def initialize(context):
1541     set_slippage(slippage.FixedSlippage(spread=0))
1542     set_commission(commission.PerShare(0, 0))
1543     schedule_function(order_stuff, time_rule=time_rules.market_open())
1544 def order_stuff(context, data):
1545     order(sid(1), 1)
1546 """
1547         algo = self.make_algo(
1548             script=algocode,
1549             sim_params=sim_params,
1550             capital_changes=capital_changes
1551         )
1552         gen = algo.get_generator()
1553         results = list(gen)
1554         cumulative_perf = \
1555             [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
1556         daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
1557         capital_change_packets = \
1558             [r['capital_change'] for r in results if 'capital_change' in r]
1559         self.assertEqual(len(capital_change_packets), len(capital_changes))
1560         expected = [
1561             {'date': pd.Timestamp(val[0], tz='UTC'),
1562              'type': 'cash',
1563              'target': val[1] if change_type == 'target' else None,
1564              'delta': 1000.0 if len(values) == 1 else 500.0}
1565             for val in values]
1566         self.assertEqual(capital_change_packets, expected)
1567         expected_daily = {}
1568         expected_capital_changes = np.array([0.0, 1000.0, 0.0])
1569         if change_loc == 'intraday':
1570             day2_return = (
1571                 (1388.0 + 149.0 + 147.0) / 1388.0 *
1572                 (2184.0 + 60.0 + 60.0) / 2184.0 *
1573                 (2804.0 + 181.0 + 181.0) / 2804.0 - 1.0
1574             )
1575         else:
1576             day2_return = (2388.0 + 390.0 + 388.0) / 2388.0 - 1
1577         expected_daily<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['returns'] = np.array([
1578             (1000.0 + 489 - 101) / 1000.0 - 1.0,
1579             day2_return,
1580             (3166.0 + 390.0 + 390.0 + 388.0) / 3166.0 - 1.0,
1581         ])
1582         expected_daily['pnl'] = np.array([
1583             388.0,
1584             390.0 + 388.0,
1585             390.0 + 390.0 + 388.0,
1586         ])
1587         expected_daily['capital_used'] = np.array([
1588             -101.0, -491.0, -881.0
1589         ])
1590         expected_daily['ending_cash'] = \
1591             np.array([1000.0] * 3) + \
1592             np.cumsum(expected_capital_changes) + \
1593             np.</b></font>cumsum(expected_daily['capital_used'])
1594         expected_daily['starting_cash'] = \
1595             expected_daily['ending_cash'] - \
1596             expected_daily['capital_used']
1597         if change_loc == 'intraday':
1598             expected_daily['starting_cash'] -= expected_capital_changes
1599         expected_daily['starting_value'] = np.array([
1600             0.0, 489.0, 879.0 * 2
1601         ])
1602         expected_daily['ending_value'] = \
1603             expected_daily['starting_value'] + \
1604             expected_daily['pnl'] - \
1605             expected_daily['capital_used']
1606         expected_daily['portfolio_value'] = \
1607             expected_daily['ending_value'] + \
1608             expected_daily['ending_cash']
1609         stats = [
1610             'returns', 'pnl', 'capital_used', 'starting_cash', 'ending_cash',
1611             'starting_value', 'ending_value', 'portfolio_value'
1612         expected_cumulative = {
1613             'returns': np.cumprod(expected_daily<font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['returns'] + 1) - 1,
1614             'pnl': np.cumsum(expected_daily['pnl']),
1615             'capital_used': np.cumsum(expected_daily['capital_used']),
1616             'starting_cash':
1617                 np.repeat(expected_daily['starting_cash'][0:1], 3),
1618             'ending_cash': expected_daily['ending_cash'],
1619             'starting_value':
1620                 np.repeat(expected_daily['starting_value'][</b></font>0:1], 3),
1621             'ending_value': expected_daily['ending_value'],
1622             'portfolio_value': expected_daily['portfolio_value'],
1623         }
1624         for stat in stats:
1625             np.testing.assert_array_almost_equal(
1626                 np.array([perf[stat] for perf in daily_perf]),
1627                 expected_daily[stat]
1628             )
1629             np.testing.assert_array_almost_equal(
1630                 np.array([perf[stat] for perf in cumulative_perf]),
1631                 expected_cumulative[stat]
1632             )
1633         if change_loc == 'interday':
1634             self.assertEqual(
1635                 algo.capital_change_deltas,
1636                 {pd.Timestamp('2006-01-04', tz='UTC'): 1000.0}
1637             )
1638         else:
1639             self.assertEqual(
1640                 algo.capital_change_deltas,
1641                 {pd.Timestamp('2006-01-04 17:00', tz='UTC'): 500.0,
1642                  pd.Timestamp('2006-01-04 18:00', tz='UTC'): 500.0}
1643     @parameterized.expand([
1644         <font color="#827d6b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('interday_target', [('2006-01-04', 2388.0)]),
1645         ('interday_delta', [('2006-01-04', 1000.0)]),
1646         ('intraday_target', [('2006-01-04 17:00', 2184.0),
1647                              ('2006-01-04 18:00', 2804.0)]),
1648         ('intraday_delta'</b></font>, [('2006-01-04 17:00', 500.0),
1649                             ('2006-01-04 18:00', 500.0)]),
1650     ])
1651     def test_capital_changes_minute_mode_minute_emission(self, change, values):
1652         change_loc, change_type = change.split('_')
1653         sim_params = SimulationParameters(
1654             start_session=pd.Timestamp('2006-01-03', tz='UTC'),
1655             end_session=pd.Timestamp('2006-01-05', tz='UTC'),
1656             data_frequency='minute',
1657             emission_rate='minute',
1658             capital_base=1000.0,
1659             trading_calendar=self.nyse_calendar,
1660         )
1661         capital_changes = {pd.Timestamp(val[0], tz='UTC'): {
1662             'type': change_type, 'value': val[1]} for val in values}
1663         algocode = """
1664 from zipline.api import set_slippage, set_commission, slippage, commission, \
1665     schedule_function, time_rules, order, sid
1666 def initialize(context):
1667     set_slippage(slippage.FixedSlippage(spread=0))
1668     set_commission(commission.PerShare(0, 0))
1669     schedule_function(order_stuff, time_rule=time_rules.market_open())
1670 def order_stuff(context, data):
1671     order(sid(1), 1)
1672 """
1673         algo = self.make_algo(
1674             script=algocode,
1675             sim_params=sim_params,
1676             capital_changes=capital_changes
1677         )
1678         gen = algo.get_generator()
1679         results = list(gen)
1680         cumulative_perf = \
1681             [r['cumulative_perf'] for r in results if 'cumulative_perf' in r]
1682         minute_perf = [r['minute_perf'] for r in results if 'minute_perf' in r]
1683         daily_perf = [r['daily_perf'] for r in results if 'daily_perf' in r]
1684         capital_change_packets = \
1685             [r['capital_change'] for r in results if 'capital_change' in r]
1686         self.assertEqual(len(capital_change_packets), len(capital_changes))
1687         expected = [
1688             {'date': pd.Timestamp(val[0], tz='UTC'),
1689              'type': 'cash',
1690              'target': val[1] if change_type == 'target' else None,
1691              'delta': 1000.0 if len(values) == 1 else 500.0}
1692             for val in values]
1693         self.assertEqual(capital_change_packets, expected)
1694         expected_minute = {}
1695         capital_changes_after_start = np.array([0.0] * 1170)
1696         if change_loc == 'intraday':
1697             capital_changes_after_start[539:599] = 500.0
1698         expected_minute['pnl'] = np.array([0.0] * 1170)
1699         expected_minute<font color="#736aff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>['pnl'][:2] = 0.0
1700         expected_minute['pnl'][2:392] = 1.0
1701         expected_minute['pnl'][392:782] = 2.0
1702         expected_minute['pnl'][782:] =</b></font> 3.0
1703         for start, end in ((0, 390), (390, 780), (780, 1170)):
1704             expected_minute['pnl'][start:end] = \
1705                 np.cumsum(expected_minute['pnl'][start:end])
1706         expected_minute['capital_used'] = np.concatenate((
1707             [0.0] * 1, [-101.0] * 389,
1708             [0.0] * 1, [-491.0] * 389,
1709             [0.0] * 1, [-881.0] * 389,
1710         ))
1711         day2adj = 0.0 if change_loc == 'intraday' else 1000.0
1712         expected_minute['starting_cash'] = np.concatenate((
1713             [1000.0] * 390,
1714             [1000.0 - 101.0 + day2adj] * 390,
1715             [1000.0 - 101.0 - 491.0 + 1000] * 390
1716         ))
1717         expected_minute['ending_cash'] = \
1718             expected_minute['starting_cash'] + \
1719             expected_minute['capital_used'] + \
1720             capital_changes_after_start
1721         expected_minute['starting_value'] = np.concatenate((
1722             [0.0] * 390,
1723             [489.0] * 390,
1724             [879.0 * 2] * 390
1725         ))
1726         expected_minute['ending_value'] = \
1727             expected_minute['starting_value'] + \
1728             expected_minute['pnl'] - \
1729             expected_minute['capital_used']
1730         expected_minute['portfolio_value'] = \
1731             expected_minute['ending_value'] + \
1732             expected_minute['ending_cash']
1733         expected_minute['returns'] = \
1734             expected_minute['pnl'] / \
1735             (expected_minute['starting_value'] +
1736              expected_minute['starting_cash'])
1737         if change_loc == 'intraday':
1738             prev_subperiod_return = expected_minute['returns'][538]
1739             cur_subperiod_pnl = \
1740                 expected_minute['pnl']<font color="#79764d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[539:599] - expected_minute['pnl'][538]
1741             cur_subperiod_starting_value = \
1742                 np.array([expected_minute['ending_value'][538]] * 60)
1743             cur_subperiod_starting_cash = \
1744                 np.array([expected_minute['ending_cash'][</b></font>538] + 500] * 60)
1745             cur_subperiod_returns = cur_subperiod_pnl / \
1746                 (cur_subperiod_starting_value + cur_subperiod_starting_cash)
1747             expected_minute['returns'][539:599] = \
1748                 (cur_subperiod_returns + 1.0) * \
1749                 (prev_subperiod_return + 1.0) - \
1750                 1.0
1751             prev_subperiod_return = expected_minute['returns'][598]
1752             cur_subperiod_pnl = \
1753                 expected_minute['pnl']<font color="#f660ab"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[599:780] - expected_minute['pnl'][598]
1754             cur_subperiod_starting_value = \
1755                 np.array([expected_minute['ending_value'][598]] * 181)
1756             cur_subperiod_starting_cash = \
1757                 np.array([expected_minute['ending_cash'][</b></font>598] + 500] * 181)
1758             cur_subperiod_returns = cur_subperiod_pnl / \
1759                 (cur_subperiod_starting_value + cur_subperiod_starting_cash)
1760             expected_minute['returns'][599:780] = \
1761                 (cur_subperiod_returns + 1.0) * \
1762                 (prev_subperiod_return + 1.0) - \
1763                 1.0
1764         expected_daily = {
1765             k: np.array([v[389], v[779], v[1169]])
1766             for k, v in iteritems(expected_minute)
1767         }
1768         stats = [
1769             'pnl', 'capital_used', 'starting_cash', 'ending_cash',
1770             'starting_value', 'ending_value', 'portfolio_value', 'returns'
1771         ]
1772         expected_cumulative = deepcopy(expected_minute)
1773         expected_cumulative['returns'][390:] = \
1774             (expected_cumulative['returns'][390:] + 1) * \
1775             (expected_daily['returns'][0] + 1) - 1
1776         expected_cumulative['returns'][780:] = \
1777             (expected_cumulative['returns'][780:] + 1) * \
1778             (expected_daily['returns'][1] + 1) - 1
1779         expected_cumulative['pnl'][390:] += expected_daily['pnl'][0]
1780         expected_cumulative['pnl'][780:] += expected_daily['pnl'][1]
1781             expected_daily['capital_used'][0]
1782         expected_cumulative['capital_used'][780:] += \
1783             expected_daily['capital_used']<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[1]
1784         expected_cumulative['starting_cash'] = \
1785             np.repeat(expected_daily['starting_cash'][0:1], 1170)
1786         expected_cumulative['starting_value'] = \
1787             np.repeat(expected_daily['starting_value'][</b></font>0:1], 1170)
1788         for stat in stats:
1789             for i in (390, 781, 1172):
1790                 expected_cumulative[stat] = np.insert(
1791                     expected_cumulative[stat],
1792                     i,
1793                     expected_cumulative[stat][i-1]
1794                 )
1795         for stat in stats:
1796             np.testing.assert_array_almost_equal(
1797                 np.array([perf[stat] for perf in minute_perf]),
1798                 expected_minute[stat]
1799             )
1800             np.testing.assert_array_almost_equal(
1801                 np.array([perf[stat] for perf in daily_perf]),
1802                 expected_daily[stat]
1803             )
1804             np.testing.assert_array_almost_equal(
1805                 np.array([perf[stat] for perf in cumulative_perf]),
1806                 expected_cumulative[stat]
1807             )
1808         if change_loc == 'interday':
1809             self.assertEqual(
1810                 algo.capital_change_deltas,
1811                 {pd.Timestamp('2006-01-04', tz='UTC'): 1000.0}
1812             )
1813         else:
1814             self.assertEqual(
1815                 algo.capital_change_deltas,
1816                 {pd.Timestamp('2006-01-04 17:00', tz='UTC'): 500.0,
1817                  pd.Timestamp('2006-01-04 18:00', tz='UTC'): 500.0}
1818             )
1819 class TestGetDatetime(zf.WithMakeAlgo, zf.ZiplineTestCase):
1820     SIM_PARAMS_DATA_FREQUENCY = 'minute'
1821     START_DATE = to_utc('2014-01-02 9:31')
1822     END_DATE = to_utc('2014-01-03 9:31')
1823     ASSET_FINDER_EQUITY_SIDS = 0, 1
1824     BENCHMARK_SID = None
1825     @parameterized.expand(
1826         [
1827             ('default', None,),
1828             ('utc', 'UTC',),
1829             ('us_east', 'US/Eastern',),
1830         ]
1831     )
1832     def test_get_datetime(self, name, tz):
1833         algo = dedent(
1834             """
1835             import pandas as pd
1836             from zipline.api import get_datetime
1837             def initialize(context):
1838                 context.tz = {tz} or 'UTC'
1839                 context.first_bar = True
1840             def handle_data(context, data):
1841                 dt = get_datetime({tz})
1842                 if dt.tz.zone != context.tz:
1843                     raise ValueError("Mismatched Zone")
1844                 if context.first_bar:
1845                     if dt.tz_convert("US/Eastern").hour != 9:
1846                         raise ValueError("Mismatched Hour")
1847                     elif dt.tz_convert("US/Eastern").minute != 31:
1848                         raise ValueError("Mismatched Minute")
1849                     context.first_bar = False
1850             """.format(tz=repr(tz))
1851         )
1852         algo = self.make_algo(script=algo)
1853         algo.run()
1854         self.assertFalse(algo.first_bar)
1855 class TestTradingControls(zf.WithMakeAlgo,
1856                           zf.ZiplineTestCase):
1857     START_DATE = pd.Timestamp('2006-01-03', tz='utc')
1858     END_DATE = pd.Timestamp('2006-01-06', tz='utc')
1859     sid = 133
1860     sids = ASSET_FINDER_EQUITY_SIDS = 133, 134
1861     SIM_PARAMS_DATA_FREQUENCY = 'daily'
1862     DATA_PORTAL_USE_MINUTE_DATA = True
1863     @classmethod
1864     def init_class_fixtures(cls):
1865         super(TestTradingControls, cls).init_class_fixtures()
1866         cls.asset = cls.asset_finder.retrieve_asset(cls.sid)
1867         cls.another_asset = cls.asset_finder.retrieve_asset(134)
1868     def _check_algo(self,
1869                     algo,
1870                     expected_order_count,
1871                     expected_exc):
1872         with self.assertRaises(expected_exc) if expected_exc else nop_context:
1873             algo.run()
1874         self.assertEqual(algo.order_count, expected_order_count)
1875     def check_algo_succeeds(self, algo, order_count=4):
1876         self._check_algo(algo, order_count, None)
1877     def check_algo_fails(self, algo, order_count):
1878         self._check_algo(algo,
1879                          order_count,
1880                          TradingControlViolation)
1881     def test_set_max_position_size(self):
1882         def initialize(self, asset, max_shares, max_notional):
1883             self.set_slippage(FixedSlippage())
1884             self.order_count = 0
1885             self.set_max_position_size(asset=asset,
1886                                        max_shares=max_shares,
1887                                        max_notional=max_notional)
1888         def handle_data(algo, data):
1889             algo.order(algo.sid(self.sid), 1)
1890             algo.order_count += 1
1891         algo = self.make_algo(
1892             asset=self.asset,
1893             max_shares=10,
1894             max_notional=500.0,
1895             initialize=initialize,
1896             handle_data=handle_data,
1897         )
1898         self.check_algo_succeeds(algo)
1899         def handle_data(algo, data):
1900             algo.order(algo.sid(self.sid), 3)
1901             algo.order_count += 1
1902         algo = self.make_algo(
1903             asset=self.asset,
1904             max_shares=10,
1905             max_notional=500.0,
1906             initialize=initialize,
1907             handle_data=handle_data,
1908         )
1909         self.check_algo_fails(algo, 3)
1910         def handle_data(algo, data):
1911             algo.order(algo.sid(self.sid), 3)
1912             algo.order_count += 1
1913         algo = self.make_algo(
1914             asset=self.asset,
1915             max_shares=10,
1916             max_notional=67.0,
1917             initialize=initialize,
1918             handle_data=handle_data,
1919         )
1920         self.check_algo_fails(algo, 2)
1921         def handle_data(algo, data):
1922             algo.order(algo.sid(self.sid), 10000)
1923             algo.order_count += 1
1924         algo = self.make_algo(
1925             asset=self.another_asset,
1926             max_shares=10,
1927             max_notional=67.0,
1928             initialize=initialize,
1929             handle_data=handle_data,
1930         )
1931         self.check_algo_succeeds(algo)
1932         def handle_data(algo, data):
1933             algo.order(algo.sid(self.sid), 10000)
1934             algo.order_count += 1
1935         algo = self.make_algo(
1936             max_shares=10,
1937             max_notional=61.0,
1938             asset=None,
1939             initialize=initialize,
1940             handle_data=handle_data,
1941         )
1942         self.check_algo_fails(algo, 0)
1943     def test_set_asset_restrictions(self):
1944         def initialize(algo, sid, restrictions, on_error):
1945             algo.order_count = 0
1946         def handle_data(algo, data):
1947             algo.could_trade = data<font color="#5b8daf"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.can_trade(algo.sid(self.sid))
1948             algo.order(algo.sid(self.sid), 100)
1949             algo.order_count +=</b></font> 1
1950         rlm = HistoricalRestrictions([
1951             Restriction(
1952                 self.sid,
1953                 self.sim_params.start_session,
1954                 RESTRICTION_STATES.FROZEN)
1955         ])
1956         algo = self.make_algo(
1957             sid=self.sid,
1958             restrictions=rlm,
1959             on_error='fail',
1960             initialize=initialize,
1961             handle_data=handle_data,
1962         )
1963         self.check_algo_fails(algo, 0)
1964         self.assertFalse(algo.could_trade)
1965         rlm = StaticRestrictions([self.sid])
1966         algo = self.make_algo(
1967             sid=self.sid,
1968             restrictions=rlm,
1969             on_error='fail',
1970             initialize=initialize,
1971             handle_data=handle_data,
1972         )
1973         self.check_algo_fails(algo, 0)
1974         self.assertFalse(algo.could_trade)
1975         algo = self.make_algo(
1976             sid=self.sid,
1977             restrictions=rlm,
1978             on_error='log',
1979             initialize=initialize,
1980             handle_data=handle_data,
1981         )
1982         with make_test_handler(self) as log_catcher:
1983             self.check_algo_succeeds(algo)
1984         logs = [r.message for r in log_catcher.records]
1985         self.assertIn("Order for 100 shares of Equity(133 [A]) at "
1986                       "2006-01-03 21:00:00+00:00 violates trading constraint "
1987                       "RestrictedListOrder({})", logs)
1988         self.assertFalse(algo.could_trade)
1989         rlm = HistoricalRestrictions([
1990             Restriction(
1991                 sid,
1992                 self.sim_params.start_session,
1993                 RESTRICTION_STATES.FROZEN) for sid in [134, 135, 136]
1994         ])
1995         algo = self.make_algo(
1996             sid=self.sid,
1997             restrictions=rlm,
1998             on_error='fail',
1999             initialize=initialize,
2000             handle_data=handle_data,
2001         )
2002         self.check_algo_succeeds(algo)
2003         self.assertTrue(algo.could_trade)
2004     @parameterized.expand([
2005         ('order_first_restricted_sid', 0),
2006         ('order_second_restricted_sid', 1)
2007     ])
2008     def test_set_multiple_asset_restrictions(self, name, to_order_idx):
2009         def initialize(algo, restrictions1, restrictions2, on_error):
2010             algo.order_count = 0
2011             algo.set_asset_restrictions(restrictions1, on_error)
2012             algo.set_asset_restrictions(restrictions2, on_error)
2013         def handle_data(algo, data):
2014             algo.could_trade1 = data.can_trade(algo.sid(self.sids[0]))
2015             algo.could_trade2 = data.can_trade(algo.sid(self.sids[1]))
2016             algo.order(algo.sid(self.sids[to_order_idx]), 100)
2017             algo.order_count += 1
2018         rl1 = StaticRestrictions([self.sids[0]])
2019         rl2 = StaticRestrictions([self.sids[1]])
2020         algo = self.make_algo(
2021             restrictions1=rl1,
2022             restrictions2=rl2,
2023             initialize=initialize,
2024             handle_data=handle_data,
2025             on_error='fail',
2026         )
2027         self.check_algo_fails(algo, 0)
2028         self.assertFalse(algo.could_trade1)
2029         self.assertFalse(algo.could_trade2)
2030     def test_set_do_not_order_list(self):
2031         def initialize(self, restricted_list):
2032             self.order_count = 0
2033             self.set_do_not_order_list(restricted_list, on_error='fail')
2034         def handle_data(algo, data):
2035             algo.could_trade = data.can_trade(algo.sid(self.sid))
2036             algo.order(algo.sid(self.sid), 100)
2037             algo.order_count += 1
2038         rlm = [self.sid]
2039         algo = self.make_algo(
2040             restricted_list=rlm,
2041             initialize=initialize,
2042             handle_data=handle_data,
2043         )
2044         self.check_algo_fails(algo, 0)
2045         self.assertFalse(algo.could_trade)
2046     def test_set_max_order_size(self):
2047         def initialize(algo, asset, max_shares, max_notional):
2048             algo.order_count = 0
2049             algo.set_max_order_size(asset=asset,
2050                                     max_shares=max_shares,
2051                                     max_notional=max_notional)
2052         def handle_data(algo, data):
2053             algo.order(algo.sid(self.sid), 1)
2054             algo.order_count += 1
2055         algo = self.make_algo(
2056             initialize=initialize,
2057             handle_data=handle_data,
2058             asset=self.asset,
2059             max_shares=10,
2060             max_notional=500.0,
2061         )
2062         self.check_algo_succeeds(algo)
2063         def handle_data(algo, data):
2064             algo.order(algo.sid(self.sid), algo.order_count + 1)
2065             algo.order_count += 1
2066         algo = self.make_algo(
2067             initialize=initialize,
2068             handle_data=handle_data,
2069             asset=self.asset,
2070             max_shares=3,
2071             max_notional=500.0,
2072         )
2073         self.check_algo_fails(algo, 3)
2074         def handle_data(algo, data):
2075             algo.order(algo.sid(self.sid), algo.order_count + 1)
2076             algo.order_count += 1
2077         algo = self.make_algo(
2078             initialize=initialize,
2079             handle_data=handle_data,
2080             asset=self.asset,
2081             max_shares=10,
2082             max_notional=40.0,
2083         )
2084         self.check_algo_fails(algo, 3)
2085         def handle_data(algo, data):
2086             algo.order(algo.sid(self.sid), 10000)
2087             algo.order_count += 1
2088         algo = self.make_algo(
2089             initialize=initialize,
2090             handle_data=handle_data,
2091             asset=self.another_asset,
2092             max_shares=1,
2093             max_notional=1.0,
2094         )
2095         self.check_algo_succeeds(algo)
2096         def handle_data(algo, data):
2097             algo.order(algo.sid(self.sid), 10000)
2098             algo.order_count += 1
2099         algo = self.make_algo(
2100             initialize=initialize,
2101             handle_data=handle_data,
2102             asset=None,
2103             max_shares=1,
2104             max_notional=1.0,
2105         )
2106         self.check_algo_fails(algo, 0)
2107     def test_set_max_order_count(self):
2108         def initialize(algo, count):
2109             algo.order_count = 0
2110             algo.set_max_order_count(count)
2111         def handle_data(algo, data):
2112             for i in range(5):
2113                 algo.order(self.asset, 1)
2114                 algo.order_count += 1
2115         algo = self.make_algo(
2116             count=3,
2117             initialize=initialize,
2118             handle_data=handle_data,
2119         )
2120         with self.assertRaises(TradingControlViolation):
2121             algo.run()
2122         self.assertEqual(algo.order_count, 3)
2123     def test_set_max_order_count_minutely(self):
2124         sim_params = self.make_simparams(data_frequency='minute')
2125         def initialize(algo, max_orders_per_day):
2126             algo.minute_count = 0
2127             algo.order_count = 0
2128             algo.set_max_order_count(max_orders_per_day)
2129         def handle_data(algo, data):
2130             if algo.minute_count == 0 or algo.minute_count == 100:
2131                 for i in range(5):
2132                     algo.order(self.asset, 1)
2133                     algo.order_count += 1
2134             algo.minute_count += 1
2135         algo = self.make_algo(
2136             initialize=initialize,
2137             handle_data=handle_data,
2138             max_orders_per_day=9,
2139             sim_params=sim_params,
2140         )
2141         with self.assertRaises(TradingControlViolation):
2142             algo.run()
2143         self.assertEqual(algo.order_count, 9)
2144         def handle_data(algo, data):
2145             if (algo.minute_count % 390) == 0:
2146                 for i in range(5):
2147                     algo.order(self.asset, 1)
2148                     algo.order_count += 1
2149             algo.minute_count += 1
2150         algo <font color="#3ea99f"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= self.make_algo(
2151             initialize=initialize,
2152             handle_data=handle_data,
2153             max_orders_per_day=5,
2154             sim_params=sim_params,
2155         )
2156         algo.run()
2157         self.assertEqual(algo.</b></font>order_count, 20)
2158     def test_long_only(self):
2159         def initialize(algo):
2160             algo.order_count = 0
2161             algo.set_long_only()
2162         def handle_data(algo, data):
2163             algo.order(algo.sid(self.sid), -1)
2164             algo.order_count += 1
2165         algo = self.make_algo(initialize=initialize, handle_data=handle_data)
2166         self.check_algo_fails(algo, 0)
2167         def handle_data(algo, data):
2168             if (algo.order_count % 2) == 0:
2169                 algo.order(algo.sid(self.sid), 1)
2170             else:
2171                 algo.order(algo.sid(self.sid), -1)
2172             algo.order_count += 1
2173         algo = self.make_algo(initialize=initialize, handle_data=handle_data)
2174         self.check_algo_succeeds(algo)
2175         def handle_data(algo, data):
2176             amounts = [1, 1, 1, -3]
2177             algo.order(algo.sid(self.sid), amounts[algo.order_count])
2178             algo.order_count += 1
2179         algo = self.make_algo(initialize=initialize, handle_data=handle_data)
2180         self.check_algo_succeeds(algo)
2181         def handle_data(algo, data):
2182             amounts = [1, 1, 1, -4]
2183             algo.order(algo.sid(self.sid), amounts[algo.order_count])
2184             algo.order_count += 1
2185         algo = self.make_algo(initialize=initialize, handle_data=handle_data)
2186         self.check_algo_fails(algo, 3)
2187     def test_register_post_init(self):
2188         def initialize(algo):
2189             algo.initialized = True
2190         def handle_data(algo, data):
2191             with self.assertRaises(RegisterTradingControlPostInit):
2192                 algo.set_max_position_size(self.sid, 1, 1)
2193             with self.assertRaises(RegisterTradingControlPostInit):
2194                 algo.set_max_order_size(self.sid, 1, 1)
2195             with self.assertRaises(RegisterTradingControlPostInit):
2196                 algo.set_max_order_count(1)
2197             with self.assertRaises(RegisterTradingControlPostInit):
2198                 algo.set_long_only()
2199         self.run_algorithm(initialize=initialize, handle_data=handle_data)
2200 class TestAssetDateBounds(zf.WithMakeAlgo, zf.ZiplineTestCase):
2201     START_DATE = pd.Timestamp('2014-01-02', tz='UTC')
2202     END_DATE = pd.Timestamp('2014-01-03', tz='UTC')
2203     SIM_PARAMS_START_DATE = END_DATE  # Only run for one day.
2204     SIM_PARAMS_DATA_FREQUENCY = 'daily'
2205     DATA_PORTAL_USE_MINUTE_DATA = False
2206     BENCHMARK_SID = 3
2207     @classmethod
2208     def make_equity_info(cls):
2209         T = partial(pd.Timestamp, tz='UTC')
2210         return pd.DataFrame.from_records([
2211             {'sid': 1,
2212              'symbol': 'OLD',
2213              'start_date': T('1990'),
2214              'end_date': T('1991'),
2215              'exchange': 'TEST'},
2216             {'sid': 2,
2217              'symbol': 'NEW',
2218              'start_date': T('2017'),
2219              'end_date': T('2018'),
2220              'exchange': 'TEST'},
2221             {'sid': 3,
2222              'symbol': 'GOOD',
2223              'start_date': cls.START_DATE,
2224              'end_date': cls.END_DATE,
2225              'exchange': 'TEST'},
2226         ])
2227     def test_asset_date_bounds(self):
2228         def initialize(algo):
2229             algo.ran = False
2230             algo.register_trading_control(AssetDateBounds(on_error='fail'))
2231         def handle_data(algo, data):
2232             algo.order(algo.sid(3), 1)
2233             with self.assertRaises(TradingControlViolation):
2234                 algo.order(algo.sid(1), 1)
2235             with self.assertRaises(TradingControlViolation):
2236                 algo.order(algo.sid(2), 1)
2237             algo.ran = True
2238         algo = self.make_algo(initialize=initialize, handle_data=handle_data)
2239         algo.run()
2240         self.assertTrue(algo.ran)
2241 class TestAccountControls(zf.WithMakeAlgo,
2242                           zf.ZiplineTestCase):
2243     START_DATE = pd.Timestamp('2006-01-03', tz='utc')
2244     END_DATE = pd.Timestamp('2006-01-06', tz='utc')
2245     sidint, = ASSET_FINDER_EQUITY_SIDS = (133,)
2246     BENCHMARK_SID = None
2247     DATA_PORTAL_USE_MINUTE_DATA = False
2248     <font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>@classmethod
2249     def make_equity_daily_bar_data(cls, country_code, sids):
2250         frame = pd.DataFrame(data={
2251             'close': [10., 10., 11., 11.],
2252             'open': [10., 10., 11., 11.],
2253             'low': [9.5, 9.5, 10.45, 10.45],
2254             'high': [10.5, 10.5, 11.55, 11.55],
2255             'volume': [100, 100, 100, 300],
2256         }, index=cls.</b></font>equity_daily_bar_days)
2257         yield cls.sidint, frame
2258     def _check_algo(self, algo, expected_exc):
2259         with self.assertRaises(expected_exc) if expected_exc else nop_context:
2260             algo.run()
2261     def check_algo_succeeds(self, algo):
2262         self._check_algo(algo, None)
2263     def check_algo_fails(self, algo):
2264         self._check_algo(algo, AccountControlViolation)
2265     def test_set_max_leverage(self):
2266         def initialize(algo, max_leverage):
2267             algo.set_max_leverage(max_leverage=max_leverage)
2268         def handle_data(algo, data):
2269             algo.order(algo.sid(self.sidint), 1)
2270             algo.record(latest_time=algo.get_datetime())
2271         algo = self.make_algo(
2272             initialize=initialize,
2273             handle_data=handle_data,
2274             max_leverage=0,
2275         )
2276         self.check_algo_fails(algo)
2277         self.assertEqual(
2278             algo.recorded_vars['latest_time'],
2279             pd.Timestamp('2006-01-04 21:00:00', tz='UTC'),
2280         )
2281         def handle_data(algo, data):
2282             algo.order(algo.sid(self.sidint), 1)
2283         algo = self.make_algo(
2284             initialize=initialize,
2285             handle_data=handle_data,
2286             max_leverage=1,
2287         )
2288         self.check_algo_succeeds(algo)
2289     def test_set_min_leverage(self):
2290         def initialize(algo, min_leverage, grace_period):
2291             algo.set_min_leverage(
2292                 min_leverage=min_leverage, grace_period=grace_period
2293             )
2294         def handle_data(algo, data):
2295             algo.order_target_percent(algo.sid(self.sidint), .5)
2296             algo.record(latest_time=algo.get_datetime())
2297         def make_algo(min_leverage, grace_period):
2298             return self.make_algo(
2299                 initialize=initialize,
2300                 handle_data=handle_data,
2301                 min_leverage=min_leverage,
2302                 grace_period=grace_period,
2303             )
2304         offset = pd.Timedelta('10 days')
2305         algo = make_algo(min_leverage=1, grace_period=offset)
2306         self.check_algo_succeeds(algo)
2307         offset = pd.Timedelta('1 days')
2308         algo = make_algo(min_leverage=1, grace_period=offset)
2309         self.check_algo_fails(algo)
2310         self.assertEqual(
2311             algo.recorded_vars['latest_time'],
2312             pd.Timestamp('2006-01-04 21:00:00', tz='UTC'),
2313         )
2314         offset = pd.Timedelta('2 days')
2315         algo = make_algo(min_leverage=1, grace_period=offset)
2316         self.check_algo_fails(algo)
2317         self.assertEqual(
2318             algo.recorded_vars['latest_time'],
2319             pd.Timestamp('2006-01-05 21:00:00', tz='UTC'),
2320         )
2321         algo = make_algo(min_leverage=.0001, grace_period=offset)
2322         self.check_algo_succeeds(algo)
2323 class TestFuturesAlgo(zf.WithMakeAlgo, zf.ZiplineTestCase):
2324     START_DATE = pd.Timestamp('2016-01-06', tz='utc')
2325     END_DATE = pd.Timestamp('2016-01-07', tz='utc')
2326     FUTURE_MINUTE_BAR_START_DATE = pd.Timestamp('2016-01-05', tz='UTC')
2327     SIM_PARAMS_DATA_FREQUENCY = 'minute'
2328     TRADING_CALENDAR_STRS = ('us_futures',)
2329     TRADING_CALENDAR_PRIMARY_CAL = 'us_futures'
2330     BENCHMARK_SID = None
2331     @classmethod
2332     def make_futures_info(cls):
2333         return pd.DataFrame.from_dict(
2334             {
2335                 1: {
2336                     'symbol': 'CLG16',
2337                     'root_symbol': 'CL',
2338                     'start_date': pd.Timestamp('2015-12-01', tz='UTC'),
2339                     'notice_date': pd.Timestamp('2016-01-20', tz='UTC'),
2340                     'expiration_date': pd.Timestamp('2016-02-19', tz='UTC'),
2341                     'auto_close_date': pd.Timestamp('2016-01-18', tz='UTC'),
2342                     'exchange': 'TEST',
2343                 },
2344             },
2345             orient='index',
2346         )
2347     def test_futures_history(self):
2348         algo_code = dedent(
2349             """
2350             from datetime import time
2351             from zipline.api import (
2352                 date_rules,
2353                 get_datetime,
2354                 schedule_function,
2355                 sid,
2356                 time_rules,
2357             )
2358             def initialize(context):
2359                 context.history_values = []
2360                 schedule_function(
2361                     make_history_call,
2362                     date_rules.every_day(),
2363                     time_rules.market_open(),
2364                 )
2365                 schedule_function(
2366                     check_market_close_time,
2367                     date_rules.every_day(),
2368                     time_rules.market_close(),
2369                 )
2370             def make_history_call(context, data):
2371                 open_time = get_datetime().tz_convert('US/Eastern').time()
2372                 assert open_time == time(6, 31)
2373                 context.history_values.append(
2374                     data.history(sid(1), 'close', 5, '1m'),
2375                 )
2376             def check_market_close_time(context, data):
2377                 close_time = get_datetime().tz_convert('US/Eastern').time()
2378                 assert close_time == time(16, 59)
2379             """
2380         )
2381         algo = self.make_algo(
2382             script=algo_code,
2383             trading_calendar=get_calendar('us_futures'),
2384         )
2385         algo.run()
2386         np.testing.assert_array_equal(
2387             algo.history_values[0].index,
2388             pd.date_range(
2389                 '2016-01-06 6:27',
2390                 '2016-01-06 6:31',
2391                 freq='min',
2392                 tz='US/Eastern',
2393             ),
2394         )
2395         np.testing.assert_array_equal(
2396             algo.history_values[1].index,
2397             pd.date_range(
2398                 '2016-01-07 6:27',
2399                 '2016-01-07 6:31',
2400                 freq='min',
2401                 tz='US/Eastern',
2402             ),
2403         )
2404         np.testing.assert_array_equal(
2405             algo.history_values[0].values, list(map(float, range(2196, 2201))),
2406         )
2407         np.testing.assert_array_equal(
2408             algo.history_values[1].values, list(map(float, range(3636, 3641))),
2409         )
2410     @staticmethod
2411     def algo_with_slippage(slippage_model):
2412         return dedent(
2413             """
2414             from zipline.api import (
2415                 commission,
2416                 order,
2417                 set_commission,
2418                 set_slippage,
2419                 sid,
2420                 slippage,
2421                 get_datetime,
2422             )
2423             def initialize(context):
2424                 commission_model = commission.PerFutureTrade(0)
2425                 set_commission(us_futures=commission_model)
2426                 slippage_model = slippage.{model}
2427                 set_slippage(us_futures=slippage_model)
2428                 context.ordered = False
2429             def handle_data(context, data):
2430                 if not context.ordered:
2431                     order(sid(1), 10)
2432                     context.ordered = True
2433                     context.order_price = data.current(sid(1), 'price')
2434             """
2435         ).format(model=slippage_model)
2436     def test_fixed_future_slippage(self):
2437         algo_code = self.algo_with_slippage('FixedSlippage(spread=0.10)')
2438         algo = self.make_algo(
2439             script=algo_code,
2440             trading_calendar=get_calendar('us_futures'),
2441         )
2442         results = algo.run()
2443         all_txns = [
2444             val for sublist in results['transactions'].tolist()
2445             for val in sublist
2446         ]
2447         self.assertEqual(len(all_txns), 1)
2448         txn = all_txns[0]
2449         expected_spread = 0.05
2450         expected_price = (algo.order_price + 1) + expected_spread
2451         self.assertEqual(txn['price'], expected_price)
2452         self.assertEqual(results['orders'][0][0]['commission'], 0.0)
2453     def test_volume_contract_slippage(self):
2454         algo_code = self.algo_with_slippage(
2455             'VolumeShareSlippage(volume_limit=0.05, price_impact=0.1)',
2456         )
2457         algo = self.make_algo(
2458             script=algo_code,
2459             trading_calendar=get_calendar('us_futures'),
2460         )
2461         results = algo.run()
2462         self.assertEqual(results['orders'][0][0]['commission'], 0.0)
2463         all_txns = [
2464             val for sublist in results['transactions'].tolist()
2465             for val in sublist
2466         ]
2467         self.assertEqual(len(all_txns), 2)
2468         for i, txn in enumerate(all_txns):
2469             order_price = algo.order_price + i + 1
2470             expected_impact = order_price * 0.1 * (0.05 ** 2)
2471             expected_price = order_price + expected_impact
2472             self.assertEqual(txn['price'], expected_price)
2473 class TestAnalyzeAPIMethod(zf.WithMakeAlgo, zf.ZiplineTestCase):
2474     START_DATE = pd.Timestamp('2016-01-05', tz='utc')
2475     END_DATE = pd.Timestamp('2016-01-05', tz='utc')
2476     SIM_PARAMS_DATA_FREQUENCY = 'daily'
2477     DATA_PORTAL_USE_MINUTE_DATA = False
2478     def test_analyze_called(self):
2479         self.perf_ref = None
2480         def initialize(context):
2481             pass
2482         def handle_data(context, data):
2483             pass
2484         def analyze(context, perf):
2485             self.perf_ref = perf
2486         algo = self.make_algo(
2487             initialize=initialize, handle_data=handle_data, analyze=analyze,
2488         )
2489         results = algo.run()
2490         self.assertIs(results, self.perf_ref)
2491 class TestOrderCancelation(zf.WithMakeAlgo, zf.ZiplineTestCase):
2492     START_DATE = pd.Timestamp('2016-01-05', tz='utc')
2493     END_DATE = pd.Timestamp('2016-01-07', tz='utc')
2494     ASSET_FINDER_EQUITY_SIDS = (1,)
2495     ASSET_FINDER_EQUITY_SYMBOLS = ('ASSET1',)
2496     BENCHMARK_SID = None
2497     code = dedent(
2498         """
2499         from zipline.api import (
2500             sid, order, set_slippage, slippage, VolumeShareSlippage,
2501             set_cancel_policy, cancel_policy, EODCancel
2502         )
2503         def initialize(context):
2504             set_slippage(
2505                 slippage.VolumeShareSlippage(
2506                     volume_limit=1,
2507                     price_impact=0
2508                 )
2509             )
2510             {0}
2511             context.ordered = False
2512         def handle_data(context, data):
2513             if not context.ordered:
2514                 order(sid(1), {1})
2515                 context.ordered = True
2516         """,
2517     )
2518     @classmethod
2519     def make_equity_minute_bar_data(cls):
2520         asset_minutes = \
2521             cls.trading_calendar.minutes_for_sessions_in_range(
2522                 cls.START_DATE,
2523                 cls.END_DATE,
2524             )
2525         minutes_count = len(asset_minutes)
2526         minutes_arr = np.arange(1, 1 + minutes_count)
2527         yield 1, pd.DataFrame(
2528             {
2529                 'open': minutes_arr + 1,
2530                 'high': minutes_arr + 2,
2531                 'low': minutes_arr - 1,
2532                 'close': minutes_arr,
2533                 'volume': np.full(minutes_count, 1.0),
2534             },
2535             index=asset_minutes,
2536         )
2537     @classmethod
2538     def make_equity_daily_bar_data(cls, country_code, sids):
2539         yield 1, pd.DataFrame(
2540             {
2541                 'open': np.full(3, 1, dtype=np.float64),
2542                 'high': np.full(3, 1, dtype=np.float64),
2543                 'low': np.full(3, 1, dtype=np.float64),
2544                 'close': np.full(3, 1, dtype=np.float64),
2545                 'volume': np.full(3, 1, dtype=np.float64),
2546             },
2547             index=cls.equity_daily_bar_days,
2548         )
2549     def prep_algo(self,
2550                   cancelation_string,
2551                   data_frequency="minute",
2552                   amount=1000,
2553                   minute_emission=False):
2554         code = self.code.format(cancelation_string, amount)
2555         return self.make_algo(
2556             script=code,
2557             sim_params=self.make_simparams(
2558                 data_frequency=data_frequency,
2559                 emission_rate='minute' if minute_emission else 'daily',
2560             )
2561         )
2562     @parameter_space(
2563         direction=[1, -1],
2564         minute_emission=[True, False],
2565     )
2566     def test_eod_order_cancel_minute(self, direction, minute_emission):
2567         """
2568         Test that EOD order cancel works in minute mode for both shorts and
2569         longs, and both daily emission and minute emission
2570         """
2571         algo = self.prep_algo(
2572             "set_cancel_policy(cancel_policy.EODCancel())",
2573             amount=np.copysign(1000, direction),
2574             minute_emission=minute_emission
2575         )
2576         log_catcher = TestHandler()
2577         with log_catcher:
2578             for daily_positions in results.positions:
2579                 self.assertEqual(1, len<font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(daily_positions))
2580                 self.assertEqual(
2581                     np.copysign(389, direction),
2582                     daily_positions[0]["amount"],
2583                 )
2584                 self.assertEqual(1, results.positions[0][0][</b></font>"sid"])
2585             np.testing.assert_array_equal([1, 0, 0],
2586                                           list(map(len, results.orders)))
2587             np.testing.assert_array_equal([389, 0, 0],
2588                                           list(map(len, results.transactions)))
2589             the_order = results.orders[0][0]
2590             self.assertEqual(ORDER_STATUS.CANCELLED, the_order["status"])
2591             self.assertEqual(np.copysign(389, direction), the_order["filled"])
2592             warnings = [record for record in log_catcher.records if
2593                         record.level == WARNING]
2594             self.assertEqual(1, len(warnings))
2595             if direction == 1:
2596                 self.assertEqual(
2597                     "Your order for 1000 shares of ASSET1 has been partially "
2598                     "filled. 389 shares were successfully purchased. "
2599                     "611 shares were not filled by the end of day and "
2600                     "were canceled.",
2601                     str(warnings[0].message)
2602                 )
2603             elif direction == -1:
2604                 self.assertEqual(
2605                     "Your order for -1000 shares of ASSET1 has been partially "
2606                     "filled. 389 shares were successfully sold. "
2607                     "611 shares were not filled by the end of day and "
2608                     "were canceled.",
2609                     str(warnings[0].message)
2610                 )
2611     def test_default_cancelation_policy(self):
2612         algo = self.prep_algo("")
2613         log_catcher = TestHandler()
2614         with log_catcher:
2615             results = algo.run()
2616             np.testing.assert_array_equal([1, 1, 1],
2617                                           list(map(len, results.orders)))
2618             np.testing.assert_array_equal([389, 390, 221],
2619                                           list(map(len, results.transactions)))
2620             self.assertFalse(log_catcher.has_warnings)
2621     def test_eod_order_cancel_daily(self):
2622         algo = self.prep_algo(
2623             "set_cancel_policy(cancel_policy.EODCancel())",
2624             "daily"
2625         )
2626         log_catcher = TestHandler()
2627         with log_catcher:
2628             results = algo.run()
2629             np.testing.assert_array_equal([1, 1, 1],
2630                                           list(map(len, results.orders)))
2631             np.testing.assert_array_equal([0, 1, 1],
2632                                           list(map(len, results.transactions)))
2633             self.assertFalse(log_catcher.has_warnings)
2634 class TestDailyEquityAutoClose(zf.WithMakeAlgo, zf.ZiplineTestCase):
2635     """
2636     Tests if delisted equities are properly removed from a portfolio holding
2637     positions in said equities.
2638     """
2639     START_DATE = pd.Timestamp('2015-01-05', tz='UTC')
2640     END_DATE = pd.Timestamp('2015-01-13', tz='UTC')
2641     SIM_PARAMS_DATA_FREQUENCY = 'daily'
2642     DATA_PORTAL_USE_MINUTE_DATA = False
2643     BENCHMARK_SID = None
2644     @classmethod
2645     def init_class_fixtures(cls):
2646         super(TestDailyEquityAutoClose, cls).init_class_fixtures()
2647         cls.assets = (
2648             cls.asset_finder.retrieve_all(cls.asset_finder.equities_sids)
2649         )
2650     @classmethod
2651     def make_equity_info(cls):
2652         cls.test_days = cls.trading_calendar.sessions_in_range(
2653             cls.START_DATE, cls.END_DATE,
2654         )
2655         assert len(cls.test_days) == 7, "Number of days in test changed!"
2656         cls.first_asset_expiration = cls.test_days[2]
2657         cls.asset_info = make_jagged_equity_info(
2658             num_assets=3,
2659             start_date=cls.test_days[0],
2660             first_end=cls.first_asset_expiration,
2661             frequency=cls.trading_calendar.day,
2662             periods_between_ends=2,
2663             auto_close_delta=2 * cls.trading_calendar.day,
2664         )
2665         return cls.asset_info
2666     @classmethod
2667     def make_equity_daily_bar_data(cls, country_code, sids):
2668         cls.daily_data = make_trade_data_for_asset_info(
2669             dates=cls.test_days,
2670             asset_info=cls.asset_info,
2671             price_start=10,
2672             price_step_by_sid=10,
2673             price_step_by_date=1,
2674             volume_start=100,
2675             volume_step_by_sid=100,
2676             volume_step_by_date=10,
2677         )
2678         return cls.daily_data.items()
2679     def daily_prices_on_tick(self, row):
2680         return [
2681             trades.iloc[row].close for trades in itervalues(self.daily_data)
2682         ]
2683     def final_daily_price(self, asset):
2684         return self.daily_data[asset.sid].loc[asset.end_date].close
2685     def default_initialize(self):
2686         """
2687         Initialize function shared between test algos.
2688         """
2689         def initialize(context):
2690             context.ordered = False
2691             context.set_commission(PerShare(0, 0))
2692             context.set_slippage(FixedSlippage(spread=0))
2693             context.num_positions = []
2694             context.cash = []
2695         return initialize
2696     def default_handle_data(self, assets, order_size):
2697         """
2698         Handle data function shared between test algos.
2699         """
2700         def handle_data(context, data):
2701             if not context.ordered:
2702                 for asset in assets:
2703                     context.order(asset, order_size)
2704                 context.ordered = True
2705             context.cash.append(context.portfolio.cash)
2706             context.num_positions.append(len(context.portfolio.positions))
2707         return handle_data
2708     @parameter_space(
2709         order_size=[10, -10],
2710         capital_base=[1, 100000],
2711         __fail_fast=True,
2712     )
2713     def test_daily_delisted_equities(self,
2714                                      order_size,
2715                                      capital_base):
2716         """
2717         Make sure that after an equity gets delisted, our portfolio holds the
2718         correct number of equities and correct amount of cash.
2719         """
2720         assets = self.assets
2721         final_prices = {
2722             asset.sid: self.final_daily_price(asset)
2723             for asset in assets
2724         }
2725         initial_fill_prices = self.daily_prices_on_tick(1)
2726         cost_basis = sum(initial_fill_prices) * order_size
2727         fp0 = final_prices[0]
2728         fp1 = final_prices[1]
2729         algo = self.make_algo(
2730             initialize=self.default_initialize(),
2731             handle_data=self.default_handle_data(assets, order_size),
2732             sim_params=self.make_simparams(
2733                 capital_base=capital_base,
2734                 data_frequency='daily',
2735             ),
2736         )
2737         output = algo.run()
2738         initial_cash = capital_base
2739         after_fills = initial_cash - cost_basis
2740         after_first_auto_close = after_fills + fp0 * (order_size)
2741         after_second_auto_close = after_first_auto_close + fp1 * (order_size)
2742         expected_cash = [
2743             initial_cash,
2744             after_fills,
2745             after_fills,
2746             after_fills,
2747             after_first_auto_close,
2748             after_first_auto_close,
2749             after_second_auto_close,
2750         ]
2751         expected_num_positions = [0, 3, 3, 3, 2, 2, 1]
2752         self.assertEqual(expected_cash, list(output['ending_cash']))
2753         expected_cash.insert(3, after_fills)
2754         self.assertEqual(algo.cash, expected_cash[:-1])
2755         if order_size &gt; 0:
2756             self.assertEqual(
2757                 expected_num_positions,
2758                 list(output['longs_count']),
2759             )
2760             self.assertEqual(
2761                 [0] * len(self.test_days),
2762                 list(output['shorts_count']),
2763             )
2764         else:
2765             self.assertEqual(
2766                 expected_num_positions,
2767                 list(output['shorts_count']),
2768             )
2769             self.assertEqual(
2770                 [0] * len(self.test_days),
2771                 list(output['longs_count']),
2772             )
2773         expected_num_positions.insert(3, 3)
2774         self.assertEqual(algo.num_positions, expected_num_positions[:-1])
2775         transactions = output['transactions']
2776         initial_fills = transactions.iloc[1]
2777         self.assertEqual(len(initial_fills), len(assets))
2778         last_minute_of_session = \
2779             self.trading_calendar.session_close(self.test_days[1])
2780         for asset, txn in zip(assets, initial_fills):
2781             self.assertDictContainsSubset(
2782                 {
2783                     'amount': order_size,
2784                     'commission': None,
2785                     'dt': last_minute_of_session,
2786                     'price': initial_fill_prices[asset],
2787                     'sid': asset,
2788                 },
2789                 txn,
2790             )
2791             self.assertIsInstance(txn['order_id'], str)
2792         def transactions_for_date(date):
2793             return transactions.iloc[self.test_days.get_loc(date)]
2794         (first_auto_close_transaction,) = transactions_for_date(
2795             assets[0].auto_close_date
2796         )
2797         self.assertEqual(
2798             first_auto_close_transaction,
2799             {
2800                 'amount': -order_size,
2801                 'commission': None,
2802                 'dt': self.trading_calendar.session_close(
2803                     assets[0].auto_close_date,
2804                 ),
2805                 'price': fp0,
2806                 'sid': assets[0],
2807                 'order_id': None,  # Auto-close txns emit Nones for order_id.
2808             },
2809         )
2810         (second_auto_close_transaction,) = transactions_for_date(
2811             assets[1].auto_close_date
2812         )
2813         self.assertEqual(
2814             second_auto_close_transaction,
2815             {
2816                 'amount': -order_size,
2817                 'commission': None,
2818                 'dt': self.trading_calendar.session_close(
2819                     assets[1].auto_close_date,
2820                 ),
2821                 'price': fp1,
2822                 'sid': assets[1],
2823                 'order_id': None,  # Auto-close txns emit Nones for order_id.
2824             },
2825         )
2826     def test_cancel_open_orders(self):
2827         """
2828         Test that any open orders for an equity that gets delisted are
2829         canceled.  Unless an equity is auto closed, any open orders for that
2830         equity will persist indefinitely.
2831         """
2832         assets = self.assets
2833         first_asset_end_date = assets[0].end_date
2834         first_asset_auto_close_date = assets[0].auto_close_date
2835         def initialize(context):
2836             pass
2837         def handle_data(context, data):
2838             assert (
2839                 context.portfolio.cash == context.portfolio.starting_cash
2840             )
2841             today_session = self.trading_calendar.minute_to_session_label(
2842                 context.get_datetime()
2843             )
2844             day_after_auto_close = self.trading_calendar.next_session_label(
2845                 first_asset_auto_close_date,
2846             )
2847             if today_session == first_asset_end_date:
2848                 assert len(context.get_open_orders()) == 0
2849                 context.order(context.sid(0), 10)
2850                 assert len(context.get_open_orders()) == 1
2851             elif today_session == first_asset_auto_close_date:
2852                 assert len(context.get_open_orders()) == 1
2853             elif today_session == day_after_auto_close:
2854                 assert len(context.get_open_orders()) == 0
2855         algo = self.make_algo(
2856             initialize=initialize,
2857             handle_data=handle_data,
2858             sim_params=self.make_simparams(
2859                 data_frequency='daily',
2860             ),
2861         )
2862         results = algo.run()
2863         orders = results['orders']
2864         def orders_for_date(date):
2865             return orders.iloc[self.test_days.get_loc(date)]
2866         original_open_orders = orders_for_date(first_asset_end_date)
2867         assert len(original_open_orders) == 1
2868         last_close_for_asset = \
2869             algo.trading_calendar.session_close(first_asset_end_date)
2870         self.assertDictContainsSubset(
2871             {
2872                 'amount': 10,
2873                 'commission': 0.0,
2874                 'created': last_close_for_asset,
2875                 'dt': last_close_for_asset,
2876                 'sid': assets[0],
2877                 'status': ORDER_STATUS.OPEN,
2878                 'filled': 0,
2879             },
2880             original_open_orders[0],
2881         )
2882         orders_after_auto_close = orders_for_date(first_asset_auto_close_date)
2883         assert len(orders_after_auto_close) == 1
2884         self.assertDictContainsSubset(
2885             {
2886                 'amount': 10,
2887                 'commission': 0.0,
2888                 'created': last_close_for_asset,
2889                 'dt': algo.trading_calendar.session_close(
2890                     first_asset_auto_close_date,
2891                 ),
2892                 'sid': assets[0],
2893                 'status': ORDER_STATUS.CANCELLED,
2894                 'filled': 0,
2895             },
2896             orders_after_auto_close[0],
2897         )
2898 class TestMinutelyEquityAutoClose(zf.WithMakeAlgo,
2899                                   zf.ZiplineTestCase):
2900     START_DATE = pd.Timestamp('2015-01-05', tz='UTC')
2901     END_DATE = pd.Timestamp('2015-01-13', tz='UTC')
2902     BENCHMARK_SID = None
2903     @classmethod
2904     def init_class_fixtures(cls):
2905         super(TestMinutelyEquityAutoClose, cls).init_class_fixtures()
2906         cls.assets = (
2907             cls.asset_finder.retrieve_all(cls.asset_finder.equities_sids)
2908         )
2909     @classmethod
2910     def make_equity_info(cls):
2911         cls.test_days = cls.trading_calendar.sessions_in_range(
2912             cls.START_DATE, cls.END_DATE,
2913         )
2914         cls.test_minutes = cls.trading_calendar.minutes_for_sessions_in_range(
2915             cls.START_DATE, cls.END_DATE,
2916         )
2917         cls.first_asset_expiration = cls.test_days[2]
2918         cls.asset_info = make_jagged_equity_info(
2919             num_assets=3,
2920             start_date=cls.test_days[0],
2921             first_end=cls.first_asset_expiration,
2922             frequency=cls.trading_calendar.day,
2923             periods_between_ends=2,
2924             auto_close_delta=1 * cls.trading_calendar.day,
2925         )
2926         return cls.asset_info
2927     @classmethod
2928     def make_equity_minute_bar_data(cls):
2929         cls.minute_data = make_trade_data_for_asset_info(
2930             dates=cls.test_minutes,
2931             asset_info=cls.asset_info,
2932             price_start=10,
2933             price_step_by_sid=10,
2934             price_step_by_date=1,
2935             volume_start=100,
2936             volume_step_by_sid=100,
2937             volume_step_by_date=10,
2938         )
2939         return cls.minute_data.items()
2940     def minute_prices_on_tick(self, row):
2941         return [
2942             trades.iloc[row].close for trades in itervalues(self.minute_data)
2943         ]
2944     def final_minute_price(self, asset):
2945         return self.minute_data[asset.sid].loc[
2946             self.trading_calendar.session_close(asset.end_date)
2947         ].close
2948     def default_initialize(self):
2949         """
2950         Initialize function shared between test algos.
2951         """
2952         def initialize(context):
2953             context.ordered = False
2954             context.set_commission(PerShare(0, 0))
2955             context.set_slippage(FixedSlippage(spread=0))
2956             context.num_positions = []
2957             context.cash = []
2958         return initialize
2959     def default_handle_data(self, assets, order_size):
2960         """
2961         Handle data function shared between test algos.
2962         """
2963         def handle_data(context, data):
2964             if not context.ordered:
2965                 for asset in assets:
2966                     context.order(asset, order_size)
2967                 context.ordered = True
2968             context.cash.append(context.portfolio.cash)
2969             context.num_positions.append(len(context.portfolio.positions))
2970         return handle_data
2971     def test_minutely_delisted_equities(self):
2972         assets = self.assets
2973         final_prices = {
2974             asset.sid: self.final_minute_price(asset)
2975             for asset in assets
2976         }
2977         backtest_minutes = self.minute_data[0].index.tolist()
2978         order_size = 10
2979         algo = self.make_algo(
2980             initialize=self.default_initialize(),
2981             handle_data=self.default_handle_data<font color="#ae694a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(assets, order_size),
2982             sim_params=self.make_simparams(
2983                 capital_base=capital_base,
2984                 data_frequency='minute',
2985             )
2986         )
2987         output = algo.run()
2988         initial_fill_prices = self.minute_prices_on_tick(</b></font>1)
2989         cost_basis = sum(initial_fill_prices) * order_size
2990         fp0 = final_prices[0]
2991         fp1 = final_prices[1]
2992         initial_cash = capital_base
2993         after_fills = initial_cash - cost_basis
2994         after_first_auto_close = after_fills + fp0 * (order_size)
2995         after_second_auto_close = after_first_auto_close + fp1 * (order_size)
2996         expected_cash = [initial_cash]
2997         expected_position_counts = [0]
2998         expected_cash<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.extend([after_fills] * (389 + 390 + 390 + 390))
2999         expected_position_counts.extend([3] * (389 + 390 + 390 + 390))
3000         expected_cash.extend([after_first_auto_close] * (390 + 390))
3001         expected_position_counts.extend([2] * (390 + 390))
3002         expected_cash.extend([after_second_auto_close] * 390)
3003         expected_position_counts.extend([1] * 390)
3004         self.assertEqual(</b></font>len(algo.cash), len(expected_cash))
3005         self.assertEqual(algo.cash, expected_cash)
3006         self.assertEqual(
3007             list(output['ending_cash']),
3008             [
3009                 after_fills,
3010                 after_fills,
3011                 after_fills,
3012                 after_first_auto_close,
3013                 after_first_auto_close,
3014                 after_second_auto_close,
3015                 after_second_auto_close,
3016             ],
3017         )
3018         self.assertEqual(algo.num_positions, expected_position_counts)
3019         self.assertEqual(
3020             list(output['longs_count']),
3021             [3, 3, 3, 2, 2, 1, 1],
3022         )
3023         transactions = output['transactions']
3024         initial_fills = transactions.iloc[0]
3025         self.assertEqual(len(initial_fills), len(assets))
3026         for asset, txn in zip(assets, initial_fills):
3027             self.assertDictContainsSubset(
3028                 {
3029                     'amount': order_size,
3030                     'commission': None,
3031                     'dt': backtest_minutes[1],
3032                     'price': initial_fill_prices[asset],
3033                     'sid': asset,
3034                 },
3035                 txn,
3036             )
3037             self.assertIsInstance(txn['order_id'], str)
3038         def transactions_for_date(date):
3039             return transactions.iloc[self.test_days.get_loc(date)]
3040         (first_auto_close_transaction,) = transactions_for_date(
3041             assets[0].auto_close_date
3042         )
3043         self.assertEqual(
3044             first_auto_close_transaction,
3045             {
3046                 'amount': -order_size,
3047                 'commission': None,
3048                 'dt': algo.trading_calendar.session_close(
3049                     assets[0].auto_close_date,
3050                 ),
3051                 'price': fp0,
3052                 'sid': assets[0],
3053                 'order_id': None,  # Auto-close txns emit Nones for order_id.
3054             },
3055         )
3056         (second_auto_close_transaction,) = transactions_for_date(
3057             assets[1].auto_close_date
3058         )
3059         self.assertEqual(
3060             second_auto_close_transaction,
3061             {
3062                 'amount': -order_size,
3063                 'commission': None,
3064                 'dt': algo.trading_calendar.session_close(
3065                     assets[1].auto_close_date,
3066                 ),
3067                 'price': fp1,
3068                 'sid': assets[1],
3069                 'order_id': None,  # Auto-close txns emit Nones for order_id.
3070             },
3071         )
3072 class TestOrderAfterDelist(zf.WithMakeAlgo, zf.ZiplineTestCase):
3073     start = pd.Timestamp('2016-01-05', tz='utc')
3074     day_1 = pd.Timestamp('2016-01-06', tz='utc')
3075     day_4 = pd.Timestamp('2016-01-11', tz='utc')
3076     end = pd.Timestamp('2016-01-15', tz='utc')
3077     BENCHMARK_SID = None
3078     @classmethod
3079     def make_equity_info(cls):
3080         return pd<font color="#af7a82"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.DataFrame.from_dict(
3081             {
3082                 1: {
3083                     'start_date': cls.start,
3084                     'end_date': cls.day_1,
3085                     'auto_close_date': cls.day_4,
3086                     'symbol': "ASSET1",
3087                     'exchange': "TEST",
3088                 },
3089                 2: {
3090                     'start_date': cls.start,
3091                     'end_date': cls.day_4,
3092                     'auto_close_date': cls.</b></font>day_1,
3093                     'symbol': 'ASSET2',
3094                     'exchange': 'TEST',
3095                 },
3096             },
3097             orient='index',
3098         )
3099     def init_instance_fixtures(self):
3100         super(TestOrderAfterDelist, self).init_instance_fixtures()
3101         self.data_portal = FakeDataPortal(self.asset_finder)
3102     @parameterized.expand([
3103         ('auto_close_after_end_date', 1),
3104         ('auto_close_before_end_date', 2),
3105     ])
3106     def test_order_in_quiet_period(self, name, sid):
3107         asset = self.asset_finder.retrieve_asset(sid)
3108         algo_code = dedent("""
3109         from zipline.api import (
3110             sid,
3111             order,
3112             order_value,
3113             order_percent,
3114             order_target,
3115             order_target_percent,
3116             order_target_value
3117         )
3118         def initialize(context):
3119             pass
3120         def handle_data(context, data):
3121             order(sid({sid}), 1)
3122             order_value(sid({sid}), 100)
3123             order_percent(sid({sid}), 0.5)
3124             order_target(sid({sid}), 50)
3125             order_target_percent(sid({sid}), 0.5)
3126             order_target_value(sid({sid}), 50)
3127         """).format(sid=sid)
3128         algo = self.make_algo(
3129             script=algo_code,
3130             sim_params=SimulationParameters(
3131                 start_session=pd.Timestamp("2016-01-06", tz='UTC'),
3132                 end_session=pd.Timestamp("2016-01-07", tz='UTC'),
3133                 trading_calendar=self.trading_calendar,
3134                 data_frequency="minute"
3135             )
3136         )
3137         with make_test_handler(self) as log_catcher:
3138             algo.run()
3139             warnings = [r for r in log_catcher.records
3140                         if r.level == logbook.WARNING]
3141             self.assertEqual(6 * 390, len(warnings))
3142             for w in warnings:
3143                 expected_message = (
3144                     'Cannot place order for ASSET{sid}, as it has de-listed. '
3145                     'Any existing positions for this asset will be liquidated '
3146                     'on {date}.'.format(sid=sid, date=asset.auto_close_date)
3147                 )
3148                 self.assertEqual(expected_message, w.message)
3149 class AlgoInputValidationTestCase(zf.WithMakeAlgo,
3150                                   zf.ZiplineTestCase):
3151     def test_reject_passing_both_api_methods_and_script(self):
3152         script = dedent(
3153             """
3154             def initialize(context):
3155                 pass
3156             def handle_data(context, data):
3157                 pass
3158             def before_trading_start(context, data):
3159                 pass
3160             def analyze(context, results):
3161                 pass
3162             """
3163         )
3164         for method in ('initialize',
3165                        'handle_data',
3166                        'before_trading_start',
3167                        'analyze'):
3168             with self.assertRaises(ValueError):
3169                 self.make_algo(
3170                     script=script,
3171                     **{method: lambda *args, **kwargs: None}
3172                 )
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_quarters_estimates.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from datetime import timedelta
2 from functools import partial
3 import blaze as bz
4 import itertools
5 from nose.tools import assert_true
6 from nose_parameterized import parameterized
7 import numpy as np
8 from numpy.testing import assert_array_equal, assert_almost_equal
9 import pandas as pd
10 from toolz import merge
11 from zipline.pipeline import SimplePipelineEngine, Pipeline, CustomFactor
12 from zipline.pipeline.common import (
13     EVENT_DATE_FIELD_NAME,
14     FISCAL_QUARTER_FIELD_NAME,
15     FISCAL_YEAR_FIELD_NAME,
16     SID_FIELD_NAME,
17     TS_FIELD_NAME,
18 )
19 from zipline.pipeline.data import DataSet
20 from zipline.pipeline.data import Column
21 from zipline.pipeline.domain import EquitySessionDomain
22 from zipline.pipeline.loaders.blaze.estimates import (
23     BlazeNextEstimatesLoader,
24     BlazeNextSplitAdjustedEstimatesLoader,
25     BlazePreviousEstimatesLoader,
26     BlazePreviousSplitAdjustedEstimatesLoader,
27 )
28 from zipline.pipeline.loaders.earnings_estimates import (
29     INVALID_NUM_QTRS_MESSAGE,
30     NextEarningsEstimatesLoader,
31     NextSplitAdjustedEarningsEstimatesLoader,
32     normalize_quarters,
33     PreviousEarningsEstimatesLoader,
34     PreviousSplitAdjustedEarningsEstimatesLoader,
35     split_normalized_quarters,
36 )
37 from zipline.testing.fixtures import (
38     WithAdjustmentReader,
39     WithTradingSessions,
40     ZiplineTestCase,
41 )
42 from zipline.testing.predicates import assert_equal, assert_raises_regex
43 from zipline.testing.predicates import assert_frame_equal
44 from zipline.utils.numpy_utils import datetime64ns_dtype
45 from</b></font> zipline.utils.numpy_utils import float64_dtype
46 class Estimates(DataSet):
47     event_date = Column(dtype=datetime64ns_dtype)
48     fiscal_quarter = Column(dtype=float64_dtype)
49     fiscal_year = Column(dtype=float64_dtype)
50     estimate = Column(dtype=float64_dtype)
51 class MultipleColumnsEstimates(DataSet):
52     event_date = Column(dtype=datetime64ns_dtype)
53     fiscal_quarter = Column(dtype=float64_dtype)
54     fiscal_year = Column(dtype=float64_dtype)
55     estimate1 = Column(dtype=float64_dtype)
56     estimate2 = Column(dtype=float64_dtype)
57 def QuartersEstimates(announcements_out):
58     class QtrEstimates(Estimates):
59         num_announcements = announcements_out
60         name = Estimates
61     return QtrEstimates
62 def MultipleColumnsQuartersEstimates(announcements_out):
63     class QtrEstimates(MultipleColumnsEstimates):
64         num_announcements = announcements_out
65         name = Estimates
66     return QtrEstimates
67 def QuartersEstimatesNoNumQuartersAttr(num_qtr):
68     class QtrEstimates(Estimates):
69         name = Estimates
70     return QtrEstimates
71 def create_expected_df_for_factor_compute(start_date,
72                                           sids,
73                                           tuples,
74                                           end_date):
75     """
76     Given a list of tuples of new data we get for each sid on each critical
77     date (when information changes), create a DataFrame that fills that
78     data through a date range ending at `end_date`.
79     """
80     df = pd.DataFrame(tuples,
81                                'estimate',
82                                'knowledge_date'])
83     df <font color="#3ea99f"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= df.pivot_table(columns=SID_FIELD_NAME,
84                         values='estimate',
85                         index='knowledge_date')
86     df = df.reindex(
87         pd.date_range(start_date, end_date)
88     )
89     df.</b></font>index = df.index.rename('knowledge_date')
90     df['at_date'] = end_date.tz_localize('utc')
91     df = df.set_index(['at_date', df.index.tz_localize('utc')]).ffill()
92     new_sids = set(sids) - set(df.columns)
93     df = df.reindex(columns=df.columns.union(new_sids))
94     return df
95 class WithEstimates(WithTradingSessions, WithAdjustmentReader):
96     """
97     ZiplineTestCase mixin providing cls.loader and cls.events as class
98     level fixtures.
99     Methods
100     -------
101     make_loader(events, columns) -&gt; PipelineLoader
102         Method which returns the loader to be used throughout tests.
103         events : pd.DataFrame
104             The raw events to be used as input to the pipeline loader.
105         columns : dict[str -&gt; str]
106             The dictionary mapping the names of BoundColumns to the
107             associated column name in the events DataFrame.
108     make_columns() -&gt; dict[BoundColumn -&gt; str]
109        Method which returns a dictionary of BoundColumns mapped to the
110        associated column names in the raw data.
111     """
112     START_DATE = pd.Timestamp('2014-12-28')
113     END_DATE = pd.Timestamp('2015-02-04')
114     @classmethod
115     def make_loader(cls, events, columns):
116         raise NotImplementedError('make_loader')
117     @classmethod
118     def make_events(cls):
119         raise NotImplementedError('make_events')
120     @classmethod
121     def get_sids(cls):
122         return cls.events[SID_FIELD_NAME].unique()
123     @classmethod
124     def make_columns(cls):
125         return {
126             Estimates.event_date: 'event_date',
127             Estimates.fiscal_quarter: 'fiscal_quarter',
128             Estimates.fiscal_year: 'fiscal_year',
129             Estimates.estimate: 'estimate'
130         }
131     def make_engine(self, loader=None):
132         if loader is None:
133             loader = self.loader
134         return SimplePipelineEngine(
135             lambda x: loader,
136             self.asset_finder,
137             default_domain=EquitySessionDomain(
138                 self.trading_days, self.ASSET_FINDER_COUNTRY_CODE,
139             ),
140         )
141     @classmethod
142     def init_class_fixtures(cls):
143         cls.events = cls.make_events()
144         cls.ASSET_FINDER_EQUITY_SIDS = cls.get_sids()
145         cls.ASSET_FINDER_EQUITY_SYMBOLS = [
146             's' + str(n) for n in cls.ASSET_FINDER_EQUITY_SIDS
147         ]
148         super(WithEstimates, cls).init_class_fixtures()
149         cls.columns = cls.make_columns()
150         cls.loader = cls.make_loader(cls.events, {column.name: val for
151                                                   column, val in
152                                                   cls.columns.items()})
153 class WithOneDayPipeline(WithEstimates):
154     """
155     ZiplineTestCase mixin providing cls.events as a class level fixture and
156     defining a test for all inheritors to use.
157     Attributes
158     ----------
159     events : pd.DataFrame
160         A simple DataFrame with columns needed for estimates and a single sid
161         and no other data.
162     Tests
163     ------
164     test_wrong_num_announcements_passed()
165         Tests that loading with an incorrect quarter number raises an error.
166     test_no_num_announcements_attr()
167         Tests that the loader throws an AssertionError if the dataset being
168         loaded has no `num_announcements` attribute.
169     """
170     @classmethod
171     def make_columns(cls):
172         return {
173             MultipleColumnsEstimates.event_date: 'event_date',
174             MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',
175             MultipleColumnsEstimates.fiscal_year: 'fiscal_year',
176             MultipleColumnsEstimates.estimate1: 'estimate1',
177             MultipleColumnsEstimates.estimate2: 'estimate2'
178         }
179     @classmethod
180     def make_events(cls):
181         return pd.DataFrame({
182             SID_FIELD_NAME: [0] * 2,
183             TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),
184                             pd.Timestamp('2015-01-06')],
185             EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),
186                                     pd.Timestamp('2015-01-20')],
187             'estimate1': [1., 2.],
188             'estimate2': [3., 4.],
189             FISCAL_QUARTER_FIELD_NAME: [1, 2],
190             FISCAL_YEAR_FIELD_NAME: [2015, 2015]
191         })
192     @classmethod
193     def make_expected_out(cls):
194         raise NotImplementedError('make_expected_out')
195     @classmethod
196     def init_class_fixtures(cls):
197         super(WithOneDayPipeline, cls).init_class_fixtures()
198         cls.sid0 = cls.asset_finder.retrieve_asset(0)
199         cls.expected_out = cls.make_expected_out()
200     def test_load_one_day(self):
201         engine = self.make_engine()
202         results = engine.run_pipeline(
203             Pipeline<font color="#5eac10"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>({c.name: c.latest for c in dataset.columns}),
204             start_date=pd.Timestamp('2015-01-15', tz='utc'),
205             end_date=pd.Timestamp('2015-01-15', tz=</b></font>'utc'),
206         )
207         assert_frame_equal(results, self.expected_out)
208 class PreviousWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):
209     """
210     Tests that previous quarter loader correctly breaks if an incorrect
211     number of quarters is passed.
212     """
213     @classmethod
214     def make_loader(cls, events, columns):
215         return PreviousEarningsEstimatesLoader(events, columns)
216     @classmethod
217     def make_expected_out(cls):
218         return pd.DataFrame(
219             {
220                 EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-10'),
221                 'estimate1': 1.,
222                 'estimate2': 3.,
223                 FISCAL_QUARTER_FIELD_NAME: 1.,
224                 FISCAL_YEAR_FIELD_NAME: 2015.,
225             },
226             index=pd.MultiIndex.from_tuples(
227                 ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)
228             )
229         )
230 class NextWithOneDayPipeline(WithOneDayPipeline, ZiplineTestCase):
231     """
232     Tests that next quarter loader correctly breaks if an incorrect
233     number of quarters is passed.
234     """
235     @classmethod
236     def make_loader(cls, events, columns):
237         return NextEarningsEstimatesLoader(events, columns)
238     @classmethod
239     def make_expected_out(cls):
240         return pd.DataFrame(
241             {
242                 EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-20'),
243                 'estimate1': 2.,
244                 'estimate2': 4.,
245                 FISCAL_QUARTER_FIELD_NAME: 2.,
246                 FISCAL_YEAR_FIELD_NAME: 2015.,
247             },
248             index=pd.MultiIndex.from_tuples(
249                 ((pd.Timestamp('2015-01-15', tz='utc'), cls.sid0),)
250             )
251         )
252 dummy_df = pd.DataFrame({SID_FIELD_NAME: 0},
253                         columns=[SID_FIELD_NAME,
254                                  TS_FIELD_NAME,
255                                  EVENT_DATE_FIELD_NAME,
256                                  FISCAL_QUARTER_FIELD_NAME,
257                                  FISCAL_YEAR_FIELD_NAME,
258                                  'estimate'],
259                         index=[0])
260 class WithWrongLoaderDefinition(WithEstimates):
261     """
262     ZiplineTestCase mixin providing cls.events as a class level fixture and
263     defining a test for all inheritors to use.
264     Attributes
265     ----------
266     events : pd.DataFrame
267         A simple DataFrame with columns needed for estimates and a single sid
268         and no other data.
269     Tests
270     ------
271     test_wrong_num_announcements_passed()
272         Tests that loading with an incorrect quarter number raises an error.
273     test_no_num_announcements_attr()
274         Tests that the loader throws an AssertionError if the dataset being
275         loaded has no `num_announcements` attribute.
276     """
277     @classmethod
278     def make_events(cls):
279         return dummy_df
280     def test_wrong_num_announcements_passed(self):
281         bad_dataset1 = QuartersEstimates(-1)
282         bad_dataset2 = QuartersEstimates(-2)
283         good_dataset = QuartersEstimates(1)
284         engine = self.make_engine()
285         columns = {c.name + str(dataset.num_announcements): c.latest
286                    for dataset in (bad_dataset1,
287                                    bad_dataset2,
288                                    good_dataset)
289                    for c in dataset.columns}
290         p = Pipeline(columns)
291         with self.assertRaises(ValueError) as e:
292             engine.run_pipeline(
293                 p,
294                 start_date=self.trading_days[0],
295                 end_date=self.trading_days[-1],
296             )
297             assert_raises_regex(e, INVALID_NUM_QTRS_MESSAGE % "-1,-2")
298     def test_no_num_announcements_attr(self):
299         dataset = QuartersEstimatesNoNumQuartersAttr(1)
300         engine = self.make_engine()
301         p = Pipeline({c.name: c.latest for c in dataset.columns})
302         with self.assertRaises(AttributeError):
303             engine.run_pipeline(
304                 p,
305                 start_date=self.trading_days[0],
306                 end_date=self.trading_days[-1],
307             )
308 class PreviousWithWrongNumQuarters(WithWrongLoaderDefinition,
309                                    ZiplineTestCase):
310     """
311     Tests that previous quarter loader correctly breaks if an incorrect
312     number of quarters is passed.
313     """
314     @classmethod
315     def make_loader(cls, events, columns):
316         return PreviousEarningsEstimatesLoader(events, columns)
317 class NextWithWrongNumQuarters(WithWrongLoaderDefinition,
318                                ZiplineTestCase):
319     """
320     Tests that next quarter loader correctly breaks if an incorrect
321     number of quarters is passed.
322     """
323     @classmethod
324     def make_loader(cls, events, columns):
325         return NextEarningsEstimatesLoader(events, columns)
326 options = ["split_adjustments_loader",
327            "split_adjusted_column_names",
328            "split_adjusted_asof"]
329 class WrongSplitsLoaderDefinition(WithEstimates, ZiplineTestCase):
330     """
331     Test class that tests that loaders break correctly when incorrectly
332     instantiated.
333     Tests
334     -----
335     test_extra_splits_columns_passed(SplitAdjustedEstimatesLoader)
336         A test that checks that the loader correctly breaks when an
337         unexpected column is passed in the list of split-adjusted columns.
338     """
339     @classmethod
340     def init_class_fixtures(cls):
341         super(WithEstimates, cls).init_class_fixtures()
342     @parameterized.expand(itertools.product(
343         (NextSplitAdjustedEarningsEstimatesLoader,
344          PreviousSplitAdjustedEarningsEstimatesLoader),
345     ))
346     def test_extra_splits_columns_passed(self, loader):
347         columns = {
348             Estimates.event_date: 'event_date',
349             Estimates.fiscal_quarter: 'fiscal_quarter',
350             Estimates.fiscal_year: 'fiscal_year',
351             Estimates.estimate: 'estimate'
352         }
353         with self.assertRaises(ValueError):
354             loader(dummy_df,
355                    {column.name: val for column, val in
356                     columns.items()},
357                    split_adjustments_loader=self.adjustment_reader,
358                    split_adjusted_column_names=["estimate", "extra_col"],
359                    split_adjusted_asof=pd.Timestamp("2015-01-01"))
360 class WithEstimatesTimeZero(WithEstimates):
361     """
362     ZiplineTestCase mixin providing cls.events as a class level fixture and
363     defining a test for all inheritors to use.
364     Attributes
365     ----------
366     cls.events : pd.DataFrame
367         Generated dynamically in order to test inter-leavings of estimates and
368         event dates for multiple quarters to make sure that we select the
369         right immediate 'next' or 'previous' quarter relative to each date -
370         i.e., the right 'time zero' on the timeline. We care about selecting
371         the right 'time zero' because we use that to calculate which quarter's
372         data needs to be returned for each day.
373     Methods
374     -------
375     get_expected_estimate(q1_knowledge,
376                           q2_knowledge,
377                           comparable_date) -&gt; pd.DataFrame
378         Retrieves the expected estimate given the latest knowledge about each
379         quarter and the date on which the estimate is being requested. If
380         there is no expected estimate, returns an empty DataFrame.
381     Tests
382     ------
383     test_estimates()
384         Tests that we get the right 'time zero' value on each day for each
385         sid and for each column.
386     """
387     END_DATE = pd.Timestamp('2015-01-28')
388     q1_knowledge_dates = [pd.Timestamp('2015-01-01'),
389                           pd.Timestamp('2015-01-04'),
390                           pd.Timestamp('2015-01-07'),
391                           pd.Timestamp('2015-01-11')]
392     q2_knowledge_dates = [pd.Timestamp('2015-01-14'),
393                           pd.Timestamp('2015-01-17'),
394                           pd.Timestamp('2015-01-20'),
395                           pd.Timestamp('2015-01-23')]
396     q1_release_dates = [pd.Timestamp('2015-01-13'),
397                         pd.Timestamp('2015-01-14')]  # One day late
398     q2_release_dates = [pd.Timestamp('2015-01-25'),  # One day early
399                         pd.Timestamp('2015-01-26')]
400     @classmethod
401     def make_events(cls):
402         """
403         In order to determine which estimate we care about for a particular
404         sid, we need to look at all estimates that we have for that sid and
405         their associated event dates.
406         We define q1 &lt; q2, and thus event1 &lt; event2 since event1 occurs
407         during q1 and event2 occurs during q2 and we assume that there can
408         only be 1 event per quarter. We assume that there can be multiple
409         estimates per quarter leading up to the event. We assume that estimates
410         will not surpass the relevant event date. We will look at 2 estimates
411         for an event before the event occurs, since that is the simplest
412         scenario that covers the interesting edge cases:
413             - estimate values changing
414             - a release date changing
415             - estimates for different quarters interleaving
416         Thus, we generate all possible inter-leavings of 2 estimates per
417         quarter-event where estimate1 &lt; estimate2 and all estimates are &lt; the
418         relevant event and assign each of these inter-leavings to a
419         different sid.
420         """
421         sid_estimates = []
422         sid_releases = []
423         it = enumerate(
424             itertools.permutations(cls.q1_knowledge_dates +
425                                    cls.q2_knowledge_dates,
426                                    4)
427         )
428         for sid, (q1e1, q1e2, q2e1, q2e2) in it:
429             if (q1e1 &lt; q1e2 and
430                     q2e1 &lt; q2e2 and
431                     q1e1 &lt; cls.q1_release_dates[0] and
432                     q1e2 &lt; cls.q1_release_dates[0]):
433                 sid_estimates.append(cls.create_estimates_df(q1e1,
434                                                              q1e2,
435                                                              q2e1,
436                                                              q2e2,
437                                                              sid))
438                 sid_releases.append(cls.create_releases_df(sid))
439         return pd.concat(sid_estimates +
440                          sid_releases).reset_index(drop=True)
441     @classmethod
442     def get_sids(cls):
443         sids = cls.events[SID_FIELD_NAME].unique()
444         return list(sids) + [max(sids) + 1]
445     @classmethod
446     def create_releases_df(cls, sid):
447         return pd.DataFrame({
448             TS_FIELD_NAME: [pd.Timestamp('2015-01-13'),
449                             pd.Timestamp('2015-01-26')],
450             EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-13'),
451                                     pd.Timestamp('2015-01-26')],
452             'estimate': [0.5, 0.8],
453             FISCAL_QUARTER_FIELD_NAME: [1.0, 2.0],
454             FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0],
455             SID_FIELD_NAME: sid
456         })
457     @classmethod
458     def create_estimates_df(cls,
459                             q1e1,
460                             q1e2,
461                             q2e1,
462                             q2e2,
463                             sid):
464         return pd.DataFrame({
465             EVENT_DATE_FIELD_NAME: cls.q1_release_dates + cls.q2_release_dates,
466             'estimate': [.1, .2, .3, .4],
467             FISCAL_QUARTER_FIELD_NAME: [1.0, 1.0, 2.0, 2.0],
468             FISCAL_YEAR_FIELD_NAME: [2015.0, 2015.0, 2015.0, 2015.0],
469             TS_FIELD_NAME: [q1e1, q1e2, q2e1, q2e2],
470             SID_FIELD_NAME: sid,
471         })
472     def get_expected_estimate(self,
473                               q1_knowledge,
474                               q2_knowledge,
475                               comparable_date):
476         return pd.DataFrame()
477     def test_estimates(self):
478         dataset = QuartersEstimates(1)
479         engine = self.make_engine()
480         results = engine.run_pipeline(
481             Pipeline({c.name: c.latest for c in dataset.columns}),
482             start_date=self.trading_days[1],
483             end_date=self.trading_days[-2],
484         )
485         for sid in self.ASSET_FINDER_EQUITY_SIDS:
486             sid_estimates = results.xs(sid, level=1)
487             if sid == max(self.ASSET_FINDER_EQUITY_SIDS):
488                 assert_true(sid_estimates.isnull().all().all())
489             else:
490                 ts_sorted_estimates = self.events[
491                     self.events[SID_FIELD_NAME] == sid
492                 ].sort_values(TS_FIELD_NAME)
493                 q1_knowledge = ts_sorted_estimates[
494                     ts_sorted_estimates[FISCAL_QUARTER_FIELD_NAME] == 1
495                 ]
496                 q2_knowledge = ts_sorted_estimates[
497                 ]
498                 all_expected = pd.concat(
499                     [self<font color="#68818b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get_expected_estimate(
500                         q1_knowledge[q1_knowledge[TS_FIELD_NAME] &lt;=
501                                      date.tz_localize(None)],
502                         q2_knowledge[q2_knowledge[TS_FIELD_NAME] &lt;=
503                                      date.tz_localize(None)],
504                         date.tz_localize(None),
505                     ).</b></font>set_index([[date]]) for date in sid_estimates.index],
506                     axis=0)
507                 assert_equal(all_expected[sid_estimates.columns],
508                              sid_estimates)
509 class NextEstimate(WithEstimatesTimeZero, ZiplineTestCase):
510     @classmethod
511     def make_loader(cls, events, columns):
512         return NextEarningsEstimatesLoader(events, columns)
513     def get_expected_estimate(self,
514                               q1_knowledge,
515                               q2_knowledge,
516                               comparable_date):
517         if (not q1_knowledge.empty and
518             q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &gt;=
519                 comparable_date):
520             return q1_knowledge.iloc[-1:]
521         elif (not q2_knowledge.empty and
522               q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &gt;=
523                 comparable_date):
524             return q2_knowledge.iloc[-1:]
525         return pd.DataFrame(columns=q1_knowledge.columns,
526                             index=[comparable_date])
527 class BlazeNextEstimateLoaderTestCase(NextEstimate):
528     """
529     Run the same tests as EventsLoaderTestCase, but using a BlazeEventsLoader.
530     """
531     @classmethod
532     def make_loader(cls, events, columns):
533         return BlazeNextEstimatesLoader(
534             bz.data(events),
535             columns,
536         )
537 class PreviousEstimate(WithEstimatesTimeZero, ZiplineTestCase):
538     @classmethod
539     def make_loader(cls, events, columns):
540         return PreviousEarningsEstimatesLoader(events, columns)
541     def get_expected_estimate(self,
542                               q1_knowledge,
543                               q2_knowledge,
544                               comparable_date):
545         if (not q2_knowledge.empty and
546             q2_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &lt;=
547                 comparable_date):
548             return q2_knowledge.iloc[-1:]
549         elif (not q1_knowledge.empty and
550               q1_knowledge[EVENT_DATE_FIELD_NAME].iloc[-1] &lt;=
551                 comparable_date):
552             return q1_knowledge.iloc[-1:]
553         return pd.DataFrame(columns=q1_knowledge.columns,
554                             index=[comparable_date])
555 class BlazePreviousEstimateLoaderTestCase(PreviousEstimate):
556     """
557     Run the same tests as EventsLoaderTestCase, but using a BlazeEventsLoader.
558     """
559     @classmethod
560     def make_loader(cls, events, columns):
561         return BlazePreviousEstimatesLoader(
562             bz.data(events),
563             columns,
564         )
565 class WithEstimateMultipleQuarters(WithEstimates):
566     """
567     ZiplineTestCase mixin providing cls.events, cls.make_expected_out as
568     class-level fixtures and self.test_multiple_qtrs_requested as a test.
569     Attributes
570     ----------
571     events : pd.DataFrame
572         Simple DataFrame with estimates for 2 quarters for a single sid.
573     Methods
574     -------
575     make_expected_out() --&gt; pd.DataFrame
576         Returns the DataFrame that is expected as a result of running a
577         Pipeline where estimates are requested for multiple quarters out.
578     fill_expected_out(expected)
579         Fills the expected DataFrame with data.
580     Tests
581     ------
582     test_multiple_qtrs_requested()
583         Runs a Pipeline that calculate which estimates for multiple quarters
584         out and checks that the returned columns contain data for the correct
585         number of quarters out.
586     """
587     @classmethod
588     def make_events(cls):
589         return pd.DataFrame({
590             SID_FIELD_NAME: [0] * 2,
591             TS_FIELD_NAME: [pd.Timestamp('2015-01-01'),
592                             pd.Timestamp('2015-01-06')],
593             EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-10'),
594                                     pd.Timestamp('2015-01-20')],
595             'estimate': [1., 2.],
596             FISCAL_QUARTER_FIELD_NAME: [1, 2],
597             FISCAL_YEAR_FIELD_NAME: [2015, 2015]
598         })
599     @classmethod
600     def init_class_fixtures(cls):
601         cls.expected_out = cls.make_expected_out()
602     <font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>@classmethod
603     def make_expected_out(cls):
604         expected = pd.DataFrame(columns=[cls.columns[col] + '1'
605                                          for col in cls.columns] +
606                                         [cls.columns[col] + '2'
607                                          for col in cls.columns],
608                                 index=cls.</b></font>trading_days)
609         for (col, raw_name), suffix in itertools.product(
610             cls.columns.items(), ('1', '2')
611         ):
612             expected_name = raw_name + suffix
613             if col.dtype == datetime64ns_dtype:
614                 expected[expected_name] = pd.to_datetime(
615                     expected[expected_name]
616                 )
617             else:
618                 expected[expected_name] = expected[
619                     expected_name
620                 ].astype(col.dtype)
621         cls.fill_expected_out(expected)
622         return expected.reindex(cls.trading_days)
623     def test_multiple_qtrs_requested(self):
624         dataset1 = QuartersEstimates(1)
625         dataset2 = QuartersEstimates(2)
626         engine = self.make_engine()
627             Pipeline(
628                 merge([{c.name + '1': c.latest for c in dataset1.columns},
629                        {c.name + '2': c<font color="#736aff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.latest for c in dataset2.columns}])
630             ),
631             start_date=self.trading_days[0],
632             end_date=self.trading_days[-1],
633         )
634         q1_columns = [col.name + '1' for col in self.columns]
635         q2_columns =</b></font> [col.name + '2' for col in self.columns]
636         assert_equal(sorted(np.array(q1_columns + q2_columns)),
637                      sorted(results.columns.values))
638         assert_equal(self.expected_out.sort_index(axis=1),
639                      results.xs(0, level=1).sort_index(axis=1))
640 class NextEstimateMultipleQuarters(
641     WithEstimateMultipleQuarters, ZiplineTestCase
642 ):
643     @classmethod
644     def make_loader(cls, events, columns):
645         return NextEarningsEstimatesLoader(events, columns)
646     @classmethod
647     def fill_expected_out(cls, expected):
648         for raw_name in cls.columns.values():
649             expected.loc[
650                 pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-11'),
651                 raw_name + '1'
652             ] = cls.events[raw_name].iloc[0]
653             expected.loc[
654                 pd.Timestamp('2015-01-11'):pd.Timestamp('2015-01-20'),
655                 raw_name + '1'
656             ] = cls.events[raw_name].iloc[1]
657         for col_name in ['estimate', 'event_date']:
658             expected.loc[
659                 pd.Timestamp('2015-01-06'):pd.Timestamp('2015-01-10'),
660                 col_name + '2'
661             ] = cls.events[col_name].iloc[1]
662         expected.loc[
663             pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-09'),
664             FISCAL_QUARTER_FIELD_NAME + '2'
665         ] = 2
666         expected.loc[
667             pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20'),
668             FISCAL_QUARTER_FIELD_NAME + '2'
669         ] = 3
670         expected.loc[
671             pd.Timestamp('2015-01-01'):pd.Timestamp('2015-01-20'),
672             FISCAL_YEAR_FIELD_NAME + '2'
673         ] = 2015
674         return expected
675 class BlazeNextEstimateMultipleQuarters(NextEstimateMultipleQuarters):
676     @classmethod
677     def make_loader(cls, events, columns):
678         return BlazeNextEstimatesLoader(
679             bz.data(events),
680             columns,
681         )
682 class PreviousEstimateMultipleQuarters(
683     WithEstimateMultipleQuarters,
684     ZiplineTestCase
685 ):
686     @classmethod
687     def make_loader(cls, events, columns):
688         return PreviousEarningsEstimatesLoader(events, columns)
689     @classmethod
690     def fill_expected_out(cls, expected):
691         for raw_name in cls.columns.values():
692             expected[raw_name + '1'].loc[
693                 pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-19')
694             ] = cls.events[raw_name].iloc[0]
695             expected[raw_name + '1'].loc[
696                 pd.Timestamp('2015-01-20'):
697             ] = cls.events[raw_name].iloc[1]
698         for col_name in ['estimate', 'event_date']:
699             expected[col_name + '2'].loc[
700                 pd.Timestamp('2015-01-20'):
701             ] = cls.events[col_name].iloc[0]
702         expected[
703             FISCAL_QUARTER_FIELD_NAME + '2'
704         ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 4
705         expected[
706             FISCAL_YEAR_FIELD_NAME + '2'
707         ].loc[pd.Timestamp('2015-01-12'):pd.Timestamp('2015-01-20')] = 2014
708         expected[
709             FISCAL_QUARTER_FIELD_NAME + '2'
710         ].loc[pd.Timestamp('2015-01-20'):] = 1
711         expected[
712             FISCAL_YEAR_FIELD_NAME + '2'
713         ].loc[pd.Timestamp('2015-01-20'):] = 2015
714         return expected
715 class BlazePreviousEstimateMultipleQuarters(PreviousEstimateMultipleQuarters):
716     @classmethod
717     def make_loader(cls, events, columns):
718         return BlazePreviousEstimatesLoader(
719             bz.data(events),
720             columns,
721         )
722 class WithVaryingNumEstimates(WithEstimates):
723     """
724     ZiplineTestCase mixin providing fixtures and a test to ensure that we
725     have the correct overwrites when the event date changes. We want to make
726     sure that if we have a quarter with an event date that gets pushed back,
727     we don't start overwriting for the next quarter early. Likewise,
728     if we have a quarter with an event date that gets pushed forward, we want
729     to make sure that we start applying adjustments at the appropriate, earlier
730     date, rather than the later date.
731     Methods
732     -------
733     assert_compute()
734         Defines how to determine that results computed for the `SomeFactor`
735         factor are correct.
736     Tests
737     -----
738     test_windows_with_varying_num_estimates()
739         Tests that we create the correct overwrites from 2015-01-13 to
740         2015-01-14 regardless of how event dates were updated for each
741         quarter for each sid.
742     """
743     @classmethod
744     def make_events(cls):
745             SID_FIELD_NAME: [0] * 3 + [1] * 3,
746             TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
747                             pd<font color="#e77471"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-12'),
748                             pd.Timestamp('2015-01-13')] * 2,
749             EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-12'),
750                                     pd.Timestamp('2015-01-13'),
751                                     pd.Timestamp('2015-01-20'),
752                                     pd.Timestamp(</b></font>'2015-01-13'),
753                                     pd.Timestamp('2015-01-12'),
754                                     pd.Timestamp('2015-01-20')],
755             'estimate': [11., 12., 21.] * 2,
756             FISCAL_QUARTER_FIELD_NAME: [1, 1, 2] * 2,
757             FISCAL_YEAR_FIELD_NAME: [2015] * 6
758         })
759     @classmethod
760     def assert_compute(cls, estimate, today):
761         raise NotImplementedError('assert_compute')
762     def test_windows_with_varying_num_estimates(self):
763         dataset = QuartersEstimates(1)
764         assert_compute = self.assert_compute
765         class SomeFactor(CustomFactor):
766             inputs = [dataset.estimate]
767             window_length = 3
768             def compute(self, today, assets, out, estimate):
769                 assert_compute(estimate, today)
770         engine = self.make_engine()
771         engine.run_pipeline(
772             Pipeline({'est': SomeFactor()}),
773             start_date=pd.Timestamp('2015-01-13', tz='utc'),
774             end_date=pd.Timestamp('2015-01-14', tz='utc'),
775         )
776 class PreviousVaryingNumEstimates(
777     WithVaryingNumEstimates,
778     ZiplineTestCase
779 ):
780         if today == pd.Timestamp('2015-01-13', tz='utc'):
781             assert_array_equal(estimate[:, 0],
782                                np.array([np.NaN, np<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, 12]))
783             assert_array_equal(estimate[:, 1],
784                                np.array([np.NaN, 12, 12]))
785         else:
786             assert_array_equal(estimate[:, 0],
787                                np.array([np.NaN, 12, 12]))
788             assert_array_equal(</b></font>estimate[:, 1],
789                                np.array([12, 12, 12]))
790     @classmethod
791     def make_loader(cls, events, columns):
792         return PreviousEarningsEstimatesLoader(events, columns)
793 class BlazePreviousVaryingNumEstimates(PreviousVaryingNumEstimates):
794     @classmethod
795     def make_loader(cls, events, columns):
796         return BlazePreviousEstimatesLoader(
797             bz.data(events),
798             columns,
799         )
800 class NextVaryingNumEstimates(
801     WithVaryingNumEstimates,
802     ZiplineTestCase
803 ):
804     def assert_compute(self, estimate, today):
805         if today == pd.Timestamp('2015-01-13', tz='utc'):
806             assert_array_equal(estimate[:, 0],
807                                np.array([11, 12, 12]))
808             assert_array_equal(estimate[:, 1],
809                                np.array([np.NaN, np.NaN, 21]))
810         else:
811             assert_array_equal(estimate[:, 0],
812                                np.array([np.NaN, 21, 21]))
813             assert_array_equal(estimate[:, 1],
814                                np.array([np.NaN, 21, 21]))
815     @classmethod
816     def make_loader(cls, events, columns):
817         return NextEarningsEstimatesLoader(events, columns)
818 class BlazeNextVaryingNumEstimates(NextVaryingNumEstimates):
819     @classmethod
820     def make_loader(cls, events, columns):
821         return BlazeNextEstimatesLoader(
822             bz.data(events),
823             columns,
824         )
825 class WithEstimateWindows(WithEstimates):
826     """
827     ZiplineTestCase mixin providing fixures and a test to test running a
828     Pipeline with an estimates loader over differently-sized windows.
829     Attributes
830     ----------
831     events : pd.DataFrame
832         DataFrame with estimates for 2 quarters for 2 sids.
833     window_test_start_date : pd.Timestamp
834         The date from which the window should start.
835     timelines : dict[int -&gt; pd.DataFrame]
836         A dictionary mapping to the number of quarters out to
837         snapshots of how the data should look on each date in the date range.
838     Methods
839     -------
840     make_expected_timelines() -&gt; dict[int -&gt; pd.DataFrame]
841         Creates a dictionary of expected data. See `timelines`, above.
842     Tests
843     -----
844     test_estimate_windows_at_quarter_boundaries()
845         the correct dates when we have a factor that asks for a window of data.
846     """
847     END_DATE <font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= pd.Timestamp('2015-02-10')
848     window_test_start_date = pd.Timestamp('2015-01-05')
849     critical_dates = [pd.Timestamp('2015-01-09', tz='utc'),
850                       pd.Timestamp('2015-01-15', tz='utc'),
851                       pd.Timestamp('2015-01-20', tz='utc'),
852                       pd.</b></font>Timestamp('2015-01-26', tz='utc'),
853                       pd.Timestamp('2015-02-05', tz='utc'),
854                       pd.Timestamp('2015-02-10', tz='utc')]
855     window_test_cases = list(itertools.product(critical_dates, (1, 2)))
856     @classmethod
857     def make_events(cls):
858         sid_0_timeline = pd.DataFrame({
859             TS_FIELD_NAME: [cls.window_test_start_date,
860                             pd.Timestamp('2015-01-20'),
861                             pd.Timestamp('2015-01-12'),
862                             pd.Timestamp('2015-02-10'),
863                             pd.Timestamp('2015-01-18')],
864             EVENT_DATE_FIELD_NAME:
865                 [pd.Timestamp('2015-01-20'),
866                  pd.Timestamp('2015-01-20'),
867                  pd.Timestamp('2015-02-10'),
868                  pd.Timestamp('2015-02-10'),
869                  pd.Timestamp('2015-04-01')],
870             'estimate': [100., 101.] + [200., 201.] + [400],
871             FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2 + [4],
872             FISCAL_YEAR_FIELD_NAME: 2015,
873             SID_FIELD_NAME: 0,
874         })
875         sid_10_timeline = pd.DataFrame({
876             TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
877                             pd.Timestamp('2015-01-12'),
878                             pd.Timestamp('2015-01-09'),
879                             pd.Timestamp('2015-01-15')],
880             EVENT_DATE_FIELD_NAME:
881                 [pd.Timestamp('2015-01-22'), pd.Timestamp('2015-01-22'),
882                  pd.Timestamp('2015-02-05'), pd.Timestamp('2015-02-05')],
883             'estimate': [110., 111.] + [310., 311.],
884             FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [3] * 2,
885             FISCAL_YEAR_FIELD_NAME: 2015,
886             SID_FIELD_NAME: 10
887         })
888         sid_20_timeline = pd.DataFrame({
889             TS_FIELD_NAME: [cls.window_test_start_date,
890                             pd.Timestamp('2015-01-07'),
891                             cls.window_test_start_date,
892                             pd.Timestamp('2015-01-17')],
893             EVENT_DATE_FIELD_NAME:
894                 [pd.Timestamp('2015-01-20'),
895                  pd.Timestamp('2015-01-20'),
896                  pd.Timestamp('2015-02-10'),
897                  pd.Timestamp('2015-02-10')],
898             'estimate': [120., 121.] + [220., 221.],
899             FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,
900             FISCAL_YEAR_FIELD_NAME: 2015,
901             SID_FIELD_NAME: 20
902         })
903         concatted = pd.concat([sid_0_timeline,
904                                sid_10_timeline,
905                                sid_20_timeline]).reset_index()
906         np.random.seed(0)
907         return concatted.reindex(np.random.permutation(concatted.index))
908     @classmethod
909     def get_sids(cls):
910         sids = sorted(cls.events[SID_FIELD_NAME].unique())
911         return [sid for i in range(len(sids) - 1)
912                 for sid in range(sids[i], sids[i+1])] + [sids[-1]]
913     @classmethod
914     def make_expected_timelines(cls):
915         return {}
916     @classmethod
917     def init_class_fixtures(cls):
918         super(WithEstimateWindows, cls).init_class_fixtures()
919         cls.create_expected_df_for_factor_compute = partial(
920             create_expected_df_for_factor_compute,
921             cls.window_test_start_date,
922             cls.get_sids()
923         )
924         cls.timelines = cls.make_expected_timelines()
925     @parameterized.expand(window_test_cases)
926     def test_estimate_windows_at_quarter_boundaries(self,
927                                                     start_date,
928                                                     num_announcements_out):
929         dataset = QuartersEstimates(num_announcements_out)
930         trading_days = self.trading_days
931         timelines = self.timelines
932         window_len = (
933             self.trading_days.get_loc(start_date) -
934             self.trading_days.get_loc(self.window_test_start_date) + 1
935         )
936         class SomeFactor(CustomFactor):
937             inputs = [dataset.estimate]
938             window_length = window_len
939             def compute(self, today, assets, out, estimate):
940                 today_idx = trading_days.get_loc(today)
941                 today_timeline = timelines[
942                     num_announcements_out
943                 ].loc[today].reindex(
944                     trading_days[:today_idx + 1]
945                 ).values
946                 timeline_start_idx = (len(today_timeline) - window_len)
947                 assert_almost_equal(estimate,
948                                     today_timeline[timeline_start_idx:])
949         engine = self.make_engine()
950         engine.run_pipeline(
951             Pipeline({'est': SomeFactor()}),
952             start_date=start_date,
953             end_date=pd.Timestamp('2015-02-10', tz='utc'),
954         )
955 class PreviousEstimateWindows(WithEstimateWindows, ZiplineTestCase):
956     @classmethod
957     def make_loader(cls, events, columns):
958         return PreviousEarningsEstimatesLoader(events, columns)
959     @classmethod
960         oneq_previous = pd.concat([
961             pd.concat([
962                 <font color="#af7a82"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>cls.create_expected_df_for_factor_compute([
963                     (0, np.NaN, cls.window_test_start_date),
964                     (10, np.NaN, cls.window_test_start_date),
965                     (20, np.</b></font>NaN, cls.window_test_start_date)
966                 ], end_date)
967                 for end_date in pd.date_range('2015-01-09', '2015-01-19')
968             ]),
969                 [(0, 101, pd.Timestamp('2015-01-20')),
970                  (10, np.NaN, cls.window_test_start_date),
971                  (20, 121, pd.Timestamp<font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>('2015-01-20'))],
972                 pd.Timestamp('2015-01-20')
973             ),
974             cls.create_expected_df_for_factor_compute(
975                 [(0, 101, pd.Timestamp('2015-01-20')),
976                  (10, np.NaN, cls.window_test_start_date),
977                  (20</b></font>, 121, pd.Timestamp('2015-01-20'))],
978                 pd.Timestamp('2015-01-21')
979             ),
980             pd.concat([
981                 cls.create_expected_df_for_factor_compute(
982                     [(0, 101, pd.Timestamp('2015-01-20')),
983                      (10, 111, pd.Timestamp('2015-01-22')),
984                      (20, 121, pd.Timestamp('2015-01-20'))],
985                     end_date
986                 ) for end_date in pd.date_range('2015-01-22', '2015-02-04')
987             ]),
988             pd.concat([
989                 cls.create_expected_df_for_factor_compute(
990                     [(0, 101, pd.Timestamp('2015-01-20')),
991                      (10, 311, pd.Timestamp('2015-02-05')),
992                      (20, 121, pd.Timestamp('2015-01-20'))],
993                     end_date
994                 ) for end_date in pd.date_range('2015-02-05', '2015-02-09')
995                 ]),
996             cls.create_expected_df_for_factor_compute(
997                 [(0, 201, pd.Timestamp('2015-02-10')),
998                  (10, 311, pd.Timestamp('2015-02-05')),
999                  (20, 221, pd.Timestamp('2015-02-10'))],
1000                 pd.Timestamp('2015-02-10')
1001             ),
1002         ])
1003         twoq_previous = pd.concat(
1004                 [(0, np.NaN, cls.window_test_start_date),
1005                  (10, np.NaN, cls.window_test_start_date),
1006                  (20, np<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, cls.window_test_start_date)],
1007                 end_date
1008             ) for end_date in pd.date_range('2015-01-09', '2015-02-09')] +
1009             [cls.create_expected_df_for_factor_compute(
1010                 [(0, 101, pd.Timestamp('2015-02-10')),
1011                  (10, np.NaN, pd.Timestamp('2015-02-05')),
1012                  (20, 121, pd.Timestamp('2015-02-10'))],
1013                 pd.</b></font>Timestamp('2015-02-10')
1014             )]
1015         )
1016         return {
1017             1: oneq_previous,
1018             2: twoq_previous
1019         }
1020 class BlazePreviousEstimateWindows(PreviousEstimateWindows):
1021     @classmethod
1022     def make_loader(cls, events, columns):
1023         return BlazePreviousEstimatesLoader(bz.data(events), columns)
1024 class NextEstimateWindows(WithEstimateWindows, ZiplineTestCase):
1025     @classmethod
1026     def make_loader(cls, events, columns):
1027         return NextEarningsEstimatesLoader(events, columns)
1028     @classmethod
1029     def make_expected_timelines(cls):
1030         oneq_next = pd.concat([
1031             cls.create_expected_df_for_factor_compute(
1032                 [(0, 100, cls.window_test_start_date),
1033                  (10, 110, pd.Timestamp('2015-01-09')),
1034                  (20, 120, cls.window_test_start_date),
1035                  (20, 121, pd.Timestamp('2015-01-07'))],
1036                 pd.Timestamp('2015-01-09')
1037             ),
1038             pd.concat([
1039                 cls.create_expected_df_for_factor_compute(
1040                     [(0, 100, cls.window_test_start_date),
1041                      (10, 110, pd.Timestamp('2015-01-09')),
1042                      (10, 111, pd.Timestamp('2015-01-12')),
1043                      (20, 120, cls.window_test_start_date),
1044                      (20, 121, pd.Timestamp('2015-01-07'))],
1045                     end_date
1046                 ) for end_date in pd.date_range('2015-01-12', '2015-01-19')
1047             ]),
1048             cls.create_expected_df_for_factor_compute(
1049                 [(0, 100, cls.window_test_start_date),
1050                  (0, 101, pd.Timestamp('2015-01-20')),
1051                  (10, 110, pd.Timestamp('2015-01-09')),
1052                  (10, 111, pd.Timestamp('2015-01-12')),
1053                  (20, 120, cls.window_test_start_date),
1054                  (20, 121, pd.Timestamp('2015-01-07'))],
1055                 pd.Timestamp('2015-01-20')
1056             ),
1057             pd.concat([
1058                 cls.create_expected_df_for_factor_compute(
1059                     [(0, 200, pd.Timestamp('2015-01-12')),
1060                      (10, 110, pd.Timestamp('2015-01-09')),
1061                      (10, 111, pd.Timestamp('2015-01-12')),
1062                      (20, 220, cls.window_test_start_date),
1063                      (20, 221, pd.Timestamp('2015-01-17'))],
1064                     end_date
1065                 ) for end_date in pd.date_range('2015-01-21', '2015-01-22')
1066             ]),
1067             pd.concat([
1068                 cls.create_expected_df_for_factor_compute(
1069                     [(0, 200, pd.Timestamp('2015-01-12')),
1070                      (10, 310, pd.Timestamp('2015-01-09')),
1071                      (10, 311, pd.Timestamp('2015-01-15')),
1072                      (20, 220, cls.window_test_start_date),
1073                      (20, 221, pd.Timestamp('2015-01-17'))],
1074                     end_date
1075                 ) for end_date in pd.date_range('2015-01-23', '2015-02-05')
1076             ]),
1077             pd.concat([
1078                 cls.create_expected_df_for_factor_compute(
1079                     [(0, 200, pd.Timestamp('2015-01-12')),
1080                      (10, np.NaN, cls.window_test_start_date),
1081                      (20, 220, cls.window_test_start_date),
1082                      (20, 221, pd.Timestamp('2015-01-17'))],
1083                     end_date
1084                 ) for end_date in pd.date_range('2015-02-06', '2015-02-09')
1085             ]),
1086             cls.create_expected_df_for_factor_compute(
1087                 [(0, 200, pd.Timestamp('2015-01-12')),
1088                  (0, 201, pd.Timestamp('2015-02-10')),
1089                  (10, np.NaN, cls.window_test_start_date),
1090                  (20, 220, cls.window_test_start_date),
1091                  (20, 221, pd.Timestamp('2015-01-17'))],
1092                 pd.Timestamp('2015-02-10')
1093             )
1094         ])
1095         twoq_next = pd.concat(
1096             [cls.create_expected_df_for_factor_compute(
1097                 [(0, np.NaN, cls.window_test_start_date),
1098                  (10, np.NaN, cls.window_test_start_date),
1099                 end_date
1100             ) for end_date in pd.date_range('2015-01-09', '2015-01-11')] +
1101             [<font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>cls.create_expected_df_for_factor_compute(
1102                 [(0, 200, pd.Timestamp('2015-01-12')),
1103                  (10, np.NaN, cls.window_test_start_date),
1104                  (20, 220, cls.window_test_start_date)],
1105                 end_date
1106             ) for end_date in pd.date_range('2015-01-12', '2015-01-16')] +
1107             [cls.</b></font>create_expected_df_for_factor_compute(
1108                 [(0, 200, pd.Timestamp('2015-01-12')),
1109                  (10, np.NaN, cls.window_test_start_date),
1110                  (20, 220, cls.window_test_start_date),
1111                  (20, 221, pd.Timestamp('2015-01-17'))],
1112                 pd.Timestamp('2015-01-20')
1113             )] +
1114             [cls.create_expected_df_for_factor_compute(
1115                 [(0, np.NaN, cls.window_test_start_date),
1116                  (10, np.NaN, cls.window_test_start_date),
1117                  (20, np.NaN, cls.window_test_start_date)],
1118                 end_date
1119             ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]
1120         )
1121         return {
1122             1: oneq_next,
1123             2: twoq_next
1124         }
1125 class BlazeNextEstimateWindows(NextEstimateWindows):
1126     @classmethod
1127     def make_loader(cls, events, columns):
1128         return BlazeNextEstimatesLoader(bz.data(events), columns)
1129 class WithSplitAdjustedWindows(WithEstimateWindows):
1130     """
1131     ZiplineTestCase mixin providing fixures and a test to test running a
1132     Pipeline with an estimates loader over differently-sized windows and with
1133     split adjustments.
1134     """
1135     split_adjusted_asof_date = pd.Timestamp('2015-01-14')
1136     @classmethod
1137     def make_events(cls):
1138         sid_30 = pd.DataFrame({
1139             TS_FIELD_NAME: [cls.window_test_start_date,
1140                             pd.Timestamp('2015-01-09'),
1141                             cls.window_test_start_date,
1142                             pd.Timestamp('2015-01-20')],
1143             EVENT_DATE_FIELD_NAME:
1144                 [pd.Timestamp('2015-01-09'),
1145                  pd.Timestamp('2015-01-09'),
1146                  pd.Timestamp('2015-01-20'),
1147                  pd.Timestamp('2015-01-20')],
1148             'estimate': [130., 131., 230., 231.],
1149             FISCAL_QUARTER_FIELD_NAME: [1] * 2 + [2] * 2,
1150             FISCAL_YEAR_FIELD_NAME: 2015,
1151             SID_FIELD_NAME: 30
1152         })
1153         sid_40 = pd.DataFrame({
1154             TS_FIELD_NAME: [pd<font color="#5b8daf"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-09'),
1155                             pd.Timestamp('2015-01-15')],
1156             EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),
1157                                     pd.Timestamp('2015-02-10')],
1158             'estimate': [140., 240.],
1159             FISCAL_QUARTER_FIELD_NAME: [1, 2],
1160             FISCAL_YEAR_FIELD_NAME: 2015,
1161             SID_FIELD_NAME: 40
1162         })
1163         sid_50 =</b></font> pd.DataFrame({
1164             TS_FIELD_NAME: [pd.Timestamp('2015-01-09'),
1165                             pd.Timestamp('2015-01-12')],
1166             EVENT_DATE_FIELD_NAME: [pd.Timestamp('2015-01-09'),
1167                                     pd.Timestamp('2015-02-10')],
1168             'estimate': [150., 250.],
1169             FISCAL_QUARTER_FIELD_NAME: [1, 2],
1170             FISCAL_YEAR_FIELD_NAME: 2015,
1171             SID_FIELD_NAME: 50
1172         })
1173         return pd.concat([
1174             cls.__base__.make_events(),
1175             sid_30,
1176             sid_40,
1177             sid_50,
1178         ])
1179     @classmethod
1180     def make_splits_data(cls):
1181             SID_FIELD_NAME: 0,
1182             'ratio': (-1., 2., 3., 4., 5., 6., 7., 100),
1183             'effective_date': (pd<font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2014-01-01'),  # Filter out
1184                                pd.Timestamp('2015-01-07'),
1185                                pd.Timestamp('2015-01-09'),
1186                                pd.Timestamp('2015-01-13'),
1187                                pd.Timestamp('2015-01-15'),
1188                                pd.Timestamp('2015-01-18'),
1189                                pd.Timestamp(</b></font>'2015-01-30'),
1190                                pd.Timestamp('2016-01-01'))
1191         })
1192         sid_10_splits = pd.DataFrame({
1193             SID_FIELD_NAME: 10,
1194             'ratio': (.2, .3),
1195             'effective_date': (
1196                 pd.Timestamp('2015-01-07'),
1197                 pd.Timestamp('2015-01-20')),
1198         })
1199         sid_20_splits = pd.DataFrame({
1200             SID_FIELD_NAME: 20,
1201             'ratio': (.4, .5, .6, .7, .8, .9,),
1202             'effective_date': (
1203                 pd.Timestamp('2015-01-07'),
1204                 pd.Timestamp('2015-01-09'),
1205                 pd.Timestamp('2015-01-13'),
1206                 pd.Timestamp('2015-01-15'),
1207                 pd.Timestamp('2015-01-18'),
1208                 pd.Timestamp('2015-01-30')),
1209         })
1210         sid_30_splits = pd.DataFrame({
1211             SID_FIELD_NAME: 30,
1212             'ratio': (8, 9, 10, 11, 12),
1213             'effective_date': (
1214                 pd.Timestamp('2015-01-07'),
1215                 pd.Timestamp('2015-01-09'),
1216                 pd.Timestamp('2015-01-13'),
1217                 pd.Timestamp('2015-01-15'),
1218                 pd.Timestamp('2015-01-18')),
1219         })
1220         sid_40_splits = pd.DataFrame({
1221             SID_FIELD_NAME: 40,
1222             'ratio': (13, 14),
1223             'effective_date': (
1224                 pd.Timestamp('2015-01-20'),
1225                 pd.Timestamp('2015-01-22')
1226             )
1227         })
1228         sid_50_splits = pd.DataFrame({
1229             SID_FIELD_NAME: 50,
1230             'ratio': (15, 16),
1231             'effective_date': (
1232                 pd.Timestamp('2015-01-13'),
1233                 pd.Timestamp('2015-01-14')
1234             )
1235         })
1236         return pd.concat([
1237             sid_0_splits,
1238             sid_10_splits,
1239             sid_20_splits,
1240             sid_30_splits,
1241             sid_40_splits,
1242             sid_50_splits,
1243         ])
1244 class PreviousWithSplitAdjustedWindows(WithSplitAdjustedWindows,
1245                                        ZiplineTestCase):
1246     @classmethod
1247     def make_loader(cls, events, columns):
1248         return PreviousSplitAdjustedEarningsEstimatesLoader(
1249             events,
1250             columns,
1251             split_adjustments_loader=cls.adjustment_reader,
1252             split_adjusted_column_names=['estimate'],
1253             split_adjusted_asof=cls.split_adjusted_asof_date,
1254         )
1255     @classmethod
1256     def make_expected_timelines(cls):
1257             pd.concat([
1258                 cls.create_expected_df_for_factor_compute([
1259                     <font color="#41a317"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
1260                     (10, np.NaN, cls.window_test_start_date),
1261                     (20, np.NaN, cls.window_test_start_date),
1262                     (30, 131*1/10, pd.</b></font>Timestamp('2015-01-09')),
1263                     (40, 140., pd.Timestamp('2015-01-09')),
1264                     (50, 150 * 1 / 15 * 1 / 16, pd.Timestamp('2015-01-09')),
1265                 ], end_date)
1266             ]),
1267             cls.create_expected_df_for_factor_compute([
1268                 <font color="#827d6b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
1269                 (10, np.NaN, cls.window_test_start_date),
1270                 (20, np.NaN, cls.window_test_start_date),
1271                 (30, 131, pd.</b></font>Timestamp('2015-01-09')),
1272                 (40, 140., pd.Timestamp('2015-01-09')),
1273                 (50, 150. * 1 / 16, pd.Timestamp('2015-01-09')),
1274             ], pd.Timestamp('2015-01-13')),
1275             cls.create_expected_df_for_factor_compute([
1276                 (0, np.NaN, cls.window_test_start_date),
1277                 (10, np.NaN, cls.window_test_start_date),
1278                 (20, np.NaN, cls.window_test_start_date),
1279                 (30, 131, pd.Timestamp('2015-01-09')),
1280                 (40, 140., pd.Timestamp('2015-01-09')),
1281                 (50, 150., pd.Timestamp('2015-01-09'))
1282             ], pd.Timestamp('2015-01-14')),
1283             pd.concat([
1284                 cls.create_expected_df_for_factor_compute([
1285                     (0, np.NaN, cls.window_test_start_date),
1286                     (10, np.NaN, cls.window_test_start_date),
1287                     (20, np.NaN, cls.window_test_start_date),
1288                     (30, 131*11, pd.Timestamp('2015-01-09')),
1289                     (40, 140., pd.Timestamp('2015-01-09')),
1290                     (50, 150., pd.Timestamp('2015-01-09')),
1291                 ], end_date)
1292                 for end_date in pd.date_range('2015-01-15', '2015-01-16')
1293             ]),
1294             pd.concat([
1295                 cls.create_expected_df_for_factor_compute(
1296                     [(0, 101, pd.Timestamp('2015-01-20')),
1297                      (10, np.NaN, cls.window_test_start_date),
1298                      (20, 121*.7*.8, pd.Timestamp('2015-01-20')),
1299                      (30, 231, pd.Timestamp('2015-01-20')),
1300                      (40, 140.*13, pd.Timestamp('2015-01-09')),
1301                      (50, 150., pd.Timestamp('2015-01-09'))],
1302                     end_date
1303                 ) for end_date in pd.date_range('2015-01-20', '2015-01-21')
1304             pd.concat([
1305                 cls.create_expected_df_for_factor_compute(
1306                     [(0, 101, pd.Timestamp<font color="#717d7d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>('2015-01-20')),
1307                      (10, 111*.3, pd.Timestamp('2015-01-22')),
1308                      (20, 121*.7*.8, pd.Timestamp('2015-01-20')),
1309                      (30, 231, pd.Timestamp('2015-01-20')),
1310                      (40, 140.*13*14, pd.Timestamp(</b></font>'2015-01-09')),
1311                      (50, 150., pd.Timestamp('2015-01-09'))],
1312                     end_date
1313                 ) for end_date in pd.date_range('2015-01-22', '2015-01-29')
1314             ]),
1315             pd.concat([
1316                 cls.create_expected_df_for_factor_compute(
1317                     [(0, 101*7, pd.Timestamp('2015-01-20')),
1318                      (10, 111*.3, pd.Timestamp('2015-01-22')),
1319                      (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),
1320                      (30, 231, pd.Timestamp('2015-01-20')),
1321                      (40, 140.*13*14, pd.Timestamp('2015-01-09')),
1322                      (50, 150., pd.Timestamp('2015-01-09'))],
1323                     end_date
1324                 ) for end_date in pd.date_range('2015-01-30', '2015-02-04')
1325             ]),
1326             pd.concat([
1327                 cls.create_expected_df_for_factor_compute(
1328                     [(0, 101*7, pd.Timestamp('2015-01-20')),
1329                      (10, 311*.3, pd.Timestamp('2015-02-05')),
1330                      (20, 121*.7*.8*.9, pd.Timestamp('2015-01-20')),
1331                      (30, 231, pd.Timestamp('2015-01-20')),
1332                      (40, 140.*13*14, pd.Timestamp('2015-01-09')),
1333                      (50, 150., pd.Timestamp('2015-01-09'))],
1334                     end_date
1335                 ) for end_date in pd.date_range('2015-02-05', '2015-02-09')
1336                 ]),
1337             cls.create_expected_df_for_factor_compute(
1338                 [(0, 201, pd.Timestamp('2015-02-10')),
1339                  (10, 311*.3, pd.Timestamp('2015-02-05')),
1340                  (20, 221*.8*.9, pd.Timestamp('2015-02-10')),
1341                  (30, 231, pd.Timestamp('2015-01-20')),
1342                  (40, 240.*13*14, pd.Timestamp('2015-02-10')),
1343                  (50, 250., pd.Timestamp('2015-02-10'))],
1344                 pd.Timestamp('2015-02-10')
1345             ),
1346         twoq_previous = pd.concat(
1347             [<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>cls.create_expected_df_for_factor_compute(
1348                 [(0, np.NaN, cls.window_test_start_date),
1349                  (10, np.NaN, cls.window_test_start_date),
1350                  (20, np.NaN, cls.window_test_start_date),
1351                  (30, np.NaN, cls.window_test_start_date)],
1352                 end_date
1353             ) for end_date in pd.</b></font>date_range('2015-01-09', '2015-01-19')] +
1354             [cls.create_expected_df_for_factor_compute(
1355                 [(0, np.NaN, cls.window_test_start_date),
1356                  (10, np.NaN, cls.window_test_start_date),
1357                  (20, np.NaN, cls.window_test_start_date),
1358                  (30, 131*11*12, pd.Timestamp('2015-01-20'))],
1359                 end_date
1360             ) for end_date in pd.date_range('2015-01-20', '2015-02-09')] +
1361             [cls.create_expected_df_for_factor_compute(
1362                 [(0, 101*7, pd.Timestamp('2015-02-10')),
1363                  (10, np.NaN, pd.Timestamp('2015-02-05')),
1364                  (20, 121*.7*.8*.9, pd.Timestamp('2015-02-10')),
1365                  (30, 131*11*12, pd.Timestamp('2015-01-20')),
1366                  (40, 140. * 13 * 14, pd.Timestamp('2015-02-10')),
1367                  (50, 150., pd.Timestamp('2015-02-10'))],
1368                 pd.Timestamp('2015-02-10')
1369             )]
1370         )
1371         return {
1372             1: oneq_previous,
1373             2: twoq_previous
1374         }
1375 class BlazePreviousWithSplitAdjustedWindows(PreviousWithSplitAdjustedWindows):
1376     @classmethod
1377     def make_loader(cls, events, columns):
1378         return BlazePreviousSplitAdjustedEstimatesLoader(
1379             bz.data(events),
1380             columns,
1381             split_adjustments_loader=cls.adjustment_reader,
1382             split_adjusted_column_names=['estimate'],
1383             split_adjusted_asof=cls.split_adjusted_asof_date,
1384         )
1385 class NextWithSplitAdjustedWindows(WithSplitAdjustedWindows, ZiplineTestCase):
1386     @classmethod
1387     def make_loader(cls, events, columns):
1388         return NextSplitAdjustedEarningsEstimatesLoader(
1389             events,
1390             columns,
1391             split_adjustments_loader=cls.adjustment_reader,
1392             split_adjusted_column_names=['estimate'],
1393             split_adjusted_asof=cls.split_adjusted_asof_date,
1394         )
1395     @classmethod
1396         oneq_next = pd.concat([
1397             cls.create_expected_df_for_factor_compute(
1398                 [(<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 100*1/4, cls.window_test_start_date),
1399                  (10, 110, pd.Timestamp('2015-01-09')),
1400                  (20, 120*5/3, cls.window_test_start_date),
1401                  (20, 121*5/3, pd.Timestamp('2015-01-07')),
1402                  (30, 130*1/10, cls.window_test_start_date),
1403                  (30, 131*1/10, pd.Timestamp('2015-01-09')),
1404                  (40, 140, pd.</b></font>Timestamp('2015-01-09')),
1405                  (50, 150.*1/15*1/16, pd.Timestamp('2015-01-09'))],
1406                 pd.Timestamp('2015-01-09')
1407             ),
1408             cls.create_expected_df_for_factor_compute(
1409                 [(0, 100*1/4, cls.window_test_start_date),
1410                  (10, 110, pd.Timestamp('2015-01-09')),
1411                  (10, 111, pd.Timestamp('2015-01-12')),
1412                  (20, 120*5/3, cls.window_test_start_date),
1413                  (20, 121*5/3, pd.Timestamp('2015-01-07')),
1414                  (30, 230*1/10, cls.window_test_start_date),
1415                  (40, np.NaN, pd.Timestamp('2015-01-10')),
1416                  (50, 250.*1/15*1/16, pd.Timestamp('2015-01-12'))],
1417                 pd.Timestamp('2015-01-12')
1418             ),
1419             cls.create_expected_df_for_factor_compute(
1420                 [(0, 100, cls.window_test_start_date),
1421                  (10, 110, pd.Timestamp('2015-01-09')),
1422                  (10, 111, pd.Timestamp('2015-01-12')),
1423                  (20, 120, cls.window_test_start_date),
1424                  (20, 121, pd.Timestamp('2015-01-07')),
1425                  (30, 230, cls.window_test_start_date),
1426                  (40, np.NaN, pd.Timestamp('2015-01-10')),
1427                  (50, 250.*1/16, pd.Timestamp('2015-01-12'))],
1428                 pd.Timestamp('2015-01-13')
1429             ),
1430             cls.create_expected_df_for_factor_compute(
1431                 [(0, 100, cls.window_test_start_date),
1432                  (10, 110, pd.Timestamp('2015-01-09')),
1433                  (10, 111, pd.Timestamp('2015-01-12')),
1434                  (20, 120, cls.window_test_start_date),
1435                  (20, 121, pd.Timestamp('2015-01-07')),
1436                  (30, 230, cls.window_test_start_date),
1437                  (40, np.NaN, pd.Timestamp('2015-01-10')),
1438                  (50, 250., pd.Timestamp('2015-01-12'))],
1439                 pd.Timestamp('2015-01-14')
1440             ),
1441             pd.concat([
1442                     [(0, 100*5, cls.window_test_start_date),
1443                      (10, 110, pd.Timestamp('2015-01-09')),
1444                      (10, 111, pd<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-12')),
1445                      (20, 120*.7, cls.window_test_start_date),
1446                      (20, 121*.7, pd.Timestamp('2015-01-07')),
1447                      (30, 230*11, cls.window_test_start_date),
1448                      (40, 240, pd.Timestamp('2015-01-15')),
1449                      (50, 250., pd.</b></font>Timestamp('2015-01-12'))],
1450                     end_date
1451                 ) for end_date in pd.date_range('2015-01-15', '2015-01-16')
1452             cls.create_expected_df_for_factor_compute(
1453                 [(0, 100*5*6, cls.window_test_start_date),
1454                  (<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 101, pd.Timestamp('2015-01-20')),
1455                  (10, 110*.3, pd.Timestamp('2015-01-09')),
1456                  (10, 111*.3, pd.Timestamp('2015-01-12')),
1457                  (20, 120*.7*.8, cls.window_test_start_date),
1458                  (20, 121*.7*.8, pd.Timestamp('2015-01-07')),
1459                  (30, 230*11*12, cls.window_test_start_date),
1460                  (30, 231, pd.</b></font>Timestamp('2015-01-20')),
1461                  (40, 240*13, pd.Timestamp('2015-01-15')),
1462                  (50, 250., pd.Timestamp('2015-01-12'))],
1463             ),
1464             cls.create_expected_df_for_factor_compute(
1465                 [(<font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),
1466                  (10, 110 * .3, pd.Timestamp('2015-01-09')),
1467                  (10, 111 * .3, pd.Timestamp('2015-01-12')),
1468                  (20, 220 * .7 * .8, cls.window_test_start_date),
1469                  (20, 221 * .8, pd.Timestamp('2015-01-17')),
1470                  (40, 240 * 13, pd.</b></font>Timestamp('2015-01-15')),
1471                  (50, 250., pd.Timestamp('2015-01-12'))],
1472                 pd.Timestamp('2015-01-21')
1473             cls.create_expected_df_for_factor_compute(
1474                 [(0, 200 * 5 * 6, pd.Timestamp('2015-01-12')),
1475                  (10, 110 * .3, pd<font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.Timestamp('2015-01-09')),
1476                  (10, 111 * .3, pd.Timestamp('2015-01-12')),
1477                  (20, 220 * .7 * .8, cls.window_test_start_date),
1478                  (20, 221 * .8, pd.Timestamp('2015-01-17')),
1479                  (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
1480                  (50, 250., pd.</b></font>Timestamp('2015-01-12'))],
1481                 pd.Timestamp('2015-01-22')
1482             ),
1483             pd.concat([
1484                 cls.create_expected_df_for_factor_compute(
1485                     [(0, 200*5*6, pd.Timestamp('2015-01-12')),
1486                      (10, 310*.3, pd.Timestamp('2015-01-09')),
1487                      (10, 311*.3, pd.Timestamp('2015-01-15')),
1488                      (20, 220*.7*.8, cls.window_test_start_date),
1489                      (20, 221*.8, pd.Timestamp('2015-01-17')),
1490                      (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
1491                      (50, 250., pd.Timestamp('2015-01-12'))],
1492                     end_date
1493                 ) for end_date in pd.date_range('2015-01-23', '2015-01-29')
1494             ]),
1495             pd.concat([
1496                 cls.create_expected_df_for_factor_compute(
1497                     [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
1498                      (10, 310*.3, pd.Timestamp('2015-01-09')),
1499                      (10, 311*.3, pd.Timestamp('2015-01-15')),
1500                      (20, 220*.7*.8*.9, cls.window_test_start_date),
1501                      (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
1502                      (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
1503                      (50, 250., pd.Timestamp('2015-01-12'))],
1504                     end_date
1505                 ) for end_date in pd.date_range('2015-01-30', '2015-02-05')
1506             ]),
1507             pd.concat([
1508                 cls.create_expected_df_for_factor_compute(
1509                     [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
1510                      (10, np.NaN, cls.window_test_start_date),
1511                      (20, 220*.7*.8*.9, cls.window_test_start_date),
1512                      (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
1513                      (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
1514                      (50, 250., pd.Timestamp('2015-01-12'))],
1515                     end_date
1516                 ) for end_date in pd.date_range('2015-02-06', '2015-02-09')
1517             ]),
1518             cls.create_expected_df_for_factor_compute(
1519                 [(0, 200*5*6*7, pd.Timestamp('2015-01-12')),
1520                  (0, 201, pd.Timestamp('2015-02-10')),
1521                  (10, np.NaN, cls.window_test_start_date),
1522                  (20, 220*.7*.8*.9, cls.window_test_start_date),
1523                  (20, 221*.8*.9, pd.Timestamp('2015-01-17')),
1524                  (40, 240 * 13 * 14, pd.Timestamp('2015-01-15')),
1525                  (50, 250., pd.Timestamp('2015-01-12'))],
1526                 pd.Timestamp('2015-02-10')
1527             )
1528         ])
1529         twoq_next = pd.concat(
1530             [cls.create_expected_df_for_factor_compute(
1531                 [<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
1532                  (10, np.NaN, cls.window_test_start_date),
1533                  (20, 220*5/3, cls.window_test_start_date),
1534                  (30, 230*1/10, cls.window_test_start_date),
1535                  (40, np.NaN, cls.window_test_start_date),
1536                  (50, np.NaN, cls.window_test_start_date)],
1537                 pd.</b></font>Timestamp('2015-01-09')
1538             )] +
1539             [cls.create_expected_df_for_factor_compute(
1540                 [(0, 200*1/4, pd.Timestamp('2015-01-12')),
1541                  (20, 220*5/3, cls.window_test_start_date),
1542                  (30, np.NaN, cls.window_test_start_date),
1543                  (40, np<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, cls.window_test_start_date)],
1544                 pd.Timestamp('2015-01-12')
1545             )] +
1546             [cls.create_expected_df_for_factor_compute(
1547                 [(0, 200, pd.Timestamp('2015-01-12')),
1548                  (10, np.NaN, cls.window_test_start_date),
1549                  (20, 220, cls.</b></font>window_test_start_date),
1550                  (30, np.NaN, cls.window_test_start_date),
1551                  (40, np.NaN, cls.window_test_start_date)],
1552                 end_date
1553             ) for end_date in pd.date_range('2015-01-13', '2015-01-14')] +
1554             [cls.create_expected_df_for_factor_compute(
1555                 [(0, 200*5, pd.Timestamp('2015-01-12')),
1556                  (10, np.NaN, cls.window_test_start_date),
1557                  (20, 220*.7, cls.window_test_start_date),
1558                  (30, np.NaN, cls.window_test_start_date),
1559                  (40, np.NaN, cls.window_test_start_date)],
1560             ) for end_date in pd.date_range('2015-01-15', '2015-01-16')] +
1561             [cls.create_expected_df_for_factor_compute(
1562                 [<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, 200*5*6, pd.Timestamp('2015-01-12')),
1563                  (10, np.NaN, cls.window_test_start_date),
1564                  (20, 220*.7*.8, cls.window_test_start_date),
1565                  (20, 221*.8, pd.Timestamp('2015-01-17')),
1566                  (30, np.NaN, cls.window_test_start_date),
1567                  (40, np.NaN, cls.window_test_start_date)],
1568             )] +
1569             [cls</b></font>.create_expected_df_for_factor_compute(
1570                 [<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(0, np.NaN, cls.window_test_start_date),
1571                  (10, np.NaN, cls.window_test_start_date),
1572                  (20, np.NaN, cls.window_test_start_date),
1573                  (30, np.NaN, cls.window_test_start_date),
1574                  (40, np.</b></font>NaN, cls.window_test_start_date)],
1575                 end_date
1576             ) for end_date in pd.date_range('2015-01-21', '2015-02-10')]
1577         )
1578         return {
1579             1: oneq_next,
1580             2: twoq_next
1581         }
1582 class BlazeNextWithSplitAdjustedWindows(NextWithSplitAdjustedWindows):
1583     @classmethod
1584     def make_loader(cls, events, columns):
1585         return BlazeNextSplitAdjustedEstimatesLoader(
1586             bz.data(events),
1587             columns,
1588             split_adjustments_loader=cls.adjustment_reader,
1589             split_adjusted_column_names=['estimate'],
1590             split_adjusted_asof=cls.split_adjusted_asof_date,
1591         )
1592 class WithSplitAdjustedMultipleEstimateColumns(WithEstimates):
1593     """
1594     ZiplineTestCase mixin for having multiple estimate columns that are
1595     split-adjusted to make sure that adjustments are applied correctly.
1596     Attributes
1597     ----------
1598     test_start_date : pd.Timestamp
1599         The start date of the test.
1600     test_end_date : pd.Timestamp
1601         The start date of the test.
1602     split_adjusted_asof : pd.Timestamp
1603         The split-adjusted-asof-date of the data used in the test, to be used
1604         to create all loaders of test classes that subclass this mixin.
1605     Methods
1606     -------
1607     make_expected_timelines_1q_out -&gt; dict[pd.Timestamp -&gt; dict[str -&gt;
1608         np.array]]
1609         The expected array of results for each date of the date range for
1610         each column. Only for 1 quarter out.
1611     make_expected_timelines_2q_out -&gt; dict[pd.Timestamp -&gt; dict[str -&gt;
1612         np.array]]
1613         The expected array of results for each date of the date range. For 2
1614         quarters out, so only for the column that is requested to be loaded
1615         with 2 quarters out.
1616     Tests
1617     -----
1618     test_adjustments_with_multiple_adjusted_columns
1619         Tests that if you have multiple columns, we still split-adjust
1620         correctly.
1621     test_multiple_datasets_different_num_announcements
1622         Tests that if you have multiple datasets that ask for a different
1623         we still split-adjust correctly.
1624     """
1625     END_DATE <font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= pd.Timestamp('2015-02-10')
1626     test_start_date = pd.Timestamp('2015-01-06', tz='utc')
1627     test_end_date = pd.Timestamp('2015-01-12', tz='utc')
1628     split_adjusted_asof = pd.Timestamp(</b></font>'2015-01-08')
1629     @classmethod
1630     def make_columns(cls):
1631         return {
1632             MultipleColumnsEstimates.event_date: 'event_date',
1633             MultipleColumnsEstimates.fiscal_quarter: 'fiscal_quarter',
1634             MultipleColumnsEstimates.fiscal_year: 'fiscal_year',
1635             MultipleColumnsEstimates.estimate1: 'estimate1',
1636             MultipleColumnsEstimates.estimate2: 'estimate2'
1637         }
1638     @classmethod
1639     def make_events(cls):
1640         sid_0_events = pd.DataFrame({
1641             TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),
1642                             pd.Timestamp('2015-01-05')],
1643             EVENT_DATE_FIELD_NAME:
1644                 [pd.Timestamp('2015-01-09'),
1645                  pd.Timestamp('2015-01-12')],
1646             'estimate1': [1100., 1200.],
1647             'estimate2': [2100., 2200.],
1648             FISCAL_QUARTER_FIELD_NAME: [1, 2],
1649             FISCAL_YEAR_FIELD_NAME: 2015,
1650             SID_FIELD_NAME: 0,
1651         })
1652         sid_1_events = pd.DataFrame({
1653             TS_FIELD_NAME: [pd.Timestamp('2015-01-05'),
1654                             pd.Timestamp('2015-01-05')],
1655             EVENT_DATE_FIELD_NAME:
1656                 [pd.Timestamp('2015-01-08'),
1657                  pd.Timestamp('2015-01-11')],
1658             'estimate1': [1110., 1210.],
1659             'estimate2': [2110., 2210.],
1660             FISCAL_QUARTER_FIELD_NAME: [1, 2],
1661             FISCAL_YEAR_FIELD_NAME: 2015,
1662             SID_FIELD_NAME: 1,
1663         })
1664         return pd.concat([sid_0_events, sid_1_events])
1665     @classmethod
1666     def make_splits_data(cls):
1667         sid_0_splits = pd.DataFrame({
1668             SID_FIELD_NAME: 0,
1669             'ratio': (.3, 3.),
1670             'effective_date': (pd.Timestamp('2015-01-07'),
1671                                pd.Timestamp('2015-01-09')),
1672         })
1673         sid_1_splits = pd.DataFrame({
1674             SID_FIELD_NAME: 1,
1675             'ratio': (.4, 4.),
1676             'effective_date': (pd.Timestamp('2015-01-07'),
1677                                pd.Timestamp('2015-01-09')),
1678         })
1679         return pd.concat([sid_0_splits, sid_1_splits])
1680     @classmethod
1681     def make_expected_timelines_1q_out(cls):
1682         return {}
1683     @classmethod
1684     def make_expected_timelines_2q_out(cls):
1685         return {}
1686     @classmethod
1687     def init_class_fixtures(cls):
1688         super(
1689             WithSplitAdjustedMultipleEstimateColumns, cls
1690         ).init_class_fixtures()
1691         cls.timelines_1q_out = cls.make_expected_timelines_1q_out()
1692         cls.timelines_2q_out = cls.make_expected_timelines_2q_out()
1693     def test_adjustments_with_multiple_adjusted_columns(self):
1694         dataset = MultipleColumnsQuartersEstimates(1)
1695         timelines = self.timelines_1q_out
1696         window_len = 3
1697         class SomeFactor(CustomFactor):
1698             inputs = [dataset.estimate1, dataset.estimate2]
1699             window_length = window_len
1700             def compute(self, today, assets, out, estimate1, estimate2):
1701                 assert_almost_equal(estimate1, timelines[today]['estimate1'])
1702                 assert_almost_equal(estimate2, timelines[today]['estimate2'])
1703         engine = self.make_engine()
1704         engine.run_pipeline(
1705             Pipeline({'est': SomeFactor()}),
1706             start_date=self.test_start_date,
1707             end_date=self.test_end_date,
1708         )
1709     def test_multiple_datasets_different_num_announcements(self):
1710         dataset1 = MultipleColumnsQuartersEstimates(1)
1711         dataset2 = MultipleColumnsQuartersEstimates(2)
1712         timelines_1q_out = self.timelines_1q_out
1713         timelines_2q_out = self.timelines_2q_out
1714         window_len = 3
1715         class SomeFactor1(CustomFactor):
1716             inputs = [dataset1.estimate1]
1717             window_length = window_len
1718             def compute(self, today, assets, out, estimate1):
1719                 assert_almost_equal(
1720                     estimate1, timelines_1q_out[today]['estimate1']
1721                 )
1722         class SomeFactor2(CustomFactor):
1723             inputs = [dataset2.estimate2]
1724             window_length = window_len
1725             def compute(self, today, assets, out, estimate2):
1726                 assert_almost_equal(
1727                     estimate2, timelines_2q_out[today]['estimate2']
1728                 )
1729         engine = self.make_engine()
1730         engine.run_pipeline(
1731             Pipeline({'est1': SomeFactor1(), 'est2': SomeFactor2()}),
1732             start_date=self.test_start_date,
1733             end_date=self.test_end_date,
1734         )
1735 class PreviousWithSplitAdjustedMultipleEstimateColumns(
1736     WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase
1737 ):
1738     @classmethod
1739     def make_loader(cls, events, columns):
1740         return PreviousSplitAdjustedEarningsEstimatesLoader(
1741             events,
1742             columns,
1743             split_adjustments_loader=cls.adjustment_reader,
1744             split_adjusted_column_names=['estimate1', 'estimate2'],
1745             split_adjusted_asof=cls.split_adjusted_asof,
1746         )
1747     @classmethod
1748     def make_expected_timelines_1q_out(cls):
1749         return {
1750             pd.Timestamp('2015-01-06', tz='utc'): {
1751                 'estimate1': np.array([[np.NaN, np.NaN]] * 3),
1752                 'estimate2': np.array([[np.NaN, np.NaN]] * 3)
1753             },
1754             pd.Timestamp('2015-01-07', tz='utc'): {
1755                 'estimate1': np.array([[np.NaN, np.NaN]] * 3),
1756             },
1757             pd.Timestamp('2015-01-08', tz='utc'): {
1758                 'estimate1': np.array([[np<font color="#810541"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.NaN, np.NaN]] * 2 +
1759                                       [[np.NaN, 1110.]]),
1760                 'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
1761                                       [[</b></font>np.NaN, 2110.]])
1762             },
1763             pd.Timestamp('2015-01-09', tz='utc'): {
1764                 'estimate1': np.array([[np.NaN, np.NaN]] +
1765                                       [[np.NaN, 1110. * 4]] +
1766                                       [[1100 * 3., 1110. * 4]]),
1767                 'estimate2': np.array([[np.NaN, np.NaN]] +
1768                                       [[np.NaN, 2110. * 4]] +
1769                                       [[2100 * 3., 2110. * 4]])
1770             },
1771             pd.Timestamp('2015-01-12', tz='utc'): {
1772                 'estimate1': np.array([[np.NaN, np.NaN]] * 2 +
1773                                       [[1200 * 3., 1210. * 4]]),
1774                 'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
1775                                       [[2200 * 3., 2210. * 4]])
1776             }
1777         }
1778     @classmethod
1779     def make_expected_timelines_2q_out(cls):
1780         return {
1781             pd.Timestamp('2015-01-06', tz='utc'): {
1782                 'estimate2': np.array([[np.NaN, np.NaN]] * 3)
1783             },
1784             pd.Timestamp('2015-01-07', tz='utc'): {
1785                 'estimate2': np.array([[np.NaN, np.NaN]] * 3)
1786             },
1787             pd.Timestamp('2015-01-08', tz='utc'): {
1788                 'estimate2': np.array([[np.NaN, np.NaN]] * 3)
1789             },
1790             pd.Timestamp('2015-01-09', tz='utc'): {
1791                 'estimate2': np.array([[np.NaN, np.NaN]] * 3)
1792             },
1793             pd.Timestamp('2015-01-12', tz='utc'): {
1794                 'estimate2': np.array([[np.NaN, np.NaN]] * 2 +
1795                                       [[2100 * 3., 2110. * 4]])
1796             }
1797         }
1798 class BlazePreviousWithMultipleEstimateColumns(
1799     PreviousWithSplitAdjustedMultipleEstimateColumns
1800 ):
1801     @classmethod
1802     def make_loader(cls, events, columns):
1803         return BlazePreviousSplitAdjustedEstimatesLoader(
1804             bz.data(events),
1805             columns,
1806             split_adjustments_loader=cls.adjustment_reader,
1807             split_adjusted_column_names=['estimate1', 'estimate2'],
1808             split_adjusted_asof=cls.split_adjusted_asof,
1809         )
1810 class NextWithSplitAdjustedMultipleEstimateColumns(
1811     WithSplitAdjustedMultipleEstimateColumns, ZiplineTestCase
1812 ):
1813     @classmethod
1814     def make_loader(cls, events, columns):
1815         return NextSplitAdjustedEarningsEstimatesLoader(
1816             events,
1817             columns,
1818             split_adjustments_loader=cls.adjustment_reader,
1819             split_adjusted_column_names=['estimate1', 'estimate2'],
1820             split_adjusted_asof=cls.split_adjusted_asof,
1821         )
1822     @classmethod
1823         return {
1824             pd.Timestamp('2015-01-06', tz='utc'): {
1825                 'estimate1': np.array<font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>([[np.NaN, np.NaN]] +
1826                                       [[1100. * 1/.3, 1110. * 1/.4]] * 2),
1827                 'estimate2': np.array([[np.NaN, np.NaN]] +
1828                                       [[2100. * 1/.3, 2110. * 1/.4]] * 2),
1829             },
1830             pd.Timestamp(</b></font>'2015-01-07', tz='utc'): {
1831                 'estimate1': np.array([[1100., 1110.]] * 3),
1832                 'estimate2': np.array([[2100., 2110.]] * 3)
1833             },
1834             pd.Timestamp('2015-01-08', tz='utc'): {
1835                 'estimate1': np.array([[1100., 1110.]] * 3),
1836                 'estimate2': np.array([[2100., 2110.]] * 3)
1837             },
1838             pd.Timestamp('2015-01-09', tz='utc'): {
1839                 'estimate1': np.array([[1100 * 3., 1210. * 4]] * 3),
1840             },
1841             pd.Timestamp('2015-01-12', tz='utc'): {
1842                 <font color="#ff00ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>'estimate1': np.array([[1200 * 3., np.NaN]] * 3),
1843                 'estimate2': np.array([[2200 * 3., np.</b></font>NaN]] * 3)
1844             }
1845         }
1846     @classmethod
1847     def make_expected_timelines_2q_out(cls):
1848         return {
1849             pd.Timestamp('2015-01-06', tz='utc'): {
1850                 'estimate2': np.array([[np.NaN, np.NaN]] +
1851                                       [[2200 * 1/.3, 2210. * 1/.4]] * 2)
1852             },
1853             pd.Timestamp('2015-01-07', tz='utc'): {
1854                 'estimate2': np.array([[2200., 2210.]] * 3)
1855             },
1856             pd.Timestamp('2015-01-08', tz='utc'): {
1857                 'estimate2': np.array([[2200, 2210.]] * 3)
1858             },
1859             pd.Timestamp('2015-01-09', tz='utc'): {
1860                 'estimate2': np.array([[2200 * 3., np.NaN]] * 3)
1861             },
1862             pd.Timestamp('2015-01-12', tz='utc'): {
1863                 'estimate2': np.array([[np.NaN, np.NaN]] * 3)
1864             }
1865         }
1866 class BlazeNextWithMultipleEstimateColumns(
1867     NextWithSplitAdjustedMultipleEstimateColumns
1868 ):
1869     @classmethod
1870     def make_loader(cls, events, columns):
1871         return BlazeNextSplitAdjustedEstimatesLoader(
1872             bz.data(events),
1873             columns,
1874             split_adjustments_loader=cls.adjustment_reader,
1875             split_adjusted_column_names=['estimate1', 'estimate2'],
1876             split_adjusted_asof=cls.split_adjusted_asof,
1877         )
1878 class WithAdjustmentBoundaries(WithEstimates):
1879     """
1880     ZiplineTestCase mixin providing class-level attributes, methods,
1881     and a test to make sure that when the split-adjusted-asof-date is not
1882     strictly within the date index, we can still apply adjustments correctly.
1883     Attributes
1884     ----------
1885     split_adjusted_before_start : pd.Timestamp
1886         A split-adjusted-asof-date before the start date of the test.
1887     split_adjusted_after_end : pd.Timestamp
1888         A split-adjusted-asof-date before the end date of the test.
1889     split_adjusted_asof_dates : list of tuples of pd.Timestamp
1890         All the split-adjusted-asof-dates over which we want to parameterize
1891         the test.
1892     Methods
1893     -------
1894     make_expected_out -&gt; dict[pd.Timestamp -&gt; pd.DataFrame]
1895         A dictionary of the expected output of the pipeline at each of the
1896         dates of interest.
1897     """
1898     START_DATE = pd.Timestamp('2015-01-04')
1899     test_start_date = pd.Timestamp('2015-01-05')
1900     END_DATE = test_end_date = pd.Timestamp('2015-01-12')
1901     split_adjusted_before_start = (
1902         test_start_date - timedelta(days=1)
1903     )
1904     split_adjusted_after_end = (
1905         test_end_date + timedelta(days=1)
1906     )
1907     split_adjusted_asof_dates = [(test_start_date,),
1908                                  (test_end_date,),
1909                                  (split_adjusted_before_start,),
1910                                  (split_adjusted_after_end,)]
1911     @classmethod
1912     def init_class_fixtures(cls):
1913         super(WithAdjustmentBoundaries, cls).init_class_fixtures()
1914         cls.s0 = cls.asset_finder.retrieve_asset(0)
1915         cls.s1 = cls.asset_finder.retrieve_asset(1)
1916         cls.s2 = cls.asset_finder.retrieve_asset(2)
1917         cls.s3 = cls.asset_finder.retrieve_asset(3)
1918         cls.s4 = cls.asset_finder.retrieve_asset(4)
1919         cls.expected = cls.make_expected_out()
1920     @classmethod
1921     def make_events(cls):
1922         sid_0_timeline = pd.DataFrame({
1923             TS_FIELD_NAME: cls.test_start_date,
1924             EVENT_DATE_FIELD_NAME: pd.Timestamp('2015-01-09'),
1925             'estimate': 10.,
1926             FISCAL_QUARTER_FIELD_NAME: 1,
1927             FISCAL_YEAR_FIELD_NAME: 2015,
1928             SID_FIELD_NAME: 0,
1929         }, index=[0])
1930         sid_1_timeline = pd.DataFrame({
1931             TS_FIELD_NAME: cls.test_start_date,
1932             EVENT_DATE_FIELD_NAME: cls.test_start_date,
1933             'estimate': 11.,
1934             FISCAL_QUARTER_FIELD_NAME: 1,
1935             FISCAL_YEAR_FIELD_NAME: 2015,
1936             SID_FIELD_NAME: 1,
1937         }, index=[0])
1938         sid_2_timeline = pd.DataFrame({
1939             TS_FIELD_NAME: cls.test_end_date,
1940             EVENT_DATE_FIELD_NAME: cls.test_end_date + timedelta(days=1),
1941             'estimate': 12.,
1942             FISCAL_QUARTER_FIELD_NAME: 1,
1943             FISCAL_YEAR_FIELD_NAME: 2015,
1944             SID_FIELD_NAME: 2,
1945         }, index=[0])
1946         sid_3_timeline = pd.DataFrame({
1947             TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),
1948             EVENT_DATE_FIELD_NAME: cls.test_end_date,
1949             'estimate': 13.,
1950             FISCAL_QUARTER_FIELD_NAME: 1,
1951             FISCAL_YEAR_FIELD_NAME: 2015,
1952             SID_FIELD_NAME: 3,
1953         }, index=[0])
1954         sid_4_timeline = pd.DataFrame({
1955             TS_FIELD_NAME: cls.test_end_date - timedelta(days=1),
1956             EVENT_DATE_FIELD_NAME: cls.test_end_date - timedelta(days=1),
1957             'estimate': 14.,
1958             FISCAL_QUARTER_FIELD_NAME: 1,
1959             FISCAL_YEAR_FIELD_NAME: 2015,
1960             SID_FIELD_NAME: 4,
1961         }, index=[0])
1962         return pd.concat([sid_0_timeline,
1963                           sid_1_timeline,
1964                           sid_2_timeline,
1965                           sid_3_timeline,
1966                           sid_4_timeline])
1967     @classmethod
1968     def make_splits_data(cls):
1969         sid_0_splits = pd.DataFrame({
1970             SID_FIELD_NAME: 0,
1971             'ratio': .10,
1972             'effective_date': cls.test_start_date,
1973         }, index=[0])
1974         sid_1_splits = pd.DataFrame({
1975             SID_FIELD_NAME: 1,
1976             'ratio': .11,
1977             'effective_date': cls.test_start_date,
1978         }, index=[0])
1979         sid_2_splits = pd.DataFrame({
1980             SID_FIELD_NAME: 2,
1981             'ratio': .12,
1982             'effective_date': cls.test_end_date,
1983         }, index=[0])
1984         sid_3_splits = pd.DataFrame({
1985             SID_FIELD_NAME: 3,
1986             'ratio': .13,
1987             'effective_date': cls.test_end_date,
1988         }, index=[0])
1989         sid_4_splits = pd.DataFrame({
1990             SID_FIELD_NAME: 4,
1991             'ratio': (.14, .15),
1992             'effective_date': (cls.test_start_date, cls.test_end_date),
1993         })
1994         return pd.concat([sid_0_splits,
1995                           sid_1_splits,
1996                           sid_2_splits,
1997                           sid_3_splits,
1998                           sid_4_splits])
1999     @parameterized.expand(split_adjusted_asof_dates)
2000     def test_boundaries(self, split_date):
2001         dataset = QuartersEstimates<font color="#ae694a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(1)
2002         loader = self.loader(split_adjusted_asof=split_date)
2003         engine = engine = self.make_engine(loader)
2004         result = engine.run_pipeline(</b></font>
2005             Pipeline({'estimate': dataset.estimate.latest}),
2006             start_date=self.trading_days[0],
2007             end_date=self.trading_days[-1],
2008         )
2009         expected = self.expected[split_date]
2010         assert_frame_equal(result, expected, check_names=False)
2011     @classmethod
2012     def make_expected_out(cls):
2013         return {}
2014 class PreviousWithAdjustmentBoundaries(WithAdjustmentBoundaries,
2015                                        ZiplineTestCase):
2016     @classmethod
2017     def make_loader(cls, events, columns):
2018         return partial(PreviousSplitAdjustedEarningsEstimatesLoader,
2019                        events,
2020                        columns,
2021                        split_adjustments_loader=cls.adjustment_reader,
2022                        split_adjusted_column_names=['estimate'])
2023     @classmethod
2024     def make_expected_out(cls):
2025         split_adjusted_at_start_boundary = pd.concat([
2026             pd.DataFrame({
2027                 SID_FIELD_NAME: cls.s0,
2028                 'estimate': np.NaN,
2029             }, index=pd.date_range(
2030                 cls.test_start_date,
2031                 pd.Timestamp('2015-01-08'),
2032                 tz='utc'
2033             )),
2034             pd.DataFrame({
2035                 SID_FIELD_NAME: cls.s0,
2036                 'estimate': 10.,
2037             }, index=pd.date_range(
2038                 pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'
2039             )),
2040             pd.DataFrame({
2041                 SID_FIELD_NAME: cls.s1,
2042                 'estimate': 11.,
2043                                    tz='utc')),
2044             pd.DataFrame({
2045                 <font color="#79764d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>SID_FIELD_NAME: cls.s2,
2046                 'estimate': np.NaN
2047             }, index=pd.date_range(cls.test_start_date,
2048                                    cls.test_end_date,
2049                                    tz='utc')),
2050             pd.DataFrame({
2051                 SID_FIELD_NAME: cls.</b></font>s3,
2052                 'estimate': np.NaN
2053             }, index=pd.date_range(
2054                 cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'
2055             )),
2056                 SID_FIELD_NAME: cls.s3,
2057                 'estimate': 13. * .13
2058             }, index=pd.date_range(cls<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.test_end_date,
2059                                    cls.test_end_date,
2060                                    tz='utc')),
2061             pd.DataFrame({
2062                 SID_FIELD_NAME: cls.s4,
2063                 'estimate': np.NaN
2064             }, index=pd.date_range(
2065                 cls.test_start_date, cls.</b></font>test_end_date - timedelta(2), tz='utc'
2066             )),
2067             pd.DataFrame({
2068                 SID_FIELD_NAME: cls.s4,
2069                 'estimate': 14. * .15
2070             }, index=pd.date_range(
2071                 cls.test_end_date - timedelta(1), cls.test_end_date, tz='utc'
2072             )),
2073         ]).set_index(SID_FIELD_NAME, append=True).unstack(
2074             SID_FIELD_NAME).reindex(cls.trading_days).stack(
2075             SID_FIELD_NAME, dropna=False)
2076         split_adjusted_at_end_boundary = pd.concat([
2077             pd.DataFrame({
2078                 SID_FIELD_NAME: cls.s0,
2079                 'estimate': np.NaN,
2080             }, index=pd.date_range(
2081                 cls.test_start_date, pd.Timestamp('2015-01-08'), tz='utc'
2082             )),
2083             pd.DataFrame({
2084                 SID_FIELD_NAME: cls.s0,
2085                 'estimate': 10.,
2086             }, index=pd.date_range(
2087                 pd.Timestamp('2015-01-09'), cls.test_end_date, tz='utc'
2088             )),
2089             pd.DataFrame({
2090                 SID_FIELD_NAME: cls.s1,
2091                 'estimate': 11.,
2092             }, index=pd.date_range(cls.test_start_date,
2093                                    tz='utc')),
2094             pd.DataFrame({
2095                 <font color="#f660ab"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>SID_FIELD_NAME: cls.s2,
2096                 'estimate': np.NaN
2097             }, index=pd.date_range(cls.test_start_date,
2098                                    cls.test_end_date,
2099                                    tz='utc')),
2100             pd.DataFrame({
2101                 SID_FIELD_NAME: cls.</b></font>s3,
2102                 'estimate': np.NaN
2103             }, index=pd.date_range(
2104                 cls.test_start_date, cls.test_end_date - timedelta(1), tz='utc'
2105             )),
2106             pd.DataFrame({
2107                 SID_FIELD_NAME: cls.s3,
2108                 'estimate': 13.
2109             }, index=pd.date_range(cls.test_end_date,
2110                                    cls.test_end_date,
2111                                    tz='utc')),
2112             pd.DataFrame({
2113                 SID_FIELD_NAME: cls.s4,
2114                 'estimate': np.NaN
2115             }, index=pd.date_range(
2116                 cls.test_start_date, cls.test_end_date - timedelta(2), tz='utc'
2117             )),
2118             pd.DataFrame({
2119                 SID_FIELD_NAME: cls.s4,
2120                 'estimate': 14.
2121             }, index=pd.date_range(cls.test_end_date - timedelta(1),
2122                                    cls.test_end_date,
2123                                    tz='utc')),
2124         ]).set_index(SID_FIELD_NAME, append=True).unstack(
2125             SID_FIELD_NAME).reindex(cls.trading_days).stack(SID_FIELD_NAME,
2126                                                             dropna=False)
2127         split_adjusted_before_start_boundary = split_adjusted_at_start_boundary
2128         split_adjusted_after_end_boundary = split_adjusted_at_end_boundary
2129         return {cls.test_start_date:
2130                 split_adjusted_at_start_boundary,
2131                 cls.split_adjusted_before_start:
2132                 split_adjusted_before_start_boundary,
2133                 cls.test_end_date:
2134                 split_adjusted_at_end_boundary,
2135                 cls.split_adjusted_after_end:
2136                 split_adjusted_after_end_boundary}
2137 class BlazePreviousWithAdjustmentBoundaries(PreviousWithAdjustmentBoundaries):
2138     @classmethod
2139     def make_loader(cls, events, columns):
2140         return partial(BlazePreviousSplitAdjustedEstimatesLoader,
2141                        bz.data(events),
2142                        columns,
2143                        split_adjustments_loader=cls.adjustment_reader,
2144                        split_adjusted_column_names=['estimate'])
2145 class NextWithAdjustmentBoundaries(WithAdjustmentBoundaries,
2146                                    ZiplineTestCase):
2147     @classmethod
2148     def make_loader(cls, events, columns):
2149         return partial(NextSplitAdjustedEarningsEstimatesLoader,
2150                        events,
2151                        columns,
2152                        split_adjustments_loader=cls.adjustment_reader,
2153                        split_adjusted_column_names=['estimate'])
2154     @classmethod
2155     def make_expected_out(cls):
2156         split_adjusted_at_start_boundary = pd.concat([
2157             pd.DataFrame({
2158                 SID_FIELD_NAME: cls.s0,
2159                 'estimate': 10,
2160             }, index=pd.date_range(
2161                 cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'
2162             )),
2163                 SID_FIELD_NAME: cls.s1,
2164                 'estimate': 11.,
2165             }, index<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>=pd.date_range(cls.test_start_date,
2166                                    cls.test_start_date,
2167                                    tz='utc')),
2168             pd.DataFrame({
2169                 SID_FIELD_NAME: cls.s2,
2170                 'estimate': 12.,
2171             }, index=pd.date_range(cls.test_end_date,
2172                                    cls.test_end_date,
2173                                    tz='utc')),
2174             pd.DataFrame({
2175                 SID_FIELD_NAME: cls.s3,
2176                 'estimate': 13. * .13,
2177             }, index=pd.date_range(
2178                 cls.test_end_date - timedelta(1), cls.</b></font>test_end_date, tz='utc'
2179             )),
2180             pd.DataFrame({
2181                 SID_FIELD_NAME: cls.s4,
2182                 'estimate': 14.,
2183             }, index=pd.date_range(
2184                 cls.test_end_date - timedelta(1),
2185                 cls.test_end_date - timedelta(1),
2186                 tz='utc'
2187             )),
2188         ]).set_index(SID_FIELD_NAME, append=True).unstack(
2189             SID_FIELD_NAME).reindex(cls.trading_days).stack(
2190             SID_FIELD_NAME, dropna=False)
2191         split_adjusted_at_end_boundary = pd.concat([
2192             pd.DataFrame({
2193                 SID_FIELD_NAME: cls.s0,
2194                 'estimate': 10,
2195             }, index=pd.date_range(
2196                 cls.test_start_date, pd.Timestamp('2015-01-09'), tz='utc'
2197             )),
2198             pd.DataFrame({
2199                 'estimate': 11.,
2200             }, index=pd.date_range(cls.test_start_date,
2201                                    cls<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.test_start_date,
2202                                    tz='utc')),
2203             pd.DataFrame({
2204                 SID_FIELD_NAME: cls.s2,
2205                 'estimate': 12.,
2206             }, index=pd.date_range(cls.test_end_date,
2207                                    cls.test_end_date,
2208                                    tz='utc')),
2209             pd.DataFrame({
2210                 SID_FIELD_NAME: cls.s3,
2211                 'estimate': 13.,
2212             }, index=pd.date_range(
2213                 cls.test_end_date - timedelta(1), cls.</b></font>test_end_date, tz='utc'
2214             )),
2215             pd.DataFrame({
2216                 SID_FIELD_NAME: cls.s4,
2217                 'estimate': 14.,
2218             }, index=pd.date_range(
2219                 cls.test_end_date - timedelta(1),
2220                 cls.test_end_date - timedelta(1),
2221                 tz='utc'
2222             )),
2223         ]).set_index(SID_FIELD_NAME, append=True).unstack(
2224             SID_FIELD_NAME).reindex(cls.trading_days).stack(
2225             SID_FIELD_NAME, dropna=False)
2226         split_adjusted_before_start_boundary = split_adjusted_at_start_boundary
2227         split_adjusted_after_end_boundary = split_adjusted_at_end_boundary
2228         return {cls.test_start_date:
2229                 split_adjusted_at_start_boundary,
2230                 cls.split_adjusted_before_start:
2231                 split_adjusted_before_start_boundary,
2232                 cls.test_end_date:
2233                 split_adjusted_at_end_boundary,
2234                 cls.split_adjusted_after_end:
2235                 split_adjusted_after_end_boundary}
2236 class BlazeNextWithAdjustmentBoundaries(NextWithAdjustmentBoundaries):
2237     @classmethod
2238     def make_loader(cls, events, columns):
2239         return partial(BlazeNextSplitAdjustedEstimatesLoader,
2240                        bz.data(events),
2241                        columns,
2242                        split_adjustments_loader=cls.adjustment_reader,
2243                        split_adjusted_column_names=['estimate'])
2244 class QuarterShiftTestCase(ZiplineTestCase):
2245     """
2246     This tests, in isolation, quarter calculation logic for shifting quarters
2247     backwards/forwards from a starting point.
2248     """
2249     def test_quarter_normalization(self):
2250         input_yrs = pd.Series(range(2011, 2015), dtype=np.int64)
2251         input_qtrs = pd.Series(range(1, 5), dtype=np.int64)
2252         result_years, result_quarters = split_normalized_quarters(
2253             normalize_quarters(input_yrs, input_qtrs)
2254         )
2255         assert_equal(input_yrs, result_years)
2256         assert_equal(input_qtrs, result_quarters)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
