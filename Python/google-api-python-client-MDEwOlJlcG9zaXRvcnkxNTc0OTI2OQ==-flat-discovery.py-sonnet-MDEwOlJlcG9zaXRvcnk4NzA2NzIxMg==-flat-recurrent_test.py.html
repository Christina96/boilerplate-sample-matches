
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 9.287925696594428%, Tokens: 9</h2>
        <div class="column">
            <h3>google-api-python-client-MDEwOlJlcG9zaXRvcnkxNTc0OTI2OQ==-flat-discovery.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  __author__ = "jcgregorio@google.com (Joe Gregorio)"
3  __all__ = ["build", "build_from_document", "fix_method_name", "key2param"]
4  from collections import OrderedDict
5  import collections.abc
6  import copy
7  from email.generator import BytesGenerator
8  from email.mime.multipart import MIMEMultipart
9  from email.mime.nonmultipart import MIMENonMultipart
10  import http.client as http_client
11  import io
12  import json
13  import keyword
14  import logging
15  import mimetypes
16  import os
17  import re
18  import urllib
19  import google.api_core.client_options
20  from google.auth.exceptions import MutualTLSChannelError
21  from google.auth.transport import mtls
22  from google.oauth2 import service_account
23  import httplib2
24  import uritemplate
25  try:
26      import google_auth_httplib2
27  except ImportError:  # pragma: NO COVER
28      google_auth_httplib2 = None
29  from googleapiclient import _auth, mimeparse
30  from googleapiclient._helpers import _add_query_parameter, positional
31  from googleapiclient.errors import (
32      HttpError,
33      InvalidJsonError,
34      MediaUploadSizeError,
35      UnacceptableMimeTypeError,
36      UnknownApiNameOrVersion,
37      UnknownFileType,
38  )
39  from googleapiclient.http import (
40      BatchHttpRequest,
41      HttpMock,
42      HttpMockSequence,
43      HttpRequest,
44      MediaFileUpload,
45      MediaUpload,
46      build_http,
47  )
48  from googleapiclient.model import JsonModel, MediaModel, RawModel
49  from googleapiclient.schema import Schemas
50  httplib2.RETRIES = 1
51  logger = logging.getLogger(__name__)
52  URITEMPLATE = re.compile("{[^}]*}")
53  VARNAME = re.compile("[a-zA-Z0-9_-]+")
54  DISCOVERY_URI = (
55      "https://www.googleapis.com/discovery/v1/apis/" "{api}/{apiVersion}/rest"
56  )
57  V1_DISCOVERY_URI = DISCOVERY_URI
58  V2_DISCOVERY_URI = (
59      "https://{api}.googleapis.com/$discovery/rest?" "version={apiVersion}"
60  )
61  DEFAULT_METHOD_DOC = "A description of how to use this function"
62  HTTP_PAYLOAD_METHODS = frozenset(["PUT", "POST", "PATCH"])
63  _MEDIA_SIZE_BIT_SHIFTS = {"KB": 10, "MB": 20, "GB": 30, "TB": 40}
64  BODY_PARAMETER_DEFAULT_VALUE = {"description": "The request body.", "type": "object"}
65  MEDIA_BODY_PARAMETER_DEFAULT_VALUE = {
66      "description": (
67          "The filename of the media request body, or an instance "
68          "of a MediaUpload object."
69      ),
70      "type": "string",
71      "required": False,
72  }
73  MEDIA_MIME_TYPE_PARAMETER_DEFAULT_VALUE = {
74      "description": (
75          "The MIME type of the media request body, or an instance "
76          "of a MediaUpload object."
77      ),
78      "type": "string",
79      "required": False,
80  }
81  _PAGE_TOKEN_NAMES = ("pageToken", "nextPageToken")
82  GOOGLE_API_USE_CLIENT_CERTIFICATE = "GOOGLE_API_USE_CLIENT_CERTIFICATE"
83  GOOGLE_API_USE_MTLS_ENDPOINT = "GOOGLE_API_USE_MTLS_ENDPOINT"
84  STACK_QUERY_PARAMETERS = frozenset(["trace", "pp", "userip", "strict"])
85  STACK_QUERY_PARAMETER_DEFAULT_VALUE = {"type": "string", "location": "query"}
86  RESERVED_WORDS = frozenset(["body"])
87  class _BytesGenerator(BytesGenerator):
88      _write_lines = BytesGenerator.write
89  def fix_method_name(name):
90      name = name.replace("$", "_").replace("-", "_")
91      if keyword.iskeyword(name) or name in RESERVED_WORDS:
92          return name + "_"
93      else:
94          return name
95  def key2param(key):
96      result = []
97      key = list(key)
98      if not key[0].isalpha():
99          result.append("x")
100      for c in key:
101          if c.isalnum():
102              result.append(c)
103          else:
104              result.append("_")
105      return "".join(result)
106  @positional(2)
107  def build(
108      serviceName,
109      version,
110      http=None,
111      discoveryServiceUrl=None,
112      developerKey=None,
113      model=None,
114      requestBuilder=HttpRequest,
115      credentials=None,
116      cache_discovery=True,
117      cache=None,
118      client_options=None,
119      adc_cert_path=None,
120      adc_key_path=None,
121      num_retries=1,
122      static_discovery=None,
123      always_use_jwt_access=False,
124  ):
125      params = {"api": serviceName, "apiVersion": version}
126      if static_discovery is None:
127          if discoveryServiceUrl is None:
128              static_discovery = True
129          else:
130              static_discovery = False
131      if http is None:
132          discovery_http = build_http()
133      else:
134          discovery_http = http
135      service = None
136      for discovery_url in _discovery_service_uri_options(discoveryServiceUrl, version):
137          requested_url = uritemplate.expand(discovery_url, params)
138          try:
139              content = _retrieve_discovery_doc(
140                  requested_url,
141                  discovery_http,
142                  cache_discovery,
143                  serviceName,
144                  version,
145                  cache,
146                  developerKey,
147                  num_retries=num_retries,
148                  static_discovery=static_discovery,
149              )
150              service = build_from_document(
151                  content,
152                  base=discovery_url,
153                  http=http,
154                  developerKey=developerKey,
155                  model=model,
156                  requestBuilder=requestBuilder,
157                  credentials=credentials,
158                  client_options=client_options,
159                  adc_cert_path=adc_cert_path,
160                  adc_key_path=adc_key_path,
161                  always_use_jwt_access=always_use_jwt_access,
162              )
163              break  # exit if a service was created
164          except HttpError as e:
165              if e.resp.status == http_client.NOT_FOUND:
166                  continue
167              else:
168                  raise e
169      if http is None:
170          discovery_http.close()
171      if service is None:
172          raise UnknownApiNameOrVersion("name: %s  version: %s" % (serviceName, version))
173      else:
174          return service
175  def _discovery_service_uri_options(discoveryServiceUrl, version):
176      if discoveryServiceUrl is not None:
177          return [discoveryServiceUrl]
178      if version is None:
179          logger.warning(
180              "Discovery V1 does not support empty versions. Defaulting to V2..."
181          )
182          return [V2_DISCOVERY_URI]
183      else:
184          return [DISCOVERY_URI, V2_DISCOVERY_URI]
185  def _retrieve_discovery_doc(
186      url,
187      http,
188      cache_discovery,
189      serviceName,
190      version,
191      cache=None,
192      developerKey=None,
193      num_retries=1,
194      static_discovery=True,
195  ):
196      from . import discovery_cache
197      if cache_discovery:
198          if cache is None:
199              cache = discovery_cache.autodetect()
200          if cache:
201              content = cache.get(url)
202              if content:
203                  return content
204      if static_discovery:
205          content = discovery_cache.get_static_doc(serviceName, version)
206          if content:
207              return content
208          else:
209              raise UnknownApiNameOrVersion(
210                  "name: %s  version: %s" % (serviceName, version)
211              )
212      actual_url = url
213      if "REMOTE_ADDR" in os.environ:
214          actual_url = _add_query_parameter(url, "userIp", os.environ["REMOTE_ADDR"])
215      if developerKey:
216          actual_url = _add_query_parameter(url, "key", developerKey)
217      logger.debug("URL being requested: GET %s", actual_url)
218      req = HttpRequest(http, HttpRequest.null_postproc, actual_url)
219      resp, content = req.execute(num_retries=num_retries)
220      try:
221          content = content.decode("utf-8")
222      except AttributeError:
223          pass
224      try:
225          service = json.loads(content)
226      except ValueError as e:
227          logger.error("Failed to parse as JSON: " + content)
228          raise InvalidJsonError()
229      if cache_discovery and cache:
230          cache.set(url, content)
231      return content
232  @positional(1)
233  def build_from_document(
234      service,
235      base=None,
236      future=None,
237      http=None,
238      developerKey=None,
239      model=None,
240      requestBuilder=HttpRequest,
241      credentials=None,
242      client_options=None,
243      adc_cert_path=None,
244      adc_key_path=None,
245      always_use_jwt_access=False,
246  ):
247      if client_options is None:
248          client_options = google.api_core.client_options.ClientOptions()
249      if isinstance(client_options, collections.abc.Mapping):
250          client_options = google.api_core.client_options.from_dict(client_options)
251      if http is not None:
252          banned_options = [
253              (credentials, "credentials"),
254              (client_options.credentials_file, "client_options.credentials_file"),
255          ]
256          for option, name in banned_options:
257              if option is not None:
258                  raise ValueError(
259                      "Arguments http and {} are mutually exclusive".format(name)
260                  )
261      if isinstance(service, str):
262          service = json.loads(service)
263      elif isinstance(service, bytes):
264          service = json.loads(service.decode("utf-8"))
265      if "rootUrl" not in service and isinstance(http, (HttpMock, HttpMockSequence)):
266          logger.error(
267              "You are using HttpMock or HttpMockSequence without"
268              + "having the service discovery doc in cache. Try calling "
269              + "build() without mocking once first to populate the "
270              + "cache."
271          )
272          raise InvalidJsonError()
273      base = urllib.parse.urljoin(service["rootUrl"], service["servicePath"])
274      audience_for_self_signed_jwt = base
275      if client_options.api_endpoint:
276          base = client_options.api_endpoint
277      schema = Schemas(service)
278      if http is None:
279          scopes = list(
280              service.get("auth", {}).get("oauth2", {}).get("scopes", {}).keys()
281          )
282          if scopes and not developerKey:
283              if client_options.credentials_file and credentials:
284                  raise google.api_core.exceptions.DuplicateCredentialArgs(
285                      "client_options.credentials_file and credentials are mutually exclusive."
286                  )
287              if client_options.credentials_file:
288                  credentials = _auth.credentials_from_file(
289                      client_options.credentials_file,
290                      scopes=client_options.scopes,
291                      quota_project_id=client_options.quota_project_id,
292                  )
293              if credentials is None:
294                  credentials = _auth.default_credentials(
295                      scopes=client_options.scopes,
296                      quota_project_id=client_options.quota_project_id,
297                  )
298              if not client_options.scopes:
299                  credentials = _auth.with_scopes(credentials, scopes)
300          if (
301              credentials
302              and isinstance(credentials, service_account.Credentials)
303              and always_use_jwt_access
304              and hasattr(service_account.Credentials, "with_always_use_jwt_access")
305          ):
306              credentials = credentials.with_always_use_jwt_access(always_use_jwt_access)
307              credentials._create_self_signed_jwt(audience_for_self_signed_jwt)
308          if credentials:
309              http = _auth.authorized_http(credentials)
310          else:
311              http = build_http()
312          client_cert_to_use = None
313          use_client_cert = os.getenv(GOOGLE_API_USE_CLIENT_CERTIFICATE, "false")
314          if not use_client_cert in ("true", "false"):
315              raise MutualTLSChannelError(
316                  "Unsupported GOOGLE_API_USE_CLIENT_CERTIFICATE value. Accepted values: true, false"
317              )
318          if client_options and client_options.client_cert_source:
319              raise MutualTLSChannelError(
320                  "ClientOptions.client_cert_source is not supported, please use ClientOptions.client_encrypted_cert_source."
321              )
322          if use_client_cert == "true":
323              if (
324                  client_options
325                  and hasattr(client_options, "client_encrypted_cert_source")
326                  and client_options.client_encrypted_cert_source
327              ):
328                  client_cert_to_use = client_options.client_encrypted_cert_source
329              elif (
330                  adc_cert_path and adc_key_path and mtls.has_default_client_cert_source()
331              ):
332                  client_cert_to_use = mtls.default_client_encrypted_cert_source(
333                      adc_cert_path, adc_key_path
334                  )
335          if client_cert_to_use:
336              cert_path, key_path, passphrase = client_cert_to_use()
337              http_channel = (
338                  http.http
339                  if google_auth_httplib2
340                  and isinstance(http, google_auth_httplib2.AuthorizedHttp)
341                  else http
342              )
343              http_channel.add_certificate(key_path, cert_path, "", passphrase)
344          if "mtlsRootUrl" in service and (
345              not client_options or not client_options.api_endpoint
346          ):
347              mtls_endpoint = urllib.parse.urljoin(
348                  service["mtlsRootUrl"], service["servicePath"]
349              )
350              use_mtls_endpoint = os.getenv(GOOGLE_API_USE_MTLS_ENDPOINT, "auto")
351              if not use_mtls_endpoint in ("never", "auto", "always"):
352                  raise MutualTLSChannelError(
353                      "Unsupported GOOGLE_API_USE_MTLS_ENDPOINT value. Accepted values: never, auto, always"
354                  )
355              if use_mtls_endpoint == "always" or (
356                  use_mtls_endpoint == "auto" and client_cert_to_use
357              ):
358                  base = mtls_endpoint
359      if model is None:
360          features = service.get("features", [])
361          model = JsonModel("dataWrapper" in features)
362      return Resource(
363          http=http,
364          baseUrl=base,
365          model=model,
366          developerKey=developerKey,
367          requestBuilder=requestBuilder,
368          resourceDesc=service,
369          rootDesc=service,
370          schema=schema,
371      )
372  def _cast(value, schema_type):
373      if schema_type == "string":
374          if type(value) == type("") or type(value) == type(""):
375              return value
376          else:
377              return str(value)
378      elif schema_type == "integer":
379          return str(int(value))
380      elif schema_type == "number":
381          return str(float(value))
382      elif schema_type == "boolean":
383          return str(bool(value)).lower()
384      else:
385          if type(value) == type("") or type(value) == type(""):
386              return value
387          else:
388              return str(value)
389  def _media_size_to_long(maxSize):
390      if len(maxSize) < 2:
391          return 0
392      units = maxSize[-2:].upper()
393      bit_shift = _MEDIA_SIZE_BIT_SHIFTS.get(units)
394      if bit_shift is not None:
395          return int(maxSize[:-2]) << bit_shift
396      else:
397          return int(maxSize)
398  def _media_path_url_from_info(root_desc, path_url):
399      return "%(root)supload/%(service_path)s%(path)s" % {
400          "root": root_desc["rootUrl"],
401          "service_path": root_desc["servicePath"],
402          "path": path_url,
403      }
404  def _fix_up_parameters(method_desc, root_desc, http_method, schema):
405      parameters = method_desc.setdefault("parameters", {})
406      for name, description in root_desc.get("parameters", {}).items():
407          parameters[name] = description
408      for name in STACK_QUERY_PARAMETERS:
409          parameters[name] = STACK_QUERY_PARAMETER_DEFAULT_VALUE.copy()
410      if http_method in HTTP_PAYLOAD_METHODS and "request" in method_desc:
411          body = BODY_PARAMETER_DEFAULT_VALUE.copy()
412          body.update(method_desc["request"])
413          parameters["body"] = body
414      return parameters
415  def _fix_up_media_upload(method_desc, root_desc, path_url, parameters):
416      media_upload = method_desc.get("mediaUpload", {})
417      accept = media_upload.get("accept", [])
418      max_size = _media_size_to_long(media_upload.get("maxSize", ""))
419      media_path_url = None
420      if media_upload:
421          media_path_url = _media_path_url_from_info(root_desc, path_url)
422          parameters["media_body"] = MEDIA_BODY_PARAMETER_DEFAULT_VALUE.copy()
423          parameters["media_mime_type"] = MEDIA_MIME_TYPE_PARAMETER_DEFAULT_VALUE.copy()
424      return accept, max_size, media_path_url
425  def _fix_up_method_description(method_desc, root_desc, schema):
426      path_url = method_desc["path"]
427      http_method = method_desc["httpMethod"]
428      method_id = method_desc["id"]
429      parameters = _fix_up_parameters(method_desc, root_desc, http_method, schema)
430      accept, max_size, media_path_url = _fix_up_media_upload(
431          method_desc, root_desc, path_url, parameters
432      )
433      return path_url, http_method, method_id, accept, max_size, media_path_url
434  def _fix_up_media_path_base_url(media_path_url, base_url):
435      parsed_media_url = urllib.parse.urlparse(media_path_url)
436      parsed_base_url = urllib.parse.urlparse(base_url)
437      if parsed_media_url.netloc == parsed_base_url.netloc:
438          return media_path_url
439      return urllib.parse.urlunparse(
440          parsed_media_url._replace(netloc=parsed_base_url.netloc)
441      )
442  def _urljoin(base, url):
443      if url.startswith("http://") or url.startswith("https://"):
444          return urllib.parse.urljoin(base, url)
445      new_base = base if base.endswith("/") else base + "/"
446      new_url = url[1:] if url.startswith("/") else url
447      return new_base + new_url
448  class ResourceMethodParameters(object):
449      def __init__(self, method_desc):
450          self.argmap = {}
451          self.required_params = []
452          self.repeated_params = []
453          self.pattern_params = {}
454          self.query_params = []
455          self.path_params = set()
456          self.param_types = {}
457          self.enum_params = {}
458          self.set_parameters(method_desc)
459      def set_parameters(self, method_desc):
460          parameters = method_desc.get("parameters", {})
461          sorted_parameters = OrderedDict(sorted(parameters.items()))
462          for arg, desc in sorted_parameters.items():
463              param = key2param(arg)
464              self.argmap[param] = arg
465              if desc.get("pattern"):
466                  self.pattern_params[param] = desc["pattern"]
467              if desc.get("enum"):
468                  self.enum_params[param] = desc["enum"]
469              if desc.get("required"):
470                  self.required_params.append(param)
471              if desc.get("repeated"):
472                  self.repeated_params.append(param)
473              if desc.get("location") == "query":
474                  self.query_params.append(param)
475              if desc.get("location") == "path":
476                  self.path_params.add(param)
477              self.param_types[param] = desc.get("type", "string")
478          for match in URITEMPLATE.finditer(method_desc["path"]):
479              for namematch in VARNAME.finditer(match.group(0)):
480                  name = key2param(namematch.group(0))
481                  self.path_params.add(name)
482                  if name in self.query_params:
483                      self.query_params.remove(name)
484  def createMethod(methodName, methodDesc, rootDesc, schema):
485      methodName = fix_method_name(methodName)
486      (
487          pathUrl,
488          httpMethod,
489          methodId,
490          accept,
491          maxSize,
492          mediaPathUrl,
493      ) = _fix_up_method_description(methodDesc, rootDesc, schema)
494      parameters = ResourceMethodParameters(methodDesc)
495      def method(self, **kwargs):
496          for name in kwargs:
497              if name not in parameters.argmap:
498                  raise TypeError("Got an unexpected keyword argument {}".format(name))
499          keys = list(kwargs.keys())
500          for name in keys:
501              if kwargs[name] is None:
502                  del kwargs[name]
503          for name in parameters.required_params:
504              if name not in kwargs:
505                  if name not in _PAGE_TOKEN_NAMES or _findPageTokenName(
506                      _methodProperties(methodDesc, schema, "response")
507                  ):
508                      raise TypeError('Missing required parameter "%s"' % name)
509          for name, regex in parameters.pattern_params.items():
510              if name in kwargs:
511                  if isinstance(kwargs[name], str):
512                      pvalues = [kwargs[name]]
513                  else:
514                      pvalues = kwargs[name]
515                  for pvalue in pvalues:
516                      if re.match(regex, pvalue) is None:
517                          raise TypeError(
518                              'Parameter "%s" value "%s" does not match the pattern "%s"'
519                              % (name, pvalue, regex)
520                          )
521          for name, enums in parameters.enum_params.items():
522              if name in kwargs:
523                  if name in parameters.repeated_params and not isinstance(
524                      kwargs[name], str
525                  ):
526                      values = kwargs[name]
527                  else:
528                      values = [kwargs[name]]
529                  for value in values:
530                      if value not in enums:
531                          raise TypeError(
532                              'Parameter "%s" value "%s" is not an allowed value in "%s"'
533                              % (name, value, str(enums))
534                          )
535          actual_query_params = {}
536          actual_path_params = {}
537          for key, value in kwargs.items():
538              to_type = parameters.param_types.get(key, "string")
539              if key in parameters.repeated_params and type(value) == type([]):
540                  cast_value = [_cast(x, to_type) for x in value]
541              else:
542                  cast_value = _cast(value, to_type)
543              if key in parameters.query_params:
544                  actual_query_params[parameters.argmap[key]] = cast_value
545              if key in parameters.path_params:
546                  actual_path_params[parameters.argmap[key]] = cast_value
547          body_value = kwargs.get("body", None)
548          media_filename = kwargs.get("media_body", None)
549          media_mime_type = kwargs.get("media_mime_type", None)
550          if self._developerKey:
551              actual_query_params["key"] = self._developerKey
552          model = self._model
553          if methodName.endswith("_media"):
554              model = MediaModel()
555          elif "response" not in methodDesc:
556              model = RawModel()
557          headers = {}
558          headers, params, query, body = model.request(
559              headers, actual_path_params, actual_query_params, body_value
560          )
561          expanded_url = uritemplate.expand(pathUrl, params)
562          url = _urljoin(self._baseUrl, expanded_url + query)
563          resumable = None
564          multipart_boundary = ""
565          if media_filename:
566              if isinstance(media_filename, str):
567                  if media_mime_type is None:
568                      logger.warning(
569                          "media_mime_type argument not specified: trying to auto-detect for %s",
570                          media_filename,
571                      )
572                      media_mime_type, _ = mimetypes.guess_type(media_filename)
573                  if media_mime_type is None:
574                      raise UnknownFileType(media_filename)
575                  if not mimeparse.best_match([media_mime_type], ",".join(accept)):
576                      raise UnacceptableMimeTypeError(media_mime_type)
577                  media_upload = MediaFileUpload(media_filename, mimetype=media_mime_type)
578              elif isinstance(media_filename, MediaUpload):
579                  media_upload = media_filename
580              else:
581                  raise TypeError("media_filename must be str or MediaUpload.")
582              if media_upload.size() is not None and media_upload.size() > maxSize > 0:
583                  raise MediaUploadSizeError("Media larger than: %s" % maxSize)
584              expanded_url = uritemplate.expand(mediaPathUrl, params)
585              url = _urljoin(self._baseUrl, expanded_url + query)
586              url = _fix_up_media_path_base_url(url, self._baseUrl)
587              if media_upload.resumable():
588                  url = _add_query_parameter(url, "uploadType", "resumable")
589              if media_upload.resumable():
590                  resumable = media_upload
591              else:
592                  if body is None:
593                      headers["content-type"] = media_upload.mimetype()
594                      body = media_upload.getbytes(0, media_upload.size())
595                      url = _add_query_parameter(url, "uploadType", "media")
596                  else:
597                      msgRoot = MIMEMultipart("related")
598                      setattr(msgRoot, "_write_headers", lambda self: None)
599                      msg = MIMENonMultipart(*headers["content-type"].split("/"))
600                      msg.set_payload(body)
601                      msgRoot.attach(msg)
602                      msg = MIMENonMultipart(*media_upload.mimetype().split("/"))
603                      msg["Content-Transfer-Encoding"] = "binary"
604                      payload = media_upload.getbytes(0, media_upload.size())
605                      msg.set_payload(payload)
606                      msgRoot.attach(msg)
607                      fp = io.BytesIO()
608                      g = _BytesGenerator(fp, mangle_from_=False)
609                      g.flatten(msgRoot, unixfrom=False)
610                      body = fp.getvalue()
611                      multipart_boundary = msgRoot.get_boundary()
612                      headers["content-type"] = (
613                          "multipart/related; " 'boundary="%s"'
614                      ) % multipart_boundary
615                      url = _add_query_parameter(url, "uploadType", "multipart")
616          logger.debug("URL being requested: %s %s" % (httpMethod, url))
617          return self._requestBuilder(
618              self._http,
619              model.response,
620              url,
621              method=httpMethod,
622              body=body,
623              headers=headers,
624              methodId=methodId,
625              resumable=resumable,
626          )
627      docs = [methodDesc.get("description", DEFAULT_METHOD_DOC), "\n\n"]
628      if len(parameters.argmap) > 0:
629          docs.append("Args:\n")
630      skip_parameters = list(rootDesc.get("parameters", {}).keys())
631      skip_parameters.extend(STACK_QUERY_PARAMETERS)
632      all_args = list(parameters.argmap.keys())
633      args_ordered = [key2param(s) for s in methodDesc.get("parameterOrder", [])]
634      if "body" in all_args:
635          args_ordered.append("body")
636      for name in sorted(all_args):
637          if name not in args_ordered:
638              args_ordered.append(name)
639      for arg in args_ordered:
640          if arg in skip_parameters:
641              continue
642          repeated = ""
643          if arg in parameters.repeated_params:
644              repeated = " (repeated)"
645          required = ""
646          if arg in parameters.required_params:
647              required = " (required)"
648          paramdesc = methodDesc["parameters"][parameters.argmap[arg]]
649          paramdoc = paramdesc.get("description", "A parameter")
650          if "$ref" in paramdesc:
651              docs.append(
652                  ("  %s: object, %s%s%s\n    The object takes the form of:\n\n%s\n\n")
653                  % (
654                      arg,
655                      paramdoc,
656                      required,
657                      repeated,
658                      schema.prettyPrintByName(paramdesc["$ref"]),
659                  )
660              )
661          else:
662              paramtype = paramdesc.get("type", "string")
663              docs.append(
664                  "  %s: %s, %s%s%s\n" % (arg, paramtype, paramdoc, required, repeated)
665              )
666          enum = paramdesc.get("enum", [])
667          enumDesc = paramdesc.get("enumDescriptions", [])
668          if enum and enumDesc:
669              docs.append("    Allowed values\n")
670              for (name, desc) in zip(enum, enumDesc):
671                  docs.append("      %s - %s\n" % (name, desc))
672      if "response" in methodDesc:
<span onclick='openModal()' class='match'>673          if methodName.endswith("_media"):
674              docs.append("\nReturns:\n  The media object as a string.\n\n    ")
675          else:
676              docs.append("\nReturns:\n  An object of the form:\n\n    ")
677              docs.append(schema.prettyPrintSchema(methodDesc["response"]))
</span>678      setattr(method, "__doc__", "".join(docs))
679      return (methodName, method)
680  def createNextMethod(
681      methodName,
682      pageTokenName="pageToken",
683      nextPageTokenName="nextPageToken",
684      isPageTokenParameter=True,
685  ):
686      methodName = fix_method_name(methodName)
687      def methodNext(self, previous_request, previous_response):
688          nextPageToken = previous_response.get(nextPageTokenName, None)
689          if not nextPageToken:
690              return None
691          request = copy.copy(previous_request)
692          if isPageTokenParameter:
693              request.uri = _add_query_parameter(
694                  request.uri, pageTokenName, nextPageToken
695              )
696              logger.debug("Next page request URL: %s %s" % (methodName, request.uri))
697          else:
698              model = self._model
699              body = model.deserialize(request.body)
700              body[pageTokenName] = nextPageToken
701              request.body = model.serialize(body)
702              request.body_size = len(request.body)
703              if "content-length" in request.headers:
704                  del request.headers["content-length"]
705              logger.debug("Next page request body: %s %s" % (methodName, body))
706          return request
707      return (methodName, methodNext)
708  class Resource(object):
709      def __init__(
710          self,
711          http,
712          baseUrl,
713          model,
714          requestBuilder,
715          developerKey,
716          resourceDesc,
717          rootDesc,
718          schema,
719      ):
720          self._dynamic_attrs = []
721          self._http = http
722          self._baseUrl = baseUrl
723          self._model = model
724          self._developerKey = developerKey
725          self._requestBuilder = requestBuilder
726          self._resourceDesc = resourceDesc
727          self._rootDesc = rootDesc
728          self._schema = schema
729          self._set_service_methods()
730      def _set_dynamic_attr(self, attr_name, value):
731          self._dynamic_attrs.append(attr_name)
732          self.__dict__[attr_name] = value
733      def __getstate__(self):
734          state_dict = copy.copy(self.__dict__)
735          for dynamic_attr in self._dynamic_attrs:
736              del state_dict[dynamic_attr]
737          del state_dict["_dynamic_attrs"]
738          return state_dict
739      def __setstate__(self, state):
740          self.__dict__.update(state)
741          self._dynamic_attrs = []
742          self._set_service_methods()
743      def __enter__(self):
744          return self
745      def __exit__(self, exc_type, exc, exc_tb):
746          self.close()
747      def close(self):
748          self._http.close()
749      def _set_service_methods(self):
750          self._add_basic_methods(self._resourceDesc, self._rootDesc, self._schema)
751          self._add_nested_resources(self._resourceDesc, self._rootDesc, self._schema)
752          self._add_next_methods(self._resourceDesc, self._schema)
753      def _add_basic_methods(self, resourceDesc, rootDesc, schema):
754          if resourceDesc == rootDesc:
755              batch_uri = "%s%s" % (
756                  rootDesc["rootUrl"],
757                  rootDesc.get("batchPath", "batch"),
758              )
759              def new_batch_http_request(callback=None):
760                  return BatchHttpRequest(callback=callback, batch_uri=batch_uri)
761              self._set_dynamic_attr("new_batch_http_request", new_batch_http_request)
762          if "methods" in resourceDesc:
763              for methodName, methodDesc in resourceDesc["methods"].items():
764                  fixedMethodName, method = createMethod(
765                      methodName, methodDesc, rootDesc, schema
766                  )
767                  self._set_dynamic_attr(
768                      fixedMethodName, method.__get__(self, self.__class__)
769                  )
770                  if methodDesc.get("supportsMediaDownload", False):
771                      fixedMethodName, method = createMethod(
772                          methodName + "_media", methodDesc, rootDesc, schema
773                      )
774                      self._set_dynamic_attr(
775                          fixedMethodName, method.__get__(self, self.__class__)
776                      )
777      def _add_nested_resources(self, resourceDesc, rootDesc, schema):
778          if "resources" in resourceDesc:
779              def createResourceMethod(methodName, methodDesc):
780                  methodName = fix_method_name(methodName)
781                  def methodResource(self):
782                      return Resource(
783                          http=self._http,
784                          baseUrl=self._baseUrl,
785                          model=self._model,
786                          developerKey=self._developerKey,
787                          requestBuilder=self._requestBuilder,
788                          resourceDesc=methodDesc,
789                          rootDesc=rootDesc,
790                          schema=schema,
791                      )
792                  setattr(methodResource, "__doc__", "A collection resource.")
793                  setattr(methodResource, "__is_resource__", True)
794                  return (methodName, methodResource)
795              for methodName, methodDesc in resourceDesc["resources"].items():
796                  fixedMethodName, method = createResourceMethod(methodName, methodDesc)
797                  self._set_dynamic_attr(
798                      fixedMethodName, method.__get__(self, self.__class__)
799                  )
800      def _add_next_methods(self, resourceDesc, schema):
801          if "methods" not in resourceDesc:
802              return
803          for methodName, methodDesc in resourceDesc["methods"].items():
804              nextPageTokenName = _findPageTokenName(
805                  _methodProperties(methodDesc, schema, "response")
806              )
807              if not nextPageTokenName:
808                  continue
809              isPageTokenParameter = True
810              pageTokenName = _findPageTokenName(methodDesc.get("parameters", {}))
811              if not pageTokenName:
812                  isPageTokenParameter = False
813                  pageTokenName = _findPageTokenName(
814                      _methodProperties(methodDesc, schema, "request")
815                  )
816              if not pageTokenName:
817                  continue
818              fixedMethodName, method = createNextMethod(
819                  methodName + "_next",
820                  pageTokenName,
821                  nextPageTokenName,
822                  isPageTokenParameter,
823              )
824              self._set_dynamic_attr(
825                  fixedMethodName, method.__get__(self, self.__class__)
826              )
827  def _findPageTokenName(fields):
828      return next(
829          (tokenName for tokenName in _PAGE_TOKEN_NAMES if tokenName in fields), None
830      )
831  def _methodProperties(methodDesc, schema, name):
832      desc = methodDesc.get(name, {})
833      if "$ref" in desc:
834          desc = schema.get(desc["$ref"], {})
835      return desc.get("properties", {})
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-recurrent_test.py</h3>
            <pre><code>1  import itertools
2  import unittest
3  from absl.testing import parameterized
4  import numpy as np
5  from sonnet.src import initializers
6  from sonnet.src import recurrent
7  from sonnet.src import test_utils
8  import tensorflow as tf
9  import tree
10  class VanillaRNNTest(test_utils.TestCase, parameterized.TestCase):
11    def setUp(self):
12      super().setUp()
13      self.batch_size = 3
14      self.input_size = 2
15      self.hidden_size = 16
16    @parameterized.parameters([False, True])
17    def testComputationAgainstNumPy(self, use_tf_function):
18      inputs = self.evaluate(
19          tf.random.uniform([self.batch_size, self.input_size]))
20      core = recurrent.VanillaRNN(
21          hidden_size=self.hidden_size, activation=tf.tanh)
22      prev_state = self.evaluate(core.initial_state(self.batch_size))
23      core_fn = tf.function(core) if use_tf_function else core
24      outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
25      expected_output = np.tanh(
26          inputs.dot(self.evaluate(core.input_to_hidden)) +
27          prev_state.dot(self.evaluate(core.hidden_to_hidden)) +
28          self.evaluate(core._b))
29      atol = 3e-2 if self.primary_device == "TPU" else 1e-6
30      self.assertAllClose(outputs, expected_output, atol=atol)
31      self.assertAllClose(next_state, expected_output, atol=atol)
32    def testDtypeMismatch(self):
33      core = recurrent.VanillaRNN(hidden_size=self.hidden_size, dtype=tf.bfloat16)
34      inputs = tf.random.uniform([self.batch_size, self.input_size])
35      prev_state = core.initial_state(self.batch_size)
36      self.assertIs(prev_state.dtype, tf.bfloat16)
37      with self.assertRaisesRegex(
38          TypeError, "inputs must have dtype tf.bfloat16, got tf.float32"):
39        core(inputs, prev_state)
40    def testInitialization(self):
41      core = recurrent.VanillaRNN(
42          hidden_size=self.hidden_size,
43          w_i_init=initializers.Ones(),
44          w_h_init=initializers.Ones(),
45          b_init=initializers.Ones())
46      inputs = tf.random.uniform([self.batch_size, self.input_size])
47      prev_state = core.initial_state(self.batch_size)
48      core(inputs, prev_state)
49      for v in core.variables:
50        self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))
51  class DeepRNNTest(test_utils.TestCase, parameterized.TestCase):
52    def setUp(self):
53      super().setUp()
54      self.batch_size = 3
55      self.input_size = 2
56      self.hidden_size = 16
57    @parameterized.parameters([False, True])
58    def testComputationAgainstNumPy(self, use_tf_function):
59      inputs = self.evaluate(
60          tf.random.uniform([self.batch_size, self.input_size]))
61      core = recurrent.DeepRNN([
62          recurrent.VanillaRNN(hidden_size=self.hidden_size),
63          recurrent.VanillaRNN(hidden_size=2 * self.hidden_size)
64      ])
65      prev_state = self.evaluate(core.initial_state(self.batch_size))
66      core_fn = tf.function(core) if use_tf_function else core
67      outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
68      expected_outputs = inputs
69      expected_next_state = list(prev_state)
70      for idx, l in enumerate(core._layers):
71        expected_outputs, expected_next_state[idx] = l(expected_outputs,
72                                                       prev_state[idx])
73      self.assertAllClose(outputs, expected_outputs)
74      self.assertAllClose(next_state, tuple(expected_next_state))
75    @parameterized.parameters([False, True])
76    def testComputationAgainstNumPyWithCallables(self, use_tf_function):
77      inputs = self.evaluate(
78          tf.random.uniform([self.batch_size, self.input_size]))
79      core = recurrent.DeepRNN([tf.tanh, tf.sign])
80      prev_state = self.evaluate(core.initial_state(self.batch_size))
81      core_fn = tf.function(core) if use_tf_function else core
82      outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
83      self.assertAllClose(outputs, np.sign(np.tanh(inputs)))
84      self.assertEqual(next_state, prev_state)
85    def testInitialState(self):
86      core0 = recurrent.VanillaRNN(hidden_size=self.hidden_size)
87      core1 = recurrent.VanillaRNN(hidden_size=2 * self.hidden_size)
88      deep_rnn = recurrent.DeepRNN([core0, tf.tanh, core1, tf.sign])
89      prev_state = deep_rnn.initial_state(self.batch_size)
90      self.assertAllClose(prev_state[0], core0.initial_state(self.batch_size))
91      self.assertAllClose(prev_state[1], core1.initial_state(self.batch_size))
92    @parameterized.parameters([False, True])
93    def testWithSkipConnectionsOutputs(self, use_tf_function):
94      inputs = self.evaluate(
95          tf.random.uniform([self.batch_size, self.input_size]))
96      core = recurrent.deep_rnn_with_skip_connections([
97          recurrent.VanillaRNN(hidden_size=self.hidden_size),
98          recurrent.VanillaRNN(hidden_size=2 * self.hidden_size)
99      ],
100                                                      concat_final_output=False)
101      prev_state = self.evaluate(core.initial_state(self.batch_size))
102      core_fn = tf.function(core) if use_tf_function else core
103      outputs, _ = core_fn(tf.convert_to_tensor(inputs), prev_state)
104      self.assertEqual(outputs.shape,
105                       tf.TensorShape([self.batch_size, 2 * self.hidden_size]))
106    def testWithConnectionsValidation(self):
107      with self.assertRaisesRegex(ValueError, "to be instances of RNNCore"):
108        recurrent.deep_rnn_with_skip_connections([tf.tanh])
109      with self.assertRaisesRegex(ValueError, "to be instances of RNNCore"):
110        recurrent.deep_rnn_with_residual_connections([tf.tanh])
111  class LSTMTest(test_utils.TestCase, parameterized.TestCase):
112    def setUp(self):
113      super().setUp()
114      self.batch_size = 3
115      self.input_size = 2
116      self.hidden_size = 16
117    @parameterized.parameters(
118        itertools.product([False, True], [None, 4], [0.0, 1.0]))
119    def testComputationAgainstNumPy(self, use_tf_function, projection_size,
120                                    forget_bias):
121      inputs = self.evaluate(
122          tf.random.uniform([self.batch_size, self.input_size]))
123      core = recurrent.LSTM(
124          self.hidden_size,
125          projection_size=projection_size,
126          forget_bias=forget_bias)
127      prev_state = self.evaluate(core.initial_state(self.batch_size))
128      core_fn = tf.function(core) if use_tf_function else core
129      outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
130      w_ii, w_if, w_ig, w_io = np.hsplit(self.evaluate(core.input_to_hidden), 4)
131      w_hi, w_hf, w_hg, w_ho = np.hsplit(self.evaluate(core.hidden_to_hidden), 4)
132      b_i, b_f, b_g, b_o = np.hsplit(self.evaluate(core.b), 4)
133      i = expit(inputs.dot(w_ii) + prev_state.hidden.dot(w_hi) + b_i)
134      f = expit(inputs.dot(w_if) + prev_state.hidden.dot(w_hf) + b_f)
135      g = np.tanh(inputs.dot(w_ig) + prev_state.hidden.dot(w_hg) + b_g)
136      o = expit(inputs.dot(w_io) + prev_state.hidden.dot(w_ho) + b_o)
137      expected_cell = f * prev_state.cell + i * g
138      expected_hidden = o * np.tanh(expected_cell)
139      if projection_size is not None:
140        expected_hidden = expected_hidden.dot(self.evaluate(core.projection))
141      atol = 1e-2 if self.primary_device == "TPU" else 1e-6
142      self.assertAllClose(outputs, next_state.hidden, atol=atol)
143      self.assertAllClose(expected_hidden, next_state.hidden, atol=atol)
144      self.assertAllClose(expected_cell, next_state.cell, atol=atol)
145    def testDtypeMismatch(self):
146      core = recurrent.LSTM(hidden_size=self.hidden_size, dtype=tf.bfloat16)
147      inputs = tf.random.uniform([self.batch_size, self.input_size])
148      prev_state = core.initial_state(self.batch_size)
149      self.assertIs(prev_state.hidden.dtype, tf.bfloat16)
150      self.assertIs(prev_state.cell.dtype, tf.bfloat16)
151      with self.assertRaisesRegex(
152          TypeError, "inputs must have dtype tf.bfloat16, got tf.float32"):
153        core(inputs, prev_state)
154    def testInitialization(self):
155      projection_size = 4
156      core = recurrent.LSTM(
157          hidden_size=self.hidden_size,
158          projection_size=projection_size,
159          projection_init=initializers.Ones(),
160          w_i_init=initializers.Ones(),
161          w_h_init=initializers.Ones(),
162          b_init=initializers.Ones(),
163          forget_bias=0.0)
164      inputs = tf.random.uniform([self.batch_size, self.input_size])
165      prev_state = core.initial_state(self.batch_size)
166      core(inputs, prev_state)
167      for v in core.variables:
168        self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))
169    @parameterized.parameters([1e-6, 0.5, 1 - 1e-6])
170    def testRecurrentDropout(self, rate):
171      num_steps = 2
172      inputs = tf.random.uniform([num_steps, self.batch_size, self.input_size])
173      train_core, test_core = recurrent.lstm_with_recurrent_dropout(
174          self.hidden_size, dropout=rate)
175      [_, train_output
176      ], _ = recurrent.dynamic_unroll(train_core, inputs,
177                                      train_core.initial_state(self.batch_size))
178      [_, test_output
179      ], _ = recurrent.dynamic_unroll(test_core, inputs,
180                                      test_core.initial_state(self.batch_size))
181      almost_zero = rate == 1e-6
182      if almost_zero:
183        rtol = 1e-3 if self.primary_device == "TPU" else 1e-6
184        self.assertAllClose(train_output, test_output, rtol=rtol)
185      else:
186        self.assertGreater(
187            self.evaluate(tf.reduce_max(tf.abs(train_output - test_output))),
188            0.001)
189    def testRecurrentDropoutInvalid(self):
190      with self.assertRaisesRegex(ValueError,
191                                  r"dropout must be in the range \[0, 1\).+"):
192        recurrent.lstm_with_recurrent_dropout(self.hidden_size, -1)
193  class UnrolledLSTMTest(test_utils.TestCase, parameterized.TestCase):
194    def setUp(self):
195      super().setUp()
196      self.batch_size = 3
197      self.input_size = 2
198      self.hidden_size = 16
199    @parameterized.parameters(itertools.product([1, 4], [True, False]))
200    def testComputationAgainstLSTM(self, num_steps, use_tf_function):
201      unrolled_lstm = recurrent.UnrolledLSTM(self.hidden_size)
202      initial_state = unrolled_lstm.initial_state(self.batch_size)
203      if use_tf_function:
204        @tf.function
205        def unrolled_lstm_fn(*args, **kwargs):
206          with tf.device("/device:{}:0".format(self.primary_device)):
207            return unrolled_lstm(*args, **kwargs)
208      else:
209        unrolled_lstm_fn = unrolled_lstm
210      input_sequence = tf.random.uniform(
211          [num_steps, self.batch_size, self.input_size])
212      output_sequence, final_state = unrolled_lstm_fn(input_sequence,
213                                                      initial_state)
214      with tf.device("/device:CPU:0"):  # Use CPU as the baseline.
215        lstm = recurrent.LSTM(self.hidden_size)
216        lstm._initialize(input_sequence[0])
217        lstm._w_i = unrolled_lstm._w_i
218        lstm._w_h = unrolled_lstm._w_h
219        lstm.b = unrolled_lstm.b
220        expected_output_sequence, expected_final_state = recurrent.dynamic_unroll(
221            lstm, input_sequence, lstm.initial_state(self.batch_size))
222      atol = 1e-2 if self.primary_device == "TPU" else 1e-6
223      self.assertAllClose(output_sequence, expected_output_sequence, atol=atol)
224      self.assertAllClose(
225          final_state.hidden, expected_final_state.hidden, atol=atol)
226      self.assertAllClose(final_state.cell, expected_final_state.cell, atol=atol)
227    @parameterized.parameters([True, False])
228    def testNumStepsPolymorphism(self, use_tf_function):
229      unrolled_lstm = recurrent.UnrolledLSTM(self.hidden_size)
230      initial_state = unrolled_lstm.initial_state(self.batch_size)
231      if use_tf_function:
232        @tf.function
233        def unrolled_lstm_fn(*args, **kwargs):
234          with tf.device("/device:%s:0" % self.primary_device):
235            return unrolled_lstm(*args, **kwargs)
236      else:
237        unrolled_lstm_fn = unrolled_lstm
238      for num_steps in [1, 2, 4]:
239        output_sequence, _ = unrolled_lstm_fn(
240            tf.random.uniform([num_steps, self.batch_size, self.input_size]),
241            initial_state)
242        self.assertEqual(output_sequence.shape[0], num_steps)
243    def testDtypeMismatch(self):
244      unrolled_lstm = recurrent.UnrolledLSTM(
245          hidden_size=self.hidden_size, dtype=tf.bfloat16)
246      input_sequence = tf.random.uniform([1, self.batch_size, self.input_size])
247      initial_state = unrolled_lstm.initial_state(self.batch_size)
248      self.assertIs(initial_state.hidden.dtype, tf.bfloat16)
249      self.assertIs(initial_state.cell.dtype, tf.bfloat16)
250      with self.assertRaisesRegex(
251          TypeError, "inputs must have dtype tf.bfloat16, got tf.float32"):
252        unrolled_lstm(input_sequence, initial_state)
253    def testInitialization(self):
254      unrolled_lstm = recurrent.UnrolledLSTM(
255          hidden_size=self.hidden_size,
256          forget_bias=0.0,
257          w_i_init=initializers.Ones(),
258          w_h_init=initializers.Ones(),
259          b_init=initializers.Ones())
260      input_sequence = tf.random.uniform([1, self.batch_size, self.input_size])
261      initial_state = unrolled_lstm.initial_state(self.batch_size)
262      unrolled_lstm(input_sequence, initial_state)
263      for v in unrolled_lstm.variables:
264        self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))
265  class ConvNDLSTMTest(test_utils.TestCase, parameterized.TestCase):
266    def setUp(self):
267      super().setUp()
268      self.batch_size = 3
269      self.input_size = 2
270      self.hidden_size = 16
271      self.input_channels = 3
272      self.output_channels = 5
273    @parameterized.parameters(
274        itertools.product(
275            [False, True],
276            [recurrent.Conv1DLSTM, recurrent.Conv2DLSTM, recurrent.Conv3DLSTM]))
277    def testComputationAgainstNumPy(self, use_tf_function, core_cls):
278      if core_cls is recurrent.Conv1DLSTM:
279        num_spatial_dims = 1
280      elif core_cls is recurrent.Conv2DLSTM:
281        num_spatial_dims = 2
282      else:
283        assert core_cls is recurrent.Conv3DLSTM
284        num_spatial_dims = 3
285      input_shape = ((self.batch_size,) + (self.input_size,) * num_spatial_dims +
286                     (self.input_channels,))
287      inputs = self.evaluate(tf.random.uniform(input_shape))
288      core = core_cls(input_shape[1:], self.output_channels, kernel_shape=1)
289      prev_state = self.evaluate(core.initial_state(self.batch_size))
290      core_fn = tf.function(core) if use_tf_function else core
291      outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
292      def conv(x, f):
293        return self.evaluate(tf.nn.convolution(x, f, strides=1, padding="SAME"))
294      w_i = self.evaluate(core.input_to_hidden)
295      w_h = self.evaluate(core.hidden_to_hidden)
296      w_ii, w_if, w_ig, w_io = np.split(w_i, 4, axis=-1)
297      w_hi, w_hf, w_hg, w_ho = np.split(w_h, 4, axis=-1)
298      b_i, b_f, b_g, b_o = np.hsplit(self.evaluate(core.b), 4)
299      i = expit(conv(inputs, w_ii) + conv(prev_state.hidden, w_hi) + b_i)
300      f = expit(conv(inputs, w_if) + conv(prev_state.hidden, w_hf) + b_f)
301      g = np.tanh(conv(inputs, w_ig) + conv(prev_state.hidden, w_hg) + b_g)
302      o = expit(conv(inputs, w_io) + conv(prev_state.hidden, w_ho) + b_o)
303      expected_cell = f * prev_state.cell + i * g
304      expected_hidden = o * np.tanh(expected_cell)
305      atol = 1e-2 if self.primary_device == "TPU" else 1e-6
306      self.assertAllClose(outputs, next_state.hidden, atol=atol)
307      self.assertAllClose(expected_hidden, next_state.hidden, atol=atol)
308      self.assertAllClose(expected_cell, next_state.cell, atol=atol)
309    def testDtypeMismatch(self):
310      num_spatial_dims = 1
311      input_shape = ((self.batch_size,) + (self.input_size,) * num_spatial_dims +
312                     (self.input_channels,))
313      core = recurrent.Conv1DLSTM(
314          input_shape[1:],
315          self.output_channels,
316          kernel_shape=1,
317          dtype=tf.bfloat16)
318      inputs = tf.random.uniform(input_shape)
319      prev_state = core.initial_state(self.batch_size)
320      self.assertIs(prev_state.hidden.dtype, tf.bfloat16)
321      self.assertIs(prev_state.cell.dtype, tf.bfloat16)
322      with self.assertRaisesRegex(
323          TypeError, "inputs must have dtype tf.bfloat16, got tf.float32"):
324        core(inputs, prev_state)
325    def testInitialization(self):
326      num_spatial_dims = 1
327      input_shape = ((self.batch_size,) + (self.input_size,) * num_spatial_dims +
328                     (self.input_channels,))
329      inputs = tf.random.uniform(input_shape)
330      core = recurrent.Conv1DLSTM(
331          input_shape[1:],
332          self.output_channels,
333          kernel_shape=1,
334          forget_bias=0.0,
335          w_i_init=initializers.Ones(),
336          w_h_init=initializers.Ones(),
337          b_init=initializers.Ones())
338      prev_state = core.initial_state(self.batch_size)
339      core(inputs, prev_state)
340      for v in core.variables:
341        self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))
342  class GRUTest(test_utils.TestCase, parameterized.TestCase):
343    def setUp(self):
344      super().setUp()
345      self.batch_size = 3
346      self.input_size = 2
347      self.hidden_size = 16
348    @parameterized.parameters([False, True])
349    def testComputationAgainstNumPy(self, use_tf_function):
350      inputs = self.evaluate(
351          tf.random.uniform([self.batch_size, self.input_size]))
352      core = recurrent.GRU(self.hidden_size)
353      prev_state = self.evaluate(core.initial_state(self.batch_size))
354      core_fn = tf.function(core) if use_tf_function else core
355      outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)
356      w_iz, w_ir, w_ia = np.hsplit(self.evaluate(core.input_to_hidden), 3)
357      w_hz, w_hr, w_ha = np.hsplit(self.evaluate(core.hidden_to_hidden), 3)
358      b_z, b_r, b_a = np.hsplit(self.evaluate(core.b), 3)
359      z = expit(inputs.dot(w_iz) + prev_state.dot(w_hz) + b_z)
360      r = expit(inputs.dot(w_ir) + prev_state.dot(w_hr) + b_r)
361      a = np.tanh(inputs.dot(w_ia) + (r * prev_state).dot(w_ha) + b_a)
362      expected_state = (1 - z) * prev_state + z * a
363      atol = 1e-2 if self.primary_device == "TPU" else 1e-6
364      self.assertAllClose(outputs, next_state, atol=atol)
365      self.assertAllClose(self.evaluate(next_state), expected_state, atol=atol)
366    def testDtypeMismatch(self):
367      core = recurrent.GRU(hidden_size=self.hidden_size, dtype=tf.bfloat16)
368      inputs = tf.random.uniform([self.batch_size, self.input_size])
369      prev_state = core.initial_state(self.batch_size)
370      self.assertIs(prev_state.dtype, tf.bfloat16)
371      with self.assertRaisesRegex(
372          TypeError, "inputs must have dtype tf.bfloat16, got tf.float32"):
373        core(inputs, prev_state)
374    def testInitialization(self):
375      core = recurrent.GRU(
376          hidden_size=self.hidden_size,
377          w_i_init=initializers.Ones(),
378          w_h_init=initializers.Ones(),
379          b_init=initializers.Ones())
380      inputs = tf.random.uniform([self.batch_size, self.input_size])
381      prev_state = core.initial_state(self.batch_size)
382      core(inputs, prev_state)
383      for v in core.variables:
384        self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))
385  def expit(x):
386    return 1.0 / (1 + np.exp(-x))
387  class CuDNNGRUTest(test_utils.TestCase, parameterized.TestCase):
388    def setUp(self):
389      super().setUp()
390      if self.primary_device != "GPU":
391        self.skipTest("Only available on GPU")
392      self.batch_size = 1
393      self.input_size = 1
394      self.hidden_size = 1
395    @parameterized.parameters([1, 4])
396    def testComputationAgainstTF(self, num_steps):
397      inputs = tf.random.uniform([num_steps, self.batch_size, self.input_size])
398      cudnn_gru = recurrent.CuDNNGRU(self.hidden_size)
399      prev_state = cudnn_gru.initial_state(self.batch_size)
400      outputs, states = cudnn_gru(inputs, prev_state)
401      def cudnn_compatible_gru_fn(inputs, prev_state):
402        w_i = cudnn_gru.input_to_hidden
403        w_h = cudnn_gru.hidden_to_hidden
404        w_iz, w_ir, w_ia = tf.split(w_i, num_or_size_splits=3, axis=1)
405        w_hz, w_hr, w_ha = tf.split(w_h, num_or_size_splits=3, axis=1)
406        b_z, b_r, b_a = tf.split(cudnn_gru.b, num_or_size_splits=3)
407        z = tf.sigmoid(
408            tf.matmul(inputs, w_iz) + tf.matmul(prev_state, w_hz) + b_z)
409        r = tf.sigmoid(
410            tf.matmul(inputs, w_ir) + tf.matmul(prev_state, w_hr) + b_r)
411        a = tf.tanh(
412            tf.matmul(inputs, w_ia) + r * tf.matmul(prev_state, w_ha) + b_a)
413        next_state = (1 - z) * a + z * prev_state
414        return next_state, next_state
415      expected_outputs, expected_final_state = recurrent.dynamic_unroll(
416          cudnn_compatible_gru_fn, inputs, prev_state)
417      self.assertAllClose(outputs, expected_outputs)
418      self.assertAllClose(states[-1], expected_final_state)
419    def testDtypeMismatch(self):
420      core = recurrent.CuDNNGRU(hidden_size=self.hidden_size, dtype=tf.bfloat16)
421      inputs = tf.random.uniform([1, self.batch_size, self.input_size])
422      prev_state = core.initial_state(self.batch_size)
423      self.assertIs(prev_state.dtype, tf.bfloat16)
424      with self.assertRaisesRegex(
425          TypeError, "inputs must have dtype tf.bfloat16, got tf.float32"):
426        core(inputs, prev_state)
427    def testInitialization(self):
428      core = recurrent.CuDNNGRU(
429          hidden_size=self.hidden_size,
430          w_i_init=initializers.Ones(),
431          w_h_init=initializers.Ones(),
432          b_init=initializers.Ones())
433      inputs = tf.random.uniform([1, self.batch_size, self.input_size])
434      prev_state = core.initial_state(self.batch_size)
435      core(inputs, prev_state)
436      for v in core.variables:
437        self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))
438  class Counter(recurrent.RNNCore):
439    def __init__(self, hidden_size, name=None):
440      super().__init__(name)
441      self._hidden_size = hidden_size
442      self._built = False
443    def __call__(self, inputs, prev_state):
444      if not self._built:
445        self.one = tf.Variable(1.0)
446        self._built = True
447      t, h = prev_state
448      return inputs * (h + t), (t + self.one, h)
449    def initial_state(self, batch_size):
450      return (tf.cast(0.0, tf.float32), tf.zeros([batch_size, self._hidden_size]))
451  class Replicate(recurrent.RNNCore):
452    def __init__(self, base_core, n, name=None):
453      super().__init__(name)
454      self._base_core = base_core
455      self._n = n
456    def __call__(self, inputs, prev_state):
457      outputs, next_state = self._base_core(inputs, prev_state)
458      return (outputs,) * self._n, next_state
459    def initial_state(self, batch_size, **kwargs):
460      return self._base_core.initial_state(batch_size, **kwargs)
461  class TrainableStateTest(test_utils.TestCase, parameterized.TestCase):
462    @parameterized.parameters([
463        {
464            "initial_values_shape": []
465        },
466        {
467            "initial_values_shape": tf.TensorShape([42])
468        },
469        {
470            "initial_values_shape": (tf.TensorShape([4]), tf.TensorShape([2]))
471        },
472    ])
473    def testUnmasked(self, initial_values_shape):
474      trainable_state = recurrent.TrainableState(
475          tree.map_structure(tf.ones, initial_values_shape))
476      if initial_values_shape:
477        self.assertEqual(
478            len(trainable_state.trainable_variables), len(initial_values_shape))
479      initial_state = trainable_state(batch_size=42)
480      for s, shape in zip(
481          tree.flatten(initial_state), tree.flatten(initial_values_shape)):
482        self.assertEqual(s.shape, tf.TensorShape([42] + shape.as_list()))
483    def testMasked(self):
484      mask = (True, False)
485      trainable_state = recurrent.TrainableState((tf.zeros([16]), tf.zeros([3])),
486                                                 mask)
487      for var in trainable_state.trainable_variables:
488        var.assign_add(tf.ones_like(var))
489      initial_state = trainable_state(batch_size=42)
490      for s, trainable in zip(tree.flatten(initial_state), tree.flatten(mask)):
<span onclick='openModal()' class='match'>491        if trainable:
492          self.assertNotAllClose(s, tf.zeros_like(s))
493        else:
494          self.assertAllClose(s, tf.zeros_like(s))
</span>495    def testForCore(self):
496      core = recurrent.LSTM(hidden_size=16)
497      trainable_state = recurrent.TrainableState.for_core(core)
498      self.assertAllClose(
499          trainable_state(batch_size=42), core.initial_state(batch_size=42))
500  @parameterized.parameters([
501      {
502          "use_tf_function": False,
503          "unroll_fn": recurrent.dynamic_unroll
504      },
505      {
506          "use_tf_function": False,
507          "unroll_fn": recurrent.static_unroll
508      },
509      {
510          "use_tf_function": True,
511          "unroll_fn": recurrent.dynamic_unroll
512      },
513      {
514          "use_tf_function": True,
515          "unroll_fn": recurrent.static_unroll
516      },
517  ])
518  class UnrollTest(test_utils.TestCase, parameterized.TestCase):
519    def setUp(self):
520      super().setUp()
521      self.num_steps = 5
522      self.batch_size = 3
523      self.hidden_size = 2
524      self.core = Counter(self.hidden_size)
525    def testFlat(self, use_tf_function, unroll_fn):
526      if use_tf_function:
527        unroll_fn = tf.function(unroll_fn)
528      initial_state = _, h = self.core.initial_state(self.batch_size)
529      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
530      output_sequence, final_state = unroll_fn(self.core, input_sequence,
531                                               initial_state)
532      self.assertAllClose(
533          output_sequence,
534          [inputs * (h + t) for t, inputs in enumerate(input_sequence)])
535      self.assertAllClose(final_state, (tf.cast(self.num_steps, tf.float32), h))
536    def testNestedInputs(self, use_tf_function, unroll_fn):
537      if use_tf_function:
538        unroll_fn = tf.function(unroll_fn)
539      initial_state = _, h = self.core.initial_state(self.batch_size)
540      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
541      output_sequence, final_state = unroll_fn(
542          lambda inputs, prev_state: self.core(inputs["x"]["y"], prev_state),
543          {"x": {
544              "y": input_sequence
545          }}, initial_state)
546      self.assertAllClose(
547          output_sequence,
548          [inputs * (h + t) for t, inputs in enumerate(input_sequence)])
549      self.assertAllClose(final_state, (tf.cast(self.num_steps, tf.float32), h))
550    def testNestedOutputs(self, use_tf_function, unroll_fn):
551      if use_tf_function:
552        unroll_fn = tf.function(unroll_fn)
553      num_replicas = 2
554      core = Replicate(self.core, num_replicas)
555      initial_state = _, h = core.initial_state(self.batch_size)
556      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
557      output_sequence, final_state = unroll_fn(core, input_sequence,
558                                               initial_state)
559      expected_outputs = [
560          inputs * (h + t) for t, inputs in enumerate(input_sequence)
561      ]
562      self.assertAllClose(output_sequence, (expected_outputs,) * num_replicas)
563      self.assertAllClose(final_state, (tf.cast(self.num_steps, tf.float32), h))
564    def testEmptyOutputs(self, use_tf_function, unroll_fn):
565      if use_tf_function:
566        unroll_fn = tf.function(unroll_fn)
567      def core_fn(inputs, prev_state):
568        return (inputs, tf.zeros(shape=(0,))), prev_state
569      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
570      (_, empty), unused_final_state = unroll_fn(
571          core_fn, input_sequence, initial_state=tf.constant(0.0))
572      self.assertEqual(empty.shape, tf.TensorShape([self.num_steps, 0]))
573    def testZeroSteps(self, use_tf_function, unroll_fn):
574      if use_tf_function:
575        unroll_fn = tf.function(unroll_fn)
576      initial_state = self.core.initial_state(self.batch_size)
577      input_sequence = tf.random.uniform([0, self.batch_size])
578      with self.assertRaisesRegex(ValueError,
579                                  "must have at least a single time step"):
580        unroll_fn(self.core, input_sequence, initial_state)
581    def testInconsistentSteps(self, use_tf_function, unroll_fn):
582      if use_tf_function:
583        unroll_fn = tf.function(unroll_fn)
584      initial_state = self.core.initial_state(self.batch_size)
585      input_sequence = (tf.random.uniform([1, self.batch_size]),
586                        tf.random.uniform([2, self.batch_size]))
587      with self.assertRaisesRegex(ValueError,
588                                  "must have consistent number of time steps"):
589        unroll_fn(self.core, input_sequence, initial_state)
590    def testVariableLengthOneZeroLength(self, use_tf_function, unroll_fn):
591      if use_tf_function:
592        unroll_fn = tf.function(unroll_fn)
593      sequence_length = tf.constant([0] + [self.num_steps] *
594                                    (self.batch_size - 1))
595      initial_state = self.core.initial_state(self.batch_size)
596      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
597      output_sequence, _ = unroll_fn(
598          self.core,
599          input_sequence,
600          initial_state,
601          sequence_length=sequence_length)
602      self.assertConsistentWithLength(output_sequence, sequence_length)
603    def testVariableLengthRange(self, use_tf_function, unroll_fn):
604      if use_tf_function:
605        unroll_fn = tf.function(unroll_fn)
606      sequence_length = tf.range(self.batch_size)
607      initial_state = self.core.initial_state(self.batch_size)
608      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
609      output_sequence, _ = unroll_fn(
610          self.core,
611          input_sequence,
612          initial_state,
613          sequence_length=sequence_length)
614      self.assertConsistentWithLength(output_sequence, sequence_length)
615    def assertConsistentWithLength(self, output_sequence, sequence_length):
616      for t, _ in enumerate(output_sequence):
617        for b in range(self.batch_size):
618          if tf.equal(sequence_length[b], t):
619            if t == 0:
620              self.assertAllEqual(tf.reduce_sum(output_sequence[t, b]), 0.0)
621            else:
622              self.assertAllClose(output_sequence[t, b], output_sequence[t - 1,
623                                                                         b])
624    def testVariableLengthAllFull(self, use_tf_function, unroll_fn):
625      if use_tf_function:
626        unroll_fn = tf.function(unroll_fn)
627      initial_state = self.core.initial_state(self.batch_size)
628      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
629      output_sequence, final_state = unroll_fn(
630          self.core,
631          input_sequence,
632          initial_state,
633          sequence_length=tf.constant([self.num_steps] * self.batch_size))
634      expected_output_sequence, expected_final_state = unroll_fn(
635          self.core, input_sequence, initial_state)
636      self.assertAllClose(output_sequence, expected_output_sequence)
637      self.assertAllClose(final_state, expected_final_state)
638    def testVariableLengthAllEmpty(self, use_tf_function, unroll_fn):
639      if use_tf_function:
640        unroll_fn = tf.function(unroll_fn)
641      initial_state = self.core.initial_state(self.batch_size)
642      input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])
643      output_sequence, final_state = unroll_fn(
644          self.core,
645          input_sequence,
646          initial_state,
647          sequence_length=tf.zeros(self.batch_size, tf.int32))
648      self.assertAllClose(output_sequence, tf.zeros_like(output_sequence))
649      self.assertAllClose(final_state[0], self.num_steps)
650      self.assertAllClose(final_state[1], initial_state[1])
651  class UnknownStepsUnrollTest(test_utils.TestCase):
652    def setUp(self):
653      super().setUp()
654      self.num_steps = 5
655      self.batch_size = 3
656      self.hidden_size = 2
657      self.core = Counter(self.hidden_size)
658    def testStaticUnroll(self):
659      def do_unroll(input_sequence):
660        initial_state = self.core.initial_state(self.batch_size)
661        return recurrent.static_unroll(self.core, input_sequence, initial_state)
662      with self.assertRaisesRegex(
663          ValueError, "must have a statically known number of time steps"):
664        tf.function(do_unroll).get_concrete_function(
665            tf.TensorSpec([None, None, 1]))
666    def testDynamicUnroll(self):
667      def do_unroll(input_sequence):
668        initial_state = self.core.initial_state(self.batch_size)
669        return recurrent.dynamic_unroll(self.core, input_sequence, initial_state)
670      cf = tf.function(do_unroll).get_concrete_function(
671          tf.TensorSpec([None, None, 1]))
672      output_sequence, unused_final_state = cf(
673          tf.random.uniform([self.num_steps, self.batch_size, 1]))
674      self.assertEqual(output_sequence.shape[0], self.num_steps)
675    @unittest.skip("b/141910613")
676    def testDynamicUnrollInconsistentSteps(self):
677      def do_unroll(*input_sequence):
678        return recurrent.dynamic_unroll(lambda inputs, _: inputs, input_sequence,
679                                        ())
680      cf = tf.function(do_unroll).get_concrete_function(
681          tf.TensorSpec([None, None, 1]), tf.TensorSpec([None, None, 1]))
682      with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
683                                  "must have consistent number of time steps"):
684        cf(
685            tf.random.uniform([self.num_steps, self.batch_size, 1]),
686            tf.random.uniform([self.num_steps + 1, self.batch_size, 1]))
687  if __name__ == "__main__":
688    tf.test.main()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from google-api-python-client-MDEwOlJlcG9zaXRvcnkxNTc0OTI2OQ==-flat-discovery.py</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-recurrent_test.py</div>
                <div class="column column_space"><pre><code>673          if methodName.endswith("_media"):
674              docs.append("\nReturns:\n  The media object as a string.\n\n    ")
675          else:
676              docs.append("\nReturns:\n  An object of the form:\n\n    ")
677              docs.append(schema.prettyPrintSchema(methodDesc["response"]))
</pre></code></div>
                <div class="column column_space"><pre><code>491        if trainable:
492          self.assertNotAllClose(s, tf.zeros_like(s))
493        else:
494          self.assertAllClose(s, tf.zeros_like(s))
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    