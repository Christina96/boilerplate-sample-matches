<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for basic_3.py &amp; test_raw_random.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for basic_3.py &amp; test_raw_random.py
      </h3>
<h1 align="center">
        1.2%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>basic_3.py (0.8382449%)<th>test_raw_random.py (2.5733817%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(790-796)<td><a href="#" name="0">(54-65)</a><td align="center"><font color="#ff0000">15</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1290-1298)<td><a href="#" name="1">(312-322)</a><td align="center"><font color="#dd0000">13</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(5674-5677)<td><a href="#" name="2">(215-221)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(5466-5470)<td><a href="#" name="3">(628-630)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(661-668)<td><a href="#" name="4">(260-268)</a><td align="center"><font color="#cc0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>basic_3.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 from six.moves import builtins
3 import sys
4 import warnings
5 import numpy as np
6 from six import integer_types
7 from six.moves import xrange
8 import numbers
9 import theano
10 from theano.compat import izip
11 from theano import config
12 from theano import gof
13 from theano.gof import Apply, Constant, Op, Variable, ParamsType
14 from theano.gof.type import Generic
15 from theano.scalar import int32 as int32_t
16 from theano.tensor import elemwise
17 from theano.tensor.var import (AsTensorError, TensorVariable,
18                                TensorConstant, TensorConstantSignature,
19                                _tensor_py_operators)
20 from theano.tensor.type import TensorType, values_eq_approx_always_true
21 from theano.tensor.type_other import NoneConst
22 from theano import scalar as scal
23 from functools import partial
24 from theano import compile, printing
25 from theano.printing import pprint, min_informative_str
26 from theano.compile import Rebroadcast, Shape, shape
27 from theano.scalar import int32
28 import theano.scalar.sharedvar
29 from theano.gradient import grad_undefined
30 from theano.gradient import grad_not_implemented
31 from theano.gradient import DisconnectedType
32 from theano.tensor.elemwise import Elemwise, DimShuffle, CAReduce, Sum
33 import logging
34 _logger = logging.getLogger("theano.tensor.basic")
35 __docformat__ = "restructuredtext en"
36 python_complex = complex
37 python_any = any
38 python_all = all
39 complex_dtypes = list(map(str, scal.complex_types))
40 continuous_dtypes = list(map(str, scal.continuous_types))
41 float_dtypes = list(map(str, scal.float_types))
42 integer_dtypes = list(map(str, scal.integer_types))
43 discrete_dtypes = list(map(str, scal.discrete_types))
44 all_dtypes = list(map(str, scal.all_types))
45 int_dtypes = list(map(str, scal.int_types))
46 uint_dtypes = list(map(str, scal.uint_types))
47 class ShapeError(Exception):
48     pass
49 def check_equal_numpy(x, y):
50     if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):
51         return (x.dtype == y.dtype and x.shape == y.shape and
52                 np.all(abs(x - y) &lt; 1e-10))
53     elif (isinstance(x, np.random.RandomState) and
54           isinstance(y, np.random.RandomState)):
55         return python_all(np.all(a == b) for a, b in
56                           izip(x.__getstate__(), y.__getstate__()))
57     else:
58         return x == y
59 compile.register_checker(check_equal_numpy)
60 __oplist_constructor_list = []
61     Make `f` appear as a constructor in the oplist (`gen_oplist`,
62     doc/oplist.txt).
63     This function is often used by `make_node` methods of `Op` subclasses
64     to turn ndarrays, numbers, `Scalar` instances, `Apply` instances and
65     `TensorType` instances into valid input list elements.
66     Parameters
67     ----------
68     x : Apply instance, Variable instance, numpy.ndarray, or number
69         This thing will be transformed into a `Variable` in a sensible way. An
70         ndarray argument will not be copied, but a list of numbers will be
71         copied to make an ndarray.
72     name : str or None
73         If a new `Variable` instance is created, it will be named with this
74         string.
75     ndim : None or integer
76         Return a Variable with this many dimensions.
77     Raises
78     ------
79     ValueError
80         If an `Apply` with more than one output is fetched or
81         if `x` cannot be made into a Variable with `ndim` dimensions.
82     AsTensorError
83         If `x` cannot be converted to a TensorType Variable.
84     Raises
85     ------
86     TypeError
87         `x` could not be converted to a numpy.ndarray.
88     ValueError
89         `x` could not be expanded to have ndim dimensions.
90     Note
91     ----
92     We create a small cache of frequently used constant.
93     This speed up the Merge optimization for big graph.
94     We want to cache all scalar to don't merge as frequently constants.
95     But we don't want to cache too much stuff.
96     So we cache integer with dtype [u]int and float where the value is
97     between -10 and 10.
98     We cache all broadcast pattern for scalar.
99     Raised by get_scalar_constant_value if called on something that is
100     not a scalar constant.
101     Raised by get_scalar_const_value if called on something that is a
102     zero dimensional constant.
103     Raises
104     ------
105      NotScalarConstantError
106         If the numpy ndarray is not a scalar.
107     If `v` is the output of dimshuffles, fills, allocs, rebroadcasts,
108     cast, OutputGuard, DeepCopyOp, ScalarFromTensor, ScalarOp, Elemwise
109     and some pattern with Subtensor, this function digs through them.
110     If `v` is not some view of constant scalar data, then raise a
111     NotScalarConstantError.
112     Parameters
113     ----------
114     elemwise : bool
115         If False, we won't try to go into elemwise. So this call is faster.
116         But we still investigate in Second Elemwise (as this is a substitute
117         for Alloc)
118     only_process_constants : bool
119         If True, we only attempt to obtain the value of `orig_v` if it's
120         directly constant and don't try to dig through dimshuffles, fills,
121         allocs, and other to figure out its value.
122     max_recur : int
123         The maximum number of recursion.
124     Notes
125     -----
126         There may be another function similar to this one in the code,
127         but I'm not sure where it is.
128         return [partial(f<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>2, f) for f in fns]
129 cscalar = TensorType('complex64', ())
130 zscalar = TensorType('complex128', ())
131 fscalar = TensorType('float32', ())
132 dscalar = TensorType('float64', ())
133 bscalar = TensorType('int8', ())
134 wscalar =</b></font> TensorType('int16', ())
135 iscalar = TensorType('int32', ())
136 lscalar = TensorType('int64', ())
137 def scalar(name=None, dtype=None):
138     if dtype is None:
139         dtype = config.floatX
140     type = TensorType(dtype, ())
141     return type(name)
142 scalars, fscalars, dscalars, iscalars, lscalars = _multi(
143     scalar, fscalar, dscalar, iscalar, lscalar)
144 int_types = bscalar, wscalar, iscalar, lscalar
145 float_types = fscalar, dscalar
146 complex_types = cscalar, zscalar
147 int_scalar_types = int_types
148 float_scalar_types = float_types
149 complex_scalar_types = complex_types
150 cvector = TensorType('complex64', (False, ))
151 zvector = TensorType('complex128', (False, ))
152 fvector = TensorType('float32', (False, ))
153 dvector = TensorType('float64', (False, ))
154 bvector = TensorType('int8', (False,))
155 wvector = TensorType('int16', (False,))
156 ivector = TensorType('int32', (False, ))
157 lvector = TensorType('int64', (False, ))
158 def vector(name=None, dtype=None):
159     if dtype is None:
160         dtype = config.floatX
161     type = TensorType(dtype, (False, ))
162     return type(name)
163 vectors, fvectors, dvectors, ivectors, lvectors = _multi(
164     vector, fvector, dvector, ivector, lvector)
165 int_vector_types = bvector, wvector, ivector, lvector
166 float_vector_types = fvector, dvector
167 complex_vector_types = cvector, zvector
168 cmatrix = TensorType('complex64', (False, False))
169 zmatrix = TensorType('complex128', (False, False))
170 fmatrix = TensorType('float32', (False, False))
171 dmatrix = TensorType('float64', (False, False))
172 bmatrix = TensorType('int8', (False, False))
173 wmatrix = TensorType('int16', (False, False))
174 imatrix = TensorType('int32', (False, False))
175 lmatrix = TensorType('int64', (False, False))
176 def matrix(name=None, dtype=None):
177     if dtype is None:
178         dtype = config.floatX
179     type = TensorType(dtype, (False, False))
180     return type(name)
181 matrices, fmatrices, dmatrices, imatrices, lmatrices = _multi(
182     matrix, fmatrix, dmatrix, imatrix, lmatrix)
183 int_matrix_types = bmatrix, wmatrix, imatrix, lmatrix
184 float_matrix_types = fmatrix, dmatrix
185 complex_matrix_types = cmatrix, zmatrix
186 crow = TensorType('complex64', (True, False))
187 zrow = TensorType('complex128', (True, False))
188 frow = TensorType('float32', (True, False))
189 drow = TensorType('float64', (True, False))
190 brow = TensorType('int8', (True, False))
191 wrow = TensorType('int16', (True, False))
192 irow = TensorType('int32', (True, False))
193 lrow = TensorType('int64', (True, False))
194 def row(name=None, dtype=None):
195     if dtype is None:
196 <a name="0"></a>        dtype = config.floatX
197     type = TensorType(dtype, (True, False))
198     return type(name)
199 rows, frows, drows, irows, lrows <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= _multi(row, frow, drow, irow, lrow)
200 ccol = TensorType('complex64', (False, True))
201 zcol = TensorType('complex128', (False, True))
202 fcol = TensorType('float32', (False, True))
203 dcol = TensorType('float64', (False, True))
204 bcol =</b></font> TensorType('int8', (False, True))
205 wcol = TensorType('int16', (False, True))
206 icol = TensorType('int32', (False, True))
207 lcol = TensorType('int64', (False, True))
208 def col(name=None, dtype=None):
209     if dtype is None:
210         dtype = config.floatX
211     type = TensorType(dtype, (False, True))
212     return type(name)
213 cols, fcols, dcols, icols, lcols = _multi(col, fcol, dcol, icol, lcol)
214 ctensor3 = TensorType('complex64', ((False,) * 3))
215 ztensor3 = TensorType('complex128', ((False,) * 3))
216 ftensor3 = TensorType('float32', ((False,) * 3))
217 dtensor3 = TensorType('float64', ((False,) * 3))
218 btensor3 = TensorType('int8', ((False,) * 3))
219 wtensor3 = TensorType('int16', ((False,) * 3))
220 itensor3 = TensorType('int32', ((False,) * 3))
221 ltensor3 = TensorType('int64', ((False,) * 3))
222 def tensor3(name=None, dtype=None):
223     if dtype is None:
224         dtype = config.floatX
225     type = TensorType(dtype, (False, False, False))
226     return type(name)
227 tensor3s, ftensor3s, dtensor3s, itensor3s, ltensor3s = _multi(
228     tensor3, ftensor3, dtensor3, itensor3, ltensor3)
229 ctensor4 = TensorType('complex64', ((False,) * 4))
230 ztensor4 = TensorType('complex128', ((False,) * 4))
231 ftensor4 = TensorType('float32', ((False,) * 4))
232 dtensor4 = TensorType('float64', ((False,) * 4))
233 btensor4 = TensorType('int8', ((False,) * 4))
234 wtensor4 = TensorType('int16', ((False,) * 4))
235 itensor4 = TensorType('int32', ((False,) * 4))
236 ltensor4 = TensorType('int64', ((False,) * 4))
237 def tensor4(name=None, dtype=None):
238     if dtype is None:
239         dtype = config.floatX
240     type = TensorType(dtype, (False, False, False, False))
241     return type(name)
242 tensor4s, ftensor4s, dtensor4s, itensor4s, ltensor4s = _multi(
243     tensor4, ftensor4, dtensor4, itensor4, ltensor4)
244 ctensor5 = TensorType('complex64', ((False,) * 5))
245 ztensor5 = TensorType('complex128', ((False,) * 5))
246 ftensor5 = TensorType('float32', ((False,) * 5))
247 dtensor5 = TensorType('float64', ((False,) * 5))
248 btensor5 = TensorType('int8', ((False,) * 5))
249 wtensor5 = TensorType('int16', ((False,) * 5))
250 itensor5 = TensorType('int32', ((False,) * 5))
251 ltensor5 = TensorType('int64', ((False,) * 5))
252 def tensor5(name=None, dtype=None):
253     if dtype is None:
254         dtype = config.floatX
255     type = TensorType(dtype, (False, False, False, False, False))
256     return type(name)
257 tensor5s, ftensor5s, dtensor5s, itensor5s, ltensor5s = _multi(
258     tensor5, ftensor5, dtensor5, itensor5, ltensor5)
259 ctensor6 = TensorType('complex64', ((False,) * 6))
260 ztensor6 = TensorType('complex128', ((False,) * 6))
261 ftensor6 = TensorType('float32', ((False,) * 6))
262 dtensor6 = TensorType('float64', ((False,) * 6))
263 btensor6 = TensorType('int8', ((False,) * 6))
264 wtensor6 = TensorType('int16', ((False,) * 6))
265 itensor6 = TensorType('int32', ((False,) * 6))
266 ltensor6 = TensorType('int64', ((False,) * 6))
267 def tensor6(name=None, dtype=None):
268     if dtype is None:
269         dtype = config.floatX
270     type = TensorType(dtype, (False,) * 6)
271     return type(name)
272 tensor6s, ftensor6s, dtensor6s, itensor6s, ltensor6s = _multi(
273     tensor6, ftensor6, dtensor6, itensor6, ltensor6)
274 ctensor7 = TensorType('complex64', ((False,) * 7))
275 ztensor7 = TensorType('complex128', ((False,) * 7))
276 ftensor7 = TensorType('float32', ((False,) * 7))
277 dtensor7 = TensorType('float64', ((False,) * 7))
278 btensor7 = TensorType('int8', ((False,) * 7))
279 wtensor7 = TensorType('int16', ((False,) * 7))
280 itensor7 = TensorType('int32', ((False,) * 7))
281 ltensor7 = TensorType('int64', ((False,) * 7))
282 def tensor7(name=None, dtype=None):
283     if dtype is None:
284         dtype = config.floatX
285     type = TensorType(dtype, (False,) * 7)
286     return type(name)
287 tensor7s, ftensor7s, dtensor7s, itensor7s, ltensor7s = _multi(
288     tensor7, ftensor7, dtensor7, itensor7, ltensor7)
289 Tensor = TensorType
290 elemwise.as_tensor_variable = as_tensor_variable
291 elemwise.TensorType = TensorType
292 elemwise.TensorVariable = TensorVariable
293 elemwise.TensorConstant = TensorConstant
294 def _scal_elemwise_with_nfunc(nfunc, nin, nout):
295     def construct(symbol):
296         symbolname = symbol.__name__
297         inplace = symbolname.endswith('_inplace')
298         if inplace:
299             msg = "inplace"
300         else:
301             msg = "no_inplace"
302         n = "Elemwise{%s,%s}" % (symbolname, msg)
303         if inplace:
304             scalar_op = getattr(scal, symbolname[:-len('_inplace')])
305             inplace_scalar_op = scalar_op.__class__(scal.transfer_type(0))
306             rval = elemwise.Elemwise(inplace_scalar_op, {0: 0}, name=n,
307                                      nfunc_spec=(nfunc and (nfunc, nin, nout)))
308         else:
309             scalar_op = getattr(scal, symbolname)
310             rval = elemwise.Elemwise(scalar_op, name=n,
311                                      nfunc_spec=(nfunc and (nfunc, nin, nout)))
312         if getattr(symbol, '__doc__', False):
313             rval.__doc__ = symbol.__doc__ + '\n' + rval.__doc__
314         rval.__epydoc_asRoutine = symbol
315         rval.__module__ = 'tensor'
316         pprint.assign(rval, printing.FunctionPrinter(symbolname))
317         return rval
318     return construct
319 _scal_elemwise = _scal_elemwise_with_nfunc(None, None, None)
320 def _pack(x):
321     try:
322         return list(x)
323     except TypeError:
324         return [x]
325 def check_and_normalize_axes(x, axis):
326     x = as_tensor_variable(x)
327     if axis is None:
328         axis = []
329     elif (isinstance(axis, (integer_types, np.integer)) or
330             (isinstance(axis, np.ndarray) and axis.ndim == 0)):
331                 axis = [int(axis)]
332     elif isinstance(axis, (tuple, list, np.ndarray)):
333         axis = [int(i) for i in axis]
334     elif isinstance(axis, Variable):
335         if NoneConst.equals(axis):
336             axis = []
337         elif not isinstance(axis, TensorConstant):
338             raise TypeError("Computation needs a constant axis. Got %s" % axis)
339         else:
340             assert axis.dtype in integer_dtypes
341             if (isinstance(axis.data, (integer_types, np.integer)) or
342                     (isinstance(axis.data, np.ndarray) and axis.data.ndim == 0)):
343                         axis = [int(axis.data)]
344             elif isinstance(axis.data, (list, np.ndarray)):
345                 axis = [int(i) for i in axis.data]
346     else:
347         raise TypeError("Axis must be an integer, tuple, list of integers or a TensorVariable. Got %s" % axis)
348     if len(axis) &gt; 0:
349         for i in range(len(axis)):
350             if axis[i] &lt; 0:
351                 axis[i] += x.type.ndim
352             if axis[i] &lt; 0 or axis[i] &gt;= x.type.ndim:
353                 raise ValueError("Computation needs a valid axis number for %d-D tensor. Got %d" % (x.type.ndim, axis[i]))
354         axis = list(set(axis))
355         axis.sort()
356     return axis
357 class TensorFromScalar(Op):
358     __props__ = ()
359     def make_node(self, s):
360         assert isinstance(s.type, scal.Scalar)
361         return Apply(self,
362                      [s],
363                      [tensor(dtype=s.type.dtype,
364                              broadcastable=())])
365     def perform(self, node, inp, out_):
366         s, = inp
367         out, = out_
368         out[0] = np.asarray(s)
369     def infer_shape(self, node, in_shapes):
370         return [()]
371     def grad(self, inp, grads):
372         s, = inp
373         dt, = grads
374         if s.type.dtype in float_dtypes:
375             assert dt.type.dtype in float_dtypes
376             return [scalar_from_tensor(dt)]
377         if s.type.dtype in discrete_dtypes:
378             return [s.zeros_like().astype(theano.config.floatX)]
379         raise NotImplementedError("grad not implemented for complex dtypes")
380 tensor_from_scalar = TensorFromScalar()
381 class ScalarFromTensor(Op):
382     __props__ = ()
383     def make_node(self, t):
384         assert isinstance(t.type, TensorType)
385         assert t.type.broadcastable == ()
386         return Apply(self,
387                      [t],
388                      [scal.get_scalar_type(dtype=t.type.dtype).make_variable()]
389                      )
390     def perform(self, node, inp, out_):
391         s, = inp
392         out, = out_
393         out[0] = s.flatten()[0]
394     def infer_shape(self, node, in_shapes):
395         return [()]
396     def grad(self, inp, grads):
397         s, = inp
398         dt, = grads
399         return [tensor_from_scalar(dt)]
400     def R_op(self, inputs, eval_points):
401         if None in eval_points:
402             return [None]
403         return self.make_node(*eval_points).outputs
404     def c_code(self, node, name, inputs, outputs, sub):
405         x, = inputs
406         z, = outputs
407         fail = sub['fail']
408         return """
409         %(z)s = ((dtype_%(x)s*)(PyArray_DATA(%(x)s)))[0];
410     Calculate the max and argmax over a given axis or over all axes.
411         return self<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.axis
412     def make_node(self, x):
413         x = _as_tensor_variable(x)
414         all_axes = set(self.axis)
415         broadcastable = [b for i, b in enumerate(x.type.</b></font>broadcastable)
416                          if i not in all_axes]
417         inputs = [x]
418         outputs = [tensor(x.type.dtype, broadcastable, name='max'),
419                    tensor('int64', broadcastable, name='argmax')]
420         return Apply(self, inputs, outputs)
421     def perform(self, node, inp, outs, params):
422         x = inp[0]
423         axes = params
424         max, max_idx = outs
425         if axes is None:
426             axes = tuple(range(x.ndim))
427         else:
428             axes = tuple(int(ax) for ax in axes)
429         max[0] = theano._asarray(np.max(x, axes),
430                                  dtype=node.outputs[0].dtype)
431         keep_axes = np.array([i for i in range(x.ndim) if i not in axes],
432                              dtype='int64')
433         transposed_x = np.transpose(x, np.concatenate((keep_axes, axes)))
434         kept_shape = transposed_x.shape[:len(keep_axes)]
435         reduced_shape = transposed_x.shape[len(keep_axes):]
436         new_shape = kept_shape + (np.prod(reduced_shape, dtype='int64'),)
437         reshaped_x = transposed_x.reshape(new_shape)
438         max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),
439                                      dtype='int64')
440     def c_code(self, node, name, inp, out, sub):
441         if len(self.axis) != 1 and len(self.axis) != node.inputs[0].ndim:
442             raise NotImplementedError("NumPy C-API can compute max and argmax only for 1 axis or for all axes.")
443         x = inp[0]
444         axis = sub['params']
445         max, argmax = out
446         fail = sub["fail"]
447         ret = """
448         int axis;
449         if (PyTuple_GET_SIZE(%(axis)s) == PyArray_NDIM(%(x)s)) {
450             axis = NPY_MAXDIMS;
451         } else if(PyTuple_GET_SIZE(%(axis)s) == 1) {
452             PyObject* axis_object = PyTuple_GET_ITEM(%(axis)s, 0);
453             axis = (int)PyInt_AS_LONG(axis_object);
454             if (axis &gt; PyArray_NDIM(%(x)s)-1 || axis &lt; -PyArray_NDIM(%(x)s)) {
455                 PyErr_SetString(PyExc_ValueError,
456                 "MaxAndArgmax: bad axis argument");
457                 %(fail)s
458             }
459         } else {
460             PyErr_SetString(PyExc_NotImplementedError,
461             "MaxAndArgmax: NumPy C-API can compute max and argmax only for 1 axis or for all axes.");
462             %(fail)s
463         }
464         Py_CLEAR(%(max)s);
465         Py_CLEAR(%(argmax)s);//todo pass them as out parameter.
466         %(max)s = (PyArrayObject*)PyArray_Max(%(x)s, axis, NULL);
467         if (%(max)s == NULL) {
468             %(fail)s;
469         }
470         if (!PyArray_CheckExact(%(max)s)) {
471             %(max)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(max)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);
472             if(%(max)s == NULL){
473                 %(fail)s;
474             }
475         }
476         %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);
477         if (%(argmax)s == NULL) {
478             Py_CLEAR(%(max)s);
479             %(fail)s;
480         }
481         if (!PyArray_CheckExact(%(argmax)s)) {
482             %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);
483             if(%(argmax)s == NULL){
484                 %(fail)s;
485             }
486         }
487         if (PyArray_TYPE(%(argmax)s) != NPY_INT64) {
488             PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);
489             if (NULL == tmp){
490                 %(fail)s;
491             }
492             Py_DECREF(%(argmax)s);
493             %(argmax)s = (PyArrayObject*)tmp;
494         }
495     Calculate the argmax over a given axis or over all axes.
496         ret = """
497         int axis;
498         Py_CLEAR(%(argmax)s);//todo pass them as out parameter.
499         %(axis_code)s
500         %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);
501         if(%(argmax)s == NULL){
502             %(fail)s;
503         }
504         if(!PyArray_CheckExact(%(argmax)s)){
505             %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);
506             if(%(argmax)s == NULL){
507                 %(fail)s;
508             }
509         }
510         if(PyArray_TYPE(%(argmax)s) != NPY_INT64){
511             PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);
512             if (NULL == tmp){
513                 %(fail)s;
514             }
515             Py_DECREF(%(argmax)s);
516             %(argmax)s = (PyArrayObject*)tmp;
517         }
518     Reintroduces in y with length one the axes of x which have been left out
519     in a prior reduction of x. With this option, the resulting tensor will
520     broadcast correctly against the original tensor x.
521     Returns maximum elements and their indices obtained by iterating over
522     given axis.
523     When axis is None (the default value), the max is performed
524     over the flattened tensor.
525     Parameters
526     ----------
527     keepdims : bool
528         If this is set to True, the axes which are reduced are left in
529         the result as dimensions with size one. With this option, the result
530         will broadcast correctly against the original tensor.
531     Returns maximum elements obtained by iterating over given axis.
532     When axis is None (the default value), the max is performed
533     over the flattened tensor.
534     Parameters
535     ----------
536     keepdims: bool
537         If this is set to True, the axes which are reduced are left in
538         the result as dimensions with size one. With this option, the result
539         will broadcast correctly against the original tensor.
540     Notes
541     -----
542     We return an error as numpy when we reduce a dim with a shape of 0.
543     Returns indices of maximum elements obtained by iterating over given axis.
544     When axis is None (the default value), the argmax is performed
545     over the flattened tensor.
546     Parameters
547     ----------
548     keepdims : bool
549         If this is set to True, the axes which are reduced are left in
550         the result as dimensions with size one. With this option, the result
551         will broadcast correctly against the original tensor.
552     Returns minimum elements obtained by iterating over given axis.
553     When axis is None (the default value), the min is performed
554     over the flattened tensor.
555     Parameters
556     ----------
557     keepdims: bool
558         If this is set to True, the axes which are reduced are left in
559         the result as dimensions with size one. With this option, the result
560         will broadcast correctly against the original tensor.
561     Returns indices of minimum elements obtained by iterating over given axis.
562     When axis is None (the default value), the argmin is performed
563     over the flattened tensor.
564     Parameters
565     ----------
566     keepdims: bool
567         If this is set to True, the axes which are reduced are left in
568         the result as dimensions with size one. With this option, the result
569         will broadcast correctly against the original tensor.
570     Return the [elementwise] smallest of a variable number of arguments.
571     Like python's min.
572     Return the [elementwise] largest of a variable number of arguments.
573     Like python's max.
574     Implement Numpy's ``allclose`` on tensors.
575     ``absolute(a - b) &lt;= (atol + rtol * absolute(b))``
576     Parameters
577     ----------
578     a : tensor
579         Input to compare.
580     b : tensor
581         Input to compare.
582     rtol : float
583         The relative tolerance parameter.
584     atol : float
585         The absolute tolerance parameter.
586     equal_nan: bool
587         Whether to consider nan's in the same place to be close.
588     Returns
589     -------
590     bool
591         A boolean value (of type int8 returned by the tensor elementwise `all`
592         function) whether all elements in a and b are in the tolerance range
593         defined above.
594     Notes
595     -----
596     Not a symmetric equation. See Numpy's documentation.
597     Implements Numpy's ``isclose`` on tensors.
598     The tolerance values are positive, typically very small numbers. The
599     relative difference (`rtol` * abs(`b`)) and the absolute difference
600     `atol` are added together to compare against the absolute difference
601     between `a` and `b`.
602     ``absolute(a - b) &lt;= (atol + rtol * absolute(b))``
603     Parameters
604     ----------
605     a : tensor
606         Input to compare.
607     b : tensor
608         Input to compare.
609     rtol : float
610         The relative tolerance parameter.
611     atol : float
612         The absolute tolerance parameter.
613     equal_nan : bool
614         Whether to consider nan's in the same place to be close
615     Returns
616     -------
617     int8
618         A boolean (int8) array where two arrays are element-wise equal
619         within a tolerance.
620     Notes
621     -----
622     Not a symmetric equation. See Numpy's documentation.
623     Examples
624     --------
625     &gt;&gt;&gt; import theano
626     &gt;&gt;&gt; import numpy as np
627     &gt;&gt;&gt; a = theano._asarray([1e10, 1e-7], dtype="float64")
628     &gt;&gt;&gt; b = theano._asarray([1.00001e10, 1e-8], dtype="float64")
629     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
630     array([1, 0], dtype=int8)
631     &gt;&gt;&gt; a = theano._asarray([1e10, 1e-8], dtype="float64")
632     &gt;&gt;&gt; b = theano._asarray([1.00001e10, 1e-9], dtype="float64")
633     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
634     array([1, 1], dtype=int8)
635     &gt;&gt;&gt; a = theano._asarray([1e10, 1e-8], dtype="float64")
636     &gt;&gt;&gt; b = theano._asarray([1.0001e10, 1e-9], dtype="float64")
637     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
638     array([0, 1], dtype=int8)
639     &gt;&gt;&gt; a = theano._asarray([1.0, np.nan], dtype="float64")
640     &gt;&gt;&gt; b = theano._asarray([1.0, np.nan], dtype="float64")
641     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
642     array([1, 0], dtype==int8)
643     &gt;&gt;&gt; a = theano._asarray([1.0, np.nan], dtype="float64")
644     &gt;&gt;&gt; b = theano._asarray([1.0, np.nan], dtype="float64")
645     &gt;&gt;&gt; theano.tensor.isclose(a, b, equal_nan=True).eval()
646     array([1, 1], dtype==int8)
647     &gt;&gt;&gt; a = theano._asarray([1.0, np.inf], dtype="float64")
648     &gt;&gt;&gt; b = theano._asarray([1.0, -np.inf], dtype="float64")
649     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
650     array([1, 0], dtype==int8)
651     &gt;&gt;&gt; a = theano._asarray([1.0, np.inf], dtype="float64")
652     &gt;&gt;&gt; b = theano._asarray([1.0, np.inf], dtype="float64")
653     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
654     array([1, 1], dtype==int8)
655     TensorVariable overloads the `TensorVariable.__abs__` operator so that
656     this function is called when you type abs(a).
657     Default to half_to_even."""
658     if mode is None:
659         mode = "half_to_even"
660         if config.warn.round:
661             warnings.warn(
662                 "theano.tensor.round() changed its default from"
663                 " `half_away_from_zero` to `half_to_even` to have"
664                 " the same default as NumPy. Use the Theano flag"
665                 " `warn.round=False` to disable this warning.")
666     if mode == "half_away_from_zero":
667         return round_half_away_from_zero(a)
668     elif mode == "half_to_even":
669         return round_half_to_even(a)
670     else:
671         raise Exception("round mode %s is not implemented." % mode)
672 @_scal_elemwise
673 def round_half_to_even(a):
674 @_scal_elemwise
675 def round_half_away_from_zero(a):
676 @_scal_elemwise
677 def sqr(a):
678 square = sqr
679 def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None):
680     if fweights is not None:
681         raise NotImplementedError('fweights are not implemented')
682     if aweights is not None:
683         raise NotImplementedError('aweights are not implemented')
684     if not rowvar and m.shape[0] != 1:
685         m = m.T
686     if y is not None:
687         if not rowvar and y.shape[0] != 1:
688             y = y.T
689         m = theano.tensor.concatenate((m, y), axis=0)
690     if ddof is None:
691         if not bias:
692             ddof = 1
693         else:
694             ddof = 0
695     fact = m.shape[1] - ddof
696     m -= m.mean(axis=1, keepdims=1)
697     c = m.dot(m.T)
698     c *= theano.tensor.constant(1) / fact
699     return c.squeeze()
700 @_scal_elemwise
701 def sqrt(a):
702 @_scal_elemwise
703 def deg2rad(a):
704 @_scal_elemwise
705 def rad2deg(a):
706 @_scal_elemwise
707 def cos(a):
708 @_scal_elemwise
709 def arccos(a):
710 @_scal_elemwise
711 def sin(a):
712 @_scal_elemwise
713 def arcsin(a):
714 @_scal_elemwise
715 def tan(a):
716 @_scal_elemwise
717 def arctan(a):
718 @_scal_elemwise
719 def arctan2(a, b):
720 @_scal_elemwise
721 def cosh(a):
722 @_scal_elemwise
723 def arccosh(a):
724 @_scal_elemwise
725 def sinh(a):
726 @_scal_elemwise
727 def arcsinh(a):
728 @_scal_elemwise
729 def tanh(a):
730 @_scal_elemwise
731 def arctanh(a):
732 @_scal_elemwise
733 def erf(a):
734 @_scal_elemwise
735 def erfc(a):
736 @_scal_elemwise
737 def erfcx(a):
738 @_scal_elemwise
739 def erfinv(a):
740 @_scal_elemwise
741 def erfcinv(a):
742 @_scal_elemwise
743 def gamma(a):
744 @_scal_elemwise
745 def gammaln(a):
746 @_scal_elemwise
747 def psi(a):
748 @_scal_elemwise
749 def tri_gamma(a):
750 @_scal_elemwise
751 def chi2sf(x, k):
752 @_scal_elemwise
753 def gammainc(k, x):
754 @_scal_elemwise
755 def gammaincc(k, x):
756 @_scal_elemwise
757 def gammau(k, x):
758 @_scal_elemwise
759 def gammal(k, x):
760 @_scal_elemwise
761 def j0(x):
762 @_scal_elemwise
763 def j1(x):
764 @_scal_elemwise
765 def jv(v, x):
766 @_scal_elemwise
767 def i0(x):
768 @_scal_elemwise
769 def i1(x):
770 @_scal_elemwise
771 def iv(v, x):
772 @_scal_elemwise
773 def real(z):
774 _tensor_py_operators.real = property(real)
775 @_scal_elemwise
776 def imag(z):
777 _tensor_py_operators.imag = property(imag)
778 @_scal_elemwise
779 def angle(z):
780 @_scal_elemwise  # numpy.complex cannot build tensors
781 def complex(real, imag):
782 @_scal_elemwise
783 def conj(z):
784 @_scal_elemwise
785 def complex_from_polar(abs, angle):
786 @_scal_elemwise
787 def second(a, b):
788 fill = second
789 pprint.assign(fill, printing.FunctionPrinter('fill'))
790 @constructor
791 def ones_like(model, dtype=None, opt=False):
792     if dtype is None:
793         dtype = model.type.dtype
794     ret = constant(1.0, dtype=dtype)
795     if opt and ret.type == model.type:
796         return ret
797     return fill(model, ret)
798 @constructor
799 def zeros_like(model, dtype=None, opt=False):
800     if dtype is None:
801         dtype = model.type.dtype
802     ret = constant(0.0, dtype=dtype)
803     if opt and ret.type == model.type:
804         return ret
805     return fill(model, ret)
806 def zeros(shape, dtype=None):
807     if not isinstance(shape, (list, tuple, TensorVariable)):
808         shape = [shape]
809     if dtype is None:
810         dtype = config.floatX
811     return alloc(np.array(0, dtype=dtype), *shape)
812 def ones(shape, dtype=None):
813     if not isinstance(shape, (list, tuple, TensorVariable)):
814         shape = [shape]
815     if dtype is None:
816         dtype = config.floatX
817     return alloc(np.array(1, dtype=dtype), *shape)
818 class Nonzero(gof.Op):
819     __props__ = ()
820     def make_node(self, a):
821         a = as_tensor_variable(a)
822         if a.ndim == 0:
823             raise ValueError('Nonzero only supports non-scalar arrays.')
824         output = [TensorType(dtype='int64', broadcastable=(False, False))()]
825         return gof.Apply(self, [a], output)
826     def perform(self, node, inp, out_):
827         a = inp[0]
828         out, = out_
829         result_tuple = np.nonzero(a)
830         if len(result_tuple[0]) &gt; 0:
831             result = np.vstack(result_tuple)
832         else:
833             result = np.zeros((len(result_tuple), 0))
834         out[0] = result.astype('int64')
835     def grad(self, inp, grads):
836         return [grad_undefined(self, 0, inp[0])]
837 _nonzero = Nonzero()
838 def nonzero(a, return_matrix=False):
839     matrix_result = _nonzero(a)
840     if return_matrix:
841         return matrix_result
842     else:
843         if a.ndim &gt; 0:
844             tuple_result = tuple([matrix_result[i] for i in xrange(a.ndim)])
845         else:
846             tuple_result = tuple([matrix_result[0]])
847         return tuple_result
848 def flatnonzero(a):
849     if a.ndim == 0:
850         raise ValueError('Nonzero only supports non-scalar arrays.')
851     return nonzero(a.flatten(), return_matrix=True)[0]
852 def nonzero_values(a):
853     return a.flatten()[flatnonzero(a)]
854 class Tri(gof.Op):
855     __props__ = ("dtype",)
856     def __init__(self, dtype=None):
857         if dtype is None:
858             dtype = config.floatX
859         self.dtype = dtype
860     def make_node(self, N, M, k):
861         N = as_tensor_variable(N)
862         M = as_tensor_variable(M)
863         k = as_tensor_variable(k)
864         return gof.Apply(
865             self,
866             [N, M, k],
867             [TensorType(dtype=self.dtype, broadcastable=(False, False))()])
868     def perform(self, node, inp, out_):
869         N, M, k = inp
870         out, = out_
871         out[0] = np.tri(N, M, k, dtype=self.dtype)
872     def infer_shape(self, node, in_shapes):
873         out_shape = [node.inputs[0], node.inputs[1]]
874         return [out_shape]
875     def grad(self, inp, grads):
876         return [grad_undefined(self, i, inp[i]) for i in xrange(3)]
877 def tri(N, M=None, k=0, dtype=None):
878     if dtype is None:
879         dtype = config.floatX
880     if M is None:
881         M = N
882     op = Tri(dtype)
883     return op(N, M, k)
884 def tril(m, k=0):
885     return m * tri(m.shape[0], m.shape[1], k=k, dtype=m.dtype)
886 def triu(m, k=0):
887     return m * (1 - tri(m.shape[0], m.shape[1], k=k - 1, dtype=m.dtype))
888 class Eye(gof.Op):
889     __props__ = ("dtype", )
890     def __init__(self, dtype=None):
891         if dtype is None:
892             dtype = config.floatX
893         self.dtype = dtype
894     def make_node(self, n, m, k):
895         n = as_tensor_variable(n)
896         m = as_tensor_variable(m)
897         k = as_tensor_variable(k)
898         assert n.ndim == 0
899         assert m.ndim == 0
900         assert k.ndim == 0
901         return gof.Apply(
902             self,
903             [n, m, k],
904             [TensorType(dtype=self.dtype, broadcastable=(False, False))()])
905     def perform(self, node, inp, out_):
906         n, m, k = inp
907         out, = out_
908         out[0] = np.eye(n, m, k, dtype=self.dtype)
909     def infer_shape(self, node, in_shapes):
910         out_shape = [node.inputs[0], node.inputs[1]]
911         return [out_shape]
912     def grad(self, inp, grads):
913         return [grad_undefined(self, i, inp[i]) for i in xrange(3)]
914 def eye(n, m=None, k=0, dtype=None):
915     if dtype is None:
916         dtype = config.floatX
917     if m is None:
918         m = n
919     localop = Eye(dtype)
920     return localop(n, m, k)
921 def identity_like(x):
922     return eye(x.shape[0], x.shape[1], k=0, dtype=x.dtype)
923 def alloc_validate_shape(shape):
924     sh = [as_tensor_variable(s) for s in shape]
925     bcast = []
926     for i, s in enumerate(sh):
927         def err_str():
928             if config.exception_verbosity == 'high':
929                 return '\n' + min_informative_str(s)
930             else:
931                 return str(s)
932         if s.type.dtype not in integer_dtypes:
933             s_as_str = err_str()
934             raise TypeError('Shape arguments to Alloc must be integers, '
935                             'but argument %s is not for apply node: %s' %
936                             (i, s_as_str))
937         if s.ndim != 0:
938             s_as_str = err_str()
939             raise TypeError(
940                 "Each shape dimension to Alloc must be a scalar, ",
941                 'but dimension %s have %d dimensions for apply node: %s' %
942                 (i, s.ndim, s_as_str))
943         try:
944             const_shp = get_scalar_constant_value(s)
945         except NotScalarConstantError:
946             const_shp = None
947         bcast.append(1 == const_shp)
948     return sh, bcast
949 class Alloc(gof.Op):
950     _f16_ok = True
951     __props__ = ()
952     def validate_shape(self, shape):
953         return alloc_validate_shape(shape)
954     def make_node(self, value, *shape):
955         v = as_tensor_variable(value)
956         sh, bcast = alloc_validate_shape(shape)
957         if v.ndim &gt; len(sh):
958             raise TypeError("The Alloc value to use has more dimensions"
959                             " than the specified dimensions",
960                             v.ndim, len(sh))
961         otype = TensorType(dtype=v.dtype, broadcastable=bcast)
962         return gof.Apply(self, [v] + sh, [otype()])
963     def perform(self, node, inputs, out_):
964         out, = out_
965         v = inputs[0]
966         sh = tuple([int(i) for i in inputs[1:]])
967         if out[0] is None or out[0].shape != sh:
968             if v.size == 1 and v.item() == 0:
969                 out[0] = np.zeros(sh, dtype=v.dtype)
970             else:
971                 out[0] = np.empty(sh, dtype=v.dtype)
972                 out[0][...] = v  # broadcast v to fill us up
973         else:
974             out[0][...] = v  # broadcast v to fill us up
975     def c_code(self, node, name, inp, out, sub):
976         vv = inp[0]
977         ndim = len(inp[1:])
978         zz, = out
979         fail = sub['fail']
980         code = """
981             npy_intp shape[%(ndim)s];
982         code += """
983             int need_new_out = (NULL == %(zz)s);
984             for (int i = 0; i &lt; %(ndim)s; i++)
985                 need_new_out = (need_new_out
986                                 || (PyArray_DIMS(%(zz)s)[i] != shape[i]));
987             if (need_new_out)
988             {
989                 Py_XDECREF(%(zz)s);
990                 %(zz)s = (PyArrayObject*) PyArray_SimpleNew(%(ndim)s,
991                     shape, PyArray_TYPE((PyArrayObject*) py_%(vv)s));
992                 if (!%(zz)s)
993                 {
994                     PyErr_SetString(PyExc_MemoryError, "alloc failed");
995                     %(fail)s
996                 }
997             }
998             // This function takes care of broadcasting
999             if (PyArray_CopyInto(%(zz)s, %(vv)s) == -1)
1000               %(fail)s
1001         If the alloc would be useless, this function returns val.
1002         If this function is called outside of a graph optimization context
1003         (for instance, it is manually called by a user building a graph),
1004         then we always return an Alloc node, to allow for DebugMode to check
1005         for size mismatches.
1006         If you always want an Alloc node, call make_node.
1007     Return a version of `var` transferred to `target`.
1008     `cpu` mean a TensorType (on the CPU).  Other types may define
1009     additional targets.
1010     Parameters
1011     ----------
1012     var : variable
1013         A theano variable
1014     target : str
1015         The target of the transfer
1016     Register a transfer function for alternative targets.
1017     Parameters
1018     ----------
1019     fn : callable
1020     Computes the sum along the given axis(es) of a tensor `input`.
1021     When axis is None (the default value), the sum is performed
1022     over the flattened tensor.
1023     For full documentation see ``tensor.elemwise.Sum``.
1024     In particular please pay attention to the important warning when using
1025     a custom acc_dtype.
1026     Parameters
1027     ----------
1028     keepdims: bool
1029         If this is set to True, the axes which are reduced are left in
1030         the result as dimensions with size one. With this option, the result
1031         will broadcast correctly against the original tensor.
1032     Computes the product along the given axis(es) of a tensor `input`.
1033     When axis is None (the default value), the product is performed
1034     over the flattened tensor.
1035     For full documentation see ``tensor.elemwise.Prod``.
1036     Parameters
1037     ----------
1038     keepdims: bool
1039         If this is set to True, the axes which are reduced are left in
1040         the result as dimensions with size one. With this option, the result
1041         will broadcast correctly against the original tensor.
1042 @constructor
1043 def mean(input, axis=None, dtype=None, op=False, keepdims=False,
1044          acc_dtype=None):
1045     input = as_tensor_variable(input)
1046     if op:
1047         if dtype not in (None, 'float64'):
1048             raise NotImplementedError(
1049                 'The Mean op does not support the dtype argument, '
1050                 'and will always use float64. If you want to specify '
1051                 'the dtype, call tensor.mean(..., op=False).',
1052                 dtype)
1053         if acc_dtype not in (None, 'float64'):
1054             raise NotImplementedError(
1055                 'The Mean op does not support the acc_dtype argument, '
1056                 'and will always use float64. If you want to specify '
1057                 'acc_dtype, call tensor.mean(..., op=False).',
1058                 dtype)
1059         out = Mean(axis)(input)
1060         if keepdims:
1061             out = makeKeepDims(input, out, axis)
1062         return out
1063     if dtype is not None:
1064         sum_dtype = dtype
1065     else:
1066         sum_dtype = None
1067         if input.dtype == 'float16':
1068             sum_dtype = 'float32'
1069     s = sum(input, axis=axis, dtype=sum_dtype, keepdims=keepdims,
1070             acc_dtype=acc_dtype)
1071     shp = shape(input)
1072     if s.dtype in ('float16', 'float32', 'complex64'):
1073         shp = cast(shp, 'float32')
1074     else:
1075         shp = cast(shp, 'float64')
1076     if axis is None:
1077         axis = list(range(input.ndim))
1078     elif isinstance(axis, (integer_types, np.integer)):
1079         axis = [axis]
1080     elif isinstance(axis, np.ndarray) and axis.ndim == 0:
1081         axis = [int(axis)]
1082     else:
1083         axis = [int(a) for a in axis]
1084     for i in axis:
1085         s = true_div(s, shp[i])
1086     if s.dtype != shp.dtype and s.dtype in discrete_dtypes:
1087         s = cast(s, shp.dtype)
1088     if dtype == 'float16' or (dtype is None and input.dtype == 'float16'):
1089         s = cast(s, 'float16')
1090     s.name = 'mean'
1091     return s
1092 @constructor
1093 def var(input, axis=None, ddof=0, keepdims=False, corrected=False):
1094     if isinstance(ddof, (bool)):
1095         raise ValueError('Parameter keepdims is now at index 3: (input, \
1096                           axis=None, ddof=0, keepdims=False, corrected=False)')
1097     input_ndim = input.type.ndim
1098     if axis is None:
1099         axis = list(range(input_ndim))
1100     elif isinstance(axis, (integer_types, np.integer)):
1101         axis = [axis]
1102     elif isinstance(axis, np.ndarray) and axis.ndim == 0:
1103         axis = [int(axis)]
1104     else:
1105         axis = [int(a) for a in axis]
1106     mean_input = mean(input, axis, keepdims=True)
1107     centered_input = input - mean_input
1108     two = constant(2, dtype=centered_input.dtype)
1109     if ddof == 0:
1110         v = mean((centered_input ** two), axis, keepdims=keepdims)
1111     else:
1112         shp = shape(input) - ddof
1113         v = sum((centered_input ** two), axis=axis, keepdims=keepdims)
1114         for i in axis:
1115             v = true_div(v, shp[i])
1116     if corrected:
1117         if ddof == 0:
1118             error = mean(centered_input, axis, keepdims=keepdims) ** 2
1119         else:
1120             shp = shape(input) - ddof
1121             shp_inp = shape(input)
1122             error = sum(centered_input, axis=axis, keepdims=keepdims) ** 2
1123             for i in axis:
1124                 error = true_div(error, shp[i] * shp_inp[i])
1125         v = v - error
1126     v.name = 'var'
1127     return v
1128 @constructor
1129 def std(input, axis=None, ddof=0, keepdims=False, corrected=False):
1130     if isinstance(ddof, (bool)):
1131         raise ValueError('Parameter keepdims is now at index 3: (input, \
1132                           axis=None, ddof=0, keepdims=False, corrected=False)')
1133     ret = sqrt(var(input=input, axis=axis, ddof=ddof,
1134                    keepdims=keepdims, corrected=corrected))
1135     ret.name = 'std'
1136     return ret
1137 class Default(gof.Op):
1138     view_map = {0: [0]}
1139     __props__ = ()
1140     def make_node(self, x, default):
1141         x, default = as_tensor_variable(x), as_tensor_variable(default)
1142         if x.type != default.type:
1143             raise TypeError('Both default() arguments must have same type',
1144                             x, default)
1145         return gof.Apply(self, [x, default], [default.type()])
1146     def perform(self, node, inp, out_):
1147         x, default = inp
1148         out, = out_
1149         if x is None:
1150             out[0] = default.copy()
1151         else:
1152             out[0] = x
1153 default = Default()
1154 setdefault = default  # legacy
1155 @_scal_elemwise
1156 def maximum(x, y):
1157 @_scal_elemwise
1158 def minimum(x, y):
1159 def div_proxy(x, y):
1160     f = scal.int_or_true_div(
1161         as_tensor_variable(x).dtype in discrete_dtypes,
1162         as_tensor_variable(y).dtype in discrete_dtypes)
1163     if f is scal.int_div:
1164         return int_div(x, y)
1165     else:
1166         return true_div(x, y)
1167 def divmod(x, y):
1168     return floor_div(x, y), mod_check(x, y)
1169 @_scal_elemwise
1170 def add(a, *other_terms):
1171 @_scal_elemwise
1172 def sub(a, b):
1173 @_scal_elemwise
1174 def mul(a, *other_terms):
1175 @_scal_elemwise
1176 def true_div(a, b):
1177 @_scal_elemwise
1178 def int_div(a, b):
1179 floor_div = int_div
1180 def ceil_intdiv(a, b):
1181     div = int_div(a, b)
1182     ret = cast(neq(a % b, 0), div.dtype) + div
1183     assert ret.dtype == scal.upcast(div.owner.inputs[0], div.owner.inputs[1])
1184     return ret
1185 def mod_check(x, y):
1186     if ((as_tensor_variable(x).dtype in complex_dtypes or
1187          as_tensor_variable(y).dtype in complex_dtypes)):
1188         raise scal.Mod.complex_error
1189     else:
1190         return mod(x, y)
1191 @_scal_elemwise
1192 def mod(a, b):
1193 @_scal_elemwise
1194 def pow(a, b):
1195 @_scal_elemwise
1196 def clip(x, min, max):
1197 pprint.assign(add, printing.OperatorPrinter('+', -2, 'either'))
1198 pprint.assign(mul, printing.OperatorPrinter('*', -1, 'either'))
1199 pprint.assign(sub, printing.OperatorPrinter('-', -2, 'left'))
1200 pprint.assign(neg, printing.OperatorPrinter('-', 0, 'either'))
1201 pprint.assign(true_div, printing.OperatorPrinter('/', -1, 'left'))
1202 pprint.assign(int_div, printing.OperatorPrinter('//', -1, 'left'))
1203 pprint.assign(pow, printing.OperatorPrinter('**', 1, 'right'))
1204 def extract_constant(x, elemwise=True, only_process_constants=False):
1205     try:
1206         x = get_scalar_constant_value(x,
1207                                       elemwise,
1208                                       only_process_constants)
1209     except NotScalarConstantError:
1210         pass
1211     if ((isinstance(x, scal.ScalarVariable) or
1212          isinstance(x, scal.sharedvar.ScalarSharedVariable))):
1213         if x.owner and isinstance(x.owner.op, ScalarFromTensor):
1214             x = x.owner.inputs[0]
1215         else:
1216             x = tensor_from_scalar(x)
1217     return x
1218 def transpose(x, axes=None):
1219     if axes is None:
1220         axes = list(range((x.ndim - 1), -1, -1))
1221     ret = DimShuffle(x.broadcastable, axes)(x)
1222     if x.name and axes == list(range((x.ndim - 1), -1, -1)):
1223         ret.name = x.name + '.T'
1224     return ret
1225 def batched_dot(a, b):
1226     a, b = as_tensor_variable(a), as_tensor_variable(b)
1227     if a.ndim == 0:
1228         raise TypeError("a must have at least one (batch) axis")
1229     elif b.ndim == 0:
1230         raise TypeError("b must have at least one (batch) axis")
1231     elif a.ndim == 1:
1232         return a.dimshuffle(*([0] + ["x"] * (b.ndim - 1))) * b
1233     elif b.ndim == 1:
1234         return a * b.dimshuffle(*([0] + ["x"] * (a.ndim - 1)))
1235     elif a.ndim &gt; 3 or b.ndim &gt; 3:
1236         return batched_tensordot(
1237             a, b, [[a.ndim - 1], [np.maximum(1, b.ndim - 2)]])
1238     else:
1239         return theano.tensor.blas.BatchedDot()(a, b)
1240 def batched_tensordot(x, y, axes=2):
1241     return _tensordot_as_dot(x, y, axes, dot=batched_dot, batched=True)
1242 def split(x, splits_size, n_splits, axis=0):
1243     the_split = Split(n_splits)
1244     return the_split(x, axis, splits_size)
1245 class Split(Op):
1246     len_splits = None
1247     __props__ = ("len_splits",)
1248     def __init__(self, len_splits):
1249         self.len_splits = int(len_splits)
1250     def __str__(self):
1251         return self.__class__.__name__ + "{%s}" % self.len_splits
1252     def make_node(self, x, axis, splits):
1253         x = as_tensor_variable(x)
1254         axis = as_tensor_variable(axis)
1255         splits = as_tensor_variable(splits)
1256         if splits.type not in int_vector_types:
1257             raise TypeError('splits must have type tensor.lvector',
1258                             splits.type)
1259         if axis.type not in int_types:
1260             raise TypeError('axis must have type lscalar', axis.type)
1261         inputs = [x, axis, splits]
1262         outputs = [x.type() for i in xrange(self.len_splits)]
1263         return Apply(self, inputs, outputs)
1264     def perform(self, node, inputs, outputs):
1265         x, axis, splits = inputs
1266         if sys.version_info[0:2] == (2, 4) and axis.size == 1:
1267             axis = int(axis)
1268         try:
1269             len_along_axis = x.shape[axis]
1270         except Exception:
1271             raise ValueError('Split.perform() with axis=(%s) is invalid'
1272                              ' for x.shape==(%s)'
1273                              % (axis, x.shape))
1274         if len(splits) != self.len_splits:
1275             raise ValueError('In Split.perform(), len(splits) != len_splits.',
1276                              (len(splits), self.len_splits))
1277         if np.sum(splits) != len_along_axis:
1278             raise ValueError('The splits sum to %s, expected %s' %
1279                              (np.sum(splits), len_along_axis))
1280         if python_any([nb &lt; 0 for nb in splits]):
1281             raise ValueError('Split: you tried to make an ndarray with a '
1282                              'negative number of elements.')
1283         general_key = [slice(None, None, None) for s in x.shape]
1284         lower_idx = 0
1285         for i in xrange(self.len_splits):
1286             upper_idx = lower_idx + splits[i]
1287             general_key[axis] = slice(lower_idx, upper_idx, None)
1288             outputs[i][0] = x.__getitem__(tuple(general_key)).copy()
1289             lower_idx = upper_idx
1290     def infer_shape(self, node, in_shapes):
1291         axis = node.inputs[1]
1292         splits = node.inputs[2]
1293         shp_x, shp_axis, shp_splits = in_shapes
1294         out_shapes = []
1295         for i in xrange(self.len_splits):
1296             temp = as_tensor_variable(shp_x)
1297             temp = theano.tensor.subtensor.set_subtensor(temp[axis], splits[i])
1298             temp = [temp[i] for i in xrange(len(shp_x))]
1299             out_shapes.append(temp)
1300         return out_shapes
1301     def grad(self, inputs, g_outputs):
1302         x, axis, n = inputs
1303         outputs = self(*inputs, **dict(return_list=True))
1304         if python_all([isinstance(g.type, DisconnectedType)
1305                        for g in g_outputs]):
1306             return [DisconnectedType()(),
1307                     grad_undefined(self, 1, axis),
1308                     grad_undefined(self, 2, n)]
1309         new_g_outputs = []
1310         for o, g in zip(outputs, g_outputs):
1311             if isinstance(g.type, DisconnectedType):
1312                 new_g_outputs.append(o.zeros_like())
1313             else:
1314                 new_g_outputs.append(g)
1315         return [join(axis, *new_g_outputs),
1316                 grad_undefined(self, 1, axis),
1317                 grad_undefined(self, 2, n)]
1318     def R_op(self, inputs, eval_points):
1319         if eval_points[0] is None:
1320             return [None for i in self.len_splits]
1321         return self.make_node(eval_points[0], *inputs[1:]).outputs
1322     def c_code_cache_version(self):
1323         return (2,)
1324     def c_support_code(self):
1325         return """
1326         /* Return 1 if output has the correct shape. */
1327         int split_output_shape_is_correct (
1328             PyArrayObject* output, PyArrayObject* array_to_split, int axis_to_split, npy_intp split_size
1329         ) {
1330             return
1331                 PyArray_NDIM(output) == PyArray_NDIM(array_to_split)
1332                 &amp;&amp; memcmp(
1333                     PyArray_DIMS(output),
1334                     PyArray_DIMS(array_to_split),
1335                     axis_to_split * sizeof(npy_intp)
1336                 ) == 0
1337                 &amp;&amp; memcmp(
1338                     PyArray_DIMS(output) + axis_to_split + 1,
1339                     PyArray_DIMS(array_to_split) + axis_to_split + 1,
1340                     (PyArray_NDIM(array_to_split) - axis_to_split - 1) * sizeof(npy_intp)
1341                 ) == 0
1342                 &amp;&amp; split_size == PyArray_DIM(output, axis_to_split);
1343         }
1344 def addbroadcast(x, *axes):
1345     rval = Rebroadcast(*[(axis, True) for axis in axes])(x)
1346     return theano.tensor.opt.apply_rebroadcast_opt(rval)
1347 def unbroadcast(x, *axes):
1348     rval = Rebroadcast(*[(axis, False) for axis in axes])(x)
1349     return theano.tensor.opt.apply_rebroadcast_opt(rval)
1350 def patternbroadcast(x, broadcastable):
1351     rval = Rebroadcast(*[(i, broadcastable[i])
1352                          for i in xrange(len(broadcastable))])(x)
1353     return theano.tensor.opt.apply_rebroadcast_opt(rval)
1354 class Join(Op):
1355     check_input = False
1356     __props__ = ("view",)
1357     def __init__(self, view=-1):
1358         self.view = view
1359         if view != -1:
1360             self.view_map = {0: [1 + view]}
1361     def __str__(self):
1362         if self.view == -1:
1363             return self.__class__.__name__
1364         else:
1365             return "%s{%s}" % (
1366                 self.__class__.__name__,
1367                 ", ".join("%s=%r" % (p, getattr(self, p))
1368                           for p in self.__props__))
1369     def __setstate__(self, d):
1370         self.__dict__.update(d)
1371         if not hasattr(self, "view"):
1372             self.view = -1
1373     def make_node(self, *axis_and_tensors):
1374         axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]
1375         if not tensors:
1376             raise ValueError('Cannot join an empty list of tensors')
1377         as_tensor_variable_args = [as_tensor_variable(x) for x in tensors]
1378         dtypes = [x.type.dtype for x in as_tensor_variable_args]
1379         out_dtype = scal.upcast(*dtypes)
1380         def output_maker(bcastable):
1381             return tensor(dtype=out_dtype, broadcastable=bcastable)
1382         return self._make_node_internal(
1383             axis, tensors, as_tensor_variable_args, output_maker)
1384     def _make_node_internal(self, axis, tensors,
1385                             as_tensor_variable_args, output_maker):
1386         if not python_all(targs.type.ndim for targs
1387                           in as_tensor_variable_args):
1388             raise TypeError('Join cannot handle arguments of dimension 0.'
1389                             ' For joining scalar values, see @stack')
1390         if len(as_tensor_variable_args) == 1:
1391             bcastable = list(as_tensor_variable_args[0].type.broadcastable)
1392         else:
1393             bcastable = [False] * len(
1394                 as_tensor_variable_args[0].type.broadcastable)
1395             ndim = len(bcastable)
1396             if not isinstance(axis, integer_types):
1397                 try:
1398                     axis = int(get_scalar_constant_value(axis))
1399                 except NotScalarConstantError:
1400                     pass
1401             if isinstance(axis, integer_types):
1402                 if axis &lt; -ndim:
1403                     raise IndexError("Join axis %d out of bounds [0, %d)" %
1404                                      (axis, ndim))
1405                 if axis &lt; 0:
1406                     axis += ndim
1407                 for x in as_tensor_variable_args:
1408                     for current_axis, bflag in enumerate(x.type.broadcastable):
1409                         if current_axis == axis:
1410                             continue
1411                         if bflag:
1412                             bcastable[current_axis] = True
1413                 try:
1414                     bcastable[axis] = False
1415                 except IndexError:
1416                     raise ValueError('Join argument "axis" is out of range'
1417                                      ' (given input dimensions)')
1418             else:
1419                 bcastable = [False] * len(
1420                     as_tensor_variable_args[0].type.broadcastable)
1421         if not python_all([x.ndim == len(bcastable)
1422                            for x in as_tensor_variable_args[1:]]):
1423             raise TypeError("Join() can only join tensors with the same "
1424                             "number of dimensions.")
1425         inputs = [as_tensor_variable(axis)] + list(as_tensor_variable_args)
1426         if inputs[0].type not in int_types:
1427             raise TypeError('Axis could not be cast to an integer type',
1428                             axis, inputs[0].type, int_types)
1429         outputs = [output_maker(bcastable)]
1430         node = Apply(self, inputs, outputs)
1431         return node
1432     def perform(self, node, axis_and_tensors, out_):
1433         out, = out_
1434         view = self.view
1435         axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]
1436         if (view != -1) and np.all(
1437                 [tensor.shape[axis] == 0 for tensor in
1438                  tensors[0:view] + tensors[view + 1:]]):
1439             out[0] = tensors[view]
1440         else:
1441             ndim = tensors[0].ndim
1442             if axis &lt; -ndim:
1443                 raise IndexError("Join axis %d out of bounds [0, %d)" %
1444                                  (axis, ndim))
1445             out[0] = theano._asarray(np.concatenate(tensors, axis=axis),
1446                                      dtype=node.outputs[0].type.dtype)
1447     def c_code_cache_version(self):
1448         return (5,)
1449     def c_code(self, node, name, inputs, outputs, sub):
1450         axis, tensors = inputs[0], inputs[1:]
1451         view = self.view
1452         non_empty_tensor = tensors[view]
1453         input_1 = tensors[0]
1454         l = len(tensors)
1455         out, = outputs
1456         fail = sub['fail']
1457         adtype = node.inputs[0].type.dtype_specs()[1]
1458         copy_to_list = []
1459         for i, inp in enumerate(tensors):
1460             copy_to_list.append(
1461         return code
1462     def R_op(self, inputs, eval_points):
1463         if None in eval_points[1:]:
1464             return [None]
1465         return self.make_node(inputs[0], *eval_points[1:]).outputs
1466     def grad(self, axis_and_tensors, grads):
1467         gz, = grads
1468         axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]
1469         rval = [grad_undefined(self, 0, axis)]
1470         dtypes = [as_tensor_variable(x).type.dtype for x in tensors]
1471         out_dtype = scal.upcast(*dtypes)
1472         if 'float' in out_dtype or 'complex' in out_dtype:
1473             split = Split(len(tensors))
1474             split_gz = split(gz, axis, stack([shape(x)[axis]
1475                                               for x in tensors]))
1476             if not isinstance(split_gz, list):
1477                 split_gz = [split_gz]
1478             split_gz = [patternbroadcast(g, t.broadcastable)
1479                         for t, g in zip(tensors, split_gz)]
1480             rval = rval + split_gz
1481         else:
1482             rval = rval + [tensor.zeros_like(dtype=config.floatX)
1483                            for tensor in tensors]
1484         return rval
1485     def infer_shape(self, node, ishapes):
1486         assert len(ishapes) &gt; 1
1487         n_dim = len(ishapes[1])
1488         for shp in ishapes[1:]:
1489             assert shp is not None
1490             assert len(shp) == n_dim
1491         join_dim = switch(ge(node.inputs[0], 0),
1492                           node.inputs[0],
1493                           node.inputs[0] + n_dim)
1494         out_shapes = []
1495         for dim in xrange(n_dim):
1496             t_side = ishapes[1][dim]
1497             f_side = ishapes[1][dim]
1498             for shp in ishapes[2:]:
1499                 t_side = t_side + shp[dim]
1500             out_shapes.append(switch(eq(dim, join_dim),
1501                               t_side, f_side))
1502         return [tuple(out_shapes)]
1503 join_ = Join()
1504 pprint.assign(Join, printing.FunctionPrinter('join'))
1505 def join(axis, *tensors_list):
1506     if len(tensors_list) == 1:
1507         return tensors_list[0]
1508     else:
1509         return join_(axis, *tensors_list)
1510 def roll(x, shift, axis=None):
1511     if axis is None:
1512         if x.ndim &gt; 1:
1513             y = x.flatten()
1514             return roll(y, shift, axis=0).reshape(x.shape)
1515         else:
1516             axis = 0
1517     if axis &lt; 0:
1518         axis += x.ndim
1519     shift = shift % x.shape[axis]
1520     allslice = slice(None)
1521     front_slice = slice(-shift, None)
1522     front_list = ([allslice] * axis + [front_slice] +
1523                   [allslice] * (x.ndim - axis - 1))
1524     end_slice = slice(0, -shift)
1525     end_list = ([allslice] * axis + [end_slice] +
1526                 [allslice] * (x.ndim - axis - 1))
1527     return join(axis,
1528                 x.__getitem__(tuple(front_list)),
1529                 x.__getitem__(tuple(end_list)))
1530 @constructor
1531 def shape_padleft(t, n_ones=1):
1532     _t = as_tensor_variable(t)
1533     pattern = ['x'] * n_ones + [i for i in xrange(_t.type.ndim)]
1534     return DimShuffle(_t.broadcastable, pattern)(_t)
1535 @constructor
1536 def shape_padright(t, n_ones=1):
1537     _t = as_tensor_variable(t)
1538     pattern = [i for i in xrange(_t.type.ndim)] + ['x'] * n_ones
1539     return DimShuffle(_t.broadcastable, pattern)(_t)
1540 @constructor
1541 def shape_padaxis(t, axis):
1542     _t = as_tensor_variable(t)
1543     ndim = _t.ndim + 1
1544     if not -ndim &lt;= axis &lt; ndim:
1545         msg = 'axis {0} is out of bounds [-{1}, {1})'.format(axis, ndim)
1546         raise IndexError(msg)
1547     if axis &lt; 0:
1548         axis += ndim
1549     pattern = [i for i in xrange(_t.type.ndim)]
1550     pattern.insert(axis, 'x')
1551     return DimShuffle(_t.broadcastable, pattern)(_t)
1552 @constructor
1553 def stack(*tensors, **kwargs):
1554     if not tensors and not kwargs:
1555         raise Exception('theano.tensor.stack(tensors, axis) must have at least'
1556                         ' one parameter')
1557     if not kwargs and not isinstance(tensors[0], (list, tuple)):
1558         warnings.warn('stack(*tensors) interface is deprecated, use'
1559                       ' stack(tensors, axis=0) instead.', DeprecationWarning,
1560                       stacklevel=3)
1561         axis = 0
1562     elif 'tensors' in kwargs:
1563         tensors = kwargs['tensors']
1564         if 'axis' in kwargs:
1565             axis = kwargs['axis']
1566         else:
1567             axis = 0
1568     else:
1569         if len(tensors) == 2:
1570             axis = tensors[1]
1571         elif 'axis' in kwargs:
1572             axis = kwargs['axis']
1573         else:
1574             axis = 0
1575         tensors = tensors[0]
1576     if len(tensors) == 0:
1577         raise Exception('tensors is empty. You should at least provide one'
1578                         ' tensor to theano.tensor.stack(tensors, axis).')
1579     if np.all(
1580         [  # in case there is direct int in tensors.
1581             isinstance(t, (np.number, float, integer_types,
1582                            python_complex)) or
1583             (isinstance(t, Variable) and
1584              isinstance(t.type, TensorType) and
1585              t.ndim == 0)
1586             for t in tensors]):
1587         tensors = list(map(as_tensor_variable, tensors))
1588         dtype = scal.upcast(*[i.dtype for i in tensors])
1589         return theano.tensor.opt.MakeVector(dtype)(*tensors)
1590     return join(axis, *[shape_padaxis(t, axis) for t in tensors])
1591 @constructor
1592 def concatenate(tensor_list, axis=0):
1593     if not isinstance(tensor_list, (tuple, list)):
1594         raise TypeError(
1595             "The 'tensors' argument must be either a tuple "
1596             "or a list, make sure you did not forget () or [] around "
1597             "arguments of concatenate.", tensor_list)
1598     return join(axis, *tensor_list)
1599 def get_vector_length(v):
1600     v = as_tensor_variable(v)
1601     if v.ndim != 1:
1602         raise TypeError("argument must be symbolic vector, got '%s'" %
1603                         v)
1604     if v.type.broadcastable[0]:
1605         return 1
1606     if isinstance(v, gof.Constant) and v.type.ndim == 1:
1607         return len(v.data)
1608     if v.owner and isinstance(v.owner.op, theano.tensor.opt.MakeVector):
1609         return len(v.owner.inputs)
1610     if v.owner and isinstance(v.owner.op, Shape):
1611         return v.owner.inputs[0].type.ndim
1612     if ((v.owner and
1613          isinstance(v.owner.op, theano.tensor.subtensor.Subtensor) and
1614          isinstance(v.owner.op.idx_list[0], slice) and
1615          v.owner.inputs[0].owner and
1616          isinstance(v.owner.inputs[0].owner.op, theano.compile.ops.Shape))):
1617         start = extract_constant(theano.tensor.subtensor.get_idx_list(
1618             v.owner.inputs, v.owner.op.idx_list)[0].start)
1619         stop = extract_constant(theano.tensor.subtensor.get_idx_list(
1620             v.owner.inputs, v.owner.op.idx_list)[0].stop)
1621         step = extract_constant(theano.tensor.subtensor.get_idx_list(
1622             v.owner.inputs, v.owner.op.idx_list)[0].step)
1623         ndim = v.owner.inputs[0].owner.inputs[0].ndim
1624         types = (numbers.Integral, np.integer)
1625         if start is None:
1626             start = 0
1627         elif isinstance(start, types) and start &lt; 0:
1628             start += ndim
1629             if start &lt; 0:
1630                 start = 0
1631         if stop is None:
1632             stop = ndim
1633         elif isinstance(stop, types):
1634             if stop &gt; ndim:
1635                 stop = ndim
1636             elif stop &lt; 0:
1637                 stop += ndim
1638         if step is None:
1639             step = 1
1640         if (isinstance(stop, types) and
1641                 isinstance(start, types) and
1642                 isinstance(step, types) and
1643                 start &gt;= 0 and stop &gt;= 0 and
1644                 step &gt; 0 and stop &gt;= start):
1645             return (stop - start - 1) // step + 1
1646     if isinstance(v, Variable):
1647         msg = theano.printing.debugprint(v, file='str')
1648     else:
1649         msg = str(v)
1650     raise ValueError("length not known: %s" % msg)
1651 @constructor
1652 def horizontal_stack(*args):
1653     assert len(args) &gt;= 2
1654     for arg in args:
1655         assert arg.type.ndim == 2
1656     return concatenate(args, axis=1)
1657 @constructor
1658 def vertical_stack(*args):
1659     assert len(args) &gt;= 2
1660     for arg in args:
1661         assert arg.type.ndim == 2
1662     return concatenate(args, axis=0)
1663 class Reshape(Op):
1664     view_map = {0: [0]}  # output 0 is potentially aliased to inputs [0]
1665     _f16_ok = True
1666     check_input = False
1667     __props__ = ("ndim",)
1668     params_type = ParamsType(ndim=int32)
1669     def __init__(self, ndim, name=None):
1670         self.ndim = int(ndim)
1671         if ndim &lt; 0:
1672             raise ValueError("The output dimensions after reshape must be 0 or greater")
1673         assert name is None, 'name attribute for Reshape has been deprecated'
1674     def __str__(self):
1675         return '%s{%s}' % (self.__class__.__name__, self.ndim)
1676     def make_node(self, x, shp):
1677         x = as_tensor_variable(x)
1678         shp_orig = shp
1679         shp = as_tensor_variable(shp, ndim=1)
1680         if not (shp.dtype in int_dtypes or
1681                 (isinstance(shp, TensorConstant) and shp.data.size == 0)):
1682             raise TypeError("Shape must be integers", shp, shp.dtype)
1683         assert shp.ndim == 1
1684         if isinstance(shp, TensorConstant):
1685             bcast = [s == 1 for s in shp.data]
1686             return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcast)])
1687         else:
1688             bcasts = [False] * self.ndim
1689             shp_list = shp_orig
1690             if hasattr(shp_orig, "ndim") and shp_orig.ndim == 0:
1691                 shp_list = [shp_orig]
1692             for index in xrange(self.ndim):
1693                 y = shp_list[index]
1694                 y = as_tensor_variable(y)
1695                 try:
1696                     bcasts[index] = (
1697                         hasattr(y, 'get_scalar_constant_value') and
1698                         y.get_scalar_constant_value() == 1)
1699                 except NotScalarConstantError:
1700                     pass
1701             return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcasts)])
1702     def perform(self, node, inp, out_, params):
1703         x, shp = inp
1704         out, = out_
1705         if (len(shp) != self.ndim):
1706             raise ValueError('shape argument to Reshape.perform has incorrect'
1707                              ' length %i'
1708                              ', should be %i' % (len(shp), self.ndim), shp)
1709         try:
1710             out[0] = np.reshape(x, shp)
1711         except Exception:
1712             raise ValueError('Cannot reshape input of shape %s to shape %s' %
1713                              (x.shape, shp))
1714     def connection_pattern(self, node):
1715         return [[True], [False]]
1716     def grad(self, inp, grads):
1717         x, shp = inp
1718         g_out, = grads
1719         return [reshape(g_out, shape(x), ndim=x.ndim),
1720                 DisconnectedType()()]
1721     def R_op(self, inputs, eval_points):
1722         if eval_points[0] is None:
1723             return [None]
1724         return self(eval_points[0], *inputs[1:], **dict(return_list=True))
1725     def infer_shape(self, node, ishapes):
1726         if len(ishapes[0]) == 0:
1727             return [(1,) * self.ndim]
1728         requ = node.inputs[1]
1729         input_size = mul(*ishapes[0])
1730         if isinstance(requ, theano.tensor.TensorConstant):
1731             requ = list(requ.data)
1732             requ_part = [ele for ele in requ if ele != -1]
1733             crit = len(requ) - len(requ_part)
1734             if crit == 1 and len(requ_part) &gt; 0:
1735                 requ_size = mul(*requ_part)
1736                 missing = input_size // (1 if requ_size == 0 else requ_size)
1737                 for i, ele in enumerate(requ):
1738                     if ele == -1:
1739                         requ[i] = missing
1740             elif crit == 1:  # we reshape to -1
1741                 requ = [input_size] if ishapes[0] else [1]
1742             elif crit &gt; 1:
1743                 raise ValueError('shape argument to Reshape.perform'
1744                                  ' must have at most one entry equal to -1')
1745             return [requ]
1746         else:
1747             requ = [requ[i] for i in xrange(self.ndim)]
1748             if self.ndim:
1749                 requ_size = -mul(*requ)
1750                 rest_size = input_size // maximum(requ_size, 1)
1751             return [tuple([switch(eq(requ[i], -1),
1752                                   rest_size,
1753                                   requ[i])
1754                            for i in xrange(self.ndim)])]
1755     def c_code_cache_version(self):
1756         return (8,)
1757     def c_code(self, node, name, inputs, outputs, sub):
1758         if isinstance(node.inputs[0], TensorVariable):
1759             x, shp = inputs
1760             z, = outputs
1761             sdtype = node.inputs[1].type.dtype_specs()[1]
1762             fail = sub['fail']
1763             params = sub['params']
1764             return """
1765             assert (PyArray_NDIM(%(shp)s) == 1);
1766             npy_intp new_dims[%(params)s-&gt;ndim];
1767             PyArray_Dims newshape;
1768             newshape.ptr = new_dims;
1769             newshape.len = %(params)s-&gt;ndim;
1770             for (int ii = 0; ii &lt; %(params)s-&gt;ndim; ++ii)
1771             {
1772                 // -- We do not want an explicit cast here. the shp can be any
1773                 // -- int* dtype. The compiler will explicitly upcast it, but
1774                 // -- will err if this will downcast. This could happen if the
1775                 // -- user pass an int64 dtype, but npy_intp endup being int32.
1776                 new_dims[ii] = ((%(sdtype)s*)(
1777                         PyArray_BYTES(%(shp)s) +
1778                         ii * PyArray_STRIDES(%(shp)s)[0]))[0];
1779             }
1780             Py_XDECREF(%(z)s);
1781             %(z)s = (PyArrayObject *) PyArray_Newshape(%(x)s, &amp;newshape, NPY_CORDER);
1782             if (!%(z)s)
1783             {
1784                 //The error message should have been set by PyArray_Newshape
1785                 %(fail)s;
1786             }
1787     Flatten a tensor.
1788     Flattens a tensor to `outdim` dimensions by preserving the leading
1789     outdim - 1 shape components.
1790     .. note:: The interface Flatten(Op) is deprecated, you should use flatten.
1791 def is_flat(var, ndim=None, outdim=None):
1792     if outdim is None and ndim is None:
1793         ndim = 1
1794     elif outdim is not None and ndim is not None:
1795         raise ValueError("You should only specify ndim")
1796     elif outdim is not None:
1797         warnings.warn(
1798             "flatten outdim parameter is deprecated, use ndim instead.")
1799         ndim = outdim
1800     return var.ndim == ndim
1801 def flatten(x, ndim=None, outdim=None):
1802     if outdim is None and ndim is None:
1803         ndim = 1
1804     elif outdim is not None and ndim is not None:
1805         raise ValueError("You should only specify ndim")
1806     elif outdim is not None:
1807         warnings.warn(
1808             "flatten outdim parameter is deprecated, use ndim instead.")
1809         ndim = outdim
1810     if ndim &lt; 1 or (ndim &gt; 1 and ndim &gt; x.ndim):
1811         raise ValueError('ndim %s out of bound [1, %d)'
1812                          % (ndim, x.ndim + 1))
1813     if ndim &gt; 1:
1814         dims = tuple(x.shape[:ndim - 1]) + (-1,)
1815     else:
1816         dims = (-1,)
1817     x_reshaped = x.reshape(dims)
1818     bcast_kept_dims = x.broadcastable[:ndim - 1]
1819     bcast_new_dim = python_all(x.broadcastable[ndim - 1:])
1820     broadcastable = bcast_kept_dims + (bcast_new_dim,)
1821     x_reshaped = theano.tensor.addbroadcast(
1822         x_reshaped, *filter(lambda i: broadcastable[i], range(ndim)))
1823     return x_reshaped
1824 class Tile(Op):
1825     __props__ = ("ndim",)
1826     def __init__(self, ndim):
1827         self.ndim = ndim
1828     def __str__(self):
1829         return self.__class__.__name__ + "{ndim=%d}" % self.ndim
1830     def make_node(self, x, reps):
1831         warnings.warn((
1832             "Tile op is deprecated, use tile function instead."), stacklevel=3)
1833         x = as_tensor_variable(x)
1834         reps = as_tensor_variable(reps)
1835         return gof.Apply(self, [x, reps], [tensor(x.type.dtype, [False] *
1836                                                   self.ndim)])
1837     def perform(self, node, inp, out_):
1838         x, reps = inp
1839         out, = out_
1840         res = np.tile(x, reps)
1841         if res.ndim != self.ndim:
1842             raise ValueError(
1843                 'Tile.perform produced incorrect number of dimensions')
1844         if (np.asarray(reps) == 1).all():
1845             if np.may_share_memory(res, x):
1846                 res = res.copy()
1847         out[0] = res
1848     def infer_shape(self, node, in_shapes):
1849         x, reps = node.inputs
1850         shp = in_shapes[0]
1851         tiled_shp = shp * reps
1852         out_shape = []
1853         for i in xrange(self.ndim):
1854             out_shape.append(tiled_shp[i])
1855         return [out_shape]
1856     def grad(self, inp, grads):
1857         x, reps = inp
1858         g_out, = grads
1859         raise NotImplementedError()
1860 def tile(x, reps, ndim=None):
1861     if ndim is not None and ndim &lt; x.ndim:
1862         raise ValueError("ndim should be equal or larger than x.ndim")
1863     if not isinstance(reps, (list, tuple)):
1864         reps_astensor = as_tensor_variable(reps)
1865         ndim_check = reps_astensor.ndim
1866         if reps_astensor.dtype not in theano.tensor.discrete_dtypes:
1867             raise ValueError("elements of reps must be integer dtype")
1868         if ndim_check == 0:
1869             reps = [reps]
1870         elif ndim_check == 1:
1871             if ndim is None:
1872                 raise ValueError("if reps is tensor.vector, you should specify "
1873                                  "the ndim")
1874             else:
1875                 offset = ndim - reps.shape[0]
1876                 offset = theano.tensor.opt.assert_(offset, ge(offset, 0))
1877                 reps_ = [switch(i &lt; offset, 1, reps[i - offset]) for i in range(ndim)]
1878                 reps = reps_
1879         else:
1880             raise ValueError("the dimension of reps should not exceed 1")
1881     else:
1882         if ndim is not None and len(reps) &gt; ndim:
1883             raise ValueError("len(reps) should be equal or less than ndim")
1884         if not np.all([isinstance(r, integer_types) or
1885                        (isinstance(r, TensorVariable) and
1886                         r.dtype in theano.tensor.discrete_dtypes) for r in reps]):
1887             raise ValueError("elements of reps must be scalars of integer dtype")
1888     reps = list(reps)
1889     if ndim is None:
1890         ndim = builtins.max(len(reps), x.ndim)
1891 <a name="3"></a>    if len(reps) &lt; ndim:
1892         reps = [1] * (ndim - len(reps)) + reps
1893     shape = [1] * (ndim - x.ndim) + [x.shape<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[i] for i in xrange(x.ndim)]
1894     alloc_shape = reps + shape
1895     y = alloc(x, *alloc_shape)
1896     shuffle_ind = np.arange(ndim * 2).reshape(2, ndim)
1897     shuffle_ind =</b></font> shuffle_ind.transpose().flatten()
1898     y = y.dimshuffle(*shuffle_ind)
1899     new_shapes = [sh * reps[i] for i, sh in enumerate(shape)]
1900     y = y.reshape(new_shapes)
1901     return y
1902 class ARange(Op):
1903     __props__ = ("dtype",)
1904     def __init__(self, dtype):
1905         self.dtype = dtype
1906     def make_node(self, start, stop, step):
1907         start, stop, step = map(as_tensor_variable, (start, stop, step))
1908         assert start.ndim == 0
1909         assert stop.ndim == 0
1910         assert step.ndim == 0
1911         inputs = [start, stop, step]
1912         outputs = [tensor(self.dtype, (False,))]
1913         return Apply(self, inputs, outputs)
1914     @theano.configparser.change_flags(warn_float64='ignore')
1915     def infer_shape(self, node, i_shapes):
1916         start, stop, step = node.inputs
1917         def is_constant_value(var, value):
1918             try:
1919                 v = get_scalar_constant_value(var)
1920                 return np.all(v == value)
1921             except NotScalarConstantError:
1922                 pass
1923             return False
1924         def upcast(var):
1925             if (var.dtype in integer_dtypes and
1926                     scal.upcast(var.dtype, 'int64') == 'int64'):
1927                 return cast(var, 'int64')
1928             return var
1929         if is_constant_value(step, 1):
1930             if is_constant_value(start, 0):
1931                 return [(cast(stop, 'int64'),)]
1932             else:
1933                 stop = upcast(stop)
1934                 start = upcast(start)
1935                 return [(maximum(cast(stop - start, 'int64'), 0),)]
1936         else:
1937             stop = upcast(stop)
1938             start = upcast(start)
1939             return [(maximum(cast(ceil(cast((stop - start), 'float64') / step),
1940                     'int64'), 0),)]
1941     def perform(self, node, inp, out_):
1942         start, stop, step = inp
1943         out, = out_
1944         start = start.item()
1945         stop = stop.item()
1946         step = step.item()
1947         out[0] = np.arange(start, stop, step, dtype=self.dtype)
1948     def connection_pattern(self, node):
1949         return [[True], [False], [True]]
1950     def L_op(self, inputs, outputs, grads):
1951         start, stop, step = inputs
1952         gz, = grads
1953         if self.dtype in discrete_dtypes:
1954             return [start.zeros_like(dtype=config.floatX),
1955                     DisconnectedType()(),
1956                     step.zeros_like(dtype=config.floatX)]
1957         else:
1958             num_steps_taken = outputs[0].shape[0]
1959             return [gz.sum(),
1960                     DisconnectedType()(),
1961                     (gz * arange(num_steps_taken, dtype=self.dtype)).sum()]
1962     def R_op(self, inputs, eval_points):
1963         return [None]
1964 _arange = {}
1965 def arange(start, stop=None, step=1, dtype=None):
1966     if stop is None:
1967         start, stop = 0, start
1968     start, stop, step = map(as_tensor_variable, (start, stop, step))
1969     if dtype is None:
1970         dtype = scal.upcast(start.type.dtype, stop.type.dtype, step.type.dtype)
1971         if dtype in int_dtypes:
1972             dtype = 'int64'
1973         if dtype in uint_dtypes:
1974             dtype = 'uint64'
1975         if config.cast_policy in ('numpy', 'numpy+floatX'):
1976             numpy_dtype = np.arange(
1977                 start=np.array(0, dtype=start.dtype),
1978                 stop=np.array(1, dtype=stop.dtype),
1979                 step=np.array(1, dtype=step.dtype)).dtype
1980             if numpy_dtype != dtype:
1981                 if (config.cast_policy == 'numpy+floatX' and
1982                     config.floatX == 'float32' and
1983                     numpy_dtype == 'float64' and
1984                     python_all(
1985                         dt != 'float64'
1986                         for dt in [s.dtype for s in (start, stop, step)])):
1987                     assert dtype != 'float64'
1988                     dtype = 'float32'
1989                 else:
1990                     dtype = str(numpy_dtype)
1991     if dtype not in _arange:
1992         _arange[dtype] = ARange(dtype)
1993     return _arange[dtype](start, stop, step)
1994 class _nd_grid(object):
1995     def __init__(self, sparse=False):
1996         self.sparse = sparse
1997     def __getitem__(self, *args):
1998         ndim = len(args[0])
1999         for sl in args[0]:
2000             if isinstance(sl.step, python_complex):
2001                 raise NotImplementedError("Not implemented for slices "
2002 <a name="2"></a>                                          "whose step is complex")
2003         ranges = [arange(sl.start or 0,
2004                          sl.stop,
2005                          sl<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.step or 1) for sl in args[0]]
2006         shapes = [tuple([1] * j + [r.shape[0]] + [1] * (ndim - 1 - j))
2007                   for j, r in enumerate(ranges)]
2008         ranges = [r.reshape(</b></font>shape) for r, shape in zip(ranges, shapes)]
2009         if self.sparse:
2010             grids = ranges
2011         else:
2012             grids = []
2013             ones = [ones_like(r) for r in ranges]
2014             for i in range(ndim):
2015                 grid = 1
2016                 for j in range(ndim):
2017                     if j == i:
2018                         grid = grid * ranges[j]
2019                     else:
2020                         grid = grid * ones[j]
2021                 grids.append(grid)
2022         return grids
2023 mgrid = _nd_grid()
2024 ogrid = _nd_grid(sparse=True)
2025 class PermuteRowElements(Op):
2026     __props__ = ()
2027     def make_node(self, x, y, inverse):
2028         x = as_tensor_variable(x)
2029         y = as_tensor_variable(y)
2030         if inverse:  # as_tensor_variable does not accept booleans
2031             inverse = as_tensor_variable(1)
2032         else:
2033             inverse = as_tensor_variable(0)
2034         assert y.type.dtype in integer_dtypes
2035         assert (inverse.type.ndim == 0 and inverse.type.dtype in integer_dtypes)
2036         x_dim = x.type.ndim
2037         y_dim = y.type.ndim
2038         if x_dim &gt; y_dim:
2039             y = shape_padleft(y, n_ones=(x_dim - y_dim))
2040         elif x_dim &lt; y_dim:
2041             x = shape_padleft(x, n_ones=(y_dim - x_dim))
2042         out_broadcastable = [xb and yb for xb, yb in
2043                              izip(x.type.broadcastable, y.type.broadcastable)]
2044         out_type = tensor(dtype=x.type.dtype, broadcastable=out_broadcastable)
2045         inputlist = [x, y, inverse]
2046         outputlist = [out_type]
2047         return Apply(self, inputlist, outputlist)
2048     def _rec_perform(self, node, x, y, inverse, out, curdim):
2049         if len(x.shape) == 1:
2050             if inverse:
2051                 out[y] = x[:]
2052             else:
2053                 out[:] = x[y]
2054         else:
2055             xs0 = x.shape[0]
2056             ys0 = y.shape[0]
2057             if xs0 == ys0:
2058                 for i in xrange(xs0):
2059                     self._rec_perform(node, x[i], y[i], inverse, out[i],
2060                                       curdim + 1)
2061             elif ys0 == 1 and node.inputs[1].type.broadcastable[curdim]:
2062                 for i in xrange(xs0):
2063                     self._rec_perform(node, x[i], y[0], inverse, out[i],
2064                                       curdim + 1)
2065             elif xs0 == 1 and node.inputs[0].type.broadcastable[curdim]:
2066                 for i in xrange(ys0):
2067                     self._rec_perform(node, x[0], y[i], inverse, out[i],
2068                                       curdim + 1)
2069             else:
2070                 raise ValueError('Dimension mismatch: %s, %s' % (xs0, ys0))
2071     def perform(self, node, inp, out):
2072         x, y, inverse = inp
2073         outs, = out
2074         x_s = x.shape
2075         y_s = y.shape
2076         assert len(x_s) == len(y_s)
2077         out_s = []
2078         for xdim, ydim in izip(x_s, y_s):
2079             if xdim == ydim:
2080                 outdim = xdim
2081             elif xdim == 1:
2082                 outdim = ydim
2083             elif ydim == 1:
2084                 outdim = xdim
2085             else:
2086                 raise ValueError('Dimension mismatch: %s, %s' % (xdim, ydim))
2087             out_s.append(outdim)
2088         if outs[0] is None or outs[0].shape != out_s:
2089             outs[0] = np.empty(out_s, dtype=x.dtype)
2090         self._rec_perform(node, x, y, inverse, outs[0], curdim=0)
2091     def infer_shape(self, node, in_shapes):
2092         shp_x = in_shapes[0]
2093         shp_y = in_shapes[1]
2094         assert len(shp_x) == len(shp_y)
2095         out_shape = []
2096         for i in xrange(len(shp_x)):
2097             out_shape.append(maximum(shp_x[i], shp_y[i]))
2098         return [out_shape]
2099     def grad(self, inp, grads):
2100         x, y, inverse = inp
2101         gz, = grads
2102         gx = permute_row_elements(gz, y, eq(inverse, 0))
2103         broadcasted_dims = [dim for dim in xrange(gz.type.ndim)
2104                             if x.type.broadcastable[dim] and
2105                             not gz.type.broadcastable[dim]]
2106         gx = Sum(axis=broadcasted_dims)(gx)
2107         newdims = []
2108         i = 0
2109         for dim in xrange(gz.type.ndim):
2110             if dim in broadcasted_dims:
2111                 newdims.append('x')
2112             else:
2113                 newdims.append(i)
2114                 i += 1
2115         gx = DimShuffle(gx.type.broadcastable, newdims)(gx)
2116         assert gx.type.broadcastable == x.type.broadcastable
2117         if x.type.dtype in discrete_dtypes:
2118             gx = x.zeros_like()
2119         return [gx, grad_undefined(self, 1, y),
2120                 grad_undefined(self, 1, inverse)]
2121 _permute_row_elements = PermuteRowElements()
2122 def permute_row_elements(x, y, inverse=0):
2123     return _permute_row_elements(x, y, inverse)
2124 def inverse_permutation(perm):
2125     return permute_row_elements(
2126         arange(perm.shape[-1], dtype=perm.dtype),
2127         perm,
2128         inverse=True)
2129 class Dot(Op):
2130     __props__ = ()
2131     def make_node(self, *inputs):
2132         inputs = list(map(as_tensor_variable, inputs))
2133         if len(inputs) != 2:
2134             raise TypeError(
2135                 'theano.tensor.Dot: 2 arguments required, %d given ' %
2136                 len(inputs))
2137         if inputs[0].ndim not in (1, 2):
2138             raise TypeError(
2139                 'theano.tensor.Dot: input 0 (0-indexed) must have ndim of '
2140                 '1 or 2, %d given. Consider calling theano.tensor.dot '
2141                 'instead.' % inputs[0].ndim)
2142         if inputs[1].ndim not in (1, 2):
2143             raise TypeError(
2144                 'theano.tensor.Dot: input 1 (0-indexed) must have ndim of '
2145                 '1 or 2, %d given. Consider calling theano.tensor.dot '
2146                 'instead.' % inputs[1].ndim)
2147         i_broadcastables = [input.type.broadcastable for input in inputs]
2148         bx, by = i_broadcastables
2149         if len(by) == 2:  # y is a matrix
2150             bz = bx[:-1] + by[-1:]
2151         elif len(by) == 1:  # y is vector
2152             bz = bx[:-1]
2153         i_dtypes = [input.type.dtype for input in inputs]
2154         outputs = [tensor(scal.upcast(*i_dtypes), bz)]
2155         return Apply(self, inputs, outputs)
2156     def perform(self, node, inp, out):
2157         x, y = inp
2158         z, = out
2159         z[0] = np.asarray(np.dot(x, y))
2160     def grad(self, inp, grads):
2161         x, y = inp
2162         gz, = grads
2163         xdim, ydim, gdim = x.type.ndim, y.type.ndim, gz.type.ndim
2164         if gdim == 0:
2165             xgrad = gz * y
2166             ygrad = gz * x
2167         elif xdim == 1 and ydim == 2:
2168             xgrad = dot(gz, y.T)
2169             ygrad = outer(x.T, gz)
2170         elif xdim == 2 and ydim == 1:
2171             xgrad = outer(gz, y.T)
2172             ygrad = dot(x.T, gz)
2173         elif xdim == ydim == 2:
2174             xgrad = dot(gz, y.T)
2175             ygrad = dot(x.T, gz)
2176         if xgrad.broadcastable != x.broadcastable:
2177             xgrad = patternbroadcast(xgrad, x.broadcastable)
2178         if ygrad.broadcastable != y.broadcastable:
2179             ygrad = patternbroadcast(ygrad, y.broadcastable)
2180         rval = xgrad, ygrad
2181         for elem in rval:
2182             assert elem.dtype.find('float') != -1
2183         return rval
2184     def R_op(self, inputs, eval_points):
2185         assert len(inputs) == 2
2186         assert len(eval_points) == 2
2187         if eval_points[0] is None and eval_points[1] is None:
2188             return [None]
2189         if eval_points[0]:
2190             t1 = self(eval_points[0], inputs[1])
2191         if eval_points[1]:
2192             t2 = self(inputs[0], eval_points[1])
2193         if eval_points[0] and eval_points[1]:
2194             return [t1 + t2]
2195         elif eval_points[0]:
2196             return [t1]
2197         else:
2198             return [t2]
2199     def infer_shape(self, node, shapes):
2200         xshp, yshp = shapes
2201         x, y = node.inputs
2202         if x.ndim == 1 and y.ndim == 1:
2203             return [()]
2204         if x.ndim == 2 and y.ndim == 1:
2205             return [xshp[:-1]]
2206         if x.ndim == 1 and y.ndim == 2:
2207             return [yshp[-1:]]
2208         if x.ndim == 2 and y.ndim == 2:
2209             return [xshp[:-1] + yshp[-1:]]
2210         raise NotImplementedError()
2211     def __str__(self):
2212         return "dot"
2213 _dot = Dot()
2214 pprint.assign(_dot, printing.OperatorPrinter(printing.special['middle_dot'],
2215                                              -1, 'left'))
2216 def dot(a, b):
2217     a, b = as_tensor_variable(a), as_tensor_variable(b)
2218     if a.ndim == 0 or b.ndim == 0:
2219         return a * b
2220     elif a.ndim &gt; 2 or b.ndim &gt; 2:
2221         return tensordot(a, b, [[a.ndim - 1], [np.maximum(0, b.ndim - 2)]])
2222     else:
2223         return _dot(a, b)
2224 def _tensordot_as_dot(a, b, axes, dot, batched):
2225     a, b = as_tensor_variable(a), as_tensor_variable(b)
2226     if not np.isscalar(axes) and len(axes) != 2:
2227         raise ValueError('Axes should be an integer or a '
2228                          'list/tuple of len 2 (%s was provided)'
2229                          % str(axes))
2230     elif np.isscalar(axes):
2231         axes = int(axes)
2232         for operand_name, operand in (("a", a), ("b", b)):
2233             if axes &gt; operand.ndim:
2234                 raise ValueError(
2235                     'axes can not be larger than the dimension of %s '
2236                     '(%s.ndim=%i, axes=%i)'
2237                     % (operand_name, operand_name, operand.ndim, axes))
2238             if batched and axes == operand.ndim:
2239                 raise ValueError(
2240                     'axes to sum over must not include the batch axis '
2241                     'of %s (%s.ndim=%i, axes=%i)'
2242                     % (operand_name, operand_name, operand.ndim, axes))
2243         batch_axes = 1 if batched else 0
2244         a_outaxes = slice(0, a.ndim - axes)
2245         b_outaxes = slice(batch_axes + axes, b.ndim)
2246         outshape = concatenate([a.shape[a_outaxes], b.shape[b_outaxes]])
2247         outbcast = a.broadcastable[a_outaxes] + b.broadcastable[b_outaxes]
2248         outndim = len(outbcast)
2249         a_shape = [1] * 2
2250         b_shape = [1] * 2
2251         for i in xrange(0, axes):
2252             a_shape[1] *= a.shape[-(i + 1)]
2253             b_shape[0] *= b.shape[batch_axes + i]
2254         for i in xrange(0, a.ndim - axes - batch_axes):
2255             a_shape[0] *= a.shape[batch_axes + i]
2256         for i in xrange(0, b.ndim - axes - batch_axes):
2257             b_shape[1] *= b.shape[-(i + 1)]
2258         if batched:
2259             a_shape.insert(0, a.shape[0])
2260             b_shape.insert(0, b.shape[0])
2261         a_reshaped = a.reshape(a_shape)
2262         b_reshaped = b.reshape(b_shape)
2263         out_reshaped = dot(a_reshaped, b_reshaped)
2264         out = out_reshaped.reshape(outshape, outndim)
2265         return patternbroadcast(out, outbcast)
2266     else:
2267         axes = [_pack(axes_) for axes_ in axes]
2268         if len(axes[0]) != len(axes[1]):
2269             raise ValueError('Axes elements must have the same length.')
2270         for i, (operand_name, operand) in enumerate((("a", a),
2271                                                      ("b", b))):
2272             if len(axes[i]) &gt; operand.ndim:
2273                 raise ValueError(
2274                     'axes[%i] should be array_like with length less than '
2275                     'the dimensions of %s (%s.ndim=%i, len(axes[0])=%i).' %
2276                     (i, operand_name, operand_name, operand.ndim,
2277                      len(axes[i])))
2278             if len(axes[i]) &gt; 0 and np.max(axes[i]) &gt;= operand.ndim:
2279                 raise ValueError(
2280                     'axes[%i] contains dimensions greater than or equal '
2281                     'to %s.ndim (%s.ndim=%i, max(axes[0])=%i).' %
2282                     (i, operand_name, operand_name, operand.ndim,
2283                      np.max(np.array(axes[i]))))
2284             if batched and 0 in axes[i]:
2285                 raise ValueError(
2286                     'axes to sum over must not contain the batch axis '
2287                     '(axes[%i]=%s)' %
2288                     (i, axes[i]))
2289         batch_axes = [0] if batched else []
2290         other_axes = [[x for x in xrange(operand.ndim)
2291                        if x not in axes[i] and x not in batch_axes]
2292                       for i, operand in enumerate((a, b))]
2293         a_shuffled = a.dimshuffle(batch_axes + other_axes[0] + axes[0])
2294         b_shuffled = b.dimshuffle(batch_axes + axes[1] + other_axes[1])
2295         return _tensordot_as_dot(a_shuffled, b_shuffled, len(axes[0]),
2296                                  dot=dot, batched=batched)
2297 def tensordot(a, b, axes=2):
2298     return _tensordot_as_dot(a, b, axes, dot=dot, batched=False)
2299 def outer(x, y):
2300     if x.ndim != 1:
2301         x = x.flatten()
2302     if y.ndim != 1:
2303         y = y.flatten()
2304     return dot(
2305         x.dimshuffle(0, 'x'),
2306         y.dimshuffle('x', 0))
2307 def any(x, axis=None, keepdims=False):
2308     out = elemwise.Any(axis)(x)
2309     if keepdims:
2310         out = makeKeepDims(x, out, axis)
2311     return out
2312 def all(x, axis=None, keepdims=False):
2313     out = elemwise.All(axis)(x)
2314     if keepdims:
2315         out = makeKeepDims(x, out, axis)
2316     return out
2317 x = np.zeros((4, 4))
2318 numpy_diagonal_return_view = np.may_share_memory(np.diagonal(x), x)
2319 del x
2320 class ExtractDiag(Op):
2321     __props__ = ("offset", "axis1", "axis2", "view")
2322     def __init__(self, offset=0, axis1=0, axis2=1, view=False):
2323         self.view = view
2324         if self.view and not numpy_diagonal_return_view:
2325             warnings.warn("View will forced to False. ExtractDiag property view is "
2326                           "set to True but numpy version %s and prior versions of "
2327                           "numpy.diagonal() do not return a view. Update "
2328                           "numpy to use ExtractDiag(view=True)" %
2329                           np.version.version)
2330             self.view = False
2331         if self.view:
2332             self.view_map = {0: [0]}
2333         self.offset = offset
2334         self.axis1 = axis1
2335         self.axis2 = axis2
2336     def make_node(self, x):
2337         x = as_tensor_variable(x)
2338         if x.ndim &lt; 2:
2339             raise ValueError('ExtractDiag needs an input with 2 or more '
2340                              'dimensions', x)
2341         return Apply(self, [x], [x.type.__class__(
2342             dtype=x.dtype,
2343             broadcastable=[False] * (x.ndim - 1))()])
2344     def perform(self, node, inputs, outputs):
2345         (x,) = inputs
2346         (z,) = outputs
2347         z[0] = x.diagonal(self.offset, self.axis1, self.axis2)
2348         if not self.view:
2349             z[0] = z[0].copy()
2350     def grad(self, inputs, gout):
2351         (x,) = inputs
2352         (gz,) = gout
2353         if x.ndim == 2:
2354             x = theano.tensor.zeros_like(x)
2355             xdiag = theano.tensor.AllocDiag(offset=self.offset)(gz)
2356             return [theano.tensor.set_subtensor(
2357                 x[:xdiag.shape[0], :xdiag.shape[1]], xdiag)]
2358         else:
2359             warnings.warn("gradient of theano.tensor.basic.ExtractDiag only"
2360                           "works for matrices.")
2361             return [grad_not_implemented(self, 0, x)]
2362     def infer_shape(self, node, shapes):
2363         in_shape, = shapes
2364         dim1 = in_shape[self.axis1]
2365         dim2 = in_shape[self.axis2]
2366         out_shape = [d for i, d in enumerate(in_shape)
2367                      if i not in (self.axis1, self.axis2)]
2368         offset = self.offset
2369         if offset &gt; 0:
2370             diag_size = clip(dim2 - offset, 0, dim1)
2371         elif offset &lt; 0:
2372             diag_size = clip(dim1 + offset, 0, dim2)
2373         else:
2374             diag_size = minimum(dim1, dim2)
2375         out_shape.append(diag_size)
2376         return [tuple(out_shape)]
2377     def __setstate__(self, state):
2378         self.__dict__.update(state)
2379         if self.view and not numpy_diagonal_return_view:
2380             warnings.warn("View will forced to False. ExtractDiag property view is "
2381                           "set to True but numpy version %s and prior versions of "
2382                           "numpy.diagonal() do not return a view. Update "
2383                           "numpy to use ExtractDiag(view=True)" %
2384                           np.version.version)
2385             self.view = False
2386         if self.view:
2387             self.view_map = {0: [0]}
2388         if "offset" not in state:
2389             self.offset = 0
2390         if "axis1" not in state:
2391             self.axis1 = 0
2392         if "axis2" not in state:
2393             self.axis2 = 1
2394 def diagonal(a, offset=0, axis1=0, axis2=1):
2395     return ExtractDiag(offset, axis1, axis2)(a)
2396 class AllocDiag(Op):
2397     __props__ = ("offset", "axis1", "axis2")
2398     def __init__(self, offset=0, axis1=0, axis2=1):
2399         self.offset = offset
2400         self.axis1 = axis1
2401         self.axis2 = axis2
2402     def make_node(self, diag):
2403         diag = as_tensor_variable(diag)
2404         if diag.type.ndim &lt; 1:
2405             raise ValueError('AllocDiag needs an input with 1 or more '
2406                              'dimensions', diag.type)
2407         return Apply(
2408             self, [diag],
2409             [diag.type.__class__(
2410                 dtype=diag.dtype,
2411                 broadcastable=[False] * (diag.ndim + 1))()]
2412         )
2413     def perform(self, node, inputs, outputs):
2414         (x,) = inputs
2415         (z,) = outputs
2416         axis1 = np.minimum(self.axis1, self.axis2)
2417         axis2 = np.maximum(self.axis1, self.axis2)
2418         offset = self.offset
2419         result_shape = x.shape[:-1] + (x.shape[-1] + abs(offset),) * 2
2420         result = np.zeros(result_shape, dtype=x.dtype)
2421         idxs = np.arange(x.shape[-1])
2422         diagonal_slice = ((len(result_shape) - 2) * [slice(None)] +
2423                           [idxs + np.maximum(0, -offset),
2424                            idxs + np.maximum(0, offset)])
2425         result[tuple(diagonal_slice)] = x
2426         if len(x.shape) &gt; 1:
2427             axes = list(range(len(x.shape[:-1])))
2428             last_idx = axes[-1]
2429             axes = axes[:axis1] + [last_idx + 1] + axes[axis1:]
2430             axes = axes[:axis2] + [last_idx + 2] + axes[axis2:]
2431             result = result.transpose(axes)
2432         z[0] = result
2433     def grad(self, inputs, gout):
2434         (gz,) = gout
2435         return [diagonal(
2436             gz,
2437             offset=self.offset,
2438             axis1=self.axis1,
2439             axis2=self.axis2
2440         )]
2441     def infer_shape(self, nodes, shapes):
2442         (x_shape,) = shapes
2443         axis1 = np.minimum(self.axis1, self.axis2)
2444         axis2 = np.maximum(self.axis1, self.axis2)
2445         result_shape = list(x_shape[:-1])
2446         diag_shape = x_shape[-1] + abs(self.offset)
2447         result_shape = result_shape[:axis1] + [diag_shape] + result_shape[axis1:]
2448         result_shape = result_shape[:axis2] + [diag_shape] + result_shape[axis2:]
2449         return [tuple(result_shape)]
2450     def __setstate__(self, state):
2451         if "view_map" in state:
2452             del state["view_map"]
2453         self.__dict__.update(state)
2454         if "offset" not in state:
2455             self.offset = 0
2456         if "axis1" not in state:
2457             self.axis1 = 0
2458         if "axis2" not in state:
2459             self.axis2 = 1
2460 def diag(v, k=0):
2461     if v.ndim == 1:
2462         return AllocDiag(k)(v)
2463     elif v.ndim &gt;= 2:
2464         return diagonal(v, offset=k)
2465     else:
2466         raise ValueError("Input must has v.ndim &gt;= 1.")
2467 def stacklists(arg):
2468     if isinstance(arg, (tuple, list)):
2469         return stack(list(map(stacklists, arg)))
2470     else:
2471         return arg
2472 def ptp(a, axis=None):
2473     a = as_tensor_variable(a)
2474     out = max(a, axis) - min(a, axis)
2475     return out
2476 def power(x, y):
2477     return x ** y
2478 def swapaxes(y, axis1, axis2):
2479     "swap axes of inputted tensor"
2480     y = as_tensor_variable(y)
2481     ndim = y.ndim
2482     li = list(range(0, ndim))
2483     li[axis1], li[axis2] = li[axis2], li[axis1]
2484     return y.dimshuffle(li)
2485 def choose(a, choices, out=None, mode='raise'):
2486     assert out is None
2487     return Choose(mode)(a, choices)
2488 class Choose(Op):
2489     __props__ = ('mode',)
2490     def __init__(self, mode):
2491         assert mode in ("raise", "wrap", "clip")
2492         self.mode = mode
2493     def infer_shape(self, node, shapes):
2494         if isinstance(node.inputs[1], TensorVariable):
2495             l = []
2496             for sh1, sh2, b1 in zip(shapes[0],
2497                                     shapes[1][1:],
2498                                     node.inputs[0].broadcastable):
2499                 if b1:
2500                     l.append(sh2)
2501                 else:
2502                     l.append(sh1)
2503             return [tuple(l)]
2504         else:
2505             import theano.typed_list
2506             assert isinstance(node.inputs[1],
2507                               theano.typed_list.TypedListVariable)
2508             raise ShapeError("Case not implemented")
2509             shape = shapes[0]
2510             for i in xrange(len(shapes[0]) - 1):
2511                 shape[i] = shapes[1][i]
2512             return [(shape)]
2513     def make_node(self, a, choices):
2514         import theano.typed_list
2515         a = as_tensor_variable(a)
2516         if a.dtype not in theano.tensor.discrete_dtypes:
2517             raise TypeError(
2518                 'choose first argument must have an [u]int* dtype. Got %s.'
2519                 % a.dtype)
2520         if isinstance(choices, (tuple, list,
2521                                 theano.typed_list.TypedListVariable)):
2522             choice = theano.typed_list.make_list(choices)
2523             choice_ndim = choice.ttype.ndim
2524             choice_bcast = choice.ttype.broadcastable
2525         else:
2526             choice = as_tensor_variable(choices)
2527             choice_ndim = choice.ndim - 1
2528             choice_bcast = choice.broadcastable[1:]
2529         out_ndim = np.max([a.ndim, choice_ndim])
2530         a = shape_padleft(a, out_ndim - a.ndim)
2531         if len(choice_bcast) != out_ndim:
2532             if isinstance(choice.type, TensorType):
2533                 choice = choice.dimshuffle(0,
2534                                            *(('x',) * (out_ndim - choice_ndim) +
2535                                              tuple(range(1, choice.ndim))))
2536                 choice_ndim = choice.ndim - 1
2537                 choice_bcast = choice.broadcastable[1:]
2538             else:
2539                 raise NotImplementedError(
2540                     "We currently didn't implemented that case. "
2541                     "To make it work, explicitly add dimensions "
2542                     "of size one for dimensions that will be broadcasted")
2543         bcast = [False] * out_ndim
2544         for idx, (b1, b2) in enumerate(
2545             zip(a.broadcastable,
2546                 (True,) * (out_ndim - choice_ndim) + choice_bcast)):
2547             if b1 and b2:
2548                 bcast[idx] = True
2549         o = TensorType(choice.dtype, bcast)
2550         return Apply(self, [a, choice], [o()])
2551     def perform(self, node, inputs, outputs):
2552         (z,) = outputs
2553         a = inputs[0]
2554         choice = inputs[1]
2555         z[0] = np.choose(a, choice, mode=self.mode)
2556 class AllocEmpty(gof.Op):
2557     __props__ = ("dtype", )
2558     params_type = ParamsType(typecode=int32_t)
2559     def __init__(self, dtype):
2560         assert isinstance(dtype, str), dtype
2561         self.dtype = dtype.lower()
2562     @property
2563     def typecode(self):
2564         return np.dtype(self.dtype).num
2565     def make_node(self, *shape):
2566         shape, bcast = alloc_validate_shape(shape)
2567         otype = TensorType(dtype=self.dtype, broadcastable=bcast)
2568         output = otype()
2569         output.tag.values_eq_approx = values_eq_approx_always_true
2570         output.type.filter_checks_isfinite = False
2571         output.tag.nan_guard_mode_check = False
2572         return Apply(self, shape, [output])
2573     def debug_perform(self, node, inputs, out_, params):
2574         self.perform(node, inputs, out_, params)
2575         out_[0][0].fill(-123456789)
2576     def perform(self, node, inputs, out_, params):
2577         out, = out_
2578         sh = tuple([int(i) for i in inputs])
2579         if out[0] is None or out[0].shape != sh:
2580             out[0] = np.empty(sh, dtype=self.dtype)
2581     def c_code(self, node, name, inputs, out_, sub):
2582         out, = out_
2583         fail = sub['fail']
2584         shps = inputs
2585         nd = len(shps)
2586         params = sub['params']
2587         str = "npy_intp dims[%(nd)s];\n" % locals()
2588         for idx, sh in enumerate(shps):
2589             str += "dims[%(idx)s] =" \
2590                    "((npy_intp)((dtype_%(sh)s*)" \
2591                    " PyArray_DATA(%(sh)s))[0]);\n" % locals()
2592         str += "if(%(out)s==NULL\n" % locals()
2593         for idx, sh in enumerate(shps):
2594             str += "||PyArray_DIMS(%(out)s)[%(idx)s]!=dims[%(idx)s]" % locals()
2595         str += """){
2596             /* Reference received to invalid output variable.
2597             Decrease received reference's ref count and allocate new
2598             output variable */
2599             Py_XDECREF(%(out)s);
2600             %(out)s = (PyArrayObject*)PyArray_EMPTY(%(nd)s,
2601                                                     dims,
2602                                                     %(params)s-&gt;typecode,
2603                                                     0);
2604             if (!%(out)s)
2605             {
2606                 PyErr_SetString(PyExc_MemoryError, "alloc failed");
2607                 %(fail)s;
2608             }
2609         }
2610         rng_R <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= random_state_type()
2611         post_r2, out2 = rf2(rng_R, (4,), -2, 2)  # NOT INPLACE
2612         post_r4, out4 = rf4(rng_R, (4,), -4, 4)  # INPLACE
2613         post_r2_4, out2_4 = rf2(rng_R, (4, ), -4.0, 2)  # NOT INPLACE
2614         post_r2_4_4, out2_4_4 = rf2(rng_R, (4, ), -4.0, 4.0)  # NOT INPLACE
2615         f =</b></font> compile.function(
2616                 [compile.In(rng_R,
2617                             value=np.random.RandomState(utt.fetch_seed()),
2618                             update=post_r4,
2619                             mutable=True)],
2620                 [out2, out4, out2_4, out2_4_4],
2621                 accept_inplace=True)
2622         f2, f4, f2_4, f2_4_4 = f()
2623         f2b, f4b, f2_4b, f2_4_4b = f()
2624         assert np.allclose(f2 * 2, f4), (f2, f4)
2625         assert np.allclose(f2_4_4, f4), (f2_4_4, f4)
2626         assert not np.allclose(f4, f4b), (f4, f4b)
2627     def test_inplace_optimization(self):
2628         rf2 = RandomFunction(np.random.RandomState.uniform, tensor.dvector)
2629         rng_R = random_state_type()
2630         post_r2, out2 = rf2(rng_R, (4,), 0., 1.)
2631         f = compile.function(
2632                 [compile.In(rng_R,
2633                     value=np.random.RandomState(utt.fetch_seed()),
2634                     update=post_r2,
2635                     mutable=True)],
2636                 out2,
2637                 mode='FAST_RUN')  # DEBUG_MODE can't pass the id-based
2638         id0 = id(f[rng_R])
2639         val0 = f()
2640         assert id0 == id(f[rng_R])
2641         val1 = f()
2642         assert id0 == id(f[rng_R])
2643         assert not np.allclose(val0, val1)
2644     def test_no_inplace(self):
2645         rf = RandomFunction('uniform', tensor.dvector)
2646         rng_R = random_state_type()
2647         post_r, out = rf(rng_R, (3,), 0., 1.)
2648         f = compile.function([rng_R], [post_r, out])
2649         rng = np.random.RandomState(utt.fetch_seed())
2650         rng0, val0 = f(rng)
2651         rng_ = np.random.RandomState(utt.fetch_seed())
2652         self.assertTrue(rng_R.type.values_eq(rng, rng_))
2653         self.assertFalse(rng_R.type.values_eq(rng, rng0))
2654         f2 = compile.function(
2655                 [compile.In(rng_R,
2656                     value=rng,
2657                     update=post_r,
2658                     mutable=False)],
2659                 [post_r, out])
2660         rng2, val2 = f2()
2661         self.assertTrue(rng_R.type.values_eq(rng, rng_))
2662         self.assertFalse(rng_R.type.values_eq(rng, rng2))
2663         self.assertTrue(rng_R.type.values_eq(rng2, rng0))
2664         rng3, val3 = f2()
2665         self.assertTrue(rng_R.type.values_eq(rng2, rng0))
2666         self.assertFalse(rng_R.type.values_eq(rng3, rng2))
2667         self.assertFalse(rng_R.type.values_eq(rng3, rng))
2668     def test_random_function_ndim(self):
2669         rng_R = random_state_type()
2670         post_out4, out4 = uniform(rng_R, (4,))
2671         post_out1_4, out1_4 = uniform(rng_R, (4, ), ndim=1)
2672         post_out2_4_4, out2_4_4 = uniform(rng_R, (4, 4), ndim=2)
2673         self.assertRaises(ValueError, uniform, rng_R, (4,), ndim=2)
2674         f_ok = compile.function(
2675                 [compile.In(rng_R,
2676                     value=np.random.RandomState(utt.fetch_seed()),
2677                     update=post_out2_4_4,
2678                     mutable=True)],
2679                 [out4, out1_4, out2_4_4],
2680                 accept_inplace=True)
2681         o4, o1_4, o2_4_4 = f_ok()
2682         self.assertTrue(np.allclose(o4, o1_4))
2683         self.assertTrue(np.allclose(o4, o2_4_4[0]))
2684     def test_random_function_noshape_args(self):
2685         rng_R = random_state_type()
2686         post_out, out = uniform(rng_R, size=None, ndim=2)
2687         f = compile.function(
2688                 [compile.In(rng_R,
2689                     value=np.random.RandomState(utt.fetch_seed()),
2690                     update=post_out,
2691                     mutable=True)],
2692                 [out],
2693                 accept_inplace=True)
2694         o, = f()
2695         low = tensor.TensorType(dtype='float64',
2696                 broadcastable=(False, True, True))()
2697         high = tensor.TensorType(dtype='float64',
2698 <a name="2"></a>                broadcastable=(True, True, True, False))()
2699         post_out2, out2 = uniform(rng_R, size=None, ndim=2, low=low, high=high)
2700         self.assertEqual(out2.ndim, 4)
2701         self.assertEqual(out2<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.broadcastable, (True, False, True, False))
2702         g = compile.function(
2703                 [low,
2704                  high,
2705                  compile.In(rng_R,
2706                     value=np.random.RandomState(</b></font>utt.fetch_seed()),
2707                     update=post_out2,
2708                     mutable=True)],
2709                 [out2],
2710                 accept_inplace=True)
2711         low_v = [[[3]], [[4]], [[-5]]]
2712         high_v = [[[[5, 8]]]]
2713         o2, = g(low_v, high_v)
2714         self.assertEqual(o2.shape, (1, 3, 1, 2))
2715     def test_random_function_noshape_noargs(self):
2716         rng_R = random_state_type()
2717         self.assertRaises(TypeError, poisson, rng_R, size=None, ndim=2)
2718     def test_random_function_ndim_added(self):
2719         def ndim_added_deco(ndim_added):
2720             def randomfunction(random_state, size=(), low=0.0, high=0.0,
2721                                ndim=None):
2722                 ndim, size, bcast = raw_random._infer_ndim_bcast(ndim, size)
2723                 if ndim_added &lt; 0:
2724                     bcast = bcast[:ndim_added]
2725                 else:
2726                     bcast = bcast + ((False,) * ndim_added)
2727                 assert len(bcast) == ndim + ndim_added
2728                 op = RandomFunction('uniform',
2729                         tensor.TensorType(dtype='float64',
2730 <a name="4"></a>                        broadcastable=bcast),
2731                         ndim_added=ndim_added)
2732                 return op(random_state, size, low, high)
2733             r<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>eturn randomfunction
2734         uni_1 = ndim_added_deco(1)
2735         uni_0 = ndim_added_deco(0)
2736         uni_m1 = ndim_added_deco(-1)
2737         rng_R = random_state_type()
2738         p_uni11, uni11 = uni_1(rng_R, size=</b></font>(4,))
2739         p_uni12, uni12 = uni_1(rng_R, size=(3, 4))
2740         p_uni01, uni01 = uni_0(rng_R, size=(4,))
2741         p_uni02, uni02 = uni_0(rng_R, size=(3, 4))
2742         p_unim11, unim11 = uni_m1(rng_R, size=(4,))
2743         p_unim12, unim12 = uni_m1(rng_R, size=(3, 4))
2744         self.assertEqual(uni11.ndim, 2)
2745         self.assertEqual(uni12.ndim, 3)
2746         self.assertEqual(uni01.ndim, 1)
2747         self.assertEqual(uni02.ndim, 2)
2748         self.assertEqual(unim11.ndim, 0)
2749         self.assertEqual(unim12.ndim, 1)
2750         f11 = compile.function(
2751                 [compile.In(rng_R,
2752                     value=np.random.RandomState(utt.fetch_seed()),
2753                     update=p_uni11, mutable=True)],
2754                 [uni11], accept_inplace=True)
2755         f12 = compile.function(
2756                 [compile.In(rng_R,
2757                     value=np.random.RandomState(utt.fetch_seed()),
2758                     update=p_uni12, mutable=True)],
2759                 [uni12], accept_inplace=True)
2760         fm11 = compile.function(
2761                 [compile.In(rng_R,
2762                     value=np.random.RandomState(utt.fetch_seed()),
2763                     update=p_unim11, mutable=True)],
2764                 [unim11], accept_inplace=True)
2765         fm12 = compile.function(
2766                 [compile.In(rng_R,
2767                     value=np.random.RandomState(utt.fetch_seed()),
2768                     update=p_unim12, mutable=True)],
2769                 [unim12], accept_inplace=True)
2770         f0 = compile.function(
2771                 [compile.In(rng_R,
2772                     value=np.random.RandomState(utt.fetch_seed()),
2773                     update=p_uni02, mutable=True)],
2774                 [uni01, uni02], accept_inplace=True)
2775         self.assertRaises(ValueError, f11)
2776         self.assertRaises(ValueError, f12)
2777 <a name="1"></a>        self.assertRaises(ValueError, fm11)
2778         self.assertRaises(ValueError, fm12)
2779         u01, u02 = f0()
2780         self.assertTrue(np.allclose(u01, u02<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>[0]))
2781     def test_uniform(self):
2782         rng_R = random_state_type()
2783         post_r, out = uniform(rng_R, (4,), -2.0, 2.0)
2784         f = compile.function(
2785                 [compile.</b></font>In(rng_R,
2786                     value=np.random.RandomState(utt.fetch_seed()),
2787                     update=post_r, mutable=True)],
2788                 [out], accept_inplace=True)
2789         numpy_rng = np.random.RandomState(utt.fetch_seed())
2790         val0 = f()
2791         val1 = f()
2792         numpy_val0 = numpy_rng.uniform(-2.0, 2.0, size=(4,))
2793         numpy_val1 = numpy_rng.uniform(-2.0, 2.0, size=(4,))
2794         self.assertTrue(np.allclose(val0, numpy_val0))
2795         self.assertTrue(np.allclose(val1, numpy_val1))
2796     def test_binomial(self):
2797         rng_R = random_state_type()
2798         post_r, bin = binomial(rng_R, (7, 12), 5, 0.8)
2799         f = compile.function(
2800                 [compile.In(rng_R,
2801                     value=np.random.RandomState(utt.fetch_seed()),
2802                     update=post_r, mutable=True)],
2803                 [bin], accept_inplace=True)
2804         numpy_rng = np.random.RandomState(utt.fetch_seed())
2805         val0 = f()
2806         val1 = f()
2807         numpy_val0 = numpy_rng.binomial(5, 0.8, size=(7, 12))
2808         numpy_val1 = numpy_rng.binomial(5, 0.8, size=(7, 12))
2809         self.assertTrue(np.all(val0 == numpy_val0))
2810         self.assertTrue(np.all(val1 == numpy_val1))
2811     def test_normal(self):
2812         rng_R = random_state_type()
2813         post_r, out = normal(rng_R, (2, 3), 4.0, 2.0)
2814         f = compile.function(
2815                 [compile.In(rng_R,
2816                     value=np.random.RandomState(utt.fetch_seed()),
2817                     update=post_r, mutable=True)],
2818                 [out], accept_inplace=True)
2819         numpy_rng = np.random.RandomState(utt.fetch_seed())
2820         val0 = f()
2821         val1 = f()
2822         numpy_val0 = numpy_rng.normal(4.0, 2.0, size=(2, 3))
2823         numpy_val1 = numpy_rng.normal(4.0, 2.0, size=(2, 3))
2824         self.assertTrue(np.allclose(val0, numpy_val0))
2825         self.assertTrue(np.allclose(val1, numpy_val1))
2826     def test_random_integers(self):
2827         rng_R = random_state_type()
2828         post_r, out = random_integers(rng_R, (11, 8), -3, 16)
2829         f = compile.function(
2830                 [compile.In(rng_R,
2831                     value=np.random.RandomState(utt.fetch_seed()),
2832                     update=post_r, mutable=True)],
2833                 [out], accept_inplace=True)
2834         numpy_rng = np.random.RandomState(utt.fetch_seed())
2835         val0 = f()
2836         val1 = f()
2837         numpy_val0 = numpy_rng.randint(-3, 17, size=(11, 8))
2838         numpy_val1 = numpy_rng.randint(-3, 17, size=(11, 8))
2839         self.assertTrue(np.allclose(val0, numpy_val0))
2840         self.assertTrue(np.allclose(val1, numpy_val1))
2841     def test_permutation_helper(self):
2842         rf = RandomFunction(permutation_helper, tensor.imatrix, 8,
2843                             ndim_added=1)
2844         rng_R = random_state_type()
2845         post_r, out = rf(rng_R, (7,), 8)
2846         f = compile.function(
2847                 [compile.In(rng_R,
2848                     value=np.random.RandomState(utt.fetch_seed()),
2849                     update=post_r, mutable=True)],
2850                 [out], accept_inplace=True)
2851         numpy_rng = np.random.RandomState(utt.fetch_seed())
2852         val0 = f()
2853         val1 = f()
2854         numpy_val0 = np.asarray([numpy_rng.permutation(8)
2855                                     for i in range(7)])
2856         numpy_val1 = np.asarray([numpy_rng.permutation(8)
2857                                     for i in range(7)])
2858         self.assertTrue(np.all(val0 == numpy_val0))
2859         self.assertTrue(np.all(val1 == numpy_val1))
2860         rf0 = RandomFunction(permutation_helper, tensor.imatrix, 8)
2861         post_r0, out0 = rf0(rng_R, (7,), 8)
2862         f0 = compile.function(
2863                 [compile.In(rng_R,
2864                     value=np.random.RandomState(utt.fetch_seed()),
2865                     update=post_r0, mutable=True)],
2866                 [out0], accept_inplace=True)
2867         self.assertRaises(ValueError, f0)
2868         rf2 = RandomFunction(permutation_helper, tensor.imatrix, 8,
2869                              ndim_added=2)
2870         post_r2, out2 = rf2(rng_R, (7,), 8)
2871         f2 = compile.function(
2872                 [compile.In(rng_R,
2873                     value=np.random.RandomState(utt.fetch_seed()),
2874                     update=post_r2, mutable=True)],
2875                 [out2], accept_inplace=True)
2876         self.assertRaises(ValueError, f2)
2877     def test_choice(self):
2878         rng_R = random_state_type()
2879         post_r, out = choice(rng_R, (11, 8), 10, 1, 0)
2880         f = compile.function(
2881                 [compile.In(rng_R,
2882                     value=np.random.RandomState(utt.fetch_seed()),
2883                     update=post_r, mutable=True)],
2884                 [out], accept_inplace=True)
2885         numpy_rng = np.random.RandomState(utt.fetch_seed())
2886         val0 = f()
2887         val1 = f()
2888         numpy_val0 = numpy_rng.choice(10, (11, 8), True, None)
2889         numpy_val1 = numpy_rng.choice(10, (11, 8), True, None)
2890         self.assertTrue(np.allclose(val0, numpy_val0))
2891         self.assertTrue(np.allclose(val1, numpy_val1))
2892     def test_poisson(self):
2893         rng_R = random_state_type()
2894         post_r, out = poisson(rng_R, lam=5, size=(11, 8))
2895         f = compile.function(
2896                 [compile.In(rng_R,
2897                     value=np.random.RandomState(utt.fetch_seed()),
2898                     update=post_r, mutable=True)],
2899                 [out], accept_inplace=True)
2900         numpy_rng = np.random.RandomState(utt.fetch_seed())
2901         val0 = f()
2902         val1 = f()
2903         numpy_val0 = numpy_rng.poisson(5, size=(11, 8))
2904         numpy_val1 = numpy_rng.poisson(5, size=(11, 8))
2905         self.assertTrue(np.allclose(val0, numpy_val0))
2906         self.assertTrue(np.allclose(val1, numpy_val1))
2907     def test_permutation(self):
2908         rng_R = random_state_type()
2909         post_r, out = permutation(rng_R, size=(9,), n=6)
2910         f = compile.function(
2911                 [compile.In(rng_R,
2912                     value=np.random.RandomState(utt.fetch_seed()),
2913                     update=post_r, mutable=True)],
2914                 [out], accept_inplace=True)
2915         numpy_rng = np.random.RandomState(utt.fetch_seed())
2916         val0 = f()
2917         val1 = f()
2918         numpy_val0 = np.asarray([numpy_rng.permutation(6)
2919                                     for i in range(9)])
2920         numpy_val1 = np.asarray([numpy_rng.permutation(6)
2921                                     for i in range(9)])
2922         self.assertTrue(np.all(val0 == numpy_val0))
2923         self.assertTrue(np.all(val1 == numpy_val1))
2924         for ndim in [1, None]:
2925             post_r, out = permutation(rng_R, n=10, size=None, ndim=ndim)
2926             inp = compile.In(rng_R,
2927                              value=np.random.RandomState(utt.fetch_seed()),
2928                              update=post_r, mutable=True)
2929             f = theano.function([inp], out)
2930             o = f()
2931             assert o.shape == (10,)
2932             assert (np.sort(o) == np.arange(10)).all()
2933         self.assertRaises(TypeError, permutation, rng_R, size=None, ndim=2)
2934     def test_multinomial(self):
2935         rng_R = random_state_type()
2936         post_r, out = multinomial(rng_R, (7, 3), 6, [0.2] * 5)
2937         f = compile.function(
2938                 [compile.In(rng_R,
2939                     value=np.random.RandomState(utt.fetch_seed()),
2940                     update=post_r, mutable=True)],
2941                 [out], accept_inplace=True)
2942         numpy_rng = np.random.RandomState(utt.fetch_seed())
2943         val0, = f()
2944         val1, = f()
2945         numpy_val0 = numpy_rng.multinomial(6, [0.2] * 5, (7, 3))
2946         numpy_val1 = numpy_rng.multinomial(6, [0.2] * 5, (7, 3))
2947         self.assertTrue(np.all(val0 == numpy_val0))
2948         self.assertTrue(np.all(val1 == numpy_val1))
2949         self.assertTrue(val0.shape == (7, 3, 5))
2950         self.assertTrue(val1.shape == (7, 3, 5))
2951     def test_symbolic_shape(self):
2952         rng_R = random_state_type()
2953         shape = tensor.lvector()
2954         post_r, out = uniform(rng_R, shape, ndim=2)
2955         f = compile.function([rng_R, shape], out)
2956         rng_state0 = np.random.RandomState(utt.fetch_seed())
2957         assert f(rng_state0, [2, 3]).shape == (2, 3)
2958         assert f(rng_state0, [4, 8]).shape == (4, 8)
2959         self.assertRaises(ValueError, f, rng_state0, [4])
2960         self.assertRaises(ValueError, f, rng_state0, [4, 3, 4, 5])
2961     def test_mixed_shape(self):
2962         rng_R = random_state_type()
2963         shape0 = tensor.lscalar()
2964         shape = (shape0, 3)
2965         post_r, u = uniform(rng_R, size=shape, ndim=2)
2966         f = compile.function([rng_R, shape0], u)
2967         rng_state0 = np.random.RandomState(utt.fetch_seed())
2968         assert f(rng_state0, 2).shape == (2, 3)
2969         assert f(rng_state0, 8).shape == (8, 3)
2970         post_r, v = uniform(rng_R, size=shape)
2971         g = compile.function([rng_R, shape0], v)
2972         assert g(rng_state0, 2).shape == (2, 3)
2973         assert g(rng_state0, 8).shape == (8, 3)
2974     def test_mixed_shape_bcastable(self):
2975         rng_R = random_state_type()
2976         shape0 = tensor.lscalar()
2977         shape = (shape0, 1)
2978         post_r, u = uniform(rng_R, size=shape, ndim=2)
2979         assert u.broadcastable == (False, True)
2980         f = compile.function([rng_R, shape0], u)
2981         rng_state0 = np.random.RandomState(utt.fetch_seed())
2982         assert f(rng_state0, 2).shape == (2, 1)
2983         assert f(rng_state0, 8).shape == (8, 1)
2984         post_r, v = uniform(rng_R, size=shape)
2985         assert v.broadcastable == (False, True)
2986         g = compile.function([rng_R, shape0], v)
2987         assert g(rng_state0, 2).shape == (2, 1)
2988         assert g(rng_state0, 8).shape == (8, 1)
2989     def test_default_shape(self):
2990         rng_R = random_state_type()
2991         post_r, out = uniform(rng_R)
2992         f = compile.function([rng_R], [post_r, out], accept_inplace=True)
2993         rng_state0 = np.random.RandomState(utt.fetch_seed())
2994         numpy_rng = np.random.RandomState(utt.fetch_seed())
2995         post0, val0 = f(rng_state0)
2996         post1, val1 = f(post0)
2997         numpy_val0 = np.asarray(numpy_rng.uniform(),
2998                                    dtype=theano.config.floatX)
2999         numpy_val1 = np.asarray(numpy_rng.uniform(),
3000                                    dtype=theano.config.floatX)
3001         assert np.all(val0 == numpy_val0)
3002 <a name="3"></a>        assert np.all(val1 == numpy_val1)
3003         post_r, out = multinomial(rng_R)
3004         g = compile<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.function([rng_R], [post_r, out], accept_inplace=True)
3005         post2, val2 = g(post1)
3006         numpy_val2 = np.asarray(numpy_rng.multinomial(n=</b></font>1, pvals=[.5, .5]),
3007                 dtype=theano.config.floatX)
3008         assert np.all(val2 == numpy_val2)
3009     def test_vector_arguments(self):
3010         rng_R = random_state_type()
3011         low = tensor.vector()
3012         post_r, out = uniform(rng_R, low=low, high=1)
3013         assert out.ndim == 1
3014         f = compile.function([rng_R, low], [post_r, out], accept_inplace=True)
3015         def as_floatX(thing):
3016             return np.asarray(thing, dtype=theano.config.floatX)
3017         rng_state0 = np.random.RandomState(utt.fetch_seed())
3018         numpy_rng = np.random.RandomState(utt.fetch_seed())
3019         post0, val0 = f(rng_state0, [-5, .5, 0, 1])
3020         post1, val1 = f(post0, as_floatX([.9]))
3021         numpy_val0 = as_floatX(numpy_rng.uniform(low=[-5, .5, 0, 1], high=1))
3022         numpy_val1 = as_floatX(numpy_rng.uniform(low=as_floatX([.9]), high=1))
3023         assert np.all(val0 == numpy_val0)
3024         assert np.all(val1 == numpy_val1)
3025         high = tensor.vector()
3026         post_rb, outb = uniform(rng_R, low=low, high=high)
3027         assert outb.ndim == 1
3028         fb = compile.function([rng_R, low, high], [post_rb, outb],
3029                               accept_inplace=True)
3030         post0b, val0b = fb(post1, [-4., -2], [-1, 0])
3031         post1b, val1b = fb(post0b, [-4.], [-1])
3032         numpy_val0b = as_floatX(numpy_rng.uniform(low=[-4., -2], high=[-1, 0]))
3033         numpy_val1b = as_floatX(numpy_rng.uniform(low=[-4.], high=[-1]))
3034         assert np.all(val0b == numpy_val0b)
3035         assert np.all(val1b == numpy_val1b)
3036         self.assertRaises(ValueError, fb, post1b, [-4., -2], [-1, 0, 1])
3037         size = tensor.lvector()
3038         post_rc, outc = uniform(rng_R, low=low, high=high, size=size, ndim=1)
3039         fc = compile.function([rng_R, low, high, size], [post_rc, outc],
3040                               accept_inplace=True)
3041         post0c, val0c = fc(post1b, [-4., -2], [-1, 0], [2])
3042         post1c, val1c = fc(post0c, [-4.], [-1], [1])
3043         numpy_val0c = as_floatX(numpy_rng.uniform(low=[-4., -2], high=[-1, 0]))
3044         numpy_val1c = as_floatX(numpy_rng.uniform(low=[-4.], high=[-1]))
3045         assert np.all(val0c == numpy_val0c)
3046         assert np.all(val1c == numpy_val1c)
3047         self.assertRaises(ValueError, fc, post1c, [-4., -2], [-1, 0], [1, 2])
3048         self.assertRaises(ValueError, fc, post1c, [-4., -2], [-1, 0], [2, 1])
3049     def test_broadcast_arguments(self):
3050         rng_R = random_state_type()
3051         low = tensor.dvector()
3052         high = tensor.dcol()
3053         post_r, out = uniform(rng_R, low=low, high=high)
3054         assert out.ndim == 2
3055         f = compile.function([rng_R, low, high], [post_r, out],
3056                              accept_inplace=True)
3057         rng_state0 = np.random.RandomState(utt.fetch_seed())
3058         numpy_rng = np.random.RandomState(utt.fetch_seed())
3059         post0, val0 = f(rng_state0, [-5, .5, 0, 1], [[1.]])
3060         post1, val1 = f(post0, [.9], [[1.], [1.1], [1.5]])
3061         post2, val2 = f(post1, [-5, .5, 0, 1], [[1.], [1.1], [1.5]])
3062         numpy_val0 = numpy_rng.uniform(low=[-5, .5, 0, 1], high=[1.])
3063         numpy_val1 = numpy_rng.uniform(low=[.9], high=[[1.], [1.1], [1.5]])
3064         numpy_val2 = numpy_rng.uniform(low=[-5, .5, 0, 1],
3065                                        high=[[1.], [1.1], [1.5]])
3066         assert np.all(val0 == numpy_val0), (val0, numpy_val0)
3067         assert np.all(val1 == numpy_val1)
3068         assert np.all(val2 == numpy_val2)
3069     def test_uniform_vector(self):
3070         rng_R = random_state_type()
3071         low = tensor.vector()
3072         high = tensor.vector()
3073         post_r, out = uniform(rng_R, low=low, high=high)
3074         assert out.ndim == 1
3075         f = compile.function([rng_R, low, high], [post_r, out],
3076                              accept_inplace=True)
3077         def as_floatX(thing):
3078             return np.asarray(thing, dtype=theano.config.floatX)
3079         low_val = as_floatX([.1, .2, .3])
3080         high_val = as_floatX([1.1, 2.2, 3.3])
3081         rng = np.random.RandomState(utt.fetch_seed())
3082         numpy_rng = np.random.RandomState(utt.fetch_seed())
3083         rng0, val0 = f(rng, low_val, high_val)
3084         numpy_val0 = as_floatX(numpy_rng.uniform(low=low_val, high=high_val))
3085         assert np.all(val0 == numpy_val0)
3086         rng1, val1 = f(rng0, low_val[:-1], high_val[:-1])
3087         numpy_val1 = as_floatX(numpy_rng.uniform(low=low_val[:-1],
3088                                                  high=high_val[:-1]))
3089         assert np.all(val1 == numpy_val1)
3090         g = compile.function([rng_R, low, high],
3091                 uniform(rng_R, low=low, high=high, size=(3,)),
3092                 accept_inplace=True)
3093         rng2, val2 = g(rng1, low_val, high_val)
3094         numpy_val2 = as_floatX(numpy_rng.uniform(low=low_val, high=high_val,
3095                                                  size=(3,)))
3096         assert np.all(val2 == numpy_val2)
3097         self.assertRaises(ValueError, g, rng2, low_val[:-1], high_val[:-1])
3098     def test_binomial_vector(self):
3099         rng_R = random_state_type()
3100         n = tensor.lvector()
3101         prob = tensor.vector()
3102         post_r, out = binomial(rng_R, n=n, p=prob)
3103         assert out.ndim == 1
3104         f = compile.function([rng_R, n, prob], [post_r, out],
3105                              accept_inplace=True)
3106         n_val = [1, 2, 3]
3107         prob_val = np.asarray([.1, .2, .3], dtype=config.floatX)
3108         rng = np.random.RandomState(utt.fetch_seed())
3109         numpy_rng = np.random.RandomState(utt.fetch_seed())
3110         rng0, val0 = f(rng, n_val, prob_val)
3111         numpy_val0 = numpy_rng.binomial(n=n_val, p=prob_val)
3112         assert np.all(val0 == numpy_val0)
3113         rng1, val1 = f(rng0, n_val[:-1], prob_val[:-1])
3114         numpy_val1 = numpy_rng.binomial(n=n_val[:-1], p=prob_val[:-1])
3115         assert np.all(val1 == numpy_val1)
3116         g = compile.function([rng_R, n, prob],
3117                 binomial(rng_R, n=n, p=prob, size=(3,)),
3118                 accept_inplace=True)
3119         rng2, val2 = g(rng1, n_val, prob_val)
3120         numpy_val2 = numpy_rng.binomial(n=n_val, p=prob_val, size=(3,))
3121         assert np.all(val2 == numpy_val2)
3122         self.assertRaises(ValueError, g, rng2, n_val[:-1], prob_val[:-1])
3123     def test_normal_vector(self):
3124         rng_R = random_state_type()
3125         avg = tensor.vector()
3126         std = tensor.vector()
3127         post_r, out = normal(rng_R, avg=avg, std=std)
3128         assert out.ndim == 1
3129         f = compile.function([rng_R, avg, std], [post_r, out],
3130                              accept_inplace=True)
3131         def as_floatX(thing):
3132             return np.asarray(thing, dtype=theano.config.floatX)
3133         avg_val = [1, 2, 3]
3134         std_val = as_floatX([.1, .2, .3])
3135         rng = np.random.RandomState(utt.fetch_seed())
3136         numpy_rng = np.random.RandomState(utt.fetch_seed())
3137         rng0, val0 = f(rng, avg_val, std_val)
3138         numpy_val0 = as_floatX(numpy_rng.normal(loc=as_floatX(avg_val),
3139             scale=as_floatX(std_val)))
3140         assert np.all(val0 == numpy_val0)
3141         rng1, val1 = f(rng0, avg_val[:-1], std_val[:-1])
3142         numpy_val1 = np.asarray(numpy_rng.normal(loc=avg_val[:-1],
3143                                                     scale=std_val[:-1]),
3144                 dtype=theano.config.floatX)
3145         assert np.all(val1 == numpy_val1)
3146         g = compile.function([rng_R, avg, std],
3147                 normal(rng_R, avg=avg, std=std, size=(3,)),
3148                 accept_inplace=True)
3149         rng2, val2 = g(rng1, avg_val, std_val)
3150         numpy_val2 = np.asarray(numpy_rng.normal(loc=avg_val, scale=std_val,
3151                                                     size=(3,)),
3152                 dtype=theano.config.floatX)
3153         assert np.all(val2 == numpy_val2)
3154         self.assertRaises(ValueError, g, rng2, avg_val[:-1], std_val[:-1])
3155     def test_random_integers_vector(self):
3156         rng_R = random_state_type()
3157         low = tensor.lvector()
3158         high = tensor.lvector()
3159         post_r, out = random_integers(rng_R, low=low, high=high)
3160         assert out.ndim == 1
3161         f = compile.function([rng_R, low, high], [post_r, out],
3162                              accept_inplace=True)
3163         low_val = [100, 200, 300]
3164         high_val = [110, 220, 330]
3165         rng = np.random.RandomState(utt.fetch_seed())
3166         numpy_rng = np.random.RandomState(utt.fetch_seed())
3167         rng0, val0 = f(rng, low_val, high_val)
3168         numpy_val0 = np.asarray([numpy_rng.randint(low=lv, high=hv+1)
3169             for lv, hv in zip(low_val, high_val)])
3170         assert np.all(val0 == numpy_val0)
3171         rng1, val1 = f(rng0, low_val[:-1], high_val[:-1])
3172         numpy_val1 = np.asarray([numpy_rng.randint(low=lv, high=hv+1)
3173             for lv, hv in zip(low_val[:-1], high_val[:-1])])
3174         assert np.all(val1 == numpy_val1)
3175         g = compile.function([rng_R, low, high],
3176                 random_integers(rng_R, low=low, high=high, size=(3,)),
3177                 accept_inplace=True)
3178         rng2, val2 = g(rng1, low_val, high_val)
3179         numpy_val2 = np.asarray([numpy_rng.randint(low=lv, high=hv+1)
3180             for lv, hv in zip(low_val, high_val)])
3181         assert np.all(val2 == numpy_val2)
3182         self.assertRaises(ValueError, g, rng2, low_val[:-1], high_val[:-1])
3183     def test_multinomial_vector(self):
3184         rng_R = random_state_type()
3185         n = tensor.lvector()
3186         pvals = tensor.matrix()
3187         post_r, out = multinomial(rng_R, n=n, pvals=pvals)
3188         assert out.ndim == 2
3189         f = compile.function([rng_R, n, pvals], [post_r, out],
3190                              accept_inplace=True)
3191         n_val = [1, 2, 3]
3192         pvals_val = [[.1, .9], [.2, .8], [.3, .7]]
3193         pvals_val = np.asarray(pvals_val, dtype=config.floatX)
3194         rng = np.random.RandomState(utt.fetch_seed())
3195         numpy_rng = np.random.RandomState(utt.fetch_seed())
3196         rng0, val0 = f(rng, n_val, pvals_val)
3197         numpy_val0 = np.asarray([numpy_rng.multinomial(n=nv, pvals=pv)
3198             for nv, pv in zip(n_val, pvals_val)])
3199         assert np.all(val0 == numpy_val0)
3200         rng1, val1 = f(rng0, n_val[:-1], pvals_val[:-1])
3201         numpy_val1 = np.asarray([numpy_rng.multinomial(n=nv, pvals=pv)
3202             for nv, pv in zip(n_val[:-1], pvals_val[:-1])])
3203         assert np.all(val1 == numpy_val1)
3204         g = compile.function([rng_R, n, pvals],
3205                 multinomial(rng_R, n=n, pvals=pvals, size=(3,)),
3206                 accept_inplace=True)
3207         rng2, val2 = g(rng1, n_val, pvals_val)
3208         numpy_val2 = np.asarray([numpy_rng.multinomial(n=nv, pvals=pv)
3209             for nv, pv in zip(n_val, pvals_val)])
3210         assert np.all(val2 == numpy_val2)
3211         self.assertRaises(ValueError, g, rng2, n_val[:-1], pvals_val[:-1])
3212     def test_multinomial_tensor3_a(self):
3213         rng_R = random_state_type()
3214         n = 9
3215         pvals = tensor.dtensor3()
3216         post_r, out = multinomial(rng_R, n=n, pvals=pvals, size=(1, -1))
3217         assert out.ndim == 3
3218         assert out.broadcastable == (True, False, False)
3219         f = compile.function([rng_R, pvals], [post_r, out],
3220                              accept_inplace=True)
3221         rng = np.random.RandomState(utt.fetch_seed())
3222         numpy_rng = np.random.RandomState(utt.fetch_seed())
3223         pvals_val = np.asarray([[[.1, .9], [.2, .8], [.3, .7]]])
3224         assert pvals_val.shape == (1, 3, 2)
3225         new_rng, draw = f(rng, pvals_val)
3226         assert draw.shape == (1, 3, 2)
3227         assert np.allclose(draw.sum(axis=2), 9)
3228     def test_multinomial_tensor3_b(self):
3229         rng_R = random_state_type()
3230         n = 9
3231         pvals = tensor.dtensor3()
3232         post_r, out = multinomial(rng_R, n=n, pvals=pvals, size=(10, 1, -1))
3233         assert out.ndim == 4
3234         assert out.broadcastable == (False, True, False, False)
3235         f = compile.function([rng_R, pvals], [post_r, out],
3236                              accept_inplace=True)
3237         rng = np.random.RandomState(utt.fetch_seed())
3238         numpy_rng = np.random.RandomState(utt.fetch_seed())
3239         pvals_val = np.asarray([[[.1, .9], [.2, .8], [.3, .7]]])
3240         assert pvals_val.shape == (1, 3, 2)
3241         out_rng, draw = f(rng, pvals_val)
3242         assert draw.shape == (10, 1, 3, 2)
3243         assert np.allclose(draw.sum(axis=3), 9)
3244     def test_dtype(self):
3245         rng_R = random_state_type()
3246         low = tensor.lscalar()
3247         high = tensor.lscalar()
3248         post_r, out = random_integers(rng_R, low=low, high=high, size=(20, ),
3249                                       dtype='int8')
3250         assert out.dtype == 'int8'
3251         f = compile.function([rng_R, low, high], [post_r, out])
3252         rng = np.random.RandomState(utt.fetch_seed())
3253         rng0, val0 = f(rng, 0, 9)
3254         assert val0.dtype == 'int8'
3255         rng1, val1 = f(rng0, 255, 257)
3256         assert val1.dtype == 'int8'
3257         assert np.all(abs(val1) &lt;= 1)
3258     def test_dtype_normal_uniform_687(self):
3259         rng_R = random_state_type()
3260         assert uniform(rng_R, low=tensor.constant(0, dtype='float64'),
3261                        dtype='float32')[1].dtype == 'float32'
3262         assert normal(rng_R, avg=tensor.constant(0, dtype='float64'),
3263                       dtype='float32')[1].dtype == 'float32'
3264     def setUp(self):
3265         super(T_random_function, self).setUp()
3266     def test_infer_shape(self):
3267         rng_R = random_state_type()
3268         rng_R_val = np.random.RandomState(utt.fetch_seed())
3269         post_r, out = uniform(rng_R)
3270         self._compile_and_check([rng_R], [out], [rng_R_val],
3271                              RandomFunction)
3272         post_r, out = uniform(rng_R, size=None, ndim=2)
3273         self._compile_and_check([rng_R], [out], [rng_R_val],
3274                                 RandomFunction)
3275         low = tensor.TensorType(dtype='float64',
3276                 broadcastable=(False, True, True))()
3277         high = tensor.TensorType(dtype='float64',
3278                 broadcastable=(True, True, True, False))()
3279         post_r, out = uniform(rng_R, size=None, ndim=2, low=low, high=high)
3280         low_val = [[[3]], [[4]], [[-5]]]
3281         high_val = [[[[5, 8]]]]
3282         self._compile_and_check([rng_R, low, high], [out],
3283                                 [rng_R_val, low_val, high_val],
3284                                 RandomFunction)
3285         low = dvector()
3286         high = dvector()
3287         post_r, out = uniform(rng_R, low=low, high=1)
3288         low_val = [-5, .5, 0, 1]
3289         self._compile_and_check([rng_R, low], [out], [rng_R_val, low_val],
3290                           RandomFunction)
3291         low_val = [.9]
3292         self._compile_and_check([rng_R, low], [out], [rng_R_val, low_val],
3293                           RandomFunction)
3294         post_r, out = uniform(rng_R, low=low, high=high)
3295         low_val = [-4., -2]
3296         high_val = [-1, 0]
3297         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
3298                                 high_val], RandomFunction)
3299         low_val = [-4.]
3300         high_val = [-1]
3301         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
3302                                 high_val], RandomFunction)
3303         low = dvector()
3304         high = dcol()
3305         post_r, out = uniform(rng_R, low=low, high=high)
3306         low_val = [-5, .5, 0, 1]
3307         high_val = [[1.]]
3308         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
3309                                 high_val], RandomFunction)
3310         low_val = [.9]
3311         high_val = [[1.], [1.1], [1.5]]
3312         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
3313                                 high_val], RandomFunction)
3314         low_val = [-5, .5, 0, 1]
3315         high_val = [[1.], [1.1], [1.5]]
3316         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
3317                                 high_val], RandomFunction)
3318         low = dvector()
3319         high = dvector()
3320         post_r, out = uniform(rng_R, low=low, high=high)
3321         low_val = [.1, .2, .3]
3322         high_val = [1.1, 2.2, 3.3]
3323         size_val = (3, )
3324         self._compile_and_check([rng_R, low, high], [out],
3325                                 [rng_R_val, low_val[:-1],
3326                                 high_val[:-1]], RandomFunction)
3327         post_r, out = uniform(rng_R, size=size_val, low=low, high=high)
3328         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
3329                                 high_val], RandomFunction)
3330         n = ivector()
3331         prob = dvector()
3332         post_r, out = binomial(rng_R, n=n, p=prob)
3333         n_val = [1, 2, 3]
3334         prob_val = [.1, .2, .3]
3335         size_val = (3, )
3336         self._compile_and_check([rng_R, n, prob], [out],
3337                                 [rng_R_val, n_val[:-1],
3338                                 prob_val[:-1]], RandomFunction)
3339         post_r, out = binomial(rng_R, n=n, p=prob, size=size_val)
3340         self._compile_and_check([rng_R, n, prob], [out], [rng_R_val, n_val,
3341                                 prob_val], RandomFunction)
3342         avg = dvector()
3343         std = dvector()
3344         post_r, out = normal(rng_R, avg=avg, std=std)
3345         avg_val = [1, 2, 3]
3346         std_val = [.1, .2, .3]
3347         size_val = (3, )
3348         self._compile_and_check([rng_R, avg, std], [out],
3349                                 [rng_R_val, avg_val[:-1],
3350                                 std_val[:-1]], RandomFunction)
3351         post_r, out = normal(rng_R, avg=avg, std=std, size=size_val)
3352         self._compile_and_check([rng_R, avg, std], [out], [rng_R_val, avg_val,
3353                                 std_val], RandomFunction)
3354     def test_pkl(self):
3355         rng_r = random_state_type()
3356         mode = None
3357         if theano.config.mode in ["DEBUG_MODE", "DebugMode"]:
3358             mode = 'FAST_COMPILE'
3359         post_bin_r, bin_sample = binomial(rng_r, (3, 5), 1, .3)
3360         f = theano.function([rng_r], [post_bin_r, bin_sample], mode=mode)
3361         pkl_f = pickle.dumps(f)
3362         post_int_r, int_sample = random_integers(rng_r, (3, 5), -1, 8)
3363         g = theano.function([rng_r], [post_int_r, int_sample], mode=mode)
3364         pkl_g = pickle.dumps(g)
3365         pickle.loads(pkl_g)
3366 if __name__ == '__main__':
3367     from theano.tests import main
3368     main("test_raw_random")
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
