<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for basic_3.py &amp; test_raw_random.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for basic_3.py &amp; test_raw_random.py
      </h3>
<h1 align="center">
        1.2%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>basic_3.py (0.8382449%)<th>test_raw_random.py (2.5733817%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(790-796)<td><a href="#" name="0">(54-65)</a><td align="center"><font color="#ff0000">15</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1290-1298)<td><a href="#" name="1">(312-322)</a><td align="center"><font color="#dd0000">13</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(5674-5677)<td><a href="#" name="2">(215-221)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(5466-5470)<td><a href="#" name="3">(628-630)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(661-668)<td><a href="#" name="4">(260-268)</a><td align="center"><font color="#cc0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>basic_3.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 from six.moves import builtins
3 import sys
4 import warnings
5 import numpy as np
6 from six import integer_types
7 from six.moves import xrange
8 import numbers
9 import theano
10 from theano.compat import izip
11 from theano import config
12 from theano import gof
13 from theano.gof import Apply, Constant, Op, Variable, ParamsType
14 from theano.gof.type import Generic
15 from theano.scalar import int32 as int32_t
16 from theano.tensor import elemwise
17 from theano.tensor.var import (AsTensorError, TensorVariable,
18                                TensorConstant, TensorConstantSignature,
19                                _tensor_py_operators)
20 from theano.tensor.type import TensorType, values_eq_approx_always_true
21 from theano.tensor.type_other import NoneConst
22 from theano import scalar as scal
23 from functools import partial
24 from theano import compile, printing
25 from theano.printing import pprint, min_informative_str
26 from theano.compile import Rebroadcast, Shape, shape
27 from theano.scalar import int32
28 import theano.scalar.sharedvar
29 from theano.gradient import grad_undefined
30 from theano.gradient import grad_not_implemented
31 from theano.gradient import DisconnectedType
32 from theano.tensor.elemwise import Elemwise, DimShuffle, CAReduce, Sum
33 import logging
34 _logger = logging.getLogger("theano.tensor.basic")
35 __docformat__ = "restructuredtext en"
36 python_complex = complex
37 python_any = any
38 python_all = all
39 complex_dtypes = list(map(str, scal.complex_types))
40 continuous_dtypes = list(map(str, scal.continuous_types))
41 float_dtypes = list(map(str, scal.float_types))
42 integer_dtypes = list(map(str, scal.integer_types))
43 discrete_dtypes = list(map(str, scal.discrete_types))
44 all_dtypes = list(map(str, scal.all_types))
45 int_dtypes = list(map(str, scal.int_types))
46 uint_dtypes = list(map(str, scal.uint_types))
47 class ShapeError(Exception):
48     pass
49 def check_equal_numpy(x, y):
50     if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):
51         return (x.dtype == y.dtype and x.shape == y.shape and
52                 np.all(abs(x - y) &lt; 1e-10))
53     elif (isinstance(x, np.random.RandomState) and
54           isinstance(y, np.random.RandomState)):
55         return python_all(np.all(a == b) for a, b in
56                           izip(x.__getstate__(), y.__getstate__()))
57     else:
58         return x == y
59 compile.register_checker(check_equal_numpy)
60 __oplist_constructor_list = []
61     Make `f` appear as a constructor in the oplist (`gen_oplist`,
62     doc/oplist.txt).
63     This function is often used by `make_node` methods of `Op` subclasses
64     to turn ndarrays, numbers, `Scalar` instances, `Apply` instances and
65     `TensorType` instances into valid input list elements.
66     Parameters
67     ----------
68     x : Apply instance, Variable instance, numpy.ndarray, or number
69         This thing will be transformed into a `Variable` in a sensible way. An
70         ndarray argument will not be copied, but a list of numbers will be
71         copied to make an ndarray.
72     name : str or None
73         If a new `Variable` instance is created, it will be named with this
74         string.
75     ndim : None or integer
76         Return a Variable with this many dimensions.
77     Raises
78     ------
79     ValueError
80         If an `Apply` with more than one output is fetched or
81         if `x` cannot be made into a Variable with `ndim` dimensions.
82     AsTensorError
83         If `x` cannot be converted to a TensorType Variable.
84     Raises
85     ------
86     TypeError
87         `x` could not be converted to a numpy.ndarray.
88     ValueError
89         `x` could not be expanded to have ndim dimensions.
90     Note
91     ----
92     We create a small cache of frequently used constant.
93     This speed up the Merge optimization for big graph.
94     We want to cache all scalar to don't merge as frequently constants.
95     But we don't want to cache too much stuff.
96     So we cache integer with dtype [u]int and float where the value is
97     between -10 and 10.
98     We cache all broadcast pattern for scalar.
99     Raised by get_scalar_constant_value if called on something that is
100     not a scalar constant.
101     Raised by get_scalar_const_value if called on something that is a
102     zero dimensional constant.
103     Raises
104     ------
105      NotScalarConstantError
106         If the numpy ndarray is not a scalar.
107     If `v` is the output of dimshuffles, fills, allocs, rebroadcasts,
108     cast, OutputGuard, DeepCopyOp, ScalarFromTensor, ScalarOp, Elemwise
109     and some pattern with Subtensor, this function digs through them.
110     If `v` is not some view of constant scalar data, then raise a
111     NotScalarConstantError.
112     Parameters
113     ----------
114     elemwise : bool
115         If False, we won't try to go into elemwise. So this call is faster.
116         But we still investigate in Second Elemwise (as this is a substitute
117         for Alloc)
118     only_process_constants : bool
119         If True, we only attempt to obtain the value of `orig_v` if it's
120         directly constant and don't try to dig through dimshuffles, fills,
121         allocs, and other to figure out its value.
122     max_recur : int
123         The maximum number of recursion.
124     Notes
125     -----
126         There may be another function similar to this one in the code,
127         but I'm not sure where it is.
128     """
129         return [partial(f<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>2, f) for f in fns]
130 cscalar = TensorType('complex64', ())
131 zscalar = TensorType('complex128', ())
132 fscalar = TensorType('float32', ())
133 dscalar = TensorType('float64', ())
134 bscalar = TensorType('int8', ())
135 wscalar =</b></font> TensorType('int16', ())
136 iscalar = TensorType('int32', ())
137 lscalar = TensorType('int64', ())
138 def scalar(name=None, dtype=None):
139     """Return a symbolic scalar variable.
140     Parameters
141     ----------
142     dtype: numeric
143         None means to use theano.config.floatX.
144     name
145         A name to attach to this variable.
146     """
147     if dtype is None:
148         dtype = config.floatX
149     type = TensorType(dtype, ())
150     return type(name)
151 scalars, fscalars, dscalars, iscalars, lscalars = _multi(
152     scalar, fscalar, dscalar, iscalar, lscalar)
153 int_types = bscalar, wscalar, iscalar, lscalar
154 float_types = fscalar, dscalar
155 complex_types = cscalar, zscalar
156 int_scalar_types = int_types
157 float_scalar_types = float_types
158 complex_scalar_types = complex_types
159 cvector = TensorType('complex64', (False, ))
160 zvector = TensorType('complex128', (False, ))
161 fvector = TensorType('float32', (False, ))
162 dvector = TensorType('float64', (False, ))
163 bvector = TensorType('int8', (False,))
164 wvector = TensorType('int16', (False,))
165 ivector = TensorType('int32', (False, ))
166 lvector = TensorType('int64', (False, ))
167 def vector(name=None, dtype=None):
168     """Return a symbolic vector variable.
169     Parameters
170     ----------
171     dtype: numeric
172         None means to use theano.config.floatX.
173     name
174         A name to attach to this variable
175     """
176     if dtype is None:
177         dtype = config.floatX
178     type = TensorType(dtype, (False, ))
179     return type(name)
180 vectors, fvectors, dvectors, ivectors, lvectors = _multi(
181     vector, fvector, dvector, ivector, lvector)
182 int_vector_types = bvector, wvector, ivector, lvector
183 float_vector_types = fvector, dvector
184 complex_vector_types = cvector, zvector
185 cmatrix = TensorType('complex64', (False, False))
186 zmatrix = TensorType('complex128', (False, False))
187 fmatrix = TensorType('float32', (False, False))
188 dmatrix = TensorType('float64', (False, False))
189 bmatrix = TensorType('int8', (False, False))
190 wmatrix = TensorType('int16', (False, False))
191 imatrix = TensorType('int32', (False, False))
192 lmatrix = TensorType('int64', (False, False))
193 def matrix(name=None, dtype=None):
194     """Return a symbolic matrix variable.
195     Parameters
196     ----------
197     dtype: numeric
198         None means to use theano.config.floatX.
199     name
200         A name to attach to this variable.
201     """
202     if dtype is None:
203         dtype = config.floatX
204     type = TensorType(dtype, (False, False))
205     return type(name)
206 matrices, fmatrices, dmatrices, imatrices, lmatrices = _multi(
207     matrix, fmatrix, dmatrix, imatrix, lmatrix)
208 int_matrix_types = bmatrix, wmatrix, imatrix, lmatrix
209 float_matrix_types = fmatrix, dmatrix
210 complex_matrix_types = cmatrix, zmatrix
211 crow = TensorType('complex64', (True, False))
212 zrow = TensorType('complex128', (True, False))
213 frow = TensorType('float32', (True, False))
214 drow = TensorType('float64', (True, False))
215 brow = TensorType('int8', (True, False))
216 wrow = TensorType('int16', (True, False))
217 irow = TensorType('int32', (True, False))
218 lrow = TensorType('int64', (True, False))
219 def row(name=None, dtype=None):
220     """Return a symbolic row variable (ndim=2, broadcastable=[True,False]).
221     Parameters
222     ----------
223     dtype: numeric type
224         None means to use theano.config.floatX.
225     name
226         A name to attach to this variable.
227     """
228     if dtype is None:
229     type = TensorType(dtype, (True, False))
230     return type(name)
231 rows, frows, drows, irows, lrows <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= _multi(row, frow, drow, irow, lrow)
232 ccol = TensorType('complex64', (False, True))
233 zcol = TensorType('complex128', (False, True))
234 fcol = TensorType('float32', (False, True))
235 dcol = TensorType('float64', (False, True))
236 bcol =</b></font> TensorType('int8', (False, True))
237 wcol = TensorType('int16', (False, True))
238 icol = TensorType('int32', (False, True))
239 lcol = TensorType('int64', (False, True))
240 def col(name=None, dtype=None):
241     """Return a symbolic column variable (ndim=2, broadcastable=[False,True]).
242     Parameters
243     ----------
244     dtype : numeric
245         None means to use theano.config.floatX.
246     name
247         A name to attach to this variable.
248     """
249     if dtype is None:
250         dtype = config.floatX
251     type = TensorType(dtype, (False, True))
252     return type(name)
253 cols, fcols, dcols, icols, lcols = _multi(col, fcol, dcol, icol, lcol)
254 ctensor3 = TensorType('complex64', ((False,) * 3))
255 ztensor3 = TensorType('complex128', ((False,) * 3))
256 ftensor3 = TensorType('float32', ((False,) * 3))
257 dtensor3 = TensorType('float64', ((False,) * 3))
258 btensor3 = TensorType('int8', ((False,) * 3))
259 wtensor3 = TensorType('int16', ((False,) * 3))
260 itensor3 = TensorType('int32', ((False,) * 3))
261 ltensor3 = TensorType('int64', ((False,) * 3))
262 def tensor3(name=None, dtype=None):
263     """Return a symbolic 3-D variable.
264     Parameters
265     ----------
266     dtype: numeric type
267         None means to use theano.config.floatX.
268     name
269         A name to attach to this variable.
270     """
271     if dtype is None:
272         dtype = config.floatX
273     type = TensorType(dtype, (False, False, False))
274     return type(name)
275 tensor3s, ftensor3s, dtensor3s, itensor3s, ltensor3s = _multi(
276     tensor3, ftensor3, dtensor3, itensor3, ltensor3)
277 ctensor4 = TensorType('complex64', ((False,) * 4))
278 ztensor4 = TensorType('complex128', ((False,) * 4))
279 ftensor4 = TensorType('float32', ((False,) * 4))
280 dtensor4 = TensorType('float64', ((False,) * 4))
281 btensor4 = TensorType('int8', ((False,) * 4))
282 wtensor4 = TensorType('int16', ((False,) * 4))
283 itensor4 = TensorType('int32', ((False,) * 4))
284 ltensor4 = TensorType('int64', ((False,) * 4))
285 def tensor4(name=None, dtype=None):
286     """Return a symbolic 4-D variable.
287     Parameters
288     ----------
289     dtype: numeric type
290         None means to use theano.config.floatX.
291     name
292         A name to attach to this variable.
293     """
294     if dtype is None:
295         dtype = config.floatX
296     type = TensorType(dtype, (False, False, False, False))
297     return type(name)
298 tensor4s, ftensor4s, dtensor4s, itensor4s, ltensor4s = _multi(
299     tensor4, ftensor4, dtensor4, itensor4, ltensor4)
300 ctensor5 = TensorType('complex64', ((False,) * 5))
301 ztensor5 = TensorType('complex128', ((False,) * 5))
302 ftensor5 = TensorType('float32', ((False,) * 5))
303 dtensor5 = TensorType('float64', ((False,) * 5))
304 btensor5 = TensorType('int8', ((False,) * 5))
305 wtensor5 = TensorType('int16', ((False,) * 5))
306 itensor5 = TensorType('int32', ((False,) * 5))
307 ltensor5 = TensorType('int64', ((False,) * 5))
308 def tensor5(name=None, dtype=None):
309     """Return a symbolic 5-D variable.
310     Parameters
311     ----------
312     dtype: numeric type
313         None means to use theano.config.floatX.
314     name
315         A name to attach to this variable.
316     """
317     if dtype is None:
318         dtype = config.floatX
319     type = TensorType(dtype, (False, False, False, False, False))
320     return type(name)
321 tensor5s, ftensor5s, dtensor5s, itensor5s, ltensor5s = _multi(
322     tensor5, ftensor5, dtensor5, itensor5, ltensor5)
323 ctensor6 = TensorType('complex64', ((False,) * 6))
324 ztensor6 = TensorType('complex128', ((False,) * 6))
325 ftensor6 = TensorType('float32', ((False,) * 6))
326 dtensor6 = TensorType('float64', ((False,) * 6))
327 btensor6 = TensorType('int8', ((False,) * 6))
328 wtensor6 = TensorType('int16', ((False,) * 6))
329 itensor6 = TensorType('int32', ((False,) * 6))
330 ltensor6 = TensorType('int64', ((False,) * 6))
331 def tensor6(name=None, dtype=None):
332     """Return a symbolic 6-D variable.
333     Parameters
334     ----------
335     dtype: numeric type
336         None means to use theano.config.floatX.
337     name
338         A name to attach to this variable.
339     """
340     if dtype is None:
341         dtype = config.floatX
342     type = TensorType(dtype, (False,) * 6)
343     return type(name)
344 tensor6s, ftensor6s, dtensor6s, itensor6s, ltensor6s = _multi(
345     tensor6, ftensor6, dtensor6, itensor6, ltensor6)
346 ctensor7 = TensorType('complex64', ((False,) * 7))
347 ztensor7 = TensorType('complex128', ((False,) * 7))
348 ftensor7 = TensorType('float32', ((False,) * 7))
349 dtensor7 = TensorType('float64', ((False,) * 7))
350 btensor7 = TensorType('int8', ((False,) * 7))
351 wtensor7 = TensorType('int16', ((False,) * 7))
352 itensor7 = TensorType('int32', ((False,) * 7))
353 ltensor7 = TensorType('int64', ((False,) * 7))
354 def tensor7(name=None, dtype=None):
355     """Return a symbolic 7-D variable.
356     Parameters
357     ----------
358     dtype: numeric type
359         None means to use theano.config.floatX.
360     name
361         A name to attach to this variable.
362     """
363     if dtype is None:
364         dtype = config.floatX
365     type = TensorType(dtype, (False,) * 7)
366     return type(name)
367 tensor7s, ftensor7s, dtensor7s, itensor7s, ltensor7s = _multi(
368     tensor7, ftensor7, dtensor7, itensor7, ltensor7)
369 Tensor = TensorType
370 elemwise.as_tensor_variable = as_tensor_variable
371 elemwise.TensorType = TensorType
372 elemwise.TensorVariable = TensorVariable
373 elemwise.TensorConstant = TensorConstant
374 def _scal_elemwise_with_nfunc(nfunc, nin, nout):
375     """
376     Replace a symbol definition with an elementwise version of the
377     corresponding scalar Op.  If it is not None, the nfunc argument
378     should be a string such that getattr(numpy, nfunc) implements
379     a vectorized version of the elemwise operation. nin is the number
380     of inputs expected by that function, and nout is the number of
381     **destination** inputs it takes. That is, the function should
382     take nin+nout inputs. nout == 0 means that the numpy function
383     does not take a numpy array argument to put its result in.
384     """
385     def construct(symbol):
386         symbolname = symbol.__name__
387         inplace = symbolname.endswith('_inplace')
388         if inplace:
389             msg = "inplace"
390         else:
391             msg = "no_inplace"
392         n = "Elemwise{%s,%s}" % (symbolname, msg)
393         if inplace:
394             scalar_op = getattr(scal, symbolname[:-len('_inplace')])
395             inplace_scalar_op = scalar_op.__class__(scal.transfer_type(0))
396             rval = elemwise.Elemwise(inplace_scalar_op, {0: 0}, name=n,
397                                      nfunc_spec=(nfunc and (nfunc, nin, nout)))
398         else:
399             scalar_op = getattr(scal, symbolname)
400             rval = elemwise.Elemwise(scalar_op, name=n,
401                                      nfunc_spec=(nfunc and (nfunc, nin, nout)))
402         if getattr(symbol, '__doc__', False):
403             rval.__doc__ = symbol.__doc__ + '\n' + rval.__doc__
404         rval.__epydoc_asRoutine = symbol
405         rval.__module__ = 'tensor'
406         pprint.assign(rval, printing.FunctionPrinter(symbolname))
407         return rval
408     return construct
409 _scal_elemwise = _scal_elemwise_with_nfunc(None, None, None)
410 def _pack(x):
411     """
412     Convert x to a list if it is an iterable, otherwise wrap it in a list.
413     """
414     try:
415         return list(x)
416     except TypeError:
417         return [x]
418 def check_and_normalize_axes(x, axis):
419     """
420     Check axes, normalize and convert them to a Python list of integers.
421     Return an empty list if argument is None.
422     Parameters
423     ----------
424     x: Tensor variable
425     axis = Integer, tuple or list of integers
426     Returns
427     -------
428     axis: list of integers
429     """
430     x = as_tensor_variable(x)
431     if axis is None:
432         axis = []
433     elif (isinstance(axis, (integer_types, np.integer)) or
434             (isinstance(axis, np.ndarray) and axis.ndim == 0)):
435                 axis = [int(axis)]
436     elif isinstance(axis, (tuple, list, np.ndarray)):
437         axis = [int(i) for i in axis]
438     elif isinstance(axis, Variable):
439         if NoneConst.equals(axis):
440             axis = []
441         elif not isinstance(axis, TensorConstant):
442             raise TypeError("Computation needs a constant axis. Got %s" % axis)
443         else:
444             assert axis.dtype in integer_dtypes
445             if (isinstance(axis.data, (integer_types, np.integer)) or
446                     (isinstance(axis.data, np.ndarray) and axis.data.ndim == 0)):
447                         axis = [int(axis.data)]
448             elif isinstance(axis.data, (list, np.ndarray)):
449                 axis = [int(i) for i in axis.data]
450     else:
451         raise TypeError("Axis must be an integer, tuple, list of integers or a TensorVariable. Got %s" % axis)
452     if len(axis) &gt; 0:
453         for i in range(len(axis)):
454             if axis[i] &lt; 0:
455                 axis[i] += x.type.ndim
456             if axis[i] &lt; 0 or axis[i] &gt;= x.type.ndim:
457                 raise ValueError("Computation needs a valid axis number for %d-D tensor. Got %d" % (x.type.ndim, axis[i]))
458         axis = list(set(axis))
459         axis.sort()
460     return axis
461 class TensorFromScalar(Op):
462     __props__ = ()
463     def make_node(self, s):
464         assert isinstance(s.type, scal.Scalar)
465         return Apply(self,
466                      [s],
467                      [tensor(dtype=s.type.dtype,
468                              broadcastable=())])
469     def perform(self, node, inp, out_):
470         s, = inp
471         out, = out_
472         out[0] = np.asarray(s)
473     def infer_shape(self, node, in_shapes):
474         return [()]
475     def grad(self, inp, grads):
476         s, = inp
477         dt, = grads
478         if s.type.dtype in float_dtypes:
479             assert dt.type.dtype in float_dtypes
480             return [scalar_from_tensor(dt)]
481         if s.type.dtype in discrete_dtypes:
482             return [s.zeros_like().astype(theano.config.floatX)]
483         raise NotImplementedError("grad not implemented for complex dtypes")
484 tensor_from_scalar = TensorFromScalar()
485 class ScalarFromTensor(Op):
486     __props__ = ()
487     def make_node(self, t):
488         assert isinstance(t.type, TensorType)
489         assert t.type.broadcastable == ()
490         return Apply(self,
491                      [t],
492                      [scal.get_scalar_type(dtype=t.type.dtype).make_variable()]
493                      )
494     def perform(self, node, inp, out_):
495         s, = inp
496         out, = out_
497         out[0] = s.flatten()[0]
498     def infer_shape(self, node, in_shapes):
499         return [()]
500     def grad(self, inp, grads):
501         s, = inp
502         dt, = grads
503         return [tensor_from_scalar(dt)]
504     def R_op(self, inputs, eval_points):
505         if None in eval_points:
506             return [None]
507         return self.make_node(*eval_points).outputs
508     def c_code(self, node, name, inputs, outputs, sub):
509         x, = inputs
510         z, = outputs
511         fail = sub['fail']
512         return """
513         %(z)s = ((dtype_%(x)s*)(PyArray_DATA(%(x)s)))[0];
514         """ % locals()
515     def c_code_cache_version(self):
516         return (1,)
517 scalar_from_tensor = ScalarFromTensor()
518 def _conversion(real_value, name):
519     __oplist_tag(real_value, 'casting')
520     real_value.__module__ = 'tensor.basic'
521     pprint.assign(real_value, printing.FunctionPrinter(name))
522     return real_value
523 _convert_to_bool = _conversion(
524     elemwise.Elemwise(scal.convert_to_bool), 'bool')
525 _convert_to_int8 = _conversion(
526     elemwise.Elemwise(scal.convert_to_int8), 'int8')
527 _convert_to_int16 = _conversion(
528     elemwise.Elemwise(scal.convert_to_int16), 'int16')
529 _convert_to_int32 = _conversion(
530     elemwise.Elemwise(scal.convert_to_int32), 'int32')
531 _convert_to_int64 = _conversion(
532     elemwise.Elemwise(scal.convert_to_int64), 'int64')
533 _convert_to_uint8 = _conversion(
534     elemwise.Elemwise(scal.convert_to_uint8), 'uint8')
535 _convert_to_uint16 = _conversion(
536     elemwise.Elemwise(scal.convert_to_uint16), 'uint16')
537 _convert_to_uint32 = _conversion(
538     elemwise.Elemwise(scal.convert_to_uint32), 'uint32')
539 _convert_to_uint64 = _conversion(
540     elemwise.Elemwise(scal.convert_to_uint64), 'uint64')
541 _convert_to_float16 = _conversion(
542     elemwise.Elemwise(scal.convert_to_float16), 'float16')
543 _convert_to_float32 = _conversion(
544     elemwise.Elemwise(scal.convert_to_float32), 'float32')
545 _convert_to_float64 = _conversion(
546     elemwise.Elemwise(scal.convert_to_float64), 'float64')
547 _convert_to_complex64 = _conversion(
548     elemwise.Elemwise(scal.convert_to_complex64), 'complex64')
549 _convert_to_complex128 = _conversion(
550     elemwise.Elemwise(scal.convert_to_complex128), 'complex128')
551 _cast_mapping = {
552     'bool': _convert_to_bool,
553     'int8': _convert_to_int8,
554     'int16': _convert_to_int16,
555     'int32': _convert_to_int32,
556     'int64': _convert_to_int64,
557     'uint8': _convert_to_uint8,
558     'uint16': _convert_to_uint16,
559     'uint32': _convert_to_uint32,
560     'uint64': _convert_to_uint64,
561     'float16': _convert_to_float16,
562     'float32': _convert_to_float32,
563     'float64': _convert_to_float64,
564     'complex64': _convert_to_complex64,
565     'complex128': _convert_to_complex128}
566 @constructor
567 def cast(x, dtype):
568     if dtype == 'floatX':
569         dtype = config.floatX
570     _x = as_tensor_variable(x)
571     if _x.type.dtype == dtype:
572         return _x
573     if _x.type.dtype.startswith('complex') and not dtype.startswith('complex'):
574         raise TypeError((
575             'Casting from complex to real is ambiguous: consider real(), '
576             'imag(), angle() or abs()'))
577     return _cast_mapping[dtype](x)
578 class MaxAndArgmax(Op):
579     """
580     Calculate the max and argmax over a given axis or over all axes.
581     """
582     nin = 2  # tensor, axis
583     nout = 2  # max val, max idx
584     E_axis = 'invalid axis'
585     params_type = Generic()
586     __props__ = ('axis',)
587     _f16_ok = True
588     def __init__(self, axis):
589         assert isinstance(axis, list)
590     def get_params(self, node):
591         return self<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.axis
592     def make_node(self, x):
593         x = _as_tensor_variable(x)
594         all_axes = set(self.axis)
595         broadcastable = [b for i, b in enumerate(x.type.</b></font>broadcastable)
596                          if i not in all_axes]
597         inputs = [x]
598         outputs = [tensor(x.type.dtype, broadcastable, name='max'),
599                    tensor('int64', broadcastable, name='argmax')]
600         return Apply(self, inputs, outputs)
601     def perform(self, node, inp, outs, params):
602         x = inp[0]
603         axes = params
604         max, max_idx = outs
605         if axes is None:
606             axes = tuple(range(x.ndim))
607         else:
608             axes = tuple(int(ax) for ax in axes)
609         max[0] = theano._asarray(np.max(x, axes),
610                                  dtype=node.outputs[0].dtype)
611         keep_axes = np.array([i for i in range(x.ndim) if i not in axes],
612                              dtype='int64')
613         transposed_x = np.transpose(x, np.concatenate((keep_axes, axes)))
614         kept_shape = transposed_x.shape[:len(keep_axes)]
615         reduced_shape = transposed_x.shape[len(keep_axes):]
616         new_shape = kept_shape + (np.prod(reduced_shape, dtype='int64'),)
617         reshaped_x = transposed_x.reshape(new_shape)
618         max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),
619                                      dtype='int64')
620     def c_code(self, node, name, inp, out, sub):
621         if len(self.axis) != 1 and len(self.axis) != node.inputs[0].ndim:
622             raise NotImplementedError("NumPy C-API can compute max and argmax only for 1 axis or for all axes.")
623         x = inp[0]
624         axis = sub['params']
625         max, argmax = out
626         fail = sub["fail"]
627         ret = """
628         int axis;
629         if (PyTuple_GET_SIZE(%(axis)s) == PyArray_NDIM(%(x)s)) {
630             axis = NPY_MAXDIMS;
631         } else if(PyTuple_GET_SIZE(%(axis)s) == 1) {
632             PyObject* axis_object = PyTuple_GET_ITEM(%(axis)s, 0);
633             axis = (int)PyInt_AS_LONG(axis_object);
634             if (axis &gt; PyArray_NDIM(%(x)s)-1 || axis &lt; -PyArray_NDIM(%(x)s)) {
635                 PyErr_SetString(PyExc_ValueError,
636                 "MaxAndArgmax: bad axis argument");
637                 %(fail)s
638             }
639         } else {
640             PyErr_SetString(PyExc_NotImplementedError,
641             "MaxAndArgmax: NumPy C-API can compute max and argmax only for 1 axis or for all axes.");
642             %(fail)s
643         }
644         Py_CLEAR(%(max)s);
645         Py_CLEAR(%(argmax)s);//todo pass them as out parameter.
646         %(max)s = (PyArrayObject*)PyArray_Max(%(x)s, axis, NULL);
647         if (%(max)s == NULL) {
648             %(fail)s;
649         }
650         if (!PyArray_CheckExact(%(max)s)) {
651             %(max)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(max)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);
652             if(%(max)s == NULL){
653                 %(fail)s;
654             }
655         }
656         %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);
657         if (%(argmax)s == NULL) {
658             Py_CLEAR(%(max)s);
659             %(fail)s;
660         }
661         if (!PyArray_CheckExact(%(argmax)s)) {
662             %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);
663             if(%(argmax)s == NULL){
664                 %(fail)s;
665             }
666         }
667         if (PyArray_TYPE(%(argmax)s) != NPY_INT64) {
668             PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);
669             if (NULL == tmp){
670                 %(fail)s;
671             }
672             Py_DECREF(%(argmax)s);
673             %(argmax)s = (PyArrayObject*)tmp;
674         }
675         """
676         return ret % locals()
677     def c_code_cache_version(self):
678         return (5,)
679     def infer_shape(self, node, shapes):
680         ishape = shapes[0]
681         rval = tuple(ishape[i] for (i, b) in enumerate(
682             node.inputs[0].type.broadcastable) if i not in self.axis)
683         return [rval, rval]
684     def R_op(self, inputs, eval_points):
685         if eval_points[0] is None:
686             return [None, None]
687         if len(self.axis) != 1:
688             raise ValueError(('R_op supported for arg_max only for '
689                               'one axis!'))
690         if self.axis[0] &gt; 1:
691             raise ValueError(('R_op supported for arg_max only when '
692                               ' axis is 0 or 1'))
693         if inputs[0].ndim != 2:
694             raise ValueError(('R_op supported for arg_max only when '
695                               ' input is a matrix'))
696         max_vals, max_pos = self.make_node(*inputs).outputs
697         if self.axis[0] == 0:
698             return [eval_points[0][max_pos,
699                                    arange(eval_points[0].shape[1])], None]
700         else:
701             return [eval_points[0][arange(eval_points[0].shape[0]),
702                                    max_pos], None]
703     def grad(self, inp, grads):
704         x = inp[0]
705         axis = _as_tensor_variable(self.axis)
706         g_max, g_max_idx = grads
707         g_max_disconnected = isinstance(g_max.type, DisconnectedType)
708         g_max_idx_disconnected = isinstance(g_max_idx.type, DisconnectedType)
709         if g_max_disconnected and g_max_idx_disconnected:
710             return [DisconnectedType()(), DisconnectedType()()]
711         if g_max_disconnected:
712             return [x.zeros_like()]
713         if NoneConst.equals(axis):
714             axis_ = list(range(x.ndim))
715         else:
716             axis_ = axis
717         xmax = max(x, axis_)
718         pattern = []
719         out_dim = 0
720         if NoneConst.equals(axis):
721             axis = None
722         for i in xrange(x.ndim):
723             if axis is None or i in axis.data:
724                 pattern.append('x')
725             else:
726                 pattern.append(out_dim)
727                 out_dim += 1
728         g_max_pad = DimShuffle(g_max.broadcastable, pattern)(g_max)
729         xmax_pad = DimShuffle(xmax.broadcastable, pattern)(xmax)
730         g_x = eq(xmax_pad, x) * g_max_pad
731         return g_x,
732 class Argmax(Op):
733     """
734     Calculate the argmax over a given axis or over all axes.
735     """
736     nin = 2  # tensor, axis
737     nout = 1
738     E_axis = 'invalid axis'
739     __props__ = ('axis',)
740     _f16_ok = True
741     params_type = ParamsType(c_axis=scal.int64)
742     def __init__(self, axis):
743         if axis is not None:
744             axis = tuple(axis)
745         self.axis = tuple(axis)
746     def get_params(self, node):
747         if self.axis is not None and len(self.axis) == 1:
748             c_axis = np.int64(self.axis[0])
749         else:
750             c_axis = np.int64(-1)
751         return self.params_type.get_params(c_axis=c_axis)
752     def make_node(self, x, axis=None):
753         x = _as_tensor_variable(x)
754         if self.axis is None:
755             all_axes = list(range(x.ndim))
756         else:
757             all_axes = self.axis
758         inputs = [x]
759         broadcastable = [b for i, b in enumerate(x.type.broadcastable)
760                          if i not in all_axes]
761         outputs = [tensor('int64', broadcastable, name='argmax')]
762         return Apply(self, inputs, outputs)
763     def prepare_node(self, node, storage_map, compute_map, impl):
764         if len(node.inputs) == 2:
765             raise ValueError('You are trying to compile a graph with an old Argmax node.  Either reoptimize your graph or rebuild it to get the new node format.')
766     def perform(self, node, inp, outs, params):
767         x, = inp
768         axes = self.axis
769         max_idx, = outs
770         if axes is None:
771             axes = tuple(range(x.ndim))
772         keep_axes = np.array([i for i in range(x.ndim) if i not in axes],
773                              dtype='int64')
774         transposed_x = np.transpose(x, np.concatenate((keep_axes,
775                                                        axes)))
776         kept_shape = transposed_x.shape[:len(keep_axes)]
777         reduced_shape = transposed_x.shape[len(keep_axes):]
778         new_shape = kept_shape + (np.prod(reduced_shape),)
779         reshaped_x = transposed_x.reshape(new_shape)
780         max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),
781                                      dtype='int64')
782     def c_code(self, node, name, inp, out, sub):
783         x, = inp
784         argmax, = out
785         fail = sub["fail"]
786         params = sub["params"]
787         if self.axis is None:
788             axis_code = "axis = NPY_MAXDIMS;"
789         else:
790             if len(self.axis) &gt; 1:
791                 raise NotImplementedError()
792             axis_code = """
793             axis = %(params)s-&gt;c_axis;
794             if(axis &gt; PyArray_NDIM(%(x)s)-1 || axis &lt; -PyArray_NDIM(%(x)s)){
795                 PyErr_SetString(PyExc_ValueError,
796                 "Argmax, bad axis argument");
797                 %(fail)s
798             }
799             """ % locals()
800         ret = """
801         int axis;
802         Py_CLEAR(%(argmax)s);//todo pass them as out parameter.
803         %(axis_code)s
804         %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);
805         if(%(argmax)s == NULL){
806             %(fail)s;
807         }
808         if(!PyArray_CheckExact(%(argmax)s)){
809             %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);
810             if(%(argmax)s == NULL){
811                 %(fail)s;
812             }
813         }
814         if(PyArray_TYPE(%(argmax)s) != NPY_INT64){
815             PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);
816             if (NULL == tmp){
817                 %(fail)s;
818             }
819             Py_DECREF(%(argmax)s);
820             %(argmax)s = (PyArrayObject*)tmp;
821         }
822         """
823         return ret % locals()
824     def c_code_cache_version(self):
825         return (1,)
826     def infer_shape(self, node, shapes):
827         ishape, = shapes
828         if self.axis is None:
829             return [()]
830         rval = tuple([ishape[i] for (i, b) in enumerate(
831             node.inputs[0].type.broadcastable) if i not in self.axis])
832         return [rval]
833     def grad(self, inp, grads):
834         x, = inp
835         return [x.zeros_like()]
836 def makeKeepDims(x, y, axis):
837     """
838     Reintroduces in y with length one the axes of x which have been left out
839     in a prior reduction of x. With this option, the resulting tensor will
840     broadcast correctly against the original tensor x.
841     """
842     x = as_tensor_variable(x)
843     y = as_tensor_variable(y)
844     if axis is None:
845         axis = list(range(x.type.ndim))
846     elif isinstance(axis, (integer_types, np.integer)):
847         axis = [axis]
848     elif isinstance(axis, np.ndarray) and axis.ndim == 0:
849         axis = [int(axis)]
850     else:
851         axis = [int(a) for a in axis]
852     newaxis = []
853     for a in axis:
854         if not isinstance(a, integer_types):
855             raise ValueError(
856                 "keepdims option can be used only with constant axis")
857         if a &lt; 0:
858             a += x.type.ndim
859         newaxis.append(a)
860     i = 0
861     new_dims = []
862     for j, _ in enumerate(x.type.broadcastable):
863         if j in newaxis:
864             new_dims.append('x')
865         else:
866             new_dims.append(i)
867             i += 1
868     return DimShuffle(y.type.broadcastable, new_dims)(y)
869 @constructor
870 def max_and_argmax(a, axis=None, keepdims=False):
871     """
872     Returns maximum elements and their indices obtained by iterating over
873     given axis.
874     When axis is None (the default value), the max is performed
875     over the flattened tensor.
876     Parameters
877     ----------
878     keepdims : bool
879         If this is set to True, the axes which are reduced are left in
880         the result as dimensions with size one. With this option, the result
881         will broadcast correctly against the original tensor.
882     """
883     a = as_tensor_variable(a)
884     axis = check_and_normalize_axes(a, axis)
885     if len(axis) == 0:
886         axis = list(range(a.type.ndim))
887     out, argout = MaxAndArgmax(axis)(a)
888     if keepdims:
889         out = makeKeepDims(a, out, axis)
890         argout = makeKeepDims(a, argout, axis)
891     return [out, argout]
892 @constructor
893 def max(x, axis=None, keepdims=False):
894     """
895     Returns maximum elements obtained by iterating over given axis.
896     When axis is None (the default value), the max is performed
897     over the flattened tensor.
898     Parameters
899     ----------
900     keepdims: bool
901         If this is set to True, the axes which are reduced are left in
902         the result as dimensions with size one. With this option, the result
903         will broadcast correctly against the original tensor.
904     Notes
905     -----
906     We return an error as numpy when we reduce a dim with a shape of 0.
907     """
908     try:
909         out = max_and_argmax(x, axis)[0]
910     except Exception:
911         out = CAReduce(scal.maximum, axis)(x)
912     if keepdims:
913         out = makeKeepDims(x, out, axis)
914     return out
915 @constructor
916 def argmax(x, axis=None, keepdims=False):
917     """
918     Returns indices of maximum elements obtained by iterating over given axis.
919     When axis is None (the default value), the argmax is performed
920     over the flattened tensor.
921     Parameters
922     ----------
923     keepdims : bool
924         If this is set to True, the axes which are reduced are left in
925         the result as dimensions with size one. With this option, the result
926         will broadcast correctly against the original tensor.
927     """
928     argout = max_and_argmax(x, axis)[1]
929     if keepdims:
930         argout = makeKeepDims(x, argout, axis)
931     return argout
932 @constructor
933 def min(x, axis=None, keepdims=False):
934     """
935     Returns minimum elements obtained by iterating over given axis.
936     When axis is None (the default value), the min is performed
937     over the flattened tensor.
938     Parameters
939     ----------
940     keepdims: bool
941         If this is set to True, the axes which are reduced are left in
942         the result as dimensions with size one. With this option, the result
943         will broadcast correctly against the original tensor.
944     """
945     x = as_tensor_variable(x)
946     str_x_type = str(x.dtype)
947     if str_x_type.startswith('float') or str_x_type in int_dtypes:
948         return -max(-x, axis=axis, keepdims=keepdims)
949     elif str_x_type in uint_dtypes:
950         itype = np.iinfo(x.dtype)
951         max_val = np.array(itype.max, dtype=itype.dtype)
952         return max_val - max(max_val - x, axis=axis, keepdims=keepdims)
953     elif str_x_type == 'bool':
954         return ~max(~x, axis=axis, keepdims=keepdims)
955     else:
956         raise NotImplementedError()
957 @constructor
958 def argmin(x, axis=None, keepdims=False):
959     """
960     Returns indices of minimum elements obtained by iterating over given axis.
961     When axis is None (the default value), the argmin is performed
962     over the flattened tensor.
963     Parameters
964     ----------
965     keepdims: bool
966         If this is set to True, the axes which are reduced are left in
967         the result as dimensions with size one. With this option, the result
968         will broadcast correctly against the original tensor.
969     """
970     x = as_tensor_variable(x)
971     str_x_type = str(x.dtype)
972     if str_x_type.startswith('float') or str_x_type in int_dtypes:
973         return argmax(-x, axis=axis, keepdims=keepdims)
974     elif str_x_type in uint_dtypes:
975         itype = np.iinfo(x.dtype)
976         return argmax(itype.max - x, axis=axis, keepdims=keepdims)
977     elif str_x_type == 'bool':
978         return argmax(~x, axis=axis, keepdims=keepdims)
979     else:
980         raise NotImplementedError()
981 @constructor
982 def smallest(*args):
983     """
984     Return the [elementwise] smallest of a variable number of arguments.
985     Like python's min.
986     """
987     if len(args) == 2:
988         a, b = args
989         return switch(a &lt; b, a, b)
990     else:
991         return min(stack(args), axis=0)
992 @constructor
993 def largest(*args):
994     """
995     Return the [elementwise] largest of a variable number of arguments.
996     Like python's max.
997     """
998     if len(args) == 2:
999         a, b = args
1000         return switch(a &gt; b, a, b)
1001     else:
1002         return max(stack(args), axis=0)
1003 @_scal_elemwise
1004 def lt(a, b):
1005 @_scal_elemwise
1006 def gt(a, b):
1007 @_scal_elemwise
1008 def le(a, b):
1009 @_scal_elemwise
1010 def ge(a, b):
1011 @_scal_elemwise
1012 def eq(a, b):
1013 @_scal_elemwise
1014 def neq(a, b):
1015 @_scal_elemwise
1016 def isnan(a):
1017 isnan_ = isnan
1018 def isnan(a):
1019     a = as_tensor_variable(a)
1020     if a.dtype in discrete_dtypes:
1021         return alloc(np.asarray(False, dtype="bool"),
1022                      *[a.shape[i] for i in range(a.ndim)])
1023     return isnan_(a)
1024 @_scal_elemwise
1025 def isinf(a):
1026 isinf_ = isinf
1027 def isinf(a):
1028     a = as_tensor_variable(a)
1029     if a.dtype in discrete_dtypes:
1030         return alloc(np.asarray(False, dtype="bool"),
1031                      *[a.shape[i] for i in range(a.ndim)])
1032     return isinf_(a)
1033 def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
1034     """
1035     Implement Numpy's ``allclose`` on tensors.
1036     ``absolute(a - b) &lt;= (atol + rtol * absolute(b))``
1037     Parameters
1038     ----------
1039     a : tensor
1040         Input to compare.
1041     b : tensor
1042         Input to compare.
1043     rtol : float
1044         The relative tolerance parameter.
1045     atol : float
1046         The absolute tolerance parameter.
1047     equal_nan: bool
1048         Whether to consider nan's in the same place to be close.
1049     Returns
1050     -------
1051     bool
1052         A boolean value (of type int8 returned by the tensor elementwise `all`
1053         function) whether all elements in a and b are in the tolerance range
1054         defined above.
1055     Notes
1056     -----
1057     Not a symmetric equation. See Numpy's documentation.
1058     """
1059     return all(isclose(a, b, rtol, atol, equal_nan))
1060 def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
1061     """
1062     Implements Numpy's ``isclose`` on tensors.
1063     The tolerance values are positive, typically very small numbers. The
1064     relative difference (`rtol` * abs(`b`)) and the absolute difference
1065     `atol` are added together to compare against the absolute difference
1066     between `a` and `b`.
1067     ``absolute(a - b) &lt;= (atol + rtol * absolute(b))``
1068     Parameters
1069     ----------
1070     a : tensor
1071         Input to compare.
1072     b : tensor
1073         Input to compare.
1074     rtol : float
1075         The relative tolerance parameter.
1076     atol : float
1077         The absolute tolerance parameter.
1078     equal_nan : bool
1079         Whether to consider nan's in the same place to be close
1080     Returns
1081     -------
1082     int8
1083         A boolean (int8) array where two arrays are element-wise equal
1084         within a tolerance.
1085     Notes
1086     -----
1087     Not a symmetric equation. See Numpy's documentation.
1088     Examples
1089     --------
1090     &gt;&gt;&gt; import theano
1091     &gt;&gt;&gt; import numpy as np
1092     &gt;&gt;&gt; a = theano._asarray([1e10, 1e-7], dtype="float64")
1093     &gt;&gt;&gt; b = theano._asarray([1.00001e10, 1e-8], dtype="float64")
1094     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
1095     array([1, 0], dtype=int8)
1096     &gt;&gt;&gt; a = theano._asarray([1e10, 1e-8], dtype="float64")
1097     &gt;&gt;&gt; b = theano._asarray([1.00001e10, 1e-9], dtype="float64")
1098     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
1099     array([1, 1], dtype=int8)
1100     &gt;&gt;&gt; a = theano._asarray([1e10, 1e-8], dtype="float64")
1101     &gt;&gt;&gt; b = theano._asarray([1.0001e10, 1e-9], dtype="float64")
1102     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
1103     array([0, 1], dtype=int8)
1104     &gt;&gt;&gt; a = theano._asarray([1.0, np.nan], dtype="float64")
1105     &gt;&gt;&gt; b = theano._asarray([1.0, np.nan], dtype="float64")
1106     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
1107     array([1, 0], dtype==int8)
1108     &gt;&gt;&gt; a = theano._asarray([1.0, np.nan], dtype="float64")
1109     &gt;&gt;&gt; b = theano._asarray([1.0, np.nan], dtype="float64")
1110     &gt;&gt;&gt; theano.tensor.isclose(a, b, equal_nan=True).eval()
1111     array([1, 1], dtype==int8)
1112     &gt;&gt;&gt; a = theano._asarray([1.0, np.inf], dtype="float64")
1113     &gt;&gt;&gt; b = theano._asarray([1.0, -np.inf], dtype="float64")
1114     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
1115     array([1, 0], dtype==int8)
1116     &gt;&gt;&gt; a = theano._asarray([1.0, np.inf], dtype="float64")
1117     &gt;&gt;&gt; b = theano._asarray([1.0, np.inf], dtype="float64")
1118     &gt;&gt;&gt; theano.tensor.isclose(a, b).eval()
1119     array([1, 1], dtype==int8)
1120     """
1121     diff = abs(a - b)
1122     tolerance = atol + rtol * abs(b)
1123     close_prelim = le(diff, tolerance)
1124     a_nan = isnan(a)
1125     b_nan = isnan(b)
1126     nans = bitwise_or(a_nan, b_nan)
1127     a_inf = isinf(a)
1128     b_inf = isinf(b)
1129     infs = bitwise_or(a_inf, b_inf)
1130     nans_or_infs = bitwise_or(nans, infs)
1131     close = bitwise_and(close_prelim, bitwise_not(nans_or_infs))
1132     both_infs = bitwise_and(a_inf, b_inf)
1133     inf_signs_eq = eq(a_inf * sgn(a), b_inf * sgn(b))
1134     inf_eq = bitwise_and(both_infs, inf_signs_eq)
1135     close_with_infs = bitwise_or(close, inf_eq)
1136     if equal_nan:
1137         both_nans = bitwise_and(a_nan, b_nan)
1138         return bitwise_or(close_with_infs, both_nans)
1139     else:
1140         return close_with_infs
1141 @_scal_elemwise
1142 def switch(cond, ift, iff):
1143 where = switch
1144 @_scal_elemwise
1145 def and_(a, b):
1146 bitwise_and = and_  # numpy name for it
1147 @_scal_elemwise
1148 def or_(a, b):
1149 bitwise_or = or_  # numpy name for it
1150 @_scal_elemwise
1151 def xor(a, b):
1152 bitwise_xor = xor  # numpy name for it
1153 @_scal_elemwise
1154 def invert(a):
1155 bitwise_not = invert  # numpy alias for it
1156 @_scal_elemwise
1157 def abs_(a):
1158     """|`a`|
1159     TensorVariable overloads the `TensorVariable.__abs__` operator so that
1160     this function is called when you type abs(a).
1161     """
1162 pprint.assign(abs_, printing.PatternPrinter(('|%(0)s|', -1000)))
1163 @_scal_elemwise
1164 def exp(a):
1165 @_scal_elemwise
1166 def exp2(a):
1167 @_scal_elemwise
1168 def expm1(a):
1169 @_scal_elemwise
1170 def neg(a):
1171 @_scal_elemwise
1172 def inv(a):
1173 @_scal_elemwise
1174 def log(a):
1175 @_scal_elemwise
1176 def log2(a):
1177 @_scal_elemwise
1178 def log10(a):
1179 @_scal_elemwise
1180 def log1p(a):
1181 @_scal_elemwise
1182 def sgn(a):
1183 @_scal_elemwise
1184 def ceil(a):
1185 @_scal_elemwise
1186 def floor(a):
1187 @_scal_elemwise
1188 def trunc(a):
1189 @constructor
1190 def iround(a, mode=None):
1191     return cast(round(a, mode), 'int64')
1192 @constructor
1193 def round(a, mode=None):
1194     """round_mode(a) with mode in [half_away_from_zero, half_to_even].
1195     Default to half_to_even."""
1196     if mode is None:
1197         mode = "half_to_even"
1198         if config.warn.round:
1199             warnings.warn(
1200                 "theano.tensor.round() changed its default from"
1201                 " `half_away_from_zero` to `half_to_even` to have"
1202                 " the same default as NumPy. Use the Theano flag"
1203                 " `warn.round=False` to disable this warning.")
1204     if mode == "half_away_from_zero":
1205         return round_half_away_from_zero(a)
1206     elif mode == "half_to_even":
1207         return round_half_to_even(a)
1208     else:
1209         raise Exception("round mode %s is not implemented." % mode)
1210 @_scal_elemwise
1211 def round_half_to_even(a):
1212 @_scal_elemwise
1213 def round_half_away_from_zero(a):
1214 @_scal_elemwise
1215 def sqr(a):
1216 square = sqr
1217 def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None):
1218     """Calculate the covariance matrix.
1219     Covariance indicates the level to which two variables vary together.
1220     If we examine N-dimensional samples, :math:`m = [x_1, x_2, ... x_N]^T`,
1221     then the covariance matrix element :math:`C_{ij}` is the covariance of
1222     :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
1223     of :math:`x_i`. Code and docstring ported from numpy.
1224     ----------
1225     m : array_like
1226         A 2-D array containing multiple variables and observations.
1227         Each row of `m` represents a variable, and each column is
1228         observations of all those variables.
1229     y : array_like, optional
1230         An additional set of variables and observations. `y` has the same form
1231         as that of `m`.
1232     rowvar : bool, optional
1233         If `rowvar` is True (default), then each row represents a
1234         variable, with observations in the columns. Otherwise, the relationship
1235         is transposed: each column represents a variable, while the rows
1236         contain observations.
1237     bias : bool, optional
1238         Default normalization (False) is by ``(N - 1)``, where ``N`` is the
1239         number of observations given (unbiased estimate). If `bias` is True, then
1240         normalization is by ``N``. These values can be overridden by using the
1241         keyword ``ddof``.
1242     ddof : int, optional
1243         If not ``None`` the default value implied by `bias` is overridden.
1244         The default value is ``None``.
1245     Returns
1246     -------
1247     out : The covariance matrix of the variables.
1248     """
1249     if fweights is not None:
1250         raise NotImplementedError('fweights are not implemented')
1251     if aweights is not None:
1252         raise NotImplementedError('aweights are not implemented')
1253     if not rowvar and m.shape[0] != 1:
1254         m = m.T
1255     if y is not None:
1256         if not rowvar and y.shape[0] != 1:
1257             y = y.T
1258         m = theano.tensor.concatenate((m, y), axis=0)
1259     if ddof is None:
1260         if not bias:
1261             ddof = 1
1262         else:
1263             ddof = 0
1264     fact = m.shape[1] - ddof
1265     m -= m.mean(axis=1, keepdims=1)
1266     c = m.dot(m.T)
1267     c *= theano.tensor.constant(1) / fact
1268     return c.squeeze()
1269 @_scal_elemwise
1270 def sqrt(a):
1271 @_scal_elemwise
1272 def deg2rad(a):
1273 @_scal_elemwise
1274 def rad2deg(a):
1275 @_scal_elemwise
1276 def cos(a):
1277 @_scal_elemwise
1278 def arccos(a):
1279 @_scal_elemwise
1280 def sin(a):
1281 @_scal_elemwise
1282 def arcsin(a):
1283 @_scal_elemwise
1284 def tan(a):
1285 @_scal_elemwise
1286 def arctan(a):
1287 @_scal_elemwise
1288 def arctan2(a, b):
1289 @_scal_elemwise
1290 def cosh(a):
1291 @_scal_elemwise
1292 def arccosh(a):
1293 @_scal_elemwise
1294 def sinh(a):
1295 @_scal_elemwise
1296 def arcsinh(a):
1297 @_scal_elemwise
1298 def tanh(a):
1299 @_scal_elemwise
1300 def arctanh(a):
1301 @_scal_elemwise
1302 def erf(a):
1303 @_scal_elemwise
1304 def erfc(a):
1305 @_scal_elemwise
1306 def erfcx(a):
1307 @_scal_elemwise
1308 def erfinv(a):
1309 @_scal_elemwise
1310 def erfcinv(a):
1311 @_scal_elemwise
1312 def gamma(a):
1313 @_scal_elemwise
1314 def gammaln(a):
1315 @_scal_elemwise
1316 def psi(a):
1317 @_scal_elemwise
1318 def tri_gamma(a):
1319 @_scal_elemwise
1320 def chi2sf(x, k):
1321 @_scal_elemwise
1322 def gammainc(k, x):
1323 @_scal_elemwise
1324 def gammaincc(k, x):
1325 @_scal_elemwise
1326 def gammau(k, x):
1327 @_scal_elemwise
1328 def gammal(k, x):
1329 @_scal_elemwise
1330 def j0(x):
1331 @_scal_elemwise
1332 def j1(x):
1333 @_scal_elemwise
1334 def jv(v, x):
1335 @_scal_elemwise
1336 def i0(x):
1337 @_scal_elemwise
1338 def i1(x):
1339 @_scal_elemwise
1340 def iv(v, x):
1341 @_scal_elemwise
1342 def real(z):
1343 _tensor_py_operators.real = property(real)
1344 @_scal_elemwise
1345 def imag(z):
1346 _tensor_py_operators.imag = property(imag)
1347 @_scal_elemwise
1348 def angle(z):
1349 @_scal_elemwise  # numpy.complex cannot build tensors
1350 def complex(real, imag):
1351 @_scal_elemwise
1352 def conj(z):
1353 @_scal_elemwise
1354 def complex_from_polar(abs, angle):
1355 @_scal_elemwise
1356 def second(a, b):
1357 fill = second
1358 pprint.assign(fill, printing.FunctionPrinter('fill'))
1359 @constructor
1360 def ones_like(model, dtype=None, opt=False):
1361     """equivalent of numpy.ones_like
1362     Parameters
1363     ----------
1364     model : tensor
1365     dtype : data-type, optional
1366     opt : If True, we will return a constant instead of a graph when possible.
1367           Useful for Theano optimization, not for user building a graph as this
1368           have the consequence that model isn't always in the graph.
1369     Returns
1370     -------
1371     tensor
1372         tensor the shape of model containing ones of the type of dtype.
1373     """
1374     if dtype is None:
1375         dtype = model.type.dtype
1376     ret = constant(1.0, dtype=dtype)
1377     if opt and ret.type == model.type:
1378         return ret
1379     return fill(model, ret)
1380 @constructor
1381 def zeros_like(model, dtype=None, opt=False):
1382     """equivalent of numpy.zeros_like
1383     Parameters
1384     ----------
1385     model : tensor
1386     dtype : data-type, optional
1387     opt : If True, we will return a constant instead of a graph when possible.
1388           Useful for Theano optimization, not for user building a graph as this
1389           have the consequence that model isn't always in the graph.
1390     Returns
1391     -------
1392     tensor
1393         tensor the shape of model containing zeros of the type of dtype.
1394     """
1395     if dtype is None:
1396         dtype = model.type.dtype
1397     ret = constant(0.0, dtype=dtype)
1398     if opt and ret.type == model.type:
1399         return ret
1400     return fill(model, ret)
1401 def zeros(shape, dtype=None):
1402     """
1403     Create a Tensor filled with zeros, closer to Numpy's syntax than ``alloc``.
1404     """
1405     if not isinstance(shape, (list, tuple, TensorVariable)):
1406         shape = [shape]
1407     if dtype is None:
1408         dtype = config.floatX
1409     return alloc(np.array(0, dtype=dtype), *shape)
1410 def ones(shape, dtype=None):
1411     """
1412     Create a Tensor filled with ones, closer to Numpy's syntax than ``alloc``.
1413     """
1414     if not isinstance(shape, (list, tuple, TensorVariable)):
1415         shape = [shape]
1416     if dtype is None:
1417         dtype = config.floatX
1418     return alloc(np.array(1, dtype=dtype), *shape)
1419 class Nonzero(gof.Op):
1420     """
1421     Return the indices of the elements that are non-zero.
1422     Returns a matrix of shape (ndim, number of nonzero elements) such that
1423     element (i,j) is the index in the ith dimension of the jth non-zero
1424     element.
1425     Note this is different than NumPy, which returns a tuple of arrays, one for
1426     each dimension of the input array.
1427     Parameters
1428     ----------
1429     a : array_like
1430         Input array.
1431     Returns
1432     -------
1433     matrix
1434         Matrix containing the indices of the non-zero elements of a.
1435     See Also
1436     --------
1437     nonzero_values : Return the non-zero elements of the input array
1438     flatnonzero : Return the indices of the non-zero elements of the
1439         flattened input array.
1440     """
1441     __props__ = ()
1442     def make_node(self, a):
1443         a = as_tensor_variable(a)
1444         if a.ndim == 0:
1445             raise ValueError('Nonzero only supports non-scalar arrays.')
1446         output = [TensorType(dtype='int64', broadcastable=(False, False))()]
1447         return gof.Apply(self, [a], output)
1448     def perform(self, node, inp, out_):
1449         a = inp[0]
1450         out, = out_
1451         result_tuple = np.nonzero(a)
1452         if len(result_tuple[0]) &gt; 0:
1453             result = np.vstack(result_tuple)
1454         else:
1455             result = np.zeros((len(result_tuple), 0))
1456         out[0] = result.astype('int64')
1457     def grad(self, inp, grads):
1458         return [grad_undefined(self, 0, inp[0])]
1459 _nonzero = Nonzero()
1460 def nonzero(a, return_matrix=False):
1461     """
1462     Returns one of the following:
1463         If return_matrix is False (default, same as NumPy):
1464             A tuple of vector arrays such that the ith element of the jth array
1465             is the index of the ith non-zero element of the input array in the
1466             jth dimension.
1467         If return_matrix is True (same as Theano Op):
1468             Returns a matrix of shape (ndim, number of nonzero elements) such
1469             that element (i,j) is the index in the ith dimension of the jth
1470             non-zero element.
1471     Parameters
1472     ----------
1473     a : array_like
1474         Input array.
1475     return_matrix : bool
1476         If True, returns a symbolic matrix. If False, returns a tuple of
1477         arrays. Defaults to False.
1478     Returns
1479     -------
1480     tuple of vectors or matrix
1481     See Also
1482     --------
1483     nonzero_values : Return the non-zero elements of the input array
1484     flatnonzero : Return the indices of the non-zero elements of the
1485         flattened input array.
1486     """
1487     matrix_result = _nonzero(a)
1488     if return_matrix:
1489         return matrix_result
1490     else:
1491         if a.ndim &gt; 0:
1492             tuple_result = tuple([matrix_result[i] for i in xrange(a.ndim)])
1493         else:
1494             tuple_result = tuple([matrix_result[0]])
1495         return tuple_result
1496 def flatnonzero(a):
1497     """
1498     Return a vector of indices that are non-zero in the flattened version of a.
1499     This is equivalent to nonzero(a.flatten(), return_matrix=True)[0]
1500     Parameters
1501     ----------
1502     a : tensor
1503         Input tensor
1504     Returns
1505     -------
1506     vector
1507         Output vector, containing the indices of the elements of `a.flatten()`
1508         that are non-zero.
1509     See Also
1510     --------
1511     nonzero : Return the indices of the non-zero elements of the input array.
1512     nonzero_values : Return the non-zero elements of the input array
1513     """
1514     if a.ndim == 0:
1515         raise ValueError('Nonzero only supports non-scalar arrays.')
1516     return nonzero(a.flatten(), return_matrix=True)[0]
1517 def nonzero_values(a):
1518     """
1519     Return a vector of non-zero elements contained in the input array.
1520     The following behavior works to extract non-zero elements from an array
1521     in NumPy but is *NOT* supported by Theano:
1522         a[numpy.nonzero(a)]
1523     Instead, the nonzero_values function or method should be used:
1524         tensor.nonzero_values(a)
1525         a.nonzero_values()
1526     This is equivalent to the following:
1527         a.flatten()[tensor.flatnonzero(a)]
1528     Parameters
1529     ----------
1530     a : tensor
1531         Input tensor
1532     Returns
1533     -------
1534     vector
1535         Output vector, containing the non-zero elements of a.
1536     See Also
1537     --------
1538     nonzero : Return the indices of the non-zero elements of the input array.
1539     flatnonzero : Return the indices of the non-zero elements of the
1540         flattened input array.
1541     """
1542     return a.flatten()[flatnonzero(a)]
1543 class Tri(gof.Op):
1544     __props__ = ("dtype",)
1545     def __init__(self, dtype=None):
1546         if dtype is None:
1547             dtype = config.floatX
1548         self.dtype = dtype
1549     def make_node(self, N, M, k):
1550         N = as_tensor_variable(N)
1551         M = as_tensor_variable(M)
1552         k = as_tensor_variable(k)
1553         return gof.Apply(
1554             self,
1555             [N, M, k],
1556             [TensorType(dtype=self.dtype, broadcastable=(False, False))()])
1557     def perform(self, node, inp, out_):
1558         N, M, k = inp
1559         out, = out_
1560         out[0] = np.tri(N, M, k, dtype=self.dtype)
1561     def infer_shape(self, node, in_shapes):
1562         out_shape = [node.inputs[0], node.inputs[1]]
1563         return [out_shape]
1564     def grad(self, inp, grads):
1565         return [grad_undefined(self, i, inp[i]) for i in xrange(3)]
1566 def tri(N, M=None, k=0, dtype=None):
1567     """
1568     An array with ones at and below the given diagonal and zeros elsewhere.
1569     Parameters
1570     ----------
1571     N : int
1572         Number of rows in the array.
1573     M : int, optional
1574         Number of columns in the array.
1575         By default, `M` is taken equal to `N`.
1576     k : int, optional
1577         The sub-diagonal at and below which the array is filled.
1578         `k` = 0 is the main diagonal, while `k` &lt; 0 is below it,
1579         and `k` &gt; 0 is above.  The default is 0.
1580     dtype : dtype, optional
1581         Data type of the returned array.  The default is float.
1582     Returns
1583     -------
1584     Array of shape (N, M)
1585         Array with its lower triangle filled with ones and zero elsewhere;
1586         in other words ``T[i,j] == 1`` for ``i &lt;= j + k``, 0 otherwise.
1587     """
1588     if dtype is None:
1589         dtype = config.floatX
1590     if M is None:
1591         M = N
1592     op = Tri(dtype)
1593     return op(N, M, k)
1594 def tril(m, k=0):
1595     """
1596     Lower triangle of an array.
1597     Return a copy of an array with elements above the `k`-th diagonal zeroed.
1598     Parameters
1599     ----------
1600     m : array_like, shape (M, N)
1601         Input array.
1602     k : int, optional
1603         Diagonal above which to zero elements.  `k = 0` (the default) is the
1604         main diagonal, `k &lt; 0` is below it and `k &gt; 0` is above.
1605     Returns
1606     -------
1607     array, shape (M, N)
1608         Lower triangle of `m`, of same shape and data-type as `m`.
1609     See Also
1610     --------
1611     triu : Same thing, only for the upper triangle.
1612     """
1613     return m * tri(m.shape[0], m.shape[1], k=k, dtype=m.dtype)
1614 def triu(m, k=0):
1615     """
1616     Upper triangle of an array.
1617     Return a copy of a matrix with the elements below the `k`-th diagonal
1618     zeroed.
1619     Please refer to the documentation for `tril` for further details.
1620     See Also
1621     --------
1622     tril : Lower triangle of an array.
1623     """
1624     return m * (1 - tri(m.shape[0], m.shape[1], k=k - 1, dtype=m.dtype))
1625 class Eye(gof.Op):
1626     __props__ = ("dtype", )
1627     def __init__(self, dtype=None):
1628         if dtype is None:
1629             dtype = config.floatX
1630         self.dtype = dtype
1631     def make_node(self, n, m, k):
1632         n = as_tensor_variable(n)
1633         m = as_tensor_variable(m)
1634         k = as_tensor_variable(k)
1635         assert n.ndim == 0
1636         assert m.ndim == 0
1637         assert k.ndim == 0
1638         return gof.Apply(
1639             self,
1640             [n, m, k],
1641             [TensorType(dtype=self.dtype, broadcastable=(False, False))()])
1642     def perform(self, node, inp, out_):
1643         n, m, k = inp
1644         out, = out_
1645         out[0] = np.eye(n, m, k, dtype=self.dtype)
1646     def infer_shape(self, node, in_shapes):
1647         out_shape = [node.inputs[0], node.inputs[1]]
1648         return [out_shape]
1649     def grad(self, inp, grads):
1650         return [grad_undefined(self, i, inp[i]) for i in xrange(3)]
1651 def eye(n, m=None, k=0, dtype=None):
1652     """Return a 2-D array with ones on the diagonal and zeros elsewhere.
1653     Parameters
1654     ----------
1655     n : int
1656         Number of rows in the output.
1657     m : int, optional
1658         Number of columns in the output. If None, defaults to `N`.
1659     k : int, optional
1660         Index of the diagonal: 0 (the default) refers to the main diagonal,
1661         a positive value refers to an upper diagonal, and a negative value
1662         to a lower diagonal.
1663     dtype : data-type, optional
1664         Data-type of the returned array.
1665     Returns
1666     -------
1667     ndarray of shape (N,M)
1668         An array where all elements are equal to zero, except for the `k`-th
1669         diagonal, whose values are equal to one.
1670     """
1671     if dtype is None:
1672         dtype = config.floatX
1673     if m is None:
1674         m = n
1675     localop = Eye(dtype)
1676     return localop(n, m, k)
1677 def identity_like(x):
1678     return eye(x.shape[0], x.shape[1], k=0, dtype=x.dtype)
1679 def alloc_validate_shape(shape):
1680     sh = [as_tensor_variable(s) for s in shape]
1681     bcast = []
1682     for i, s in enumerate(sh):
1683         def err_str():
1684             if config.exception_verbosity == 'high':
1685                 return '\n' + min_informative_str(s)
1686             else:
1687                 return str(s)
1688         if s.type.dtype not in integer_dtypes:
1689             s_as_str = err_str()
1690             raise TypeError('Shape arguments to Alloc must be integers, '
1691                             'but argument %s is not for apply node: %s' %
1692                             (i, s_as_str))
1693         if s.ndim != 0:
1694             s_as_str = err_str()
1695             raise TypeError(
1696                 "Each shape dimension to Alloc must be a scalar, ",
1697                 'but dimension %s have %d dimensions for apply node: %s' %
1698                 (i, s.ndim, s_as_str))
1699         try:
1700             const_shp = get_scalar_constant_value(s)
1701         except NotScalarConstantError:
1702             const_shp = None
1703         bcast.append(1 == const_shp)
1704     return sh, bcast
1705 class Alloc(gof.Op):
1706     """Create a Tensor from an initial value and a desired shape.
1707     alloc(value, shape0, shape1, ..., shapeN)
1708     Returns an N-dimensional tensor initialized by `value` using something
1709     equivalent to
1710         z = numpy.zeros(shape, value.dtype)
1711         z += value
1712     The result has N dimensions, has the dtype of `value` and is obtained by
1713     broadcasting value over the output ndarray.
1714     This Op is used to replace fill() during optimizations because after shapes
1715     are lifted, the first argument to fill can often be pruned from the graph.
1716     """
1717     _f16_ok = True
1718     __props__ = ()
1719     def validate_shape(self, shape):
1720         return alloc_validate_shape(shape)
1721     def make_node(self, value, *shape):
1722         v = as_tensor_variable(value)
1723         sh, bcast = alloc_validate_shape(shape)
1724         if v.ndim &gt; len(sh):
1725             raise TypeError("The Alloc value to use has more dimensions"
1726                             " than the specified dimensions",
1727                             v.ndim, len(sh))
1728         otype = TensorType(dtype=v.dtype, broadcastable=bcast)
1729         return gof.Apply(self, [v] + sh, [otype()])
1730     def perform(self, node, inputs, out_):
1731         out, = out_
1732         v = inputs[0]
1733         sh = tuple([int(i) for i in inputs[1:]])
1734         if out[0] is None or out[0].shape != sh:
1735             if v.size == 1 and v.item() == 0:
1736                 out[0] = np.zeros(sh, dtype=v.dtype)
1737             else:
1738                 out[0] = np.empty(sh, dtype=v.dtype)
1739                 out[0][...] = v  # broadcast v to fill us up
1740         else:
1741             out[0][...] = v  # broadcast v to fill us up
1742     def c_code(self, node, name, inp, out, sub):
1743         vv = inp[0]
1744         ndim = len(inp[1:])
1745         zz, = out
1746         fail = sub['fail']
1747         code = """
1748             npy_intp shape[%(ndim)s];
1749             """ % dict(ndim=ndim)
1750         for i, shp_i in enumerate(inp[1:]):
1751             code += """
1752                 shape[%(i)s] = ((dtype_%(shp_i)s*) PyArray_DATA(%(shp_i)s))[0];
1753                 """ % dict(i=i, shp_i=shp_i)
1754         code += """
1755             int need_new_out = (NULL == %(zz)s);
1756             for (int i = 0; i &lt; %(ndim)s; i++)
1757                 need_new_out = (need_new_out
1758                                 || (PyArray_DIMS(%(zz)s)[i] != shape[i]));
1759             if (need_new_out)
1760             {
1761                 Py_XDECREF(%(zz)s);
1762                 %(zz)s = (PyArrayObject*) PyArray_SimpleNew(%(ndim)s,
1763                     shape, PyArray_TYPE((PyArrayObject*) py_%(vv)s));
1764                 if (!%(zz)s)
1765                 {
1766                     PyErr_SetString(PyExc_MemoryError, "alloc failed");
1767                     %(fail)s
1768                 }
1769             }
1770             // This function takes care of broadcasting
1771             if (PyArray_CopyInto(%(zz)s, %(vv)s) == -1)
1772               %(fail)s
1773             """ % dict(vv=vv, ndim=ndim, zz=zz, fail=fail)
1774         return code
1775     def c_code_cache_version(self):
1776         return (2,)
1777     def infer_shape(self, node, input_shapes):
1778         return [node.inputs[1:]]
1779     def connection_pattern(self, node):
1780         rval = [[True]]
1781         for ipt in node.inputs[1:]:
1782             rval.append([False])
1783         return rval
1784     def grad(self, inputs, grads):
1785         x = inputs[0]
1786         gz = grads[0]
1787         n_axes_to_sum = gz.ndim - x.ndim
1788         axis = list(range(n_axes_to_sum))
1789         axis_broadcasted = []
1790         axis_kept = []
1791         for i, (ib, gb) in enumerate(
1792             zip(inputs[0].broadcastable,
1793                 grads[0].broadcastable[-inputs[0].ndim:])):
1794             if ib and not gb:
1795                 axis_broadcasted.append(i + n_axes_to_sum)
1796             else:
1797                 axis_kept.append(i)
1798         gx = gz.sum(axis=axis + axis_broadcasted)
1799         if axis_broadcasted:
1800             new_order = ['x'] * x.ndim
1801             for idx, axis in enumerate(axis_kept):
1802                 new_order[axis] = idx
1803             gx = gx.dimshuffle(new_order)
1804         return [gx] + [DisconnectedType()() for i in inputs[1:]]
1805     def __call__(self, val, *shapes, **kwargs):
1806         """
1807         If the alloc would be useless, this function returns val.
1808         If this function is called outside of a graph optimization context
1809         (for instance, it is manually called by a user building a graph),
1810         then we always return an Alloc node, to allow for DebugMode to check
1811         for size mismatches.
1812         If you always want an Alloc node, call make_node.
1813         """
1814         ret = super(Alloc, self).__call__(val, *shapes, **kwargs)
1815         try:
1816             if hasattr(val, 'fgraph') and (val.type == ret.type):
1817                 return val
1818         except AttributeError:
1819             pass
1820         return ret
1821     def R_op(self, inputs, eval_points):
1822         if eval_points[0] is None:
1823             return [None]
1824         return self(eval_points[0], *inputs[1:], **dict(return_list=True))
1825     def do_constant_folding(self, node):
1826         if not getattr(node.outputs[0], 'clients', []):
1827             return False
1828         for client in node.outputs[0].clients:
1829             if client[0] == 'output':
1830                 return False
1831             elif (
1832                 client[1] == 0 and
1833                 isinstance(client[0].op, (
1834                     theano.tensor.subtensor.IncSubtensor,
1835                     theano.tensor.subtensor.AdvancedIncSubtensor1,
1836                     theano.tensor.subtensor.AdvancedIncSubtensor,
1837                     theano.tensor.blas.Gemv,
1838                     theano.tensor.blas_c.CGemv,
1839                     theano.tensor.blas.Ger,
1840                     theano.tensor.blas_c.CGer,
1841                     theano.tensor.blas_scipy.ScipyGer))):
1842                 return False
1843             elif client[0].op.__class__.__name__.lower().startswith("gpu"):
1844                 return False
1845         return True
1846 alloc = Alloc()
1847 pprint.assign(alloc, printing.FunctionPrinter('alloc'))
1848 def transfer(var, target):
1849     """
1850     Return a version of `var` transferred to `target`.
1851     `cpu` mean a TensorType (on the CPU).  Other types may define
1852     additional targets.
1853     Parameters
1854     ----------
1855     var : variable
1856         A theano variable
1857     target : str
1858         The target of the transfer
1859     """
1860     if target == 'cpu':
1861         return as_tensor_variable(var)
1862     else:
1863         for trans in transfer._others:
1864             res = trans(var, target)
1865             if res is not None:
1866                 return res
1867     raise ValueError("Can't transfer to target %s" % (target,))
1868 transfer._others = []
1869 def register_transfer(fn):
1870     """
1871     Register a transfer function for alternative targets.
1872     Parameters
1873     ----------
1874     fn : callable
1875     """
1876     transfer._others.append(fn)
1877 tensor_copy = elemwise.Elemwise(scal.identity)
1878 pprint.assign(tensor_copy, printing.IgnorePrinter())
1879 @constructor
1880 def sum(input, axis=None, dtype=None, keepdims=False, acc_dtype=None):
1881     """
1882     Computes the sum along the given axis(es) of a tensor `input`.
1883     When axis is None (the default value), the sum is performed
1884     over the flattened tensor.
1885     For full documentation see ``tensor.elemwise.Sum``.
1886     In particular please pay attention to the important warning when using
1887     a custom acc_dtype.
1888     Parameters
1889     ----------
1890     keepdims: bool
1891         If this is set to True, the axes which are reduced are left in
1892         the result as dimensions with size one. With this option, the result
1893         will broadcast correctly against the original tensor.
1894     """
1895     out = elemwise.Sum(axis=axis, dtype=dtype, acc_dtype=acc_dtype)(input)
1896     if keepdims:
1897         out = makeKeepDims(input, out, axis)
1898     return out
1899 pprint.assign(Sum(), printing.FunctionPrinter('sum'))
1900 @constructor
1901 def prod(input, axis=None, dtype=None, keepdims=False, acc_dtype=None,
1902          no_zeros_in_input=False):
1903     """
1904     Computes the product along the given axis(es) of a tensor `input`.
1905     When axis is None (the default value), the product is performed
1906     over the flattened tensor.
1907     For full documentation see ``tensor.elemwise.Prod``.
1908     Parameters
1909     ----------
1910     keepdims: bool
1911         If this is set to True, the axes which are reduced are left in
1912         the result as dimensions with size one. With this option, the result
1913         will broadcast correctly against the original tensor.
1914     """
1915     out = elemwise.Prod(axis, dtype=dtype, acc_dtype=acc_dtype,
1916                         no_zeros_in_input=no_zeros_in_input)(input)
1917     if keepdims:
1918         out = makeKeepDims(input, out, axis)
1919     return out
1920 class Mean(elemwise.CAReduce):
1921     def __init__(self, axis=None):
1922         elemwise.CAReduce.__init__(self, scal.add, axis)
1923         assert self.axis is None or len(self.axis) == 1
1924     def __str__(self):
1925         if self.axis is not None:
1926             return "Mean{%s}" % (", ".join(str(x) for x in self.axis))
1927         else:
1928             return "Mean"
1929     def _output_dtype(self, idtype):
1930         return 'float64'
1931     def perform(self, node, inp, out):
1932         input, = inp
1933         output, = out
1934         if self.axis is None:
1935             axis = None
1936         else:
1937             axis = self.axis[0]
1938         output[0] = np.asarray(np.mean(input, dtype='float64',
1939                                        axis=axis))
1940     def c_code(self, node, name, inames, onames, sub):
1941         if self.axis is not None:
1942             return super(Op, self).c_code(node, name, inames, onames, sub)
1943         ret = elemwise.CAReduce.c_code(self, node, name, inames, onames, sub)
1944         return ret + """
1945   *((double *)PyArray_DATA(%s)) /= PyArray_SIZE(%s);
1946   """ % (onames[0], inames[0])
1947 @constructor
1948 def mean(input, axis=None, dtype=None, op=False, keepdims=False,
1949          acc_dtype=None):
1950     """
1951     Computes the mean value along the given axis(es) of a tensor `input`.
1952     Parameters
1953     ----------
1954     axis : None or int or (list of int) (see `Sum`)
1955         Compute the mean along this axis of the tensor.
1956         None means all axes (like numpy).
1957     dtype: None or string
1958         Dtype to cast the result of the inner summation into.
1959         For instance, by default, a sum of a float32 tensor will be
1960         done in float64 (acc_dtype would be float64 by default),
1961         but that result will be casted back in float32.
1962     keepdims: bool
1963         If this is set to True, the axes which are reduced are
1964         left in the result as dimensions with size one. With this option,
1965         the result will broadcast correctly against the original tensor.
1966     acc_dtype: None or string
1967         Dtype to use for the inner summation. This will not
1968         necessarily be the dtype of the output (in particular
1969         if it is a discrete (int/uint) dtype, the output will
1970         be in a float type). If None, then we use the same rules as `sum()`.
1971     Notes
1972     -----
1973     For gpu, if you specify dtype=float32, everything will be done on the gpu.
1974     """
1975     input = as_tensor_variable(input)
1976     if op:
1977         if dtype not in (None, 'float64'):
1978             raise NotImplementedError(
1979                 'The Mean op does not support the dtype argument, '
1980                 'and will always use float64. If you want to specify '
1981                 'the dtype, call tensor.mean(..., op=False).',
1982                 dtype)
1983         if acc_dtype not in (None, 'float64'):
1984             raise NotImplementedError(
1985                 'The Mean op does not support the acc_dtype argument, '
1986                 'and will always use float64. If you want to specify '
1987                 'acc_dtype, call tensor.mean(..., op=False).',
1988                 dtype)
1989         out = Mean(axis)(input)
1990         if keepdims:
1991             out = makeKeepDims(input, out, axis)
1992         return out
1993     if dtype is not None:
1994         sum_dtype = dtype
1995     else:
1996         sum_dtype = None
1997         if input.dtype == 'float16':
1998             sum_dtype = 'float32'
1999     s = sum(input, axis=axis, dtype=sum_dtype, keepdims=keepdims,
2000             acc_dtype=acc_dtype)
2001     shp = shape(input)
2002     if s.dtype in ('float16', 'float32', 'complex64'):
2003         shp = cast(shp, 'float32')
2004     else:
2005         shp = cast(shp, 'float64')
2006     if axis is None:
2007         axis = list(range(input.ndim))
2008     elif isinstance(axis, (integer_types, np.integer)):
2009         axis = [axis]
2010     elif isinstance(axis, np.ndarray) and axis.ndim == 0:
2011         axis = [int(axis)]
2012     else:
2013         axis = [int(a) for a in axis]
2014     for i in axis:
2015         s = true_div(s, shp[i])
2016     if s.dtype != shp.dtype and s.dtype in discrete_dtypes:
2017         s = cast(s, shp.dtype)
2018     if dtype == 'float16' or (dtype is None and input.dtype == 'float16'):
2019         s = cast(s, 'float16')
2020     s.name = 'mean'
2021     return s
2022 @constructor
2023 def var(input, axis=None, ddof=0, keepdims=False, corrected=False):
2024     """
2025     Computes the variance along the given axis(es) of a tensor `input`.
2026     Parameters
2027     ----------
2028     axis: None or int or (list of int) (see `Sum`)
2029         Compute the variance along this axis of the tensor.
2030         None means all axes (like numpy).
2031     ddof: Degrees of freedom; 0 would compute the ML estimate, 1 would compute
2032         the unbiased estimate.
2033     keepdims : bool
2034         If this is set to True, the axes which are reduced are
2035         left in the result as dimensions with size one. With this option,
2036         the result will broadcast correctly against the original tensor.
2037     corrected : bool
2038         If this is set to True, the 'corrected_two_pass' algorithm is
2039         used to compute the variance.
2040         Refer : http://www.cs.yale.edu/publications/techreports/tr222.pdf
2041     Notes
2042     -----
2043     Default uses the two-pass algorithm (reference below).
2044     https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm
2045     Also supports 'corrected_two_pass' algorithm (using the 'corrected' flag)
2046     which is numerically more stable. There exist other implementations that
2047     offer better stability, but probably slower.
2048     """
2049     if isinstance(ddof, (bool)):
2050         raise ValueError('Parameter keepdims is now at index 3: (input, \
2051                           axis=None, ddof=0, keepdims=False, corrected=False)')
2052     input_ndim = input.type.ndim
2053     if axis is None:
2054         axis = list(range(input_ndim))
2055     elif isinstance(axis, (integer_types, np.integer)):
2056         axis = [axis]
2057     elif isinstance(axis, np.ndarray) and axis.ndim == 0:
2058         axis = [int(axis)]
2059     else:
2060         axis = [int(a) for a in axis]
2061     mean_input = mean(input, axis, keepdims=True)
2062     centered_input = input - mean_input
2063     two = constant(2, dtype=centered_input.dtype)
2064     if ddof == 0:
2065         v = mean((centered_input ** two), axis, keepdims=keepdims)
2066     else:
2067         shp = shape(input) - ddof
2068         v = sum((centered_input ** two), axis=axis, keepdims=keepdims)
2069         for i in axis:
2070             v = true_div(v, shp[i])
2071     if corrected:
2072         if ddof == 0:
2073             error = mean(centered_input, axis, keepdims=keepdims) ** 2
2074         else:
2075             shp = shape(input) - ddof
2076             shp_inp = shape(input)
2077             error = sum(centered_input, axis=axis, keepdims=keepdims) ** 2
2078             for i in axis:
2079                 error = true_div(error, shp[i] * shp_inp[i])
2080         v = v - error
2081     v.name = 'var'
2082     return v
2083 @constructor
2084 def std(input, axis=None, ddof=0, keepdims=False, corrected=False):
2085     """
2086     Computes the standard deviation along the given axis(es) of a tensor `input`.
2087     Parameters
2088     ----------
2089     axis: None or int or (list of int) (see `Sum`)
2090         Compute the variance along this axis of the tensor.
2091         None means all axes (like numpy).
2092     ddof: Degrees of freedom; 0 would compute the ML estimate, 1 would compute
2093         the unbiased estimate.
2094     keepdims : bool
2095         If this is set to True, the axes which are reduced are
2096         left in the result as dimensions with size one. With this option,
2097         the result will broadcast correctly against the original tensor.
2098     corrected : bool
2099         If this is set to True, the 'corrected_two_pass' algorithm is
2100         used to compute the variance.
2101         Refer : http://www.cs.yale.edu/publications/techreports/tr222.pdf
2102     Notes
2103     -----
2104     It calls 'var()' and 'var()' uses the two-pass algorithm (reference below).
2105     https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm
2106     Function 'var()' also supports 'corrected_two_pass' algorithm (using the
2107     'corrected' flag) which is numerically more stable. There exist other
2108     implementations that offer better stability, but probably slower.
2109     """
2110     if isinstance(ddof, (bool)):
2111         raise ValueError('Parameter keepdims is now at index 3: (input, \
2112                           axis=None, ddof=0, keepdims=False, corrected=False)')
2113     ret = sqrt(var(input=input, axis=axis, ddof=ddof,
2114                    keepdims=keepdims, corrected=corrected))
2115     ret.name = 'std'
2116     return ret
2117 class Default(gof.Op):
2118     """
2119     Takes an input x and a default value.
2120     If the input is not None, a reference to it is returned.
2121     If the input is None, a copy of the default value is returned instead.
2122     The input and the default must have exactly the same type.
2123     """
2124     view_map = {0: [0]}
2125     __props__ = ()
2126     def make_node(self, x, default):
2127         x, default = as_tensor_variable(x), as_tensor_variable(default)
2128         if x.type != default.type:
2129             raise TypeError('Both default() arguments must have same type',
2130                             x, default)
2131         return gof.Apply(self, [x, default], [default.type()])
2132     def perform(self, node, inp, out_):
2133         x, default = inp
2134         out, = out_
2135         if x is None:
2136             out[0] = default.copy()
2137         else:
2138             out[0] = x
2139 default = Default()
2140 setdefault = default  # legacy
2141 @_scal_elemwise
2142 def maximum(x, y):
2143 @_scal_elemwise
2144 def minimum(x, y):
2145 def div_proxy(x, y):
2146     f = scal.int_or_true_div(
2147         as_tensor_variable(x).dtype in discrete_dtypes,
2148         as_tensor_variable(y).dtype in discrete_dtypes)
2149     if f is scal.int_div:
2150         return int_div(x, y)
2151     else:
2152         return true_div(x, y)
2153 def divmod(x, y):
2154     return floor_div(x, y), mod_check(x, y)
2155 @_scal_elemwise
2156 def add(a, *other_terms):
2157 @_scal_elemwise
2158 def sub(a, b):
2159 @_scal_elemwise
2160 def mul(a, *other_terms):
2161 @_scal_elemwise
2162 def true_div(a, b):
2163 @_scal_elemwise
2164 def int_div(a, b):
2165 floor_div = int_div
2166 def ceil_intdiv(a, b):
2167     """
2168     Safely compute ceil(float_division(a, b)).
2169     Works for all dtypes, but mostly useful when a and b are int.
2170     """
2171     div = int_div(a, b)
2172     ret = cast(neq(a % b, 0), div.dtype) + div
2173     assert ret.dtype == scal.upcast(div.owner.inputs[0], div.owner.inputs[1])
2174     return ret
2175 def mod_check(x, y):
2176     if ((as_tensor_variable(x).dtype in complex_dtypes or
2177          as_tensor_variable(y).dtype in complex_dtypes)):
2178         raise scal.Mod.complex_error
2179     else:
2180         return mod(x, y)
2181 @_scal_elemwise
2182 def mod(a, b):
2183 @_scal_elemwise
2184 def pow(a, b):
2185 @_scal_elemwise
2186 def clip(x, min, max):
2187     """
2188     Clip x to be between min and max.
2189     Notes
2190     -----
2191     When `x` is equal to the boundaries, the output is considered
2192     to be `x`, so at these points, the gradient of the cost wrt the output
2193     will be propagated to `x`, not to `min` nor `max`. In other words,
2194     on these points, the gradient wrt `x` will be equal to the gradient wrt
2195     the output, and the gradient wrt `min` and `max` will be zero.
2196     """
2197 pprint.assign(add, printing.OperatorPrinter('+', -2, 'either'))
2198 pprint.assign(mul, printing.OperatorPrinter('*', -1, 'either'))
2199 pprint.assign(sub, printing.OperatorPrinter('-', -2, 'left'))
2200 pprint.assign(neg, printing.OperatorPrinter('-', 0, 'either'))
2201 pprint.assign(true_div, printing.OperatorPrinter('/', -1, 'left'))
2202 pprint.assign(int_div, printing.OperatorPrinter('//', -1, 'left'))
2203 pprint.assign(pow, printing.OperatorPrinter('**', 1, 'right'))
2204 def extract_constant(x, elemwise=True, only_process_constants=False):
2205     """
2206     This function is basically a call to tensor.get_scalar_constant_value.
2207     The main difference is the behaviour in case of failure. While
2208     get_scalar_constant_value raises an TypeError, this function returns x,
2209     as a tensor if possible. If x is a ScalarVariable from a
2210     scalar_from_tensor, we remove the conversion. If x is just a
2211     ScalarVariable, we convert it to a tensor with tensor_from_scalar.
2212     """
2213     try:
2214         x = get_scalar_constant_value(x,
2215                                       elemwise,
2216                                       only_process_constants)
2217     except NotScalarConstantError:
2218         pass
2219     if ((isinstance(x, scal.ScalarVariable) or
2220          isinstance(x, scal.sharedvar.ScalarSharedVariable))):
2221         if x.owner and isinstance(x.owner.op, ScalarFromTensor):
2222             x = x.owner.inputs[0]
2223         else:
2224             x = tensor_from_scalar(x)
2225     return x
2226 def transpose(x, axes=None):
2227     """
2228     Reorder the dimensions of x. (Default: reverse them)
2229     This is a macro around dimshuffle that matches the numpy.transpose function.
2230     """
2231     if axes is None:
2232         axes = list(range((x.ndim - 1), -1, -1))
2233     ret = DimShuffle(x.broadcastable, axes)(x)
2234     if x.name and axes == list(range((x.ndim - 1), -1, -1)):
2235         ret.name = x.name + '.T'
2236     return ret
2237 def batched_dot(a, b):
2238     """
2239     Compute the batched dot product of two variables:
2240         batched_dot(a, b)[i] = dot(a[i], b[i])
2241     Note that this batched_dot function does one of three things, in the
2242     following sequence:
2243         1.  If either a or b is a vector, it returns the batched elementwise
2244             product without calling the Theano BatchedDot op.
2245         2.  If both a and b have either 2 or 3 dimensions, it calls Theano's
2246             BatchedDot op on a and b.
2247         3.  If either a or b has more than 3 dimensions, it calls Theano's
2248             batched_tensordot function with appropriate axes. The
2249             batched_tensordot function expresses high-dimensional batched
2250             dot products in terms of batched matrix-matrix dot products, so
2251             it may be possible to futherize optimize for performance.
2252     """
2253     a, b = as_tensor_variable(a), as_tensor_variable(b)
2254     if a.ndim == 0:
2255         raise TypeError("a must have at least one (batch) axis")
2256     elif b.ndim == 0:
2257         raise TypeError("b must have at least one (batch) axis")
2258     elif a.ndim == 1:
2259         return a.dimshuffle(*([0] + ["x"] * (b.ndim - 1))) * b
2260     elif b.ndim == 1:
2261         return a * b.dimshuffle(*([0] + ["x"] * (a.ndim - 1)))
2262     elif a.ndim &gt; 3 or b.ndim &gt; 3:
2263         return batched_tensordot(
2264             a, b, [[a.ndim - 1], [np.maximum(1, b.ndim - 2)]])
2265     else:
2266         return theano.tensor.blas.BatchedDot()(a, b)
2267 def batched_tensordot(x, y, axes=2):
2268     """
2269     Compute a batched tensordot product.
2270     A hybrid of batched_dot and tensordot, this function computes the
2271     tensordot product between the two tensors, by iterating over the
2272     first dimension to perform a sequence of tensordots.
2273     Parameters
2274     ----------
2275     x : tensor
2276         A Tensor with sizes e.g.: for 3D (dim1, dim3, dim2)
2277     y : tensor
2278         A Tensor with sizes e.g.: for 3D (dim1, dim2, dim4)
2279     axes: int or array-like of length 2
2280         If an integer, the number of axes to sum over.
2281         If an array, it must have two array elements containing the axes to sum
2282         over in each tensor.
2283         If an integer i, it is converted to an array containing
2284         the last i dimensions of the first tensor and the first
2285         i dimensions of the second tensor (excluding the first
2286         (batch) dimension):
2287             axes = [list(range(a.ndim - i, b.ndim)), list(range(1,i+1))]
2288         If an array, its two elements must contain compatible axes
2289         of the two tensors. For example, [[1, 2], [2, 4]] means sum
2290         over the 2nd and 3rd axes of a and the 3rd and 5th axes of b.
2291         (Remember axes are zero-indexed!) The 2nd axis of a and the
2292         3rd axis of b must have the same shape; the same is true for
2293         the 3rd axis of a and the 5th axis of b.
2294     Like tensordot, this function uses a series of dimshuffles and
2295     reshapes to reduce the tensor dot product to a matrix or vector
2296     dot product.  Finally, it calls batched_dot to compute the result.
2297     """
2298     return _tensordot_as_dot(x, y, axes, dot=batched_dot, batched=True)
2299 def split(x, splits_size, n_splits, axis=0):
2300     the_split = Split(n_splits)
2301     return the_split(x, axis, splits_size)
2302 class Split(Op):
2303     """Partition a `TensorVariable` along some axis.
2304     Examples
2305     --------
2306     &gt;&gt;&gt; x = vector()
2307     &gt;&gt;&gt; splits = lvector()
2308     You have to declare right away how many split_points there will be.
2309     &gt;&gt;&gt; ra, rb, rc = split(x, splits, n_splits = 3, axis = 0)
2310     &gt;&gt;&gt; f = function([x, splits], [ra, rb, rc])
2311     &gt;&gt;&gt; a, b, c = f([0,1,2,3,4,5], [3, 2, 1])
2312     a == [0,1,2]
2313     b == [3, 4]
2314     c == [5]
2315     """
2316     len_splits = None
2317     """A Split instance will have this many outputs, and require that
2318     the splits argument to `perform` have exactly this many elements.
2319     """
2320     __props__ = ("len_splits",)
2321     def __init__(self, len_splits):
2322         self.len_splits = int(len_splits)
2323     def __str__(self):
2324         return self.__class__.__name__ + "{%s}" % self.len_splits
2325     def make_node(self, x, axis, splits):
2326         x = as_tensor_variable(x)
2327         axis = as_tensor_variable(axis)
2328         splits = as_tensor_variable(splits)
2329         if splits.type not in int_vector_types:
2330             raise TypeError('splits must have type tensor.lvector',
2331                             splits.type)
2332         if axis.type not in int_types:
2333             raise TypeError('axis must have type lscalar', axis.type)
2334         inputs = [x, axis, splits]
2335         outputs = [x.type() for i in xrange(self.len_splits)]
2336         return Apply(self, inputs, outputs)
2337     def perform(self, node, inputs, outputs):
2338         x, axis, splits = inputs
2339         if sys.version_info[0:2] == (2, 4) and axis.size == 1:
2340             axis = int(axis)
2341         try:
2342             len_along_axis = x.shape[axis]
2343         except Exception:
2344             raise ValueError('Split.perform() with axis=(%s) is invalid'
2345                              ' for x.shape==(%s)'
2346                              % (axis, x.shape))
2347         if len(splits) != self.len_splits:
2348             raise ValueError('In Split.perform(), len(splits) != len_splits.',
2349                              (len(splits), self.len_splits))
2350         if np.sum(splits) != len_along_axis:
2351             raise ValueError('The splits sum to %s, expected %s' %
2352                              (np.sum(splits), len_along_axis))
2353         if python_any([nb &lt; 0 for nb in splits]):
2354             raise ValueError('Split: you tried to make an ndarray with a '
2355                              'negative number of elements.')
2356         general_key = [slice(None, None, None) for s in x.shape]
2357         lower_idx = 0
2358         for i in xrange(self.len_splits):
2359             upper_idx = lower_idx + splits[i]
2360             general_key[axis] = slice(lower_idx, upper_idx, None)
2361             outputs[i][0] = x.__getitem__(tuple(general_key)).copy()
2362             lower_idx = upper_idx
2363     def infer_shape(self, node, in_shapes):
2364         axis = node.inputs[1]
2365         splits = node.inputs[2]
2366         shp_x, shp_axis, shp_splits = in_shapes
2367         out_shapes = []
2368         for i in xrange(self.len_splits):
2369             temp = as_tensor_variable(shp_x)
2370             temp = theano.tensor.subtensor.set_subtensor(temp[axis], splits[i])
2371             temp = [temp[i] for i in xrange(len(shp_x))]
2372             out_shapes.append(temp)
2373         return out_shapes
2374     def grad(self, inputs, g_outputs):
2375         x, axis, n = inputs
2376         outputs = self(*inputs, **dict(return_list=True))
2377         if python_all([isinstance(g.type, DisconnectedType)
2378                        for g in g_outputs]):
2379             return [DisconnectedType()(),
2380                     grad_undefined(self, 1, axis),
2381                     grad_undefined(self, 2, n)]
2382         new_g_outputs = []
2383         for o, g in zip(outputs, g_outputs):
2384             if isinstance(g.type, DisconnectedType):
2385                 new_g_outputs.append(o.zeros_like())
2386             else:
2387                 new_g_outputs.append(g)
2388         return [join(axis, *new_g_outputs),
2389                 grad_undefined(self, 1, axis),
2390                 grad_undefined(self, 2, n)]
2391     def R_op(self, inputs, eval_points):
2392         if eval_points[0] is None:
2393             return [None for i in self.len_splits]
2394         return self.make_node(eval_points[0], *inputs[1:]).outputs
2395     def c_code_cache_version(self):
2396         return (2,)
2397     def c_support_code(self):
2398         return """
2399         /* Return 1 if output has the correct shape. */
2400         int split_output_shape_is_correct (
2401             PyArrayObject* output, PyArrayObject* array_to_split, int axis_to_split, npy_intp split_size
2402         ) {
2403             return
2404                 PyArray_NDIM(output) == PyArray_NDIM(array_to_split)
2405                 &amp;&amp; memcmp(
2406                     PyArray_DIMS(output),
2407                     PyArray_DIMS(array_to_split),
2408                     axis_to_split * sizeof(npy_intp)
2409                 ) == 0
2410                 &amp;&amp; memcmp(
2411                     PyArray_DIMS(output) + axis_to_split + 1,
2412                     PyArray_DIMS(array_to_split) + axis_to_split + 1,
2413                     (PyArray_NDIM(array_to_split) - axis_to_split - 1) * sizeof(npy_intp)
2414                 ) == 0
2415                 &amp;&amp; split_size == PyArray_DIM(output, axis_to_split);
2416         }
2417         """
2418     def c_code(self, node, name, inputs, outputs, sub):
2419         if self.len_splits == 0:
2420             return ''
2421         outputs_pointers = '&amp;' + (', &amp;'.join(outputs))
2422         x, axis, splits = inputs
2423         fail = sub['fail']
2424         x_typenum = np.dtype(node.inputs[0].dtype).num
2425         x_itemsize = np.dtype(node.inputs[0].dtype).itemsize
2426         axis_dtype = node.inputs[1].type.dtype_specs()[1]
2427         splits_dtype = node.inputs[2].type.dtype_specs()[1]
2428         expected_splits_count = self.len_splits
2429         return """
2430         int ndim = PyArray_NDIM(%(x)s);
2431         int axis = (int)(*(%(axis_dtype)s*)PyArray_GETPTR1(%(axis)s, 0));
2432         int splits_count = PyArray_DIM(%(splits)s, 0);
2433         npy_intp len_along_axis, sum_of_splits = 0, current_split_length = 0, current_split_start = 0;
2434         npy_intp* split_dims = NULL;
2435         PyObject* split_view = NULL;
2436         npy_intp data_offset;
2437         int i;
2438         PyArrayObject** outputs[] = {%(outputs_pointers)s};
2439         /* Check inputs. */
2440         if (splits_count != %(expected_splits_count)s) {
2441             PyErr_Format(PyExc_ValueError,
2442                 "Split: splits count (%%d) != expected count (%%d).", splits_count, %(expected_splits_count)s);
2443             %(fail)s
2444         }
2445         if (axis &lt; 0) {
2446             axis += ndim;
2447         }
2448         if (axis &lt; 0 || axis &gt;= ndim) {
2449             PyErr_Format(PyExc_IndexError, "Split: invalid axis %%d for a %%d-D array.", axis, ndim);
2450             %(fail)s
2451         }
2452         len_along_axis = PyArray_DIM(%(x)s, axis);
2453         for (i = 0; i &lt; splits_count; ++i) {
2454             current_split_length = (npy_intp)(*(%(splits_dtype)s*)PyArray_GETPTR1(%(splits)s, i));
2455             if (current_split_length &lt; 0) {
2456                 PyErr_Format(PyExc_ValueError,
2457                     "Split: you try to take a negative number (%%ld) of elements.", current_split_length);
2458                 %(fail)s
2459             }
2460             sum_of_splits += current_split_length;
2461         }
2462         if (sum_of_splits != len_along_axis) {
2463             PyErr_Format(PyExc_ValueError, "Split: the splits sums to %%ld, expected %%ld.", sum_of_splits, len_along_axis);
2464             %(fail)s
2465         }
2466         /* Check outputs. */
2467         split_dims = (npy_intp*) malloc(ndim * sizeof(npy_intp));
2468         if (split_dims == NULL) {
2469             PyErr_NoMemory();
2470             %(fail)s
2471         }
2472         memcpy(split_dims, PyArray_DIMS(%(x)s), ndim * sizeof(npy_intp));
2473         for (i = 0; i &lt; splits_count; ++i) {
2474             PyArrayObject** output = outputs[i];
2475             current_split_length = (npy_intp) (* (%(splits_dtype)s*) PyArray_GETPTR1(%(splits)s, i));
2476             if (*output == NULL || !split_output_shape_is_correct(*output, %(x)s, axis, current_split_length)) {
2477                 Py_XDECREF(*output);
2478                 split_dims[axis] = current_split_length;
2479                 *output = (PyArrayObject*)PyArray_EMPTY(ndim, split_dims, %(x_typenum)s, PyArray_IS_F_CONTIGUOUS(%(x)s));
2480                 if (outputs == NULL) {
2481                     PyErr_SetString(PyExc_RuntimeError, "Split: unable to allocate an output.");
2482                     free(split_dims);
2483                     %(fail)s
2484                 }
2485             }
2486         }
2487         /* Compute split. */
2488         for (i = 0; i &lt; splits_count; ++i) {
2489             current_split_length = (npy_intp) (* (%(splits_dtype)s*) PyArray_GETPTR1(%(splits)s, i));
2490             data_offset = PyArray_STRIDE(%(x)s, axis) * current_split_start;
2491             split_dims[axis] = current_split_length;
2492             split_view = PyArray_New(&amp;PyArray_Type,
2493                                     ndim, split_dims,
2494                                     %(x_typenum)s,
2495                                     PyArray_STRIDES(%(x)s),
2496                                     PyArray_BYTES(%(x)s) + data_offset,
2497                                     %(x_itemsize)s,
2498                                     PyArray_FLAGS(%(x)s),
2499                                     NULL);
2500             if (split_view == NULL) {
2501                 PyErr_SetString(PyExc_RuntimeError, "Split: unable to create a view for a split.");
2502                 free(split_dims);
2503                 %(fail)s
2504             }
2505             if (PyArray_CopyInto(*outputs[i], (PyArrayObject*)split_view) != 0) {
2506                 PyErr_SetString(PyExc_RuntimeError, "Split: unable to copy a split view into the output.");
2507                 Py_XDECREF(split_view);
2508                 free(split_dims);
2509                 %(fail)s
2510             }
2511             Py_XDECREF(split_view);
2512             current_split_start += current_split_length;
2513         }
2514         free(split_dims);
2515         """ % locals()
2516 def addbroadcast(x, *axes):
2517     """
2518     Make the input broadcastable in the specified axes.
2519     For example, addbroadcast(x, 0) will make the first dimension of
2520     x broadcastable. When performing the function, if the length of
2521     x along that dimension is not 1, a ValueError will be raised.
2522     We apply the opt here not to pollute the graph especially during
2523     the gpu optimization
2524     Parameters
2525     ----------
2526     x : tensor_like
2527         Input theano tensor.
2528     axis : an int or an iterable object such as list or tuple of int values
2529         The dimension along which the tensor x should be broadcastable.
2530         If the length of x along these dimensions is not 1, a ValueError will
2531         be raised.
2532     Returns
2533     -------
2534     tensor
2535         A theano tensor, which is broadcastable along the specified dimensions.
2536     """
2537     rval = Rebroadcast(*[(axis, True) for axis in axes])(x)
2538     return theano.tensor.opt.apply_rebroadcast_opt(rval)
2539 def unbroadcast(x, *axes):
2540     """
2541     Make the input impossible to broadcast in the specified axes.
2542     For example, addbroadcast(x, 0) will make the first dimension
2543     of x broadcastable. When performing the function, if the length
2544     of x along that dimension is not 1, a ValueError will be raised.
2545     We apply the opt here not to pollute the graph especially during
2546     the gpu optimization
2547     Parameters
2548     ----------
2549     x : tensor_like
2550         Input theano tensor.
2551     axis : an int or an iterable object such as list or tuple of int values
2552         The dimension along which the tensor x should be unbroadcastable.
2553         If the length of x along these dimensions is not 1, a ValueError will
2554         be raised.
2555     Returns
2556     -------
2557     tensor
2558         A theano tensor, which is unbroadcastable along the specified dimensions.
2559     """
2560     rval = Rebroadcast(*[(axis, False) for axis in axes])(x)
2561     return theano.tensor.opt.apply_rebroadcast_opt(rval)
2562 def patternbroadcast(x, broadcastable):
2563     """
2564     Make the input adopt a specific broadcasting pattern.
2565     Broadcastable must be iterable. For example,
2566     patternbroadcast(x, (True, False)) will make the first
2567     dimension of x broadcastable and the second dimension
2568     not broadcastable, so x will now be a row.
2569     We apply the opt here not to pollute the graph especially during the gpu
2570     optimization.
2571     Parameters
2572     ----------
2573     x : tensor_like
2574         Input theano tensor.
2575     broadcastable : an iterable object such as list or tuple of bool values
2576         A set of boolean values indicating whether a dimension should be
2577         broadcastable or not. If the length of x along these dimensions is
2578         not 1, a ValueError will be raised.
2579     Returns
2580     -------
2581     tensor
2582         A theano tensor, which is unbroadcastable along the specified dimensions.
2583     """
2584     rval = Rebroadcast(*[(i, broadcastable[i])
2585                          for i in xrange(len(broadcastable))])(x)
2586     return theano.tensor.opt.apply_rebroadcast_opt(rval)
2587 class Join(Op):
2588     """
2589     Concatenate multiple `TensorVariable`s along some axis.
2590     The axis must be given as first argument. All tensors must have the same
2591     shape along all dimensions other than this axis.
2592     Of course, TensorVariable instances do not have a shape, so this error
2593     cannot be caught until runtime.  See `perform()`.
2594     See Also
2595     --------
2596     stack : For joins involving scalar values
2597     Examples
2598     --------
2599     &gt;&gt;&gt; x, y, z = tensor.matrix(), tensor.matrix(), tensor.matrix()
2600     &gt;&gt;&gt; u = tensor.vector()
2601     &gt;&gt;&gt; r = join(0, x, y, z)
2602     &gt;&gt;&gt; c = join(1, x, y, z)
2603     &gt;&gt;&gt; join(2, x, y, z)    # WRONG: the axis has to be an index into the shape
2604     &gt;&gt;&gt; join(0, x, u)       # WRONG: joined tensors must have the same rank
2605     """
2606     check_input = False
2607     __props__ = ("view",)
2608     def __init__(self, view=-1):
2609         self.view = view
2610         if view != -1:
2611             self.view_map = {0: [1 + view]}
2612     def __str__(self):
2613         if self.view == -1:
2614             return self.__class__.__name__
2615         else:
2616             return "%s{%s}" % (
2617                 self.__class__.__name__,
2618                 ", ".join("%s=%r" % (p, getattr(self, p))
2619                           for p in self.__props__))
2620     def __setstate__(self, d):
2621         self.__dict__.update(d)
2622         if not hasattr(self, "view"):
2623             self.view = -1
2624     def make_node(self, *axis_and_tensors):
2625         """
2626         Parameters
2627         ----------
2628         axis: an Int or integer-valued Variable
2629         tensors
2630             A variable number (but not zero) of tensors to
2631             concatenate along the specified axis.  These tensors must have
2632             the same shape along all dimensions other than this axis.
2633         Returns
2634         -------
2635         A symbolic Variable
2636             It has the same ndim as the input tensors, and the most inclusive
2637             dtype.
2638         """
2639         axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]
2640         if not tensors:
2641             raise ValueError('Cannot join an empty list of tensors')
2642         as_tensor_variable_args = [as_tensor_variable(x) for x in tensors]
2643         dtypes = [x.type.dtype for x in as_tensor_variable_args]
2644         out_dtype = scal.upcast(*dtypes)
2645         def output_maker(bcastable):
2646             return tensor(dtype=out_dtype, broadcastable=bcastable)
2647         return self._make_node_internal(
2648             axis, tensors, as_tensor_variable_args, output_maker)
2649     def _make_node_internal(self, axis, tensors,
2650                             as_tensor_variable_args, output_maker):
2651         if not python_all(targs.type.ndim for targs
2652                           in as_tensor_variable_args):
2653             raise TypeError('Join cannot handle arguments of dimension 0.'
2654                             ' For joining scalar values, see @stack')
2655         if len(as_tensor_variable_args) == 1:
2656             bcastable = list(as_tensor_variable_args[0].type.broadcastable)
2657         else:
2658             bcastable = [False] * len(
2659                 as_tensor_variable_args[0].type.broadcastable)
2660             ndim = len(bcastable)
2661             if not isinstance(axis, integer_types):
2662                 try:
2663                     axis = int(get_scalar_constant_value(axis))
2664                 except NotScalarConstantError:
2665                     pass
2666             if isinstance(axis, integer_types):
2667                 if axis &lt; -ndim:
2668                     raise IndexError("Join axis %d out of bounds [0, %d)" %
2669                                      (axis, ndim))
2670                 if axis &lt; 0:
2671                     axis += ndim
2672                 for x in as_tensor_variable_args:
2673                     for current_axis, bflag in enumerate(x.type.broadcastable):
2674                         if current_axis == axis:
2675                             continue
2676                         if bflag:
2677                             bcastable[current_axis] = True
2678                 try:
2679                     bcastable[axis] = False
2680                 except IndexError:
2681                     raise ValueError('Join argument "axis" is out of range'
2682                                      ' (given input dimensions)')
2683             else:
2684                 bcastable = [False] * len(
2685                     as_tensor_variable_args[0].type.broadcastable)
2686         if not python_all([x.ndim == len(bcastable)
2687                            for x in as_tensor_variable_args[1:]]):
2688             raise TypeError("Join() can only join tensors with the same "
2689                             "number of dimensions.")
2690         inputs = [as_tensor_variable(axis)] + list(as_tensor_variable_args)
2691         if inputs[0].type not in int_types:
2692             raise TypeError('Axis could not be cast to an integer type',
2693                             axis, inputs[0].type, int_types)
2694         outputs = [output_maker(bcastable)]
2695         node = Apply(self, inputs, outputs)
2696         return node
2697     def perform(self, node, axis_and_tensors, out_):
2698         out, = out_
2699         view = self.view
2700         axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]
2701         if (view != -1) and np.all(
2702                 [tensor.shape[axis] == 0 for tensor in
2703                  tensors[0:view] + tensors[view + 1:]]):
2704             out[0] = tensors[view]
2705         else:
2706             ndim = tensors[0].ndim
2707             if axis &lt; -ndim:
2708                 raise IndexError("Join axis %d out of bounds [0, %d)" %
2709                                  (axis, ndim))
2710             out[0] = theano._asarray(np.concatenate(tensors, axis=axis),
2711                                      dtype=node.outputs[0].type.dtype)
2712     def c_code_cache_version(self):
2713         return (5,)
2714     def c_code(self, node, name, inputs, outputs, sub):
2715         axis, tensors = inputs[0], inputs[1:]
2716         view = self.view
2717         non_empty_tensor = tensors[view]
2718         input_1 = tensors[0]
2719         l = len(tensors)
2720         out, = outputs
2721         fail = sub['fail']
2722         adtype = node.inputs[0].type.dtype_specs()[1]
2723         copy_to_list = []
2724         for i, inp in enumerate(tensors):
2725             copy_to_list.append(
2726                 """Py_INCREF(%s);
2727                    PyList_SetItem(list, %s, (PyObject*)%s);"""
2728                 % (inp, i, inp))
2729         copy_inputs_to_list = '\n'.join(copy_to_list)
2730         n = len(tensors)
2731         code = """
2732         int axis = ((%(adtype)s *)PyArray_DATA(%(axis)s))[0];
2733         PyObject* list = PyList_New(%(l)s);
2734         %(copy_inputs_to_list)s
2735         int tensors_lens_sum;
2736         if(%(view)s != -1) {
2737             tensors_lens_sum = 0;
2738             for(int i=0; i &lt; %(n)s; i++){
2739                 tensors_lens_sum += PyArray_DIM((PyArrayObject *)(PyList_GetItem(list, i)), axis);
2740             }
2741             tensors_lens_sum -= PyArray_DIM(%(non_empty_tensor)s, axis);
2742         }
2743         if(%(view)s != -1 &amp;&amp; tensors_lens_sum == 0) {
2744             Py_XDECREF(%(out)s);
2745             Py_INCREF(%(non_empty_tensor)s);
2746             %(out)s = %(non_empty_tensor)s;
2747         }else{
2748             //PyObject* PyArray_Concatenate(PyObject* obj, int axis)
2749             int ndim = PyArray_NDIM(%(input_1)s);
2750             if( axis &lt; -ndim ){
2751                 PyErr_Format(PyExc_IndexError,
2752                              "Join axis %%d out of bounds [0, %%d)", axis, ndim);
2753                 %(fail)s
2754             }
2755             Py_XDECREF(%(out)s);
2756             %(out)s = (PyArrayObject *)PyArray_Concatenate(list, axis);
2757             Py_DECREF(list);
2758             if(!%(out)s){
2759                 %(fail)s
2760             }
2761         }
2762         """ % locals()
2763         return code
2764     def R_op(self, inputs, eval_points):
2765         if None in eval_points[1:]:
2766             return [None]
2767         return self.make_node(inputs[0], *eval_points[1:]).outputs
2768     def grad(self, axis_and_tensors, grads):
2769         """ The gradient wrt a join op is a `Split`, used to partition
2770         the gradient along the `axis` which was used for joining.
2771         """
2772         gz, = grads
2773         axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]
2774         rval = [grad_undefined(self, 0, axis)]
2775         dtypes = [as_tensor_variable(x).type.dtype for x in tensors]
2776         out_dtype = scal.upcast(*dtypes)
2777         if 'float' in out_dtype or 'complex' in out_dtype:
2778             split = Split(len(tensors))
2779             split_gz = split(gz, axis, stack([shape(x)[axis]
2780                                               for x in tensors]))
2781             if not isinstance(split_gz, list):
2782                 split_gz = [split_gz]
2783             split_gz = [patternbroadcast(g, t.broadcastable)
2784                         for t, g in zip(tensors, split_gz)]
2785             rval = rval + split_gz
2786         else:
2787             rval = rval + [tensor.zeros_like(dtype=config.floatX)
2788                            for tensor in tensors]
2789         return rval
2790     def infer_shape(self, node, ishapes):
2791         assert len(ishapes) &gt; 1
2792         n_dim = len(ishapes[1])
2793         for shp in ishapes[1:]:
2794             assert shp is not None
2795             assert len(shp) == n_dim
2796         join_dim = switch(ge(node.inputs[0], 0),
2797                           node.inputs[0],
2798                           node.inputs[0] + n_dim)
2799         out_shapes = []
2800         for dim in xrange(n_dim):
2801             t_side = ishapes[1][dim]
2802             f_side = ishapes[1][dim]
2803             for shp in ishapes[2:]:
2804                 t_side = t_side + shp[dim]
2805             out_shapes.append(switch(eq(dim, join_dim),
2806                               t_side, f_side))
2807         return [tuple(out_shapes)]
2808 join_ = Join()
2809 pprint.assign(Join, printing.FunctionPrinter('join'))
2810 def join(axis, *tensors_list):
2811     """
2812     Convenience function to concatenate `TensorType`s along the given axis.
2813     This function will not add the op in the graph when it is not useful.
2814     For example, in the case that the list of tensors to be concatenated
2815     is one, it will just return the tensor.
2816     Parameters
2817     ----------
2818     tensors : list of tensors (or list-like)
2819         A list of tensors to be concatenated along the given axis.
2820         The shapes of the tensors to be concatenated must be all
2821         identical, except in the dimension (`axis`) on which they are to
2822         be joined.
2823     axis : int (symbolic or literal)
2824         On which dimension should the tensors be joined?  The `axis`
2825         must be a valid index into the shape of the tensors to be
2826         concatenated.
2827         The `axis` parameter may either be an integer or an object that
2828         can be converted to a scalar using `as_scalar`(`axis`). In the
2829         former case, the axis is fixed at construction, while in the
2830         latter it may vary over time depending on the value of the
2831         `axis` variable.
2832     """
2833     if len(tensors_list) == 1:
2834         return tensors_list[0]
2835     else:
2836         return join_(axis, *tensors_list)
2837 def roll(x, shift, axis=None):
2838     """
2839     Convenience function to roll TensorTypes along the given axis.
2840     Syntax copies numpy.roll function.
2841     Parameters
2842     ----------
2843     x : tensor_like
2844         Input tensor.
2845     shift : int (symbolic or literal)
2846         The number of places by which elements are shifted.
2847     axis : int (symbolic or literal), optional
2848         The axis along which elements are shifted. By default, the array
2849         is flattened before shifting, after which the original
2850         shape is restored.
2851     Returns
2852     -------
2853     tensor
2854         Output tensor, with the same shape as ``x``.
2855     """
2856     if axis is None:
2857         if x.ndim &gt; 1:
2858             y = x.flatten()
2859             return roll(y, shift, axis=0).reshape(x.shape)
2860         else:
2861             axis = 0
2862     if axis &lt; 0:
2863         axis += x.ndim
2864     shift = shift % x.shape[axis]
2865     allslice = slice(None)
2866     front_slice = slice(-shift, None)
2867     front_list = ([allslice] * axis + [front_slice] +
2868                   [allslice] * (x.ndim - axis - 1))
2869     end_slice = slice(0, -shift)
2870     end_list = ([allslice] * axis + [end_slice] +
2871                 [allslice] * (x.ndim - axis - 1))
2872     return join(axis,
2873                 x.__getitem__(tuple(front_list)),
2874                 x.__getitem__(tuple(end_list)))
2875 @constructor
2876 def shape_padleft(t, n_ones=1):
2877     """Reshape `t` by left-padding the shape with `n_ones` 1s.
2878     See Also
2879     --------
2880     shape_padaxis
2881     shape_padright
2882     Dimshuffle
2883     """
2884     _t = as_tensor_variable(t)
2885     pattern = ['x'] * n_ones + [i for i in xrange(_t.type.ndim)]
2886     return DimShuffle(_t.broadcastable, pattern)(_t)
2887 @constructor
2888 def shape_padright(t, n_ones=1):
2889     """Reshape `t` by right-padding the shape with `n_ones` 1s.
2890     See Also
2891     --------
2892     shape_padaxis
2893     shape_padleft
2894     Dimshuffle
2895     """
2896     _t = as_tensor_variable(t)
2897     pattern = [i for i in xrange(_t.type.ndim)] + ['x'] * n_ones
2898     return DimShuffle(_t.broadcastable, pattern)(_t)
2899 @constructor
2900 def shape_padaxis(t, axis):
2901     """Reshape `t` by inserting 1 at the dimension `axis`.
2902     Example
2903     -------
2904     &gt;&gt;&gt; tensor = theano.tensor.tensor3()
2905     &gt;&gt;&gt; theano.tensor.shape_padaxis(tensor, axis=0)
2906     DimShuffle{x,0,1,2}.0
2907     &gt;&gt;&gt; theano.tensor.shape_padaxis(tensor, axis=1)
2908     DimShuffle{0,x,1,2}.0
2909     &gt;&gt;&gt; theano.tensor.shape_padaxis(tensor, axis=3)
2910     DimShuffle{0,1,2,x}.0
2911     &gt;&gt;&gt; theano.tensor.shape_padaxis(tensor, axis=-1)
2912     DimShuffle{0,1,2,x}.0
2913     See Also
2914     --------
2915     shape_padleft
2916     shape_padright
2917     Dimshuffle
2918     """
2919     _t = as_tensor_variable(t)
2920     ndim = _t.ndim + 1
2921     if not -ndim &lt;= axis &lt; ndim:
2922         msg = 'axis {0} is out of bounds [-{1}, {1})'.format(axis, ndim)
2923         raise IndexError(msg)
2924     if axis &lt; 0:
2925         axis += ndim
2926     pattern = [i for i in xrange(_t.type.ndim)]
2927     pattern.insert(axis, 'x')
2928     return DimShuffle(_t.broadcastable, pattern)(_t)
2929 @constructor
2930 def stack(*tensors, **kwargs):
2931     """Stack tensors in sequence on given axis (default is 0).
2932     Take a sequence of tensors and stack them on given axis to make a single
2933     tensor. The size in dimension `axis` of the result will be equal to the number
2934     of tensors passed.
2935     Note: The interface stack(*tensors) is deprecated, you should use
2936     stack(tensors, axis=0) insted.
2937     Parameters
2938     ----------
2939     tensors : list or tuple of tensors
2940         A list of tensors to be stacked.
2941     axis : int
2942         The index of the new axis. Default value is 0.
2943     Examples
2944     --------
2945     &gt;&gt;&gt; a = theano.tensor.scalar()
2946     &gt;&gt;&gt; b = theano.tensor.scalar()
2947     &gt;&gt;&gt; c = theano.tensor.scalar()
2948     &gt;&gt;&gt; x = theano.tensor.stack([a, b, c])
2949     &gt;&gt;&gt; x.ndim # x is a vector of length 3.
2950     1
2951     &gt;&gt;&gt; a = theano.tensor.tensor4()
2952     &gt;&gt;&gt; b = theano.tensor.tensor4()
2953     &gt;&gt;&gt; c = theano.tensor.tensor4()
2954     &gt;&gt;&gt; x = theano.tensor.stack([a, b, c])
2955     &gt;&gt;&gt; x.ndim # x is a 5d tensor.
2956     5
2957     &gt;&gt;&gt; rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))
2958     &gt;&gt;&gt; rval.shape # 3 tensors are stacked on axis 0
2959     (3, 2, 2, 2, 2)
2960     &gt;&gt;&gt; x = theano.tensor.stack([a, b, c], axis=3)
2961     &gt;&gt;&gt; x.ndim
2962     5
2963     &gt;&gt;&gt; rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))
2964     &gt;&gt;&gt; rval.shape # 3 tensors are stacked on axis 3
2965     (2, 2, 2, 3, 2)
2966     &gt;&gt;&gt; x = theano.tensor.stack([a, b, c], axis=-2)
2967     &gt;&gt;&gt; x.ndim
2968     5
2969     &gt;&gt;&gt; rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))
2970     &gt;&gt;&gt; rval.shape # 3 tensors are stacked on axis -2
2971     (2, 2, 2, 3, 2)
2972     """
2973     if not tensors and not kwargs:
2974         raise Exception('theano.tensor.stack(tensors, axis) must have at least'
2975                         ' one parameter')
2976     if not kwargs and not isinstance(tensors[0], (list, tuple)):
2977         warnings.warn('stack(*tensors) interface is deprecated, use'
2978                       ' stack(tensors, axis=0) instead.', DeprecationWarning,
2979                       stacklevel=3)
2980         axis = 0
2981     elif 'tensors' in kwargs:
2982         tensors = kwargs['tensors']
2983         if 'axis' in kwargs:
2984             axis = kwargs['axis']
2985         else:
2986             axis = 0
2987     else:
2988         if len(tensors) == 2:
2989             axis = tensors[1]
2990         elif 'axis' in kwargs:
2991             axis = kwargs['axis']
2992         else:
2993             axis = 0
2994         tensors = tensors[0]
2995     if len(tensors) == 0:
2996         raise Exception('tensors is empty. You should at least provide one'
2997                         ' tensor to theano.tensor.stack(tensors, axis).')
2998     if np.all(
2999         [  # in case there is direct int in tensors.
3000             isinstance(t, (np.number, float, integer_types,
3001                            python_complex)) or
3002             (isinstance(t, Variable) and
3003              isinstance(t.type, TensorType) and
3004              t.ndim == 0)
3005             for t in tensors]):
3006         tensors = list(map(as_tensor_variable, tensors))
3007         dtype = scal.upcast(*[i.dtype for i in tensors])
3008         return theano.tensor.opt.MakeVector(dtype)(*tensors)
3009     return join(axis, *[shape_padaxis(t, axis) for t in tensors])
3010 @constructor
3011 def concatenate(tensor_list, axis=0):
3012     """Alias for `join`(axis, *tensor_list).
3013     This function is similar to `join`, but uses the signature of
3014     numpy's concatenate function.
3015     Raises
3016     ------
3017     TypeError
3018         The tensor_list must be a tuple or list.
3019     """
3020     if not isinstance(tensor_list, (tuple, list)):
3021         raise TypeError(
3022             "The 'tensors' argument must be either a tuple "
3023             "or a list, make sure you did not forget () or [] around "
3024             "arguments of concatenate.", tensor_list)
3025     return join(axis, *tensor_list)
3026 def get_vector_length(v):
3027     """Return the run-time length of a symbolic vector.
3028     Parameters
3029     ----------
3030     v
3031         A rank-1 TensorType variable.
3032     Raises
3033     ------
3034     TypeError
3035         `v` hasn't the proper type.
3036     ValueError
3037         No special case applies, the length is not known.
3038         In general this is not possible, but for a number of special cases
3039         the length can be determined at compile / graph-construction time.
3040         This function implements these special cases.
3041     """
3042     v = as_tensor_variable(v)
3043     if v.ndim != 1:
3044         raise TypeError("argument must be symbolic vector, got '%s'" %
3045                         v)
3046     if v.type.broadcastable[0]:
3047         return 1
3048     if isinstance(v, gof.Constant) and v.type.ndim == 1:
3049         return len(v.data)
3050     if v.owner and isinstance(v.owner.op, theano.tensor.opt.MakeVector):
3051         return len(v.owner.inputs)
3052     if v.owner and isinstance(v.owner.op, Shape):
3053         return v.owner.inputs[0].type.ndim
3054     if ((v.owner and
3055          isinstance(v.owner.op, theano.tensor.subtensor.Subtensor) and
3056          isinstance(v.owner.op.idx_list[0], slice) and
3057          v.owner.inputs[0].owner and
3058          isinstance(v.owner.inputs[0].owner.op, theano.compile.ops.Shape))):
3059         start = extract_constant(theano.tensor.subtensor.get_idx_list(
3060             v.owner.inputs, v.owner.op.idx_list)[0].start)
3061         stop = extract_constant(theano.tensor.subtensor.get_idx_list(
3062             v.owner.inputs, v.owner.op.idx_list)[0].stop)
3063         step = extract_constant(theano.tensor.subtensor.get_idx_list(
3064             v.owner.inputs, v.owner.op.idx_list)[0].step)
3065         ndim = v.owner.inputs[0].owner.inputs[0].ndim
3066         types = (numbers.Integral, np.integer)
3067         if start is None:
3068             start = 0
3069         elif isinstance(start, types) and start &lt; 0:
3070             start += ndim
3071             if start &lt; 0:
3072                 start = 0
3073         if stop is None:
3074             stop = ndim
3075         elif isinstance(stop, types):
3076             if stop &gt; ndim:
3077                 stop = ndim
3078             elif stop &lt; 0:
3079                 stop += ndim
3080         if step is None:
3081             step = 1
3082         if (isinstance(stop, types) and
3083                 isinstance(start, types) and
3084                 isinstance(step, types) and
3085                 start &gt;= 0 and stop &gt;= 0 and
3086                 step &gt; 0 and stop &gt;= start):
3087             return (stop - start - 1) // step + 1
3088     if isinstance(v, Variable):
3089         msg = theano.printing.debugprint(v, file='str')
3090     else:
3091         msg = str(v)
3092     raise ValueError("length not known: %s" % msg)
3093 @constructor
3094 def horizontal_stack(*args):
3095     """
3096     Horizontally stack two L{TensorType}s.
3097     Stack two L{TensorType}s along the second axis (column wise). These
3098     L{TensorType}s must have the same shape along all dimensions but the
3099     second.
3100     """
3101     assert len(args) &gt;= 2
3102     for arg in args:
3103         assert arg.type.ndim == 2
3104     return concatenate(args, axis=1)
3105 @constructor
3106 def vertical_stack(*args):
3107     assert len(args) &gt;= 2
3108     for arg in args:
3109         assert arg.type.ndim == 2
3110     return concatenate(args, axis=0)
3111 class Reshape(Op):
3112     """Perform a reshape operation of the input x to the new shape shp.
3113     The number of dimensions to which to reshape to (ndim) must be
3114     known at graph build time.
3115     """
3116     view_map = {0: [0]}  # output 0 is potentially aliased to inputs [0]
3117     _f16_ok = True
3118     check_input = False
3119     __props__ = ("ndim",)
3120     params_type = ParamsType(ndim=int32)
3121     def __init__(self, ndim, name=None):
3122         self.ndim = int(ndim)
3123         if ndim &lt; 0:
3124             raise ValueError("The output dimensions after reshape must be 0 or greater")
3125         assert name is None, 'name attribute for Reshape has been deprecated'
3126     def __str__(self):
3127         return '%s{%s}' % (self.__class__.__name__, self.ndim)
3128     def make_node(self, x, shp):
3129         x = as_tensor_variable(x)
3130         shp_orig = shp
3131         shp = as_tensor_variable(shp, ndim=1)
3132         if not (shp.dtype in int_dtypes or
3133                 (isinstance(shp, TensorConstant) and shp.data.size == 0)):
3134             raise TypeError("Shape must be integers", shp, shp.dtype)
3135         assert shp.ndim == 1
3136         if isinstance(shp, TensorConstant):
3137             bcast = [s == 1 for s in shp.data]
3138             return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcast)])
3139         else:
3140             bcasts = [False] * self.ndim
3141             shp_list = shp_orig
3142             if hasattr(shp_orig, "ndim") and shp_orig.ndim == 0:
3143                 shp_list = [shp_orig]
3144             for index in xrange(self.ndim):
3145                 y = shp_list[index]
3146                 y = as_tensor_variable(y)
3147                 try:
3148                     bcasts[index] = (
3149                         hasattr(y, 'get_scalar_constant_value') and
3150                         y.get_scalar_constant_value() == 1)
3151                 except NotScalarConstantError:
3152                     pass
3153             return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcasts)])
3154     def perform(self, node, inp, out_, params):
3155         x, shp = inp
3156         out, = out_
3157         if (len(shp) != self.ndim):
3158             raise ValueError('shape argument to Reshape.perform has incorrect'
3159                              ' length %i'
3160                              ', should be %i' % (len(shp), self.ndim), shp)
3161         try:
3162             out[0] = np.reshape(x, shp)
3163         except Exception:
3164             raise ValueError('Cannot reshape input of shape %s to shape %s' %
3165                              (x.shape, shp))
3166     def connection_pattern(self, node):
3167         return [[True], [False]]
3168     def grad(self, inp, grads):
3169         x, shp = inp
3170         g_out, = grads
3171         return [reshape(g_out, shape(x), ndim=x.ndim),
3172                 DisconnectedType()()]
3173     def R_op(self, inputs, eval_points):
3174         if eval_points[0] is None:
3175             return [None]
3176         return self(eval_points[0], *inputs[1:], **dict(return_list=True))
3177     def infer_shape(self, node, ishapes):
3178         if len(ishapes[0]) == 0:
3179             return [(1,) * self.ndim]
3180         requ = node.inputs[1]
3181         input_size = mul(*ishapes[0])
3182         if isinstance(requ, theano.tensor.TensorConstant):
3183             requ = list(requ.data)
3184             requ_part = [ele for ele in requ if ele != -1]
3185             crit = len(requ) - len(requ_part)
3186             if crit == 1 and len(requ_part) &gt; 0:
3187                 requ_size = mul(*requ_part)
3188                 missing = input_size // (1 if requ_size == 0 else requ_size)
3189                 for i, ele in enumerate(requ):
3190                     if ele == -1:
3191                         requ[i] = missing
3192             elif crit == 1:  # we reshape to -1
3193                 requ = [input_size] if ishapes[0] else [1]
3194             elif crit &gt; 1:
3195                 raise ValueError('shape argument to Reshape.perform'
3196                                  ' must have at most one entry equal to -1')
3197             return [requ]
3198         else:
3199             requ = [requ[i] for i in xrange(self.ndim)]
3200             if self.ndim:
3201                 requ_size = -mul(*requ)
3202                 rest_size = input_size // maximum(requ_size, 1)
3203             return [tuple([switch(eq(requ[i], -1),
3204                                   rest_size,
3205                                   requ[i])
3206                            for i in xrange(self.ndim)])]
3207     def c_code_cache_version(self):
3208         return (8,)
3209     def c_code(self, node, name, inputs, outputs, sub):
3210         if isinstance(node.inputs[0], TensorVariable):
3211             x, shp = inputs
3212             z, = outputs
3213             sdtype = node.inputs[1].type.dtype_specs()[1]
3214             fail = sub['fail']
3215             params = sub['params']
3216             return """
3217             assert (PyArray_NDIM(%(shp)s) == 1);
3218             npy_intp new_dims[%(params)s-&gt;ndim];
3219             PyArray_Dims newshape;
3220             newshape.ptr = new_dims;
3221             newshape.len = %(params)s-&gt;ndim;
3222             for (int ii = 0; ii &lt; %(params)s-&gt;ndim; ++ii)
3223             {
3224                 // -- We do not want an explicit cast here. the shp can be any
3225                 // -- int* dtype. The compiler will explicitly upcast it, but
3226                 // -- will err if this will downcast. This could happen if the
3227                 // -- user pass an int64 dtype, but npy_intp endup being int32.
3228                 new_dims[ii] = ((%(sdtype)s*)(
3229                         PyArray_BYTES(%(shp)s) +
3230                         ii * PyArray_STRIDES(%(shp)s)[0]))[0];
3231             }
3232             Py_XDECREF(%(z)s);
3233             %(z)s = (PyArrayObject *) PyArray_Newshape(%(x)s, &amp;newshape, NPY_CORDER);
3234             if (!%(z)s)
3235             {
3236                 //The error message should have been set by PyArray_Newshape
3237                 %(fail)s;
3238             }
3239             """ % locals()
3240         else:
3241             return Op.c_code(self, node, name, inputs, outputs, sub)
3242 def reshape(x, newshape, ndim=None):
3243     if ndim is None:
3244         newshape = as_tensor_variable(newshape)
3245         if newshape.ndim != 1:
3246             raise TypeError(
3247                 "New shape in reshape must be a vector or a list/tuple of"
3248                 " scalar. Got %s after conversion to a vector." % newshape)
3249         try:
3250             ndim = get_vector_length(newshape)
3251         except ValueError:
3252             raise ValueError(
3253                 "The length of the provided shape (%s) cannot "
3254                 "be automatically determined, so Theano is not able "
3255                 "to know what the number of dimensions of the reshaped "
3256                 "variable will be. You can provide the 'ndim' keyword "
3257                 "argument to 'reshape' to avoid this problem." % newshape)
3258     op = Reshape(ndim)
3259     rval = op(x, newshape)
3260     return rval
3261 class Flatten(Op):
3262     """
3263     Flatten a tensor.
3264     Flattens a tensor to `outdim` dimensions by preserving the leading
3265     outdim - 1 shape components.
3266     .. note:: The interface Flatten(Op) is deprecated, you should use flatten.
3267     """
3268     view_map = {0: [0]}
3269     check_input = False
3270     __props__ = ("outdim",)
3271     def __init__(self, outdim=1):
3272         warnings.warn(
3273             "Flatten class is deprecated, "
3274             "please use flatten method instead.",
3275             DeprecationWarning,
3276             stacklevel=4)
3277         self.outdim = int(outdim)
3278     def __str__(self):
3279         return '%s{%s}' % (self.__class__.__name__, self.outdim)
3280     def make_node(self, x):
3281         t_x = as_tensor_variable(x)
3282         if self.outdim &lt; 1 or (x.ndim and self.outdim &gt; x.ndim):
3283             raise ValueError('invalid output ndimensions (%i) for tensor of '
3284                              'rank %i' % (self.outdim, t_x.ndim))
3285         bcast_kept_dims = x.broadcastable[:self.outdim - 1]
3286         bcast_new_dim = python_all(x.broadcastable[self.outdim - 1:])
3287         broadcastable = bcast_kept_dims + (bcast_new_dim,)
3288         return gof.Apply(self, [t_x], [tensor(x.type.dtype,
3289                                               broadcastable)])
3290     def perform(self, node, inp, out_):
3291         x, = inp
3292         out, = out_
3293         outdim = self.outdim
3294         if outdim == 1:
3295             try:
3296                 out[0] = x.reshape(x.size)
3297             except AttributeError:
3298                 out[0] = x.reshape((np.prod(x.shape),))
3299         elif outdim == len(x.shape):
3300             out[0] = x
3301         else:
3302             newshape = (x.shape[:outdim - 1] +
3303                         (np.prod(x.shape[outdim - 1:]),))
3304             out[0] = x.reshape(newshape)
3305     def infer_shape(self, node, in_shapes):
3306         in_shp, = in_shapes
3307         part1 = in_shp[:self.outdim - 1]
3308         part2 = in_shp[self.outdim - 1:]
3309         if len(part2) &gt; 1:
3310             part2 = (prod(part2, dtype='int64'),)
3311         elif len(part2) == 1:
3312             pass
3313         else:
3314             if len(in_shp) == 0 and self.outdim == 1:
3315                 part2 = (1,)
3316             else:
3317                 raise ValueError('invalid output ndimensions (%i) for tensor '
3318                                  'of rank %i' % (self.outdim, len(in_shp)))
3319         out_shape = (part1 + part2)
3320         return [out_shape]
3321     def grad(self, inp, grads):
3322         x, = inp
3323         g_out, = grads
3324         return [reshape(g_out, shape(x), x.ndim)]
3325     def R_op(self, inputs, eval_points):
3326         if None in eval_points:
3327             return [None]
3328         return self.make_node(*eval_points).outputs
3329     def c_code_cache_version(self):
3330         return (1, 1)
3331     def c_code(self, node, name, inputs, outputs, sub):
3332         x, = inputs
3333         out, = outputs
3334         outdim = self.outdim
3335         fail = sub['fail']
3336         return """
3337         if (%(outdim)s == PyArray_NDIM(%(x)s))
3338         {
3339             Py_XDECREF(%(out)s);
3340             Py_XINCREF(%(x)s);
3341             %(out)s = %(x)s;
3342         }
3343         else
3344         {
3345             Py_XDECREF(%(out)s);
3346             if (%(outdim)s == 1)
3347             {
3348                 npy_intp size = PyArray_SIZE(%(x)s);
3349                 PyArray_Dims newshape;
3350                 newshape.ptr = &amp;size;
3351                 newshape.len = 1;
3352                 %(out)s = (PyArrayObject*)PyArray_Newshape(%(x)s,
3353                                                            &amp;newshape,
3354                                                            NPY_CORDER);
3355             }
3356             else
3357             {
3358                 npy_intp *oldshape = PyArray_DIMS(%(x)s);
3359                 npy_intp newshape_dims[%(outdim)s];
3360                 int i;
3361                 for (i = 0; i &lt; %(outdim)s - 1; ++i)
3362                     newshape_dims[i] = oldshape[i];
3363                 newshape_dims[i] = 1;
3364                 for (int j = %(outdim)s - 1; j &lt; PyArray_NDIM(%(x)s); ++j)
3365                     newshape_dims[i] *= oldshape[j];
3366                 PyArray_Dims newshape;
3367                 newshape.ptr = newshape_dims;
3368                 newshape.len = %(outdim)s;
3369                 %(out)s = (PyArrayObject*)PyArray_Newshape(%(x)s,
3370                                                            &amp;newshape,
3371                                                            NPY_CORDER);
3372             }
3373         }
3374         if (!%(out)s)
3375         {
3376             //The error message should have been set by
3377             // PyArray_Newshape
3378             %(fail)s;
3379         }
3380         """ % locals()
3381 def is_flat(var, ndim=None, outdim=None):
3382     """
3383     Verifies the dimensionality of the var is equal to
3384     outdim. This method is usually called after flatten method on a
3385     variable, where the first outdim-1 dimension size(s) of the variable
3386     is kept intact, and the last dimension size of the variable is made
3387     equal to the multiplication of its remaining dimension size(s), such that
3388     the variable would end up with as many dimension as outdim.
3389     Parameters
3390     ----------
3391         var : theano.tensor.var.TensorVariable
3392             the theano var on which the dimensionality is checked.
3393         outdim : int
3394             the expected dimensionality of var.
3395     Returns
3396     -------
3397     bool
3398         the comparison result of var's dim
3399         and the expected outdim.
3400     """
3401     if outdim is None and ndim is None:
3402         ndim = 1
3403     elif outdim is not None and ndim is not None:
3404         raise ValueError("You should only specify ndim")
3405     elif outdim is not None:
3406         warnings.warn(
3407             "flatten outdim parameter is deprecated, use ndim instead.")
3408         ndim = outdim
3409     return var.ndim == ndim
3410 def flatten(x, ndim=None, outdim=None):
3411     """
3412     Reshapes the variable x by keeping
3413     the first outdim-1 dimension size(s) of x the same,
3414     and making the last dimension size of x equal to
3415     the multiplication of its remaining dimension size(s).
3416     Parameters
3417     ----------
3418         x : theano.tensor.var.TensorVariable
3419             the variable that should be reshaped.
3420         ndim : int
3421             the number of dimensions of the returned variable
3422             Default 1.
3423         outdim : int
3424             DEPRECATED synonym for ndim
3425     Returns
3426     -------
3427     theano.tensor.var.TensorVariable
3428         the flattend variable with dimensionality of outdim
3429     """
3430     if outdim is None and ndim is None:
3431         ndim = 1
3432     elif outdim is not None and ndim is not None:
3433         raise ValueError("You should only specify ndim")
3434     elif outdim is not None:
3435         warnings.warn(
3436             "flatten outdim parameter is deprecated, use ndim instead.")
3437         ndim = outdim
3438     if ndim &lt; 1 or (ndim &gt; 1 and ndim &gt; x.ndim):
3439         raise ValueError('ndim %s out of bound [1, %d)'
3440                          % (ndim, x.ndim + 1))
3441     if ndim &gt; 1:
3442         dims = tuple(x.shape[:ndim - 1]) + (-1,)
3443     else:
3444         dims = (-1,)
3445     x_reshaped = x.reshape(dims)
3446     bcast_kept_dims = x.broadcastable[:ndim - 1]
3447     bcast_new_dim = python_all(x.broadcastable[ndim - 1:])
3448     broadcastable = bcast_kept_dims + (bcast_new_dim,)
3449     x_reshaped = theano.tensor.addbroadcast(
3450         x_reshaped, *filter(lambda i: broadcastable[i], range(ndim)))
3451     return x_reshaped
3452 class Tile(Op):
3453     """
3454     Construct an array by repeating the input x according to reps pattern.
3455     .. note:: Deprecated
3456               Use tile() instead.
3457     Tiles its input according to reps. The length of reps is the number of
3458     dimension of x and contains the number of times to tile x in each
3459     dimension.
3460     See Also
3461     --------
3462     numpy.tile : http://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html
3463     """
3464     __props__ = ("ndim",)
3465     def __init__(self, ndim):
3466         self.ndim = ndim
3467     def __str__(self):
3468         return self.__class__.__name__ + "{ndim=%d}" % self.ndim
3469     def make_node(self, x, reps):
3470         warnings.warn((
3471             "Tile op is deprecated, use tile function instead."), stacklevel=3)
3472         x = as_tensor_variable(x)
3473         reps = as_tensor_variable(reps)
3474         return gof.Apply(self, [x, reps], [tensor(x.type.dtype, [False] *
3475                                                   self.ndim)])
3476     def perform(self, node, inp, out_):
3477         x, reps = inp
3478         out, = out_
3479         res = np.tile(x, reps)
3480         if res.ndim != self.ndim:
3481             raise ValueError(
3482                 'Tile.perform produced incorrect number of dimensions')
3483         if (np.asarray(reps) == 1).all():
3484             if np.may_share_memory(res, x):
3485                 res = res.copy()
3486         out[0] = res
3487     def infer_shape(self, node, in_shapes):
3488         x, reps = node.inputs
3489         shp = in_shapes[0]
3490         tiled_shp = shp * reps
3491         out_shape = []
3492         for i in xrange(self.ndim):
3493             out_shape.append(tiled_shp[i])
3494         return [out_shape]
3495     def grad(self, inp, grads):
3496         x, reps = inp
3497         g_out, = grads
3498         raise NotImplementedError()
3499 def tile(x, reps, ndim=None):
3500     """
3501     Tile input array `x` according to `reps`.
3502     See the docstring of `numpy.tile` for details.
3503     'reps' can be constant integer (e.g. 3), constant vector(e.g. [2 3]),
3504     symbolic scalar (e.g. tensor.iscalar()), symbolic vector (e.g. tensor.ivector())
3505     or a list of symbolic scalar (e.g. [tensor.iscalar(), tensor.iscalar()]).
3506     ndim is the number of the dimensions of the output, if it is provided, ndim
3507     should be equal or larger than x.ndim and len(reps), otherwise, we will use
3508     max(x.ndim, len(reps)) as ndim. If reps is symbolic vector, the ndim has to
3509     be provided.
3510     """
3511     if ndim is not None and ndim &lt; x.ndim:
3512         raise ValueError("ndim should be equal or larger than x.ndim")
3513     if not isinstance(reps, (list, tuple)):
3514         reps_astensor = as_tensor_variable(reps)
3515         ndim_check = reps_astensor.ndim
3516         if reps_astensor.dtype not in theano.tensor.discrete_dtypes:
3517             raise ValueError("elements of reps must be integer dtype")
3518         if ndim_check == 0:
3519             reps = [reps]
3520         elif ndim_check == 1:
3521             if ndim is None:
3522                 raise ValueError("if reps is tensor.vector, you should specify "
3523                                  "the ndim")
3524             else:
3525                 offset = ndim - reps.shape[0]
3526                 offset = theano.tensor.opt.assert_(offset, ge(offset, 0))
3527                 reps_ = [switch(i &lt; offset, 1, reps[i - offset]) for i in range(ndim)]
3528                 reps = reps_
3529         else:
3530             raise ValueError("the dimension of reps should not exceed 1")
3531     else:
3532         if ndim is not None and len(reps) &gt; ndim:
3533             raise ValueError("len(reps) should be equal or less than ndim")
3534         if not np.all([isinstance(r, integer_types) or
3535                        (isinstance(r, TensorVariable) and
3536                         r.dtype in theano.tensor.discrete_dtypes) for r in reps]):
3537             raise ValueError("elements of reps must be scalars of integer dtype")
3538     reps = list(reps)
3539     if ndim is None:
3540         ndim = builtins.max(len(reps), x.ndim)
3541         reps = [1] * (ndim - len(reps)) + reps
3542     shape = [1] * (ndim - x.ndim) + [x.shape<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>[i] for i in xrange(x.ndim)]
3543     alloc_shape = reps + shape
3544     y = alloc(x, *alloc_shape)
3545     shuffle_ind = np.arange(ndim * 2).reshape(2, ndim)
3546     shuffle_ind =</b></font> shuffle_ind.transpose().flatten()
3547     y = y.dimshuffle(*shuffle_ind)
3548     new_shapes = [sh * reps[i] for i, sh in enumerate(shape)]
3549     y = y.reshape(new_shapes)
3550     return y
3551 class ARange(Op):
3552     """Create an array containing evenly spaced values within a given interval.
3553     Parameters and behaviour are the same as numpy.arange().
3554     """
3555     __props__ = ("dtype",)
3556     def __init__(self, dtype):
3557         self.dtype = dtype
3558     def make_node(self, start, stop, step):
3559         start, stop, step = map(as_tensor_variable, (start, stop, step))
3560         assert start.ndim == 0
3561         assert stop.ndim == 0
3562         assert step.ndim == 0
3563         inputs = [start, stop, step]
3564         outputs = [tensor(self.dtype, (False,))]
3565         return Apply(self, inputs, outputs)
3566     @theano.configparser.change_flags(warn_float64='ignore')
3567     def infer_shape(self, node, i_shapes):
3568         start, stop, step = node.inputs
3569         def is_constant_value(var, value):
3570             try:
3571                 v = get_scalar_constant_value(var)
3572                 return np.all(v == value)
3573             except NotScalarConstantError:
3574                 pass
3575             return False
3576         def upcast(var):
3577             if (var.dtype in integer_dtypes and
3578                     scal.upcast(var.dtype, 'int64') == 'int64'):
3579                 return cast(var, 'int64')
3580             return var
3581         if is_constant_value(step, 1):
3582             if is_constant_value(start, 0):
3583                 return [(cast(stop, 'int64'),)]
3584             else:
3585                 stop = upcast(stop)
3586                 start = upcast(start)
3587                 return [(maximum(cast(stop - start, 'int64'), 0),)]
3588         else:
3589             stop = upcast(stop)
3590             start = upcast(start)
3591             return [(maximum(cast(ceil(cast((stop - start), 'float64') / step),
3592                     'int64'), 0),)]
3593     def perform(self, node, inp, out_):
3594         start, stop, step = inp
3595         out, = out_
3596         start = start.item()
3597         stop = stop.item()
3598         step = step.item()
3599         out[0] = np.arange(start, stop, step, dtype=self.dtype)
3600     def connection_pattern(self, node):
3601         return [[True], [False], [True]]
3602     def L_op(self, inputs, outputs, grads):
3603         start, stop, step = inputs
3604         gz, = grads
3605         if self.dtype in discrete_dtypes:
3606             return [start.zeros_like(dtype=config.floatX),
3607                     DisconnectedType()(),
3608                     step.zeros_like(dtype=config.floatX)]
3609         else:
3610             num_steps_taken = outputs[0].shape[0]
3611             return [gz.sum(),
3612                     DisconnectedType()(),
3613                     (gz * arange(num_steps_taken, dtype=self.dtype)).sum()]
3614     def R_op(self, inputs, eval_points):
3615         return [None]
3616 _arange = {}
3617 def arange(start, stop=None, step=1, dtype=None):
3618     if stop is None:
3619         start, stop = 0, start
3620     start, stop, step = map(as_tensor_variable, (start, stop, step))
3621     if dtype is None:
3622         dtype = scal.upcast(start.type.dtype, stop.type.dtype, step.type.dtype)
3623         if dtype in int_dtypes:
3624             dtype = 'int64'
3625         if dtype in uint_dtypes:
3626             dtype = 'uint64'
3627         if config.cast_policy in ('numpy', 'numpy+floatX'):
3628             numpy_dtype = np.arange(
3629                 start=np.array(0, dtype=start.dtype),
3630                 stop=np.array(1, dtype=stop.dtype),
3631                 step=np.array(1, dtype=step.dtype)).dtype
3632             if numpy_dtype != dtype:
3633                 if (config.cast_policy == 'numpy+floatX' and
3634                     config.floatX == 'float32' and
3635                     numpy_dtype == 'float64' and
3636                     python_all(
3637                         dt != 'float64'
3638                         for dt in [s.dtype for s in (start, stop, step)])):
3639                     assert dtype != 'float64'
3640                     dtype = 'float32'
3641                 else:
3642                     dtype = str(numpy_dtype)
3643     if dtype not in _arange:
3644         _arange[dtype] = ARange(dtype)
3645     return _arange[dtype](start, stop, step)
3646 class _nd_grid(object):
3647     """Create a dense n-dimensional 'meshgrid' with equally spaced points.
3648     Used to create the instance ``mgrid`` and ``ogrid`` which act similarly
3649     to their numpy equivalents.
3650     Parameters
3651     ----------
3652     sparse : boolean, optional, default=True
3653         Specifying False leads to the equivalent of numpy's mgrid functionality.
3654         Specifying True leads to the equivalent of ogrid.
3655     Examples
3656     --------
3657     &gt;&gt;&gt; a = T.mgrid[0:5, 0:3]
3658     &gt;&gt;&gt; a[0].eval()
3659     array([[0, 0, 0],
3660            [1, 1, 1],
3661            [2, 2, 2],
3662            [3, 3, 3],
3663            [4, 4, 4]], dtype=int8)
3664     &gt;&gt;&gt; a[1].eval()
3665     array([[0, 1, 2],
3666            [0, 1, 2],
3667            [0, 1, 2],
3668            [0, 1, 2],
3669            [0, 1, 2]], dtype=int8)
3670     &gt;&gt;&gt; b = T.ogrid[0:5, 0:3]
3671     &gt;&gt;&gt; b[0].eval()
3672     array([[0],
3673            [1],
3674            [2],
3675            [3],
3676            [4]], dtype=int8)
3677     &gt;&gt;&gt; b[1].eval()
3678     array([[0, 1, 2, 3]], dtype=int8)
3679     """
3680     def __init__(self, sparse=False):
3681         self.sparse = sparse
3682     def __getitem__(self, *args):
3683         ndim = len(args[0])
3684         for sl in args[0]:
3685             if isinstance(sl.step, python_complex):
3686                 raise NotImplementedError("Not implemented for slices "
3687         ranges = [arange(sl.start or 0,
3688                          sl.stop,
3689                          sl<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.step or 1) for sl in args[0]]
3690         shapes = [tuple([1] * j + [r.shape[0]] + [1] * (ndim - 1 - j))
3691                   for j, r in enumerate(ranges)]
3692         ranges = [r.reshape(</b></font>shape) for r, shape in zip(ranges, shapes)]
3693         if self.sparse:
3694             grids = ranges
3695         else:
3696             grids = []
3697             ones = [ones_like(r) for r in ranges]
3698             for i in range(ndim):
3699                 grid = 1
3700                 for j in range(ndim):
3701                     if j == i:
3702                         grid = grid * ranges[j]
3703                     else:
3704                         grid = grid * ones[j]
3705                 grids.append(grid)
3706         return grids
3707 mgrid = _nd_grid()
3708 ogrid = _nd_grid(sparse=True)
3709 class PermuteRowElements(Op):
3710     """Permute the elements of each row (inner-most dim) of a tensor.
3711     A permutation will be applied to every row (vector) of the input tensor x.
3712     Depending on the dimensionality of x and the permutation tensor y,
3713     different cases are possible.
3714     If y.ndim = 1, y is a single permutation, that will be applied to every
3715     vector of x. For instance, if x is a matrix, the same permutation will be
3716     applied to each row of x.
3717     If x.ndim = y.ndim, each row of x corresponds to a row of y, containing
3718     a permutation that will be applied to that row. For instance, if x and y
3719     are two matrices, a different permutation will be applied to each row of x.
3720     If x.ndim &gt; y.ndim, y will be broadcasted to fit x, then each row (vector)
3721     of x will be reordered according to the corresponding row of y. (This is
3722     a generalization of the first case).
3723     If x.ndim = 1, every permutation in y will be applied to x, and the output
3724     will contain all the results.
3725     If x.ndim &lt; y.ndim, x will be broadcasted to fit y, and different
3726     permutations contained in y will be applied to each vector in x. (This is
3727     a generalization of the previous case).
3728     If the "inverse" argument is True, the Op will perform the inverse
3729     permutation instead.
3730     """
3731     __props__ = ()
3732     def make_node(self, x, y, inverse):
3733         x = as_tensor_variable(x)
3734         y = as_tensor_variable(y)
3735         if inverse:  # as_tensor_variable does not accept booleans
3736             inverse = as_tensor_variable(1)
3737         else:
3738             inverse = as_tensor_variable(0)
3739         assert y.type.dtype in integer_dtypes
3740         assert (inverse.type.ndim == 0 and inverse.type.dtype in integer_dtypes)
3741         x_dim = x.type.ndim
3742         y_dim = y.type.ndim
3743         if x_dim &gt; y_dim:
3744             y = shape_padleft(y, n_ones=(x_dim - y_dim))
3745         elif x_dim &lt; y_dim:
3746             x = shape_padleft(x, n_ones=(y_dim - x_dim))
3747         out_broadcastable = [xb and yb for xb, yb in
3748                              izip(x.type.broadcastable, y.type.broadcastable)]
3749         out_type = tensor(dtype=x.type.dtype, broadcastable=out_broadcastable)
3750         inputlist = [x, y, inverse]
3751         outputlist = [out_type]
3752         return Apply(self, inputlist, outputlist)
3753     def _rec_perform(self, node, x, y, inverse, out, curdim):
3754         """Perform the permutation by doing a recursion over the input
3755         dimensions.
3756         For every dimension, starting with the leftmost, the right set of
3757         indices is determined (depending if broadcasting or not), then
3758         the function is recursively called on the appropriate subtensors.
3759         The terminal case is reached when the current tensors are vector,
3760         then the permutation contained in y is applied to x.
3761         Parameters
3762         ----------
3763         x : tensor
3764             The input tensor, on which the permutation is applied.
3765         y : tensor
3766             Tensor containing the permutations to apply.
3767         out : tensor
3768             Tensor storing the output result.
3769         curdim : int
3770             Counter of the current depth of recursion.
3771         inverse
3772             Wether to apply permutations or their inverse.
3773         """
3774         if len(x.shape) == 1:
3775             if inverse:
3776                 out[y] = x[:]
3777             else:
3778                 out[:] = x[y]
3779         else:
3780             xs0 = x.shape[0]
3781             ys0 = y.shape[0]
3782             if xs0 == ys0:
3783                 for i in xrange(xs0):
3784                     self._rec_perform(node, x[i], y[i], inverse, out[i],
3785                                       curdim + 1)
3786             elif ys0 == 1 and node.inputs[1].type.broadcastable[curdim]:
3787                 for i in xrange(xs0):
3788                     self._rec_perform(node, x[i], y[0], inverse, out[i],
3789                                       curdim + 1)
3790             elif xs0 == 1 and node.inputs[0].type.broadcastable[curdim]:
3791                 for i in xrange(ys0):
3792                     self._rec_perform(node, x[0], y[i], inverse, out[i],
3793                                       curdim + 1)
3794             else:
3795                 raise ValueError('Dimension mismatch: %s, %s' % (xs0, ys0))
3796     def perform(self, node, inp, out):
3797         x, y, inverse = inp
3798         outs, = out
3799         x_s = x.shape
3800         y_s = y.shape
3801         assert len(x_s) == len(y_s)
3802         out_s = []
3803         for xdim, ydim in izip(x_s, y_s):
3804             if xdim == ydim:
3805                 outdim = xdim
3806             elif xdim == 1:
3807                 outdim = ydim
3808             elif ydim == 1:
3809                 outdim = xdim
3810             else:
3811                 raise ValueError('Dimension mismatch: %s, %s' % (xdim, ydim))
3812             out_s.append(outdim)
3813         if outs[0] is None or outs[0].shape != out_s:
3814             outs[0] = np.empty(out_s, dtype=x.dtype)
3815         self._rec_perform(node, x, y, inverse, outs[0], curdim=0)
3816     def infer_shape(self, node, in_shapes):
3817         shp_x = in_shapes[0]
3818         shp_y = in_shapes[1]
3819         assert len(shp_x) == len(shp_y)
3820         out_shape = []
3821         for i in xrange(len(shp_x)):
3822             out_shape.append(maximum(shp_x[i], shp_y[i]))
3823         return [out_shape]
3824     def grad(self, inp, grads):
3825         x, y, inverse = inp
3826         gz, = grads
3827         gx = permute_row_elements(gz, y, eq(inverse, 0))
3828         broadcasted_dims = [dim for dim in xrange(gz.type.ndim)
3829                             if x.type.broadcastable[dim] and
3830                             not gz.type.broadcastable[dim]]
3831         gx = Sum(axis=broadcasted_dims)(gx)
3832         newdims = []
3833         i = 0
3834         for dim in xrange(gz.type.ndim):
3835             if dim in broadcasted_dims:
3836                 newdims.append('x')
3837             else:
3838                 newdims.append(i)
3839                 i += 1
3840         gx = DimShuffle(gx.type.broadcastable, newdims)(gx)
3841         assert gx.type.broadcastable == x.type.broadcastable
3842         if x.type.dtype in discrete_dtypes:
3843             gx = x.zeros_like()
3844         return [gx, grad_undefined(self, 1, y),
3845                 grad_undefined(self, 1, inverse)]
3846 _permute_row_elements = PermuteRowElements()
3847 def permute_row_elements(x, y, inverse=0):
3848     return _permute_row_elements(x, y, inverse)
3849 def inverse_permutation(perm):
3850     """Computes the inverse of permutations.
3851     Each row of input should contain a permutation of the first integers.
3852     """
3853     return permute_row_elements(
3854         arange(perm.shape[-1], dtype=perm.dtype),
3855         perm,
3856         inverse=True)
3857 class Dot(Op):
3858     """
3859     Computes the dot product of two variables. For two matrices, this is
3860     equivalent to matrix multiplication. For two vectors, this is the inner
3861     product.
3862     Notes
3863     -----
3864     Matrix-matrix products are sometimes optimized to Dot22 or Gemm ops
3865     (see tensor.blas).
3866     Vector-vector products are sometimes optimized to Ger or CGer (see
3867     tensor.blas).
3868     Matrix-vector products are sometimes optimized to Gemv, CGemv (see
3869     tensor.blas).
3870     """
3871     __props__ = ()
3872     def make_node(self, *inputs):
3873         inputs = list(map(as_tensor_variable, inputs))
3874         if len(inputs) != 2:
3875             raise TypeError(
3876                 'theano.tensor.Dot: 2 arguments required, %d given ' %
3877                 len(inputs))
3878         if inputs[0].ndim not in (1, 2):
3879             raise TypeError(
3880                 'theano.tensor.Dot: input 0 (0-indexed) must have ndim of '
3881                 '1 or 2, %d given. Consider calling theano.tensor.dot '
3882                 'instead.' % inputs[0].ndim)
3883         if inputs[1].ndim not in (1, 2):
3884             raise TypeError(
3885                 'theano.tensor.Dot: input 1 (0-indexed) must have ndim of '
3886                 '1 or 2, %d given. Consider calling theano.tensor.dot '
3887                 'instead.' % inputs[1].ndim)
3888         i_broadcastables = [input.type.broadcastable for input in inputs]
3889         bx, by = i_broadcastables
3890         if len(by) == 2:  # y is a matrix
3891             bz = bx[:-1] + by[-1:]
3892         elif len(by) == 1:  # y is vector
3893             bz = bx[:-1]
3894         i_dtypes = [input.type.dtype for input in inputs]
3895         outputs = [tensor(scal.upcast(*i_dtypes), bz)]
3896         return Apply(self, inputs, outputs)
3897     def perform(self, node, inp, out):
3898         x, y = inp
3899         z, = out
3900         z[0] = np.asarray(np.dot(x, y))
3901     def grad(self, inp, grads):
3902         x, y = inp
3903         gz, = grads
3904         xdim, ydim, gdim = x.type.ndim, y.type.ndim, gz.type.ndim
3905         if gdim == 0:
3906             xgrad = gz * y
3907             ygrad = gz * x
3908         elif xdim == 1 and ydim == 2:
3909             xgrad = dot(gz, y.T)
3910             ygrad = outer(x.T, gz)
3911         elif xdim == 2 and ydim == 1:
3912             xgrad = outer(gz, y.T)
3913             ygrad = dot(x.T, gz)
3914         elif xdim == ydim == 2:
3915             xgrad = dot(gz, y.T)
3916             ygrad = dot(x.T, gz)
3917         if xgrad.broadcastable != x.broadcastable:
3918             xgrad = patternbroadcast(xgrad, x.broadcastable)
3919         if ygrad.broadcastable != y.broadcastable:
3920             ygrad = patternbroadcast(ygrad, y.broadcastable)
3921         rval = xgrad, ygrad
3922         for elem in rval:
3923             assert elem.dtype.find('float') != -1
3924         return rval
3925     def R_op(self, inputs, eval_points):
3926         assert len(inputs) == 2
3927         assert len(eval_points) == 2
3928         if eval_points[0] is None and eval_points[1] is None:
3929             return [None]
3930         if eval_points[0]:
3931             t1 = self(eval_points[0], inputs[1])
3932         if eval_points[1]:
3933             t2 = self(inputs[0], eval_points[1])
3934         if eval_points[0] and eval_points[1]:
3935             return [t1 + t2]
3936         elif eval_points[0]:
3937             return [t1]
3938         else:
3939             return [t2]
3940     def infer_shape(self, node, shapes):
3941         xshp, yshp = shapes
3942         x, y = node.inputs
3943         if x.ndim == 1 and y.ndim == 1:
3944             return [()]
3945         if x.ndim == 2 and y.ndim == 1:
3946             return [xshp[:-1]]
3947         if x.ndim == 1 and y.ndim == 2:
3948             return [yshp[-1:]]
3949         if x.ndim == 2 and y.ndim == 2:
3950             return [xshp[:-1] + yshp[-1:]]
3951         raise NotImplementedError()
3952     def __str__(self):
3953         return "dot"
3954 _dot = Dot()
3955 pprint.assign(_dot, printing.OperatorPrinter(printing.special['middle_dot'],
3956                                              -1, 'left'))
3957 def dot(a, b):
3958     """
3959     Computes the dot product of two variables.
3960     For two matrices, this is equivalent to matrix multiplication.
3961     For two vectors, this is the inner product.
3962     When one variable is a scalar, this is like elementwise multiplication.
3963     For N dimensions, this is a sum product over the last axis
3964     of the first array and the second-to-last axis of the second array:
3965         dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])
3966     Note that this dot function does one of three things, in the following
3967     sequence:
3968         1.  If either a or b is scalar, it returns the elementwise product
3969             without calling the Theano Dot op.
3970         2.  If either a or b has more than 2 dimensions, it calls Theano's
3971             tensordot function with appropriate axes. The tensordot function
3972             expresses high-dimensional dot products in terms of 2D matrix
3973             multiplications, so it may be possible to futherize optimize for
3974             performance.
3975         3.  If both a and b have either 1 or 2 dimensions, it calls Theano's
3976             Dot op on a and b.
3977     Notes
3978     -----
3979     Matrix-matrix products are sometimes optimized to Dot22 or Gemm ops
3980     (see tensor.blas).
3981     Vector-vector products are sometimes optimized to Ger or CGer (see
3982     tensor.blas).
3983     Matrix-vector products are sometimes optimized to Gemv, CGemv (see
3984     tensor.blas).
3985     """
3986     a, b = as_tensor_variable(a), as_tensor_variable(b)
3987     if a.ndim == 0 or b.ndim == 0:
3988         return a * b
3989     elif a.ndim &gt; 2 or b.ndim &gt; 2:
3990         return tensordot(a, b, [[a.ndim - 1], [np.maximum(0, b.ndim - 2)]])
3991     else:
3992         return _dot(a, b)
3993 def _tensordot_as_dot(a, b, axes, dot, batched):
3994     """
3995     Reduces a tensor dot product to a matrix or vector dot product. Based
3996     on code from Tijmen Tieleman's gnumpy
3997     (http://www.cs.toronto.edu/~tijmen/gnumpy.html).
3998     Please see the documentation of tensordot for the meaning of the a, b
3999     and axes arguments.
4000     :param dot: a function that accepts two symbolic variables and computes
4001                 the appropriate dot product (e.g. dot, batched_dot)
4002     :type dot: function
4003     :param batched: whether to treat the first axis of a and b as a batch
4004                     axis.  If so, this axis will be preserved in the output,
4005                     allowing this function to be used also for batched
4006                     tensor dot products.
4007     :type batched: boolean
4008     :returns: a tensor with shape equal to the concatenation of a's shape
4009               (less any dimensions that were summed over) and b's shape
4010               (less the first dimension and any dimensions that were summed
4011               over).
4012     :rtype: symbolic tensor
4013     """
4014     a, b = as_tensor_variable(a), as_tensor_variable(b)
4015     if not np.isscalar(axes) and len(axes) != 2:
4016         raise ValueError('Axes should be an integer or a '
4017                          'list/tuple of len 2 (%s was provided)'
4018                          % str(axes))
4019     elif np.isscalar(axes):
4020         axes = int(axes)
4021         for operand_name, operand in (("a", a), ("b", b)):
4022             if axes &gt; operand.ndim:
4023                 raise ValueError(
4024                     'axes can not be larger than the dimension of %s '
4025                     '(%s.ndim=%i, axes=%i)'
4026                     % (operand_name, operand_name, operand.ndim, axes))
4027             if batched and axes == operand.ndim:
4028                 raise ValueError(
4029                     'axes to sum over must not include the batch axis '
4030                     'of %s (%s.ndim=%i, axes=%i)'
4031                     % (operand_name, operand_name, operand.ndim, axes))
4032         batch_axes = 1 if batched else 0
4033         a_outaxes = slice(0, a.ndim - axes)
4034         b_outaxes = slice(batch_axes + axes, b.ndim)
4035         outshape = concatenate([a.shape[a_outaxes], b.shape[b_outaxes]])
4036         outbcast = a.broadcastable[a_outaxes] + b.broadcastable[b_outaxes]
4037         outndim = len(outbcast)
4038         a_shape = [1] * 2
4039         b_shape = [1] * 2
4040         for i in xrange(0, axes):
4041             a_shape[1] *= a.shape[-(i + 1)]
4042             b_shape[0] *= b.shape[batch_axes + i]
4043         for i in xrange(0, a.ndim - axes - batch_axes):
4044             a_shape[0] *= a.shape[batch_axes + i]
4045         for i in xrange(0, b.ndim - axes - batch_axes):
4046             b_shape[1] *= b.shape[-(i + 1)]
4047         if batched:
4048             a_shape.insert(0, a.shape[0])
4049             b_shape.insert(0, b.shape[0])
4050         a_reshaped = a.reshape(a_shape)
4051         b_reshaped = b.reshape(b_shape)
4052         out_reshaped = dot(a_reshaped, b_reshaped)
4053         out = out_reshaped.reshape(outshape, outndim)
4054         return patternbroadcast(out, outbcast)
4055     else:
4056         axes = [_pack(axes_) for axes_ in axes]
4057         if len(axes[0]) != len(axes[1]):
4058             raise ValueError('Axes elements must have the same length.')
4059         for i, (operand_name, operand) in enumerate((("a", a),
4060                                                      ("b", b))):
4061             if len(axes[i]) &gt; operand.ndim:
4062                 raise ValueError(
4063                     'axes[%i] should be array_like with length less than '
4064                     'the dimensions of %s (%s.ndim=%i, len(axes[0])=%i).' %
4065                     (i, operand_name, operand_name, operand.ndim,
4066                      len(axes[i])))
4067             if len(axes[i]) &gt; 0 and np.max(axes[i]) &gt;= operand.ndim:
4068                 raise ValueError(
4069                     'axes[%i] contains dimensions greater than or equal '
4070                     'to %s.ndim (%s.ndim=%i, max(axes[0])=%i).' %
4071                     (i, operand_name, operand_name, operand.ndim,
4072                      np.max(np.array(axes[i]))))
4073             if batched and 0 in axes[i]:
4074                 raise ValueError(
4075                     'axes to sum over must not contain the batch axis '
4076                     '(axes[%i]=%s)' %
4077                     (i, axes[i]))
4078         batch_axes = [0] if batched else []
4079         other_axes = [[x for x in xrange(operand.ndim)
4080                        if x not in axes[i] and x not in batch_axes]
4081                       for i, operand in enumerate((a, b))]
4082         a_shuffled = a.dimshuffle(batch_axes + other_axes[0] + axes[0])
4083         b_shuffled = b.dimshuffle(batch_axes + axes[1] + other_axes[1])
4084         return _tensordot_as_dot(a_shuffled, b_shuffled, len(axes[0]),
4085                                  dot=dot, batched=batched)
4086 def tensordot(a, b, axes=2):
4087     """
4088     Compute a generalized dot product over provided axes.
4089     Given two tensors a and b, tensordot computes a generalized dot product over
4090     the provided axes. Theano's implementation reduces all expressions to
4091     matrix or vector dot products and is based on code from Tijmen Tieleman's
4092     gnumpy (http://www.cs.toronto.edu/~tijmen/gnumpy.html).
4093     Parameters
4094     ----------
4095     a: symbolic tensor
4096         The first tensor variable.
4097     b: symbolic tensor
4098         The second tensor variable
4099     axes: int or array-like of length 2
4100         If an integer, the number of axes to sum over.
4101         If an array, it must have two array elements containing the axes
4102         to sum over in each tensor.
4103         Note that the default value of 2 is not guaranteed to work
4104         for all values of a and b, and an error will be raised if
4105         that is the case. The reason for keeping the default is to
4106         maintain the same signature as numpy's tensordot function
4107         (and np.tensordot raises analogous errors for non-compatible
4108         inputs).
4109         If an integer i, it is converted to an array containing
4110         the last i dimensions of the first tensor and the first
4111         i dimensions of the second tensor:
4112             axes = [list(range(a.ndim - i, b.ndim)), list(range(i))]
4113         If an array, its two elements must contain compatible axes
4114         of the two tensors. For example, [[1, 2], [2, 0]] means sum
4115         over the 2nd and 3rd axes of a and the 3rd and 1st axes of b.
4116         (Remember axes are zero-indexed!) The 2nd axis of a and the
4117         3rd axis of b must have the same shape; the same is true for
4118         the 3rd axis of a and the 1st axis of b.
4119     Returns
4120     -------
4121     symbolic tensor
4122         A tensor with shape equal to the concatenation of a's shape
4123         (less any dimensions that were summed over) and b's shape
4124         (less any dimensions that were summed over).
4125     Examples
4126     --------
4127     It may be helpful to consider an example to see what tensordot does.
4128     Theano's implementation is identical to NumPy's. Here a has shape (2, 3, 4)
4129     and b has shape (5, 6, 4, 3). The axes to sum over are [[1, 2], [3, 2]] --
4130     note that a.shape[1] == b.shape[3] and a.shape[2] == b.shape[2]; these axes
4131     are compatible. The resulting tensor will have shape (2, 5, 6) -- the
4132     dimensions that are not being summed:
4133     &gt;&gt;&gt; a = np.random.random((2,3,4))
4134     &gt;&gt;&gt; b = np.random.random((5,6,4,3))
4135     &gt;&gt;&gt; c = np.tensordot(a, b, [[1,2],[3,2]])
4136     &gt;&gt;&gt; a0, a1, a2 = a.shape
4137     &gt;&gt;&gt; b0, b1, _, _ = b.shape
4138     &gt;&gt;&gt; cloop = np.zeros((a0,b0,b1))
4139     &gt;&gt;&gt; for i in range(a0):
4140     ...     for j in range(b0):
4141     ...         for k in range(b1):
4142     ...             #loop over summed indices -- these don't exist
4143     ...             #in the tensor product.
4144     ...             for l in range(a1):
4145     ...                 for m in range(a2):
4146     ...                     cloop[i,j,k] += a[i,l,m] * b[j,k,m,l]
4147     &gt;&gt;&gt; np.allclose(c, cloop)
4148     true
4149     This specific implementation avoids a loop by transposing a and b such that
4150     the summed axes of a are last and the summed axes of b are first. The
4151     resulting arrays are reshaped to 2 dimensions (or left as vectors, if
4152     appropriate) and a matrix or vector dot product is taken. The result is
4153     reshaped back to the required output dimensions.
4154     In an extreme case, no axes may be specified. The resulting tensor
4155     will have shape equal to the concatenation of the shapes of a and b:
4156     &gt;&gt;&gt; c = np.tensordot(a, b, 0)
4157     &gt;&gt;&gt; print(a.shape)
4158     (2,3,4)
4159     &gt;&gt;&gt; print(b.shape)
4160     (5,6,4,3)
4161     &gt;&gt;&gt; print(c.shape)
4162     (2,3,4,5,6,4,3)
4163     See the documentation of numpy.tensordot for more examples.
4164     """
4165     return _tensordot_as_dot(a, b, axes, dot=dot, batched=False)
4166 def outer(x, y):
4167     """Return vector-vector outer product.
4168     If an input isn't a vector, we flatten it first.
4169     """
4170     if x.ndim != 1:
4171         x = x.flatten()
4172     if y.ndim != 1:
4173         y = y.flatten()
4174     return dot(
4175         x.dimshuffle(0, 'x'),
4176         y.dimshuffle('x', 0))
4177 def any(x, axis=None, keepdims=False):
4178     out = elemwise.Any(axis)(x)
4179     if keepdims:
4180         out = makeKeepDims(x, out, axis)
4181     return out
4182 def all(x, axis=None, keepdims=False):
4183     out = elemwise.All(axis)(x)
4184     if keepdims:
4185         out = makeKeepDims(x, out, axis)
4186     return out
4187 x = np.zeros((4, 4))
4188 numpy_diagonal_return_view = np.may_share_memory(np.diagonal(x), x)
4189 del x
4190 class ExtractDiag(Op):
4191     """
4192     Return specified diagonals.
4193     If x is 2-D, returns the diagonal of x with the given offset,
4194     i.e., the collection of elements of the form x[i, i+offset].
4195     If x has more than two dimensions, then the axes specified by
4196     axis1 and axis2 are used to determine the 2-D sub-array whose
4197     diagonal is returned. The shape of the resulting array can be
4198     determined by removing axis1 and axis2 and appending an index
4199     to the right equal to the size of the resulting diagonals.
4200     Parameters
4201     ----------
4202     x: A tensor variable with x.ndim &gt;= 2.
4203     offset: Offset of the diagonal from the main diagonal.
4204         Can be positive or negative.
4205         Defaults to main diagonal (0).
4206     axis1: Axis to be used as the first axis of the 2-D
4207         sub-arrays from which the diagonals should be taken.
4208         Defaults to first axis (0).
4209     axis2: Axis to be used as the second axis of the 2-D
4210         sub-arrays from which the diagonals should be taken.
4211         Defaults to second axis (1).
4212     Returns
4213     -------
4214     array_of_diagonals:
4215         If x is 2-D, a 1-D array of the same type as a
4216         containing the diagonal is returned.
4217         If the dimension of x is greater than two, then an
4218         array of diagonals is returned, "packed" from left-most
4219         dimension to right-most (e.g., if x is 3-D, then the
4220         diagonals are "packed" along rows).
4221     Raises
4222     ------
4223     ValueError
4224         If the dimension of x is less than 2.
4225     See Also
4226     --------
4227     numpy.diagonal:
4228         https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.diagonal.html
4229     """
4230     __props__ = ("offset", "axis1", "axis2", "view")
4231     def __init__(self, offset=0, axis1=0, axis2=1, view=False):
4232         self.view = view
4233         if self.view and not numpy_diagonal_return_view:
4234             warnings.warn("View will forced to False. ExtractDiag property view is "
4235                           "set to True but numpy version %s and prior versions of "
4236                           "numpy.diagonal() do not return a view. Update "
4237                           "numpy to use ExtractDiag(view=True)" %
4238                           np.version.version)
4239             self.view = False
4240         if self.view:
4241             self.view_map = {0: [0]}
4242         self.offset = offset
4243         self.axis1 = axis1
4244         self.axis2 = axis2
4245     def make_node(self, x):
4246         x = as_tensor_variable(x)
4247         if x.ndim &lt; 2:
4248             raise ValueError('ExtractDiag needs an input with 2 or more '
4249                              'dimensions', x)
4250         return Apply(self, [x], [x.type.__class__(
4251             dtype=x.dtype,
4252             broadcastable=[False] * (x.ndim - 1))()])
4253     def perform(self, node, inputs, outputs):
4254         (x,) = inputs
4255         (z,) = outputs
4256         z[0] = x.diagonal(self.offset, self.axis1, self.axis2)
4257         if not self.view:
4258             z[0] = z[0].copy()
4259     def grad(self, inputs, gout):
4260         (x,) = inputs
4261         (gz,) = gout
4262         if x.ndim == 2:
4263             x = theano.tensor.zeros_like(x)
4264             xdiag = theano.tensor.AllocDiag(offset=self.offset)(gz)
4265             return [theano.tensor.set_subtensor(
4266                 x[:xdiag.shape[0], :xdiag.shape[1]], xdiag)]
4267         else:
4268             warnings.warn("gradient of theano.tensor.basic.ExtractDiag only"
4269                           "works for matrices.")
4270             return [grad_not_implemented(self, 0, x)]
4271     def infer_shape(self, node, shapes):
4272         in_shape, = shapes
4273         dim1 = in_shape[self.axis1]
4274         dim2 = in_shape[self.axis2]
4275         out_shape = [d for i, d in enumerate(in_shape)
4276                      if i not in (self.axis1, self.axis2)]
4277         offset = self.offset
4278         if offset &gt; 0:
4279             diag_size = clip(dim2 - offset, 0, dim1)
4280         elif offset &lt; 0:
4281             diag_size = clip(dim1 + offset, 0, dim2)
4282         else:
4283             diag_size = minimum(dim1, dim2)
4284         out_shape.append(diag_size)
4285         return [tuple(out_shape)]
4286     def __setstate__(self, state):
4287         self.__dict__.update(state)
4288         if self.view and not numpy_diagonal_return_view:
4289             warnings.warn("View will forced to False. ExtractDiag property view is "
4290                           "set to True but numpy version %s and prior versions of "
4291                           "numpy.diagonal() do not return a view. Update "
4292                           "numpy to use ExtractDiag(view=True)" %
4293                           np.version.version)
4294             self.view = False
4295         if self.view:
4296             self.view_map = {0: [0]}
4297         if "offset" not in state:
4298             self.offset = 0
4299         if "axis1" not in state:
4300             self.axis1 = 0
4301         if "axis2" not in state:
4302             self.axis2 = 1
4303 def diagonal(a, offset=0, axis1=0, axis2=1):
4304     """
4305     A helper function for `theano.tensor.ExtractDiag`. It accepts tensor with
4306     `ndim &gt;= 2` as input. The name `diagonal` is just meant to keep it
4307     consistent with numpy.
4308     Parameters
4309     ----------
4310     a : symbolic tensor
4311     offset : int
4312         offset
4313     axis1 : int
4314     axis2 : int
4315     Returns
4316     -------
4317     tensor : symbolic tensor
4318     """
4319     return ExtractDiag(offset, axis1, axis2)(a)
4320 class AllocDiag(Op):
4321     """
4322     An op that copies a vector to the diagonal of an empty matrix. It does the
4323     inverse of ExtractDiag.
4324     Usage: T.AllocDiag()(x)
4325     `x` should be a tensor vector. The parenthesis in the front should indicate
4326     which main diagonal the vector value goes into. By default it is set to
4327     `0`, which corresponds to setting the values of x to the main diagonal in
4328     the returned matrix.
4329     Parameters
4330     ----------
4331     axis1: Axis to be used as the first axis of the 2-D
4332         sub-arrays to which the diagonals will be allocated.
4333         Defaults to first axis (0).
4334     axis2: Axis to be used as the second axis of the 2-D
4335         sub-arrays to which the diagonals will be allocated.
4336         Defaults to second axis (1).
4337     offset: Offset of the diagonal from the main diagonal defined by `axis1`
4338         and `axis2`.
4339         Can be positive or negative.
4340         Defaults to main diagonal (0).
4341     x: symbolic vector
4342         A tensor vector consists of diagonal values.
4343     Returns
4344     -------
4345     tensor : symbolic tenstor
4346         A tensor with passed tensor values at their corresponding diagonals.
4347     """
4348     __props__ = ("offset", "axis1", "axis2")
4349     def __init__(self, offset=0, axis1=0, axis2=1):
4350         self.offset = offset
4351         self.axis1 = axis1
4352         self.axis2 = axis2
4353     def make_node(self, diag):
4354         diag = as_tensor_variable(diag)
4355         if diag.type.ndim &lt; 1:
4356             raise ValueError('AllocDiag needs an input with 1 or more '
4357                              'dimensions', diag.type)
4358         return Apply(
4359             self, [diag],
4360             [diag.type.__class__(
4361                 dtype=diag.dtype,
4362                 broadcastable=[False] * (diag.ndim + 1))()]
4363         )
4364     def perform(self, node, inputs, outputs):
4365         (x,) = inputs
4366         (z,) = outputs
4367         axis1 = np.minimum(self.axis1, self.axis2)
4368         axis2 = np.maximum(self.axis1, self.axis2)
4369         offset = self.offset
4370         result_shape = x.shape[:-1] + (x.shape[-1] + abs(offset),) * 2
4371         result = np.zeros(result_shape, dtype=x.dtype)
4372         idxs = np.arange(x.shape[-1])
4373         diagonal_slice = ((len(result_shape) - 2) * [slice(None)] +
4374                           [idxs + np.maximum(0, -offset),
4375                            idxs + np.maximum(0, offset)])
4376         result[tuple(diagonal_slice)] = x
4377         if len(x.shape) &gt; 1:
4378             axes = list(range(len(x.shape[:-1])))
4379             last_idx = axes[-1]
4380             axes = axes[:axis1] + [last_idx + 1] + axes[axis1:]
4381             axes = axes[:axis2] + [last_idx + 2] + axes[axis2:]
4382             result = result.transpose(axes)
4383         z[0] = result
4384     def grad(self, inputs, gout):
4385         (gz,) = gout
4386         return [diagonal(
4387             gz,
4388             offset=self.offset,
4389             axis1=self.axis1,
4390             axis2=self.axis2
4391         )]
4392     def infer_shape(self, nodes, shapes):
4393         (x_shape,) = shapes
4394         axis1 = np.minimum(self.axis1, self.axis2)
4395         axis2 = np.maximum(self.axis1, self.axis2)
4396         result_shape = list(x_shape[:-1])
4397         diag_shape = x_shape[-1] + abs(self.offset)
4398         result_shape = result_shape[:axis1] + [diag_shape] + result_shape[axis1:]
4399         result_shape = result_shape[:axis2] + [diag_shape] + result_shape[axis2:]
4400         return [tuple(result_shape)]
4401     def __setstate__(self, state):
4402         if "view_map" in state:
4403             del state["view_map"]
4404         self.__dict__.update(state)
4405         if "offset" not in state:
4406             self.offset = 0
4407         if "axis1" not in state:
4408             self.axis1 = 0
4409         if "axis2" not in state:
4410             self.axis2 = 1
4411 def diag(v, k=0):
4412     """
4413     A helper function for two ops: `theano.tensor.ExtractDiag` and
4414     `theano.tensor.AllocDiag`. The name `diag` is meant to keep it consistent
4415     with numpy. It both accepts tensor vector and tensor matrix.
4416     While the passed tensor variable `v` has `v.ndim&gt;=2`, it builds a
4417     `ExtractDiag` instance, and returns a vector with its entries equal to
4418     `v`'s main diagonal; otherwise if `v.ndim` is `1`, it builds an `AllocDiag`
4419     instance, and returns a matrix with `v` at its k-th diaogonal.
4420     Parameters
4421     ----------
4422     v : symbolic tensor
4423     k : int
4424         offset
4425     Returns
4426     -------
4427     tensor : symbolic tensor
4428     """
4429     if v.ndim == 1:
4430         return AllocDiag(k)(v)
4431     elif v.ndim &gt;= 2:
4432         return diagonal(v, offset=k)
4433     else:
4434         raise ValueError("Input must has v.ndim &gt;= 1.")
4435 def stacklists(arg):
4436     """
4437     Recursively stack lists of tensors to maintain similar structure.
4438     This function can create a tensor from a shaped list of scalars:
4439     Examples
4440     --------
4441     &gt;&gt;&gt; from theano.tensor import stacklists, scalars, matrices
4442     &gt;&gt;&gt; from theano import function
4443     &gt;&gt;&gt; a, b, c, d = scalars('abcd')
4444     &gt;&gt;&gt; X = stacklists([[a, b], [c, d]])
4445     &gt;&gt;&gt; f = function([a, b, c, d], X)
4446     &gt;&gt;&gt; f(1, 2, 3, 4)
4447     array([[ 1.,  2.],
4448            [ 3.,  4.]], dtype=float32)
4449     We can also stack arbitrarily shaped tensors. Here we stack matrices into
4450     a 2 by 2 grid:
4451     &gt;&gt;&gt; from numpy import ones
4452     &gt;&gt;&gt; a, b, c, d = matrices('abcd')
4453     &gt;&gt;&gt; X = stacklists([[a, b], [c, d]])
4454     &gt;&gt;&gt; f = function([a, b, c, d], X)
4455     &gt;&gt;&gt; x = ones((4, 4), 'float32')
4456     &gt;&gt;&gt; f(x, x, x, x).shape
4457     (2, 2, 4, 4)
4458     """
4459     if isinstance(arg, (tuple, list)):
4460         return stack(list(map(stacklists, arg)))
4461     else:
4462         return arg
4463 def ptp(a, axis=None):
4464     """
4465     Range of values (maximum - minimum) along an axis.
4466     The name of the function comes from the acronym for peak to peak.
4467     Parameters
4468     ----------
4469     a
4470         Input tensor.
4471     axis
4472         Axis along which to find the peaks. By default, flatten the array.
4473     Returns
4474     -------
4475     array
4476         A new array holding the result.
4477     """
4478     a = as_tensor_variable(a)
4479     out = max(a, axis) - min(a, axis)
4480     return out
4481 def power(x, y):
4482     return x ** y
4483 def swapaxes(y, axis1, axis2):
4484     "swap axes of inputted tensor"
4485     y = as_tensor_variable(y)
4486     ndim = y.ndim
4487     li = list(range(0, ndim))
4488     li[axis1], li[axis2] = li[axis2], li[axis1]
4489     return y.dimshuffle(li)
4490 def choose(a, choices, out=None, mode='raise'):
4491     """
4492     Construct an array from an index array and a set of arrays to choose from.
4493     First of all, if confused or uncertain, definitely look at the Examples -
4494     in its full generality, this function is less simple than it might seem
4495     from the following code description (below ndi = numpy.lib.index_tricks):
4496     np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)]).
4497     But this omits some subtleties. Here is a fully general summary:
4498     Given an ``index`` array (a) of integers and a sequence of n arrays
4499     (choices), a and each choice array are first broadcast, as necessary,
4500     to arrays of a common shape; calling these Ba and
4501     Bchoices[i], i = 0,...,n-1 we have that, necessarily,
4502     Ba.shape == Bchoices[i].shape for each i.
4503     Then, a new array with shape Ba.shape is created as follows:
4504     - if mode=raise (the default), then, first of all, each element of a
4505       (and thus Ba) must be in the range [0, n-1]; now, suppose that
4506       i (in that range) is the value at the (j0, j1, ..., jm) position in Ba -
4507       then the value at the same position in the new array is the value in
4508       Bchoices[i] at that same position;
4509     - if mode=wrap, values in a (and thus Ba) may be any (signed) integer;
4510       modular arithmetic is used to map integers outside the range [0, n-1]
4511       back into that range; and then the new array is constructed as above;
4512     - if mode=clip, values in a (and thus Ba) may be any (signed) integer;
4513       negative integers are mapped to 0; values greater than n-1 are mapped
4514       to n-1; and then the new array is constructed as above.
4515     Parameters
4516     ----------
4517     a : int array
4518         This array must contain integers in [0, n-1], where n is the number of
4519         choices, unless mode=wrap or mode=clip, in which cases any integers
4520         are permissible.
4521     choices : sequence of arrays
4522         Choice arrays. a and all of the choices must be broadcastable to
4523         the same shape. If choices is itself an array (not recommended),
4524         then its outermost dimension (i.e., the one corresponding to
4525         choices.shape[0]) is taken as defining the ``sequence``.
4526     out : array, optional
4527         If provided, the result will be inserted into this array.
4528         It should be of the appropriate shape and dtype.
4529     mode : {``raise`` (default), ``wrap``, ``clip``}, optional
4530         Specifies how indices outside [0, n-1] will be treated:
4531         ``raise`` : an exception is raised
4532         ``wrap`` : value becomes value mod n
4533         ``clip`` : values &lt; 0 are mapped to 0, values &gt; n-1 are mapped to n-1
4534     Returns
4535     -------
4536     merged_array - array
4537         The merged result.
4538     Raises
4539     ------
4540     ValueError - shape mismatch
4541         If a and each choice array are not all broadcastable to the same shape.
4542     """
4543     assert out is None
4544     return Choose(mode)(a, choices)
4545 class Choose(Op):
4546     __props__ = ('mode',)
4547     def __init__(self, mode):
4548         assert mode in ("raise", "wrap", "clip")
4549         self.mode = mode
4550     def infer_shape(self, node, shapes):
4551         if isinstance(node.inputs[1], TensorVariable):
4552             l = []
4553             for sh1, sh2, b1 in zip(shapes[0],
4554                                     shapes[1][1:],
4555                                     node.inputs[0].broadcastable):
4556                 if b1:
4557                     l.append(sh2)
4558                 else:
4559                     l.append(sh1)
4560             return [tuple(l)]
4561         else:
4562             import theano.typed_list
4563             assert isinstance(node.inputs[1],
4564                               theano.typed_list.TypedListVariable)
4565             raise ShapeError("Case not implemented")
4566             shape = shapes[0]
4567             for i in xrange(len(shapes[0]) - 1):
4568                 shape[i] = shapes[1][i]
4569             return [(shape)]
4570     def make_node(self, a, choices):
4571         import theano.typed_list
4572         a = as_tensor_variable(a)
4573         if a.dtype not in theano.tensor.discrete_dtypes:
4574             raise TypeError(
4575                 'choose first argument must have an [u]int* dtype. Got %s.'
4576                 % a.dtype)
4577         if isinstance(choices, (tuple, list,
4578                                 theano.typed_list.TypedListVariable)):
4579             choice = theano.typed_list.make_list(choices)
4580             choice_ndim = choice.ttype.ndim
4581             choice_bcast = choice.ttype.broadcastable
4582         else:
4583             choice = as_tensor_variable(choices)
4584             choice_ndim = choice.ndim - 1
4585             choice_bcast = choice.broadcastable[1:]
4586         out_ndim = np.max([a.ndim, choice_ndim])
4587         a = shape_padleft(a, out_ndim - a.ndim)
4588         if len(choice_bcast) != out_ndim:
4589             if isinstance(choice.type, TensorType):
4590                 choice = choice.dimshuffle(0,
4591                                            *(('x',) * (out_ndim - choice_ndim) +
4592                                              tuple(range(1, choice.ndim))))
4593                 choice_ndim = choice.ndim - 1
4594                 choice_bcast = choice.broadcastable[1:]
4595             else:
4596                 raise NotImplementedError(
4597                     "We currently didn't implemented that case. "
4598                     "To make it work, explicitly add dimensions "
4599                     "of size one for dimensions that will be broadcasted")
4600         bcast = [False] * out_ndim
4601         for idx, (b1, b2) in enumerate(
4602             zip(a.broadcastable,
4603                 (True,) * (out_ndim - choice_ndim) + choice_bcast)):
4604             if b1 and b2:
4605                 bcast[idx] = True
4606         o = TensorType(choice.dtype, bcast)
4607         return Apply(self, [a, choice], [o()])
4608     def perform(self, node, inputs, outputs):
4609         (z,) = outputs
4610         a = inputs[0]
4611         choice = inputs[1]
4612         z[0] = np.choose(a, choice, mode=self.mode)
4613 class AllocEmpty(gof.Op):
4614     __props__ = ("dtype", )
4615     params_type = ParamsType(typecode=int32_t)
4616     def __init__(self, dtype):
4617         assert isinstance(dtype, str), dtype
4618         self.dtype = dtype.lower()
4619     @property
4620     def typecode(self):
4621         return np.dtype(self.dtype).num
4622     def make_node(self, *shape):
4623         shape, bcast = alloc_validate_shape(shape)
4624         otype = TensorType(dtype=self.dtype, broadcastable=bcast)
4625         output = otype()
4626         output.tag.values_eq_approx = values_eq_approx_always_true
4627         output.type.filter_checks_isfinite = False
4628         output.tag.nan_guard_mode_check = False
4629         return Apply(self, shape, [output])
4630     def debug_perform(self, node, inputs, out_, params):
4631         self.perform(node, inputs, out_, params)
4632         out_[0][0].fill(-123456789)
4633     def perform(self, node, inputs, out_, params):
4634         out, = out_
4635         sh = tuple([int(i) for i in inputs])
4636         if out[0] is None or out[0].shape != sh:
4637             out[0] = np.empty(sh, dtype=self.dtype)
4638     def c_code(self, node, name, inputs, out_, sub):
4639         out, = out_
4640         fail = sub['fail']
4641         shps = inputs
4642         nd = len(shps)
4643         params = sub['params']
4644         str = "npy_intp dims[%(nd)s];\n" % locals()
4645         for idx, sh in enumerate(shps):
4646             str += "dims[%(idx)s] =" \
4647                    "((npy_intp)((dtype_%(sh)s*)" \
4648                    " PyArray_DATA(%(sh)s))[0]);\n" % locals()
4649         str += "if(%(out)s==NULL\n" % locals()
4650         for idx, sh in enumerate(shps):
4651             str += "||PyArray_DIMS(%(out)s)[%(idx)s]!=dims[%(idx)s]" % locals()
4652         str += """){
4653             /* Reference received to invalid output variable.
4654             Decrease received reference's ref count and allocate new
4655             output variable */
4656             Py_XDECREF(%(out)s);
4657             %(out)s = (PyArrayObject*)PyArray_EMPTY(%(nd)s,
4658                                                     dims,
4659                                                     %(params)s-&gt;typecode,
4660                                                     0);
4661             if (!%(out)s)
4662             {
4663                 PyErr_SetString(PyExc_MemoryError, "alloc failed");
4664                 %(fail)s;
4665             }
4666         }
4667         """ % locals()
4668         return str
4669     def infer_shape(self, node, input_shapes):
4670         return [node.inputs]
4671     def c_code_cache_version(self):
4672         return (4,)
4673     def do_constant_folding(self, node):
4674         return False
4675     def connection_pattern(self, node):
4676         return [[False] for i in node.inputs]
4677     def grad(self, inputs, grads):
4678         return [DisconnectedType()() for i in inputs]
4679     def R_op(self, inputs, eval_points):
4680         return [zeros(inputs, self.dtype)]
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_raw_random.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import numpy as np
3 import pickle
4 from theano.tests import unittest_tools as utt
5 from theano.tensor.raw_random import *
6 from theano.tensor import (raw_random, ivector, dvector, iscalar, dcol,
7                            dtensor3)
8 from theano import tensor
9 from theano import compile, config, gof
10 __docformat__ = "restructuredtext en"
11 class T_random_function(utt.InferShapeTester):
12     def setUp(self):
13         utt.seed_rng()
14     def test_basic_usage(self):
15         rf = RandomFunction(np.random.RandomState.uniform, tensor.dvector)
16         assert not rf.inplace
17         assert getattr(rf, 'destroy_map', {}) == {}
18         rng_R = random_state_type()
19         post_r, out = rf(rng_R, (4,), 0., 1.)
20         assert out.type == tensor.dvector
21         f = compile.function([rng_R], out)
22         rng_state0 = np.random.RandomState(utt.fetch_seed())
23         f_0 = f(rng_state0)
24         f_1 = f(rng_state0)
25         assert np.all(f_0 == f_1)
26     def test_inplace_norun(self):
27         rf = RandomFunction(np.random.RandomState.uniform, tensor.dvector,
28                             inplace=True)
29         assert rf.inplace
30         assert getattr(rf, 'destroy_map', {}) != {}
31     def test_args(self):
32         rf4 = RandomFunction(np.random.RandomState.uniform, tensor.dvector,
33                              inplace=True)
34         rng_R <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= random_state_type()
35         post_r2, out2 = rf2(rng_R, (4,), -2, 2)  # NOT INPLACE
36         post_r4, out4 = rf4(rng_R, (4,), -4, 4)  # INPLACE
37         post_r2_4, out2_4 = rf2(rng_R, (4, ), -4.0, 2)  # NOT INPLACE
38         post_r2_4_4, out2_4_4 = rf2(rng_R, (4, ), -4.0, 4.0)  # NOT INPLACE
39         f =</b></font> compile.function(
40                 [compile.In(rng_R,
41                             value=np.random.RandomState(utt.fetch_seed()),
42                             update=post_r4,
43                             mutable=True)],
44                 [out2, out4, out2_4, out2_4_4],
45                 accept_inplace=True)
46         f2, f4, f2_4, f2_4_4 = f()
47         f2b, f4b, f2_4b, f2_4_4b = f()
48         assert np.allclose(f2 * 2, f4), (f2, f4)
49         assert np.allclose(f2_4_4, f4), (f2_4_4, f4)
50         assert not np.allclose(f4, f4b), (f4, f4b)
51     def test_inplace_optimization(self):
52         rf2 = RandomFunction(np.random.RandomState.uniform, tensor.dvector)
53         rng_R = random_state_type()
54         post_r2, out2 = rf2(rng_R, (4,), 0., 1.)
55         f = compile.function(
56                 [compile.In(rng_R,
57                     value=np.random.RandomState(utt.fetch_seed()),
58                     update=post_r2,
59                     mutable=True)],
60                 out2,
61                 mode='FAST_RUN')  # DEBUG_MODE can't pass the id-based
62         id0 = id(f[rng_R])
63         val0 = f()
64         assert id0 == id(f[rng_R])
65         val1 = f()
66         assert id0 == id(f[rng_R])
67         assert not np.allclose(val0, val1)
68     def test_no_inplace(self):
69         rf = RandomFunction('uniform', tensor.dvector)
70         rng_R = random_state_type()
71         post_r, out = rf(rng_R, (3,), 0., 1.)
72         f = compile.function([rng_R], [post_r, out])
73         rng = np.random.RandomState(utt.fetch_seed())
74         rng0, val0 = f(rng)
75         rng_ = np.random.RandomState(utt.fetch_seed())
76         self.assertTrue(rng_R.type.values_eq(rng, rng_))
77         self.assertFalse(rng_R.type.values_eq(rng, rng0))
78         f2 = compile.function(
79                 [compile.In(rng_R,
80                     value=rng,
81                     update=post_r,
82                     mutable=False)],
83                 [post_r, out])
84         rng2, val2 = f2()
85         self.assertTrue(rng_R.type.values_eq(rng, rng_))
86         self.assertFalse(rng_R.type.values_eq(rng, rng2))
87         self.assertTrue(rng_R.type.values_eq(rng2, rng0))
88         rng3, val3 = f2()
89         self.assertTrue(rng_R.type.values_eq(rng2, rng0))
90         self.assertFalse(rng_R.type.values_eq(rng3, rng2))
91         self.assertFalse(rng_R.type.values_eq(rng3, rng))
92     def test_random_function_ndim(self):
93         rng_R = random_state_type()
94         post_out4, out4 = uniform(rng_R, (4,))
95         post_out1_4, out1_4 = uniform(rng_R, (4, ), ndim=1)
96         post_out2_4_4, out2_4_4 = uniform(rng_R, (4, 4), ndim=2)
97         self.assertRaises(ValueError, uniform, rng_R, (4,), ndim=2)
98         f_ok = compile.function(
99                 [compile.In(rng_R,
100                     value=np.random.RandomState(utt.fetch_seed()),
101                     update=post_out2_4_4,
102                     mutable=True)],
103                 [out4, out1_4, out2_4_4],
104                 accept_inplace=True)
105         o4, o1_4, o2_4_4 = f_ok()
106         self.assertTrue(np.allclose(o4, o1_4))
107         self.assertTrue(np.allclose(o4, o2_4_4[0]))
108     def test_random_function_noshape_args(self):
109         rng_R = random_state_type()
110         post_out, out = uniform(rng_R, size=None, ndim=2)
111         f = compile.function(
112                 [compile.In(rng_R,
113                     value=np.random.RandomState(utt.fetch_seed()),
114                     update=post_out,
115                     mutable=True)],
116                 [out],
117                 accept_inplace=True)
118         o, = f()
119         low = tensor.TensorType(dtype='float64',
120                 broadcastable=(False, True, True))()
121         high = tensor.TensorType(dtype='float64',
122         post_out2, out2 = uniform(rng_R, size=None, ndim=2, low=low, high=high)
123         self.assertEqual(out2.ndim, 4)
124         self.assertEqual(out2<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.broadcastable, (True, False, True, False))
125         g = compile.function(
126                 [low,
127                  high,
128                  compile.In(rng_R,
129                     value=np.random.RandomState(</b></font>utt.fetch_seed()),
130                     update=post_out2,
131                     mutable=True)],
132                 [out2],
133                 accept_inplace=True)
134         low_v = [[[3]], [[4]], [[-5]]]
135         high_v = [[[[5, 8]]]]
136         o2, = g(low_v, high_v)
137         self.assertEqual(o2.shape, (1, 3, 1, 2))
138     def test_random_function_noshape_noargs(self):
139         rng_R = random_state_type()
140         self.assertRaises(TypeError, poisson, rng_R, size=None, ndim=2)
141     def test_random_function_ndim_added(self):
142         def ndim_added_deco(ndim_added):
143             def randomfunction(random_state, size=(), low=0.0, high=0.0,
144                                ndim=None):
145                 ndim, size, bcast = raw_random._infer_ndim_bcast(ndim, size)
146                 if ndim_added &lt; 0:
147                     bcast = bcast[:ndim_added]
148                 else:
149                     bcast = bcast + ((False,) * ndim_added)
150                 assert len(bcast) == ndim + ndim_added
151                 op = RandomFunction('uniform',
152                         tensor.TensorType(dtype='float64',
153                         ndim_added=ndim_added)
154                 return op(random_state, size, low, high)
155             r<font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>eturn randomfunction
156         uni_1 = ndim_added_deco(1)
157         uni_0 = ndim_added_deco(0)
158         uni_m1 = ndim_added_deco(-1)
159         rng_R = random_state_type()
160         p_uni11, uni11 = uni_1(rng_R, size=</b></font>(4,))
161         p_uni12, uni12 = uni_1(rng_R, size=(3, 4))
162         p_uni01, uni01 = uni_0(rng_R, size=(4,))
163         p_uni02, uni02 = uni_0(rng_R, size=(3, 4))
164         p_unim11, unim11 = uni_m1(rng_R, size=(4,))
165         p_unim12, unim12 = uni_m1(rng_R, size=(3, 4))
166         self.assertEqual(uni11.ndim, 2)
167         self.assertEqual(uni12.ndim, 3)
168         self.assertEqual(uni01.ndim, 1)
169         self.assertEqual(uni02.ndim, 2)
170         self.assertEqual(unim11.ndim, 0)
171         self.assertEqual(unim12.ndim, 1)
172         f11 = compile.function(
173                 [compile.In(rng_R,
174                     value=np.random.RandomState(utt.fetch_seed()),
175                     update=p_uni11, mutable=True)],
176                 [uni11], accept_inplace=True)
177         f12 = compile.function(
178                 [compile.In(rng_R,
179                     value=np.random.RandomState(utt.fetch_seed()),
180                     update=p_uni12, mutable=True)],
181                 [uni12], accept_inplace=True)
182         fm11 = compile.function(
183                 [compile.In(rng_R,
184                     value=np.random.RandomState(utt.fetch_seed()),
185                     update=p_unim11, mutable=True)],
186                 [unim11], accept_inplace=True)
187         fm12 = compile.function(
188                 [compile.In(rng_R,
189                     value=np.random.RandomState(utt.fetch_seed()),
190                     update=p_unim12, mutable=True)],
191                 [unim12], accept_inplace=True)
192         f0 = compile.function(
193                 [compile.In(rng_R,
194                     value=np.random.RandomState(utt.fetch_seed()),
195                     update=p_uni02, mutable=True)],
196                 [uni01, uni02], accept_inplace=True)
197         self.assertRaises(ValueError, f11)
198         self.assertRaises(ValueError, f12)
199         self.assertRaises(ValueError, fm12)
200         u01, u02 = f0()
201         self.assertTrue(np.allclose(u01, u02<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>[0]))
202     def test_uniform(self):
203         rng_R = random_state_type()
204         post_r, out = uniform(rng_R, (4,), -2.0, 2.0)
205         f = compile.function(
206                 [compile.</b></font>In(rng_R,
207                     value=np.random.RandomState(utt.fetch_seed()),
208                     update=post_r, mutable=True)],
209                 [out], accept_inplace=True)
210         numpy_rng = np.random.RandomState(utt.fetch_seed())
211         val0 = f()
212         val1 = f()
213         numpy_val0 = numpy_rng.uniform(-2.0, 2.0, size=(4,))
214         numpy_val1 = numpy_rng.uniform(-2.0, 2.0, size=(4,))
215         self.assertTrue(np.allclose(val0, numpy_val0))
216         self.assertTrue(np.allclose(val1, numpy_val1))
217     def test_binomial(self):
218         rng_R = random_state_type()
219         post_r, bin = binomial(rng_R, (7, 12), 5, 0.8)
220         f = compile.function(
221                 [compile.In(rng_R,
222                     value=np.random.RandomState(utt.fetch_seed()),
223                     update=post_r, mutable=True)],
224                 [bin], accept_inplace=True)
225         numpy_rng = np.random.RandomState(utt.fetch_seed())
226         val0 = f()
227         val1 = f()
228         numpy_val0 = numpy_rng.binomial(5, 0.8, size=(7, 12))
229         numpy_val1 = numpy_rng.binomial(5, 0.8, size=(7, 12))
230         self.assertTrue(np.all(val0 == numpy_val0))
231         self.assertTrue(np.all(val1 == numpy_val1))
232     def test_normal(self):
233         rng_R = random_state_type()
234         post_r, out = normal(rng_R, (2, 3), 4.0, 2.0)
235         f = compile.function(
236                 [compile.In(rng_R,
237                     value=np.random.RandomState(utt.fetch_seed()),
238                     update=post_r, mutable=True)],
239                 [out], accept_inplace=True)
240         numpy_rng = np.random.RandomState(utt.fetch_seed())
241         val0 = f()
242         val1 = f()
243         numpy_val0 = numpy_rng.normal(4.0, 2.0, size=(2, 3))
244         numpy_val1 = numpy_rng.normal(4.0, 2.0, size=(2, 3))
245         self.assertTrue(np.allclose(val0, numpy_val0))
246         self.assertTrue(np.allclose(val1, numpy_val1))
247     def test_random_integers(self):
248         rng_R = random_state_type()
249         post_r, out = random_integers(rng_R, (11, 8), -3, 16)
250         f = compile.function(
251                 [compile.In(rng_R,
252                     value=np.random.RandomState(utt.fetch_seed()),
253                     update=post_r, mutable=True)],
254                 [out], accept_inplace=True)
255         numpy_rng = np.random.RandomState(utt.fetch_seed())
256         val0 = f()
257         val1 = f()
258         numpy_val0 = numpy_rng.randint(-3, 17, size=(11, 8))
259         numpy_val1 = numpy_rng.randint(-3, 17, size=(11, 8))
260         self.assertTrue(np.allclose(val0, numpy_val0))
261         self.assertTrue(np.allclose(val1, numpy_val1))
262     def test_permutation_helper(self):
263         rf = RandomFunction(permutation_helper, tensor.imatrix, 8,
264                             ndim_added=1)
265         rng_R = random_state_type()
266         post_r, out = rf(rng_R, (7,), 8)
267         f = compile.function(
268                 [compile.In(rng_R,
269                     value=np.random.RandomState(utt.fetch_seed()),
270                     update=post_r, mutable=True)],
271                 [out], accept_inplace=True)
272         numpy_rng = np.random.RandomState(utt.fetch_seed())
273         val0 = f()
274         val1 = f()
275         numpy_val0 = np.asarray([numpy_rng.permutation(8)
276                                     for i in range(7)])
277         numpy_val1 = np.asarray([numpy_rng.permutation(8)
278                                     for i in range(7)])
279         self.assertTrue(np.all(val0 == numpy_val0))
280         self.assertTrue(np.all(val1 == numpy_val1))
281         rf0 = RandomFunction(permutation_helper, tensor.imatrix, 8)
282         post_r0, out0 = rf0(rng_R, (7,), 8)
283         f0 = compile.function(
284                 [compile.In(rng_R,
285                     value=np.random.RandomState(utt.fetch_seed()),
286                     update=post_r0, mutable=True)],
287                 [out0], accept_inplace=True)
288         self.assertRaises(ValueError, f0)
289         rf2 = RandomFunction(permutation_helper, tensor.imatrix, 8,
290                              ndim_added=2)
291         post_r2, out2 = rf2(rng_R, (7,), 8)
292         f2 = compile.function(
293                 [compile.In(rng_R,
294                     value=np.random.RandomState(utt.fetch_seed()),
295                     update=post_r2, mutable=True)],
296                 [out2], accept_inplace=True)
297         self.assertRaises(ValueError, f2)
298     def test_choice(self):
299         rng_R = random_state_type()
300         post_r, out = choice(rng_R, (11, 8), 10, 1, 0)
301         f = compile.function(
302                 [compile.In(rng_R,
303                     value=np.random.RandomState(utt.fetch_seed()),
304                     update=post_r, mutable=True)],
305                 [out], accept_inplace=True)
306         numpy_rng = np.random.RandomState(utt.fetch_seed())
307         val0 = f()
308         val1 = f()
309         numpy_val0 = numpy_rng.choice(10, (11, 8), True, None)
310         numpy_val1 = numpy_rng.choice(10, (11, 8), True, None)
311         self.assertTrue(np.allclose(val0, numpy_val0))
312         self.assertTrue(np.allclose(val1, numpy_val1))
313     def test_poisson(self):
314         rng_R = random_state_type()
315         post_r, out = poisson(rng_R, lam=5, size=(11, 8))
316         f = compile.function(
317                 [compile.In(rng_R,
318                     value=np.random.RandomState(utt.fetch_seed()),
319                     update=post_r, mutable=True)],
320                 [out], accept_inplace=True)
321         numpy_rng = np.random.RandomState(utt.fetch_seed())
322         val0 = f()
323         val1 = f()
324         numpy_val0 = numpy_rng.poisson(5, size=(11, 8))
325         numpy_val1 = numpy_rng.poisson(5, size=(11, 8))
326         self.assertTrue(np.allclose(val0, numpy_val0))
327         self.assertTrue(np.allclose(val1, numpy_val1))
328     def test_permutation(self):
329         rng_R = random_state_type()
330         post_r, out = permutation(rng_R, size=(9,), n=6)
331         f = compile.function(
332                 [compile.In(rng_R,
333                     value=np.random.RandomState(utt.fetch_seed()),
334                     update=post_r, mutable=True)],
335                 [out], accept_inplace=True)
336         numpy_rng = np.random.RandomState(utt.fetch_seed())
337         val0 = f()
338         val1 = f()
339         numpy_val0 = np.asarray([numpy_rng.permutation(6)
340                                     for i in range(9)])
341         numpy_val1 = np.asarray([numpy_rng.permutation(6)
342                                     for i in range(9)])
343         self.assertTrue(np.all(val0 == numpy_val0))
344         self.assertTrue(np.all(val1 == numpy_val1))
345         for ndim in [1, None]:
346             post_r, out = permutation(rng_R, n=10, size=None, ndim=ndim)
347             inp = compile.In(rng_R,
348                              value=np.random.RandomState(utt.fetch_seed()),
349                              update=post_r, mutable=True)
350             f = theano.function([inp], out)
351             o = f()
352             assert o.shape == (10,)
353             assert (np.sort(o) == np.arange(10)).all()
354         self.assertRaises(TypeError, permutation, rng_R, size=None, ndim=2)
355     def test_multinomial(self):
356         rng_R = random_state_type()
357         post_r, out = multinomial(rng_R, (7, 3), 6, [0.2] * 5)
358         f = compile.function(
359                 [compile.In(rng_R,
360                     value=np.random.RandomState(utt.fetch_seed()),
361                     update=post_r, mutable=True)],
362                 [out], accept_inplace=True)
363         numpy_rng = np.random.RandomState(utt.fetch_seed())
364         val0, = f()
365         val1, = f()
366         numpy_val0 = numpy_rng.multinomial(6, [0.2] * 5, (7, 3))
367         numpy_val1 = numpy_rng.multinomial(6, [0.2] * 5, (7, 3))
368         self.assertTrue(np.all(val0 == numpy_val0))
369         self.assertTrue(np.all(val1 == numpy_val1))
370         self.assertTrue(val0.shape == (7, 3, 5))
371         self.assertTrue(val1.shape == (7, 3, 5))
372     def test_symbolic_shape(self):
373         rng_R = random_state_type()
374         shape = tensor.lvector()
375         post_r, out = uniform(rng_R, shape, ndim=2)
376         f = compile.function([rng_R, shape], out)
377         rng_state0 = np.random.RandomState(utt.fetch_seed())
378         assert f(rng_state0, [2, 3]).shape == (2, 3)
379         assert f(rng_state0, [4, 8]).shape == (4, 8)
380         self.assertRaises(ValueError, f, rng_state0, [4])
381         self.assertRaises(ValueError, f, rng_state0, [4, 3, 4, 5])
382     def test_mixed_shape(self):
383         rng_R = random_state_type()
384         shape0 = tensor.lscalar()
385         shape = (shape0, 3)
386         post_r, u = uniform(rng_R, size=shape, ndim=2)
387         f = compile.function([rng_R, shape0], u)
388         rng_state0 = np.random.RandomState(utt.fetch_seed())
389         assert f(rng_state0, 2).shape == (2, 3)
390         assert f(rng_state0, 8).shape == (8, 3)
391         post_r, v = uniform(rng_R, size=shape)
392         g = compile.function([rng_R, shape0], v)
393         assert g(rng_state0, 2).shape == (2, 3)
394         assert g(rng_state0, 8).shape == (8, 3)
395     def test_mixed_shape_bcastable(self):
396         rng_R = random_state_type()
397         shape0 = tensor.lscalar()
398         shape = (shape0, 1)
399         post_r, u = uniform(rng_R, size=shape, ndim=2)
400         assert u.broadcastable == (False, True)
401         f = compile.function([rng_R, shape0], u)
402         rng_state0 = np.random.RandomState(utt.fetch_seed())
403         assert f(rng_state0, 2).shape == (2, 1)
404         assert f(rng_state0, 8).shape == (8, 1)
405         post_r, v = uniform(rng_R, size=shape)
406         assert v.broadcastable == (False, True)
407         g = compile.function([rng_R, shape0], v)
408         assert g(rng_state0, 2).shape == (2, 1)
409         assert g(rng_state0, 8).shape == (8, 1)
410     def test_default_shape(self):
411         rng_R = random_state_type()
412         post_r, out = uniform(rng_R)
413         f = compile.function([rng_R], [post_r, out], accept_inplace=True)
414         rng_state0 = np.random.RandomState(utt.fetch_seed())
415         numpy_rng = np.random.RandomState(utt.fetch_seed())
416         post0, val0 = f(rng_state0)
417         post1, val1 = f(post0)
418         numpy_val0 = np.asarray(numpy_rng.uniform(),
419                                    dtype=theano.config.floatX)
420         numpy_val1 = np.asarray(numpy_rng.uniform(),
421                                    dtype=theano.config.floatX)
422         assert np.all(val0 == numpy_val0)
423         post_r, out = multinomial(rng_R)
424         g = compile<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.function([rng_R], [post_r, out], accept_inplace=True)
425         post2, val2 = g(post1)
426         numpy_val2 = np.asarray(numpy_rng.multinomial(n=</b></font>1, pvals=[.5, .5]),
427                 dtype=theano.config.floatX)
428         assert np.all(val2 == numpy_val2)
429     def test_vector_arguments(self):
430         rng_R = random_state_type()
431         low = tensor.vector()
432         post_r, out = uniform(rng_R, low=low, high=1)
433         assert out.ndim == 1
434         f = compile.function([rng_R, low], [post_r, out], accept_inplace=True)
435         def as_floatX(thing):
436             return np.asarray(thing, dtype=theano.config.floatX)
437         rng_state0 = np.random.RandomState(utt.fetch_seed())
438         numpy_rng = np.random.RandomState(utt.fetch_seed())
439         post0, val0 = f(rng_state0, [-5, .5, 0, 1])
440         post1, val1 = f(post0, as_floatX([.9]))
441         numpy_val0 = as_floatX(numpy_rng.uniform(low=[-5, .5, 0, 1], high=1))
442         numpy_val1 = as_floatX(numpy_rng.uniform(low=as_floatX([.9]), high=1))
443         assert np.all(val0 == numpy_val0)
444         assert np.all(val1 == numpy_val1)
445         high = tensor.vector()
446         post_rb, outb = uniform(rng_R, low=low, high=high)
447         assert outb.ndim == 1
448         fb = compile.function([rng_R, low, high], [post_rb, outb],
449                               accept_inplace=True)
450         post0b, val0b = fb(post1, [-4., -2], [-1, 0])
451         post1b, val1b = fb(post0b, [-4.], [-1])
452         numpy_val0b = as_floatX(numpy_rng.uniform(low=[-4., -2], high=[-1, 0]))
453         numpy_val1b = as_floatX(numpy_rng.uniform(low=[-4.], high=[-1]))
454         assert np.all(val0b == numpy_val0b)
455         assert np.all(val1b == numpy_val1b)
456         self.assertRaises(ValueError, fb, post1b, [-4., -2], [-1, 0, 1])
457         size = tensor.lvector()
458         post_rc, outc = uniform(rng_R, low=low, high=high, size=size, ndim=1)
459         fc = compile.function([rng_R, low, high, size], [post_rc, outc],
460                               accept_inplace=True)
461         post0c, val0c = fc(post1b, [-4., -2], [-1, 0], [2])
462         post1c, val1c = fc(post0c, [-4.], [-1], [1])
463         numpy_val0c = as_floatX(numpy_rng.uniform(low=[-4., -2], high=[-1, 0]))
464         numpy_val1c = as_floatX(numpy_rng.uniform(low=[-4.], high=[-1]))
465         assert np.all(val0c == numpy_val0c)
466         assert np.all(val1c == numpy_val1c)
467         self.assertRaises(ValueError, fc, post1c, [-4., -2], [-1, 0], [1, 2])
468         self.assertRaises(ValueError, fc, post1c, [-4., -2], [-1, 0], [2, 1])
469     def test_broadcast_arguments(self):
470         rng_R = random_state_type()
471         low = tensor.dvector()
472         high = tensor.dcol()
473         post_r, out = uniform(rng_R, low=low, high=high)
474         assert out.ndim == 2
475         f = compile.function([rng_R, low, high], [post_r, out],
476                              accept_inplace=True)
477         rng_state0 = np.random.RandomState(utt.fetch_seed())
478         numpy_rng = np.random.RandomState(utt.fetch_seed())
479         post0, val0 = f(rng_state0, [-5, .5, 0, 1], [[1.]])
480         post1, val1 = f(post0, [.9], [[1.], [1.1], [1.5]])
481         post2, val2 = f(post1, [-5, .5, 0, 1], [[1.], [1.1], [1.5]])
482         numpy_val0 = numpy_rng.uniform(low=[-5, .5, 0, 1], high=[1.])
483         numpy_val1 = numpy_rng.uniform(low=[.9], high=[[1.], [1.1], [1.5]])
484         numpy_val2 = numpy_rng.uniform(low=[-5, .5, 0, 1],
485                                        high=[[1.], [1.1], [1.5]])
486         assert np.all(val0 == numpy_val0), (val0, numpy_val0)
487         assert np.all(val1 == numpy_val1)
488         assert np.all(val2 == numpy_val2)
489     def test_uniform_vector(self):
490         rng_R = random_state_type()
491         low = tensor.vector()
492         high = tensor.vector()
493         post_r, out = uniform(rng_R, low=low, high=high)
494         assert out.ndim == 1
495         f = compile.function([rng_R, low, high], [post_r, out],
496                              accept_inplace=True)
497         def as_floatX(thing):
498             return np.asarray(thing, dtype=theano.config.floatX)
499         low_val = as_floatX([.1, .2, .3])
500         high_val = as_floatX([1.1, 2.2, 3.3])
501         rng = np.random.RandomState(utt.fetch_seed())
502         numpy_rng = np.random.RandomState(utt.fetch_seed())
503         rng0, val0 = f(rng, low_val, high_val)
504         numpy_val0 = as_floatX(numpy_rng.uniform(low=low_val, high=high_val))
505         assert np.all(val0 == numpy_val0)
506         rng1, val1 = f(rng0, low_val[:-1], high_val[:-1])
507         numpy_val1 = as_floatX(numpy_rng.uniform(low=low_val[:-1],
508                                                  high=high_val[:-1]))
509         assert np.all(val1 == numpy_val1)
510         g = compile.function([rng_R, low, high],
511                 uniform(rng_R, low=low, high=high, size=(3,)),
512                 accept_inplace=True)
513         rng2, val2 = g(rng1, low_val, high_val)
514         numpy_val2 = as_floatX(numpy_rng.uniform(low=low_val, high=high_val,
515                                                  size=(3,)))
516         assert np.all(val2 == numpy_val2)
517         self.assertRaises(ValueError, g, rng2, low_val[:-1], high_val[:-1])
518     def test_binomial_vector(self):
519         rng_R = random_state_type()
520         n = tensor.lvector()
521         prob = tensor.vector()
522         post_r, out = binomial(rng_R, n=n, p=prob)
523         assert out.ndim == 1
524         f = compile.function([rng_R, n, prob], [post_r, out],
525                              accept_inplace=True)
526         n_val = [1, 2, 3]
527         prob_val = np.asarray([.1, .2, .3], dtype=config.floatX)
528         rng = np.random.RandomState(utt.fetch_seed())
529         numpy_rng = np.random.RandomState(utt.fetch_seed())
530         rng0, val0 = f(rng, n_val, prob_val)
531         numpy_val0 = numpy_rng.binomial(n=n_val, p=prob_val)
532         assert np.all(val0 == numpy_val0)
533         rng1, val1 = f(rng0, n_val[:-1], prob_val[:-1])
534         numpy_val1 = numpy_rng.binomial(n=n_val[:-1], p=prob_val[:-1])
535         assert np.all(val1 == numpy_val1)
536         g = compile.function([rng_R, n, prob],
537                 binomial(rng_R, n=n, p=prob, size=(3,)),
538                 accept_inplace=True)
539         rng2, val2 = g(rng1, n_val, prob_val)
540         numpy_val2 = numpy_rng.binomial(n=n_val, p=prob_val, size=(3,))
541         assert np.all(val2 == numpy_val2)
542         self.assertRaises(ValueError, g, rng2, n_val[:-1], prob_val[:-1])
543     def test_normal_vector(self):
544         rng_R = random_state_type()
545         avg = tensor.vector()
546         std = tensor.vector()
547         post_r, out = normal(rng_R, avg=avg, std=std)
548         assert out.ndim == 1
549         f = compile.function([rng_R, avg, std], [post_r, out],
550                              accept_inplace=True)
551         def as_floatX(thing):
552             return np.asarray(thing, dtype=theano.config.floatX)
553         avg_val = [1, 2, 3]
554         std_val = as_floatX([.1, .2, .3])
555         rng = np.random.RandomState(utt.fetch_seed())
556         numpy_rng = np.random.RandomState(utt.fetch_seed())
557         rng0, val0 = f(rng, avg_val, std_val)
558         numpy_val0 = as_floatX(numpy_rng.normal(loc=as_floatX(avg_val),
559             scale=as_floatX(std_val)))
560         assert np.all(val0 == numpy_val0)
561         rng1, val1 = f(rng0, avg_val[:-1], std_val[:-1])
562         numpy_val1 = np.asarray(numpy_rng.normal(loc=avg_val[:-1],
563                                                     scale=std_val[:-1]),
564                 dtype=theano.config.floatX)
565         assert np.all(val1 == numpy_val1)
566         g = compile.function([rng_R, avg, std],
567                 normal(rng_R, avg=avg, std=std, size=(3,)),
568                 accept_inplace=True)
569         rng2, val2 = g(rng1, avg_val, std_val)
570         numpy_val2 = np.asarray(numpy_rng.normal(loc=avg_val, scale=std_val,
571                                                     size=(3,)),
572                 dtype=theano.config.floatX)
573         assert np.all(val2 == numpy_val2)
574         self.assertRaises(ValueError, g, rng2, avg_val[:-1], std_val[:-1])
575     def test_random_integers_vector(self):
576         rng_R = random_state_type()
577         low = tensor.lvector()
578         high = tensor.lvector()
579         post_r, out = random_integers(rng_R, low=low, high=high)
580         assert out.ndim == 1
581         f = compile.function([rng_R, low, high], [post_r, out],
582                              accept_inplace=True)
583         low_val = [100, 200, 300]
584         high_val = [110, 220, 330]
585         rng = np.random.RandomState(utt.fetch_seed())
586         numpy_rng = np.random.RandomState(utt.fetch_seed())
587         rng0, val0 = f(rng, low_val, high_val)
588         numpy_val0 = np.asarray([numpy_rng.randint(low=lv, high=hv+1)
589             for lv, hv in zip(low_val, high_val)])
590         assert np.all(val0 == numpy_val0)
591         rng1, val1 = f(rng0, low_val[:-1], high_val[:-1])
592         numpy_val1 = np.asarray([numpy_rng.randint(low=lv, high=hv+1)
593             for lv, hv in zip(low_val[:-1], high_val[:-1])])
594         assert np.all(val1 == numpy_val1)
595         g = compile.function([rng_R, low, high],
596                 random_integers(rng_R, low=low, high=high, size=(3,)),
597                 accept_inplace=True)
598         rng2, val2 = g(rng1, low_val, high_val)
599         numpy_val2 = np.asarray([numpy_rng.randint(low=lv, high=hv+1)
600             for lv, hv in zip(low_val, high_val)])
601         assert np.all(val2 == numpy_val2)
602         self.assertRaises(ValueError, g, rng2, low_val[:-1], high_val[:-1])
603     def test_multinomial_vector(self):
604         rng_R = random_state_type()
605         n = tensor.lvector()
606         pvals = tensor.matrix()
607         post_r, out = multinomial(rng_R, n=n, pvals=pvals)
608         assert out.ndim == 2
609         f = compile.function([rng_R, n, pvals], [post_r, out],
610                              accept_inplace=True)
611         n_val = [1, 2, 3]
612         pvals_val = [[.1, .9], [.2, .8], [.3, .7]]
613         pvals_val = np.asarray(pvals_val, dtype=config.floatX)
614         rng = np.random.RandomState(utt.fetch_seed())
615         numpy_rng = np.random.RandomState(utt.fetch_seed())
616         rng0, val0 = f(rng, n_val, pvals_val)
617         numpy_val0 = np.asarray([numpy_rng.multinomial(n=nv, pvals=pv)
618             for nv, pv in zip(n_val, pvals_val)])
619         assert np.all(val0 == numpy_val0)
620         rng1, val1 = f(rng0, n_val[:-1], pvals_val[:-1])
621         numpy_val1 = np.asarray([numpy_rng.multinomial(n=nv, pvals=pv)
622             for nv, pv in zip(n_val[:-1], pvals_val[:-1])])
623         assert np.all(val1 == numpy_val1)
624         g = compile.function([rng_R, n, pvals],
625                 multinomial(rng_R, n=n, pvals=pvals, size=(3,)),
626                 accept_inplace=True)
627         rng2, val2 = g(rng1, n_val, pvals_val)
628         numpy_val2 = np.asarray([numpy_rng.multinomial(n=nv, pvals=pv)
629             for nv, pv in zip(n_val, pvals_val)])
630         assert np.all(val2 == numpy_val2)
631         self.assertRaises(ValueError, g, rng2, n_val[:-1], pvals_val[:-1])
632     def test_multinomial_tensor3_a(self):
633         rng_R = random_state_type()
634         n = 9
635         pvals = tensor.dtensor3()
636         post_r, out = multinomial(rng_R, n=n, pvals=pvals, size=(1, -1))
637         assert out.ndim == 3
638         assert out.broadcastable == (True, False, False)
639         f = compile.function([rng_R, pvals], [post_r, out],
640                              accept_inplace=True)
641         rng = np.random.RandomState(utt.fetch_seed())
642         numpy_rng = np.random.RandomState(utt.fetch_seed())
643         pvals_val = np.asarray([[[.1, .9], [.2, .8], [.3, .7]]])
644         assert pvals_val.shape == (1, 3, 2)
645         new_rng, draw = f(rng, pvals_val)
646         assert draw.shape == (1, 3, 2)
647         assert np.allclose(draw.sum(axis=2), 9)
648     def test_multinomial_tensor3_b(self):
649         rng_R = random_state_type()
650         n = 9
651         pvals = tensor.dtensor3()
652         post_r, out = multinomial(rng_R, n=n, pvals=pvals, size=(10, 1, -1))
653         assert out.ndim == 4
654         assert out.broadcastable == (False, True, False, False)
655         f = compile.function([rng_R, pvals], [post_r, out],
656                              accept_inplace=True)
657         rng = np.random.RandomState(utt.fetch_seed())
658         numpy_rng = np.random.RandomState(utt.fetch_seed())
659         pvals_val = np.asarray([[[.1, .9], [.2, .8], [.3, .7]]])
660         assert pvals_val.shape == (1, 3, 2)
661         out_rng, draw = f(rng, pvals_val)
662         assert draw.shape == (10, 1, 3, 2)
663         assert np.allclose(draw.sum(axis=3), 9)
664     def test_dtype(self):
665         rng_R = random_state_type()
666         low = tensor.lscalar()
667         high = tensor.lscalar()
668         post_r, out = random_integers(rng_R, low=low, high=high, size=(20, ),
669                                       dtype='int8')
670         assert out.dtype == 'int8'
671         f = compile.function([rng_R, low, high], [post_r, out])
672         rng = np.random.RandomState(utt.fetch_seed())
673         rng0, val0 = f(rng, 0, 9)
674         assert val0.dtype == 'int8'
675         rng1, val1 = f(rng0, 255, 257)
676         assert val1.dtype == 'int8'
677         assert np.all(abs(val1) &lt;= 1)
678     def test_dtype_normal_uniform_687(self):
679         rng_R = random_state_type()
680         assert uniform(rng_R, low=tensor.constant(0, dtype='float64'),
681                        dtype='float32')[1].dtype == 'float32'
682         assert normal(rng_R, avg=tensor.constant(0, dtype='float64'),
683                       dtype='float32')[1].dtype == 'float32'
684     def setUp(self):
685         super(T_random_function, self).setUp()
686     def test_infer_shape(self):
687         rng_R = random_state_type()
688         rng_R_val = np.random.RandomState(utt.fetch_seed())
689         post_r, out = uniform(rng_R)
690         self._compile_and_check([rng_R], [out], [rng_R_val],
691                              RandomFunction)
692         post_r, out = uniform(rng_R, size=None, ndim=2)
693         self._compile_and_check([rng_R], [out], [rng_R_val],
694                                 RandomFunction)
695         '''
696         post_r, out = multinomial(rng_R)
697         self._compile_and_check([rng_R], [out], [rng_R_val],
698                                 RandomFunction)
699         '''
700         low = tensor.TensorType(dtype='float64',
701                 broadcastable=(False, True, True))()
702         high = tensor.TensorType(dtype='float64',
703                 broadcastable=(True, True, True, False))()
704         post_r, out = uniform(rng_R, size=None, ndim=2, low=low, high=high)
705         low_val = [[[3]], [[4]], [[-5]]]
706         high_val = [[[[5, 8]]]]
707         self._compile_and_check([rng_R, low, high], [out],
708                                 [rng_R_val, low_val, high_val],
709                                 RandomFunction)
710         '''
711         n = iscalar()
712         pvals = dvector()
713         size_val = (7, 3)
714         n_val = 6
715         pvals_val = [0.2] * 5
716         post_r, out = multinomial(rng_R, size=size_val, n=n, pvals=pvals,
717                                   ndim=2)
718         self._compile_and_check([rng_R, n, pvals], [out],
719                                 [rng_R_val, n_val, pvals_val],
720                                 RandomFunction)
721         '''
722         low = dvector()
723         high = dvector()
724         post_r, out = uniform(rng_R, low=low, high=1)
725         low_val = [-5, .5, 0, 1]
726         self._compile_and_check([rng_R, low], [out], [rng_R_val, low_val],
727                           RandomFunction)
728         low_val = [.9]
729         self._compile_and_check([rng_R, low], [out], [rng_R_val, low_val],
730                           RandomFunction)
731         post_r, out = uniform(rng_R, low=low, high=high)
732         low_val = [-4., -2]
733         high_val = [-1, 0]
734         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
735                                 high_val], RandomFunction)
736         low_val = [-4.]
737         high_val = [-1]
738         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
739                                 high_val], RandomFunction)
740         low = dvector()
741         high = dcol()
742         post_r, out = uniform(rng_R, low=low, high=high)
743         low_val = [-5, .5, 0, 1]
744         high_val = [[1.]]
745         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
746                                 high_val], RandomFunction)
747         low_val = [.9]
748         high_val = [[1.], [1.1], [1.5]]
749         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
750                                 high_val], RandomFunction)
751         low_val = [-5, .5, 0, 1]
752         high_val = [[1.], [1.1], [1.5]]
753         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
754                                 high_val], RandomFunction)
755         low = dvector()
756         high = dvector()
757         post_r, out = uniform(rng_R, low=low, high=high)
758         low_val = [.1, .2, .3]
759         high_val = [1.1, 2.2, 3.3]
760         size_val = (3, )
761         self._compile_and_check([rng_R, low, high], [out],
762                                 [rng_R_val, low_val[:-1],
763                                 high_val[:-1]], RandomFunction)
764         post_r, out = uniform(rng_R, size=size_val, low=low, high=high)
765         self._compile_and_check([rng_R, low, high], [out], [rng_R_val, low_val,
766                                 high_val], RandomFunction)
767         n = ivector()
768         prob = dvector()
769         post_r, out = binomial(rng_R, n=n, p=prob)
770         n_val = [1, 2, 3]
771         prob_val = [.1, .2, .3]
772         size_val = (3, )
773         self._compile_and_check([rng_R, n, prob], [out],
774                                 [rng_R_val, n_val[:-1],
775                                 prob_val[:-1]], RandomFunction)
776         post_r, out = binomial(rng_R, n=n, p=prob, size=size_val)
777         self._compile_and_check([rng_R, n, prob], [out], [rng_R_val, n_val,
778                                 prob_val], RandomFunction)
779         avg = dvector()
780         std = dvector()
781         post_r, out = normal(rng_R, avg=avg, std=std)
782         avg_val = [1, 2, 3]
783         std_val = [.1, .2, .3]
784         size_val = (3, )
785         self._compile_and_check([rng_R, avg, std], [out],
786                                 [rng_R_val, avg_val[:-1],
787                                 std_val[:-1]], RandomFunction)
788         post_r, out = normal(rng_R, avg=avg, std=std, size=size_val)
789         self._compile_and_check([rng_R, avg, std], [out], [rng_R_val, avg_val,
790                                 std_val], RandomFunction)
791         '''
792         pvals = dtensor3()
793         n = iscalar()
794         post_r, out = multinomial(rng_R, n=n, pvals=pvals, size=(1, -1))
795         pvals_val = [[[.1, .9], [.2, .8], [.3, .7]]]
796         n_val = 9
797         self._compile_and_check([rng_R, n, pvals], [out],
798                                 [rng_R_val, n_val,
799                                 pvals_val], RandomFunction)
800         post_r, out = multinomial(rng_R, n=n, pvals=pvals, size=(10, 1, -1))
801         self._compile_and_check([rng_R, n, pvals], [out],
802                                 [rng_R_val, n_val,
803                                 pvals_val], RandomFunction)
804         '''
805     def test_pkl(self):
806         rng_r = random_state_type()
807         mode = None
808         if theano.config.mode in ["DEBUG_MODE", "DebugMode"]:
809             mode = 'FAST_COMPILE'
810         post_bin_r, bin_sample = binomial(rng_r, (3, 5), 1, .3)
811         f = theano.function([rng_r], [post_bin_r, bin_sample], mode=mode)
812         pkl_f = pickle.dumps(f)
813         post_int_r, int_sample = random_integers(rng_r, (3, 5), -1, 8)
814         g = theano.function([rng_r], [post_int_r, int_sample], mode=mode)
815         pkl_g = pickle.dumps(g)
816         pickle.loads(pkl_g)
817 if __name__ == '__main__':
818     from theano.tests import main
819     main("test_raw_random")
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
