<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for mlp_test.py &amp; test_sigm.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for mlp_test.py &amp; test_sigm.py
      </h3>
<h1 align="center">
        1.5%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>mlp_test.py (3.6474164%)<th>test_sigm.py (0.955414%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(20-21)<td><a href="#" name="0">(179-182)</a><td align="center"><font color="#ff0000">12</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>mlp_test.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 __docformat__ = 'restructedtext en'
3 from collections import OrderedDict
4 import numpy as np
5 import theano
6 import theano.tensor as T
7     train_set = (<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>np.asarray(np.random.rand(10000, 784), dtype='float32'),
8                np.asarray(np.random.rand(</b></font>10000)*10, dtype='int64'))
9     valid_set = (np.asarray(np.random.rand(10000, 784), dtype='float32'),
10                np.asarray(np.random.rand(10000)*10, dtype='int64'))
11     test_set = (np.asarray(np.random.rand(10000, 784), dtype='float32'),
12                np.asarray(np.random.rand(10000)*10, dtype='int64'))
13     def shared_dataset(data_xy):
14         data_x, data_y = data_xy
15         shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX))
16         shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX))
17         return shared_x, T.cast(shared_y, 'int32')
18     test_set_x,  test_set_y  = shared_dataset(test_set)
19     valid_set_x, valid_set_y = shared_dataset(valid_set)
20     train_set_x, train_set_y = shared_dataset(train_set)
21     rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]
22     return rval
23 class LogisticRegression(object):
24     def __init__(self, input, n_in, n_out, name_prefix=''):
25         self.W = theano.shared(value=np.zeros((n_in, n_out), dtype=theano.config.floatX),
26                                 name=name_prefix+'W')
27         self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W))
28         self.y_pred = T.argmax(self.p_y_given_x, axis=1)
29         self.params = [self.W]
30     def negative_log_likelihood(self, y):
31         r"""Return the mean of the negative log-likelihood of the prediction
32         of this model under a given target distribution.
33         .. math::
34             \frac{1}{|\mathcal{D}|} \mathcal{L} (\theta=\{W,b\}, \mathcal{D}) =
35             \frac{1}{|\mathcal{D}|} \sum_{i=0}^{|\mathcal{D}|} \log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\
36                 \ell (\theta=\{W,b\}, \mathcal{D})
37         :type y: theano.tensor.TensorType
38         :param y: corresponds to a vector that gives for each example the
39                   correct label
40         Note: we use the mean instead of the sum so that
41               the learning rate is less dependent on the batch size
42         Typical hidden layer of a MLP: units are fully-connected and have
43         sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)
44         and the bias vector b is of shape (n_out,).
45         NOTE : The nonlinearity used here is tanh
46         Hidden unit activation is given by: tanh(dot(input,W) + b)
47         :type rng: numpy.random.RandomState
48         :param rng: a random number generator used to initialize weights
49         :type input: theano.tensor.dmatrix
50         :param input: a symbolic tensor of shape (n_examples, n_in)
51         :type n_in: int
52         :param n_in: dimensionality of input
53         :type n_out: int
54         :param n_out: number of hidden units
55         :type activation: theano.Op or function
56         :param activation: Non linearity to be applied in the hidden
57                               layer
58     A multilayer perceptron is a feedforward artificial neural network model
59     that has one layer or more of hidden units and nonlinear activations.
60     Intermidiate layers usually have as activation function thanh or the
61     sigmoid function (defined here by a ``SigmoidalLayer`` class)  while the
62     top layer is a softamx layer (defined here by a ``LogisticRegression``
63     class).
64         :type rng: numpy.random.RandomState
65         :param rng: a random number generator used to initialize weights
66         :type input: theano.tensor.TensorType
67         :param input: symbolic variable that describes the input of the
68         architecture (one minibatch)
69         :type n_in: int
70         :param n_in: number of input units, the dimension of the space in
71         which the datapoints lie
72         :type n_hidden: int
73         :param n_hidden: number of hidden units
74         :type n_out: int
75         :param n_out: number of output units, the dimension of the space in
76         which the labels lie
77     Demonstrate stochastic gradient descent optimization for a multilayer
78     perceptron
79     This is demonstrated on MNIST.
80     :type learning_rate: float
81     :param learning_rate: learning rate used (factor for the stochastic
82     gradient
83     :type n_epochs: int
84     :param n_epochs: maximal number of epochs to run the optimizer
85     :type dataset: string
86     :param dataset: the path of the MNIST dataset file from
87                          http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz
88         Return appropriate mode for the tests.
89         :param excluding: List of optimizations to exclude.
90         :return: The current default mode unless the `config.mode` option is
91         set to 'FAST_COMPILE' (in which case it is replaced by the 'FAST_RUN'
92         mode), without the optimizations specified in `excluding`.
93         """
94             assert ([node.op for node in f.maker<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.fgraph.toposort()] == [sigmoid,
95                     T.mul])
96             f(data)
97             f = theano.function([x], (T.fill(</b></font>x, -1.1) * T.exp(x)) /
98                                 ((1 + T.exp(x)) * (1 + T.exp(-x))), mode=m)
99             assert ([node.op for node in f.maker.fgraph.toposort()] != [sigmoid,
100                     T.mul, theano.tensor.inplace.neg_inplace])
101             f(data)
102             f = theano.function([x], (T.fill(x, -1.0) * T.exp(x)) /
103                                 ((2 + T.exp(x)) * (1 + T.exp(-x))), mode=m)
104             assert ([node.op for node in f.maker.fgraph.toposort()] != [sigmoid,
105                     T.mul, theano.tensor.inplace.neg_inplace])
106             f(data)
107             f = theano.function([x], (T.fill(x, -1.0) * T.exp(x)) /
108                                 ((1 + T.exp(x)) * (2 + T.exp(-x))), mode=m)
109             assert ([node.op for node in f.maker.fgraph.toposort()] != [sigmoid,
110                     T.mul, theano.tensor.inplace.neg_inplace])
111             f(data)
112             f = theano.function([x], (T.fill(x, -1.0) * T.exp(x)) /
113                                 ((1 + T.exp(x)) * (1 + T.exp(x))), mode=m)
114             assert ([node.op for node in f.maker.fgraph.toposort()] != [sigmoid,
115                     T.mul, theano.tensor.inplace.neg_inplace])
116             f(data)
117             f = theano.function([x], (T.fill(x, -1.0) * T.exp(x)) /
118                                 ((1 + T.exp(x)) * (2 + T.exp(-x))), mode=m)
119             assert ([node.op for node in f.maker.fgraph.toposort()] != [sigmoid,
120                     T.mul, theano.tensor.inplace.neg_inplace])
121             f(data)
122         finally:
123             config.warn.identify_1pexp_bug = backup
124     def test_1msigmoid(self):
125         if not register_local_1msigmoid:
126             return
127         m = self.get_mode()
128         x = T.fmatrix()
129         f = theano.function([x], 1 - T.exp(x) / (1 + T.exp(x)), mode=m)
130         assert check_stack_trace(f, ops_to_check=[tensor.neg, sigmoid_inplace])
131         assert [node.op for node in f.maker.fgraph.toposort()] == [
132             tensor.neg, sigmoid_inplace]
133         f = theano.function([x], 1 - T.fill(x, 1.0) / (1 + T.exp(-x)), mode=m)
134         assert check_stack_trace(f, ops_to_check=[tensor.neg, sigmoid_inplace])
135         assert ([node.op for node in f.maker.fgraph.toposort()] == [tensor.neg,
136                 sigmoid_inplace])
137     def test_local_sigm_times_exp(self):
138         def match(func, ops):
139             assert [node.op for node in func.maker.fgraph.toposort()] == ops
140         m = self.get_mode(excluding=['local_elemwise_fusion', 'inplace'])
141         x, y = tensor.vectors('x', 'y')
142         f = theano.function([x], sigmoid(-x) * tensor.exp(x), mode=m)
143         match(f, [sigmoid])
144         assert check_stack_trace(f, ops_to_check=sigmoid)
145         f = theano.function([x], sigmoid(x) * tensor.exp(-x), mode=m)
146         match(f, [tensor.neg, sigmoid])
147         assert check_stack_trace(f, ops_to_check=sigmoid)
148         f = theano.function([x], -(-(-(sigmoid(x)))) * tensor.exp(-x), mode=m)
149         match(f, [tensor.neg, sigmoid, tensor.neg])
150         f = theano.function(
151             [x, y],
152             (sigmoid(x) * sigmoid(-y) * -tensor.exp(-x) *
153                 tensor.exp(x * y) * tensor.exp(y)), mode=m)
154         topo = f.maker.fgraph.toposort()
155         for op, nb in [(sigmoid, 2), (tensor.mul, 2),
156                        (tensor.neg, 1), (tensor.exp, 1)]:
157             assert sum([n.op == op for n in topo]) == nb
158     def test_perform_sigm_times_exp(self):
159         x, y, z, t = tensor.vectors('x', 'y', 'z', 't')
160         exp = tensor.exp
161         def ok(expr1, expr2):
162             trees = [parse_mul_tree(e) for e in (expr1, expr2)]
163             perform_sigm_times_exp(trees[0])
164             trees[0] = simplify_mul(trees[0])
165             good = theano.gof.graph.is_same_graph(
166                 compute_mul(trees[0]),
167                 compute_mul(trees[1]))
168             if not good:
169                 print(trees[0])
170                 print(trees[1])
171                 print('***')
172                 theano.printing.debugprint(compute_mul(trees[0]))
173                 print('***')
174                 theano.printing.debugprint(compute_mul(trees[1]))
175             assert good
176         ok(sigmoid(x) * exp(-x), sigmoid(-x))
177         ok(-x * sigmoid(x) * (y * (-1 * z) * exp(-x)),
178            -x * sigmoid(-x) * (y * (-1 * z)))
179         ok(-sigmoid(-x) *
180            (exp(y) * (-exp(-z) * 3 * -exp(x)) *
181             (y * 2 * (-sigmoid(-y) * (z + t) * exp(z)) * sigmoid(z))) * -
182            sigmoid(x),
183            sigmoid(x) *
184            (-sigmoid(y) * (-sigmoid(-z) * 3) * (y * 2 * ((z + t) * exp(z)))) *
185            (-sigmoid(x)))
186         ok(exp(-x) * -exp(-x) * (-sigmoid(x) * -sigmoid(x)),
187            -sigmoid(-x) * sigmoid(-x))
188         ok(-exp(x) * -sigmoid(-x) * -exp(-x),
189            -sigmoid(-x))
190     def test_grad_log1msigm(self):
191         x = tensor.matrix('x')
192         lr = tensor.scalar('lr')
193         s = sigmoid(x)
194         l = T.log(1 - s)
195         c = l.mean()
196         ux = x - lr * theano.grad(c, x)
197         mode = self.get_mode()
198         if not isinstance(mode, theano.compile.DebugMode):
199             f = theano.function([x, lr], ux, mode=mode)
200             ux_v = f([[50]], 0.1)
201             assert not np.isnan(ux_v)
202     def test_local_ultra_fast_sigmoid(self):
203         x = tensor.matrix('x')
204         s = sigmoid(x)
205         mode = self.get_mode('local_ultra_fast_sigmoid')
206         f = theano.function([x], s, mode=mode)
207         assert check_stack_trace(f, ops_to_check=sigmoid)
208         topo = f.maker.fgraph.toposort()
209         assert len(topo) == 1
210         assert topo[0].op == sigmoid
211         mode = self.get_mode().including('local_ultra_fast_sigmoid')
212         f = theano.function([x], s, mode=mode)
213         assert check_stack_trace(f, ops_to_check=ultra_fast_sigmoid)
214         topo = f.maker.fgraph.toposort()
215         assert topo[0].op == ultra_fast_sigmoid
216         assert len(topo) == 1
217         f([[-50, -10, -4, -1, 0, 1, 4, 10, 50]])
218     def test_local_hard_sigmoid(self):
219         x = tensor.matrix('x')
220         s = sigmoid(x)
221         mode = self.get_mode('local_hard_sigmoid')
222         f = theano.function([x], s, mode=mode)
223         assert check_stack_trace(f, ops_to_check=sigmoid)
224         topo = f.maker.fgraph.toposort()
225         assert topo[0].op == sigmoid
226         assert len(topo) == 1
227         mode = self.get_mode().including('local_hard_sigmoid')
228         f = theano.function([x], s, mode=mode)
229         topo = f.maker.fgraph.toposort()
230         assert not any([n.op == sigmoid for n in topo])
231         f([[-50, -10, -4, -1, 0, 1, 4, 10, 50]])
232         mode2 = mode.excluding('fusion').excluding('inplace')
233         f2 = theano.function([x], s, mode=mode2)
234         self.assertTrue(check_stack_trace(f2, ops_to_check=theano.tensor.clip))
235 class T_softplus_opts(unittest.TestCase):
236     def setUp(self):
237         if theano.config.mode == 'FAST_COMPILE':
238             m = theano.compile.mode.get_mode('FAST_RUN').excluding(
239                 'local_elemwise_fusion')
240         else:
241             m = theano.compile.mode.get_default_mode().excluding(
242                 'local_elemwise_fusion')
243         self.m = m
244         utt.seed_rng()
245     def test_logsigm_to_softplus(self):
246         x = T.vector()
247         out = T.log(sigmoid(x))
248         f = theano.function([x], out, mode=self.m)
249         topo = f.maker.fgraph.toposort()
250         assert len(topo) == 3
251         assert isinstance(topo[0].op.scalar_op, theano.scalar.Neg)
252         assert isinstance(topo[1].op.scalar_op,
253                           theano.tensor.nnet.sigm.ScalarSoftplus)
254         assert isinstance(topo[2].op.scalar_op, theano.scalar.Neg)
255         f(np.random.rand(54).astype(config.floatX))
256     def test_log1msigm_to_softplus(self):
257         x = T.matrix()
258         out = T.log(1 - sigmoid(x))
259         f = theano.function([x], out, mode=self.m)
260         topo = f.maker.fgraph.toposort()
261         assert len(topo) == 2
262         assert isinstance(topo[0].op.scalar_op,
263                           theano.tensor.nnet.sigm.ScalarSoftplus)
264         assert isinstance(topo[1].op.scalar_op, theano.scalar.Neg)
265         f(np.random.rand(54, 11).astype(config.floatX))
266         out = T.log(1 - T.flatten(sigmoid(x)))
267         f = theano.function([x], out, mode=self.m)
268         topo = f.maker.fgraph.toposort()
269         assert len(topo) == 3
270         assert tensor.is_flat(topo[0].outputs[0])
271         assert isinstance(topo[1].op.scalar_op,
272                           theano.tensor.nnet.sigm.ScalarSoftplus)
273         assert isinstance(topo[2].op.scalar_op, theano.scalar.Neg)
274         f(np.random.rand(54, 11).astype(config.floatX))
275         out = T.log(1 - sigmoid(x).reshape([x.size]))
276         f = theano.function([x], out, mode=self.m)
277         topo = f.maker.fgraph.toposort()
278         assert any(isinstance(node.op, T.Reshape) for node in topo)
279         assert any(isinstance(getattr(node.op, 'scalar_op', None),
280                               theano.tensor.nnet.sigm.ScalarSoftplus)
281                    for node in topo)
282         f(np.random.rand(54, 11).astype(config.floatX))
283     def test_log1pexp_to_softplus(self):
284         m = theano.config.mode
285         if m == 'FAST_COMPILE':
286             m = 'FAST_RUN'
287         x = T.vector()
288         out = T.log(1 + T.exp(x))
289         f = theano.function([x], out, mode=self.m)
290         topo = f.maker.fgraph.toposort()
291         assert len(topo) == 1
292         assert isinstance(topo[0].op.scalar_op,
293                           theano.tensor.nnet.sigm.ScalarSoftplus)
294         f(np.random.rand(54).astype(config.floatX))
295 class T_sigmoid_utils(unittest.TestCase):
296     """
297     Test utility functions found in 'sigm.py'.
298     """
299     def test_compute_mul(self):
300         x, y, z = tensor.vectors('x', 'y', 'z')
301         tree = (x * y) * -z
302         mul_tree = parse_mul_tree(tree)
303         assert parse_mul_tree(compute_mul(mul_tree)) == mul_tree
304         assert theano.gof.graph.is_same_graph(
305             compute_mul(parse_mul_tree(tree)), tree)
306     def test_parse_mul_tree(self):
307         x, y, z = tensor.vectors('x', 'y', 'z')
308         assert parse_mul_tree(x * y) == [False, [[False, x], [False, y]]]
309         assert parse_mul_tree(-(x * y)) == [True, [[False, x], [False, y]]]
310         assert parse_mul_tree(-x * y) == [False, [[True, x], [False, y]]]
311         assert parse_mul_tree(-x) == [True, x]
312         assert parse_mul_tree((x * y) * -z) == [
313             False, [[False, [[False, x], [False, y]]], [True, z]]]
314     def test_is_1pexp(self):
315         backup = config.warn.identify_1pexp_bug
316         config.warn.identify_1pexp_bug = False
317         try:
318             x = tensor.vector('x')
319             exp = tensor.exp
320             assert is_1pexp(1 + exp(x), False) == (False, x)
321             assert is_1pexp(exp(x) + 1, False) == (False, x)
322             for neg, exp_arg in imap(lambda x:
323                                      is_1pexp(x, only_process_constants=False),
324                                      [(1 + exp(-x)), (exp(-x) + 1)]):
325                 assert not neg and theano.gof.graph.is_same_graph(exp_arg, -x)
326             assert is_1pexp(1 - exp(x), False) is None
327             assert is_1pexp(2 + exp(x), False) is None
328             assert is_1pexp(exp(x) + 2, False) is None
329             assert is_1pexp(exp(x) - 1, False) is None
330             assert is_1pexp(-1 + exp(x), False) is None
331             assert is_1pexp(1 + 2 * exp(x), False) is None
332         finally:
333             config.warn.identify_1pexp_bug = backup
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
