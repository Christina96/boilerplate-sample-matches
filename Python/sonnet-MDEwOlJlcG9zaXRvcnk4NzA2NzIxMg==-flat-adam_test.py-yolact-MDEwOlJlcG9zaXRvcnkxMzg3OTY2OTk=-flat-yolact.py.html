
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 4.4541010320478005%, Tokens: 9</h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-adam_test.py</h3>
            <pre><code>1  from sonnet.src import test_utils
2  from sonnet.src.optimizers import adam
3  from sonnet.src.optimizers import optimizer_tests
4  import tensorflow as tf
5  CONFIGS = optimizer_tests.named_product(learning_rate=(0.1, 0.01, 0.001),
6                                          beta_1=(0.9, 0.99, 0.999),
7                                          beta_2=(0.9, 0.99, 0.999),
8                                          epsilon=(1e-8,),
9                                          seed=(28, 2, 90))
10  class ComparisonTest(optimizer_tests.AbstractFuzzTest):
11    def _make_tf(self, learning_rate, beta_1, beta_2, epsilon):
12      optimizer = tf.optimizers.Adam(learning_rate=learning_rate,
13                                     beta_1=beta_1,
14                                     beta_2=beta_2,
15                                     epsilon=epsilon)
16      return lambda g, p: optimizer.apply_gradients(zip(g, p))
17    def _make_snt(self, learning_rate, beta_1, beta_2, epsilon):
18      optimizer = adam.Adam(learning_rate=learning_rate,
19                            beta1=beta_1,
20                            beta2=beta_2,
21                            epsilon=epsilon)
22      return optimizer.apply
23    @test_utils.combined_named_parameters(CONFIGS)
24    def testComparingSonnetAndTensorFlow(self, config):
25      seed = config.pop("seed")
26      self.assertParametersRemainClose(seed, config)
27  class AdamTest(optimizer_tests.OptimizerTestBase):
28    def make_optimizer(self, **kwargs):
29      if "learning_rate" not in kwargs:
30        kwargs["learning_rate"] = 0.001
31      return adam.Adam(**kwargs)
32    def testDense(self):
33      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
34      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
35      optimizer = self.make_optimizer(learning_rate=0.001)
36      optimizer.apply(updates, parameters)
37      self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
38                          [x.numpy() for x in parameters])
39      optimizer.apply(updates, parameters)
40      self.assertAllClose([[0.998, 1.998], [2.998, 3.998]],
41                          [x.numpy() for x in parameters])
42      optimizer.apply(updates, parameters)
43      self.assertAllClose([[0.997, 1.997], [2.997, 3.997]],
44                          [x.numpy() for x in parameters])
45    def testSparse(self):
46      if self.primary_device in ("GPU", "TPU"):
47        self.skipTest("IndexedSlices not supported on {}.".format(
48            self.primary_device))
49      parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
<span onclick='openModal()' class='match'>50      tf_parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
51      updates = [
52          tf.IndexedSlices(
53              tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
</span>54              tf.constant([2, 1])),
55          tf.IndexedSlices(
56              tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
57              tf.constant([2, 1]))
58      ]
59      optimizer = self.make_optimizer(learning_rate=0.001)
60      tf_optimizer = tf.optimizers.Adam(learning_rate=0.001)
61      optimizer.apply(updates, parameters)
62      self.assertAllClose([[0.999], [2.0]], parameters[0].numpy())
63      self.assertAllClose([[3.0], [3.999]], parameters[1].numpy())
64      tf_optimizer.apply_gradients(zip(updates, tf_parameters))
65      self.assertAllClose(tf_parameters[0].numpy(), parameters[0].numpy())
66      self.assertAllClose(tf_parameters[1].numpy(), parameters[1].numpy())
67      optimizer.apply(updates, parameters)
68      self.assertAllClose([[0.998], [2.0]], parameters[0].numpy())
69      self.assertAllClose([[3.0], [3.998]], parameters[1].numpy())
70      tf_optimizer.apply_gradients(zip(updates, tf_parameters))
71      self.assertAllClose(tf_parameters[0].numpy(), parameters[0].numpy())
72      self.assertAllClose(tf_parameters[1].numpy(), parameters[1].numpy())
73      optimizer.apply(updates, parameters)
74      self.assertAllClose([[0.997], [2.0]], parameters[0].numpy())
75      self.assertAllClose([[3.0], [3.997]], parameters[1].numpy())
76      tf_optimizer.apply_gradients(zip(updates, tf_parameters))
77      self.assertAllClose(tf_parameters[0].numpy(), parameters[0].numpy())
78      self.assertAllClose(tf_parameters[1].numpy(), parameters[1].numpy())
79    def testVariableHyperParams(self):
80      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
81      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
82      learning_rate = tf.Variable(0.001)
83      optimizer = self.make_optimizer(learning_rate=learning_rate)
84      optimizer.apply(updates, parameters)
85      self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
86                          [x.numpy() for x in parameters])
87      learning_rate.assign(0.1)
88      self.assertAlmostEqual(0.1, optimizer.learning_rate.numpy())
89      optimizer.apply(updates, parameters)
90      self.assertAllClose([[0.899, 1.899], [2.899, 3.899]],
91                          [x.numpy() for x in parameters],
92                          rtol=1e-4)
93    def testHyperParamDTypeConversion(self):
94      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
95      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
96      dtype = tf.float32 if self.primary_device == "TPU" else tf.float64
97      learning_rate = tf.Variable(0.001, dtype=dtype)
98      beta1 = tf.Variable(0.9, dtype=dtype)
99      beta2 = tf.Variable(0.999, dtype=dtype)
100      epsilon = tf.Variable(1e-8, dtype=dtype)
101      optimizer = self.make_optimizer(
102          learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)
103      optimizer.apply(updates, parameters)
104      self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],
105                          [x.numpy() for x in parameters],
106                          rtol=1e-4)
107    def testAuxVariablesColocatedWithOriginal(self):
108      optimizer = self.make_optimizer(learning_rate=0.001)
109      with tf.device("CPU:0"):
110        var = tf.Variable(1.0)
111      optimizer.apply([tf.constant(0.1)], [var])
112      self.assertEqual(optimizer.m[0].device, var.device)
113      self.assertEqual(optimizer.v[0].device, var.device)
114  class ReferenceAdamTest(optimizer_tests.OptimizerTestBase):
115    def make_optimizer(self, **kwargs):
116      if "learning_rate" not in kwargs:
117        kwargs["learning_rate"] = 0.001
118      return optimizer_tests.WrappedTFOptimizer(tf.optimizers.Adam(**kwargs))
119  if __name__ == "__main__":
120    tf.test.main()
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-yolact.py</h3>
            <pre><code>1  import torch, torchvision
2  import torch.nn as nn
3  import torch.nn.functional as F
4  from torchvision.models.resnet import Bottleneck
5  import numpy as np
6  from itertools import product
7  from math import sqrt
8  from typing import List
9  from collections import defaultdict
10  from data.config import cfg, mask_type
11  from layers import Detect
12  from layers.interpolate import InterpolateModule
13  from backbone import construct_backbone
14  import torch.backends.cudnn as cudnn
15  from utils import timer
16  from utils.functions import MovingAverage, make_net
17  torch.cuda.current_device()
18  use_jit = torch.cuda.device_count() <= 1
19  if not use_jit:
20      print('Multiple GPUs detected! Turning off JIT.')
21  ScriptModuleWrapper = torch.jit.ScriptModule if use_jit else nn.Module
22  script_method_wrapper = torch.jit.script_method if use_jit else lambda fn, _rcn=None: fn
23  class Concat(nn.Module):
24      def __init__(self, nets, extra_params):
25          super().__init__()
26          self.nets = nn.ModuleList(nets)
27          self.extra_params = extra_params
28      def forward(self, x):
29          return torch.cat([net(x) for net in self.nets], dim=1, **self.extra_params)
30  prior_cache = defaultdict(lambda: None)
31  class PredictionModule(nn.Module):
32      def __init__(self, in_channels, out_channels=1024, aspect_ratios=[[1]], scales=[1], parent=None, index=0):
33          super().__init__()
34          self.num_classes = cfg.num_classes
35          self.mask_dim    = cfg.mask_dim # Defined by Yolact
36          self.num_priors  = sum(len(x)*len(scales) for x in aspect_ratios)
37          self.parent      = [parent] # Don't include this in the state dict
38          self.index       = index
39          self.num_heads   = cfg.num_heads # Defined by Yolact
40          if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:
41              self.mask_dim = self.mask_dim // self.num_heads
42          if cfg.mask_proto_prototypes_as_features:
43              in_channels += self.mask_dim
44          if parent is None:
45              if cfg.extra_head_net is None:
46                  out_channels = in_channels
47              else:
48                  self.upfeature, out_channels = make_net(in_channels, cfg.extra_head_net)
49              if cfg.use_prediction_module:
50                  self.block = Bottleneck(out_channels, out_channels // 4)
51                  self.conv = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True)
52                  self.bn = nn.BatchNorm2d(out_channels)
53              self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4,                **cfg.head_layer_params)
54              self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, **cfg.head_layer_params)
55              self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim,    **cfg.head_layer_params)
56              if cfg.use_mask_scoring:
57                  self.score_layer = nn.Conv2d(out_channels, self.num_priors, **cfg.head_layer_params)
58              if cfg.use_instance_coeff:
59                  self.inst_layer = nn.Conv2d(out_channels, self.num_priors * cfg.num_instance_coeffs, **cfg.head_layer_params)
60              def make_extra(num_layers):
61                  if num_layers == 0:
62                      return lambda x: x
63                  else:
64                      return nn.Sequential(*sum([[
65                          nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
66                          nn.ReLU(inplace=True)
67                      ] for _ in range(num_layers)], []))
68              self.bbox_extra, self.conf_extra, self.mask_extra = [make_extra(x) for x in cfg.extra_layers]
69              if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_coeff_gate:
70                  self.gate_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim, kernel_size=3, padding=1)
71          self.aspect_ratios = aspect_ratios
72          self.scales = scales
73          self.priors = None
74          self.last_conv_size = None
75          self.last_img_size = None
76      def forward(self, x):
77          src = self if self.parent[0] is None else self.parent[0]
78          conv_h = x.size(2)
79          conv_w = x.size(3)
80          if cfg.extra_head_net is not None:
81              x = src.upfeature(x)
82          if cfg.use_prediction_module:
83              a = src.block(x)
84              b = src.conv(x)
85              b = src.bn(b)
86              b = F.relu(b)
87              x = a + b
88          bbox_x = src.bbox_extra(x)
89          conf_x = src.conf_extra(x)
90          mask_x = src.mask_extra(x)
91          bbox = src.bbox_layer(bbox_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)
92          conf = src.conf_layer(conf_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)
93          if cfg.eval_mask_branch:
94              mask = src.mask_layer(mask_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)
95          else:
96              mask = torch.zeros(x.size(0), bbox.size(1), self.mask_dim, device=bbox.device)
97          if cfg.use_mask_scoring:
98              score = src.score_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 1)
99          if cfg.use_instance_coeff:
100              inst = src.inst_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, cfg.num_instance_coeffs)    
101          if cfg.use_yolo_regressors:
102              bbox[:, :, :2] = torch.sigmoid(bbox[:, :, :2]) - 0.5
103              bbox[:, :, 0] /= conv_w
104              bbox[:, :, 1] /= conv_h
105          if cfg.eval_mask_branch:
106              if cfg.mask_type == mask_type.direct:
107                  mask = torch.sigmoid(mask)
108              elif cfg.mask_type == mask_type.lincomb:
109                  mask = cfg.mask_proto_coeff_activation(mask)
110                  if cfg.mask_proto_coeff_gate:
111                      gate = src.gate_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)
112                      mask = mask * torch.sigmoid(gate)
113          if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:
114              mask = F.pad(mask, (self.index * self.mask_dim, (self.num_heads - self.index - 1) * self.mask_dim), mode='constant', value=0)
115          priors = self.make_priors(conv_h, conv_w, x.device)
116          preds = { 'loc': bbox, 'conf': conf, 'mask': mask, 'priors': priors }
117          if cfg.use_mask_scoring:
118              preds['score'] = score
119          if cfg.use_instance_coeff:
120              preds['inst'] = inst
121          return preds
122      def make_priors(self, conv_h, conv_w, device):
123          global prior_cache
124          size = (conv_h, conv_w)
125          with timer.env('makepriors'):
126              if self.last_img_size != (cfg._tmp_img_w, cfg._tmp_img_h):
127                  prior_data = []
128                  for j, i in product(range(conv_h), range(conv_w)):
129                      x = (i + 0.5) / conv_w
130                      y = (j + 0.5) / conv_h
131                      for ars in self.aspect_ratios:
132                          for scale in self.scales:
133                              for ar in ars:
134                                  if not cfg.backbone.preapply_sqrt:
135                                      ar = sqrt(ar)
136                                  if cfg.backbone.use_pixel_scales:
137                                      w = scale * ar / cfg.max_size
138                                      h = scale / ar / cfg.max_size
139                                  else:
140                                      w = scale * ar / conv_w
141                                      h = scale / ar / conv_h
142                                  if cfg.backbone.use_square_anchors:
143                                      h = w
144                                  prior_data += [x, y, w, h]
145                  self.priors = torch.Tensor(prior_data, device=device).view(-1, 4).detach()
146                  self.priors.requires_grad = False
147                  self.last_img_size = (cfg._tmp_img_w, cfg._tmp_img_h)
148                  self.last_conv_size = (conv_w, conv_h)
149                  prior_cache[size] = None
150              elif self.priors.device != device:
151                  if prior_cache[size] is None:
152                      prior_cache[size] = {}
153                  if device not in prior_cache[size]:
154                      prior_cache[size][device] = self.priors.to(device)
155                  self.priors = prior_cache[size][device]
156          return self.priors
157  class FPN(ScriptModuleWrapper):
158      __constants__ = ['interpolation_mode', 'num_downsample', 'use_conv_downsample', 'relu_pred_layers',
159                       'lat_layers', 'pred_layers', 'downsample_layers', 'relu_downsample_layers']
160      def __init__(self, in_channels):
161          super().__init__()
162          self.lat_layers  = nn.ModuleList([
163              nn.Conv2d(x, cfg.fpn.num_features, kernel_size=1)
164              for x in reversed(in_channels)
165          ])
166          padding = 1 if cfg.fpn.pad else 0
167          self.pred_layers = nn.ModuleList([
168              nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=padding)
169              for _ in in_channels
170          ])
171          if cfg.fpn.use_conv_downsample:
172              self.downsample_layers = nn.ModuleList([
173                  nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=1, stride=2)
174                  for _ in range(cfg.fpn.num_downsample)
175              ])
176          self.interpolation_mode     = cfg.fpn.interpolation_mode
177          self.num_downsample         = cfg.fpn.num_downsample
178          self.use_conv_downsample    = cfg.fpn.use_conv_downsample
179          self.relu_downsample_layers = cfg.fpn.relu_downsample_layers
180          self.relu_pred_layers       = cfg.fpn.relu_pred_layers
181      @script_method_wrapper
182      def forward(self, convouts:List[torch.Tensor]):
183          out = []
184          x = torch.zeros(1, device=convouts[0].device)
185          for i in range(len(convouts)):
186              out.append(x)
187          j = len(convouts)
188          for lat_layer in self.lat_layers:
189              j -= 1
190              if j < len(convouts) - 1:
191                  _, _, h, w = convouts[j].size()
192                  x = F.interpolate(x, size=(h, w), mode=self.interpolation_mode, align_corners=False)
193              x = x + lat_layer(convouts[j])
194              out[j] = x
195          j = len(convouts)
196          for pred_layer in self.pred_layers:
197              j -= 1
198              out[j] = pred_layer(out[j])
199              if self.relu_pred_layers:
200                  F.relu(out[j], inplace=True)
201          cur_idx = len(out)
202          if self.use_conv_downsample:
203              for downsample_layer in self.downsample_layers:
204                  out.append(downsample_layer(out[-1]))
205          else:
206              for idx in range(self.num_downsample):
207                  out.append(nn.functional.max_pool2d(out[-1], 1, stride=2))
208          if self.relu_downsample_layers:
209              for idx in range(len(out) - cur_idx):
210                  out[idx] = F.relu(out[idx + cur_idx], inplace=False)
211          return out
212  class FastMaskIoUNet(ScriptModuleWrapper):
213      def __init__(self):
214          super().__init__()
215          input_channels = 1
216          last_layer = [(cfg.num_classes-1, 1, {})]
217          self.maskiou_net, _ = make_net(input_channels, cfg.maskiou_net + last_layer, include_last_relu=True)
218      def forward(self, x):
219          x = self.maskiou_net(x)
220          maskiou_p = F.max_pool2d(x, kernel_size=x.size()[2:]).squeeze(-1).squeeze(-1)
221          return maskiou_p
222  class Yolact(nn.Module):
223      def __init__(self):
224          super().__init__()
225          self.backbone = construct_backbone(cfg.backbone)
226          if cfg.freeze_bn:
227              self.freeze_bn()
228          if cfg.mask_type == mask_type.direct:
229              cfg.mask_dim = cfg.mask_size**2
230          elif cfg.mask_type == mask_type.lincomb:
231              if cfg.mask_proto_use_grid:
232                  self.grid = torch.Tensor(np.load(cfg.mask_proto_grid_file))
233                  self.num_grids = self.grid.size(0)
234              else:
235                  self.num_grids = 0
236              self.proto_src = cfg.mask_proto_src
237              if self.proto_src is None: in_channels = 3
238              elif cfg.fpn is not None: in_channels = cfg.fpn.num_features
239              else: in_channels = self.backbone.channels[self.proto_src]
240              in_channels += self.num_grids
241              self.proto_net, cfg.mask_dim = make_net(in_channels, cfg.mask_proto_net, include_last_relu=False)
242              if cfg.mask_proto_bias:
243                  cfg.mask_dim += 1
244          self.selected_layers = cfg.backbone.selected_layers
245          src_channels = self.backbone.channels
246          if cfg.use_maskiou:
247              self.maskiou_net = FastMaskIoUNet()
248          if cfg.fpn is not None:
249              self.fpn = FPN([src_channels[i] for i in self.selected_layers])
250              self.selected_layers = list(range(len(self.selected_layers) + cfg.fpn.num_downsample))
251              src_channels = [cfg.fpn.num_features] * len(self.selected_layers)
252          self.prediction_layers = nn.ModuleList()
253          cfg.num_heads = len(self.selected_layers)
254          for idx, layer_idx in enumerate(self.selected_layers):
255              parent = None
256              if cfg.share_prediction_module and idx > 0:
257                  parent = self.prediction_layers[0]
258              pred = PredictionModule(src_channels[layer_idx], src_channels[layer_idx],
259                                      aspect_ratios = cfg.backbone.pred_aspect_ratios[idx],
260                                      scales        = cfg.backbone.pred_scales[idx],
261                                      parent        = parent,
262                                      index         = idx)
263              self.prediction_layers.append(pred)
264          if cfg.use_class_existence_loss:
265              self.class_existence_fc = nn.Linear(src_channels[-1], cfg.num_classes - 1)
266          if cfg.use_semantic_segmentation_loss:
267              self.semantic_seg_conv = nn.Conv2d(src_channels[0], cfg.num_classes-1, kernel_size=1)
268          self.detect = Detect(cfg.num_classes, bkg_label=0, top_k=cfg.nms_top_k,
269              conf_thresh=cfg.nms_conf_thresh, nms_thresh=cfg.nms_thresh)
270      def save_weights(self, path):
271          torch.save(self.state_dict(), path)
272      def load_weights(self, path):
273          state_dict = torch.load(path)
274          for key in list(state_dict.keys()):
275              if key.startswith('backbone.layer') and not key.startswith('backbone.layers'):
276                  del state_dict[key]
277              if key.startswith('fpn.downsample_layers.'):
278                  if cfg.fpn is not None and int(key.split('.')[2]) >= cfg.fpn.num_downsample:
279                      del state_dict[key]
280          self.load_state_dict(state_dict)
281      def init_weights(self, backbone_path):
282          self.backbone.init_backbone(backbone_path)
283          conv_constants = getattr(nn.Conv2d(1, 1, 1), '__constants__')
284          def all_in(x, y):
285              for _x in x:
286                  if _x not in y:
287                      return False
288              return True
289          for name, module in self.named_modules():
290              is_script_conv = False
291              if 'Script' in type(module).__name__:
292                  if hasattr(module, 'original_name'):
293                      is_script_conv = 'Conv' in module.original_name
294                  else:
295                      is_script_conv = (
296                          all_in(module.__dict__['_constants_set'], conv_constants)
297                          and all_in(conv_constants, module.__dict__['_constants_set']))
298              is_conv_layer = isinstance(module, nn.Conv2d) or is_script_conv
299              if is_conv_layer and module not in self.backbone.backbone_modules:
300                  nn.init.xavier_uniform_(module.weight.data)
301                  if module.bias is not None:
302                      if cfg.use_focal_loss and 'conf_layer' in name:
303                          if not cfg.use_sigmoid_focal_loss:
304                              module.bias.data[0]  = np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)
305                              module.bias.data[1:] = -np.log(module.bias.size(0) - 1)
306                          else:
307                              module.bias.data[0]  = -np.log(cfg.focal_loss_init_pi / (1 - cfg.focal_loss_init_pi))
308                              module.bias.data[1:] = -np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)
309                      else:
310                          module.bias.data.zero_()
311      def train(self, mode=True):
312          super().train(mode)
313          if cfg.freeze_bn:
314              self.freeze_bn()
315      def freeze_bn(self, enable=False):
316          for module in self.modules():
317              if isinstance(module, nn.BatchNorm2d):
318                  module.train() if enable else module.eval()
319                  module.weight.requires_grad = enable
320                  module.bias.requires_grad = enable
321      def forward(self, x):
322          _, _, img_h, img_w = x.size()
323          cfg._tmp_img_h = img_h
324          cfg._tmp_img_w = img_w
325          with timer.env('backbone'):
326              outs = self.backbone(x)
327          if cfg.fpn is not None:
328              with timer.env('fpn'):
329                  outs = [outs[i] for i in cfg.backbone.selected_layers]
330                  outs = self.fpn(outs)
331          proto_out = None
332          if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:
333              with timer.env('proto'):
334                  proto_x = x if self.proto_src is None else outs[self.proto_src]
<span onclick='openModal()' class='match'>335                  if self.num_grids > 0:
336                      grids = self.grid.repeat(proto_x.size(0), 1, 1, 1)
337                      proto_x = torch.cat([proto_x, grids], dim=1)
</span>338                  proto_out = self.proto_net(proto_x)
339                  proto_out = cfg.mask_proto_prototype_activation(proto_out)
340                  if cfg.mask_proto_prototypes_as_features:
341                      proto_downsampled = proto_out.clone()
342                      if cfg.mask_proto_prototypes_as_features_no_grad:
343                          proto_downsampled = proto_out.detach()
344                  proto_out = proto_out.permute(0, 2, 3, 1).contiguous()
345                  if cfg.mask_proto_bias:
346                      bias_shape = [x for x in proto_out.size()]
347                      bias_shape[-1] = 1
348                      proto_out = torch.cat([proto_out, torch.ones(*bias_shape)], -1)
349          with timer.env('pred_heads'):
350              pred_outs = { 'loc': [], 'conf': [], 'mask': [], 'priors': [] }
351              if cfg.use_mask_scoring:
352                  pred_outs['score'] = []
353              if cfg.use_instance_coeff:
354                  pred_outs['inst'] = []
355              for idx, pred_layer in zip(self.selected_layers, self.prediction_layers):
356                  pred_x = outs[idx]
357                  if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_prototypes_as_features:
358                      proto_downsampled = F.interpolate(proto_downsampled, size=outs[idx].size()[2:], mode='bilinear', align_corners=False)
359                      pred_x = torch.cat([pred_x, proto_downsampled], dim=1)
360                  if cfg.share_prediction_module and pred_layer is not self.prediction_layers[0]:
361                      pred_layer.parent = [self.prediction_layers[0]]
362                  p = pred_layer(pred_x)
363                  for k, v in p.items():
364                      pred_outs[k].append(v)
365          for k, v in pred_outs.items():
366              pred_outs[k] = torch.cat(v, -2)
367          if proto_out is not None:
368              pred_outs['proto'] = proto_out
369          if self.training:
370              if cfg.use_class_existence_loss:
371                  pred_outs['classes'] = self.class_existence_fc(outs[-1].mean(dim=(2, 3)))
372              if cfg.use_semantic_segmentation_loss:
373                  pred_outs['segm'] = self.semantic_seg_conv(outs[0])
374              return pred_outs
375          else:
376              if cfg.use_mask_scoring:
377                  pred_outs['score'] = torch.sigmoid(pred_outs['score'])
378              if cfg.use_focal_loss:
379                  if cfg.use_sigmoid_focal_loss:
380                      pred_outs['conf'] = torch.sigmoid(pred_outs['conf'])
381                      if cfg.use_mask_scoring:
382                          pred_outs['conf'] *= pred_outs['score']
383                  elif cfg.use_objectness_score:
384                      objectness = torch.sigmoid(pred_outs['conf'][:, :, 0])
385                      pred_outs['conf'][:, :, 1:] = objectness[:, :, None] * F.softmax(pred_outs['conf'][:, :, 1:], -1)
386                      pred_outs['conf'][:, :, 0 ] = 1 - objectness
387                  else:
388                      pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)
389              else:
390                  if cfg.use_objectness_score:
391                      objectness = torch.sigmoid(pred_outs['conf'][:, :, 0])
392                      pred_outs['conf'][:, :, 1:] = (objectness > 0.10)[..., None] \
393                          * F.softmax(pred_outs['conf'][:, :, 1:], dim=-1)
394                  else:
395                      pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)
396              return self.detect(pred_outs, self)
397  if __name__ == '__main__':
398      from utils.functions import init_console
399      init_console()
400      import sys
401      if len(sys.argv) > 1:
402          from data.config import set_cfg
403          set_cfg(sys.argv[1])
404      net = Yolact()
405      net.train()
406      net.init_weights(backbone_path='weights/' + cfg.backbone.path)
407      net = net.cuda()
408      torch.set_default_tensor_type('torch.cuda.FloatTensor')
409      x = torch.zeros((1, 3, cfg.max_size, cfg.max_size))
410      y = net(x)
411      for p in net.prediction_layers:
412          print(p.last_conv_size)
413      print()
414      for k, a in y.items():
415          print(k + ': ', a.size(), torch.sum(a))
416      exit()
417      net(x)
418      avg = MovingAverage()
419      try:
420          while True:
421              timer.reset()
422              with timer.env('everything else'):
423                  net(x)
424              avg.add(timer.total_time())
425              print('\033[2J') # Moves console cursor to 0,0
426              timer.print_stats()
427              print('Avg fps: %.2f\tAvg ms: %.2f         ' % (1/avg.get_avg(), avg.get_avg()*1000))
428      except KeyboardInterrupt:
429          pass
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-adam_test.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-yolact.py</div>
                </div>
                <div class="column column_space"><pre><code>50      tf_parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
51      updates = [
52          tf.IndexedSlices(
53              tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
</pre></code></div>
                <div class="column column_space"><pre><code>335                  if self.num_grids > 0:
336                      grids = self.grid.repeat(proto_x.size(0), 1, 1, 1)
337                      proto_x = torch.cat([proto_x, grids], dim=1)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    