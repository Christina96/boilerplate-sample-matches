
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Tokens: 20, <button onclick='openModal()' class='match'>CODE CLONE</button></h2>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-yolact.py</h3>
            <pre><code>1  import torch, torchvision
2  import torch.nn as nn
3  import torch.nn.functional as F
4  from torchvision.models.resnet import Bottleneck
5  import numpy as np
6  from itertools import product
7  from math import sqrt
8  from typing import List
9  from collections import defaultdict
10  from data.config import cfg, mask_type
11  from layers import Detect
12  from layers.interpolate import InterpolateModule
13  from backbone import construct_backbone
14  import torch.backends.cudnn as cudnn
15  from utils import timer
16  from utils.functions import MovingAverage, make_net
17  torch.cuda.current_device()
18  use_jit = torch.cuda.device_count() &lt;= 1
19  if not use_jit:
20      print(&#x27;Multiple GPUs detected! Turning off JIT.&#x27;)
21  ScriptModuleWrapper = torch.jit.ScriptModule if use_jit else nn.Module
22  script_method_wrapper = torch.jit.script_method if use_jit else lambda fn, _rcn=None: fn
23  class Concat(nn.Module):
24      def __init__(self, nets, extra_params):
25          super().__init__()
26          self.nets = nn.ModuleList(nets)
27          self.extra_params = extra_params
28      def forward(self, x):
29          return torch.cat([net(x) for net in self.nets], dim=1, **self.extra_params)
30  prior_cache = defaultdict(lambda: None)
31  class PredictionModule(nn.Module):
32      def __init__(self, in_channels, out_channels=1024, aspect_ratios=[[1]], scales=[1], parent=None, index=0):
33          super().__init__()
34          self.num_classes = cfg.num_classes
35          self.mask_dim    = cfg.mask_dim # Defined by Yolact
36          self.num_priors  = sum(len(x)*len(scales) for x in aspect_ratios)
37          self.parent      = [parent] # Don&#x27;t include this in the state dict
38          self.index       = index
39          self.num_heads   = cfg.num_heads # Defined by Yolact
40          if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:
41              self.mask_dim = self.mask_dim // self.num_heads
42          if cfg.mask_proto_prototypes_as_features:
43              in_channels += self.mask_dim
44          if parent is None:
45              if cfg.extra_head_net is None:
46                  out_channels = in_channels
47              else:
48                  self.upfeature, out_channels = make_net(in_channels, cfg.extra_head_net)
49              if cfg.use_prediction_module:
50                  self.block = Bottleneck(out_channels, out_channels // 4)
51                  self.conv = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True)
52                  self.bn = nn.BatchNorm2d(out_channels)
53              self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4,                **cfg.head_layer_params)
54              self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, **cfg.head_layer_params)
55              self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim,    **cfg.head_layer_params)
56              if cfg.use_mask_scoring:
57                  self.score_layer = nn.Conv2d(out_channels, self.num_priors, **cfg.head_layer_params)
58              if cfg.use_instance_coeff:
59                  self.inst_layer = nn.Conv2d(out_channels, self.num_priors * cfg.num_instance_coeffs, **cfg.head_layer_params)
60              def make_extra(num_layers):
61                  if num_layers == 0:
62                      return lambda x: x
63                  else:
64                      return nn.Sequential(*sum([[
65                          nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
66                          nn.ReLU(inplace=True)
67                      ] for _ in range(num_layers)], []))
68              self.bbox_extra, self.conf_extra, self.mask_extra = [make_extra(x) for x in cfg.extra_layers]
69              if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_coeff_gate:
70                  self.gate_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim, kernel_size=3, padding=1)
71          self.aspect_ratios = aspect_ratios
72          self.scales = scales
73          self.priors = None
74          self.last_conv_size = None
75          self.last_img_size = None
76      def forward(self, x):
77          src = self if self.parent[0] is None else self.parent[0]
78          conv_h = x.size(2)
79          conv_w = x.size(3)
80          if cfg.extra_head_net is not None:
81              x = src.upfeature(x)
82          if cfg.use_prediction_module:
83              a = src.block(x)
84              b = src.conv(x)
85              b = src.bn(b)
86              b = F.relu(b)
87              x = a + b
88          bbox_x = src.bbox_extra(x)
89          conf_x = src.conf_extra(x)
90          mask_x = src.mask_extra(x)
91          bbox = src.bbox_layer(bbox_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)
92          conf = src.conf_layer(conf_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)
93          if cfg.eval_mask_branch:
94              mask = src.mask_layer(mask_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)
95          else:
96              mask = torch.zeros(x.size(0), bbox.size(1), self.mask_dim, device=bbox.device)
97          if cfg.use_mask_scoring:
98              score = src.score_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 1)
99          if cfg.use_instance_coeff:
100              inst = src.inst_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, cfg.num_instance_coeffs)    
101          if cfg.use_yolo_regressors:
102              bbox[:, :, :2] = torch.sigmoid(bbox[:, :, :2]) - 0.5
<span onclick='openModal()' class='match'>103              bbox[:, :, 0] /= conv_w
104              bbox[:, :, 1] /= conv_h
105          if cfg.eval_mask_branch:
</span>106              if cfg.mask_type == mask_type.direct:
107                  mask = torch.sigmoid(mask)
108              elif cfg.mask_type == mask_type.lincomb:
109                  mask = cfg.mask_proto_coeff_activation(mask)
110                  if cfg.mask_proto_coeff_gate:
111                      gate = src.gate_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)
112                      mask = mask * torch.sigmoid(gate)
113          if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:
114              mask = F.pad(mask, (self.index * self.mask_dim, (self.num_heads - self.index - 1) * self.mask_dim), mode=&#x27;constant&#x27;, value=0)
115          priors = self.make_priors(conv_h, conv_w, x.device)
116          preds = { &#x27;loc&#x27;: bbox, &#x27;conf&#x27;: conf, &#x27;mask&#x27;: mask, &#x27;priors&#x27;: priors }
117          if cfg.use_mask_scoring:
118              preds[&#x27;score&#x27;] = score
119          if cfg.use_instance_coeff:
120              preds[&#x27;inst&#x27;] = inst
121          return preds
122      def make_priors(self, conv_h, conv_w, device):
123          global prior_cache
124          size = (conv_h, conv_w)
125          with timer.env(&#x27;makepriors&#x27;):
126              if self.last_img_size != (cfg._tmp_img_w, cfg._tmp_img_h):
127                  prior_data = []
128                  for j, i in product(range(conv_h), range(conv_w)):
129                      x = (i + 0.5) / conv_w
130                      y = (j + 0.5) / conv_h
131                      for ars in self.aspect_ratios:
132                          for scale in self.scales:
133                              for ar in ars:
134                                  if not cfg.backbone.preapply_sqrt:
135                                      ar = sqrt(ar)
136                                  if cfg.backbone.use_pixel_scales:
137                                      w = scale * ar / cfg.max_size
138                                      h = scale / ar / cfg.max_size
139                                  else:
140                                      w = scale * ar / conv_w
141                                      h = scale / ar / conv_h
142                                  if cfg.backbone.use_square_anchors:
143                                      h = w
144                                  prior_data += [x, y, w, h]
145                  self.priors = torch.Tensor(prior_data, device=device).view(-1, 4).detach()
146                  self.priors.requires_grad = False
147                  self.last_img_size = (cfg._tmp_img_w, cfg._tmp_img_h)
148                  self.last_conv_size = (conv_w, conv_h)
149                  prior_cache[size] = None
150              elif self.priors.device != device:
151                  if prior_cache[size] is None:
152                      prior_cache[size] = {}
153                  if device not in prior_cache[size]:
154                      prior_cache[size][device] = self.priors.to(device)
155                  self.priors = prior_cache[size][device]
156          return self.priors
157  class FPN(ScriptModuleWrapper):
158      __constants__ = [&#x27;interpolation_mode&#x27;, &#x27;num_downsample&#x27;, &#x27;use_conv_downsample&#x27;, &#x27;relu_pred_layers&#x27;,
159                       &#x27;lat_layers&#x27;, &#x27;pred_layers&#x27;, &#x27;downsample_layers&#x27;, &#x27;relu_downsample_layers&#x27;]
160      def __init__(self, in_channels):
161          super().__init__()
162          self.lat_layers  = nn.ModuleList([
163              nn.Conv2d(x, cfg.fpn.num_features, kernel_size=1)
164              for x in reversed(in_channels)
165          ])
166          padding = 1 if cfg.fpn.pad else 0
167          self.pred_layers = nn.ModuleList([
168              nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=padding)
169              for _ in in_channels
170          ])
171          if cfg.fpn.use_conv_downsample:
172              self.downsample_layers = nn.ModuleList([
173                  nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=1, stride=2)
174                  for _ in range(cfg.fpn.num_downsample)
175              ])
176          self.interpolation_mode     = cfg.fpn.interpolation_mode
177          self.num_downsample         = cfg.fpn.num_downsample
178          self.use_conv_downsample    = cfg.fpn.use_conv_downsample
179          self.relu_downsample_layers = cfg.fpn.relu_downsample_layers
180          self.relu_pred_layers       = cfg.fpn.relu_pred_layers
181      @script_method_wrapper
182      def forward(self, convouts:List[torch.Tensor]):
183          out = []
184          x = torch.zeros(1, device=convouts[0].device)
185          for i in range(len(convouts)):
186              out.append(x)
187          j = len(convouts)
188          for lat_layer in self.lat_layers:
189              j -= 1
190              if j &lt; len(convouts) - 1:
191                  _, _, h, w = convouts[j].size()
192                  x = F.interpolate(x, size=(h, w), mode=self.interpolation_mode, align_corners=False)
193              x = x + lat_layer(convouts[j])
194              out[j] = x
195          j = len(convouts)
196          for pred_layer in self.pred_layers:
197              j -= 1
198              out[j] = pred_layer(out[j])
199              if self.relu_pred_layers:
200                  F.relu(out[j], inplace=True)
201          cur_idx = len(out)
202          if self.use_conv_downsample:
203              for downsample_layer in self.downsample_layers:
204                  out.append(downsample_layer(out[-1]))
205          else:
206              for idx in range(self.num_downsample):
207                  out.append(nn.functional.max_pool2d(out[-1], 1, stride=2))
208          if self.relu_downsample_layers:
209              for idx in range(len(out) - cur_idx):
210                  out[idx] = F.relu(out[idx + cur_idx], inplace=False)
211          return out
212  class FastMaskIoUNet(ScriptModuleWrapper):
213      def __init__(self):
214          super().__init__()
215          input_channels = 1
216          last_layer = [(cfg.num_classes-1, 1, {})]
217          self.maskiou_net, _ = make_net(input_channels, cfg.maskiou_net + last_layer, include_last_relu=True)
218      def forward(self, x):
219          x = self.maskiou_net(x)
220          maskiou_p = F.max_pool2d(x, kernel_size=x.size()[2:]).squeeze(-1).squeeze(-1)
221          return maskiou_p
222  class Yolact(nn.Module):
223      def __init__(self):
224          super().__init__()
225          self.backbone = construct_backbone(cfg.backbone)
226          if cfg.freeze_bn:
227              self.freeze_bn()
228          if cfg.mask_type == mask_type.direct:
229              cfg.mask_dim = cfg.mask_size**2
230          elif cfg.mask_type == mask_type.lincomb:
231              if cfg.mask_proto_use_grid:
232                  self.grid = torch.Tensor(np.load(cfg.mask_proto_grid_file))
233                  self.num_grids = self.grid.size(0)
234              else:
235                  self.num_grids = 0
236              self.proto_src = cfg.mask_proto_src
237              if self.proto_src is None: in_channels = 3
238              elif cfg.fpn is not None: in_channels = cfg.fpn.num_features
239              else: in_channels = self.backbone.channels[self.proto_src]
240              in_channels += self.num_grids
241              self.proto_net, cfg.mask_dim = make_net(in_channels, cfg.mask_proto_net, include_last_relu=False)
242              if cfg.mask_proto_bias:
243                  cfg.mask_dim += 1
244          self.selected_layers = cfg.backbone.selected_layers
245          src_channels = self.backbone.channels
246          if cfg.use_maskiou:
247              self.maskiou_net = FastMaskIoUNet()
248          if cfg.fpn is not None:
249              self.fpn = FPN([src_channels[i] for i in self.selected_layers])
250              self.selected_layers = list(range(len(self.selected_layers) + cfg.fpn.num_downsample))
251              src_channels = [cfg.fpn.num_features] * len(self.selected_layers)
252          self.prediction_layers = nn.ModuleList()
253          cfg.num_heads = len(self.selected_layers)
254          for idx, layer_idx in enumerate(self.selected_layers):
255              parent = None
256              if cfg.share_prediction_module and idx &gt; 0:
257                  parent = self.prediction_layers[0]
258              pred = PredictionModule(src_channels[layer_idx], src_channels[layer_idx],
259                                      aspect_ratios = cfg.backbone.pred_aspect_ratios[idx],
260                                      scales        = cfg.backbone.pred_scales[idx],
261                                      parent        = parent,
262                                      index         = idx)
263              self.prediction_layers.append(pred)
264          if cfg.use_class_existence_loss:
265              self.class_existence_fc = nn.Linear(src_channels[-1], cfg.num_classes - 1)
266          if cfg.use_semantic_segmentation_loss:
267              self.semantic_seg_conv = nn.Conv2d(src_channels[0], cfg.num_classes-1, kernel_size=1)
268          self.detect = Detect(cfg.num_classes, bkg_label=0, top_k=cfg.nms_top_k,
269              conf_thresh=cfg.nms_conf_thresh, nms_thresh=cfg.nms_thresh)
270      def save_weights(self, path):
271          torch.save(self.state_dict(), path)
272      def load_weights(self, path):
273          state_dict = torch.load(path)
274          for key in list(state_dict.keys()):
275              if key.startswith(&#x27;backbone.layer&#x27;) and not key.startswith(&#x27;backbone.layers&#x27;):
276                  del state_dict[key]
277              if key.startswith(&#x27;fpn.downsample_layers.&#x27;):
278                  if cfg.fpn is not None and int(key.split(&#x27;.&#x27;)[2]) &gt;= cfg.fpn.num_downsample:
279                      del state_dict[key]
280          self.load_state_dict(state_dict)
281      def init_weights(self, backbone_path):
282          self.backbone.init_backbone(backbone_path)
283          conv_constants = getattr(nn.Conv2d(1, 1, 1), &#x27;__constants__&#x27;)
284          def all_in(x, y):
285              for _x in x:
286                  if _x not in y:
287                      return False
288              return True
289          for name, module in self.named_modules():
290              is_script_conv = False
291              if &#x27;Script&#x27; in type(module).__name__:
292                  if hasattr(module, &#x27;original_name&#x27;):
293                      is_script_conv = &#x27;Conv&#x27; in module.original_name
294                  else:
295                      is_script_conv = (
296                          all_in(module.__dict__[&#x27;_constants_set&#x27;], conv_constants)
297                          and all_in(conv_constants, module.__dict__[&#x27;_constants_set&#x27;]))
298              is_conv_layer = isinstance(module, nn.Conv2d) or is_script_conv
299              if is_conv_layer and module not in self.backbone.backbone_modules:
300                  nn.init.xavier_uniform_(module.weight.data)
301                  if module.bias is not None:
302                      if cfg.use_focal_loss and &#x27;conf_layer&#x27; in name:
303                          if not cfg.use_sigmoid_focal_loss:
304                              module.bias.data[0]  = np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)
305                              module.bias.data[1:] = -np.log(module.bias.size(0) - 1)
306                          else:
307                              module.bias.data[0]  = -np.log(cfg.focal_loss_init_pi / (1 - cfg.focal_loss_init_pi))
308                              module.bias.data[1:] = -np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)
309                      else:
310                          module.bias.data.zero_()
311      def train(self, mode=True):
312          super().train(mode)
313          if cfg.freeze_bn:
314              self.freeze_bn()
315      def freeze_bn(self, enable=False):
316          for module in self.modules():
317              if isinstance(module, nn.BatchNorm2d):
318                  module.train() if enable else module.eval()
319                  module.weight.requires_grad = enable
320                  module.bias.requires_grad = enable
321      def forward(self, x):
322          _, _, img_h, img_w = x.size()
323          cfg._tmp_img_h = img_h
324          cfg._tmp_img_w = img_w
325          with timer.env(&#x27;backbone&#x27;):
326              outs = self.backbone(x)
327          if cfg.fpn is not None:
328              with timer.env(&#x27;fpn&#x27;):
329                  outs = [outs[i] for i in cfg.backbone.selected_layers]
330                  outs = self.fpn(outs)
331          proto_out = None
332          if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:
333              with timer.env(&#x27;proto&#x27;):
334                  proto_x = x if self.proto_src is None else outs[self.proto_src]
335                  if self.num_grids &gt; 0:
336                      grids = self.grid.repeat(proto_x.size(0), 1, 1, 1)
337                      proto_x = torch.cat([proto_x, grids], dim=1)
338                  proto_out = self.proto_net(proto_x)
339                  proto_out = cfg.mask_proto_prototype_activation(proto_out)
340                  if cfg.mask_proto_prototypes_as_features:
341                      proto_downsampled = proto_out.clone()
342                      if cfg.mask_proto_prototypes_as_features_no_grad:
343                          proto_downsampled = proto_out.detach()
344                  proto_out = proto_out.permute(0, 2, 3, 1).contiguous()
345                  if cfg.mask_proto_bias:
346                      bias_shape = [x for x in proto_out.size()]
347                      bias_shape[-1] = 1
348                      proto_out = torch.cat([proto_out, torch.ones(*bias_shape)], -1)
349          with timer.env(&#x27;pred_heads&#x27;):
350              pred_outs = { &#x27;loc&#x27;: [], &#x27;conf&#x27;: [], &#x27;mask&#x27;: [], &#x27;priors&#x27;: [] }
351              if cfg.use_mask_scoring:
352                  pred_outs[&#x27;score&#x27;] = []
353              if cfg.use_instance_coeff:
354                  pred_outs[&#x27;inst&#x27;] = []
355              for idx, pred_layer in zip(self.selected_layers, self.prediction_layers):
356                  pred_x = outs[idx]
357                  if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_prototypes_as_features:
358                      proto_downsampled = F.interpolate(proto_downsampled, size=outs[idx].size()[2:], mode=&#x27;bilinear&#x27;, align_corners=False)
359                      pred_x = torch.cat([pred_x, proto_downsampled], dim=1)
360                  if cfg.share_prediction_module and pred_layer is not self.prediction_layers[0]:
361                      pred_layer.parent = [self.prediction_layers[0]]
362                  p = pred_layer(pred_x)
363                  for k, v in p.items():
364                      pred_outs[k].append(v)
365          for k, v in pred_outs.items():
366              pred_outs[k] = torch.cat(v, -2)
367          if proto_out is not None:
368              pred_outs[&#x27;proto&#x27;] = proto_out
369          if self.training:
370              if cfg.use_class_existence_loss:
371                  pred_outs[&#x27;classes&#x27;] = self.class_existence_fc(outs[-1].mean(dim=(2, 3)))
372              if cfg.use_semantic_segmentation_loss:
373                  pred_outs[&#x27;segm&#x27;] = self.semantic_seg_conv(outs[0])
374              return pred_outs
375          else:
376              if cfg.use_mask_scoring:
377                  pred_outs[&#x27;score&#x27;] = torch.sigmoid(pred_outs[&#x27;score&#x27;])
378              if cfg.use_focal_loss:
379                  if cfg.use_sigmoid_focal_loss:
380                      pred_outs[&#x27;conf&#x27;] = torch.sigmoid(pred_outs[&#x27;conf&#x27;])
381                      if cfg.use_mask_scoring:
382                          pred_outs[&#x27;conf&#x27;] *= pred_outs[&#x27;score&#x27;]
383                  elif cfg.use_objectness_score:
384                      objectness = torch.sigmoid(pred_outs[&#x27;conf&#x27;][:, :, 0])
385                      pred_outs[&#x27;conf&#x27;][:, :, 1:] = objectness[:, :, None] * F.softmax(pred_outs[&#x27;conf&#x27;][:, :, 1:], -1)
386                      pred_outs[&#x27;conf&#x27;][:, :, 0 ] = 1 - objectness
387                  else:
388                      pred_outs[&#x27;conf&#x27;] = F.softmax(pred_outs[&#x27;conf&#x27;], -1)
389              else:
390                  if cfg.use_objectness_score:
391                      objectness = torch.sigmoid(pred_outs[&#x27;conf&#x27;][:, :, 0])
392                      pred_outs[&#x27;conf&#x27;][:, :, 1:] = (objectness &gt; 0.10)[..., None] \
393                          * F.softmax(pred_outs[&#x27;conf&#x27;][:, :, 1:], dim=-1)
394                  else:
395                      pred_outs[&#x27;conf&#x27;] = F.softmax(pred_outs[&#x27;conf&#x27;], -1)
396              return self.detect(pred_outs, self)
397  if __name__ == &#x27;__main__&#x27;:
398      from utils.functions import init_console
399      init_console()
400      import sys
401      if len(sys.argv) &gt; 1:
402          from data.config import set_cfg
403          set_cfg(sys.argv[1])
404      net = Yolact()
405      net.train()
406      net.init_weights(backbone_path=&#x27;weights/&#x27; + cfg.backbone.path)
407      net = net.cuda()
408      torch.set_default_tensor_type(&#x27;torch.cuda.FloatTensor&#x27;)
409      x = torch.zeros((1, 3, cfg.max_size, cfg.max_size))
410      y = net(x)
411      for p in net.prediction_layers:
412          print(p.last_conv_size)
413      print()
414      for k, a in y.items():
415          print(k + &#x27;: &#x27;, a.size(), torch.sum(a))
416      exit()
417      net(x)
418      avg = MovingAverage()
419      try:
420          while True:
421              timer.reset()
422              with timer.env(&#x27;everything else&#x27;):
423                  net(x)
424              avg.add(timer.total_time())
425              print(&#x27;\033[2J&#x27;) # Moves console cursor to 0,0
426              timer.print_stats()
427              print(&#x27;Avg fps: %.2f\tAvg ms: %.2f         &#x27; % (1/avg.get_avg(), avg.get_avg()*1000))
428      except KeyboardInterrupt:
429          pass
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</h3>
            <pre><code>1  import torch
2  from utils import timer
3  from data import cfg
4  @torch.jit.script
5  def point_form(boxes):
6      return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin
7                       boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax
8  @torch.jit.script
9  def center_size(boxes):
10      return torch.cat(( (boxes[:, 2:] + boxes[:, :2])/2,     # cx, cy
11                          boxes[:, 2:] - boxes[:, :2]  ), 1)  # w, h
12  @torch.jit.script
13  def intersect(box_a, box_b):
14      n = box_a.size(0)
15      A = box_a.size(1)
16      B = box_b.size(1)
17      max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),
18                         box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))
19      min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),
20                         box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))
21      return torch.clamp(max_xy - min_xy, min=0).prod(3)  # inter
22  def jaccard(box_a, box_b, iscrowd:bool=False):
23      use_batch = True
24      if box_a.dim() == 2:
25          use_batch = False
26          box_a = box_a[None, ...]
27          box_b = box_b[None, ...]
28      inter = intersect(box_a, box_b)
29      area_a = ((box_a[:, :, 2]-box_a[:, :, 0]) *
30                (box_a[:, :, 3]-box_a[:, :, 1])).unsqueeze(2).expand_as(inter)  # [A,B]
31      area_b = ((box_b[:, :, 2]-box_b[:, :, 0]) *
32                (box_b[:, :, 3]-box_b[:, :, 1])).unsqueeze(1).expand_as(inter)  # [A,B]
33      union = area_a + area_b - inter
34      out = inter / area_a if iscrowd else inter / union
35      return out if use_batch else out.squeeze(0)
36  def elemwise_box_iou(box_a, box_b):
37      max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])
38      min_xy = torch.max(box_a[:, :2], box_b[:, :2])
39      inter = torch.clamp((max_xy - min_xy), min=0)
40      inter = inter[:, 0] * inter[:, 1]
41      area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])
42      area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])
43      union = area_a + area_b - inter
44      union = torch.clamp(union, min=0.1)
45      return torch.clamp(inter / union, max=1)
46  def mask_iou(masks_a, masks_b, iscrowd=False):
47      masks_a = masks_a.view(masks_a.size(0), -1)
48      masks_b = masks_b.view(masks_b.size(0), -1)
49      intersection = masks_a @ masks_b.t()
50      area_a = masks_a.sum(dim=1).unsqueeze(1)
51      area_b = masks_b.sum(dim=1).unsqueeze(0)
52      return intersection / (area_a + area_b - intersection) if not iscrowd else intersection / area_a
53  def elemwise_mask_iou(masks_a, masks_b):
54      masks_a = masks_a.view(-1, masks_a.size(-1))
55      masks_b = masks_b.view(-1, masks_b.size(-1))
56      intersection = (masks_a * masks_b).sum(dim=0)
57      area_a = masks_a.sum(dim=0)
58      area_b = masks_b.sum(dim=0)
59      return torch.clamp(intersection / torch.clamp(area_a + area_b - intersection, min=0.1), max=1)
60  def change(gt, priors):
61      num_priors = priors.size(0)
62      num_gt     = gt.size(0)
63      gt_w = (gt[:, 2] - gt[:, 0])[:, None].expand(num_gt, num_priors)
64      gt_h = (gt[:, 3] - gt[:, 1])[:, None].expand(num_gt, num_priors)
65      gt_mat =     gt[:, None, :].expand(num_gt, num_priors, 4)
66      pr_mat = priors[None, :, :].expand(num_gt, num_priors, 4)
67      diff = gt_mat - pr_mat
<span onclick='openModal()' class='match'>68      diff[:, :, 0] /= gt_w
69      diff[:, :, 2] /= gt_w
70      diff[:, :, 1] /= gt_h
</span>71      diff[:, :, 3] /= gt_h
72      return -torch.sqrt( (diff ** 2).sum(dim=2) )
73  def match(pos_thresh, neg_thresh, truths, priors, labels, crowd_boxes, loc_t, conf_t, idx_t, idx, loc_data):
74      decoded_priors = decode(loc_data, priors, cfg.use_yolo_regressors) if cfg.use_prediction_matching else point_form(priors)
75      overlaps = jaccard(truths, decoded_priors) if not cfg.use_change_matching else change(truths, decoded_priors)
76      best_truth_overlap, best_truth_idx = overlaps.max(0)
77      for _ in range(overlaps.size(0)):
78          best_prior_overlap, best_prior_idx = overlaps.max(1)
79          j = best_prior_overlap.max(0)[1]
80          i = best_prior_idx[j]
81          overlaps[:, i] = -1
82          overlaps[j, :] = -1
83          best_truth_overlap[i] = 2
84          best_truth_idx[i] = j
85      matches = truths[best_truth_idx]            # Shape: [num_priors,4]
86      conf = labels[best_truth_idx] + 1           # Shape: [num_priors]
87      conf[best_truth_overlap &lt; pos_thresh] = -1  # label as neutral
88      conf[best_truth_overlap &lt; neg_thresh] =  0  # label as background
89      if crowd_boxes is not None and cfg.crowd_iou_threshold &lt; 1:
90          crowd_overlaps = jaccard(decoded_priors, crowd_boxes, iscrowd=True)
91          best_crowd_overlap, best_crowd_idx = crowd_overlaps.max(1)
92          conf[(conf &lt;= 0) &amp; (best_crowd_overlap &gt; cfg.crowd_iou_threshold)] = -1
93      loc = encode(matches, priors, cfg.use_yolo_regressors)
94      loc_t[idx]  = loc    # [num_priors,4] encoded offsets to learn
95      conf_t[idx] = conf   # [num_priors] top class label for each prior
96      idx_t[idx]  = best_truth_idx # [num_priors] indices for lookup
97  @torch.jit.script
98  def encode(matched, priors, use_yolo_regressors:bool=False):
99      if use_yolo_regressors:
100          boxes = center_size(matched)
101          loc = torch.cat((
102              boxes[:, :2] - priors[:, :2],
103              torch.log(boxes[:, 2:] / priors[:, 2:])
104          ), 1)
105      else:
106          variances = [0.1, 0.2]
107          g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]
108          g_cxcy /= (variances[0] * priors[:, 2:])
109          g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]
110          g_wh = torch.log(g_wh) / variances[1]
111          loc = torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]
112      return loc
113  @torch.jit.script
114  def decode(loc, priors, use_yolo_regressors:bool=False):
115      if use_yolo_regressors:
116          boxes = torch.cat((
117              loc[:, :2] + priors[:, :2],
118              priors[:, 2:] * torch.exp(loc[:, 2:])
119          ), 1)
120          boxes = point_form(boxes)
121      else:
122          variances = [0.1, 0.2]
123          boxes = torch.cat((
124              priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],
125              priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)
126          boxes[:, :2] -= boxes[:, 2:] / 2
127          boxes[:, 2:] += boxes[:, :2]
128      return boxes
129  def log_sum_exp(x):
130      x_max = x.data.max()
131      return torch.log(torch.sum(torch.exp(x-x_max), 1)) + x_max
132  @torch.jit.script
133  def sanitize_coordinates(_x1, _x2, img_size:int, padding:int=0, cast:bool=True):
134      _x1 = _x1 * img_size
135      _x2 = _x2 * img_size
136      if cast:
137          _x1 = _x1.long()
138          _x2 = _x2.long()
139      x1 = torch.min(_x1, _x2)
140      x2 = torch.max(_x1, _x2)
141      x1 = torch.clamp(x1-padding, min=0)
142      x2 = torch.clamp(x2+padding, max=img_size)
143      return x1, x2
144  @torch.jit.script
145  def crop(masks, boxes, padding:int=1):
146      h, w, n = masks.size()
147      x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding, cast=False)
148      y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding, cast=False)
149      rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)
150      cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)
151      masks_left  = rows &gt;= x1.view(1, 1, -1)
152      masks_right = rows &lt;  x2.view(1, 1, -1)
153      masks_up    = cols &gt;= y1.view(1, 1, -1)
154      masks_down  = cols &lt;  y2.view(1, 1, -1)
155      crop_mask = masks_left * masks_right * masks_up * masks_down
156      return masks * crop_mask.float()
157  def index2d(src, idx):
158      offs = torch.arange(idx.size(0), device=idx.device)[:, None].expand_as(idx)
159      idx  = idx + offs * idx.size(1)
160      return src.view(-1)[idx.view(-1)].view(idx.size())
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-yolact.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</div>
                </div>
                <div class="column column_space"><pre><code>103              bbox[:, :, 0] /= conv_w
104              bbox[:, :, 1] /= conv_h
105          if cfg.eval_mask_branch:
</pre></code></div>
                <div class="column column_space"><pre><code>68      diff[:, :, 0] /= gt_w
69      diff[:, :, 2] /= gt_w
70      diff[:, :, 1] /= gt_h
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    