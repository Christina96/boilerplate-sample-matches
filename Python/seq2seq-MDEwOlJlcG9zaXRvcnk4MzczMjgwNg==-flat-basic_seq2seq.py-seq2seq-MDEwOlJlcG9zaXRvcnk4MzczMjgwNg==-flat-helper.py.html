
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 4.072883172561629%, Tokens: 9</h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-basic_seq2seq.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  from pydoc import locate
6  import tensorflow as tf
7  from seq2seq.contrib.seq2seq import helper as tf_decode_helper
8  from seq2seq.models.seq2seq_model import Seq2SeqModel
9  from seq2seq.graph_utils import templatemethod
10  from seq2seq.models import bridges
11  class BasicSeq2Seq(Seq2SeqModel):
12    def __init__(self, params, mode, name="basic_seq2seq"):
13      super(BasicSeq2Seq, self).__init__(params, mode, name)
14      self.encoder_class = locate(self.params["encoder.class"])
15      self.decoder_class = locate(self.params["decoder.class"])
16    @staticmethod
17    def default_params():
18      params = Seq2SeqModel.default_params().copy()
19      params.update({
20          "bridge.class": "seq2seq.models.bridges.InitialStateBridge",
21          "bridge.params": {},
22          "encoder.class": "seq2seq.encoders.UnidirectionalRNNEncoder",
23          "encoder.params": {},  # Arbitrary parameters for the encoder
24          "decoder.class": "seq2seq.decoders.BasicDecoder",
25          "decoder.params": {}  # Arbitrary parameters for the decoder
26      })
27      return params
28    def _create_bridge(self, encoder_outputs, decoder_state_size):
29      bridge_class = locate(self.params["bridge.class"]) or \
30        getattr(bridges, self.params["bridge.class"])
31      return bridge_class(
32          encoder_outputs=encoder_outputs,
33          decoder_state_size=decoder_state_size,
34          params=self.params["bridge.params"],
35          mode=self.mode)
36    def _create_decoder(self, _encoder_output, _features, _labels):
37      return self.decoder_class(
38          params=self.params["decoder.params"],
39          mode=self.mode,
40          vocab_size=self.target_vocab_info.total_size)
41    def _decode_train(self, decoder, bridge, _encoder_output, _features, labels):
42      target_embedded = tf.nn.embedding_lookup(self.target_embedding,
<span onclick='openModal()' class='match'>43                                               labels["target_ids"])
44      helper_train = tf_decode_helper.TrainingHelper(
45          inputs=target_embedded[:, :-1],
46          sequence_length=labels["target_len"] - 1)
47      decoder_initial_state = bridge()
</span>48      return decoder(decoder_initial_state, helper_train)
49    def _decode_infer(self, decoder, bridge, _encoder_output, features, labels):
50      batch_size = self.batch_size(features, labels)
51      if self.use_beam_search:
52        batch_size = self.params["inference.beam_search.beam_width"]
53      target_start_id = self.target_vocab_info.special_vocab.SEQUENCE_START
54      helper_infer = tf_decode_helper.GreedyEmbeddingHelper(
55          embedding=self.target_embedding,
56          start_tokens=tf.fill([batch_size], target_start_id),
57          end_token=self.target_vocab_info.special_vocab.SEQUENCE_END)
58      decoder_initial_state = bridge()
59      return decoder(decoder_initial_state, helper_infer)
60    @templatemethod("encode")
61    def encode(self, features, labels):
62      source_embedded = tf.nn.embedding_lookup(self.source_embedding,
63                                               features["source_ids"])
64      encoder_fn = self.encoder_class(self.params["encoder.params"], self.mode)
65      return encoder_fn(source_embedded, features["source_len"])
66    @templatemethod("decode")
67    def decode(self, encoder_output, features, labels):
68      decoder = self._create_decoder(encoder_output, features, labels)
69      if self.use_beam_search:
70        decoder = self._get_beam_search_decoder(decoder)
71      bridge = self._create_bridge(
72          encoder_outputs=encoder_output,
73          decoder_state_size=decoder.cell.state_size)
74      if self.mode == tf.contrib.learn.ModeKeys.INFER:
75        return self._decode_infer(decoder, bridge, encoder_output, features,
76                                  labels)
77      else:
78        return self._decode_train(decoder, bridge, encoder_output, features,
79                                  labels)
</code></pre>
        </div>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-helper.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  import abc
5  import six
6  from tensorflow.contrib.distributions.python.ops import bernoulli
7  from tensorflow.contrib.distributions.python.ops import categorical
8  from tensorflow.python.framework import dtypes
9  from tensorflow.python.framework import ops
10  from tensorflow.python.layers import base as layers_base
11  from tensorflow.python.ops import array_ops
12  from tensorflow.python.ops import control_flow_ops
13  from tensorflow.python.ops import embedding_ops
14  from tensorflow.python.ops import math_ops
15  from tensorflow.python.ops import random_ops
16  from tensorflow.python.ops import tensor_array_ops
17  from tensorflow.python.util import nest
18  from seq2seq.contrib.seq2seq import decoder
19  __all__ = [
20      "Helper",
21      "TrainingHelper",
22      "GreedyEmbeddingHelper",
23      "CustomHelper",
24      "ScheduledEmbeddingTrainingHelper",
25      "ScheduledOutputTrainingHelper",
26  ]
27  _transpose_batch_time = decoder._transpose_batch_time  # pylint: disable=protected-access
28  def _unstack_ta(inp):
29    return tensor_array_ops.TensorArray(
30        dtype=inp.dtype, size=array_ops.shape(inp)[0],
31        element_shape=inp.get_shape()[1:]).unstack(inp)
32  @six.add_metaclass(abc.ABCMeta)
33  class Helper(object):
34    @abc.abstractproperty
35    def batch_size(self):
36      raise NotImplementedError("batch_size has not been implemented")
37    @abc.abstractmethod
38    def initialize(self, name=None):
39      pass
40    @abc.abstractmethod
41    def sample(self, time, outputs, state, name=None):
42      pass
43    @abc.abstractmethod
44    def next_inputs(self, time, outputs, state, sample_ids, name=None):
45      pass
46  class CustomHelper(Helper):
47    def __init__(self, initialize_fn, sample_fn, next_inputs_fn):
48      self._initialize_fn = initialize_fn
49      self._sample_fn = sample_fn
50      self._next_inputs_fn = next_inputs_fn
51      self._batch_size = None
52    @property
53    def batch_size(self):
54      if self._batch_size is None:
55        raise ValueError("batch_size accessed before initialize was called")
56      return self._batch_size
57    def initialize(self, name=None):
58      with ops.name_scope(name, "%sInitialize" % type(self).__name__):
59        (finished, next_inputs) = self._initialize_fn()
60        if self._batch_size is None:
61          self._batch_size = array_ops.size(finished)
62      return (finished, next_inputs)
63    def sample(self, time, outputs, state, name=None):
64      with ops.name_scope(
65          name, "%sSample" % type(self).__name__, (time, outputs, state)):
66        return self._sample_fn(time=time, outputs=outputs, state=state)
67    def next_inputs(self, time, outputs, state, sample_ids, name=None):
68      with ops.name_scope(
69          name, "%sNextInputs" % type(self).__name__, (time, outputs, state)):
70        return self._next_inputs_fn(
71            time=time, outputs=outputs, state=state, sample_ids=sample_ids)
72  class TrainingHelper(Helper):
73    def __init__(self, inputs, sequence_length, time_major=False, name=None):
74      with ops.name_scope(name, "TrainingHelper", [inputs, sequence_length]):
75        inputs = ops.convert_to_tensor(inputs, name="inputs")
76        if not time_major:
77          inputs = nest.map_structure(_transpose_batch_time, inputs)
78        self._input_tas = nest.map_structure(_unstack_ta, inputs)
79        self._sequence_length = ops.convert_to_tensor(
80            sequence_length, name="sequence_length")
81        if self._sequence_length.get_shape().ndims != 1:
82          raise ValueError(
83              "Expected sequence_length to be a vector, but received shape: %s" %
84              self._sequence_length.get_shape())
85        self._zero_inputs = nest.map_structure(
86            lambda inp: array_ops.zeros_like(inp[0, :]), inputs)
87        self._batch_size = array_ops.size(sequence_length)
88    @property
89    def batch_size(self):
90      return self._batch_size
91    def initialize(self, name=None):
92      with ops.name_scope(name, "TrainingHelperInitialize"):
93        finished = math_ops.equal(0, self._sequence_length)
94        all_finished = math_ops.reduce_all(finished)
95        next_inputs = control_flow_ops.cond(
96            all_finished, lambda: self._zero_inputs,
97            lambda: nest.map_structure(lambda inp: inp.read(0), self._input_tas))
98        return (finished, next_inputs)
99    def sample(self, time, outputs, name=None, **unused_kwargs):
100      with ops.name_scope(name, "TrainingHelperSample", [time, outputs]):
101        sample_ids = math_ops.cast(
102            math_ops.argmax(outputs, axis=-1), dtypes.int32)
103        return sample_ids
104    def next_inputs(self, time, outputs, state, name=None, **unused_kwargs):
105      with ops.name_scope(name, "TrainingHelperNextInputs",
106                          [time, outputs, state]):
107        next_time = time + 1
108        finished = (next_time >= self._sequence_length)
109        all_finished = math_ops.reduce_all(finished)
110        def read_from_ta(inp):
111          return inp.read(next_time)
112        next_inputs = control_flow_ops.cond(
113            all_finished, lambda: self._zero_inputs,
114            lambda: nest.map_structure(read_from_ta, self._input_tas))
115        return (finished, next_inputs, state)
116  class ScheduledEmbeddingTrainingHelper(TrainingHelper):
117    def __init__(self, inputs, sequence_length, embedding, sampling_probability,
118                 time_major=False, seed=None, scheduling_seed=None, name=None):
119      with ops.name_scope(name, "ScheduledEmbeddingSamplingWrapper",
120                          [embedding, sampling_probability]):
121        if callable(embedding):
122          self._embedding_fn = embedding
123        else:
124          self._embedding_fn = (
125              lambda ids: embedding_ops.embedding_lookup(embedding, ids))
126        self._sampling_probability = ops.convert_to_tensor(
127            sampling_probability, name="sampling_probability")
128        if self._sampling_probability.get_shape().ndims not in (0, 1):
129          raise ValueError(
130              "sampling_probability must be either a scalar or a vector. "
131              "saw shape: %s" % (self._sampling_probability.get_shape()))
132        self._seed = seed
133        self._scheduling_seed = scheduling_seed
134        super(ScheduledEmbeddingTrainingHelper, self).__init__(
135            inputs=inputs,
136            sequence_length=sequence_length,
137            time_major=time_major,
138            name=name)
139    def initialize(self, name=None):
140      return super(ScheduledEmbeddingTrainingHelper, self).initialize(name=name)
141    def sample(self, time, outputs, state, name=None):
142      with ops.name_scope(name, "ScheduledEmbeddingTrainingHelperSample",
143                          [time, outputs, state]):
144        select_sample_noise = random_ops.random_uniform(
145            [self.batch_size], seed=self._scheduling_seed)
146        select_sample = (self._sampling_probability > select_sample_noise)
147        sample_id_sampler = categorical.Categorical(logits=outputs)
148        return array_ops.where(
149            select_sample,
150            sample_id_sampler.sample(seed=self._seed),
151            array_ops.tile([-1], [self.batch_size]))
152    def next_inputs(self, time, outputs, state, sample_ids, name=None):
153      with ops.name_scope(name, "ScheduledEmbeddingTrainingHelperSample",
154                          [time, outputs, state, sample_ids]):
155        (finished, base_next_inputs, state) = (
156            super(ScheduledEmbeddingTrainingHelper, self).next_inputs(
157                time=time,
158                outputs=outputs,
159                state=state,
160                sample_ids=sample_ids,
161                name=name))
162        def maybe_sample():
163          where_sampling = math_ops.cast(
164              array_ops.where(sample_ids > -1), dtypes.int32)
165          where_not_sampling = math_ops.cast(
166              array_ops.where(sample_ids <= -1), dtypes.int32)
167          where_sampling_flat = array_ops.reshape(where_sampling, [-1])
168          where_not_sampling_flat = array_ops.reshape(where_not_sampling, [-1])
169          sample_ids_sampling = array_ops.gather(sample_ids, where_sampling_flat)
170          inputs_not_sampling = array_ops.gather(
171              base_next_inputs, where_not_sampling_flat)
172          sampled_next_inputs = self._embedding_fn(sample_ids_sampling)
173          base_shape = array_ops.shape(base_next_inputs)
174          return (array_ops.scatter_nd(indices=where_sampling,
175                                       updates=sampled_next_inputs,
176                                       shape=base_shape)
177                  + array_ops.scatter_nd(indices=where_not_sampling,
178                                         updates=inputs_not_sampling,
179                                         shape=base_shape))
180        all_finished = math_ops.reduce_all(finished)
181        next_inputs = control_flow_ops.cond(
182            all_finished, lambda: base_next_inputs, maybe_sample)
183        return (finished, next_inputs, state)
184  class ScheduledOutputTrainingHelper(TrainingHelper):
185    def __init__(self, inputs, sequence_length, sampling_probability,
186                 time_major=False, seed=None, next_input_layer=None,
187                 auxiliary_inputs=None, name=None):
188      with ops.name_scope(name, "ScheduledOutputTrainingHelper",
189                          [inputs, auxiliary_inputs, sampling_probability]):
190        self._sampling_probability = ops.convert_to_tensor(
191            sampling_probability, name="sampling_probability")
192        if self._sampling_probability.get_shape().ndims not in (0, 1):
193          raise ValueError(
194              "sampling_probability must be either a scalar or a vector. "
195              "saw shape: %s" % (self._sampling_probability.get_shape()))
196        if auxiliary_inputs is None:
197          maybe_concatenated_inputs = inputs
198        else:
199          inputs = ops.convert_to_tensor(inputs, name="inputs")
200          auxiliary_inputs = ops.convert_to_tensor(
201              auxiliary_inputs, name="auxiliary_inputs")
202          maybe_concatenated_inputs = nest.map_structure(
203              lambda x, y: array_ops.concat((x, y), -1),
204              inputs, auxiliary_inputs)
205          if not time_major:
206            auxiliary_inputs = nest.map_structure(
207                _transpose_batch_time, auxiliary_inputs)
208        self._auxiliary_input_tas = (
209            nest.map_structure(_unstack_ta, auxiliary_inputs)
210            if auxiliary_inputs is not None else None)
211        self._seed = seed
212        if (next_input_layer is not None and not isinstance(next_input_layer,
213                                                            layers_base._Layer)):  # pylint: disable=protected-access
214          raise TypeError("next_input_layer must be a Layer, received: %s" %
215                          type(next_input_layer))
216        self._next_input_layer = next_input_layer
217        super(ScheduledOutputTrainingHelper, self).__init__(
218            inputs=maybe_concatenated_inputs,
219            sequence_length=sequence_length,
220            time_major=time_major,
221            name=name)
222    def initialize(self, name=None):
223      return super(ScheduledOutputTrainingHelper, self).initialize(name=name)
224    def sample(self, time, outputs, state, name=None):
225      with ops.name_scope(name, "ScheduledOutputTrainingHelperSample",
226                          [time, outputs, state]):
227        sampler = bernoulli.Bernoulli(probs=self._sampling_probability)
228        return math_ops.cast(
229            sampler.sample(sample_shape=self.batch_size, seed=self._seed),
230            dtypes.bool)
231    def next_inputs(self, time, outputs, state, sample_ids, name=None):
232      with ops.name_scope(name, "ScheduledOutputTrainingHelperNextInputs",
233                          [time, outputs, state, sample_ids]):
234        (finished, base_next_inputs, state) = (
235            super(ScheduledOutputTrainingHelper, self).next_inputs(
236                time=time,
237                outputs=outputs,
238                state=state,
239                sample_ids=sample_ids,
240                name=name))
241        def maybe_sample():
242          def maybe_concatenate_auxiliary_inputs(outputs_, indices=None):
243            if self._auxiliary_input_tas is None:
244              return outputs_
245            next_time = time + 1
246            auxiliary_inputs = nest.map_structure(
247                lambda ta: ta.read(next_time), self._auxiliary_input_tas)
248            if indices is not None:
249              auxiliary_inputs = array_ops.gather_nd(auxiliary_inputs, indices)
250            return nest.map_structure(
251                lambda x, y: array_ops.concat((x, y), -1),
252                outputs_, auxiliary_inputs)
253          if self._next_input_layer is None:
254            return array_ops.where(
255                sample_ids, maybe_concatenate_auxiliary_inputs(outputs),
256                base_next_inputs)
257          where_sampling = math_ops.cast(
258              array_ops.where(sample_ids), dtypes.int32)
259          where_not_sampling = math_ops.cast(
260              array_ops.where(math_ops.logical_not(sample_ids)), dtypes.int32)
261          outputs_sampling = array_ops.gather_nd(outputs, where_sampling)
262          inputs_not_sampling = array_ops.gather_nd(base_next_inputs,
263                                                    where_not_sampling)
264          sampled_next_inputs = maybe_concatenate_auxiliary_inputs(
265              self._next_input_layer(outputs_sampling), where_sampling)
266          base_shape = array_ops.shape(base_next_inputs)
267          return (array_ops.scatter_nd(indices=where_sampling,
268                                       updates=sampled_next_inputs,
269                                       shape=base_shape)
270                  + array_ops.scatter_nd(indices=where_not_sampling,
271                                         updates=inputs_not_sampling,
272                                         shape=base_shape))
273        all_finished = math_ops.reduce_all(finished)
274        next_inputs = control_flow_ops.cond(
275            all_finished, lambda: base_next_inputs, maybe_sample)
276        return (finished, next_inputs, state)
277  class GreedyEmbeddingHelper(Helper):
278    def __init__(self, embedding, start_tokens, end_token):
279      if callable(embedding):
280        self._embedding_fn = embedding
281      else:
282        self._embedding_fn = (
283            lambda ids: embedding_ops.embedding_lookup(embedding, ids))
<span onclick='openModal()' class='match'>284      self._start_tokens = ops.convert_to_tensor(
285          start_tokens, dtype=dtypes.int32, name="start_tokens")
286      self._end_token = ops.convert_to_tensor(
</span>287          end_token, dtype=dtypes.int32, name="end_token")
288      if self._start_tokens.get_shape().ndims != 1:
289        raise ValueError("start_tokens must be a vector")
290      self._batch_size = array_ops.size(start_tokens)
291      if self._end_token.get_shape().ndims != 0:
292        raise ValueError("end_token must be a scalar")
293      self._start_inputs = self._embedding_fn(self._start_tokens)
294    @property
295    def batch_size(self):
296      return self._batch_size
297    def initialize(self, name=None):
298      finished = array_ops.tile([False], [self._batch_size])
299      return (finished, self._start_inputs)
300    def sample(self, time, outputs, state, name=None):
301      del time, state  # unused by sample_fn
302      if not isinstance(outputs, ops.Tensor):
303        raise TypeError("Expected outputs to be a single Tensor, got: %s" %
304                        type(outputs))
305      sample_ids = math_ops.cast(
306          math_ops.argmax(outputs, axis=-1), dtypes.int32)
307      return sample_ids
308    def next_inputs(self, time, outputs, state, sample_ids, name=None):
309      del time, outputs  # unused by next_inputs_fn
310      finished = math_ops.equal(sample_ids, self._end_token)
311      all_finished = math_ops.reduce_all(finished)
312      next_inputs = control_flow_ops.cond(
313          all_finished,
314          lambda: self._start_inputs,
315          lambda: self._embedding_fn(sample_ids))
316      return (finished, next_inputs, state)
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-basic_seq2seq.py</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-helper.py</div>
                <div class="column column_space"><pre><code>43                                               labels["target_ids"])
44      helper_train = tf_decode_helper.TrainingHelper(
45          inputs=target_embedded[:, :-1],
46          sequence_length=labels["target_len"] - 1)
47      decoder_initial_state = bridge()
</pre></code></div>
                <div class="column column_space"><pre><code>284      self._start_tokens = ops.convert_to_tensor(
285          start_tokens, dtype=dtypes.int32, name="start_tokens")
286      self._end_token = ops.convert_to_tensor(
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    