<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for master.py &amp; virt_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for master.py &amp; virt_1.py
      </h3>
<h1 align="center">
        2.0%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>master.py (3.2759109%)<th>virt_1.py (1.4954234%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(40-73)<td><a href="#" name="0">(122-152)</a><td align="center"><font color="#ff0000">29</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1555-1563)<td><a href="#" name="1">(2948-2957)</a><td align="center"><font color="#b80000">21</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(1068-1078)<td><a href="#" name="2">(6459-6469)</a><td align="center"><font color="#950000">17</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(658-665)<td><a href="#" name="3">(216-231)</a><td align="center"><font color="#720000">13</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(2131-2133)<td><a href="#" name="4">(7509-7514)</a><td align="center"><font color="#690000">12</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(328-333)<td><a href="#" name="5">(249-253)</a><td align="center"><font color="#690000">12</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(221-225)<td><a href="#" name="6">(1480-1487)</a><td align="center"><font color="#690000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>master.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 import collections
2 import copy
3 import ctypes
4 import functools
5 import logging
6 import multiprocessing
7 import os
8 import re
9 import signal
10 import stat
11 import sys
12 import threading
13 import time
14 import salt.acl
15 import salt.auth
16 import salt.channel.server
17 import salt.client
18 import salt.client.ssh.client
19 import salt.crypt
20 import salt.daemons.masterapi
21 import salt.defaults.exitcodes
22 import salt.engines
23 import salt.exceptions
24 import salt.ext.tornado.gen
25 import salt.key
26 import salt.minion
27 import salt.payload
28 import salt.pillar
29 import salt.runner
30 import salt.serializers.msgpack
31 import salt.state
32 <a name="0"></a>import salt.utils.args
33 import salt.utils.atomicfile
34 import salt.utils.crypt
35 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import salt.utils.event
36 import salt.utils.files
37 import salt.utils.gitfs
38 import salt.utils.gzip_util
39 import salt.utils.jid
40 import salt.utils.job
41 import salt.utils.master
42 import salt.utils.minions
43 import salt.utils.platform
44 import salt.utils.process
45 import salt.utils.schedule
46 import salt.utils.ssdp
47 import salt.utils.stringutils
48 import salt.utils.user
49 import salt.utils.verify
50 import salt.utils.zeromq
51 import salt.wheel
52 from salt.config import DEFAULT_INTERVAL
53 from salt.defaults import DEFAULT_TARGET_DELIM
54 from salt.ext.tornado.stack_context import StackContext
55 from salt.transport import TRANSPORTS
56 from salt.utils.channel import iter_transport_opts
57 from salt.utils.ctx import RequestContext
58 from salt.utils.debug import (
59     enable_sigusr1_handler,
60     enable_sigusr2_handler,
61     inspect_stack,
62 )
63 from salt.utils.event import tagify
64 from salt.utils.odict import OrderedDict
65 from salt.utils.zeromq import ZMQ_VERSION_INFO, zmq
66 try:
67     import</b></font> resource
68     HAS_RESOURCE = True
69 except ImportError:
70     HAS_RESOURCE = False
71 log = logging.getLogger(__name__)
72 class SMaster:
73     secrets = (
74         {}
75     )  # mapping of key -&gt; {'secret': multiprocessing type, 'reload': FUNCTION}
76     def __init__(self, opts):
77         self.opts = opts
78         self.master_key = salt.crypt.MasterKeys(self.opts)
79         self.key = self.__prep_key()
80     def __setstate__(self, state):
81         super().__setstate__(state)
82         self.master_key = state["master_key"]
83         self.key = state["key"]
84         SMaster.secrets = state["secrets"]
85     def __getstate__(self):
86         state = super().__getstate__()
87         state.update(
88             {
89                 "key": self.key,
90                 "master_key": self.master_key,
91                 "secrets": SMaster.secrets,
92             }
93         )
94         return state
95     def __prep_key(self):
96         return salt.daemons.masterapi.access_keys(self.opts)
97 class Maintenance(salt.utils.process.SignalHandlingProcess):
98     def __init__(self, opts, **kwargs):
99         super().__init__(**kwargs)
100         self.opts = opts
101         self.loop_interval = int(self.opts["loop_interval"])
102         self.rotate = int(time.time())
103     def _post_fork_init(self):
104         ropts = dict(self.opts)
105         ropts["quiet"] = True
106         runner_client = salt.runner.RunnerClient(ropts)
107         self.returners = salt.loader.returners(self.opts, {})
108         self.schedule = salt.utils.schedule.Schedule(
109             self.opts, runner_client.functions_dict(), returners=self.returners
110         )
111         self.ckminions = salt.utils.minions.CkMinions(self.opts)
112         self.event = salt.utils.event.get_master_event(
113             self.opts, self.opts["sock_dir"], listen=False
114         )
115         self.git_pillar = salt.daemons.masterapi.init_git_pillar(self.opts)
116         if self.opts["maintenance_niceness"] and not salt.utils.platform.is_windows():
117             log.info(
118                 "setting Maintenance niceness to %d", self.opts["maintenance_niceness"]
119             )
120             os.nice(self.opts["maintenance_niceness"])
121         self.presence_events = False
122         if self.opts.get("presence_events", False):
123             tcp_only = True
124             for transport, _ in iter_transport_opts(self.opts):
125                 if transport != "tcp":
126                     tcp_only = False
127             if not tcp_only:
128                 self.presence_events = True
129     def run(self):
130         self._post_fork_init()
131         last = int(time.time())
132         last_git_pillar_update = 0
133         git_pillar_update_interval = self.opts.get("git_pillar_update_interval", 0)
134         old_present = set()
135         while True:
136             now = int(time.time())
137             if (now - last) &gt;= self.loop_interval:
138                 salt.daemons.masterapi.clean_old_jobs(self.opts)
139                 salt.daemons.masterapi.clean_expired_tokens(self.opts)
140                 salt.daemons.masterapi.clean_pub_auth(self.opts)
141 <a name="6"></a>            if (now - last_git_pillar_update) &gt;= git_pillar_update_interval:
142                 last_git_pillar_update = now
143                 self.handle_git_pillar()
144             self<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.handle_schedule()
145             self.handle_key_cache()
146             self.handle_presence(old_present)
147             self.handle_key_rotate(now)
148             salt.utils.verify.check_max_open_files(</b></font>self.opts)
149             last = now
150             time.sleep(self.loop_interval)
151     def handle_key_cache(self):
152         if self.opts["key_cache"] == "sched":
153             keys = []
154             if self.opts["transport"] in TRANSPORTS:
155                 acc = "minions"
156             else:
157                 acc = "accepted"
158             for fn_ in os.listdir(os.path.join(self.opts["pki_dir"], acc)):
159                 if not fn_.startswith(".") and os.path.isfile(
160                     os.path.join(self.opts["pki_dir"], acc, fn_)
161                 ):
162                     keys.append(fn_)
163             log.debug("Writing master key cache")
164             with salt.utils.atomicfile.atomic_open(
165                 os.path.join(self.opts["pki_dir"], acc, ".key_cache"), mode="wb"
166             ) as cache_file:
167                 salt.payload.dump(keys, cache_file)
168     def handle_key_rotate(self, now):
169         to_rotate = False
170         dfn = os.path.join(self.opts["cachedir"], ".dfn")
171         try:
172             stats = os.stat(dfn)
173             if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):
174                 to_rotate = True
175                 os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
176             elif stats.st_mode == 0o100400:
177                 to_rotate = True
178             else:
179                 log.error("Found dropfile with incorrect permissions, ignoring...")
180             os.remove(dfn)
181         except os.error:
182             pass
183         if self.opts.get("publish_session"):
184             if now - self.rotate &gt;= self.opts["publish_session"]:
185                 to_rotate = True
186         if to_rotate:
187             log.info("Rotating master AES key")
188             for secret_key, secret_map in SMaster.secrets.items():
189                 with secret_map["secret"].get_lock():
190                     secret_map["secret"].value = salt.utils.stringutils.to_bytes(
191                         secret_map["reload"]()
192                     )
193                 self.event.fire_event(
194                     {"rotate_{}_key".format(secret_key): True}, tag="key"
195                 )
196             self.rotate = now
197             if self.opts.get("ping_on_rotate"):
198                 log.debug("Pinging all connected minions due to key rotation")
199                 salt.utils.master.ping_all_connected_minions(self.opts)
200     def handle_git_pillar(self):
201         try:
202             for pillar in self.git_pillar:
203                 pillar.fetch_remotes()
204         except Exception as exc:  # pylint: disable=broad-except
205             log.error("Exception caught while updating git_pillar", exc_info=True)
206     def handle_schedule(self):
207         try:
208             self.schedule.eval()
209             if self.schedule.loop_interval &lt; self.loop_interval:
210                 self.loop_interval = self.schedule.loop_interval
211         except Exception as exc:  # pylint: disable=broad-except
212             log.error("Exception %s occurred in scheduled job", exc)
213         self.schedule.cleanup_subprocesses()
214     def handle_presence(self, old_present):
215         if self.presence_events and self.event.connect_pull(timeout=3):
216             present = self.ckminions<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.connected_ids()
217             new = present.difference(old_present)
218             lost = old_present.difference(present)
219             if new or lost:
220                 data = {"new": list(</b></font>new), "lost": list(lost)}
221                 self.event.fire_event(data, tagify("change", "presence"))
222             data = {"present": list(present)}
223             self.event.fire_event(data, tagify("present", "presence"))
224             old_present.clear()
225             old_present.update(present)
226 class FileserverUpdate(salt.utils.process.SignalHandlingProcess):
227     def __init__(self, opts, **kwargs):
228         super().__init__(**kwargs)
229         self.opts = opts
230         self.update_threads = {}
231         import salt.fileserver
232         self.fileserver = salt.fileserver.Fileserver(self.opts)
233         self.fill_buckets()
234     def fill_buckets(self):
235         update_intervals = self.fileserver.update_intervals()
236         self.buckets = {}
237         for backend in self.fileserver.backends():
238             fstr = "{}.update".format(backend)
239             try:
240                 update_func = self.fileserver.servers[fstr]
241             except KeyError:
242                 log.debug("No update function for the %s filserver backend", backend)
243                 continue
244             if backend in update_intervals:
245                 for id_, interval in update_intervals[backend].items():
246                     if not interval:
247                         interval = DEFAULT_INTERVAL
248                         log.debug(
249                             "An update_interval of 0 is not supported, "
250                             "falling back to %s",
251                             interval,
252                         )
253                     i_ptr = self.buckets.setdefault(interval, OrderedDict())
254                     i_ptr.setdefault((backend, update_func), []).append(id_)
255             else:
256                 try:
257                     interval_key = "{}_update_interval".format(backend)
258                     interval = self.opts[interval_key]
259                 except KeyError:
260                     interval = DEFAULT_INTERVAL
261                     log.warning(
262                         "%s key missing from configuration. Falling back to "
263                         "default interval of %d seconds",
264                         interval_key,
265                         interval,
266                     )
267                 self.buckets.setdefault(interval, OrderedDict())[
268                     (backend, update_func)
269                 ] = None
270     @staticmethod
271     def _do_update(backends):
272         for backend, update_args in backends.items():
273             backend_name, update_func = backend
274             try:
275                 if update_args:
276                     log.debug(
277                         "Updating %s fileserver cache for the following targets: %s",
278                         backend_name,
279                         update_args,
280                     )
281                     args = (update_args,)
282                 else:
283                     log.debug("Updating %s fileserver cache", backend_name)
284                     args = ()
285                 update_func(*args)
286             except Exception as exc:  # pylint: disable=broad-except
287                 log.exception(
288                     "Uncaught exception while updating %s fileserver cache",
289                     backend_name,
290                 )
291     @classmethod
292     def update(cls, interval, backends, timeout=300):
293         start = time.time()
294         condition = threading.Condition()
295         while time.time() - start &lt; timeout:
296             log.debug(
297                 "Performing fileserver updates for items with an update interval of %d",
298                 interval,
299             )
300             cls._do_update(backends)
301             log.debug(
302                 "Completed fileserver updates for items with an update "
303                 "interval of %d, waiting %d seconds",
304                 interval,
305                 interval,
306             )
307             with condition:
308                 condition.wait(interval)
309     def run(self):
310         if (
311             self.opts["fileserver_update_niceness"]
312             and not salt.utils.platform.is_windows()
313         ):
314             log.info(
315                 "setting FileServerUpdate niceness to %d",
316                 self.opts["fileserver_update_niceness"],
317             )
318             os.nice(self.opts["fileserver_update_niceness"])
319         salt.daemons.masterapi.clean_fsbackend(self.opts)
320         for interval in self.buckets:
321             self.update_threads[interval] = threading.Thread(
322                 target=self.update,
323                 args=(interval, self.buckets[interval]),
324             )
325             self.update_threads[interval].start()
326         while self.update_threads:
327             for name, thread in list(self.update_threads.items()):
328                 thread.join(1)
329                 if not thread.is_alive():
330                     self.update_threads.pop(name)
331 class Master(SMaster):
332     def __init__(self, opts):
333         if zmq and ZMQ_VERSION_INFO &lt; (3, 2):
334             log.warning(
335                 "You have a version of ZMQ less than ZMQ 3.2! There are "
336                 "known connection keep-alive issues with ZMQ &lt; 3.2 which "
337                 "may result in loss of contact with minions. Please "
338                 "upgrade your ZMQ!"
339             )
340         SMaster.__init__(self, opts)
341     def __set_max_open_files(self):
342         if not HAS_RESOURCE:
343             return
344         mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)
345         if mof_h == resource.RLIM_INFINITY:
346             mof_h = mof_s
347         log.info(
348             "Current values for max open files soft/hard setting: %s/%s", mof_s, mof_h
349         )
350         mof_c = self.opts["max_open_files"]
351         if mof_c &gt; mof_h:
352             log.info(
353                 "The value for the 'max_open_files' setting, %s, is higher "
354                 "than the highest value the user running salt is allowed to "
355                 "set (%s). Defaulting to %s.",
356                 mof_c,
357                 mof_h,
358                 mof_h,
359             )
360             mof_c = mof_h
361         if mof_s &lt; mof_c:
362             log.info("Raising max open files value to %s", mof_c)
363             resource.setrlimit(resource.RLIMIT_NOFILE, (mof_c, mof_h))
364             try:
365                 mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)
366                 log.info(
367                     "New values for max open files soft/hard values: %s/%s",
368                     mof_s,
369                     mof_h,
370                 )
371             except ValueError:
372                 log.critical(
373                     "Failed to raise max open files setting to %s. If this "
374                     "value is too low, the salt-master will most likely fail "
375                     "to run properly.",
376                     mof_c,
377                 )
378     def _pre_flight(self):
379         errors = []
380         critical_errors = []
381         try:
382             os.chdir("/")
383         except OSError as err:
384             errors.append("Cannot change to root directory ({})".format(err))
385         if self.opts.get("fileserver_verify_config", True):
386             import salt.fileserver
387             fileserver = salt.fileserver.Fileserver(self.opts)
388             if not fileserver.servers:
389                 errors.append(
390                     "Failed to load fileserver backends, the configured backends "
391                     "are: {}".format(", ".join(self.opts["fileserver_backend"]))
392                 )
393             else:
394                 try:
395                     fileserver.init()
396                 except salt.exceptions.FileserverConfigError as exc:
397                     critical_errors.append("{}".format(exc))
398         if not self.opts["fileserver_backend"]:
399             errors.append("No fileserver backends are configured")
400         if self.opts["pillar_cache"] and not os.path.isdir(
401             os.path.join(self.opts["cachedir"], "pillar_cache")
402         ):
403             try:
404                 with salt.utils.files.set_umask(0o077):
405                     os.mkdir(os.path.join(self.opts["cachedir"], "pillar_cache"))
406             except OSError:
407                 pass
408         if self.opts.get("git_pillar_verify_config", True):
409             try:
410                 git_pillars = [
411                     x
412                     for x in self.opts.get("ext_pillar", [])
413                     if "git" in x and not isinstance(x["git"], str)
414                 ]
415             except TypeError:
416                 git_pillars = []
417                 critical_errors.append(
418                     "Invalid ext_pillar configuration. It is likely that the "
419                     "external pillar type was not specified for one or more "
420                     "external pillars."
421                 )
422             if git_pillars:
423                 try:
424                     new_opts = copy.deepcopy(self.opts)
425                     import salt.pillar.git_pillar
426                     for repo in git_pillars:
427                         new_opts["ext_pillar"] = [repo]
428                         try:
429                             git_pillar = salt.utils.gitfs.GitPillar(
430                                 new_opts,
431                                 repo["git"],
432                                 per_remote_overrides=salt.pillar.git_pillar.PER_REMOTE_OVERRIDES,
433                                 per_remote_only=salt.pillar.git_pillar.PER_REMOTE_ONLY,
434                                 global_only=salt.pillar.git_pillar.GLOBAL_ONLY,
435                             )
436                         except salt.exceptions.FileserverConfigError as exc:
437                             critical_errors.append(exc.strerror)
438                 finally:
439                     del new_opts
440         if errors or critical_errors:
441             for error in errors:
442                 log.error(error)
443             for error in critical_errors:
444                 log.critical(error)
445             log.critical("Master failed pre flight checks, exiting\n")
446             sys.exit(salt.defaults.exitcodes.EX_GENERIC)
447     def start(self):
448         self._pre_flight()
449         log.info("salt-master is starting as user '%s'", salt.utils.user.get_user())
450         enable_sigusr1_handler()
451         enable_sigusr2_handler()
452         self.__set_max_open_files()
453         with salt.utils<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.process.default_signals(signal.SIGINT, signal.SIGTERM):
454             SMaster.secrets["aes"] = {
455                 "secret": multiprocessing.Array(
456                     ctypes.c_char,
457                     salt.</b></font>utils.stringutils.to_bytes(
458                         salt.crypt.Crypticle.generate_key_string()
459                     ),
460                 ),
461                 "reload": salt.crypt.Crypticle.generate_key_string,
462             }
463             log.info("Creating master process manager")
464             self.process_manager = salt.utils.process.ProcessManager(wait_for_kill=5)
465             pub_channels = []
466             log.info("Creating master publisher process")
467             for _, opts in iter_transport_opts(self.opts):
468                 chan = salt.channel.server.PubServerChannel.factory(opts)
469                 chan.pre_fork(self.process_manager)
470                 pub_channels.append(chan)
471             log.info("Creating master event publisher process")
472             self.process_manager.add_process(
473                 salt.utils.event.EventPublisher,
474                 args=(self.opts,),
475                 name="EventPublisher",
476             )
477             if self.opts.get("reactor"):
478                 if isinstance(self.opts["engines"], list):
479                     rine = False
480                     for item in self.opts["engines"]:
481                         if "reactor" in item:
482                             rine = True
483                             break
484                     if not rine:
485                         self.opts["engines"].append({"reactor": {}})
486                 else:
487                     if "reactor" not in self.opts["engines"]:
488                         log.info("Enabling the reactor engine")
489                         self.opts["engines"]["reactor"] = {}
490             salt.engines.start_engines(self.opts, self.process_manager)
491             log.info("Creating master maintenance process")
492             self.process_manager.add_process(
493                 Maintenance, args=(self.opts,), name="Maintenance"
494             )
495             if self.opts.get("event_return"):
496                 log.info("Creating master event return process")
497                 self.process_manager.add_process(
498                     salt.utils.event.EventReturn, args=(self.opts,), name="EventReturn"
499                 )
500             ext_procs = self.opts.get("ext_processes", [])
501             for proc in ext_procs:
502                 log.info("Creating ext_processes process: %s", proc)
503                 try:
504                     mod = ".".join(proc.split(".")[:-1])
505                     cls = proc.split(".")[-1]
506                     _tmp = __import__(mod, globals(), locals(), [cls], -1)
507                     cls = _tmp.__getattribute__(cls)
508                     name = "ExtProcess({})".format(cls.__qualname__)
509                     self.process_manager.add_process(cls, args=(self.opts,), name=name)
510                 except Exception:  # pylint: disable=broad-except
511                     log.error("Error creating ext_processes process: %s", proc)
512             if self.opts["con_cache"]:
513                 log.info("Creating master concache process")
514                 self.process_manager.add_process(
515                     salt.utils.master.ConnectedCache,
516                     args=(self.opts,),
517                     name="ConnectedCache",
518                 )
519                 log.debug("Sleeping for two seconds to let concache rest")
520                 time.sleep(2)
521             log.info("Creating master request server process")
522             kwargs = {}
523             if salt.utils.platform.spawning_platform():
524                 kwargs["secrets"] = SMaster.secrets
525             self.process_manager.add_process(
526                 ReqServer,
527                 args=(self.opts, self.key, self.master_key),
528                 kwargs=kwargs,
529                 name="ReqServer",
530             )
531             self.process_manager.add_process(
532                 FileserverUpdate, args=(self.opts,), name="FileServerUpdate"
533             )
534             if self.opts["discovery"]:
535                 if salt.utils.ssdp.SSDPDiscoveryServer.is_available():
536                     self.process_manager.add_process(
537                         salt.utils.ssdp.SSDPDiscoveryServer(
538                             port=self.opts["discovery"]["port"],
539                             listen_ip=self.opts["interface"],
540                             answer={
541                                 "mapping": self.opts["discovery"].get("mapping", {})
542                             },
543                         ).run,
544                         name="SSDPDiscoveryServer",
545                     )
546                 else:
547                     log.error("Unable to load SSDP: asynchronous IO is not available.")
548                     if sys.version_info.major == 2:
549                         log.error(
550                             'You are using Python 2, please install "trollius" module'
551                             " to enable SSDP discovery."
552                         )
553         if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:
554             signal.signal(signal.SIGINT, self._handle_signals)
555         if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:
556             signal.signal(signal.SIGTERM, self._handle_signals)
557         self.process_manager.run()
558     def _handle_signals(self, signum, sigframe):
559         self.process_manager._handle_signals(signum, sigframe)
560         time.sleep(1)
561         sys.exit(0)
562 class ReqServer(salt.utils.process.SignalHandlingProcess):
563     def __init__(self, opts, key, mkey, secrets=None, **kwargs):
564         super().__init__(**kwargs)
565         self.opts = opts
566         self.master_key = mkey
567         self.key = key
568         self.secrets = secrets
569     def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument
570         self.destroy(signum)
571         super()._handle_signals(signum, sigframe)
572     def __bind(self):
573         if self.secrets is not None:
574             SMaster.secrets = self.secrets
575         dfn = os.path.join(self.opts["cachedir"], ".dfn")
576         if os.path.isfile(dfn):
577             try:
578                 if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):
579                     os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
580                 os.remove(dfn)
581             except os.error:
582                 pass
583         self.process_manager = salt.utils.process.ProcessManager(
584             name="ReqServer_ProcessManager", wait_for_kill=1
585         )
586         req_channels = []
587         tcp_only = True
588         for transport, opts in iter_transport_opts(self.opts):
589             chan = salt.channel.server.ReqServerChannel.factory(opts)
590             chan.pre_fork(self.process_manager)
591             req_channels.append(chan)
592             if transport != "tcp":
593                 tcp_only = False
594         if self.opts["req_server_niceness"] and not salt.utils.platform.is_windows():
595             log.info(
596                 "setting ReqServer_ProcessManager niceness to %d",
597                 self.opts["req_server_niceness"],
598             )
599             os.nice(self.opts["req_server_niceness"])
600         with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):
601             for ind in range(int(self.opts["worker_threads"])):
602                 name = "MWorker-{}".format(ind)
603                 self.process_manager.add_process(
604                     MWorker,
605                     args=(self.opts, self.master_key, self.key, req_channels),
606                     name=name,
607                 )
608         self.process_manager.run()
609     def run(self):
610         self.__bind()
611     def destroy(self, signum=signal.SIGTERM):
612         if hasattr(self, "process_manager"):
613             self.process_manager.stop_restarting()
614             self.process_manager.send_signal_to_processes(signum)
615             self.process_manager.kill_children()
616     def __del__(self):
617         self.destroy()
618 class MWorker(salt.utils.process.SignalHandlingProcess):
619     def __init__(self, opts, mkey, key, req_channels, **kwargs):
620         super().__init__(**kwargs)
621         self.opts = opts
622         self.req_channels = req_channels
623         self.mkey = mkey
624         self.key = key
625         self.k_mtime = 0
626         self.stats = collections.defaultdict(lambda: {"mean": 0, "runs": 0})
627         self.stat_clock = time.time()
628     def __setstate__(self, state):
629         super().__setstate__(state)
630         self.k_mtime = state["k_mtime"]
631         SMaster.secrets = state["secrets"]
632     def __getstate__(self):
633         state = super().__getstate__()
634         state.update({"k_mtime": self.k_mtime, "secrets": SMaster.secrets})
635         return state
636     def _handle_signals(self, signum, sigframe):
637         for channel in getattr(self, "req_channels", ()):
638             channel.close()
639         self.clear_funcs.destroy()
640         super()._handle_signals(signum, sigframe)
641     def __bind(self):
642         self.io_loop = salt.ext.tornado.ioloop.IOLoop()
643         self.io_loop.make_current()
644         for req_channel in self.req_channels:
645             req_channel.post_fork(
646                 self._handle_payload, io_loop=self.io_loop
647             )  # TODO: cleaner? Maybe lazily?
648         try:
649             self.io_loop.start()
650         except (KeyboardInterrupt, SystemExit):
651             pass
652     @salt.ext.tornado.gen.coroutine
653     def _handle_payload(self, payload):
654         key = payload["enc"]
655         load = payload["load"]
656         ret = {"aes": self._handle_aes, "clear": self._handle_clear}[key](load)
657         raise salt.ext.tornado.gen.Return(ret)
658     def _post_stats(self, start, cmd):
659         end = time.time()
660         duration = end - start
661         self.stats[cmd]["mean"] = (
662             self.stats[cmd]["mean"] * (self.stats[cmd]["runs"] - 1) + duration
663         ) / self.stats[cmd]["runs"]
664         if end - self.stat_clock &gt; self.opts["master_stats_event_iter"]:
665             self.aes_funcs.event.fire_event(
666                 {
667                     "time": end - self.stat_clock,
668                     "worker": self.name,
669                     "stats": self.stats,
670                 },
671                 tagify(self.name, "stats"),
672             )
673             self.stats = collections.defaultdict(lambda: {"mean": 0, "runs": 0})
674             self.stat_clock = end
675     def _handle_clear(self, load):
676         log.trace("Clear payload received with command %s", load["cmd"])
677         cmd = load["cmd"]
678         method = self.clear_funcs.get_method(cmd)
679         if not method:
680             return {}, {"fun": "send_clear"}
681         if self.opts["master_stats"]:
682             start = time.time()
683             self.stats[cmd]["runs"] += 1
684         ret = method(load), {"fun": "send_clear"}
685         if self.opts["master_stats"]:
686             self._post_stats(start, cmd)
687         return ret
688     def _handle_aes(self, data):
689         if "cmd" not in data:
690             log.error("Received malformed command %s", data)
691             return {}
692         cmd = data["cmd"]
693         log.trace("AES payload received with command %s", data["cmd"])
694         method = self.aes_funcs.get_method(cmd)
695         if not method:
696             return {}, {"fun": "send"}
697         if self.opts["master_stats"]:
698             start = time.time()
699             self.stats[cmd]["runs"] += 1
700         def run_func(data):
701             return self.aes_funcs.run_func(data["cmd"], data)
702         with StackContext(
703             functools.partial(RequestContext, {"data": data, "opts": self.opts})
704         ):
705             ret = run_func(data)
706         if self.opts["master_stats"]:
707             self._post_stats(start, cmd)
708         return ret
709     def run(self):
710 <a name="2"></a>        if not salt.utils.platform.is_windows():
711             enforce_mworker_niceness = True
712             if self.opts["req_server_niceness"]:
713                 if salt.utils<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.user.get_user() == "root":
714                     log.info(
715                         "%s decrementing inherited ReqServer niceness to 0", self.name
716                     )
717                     log.info(os.nice())
718                     os.nice(-1 * self.opts["req_server_niceness"])
719                 else:
720                     log.error(
721                         "%s unable to decrement niceness for MWorker, not running as"
722                         " root",
723                         self.</b></font>name,
724                     )
725                     enforce_mworker_niceness = False
726             if enforce_mworker_niceness and self.opts["mworker_niceness"]:
727                 log.info(
728                     "setting %s niceness to %i",
729                     self.name,
730                     self.opts["mworker_niceness"],
731                 )
732                 os.nice(self.opts["mworker_niceness"])
733         self.clear_funcs = ClearFuncs(
734             self.opts,
735             self.key,
736         )
737         self.clear_funcs.connect()
738         self.aes_funcs = AESFuncs(self.opts)
739         salt.utils.crypt.reinit_crypto()
740         self.__bind()
741 class TransportMethods:
742     expose_methods = ()
743     def get_method(self, name):
744         if name in self.expose_methods:
745             try:
746                 return getattr(self, name)
747             except AttributeError:
748                 log.error("Requested method not exposed: %s", name)
749         else:
750             log.error("Requested method not exposed: %s", name)
751 class AESFuncs(TransportMethods):
752     expose_methods = (
753         "verify_minion",
754         "_master_tops",
755         "_master_opts",
756         "_mine_get",
757         "_mine",
758         "_mine_delete",
759         "_mine_flush",
760         "_file_recv",
761         "_pillar",
762         "_minion_event",
763         "_handle_minion_event",
764         "_return",
765         "_syndic_return",
766         "minion_runner",
767         "pub_ret",
768         "minion_pub",
769         "minion_publish",
770         "revoke_auth",
771         "_serve_file",
772         "_file_find",
773         "_file_hash",
774         "_file_hash_and_stat",
775         "_file_list",
776         "_file_list_emptydirs",
777         "_dir_list",
778         "_symlink_list",
779         "_file_envs",
780         "_ext_nodes",  # To be removed in 3006 (Sulfur) #60980
781     )
782     def __init__(self, opts):
783         self.opts = opts
784         self.event = salt.utils.event.get_master_event(
785             self.opts, self.opts["sock_dir"], listen=False
786         )
787         self.ckminions = salt.utils.minions.CkMinions(opts)
788         self.local = salt.client.get_local_client(self.opts["conf_file"])
789         self.mminion = salt.minion.MasterMinion(
790             self.opts, states=False, rend=False, ignore_config_errors=True
791         )
792         self.__setup_fileserver()
793         self.masterapi = salt.daemons.masterapi.RemoteFuncs(opts)
794     def __setup_fileserver(self):
795         import salt.fileserver
796         self.fs_ = salt.fileserver.Fileserver(self.opts)
797         self._serve_file = self.fs_.serve_file
798         self._file_find = self.fs_._find_file
799         self._file_hash = self.fs_.file_hash
800         self._file_hash_and_stat = self.fs_.file_hash_and_stat
801         self._file_list = self.fs_.file_list
802         self._file_list_emptydirs = self.fs_.file_list_emptydirs
803         self._dir_list = self.fs_.dir_list
804         self._symlink_list = self.fs_.symlink_list
805         self._file_envs = self.fs_.file_envs
806     def __verify_minion(self, id_, token):
807         if not salt.utils.verify.valid_id(self.opts, id_):
808             return False
809         pub_path = os.path.join(self.opts["pki_dir"], "minions", id_)
810         try:
811             pub = salt.crypt.get_rsa_pub_key(pub_path)
812         except OSError:
813             log.warning(
814                 "Salt minion claiming to be %s attempted to communicate with "
815                 "master, but key could not be read and verification was denied.",
816                 id_,
817             )
818             return False
819         except (ValueError, IndexError, TypeError) as err:
820             log.error('Unable to load public key "%s": %s', pub_path, err)
821         try:
822             if salt.crypt.public_decrypt(pub, token) == b"salt":
823                 return True
824         except ValueError as err:
825             log.error("Unable to decrypt token: %s", err)
826         log.error(
827             "Salt minion claiming to be %s has attempted to communicate with "
828             "the master and could not be verified",
829             id_,
830         )
831         return False
832     def verify_minion(self, id_, token):
833         return self.__verify_minion(id_, token)
834     def __verify_minion_publish(self, clear_load):
835         if "peer" not in self.opts:
836             return False
837         if not isinstance(self.opts["peer"], dict):
838             return False
839         if any(
840             key not in clear_load for key in ("fun", "arg", "tgt", "ret", "tok", "id")
841         ):
842             return False
843         if clear_load["fun"].startswith("publish."):
844             return False
845         if not self.__verify_minion(clear_load["id"], clear_load["tok"]):
846             log.warning(
847                 "Minion id %s is not who it says it is and is attempting "
848                 "to issue a peer command",
849                 clear_load["id"],
850             )
851             return False
852         clear_load.pop("tok")
853         perms = []
854         for match in self.opts["peer"]:
855             if re.match(match, clear_load["id"]):
856                 if isinstance(self.opts["peer"][match], list):
857                     perms.extend(self.opts["peer"][match])
858         if "," in clear_load["fun"]:
859             clear_load["fun"] = clear_load["fun"].split(",")
860             arg_ = []
861             for arg in clear_load["arg"]:
862                 arg_.append(arg.split())
863             clear_load["arg"] = arg_
864         return self.ckminions.auth_check(
865             perms,
866             clear_load["fun"],
867             clear_load["arg"],
868             clear_load["tgt"],
869             clear_load.get("tgt_type", "glob"),
870             publish_validate=True,
871         )
872     def __verify_load(self, load, verify_keys):
873         if any(key not in load for key in verify_keys):
874             return False
875         if "tok" not in load:
876             log.error(
877                 "Received incomplete call from %s for '%s', missing '%s'",
878                 load["id"],
879                 inspect_stack()["co_name"],
880                 "tok",
881             )
882             return False
883         if not self.__verify_minion(load["id"], load["tok"]):
884             log.warning("Minion id %s is not who it says it is!", load["id"])
885             return False
886         if "tok" in load:
887             load.pop("tok")
888         return load
889     def _master_tops(self, load):
890         load = self.__verify_load(load, ("id", "tok"))
891         if load is False:
892             return {}
893         return self.masterapi._master_tops(load, skip_verify=True)
894     _ext_nodes = _master_tops
895     def _master_opts(self, load):
896         mopts = {}
897         file_roots = {}
898         envs = self._file_envs()
899         for saltenv in envs:
900             if saltenv not in file_roots:
901                 file_roots[saltenv] = []
902         mopts["file_roots"] = file_roots
903         mopts["top_file_merging_strategy"] = self.opts["top_file_merging_strategy"]
904         mopts["env_order"] = self.opts["env_order"]
905         mopts["default_top"] = self.opts["default_top"]
906         if load.get("env_only"):
907             return mopts
908         mopts["renderer"] = self.opts["renderer"]
909         mopts["failhard"] = self.opts["failhard"]
910         mopts["state_top"] = self.opts["state_top"]
911         mopts["state_top_saltenv"] = self.opts["state_top_saltenv"]
912         mopts["nodegroups"] = self.opts["nodegroups"]
913         mopts["state_auto_order"] = self.opts["state_auto_order"]
914         mopts["state_events"] = self.opts["state_events"]
915         mopts["state_aggregate"] = self.opts["state_aggregate"]
916         mopts["jinja_env"] = self.opts["jinja_env"]
917         mopts["jinja_sls_env"] = self.opts["jinja_sls_env"]
918         mopts["jinja_lstrip_blocks"] = self.opts["jinja_lstrip_blocks"]
919         mopts["jinja_trim_blocks"] = self.opts["jinja_trim_blocks"]
920         return mopts
921     def _mine_get(self, load):
922         load = self.__verify_load(load, ("id", "tgt", "fun", "tok"))
923         if load is False:
924             return {}
925         else:
926             return self.masterapi._mine_get(load, skip_verify=True)
927     def _mine(self, load):
928         load = self.__verify_load(load, ("id", "data", "tok"))
929         if load is False:
930             return {}
931         return self.masterapi._mine(load, skip_verify=True)
932     def _mine_delete(self, load):
933         load = self.__verify_load(load, ("id", "fun", "tok"))
934         if load is False:
935             return {}
936         else:
937             return self.masterapi._mine_delete(load)
938     def _mine_flush(self, load):
939         load = self.__verify_load(load, ("id", "tok"))
940         if load is False:
941             return {}
942         else:
943             return self.masterapi._mine_flush(load, skip_verify=True)
944     def _file_recv(self, load):
945         if any(key not in load for key in ("id", "path", "loc")):
946             return False
947         if not isinstance(load["path"], list):
948             return False
949         if not self.opts["file_recv"]:
950             return False
951         if not salt.utils.verify.valid_id(self.opts, load["id"]):
952             return False
953         file_recv_max_size = 1024 * 1024 * self.opts["file_recv_max_size"]
954         if "loc" in load and load["loc"] &lt; 0:
955             log.error("Invalid file pointer: load[loc] &lt; 0")
956             return False
957         if len(load["data"]) + load.get("loc", 0) &gt; file_recv_max_size:
958             log.error(
959                 "file_recv_max_size limit of %d MB exceeded! %s will be "
960                 "truncated. To successfully push this file, adjust "
961                 "file_recv_max_size to an integer (in MB) large enough to "
962                 "accommodate it.",
963                 file_recv_max_size,
964                 load["path"],
965             )
966             return False
967         if "tok" not in load:
968             log.error(
969                 "Received incomplete call from %s for '%s', missing '%s'",
970                 load["id"],
971                 inspect_stack()["co_name"],
972                 "tok",
973             )
974             return False
975         if not self.__verify_minion(load["id"], load["tok"]):
976             log.warning("Minion id %s is not who it says it is!", load["id"])
977             return {}
978         load.pop("tok")
979         sep_path = os.sep.join(load["path"])
980         normpath = os.path.normpath(sep_path)
981         if os.path.isabs(normpath) or "../" in load["path"]:
982             return False
983         cpath = os.path.join(
984             self.opts["cachedir"], "minions", load["id"], "files", normpath
985         )
986         if not os.path.normpath(cpath).startswith(self.opts["cachedir"]):
987             log.warning(
988                 "Attempt to write received file outside of master cache "
989                 "directory! Requested path: %s. Access denied.",
990                 cpath,
991             )
992             return False
993         cdir = os.path.dirname(cpath)
994         if not os.path.isdir(cdir):
995             try:
996                 os.makedirs(cdir)
997             except os.error:
998                 pass
999         if os.path.isfile(cpath) and load["loc"] != 0:
1000             mode = "ab"
1001         else:
1002             mode = "wb"
1003         with salt.utils.files.fopen(cpath, mode) as fp_:
1004             if load["loc"]:
1005                 fp_.seek(load["loc"])
1006             fp_.write(salt.utils.stringutils.to_bytes(load["data"]))
1007         return True
1008     def _pillar(self, load):
1009         if any(key not in load for key in ("id", "grains")):
1010             return False
1011         if not salt.utils.verify.valid_id(self.opts, load["id"]):
1012             return False
1013         load["grains"]["id"] = load["id"]
1014         pillar = salt.pillar.get_pillar(
1015 <a name="1"></a>            self.opts,
1016             load["grains"],
1017             load["id"],
1018             load.get("saltenv", load<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.get("env")),
1019             ext=load.get("ext"),
1020             pillar_override=load.get("pillar_override", {}),
1021             pillarenv=load.get("pillarenv"),
1022             extra_minion_data=load.get("extra_minion_data"),
1023             clean_cache=load.get("clean_cache"),
1024         )
1025         data = pillar.compile_pillar()
1026         self.</b></font>fs_.update_opts()
1027         if self.opts.get("minion_data_cache", False):
1028             self.masterapi.cache.store(
1029                 "minions/{}".format(load["id"]),
1030                 "data",
1031                 {"grains": load["grains"], "pillar": data},
1032             )
1033             if self.opts.get("minion_data_cache_events") is True:
1034                 self.event.fire_event(
1035                     {"Minion data cache refresh": load["id"]},
1036                     tagify(load["id"], "refresh", "minion"),
1037                 )
1038         return data
1039     def _minion_event(self, load):
1040         load = self.__verify_load(load, ("id", "tok"))
1041         if load is False:
1042             return {}
1043         self.masterapi._minion_event(load)
1044         self._handle_minion_event(load)
1045     def _handle_minion_event(self, load):
1046         id_ = load["id"]
1047         if load.get("tag", "") == "_salt_error":
1048             log.error(
1049                 "Received minion error from [%s]: %s", id_, load["data"]["message"]
1050             )
1051         for event in load.get("events", []):
1052             event_data = event.get("data", {})
1053             if "minions" in event_data:
1054                 jid = event_data.get("jid")
1055                 if not jid:
1056                     continue
1057                 minions = event_data["minions"]
1058                 try:
1059                     salt.utils.job.store_minions(
1060                         self.opts, jid, minions, mminion=self.mminion, syndic_id=id_
1061                     )
1062                 except (KeyError, salt.exceptions.SaltCacheError) as exc:
1063                     log.error(
1064                         "Could not add minion(s) %s for job %s: %s", minions, jid, exc
1065                     )
1066     def _return(self, load):
1067         if self.opts["require_minion_sign_messages"] and "sig" not in load:
1068             log.critical(
1069                 "_return: Master is requiring minions to sign their "
1070                 "messages, but there is no signature in this payload from "
1071                 "%s.",
1072                 load["id"],
1073             )
1074             return False
1075         if "sig" in load:
1076             log.trace("Verifying signed event publish from minion")
1077             sig = load.pop("sig")
1078             this_minion_pubkey = os.path.join(
1079                 self.opts["pki_dir"], "minions/{}".format(load["id"])
1080             )
1081             serialized_load = salt.serializers.msgpack.serialize(load)
1082             if not salt.crypt.verify_signature(
1083                 this_minion_pubkey, serialized_load, sig
1084             ):
1085                 log.info("Failed to verify event signature from minion %s.", load["id"])
1086                 if self.opts["drop_messages_signature_fail"]:
1087                     log.critical(
1088                         "drop_messages_signature_fail is enabled, dropping "
1089                         "message from %s",
1090                         load["id"],
1091                     )
1092                     return False
1093                 else:
1094                     log.info(
1095                         "But 'drop_message_signature_fail' is disabled, so message is"
1096                         " still accepted."
1097                     )
1098             load["sig"] = sig
1099         try:
1100             salt.utils.job.store_job(
1101                 self.opts, load, event=self.event, mminion=self.mminion
1102             )
1103         except salt.exceptions.SaltCacheError:
1104             log.error("Could not store job information for load: %s", load)
1105     def _syndic_return(self, load):
1106         loads = load.get("load")
1107         if not isinstance(loads, list):
1108             loads = [load]  # support old syndics not aggregating returns
1109         for load in loads:
1110             if any(key not in load for key in ("return", "jid", "id")):
1111                 continue
1112             if load.get("load"):
1113                 fstr = "{}.save_load".format(self.opts["master_job_cache"])
1114                 self.mminion.returners[fstr](load["jid"], load["load"])
1115             syndic_cache_path = os.path.join(
1116                 self.opts["cachedir"], "syndics", load["id"]
1117             )
1118             if not os.path.exists(syndic_cache_path):
1119                 path_name = os.path.split(syndic_cache_path)[0]
1120                 if not os.path.exists(path_name):
1121                     os.makedirs(path_name)
1122                 with salt.utils.files.fopen(syndic_cache_path, "w") as wfh:
1123                     wfh.write("")
1124             for key, item in load["return"].items():
1125                 ret = {"jid": load["jid"], "id": key}
1126                 ret.update(item)
1127                 if "master_id" in load:
1128                     ret["master_id"] = load["master_id"]
1129                 if "fun" in load:
1130                     ret["fun"] = load["fun"]
1131                 if "arg" in load:
1132                     ret["fun_args"] = load["arg"]
1133                 if "out" in load:
1134                     ret["out"] = load["out"]
1135                 if "sig" in load:
1136                     ret["sig"] = load["sig"]
1137                 self._return(ret)
1138     def minion_runner(self, clear_load):
1139         load = self.__verify_load(clear_load, ("fun", "arg", "id", "tok"))
1140         if load is False:
1141             return {}
1142         else:
1143             return self.masterapi.minion_runner(clear_load)
1144     def pub_ret(self, load):
1145         load = self.__verify_load(load, ("jid", "id", "tok"))
1146         if load is False:
1147             return {}
1148         auth_cache = os.path.join(self.opts["cachedir"], "publish_auth")
1149         if not os.path.isdir(auth_cache):
1150             os.makedirs(auth_cache)
1151         jid_fn = os.path.join(auth_cache, str(load["jid"]))
1152         with salt.utils.files.fopen(jid_fn, "r") as fp_:
1153             if not load["id"] == fp_.read():
1154                 return {}
1155         return self.local.get_cache_returns(load["jid"])
1156     def minion_pub(self, clear_load):
1157         if not self.__verify_minion_publish(clear_load):
1158             return {}
1159         else:
1160             return self.masterapi.minion_pub(clear_load)
1161     def minion_publish(self, clear_load):
1162         if not self.__verify_minion_publish(clear_load):
1163             return {}
1164         else:
1165             return self.masterapi.minion_publish(clear_load)
1166     def revoke_auth(self, load):
1167         load = self.__verify_load(load, ("id", "tok"))
1168         if not self.opts.get("allow_minion_key_revoke", False):
1169             log.warning(
1170                 "Minion %s requested key revoke, but allow_minion_key_revoke "
1171                 "is set to False",
1172                 load["id"],
1173             )
1174             return load
1175         if load is False:
1176             return load
1177         else:
1178             return self.masterapi.revoke_auth(load)
1179     def run_func(self, func, load):
1180         if func.startswith("__"):
1181             return {}, {"fun": "send"}
1182         if hasattr(self, func):
1183             try:
1184                 start = time.time()
1185                 ret = getattr(self, func)(load)
1186                 log.trace(
1187                     "Master function call %s took %s seconds", func, time.time() - start
1188                 )
1189             except Exception:  # pylint: disable=broad-except
1190                 ret = ""
1191                 log.error("Error in function %s:\n", func, exc_info=True)
1192         else:
1193             log.error(
1194                 "Received function %s which is unavailable on the master, "
1195                 "returning False",
1196                 func,
1197             )
1198             return False, {"fun": "send"}
1199         if func == "_return":
1200             return ret, {"fun": "send"}
1201         if func == "_pillar" and "id" in load:
1202             if load.get("ver") != "2" and self.opts["pillar_version"] == 1:
1203                 return ret, {"fun": "send"}
1204             return ret, {"fun": "send_private", "key": "pillar", "tgt": load["id"]}
1205         return ret, {"fun": "send"}
1206     def destroy(self):
1207         self.masterapi.destroy()
1208         if self.local is not None:
1209             self.local.destroy()
1210             self.local = None
1211 class ClearFuncs(TransportMethods):
1212     expose_methods = (
1213         "ping",
1214         "publish",
1215         "get_token",
1216         "mk_token",
1217         "wheel",
1218         "runner",
1219     )
1220     def __init__(self, opts, key):
1221         self.opts = opts
1222         self.key = key
1223         self.event = salt.utils.event.get_master_event(
1224             self.opts, self.opts["sock_dir"], listen=False
1225         )
1226         self.local = salt.client.get_local_client(self.opts["conf_file"])
1227         self.ckminions = salt.utils.minions.CkMinions(opts)
1228         self.loadauth = salt.auth.LoadAuth(opts)
1229         self.mminion = salt.minion.MasterMinion(
1230             self.opts, states=False, rend=False, ignore_config_errors=True
1231         )
1232         self.wheel_ = salt.wheel.Wheel(opts)
1233         self.masterapi = salt.daemons.masterapi.LocalFuncs(opts, key)
1234         self.channels = []
1235     def runner(self, clear_load):
1236         auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)
1237         auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)
1238         error = auth_check.get("error")
1239         if error:
1240             return {"error": error}
1241         username = auth_check.get("username")
1242         if auth_type != "user":
1243             runner_check = self.ckminions.runner_check(
1244                 auth_check.get("auth_list", []),
1245                 clear_load["fun"],
1246                 clear_load.get("kwarg", {}),
1247             )
1248             if not runner_check:
1249                 return {
1250                     "error": {
1251                         "name": err_name,
1252                         "message": (
1253                             'Authentication failure of type "{}" occurred for '
1254                             "user {}.".format(auth_type, username)
1255                         ),
1256                     }
1257                 }
1258             elif isinstance(runner_check, dict) and "error" in runner_check:
1259                 return runner_check
1260             for item in sensitive_load_keys:
1261                 clear_load.pop(item, None)
1262         else:
1263             if "user" in clear_load:
1264                 username = clear_load["user"]
1265                 if salt.auth.AuthUser(username).is_sudo():
1266                     username = self.opts.get("user", "root")
1267             else:
1268                 username = salt.utils.user.get_user()
1269         try:
1270             fun = clear_load.pop("fun")
1271             runner_client = salt.runner.RunnerClient(self.opts)
1272             return runner_client.asynchronous(
1273                 fun, clear_load.get("kwarg", {}), username, local=True
1274             )
1275         except Exception as exc:  # pylint: disable=broad-except
1276             log.error("Exception occurred while introspecting %s: %s", fun, exc)
1277             return {
1278                 "error": {
1279                     "name": exc.__class__.__name__,
1280                     "args": exc.args,
1281                     "message": str(exc),
1282                 }
1283             }
1284     def wheel(self, clear_load):
1285         auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)
1286         auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)
1287         error = auth_check.get("error")
1288         if error:
1289             return {"error": error}
1290         username = auth_check.get("username")
1291         if auth_type != "user":
1292             wheel_check = self.ckminions.wheel_check(
1293                 auth_check.get("auth_list", []),
1294                 clear_load["fun"],
1295                 clear_load.get("kwarg", {}),
1296             )
1297             if not wheel_check:
1298                 return {
1299                     "error": {
1300                         "name": err_name,
1301                         "message": (
1302                             'Authentication failure of type "{}" occurred for '
1303                             "user {}.".format(auth_type, username)
1304                         ),
1305                     }
1306                 }
1307             elif isinstance(wheel_check, dict) and "error" in wheel_check:
1308                 return wheel_check
1309             for item in sensitive_load_keys:
1310                 clear_load.pop(item, None)
1311         else:
1312             if "user" in clear_load:
1313                 username = clear_load["user"]
1314                 if salt.auth.AuthUser(username).is_sudo():
1315                     username = self.opts.get("user", "root")
1316             else:
1317                 username = salt.utils.user.get_user()
1318         try:
1319             jid = salt.utils.jid.gen_jid(self.opts)
1320             fun = clear_load.pop("fun")
1321             tag = tagify(jid, prefix="wheel")
1322             data = {
1323                 "fun": "wheel.{}".format(fun),
1324                 "jid": jid,
1325                 "tag": tag,
1326                 "user": username,
1327             }
1328             self.event.fire_event(data, tagify([jid, "new"], "wheel"))
1329             ret = self.wheel_.call_func(fun, full_return=True, **clear_load)
1330             data["return"] = ret["return"]
1331             data["success"] = ret["success"]
1332             self.event.fire_event(data, tagify([jid, "ret"], "wheel"))
1333             return {"tag": tag, "data": data}
1334         except Exception as exc:  # pylint: disable=broad-except
1335             log.error("Exception occurred while introspecting %s: %s", fun, exc)
1336             data["return"] = "Exception occurred in wheel {}: {}: {}".format(
1337                 fun,
1338                 exc.__class__.__name__,
1339                 exc,
1340             )
1341             data["success"] = False
1342             self.event.fire_event(data, tagify([jid, "ret"], "wheel"))
1343             return {"tag": tag, "data": data}
1344     def mk_token(self, clear_load):
1345         token = self.loadauth.mk_token(clear_load)
1346         if not token:
1347             log.warning('Authentication failure of type "eauth" occurred.')
1348             return ""
1349         return token
1350     def get_token(self, clear_load):
1351         if "token" not in clear_load:
1352             return False
1353         return self.loadauth.get_tok(clear_load["token"])
1354     def publish(self, clear_load):
1355         extra = clear_load.get("kwargs", {})
1356         publisher_acl = salt.acl.PublisherACL(self.opts["publisher_acl_blacklist"])
1357         if publisher_acl.user_is_blacklisted(
1358             clear_load["user"]
1359         ) or publisher_acl.cmd_is_blacklisted(clear_load["fun"]):
1360             log.error(
1361                 "%s does not have permissions to run %s. Please contact "
1362                 "your local administrator if you believe this is in "
1363                 "error.\n",
1364                 clear_load["user"],
1365                 clear_load["fun"],
1366             )
1367             return {
1368                 "error": {
1369                     "name": "AuthorizationError",
1370                     "message": "Authorization error occurred.",
1371                 }
1372 <a name="4"></a>            }
1373         delimiter <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= clear_load.get("kwargs", {}).get("delimiter", DEFAULT_TARGET_DELIM)
1374         _res = self.ckminions.check_minions(
1375             clear_load["tgt"], clear_load.get(</b></font>"tgt_type", "glob"), delimiter
1376         )
1377         minions = _res.get("minions", list())
1378         missing = _res.get("missing", list())
1379         ssh_minions = _res.get("ssh_minions", False)
1380         auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(extra)
1381         if auth_type == "user":
1382             auth_check = self.loadauth.check_authentication(
1383                 clear_load, auth_type, key=key
1384             )
1385         else:
1386             auth_check = self.loadauth.check_authentication(extra, auth_type)
1387         auth_list = auth_check.get("auth_list", [])
1388         err_msg = 'Authentication failure of type "{}" occurred.'.format(auth_type)
1389         if auth_check.get("error"):
1390             log.warning(err_msg)
1391             return {
1392                 "error": {
1393                     "name": "AuthenticationError",
1394                     "message": "Authentication error occurred.",
1395                 }
1396             }
1397         if auth_type != "user" or (auth_type == "user" and auth_list):
1398             authorized = self.ckminions.auth_check(
1399                 auth_list,
1400                 clear_load["fun"],
1401                 clear_load["arg"],
1402                 clear_load["tgt"],
1403                 clear_load.get("tgt_type", "glob"),
1404                 minions=minions,
1405                 whitelist=["saltutil.find_job"],
1406             )
1407             if not authorized:
1408                 if (
1409                     auth_type == "eauth"
1410                     and not auth_list
1411                     and "username" in extra
1412                     and "eauth" in extra
1413                 ):
1414                     log.debug(
1415                         'Auth configuration for eauth "%s" and user "%s" is empty',
1416                         extra["eauth"],
1417                         extra["username"],
1418                     )
1419                 log.warning(err_msg)
1420                 return {
1421                     "error": {
1422                         "name": "AuthorizationError",
1423                         "message": "Authorization error occurred.",
1424                     }
1425                 }
1426             if auth_type == "token":
1427                 username = auth_check.get("username")
1428                 clear_load["user"] = username
1429                 log.debug('Minion tokenized user = "%s"', username)
1430             elif auth_type == "eauth":
1431                 clear_load["user"] = self.loadauth.load_name(extra)
1432         if not self.opts.get("order_masters"):
1433             if not minions:
1434                 return {
1435                     "enc": "clear",
1436                     "load": {
1437                         "jid": None,
1438                         "minions": minions,
1439                         "error": (
1440                             "Master could not resolve minions for target {}".format(
1441                                 clear_load["tgt"]
1442                             )
1443                         ),
1444                     },
1445                 }
1446         jid = self._prep_jid(clear_load, extra)
1447         if jid is None:
1448             return {"enc": "clear", "load": {"error": "Master failed to assign jid"}}
1449         payload = self._prep_pub(minions, jid, clear_load, extra, missing)
1450         self._send_ssh_pub(payload, ssh_minions=ssh_minions)
1451         self._send_pub(payload)
1452         return {
1453             "enc": "clear",
1454             "load": {"jid": clear_load["jid"], "minions": minions, "missing": missing},
1455         }
1456     def _prep_auth_info(self, clear_load):
1457         sensitive_load_keys = []
1458         key = None
1459         if "token" in clear_load:
1460             auth_type = "token"
1461             err_name = "TokenAuthenticationError"
1462             sensitive_load_keys = ["token"]
1463         elif "eauth" in clear_load:
1464             auth_type = "eauth"
1465             err_name = "EauthAuthenticationError"
1466             sensitive_load_keys = ["username", "password"]
1467         else:
1468             auth_type = "user"
1469             err_name = "UserAuthenticationError"
1470             key = self.key
1471         return auth_type, err_name, key, sensitive_load_keys
1472     def _prep_jid(self, clear_load, extra):
1473         passed_jid = clear_load["jid"] if clear_load.get("jid") else None
1474         nocache = extra.get("nocache", False)
1475         fstr = "{}.prep_jid".format(self.opts["master_job_cache"])
1476         try:
1477             jid = self.mminion.returners[fstr](nocache=nocache, passed_jid=passed_jid)
1478         except (KeyError, TypeError):
1479             msg = (
1480                 "Failed to allocate a jid. The requested returner '{}' "
1481                 "could not be loaded.".format(fstr.split(".")[0])
1482             )
1483             log.error(msg)
1484             return {"error": msg}
1485         return jid
1486     def _send_pub(self, load):
1487         if not self.channels:
1488             for transport, opts in iter_transport_opts(self.opts):
1489                 chan = salt.channel.server.PubServerChannel.factory(opts)
1490                 self.channels.append(chan)
1491         for chan in self.channels:
1492             chan.publish(load)
1493     @property
1494     def ssh_client(self):
1495         if not hasattr(self, "_ssh_client"):
1496             self._ssh_client = salt.client.ssh.client.SSHClient(mopts=self.opts)
1497         return self._ssh_client
1498     def _send_ssh_pub(self, load, ssh_minions=False):
1499         if self.opts["enable_ssh_minions"] is True and ssh_minions is True:
1500             log.debug("Send payload to ssh minions")
1501             threading.Thread(target=self.ssh_client.cmd, kwargs=load).start()
1502     def _prep_pub(self, minions, jid, clear_load, extra, missing):
1503         clear_load["jid"] = jid
1504         delimiter = clear_load.get("kwargs", {}).get("delimiter", DEFAULT_TARGET_DELIM)
1505         self.event.fire_event({"minions": minions}, clear_load["jid"])
1506         new_job_load = {
1507             "jid": clear_load["jid"],
1508             "tgt_type": clear_load["tgt_type"],
1509             "tgt": clear_load["tgt"],
1510             "user": clear_load["user"],
1511             "fun": clear_load["fun"],
1512             "arg": clear_load["arg"],
1513             "minions": minions,
1514             "missing": missing,
1515         }
1516         self.event.fire_event(new_job_load, tagify([clear_load["jid"], "new"], "job"))
1517         if self.opts["ext_job_cache"]:
1518             fstr = "{}.save_load".format(self.opts["ext_job_cache"])
1519             save_load_func = True
1520             try:
1521                 arg_spec = salt.utils.args.get_function_argspec(
1522                     self.mminion.returners[fstr]
1523                 )
1524                 if "minions" not in arg_spec.args:
1525                     log.critical(
1526                         "The specified returner used for the external job cache "
1527                         "'%s' does not have a 'minions' kwarg in the returner's "
1528                         "save_load function.",
1529                         self.opts["ext_job_cache"],
1530                     )
1531             except (AttributeError, KeyError):
1532                 save_load_func = False
1533                 log.critical(
1534                     "The specified returner used for the external job cache "
1535                     '"%s" does not have a save_load function!',
1536                     self.opts["ext_job_cache"],
1537                 )
1538             if save_load_func:
1539                 try:
1540                     self.mminion.returners[fstr](
1541                         clear_load["jid"], clear_load, minions=minions
1542                     )
1543                 except Exception:  # pylint: disable=broad-except
1544                     log.critical(
1545                         "The specified returner threw a stack trace:\n", exc_info=True
1546                     )
1547         try:
1548             fstr = "{}.save_load".format(self.opts["master_job_cache"])
1549             self.mminion.returners[fstr](clear_load["jid"], clear_load, minions)
1550         except KeyError:
1551             log.critical(
1552                 "The specified returner used for the master job cache "
1553                 '"%s" does not have a save_load function!',
1554                 self.opts["master_job_cache"],
1555             )
1556         except Exception:  # pylint: disable=broad-except
1557             log.critical("The specified returner threw a stack trace:\n", exc_info=True)
1558         payload = {"enc": "aes"}
1559         load = {
1560             "fun": clear_load["fun"],
1561             "arg": clear_load["arg"],
1562             "tgt": clear_load["tgt"],
1563             "jid": clear_load["jid"],
1564             "ret": clear_load["ret"],
1565         }
1566         if "master_id" in self.opts:
1567             load["master_id"] = self.opts["master_id"]
1568         if "master_id" in extra:
1569             load["master_id"] = extra["master_id"]
1570         if delimiter != DEFAULT_TARGET_DELIM:
1571             load["delimiter"] = delimiter
1572         if "id" in extra:
1573             load["id"] = extra["id"]
1574         if "tgt_type" in clear_load:
1575             load["tgt_type"] = clear_load["tgt_type"]
1576         if "to" in clear_load:
1577             load["to"] = clear_load["to"]
1578         if "kwargs" in clear_load:
1579             if "ret_config" in clear_load["kwargs"]:
1580                 load["ret_config"] = clear_load["kwargs"].get("ret_config")
1581             if "metadata" in clear_load["kwargs"]:
1582                 load["metadata"] = clear_load["kwargs"].get("metadata")
1583             if "module_executors" in clear_load["kwargs"]:
1584                 load["module_executors"] = clear_load["kwargs"].get("module_executors")
1585             if "executor_opts" in clear_load["kwargs"]:
1586                 load["executor_opts"] = clear_load["kwargs"].get("executor_opts")
1587             if "ret_kwargs" in clear_load["kwargs"]:
1588                 load["ret_kwargs"] = clear_load["kwargs"].get("ret_kwargs")
1589         if "user" in clear_load:
1590             log.info(
1591                 "User %s Published command %s with jid %s",
1592                 clear_load["user"],
1593                 clear_load["fun"],
1594                 clear_load["jid"],
1595             )
1596             load["user"] = clear_load["user"]
1597         else:
1598             log.info(
1599                 "Published command %s with jid %s", clear_load["fun"], clear_load["jid"]
1600             )
1601         log.debug("Published command details %s", load)
1602         return load
1603     def ping(self, clear_load):
1604         return clear_load
1605     def destroy(self):
1606         if self.masterapi is not None:
1607             self.masterapi.destroy()
1608             self.masterapi = None
1609         if self.local is not None:
1610             self.local.destroy()
1611             self.local = None
1612         while self.channels:
1613             chan = self.channels.pop()
1614             chan.close()
1615     def connect(self):
1616         if self.channels:
1617             return
1618         for transport, opts in iter_transport_opts(self.opts):
1619             chan = salt.channel.server.PubServerChannel.factory(opts)
1620             self.channels.append(chan)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>virt_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import base64
2 import collections
3 import copy
4 import datetime
5 import logging
6 import os
7 import re
8 import shutil
9 import string  # pylint: disable=deprecated-module
10 import subprocess
11 import sys
12 import time
13 import urllib.parse
14 from xml.etree import ElementTree
15 from xml.sax import saxutils
16 import jinja2.exceptions
17 import salt.utils.data
18 import salt.utils.files
19 import salt.utils.json
20 import salt.utils.path
21 import salt.utils.stringutils
22 import salt.utils.templates
23 import salt.utils.virt
24 import salt.utils.xmlutil as xmlutil
25 import salt.utils.yaml
26 from salt._compat import ipaddress
27 from salt.exceptions import CommandExecutionError, SaltInvocationError
28 try:
29     import</b></font> libvirt  # pylint: disable=import-error
30     from libvirt import libvirtError
31     HAS_LIBVIRT = True
32 except ImportError:
33     HAS_LIBVIRT = False
34 log = logging.getLogger(__name__)
35 JINJA = jinja2.Environment(
36     loader=jinja2.FileSystemLoader(
37         os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, "virt")
38     )
39 )
40 CACHE_DIR = "/var/lib/libvirt/saltinst"
41 VIRT_STATE_NAME_MAP = {
42     0: "running",
43     1: "running",
44     2: "running",
45     3: "paused",
46     4: "shutdown",
47     5: "shutdown",
48     6: "crashed",
49 }
50 def __virtual__():
51     if not HAS_LIBVIRT:
52         return (False, "Unable to locate or import python libvirt library.")
53     return "virt"
54 def __get_request_auth(username, password):
55     def __request_auth(credentials, user_data):
56 <a name="3"></a>        for credential in credentials:
57             if credential[0] == libvirt.VIR_CRED_AUTHNAME:
58                 credential[4] = (
59                     <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>username
60                     if username
61                     else __salt__["config.get"](
62                         "virt:connection:auth:username", credential[3]
63                     )
64                 )
65             elif credential[0] == libvirt.VIR_CRED_NOECHOPROMPT:
66                 credential[4] = (
67                     password
68                     if password
69                     else __salt__["config.get"](
70                         "virt:connection:auth:password", credential[3]
71                     )
72                 )
73             else:
74                 log.</b></font>info("Unhandled credential type: %s", credential[0])
75         return 0
76 def __get_conn(**kwargs):
77     username = kwargs<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get("username", None)
78     password = kwargs.get("password", None)
79     conn_str = kwargs.get("connection", None)
80     if not conn_str:
81         conn_str = __salt__["config.get"](</b></font>"virt:connection:uri", conn_str)
82     try:
83         auth_types = [
84             libvirt.VIR_CRED_AUTHNAME,
85             libvirt.VIR_CRED_NOECHOPROMPT,
86             libvirt.VIR_CRED_ECHOPROMPT,
87             libvirt.VIR_CRED_PASSPHRASE,
88             libvirt.VIR_CRED_EXTERNAL,
89         ]
90         conn = libvirt.openAuth(
91             conn_str, [auth_types, __get_request_auth(username, password), None], 0
92         )
93     except Exception:  # pylint: disable=broad-except
94         raise CommandExecutionError(
95             "Sorry, {} failed to open a connection to the hypervisor "
96             "software at {}".format(__grains__["fqdn"], conn_str)
97         )
98     return conn
99 def _get_domain(conn, *vms, **kwargs):
100     ret = list()
101     lookup_vms = list()
102     all_vms = []
103     if kwargs.get("active", True):
104         for id_ in conn.listDomainsID():
105             all_vms.append(conn.lookupByID(id_).name())
106     if kwargs.get("inactive", True):
107         for id_ in conn.listDefinedDomains():
108             all_vms.append(id_)
109     if vms and not all_vms:
110         raise CommandExecutionError("No virtual machines found.")
111     if vms:
112         for name in vms:
113             if name not in all_vms:
114                 raise CommandExecutionError(
115                     'The VM "{name}" is not present'.format(name=name)
116                 )
117             else:
118                 lookup_vms.append(name)
119     else:
120         lookup_vms = list(all_vms)
121     for name in lookup_vms:
122         ret.append(conn.lookupByName(name))
123     return len(ret) == 1 and not kwargs.get("iterable") and ret[0] or ret
124 def _parse_qemu_img_info(info):
125     raw_infos = salt.utils.json.loads(info)
126     disks = []
127     for disk_infos in raw_infos:
128         disk = {
129             "file": disk_infos["filename"],
130             "file format": disk_infos["format"],
131             "disk size": disk_infos["actual-size"],
132             "virtual size": disk_infos["virtual-size"],
133             "cluster size": disk_infos["cluster-size"]
134             if "cluster-size" in disk_infos
135             else None,
136         }
137         if "full-backing-filename" in disk_infos.keys():
138             disk["backing file"] = format(disk_infos["full-backing-filename"])
139         if "snapshots" in disk_infos.keys():
140             disk["snapshots"] = [
141                 {
142                     "id": snapshot["id"],
143                     "tag": snapshot["name"],
144                     "vmsize": snapshot["vm-state-size"],
145                     "date": datetime.datetime.fromtimestamp(
146                         float(
147                             "{}.{}".format(snapshot["date-sec"], snapshot["date-nsec"])
148                         )
149                     ).isoformat(),
150                     "vmclock": datetime.datetime.utcfromtimestamp(
151                         float(
152                             "{}.{}".format(
153                                 snapshot["vm-clock-sec"], snapshot["vm-clock-nsec"]
154                             )
155                         )
156                     )
157                     .time()
158                     .isoformat(),
159                 }
160                 for snapshot in disk_infos["snapshots"]
161             ]
162         disks.append(disk)
163     for disk in disks:
164         if "backing file" in disk.keys():
165             candidates = [
166                 info
167                 for info in disks
168                 if "file" in info.keys() and info["file"] == disk["backing file"]
169             ]
170             if candidates:
171                 disk["backing file"] = candidates[0]
172     return disks[0]
173 def _get_uuid(dom):
174     return ElementTree.fromstring(get_xml(dom)).find("uuid").text
175 def _get_on_poweroff(dom):
176     node = ElementTree.fromstring(get_xml(dom)).find("on_poweroff")
177     return node.text if node is not None else ""
178 def _get_on_reboot(dom):
179     node = ElementTree.fromstring(get_xml(dom)).find("on_reboot")
180     return node.text if node is not None else ""
181 def _get_on_crash(dom):
182     node = ElementTree.fromstring(get_xml(dom)).find("on_crash")
183     return node.text if node is not None else ""
184 def _get_nics(dom):
185     nics = {}
186     doc = ElementTree.fromstring(dom.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE))
187     for iface_node in doc.findall("devices/interface"):
188         nic = {}
189         nic["type"] = iface_node.get("type")
190         for v_node in iface_node:
191             if v_node.tag == "mac":
192                 nic["mac"] = v_node.get("address")
193             if v_node.tag == "model":
194                 nic["model"] = v_node.get("type")
195             if v_node.tag == "target":
196                 nic["target"] = v_node.get("dev")
197             if re.match("(driver|source|address)", v_node.tag):
198                 temp = {}
199                 for key, value in v_node.attrib.items():
200                     temp[key] = value
201                 nic[v_node.tag] = temp
202             if v_node.tag == "virtualport":
203                 temp = {}
204                 temp["type"] = v_node.get("type")
205                 for key, value in v_node.attrib.items():
206                     temp[key] = value
207                 nic["virtualport"] = temp
208         if "mac" not in nic:
209             continue
210         nics[nic["mac"]] = nic
211     return nics
212 def _get_graphics(dom):
213     out = {
214         "autoport": "None",
215         "keymap": "None",
216         "listen": "None",
217         "port": "None",
218         "type": "None",
219     }
220     doc = ElementTree.fromstring(dom.XMLDesc(0))
221     for g_node in doc.findall("devices/graphics"):
222         for key, value in g_node.attrib.items():
223             out[key] = value
224     return out
225 def _get_loader(dom):
226     out = {"path": "None"}
227     doc = ElementTree.fromstring(dom.XMLDesc(0))
228     for g_node in doc.findall("os/loader"):
229         out["path"] = g_node.text
230         for key, value in g_node.attrib.items():
231             out[key] = value
232     return out
233 def _get_disks(conn, dom):
234     disks = {}
235     doc = ElementTree.fromstring(dom.XMLDesc(0))
236     all_volumes = _get_all_volumes_paths(conn)
237     for elem in doc.findall("devices/disk"):
238         source = elem.find("source")
239         if source is None:
240             continue
241         target = elem.find("target")
242         driver = elem.find("driver")
243         if target is None:
244             continue
245         qemu_target = None
246         extra_properties = None
247         if "dev" in target.attrib:
248             disk_type = elem.get("type")
249             def _get_disk_volume_data(pool_name, volume_name):
250                 qemu_target = "{}/{}".format(pool_name, volume_name)
251                 pool = conn.storagePoolLookupByName(pool_name)
252                 extra_properties = {}
253                 try:
254                     vol = pool.storageVolLookupByName(volume_name)
255                     vol_info = vol.info()
256                     extra_properties = {
257                         "virtual size": vol_info[1],
258                         "disk size": vol_info[2],
259                     }
260                     backing_files = [
261                         {
262                             "file": node.find("source").get("file"),
263                             "file format": node.find("format").get("type"),
264                         }
265                         for node in elem.findall(".//backingStore[source]")
266                     ]
267                     if backing_files:
268                         extra_properties["backing file"] = backing_files[0]
269                         parent = extra_properties["backing file"]
270                         for sub_backing_file in backing_files[1:]:
271                             parent["backing file"] = sub_backing_file
272                             parent = sub_backing_file
273                     else:
274                         vol_desc = ElementTree.fromstring(vol.XMLDesc())
275                         backing_path = vol_desc.find("./backingStore/path")
276                         backing_format = vol_desc.find("./backingStore/format")
277                         if backing_path is not None:
278                             extra_properties["backing file"] = {
279                                 "file": backing_path.text
280                             }
281                             if backing_format is not None:
282                                 extra_properties["backing file"][
283                                     "file format"
284                                 ] = backing_format.get("type")
285                 except libvirt.libvirtError:
286                     log.info(
287                         "Couldn't extract all volume informations: pool is likely not"
288                         " running or refreshed"
289                     )
290                 return (qemu_target, extra_properties)
291             if disk_type == "file":
292                 qemu_target = source.get("file", "")
293                 if qemu_target.startswith("/dev/zvol/"):
294                     disks[target.get("dev")] = {"file": qemu_target, "zfs": True}
295                     continue
296                 if qemu_target in all_volumes.keys():
297                     volume = all_volumes[qemu_target]
298                     qemu_target, extra_properties = _get_disk_volume_data(
299                         volume["pool"], volume["name"]
300                     )
301                 elif elem.get("device", "disk") != "cdrom":
302                     try:
303                         process = subprocess.Popen(
304                             [
305                                 "qemu-img",
306                                 "info",
307                                 "-U",
308                                 "--output",
309                                 "json",
310                                 "--backing-chain",
311                                 qemu_target,
312                             ],
313                             shell=False,
314                             stdout=subprocess.PIPE,
315                             stderr=subprocess.PIPE,
316                         )
317                         stdout, stderr = process.communicate()
318                         if process.returncode == 0:
319                             qemu_output = salt.utils.stringutils.to_str(stdout)
320                             output = _parse_qemu_img_info(qemu_output)
321                             extra_properties = output
322                         else:
323                             extra_properties = {"error": stderr}
324                     except FileNotFoundError:
325                         extra_properties = {"error": "qemu-img not found"}
326             elif disk_type == "block":
327                 qemu_target = source.get("dev", "")
328                 if qemu_target in all_volumes.keys():
329                     volume = all_volumes[qemu_target]
330                     qemu_target, extra_properties = _get_disk_volume_data(
331                         volume["pool"], volume["name"]
332                     )
333             elif disk_type == "network":
334                 qemu_target = source.get("protocol")
335                 source_name = source.get("name")
336                 if source_name:
337                     qemu_target = "{}:{}".format(qemu_target, source_name)
338                 if source.get("protocol") in ["rbd", "gluster"]:
339                     for pool_i in conn.listAllStoragePools():
340                         pool_i_xml = ElementTree.fromstring(pool_i.XMLDesc())
341                         name_node = pool_i_xml.find("source/name")
342                         if name_node is not None and source_name.startswith(
343                             "{}/".format(name_node.text)
344                         ):
345                             qemu_target = "{}{}".format(
346                                 pool_i.name(), source_name[len(name_node.text) :]
347                             )
348                             break
349                 if elem.get("device", "disk") == "cdrom":
350                     host_node = source.find("host")
351                     if host_node is not None:
352                         hostname = host_node.get("name")
353                         port = host_node.get("port")
354                         qemu_target = urllib.parse.urlunparse(
355                             (
356                                 source.get("protocol"),
357                                 "{}:{}".format(hostname, port) if port else hostname,
358                                 source_name,
359                                 "",
360                                 saxutils.unescape(source.get("query", "")),
361                                 "",
362                             )
363                         )
364             elif disk_type == "volume":
365                 pool_name = source.get("pool")
366                 volume_name = source.get("volume")
367                 qemu_target, extra_properties = _get_disk_volume_data(
368                     pool_name, volume_name
369                 )
370             if not qemu_target:
371                 continue
372             disk = {
373                 "file": qemu_target,
374                 "type": elem.get("device"),
375             }
376             if driver is not None and "type" in driver.attrib:
377                 disk["file format"] = driver.get("type")
378             if extra_properties:
379                 disk.update(extra_properties)
380             disks[target.get("dev")] = disk
381     return disks
382 def _libvirt_creds():
383     g_cmd = ["grep", "^\\s*group", "/etc/libvirt/qemu.conf"]
384     u_cmd = ["grep", "^\\s*user", "/etc/libvirt/qemu.conf"]
385     try:
386         stdout = subprocess.Popen(g_cmd, stdout=subprocess.PIPE).communicate()[0]
387         group = salt.utils.stringutils.to_str(stdout).split('"')[1]
388     except IndexError:
389         group = "root"
390     try:
391         stdout = subprocess.Popen(u_cmd, stdout=subprocess.PIPE).communicate()[0]
392         user = salt.utils.stringutils.to_str(stdout).split('"')[1]
393     except IndexError:
394         user = "root"
395     return {"user": user, "group": group}
396 def _migrate(dom, dst_uri, **kwargs):
397     flags = 0
398     params = {}
399     migrated_state = libvirt.VIR_DOMAIN_RUNNING_MIGRATED
400     if kwargs.get("live", True):
401         flags |= libvirt.VIR_MIGRATE_LIVE
402     if kwargs.get("persistent", True):
403         flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
404     if kwargs.get("undefinesource", True):
405         flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
406     max_bandwidth = kwargs.get("max_bandwidth")
407     if max_bandwidth:
408         try:
409             bandwidth_value = int(max_bandwidth)
410         except ValueError:
411             raise SaltInvocationError(
412                 "Invalid max_bandwidth value: {}".format(max_bandwidth)
413             )
414         dom.migrateSetMaxSpeed(bandwidth_value)
415     max_downtime = kwargs.get("max_downtime")
416     if max_downtime:
417         try:
418             downtime_value = int(max_downtime)
419         except ValueError:
420             raise SaltInvocationError(
421                 "Invalid max_downtime value: {}".format(max_downtime)
422             )
423         dom.migrateSetMaxDowntime(downtime_value)
424     if kwargs.get("offline") is True:
425         flags |= libvirt.VIR_MIGRATE_OFFLINE
426         migrated_state = libvirt.VIR_DOMAIN_RUNNING_UNPAUSED
427     if kwargs.get("compressed") is True:
428         flags |= libvirt.VIR_MIGRATE_COMPRESSED
429     comp_methods = kwargs.get("comp_methods")
430     if comp_methods:
431         params[libvirt.VIR_MIGRATE_PARAM_COMPRESSION] = comp_methods.split(",")
432     comp_options = {
433         "comp_mt_level": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_MT_LEVEL,
434         "comp_mt_threads": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_MT_THREADS,
435         "comp_mt_dthreads": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_MT_DTHREADS,
436         "comp_xbzrle_cache": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_XBZRLE_CACHE,
437     }
438     for (comp_option, param_key) in comp_options.items():
439         comp_option_value = kwargs.get(comp_option)
440         if comp_option_value:
441             try:
442                 params[param_key] = int(comp_option_value)
443             except ValueError:
444                 raise SaltInvocationError("Invalid {} value".format(comp_option))
445     parallel_connections = kwargs.get("parallel_connections")
446     if parallel_connections:
447         try:
448             params[libvirt.VIR_MIGRATE_PARAM_PARALLEL_CONNECTIONS] = int(
449                 parallel_connections
450             )
451         except ValueError:
452             raise SaltInvocationError("Invalid parallel_connections value")
453         flags |= libvirt.VIR_MIGRATE_PARALLEL
454     if __salt__["config.get"]("virt:tunnel"):
455         if parallel_connections:
456             raise SaltInvocationError(
457                 "Parallel migration isn't compatible with tunneled migration"
458             )
459         flags |= libvirt.VIR_MIGRATE_PEER2PEER
460         flags |= libvirt.VIR_MIGRATE_TUNNELLED
461     if kwargs.get("postcopy") is True:
462         flags |= libvirt.VIR_MIGRATE_POSTCOPY
463     postcopy_bandwidth = kwargs.get("postcopy_bandwidth")
464     if postcopy_bandwidth:
465         try:
466             postcopy_bandwidth_value = int(postcopy_bandwidth)
467         except ValueError:
468             raise SaltInvocationError("Invalid postcopy_bandwidth value")
469         dom.migrateSetMaxSpeed(
470             postcopy_bandwidth_value,
471             flags=libvirt.VIR_DOMAIN_MIGRATE_MAX_SPEED_POSTCOPY,
472         )
473     copy_storage = kwargs.get("copy_storage")
474     if copy_storage:
475         if copy_storage == "all":
476             flags |= libvirt.VIR_MIGRATE_NON_SHARED_DISK
477         elif copy_storage in ["inc", "incremental"]:
478             flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
479         else:
480             raise SaltInvocationError("invalid copy_storage value")
481     try:
482         state = False
483         dst_conn = __get_conn(
484             connection=dst_uri,
485             username=kwargs.get("username"),
486             password=kwargs.get("password"),
487         )
488         new_dom = dom.migrate3(dconn=dst_conn, params=params, flags=flags)
489         if new_dom:
490             state = new_dom.state()
491         dst_conn.close()
492         return state and migrated_state in state
493     except libvirt.libvirtError as err:
494         dst_conn.close()
495         raise CommandExecutionError(err.get_error_message())
496 def _get_volume_path(pool, volume_name):
497     if volume_name in pool.listVolumes():
498         volume = pool.storageVolLookupByName(volume_name)
499         volume_xml = ElementTree.fromstring(volume.XMLDesc())
500         return volume_xml.find("./target/path").text
501     pool_xml = ElementTree.fromstring(pool.XMLDesc())
502     pool_path = pool_xml.find("./target/path").text
503     return pool_path + "/" + volume_name
504 def _disk_from_pool(conn, pool, pool_xml, volume_name):
505     pool_type = pool_xml.get("type")
506     disk_context = {}
507     if pool_type in ["dir", "netfs", "fs"]:
508         disk_context["type"] = "file"
509         disk_context["source_file"] = _get_volume_path(pool, volume_name)
510     elif pool_type in ["logical", "disk", "iscsi", "scsi"]:
511         disk_context["type"] = "block"
512         disk_context["format"] = "raw"
513         disk_context["source_file"] = _get_volume_path(pool, volume_name)
514     elif pool_type in ["rbd", "gluster", "sheepdog"]:
515         disk_context["type"] = "network"
516         disk_context["protocol"] = pool_type
517         disk_context["hosts"] = [
518             {"name": host.get("name"), "port": host.get("port")}
519             for host in pool_xml.findall(".//host")
520         ]
521         dir_node = pool_xml.find("./source/dir")
522         name_node = pool_xml.find("./source/name")
523         if name_node is not None:
524             disk_context["volume"] = "{}/{}".format(name_node.text, volume_name)
525         auth_node = pool_xml.find("./source/auth")
526         if auth_node is not None:
527             username = auth_node.get("username")
528             secret_node = auth_node.find("./secret")
529             usage = secret_node.get("usage")
530             if not usage:
531                 uuid = secret_node.get("uuid")
532                 usage = conn.secretLookupByUUIDString(uuid).usageID()
533             disk_context["auth"] = {
534                 "type": "ceph",
535                 "username": username,
536                 "usage": usage,
537             }
538     return disk_context
539 def _handle_unit(s, def_unit="m"):
540     m = re.match(r"(?P&lt;value&gt;[0-9.]*)\s*(?P&lt;unit&gt;.*)$", str(s).strip())
541     value = m.group("value")
542     unit = m.group("unit").lower() or def_unit
543     try:
544         value = int(value)
545     except ValueError:
546         try:
547             value = float(value)
548         except ValueError:
549             raise SaltInvocationError("invalid number")
550     dec = False
551     if re.match(r"[kmgtpezy]b$", unit):
552         dec = True
553     elif not re.match(r"(b|[kmgtpezy](ib)?)$", unit):
554         raise SaltInvocationError("invalid units")
555     p = "bkmgtpezy".index(unit[0])
556     value *= 10 ** (p * 3) if dec else 2 ** (p * 10)
557     return int(value)
558 def nesthash(value=None):
559     return collections.defaultdict(nesthash, value or {})
560 def _gen_xml(
561     conn,
562     name,
563     cpu,
564     mem,
565     diskp,
566     nicp,
567     hypervisor,
568     os_type,
569     arch,
570     graphics=None,
571     boot=None,
572     boot_dev=None,
573     numatune=None,
574     hypervisor_features=None,
575     clock=None,
576     serials=None,
577     consoles=None,
578     stop_on_reboot=False,
579     host_devices=None,
580     **kwargs
581 ):
582     context = {
583         "hypervisor": hypervisor,
584         "name": name,
585         "hypervisor_features": hypervisor_features or {},
586         "clock": clock or {},
587         "on_reboot": "destroy" if stop_on_reboot else "restart",
588     }
589     context["to_kib"] = lambda v: int(_handle_unit(v) / 1024)
590     context["yesno"] = lambda v: "yes" if v else "no"
591     context["mem"] = nesthash()
592     if isinstance(mem, int):
593         context["mem"]["boot"] = mem
594         context["mem"]["current"] = mem
595     elif isinstance(mem, dict):
596         context["mem"] = nesthash(mem)
597     context["cpu"] = nesthash()
598     context["cputune"] = nesthash()
599     if isinstance(cpu, int):
600         context["cpu"]["maximum"] = str(cpu)
601     elif isinstance(cpu, dict):
602         context["cpu"] = nesthash(cpu)
603     if clock:
604         offset = "utc" if clock.get("utc", True) else "localtime"
605         if "timezone" in clock:
606             offset = "timezone"
607         context["clock"]["offset"] = offset
608     if hypervisor in ["qemu", "kvm"]:
609         context["numatune"] = numatune if numatune else {}
610         context["controller_model"] = False
611     elif hypervisor == "vmware":
612         context["controller_model"] = "lsilogic"
613     if graphics:
614         if "listen" not in graphics:
615             graphics["listen"] = {"type": "address", "address": "0.0.0.0"}
616         elif (
617             "address" not in graphics["listen"]
618             and graphics["listen"]["type"] == "address"
619         ):
620             graphics["listen"]["address"] = "0.0.0.0"
621         if graphics.get("type", "none") == "none":
622             graphics = None
623     context["graphics"] = graphics
624     context["boot_dev"] = boot_dev.split() if boot_dev is not None else ["hd"]
625     context["boot"] = boot if boot else {}
626     efi_value = context["boot"].get("efi", None) if boot else None
627     if efi_value is True:
628         context["boot"]["os_attrib"] = "firmware='efi'"
629     elif efi_value is not None and type(efi_value) != bool:
630         raise SaltInvocationError("Invalid efi value")
631     if os_type == "xen":
632         if __grains__["os_family"] == "Suse":
633             if not boot or not boot.get("kernel", None):
634                 paths = [
635                     path
636                     for path in ["/usr/share", "/usr/lib"]
637                     if os.path.exists(path + "/grub2/x86_64-xen/grub.xen")
638                 ]
639                 if not paths:
640                     raise CommandExecutionError("grub-x86_64-xen needs to be installed")
641                 context["boot"]["kernel"] = paths[0] + "/grub2/x86_64-xen/grub.xen"
642                 context["boot_dev"] = []
643     default_port = 23023
644     default_chardev_type = "tcp"
645     chardev_types = ["serial", "console"]
646     for chardev_type in chardev_types:
647         context[chardev_type + "s"] = []
648         parameter_value = locals()[chardev_type + "s"]
649         if parameter_value is not None:
650             for chardev in parameter_value:
651                 chardev_context = chardev
652                 chardev_context["type"] = chardev.get("type", default_chardev_type)
653                 if chardev_context["type"] == "tcp":
654                     chardev_context["port"] = chardev.get("port", default_port)
655                     chardev_context["protocol"] = chardev.get("protocol", "telnet")
656                 context[chardev_type + "s"].append(chardev_context)
657     context["disks"] = []
658     disk_bus_map = {"virtio": "vd", "xen": "xvd", "fdc": "fd", "ide": "hd"}
659     targets = []
660     for i, disk in enumerate(diskp):
661         prefix = disk_bus_map.get(disk["model"], "sd")
662         disk_context = {
663             "device": disk.get("device", "disk"),
664             "target_dev": _get_disk_target(targets, len(diskp), prefix),
665             "disk_bus": disk["model"],
666             "format": disk.get("format", "raw"),
667             "index": str(i),
668             "io": disk.get("io", "native"),
669             "iothread": disk.get("iothread_id", None),
670         }
671         targets.append(disk_context["target_dev"])
672         if disk.get("source_file"):
673             url = urllib.parse.urlparse(disk["source_file"])
674             if not url.scheme or not url.hostname:
675                 disk_context["source_file"] = disk["source_file"]
676                 disk_context["type"] = "file"
677             elif url.scheme in ["http", "https", "ftp", "ftps", "tftp"]:
678                 disk_context["type"] = "network"
679                 disk_context["protocol"] = url.scheme
680                 disk_context["volume"] = url.path
681                 disk_context["query"] = saxutils.escape(url.query)
682                 disk_context["hosts"] = [{"name": url.hostname, "port": url.port}]
683         elif disk.get("pool"):
684             disk_context["volume"] = disk["filename"]
685             pool = conn.storagePoolLookupByName(disk["pool"])
686             pool_xml = ElementTree.fromstring(pool.XMLDesc())
687             pool_type = pool_xml.get("type")
688             if hypervisor == "xen" or pool_type in ["rbd", "gluster", "sheepdog"]:
689                 disk_context.update(
690                     _disk_from_pool(conn, pool, pool_xml, disk_context["volume"])
691                 )
692             else:
693                 if pool_type in ["disk", "logical"]:
694                     disk_context["format"] = "raw"
695                 disk_context["type"] = "volume"
696                 disk_context["pool"] = disk["pool"]
697         else:
698             disk_context["type"] = "file"
699         if hypervisor in ["qemu", "kvm", "bhyve", "xen"]:
700             disk_context["address"] = False
701             disk_context["driver"] = True
702         elif hypervisor in ["esxi", "vmware"]:
703             disk_context["address"] = True
704             disk_context["driver"] = False
705         context["disks"].append(disk_context)
706     context["nics"] = nicp
707     hostdev_context = []
708     try:
709         for hostdev_name in host_devices or []:
710             hostdevice = conn.nodeDeviceLookupByName(hostdev_name)
711             doc = ElementTree.fromstring(hostdevice.XMLDesc())
712             if "pci" in hostdevice.listCaps():
713                 hostdev_context.append(
714                     {
715                         "type": "pci",
716                         "domain": "0x{:04x}".format(
717                             int(doc.find("./capability[@type='pci']/domain").text)
718                         ),
719                         "bus": "0x{:02x}".format(
720                             int(doc.find("./capability[@type='pci']/bus").text)
721                         ),
722                         "slot": "0x{:02x}".format(
723                             int(doc.find("./capability[@type='pci']/slot").text)
724                         ),
725                         "function": "0x{}".format(
726                             doc.find("./capability[@type='pci']/function").text
727                         ),
728                     }
729                 )
730             elif "usb_device" in hostdevice.listCaps():
731                 vendor_id = doc.find(".//vendor").get("id")
732                 product_id = doc.find(".//product").get("id")
733                 hostdev_context.append(
734                     {"type": "usb", "vendor": vendor_id, "product": product_id}
735                 )
736     except libvirt.libvirtError as err:
737         conn.close()
738         raise CommandExecutionError(
739             "Failed to get host devices: " + err.get_error_message()
740         )
741     context["hostdevs"] = hostdev_context
742     context["os_type"] = os_type
743     context["arch"] = arch
744     fn_ = "libvirt_domain.jinja"
745     try:
746         template = JINJA.get_template(fn_)
747     except jinja2.exceptions.TemplateNotFound:
748         log.error("Could not load template %s", fn_)
749         return ""
750     return template.render(**context)
751 def _gen_vol_xml(
752     name,
753     size,
754     format=None,
755     allocation=0,
756     type=None,
757     permissions=None,
758     backing_store=None,
759     nocow=False,
760 ):
761     size = int(size) * 1024  # MB
762     context = {
763         "type": type,
764         "name": name,
765         "target": {"permissions": permissions, "nocow": nocow},
766         "format": format,
767         "size": str(size),
768         "allocation": str(int(allocation) * 1024),
769         "backingStore": backing_store,
770     }
771     fn_ = "libvirt_volume.jinja"
772     try:
773         template = JINJA.get_template(fn_)
774     except jinja2.exceptions.TemplateNotFound:
775         log.error("Could not load template %s", fn_)
776         return ""
777     return template.render(**context)
778 def _gen_net_xml(
779     name,
780     bridge,
781     forward,
782     vport,
783     tag=None,
784     ip_configs=None,
785     mtu=None,
786     domain=None,
787     nat=None,
788     interfaces=None,
789     addresses=None,
790     physical_function=None,
791     dns=None,
792 ):
793     if isinstance(vport, str):
794         vport_context = {"type": vport}
795     else:
796         vport_context = vport
797     if isinstance(tag, (str, int)):
798         tag_context = {"tags": [{"id": tag}]}
799     else:
800         tag_context = tag
801     addresses_context = []
802     if addresses:
803         matches = [
804             re.fullmatch(r"([0-9]+):([0-9A-Fa-f]+):([0-9A-Fa-f]+)\.([0-9])", addr)
805             for addr in addresses.lower().split(" ")
806         ]
807         addresses_context = [
808             {
809                 "domain": m.group(1),
810                 "bus": m.group(2),
811                 "slot": m.group(3),
812                 "function": m.group(4),
813             }
814             for m in matches
815             if m
816         ]
817     context = {
818         "name": name,
819         "bridge": bridge,
820         "mtu": mtu,
821         "domain": domain,
822         "forward": forward,
823         "nat": nat,
824         "interfaces": interfaces.split(" ") if interfaces else [],
825         "addresses": addresses_context,
826         "pf": physical_function,
827         "vport": vport_context,
828         "vlan": tag_context,
829         "dns": dns,
830         "ip_configs": [
831             {
832                 "address": ipaddress.ip_network(config["cidr"]),
833                 "dhcp_ranges": config.get("dhcp_ranges", []),
834                 "hosts": config.get("hosts", {}),
835                 "bootp": config.get("bootp", {}),
836                 "tftp": config.get("tftp"),
837             }
838             for config in ip_configs or []
839         ],
840         "yesno": lambda v: "yes" if v else "no",
841     }
842     fn_ = "libvirt_network.jinja"
843     try:
844         template = JINJA.get_template(fn_)
845     except jinja2.exceptions.TemplateNotFound:
846         log.error("Could not load template %s", fn_)
847         return ""
848     return template.render(**context)
849 def _gen_pool_xml(
850     name,
851     ptype,
852     target=None,
853     permissions=None,
854     source_devices=None,
855     source_dir=None,
856     source_adapter=None,
857     source_hosts=None,
858     source_auth=None,
859     source_name=None,
860     source_format=None,
861     source_initiator=None,
862 ):
863     hosts = [host.split(":") for host in source_hosts or []]
864     source = None
865     if any(
866         [
867             source_devices,
868             source_dir,
869             source_adapter,
870             hosts,
871             source_auth,
872             source_name,
873             source_format,
874             source_initiator,
875         ]
876     ):
877         source = {
878             "devices": source_devices or [],
879             "dir": source_dir
880             if source_format != "cifs" or not source_dir
881             else source_dir.lstrip("/"),
882             "adapter": source_adapter,
883             "hosts": [
884                 {"name": host[0], "port": host[1] if len(host) &gt; 1 else None}
885                 for host in hosts
886             ],
887             "auth": source_auth,
888             "name": source_name,
889             "format": source_format,
890             "initiator": source_initiator,
891         }
892     context = {
893         "name": name,
894         "ptype": ptype,
895         "target": {"path": target, "permissions": permissions},
896         "source": source,
897     }
898     fn_ = "libvirt_pool.jinja"
899     try:
900         template = JINJA.get_template(fn_)
901     except jinja2.exceptions.TemplateNotFound:
902         log.error("Could not load template %s", fn_)
903         return ""
904     return template.render(**context)
905 def _gen_secret_xml(auth_type, usage, description):
906     context = {
907         "type": auth_type,
908         "usage": usage,
909         "description": description,
910     }
911     fn_ = "libvirt_secret.jinja"
912     try:
913         template = JINJA.get_template(fn_)
914     except jinja2.exceptions.TemplateNotFound:
915         log.error("Could not load template %s", fn_)
916         return ""
917     return template.render(**context)
918 def _get_images_dir():
919     img_dir = __salt__["config.get"]("virt:images")
920     log.debug("Image directory from config option `virt:images` is %s", img_dir)
921     return img_dir
922 def _zfs_image_create(
923     vm_name,
924     pool,
925     disk_name,
926     hostname_property_name,
927     sparse_volume,
928     disk_size,
929     disk_image_name,
930 ):
931     if not disk_image_name and not disk_size:
932         raise CommandExecutionError(
933             "Unable to create new disk {}, please specify"
934             " the disk image name or disk size argument".format(disk_name)
935         )
936     if not pool:
937         raise CommandExecutionError(
938             "Unable to create new disk {}, please specify the disk pool name".format(
939                 disk_name
940             )
941         )
942     destination_fs = os.path.join(pool, "{}.{}".format(vm_name, disk_name))
943     log.debug("Image destination will be %s", destination_fs)
944     existing_disk = __salt__["zfs.list"](name=pool)
945     if "error" in existing_disk:
946         raise CommandExecutionError(
947             "Unable to create new disk {}. {}".format(
948                 destination_fs, existing_disk["error"]
949             )
950         )
951     elif destination_fs in existing_disk:
952         log.info("ZFS filesystem %s already exists. Skipping creation", destination_fs)
953         blockdevice_path = os.path.join("/dev/zvol", pool, vm_name)
954         return blockdevice_path
955     properties = {}
956     if hostname_property_name:
957         properties[hostname_property_name] = vm_name
958     if disk_image_name:
959         __salt__["zfs.clone"](
960             name_a=disk_image_name, name_b=destination_fs, properties=properties
961         )
962     elif disk_size:
963         __salt__["zfs.create"](
964             name=destination_fs,
965             properties=properties,
966             volume_size=disk_size,
967             sparse=sparse_volume,
968         )
969     blockdevice_path = os.path.join(
970         "/dev/zvol", pool, "{}.{}".format(vm_name, disk_name)
971     )
972     log.debug("Image path will be %s", blockdevice_path)
973     return blockdevice_path
974 def _qemu_image_create(disk, create_overlay=False, saltenv="base"):
975     disk_size = disk.get("size", None)
976     disk_image = disk.get("image", None)
977     if not disk_size and not disk_image:
978         raise CommandExecutionError(
979             "Unable to create new disk {}, please specify"
980             " disk size and/or disk image argument".format(disk["filename"])
981         )
982     img_dest = disk["source_file"]
983     log.debug("Image destination will be %s", img_dest)
984     img_dir = os.path.dirname(img_dest)
985     log.debug("Image destination directory is %s", img_dir)
986     if not os.path.exists(img_dir):
987         os.makedirs(img_dir)
988     if disk_image:
989         log.debug("Create disk from specified image %s", disk_image)
990         sfn = __salt__["cp.cache_file"](disk_image, saltenv)
991         qcow2 = False
992         if salt.utils.path.which("qemu-img"):
993             res = __salt__["cmd.run"]('qemu-img info "{}"'.format(sfn))
994             imageinfo = salt.utils.yaml.safe_load(res)
995             qcow2 = imageinfo["file format"] == "qcow2"
996 <a name="6"></a>        try:
997             if create_overlay and qcow2:
998                 log.info("Cloning qcow2 image %s using copy on write", sfn)
999                 __salt__<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>["cmd.run"](
1000                     'qemu-img create -f qcow2 -o backing_file="{}" "{}"'.format(
1001                         sfn, img_dest
1002                     ).split()
1003                 )
1004             else:
1005                 log.debug("Copying %s to %s", sfn, img_dest)
1006                 salt.utils.files.copyfile(</b></font>sfn, img_dest)
1007             mask = salt.utils.files.get_umask()
1008             if disk_size and qcow2:
1009                 log.debug("Resize qcow2 image to %sM", disk_size)
1010                 __salt__["cmd.run"](
1011                     'qemu-img resize "{}" {}M'.format(img_dest, disk_size)
1012                 )
1013             log.debug("Apply umask and remove exec bit")
1014             mode = (0o0777 ^ mask) &amp; 0o0666
1015             os.chmod(img_dest, mode)
1016         except OSError as err:
1017             raise CommandExecutionError(
1018                 "Problem while copying image. {} - {}".format(disk_image, err)
1019             )
1020     else:
1021         try:
1022             mask = salt.utils.files.get_umask()
1023             if disk_size:
1024                 log.debug("Create empty image with size %sM", disk_size)
1025                 __salt__["cmd.run"](
1026                     'qemu-img create -f {} "{}" {}M'.format(
1027                         disk.get("format", "qcow2"), img_dest, disk_size
1028                     )
1029                 )
1030             else:
1031                 raise CommandExecutionError(
1032                     "Unable to create new disk {},"
1033                     " please specify &lt;size&gt; argument".format(img_dest)
1034                 )
1035             log.debug("Apply umask and remove exec bit")
1036             mode = (0o0777 ^ mask) &amp; 0o0666
1037             os.chmod(img_dest, mode)
1038         except OSError as err:
1039             raise CommandExecutionError(
1040                 "Problem while creating volume {} - {}".format(img_dest, err)
1041             )
1042     return img_dest
1043 def _seed_image(seed_cmd, img_path, name, config, install, pub_key, priv_key):
1044     log.debug("Seeding image")
1045     __salt__[seed_cmd](
1046         img_path,
1047         id_=name,
1048         config=config,
1049         install=install,
1050         pub_key=pub_key,
1051         priv_key=priv_key,
1052     )
1053 def _disk_volume_create(conn, disk, seeder=None, saltenv="base"):
1054     if disk.get("overlay_image"):
1055         raise SaltInvocationError(
1056             "Disk overlay_image property is not supported when creating volumes,"
1057             "use backing_store_path and backing_store_format instead."
1058         )
1059     pool = conn.storagePoolLookupByName(disk["pool"])
1060     if disk["filename"] in pool.listVolumes():
1061         return
1062     pool_type = ElementTree.fromstring(pool.XMLDesc()).get("type")
1063     backing_path = disk.get("backing_store_path")
1064     backing_format = disk.get("backing_store_format")
1065     backing_store = None
1066     if (
1067         backing_path
1068         and backing_format
1069         and (disk.get("format") == "qcow2" or pool_type == "logical")
1070     ):
1071         backing_store = {"path": backing_path, "format": backing_format}
1072     if backing_store and disk.get("image"):
1073         raise SaltInvocationError(
1074             "Using a template image with a backing store is not possible, "
1075             "choose either of them."
1076         )
1077     vol_xml = _gen_vol_xml(
1078         disk["filename"],
1079         disk.get("size", 0),
1080         format=disk.get("format"),
1081         backing_store=backing_store,
1082     )
1083     _define_vol_xml_str(conn, vol_xml, disk.get("pool"))
1084     if disk.get("image"):
1085         log.debug("Caching disk template image: %s", disk.get("image"))
1086         cached_path = __salt__["cp.cache_file"](disk.get("image"), saltenv)
1087         if seeder:
1088             seeder(cached_path)
1089         _volume_upload(
1090             conn,
1091             disk["pool"],
1092             disk["filename"],
1093             cached_path,
1094             sparse=disk.get("format") == "qcow2",
1095         )
1096 def _disk_profile(conn, profile, hypervisor, disks, vm_name):
1097     default = [{"system": {"size": 8192}}]
1098     if hypervisor == "vmware":
1099         overlay = {"format": "vmdk", "model": "scsi", "device": "disk"}
1100     elif hypervisor in ["qemu", "kvm"]:
1101         overlay = {"device": "disk", "model": "virtio"}
1102     elif hypervisor == "xen":
1103         overlay = {"device": "disk", "model": "xen"}
1104     elif hypervisor == "bhyve":
1105         overlay = {"format": "raw", "model": "virtio", "sparse_volume": False}
1106     else:
1107         overlay = {}
1108     disklist = []
1109     if profile:
1110         disklist = copy.deepcopy(
1111             __salt__["config.get"]("virt:disk", {}).get(profile, default)
1112         )
1113         disklist = [dict(d, name=name) for disk in disklist for name, d in disk.items()]
1114     if disks:
1115         for udisk in disks:
1116             if "name" in udisk:
1117                 found = [disk for disk in disklist if udisk["name"] == disk["name"]]
1118                 if found:
1119                     found[0].update(udisk)
1120                 else:
1121                     disklist.append(udisk)
1122     pool_caps = _pool_capabilities(conn)
1123     for disk in disklist:
1124         if disk.get("device", "disk") == "cdrom" and "model" not in disk:
1125             disk["model"] = "ide"
1126         for key, val in overlay.items():
1127             if key not in disk:
1128                 disk[key] = val
1129         if disk.get("source_file") and os.path.exists(disk["source_file"]):
1130             disk["filename"] = os.path.basename(disk["source_file"])
1131             if not disk.get("format"):
1132                 disk["format"] = (
1133                     "qcow2" if disk.get("device", "disk") != "cdrom" else "raw"
1134                 )
1135         elif vm_name and disk.get("device", "disk") == "disk":
1136             _fill_disk_filename(conn, vm_name, disk, hypervisor, pool_caps)
1137     return disklist
1138 def _fill_disk_filename(conn, vm_name, disk, hypervisor, pool_caps):
1139     disk["filename"] = "{}_{}".format(vm_name, disk["name"])
1140     base_dir = disk.get("pool", None)
1141     if hypervisor in ["qemu", "kvm", "xen"]:
1142         if not base_dir:
1143             base_dir = _get_images_dir()
1144         if base_dir not in conn.listStoragePools():
1145             if not disk.get("format"):
1146                 disk["format"] = "qcow2"
1147             disk["filename"] = "{}.{}".format(disk["filename"], disk["format"])
1148             disk["source_file"] = os.path.join(base_dir, disk["filename"])
1149         else:
1150             if "pool" not in disk:
1151                 disk["pool"] = base_dir
1152             pool_obj = conn.storagePoolLookupByName(base_dir)
1153             pool_xml = ElementTree.fromstring(pool_obj.XMLDesc())
1154             pool_type = pool_xml.get("type")
1155             if pool_type == "disk":
1156                 device = pool_xml.find("./source/device").get("path")
1157                 all_volumes = pool_obj.listVolumes()
1158                 if disk.get("source_file") not in all_volumes:
1159                     indexes = [
1160                         int(re.sub("[a-z]+", "", vol_name)) for vol_name in all_volumes
1161                     ] or [0]
1162                     index = min(
1163                         idx for idx in range(1, max(indexes) + 2) if idx not in indexes
1164                     )
1165                     disk["filename"] = "{}{}".format(os.path.basename(device), index)
1166             if disk.get("source_file"):
1167                 if not disk.get("source_file") in pool_obj.listVolumes():
1168                     raise SaltInvocationError(
1169                         "{} volume doesn't exist in pool {}".format(
1170                             disk.get("source_file"), base_dir
1171                         )
1172                     )
1173                 disk["filename"] = disk["source_file"]
1174                 del disk["source_file"]
1175             if not disk.get("format"):
1176                 volume_options = (
1177                     [
1178                         type_caps.get("options", {}).get("volume", {})
1179                         for type_caps in pool_caps.get("pool_types")
1180                         if type_caps["name"] == pool_type
1181                     ]
1182                     or [{}]
1183                 )[0]
1184                 if "qcow2" in volume_options.get("targetFormatType", []):
1185                     disk["format"] = "qcow2"
1186                 else:
1187                     disk["format"] = volume_options.get("default_format", None)
1188     elif hypervisor == "bhyve" and vm_name:
1189         disk["filename"] = "{}.{}".format(vm_name, disk["name"])
1190         disk["source_file"] = os.path.join(
1191             "/dev/zvol", base_dir or "", disk["filename"]
1192         )
1193     elif hypervisor in ["esxi", "vmware"]:
1194         if not base_dir:
1195             base_dir = __salt__["config.get"]("virt:storagepool", "[0] ")
1196         disk["filename"] = "{}.{}".format(disk["filename"], disk["format"])
1197         disk["source_file"] = "{}{}".format(base_dir, disk["filename"])
1198 def _complete_nics(interfaces, hypervisor):
1199     vmware_overlay = {"type": "bridge", "source": "DEFAULT", "model": "e1000"}
1200     kvm_overlay = {"type": "bridge", "source": "br0", "model": "virtio"}
1201     xen_overlay = {"type": "bridge", "source": "br0", "model": None}
1202     bhyve_overlay = {"type": "bridge", "source": "bridge0", "model": "virtio"}
1203     overlays = {
1204         "xen": xen_overlay,
1205         "kvm": kvm_overlay,
1206         "qemu": kvm_overlay,
1207         "vmware": vmware_overlay,
1208         "bhyve": bhyve_overlay,
1209     }
1210     def _normalize_net_types(attributes):
1211         for type_ in ["bridge", "network"]:
1212             if type_ in attributes:
1213                 attributes["type"] = type_
1214                 attributes["source"] = attributes.pop(type_)
1215         attributes["type"] = attributes.get("type", None)
1216         attributes["source"] = attributes.get("source", None)
1217     def _apply_default_overlay(attributes):
1218         for key, value in overlays[hypervisor].items():
1219             if key not in attributes or not attributes[key]:
1220                 attributes[key] = value
1221     for interface in interfaces:
1222         _normalize_net_types(interface)
1223         if hypervisor in overlays:
1224             _apply_default_overlay(interface)
1225     return interfaces
1226 def _nic_profile(profile_name, hypervisor):
1227     config_data = __salt__["config.get"]("virt:nic", {}).get(
1228         profile_name, [{"eth0": {}}]
1229     )
1230     interfaces = []
1231     def append_dict_profile_to_interface_list(profile_dict):
1232         for interface_name, attributes in profile_dict.items():
1233             attributes["name"] = interface_name
1234             interfaces.append(attributes)
1235     if isinstance(config_data, dict):
1236         append_dict_profile_to_interface_list(config_data)
1237     elif isinstance(config_data, list):
1238         for interface in config_data:
1239             if isinstance(interface, dict):
1240                 if len(interface) == 1:
1241                     append_dict_profile_to_interface_list(interface)
1242                 else:
1243                     interfaces.append(interface)
1244     return _complete_nics(interfaces, hypervisor)
1245 def _get_merged_nics(hypervisor, profile, interfaces=None):
1246     nicp = _nic_profile(profile, hypervisor) if profile else []
1247     log.debug("NIC profile is %s", nicp)
1248     if interfaces:
1249         users_nics = _complete_nics(interfaces, hypervisor)
1250         for unic in users_nics:
1251             found = [nic for nic in nicp if nic["name"] == unic["name"]]
1252             if found:
1253                 found[0].update(unic)
1254             else:
1255                 nicp.append(unic)
1256         log.debug("Merged NICs: %s", nicp)
1257     return nicp
1258 def _handle_remote_boot_params(orig_boot):
1259     saltinst_dir = None
1260     new_boot = orig_boot.copy()
1261     keys = orig_boot.keys()
1262     cases = [
1263         {"efi"},
1264         {"kernel", "initrd", "efi"},
1265         {"kernel", "initrd", "cmdline", "efi"},
1266         {"loader", "nvram"},
1267         {"kernel", "initrd"},
1268         {"kernel", "initrd", "cmdline"},
1269         {"kernel", "initrd", "loader", "nvram"},
1270         {"kernel", "initrd", "cmdline", "loader", "nvram"},
1271     ]
1272     if keys in cases:
1273         for key in keys:
1274             if key == "efi" and type(orig_boot.get(key)) == bool:
1275                 new_boot[key] = orig_boot.get(key)
1276             elif orig_boot.get(key) is not None and salt.utils.virt.check_remote(
1277                 orig_boot.get(key)
1278             ):
1279                 if saltinst_dir is None:
1280                     os.makedirs(CACHE_DIR)
1281                     saltinst_dir = CACHE_DIR
1282                 new_boot[key] = salt.utils.virt.download_remote(
1283                     orig_boot.get(key), saltinst_dir
1284                 )
1285         return new_boot
1286     else:
1287         raise SaltInvocationError(
1288             "Invalid boot parameters,It has to follow this combination: [(kernel,"
1289             " initrd) or/and cmdline] or/and [(loader, nvram) or efi]"
1290         )
1291 def _handle_efi_param(boot, desc):
1292     efi_value = boot.get("efi", None) if boot else None
1293     parent_tag = desc.find("os")
1294     os_attrib = parent_tag.attrib
1295     if efi_value is False and os_attrib != {}:
1296         parent_tag.attrib.pop("firmware", None)
1297         return True
1298     elif type(efi_value) == bool and os_attrib == {}:
1299         if efi_value is True and parent_tag.find("loader") is None:
1300             parent_tag.set("firmware", "efi")
1301             return True
1302         if efi_value is False and parent_tag.find("loader") is not None:
1303             parent_tag.remove(parent_tag.find("loader"))
1304             parent_tag.remove(parent_tag.find("nvram"))
1305             return True
1306     elif type(efi_value) != bool:
1307         raise SaltInvocationError("Invalid efi value")
1308     return False
1309 def init(
1310     name,
1311     cpu,
1312     mem,
1313     nic="default",
1314     interfaces=None,
1315     hypervisor=None,
1316     start=True,  # pylint: disable=redefined-outer-name
1317     disk="default",
1318     disks=None,
1319     saltenv="base",
1320     seed=True,
1321     install=True,
1322     pub_key=None,
1323     priv_key=None,
1324     seed_cmd="seed.apply",
1325     graphics=None,
1326     os_type=None,
1327     arch=None,
1328     boot=None,
1329     boot_dev=None,
1330     numatune=None,
1331     hypervisor_features=None,
1332     clock=None,
1333     serials=None,
1334     consoles=None,
1335     stop_on_reboot=False,
1336     host_devices=None,
1337     **kwargs
1338 ):
1339     try:
1340         conn = __get_conn(**kwargs)
1341         caps = _capabilities(conn)
1342         os_types = sorted({guest["os_type"] for guest in caps["guests"]})
1343         arches = sorted({guest["arch"]["name"] for guest in caps["guests"]})
1344         virt_hypervisor = hypervisor
1345         if not virt_hypervisor:
1346             hypervisors = sorted(
1347                 {
1348                     x
1349                     for y in [
1350                         guest["arch"]["domains"].keys() for guest in caps["guests"]
1351                     ]
1352                     for x in y
1353                 }
1354             )
1355             if len(hypervisors) == 0:
1356                 raise SaltInvocationError("No supported hypervisors were found")
1357             virt_hypervisor = "kvm" if "kvm" in hypervisors else hypervisors[0]
1358         virt_hypervisor = "vmware" if virt_hypervisor == "esxi" else virt_hypervisor
1359         log.debug("Using hypervisor %s", virt_hypervisor)
1360         nicp = _get_merged_nics(virt_hypervisor, nic, interfaces)
1361         diskp = _disk_profile(conn, disk, virt_hypervisor, disks, name)
1362         for _disk in diskp:
1363             if _disk.get("device", "disk") == "cdrom":
1364                 continue
1365             log.debug("Creating disk for VM [ %s ]: %s", name, _disk)
1366             if virt_hypervisor == "vmware":
1367                 if "image" in _disk:
1368                     raise SaltInvocationError(
1369                         "virt.init does not support image "
1370                         "template in conjunction with esxi hypervisor"
1371                     )
1372                 else:
1373                     log.debug("Generating libvirt XML for %s", _disk)
1374                     volume_name = "{}/{}".format(name, _disk["name"])
1375                     filename = "{}.{}".format(volume_name, _disk["format"])
1376                     vol_xml = _gen_vol_xml(
1377                         filename, _disk["size"], format=_disk["format"]
1378                     )
1379                     _define_vol_xml_str(conn, vol_xml, pool=_disk.get("pool"))
1380             elif virt_hypervisor in ["qemu", "kvm", "xen"]:
1381                 def seeder(path):
1382                     _seed_image(
1383                         seed_cmd,
1384                         path,
1385                         name,
1386                         kwargs.get("config"),
1387                         install,
1388                         pub_key,
1389                         priv_key,
1390                     )
1391                 create_overlay = _disk.get("overlay_image", False)
1392                 format = _disk.get("format")
1393                 if _disk.get("source_file"):
1394                     if os.path.exists(_disk["source_file"]):
1395                         img_dest = _disk["source_file"]
1396                     else:
1397                         img_dest = _qemu_image_create(_disk, create_overlay, saltenv)
1398                 else:
1399                     _disk_volume_create(conn, _disk, seeder if seed else None, saltenv)
1400                     img_dest = None
1401                 if seed and img_dest and _disk.get("image", None):
1402                     seeder(img_dest)
1403             elif hypervisor in ["bhyve"]:
1404                 img_dest = _zfs_image_create(
1405                     vm_name=name,
1406                     pool=_disk.get("pool"),
1407                     disk_name=_disk.get("name"),
1408                     disk_size=_disk.get("size"),
1409                     disk_image_name=_disk.get("image"),
1410                     hostname_property_name=_disk.get("hostname_property"),
1411                     sparse_volume=_disk.get("sparse_volume"),
1412                 )
1413             else:
1414                 raise SaltInvocationError(
1415                     "Unsupported hypervisor when handling disk image: {}".format(
1416                         virt_hypervisor
1417                     )
1418                 )
1419         log.debug("Generating VM XML")
1420         if os_type is None:
1421             os_type = "hvm" if "hvm" in os_types else os_types[0]
1422         if arch is None:
1423             arch = "x86_64" if "x86_64" in arches else arches[0]
1424         if boot is not None:
1425             boot = _handle_remote_boot_params(boot)
1426         vm_xml = _gen_xml(
1427             conn,
1428             name,
1429             cpu,
1430             mem,
1431             diskp,
1432             nicp,
1433             virt_hypervisor,
1434             os_type,
1435             arch,
1436             graphics,
1437             boot,
1438             boot_dev,
1439             numatune,
1440             hypervisor_features,
1441             clock,
1442             serials,
1443             consoles,
1444             stop_on_reboot,
1445             host_devices,
1446             **kwargs
1447         )
1448         log.debug("New virtual machine definition: %s", vm_xml)
1449         conn.defineXML(vm_xml)
1450     except libvirt.libvirtError as err:
1451         conn.close()
1452         raise CommandExecutionError(err.get_error_message())
1453     if start:
1454         log.debug("Starting VM %s", name)
1455         _get_domain(conn, name).create()
1456     conn.close()
1457     return True
1458 def _disks_equal(disk1, disk2):
1459 <a name="1"></a>    """
1460     Test if two disk elements should be considered like the same device
1461     target1 = disk1<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.find("target")
1462     target2 = disk2.find("target")
1463     disk1_dict = xmlutil.to_dict(disk1, True)
1464     disk2_dict = xmlutil.to_dict(disk2, True)
1465     source1_dict = disk1_dict.get("source", {})
1466     source2_dict = disk2_dict.get("source", {})
1467     io1 = disk1_dict.get("driver", {}).</b></font>get("io", "native")
1468     io2 = disk2_dict.get("driver", {}).get("io", "native")
1469     if source1_dict:
1470         source1_dict.pop("index", None)
1471     if source2_dict:
1472         source2_dict.pop("index", None)
1473     return (
1474         source1_dict == source2_dict
1475         and target1 is not None
1476         and target2 is not None
1477         and target1.get("bus") == target2.get("bus")
1478         and disk1.get("device", "disk") == disk2.get("device", "disk")
1479         and target1.get("dev") == target2.get("dev")
1480         and io1 == io2
1481     )
1482 def _nics_equal(nic1, nic2):
1483     def _filter_nic(nic):
1484         source_node = nic.find("source")
1485         source_attrib = source_node.attrib if source_node is not None else {}
1486         source_type = "network" if "network" in source_attrib else nic.attrib["type"]
1487         source_getters = {
1488             "network": lambda n: n.get("network"),
1489             "bridge": lambda n: n.get("bridge"),
1490             "direct": lambda n: n.get("dev"),
1491             "hostdev": lambda n: _format_pci_address(n.find("address")),
1492         }
1493         return {
1494             "type": source_type,
1495             "source": source_getters[source_type](source_node)
1496             if source_node is not None
1497             else None,
1498             "model": nic.find("model").attrib["type"]
1499             if nic.find("model") is not None
1500             else None,
1501         }
1502     def _get_mac(nic):
1503         return (
1504             nic.find("mac").attrib["address"].lower()
1505             if nic.find("mac") is not None
1506             else None
1507         )
1508     mac1 = _get_mac(nic1)
1509     mac2 = _get_mac(nic2)
1510     macs_equal = not mac1 or not mac2 or mac1 == mac2
1511     return _filter_nic(nic1) == _filter_nic(nic2) and macs_equal
1512 def _graphics_equal(gfx1, gfx2):
1513     def _filter_graphics(gfx):
1514         gfx_copy = copy.deepcopy(gfx)
1515         defaults = [
1516             {"node": ".", "attrib": "port", "values": ["5900", "-1"]},
1517             {"node": ".", "attrib": "address", "values": ["127.0.0.1"]},
1518             {"node": "listen", "attrib": "address", "values": ["127.0.0.1"]},
1519         ]
1520         for default in defaults:
1521             node = gfx_copy.find(default["node"])
1522             attrib = default["attrib"]
1523             if node is not None and (
1524                 attrib in node.attrib and node.attrib[attrib] in default["values"]
1525             ):
1526                 node.attrib.pop(attrib)
1527         return gfx_copy
1528     return xmlutil.to_dict(_filter_graphics(gfx1), True) == xmlutil.to_dict(
1529         _filter_graphics(gfx2), True
1530     )
1531 def _hostdevs_equal(dev1, dev2):
1532     def _filter_hostdevs(dev):
1533         type_ = dev.get("type")
1534         definition = {
1535             "type": type_,
1536         }
1537         if type_ == "pci":
1538             address_node = dev.find("./source/address")
1539             for attr in ["domain", "bus", "slot", "function"]:
1540                 definition[attr] = address_node.get(attr)
1541         elif type_ == "usb":
1542             for attr in ["vendor", "product"]:
1543                 definition[attr] = dev.find("./source/" + attr).get("id")
1544         return definition
1545     return _filter_hostdevs(dev1) == _filter_hostdevs(dev2)
1546 def _diff_lists(old, new, comparator):
1547     def _remove_indent(node):
1548         node_copy = copy.deepcopy(node)
1549         node_copy.text = None
1550         for item in node_copy.iter():
1551             item.tail = None
1552         return node_copy
1553     diff = {"unchanged": [], "new": [], "deleted": [], "sorted": []}
1554     old_devices = copy.deepcopy(old)
1555     for new_item in new:
1556         found = [
1557             item
1558             for item in old_devices
1559             if comparator(_remove_indent(item), _remove_indent(new_item))
1560         ]
1561         if found:
1562             old_devices.remove(found[0])
1563             diff["unchanged"].append(found[0])
1564             diff["sorted"].append(found[0])
1565         else:
1566             diff["new"].append(new_item)
1567             diff["sorted"].append(new_item)
1568     diff["deleted"] = old_devices
1569     return diff
1570 def _get_disk_target(targets, disks_count, prefix):
1571     for i in range(disks_count):
1572         ret = "{}{}".format(prefix, string.ascii_lowercase[i])
1573         if ret not in targets:
1574             return ret
1575     return None
1576 def _diff_disk_lists(old, new):
1577     targets = []
1578     prefixes = ["fd", "hd", "vd", "sd", "xvd", "ubd"]
1579     for disk in new:
1580         target_node = disk.find("target")
1581         target = target_node.get("dev")
1582         prefix = [item for item in prefixes if target.startswith(item)][0]
1583         new_target = _get_disk_target(targets, len(new), prefix)
1584         target_node.set("dev", new_target)
1585         targets.append(new_target)
1586     return _diff_lists(old, new, _disks_equal)
1587 def _diff_interface_lists(old, new):
1588     return _diff_lists(old, new, _nics_equal)
1589 def _diff_graphics_lists(old, new):
1590     return _diff_lists(old, new, _graphics_equal)
1591 def _diff_hostdev_lists(old, new):
1592     return _diff_lists(old, new, _hostdevs_equal)
1593 def _expand_cpuset(cpuset):
1594     if cpuset is None:
1595         return None
1596     if isinstance(cpuset, int):
1597         return str(cpuset)
1598     result = set()
1599     toremove = set()
1600     for part in cpuset.split(","):
1601         m = re.match("([0-9]+)-([0-9]+)", part)
1602         if m:
1603             result |= set(range(int(m.group(1)), int(m.group(2)) + 1))
1604         elif part.startswith("^"):
1605             toremove.add(int(part[1:]))
1606         else:
1607             result.add(int(part))
1608     cpus = list(result - toremove)
1609     cpus.sort()
1610     cpus = [str(cpu) for cpu in cpus]
1611     return ",".join(cpus)
1612 def _normalize_cpusets(desc, data):
1613     xpaths = ["cputune/cachetune", "cputune/cachetune/monitor", "cputune/memorytune"]
1614     for xpath in xpaths:
1615         nodes = desc.findall(xpath)
1616         for node in nodes:
1617             node.set("vcpus", _expand_cpuset(node.get("vcpus")))
1618     if not isinstance(data.get("cpu"), dict):
1619         return
1620     tuning = data["cpu"].get("tuning", {})
1621     for child in ["cachetune", "memorytune"]:
1622         if tuning.get(child):
1623             new_item = dict()
1624             for cpuset, value in tuning[child].items():
1625                 if child == "cachetune" and value.get("monitor"):
1626                     value["monitor"] = {
1627                         _expand_cpuset(monitor_cpus): monitor
1628                         for monitor_cpus, monitor in value["monitor"].items()
1629                     }
1630                 new_item[_expand_cpuset(cpuset)] = value
1631             tuning[child] = new_item
1632 def _serial_or_concole_equal(old, new):
1633     def _filter_serial_or_concole(item):
1634         return {
1635             "type": item.attrib["type"],
1636             "port": item.find("source").get("service")
1637             if item.find("source") is not None
1638             else None,
1639             "protocol": item.find("protocol").get("type")
1640             if item.find("protocol") is not None
1641             else None,
1642         }
1643     return _filter_serial_or_concole(old) == _filter_serial_or_concole(new)
1644 def _diff_serial_lists(old, new):
1645     return _diff_lists(old, new, _serial_or_concole_equal)
1646 def _diff_console_lists(old, new):
1647     return _diff_lists(old, new, _serial_or_concole_equal)
1648 def _format_pci_address(node):
1649     return "{}:{}:{}.{}".format(
1650         node.get("domain").replace("0x", ""),
1651         node.get("bus").replace("0x", ""),
1652         node.get("slot").replace("0x", ""),
1653         node.get("function").replace("0x", ""),
1654     )
1655 def _almost_equal(current, new):
1656     if current is None or new is None:
1657         return False
1658     return abs(current - new) / current &lt; 1e-03
1659 def _compute_device_changes(old_xml, new_xml, to_skip):
1660     devices_node = old_xml.find("devices")
1661     changes = {}
1662     for dev_type in to_skip:
1663         changes[dev_type] = {}
1664         if not to_skip[dev_type]:
1665             old = devices_node.findall(dev_type)
1666             new = new_xml.findall("devices/{}".format(dev_type))
1667             changes[dev_type] = globals()["_diff_{}_lists".format(dev_type)](old, new)
1668     return changes
1669 def _get_pci_addresses(node):
1670     return {_format_pci_address(address) for address in node.findall(".//address")}
1671 def _correct_networks(conn, desc):
1672     networks = [ElementTree.fromstring(net.XMLDesc()) for net in conn.listAllNetworks()]
1673     nics = desc.findall("devices/interface")
1674     device_map = {}
1675     for nic in nics:
1676         if nic.get("type") == "hostdev":
1677             addr = _get_pci_addresses(nic.find("source"))
1678             matching_nets = [
1679                 net
1680                 for net in networks
1681                 if net.find("forward").get("mode") == "hostdev"
1682                 and addr &amp; _get_pci_addresses(net)
1683             ]
1684             if matching_nets:
1685                 old_xml = ElementTree.tostring(nic)
1686                 nic.set("type", "network")
1687                 nic.find("source").set("network", matching_nets[0].find("name").text)
1688                 device_map[nic] = old_xml
1689     return device_map
1690 def _update_live(domain, new_desc, mem, cpu, old_mem, old_cpu, to_skip, test):
1691     status = {}
1692     errors = []
1693     if not domain.isActive():
1694         return status, errors
1695     commands = []
1696     if cpu and (isinstance(cpu, int) or isinstance(cpu, dict) and cpu.get("maximum")):
1697         new_cpu = cpu.get("maximum") if isinstance(cpu, dict) else cpu
1698         if old_cpu != new_cpu and new_cpu is not None:
1699             commands.append(
1700                 {
1701                     "device": "cpu",
1702                     "cmd": "setVcpusFlags",
1703                     "args": [new_cpu, libvirt.VIR_DOMAIN_AFFECT_LIVE],
1704                 }
1705             )
1706     if mem:
1707         if isinstance(mem, dict):
1708             new_mem = (
1709                 int(_handle_unit(mem.get("current")) / 1024)
1710                 if "current" in mem
1711                 else None
1712             )
1713         elif isinstance(mem, int):
1714             new_mem = int(mem * 1024)
1715         if not _almost_equal(old_mem, new_mem) and new_mem is not None:
1716             commands.append(
1717                 {
1718                     "device": "mem",
1719                     "cmd": "setMemoryFlags",
1720                     "args": [new_mem, libvirt.VIR_DOMAIN_AFFECT_LIVE],
1721                 }
1722             )
1723     old_desc = ElementTree.fromstring(domain.XMLDesc(0))
1724     changed_devices = {"interface": _correct_networks(domain.connect(), old_desc)}
1725     changes = _compute_device_changes(old_desc, new_desc, to_skip)
1726     removable_changes = []
1727     new_disks = []
1728     for new_disk in changes["disk"].get("new", []):
1729         device = new_disk.get("device", "disk")
1730         if device not in ["cdrom", "floppy"]:
1731             new_disks.append(new_disk)
1732             continue
1733         target_dev = new_disk.find("target").get("dev")
1734         matching = [
1735             old_disk
1736             for old_disk in changes["disk"].get("deleted", [])
1737             if old_disk.get("device", "disk") == device
1738             and old_disk.find("target").get("dev") == target_dev
1739         ]
1740         if not matching:
1741             new_disks.append(new_disk)
1742         else:
1743             updated_disk = matching[0]
1744             changes["disk"]["deleted"].remove(updated_disk)
1745             removable_changes.append(updated_disk)
1746             source_node = updated_disk.find("source")
1747             new_source_node = new_disk.find("source")
1748             source_file = (
1749                 new_source_node.get("file") if new_source_node is not None else None
1750             )
1751             updated_disk.set("type", "file")
1752             if source_node is not None:
1753                 updated_disk.remove(source_node)
1754             if source_file:
1755                 ElementTree.SubElement(
1756                     updated_disk, "source", attrib={"file": source_file}
1757                 )
1758     changes["disk"]["new"] = new_disks
1759     for dev_type in ["disk", "interface", "hostdev"]:
1760         for added in changes[dev_type].get("new", []):
1761             commands.append(
1762                 {
1763                     "device": dev_type,
1764                     "cmd": "attachDevice",
1765                     "args": [xmlutil.element_to_str(added)],
1766                 }
1767             )
1768         for removed in changes[dev_type].get("deleted", []):
1769             removed_def = changed_devices.get(dev_type, {}).get(
1770                 removed, ElementTree.tostring(removed)
1771             )
1772             commands.append(
1773                 {
1774                     "device": dev_type,
1775                     "cmd": "detachDevice",
1776                     "args": [salt.utils.stringutils.to_str(removed_def)],
1777                 }
1778             )
1779     for updated_disk in removable_changes:
1780         commands.append(
1781             {
1782                 "device": "disk",
1783                 "cmd": "updateDeviceFlags",
1784                 "args": [xmlutil.element_to_str(updated_disk)],
1785             }
1786         )
1787     for cmd in commands:
1788         try:
1789             ret = 0 if test else getattr(domain, cmd["cmd"])(*cmd["args"])
1790             device_type = cmd["device"]
1791             if device_type in ["cpu", "mem"]:
1792                 status[device_type] = not ret
1793             else:
1794                 actions = {
1795                     "attachDevice": "attached",
1796                     "detachDevice": "detached",
1797                     "updateDeviceFlags": "updated",
1798                 }
1799                 device_status = status.setdefault(device_type, {})
1800                 cmd_status = device_status.setdefault(actions[cmd["cmd"]], [])
1801                 cmd_status.append(cmd["args"][0])
1802         except libvirt.libvirtError as err:
1803             errors.append(str(err))
1804     return status, errors
1805 def update(
1806     name,
1807     cpu=0,
1808     mem=0,
1809     disk_profile=None,
1810     disks=None,
1811     nic_profile=None,
1812     interfaces=None,
1813     graphics=None,
1814     live=True,
1815     boot=None,
1816     numatune=None,
1817     test=False,
1818     boot_dev=None,
1819     hypervisor_features=None,
1820     clock=None,
1821     serials=None,
1822     consoles=None,
1823     stop_on_reboot=False,
1824     host_devices=None,
1825     **kwargs
1826 ):
1827     status = {
1828         "definition": False,
1829         "disk": {"attached": [], "detached": [], "updated": []},
1830         "interface": {"attached": [], "detached": []},
1831     }
1832     conn = __get_conn(**kwargs)
1833     domain = _get_domain(conn, name)
1834     desc = ElementTree.fromstring(domain.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE))
1835     need_update = False
1836     hypervisor = desc.get("type")
1837     all_disks = _disk_profile(conn, disk_profile, hypervisor, disks, name)
1838     if boot is not None:
1839         boot = _handle_remote_boot_params(boot)
1840         if boot.get("efi", None) is not None:
1841             need_update = _handle_efi_param(boot, desc)
1842     new_desc = ElementTree.fromstring(
1843         _gen_xml(
1844             conn,
1845             name,
1846             cpu,
1847             mem or 0,
1848             all_disks,
1849             _get_merged_nics(hypervisor, nic_profile, interfaces),
1850             hypervisor,
1851             domain.OSType(),
1852             desc.find(".//os/type").get("arch"),
1853             graphics,
1854             boot,
1855             boot_dev,
1856             numatune,
1857             serials=serials,
1858             consoles=consoles,
1859             stop_on_reboot=stop_on_reboot,
1860             host_devices=host_devices,
1861             **kwargs
1862         )
1863     )
1864     if clock:
1865         offset = "utc" if clock.get("utc", True) else "localtime"
1866         if "timezone" in clock:
1867             offset = "timezone"
1868         clock["offset"] = offset
1869     def _set_loader(node, value):
1870         salt.utils.xmlutil.set_node_text(node, value)
1871         if value is not None:
1872             node.set("readonly", "yes")
1873             node.set("type", "pflash")
1874     def _set_nvram(node, value):
1875         node.set("template", value)
1876     def _set_with_byte_unit(attr_name=None):
1877         def _setter(node, value):
1878             if attr_name:
1879                 node.set(attr_name, str(value))
1880             else:
1881                 node.text = str(value)
1882             node.set("unit", "bytes")
1883         return _setter
1884     def _get_with_unit(node):
1885         unit = node.get("unit", "KiB")
1886         unit = unit if unit != "bytes" else "b"
1887         value = node.get("memory") or node.get("size") or node.text
1888         return _handle_unit("{}{}".format(value, unit)) if value else None
1889     def _set_vcpu(node, value):
1890         node.text = str(value)
1891         node.set("current", str(value))
1892     old_mem = int(_get_with_unit(desc.find("memory")) / 1024)
1893     old_cpu = int(desc.find("./vcpu").text)
1894     def _yesno_attribute(path, xpath, attr_name, ignored=None):
1895         return xmlutil.attribute(
1896             path, xpath, attr_name, ignored, lambda v: "yes" if v else "no"
1897         )
1898     def _memory_parameter(path, xpath, attr_name=None, ignored=None):
1899         entry = {
1900             "path": path,
1901             "xpath": xpath,
1902             "convert": _handle_unit,
1903             "get": _get_with_unit,
1904             "set": _set_with_byte_unit(attr_name),
1905             "equals": _almost_equal,
1906         }
1907         if attr_name:
1908             entry["del"] = salt.utils.xmlutil.del_attribute(attr_name, ignored)
1909         return entry
1910     def _cpuset_parameter(path, xpath, attr_name=None, ignored=None):
1911         def _set_cpuset(node, value):
1912             if attr_name:
1913                 node.set(attr_name, value)
1914             else:
1915                 node.text = value
1916         entry = {
1917             "path": path,
1918             "xpath": xpath,
1919             "convert": _expand_cpuset,
1920             "get": lambda n: _expand_cpuset(n.get(attr_name) if attr_name else n.text),
1921             "set": _set_cpuset,
1922         }
1923         if attr_name:
1924             entry["del"] = salt.utils.xmlutil.del_attribute(attr_name, ignored)
1925         return entry
1926     data = {k: v for k, v in locals().items() if bool(v)}
1927     data["stop_on_reboot"] = stop_on_reboot
1928     if boot_dev:
1929         data["boot_dev"] = boot_dev.split()
1930     timer_names = [
1931         "platform",
1932         "hpet",
1933         "kvmclock",
1934         "pit",
1935         "rtc",
1936         "tsc",
1937         "hypervclock",
1938         "armvtimer",
1939     ]
1940     if data.get("clock", {}).get("timers"):
1941         attributes = [
1942             "track",
1943             "tickpolicy",
1944             "frequency",
1945             "mode",
1946             "present",
1947             "slew",
1948             "threshold",
1949             "limit",
1950         ]
1951         for timer in data["clock"]["timers"].values():
1952             for attribute in attributes:
1953                 if attribute not in timer:
1954                     timer[attribute] = None
1955         for timer_name in timer_names:
1956             if timer_name not in data["clock"]["timers"]:
1957                 data["clock"]["timers"][timer_name] = None
1958     _normalize_cpusets(desc, data)
1959     params_mapping = [
1960         {
1961             "path": "stop_on_reboot",
1962             "xpath": "on_reboot",
1963             "convert": lambda v: "destroy" if v else "restart",
1964         },
1965         {"path": "boot:kernel", "xpath": "os/kernel"},
1966         {"path": "boot:initrd", "xpath": "os/initrd"},
1967         {"path": "boot:cmdline", "xpath": "os/cmdline"},
1968         {"path": "boot:loader", "xpath": "os/loader", "set": _set_loader},
1969         {"path": "boot:nvram", "xpath": "os/nvram", "set": _set_nvram},
1970         _memory_parameter("mem", "memory"),
1971         _memory_parameter("mem", "currentMemory"),
1972         _memory_parameter("mem:max", "maxMemory"),
1973         _memory_parameter("mem:boot", "memory"),
1974         _memory_parameter("mem:current", "currentMemory"),
1975         xmlutil.attribute("mem:slots", "maxMemory", "slots", ["unit"]),
1976         _memory_parameter("mem:hard_limit", "memtune/hard_limit"),
1977         _memory_parameter("mem:soft_limit", "memtune/soft_limit"),
1978         _memory_parameter("mem:swap_hard_limit", "memtune/swap_hard_limit"),
1979         _memory_parameter("mem:min_guarantee", "memtune/min_guarantee"),
1980         xmlutil.attribute("boot_dev:{dev}", "os/boot[$dev]", "dev"),
1981         _memory_parameter(
1982             "mem:hugepages:{id}:size",
1983             "memoryBacking/hugepages/page[$id]",
1984             "size",
1985             ["unit", "nodeset"],
1986         ),
1987         _cpuset_parameter(
1988             "mem:hugepages:{id}:nodeset", "memoryBacking/hugepages/page[$id]", "nodeset"
1989         ),
1990         {
1991             "path": "mem:nosharepages",
1992             "xpath": "memoryBacking/nosharepages",
1993             "get": lambda n: n is not None,
1994             "set": lambda n, v: None,
1995         },
1996         {
1997             "path": "mem:locked",
1998             "xpath": "memoryBacking/locked",
1999             "get": lambda n: n is not None,
2000             "set": lambda n, v: None,
2001         },
2002         xmlutil.attribute("mem:source", "memoryBacking/source", "type"),
2003         xmlutil.attribute("mem:access", "memoryBacking/access", "mode"),
2004         xmlutil.attribute("mem:allocation", "memoryBacking/allocation", "mode"),
2005         {"path": "mem:discard", "xpath": "memoryBacking/discard"},
2006         {
2007             "path": "cpu",
2008             "xpath": "vcpu",
2009             "get": lambda n: int(n.text),
2010             "set": _set_vcpu,
2011         },
2012         {"path": "cpu:maximum", "xpath": "vcpu", "get": lambda n: int(n.text)},
2013         xmlutil.attribute("cpu:placement", "vcpu", "placement"),
2014         _cpuset_parameter("cpu:cpuset", "vcpu", "cpuset"),
2015         xmlutil.attribute("cpu:current", "vcpu", "current"),
2016         xmlutil.attribute("cpu:match", "cpu", "match"),
2017         xmlutil.attribute("cpu:mode", "cpu", "mode"),
2018         xmlutil.attribute("cpu:check", "cpu", "check"),
2019         {"path": "cpu:model:name", "xpath": "cpu/model"},
2020         xmlutil.attribute("cpu:model:fallback", "cpu/model", "fallback"),
2021         xmlutil.attribute("cpu:model:vendor_id", "cpu/model", "vendor_id"),
2022         {"path": "cpu:vendor", "xpath": "cpu/vendor"},
2023         xmlutil.attribute("cpu:topology:sockets", "cpu/topology", "sockets"),
2024         xmlutil.attribute("cpu:topology:cores", "cpu/topology", "cores"),
2025         xmlutil.attribute("cpu:topology:threads", "cpu/topology", "threads"),
2026         xmlutil.attribute("cpu:cache:level", "cpu/cache", "level"),
2027         xmlutil.attribute("cpu:cache:mode", "cpu/cache", "mode"),
2028         xmlutil.attribute(
2029             "cpu:features:{id}", "cpu/feature[@name='$id']", "policy", ["name"]
2030         ),
2031         _yesno_attribute(
2032             "cpu:vcpus:{id}:enabled", "vcpus/vcpu[@id='$id']", "enabled", ["id"]
2033         ),
2034         _yesno_attribute(
2035             "cpu:vcpus:{id}:hotpluggable",
2036             "vcpus/vcpu[@id='$id']",
2037             "hotpluggable",
2038             ["id"],
2039         ),
2040         xmlutil.int_attribute(
2041             "cpu:vcpus:{id}:order", "vcpus/vcpu[@id='$id']", "order", ["id"]
2042         ),
2043         _cpuset_parameter(
2044             "cpu:numa:{id}:cpus", "cpu/numa/cell[@id='$id']", "cpus", ["id"]
2045         ),
2046         _memory_parameter(
2047             "cpu:numa:{id}:memory", "cpu/numa/cell[@id='$id']", "memory", ["id"]
2048         ),
2049         _yesno_attribute(
2050             "cpu:numa:{id}:discard", "cpu/numa/cell[@id='$id']", "discard", ["id"]
2051         ),
2052         xmlutil.attribute(
2053             "cpu:numa:{id}:memAccess", "cpu/numa/cell[@id='$id']", "memAccess", ["id"]
2054         ),
2055         xmlutil.attribute(
2056             "cpu:numa:{id}:distances:{sid}",
2057             "cpu/numa/cell[@id='$id']/distances/sibling[@id='$sid']",
2058             "value",
2059             ["id"],
2060         ),
2061         {"path": "cpu:iothreads", "xpath": "iothreads"},
2062         {"path": "cpu:tuning:shares", "xpath": "cputune/shares"},
2063         {"path": "cpu:tuning:period", "xpath": "cputune/period"},
2064         {"path": "cpu:tuning:quota", "xpath": "cputune/quota"},
2065         {"path": "cpu:tuning:global_period", "xpath": "cputune/global_period"},
2066         {"path": "cpu:tuning:global_quota", "xpath": "cputune/global_quota"},
2067         {"path": "cpu:tuning:emulator_period", "xpath": "cputune/emulator_period"},
2068         {"path": "cpu:tuning:emulator_quota", "xpath": "cputune/emulator_quota"},
2069         {"path": "cpu:tuning:iothread_period", "xpath": "cputune/iothread_period"},
2070         {"path": "cpu:tuning:iothread_quota", "xpath": "cputune/iothread_quota"},
2071         _cpuset_parameter(
2072             "cpu:tuning:vcpupin:{id}",
2073             "cputune/vcpupin[@vcpu='$id']",
2074             "cpuset",
2075             ["vcpu"],
2076         ),
2077         _cpuset_parameter("cpu:tuning:emulatorpin", "cputune/emulatorpin", "cpuset"),
2078         _cpuset_parameter(
2079             "cpu:tuning:iothreadpin:{id}",
2080             "cputune/iothreadpin[@iothread='$id']",
2081             "cpuset",
2082             ["iothread"],
2083         ),
2084         xmlutil.attribute(
2085             "cpu:tuning:vcpusched:{id}:scheduler",
2086             "cputune/vcpusched[$id]",
2087             "scheduler",
2088             ["priority", "vcpus"],
2089         ),
2090         xmlutil.attribute(
2091             "cpu:tuning:vcpusched:{id}:priority", "cputune/vcpusched[$id]", "priority"
2092         ),
2093         _cpuset_parameter(
2094             "cpu:tuning:vcpusched:{id}:vcpus", "cputune/vcpusched[$id]", "vcpus"
2095         ),
2096         xmlutil.attribute(
2097             "cpu:tuning:iothreadsched:{id}:scheduler",
2098             "cputune/iothreadsched[$id]",
2099             "scheduler",
2100             ["priority", "iothreads"],
2101         ),
2102         xmlutil.attribute(
2103             "cpu:tuning:iothreadsched:{id}:priority",
2104             "cputune/iothreadsched[$id]",
2105             "priority",
2106         ),
2107         _cpuset_parameter(
2108             "cpu:tuning:iothreadsched:{id}:iothreads",
2109             "cputune/iothreadsched[$id]",
2110             "iothreads",
2111         ),
2112         xmlutil.attribute(
2113             "cpu:tuning:emulatorsched:scheduler",
2114             "cputune/emulatorsched",
2115             "scheduler",
2116             ["priority"],
2117         ),
2118         xmlutil.attribute(
2119             "cpu:tuning:emulatorsched:priority", "cputune/emulatorsched", "priority"
2120         ),
2121         xmlutil.attribute(
2122             "cpu:tuning:cachetune:{id}:monitor:{sid}",
2123             "cputune/cachetune[@vcpus='$id']/monitor[@vcpus='$sid']",
2124             "level",
2125             ["vcpus"],
2126         ),
2127         xmlutil.attribute(
2128             "cpu:tuning:memorytune:{id}:{sid}",
2129             "cputune/memorytune[@vcpus='$id']/node[@id='$sid']",
2130             "bandwidth",
2131             ["id", "vcpus"],
2132         ),
2133         xmlutil.attribute("clock:offset", "clock", "offset"),
2134         xmlutil.attribute("clock:adjustment", "clock", "adjustment", convert=str),
2135         xmlutil.attribute("clock:timezone", "clock", "timezone"),
2136     ]
2137     for timer in timer_names:
2138         params_mapping += [
2139             xmlutil.attribute(
2140                 "clock:timers:{}:track".format(timer),
2141                 "clock/timer[@name='{}']".format(timer),
2142                 "track",
2143                 ["name"],
2144             ),
2145             xmlutil.attribute(
2146                 "clock:timers:{}:tickpolicy".format(timer),
2147                 "clock/timer[@name='{}']".format(timer),
2148                 "tickpolicy",
2149                 ["name"],
2150             ),
2151             xmlutil.int_attribute(
2152                 "clock:timers:{}:frequency".format(timer),
2153                 "clock/timer[@name='{}']".format(timer),
2154                 "frequency",
2155                 ["name"],
2156             ),
2157             xmlutil.attribute(
2158                 "clock:timers:{}:mode".format(timer),
2159                 "clock/timer[@name='{}']".format(timer),
2160                 "mode",
2161                 ["name"],
2162             ),
2163             _yesno_attribute(
2164                 "clock:timers:{}:present".format(timer),
2165                 "clock/timer[@name='{}']".format(timer),
2166                 "present",
2167                 ["name"],
2168             ),
2169         ]
2170         for attr in ["slew", "threshold", "limit"]:
2171             params_mapping.append(
2172                 xmlutil.int_attribute(
2173                     "clock:timers:{}:{}".format(timer, attr),
2174                     "clock/timer[@name='{}']/catchup".format(timer),
2175                     attr,
2176                 )
2177             )
2178     for attr in ["level", "type", "size"]:
2179         params_mapping.append(
2180             xmlutil.attribute(
2181                 "cpu:tuning:cachetune:{id}:{sid}:" + attr,
2182                 "cputune/cachetune[@vcpus='$id']/cache[@id='$sid']",
2183                 attr,
2184                 ["id", "unit", "vcpus"],
2185             )
2186         )
2187     if hypervisor in ["qemu", "kvm"]:
2188         params_mapping += [
2189             xmlutil.attribute("numatune:memory:mode", "numatune/memory", "mode"),
2190             _cpuset_parameter("numatune:memory:nodeset", "numatune/memory", "nodeset"),
2191             xmlutil.attribute(
2192                 "numatune:memnodes:{id}:mode",
2193                 "numatune/memnode[@cellid='$id']",
2194                 "mode",
2195                 ["cellid"],
2196             ),
2197             _cpuset_parameter(
2198                 "numatune:memnodes:{id}:nodeset",
2199                 "numatune/memnode[@cellid='$id']",
2200                 "nodeset",
2201                 ["cellid"],
2202             ),
2203             xmlutil.attribute(
2204                 "hypervisor_features:kvm-hint-dedicated",
2205                 "features/kvm/hint-dedicated",
2206                 "state",
2207                 convert=lambda v: "on" if v else "off",
2208             ),
2209         ]
2210     need_update = (
2211         salt.utils.xmlutil.change_xml(desc, data, params_mapping) or need_update
2212     )
2213     devices_node = desc.find("devices")
2214     func_locals = locals()
2215     def _skip_update(names):
2216         return all(func_locals.get(n) is None for n in names)
2217     to_skip = {
2218         "disk": _skip_update(["disks", "disk_profile"]),
2219         "interface": _skip_update(["interfaces", "nic_profile"]),
2220         "graphics": _skip_update(["graphics"]),
2221         "serial": _skip_update(["serials"]),
2222         "console": _skip_update(["consoles"]),
2223         "hostdev": _skip_update(["host_devices"]),
2224     }
2225     changes = _compute_device_changes(desc, new_desc, to_skip)
2226     for dev_type in changes:
2227         if not to_skip[dev_type]:
2228             old = devices_node.findall(dev_type)
2229             if changes[dev_type].get("deleted") or changes[dev_type].get("new"):
2230                 for item in old:
2231                     devices_node.remove(item)
2232                 devices_node.extend(changes[dev_type]["sorted"])
2233                 need_update = True
2234     if need_update:
2235         try:
2236             if changes["disk"]:
2237                 for idx, item in enumerate(changes["disk"]["sorted"]):
2238                     source_file = all_disks[idx].get("source_file")
2239                     if all_disks[idx].get("device", "disk") == "cdrom":
2240                         continue
2241                     if (
2242                         item in changes["disk"]["new"]
2243                         and source_file
2244                         and not os.path.exists(source_file)
2245                     ):
2246                         _qemu_image_create(all_disks[idx])
2247                     elif item in changes["disk"]["new"] and not source_file:
2248                         _disk_volume_create(conn, all_disks[idx])
2249             if not test:
2250                 xml_desc = xmlutil.element_to_str(desc)
2251                 log.debug("Update virtual machine definition: %s", xml_desc)
2252                 conn.defineXML(xml_desc)
2253             status["definition"] = True
2254         except libvirt.libvirtError as err:
2255             conn.close()
2256             raise err
2257     if live:
2258         live_status, errors = _update_live(
2259             domain, new_desc, mem, cpu, old_mem, old_cpu, to_skip, test
2260         )
2261         status.update(live_status)
2262         if errors:
2263             status_errors = status.setdefault("errors", [])
2264             status_errors += errors
2265     conn.close()
2266     return status
2267 def list_domains(**kwargs):
2268     vms = []
2269     conn = __get_conn(**kwargs)
2270     for dom in _get_domain(conn, iterable=True):
2271         vms.append(dom.name())
2272     conn.close()
2273     return vms
2274 def list_active_vms(**kwargs):
2275     vms = []
2276     conn = __get_conn(**kwargs)
2277     for dom in _get_domain(conn, iterable=True, inactive=False):
2278         vms.append(dom.name())
2279     conn.close()
2280     return vms
2281 def list_inactive_vms(**kwargs):
2282     vms = []
2283     conn = __get_conn(**kwargs)
2284     for dom in _get_domain(conn, iterable=True, active=False):
2285         vms.append(dom.name())
2286     conn.close()
2287     return vms
2288 def vm_info(vm_=None, **kwargs):
2289     def _info(conn, dom):
2290         raw = dom.info()
2291         return {
2292             "cpu": raw[3],
2293             "cputime": int(raw[4]),
2294             "disks": _get_disks(conn, dom),
2295             "graphics": _get_graphics(dom),
2296             "nics": _get_nics(dom),
2297             "uuid": _get_uuid(dom),
2298             "loader": _get_loader(dom),
2299             "on_crash": _get_on_crash(dom),
2300             "on_reboot": _get_on_reboot(dom),
2301             "on_poweroff": _get_on_poweroff(dom),
2302             "maxMem": int(raw[1]),
2303             "mem": int(raw[2]),
2304             "state": VIRT_STATE_NAME_MAP.get(raw[0], "unknown"),
2305         }
2306     info = {}
2307     conn = __get_conn(**kwargs)
2308     if vm_:
2309         info[vm_] = _info(conn, _get_domain(conn, vm_))
2310     else:
2311         for domain in _get_domain(conn, iterable=True):
2312             info[domain.name()] = _info(conn, domain)
2313     conn.close()
2314     return info
2315 def vm_state(vm_=None, **kwargs):
2316     def _info(dom):
2317         state = ""
2318         raw = dom.info()
2319         state = VIRT_STATE_NAME_MAP.get(raw[0], "unknown")
2320         return state
2321     info = {}
2322     conn = __get_conn(**kwargs)
2323     if vm_:
2324         info[vm_] = _info(_get_domain(conn, vm_))
2325     else:
2326         for domain in _get_domain(conn, iterable=True):
2327             info[domain.name()] = _info(domain)
2328     conn.close()
2329     return info
2330 def _node_info(conn):
2331     raw = conn.getInfo()
2332     info = {
2333         "cpucores": raw[6],
2334         "cpumhz": raw[3],
2335         "cpumodel": str(raw[0]),
2336         "cpus": raw[2],
2337         "cputhreads": raw[7],
2338         "numanodes": raw[4],
2339         "phymemory": raw[1],
2340         "sockets": raw[5],
2341     }
2342     return info
2343 def node_info(**kwargs):
2344     conn = __get_conn(**kwargs)
2345     info = _node_info(conn)
2346     conn.close()
2347     return info
2348 def _node_devices(conn):
2349     devices = conn.listAllDevices()
2350     devices_infos = []
2351     for dev in devices:
2352         root = ElementTree.fromstring(dev.XMLDesc())
2353         if not set(dev.listCaps()) &amp; {"pci", "usb_device", "net"}:
2354             continue
2355         infos = {
2356             "caps": " ".join(dev.listCaps()),
2357         }
2358         if "net" in dev.listCaps():
2359             parent = root.find(".//parent").text
2360             if parent == "computer":
2361                 continue
2362             infos.update(
2363                 {
2364                     "name": root.find(".//interface").text,
2365                     "address": root.find(".//address").text,
2366                     "device name": parent,
2367                     "state": root.find(".//link").get("state"),
2368                 }
2369             )
2370             devices_infos.append(infos)
2371             continue
2372         vendor_node = root.find(".//vendor")
2373         vendor_id = vendor_node.get("id").lower()
2374         product_node = root.find(".//product")
2375         product_id = product_node.get("id").lower()
2376         infos.update(
2377             {"name": dev.name(), "vendor_id": vendor_id, "product_id": product_id}
2378         )
2379         if vendor_node.text:
2380             infos["vendor"] = vendor_node.text
2381         if product_node.text:
2382             infos["product"] = product_node.text
2383         if "pci" in dev.listCaps():
2384             infos["address"] = "{:04x}:{:02x}:{:02x}.{}".format(
2385                 int(root.find(".//domain").text),
2386                 int(root.find(".//bus").text),
2387                 int(root.find(".//slot").text),
2388                 root.find(".//function").text,
2389             )
2390             class_node = root.find(".//class")
2391             if class_node is not None:
2392                 infos["PCI class"] = class_node.text
2393             vf_addresses = [
2394                 _format_pci_address(vf)
2395                 for vf in root.findall(
2396                     "./capability[@type='pci']/capability[@type='virt_functions']/address"
2397                 )
2398             ]
2399             if vf_addresses:
2400                 infos["virtual functions"] = vf_addresses
2401             pf = root.find(
2402                 "./capability[@type='pci']/capability[@type='phys_function']/address"
2403             )
2404             if pf is not None:
2405                 infos["physical function"] = _format_pci_address(pf)
2406         elif "usb_device" in dev.listCaps():
2407             infos["address"] = "{:03}:{:03}".format(
2408                 int(root.find(".//bus").text), int(root.find(".//device").text)
2409             )
2410         linux_usb_host = vendor_id == "0x1d6b" and product_id in [
2411             "0x0001",
2412             "0x0002",
2413             "0x0003",
2414         ]
2415         if (
2416             root.find(".//capability[@type='pci-bridge']") is None
2417             and not linux_usb_host
2418         ):
2419             devices_infos.append(infos)
2420     return devices_infos
2421 def node_devices(**kwargs):
2422     conn = __get_conn(**kwargs)
2423     devs = _node_devices(conn)
2424     conn.close()
2425     return devs
2426 def get_nics(vm_, **kwargs):
2427     conn = __get_conn(**kwargs)
2428     nics = _get_nics(_get_domain(conn, vm_))
2429     conn.close()
2430     return nics
2431 def get_macs(vm_, **kwargs):
2432     doc = ElementTree.fromstring(get_xml(vm_, **kwargs))
2433     return [node.get("address") for node in doc.findall("devices/interface/mac")]
2434 def get_graphics(vm_, **kwargs):
2435     conn = __get_conn(**kwargs)
2436     graphics = _get_graphics(_get_domain(conn, vm_))
2437     conn.close()
2438     return graphics
2439 def get_loader(vm_, **kwargs):
2440     conn = __get_conn(**kwargs)
2441     try:
2442         loader = _get_loader(_get_domain(conn, vm_))
2443         return loader
2444     finally:
2445         conn.close()
2446 def get_disks(vm_, **kwargs):
2447     conn = __get_conn(**kwargs)
2448     disks = _get_disks(conn, _get_domain(conn, vm_))
2449     conn.close()
2450     return disks
2451 def setmem(vm_, memory, config=False, **kwargs):
2452     conn = __get_conn(**kwargs)
2453     dom = _get_domain(conn, vm_)
2454     if VIRT_STATE_NAME_MAP.get(dom.info()[0], "unknown") != "shutdown":
2455         return False
2456     flags = libvirt.VIR_DOMAIN_MEM_MAXIMUM
2457     if config:
2458         flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
2459     ret1 = dom.setMemoryFlags(memory * 1024, flags)
2460     ret2 = dom.setMemoryFlags(memory * 1024, libvirt.VIR_DOMAIN_AFFECT_CURRENT)
2461     conn.close()
2462     return ret1 == ret2 == 0
2463 def setvcpus(vm_, vcpus, config=False, **kwargs):
2464     conn = __get_conn(**kwargs)
2465     dom = _get_domain(conn, vm_)
2466     if VIRT_STATE_NAME_MAP.get(dom.info()[0], "unknown") != "shutdown":
2467         return False
2468     flags = libvirt.VIR_DOMAIN_VCPU_MAXIMUM
2469     if config:
2470         flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
2471     ret1 = dom.setVcpusFlags(vcpus, flags)
2472     ret2 = dom.setVcpusFlags(vcpus, libvirt.VIR_DOMAIN_AFFECT_CURRENT)
2473     conn.close()
2474     return ret1 == ret2 == 0
2475 def _freemem(conn):
2476     mem = conn.getInfo()[1]
2477     mem -= 256
2478     for dom in _get_domain(conn, iterable=True):
2479         if dom.ID() &gt; 0:
2480             mem -= dom.info()[2] / 1024
2481     return mem
2482 def freemem(**kwargs):
2483     conn = __get_conn(**kwargs)
2484     mem = _freemem(conn)
2485     conn.close()
2486     return mem
2487 def _freecpu(conn):
2488     cpus = conn.getInfo()[2]
2489     for dom in _get_domain(conn, iterable=True):
2490         if dom.ID() &gt; 0:
2491             cpus -= dom.info()[3]
2492     return cpus
2493 def freecpu(**kwargs):
2494     conn = __get_conn(**kwargs)
2495     cpus = _freecpu(conn)
2496     conn.close()
2497     return cpus
2498 def full_info(**kwargs):
2499     conn = __get_conn(**kwargs)
2500     info = {
2501         "freecpu": _freecpu(conn),
2502         "freemem": _freemem(conn),
2503         "node_info": _node_info(conn),
2504         "vm_info": vm_info(),
2505     }
2506     conn.close()
2507     return info
2508 def get_xml(vm_, **kwargs):
2509     conn = __get_conn(**kwargs)
2510     xml_desc = (
2511         vm_.XMLDesc(0)
2512         if isinstance(vm_, libvirt.virDomain)
2513         else _get_domain(conn, vm_).XMLDesc(0)
2514     )
2515     conn.close()
2516     return xml_desc
2517 def get_profiles(hypervisor=None, **kwargs):
2518     conn = __get_conn(**kwargs)
2519     caps = _capabilities(conn)
2520     hypervisors = sorted(
2521         {
2522             x
2523             for y in [guest["arch"]["domains"].keys() for guest in caps["guests"]]
2524             for x in y
2525         }
2526     )
2527     if len(hypervisors) == 0:
2528         raise SaltInvocationError("No supported hypervisors were found")
2529     if not hypervisor:
2530         hypervisor = "kvm" if "kvm" in hypervisors else hypervisors[0]
2531     ret = {
2532         "disk": {"default": _disk_profile(conn, "default", hypervisor, [], None)},
2533         "nic": {"default": _nic_profile("default", hypervisor)},
2534     }
2535     virtconf = __salt__["config.get"]("virt", {})
2536     for profile in virtconf.get("disk", []):
2537         ret["disk"][profile] = _disk_profile(conn, profile, hypervisor, [], None)
2538     for profile in virtconf.get("nic", []):
2539         ret["nic"][profile] = _nic_profile(profile, hypervisor)
2540     return ret
2541 def shutdown(vm_, **kwargs):
2542     conn = __get_conn(**kwargs)
2543     dom = _get_domain(conn, vm_)
2544     ret = dom.shutdown() == 0
2545     conn.close()
2546     return ret
2547 def pause(vm_, **kwargs):
2548     conn = __get_conn(**kwargs)
2549     dom = _get_domain(conn, vm_)
2550     ret = dom.suspend() == 0
2551     conn.close()
2552     return ret
2553 def resume(vm_, **kwargs):
2554     conn = __get_conn(**kwargs)
2555     dom = _get_domain(conn, vm_)
2556     ret = dom.resume() == 0
2557     conn.close()
2558     return ret
2559 def start(name, **kwargs):
2560     conn = __get_conn(**kwargs)
2561     ret = _get_domain(conn, name).create() == 0
2562     conn.close()
2563     return ret
2564 def stop(name, **kwargs):
2565     conn = __get_conn(**kwargs)
2566     ret = _get_domain(conn, name).destroy() == 0
2567     conn.close()
2568     return ret
2569 def reboot(name, **kwargs):
2570     conn = __get_conn(**kwargs)
2571     ret = _get_domain(conn, name).reboot(libvirt.VIR_DOMAIN_REBOOT_DEFAULT) == 0
2572     conn.close()
2573     return ret
2574 def reset(vm_, **kwargs):
2575     conn = __get_conn(**kwargs)
2576     dom = _get_domain(conn, vm_)
2577     ret = dom.reset(0) == 0
2578     conn.close()
2579     return ret
2580 def ctrl_alt_del(vm_, **kwargs):
2581     conn = __get_conn(**kwargs)
2582     dom = _get_domain(conn, vm_)
2583     ret = dom.sendKey(0, 0, [29, 56, 111], 3, 0) == 0
2584     conn.close()
2585     return ret
2586 def create_xml_str(xml, **kwargs):  # pylint: disable=redefined-outer-name
2587     conn = __get_conn(**kwargs)
2588     ret = conn.createXML(xml, 0) is not None
2589     conn.close()
2590     return ret
2591 def create_xml_path(path, **kwargs):
2592     try:
2593         with salt.utils.files.fopen(path, "r") as fp_:
2594             return create_xml_str(
2595                 salt.utils.stringutils.to_unicode(fp_.read()), **kwargs
2596             )
2597     except OSError:
2598         return False
2599 def define_xml_str(xml, **kwargs):  # pylint: disable=redefined-outer-name
2600     conn = __get_conn(**kwargs)
2601     ret = conn.defineXML(xml) is not None
2602     conn.close()
2603     return ret
2604 def define_xml_path(path, **kwargs):
2605     try:
2606         with salt.utils.files.fopen(path, "r") as fp_:
2607             return define_xml_str(
2608                 salt.utils.stringutils.to_unicode(fp_.read()), **kwargs
2609             )
2610     except OSError:
2611         return False
2612 def _define_vol_xml_str(conn, xml, pool=None):  # pylint: disable=redefined-outer-name
2613     default_pool = "default" if conn.getType() != "ESX" else "0"
2614     poolname = (
2615         pool if pool else __salt__["config.get"]("virt:storagepool", default_pool)
2616     )
2617     pool = conn.storagePoolLookupByName(str(poolname))
2618     ret = pool.createXML(xml, 0) is not None
2619     return ret
2620 def define_vol_xml_str(
2621     xml, pool=None, **kwargs
2622 ):  # pylint: disable=redefined-outer-name
2623     conn = __get_conn(**kwargs)
2624     ret = False
2625     try:
2626         ret = _define_vol_xml_str(conn, xml, pool=pool)
2627     except libvirtError as err:
2628         raise CommandExecutionError(err.get_error_message())
2629     finally:
2630         conn.close()
2631     return ret
2632 def define_vol_xml_path(path, pool=None, **kwargs):
2633     try:
2634         with salt.utils.files.fopen(path, "r") as fp_:
2635             return define_vol_xml_str(
2636                 salt.utils.stringutils.to_unicode(fp_.read()), pool=pool, **kwargs
2637             )
2638     except OSError:
2639         return False
2640 def migrate(vm_, target, **kwargs):
2641     conn = __get_conn()
2642     dom = _get_domain(conn, vm_)
2643     if not urllib.parse.urlparse(target).scheme:
2644         proto = "qemu"
2645         dst_uri = "{}://{}/system".format(proto, target)
2646     else:
2647         dst_uri = target
2648     ret = _migrate(dom, dst_uri, **kwargs)
2649     conn.close()
2650     return ret
2651 def migrate_start_postcopy(vm_):
2652     conn = __get_conn()
2653     dom = _get_domain(conn, vm_)
2654     try:
2655         dom.migrateStartPostCopy()
2656     except libvirt.libvirtError as err:
2657         conn.close()
2658         raise CommandExecutionError(err.get_error_message())
2659     conn.close()
2660 def seed_non_shared_migrate(disks, force=False):
2661     for _, data in disks.items():
2662         fn_ = data["file"]
2663         form = data["file format"]
2664         size = data["virtual size"].split()[1][1:]
2665         if os.path.isfile(fn_) and not force:
2666             pre = salt.utils.yaml.safe_load(
2667                 subprocess.Popen(
2668                     ["qemu-img", "info", "arch"], stdout=subprocess.PIPE
2669                 ).communicate()[0]
2670             )
2671             if (
2672                 pre["file format"] != data["file format"]
2673                 and pre["virtual size"] != data["virtual size"]
2674             ):
2675                 return False
2676         if not os.path.isdir(os.path.dirname(fn_)):
2677             os.makedirs(os.path.dirname(fn_))
2678         if os.path.isfile(fn_):
2679             os.remove(fn_)
2680         subprocess.call(["qemu-img", "create", "-f", form, fn_, size])
2681         creds = _libvirt_creds()
2682         subprocess.call(["chown", "{user}:{group}".format(**creds), fn_])
2683     return True
2684 def set_autostart(vm_, state="on", **kwargs):
2685     conn = __get_conn(**kwargs)
2686     dom = _get_domain(conn, vm_)
2687     ret = False
2688     if state == "on":
2689         ret = dom.setAutostart(1) == 0
2690     elif state == "off":
2691         ret = dom.setAutostart(0) == 0
2692     conn.close()
2693     return ret
2694 def undefine(vm_, **kwargs):
2695     conn = __get_conn(**kwargs)
2696     dom = _get_domain(conn, vm_)
2697     if getattr(libvirt, "VIR_DOMAIN_UNDEFINE_NVRAM", False):
2698         ret = dom.undefineFlags(libvirt.VIR_DOMAIN_UNDEFINE_NVRAM) == 0
2699     else:
2700         ret = dom.undefine() == 0
2701     conn.close()
2702     return ret
2703 def purge(vm_, dirs=False, removables=False, **kwargs):
2704     conn = __get_conn(**kwargs)
2705     dom = _get_domain(conn, vm_)
2706     disks = _get_disks(conn, dom)
2707     if (
2708         VIRT_STATE_NAME_MAP.get(dom.info()[0], "unknown") != "shutdown"
2709         and dom.destroy() != 0
2710     ):
2711         return False
2712     directories = set()
2713     for disk in disks:
2714         if not removables and disks[disk]["type"] in ["cdrom", "floppy"]:
2715             continue
2716         if disks[disk].get("zfs", False):
2717             time.sleep(3)
2718             fs_name = disks[disk]["file"][len("/dev/zvol/") :]
2719             log.info("Destroying VM ZFS volume %s", fs_name)
2720             __salt__["zfs.destroy"](name=fs_name, force=True)
2721         elif os.path.exists(disks[disk]["file"]):
2722             os.remove(disks[disk]["file"])
2723             directories.add(os.path.dirname(disks[disk]["file"]))
2724         else:
2725             matcher = re.match("^(?P&lt;pool&gt;[^/]+)/(?P&lt;volume&gt;.*)$", disks[disk]["file"])
2726             if matcher:
2727                 pool_name = matcher.group("pool")
2728                 pool = None
2729                 if pool_name in conn.listStoragePools():
2730                     pool = conn.storagePoolLookupByName(pool_name)
2731                 if pool and matcher.group("volume") in pool.listVolumes():
2732                     volume = pool.storageVolLookupByName(matcher.group("volume"))
2733                     volume.delete()
2734     if dirs:
2735         for dir_ in directories:
2736             shutil.rmtree(dir_)
2737     if getattr(libvirt, "VIR_DOMAIN_UNDEFINE_NVRAM", False):
2738         try:
2739             dom.undefineFlags(libvirt.VIR_DOMAIN_UNDEFINE_NVRAM)
2740         except Exception:  # pylint: disable=broad-except
2741             dom.undefine()
2742     else:
2743         dom.undefine()
2744     conn.close()
2745     return True
2746 def virt_type():
2747     return __grains__["virtual"]
2748 def _is_kvm_hyper():
2749     if not os.path.exists("/dev/kvm"):
2750         return False
2751     return "libvirtd" in __salt__["cmd.run"](__grains__["ps"])
2752 def _is_xen_hyper():
2753     try:
2754         if __grains__["virtual_subtype"] != "Xen Dom0":
2755             return False
2756     except KeyError:
2757         return False
2758     try:
2759         with salt.utils.files.fopen("/proc/modules") as fp_:
2760             if "xen_" not in salt.utils.stringutils.to_unicode(fp_.read()):
2761                 return False
2762     except OSError:
2763         return False
2764     return "libvirtd" in __salt__["cmd.run"](__grains__["ps"])
2765 def get_hypervisor():
2766     hypervisors = ["kvm", "xen", "bhyve"]
2767     result = [
2768         hyper
2769         for hyper in hypervisors
2770         if getattr(sys.modules[__name__], "_is_{}_hyper".format(hyper))()
2771     ]
2772     return result[0] if result else None
2773 def _is_bhyve_hyper():
2774     sysctl_cmd = "sysctl hw.vmm.create"
2775     vmm_enabled = False
2776     try:
2777         stdout = subprocess.Popen(
2778             ["sysctl", "hw.vmm.create"], stdout=subprocess.PIPE
2779         ).communicate()[0]
2780         vmm_enabled = len(salt.utils.stringutils.to_str(stdout).split('"')[1]) != 0
2781     except IndexError:
2782         pass
2783     return vmm_enabled
2784 def is_hyper():
2785     if HAS_LIBVIRT:
2786         return _is_xen_hyper() or _is_kvm_hyper() or _is_bhyve_hyper()
2787     return False
2788 def vm_cputime(vm_=None, **kwargs):
2789     conn = __get_conn(**kwargs)
2790     host_cpus = conn.getInfo()[2]
2791     def _info(dom):
2792         raw = dom.info()
2793         vcpus = int(raw[3])
2794         cputime = int(raw[4])
2795         cputime_percent = 0
2796         if cputime:
2797             cputime_percent = (1.0e-7 * cputime / host_cpus) / vcpus
2798         return {
2799             "cputime": int(raw[4]),
2800             "cputime_percent": int("{:.0f}".format(cputime_percent)),
2801         }
2802     info = {}
2803     if vm_:
2804         info[vm_] = _info(_get_domain(conn, vm_))
2805     else:
2806         for domain in _get_domain(conn, iterable=True):
2807             info[domain.name()] = _info(domain)
2808     conn.close()
2809     return info
2810 def vm_netstats(vm_=None, **kwargs):
2811     def _info(dom):
2812         nics = _get_nics(dom)
2813         ret = {
2814             "rx_bytes": 0,
2815             "rx_packets": 0,
2816             "rx_errs": 0,
2817             "rx_drop": 0,
2818             "tx_bytes": 0,
2819             "tx_packets": 0,
2820             "tx_errs": 0,
2821             "tx_drop": 0,
2822         }
2823         for attrs in nics.values():
2824             if "target" in attrs:
2825                 dev = attrs["target"]
2826                 stats = dom.interfaceStats(dev)
2827                 ret["rx_bytes"] += stats[0]
2828                 ret["rx_packets"] += stats[1]
2829                 ret["rx_errs"] += stats[2]
2830                 ret["rx_drop"] += stats[3]
2831                 ret["tx_bytes"] += stats[4]
2832                 ret["tx_packets"] += stats[5]
2833                 ret["tx_errs"] += stats[6]
2834                 ret["tx_drop"] += stats[7]
2835         return ret
2836     info = {}
2837     conn = __get_conn(**kwargs)
2838     if vm_:
2839         info[vm_] = _info(_get_domain(conn, vm_))
2840     else:
2841         for domain in _get_domain(conn, iterable=True):
2842             info[domain.name()] = _info(domain)
2843     conn.close()
2844     return info
2845 def vm_diskstats(vm_=None, **kwargs):
2846     def get_disk_devs(dom):
2847         doc = ElementTree.fromstring(get_xml(dom, **kwargs))
2848         return [target.get("dev") for target in doc.findall("devices/disk/target")]
2849     def _info(dom):
2850         disks = get_disk_devs(dom)
2851         ret = {"rd_req": 0, "rd_bytes": 0, "wr_req": 0, "wr_bytes": 0, "errs": 0}
2852         for disk in disks:
2853             stats = dom.blockStats(disk)
2854             ret["rd_req"] += stats[0]
2855             ret["rd_bytes"] += stats[1]
2856             ret["wr_req"] += stats[2]
2857             ret["wr_bytes"] += stats[3]
2858             ret["errs"] += stats[4]
2859         return ret
2860     info = {}
2861     conn = __get_conn(**kwargs)
2862     if vm_:
2863         info[vm_] = _info(_get_domain(conn, vm_))
2864     else:
2865         for domain in _get_domain(conn, iterable=True, inactive=False):
2866             info[domain.name()] = _info(domain)
2867     conn.close()
2868     return info
2869 def _parse_snapshot_description(vm_snapshot, unix_time=False):
2870     ret = dict()
2871     tree = ElementTree.fromstring(vm_snapshot.getXMLDesc())
2872     for node in tree:
2873         if node.tag == "name":
2874             ret["name"] = node.text
2875         elif node.tag == "creationTime":
2876             ret["created"] = (
2877                 datetime.datetime.fromtimestamp(float(node.text)).isoformat(" ")
2878                 if not unix_time
2879                 else float(node.text)
2880             )
2881         elif node.tag == "state":
2882             ret["running"] = node.text == "running"
2883     ret["current"] = vm_snapshot.isCurrent() == 1
2884     return ret
2885 def list_snapshots(domain=None, **kwargs):
2886     ret = dict()
2887     conn = __get_conn(**kwargs)
2888     for vm_domain in _get_domain(conn, *(domain and [domain] or list()), iterable=True):
2889         ret[vm_domain.name()] = [
2890             _parse_snapshot_description(snap) for snap in vm_domain.listAllSnapshots()
2891         ] or "N/A"
2892     conn.close()
2893     return ret
2894 def snapshot(domain, name=None, suffix=None, **kwargs):
2895     if name and name.lower() == domain.lower():
2896         raise CommandExecutionError(
2897             "Virtual Machine {name} is already defined. "
2898             "Please choose another name for the snapshot".format(name=name)
2899         )
2900     if not name:
2901         name = "{domain}-{tsnap}".format(
2902             domain=domain, tsnap=time.strftime("%Y%m%d-%H%M%S", time.localtime())
2903         )
2904     if suffix:
2905         name = "{name}-{suffix}".format(name=name, suffix=suffix)
2906     doc = ElementTree.Element("domainsnapshot")
2907     n_name = ElementTree.SubElement(doc, "name")
2908     n_name.text = name
2909     conn = __get_conn(**kwargs)
2910     _get_domain(conn, domain).snapshotCreateXML(xmlutil.element_to_str(doc))
2911     conn.close()
2912     return {"name": name}
2913 def delete_snapshots(name, *names, **kwargs):
2914     deleted = dict()
2915     conn = __get_conn(**kwargs)
2916     domain = _get_domain(conn, name)
2917     for snap in domain.listAllSnapshots():
2918         if snap.getName() in names or not names:
2919             deleted[snap.getName()] = _parse_snapshot_description(snap)
2920             snap.delete()
2921     conn.close()
2922     available = {
2923         name: [_parse_snapshot_description(snap) for snap in domain.listAllSnapshots()]
2924         or "N/A"
2925     }
2926     return {"available": available, "deleted": deleted}
2927 def revert_snapshot(name, vm_snapshot=None, cleanup=False, **kwargs):
2928     ret = dict()
2929     conn = __get_conn(**kwargs)
2930     domain = _get_domain(conn, name)
2931     snapshots = domain.listAllSnapshots()
2932     _snapshots = list()
2933     for snap_obj in snapshots:
2934         _snapshots.append(
2935             {
2936                 "idx": _parse_snapshot_description(snap_obj, unix_time=True)["created"],
2937                 "ptr": snap_obj,
2938             }
2939         )
2940     snapshots = [
2941         w_ptr["ptr"]
2942         for w_ptr in sorted(_snapshots, key=lambda item: item["idx"], reverse=True)
2943     ]
2944     del _snapshots
2945     if not snapshots:
2946         conn.close()
2947         raise CommandExecutionError("No snapshots found")
2948     elif len(snapshots) == 1:
2949         conn.close()
2950         raise CommandExecutionError(
2951             "Cannot revert to itself: only one snapshot is available."
2952         )
2953     snap = None
2954     for p_snap in snapshots:
2955         if not vm_snapshot:
2956             if p_snap.isCurrent() and snapshots[snapshots.index(p_snap) + 1 :]:
2957                 snap = snapshots[snapshots.index(p_snap) + 1 :][0]
2958                 break
2959         elif p_snap.getName() == vm_snapshot:
2960             snap = p_snap
2961             break
2962     if not snap:
2963         conn.close()
2964         raise CommandExecutionError(
2965             snapshot
2966             and 'Snapshot "{}" not found'.format(vm_snapshot)
2967             or "No more previous snapshots available"
2968         )
2969     elif snap.isCurrent():
2970         conn.close()
2971         raise CommandExecutionError("Cannot revert to the currently running snapshot.")
2972     domain.revertToSnapshot(snap)
2973     ret["reverted"] = snap.getName()
2974     if cleanup:
2975         delete = list()
2976         for p_snap in snapshots:
2977             if p_snap.getName() != snap.getName():
2978                 delete.append(p_snap.getName())
2979                 p_snap.delete()
2980             else:
2981                 break
2982         ret["deleted"] = delete
2983     else:
2984         ret["deleted"] = "N/A"
2985     conn.close()
2986     return ret
2987 def _caps_add_machine(machines, node):
2988     maxcpus = node.get("maxCpus")
2989     canonical = node.get("canonical")
2990     name = node.text
2991     alternate_name = ""
2992     if canonical:
2993         alternate_name = name
2994         name = canonical
2995     machine = machines.get(name)
2996     if not machine:
2997         machine = {"alternate_names": []}
2998         if maxcpus:
2999             machine["maxcpus"] = int(maxcpus)
3000         machines[name] = machine
3001     if alternate_name:
3002         machine["alternate_names"].append(alternate_name)
3003 def _parse_caps_guest(guest):
3004     arch_node = guest.find("arch")
3005     result = {
3006         "os_type": guest.find("os_type").text,
3007         "arch": {"name": arch_node.get("name"), "machines": {}, "domains": {}},
3008     }
3009     child = None
3010     for child in arch_node:
3011         if child.tag == "wordsize":
3012             result["arch"]["wordsize"] = int(child.text)
3013         elif child.tag == "emulator":
3014             result["arch"]["emulator"] = child.text
3015         elif child.tag == "machine":
3016             _caps_add_machine(result["arch"]["machines"], child)
3017         elif child.tag == "domain":
3018             domain_type = child.get("type")
3019             domain = {"emulator": None, "machines": {}}
3020             emulator_node = child.find("emulator")
3021             if emulator_node is not None:
3022                 domain["emulator"] = emulator_node.text
3023             for machine in child.findall("machine"):
3024                 _caps_add_machine(domain["machines"], machine)
3025             result["arch"]["domains"][domain_type] = domain
3026     features_nodes = guest.find("features")
3027     if features_nodes is not None and child is not None:
3028         result["features"] = {
3029             child.tag: {
3030                 "toggle": child.get("toggle", "no") == "yes",
3031                 "default": child.get("default", "on") == "on",
3032             }
3033             for child in features_nodes
3034         }
3035     return result
3036 def _parse_caps_cell(cell):
3037     result = {"id": int(cell.get("id"))}
3038     mem_node = cell.find("memory")
3039     if mem_node is not None:
3040         unit = mem_node.get("unit", "KiB")
3041         memory = mem_node.text
3042         result["memory"] = "{} {}".format(memory, unit)
3043     pages = [
3044         {
3045             "size": "{} {}".format(page.get("size"), page.get("unit", "KiB")),
3046             "available": int(page.text),
3047         }
3048         for page in cell.findall("pages")
3049     ]
3050     if pages:
3051         result["pages"] = pages
3052     distances = {
3053         int(distance.get("id")): int(distance.get("value"))
3054         for distance in cell.findall("distances/sibling")
3055     }
3056     if distances:
3057         result["distances"] = distances
3058     cpus = []
3059     for cpu_node in cell.findall("cpus/cpu"):
3060         cpu = {"id": int(cpu_node.get("id"))}
3061         socket_id = cpu_node.get("socket_id")
3062         if socket_id:
3063             cpu["socket_id"] = int(socket_id)
3064         core_id = cpu_node.get("core_id")
3065         if core_id:
3066             cpu["core_id"] = int(core_id)
3067         siblings = cpu_node.get("siblings")
3068         if siblings:
3069             cpu["siblings"] = siblings
3070         cpus.append(cpu)
3071     if cpus:
3072         result["cpus"] = cpus
3073     return result
3074 def _parse_caps_bank(bank):
3075     result = {
3076         "id": int(bank.get("id")),
3077         "level": int(bank.get("level")),
3078         "type": bank.get("type"),
3079         "size": "{} {}".format(bank.get("size"), bank.get("unit")),
3080         "cpus": bank.get("cpus"),
3081     }
3082     controls = []
3083     for control in bank.findall("control"):
3084         unit = control.get("unit")
3085         result_control = {
3086             "granularity": "{} {}".format(control.get("granularity"), unit),
3087             "type": control.get("type"),
3088             "maxAllocs": int(control.get("maxAllocs")),
3089         }
3090         minimum = control.get("min")
3091         if minimum:
3092             result_control["min"] = "{} {}".format(minimum, unit)
3093         controls.append(result_control)
3094     if controls:
3095         result["controls"] = controls
3096     return result
3097 def _parse_caps_host(host):
3098     result = {}
3099     for child in host:
3100         if child.tag == "uuid":
3101             result["uuid"] = child.text
3102         elif child.tag == "cpu":
3103             cpu = {
3104 <a name="2"></a>                "arch": child.find("arch").text
3105                 if child.find("arch") is not None
3106                 else None,
3107                 "model": child.find("model")<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.text
3108                 if child.find("model") is not None
3109                 else None,
3110                 "vendor": child.find("vendor").text
3111                 if child.find("vendor") is not None
3112                 else None,
3113                 "features": [
3114                     feature.get("name") for feature in child.findall("feature")
3115                 ],
3116                 "pages": [
3117                     {"size": "{} {}".format(page.</b></font>get("size"), page.get("unit", "KiB"))}
3118                     for page in child.findall("pages")
3119                 ],
3120             }
3121             microcode = child.find("microcode")
3122             if microcode is not None:
3123                 cpu["microcode"] = microcode.get("version")
3124             topology = child.find("topology")
3125             if topology is not None:
3126                 cpu["sockets"] = int(topology.get("sockets"))
3127                 cpu["cores"] = int(topology.get("cores"))
3128                 cpu["threads"] = int(topology.get("threads"))
3129             result["cpu"] = cpu
3130         elif child.tag == "power_management":
3131             result["power_management"] = [node.tag for node in child]
3132         elif child.tag == "migration_features":
3133             result["migration"] = {
3134                 "live": child.find("live") is not None,
3135                 "transports": [
3136                     node.text for node in child.findall("uri_transports/uri_transport")
3137                 ],
3138             }
3139         elif child.tag == "topology":
3140             result["topology"] = {
3141                 "cells": [
3142                     _parse_caps_cell(cell) for cell in child.findall("cells/cell")
3143                 ]
3144             }
3145         elif child.tag == "cache":
3146             result["cache"] = {
3147                 "banks": [_parse_caps_bank(bank) for bank in child.findall("bank")]
3148             }
3149     result["security"] = [
3150         {
3151             "model": secmodel.find("model").text
3152             if secmodel.find("model") is not None
3153             else None,
3154             "doi": secmodel.find("doi").text
3155             if secmodel.find("doi") is not None
3156             else None,
3157             "baselabels": [
3158                 {"type": label.get("type"), "label": label.text}
3159                 for label in secmodel.findall("baselabel")
3160             ],
3161         }
3162         for secmodel in host.findall("secmodel")
3163     ]
3164     return result
3165 def _capabilities(conn):
3166     caps = ElementTree.fromstring(conn.getCapabilities())
3167     return {
3168         "host": _parse_caps_host(caps.find("host")),
3169         "guests": [_parse_caps_guest(guest) for guest in caps.findall("guest")],
3170     }
3171 def capabilities(**kwargs):
3172     conn = __get_conn(**kwargs)
3173     try:
3174         caps = _capabilities(conn)
3175     except libvirt.libvirtError as err:
3176         raise CommandExecutionError(str(err))
3177     finally:
3178         conn.close()
3179     return caps
3180 def _parse_caps_enum(node):
3181     return (node.get("name"), [value.text for value in node.findall("value")])
3182 def _parse_caps_cpu(node):
3183     result = {}
3184     for mode in node.findall("mode"):
3185         if not mode.get("supported") == "yes":
3186             continue
3187         name = mode.get("name")
3188         if name == "host-passthrough":
3189             result[name] = True
3190         elif name == "host-model":
3191             host_model = {}
3192             model_node = mode.find("model")
3193             if model_node is not None:
3194                 model = {"name": model_node.text}
3195                 vendor_id = model_node.get("vendor_id")
3196                 if vendor_id:
3197                     model["vendor_id"] = vendor_id
3198                 fallback = model_node.get("fallback")
3199                 if fallback:
3200                     model["fallback"] = fallback
3201                 host_model["model"] = model
3202             vendor = (
3203                 mode.find("vendor").text if mode.find("vendor") is not None else None
3204             )
3205             if vendor:
3206                 host_model["vendor"] = vendor
3207             features = {
3208                 feature.get("name"): feature.get("policy")
3209                 for feature in mode.findall("feature")
3210             }
3211             if features:
3212                 host_model["features"] = features
3213             result[name] = host_model
3214         elif name == "custom":
3215             custom_model = {}
3216             models = {
3217                 model.text: model.get("usable") for model in mode.findall("model")
3218             }
3219             if models:
3220                 custom_model["models"] = models
3221             result[name] = custom_model
3222     return result
3223 def _parse_caps_devices_features(node):
3224     result = {}
3225     for child in node:
3226         if child.get("supported") == "yes":
3227             enums = [_parse_caps_enum(node) for node in child.findall("enum")]
3228             result[child.tag] = {item[0]: item[1] for item in enums if item[0]}
3229     return result
3230 def _parse_caps_loader(node):
3231     enums = [_parse_caps_enum(enum) for enum in node.findall("enum")]
3232     result = {item[0]: item[1] for item in enums if item[0]}
3233     values = [child.text for child in node.findall("value")]
3234     if values:
3235         result["values"] = values
3236     return result
3237 def _parse_domain_caps(caps):
3238     result = {
3239         "emulator": caps.find("path").text if caps.find("path") is not None else None,
3240         "domain": caps.find("domain").text if caps.find("domain") is not None else None,
3241         "machine": caps.find("machine").text
3242         if caps.find("machine") is not None
3243         else None,
3244         "arch": caps.find("arch").text if caps.find("arch") is not None else None,
3245     }
3246     for child in caps:
3247         if child.tag == "vcpu" and child.get("max"):
3248             result["max_vcpus"] = int(child.get("max"))
3249         elif child.tag == "iothreads":
3250             result["iothreads"] = child.get("supported") == "yes"
3251         elif child.tag == "os":
3252             result["os"] = {}
3253             loader_node = child.find("loader")
3254             if loader_node is not None and loader_node.get("supported") == "yes":
3255                 loader = _parse_caps_loader(loader_node)
3256                 result["os"]["loader"] = loader
3257         elif child.tag == "cpu":
3258             cpu = _parse_caps_cpu(child)
3259             if cpu:
3260                 result["cpu"] = cpu
3261         elif child.tag == "devices":
3262             devices = _parse_caps_devices_features(child)
3263             if devices:
3264                 result["devices"] = devices
3265         elif child.tag == "features":
3266             features = _parse_caps_devices_features(child)
3267             if features:
3268                 result["features"] = features
3269     return result
3270 def domain_capabilities(emulator=None, arch=None, machine=None, domain=None, **kwargs):
3271     conn = __get_conn(**kwargs)
3272     result = []
3273     try:
3274         caps = ElementTree.fromstring(
3275             conn.getDomainCapabilities(emulator, arch, machine, domain, 0)
3276         )
3277         result = _parse_domain_caps(caps)
3278     finally:
3279         conn.close()
3280     return result
3281 def all_capabilities(**kwargs):
3282     conn = __get_conn(**kwargs)
3283     try:
3284         host_caps = ElementTree.fromstring(conn.getCapabilities())
3285         domains = [
3286             [
3287                 (
3288                     guest.get("arch", {}).get("name", None),
3289                     key,
3290                     guest.get("arch", {}).get("emulator", None),
3291                 )
3292                 for key in guest.get("arch", {}).get("domains", {}).keys()
3293             ]
3294             for guest in [
3295                 _parse_caps_guest(guest) for guest in host_caps.findall("guest")
3296             ]
3297         ]
3298         flattened = [pair for item in (x for x in domains) for pair in item]
3299         result = {
3300             "host": {
3301                 "host": _parse_caps_host(host_caps.find("host")),
3302                 "guests": [
3303                     _parse_caps_guest(guest) for guest in host_caps.findall("guest")
3304                 ],
3305             },
3306             "domains": [
3307                 _parse_domain_caps(
3308                     ElementTree.fromstring(
3309                         conn.getDomainCapabilities(emulator, arch, None, domain)
3310                     )
3311                 )
3312                 for (arch, domain, emulator) in flattened
3313             ],
3314         }
3315         return result
3316     finally:
3317         conn.close()
3318 def cpu_baseline(full=False, migratable=False, out="libvirt", **kwargs):
3319     conn = __get_conn(**kwargs)
3320     caps = ElementTree.fromstring(conn.getCapabilities())
3321     cpu = caps.find("host/cpu")
3322     host_cpu_def = xmlutil.element_to_str(cpu)
3323     log.debug("Host CPU model definition: %s", host_cpu_def)
3324     flags = 0
3325     if migratable:
3326         if getattr(libvirt, "VIR_CONNECT_BASELINE_CPU_MIGRATABLE", False):
3327             flags += libvirt.VIR_CONNECT_BASELINE_CPU_MIGRATABLE
3328         else:
3329             conn.close()
3330             raise ValueError
3331     if full and getattr(libvirt, "VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES", False):
3332         flags += libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES
3333     cpu = ElementTree.fromstring(conn.baselineCPU([host_cpu_def], flags))
3334     conn.close()
3335     if full and not getattr(libvirt, "VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES", False):
3336         with salt.utils.files.fopen("/usr/share/libvirt/cpu_map.xml", "r") as cpu_map:
3337             cpu_map = ElementTree.parse(cpu_map)
3338         cpu_model = cpu.find("model").text
3339         while cpu_model:
3340             cpu_map_models = cpu_map.findall("arch/model")
3341             cpu_specs = [
3342                 el
3343                 for el in cpu_map_models
3344                 if el.get("name") == cpu_model and bool(len(el))
3345             ]
3346             if not cpu_specs:
3347                 raise ValueError("Model {} not found in CPU map".format(cpu_model))
3348             elif len(cpu_specs) &gt; 1:
3349                 raise ValueError(
3350                     "Multiple models {} found in CPU map".format(cpu_model)
3351                 )
3352             cpu_specs = cpu_specs[0]
3353             model_node = cpu_specs.find("model")
3354             if model_node is None:
3355                 cpu_model = None
3356             else:
3357                 cpu_model = model_node.get("name")
3358             cpu.extend([feature for feature in cpu_specs.findall("feature")])
3359     if out == "salt":
3360         return {
3361             "model": cpu.find("model").text,
3362             "vendor": cpu.find("vendor").text,
3363             "features": [feature.get("name") for feature in cpu.findall("feature")],
3364         }
3365     return ElementTree.tostring(cpu)
3366 def network_define(
3367     name,
3368     bridge,
3369     forward,
3370     ipv4_config=None,
3371     ipv6_config=None,
3372     vport=None,
3373     tag=None,
3374     autostart=True,
3375     start=True,
3376     mtu=None,
3377     domain=None,
3378     nat=None,
3379     interfaces=None,
3380     addresses=None,
3381     physical_function=None,
3382     dns=None,
3383     **kwargs
3384 ):
3385     conn = __get_conn(**kwargs)
3386     vport = kwargs.get("vport", None)
3387     tag = kwargs.get("tag", None)
3388     net_xml = _gen_net_xml(
3389         name,
3390         bridge,
3391         forward,
3392         vport,
3393         tag=tag,
3394         ip_configs=[config for config in [ipv4_config, ipv6_config] if config],
3395         mtu=mtu,
3396         domain=domain,
3397         nat=nat,
3398         interfaces=interfaces,
3399         addresses=addresses,
3400         physical_function=physical_function,
3401         dns=dns,
3402     )
3403     try:
3404         conn.networkDefineXML(net_xml)
3405     except libvirt.libvirtError as err:
3406         log.warning(err)
3407         conn.close()
3408         raise err  # a real error we should report upwards
3409     try:
3410         network = conn.networkLookupByName(name)
3411     except libvirt.libvirtError as err:
3412         log.warning(err)
3413         conn.close()
3414         raise err  # a real error we should report upwards
3415     if network is None:
3416         conn.close()
3417         return False
3418     if (start or autostart) and network.isActive() != 1:
3419         network.create()
3420     if autostart and network.autostart() != 1:
3421         network.setAutostart(int(autostart))
3422     elif not autostart and network.autostart() == 1:
3423         network.setAutostart(int(autostart))
3424     conn.close()
3425     return True
3426 def _remove_empty_xml_node(node):
3427     for child in node:
3428         if not child.tail and not child.text and not child.items() and not child:
3429             node.remove(child)
3430         else:
3431             _remove_empty_xml_node(child)
3432     return node
3433 def network_update(
3434     name,
3435     bridge,
3436     forward,
3437     ipv4_config=None,
3438     ipv6_config=None,
3439     vport=None,
3440     tag=None,
3441     mtu=None,
3442     domain=None,
3443     nat=None,
3444     interfaces=None,
3445     addresses=None,
3446     physical_function=None,
3447     dns=None,
3448     test=False,
3449     **kwargs
3450 ):
3451     conn = __get_conn(**kwargs)
3452     needs_update = False
3453     try:
3454         net = conn.networkLookupByName(name)
3455         old_xml = ElementTree.fromstring(net.XMLDesc())
3456         new_xml = ElementTree.fromstring(
3457             _gen_net_xml(
3458                 name,
3459                 bridge,
3460                 forward,
3461                 vport,
3462                 tag=tag,
3463                 ip_configs=[config for config in [ipv4_config, ipv6_config] if config],
3464                 mtu=mtu,
3465                 domain=domain,
3466                 nat=nat,
3467                 interfaces=interfaces,
3468                 addresses=addresses,
3469                 physical_function=physical_function,
3470                 dns=dns,
3471             )
3472         )
3473         elements_to_copy = ["uuid", "mac"]
3474         for to_copy in elements_to_copy:
3475             element = old_xml.find(to_copy)
3476             if element is not None:
3477                 new_xml.insert(1, element)
3478         old_xml.attrib.pop("connections", None)
3479         if old_xml.find("forward/pf") is not None:
3480             forward_node = old_xml.find("forward")
3481             address_nodes = forward_node.findall("address")
3482             for node in address_nodes:
3483                 forward_node.remove(node)
3484         default_bridge_attribs = {"stp": "on", "delay": "0"}
3485         old_bridge_node = old_xml.find("bridge")
3486         if old_bridge_node is not None:
3487             for key, value in default_bridge_attribs.items():
3488                 if old_bridge_node.get(key, None) == value:
3489                     old_bridge_node.attrib.pop(key, None)
3490             old_forward = (
3491                 old_xml.find("forward").get("mode")
3492                 if old_xml.find("forward") is not None
3493                 else None
3494             )
3495             if (
3496                 old_forward == forward
3497                 and forward in ["nat", "route", "open", None]
3498                 and bridge is None
3499                 and old_bridge_node.get("name", "").startswith("virbr")
3500             ):
3501                 old_bridge_node.attrib.pop("name", None)
3502         ipv4_nodes = [
3503             node
3504             for node in old_xml.findall("ip")
3505             if node.get("family", "ipv4") == "ipv4"
3506         ]
3507         for ip_node in ipv4_nodes:
3508             netmask = ip_node.attrib.pop("netmask", None)
3509             if netmask:
3510                 address = ipaddress.ip_network(
3511                     "{}/{}".format(ip_node.get("address"), netmask), strict=False
3512                 )
3513                 ip_node.set("prefix", str(address.prefixlen))
3514         for doc in [old_xml, new_xml]:
3515             for node in doc.findall("ip"):
3516                 if "family" not in node.keys():
3517                     node.set("family", "ipv4")
3518         _remove_empty_xml_node(xmlutil.strip_spaces(old_xml))
3519         xmlutil.strip_spaces(new_xml)
3520         needs_update = xmlutil.to_dict(old_xml, True) != xmlutil.to_dict(new_xml, True)
3521         if needs_update and not test:
3522             conn.networkDefineXML(xmlutil.element_to_str(new_xml))
3523     finally:
3524         conn.close()
3525     return needs_update
3526 def list_networks(**kwargs):
3527     conn = __get_conn(**kwargs)
3528     try:
3529         return [net.name() for net in conn.listAllNetworks()]
3530     finally:
3531         conn.close()
3532 def network_info(name=None, **kwargs):
3533     result = {}
3534     conn = __get_conn(**kwargs)
3535     def _net_get_leases(net):
3536         leases = net.DHCPLeases()
3537         for lease in leases:
3538             if lease["type"] == libvirt.VIR_IP_ADDR_TYPE_IPV4:
3539                 lease["type"] = "ipv4"
3540             elif lease["type"] == libvirt.VIR_IP_ADDR_TYPE_IPV6:
3541                 lease["type"] = "ipv6"
3542             else:
3543                 lease["type"] = "unknown"
3544         return leases
3545     def _net_get_bridge(net):
3546         try:
3547             return net.bridgeName()
3548         except libvirt.libvirtError as err:
3549 <a name="4"></a>            return None
3550     try:
3551         nets <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [
3552             net for net in conn.listAllNetworks() if name is None or net.name() == name
3553         ]
3554         result = {
3555             net.name(): {
3556                 "uuid": net.UUIDString(</b></font>),
3557                 "bridge": _net_get_bridge(net),
3558                 "autostart": net.autostart(),
3559                 "active": net.isActive(),
3560                 "persistent": net.isPersistent(),
3561                 "leases": _net_get_leases(net),
3562             }
3563             for net in nets
3564         }
3565     except libvirt.libvirtError as err:
3566         log.debug("Silenced libvirt error: %s", err)
3567     finally:
3568         conn.close()
3569     return result
3570 def network_get_xml(name, **kwargs):
3571     conn = __get_conn(**kwargs)
3572     try:
3573         return conn.networkLookupByName(name).XMLDesc()
3574     finally:
3575         conn.close()
3576 def network_start(name, **kwargs):
3577     conn = __get_conn(**kwargs)
3578     try:
3579         net = conn.networkLookupByName(name)
3580         return not bool(net.create())
3581     finally:
3582         conn.close()
3583 def network_stop(name, **kwargs):
3584     conn = __get_conn(**kwargs)
3585     try:
3586         net = conn.networkLookupByName(name)
3587         return not bool(net.destroy())
3588     finally:
3589         conn.close()
3590 def network_undefine(name, **kwargs):
3591     conn = __get_conn(**kwargs)
3592     try:
3593         net = conn.networkLookupByName(name)
3594         return not bool(net.undefine())
3595     finally:
3596         conn.close()
3597 def network_set_autostart(name, state="on", **kwargs):
3598     conn = __get_conn(**kwargs)
3599     try:
3600         net = conn.networkLookupByName(name)
3601         return not bool(net.setAutostart(1 if state == "on" else 0))
3602     finally:
3603         conn.close()
3604 def _parse_pools_caps(doc):
3605     def _parse_pool_caps(pool):
3606         pool_caps = {
3607             "name": pool.get("type"),
3608             "supported": pool.get("supported", "no") == "yes",
3609         }
3610         for option_kind in ["pool", "vol"]:
3611             options = {}
3612             default_format_node = pool.find(
3613                 "{}Options/defaultFormat".format(option_kind)
3614             )
3615             if default_format_node is not None:
3616                 options["default_format"] = default_format_node.get("type")
3617             options_enums = {
3618                 enum.get("name"): [value.text for value in enum.findall("value")]
3619                 for enum in pool.findall("{}Options/enum".format(option_kind))
3620             }
3621             if options_enums:
3622                 options.update(options_enums)
3623             if options:
3624                 if "options" not in pool_caps:
3625                     pool_caps["options"] = {}
3626                 kind = option_kind if option_kind != "vol" else "volume"
3627                 pool_caps["options"][kind] = options
3628         return pool_caps
3629     return [_parse_pool_caps(pool) for pool in doc.findall("pool")]
3630 def _pool_capabilities(conn):
3631     has_pool_capabilities = bool(getattr(conn, "getStoragePoolCapabilities", None))
3632     if has_pool_capabilities:
3633         caps = ElementTree.fromstring(conn.getStoragePoolCapabilities())
3634         pool_types = _parse_pools_caps(caps)
3635     else:
3636         all_hypervisors = ["xen", "kvm", "bhyve"]
3637         images_formats = [
3638             "none",
3639             "raw",
3640             "dir",
3641             "bochs",
3642             "cloop",
3643             "dmg",
3644             "iso",
3645             "vpc",
3646             "vdi",
3647             "fat",
3648             "vhd",
3649             "ploop",
3650             "cow",
3651             "qcow",
3652             "qcow2",
3653             "qed",
3654             "vmdk",
3655         ]
3656         common_drivers = [
3657             {
3658                 "name": "fs",
3659                 "default_source_format": "auto",
3660                 "source_formats": [
3661                     "auto",
3662                     "ext2",
3663                     "ext3",
3664                     "ext4",
3665                     "ufs",
3666                     "iso9660",
3667                     "udf",
3668                     "gfs",
3669                     "gfs2",
3670                     "vfat",
3671                     "hfs+",
3672                     "xfs",
3673                     "ocfs2",
3674                 ],
3675                 "default_target_format": "raw",
3676                 "target_formats": images_formats,
3677             },
3678             {
3679                 "name": "dir",
3680                 "default_target_format": "raw",
3681                 "target_formats": images_formats,
3682             },
3683             {"name": "iscsi"},
3684             {"name": "scsi"},
3685             {
3686                 "name": "logical",
3687                 "default_source_format": "lvm2",
3688                 "source_formats": ["unknown", "lvm2"],
3689             },
3690             {
3691                 "name": "netfs",
3692                 "default_source_format": "auto",
3693                 "source_formats": ["auto", "nfs", "glusterfs", "cifs"],
3694                 "default_target_format": "raw",
3695                 "target_formats": images_formats,
3696             },
3697             {
3698                 "name": "disk",
3699                 "default_source_format": "unknown",
3700                 "source_formats": [
3701                     "unknown",
3702                     "dos",
3703                     "dvh",
3704                     "gpt",
3705                     "mac",
3706                     "bsd",
3707                     "pc98",
3708                     "sun",
3709                     "lvm2",
3710                 ],
3711                 "default_target_format": "none",
3712                 "target_formats": [
3713                     "none",
3714                     "linux",
3715                     "fat16",
3716                     "fat32",
3717                     "linux-swap",
3718                     "linux-lvm",
3719                     "linux-raid",
3720                     "extended",
3721                 ],
3722             },
3723             {"name": "mpath"},
3724             {"name": "rbd", "default_target_format": "raw", "target_formats": []},
3725             {
3726                 "name": "sheepdog",
3727                 "version": 10000,
3728                 "hypervisors": ["kvm"],
3729                 "default_target_format": "raw",
3730                 "target_formats": images_formats,
3731             },
3732             {
3733                 "name": "gluster",
3734                 "version": 1002000,
3735                 "hypervisors": ["kvm"],
3736                 "default_target_format": "raw",
3737                 "target_formats": images_formats,
3738             },
3739             {"name": "zfs", "version": 1002008, "hypervisors": ["bhyve"]},
3740             {
3741                 "name": "iscsi-direct",
3742                 "version": 4007000,
3743                 "hypervisors": ["kvm", "xen"],
3744             },
3745         ]
3746         libvirt_version = conn.getLibVersion()
3747         hypervisor = get_hypervisor()
3748         def _get_backend_output(backend):
3749             output = {
3750                 "name": backend["name"],
3751                 "supported": (
3752                     not backend.get("version") or libvirt_version &gt;= backend["version"]
3753                 )
3754                 and hypervisor in backend.get("hypervisors", all_hypervisors),
3755                 "options": {
3756                     "pool": {
3757                         "default_format": backend.get("default_source_format"),
3758                         "sourceFormatType": backend.get("source_formats"),
3759                     },
3760                     "volume": {
3761                         "default_format": backend.get("default_target_format"),
3762                         "targetFormatType": backend.get("target_formats"),
3763                     },
3764                 },
3765             }
3766             for option_kind in ["pool", "volume"]:
3767                 if not [
3768                     value
3769                     for value in output["options"][option_kind].values()
3770                     if value is not None
3771                 ]:
3772                     del output["options"][option_kind]
3773             if not output["options"]:
3774                 del output["options"]
3775             return output
3776         pool_types = [_get_backend_output(backend) for backend in common_drivers]
3777     return {
3778         "computed": not has_pool_capabilities,
3779         "pool_types": pool_types,
3780     }
3781 def pool_capabilities(**kwargs):
3782     try:
3783         conn = __get_conn(**kwargs)
3784         return _pool_capabilities(conn)
3785     finally:
3786         conn.close()
3787 def pool_define(
3788     name,
3789     ptype,
3790     target=None,
3791     permissions=None,
3792     source_devices=None,
3793     source_dir=None,
3794     source_initiator=None,
3795     source_adapter=None,
3796     source_hosts=None,
3797     source_auth=None,
3798     source_name=None,
3799     source_format=None,
3800     transient=False,
3801     start=True,  # pylint: disable=redefined-outer-name
3802     **kwargs
3803 ):
3804     conn = __get_conn(**kwargs)
3805     auth = _pool_set_secret(conn, ptype, name, source_auth)
3806     pool_xml = _gen_pool_xml(
3807         name,
3808         ptype,
3809         target,
3810         permissions=permissions,
3811         source_devices=source_devices,
3812         source_dir=source_dir,
3813         source_adapter=source_adapter,
3814         source_hosts=source_hosts,
3815         source_auth=auth,
3816         source_name=source_name,
3817         source_format=source_format,
3818         source_initiator=source_initiator,
3819     )
3820     try:
3821         if transient:
3822             pool = conn.storagePoolCreateXML(pool_xml)
3823         else:
3824             pool = conn.storagePoolDefineXML(pool_xml)
3825             if start:
3826                 pool.create()
3827     except libvirt.libvirtError as err:
3828         raise err  # a real error we should report upwards
3829     finally:
3830         conn.close()
3831     return True
3832 def _pool_set_secret(
3833     conn, pool_type, pool_name, source_auth, uuid=None, usage=None, test=False
3834 ):
3835     secret_types = {"rbd": "ceph", "iscsi": "chap", "iscsi-direct": "chap"}
3836     secret_type = secret_types.get(pool_type)
3837     auth = source_auth
3838     if source_auth and "username" in source_auth and "password" in source_auth:
3839         if secret_type:
3840             secret = None
3841             try:
3842                 if usage:
3843                     usage_type = (
3844                         libvirt.VIR_SECRET_USAGE_TYPE_CEPH
3845                         if secret_type == "ceph"
3846                         else libvirt.VIR_SECRET_USAGE_TYPE_ISCSI
3847                     )
3848                     secret = conn.secretLookupByUsage(usage_type, usage)
3849                 elif uuid:
3850                     secret = conn.secretLookupByUUIDString(uuid)
3851             except libvirt.libvirtError as err:
3852                 log.info("Secret not found: %s", err.get_error_message())
3853             if not secret:
3854                 description = "Passphrase for {} pool created by Salt".format(pool_name)
3855                 if not usage:
3856                     usage = "pool_{}".format(pool_name)
3857                 secret_xml = _gen_secret_xml(secret_type, usage, description)
3858                 if not test:
3859                     secret = conn.secretDefineXML(secret_xml)
3860             password = auth["password"]
3861             if pool_type == "rbd":
3862                 password = base64.b64decode(salt.utils.stringutils.to_bytes(password))
3863             if not test:
3864                 secret.setValue(password)
3865             auth["type"] = secret_type
3866             auth["secret"] = {
3867                 "type": "uuid" if uuid else "usage",
3868                 "value": uuid if uuid else usage,
3869             }
3870     return auth
3871 def pool_update(
3872     name,
3873     ptype,
3874     target=None,
3875     permissions=None,
3876     source_devices=None,
3877     source_dir=None,
3878     source_initiator=None,
3879     source_adapter=None,
3880     source_hosts=None,
3881     source_auth=None,
3882     source_name=None,
3883     source_format=None,
3884     test=False,
3885     **kwargs
3886 ):
3887     conn = __get_conn(**kwargs)
3888     needs_update = False
3889     try:
3890         pool = conn.storagePoolLookupByName(name)
3891         old_xml = ElementTree.fromstring(pool.XMLDesc())
3892         secret_node = old_xml.find("source/auth/secret")
3893         usage = secret_node.get("usage") if secret_node is not None else None
3894         uuid = secret_node.get("uuid") if secret_node is not None else None
3895         auth = _pool_set_secret(
3896             conn, ptype, name, source_auth, uuid=uuid, usage=usage, test=test
3897         )
3898         new_xml = ElementTree.fromstring(
3899             _gen_pool_xml(
3900                 name,
3901                 ptype,
3902                 target,
3903                 permissions=permissions,
3904                 source_devices=source_devices,
3905                 source_dir=source_dir,
3906                 source_initiator=source_initiator,
3907                 source_adapter=source_adapter,
3908                 source_hosts=source_hosts,
3909                 source_auth=auth,
3910                 source_name=source_name,
3911                 source_format=source_format,
3912             )
3913         )
3914         elements_to_copy = ["available", "allocation", "capacity", "uuid"]
3915         for to_copy in elements_to_copy:
3916             element = old_xml.find(to_copy)
3917             new_xml.insert(1, element)
3918         _remove_empty_xml_node(xmlutil.strip_spaces(old_xml))
3919         xmlutil.strip_spaces(new_xml)
3920         needs_update = xmlutil.to_dict(old_xml, True) != xmlutil.to_dict(new_xml, True)
3921         if needs_update and not test:
3922             conn.storagePoolDefineXML(xmlutil.element_to_str(new_xml))
3923     finally:
3924         conn.close()
3925     return needs_update
3926 def list_pools(**kwargs):
3927     conn = __get_conn(**kwargs)
3928     try:
3929         return [pool.name() for pool in conn.listAllStoragePools()]
3930     finally:
3931         conn.close()
3932 def pool_info(name=None, **kwargs):
3933     result = {}
3934     conn = __get_conn(**kwargs)
3935     def _pool_extract_infos(pool):
3936         states = ["inactive", "building", "running", "degraded", "inaccessible"]
3937         infos = pool.info()
3938         state = states[infos[0]] if infos[0] &lt; len(states) else "unknown"
3939         desc = ElementTree.fromstring(pool.XMLDesc())
3940         path_node = desc.find("target/path")
3941         return {
3942             "uuid": pool.UUIDString(),
3943             "state": state,
3944             "capacity": infos[1],
3945             "allocation": infos[2],
3946             "free": infos[3],
3947             "autostart": pool.autostart(),
3948             "persistent": pool.isPersistent(),
3949             "target_path": path_node.text if path_node is not None else None,
3950             "type": desc.get("type"),
3951         }
3952     try:
3953         pools = [
3954             pool
3955             for pool in conn.listAllStoragePools()
3956             if name is None or pool.name() == name
3957         ]
3958         result = {pool.name(): _pool_extract_infos(pool) for pool in pools}
3959     except libvirt.libvirtError as err:
3960         log.debug("Silenced libvirt error: %s", err)
3961     finally:
3962         conn.close()
3963     return result
3964 def pool_get_xml(name, **kwargs):
3965     conn = __get_conn(**kwargs)
3966     try:
3967         return conn.storagePoolLookupByName(name).XMLDesc()
3968     finally:
3969         conn.close()
3970 def pool_start(name, **kwargs):
3971     conn = __get_conn(**kwargs)
3972     try:
3973         pool = conn.storagePoolLookupByName(name)
3974         return not bool(pool.create())
3975     finally:
3976         conn.close()
3977 def pool_build(name, **kwargs):
3978     conn = __get_conn(**kwargs)
3979     try:
3980         pool = conn.storagePoolLookupByName(name)
3981         return not bool(pool.build())
3982     finally:
3983         conn.close()
3984 def pool_stop(name, **kwargs):
3985     conn = __get_conn(**kwargs)
3986     try:
3987         pool = conn.storagePoolLookupByName(name)
3988         return not bool(pool.destroy())
3989     finally:
3990         conn.close()
3991 def pool_undefine(name, **kwargs):
3992     conn = __get_conn(**kwargs)
3993     try:
3994         pool = conn.storagePoolLookupByName(name)
3995         desc = ElementTree.fromstring(pool.XMLDesc())
3996         auth_node = desc.find("source/auth")
3997         if auth_node is not None:
3998             auth_types = {
3999                 "ceph": libvirt.VIR_SECRET_USAGE_TYPE_CEPH,
4000                 "iscsi": libvirt.VIR_SECRET_USAGE_TYPE_ISCSI,
4001             }
4002             secret_type = auth_types[auth_node.get("type")]
4003             secret_usage = auth_node.find("secret").get("usage")
4004             if secret_type and "pool_{}".format(name) == secret_usage:
4005                 secret = conn.secretLookupByUsage(secret_type, secret_usage)
4006                 secret.undefine()
4007         return not bool(pool.undefine())
4008     finally:
4009         conn.close()
4010 def pool_delete(name, **kwargs):
4011     conn = __get_conn(**kwargs)
4012     try:
4013         pool = conn.storagePoolLookupByName(name)
4014         return not bool(pool.delete(libvirt.VIR_STORAGE_POOL_DELETE_NORMAL))
4015     finally:
4016         conn.close()
4017 def pool_refresh(name, **kwargs):
4018     conn = __get_conn(**kwargs)
4019     try:
4020         pool = conn.storagePoolLookupByName(name)
4021         return not bool(pool.refresh())
4022     finally:
4023         conn.close()
4024 def pool_set_autostart(name, state="on", **kwargs):
4025     conn = __get_conn(**kwargs)
4026     try:
4027         pool = conn.storagePoolLookupByName(name)
4028         return not bool(pool.setAutostart(1 if state == "on" else 0))
4029     finally:
4030         conn.close()
4031 def pool_list_volumes(name, **kwargs):
4032     conn = __get_conn(**kwargs)
4033     try:
4034         pool = conn.storagePoolLookupByName(name)
4035         return pool.listVolumes()
4036     finally:
4037         conn.close()
4038 def _get_storage_vol(conn, pool, vol):
4039     pool_obj = conn.storagePoolLookupByName(pool)
4040     return pool_obj.storageVolLookupByName(vol)
4041 def _is_valid_volume(vol):
4042     try:
4043         def discarder(ctxt, error):  # pylint: disable=unused-argument
4044             log.debug("Ignore libvirt error: %s", error[2])
4045         libvirt.registerErrorHandler(discarder, None)
4046         vol.info()
4047         libvirt.registerErrorHandler(None, None)
4048         return True
4049     except libvirt.libvirtError as err:
4050         return False
4051 def _get_all_volumes_paths(conn):
4052     pools = [
4053         pool
4054         for pool in conn.listAllStoragePools()
4055         if pool.info()[0] == libvirt.VIR_STORAGE_POOL_RUNNING
4056     ]
4057     volumes = {}
4058     for pool in pools:
4059         pool_volumes = {
4060             volume.path(): {
4061                 "pool": pool.name(),
4062                 "name": volume.name(),
4063                 "backing_stores": [
4064                     path.text
4065                     for path in ElementTree.fromstring(volume.XMLDesc()).findall(
4066                         ".//backingStore/path"
4067                     )
4068                 ],
4069             }
4070             for volume in pool.listAllVolumes()
4071             if _is_valid_volume(volume)
4072         }
4073         volumes.update(pool_volumes)
4074     return volumes
4075 def volume_infos(pool=None, volume=None, **kwargs):
4076     result = {}
4077     conn = __get_conn(**kwargs)
4078     try:
4079         backing_stores = _get_all_volumes_paths(conn)
4080         try:
4081             domains = _get_domain(conn)
4082             domains_list = domains if isinstance(domains, list) else [domains]
4083         except CommandExecutionError:
4084             domains_list = []
4085         disks = {
4086             domain.name(): {
4087                 node.get("file")
4088                 for node in ElementTree.fromstring(domain.XMLDesc(0)).findall(
4089                     ".//disk/source/[@file]"
4090                 )
4091             }
4092             for domain in domains_list
4093         }
4094         def _volume_extract_infos(vol):
4095             types = ["file", "block", "dir", "network", "netdir", "ploop"]
4096             infos = vol.info()
4097             vol_xml = ElementTree.fromstring(vol.XMLDesc())
4098             backing_store_path = vol_xml.find("./backingStore/path")
4099             backing_store_format = vol_xml.find("./backingStore/format")
4100             backing_store = None
4101             if backing_store_path is not None:
4102                 backing_store = {
4103                     "path": backing_store_path.text,
4104                     "format": backing_store_format.get("type")
4105                     if backing_store_format is not None
4106                     else None,
4107                 }
4108             format_node = vol_xml.find("./target/format")
4109             used_by = []
4110             if vol.path():
4111                 as_backing_store = {
4112                     path
4113                     for (path, volume) in backing_stores.items()
4114                     if vol.path() in volume.get("backing_stores")
4115                 }
4116                 used_by = [
4117                     vm_name
4118                     for (vm_name, vm_disks) in disks.items()
4119                     if vm_disks &amp; as_backing_store or vol.path() in vm_disks
4120                 ]
4121             return {
4122                 "type": types[infos[0]] if infos[0] &lt; len(types) else "unknown",
4123                 "key": vol.key(),
4124                 "path": vol.path(),
4125                 "capacity": infos[1],
4126                 "allocation": infos[2],
4127                 "used_by": used_by,
4128                 "backing_store": backing_store,
4129                 "format": format_node.get("type") if format_node is not None else None,
4130             }
4131         pools = [
4132             obj
4133             for obj in conn.listAllStoragePools()
4134             if (pool is None or obj.name() == pool)
4135             and obj.info()[0] == libvirt.VIR_STORAGE_POOL_RUNNING
4136         ]
4137         vols = {
4138             pool_obj.name(): {
4139                 vol.name(): _volume_extract_infos(vol)
4140                 for vol in pool_obj.listAllVolumes()
4141                 if (volume is None or vol.name() == volume) and _is_valid_volume(vol)
4142             }
4143             for pool_obj in pools
4144         }
4145         return {pool_name: volumes for (pool_name, volumes) in vols.items() if volumes}
4146     except libvirt.libvirtError as err:
4147         log.debug("Silenced libvirt error: %s", err)
4148     finally:
4149         conn.close()
4150     return result
4151 def volume_delete(pool, volume, **kwargs):
4152     conn = __get_conn(**kwargs)
4153     try:
4154         vol = _get_storage_vol(conn, pool, volume)
4155         return not bool(vol.delete())
4156     finally:
4157         conn.close()
4158 def volume_define(
4159     pool,
4160     name,
4161     size,
4162     allocation=0,
4163     format=None,
4164     type=None,
4165     permissions=None,
4166     backing_store=None,
4167     nocow=False,
4168     **kwargs
4169 ):
4170     ret = False
4171     try:
4172         conn = __get_conn(**kwargs)
4173         pool_obj = conn.storagePoolLookupByName(pool)
4174         pool_type = ElementTree.fromstring(pool_obj.XMLDesc()).get("type")
4175         new_allocation = allocation
4176         if pool_type == "logical" and size != allocation:
4177             new_allocation = size
4178         xml = _gen_vol_xml(
4179             name,
4180             size,
4181             format=format,
4182             allocation=new_allocation,
4183             type=type,
4184             permissions=permissions,
4185             backing_store=backing_store,
4186             nocow=nocow,
4187         )
4188         ret = _define_vol_xml_str(conn, xml, pool=pool)
4189     except libvirt.libvirtError as err:
4190         raise CommandExecutionError(err.get_error_message())
4191     finally:
4192         conn.close()
4193     return ret
4194 def _volume_upload(conn, pool, volume, file, offset=0, length=0, sparse=False):
4195     def handler(stream, nbytes, opaque):
4196         return os.read(opaque, nbytes)
4197     def holeHandler(stream, opaque):
4198         fd = opaque
4199         cur = os.lseek(fd, 0, os.SEEK_CUR)
4200         try:
4201             data = os.lseek(fd, cur, os.SEEK_DATA)
4202         except OSError as e:
4203             if e.errno != 6:
4204                 raise e
4205             else:
4206                 data = -1
4207         if data &lt; 0:
4208             inData = False
4209             eof = os.lseek(fd, 0, os.SEEK_END)
4210             if eof &lt; cur:
4211                 raise RuntimeError("Current position in file after EOF: {}".format(cur))
4212             sectionLen = eof - cur
4213         else:
4214             if data &gt; cur:
4215                 inData = False
4216                 sectionLen = data - cur
4217             else:
4218                 inData = True
4219                 hole = os.lseek(fd, data, os.SEEK_HOLE)
4220                 if hole &lt; 0:
4221                     raise RuntimeError("No trailing hole")
4222                 if hole == data:
4223                     raise RuntimeError("Impossible happened")
4224                 else:
4225                     sectionLen = hole - data
4226         os.lseek(fd, cur, os.SEEK_SET)
4227         return [inData, sectionLen]
4228     def skipHandler(stream, length, opaque):
4229         return os.lseek(opaque, length, os.SEEK_CUR)
4230     stream = None
4231     fd = None
4232     ret = False
4233     try:
4234         pool_obj = conn.storagePoolLookupByName(pool)
4235         vol_obj = pool_obj.storageVolLookupByName(volume)
4236         stream = conn.newStream()
4237         fd = os.open(file, os.O_RDONLY)
4238         vol_obj.upload(
4239             stream,
4240             offset,
4241             length,
4242             libvirt.VIR_STORAGE_VOL_UPLOAD_SPARSE_STREAM if sparse else 0,
4243         )
4244         if sparse:
4245             stream.sparseSendAll(handler, holeHandler, skipHandler, fd)
4246         else:
4247             stream.sendAll(handler, fd)
4248         ret = True
4249     except libvirt.libvirtError as err:
4250         raise CommandExecutionError(err.get_error_message())
4251     finally:
4252         if fd:
4253             try:
4254                 os.close(fd)
4255             except OSError as err:
4256                 if stream:
4257                     stream.abort()
4258                 if ret:
4259                     raise CommandExecutionError(
4260                         "Failed to close file: {}".format(err.strerror)
4261                     )
4262         if stream:
4263             try:
4264                 stream.finish()
4265             except libvirt.libvirtError as err:
4266                 if ret:
4267                     raise CommandExecutionError(
4268                         "Failed to finish stream: {}".format(err.get_error_message())
4269                     )
4270     return ret
4271 def volume_upload(pool, volume, file, offset=0, length=0, sparse=False, **kwargs):
4272     conn = __get_conn(**kwargs)
4273     ret = False
4274     try:
4275         ret = _volume_upload(
4276             conn, pool, volume, file, offset=offset, length=length, sparse=sparse
4277         )
4278     finally:
4279         conn.close()
4280     return ret
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
