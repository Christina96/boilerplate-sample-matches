<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for master.py &amp; virt_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for master.py &amp; virt_1.py
      </h3>
<h1 align="center">
        2.0%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>master.py (3.2759109%)<th>virt_1.py (1.4954234%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(40-73)<td><a href="#" name="0">(122-152)</a><td align="center"><font color="#ff0000">29</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1555-1563)<td><a href="#" name="1">(2948-2957)</a><td align="center"><font color="#b80000">21</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(1068-1078)<td><a href="#" name="2">(6459-6469)</a><td align="center"><font color="#950000">17</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(658-665)<td><a href="#" name="3">(216-231)</a><td align="center"><font color="#720000">13</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(2131-2133)<td><a href="#" name="4">(7509-7514)</a><td align="center"><font color="#690000">12</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(328-333)<td><a href="#" name="5">(249-253)</a><td align="center"><font color="#690000">12</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(221-225)<td><a href="#" name="6">(1480-1487)</a><td align="center"><font color="#690000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>master.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 import collections
2 import copy
3 import ctypes
4 import functools
5 import logging
6 import multiprocessing
7 import os
8 import re
9 import signal
10 import stat
11 import sys
12 import threading
13 import time
14 import salt.acl
15 import salt.auth
16 import salt.channel.server
17 import salt.client
18 import salt.client.ssh.client
19 import salt.crypt
20 import salt.daemons.masterapi
21 import salt.defaults.exitcodes
22 import salt.engines
23 import salt.exceptions
24 import salt.ext.tornado.gen
25 import salt.key
26 import salt.minion
27 import salt.payload
28 import salt.pillar
29 import salt.runner
30 import salt.serializers.msgpack
31 import salt.state
32 import salt.utils.atomicfile
33 import salt.utils.crypt
34 <font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import salt.utils.event
35 import salt.utils.files
36 import salt.utils.gitfs
37 import salt.utils.gzip_util
38 import salt.utils.jid
39 import salt.utils.job
40 import salt.utils.master
41 import salt.utils.minions
42 import salt.utils.platform
43 import salt.utils.process
44 import salt.utils.schedule
45 import salt.utils.ssdp
46 import salt.utils.stringutils
47 import salt.utils.user
48 import salt.utils.verify
49 import salt.utils.zeromq
50 import salt.wheel
51 from salt.config import DEFAULT_INTERVAL
52 from salt.defaults import DEFAULT_TARGET_DELIM
53 from salt.ext.tornado.stack_context import StackContext
54 from salt.transport import TRANSPORTS
55 from salt.utils.channel import iter_transport_opts
56 from salt.utils.ctx import RequestContext
57 from salt.utils.debug import (
58     enable_sigusr1_handler,
59     enable_sigusr2_handler,
60     inspect_stack,
61 )
62 from salt.utils.event import tagify
63 from salt.utils.odict import OrderedDict
64 from salt.utils.zeromq import ZMQ_VERSION_INFO, zmq
65 try:
66     import</b></font> resource
67     HAS_RESOURCE = True
68 except ImportError:
69     HAS_RESOURCE = False
70 log = logging.getLogger(__name__)
71 class SMaster:
72     secrets = (
73         {}
74     )  # mapping of key -&gt; {'secret': multiprocessing type, 'reload': FUNCTION}
75     def __init__(self, opts):
76         self.opts = opts
77         self.master_key = salt.crypt.MasterKeys(self.opts)
78         self.key = self.__prep_key()
79     def __setstate__(self, state):
80         super().__setstate__(state)
81         self.master_key = state["master_key"]
82         self.key = state["key"]
83         SMaster.secrets = state["secrets"]
84     def __getstate__(self):
85         state = super().__getstate__()
86         state.update(
87             {
88                 "key": self.key,
89                 "master_key": self.master_key,
90                 "secrets": SMaster.secrets,
91             }
92         )
93         return state
94     def __prep_key(self):
95         return salt.daemons.masterapi.access_keys(self.opts)
96 class Maintenance(salt.utils.process.SignalHandlingProcess):
97     def __init__(self, opts, **kwargs):
98         super().__init__(**kwargs)
99         self.opts = opts
100         self.loop_interval = int(self.opts["loop_interval"])
101         self.rotate = int(time.time())
102     def _post_fork_init(self):
103         ropts = dict(self.opts)
104         ropts["quiet"] = True
105         runner_client = salt.runner.RunnerClient(ropts)
106         self.returners = salt.loader.returners(self.opts, {})
107         self.schedule = salt.utils.schedule.Schedule(
108             self.opts, runner_client.functions_dict(), returners=self.returners
109         )
110         self.ckminions = salt.utils.minions.CkMinions(self.opts)
111         self.event = salt.utils.event.get_master_event(
112             self.opts, self.opts["sock_dir"], listen=False
113         )
114         self.git_pillar = salt.daemons.masterapi.init_git_pillar(self.opts)
115         if self.opts["maintenance_niceness"] and not salt.utils.platform.is_windows():
116             log.info(
117                 "setting Maintenance niceness to %d", self.opts["maintenance_niceness"]
118             )
119             os.nice(self.opts["maintenance_niceness"])
120         self.presence_events = False
121         if self.opts.get("presence_events", False):
122             tcp_only = True
123             for transport, _ in iter_transport_opts(self.opts):
124                 if transport != "tcp":
125                     tcp_only = False
126             if not tcp_only:
127                 self.presence_events = True
128     def run(self):
129         self._post_fork_init()
130         last = int(time.time())
131         last_git_pillar_update = 0
132         git_pillar_update_interval = self.opts.get("git_pillar_update_interval", 0)
133         old_present = set()
134         while True:
135             now = int(time.time())
136             if (now - last) &gt;= self.loop_interval:
137                 salt.daemons.masterapi.clean_old_jobs(self.opts)
138                 salt.daemons.masterapi.clean_expired_tokens(self.opts)
139                 salt.daemons.masterapi.clean_pub_auth(self.opts)
140                 last_git_pillar_update = now
141                 self.handle_git_pillar()
142             self<font color="#8c8774"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.handle_schedule()
143             self.handle_key_cache()
144             self.handle_presence(old_present)
145             self.handle_key_rotate(now)
146             salt.utils.verify.check_max_open_files(</b></font>self.opts)
147             last = now
148             time.sleep(self.loop_interval)
149     def handle_key_cache(self):
150         if self.opts["key_cache"] == "sched":
151             keys = []
152             if self.opts["transport"] in TRANSPORTS:
153                 acc = "minions"
154             else:
155                 acc = "accepted"
156             for fn_ in os.listdir(os.path.join(self.opts["pki_dir"], acc)):
157                 if not fn_.startswith(".") and os.path.isfile(
158                     os.path.join(self.opts["pki_dir"], acc, fn_)
159                 ):
160                     keys.append(fn_)
161             log.debug("Writing master key cache")
162             with salt.utils.atomicfile.atomic_open(
163                 os.path.join(self.opts["pki_dir"], acc, ".key_cache"), mode="wb"
164             ) as cache_file:
165                 salt.payload.dump(keys, cache_file)
166     def handle_key_rotate(self, now):
167         to_rotate = False
168         dfn = os.path.join(self.opts["cachedir"], ".dfn")
169         try:
170             stats = os.stat(dfn)
171             if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):
172                 to_rotate = True
173                 os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
174             elif stats.st_mode == 0o100400:
175                 to_rotate = True
176             else:
177                 log.error("Found dropfile with incorrect permissions, ignoring...")
178             os.remove(dfn)
179         except os.error:
180             pass
181         if self.opts.get("publish_session"):
182             if now - self.rotate &gt;= self.opts["publish_session"]:
183                 to_rotate = True
184         if to_rotate:
185             log.info("Rotating master AES key")
186             for secret_key, secret_map in SMaster.secrets.items():
187                 with secret_map["secret"].get_lock():
188                     secret_map["secret"].value = salt.utils.stringutils.to_bytes(
189                         secret_map["reload"]()
190                     )
191                 self.event.fire_event(
192                     {"rotate_{}_key".format(secret_key): True}, tag="key"
193                 )
194             self.rotate = now
195             if self.opts.get("ping_on_rotate"):
196                 log.debug("Pinging all connected minions due to key rotation")
197                 salt.utils.master.ping_all_connected_minions(self.opts)
198     def handle_git_pillar(self):
199         try:
200             for pillar in self.git_pillar:
201                 pillar.fetch_remotes()
202         except Exception as exc:  # pylint: disable=broad-except
203             log.error("Exception caught while updating git_pillar", exc_info=True)
204     def handle_schedule(self):
205         try:
206             self.schedule.eval()
207             if self.schedule.loop_interval &lt; self.loop_interval:
208                 self.loop_interval = self.schedule.loop_interval
209         except Exception as exc:  # pylint: disable=broad-except
210             log.error("Exception %s occurred in scheduled job", exc)
211         self.schedule.cleanup_subprocesses()
212     def handle_presence(self, old_present):
213         if self.presence_events and self.event.connect_pull(timeout=3):
214             present = self.ckminions<font color="#151b8d"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.connected_ids()
215             new = present.difference(old_present)
216             lost = old_present.difference(present)
217             if new or lost:
218                 data = {"new": list(</b></font>new), "lost": list(lost)}
219                 self.event.fire_event(data, tagify("change", "presence"))
220             data = {"present": list(present)}
221             self.event.fire_event(data, tagify("present", "presence"))
222             old_present.clear()
223             old_present.update(present)
224 class FileserverUpdate(salt.utils.process.SignalHandlingProcess):
225     def __init__(self, opts, **kwargs):
226         super().__init__(**kwargs)
227         self.opts = opts
228         self.update_threads = {}
229         import salt.fileserver
230         self.fileserver = salt.fileserver.Fileserver(self.opts)
231         self.fill_buckets()
232     def fill_buckets(self):
233         update_intervals = self.fileserver.update_intervals()
234         self.buckets = {}
235         for backend in self.fileserver.backends():
236             fstr = "{}.update".format(backend)
237             try:
238                 update_func = self.fileserver.servers[fstr]
239             except KeyError:
240                 log.debug("No update function for the %s filserver backend", backend)
241                 continue
242             if backend in update_intervals:
243                 for id_, interval in update_intervals[backend].items():
244                     if not interval:
245                         interval = DEFAULT_INTERVAL
246                         log.debug(
247                             "An update_interval of 0 is not supported, "
248                             "falling back to %s",
249                             interval,
250                         )
251                     i_ptr = self.buckets.setdefault(interval, OrderedDict())
252                     i_ptr.setdefault((backend, update_func), []).append(id_)
253             else:
254                 try:
255                     interval_key = "{}_update_interval".format(backend)
256                     interval = self.opts[interval_key]
257                 except KeyError:
258                     interval = DEFAULT_INTERVAL
259                     log.warning(
260                         "%s key missing from configuration. Falling back to "
261                         "default interval of %d seconds",
262                         interval_key,
263                         interval,
264                     )
265                 self.buckets.setdefault(interval, OrderedDict())[
266                     (backend, update_func)
267                 ] = None
268     @staticmethod
269     def _do_update(backends):
270         for backend, update_args in backends.items():
271             backend_name, update_func = backend
272             try:
273                 if update_args:
274                     log.debug(
275                         "Updating %s fileserver cache for the following targets: %s",
276                         backend_name,
277                         update_args,
278                     )
279                     args = (update_args,)
280                 else:
281                     log.debug("Updating %s fileserver cache", backend_name)
282                     args = ()
283                 update_func(*args)
284             except Exception as exc:  # pylint: disable=broad-except
285                 log.exception(
286                     "Uncaught exception while updating %s fileserver cache",
287                     backend_name,
288                 )
289     @classmethod
290     def update(cls, interval, backends, timeout=300):
291         start = time.time()
292         condition = threading.Condition()
293         while time.time() - start &lt; timeout:
294             log.debug(
295                 "Performing fileserver updates for items with an update interval of %d",
296                 interval,
297             )
298             cls._do_update(backends)
299             log.debug(
300                 "Completed fileserver updates for items with an update "
301                 "interval of %d, waiting %d seconds",
302                 interval,
303                 interval,
304             )
305             with condition:
306                 condition.wait(interval)
307     def run(self):
308         if (
309             self.opts["fileserver_update_niceness"]
310             and not salt.utils.platform.is_windows()
311         ):
312             log.info(
313                 "setting FileServerUpdate niceness to %d",
314                 self.opts["fileserver_update_niceness"],
315             )
316             os.nice(self.opts["fileserver_update_niceness"])
317         salt.daemons.masterapi.clean_fsbackend(self.opts)
318         for interval in self.buckets:
319             self.update_threads[interval] = threading.Thread(
320                 target=self.update,
321                 args=(interval, self.buckets[interval]),
322             )
323             self.update_threads[interval].start()
324         while self.update_threads:
325             for name, thread in list(self.update_threads.items()):
326                 thread.join(1)
327                 if not thread.is_alive():
328                     self.update_threads.pop(name)
329 class Master(SMaster):
330     def __init__(self, opts):
331         if zmq and ZMQ_VERSION_INFO &lt; (3, 2):
332             log.warning(
333                 "You have a version of ZMQ less than ZMQ 3.2! There are "
334                 "known connection keep-alive issues with ZMQ &lt; 3.2 which "
335                 "may result in loss of contact with minions. Please "
336                 "upgrade your ZMQ!"
337             )
338         SMaster.__init__(self, opts)
339     def __set_max_open_files(self):
340         if not HAS_RESOURCE:
341             return
342         mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)
343         if mof_h == resource.RLIM_INFINITY:
344             mof_h = mof_s
345         log.info(
346             "Current values for max open files soft/hard setting: %s/%s", mof_s, mof_h
347         )
348         mof_c = self.opts["max_open_files"]
349         if mof_c &gt; mof_h:
350             log.info(
351                 "The value for the 'max_open_files' setting, %s, is higher "
352                 "than the highest value the user running salt is allowed to "
353                 "set (%s). Defaulting to %s.",
354                 mof_c,
355                 mof_h,
356                 mof_h,
357             )
358             mof_c = mof_h
359         if mof_s &lt; mof_c:
360             log.info("Raising max open files value to %s", mof_c)
361             resource.setrlimit(resource.RLIMIT_NOFILE, (mof_c, mof_h))
362             try:
363                 mof_s, mof_h = resource.getrlimit(resource.RLIMIT_NOFILE)
364                 log.info(
365                     "New values for max open files soft/hard values: %s/%s",
366                     mof_s,
367                     mof_h,
368                 )
369             except ValueError:
370                 log.critical(
371                     "Failed to raise max open files setting to %s. If this "
372                     "value is too low, the salt-master will most likely fail "
373                     "to run properly.",
374                     mof_c,
375                 )
376     def _pre_flight(self):
377         errors = []
378         critical_errors = []
379         try:
380             os.chdir("/")
381         except OSError as err:
382             errors.append("Cannot change to root directory ({})".format(err))
383         if self.opts.get("fileserver_verify_config", True):
384             import salt.fileserver
385             fileserver = salt.fileserver.Fileserver(self.opts)
386             if not fileserver.servers:
387                 errors.append(
388                     "Failed to load fileserver backends, the configured backends "
389                     "are: {}".format(", ".join(self.opts["fileserver_backend"]))
390                 )
391             else:
392                 try:
393                     fileserver.init()
394                 except salt.exceptions.FileserverConfigError as exc:
395                     critical_errors.append("{}".format(exc))
396         if not self.opts["fileserver_backend"]:
397             errors.append("No fileserver backends are configured")
398         if self.opts["pillar_cache"] and not os.path.isdir(
399             os.path.join(self.opts["cachedir"], "pillar_cache")
400         ):
401             try:
402                 with salt.utils.files.set_umask(0o077):
403                     os.mkdir(os.path.join(self.opts["cachedir"], "pillar_cache"))
404             except OSError:
405                 pass
406         if self.opts.get("git_pillar_verify_config", True):
407             try:
408                 git_pillars = [
409                     x
410                     for x in self.opts.get("ext_pillar", [])
411                     if "git" in x and not isinstance(x["git"], str)
412                 ]
413             except TypeError:
414                 git_pillars = []
415                 critical_errors.append(
416                     "Invalid ext_pillar configuration. It is likely that the "
417                     "external pillar type was not specified for one or more "
418                     "external pillars."
419                 )
420             if git_pillars:
421                 try:
422                     new_opts = copy.deepcopy(self.opts)
423                     import salt.pillar.git_pillar
424                     for repo in git_pillars:
425                         new_opts["ext_pillar"] = [repo]
426                         try:
427                             git_pillar = salt.utils.gitfs.GitPillar(
428                                 new_opts,
429                                 repo["git"],
430                                 per_remote_overrides=salt.pillar.git_pillar.PER_REMOTE_OVERRIDES,
431                                 per_remote_only=salt.pillar.git_pillar.PER_REMOTE_ONLY,
432                                 global_only=salt.pillar.git_pillar.GLOBAL_ONLY,
433                             )
434                         except salt.exceptions.FileserverConfigError as exc:
435                             critical_errors.append(exc.strerror)
436                 finally:
437                     del new_opts
438         if errors or critical_errors:
439             for error in errors:
440                 log.error(error)
441             for error in critical_errors:
442                 log.critical(error)
443             log.critical("Master failed pre flight checks, exiting\n")
444             sys.exit(salt.defaults.exitcodes.EX_GENERIC)
445     def start(self):
446         self._pre_flight()
447         log.info("salt-master is starting as user '%s'", salt.utils.user.get_user())
448         enable_sigusr1_handler()
449         enable_sigusr2_handler()
450         self.__set_max_open_files()
451         with salt.utils<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.process.default_signals(signal.SIGINT, signal.SIGTERM):
452             SMaster.secrets["aes"] = {
453                 "secret": multiprocessing.Array(
454                     ctypes.c_char,
455                     salt.</b></font>utils.stringutils.to_bytes(
456                         salt.crypt.Crypticle.generate_key_string()
457                     ),
458                 ),
459                 "reload": salt.crypt.Crypticle.generate_key_string,
460             }
461             log.info("Creating master process manager")
462             self.process_manager = salt.utils.process.ProcessManager(wait_for_kill=5)
463             pub_channels = []
464             log.info("Creating master publisher process")
465             for _, opts in iter_transport_opts(self.opts):
466                 chan = salt.channel.server.PubServerChannel.factory(opts)
467                 chan.pre_fork(self.process_manager)
468                 pub_channels.append(chan)
469             log.info("Creating master event publisher process")
470             self.process_manager.add_process(
471                 salt.utils.event.EventPublisher,
472                 args=(self.opts,),
473                 name="EventPublisher",
474             )
475             if self.opts.get("reactor"):
476                 if isinstance(self.opts["engines"], list):
477                     rine = False
478                     for item in self.opts["engines"]:
479                         if "reactor" in item:
480                             rine = True
481                             break
482                     if not rine:
483                         self.opts["engines"].append({"reactor": {}})
484                 else:
485                     if "reactor" not in self.opts["engines"]:
486                         log.info("Enabling the reactor engine")
487                         self.opts["engines"]["reactor"] = {}
488             salt.engines.start_engines(self.opts, self.process_manager)
489             log.info("Creating master maintenance process")
490             self.process_manager.add_process(
491                 Maintenance, args=(self.opts,), name="Maintenance"
492             )
493             if self.opts.get("event_return"):
494                 log.info("Creating master event return process")
495                 self.process_manager.add_process(
496                     salt.utils.event.EventReturn, args=(self.opts,), name="EventReturn"
497                 )
498             ext_procs = self.opts.get("ext_processes", [])
499             for proc in ext_procs:
500                 log.info("Creating ext_processes process: %s", proc)
501                 try:
502                     mod = ".".join(proc.split(".")[:-1])
503                     cls = proc.split(".")[-1]
504                     _tmp = __import__(mod, globals(), locals(), [cls], -1)
505                     cls = _tmp.__getattribute__(cls)
506                     name = "ExtProcess({})".format(cls.__qualname__)
507                     self.process_manager.add_process(cls, args=(self.opts,), name=name)
508                 except Exception:  # pylint: disable=broad-except
509                     log.error("Error creating ext_processes process: %s", proc)
510             if self.opts["con_cache"]:
511                 log.info("Creating master concache process")
512                 self.process_manager.add_process(
513                     salt.utils.master.ConnectedCache,
514                     args=(self.opts,),
515                     name="ConnectedCache",
516                 )
517                 log.debug("Sleeping for two seconds to let concache rest")
518                 time.sleep(2)
519             log.info("Creating master request server process")
520             kwargs = {}
521             if salt.utils.platform.spawning_platform():
522                 kwargs["secrets"] = SMaster.secrets
523             self.process_manager.add_process(
524                 ReqServer,
525                 args=(self.opts, self.key, self.master_key),
526                 kwargs=kwargs,
527                 name="ReqServer",
528             )
529             self.process_manager.add_process(
530                 FileserverUpdate, args=(self.opts,), name="FileServerUpdate"
531             )
532             if self.opts["discovery"]:
533                 if salt.utils.ssdp.SSDPDiscoveryServer.is_available():
534                     self.process_manager.add_process(
535                         salt.utils.ssdp.SSDPDiscoveryServer(
536                             port=self.opts["discovery"]["port"],
537                             listen_ip=self.opts["interface"],
538                             answer={
539                                 "mapping": self.opts["discovery"].get("mapping", {})
540                             },
541                         ).run,
542                         name="SSDPDiscoveryServer",
543                     )
544                 else:
545                     log.error("Unable to load SSDP: asynchronous IO is not available.")
546                     if sys.version_info.major == 2:
547                         log.error(
548                             'You are using Python 2, please install "trollius" module'
549                             " to enable SSDP discovery."
550                         )
551         if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:
552             signal.signal(signal.SIGINT, self._handle_signals)
553         if signal.getsignal(signal.SIGTERM) is signal.SIG_DFL:
554             signal.signal(signal.SIGTERM, self._handle_signals)
555         self.process_manager.run()
556     def _handle_signals(self, signum, sigframe):
557         self.process_manager._handle_signals(signum, sigframe)
558         time.sleep(1)
559         sys.exit(0)
560 class ReqServer(salt.utils.process.SignalHandlingProcess):
561     def __init__(self, opts, key, mkey, secrets=None, **kwargs):
562         super().__init__(**kwargs)
563         self.opts = opts
564         self.master_key = mkey
565         self.key = key
566         self.secrets = secrets
567     def _handle_signals(self, signum, sigframe):  # pylint: disable=unused-argument
568         self.destroy(signum)
569         super()._handle_signals(signum, sigframe)
570     def __bind(self):
571         if self.secrets is not None:
572             SMaster.secrets = self.secrets
573         dfn = os.path.join(self.opts["cachedir"], ".dfn")
574         if os.path.isfile(dfn):
575             try:
576                 if salt.utils.platform.is_windows() and not os.access(dfn, os.W_OK):
577                     os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
578                 os.remove(dfn)
579             except os.error:
580                 pass
581         self.process_manager = salt.utils.process.ProcessManager(
582             name="ReqServer_ProcessManager", wait_for_kill=1
583         )
584         req_channels = []
585         tcp_only = True
586         for transport, opts in iter_transport_opts(self.opts):
587             chan = salt.channel.server.ReqServerChannel.factory(opts)
588             chan.pre_fork(self.process_manager)
589             req_channels.append(chan)
590             if transport != "tcp":
591                 tcp_only = False
592         if self.opts["req_server_niceness"] and not salt.utils.platform.is_windows():
593             log.info(
594                 "setting ReqServer_ProcessManager niceness to %d",
595                 self.opts["req_server_niceness"],
596             )
597             os.nice(self.opts["req_server_niceness"])
598         with salt.utils.process.default_signals(signal.SIGINT, signal.SIGTERM):
599             for ind in range(int(self.opts["worker_threads"])):
600                 name = "MWorker-{}".format(ind)
601                 self.process_manager.add_process(
602                     MWorker,
603                     args=(self.opts, self.master_key, self.key, req_channels),
604                     name=name,
605                 )
606         self.process_manager.run()
607     def run(self):
608         self.__bind()
609     def destroy(self, signum=signal.SIGTERM):
610         if hasattr(self, "process_manager"):
611             self.process_manager.stop_restarting()
612             self.process_manager.send_signal_to_processes(signum)
613             self.process_manager.kill_children()
614     def __del__(self):
615         self.destroy()
616 class MWorker(salt.utils.process.SignalHandlingProcess):
617     def __init__(self, opts, mkey, key, req_channels, **kwargs):
618         super().__init__(**kwargs)
619         self.opts = opts
620         self.req_channels = req_channels
621         self.mkey = mkey
622         self.key = key
623         self.k_mtime = 0
624         self.stats = collections.defaultdict(lambda: {"mean": 0, "runs": 0})
625         self.stat_clock = time.time()
626     def __setstate__(self, state):
627         super().__setstate__(state)
628         self.k_mtime = state["k_mtime"]
629         SMaster.secrets = state["secrets"]
630     def __getstate__(self):
631         state = super().__getstate__()
632         state.update({"k_mtime": self.k_mtime, "secrets": SMaster.secrets})
633         return state
634     def _handle_signals(self, signum, sigframe):
635         for channel in getattr(self, "req_channels", ()):
636             channel.close()
637         self.clear_funcs.destroy()
638         super()._handle_signals(signum, sigframe)
639     def __bind(self):
640         self.io_loop = salt.ext.tornado.ioloop.IOLoop()
641         self.io_loop.make_current()
642         for req_channel in self.req_channels:
643             req_channel.post_fork(
644                 self._handle_payload, io_loop=self.io_loop
645             )  # TODO: cleaner? Maybe lazily?
646         try:
647             self.io_loop.start()
648         except (KeyboardInterrupt, SystemExit):
649             pass
650     @salt.ext.tornado.gen.coroutine
651     def _handle_payload(self, payload):
652         key = payload["enc"]
653         load = payload["load"]
654         ret = {"aes": self._handle_aes, "clear": self._handle_clear}[key](load)
655         raise salt.ext.tornado.gen.Return(ret)
656     def _post_stats(self, start, cmd):
657         end = time.time()
658         duration = end - start
659         self.stats[cmd]["mean"] = (
660             self.stats[cmd]["mean"] * (self.stats[cmd]["runs"] - 1) + duration
661         ) / self.stats[cmd]["runs"]
662         if end - self.stat_clock &gt; self.opts["master_stats_event_iter"]:
663             self.aes_funcs.event.fire_event(
664                 {
665                     "time": end - self.stat_clock,
666                     "worker": self.name,
667                     "stats": self.stats,
668                 },
669                 tagify(self.name, "stats"),
670             )
671             self.stats = collections.defaultdict(lambda: {"mean": 0, "runs": 0})
672             self.stat_clock = end
673     def _handle_clear(self, load):
674         log.trace("Clear payload received with command %s", load["cmd"])
675         cmd = load["cmd"]
676         method = self.clear_funcs.get_method(cmd)
677         if not method:
678             return {}, {"fun": "send_clear"}
679         if self.opts["master_stats"]:
680             start = time.time()
681             self.stats[cmd]["runs"] += 1
682         ret = method(load), {"fun": "send_clear"}
683         if self.opts["master_stats"]:
684             self._post_stats(start, cmd)
685         return ret
686     def _handle_aes(self, data):
687         if "cmd" not in data:
688             log.error("Received malformed command %s", data)
689             return {}
690         cmd = data["cmd"]
691         log.trace("AES payload received with command %s", data["cmd"])
692         method = self.aes_funcs.get_method(cmd)
693         if not method:
694             return {}, {"fun": "send"}
695         if self.opts["master_stats"]:
696             start = time.time()
697             self.stats[cmd]["runs"] += 1
698         def run_func(data):
699             return self.aes_funcs.run_func(data["cmd"], data)
700         with StackContext(
701             functools.partial(RequestContext, {"data": data, "opts": self.opts})
702         ):
703             ret = run_func(data)
704         if self.opts["master_stats"]:
705             self._post_stats(start, cmd)
706         return ret
707     def run(self):
708             enforce_mworker_niceness = True
709             if self.opts["req_server_niceness"]:
710                 if salt.utils<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.user.get_user() == "root":
711                     log.info(
712                         "%s decrementing inherited ReqServer niceness to 0", self.name
713                     )
714                     log.info(os.nice())
715                     os.nice(-1 * self.opts["req_server_niceness"])
716                 else:
717                     log.error(
718                         "%s unable to decrement niceness for MWorker, not running as"
719                         " root",
720                         self.</b></font>name,
721                     )
722                     enforce_mworker_niceness = False
723             if enforce_mworker_niceness and self.opts["mworker_niceness"]:
724                 log.info(
725                     "setting %s niceness to %i",
726                     self.name,
727                     self.opts["mworker_niceness"],
728                 )
729                 os.nice(self.opts["mworker_niceness"])
730         self.clear_funcs = ClearFuncs(
731             self.opts,
732             self.key,
733         )
734         self.clear_funcs.connect()
735         self.aes_funcs = AESFuncs(self.opts)
736         salt.utils.crypt.reinit_crypto()
737         self.__bind()
738 class TransportMethods:
739     expose_methods = ()
740     def get_method(self, name):
741         if name in self.expose_methods:
742             try:
743                 return getattr(self, name)
744             except AttributeError:
745                 log.error("Requested method not exposed: %s", name)
746         else:
747             log.error("Requested method not exposed: %s", name)
748 class AESFuncs(TransportMethods):
749     expose_methods = (
750         "verify_minion",
751         "_master_tops",
752         "_master_opts",
753         "_mine_get",
754         "_mine",
755         "_mine_delete",
756         "_mine_flush",
757         "_file_recv",
758         "_pillar",
759         "_minion_event",
760         "_handle_minion_event",
761         "_return",
762         "_syndic_return",
763         "minion_runner",
764         "pub_ret",
765         "minion_pub",
766         "minion_publish",
767         "revoke_auth",
768         "_serve_file",
769         "_file_find",
770         "_file_hash",
771         "_file_hash_and_stat",
772         "_file_list",
773         "_file_list_emptydirs",
774         "_dir_list",
775         "_symlink_list",
776         "_file_envs",
777         "_ext_nodes",  # To be removed in 3006 (Sulfur) #60980
778     )
779     def __init__(self, opts):
780         self.opts = opts
781         self.event = salt.utils.event.get_master_event(
782             self.opts, self.opts["sock_dir"], listen=False
783         )
784         self.ckminions = salt.utils.minions.CkMinions(opts)
785         self.local = salt.client.get_local_client(self.opts["conf_file"])
786         self.mminion = salt.minion.MasterMinion(
787             self.opts, states=False, rend=False, ignore_config_errors=True
788         )
789         self.__setup_fileserver()
790         self.masterapi = salt.daemons.masterapi.RemoteFuncs(opts)
791     def __setup_fileserver(self):
792         import salt.fileserver
793         self.fs_ = salt.fileserver.Fileserver(self.opts)
794         self._serve_file = self.fs_.serve_file
795         self._file_find = self.fs_._find_file
796         self._file_hash = self.fs_.file_hash
797         self._file_hash_and_stat = self.fs_.file_hash_and_stat
798         self._file_list = self.fs_.file_list
799         self._file_list_emptydirs = self.fs_.file_list_emptydirs
800         self._dir_list = self.fs_.dir_list
801         self._symlink_list = self.fs_.symlink_list
802         self._file_envs = self.fs_.file_envs
803     def __verify_minion(self, id_, token):
804         if not salt.utils.verify.valid_id(self.opts, id_):
805             return False
806         pub_path = os.path.join(self.opts["pki_dir"], "minions", id_)
807         try:
808             pub = salt.crypt.get_rsa_pub_key(pub_path)
809         except OSError:
810             log.warning(
811                 "Salt minion claiming to be %s attempted to communicate with "
812                 "master, but key could not be read and verification was denied.",
813                 id_,
814             )
815             return False
816         except (ValueError, IndexError, TypeError) as err:
817             log.error('Unable to load public key "%s": %s', pub_path, err)
818         try:
819             if salt.crypt.public_decrypt(pub, token) == b"salt":
820                 return True
821         except ValueError as err:
822             log.error("Unable to decrypt token: %s", err)
823         log.error(
824             "Salt minion claiming to be %s has attempted to communicate with "
825             "the master and could not be verified",
826             id_,
827         )
828         return False
829     def verify_minion(self, id_, token):
830         return self.__verify_minion(id_, token)
831     def __verify_minion_publish(self, clear_load):
832         if "peer" not in self.opts:
833             return False
834         if not isinstance(self.opts["peer"], dict):
835             return False
836         if any(
837             key not in clear_load for key in ("fun", "arg", "tgt", "ret", "tok", "id")
838         ):
839             return False
840         if clear_load["fun"].startswith("publish."):
841             return False
842         if not self.__verify_minion(clear_load["id"], clear_load["tok"]):
843             log.warning(
844                 "Minion id %s is not who it says it is and is attempting "
845                 "to issue a peer command",
846                 clear_load["id"],
847             )
848             return False
849         clear_load.pop("tok")
850         perms = []
851         for match in self.opts["peer"]:
852             if re.match(match, clear_load["id"]):
853                 if isinstance(self.opts["peer"][match], list):
854                     perms.extend(self.opts["peer"][match])
855         if "," in clear_load["fun"]:
856             clear_load["fun"] = clear_load["fun"].split(",")
857             arg_ = []
858             for arg in clear_load["arg"]:
859                 arg_.append(arg.split())
860             clear_load["arg"] = arg_
861         return self.ckminions.auth_check(
862             perms,
863             clear_load["fun"],
864             clear_load["arg"],
865             clear_load["tgt"],
866             clear_load.get("tgt_type", "glob"),
867             publish_validate=True,
868         )
869     def __verify_load(self, load, verify_keys):
870         if any(key not in load for key in verify_keys):
871             return False
872         if "tok" not in load:
873             log.error(
874                 "Received incomplete call from %s for '%s', missing '%s'",
875                 load["id"],
876                 inspect_stack()["co_name"],
877                 "tok",
878             )
879             return False
880         if not self.__verify_minion(load["id"], load["tok"]):
881             log.warning("Minion id %s is not who it says it is!", load["id"])
882             return False
883         if "tok" in load:
884             load.pop("tok")
885         return load
886     def _master_tops(self, load):
887         load = self.__verify_load(load, ("id", "tok"))
888         if load is False:
889             return {}
890         return self.masterapi._master_tops(load, skip_verify=True)
891     _ext_nodes = _master_tops
892     def _master_opts(self, load):
893         mopts = {}
894         file_roots = {}
895         envs = self._file_envs()
896         for saltenv in envs:
897             if saltenv not in file_roots:
898                 file_roots[saltenv] = []
899         mopts["file_roots"] = file_roots
900         mopts["top_file_merging_strategy"] = self.opts["top_file_merging_strategy"]
901         mopts["env_order"] = self.opts["env_order"]
902         mopts["default_top"] = self.opts["default_top"]
903         if load.get("env_only"):
904             return mopts
905         mopts["renderer"] = self.opts["renderer"]
906         mopts["failhard"] = self.opts["failhard"]
907         mopts["state_top"] = self.opts["state_top"]
908         mopts["state_top_saltenv"] = self.opts["state_top_saltenv"]
909         mopts["nodegroups"] = self.opts["nodegroups"]
910         mopts["state_auto_order"] = self.opts["state_auto_order"]
911         mopts["state_events"] = self.opts["state_events"]
912         mopts["state_aggregate"] = self.opts["state_aggregate"]
913         mopts["jinja_env"] = self.opts["jinja_env"]
914         mopts["jinja_sls_env"] = self.opts["jinja_sls_env"]
915         mopts["jinja_lstrip_blocks"] = self.opts["jinja_lstrip_blocks"]
916         mopts["jinja_trim_blocks"] = self.opts["jinja_trim_blocks"]
917         return mopts
918     def _mine_get(self, load):
919         load = self.__verify_load(load, ("id", "tgt", "fun", "tok"))
920         if load is False:
921             return {}
922         else:
923             return self.masterapi._mine_get(load, skip_verify=True)
924     def _mine(self, load):
925         load = self.__verify_load(load, ("id", "data", "tok"))
926         if load is False:
927             return {}
928         return self.masterapi._mine(load, skip_verify=True)
929     def _mine_delete(self, load):
930         load = self.__verify_load(load, ("id", "fun", "tok"))
931         if load is False:
932             return {}
933         else:
934             return self.masterapi._mine_delete(load)
935     def _mine_flush(self, load):
936         load = self.__verify_load(load, ("id", "tok"))
937         if load is False:
938             return {}
939         else:
940             return self.masterapi._mine_flush(load, skip_verify=True)
941     def _file_recv(self, load):
942         if any(key not in load for key in ("id", "path", "loc")):
943             return False
944         if not isinstance(load["path"], list):
945             return False
946         if not self.opts["file_recv"]:
947             return False
948         if not salt.utils.verify.valid_id(self.opts, load["id"]):
949             return False
950         file_recv_max_size = 1024 * 1024 * self.opts["file_recv_max_size"]
951         if "loc" in load and load["loc"] &lt; 0:
952             log.error("Invalid file pointer: load[loc] &lt; 0")
953             return False
954         if len(load["data"]) + load.get("loc", 0) &gt; file_recv_max_size:
955             log.error(
956                 "file_recv_max_size limit of %d MB exceeded! %s will be "
957                 "truncated. To successfully push this file, adjust "
958                 "file_recv_max_size to an integer (in MB) large enough to "
959                 "accommodate it.",
960                 file_recv_max_size,
961                 load["path"],
962             )
963             return False
964         if "tok" not in load:
965             log.error(
966                 "Received incomplete call from %s for '%s', missing '%s'",
967                 load["id"],
968                 inspect_stack()["co_name"],
969                 "tok",
970             )
971             return False
972         if not self.__verify_minion(load["id"], load["tok"]):
973             log.warning("Minion id %s is not who it says it is!", load["id"])
974             return {}
975         load.pop("tok")
976         sep_path = os.sep.join(load["path"])
977         normpath = os.path.normpath(sep_path)
978         if os.path.isabs(normpath) or "../" in load["path"]:
979             return False
980         cpath = os.path.join(
981             self.opts["cachedir"], "minions", load["id"], "files", normpath
982         )
983         if not os.path.normpath(cpath).startswith(self.opts["cachedir"]):
984             log.warning(
985                 "Attempt to write received file outside of master cache "
986                 "directory! Requested path: %s. Access denied.",
987                 cpath,
988             )
989             return False
990         cdir = os.path.dirname(cpath)
991         if not os.path.isdir(cdir):
992             try:
993                 os.makedirs(cdir)
994             except os.error:
995                 pass
996         if os.path.isfile(cpath) and load["loc"] != 0:
997             mode = "ab"
998         else:
999             mode = "wb"
1000         with salt.utils.files.fopen(cpath, mode) as fp_:
1001             if load["loc"]:
1002                 fp_.seek(load["loc"])
1003             fp_.write(salt.utils.stringutils.to_bytes(load["data"]))
1004         return True
1005     def _pillar(self, load):
1006         if any(key not in load for key in ("id", "grains")):
1007             return False
1008         if not salt.utils.verify.valid_id(self.opts, load["id"]):
1009             return False
1010         load["grains"]["id"] = load["id"]
1011         pillar = salt.pillar.get_pillar(
1012             load["grains"],
1013             load["id"],
1014             load.get("saltenv", load<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.get("env")),
1015             ext=load.get("ext"),
1016             pillar_override=load.get("pillar_override", {}),
1017             pillarenv=load.get("pillarenv"),
1018             extra_minion_data=load.get("extra_minion_data"),
1019             clean_cache=load.get("clean_cache"),
1020         )
1021         data = pillar.compile_pillar()
1022         self.</b></font>fs_.update_opts()
1023         if self.opts.get("minion_data_cache", False):
1024             self.masterapi.cache.store(
1025                 "minions/{}".format(load["id"]),
1026                 "data",
1027                 {"grains": load["grains"], "pillar": data},
1028             )
1029             if self.opts.get("minion_data_cache_events") is True:
1030                 self.event.fire_event(
1031                     {"Minion data cache refresh": load["id"]},
1032                     tagify(load["id"], "refresh", "minion"),
1033                 )
1034         return data
1035     def _minion_event(self, load):
1036         load = self.__verify_load(load, ("id", "tok"))
1037         if load is False:
1038             return {}
1039         self.masterapi._minion_event(load)
1040         self._handle_minion_event(load)
1041     def _handle_minion_event(self, load):
1042         id_ = load["id"]
1043         if load.get("tag", "") == "_salt_error":
1044             log.error(
1045                 "Received minion error from [%s]: %s", id_, load["data"]["message"]
1046             )
1047         for event in load.get("events", []):
1048             event_data = event.get("data", {})
1049             if "minions" in event_data:
1050                 jid = event_data.get("jid")
1051                 if not jid:
1052                     continue
1053                 minions = event_data["minions"]
1054                 try:
1055                     salt.utils.job.store_minions(
1056                         self.opts, jid, minions, mminion=self.mminion, syndic_id=id_
1057                     )
1058                 except (KeyError, salt.exceptions.SaltCacheError) as exc:
1059                     log.error(
1060                         "Could not add minion(s) %s for job %s: %s", minions, jid, exc
1061                     )
1062     def _return(self, load):
1063         if self.opts["require_minion_sign_messages"] and "sig" not in load:
1064             log.critical(
1065                 "_return: Master is requiring minions to sign their "
1066                 "messages, but there is no signature in this payload from "
1067                 "%s.",
1068                 load["id"],
1069             )
1070             return False
1071         if "sig" in load:
1072             log.trace("Verifying signed event publish from minion")
1073             sig = load.pop("sig")
1074             this_minion_pubkey = os.path.join(
1075                 self.opts["pki_dir"], "minions/{}".format(load["id"])
1076             )
1077             serialized_load = salt.serializers.msgpack.serialize(load)
1078             if not salt.crypt.verify_signature(
1079                 this_minion_pubkey, serialized_load, sig
1080             ):
1081                 log.info("Failed to verify event signature from minion %s.", load["id"])
1082                 if self.opts["drop_messages_signature_fail"]:
1083                     log.critical(
1084                         "drop_messages_signature_fail is enabled, dropping "
1085                         "message from %s",
1086                         load["id"],
1087                     )
1088                     return False
1089                 else:
1090                     log.info(
1091                         "But 'drop_message_signature_fail' is disabled, so message is"
1092                         " still accepted."
1093                     )
1094             load["sig"] = sig
1095         try:
1096             salt.utils.job.store_job(
1097                 self.opts, load, event=self.event, mminion=self.mminion
1098             )
1099         except salt.exceptions.SaltCacheError:
1100             log.error("Could not store job information for load: %s", load)
1101     def _syndic_return(self, load):
1102         loads = load.get("load")
1103         if not isinstance(loads, list):
1104             loads = [load]  # support old syndics not aggregating returns
1105         for load in loads:
1106             if any(key not in load for key in ("return", "jid", "id")):
1107                 continue
1108             if load.get("load"):
1109                 fstr = "{}.save_load".format(self.opts["master_job_cache"])
1110                 self.mminion.returners[fstr](load["jid"], load["load"])
1111             syndic_cache_path = os.path.join(
1112                 self.opts["cachedir"], "syndics", load["id"]
1113             )
1114             if not os.path.exists(syndic_cache_path):
1115                 path_name = os.path.split(syndic_cache_path)[0]
1116                 if not os.path.exists(path_name):
1117                     os.makedirs(path_name)
1118                 with salt.utils.files.fopen(syndic_cache_path, "w") as wfh:
1119                     wfh.write("")
1120             for key, item in load["return"].items():
1121                 ret = {"jid": load["jid"], "id": key}
1122                 ret.update(item)
1123                 if "master_id" in load:
1124                     ret["master_id"] = load["master_id"]
1125                 if "fun" in load:
1126                     ret["fun"] = load["fun"]
1127                 if "arg" in load:
1128                     ret["fun_args"] = load["arg"]
1129                 if "out" in load:
1130                     ret["out"] = load["out"]
1131                 if "sig" in load:
1132                     ret["sig"] = load["sig"]
1133                 self._return(ret)
1134     def minion_runner(self, clear_load):
1135         load = self.__verify_load(clear_load, ("fun", "arg", "id", "tok"))
1136         if load is False:
1137             return {}
1138         else:
1139             return self.masterapi.minion_runner(clear_load)
1140     def pub_ret(self, load):
1141         load = self.__verify_load(load, ("jid", "id", "tok"))
1142         if load is False:
1143             return {}
1144         auth_cache = os.path.join(self.opts["cachedir"], "publish_auth")
1145         if not os.path.isdir(auth_cache):
1146             os.makedirs(auth_cache)
1147         jid_fn = os.path.join(auth_cache, str(load["jid"]))
1148         with salt.utils.files.fopen(jid_fn, "r") as fp_:
1149             if not load["id"] == fp_.read():
1150                 return {}
1151         return self.local.get_cache_returns(load["jid"])
1152     def minion_pub(self, clear_load):
1153         if not self.__verify_minion_publish(clear_load):
1154             return {}
1155         else:
1156             return self.masterapi.minion_pub(clear_load)
1157     def minion_publish(self, clear_load):
1158         if not self.__verify_minion_publish(clear_load):
1159             return {}
1160         else:
1161             return self.masterapi.minion_publish(clear_load)
1162     def revoke_auth(self, load):
1163         load = self.__verify_load(load, ("id", "tok"))
1164         if not self.opts.get("allow_minion_key_revoke", False):
1165             log.warning(
1166                 "Minion %s requested key revoke, but allow_minion_key_revoke "
1167                 "is set to False",
1168                 load["id"],
1169             )
1170             return load
1171         if load is False:
1172             return load
1173         else:
1174             return self.masterapi.revoke_auth(load)
1175     def run_func(self, func, load):
1176         if func.startswith("__"):
1177             return {}, {"fun": "send"}
1178         if hasattr(self, func):
1179             try:
1180                 start = time.time()
1181                 ret = getattr(self, func)(load)
1182                 log.trace(
1183                     "Master function call %s took %s seconds", func, time.time() - start
1184                 )
1185             except Exception:  # pylint: disable=broad-except
1186                 ret = ""
1187                 log.error("Error in function %s:\n", func, exc_info=True)
1188         else:
1189             log.error(
1190                 "Received function %s which is unavailable on the master, "
1191                 "returning False",
1192                 func,
1193             )
1194             return False, {"fun": "send"}
1195         if func == "_return":
1196             return ret, {"fun": "send"}
1197         if func == "_pillar" and "id" in load:
1198             if load.get("ver") != "2" and self.opts["pillar_version"] == 1:
1199                 return ret, {"fun": "send"}
1200             return ret, {"fun": "send_private", "key": "pillar", "tgt": load["id"]}
1201         return ret, {"fun": "send"}
1202     def destroy(self):
1203         self.masterapi.destroy()
1204         if self.local is not None:
1205             self.local.destroy()
1206             self.local = None
1207 class ClearFuncs(TransportMethods):
1208     expose_methods = (
1209         "ping",
1210         "publish",
1211         "get_token",
1212         "mk_token",
1213         "wheel",
1214         "runner",
1215     )
1216     def __init__(self, opts, key):
1217         self.opts = opts
1218         self.key = key
1219         self.event = salt.utils.event.get_master_event(
1220             self.opts, self.opts["sock_dir"], listen=False
1221         )
1222         self.local = salt.client.get_local_client(self.opts["conf_file"])
1223         self.ckminions = salt.utils.minions.CkMinions(opts)
1224         self.loadauth = salt.auth.LoadAuth(opts)
1225         self.mminion = salt.minion.MasterMinion(
1226             self.opts, states=False, rend=False, ignore_config_errors=True
1227         )
1228         self.wheel_ = salt.wheel.Wheel(opts)
1229         self.masterapi = salt.daemons.masterapi.LocalFuncs(opts, key)
1230         self.channels = []
1231     def runner(self, clear_load):
1232         auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)
1233         auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)
1234         error = auth_check.get("error")
1235         if error:
1236             return {"error": error}
1237         username = auth_check.get("username")
1238         if auth_type != "user":
1239             runner_check = self.ckminions.runner_check(
1240                 auth_check.get("auth_list", []),
1241                 clear_load["fun"],
1242                 clear_load.get("kwarg", {}),
1243             )
1244             if not runner_check:
1245                 return {
1246                     "error": {
1247                         "name": err_name,
1248                         "message": (
1249                             'Authentication failure of type "{}" occurred for '
1250                             "user {}.".format(auth_type, username)
1251                         ),
1252                     }
1253                 }
1254             elif isinstance(runner_check, dict) and "error" in runner_check:
1255                 return runner_check
1256             for item in sensitive_load_keys:
1257                 clear_load.pop(item, None)
1258         else:
1259             if "user" in clear_load:
1260                 username = clear_load["user"]
1261                 if salt.auth.AuthUser(username).is_sudo():
1262                     username = self.opts.get("user", "root")
1263             else:
1264                 username = salt.utils.user.get_user()
1265         try:
1266             fun = clear_load.pop("fun")
1267             runner_client = salt.runner.RunnerClient(self.opts)
1268             return runner_client.asynchronous(
1269                 fun, clear_load.get("kwarg", {}), username, local=True
1270             )
1271         except Exception as exc:  # pylint: disable=broad-except
1272             log.error("Exception occurred while introspecting %s: %s", fun, exc)
1273             return {
1274                 "error": {
1275                     "name": exc.__class__.__name__,
1276                     "args": exc.args,
1277                     "message": str(exc),
1278                 }
1279             }
1280     def wheel(self, clear_load):
1281         auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(clear_load)
1282         auth_check = self.loadauth.check_authentication(clear_load, auth_type, key=key)
1283         error = auth_check.get("error")
1284         if error:
1285             return {"error": error}
1286         username = auth_check.get("username")
1287         if auth_type != "user":
1288             wheel_check = self.ckminions.wheel_check(
1289                 auth_check.get("auth_list", []),
1290                 clear_load["fun"],
1291                 clear_load.get("kwarg", {}),
1292             )
1293             if not wheel_check:
1294                 return {
1295                     "error": {
1296                         "name": err_name,
1297                         "message": (
1298                             'Authentication failure of type "{}" occurred for '
1299                             "user {}.".format(auth_type, username)
1300                         ),
1301                     }
1302                 }
1303             elif isinstance(wheel_check, dict) and "error" in wheel_check:
1304                 return wheel_check
1305             for item in sensitive_load_keys:
1306                 clear_load.pop(item, None)
1307         else:
1308             if "user" in clear_load:
1309                 username = clear_load["user"]
1310                 if salt.auth.AuthUser(username).is_sudo():
1311                     username = self.opts.get("user", "root")
1312             else:
1313                 username = salt.utils.user.get_user()
1314         try:
1315             jid = salt.utils.jid.gen_jid(self.opts)
1316             fun = clear_load.pop("fun")
1317             tag = tagify(jid, prefix="wheel")
1318             data = {
1319                 "fun": "wheel.{}".format(fun),
1320                 "jid": jid,
1321                 "tag": tag,
1322                 "user": username,
1323             }
1324             self.event.fire_event(data, tagify([jid, "new"], "wheel"))
1325             ret = self.wheel_.call_func(fun, full_return=True, **clear_load)
1326             data["return"] = ret["return"]
1327             data["success"] = ret["success"]
1328             self.event.fire_event(data, tagify([jid, "ret"], "wheel"))
1329             return {"tag": tag, "data": data}
1330         except Exception as exc:  # pylint: disable=broad-except
1331             log.error("Exception occurred while introspecting %s: %s", fun, exc)
1332             data["return"] = "Exception occurred in wheel {}: {}: {}".format(
1333                 fun,
1334                 exc.__class__.__name__,
1335                 exc,
1336             )
1337             data["success"] = False
1338             self.event.fire_event(data, tagify([jid, "ret"], "wheel"))
1339             return {"tag": tag, "data": data}
1340     def mk_token(self, clear_load):
1341         token = self.loadauth.mk_token(clear_load)
1342         if not token:
1343             log.warning('Authentication failure of type "eauth" occurred.')
1344             return ""
1345         return token
1346     def get_token(self, clear_load):
1347         if "token" not in clear_load:
1348             return False
1349         return self.loadauth.get_tok(clear_load["token"])
1350     def publish(self, clear_load):
1351         extra = clear_load.get("kwargs", {})
1352         publisher_acl = salt.acl.PublisherACL(self.opts["publisher_acl_blacklist"])
1353         if publisher_acl.user_is_blacklisted(
1354             clear_load["user"]
1355         ) or publisher_acl.cmd_is_blacklisted(clear_load["fun"]):
1356             log.error(
1357                 "%s does not have permissions to run %s. Please contact "
1358                 "your local administrator if you believe this is in "
1359                 "error.\n",
1360                 clear_load["user"],
1361                 clear_load["fun"],
1362             )
1363             return {
1364                 "error": {
1365                     "name": "AuthorizationError",
1366                     "message": "Authorization error occurred.",
1367                 }
1368         delimiter <font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>= clear_load.get("kwargs", {}).get("delimiter", DEFAULT_TARGET_DELIM)
1369         _res = self.ckminions.check_minions(
1370             clear_load["tgt"], clear_load.get(</b></font>"tgt_type", "glob"), delimiter
1371         )
1372         minions = _res.get("minions", list())
1373         missing = _res.get("missing", list())
1374         ssh_minions = _res.get("ssh_minions", False)
1375         auth_type, err_name, key, sensitive_load_keys = self._prep_auth_info(extra)
1376         if auth_type == "user":
1377             auth_check = self.loadauth.check_authentication(
1378                 clear_load, auth_type, key=key
1379             )
1380         else:
1381             auth_check = self.loadauth.check_authentication(extra, auth_type)
1382         auth_list = auth_check.get("auth_list", [])
1383         err_msg = 'Authentication failure of type "{}" occurred.'.format(auth_type)
1384         if auth_check.get("error"):
1385             log.warning(err_msg)
1386             return {
1387                 "error": {
1388                     "name": "AuthenticationError",
1389                     "message": "Authentication error occurred.",
1390                 }
1391             }
1392         if auth_type != "user" or (auth_type == "user" and auth_list):
1393             authorized = self.ckminions.auth_check(
1394                 auth_list,
1395                 clear_load["fun"],
1396                 clear_load["arg"],
1397                 clear_load["tgt"],
1398                 clear_load.get("tgt_type", "glob"),
1399                 minions=minions,
1400                 whitelist=["saltutil.find_job"],
1401             )
1402             if not authorized:
1403                 if (
1404                     auth_type == "eauth"
1405                     and not auth_list
1406                     and "username" in extra
1407                     and "eauth" in extra
1408                 ):
1409                     log.debug(
1410                         'Auth configuration for eauth "%s" and user "%s" is empty',
1411                         extra["eauth"],
1412                         extra["username"],
1413                     )
1414                 log.warning(err_msg)
1415                 return {
1416                     "error": {
1417                         "name": "AuthorizationError",
1418                         "message": "Authorization error occurred.",
1419                     }
1420                 }
1421             if auth_type == "token":
1422                 username = auth_check.get("username")
1423                 clear_load["user"] = username
1424                 log.debug('Minion tokenized user = "%s"', username)
1425             elif auth_type == "eauth":
1426                 clear_load["user"] = self.loadauth.load_name(extra)
1427         if not self.opts.get("order_masters"):
1428             if not minions:
1429                 return {
1430                     "enc": "clear",
1431                     "load": {
1432                         "jid": None,
1433                         "minions": minions,
1434                         "error": (
1435                             "Master could not resolve minions for target {}".format(
1436                                 clear_load["tgt"]
1437                             )
1438                         ),
1439                     },
1440                 }
1441         jid = self._prep_jid(clear_load, extra)
1442         if jid is None:
1443             return {"enc": "clear", "load": {"error": "Master failed to assign jid"}}
1444         payload = self._prep_pub(minions, jid, clear_load, extra, missing)
1445         self._send_ssh_pub(payload, ssh_minions=ssh_minions)
1446         self._send_pub(payload)
1447         return {
1448             "enc": "clear",
1449             "load": {"jid": clear_load["jid"], "minions": minions, "missing": missing},
1450         }
1451     def _prep_auth_info(self, clear_load):
1452         sensitive_load_keys = []
1453         key = None
1454         if "token" in clear_load:
1455             auth_type = "token"
1456             err_name = "TokenAuthenticationError"
1457             sensitive_load_keys = ["token"]
1458         elif "eauth" in clear_load:
1459             auth_type = "eauth"
1460             err_name = "EauthAuthenticationError"
1461             sensitive_load_keys = ["username", "password"]
1462         else:
1463             auth_type = "user"
1464             err_name = "UserAuthenticationError"
1465             key = self.key
1466         return auth_type, err_name, key, sensitive_load_keys
1467     def _prep_jid(self, clear_load, extra):
1468         passed_jid = clear_load["jid"] if clear_load.get("jid") else None
1469         nocache = extra.get("nocache", False)
1470         fstr = "{}.prep_jid".format(self.opts["master_job_cache"])
1471         try:
1472             jid = self.mminion.returners[fstr](nocache=nocache, passed_jid=passed_jid)
1473         except (KeyError, TypeError):
1474             msg = (
1475                 "Failed to allocate a jid. The requested returner '{}' "
1476                 "could not be loaded.".format(fstr.split(".")[0])
1477             )
1478             log.error(msg)
1479             return {"error": msg}
1480         return jid
1481     def _send_pub(self, load):
1482         if not self.channels:
1483             for transport, opts in iter_transport_opts(self.opts):
1484                 chan = salt.channel.server.PubServerChannel.factory(opts)
1485                 self.channels.append(chan)
1486         for chan in self.channels:
1487             chan.publish(load)
1488     @property
1489     def ssh_client(self):
1490         if not hasattr(self, "_ssh_client"):
1491             self._ssh_client = salt.client.ssh.client.SSHClient(mopts=self.opts)
1492         return self._ssh_client
1493     def _send_ssh_pub(self, load, ssh_minions=False):
1494         if self.opts["enable_ssh_minions"] is True and ssh_minions is True:
1495             log.debug("Send payload to ssh minions")
1496             threading.Thread(target=self.ssh_client.cmd, kwargs=load).start()
1497     def _prep_pub(self, minions, jid, clear_load, extra, missing):
1498         clear_load["jid"] = jid
1499         delimiter = clear_load.get("kwargs", {}).get("delimiter", DEFAULT_TARGET_DELIM)
1500         self.event.fire_event({"minions": minions}, clear_load["jid"])
1501         new_job_load = {
1502             "jid": clear_load["jid"],
1503             "tgt_type": clear_load["tgt_type"],
1504             "tgt": clear_load["tgt"],
1505             "user": clear_load["user"],
1506             "fun": clear_load["fun"],
1507             "arg": clear_load["arg"],
1508             "minions": minions,
1509             "missing": missing,
1510         }
1511         self.event.fire_event(new_job_load, tagify([clear_load["jid"], "new"], "job"))
1512         if self.opts["ext_job_cache"]:
1513             fstr = "{}.save_load".format(self.opts["ext_job_cache"])
1514             save_load_func = True
1515             try:
1516                 arg_spec = salt.utils.args.get_function_argspec(
1517                     self.mminion.returners[fstr]
1518                 )
1519                 if "minions" not in arg_spec.args:
1520                     log.critical(
1521                         "The specified returner used for the external job cache "
1522                         "'%s' does not have a 'minions' kwarg in the returner's "
1523                         "save_load function.",
1524                         self.opts["ext_job_cache"],
1525                     )
1526             except (AttributeError, KeyError):
1527                 save_load_func = False
1528                 log.critical(
1529                     "The specified returner used for the external job cache "
1530                     '"%s" does not have a save_load function!',
1531                     self.opts["ext_job_cache"],
1532                 )
1533             if save_load_func:
1534                 try:
1535                     self.mminion.returners[fstr](
1536                         clear_load["jid"], clear_load, minions=minions
1537                     )
1538                 except Exception:  # pylint: disable=broad-except
1539                     log.critical(
1540                         "The specified returner threw a stack trace:\n", exc_info=True
1541                     )
1542         try:
1543             fstr = "{}.save_load".format(self.opts["master_job_cache"])
1544             self.mminion.returners[fstr](clear_load["jid"], clear_load, minions)
1545         except KeyError:
1546             log.critical(
1547                 "The specified returner used for the master job cache "
1548                 '"%s" does not have a save_load function!',
1549                 self.opts["master_job_cache"],
1550             )
1551         except Exception:  # pylint: disable=broad-except
1552             log.critical("The specified returner threw a stack trace:\n", exc_info=True)
1553         payload = {"enc": "aes"}
1554         load = {
1555             "fun": clear_load["fun"],
1556             "arg": clear_load["arg"],
1557             "tgt": clear_load["tgt"],
1558             "jid": clear_load["jid"],
1559             "ret": clear_load["ret"],
1560         }
1561         if "master_id" in self.opts:
1562             load["master_id"] = self.opts["master_id"]
1563         if "master_id" in extra:
1564             load["master_id"] = extra["master_id"]
1565         if delimiter != DEFAULT_TARGET_DELIM:
1566             load["delimiter"] = delimiter
1567         if "id" in extra:
1568             load["id"] = extra["id"]
1569         if "tgt_type" in clear_load:
1570             load["tgt_type"] = clear_load["tgt_type"]
1571         if "to" in clear_load:
1572             load["to"] = clear_load["to"]
1573         if "kwargs" in clear_load:
1574             if "ret_config" in clear_load["kwargs"]:
1575                 load["ret_config"] = clear_load["kwargs"].get("ret_config")
1576             if "metadata" in clear_load["kwargs"]:
1577                 load["metadata"] = clear_load["kwargs"].get("metadata")
1578             if "module_executors" in clear_load["kwargs"]:
1579                 load["module_executors"] = clear_load["kwargs"].get("module_executors")
1580             if "executor_opts" in clear_load["kwargs"]:
1581                 load["executor_opts"] = clear_load["kwargs"].get("executor_opts")
1582             if "ret_kwargs" in clear_load["kwargs"]:
1583                 load["ret_kwargs"] = clear_load["kwargs"].get("ret_kwargs")
1584         if "user" in clear_load:
1585             log.info(
1586                 "User %s Published command %s with jid %s",
1587                 clear_load["user"],
1588                 clear_load["fun"],
1589                 clear_load["jid"],
1590             )
1591             load["user"] = clear_load["user"]
1592         else:
1593             log.info(
1594                 "Published command %s with jid %s", clear_load["fun"], clear_load["jid"]
1595             )
1596         log.debug("Published command details %s", load)
1597         return load
1598     def ping(self, clear_load):
1599         return clear_load
1600     def destroy(self):
1601         if self.masterapi is not None:
1602             self.masterapi.destroy()
1603             self.masterapi = None
1604         if self.local is not None:
1605             self.local.destroy()
1606             self.local = None
1607         while self.channels:
1608             chan = self.channels.pop()
1609             chan.close()
1610     def connect(self):
1611         if self.channels:
1612             return
1613         for transport, opts in iter_transport_opts(self.opts):
1614             chan = salt.channel.server.PubServerChannel.factory(opts)
1615             self.channels.append(chan)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>virt_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import base64
2 import collections
3 import copy
4 import datetime
5 import logging
6 import os
7 import re
8 import shutil
9 import string  # pylint: disable=deprecated-module
10 import subprocess
11 import sys
12 import time
13 import urllib.parse
14 from xml.etree import ElementTree
15 from xml.sax import saxutils
16 import jinja2.exceptions
17 import salt.utils.data
18 import salt.utils.files
19 import salt.utils.json
20 import salt.utils.path
21 import salt.utils.stringutils
22 import salt.utils.templates
23 import salt.utils.virt
24 import salt.utils.xmlutil as xmlutil
25 import salt.utils.yaml
26 from salt._compat import ipaddress
27 from salt.exceptions import CommandExecutionError, SaltInvocationError
28 try:
29     import</b></font> libvirt  # pylint: disable=import-error
30     from libvirt import libvirtError
31     HAS_LIBVIRT = True
32 except ImportError:
33     HAS_LIBVIRT = False
34 log = logging.getLogger(__name__)
35 JINJA = jinja2.Environment(
36     loader=jinja2.FileSystemLoader(
37         os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, "virt")
38     )
39 )
40 CACHE_DIR = "/var/lib/libvirt/saltinst"
41 VIRT_STATE_NAME_MAP = {
42     0: "running",
43     1: "running",
44     2: "running",
45     3: "paused",
46     4: "shutdown",
47     5: "shutdown",
48     6: "crashed",
49 }
50 def __virtual__():
51     if not HAS_LIBVIRT:
52         return (False, "Unable to locate or import python libvirt library.")
53     return "virt"
54 def __get_request_auth(username, password):
55     def __request_auth(credentials, user_data):
56             if credential[0] == libvirt.VIR_CRED_AUTHNAME:
57                 credential[4] = (
58                     <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>username
59                     if username
60                     else __salt__["config.get"](
61                         "virt:connection:auth:username", credential[3]
62                     )
63                 )
64             elif credential[0] == libvirt.VIR_CRED_NOECHOPROMPT:
65                 credential[4] = (
66                     password
67                     if password
68                     else __salt__["config.get"](
69                         "virt:connection:auth:password", credential[3]
70                     )
71                 )
72             else:
73                 log.</b></font>info("Unhandled credential type: %s", credential[0])
74         return 0
75 def __get_conn(**kwargs):
76     username = kwargs<font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.get("username", None)
77     password = kwargs.get("password", None)
78     conn_str = kwargs.get("connection", None)
79     if not conn_str:
80         conn_str = __salt__["config.get"](</b></font>"virt:connection:uri", conn_str)
81     try:
82         auth_types = [
83             libvirt.VIR_CRED_AUTHNAME,
84             libvirt.VIR_CRED_NOECHOPROMPT,
85             libvirt.VIR_CRED_ECHOPROMPT,
86             libvirt.VIR_CRED_PASSPHRASE,
87             libvirt.VIR_CRED_EXTERNAL,
88         ]
89         conn = libvirt.openAuth(
90             conn_str, [auth_types, __get_request_auth(username, password), None], 0
91         )
92     except Exception:  # pylint: disable=broad-except
93         raise CommandExecutionError(
94             "Sorry, {} failed to open a connection to the hypervisor "
95             "software at {}".format(__grains__["fqdn"], conn_str)
96         )
97     return conn
98 def _get_domain(conn, *vms, **kwargs):
99     ret = list()
100     lookup_vms = list()
101     all_vms = []
102     if kwargs.get("active", True):
103         for id_ in conn.listDomainsID():
104             all_vms.append(conn.lookupByID(id_).name())
105     if kwargs.get("inactive", True):
106         for id_ in conn.listDefinedDomains():
107             all_vms.append(id_)
108     if vms and not all_vms:
109         raise CommandExecutionError("No virtual machines found.")
110     if vms:
111         for name in vms:
112             if name not in all_vms:
113                 raise CommandExecutionError(
114                     'The VM "{name}" is not present'.format(name=name)
115                 )
116             else:
117                 lookup_vms.append(name)
118     else:
119         lookup_vms = list(all_vms)
120     for name in lookup_vms:
121         ret.append(conn.lookupByName(name))
122     return len(ret) == 1 and not kwargs.get("iterable") and ret[0] or ret
123 def _parse_qemu_img_info(info):
124     raw_infos = salt.utils.json.loads(info)
125     disks = []
126     for disk_infos in raw_infos:
127         disk = {
128             "file": disk_infos["filename"],
129             "file format": disk_infos["format"],
130             "disk size": disk_infos["actual-size"],
131             "virtual size": disk_infos["virtual-size"],
132             "cluster size": disk_infos["cluster-size"]
133             if "cluster-size" in disk_infos
134             else None,
135         }
136         if "full-backing-filename" in disk_infos.keys():
137             disk["backing file"] = format(disk_infos["full-backing-filename"])
138         if "snapshots" in disk_infos.keys():
139             disk["snapshots"] = [
140                 {
141                     "id": snapshot["id"],
142                     "tag": snapshot["name"],
143                     "vmsize": snapshot["vm-state-size"],
144                     "date": datetime.datetime.fromtimestamp(
145                         float(
146                             "{}.{}".format(snapshot["date-sec"], snapshot["date-nsec"])
147                         )
148                     ).isoformat(),
149                     "vmclock": datetime.datetime.utcfromtimestamp(
150                         float(
151                             "{}.{}".format(
152                                 snapshot["vm-clock-sec"], snapshot["vm-clock-nsec"]
153                             )
154                         )
155                     )
156                     .time()
157                     .isoformat(),
158                 }
159                 for snapshot in disk_infos["snapshots"]
160             ]
161         disks.append(disk)
162     for disk in disks:
163         if "backing file" in disk.keys():
164             candidates = [
165                 info
166                 for info in disks
167                 if "file" in info.keys() and info["file"] == disk["backing file"]
168             ]
169             if candidates:
170                 disk["backing file"] = candidates[0]
171     return disks[0]
172 def _get_uuid(dom):
173     return ElementTree.fromstring(get_xml(dom)).find("uuid").text
174 def _get_on_poweroff(dom):
175     node = ElementTree.fromstring(get_xml(dom)).find("on_poweroff")
176     return node.text if node is not None else ""
177 def _get_on_reboot(dom):
178     node = ElementTree.fromstring(get_xml(dom)).find("on_reboot")
179     return node.text if node is not None else ""
180 def _get_on_crash(dom):
181     node = ElementTree.fromstring(get_xml(dom)).find("on_crash")
182     return node.text if node is not None else ""
183 def _get_nics(dom):
184     nics = {}
185     doc = ElementTree.fromstring(dom.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE))
186     for iface_node in doc.findall("devices/interface"):
187         nic = {}
188         nic["type"] = iface_node.get("type")
189         for v_node in iface_node:
190             if v_node.tag == "mac":
191                 nic["mac"] = v_node.get("address")
192             if v_node.tag == "model":
193                 nic["model"] = v_node.get("type")
194             if v_node.tag == "target":
195                 nic["target"] = v_node.get("dev")
196             if re.match("(driver|source|address)", v_node.tag):
197                 temp = {}
198                 for key, value in v_node.attrib.items():
199                     temp[key] = value
200                 nic[v_node.tag] = temp
201             if v_node.tag == "virtualport":
202                 temp = {}
203                 temp["type"] = v_node.get("type")
204                 for key, value in v_node.attrib.items():
205                     temp[key] = value
206                 nic["virtualport"] = temp
207         if "mac" not in nic:
208             continue
209         nics[nic["mac"]] = nic
210     return nics
211 def _get_graphics(dom):
212     out = {
213         "autoport": "None",
214         "keymap": "None",
215         "listen": "None",
216         "port": "None",
217         "type": "None",
218     }
219     doc = ElementTree.fromstring(dom.XMLDesc(0))
220     for g_node in doc.findall("devices/graphics"):
221         for key, value in g_node.attrib.items():
222             out[key] = value
223     return out
224 def _get_loader(dom):
225     out = {"path": "None"}
226     doc = ElementTree.fromstring(dom.XMLDesc(0))
227     for g_node in doc.findall("os/loader"):
228         out["path"] = g_node.text
229         for key, value in g_node.attrib.items():
230             out[key] = value
231     return out
232 def _get_disks(conn, dom):
233     disks = {}
234     doc = ElementTree.fromstring(dom.XMLDesc(0))
235     all_volumes = _get_all_volumes_paths(conn)
236     for elem in doc.findall("devices/disk"):
237         source = elem.find("source")
238         if source is None:
239             continue
240         target = elem.find("target")
241         driver = elem.find("driver")
242         if target is None:
243             continue
244         qemu_target = None
245         extra_properties = None
246         if "dev" in target.attrib:
247             disk_type = elem.get("type")
248             def _get_disk_volume_data(pool_name, volume_name):
249                 qemu_target = "{}/{}".format(pool_name, volume_name)
250                 pool = conn.storagePoolLookupByName(pool_name)
251                 extra_properties = {}
252                 try:
253                     vol = pool.storageVolLookupByName(volume_name)
254                     vol_info = vol.info()
255                     extra_properties = {
256                         "virtual size": vol_info[1],
257                         "disk size": vol_info[2],
258                     }
259                     backing_files = [
260                         {
261                             "file": node.find("source").get("file"),
262                             "file format": node.find("format").get("type"),
263                         }
264                         for node in elem.findall(".//backingStore[source]")
265                     ]
266                     if backing_files:
267                         extra_properties["backing file"] = backing_files[0]
268                         parent = extra_properties["backing file"]
269                         for sub_backing_file in backing_files[1:]:
270                             parent["backing file"] = sub_backing_file
271                             parent = sub_backing_file
272                     else:
273                         vol_desc = ElementTree.fromstring(vol.XMLDesc())
274                         backing_path = vol_desc.find("./backingStore/path")
275                         backing_format = vol_desc.find("./backingStore/format")
276                         if backing_path is not None:
277                             extra_properties["backing file"] = {
278                                 "file": backing_path.text
279                             }
280                             if backing_format is not None:
281                                 extra_properties["backing file"][
282                                     "file format"
283                                 ] = backing_format.get("type")
284                 except libvirt.libvirtError:
285                     log.info(
286                         "Couldn't extract all volume informations: pool is likely not"
287                         " running or refreshed"
288                     )
289                 return (qemu_target, extra_properties)
290             if disk_type == "file":
291                 qemu_target = source.get("file", "")
292                 if qemu_target.startswith("/dev/zvol/"):
293                     disks[target.get("dev")] = {"file": qemu_target, "zfs": True}
294                     continue
295                 if qemu_target in all_volumes.keys():
296                     volume = all_volumes[qemu_target]
297                     qemu_target, extra_properties = _get_disk_volume_data(
298                         volume["pool"], volume["name"]
299                     )
300                 elif elem.get("device", "disk") != "cdrom":
301                     try:
302                         process = subprocess.Popen(
303                             [
304                                 "qemu-img",
305                                 "info",
306                                 "-U",
307                                 "--output",
308                                 "json",
309                                 "--backing-chain",
310                                 qemu_target,
311                             ],
312                             shell=False,
313                             stdout=subprocess.PIPE,
314                             stderr=subprocess.PIPE,
315                         )
316                         stdout, stderr = process.communicate()
317                         if process.returncode == 0:
318                             qemu_output = salt.utils.stringutils.to_str(stdout)
319                             output = _parse_qemu_img_info(qemu_output)
320                             extra_properties = output
321                         else:
322                             extra_properties = {"error": stderr}
323                     except FileNotFoundError:
324                         extra_properties = {"error": "qemu-img not found"}
325             elif disk_type == "block":
326                 qemu_target = source.get("dev", "")
327                 if qemu_target in all_volumes.keys():
328                     volume = all_volumes[qemu_target]
329                     qemu_target, extra_properties = _get_disk_volume_data(
330                         volume["pool"], volume["name"]
331                     )
332             elif disk_type == "network":
333                 qemu_target = source.get("protocol")
334                 source_name = source.get("name")
335                 if source_name:
336                     qemu_target = "{}:{}".format(qemu_target, source_name)
337                 if source.get("protocol") in ["rbd", "gluster"]:
338                     for pool_i in conn.listAllStoragePools():
339                         pool_i_xml = ElementTree.fromstring(pool_i.XMLDesc())
340                         name_node = pool_i_xml.find("source/name")
341                         if name_node is not None and source_name.startswith(
342                             "{}/".format(name_node.text)
343                         ):
344                             qemu_target = "{}{}".format(
345                                 pool_i.name(), source_name[len(name_node.text) :]
346                             )
347                             break
348                 if elem.get("device", "disk") == "cdrom":
349                     host_node = source.find("host")
350                     if host_node is not None:
351                         hostname = host_node.get("name")
352                         port = host_node.get("port")
353                         qemu_target = urllib.parse.urlunparse(
354                             (
355                                 source.get("protocol"),
356                                 "{}:{}".format(hostname, port) if port else hostname,
357                                 source_name,
358                                 "",
359                                 saxutils.unescape(source.get("query", "")),
360                                 "",
361                             )
362                         )
363             elif disk_type == "volume":
364                 pool_name = source.get("pool")
365                 volume_name = source.get("volume")
366                 qemu_target, extra_properties = _get_disk_volume_data(
367                     pool_name, volume_name
368                 )
369             if not qemu_target:
370                 continue
371             disk = {
372                 "file": qemu_target,
373                 "type": elem.get("device"),
374             }
375             if driver is not None and "type" in driver.attrib:
376                 disk["file format"] = driver.get("type")
377             if extra_properties:
378                 disk.update(extra_properties)
379             disks[target.get("dev")] = disk
380     return disks
381 def _libvirt_creds():
382     g_cmd = ["grep", "^\\s*group", "/etc/libvirt/qemu.conf"]
383     u_cmd = ["grep", "^\\s*user", "/etc/libvirt/qemu.conf"]
384     try:
385         stdout = subprocess.Popen(g_cmd, stdout=subprocess.PIPE).communicate()[0]
386         group = salt.utils.stringutils.to_str(stdout).split('"')[1]
387     except IndexError:
388         group = "root"
389     try:
390         stdout = subprocess.Popen(u_cmd, stdout=subprocess.PIPE).communicate()[0]
391         user = salt.utils.stringutils.to_str(stdout).split('"')[1]
392     except IndexError:
393         user = "root"
394     return {"user": user, "group": group}
395 def _migrate(dom, dst_uri, **kwargs):
396     flags = 0
397     params = {}
398     migrated_state = libvirt.VIR_DOMAIN_RUNNING_MIGRATED
399     if kwargs.get("live", True):
400         flags |= libvirt.VIR_MIGRATE_LIVE
401     if kwargs.get("persistent", True):
402         flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
403     if kwargs.get("undefinesource", True):
404         flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
405     max_bandwidth = kwargs.get("max_bandwidth")
406     if max_bandwidth:
407         try:
408             bandwidth_value = int(max_bandwidth)
409         except ValueError:
410             raise SaltInvocationError(
411                 "Invalid max_bandwidth value: {}".format(max_bandwidth)
412             )
413         dom.migrateSetMaxSpeed(bandwidth_value)
414     max_downtime = kwargs.get("max_downtime")
415     if max_downtime:
416         try:
417             downtime_value = int(max_downtime)
418         except ValueError:
419             raise SaltInvocationError(
420                 "Invalid max_downtime value: {}".format(max_downtime)
421             )
422         dom.migrateSetMaxDowntime(downtime_value)
423     if kwargs.get("offline") is True:
424         flags |= libvirt.VIR_MIGRATE_OFFLINE
425         migrated_state = libvirt.VIR_DOMAIN_RUNNING_UNPAUSED
426     if kwargs.get("compressed") is True:
427         flags |= libvirt.VIR_MIGRATE_COMPRESSED
428     comp_methods = kwargs.get("comp_methods")
429     if comp_methods:
430         params[libvirt.VIR_MIGRATE_PARAM_COMPRESSION] = comp_methods.split(",")
431     comp_options = {
432         "comp_mt_level": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_MT_LEVEL,
433         "comp_mt_threads": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_MT_THREADS,
434         "comp_mt_dthreads": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_MT_DTHREADS,
435         "comp_xbzrle_cache": libvirt.VIR_MIGRATE_PARAM_COMPRESSION_XBZRLE_CACHE,
436     }
437     for (comp_option, param_key) in comp_options.items():
438         comp_option_value = kwargs.get(comp_option)
439         if comp_option_value:
440             try:
441                 params[param_key] = int(comp_option_value)
442             except ValueError:
443                 raise SaltInvocationError("Invalid {} value".format(comp_option))
444     parallel_connections = kwargs.get("parallel_connections")
445     if parallel_connections:
446         try:
447             params[libvirt.VIR_MIGRATE_PARAM_PARALLEL_CONNECTIONS] = int(
448                 parallel_connections
449             )
450         except ValueError:
451             raise SaltInvocationError("Invalid parallel_connections value")
452         flags |= libvirt.VIR_MIGRATE_PARALLEL
453     if __salt__["config.get"]("virt:tunnel"):
454         if parallel_connections:
455             raise SaltInvocationError(
456                 "Parallel migration isn't compatible with tunneled migration"
457             )
458         flags |= libvirt.VIR_MIGRATE_PEER2PEER
459         flags |= libvirt.VIR_MIGRATE_TUNNELLED
460     if kwargs.get("postcopy") is True:
461         flags |= libvirt.VIR_MIGRATE_POSTCOPY
462     postcopy_bandwidth = kwargs.get("postcopy_bandwidth")
463     if postcopy_bandwidth:
464         try:
465             postcopy_bandwidth_value = int(postcopy_bandwidth)
466         except ValueError:
467             raise SaltInvocationError("Invalid postcopy_bandwidth value")
468         dom.migrateSetMaxSpeed(
469             postcopy_bandwidth_value,
470             flags=libvirt.VIR_DOMAIN_MIGRATE_MAX_SPEED_POSTCOPY,
471         )
472     copy_storage = kwargs.get("copy_storage")
473     if copy_storage:
474         if copy_storage == "all":
475             flags |= libvirt.VIR_MIGRATE_NON_SHARED_DISK
476         elif copy_storage in ["inc", "incremental"]:
477             flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
478         else:
479             raise SaltInvocationError("invalid copy_storage value")
480     try:
481         state = False
482         dst_conn = __get_conn(
483             connection=dst_uri,
484             username=kwargs.get("username"),
485             password=kwargs.get("password"),
486         )
487         new_dom = dom.migrate3(dconn=dst_conn, params=params, flags=flags)
488         if new_dom:
489             state = new_dom.state()
490         dst_conn.close()
491         return state and migrated_state in state
492     except libvirt.libvirtError as err:
493         dst_conn.close()
494         raise CommandExecutionError(err.get_error_message())
495 def _get_volume_path(pool, volume_name):
496     if volume_name in pool.listVolumes():
497         volume = pool.storageVolLookupByName(volume_name)
498         volume_xml = ElementTree.fromstring(volume.XMLDesc())
499         return volume_xml.find("./target/path").text
500     pool_xml = ElementTree.fromstring(pool.XMLDesc())
501     pool_path = pool_xml.find("./target/path").text
502     return pool_path + "/" + volume_name
503 def _disk_from_pool(conn, pool, pool_xml, volume_name):
504     pool_type = pool_xml.get("type")
505     disk_context = {}
506     if pool_type in ["dir", "netfs", "fs"]:
507         disk_context["type"] = "file"
508         disk_context["source_file"] = _get_volume_path(pool, volume_name)
509     elif pool_type in ["logical", "disk", "iscsi", "scsi"]:
510         disk_context["type"] = "block"
511         disk_context["format"] = "raw"
512         disk_context["source_file"] = _get_volume_path(pool, volume_name)
513     elif pool_type in ["rbd", "gluster", "sheepdog"]:
514         disk_context["type"] = "network"
515         disk_context["protocol"] = pool_type
516         disk_context["hosts"] = [
517             {"name": host.get("name"), "port": host.get("port")}
518             for host in pool_xml.findall(".//host")
519         ]
520         dir_node = pool_xml.find("./source/dir")
521         name_node = pool_xml.find("./source/name")
522         if name_node is not None:
523             disk_context["volume"] = "{}/{}".format(name_node.text, volume_name)
524         auth_node = pool_xml.find("./source/auth")
525         if auth_node is not None:
526             username = auth_node.get("username")
527             secret_node = auth_node.find("./secret")
528             usage = secret_node.get("usage")
529             if not usage:
530                 uuid = secret_node.get("uuid")
531                 usage = conn.secretLookupByUUIDString(uuid).usageID()
532             disk_context["auth"] = {
533                 "type": "ceph",
534                 "username": username,
535                 "usage": usage,
536             }
537     return disk_context
538 def _handle_unit(s, def_unit="m"):
539     m = re.match(r"(?P&lt;value&gt;[0-9.]*)\s*(?P&lt;unit&gt;.*)$", str(s).strip())
540     value = m.group("value")
541     unit = m.group("unit").lower() or def_unit
542     try:
543         value = int(value)
544     except ValueError:
545         try:
546             value = float(value)
547         except ValueError:
548             raise SaltInvocationError("invalid number")
549     dec = False
550     if re.match(r"[kmgtpezy]b$", unit):
551         dec = True
552     elif not re.match(r"(b|[kmgtpezy](ib)?)$", unit):
553         raise SaltInvocationError("invalid units")
554     p = "bkmgtpezy".index(unit[0])
555     value *= 10 ** (p * 3) if dec else 2 ** (p * 10)
556     return int(value)
557 def nesthash(value=None):
558     return collections.defaultdict(nesthash, value or {})
559 def _gen_xml(
560     conn,
561     name,
562     cpu,
563     mem,
564     diskp,
565     nicp,
566     hypervisor,
567     os_type,
568     arch,
569     graphics=None,
570     boot=None,
571     boot_dev=None,
572     numatune=None,
573     hypervisor_features=None,
574     clock=None,
575     serials=None,
576     consoles=None,
577     stop_on_reboot=False,
578     host_devices=None,
579     **kwargs
580 ):
581     context = {
582         "hypervisor": hypervisor,
583         "name": name,
584         "hypervisor_features": hypervisor_features or {},
585         "clock": clock or {},
586         "on_reboot": "destroy" if stop_on_reboot else "restart",
587     }
588     context["to_kib"] = lambda v: int(_handle_unit(v) / 1024)
589     context["yesno"] = lambda v: "yes" if v else "no"
590     context["mem"] = nesthash()
591     if isinstance(mem, int):
592         context["mem"]["boot"] = mem
593         context["mem"]["current"] = mem
594     elif isinstance(mem, dict):
595         context["mem"] = nesthash(mem)
596     context["cpu"] = nesthash()
597     context["cputune"] = nesthash()
598     if isinstance(cpu, int):
599         context["cpu"]["maximum"] = str(cpu)
600     elif isinstance(cpu, dict):
601         context["cpu"] = nesthash(cpu)
602     if clock:
603         offset = "utc" if clock.get("utc", True) else "localtime"
604         if "timezone" in clock:
605             offset = "timezone"
606         context["clock"]["offset"] = offset
607     if hypervisor in ["qemu", "kvm"]:
608         context["numatune"] = numatune if numatune else {}
609         context["controller_model"] = False
610     elif hypervisor == "vmware":
611         context["controller_model"] = "lsilogic"
612     if graphics:
613         if "listen" not in graphics:
614             graphics["listen"] = {"type": "address", "address": "0.0.0.0"}
615         elif (
616             "address" not in graphics["listen"]
617             and graphics["listen"]["type"] == "address"
618         ):
619             graphics["listen"]["address"] = "0.0.0.0"
620         if graphics.get("type", "none") == "none":
621             graphics = None
622     context["graphics"] = graphics
623     context["boot_dev"] = boot_dev.split() if boot_dev is not None else ["hd"]
624     context["boot"] = boot if boot else {}
625     efi_value = context["boot"].get("efi", None) if boot else None
626     if efi_value is True:
627         context["boot"]["os_attrib"] = "firmware='efi'"
628     elif efi_value is not None and type(efi_value) != bool:
629         raise SaltInvocationError("Invalid efi value")
630     if os_type == "xen":
631         if __grains__["os_family"] == "Suse":
632             if not boot or not boot.get("kernel", None):
633                 paths = [
634                     path
635                     for path in ["/usr/share", "/usr/lib"]
636                     if os.path.exists(path + "/grub2/x86_64-xen/grub.xen")
637                 ]
638                 if not paths:
639                     raise CommandExecutionError("grub-x86_64-xen needs to be installed")
640                 context["boot"]["kernel"] = paths[0] + "/grub2/x86_64-xen/grub.xen"
641                 context["boot_dev"] = []
642     default_port = 23023
643     default_chardev_type = "tcp"
644     chardev_types = ["serial", "console"]
645     for chardev_type in chardev_types:
646         context[chardev_type + "s"] = []
647         parameter_value = locals()[chardev_type + "s"]
648         if parameter_value is not None:
649             for chardev in parameter_value:
650                 chardev_context = chardev
651                 chardev_context["type"] = chardev.get("type", default_chardev_type)
652                 if chardev_context["type"] == "tcp":
653                     chardev_context["port"] = chardev.get("port", default_port)
654                     chardev_context["protocol"] = chardev.get("protocol", "telnet")
655                 context[chardev_type + "s"].append(chardev_context)
656     context["disks"] = []
657     disk_bus_map = {"virtio": "vd", "xen": "xvd", "fdc": "fd", "ide": "hd"}
658     targets = []
659     for i, disk in enumerate(diskp):
660         prefix = disk_bus_map.get(disk["model"], "sd")
661         disk_context = {
662             "device": disk.get("device", "disk"),
663             "target_dev": _get_disk_target(targets, len(diskp), prefix),
664             "disk_bus": disk["model"],
665             "format": disk.get("format", "raw"),
666             "index": str(i),
667             "io": disk.get("io", "native"),
668             "iothread": disk.get("iothread_id", None),
669         }
670         targets.append(disk_context["target_dev"])
671         if disk.get("source_file"):
672             url = urllib.parse.urlparse(disk["source_file"])
673             if not url.scheme or not url.hostname:
674                 disk_context["source_file"] = disk["source_file"]
675                 disk_context["type"] = "file"
676             elif url.scheme in ["http", "https", "ftp", "ftps", "tftp"]:
677                 disk_context["type"] = "network"
678                 disk_context["protocol"] = url.scheme
679                 disk_context["volume"] = url.path
680                 disk_context["query"] = saxutils.escape(url.query)
681                 disk_context["hosts"] = [{"name": url.hostname, "port": url.port}]
682         elif disk.get("pool"):
683             disk_context["volume"] = disk["filename"]
684             pool = conn.storagePoolLookupByName(disk["pool"])
685             pool_xml = ElementTree.fromstring(pool.XMLDesc())
686             pool_type = pool_xml.get("type")
687             if hypervisor == "xen" or pool_type in ["rbd", "gluster", "sheepdog"]:
688                 disk_context.update(
689                     _disk_from_pool(conn, pool, pool_xml, disk_context["volume"])
690                 )
691             else:
692                 if pool_type in ["disk", "logical"]:
693                     disk_context["format"] = "raw"
694                 disk_context["type"] = "volume"
695                 disk_context["pool"] = disk["pool"]
696         else:
697             disk_context["type"] = "file"
698         if hypervisor in ["qemu", "kvm", "bhyve", "xen"]:
699             disk_context["address"] = False
700             disk_context["driver"] = True
701         elif hypervisor in ["esxi", "vmware"]:
702             disk_context["address"] = True
703             disk_context["driver"] = False
704         context["disks"].append(disk_context)
705     context["nics"] = nicp
706     hostdev_context = []
707     try:
708         for hostdev_name in host_devices or []:
709             hostdevice = conn.nodeDeviceLookupByName(hostdev_name)
710             doc = ElementTree.fromstring(hostdevice.XMLDesc())
711             if "pci" in hostdevice.listCaps():
712                 hostdev_context.append(
713                     {
714                         "type": "pci",
715                         "domain": "0x{:04x}".format(
716                             int(doc.find("./capability[@type='pci']/domain").text)
717                         ),
718                         "bus": "0x{:02x}".format(
719                             int(doc.find("./capability[@type='pci']/bus").text)
720                         ),
721                         "slot": "0x{:02x}".format(
722                             int(doc.find("./capability[@type='pci']/slot").text)
723                         ),
724                         "function": "0x{}".format(
725                             doc.find("./capability[@type='pci']/function").text
726                         ),
727                     }
728                 )
729             elif "usb_device" in hostdevice.listCaps():
730                 vendor_id = doc.find(".//vendor").get("id")
731                 product_id = doc.find(".//product").get("id")
732                 hostdev_context.append(
733                     {"type": "usb", "vendor": vendor_id, "product": product_id}
734                 )
735     except libvirt.libvirtError as err:
736         conn.close()
737         raise CommandExecutionError(
738             "Failed to get host devices: " + err.get_error_message()
739         )
740     context["hostdevs"] = hostdev_context
741     context["os_type"] = os_type
742     context["arch"] = arch
743     fn_ = "libvirt_domain.jinja"
744     try:
745         template = JINJA.get_template(fn_)
746     except jinja2.exceptions.TemplateNotFound:
747         log.error("Could not load template %s", fn_)
748         return ""
749     return template.render(**context)
750 def _gen_vol_xml(
751     name,
752     size,
753     format=None,
754     allocation=0,
755     type=None,
756     permissions=None,
757     backing_store=None,
758     nocow=False,
759 ):
760     size = int(size) * 1024  # MB
761     context = {
762         "type": type,
763         "name": name,
764         "target": {"permissions": permissions, "nocow": nocow},
765         "format": format,
766         "size": str(size),
767         "allocation": str(int(allocation) * 1024),
768         "backingStore": backing_store,
769     }
770     fn_ = "libvirt_volume.jinja"
771     try:
772         template = JINJA.get_template(fn_)
773     except jinja2.exceptions.TemplateNotFound:
774         log.error("Could not load template %s", fn_)
775         return ""
776     return template.render(**context)
777 def _gen_net_xml(
778     name,
779     bridge,
780     forward,
781     vport,
782     tag=None,
783     ip_configs=None,
784     mtu=None,
785     domain=None,
786     nat=None,
787     interfaces=None,
788     addresses=None,
789     physical_function=None,
790     dns=None,
791 ):
792     if isinstance(vport, str):
793         vport_context = {"type": vport}
794     else:
795         vport_context = vport
796     if isinstance(tag, (str, int)):
797         tag_context = {"tags": [{"id": tag}]}
798     else:
799         tag_context = tag
800     addresses_context = []
801     if addresses:
802         matches = [
803             re.fullmatch(r"([0-9]+):([0-9A-Fa-f]+):([0-9A-Fa-f]+)\.([0-9])", addr)
804             for addr in addresses.lower().split(" ")
805         ]
806         addresses_context = [
807             {
808                 "domain": m.group(1),
809                 "bus": m.group(2),
810                 "slot": m.group(3),
811                 "function": m.group(4),
812             }
813             for m in matches
814             if m
815         ]
816     context = {
817         "name": name,
818         "bridge": bridge,
819         "mtu": mtu,
820         "domain": domain,
821         "forward": forward,
822         "nat": nat,
823         "interfaces": interfaces.split(" ") if interfaces else [],
824         "addresses": addresses_context,
825         "pf": physical_function,
826         "vport": vport_context,
827         "vlan": tag_context,
828         "dns": dns,
829         "ip_configs": [
830             {
831                 "address": ipaddress.ip_network(config["cidr"]),
832                 "dhcp_ranges": config.get("dhcp_ranges", []),
833                 "hosts": config.get("hosts", {}),
834                 "bootp": config.get("bootp", {}),
835                 "tftp": config.get("tftp"),
836             }
837             for config in ip_configs or []
838         ],
839         "yesno": lambda v: "yes" if v else "no",
840     }
841     fn_ = "libvirt_network.jinja"
842     try:
843         template = JINJA.get_template(fn_)
844     except jinja2.exceptions.TemplateNotFound:
845         log.error("Could not load template %s", fn_)
846         return ""
847     return template.render(**context)
848 def _gen_pool_xml(
849     name,
850     ptype,
851     target=None,
852     permissions=None,
853     source_devices=None,
854     source_dir=None,
855     source_adapter=None,
856     source_hosts=None,
857     source_auth=None,
858     source_name=None,
859     source_format=None,
860     source_initiator=None,
861 ):
862     hosts = [host.split(":") for host in source_hosts or []]
863     source = None
864     if any(
865         [
866             source_devices,
867             source_dir,
868             source_adapter,
869             hosts,
870             source_auth,
871             source_name,
872             source_format,
873             source_initiator,
874         ]
875     ):
876         source = {
877             "devices": source_devices or [],
878             "dir": source_dir
879             if source_format != "cifs" or not source_dir
880             else source_dir.lstrip("/"),
881             "adapter": source_adapter,
882             "hosts": [
883                 {"name": host[0], "port": host[1] if len(host) &gt; 1 else None}
884                 for host in hosts
885             ],
886             "auth": source_auth,
887             "name": source_name,
888             "format": source_format,
889             "initiator": source_initiator,
890         }
891     context = {
892         "name": name,
893         "ptype": ptype,
894         "target": {"path": target, "permissions": permissions},
895         "source": source,
896     }
897     fn_ = "libvirt_pool.jinja"
898     try:
899         template = JINJA.get_template(fn_)
900     except jinja2.exceptions.TemplateNotFound:
901         log.error("Could not load template %s", fn_)
902         return ""
903     return template.render(**context)
904 def _gen_secret_xml(auth_type, usage, description):
905     context = {
906         "type": auth_type,
907         "usage": usage,
908         "description": description,
909     }
910     fn_ = "libvirt_secret.jinja"
911     try:
912         template = JINJA.get_template(fn_)
913     except jinja2.exceptions.TemplateNotFound:
914         log.error("Could not load template %s", fn_)
915         return ""
916     return template.render(**context)
917 def _get_images_dir():
918     img_dir = __salt__["config.get"]("virt:images")
919     log.debug("Image directory from config option `virt:images` is %s", img_dir)
920     return img_dir
921 def _zfs_image_create(
922     vm_name,
923     pool,
924     disk_name,
925     hostname_property_name,
926     sparse_volume,
927     disk_size,
928     disk_image_name,
929 ):
930     if not disk_image_name and not disk_size:
931         raise CommandExecutionError(
932             "Unable to create new disk {}, please specify"
933             " the disk image name or disk size argument".format(disk_name)
934         )
935     if not pool:
936         raise CommandExecutionError(
937             "Unable to create new disk {}, please specify the disk pool name".format(
938                 disk_name
939             )
940         )
941     destination_fs = os.path.join(pool, "{}.{}".format(vm_name, disk_name))
942     log.debug("Image destination will be %s", destination_fs)
943     existing_disk = __salt__["zfs.list"](name=pool)
944     if "error" in existing_disk:
945         raise CommandExecutionError(
946             "Unable to create new disk {}. {}".format(
947                 destination_fs, existing_disk["error"]
948             )
949         )
950     elif destination_fs in existing_disk:
951         log.info("ZFS filesystem %s already exists. Skipping creation", destination_fs)
952         blockdevice_path = os.path.join("/dev/zvol", pool, vm_name)
953         return blockdevice_path
954     properties = {}
955     if hostname_property_name:
956         properties[hostname_property_name] = vm_name
957     if disk_image_name:
958         __salt__["zfs.clone"](
959             name_a=disk_image_name, name_b=destination_fs, properties=properties
960         )
961     elif disk_size:
962         __salt__["zfs.create"](
963             name=destination_fs,
964             properties=properties,
965             volume_size=disk_size,
966             sparse=sparse_volume,
967         )
968     blockdevice_path = os.path.join(
969         "/dev/zvol", pool, "{}.{}".format(vm_name, disk_name)
970     )
971     log.debug("Image path will be %s", blockdevice_path)
972     return blockdevice_path
973 def _qemu_image_create(disk, create_overlay=False, saltenv="base"):
974     disk_size = disk.get("size", None)
975     disk_image = disk.get("image", None)
976     if not disk_size and not disk_image:
977         raise CommandExecutionError(
978             "Unable to create new disk {}, please specify"
979             " disk size and/or disk image argument".format(disk["filename"])
980         )
981     img_dest = disk["source_file"]
982     log.debug("Image destination will be %s", img_dest)
983     img_dir = os.path.dirname(img_dest)
984     log.debug("Image destination directory is %s", img_dir)
985     if not os.path.exists(img_dir):
986         os.makedirs(img_dir)
987     if disk_image:
988         log.debug("Create disk from specified image %s", disk_image)
989         sfn = __salt__["cp.cache_file"](disk_image, saltenv)
990         qcow2 = False
991         if salt.utils.path.which("qemu-img"):
992             res = __salt__["cmd.run"]('qemu-img info "{}"'.format(sfn))
993             imageinfo = salt.utils.yaml.safe_load(res)
994             qcow2 = imageinfo["file format"] == "qcow2"
995             if create_overlay and qcow2:
996                 log.info("Cloning qcow2 image %s using copy on write", sfn)
997                 __salt__<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>["cmd.run"](
998                     'qemu-img create -f qcow2 -o backing_file="{}" "{}"'.format(
999                         sfn, img_dest
1000                     ).split()
1001                 )
1002             else:
1003                 log.debug("Copying %s to %s", sfn, img_dest)
1004                 salt.utils.files.copyfile(</b></font>sfn, img_dest)
1005             mask = salt.utils.files.get_umask()
1006             if disk_size and qcow2:
1007                 log.debug("Resize qcow2 image to %sM", disk_size)
1008                 __salt__["cmd.run"](
1009                     'qemu-img resize "{}" {}M'.format(img_dest, disk_size)
1010                 )
1011             log.debug("Apply umask and remove exec bit")
1012             mode = (0o0777 ^ mask) &amp; 0o0666
1013             os.chmod(img_dest, mode)
1014         except OSError as err:
1015             raise CommandExecutionError(
1016                 "Problem while copying image. {} - {}".format(disk_image, err)
1017             )
1018     else:
1019         try:
1020             mask = salt.utils.files.get_umask()
1021             if disk_size:
1022                 log.debug("Create empty image with size %sM", disk_size)
1023                 __salt__["cmd.run"](
1024                     'qemu-img create -f {} "{}" {}M'.format(
1025                         disk.get("format", "qcow2"), img_dest, disk_size
1026                     )
1027                 )
1028             else:
1029                 raise CommandExecutionError(
1030                     "Unable to create new disk {},"
1031                     " please specify &lt;size&gt; argument".format(img_dest)
1032                 )
1033             log.debug("Apply umask and remove exec bit")
1034             mode = (0o0777 ^ mask) &amp; 0o0666
1035             os.chmod(img_dest, mode)
1036         except OSError as err:
1037             raise CommandExecutionError(
1038                 "Problem while creating volume {} - {}".format(img_dest, err)
1039             )
1040     return img_dest
1041 def _seed_image(seed_cmd, img_path, name, config, install, pub_key, priv_key):
1042     log.debug("Seeding image")
1043     __salt__[seed_cmd](
1044         img_path,
1045         id_=name,
1046         config=config,
1047         install=install,
1048         pub_key=pub_key,
1049         priv_key=priv_key,
1050     )
1051 def _disk_volume_create(conn, disk, seeder=None, saltenv="base"):
1052     if disk.get("overlay_image"):
1053         raise SaltInvocationError(
1054             "Disk overlay_image property is not supported when creating volumes,"
1055             "use backing_store_path and backing_store_format instead."
1056         )
1057     pool = conn.storagePoolLookupByName(disk["pool"])
1058     if disk["filename"] in pool.listVolumes():
1059         return
1060     pool_type = ElementTree.fromstring(pool.XMLDesc()).get("type")
1061     backing_path = disk.get("backing_store_path")
1062     backing_format = disk.get("backing_store_format")
1063     backing_store = None
1064     if (
1065         backing_path
1066         and backing_format
1067         and (disk.get("format") == "qcow2" or pool_type == "logical")
1068     ):
1069         backing_store = {"path": backing_path, "format": backing_format}
1070     if backing_store and disk.get("image"):
1071         raise SaltInvocationError(
1072             "Using a template image with a backing store is not possible, "
1073             "choose either of them."
1074         )
1075     vol_xml = _gen_vol_xml(
1076         disk["filename"],
1077         disk.get("size", 0),
1078         format=disk.get("format"),
1079         backing_store=backing_store,
1080     )
1081     _define_vol_xml_str(conn, vol_xml, disk.get("pool"))
1082     if disk.get("image"):
1083         log.debug("Caching disk template image: %s", disk.get("image"))
1084         cached_path = __salt__["cp.cache_file"](disk.get("image"), saltenv)
1085         if seeder:
1086             seeder(cached_path)
1087         _volume_upload(
1088             conn,
1089             disk["pool"],
1090             disk["filename"],
1091             cached_path,
1092             sparse=disk.get("format") == "qcow2",
1093         )
1094 def _disk_profile(conn, profile, hypervisor, disks, vm_name):
1095     default = [{"system": {"size": 8192}}]
1096     if hypervisor == "vmware":
1097         overlay = {"format": "vmdk", "model": "scsi", "device": "disk"}
1098     elif hypervisor in ["qemu", "kvm"]:
1099         overlay = {"device": "disk", "model": "virtio"}
1100     elif hypervisor == "xen":
1101         overlay = {"device": "disk", "model": "xen"}
1102     elif hypervisor == "bhyve":
1103         overlay = {"format": "raw", "model": "virtio", "sparse_volume": False}
1104     else:
1105         overlay = {}
1106     disklist = []
1107     if profile:
1108         disklist = copy.deepcopy(
1109             __salt__["config.get"]("virt:disk", {}).get(profile, default)
1110         )
1111         disklist = [dict(d, name=name) for disk in disklist for name, d in disk.items()]
1112     if disks:
1113         for udisk in disks:
1114             if "name" in udisk:
1115                 found = [disk for disk in disklist if udisk["name"] == disk["name"]]
1116                 if found:
1117                     found[0].update(udisk)
1118                 else:
1119                     disklist.append(udisk)
1120     pool_caps = _pool_capabilities(conn)
1121     for disk in disklist:
1122         if disk.get("device", "disk") == "cdrom" and "model" not in disk:
1123             disk["model"] = "ide"
1124         for key, val in overlay.items():
1125             if key not in disk:
1126                 disk[key] = val
1127         if disk.get("source_file") and os.path.exists(disk["source_file"]):
1128             disk["filename"] = os.path.basename(disk["source_file"])
1129             if not disk.get("format"):
1130                 disk["format"] = (
1131                     "qcow2" if disk.get("device", "disk") != "cdrom" else "raw"
1132                 )
1133         elif vm_name and disk.get("device", "disk") == "disk":
1134             _fill_disk_filename(conn, vm_name, disk, hypervisor, pool_caps)
1135     return disklist
1136 def _fill_disk_filename(conn, vm_name, disk, hypervisor, pool_caps):
1137     disk["filename"] = "{}_{}".format(vm_name, disk["name"])
1138     base_dir = disk.get("pool", None)
1139     if hypervisor in ["qemu", "kvm", "xen"]:
1140         if not base_dir:
1141             base_dir = _get_images_dir()
1142         if base_dir not in conn.listStoragePools():
1143             if not disk.get("format"):
1144                 disk["format"] = "qcow2"
1145             disk["filename"] = "{}.{}".format(disk["filename"], disk["format"])
1146             disk["source_file"] = os.path.join(base_dir, disk["filename"])
1147         else:
1148             if "pool" not in disk:
1149                 disk["pool"] = base_dir
1150             pool_obj = conn.storagePoolLookupByName(base_dir)
1151             pool_xml = ElementTree.fromstring(pool_obj.XMLDesc())
1152             pool_type = pool_xml.get("type")
1153             if pool_type == "disk":
1154                 device = pool_xml.find("./source/device").get("path")
1155                 all_volumes = pool_obj.listVolumes()
1156                 if disk.get("source_file") not in all_volumes:
1157                     indexes = [
1158                         int(re.sub("[a-z]+", "", vol_name)) for vol_name in all_volumes
1159                     ] or [0]
1160                     index = min(
1161                         idx for idx in range(1, max(indexes) + 2) if idx not in indexes
1162                     )
1163                     disk["filename"] = "{}{}".format(os.path.basename(device), index)
1164             if disk.get("source_file"):
1165                 if not disk.get("source_file") in pool_obj.listVolumes():
1166                     raise SaltInvocationError(
1167                         "{} volume doesn't exist in pool {}".format(
1168                             disk.get("source_file"), base_dir
1169                         )
1170                     )
1171                 disk["filename"] = disk["source_file"]
1172                 del disk["source_file"]
1173             if not disk.get("format"):
1174                 volume_options = (
1175                     [
1176                         type_caps.get("options", {}).get("volume", {})
1177                         for type_caps in pool_caps.get("pool_types")
1178                         if type_caps["name"] == pool_type
1179                     ]
1180                     or [{}]
1181                 )[0]
1182                 if "qcow2" in volume_options.get("targetFormatType", []):
1183                     disk["format"] = "qcow2"
1184                 else:
1185                     disk["format"] = volume_options.get("default_format", None)
1186     elif hypervisor == "bhyve" and vm_name:
1187         disk["filename"] = "{}.{}".format(vm_name, disk["name"])
1188         disk["source_file"] = os.path.join(
1189             "/dev/zvol", base_dir or "", disk["filename"]
1190         )
1191     elif hypervisor in ["esxi", "vmware"]:
1192         if not base_dir:
1193             base_dir = __salt__["config.get"]("virt:storagepool", "[0] ")
1194         disk["filename"] = "{}.{}".format(disk["filename"], disk["format"])
1195         disk["source_file"] = "{}{}".format(base_dir, disk["filename"])
1196 def _complete_nics(interfaces, hypervisor):
1197     vmware_overlay = {"type": "bridge", "source": "DEFAULT", "model": "e1000"}
1198     kvm_overlay = {"type": "bridge", "source": "br0", "model": "virtio"}
1199     xen_overlay = {"type": "bridge", "source": "br0", "model": None}
1200     bhyve_overlay = {"type": "bridge", "source": "bridge0", "model": "virtio"}
1201     overlays = {
1202         "xen": xen_overlay,
1203         "kvm": kvm_overlay,
1204         "qemu": kvm_overlay,
1205         "vmware": vmware_overlay,
1206         "bhyve": bhyve_overlay,
1207     }
1208     def _normalize_net_types(attributes):
1209         for type_ in ["bridge", "network"]:
1210             if type_ in attributes:
1211                 attributes["type"] = type_
1212                 attributes["source"] = attributes.pop(type_)
1213         attributes["type"] = attributes.get("type", None)
1214         attributes["source"] = attributes.get("source", None)
1215     def _apply_default_overlay(attributes):
1216         for key, value in overlays[hypervisor].items():
1217             if key not in attributes or not attributes[key]:
1218                 attributes[key] = value
1219     for interface in interfaces:
1220         _normalize_net_types(interface)
1221         if hypervisor in overlays:
1222             _apply_default_overlay(interface)
1223     return interfaces
1224 def _nic_profile(profile_name, hypervisor):
1225     config_data = __salt__["config.get"]("virt:nic", {}).get(
1226         profile_name, [{"eth0": {}}]
1227     )
1228     interfaces = []
1229     def append_dict_profile_to_interface_list(profile_dict):
1230         for interface_name, attributes in profile_dict.items():
1231             attributes["name"] = interface_name
1232             interfaces.append(attributes)
1233     if isinstance(config_data, dict):
1234         append_dict_profile_to_interface_list(config_data)
1235     elif isinstance(config_data, list):
1236         for interface in config_data:
1237             if isinstance(interface, dict):
1238                 if len(interface) == 1:
1239                     append_dict_profile_to_interface_list(interface)
1240                 else:
1241                     interfaces.append(interface)
1242     return _complete_nics(interfaces, hypervisor)
1243 def _get_merged_nics(hypervisor, profile, interfaces=None):
1244     nicp = _nic_profile(profile, hypervisor) if profile else []
1245     log.debug("NIC profile is %s", nicp)
1246     if interfaces:
1247         users_nics = _complete_nics(interfaces, hypervisor)
1248         for unic in users_nics:
1249             found = [nic for nic in nicp if nic["name"] == unic["name"]]
1250             if found:
1251                 found[0].update(unic)
1252             else:
1253                 nicp.append(unic)
1254         log.debug("Merged NICs: %s", nicp)
1255     return nicp
1256 def _handle_remote_boot_params(orig_boot):
1257     saltinst_dir = None
1258     new_boot = orig_boot.copy()
1259     keys = orig_boot.keys()
1260     cases = [
1261         {"efi"},
1262         {"kernel", "initrd", "efi"},
1263         {"kernel", "initrd", "cmdline", "efi"},
1264         {"loader", "nvram"},
1265         {"kernel", "initrd"},
1266         {"kernel", "initrd", "cmdline"},
1267         {"kernel", "initrd", "loader", "nvram"},
1268         {"kernel", "initrd", "cmdline", "loader", "nvram"},
1269     ]
1270     if keys in cases:
1271         for key in keys:
1272             if key == "efi" and type(orig_boot.get(key)) == bool:
1273                 new_boot[key] = orig_boot.get(key)
1274             elif orig_boot.get(key) is not None and salt.utils.virt.check_remote(
1275                 orig_boot.get(key)
1276             ):
1277                 if saltinst_dir is None:
1278                     os.makedirs(CACHE_DIR)
1279                     saltinst_dir = CACHE_DIR
1280                 new_boot[key] = salt.utils.virt.download_remote(
1281                     orig_boot.get(key), saltinst_dir
1282                 )
1283         return new_boot
1284     else:
1285         raise SaltInvocationError(
1286             "Invalid boot parameters,It has to follow this combination: [(kernel,"
1287             " initrd) or/and cmdline] or/and [(loader, nvram) or efi]"
1288         )
1289 def _handle_efi_param(boot, desc):
1290     efi_value = boot.get("efi", None) if boot else None
1291     parent_tag = desc.find("os")
1292     os_attrib = parent_tag.attrib
1293     if efi_value is False and os_attrib != {}:
1294         parent_tag.attrib.pop("firmware", None)
1295         return True
1296     elif type(efi_value) == bool and os_attrib == {}:
1297         if efi_value is True and parent_tag.find("loader") is None:
1298             parent_tag.set("firmware", "efi")
1299             return True
1300         if efi_value is False and parent_tag.find("loader") is not None:
1301             parent_tag.remove(parent_tag.find("loader"))
1302             parent_tag.remove(parent_tag.find("nvram"))
1303             return True
1304     elif type(efi_value) != bool:
1305         raise SaltInvocationError("Invalid efi value")
1306     return False
1307 def init(
1308     name,
1309     cpu,
1310     mem,
1311     nic="default",
1312     interfaces=None,
1313     hypervisor=None,
1314     start=True,  # pylint: disable=redefined-outer-name
1315     disk="default",
1316     disks=None,
1317     saltenv="base",
1318     seed=True,
1319     install=True,
1320     pub_key=None,
1321     priv_key=None,
1322     seed_cmd="seed.apply",
1323     graphics=None,
1324     os_type=None,
1325     arch=None,
1326     boot=None,
1327     boot_dev=None,
1328     numatune=None,
1329     hypervisor_features=None,
1330     clock=None,
1331     serials=None,
1332     consoles=None,
1333     stop_on_reboot=False,
1334     host_devices=None,
1335     **kwargs
1336 ):
1337     try:
1338         conn = __get_conn(**kwargs)
1339         caps = _capabilities(conn)
1340         os_types = sorted({guest["os_type"] for guest in caps["guests"]})
1341         arches = sorted({guest["arch"]["name"] for guest in caps["guests"]})
1342         virt_hypervisor = hypervisor
1343         if not virt_hypervisor:
1344             hypervisors = sorted(
1345                 {
1346                     x
1347                     for y in [
1348                         guest["arch"]["domains"].keys() for guest in caps["guests"]
1349                     ]
1350                     for x in y
1351                 }
1352             )
1353             if len(hypervisors) == 0:
1354                 raise SaltInvocationError("No supported hypervisors were found")
1355             virt_hypervisor = "kvm" if "kvm" in hypervisors else hypervisors[0]
1356         virt_hypervisor = "vmware" if virt_hypervisor == "esxi" else virt_hypervisor
1357         log.debug("Using hypervisor %s", virt_hypervisor)
1358         nicp = _get_merged_nics(virt_hypervisor, nic, interfaces)
1359         diskp = _disk_profile(conn, disk, virt_hypervisor, disks, name)
1360         for _disk in diskp:
1361             if _disk.get("device", "disk") == "cdrom":
1362                 continue
1363             log.debug("Creating disk for VM [ %s ]: %s", name, _disk)
1364             if virt_hypervisor == "vmware":
1365                 if "image" in _disk:
1366                     raise SaltInvocationError(
1367                         "virt.init does not support image "
1368                         "template in conjunction with esxi hypervisor"
1369                     )
1370                 else:
1371                     log.debug("Generating libvirt XML for %s", _disk)
1372                     volume_name = "{}/{}".format(name, _disk["name"])
1373                     filename = "{}.{}".format(volume_name, _disk["format"])
1374                     vol_xml = _gen_vol_xml(
1375                         filename, _disk["size"], format=_disk["format"]
1376                     )
1377                     _define_vol_xml_str(conn, vol_xml, pool=_disk.get("pool"))
1378             elif virt_hypervisor in ["qemu", "kvm", "xen"]:
1379                 def seeder(path):
1380                     _seed_image(
1381                         seed_cmd,
1382                         path,
1383                         name,
1384                         kwargs.get("config"),
1385                         install,
1386                         pub_key,
1387                         priv_key,
1388                     )
1389                 create_overlay = _disk.get("overlay_image", False)
1390                 format = _disk.get("format")
1391                 if _disk.get("source_file"):
1392                     if os.path.exists(_disk["source_file"]):
1393                         img_dest = _disk["source_file"]
1394                     else:
1395                         img_dest = _qemu_image_create(_disk, create_overlay, saltenv)
1396                 else:
1397                     _disk_volume_create(conn, _disk, seeder if seed else None, saltenv)
1398                     img_dest = None
1399                 if seed and img_dest and _disk.get("image", None):
1400                     seeder(img_dest)
1401             elif hypervisor in ["bhyve"]:
1402                 img_dest = _zfs_image_create(
1403                     vm_name=name,
1404                     pool=_disk.get("pool"),
1405                     disk_name=_disk.get("name"),
1406                     disk_size=_disk.get("size"),
1407                     disk_image_name=_disk.get("image"),
1408                     hostname_property_name=_disk.get("hostname_property"),
1409                     sparse_volume=_disk.get("sparse_volume"),
1410                 )
1411             else:
1412                 raise SaltInvocationError(
1413                     "Unsupported hypervisor when handling disk image: {}".format(
1414                         virt_hypervisor
1415                     )
1416                 )
1417         log.debug("Generating VM XML")
1418         if os_type is None:
1419             os_type = "hvm" if "hvm" in os_types else os_types[0]
1420         if arch is None:
1421             arch = "x86_64" if "x86_64" in arches else arches[0]
1422         if boot is not None:
1423             boot = _handle_remote_boot_params(boot)
1424         vm_xml = _gen_xml(
1425             conn,
1426             name,
1427             cpu,
1428             mem,
1429             diskp,
1430             nicp,
1431             virt_hypervisor,
1432             os_type,
1433             arch,
1434             graphics,
1435             boot,
1436             boot_dev,
1437             numatune,
1438             hypervisor_features,
1439             clock,
1440             serials,
1441             consoles,
1442             stop_on_reboot,
1443             host_devices,
1444             **kwargs
1445         )
1446         log.debug("New virtual machine definition: %s", vm_xml)
1447         conn.defineXML(vm_xml)
1448     except libvirt.libvirtError as err:
1449         conn.close()
1450         raise CommandExecutionError(err.get_error_message())
1451     if start:
1452         log.debug("Starting VM %s", name)
1453         _get_domain(conn, name).create()
1454     conn.close()
1455     return True
1456 def _disks_equal(disk1, disk2):
1457     Test if two disk elements should be considered like the same device
1458     """
1459     target1 = disk1<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.find("target")
1460     target2 = disk2.find("target")
1461     disk1_dict = xmlutil.to_dict(disk1, True)
1462     disk2_dict = xmlutil.to_dict(disk2, True)
1463     source1_dict = disk1_dict.get("source", {})
1464     source2_dict = disk2_dict.get("source", {})
1465     io1 = disk1_dict.get("driver", {}).</b></font>get("io", "native")
1466     io2 = disk2_dict.get("driver", {}).get("io", "native")
1467     if source1_dict:
1468         source1_dict.pop("index", None)
1469     if source2_dict:
1470         source2_dict.pop("index", None)
1471     return (
1472         source1_dict == source2_dict
1473         and target1 is not None
1474         and target2 is not None
1475         and target1.get("bus") == target2.get("bus")
1476         and disk1.get("device", "disk") == disk2.get("device", "disk")
1477         and target1.get("dev") == target2.get("dev")
1478         and io1 == io2
1479     )
1480 def _nics_equal(nic1, nic2):
1481     """
1482     Test if two interface elements should be considered like the same device
1483     """
1484     def _filter_nic(nic):
1485         """
1486         Filter out elements to ignore when comparing nics
1487         """
1488         source_node = nic.find("source")
1489         source_attrib = source_node.attrib if source_node is not None else {}
1490         source_type = "network" if "network" in source_attrib else nic.attrib["type"]
1491         source_getters = {
1492             "network": lambda n: n.get("network"),
1493             "bridge": lambda n: n.get("bridge"),
1494             "direct": lambda n: n.get("dev"),
1495             "hostdev": lambda n: _format_pci_address(n.find("address")),
1496         }
1497         return {
1498             "type": source_type,
1499             "source": source_getters[source_type](source_node)
1500             if source_node is not None
1501             else None,
1502             "model": nic.find("model").attrib["type"]
1503             if nic.find("model") is not None
1504             else None,
1505         }
1506     def _get_mac(nic):
1507         return (
1508             nic.find("mac").attrib["address"].lower()
1509             if nic.find("mac") is not None
1510             else None
1511         )
1512     mac1 = _get_mac(nic1)
1513     mac2 = _get_mac(nic2)
1514     macs_equal = not mac1 or not mac2 or mac1 == mac2
1515     return _filter_nic(nic1) == _filter_nic(nic2) and macs_equal
1516 def _graphics_equal(gfx1, gfx2):
1517     """
1518     Test if two graphics devices should be considered the same device
1519     """
1520     def _filter_graphics(gfx):
1521         """
1522         When the domain is running, the graphics element may contain additional properties
1523         with the default values. This function will strip down the default values.
1524         """
1525         gfx_copy = copy.deepcopy(gfx)
1526         defaults = [
1527             {"node": ".", "attrib": "port", "values": ["5900", "-1"]},
1528             {"node": ".", "attrib": "address", "values": ["127.0.0.1"]},
1529             {"node": "listen", "attrib": "address", "values": ["127.0.0.1"]},
1530         ]
1531         for default in defaults:
1532             node = gfx_copy.find(default["node"])
1533             attrib = default["attrib"]
1534             if node is not None and (
1535                 attrib in node.attrib and node.attrib[attrib] in default["values"]
1536             ):
1537                 node.attrib.pop(attrib)
1538         return gfx_copy
1539     return xmlutil.to_dict(_filter_graphics(gfx1), True) == xmlutil.to_dict(
1540         _filter_graphics(gfx2), True
1541     )
1542 def _hostdevs_equal(dev1, dev2):
1543     """
1544     Test if two hostdevs devices should be considered the same device
1545     """
1546     def _filter_hostdevs(dev):
1547         """
1548         When the domain is running, the hostdevs element may contain additional properties.
1549         This function will only keep the ones we care about
1550         """
1551         type_ = dev.get("type")
1552         definition = {
1553             "type": type_,
1554         }
1555         if type_ == "pci":
1556             address_node = dev.find("./source/address")
1557             for attr in ["domain", "bus", "slot", "function"]:
1558                 definition[attr] = address_node.get(attr)
1559         elif type_ == "usb":
1560             for attr in ["vendor", "product"]:
1561                 definition[attr] = dev.find("./source/" + attr).get("id")
1562         return definition
1563     return _filter_hostdevs(dev1) == _filter_hostdevs(dev2)
1564 def _diff_lists(old, new, comparator):
1565     """
1566     Compare lists to extract the changes
1567     :param old: old list
1568     :param new: new list
1569     :return: a dictionary with ``unchanged``, ``new``, ``deleted`` and ``sorted`` keys
1570     The sorted list is the union of unchanged and new lists, but keeping the original
1571     order from the new list.
1572     """
1573     def _remove_indent(node):
1574         """
1575         Remove the XML indentation to compare XML trees more easily
1576         """
1577         node_copy = copy.deepcopy(node)
1578         node_copy.text = None
1579         for item in node_copy.iter():
1580             item.tail = None
1581         return node_copy
1582     diff = {"unchanged": [], "new": [], "deleted": [], "sorted": []}
1583     old_devices = copy.deepcopy(old)
1584     for new_item in new:
1585         found = [
1586             item
1587             for item in old_devices
1588             if comparator(_remove_indent(item), _remove_indent(new_item))
1589         ]
1590         if found:
1591             old_devices.remove(found[0])
1592             diff["unchanged"].append(found[0])
1593             diff["sorted"].append(found[0])
1594         else:
1595             diff["new"].append(new_item)
1596             diff["sorted"].append(new_item)
1597     diff["deleted"] = old_devices
1598     return diff
1599 def _get_disk_target(targets, disks_count, prefix):
1600     """
1601     Compute the disk target name for a given prefix.
1602     :param targets: the list of already computed targets
1603     :param disks: the number of disks
1604     :param prefix: the prefix of the target name, i.e. "hd"
1605     """
1606     for i in range(disks_count):
1607         ret = "{}{}".format(prefix, string.ascii_lowercase[i])
1608         if ret not in targets:
1609             return ret
1610     return None
1611 def _diff_disk_lists(old, new):
1612     """
1613     Compare disk definitions to extract the changes and fix target devices
1614     :param old: list of ElementTree nodes representing the old disks
1615     :param new: list of ElementTree nodes representing the new disks
1616     """
1617     targets = []
1618     prefixes = ["fd", "hd", "vd", "sd", "xvd", "ubd"]
1619     for disk in new:
1620         target_node = disk.find("target")
1621         target = target_node.get("dev")
1622         prefix = [item for item in prefixes if target.startswith(item)][0]
1623         new_target = _get_disk_target(targets, len(new), prefix)
1624         target_node.set("dev", new_target)
1625         targets.append(new_target)
1626     return _diff_lists(old, new, _disks_equal)
1627 def _diff_interface_lists(old, new):
1628     """
1629     Compare network interface definitions to extract the changes
1630     :param old: list of ElementTree nodes representing the old interfaces
1631     :param new: list of ElementTree nodes representing the new interfaces
1632     """
1633     return _diff_lists(old, new, _nics_equal)
1634 def _diff_graphics_lists(old, new):
1635     """
1636     Compare graphic devices definitions to extract the changes
1637     :param old: list of ElementTree nodes representing the old graphic devices
1638     :param new: list of ElementTree nodes representing the new graphic devices
1639     """
1640     return _diff_lists(old, new, _graphics_equal)
1641 def _diff_hostdev_lists(old, new):
1642     """
1643     Compare hostdev devices definitions to extract the changes
1644     :param old: list of ElementTree nodes representing the old hostdev devices
1645     :param new: list of ElementTree nodes representing the new hostdev devices
1646     """
1647     return _diff_lists(old, new, _hostdevs_equal)
1648 def _expand_cpuset(cpuset):
1649     """
1650     Expand the libvirt cpuset and nodeset values into a list of cpu/node IDs
1651     """
1652     if cpuset is None:
1653         return None
1654     if isinstance(cpuset, int):
1655         return str(cpuset)
1656     result = set()
1657     toremove = set()
1658     for part in cpuset.split(","):
1659         m = re.match("([0-9]+)-([0-9]+)", part)
1660         if m:
1661             result |= set(range(int(m.group(1)), int(m.group(2)) + 1))
1662         elif part.startswith("^"):
1663             toremove.add(int(part[1:]))
1664         else:
1665             result.add(int(part))
1666     cpus = list(result - toremove)
1667     cpus.sort()
1668     cpus = [str(cpu) for cpu in cpus]
1669     return ",".join(cpus)
1670 def _normalize_cpusets(desc, data):
1671     """
1672     Expand the cpusets that can't be expanded by the change_xml() function,
1673     namely the ones that are used as keys and in the middle of the XPath expressions.
1674     """
1675     xpaths = ["cputune/cachetune", "cputune/cachetune/monitor", "cputune/memorytune"]
1676     for xpath in xpaths:
1677         nodes = desc.findall(xpath)
1678         for node in nodes:
1679             node.set("vcpus", _expand_cpuset(node.get("vcpus")))
1680     if not isinstance(data.get("cpu"), dict):
1681         return
1682     tuning = data["cpu"].get("tuning", {})
1683     for child in ["cachetune", "memorytune"]:
1684         if tuning.get(child):
1685             new_item = dict()
1686             for cpuset, value in tuning[child].items():
1687                 if child == "cachetune" and value.get("monitor"):
1688                     value["monitor"] = {
1689                         _expand_cpuset(monitor_cpus): monitor
1690                         for monitor_cpus, monitor in value["monitor"].items()
1691                     }
1692                 new_item[_expand_cpuset(cpuset)] = value
1693             tuning[child] = new_item
1694 def _serial_or_concole_equal(old, new):
1695     def _filter_serial_or_concole(item):
1696         """
1697         Filter out elements to ignore when comparing items
1698         """
1699         return {
1700             "type": item.attrib["type"],
1701             "port": item.find("source").get("service")
1702             if item.find("source") is not None
1703             else None,
1704             "protocol": item.find("protocol").get("type")
1705             if item.find("protocol") is not None
1706             else None,
1707         }
1708     return _filter_serial_or_concole(old) == _filter_serial_or_concole(new)
1709 def _diff_serial_lists(old, new):
1710     """
1711     Compare serial definitions to extract the changes
1712     :param old: list of ElementTree nodes representing the old serials
1713     :param new: list of ElementTree nodes representing the new serials
1714     """
1715     return _diff_lists(old, new, _serial_or_concole_equal)
1716 def _diff_console_lists(old, new):
1717     """
1718     Compare console definitions to extract the changes
1719     :param old: list of ElementTree nodes representing the old consoles
1720     :param new: list of ElementTree nodes representing the new consoles
1721     """
1722     return _diff_lists(old, new, _serial_or_concole_equal)
1723 def _format_pci_address(node):
1724     return "{}:{}:{}.{}".format(
1725         node.get("domain").replace("0x", ""),
1726         node.get("bus").replace("0x", ""),
1727         node.get("slot").replace("0x", ""),
1728         node.get("function").replace("0x", ""),
1729     )
1730 def _almost_equal(current, new):
1731     """
1732     return True if the parameters are numbers that are almost
1733     """
1734     if current is None or new is None:
1735         return False
1736     return abs(current - new) / current &lt; 1e-03
1737 def _compute_device_changes(old_xml, new_xml, to_skip):
1738     """
1739     Compute the device changes between two domain XML definitions.
1740     """
1741     devices_node = old_xml.find("devices")
1742     changes = {}
1743     for dev_type in to_skip:
1744         changes[dev_type] = {}
1745         if not to_skip[dev_type]:
1746             old = devices_node.findall(dev_type)
1747             new = new_xml.findall("devices/{}".format(dev_type))
1748             changes[dev_type] = globals()["_diff_{}_lists".format(dev_type)](old, new)
1749     return changes
1750 def _get_pci_addresses(node):
1751     """
1752     Get all the pci addresses in the node in 0000:00:00.0 form
1753     """
1754     return {_format_pci_address(address) for address in node.findall(".//address")}
1755 def _correct_networks(conn, desc):
1756     """
1757     Adjust the interface devices matching existing networks.
1758     Returns the network interfaces XML definition as string mapped to the new device node.
1759     """
1760     networks = [ElementTree.fromstring(net.XMLDesc()) for net in conn.listAllNetworks()]
1761     nics = desc.findall("devices/interface")
1762     device_map = {}
1763     for nic in nics:
1764         if nic.get("type") == "hostdev":
1765             addr = _get_pci_addresses(nic.find("source"))
1766             matching_nets = [
1767                 net
1768                 for net in networks
1769                 if net.find("forward").get("mode") == "hostdev"
1770                 and addr &amp; _get_pci_addresses(net)
1771             ]
1772             if matching_nets:
1773                 old_xml = ElementTree.tostring(nic)
1774                 nic.set("type", "network")
1775                 nic.find("source").set("network", matching_nets[0].find("name").text)
1776                 device_map[nic] = old_xml
1777     return device_map
1778 def _update_live(domain, new_desc, mem, cpu, old_mem, old_cpu, to_skip, test):
1779     """
1780     Perform the live update of a domain.
1781     """
1782     status = {}
1783     errors = []
1784     if not domain.isActive():
1785         return status, errors
1786     commands = []
1787     if cpu and (isinstance(cpu, int) or isinstance(cpu, dict) and cpu.get("maximum")):
1788         new_cpu = cpu.get("maximum") if isinstance(cpu, dict) else cpu
1789         if old_cpu != new_cpu and new_cpu is not None:
1790             commands.append(
1791                 {
1792                     "device": "cpu",
1793                     "cmd": "setVcpusFlags",
1794                     "args": [new_cpu, libvirt.VIR_DOMAIN_AFFECT_LIVE],
1795                 }
1796             )
1797     if mem:
1798         if isinstance(mem, dict):
1799             new_mem = (
1800                 int(_handle_unit(mem.get("current")) / 1024)
1801                 if "current" in mem
1802                 else None
1803             )
1804         elif isinstance(mem, int):
1805             new_mem = int(mem * 1024)
1806         if not _almost_equal(old_mem, new_mem) and new_mem is not None:
1807             commands.append(
1808                 {
1809                     "device": "mem",
1810                     "cmd": "setMemoryFlags",
1811                     "args": [new_mem, libvirt.VIR_DOMAIN_AFFECT_LIVE],
1812                 }
1813             )
1814     old_desc = ElementTree.fromstring(domain.XMLDesc(0))
1815     changed_devices = {"interface": _correct_networks(domain.connect(), old_desc)}
1816     changes = _compute_device_changes(old_desc, new_desc, to_skip)
1817     removable_changes = []
1818     new_disks = []
1819     for new_disk in changes["disk"].get("new", []):
1820         device = new_disk.get("device", "disk")
1821         if device not in ["cdrom", "floppy"]:
1822             new_disks.append(new_disk)
1823             continue
1824         target_dev = new_disk.find("target").get("dev")
1825         matching = [
1826             old_disk
1827             for old_disk in changes["disk"].get("deleted", [])
1828             if old_disk.get("device", "disk") == device
1829             and old_disk.find("target").get("dev") == target_dev
1830         ]
1831         if not matching:
1832             new_disks.append(new_disk)
1833         else:
1834             updated_disk = matching[0]
1835             changes["disk"]["deleted"].remove(updated_disk)
1836             removable_changes.append(updated_disk)
1837             source_node = updated_disk.find("source")
1838             new_source_node = new_disk.find("source")
1839             source_file = (
1840                 new_source_node.get("file") if new_source_node is not None else None
1841             )
1842             updated_disk.set("type", "file")
1843             if source_node is not None:
1844                 updated_disk.remove(source_node)
1845             if source_file:
1846                 ElementTree.SubElement(
1847                     updated_disk, "source", attrib={"file": source_file}
1848                 )
1849     changes["disk"]["new"] = new_disks
1850     for dev_type in ["disk", "interface", "hostdev"]:
1851         for added in changes[dev_type].get("new", []):
1852             commands.append(
1853                 {
1854                     "device": dev_type,
1855                     "cmd": "attachDevice",
1856                     "args": [xmlutil.element_to_str(added)],
1857                 }
1858             )
1859         for removed in changes[dev_type].get("deleted", []):
1860             removed_def = changed_devices.get(dev_type, {}).get(
1861                 removed, ElementTree.tostring(removed)
1862             )
1863             commands.append(
1864                 {
1865                     "device": dev_type,
1866                     "cmd": "detachDevice",
1867                     "args": [salt.utils.stringutils.to_str(removed_def)],
1868                 }
1869             )
1870     for updated_disk in removable_changes:
1871         commands.append(
1872             {
1873                 "device": "disk",
1874                 "cmd": "updateDeviceFlags",
1875                 "args": [xmlutil.element_to_str(updated_disk)],
1876             }
1877         )
1878     for cmd in commands:
1879         try:
1880             ret = 0 if test else getattr(domain, cmd["cmd"])(*cmd["args"])
1881             device_type = cmd["device"]
1882             if device_type in ["cpu", "mem"]:
1883                 status[device_type] = not ret
1884             else:
1885                 actions = {
1886                     "attachDevice": "attached",
1887                     "detachDevice": "detached",
1888                     "updateDeviceFlags": "updated",
1889                 }
1890                 device_status = status.setdefault(device_type, {})
1891                 cmd_status = device_status.setdefault(actions[cmd["cmd"]], [])
1892                 cmd_status.append(cmd["args"][0])
1893         except libvirt.libvirtError as err:
1894             errors.append(str(err))
1895     return status, errors
1896 def update(
1897     name,
1898     cpu=0,
1899     mem=0,
1900     disk_profile=None,
1901     disks=None,
1902     nic_profile=None,
1903     interfaces=None,
1904     graphics=None,
1905     live=True,
1906     boot=None,
1907     numatune=None,
1908     test=False,
1909     boot_dev=None,
1910     hypervisor_features=None,
1911     clock=None,
1912     serials=None,
1913     consoles=None,
1914     stop_on_reboot=False,
1915     host_devices=None,
1916     **kwargs
1917 ):
1918     """
1919     Update the definition of an existing domain.
1920     :param name: Name of the domain to update
1921     :param cpu:
1922         Number of virtual CPUs to assign to the virtual machine or a dictionary with detailed information to configure
1923         cpu model and topology, numa node tuning, cpu tuning and iothreads allocation. The structure of the dictionary is
1924         documented in :ref:`init-cpu-def`.
1925         To update any cpu parameters specify the new values to the corresponding tag. To remove any element or attribute,
1926         specify ``None`` object. Please note that ``None`` object is mapped to ``null`` in yaml, use ``null`` in sls file
1927         instead.
1928     :param mem: Amount of memory to allocate to the virtual machine in MiB. Since 3002, a dictionary can be used to
1929         contain detailed configuration which support memory allocation or tuning. Supported parameters are ``boot``,
1930         ``current``, ``max``, ``slots``, ``hard_limit``, ``soft_limit``, ``swap_hard_limit``, ``min_guarantee``,
1931         ``hugepages`` ,  ``nosharepages``, ``locked``, ``source``, ``access``, ``allocation`` and ``discard``. The structure
1932         of the dictionary is documented in  :ref:`init-mem-def`. Both decimal and binary base are supported. Detail unit
1933         specification is documented  in :ref:`virt-units`. Please note that the value for ``slots`` must be an integer.
1934         To remove any parameters, pass a None object, for instance: 'soft_limit': ``None``. Please note  that ``None``
1935         is mapped to ``null`` in sls file, pass ``null`` in sls file instead.
1936         .. code-block:: yaml
1937             - mem:
1938                 hard_limit: null
1939                 soft_limit: null
1940         .. versionchanged:: 3002
1941     :param disk_profile: disk profile to use
1942     :param disks:
1943         Disk definitions as documented in the :func:`init` function.
1944         If neither the profile nor this parameter are defined, the disk devices
1945         will not be changed. However to clear disks set this parameter to empty list.
1946     :param nic_profile: network interfaces profile to use
1947     :param interfaces:
1948         Network interface definitions as documented in the :func:`init` function.
1949         If neither the profile nor this parameter are defined, the interface devices
1950         will not be changed. However to clear network interfaces set this parameter
1951         to empty list.
1952     :param graphics:
1953         The new graphics definition as defined in :ref:`init-graphics-def`. If not set,
1954         the graphics will not be changed. To remove a graphics device, set this parameter
1955         to ``{'type': 'none'}``.
1956     :param live:
1957         ``False`` to avoid trying to live update the definition. In such a case, the
1958         new definition is applied at the next start of the virtual machine. If ``True``,
1959         not all aspects of the definition can be live updated, but as much as possible
1960         will be attempted. (Default: ``True``)
1961     :param connection: libvirt connection URI, overriding defaults
1962     :param username: username to connect with, overriding defaults
1963     :param password: password to connect with, overriding defaults
1964     :param boot:
1965         Specifies kernel, initial ramdisk and kernel command line parameters for the virtual machine.
1966         This is an optional parameter, all of the keys are optional within the dictionary.
1967         Refer to :ref:`init-boot-def` for the complete boot parameter description.
1968         To update any boot parameters, specify the new path for each. To remove any boot parameters, pass ``None`` object,
1969         for instance: 'kernel': ``None``. To switch back to BIOS boot, specify ('loader': ``None`` and 'nvram': ``None``)
1970         or 'efi': ``False``. Please note that ``None`` is mapped to ``null`` in sls file, pass ``null`` in sls file instead.
1971         SLS file Example:
1972         .. code-block:: yaml
1973             - boot:
1974                 loader: null
1975                 nvram: null
1976         .. versionadded:: 3000
1977     :param boot_dev:
1978         Space separated list of devices to boot from sorted by decreasing priority.
1979         Values can be ``hd``, ``fd``, ``cdrom`` or ``network``.
1980         By default, the value will ``"hd"``.
1981         .. versionadded:: 3002
1982     :param numatune:
1983         The optional numatune element provides details of how to tune the performance of a NUMA host via controlling NUMA
1984         policy for domain process. The optional ``memory`` element specifies how to allocate memory for the domain process
1985         on a NUMA host. ``memnode`` elements can specify memory allocation policies per each guest NUMA node. The definition
1986         used in the dictionary can be found at :ref:`init-cpu-def`.
1987         To update any numatune parameters, specify the new value. To remove any ``numatune`` parameters, pass a None object,
1988         for instance: 'numatune': ``None``. Please note that ``None`` is mapped to ``null`` in sls file, pass ``null`` in
1989         sls file instead.
1990         .. versionadded:: 3003
1991     :param serials:
1992         Dictionary providing details on the serials connection to create. (Default: ``None``)
1993         See :ref:`init-chardevs-def` for more details on the possible values.
1994         .. versionadded:: 3003
1995     :param consoles:
1996         Dictionary providing details on the consoles device to create. (Default: ``None``)
1997         See :ref:`init-chardevs-def` for more details on the possible values.
1998         .. versionadded:: 3003
1999     :param stop_on_reboot:
2000         If set to ``True`` the guest will stop instead of rebooting.
2001         This is specially useful when creating a virtual machine with an installation cdrom or
2002         an autoinstallation needing a special first boot configuration.
2003         Defaults to ``False``
2004         .. versionadded:: 3003
2005     :param test: run in dry-run mode if set to True
2006         .. versionadded:: 3001
2007     :param hypervisor_features:
2008         Enable or disable hypervisor-specific features on the virtual machine.
2009         .. versionadded:: 3003
2010         .. code-block:: yaml
2011             hypervisor_features:
2012               kvm-hint-dedicated: True
2013     :param clock:
2014         Configure the guest clock.
2015         The value is a dictionary with the following keys:
2016         adjustment
2017             time adjustment in seconds or ``reset``
2018         utc
2019             set to ``False`` to use the host local time as the guest clock. Defaults to ``True``.
2020         timezone
2021             synchronize the guest to the correspding timezone
2022         timers
2023             a dictionary associating the timer name with its configuration.
2024             This configuration is a dictionary with the properties ``track``, ``tickpolicy``,
2025             ``catchup``, ``frequency``, ``mode``, ``present``, ``slew``, ``threshold`` and ``limit``.
2026             See `libvirt time keeping documentation &lt;https://libvirt.org/formatdomain.html#time-keeping&gt;`_ for the possible values.
2027         .. versionadded:: 3003
2028         Set the clock to local time using an offset in seconds
2029         .. code-block:: yaml
2030             clock:
2031               adjustment: 3600
2032               utc: False
2033         Set the clock to a specific time zone:
2034         .. code-block:: yaml
2035             clock:
2036               timezone: CEST
2037         Tweak guest timers:
2038         .. code-block:: yaml
2039             clock:
2040               timers:
2041                 tsc:
2042                   frequency: 3504000000
2043                   mode: native
2044                 rtc:
2045                   track: wall
2046                   tickpolicy: catchup
2047                   slew: 4636
2048                   threshold: 123
2049                   limit: 2342
2050                 hpet:
2051                   present: False
2052     :param host_devices:
2053         List of host devices to passthrough to the guest.
2054         The value is a list of device names as provided by the :py:func:`~salt.modules.virt.node_devices` function.
2055         (Default: ``None``)
2056         .. versionadded:: 3003
2057     :return:
2058         Returns a dictionary indicating the status of what has been done. It is structured in
2059         the following way:
2060         .. code-block:: python
2061             {
2062               'definition': True,
2063               'cpu': True,
2064               'mem': True,
2065               'disks': {'attached': [list of actually attached disks],
2066                         'detached': [list of actually detached disks]},
2067               'nics': {'attached': [list of actually attached nics],
2068                        'detached': [list of actually detached nics]},
2069               'errors': ['error messages for failures']
2070             }
2071     .. versionadded:: 2019.2.0
2072     CLI Example:
2073     .. code-block:: bash
2074         salt '*' virt.update domain cpu=2 mem=1024
2075     """
2076     status = {
2077         "definition": False,
2078         "disk": {"attached": [], "detached": [], "updated": []},
2079         "interface": {"attached": [], "detached": []},
2080     }
2081     conn = __get_conn(**kwargs)
2082     domain = _get_domain(conn, name)
2083     desc = ElementTree.fromstring(domain.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE))
2084     need_update = False
2085     hypervisor = desc.get("type")
2086     all_disks = _disk_profile(conn, disk_profile, hypervisor, disks, name)
2087     if boot is not None:
2088         boot = _handle_remote_boot_params(boot)
2089         if boot.get("efi", None) is not None:
2090             need_update = _handle_efi_param(boot, desc)
2091     new_desc = ElementTree.fromstring(
2092         _gen_xml(
2093             conn,
2094             name,
2095             cpu,
2096             mem or 0,
2097             all_disks,
2098             _get_merged_nics(hypervisor, nic_profile, interfaces),
2099             hypervisor,
2100             domain.OSType(),
2101             desc.find(".//os/type").get("arch"),
2102             graphics,
2103             boot,
2104             boot_dev,
2105             numatune,
2106             serials=serials,
2107             consoles=consoles,
2108             stop_on_reboot=stop_on_reboot,
2109             host_devices=host_devices,
2110             **kwargs
2111         )
2112     )
2113     if clock:
2114         offset = "utc" if clock.get("utc", True) else "localtime"
2115         if "timezone" in clock:
2116             offset = "timezone"
2117         clock["offset"] = offset
2118     def _set_loader(node, value):
2119         salt.utils.xmlutil.set_node_text(node, value)
2120         if value is not None:
2121             node.set("readonly", "yes")
2122             node.set("type", "pflash")
2123     def _set_nvram(node, value):
2124         node.set("template", value)
2125     def _set_with_byte_unit(attr_name=None):
2126         def _setter(node, value):
2127             if attr_name:
2128                 node.set(attr_name, str(value))
2129             else:
2130                 node.text = str(value)
2131             node.set("unit", "bytes")
2132         return _setter
2133     def _get_with_unit(node):
2134         unit = node.get("unit", "KiB")
2135         unit = unit if unit != "bytes" else "b"
2136         value = node.get("memory") or node.get("size") or node.text
2137         return _handle_unit("{}{}".format(value, unit)) if value else None
2138     def _set_vcpu(node, value):
2139         node.text = str(value)
2140         node.set("current", str(value))
2141     old_mem = int(_get_with_unit(desc.find("memory")) / 1024)
2142     old_cpu = int(desc.find("./vcpu").text)
2143     def _yesno_attribute(path, xpath, attr_name, ignored=None):
2144         return xmlutil.attribute(
2145             path, xpath, attr_name, ignored, lambda v: "yes" if v else "no"
2146         )
2147     def _memory_parameter(path, xpath, attr_name=None, ignored=None):
2148         entry = {
2149             "path": path,
2150             "xpath": xpath,
2151             "convert": _handle_unit,
2152             "get": _get_with_unit,
2153             "set": _set_with_byte_unit(attr_name),
2154             "equals": _almost_equal,
2155         }
2156         if attr_name:
2157             entry["del"] = salt.utils.xmlutil.del_attribute(attr_name, ignored)
2158         return entry
2159     def _cpuset_parameter(path, xpath, attr_name=None, ignored=None):
2160         def _set_cpuset(node, value):
2161             if attr_name:
2162                 node.set(attr_name, value)
2163             else:
2164                 node.text = value
2165         entry = {
2166             "path": path,
2167             "xpath": xpath,
2168             "convert": _expand_cpuset,
2169             "get": lambda n: _expand_cpuset(n.get(attr_name) if attr_name else n.text),
2170             "set": _set_cpuset,
2171         }
2172         if attr_name:
2173             entry["del"] = salt.utils.xmlutil.del_attribute(attr_name, ignored)
2174         return entry
2175     data = {k: v for k, v in locals().items() if bool(v)}
2176     data["stop_on_reboot"] = stop_on_reboot
2177     if boot_dev:
2178         data["boot_dev"] = boot_dev.split()
2179     timer_names = [
2180         "platform",
2181         "hpet",
2182         "kvmclock",
2183         "pit",
2184         "rtc",
2185         "tsc",
2186         "hypervclock",
2187         "armvtimer",
2188     ]
2189     if data.get("clock", {}).get("timers"):
2190         attributes = [
2191             "track",
2192             "tickpolicy",
2193             "frequency",
2194             "mode",
2195             "present",
2196             "slew",
2197             "threshold",
2198             "limit",
2199         ]
2200         for timer in data["clock"]["timers"].values():
2201             for attribute in attributes:
2202                 if attribute not in timer:
2203                     timer[attribute] = None
2204         for timer_name in timer_names:
2205             if timer_name not in data["clock"]["timers"]:
2206                 data["clock"]["timers"][timer_name] = None
2207     _normalize_cpusets(desc, data)
2208     params_mapping = [
2209         {
2210             "path": "stop_on_reboot",
2211             "xpath": "on_reboot",
2212             "convert": lambda v: "destroy" if v else "restart",
2213         },
2214         {"path": "boot:kernel", "xpath": "os/kernel"},
2215         {"path": "boot:initrd", "xpath": "os/initrd"},
2216         {"path": "boot:cmdline", "xpath": "os/cmdline"},
2217         {"path": "boot:loader", "xpath": "os/loader", "set": _set_loader},
2218         {"path": "boot:nvram", "xpath": "os/nvram", "set": _set_nvram},
2219         _memory_parameter("mem", "memory"),
2220         _memory_parameter("mem", "currentMemory"),
2221         _memory_parameter("mem:max", "maxMemory"),
2222         _memory_parameter("mem:boot", "memory"),
2223         _memory_parameter("mem:current", "currentMemory"),
2224         xmlutil.attribute("mem:slots", "maxMemory", "slots", ["unit"]),
2225         _memory_parameter("mem:hard_limit", "memtune/hard_limit"),
2226         _memory_parameter("mem:soft_limit", "memtune/soft_limit"),
2227         _memory_parameter("mem:swap_hard_limit", "memtune/swap_hard_limit"),
2228         _memory_parameter("mem:min_guarantee", "memtune/min_guarantee"),
2229         xmlutil.attribute("boot_dev:{dev}", "os/boot[$dev]", "dev"),
2230         _memory_parameter(
2231             "mem:hugepages:{id}:size",
2232             "memoryBacking/hugepages/page[$id]",
2233             "size",
2234             ["unit", "nodeset"],
2235         ),
2236         _cpuset_parameter(
2237             "mem:hugepages:{id}:nodeset", "memoryBacking/hugepages/page[$id]", "nodeset"
2238         ),
2239         {
2240             "path": "mem:nosharepages",
2241             "xpath": "memoryBacking/nosharepages",
2242             "get": lambda n: n is not None,
2243             "set": lambda n, v: None,
2244         },
2245         {
2246             "path": "mem:locked",
2247             "xpath": "memoryBacking/locked",
2248             "get": lambda n: n is not None,
2249             "set": lambda n, v: None,
2250         },
2251         xmlutil.attribute("mem:source", "memoryBacking/source", "type"),
2252         xmlutil.attribute("mem:access", "memoryBacking/access", "mode"),
2253         xmlutil.attribute("mem:allocation", "memoryBacking/allocation", "mode"),
2254         {"path": "mem:discard", "xpath": "memoryBacking/discard"},
2255         {
2256             "path": "cpu",
2257             "xpath": "vcpu",
2258             "get": lambda n: int(n.text),
2259             "set": _set_vcpu,
2260         },
2261         {"path": "cpu:maximum", "xpath": "vcpu", "get": lambda n: int(n.text)},
2262         xmlutil.attribute("cpu:placement", "vcpu", "placement"),
2263         _cpuset_parameter("cpu:cpuset", "vcpu", "cpuset"),
2264         xmlutil.attribute("cpu:current", "vcpu", "current"),
2265         xmlutil.attribute("cpu:match", "cpu", "match"),
2266         xmlutil.attribute("cpu:mode", "cpu", "mode"),
2267         xmlutil.attribute("cpu:check", "cpu", "check"),
2268         {"path": "cpu:model:name", "xpath": "cpu/model"},
2269         xmlutil.attribute("cpu:model:fallback", "cpu/model", "fallback"),
2270         xmlutil.attribute("cpu:model:vendor_id", "cpu/model", "vendor_id"),
2271         {"path": "cpu:vendor", "xpath": "cpu/vendor"},
2272         xmlutil.attribute("cpu:topology:sockets", "cpu/topology", "sockets"),
2273         xmlutil.attribute("cpu:topology:cores", "cpu/topology", "cores"),
2274         xmlutil.attribute("cpu:topology:threads", "cpu/topology", "threads"),
2275         xmlutil.attribute("cpu:cache:level", "cpu/cache", "level"),
2276         xmlutil.attribute("cpu:cache:mode", "cpu/cache", "mode"),
2277         xmlutil.attribute(
2278             "cpu:features:{id}", "cpu/feature[@name='$id']", "policy", ["name"]
2279         ),
2280         _yesno_attribute(
2281             "cpu:vcpus:{id}:enabled", "vcpus/vcpu[@id='$id']", "enabled", ["id"]
2282         ),
2283         _yesno_attribute(
2284             "cpu:vcpus:{id}:hotpluggable",
2285             "vcpus/vcpu[@id='$id']",
2286             "hotpluggable",
2287             ["id"],
2288         ),
2289         xmlutil.int_attribute(
2290             "cpu:vcpus:{id}:order", "vcpus/vcpu[@id='$id']", "order", ["id"]
2291         ),
2292         _cpuset_parameter(
2293             "cpu:numa:{id}:cpus", "cpu/numa/cell[@id='$id']", "cpus", ["id"]
2294         ),
2295         _memory_parameter(
2296             "cpu:numa:{id}:memory", "cpu/numa/cell[@id='$id']", "memory", ["id"]
2297         ),
2298         _yesno_attribute(
2299             "cpu:numa:{id}:discard", "cpu/numa/cell[@id='$id']", "discard", ["id"]
2300         ),
2301         xmlutil.attribute(
2302             "cpu:numa:{id}:memAccess", "cpu/numa/cell[@id='$id']", "memAccess", ["id"]
2303         ),
2304         xmlutil.attribute(
2305             "cpu:numa:{id}:distances:{sid}",
2306             "cpu/numa/cell[@id='$id']/distances/sibling[@id='$sid']",
2307             "value",
2308             ["id"],
2309         ),
2310         {"path": "cpu:iothreads", "xpath": "iothreads"},
2311         {"path": "cpu:tuning:shares", "xpath": "cputune/shares"},
2312         {"path": "cpu:tuning:period", "xpath": "cputune/period"},
2313         {"path": "cpu:tuning:quota", "xpath": "cputune/quota"},
2314         {"path": "cpu:tuning:global_period", "xpath": "cputune/global_period"},
2315         {"path": "cpu:tuning:global_quota", "xpath": "cputune/global_quota"},
2316         {"path": "cpu:tuning:emulator_period", "xpath": "cputune/emulator_period"},
2317         {"path": "cpu:tuning:emulator_quota", "xpath": "cputune/emulator_quota"},
2318         {"path": "cpu:tuning:iothread_period", "xpath": "cputune/iothread_period"},
2319         {"path": "cpu:tuning:iothread_quota", "xpath": "cputune/iothread_quota"},
2320         _cpuset_parameter(
2321             "cpu:tuning:vcpupin:{id}",
2322             "cputune/vcpupin[@vcpu='$id']",
2323             "cpuset",
2324             ["vcpu"],
2325         ),
2326         _cpuset_parameter("cpu:tuning:emulatorpin", "cputune/emulatorpin", "cpuset"),
2327         _cpuset_parameter(
2328             "cpu:tuning:iothreadpin:{id}",
2329             "cputune/iothreadpin[@iothread='$id']",
2330             "cpuset",
2331             ["iothread"],
2332         ),
2333         xmlutil.attribute(
2334             "cpu:tuning:vcpusched:{id}:scheduler",
2335             "cputune/vcpusched[$id]",
2336             "scheduler",
2337             ["priority", "vcpus"],
2338         ),
2339         xmlutil.attribute(
2340             "cpu:tuning:vcpusched:{id}:priority", "cputune/vcpusched[$id]", "priority"
2341         ),
2342         _cpuset_parameter(
2343             "cpu:tuning:vcpusched:{id}:vcpus", "cputune/vcpusched[$id]", "vcpus"
2344         ),
2345         xmlutil.attribute(
2346             "cpu:tuning:iothreadsched:{id}:scheduler",
2347             "cputune/iothreadsched[$id]",
2348             "scheduler",
2349             ["priority", "iothreads"],
2350         ),
2351         xmlutil.attribute(
2352             "cpu:tuning:iothreadsched:{id}:priority",
2353             "cputune/iothreadsched[$id]",
2354             "priority",
2355         ),
2356         _cpuset_parameter(
2357             "cpu:tuning:iothreadsched:{id}:iothreads",
2358             "cputune/iothreadsched[$id]",
2359             "iothreads",
2360         ),
2361         xmlutil.attribute(
2362             "cpu:tuning:emulatorsched:scheduler",
2363             "cputune/emulatorsched",
2364             "scheduler",
2365             ["priority"],
2366         ),
2367         xmlutil.attribute(
2368             "cpu:tuning:emulatorsched:priority", "cputune/emulatorsched", "priority"
2369         ),
2370         xmlutil.attribute(
2371             "cpu:tuning:cachetune:{id}:monitor:{sid}",
2372             "cputune/cachetune[@vcpus='$id']/monitor[@vcpus='$sid']",
2373             "level",
2374             ["vcpus"],
2375         ),
2376         xmlutil.attribute(
2377             "cpu:tuning:memorytune:{id}:{sid}",
2378             "cputune/memorytune[@vcpus='$id']/node[@id='$sid']",
2379             "bandwidth",
2380             ["id", "vcpus"],
2381         ),
2382         xmlutil.attribute("clock:offset", "clock", "offset"),
2383         xmlutil.attribute("clock:adjustment", "clock", "adjustment", convert=str),
2384         xmlutil.attribute("clock:timezone", "clock", "timezone"),
2385     ]
2386     for timer in timer_names:
2387         params_mapping += [
2388             xmlutil.attribute(
2389                 "clock:timers:{}:track".format(timer),
2390                 "clock/timer[@name='{}']".format(timer),
2391                 "track",
2392                 ["name"],
2393             ),
2394             xmlutil.attribute(
2395                 "clock:timers:{}:tickpolicy".format(timer),
2396                 "clock/timer[@name='{}']".format(timer),
2397                 "tickpolicy",
2398                 ["name"],
2399             ),
2400             xmlutil.int_attribute(
2401                 "clock:timers:{}:frequency".format(timer),
2402                 "clock/timer[@name='{}']".format(timer),
2403                 "frequency",
2404                 ["name"],
2405             ),
2406             xmlutil.attribute(
2407                 "clock:timers:{}:mode".format(timer),
2408                 "clock/timer[@name='{}']".format(timer),
2409                 "mode",
2410                 ["name"],
2411             ),
2412             _yesno_attribute(
2413                 "clock:timers:{}:present".format(timer),
2414                 "clock/timer[@name='{}']".format(timer),
2415                 "present",
2416                 ["name"],
2417             ),
2418         ]
2419         for attr in ["slew", "threshold", "limit"]:
2420             params_mapping.append(
2421                 xmlutil.int_attribute(
2422                     "clock:timers:{}:{}".format(timer, attr),
2423                     "clock/timer[@name='{}']/catchup".format(timer),
2424                     attr,
2425                 )
2426             )
2427     for attr in ["level", "type", "size"]:
2428         params_mapping.append(
2429             xmlutil.attribute(
2430                 "cpu:tuning:cachetune:{id}:{sid}:" + attr,
2431                 "cputune/cachetune[@vcpus='$id']/cache[@id='$sid']",
2432                 attr,
2433                 ["id", "unit", "vcpus"],
2434             )
2435         )
2436     if hypervisor in ["qemu", "kvm"]:
2437         params_mapping += [
2438             xmlutil.attribute("numatune:memory:mode", "numatune/memory", "mode"),
2439             _cpuset_parameter("numatune:memory:nodeset", "numatune/memory", "nodeset"),
2440             xmlutil.attribute(
2441                 "numatune:memnodes:{id}:mode",
2442                 "numatune/memnode[@cellid='$id']",
2443                 "mode",
2444                 ["cellid"],
2445             ),
2446             _cpuset_parameter(
2447                 "numatune:memnodes:{id}:nodeset",
2448                 "numatune/memnode[@cellid='$id']",
2449                 "nodeset",
2450                 ["cellid"],
2451             ),
2452             xmlutil.attribute(
2453                 "hypervisor_features:kvm-hint-dedicated",
2454                 "features/kvm/hint-dedicated",
2455                 "state",
2456                 convert=lambda v: "on" if v else "off",
2457             ),
2458         ]
2459     need_update = (
2460         salt.utils.xmlutil.change_xml(desc, data, params_mapping) or need_update
2461     )
2462     devices_node = desc.find("devices")
2463     func_locals = locals()
2464     def _skip_update(names):
2465         return all(func_locals.get(n) is None for n in names)
2466     to_skip = {
2467         "disk": _skip_update(["disks", "disk_profile"]),
2468         "interface": _skip_update(["interfaces", "nic_profile"]),
2469         "graphics": _skip_update(["graphics"]),
2470         "serial": _skip_update(["serials"]),
2471         "console": _skip_update(["consoles"]),
2472         "hostdev": _skip_update(["host_devices"]),
2473     }
2474     changes = _compute_device_changes(desc, new_desc, to_skip)
2475     for dev_type in changes:
2476         if not to_skip[dev_type]:
2477             old = devices_node.findall(dev_type)
2478             if changes[dev_type].get("deleted") or changes[dev_type].get("new"):
2479                 for item in old:
2480                     devices_node.remove(item)
2481                 devices_node.extend(changes[dev_type]["sorted"])
2482                 need_update = True
2483     if need_update:
2484         try:
2485             if changes["disk"]:
2486                 for idx, item in enumerate(changes["disk"]["sorted"]):
2487                     source_file = all_disks[idx].get("source_file")
2488                     if all_disks[idx].get("device", "disk") == "cdrom":
2489                         continue
2490                     if (
2491                         item in changes["disk"]["new"]
2492                         and source_file
2493                         and not os.path.exists(source_file)
2494                     ):
2495                         _qemu_image_create(all_disks[idx])
2496                     elif item in changes["disk"]["new"] and not source_file:
2497                         _disk_volume_create(conn, all_disks[idx])
2498             if not test:
2499                 xml_desc = xmlutil.element_to_str(desc)
2500                 log.debug("Update virtual machine definition: %s", xml_desc)
2501                 conn.defineXML(xml_desc)
2502             status["definition"] = True
2503         except libvirt.libvirtError as err:
2504             conn.close()
2505             raise err
2506     if live:
2507         live_status, errors = _update_live(
2508             domain, new_desc, mem, cpu, old_mem, old_cpu, to_skip, test
2509         )
2510         status.update(live_status)
2511         if errors:
2512             status_errors = status.setdefault("errors", [])
2513             status_errors += errors
2514     conn.close()
2515     return status
2516 def list_domains(**kwargs):
2517     """
2518     Return a list of available domains.
2519     :param connection: libvirt connection URI, overriding defaults
2520         .. versionadded:: 2019.2.0
2521     :param username: username to connect with, overriding defaults
2522         .. versionadded:: 2019.2.0
2523     :param password: password to connect with, overriding defaults
2524         .. versionadded:: 2019.2.0
2525     CLI Example:
2526     .. code-block:: bash
2527         salt '*' virt.list_domains
2528     """
2529     vms = []
2530     conn = __get_conn(**kwargs)
2531     for dom in _get_domain(conn, iterable=True):
2532         vms.append(dom.name())
2533     conn.close()
2534     return vms
2535 def list_active_vms(**kwargs):
2536     """
2537     Return a list of names for active virtual machine on the minion
2538     :param connection: libvirt connection URI, overriding defaults
2539         .. versionadded:: 2019.2.0
2540     :param username: username to connect with, overriding defaults
2541         .. versionadded:: 2019.2.0
2542     :param password: password to connect with, overriding defaults
2543         .. versionadded:: 2019.2.0
2544     CLI Example:
2545     .. code-block:: bash
2546         salt '*' virt.list_active_vms
2547     """
2548     vms = []
2549     conn = __get_conn(**kwargs)
2550     for dom in _get_domain(conn, iterable=True, inactive=False):
2551         vms.append(dom.name())
2552     conn.close()
2553     return vms
2554 def list_inactive_vms(**kwargs):
2555     """
2556     Return a list of names for inactive virtual machine on the minion
2557     :param connection: libvirt connection URI, overriding defaults
2558         .. versionadded:: 2019.2.0
2559     :param username: username to connect with, overriding defaults
2560         .. versionadded:: 2019.2.0
2561     :param password: password to connect with, overriding defaults
2562         .. versionadded:: 2019.2.0
2563     CLI Example:
2564     .. code-block:: bash
2565         salt '*' virt.list_inactive_vms
2566     """
2567     vms = []
2568     conn = __get_conn(**kwargs)
2569     for dom in _get_domain(conn, iterable=True, active=False):
2570         vms.append(dom.name())
2571     conn.close()
2572     return vms
2573 def vm_info(vm_=None, **kwargs):
2574     """
2575     Return detailed information about the vms on this hyper in a
2576     list of dicts:
2577     :param vm_: name of the domain
2578     :param connection: libvirt connection URI, overriding defaults
2579         .. versionadded:: 2019.2.0
2580     :param username: username to connect with, overriding defaults
2581         .. versionadded:: 2019.2.0
2582     :param password: password to connect with, overriding defaults
2583         .. versionadded:: 2019.2.0
2584     .. code-block:: python
2585         [
2586             'your-vm': {
2587                 'cpu': &lt;int&gt;,
2588                 'maxMem': &lt;int&gt;,
2589                 'mem': &lt;int&gt;,
2590                 'state': '&lt;state&gt;',
2591                 'cputime' &lt;int&gt;
2592                 },
2593             ...
2594             ]
2595     If you pass a VM name in as an argument then it will return info
2596     for just the named VM, otherwise it will return all VMs.
2597     CLI Example:
2598     .. code-block:: bash
2599         salt '*' virt.vm_info
2600     """
2601     def _info(conn, dom):
2602         """
2603         Compute the infos of a domain
2604         """
2605         raw = dom.info()
2606         return {
2607             "cpu": raw[3],
2608             "cputime": int(raw[4]),
2609             "disks": _get_disks(conn, dom),
2610             "graphics": _get_graphics(dom),
2611             "nics": _get_nics(dom),
2612             "uuid": _get_uuid(dom),
2613             "loader": _get_loader(dom),
2614             "on_crash": _get_on_crash(dom),
2615             "on_reboot": _get_on_reboot(dom),
2616             "on_poweroff": _get_on_poweroff(dom),
2617             "maxMem": int(raw[1]),
2618             "mem": int(raw[2]),
2619             "state": VIRT_STATE_NAME_MAP.get(raw[0], "unknown"),
2620         }
2621     info = {}
2622     conn = __get_conn(**kwargs)
2623     if vm_:
2624         info[vm_] = _info(conn, _get_domain(conn, vm_))
2625     else:
2626         for domain in _get_domain(conn, iterable=True):
2627             info[domain.name()] = _info(conn, domain)
2628     conn.close()
2629     return info
2630 def vm_state(vm_=None, **kwargs):
2631     """
2632     Return list of all the vms and their state.
2633     If you pass a VM name in as an argument then it will return info
2634     for just the named VM, otherwise it will return all VMs.
2635     :param vm_: name of the domain
2636     :param connection: libvirt connection URI, overriding defaults
2637         .. versionadded:: 2019.2.0
2638     :param username: username to connect with, overriding defaults
2639         .. versionadded:: 2019.2.0
2640     :param password: password to connect with, overriding defaults
2641         .. versionadded:: 2019.2.0
2642     CLI Example:
2643     .. code-block:: bash
2644         salt '*' virt.vm_state &lt;domain&gt;
2645     """
2646     def _info(dom):
2647         """
2648         Compute domain state
2649         """
2650         state = ""
2651         raw = dom.info()
2652         state = VIRT_STATE_NAME_MAP.get(raw[0], "unknown")
2653         return state
2654     info = {}
2655     conn = __get_conn(**kwargs)
2656     if vm_:
2657         info[vm_] = _info(_get_domain(conn, vm_))
2658     else:
2659         for domain in _get_domain(conn, iterable=True):
2660             info[domain.name()] = _info(domain)
2661     conn.close()
2662     return info
2663 def _node_info(conn):
2664     """
2665     Internal variant of node_info taking a libvirt connection as parameter
2666     """
2667     raw = conn.getInfo()
2668     info = {
2669         "cpucores": raw[6],
2670         "cpumhz": raw[3],
2671         "cpumodel": str(raw[0]),
2672         "cpus": raw[2],
2673         "cputhreads": raw[7],
2674         "numanodes": raw[4],
2675         "phymemory": raw[1],
2676         "sockets": raw[5],
2677     }
2678     return info
2679 def node_info(**kwargs):
2680     """
2681     Return a dict with information about this node
2682     :param connection: libvirt connection URI, overriding defaults
2683         .. versionadded:: 2019.2.0
2684     :param username: username to connect with, overriding defaults
2685         .. versionadded:: 2019.2.0
2686     :param password: password to connect with, overriding defaults
2687         .. versionadded:: 2019.2.0
2688     CLI Example:
2689     .. code-block:: bash
2690         salt '*' virt.node_info
2691     """
2692     conn = __get_conn(**kwargs)
2693     info = _node_info(conn)
2694     conn.close()
2695     return info
2696 def _node_devices(conn):
2697     """
2698     List the host available devices, using an established connection.
2699     :param conn: the libvirt connection handle to use.
2700     .. versionadded:: 3003
2701     """
2702     devices = conn.listAllDevices()
2703     devices_infos = []
2704     for dev in devices:
2705         root = ElementTree.fromstring(dev.XMLDesc())
2706         if not set(dev.listCaps()) &amp; {"pci", "usb_device", "net"}:
2707             continue
2708         infos = {
2709             "caps": " ".join(dev.listCaps()),
2710         }
2711         if "net" in dev.listCaps():
2712             parent = root.find(".//parent").text
2713             if parent == "computer":
2714                 continue
2715             infos.update(
2716                 {
2717                     "name": root.find(".//interface").text,
2718                     "address": root.find(".//address").text,
2719                     "device name": parent,
2720                     "state": root.find(".//link").get("state"),
2721                 }
2722             )
2723             devices_infos.append(infos)
2724             continue
2725         vendor_node = root.find(".//vendor")
2726         vendor_id = vendor_node.get("id").lower()
2727         product_node = root.find(".//product")
2728         product_id = product_node.get("id").lower()
2729         infos.update(
2730             {"name": dev.name(), "vendor_id": vendor_id, "product_id": product_id}
2731         )
2732         if vendor_node.text:
2733             infos["vendor"] = vendor_node.text
2734         if product_node.text:
2735             infos["product"] = product_node.text
2736         if "pci" in dev.listCaps():
2737             infos["address"] = "{:04x}:{:02x}:{:02x}.{}".format(
2738                 int(root.find(".//domain").text),
2739                 int(root.find(".//bus").text),
2740                 int(root.find(".//slot").text),
2741                 root.find(".//function").text,
2742             )
2743             class_node = root.find(".//class")
2744             if class_node is not None:
2745                 infos["PCI class"] = class_node.text
2746             vf_addresses = [
2747                 _format_pci_address(vf)
2748                 for vf in root.findall(
2749                     "./capability[@type='pci']/capability[@type='virt_functions']/address"
2750                 )
2751             ]
2752             if vf_addresses:
2753                 infos["virtual functions"] = vf_addresses
2754             pf = root.find(
2755                 "./capability[@type='pci']/capability[@type='phys_function']/address"
2756             )
2757             if pf is not None:
2758                 infos["physical function"] = _format_pci_address(pf)
2759         elif "usb_device" in dev.listCaps():
2760             infos["address"] = "{:03}:{:03}".format(
2761                 int(root.find(".//bus").text), int(root.find(".//device").text)
2762             )
2763         linux_usb_host = vendor_id == "0x1d6b" and product_id in [
2764             "0x0001",
2765             "0x0002",
2766             "0x0003",
2767         ]
2768         if (
2769             root.find(".//capability[@type='pci-bridge']") is None
2770             and not linux_usb_host
2771         ):
2772             devices_infos.append(infos)
2773     return devices_infos
2774 def node_devices(**kwargs):
2775     """
2776     List the host available devices.
2777     :param connection: libvirt connection URI, overriding defaults
2778     :param username: username to connect with, overriding defaults
2779     :param password: password to connect with, overriding defaults
2780     .. versionadded:: 3003
2781     """
2782     conn = __get_conn(**kwargs)
2783     devs = _node_devices(conn)
2784     conn.close()
2785     return devs
2786 def get_nics(vm_, **kwargs):
2787     """
2788     Return info about the network interfaces of a named vm
2789     :param vm_: name of the domain
2790     :param connection: libvirt connection URI, overriding defaults
2791         .. versionadded:: 2019.2.0
2792     :param username: username to connect with, overriding defaults
2793         .. versionadded:: 2019.2.0
2794     :param password: password to connect with, overriding defaults
2795         .. versionadded:: 2019.2.0
2796     CLI Example:
2797     .. code-block:: bash
2798         salt '*' virt.get_nics &lt;domain&gt;
2799     """
2800     conn = __get_conn(**kwargs)
2801     nics = _get_nics(_get_domain(conn, vm_))
2802     conn.close()
2803     return nics
2804 def get_macs(vm_, **kwargs):
2805     """
2806     Return a list off MAC addresses from the named vm
2807     :param vm_: name of the domain
2808     :param connection: libvirt connection URI, overriding defaults
2809         .. versionadded:: 2019.2.0
2810     :param username: username to connect with, overriding defaults
2811         .. versionadded:: 2019.2.0
2812     :param password: password to connect with, overriding defaults
2813         .. versionadded:: 2019.2.0
2814     CLI Example:
2815     .. code-block:: bash
2816         salt '*' virt.get_macs &lt;domain&gt;
2817     """
2818     doc = ElementTree.fromstring(get_xml(vm_, **kwargs))
2819     return [node.get("address") for node in doc.findall("devices/interface/mac")]
2820 def get_graphics(vm_, **kwargs):
2821     """
2822     Returns the information on vnc for a given vm
2823     :param vm_: name of the domain
2824     :param connection: libvirt connection URI, overriding defaults
2825         .. versionadded:: 2019.2.0
2826     :param username: username to connect with, overriding defaults
2827         .. versionadded:: 2019.2.0
2828     :param password: password to connect with, overriding defaults
2829         .. versionadded:: 2019.2.0
2830     CLI Example:
2831     .. code-block:: bash
2832         salt '*' virt.get_graphics &lt;domain&gt;
2833     """
2834     conn = __get_conn(**kwargs)
2835     graphics = _get_graphics(_get_domain(conn, vm_))
2836     conn.close()
2837     return graphics
2838 def get_loader(vm_, **kwargs):
2839     """
2840     Returns the information on the loader for a given vm
2841     :param vm_: name of the domain
2842     :param connection: libvirt connection URI, overriding defaults
2843     :param username: username to connect with, overriding defaults
2844     :param password: password to connect with, overriding defaults
2845     CLI Example:
2846     .. code-block:: bash
2847         salt '*' virt.get_loader &lt;domain&gt;
2848     .. versionadded:: 2019.2.0
2849     """
2850     conn = __get_conn(**kwargs)
2851     try:
2852         loader = _get_loader(_get_domain(conn, vm_))
2853         return loader
2854     finally:
2855         conn.close()
2856 def get_disks(vm_, **kwargs):
2857     """
2858     Return the disks of a named vm
2859     :param vm_: name of the domain
2860     :param connection: libvirt connection URI, overriding defaults
2861         .. versionadded:: 2019.2.0
2862     :param username: username to connect with, overriding defaults
2863         .. versionadded:: 2019.2.0
2864     :param password: password to connect with, overriding defaults
2865         .. versionadded:: 2019.2.0
2866     CLI Example:
2867     .. code-block:: bash
2868         salt '*' virt.get_disks &lt;domain&gt;
2869     """
2870     conn = __get_conn(**kwargs)
2871     disks = _get_disks(conn, _get_domain(conn, vm_))
2872     conn.close()
2873     return disks
2874 def setmem(vm_, memory, config=False, **kwargs):
2875     """
2876     Changes the amount of memory allocated to VM. The VM must be shutdown
2877     for this to work.
2878     :param vm_: name of the domain
2879     :param memory: memory amount to set in MB
2880     :param config: if True then libvirt will be asked to modify the config as well
2881     :param connection: libvirt connection URI, overriding defaults
2882         .. versionadded:: 2019.2.0
2883     :param username: username to connect with, overriding defaults
2884         .. versionadded:: 2019.2.0
2885     :param password: password to connect with, overriding defaults
2886         .. versionadded:: 2019.2.0
2887     CLI Example:
2888     .. code-block:: bash
2889         salt '*' virt.setmem &lt;domain&gt; &lt;size&gt;
2890         salt '*' virt.setmem my_domain 768
2891     """
2892     conn = __get_conn(**kwargs)
2893     dom = _get_domain(conn, vm_)
2894     if VIRT_STATE_NAME_MAP.get(dom.info()[0], "unknown") != "shutdown":
2895         return False
2896     flags = libvirt.VIR_DOMAIN_MEM_MAXIMUM
2897     if config:
2898         flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
2899     ret1 = dom.setMemoryFlags(memory * 1024, flags)
2900     ret2 = dom.setMemoryFlags(memory * 1024, libvirt.VIR_DOMAIN_AFFECT_CURRENT)
2901     conn.close()
2902     return ret1 == ret2 == 0
2903 def setvcpus(vm_, vcpus, config=False, **kwargs):
2904     """
2905     Changes the amount of vcpus allocated to VM. The VM must be shutdown
2906     for this to work.
2907     If config is True then we ask libvirt to modify the config as well
2908     :param vm_: name of the domain
2909     :param vcpus: integer representing the number of CPUs to be assigned
2910     :param config: if True then libvirt will be asked to modify the config as well
2911     :param connection: libvirt connection URI, overriding defaults
2912         .. versionadded:: 2019.2.0
2913     :param username: username to connect with, overriding defaults
2914         .. versionadded:: 2019.2.0
2915     :param password: password to connect with, overriding defaults
2916         .. versionadded:: 2019.2.0
2917     CLI Example:
2918     .. code-block:: bash
2919         salt '*' virt.setvcpus &lt;domain&gt; &lt;amount&gt;
2920         salt '*' virt.setvcpus my_domain 4
2921     """
2922     conn = __get_conn(**kwargs)
2923     dom = _get_domain(conn, vm_)
2924     if VIRT_STATE_NAME_MAP.get(dom.info()[0], "unknown") != "shutdown":
2925         return False
2926     flags = libvirt.VIR_DOMAIN_VCPU_MAXIMUM
2927     if config:
2928         flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
2929     ret1 = dom.setVcpusFlags(vcpus, flags)
2930     ret2 = dom.setVcpusFlags(vcpus, libvirt.VIR_DOMAIN_AFFECT_CURRENT)
2931     conn.close()
2932     return ret1 == ret2 == 0
2933 def _freemem(conn):
2934     """
2935     Internal variant of freemem taking a libvirt connection as parameter
2936     """
2937     mem = conn.getInfo()[1]
2938     mem -= 256
2939     for dom in _get_domain(conn, iterable=True):
2940         if dom.ID() &gt; 0:
2941             mem -= dom.info()[2] / 1024
2942     return mem
2943 def freemem(**kwargs):
2944     """
2945     Return an int representing the amount of memory (in MB) that has not
2946     been given to virtual machines on this node
2947     :param connection: libvirt connection URI, overriding defaults
2948         .. versionadded:: 2019.2.0
2949     :param username: username to connect with, overriding defaults
2950         .. versionadded:: 2019.2.0
2951     :param password: password to connect with, overriding defaults
2952         .. versionadded:: 2019.2.0
2953     CLI Example:
2954     .. code-block:: bash
2955         salt '*' virt.freemem
2956     """
2957     conn = __get_conn(**kwargs)
2958     mem = _freemem(conn)
2959     conn.close()
2960     return mem
2961 def _freecpu(conn):
2962     """
2963     Internal variant of freecpu taking a libvirt connection as parameter
2964     """
2965     cpus = conn.getInfo()[2]
2966     for dom in _get_domain(conn, iterable=True):
2967         if dom.ID() &gt; 0:
2968             cpus -= dom.info()[3]
2969     return cpus
2970 def freecpu(**kwargs):
2971     """
2972     Return an int representing the number of unallocated cpus on this
2973     hypervisor
2974     :param connection: libvirt connection URI, overriding defaults
2975         .. versionadded:: 2019.2.0
2976     :param username: username to connect with, overriding defaults
2977         .. versionadded:: 2019.2.0
2978     :param password: password to connect with, overriding defaults
2979         .. versionadded:: 2019.2.0
2980     CLI Example:
2981     .. code-block:: bash
2982         salt '*' virt.freecpu
2983     """
2984     conn = __get_conn(**kwargs)
2985     cpus = _freecpu(conn)
2986     conn.close()
2987     return cpus
2988 def full_info(**kwargs):
2989     """
2990     Return the node_info, vm_info and freemem
2991     :param connection: libvirt connection URI, overriding defaults
2992         .. versionadded:: 2019.2.0
2993     :param username: username to connect with, overriding defaults
2994         .. versionadded:: 2019.2.0
2995     :param password: password to connect with, overriding defaults
2996         .. versionadded:: 2019.2.0
2997     CLI Example:
2998     .. code-block:: bash
2999         salt '*' virt.full_info
3000     """
3001     conn = __get_conn(**kwargs)
3002     info = {
3003         "freecpu": _freecpu(conn),
3004         "freemem": _freemem(conn),
3005         "node_info": _node_info(conn),
3006         "vm_info": vm_info(),
3007     }
3008     conn.close()
3009     return info
3010 def get_xml(vm_, **kwargs):
3011     """
3012     Returns the XML for a given vm
3013     :param vm_: domain name
3014     :param connection: libvirt connection URI, overriding defaults
3015         .. versionadded:: 2019.2.0
3016     :param username: username to connect with, overriding defaults
3017         .. versionadded:: 2019.2.0
3018     :param password: password to connect with, overriding defaults
3019         .. versionadded:: 2019.2.0
3020     CLI Example:
3021     .. code-block:: bash
3022         salt '*' virt.get_xml &lt;domain&gt;
3023     """
3024     conn = __get_conn(**kwargs)
3025     xml_desc = (
3026         vm_.XMLDesc(0)
3027         if isinstance(vm_, libvirt.virDomain)
3028         else _get_domain(conn, vm_).XMLDesc(0)
3029     )
3030     conn.close()
3031     return xml_desc
3032 def get_profiles(hypervisor=None, **kwargs):
3033     """
3034     Return the virt profiles for hypervisor.
3035     Currently there are profiles for:
3036     - nic
3037     - disk
3038     :param hypervisor: override the default machine type.
3039     :param connection: libvirt connection URI, overriding defaults
3040         .. versionadded:: 2019.2.0
3041     :param username: username to connect with, overriding defaults
3042         .. versionadded:: 2019.2.0
3043     :param password: password to connect with, overriding defaults
3044         .. versionadded:: 2019.2.0
3045     CLI Example:
3046     .. code-block:: bash
3047         salt '*' virt.get_profiles
3048         salt '*' virt.get_profiles hypervisor=vmware
3049     """
3050     conn = __get_conn(**kwargs)
3051     caps = _capabilities(conn)
3052     hypervisors = sorted(
3053         {
3054             x
3055             for y in [guest["arch"]["domains"].keys() for guest in caps["guests"]]
3056             for x in y
3057         }
3058     )
3059     if len(hypervisors) == 0:
3060         raise SaltInvocationError("No supported hypervisors were found")
3061     if not hypervisor:
3062         hypervisor = "kvm" if "kvm" in hypervisors else hypervisors[0]
3063     ret = {
3064         "disk": {"default": _disk_profile(conn, "default", hypervisor, [], None)},
3065         "nic": {"default": _nic_profile("default", hypervisor)},
3066     }
3067     virtconf = __salt__["config.get"]("virt", {})
3068     for profile in virtconf.get("disk", []):
3069         ret["disk"][profile] = _disk_profile(conn, profile, hypervisor, [], None)
3070     for profile in virtconf.get("nic", []):
3071         ret["nic"][profile] = _nic_profile(profile, hypervisor)
3072     return ret
3073 def shutdown(vm_, **kwargs):
3074     """
3075     Send a soft shutdown signal to the named vm
3076     :param vm_: domain name
3077     :param connection: libvirt connection URI, overriding defaults
3078         .. versionadded:: 2019.2.0
3079     :param username: username to connect with, overriding defaults
3080         .. versionadded:: 2019.2.0
3081     :param password: password to connect with, overriding defaults
3082         .. versionadded:: 2019.2.0
3083     CLI Example:
3084     .. code-block:: bash
3085         salt '*' virt.shutdown &lt;domain&gt;
3086     """
3087     conn = __get_conn(**kwargs)
3088     dom = _get_domain(conn, vm_)
3089     ret = dom.shutdown() == 0
3090     conn.close()
3091     return ret
3092 def pause(vm_, **kwargs):
3093     """
3094     Pause the named vm
3095     :param vm_: domain name
3096     :param connection: libvirt connection URI, overriding defaults
3097         .. versionadded:: 2019.2.0
3098     :param username: username to connect with, overriding defaults
3099         .. versionadded:: 2019.2.0
3100     :param password: password to connect with, overriding defaults
3101         .. versionadded:: 2019.2.0
3102     CLI Example:
3103     .. code-block:: bash
3104         salt '*' virt.pause &lt;domain&gt;
3105     """
3106     conn = __get_conn(**kwargs)
3107     dom = _get_domain(conn, vm_)
3108     ret = dom.suspend() == 0
3109     conn.close()
3110     return ret
3111 def resume(vm_, **kwargs):
3112     """
3113     Resume the named vm
3114     :param vm_: domain name
3115     :param connection: libvirt connection URI, overriding defaults
3116         .. versionadded:: 2019.2.0
3117     :param username: username to connect with, overriding defaults
3118         .. versionadded:: 2019.2.0
3119     :param password: password to connect with, overriding defaults
3120         .. versionadded:: 2019.2.0
3121     CLI Example:
3122     .. code-block:: bash
3123         salt '*' virt.resume &lt;domain&gt;
3124     """
3125     conn = __get_conn(**kwargs)
3126     dom = _get_domain(conn, vm_)
3127     ret = dom.resume() == 0
3128     conn.close()
3129     return ret
3130 def start(name, **kwargs):
3131     """
3132     Start a defined domain
3133     :param vm_: domain name
3134     :param connection: libvirt connection URI, overriding defaults
3135         .. versionadded:: 2019.2.0
3136     :param username: username to connect with, overriding defaults
3137         .. versionadded:: 2019.2.0
3138     :param password: password to connect with, overriding defaults
3139         .. versionadded:: 2019.2.0
3140     CLI Example:
3141     .. code-block:: bash
3142         salt '*' virt.start &lt;domain&gt;
3143     """
3144     conn = __get_conn(**kwargs)
3145     ret = _get_domain(conn, name).create() == 0
3146     conn.close()
3147     return ret
3148 def stop(name, **kwargs):
3149     """
3150     Hard power down the virtual machine, this is equivalent to pulling the power.
3151     :param vm_: domain name
3152     :param connection: libvirt connection URI, overriding defaults
3153         .. versionadded:: 2019.2.0
3154     :param username: username to connect with, overriding defaults
3155         .. versionadded:: 2019.2.0
3156     :param password: password to connect with, overriding defaults
3157         .. versionadded:: 2019.2.0
3158     CLI Example:
3159     .. code-block:: bash
3160         salt '*' virt.stop &lt;domain&gt;
3161     """
3162     conn = __get_conn(**kwargs)
3163     ret = _get_domain(conn, name).destroy() == 0
3164     conn.close()
3165     return ret
3166 def reboot(name, **kwargs):
3167     """
3168     Reboot a domain via ACPI request
3169     :param vm_: domain name
3170     :param connection: libvirt connection URI, overriding defaults
3171         .. versionadded:: 2019.2.0
3172     :param username: username to connect with, overriding defaults
3173         .. versionadded:: 2019.2.0
3174     :param password: password to connect with, overriding defaults
3175         .. versionadded:: 2019.2.0
3176     CLI Example:
3177     .. code-block:: bash
3178         salt '*' virt.reboot &lt;domain&gt;
3179     """
3180     conn = __get_conn(**kwargs)
3181     ret = _get_domain(conn, name).reboot(libvirt.VIR_DOMAIN_REBOOT_DEFAULT) == 0
3182     conn.close()
3183     return ret
3184 def reset(vm_, **kwargs):
3185     """
3186     Reset a VM by emulating the reset button on a physical machine
3187     :param vm_: domain name
3188     :param connection: libvirt connection URI, overriding defaults
3189         .. versionadded:: 2019.2.0
3190     :param username: username to connect with, overriding defaults
3191         .. versionadded:: 2019.2.0
3192     :param password: password to connect with, overriding defaults
3193         .. versionadded:: 2019.2.0
3194     CLI Example:
3195     .. code-block:: bash
3196         salt '*' virt.reset &lt;domain&gt;
3197     """
3198     conn = __get_conn(**kwargs)
3199     dom = _get_domain(conn, vm_)
3200     ret = dom.reset(0) == 0
3201     conn.close()
3202     return ret
3203 def ctrl_alt_del(vm_, **kwargs):
3204     """
3205     Sends CTRL+ALT+DEL to a VM
3206     :param vm_: domain name
3207     :param connection: libvirt connection URI, overriding defaults
3208         .. versionadded:: 2019.2.0
3209     :param username: username to connect with, overriding defaults
3210         .. versionadded:: 2019.2.0
3211     :param password: password to connect with, overriding defaults
3212         .. versionadded:: 2019.2.0
3213     CLI Example:
3214     .. code-block:: bash
3215         salt '*' virt.ctrl_alt_del &lt;domain&gt;
3216     """
3217     conn = __get_conn(**kwargs)
3218     dom = _get_domain(conn, vm_)
3219     ret = dom.sendKey(0, 0, [29, 56, 111], 3, 0) == 0
3220     conn.close()
3221     return ret
3222 def create_xml_str(xml, **kwargs):  # pylint: disable=redefined-outer-name
3223     """
3224     Start a transient domain based on the XML passed to the function
3225     :param xml: libvirt XML definition of the domain
3226     :param connection: libvirt connection URI, overriding defaults
3227         .. versionadded:: 2019.2.0
3228     :param username: username to connect with, overriding defaults
3229         .. versionadded:: 2019.2.0
3230     :param password: password to connect with, overriding defaults
3231         .. versionadded:: 2019.2.0
3232     CLI Example:
3233     .. code-block:: bash
3234         salt '*' virt.create_xml_str &lt;XML in string format&gt;
3235     """
3236     conn = __get_conn(**kwargs)
3237     ret = conn.createXML(xml, 0) is not None
3238     conn.close()
3239     return ret
3240 def create_xml_path(path, **kwargs):
3241     """
3242     Start a transient domain based on the XML-file path passed to the function
3243     :param path: path to a file containing the libvirt XML definition of the domain
3244     :param connection: libvirt connection URI, overriding defaults
3245         .. versionadded:: 2019.2.0
3246     :param username: username to connect with, overriding defaults
3247         .. versionadded:: 2019.2.0
3248     :param password: password to connect with, overriding defaults
3249         .. versionadded:: 2019.2.0
3250     CLI Example:
3251     .. code-block:: bash
3252         salt '*' virt.create_xml_path &lt;path to XML file on the node&gt;
3253     """
3254     try:
3255         with salt.utils.files.fopen(path, "r") as fp_:
3256             return create_xml_str(
3257                 salt.utils.stringutils.to_unicode(fp_.read()), **kwargs
3258             )
3259     except OSError:
3260         return False
3261 def define_xml_str(xml, **kwargs):  # pylint: disable=redefined-outer-name
3262     """
3263     Define a persistent domain based on the XML passed to the function
3264     :param xml: libvirt XML definition of the domain
3265     :param connection: libvirt connection URI, overriding defaults
3266         .. versionadded:: 2019.2.0
3267     :param username: username to connect with, overriding defaults
3268         .. versionadded:: 2019.2.0
3269     :param password: password to connect with, overriding defaults
3270         .. versionadded:: 2019.2.0
3271     CLI Example:
3272     .. code-block:: bash
3273         salt '*' virt.define_xml_str &lt;XML in string format&gt;
3274     """
3275     conn = __get_conn(**kwargs)
3276     ret = conn.defineXML(xml) is not None
3277     conn.close()
3278     return ret
3279 def define_xml_path(path, **kwargs):
3280     """
3281     Define a persistent domain based on the XML-file path passed to the function
3282     :param path: path to a file containing the libvirt XML definition of the domain
3283     :param connection: libvirt connection URI, overriding defaults
3284         .. versionadded:: 2019.2.0
3285     :param username: username to connect with, overriding defaults
3286         .. versionadded:: 2019.2.0
3287     :param password: password to connect with, overriding defaults
3288         .. versionadded:: 2019.2.0
3289     CLI Example:
3290     .. code-block:: bash
3291         salt '*' virt.define_xml_path &lt;path to XML file on the node&gt;
3292     """
3293     try:
3294         with salt.utils.files.fopen(path, "r") as fp_:
3295             return define_xml_str(
3296                 salt.utils.stringutils.to_unicode(fp_.read()), **kwargs
3297             )
3298     except OSError:
3299         return False
3300 def _define_vol_xml_str(conn, xml, pool=None):  # pylint: disable=redefined-outer-name
3301     """
3302     Same function than define_vml_xml_str but using an already opened libvirt connection
3303     """
3304     default_pool = "default" if conn.getType() != "ESX" else "0"
3305     poolname = (
3306         pool if pool else __salt__["config.get"]("virt:storagepool", default_pool)
3307     )
3308     pool = conn.storagePoolLookupByName(str(poolname))
3309     ret = pool.createXML(xml, 0) is not None
3310     return ret
3311 def define_vol_xml_str(
3312     xml, pool=None, **kwargs
3313 ):  # pylint: disable=redefined-outer-name
3314     """
3315     Define a volume based on the XML passed to the function
3316     :param xml: libvirt XML definition of the storage volume
3317     :param pool:
3318         storage pool name to define the volume in.
3319         If defined, this parameter will override the configuration setting.
3320         .. versionadded:: 3001
3321     :param connection: libvirt connection URI, overriding defaults
3322         .. versionadded:: 2019.2.0
3323     :param username: username to connect with, overriding defaults
3324         .. versionadded:: 2019.2.0
3325     :param password: password to connect with, overriding defaults
3326         .. versionadded:: 2019.2.0
3327     CLI Example:
3328     .. code-block:: bash
3329         salt '*' virt.define_vol_xml_str &lt;XML in string format&gt;
3330     The storage pool where the disk image will be defined is ``default``
3331     unless changed with the pool parameter or a configuration like this:
3332     .. code-block:: yaml
3333         virt:
3334             storagepool: mine
3335     """
3336     conn = __get_conn(**kwargs)
3337     ret = False
3338     try:
3339         ret = _define_vol_xml_str(conn, xml, pool=pool)
3340     except libvirtError as err:
3341         raise CommandExecutionError(err.get_error_message())
3342     finally:
3343         conn.close()
3344     return ret
3345 def define_vol_xml_path(path, pool=None, **kwargs):
3346     """
3347     Define a volume based on the XML-file path passed to the function
3348     :param path: path to a file containing the libvirt XML definition of the volume
3349     :param pool:
3350         storage pool name to define the volume in.
3351         If defined, this parameter will override the configuration setting.
3352         .. versionadded:: 3001
3353     :param connection: libvirt connection URI, overriding defaults
3354         .. versionadded:: 2019.2.0
3355     :param username: username to connect with, overriding defaults
3356         .. versionadded:: 2019.2.0
3357     :param password: password to connect with, overriding defaults
3358         .. versionadded:: 2019.2.0
3359     CLI Example:
3360     .. code-block:: bash
3361         salt '*' virt.define_vol_xml_path &lt;path to XML file on the node&gt;
3362     """
3363     try:
3364         with salt.utils.files.fopen(path, "r") as fp_:
3365             return define_vol_xml_str(
3366                 salt.utils.stringutils.to_unicode(fp_.read()), pool=pool, **kwargs
3367             )
3368     except OSError:
3369         return False
3370 def migrate(vm_, target, **kwargs):
3371     """
3372     Shared storage migration
3373     :param vm_: domain name
3374     :param target: target libvirt URI or host name
3375     :param kwargs:
3376         - live:            Use live migration. Default value is True.
3377         - persistent:      Leave the domain persistent on destination host.
3378                            Default value is True.
3379         - undefinesource:  Undefine the domain on the source host.
3380                            Default value is True.
3381         - offline:         If set to True it will migrate the domain definition
3382                            without starting the domain on destination and without
3383                            stopping it on source host. Default value is False.
3384         - max_bandwidth:   The maximum bandwidth (in MiB/s) that will be used.
3385         - max_downtime:    Set maximum tolerable downtime for live-migration.
3386                            The value represents a number of milliseconds the guest
3387                            is allowed to be down at the end of live migration.
3388         - parallel_connections: Specify a number of parallel network connections
3389                            to be used to send memory pages to the destination host.
3390         - compressed:      Activate compression.
3391         - comp_methods:    A comma-separated list of compression methods. Supported
3392                            methods are "mt" and "xbzrle" and can be  used in any
3393                            combination. QEMU defaults to "xbzrle".
3394         - comp_mt_level:   Set compression level. Values are in range from 0 to 9,
3395                            where 1 is maximum speed and 9 is  maximum compression.
3396         - comp_mt_threads: Set number of compress threads on source host.
3397         - comp_mt_dthreads: Set number of decompress threads on target host.
3398         - comp_xbzrle_cache: Set the size of page cache for xbzrle compression in bytes.
3399         - copy_storage:    Migrate non-shared storage. It must be one of the following
3400                            values: all (full disk copy) or incremental (Incremental copy)
3401         - postcopy:        Enable the use of post-copy migration.
3402         - postcopy_bandwidth: The maximum bandwidth allowed in post-copy phase. (MiB/s)
3403         - username:        Username to connect with target host
3404         - password:        Password to connect with target host
3405         .. versionadded:: 3002
3406     CLI Example:
3407     .. code-block:: bash
3408         salt '*' virt.migrate &lt;domain&gt; &lt;target hypervisor URI&gt;
3409         salt src virt.migrate guest qemu+ssh://dst/system
3410         salt src virt.migrate guest qemu+tls://dst/system
3411         salt src virt.migrate guest qemu+tcp://dst/system
3412     A tunnel data migration can be performed by setting this in the
3413     configuration:
3414     .. code-block:: yaml
3415         virt:
3416             tunnel: True
3417     For more details on tunnelled data migrations, report to
3418     https://libvirt.org/migration.html#transporttunnel
3419     """
3420     conn = __get_conn()
3421     dom = _get_domain(conn, vm_)
3422     if not urllib.parse.urlparse(target).scheme:
3423         proto = "qemu"
3424         dst_uri = "{}://{}/system".format(proto, target)
3425     else:
3426         dst_uri = target
3427     ret = _migrate(dom, dst_uri, **kwargs)
3428     conn.close()
3429     return ret
3430 def migrate_start_postcopy(vm_):
3431     """
3432     Starts post-copy migration. This function has to be called
3433     while live migration is in progress and it has been initiated
3434     with the `postcopy=True` option.
3435     CLI Example:
3436     .. code-block:: bash
3437         salt '*' virt.migrate_start_postcopy &lt;domain&gt;
3438     """
3439     conn = __get_conn()
3440     dom = _get_domain(conn, vm_)
3441     try:
3442         dom.migrateStartPostCopy()
3443     except libvirt.libvirtError as err:
3444         conn.close()
3445         raise CommandExecutionError(err.get_error_message())
3446     conn.close()
3447 def seed_non_shared_migrate(disks, force=False):
3448     """
3449     Non shared migration requires that the disks be present on the migration
3450     destination, pass the disks information via this function, to the
3451     migration destination before executing the migration.
3452     :param disks: the list of disk data as provided by virt.get_disks
3453     :param force: skip checking the compatibility of source and target disk
3454                   images if True. (default: False)
3455     CLI Example:
3456     .. code-block:: bash
3457         salt '*' virt.seed_non_shared_migrate &lt;disks&gt;
3458     """
3459     for _, data in disks.items():
3460         fn_ = data["file"]
3461         form = data["file format"]
3462         size = data["virtual size"].split()[1][1:]
3463         if os.path.isfile(fn_) and not force:
3464             pre = salt.utils.yaml.safe_load(
3465                 subprocess.Popen(
3466                     ["qemu-img", "info", "arch"], stdout=subprocess.PIPE
3467                 ).communicate()[0]
3468             )
3469             if (
3470                 pre["file format"] != data["file format"]
3471                 and pre["virtual size"] != data["virtual size"]
3472             ):
3473                 return False
3474         if not os.path.isdir(os.path.dirname(fn_)):
3475             os.makedirs(os.path.dirname(fn_))
3476         if os.path.isfile(fn_):
3477             os.remove(fn_)
3478         subprocess.call(["qemu-img", "create", "-f", form, fn_, size])
3479         creds = _libvirt_creds()
3480         subprocess.call(["chown", "{user}:{group}".format(**creds), fn_])
3481     return True
3482 def set_autostart(vm_, state="on", **kwargs):
3483     """
3484     Set the autostart flag on a VM so that the VM will start with the host
3485     system on reboot.
3486     :param vm_: domain name
3487     :param state: 'on' to auto start the pool, anything else to mark the
3488                   pool not to be started when the host boots
3489     :param connection: libvirt connection URI, overriding defaults
3490         .. versionadded:: 2019.2.0
3491     :param username: username to connect with, overriding defaults
3492         .. versionadded:: 2019.2.0
3493     :param password: password to connect with, overriding defaults
3494         .. versionadded:: 2019.2.0
3495     CLI Example:
3496     .. code-block:: bash
3497         salt "*" virt.set_autostart &lt;domain&gt; &lt;on | off&gt;
3498     """
3499     conn = __get_conn(**kwargs)
3500     dom = _get_domain(conn, vm_)
3501     ret = False
3502     if state == "on":
3503         ret = dom.setAutostart(1) == 0
3504     elif state == "off":
3505         ret = dom.setAutostart(0) == 0
3506     conn.close()
3507     return ret
3508 def undefine(vm_, **kwargs):
3509     """
3510     Remove a defined vm, this does not purge the virtual machine image, and
3511     this only works if the vm is powered down
3512     :param vm_: domain name
3513     :param connection: libvirt connection URI, overriding defaults
3514         .. versionadded:: 2019.2.0
3515     :param username: username to connect with, overriding defaults
3516         .. versionadded:: 2019.2.0
3517     :param password: password to connect with, overriding defaults
3518         .. versionadded:: 2019.2.0
3519     CLI Example:
3520     .. code-block:: bash
3521         salt '*' virt.undefine &lt;domain&gt;
3522     """
3523     conn = __get_conn(**kwargs)
3524     dom = _get_domain(conn, vm_)
3525     if getattr(libvirt, "VIR_DOMAIN_UNDEFINE_NVRAM", False):
3526         ret = dom.undefineFlags(libvirt.VIR_DOMAIN_UNDEFINE_NVRAM) == 0
3527     else:
3528         ret = dom.undefine() == 0
3529     conn.close()
3530     return ret
3531 def purge(vm_, dirs=False, removables=False, **kwargs):
3532     """
3533     Recursively destroy and delete a persistent virtual machine, pass True for
3534     dir's to also delete the directories containing the virtual machine disk
3535     images - USE WITH EXTREME CAUTION!
3536     :param vm_: domain name
3537     :param dirs: pass True to remove containing directories
3538     :param removables: pass True to remove removable devices
3539         .. versionadded:: 2019.2.0
3540     :param connection: libvirt connection URI, overriding defaults
3541         .. versionadded:: 2019.2.0
3542     :param username: username to connect with, overriding defaults
3543         .. versionadded:: 2019.2.0
3544     :param password: password to connect with, overriding defaults
3545         .. versionadded:: 2019.2.0
3546     CLI Example:
3547     .. code-block:: bash
3548         salt '*' virt.purge &lt;domain&gt;
3549     """
3550     conn = __get_conn(**kwargs)
3551     dom = _get_domain(conn, vm_)
3552     disks = _get_disks(conn, dom)
3553     if (
3554         VIRT_STATE_NAME_MAP.get(dom.info()[0], "unknown") != "shutdown"
3555         and dom.destroy() != 0
3556     ):
3557         return False
3558     directories = set()
3559     for disk in disks:
3560         if not removables and disks[disk]["type"] in ["cdrom", "floppy"]:
3561             continue
3562         if disks[disk].get("zfs", False):
3563             time.sleep(3)
3564             fs_name = disks[disk]["file"][len("/dev/zvol/") :]
3565             log.info("Destroying VM ZFS volume %s", fs_name)
3566             __salt__["zfs.destroy"](name=fs_name, force=True)
3567         elif os.path.exists(disks[disk]["file"]):
3568             os.remove(disks[disk]["file"])
3569             directories.add(os.path.dirname(disks[disk]["file"]))
3570         else:
3571             matcher = re.match("^(?P&lt;pool&gt;[^/]+)/(?P&lt;volume&gt;.*)$", disks[disk]["file"])
3572             if matcher:
3573                 pool_name = matcher.group("pool")
3574                 pool = None
3575                 if pool_name in conn.listStoragePools():
3576                     pool = conn.storagePoolLookupByName(pool_name)
3577                 if pool and matcher.group("volume") in pool.listVolumes():
3578                     volume = pool.storageVolLookupByName(matcher.group("volume"))
3579                     volume.delete()
3580     if dirs:
3581         for dir_ in directories:
3582             shutil.rmtree(dir_)
3583     if getattr(libvirt, "VIR_DOMAIN_UNDEFINE_NVRAM", False):
3584         try:
3585             dom.undefineFlags(libvirt.VIR_DOMAIN_UNDEFINE_NVRAM)
3586         except Exception:  # pylint: disable=broad-except
3587             dom.undefine()
3588     else:
3589         dom.undefine()
3590     conn.close()
3591     return True
3592 def virt_type():
3593     """
3594     Returns the virtual machine type as a string
3595     CLI Example:
3596     .. code-block:: bash
3597         salt '*' virt.virt_type
3598     """
3599     return __grains__["virtual"]
3600 def _is_kvm_hyper():
3601     """
3602     Returns a bool whether or not this node is a KVM hypervisor
3603     """
3604     if not os.path.exists("/dev/kvm"):
3605         return False
3606     return "libvirtd" in __salt__["cmd.run"](__grains__["ps"])
3607 def _is_xen_hyper():
3608     """
3609     Returns a bool whether or not this node is a XEN hypervisor
3610     """
3611     try:
3612         if __grains__["virtual_subtype"] != "Xen Dom0":
3613             return False
3614     except KeyError:
3615         return False
3616     try:
3617         with salt.utils.files.fopen("/proc/modules") as fp_:
3618             if "xen_" not in salt.utils.stringutils.to_unicode(fp_.read()):
3619                 return False
3620     except OSError:
3621         return False
3622     return "libvirtd" in __salt__["cmd.run"](__grains__["ps"])
3623 def get_hypervisor():
3624     """
3625     Returns the name of the hypervisor running on this node or ``None``.
3626     Detected hypervisors:
3627     - kvm
3628     - xen
3629     - bhyve
3630     CLI Example:
3631     .. code-block:: bash
3632         salt '*' virt.get_hypervisor
3633     .. versionadded:: 2019.2.0
3634         the function and the ``kvm``, ``xen`` and ``bhyve`` hypervisors support
3635     """
3636     hypervisors = ["kvm", "xen", "bhyve"]
3637     result = [
3638         hyper
3639         for hyper in hypervisors
3640         if getattr(sys.modules[__name__], "_is_{}_hyper".format(hyper))()
3641     ]
3642     return result[0] if result else None
3643 def _is_bhyve_hyper():
3644     sysctl_cmd = "sysctl hw.vmm.create"
3645     vmm_enabled = False
3646     try:
3647         stdout = subprocess.Popen(
3648             ["sysctl", "hw.vmm.create"], stdout=subprocess.PIPE
3649         ).communicate()[0]
3650         vmm_enabled = len(salt.utils.stringutils.to_str(stdout).split('"')[1]) != 0
3651     except IndexError:
3652         pass
3653     return vmm_enabled
3654 def is_hyper():
3655     """
3656     Returns a bool whether or not this node is a hypervisor of any kind
3657     CLI Example:
3658     .. code-block:: bash
3659         salt '*' virt.is_hyper
3660     """
3661     if HAS_LIBVIRT:
3662         return _is_xen_hyper() or _is_kvm_hyper() or _is_bhyve_hyper()
3663     return False
3664 def vm_cputime(vm_=None, **kwargs):
3665     """
3666     Return cputime used by the vms on this hyper in a
3667     list of dicts:
3668     :param vm_: domain name
3669     :param connection: libvirt connection URI, overriding defaults
3670         .. versionadded:: 2019.2.0
3671     :param username: username to connect with, overriding defaults
3672         .. versionadded:: 2019.2.0
3673     :param password: password to connect with, overriding defaults
3674         .. versionadded:: 2019.2.0
3675     .. code-block:: python
3676         [
3677             'your-vm': {
3678                 'cputime' &lt;int&gt;
3679                 'cputime_percent' &lt;int&gt;
3680                 },
3681             ...
3682             ]
3683     If you pass a VM name in as an argument then it will return info
3684     for just the named VM, otherwise it will return all VMs.
3685     CLI Example:
3686     .. code-block:: bash
3687         salt '*' virt.vm_cputime
3688     """
3689     conn = __get_conn(**kwargs)
3690     host_cpus = conn.getInfo()[2]
3691     def _info(dom):
3692         """
3693         Compute cputime info of a domain
3694         """
3695         raw = dom.info()
3696         vcpus = int(raw[3])
3697         cputime = int(raw[4])
3698         cputime_percent = 0
3699         if cputime:
3700             cputime_percent = (1.0e-7 * cputime / host_cpus) / vcpus
3701         return {
3702             "cputime": int(raw[4]),
3703             "cputime_percent": int("{:.0f}".format(cputime_percent)),
3704         }
3705     info = {}
3706     if vm_:
3707         info[vm_] = _info(_get_domain(conn, vm_))
3708     else:
3709         for domain in _get_domain(conn, iterable=True):
3710             info[domain.name()] = _info(domain)
3711     conn.close()
3712     return info
3713 def vm_netstats(vm_=None, **kwargs):
3714     """
3715     Return combined network counters used by the vms on this hyper in a
3716     list of dicts:
3717     :param vm_: domain name
3718     :param connection: libvirt connection URI, overriding defaults
3719         .. versionadded:: 2019.2.0
3720     :param username: username to connect with, overriding defaults
3721         .. versionadded:: 2019.2.0
3722     :param password: password to connect with, overriding defaults
3723         .. versionadded:: 2019.2.0
3724     .. code-block:: python
3725         [
3726             'your-vm': {
3727                 'rx_bytes'   : 0,
3728                 'rx_packets' : 0,
3729                 'rx_errs'    : 0,
3730                 'rx_drop'    : 0,
3731                 'tx_bytes'   : 0,
3732                 'tx_packets' : 0,
3733                 'tx_errs'    : 0,
3734                 'tx_drop'    : 0
3735                 },
3736             ...
3737             ]
3738     If you pass a VM name in as an argument then it will return info
3739     for just the named VM, otherwise it will return all VMs.
3740     CLI Example:
3741     .. code-block:: bash
3742         salt '*' virt.vm_netstats
3743     """
3744     def _info(dom):
3745         """
3746         Compute network stats of a domain
3747         """
3748         nics = _get_nics(dom)
3749         ret = {
3750             "rx_bytes": 0,
3751             "rx_packets": 0,
3752             "rx_errs": 0,
3753             "rx_drop": 0,
3754             "tx_bytes": 0,
3755             "tx_packets": 0,
3756             "tx_errs": 0,
3757             "tx_drop": 0,
3758         }
3759         for attrs in nics.values():
3760             if "target" in attrs:
3761                 dev = attrs["target"]
3762                 stats = dom.interfaceStats(dev)
3763                 ret["rx_bytes"] += stats[0]
3764                 ret["rx_packets"] += stats[1]
3765                 ret["rx_errs"] += stats[2]
3766                 ret["rx_drop"] += stats[3]
3767                 ret["tx_bytes"] += stats[4]
3768                 ret["tx_packets"] += stats[5]
3769                 ret["tx_errs"] += stats[6]
3770                 ret["tx_drop"] += stats[7]
3771         return ret
3772     info = {}
3773     conn = __get_conn(**kwargs)
3774     if vm_:
3775         info[vm_] = _info(_get_domain(conn, vm_))
3776     else:
3777         for domain in _get_domain(conn, iterable=True):
3778             info[domain.name()] = _info(domain)
3779     conn.close()
3780     return info
3781 def vm_diskstats(vm_=None, **kwargs):
3782     """
3783     Return disk usage counters used by the vms on this hyper in a
3784     list of dicts:
3785     :param vm_: domain name
3786     :param connection: libvirt connection URI, overriding defaults
3787         .. versionadded:: 2019.2.0
3788     :param username: username to connect with, overriding defaults
3789         .. versionadded:: 2019.2.0
3790     :param password: password to connect with, overriding defaults
3791         .. versionadded:: 2019.2.0
3792     .. code-block:: python
3793         [
3794             'your-vm': {
3795                 'rd_req'   : 0,
3796                 'rd_bytes' : 0,
3797                 'wr_req'   : 0,
3798                 'wr_bytes' : 0,
3799                 'errs'     : 0
3800                 },
3801             ...
3802             ]
3803     If you pass a VM name in as an argument then it will return info
3804     for just the named VM, otherwise it will return all VMs.
3805     CLI Example:
3806     .. code-block:: bash
3807         salt '*' virt.vm_blockstats
3808     """
3809     def get_disk_devs(dom):
3810         """
3811         Extract the disk devices names from the domain XML definition
3812         """
3813         doc = ElementTree.fromstring(get_xml(dom, **kwargs))
3814         return [target.get("dev") for target in doc.findall("devices/disk/target")]
3815     def _info(dom):
3816         """
3817         Compute the disk stats of a domain
3818         """
3819         disks = get_disk_devs(dom)
3820         ret = {"rd_req": 0, "rd_bytes": 0, "wr_req": 0, "wr_bytes": 0, "errs": 0}
3821         for disk in disks:
3822             stats = dom.blockStats(disk)
3823             ret["rd_req"] += stats[0]
3824             ret["rd_bytes"] += stats[1]
3825             ret["wr_req"] += stats[2]
3826             ret["wr_bytes"] += stats[3]
3827             ret["errs"] += stats[4]
3828         return ret
3829     info = {}
3830     conn = __get_conn(**kwargs)
3831     if vm_:
3832         info[vm_] = _info(_get_domain(conn, vm_))
3833     else:
3834         for domain in _get_domain(conn, iterable=True, inactive=False):
3835             info[domain.name()] = _info(domain)
3836     conn.close()
3837     return info
3838 def _parse_snapshot_description(vm_snapshot, unix_time=False):
3839     """
3840     Parse XML doc and return a dict with the status values.
3841     :param xmldoc:
3842     :return:
3843     """
3844     ret = dict()
3845     tree = ElementTree.fromstring(vm_snapshot.getXMLDesc())
3846     for node in tree:
3847         if node.tag == "name":
3848             ret["name"] = node.text
3849         elif node.tag == "creationTime":
3850             ret["created"] = (
3851                 datetime.datetime.fromtimestamp(float(node.text)).isoformat(" ")
3852                 if not unix_time
3853                 else float(node.text)
3854             )
3855         elif node.tag == "state":
3856             ret["running"] = node.text == "running"
3857     ret["current"] = vm_snapshot.isCurrent() == 1
3858     return ret
3859 def list_snapshots(domain=None, **kwargs):
3860     """
3861     List available snapshots for certain vm or for all.
3862     :param domain: domain name
3863     :param connection: libvirt connection URI, overriding defaults
3864         .. versionadded:: 2019.2.0
3865     :param username: username to connect with, overriding defaults
3866         .. versionadded:: 2019.2.0
3867     :param password: password to connect with, overriding defaults
3868         .. versionadded:: 2019.2.0
3869     .. versionadded:: 2016.3.0
3870     CLI Example:
3871     .. code-block:: bash
3872         salt '*' virt.list_snapshots
3873         salt '*' virt.list_snapshots &lt;domain&gt;
3874     """
3875     ret = dict()
3876     conn = __get_conn(**kwargs)
3877     for vm_domain in _get_domain(conn, *(domain and [domain] or list()), iterable=True):
3878         ret[vm_domain.name()] = [
3879             _parse_snapshot_description(snap) for snap in vm_domain.listAllSnapshots()
3880         ] or "N/A"
3881     conn.close()
3882     return ret
3883 def snapshot(domain, name=None, suffix=None, **kwargs):
3884     """
3885     Create a snapshot of a VM.
3886     :param domain: domain name
3887     :param name: Name of the snapshot. If the name is omitted, then will be used original domain
3888                  name with ISO 8601 time as a suffix.
3889     :param suffix: Add suffix for the new name. Useful in states, where such snapshots
3890                    can be distinguished from manually created.
3891     :param connection: libvirt connection URI, overriding defaults
3892         .. versionadded:: 2019.2.0
3893     :param username: username to connect with, overriding defaults
3894         .. versionadded:: 2019.2.0
3895     :param password: password to connect with, overriding defaults
3896         .. versionadded:: 2019.2.0
3897     .. versionadded:: 2016.3.0
3898     CLI Example:
3899     .. code-block:: bash
3900         salt '*' virt.snapshot &lt;domain&gt;
3901     """
3902     if name and name.lower() == domain.lower():
3903         raise CommandExecutionError(
3904             "Virtual Machine {name} is already defined. "
3905             "Please choose another name for the snapshot".format(name=name)
3906         )
3907     if not name:
3908         name = "{domain}-{tsnap}".format(
3909             domain=domain, tsnap=time.strftime("%Y%m%d-%H%M%S", time.localtime())
3910         )
3911     if suffix:
3912         name = "{name}-{suffix}".format(name=name, suffix=suffix)
3913     doc = ElementTree.Element("domainsnapshot")
3914     n_name = ElementTree.SubElement(doc, "name")
3915     n_name.text = name
3916     conn = __get_conn(**kwargs)
3917     _get_domain(conn, domain).snapshotCreateXML(xmlutil.element_to_str(doc))
3918     conn.close()
3919     return {"name": name}
3920 def delete_snapshots(name, *names, **kwargs):
3921     """
3922     Delete one or more snapshots of the given VM.
3923     :param name: domain name
3924     :param names: names of the snapshots to remove
3925     :param connection: libvirt connection URI, overriding defaults
3926         .. versionadded:: 2019.2.0
3927     :param username: username to connect with, overriding defaults
3928         .. versionadded:: 2019.2.0
3929     :param password: password to connect with, overriding defaults
3930         .. versionadded:: 2019.2.0
3931     .. versionadded:: 2016.3.0
3932     CLI Example:
3933     .. code-block:: bash
3934         salt '*' virt.delete_snapshots &lt;domain&gt; all=True
3935         salt '*' virt.delete_snapshots &lt;domain&gt; &lt;snapshot&gt;
3936         salt '*' virt.delete_snapshots &lt;domain&gt; &lt;snapshot1&gt; &lt;snapshot2&gt; ...
3937     """
3938     deleted = dict()
3939     conn = __get_conn(**kwargs)
3940     domain = _get_domain(conn, name)
3941     for snap in domain.listAllSnapshots():
3942         if snap.getName() in names or not names:
3943             deleted[snap.getName()] = _parse_snapshot_description(snap)
3944             snap.delete()
3945     conn.close()
3946     available = {
3947         name: [_parse_snapshot_description(snap) for snap in domain.listAllSnapshots()]
3948         or "N/A"
3949     }
3950     return {"available": available, "deleted": deleted}
3951 def revert_snapshot(name, vm_snapshot=None, cleanup=False, **kwargs):
3952     """
3953     Revert snapshot to the previous from current (if available) or to the specific.
3954     :param name: domain name
3955     :param vm_snapshot: name of the snapshot to revert
3956     :param cleanup: Remove all newer than reverted snapshots. Values: True or False (default False).
3957     :param connection: libvirt connection URI, overriding defaults
3958         .. versionadded:: 2019.2.0
3959     :param username: username to connect with, overriding defaults
3960         .. versionadded:: 2019.2.0
3961     :param password: password to connect with, overriding defaults
3962         .. versionadded:: 2019.2.0
3963     .. versionadded:: 2016.3.0
3964     CLI Example:
3965     .. code-block:: bash
3966         salt '*' virt.revert &lt;domain&gt;
3967         salt '*' virt.revert &lt;domain&gt; &lt;snapshot&gt;
3968     """
3969     ret = dict()
3970     conn = __get_conn(**kwargs)
3971     domain = _get_domain(conn, name)
3972     snapshots = domain.listAllSnapshots()
3973     _snapshots = list()
3974     for snap_obj in snapshots:
3975         _snapshots.append(
3976             {
3977                 "idx": _parse_snapshot_description(snap_obj, unix_time=True)["created"],
3978                 "ptr": snap_obj,
3979             }
3980         )
3981     snapshots = [
3982         w_ptr["ptr"]
3983         for w_ptr in sorted(_snapshots, key=lambda item: item["idx"], reverse=True)
3984     ]
3985     del _snapshots
3986     if not snapshots:
3987         conn.close()
3988         raise CommandExecutionError("No snapshots found")
3989     elif len(snapshots) == 1:
3990         conn.close()
3991         raise CommandExecutionError(
3992             "Cannot revert to itself: only one snapshot is available."
3993         )
3994     snap = None
3995     for p_snap in snapshots:
3996         if not vm_snapshot:
3997             if p_snap.isCurrent() and snapshots[snapshots.index(p_snap) + 1 :]:
3998                 snap = snapshots[snapshots.index(p_snap) + 1 :][0]
3999                 break
4000         elif p_snap.getName() == vm_snapshot:
4001             snap = p_snap
4002             break
4003     if not snap:
4004         conn.close()
4005         raise CommandExecutionError(
4006             snapshot
4007             and 'Snapshot "{}" not found'.format(vm_snapshot)
4008             or "No more previous snapshots available"
4009         )
4010     elif snap.isCurrent():
4011         conn.close()
4012         raise CommandExecutionError("Cannot revert to the currently running snapshot.")
4013     domain.revertToSnapshot(snap)
4014     ret["reverted"] = snap.getName()
4015     if cleanup:
4016         delete = list()
4017         for p_snap in snapshots:
4018             if p_snap.getName() != snap.getName():
4019                 delete.append(p_snap.getName())
4020                 p_snap.delete()
4021             else:
4022                 break
4023         ret["deleted"] = delete
4024     else:
4025         ret["deleted"] = "N/A"
4026     conn.close()
4027     return ret
4028 def _caps_add_machine(machines, node):
4029     """
4030     Parse the &lt;machine&gt; element of the host capabilities and add it
4031     to the machines list.
4032     """
4033     maxcpus = node.get("maxCpus")
4034     canonical = node.get("canonical")
4035     name = node.text
4036     alternate_name = ""
4037     if canonical:
4038         alternate_name = name
4039         name = canonical
4040     machine = machines.get(name)
4041     if not machine:
4042         machine = {"alternate_names": []}
4043         if maxcpus:
4044             machine["maxcpus"] = int(maxcpus)
4045         machines[name] = machine
4046     if alternate_name:
4047         machine["alternate_names"].append(alternate_name)
4048 def _parse_caps_guest(guest):
4049     """
4050     Parse the &lt;guest&gt; element of the connection capabilities XML
4051     """
4052     arch_node = guest.find("arch")
4053     result = {
4054         "os_type": guest.find("os_type").text,
4055         "arch": {"name": arch_node.get("name"), "machines": {}, "domains": {}},
4056     }
4057     child = None
4058     for child in arch_node:
4059         if child.tag == "wordsize":
4060             result["arch"]["wordsize"] = int(child.text)
4061         elif child.tag == "emulator":
4062             result["arch"]["emulator"] = child.text
4063         elif child.tag == "machine":
4064             _caps_add_machine(result["arch"]["machines"], child)
4065         elif child.tag == "domain":
4066             domain_type = child.get("type")
4067             domain = {"emulator": None, "machines": {}}
4068             emulator_node = child.find("emulator")
4069             if emulator_node is not None:
4070                 domain["emulator"] = emulator_node.text
4071             for machine in child.findall("machine"):
4072                 _caps_add_machine(domain["machines"], machine)
4073             result["arch"]["domains"][domain_type] = domain
4074     features_nodes = guest.find("features")
4075     if features_nodes is not None and child is not None:
4076         result["features"] = {
4077             child.tag: {
4078                 "toggle": child.get("toggle", "no") == "yes",
4079                 "default": child.get("default", "on") == "on",
4080             }
4081             for child in features_nodes
4082         }
4083     return result
4084 def _parse_caps_cell(cell):
4085     """
4086     Parse the &lt;cell&gt; nodes of the connection capabilities XML output.
4087     """
4088     result = {"id": int(cell.get("id"))}
4089     mem_node = cell.find("memory")
4090     if mem_node is not None:
4091         unit = mem_node.get("unit", "KiB")
4092         memory = mem_node.text
4093         result["memory"] = "{} {}".format(memory, unit)
4094     pages = [
4095         {
4096             "size": "{} {}".format(page.get("size"), page.get("unit", "KiB")),
4097             "available": int(page.text),
4098         }
4099         for page in cell.findall("pages")
4100     ]
4101     if pages:
4102         result["pages"] = pages
4103     distances = {
4104         int(distance.get("id")): int(distance.get("value"))
4105         for distance in cell.findall("distances/sibling")
4106     }
4107     if distances:
4108         result["distances"] = distances
4109     cpus = []
4110     for cpu_node in cell.findall("cpus/cpu"):
4111         cpu = {"id": int(cpu_node.get("id"))}
4112         socket_id = cpu_node.get("socket_id")
4113         if socket_id:
4114             cpu["socket_id"] = int(socket_id)
4115         core_id = cpu_node.get("core_id")
4116         if core_id:
4117             cpu["core_id"] = int(core_id)
4118         siblings = cpu_node.get("siblings")
4119         if siblings:
4120             cpu["siblings"] = siblings
4121         cpus.append(cpu)
4122     if cpus:
4123         result["cpus"] = cpus
4124     return result
4125 def _parse_caps_bank(bank):
4126     """
4127     Parse the &lt;bank&gt; element of the connection capabilities XML.
4128     """
4129     result = {
4130         "id": int(bank.get("id")),
4131         "level": int(bank.get("level")),
4132         "type": bank.get("type"),
4133         "size": "{} {}".format(bank.get("size"), bank.get("unit")),
4134         "cpus": bank.get("cpus"),
4135     }
4136     controls = []
4137     for control in bank.findall("control"):
4138         unit = control.get("unit")
4139         result_control = {
4140             "granularity": "{} {}".format(control.get("granularity"), unit),
4141             "type": control.get("type"),
4142             "maxAllocs": int(control.get("maxAllocs")),
4143         }
4144         minimum = control.get("min")
4145         if minimum:
4146             result_control["min"] = "{} {}".format(minimum, unit)
4147         controls.append(result_control)
4148     if controls:
4149         result["controls"] = controls
4150     return result
4151 def _parse_caps_host(host):
4152     """
4153     Parse the &lt;host&gt; element of the connection capabilities XML.
4154     """
4155     result = {}
4156     for child in host:
4157         if child.tag == "uuid":
4158             result["uuid"] = child.text
4159         elif child.tag == "cpu":
4160             cpu = {
4161                 if child.find("arch") is not None
4162                 else None,
4163                 "model": child.find("model")<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.text
4164                 if child.find("model") is not None
4165                 else None,
4166                 "vendor": child.find("vendor").text
4167                 if child.find("vendor") is not None
4168                 else None,
4169                 "features": [
4170                     feature.get("name") for feature in child.findall("feature")
4171                 ],
4172                 "pages": [
4173                     {"size": "{} {}".format(page.</b></font>get("size"), page.get("unit", "KiB"))}
4174                     for page in child.findall("pages")
4175                 ],
4176             }
4177             microcode = child.find("microcode")
4178             if microcode is not None:
4179                 cpu["microcode"] = microcode.get("version")
4180             topology = child.find("topology")
4181             if topology is not None:
4182                 cpu["sockets"] = int(topology.get("sockets"))
4183                 cpu["cores"] = int(topology.get("cores"))
4184                 cpu["threads"] = int(topology.get("threads"))
4185             result["cpu"] = cpu
4186         elif child.tag == "power_management":
4187             result["power_management"] = [node.tag for node in child]
4188         elif child.tag == "migration_features":
4189             result["migration"] = {
4190                 "live": child.find("live") is not None,
4191                 "transports": [
4192                     node.text for node in child.findall("uri_transports/uri_transport")
4193                 ],
4194             }
4195         elif child.tag == "topology":
4196             result["topology"] = {
4197                 "cells": [
4198                     _parse_caps_cell(cell) for cell in child.findall("cells/cell")
4199                 ]
4200             }
4201         elif child.tag == "cache":
4202             result["cache"] = {
4203                 "banks": [_parse_caps_bank(bank) for bank in child.findall("bank")]
4204             }
4205     result["security"] = [
4206         {
4207             "model": secmodel.find("model").text
4208             if secmodel.find("model") is not None
4209             else None,
4210             "doi": secmodel.find("doi").text
4211             if secmodel.find("doi") is not None
4212             else None,
4213             "baselabels": [
4214                 {"type": label.get("type"), "label": label.text}
4215                 for label in secmodel.findall("baselabel")
4216             ],
4217         }
4218         for secmodel in host.findall("secmodel")
4219     ]
4220     return result
4221 def _capabilities(conn):
4222     """
4223     Return the hypervisor connection capabilities.
4224     :param conn: opened libvirt connection to use
4225     """
4226     caps = ElementTree.fromstring(conn.getCapabilities())
4227     return {
4228         "host": _parse_caps_host(caps.find("host")),
4229         "guests": [_parse_caps_guest(guest) for guest in caps.findall("guest")],
4230     }
4231 def capabilities(**kwargs):
4232     """
4233     Return the hypervisor connection capabilities.
4234     :param connection: libvirt connection URI, overriding defaults
4235     :param username: username to connect with, overriding defaults
4236     :param password: password to connect with, overriding defaults
4237     .. versionadded:: 2019.2.0
4238     CLI Example:
4239     .. code-block:: bash
4240         salt '*' virt.capabilities
4241     """
4242     conn = __get_conn(**kwargs)
4243     try:
4244         caps = _capabilities(conn)
4245     except libvirt.libvirtError as err:
4246         raise CommandExecutionError(str(err))
4247     finally:
4248         conn.close()
4249     return caps
4250 def _parse_caps_enum(node):
4251     """
4252     Return a tuple containing the name of the enum and the possible values
4253     """
4254     return (node.get("name"), [value.text for value in node.findall("value")])
4255 def _parse_caps_cpu(node):
4256     """
4257     Parse the &lt;cpu&gt; element of the domain capabilities
4258     """
4259     result = {}
4260     for mode in node.findall("mode"):
4261         if not mode.get("supported") == "yes":
4262             continue
4263         name = mode.get("name")
4264         if name == "host-passthrough":
4265             result[name] = True
4266         elif name == "host-model":
4267             host_model = {}
4268             model_node = mode.find("model")
4269             if model_node is not None:
4270                 model = {"name": model_node.text}
4271                 vendor_id = model_node.get("vendor_id")
4272                 if vendor_id:
4273                     model["vendor_id"] = vendor_id
4274                 fallback = model_node.get("fallback")
4275                 if fallback:
4276                     model["fallback"] = fallback
4277                 host_model["model"] = model
4278             vendor = (
4279                 mode.find("vendor").text if mode.find("vendor") is not None else None
4280             )
4281             if vendor:
4282                 host_model["vendor"] = vendor
4283             features = {
4284                 feature.get("name"): feature.get("policy")
4285                 for feature in mode.findall("feature")
4286             }
4287             if features:
4288                 host_model["features"] = features
4289             result[name] = host_model
4290         elif name == "custom":
4291             custom_model = {}
4292             models = {
4293                 model.text: model.get("usable") for model in mode.findall("model")
4294             }
4295             if models:
4296                 custom_model["models"] = models
4297             result[name] = custom_model
4298     return result
4299 def _parse_caps_devices_features(node):
4300     """
4301     Parse the devices or features list of the domain capatilities
4302     """
4303     result = {}
4304     for child in node:
4305         if child.get("supported") == "yes":
4306             enums = [_parse_caps_enum(node) for node in child.findall("enum")]
4307             result[child.tag] = {item[0]: item[1] for item in enums if item[0]}
4308     return result
4309 def _parse_caps_loader(node):
4310     """
4311     Parse the &lt;loader&gt; element of the domain capabilities.
4312     """
4313     enums = [_parse_caps_enum(enum) for enum in node.findall("enum")]
4314     result = {item[0]: item[1] for item in enums if item[0]}
4315     values = [child.text for child in node.findall("value")]
4316     if values:
4317         result["values"] = values
4318     return result
4319 def _parse_domain_caps(caps):
4320     """
4321     Parse the XML document of domain capabilities into a structure.
4322     """
4323     result = {
4324         "emulator": caps.find("path").text if caps.find("path") is not None else None,
4325         "domain": caps.find("domain").text if caps.find("domain") is not None else None,
4326         "machine": caps.find("machine").text
4327         if caps.find("machine") is not None
4328         else None,
4329         "arch": caps.find("arch").text if caps.find("arch") is not None else None,
4330     }
4331     for child in caps:
4332         if child.tag == "vcpu" and child.get("max"):
4333             result["max_vcpus"] = int(child.get("max"))
4334         elif child.tag == "iothreads":
4335             result["iothreads"] = child.get("supported") == "yes"
4336         elif child.tag == "os":
4337             result["os"] = {}
4338             loader_node = child.find("loader")
4339             if loader_node is not None and loader_node.get("supported") == "yes":
4340                 loader = _parse_caps_loader(loader_node)
4341                 result["os"]["loader"] = loader
4342         elif child.tag == "cpu":
4343             cpu = _parse_caps_cpu(child)
4344             if cpu:
4345                 result["cpu"] = cpu
4346         elif child.tag == "devices":
4347             devices = _parse_caps_devices_features(child)
4348             if devices:
4349                 result["devices"] = devices
4350         elif child.tag == "features":
4351             features = _parse_caps_devices_features(child)
4352             if features:
4353                 result["features"] = features
4354     return result
4355 def domain_capabilities(emulator=None, arch=None, machine=None, domain=None, **kwargs):
4356     """
4357     Return the domain capabilities given an emulator, architecture, machine or virtualization type.
4358     .. versionadded:: 2019.2.0
4359     :param emulator: return the capabilities for the given emulator binary
4360     :param arch: return the capabilities for the given CPU architecture
4361     :param machine: return the capabilities for the given emulated machine type
4362     :param domain: return the capabilities for the given virtualization type.
4363     :param connection: libvirt connection URI, overriding defaults
4364     :param username: username to connect with, overriding defaults
4365     :param password: password to connect with, overriding defaults
4366     The list of the possible emulator, arch, machine and domain can be found in
4367     the host capabilities output.
4368     If none of the parameters is provided, the libvirt default one is returned.
4369     CLI Example:
4370     .. code-block:: bash
4371         salt '*' virt.domain_capabilities arch='x86_64' domain='kvm'
4372     """
4373     conn = __get_conn(**kwargs)
4374     result = []
4375     try:
4376         caps = ElementTree.fromstring(
4377             conn.getDomainCapabilities(emulator, arch, machine, domain, 0)
4378         )
4379         result = _parse_domain_caps(caps)
4380     finally:
4381         conn.close()
4382     return result
4383 def all_capabilities(**kwargs):
4384     """
4385     Return the host and domain capabilities in a single call.
4386     .. versionadded:: 3001
4387     :param connection: libvirt connection URI, overriding defaults
4388     :param username: username to connect with, overriding defaults
4389     :param password: password to connect with, overriding defaults
4390     CLI Example:
4391     .. code-block:: bash
4392         salt '*' virt.all_capabilities
4393     """
4394     conn = __get_conn(**kwargs)
4395     try:
4396         host_caps = ElementTree.fromstring(conn.getCapabilities())
4397         domains = [
4398             [
4399                 (
4400                     guest.get("arch", {}).get("name", None),
4401                     key,
4402                     guest.get("arch", {}).get("emulator", None),
4403                 )
4404                 for key in guest.get("arch", {}).get("domains", {}).keys()
4405             ]
4406             for guest in [
4407                 _parse_caps_guest(guest) for guest in host_caps.findall("guest")
4408             ]
4409         ]
4410         flattened = [pair for item in (x for x in domains) for pair in item]
4411         result = {
4412             "host": {
4413                 "host": _parse_caps_host(host_caps.find("host")),
4414                 "guests": [
4415                     _parse_caps_guest(guest) for guest in host_caps.findall("guest")
4416                 ],
4417             },
4418             "domains": [
4419                 _parse_domain_caps(
4420                     ElementTree.fromstring(
4421                         conn.getDomainCapabilities(emulator, arch, None, domain)
4422                     )
4423                 )
4424                 for (arch, domain, emulator) in flattened
4425             ],
4426         }
4427         return result
4428     finally:
4429         conn.close()
4430 def cpu_baseline(full=False, migratable=False, out="libvirt", **kwargs):
4431     """
4432     Return the optimal 'custom' CPU baseline config for VM's on this minion
4433     .. versionadded:: 2016.3.0
4434     :param full: Return all CPU features rather than the ones on top of the closest CPU model
4435     :param migratable: Exclude CPU features that are unmigratable (libvirt 2.13+)
4436     :param out: 'libvirt' (default) for usable libvirt XML definition, 'salt' for nice dict
4437     :param connection: libvirt connection URI, overriding defaults
4438         .. versionadded:: 2019.2.0
4439     :param username: username to connect with, overriding defaults
4440         .. versionadded:: 2019.2.0
4441     :param password: password to connect with, overriding defaults
4442         .. versionadded:: 2019.2.0
4443     CLI Example:
4444     .. code-block:: bash
4445         salt '*' virt.cpu_baseline
4446     """
4447     conn = __get_conn(**kwargs)
4448     caps = ElementTree.fromstring(conn.getCapabilities())
4449     cpu = caps.find("host/cpu")
4450     host_cpu_def = xmlutil.element_to_str(cpu)
4451     log.debug("Host CPU model definition: %s", host_cpu_def)
4452     flags = 0
4453     if migratable:
4454         if getattr(libvirt, "VIR_CONNECT_BASELINE_CPU_MIGRATABLE", False):
4455             flags += libvirt.VIR_CONNECT_BASELINE_CPU_MIGRATABLE
4456         else:
4457             conn.close()
4458             raise ValueError
4459     if full and getattr(libvirt, "VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES", False):
4460         flags += libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES
4461     cpu = ElementTree.fromstring(conn.baselineCPU([host_cpu_def], flags))
4462     conn.close()
4463     if full and not getattr(libvirt, "VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES", False):
4464         with salt.utils.files.fopen("/usr/share/libvirt/cpu_map.xml", "r") as cpu_map:
4465             cpu_map = ElementTree.parse(cpu_map)
4466         cpu_model = cpu.find("model").text
4467         while cpu_model:
4468             cpu_map_models = cpu_map.findall("arch/model")
4469             cpu_specs = [
4470                 el
4471                 for el in cpu_map_models
4472                 if el.get("name") == cpu_model and bool(len(el))
4473             ]
4474             if not cpu_specs:
4475                 raise ValueError("Model {} not found in CPU map".format(cpu_model))
4476             elif len(cpu_specs) &gt; 1:
4477                 raise ValueError(
4478                     "Multiple models {} found in CPU map".format(cpu_model)
4479                 )
4480             cpu_specs = cpu_specs[0]
4481             model_node = cpu_specs.find("model")
4482             if model_node is None:
4483                 cpu_model = None
4484             else:
4485                 cpu_model = model_node.get("name")
4486             cpu.extend([feature for feature in cpu_specs.findall("feature")])
4487     if out == "salt":
4488         return {
4489             "model": cpu.find("model").text,
4490             "vendor": cpu.find("vendor").text,
4491             "features": [feature.get("name") for feature in cpu.findall("feature")],
4492         }
4493     return ElementTree.tostring(cpu)
4494 def network_define(
4495     name,
4496     bridge,
4497     forward,
4498     ipv4_config=None,
4499     ipv6_config=None,
4500     vport=None,
4501     tag=None,
4502     autostart=True,
4503     start=True,
4504     mtu=None,
4505     domain=None,
4506     nat=None,
4507     interfaces=None,
4508     addresses=None,
4509     physical_function=None,
4510     dns=None,
4511     **kwargs
4512 ):
4513     """
4514     Create libvirt network.
4515     :param name: Network name.
4516     :param bridge: Bridge name.
4517     :param forward: Forward mode (bridge, router, nat).
4518         .. versionchanged:: 3003
4519            a ``None`` value creates an isolated network with no forwarding at all
4520     :param vport: Virtualport type.
4521         The value can also be a dictionary with ``type`` and ``parameters`` keys.
4522         The ``parameters`` value is a dictionary of virtual port parameters.
4523         .. code-block:: yaml
4524           - vport:
4525               type: openvswitch
4526               parameters:
4527                 interfaceid: 09b11c53-8b5c-4eeb-8f00-d84eaa0aaa4f
4528         .. versionchanged:: 3003
4529            possible dictionary value
4530     :param tag: Vlan tag.
4531         The value can also be a dictionary with the ``tags`` and optional ``trunk`` keys.
4532         ``trunk`` is a boolean value indicating whether to use VLAN trunking.
4533         ``tags`` is a list of dictionaries with keys ``id`` and ``nativeMode``.
4534         The ``nativeMode`` value can be one of ``tagged`` or ``untagged``.
4535         .. code-block:: yaml
4536           - tag:
4537               trunk: True
4538               tags:
4539                 - id: 42
4540                   nativeMode: untagged
4541                 - id: 47
4542         .. versionchanged:: 3003
4543            possible dictionary value
4544     :param autostart: Network autostart (default True).
4545     :param start: Network start (default True).
4546     :param ipv4_config: IP v4 configuration.
4547         Dictionary describing the IP v4 setup like IP range and
4548         a possible DHCP configuration. The structure is documented
4549         in net-define-ip_.
4550         .. versionadded:: 3000
4551     :type ipv4_config: dict or None
4552     :param ipv6_config: IP v6 configuration.
4553         Dictionary describing the IP v6 setup like IP range and
4554         a possible DHCP configuration. The structure is documented
4555         in net-define-ip_.
4556         .. versionadded:: 3000
4557     :type ipv6_config: dict or None
4558     :param connection: libvirt connection URI, overriding defaults.
4559     :param username: username to connect with, overriding defaults.
4560     :param password: password to connect with, overriding defaults.
4561     :param mtu: size of the Maximum Transmission Unit (MTU) of the network.
4562         (default ``None``)
4563         .. versionadded:: 3003
4564     :param domain: DNS domain name of the DHCP server.
4565         The value is a dictionary with a mandatory ``name`` property and an optional ``localOnly`` boolean one.
4566         (default ``None``)
4567         .. code-block:: yaml
4568           - domain:
4569               name: lab.acme.org
4570               localOnly: True
4571         .. versionadded:: 3003
4572     :param nat: addresses and ports to route in NAT forward mode.
4573         The value is a dictionary with optional keys ``address`` and ``port``.
4574         Both values are a dictionary with ``start`` and ``end`` values.
4575         (default ``None``)
4576         .. code-block:: yaml
4577           - forward: nat
4578           - nat:
4579               address:
4580                 start: 1.2.3.4
4581                 end: 1.2.3.10
4582               port:
4583                 start: 500
4584                 end: 1000
4585         .. versionadded:: 3003
4586     :param interfaces: whitespace separated list of network interfaces devices that can be used for this network.
4587         (default ``None``)
4588         .. code-block:: yaml
4589           - forward: passthrough
4590           - interfaces: "eth10 eth11 eth12"
4591         .. versionadded:: 3003
4592     :param addresses: whitespace separated list of addresses of PCI devices that can be used for this network in `hostdev` forward mode.
4593         (default ``None``)
4594         .. code-block:: yaml
4595           - forward: hostdev
4596           - interfaces: "0000:04:00.1 0000:e3:01.2"
4597         .. versionadded:: 3003
4598     :param physical_function: device name of the physical interface to use in ``hostdev`` forward mode.
4599         (default ``None``)
4600         .. code-block:: yaml
4601           - forward: hostdev
4602           - physical_function: "eth0"
4603         .. versionadded:: 3003
4604     :param dns: virtual network DNS configuration.
4605         The value is a dictionary described in net-define-dns_.
4606         (default ``None``)
4607         .. code-block:: yaml
4608           - dns:
4609               forwarders:
4610                 - domain: example.com
4611                   addr: 192.168.1.1
4612                 - addr: 8.8.8.8
4613                 - domain: www.example.com
4614               txt:
4615                 example.com: "v=spf1 a -all"
4616                 _http.tcp.example.com: "name=value,paper=A4"
4617               hosts:
4618                 192.168.1.2:
4619                   - mirror.acme.lab
4620                   - test.acme.lab
4621               srvs:
4622                 - name: ldap
4623                   protocol: tcp
4624                   domain: ldapserver.example.com
4625                   target: .
4626                   port: 389
4627                   priority: 1
4628                   weight: 10
4629         .. versionadded:: 3003
4630     .. _net-define-ip:
4631     .. rubric:: IP configuration definition
4632     Both the IPv4 and IPv6 configuration dictionaries can contain the following properties:
4633     cidr
4634         CIDR notation for the network. For example '192.168.124.0/24'
4635     dhcp_ranges
4636         A list of dictionaries with ``'start'`` and ``'end'`` properties.
4637     hosts
4638         A list of dictionaries with ``ip`` property and optional ``name``, ``mac`` and ``id`` properties.
4639         .. versionadded:: 3003
4640     bootp
4641         A dictionary with a ``file`` property and an optional ``server`` one.
4642         .. versionadded:: 3003
4643     tftp
4644         The path to the TFTP root directory to serve.
4645         .. versionadded:: 3003
4646     .. _net-define-dns:
4647     .. rubric:: DNS configuration definition
4648     The DNS configuration dictionary contains the following optional properties:
4649     forwarders
4650         List of alternate DNS forwarders to use.
4651         Each item is a dictionary with the optional ``domain`` and ``addr`` keys.
4652         If both are provided, the requests to the domain are forwarded to the server at the ``addr``.
4653         If only ``domain`` is provided the requests matching this domain will be resolved locally.
4654         If only ``addr`` is provided all requests will be forwarded to this DNS server.
4655     txt:
4656         Dictionary of TXT fields to set.
4657     hosts:
4658         Dictionary of host DNS entries.
4659         The key is the IP of the host, and the value is a list of hostnames for it.
4660     srvs:
4661         List of SRV DNS entries.
4662         Each entry is a dictionary with the mandatory ``name`` and ``protocol`` keys.
4663         Entries can also have ``target``, ``port``, ``priority``, ``domain`` and ``weight`` optional properties.
4664     CLI Example:
4665     .. code-block:: bash
4666         salt '*' virt.network_define network main bridge openvswitch
4667     .. versionadded:: 2019.2.0
4668     """
4669     conn = __get_conn(**kwargs)
4670     vport = kwargs.get("vport", None)
4671     tag = kwargs.get("tag", None)
4672     net_xml = _gen_net_xml(
4673         name,
4674         bridge,
4675         forward,
4676         vport,
4677         tag=tag,
4678         ip_configs=[config for config in [ipv4_config, ipv6_config] if config],
4679         mtu=mtu,
4680         domain=domain,
4681         nat=nat,
4682         interfaces=interfaces,
4683         addresses=addresses,
4684         physical_function=physical_function,
4685         dns=dns,
4686     )
4687     try:
4688         conn.networkDefineXML(net_xml)
4689     except libvirt.libvirtError as err:
4690         log.warning(err)
4691         conn.close()
4692         raise err  # a real error we should report upwards
4693     try:
4694         network = conn.networkLookupByName(name)
4695     except libvirt.libvirtError as err:
4696         log.warning(err)
4697         conn.close()
4698         raise err  # a real error we should report upwards
4699     if network is None:
4700         conn.close()
4701         return False
4702     if (start or autostart) and network.isActive() != 1:
4703         network.create()
4704     if autostart and network.autostart() != 1:
4705         network.setAutostart(int(autostart))
4706     elif not autostart and network.autostart() == 1:
4707         network.setAutostart(int(autostart))
4708     conn.close()
4709     return True
4710 def _remove_empty_xml_node(node):
4711     """
4712     Remove the nodes with no children, no text and no attribute
4713     """
4714     for child in node:
4715         if not child.tail and not child.text and not child.items() and not child:
4716             node.remove(child)
4717         else:
4718             _remove_empty_xml_node(child)
4719     return node
4720 def network_update(
4721     name,
4722     bridge,
4723     forward,
4724     ipv4_config=None,
4725     ipv6_config=None,
4726     vport=None,
4727     tag=None,
4728     mtu=None,
4729     domain=None,
4730     nat=None,
4731     interfaces=None,
4732     addresses=None,
4733     physical_function=None,
4734     dns=None,
4735     test=False,
4736     **kwargs
4737 ):
4738     """
4739     Update a virtual network if needed.
4740     :param name: Network name.
4741     :param bridge: Bridge name.
4742     :param forward: Forward mode (bridge, router, nat).
4743         A ``None`` value creates an isolated network with no forwarding at all.
4744     :param vport: Virtualport type.
4745         The value can also be a dictionary with ``type`` and ``parameters`` keys.
4746         The ``parameters`` value is a dictionary of virtual port parameters.
4747         .. code-block:: yaml
4748           - vport:
4749               type: openvswitch
4750               parameters:
4751                 interfaceid: 09b11c53-8b5c-4eeb-8f00-d84eaa0aaa4f
4752     :param tag: Vlan tag.
4753         The value can also be a dictionary with the ``tags`` and optional ``trunk`` keys.
4754         ``trunk`` is a boolean value indicating whether to use VLAN trunking.
4755         ``tags`` is a list of dictionaries with keys ``id`` and ``nativeMode``.
4756         The ``nativeMode`` value can be one of ``tagged`` or ``untagged``.
4757         .. code-block:: yaml
4758           - tag:
4759               trunk: True
4760               tags:
4761                 - id: 42
4762                   nativeMode: untagged
4763                 - id: 47
4764     :param ipv4_config: IP v4 configuration.
4765         Dictionary describing the IP v4 setup like IP range and
4766         a possible DHCP configuration. The structure is documented
4767         in net-define-ip_.
4768     :type ipv4_config: dict or None
4769     :param ipv6_config: IP v6 configuration.
4770         Dictionary describing the IP v6 setup like IP range and
4771         a possible DHCP configuration. The structure is documented
4772         in net-define-ip_.
4773     :type ipv6_config: dict or None
4774     :param connection: libvirt connection URI, overriding defaults.
4775     :param username: username to connect with, overriding defaults.
4776     :param password: password to connect with, overriding defaults.
4777     :param mtu: size of the Maximum Transmission Unit (MTU) of the network.
4778         (default ``None``)
4779     :param domain: DNS domain name of the DHCP server.
4780         The value is a dictionary with a mandatory ``name`` property and an optional ``localOnly`` boolean one.
4781         (default ``None``)
4782         .. code-block:: yaml
4783           - domain:
4784               name: lab.acme.org
4785               localOnly: True
4786     :param nat: addresses and ports to route in NAT forward mode.
4787         The value is a dictionary with optional keys ``address`` and ``port``.
4788         Both values are a dictionary with ``start`` and ``end`` values.
4789         (default ``None``)
4790         .. code-block:: yaml
4791           - forward: nat
4792           - nat:
4793               address:
4794                 start: 1.2.3.4
4795                 end: 1.2.3.10
4796               port:
4797                 start: 500
4798                 end: 1000
4799     :param interfaces: whitespace separated list of network interfaces devices that can be used for this network.
4800         (default ``None``)
4801         .. code-block:: yaml
4802           - forward: passthrough
4803           - interfaces: "eth10 eth11 eth12"
4804     :param addresses: whitespace separated list of addresses of PCI devices that can be used for this network in `hostdev` forward mode.
4805         (default ``None``)
4806         .. code-block:: yaml
4807           - forward: hostdev
4808           - interfaces: "0000:04:00.1 0000:e3:01.2"
4809     :param physical_function: device name of the physical interface to use in ``hostdev`` forward mode.
4810         (default ``None``)
4811         .. code-block:: yaml
4812           - forward: hostdev
4813           - physical_function: "eth0"
4814     :param dns: virtual network DNS configuration.
4815         The value is a dictionary described in net-define-dns_.
4816         (default ``None``)
4817         .. code-block:: yaml
4818           - dns:
4819               forwarders:
4820                 - domain: example.com
4821                   addr: 192.168.1.1
4822                 - addr: 8.8.8.8
4823                 - domain: www.example.com
4824               txt:
4825                 example.com: "v=spf1 a -all"
4826                 _http.tcp.example.com: "name=value,paper=A4"
4827               hosts:
4828                 192.168.1.2:
4829                   - mirror.acme.lab
4830                   - test.acme.lab
4831               srvs:
4832                 - name: ldap
4833                   protocol: tcp
4834                   domain: ldapserver.example.com
4835                   target: .
4836                   port: 389
4837                   priority: 1
4838                   weight: 10
4839     .. versionadded:: 3003
4840     """
4841     conn = __get_conn(**kwargs)
4842     needs_update = False
4843     try:
4844         net = conn.networkLookupByName(name)
4845         old_xml = ElementTree.fromstring(net.XMLDesc())
4846         new_xml = ElementTree.fromstring(
4847             _gen_net_xml(
4848                 name,
4849                 bridge,
4850                 forward,
4851                 vport,
4852                 tag=tag,
4853                 ip_configs=[config for config in [ipv4_config, ipv6_config] if config],
4854                 mtu=mtu,
4855                 domain=domain,
4856                 nat=nat,
4857                 interfaces=interfaces,
4858                 addresses=addresses,
4859                 physical_function=physical_function,
4860                 dns=dns,
4861             )
4862         )
4863         elements_to_copy = ["uuid", "mac"]
4864         for to_copy in elements_to_copy:
4865             element = old_xml.find(to_copy)
4866             if element is not None:
4867                 new_xml.insert(1, element)
4868         old_xml.attrib.pop("connections", None)
4869         if old_xml.find("forward/pf") is not None:
4870             forward_node = old_xml.find("forward")
4871             address_nodes = forward_node.findall("address")
4872             for node in address_nodes:
4873                 forward_node.remove(node)
4874         default_bridge_attribs = {"stp": "on", "delay": "0"}
4875         old_bridge_node = old_xml.find("bridge")
4876         if old_bridge_node is not None:
4877             for key, value in default_bridge_attribs.items():
4878                 if old_bridge_node.get(key, None) == value:
4879                     old_bridge_node.attrib.pop(key, None)
4880             old_forward = (
4881                 old_xml.find("forward").get("mode")
4882                 if old_xml.find("forward") is not None
4883                 else None
4884             )
4885             if (
4886                 old_forward == forward
4887                 and forward in ["nat", "route", "open", None]
4888                 and bridge is None
4889                 and old_bridge_node.get("name", "").startswith("virbr")
4890             ):
4891                 old_bridge_node.attrib.pop("name", None)
4892         ipv4_nodes = [
4893             node
4894             for node in old_xml.findall("ip")
4895             if node.get("family", "ipv4") == "ipv4"
4896         ]
4897         for ip_node in ipv4_nodes:
4898             netmask = ip_node.attrib.pop("netmask", None)
4899             if netmask:
4900                 address = ipaddress.ip_network(
4901                     "{}/{}".format(ip_node.get("address"), netmask), strict=False
4902                 )
4903                 ip_node.set("prefix", str(address.prefixlen))
4904         for doc in [old_xml, new_xml]:
4905             for node in doc.findall("ip"):
4906                 if "family" not in node.keys():
4907                     node.set("family", "ipv4")
4908         _remove_empty_xml_node(xmlutil.strip_spaces(old_xml))
4909         xmlutil.strip_spaces(new_xml)
4910         needs_update = xmlutil.to_dict(old_xml, True) != xmlutil.to_dict(new_xml, True)
4911         if needs_update and not test:
4912             conn.networkDefineXML(xmlutil.element_to_str(new_xml))
4913     finally:
4914         conn.close()
4915     return needs_update
4916 def list_networks(**kwargs):
4917     """
4918     List all virtual networks.
4919     :param connection: libvirt connection URI, overriding defaults
4920     :param username: username to connect with, overriding defaults
4921     :param password: password to connect with, overriding defaults
4922     .. versionadded:: 2019.2.0
4923     CLI Example:
4924     .. code-block:: bash
4925        salt '*' virt.list_networks
4926     """
4927     conn = __get_conn(**kwargs)
4928     try:
4929         return [net.name() for net in conn.listAllNetworks()]
4930     finally:
4931         conn.close()
4932 def network_info(name=None, **kwargs):
4933     """
4934     Return information on a virtual network provided its name.
4935     :param name: virtual network name
4936     :param connection: libvirt connection URI, overriding defaults
4937     :param username: username to connect with, overriding defaults
4938     :param password: password to connect with, overriding defaults
4939     If no name is provided, return the infos for all defined virtual networks.
4940     .. versionadded:: 2019.2.0
4941     CLI Example:
4942     .. code-block:: bash
4943         salt '*' virt.network_info default
4944     """
4945     result = {}
4946     conn = __get_conn(**kwargs)
4947     def _net_get_leases(net):
4948         """
4949         Get all DHCP leases for a network
4950         """
4951         leases = net.DHCPLeases()
4952         for lease in leases:
4953             if lease["type"] == libvirt.VIR_IP_ADDR_TYPE_IPV4:
4954                 lease["type"] = "ipv4"
4955             elif lease["type"] == libvirt.VIR_IP_ADDR_TYPE_IPV6:
4956                 lease["type"] = "ipv6"
4957             else:
4958                 lease["type"] = "unknown"
4959         return leases
4960     def _net_get_bridge(net):
4961         """
4962         Get the bridge of the network or None
4963         """
4964         try:
4965             return net.bridgeName()
4966         except libvirt.libvirtError as err:
4967     try:
4968         nets <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= [
4969             net for net in conn.listAllNetworks() if name is None or net.name() == name
4970         ]
4971         result = {
4972             net.name(): {
4973                 "uuid": net.UUIDString(</b></font>),
4974                 "bridge": _net_get_bridge(net),
4975                 "autostart": net.autostart(),
4976                 "active": net.isActive(),
4977                 "persistent": net.isPersistent(),
4978                 "leases": _net_get_leases(net),
4979             }
4980             for net in nets
4981         }
4982     except libvirt.libvirtError as err:
4983         log.debug("Silenced libvirt error: %s", err)
4984     finally:
4985         conn.close()
4986     return result
4987 def network_get_xml(name, **kwargs):
4988     """
4989     Return the XML definition of a virtual network
4990     :param name: libvirt network name
4991     :param connection: libvirt connection URI, overriding defaults
4992     :param username: username to connect with, overriding defaults
4993     :param password: password to connect with, overriding defaults
4994     .. versionadded:: 3000
4995     CLI Example:
4996     .. code-block:: bash
4997         salt '*' virt.network_get_xml default
4998     """
4999     conn = __get_conn(**kwargs)
5000     try:
5001         return conn.networkLookupByName(name).XMLDesc()
5002     finally:
5003         conn.close()
5004 def network_start(name, **kwargs):
5005     """
5006     Start a defined virtual network.
5007     :param name: virtual network name
5008     :param connection: libvirt connection URI, overriding defaults
5009     :param username: username to connect with, overriding defaults
5010     :param password: password to connect with, overriding defaults
5011     .. versionadded:: 2019.2.0
5012     CLI Example:
5013     .. code-block:: bash
5014         salt '*' virt.network_start default
5015     """
5016     conn = __get_conn(**kwargs)
5017     try:
5018         net = conn.networkLookupByName(name)
5019         return not bool(net.create())
5020     finally:
5021         conn.close()
5022 def network_stop(name, **kwargs):
5023     """
5024     Stop a defined virtual network.
5025     :param name: virtual network name
5026     :param connection: libvirt connection URI, overriding defaults
5027     :param username: username to connect with, overriding defaults
5028     :param password: password to connect with, overriding defaults
5029     .. versionadded:: 2019.2.0
5030     CLI Example:
5031     .. code-block:: bash
5032         salt '*' virt.network_stop default
5033     """
5034     conn = __get_conn(**kwargs)
5035     try:
5036         net = conn.networkLookupByName(name)
5037         return not bool(net.destroy())
5038     finally:
5039         conn.close()
5040 def network_undefine(name, **kwargs):
5041     """
5042     Remove a defined virtual network. This does not stop the virtual network.
5043     :param name: virtual network name
5044     :param connection: libvirt connection URI, overriding defaults
5045     :param username: username to connect with, overriding defaults
5046     :param password: password to connect with, overriding defaults
5047     .. versionadded:: 2019.2.0
5048     CLI Example:
5049     .. code-block:: bash
5050         salt '*' virt.network_undefine default
5051     """
5052     conn = __get_conn(**kwargs)
5053     try:
5054         net = conn.networkLookupByName(name)
5055         return not bool(net.undefine())
5056     finally:
5057         conn.close()
5058 def network_set_autostart(name, state="on", **kwargs):
5059     """
5060     Set the autostart flag on a virtual network so that the network
5061     will start with the host system on reboot.
5062     :param name: virtual network name
5063     :param state: 'on' to auto start the network, anything else to mark the
5064                   virtual network not to be started when the host boots
5065     :param connection: libvirt connection URI, overriding defaults
5066     :param username: username to connect with, overriding defaults
5067     :param password: password to connect with, overriding defaults
5068     .. versionadded:: 2019.2.0
5069     CLI Example:
5070     .. code-block:: bash
5071         salt "*" virt.network_set_autostart &lt;pool&gt; &lt;on | off&gt;
5072     """
5073     conn = __get_conn(**kwargs)
5074     try:
5075         net = conn.networkLookupByName(name)
5076         return not bool(net.setAutostart(1 if state == "on" else 0))
5077     finally:
5078         conn.close()
5079 def _parse_pools_caps(doc):
5080     """
5081     Parse libvirt pool capabilities XML
5082     """
5083     def _parse_pool_caps(pool):
5084         pool_caps = {
5085             "name": pool.get("type"),
5086             "supported": pool.get("supported", "no") == "yes",
5087         }
5088         for option_kind in ["pool", "vol"]:
5089             options = {}
5090             default_format_node = pool.find(
5091                 "{}Options/defaultFormat".format(option_kind)
5092             )
5093             if default_format_node is not None:
5094                 options["default_format"] = default_format_node.get("type")
5095             options_enums = {
5096                 enum.get("name"): [value.text for value in enum.findall("value")]
5097                 for enum in pool.findall("{}Options/enum".format(option_kind))
5098             }
5099             if options_enums:
5100                 options.update(options_enums)
5101             if options:
5102                 if "options" not in pool_caps:
5103                     pool_caps["options"] = {}
5104                 kind = option_kind if option_kind != "vol" else "volume"
5105                 pool_caps["options"][kind] = options
5106         return pool_caps
5107     return [_parse_pool_caps(pool) for pool in doc.findall("pool")]
5108 def _pool_capabilities(conn):
5109     """
5110     Return the hypervisor connection storage pool capabilities.
5111     :param conn: opened libvirt connection to use
5112     """
5113     has_pool_capabilities = bool(getattr(conn, "getStoragePoolCapabilities", None))
5114     if has_pool_capabilities:
5115         caps = ElementTree.fromstring(conn.getStoragePoolCapabilities())
5116         pool_types = _parse_pools_caps(caps)
5117     else:
5118         all_hypervisors = ["xen", "kvm", "bhyve"]
5119         images_formats = [
5120             "none",
5121             "raw",
5122             "dir",
5123             "bochs",
5124             "cloop",
5125             "dmg",
5126             "iso",
5127             "vpc",
5128             "vdi",
5129             "fat",
5130             "vhd",
5131             "ploop",
5132             "cow",
5133             "qcow",
5134             "qcow2",
5135             "qed",
5136             "vmdk",
5137         ]
5138         common_drivers = [
5139             {
5140                 "name": "fs",
5141                 "default_source_format": "auto",
5142                 "source_formats": [
5143                     "auto",
5144                     "ext2",
5145                     "ext3",
5146                     "ext4",
5147                     "ufs",
5148                     "iso9660",
5149                     "udf",
5150                     "gfs",
5151                     "gfs2",
5152                     "vfat",
5153                     "hfs+",
5154                     "xfs",
5155                     "ocfs2",
5156                 ],
5157                 "default_target_format": "raw",
5158                 "target_formats": images_formats,
5159             },
5160             {
5161                 "name": "dir",
5162                 "default_target_format": "raw",
5163                 "target_formats": images_formats,
5164             },
5165             {"name": "iscsi"},
5166             {"name": "scsi"},
5167             {
5168                 "name": "logical",
5169                 "default_source_format": "lvm2",
5170                 "source_formats": ["unknown", "lvm2"],
5171             },
5172             {
5173                 "name": "netfs",
5174                 "default_source_format": "auto",
5175                 "source_formats": ["auto", "nfs", "glusterfs", "cifs"],
5176                 "default_target_format": "raw",
5177                 "target_formats": images_formats,
5178             },
5179             {
5180                 "name": "disk",
5181                 "default_source_format": "unknown",
5182                 "source_formats": [
5183                     "unknown",
5184                     "dos",
5185                     "dvh",
5186                     "gpt",
5187                     "mac",
5188                     "bsd",
5189                     "pc98",
5190                     "sun",
5191                     "lvm2",
5192                 ],
5193                 "default_target_format": "none",
5194                 "target_formats": [
5195                     "none",
5196                     "linux",
5197                     "fat16",
5198                     "fat32",
5199                     "linux-swap",
5200                     "linux-lvm",
5201                     "linux-raid",
5202                     "extended",
5203                 ],
5204             },
5205             {"name": "mpath"},
5206             {"name": "rbd", "default_target_format": "raw", "target_formats": []},
5207             {
5208                 "name": "sheepdog",
5209                 "version": 10000,
5210                 "hypervisors": ["kvm"],
5211                 "default_target_format": "raw",
5212                 "target_formats": images_formats,
5213             },
5214             {
5215                 "name": "gluster",
5216                 "version": 1002000,
5217                 "hypervisors": ["kvm"],
5218                 "default_target_format": "raw",
5219                 "target_formats": images_formats,
5220             },
5221             {"name": "zfs", "version": 1002008, "hypervisors": ["bhyve"]},
5222             {
5223                 "name": "iscsi-direct",
5224                 "version": 4007000,
5225                 "hypervisors": ["kvm", "xen"],
5226             },
5227         ]
5228         libvirt_version = conn.getLibVersion()
5229         hypervisor = get_hypervisor()
5230         def _get_backend_output(backend):
5231             output = {
5232                 "name": backend["name"],
5233                 "supported": (
5234                     not backend.get("version") or libvirt_version &gt;= backend["version"]
5235                 )
5236                 and hypervisor in backend.get("hypervisors", all_hypervisors),
5237                 "options": {
5238                     "pool": {
5239                         "default_format": backend.get("default_source_format"),
5240                         "sourceFormatType": backend.get("source_formats"),
5241                     },
5242                     "volume": {
5243                         "default_format": backend.get("default_target_format"),
5244                         "targetFormatType": backend.get("target_formats"),
5245                     },
5246                 },
5247             }
5248             for option_kind in ["pool", "volume"]:
5249                 if not [
5250                     value
5251                     for value in output["options"][option_kind].values()
5252                     if value is not None
5253                 ]:
5254                     del output["options"][option_kind]
5255             if not output["options"]:
5256                 del output["options"]
5257             return output
5258         pool_types = [_get_backend_output(backend) for backend in common_drivers]
5259     return {
5260         "computed": not has_pool_capabilities,
5261         "pool_types": pool_types,
5262     }
5263 def pool_capabilities(**kwargs):
5264     """
5265     Return the hypervisor connection storage pool capabilities.
5266     The returned data are either directly extracted from libvirt or computed.
5267     In the latter case some pool types could be listed as supported while they
5268     are not. To distinguish between the two cases, check the value of the ``computed`` property.
5269     :param connection: libvirt connection URI, overriding defaults
5270     :param username: username to connect with, overriding defaults
5271     :param password: password to connect with, overriding defaults
5272     .. versionadded:: 3000
5273     CLI Example:
5274     .. code-block:: bash
5275         salt '*' virt.pool_capabilities
5276     """
5277     try:
5278         conn = __get_conn(**kwargs)
5279         return _pool_capabilities(conn)
5280     finally:
5281         conn.close()
5282 def pool_define(
5283     name,
5284     ptype,
5285     target=None,
5286     permissions=None,
5287     source_devices=None,
5288     source_dir=None,
5289     source_initiator=None,
5290     source_adapter=None,
5291     source_hosts=None,
5292     source_auth=None,
5293     source_name=None,
5294     source_format=None,
5295     transient=False,
5296     start=True,  # pylint: disable=redefined-outer-name
5297     **kwargs
5298 ):
5299     """
5300     Create libvirt pool.
5301     :param name: Pool name
5302     :param ptype:
5303         Pool type. See `libvirt documentation &lt;https://libvirt.org/storage.html&gt;`_  for the
5304         possible values.
5305     :param target: Pool full path target
5306     :param permissions:
5307         Permissions to set on the target folder. This is mostly used for filesystem-based
5308         pool types. See :ref:`pool-define-permissions` for more details on this structure.
5309     :param source_devices:
5310         List of source devices for pools backed by physical devices. (Default: ``None``)
5311         Each item in the list is a dictionary with ``path`` and optionally ``part_separator``
5312         keys. The path is the qualified name for iSCSI devices.
5313         Report to `this libvirt page &lt;https://libvirt.org/formatstorage.html#StoragePool&gt;`_
5314         for more information on the use of ``part_separator``
5315     :param source_dir:
5316         Path to the source directory for pools of type ``dir``, ``netfs`` or ``gluster``.
5317         (Default: ``None``)
5318     :param source_initiator:
5319         Initiator IQN for libiscsi-direct pool types. (Default: ``None``)
5320         .. versionadded:: 3000
5321     :param source_adapter:
5322         SCSI source definition. The value is a dictionary with ``type``, ``name``, ``parent``,
5323         ``managed``, ``parent_wwnn``, ``parent_wwpn``, ``parent_fabric_wwn``, ``wwnn``, ``wwpn``
5324         and ``parent_address`` keys.
5325         The ``parent_address`` value is a dictionary with ``unique_id`` and ``address`` keys.
5326         The address represents a PCI address and is itself a dictionary with ``domain``, ``bus``,
5327         ``slot`` and ``function`` properties.
5328         Report to `this libvirt page &lt;https://libvirt.org/formatstorage.html#StoragePool&gt;`_
5329         for the meaning and possible values of these properties.
5330     :param source_hosts:
5331         List of source for pools backed by storage from remote servers. Each item is the hostname
5332         optionally followed by the port separated by a colon. (Default: ``None``)
5333     :param source_auth:
5334         Source authentication details. (Default: ``None``)
5335         The value is a dictionary with ``type``, ``username`` and ``secret`` keys. The type
5336         can be one of ``ceph`` for Ceph RBD or ``chap`` for iSCSI sources.
5337         The ``secret`` value links to a libvirt secret object. It is a dictionary with
5338         ``type`` and ``value`` keys. The type value can be either ``uuid`` or ``usage``.
5339         Examples:
5340         .. code-block:: python
5341             source_auth={
5342                 'type': 'ceph',
5343                 'username': 'admin',
5344                 'secret': {
5345                     'type': 'uuid',
5346                     'value': '2ec115d7-3a88-3ceb-bc12-0ac909a6fd87'
5347                 }
5348             }
5349         .. code-block:: python
5350             source_auth={
5351                 'type': 'chap',
5352                 'username': 'myname',
5353                 'secret': {
5354                     'type': 'usage',
5355                     'value': 'mycluster_myname'
5356                 }
5357             }
5358         Since 3000, instead the source authentication can only contain ``username``
5359         and ``password`` properties. In this case the libvirt secret will be defined and used.
5360         For Ceph authentications a base64 encoded key is expected.
5361     :param source_name:
5362         Identifier of name-based sources.
5363     :param source_format:
5364         String representing the source format. The possible values are depending on the
5365         source type. See `libvirt documentation &lt;https://libvirt.org/storage.html&gt;`_ for
5366         the possible values.
5367     :param start: Pool start (default True)
5368     :param transient:
5369         When ``True``, the pool will be automatically undefined after being stopped.
5370         Note that a transient pool will force ``start`` to ``True``. (Default: ``False``)
5371     :param connection: libvirt connection URI, overriding defaults
5372     :param username: username to connect with, overriding defaults
5373     :param password: password to connect with, overriding defaults
5374     .. _pool-define-permissions:
5375     .. rubric:: Permissions definition
5376     The permissions are described by a dictionary containing the following keys:
5377     mode
5378         The octal representation of the permissions. (Default: `0711`)
5379     owner
5380         the numeric user ID of the owner. (Default: from the parent folder)
5381     group
5382         the numeric ID of the group. (Default: from the parent folder)
5383     label
5384         the SELinux label. (Default: `None`)
5385     .. rubric:: CLI Example:
5386     Local folder pool:
5387     .. code-block:: bash
5388         salt '*' virt.pool_define somepool dir target=/srv/mypool \
5389                                   permissions="{'mode': '0744' 'ower': 107, 'group': 107 }"
5390     CIFS backed pool:
5391     .. code-block:: bash
5392         salt '*' virt.pool_define myshare netfs source_format=cifs \
5393                                   source_dir=samba_share source_hosts="['example.com']" target=/mnt/cifs
5394     .. versionadded:: 2019.2.0
5395     """
5396     conn = __get_conn(**kwargs)
5397     auth = _pool_set_secret(conn, ptype, name, source_auth)
5398     pool_xml = _gen_pool_xml(
5399         name,
5400         ptype,
5401         target,
5402         permissions=permissions,
5403         source_devices=source_devices,
5404         source_dir=source_dir,
5405         source_adapter=source_adapter,
5406         source_hosts=source_hosts,
5407         source_auth=auth,
5408         source_name=source_name,
5409         source_format=source_format,
5410         source_initiator=source_initiator,
5411     )
5412     try:
5413         if transient:
5414             pool = conn.storagePoolCreateXML(pool_xml)
5415         else:
5416             pool = conn.storagePoolDefineXML(pool_xml)
5417             if start:
5418                 pool.create()
5419     except libvirt.libvirtError as err:
5420         raise err  # a real error we should report upwards
5421     finally:
5422         conn.close()
5423     return True
5424 def _pool_set_secret(
5425     conn, pool_type, pool_name, source_auth, uuid=None, usage=None, test=False
5426 ):
5427     secret_types = {"rbd": "ceph", "iscsi": "chap", "iscsi-direct": "chap"}
5428     secret_type = secret_types.get(pool_type)
5429     auth = source_auth
5430     if source_auth and "username" in source_auth and "password" in source_auth:
5431         if secret_type:
5432             secret = None
5433             try:
5434                 if usage:
5435                     usage_type = (
5436                         libvirt.VIR_SECRET_USAGE_TYPE_CEPH
5437                         if secret_type == "ceph"
5438                         else libvirt.VIR_SECRET_USAGE_TYPE_ISCSI
5439                     )
5440                     secret = conn.secretLookupByUsage(usage_type, usage)
5441                 elif uuid:
5442                     secret = conn.secretLookupByUUIDString(uuid)
5443             except libvirt.libvirtError as err:
5444                 log.info("Secret not found: %s", err.get_error_message())
5445             if not secret:
5446                 description = "Passphrase for {} pool created by Salt".format(pool_name)
5447                 if not usage:
5448                     usage = "pool_{}".format(pool_name)
5449                 secret_xml = _gen_secret_xml(secret_type, usage, description)
5450                 if not test:
5451                     secret = conn.secretDefineXML(secret_xml)
5452             password = auth["password"]
5453             if pool_type == "rbd":
5454                 password = base64.b64decode(salt.utils.stringutils.to_bytes(password))
5455             if not test:
5456                 secret.setValue(password)
5457             auth["type"] = secret_type
5458             auth["secret"] = {
5459                 "type": "uuid" if uuid else "usage",
5460                 "value": uuid if uuid else usage,
5461             }
5462     return auth
5463 def pool_update(
5464     name,
5465     ptype,
5466     target=None,
5467     permissions=None,
5468     source_devices=None,
5469     source_dir=None,
5470     source_initiator=None,
5471     source_adapter=None,
5472     source_hosts=None,
5473     source_auth=None,
5474     source_name=None,
5475     source_format=None,
5476     test=False,
5477     **kwargs
5478 ):
5479     """
5480     Update a libvirt storage pool if needed.
5481     If called with test=True, this is also reporting whether an update would be performed.
5482     :param name: Pool name
5483     :param ptype:
5484         Pool type. See `libvirt documentation &lt;https://libvirt.org/storage.html&gt;`_  for the
5485         possible values.
5486     :param target: Pool full path target
5487     :param permissions:
5488         Permissions to set on the target folder. This is mostly used for filesystem-based
5489         pool types. See :ref:`pool-define-permissions` for more details on this structure.
5490     :param source_devices:
5491         List of source devices for pools backed by physical devices. (Default: ``None``)
5492         Each item in the list is a dictionary with ``path`` and optionally ``part_separator``
5493         keys. The path is the qualified name for iSCSI devices.
5494         Report to `this libvirt page &lt;https://libvirt.org/formatstorage.html#StoragePool&gt;`_
5495         for more information on the use of ``part_separator``
5496     :param source_dir:
5497         Path to the source directory for pools of type ``dir``, ``netfs`` or ``gluster``.
5498         (Default: ``None``)
5499     :param source_initiator:
5500         Initiator IQN for libiscsi-direct pool types. (Default: ``None``)
5501         .. versionadded:: 3000
5502     :param source_adapter:
5503         SCSI source definition. The value is a dictionary with ``type``, ``name``, ``parent``,
5504         ``managed``, ``parent_wwnn``, ``parent_wwpn``, ``parent_fabric_wwn``, ``wwnn``, ``wwpn``
5505         and ``parent_address`` keys.
5506         The ``parent_address`` value is a dictionary with ``unique_id`` and ``address`` keys.
5507         The address represents a PCI address and is itself a dictionary with ``domain``, ``bus``,
5508         ``slot`` and ``function`` properties.
5509         Report to `this libvirt page &lt;https://libvirt.org/formatstorage.html#StoragePool&gt;`_
5510         for the meaning and possible values of these properties.
5511     :param source_hosts:
5512         List of source for pools backed by storage from remote servers. Each item is the hostname
5513         optionally followed by the port separated by a colon. (Default: ``None``)
5514     :param source_auth:
5515         Source authentication details. (Default: ``None``)
5516         The value is a dictionary with ``type``, ``username`` and ``secret`` keys. The type
5517         can be one of ``ceph`` for Ceph RBD or ``chap`` for iSCSI sources.
5518         The ``secret`` value links to a libvirt secret object. It is a dictionary with
5519         ``type`` and ``value`` keys. The type value can be either ``uuid`` or ``usage``.
5520         Examples:
5521         .. code-block:: python
5522             source_auth={
5523                 'type': 'ceph',
5524                 'username': 'admin',
5525                 'secret': {
5526                     'type': 'uuid',
5527                     'uuid': '2ec115d7-3a88-3ceb-bc12-0ac909a6fd87'
5528                 }
5529             }
5530         .. code-block:: python
5531             source_auth={
5532                 'type': 'chap',
5533                 'username': 'myname',
5534                 'secret': {
5535                     'type': 'usage',
5536                     'uuid': 'mycluster_myname'
5537                 }
5538             }
5539         Since 3000, instead the source authentication can only contain ``username``
5540         and ``password`` properties. In this case the libvirt secret will be defined and used.
5541         For Ceph authentications a base64 encoded key is expected.
5542     :param source_name:
5543         Identifier of name-based sources.
5544     :param source_format:
5545         String representing the source format. The possible values are depending on the
5546         source type. See `libvirt documentation &lt;https://libvirt.org/storage.html&gt;`_ for
5547         the possible values.
5548     :param test: run in dry-run mode if set to True
5549     :param connection: libvirt connection URI, overriding defaults
5550     :param username: username to connect with, overriding defaults
5551     :param password: password to connect with, overriding defaults
5552     .. rubric:: Example:
5553     Local folder pool:
5554     .. code-block:: bash
5555         salt '*' virt.pool_update somepool dir target=/srv/mypool \
5556                                   permissions="{'mode': '0744' 'ower': 107, 'group': 107 }"
5557     CIFS backed pool:
5558     .. code-block:: bash
5559         salt '*' virt.pool_update myshare netfs source_format=cifs \
5560                                   source_dir=samba_share source_hosts="['example.com']" target=/mnt/cifs
5561     .. versionadded:: 3000
5562     """
5563     conn = __get_conn(**kwargs)
5564     needs_update = False
5565     try:
5566         pool = conn.storagePoolLookupByName(name)
5567         old_xml = ElementTree.fromstring(pool.XMLDesc())
5568         secret_node = old_xml.find("source/auth/secret")
5569         usage = secret_node.get("usage") if secret_node is not None else None
5570         uuid = secret_node.get("uuid") if secret_node is not None else None
5571         auth = _pool_set_secret(
5572             conn, ptype, name, source_auth, uuid=uuid, usage=usage, test=test
5573         )
5574         new_xml = ElementTree.fromstring(
5575             _gen_pool_xml(
5576                 name,
5577                 ptype,
5578                 target,
5579                 permissions=permissions,
5580                 source_devices=source_devices,
5581                 source_dir=source_dir,
5582                 source_initiator=source_initiator,
5583                 source_adapter=source_adapter,
5584                 source_hosts=source_hosts,
5585                 source_auth=auth,
5586                 source_name=source_name,
5587                 source_format=source_format,
5588             )
5589         )
5590         elements_to_copy = ["available", "allocation", "capacity", "uuid"]
5591         for to_copy in elements_to_copy:
5592             element = old_xml.find(to_copy)
5593             new_xml.insert(1, element)
5594         _remove_empty_xml_node(xmlutil.strip_spaces(old_xml))
5595         xmlutil.strip_spaces(new_xml)
5596         needs_update = xmlutil.to_dict(old_xml, True) != xmlutil.to_dict(new_xml, True)
5597         if needs_update and not test:
5598             conn.storagePoolDefineXML(xmlutil.element_to_str(new_xml))
5599     finally:
5600         conn.close()
5601     return needs_update
5602 def list_pools(**kwargs):
5603     """
5604     List all storage pools.
5605     :param connection: libvirt connection URI, overriding defaults
5606     :param username: username to connect with, overriding defaults
5607     :param password: password to connect with, overriding defaults
5608     .. versionadded:: 2019.2.0
5609     CLI Example:
5610     .. code-block:: bash
5611         salt '*' virt.list_pools
5612     """
5613     conn = __get_conn(**kwargs)
5614     try:
5615         return [pool.name() for pool in conn.listAllStoragePools()]
5616     finally:
5617         conn.close()
5618 def pool_info(name=None, **kwargs):
5619     """
5620     Return information on a storage pool provided its name.
5621     :param name: libvirt storage pool name
5622     :param connection: libvirt connection URI, overriding defaults
5623     :param username: username to connect with, overriding defaults
5624     :param password: password to connect with, overriding defaults
5625     If no name is provided, return the infos for all defined storage pools.
5626     .. versionadded:: 2019.2.0
5627     CLI Example:
5628     .. code-block:: bash
5629         salt '*' virt.pool_info default
5630     """
5631     result = {}
5632     conn = __get_conn(**kwargs)
5633     def _pool_extract_infos(pool):
5634         """
5635         Format the pool info dictionary
5636         :param pool: the libvirt pool object
5637         """
5638         states = ["inactive", "building", "running", "degraded", "inaccessible"]
5639         infos = pool.info()
5640         state = states[infos[0]] if infos[0] &lt; len(states) else "unknown"
5641         desc = ElementTree.fromstring(pool.XMLDesc())
5642         path_node = desc.find("target/path")
5643         return {
5644             "uuid": pool.UUIDString(),
5645             "state": state,
5646             "capacity": infos[1],
5647             "allocation": infos[2],
5648             "free": infos[3],
5649             "autostart": pool.autostart(),
5650             "persistent": pool.isPersistent(),
5651             "target_path": path_node.text if path_node is not None else None,
5652             "type": desc.get("type"),
5653         }
5654     try:
5655         pools = [
5656             pool
5657             for pool in conn.listAllStoragePools()
5658             if name is None or pool.name() == name
5659         ]
5660         result = {pool.name(): _pool_extract_infos(pool) for pool in pools}
5661     except libvirt.libvirtError as err:
5662         log.debug("Silenced libvirt error: %s", err)
5663     finally:
5664         conn.close()
5665     return result
5666 def pool_get_xml(name, **kwargs):
5667     """
5668     Return the XML definition of a virtual storage pool
5669     :param name: libvirt storage pool name
5670     :param connection: libvirt connection URI, overriding defaults
5671     :param username: username to connect with, overriding defaults
5672     :param password: password to connect with, overriding defaults
5673     .. versionadded:: 3000
5674     CLI Example:
5675     .. code-block:: bash
5676         salt '*' virt.pool_get_xml default
5677     """
5678     conn = __get_conn(**kwargs)
5679     try:
5680         return conn.storagePoolLookupByName(name).XMLDesc()
5681     finally:
5682         conn.close()
5683 def pool_start(name, **kwargs):
5684     """
5685     Start a defined libvirt storage pool.
5686     :param name: libvirt storage pool name
5687     :param connection: libvirt connection URI, overriding defaults
5688     :param username: username to connect with, overriding defaults
5689     :param password: password to connect with, overriding defaults
5690     .. versionadded:: 2019.2.0
5691     CLI Example:
5692     .. code-block:: bash
5693         salt '*' virt.pool_start default
5694     """
5695     conn = __get_conn(**kwargs)
5696     try:
5697         pool = conn.storagePoolLookupByName(name)
5698         return not bool(pool.create())
5699     finally:
5700         conn.close()
5701 def pool_build(name, **kwargs):
5702     """
5703     Build a defined libvirt storage pool.
5704     :param name: libvirt storage pool name
5705     :param connection: libvirt connection URI, overriding defaults
5706     :param username: username to connect with, overriding defaults
5707     :param password: password to connect with, overriding defaults
5708     .. versionadded:: 2019.2.0
5709     CLI Example:
5710     .. code-block:: bash
5711         salt '*' virt.pool_build default
5712     """
5713     conn = __get_conn(**kwargs)
5714     try:
5715         pool = conn.storagePoolLookupByName(name)
5716         return not bool(pool.build())
5717     finally:
5718         conn.close()
5719 def pool_stop(name, **kwargs):
5720     """
5721     Stop a defined libvirt storage pool.
5722     :param name: libvirt storage pool name
5723     :param connection: libvirt connection URI, overriding defaults
5724     :param username: username to connect with, overriding defaults
5725     :param password: password to connect with, overriding defaults
5726     .. versionadded:: 2019.2.0
5727     CLI Example:
5728     .. code-block:: bash
5729         salt '*' virt.pool_stop default
5730     """
5731     conn = __get_conn(**kwargs)
5732     try:
5733         pool = conn.storagePoolLookupByName(name)
5734         return not bool(pool.destroy())
5735     finally:
5736         conn.close()
5737 def pool_undefine(name, **kwargs):
5738     """
5739     Remove a defined libvirt storage pool. The pool needs to be stopped before calling.
5740     :param name: libvirt storage pool name
5741     :param connection: libvirt connection URI, overriding defaults
5742     :param username: username to connect with, overriding defaults
5743     :param password: password to connect with, overriding defaults
5744     .. versionadded:: 2019.2.0
5745     CLI Example:
5746     .. code-block:: bash
5747         salt '*' virt.pool_undefine default
5748     """
5749     conn = __get_conn(**kwargs)
5750     try:
5751         pool = conn.storagePoolLookupByName(name)
5752         desc = ElementTree.fromstring(pool.XMLDesc())
5753         auth_node = desc.find("source/auth")
5754         if auth_node is not None:
5755             auth_types = {
5756                 "ceph": libvirt.VIR_SECRET_USAGE_TYPE_CEPH,
5757                 "iscsi": libvirt.VIR_SECRET_USAGE_TYPE_ISCSI,
5758             }
5759             secret_type = auth_types[auth_node.get("type")]
5760             secret_usage = auth_node.find("secret").get("usage")
5761             if secret_type and "pool_{}".format(name) == secret_usage:
5762                 secret = conn.secretLookupByUsage(secret_type, secret_usage)
5763                 secret.undefine()
5764         return not bool(pool.undefine())
5765     finally:
5766         conn.close()
5767 def pool_delete(name, **kwargs):
5768     """
5769     Delete the resources of a defined libvirt storage pool.
5770     :param name: libvirt storage pool name
5771     :param connection: libvirt connection URI, overriding defaults
5772     :param username: username to connect with, overriding defaults
5773     :param password: password to connect with, overriding defaults
5774     .. versionadded:: 2019.2.0
5775     CLI Example:
5776     .. code-block:: bash
5777         salt '*' virt.pool_delete default
5778     """
5779     conn = __get_conn(**kwargs)
5780     try:
5781         pool = conn.storagePoolLookupByName(name)
5782         return not bool(pool.delete(libvirt.VIR_STORAGE_POOL_DELETE_NORMAL))
5783     finally:
5784         conn.close()
5785 def pool_refresh(name, **kwargs):
5786     """
5787     Refresh a defined libvirt storage pool.
5788     :param name: libvirt storage pool name
5789     :param connection: libvirt connection URI, overriding defaults
5790     :param username: username to connect with, overriding defaults
5791     :param password: password to connect with, overriding defaults
5792     .. versionadded:: 2019.2.0
5793     CLI Example:
5794     .. code-block:: bash
5795         salt '*' virt.pool_refresh default
5796     """
5797     conn = __get_conn(**kwargs)
5798     try:
5799         pool = conn.storagePoolLookupByName(name)
5800         return not bool(pool.refresh())
5801     finally:
5802         conn.close()
5803 def pool_set_autostart(name, state="on", **kwargs):
5804     """
5805     Set the autostart flag on a libvirt storage pool so that the storage pool
5806     will start with the host system on reboot.
5807     :param name: libvirt storage pool name
5808     :param state: 'on' to auto start the pool, anything else to mark the
5809                   pool not to be started when the host boots
5810     :param connection: libvirt connection URI, overriding defaults
5811     :param username: username to connect with, overriding defaults
5812     :param password: password to connect with, overriding defaults
5813     .. versionadded:: 2019.2.0
5814     CLI Example:
5815     .. code-block:: bash
5816         salt "*" virt.pool_set_autostart &lt;pool&gt; &lt;on | off&gt;
5817     """
5818     conn = __get_conn(**kwargs)
5819     try:
5820         pool = conn.storagePoolLookupByName(name)
5821         return not bool(pool.setAutostart(1 if state == "on" else 0))
5822     finally:
5823         conn.close()
5824 def pool_list_volumes(name, **kwargs):
5825     """
5826     List the volumes contained in a defined libvirt storage pool.
5827     :param name: libvirt storage pool name
5828     :param connection: libvirt connection URI, overriding defaults
5829     :param username: username to connect with, overriding defaults
5830     :param password: password to connect with, overriding defaults
5831     .. versionadded:: 2019.2.0
5832     CLI Example:
5833     .. code-block:: bash
5834         salt "*" virt.pool_list_volumes &lt;pool&gt;
5835     """
5836     conn = __get_conn(**kwargs)
5837     try:
5838         pool = conn.storagePoolLookupByName(name)
5839         return pool.listVolumes()
5840     finally:
5841         conn.close()
5842 def _get_storage_vol(conn, pool, vol):
5843     """
5844     Helper function getting a storage volume. Will throw a libvirtError
5845     if the pool or the volume couldn't be found.
5846     :param conn: libvirt connection object to use
5847     :param pool: pool name
5848     :param vol: volume name
5849     """
5850     pool_obj = conn.storagePoolLookupByName(pool)
5851     return pool_obj.storageVolLookupByName(vol)
5852 def _is_valid_volume(vol):
5853     """
5854     Checks whether a volume is valid for further use since those may have disappeared since
5855     the last pool refresh.
5856     """
5857     try:
5858         def discarder(ctxt, error):  # pylint: disable=unused-argument
5859             log.debug("Ignore libvirt error: %s", error[2])
5860         libvirt.registerErrorHandler(discarder, None)
5861         vol.info()
5862         libvirt.registerErrorHandler(None, None)
5863         return True
5864     except libvirt.libvirtError as err:
5865         return False
5866 def _get_all_volumes_paths(conn):
5867     """
5868     Extract the path, name, pool name and backing stores path of all volumes.
5869     :param conn: libvirt connection to use
5870     """
5871     pools = [
5872         pool
5873         for pool in conn.listAllStoragePools()
5874         if pool.info()[0] == libvirt.VIR_STORAGE_POOL_RUNNING
5875     ]
5876     volumes = {}
5877     for pool in pools:
5878         pool_volumes = {
5879             volume.path(): {
5880                 "pool": pool.name(),
5881                 "name": volume.name(),
5882                 "backing_stores": [
5883                     path.text
5884                     for path in ElementTree.fromstring(volume.XMLDesc()).findall(
5885                         ".//backingStore/path"
5886                     )
5887                 ],
5888             }
5889             for volume in pool.listAllVolumes()
5890             if _is_valid_volume(volume)
5891         }
5892         volumes.update(pool_volumes)
5893     return volumes
5894 def volume_infos(pool=None, volume=None, **kwargs):
5895     """
5896     Provide details on a storage volume. If no volume name is provided, the infos
5897     all the volumes contained in the pool are provided. If no pool is provided,
5898     the infos of the volumes of all pools are output.
5899     :param pool: libvirt storage pool name (default: ``None``)
5900     :param volume: name of the volume to get infos from (default: ``None``)
5901     :param connection: libvirt connection URI, overriding defaults
5902     :param username: username to connect with, overriding defaults
5903     :param password: password to connect with, overriding defaults
5904     .. versionadded:: 3000
5905     CLI Example:
5906     .. code-block:: bash
5907         salt "*" virt.volume_infos &lt;pool&gt; &lt;volume&gt;
5908     """
5909     result = {}
5910     conn = __get_conn(**kwargs)
5911     try:
5912         backing_stores = _get_all_volumes_paths(conn)
5913         try:
5914             domains = _get_domain(conn)
5915             domains_list = domains if isinstance(domains, list) else [domains]
5916         except CommandExecutionError:
5917             domains_list = []
5918         disks = {
5919             domain.name(): {
5920                 node.get("file")
5921                 for node in ElementTree.fromstring(domain.XMLDesc(0)).findall(
5922                     ".//disk/source/[@file]"
5923                 )
5924             }
5925             for domain in domains_list
5926         }
5927         def _volume_extract_infos(vol):
5928             """
5929             Format the volume info dictionary
5930             :param vol: the libvirt storage volume object.
5931             """
5932             types = ["file", "block", "dir", "network", "netdir", "ploop"]
5933             infos = vol.info()
5934             vol_xml = ElementTree.fromstring(vol.XMLDesc())
5935             backing_store_path = vol_xml.find("./backingStore/path")
5936             backing_store_format = vol_xml.find("./backingStore/format")
5937             backing_store = None
5938             if backing_store_path is not None:
5939                 backing_store = {
5940                     "path": backing_store_path.text,
5941                     "format": backing_store_format.get("type")
5942                     if backing_store_format is not None
5943                     else None,
5944                 }
5945             format_node = vol_xml.find("./target/format")
5946             used_by = []
5947             if vol.path():
5948                 as_backing_store = {
5949                     path
5950                     for (path, volume) in backing_stores.items()
5951                     if vol.path() in volume.get("backing_stores")
5952                 }
5953                 used_by = [
5954                     vm_name
5955                     for (vm_name, vm_disks) in disks.items()
5956                     if vm_disks &amp; as_backing_store or vol.path() in vm_disks
5957                 ]
5958             return {
5959                 "type": types[infos[0]] if infos[0] &lt; len(types) else "unknown",
5960                 "key": vol.key(),
5961                 "path": vol.path(),
5962                 "capacity": infos[1],
5963                 "allocation": infos[2],
5964                 "used_by": used_by,
5965                 "backing_store": backing_store,
5966                 "format": format_node.get("type") if format_node is not None else None,
5967             }
5968         pools = [
5969             obj
5970             for obj in conn.listAllStoragePools()
5971             if (pool is None or obj.name() == pool)
5972             and obj.info()[0] == libvirt.VIR_STORAGE_POOL_RUNNING
5973         ]
5974         vols = {
5975             pool_obj.name(): {
5976                 vol.name(): _volume_extract_infos(vol)
5977                 for vol in pool_obj.listAllVolumes()
5978                 if (volume is None or vol.name() == volume) and _is_valid_volume(vol)
5979             }
5980             for pool_obj in pools
5981         }
5982         return {pool_name: volumes for (pool_name, volumes) in vols.items() if volumes}
5983     except libvirt.libvirtError as err:
5984         log.debug("Silenced libvirt error: %s", err)
5985     finally:
5986         conn.close()
5987     return result
5988 def volume_delete(pool, volume, **kwargs):
5989     """
5990     Delete a libvirt managed volume.
5991     :param pool: libvirt storage pool name
5992     :param volume: name of the volume to delete
5993     :param connection: libvirt connection URI, overriding defaults
5994     :param username: username to connect with, overriding defaults
5995     :param password: password to connect with, overriding defaults
5996     .. versionadded:: 3000
5997     CLI Example:
5998     .. code-block:: bash
5999         salt "*" virt.volume_delete &lt;pool&gt; &lt;volume&gt;
6000     """
6001     conn = __get_conn(**kwargs)
6002     try:
6003         vol = _get_storage_vol(conn, pool, volume)
6004         return not bool(vol.delete())
6005     finally:
6006         conn.close()
6007 def volume_define(
6008     pool,
6009     name,
6010     size,
6011     allocation=0,
6012     format=None,
6013     type=None,
6014     permissions=None,
6015     backing_store=None,
6016     nocow=False,
6017     **kwargs
6018 ):
6019     """
6020     Create libvirt volume.
6021     :param pool: name of the pool to create the volume in
6022     :param name: name of the volume to define
6023     :param size: capacity of the volume to define in MiB
6024     :param allocation: allocated size of the volume in MiB. Defaults to 0.
6025     :param format:
6026         volume format. The allowed values are depending on the pool type.
6027         Check the virt.pool_capabilities output for the possible values and the default.
6028     :param type:
6029         type of the volume. One of file, block, dir, network, netdiri, ploop or None.
6030         By default, the type is guessed by libvirt from the pool type.
6031     :param permissions:
6032         Permissions to set on the target folder. This is mostly used for filesystem-based
6033         pool types. See :ref:`pool-define-permissions` for more details on this structure.
6034     :param backing_store:
6035         dictionary describing a backing file for the volume. It must contain a ``path``
6036         property pointing to the base volume and a ``format`` property defining the format
6037         of the base volume.
6038         The base volume format will not be guessed for security reasons and is thus mandatory.
6039     :param nocow: disable COW for the volume.
6040     :param connection: libvirt connection URI, overriding defaults
6041     :param username: username to connect with, overriding defaults
6042     :param password: password to connect with, overriding defaults
6043     .. rubric:: CLI Example:
6044     Volume on ESX:
6045     .. code-block:: bash
6046         salt '*' virt.volume_define "[local-storage]" myvm/myvm.vmdk vmdk 8192
6047     QCow2 volume with backing file:
6048     .. code-block:: bash
6049         salt '*' virt.volume_define default myvm.qcow2 qcow2 8192 \
6050                             permissions="{'mode': '0775', 'owner': '123', 'group': '345'"}" \
6051                             backing_store="{'path': '/path/to/base.img', 'format': 'raw'}" \
6052                             nocow=True
6053     .. versionadded:: 3001
6054     """
6055     ret = False
6056     try:
6057         conn = __get_conn(**kwargs)
6058         pool_obj = conn.storagePoolLookupByName(pool)
6059         pool_type = ElementTree.fromstring(pool_obj.XMLDesc()).get("type")
6060         new_allocation = allocation
6061         if pool_type == "logical" and size != allocation:
6062             new_allocation = size
6063         xml = _gen_vol_xml(
6064             name,
6065             size,
6066             format=format,
6067             allocation=new_allocation,
6068             type=type,
6069             permissions=permissions,
6070             backing_store=backing_store,
6071             nocow=nocow,
6072         )
6073         ret = _define_vol_xml_str(conn, xml, pool=pool)
6074     except libvirt.libvirtError as err:
6075         raise CommandExecutionError(err.get_error_message())
6076     finally:
6077         conn.close()
6078     return ret
6079 def _volume_upload(conn, pool, volume, file, offset=0, length=0, sparse=False):
6080     """
6081     Function performing the heavy duty for volume_upload but using an already
6082     opened libvirt connection.
6083     """
6084     def handler(stream, nbytes, opaque):
6085         return os.read(opaque, nbytes)
6086     def holeHandler(stream, opaque):
6087         """
6088         Taken from the sparsestream.py libvirt-python example.
6089         """
6090         fd = opaque
6091         cur = os.lseek(fd, 0, os.SEEK_CUR)
6092         try:
6093             data = os.lseek(fd, cur, os.SEEK_DATA)
6094         except OSError as e:
6095             if e.errno != 6:
6096                 raise e
6097             else:
6098                 data = -1
6099         if data &lt; 0:
6100             inData = False
6101             eof = os.lseek(fd, 0, os.SEEK_END)
6102             if eof &lt; cur:
6103                 raise RuntimeError("Current position in file after EOF: {}".format(cur))
6104             sectionLen = eof - cur
6105         else:
6106             if data &gt; cur:
6107                 inData = False
6108                 sectionLen = data - cur
6109             else:
6110                 inData = True
6111                 hole = os.lseek(fd, data, os.SEEK_HOLE)
6112                 if hole &lt; 0:
6113                     raise RuntimeError("No trailing hole")
6114                 if hole == data:
6115                     raise RuntimeError("Impossible happened")
6116                 else:
6117                     sectionLen = hole - data
6118         os.lseek(fd, cur, os.SEEK_SET)
6119         return [inData, sectionLen]
6120     def skipHandler(stream, length, opaque):
6121         return os.lseek(opaque, length, os.SEEK_CUR)
6122     stream = None
6123     fd = None
6124     ret = False
6125     try:
6126         pool_obj = conn.storagePoolLookupByName(pool)
6127         vol_obj = pool_obj.storageVolLookupByName(volume)
6128         stream = conn.newStream()
6129         fd = os.open(file, os.O_RDONLY)
6130         vol_obj.upload(
6131             stream,
6132             offset,
6133             length,
6134             libvirt.VIR_STORAGE_VOL_UPLOAD_SPARSE_STREAM if sparse else 0,
6135         )
6136         if sparse:
6137             stream.sparseSendAll(handler, holeHandler, skipHandler, fd)
6138         else:
6139             stream.sendAll(handler, fd)
6140         ret = True
6141     except libvirt.libvirtError as err:
6142         raise CommandExecutionError(err.get_error_message())
6143     finally:
6144         if fd:
6145             try:
6146                 os.close(fd)
6147             except OSError as err:
6148                 if stream:
6149                     stream.abort()
6150                 if ret:
6151                     raise CommandExecutionError(
6152                         "Failed to close file: {}".format(err.strerror)
6153                     )
6154         if stream:
6155             try:
6156                 stream.finish()
6157             except libvirt.libvirtError as err:
6158                 if ret:
6159                     raise CommandExecutionError(
6160                         "Failed to finish stream: {}".format(err.get_error_message())
6161                     )
6162     return ret
6163 def volume_upload(pool, volume, file, offset=0, length=0, sparse=False, **kwargs):
6164     """
6165     Create libvirt volume.
6166     :param pool: name of the pool to create the volume in
6167     :param name: name of the volume to define
6168     :param file: the file to upload to the volume
6169     :param offset: where to start writing the data in the volume
6170     :param length: amount of bytes to transfer to the volume
6171     :param sparse: set to True to preserve data sparsiness.
6172     :param connection: libvirt connection URI, overriding defaults
6173     :param username: username to connect with, overriding defaults
6174     :param password: password to connect with, overriding defaults
6175     .. rubric:: CLI Example:
6176     .. code-block:: bash
6177         salt '*' virt.volume_upload default myvm.qcow2 /path/to/disk.qcow2
6178     .. versionadded:: 3001
6179     """
6180     conn = __get_conn(**kwargs)
6181     ret = False
6182     try:
6183         ret = _volume_upload(
6184             conn, pool, volume, file, offset=offset, length=length, sparse=sparse
6185         )
6186     finally:
6187         conn.close()
6188     return ret
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
