<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_sort_1.py &amp; test_opt_4.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_sort_1.py &amp; test_opt_4.py
      </h3>
<h1 align="center">
        3.0%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_sort_1.py (22.916666%)<th>test_opt_4.py (1.6571811%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(163-170)<td><a href="#" name="0">(4410-4416)</a><td align="center"><font color="#ff0000">18</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(386-388)<td><a href="#" name="1">(5008-5011)</a><td align="center"><font color="#c60000">14</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(321-324)<td><a href="#" name="2">(4383-4385)</a><td align="center"><font color="#c60000">14</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(293-295)<td><a href="#" name="3">(4368-4370)</a><td align="center"><font color="#c60000">14</font>
<tr onclick='openModal("#6cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#6cc417"><font color="#6cc417">-</font><td><a href="#" name="4">(156-163)<td><a href="#" name="4">(4198-4204)</a><td align="center"><font color="#c60000">14</font>
<tr onclick='openModal("#151b8d")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#151b8d"><font color="#151b8d">-</font><td><a href="#" name="5">(1-16)<td><a href="#" name="5">(44-69)</a><td align="center"><font color="#c60000">14</font>
<tr onclick='openModal("#8c8774")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#8c8774"><font color="#8c8774">-</font><td><a href="#" name="6">(290-292)<td><a href="#" name="6">(6810-6815)</a><td align="center"><font color="#b80000">13</font>
<tr onclick='openModal("#38a4a5")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#38a4a5"><font color="#38a4a5">-</font><td><a href="#" name="7">(171-178)<td><a href="#" name="7">(4359-4363)</a><td align="center"><font color="#b80000">13</font>
<tr onclick='openModal("#c58917")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#c58917"><font color="#c58917">-</font><td><a href="#" name="8">(151-156)<td><a href="#" name="8">(695-701)</a><td align="center"><font color="#b80000">13</font>
<tr onclick='openModal("#83a33a")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#83a33a"><font color="#83a33a">-</font><td><a href="#" name="9">(80-84)<td><a href="#" name="9">(3623-3628)</a><td align="center"><font color="#b80000">13</font>
<tr onclick='openModal("#ad5910")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#ad5910"><font color="#ad5910">-</font><td><a href="#" name="10">(29-33)<td><a href="#" name="10">(6560-6567)</a><td align="center"><font color="#b80000">13</font>
<tr onclick='openModal("#b041ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#b041ff"><font color="#b041ff">-</font><td><a href="#" name="11">(418-419)<td><a href="#" name="11">(2086-2090)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#571b7e")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#571b7e"><font color="#571b7e">-</font><td><a href="#" name="12">(410-411)<td><a href="#" name="12">(5648-5651)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#3b9c9c")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3b9c9c"><font color="#3b9c9c">-</font><td><a href="#" name="13">(382-383)<td><a href="#" name="13">(4683-4687)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#842dce")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#842dce"><font color="#842dce">-</font><td><a href="#" name="14">(349-350)<td><a href="#" name="14">(5687-5690)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#f52887")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f52887"><font color="#f52887">-</font><td><a href="#" name="15">(270-271)<td><a href="#" name="15">(5657-5660)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#2981b2")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#2981b2"><font color="#2981b2">-</font><td><a href="#" name="16">(259-261)<td><a href="#" name="16">(4455-4456)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#3090c7")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#3090c7"><font color="#3090c7">-</font><td><a href="#" name="17">(248-250)<td><a href="#" name="17">(4437-4438)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#800517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#800517"><font color="#800517">-</font><td><a href="#" name="18">(244-247)<td><a href="#" name="18">(960-963)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#f62817")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f62817"><font color="#f62817">-</font><td><a href="#" name="19">(218-224)<td><a href="#" name="19">(6301-6304)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#4e9258")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4e9258"><font color="#4e9258">-</font><td><a href="#" name="20">(189-195)<td><a href="#" name="20">(3523-3528)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#947010")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#947010"><font color="#947010">-</font><td><a href="#" name="21">(53-59)<td><a href="#" name="21">(852-862)</a><td align="center"><font color="#aa0000">12</font>
<tr onclick='openModal("#4cc417")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#4cc417"><font color="#4cc417">-</font><td><a href="#" name="22">(49-53)<td><a href="#" name="22">(301-313)</a><td align="center"><font color="#aa0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_sort_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from itertools import product, chain
2 from functools import reduce
3 import unittest
4 from theano.tests import unittest_tools as utt
5 import numpy as np
6 import theano
7 from theano import tensor
8 from theano.tensor.sort import sort, SortOp
9 from theano.tensor.sort import argsort, ArgSortOp
10 from theano.tensor.sort import topk, argtopk, topk_and_argtopk, TopKOp
11 _all_dtypes = tensor.integer_dtypes + tensor.</b></font>float_dtypes
12 def gen_unique_vector(size, dtype):
13     retval = np.arange(size) * 3. + np.random.uniform(-1., 1.)
14     return (retval[np.random.permutation(size)] - size * 1.5).astype(dtype)
15 class Test_sort(unittest.TestCase):
16     def setUp(self):
17         self.rng = np.random.RandomState(seed=utt.fetch_seed())
18         self.m_val = self<font color="#ad5910"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.rng.rand(3, 2)
19         self.v_val = self.rng.rand(4)
20     def test1(self):
21         a = tensor.dmatrix(</b></font>)
22         w = sort(a)
23         f = theano.function([a], w)
24         utt.assert_allclose(f(self.m_val), np.sort(self.m_val))
25     def test2(self):
26         a = tensor.dmatrix()
27         axis = tensor.scalar()
28         w = sort(a, axis)
29         f = theano.function([a, axis], w)
30         for axis_val in 0, 1:
31             gv = f(self.m_val, axis_val)
32             gt = np.sort(self.m_val, axis_val)
33     def test3(self):
34         a = tensor.dvector<font color="#4cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>()
35         f = theano.function([a], w2)
36         gv = f(self.v_val)
37         gt = np.sort(</b></font>self<font color="#947010"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.v_val)
38         utt.assert_allclose(gv, gt)
39     def test4(self):
40         a = tensor.dmatrix()
41         axis = tensor.scalar()
42         l =</b></font> sort(a, axis, "mergesort")
43         f = theano.function([a, axis], l)
44         for axis_val in 0, 1:
45             gv = f(self.m_val, axis_val)
46             gt = np.sort(self.m_val, axis_val)
47             utt.assert_allclose(gv, gt)
48     def test5(self):
49         a1 = SortOp("mergesort", [])
50         a2 = SortOp("quicksort", [])
51         assert a1 != a2
52         assert a1 == SortOp("mergesort", [])
53         assert a2 == SortOp("quicksort", [])
54     def test_None(self):
55         a = tensor.dmatrix()
56         f = theano.function([a], l)
57         gv = f(self.m_val)
58         gt = np<font color="#83a33a"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.sort(self.m_val, None)
59         utt.assert_allclose(gv, gt)
60     def test_grad_vector(self):
61         data = np.random.rand(10).astype(</b></font>theano.config.floatX)
62         utt.verify_grad(sort, [data])
63     def test_grad_none_axis(self):
64         data = np.random.rand(10).astype(theano.config.floatX)
65         utt.verify_grad(lambda x: sort(x, None), [data])
66         utt.verify_grad(lambda x: sort(x, 0), [data])
67         data = np.random.rand(2, 3).astype(theano.config.floatX)
68         utt.verify_grad(lambda x: sort(x, None), [data])
69         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
70         utt.verify_grad(lambda x: sort(x, None), [data])
71     def test_grad_negative_axis_2d(self):
72         data = np.random.rand(2, 3).astype(theano.config.floatX)
73         utt.verify_grad(lambda x: sort(x, -1), [data])
74         data = np.random.rand(2, 3).astype(theano.config.floatX)
75         utt.verify_grad(lambda x: sort(x, -2), [data])
76     def test_grad_negative_axis_3d(self):
77         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
78         utt.verify_grad(lambda x: sort(x, -1), [data])
79         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
80         utt.verify_grad(lambda x: sort(x, -2), [data])
81         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
82         utt.verify_grad(lambda x: sort(x, -3), [data])
83     def test_grad_negative_axis_4d(self):
84         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
85         utt.verify_grad(lambda x: sort(x, -1), [data])
86         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
87         utt.verify_grad(lambda x: sort(x, -2), [data])
88         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
89         utt.verify_grad(lambda x: sort(x, -3), [data])
90         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
91         utt.verify_grad(lambda x: sort(x, -4), [data])
92     def test_grad_nonnegative_axis_2d(self):
93         data = np.random.rand(2, 3).astype(theano.config.floatX)
94         utt.verify_grad(lambda x: sort(x, 0), [data])
95         data = np.random.rand(2, 3).astype(theano.config.floatX)
96         utt.verify_grad(lambda x: sort(x, 1), [data])
97     def test_grad_nonnegative_axis_3d(self):
98         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
99         utt.verify_grad(lambda x: sort(x, 0), [data])
100         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
101         utt.verify_grad(lambda x: sort(x, 1), [data])
102         data = np.random.rand(2, 3, 4).astype(theano.config.floatX)
103         utt.verify_grad(lambda x: sort(x, 2), [data])
104     def test_grad_nonnegative_axis_4d(self):
105         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
106         utt.verify_grad(lambda x: sort(x, 0), [data])
107         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
108         utt.verify_grad(lambda x: sort(x, 1), [data])
109         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
110         utt.verify_grad(lambda x: sort(x, 2), [data])
111         data = np.random.rand(2, 3, 4, 2).astype(theano.config.floatX)
112         utt.verify_grad(lambda x: sort(x, 3), [data])
113 class SortInferShapeTester(utt.InferShapeTester):
114     def test_sort(self):
115         self._compile_and_check(
116             [x],
117             [sort<font color="#c58917"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(x)],
118             [np.random.randn(10, 40).astype(theano.config.floatX)],
119         self._compile_and_check(
120             [x],
121             [sort(</b></font>x, axis<font color="#6cc417"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>=None)],
122             [np.random.randn(10, 40).astype(theano.config.floatX)],
123             SortOp)
124 def test_argsort():
125     rng = np.</b></font>random<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.RandomState(seed=utt.fetch_seed())
126     m_val = rng.rand(3, 2)
127     v_val = rng.rand(4)
128     w = argsort(a)
129     f = theano.</b></font>function([a], w)
130     gv = f<font color="#38a4a5"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(m_val)
131     gt = np.argsort(m_val)
132     utt.assert_allclose(gv, gt)
133     a = tensor.dmatrix()
134     axis = tensor.lscalar()
135     w =</b></font> argsort(a, axis)
136     f = theano.function([a, axis], w)
137     for axis_val in 0, 1:
138         gv = f(m_val, axis_val)
139         gt = np.argsort(m_val, axis_val)
140         utt.assert_allclose(gv, gt)
141     w2 = argsort(a)
142     f = theano.function([a], w2)
143     gv = f<font color="#4e9258"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(v_val)
144     gt = np.argsort(v_val)
145     utt.assert_allclose(gv, gt)
146     a = tensor.dmatrix()
147     axis = tensor.lscalar(</b></font>)
148     l = argsort(a, axis, "mergesort")
149     f = theano.function([a, axis], l)
150     for axis_val in 0, 1:
151         gv = f(m_val, axis_val)
152         gt = np.argsort(m_val, axis_val)
153         utt.assert_allclose(gv, gt)
154     a = tensor.dmatrix()
155     axis = tensor.lscalar()
156     a1 = ArgSortOp("mergesort", [])
157     a2 = ArgSortOp("quicksort", [])
158     assert a1 != a2
159     assert a1 == ArgSortOp("mergesort", [])
160     assert a2 == ArgSortOp("quicksort", [])
161     a = tensor.dmatrix()
162     f = theano.function([a], w2)
163     gv = f(m_val)
164     gt = np<font color="#f62817"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.argsort(m_val, None)
165     utt.assert_allclose(gv, gt)
166 def test_argsort_grad():
167     data = np.random.rand(2, 3).astype(</b></font>theano.config.floatX)
168     utt.verify_grad(lambda x: argsort(x, axis=-1), [data])
169     data = np.random.rand(2, 3, 4, 5).astype(theano.config.floatX)
170     utt.verify_grad(lambda x: argsort(x, axis=-3), [data])
171     data = np.random.rand(2, 3, 3).astype(theano.config.floatX)
172     utt.verify_grad(lambda x: argsort(x, axis=2), [data])
173 class Test_TopK(unittest.TestCase):
174     mode = None
175     op_class = TopKOp
176     def setUp(self):
177         pass
178         _all_dtypes, tensor.integer_dtypes, [-1, 0, None], [False]))
179     def test_argtopk_sanity(self, dtype, idx_dtype, axis, sorted):
180         x = tensor.vector<font color="#800517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(name='x', dtype=dtype)
181                              argtopk(x, 1, axis=axis, sorted=sorted, idx_dtype=idx_dtype),
182                              mode=self.</b></font>mode)
183         assert any([isinstance<font color="#3090c7"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(n.op, self.op_class) for n in fn.maker.fgraph.apply_nodes])
184         xval = np.asarray([1]).astype(dtype)
185         yval =</b></font> fn(xval)
186         assert yval == np.asarray([0], dtype=idx_dtype)
187         assert yval.dtype == np.dtype(idx_dtype)
188     @utt.parameterized.expand(product(
189         _all_dtypes, [-1, 0, None], [False]))
190         x = tensor.vector(name='x', dtype=dtype)
191         fn = theano.function([x], topk(x, 1, axis=axis, sorted=sorted), mode=self.mode)
192         assert any([isinstance<font color="#2981b2"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>(n.op, self.op_class) for n in fn.maker.fgraph.apply_nodes])
193         xval = np.asarray([1]).astype(dtype)
194         yval =</b></font> fn(xval)
195         assert yval == xval
196         assert yval.dtype == xval.dtype
197     @utt.parameterized.expand(product(
198         _all_dtypes, tensor.integer_dtypes, [-1, 0, None], [False]))
199         x = tensor.vector(name='x', dtype=dtype)
200         yv, yi = topk_and_argtopk(x, 1, axis=axis, sorted=sorted, idx_dtype=idx_dtype)
201         fn = theano.function([x], [<font color="#f52887"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>yv, yi], mode=self.mode)
202         assert any([isinstance(n.op, self.op_class) for n in fn.maker.fgraph.</b></font>apply_nodes])
203         xval = np.asarray([1]).astype(dtype)
204         yvval, yival = fn(xval)
205         assert yival == np.asarray([0], dtype=idx_dtype)
206         utt.assert_allclose(xval, yvval)
207         assert yvval.dtype == xval.dtype
208         assert yival.dtype == np.dtype(idx_dtype)
209     @utt.parameterized.expand(chain(
210         product(
211             (16, 61, 257),
212             (1, -1, -10, 'n//2', 'n-1', '-n', '1-n'),
213             ('float64', 'float16', 'int16', 'int8'),
214             (False,)),
215         ((2049, 1337, 'float64', False),)))
216     def test_topk_1d(self, size, k, dtype, sorted):
217             k = eval(k.replace('n', str(size)))
218         y = topk(x, k, sorted=sorted)
219         fn = theano.function([x], y, mode=self.</b></font>mode)
220         assert any([<font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>isinstance(n.op, self.op_class) for n in fn.maker.fgraph.apply_nodes])
221         assert 1 == len(fn.maker.fgraph.outputs[0].</b></font>owner.outputs)
222         xval = gen_unique_vector(size, dtype)
223         yval = fn(xval)
224         idx = (slice(-k, None) if k &gt; 0 else slice(-k))
225         goal = np.sort(xval)[idx]
226         assert yval.dtype == goal.dtype
227         utt.assert_allclose(goal, np.sort(yval))
228     @utt.parameterized.expand(chain(
229         product(
230             (16, 61, 257),
231             (1, -1, -10, 'n//2', 'n-1', '-n'),
232             ('float32', 'int32'),
233             (False,),
234             ('int32', 'int64')),
235         ((2049, 1337, 'float32', False, 'int32'),)))
236     def test_argtopk_1d(self, size, k, dtype, sorted, idx_dtype):
237         if isinstance(k, str):
238             k = eval(k.replace('n', str(size)))
239         y = argtopk(x, k, sorted=sorted, idx_dtype=idx_dtype)
240         fn = theano.function([x], y, mode=self.mode)
241         assert any([<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>isinstance(n.op, self.op_class) for n in fn.maker.fgraph.apply_nodes])
242         assert 1 == len(fn.maker.fgraph.outputs[0].</b></font>owner.outputs)
243         xval = gen_unique_vector(size, dtype)
244         yval = fn(xval)
245         idx = (slice(-k, None) if k &gt; 0 else slice(-k))
246         goal = np.argsort(xval)[idx].astype(idx_dtype)
247         assert np.all(xval[np.sort(yval)] == xval[np.sort(goal)])
248     @utt.parameterized.expand(chain(
249         product(
250             (16, 61, 257),
251             (1, -1, 10, 'n//2', 'n-1', '1-n'),
252             ('float32', 'int32'),
253             (False,),
254             ('int32', 'int64')),
255         ((2049, 1337, 'float32', False, 'int32'),)))
256     def test_combined_1d(self, size, k, dtype, sorted, idx_dtype):
257         if isinstance(k, str):
258             k = eval(k.replace('n', str(size)))
259         x = theano.tensor.vector(name='x', dtype=dtype)
260         yv, yi = topk_and_argtopk(x, k, sorted=sorted, idx_dtype=idx_dtype)
261         fn = theano.function([x], [<font color="#842dce"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>yv, yi], mode=self.mode)
262         assert any([isinstance(n.op, self.op_class) for n in fn.maker.fgraph.</b></font>apply_nodes])
263         xval = gen_unique_vector(size, dtype)
264         yvval, yival = fn(xval)
265         idx = (slice(-k, None) if k &gt; 0 else slice(-k))
266         goali = np.argsort(xval)[idx].astype(idx_dtype)
267         goalv = xval[goali]
268         assert np.all(xval[np.sort(yival)] == xval[np.sort(goali)])
269         utt.assert_allclose(np.sort(yvval), goalv)
270     @utt.parameterized.expand(chain(
271         product(
272             (18, 62, 258),
273             (1, -1, 'n//2'),
274             ('int32', 'float32'),
275             (False,)),
276         ((2048, 1337, 'float32', False),)))
277     def test_argtopk_1d_collision(self, size, k, dtype, sorted):
278         if isinstance(k, str):
279             k = eval(k.replace('n', str(size)))
280         x = theano.tensor.vector(name='x', dtype=dtype)
281         y = argtopk(x, k, sorted=sorted, idx_dtype='int32')
282         mode = self.mode
283             mode = theano.Mode(optimizer=mode.optimizer)
284         fn = theano.function([x], y, mode=mode)
285         assert any([<font color="#3b9c9c"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>isinstance(n.op, self.op_class) for n in fn.maker.fgraph.apply_nodes])
286         xval = xval[np.random.permutation(size)]
287         yval = fn(xval)
288         idx = (<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>slice(-k, None) if k &gt; 0 else slice(-k))
289         goal = np.argsort(xval)[idx].astype('int32')
290         utt.assert_allclose(np.sort(xval[</b></font>yval]), np.sort(xval[goal]))
291     @utt.parameterized.expand(product(
292         ((17, 15), (2, 3, 5, 7, 11), (500, 5, 3)),  # NB: Test may fail with bigger sizes (e.g. (2017, 5, 3)) due to "too many resources requested" kernel error on some GPUs.
293         (-1, '(1+n)//2', '-n', '1-n'),
294         ('float32', 'int32'),
295         (False,),
296         ('int32', 'int64')))
297     def test_argtopk_nd(self, shp, k_, dtype, sorted, idx_dtype):
298         ndim = len(shp)
299         for axis in range(-ndim, ndim):
300             if isinstance(k_, str):
301                 k = eval(k_.replace('n', str(shp[axis])))
302             else:
303                 k = k_
304             if k == 0:
305                 continue
306                 name='x', broadcastable=(False,) * len(shp), dtype=dtype)
307             y = argtopk(x, k, axis=axis, sorted=sorted, idx_dtype=idx_dtype)
308             fn = theano.function<font color="#571b7e"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>([x], y, mode=self.mode)
309             assert any([isinstance(n.op, self.op_class) for n in fn.maker.fgraph.</b></font>apply_nodes])
310             size = reduce(int.__mul__, shp)
311             xval = gen_unique_vector(size, dtype).reshape(shp)
312             yval = fn(xval)
313             l = axis % ndim
314             r = ndim - l
315             idx = (<font color="#b041ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>slice(None),) * l + (idx,) + (slice(None),) * (r - 1)
316             goal = np.argsort(xval, axis=axis)[idx].astype(</b></font>idx_dtype)
317             assert np.all(np.sort(yval, axis=axis) == np.sort(goal, axis=axis))
318     @utt.parameterized.expand(product(
319         ((257,), (17, 15), (5, 3, 5, 3), (2, 3, 5, 7, 11)),
320         (1, -1, '(1+n)//2', 'n-1', '-n', '1-n'), (False,)))
321     def test_grad(self, shp, k_, sorted):
322         ndim = len(shp)
323         for axis in range(-ndim, ndim):
324             if isinstance(k_, str):
325                 k = eval(k_.replace('n', str(shp[axis])))
326             else:
327                 k = k_
328             if k == 0:
329                 continue
330             xval = gen_unique_vector(
331                 reduce(int.__mul__, shp),
332                 dtype=theano.config.floatX
333             ).reshape(shp)
334             utt.verify_grad(lambda x: topk(x, k, axis=axis, sorted=sorted), [xval], eps=1e-2)
335 class TopKInferShapeTester(utt.InferShapeTester):
336     @utt.parameterized.expand(product(
337         ((2, 3), (15, 17), (11, 7, 5), (2, 3, 5, 7, 11), (2, 4, 3, 1)),
338         (1, '(1+n)//2', 'n-1', 'n')))
339     def test_combined_infer_shape(self, shp, k_):
340         ndim = len(shp)
341         for axis in range(-ndim, ndim):
342             if isinstance(k_, str):
343                 k = eval(k_.replace('n', str(shp[axis])))
344             else:
345                 k = k_
346             if k == 0:
347                 continue
348             x = theano.tensor.tensor(
349                 name='x', broadcastable=(False,) * len(shp),
350                 dtype=theano.config.floatX)
351             yv, yi = topk_and_argtopk(x, k, axis=axis, sorted=False, idx_dtype='int32')
352             size = reduce(int.__mul__, shp)
353             xval = gen_unique_vector(size, theano.config.floatX).reshape(shp)
354             self._compile_and_check(
355                 [x], [yv, yi], [xval], TopKOp)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_opt_4.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 import copy
3 import logging
4 import time
5 import unittest
6 import numpy as np
7 from six.moves import xrange
8 from nose.plugins.skip import SkipTest
9 from nose.tools import assert_raises, assert_true
10 import theano
11 import theano.scalar as scal
12 from six import StringIO
13 from theano import compile
14 from theano.compile import deep_copy_op, DeepCopyOp
15 from theano.compile import get_mode
16 from theano import config
17 from theano import function
18 from theano import gof
19 from theano import pprint
20 from theano import shared
21 from theano.gof import FunctionGraph
22 import theano.tensor.opt as opt
23 from theano.tensor.opt import (
24     local_add_specialize,
25     local_dimshuffle_lift,
26     local_useless_dimshuffle_in_reshape,
27     local_useless_alloc,
28     local_merge_alloc,
29     local_greedy_distributor,
30     local_useless_reshape,
31     local_reshape_to_dimshuffle,
32     mul_canonizer,
33     Shape_i,
34     Assert,
35     MakeVector,
36     make_vector,
37     local_canonicalize_alloc
38 from theano import tensor
39 from theano import tensor as T
40 <font color="#151b8d"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from theano.tensor import scalar, iscalar, lscalar, fscalar, dscalar
41 from theano.tensor import vector, lvector, fvector, dvector
42 from theano.tensor import matrix, fmatrix, dmatrix, tensor3
43 from theano.tensor import vectors, matrices, fmatrices, dmatrices
44 from theano.tensor import (
45     AdvancedSubtensor,
46     AdvancedSubtensor1,
47     as_tensor_variable,
48     IncSubtensor,
49     AdvancedIncSubtensor,
50     AdvancedIncSubtensor1,
51     inplace,
52     Join,
53     join,
54     Subtensor,
55     TensorType,
56     tile
57     )
58 from theano.tensor.elemwise import DimShuffle
59 from theano.tensor.type import values_eq_approx_remove_nan
60 from theano.tests import unittest_tools as utt
61 from theano.gof.opt import check_stack_trace, out2in
62 from theano import change_flags
63 from nose.plugins.attrib import attr
64 mode_opt = theano.config.</b></font>mode
65 if mode_opt == 'FAST_COMPILE':
66     mode_opt = 'FAST_RUN'
67 mode_opt = theano.compile.mode.get_mode(mode_opt)
68 dimshuffle_lift = out2in(local_dimshuffle_lift)
69 _optimizer_stabilize = gof.Query(include=['fast_run'])
70 _optimizer_stabilize.position_cutoff = 1.51
71 _optimizer_stabilize = compile.optdb.query(_optimizer_stabilize)
72 _optimizer_specialize = gof.Query(include=['fast_run'])
73 _optimizer_specialize.position_cutoff = 2.01
74 _optimizer_specialize = compile.optdb.query(_optimizer_specialize)
75 _optimizer_fast_run = gof.Query(include=['fast_run'])
76 _optimizer_fast_run = compile.optdb.query(_optimizer_fast_run)
77 def ds(x, y):
78     return DimShuffle(x.type.broadcastable, y)(x)
79 def optimize(g, level='fast_run'):
80     if level == 'fast_run':
81         _optimizer_fast_run.optimize(g)
82     elif level == 'specialize':
83         _optimizer_specialize.optimize(g)
84     elif level == 'stabilize':
85         _optimizer_stabilize.optimize(g)
86     else:
87         raise ValueError(level)
88     return g
89 def inputs(xbc=(0, 0), ybc=(0, 0), zbc=(0, 0)):
90     x = TensorType(broadcastable=xbc, dtype='float64')('x')
91     y = TensorType(broadcastable=ybc, dtype='float64')('y')
92     z = TensorType(broadcastable=zbc, dtype='float64')('z')
93     return x, y, z
94 class test_dimshuffle_lift(unittest.TestCase):
95     def test_double_transpose(self):
96         x, y, z = inputs()
97         e = ds(ds(x, (1, 0)), (1, 0))
98         g = FunctionGraph([x], [e])
99         self.assertTrue(str(g) == "[InplaceDimShuffle{1,0}(InplaceDimShuffle{1,0}(x))]")
100         dimshuffle_lift.optimize(g)
101         self.assertTrue(str(g) == "[x]")
102     def test_merge2(self):
103         x, y, z = inputs()
104         e = ds(ds(x, (1, 'x', 0)), (2, 0, 'x', 1))
105         g = FunctionGraph([x], [e])
106         self.assertTrue(str(g) == "[InplaceDimShuffle{2,0,x,1}(InplaceDimShuffle{1,x,0}(x))]",
107                         str(g))
108         dimshuffle_lift.optimize(g)
109         self.assertTrue(str(g) == "[InplaceDimShuffle{0,1,x,x}(x)]", str(g))
110         self.assertTrue(check_stack_trace(g, ops_to_check='all'))
111     def test_elim3(self):
112         x, y, z = inputs()
113         e = ds(ds(ds(x, (0, 'x', 1)), (2, 0, 'x', 1)), (1, 0))
114         g = FunctionGraph([x], [e])
115         self.assertTrue(str(g) == ("[InplaceDimShuffle{1,0}(InplaceDimShuffle{2,0,x,1}"
116                                    "(InplaceDimShuffle{0,x,1}(x)))]"),
117                         str(g))
118         dimshuffle_lift.optimize(g)
119         self.assertTrue(str(g) == "[x]", str(g))
120     def test_lift(self):
121         x, y, z = inputs([False] * 1, [False] * 2, [False] * 3)
122         e = x + y + z
123         g = FunctionGraph([x, y, z], [e])
124         init_str_g_inplace = (
125             "[Elemwise{add,no_inplace}(InplaceDimShuffle{x,0,1}"
126             "(Elemwise{add,no_inplace}(InplaceDimShuffle{x,0}(x), y)), z)]")
127         init_str_g_noinplace = (
128             "[Elemwise{add,no_inplace}(DimShuffle{x,0,1}"
129             "(Elemwise{add,no_inplace}(DimShuffle{x,0}(x), y)), z)]")
130         self.assertTrue(str(g) in (init_str_g_inplace, init_str_g_noinplace),
131                         str(g))
132         opt_str_g_inplace = (
133             "[Elemwise{add,no_inplace}(Elemwise{add,no_inplace}"
134             "(InplaceDimShuffle{x,x,0}(x), InplaceDimShuffle{x,0,1}(y)), z)]")
135         opt_str_g_noinplace = (
136             "[Elemwise{add,no_inplace}(Elemwise{add,no_inplace}"
137             "(DimShuffle{x,x,0}(x), DimShuffle{x,0,1}(y)), z)]")
138         dimshuffle_lift.optimize(g)
139         self.assertTrue(str(g) in (opt_str_g_inplace, opt_str_g_noinplace),
140                         str(g))
141         self.assertTrue(check_stack_trace(g, ops_to_check='all'))
142     def test_recursive_lift(self):
143         v = T.vector(dtype="float64")
144         m = T.matrix(dtype="float64")
145         out = ((v + 42) * (m + 84)).T
146         g = FunctionGraph([v, m], [out])
147         init_str_g = ("[InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}"
148                       "(InplaceDimShuffle{x,0}(Elemwise{add,no_inplace}"
149                       "(&lt;TensorType(float64, vector)&gt;, "
150                       "InplaceDimShuffle{x}(TensorConstant{42}))), "
151                       "Elemwise{add,no_inplace}"
152                       "(&lt;TensorType(float64, matrix)&gt;, "
153                       "InplaceDimShuffle{x,x}(TensorConstant{84}))))]")
154         self.assertTrue(str(g) == init_str_g)
155         new_out = local_dimshuffle_lift.transform(g.outputs[0].owner)[0]
156         new_g = FunctionGraph(g.inputs, [new_out])
157         opt_str_g = ("[Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}"
158                      "(InplaceDimShuffle{0,x}(&lt;TensorType(float64, vector)&gt;), "
159                      "InplaceDimShuffle{x,x}(TensorConstant{42})), "
160                      "Elemwise{add,no_inplace}(InplaceDimShuffle{1,0}"
161                      "(&lt;TensorType(float64, matrix)&gt;), "
162                      "InplaceDimShuffle{x,x}(TensorConstant{84})))]")
163         self.assertTrue(str(new_g) == opt_str_g)
164         self.assertTrue(check_stack_trace(new_g, ops_to_check='all'))
165     def test_useless_dimshuffle(self):
166         x, _, _ = inputs()
167         e = ds(x, (0, 1))
168         g = FunctionGraph([x], [e])
169         self.assertTrue(str(g) == "[InplaceDimShuffle{0,1}(x)]")
170         dimshuffle_lift.optimize(g)
171         self.assertTrue(str(g) == "[x]")
172         self.assertTrue(hasattr(g.outputs[0].tag, 'trace'))
173     def test_dimshuffle_on_broadcastable(self):
174         x, y, z = inputs([False, True], [True, False, True], [False, False, True])
175         u = tensor.constant(1)
176         ds_x = ds(x, (0, 'x'))   # useless
177         ds_y = ds(y, (2, 1, 0))  # useless
178         ds_z = ds(z, (2, 1, 0))  # useful
179         ds_u = ds(u, ('x'))  # useful
180         g = FunctionGraph([x, y, z, u], [ds_x, ds_y, ds_z, ds_u])
181         self.assertTrue(str(g) == "[InplaceDimShuffle{0,x}(x), InplaceDimShuffle{2,1,0}(y), InplaceDimShuffle{2,1,0}(z), InplaceDimShuffle{x}(TensorConstant{1})]")
182         dimshuffle_lift.optimize(g)
183         self.assertTrue(str(g) == "[x, y, InplaceDimShuffle{2,1,0}(z), InplaceDimShuffle{x}(TensorConstant{1})]")
184         self.assertTrue(hasattr(g.outputs[0].tag, 'trace'))
185 def test_local_useless_dimshuffle_in_reshape():
186     vector = TensorType(broadcastable=(False,), dtype='float64')('vector')
187     mat = TensorType(broadcastable=(False, False), dtype='float64')('mat')
188     row = TensorType(broadcastable=(True, False), dtype='float64')('row')
189     col = TensorType(broadcastable=(False, True), dtype='float64')('col')
190     reshape_dimshuffle_vector = tensor.reshape(vector.dimshuffle('x', 0), vector.shape)
191     reshape_dimshuffle_mat = tensor.reshape(mat.dimshuffle('x', 0, 'x', 1), mat.shape)
192     reshape_dimshuffle_row = tensor.reshape(row.dimshuffle(1, 'x'), row.shape)
193     reshape_dimshuffle_col = tensor.reshape(col.dimshuffle(0), col.shape)
194     g = FunctionGraph([vector, mat, row, col],
195                       [reshape_dimshuffle_vector, reshape_dimshuffle_mat,
196                        reshape_dimshuffle_row, reshape_dimshuffle_col])
197     print(str(g))
198     assert_true(str(g) == "[Reshape{1}(InplaceDimShuffle{x,0}(vector), Shape(vector)), "
199                           "Reshape{2}(InplaceDimShuffle{x,0,x,1}(mat), Shape(mat)), "
200                           "Reshape{2}(InplaceDimShuffle{1,x}(row), Shape(row)), "
201                           "Reshape{2}(InplaceDimShuffle{0}(col), Shape(col))]")
202     useless_dimshuffle_in_reshape = out2in(local_useless_dimshuffle_in_reshape)
203     useless_dimshuffle_in_reshape.optimize(g)
204     assert_true(str(g) == "[Reshape{1}(vector, Shape(vector)), "
205                           "Reshape{2}(mat, Shape(mat)), "
206                           "Reshape{2}(row, Shape(row)), "
207                           "Reshape{2}(col, Shape(col))]")
208     assert_true(check_stack_trace(g, ops_to_check='all'))
209     reshape_dimshuffle_mat2 = tensor.reshape(mat.dimshuffle('x', 1, 'x', 0), mat.shape)
210     h = FunctionGraph([mat], [reshape_dimshuffle_mat2])
211     str_h = str(h)
212     useless_dimshuffle_in_reshape.optimize(h)
213     assert_true(str(h) == str_h)
214 def test_add_canonizer_problem0():
215     n_segments = 10
216     label = lscalar('label')
217     segment_labels = label + theano._asarray([0] * n_segments, dtype='int64')
218     r = segment_labels * 5
219     f = function([label], r)
220     f(3)
221     c0 = theano.tensor.constant([True])
222     c1 = theano.tensor.constant([True])
223     theano.function([], c0 + c1)
224 class test_greedy_distribute(unittest.TestCase):
225     def test_main(self):
226         a, b, c, d, x, y, z = matrices('abcdxyz')
227         e = (a / z + b / x) * x * z
228         g = FunctionGraph([a, b, c, d, x, y, z], [e])
229         mul_canonizer.optimize(g)
230         gof.TopoOptimizer(gof.LocalOptGroup(local_greedy_distributor),
231                           order='out_to_in').optimize(g)
232         assert str(pprint(g.outputs[0])) == "((a * x) + (b * z))"
233         e = (a / x + b) * x
234         g = FunctionGraph([a, b, x], [e])
235         mul_canonizer.optimize(g)
236         gof.TopoOptimizer(gof.LocalOptGroup(local_greedy_distributor),
237                           order='out_to_in').optimize(g)
238         assert str(pprint(g.outputs[0])) == "(a + (b * x))"
239     def test_kording_bug(self):
240         x, y = vectors('xy')
241         eps = scalar<font color="#4cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>('eps')
242         s = scalar('s')
243         r = tensor.mul(s - 1,
244                        eps + x / s,
245                        eps + y / s,
246                        s)
247         f = function([s, eps, x, y], r ** 2)
248         s_val = np.asarray(</b></font>4, dtype=config.floatX)
249         eps_val = np.asarray(1.e-6, dtype=config.floatX)
250         x_val = np.asarray([1.5, 2], dtype=config.floatX)
251         y_val = np.asarray([2.3, 3.1], dtype=config.floatX)
252         r0 = f(s_val, eps_val, x_val, y_val)
253         r1 = f(s_val, eps_val, x_val, y_val)
254         r2 = f(s_val, eps_val, x_val, y_val)
255         assert np.all(r0 == r1)
256         assert np.all(r0 == r2)
257 class test_canonize(unittest.TestCase):
258     def test_muldiv(self):
259         x, y, z = matrices('xyz')
260         a, b, c, d = matrices('abcd')
261         e = (-1 * x) / y / (-2 * z)
262         g = FunctionGraph([x, y, z, a, b, c, d], [e])
263         print(pprint(g.outputs[0]))
264         mul_canonizer.optimize(g)
265         print(pprint(g.outputs[0]))
266     def test_elemwise_multiple_inputs_optimisation(self):
267         shp = (5, 5)
268         fx, fy, fz = fmatrices('xyz')
269         dx, dy, dz = dmatrices('xyz')
270         fxv = theano._asarray(np.random.rand(*shp), dtype='float32')
271         fyv = theano._asarray(np.random.rand(*shp), dtype='float32')
272         fzv = theano._asarray(np.random.rand(*shp), dtype='float32')
273         cases = [
274             (fx + fy, (fx, fy), (fxv, fyv), 1, 'float32'),
275             (fx * fy, (fx, fy), (fxv, fyv), 1, 'float32'),
276             (fx + fy + fz + 2, (fx, fy, fz), (fxv, fyv, fzv), 1,
277                 {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
278             (fx * fy * fz * 2, (fx, fy, fz), (fxv, fyv, fzv), 1,
279                 {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
280             (2 + fx + fy + fz + 2, (fx, fy, fz), (fxv, fyv, fzv), 1,
281                 {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
282             (2 * fx * fy * fz * 2, (fx, fy, fz), (fxv, fyv, fzv), 1,
283                 {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
284             (fx * fy * 2 * (fx + fy + fz + 2), (fx, fy, fz), (fxv, fyv, fzv), 2,
285                 {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
286             ]  # [10:11]
287         mode = compile.mode.get_default_mode()
288         opt = gof.Query(["canonicalize"])
289         opt = opt.excluding('local_elemwise_fusion')
290         mode = mode.__class__(linker=mode.linker, optimizer=opt)
291         for id, [g, sym_inputs, val_inputs,
292                  nb_elemwise, out_dtype] in enumerate(cases):
293             if isinstance(out_dtype, dict):
294                 out_dtype = out_dtype[config.cast_policy]
295             f = compile.function(list(sym_inputs), g,
296                                  mode=mode)
297             out = f(*val_inputs)
298             assert(len(f.maker.fgraph.toposort()) == nb_elemwise)
299             assert(out_dtype == out.dtype)
300     def test_elemwise_multiple_inputs_optimisation2(self):
301         raise SkipTest("Current implementation of Canonizer does not "
302                        "implement all cases. Skip the corresponding test.")
303         shp = (5, 5)
304         fx, fy, fz = fmatrices('xyz')
305         dx, dy, dz = dmatrices('xyz')
306         fv = fvector('r').dimshuffle('x', 0)
307         dv = dvector('s').dimshuffle('x', 0)
308         fxv = theano._asarray(np.random.rand(*shp), dtype='float32')
309         fyv = theano._asarray(np.random.rand(*shp), dtype='float32')
310         fzv = theano._asarray(np.random.rand(*shp), dtype='float32')
311         fvv = theano._asarray(np.random.rand(shp[0]), dtype='float32').reshape(1, shp[0])
312         dxv = theano._asarray(np.random.rand(*shp), dtype='float64')
313         dyv = theano._asarray(np.random.rand(*shp), dtype='float64')
314         dzv = theano._asarray(np.random.rand(*shp), dtype='float64')
315         dvv = theano._asarray(np.random.rand(shp[0]), dtype='float64').reshape(1, shp[0])
316         cases = [
317             (fx + fy, (fx, fy), (fxv, fyv), 1, 'float32'),
318             (fx * fy, (fx, fy), (fxv, fyv), 1, 'float32'),
319             (fx + fy + fz, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
320             (dx + dy + dz, (dx, dy, dz), (dxv, dyv, dzv), 1, 'float64'),
321             (fx * fy * fz, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
322             (dx * dy * dz, (dx, dy, dz), (dxv, dyv, dzv), 1, 'float64'),
323             (fx * fy * (fx + fy + fz), (fx, fy, fz), (fxv, fyv, fzv), 2, 'float32'),
324             (dx * dy * (dx + dy + dz), (dx, dy, dz), (dxv, dyv, dzv), 2, 'float64'),
325             (fx * fy * (fx + fy + dz), (fx, fy, dz), (dxv, dyv, dzv), 2, 'float64'),  # check mixed type add
326             (dz * fy * (fx + fy), (fx, fy, dz), (dxv, dyv, dzv), 2, 'float64'),  # check mixed type mul
327             (fx + fy + fz + 2, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
328             (fx * fy * fz * 2, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
329             (2 + fx + fy + fz, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
330             (2 * fx * fy * fz, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
331             (2 + fx + fy + fz + 2, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
332             (2 * fx * fy * fz * 2, (fx, fy, fz), (fxv, fyv, fzv), 1, 'float32'),
333             (fx * fy * 2 * (fx + fy + fz), (fx, fy, fz), (fxv, fyv, fzv), 2, 'float32'),
334             (fx * fy * (2 + fx + fy + fz), (fx, fy, fz), (fxv, fyv, fzv), 2, 'float32'),
335             (fx * fy * 2 * (fx + fy + fz + 2), (fx, fy, fz), (fxv, fyv, fzv), 2, 'float32'),
336             (fx + fy + fz + fv, (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 1, 'float32'),
337             (fx * fy * fz * fv, (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 1, 'float32'),
338             (fv + fx + fy + fz, (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 1, 'float32'),
339             (fv * fx * fy * fz, (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 1, 'float32'),
340             (fx * fy * fv * (fx + fy + fz), (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 2, 'float32'),
341             (fx * fy * (fv + fx + fy + fz), (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 2, 'float32'),
342             (fx * fy * fv * (fv + fx + fy + fz), (fx, fy, fz, fv), (fxv, fyv, fzv, fvv), 2, 'float32'),
343             (dx + dy + dz + dv, (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 1, 'float64'),
344             (dx * dy * dz * dv, (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 1, 'float64'),
345             (dv + dx + dy + dz, (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 1, 'float64'),
346             (dv * dx * dy * dz, (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 1, 'float64'),
347             (dx * dy * dv * (dx + dy + dz), (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 2, 'float64'),
348             (dx * dy * (dv + dx + dy + dz), (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 2, 'float64'),
349             (dx * dy * dv * (dv + dx + dy + dz), (dx, dy, dz, dv), (dxv, dyv, dzv, dvv), 2, 'float64'),
350             ]  # [10:11]
351         mode = compile.mode.get_default_mode()
352         mode._optimizer = gof.Query(["canonicalize"])
353         mode._optimizer = mode._optimizer.excluding('local_elemwise_fusion')
354         for id, [g, sym_inputs, val_inputs, nb_elemwise, out_dtype] in enumerate(cases):
355             f = compile.function(list(sym_inputs), g,
356                                  mode=mode)
357             out = f(*val_inputs)
358             assert(len(f.maker.fgraph.toposort()) == nb_elemwise)
359             assert(out_dtype == out.dtype)
360     @attr('slow')
361     def test_multiple_case(self):
362         shp = (3, 3)
363         fx, fy, fz, fw = fmatrices('xyzw')
364         dx, dy, dz, dw = dmatrices('xyzw')
365         fv = fvector('r').dimshuffle('x', 0)
366         dv = dvector('s').dimshuffle('x', 0)
367         fxv = theano._asarray(np.random.rand(*shp), dtype='float32')
368         fyv = theano._asarray(np.random.rand(*shp), dtype='float32')
369         fzv = theano._asarray(np.random.rand(*shp), dtype='float32')
370         fwv = theano._asarray(np.random.rand(*shp), dtype='float32')
371         fvv = theano._asarray(np.random.rand(shp[0]), dtype='float32').reshape(1, shp[0])
372         dxv = theano._asarray(np.random.rand(*shp), dtype='float64')
373         dyv = theano._asarray(np.random.rand(*shp), dtype='float64')
374         dzv = theano._asarray(np.random.rand(*shp), dtype='float64')
375         dwv = theano._asarray(np.random.rand(*shp), dtype='float64')
376         dvv = theano._asarray(np.random.rand(shp[0]), dtype='float64').reshape(1, shp[0])
377         mode = compile.mode.get_default_mode()
378         opt = gof.Query(["canonicalize"])
379         opt = opt.including('ShapeOpt', 'local_fill_to_alloc')
380         opt = opt.excluding(
381             'local_elemwise_fusion')
382         mode = mode.__class__(linker=mode.linker, optimizer=opt)
383         for id, (g, sym_inputs, val_inputs, out_dtype) in enumerate([
384                 (fx / fx, [fx], [fxv], 'float32'),
385                 (dx / dx, [dx], [dxv], 'float64'),
386                 (fv / fv, [fv], [fvv], 'float32'),
387                 (dv / dv, [dv], [dvv], 'float64')]):
388             f = compile.function(list(sym_inputs), g,
389                                  mode=mode)
390             out = f(*val_inputs)
391             assert (out == np.ones(shp, dtype=out_dtype)).all()
392             topo = f.maker.fgraph.toposort()
393             if sym_inputs[0].broadcastable[0]:
394                 assert len(topo) == 2
395                 assert isinstance(topo[0].op, Shape_i)
396                 assert isinstance(topo[1].op, tensor.Alloc)
397             else:
398                 assert len(topo) == 3
399                 assert isinstance(topo[0].op, Shape_i)
400                 assert isinstance(topo[1].op, Shape_i)
401                 assert isinstance(topo[2].op, tensor.Alloc)
402             assert(out_dtype == out.dtype)
403         for id, (g, sym_inputs, val_inputs, nb_elemwise, out_dtype) in enumerate([
404                 ((dx * dy) / dx, [dx, dy], [dxv, dyv], 0, 'float64'),
405                 ((fx * fy) / fx, [fx, fy], [fxv, fyv], 0, 'float32'),
406                 ((dv * dy) / dv, [dv, dy], [dvv, dyv], 0, 'float64'),
407                 ((fv * fy) / fv, [fv, fy], [fvv, fyv], 0, 'float32'),
408                 ((dx * dv) / dx, [dx, dv], [dxv, dvv], 1, 'float64'),
409                 ((fx * fv) / fx, [fx, fv], [fxv, fvv], 1, 'float32')
410                 ]):
411             f = compile.function(list(sym_inputs), g,
412                                  mode=mode)
413             out = f(*val_inputs)
414             assert(out_dtype == out.dtype)
415             utt.assert_allclose(out, val_inputs[1])
416             topo = f.maker.fgraph.toposort()
417             if topo and not(len(topo) == 1 and topo[0].op == deep_copy_op):
418                 for node in topo[:-1]:
419                     assert isinstance(node.op, Shape_i)
420                 assert isinstance(topo[-1].op, tensor.Alloc)
421         for id, (g, sym_inputs, val_inputs, nb_elemwise, out_dtype) in enumerate([
422                 ((dx / dy) / dx, [dx, dy], [dxv, dyv], 1, 'float64'),
423                 ((fx / fy) / fx, [fx, fy], [fxv, fyv], 1, 'float32'),
424                 ((dv / dy) / dv, [dv, dy], [dvv, dyv], 1, 'float64'),
425                 ((fv / fy) / fv, [fv, fy], [fvv, fyv], 1, 'float32'),
426                 ((dx / dv) / dx, [dx, dv], [dxv, dvv], 1, 'float64'),
427                 ((fx / fv) / fx, [fx, fv], [fxv, fvv], 1, 'float32'),
428                 ]):
429             f = compile.function(list(sym_inputs), g, mode=mode)
430             out = f(*val_inputs)
431             utt.assert_allclose(out, (1 / val_inputs[1]))
432             topo = f.maker.fgraph.toposort()
433             elem = [t for t in topo if isinstance(t.op, T.Elemwise)]
434             assert len(elem) == nb_elemwise
435             assert isinstance(elem[0].op, (T.Elemwise, ))
436             assert isinstance(elem[0].op.scalar_op, (
437                 theano.scalar.basic.Inv, theano.scalar.basic.TrueDiv))
438             assert(out_dtype == out.dtype)
439         for id, (g, sym_inputs, val_inputs, out_dtype) in enumerate([
440                 ((dx / dy) * (dy / dz) * (dz / dw), [dx, dy, dz, dw], [dxv, dyv, dzv, dwv], 'float64'),
441                 ((fx / fy) * (fy / fz) * (fz / fw), [fx, fy, fz, fw], [fxv, fyv, fzv, fwv], 'float32'),
442                 ((dv / dy) * (dy / dz) * (dz / dw), [dv, dy, dz, dw], [dvv, dyv, dzv, dwv], 'float64'),
443                 ((fv / fy) * (fy / fz) * (fz / fw), [fv, fy, fz, fw], [fvv, fyv, fzv, fwv], 'float32'),
444                 ((dx / dv) * (dv / dz) * (dz / dw), [dx, dv, dz, dw], [dxv, dvv, dzv, dwv], 'float64'),
445                 ((fx / fv) * (fv / fz) * (fz / fw), [fx, fv, fz, fw], [fxv, fvv, fzv, fwv], 'float32'),
446                 ((dx / dy) * (dy / dv) * (dv / dw), [dx, dy, dv, dw], [dxv, dyv, dvv, dwv], 'float64'),
447                 ((fx / fy) * (fy / fv) * (fv / fw), [fx, fy, fv, fw], [fxv, fyv, fvv, fwv], 'float32'),
448                 ((dx / dy) * (dy / dz) * (dz / dv), [dx, dy, dz, dv], [dxv, dyv, dzv, dvv], 'float64'),
449                 ((fx / fy) * (fy / fz) * (fz / fv), [fx, fy, fz, fv], [fxv, fyv, fzv, fvv], 'float32'),
450                 ]):
451             f = compile.function(list(sym_inputs), g, mode=mode)
452             out = f(*val_inputs)
453             utt.assert_allclose(out, (val_inputs[0] / val_inputs[3]))
454             topo = f.maker.fgraph.toposort()
455             assert len(topo) == 1
456             assert isinstance(topo[0].op, (T.Elemwise, ))
457             assert isinstance(topo[0].op.scalar_op, theano.scalar.basic.TrueDiv)
458             assert len(topo[0].inputs) == 2
459             assert(out_dtype == out.dtype)
460         for id, (g, sym_inputs, val_inputs, out_dtype) in enumerate([
461                 (((2.0 * dx) / (4.0 * dy)), [dx, dy], [dxv, dyv], 'float64'),
462                 (((2.0 * fx) / (4.0 * fy)), [fx, fy], [fxv, fyv], {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
463                 (((2.0 * dv) / (4.0 * dy)), [dv, dy], [dvv, dyv], 'float64'),
464                 (((2.0 * fv) / (4.0 * fy)), [fv, fy], [fvv, fyv], {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
465                 (((2.0 * dx) / (4.0 * dv)), [dx, dv], [dxv, dvv], 'float64'),
466                 (((2.0 * fx) / (4.0 * fv)), [fx, fv], [fxv, fvv], {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
467                 ]):
468             if isinstance(out_dtype, dict):
469                 out_dtype = out_dtype[config.cast_policy]
470             f = compile.function(list(sym_inputs), g, mode=mode)
471             out = f(*val_inputs)
472             utt.assert_allclose(out, (0.5 * val_inputs[0] / val_inputs[1]))
473             topo = f.maker.fgraph.toposort()
474             assert len(topo) == 2
475             assert isinstance(topo[0].op, (T.Elemwise, ))
476             assert isinstance(topo[0].op.scalar_op, theano.scalar.basic.Mul)
477             assert len(topo[0].inputs) == 2
478             assert isinstance(topo[1].op, (T.Elemwise, ))
479             assert isinstance(topo[1].op.scalar_op, theano.scalar.basic.TrueDiv)
480             assert len(topo[1].inputs) == 2
481             assert(out_dtype == out.dtype)
482         for id, (g, sym_inputs, val_inputs, out_dtype) in enumerate([
483                 ((2 * dx) / 2, [dx], [dxv], 'float64'),
484                 ((2 * fx) / 2, [fx], [fxv], {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
485                 ((2 * dv) / 2, [dv], [dvv], 'float64'),
486                 ((2 * fv) / 2, [fv], [fvv], {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
487                 ]):
488             if isinstance(out_dtype, dict):
489                 out_dtype = out_dtype[config.cast_policy]
490             f = compile.function(list(sym_inputs), g, mode=mode)
491             out = f(*val_inputs)
492             utt.assert_allclose(out, val_inputs[0])
493             topo = f.maker.fgraph.toposort()
494             assert len(topo) == 1
495             topo[0].op == deep_copy_op
496             assert(out_dtype == out.dtype)
497         for id, (g, sym_inputs, val_inputs, out_dtype) in enumerate([
498                 (dx / abs(dx), [dx], [0.5 - dxv], 'float64'),
499                 (fx / abs(fx), [fx], [0.5 - fxv], 'float32'),
500                 (dx / abs(dx), [dx], [0.1 * dxv], 'float64'),
501                 (fx / abs(fx), [fx], [0.1 * fxv], 'float32'),
502                 (dv / abs(dv), [dv], [0.5 - dvv], 'float64'),
503                 (fv / abs(fv), [fv], [0.5 - fvv], 'float32'),
504                 ]):
505             f = compile.function(list(sym_inputs), g, mode=mode)
506             out = f(*val_inputs)
507             assert np.all(np.isfinite(out))
508             utt.assert_allclose(out, np.sign(val_inputs[0]))
509             assert(out_dtype == out.dtype)
510             assert len(f.maker.fgraph.toposort()) == 1
511         for id, (g, sym_inputs, val_inputs, out_dtype) in enumerate([
512                 ((2 * dx) / (3 * abs(dx)), [dx], [0.5 - dxv], 'float64'),
513                 ((2 * fx) / (3 * abs<font color="#c58917"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(fx)), [fx], [0.5 - fxv],
514                     {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
515                 ((2 * dx) / (3 * abs(dx)), [dx], [0.1 * dxv], 'float64'),
516                 ((2 * fx) / (3 * abs(fx)), [fx], [0.1 * fxv],
517                     {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
518                 ((2 * dv) / (3 * abs(dv)), [dv], [0.5 - dvv], 'float64'),
519                 ((2 * fv) / (3 * abs(</b></font>fv)), [fv], [0.5 - fvv],
520                     {'custom': 'float32', 'numpy+floatX': config.floatX, 'numpy': 'float64'}),
521                 ]):
522             if isinstance(out_dtype, dict):
523                 out_dtype = out_dtype[config.cast_policy]
524             f = compile.function(list(sym_inputs), g,
525                                  mode=mode)
526             topo = f.maker.fgraph.toposort()
527             out = f(*val_inputs)
528             assert np.all(np.isfinite(out))
529             utt.assert_allclose(out, np.sign(val_inputs[0]) * 2 / 3)
530             assert(out_dtype == out.dtype)
531     def test_abs_mul_div(self):
532         x = T.dscalar()
533         if theano.config.mode == 'FAST_COMPILE':
534             mode = theano.compile.mode.get_mode('FAST_RUN').excluding(
535                 "local_elemwise_fusion")
536         else:
537             mode = theano.compile.mode.get_default_mode().excluding(
538                 "local_elemwise_fusion")
539         f = theano.function([x], [(4 * x) / abs(2 * x)], mode=mode)
540         print(f.maker.fgraph.toposort())
541         print()
542         f(.1)
543         f(-1)
544         if not isinstance(mode, theano.compile.debugmode.DebugMode):
545             assert np.isfinite(f(0))
546         assert len(f.maker.fgraph.toposort()) == 2
547         assert f.maker.fgraph.toposort()[0].op == T.sgn
548         f = theano.function([x], [(4 * x) / abs(x / 2)], mode=mode)
549         print(f.maker.fgraph.toposort())
550         print()
551         f(.1)
552         f(-1)
553         if not isinstance(mode, theano.compile.debugmode.DebugMode):
554             assert np.isfinite(f(0))
555         assert len(f.maker.fgraph.toposort()) == 2
556         assert f.maker.fgraph.toposort()[0].op == T.sgn
557     def test_multiple_case_that_fail(self):
558         raise SkipTest("Current implementation of Canonizer does not "
559                        "implement all cases. Skip the corresponding test.")
560         shp = (4, 4)
561         fx, fy, fz = fmatrices('xyz')
562         dx, dy, dz = dmatrices('xyz')
563         fxv = theano._asarray(np.random.rand(*shp), dtype='float32')
564         fyv = theano._asarray(np.random.rand(*shp), dtype='float32')
565         fzv = theano._asarray(np.random.rand(*shp), dtype='float32')
566         dxv = theano._asarray(np.random.rand(*shp), dtype='float32')
567         dyv = theano._asarray(np.random.rand(*shp), dtype='float32')
568         dzv = theano._asarray(np.random.rand(*shp), dtype='float32')
569         mode = compile.mode.get_default_mode()
570         opt = gof.Query(["canonicalize"])
571         opt = opt.excluding('local_elemwise_fusion')
572         mode = mode.__class__(linker=mode.linker, optimizer=opt)
573         for (g, sym_inputs, val_inputs, out_dtype) in [
574                 ((dx / dy) / dz, [dx, dy, dz], [dxv, dyv, dzv], 'float64'),
575                 ((fx / fy) / fz, [fx, fy, fz], [fxv, fyv, fzv], 'float32')
576                 ]:
577             f = compile.function(list(sym_inputs), g, mode=mode)
578             out = f(*val_inputs)
579             utt.assert_allclose(out, val_inputs[0] / val_inputs[1] / val_inputs[2])
580             topo = f.maker.fgraph.toposort()
581             assert len(topo) == 2
582             assert isinstance(topo[0].op, (T.Elemwise, ))
583             assert isinstance(topo[0].op.scalar_op, theano.scalar.basic.Inv)
584             assert len(topo[0].inputs) == 1
585             assert(out_dtype == out.dtype)
586         for (g, sym_inputs, val_inputs, out_dtype) in [
587                 (dx / (dy / dz), [dx, dy, dz], [dxv, dyv, dzv], 'float64'),
588                 (fx / (fy / fz), [fx, fy, fz], [fxv, fyv, fzv], 'float32')
589                 ]:
590             f = compile.function(list(sym_inputs), g,
591                                  mode=mode)
592             out = f(*val_inputs)
593             utt.assert_allclose(out, val_inputs[0] / (val_inputs[1] / val_inputs[2]))
594             topo = f.maker.fgraph.toposort()
595             assert len(topo) == 2
596             assert isinstance(topo[0].op, (T.Elemwise, ))
597             assert isinstance(topo[0].op.scalar_op, theano.scalar.basic.Inv)
598             assert len(topo[0].inputs) == 1
599             assert(out_dtype == out.dtype)
600     def test_dont_merge_if_multiple_client(self):
601         raise SkipTest("Not implemented")
602     def test_canonicalize_nan(self):
603         sio = StringIO()
604         handler = logging.StreamHandler(sio)
605         handler.setLevel(logging.ERROR)
606         logging.getLogger('theano.gof.opt').addHandler(handler)
607         try:
608             x = vector()
609             theano.function([x], x + np.nan)
610         finally:
611             logging.getLogger('theano.gof.opt').removeHandler(handler)
612         assert not sio.getvalue()
613 def test_local_merge_abs():
614     x, y, z = T.matrices('xyz')
615     x_val = np.random.rand(5, 5).astype(config.floatX)
616     y_val = np.random.rand(5, 5).astype(config.floatX)
617     z_val = np.random.rand(5, 5).astype(config.floatX)
618     mode = theano.config.mode
619     if mode == "FAST_COMPILE":
620         mode = "FAST_RUN"
621     mode = theano.compile.mode.get_mode(mode).excluding(
622         "local_elemwise_fusion")
623     f = theano.function([y, z], (abs(y * z * -2)), mode=mode)
624     f(y_val, z_val)
625     assert isinstance(f.maker.fgraph.toposort()[1].op.scalar_op, scal.Abs)
626     assert len(f.maker.fgraph.toposort()) == 2
627     f(x_val, y_val)
628     assert isinstance(f.maker.fgraph.toposort()[1].op.scalar_op, scal.Abs)
629     assert len(f.maker<font color="#947010"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.fgraph.toposort()) == 2
630 def test_merge_abs_bugfix():
631     input = T.matrix()
632     step1 = input / input.sum(0)
633     step2 =</b></font> step1 / step1.sum(1)
634     l1_norm = T.abs_(step2).sum()
635     theano.function([input], T.grad(l1_norm, input))
636 def test_mixeddiv():
637     i = iscalar()
638     d = dscalar()
639     assert 0 == function([i, d], d * (i // (i + 1)))(3, 1.0)
640 def test_const_type_in_mul_canonizer():
641     input = dmatrix()
642     w = dmatrix()
643     visb = dvector()
644     hidb = dvector()
645     betas = dvector()
646     a = dvector()
647     def sigm(x):
648         return 1. / (1 + tensor.exp(-x))
649     hid = sigm((tensor.dot(w, input) + hidb) * betas)
650     vis_gauss1 = (tensor.dot(w.T, hid) + visb) * betas / (2 * a * a)
651     vis_gauss2 = (tensor.dot(w.T, hid) + visb) * betas / (2. * a * a)
652     f1 = function([input, w, visb, hidb, betas, a], vis_gauss1)
653     f2 = function([input, w, visb, hidb, betas, a], vis_gauss2)
654     ival = np.random.rand(5, 5)
655     wval = np.random.rand(5, 5)
656     visbval = np.random.rand(5)
657     hidbval = np.random.rand(5)
658     betaval = np.random.rand(5)
659     aval = np.random.rand(5)
660     utt.assert_allclose(
661         f2(ival, wval, visbval, hidbval, betaval, aval),
662         f1(ival, wval, visbval, hidbval, betaval, aval))
663 def test_cast_in_mul_canonizer():
664     x, y = tensor.vectors('xy')
665     m = tensor.minimum(x, y)
666     o = m.sum()
667     go = tensor.fill(o, 1)
668     e = tensor.eq(go, x)
669     o1 = (1 - e) * go
670     o2 = e * go
671     mode = theano.compile.get_default_mode().excluding('fusion').including('fast_run')
672     f = theano.function([x, y], [o1, o2], mode=mode)
673     theano.printing.debugprint(f, print_type=True)
674     nodes = f.maker.fgraph.apply_nodes
675     assert len([n for n in nodes if isinstance(getattr(n.op, 'scalar_op', None),
676                                                theano.scalar.Identity)]) == 0
677     assert len([n for n in nodes if isinstance(getattr(n.op, 'scalar_op'),
678                                                theano.scalar.Cast)]) == 1
679     f([1], [1])
680 class test_fusion(unittest.TestCase):
681     mode = copy.copy(compile.mode.get_default_mode())
682     _shared = staticmethod(shared)
683     topo_exclude = ()
684     def do(self, mode, shared_fn, shp, nb_repeat=1, assert_len_topo=True, slice=None):
685         def my_init(shp, dtype='float64', num=0):
686             ret = np.zeros(shp, dtype=dtype) + num
687             return ret
688         fw, fx, fy, fz = [theano.tensor.tensor(dtype='float32',
689                                                broadcastable=[False] * len(shp),
690                                                name=n) for n in 'wxyz']
691         dw, dx, dy, dz = [theano.tensor.tensor(dtype='float64',
692                                                broadcastable=[False] * len(shp),
693                                                name=n) for n in 'wxyz']
694         ix, iy, iz = [theano.tensor.tensor(dtype='int32',
695                                            broadcastable=[False] * len(shp),
696                                            name=n) for n in 'xyz']
697         fv = fvector('v')
698         fs = fscalar('s')
699         fwv = my_init(shp, 'float32', 1)
700         fxv = my_init(shp, 'float32', 2)
701         fyv = my_init(shp, 'float32', 3)
702         fzv = my_init(shp, 'float32', 4)
703         fvv = theano._asarray(np.random.rand(shp[0]), dtype='float32')
704         dwv = my_init(shp, 'float64', 5)
705         ixv = theano._asarray(my_init(shp, num=60), dtype='int32')
706         iyv = theano._asarray(my_init<font color="#800517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(shp, num=70), dtype='int32')
707         izv = theano._asarray(my_init(shp, num=70), dtype='int32')
708         fwx = fw + fx
709         ftanx = theano.</b></font>tensor.tan(fx)
710         cases = [
711             (fx + fy + fz, (fx, fy, fz), (fxv, fyv, fzv), 1, fxv +
712                 fyv + fzv, 'float32'),  # 0
713             (fx * fy * fz, (fx, fy, fz), (fxv, fyv, fzv), 1, fxv *
714                 fyv * fzv, 'float32'),  # 1
715             (fx + fy * fz, (fx, fy, fz), (fxv, fyv, fzv), 1, fxv +
716                 fyv * fzv, 'float32'),  # 2
717             (fx * fy + fz, (fx, fy, fz), (fxv, fyv, fzv), 1, fxv *
718                 fyv + fzv, 'float32'),  # 3
719             (fw + fx + fy + fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
720                 fwv + fxv + fyv + fzv, 'float32'),
721             ((fw + fx) + (fy + fz), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
722                 fwv + fxv + fyv + fzv, 'float32'),  # 5
723             (((fw + fx) + fy) + fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
724                 fwv + fxv + fyv + fzv, 'float32'),
725             ((fw + (fx + fy)) + fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
726                 fwv + fxv + fyv + fzv, 'float32'),
727             ((fw + (fx + fy) + fz), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
728                 fwv + fxv + fyv + fzv, 'float32'),
729             (fw + (fx + (fy + fz)), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
730                 fwv + fxv + fyv + fzv, 'float32'),
731             ((fw + fx) + (fy + fz), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
732                 fwv + fxv + fyv + fzv, 'float32'),  # 10
733             (fw * fx * fy * fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
734                 fwv * fxv * fyv * fzv, 'float32'),
735             (fw + fx * fy * fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
736                 fwv + fxv * fyv * fzv, 'float32'),
737             (fx + fy * fz * fx, (fx, fy, fz), (fxv, fyv, fzv), 1,
738                 fxv + fyv * fzv * fxv, 'float32'),
739             (fx * fy + fz + fy, (fx, fy, fz), (fxv, fyv, fzv), 1,
740                 fxv * fyv + fzv + fyv, 'float32'),
741             (fx * fy * fz * fw + fx + fy + fz + fw, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv), 1,
742                 fxv * fyv * fzv * fwv + fxv + fyv + fzv + fwv, 'float32'),  # 15
743             ((fw + fx) + (fy + fz) + 2., (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
744                 1, fwv + fxv + fyv + fzv + 2, 'float32'),
745             (((fw + fx) + 2. + fy) + fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
746                 1, fwv + fxv + fyv + fzv + 2, 'float32'),
747             ((fw + (fx + 2. + fy)) + fz, (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
748                 1, fwv + fxv + fyv + fzv + 2, 'float32'),
749             ((fw + (fx + fy) + 2 + fz), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
750                 1, fwv + fxv + fyv + fzv + 2, 'float32'),
751             (fw + (fx + (fy + fz) + 2.), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
752                 1, fwv + fxv + fyv + fzv + 2, 'float32'),  # 20
753             (2 + (fw + fx) + (fy + fz), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
754                 1, fwv + fxv + fyv + fzv + 2, 'float32'),
755             (2 + (dw + fx) + (fy + fz), (dw, fx, fy, fz), (dwv, fxv, fyv, fzv),
756                 1, dwv + fxv + fyv + fzv + 2, 'float64'),
757             (2 + (fw + dw) + (fy + fz), (fw, dw, fy, fz), (fwv, dwv, fyv, fzv),
758                 1, fwv + dwv + fyv + fzv + 2, 'float64'),
759             (2 + (fw + fx) + (dw + fz), (fw, fx, dw, fz), (fwv, fxv, dwv, fzv),
760                 1, fwv + fxv + dwv + fzv + 2, 'float64'),
761             (2 + (fw + fx) + (fy + dw), (fw, fx, fy, dw), (fwv, fxv, fyv, dwv),
762                 1, fwv + fxv + fyv + dwv + 2, 'float64'),  # 25
763             ((fwx.sum()) + (fwx) + (fy + fz), (fw, fx, fy, fz), (fwv, fxv, fyv, fzv),
764                 4, (fwv + fxv).sum() + fwv + fxv + fyv + fzv, 'float32'),
765             (fx + fy + tensor.cos(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
766                 fxv + fyv + np.cos(fzv), 'float32'),
767             (fx + fy + tensor.cosh(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
768                 fxv + fyv + np.cosh(fzv), 'float32'),
769             (fx + fy + abs(fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv + fyv +
770                 np.absolute(fzv), 'float32'),
771             (ix + iy + abs(iz), (ix, iy, iz), (ixv, iyv, izv), 1, ixv + iyv +
772                 np.absolute(izv), 'int32'),  # 30
773             (fx + fy + theano.tensor.log(fz), (fx, fy, fz), (fxv, fyv, fzv),
774                 1, fxv + fyv + np.log(fzv), 'float32'),
775             (fx + fy + theano.tensor.log2(fz), (fx, fy, fz), (fxv, fyv, fzv),
776                 1, fxv + fyv + np.log2(fzv), 'float32'),
777             (fx + fy + theano.tensor.log10(fz), (fx, fy, fz), (fxv, fyv, fzv),
778                 1, fxv + fyv + np.log10(fzv), 'float32'),
779             (fx + fy ** fz, (fx, fy, fz), (fxv, fyv, fzv), 1, fxv + fyv ** fzv,
780                 'float32'),  # pow
781             (fx + fy + theano.tensor.exp(fz), (fx, fy, fz), (fxv, fyv, fzv),
782                 1, fxv + fyv + np.exp(fzv), 'float32'),  # 35
783             (fx - fy - fz, (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - fyv - fzv, 'float32'),
784             (fx - (fy / fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (fyv / fzv), 'float32'),
785             (fx - theano.tensor.true_div(fy, 2), (fx, fy), (fxv, fyv),
786                 1, fxv - (fyv / 2), 'float32'),
787             (fx - theano.tensor.true_div(fy, fz), (fx, fy, fz), (fxv, fyv, fzv),
788                 1, fxv - (fyv / fzv), 'float32'),
789             (fx - theano.tensor.int_div(ix * 100, iy * 1000), (fx, ix, iy), (fxv, ixv, iyv),
790                 1, fxv - ((ixv * 100) // (iyv * 1000)), {'custom': 'float64', 'numpy + floatX': config.floatX, 'numpy': 'float64'}),  # 40
791             (fx - (fy / 2), (fx, fy), (fxv, fyv), 1, fxv - (fyv / 2), 'float32'),
792             (fx - (fy % fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (fyv % fzv), 'float32'),
793             (fx - (fy &gt; fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (fyv &gt; fzv), 'float32'),
794             (fx - (fy &gt;= fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (fyv &gt;= fzv), 'float32'),
795             (fx - (fy &lt; fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (fyv &lt; fzv), 'float32'),  # 45
796             (fx - (fy &lt;= fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (fyv &lt;= fzv), 'float32'),
797             (fx - T.eq(fy, fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
798                 fxv - (fyv == fzv), 'float32'),
799             (fx - T.neq(fy, fz), (fx, fy, fz), (fxv, fyv, fzv), 1, fxv - (
800                 fyv != fzv), 'float32'),
801             (fx - fy + tensor.tan(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
802                 fxv - fyv + np.tan(fzv), 'float32'),
803             (fx - fy + tensor.tanh(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
804                 fxv - fyv + np.tanh(fzv), 'float32'),  # 50
805             (fx - fy + tensor.sin(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
806                 fxv - fyv + np.sin(fzv), 'float32'),
807             (fx - fy + tensor.sinh(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
808                 fxv - fyv + np.sinh(fzv), 'float32'),
809             (fx - fy + theano.tensor.sqr(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
810                 fxv - fyv + (fzv * fzv), 'float32'),
811             (fx - fy + theano.tensor.sqrt(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
812                 fxv - fyv + np.sqrt(fzv), 'float32'),
813             (fx - fy + theano.tensor.inv(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
814                 fxv - fyv + (1 / fzv), 'float32'),  # 55
815             (fx - fy + theano.tensor.neg(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
816                 fxv - fyv + (-fzv), 'float32'),
817             (fx - fy + theano.tensor.round(fz), (fx, fy, fz), (fxv, fyv, fzv), 1,
818                 fxv - fyv + np.round(fzv), 'float32'),
819             (ix - iy + theano.tensor.iround(fz), (ix, iy, fz), (ixv, iyv, fzv), 1,
820                 ixv - iyv + np.round(fzv), 'int64'),
821             (fx - theano.tensor.or_(iy, iz), (fx, iy, iz), (fxv, iyv, izv), 1,
822                 fxv - (iyv | izv), {'custom': 'float64', 'numpy + floatX': config.floatX, 'numpy': 'float64'}),
823             (fx - theano.tensor.xor(iy, iz), (fx, iy, iz), (fxv, iyv, izv), 1,
824                 fxv - (iyv ^ izv), {'custom': 'float64', 'numpy + floatX': config.floatX, 'numpy': 'float64'}),  # 60
825             (fx - theano.tensor.and_(iy, iz), (fx, iy, iz), (fxv, iyv, izv), 1,
826                 fxv - (iyv &amp; izv), {'custom': 'float64', 'numpy + floatX': config.floatX, 'numpy': 'float64'}),
827             (fx - theano.tensor.invert(iy), (fx, iy), (fxv, iyv), 1,
828                 fxv - (~iyv), {'custom': 'float64', 'numpy + floatX': config.floatX, 'numpy': 'float64'}),
829             (fx - theano.tensor.cast(fy, dtype='float64'), (fx, fy), (fxv, fyv), 1,
830                 fxv - np.asarray(fyv, 'float64'), 'float64'),
831             (theano.tensor.pow(fx * fy + fz, fx * fy), (fx, fy, fz), (fxv, fyv, fzv), 1,
832                 np.power(fxv * fyv + fzv, fxv * fyv), 'float32'),
833             (fv + fy ** fz, (fv, fy, fz), (fvv, fyv, fzv), 2, fvv + fyv ** fzv, 'float32'),  # fused with a dimshuffle #65
834             (fv - fy + tensor.tanh(fz), (fv, fy, fz), (fvv, fyv, fzv), 2,
835                 fvv - fyv + np.tanh(fzv), 'float32'),  # fused with a dimshuffle
836             (theano.tensor.mul(fx, fx, fx, fx), (fx,), (fxv,), 1, fxv *
837                 fxv * fxv * fxv, 'float32'),
838             (theano.tensor.mul(fx, ftanx, ftanx), (fx,), (fxv,), 1,
839                 fxv * np.tan(fxv) * np.tan(fxv), 'float32'),
840             (theano.tensor.mul(fx, ftanx, ftanx, fx), (fx,), (fxv,),
841                 1, fxv * np.tan(fxv) * np.tan(fxv) * fxv, 'float32'),
842             (theano.tensor.mul(ftanx, ftanx, fx + fy), (fx, fy), (fxv, fyv),
843                 1, np.tan(fxv) * np.tan(fxv) * (fxv + fyv), 'float32'),  # 70
844             (fx * theano.tensor.sin(fs), (fx, fs), (fxv, fsv), 3,
845                 fxv * np.sin(fsv), 'float32'),
846             ]
847         if slice:
848             cases = cases[slice]
849         times = np.zeros(len(cases))
850         fail1 = []
851         fail2 = []
852         fail3 = []
853         fail4 = []
854         for id, [g, sym_inputs, val_inputs,
855                  nb_elemwise, answer, out_dtype] in enumerate(cases):
856             if isinstance(out_dtype, dict):
857                 out_dtype = out_dtype[config.cast_policy]
858             if shared_fn is None:
859                 f = compile.function(list(sym_inputs), g, mode=mode)
860                 for x in xrange(nb_repeat):
861                     out = f(*val_inputs)
862                 t1 = time.time()
863             else:
864                 out = shared_fn(np.zeros(shp, dtype=out_dtype), 'out')
865                 assert out.dtype == g.dtype
866                 f = function(sym_inputs, [], updates=[(out, g)], mode=mode)
867                 t0 = time.time()
868                 for x in xrange(nb_repeat):
869                     f(*val_inputs)
870                 t1 = time.time()
871                 out = out.get_value()
872             times[id] = t1 - t0
873             atol = 1e-8
874             if out_dtype == 'float32':
875                 atol = 1e-6
876             if not np.allclose(out, answer * nb_repeat, atol=atol):
877                 fail1.append(id)
878                 print("cases", id)
879                 print(val_inputs)
880                 print(out)
881                 print(answer * nb_repeat)
882             topo = f.maker.fgraph.toposort()
883             topo_ = [n for n in topo
884                      if not isinstance(n.op, self.topo_exclude)]
885             if assert_len_topo:
886                 if not len(topo_) == nb_elemwise:
887                     fail3.append((id, topo_, nb_elemwise))
888                 if nb_elemwise == 1:
889                     if len(set(g.owner.inputs)) == len(g.owner.inputs):
890                         expected_len_sym_inputs = np.sum(
891                             [not isinstance(x, theano.gof.Constant)
892                              for x in topo_[0].inputs])
893                         assert expected_len_sym_inputs == len(sym_inputs)
894             if not out_dtype == out.dtype:
895                 fail4.append((id, out_dtype, out.dtype))
896         failed = len(fail1 + fail2 + fail3 + fail4)
897         if failed &gt; 0:
898             print("Executed", len(cases), "cases", "failed", failed)
899             raise Exception("Failed %d cases" % failed, fail1,
900                             fail2, fail3, fail4)
901         return times
902     def test_elemwise_fusion(self):
903         shp = (5, 5)
904         mode = copy.copy(self.mode)
905         mode._optimizer = mode._optimizer.including(
906             'local_elemwise_fusion', 'composite_elemwise_fusion',
907             'canonicalize')
908         self.do(mode, self._shared, shp)
909     @attr('slow')
910     def test_elemwise_fusion_4d(self):
911         shp = (3, 3, 3, 3)
912         mode = copy.copy(self.mode)
913         mode._optimizer = mode._optimizer.including(
914             'local_elemwise_fusion', 'composite_elemwise_fusion',
915             'canonicalize')
916         self.do(mode, self._shared, shp, slice=slice(0, 1))
917     def test_fusion_35inputs(self):
918         inpts = vectors(['i%i' % i for i in xrange(35)])
919         out = tensor.sin(inpts[0])
920         for idx in xrange(1, 35):
921             out = tensor.sin(inpts[idx] + out)
922         f = function(inpts, out, mode=self.mode)
923         f(*[list(range(i, 4 + i)) for i in xrange(35)])
924     def test_pickle_big_fusion(self):
925         if not theano.config.cxx:
926             raise SkipTest("no c compiler, so can't use big elemwise!")
927         factors = []
928         sd = tensor.dscalar()
929         means = tensor.dvector()
930         cst_05 = theano.tensor.constant(.5)
931         cst_m05 = theano.tensor.constant(-.5)
932         cst_2 = theano.tensor.constant(2)
933         cst_m2 = theano.tensor.constant(-2)
934         ones = theano.tensor.constant(np.ones(10))
935         n = 85
936         if theano.config.mode in ["DebugMode", "DEBUG_MODE"]:
937             n = 10
938         for i in xrange(n):
939             f = (cst_m05 * sd ** cst_m2 * (ones - means[i]) ** cst_2 +
940                  cst_05 * tensor.log(cst_05 * (sd ** cst_m2) / np.pi))
941             factors.append(tensor.sum(f))
942         logp = tensor.add(*factors)
943         vars = [sd, means]
944         dlogp = function(vars, [theano.grad(logp, v) for v in vars])
945         dlogp(2, np.random.rand(n))
946     def speed_fusion(self, s=None):
947         shp = (3000, 3000)
948         shp = (1000, 1000)
949         nb_repeat = 50
950         mode1 = copy.copy(self.mode)
951         mode1._optimizer = mode1._optimizer.including('local_elemwise_fusion')
952         mode2 = copy.copy(self.mode)
953         mode2._optimizer = mode2._optimizer.excluding('local_elemwise_fusion')
954         print("test with linker", str(mode1.linker))
955         times1 = self.do(mode1, self._shared, shp, nb_repeat=nb_repeat,
956                          assert_len_topo=False, slice=s)
957         times2 = self.do(mode2, self._shared, shp, nb_repeat=nb_repeat,
958                          assert_len_topo=False, slice=s)
959         print("times1 with local_elemwise_fusion")
960         print(times1, times1.min(), times1.max(), times1.sum())
961         print("times2 without local_elemwise_fusion")
962         print(times2, times2.min(), times2.max(), times2.sum())
963         d = times2 / times1
964         print("times2/times1")
965         print(d)
966         print("min", d.min(), "argmin", d.argmin(), "max", d.max(),
967               "mean", d.mean(), "std", d.std())
968     def test_fusion_inplace(self):
969         mode = copy.copy(self.mode)
970         mode._optimizer = mode._optimizer.including(
971             'local_elemwise_fusion', 'composite_elemwise_fusion',
972             'canonicalize', 'inplace')
973         x, y, z = dmatrices('xyz')
974         f = theano.function([x, y, z], tensor.dot(x, y) + x + y + z, mode=mode)
975         topo = [n for n in f.maker.fgraph.toposort()
976                 if not isinstance(n.op, self.topo_exclude)]
977         assert len(topo) == 2
978         assert topo[-1].op.inplace_pattern
979         f(np.random.random((5, 5)), np.random.random((5, 5)),
980             np.random.random((5, 5)))
981     def speed_log_exp(self):
982         s = slice(31, 36)
983         print("time", self.do(self.mode, self._shared, shp=(1000, 1000),
984                               assert_len_topo=False, slice=s, nb_repeat=100))
985 class TimesN(theano.scalar.basic.UnaryScalarOp):
986     def __eq__(self, other):
987         return super(TimesN, self).__eq__(other) and self.n == other.n
988     def __hash__(self):
989         return super(TimesN, self).__hash__() ^ hash(self.n)
990     def __init__(self, n, *args, **kwargs):
991         self.n = n
992         theano.scalar.basic.UnaryScalarOp.__init__(self, *args, **kwargs)
993     def impl(self, x):
994         return x * self.n
995     def c_support_code_apply(self, node, nodename):
996         n = str(self.n)
997         return """
998         float %(nodename)s_timesn(float x) { return x * %(n)s; }
999     Test The Composite Ops code generation in a case where there is multiple
1000     scalar ops with support code.
1001     """
1002         assert (<font color="#b041ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>f1(xval) == xval[:2, :5]).all()
1003         y = tensor.tensor4('x')
1004         yval = np.random.rand(</b></font>1, 10, 1, 3).astype(config.floatX)
1005         assert y.broadcastable == (False, False, False, False)
1006         newy = tensor.Rebroadcast((0, True), (2, True))(y)
1007         assert newy.broadcastable == (True, False, True, False)
1008         f2 = function([y], newy[:, 3, 0, :], mode=mode_opt)
1009         self.assertTrue(check_stack_trace(f2, ops_to_check=[
1010                         Subtensor, tensor.Rebroadcast]))
1011         prog = f2.maker.fgraph.toposort()
1012         assert isinstance(prog[0].op, tensor.Subtensor)
1013         assert isinstance(prog[1].op, tensor.Rebroadcast)
1014         assert (f2(yval) == yval[:, 3, 0, :]).all()
1015         f3 = function([y], newy[:, 3, 0], mode=mode_opt)
1016         self.assertTrue(check_stack_trace(f3, ops_to_check=[
1017             Subtensor, tensor.Rebroadcast]))
1018         prog = f3.maker.fgraph.toposort()
1019         assert isinstance(prog[0].op, tensor.Subtensor)
1020         assert isinstance(prog[1].op, tensor.Rebroadcast)
1021         assert (f3(yval) == yval[:, 3, 0]).all()
1022         z = tensor.tensor4('x')
1023         zval = np.random.rand(4, 10, 3, 1).astype(config.floatX)
1024         assert z.broadcastable == (False, False, False, False)
1025         newz = tensor.Rebroadcast((3, True))(z)
1026         assert newz.broadcastable == (False, False, False, True)
1027         f4 = function([z], newz[:, 3, 0], mode=mode_opt)
1028         self.assertTrue(check_stack_trace(f4, ops_to_check=[
1029             Subtensor, tensor.Rebroadcast]))
1030         prog = f4.maker.fgraph.toposort()
1031         assert isinstance(prog[0].op, tensor.Subtensor)
1032         assert isinstance(prog[1].op, tensor.Rebroadcast)
1033         assert (f4(zval) == zval[:, 3, 0]).all()
1034 class test_local_subtensor_merge(unittest.TestCase):
1035     def setUp(self):
1036         utt.seed_rng()
1037         self.x_shapes = [(2, 2), (5, 3), (4, 1), (1, 2),
1038                          (0, 2), (2, 0), (1, 0), (0, 0)]
1039         self.rng = np.random.RandomState(seed=utt.fetch_seed())
1040     def test_const(self):
1041         x = tensor.matrix('x')
1042         for idx in xrange(-7, 6):
1043             f = function([x], x[idx::][-1], mode=mode_opt)
1044             g = function([x], x[idx::][-1], mode=mode_opt.excluding(
1045                 'local_subtensor_merge'))
1046             self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1047             topo = f.maker.fgraph.toposort()
1048             assert len([t for t in topo
1049                         if isinstance(t.op, tensor.Subtensor)]) == 1
1050             assert isinstance(topo[-1].op, DeepCopyOp)
1051             for x_s in self.x_shapes:
1052                 x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1053                 if idx &lt; x_s[0] and x_s[0] &gt; 0:
1054                     f(x_val)  # let debugmode test something
1055                 else:
1056                     self.assertRaises(IndexError, f, x_val)
1057                     self.assertRaises(IndexError, g, x_val)
1058     def test_scalar(self):
1059         x = tensor.matrix('x')
1060         y = tensor.iscalar('y')
1061         f = function([x, y], x[y::][-1], mode=mode_opt)
1062         g = function([x, y], x[y::][-1],
1063                      mode=mode_opt.excluding('local_subtensor_merge'))
1064         self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1065         topo = f.maker.fgraph.toposort()
1066         assert len([t for t in topo
1067                     if isinstance(t.op, tensor.Subtensor)]) == 1
1068         assert isinstance(topo[-1].op, DeepCopyOp)
1069         for x_s in self.x_shapes:
1070             x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1071             for idx in xrange(-9, 8):
1072                 if (idx &lt; x_s[0]) and (x_s[0] &gt; 0):
1073                     f(x_val, idx)  # let debugmode test something
1074                 else:
1075                     self.assertRaises(IndexError, f, x_val, idx)
1076                     self.assertRaises(IndexError, g, x_val, idx)
1077     @attr('slow')
1078     def test_const2(self):
1079         x = tensor.matrix('x')
1080         for idx in xrange(-8, 7):
1081             f = function([x], x[::-1][idx], mode=mode_opt)
1082             g = function([x], x[::-1][idx],
1083                          mode=mode_opt.excluding('local_subtensor_merge'))
1084             self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1085             topo = f.maker.fgraph.toposort()
1086             assert len([t for t in topo
1087                         if isinstance(t.op, tensor.Subtensor)]) == 1
1088             assert isinstance(topo[-1].op, DeepCopyOp)
1089             for x_s in self.x_shapes:
1090                 x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1091                 if (idx &lt; x_s[0]) and (idx &gt;= -x_s[0]):
1092                     f(x_val)  # let debugmode test something
1093                 else:
1094                     self.assertRaises(IndexError, f, x_val)
1095                     self.assertRaises(IndexError, g, x_val)
1096     def test_scalar2(self):
1097         x = tensor.matrix('x')
1098         y = tensor.iscalar('y')
1099         f = function([x, y], x[::-1][y], mode=mode_opt)
1100         g = function([x, y], x[::-1][y],
1101                      mode=mode_opt.excluding('local_subtensor_merge'))
1102         self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1103         topo = f.maker.fgraph.toposort()
1104         assert len([t for t in topo
1105                     if isinstance(t.op, tensor.Subtensor)]) == 1
1106         assert isinstance(topo[-1].op, DeepCopyOp)
1107         for x_s in self.x_shapes:
1108             x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1109             for idx in xrange(-x_s[0], x_s[0]):
1110                 f(x_val, idx)  # let debugmode test something
1111             for idx in (list(range(x_s[0], 9)) + list(range(-9, -x_s[0]))):
1112                 self.assertRaises(IndexError, f, x_val, idx)
1113                 self.assertRaises(IndexError, g, x_val, idx)
1114     def test_const3(self):
1115         x = tensor.matrix('x')
1116         for idx in xrange(-9, 8):
1117             f = function([x], x[::-1][:idx], mode=mode_opt)
1118             self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1119             topo = f.maker.fgraph.toposort()
1120             assert len([t for t in topo
1121                         if isinstance(t.op, tensor.Subtensor)]) == 1
1122             assert isinstance(topo[-1].op, DeepCopyOp)
1123             for x_s in self.x_shapes:
1124                 x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1125                 f(x_val)  # let debugmode test something
1126     def test_scalar3(self):
1127         x = tensor.matrix('x')
1128         y = tensor.iscalar('y')
1129         f = function([x, y], x[::-1][:y], mode=mode_opt)
1130         self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1131         topo = f.maker.fgraph.toposort()
1132         assert len([t for t in topo
1133                     if isinstance(t.op, tensor.Subtensor)]) == 1
1134         assert isinstance(topo[-1].op, DeepCopyOp)
1135         for x_s in self.x_shapes:
1136             x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1137             for idx in xrange(-7, 7):
1138                 f(x_val, idx)  # let debugmode test something
1139     def test_const4(self):
1140         x = tensor.matrix('x')
1141         for idx1 in xrange(-7, 7):
1142             for idx2 in xrange(-7, 7):
1143                 f = function([x], x[idx1:][:idx2], mode=mode_opt)
1144                 self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1145                 topo = f.maker.fgraph.toposort()
1146                 assert len([t for t in topo
1147                             if isinstance(t.op, tensor.Subtensor)]) == 1
1148                 assert isinstance(topo[-1].op, DeepCopyOp)
1149                 for x_s in self.x_shapes:
1150                     x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1151                     f(x_val)  # let debugmode test something
1152     def test_scalar4(self):
1153         x = tensor.matrix('x')
1154         y = tensor.iscalar('y')
1155         z = tensor.iscalar('y')
1156         f = function([x, y, z], x[y:][:z], mode=mode_opt)
1157         self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1158         topo = f.maker.fgraph.toposort()
1159         assert len([t for t in topo
1160                     if isinstance(t.op, tensor.Subtensor)]) == 1
1161         assert isinstance(topo[-1].op, DeepCopyOp)
1162         for x_s in self.x_shapes:
1163             x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1164             for idx1 in xrange(-11, 11):
1165                 for idx2 in xrange(-11, 11):
1166                     f(x_val, idx1, idx2)  # let debugmode test something
1167     def test_const_general(self):
1168         cases = [
1169             ((2, 3), (None, None, None), (None, None, -1)),
1170             ((12, 1), (None, None, -4), (None, None, 1)),
1171             ((5, 3), (1, 4, 2), (None, None, -1)),
1172         ]
1173         x = tensor.matrix('x')
1174         for shape, sl1, sl2 in cases:
1175             z = x[slice(*sl1)][slice(*sl2)]
1176             f = function([x], z, mode=mode_opt)
1177             self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1178             x_val = self.rng.uniform(size=shape).astype(config.floatX)
1179             f(x_val)
1180     def test_scalar5(self):
1181         x = tensor.matrix('x')
1182         b1 = tensor.iscalar('b1')
1183         e1 = tensor.iscalar('e1')
1184         s1 = tensor.iscalar('s1')
1185         b2 = tensor.iscalar('b2')
1186         e2 = tensor.iscalar('e2')
1187         s2 = tensor.iscalar('s2')
1188         f = function([x, b1, e1, s1, b2, e2, s2], x[b1:e1:s1][b2:e2:s2],
1189                      mode=mode_opt)
1190         self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1191         topo = f.maker.fgraph.toposort()
1192         assert len([t for t in topo if isinstance(t.op, tensor.
1193                                                   Subtensor)]) == 1
1194         assert isinstance(topo[-1].op, DeepCopyOp)
1195         b1r = self.rng.permutation(list(range(-8, 8)))[:2]
1196         e1r = self.rng.permutation(list(range(-8, 8)))[:2]
1197         b2r = self.rng.permutation(list(range(-8, 8)))[:2]
1198         e2r = self.rng.permutation(list(range(-8, 8)))[:2]
1199         s1r = self.rng.permutation([-7, -6, -5, -4, -3, -2, -1, 1,
1200                                     2, 3, 4, 5, 6, 7])[:2]
1201         s2r = self.rng.permutation([-7, -6, -5, -4, -3, -2, -1, 1,
1202                                     2, 3, 4, 5, 6, 7])[:2]
1203         for x_s in self.x_shapes:
1204             x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1205             for b1 in b1r:
1206                 for e1 in e1r:
1207                     for s1 in s1r:
1208                         for b2 in b2r:
1209                             for e2 in e2r:
1210                                 for s2 in s2r:
1211                                     f(x_val, b1, e1, s1, b2, e2, s2)
1212     def test_const5(self):
1213         data = np.asarray(np.arange(8),
1214                           dtype=theano.config.floatX)
1215         x = theano.tensor.vector('x')
1216         y = x[7:1:-1]
1217         t = theano.shared(np.int64(0))
1218         fun = theano.function([x], y[t])
1219         val = fun(data)
1220         assert val == data[7:1:-1][0]
1221     def test_const6(self):
1222         data = self.rng.uniform(size=(8, 8, 8)).astype(theano.config.floatX)
1223         x = theano.tensor.tensor3('x')
1224         nops = 1
1225         if theano.config.mode == "FAST_COMPILE":
1226             nops = 2
1227         y = x[3:6, 2:6, 1:7][1]
1228         fun = theano.function([x], y)
1229         val = fun(data)
1230         assert np.all(val == data[3:6, 2:6, 1:7][1])
1231         assert len([n for n in fun.maker.fgraph.toposort()
1232                     if isinstance(n.op, Subtensor)]) == nops
1233         y = x[2, 3][1]
1234         fun = theano.function([x], y)
1235         val = fun(data)
1236         assert np.all(val == data[2, 3][1])
1237         assert len([n for n in fun.maker.fgraph.toposort()
1238                     if isinstance(n.op, Subtensor)]) == nops
1239         y = x[3:6, 2, 1:7][1]
1240         fun = theano.function([x], y)
1241         val = fun(data)
1242         assert np.all(val == data[3:6, 2, 1:7][1])
1243         assert len([n for n in fun.maker.fgraph.toposort()
1244                     if isinstance(n.op, Subtensor)]) == nops
1245     def test_scalar6(self):
1246         x = tensor.matrix('x')
1247         b = tensor.iscalar('b')
1248         e = tensor.iscalar('e')
1249         s = tensor.iscalar('s')
1250         i = tensor.iscalar('i')
1251         f = function([x, b, e, s, i], x[b:e:s][i], mode=mode_opt)
1252         self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1253         topo = f.maker.fgraph.toposort()
1254         assert len([t for t in topo if isinstance(t.op, tensor.
1255                                                   Subtensor)]) == 1
1256         assert isinstance(topo[-1].op, DeepCopyOp)
1257         b_r = self.rng.permutation(list(range(-4, 4)))[:3]
1258         e_r = self.rng.permutation(list(range(-4, 4)))[:3]
1259         i_r = self.rng.permutation(list(range(-4, 4)))[:3]
1260         s_r = self.rng.permutation([-3, -2, -1, 1, 2, 3])[:3]
1261         for x_s in self.x_shapes:
1262             n_index_err = 0
1263             n_ok = 0
1264             x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1265             for b_v in b_r:
1266                 for e_v in e_r:
1267                     for s_v in s_r:
1268                         for i_v in i_r:
1269                             try:
1270                                 x_val[b_v:e_v:s_v][i_v]
1271                             except IndexError:
1272                                 n_index_err += 1
1273                                 self.assertRaises(IndexError,
1274                                                   f, x_val, b_v, e_v, s_v, i_v)
1275                             else:
1276                                 n_ok += 1
1277                                 f(x_val, b_v, e_v, s_v, i_v)
1278     @attr('slow')
1279     def test_none_slice(self):
1280         x = tensor.matrix('x')
1281         b1 = tensor.iscalar('b1')
1282         e1 = tensor.iscalar('e1')
1283         s1 = tensor.iscalar('s1')
1284         b2 = tensor.iscalar('b2')
1285         e2 = tensor.iscalar('e2')
1286         s2 = tensor.iscalar('s2')
1287         none_positions = np.ndindex(2, 2, 2, 2, 2, 2)
1288         b1r = self.rng.permutation(list(range(-4, 4)))[:]
1289         e1r = self.rng.permutation(list(range(-4, 4)))[:]
1290         b2r = self.rng.permutation(list(range(-4, 4)))[:]
1291         e2r = self.rng.permutation(list(range(-4, 4)))[:]
1292         s1r = self.rng.permutation([-4, -3, -2, -1, 1, 2, 3, 4])[:]
1293         s2r = self.rng.permutation([-4, -3, -2, -1, 1, 2, 3, 4])[:]
1294         scalar_vars = [b1, e1, s1, b2, e2, s2]
1295         scalar_ranges = [b1r, e1r, s1r, b2r, e2r, s2r]
1296         for none_pos in none_positions:
1297             slice_inputs = []
1298             input_vars = []
1299             values = []
1300             if sum(none_pos) == 0:
1301                 continue
1302             for i, none_i in enumerate(none_pos):
1303                 if none_i:
1304                     slice_inputs.append(None)
1305                 else:
1306                     slice_inputs.append(scalar_vars[i])
1307                     input_vars.append(scalar_vars[i])
1308                     values.append(scalar_ranges[i])
1309             slice1 = slice(*slice_inputs[:3])
1310             slice2 = slice(*slice_inputs[3:])
1311             sub_x = x[slice1][slice2]
1312             f = theano.function([x] + input_vars, sub_x, mode=mode_opt)
1313             self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor,
1314                                               bug_print='ignore'))
1315             topo = f.maker.fgraph.toposort()
1316             assert len([t for t in topo if isinstance(t.op,
1317                                                       tensor.Subtensor)]) &lt;= 1
1318             assert isinstance(topo[-1].op, DeepCopyOp)
1319             for x_s in self.x_shapes:
1320                 x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1321                 for i_val in zip(*values):
1322                     f(x_val, *i_val)
1323     def test_none_index(self):
1324         x = tensor.matrix('x')
1325         b = tensor.iscalar('b')
1326         e = tensor.iscalar('e')
1327         s = tensor.iscalar('s')
1328         i = tensor.iscalar('i')
1329         none_positions = np.ndindex(2, 2, 2, 1)
1330         b_r = self.rng.permutation(list(range(-4, 4)))[:]
1331         e_r = self.rng.permutation(list(range(-4, 4)))[:]
1332         i_r = self.rng.permutation(list(range(-4, 4)))[:]
1333         s_r = self.rng.permutation([-4, -3, -2, -1, 1, 2, 3, 4])[:]
1334         scalar_vars = [b, e, s, i]
1335         scalar_ranges = [b_r, e_r, s_r, i_r]
1336         for none_pos in none_positions:
1337             slice_inputs = []
1338             input_vars = []
1339             values = []
1340             if sum(none_pos) == 0:
1341                 continue
1342             for j, none_j in enumerate(none_pos):
1343                 if none_j:
1344                     slice_inputs.append(None)
1345                 else:
1346                     slice_inputs.append(scalar_vars[j])
1347                     input_vars.append(scalar_vars[j])
1348                     values.append(scalar_ranges[j])
1349             symbol_slice = slice(*slice_inputs[:3])
1350             sub_x = x[symbol_slice][i]
1351             f = theano.function([x] + input_vars, sub_x, mode=mode_opt)
1352             self.assertTrue(check_stack_trace(f, ops_to_check=Subtensor))
1353             topo = f.maker.fgraph.toposort()
1354             assert len([t for t in topo if isinstance(t.op,
1355                                                       tensor.Subtensor)]) &lt;= 1
1356             assert isinstance(topo[-1].op, DeepCopyOp)
1357             for x_s in self.x_shapes:
1358                 x_val = self.rng.uniform(size=x_s).astype(config.floatX)
1359                 for i_val in zip(*values):
1360                     i_val_idx = 0
1361                     num_slice_inputs = []
1362                     for none_j in none_pos:
1363                         if none_j:
1364                             num_slice_inputs.append(None)
1365                         else:
1366                             num_slice_inputs.append(i_val[i_val_idx])
1367                             i_val_idx += 1
1368                     num_slice = slice(*num_slice_inputs[:3])
1369                     num_i = num_slice_inputs[3]
1370                     try:
1371                         x_val[num_slice][num_i]
1372                     except IndexError:
1373                         self.assertRaises(IndexError, f, x_val, *i_val)
1374                     else:
1375                         f(x_val, *i_val)
1376 class test_local_adv_sub1_adv_inc_sub1(unittest.TestCase):
1377     def setUp(self):
1378         utt.seed_rng()
1379         mode = theano.compile.mode.get_default_mode()
1380         self.mode = mode.including("local_adv_sub1_adv_inc_sub1").excluding("fusion")
1381         self.mode_no_assert = self.mode.including("local_remove_all_assert")
1382     def test0(self):
1383         for dtype1, dtype2 in [("float32", "float32"),
1384                                ("float32", "float64"),
1385                                ("float64", "float32"),
1386                                ("float64", "float64")]:
1387             x = tensor.matrix(dtype=dtype1)
1388             y = tensor.matrix(dtype=dtype2)
1389             idx = tensor.ivector()
1390             dx = np.random.rand(4, 5).astype(dtype1)
1391             dy = np.random.rand(2, 5).astype(dtype2)
1392             dy = np.vstack([dy, dy[-1:]])
1393             didx = np.asarray([1, 3, 3], "int32")
1394             inc = tensor.set_subtensor(x[idx], y)
1395             o = inc[idx]
1396             f = theano.function([x, y, idx], o, self.mode_no_assert)
1397             res = f(dx, dy, didx)
1398             utt.assert_allclose(dy, res)
1399             topo = f.maker.fgraph.toposort()
1400             assert len(topo) == 1
1401             assert isinstance(topo[0].op, (compile.DeepCopyOp, T.Elemwise))
1402             inc = tensor.inc_subtensor(x[idx], y)
1403             o = inc[idx]
1404             f = theano.function([x, y, idx], o, self.mode_no_assert)
1405             res = f(dx, dy, didx)
1406             _dx = dx.copy()
1407             np.add.at(_dx, didx, dy)
1408             utt.assert_allclose(_dx[didx], res)
1409             topo = f.maker.fgraph.toposort()
1410             len(topo) == 2
1411             inc = tensor.inc_subtensor(x.zeros_like()[idx], y)
1412             o = inc[idx]
1413             f = theano.function([x, y, idx], o, self.mode_no_assert)
1414             res = f(dx, dy, didx)
1415             utt.assert_allclose(np.vstack([dy[0], 2 * dy[1], 2 * dy[2]]), res)
1416     def test_assert(self):
1417             x = tensor.matrix("x")
1418             y = tensor.matrix("y")
1419             idx = tensor.ivector()
1420             dx = np.random.rand(4, 5).astype(config.floatX)
1421             dy = np.random.rand(2, 5).astype(config.floatX)
1422             inc = tensor.set_subtensor(x[idx], y)
1423             o = inc[idx]
1424             f = theano.function([x, y, idx], o, self.mode)
1425             for i in [dx.shape[0], -dx.shape[0] - 1]:
1426                 self.assertRaises((AssertionError, IndexError),
1427                                   f, dx, dy, [i, i])
1428             self.assertRaises((AssertionError, ValueError),
1429                               f, dx, dy, [1])
1430     def test_stack_trace(self):
1431         x = tensor.matrix("x")
1432         ys = [tensor.matrix("y"), tensor.dmatrix("y")]
1433         idx = tensor.ivector()
1434         incs = [tensor.set_subtensor(x[idx], y) for y in ys]
1435         outs = [inc[idx] for inc in incs]
1436         for y, out in zip(ys, outs):
1437             f = theano.function([x, y, idx], out, self.mode)
1438             self.assertTrue(check_stack_trace(
1439                 f, ops_to_check=(Assert, scal.Cast)))
1440 class Test_alloc_zero(unittest.TestCase):
1441     def setUp(self):
1442         mode = theano.compile.mode.get_default_mode()
1443         self.mode = mode.including("local_incsubtensor_of_zeros",
1444                                    "local_setsubtensor_of_constants",
1445                                    "local_0_dot_x")
1446     def test_setsubtensor_allocs0(self):
1447         x = tensor.matrix()
1448         y = tensor.matrix()
1449         x0 = tensor.zeros_like(x)
1450         y0 = tensor.zeros_like(y)
1451         z = tensor.set_subtensor(x0[:4], y0)
1452         f = theano.function([x, y], z, mode=self.mode)
1453         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1454                        for n in f.maker.fgraph.toposort()])
1455     def test_setsubtensor_allocs1(self):
1456         y = tensor.matrix()
1457         x0 = tensor.constant(np.asarray(np.zeros((4, 4)),
1458                                         dtype=config.floatX))
1459         y0 = tensor.zeros_like(y)
1460         z = tensor.set_subtensor(x0[:4], y0)
1461         f = theano.function([y], z, mode=self.mode)
1462         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1463                        for n in f.maker.fgraph.toposort()])
1464     def test_setsubtensor_allocs1t(self):
1465         y = tensor.matrix()
1466         x0 = tensor.constant(np.asarray(np.zeros((4, 4)),
1467                                         dtype=config.floatX))
1468         y0 = tensor.zeros_like(y)
1469         z = tensor.set_subtensor(x0[:4], y0.T)
1470         f = theano.function([y], z, mode=mode_opt)
1471         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1472                       for n in f.maker.fgraph.toposort()])
1473     def test_setsubtensor_allocs2(self):
1474         x = tensor.matrix()
1475         y0 = tensor.constant(np.asarray(np.zeros_like((4, 4)),
1476                                         dtype=config.floatX))
1477         x0 = tensor.zeros_like(x)
1478         z = tensor.set_subtensor(x0[:4], y0)
1479         f = theano.function([x], z, mode=self.mode)
1480         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1481                        for n in f.maker.fgraph.toposort()])
1482     def test_incsubtensor_allocs0(self):
1483         x = tensor.matrix()
1484         y = tensor.matrix()
1485         y0 = tensor.zeros_like(y)
1486         z = tensor.inc_subtensor(x[:4], y0)
1487         f = theano.function([x, y], z, mode=self.mode)
1488         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1489                        for n in f.maker.fgraph.toposort()])
1490     def test_incsubtensor_allocs0t(self):
1491         x = tensor.matrix()
1492         y = tensor.matrix()
1493         y0 = tensor.zeros_like(y)
1494         z = tensor.inc_subtensor(x[:4], y0.T)
1495         f = theano.function([x, y], z, mode=mode_opt)
1496         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1497                        for n in f.maker.fgraph.toposort()])
1498     def test_incsubtensor_allocs1(self):
1499         x = tensor.matrix()
1500         y0 = tensor.constant(np.asarray(np.zeros_like((4, 4)),
1501                                         dtype=config.floatX))
1502         z = tensor.inc_subtensor(x[:4], y0)
1503         f = theano.function([x], z, mode=self.mode)
1504         assert np.all([not isinstance(n.op, tensor.IncSubtensor)
1505                        for n in f.maker.fgraph.toposort()])
1506     def test_incsubtensor_x_zeros(self):
1507         x = tensor.constant(np.asarray(np.zeros((4, 4)),
1508                                        dtype=config.floatX))
1509         y = tensor.matrix()
1510         z = tensor.inc_subtensor(x[:4], y)
1511         f = theano.function([y], z)
1512         inc_nodes = [n for n in f.maker.fgraph.toposort()
1513                      if isinstance(n.op, tensor.IncSubtensor)]
1514         assert(len(inc_nodes) == 1)
1515         node_is_set_instead_of_inc = inc_nodes[0].op.set_instead_of_inc
1516         mode = theano.config.mode
1517         assert((mode != "FAST_COMPILE" and node_is_set_instead_of_inc) or
1518                (mode == "FAST_COMPILE" and not node_is_set_instead_of_inc))
1519         test_X = np.random.random((4, 4)).astype(config.floatX)
1520         utt.assert_allclose(f(test_X), test_X)
1521         not_all_zeros = np.zeros((4, 4))
1522         not_all_zeros[1, 0] = 0.001
1523         x = tensor.constant(np.asarray(not_all_zeros, dtype=config.floatX))
1524         y = tensor.matrix()
1525         z = tensor.inc_subtensor(x[:4], y)
1526         f = theano.function([y], z)
1527         inc_nodes = [n for n in f.maker.fgraph.toposort()
1528                      if isinstance(n.op, tensor.IncSubtensor)]
1529         assert(len(inc_nodes) == 1)
1530         assert(inc_nodes[0].op.set_instead_of_inc is False)
1531         test_X = np.random.random((4, 4)).astype(config.floatX)
1532         utt.assert_allclose(f(test_X), test_X + not_all_zeros)
1533     def test_advancedincsubtensor1_allocs0(self):
1534         x = tensor.matrix()
1535         y = tensor.matrix()
1536         y0 = tensor.zeros_like(y)
1537         z = tensor.inc_subtensor(x[[0, 1, 2, 3]], y0)
1538         f = theano.function([x, y], z, mode=self.mode)
1539         assert np.all([not isinstance(n.op, tensor.AdvancedIncSubtensor1)
1540                        for n in f.maker.fgraph.toposort()])
1541     def test_advancedincsubtensor1_allocs0t(self):
1542         x = tensor.matrix()
1543         y = tensor.matrix()
1544         y0 = tensor.zeros_like(y)
1545         z = tensor.inc_subtensor(x[[0, 1, 2, 3]], y0.T)
1546         f = theano.function([x, y], z, mode=mode_opt)
1547         assert np.all([not isinstance(n.op, tensor.AdvancedIncSubtensor1)
1548                        for n in f.maker.fgraph.toposort()])
1549     def test_advancedincsubtensor1_allocs1(self):
1550         x = tensor.matrix()
1551         y0 = tensor.constant(np.asarray(np.zeros_like((4, 4)),
1552                                         dtype=config.floatX))
1553         z = tensor.inc_subtensor(x[[0, 1, 2, 3]], y0)
1554         f = theano.function([x], z, mode=self.mode)
1555         assert np.all([not isinstance(n.op, tensor.AdvancedIncSubtensor1)
1556                        for n in f.maker.fgraph.toposort()])
1557     def test_advancedincsubtensor_allocs0(self):
1558         x = tensor.matrix()
1559         y = tensor.matrix()
1560         y0 = tensor.zeros_like(y)
1561         z = tensor.inc_subtensor(x[[[0, 0], [1, 1]], [[0, 1], [0, 1]]], y0)
1562         f = theano.function([x, y], z, mode=self.mode)
1563         assert np.all([not isinstance(n.op, tensor.AdvancedIncSubtensor)
1564                        for n in f.maker.fgraph.toposort()])
1565     def test_advancedincsubtensor_allocs0t(self):
1566         x = tensor.matrix()
1567         y = tensor.matrix()
1568         y0 = tensor.zeros_like(y)
1569         z = tensor.inc_subtensor(x[[[0, 0], [1, 1]], [[0, 1], [0, 1]]], y0.T)
1570         f = theano.function([x, y], z, mode=mode_opt)
1571         assert np.all([not isinstance(n.op, tensor.AdvancedIncSubtensor)
1572                        for n in f.maker.fgraph.toposort()])
1573     def test_advancedincsubtensor_allocs1(self):
1574         x = tensor.matrix()
1575         y0 = tensor.constant(np.asarray(np.zeros_like((2, 2)),
1576                                         dtype=config.floatX))
1577         z = tensor.inc_subtensor(x[[[0, 0], [1, 1]], [[0, 1], [0, 1]]], y0)
1578         f = theano.function([x], z, mode=self.mode)
1579         assert np.all([not isinstance(n.op, tensor.AdvancedIncSubtensor)
1580                        for n in f.maker.fgraph.toposort()])
1581     def test_dot_allocs_0(self):
1582         v1 = tensor.vector('v1')
1583         v2 = tensor.vector('v2')
1584         m1 = tensor.matrix('m1')
1585         m2 = tensor.matrix('m2')
1586         vv2 = np.asarray([0, 1], dtype=theano.config.floatX)
1587         vm2 = np.asarray([[1, 2], [4, 5]],
1588                          dtype=theano.config.floatX)
1589         vv3 = np.asarray([0, 1, 2], dtype=theano.config.floatX)
1590         vm3 = np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]],
1591                          dtype=theano.config.floatX)
1592         for _e1 in [(v1, vv2, vv3), (m1, vm2, vm3)]:
1593             for _e2 in [(v2, vv2, vv3), (m2, vm2, vm3)]:
1594                 for p in [0, 1]:
1595                     if p == 0:
1596                         e1 = tensor.zeros_like(_e1[0])
1597                         e2 = _e2[0]
1598                     else:
1599                         e1 = _e1[0]
1600                         e2 = tensor.zeros_like(_e2[0])
1601                     o = tensor.dot(e1, e2)
1602                     f = theano.function([_e1[0], _e2[0]], o, mode=self.mode)
1603                     f(_e1[1], _e2[1])
1604                     f(_e1[2], _e2[2])
1605                     assert np.all([not isinstance(n.op, tensor.Dot) for n in
1606                                    f.maker.fgraph.toposort()])
1607                     self.assertRaises((ValueError, AssertionError), f,
1608                                       _e1[1], _e2[2])
1609                     self.assertRaises((ValueError, AssertionError), f,
1610                                       _e1[2], _e2[1])
1611 def test_local_IncSubtensor_serialize():
1612     d = np.random.normal(0, 0.01, size=(100, 100))
1613     d = d.astype(theano.config.floatX)
1614     W = theano.shared(d, name='W')
1615     i = T.vector('i', dtype='int64')
1616     j = T.vector('j', dtype='int64')
1617     t = T.scalar('t')
1618     y = (W[i] + W[j] + W[1] + W[i, j]).sum()
1619     cost = T.sqr(t - y)
1620     dW = theano.grad(cost, W)
1621     mode = theano.compile.mode.get_default_mode().excluding('fusion')
1622     mode = mode.including("local_IncSubtensor_serialize")
1623     f = theano.function([i, j, t], updates=[(W, W - 0.01 * dW)], mode=mode)
1624     topo = f.maker.fgraph.toposort()
1625     adds = [n for n in topo if isinstance(n.op, T.Elemwise) and
1626             isinstance(n.op.scalar_op, theano.scalar.Add)]
1627     for a in adds:
1628         assert not any([inp.owner and
1629                         isinstance(inp.owner.op,
1630                                    (tensor.IncSubtensor,
1631                                     tensor.AdvancedIncSubtensor,
1632                                     tensor.AdvancedIncSubtensor1))
1633                         for inp in a.inputs])
1634     f = theano.function([i, j, t], dW, mode=mode)
1635     assert check_stack_trace(f, ops_to_check=[
1636         tensor.IncSubtensor, tensor.AdvancedIncSubtensor,
1637         tensor.AdvancedIncSubtensor1])
1638 def test_local_set_to_inc_subtensor():
1639     v = theano.tensor.fmatrix()
1640     s = v[[2, 1]]
1641     g = s + 3
1642     r = theano.tensor.set_subtensor(s, g)
1643     moder = compile.get_default_mode().excluding('local_set_to_inc_subtensor')
1644     modet = compile.get_default_mode().including('local_set_to_inc_subtensor')
1645     f1 = theano.function([v], r, mode=moder)
1646     f2 = theano.function([v], r, mode=modet)
1647     advi1 = [n for n in f1.maker.fgraph.toposort()
1648              if isinstance(n.op, tensor.AdvancedIncSubtensor1)]
1649     advi2 = [n for n in f2.maker.fgraph.toposort()
1650              if isinstance(n.op, tensor.AdvancedIncSubtensor1)]
1651     assert all(n.op.set_instead_of_inc for n in advi1)
1652     assert all(not n.op.set_instead_of_inc for n in advi2)
1653     val = np.random.randn(3, 2).astype('float32')
1654     r1 = f1(val)
1655     r2 = f2(val)
1656     utt.assert_allclose(r1, r2)
1657     assert check_stack_trace(f1, ops_to_check=tensor.AdvancedIncSubtensor1)
1658     assert check_stack_trace(f2, ops_to_check='all')
1659 def test_local_subtensor_of_dot():
1660     m1 = theano.tensor.matrix()
1661     m2 = theano.tensor.matrix()
1662     d1 = np.arange(6).reshape((3, 2)).astype(config.floatX)
1663     d2 = np.arange(8).reshape((2, 4)).astype(config.floatX) + 10
1664     mode = compile.get_default_mode().including("local_subtensor_of_dot")
1665     def test_equality(a, b):
1666         return a.shape == b.shape and np.allclose(a, b)
1667     f = theano.function([m1, m2], theano.dot(m1, m2)[1], mode=mode)
1668     topo = f.maker.fgraph.toposort()
1669     assert test_equality(f(d1, d2), np.dot(d1, d2)[1])
1670     assert isinstance(topo[-1].op, (T.blas_c.CGemv, T.blas.Gemv, T.DimShuffle))
1671     f = theano.function([m1, m2], theano.dot(m1, m2)[1:2], mode=mode)
1672     topo = f.maker.fgraph.toposort()
1673     assert test_equality(f(d1, d2), np.dot(d1, d2)[1:2])
1674     assert isinstance(topo[-1].op, (T.blas.Dot22))
1675     m1 = theano.tensor.tensor3()
1676     m2 = theano.tensor.tensor3()
1677     idx = theano.tensor.iscalar()
1678     d1 = np.arange(30).reshape(2, 5, 3).astype(config.floatX)
1679     d2 = np.arange(72).reshape(4, 3, 6).astype(config.floatX) + 100
1680     f = theano.function([m1, m2, idx], theano.dot(m1, m2)[idx, 1:4, :, idx:], mode=mode)
1681     assert test_equality(f(d1, d2, 1), np.dot(d1, d2)[1, 1:4, :, 1:])
1682     assert check_stack_trace(f, ops_to_check='last')
1683     f = theano.function([m1, m2, idx], theano.dot(m1, m2)[1:4, :, idx:, idx], mode=mode)
1684     assert test_equality(f(d1, d2, 1), np.dot(d1, d2)[1:4, :, 1:, 1])
1685     assert check_stack_trace(f, ops_to_check='last')
1686 class Test_local_elemwise_alloc(unittest.TestCase):
1687     dtype = config.floatX
1688     def setUp(self):
1689         self.fast_compile_mode = get_mode('FAST_COMPILE')
1690         self.fast_run_mode = get_mode('FAST_RUN')
1691         self.vec = T.vector('vec', dtype=self.dtype)
1692         self.mat = T.matrix('mat', dtype=self.dtype)
1693         self.tens = T.tensor3('tens', dtype=self.dtype)
1694         self.alloc_wo_dep = T.alloc(self.vec, 2, 2)
1695         self.alloc_wo_dep_broad = T.alloc(self.vec, 1, 2)
1696         self.alloc_w_dep = T.alloc(self.vec, *self.mat.shape)
1697         self.alloc_w_dep_broad = T.alloc(self.vec, 1, *self.mat.shape)
1698         self.alloc_w_dep_broad2 = T.alloc(self.vec, self.mat.shape[0],
1699                                           self.mat.shape[1], 1)
1700         self.alloc_w_dep_tens = T.alloc(
1701             self.vec,
1702             self.tens.shape[0],
1703             self.tens.shape[1]
1704         )
1705         self.tv_wo_dep = T.alloc(self.vec, 5, 5)
1706         self.tm_wo_dep = T.alloc(self.mat, 5, 5, 5)
1707         self.s = T.iscalar('s')
1708         self.tv_w_dep = T.alloc(self.vec, self.s, self.s)
1709         self.tm_w_dep = T.alloc(self.mat, 5, 5, 5)
1710         self.row = theano.tensor.row(dtype=self.dtype)
1711         self.o = T.alloc(self.row, 5, 5)
1712     def _verify_alloc_count(self, f, count):
1713         assert(
1714             sum([isinstance(elem.op, T.Alloc)
1715                  for elem in f.maker.fgraph.toposort()
1716                  if elem.op is not None]) == count
1717         )
1718     def _verify_assert_count(self, f, count):
1719         assert(
1720             sum([isinstance(elem.op, T.opt.Assert)
1721                  for elem in f.maker.fgraph.toposort()
1722                  if elem.op is not None]) == count
1723         )
1724     def test_remove_alloc_wo_dimshuffle(self):
1725         self.fast_run_mode = self.fast_run_mode.excluding(
1726             'local_useless_alloc', 'local_canonicalize_alloc')
1727         func = function(
1728             [self.vec, self.mat],
1729             self.alloc_wo_dep + self.mat,
1730             mode=self.fast_compile_mode
1731         )
1732         self._verify_alloc_count(func, 1)
1733         self._verify_assert_count(func, 0)
1734         self.assertTrue(check_stack_trace(func, ops_to_check='all'))
1735         func = function(
1736             [self.vec, self.mat],
1737             self.alloc_wo_dep + self.mat,
1738             mode=self.fast_run_mode
1739         )
1740         self._verify_alloc_count(func, 0)
1741         self._verify_assert_count(func, 1)
1742         func = function(
1743             [self.vec, self.mat],
1744             self.alloc_wo_dep_broad + self.mat,
1745             mode=self.fast_run_mode
1746         )
1747         self._verify_alloc_count(func, 0)
1748         self._verify_assert_count(func, 1)
1749         func = function(
1750             [self.vec, self.mat],
1751             self.alloc_w_dep + self.mat,
1752             mode=self.fast_compile_mode
1753         )
1754         self._verify_alloc_count(func, 1)
1755         self._verify_assert_count(func, 0)
1756         func = function(
1757             [self.vec, self.mat],
1758             self.alloc_w_dep + self. mat,
1759             mode=self.fast_run_mode
1760         )
1761         self._verify_alloc_count(func, 0)
1762         self._verify_assert_count(func, 0)
1763         func = function(
1764             [self.vec, self.mat],
1765             self.alloc_w_dep_broad + self. mat,
1766             mode=self.fast_run_mode
1767         )
1768         self._verify_alloc_count(func, 0)
1769         self._verify_assert_count(func, 0)
1770         func = function(
1771             [self.vec, self.mat],
1772             self.alloc_w_dep_broad2 + self. mat,
1773             mode=self.fast_run_mode
1774         )
1775         self._verify_alloc_count(func, 1)
1776         self._verify_assert_count(func, 0)
1777     def test_remove_alloc_w_dimshuffle(self):
1778         func = function(
1779             [self.vec, self.tens],
1780             self.alloc_wo_dep.dimshuffle(0, 1, 'x') + self.tens,
1781             mode=self.fast_compile_mode
1782         )
1783         self._verify_alloc_count(func, 1)
1784         self._verify_assert_count(func, 0)
1785         func = function(
1786             [self.vec, self.tens],
1787             self.alloc_wo_dep.dimshuffle(0, 1, 'x') + self.tens,
1788             mode=self.fast_run_mode
1789         )
1790         self._verify_alloc_count(func, 0)
1791         self._verify_assert_count(func, 1)
1792         func = function(
1793             [self.vec, self.tens],
1794             self.alloc_w_dep_tens.dimshuffle(0, 1, 'x') + self.tens,
1795             mode=self.fast_compile_mode
1796         )
1797         self._verify_alloc_count(func, 1)
1798         self._verify_assert_count(func, 0)
1799         func = function(
1800             [self.vec, self.tens],
1801             self.alloc_w_dep_tens.dimshuffle(0, 1, 'x') + self.tens,
1802             mode=self.fast_run_mode
1803         )
1804         self._verify_alloc_count(func, 0)
1805         self._verify_assert_count(func, 0)
1806     def test_multi_input_single_alloc(self):
1807         func = function(
1808             [self.vec, self.mat],
1809             self.tv_wo_dep + self.tm_wo_dep,
1810             mode=self.fast_compile_mode
1811         )
1812         self._verify_alloc_count(func, 2)
1813         self._verify_assert_count(func, 0)
1814         func = function(
1815             [self.vec, self.mat],
1816             self.tv_wo_dep + self.tm_wo_dep,
1817             mode=self.fast_run_mode
1818         )
1819         self._verify_alloc_count(func, 1)
1820         self._verify_assert_count(func, 0)
1821         func = function(
1822             [self.vec, self.mat, self.s],
1823             self.tv_w_dep + self.tm_w_dep,
1824             mode=self.fast_compile_mode
1825         )
1826         self._verify_alloc_count(func, 2)
1827         self._verify_assert_count(func, 0)
1828         func = function(
1829             [self.vec, self.mat, self.s],
1830             self.tv_w_dep + self.tm_w_dep,
1831             mode=self.fast_run_mode
1832         )
1833         self._verify_alloc_count(func, 1)
1834         self._verify_assert_count(func, 1)
1835     def test_error(self):
1836         t3fft = theano.tensor.tensor(dtype=self.dtype,
1837                                      broadcastable=(False, False, True))
1838         o = self.o.dimshuffle(0, 1, 'x') + t3fft
1839         func = function(
1840             [t3fft, self.row],
1841             o,
1842             mode=self.fast_run_mode
1843         )
1844         self._verify_alloc_count(func, 0)
1845         self._verify_assert_count(func, 1)
1846         d = np.random.rand(5, 5, 1).astype(self.dtype)
1847         r = np.random.rand(1, 5).astype(self.dtype)
1848         func(d, r)
1849 def test_local_subtensor_of_alloc():
1850     for shape in [(3, 5), (4, 6), (3, 8), (4, 7),
1851                   (1, 5), (5, 1)]:
1852         x = tensor.tensor(dtype=theano.config.floatX,
1853                           broadcastable=(shape[0] == 1, shape[1] == 1))
1854         xval = np.zeros(shape, dtype=config.floatX)
1855         yval = np.arange(shape[1], dtype=config.floatX)
1856         for y in [theano.shared(yval), tensor.constant([1.])]:
1857             yx = tensor.alloc(y, x.shape[0], x.shape[1])
1858             z_mat = yx[:, 3:]
1859             assert z_mat.ndim == 2
1860             z_vec = yx[:, 3]
1861             assert z_vec.ndim == 1
1862             slicess = []
1863             if shape[0] != 1:
1864                 slicess.append((2, slice(None)))
1865             if shape[1] != 1:
1866                 slicess.append((slice(None), 3))
1867             slicess += [
1868                 (slice(None), slice(3, None)),
1869                 (slice(3, None), ),
1870                 (slice(3, None), slice(3, None)),
1871                 (slice(1, 3), slice(None, -1)),
1872                 (slice(None, None, 2)),
1873                 (slice(1, None, 2)),
1874             ]
1875             for slices in slicess:
1876                 z = yx.__getitem__(slices)
1877                 f = theano.function([x], z)
1878                 if theano.config.mode != 'FAST_COMPILE':
1879                     assert not isinstance(f.maker.fgraph.toposort()[-1].op,
1880                                           Subtensor)
1881                 val = f(xval)
1882                 assert xval.__getitem__(slices).shape == val.shape
1883 def test_local_fill_useless():
1884     x = dvector()
1885     y = dvector()
1886     z = lvector()
1887     m = dmatrix()
1888     x_ = np.random.rand(5,)
1889     y_ = np.random.rand(5,)
1890     z_ = (np.random.rand(5,) * 5).astype("int64")
1891     m_ = np.random.rand(5, 5)
1892     f = function([x], T.fill(x, x) * 2, mode=mode_opt)
1893     assert [node.op for node in f.maker.fgraph.toposort()] == [T.mul]
1894     f(x_)
1895     f = function([x, y], T.second(y, x) * 2, mode=mode_opt)
1896     assert [node.op for node in f.maker.fgraph.toposort()] == [T.mul]
1897     f(x_, y_)
1898     f = function([x, y], T.fill(x, y) * 2, mode=mode_opt)
1899     assert [node.op for node in f.maker.fgraph.toposort()] == [T.mul]
1900     f(x_, y_)
1901     f = function([x, z], T.fill(z, x) * 2, mode=mode_opt)
1902     assert [node.op for node in f.maker.fgraph.toposort()] == [T.mul]
1903     f(x_, z_)
1904     f = function([x, z], T.fill(x, z) * 2, mode=mode_opt)
1905     assert [node.op for node in f.maker.fgraph.toposort()] == [T.mul]
1906     f(x_, z_)
1907     f = function([x, y], T.fill(x, y) * 2, mode=mode_opt)
1908     assert [node.op for node in f.maker.fgraph.toposort()] == [T.mul]
1909     f(x_, y_)
1910     f = function([m, x], T.fill(m, x) * 2, mode=mode_opt)
1911     ops = [node.op.__class__ for node in f.maker.fgraph.toposort()]
1912     assert T.Alloc in ops
1913     f(m_, x_)
1914 def test_local_elemwise_sub_zeros():
1915     scalar = T.scalar()
1916     vect = T.vector()
1917     mat = T.matrix()
1918     rng = np.random.RandomState(seed=utt.fetch_seed())
1919     scalar_val = rng.rand(1).astype(config.floatX)[0]
1920     vect_val = rng.rand(5).astype(config.floatX)
1921     mat_val = rng.rand(3, 2).astype(config.floatX)
1922     mode = theano.compile.get_default_mode()\
1923         .excluding('canonicalize', 'uncanonicalize',
1924                    'ShapeOpt', 'local_fill_to_alloc',
1925                    'local_elemwise_alloc')\
1926         .including('local_elemwise_sub_zeros')
1927     f = function([scalar], scalar - scalar, mode=mode)
1928     assert isinstance(f.maker.fgraph.toposort()[0].op, T.Elemwise)
1929     assert isinstance(f.maker.fgraph.toposort()[0].op.scalar_op,
1930                       theano.scalar.Second)
1931     assert isinstance(f.maker.fgraph.toposort()[0].inputs[1],
1932                       T.TensorConstant) or\
1933         isinstance(f.maker.fgraph.toposort()[0].inputs[1],
1934                    T.TensorConstant)
1935     utt.assert_allclose(f(scalar_val), 0.0)
1936     assert check_stack_trace(f, ops_to_check='all')
1937     f = function([vect], vect - vect, mode=mode)
1938     assert isinstance(f.maker.fgraph.toposort()[0].op, T.Elemwise)
1939     assert isinstance(f.maker.fgraph.toposort()[0].op.scalar_op,
1940                       theano.scalar.Second)
1941     assert isinstance(f.maker.fgraph.toposort()[0].inputs[1],
1942                       T.TensorConstant) or\
1943         isinstance(f.maker.fgraph.toposort()[0].inputs[1],
1944                    T.TensorConstant)
1945     utt.assert_allclose(f(vect_val), np.zeros(vect_val.shape))
1946     assert check_stack_trace(f, ops_to_check='all')
1947     f = function([mat], mat - mat, mode=mode)
1948     assert isinstance(f.maker.fgraph.toposort()[0].op, T.Elemwise)
1949     assert isinstance(f.maker.fgraph.toposort()[0].op.scalar_op,
1950                       theano.scalar.Second)
1951     assert isinstance(f.maker.fgraph.toposort()[0].inputs[1],
1952                       T.TensorConstant) or\
1953         isinstance(f.maker.fgraph.toposort()[0].inputs[1],
1954                    T.TensorConstant)
1955     utt.assert_allclose(f(mat_val), np.zeros(mat_val.shape))
1956     assert check_stack_trace(f, ops_to_check='all')
1957 class Test_local_useless_elemwise_comparison(unittest.TestCase):
1958     def setUp(self):
1959         self.rng = np.random.RandomState(utt.fetch_seed())
1960     def test_local_useless_elemwise_comparison(self):
1961         X = T.matrix('X')
1962         Y = T.vector('Y')
1963         X_sum, updates = theano.scan(fn=lambda x: x.sum(),
1964                                      outputs_info=None,
1965                                      sequences=[X],
1966                                      non_sequences=None)
1967         Z = X_sum + Y
1968         """
1969         Elemwise{add,no_inplace} [id A] ''
1970          |for{cpu,scan_fn} [id B] ''
1971          | |Subtensor{int64} [id C] ''
1972          | | |Shape [id D] ''
1973          | | | |Subtensor{int64::} [id E] 'X[0:]'
1974          | | |   |X [id F]
1975          | | |   |Constant{0} [id G]
1976          | | |Constant{0} [id H]
1977          | |Subtensor{:int64:} [id I] ''
1978          | | |Subtensor{int64::} [id E] 'X[0:]'
1979          | | |ScalarFromTensor [id J] ''
1980          | |   |Subtensor{int64} [id C] ''
1981          | |Subtensor{int64} [id C] ''
1982          |Y [id K]
1983         Inner graphs of the scan ops:
1984         for{cpu,scan_fn} [id B] ''
1985          &gt;Sum{acc_dtype=float64} [id L] ''
1986          &gt; |X[t] [id M] -&gt; [id I]
1987         """
1988         mode = theano.compile.get_default_mode().excluding('fusion')
1989         f = theano.function([X, Y], Z, mode=mode)
1990         f(self.rng.rand(2, 3).astype(config.floatX),
1991           self.rng.rand(2).astype(config.floatX))
1992         """
1993         Elemwise{Add}[(0, 0)] [id A] &lt;TensorType(float64, vector)&gt; ''   7
1994          |for{cpu,scan_fn} [id B] &lt;TensorType(float64, vector)&gt; ''   6
1995          | |Shape_i{0} [id C] &lt;TensorType(int64, scalar)&gt; ''   0
1996          | | |X [id D] &lt;TensorType(float64, matrix)&gt;
1997          | |Subtensor{int64:int64:int8} [id E] &lt;TensorType(float64, matrix)&gt; ''   5
1998          | | |X [id D] &lt;TensorType(float64, matrix)&gt;
1999          | | |ScalarFromTensor [id F] &lt;int64&gt; ''   4
2000          | | | |Elemwise{switch,no_inplace} [id G] &lt;TensorType(int64, scalar)&gt; ''   3
2001          | | |   |Elemwise{le,no_inplace} [id H] &lt;TensorType(int8, scalar)&gt; ''   2
2002          | | |   | |Shape_i{0} [id C] &lt;TensorType(int64, scalar)&gt; ''   0
2003          | | |   | |TensorConstant{0} [id I] &lt;TensorType(int8, scalar)&gt;
2004          | | |   |TensorConstant{0} [id I] &lt;TensorType(int8, scalar)&gt;
2005          | | |   |TensorConstant{0} [id J] &lt;TensorType(int64, scalar)&gt;
2006          | | |ScalarFromTensor [id K] &lt;int64&gt; ''   1
2007          | | | |Shape_i{0} [id C] &lt;TensorType(int64, scalar)&gt; ''   0
2008          | | |Constant{1} [id L] &lt;int8&gt;
2009          | |Shape_i{0} [id C] &lt;TensorType(int64, scalar)&gt; ''   0
2010          |Y [id M] &lt;TensorType(float64, vector)&gt;
2011         Inner graphs of the scan ops:
2012         for{cpu,scan_fn} [id B] &lt;TensorType(float64, vector)&gt; ''
2013          &gt;Sum{acc_dtype=float64} [id N] &lt;TensorType(float64, scalar)&gt; ''
2014          &gt; |X[t] [id O] &lt;TensorType(float64, vector)&gt; -&gt; [id E]
2015         """
2016     def assert_eqs_const(self, f, val, op=deep_copy_op):
2017         topo = f.maker.fgraph.toposort()
2018         elem = topo[0]
2019         assert len(topo) == 1, topo
2020         assert elem.op == op, elem.op
2021         if op == deep_copy_op:
2022             assert len(elem.inputs) == 1, elem.inputs
2023             assert isinstance(elem.inputs[0], T.TensorConstant), elem
2024             assert T.extract_constant(elem.inputs[0]) == val, val
2025         else:
2026             assert len(elem.inputs) == 2, elem.inputs
2027             assert isinstance(elem.inputs[0], T.TensorConstant), elem
2028             assert T.extract_constant(elem.inputs[0]) == val, val
2029     def assert_identity(self, f):
2030         topo = f.maker.fgraph.toposort()
2031         assert len(topo) == 1
2032         assert topo[0].op == deep_copy_op
2033         if f.outputs[0].variable.dtype == 'bool':
2034             x_vals = [0, 1]
2035         else:
2036             x_vals = [0, 1, 10]
2037         for x_val in x_vals:
2038             assert f(x_val) == x_val
2039     def test_inequality_with_self(self):
2040         x = T.scalar('x', dtype=config.floatX)
2041         mode = theano.compile.get_default_mode().including<font color="#4e9258"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>('local_useless_elemwise_comparison')
2042         f = theano.function([x], T.lt(x, x), mode=mode)
2043         self.assert_eqs_const(f, 0)
2044         f = theano.function(</b></font>[x], T.le(x, x), mode=mode)
2045         self.assert_eqs_const(f, 1)
2046         f = theano.function([x], T.gt(x, x), mode=mode)
2047         self.assert_eqs_const(f, 0)
2048         f = theano.function([x], T.ge(x, x), mode=mode)
2049         self.assert_eqs_const(f, 1)
2050         f = theano.function([x], T.minimum(x, x), mode=mode)
2051         self.assert_identity(f)
2052         f = theano.function([x], T.maximum(x, x), mode=mode)
2053         self.assert_identity(f)
2054     def test_shape_inequality_with_self(self):
2055         x = T.vector('x', dtype=config.floatX)
2056         mode = theano.compile.get_default_mode().including(
2057             'local_useless_elemwise_comparison',
2058             'local_shape_to_shape_i',
2059             'local_track_shape_i',
2060             'local_subtensor_make_vector')
2061         f = theano.function([x], T.lt(x.shape[0], 0), mode=mode)
2062         self.assert_eqs_const(f, 0)
2063         f = theano.function([x], T.ge(x.shape[0], 0), mode=mode)
2064         self.assert_eqs_const(f, 1)
2065         f = theano.function([x], T.maximum(x.shape[0], 0), mode=mode)
2066         topo = f.maker.fgraph.toposort()
2067         assert len(topo) == 1
2068         assert isinstance(topo[0].op, Shape_i), topo[0].op
2069         x_val = np.ones(100, dtype=config.floatX)
2070         assert f(x_val) == x_val.shape[0]
2071         f = theano.function([x], T.maximum(0, x.shape[0]), mode=mode)
2072         topo = f.maker.fgraph.toposort()
2073         assert len(topo) == 1
2074         assert isinstance(topo[0].op, Shape_i), topo[0].op
2075         x_val = np.ones(100, dtype=config.floatX)
2076         assert f(x_val) == x_val.shape[0]
2077         f = theano.function([x], T.minimum(x.shape[0], 0), mode=mode)
2078         self.assert_eqs_const(f, 0)
2079         assert f(x_val) == 0
2080         f = theano.function([x], T.minimum(0, x.shape[0]), mode=mode)
2081         self.assert_eqs_const(f, 0)
2082         assert f(x_val) == 0
2083         f = theano.function([x], T.minimum([0, 0], x.shape[0]), mode=mode)
2084         utt.assert_allclose(f(x_val), [0, 0])
2085     def test_shape_add_inequality(self):
2086         x = T.vector('x', dtype=config.floatX)
2087         mode = theano.compile.get_default_mode().including(
2088             'local_useless_elemwise_comparison',
2089             'local_shape_to_shape_i',
2090             'local_track_shape_i',
2091             'local_subtensor_make_vector')
2092         y = T.vector('y', dtype=config.floatX)
2093         f = theano.function([x, y], T.lt(x.shape[0] + y.shape[0], 0), mode=mode)
2094         self.assert_eqs_const(f, 0)
2095         f = theano.function([x, y], T.ge(x.shape[0] + y.shape[0], 0), mode=mode)
2096         self.assert_eqs_const(f, 1)
2097     def test_equality_shapes(self):
2098         if theano.config.mode == "FAST_COMPILE":
2099             raise SkipTest("Skip opt test as the opt is disabled")
2100         x = T.vector('x', dtype=config.floatX)
2101         for g in [x.shape[0],
2102                   Shape_i(0)(x)]:
2103             f = theano.function([x], T.eq(g, 0))
2104             assert f([3, 3]) == 0
2105             assert f([]) == 1
2106             f = theano.function([x], T.eq(g, -1))
2107             self.assert_eqs_const(f, 0)
2108             assert f([3, 3]) == 0
2109         g = join(0,
2110                  x.shape[0:],  # todo test reshape, dimshuffle
2111                  x.shape[0:1])
2112         f = theano.function([x], T.eq(g, 0))
2113         assert (f([3, 3]) == 0).all()
2114         assert (f([]) == 1).all()
2115         f = theano.function([x], T.eq(g, -1))
2116         self.assert_eqs_const(f, 0, op=T.alloc)
2117         assert (<font color="#83a33a"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>f([3, 3]) == 0).all()
2118     def test_and(self):
2119         mode = theano.compile.get_default_mode().including(</b></font>'canonicalize')
2120         for dtype, zero, one in [('bool', np.array(False), np.array(True)),
2121                                  ('int8', np.int8(0), np.int8(1)),
2122                                  ('int8', 0, 1)]:
2123             x = T.scalar('x', dtype=dtype)
2124             f = theano.function([x], T.and_(x, zero), mode=mode)
2125             self.assert_eqs_const(f, 0)
2126             f = theano.function([x], T.and_(zero, x), mode=mode)
2127             self.assert_eqs_const(f, 0)
2128             f = theano.function([x], T.and_(x, one), mode=mode)
2129             if dtype == 'bool':
2130                 self.assert_identity(f)
2131             f = theano.function([x], T.and_(one, x), mode=mode)
2132             if dtype == 'bool':
2133                 self.assert_identity(f)
2134     def test_and_int(self):
2135         f = theano.function([], T.and_(5, 6))
2136         assert f() == 4
2137     def test_or(self):
2138         mode = theano.compile.get_default_mode().including('canonicalize')
2139         for dtype, zero, one in [('bool', np.array(False), np.array(True)),
2140                                  ('int8', np.int8(0), np.int8(1)),
2141                                  ('int8', 0, 1)]:
2142             x = T.scalar('x', dtype=dtype)
2143             f = theano.function([x], T.or_(x, one), mode=mode)
2144             if dtype == 'bool':
2145                 self.assert_eqs_const(f, 1)
2146             f = theano.function([x], T.or_(one, x), mode=mode)
2147             if dtype == 'bool':
2148                 self.assert_eqs_const(f, 1)
2149             f = theano.function([x], T.or_(x, zero), mode=mode)
2150             self.assert_identity(f)
2151             f = theano.function([x], T.or_(zero, x), mode=mode)
2152             self.assert_identity(f)
2153     def test_or_int(self):
2154         f = theano.function([], T.or_(5, 6))
2155         assert f() == 7
2156     def test_xor(self):
2157         mode = theano.compile.get_default_mode().including('canonicalize')
2158         for dtype in ('bool', 'int8'):
2159             x = T.scalar('x', dtype=dtype)
2160             f = theano.function([x], T.xor(x, x), mode=mode)
2161             self.assert_eqs_const(f, 0)
2162     def test_stacktrace(self):
2163         mode = theano.compile.get_default_mode().including(
2164             'local_useless_elemwise_comparison')
2165         x = T.vector('x', dtype=config.floatX)
2166         f = theano.function([x], T.gt(x, x), mode=mode)
2167         self.assertTrue(check_stack_trace(f, ops_to_check='last'))
2168         f = theano.function([x], T.le(x, x), mode=mode)
2169         self.assertTrue(check_stack_trace(f, ops_to_check='last'))
2170 class Test_local_canonicalize_alloc(unittest.TestCase):
2171     def setUp(self):
2172         self.rng = np.random.RandomState(utt.fetch_seed())
2173     @change_flags(compute_test_value='off')
2174     def test0(self):
2175         x = shared(self.rng.randn(3, 7))
2176         a = tensor.alloc(x, 6, 7)
2177         assert a.owner and isinstance(a.owner.op, tensor.Alloc)
2178         f = function([], a, mode=mode_opt)
2179         assert ([node.op for node in f.maker.fgraph.toposort()] ==
2180                 [deep_copy_op])
2181         if isinstance(mode_opt, compile.DebugMode):
2182             self.assertRaises(ValueError, f)
2183     def test1(self):
2184         mode = mode_opt.excluding('local_canonicalize_alloc')
2185         x = tensor.matrix('x')
2186         xx = tensor.fill(x, x)
2187         f = function([x], [xx], mode=mode)
2188         op_classes = [node.op.__class__ for node in f.maker.fgraph.toposort()]
2189         assert tensor.Alloc not in op_classes
2190     def test2(self):
2191         mode = mode_opt.excluding('local_canonicalize_alloc')
2192         x = tensor.matrix('x')
2193         y = tensor.tile(x, (1,) * 2)
2194         f = function([x], [y], mode=mode)
2195         op_classes = [node.op.__class__ for node in f.maker.fgraph.toposort()]
2196         print(op_classes)
2197         assert tensor.Alloc in op_classes
2198     def test_useless_alloc_with_shape_one(self):
2199         alloc_lift = out2in(local_canonicalize_alloc)
2200         x = shared(self.rng.randn(2,))
2201         y = shared(self.rng.randn())
2202         z = shared(self.rng.randn(1, 1))
2203         w = shared(self.rng.randn(1, 1))
2204         alloc_x = tensor.alloc(x, 1, 3, 2)
2205         alloc_y = tensor.alloc(y, 1, 1)
2206         alloc_z = tensor.alloc(z, 1, 1, 2)
2207         alloc_w = tensor.alloc(w, 1, 2)
2208         g = FunctionGraph([x, y, z, w], [alloc_x, alloc_y, alloc_z, alloc_w])
2209         self.assertTrue(str(g) == ("[Alloc(&lt;TensorType(float64, vector)&gt;, "
2210                                    "TensorConstant{1}, "
2211                                    "TensorConstant{3}, "
2212                                    "TensorConstant{2}), "
2213                                    "Alloc(&lt;TensorType(float64, scalar)&gt;, "
2214                                    "TensorConstant{1}, "
2215                                    "TensorConstant{1}), "
2216                                    "Alloc(&lt;TensorType(float64, matrix)&gt;, "
2217                                    "TensorConstant{1}, "
2218                                    "TensorConstant{1}, "
2219                                    "TensorConstant{2}), "
2220                                    "Alloc(&lt;TensorType(float64, matrix)&gt;, "
2221                                    "TensorConstant{1}, "
2222                                    "TensorConstant{2})]"))
2223         alloc_lift.optimize(g)
2224         self.assertTrue(str(g) == "[InplaceDimShuffle{x,0,1}"
2225                                   "(Alloc(&lt;TensorType(float64, vector)&gt;, "
2226                                   "TensorConstant{3}, "
2227                                   "TensorConstant{2})), "
2228                                   "InplaceDimShuffle{x,x}"
2229                                   "(&lt;TensorType(float64, scalar)&gt;), "
2230                                   "InplaceDimShuffle{x,0,1}"
2231                                   "(Alloc(&lt;TensorType(float64, matrix)&gt;, "
2232                                   "TensorConstant{1}, "
2233                                   "TensorConstant{2})), "
2234                                   "Alloc(&lt;TensorType(float64, matrix)&gt;, "
2235                                   "TensorConstant{1}, "
2236                                   "TensorConstant{2})]")
2237         self.assertTrue(check_stack_trace(g, ops_to_check='all'))
2238 class Test_local_useless_inc_subtensor_alloc(unittest.TestCase):
2239     opt_name = 'local_useless_inc_subtensor_alloc'
2240     def setUp(self):
2241         mode = theano.config.mode
2242         if mode == 'FAST_COMPILE':
2243             mode = 'FAST_RUN'
2244         self.mode = compile.mode.get_mode(mode)
2245     def test_advanced_inc_subtensor(self):
2246         x = tensor.vector('x')
2247         y = tensor.scalar('y')
2248         i = tensor.matrix('i', dtype='int64')
2249         z = tensor.advanced_inc_subtensor(x, T.alloc(y, *i.shape), i)
2250         mode1 = self.mode.excluding(self.opt_name)
2251         mode2 = self.mode.including(self.opt_name)
2252         f1 = theano.function([x, i, y], z, mode=mode1)
2253         f2 = theano.function([x, i, y], z, mode=mode2)
2254         assert (len([n for n in f1.maker.fgraph.toposort()
2255                      if isinstance(n.op, tensor.Alloc)]) == 1)
2256         assert (len([n for n in f2.maker.fgraph.toposort()
2257                      if isinstance(n.op, tensor.Alloc)]) == 0)
2258         x_value = np.random.randn(5).astype(config.floatX)
2259         y_value = np.random.randn()
2260         i_value = np.random.randint(0, 3, size=(2, 3))
2261         r1 = f1(x_value, i_value, y_value)
2262         r2 = f2(x_value, i_value, y_value)
2263         utt.assert_allclose(r1, r2)
2264         self.assertTrue(check_stack_trace(f1, ops_to_check=tensor.AdvancedIncSubtensor))
2265         self.assertTrue(check_stack_trace(f2, ops_to_check=tensor.AdvancedIncSubtensor))
2266     def test_advanced_inc_subtensor1(self):
2267         x = tensor.vector('x')
2268         y = tensor.scalar('y')
2269         i = tensor.vector('i', dtype='int64')
2270         z = tensor.advanced_inc_subtensor1(x, T.alloc(y, *i.shape), i)
2271         mode1 = self.mode.excluding(self.opt_name)
2272         mode2 = self.mode.including(self.opt_name)
2273         f1 = theano.function([x, i, y], z, mode=mode1)
2274         f2 = theano.function([x, i, y], z, mode=mode2)
2275         assert (len([n for n in f1.maker.fgraph.toposort()
2276                      if isinstance(n.op, tensor.Alloc)]) == 1)
2277         assert (len([n for n in f2.maker.fgraph.toposort()
2278                      if isinstance(n.op, tensor.Alloc)]) == 0)
2279         x_value = np.random.randn(5).astype(config.floatX)
2280         y_value = np.random.randn()
2281         i_value = np.random.randint(0, 3, size=2)
2282         r1 = f1(x_value, i_value, y_value)
2283         r2 = f2(x_value, i_value, y_value)
2284         utt.assert_allclose(r1, r2)
2285         self.assertTrue(check_stack_trace(
2286             f1, ops_to_check=tensor.AdvancedIncSubtensor1))
2287         self.assertTrue(check_stack_trace(f2, ops_to_check='all'))
2288     def test_incsubtensor(self):
2289         x = tensor.vector('x')
2290         y = tensor.scalar('y')
2291         i = tensor.scalar('i', dtype='int64')
2292         z = tensor.inc_subtensor(x[:i], T.alloc(y, i))
2293         mode1 = self.mode.excluding(self.opt_name)
2294         mode2 = self.mode.including(self.opt_name)
2295         f1 = theano.function([x, i, y], z, mode=mode1)
2296         f2 = theano.function([x, i, y], z, mode=mode2)
2297         assert (len([n for n in f1.maker.fgraph.toposort()
2298                      if isinstance(n.op, tensor.Alloc)]) == 1)
2299         assert (len([n for n in f2.maker.fgraph.toposort()
2300                      if isinstance(n.op, tensor.Alloc)]) == 0)
2301         x_value = np.random.randn(5).astype(config.floatX)
2302         y_value = np.random.randn()
2303         i_value = 3
2304         r1 = f1(x_value, i_value, y_value)
2305         r2 = f2(x_value, i_value, y_value)
2306         utt.assert_allclose(r1, r2)
2307         self.assertTrue(check_stack_trace(f1, ops_to_check='last'))
2308         self.assertTrue(check_stack_trace(f2, ops_to_check='last'))
2309 class test_shapeoptimizer(unittest.TestCase):
2310     def setUp(self):
2311         utt.seed_rng()
2312     def test0(self):
2313         mode = theano.config.mode
2314         if mode == 'FAST_COMPILE':
2315             mode = 'FAST_RUN'
2316         v = T.vector()
2317         m = T.matrix()
2318         f = function([v, m], (v + m).shape, mode=mode)
2319         for node in f.maker.fgraph.toposort():
2320             assert node.op != T.add
2321     def test_constant(self):
2322         mode = theano.config.mode
2323         if mode == 'FAST_COMPILE':
2324             mode = 'FAST_RUN'
2325         v = T.vector()
2326         f = function([v], v.dimshuffle('x', 'x', 0).shape[1], mode=mode)
2327         topo = f.maker.fgraph.toposort()
2328         assert len(topo) == 1
2329         assert topo[0].op == deep_copy_op
2330     @staticmethod
2331     def max_pool_c01b(c01b, pool_shp, pool_stride, img_shp):
2332         """
2333         Like max_pool but with input using axes ('c', 0, 1, 'b')
2334           (Alex Krizhevsky format)
2335         pool_shp, pool_stride and img_shp are int that represent
2336         the same shp in x and y.
2337         """
2338         mx = None
2339         def last_pool(im_shp, p_shp, p_strd):
2340             rval = int(np.ceil(float(im_shp - p_shp) / p_strd))
2341             assert p_strd * rval + p_shp &gt;= im_shp
2342             assert p_strd * (rval - 1) + p_shp &lt; im_shp
2343             return rval
2344         last_pool_r = last_pool(img_shp, pool_shp, pool_stride) * pool_stride
2345         required_r = last_pool_r + pool_shp
2346         last_pool_c = last_pool(img_shp, pool_shp, pool_stride) * pool_stride
2347         required_c = last_pool_c + pool_shp
2348         wide_infinity = T.alloc(-np.inf, c01b.shape[0],
2349                                 required_r, required_c, c01b.shape[3])
2350         c01b = T.set_subtensor(wide_infinity[:, 0:img_shp, 0:img_shp, :], c01b)
2351         for row_within_pool in xrange(pool_shp):
2352             row_stop = last_pool_r + row_within_pool + 1
2353             for col_within_pool in xrange(pool_shp):
2354                 col_stop = last_pool_c + col_within_pool + 1
2355                 cur = c01b[:, row_within_pool:row_stop:pool_stride,
2356                            col_within_pool:col_stop:pool_stride, :]
2357                 if mx is None:
2358                     mx = cur
2359                 else:
2360                     mx = T.maximum(mx, cur)
2361         return mx
2362     def test_broadcasted_dims(self):
2363         shp = (1, 1, 1, 1)
2364         rng = np.random.RandomState(utt.fetch_seed())
2365         a = shared(rng.rand(*shp).astype(config.floatX))
2366         out = self.max_pool_c01b(a, 1, 1, 1)
2367         mode = copy.copy(theano.compile.get_default_mode())
2368         mode.check_isfinite = False
2369         f = theano.function([], out, mode=mode)
2370         f()
2371     def test_constant_merge(self):
2372         x = tensor.constant([0, 0])
2373         y = x[1:]
2374         x1 = x - tensor.join(0, y, y)
2375         x1.eval()
2376     def test_local_track_shape_i(self):
2377         class IdentityNoShape(gof.Op):
2378             def make_node(self, x):
2379                 x = as_tensor_variable(x)
2380                 return gof.Apply(self, [x], [x.type()])
2381             def perform(self, node, inp, out_):
2382                 x, = inp
2383                 out, = out_
2384                 out[0] = x.copy()
2385         identity_noshape = IdentityNoShape()
2386         class IdentityShape(gof.Op):
2387             def make_node(self, x):
2388                 x = as_tensor_variable(x)
2389                 return gof.Apply(self, [x], [x.type()])
2390             def perform(self, node, inp, out_):
2391                 x, = inp
2392                 out, = out_
2393                 out[0] = x.copy()
2394             def infer_shape(self, node, xshp_):
2395                 xshp, = xshp_
2396                 return (xshp,)
2397         identity_shape = IdentityShape()
2398         @gof.local_optimizer([IdentityNoShape])
2399         def local_identity_noshape_to_identity_shape(node):
2400             if isinstance(node.op, IdentityNoShape):
2401                 return [identity_shape(node.inputs[0])]
2402         mode = theano.compile.get_default_mode().including(
2403             'ShapeOpt', 'specialize')
2404         rng = np.random.RandomState(utt.fetch_seed())
2405         x = T.tensor3('x')
2406         ins_x = identity_noshape(x)
2407         f = theano.function([x], ins_x.shape, mode=mode)
2408         xval = rng.randn(3, 4, 7).astype(config.floatX)
2409         assert np.all(f(xval) == [3, 4, 7])
2410         f_ops = [node.op for node in f.maker.fgraph.toposort()]
2411         assert len(f_ops) == 5
2412         assert identity_noshape in f_ops
2413         assert identity_shape not in f_ops
2414         opt.register_specialize(local_identity_noshape_to_identity_shape)
2415         mode = theano.compile.get_default_mode().including(
2416             'ShapeOpt', 'specialize')
2417         g = theano.function([x], ins_x.shape, mode=mode)
2418         xval = rng.randn(6, 1, 2).astype(config.floatX)
2419         assert np.all(g(xval) == [6, 1, 2])
2420         g_ops = [node.op for node in g.maker.fgraph.toposort()]
2421         assert len(g_ops) == 4
2422         assert identity_noshape not in g_ops
2423         assert identity_shape not in g_ops
2424         ins_x3 = identity_noshape(identity_noshape(identity_noshape(x)))
2425         h = theano.function([x], ins_x3.shape, mode=mode)
2426         xval = rng.randn(6, 1, 2).astype(config.floatX)
2427         assert np.all(h(xval) == [6, 1, 2])
2428         h_ops = [node.op for node in h.maker.fgraph.toposort()]
2429         assert len(h_ops) == 4
2430         assert identity_noshape not in h_ops
2431         assert identity_shape not in h_ops
2432     def test_no_shapeopt(self):
2433         X = T.matrix()
2434         expr = X.shape[0]
2435         mode = theano.compile.get_default_mode().excluding('ShapeOpt')
2436         f = theano.function([X], expr, mode=mode)
2437         print(f([[1, 2], [2, 3]]))
2438 class test_assert(utt.InferShapeTester):
2439     def setUp(self):
2440         super(test_assert, self).setUp()
2441     def test0(self):
2442         x = T.scalar()
2443         y = T.scalar()
2444         f = theano.function([x, y], theano.tensor.opt.assert_op(x, T.eq(x, y)))
2445         f(1, 1)
2446         self.assertRaises(AssertionError, f, 1, 0)
2447     def test_local_remove_useless_assert1(self):
2448         mode = theano.config.mode
2449         if mode == 'FAST_COMPILE':
2450             mode = 'FAST_RUN'
2451         mode = compile.mode.get_mode(mode)
2452         x = T.scalar()
2453         f = theano.function([x], theano.tensor.opt.assert_op(x, 1), mode=mode)
2454         assert f(1) == 1
2455         assert f(5) == 5
2456         topo = f.maker.fgraph.toposort()
2457         assert len(topo) == 1
2458         assert topo[0].op == deep_copy_op
2459     def test_test_local_remove_useless_assert2(self):
2460         mode = theano.config.mode
2461         if mode == 'FAST_COMPILE':
2462             mode = 'FAST_RUN'
2463         mode = compile.mode.get_mode(mode)
2464         x = T.scalar()
2465         y = T.scalar()
2466         f = theano.function([x, y], theano.tensor.opt.assert_op(x, y, 1),
2467                             mode=mode)
2468         assert f(1, 1) == 1
2469         assert f(5, 1) == 5
2470         topo = f.maker.fgraph.toposort()
2471         assert len(topo) == 2
2472         assert len(topo[0].inputs) == 2
2473         assert topo[1].op == deep_copy_op
2474     def test_local_remove_useless_assert3(self):
2475         mode = theano.config.mode
2476         if mode == 'FAST_COMPILE':
2477             mode = 'FAST_RUN'
2478         mode = compile.mode.get_mode(mode)
2479         x = T.scalar()
2480         y = T.scalar()
2481         f = theano.function([x, y], theano.tensor.opt.assert_op(x, y, 0),
2482                             mode=mode)
2483         self.assertRaises(AssertionError, f, 1, 0)
2484         topo = f.maker.fgraph.toposort()
2485         assert len(topo) == 2
2486         assert len(topo[0].inputs) == 3
2487         assert topo[1].op == deep_copy_op
2488     def test_local_remove_all_assert1(self):
2489         mode = theano.config.mode
2490         if mode == 'FAST_COMPILE':
2491             mode = 'FAST_RUN'
2492         mode = compile.mode.get_mode(mode).including('local_remove_all_assert')
2493         x = T.scalar()
2494         y = T.scalar()
2495         f = theano.function([x, y], theano.tensor.opt.assert_op(x, y),
2496                             mode=mode)
2497         if isinstance(mode, theano.compile.debugmode.DebugMode):
2498             self.assertRaises(AssertionError, f, 1, 0)
2499         else:
2500             f(1, 0)  # Without opt, it should fail.
2501         topo = f.maker.fgraph.toposort()
2502         assert len(topo) == 1, topo
2503         assert topo[0].op == deep_copy_op, topo
2504         mode = compile.mode.get_default_mode()
2505         a = theano.tensor.opt.assert_op(x, T.eq(x, 0).any())
2506         f = theano.function([x], a, mode=mode.excluding('unsafe'))
2507         topo = f.maker.fgraph.toposort()
2508         a_op = [n for n in topo if isinstance(n.op, T.opt.Assert)]
2509         assert len(a_op) == 1
2510     def test_infer_shape(self):
2511         adscal = dscalar()
2512         bdscal = dscalar()
2513         adscal_val = np.random.rand()
2514         bdscal_val = np.random.rand() + 1
2515         out = theano.tensor.opt.assert_op(adscal, bdscal)
2516         self._compile_and_check([adscal, bdscal], [out],
2517                                 [adscal_val, bdscal_val], Assert)
2518         admat_val = np.random.rand(3, 4)
2519         adscal_val += 1
2520         out <font color="#6cc417"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>= theano.tensor.opt.assert_op(admat, adscal, bdscal)
2521         self._compile_and_check([admat, adscal, bdscal], [out],
2522                                 [admat_val, adscal_val, bdscal_val], Assert)
2523 def test_local_mul_specialize():
2524     mode = theano.</b></font>config.mode
2525     if mode == 'FAST_COMPILE':
2526         mode = 'FAST_RUN'
2527     mode = compile.mode.get_mode(mode)
2528     mode = mode.excluding('fusion')
2529     v = T.vector()
2530     m = T.vector()
2531     f = function([v], v * 1, mode=mode)
2532     nodes = [node.op for node in f.maker.fgraph.toposort()]
2533     nodes == [deep_copy_op]
2534     f = function([v], v * 0, mode=mode)
2535     nodes = [node.op for node in f.maker.fgraph.toposort()]
2536     assert nodes == [Shape_i(0), T.alloc]
2537     f = function([v], v * (-1), mode=mode)
2538     nodes = [node.op for node in f.maker.fgraph.toposort()]
2539     assert nodes == [T.neg]
2540     f = function([v, m], v * 1 * (-m), mode=mode)
2541     nodes = [node.op for node in f.maker.fgraph.toposort()]
2542     assert nodes == [T.mul]
2543     f = function([v, m], v * 0 * (-m), mode=mode)
2544     nodes = [node.op for node in f.maker.fgraph.toposort()]
2545     assert nodes == [Shape_i(0), T.alloc]
2546     f = function([v, m], v * (-1) * (-m), mode=mode)
2547     nodes = [node.op for node in f.maker.fgraph.toposort()]
2548     assert nodes == [T.mul]
2549     f = function([v, m], v * (-1) * m, mode=mode)
2550     nodes = [node.op for node in f.maker.fgraph.toposort()]
2551     assert nodes == [T.mul]
2552 class T_Tile(unittest.TestCase):
2553     def test_local_useless_tile(self):
2554         v = T.vector()
2555         m = T.matrix()
2556         mode = None
2557         if theano.config.mode == "FAST_COMPILE":
2558             mode = "FAST_RUN"
2559         for var, data in [(v, [1, 2, 3]), (m, [[1, 2], [3, 4]])]:
2560             for ndim in range(var.ndim + 1):
2561                 f = theano.function([var], tile(var, (1,) * ndim), mode=mode)
2562                 topo = f.maker.fgraph.toposort()
2563                 assert len(topo) == 1
2564                 assert isinstance(topo[0].op, compile.DeepCopyOp)
2565                 f(data)
2566             for ndim in range(var.ndim + 1, var.ndim + 3):
2567                 f = theano.function([var], tile(var, (1,) * ndim), mode=mode)
2568                 topo = f.maker.fgraph.toposort()
2569                 assert len(topo) &lt;= 2
2570                 assert isinstance(topo[0].op, DimShuffle)
2571                 assert check_stack_trace(f, ops_to_check=[DimShuffle])
2572                 f(data)
2573 def speed_local_pow_specialize_range():
2574     val = np.random.rand(1e7)
2575     v = T.vector()
2576     mode = compile.mode.get_default_mode()
2577     mode_without_pow_opt = mode.excluding('local_pow_specialize')
2578     for i in xrange(500, 513):
2579         f1 = function([v], v ** i, mode=mode)
2580         f2 = function([v], v ** i, mode=mode_without_pow_opt)
2581         assert len(f1.maker.fgraph.toposort()) == 1
2582         t1 = time.time()
2583         f1(val)
2584         t2 = time.time()
2585         f2(val)
2586         t3 = time.time()
2587         print(i, t2 - t1, t3 - t2, t2 - t1 &lt; t3 - t2)
2588         if not t2 - t1 &lt; t3 - t2:
2589             print("WARNING WE ARE SLOWER")
2590     for i in xrange(-3, -1500, -1):
2591         f1 = function([v], v ** i, mode=mode)
2592         f2 = function([v], v ** i, mode=mode_without_pow_opt)
2593         assert len(f1.maker.fgraph.toposort()) == 1
2594         t1 = time.time()
2595         f1(val)
2596         t2 = time.time()
2597         f2(val)
2598         t3 = time.time()
2599         print(i, t2 - t1, t3 - t2, t2 - t1 &lt; t3 - t2)
2600         if not t2 - t1 &lt; t3 - t2:
2601             print("WARNING WE ARE SLOWER")
2602 def test_local_pow_specialize():
2603     mode = theano.config.mode
2604     if mode == 'FAST_COMPILE':
2605         mode = 'FAST_RUN'
2606     mode = compile.mode.get_mode(mode)
2607     mode = mode.excluding('fusion')
2608     v = T.vector()
2609     val = np.arange(10, dtype=theano.config.floatX)
2610     val_no0 = np.arange(1, 10, dtype=theano.config.floatX)
2611     f = function([v], v ** 0, mode=mode)
2612     nodes = [node.op for node in f.maker.fgraph.toposort()]
2613     assert nodes == [Shape_i(0), T.alloc]
2614     utt.assert_allclose(f(val), val ** 0)
2615     f = function([v], v ** 1, mode=mode)
2616     nodes = [node.op for node in f.maker.fgraph.toposort()]
2617     nodes == [deep_copy_op]
2618     utt.assert_allclose(f(val), val ** 1)
2619     f = function([v], v ** (-1), mode=mode)
2620     nodes = [node.op for node in f.maker.fgraph.toposort()]
2621     assert nodes == [T.inv]
2622     utt.assert_allclose(f(val_no0), val_no0 ** (-1))
2623     f = function([v], v ** 2, mode=mode)
2624     nodes = [node.op for node in f.maker.fgraph.toposort()]
2625     assert nodes == [T.sqr]
2626     utt.assert_allclose(f(val), val ** 2)
2627     f = function([v], v ** (-2), mode=mode)
2628     nodes = [node.op for node in f.maker.fgraph.toposort()]
2629     assert len(nodes) == 2
2630     assert nodes[0] == T.sqr
2631     assert isinstance(nodes[1].scalar_op, theano.scalar.basic.Inv)
2632     utt.assert_allclose(f(val_no0), val_no0 ** (-2))
2633     f = function([v], v ** (.5), mode=mode)
2634     nodes = [node.op for node in f.maker.fgraph.toposort()]
2635     assert nodes == [T.sqrt]
2636     utt.assert_allclose(f(val), val ** (.5))
2637     f = function([v], v ** (-.5), mode=mode)
2638     nodes = [node.op for node in f.maker.fgraph.toposort()]
2639     assert len(nodes) == 2
2640     assert nodes[0] == T.sqrt
2641     assert isinstance(nodes[1].scalar_op, theano.scalar.basic.Inv)
2642     utt.assert_allclose(f(val_no0), val_no0 ** (-.5))
2643 def test_local_pow_specialize_device_more_aggressive_on_cpu():
2644     if mode == 'FAST_COMPILE':
2645         mode = 'FAST_RUN'
2646     mode = compile.mode.get_mode<font color="#38a4a5"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(mode)
2647     mode = mode.excluding('fusion').excluding('gpu')
2648     v = T.vector()
2649     val = np.arange(10, dtype=</b></font>theano.config.floatX)
2650     val_no0 = np.arange(1, 10, dtype=theano.config.floatX)
2651     nodes = [node.op for node in f.maker.fgraph.toposort()]
2652     assert len(nodes) == 1
2653     assert len(f.maker.fgraph<font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.toposort()[0].op.scalar_op.fgraph.apply_nodes) == 6
2654     assert isinstance(nodes[0].scalar_op, theano.scalar.Composite)
2655     utt.</b></font>assert_allclose(f(val), val ** 15)
2656     f = function([v], v ** (-15), mode=mode)
2657     nodes = [node.op for node in f.maker.fgraph.toposort()]
2658     assert len(nodes) == 2
2659     assert len(f.maker.fgraph.toposort()[0].op.scalar_op.fgraph.apply_nodes) == 6
2660     assert isinstance(nodes[0].scalar_op, theano.scalar.Composite)
2661     assert isinstance(nodes[-1].scalar_op, theano.scalar.basic.Inv)
2662     utt.assert_allclose(f(val_no0), val_no0 ** (-15))
2663     nodes = [node.op for node in f.maker.fgraph.toposort()]
2664     assert len(nodes) == 1
2665     assert len(f.maker.fgraph<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.toposort()[0].op.scalar_op.fgraph.apply_nodes) == 4
2666     assert isinstance(nodes[0].scalar_op, theano.scalar.Composite)
2667     utt.</b></font>assert_allclose(f(val), val ** 16)
2668     f = function([v], v ** (-16), mode=mode)
2669     nodes = [node.op for node in f.maker.fgraph.toposort()]
2670     assert len(nodes) == 2
2671     assert len(f.maker.fgraph.toposort()[0].op.scalar_op.fgraph.apply_nodes) == 4
2672     assert isinstance(nodes[0].scalar_op, theano.scalar.Composite)
2673     assert isinstance(nodes[-1].scalar_op, theano.scalar.basic.Inv)
2674     utt.assert_allclose(f(val_no0), val_no0 ** (-16))
2675 class T_Rebroadcast(unittest.TestCase):
2676     def test_local_useless_rebroadcast(self):
2677         mode = theano.compile.get_default_mode().including('canonicalize')
2678         v1 = T.vector()
2679         v2 = T.vector()
2680         j = T.join(0, v1, v2)
2681         f = theano.function([v1, v2], j, mode=mode)
2682         f([1, 2], [3, 4, 5])
2683         e = f.maker.fgraph.toposort()
2684         assert len([n for n in e if isinstance(n.op, T.Rebroadcast)]) == 0
2685     def test_rebroadcast_rebroadcast(self):
2686         mode = theano.compile.get_default_mode()<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.including('canonicalize')
2687         m = T.matrix()
2688         s = T.addbroadcast(m, 0, 1)
2689         v = T.unbroadcast(s, 1)
2690         f = theano.function([m], v, mode=mode)
2691         f([[76]])
2692         e = f.</b></font>maker.fgraph.toposort()
2693         rebroadcast_nodes = [n for n in e if isinstance(n.op, T.Rebroadcast)]
2694         assert len(rebroadcast_nodes) == 1
2695         assert rebroadcast_nodes[0].op.axis == {0: True}
2696 class T_useless_elemwise(unittest.TestCase):
2697     def setUp(self):
2698         self.mode = theano.compile.get_default_mode().including(
2699             'canonicalize', 'local_fill_to_alloc')
2700     def test_eq(self):
2701         x = T.dmatrix()
2702         y = T.dmatrix()
2703         f = theano.function([x, y], T.eq(x, y), mode=self.mode)
2704         vx = np.random.rand(5, 4)
2705         vy = np.random.rand(5, 4)
2706         f(vx, vy)
2707         assert len(topo) == 1
2708         assert isinstance(topo[0].op, T.Elemwise)
2709         assert isinstance<font color="#3090c7"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(topo[0].op.scalar_op, theano.scalar.EQ)
2710         f2 = theano.function([x], T.eq(x, x), mode=</b></font>self.mode)
2711         assert np.all(f2(vx) == np.ones((5, 4)))
2712         topo2 = f2.maker.fgraph.toposort()
2713         assert len(topo2) == 3
2714         assert isinstance(topo2[-1].op, T.Alloc)
2715     def test_neq(self):
2716         x = T.dmatrix()
2717         y = T.dmatrix()
2718         f = theano.function([x, y], T.neq(x, y), mode=self.mode)
2719         vx = np.random.rand(5, 4)
2720         vy = np.random.rand(5, 4)
2721         f(vx, vy)
2722         assert len(topo) == 1
2723         assert isinstance(topo[0].op, T.Elemwise)
2724         assert isinstance<font color="#2981b2"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(topo[0].op.scalar_op, theano.scalar.NEQ)
2725         f2 = theano.function([x], T.neq(x, x), mode=</b></font>self.mode)
2726         assert np.all(f2(vx) == np.zeros((5, 4)))
2727         topo2 = f2.maker.fgraph.toposort()
2728         assert len(topo2) == 3
2729         assert isinstance(topo2[-1].op, T.Alloc)
2730     def test_mul(self):
2731         x = T.dmatrix()
2732         y = T.dmatrix()
2733         f = theano.function([x], T.mul(x), mode=self.mode)
2734         vx = np.random.rand(5, 4)
2735         vy = np.random.rand(5, 4)
2736         f(vx)
2737         topo = f.maker.fgraph.toposort()
2738         assert len(topo) == 1
2739         assert topo[0].op == deep_copy_op
2740         f2 = theano.function([x, y], T.mul(x, y), mode=self.mode)
2741         assert np.all(f2(vx, vy) == vx * vy)
2742         topo2 = f2.maker.fgraph.toposort()
2743         assert len(topo2) == 1
2744         assert isinstance(topo2[0].op, T.Elemwise)
2745         assert isinstance(topo2[0].op.scalar_op, theano.scalar.Mul)
2746     def test_add(self):
2747         x = T.dmatrix()
2748         y = T.dmatrix()
2749         f = theano.function([x], T.add(x), mode=self.mode)
2750         vx = np.random.rand(5, 4)
2751         vy = np.random.rand(5, 4)
2752         f(vx)
2753         topo = f.maker.fgraph.toposort()
2754         assert len(topo) == 1
2755         assert topo[0].op == deep_copy_op
2756         f2 = theano.function([x, y], T.add(x, y), mode=self.mode)
2757         assert np.all(f2(vx, vy) == vx + vy)
2758         topo2 = f2.maker.fgraph.toposort()
2759         assert len(topo2) == 1
2760         assert isinstance(topo2[0].op, T.Elemwise)
2761         assert isinstance(topo2[0].op.scalar_op, theano.scalar.Add)
2762     def test_identity(self):
2763         x = T.matrix()
2764         f = theano.function([x], T.tensor_copy(x), mode=self.mode)
2765         vx = np.random.rand(5, 4).astype(config.floatX)
2766         f(vx)
2767         topo = f.maker.fgraph.toposort()
2768         assert len(topo) == 1
2769         assert topo[0].op == deep_copy_op
2770 class T_cast_cast(unittest.TestCase):
2771     def setUp(self):
2772         mode = theano.compile.get_default_mode()
2773         self.mode = mode.including('local_cast_cast')
2774     def test_consecutive(self):
2775         x = T.fmatrix()
2776         o = T.Elemwise(scal.Cast(scal.Scalar("float64")))(x.astype("float64"))
2777         f = theano.function([x], o, mode=self.mode)
2778         dx = np.random.rand(5, 4).astype("float32")
2779         f(dx)
2780         topo = f.maker.fgraph.toposort()
2781         assert len(topo) == 1
2782         assert isinstance(topo[0].op.scalar_op, scal.basic.Cast)
2783         x = T.dmatrix()
2784         o = T.Elemwise(scal.Cast(scal.Scalar("float32")))(x.astype("float32"))
2785         f = theano.function([x], o, mode=self.mode)
2786         dx = np.random.rand(5, 4)
2787         f(dx)
2788         topo = f.maker.fgraph.toposort()
2789         assert len(topo) == 1
2790         assert isinstance(topo[0].op.scalar_op, scal.basic.Cast)
2791     def test_upcast(self):
2792         x = T.fmatrix()
2793         o = T.Elemwise(scal.Cast(scal.Scalar("complex128")))(x.astype("complex64"))
2794         f = theano.function([x], o, mode=self.mode)
2795         dx = np.random.rand(5, 4).astype("float32")
2796         f(dx)
2797         topo = f.maker.fgraph.toposort()
2798         assert len(topo) == 1
2799         assert isinstance(topo[0].op.scalar_op, scal.basic.Cast)
2800         x = T.fmatrix()
2801         o = T.Elemwise(scal.Cast(scal.Scalar("float32")))(x.astype("float64"))
2802         f = theano.function([x], o, mode=self.mode)
2803         dx = np.random.rand(5, 4).astype('float32')
2804         f(dx)
2805         topo = f.maker.fgraph.toposort()
2806         assert len(topo) == 1
2807         assert isinstance(topo[0].op, DeepCopyOp)
2808         x = T.dmatrix()
2809         o = T.Elemwise(scal.Cast(scal.Scalar("float64")))(x.astype("float32"))
2810         f = theano.function([x], o, mode=self.mode)
2811         dx = np.random.rand(5, 4)
2812         f(dx)
2813         topo = f.maker.fgraph.toposort()
2814         assert (len(topo) == 1 and isinstance(topo[0].op.scalar_op, scal.basic.Composite)) or (len(topo) &gt; 1)
2815 class T_func_inverse(unittest.TestCase):
2816     def setUp(self):
2817         mode = theano.compile.get_default_mode()
2818         self.mode = mode.including('local_func_inv')
2819     def assert_func_pair_optimized(self, func1, func2, data,
2820                                    should_copy=True, is_complex=False):
2821         x = T.cmatrix() if is_complex else T.fmatrix()
2822         o = func2(func1(x))
2823         f = theano.function([x], o, mode=self.mode)
2824         delta = f(data) - data
2825         topo = f.maker.fgraph.toposort()
2826         if should_copy:
2827             acceptable_topo_lens = [1]
2828         else:
2829             acceptable_topo_lens = [1, 2]
2830         if should_copy:
2831             delta_condition = np.all(delta == 0)
2832         else:
2833             delta_condition = np.all(delta != 0)
2834         self.assertTrue(len(topo) in acceptable_topo_lens)
2835         self.assertTrue(delta_condition)
2836         self.assertEqual(isinstance(topo[0].op, DeepCopyOp), should_copy,
2837                          "Inverse functions not removed!")
2838     def test(self):
2839         dx = np.random.rand(5, 4).astype("float32")
2840         self.assert_func_pair_optimized(T.deg2rad, T.rad2deg, dx)
2841         dx = np.random.rand(5, 4).astype("float32") * 180
2842         self.assert_func_pair_optimized(T.rad2deg, T.deg2rad, dx)
2843         dx = np.random.rand(5, 4).astype("float32")
2844         self.assert_func_pair_optimized(T.cosh, T.arccosh, dx)
2845         self.assert_func_pair_optimized(T.arcsinh, T.sinh, dx)
2846         self.assert_func_pair_optimized(T.arctanh, T.tanh, dx)
2847         self.assert_func_pair_optimized(T.inv, T.inv, dx)
2848         self.assert_func_pair_optimized(T.neg, T.neg, dx)
2849         cx = dx + complex(0, 1) * (dx + 0.01)
2850         self.assert_func_pair_optimized(T.conj, T.conj, cx, is_complex=True)
2851         self.assert_func_pair_optimized(T.conj, T.neg, cx,
2852                                         should_copy=False, is_complex=True)
2853         dx = np.random.rand(5, 4).astype("float32") + 0.01
2854         self.assert_func_pair_optimized(T.rad2deg, T.rad2deg, dx,
2855                                         should_copy=False)
2856         self.assert_func_pair_optimized(T.rad2deg, T.cosh, dx,
2857                                         should_copy=False)
2858 def test_constant_folding():
2859     x = tensor.dvector()
2860     mode = theano.compile.get_mode("FAST_COMPILE").excluding("fusion")
2861     f = theano.function([x], [x * 2, x + x], mode=mode)
2862     topo = f.maker.fgraph.toposort()
2863     assert len(topo) == 2
2864     x = tensor.constant(3)
2865     assert x.ndim == 0
2866     mode = theano.compile.get_mode("FAST_COMPILE").excluding("fusion")
2867     f = theano.function([], [x * 2, x + x], mode=mode)
2868     topo = f.maker.fgraph.toposort()
2869     assert len(topo) == 2
2870     assert all([isinstance(n.op, DeepCopyOp) for n in topo])
2871 def test_constant_get_stabilized():
2872     x2 = T.scalar()
2873     y2 = T.log(1 + T.exp(x2))
2874     mode = theano.compile.get_default_mode()
2875     mode.check_isfinite = False
2876     f2 = theano.function([x2], y2, mode=mode)
2877     try:
2878         assert len(f2.maker.fgraph.toposort()) == 1
2879         assert (f2.maker.fgraph.toposort()[0].op ==
2880                 theano.tensor.nnet.sigm.softplus)
2881         assert f2(800) == 800
2882         x = T.as_tensor_variable(800)
2883         y = T.log(1 + T.exp(x))
2884         f = theano.function([], y, mode=mode)
2885         assert len(f.maker.fgraph.toposort()) == 0
2886         assert np.isinf(f())
2887         assert f() == 800, f()
2888     except AssertionError:
2889         raise SkipTest('Theano optimizes constant before stabilization. '
2890                        'This breaks stabilization optimization in some '
2891                        'cases. See #504.')
2892     def setUp(self):
2893         self.condm = np<font color="#3b9c9c"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.asarray([[0.1, 0, 1, -1],
2894                                  [0., 0., 0., 0.],
2895                                  [1, 1, 1, 1]])
2896         self.condv = np.asarray([0.1, 0, 1, -1])
2897         self.</b></font>conds = [0.1, 0, 1, -1]
2898         self.xm = np.ones((3, 4))
2899         self.xv = np.ones((4,))
2900         self.xs = 1.
2901         self.resm = (
2902             [np.asarray([[1, 0, 1, 0], [0, 0, 0, 0], [1, 1, 1, 1]])] * 3 +
2903             [np.asarray([[1, 0, 1, 0], [1, 0, 1, 0], [1, 0, 1, 0]])] +
2904             2 * [np.asarray([[1, 0, 1, 0]])] +
2905             [[np.ones((3, 4)), np.zeros((3, 4)), np.ones((3, 4)), np.zeros((3, 4))]] +
2906             [[np.ones((4,)), np.zeros((4,)), np.ones((4,)), np.zeros((4,))]] +
2907             [[np.asarray(1.0), np.asarray(0.0), np.asarray(1.0), np.asarray(0.0)]])
2908         self.mode = theano.compile.mode.get_default_mode().including(
2909             'canonicalize', 'fast_run').excluding('gpu', 'fusion')
2910         self.mode = copy.copy(self.mode)
2911         self.mode.check_isfinite = False
2912     def function_remove_nan(self, *args, **kwargs):
2913         """
2914         Wrapper around theano.function for this test.
2915         It disables checking for NaN removed by optimizations in DebugMode
2916         (it has false positives in that case).
2917         """
2918         f = theano.function(*args, **kwargs)
2919         def wrapped_f(*args, **kwargs):
2920             old_values_eq_approx = staticmethod(TensorType.values_eq_approx)
2921             TensorType.values_eq_approx = staticmethod(values_eq_approx_remove_nan)
2922             try:
2923                 out = f(*args, **kwargs)
2924             finally:
2925                 TensorType.values_eq_approx = old_values_eq_approx
2926             return out
2927         return wrapped_f
2928     def test_local_mul_switch_sink(self):
2929         c = T.dscalar()
2930         idx = 0
2931         for condition in [(T.dmatrix('cond'), self.condm),
2932                           (T.dvector('cond'), self.condv),
2933                           (T.dscalar('cond'), self.conds)]:
2934             for x in [(T.dmatrix('x'), self.xm), (T.dvector('x'), self.xv),
2935                       (T.dscalar('x'), self.xs)]:
2936                 y = T.mul(T.switch(condition[0] &gt; 0, 1. * x[0], 0. * x[0]),
2937                           T.switch(condition[0] &gt; 0,
2938                                    1. * x[0], T.log(c) * x[0]))
2939                 f = self.function_remove_nan([condition[0], x[0], c],
2940                                              [y], mode=self.mode)
2941                 if type(condition[1]) is list:
2942                     for i in xrange(len(condition[1])):
2943                         res = f(condition[1][i], x[1], -1)
2944                         assert (res == np.asarray(
2945                             self.resm[idx][i])).sum() == self.resm[idx][i].size
2946                 else:
2947                     res = f(condition[1], x[1], -1)
2948                     assert ((res == np.asarray(self.resm[idx])).sum() ==
2949                             self.resm[idx].size)
2950                 idx += 1
2951         x = T.dscalar('x')
2952         y = T.switch(x &lt; 7, x, T.sqrt(x - 7))
2953         f = self.function_remove_nan([x], T.grad(y, x), self.mode)
2954         assert f(5) == 1, f(5)
2955     @attr('slow')
2956     def test_local_div_switch_sink(self):
2957         c = T.dscalar()
2958         idx = 0
2959         for condition in [(T.dmatrix('cond'), self.condm), (T.dvector('cond'), self.condv), (T.dscalar('cond'), self.conds)]:
2960             for x in [(T.dmatrix('x'), self.xm), (T.dvector('x'), self.xv), (T.dscalar('x'), self.xs)]:
2961                 y = T.true_div(
2962                     T.switch(condition[0] &gt; 0, 1. * x[0], 0. * x[0]),
2963                     T.switch(condition[0] &gt; 0, 1. * x[0], T.log(c) * x[0]))
2964                 f = self.function_remove_nan([condition[0], x[0], c],
2965                                              [y], mode=self.mode)
2966                 if type(condition[1]) is list:
2967                     for i in xrange(len(condition[1])):
2968                         res = f(condition[1][i], x[1], -1)
2969                         assert ((res == np.asarray(self.resm[idx][i])).sum() ==
2970                                 self.resm[idx][i].size)
2971                 else:
2972                     res = f(condition[1], x[1], -1)
2973                     assert ((res == np.asarray(self.resm[idx])).sum() ==
2974                             self.resm[idx].size)
2975                 idx += 1
2976 class T_local_erf(unittest.TestCase):
2977     def setUp(self):
2978         self.mode = theano.compile.mode.get_default_mode().including(
2979             'canonicalize', 'fast_run').excluding('gpu', 'fusion')
2980         self.mode._optimizer.position_cutoff = 1.50001
2981         if theano.config.cxx == '' and not theano.scalar.basic_scipy.imported_scipy_special:
2982             raise SkipTest("erf need a c++ compiler or scipy")
2983     def test_local_one_plus_erf(self):
2984         val = np.asarray([-30, -3, -2, -1, 0, 1, 2, 3, 30],
2985                          dtype=config.floatX)
2986         x = T.vector()
2987         f = theano.function([x], 1 + T.erf(x), mode=self.mode)
2988         assert [n.op for n in f.maker.fgraph.toposort()] == [
2989             T.mul, T.erfc], f.maker.fgraph.toposort()
2990         f(val)
2991         f = theano.function([x], T.erf(x) + 1, mode=self.mode)
2992         assert [n.op for n in f.maker.fgraph.toposort()] == [
2993             T.mul, T.erfc], f.maker.fgraph.toposort()
2994         f(val)
2995         f = theano.function([x], T.erf(x) + 2, mode=self.mode)
2996         topo = f.maker.fgraph.toposort()
2997         assert len(topo) == 2
2998         assert topo[0].op == T.erf
2999         assert isinstance(topo[1].op, T.Elemwise)
3000         assert isinstance(topo[1].op.scalar_op, scal.Add)
3001         f(val)
3002     def test_local_one_minus_erf(self):
3003         val = np.asarray([-30, -3, -2, -1, 0, 1, 2, 3, 30],
3004                          dtype=config.floatX)
3005         x = T.vector()
3006         f = theano.function([x], 1 - T.erf(x), mode=self.mode)
3007         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erfc],\
3008             f.maker.fgraph.toposort()
3009         print(f(val))
3010         f = theano.function([x], 1 + (-T.erf(x)), mode=self.mode)
3011         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erfc],\
3012             f.maker.fgraph.toposort()
3013         print(f(val))
3014         f = theano.function([x], (-T.erf(x)) + 1, mode=self.mode)
3015         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erfc],\
3016             f.maker.fgraph.toposort()
3017         print(f(val))
3018         f = theano.function([x], 2 - T.erf(x), mode=self.mode)
3019         topo = f.maker.fgraph.toposort()
3020         assert len(topo) == 2, f.maker.fgraph.toposort()
3021         assert topo[0].op == T.erf, f.maker.fgraph.toposort()
3022         assert isinstance(topo[1].op, T.Elemwise), f.maker.fgraph.toposort()
3023         assert isinstance(topo[1].op.scalar_op, scal.Add)\
3024             or isinstance(topo[1].op.scalar_op, scal.Sub), f.maker.fgraph.toposort()
3025         print(f(val))
3026     def test_local_erf_minus_one(self):
3027         val = np.asarray([-30, -3, -2, -1, 0, 1, 2, 3, 30],
3028                          dtype=config.floatX)
3029         x = T.vector()
3030         f = theano.function([x], T.erf(x) - 1, mode=self.mode)
3031         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erfc, T.mul]
3032         print(f(val))
3033         f = theano.function([x], T.erf(x) + (-1), mode=self.mode)
3034         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erfc, T.mul]
3035         print(f(val))
3036         f = theano.function([x], -1 + T.erf(x), mode=self.mode)
3037         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erfc, T.mul]
3038         print(f(val))
3039         f = theano.function([x], T.erf(x) - 2, mode=self.mode)
3040         topo = f.maker.fgraph.toposort()
3041         assert len(topo) == 2
3042         assert topo[0].op == T.erf
3043         assert isinstance(topo[1].op, T.Elemwise)
3044         assert isinstance(topo[1].op.scalar_op, scal.Add)\
3045             or isinstance(topo[1].op.scalar_op, scal.Sub)
3046         print(f(val))
3047 class T_local_erfc(unittest.TestCase):
3048     def setUp(self):
3049         self.mode_fusion = theano.compile.mode.get_default_mode().including(
3050             'canonicalize').including('fast_run').excluding('gpu')
3051         self.mode = self.mode_fusion.excluding('fusion')
3052         self.mode._optimizer.position_cutoff = 1.50001
3053         if (theano.config.cxx == '' and
3054                 not theano.scalar.basic_scipy.imported_scipy_special):
3055             raise SkipTest("erfc need a c++ compiler or scipy")
3056     def test_local_one_minus_erfc(self):
3057         val = np.asarray([-30, -3, -2, -1, 0, 1, 2, 3, 30],
3058                          dtype=config.floatX)
3059         x = T.vector('x')
3060         f = theano.function([x], 1 - T.erfc(x), mode=self.mode)
3061         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erf],\
3062             f.maker.fgraph.toposort()
3063         print(f(val))
3064         f = theano.function([x], (-T.erfc(x)) + 1, mode=self.mode)
3065         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erf],\
3066             f.maker.fgraph.toposort()
3067         print(f(val))
3068         f = theano.function([x], 2 - T.erfc(x), mode=self.mode)
3069         topo = f.maker.fgraph.toposort()
3070         assert len(topo) == 2, f.maker.fgraph.toposort()
3071         assert topo[0].op == T.erfc, f.maker.fgraph.toposort()
3072         assert isinstance(topo[1].op, T.Elemwise), f.maker.fgraph.toposort()
3073         assert isinstance(topo[1].op.scalar_op, scal.Sub),\
3074             f.maker.fgraph.toposort()
3075         print(f(val))
3076     def test_local_erf_neg_minus_one(self):
3077         val = np.asarray([-30, -3, -2, -1, 0, 1, 2, 3, 30],
3078                          dtype=config.floatX)
3079         x = T.vector('x')
3080         f = theano.function([x], -1 + T.erfc(-x), mode=self.mode)
3081         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erf],\
3082             f.maker.fgraph.toposort()
3083         print(f(val))
3084         f = theano.function([x], T.erfc(-x) - 1, mode=self.mode)
3085         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erf],\
3086             f.maker.fgraph.toposort()
3087         print(f(val))
3088         f = theano.function([x], T.erfc(-x) + (-1), mode=self.mode)
3089         assert [n.op for n in f.maker.fgraph.toposort()] == [T.erf],\
3090             f.maker.fgraph.toposort()
3091         print(f(val))
3092     def test_local_log_erfc(self):
3093         val = [-30, -27, -26, -11, -10, -3, -2, -1, 0, 1, 2, 3, 10,
3094                11, 26, 27, 28, 30]
3095         if theano.config.mode in ["DebugMode", "DEBUG_MODE", "FAST_COMPILE"]:
3096             val.remove(0)
3097         val = np.asarray(val, dtype=config.floatX)
3098         x = T.vector('x')
3099         mode = copy.copy(self.mode)
3100         mode.check_isfinite = False
3101         mode_fusion = copy.copy(self.mode_fusion)
3102         mode_fusion.check_isfinite = False
3103         f = theano.function([x], T.log(T.erfc(x)), mode=mode)
3104         assert len(f.maker.fgraph.apply_nodes) == 23, len(f.maker.fgraph.apply_nodes)
3105         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3106         assert all(np.isfinite(f(val)))
3107         f = theano.function([x], T.log(T.erfc(-x)), mode=mode)
3108         assert len(f.maker.fgraph.apply_nodes) == 24, len(f.maker.fgraph.apply_nodes)
3109         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3110         assert all(np.isfinite(f(-val)))
3111         f = theano.function([x], T.log(T.erfc(x)), mode=mode_fusion)
3112         assert len(f.maker.fgraph.apply_nodes) == 1, len(f.maker.fgraph.apply_nodes)
3113         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3114         assert len(f.maker.fgraph.toposort()[0].fgraph.toposort()[
3115             0].op.scalar_op.fgraph.apply_nodes) == 22, len(f.maker.fgraph.toposort()[0].fgraph.toposort()[0].op.scalar_op.fgraph.apply_nodes)
3116         if theano.config.floatX == "float32" and theano.config.mode in ["DebugMode", "DEBUG_MODE"]:
3117             raise SkipTest('The python code upcast somewhere internally '
3118                            'some value of float32 to python float for '
3119                            'part of its computation. That make that the '
3120                            'c and python code don\'t generate the same value. '
3121                            'You can ignore this error.')
3122         assert all(np.isfinite(f(val)))
3123     def test_local_grad_log_erfc_neg(self):
3124         val = [-100, -30, -27, -26.4, -26.2, -26, -11, -10, -9, -3, -2, -1, 0,
3125                1, 2, 3, 9, 10, 11, 27, 26.4, 26.2, 26, 28, 30, 100]
3126         if theano.config.mode in ["DebugMode", "DEBUG_MODE", "FAST_COMPILE"]:
3127             val.remove(0)
3128         if theano.config.mode in ["DebugMode", "DEBUG_MODE"] and theano.config.floatX == 'float32':
3129             val.remove(10)
3130         val = np.asarray(val, dtype=config.floatX)
3131         x = T.vector('x')
3132         y = T.vector('y')
3133         mode = copy.copy(self.mode)
3134         mode.check_isfinite = False
3135         mode_fusion = copy.copy(self.mode_fusion)
3136         mode_fusion.check_isfinite = False
3137         f = theano.function([x], T.grad(T.log(T.erfc(x)).sum(), x), mode=mode)
3138         assert len(f.maker.fgraph.apply_nodes) == 23, len(f.maker.fgraph.apply_nodes)
3139         assert all(np.isfinite(f(val)))
3140         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3141         f = theano.function(
3142             [x],
3143             T.mul(T.exp(T.neg(T.sqr(x))), - 10.12837917) / T.erfc(x),
3144             mode=mode)
3145         assert len(f.maker.fgraph.apply_nodes) == 23, len(f.maker.fgraph.apply_nodes)
3146         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3147         assert all(np.isfinite(f(val)))
3148         assert len(f.maker.fgraph.apply_nodes) == 22, len(f.maker.fgraph.apply_nodes)
3149         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3150         assert all(np<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.isfinite(f(val)))
3151         f = theano.function([x, y], T.exp(T.neg(T.sqr(x))) / T.</b></font>erfc(
3152             y), mode=mode)
3153         assert len(f.maker.fgraph.apply_nodes) == 5, len(f.maker.fgraph.apply_nodes)
3154         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3155         f(val, val - 3)
3156         f = theano.function([x], T.exp(T.mul(-1, x, x)) / T.erfc(x), mode=mode)
3157         assert len(f.maker.fgraph.apply_nodes) == 21, len(f.maker.fgraph.apply_nodes)
3158         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3159         assert all(np.isfinite(f(val)))
3160         f = theano.function([x], T.grad(T.log(T.erfc(2 * x)).sum(), x), mode=mode)
3161         assert len(f.maker.fgraph.apply_nodes) == 23, len(f.maker.fgraph.apply_nodes)
3162         assert np.isfinite(f(val)).all()
3163         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3164         f = theano.function([x], T.grad(T.log(T.erfc(x)).sum(), x), mode=mode_fusion)
3165         assert len(f.maker.fgraph.apply_nodes) == 1, len(f.maker.fgraph.apply_nodes)
3166         assert f.maker.fgraph.outputs[0].dtype == theano.config.floatX
3167         if theano.config.floatX == "float32" and theano.config.mode in ["DebugMode", "DEBUG_MODE"]:
3168             pass
3169         else:
3170             assert all(np.isfinite(f(val)))
3171     def speed_local_log_erfc(self):
3172         val = np.random.rand(1e6)
3173         x = T.vector()
3174         mode = theano.compile.mode.get_mode("FAST_RUN")
3175         f1 = theano.function([x], T.log(T.erfc(x)),
3176                              mode=mode.excluding("local_log_erfc"))
3177         f2 = theano.function([x], T.log(T.erfc(x)), mode=mode)
3178         print(f1.maker.fgraph.toposort())
3179         print(f2.maker.fgraph.toposort())
3180         t0 = time.time()
3181         f1(val)
3182         t1 = time.time()
3183         f2(val)
3184         t2 = time.time()
3185         print(t1 - t0, t2 - t1)
3186 class test_local_useless_switch(unittest.TestCase):
3187     def setUp(self):
3188         self.mode = mode_opt.excluding('constant_folding')
3189     def test_const0(self):
3190         for dtype1 in ['int32', 'int64']:
3191             for dtype2 in ['int32', 'int64']:
3192                 x = theano.tensor.matrix('x', dtype=dtype1)
3193                 y = theano.tensor.matrix('y', dtype=dtype2)
3194                 z = theano.tensor.switch(0, x, y)
3195                 f = theano.function([x, y], z, mode=self.mode)
3196                 assert len([node.op for node in f.maker.fgraph.toposort() if
3197                             (isinstance(node.op, theano.tensor.Elemwise) and
3198                              isinstance(node.op.scalar_op,
3199                                         theano.scalar.basic.Switch))]) == 0
3200                 vx = np.array([[1, 2, 3], [4, 5, 6]], dtype=dtype1)
3201                 vy = np.array([[7, 8, 9], [10, 11, 12]], dtype=dtype2)
3202                 assert np.all(f(vx, vy) == vy)
3203     def test_const1(self):
3204         for dtype1 in ['int32', 'int64']:
3205             for dtype2 in ['int32', 'int64']:
3206                 x = theano.tensor.matrix('x', dtype=dtype1)
3207                 y = theano.tensor.matrix('y', dtype=dtype2)
3208                 z = theano.tensor.switch(1, x, y)
3209                 f = theano.function([x, y], z, mode=self.mode)
3210                 assert len([node.op for node in f.maker.fgraph.toposort() if
3211                             (isinstance(node.op, theano.tensor.Elemwise) and
3212                              isinstance(node.op.scalar_op,
3213                                         theano.scalar.basic.Switch))]) == 0
3214                 vx = np.array([[1, 2, 3], [4, 5, 6]], dtype=dtype1)
3215                 vy = np.array([[7, 8, 9], [10, 11, 12]], dtype=dtype2)
3216                 assert np.all(f(vx, vy) == vx)
3217     def test_left_is_right(self):
3218         for dtype1 in ['int32', 'int64']:
3219             x = theano.tensor.matrix('x', dtype=dtype1)
3220             varc = theano.tensor.matrix('varc', dtype=dtype1)
3221             z1 = theano.tensor.switch(1, x, x)
3222             z0 = theano.tensor.switch(0, x, x)
3223             z2 = theano.tensor.switch(varc, x, x)
3224             f1 = theano.function([x], z1, mode=self.mode)
3225             f0 = theano.function([x], z0, mode=self.mode)
3226             f2 = theano.function([x, varc], z2, mode=self.mode)
3227             topo = f1.maker.fgraph.toposort()
3228             assert len(topo) == 1
3229             assert topo[0].op == deep_copy_op
3230             topo = f0.maker.fgraph.toposort()
3231             assert len(topo) == 1
3232             assert topo[0].op == deep_copy_op
3233             topo = f2.maker.fgraph.toposort()
3234             assert len(topo) == 1
3235             assert topo[0].op == deep_copy_op
3236             vx = np.array([[1, 2, 3], [4, 5, 6]], dtype=dtype1)
3237             vc = np.array([[1, 2, 3], [4, 5, 6]], dtype=dtype1)
3238             assert np.all(f1(vx) == vx)
3239             assert np.all(f0(vx) == vx)
3240             assert np.all(f2(vx, vc) == vx)
3241     def test_shape_le_0(self):
3242         for dtype1 in ['float32', 'float64']:
3243             x = theano.tensor.matrix('x', dtype=dtype1)
3244             z0 = theano.tensor.switch(theano.tensor.le(x.shape[0], 0), 0, x.shape[0])
3245             f0 = theano.function([x], z0, mode=self.mode)
3246             assert isinstance(f0.maker.fgraph.toposort()[0].op, Shape_i)
3247             z1 = theano.tensor.switch(theano.tensor.le(x.shape[1], 0), 0, x.shape[1])
3248             f1 = theano.function([x], z1, mode=self.mode)
3249             assert isinstance(f1.maker.fgraph.toposort()[0].op, Shape_i)
3250             vx = np.random.randn(0, 5).astype(dtype1)
3251             assert f0(vx) == 0
3252             assert f1(vx) == 5
3253     def test_broadcast1(self):
3254         x = theano.tensor.matrix('x', dtype='int32')
3255         y = theano.tensor.vector('y', dtype='int64')
3256         z = theano.tensor.switch(1, x, y)
3257         f = theano.function([x, y], z, mode=self.mode)
3258         assert len([node.op for node in f.maker.fgraph.toposort() if
3259                     isinstance(node.op, theano.tensor.Elemwise) and
3260                     not isinstance(node.op.scalar_op, theano.scalar.basic.Cast)]) == 0
3261         vx = np.array([[1, 2, 3], [4, 5, 6]], dtype='int32')
3262         vy = np.array([10, 11, 12], dtype='int64')
3263         assert np.all(f(vx, vy) == vx)
3264         z = theano.tensor.switch(0, x, y)
3265         f = theano.function([x, y], z, mode=self.mode)
3266         assert len([node.op for node in f.maker.fgraph.toposort() if
3267                     isinstance(node.op, theano.tensor.Elemwise)]) == 0
3268         vx = np.array([[1, 2, 3], [4, 5, 6]], dtype='int32')
3269         vy = np.array([10, 11, 12], dtype='int64')
3270         assert np.all(f(vx, vy) == vy)
3271     def test_broadcast2(self):
3272         x = theano.tensor.vector('x', dtype='int32')
3273         y = theano.tensor.matrix('y', dtype='int64')
3274         z = theano.tensor.switch(1, x, y)
3275         f = theano.function([x, y], z, mode=self.mode)
3276         assert len([node.op for node in f.maker.fgraph.toposort() if
3277                     isinstance(node.op, theano.tensor.Elemwise) and
3278                     not isinstance(node.op.scalar_op, theano.scalar.basic.Cast)]) == 0
3279         vx = np.array([4, 5, 6], dtype='int32')
3280         vy = np.array([[7, 8, 9], [10, 11, 12]], dtype='int64')
3281         assert np.all(f(vx, vy) == vx)
3282         z = theano.tensor.switch(0, x, y)
3283         f = theano.function([x, y], z, mode=self.mode)
3284         assert len([node.op for node in f.maker.fgraph.toposort() if
3285                     isinstance(node.op, theano.tensor.Elemwise)]) == 0
3286         vx = np.array([4, 5, 6], dtype='int32')
3287         vy = np.array([[7, 8, 9], [10, 11, 12]], dtype='int64')
3288         assert np.all(f(vx, vy) == vy)
3289     def test_broadcast3(self):
3290         x = theano.tensor.matrix('x', dtype='int32')
3291         y = theano.tensor.vector('y', dtype='int64')
3292         z = theano.tensor.switch(x, y, y)
3293         f = theano.function([x, y], z, mode=self.mode)
3294         vx = np.array([[0, 1], [1, 0]], dtype='int32')
3295         vy = np.array([7, 8], dtype='int64')
3296         utt.assert_allclose(f(vx, vy), np.where(vx, vy, vy))
3297         assert len([node.op for node in f.maker.fgraph.toposort() if
3298                     isinstance(node.op, theano.tensor.Elemwise)]) == 0
3299 class test_local_merge_switch_same_cond(unittest.TestCase):
3300     def test_elemwise(self):
3301         mats = theano.tensor.matrices('cabxy')
3302         c, a, b, x, y = mats
3303         s1 = T.switch(c, a, b)
3304         s2 = T.switch(c, x, y)
3305         for op in (T.add, T.sub, T.mul, T.true_div, T.int_div, T.floor_div,
3306                    T.minimum, T.maximum, T.gt, T.lt, T.ge, T.le, T.eq, T.neq,
3307                    T.pow):
3308             g = optimize(FunctionGraph(mats, [op(s1, s2)]))
3309             assert str(g).count('Switch') == 1
3310         mats = theano.tensor.imatrices('cabxy')
3311         c, a, b, x, y = mats
3312         s1 = T.switch(c, a, b)
3313         s2 = T.switch(c, x, y)
3314         for op in (T.and_, T.or_, T.xor,
3315                    T.bitwise_and, T.bitwise_or, T.bitwise_xor):
3316             g = optimize(FunctionGraph(mats, [op(s1, s2)]))
3317             assert str(g).count('Switch') == 1
3318         u, v = theano.tensor.matrices('uv')
3319         s3 = T.switch(c, u, v)
3320         for op in (T.add, T.mul):
3321             g = optimize(FunctionGraph(mats + [u, v], [op(s1, s2, s3)]))
3322             assert str(g).count('Switch') == 1
3323 class T_local_sum_prod(unittest.TestCase):
3324     """
3325     Test sum/prod opts in opt.py
3326     """
3327     def setUp(self):
3328         self.mode = theano.compile.get_default_mode().including('canonicalize',
3329                                                                 'specialize')
3330     def test_local_sum_prod_mul_by_scalar(self):
3331         vect = T.dvector()
3332         mat = T.dmatrix()
3333         scalar1 = T.dscalar()
3334         scalar2 = T.dscalar()
3335         v_val = np.random.rand(2)
3336         m_val = np.random.rand(2, 2)
3337         s1_val = np.random.rand()
3338         s2_val = np.random.rand()
3339         def test_reduction_opt(inputs, inputs_val, reduction_op,
3340                                expected_output, nb_expected_sum_nodes):
3341             mul_out = T.mul(*inputs)
3342             f = theano.function(inputs, reduction_op()(mul_out),
3343                                 mode=self.mode)
3344             out = f(*inputs_val)
3345             utt.assert_allclose(out, expected_output)
3346             prod_nodes = [n for n in f.maker.fgraph.toposort()
3347                           if isinstance(n.op, reduction_op)]
3348             assert len(prod_nodes) == nb_expected_sum_nodes
3349         test_reduction_opt([scalar1], [s1_val], T.Sum, s1_val, 0)
3350         test_reduction_opt([vect, scalar1], [v_val, s1_val], T.Sum,
3351                            s1_val * v_val.sum(), 1)
3352         test_reduction_opt([vect, mat, scalar1], [v_val, m_val, s1_val], T.Sum,
3353                            s1_val * (v_val * m_val).sum(), 1)
3354         test_reduction_opt([scalar1, scalar2], [s1_val, s2_val], T.Sum,
3355                            s1_val * s2_val, 0)
3356         test_reduction_opt([vect, scalar1, scalar2], [v_val, s1_val, s2_val],
3357                            T.Sum, s1_val * s2_val * v_val.sum(), 1)
3358         test_reduction_opt([vect, mat, scalar1, scalar2],
3359                            [v_val, m_val, s1_val, s2_val], T.Sum,
3360                            s1_val * s2_val * (v_val * m_val).sum(), 1)
3361         test_reduction_opt([scalar1], [s1_val], T.elemwise.Prod, s1_val, 0)
3362         test_reduction_opt([vect, scalar1], [v_val, s1_val], T.elemwise.Prod,
3363                            (s1_val * v_val).prod(), 1)
3364         test_reduction_opt([vect, mat, scalar1], [v_val, m_val, s1_val],
3365                            T.elemwise.Prod, (s1_val * v_val * m_val).prod(), 2)
3366         test_reduction_opt([scalar1, scalar2], [s1_val, s2_val],
3367                            T.elemwise.Prod, s1_val * s2_val, 0)
3368         test_reduction_opt([vect, scalar1, scalar2], [v_val, s1_val, s2_val],
3369                            T.elemwise.Prod, (s1_val * s2_val * v_val).prod(),
3370                            1)
3371         test_reduction_opt([vect, mat, scalar1, scalar2],
3372                            [v_val, m_val, s1_val, s2_val], T.elemwise.Prod,
3373                            (s1_val * s2_val * v_val * m_val).prod(), 2)
3374     def test_local_sum_prod_all_to_none(self):
3375         a = T.tensor3()
3376         input = np.arange(3 * 4 * 5, dtype=config.floatX).reshape(3, 4, 5)
3377         f = theano.function([a], a.sum(), mode=self.mode)
3378         assert len(f.maker.fgraph.apply_nodes) == 1
3379         utt.assert_allclose(f(input), input.sum())
3380         f = theano.function([a], a.prod(), mode=self.mode)
3381         assert len(f.maker.fgraph.apply_nodes) == 1
3382         utt.assert_allclose(f(input), input.prod())
3383         f = theano.function([a], a.sum([0, 1, 2]), mode=self.mode)
3384         assert len(f.maker.fgraph.apply_nodes) == 1
3385         utt.assert_allclose(f(input), input.sum())
3386         f = theano.function([a], a.prod([0, 1, 2]), mode=self.mode)
3387         assert len(f.maker.fgraph.apply_nodes) == 1
3388         utt.assert_allclose(f(input), input.prod())
3389         backup = config.warn.sum_sum_bug
3390         config.warn.sum_sum_bug = False
3391         try:
3392             f = theano.function([a], a.sum(0).sum(0).sum(0), mode=self.mode)
3393             assert len(f.maker.fgraph.apply_nodes) == 1
3394             utt.assert_allclose(f(input), input.sum())
3395         finally:
3396             config.warn.sum_sum_bug = backup
3397     def test_local_sum_sum_prod_prod(self):
3398         a = T.tensor3()
3399         input = np.arange(3 * 4 * 5, dtype=config.floatX).reshape(3, 4, 5)
3400         dims = [(0, 0), (1, 0), (2, 0), (0, 1), (1, 1), (2, 1),
3401                 ((0, 1), 0), ((1, 2), 0), (0, (0, 1)),
3402                 (1, (0, 1)), (2, (0, 1))]
3403         backup = config.warn.sum_sum_bug
3404         config.warn.sum_sum_bug = False
3405         def my_prod(data, d, dd):
3406             if not isinstance(d, tuple) and not isinstance(dd, tuple):
3407                 return data.prod(d).prod(dd)
3408             if isinstance(d, tuple):
3409                 d = sorted(d)
3410                 return data.prod(d[1]).prod(d[0]).prod(dd)
3411             else:
3412                 dd = sorted(dd)
3413                 return data.prod(d).prod(dd[1]).prod(dd[0])
3414         def my_sum(data, d, dd):
3415             if not isinstance(d, tuple) and not isinstance(dd, tuple):
3416                 return data.sum(d).sum(dd)
3417             if isinstance(d, tuple):
3418                 d = sorted(d)
3419                 return data.sum(d[1]).sum(d[0]).sum(dd)
3420             else:
3421                 dd = sorted(dd)
3422                 return data.sum(d).sum(dd[1]).sum(dd[0])
3423         def my_sum_prod(data, d, dd):
3424             if not isinstance(d, tuple) and not isinstance(dd, tuple):
3425                 return data.sum(d).prod(dd)
3426             if isinstance(d, tuple):
3427                 d = sorted(d)
3428                 return data.sum(d[1]).sum(d[0]).prod(dd)
3429             else:
3430                 dd = sorted(dd)
3431                 return data.sum(d).prod(dd[1]).prod(dd[0])
3432         try:
3433             for d, dd in dims:
3434                 expected = my_sum(input, d, dd)
3435                 f = theano.function([a], a.sum(d).sum(dd), mode=self.mode)
3436                 utt.assert_allclose(f(input), expected)
3437                 assert len(f.maker.fgraph.apply_nodes) == 1
3438             for d, dd in dims[:6]:
3439                 f = theano.function([a], a.sum(d).sum(dd).
3440                                     sum(0), mode=self.mode)
3441                 utt.assert_allclose(f(input), input.sum(d).sum(dd).sum(0))
3442                 assert len(f.maker.fgraph.apply_nodes) == 1
3443             for d in [0, 1, 2]:
3444                 f = theano.function([a], a.sum(d).sum(None), mode=self.mode)
3445                 utt.assert_allclose(f(input), input.sum(d).sum())
3446                 assert len(f.maker.fgraph.apply_nodes) == 1
3447             f = theano.function([a], a.sum(None).sum(), mode=self.mode)
3448             utt.assert_allclose(f(input), input.sum())
3449             assert len(f.maker.fgraph.apply_nodes) == 1
3450         finally:
3451             config.warn.sum_sum_bug = backup
3452         for d, dd in dims:
3453             expected = my_prod(input, d, dd)
3454             f = theano.function([a], a.prod(d).prod(dd), mode=self.mode)
3455             utt.assert_allclose(f(input), expected)
3456             assert len(f.maker.fgraph.apply_nodes) == 1
3457         for d, dd in dims[:6]:
3458             f = theano.function([a], a.prod(d).prod(dd).
3459                                 prod(0), mode=self.mode)
3460             utt.assert_allclose(f(input), input.prod(d).prod(dd).prod(0))
3461             assert len(f.maker.fgraph.apply_nodes) == 1
3462         for d in [0, 1, 2]:
3463             f = theano.function([a], a.prod(d).prod(None), mode=self.mode)
3464             utt.assert_allclose(f(input), input.prod(d).prod())
3465             assert len(f.maker.fgraph.apply_nodes) == 1
3466         f = theano.function([a], a.prod(None).prod(), mode=self.mode)
3467         utt.assert_allclose(f(input), input.prod())
3468         assert len(f.maker.fgraph.apply_nodes) == 1
3469         for d, dd in dims:
3470             expected = my_sum_prod(input, d, dd)
3471             f = theano.function([a], a.sum(d).prod(dd), mode=self.mode)
3472             utt.assert_allclose(f(input), expected)
3473             assert len(f.maker.fgraph.apply_nodes) == 2
3474         for d, dd in dims[:6]:
3475             f = theano.function([a], a.sum(d).prod(dd).
3476                                 prod(0), mode=self.mode)
3477             utt.assert_allclose(f(input), input.sum(d).prod(dd).prod(0))
3478             assert len(f.maker.fgraph.apply_nodes) == 2
3479         for d in [0, 1, 2]:
3480             f = theano.function([a], a.sum(d).prod(None), mode=self.mode)
3481             utt.assert_allclose(f(input), input.sum(d).prod())
3482             assert len(f.maker.fgraph.apply_nodes) == 2
3483         f = theano.function([a], a.sum(None).prod(), mode=self.mode)
3484         utt.assert_allclose(f(input), input.sum())
3485         assert len(f.maker.fgraph.apply_nodes) == 1
3486     def test_local_sum_prod_alloc(self):
3487         a = T.dtensor3()
3488         input = np.asarray(np.arange(2 * 3 * 4).reshape(2, 3, 4),
3489                            dtype='float64')
3490         mode = self.mode.including('specialize').excluding('fusion')
3491         for t_like, n_like, nb_nodes in [
3492                 (tensor.zeros_like, np.zeros_like, (1, 3, 3, 2)),
3493                 (tensor.ones_like, np.ones_like, (5, 5, 5, 6))]:
3494             f = theano.function([a], t_like(a).sum(None), mode=mode)
3495             utt.assert_allclose(f(input), n_like(input).sum())
3496             assert len(f.maker.fgraph.apply_nodes) == nb_nodes[0]
3497             f = theano.function([a], t_like(a).sum([0, 1, 2]), mode=mode)
3498             utt.assert_allclose(f(input), n_like(input).sum())
3499             assert len(f.maker.fgraph.apply_nodes) == nb_nodes[0]
3500             for d in xrange(3):
3501                 f = theano.function([a], t_like(a).sum(d), mode=mode)
3502                 utt.assert_allclose(f(input), n_like(input).sum(d))
3503                 assert len(f.maker.fgraph.apply_nodes) == nb_nodes[1]
3504                 topo = f.maker.fgraph.toposort()
3505                 assert topo[-1].op == T.alloc
3506                 assert not any([isinstance(node.op, T.Sum) for node in topo])
3507             for i in xrange(3):
3508                 f = theano.function([a], t_like(a).sum(i), mode=mode)
3509                 utt.assert_allclose(f(input), n_like(input).sum(i))
3510                 assert len(f.maker.fgraph.apply_nodes) == nb_nodes[2]
3511                 topo = f.maker.fgraph.toposort()
3512                 assert topo[-1].op == T.alloc
3513                 assert not any([isinstance(node.op, T.Sum) for node in topo])
3514             f = theano.function([a], t_like(a).prod(None), mode=mode)
3515             utt.assert_allclose(f(input), n_like(input).prod())
3516             f = theano.function([a], t_like(a).prod([0, 1, 2]), mode=mode)
3517             utt.assert_allclose(f(input), n_like(input).prod())
3518             for d in range(3):
3519                 f = theano.function([a], t_like(a).prod(d), mode=mode)
3520                 utt.assert_allclose(f(input), n_like(input).prod(d))
3521                 topo = f.maker.fgraph.toposort()
3522                 assert topo[-1].op == T.alloc
3523                 assert not any([isinstance(node.op, T.elemwise.Prod) for node in topo])
3524             for i in range(3):
3525                 f = theano.function([a], t_like(a).prod(i), mode=mode)
3526                 utt.assert_allclose(f(input), n_like(input).prod(i))
3527                 topo = f.maker.fgraph.toposort()
3528                 assert topo[-1].op == T.alloc
3529                 assert not any([isinstance(node.op, T.elemwise.Prod) for node in topo])
3530             backup = config.warn.sum_sum_bug
3531             config.warn.sum_sum_bug = False
3532             try:
3533                 for d, dd in [(0, 0), (1, 0), (2, 0), (0, 1), (1, 1), (2, 1)]:
3534                     f = theano.function([a], t_like(a).
3535                                         sum(d).sum(dd), mode=mode)
3536                     utt.assert_allclose(f(input),
3537                                         n_like(input).sum(d).sum(dd))
3538                     assert len(f.maker.fgraph.apply_nodes) == nb_nodes[3]
3539                     topo = f.maker.fgraph.toposort()
3540                     assert topo[-1].op == T.alloc
3541                     assert not any([isinstance(node.op,
3542                                                T.Sum) for node in topo])
3543             finally:
3544                 config.warn.sum_sum_bug = backup
3545     def test_local_sum_sum_int8(self):
3546         x = tensor.tensor3(dtype='int8')
3547         y = x.sum(axis=0).sum(axis=1)
3548         backup = config.on_opt_error
3549         config.on_opt_error = 'raise'
3550         try:
3551             theano.function([x], y)
3552         finally:
3553             config.on_opt_error = backup
3554     def test_local_sum_sum_dtype(self):
3555         x = tensor.tensor3(dtype='int8')
3556         y = x.sum(axis=0, dtype='int32').sum(axis=1, dtype='int64')
3557         backup = config.on_opt_error
3558         config.on_opt_error = 'raise'
3559         try:
3560             theano.function([x], y)
3561         finally:
3562             config.on_opt_error = backup
3563     def test_local_sum_prod_mul_by_scalar_stack_trace(self):
3564         m0 = theano.compile.get_default_mode()\
3565             .excluding('inplace_elemwise_opt')\
3566             .including('canonicalize', 'specialize')
3567         vect = T.dvector()
3568         mat = T.dmatrix()
3569         scalar = T.dscalar()
3570         f = theano.function([vect, scalar], T.sum(vect * scalar), mode=m0)
3571         assert check_stack_trace(f, ops_to_check='all')
3572         f = theano.function([vect], T.sum(-vect), mode=m0)
3573         assert check_stack_trace(f, ops_to_check=[T.Sum])
3574         f = theano.function([vect, scalar],
3575                             T.elemwise.Prod()(vect * scalar), mode=m0)
3576         assert check_stack_trace(f, ops_to_check=[T.elemwise.Prod])
3577         f = theano.function([vect], T.elemwise.Prod()(-vect), mode=m0)
3578         assert check_stack_trace(f, ops_to_check=[T.elemwise.Prod])
3579         f = theano.function([mat, scalar], T.sum(mat * scalar), mode=m0)
3580         assert check_stack_trace(f, ops_to_check='all')
3581         f = theano.function([mat], T.sum(-mat), mode=m0)
3582         assert check_stack_trace(f, ops_to_check=[T.Sum])
3583 class T_local_opt_alloc(unittest.TestCase):
3584     dtype = 'float32'
3585     def test_sum_upcast(self):
3586         s = theano.tensor.lscalar()
3587         a = theano.tensor.alloc(np.asarray(5, dtype=self.dtype), s, s)
3588         orig = theano.config.warn_float64
3589         theano.config.warn_float64 = "raise"
3590         try:
3591             f = theano.function([s], a.sum())
3592             f(5)
3593         finally:
3594             theano.config.warn_float64 = orig
3595     def test_prod_upcast(self):
3596         s = theano.tensor.lscalar()
3597         a = theano.tensor.alloc(np.asarray(5, dtype=self.dtype), s, s)
3598         orig = theano.config.warn_float64
3599         theano.config.warn_float64 = "raise"
3600         try:
3601             f = theano.function([s], a.prod())
3602             f(5)
3603         finally:
3604             theano.config.warn_float64 = orig
3605     @change_flags(on_opt_error='raise')
3606     def test_sum_bool_upcast(self):
3607         s = theano.tensor.lscalar()
3608         a = theano.tensor.alloc(np.asarray(True, dtype='bool'), s, s)
3609         f = theano.function([s], a.sum())
3610         f(5)
3611         f = theano.function([s], a.sum(dtype=self.dtype))
3612         f(5)
3613         f = theano.function([s], a.sum(axis=0, dtype=self.dtype))
3614         f(5)
3615         print(self.dtype)
3616 class T_local_opt_alloc_f16(T_local_opt_alloc):
3617     dtype = 'float16'
3618 class T_local_reduce(unittest.TestCase):
3619     def setUp(self):
3620         self.mode = theano.compile.get_default_mode().including(
3621             'canonicalize',
3622             'specialize',
3623             'uncanonicalize', 'local_max_and_argmax')
3624     def test_local_reduce_broadcast_all_0(self):
3625                     tensor.max, tensor.min]:
3626             x = T.TensorType('int64', (True, True, True))()
3627             f = theano.function([x], [fct<font color="#571b7e"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>(x)], mode=self.mode)
3628             assert not any([
3629                 isinstance(node.op, T.CAReduce)
3630                 for node in f.maker.fgraph.</b></font>toposort()])
3631     def test_local_reduce_broadcast_all_1(self):
3632                     tensor.max, tensor.min]:
3633             x = T.TensorType('int64', (True, True))()
3634             f = theano.function([x], [fct(x, axis=[<font color="#f52887"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 1])], mode=self.mode)
3635             assert not any([
3636                 isinstance(node.op, T.CAReduce)
3637                 for node in f.maker.fgraph.</b></font>toposort()])
3638     def test_local_reduce_broadcast_some_0(self):
3639         for fct in [tensor.sum, tensor.all, tensor.any, tensor.prod,
3640                     tensor.max, tensor.min]:
3641             x = T.TensorType('int64', (True, False, True))()
3642             f = theano.function([x], [fct(x, axis=[0, 1])], mode=self.mode)
3643             order = f.maker.fgraph.toposort()
3644             assert 1 == sum([isinstance(node.op, T.CAReduce)
3645                              for node in order])
3646             node = [node for node in order if isinstance(node.op,
3647                                                          tensor.CAReduce)][0]
3648             op = node.op
3649             assert isinstance(op, T.CAReduce)
3650             assert node.inputs[0].ndim == 2, node
3651             assert op.axis == (0,), op.axis
3652     def test_local_reduce_broadcast_some_1(self):
3653                     tensor.max, tensor.min]:
3654             x = T.TensorType('int64', (True, True, True))()
3655             f = theano.function([x], [fct(x, axis=[<font color="#842dce"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>0, 2])], mode=self.mode)
3656             assert not any([
3657                 isinstance(node.op, T.CAReduce)
3658                 for node in f.maker.fgraph.</b></font>toposort()])
3659     def test_local_reduce_join(self):
3660         vx = matrix()
3661         vy = matrix()
3662         vz = matrix()
3663         x = np.asarray([[1, 0], [3, 4]], dtype=config.floatX)
3664         y = np.asarray([[4, 0], [2, 1]], dtype=config.floatX)
3665         z = np.asarray([[5, 0], [1, 2]], dtype=config.floatX)
3666         for out, res in [
3667             (T.max((vx, vy), 0), np.max((x, y), 0)),
3668             (T.min((vx, vy), 0), np.min((x, y), 0)),
3669             (T.sum((vx, vy, vz), 0), np.sum((x, y, z), 0)),
3670             (T.prod((vx, vy, vz), 0), np.prod((x, y, z), 0)),
3671             (T.prod((vx, vy.T, vz), 0), np.prod((x, y.T, z), 0)),
3672         ]:
3673             f = theano.function([vx, vy, vz], out,
3674                                 on_unused_input='ignore', mode=self.mode)
3675             assert (f(x, y, z) == res).all(), out
3676             topo = f.maker.fgraph.toposort()
3677             assert len(topo) &lt;= 2, out
3678             assert isinstance(topo[-1].op, T.Elemwise), out
3679         A = theano.shared(np.array([1, 2, 3, 4, 5], dtype='int64'))
3680         f = theano.function([], T.sum(T.stack([A, A]), axis=0), mode=self.mode)
3681         utt.assert_allclose(f(), [2, 4, 6, 8, 10])
3682         topo = f.maker.fgraph.toposort()
3683         assert isinstance(topo[-1].op, T.Elemwise)
3684         try:
3685             old = theano.config.warn.reduce_join
3686             theano.config.warn.reduce_join = False
3687             f = theano.function([], T.sum(T.stack([A, A]), axis=1),
3688                                 mode=self.mode)
3689         finally:
3690             theano.config.warn.reduce_join = old
3691         utt.assert_allclose(f(), [15, 15])
3692         topo = f.maker.fgraph.toposort()
3693         assert not isinstance(topo[-1].op, T.Elemwise)
3694         A = theano.shared(np.array([1, 2, 3, 4, 5]).reshape(5, 1))
3695         f = theano.function([], T.sum(T.concatenate((A, A), axis=1), axis=1),
3696                             mode=self.mode)
3697         utt.assert_allclose(f(), [2, 4, 6, 8, 10])
3698         topo = f.maker.fgraph.toposort()
3699         assert not isinstance(topo[-1].op, T.Elemwise)
3700         A = theano.shared(np.array([1, 2, 3, 4, 5]).reshape(5, 1))
3701         f = theano.function([], T.sum(T.concatenate((A, A), axis=1), axis=0),
3702                             mode=self.mode)
3703         utt.assert_allclose(f(), [15, 15])
3704         topo = f.maker.fgraph.toposort()
3705         assert not isinstance(topo[-1].op, T.Elemwise)
3706         old = theano.config.warn.reduce_join
3707         try:
3708             theano.config.warn.reduce_join = False
3709             out = tensor.sum([vx, vy, vz], axis=None)
3710             f = theano.function([vx, vy, vz], out)
3711         finally:
3712             theano.config.warn.reduce_join = old
3713 class T_local_sum_prod_dimshuffle(unittest.TestCase):
3714     def setUp(self):
3715         self.mode = theano.compile.get_default_mode().including('canonicalize')
3716     def test_local_sum_div_dimshuffle(self):
3717         a = T.matrix('a')
3718         b = T.vector('b')
3719         c = T.tensor3('c')
3720         d = T.scalar('d')
3721         sum = tensor.sum
3722         sums = [
3723             sum(a / d),
3724             sum(a / d.dimshuffle('x', 'x')),
3725             sum(a / d.dimshuffle('x', 'x'), axis=0),
3726             sum(a / d.dimshuffle('x', 'x'), axis=1),
3727             sum(b / d),
3728             sum(b / d.dimshuffle('x')),
3729             sum(c / d),
3730             sum(c / d.dimshuffle('x', 'x', 'x')),
3731             sum(c / d.dimshuffle('x', 'x', 'x'), axis=0),
3732             sum(c / d.dimshuffle('x', 'x', 'x'), axis=1),
3733             sum(c / d.dimshuffle('x', 'x', 'x'), axis=2),
3734             sum(a / b, axis=0),
3735             sum(a / b.dimshuffle(0, 'x'), axis=1),
3736             sum(a.dimshuffle(0, 1) / b.dimshuffle(0, 'x'), axis=1),
3737             sum(a.dimshuffle(1, 0) / b.dimshuffle(0, 'x'), axis=1),
3738             sum(c / a, axis=0),
3739             sum(c / a.dimshuffle(1, 0), axis=0),
3740             sum(c / a.dimshuffle(0, 'x', 1), axis=1),
3741             sum(c / a.dimshuffle(1, 'x', 0), axis=1),
3742             sum(c / a.dimshuffle(0, 1, 'x'), axis=2),
3743             sum(c / a.dimshuffle(1, 0, 'x'), axis=2),
3744             sum(c / b, axis=0),
3745             sum(c / b, axis=1),
3746             sum(c / b, axis=(0, 1)),
3747             sum(c / b.dimshuffle(0, 'x'), axis=0),
3748             sum(c / b.dimshuffle(0, 'x'), axis=2),
3749             sum(c / b.dimshuffle(0, 'x'), axis=(0, 2)),
3750             sum(c / b.dimshuffle(0, 'x', 'x'), axis=1),
3751             sum(c / b.dimshuffle(0, 'x', 'x'), axis=2),
3752             sum(c / b.dimshuffle(0, 'x', 'x'), axis=(1, 2)),
3753             sum(sum(c, axis=0) / b, axis=0),
3754             sum(sum(c, axis=1) / b, axis=0),
3755             ]
3756         rng = np.random.RandomState(utt.fetch_seed())
3757         a_val = rng.randn(2, 2).astype(config.floatX)
3758         b_val = rng.randn(2).astype(config.floatX)
3759         c_val = rng.randn(2, 2, 2).astype(config.floatX)
3760         d_val = np.asarray(rng.randn(), config.floatX)
3761         backup = config.warn.sum_sum_bug, config.warn.sum_div_dimshuffle_bug
3762         config.warn.sum_sum_bug = False
3763         config.warn.sum_div_dimshuffle_bug = False
3764         try:
3765             for i, s in enumerate(sums):
3766                 print(i)
3767                 f = theano.function([a, b, c, d], s, mode=self.mode,
3768                                     on_unused_input='ignore')
3769                 g = f.maker.fgraph.toposort()
3770                 assert isinstance(g[-1].op.scalar_op,
3771                                   theano.scalar.basic.TrueDiv)
3772                 f(a_val, b_val, c_val, d_val)
3773         finally:
3774             config.warn.sum_sum_bug, config.warn.sum_div_dimshuffle_bug =\
3775                 backup
3776     def test_local_prod_div_dimshuffle(self):
3777         a = T.matrix('a')
3778         b = T.vector('b')
3779         c = T.tensor3('c')
3780         e = T.matrix('e')
3781         d = T.scalar('d')
3782         prod = T.prod
3783         prods = [
3784             prod(a / d),
3785             prod(a / d.dimshuffle('x', 'x')),
3786             prod(a / d.dimshuffle('x', 'x'), axis=0),
3787             prod(a / d.dimshuffle('x', 'x'), axis=1),
3788             prod(b / d),
3789             prod(b / d.dimshuffle('x')),
3790             prod(c / d),
3791             prod(c / d.dimshuffle('x', 'x', 'x')),
3792             prod(c / d.dimshuffle('x', 'x', 'x'), axis=0),
3793             prod(c / d.dimshuffle('x', 'x', 'x'), axis=1),
3794             prod(c / d.dimshuffle('x', 'x', 'x'), axis=2),
3795             prod(a / b, axis=0),
3796             prod(a / b.dimshuffle(0, 'x'), axis=1),
3797             prod(a.dimshuffle(0, 1) / b.dimshuffle(0, 'x'), axis=1),
3798             prod(a.dimshuffle(1, 0) / b.dimshuffle(0, 'x'), axis=1),
3799             prod(c / a, axis=0),
3800             prod(c / a.dimshuffle(1, 0), axis=0),
3801             prod(c / a.dimshuffle(0, 'x', 1), axis=1),
3802             prod(c / a.dimshuffle(1, 'x', 0), axis=1),
3803             prod(c / a.dimshuffle(0, 1, 'x'), axis=2),
3804             prod(c / a.dimshuffle(1, 0, 'x'), axis=2),
3805             prod(c / b, axis=0),
3806             prod(c / b, axis=1),
3807             prod(c / b, axis=(0, 1)),
3808             prod(c / b.dimshuffle(0, 'x'), axis=0),
3809             prod(c / b.dimshuffle(0, 'x'), axis=2),
3810             prod(c / b.dimshuffle(0, 'x'), axis=(0, 2)),
3811             prod(c / b.dimshuffle(0, 'x', 'x'), axis=1),
3812             prod(c / b.dimshuffle(0, 'x', 'x'), axis=2),
3813             prod(c / b.dimshuffle(0, 'x', 'x'), axis=(1, 2)),
3814             prod(c / b.dimshuffle(0, 'x', 'x'), axis=(0, 1)),
3815             prod(c / b.dimshuffle(0, 'x', 'x'), axis=(1, 0)),
3816             prod(prod(c, axis=0) / b, axis=0),
3817             prod(prod(c, axis=1) / b, axis=0)]
3818         rng = np.random.RandomState(utt.fetch_seed())
3819         a_val = rng.randn(2, 2).astype(config.floatX)
3820         b_val = rng.randn(2).astype(config.floatX)
3821         c_val = rng.randn(2, 2, 2).astype(config.floatX)
3822         d_val = np.asarray(rng.randn(), config.floatX)
3823         default_mode = theano.compile.mode.get_default_mode()
3824         mode_with_opt = default_mode.including('local_sum_prod_div_dimshuffle',
3825                                                'FusionOptimizer')
3826         mode_without_opt = default_mode.excluding('local_sum_prod_div_dimshuffle')
3827         for i, s in enumerate(prods):
3828             f = theano.function([a, b, c, d], s,
3829                                 on_unused_input='ignore',
3830                                 mode=mode_without_opt)
3831             g = theano.function([a, b, c, d], s,
3832                                 on_unused_input='ignore',
3833                                 mode=mode_with_opt)
3834             utt.assert_allclose(f(a_val, b_val, c_val, d_val),
3835                                 g(a_val, b_val, c_val, d_val))
3836         prods = [
3837             prod(a / e),
3838             prod(a / d),
3839             prod(a / d.dimshuffle('x', 'x')),
3840             prod(c / d.dimshuffle('x', 'x', 'x'), axis=1),
3841             prod(a.dimshuffle(1, 0) / b.dimshuffle(0, 'x'), axis=1),
3842             prod(c / b.dimshuffle(0, 'x', 'x'), axis=(1, 0)),
3843             prod(prod(c, axis=1) / b, axis=0),
3844             prod(prod(c, axis=(1, 2)) / b, axis=0)]
3845         expected_outer_operator = [theano.scalar.basic.Mul,
3846                                    theano.scalar.basic.Composite,
3847                                    theano.scalar.basic.Composite,
3848                                    theano.scalar.basic.TrueDiv,
3849                                    theano.scalar.basic.Composite,
3850                                    theano.scalar.basic.Mul,
3851                                    theano.scalar.basic.Composite,
3852                                    theano.scalar.basic.Mul]
3853         for i, s in enumerate(prods):
3854             g = theano.function([a, b, c, d, e], s,
3855                                 on_unused_input='ignore',
3856                                 mode=mode_with_opt)
3857             assert isinstance(g.maker.fgraph.toposort()[-1].op.scalar_op,
3858                               expected_outer_operator[i])
3859 class TestMakeVector(utt.InferShapeTester):
3860     def setUp(self):
3861         super(TestMakeVector, self).setUp()
3862     def test_make_vector(self):
3863         b = T.bscalar()
3864         i = T.iscalar()
3865         d = T.dscalar()
3866         val = {b: 2,
3867                i: -3,
3868                d: 0.7}
3869         for (dtype, inputs) in [("int8", (b, b)),
3870                                 ("int32", (i, b)),
3871                                 ("int32", (b, i)),
3872                                 ("float64", (b, i)),
3873                                 ("float64", (b, d)),
3874                                 ("float64", (d, i)),
3875                                 ("float64", ()),
3876                                 ("int64", ()),
3877                                 ]:
3878             mv = opt.MakeVector(dtype=dtype)(*inputs)
3879             assert mv.dtype == dtype
3880             f = theano.function([b, i, d], mv, on_unused_input='ignore')
3881             f(val[b], val[i], val[d])
3882             s = mv.sum()
3883             gb = T.grad(s, b, disconnected_inputs='ignore')
3884             gi = T.grad(s, i, disconnected_inputs='ignore')
3885             gd = T.grad(s, d, disconnected_inputs='ignore')
3886             g = theano.function([b, i, d], [gb, gi, gd])
3887             g_val = g(val[b], val[i], val[d])
3888             if dtype in tensor.int_dtypes:
3889                 utt.assert_allclose(g_val, 0)
3890             else:
3891                 for var, grval in zip((b, i, d), g_val):
3892                     float_inputs = []
3893                     if var.dtype in tensor.int_dtypes:
3894                         pass
3895                     elif var not in inputs:
3896                         assert grval == 0
3897                     else:
3898                         float_inputs.append(var)
3899                 if float_inputs:
3900                     def fun(*fl_inputs):
3901                         f_inputs = []
3902                         for var in f_inputs:
3903                             if var in fl_inputs:
3904                                 f_inputs.append(var)
3905                             else:
3906                                 f_inputs.append(val[var])
3907                         return opt.MakeVector(dtype=dtype)(*f_inputs)
3908                     utt.verify_grad(fun, [val[ri] for ri in float_inputs])
3909         for (dtype, inputs) in [("int8", (b, i)),
3910                                 ("int8", (i, b)),
3911                                 ("int8", (b, d)),
3912                                 ("int8", (i, i)),
3913                                 ("int32", (d, i)),
3914                                 ("int32", (i, d)),
3915                                 ("float32", (i, d)),
3916                                 ]:
3917             try:
3918                 opt.MakeVector(dtype=dtype)(*inputs)
3919                 raise Exception("Theano should have raised an error")
3920             except AssertionError:
3921                 pass
3922     def test_infer_shape(self):
3923         adscal = dscalar()
3924         bdscal = dscalar()
3925         aiscal = iscalar()
3926         biscal = iscalar()
3927         ciscal = iscalar()
3928         discal = iscalar()
3929         adscal_val = np.random.rand()
3930         bdscal_val = np.random.rand()
3931         aiscal_val = np.random.randint(10)
3932         biscal_val = np.random.randint(10)
3933         ciscal_val = np.random.randint(10)
3934         discal_val = np.random.randint(10)
3935         self._compile_and_check([adscal, aiscal],
3936                                 [MakeVector('float64')(adscal, aiscal)],
3937                                 [adscal_val, aiscal_val], MakeVector)
3938         self._compile_and_check([adscal, bdscal, aiscal],
3939                                 [MakeVector('float64')(adscal, bdscal, aiscal)],
3940                                 [adscal_val, bdscal_val, aiscal_val], MakeVector)
3941         self._compile_and_check([aiscal, biscal, ciscal, discal],
3942                                 [MakeVector('int32')(aiscal, biscal, ciscal, discal)],
3943                                 [aiscal_val, biscal_val, ciscal_val, discal_val],
3944                                 MakeVector)
3945 def test_local_join_1():
3946     a = tensor.vector('a')
3947     s = tensor.stack([a])
3948     f = function([a], s, mode=mode_opt)
3949     val = f([1])
3950     assert np.all(val == [1])
3951     e = f.maker.fgraph.toposort()
3952     assert len([n for n in e if isinstance(n.op, Join)]) == 0
3953     assert f.maker.fgraph.outputs[0].dtype == config.floatX
3954     a = tensor.matrix('a')
3955     s = join(0, a)
3956     f = function([a], s, mode=mode_opt)
3957     val = f([[1]])
3958     assert np.all(val == [[1]])
3959     e = f.maker.fgraph.toposort()
3960     assert len([n for n in e if isinstance(n.op, Join)]) == 0
3961     assert f.maker.fgraph.outputs[0].dtype == config.floatX
3962     s = join(1, a)
3963     f = function([a], s, mode=mode_opt)
3964     val = f([[1]])
3965     assert np.all(val == [[1]])
3966     e = f.maker.fgraph.toposort()
3967     assert len([n for n in e if isinstance(n.op, Join)]) == 0
3968     assert f.maker.fgraph.outputs[0].dtype == config.floatX
3969     s = join(1, a, a)
3970     f = function([a], s, mode=mode_opt)
3971     val = f([[1]])
3972     assert np.all(val == [[1]])
3973     e = f.maker.fgraph.toposort()
3974     assert len([n for n in e if isinstance(n.op, Join)]) == 1
3975     assert f.maker.fgraph.outputs[0].dtype == config.floatX
3976 def test_local_join_empty():
3977     empty_vec = np.asarray([], dtype=config.floatX)
3978     a = tensor.vector('a')
3979     s = tensor.join(0, a, a, empty_vec)
3980     f = function([a], s, mode=mode_opt)
3981     val = f([1])
3982     assert np.all(val == [1])
3983     e = f.maker.fgraph.toposort()
3984     assert len([n for n in e if isinstance(n.op, Join)]) == 1
3985     assert all([not isinstance(n.op, Join) or len(n.inputs) == 3
3986                 for n in e if isinstance(n.op, Join)])
3987     assert f.maker.fgraph.outputs[0].dtype == config.floatX
3988     empty_mat = np.asarray([[]], dtype=config.floatX)
3989     m = tensor.matrix('m')
3990     s = join(1, empty_mat, m, m, m)
3991     f = function([m], s, mode=mode_opt)
3992     val = f([[1]])
3993     assert np.all(val == [[1]])
3994     e = f.maker.fgraph.toposort()
3995     assert len([n for n in e if isinstance(n.op, Join)]) == 1
3996     assert all([not isinstance(n.op, Join) or len(n.inputs) == 4
3997                 for n in e if isinstance(n.op, Join)])
3998     assert f.maker.fgraph.outputs[0].dtype == config.floatX
3999     s = tensor.stack([a, a, empty_vec])
4000     f = function([a], s, mode=mode_opt)
4001     val = f([])
4002     assert np.all(val == [1])
4003     e = f.maker.fgraph.toposort()
4004     assert len([n for n in e if isinstance(n.op, Join)]) == 1
4005     assert all([not isinstance(n.op, Join) or len(n.inputs) == 4
4006                 for n in e if isinstance(n.op, Join)])
4007     assert f.maker.fgraph.outputs[0].dtype == config.floatX
4008     s = join(0, m, np.asarray([[2.]], dtype=config.floatX), m)
4009     f = function([m], s, mode=mode_opt)
4010     val = f([[1]])
4011     assert np.all(val == [[1], [2], [1]])
4012     e = f.maker.fgraph.toposort()
4013     assert len([n for n in e if isinstance(n.op, Join)]) == 1
4014     assert all([not isinstance(n.op, Join) or len(n.inputs) == 4
4015                 for n in e if isinstance(n.op, Join)])
4016     assert f.maker.fgraph.outputs[0].dtype == config.floatX
4017 def test_local_join_make_vector():
4018     a, b, c, d, e = tensor.scalars('abcde')
4019     v = tensor.vector('v')
4020     mv = MakeVector(config.floatX)
4021     s = tensor.join(0, mv(a), v, mv(b, c), mv(d, e))
4022     f = function([a, b, c, d, e, v], s, mode=mode_opt)
4023     theano.printing.debugprint(f)
4024     val = f(1, 2, 3, 4, 6, [7, 8])
4025     assert np.all(val == [1, 7, 8, 2, 3, 4, 6])
4026     e = f.maker.fgraph.toposort()
4027     assert len([n for n in e if isinstance(n.op, Join)]) == 1
4028     assert all([not isinstance(n.op, Join) or len(n.inputs) == 4
4029                 for n in e if isinstance(n.op, Join)])
4030     assert f.maker.fgraph.outputs[0].dtype == config.floatX
4031     assert check_stack_trace(f, ops_to_check='all')
4032 def test_local_add_specialize():
4033     a = tensor.vector()
4034     s = tensor.add(tensor.zeros_like(a))
4035     assert local_add_specialize.transform(s.owner)
4036     a = tensor.scalar()
4037     s = tensor.add(tensor.zeros_like(a))
4038     assert local_add_specialize.transform(s.owner)
4039     a = tensor.constant(0, dtype='int64')
4040     b = tensor.constant(1, dtype='int32')
4041     s = a + b
4042     transformed = local_add_specialize.transform(s.owner)
4043     assert transformed
4044     assert transformed[0].type == s.type
4045 def test_local_tensor_scalar_tensor():
4046     dtypes = ['int8', 'int16', 'int32', 'int64',
4047               'uint8', 'uint16', 'uint32', 'uint64',
4048               'float32', 'float64',
4049               'complex64', 'complex128'
4050               ]
4051     for dtype in dtypes:
4052         t_type = TensorType(dtype=dtype, broadcastable=())
4053         t = t_type()
4054         s = tensor.scalar_from_tensor(t)
4055         t2 = tensor.tensor_from_scalar(s)
4056         f = function([t], t2, mode=mode_opt)
4057         e = f.maker.fgraph.toposort()
4058         cast_nodes = [n for n in e
4059                       if isinstance(n.op, (tensor.TensorFromScalar,
4060                                            tensor.ScalarFromTensor))]
4061         assert len(cast_nodes) == 0
4062         f(0)
4063 def test_local_scalar_tensor_scalar():
4064     dtypes = ['int8', 'int16', 'int32', 'int64',
4065               'uint8', 'uint16', 'uint32', 'uint64',
4066               'float32', 'float64',
4067               'complex64', 'complex128'
4068               ]
4069     for dtype in dtypes:
4070         s_type = theano.scalar.Scalar(dtype=dtype)
4071         s = s_type()
4072         t = tensor.tensor_from_scalar(s)
4073         s2 = tensor.scalar_from_tensor(t)
4074         f = function([s], s2, mode=mode_opt)
4075         e = f.maker.fgraph.toposort()
4076         cast_nodes = [n for n in e
4077                       if isinstance(n.op, (tensor.TensorFromScalar,
4078                                            tensor.ScalarFromTensor))]
4079         assert len(cast_nodes) == 0
4080         f(0)
4081 def test_local_div_to_inv():
4082     num_len_s = tensor.lscalar('num_len')
4083     denom_s = tensor.scalar('denom')
4084     num_v = tensor.alloc(1, num_len_s)
4085     denom_m = denom_s.dimshuffle('x', 'x')
4086     out = num_v / denom_m
4087     assert np.all(out.broadcastable == (True, False))
4088     f = theano.function([num_len_s, denom_s], out)
4089     out_val = f(3, 2.)
4090     assert out_val.shape == (1, 3)
4091     utt.assert_allclose(out_val, 0.5)
4092 def test_local_useless_split():
4093     x = tensor.matrix('x')
4094     splits = tensor.ivector('splits')
4095     opt = tensor.split(x, splits, n_splits=1)
4096     nonopt = tensor.split(x, splits, n_splits=3)
4097     mode = compile.get_default_mode().including("local_useless_split")
4098     f_opt = theano.function([x, splits], opt, mode=mode)
4099     f_nonopt = theano.function([x, splits], nonopt, mode=mode)
4100     f_opt(np.random.rand(4, 4).astype(config.floatX), [4])
4101     f_nonopt(np.random.rand(4, 4).astype(config.floatX), [1, 2, 1])
4102     graph_opt = f_opt.maker.fgraph.toposort()
4103     graph_nonopt = f_nonopt.maker.fgraph.toposort()
4104     assert isinstance(graph_opt[-1].op, DeepCopyOp)
4105     assert len(graph_nonopt) == 1
4106     assert isinstance(graph_nonopt[0].op, tensor.Split)
4107     assert check_stack_trace(f_opt, ops_to_check=[Assert])
4108     assert check_stack_trace(f_nonopt, ops_to_check='all')
4109 def test_local_flatten_lift():
4110     for i in xrange(1, 4):
4111         x = tensor.tensor4()
4112         out = tensor.flatten(T.exp(x), i)
4113         assert out.ndim == i
4114         mode = compile.mode.get_default_mode()
4115         mode = mode.including('local_reshape_lift')
4116         f = theano.function([x], out, mode=mode)
4117         x_np = np.random.rand(5, 4, 3, 2).astype(config.floatX)
4118         out_np = f(x_np)
4119         topo = f.maker.fgraph.toposort()
4120         shape_out_np = tuple(x_np.shape[:i - 1]) + (np.prod(x_np.shape[i - 1:]),)
4121         assert shape_out_np == out_np.shape
4122         reshape_nodes = [n for n in topo if isinstance(n.op, tensor.Reshape)]
4123         assert (len(reshape_nodes) == 1 and
4124                 tensor.is_flat(reshape_nodes[0].outputs[0], ndim=i))
4125         assert isinstance(topo[-1].op, tensor.Elemwise)
4126 class Test_Reshape(unittest.TestCase):
4127     def setUp(self):
4128         self.mode = mode_opt
4129         self.op = tensor.Reshape
4130     def test_local_reshape(self):
4131         a = tensor.fmatrix()
4132         b = self.op(3)(a, [2, 3, 4])
4133         c = self.op(1)(b, [24])
4134         f = theano.function([a], c, mode=self.mode)
4135         topo = f.maker.fgraph.toposort()
4136         assert sum(isinstance(node.op, self.op) for node in topo) == 1
4137         self.assertTrue(check_stack_trace(f, ops_to_check=[self.op]))
4138 class Test_local_useless_reshape(unittest.TestCase):
4139     def setUp(self):
4140         self.rng = np.random<font color="#f62817"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.RandomState(utt.fetch_seed())
4141     def test_0(self):
4142         mode = theano.compile.get_default_mode().including(</b></font>
4143             'local_useless_reshape')
4144         i = T.iscalar('i')
4145         m = theano.tensor.mgrid[0:i, ]
4146         f = theano.function([i], m, mode=mode)
4147         topo = f.maker.fgraph.toposort()
4148         assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4149     def test_1(self):
4150         x = theano.tensor.matrix('x')
4151         r = x.reshape(x.shape)
4152         m0 = theano.compile.get_default_mode()
4153         m1 = m0.including('local_useless_reshape')
4154         f1 = theano.function([x], r, mode=m1)
4155         topo = f1.maker.fgraph.toposort()
4156         assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4157         m2 = m1.excluding('ShapeOpt')
4158         f2 = theano.function([x], r, mode=m2)
4159         topo = f2.maker.fgraph.toposort()
4160         assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4161     def test_2(self):
4162         x = theano.tensor.matrix('x')
4163         r = x.reshape([Shape_i(i)(x) for i in xrange(x.ndim)])
4164         m0 = theano.compile.get_default_mode()
4165         m1 = m0.including('local_useless_reshape')
4166         f1 = theano.function([x], r, mode=m1)
4167         topo = f1.maker.fgraph.toposort()
4168         assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4169         m2 = m1.excluding('ShapeOpt')
4170         f2 = theano.function([x], r, mode=m2)
4171         topo = f2.maker.fgraph.toposort()
4172         assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4173     def test_m1(self):
4174             x = theano.tensor.matrix('x')
4175             r = x.reshape((x.shape[0], -1))
4176             m0 = theano.compile.get_default_mode()
4177             m1 = m0.including('local_useless_reshape')
4178             f1 = theano.function([x], r, mode=m1)
4179             topo = f1.maker.fgraph.toposort()
4180             assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4181             m2 = m1.excluding('ShapeOpt')
4182             f2 = theano.function([x], r, mode=m2)
4183             topo = f2.maker.fgraph.toposort()
4184             assert not any(isinstance(n.op, tensor.basic.Reshape) for n in topo)
4185 class Test_local_reshape_to_dimshuffle(unittest.TestCase):
4186     def setUp(self):
4187         self.rng = np.random.RandomState(utt.fetch_seed())
4188     def test_1(self):
4189         reshape_lift = out2in(local_reshape_to_dimshuffle)
4190         useless_reshape = out2in(local_useless_reshape)
4191         x = shared(self.rng.randn(4,))
4192         y = shared(self.rng.randn(5, 6))
4193         reshape_x = tensor.reshape(x, (1, 4))
4194         reshape_y = tensor.reshape(y, (1, 5, 1, 6, 1, 1))
4195         g = FunctionGraph([x, y], [reshape_x, reshape_y])
4196         self.assertTrue(str(g) == ("[Reshape{2}"
4197                                    "(&lt;TensorType(float64, vector)&gt;, "
4198                                    "TensorConstant{[1 4]}), "
4199                                    "Reshape{6}"
4200                                    "(&lt;TensorType(float64, matrix)&gt;, "
4201                                    "TensorConstant{[1 5 1 6 1 1]})]"))
4202         reshape_lift.optimize(g)
4203         useless_reshape.optimize(g)
4204         self.assertTrue(str(g) == "[InplaceDimShuffle{x,0}"
4205                                   "(&lt;TensorType(float64, vector)&gt;), "
4206                                   "InplaceDimShuffle{x,0,x,1,x,x}"
4207                                   "(Reshape{2}(&lt;TensorType(float64, matrix)&gt;, "
4208                                   "TensorConstant{[5 6]}))]")
4209         assert check_stack_trace(g, ops_to_check=(T.DimShuffle, T.Reshape))
4210 def test_local_reshape_lift():
4211     x = tensor.tensor4()
4212     out = T.exp(x).reshape([x.size])
4213     assert out.ndim == 1
4214     mode = compile.mode.get_default_mode()
4215     mode = mode.including('local_reshape_lift')
4216     f = theano.function([x], out, mode=mode)
4217     f(np.random.rand(5, 4, 3, 2).astype(config.floatX))
4218     topo = f.maker.fgraph.toposort()
4219     assert isinstance(topo[-2].op, tensor.Reshape)
4220     assert isinstance(topo[-1].op, tensor.Elemwise)
4221     assert check_stack_trace(f, ops_to_check='last')
4222 class Test_lift_transpose_through_dot(unittest.TestCase):
4223     def simple_optimize(self, g):
4224         out2in(opt.local_useless_elemwise).optimize(g)
4225         out2in(opt.local_lift_transpose_through_dot).optimize(g)
4226         out2in(opt.local_useless_elemwise).optimize(g)
4227         return g
4228     def test_matrix_matrix(self):
4229         a, b = matrices('ab')
4230         g = self.simple_optimize(FunctionGraph([a, b], [tensor.dot(a, b).T]))
4231         sg = '[dot(InplaceDimShuffle{1,0}(b), InplaceDimShuffle{1,0}(a))]'
4232         assert str(g) == sg, (str(g), sg)
4233         self.assertTrue(check_stack_trace(g, ops_to_check='all'))
4234     def test_row_matrix(self):
4235         a = vector('a')
4236         b = matrix('b')
4237         g = optimize(FunctionGraph(
4238             [a, b],
4239             [tensor.dot(a.dimshuffle('x', 0), b).T]),
4240             level='stabilize')
4241         sg = '[dot(InplaceDimShuffle{1,0}(b), InplaceDimShuffle{0,x}(a))]'
4242         assert str(g) == sg, (str(g), sg)
4243         self.assertTrue(check_stack_trace(g, ops_to_check='all'))
4244     def test_matrix_col(self):
4245         a = vector('a')
4246         b = matrix('b')
4247         g = optimize(FunctionGraph(
4248             [a, b],
4249             [tensor.dot(b, a.dimshuffle(0, 'x')).T]),
4250             level='stabilize')
4251         sg = '[dot(InplaceDimShuffle{x,0}(a), InplaceDimShuffle{1,0}(b))]'
4252         assert str(g) == sg, (str(g), sg)
4253         self.assertTrue(check_stack_trace(g, ops_to_check='all'))
4254 def test_local_upcast_elemwise_constant_inputs():
4255     s = dvector("s")
4256     x = tensor.sum(tensor.log(10 ** s))
4257     f = function([s], [tensor.grad(x, s)])
4258     f([-42, -2.1, -1, -0.5, 0, 0.2, 1, 2, 12])
4259     old = theano.config.floatX
4260     theano.config.floatX = 'float32'
4261     try:
4262         v = lvector()
4263         function([v], theano.tensor.basic.true_div(v, 2))
4264     finally:
4265         theano.config.floatX = old
4266 class TestShape_i(utt.InferShapeTester):
4267     def setUp(self):
4268         super(TestShape_i, self).setUp()
4269     def test_perform(self):
4270         advec = vector()
4271         advec_val = np.random.rand(3).astype(config.floatX)
4272         f = function([advec], Shape_i(0)(advec))
4273         out = f(advec_val)
4274         utt.assert_allclose(out, advec_val.shape[0])
4275         admat = matrix()
4276         admat_val = np.random.rand(4, 3).astype(config.floatX)
4277         for i in xrange(2):
4278             f = function([admat], Shape_i(i)(admat))
4279             out = f(admat_val)
4280             utt.assert_allclose(out, admat_val.shape[i])
4281     def test_infer_shape(self):
4282         admat = matrix()
4283         admat_val = np.random.rand(3, 4).astype(config.floatX)
4284         self._compile_and_check([admat], [Shape_i(0)(admat)],
4285                                 [admat_val], Shape_i)
4286         self._compile_and_check([admat], [Shape_i(1)(admat)],
4287                                 [admat_val], Shape_i)
4288 class TestShapeFeature(unittest.TestCase):
4289     def test_scalar(self):
4290         x = scalar()
4291         cst = T.constant(1).clone()
4292         o = x + cst
4293         fgraph = FunctionGraph([x], [o], clone=False)
4294         shape_feature = opt.ShapeFeature()
4295         fgraph.attach_feature(shape_feature)
4296         assert shape_feature.same_shape(x, o)
4297     def test_vector(self):
4298         x = vector()
4299         cst = T.constant(1).clone()
4300         o = x + cst
4301         fgraph = FunctionGraph([x], [o], clone=False)
4302         shape_feature = opt.ShapeFeature()
4303         fgraph.attach_feature(shape_feature)
4304         assert shape_feature.same_shape(x, o)
4305     def test_vector2(self):
4306         x = vector()
4307         y = vector()
4308         o = x + y
4309         fgraph = FunctionGraph([x, y], [o], clone=False)
4310         shape_feature = opt.ShapeFeature()
4311         fgraph.attach_feature(shape_feature)
4312         assert shape_feature.same_shape(x, o)
4313         assert not shape_feature.same_shape(y, o)
4314     def test_vector_dim(self):
4315         x = vector()
4316         y = vector()
4317         o = x + y
4318         fgraph = FunctionGraph([x, y], [o], clone=False)
4319         shape_feature = opt.ShapeFeature()
4320         fgraph.attach_feature(shape_feature)
4321         assert shape_feature.same_shape(x, o, 0, 0)
4322         assert not shape_feature.same_shape(y, o, 0, 0)
4323     def test_vector_dim_err(self):
4324         x = vector()
4325         y = vector()
4326         o = x + y
4327         fgraph = FunctionGraph([x, y], [o], clone=False)
4328         shape_feature = opt.ShapeFeature()
4329         fgraph.attach_feature(shape_feature)
4330         self.assertRaises(IndexError, shape_feature.same_shape, x, o, 1, 0)
4331         self.assertRaises(IndexError, shape_feature.same_shape, x, o, 0, 1)
4332 def test_assert_op_gradient():
4333     x = T.vector('x')
4334     assert_op = Assert()
4335     cost = T.sum(assert_op(x, x.size &lt; 2))
4336     grad = T.grad(cost, x)
4337     func = theano.function([x], grad)
4338     x_val = np.ones(shape=(1,), dtype=theano.config.floatX)
4339     assert func(x_val) == 1
4340     def setUp(self):
4341         self.mode = theano.compile<font color="#ad5910"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.mode.get_default_mode()
4342         self.mode = self.mode.including('local_intdiv_by_one')
4343     def test1(self):
4344         y = T.tensor4(</b></font>'y')
4345         self.mode = self.mode.excluding('fusion')
4346         f = theano.function([y], y[::-1][::-1], mode=self.mode)
4347         graph = f.maker.fgraph.toposort()
4348         divs = [node for node in graph
4349                 if isinstance(node.op, T.elemwise.Elemwise) and
4350                 isinstance(node.op.scalar_op, theano.scalar.IntDiv)]
4351         assert len(divs) == 0
4352     def test2(self):
4353         y = T.tensor4('y')
4354         z = y // 1
4355         f = theano.function([y], z, mode=self.mode)
4356         graph = f.maker.fgraph.toposort()
4357         divs = [node for node in graph
4358                 if isinstance(node.op, T.elemwise.Elemwise) and
4359                 isinstance(node.op.scalar_op, theano.scalar.IntDiv)]
4360         assert len(divs) == 0
4361     def test3(self):
4362         y = T.tensor4('y')
4363         z = y // np.ones((2, 2, 2, 2))
4364         f = theano.function([y], z, mode=self.mode)
4365         graph = f.maker.fgraph.toposort()
4366         divs = [node for node in graph
4367                 if isinstance(node.op, T.elemwise.Elemwise) and
4368                 isinstance(node.op.scalar_op, theano.scalar.IntDiv)]
4369         assert len(divs) == 0
4370 def test_local_zero_div():
4371     for t in (T.scalar, T.ivector, T.ftensor4):
4372         x = t('x')
4373         for op in (T.int_div, T.true_div):
4374             y = op(0, x)
4375             g = optimize(FunctionGraph([x], [y]))
4376             divs = [node for node in g.toposort()
4377                     if isinstance(node.op, T.elemwise.Elemwise) and
4378                     isinstance(node.op.scalar_op, type(op.scalar_op))]
4379             assert len(divs) == 0
4380             output = g.outputs[0]
4381             assert output.ndim == y.ndim
4382             assert output.type == y.type
4383             assert theano.tensor.get_scalar_constant_value(output) == 0
4384 def test_local_sumsqr2dot():
4385     G = matrix('G')
4386     W = matrix('W')
4387     y = T.sqr(W.dimshuffle('x', 0, 1) * G.dimshuffle(0, 'x', 1)).sum(axis=(1, 2))
4388     MODE = theano.compile.get_default_mode().including('local_sumsqr2dot')
4389     f = function([W, G], y, mode=MODE)
4390     w_val = np.random.rand(4, 3).astype(config.floatX)
4391     g_val = np.random.rand(5, 3).astype(config.floatX)
4392     f_val = f(w_val, g_val)
4393     f_test = np.dot(np.square(g_val), np.square(w_val).sum(axis=0))
4394     utt.assert_allclose(f_val, f_test)
4395     assert any(isinstance(n.op, (tensor.basic.Dot, tensor.blas.Dot22,
4396                                  tensor.blas.Gemv, tensor.blas_c.CGemv))
4397                for n in f.maker.fgraph.toposort())
4398 def test_local_expm1():
4399     x = matrix('x')
4400     u = T.scalar('u')
4401     y = T.exp(x) - 1.
4402     z = T.exp(x) - 2.
4403     t = T.exp(x) - x
4404     s = T.exp(u) - np.ones((4, 3)).astype(config.floatX)
4405     MODE = theano.compile.get_default_mode().including('local_expm1')
4406     f = function([x], y, mode=MODE)
4407     g = function([x], z, mode=MODE)
4408     h = function([x], t, mode=MODE)
4409     r = function([u], s, mode=MODE)
4410     x_val = np.random.rand(4, 3).astype(config.floatX)
4411     f_val = f(x_val)
4412     f_test = function([x], T.expm1(x), mode=MODE)
4413     utt.assert_allclose(f_val, f_test(x_val))
4414     assert any(isinstance(n.op, T.Elemwise) and isinstance(n.op.scalar_op, theano.scalar.basic.Expm1)
4415                for n in f.maker.fgraph.toposort())
4416     assert not any(isinstance(n.op, T.Elemwise) and isinstance(n.op.scalar_op, theano.scalar.basic.Expm1)
4417                    for n in g.maker.fgraph.toposort())
4418     assert not any(isinstance(n.op, T.Elemwise) and isinstance(n.op.scalar_op, theano.scalar.basic.Expm1)
4419                    for n in h.maker.fgraph.toposort())
4420     assert not any(isinstance(n.op, T.Elemwise) and isinstance(n.op.scalar_op, theano.scalar.basic.Expm1)
4421                    for n in r.maker.fgraph.toposort())
4422 def test_local_merge_alloc():
4423     default_mode = theano.compile.mode.get_default_mode()
4424     opt_mode = default_mode.including("local_merge_alloc")
4425     x = T.iscalar('x')
4426     y = T.iscalar('y')
4427     y2 = T.iscalar('y2')
4428     z = T.iscalar('z')
4429     w = T.iscalar('w')
4430     m = T.fscalar('m')
4431     output = T.alloc(T.alloc(m, 1, y, 1, 1), x, y, z, w)
4432     f = theano.function([m, x, y, z, w], output, mode=opt_mode)
4433     topo = f.maker.fgraph.toposort()
4434     assert len(topo) == 1
4435     assert isinstance(topo[0].op, T.Alloc)
4436     o = f(0., 1, 2, 3, 4)
4437     assert o.shape == (1, 2, 3, 4)
4438     output = T.alloc(T.alloc(m, y, 1, 1), x, y, z, w)
4439     f = theano.function([m, x, y, z, w], output, mode=opt_mode)
4440     topo = f.maker.fgraph.toposort()
4441     assert len(topo) == 1
4442     assert isinstance(topo[0].op, T.Alloc)
4443     o = f(0., 1, 2, 3, 4)
4444     assert o.shape == (1, 2, 3, 4)
4445     output = T.alloc(T.alloc(m, y, 1, 1), x, y2, z, w)
4446     f = theano.function([m, x, y, y2, z, w], output, mode=opt_mode)
4447     topo = f.maker.fgraph.toposort()
4448     assert len(topo) == 3
4449     assert isinstance(topo[-2].op, T.opt.Assert)
4450     assert isinstance(topo[-1].op, T.Alloc)
4451     o = f(0., 1, 2, 2, 3, 4)
4452     assert o.shape == (1, 2, 3, 4)
4453     assert_raises((AssertionError, ValueError), f, 0., 1, 2, 5, 3, 4)
4454 def test_local_useless_alloc():
4455     useless_alloc = out2in(local_useless_alloc)
4456     merge_alloc = out2in(local_merge_alloc)
4457     x = T.iscalar('x')
4458     y = T.iscalar('y')
4459     y2 = T.iscalar('y2')
4460     z = T.iscalar('z')
4461     w = T.iscalar('w')
4462     m = T.fscalar('m')
4463     output = T.alloc(T.alloc(m, 1, y, 1, 1), x, y, z, w)
4464     g = FunctionGraph([m, x, y, z, w], [output])
4465     useless_alloc.optimize(g)
4466     merge_alloc.optimize(g)
4467     useless_alloc.optimize(g)
4468     topo = g.toposort()
4469     assert len(topo) == 1
4470     assert isinstance(topo[0].op, T.Alloc)
4471     output = T.alloc(T.alloc(m, y, 1, 1), x, y, z, w)
4472     g = FunctionGraph([m, x, y, z, w], [output])
4473     useless_alloc.optimize(g)
4474     merge_alloc.optimize(g)
4475     useless_alloc.optimize(g)
4476     topo = g.toposort()
4477     assert len(topo) == 1
4478     assert isinstance(topo[0].op, T.Alloc)
4479     output = T.alloc(T.alloc(m, y, 1, 1), x, y2, z, w)
4480     g = FunctionGraph([m, x, y, y2, z, w], [output])
4481     useless_alloc.optimize(g)
4482     merge_alloc.optimize(g)
4483     useless_alloc.optimize(g)
4484     topo = g.toposort()
4485     assert len(topo) == 3
4486     assert isinstance(topo[-2].op, T.opt.Assert)
4487     assert isinstance(topo[-1].op, T.Alloc)
4488 def compile_graph_log_sum_exp(x, axis, dimshuffle_op=None):
4489     sum_exp = T.sum(T.exp(x), axis=axis)
4490     if dimshuffle_op:
4491         sum_exp = dimshuffle_op(sum_exp)
4492     y = T.log(sum_exp)
4493     MODE = theano.compile.get_default_mode().including('local_log_sum_exp')
4494     return function([x], y, mode=MODE)
4495 def check_max_log_sum_exp(x, axis, dimshuffle_op=None):
4496     f = compile_graph_log_sum_exp(x, axis, dimshuffle_op)
4497     fgraph = f.maker.fgraph.toposort()
4498     for node in fgraph:
4499         if (hasattr(node.op, 'scalar_op') and
4500                 node.op.scalar_op == theano.scalar.basic.maximum):
4501             return
4502         if isinstance(node.op, theano.tensor.MaxAndArgmax):
4503             return
4504     raise Exception('No maximum detected after log_sum_exp optimisation')
4505 def test_local_log_sum_exp1():
4506     x = tensor3('x')
4507     check_max_log_sum_exp(x, axis=(0,), dimshuffle_op=None)
4508     check_max_log_sum_exp(x, axis=(1,), dimshuffle_op=None)
4509     check_max_log_sum_exp(x, axis=(2,), dimshuffle_op=None)
4510     check_max_log_sum_exp(x, axis=(0, 1), dimshuffle_op=None)
4511     transpose_op = DimShuffle((<font color="#8c8774"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>False, False), (1, 0))
4512     check_max_log_sum_exp(x, axis=2, dimshuffle_op=transpose_op)
4513     x = TensorType(dtype='floatX', broadcastable=(False, True, False))('x')
4514     sum_keepdims_op = x.</b></font>sum(axis=(0, 1), keepdims=True).owner.op
4515     check_max_log_sum_exp(x, axis=(0, 1), dimshuffle_op=sum_keepdims_op)
4516 def test_local_log_sum_exp2():
4517     x = tensor3('x')
4518     x_val = 1.0 + np.random.rand(4, 3, 2).astype(config.floatX) / 10.0
4519     f = compile_graph_log_sum_exp(x, axis=(1,))
4520     naive_ret = np.log(np.sum(np.exp(x_val), axis=1))
4521     optimised_ret = f(x_val)
4522     assert np.allclose(naive_ret, optimised_ret)
4523     transpose_op = DimShuffle((False, False), (1, 0))
4524     f = compile_graph_log_sum_exp(x, axis=(1,), dimshuffle_op=transpose_op)
4525     naive_ret = np.log(np.sum(np.exp(x_val), axis=1).T)
4526     optimised_ret = f(x_val)
4527     assert np.allclose(naive_ret, optimised_ret)
4528 def test_local_log_sum_exp3():
4529     x = vector('x')
4530     f = compile_graph_log_sum_exp(x, axis=0)
4531     x_val = np.array([-100., 100.]).astype(config.floatX)
4532     optimised_ret = f(x_val)
4533     assert np.allclose(optimised_ret, 100.)
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
