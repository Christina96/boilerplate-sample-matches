
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 9.76933514246947%, Tokens: 10</h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-function_test.py</h3>
            <pre><code>1  from typing import Callable, Tuple
2  from absl.testing import parameterized
3  import sonnet as snt
4  from sonnet.src import test_utils
5  from sonnet.src.conformance import descriptors
6  import tensorflow as tf
7  ModuleFn = Callable[[], snt.Module]
<span onclick='openModal()' class='match'>8  BATCH_MODULES = descriptors.BATCH_MODULES
9  RECURRENT_MODULES = descriptors.RECURRENT_MODULES
10  OPTIMIZER_MODULES = descriptors.OPTIMIZER_MODULES
11  IGNORED_MODULES = descriptors.IGNORED_MODULES
12  class FunctionTest(test_utils.TestCase, parameterized.TestCase):
</span>13    @test_utils.combined_named_parameters(BATCH_MODULES + RECURRENT_MODULES,
14                                          test_utils.named_bools("autograph"))
15    def test_trace(
16        self,
17        module_fn: ModuleFn,
18        input_shape: Tuple[int],
19        dtype: tf.DType,
20        autograph: bool,
21    ):
22      module = module_fn()
23      forward = tf.function(module, autograph=autograph)
24      forward(tf.ones(input_shape, dtype=dtype))
25    @test_utils.combined_named_parameters(BATCH_MODULES + RECURRENT_MODULES,
26                                          test_utils.named_bools("autograph"))
27    def test_create_variables_eagerly(
28        self,
29        module_fn: ModuleFn,
30        input_shape: Tuple[int],
31        dtype: tf.DType,
32        autograph: bool,
33    ):
34      module = module_fn()
35      f = snt.distribute.create_variables_eagerly(module)
36      forward = tf.function(f, autograph=autograph)
37      forward(tf.ones(input_shape, dtype=dtype))
38    @test_utils.combined_named_parameters(BATCH_MODULES + RECURRENT_MODULES,
39                                          test_utils.named_bools("autograph"))
40    def test_trace_batch_agnostic(
41        self,
42        module_fn: ModuleFn,
43        input_shape: Tuple[int],
44        dtype: tf.DType,
45        autograph: bool,
46    ):
47      module = module_fn()
48      forward = tf.function(module, autograph=autograph)
49      input_spec = tf.TensorSpec((None,) + input_shape[1:], dtype=dtype)
50      cf = forward.get_concrete_function(input_spec)
51      cf(tf.ones(input_shape, dtype=dtype))
52    @test_utils.combined_named_parameters(BATCH_MODULES,
53                                          test_utils.named_bools("autograph"))
54    def test_trace_batch_apply_batch_agnostic(
55        self,
56        module_fn: ModuleFn,
57        input_shape: Tuple[int],
58        dtype: tf.DType,
59        autograph: bool,
60    ):
61      module = snt.BatchApply(module_fn())
62      forward = tf.function(module, autograph=autograph)
63      input_shape = (8,) + input_shape
64      input_spec = tf.TensorSpec((None, None) + input_shape[2:], dtype=dtype)
65      cf = forward.get_concrete_function(input_spec)
66      if isinstance(
67          descriptors.unwrap(module.module),
68          (snt.nets.VectorQuantizer, snt.nets.VectorQuantizerEMA)):
69        return
70      cf(tf.ones(input_shape, dtype=dtype))
71    @test_utils.combined_named_parameters(OPTIMIZER_MODULES,
72                                          test_utils.named_bools("autograph"))
73    def test_optimizer_dense(
74        self,
75        optimizer_fn: ModuleFn,
76        input_shape: Tuple[int],
77        dtype: tf.DType,
78        autograph: bool,
79    ):
80      del input_shape, dtype  # Unused.
81      parameters = [tf.Variable([1., 2.])]
82      updates = [tf.constant([5., 5.])]
83      optimizer = optimizer_fn()
84      optimizer_apply = tf.function(optimizer.apply, autograph=autograph)
85      optimizer_apply(updates, parameters)
86    @test_utils.combined_named_parameters(OPTIMIZER_MODULES,
87                                          test_utils.named_bools("autograph"))
88    def test_optimizer_sparse(
89        self,
90        optimizer_fn: ModuleFn,
91        input_shape: Tuple[int],
92        dtype: tf.DType,
93        autograph: bool,
94    ):
95      del input_shape, dtype  # Unused.
96      if self.primary_device == "TPU":
97        self.skipTest("IndexedSlices not supported on TPU.")
98      parameters = [tf.Variable([[1.], [2.]])]
99      updates = [
100          tf.IndexedSlices(
101              tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
102              tf.constant([2, 1]))
103      ]
104      optimizer = optimizer_fn()
105      optimizer_apply = tf.function(optimizer.apply, autograph=autograph)
106      optimizer_apply(updates, parameters)
107  if __name__ == "__main__":
108    tf.test.main()
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-momentum_test.py</h3>
            <pre><code>1  from sonnet.src import test_utils
2  from sonnet.src.optimizers import momentum as momentum_lib
3  from sonnet.src.optimizers import optimizer_tests
4  import tensorflow as tf
<span onclick='openModal()' class='match'>5  CONFIGS = optimizer_tests.named_product(learning_rate=(0.1, 0.01, 0.001),
6                                          momentum=(0.9, 0.5, 0.2),
7                                          use_nesterov=(True, False),
8                                          seed=(28, 2, 90))
9  class ComparisonTest(optimizer_tests.AbstractFuzzTest):
</span>10    def _make_tf(self, learning_rate, momentum, use_nesterov):
11      optimizer = tf.optimizers.SGD(learning_rate=learning_rate,
12                                    momentum=momentum,
13                                    nesterov=use_nesterov)
14      return lambda g, p: optimizer.apply_gradients(zip(g, p))
15    def _make_snt(self, learning_rate, momentum, use_nesterov):
16      optimizer = momentum_lib.Momentum(learning_rate=learning_rate,
17                                        momentum=momentum,
18                                        use_nesterov=use_nesterov)
19      return optimizer.apply
20    @test_utils.combined_named_parameters(CONFIGS)
21    def testComparingSonnetAndTensorFlow(self, config):
22      seed = config.pop("seed")
23      self.assertParametersRemainClose(seed, config)
24  class MomentumTest(optimizer_tests.OptimizerTestBase):
25    def make_optimizer(self, **kwargs):
26      if "learning_rate" not in kwargs:
27        kwargs["learning_rate"] = 0.1
28      if "momentum" not in kwargs:
29        kwargs["momentum"] = 0.9
30      return momentum_lib.Momentum(**kwargs)
31    def testDense(self):
32      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
33      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
34      optimizer = self.make_optimizer(learning_rate=0.1, momentum=0.9)
35      optimizer.apply(updates, parameters)
36      self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],
37                          [x.numpy() for x in parameters])
38      optimizer.apply(updates, parameters)
39      self.assertAllClose([[-0.45, 0.55], [2.13, 3.13]],
40                          [x.numpy() for x in parameters])
41      optimizer.apply(updates, parameters)
42      self.assertAllClose([[-1.805, -0.805], [1.317, 2.317]],
43                          [x.numpy() for x in parameters])
44    def testDenseNesterov(self):
45      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
46      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
47      optimizer = self.make_optimizer(
48          learning_rate=0.1, momentum=0.9, use_nesterov=True)
49      optimizer.apply(updates, parameters)
50      self.assertAllClose([[0.05, 1.05], [2.43, 3.43]],
51                          [x.numpy() for x in parameters])
52      optimizer.apply(updates, parameters)
53      self.assertAllClose([[-1.305, -0.305], [1.617, 2.617]],
54                          [x.numpy() for x in parameters])
55      optimizer.apply(updates, parameters)
56      self.assertAllClose([[-3.0245, -2.0245], [0.5853, 1.5853]],
57                          [x.numpy() for x in parameters])
58    def testSparse(self):
59      if self.primary_device in ("GPU", "TPU"):
60        self.skipTest("IndexedSlices not supported on {}.".format(
61            self.primary_device))
62      parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
63      updates = [
64          tf.IndexedSlices(
65              tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
66              tf.constant([2, 1])),
67          tf.IndexedSlices(
68              tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
69              tf.constant([2, 1]))
70      ]
71      optimizer = self.make_optimizer(learning_rate=3., momentum=0.9)
72      optimizer.apply(updates, parameters)
73      self.assertAllClose([[1.0 - 3.0 * 0.1], [2.0]], parameters[0].numpy())
74      self.assertAllClose([[3.0], [4.0 - 3.0 * 0.01]], parameters[1].numpy())
75      optimizer.apply(updates, parameters)
76      self.assertAllClose([[0.7 - 3.0 * 0.19], [2.0]], parameters[0].numpy())
77      self.assertAllClose([[3.0], [3.97 - 3.0 * 0.019]], parameters[1].numpy())
78      optimizer.apply(updates, parameters)
79      self.assertAllClose([[0.13 - 3.0 * 0.271], [2.0]], parameters[0].numpy())
80      self.assertAllClose([[3.0], [3.913 - 3.0 * 0.0271]], parameters[1].numpy())
81    def testSparseNesterov(self):
82      if self.primary_device in ("GPU", "TPU"):
83        self.skipTest("IndexedSlices not supported on {}.".format(
84            self.primary_device))
85      parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]
86      updates = [
87          tf.IndexedSlices(
88              tf.constant([0.1], shape=[1, 1]), tf.constant([0]),
89              tf.constant([2, 1])),
90          tf.IndexedSlices(
91              tf.constant([0.01], shape=[1, 1]), tf.constant([1]),
92              tf.constant([2, 1]))
93      ]
94      optimizer = self.make_optimizer(
95          learning_rate=3., momentum=0.9, use_nesterov=True)
96      optimizer.apply(updates, parameters)
97      self.assertAllClose([[0.43], [2.0]], parameters[0].numpy())
98      self.assertAllClose([[3.0], [3.943]], parameters[1].numpy())
99      optimizer.apply(updates, parameters)
100      self.assertAllClose([[-0.383], [2.0]], parameters[0].numpy())
101      self.assertAllClose([[3.0], [3.8617]], parameters[1].numpy())
102      optimizer.apply(updates, parameters)
103      self.assertAllClose([[-1.4147], [2.0]], parameters[0].numpy())
104      self.assertAllClose([[3.0], [3.75853]], parameters[1].numpy())
105    def testVariableHyperParams(self):
106      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
107      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
108      learning_rate = tf.Variable(0.1)
109      momentum_coeff = tf.Variable(0.9)
110      optimizer = self.make_optimizer(
111          learning_rate=learning_rate, momentum=momentum_coeff)
112      if optimizer_tests.is_tf_optimizer(optimizer):
113        self.skipTest("TF SGD optimizer doesn't support variable learning rate.")
114      optimizer.apply(updates, parameters)
115      self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],
116                          [x.numpy() for x in parameters])
117      learning_rate.assign(0.01)
118      momentum_coeff.assign(0.09)
119      self.assertAlmostEqual(0.01, optimizer.learning_rate.numpy())
120      self.assertAlmostEqual(0.09, optimizer.momentum.numpy())
121      optimizer.apply(updates, parameters)
122      self.assertAllClose([[0.4455, 1.4455], [2.6673, 3.6673]],
123                          [x.numpy() for x in parameters])
124    def testHyperParamDTypeConversion(self):
125      parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]
126      updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]
127      dtype = tf.float32 if self.primary_device == "TPU" else tf.float64
128      learning_rate = tf.Variable(0.1, dtype=dtype)
129      momentum_coeff = tf.Variable(0.9, dtype=dtype)
130      optimizer = self.make_optimizer(
131          learning_rate=learning_rate, momentum=momentum_coeff)
132      optimizer.apply(updates, parameters)
133      self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],
134                          [x.numpy() for x in parameters])
135    def testAuxVariablesColocatedWithOriginal(self):
136      optimizer = self.make_optimizer(learning_rate=0.1, momentum=0.9)
137      if optimizer_tests.is_tf_optimizer(optimizer):
138        self.skipTest("TF slot variables are in a different location.")
139      with tf.device("CPU:0"):
140        var = tf.Variable(1.0)
141      optimizer.apply([tf.constant(0.1)], [var])
142      self.assertEqual(optimizer.accumulated_momentum[0].device, var.device)
143  class ReferenceMomentumTest(MomentumTest):
144    def make_optimizer(self, **kwargs):
145      if "learning_rate" not in kwargs:
146        kwargs["learning_rate"] = 0.1
147      if "momentum" not in kwargs:
148        kwargs["momentum"] = 0.9
149      if "use_nesterov" in kwargs:
150        kwargs["nesterov"] = kwargs["use_nesterov"]
151        del kwargs["use_nesterov"]
152      if hasattr(tf.keras.optimizers, "legacy"):
153        return optimizer_tests.WrappedTFOptimizer(
154            tf.keras.optimizers.legacy.SGD(**kwargs))
155      return optimizer_tests.WrappedTFOptimizer(tf.keras.optimizers.SGD(**kwargs))
156  if __name__ == "__main__":
157    tf.test.main()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-function_test.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-momentum_test.py</div>
                </div>
                <div class="column column_space"><pre><code>8  BATCH_MODULES = descriptors.BATCH_MODULES
9  RECURRENT_MODULES = descriptors.RECURRENT_MODULES
10  OPTIMIZER_MODULES = descriptors.OPTIMIZER_MODULES
11  IGNORED_MODULES = descriptors.IGNORED_MODULES
12  class FunctionTest(test_utils.TestCase, parameterized.TestCase):
</pre></code></div>
                <div class="column column_space"><pre><code>5  CONFIGS = optimizer_tests.named_product(learning_rate=(0.1, 0.01, 0.001),
6                                          momentum=(0.9, 0.5, 0.2),
7                                          use_nesterov=(True, False),
8                                          seed=(28, 2, 90))
9  class ComparisonTest(optimizer_tests.AbstractFuzzTest):
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    