
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 10.222804718217562%, Tokens: 10</h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-data_test.py</h3>
            <pre><code><span onclick='openModal()' class='match'>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import tempfile
6  import tensorflow as tf
7  import numpy as np
8  from seq2seq.data import split_tokens_decoder
9  from seq2seq.data.parallel_data_provider import make_parallel_data_provider
10  class SplitTokensDecoderTest(tf.test.TestCase):
</span>11    def test_decode(self):
12      decoder = split_tokens_decoder.SplitTokensDecoder(
13          delimiter=" ",
14          tokens_feature_name="source_tokens",
15          length_feature_name="source_len")
16      self.assertEqual(decoder.list_items(), ["source_tokens", "source_len"])
17      data = tf.constant("Hello world ! 笑ｗ")
18      decoded_tokens = decoder.decode(data, ["source_tokens"])
19      decoded_length = decoder.decode(data, ["source_len"])
20      decoded_both = decoder.decode(data, decoder.list_items())
21      with self.test_session() as sess:
22        decoded_tokens_ = sess.run(decoded_tokens)[0]
23        decoded_length_ = sess.run(decoded_length)[0]
24        decoded_both_ = sess.run(decoded_both)
25      self.assertEqual(decoded_length_, 4)
26      np.testing.assert_array_equal(
27          np.char.decode(decoded_tokens_.astype("S"), "utf-8"),
28          ["Hello", "world", "!", "笑ｗ"])
29      self.assertEqual(decoded_both_[1], 4)
30      np.testing.assert_array_equal(
31          np.char.decode(decoded_both_[0].astype("S"), "utf-8"),
32          ["Hello", "world", "!", "笑ｗ"])
33  class ParallelDataProviderTest(tf.test.TestCase):
34    def setUp(self):
35      super(ParallelDataProviderTest, self).setUp()
36      self.source_lines = ["Hello", "World", "!", "笑"]
37      self.target_lines = ["1", "2", "3", "笑"]
38      self.source_to_target = dict(zip(self.source_lines, self.target_lines))
39      self.source_file = tempfile.NamedTemporaryFile()
40      self.target_file = tempfile.NamedTemporaryFile()
41      self.source_file.write("\n".join(self.source_lines).encode("utf-8"))
42      self.source_file.flush()
43      self.target_file.write("\n".join(self.target_lines).encode("utf-8"))
44      self.target_file.flush()
45    def tearDown(self):
46      super(ParallelDataProviderTest, self).tearDown()
47      self.source_file.close()
48      self.target_file.close()
49    def test_reading(self):
50      num_epochs = 50
51      data_provider = make_parallel_data_provider(
52          data_sources_source=[self.source_file.name],
53          data_sources_target=[self.target_file.name],
54          num_epochs=num_epochs,
55          shuffle=True)
56      item_keys = list(data_provider.list_items())
57      item_values = data_provider.get(item_keys)
58      items_dict = dict(zip(item_keys, item_values))
59      self.assertEqual(
60          set(item_keys),
61          set(["source_tokens", "source_len", "target_tokens", "target_len"]))
62      with self.test_session() as sess:
63        sess.run(tf.global_variables_initializer())
64        sess.run(tf.local_variables_initializer())
65        with tf.contrib.slim.queues.QueueRunners(sess):
66          item_dicts_ = [sess.run(items_dict) for _ in range(num_epochs * 3)]
67      for item_dict in item_dicts_:
68        item_dict["target_tokens"] = np.char.decode(
69            item_dict["target_tokens"].astype("S"), "utf-8")
70        item_dict["source_tokens"] = np.char.decode(
71            item_dict["source_tokens"].astype("S"), "utf-8")
72        self.assertEqual(item_dict["source_len"], 2)
73        self.assertEqual(item_dict["source_tokens"][-1], "SEQUENCE_END")
74        self.assertEqual(item_dict["target_len"], 3)
75        self.assertEqual(item_dict["target_tokens"][0], "SEQUENCE_START")
76        self.assertEqual(item_dict["target_tokens"][-1], "SEQUENCE_END")
77        source_joined = " ".join(item_dict["source_tokens"][:-1])
78        expected_target = self.source_to_target[source_joined]
79        np.testing.assert_array_equal(
80            item_dict["target_tokens"],
81            ["SEQUENCE_START"] + expected_target.split(" ") + ["SEQUENCE_END"])
82    def test_reading_without_targets(self):
83      num_epochs = 50
84      data_provider = make_parallel_data_provider(
85          data_sources_source=[self.source_file.name],
86          data_sources_target=None,
87          num_epochs=num_epochs,
88          shuffle=True)
89      item_keys = list(data_provider.list_items())
90      item_values = data_provider.get(item_keys)
91      items_dict = dict(zip(item_keys, item_values))
92      self.assertEqual(set(item_keys), set(["source_tokens", "source_len"]))
93      with self.test_session() as sess:
94        sess.run(tf.global_variables_initializer())
95        sess.run(tf.local_variables_initializer())
96        with tf.contrib.slim.queues.QueueRunners(sess):
97          item_dicts_ = [sess.run(items_dict) for _ in range(num_epochs * 3)]
98      for item_dict in item_dicts_:
99        self.assertEqual(item_dict["source_len"], 2)
100        item_dict["source_tokens"] = np.char.decode(
101            item_dict["source_tokens"].astype("S"), "utf-8")
102        self.assertEqual(item_dict["source_tokens"][-1], "SEQUENCE_END")
103  if __name__ == "__main__":
104    tf.test.main()
</code></pre>
        </div>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-seq2seq_model.py</h3>
            <pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import collections
<span onclick='openModal()' class='match'>6  import tensorflow as tf
7  from seq2seq import graph_utils
8  from seq2seq import losses as seq2seq_losses
9  from seq2seq.contrib.seq2seq.decoder import _transpose_batch_time
10  from seq2seq.data import vocab
11  from seq2seq.graph_utils import templatemethod
12  from seq2seq.decoders.beam_search_decoder import BeamSearchDecoder
13  from seq2seq.inference import beam_search
14  from seq2seq.models.model_base import ModelBase, _flatten_dict
15  class Seq2SeqModel(ModelBase):
</span>16    def __init__(self, params, mode, name):
17      super(Seq2SeqModel, self).__init__(params, mode, name)
18      self.source_vocab_info = None
19      if "vocab_source" in self.params and self.params["vocab_source"]:
20        self.source_vocab_info = vocab.get_vocab_info(self.params["vocab_source"])
21      self.target_vocab_info = None
22      if "vocab_target" in self.params and self.params["vocab_target"]:
23        self.target_vocab_info = vocab.get_vocab_info(self.params["vocab_target"])
24    @staticmethod
25    def default_params():
26      params = ModelBase.default_params()
27      params.update({
28          "source.max_seq_len": 50,
29          "source.reverse": True,
30          "target.max_seq_len": 50,
31          "embedding.dim": 100,
32          "embedding.init_scale": 0.04,
33          "embedding.share": False,
34          "inference.beam_search.beam_width": 0,
35          "inference.beam_search.length_penalty_weight": 0.0,
36          "inference.beam_search.choose_successors_fn": "choose_top_k",
37          "optimizer.clip_embed_gradients": 0.1,
38          "vocab_source": "",
39          "vocab_target": "",
40      })
41      return params
42    def _clip_gradients(self, grads_and_vars):
43      grads_and_vars = super(Seq2SeqModel, self)._clip_gradients(grads_and_vars)
44      clipped_gradients = []
45      variables = []
46      for gradient, variable in grads_and_vars:
47        if "embedding" in variable.name:
48          tmp = tf.clip_by_norm(
49              gradient.values, self.params["optimizer.clip_embed_gradients"])
50          gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)
51        clipped_gradients.append(gradient)
52        variables.append(variable)
53      return list(zip(clipped_gradients, variables))
54    def _create_predictions(self, decoder_output, features, labels, losses=None):
55      predictions = {}
56      predictions.update(_flatten_dict({"features": features}))
57      if labels is not None:
58        predictions.update(_flatten_dict({"labels": labels}))
59      if losses is not None:
60        predictions["losses"] = _transpose_batch_time(losses)
61      output_dict = collections.OrderedDict(
62          zip(decoder_output._fields, decoder_output))
63      decoder_output_flat = _flatten_dict(output_dict)
64      decoder_output_flat = {
65          k: _transpose_batch_time(v)
66          for k, v in decoder_output_flat.items()
67      }
68      predictions.update(decoder_output_flat)
69      if "predicted_ids" in predictions.keys():
70        vocab_tables = graph_utils.get_dict_from_collection("vocab_tables")
71        target_id_to_vocab = vocab_tables["target_id_to_vocab"]
72        predicted_tokens = target_id_to_vocab.lookup(
73            tf.to_int64(predictions["predicted_ids"]))
74        predictions["predicted_tokens"] = predicted_tokens
75      return predictions
76    def batch_size(self, features, labels):
77      return tf.shape(features["source_ids"])[0]
78    @property
79    @templatemethod("source_embedding")
80    def source_embedding(self):
81      return tf.get_variable(
82          name="W",
83          shape=[self.source_vocab_info.total_size, self.params["embedding.dim"]],
84          initializer=tf.random_uniform_initializer(
85              -self.params["embedding.init_scale"],
86              self.params["embedding.init_scale"]))
87    @property
88    @templatemethod("target_embedding")
89    def target_embedding(self):
90      if self.params["embedding.share"]:
91        return self.source_embedding
92      return tf.get_variable(
93          name="W",
94          shape=[self.target_vocab_info.total_size, self.params["embedding.dim"]],
95          initializer=tf.random_uniform_initializer(
96              -self.params["embedding.init_scale"],
97              self.params["embedding.init_scale"]))
98    @templatemethod("encode")
99    def encode(self, features, labels):
100      raise NotImplementedError()
101    @templatemethod("decode")
102    def decode(self, encoder_output, features, labels):
103      raise NotImplementedError()
104    def _get_beam_search_decoder(self, decoder):
105      config = beam_search.BeamSearchConfig(
106          beam_width=self.params["inference.beam_search.beam_width"],
107          vocab_size=self.target_vocab_info.total_size,
108          eos_token=self.target_vocab_info.special_vocab.SEQUENCE_END,
109          length_penalty_weight=self.params[
110              "inference.beam_search.length_penalty_weight"],
111          choose_successors_fn=getattr(
112              beam_search,
113              self.params["inference.beam_search.choose_successors_fn"]))
114      return BeamSearchDecoder(decoder=decoder, config=config)
115    @property
116    def use_beam_search(self):
117      return self.params["inference.beam_search.beam_width"] > 1
118    def _preprocess(self, features, labels):
119      source_vocab_to_id, source_id_to_vocab, source_word_to_count, _ = \
120        vocab.create_vocabulary_lookup_table(self.source_vocab_info.path)
121      target_vocab_to_id, target_id_to_vocab, target_word_to_count, _ = \
122        vocab.create_vocabulary_lookup_table(self.target_vocab_info.path)
123      graph_utils.add_dict_to_collection({
124          "source_vocab_to_id": source_vocab_to_id,
125          "source_id_to_vocab": source_id_to_vocab,
126          "source_word_to_count": source_word_to_count,
127          "target_vocab_to_id": target_vocab_to_id,
128          "target_id_to_vocab": target_id_to_vocab,
129          "target_word_to_count": target_word_to_count
130      }, "vocab_tables")
131      if self.params["source.max_seq_len"] is not None:
132        features["source_tokens"] = features["source_tokens"][:, :self.params[
133            "source.max_seq_len"]]
134        features["source_len"] = tf.minimum(features["source_len"],
135                                            self.params["source.max_seq_len"])
136      features["source_ids"] = source_vocab_to_id.lookup(features[
137          "source_tokens"])
138      if self.params["source.reverse"] is True:
139        features["source_ids"] = tf.reverse_sequence(
140            input=features["source_ids"],
141            seq_lengths=features["source_len"],
142            seq_dim=1,
143            batch_dim=0,
144            name=None)
145      features["source_len"] = tf.to_int32(features["source_len"])
146      tf.summary.histogram("source_len", tf.to_float(features["source_len"]))
147      if labels is None:
148        return features, None
149      labels = labels.copy()
150      if self.params["target.max_seq_len"] is not None:
151        labels["target_tokens"] = labels["target_tokens"][:, :self.params[
152            "target.max_seq_len"]]
153        labels["target_len"] = tf.minimum(labels["target_len"],
154                                          self.params["target.max_seq_len"])
155      labels["target_ids"] = target_vocab_to_id.lookup(labels["target_tokens"])
156      labels["target_len"] = tf.to_int32(labels["target_len"])
157      tf.summary.histogram("target_len", tf.to_float(labels["target_len"]))
158      num_tokens = tf.reduce_sum(labels["target_len"])
159      num_tokens += tf.reduce_sum(features["source_len"])
160      token_counter_var = tf.Variable(0, "tokens_counter")
161      total_tokens = tf.assign_add(token_counter_var, num_tokens)
162      tf.summary.scalar("num_tokens", total_tokens)
163      with tf.control_dependencies([total_tokens]):
164        features["source_tokens"] = tf.identity(features["source_tokens"])
165      graph_utils.add_dict_to_collection(features, "features")
166      if labels:
167        graph_utils.add_dict_to_collection(labels, "labels")
168      return features, labels
169    def compute_loss(self, decoder_output, _features, labels):
170      losses = seq2seq_losses.cross_entropy_sequence_loss(
171          logits=decoder_output.logits[:, :, :],
172          targets=tf.transpose(labels["target_ids"][:, 1:], [1, 0]),
173          sequence_length=labels["target_len"] - 1)
174      loss = tf.reduce_sum(losses) / tf.to_float(
175          tf.reduce_sum(labels["target_len"] - 1))
176      return losses, loss
177    def _build(self, features, labels, params):
178      features, labels = self._preprocess(features, labels)
179      encoder_output = self.encode(features, labels)
180      decoder_output, _, = self.decode(encoder_output, features, labels)
181      if self.mode == tf.contrib.learn.ModeKeys.INFER:
182        predictions = self._create_predictions(
183            decoder_output=decoder_output, features=features, labels=labels)
184        loss = None
185        train_op = None
186      else:
187        losses, loss = self.compute_loss(decoder_output, features, labels)
188        train_op = None
189        if self.mode == tf.contrib.learn.ModeKeys.TRAIN:
190          train_op = self._build_train_op(loss)
191        predictions = self._create_predictions(
192            decoder_output=decoder_output,
193            features=features,
194            labels=labels,
195            losses=losses)
196      graph_utils.add_dict_to_collection(predictions, "predictions")
197      return predictions, loss, train_op
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-data_test.py</div>
                <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-seq2seq_model.py</div>
                <div class="column column_space"><pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  from __future__ import unicode_literals
5  import tempfile
6  import tensorflow as tf
7  import numpy as np
8  from seq2seq.data import split_tokens_decoder
9  from seq2seq.data.parallel_data_provider import make_parallel_data_provider
10  class SplitTokensDecoderTest(tf.test.TestCase):
</pre></code></div>
                <div class="column column_space"><pre><code>6  import tensorflow as tf
7  from seq2seq import graph_utils
8  from seq2seq import losses as seq2seq_losses
9  from seq2seq.contrib.seq2seq.decoder import _transpose_batch_time
10  from seq2seq.data import vocab
11  from seq2seq.graph_utils import templatemethod
12  from seq2seq.decoders.beam_search_decoder import BeamSearchDecoder
13  from seq2seq.inference import beam_search
14  from seq2seq.models.model_base import ModelBase, _flatten_dict
15  class Seq2SeqModel(ModelBase):
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    