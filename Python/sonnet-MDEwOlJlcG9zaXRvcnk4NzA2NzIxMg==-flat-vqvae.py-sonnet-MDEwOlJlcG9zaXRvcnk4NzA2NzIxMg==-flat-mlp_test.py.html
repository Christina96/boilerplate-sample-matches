
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 6.779661016949152%, Tokens: 10</h2>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-vqvae.py</h3>
            <pre><code>1  from sonnet.src import base
2  from sonnet.src import initializers
3  from sonnet.src import moving_averages
4  from sonnet.src import types
5  import tensorflow as tf
6  class VectorQuantizer(base.Module):
7    def __init__(self,
8                 embedding_dim: int,
9                 num_embeddings: int,
10                 commitment_cost: types.FloatLike,
11                 dtype: tf.DType = tf.float32,
12                 name: str = 'vector_quantizer'):
13      super().__init__(name=name)
14      self.embedding_dim = embedding_dim
15      self.num_embeddings = num_embeddings
16      self.commitment_cost = commitment_cost
17      embedding_shape = [embedding_dim, num_embeddings]
18      initializer = initializers.VarianceScaling(distribution='uniform')
19      self.embeddings = tf.Variable(
20          initializer(embedding_shape, dtype), name='embeddings')
21    def __call__(self, inputs, is_training):
22      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
23      distances = (
24          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
25          2 * tf.matmul(flat_inputs, self.embeddings) +
26          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
27      encoding_indices = tf.argmax(-distances, 1)
28      encodings = tf.one_hot(encoding_indices,
29                             self.num_embeddings,
30                             dtype=distances.dtype)
31      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
32      quantized = self.quantize(encoding_indices)
33      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
34      q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs))**2)
35      loss = q_latent_loss + self.commitment_cost * e_latent_loss
36      quantized = inputs + tf.stop_gradient(quantized - inputs)
37      avg_probs = tf.reduce_mean(encodings, 0)
38      perplexity = tf.exp(-tf.reduce_sum(avg_probs *
39                                         tf.math.log(avg_probs + 1e-10)))
40      return {
41          'quantize': quantized,
42          'loss': loss,
43          'perplexity': perplexity,
44          'encodings': encodings,
45          'encoding_indices': encoding_indices,
46          'distances': distances,
47      }
48    def quantize(self, encoding_indices):
49      w = tf.transpose(self.embeddings, [1, 0])
50      return tf.nn.embedding_lookup(w, encoding_indices)
51  class VectorQuantizerEMA(base.Module):
<span onclick='openModal()' class='match'>52    def __init__(self,
53                 embedding_dim,
54                 num_embeddings,
55                 commitment_cost,
56                 decay,
57                 epsilon=1e-5,
58                 dtype=tf.float32,
59                 name='vector_quantizer_ema'):
60      super().__init__(name=name)
61      self.embedding_dim = embedding_dim
62      self.num_embeddings = num_embeddings
63      if not 0 <= decay <= 1:
64        raise ValueError('decay must be in range [0, 1]')
65      self.decay = decay
66      self.commitment_cost = commitment_cost
67      self.epsilon = epsilon
68      embedding_shape = [embedding_dim, num_embeddings]
69      initializer = initializers.VarianceScaling(distribution='uniform')
70      self.embeddings = tf.Variable(
71          initializer(embedding_shape, dtype), trainable=False, name='embeddings')
72      self.ema_cluster_size = moving_averages.ExponentialMovingAverage(
73          decay=self.decay, name='ema_cluster_size')
74      self.ema_cluster_size.initialize(tf.zeros([num_embeddings], dtype=dtype))
75      self.ema_dw = moving_averages.ExponentialMovingAverage(
76          decay=self.decay, name='ema_dw')
77      self.ema_dw.initialize(self.embeddings)
78    def __call__(self, inputs, is_training):
79      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
</span>80      distances = (
81          tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -
82          2 * tf.matmul(flat_inputs, self.embeddings) +
83          tf.reduce_sum(self.embeddings**2, 0, keepdims=True))
84      encoding_indices = tf.argmax(-distances, 1)
85      encodings = tf.one_hot(encoding_indices,
86                             self.num_embeddings,
87                             dtype=distances.dtype)
88      encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])
89      quantized = self.quantize(encoding_indices)
90      e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)
91      if is_training:
92        updated_ema_cluster_size = self.ema_cluster_size(
93            tf.reduce_sum(encodings, axis=0))
94        dw = tf.matmul(flat_inputs, encodings, transpose_a=True)
95        updated_ema_dw = self.ema_dw(dw)
96        n = tf.reduce_sum(updated_ema_cluster_size)
97        updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /
98                                    (n + self.num_embeddings * self.epsilon) * n)
99        normalised_updated_ema_w = (
100            updated_ema_dw / tf.reshape(updated_ema_cluster_size, [1, -1]))
101        self.embeddings.assign(normalised_updated_ema_w)
102        loss = self.commitment_cost * e_latent_loss
103      else:
104        loss = self.commitment_cost * e_latent_loss
105      quantized = inputs + tf.stop_gradient(quantized - inputs)
106      avg_probs = tf.reduce_mean(encodings, 0)
107      perplexity = tf.exp(-tf.reduce_sum(avg_probs *
108                                         tf.math.log(avg_probs + 1e-10)))
109      return {
110          'quantize': quantized,
111          'loss': loss,
112          'perplexity': perplexity,
113          'encodings': encodings,
114          'encoding_indices': encoding_indices,
115          'distances': distances,
116      }
117    def quantize(self, encoding_indices):
118      w = tf.transpose(self.embeddings, [1, 0])
119      return tf.nn.embedding_lookup(w, encoding_indices)
</code></pre>
        </div>
        <div class="column">
            <h3>sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-mlp_test.py</h3>
            <pre><code>1  import itertools
2  from absl.testing import parameterized
3  from sonnet.src import test_utils
4  from sonnet.src.nets import mlp
5  import tensorflow as tf
6  class MLPTest(test_utils.TestCase, parameterized.TestCase):
7    def test_b_init_when_with_bias_false(self):
8      with self.assertRaisesRegex(ValueError, "b_init must not be set"):
9        mlp.MLP([1], with_bias=False, b_init=object())
10    @parameterized.parameters(itertools.product((1, 2, 3), (0.1, 0.0, None)))
11    def test_submodules(self, num_layers, dropout_rate):
12      mod = mlp.MLP([1] * num_layers, dropout_rate=dropout_rate)
13      self.assertLen(mod.submodules, num_layers)
14    @parameterized.parameters(1, 2, 3)
15    def test_applies_activation(self, num_layers):
16      activation = CountingActivation()
17      mod = mlp.MLP([1] * num_layers, activation=activation)
18      mod(tf.ones([1, 1]))
19      self.assertEqual(activation.count, num_layers - 1)
20    @parameterized.parameters(1, 2, 3)
21    def test_activate_final(self, num_layers):
22      activation = CountingActivation()
23      mod = mlp.MLP([1] * num_layers, activate_final=True, activation=activation)
24      mod(tf.ones([1, 1]))
25      self.assertEqual(activation.count, num_layers)
26    @parameterized.parameters(1, 2, 3)
27    def test_adds_index_to_layer_names(self, num_layers):
28      mod = mlp.MLP([1] * num_layers)
29      for index, linear in enumerate(mod.submodules):
30        self.assertEqual(linear.name, "linear_%d" % index)
31    @parameterized.parameters(False, True)
32    def test_passes_with_bias_to_layers(self, with_bias):
33      mod = mlp.MLP([1, 1, 1], with_bias=with_bias)
34      for linear in mod.submodules:
35        self.assertEqual(linear.with_bias, with_bias)
<span onclick='openModal()' class='match'>36    def test_repeat_initializer(self):
37      w_init = CountingInitializer()
38      b_init = CountingInitializer()
39      mod = mlp.MLP([1, 1, 1], w_init=w_init, b_init=b_init)
40      mod(tf.ones([1, 1]))
41      self.assertEqual(w_init.count, 3)
42      self.assertEqual(b_init.count, 3)
43    def test_default_name(self):
44      mod = mlp.MLP([1])
45      self.assertEqual(mod.name, "mlp")
</span>46    def test_custom_name(self):
47      mod = mlp.MLP([1], name="foobar")
48      self.assertEqual(mod.name, "foobar")
49    def test_reverse_default_name(self):
50      mod = reversed_mlp()
51      self.assertEqual(mod.name, "mlp_reversed")
52    def test_reverse_custom_name(self):
53      mod = reversed_mlp(name="foobar")
54      self.assertEqual(mod.name, "foobar_reversed")
55    def test_reverse_override_name(self):
56      mod = mlp.MLP([2, 3, 4])
57      mod(tf.ones([1, 1]))
58      rev = mod.reverse(name="foobar")
59      self.assertEqual(rev.name, "foobar")
60    def test_reverse(self):
61      mod = reversed_mlp()
62      self.assertEqual([l.output_size for l in mod.submodules], [3, 2, 1])
63    @parameterized.parameters(True, False)
64    def test_reverse_passed_with_bias(self, with_bias):
65      mod = reversed_mlp(with_bias=with_bias)
66      for linear in mod.submodules:
67        self.assertEqual(linear.with_bias, with_bias)
68    def test_reverse_w_init(self):
69      w_init = CountingInitializer()
70      mod = reversed_mlp(w_init=w_init)
71      for linear in mod.submodules:
72        self.assertIs(linear.w_init, w_init)
73    def test_reverse_b_init(self):
74      b_init = CountingInitializer()
75      mod = reversed_mlp(b_init=b_init)
76      for linear in mod.submodules:
77        self.assertIs(linear.b_init, b_init)
78    def test_reverse_activation(self):
79      activation = CountingActivation()
80      mod = reversed_mlp(activation=activation)
81      activation.count = 0
82      mod(tf.ones([1, 1]))
83      self.assertEqual(activation.count, 2)
84    def test_dropout_requires_is_training(self):
85      mod = mlp.MLP([1, 1], dropout_rate=0.5)
86      with self.assertRaisesRegex(ValueError, "is_training.* is required"):
87        mod(tf.ones([1, 1]))
88    @parameterized.parameters(False, True)
89    def test_no_dropout_rejects_is_training(self, is_training):
90      mod = mlp.MLP([1, 1])
91      with self.assertRaisesRegex(ValueError, "is_training.*only.*with dropout"):
92        mod(tf.ones([1, 1]), is_training=is_training)
93    @parameterized.parameters(False, True)
94    def test_reverse_activate_final(self, activate_final):
95      activation = CountingActivation()
96      mod = reversed_mlp(activation=activation, activate_final=activate_final)
97      activation.count = 0
98      mod(tf.ones([1, 1]))
99      self.assertEqual(activation.count, 3 if activate_final else 2)
100    @parameterized.parameters(itertools.product((False, True), (False, True)))
101    def test_applies_activation_with_dropout(self, use_dropout, is_training):
102      activation = CountingActivation()
103      mod = mlp.MLP([1, 1, 1],
104                    dropout_rate=(0.5 if use_dropout else None),
105                    activation=activation)
106      mod(tf.ones([1, 1]), is_training=(is_training if use_dropout else None))
107      self.assertEqual(activation.count, 2)
108    def test_repr(self):
109      mod = mlp.MLP([1, 2, 3])
110      for index, linear in enumerate(mod.submodules):
111        self.assertEqual(
112            repr(linear),
113            "Linear(output_size={}, name='linear_{}')".format(index + 1, index))
114  def reversed_mlp(**kwargs):
115    mod = mlp.MLP([2, 3, 4], **kwargs)
116    mod(tf.ones([1, 1]))
117    return mod.reverse()
118  class CountingActivation:
119    def __init__(self):
120      self.count = 0
121    def __call__(self, x):
122      self.count += 1
123      return x
124  class CountingInitializer:
125    def __init__(self):
126      self.count = 0
127    def __call__(self, shape, dtype=tf.float32):
128      self.count += 1
129      return tf.ones(shape, dtype=dtype)
130  if __name__ == "__main__":
131    tf.test.main()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-vqvae.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from sonnet-MDEwOlJlcG9zaXRvcnk4NzA2NzIxMg==-flat-mlp_test.py</div>
                </div>
                <div class="column column_space"><pre><code>52    def __init__(self,
53                 embedding_dim,
54                 num_embeddings,
55                 commitment_cost,
56                 decay,
57                 epsilon=1e-5,
58                 dtype=tf.float32,
59                 name='vector_quantizer_ema'):
60      super().__init__(name=name)
61      self.embedding_dim = embedding_dim
62      self.num_embeddings = num_embeddings
63      if not 0 <= decay <= 1:
64        raise ValueError('decay must be in range [0, 1]')
65      self.decay = decay
66      self.commitment_cost = commitment_cost
67      self.epsilon = epsilon
68      embedding_shape = [embedding_dim, num_embeddings]
69      initializer = initializers.VarianceScaling(distribution='uniform')
70      self.embeddings = tf.Variable(
71          initializer(embedding_shape, dtype), trainable=False, name='embeddings')
72      self.ema_cluster_size = moving_averages.ExponentialMovingAverage(
73          decay=self.decay, name='ema_cluster_size')
74      self.ema_cluster_size.initialize(tf.zeros([num_embeddings], dtype=dtype))
75      self.ema_dw = moving_averages.ExponentialMovingAverage(
76          decay=self.decay, name='ema_dw')
77      self.ema_dw.initialize(self.embeddings)
78    def __call__(self, inputs, is_training):
79      flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])
</pre></code></div>
                <div class="column column_space"><pre><code>36    def test_repeat_initializer(self):
37      w_init = CountingInitializer()
38      b_init = CountingInitializer()
39      mod = mlp.MLP([1, 1, 1], w_init=w_init, b_init=b_init)
40      mod(tf.ones([1, 1]))
41      self.assertEqual(w_init.count, 3)
42      self.assertEqual(b_init.count, 3)
43    def test_default_name(self):
44      mod = mlp.MLP([1])
45      self.assertEqual(mod.name, "mlp")
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    