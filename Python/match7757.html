<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_subtensor.py &amp; opt_2.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_subtensor.py &amp; opt_2.py
      </h3>
<h1 align="center">
        1.3%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_subtensor.py (4.185022%)<th>opt_2.py (0.8235804%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(212-218)<td><a href="#" name="0">(2510-2514)</a><td align="center"><font color="#ff0000">13</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1-24)<td><a href="#" name="1">(6-21)</a><td align="center"><font color="#ff0000">13</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(109-114)<td><a href="#" name="2">(95-97)</a><td align="center"><font color="#eb0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_subtensor.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 import numpy as np
2 import unittest
3 import theano
4 from theano import tensor
5 from theano.compile import DeepCopyOp
6 from theano.tensor.tests import test_subtensor, test_basic
7 from theano.tests import unittest_tools as utt
8 from ..basic_ops import HostFromGpu, GpuFromHost, GpuContiguous
9 from ..elemwise import GpuDimShuffle
10 from ..subtensor import (GpuIncSubtensor, GpuSubtensor,
11                          GpuAdvancedSubtensor1,
12                          GpuAdvancedSubtensor,
13                          GpuAdvancedBooleanSubtensor,
14                          GpuAdvancedIncSubtensor,
15                          GpuAdvancedIncSubtensor1,
16                          GpuAdvancedIncSubtensor1_dev20,
17                          GpuExtractDiag,
18                          GpuAllocDiag)
19 from ..type import gpuarray_shared_constructor
20 from</b></font> .config import mode_with_gpu, test_ctx_name
21 class G_subtensor(test_subtensor.T_subtensor):
22     def shortDescription(self):
23         return None
24     def __init__(self, name):
25         def shared(x, **kwargs):
26             return gpuarray_shared_constructor(x, target=test_ctx_name,
27                                                **kwargs)
28         test_subtensor.T_subtensor.__init__(
29             self, name,
30             shared=shared,
31             sub=GpuSubtensor,
32             inc_sub=GpuIncSubtensor,
33             adv_sub1=GpuAdvancedSubtensor1,
34             adv_incsub1=GpuAdvancedIncSubtensor1,
35             adv_sub=GpuAdvancedSubtensor,
36             adv_bool_sub=GpuAdvancedBooleanSubtensor,
37             dimshuffle=GpuDimShuffle,
38             mode=mode_with_gpu,
39             dtype='float32',
40             ignore_topo=(HostFromGpu, GpuFromHost,
41                          DeepCopyOp, GpuContiguous))
42         self.fast_compile = False
43         assert self.sub == GpuSubtensor
44 class G_subtensorF16(test_subtensor.T_subtensor):
45     def shortDescription(self):
46         return None
47     def __init__(self, name):
48         def shared(x, **kwargs):
49             return gpuarray_shared_constructor(x, target=test_ctx_name,
50                                                **kwargs)
51         test_subtensor.T_subtensor.__init__(
52             self, name,
53             shared=shared,
54             sub=GpuSubtensor,
55             inc_sub=GpuIncSubtensor,
56             adv_sub1=GpuAdvancedSubtensor1,
57             adv_incsub1=GpuAdvancedIncSubtensor1,
58             adv_sub=GpuAdvancedSubtensor,
59             adv_bool_sub=GpuAdvancedBooleanSubtensor,
60             dimshuffle=GpuDimShuffle,
61             mode=mode_with_gpu,
62             dtype='float16',  # use floatX?
63             ignore_topo=(HostFromGpu, GpuFromHost,
64                          DeepCopyOp, GpuContiguous))
65         self.fast_compile = False
66         assert self.sub == GpuSubtensor
67 def test_advinc_subtensor1():
68     for shp in [(3, 3), (3, 3, 3)]:
69         shared = gpuarray_shared_constructor
70         xval = np.arange(np.prod(shp), dtype='float32').reshape(shp) + 1
71         yval = np.empty((2,) + shp[1:], dtype='float32')
72         yval[:] = 10
73         x = shared(xval, name='x')
74         y = tensor.tensor(dtype='float32',
75                           broadcastable=(False,) * len(shp),
76                           name='y')
77         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
78         f = theano.function([y], expr, mode=mode_with_gpu)
79         assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1)
80                     for node in f.maker.fgraph.toposort()]) == 1
81         rval = f(yval)
82         rep = xval.copy()
83         np.add.at(rep, [0, 2], yval)
84         assert np.allclose(rval, rep)
85     shp = (3, 4)
86     for dtype1, dtype2 in [<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('float32', 'int8'), ('float32', 'float64'),
87                            ('uint64', 'int8'), ('int64', 'uint8'),
88                            ('float16', 'int8'), ('float16', 'float64'),
89                            ('float16', 'float16')]:
90         shared = gpuarray_shared_constructor
91         xval = np.arange(</b></font>np.prod(shp), dtype=dtype1).reshape(shp) + 1
92         yval = np.empty((2,) + shp[1:], dtype=dtype2)
93         yval[:] = 10
94         x = shared(xval, name='x')
95         y = tensor.tensor(dtype=yval.dtype,
96                           broadcastable=(False,) * len(yval.shape),
97                           name='y')
98         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
99         f = theano.function([y], expr, mode=mode_with_gpu)
100         assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1_dev20)
101                     for node in f.maker.fgraph.toposort()]) == 1
102         rval = f(yval)
103         rep = xval.copy()
104         np.add.at(rep, [[0, 2]], yval)
105         assert np.allclose(rval, rep)
106 @theano.change_flags(deterministic='more')
107 def test_deterministic_flag():
108     shp = (3, 4)
109     for dtype1, dtype2 in [('float32', 'int8')]:
110         shared = gpuarray_shared_constructor
111         xval = np.arange(np.prod(shp), dtype=dtype1).reshape(shp) + 1
112         yval = np.empty((2,) + shp[1:], dtype=dtype2)
113         yval[:] = 10
114         x = shared(xval, name='x')
115         y = tensor.tensor(dtype=yval.dtype,
116                           broadcastable=(False,) * len(yval.shape),
117                           name='y')
118         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
119         f = theano.function([y], expr, mode=mode_with_gpu)
120         assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1)
121                     for node in f.maker.fgraph.toposort()]) == 1
122         rval = f(yval)
123         rep = xval.copy()
124         np.add.at(rep, [[0, 2]], yval)
125         assert np.allclose(rval, rep)
126 def test_advinc_subtensor1_vector_scalar():
127     shp = (3,)
128     for dtype1, dtype2 in [('float32', 'int8'), ('float32', 'float64'),
129                            ('float16', 'int8'), ('float16', 'float64'),
130                            ('float16', 'float16'), ('int8', 'int8'),
131                            ('int16', 'int16')]:
132         shared = gpuarray_shared_constructor
133         xval = np.arange(np.prod(shp), dtype=dtype1).reshape(shp) + 1
134         yval = np.asarray(10, dtype=dtype2)
135         x = shared(xval, name='x')
136         y = tensor.tensor(dtype=yval.dtype,
137                           broadcastable=(False,) * len(yval.shape),
138                           name='y')
139         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
140         f = theano.function([y], expr, mode=mode_with_gpu)
141         assert sum([isinstance(node.op, (GpuAdvancedIncSubtensor1_dev20,
142                                          GpuAdvancedIncSubtensor1))
143                     for node in f.maker.fgraph.toposort()]) == 1
144         rval = f(yval)
145         rep = xval.copy()
146         rep[[0, 2]] += yval
147         assert np.allclose(rval, rep)
148 def test_incsub_f16():
149     shp = (3, 3)
150     shared = gpuarray_shared_constructor
151     xval = np.arange(np.prod(shp), dtype='float16').reshape(shp) + 1
152     yval = np.empty((2,) + shp[1:], dtype='float16')
153     yval[:] = 2
154     x = shared(xval, name='x')
155     y = tensor.tensor(dtype='float16',
156                       broadcastable=(False,) * len(shp),
157                       name='y')
158     expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
159     f = theano.function([y], expr, mode=mode_with_gpu)
160     assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1)
161                 for node in f.maker.fgraph.toposort()]) == 1
162     rval = f(yval)
163     rep = xval.copy()
164     np.add.at(rep, [[0, 2]], yval)
165     assert np.allclose(rval, rep)
166     expr = tensor.inc_subtensor(x[1:], y)
167     f = theano.function([y], expr, mode=mode_with_gpu)
168     assert sum([isinstance(node.op, GpuIncSubtensor)
169                 for node in f.maker.fgraph.toposort()]) == 1
170     rval = f(yval)
171     rep = xval.copy()
172     rep[1:] += yval
173     assert np.allclose(rval, rep)
174 def test_incsub_offset():
175     x = gpuarray_shared_constructor(np.zeros(5, dtype=theano.config<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.floatX))
176     x1 = x[1:]
177     y = tensor.vector()
178     z = tensor.inc_subtensor(x1[2:], y)
179     f = theano.function(</b></font>[y], z, updates={x: z}, mode=mode_with_gpu)
180     utt.assert_allclose(
181         f([1, 2]),
182         np.array([0, 0, 1, 2], dtype=theano.config.floatX))
183 class G_advancedsubtensor(test_subtensor.TestAdvancedSubtensor):
184     def shortDescription(self):
185         return None
186     def __init__(self, name):
187         test_subtensor.TestAdvancedSubtensor.__init__(
188             self, name,
189             shared=gpuarray_shared_constructor,
190             sub=GpuAdvancedSubtensor,
191             inc_sub=GpuAdvancedIncSubtensor,
192             mode=mode_with_gpu,
193             dtype='float32',  # floatX?
194             ignore_topo=(HostFromGpu, GpuFromHost,
195                          DeepCopyOp))
196         self.fast_compile = False
197         assert self.sub == GpuAdvancedSubtensor
198 class G_advancedsubtensorF16(test_subtensor.TestAdvancedSubtensor):
199     def shortDescription(self):
200         return None
201     def __init__(self, name):
202         test_subtensor.TestAdvancedSubtensor.__init__(
203             self, name,
204             shared=gpuarray_shared_constructor,
205             sub=GpuAdvancedSubtensor,
206             mode=mode_with_gpu,
207             dtype='float16',  # floatX?
208             ignore_topo=(HostFromGpu, GpuFromHost,
209                          DeepCopyOp))
210         self.fast_compile = False
211         assert self.sub == GpuAdvancedSubtensor
212 def test_adv_subtensor():
213     shp = (2, 3, 4)
214     shared = gpuarray_shared_constructor
215     xval = np.arange(np.prod(shp), dtype=theano.config.floatX).reshape(shp)
216     idx1, idx2 = tensor.ivectors('idx1', 'idx2')
217     idxs = [idx1, None, slice(0, 2, 1), idx2, None]
218     x = shared(xval, name='x')
219     expr = x[idxs]
220     f = theano.function([idx1, idx2], expr, mode=mode_with_gpu)
221     assert sum([isinstance(node.op, GpuAdvancedSubtensor)
222                for node in f.maker.fgraph.toposort()]) == 1
223     idx1_val = [0, 1]
224     idx2_val = [0, 1]
225     rval = f(idx1_val, idx2_val)
226     rep = xval[idx1_val, None, slice(0, 2, 1), idx2_val, None]
227     assert np.allclose(rval, rep)
228 class test_gpuextractdiag(unittest.TestCase):
229     def test_extractdiag_opt(self):
230         x = tensor.matrix()
231         fn = theano.function([x], tensor.ExtractDiag()(x), mode=mode_with_gpu)
232         assert any([isinstance(node.op, GpuExtractDiag)
233                     for node in fn.maker.fgraph.toposort()])
234     def test_matrix(self):
235         x = tensor.matrix()
236         np_x = np.arange(77).reshape(7, 11).astype(theano.config.floatX)
237         fn = theano.function([x], GpuExtractDiag()(x), mode=mode_with_gpu)
238         assert np.allclose(fn(np_x), np_x.diagonal())
239         fn = theano.function([x], GpuExtractDiag(2)(x), mode=mode_with_gpu)
240         assert np.allclose(fn(np_x), np_x.diagonal(2))
241         fn = theano.function([x], GpuExtractDiag(-3)(x), mode=mode_with_gpu)
242         assert np.allclose(fn(np_x), np_x.diagonal(-3))
243     def test_tensor(self):
244         x = tensor.tensor4()
245         np_x = np.arange(30107).reshape(7, 11, 17, 23).astype(theano.config.floatX)
246         for offset, axis1, axis2 in [
247                 (1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0),
248                 (-3, 1, 0), (-2, 2, 0), (3, 3, 0), (-1, 3, 2),
249                 (2, 2, 3), (-1, 2, 1), (1, 3, 1), (-1, 1, 3)]:
250             assert np.allclose(
251                 GpuExtractDiag(offset, axis1, axis2)(x).eval({x: np_x}),
252                 np_x.diagonal(offset, axis1, axis2))
253     def test_tensor_float16(self):
254         x = tensor.tensor4()
255         np_x = np.arange(30107).reshape(7, 11, 17, 23).astype('float16')
256         for offset, axis1, axis2 in [
257                 (1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0),
258                 (-3, 1, 0), (-2, 2, 0), (3, 3, 0), (-1, 3, 2),
259                 (2, 2, 3), (-1, 2, 1), (1, 3, 1), (-1, 1, 3)]:
260             assert np.allclose(
261                 GpuExtractDiag(offset, axis1, axis2)(x).eval({x: np_x}),
262                 np_x.diagonal(offset, axis1, axis2))
263 class TestGpuAllocDiag(test_basic.TestAllocDiag):
264     def __init__(self, name):
265         test_basic.TestAllocDiag.__init__(
266             self, name,
267             alloc_diag=GpuAllocDiag,
268             mode=mode_with_gpu
269         )
270 class test_gpuallocdiag(unittest.TestCase):
271     def test_allocdiag_opt(self):
272         x = tensor.vector()
273         fn = theano.function([x], tensor.AllocDiag()(x), mode=mode_with_gpu)
274         assert any([isinstance(node.op, GpuAllocDiag)
275                     for node in fn.maker.fgraph.toposort()])
276     def test_matrix(self):
277         x = tensor.vector()
278         np_x = np.arange(7).astype(theano.config.floatX)
279         fn = theano.function([x], GpuAllocDiag()(x), mode=mode_with_gpu)
280         assert np.allclose(fn(np_x), np.diag(np_x))
281         fn = theano.function([x], GpuAllocDiag(2)(x), mode=mode_with_gpu)
282         assert np.allclose(fn(np_x), np.diag(np_x, 2))
283         fn = theano.function([x], GpuAllocDiag(-3)(x), mode=mode_with_gpu)
284         assert np.allclose(fn(np_x), np.diag(np_x, -3))
285     def test_grad(self):
286         x = tensor.vector()
287         np_x = np.random.randn(7).astype(theano.config.floatX)
288         mtx_x = GpuAllocDiag()(x)
289         sum_mtx_x = tensor.sum(mtx_x)
290         grad_x = tensor.grad(sum_mtx_x, x)
291         grad_mtx_x = tensor.grad(sum_mtx_x, mtx_x)
292         fn_grad_x = theano.function([x], grad_x, mode=mode_with_gpu)
293         fn_grad_mtx_x = theano.function([x], grad_mtx_x, mode=mode_with_gpu)
294         computed_grad_x = fn_grad_x(np_x)
295         computed_grad_mtx_x = fn_grad_mtx_x(np_x)
296         true_grad_x = np.diagonal(computed_grad_mtx_x, 0)
297         assert np.allclose(computed_grad_x, true_grad_x)
298         mtx_x = GpuAllocDiag(2)(x)
299         sum_mtx_x = tensor.sum(mtx_x)
300         grad_x = tensor.grad(sum_mtx_x, x)
301         grad_mtx_x = tensor.grad(sum_mtx_x, mtx_x)
302         fn_grad_x = theano.function([x], grad_x, mode=mode_with_gpu)
303         fn_grad_mtx_x = theano.function([x], grad_mtx_x, mode=mode_with_gpu)
304         computed_grad_x = fn_grad_x(np_x)
305         computed_grad_mtx_x = fn_grad_mtx_x(np_x)
306         true_grad_x = np.diagonal(computed_grad_mtx_x, 2)
307         assert np.allclose(computed_grad_x, true_grad_x)
308         mtx_x = GpuAllocDiag(-3)(x)
309         sum_mtx_x = tensor.sum(mtx_x)
310         grad_x = tensor.grad(sum_mtx_x, x)
311         grad_mtx_x = tensor.grad(sum_mtx_x, mtx_x)
312         fn_grad_x = theano.function([x], grad_x, mode=mode_with_gpu)
313         fn_grad_mtx_x = theano.function([x], grad_mtx_x, mode=mode_with_gpu)
314         computed_grad_x = fn_grad_x(np_x)
315         computed_grad_mtx_x = fn_grad_mtx_x(np_x)
316         true_grad_x = np.diagonal(computed_grad_mtx_x, -3)
317         assert np.allclose(computed_grad_x, true_grad_x)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>opt_2.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
2 from collections import deque, defaultdict, OrderedDict
3 import contextlib
4 import copy
5 import inspect
6 import logging
7 import pdb
8 import sys
9 import time
10 import warnings
11 import traceback
12 import numpy as np
13 import</b></font> theano
14 from theano import config
15 from theano.compat import izip
16 from six import string_types, iteritems, itervalues, integer_types
17 from six.moves import reduce
18 from theano.gof import graph, op, utils, unify, toolbox
19 from theano.gof.fg import InconsistencyError
20 from theano.misc.ordered_set import OrderedSet
21 from . import destroyhandler as dh
22 _logger = logging.getLogger('theano.gof.opt')
23 _optimizer_idx = [0]
24 def _list_of_nodes(fgraph):
25     return list(graph.io_toposort(fgraph.inputs, fgraph.outputs))
26 class LocalMetaOptimizerSkipAssertionError(AssertionError):
27     pass
28 class Optimizer(object):
29     def __hash__(self):
30         if not hasattr(self, '_optimizer_idx'):
31             self._optimizer_idx = _optimizer_idx[0]
32             _optimizer_idx[0] += 1
33         return self._optimizer_idx
34     def __eq__(self, other):
35         return id(self) == id(other)
36     def __ne__(self, other):
37         return id(self) != id(other)
38     def apply(self, fgraph):
39         pass
40     def optimize(self, fgraph, *args, **kwargs):
41         """
42             orig = theano<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.tensor.basic.constant.enable
43             theano.tensor.basic.constant.enable = False
44             ret = self.apply(</b></font>fgraph, *args, **kwargs)
45         finally:
46             theano.tensor.basic.constant.enable = orig
47         return ret
48     def __call__(self, fgraph):
49         """
50         Same as self.optimize(fgraph).
51         """
52         return self.optimize(fgraph)
53     def add_requirements(self, fgraph):
54         """
55         Add features to the fgraph that are required to apply the optimization.
56         For example:
57           fgraph.attach_feature(History())
58           fgraph.attach_feature(MyFeature())
59           etc.
60         """
61         pass
62     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
63         name = getattr(self, 'name', None)
64         print("%s%s %s id=%i" % (
65             (' ' * level), self.__class__.__name__, name, id(self)), file=stream)
66     @staticmethod
67     def print_profile(stream, prof, level=0):
68         if prof is not None:
69             raise NotImplementedError(
70                 "The function print_profile must be overrided if the"
71                 " optimizer return profiling information.")
72 class FromFunctionOptimizer(Optimizer):
73     """
74     WRITEME
75     """
76     def __init__(self, fn, requirements=()):
77         self.apply = fn
78         self.requirements = requirements
79     def add_requirements(self, fgraph):
80         for req in self.requirements:
81             req(fgraph)
82     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
83         print("%s%s id=%i" % (
84             ' ' * level,
85             str(self.apply),
86             id(self)), file=stream)
87     def __call__(self, *args, **kwargs):
88         return self.fn(*args, **kwargs)
89     def __str__(self):
90         return self.__name__
91 def optimizer(f):
92     """
93     Decorator for FromFunctionOptimizer.
94     """
95     rval = FromFunctionOptimizer(f)
96     rval.__name__ = f.__name__
97     return rval
98 def inplace_optimizer(f):
99     """
100     Decorator for FromFunctionOptimizer.
101     """
102     dh_handler = dh.DestroyHandler
103     requirements = (lambda fgraph:
104                     fgraph.attach_feature(dh_handler()),)
105     rval = FromFunctionOptimizer(f, requirements)
106     rval.__name__ = f.__name__
107     return rval
108 class SeqOptimizer(Optimizer, list):
109     """
110     Takes a list of L{Optimizer} instances and applies them
111     sequentially.
112     """
113     @staticmethod
114     def warn(exc, self, optimizer):
115         """
116         Default failure_callback for SeqOptimizer.
117         """
118         _logger.error("SeqOptimizer apply %s" % str(optimizer))
119         _logger.error("Traceback:")
120         _logger.error(traceback.format_exc())
121         if config.on_opt_error == 'raise':
122             raise exc
123         elif config.on_opt_error == 'pdb':
124             pdb.post_mortem(sys.exc_info()[2])
125     def __init__(self, *opts, **kw):
126         """
127         Parameters
128         ----------
129         *opts :
130             The List of optimizers to be applied to a node
131         failure_callback : callable or None
132             Keyword only argument. A callback used when a failure
133             happen during optimization.
134         """
135         if len(opts) == 1 and isinstance(opts[0], (list, tuple)):
136             opts = opts[0]
137         self[:] = opts
138         self.failure_callback = kw.pop('failure_callback', None)
139         assert len(kw) == 0
140     def apply(self, fgraph):
141         """
142         Applies each L{Optimizer} in self in turn.
143         """
144         l = []
145         if fgraph.profile:
146             validate_before = fgraph.profile.validate_time
147             sub_validate_time = [validate_before]
148             callbacks_before = fgraph.execute_callbacks_times.copy()
149         else:
150             sub_validate_time = []
151             callbacks_before = []
152         callback_before = fgraph.execute_callbacks_time
153         nb_node_before = len(fgraph.apply_nodes)
154         sub_profs = []
155         nb_nodes = []
156         self.pre_profile = (
157             self, l, -1, -1, nb_node_before,
158             -1, sub_profs, sub_validate_time,
159             nb_nodes, {})
160         try:
161             for optimizer in self:
162                 try:
163                     nb_nodes_before = len(fgraph.apply_nodes)
164                     t0 = time.time()
165                     sub_prof = optimizer.optimize(fgraph)
166                     l.append(float(time.time() - t0))
167                     sub_profs.append(sub_prof)
168                     nb_nodes.append((nb_nodes_before,
169                                      len(fgraph.apply_nodes)))
170                     if fgraph.profile:
171                         sub_validate_time.append(fgraph.profile.validate_time)
172                 except AssertionError:
173                     raise
174                 except Exception as e:
175                     if self.failure_callback:
176                         self.failure_callback(e, self, optimizer)
177                         continue
178                     else:
179                         raise
180         finally:
181             if fgraph.profile:
182                 validate_time = fgraph.profile.validate_time - validate_before
183                 callbacks_time = {}
184                 for k, v in iteritems(fgraph.execute_callbacks_times):
185                     if k in callbacks_before:
186                         t = v - callbacks_before[k]
187                         if t &gt; 0:
188                             callbacks_time[k] = t
189                     else:
190                         callbacks_time[k] = v
191             else:
192                 validate_time = None
193                 callbacks_time = {}
194             callback_time = fgraph.execute_callbacks_time - callback_before
195             self.pre_profile = (
196                 self, l, validate_time, callback_time, nb_node_before,
197                 len(fgraph.apply_nodes), sub_profs, sub_validate_time,
198                 nb_nodes, callbacks_time)
199         return self.pre_profile
200     def __str__(self):
201         return "SeqOpt(%s)" % list.__str__(self)
202     def __repr__(self):
203         return list.__repr__(self)
204     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
205         name = getattr(self, 'name', None)
206         print("%s%s %s id=%i" % (
207             (' ' * level), self.__class__.__name__, name, id(self)), file=stream)
208         if depth != 0:
209             depth -= 1
210             for opt in self:
211                 opt.print_summary(stream, level=(level + 2), depth=depth)
212     @staticmethod
213     def print_profile(stream, prof, level=0):
214         (opts, prof, validate_time, callback_time,
215          nb_node_before, nb_node_after, sub_profs, sub_validate_time,
216          nb_nodes, callbacks_time) = prof
217         blanc = ('    ' * level)
218         print(blanc, "SeqOptimizer", end=' ', file=stream)
219         if hasattr(opts, "name"):
220             print(blanc, opts.name, end=' ', file=stream)
221         elif hasattr(opts, "__name__"):
222             print(blanc, opts.__name__, end=' ', file=stream)
223         print((" time %.3fs for %d/%d nodes"
224                " before/after optimization" % (
225                    sum(prof), nb_node_before, nb_node_after)), file=stream)
226         print(blanc, "  %.3fs for callback" % (callback_time), file=stream)
227         print(blanc, "      %.3fs for fgraph.validate()" % (validate_time),
228               file=stream)
229         if callback_time &gt; 1:
230             print(blanc, "  callbacks_time", file=stream)
231             for i in sorted(iteritems(callbacks_time), key=lambda a: -a[1]):
232                 if i[1] &gt; 0:
233                     print(blanc, "      ", i[0], ',', i[1], file=stream)
234         if level == 0:
235             print(blanc,
236                   "  time      - (name, class, index, nodes before, nodes after) - validate time",
237                   file=stream)
238         ll = []
239         for (opt, nb_n) in zip(opts, nb_nodes):
240             if hasattr(opt, "__name__"):
241                 name = opt.__name__
242             else:
243                 name = opt.name
244             idx = opts.index(opt)
245             ll.append((name, opt.__class__.__name__,
246                        idx) + nb_n)
247         lll = sorted(zip(prof, ll), key=lambda a: a[0])
248         for (t, opt) in lll[::-1]:
249             i = opt[2]
250             if sub_validate_time:
251                 val_time = sub_validate_time[i + 1] - sub_validate_time[i]
252                 print(blanc, '  %.6fs - %s - %.3fs' % (
253                     t, opt, val_time), file=stream)
254             else:
255                 print(blanc, '  %.6fs - %s' % (t, opt), file=stream)
256             if sub_profs[i]:
257                 opts[i].print_profile(stream, sub_profs[i],
258                                       level=level + 1)
259         print(file=stream)
260     @staticmethod
261     def merge_profile(prof1, prof2):
262         """
263         Merge 2 profiles returned by this cass apply() fct.
264         """
265         new_t = []  # the time for the optimization
266         new_l = []  # the optimization
267         new_sub_profile = []
268         for l in set(prof1[0]).intersection(set(prof2[0])):
269             idx1 = prof1[0].index(l)
270             idx2 = prof2[0].index(l)
271             new_t.append(prof1[1][idx1] +
272                          prof2[1][idx2])
273             new_l.append(l)
274             if hasattr(l, 'merge_profile'):
275                 assert len(prof1[6][idx1]) == len(prof2[6][idx2])
276                 new_sub_profile.append(l.merge_profile(prof1[6][idx1],
277                                                        prof2[6][idx2]))
278             else:
279                 new_sub_profile.append(None)
280         from six import StringIO
281         for l in set(prof1[0]).symmetric_difference(set(prof2[0])):
282             new_l_names = [o.name for o in new_l]
283             if l.name in new_l_names:
284                 idx = new_l_names.index(l.name)
285                 io1 = StringIO()
286                 io2 = StringIO()
287                 l.print_summary(io1)
288                 new_l[idx].print_summary(io2)
289                 if io1.read() == io2.read():
290                     if l in prof1[0]:
291                         p = prof1
292                     else:
293                         p = prof2
294                     new_t[idx] += p[1][p[0].index(l)]
295                     if hasattr(l, 'merge_profile'):
296                         assert len(p[6][p[0].index(l)]) == \
297                             len(new_sub_profile[idx])
298                         new_sub_profile[idx] = l.merge_profile(
299                             new_sub_profile[idx], p[6][p[0].index(l)])
300                     else:
301                         new_sub_profile[idx] = None
302                 continue
303             if l in prof1[0]:
304                 p = prof1
305             else:
306                 p = prof2
307             new_t.append(p[1][p[0].index(l)])
308             idx = p[0].index(l)
309             new_l.append(l)
310             new_sub_profile.append(p[6][idx])
311         new_opt = SeqOptimizer(*new_l)
312         new_nb_nodes = []
313         for p1, p2 in zip(prof1[8], prof2[8]):
314             new_nb_nodes.append((p1[0] + p2[0], p1[1] + p2[1]))
315         new_nb_nodes.extend(prof1[8][len(new_nb_nodes):])
316         new_nb_nodes.extend(prof2[8][len(new_nb_nodes):])
317         new_callbacks_times = merge_dict(prof1[9], prof2[9])
318         assert set([l.name for l in prof1[0]]).issubset(
319             set([l.name for l in new_l]))
320         assert set([l.name for l in prof2[0]]).issubset(
321             set([l.name for l in new_l]))
322         assert len(new_t) == len(new_opt) == len(new_sub_profile)
323         return (new_opt, new_t, prof1[2] + prof2[2],
324                 prof1[3] + prof2[3],
325                 -1, -1, new_sub_profile, [],
326                 new_nb_nodes,
327                 new_callbacks_times)
328 class _metadict:
329     """
330     WRITEME
331     """
332     def __init__(self):
333         self.d = {}
334         self.l = []
335     def __getitem__(self, item):
336         return self.get(item, None)
337     def __setitem__(self, item, value):
338         try:
339             self.d[item] = value
340         except Exception:
341             for i, (key, val) in enumerate(self.l):
342                 if key == item:
343                     self.l[i] = (item, value)
344                     return
345             self.l.append((item, value))
346     def __delitem__(self, item):
347         try:
348             if item in self.d:
349                 del self.d[item]
350                 return
351         except TypeError as e:
352             assert "unhashable type" in str(e)
353         for i, (key, val) in enumerate(self.l):
354             if key == item:
355                 del self.l[i]
356                 return
357             raise KeyError(item)
358     def discard(self, item):
359         try:
360             if item in self.d:
361                 del self.d[item]
362                 return
363         except TypeError as e:
364             assert "unhashable type" in str(e)
365         for i, (key, val) in enumerate(self.l):
366             if key == item:
367                 del self.l[i]
368                 return
369     def get(self, item, default):
370         try:
371             return self.d[item]
372         except Exception:
373             for item2, value in self.l:
374                 try:
375                     if item == item2:
376                         return value
377                     if item.equals(item2):
378                         return value
379                 except Exception:
380                     if item is item2:
381                         return value
382             return default
383     def clear(self):
384         self.d = {}
385         self.l = []
386     def __str__(self):
387         return "(%s, %s)" % (self.d, self.l)
388 class MergeFeature(object):
389     """
390     Keeps track of variables in fgraph that cannot be merged together.
391     That way, the MergeOptimizer can remember the result of the last merge
392     pass on the fgraph.
393     """
394     def on_attach(self, fgraph):
395         assert not hasattr(fgraph, 'merge_feature')
396         fgraph.merge_feature = self
397         self.seen_constants = set()
398         self.const_sig = _metadict()
399         self.const_sig_inv = _metadict()
400         self.nodes_seen = set()
401         self.noinput_nodes = OrderedSet()
402         self.scheduled = []
403         self.blacklist = []
404         for node in fgraph.toposort():
405             self.on_import(fgraph, node, "on_attach")
406     def on_change_input(self, fgraph, node, i, r, new_r, reason):
407         if node in self.nodes_seen:
408             self.nodes_seen.discard(node)
409             self.process_node(fgraph, node)
410         if not isinstance(node, string_types):
411             assert node.inputs
412         if isinstance(new_r, graph.Constant):
413             self.process_constant(fgraph, new_r)
414     def on_import(self, fgraph, node, reason):
415         for c in node.inputs:
416             if isinstance(c, graph.Constant):
417                 self.process_constant(fgraph, c)
418         self.process_node(fgraph, node)
419     def on_prune(self, fgraph, node, reason):
420         self.nodes_seen.discard(node)
421         if not node.inputs:
422             self.noinput_nodes.discard(node)
423         for c in node.inputs:
424             if isinstance(c, graph.Constant) and (len(c.clients) &lt;= 1):
425                 sig = self.const_sig[c]
426                 self.const_sig.discard(c)
427                 self.const_sig_inv.discard(sig)
428                 self.seen_constants.discard(id(c))
429     def process_constant(self, fgraph, c):
430         """
431         Check if a constant can be merged, and queue that replacement.
432         """
433         if id(c) in self.seen_constants:
434             return
435         sig = c.merge_signature()
436         other_c = self.const_sig_inv.get(sig, None)
437         if other_c is not None:
438             if c.name:
439                 other_c.name = c.name
440             self.scheduled.append([[(c, other_c, 'merge')]])
441         else:
442             self.const_sig[c] = sig
443             self.const_sig_inv[sig] = c
444             self.seen_constants.add(id(c))
445     def process_node(self, fgraph, node):
446         """
447         Check if a node can be merged, and queue that replacement.
448         """
449         if node in self.nodes_seen:
450             return
451         node_has_assert = False
452         if node.inputs:
453             if len(node.inputs[0].clients) &lt; len(node.inputs[-1].clients):
454                 clients = node.inputs[0].clients
455             else:
456                 clients = node.inputs[-1].clients
457             assert len(clients) &gt; 0
458             merge_candidates = [c for c, i in clients if c in self.nodes_seen]
459             for i in []:  # node.inputs:
460                 if i.owner and isinstance(i.owner.op,
461                                           theano.tensor.opt.Assert):
462                     node_has_assert = True
463                     assert_clients = [c for (c, _) in i.owner.inputs[0].clients
464                                       if c in self.nodes_seen]
465                     for idx in range(len(assert_clients)):
466                         client = assert_clients[idx]
467                         if isinstance(i.owner.op, theano.tensor.opt.Assert):
468                             for c in client.outputs[0].clients:
469                                 if c[0] in self.nodes_seen:
470                                     assert_clients.append(c[0])
471                     merge_candidates.extend(assert_clients)
472         else:
473             merge_candidates = self.noinput_nodes
474         replacement_candidates = []
475         for candidate in merge_candidates:
476             if candidate is node:
477                 continue
478             if len(node.inputs) != len(candidate.inputs):
479                 continue
480             cand_has_assert = False
481             cand_inputs_assert_removed = []
482             for i in []:  # candidate.inputs:
483                 if i.owner and isinstance(i.owner.op,
484                                           theano.tensor.opt.Assert):
485                     cand_has_assert = True
486                     cand_inputs_assert_removed.append(i.owner.inputs[0])
487                 else:
488                     cand_inputs_assert_removed.append(i)
489             cand_inputs_assert_removed = candidate.inputs
490             if node_has_assert:
491                 node_inputs_assert_removed = []
492                 for i in node.inputs:
493                     if i.owner and isinstance(i.owner.op,
494                                               theano.tensor.opt.Assert):
495                         node_inputs_assert_removed.append(i.owner.inputs[0])
496                     else:
497                         node_inputs_assert_removed.append(i)
498             else:
499                 node_inputs_assert_removed = node.inputs
500             inputs_match = all(node_in is cand_in
501                                for node_in, cand_in
502                                in zip(node_inputs_assert_removed,
503                                       cand_inputs_assert_removed))
504             if inputs_match and node.op == candidate.op:
505                 if (node, candidate) in self.blacklist:
506                     continue
507                 if not (node_has_assert or cand_has_assert):
508                     pairs = list(zip(node.outputs,
509                                      candidate.outputs,
510                                      ['merge'] * len(node.outputs)))
511                 elif node_has_assert and not cand_has_assert:
512                     pairs = list(zip(candidate.outputs,
513                                      node.outputs,
514                                      ['merge'] * len(node.outputs)))
515                 else:
516                     new_inputs = self.get_merged_assert_input(node, candidate)
517                     new_node = node.op(*new_inputs)
518                     pairs = list(zip(node.outputs,
519                                      new_node.owner.outputs,
520                                      ['new_node'] * len(node.outputs))) +\
521                         list(zip(candidate.outputs,
522                                  new_node.owner.outputs,
523                                  ['new_node'] * len(node.outputs)))
524                 for pair in pairs:
525                     node_output, cand_output = pair[:2]
526                     if node_output.name:
527                         cand_output.name = node_output.name
528                 replacement_candidates.append(pairs)
529         if replacement_candidates:
530             self.scheduled.append(replacement_candidates)
531         else:
532             self.nodes_seen.add(node)
533             if not node.inputs:
534                 self.noinput_nodes.add(node)
535     def get_merged_assert_input(self, node, candidate):
536         new_inputs = []
537         for node_i, cand_i in zip(node.inputs, candidate.inputs):
538             if (node_i.owner and
539                     isinstance(node_i.owner.op,
540                                theano.tensor.opt.Assert)):
541                 if (cand_i.owner and
542                         isinstance(cand_i.owner.op,
543                                    theano.tensor.opt.Assert)):
544                     node_cond = node_i.owner.inputs[1:]
545                     cand_cond = cand_i.owner.inputs[1:]
546                     new_cond = list(set(node_cond + cand_cond))
547                     new_inputs.append(
548                         theano.tensor.opt.assert_op(
549                             node_i.owner.inputs[0],
550                             *new_cond))
551                 else:
552                     new_inputs.append(node_i)
553             else:
554                 new_inputs.append(cand_i)
555         return new_inputs
556 class MergeOptimizer(Optimizer):
557     """
558     Merges parts of the graph that are identical and redundant.
559     The basic principle is that if two Applies have ops that compare equal, and
560     identical inputs, then they do not both need to be computed. The clients of
561     one are transferred to the other and one of them is removed from the graph.
562     This procedure is carried out in input-&gt;output order through the graph.
563     The first step of merging is constant-merging, so that all clients of an
564     int(1) for example, are transferred to a particular instance of int(1).
565     """
566     def add_requirements(self, fgraph):
567         if not hasattr(fgraph, 'merge_feature'):
568             fgraph.attach_feature(MergeFeature())
569     def apply(self, fgraph):
570         sched = fgraph.merge_feature.scheduled
571         nb_fail = 0
572         t0 = time.time()
573         if fgraph.profile:
574             validate_before = fgraph.profile.validate_time
575             callback_before = fgraph.execute_callbacks_time
576             callbacks_before = fgraph.execute_callbacks_times.copy()
577         nb_merged = 0
578         nb_constant = 0
579         while sched:
580             pairs_list = sched.pop()
581             success = True
582             for pairs_ in pairs_list:
583                 var, candidate, merge_mode = pairs_[0]
584                 if merge_mode == "new_node" and hasattr(var, 'fgraph'):
585                     pass
586                 elif (not hasattr(var, 'fgraph') or
587                       not hasattr(candidate, 'fgraph')):
588                     continue
589                 pairs = [pair[:2] for pair in pairs_]
590                 if var.owner and candidate.owner:
591                     node = var.owner
592                     candidate = candidate.owner
593                     cand_inputs_assert_removed = []
594                     for i in candidate.inputs:
595                         if i.owner and isinstance(i.owner.op,
596                                                   theano.tensor.opt.Assert):
597                             cand_inputs_assert_removed.append(
598                                 i.owner.inputs[0])
599                         else:
600                             cand_inputs_assert_removed.append(i)
601                     node_inputs_assert_removed = []
602                     for i in node.inputs:
603                         if i.owner and isinstance(i.owner.op,
604                                                   theano.tensor.opt.Assert):
605                             node_inputs_assert_removed.append(
606                                 i.owner.inputs[0])
607                         else:
608                             node_inputs_assert_removed.append(i)
609                     if merge_mode == "new_node":
610                         inputs_match = True
611                     else:
612                         inputs_match = all(node_in is cand_in
613                                            for node_in, cand_in in
614                                            zip(node_inputs_assert_removed,
615                                                cand_inputs_assert_removed))
616                     if not inputs_match:
617                         continue
618                     if hasattr(pairs[0][0].fgraph, 'destroy_handler'):
619                         clients = pairs[0][0].clients + pairs[0][1].clients
620                         if sum([i in utils.flatten(c.op.destroy_map.values())
621                                 for c, i in clients
622                                 if c != 'output' and
623                                 hasattr(c.op, 'destroy_map')]) &gt; 1:
624                             continue
625                 if len(pairs) == 1 and pairs[0][0].type != pairs[0][1].type:
626                     res = pairs[0][0].type.convert_variable(pairs[0][1])
627                     if not res:
628                         pairs = [(pairs[0][1], pairs[0][0])]
629                 try:
630                     if all([isinstance(old, graph.Constant) for old, new in pairs]):
631                         fgraph.replace_all(pairs, 'MergeOptimizer')
632                     else:
633                         fgraph.replace_all_validate(pairs, 'MergeOptimizer')
634                 except InconsistencyError:
635                     success = False
636                     nb_fail += 1
637                     fgraph.merge_feature.blacklist.append(
638                         (pairs[0][0].owner, pairs[0][1].owner))
639                 if success:
640                     nb_merged += len(pairs)
641                     if isinstance(pairs[0][0], graph.Constant):
642                         nb_constant += 1
643                     break
644         if fgraph.profile:
645             validate_time = fgraph.profile.validate_time - validate_before
646             callback_time = fgraph.execute_callbacks_time - callback_before
647             callbacks_time = {}
648             for k, v in iteritems(fgraph.execute_callbacks_times):
649                 if k in callbacks_before:
650                     t = v - callbacks_before[k]
651                     if t &gt; 0:
652                         callbacks_time[k] = t
653                 else:
654                     callbacks_time[k] = v
655         else:
656             validate_time = None
657             callback_time = None
658             callbacks_time = {}
659         fgraph.merge_feature.blacklist = []
660         return (nb_fail, time.time() - t0, validate_time,
661                 callback_time, callbacks_time, nb_merged, nb_constant)
662     def __str__(self):
663         return self.__class__.__name__
664     @staticmethod
665     def print_profile(stream, prof, level=0):
666         (nb_fail, replace_time, validate_time,
667          callback_time, callbacks_time, nb_merged, nb_constant) = prof
668         blanc = ('    ' * level)
669         print(blanc, "MergeOptimizer", file=stream)
670         print(blanc, "  nb fail=%5d merged=%5d constant=%5d" % (
671               nb_fail, nb_merged, nb_constant), file=stream)
672         print(blanc, "  time replace=%2.2f validate=%2.2f callback=%2.2f" % (
673               replace_time, validate_time, callback_time), file=stream)
674         if callback_time &gt; 1:
675             print(blanc, "  callbacks_time", file=stream)
676             for i in sorted(iteritems(callbacks_time), key=lambda a: a[1]):
677                 if i[1] &gt; 0:
678                     print(blanc, "      ", i[0], ',', i[1], file=stream)
679     @staticmethod
680     def merge_profile(prof1, prof2):
681         def merge_none_number(v1, v2):
682             if v1 is None:
683                 return v2
684             if v2 is None:
685                 return v1
686             return v1 + v2
687         nb_fail = prof1[0] + prof2[0]
688         replace_time = prof1[1] + prof2[1]
689         validate_time = merge_none_number(prof1[2], prof2[2])
690         callback_time = merge_none_number(prof1[3], prof2[3])
691         callbacks_time = merge_dict(prof1[4], prof2[4])
692         nb_merged = prof1[5] + prof2[5]
693         nb_constant = prof1[6] + prof2[6]
694         return (nb_fail, replace_time, validate_time,
695                 callback_time, callbacks_time, nb_merged, nb_constant)
696 def is_same_graph_with_merge(var1, var2, givens=None):
697     """
698     Merge-based implementation of `theano.gof.graph.is_same_graph`.
699     See help on `theano.gof.graph.is_same_graph` for additional documentation.
700     """
701     if givens is None:
702         givens = {}
703     copied = copy.deepcopy([var1, var2, givens])
704     vars = copied[0:2]
705     givens = copied[2]
706     inputs = theano.gof.graph.inputs(vars)
707     fgraph = theano.gof.fg.FunctionGraph(inputs, vars, clone=False)
708     for to_replace, replace_by in iteritems(givens):
709         fgraph.replace(to_replace, replace_by)
710     MergeOptimizer().optimize(fgraph)
711     vars_replaced = [givens.get(v, v) for v in vars]
712     o1, o2 = [v.owner for v in vars_replaced]
713     if o1 is None and o2 is None:
714         return vars_replaced[0] == vars_replaced[1]
715     else:
716         return o1 is o2
717 def pre_constant_merge(vars):
718     """
719     Merge constants in the subgraph used to compute nodes in `vars`.
720     `vars` is a list of nodes, and we want to merge together nodes
721     that are constant inputs used to compute nodes in that list.
722     Notes
723     -----
724     This function will ignore nodes that are in an fgraph.
725     It is used to pre-merge nodes generated inside an optimization,
726     before it is inserted in the fgraph.
727     It is useful if there are many such replacements to make,
728     so that DebugMode will not check each of them.
729     """
730     seen_var = set()
731     const_sig_inv = {}
732     if isinstance(vars, graph.Variable):
733         vars = [vars]
734     def recursive_merge(var):
735         if var in seen_var:
736             return var
737         if not hasattr(var, 'owner'):
738             return var
739         if var.owner and hasattr(var.owner, "fgraph"):
740             return var
741         seen_var.add(var)
742         if isinstance(var, graph.Constant):
743             sig = var.signature()
744             try:
745                 if sig in const_sig_inv:
746                     return const_sig_inv[sig]
747                 const_sig_inv[sig] = var
748             except TypeError:  # unhashable type
749                 warnings.warn(
750                     "We work around a problem, the following variable"
751                     " signature isn't hashable. Please, report this to"
752                     " theano-dev so that the better fix is done. %s" % var)
753                 pass
754             return var
755         if var.owner:
756             for idx, inp in enumerate(var.owner.inputs):
757                 var.owner.inputs[idx] = recursive_merge(inp)
758         return var
759     return list(map(recursive_merge, vars))
760 class LocalOptimizer(object):
761     """
762     A class for node-based optimizations.
763     Instances should implement the transform function,
764     and be passed to configure a fgraph-based Optimizer instance.
765     """
766     def __hash__(self):
767         if not hasattr(self, '_optimizer_idx'):
768             self._optimizer_idx = _optimizer_idx[0]
769             _optimizer_idx[0] += 1
770         return self._optimizer_idx
771     def tracks(self):
772         """
773         Return the list of op classes that this opt applies to.
774         Return None to apply to all nodes.
775         """
776         return None
777     def transform(self, node):
778         """
779         Transform a subgraph whose output is `node`.
780         Subclasses should implement this function so that it returns one of two
781         kinds of things:
782         - False to indicate that no optimization can be applied to this `node`;
783           or
784         - &lt;list of variables&gt; to use in place of `node`'s outputs in the
785           greater graph.
786         - dict(old variables -&gt; new variables). A dictionary that map
787           from old variables to new variables to replace.
788         Parameters
789         ----------
790         node : an Apply instance
791         """
792         raise utils.MethodNotDefined("transform",
793                                      type(self), self.__class__.__name__)
794     def add_requirements(self, fgraph):
795         """
796         If this local optimization wants to add some requirements to the
797         fgraph, this is the place to do it.
798         """
799         pass
800     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
801         print("%s%s id=%i" % (
802             (' ' * level), self.__class__.__name__, id(self)), file=stream)
803 class LocalMetaOptimizer(LocalOptimizer):
804     """
805     Base class for meta-optimizers that try a set of LocalOptimizers
806     to replace a node and choose the one that executes the fastest.
807     If the error LocalMetaOptimizerSkipAssertionError is raised during
808     compilation, we will skip that function compilation and not print
809     the error.
810     """
811     def __init__(self):
812         self.verbose = config.metaopt.verbose
813         self.track_dict = defaultdict(lambda: [])
814         self.tag_dict = defaultdict(lambda: [])
815         self._tracks = []
816         self.optimizers = []
817     def register(self, optimizer, tag_list):
818         self.optimizers.append(optimizer)
819         for c in optimizer.tracks():
820             self.track_dict[c].append(optimizer)
821             self._tracks.append(c)
822         for tag in tag_list:
823             self.tag_dict[tag].append(optimizer)
824     def tracks(self):
825         return self._tracks
826     def transform(self, node):
827         if self._tracks is not None:
828             if not isinstance(node.op, tuple(self._tracks)):
829                 return
830         givens = {}
831         missing = set()
832         for input in node.inputs:
833             if isinstance(input, theano.compile.SharedVariable):
834                 pass
835             elif hasattr(input.tag, 'test_value'):
836                 givens[input] = theano.shared(
837                     input.type.filter(input.tag.test_value),
838                     input.name,
839                     broadcastable=input.broadcastable,
840                     borrow=True)
841             else:
842                 missing.add(input)
843         if missing:
844             givens.update(self.provide_inputs(node, missing))
845             missing.difference_update(givens.keys())
846         if missing:
847             if self.verbose &gt; 0:
848                 print(("%s cannot meta-optimize %s, "
849                        "%d of %d input shapes unknown" %
850                        (self.__class__.__name__, node, len(missing), node.nin)))
851             return
852         if self.verbose &gt; 1:
853             print(("%s meta-optimizing %s (%d choices):" %
854                    (self.__class__.__name__, node, len(self.get_opts(node)))))
855         timings = []
856         for opt in self.get_opts(node):
857             outputs = opt.transform(node)
858             if outputs:
859                 try:
860                     fn = theano.function([], outputs, givens=givens,
861                                          on_unused_input='ignore')
862                     fn.trust_input = True
863                     timing = min(self.time_call(fn) for _ in range(2))
864                 except LocalMetaOptimizerSkipAssertionError:
865                     continue
866                 except Exception as e:
867                     if self.verbose &gt; 0:
868                         print("* %s: exception" % opt, e)
869                     continue
870                 else:
871                     if self.verbose &gt; 1:
872                         print("* %s: %.5g sec" % (opt, timing))
873                     timings.append((timing, outputs, opt))
874             else:
875                 if self.verbose &gt; 0:
876                     print("* %s: not applicable" % opt)
877         if timings:
878             timings.sort()
879             if self.verbose &gt; 1:
880                 print("= %s" % timings[0][2])
881             return timings[0][1]
882         return
883     def provide_inputs(self, node, inputs):
884         """
885         If implemented, returns a dictionary mapping all symbolic variables
886         in ``inputs`` to SharedVariable instances of suitable dummy values.
887         The ``node`` can be inspected to infer required input shapes.
888         """
889         raise NotImplementedError()
890     def get_opts(self, node):
891         """
892         Can be overrided to change the way opts are selected
893         """
894         return self.track_dict[type(node.op)]
895     def time_call(self, fn):
896         start = time.time()
897         fn()
898         return time.time() - start
899 class FromFunctionLocalOptimizer(LocalOptimizer):
900     """
901     WRITEME
902     """
903     def __init__(self, fn, tracks=None, requirements=()):
904         self.transform = fn
905         self._tracks = tracks
906         self.requirements = requirements
907     def add_requirements(self, fgraph):
908         for req in self.requirements:
909             req(fgraph)
910     def tracks(self):
911         return self._tracks
912     def __str__(self):
913         return getattr(self, '__name__',
914                        '&lt;FromFunctionLocalOptimizer instance&gt;')
915     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
916         print("%s%s id=%i" % (
917             ' ' * level,
918             str(self.transform),
919             id(self)), file=stream)
920 def local_optimizer(tracks, inplace=False, requirements=()):
921     def decorator(f):
922         """
923         WRITEME
924         """
925         if tracks is not None:
926             if len(tracks) == 0:
927                 raise ValueError("Use None instead of an empty list to apply to all nodes.", f.__module__, f.__name__)
928             for t in tracks:
929                 if not (isinstance(t, op.Op) or issubclass(t, op.PureOp)):
930                     raise ValueError("Tracks are op classes or instances", f.__module__, f.__name__)
931         req = requirements
932         if inplace:
933             dh_handler = dh.DestroyHandler
934             req = tuple(requirements) + (
935                 lambda fgraph:
936                 fgraph.attach_feature(dh_handler()),)
937         rval = FromFunctionLocalOptimizer(f, tracks, req)
938         rval.__name__ = f.__name__
939         return rval
940     return decorator
941 class LocalOptGroup(LocalOptimizer):
942     """Takes a list of LocalOptimizer and applies them to the node.
943     Parameters
944     ----------
945     optimizers :
946         The List of optimizers to be applied to a node
947     reentrant : bool (Default True)
948         Keyword only argument. Reentrant information. Some global
949         optimizer like NavigatorOptimizer can use this value to
950         determine if it ignore new nodes during a pass on the
951         nodes. Sometimes, ignore_newtrees is not reentrant.
952     apply_all_opts : bool (Default False)
953         If False, it will return after the new node after the first optimizer
954         applied. Otherwise, it will start again with the new node until no new
955         optimization apply.
956     """
957     def __init__(self, *optimizers, **kwargs):
958         if len(optimizers) == 1 and isinstance(optimizers[0], list):
959             optimizers = tuple(optimizers[0])
960         self.opts = optimizers
961         assert isinstance(self.opts, tuple)
962         self.reentrant = any(getattr(opt, 'reentrant', True)
963                              for opt in optimizers)
964         self.retains_inputs = all(getattr(opt, 'retains_inputs', False)
965                                   for opt in optimizers)
966         self.apply_all_opts = kwargs.pop('apply_all_opts', False)
967         self.profile = kwargs.pop('profile', False)
968         self.track_map = defaultdict(lambda: [])
969         assert len(kwargs) == 0
970         if self.profile:
971             self.time_opts = {}
972             self.process_count = {}
973             self.applied_true = {}
974             self.node_created = {}
975         for o in self.opts:
976             if self.profile:
977                 self.time_opts.setdefault(o, 0)
978                 self.process_count.setdefault(o, 0)
979                 self.applied_true.setdefault(o, 0)
980                 self.node_created.setdefault(o, 0)
981             tracks = o.tracks()
982             if tracks is None:
983                 self.track_map[None].append(o)
984             else:
985                 for c in tracks:
986                     self.track_map[c].append(o)
987     def __str__(self):
988         return getattr(self, '__name__',
989                        ('LocalOptGroup(%s)' %
990                         ','.join([str(o) for o in self.opts])))
991     def tracks(self):
992         t = []
993         for l in self.opts:
994             tt = l.tracks()
995             if tt:
996                 t.extend(tt)
997         return t
998     def transform(self, node):
999         if len(self.opts) == 0:
1000             return
1001         fgraph = node.fgraph
1002         repl = None
1003         while True:
1004             opts = self.track_map[type(node.op)] + self.track_map[node.op] + self.track_map[None]
1005             new_repl = None
1006             for opt in opts:
1007                 opt_start = time.time()
1008                 new_repl = opt.transform(node)
1009                 opt_finish = time.time()
1010                 if self.profile:
1011                     self.time_opts[opt] += opt_start - opt_finish
1012                     self.process_count[opt] += 1
1013                 if not new_repl:
1014                     continue
1015                 if isinstance(new_repl, (tuple, list)):
1016                     new_vars = new_repl
1017                 else:  # It must be a dict
1018                     new_vars = list(new_repl.values())
1019                 if self.profile:
1020                     self.node_created[opt] += len(graph.ops(fgraph.variables, new_vars))
1021                     self.applied_true[opt] += 1
1022                 break  # break from the for loop over optimization.
1023             if not new_repl:  # No optimization applied in the last iteration
1024                 return repl
1025             if not self.apply_all_opts:
1026                 return new_repl
1027             if not new_vars[0].owner:
1028                 return new_repl
1029             if len(new_repl) &gt; 1:
1030                 s = set([v.owner for v in new_repl])
1031                 assert len(s) == 1
1032             repl = new_repl
1033             node = new_vars[0].owner
1034     @staticmethod
1035     def print_profile(stream, prof, level=0):
1036         (time_opts, process_count, applied_true, node_created, profile) = prof
1037         if not profile:
1038             return
1039         blanc = ('    ' * int(level))
1040         print(blanc, "LocalOptGroup", file=stream)
1041         print(blanc, "---------------------", file=stream)
1042         count_opt = []
1043         not_used = []
1044         not_used_time = 0
1045         for o, count in iteritems(process_count):
1046             if count &gt; 0:
1047                 count_opt.append((time_opts[o], applied_true[o], count, o, node_created[o]))
1048             else:
1049                 not_used.append((time_opts[o], o))
1050                 not_used_time += time_opts[o]
1051         if count_opt:
1052             print(blanc,
1053                   '  time taken - times applied - times tried - name - node_created:',
1054                   file=stream)
1055             count_opt.sort()
1056             for (t, a_t, count, o, n_c) in count_opt[::-1]:
1057                 print(blanc, '  %.3fs - %d - %d - %s - %d' % (
1058                       t, a_t, count, o, n_c), file=stream)
1059             print(blanc, '  %.3fs - in %d optimization that were not used (display those with runtime greater than 0)' % (
1060                 not_used_time, len(not_used)), file=stream)
1061             not_used.sort(key=lambda nu: (nu[0], str(nu[1])))
1062             for (t, o) in not_used[::-1]:
1063                 if t &gt; 0:
1064                     print(blanc + "  ", '  %.3fs - %s' % (t, o), file=stream)
1065         else:
1066             print(blanc, " The Optimizer wasn't successful ", file=stream)
1067         print(file=stream)
1068     def merge_profile(prof1, prof2):
1069         raise NotImplementedError
1070     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1071         print("%s%s id=%i" % (
1072             (' ' * level), self.__class__.__name__, id(self)), file=stream)
1073         if depth != 0:
1074             depth -= 1
1075             for lopt in self.opts:
1076                 lopt.print_summary(stream, level=(level + 2), depth=depth)
1077     def add_requirements(self, fgraph):
1078         for opt in self.opts:
1079             opt.add_requirements(fgraph)
1080 class GraphToGPULocalOptGroup(LocalOptGroup):
1081     """This is the equivalent of LocalOptGroup for GraphToGPU.
1082     The main different is the function signature of the local
1083     optimizer that use the GraphToGPU signature and not the normal
1084     LocalOptimizer signature.
1085     apply_all_opts=True is not supported
1086     """
1087     def __init__(self, *optimizers, **kwargs):
1088         super(GraphToGPULocalOptGroup, self).__init__(*optimizers, **kwargs)
1089         assert self.apply_all_opts is False
1090     def transform(self, op, context_name, inputs, outputs):
1091         if len(self.opts) == 0:
1092             return
1093         fgraph = outputs[0].fgraph
1094         opts = self.track_map[type(op)] + self.track_map[op] + self.track_map[None]
1095         for opt in opts:
1096             opt_start = time.time()
1097             new_repl = opt.transform(op, context_name, inputs, outputs)
1098             opt_finish = time.time()
1099             if self.profile:
1100                 self.time_opts[opt] += opt_start - opt_finish
1101                 self.process_count[opt] += 1
1102             if not new_repl:
1103                 continue
1104             if self.profile:
1105                 self.node_created[opt] += len(graph.ops(fgraph.variables, new_repl))
1106                 self.applied_true[opt] += 1
1107             return new_repl
1108 class OpSub(LocalOptimizer):
1109     """
1110     Replaces the application of a certain op by the application of
1111     another op that takes the same inputs as what they are replacing.
1112     Parameters
1113     ----------
1114     op1, op2
1115         op1.make_node and op2.make_node must take the same number of
1116         inputs and have the same number of outputs.
1117     Examples
1118     --------
1119     OpSub(add, sub) ==&gt;
1120         add(div(x, y), add(y, x)) -&gt; sub(div(x, y), sub(y, x))
1121     """
1122     reentrant = False
1123     retains_inputs = True
1124     def __init__(self, op1, op2, transfer_tags=True):
1125         self.op1 = op1
1126         self.op2 = op2
1127         self.transfer_tags = transfer_tags
1128     def op_key(self):
1129         return self.op1
1130     def tracks(self):
1131         return [self.op1]
1132     def transform(self, node):
1133         if node.op != self.op1:
1134             return False
1135         repl = self.op2.make_node(*node.inputs)
1136         if self.transfer_tags:
1137             repl.tag = copy.copy(node.tag)
1138             for output, new_output in zip(node.outputs, repl.outputs):
1139                 new_output.tag = copy.copy(output.tag)
1140         return repl.outputs
1141     def __str__(self):
1142         return "%s -&gt; %s" % (self.op1, self.op2)
1143 class OpRemove(LocalOptimizer):
1144     """
1145     Removes all applications of an op by transferring each of its
1146     outputs to the corresponding input.
1147     """
1148     reentrant = False      # no nodes are added at all
1149     def __init__(self, op):
1150         self.op = op
1151     def op_key(self):
1152         return self.op
1153     def tracks(self):
1154         return [self.op]
1155     def transform(self, node):
1156         if node.op != self.op:
1157             return False
1158         return node.inputs
1159     def __str__(self):
1160         return "%s(x) -&gt; x" % (self.op)
1161     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1162         print("%s%s(%s) id=%i" % (
1163             ' ' * level,
1164             self.__class__.__name__,
1165             str(self.op),
1166             id(self)), file=stream)
1167 class PatternSub(LocalOptimizer):
1168     """
1169     @todo update
1170     Replaces all occurrences of the input pattern by the output pattern:
1171     input_pattern ::= (op, &lt;sub_pattern1&gt;, &lt;sub_pattern2&gt;, ...)
1172     input_pattern ::= dict(pattern = &lt;input_pattern&gt;,
1173                             constraint = &lt;constraint&gt;)
1174     sub_pattern ::= input_pattern
1175     sub_pattern ::= string
1176     sub_pattern ::= a Constant instance
1177     sub_pattern ::= int
1178     sub_pattern ::= float
1179     constraint ::= lambda fgraph, expr: additional matching condition
1180     output_pattern ::= (op, &lt;output_pattern1&gt;, &lt;output_pattern2&gt;, ...)
1181     output_pattern ::= string
1182     output_pattern ::= int
1183     output_pattern ::= float
1184     Each string in the input pattern is a variable that will be set to
1185     whatever expression is found in its place. If the same string is
1186     used more than once, the same expression must be found in those
1187     places. If a string used in the input pattern is used in the
1188     output pattern, the matching expression will be inserted in its
1189     place. The input pattern cannot just be a string but the output
1190     pattern can.
1191     If you put a constant variable in the input pattern, there will be a
1192     match iff a constant variable with the same value and the same type
1193     is found in its place.
1194     You can add a constraint to the match by using the dict(...)  form
1195     described above with a 'constraint' key. The constraint must be a
1196     function that takes the fgraph and the current Variable that we are
1197     trying to match and returns True or False according to an
1198     arbitrary criterion.
1199     The constructor creates a PatternSub that replaces occurrences of
1200     in_pattern by occurrences of out_pattern.
1201     Parameters
1202     ----------
1203     in_pattern
1204         The input pattern that we want to replace.
1205     out_pattern
1206         The replacement pattern.
1207     allow_multiple_clients : bool
1208         If False, the pattern matching will fail if one of the subpatterns has
1209         more than one client.
1210     skip_identities_fn : TODO
1211     name
1212         Allows to override this optimizer name.
1213     pdb : bool
1214         If True, we invoke pdb when the first node in the pattern matches.
1215     tracks : optional
1216         The values that self.tracks() will return. Useful to speed up
1217         optimization sometimes.
1218     get_nodes : optional
1219         If you provide `tracks`, you must provide this parameter. It must be a
1220         function that takes the tracked node and returns a list of nodes on
1221         which we will try this optimizer.
1222     Notes
1223     -----
1224     `tracks` and `get_nodes` can be used to make this optimizer track a less
1225     frequent Op, so this will make this optimizer tried less frequently.
1226     Examples
1227     --------
1228     PatternSub((add, 'x', 'y'), (add, 'y', 'x'))
1229     PatternSub((multiply, 'x', 'x'), (square, 'x'))
1230     PatternSub((subtract, (add, 'x', 'y'), 'y'), 'x')
1231     PatternSub((power, 'x', Constant(double, 2.0)), (square, 'x'))
1232     PatternSub((boggle, {'pattern': 'x',
1233                          'constraint': lambda expr: expr.type == scrabble}),
1234                (scrabble, 'x'))
1235     """
1236     def __init__(self, in_pattern, out_pattern,
1237                  allow_multiple_clients=False,
1238                  skip_identities_fn=None, name=None, pdb=False,
1239                  tracks=(), get_nodes=None,
1240                  values_eq_approx=None):
1241         self.in_pattern = in_pattern
1242         self.out_pattern = out_pattern
1243         self.values_eq_approx = values_eq_approx
1244         if isinstance(in_pattern, (list, tuple)):
1245             self.op = self.in_pattern[0]
1246         elif isinstance(in_pattern, dict):
1247             self.op = self.in_pattern['pattern'][0]
1248         else:
1249             raise TypeError("The pattern to search for must start with "
1250                             "a specific Op instance.")
1251         self.__doc__ = (self.__class__.__doc__ +
1252                         "\n\nThis instance does: " +
1253                         str(self) + "\n")
1254         self.allow_multiple_clients = allow_multiple_clients
1255         self.skip_identities_fn = skip_identities_fn
1256         if name:
1257             self.__name__ = name
1258         self.pdb = pdb
1259         self._tracks = tracks
1260         self.get_nodes = get_nodes
1261         if tracks != ():
1262             assert get_nodes
1263     def op_key(self):
1264         return self.op
1265     def tracks(self):
1266         if self._tracks != ():
1267             return self._tracks
1268         return [self.op]
1269     def transform(self, node, get_nodes=True):
1270         """
1271         Checks if the graph from node corresponds to in_pattern. If it does,
1272         constructs out_pattern and performs the replacement.
1273         """
1274         if get_nodes and self.get_nodes is not None:
1275             for real_node in self.get_nodes(node):
1276                 if real_node == "output":
1277                     continue
1278                 ret = self.transform(real_node, get_nodes=False)
1279                 if ret is not False and ret is not None:
1280                     assert len(real_node.outputs) == len(ret)
1281                     if self.values_eq_approx:
1282                         ret.tag.values_eq_approx = self.values_eq_approx
1283                     return dict(izip(real_node.outputs, ret))
1284         if node.op != self.op:
1285             return False
1286         def match(pattern, expr, u, allow_multiple_clients=False, pdb=False):
1287             def retry_with_equiv():
1288                 if not self.skip_identities_fn:
1289                     return False
1290                 expr_equiv = self.skip_identities_fn(expr)
1291                 if expr_equiv is None:
1292                     return False
1293                 return match(pattern, expr_equiv, u,
1294                              allow_multiple_clients=allow_multiple_clients)
1295             if isinstance(pattern, (list, tuple)):
1296                 if expr.owner is None:
1297                     return False
1298                 if (not (expr.owner.op == pattern[0]) or
1299                         (not allow_multiple_clients and len(expr.clients) &gt; 1)):
1300                     return retry_with_equiv()
1301                 if len(pattern) - 1 != len(expr.owner.inputs):
1302                     return retry_with_equiv()
1303                 for p, v in zip(pattern[1:], expr.owner.inputs):
1304                     u = match(p, v, u, self.allow_multiple_clients)
1305                     if not u:
1306                         return False
1307             elif isinstance(pattern, dict):
1308                 try:
1309                     real_pattern = pattern['pattern']
1310                 except KeyError:
1311                     raise KeyError(
1312                         "Malformed pattern: %s (expected key 'pattern')"
1313                         % pattern)
1314                 constraint = pattern.get('constraint', lambda expr: True)
1315                 if constraint(expr):
1316                     return match(real_pattern, expr, u,
1317                                  pattern.get('allow_multiple_clients',
1318                                              allow_multiple_clients))
1319                 else:
1320                     return retry_with_equiv()
1321             elif isinstance(pattern, string_types):
1322                 v = unify.Var(pattern)
1323                 if u[v] is not v and u[v] is not expr:
1324                     return retry_with_equiv()
1325                 else:
1326                     u = u.merge(expr, v)
1327             elif (isinstance(pattern, (integer_types, float)) and
1328                     isinstance(expr, graph.Constant)):
1329                 if np.all(theano.tensor.constant(pattern).value == expr.value):
1330                     return u
1331                 else:
1332                     return retry_with_equiv()
1333             elif (isinstance(pattern, graph.Constant) and
1334                     isinstance(expr, graph.Constant) and
1335                     pattern.equals(expr)):
1336                 return u
1337             else:
1338                 return retry_with_equiv()
1339             if pdb:
1340                 import pdb
1341                 pdb.set_trace()
1342             return u
1343         u = match(self.in_pattern, node.out, unify.Unification(), True,
1344                   self.pdb)
1345         if u:
1346             def build(pattern, u):
1347                 if isinstance(pattern, (list, tuple)):
1348                     args = [build(p, u) for p in pattern[1:]]
1349                     return pattern[0](*args)
1350                 elif isinstance(pattern, string_types):
1351                     return u[unify.Var(pattern)]
1352                 elif isinstance(pattern, (integer_types, float)):
1353                     return pattern
1354                 else:
1355                     return pattern.clone()
1356             p = self.out_pattern
1357             ret = build(p, u)
1358             if self.values_eq_approx:
1359                 ret.tag.values_eq_approx = self.values_eq_approx
1360             return [ret]
1361         else:
1362             return False
1363     def __str__(self):
1364         if getattr(self, '__name__', None):
1365             return self.__name__
1366         def pattern_to_str(pattern):
1367             if isinstance(pattern, (list, tuple)):
1368                 return "%s(%s)" % (
1369                     str(pattern[0]),
1370                     ", ".join([pattern_to_str(p) for p in pattern[1:]]))
1371             elif isinstance(pattern, dict):
1372                 return "%s subject to %s" % (
1373                     pattern_to_str(pattern['pattern']),
1374                     str(pattern.get('constraint', 'no conditions')))
1375             else:
1376                 return str(pattern)
1377         return "%s -&gt; %s" % (
1378             pattern_to_str(self.in_pattern),
1379             pattern_to_str(self.out_pattern))
1380     def __repr__(self):
1381         return str(self)
1382     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1383         name = getattr(self, '__name__', getattr(self, 'name', None))
1384         print("%s%s %s(%s, %s) id=%i" % (
1385             ' ' * level,
1386             self.__class__.__name__,
1387             name,
1388             str(self.in_pattern),
1389             str(self.out_pattern),
1390             id(self)), file=stream)
1391 class Updater:
1392     def __init__(self, importer, pruner, chin, name=None):
1393         self.importer = importer
1394         self.pruner = pruner
1395         self.chin = chin
1396         self.name = name
1397     def __str__(self):
1398         return "Updater{%s}" % str(self.name)
1399     def on_import(self, fgraph, node, reason):
1400         if self.importer:
1401             self.importer(node)
1402     def on_prune(self, fgraph, node, reason):
1403         if self.pruner:
1404             self.pruner(node)
1405     def on_change_input(self, fgraph, node, i, r, new_r, reason):
1406         if self.chin:
1407             self.chin(node, i, r, new_r, reason)
1408     def on_detach(self, fgraph):
1409         self.importer = None
1410         self.pruner = None
1411         self.chin = None
1412 class NavigatorOptimizer(Optimizer):
1413     """
1414     Abstract class.
1415     Parameters
1416     ----------
1417     local_opt
1418         A LocalOptimizer to apply over a FunctionGraph (or None is Ok too).
1419     ignore_newtrees
1420         - True: new subgraphs returned by an optimization is not a
1421           candidate for optimization.
1422         - False: new subgraphs returned by an optimization is a candidate
1423           for optimization.
1424         - 'auto': let the local_opt set this parameter via its 'reentrant'
1425           attribute.
1426     failure_callback
1427             A function that takes (exception, navigator, [(old, new),
1428             (old,new),...]) and we call it if there's an exception.
1429             If the trouble is from local_opt.transform(), the new variables
1430             will be 'None'.
1431             If the trouble is from validation (the new types don't match for
1432             example) then the new variables will be the ones created by
1433             transform().
1434             If this parameter is None, then exceptions are not caught here
1435             (raised normally).
1436     """
1437     @staticmethod
1438     def warn(exc, nav, repl_pairs, local_opt, node):
1439         """
1440         Failure_callback for NavigatorOptimizer: print traceback.
1441         """
1442         if config.on_opt_error != 'ignore':
1443             _logger.error("Optimization failure due to: %s" % str(local_opt))
1444             _logger.error("node: %s" % str(node))
1445             _logger.error("TRACEBACK:")
1446             _logger.error(traceback.format_exc())
1447         if config.on_opt_error == 'pdb':
1448             pdb.post_mortem(sys.exc_info()[2])
1449         elif isinstance(exc, AssertionError) or config.on_opt_error == 'raise':
1450             raise exc
1451     @staticmethod
1452     def warn_inplace(exc, nav, repl_pairs, local_opt, node):
1453         """
1454         Failure_callback for NavigatorOptimizer.
1455         Ignore InconsistencyErrors, print traceback.
1456         If error during replacement repl_pairs is set. Otherwise None.
1457         """
1458         if isinstance(exc, InconsistencyError):
1459             return
1460         return NavigatorOptimizer.warn(exc, nav, repl_pairs, local_opt, node)
1461     @staticmethod
1462     def warn_ignore(exc, nav, repl_pairs, local_opt, node):
1463         """
1464         Failure_callback for NavigatorOptimizer: ignore all errors.
1465         """
1466         pass
1467     def __init__(self, local_opt, ignore_newtrees='auto',
1468                  failure_callback=None):
1469         self.local_opt = local_opt
1470         if ignore_newtrees == 'auto':
1471             self.ignore_newtrees = not getattr(local_opt, 'reentrant', True)
1472         else:
1473             self.ignore_newtrees = ignore_newtrees
1474         self.failure_callback = failure_callback
1475     def attach_updater(self, fgraph, importer, pruner, chin=None, name=None):
1476         """
1477         Install some FunctionGraph listeners to help the navigator deal with
1478         the ignore_trees-related functionality.
1479         Parameters
1480         ----------
1481         importer
1482             Function that will be called whenever optimizations add stuff
1483             to the graph.
1484         pruner
1485             Function to be called when optimizations remove stuff
1486             from the graph.
1487         chin
1488             "on change input" called whenever a node's inputs change.
1489         name
1490             name of the Updater to attach.
1491         Returns
1492         -------
1493         object
1494             The FunctionGraph plugin that handles the three tasks.
1495             Keep this around so that you can detach later!
1496         """
1497         if self.ignore_newtrees:
1498             importer = None
1499         if importer is None and pruner is None:
1500             return None
1501         u = Updater(importer, pruner, chin, name=name)
1502         fgraph.attach_feature(u)
1503         return u
1504     def detach_updater(self, fgraph, u):
1505         """
1506         Undo the work of attach_updater.
1507         Parameters
1508         ----------
1509         u
1510             A return-value of attach_updater.
1511         Returns
1512         -------
1513         None
1514         """
1515         if u is not None:
1516             fgraph.remove_feature(u)
1517     def process_node(self, fgraph, node, lopt=None):
1518         """
1519         This function will use `lopt` to `transform` the `node`. The
1520         `transform` method will return either False or a list of Variables
1521         that are intended to replace `node.outputs`.
1522         If the fgraph accepts the replacement, then the optimization is
1523         successful, and this function returns True.
1524         If there are no replacement candidates or the fgraph rejects the
1525         replacements, this function returns False.
1526         Parameters
1527         ----------
1528         fgraph
1529             A FunctionGraph.
1530         node
1531             An Apply instance in `fgraph`
1532         lopt
1533             A LocalOptimizer instance that may have a better idea for
1534             how to compute node's outputs.
1535         Returns
1536         -------
1537         bool
1538             True iff the `node`'s outputs were replaced in the `fgraph`.
1539         """
1540         lopt = lopt or self.local_opt
1541         try:
1542             replacements = lopt.transform(node)
1543         except Exception as e:
1544             if self.failure_callback is not None:
1545                 self.failure_callback(e, self,
1546                                       [(x, None) for x in node.outputs],
1547                                       lopt, node)
1548                 return False
1549             else:
1550                 raise
1551         if replacements is False or replacements is None:
1552             return False
1553         old_vars = node.outputs
1554         remove = []
1555         if isinstance(replacements, dict):
1556             if "remove" in replacements:
1557                 remove = replacements.pop("remove")
1558             old_vars = list(replacements.keys())
1559             replacements = list(replacements.values())
1560         elif not isinstance(replacements, (tuple, list)):
1561             raise TypeError('Optimizer %s gave wrong type of replacement. '
1562                             'Expected list or tuple. Got %s' % (
1563                                 lopt, replacements))
1564         if len(old_vars) != len(replacements):
1565             raise ValueError('Optimizer %s gave wrong number of replacements'
1566                              % lopt)
1567         for r, rnew in zip(old_vars, replacements):
1568             if rnew is None and len(r.clients) &gt; 0:
1569                 raise ValueError("A local optimizer tried to remove a Variable that is used")
1570         repl_pairs = [(r, rnew) for r, rnew in zip(old_vars, replacements)
1571                       if rnew is not r and rnew is not None]
1572         if len(repl_pairs) == 0:
1573             return False
1574         try:
1575             fgraph.replace_all_validate_remove(repl_pairs,
1576                                                reason=lopt,
1577                                                remove=remove)
1578             return True
1579         except Exception as e:
1580             if self.failure_callback is not None:
1581                 self.failure_callback(e, self, repl_pairs, lopt, node)
1582                 return False
1583             else:
1584                 raise
1585     def add_requirements(self, fgraph):
1586         super(NavigatorOptimizer, self).add_requirements(fgraph)
1587         if self.local_opt:
1588             self.local_opt.add_requirements(fgraph)
1589     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1590         print("%s%s (%i)" % (
1591             (' ' * level), self.__class__.__name__, id(self)), file=stream)
1592         if depth != 0:
1593             self.local_opt.print_summary(stream, level=(level + 2),
1594                                          depth=(depth - 1))
1595 class TopoOptimizer(NavigatorOptimizer):
1596     """
1597     TopoOptimizer has one local optimizer. It tries to apply to each node, in topological order (or reverse).
1598     Each time the local optimizer applies, the node gets replaced, and the topooptimizer moves on to the next one.
1599     """
1600     def __init__(self, local_opt, order='in_to_out', ignore_newtrees=False,
1601                  failure_callback=None):
1602         if order not in ['out_to_in', 'in_to_out']:
1603             raise ValueError("order must be 'out_to_in' or 'in_to_out'")
1604         self.order = order
1605         NavigatorOptimizer.__init__(self, local_opt, ignore_newtrees,
1606                                     failure_callback)
1607     def apply(self, fgraph, start_from=None):
1608         if start_from is None:
1609             start_from = fgraph.outputs
1610         callback_before = fgraph.execute_callbacks_time
1611         nb_nodes_start = len(fgraph.apply_nodes)
1612         t0 = time.time()
1613         q = deque(graph.io_toposort(fgraph.inputs, start_from))
1614         io_t = time.time() - t0
1615         def importer(node):
1616             if node is not current_node:
1617                 q.append(node)
1618         u = self.attach_updater(fgraph, importer, None,
1619                                 name=getattr(self, 'name', None))
1620         nb = 0
1621         try:
1622             t0 = time.time()
1623             while q:
1624                 if self.order == 'out_to_in':
1625                     node = q.pop()
1626                 else:
1627                     node = q.popleft()
1628                 if node not in fgraph.apply_nodes:
1629                     continue
1630                 current_node = node
1631                 nb += self.process_node(fgraph, node)
1632             loop_t = time.time() - t0
1633         finally:
1634             self.detach_updater(fgraph, u)
1635         callback_time = fgraph.execute_callbacks_time - callback_before
1636         nb_nodes_end = len(fgraph.apply_nodes)
1637         return (self, nb, nb_nodes_start, nb_nodes_end,
1638                 io_t, loop_t, callback_time, self.local_opt)
1639     @staticmethod
1640     def print_profile(stream, prof, level=0):
1641         blanc = ('    ' * level)
1642         if prof is None:  # Happen as merge_profile() isn't implemented
1643             print(blanc, "TopoOptimizer merge_profile not implemented",
1644                   file=stream)
1645             return
1646         (opt, nb, nb_nodes_start, nb_nodes_end,
1647          io_t, loop_t, callback_time, lopt) = prof
1648         print(blanc, "TopoOptimizer ",
1649               getattr(opt, "name", getattr(opt, "__name__", "")), file=stream)
1650         print(blanc, "  nb_node (start, end, changed)", (
1651             nb_nodes_start, nb_nodes_end, nb), file=stream)
1652         print(blanc, "  init io_toposort", io_t, file=stream)
1653         print(blanc, "  loop time", loop_t, file=stream)
1654         print(blanc, "  callback_time", callback_time, file=stream)
1655         if isinstance(lopt, LocalOptGroup):
1656             if lopt.profile:
1657                 lopt.print_profile(stream, (lopt.time_opts,
1658                                             lopt.process_count,
1659                                             lopt.applied_true,
1660                                             lopt.node_created,
1661                                             lopt.profile),
1662                                    level=level + 1)
1663     def __str__(self):
1664         return getattr(self, '__name__',
1665                        '&lt;TopoOptimizer instance&gt;')
1666 def out2in(*local_opts, **kwargs):
1667     """
1668     Uses the TopoOptimizer from the output nodes to input nodes of the graph.
1669     """
1670     name = (kwargs and kwargs.pop('name', None))
1671     if len(local_opts) &gt; 1:
1672         local_opts = LocalOptGroup(*local_opts)
1673     else:
1674         local_opts, = local_opts
1675         if not name:
1676             name = local_opts.__name__
1677     ret = TopoOptimizer(local_opts,
1678                         order='out_to_in',
1679                         failure_callback=TopoOptimizer.warn_inplace,
1680                         **kwargs)
1681     if name:
1682         ret.__name__ = name
1683     return ret
1684 def in2out(*local_opts, **kwargs):
1685     """
1686     Uses the TopoOptimizer from the input nodes to output nodes of the graph.
1687     """
1688     name = (kwargs and kwargs.pop('name', None))
1689     if len(local_opts) &gt; 1:
1690         local_opts = LocalOptGroup(*local_opts)
1691     else:
1692         local_opts, = local_opts
1693         if not name:
1694             name = local_opts.__name__
1695     ret = TopoOptimizer(local_opts,
1696                         order='in_to_out',
1697                         failure_callback=TopoOptimizer.warn_inplace,
1698                         **kwargs)
1699     if name:
1700         ret.__name__ = name
1701     return ret
1702 class OpKeyOptimizer(NavigatorOptimizer):
1703     """
1704     WRITEME
1705     """
1706     def __init__(self, local_opt, ignore_newtrees=False,
1707                  failure_callback=None):
1708         if not hasattr(local_opt, 'op_key'):
1709             raise TypeError("LocalOptimizer for OpKeyOptimizer must have "
1710                             "an 'op_key' method.")
1711         NavigatorOptimizer.__init__(self, local_opt, ignore_newtrees,
1712                                     failure_callback)
1713     def apply(self, fgraph):
1714         op = self.local_opt.op_key()
1715         if isinstance(op, (list, tuple)):
1716             q = reduce(list.__iadd__, map(fgraph.get_nodes, op))
1717         else:
1718             q = list(fgraph.get_nodes(op))
1719         def importer(node):
1720             if node is not current_node:
1721                 if node.op == op:
1722                     q.append(node)
1723         u = self.attach_updater(fgraph, importer, None,
1724                                 name=getattr(self, 'name', None))
1725         try:
1726             while q:
1727                 node = q.pop()
1728                 if node not in fgraph.apply_nodes:
1729                     continue
1730                 current_node = node
1731                 self.process_node(fgraph, node)
1732         finally:
1733             self.detach_updater(fgraph, u)
1734     def add_requirements(self, fgraph):
1735         """
1736         Requires the following features:
1737           - NodeFinder
1738           - ReplaceValidate(Added by default)
1739         """
1740         super(OpKeyOptimizer, self).add_requirements(fgraph)
1741         fgraph.attach_feature(toolbox.NodeFinder())
1742 class ChangeTracker:
1743     def __init__(self):
1744         self.changed = False
1745         self.nb_imported = 0
1746     def on_import(self, fgraph, node, reason):
1747         self.nb_imported += 1
1748         self.changed = True
1749     def on_change_input(self, fgraph, node, i, r, new_r, reason):
1750         self.changed = True
1751     def reset(self):
1752         self.changed = False
1753     def on_attach(self, fgraph):
1754         fgraph.change_tracker = self
1755     def on_detach(self, fgraph):
1756         del fgraph.change_tracker
1757 def merge_dict(d1, d2):
1758     """
1759     merge 2 dicts by adding the values.
1760     """
1761     d = d1.copy()
1762     for k, v in iteritems(d2):
1763         if k in d:
1764             d[k] += v
1765         else:
1766             d[k] = v
1767     return d
1768 class EquilibriumOptimizer(NavigatorOptimizer):
1769     """
1770     Apply optimizations until equilibrium point.
1771     Parameters
1772     ----------
1773     optimizers : list or set
1774         Local or global optimizations to apply until equilibrium.
1775         The global optimizer will be run at the start of each iteration before
1776         the local optimizer.
1777     max_use_ratio : int or float
1778         Each optimizer can be applied at most (size of graph * this number)
1779         times.
1780     ignore_newtrees
1781         See EquilibriumDB ignore_newtrees parameter definition.
1782     final_optimizers
1783         Global optimizers that will be run after each iteration.
1784     cleanup_optimizers
1785         Global optimizers that apply a list of pre determined optimization.
1786         They must not traverse the graph as they are called very frequently.
1787         The MergeOptimizer is one example of optimization that respect this.
1788         They are applied after all global optimizer, then when one local optimizer is applied, then after all final optimizer.
1789     """
1790     def __init__(self,
1791                  optimizers,
1792                  failure_callback=None,
1793                  ignore_newtrees=True,
1794                  tracks_on_change_inputs=False,
1795                  max_use_ratio=None,
1796                  final_optimizers=None,
1797                  cleanup_optimizers=None):
1798         super(EquilibriumOptimizer, self).__init__(
1799             None,
1800             ignore_newtrees=ignore_newtrees,
1801             failure_callback=failure_callback)
1802         self.local_optimizers_map = OrderedDict()
1803         self.local_optimizers_all = []
1804         self.global_optimizers = []
1805         self.final_optimizers = []
1806         self.cleanup_optimizers = []
1807         self.tracks_on_change_inputs = tracks_on_change_inputs
1808         for opt in optimizers:
1809             if isinstance(opt, LocalOptimizer):
1810                 if opt.tracks() is None:
1811                     self.local_optimizers_all.append(opt)
1812                 else:
1813                     for c in opt.tracks():
1814                         self.local_optimizers_map.setdefault(c, []).append(opt)
1815             else:
1816                 self.global_optimizers.append(opt)
1817         if final_optimizers:
1818             self.final_optimizers = final_optimizers
1819         if cleanup_optimizers:
1820             self.cleanup_optimizers = cleanup_optimizers
1821         self.max_use_ratio = max_use_ratio
1822         assert self.max_use_ratio is not None, (
1823             'max_use_ratio has to be a number')
1824     def get_local_optimizers(self):
1825         for opt in self.local_optimizers_all:
1826             yield opt
1827         s = set()
1828         for lopt in itervalues(self.local_optimizers_map):
1829             for opt in lopt:
1830                 if opt not in s:
1831                     yield opt
1832                     s.add(opt)
1833     def add_requirements(self, fgraph):
1834         super(EquilibriumOptimizer, self).add_requirements(fgraph)
1835         for opt in self.get_local_optimizers():
1836             opt.add_requirements(fgraph)
1837         for opt in self.global_optimizers:
1838             opt.add_requirements(fgraph)
1839         for opt in self.final_optimizers:
1840             opt.add_requirements(fgraph)
1841         for opt in self.cleanup_optimizers:
1842             opt.add_requirements(fgraph)
1843     def apply(self, fgraph, start_from=None):
1844         change_tracker = ChangeTracker()
1845         fgraph.attach_feature(change_tracker)
1846         if start_from is None:
1847             start_from = fgraph.outputs
1848         else:
1849             for node in start_from:
1850                 assert node in fgraph.outputs
1851         changed = True
1852         max_use_abort = False
1853         opt_name = None
1854         global_process_count = {}
1855         start_nb_nodes = len(fgraph.apply_nodes)
1856         max_nb_nodes = len(fgraph.apply_nodes)
1857         max_use = max_nb_nodes * self.max_use_ratio
1858         loop_timing = []
1859         loop_process_count = []
1860         global_opt_timing = []
1861         time_opts = {}
1862         io_toposort_timing = []
1863         nb_nodes = []
1864         node_created = {}
1865         global_sub_profs = []
1866         final_sub_profs = []
1867         cleanup_sub_profs = []
1868         for opt in (self.global_optimizers +
1869                     list(self.get_local_optimizers()) +
1870                     self.final_optimizers +
1871                     self.cleanup_optimizers):
1872             global_process_count.setdefault(opt, 0)
1873             time_opts.setdefault(opt, 0)
1874             node_created.setdefault(opt, 0)
1875         def apply_cleanup(profs_dict):
1876             changed = False
1877             for copt in self.cleanup_optimizers:
1878                 change_tracker.reset()
1879                 nb = change_tracker.nb_imported
1880                 t_opt = time.time()
1881                 sub_prof = copt.apply(fgraph)
1882                 time_opts[copt] += time.time() - t_opt
1883                 profs_dict[copt].append(sub_prof)
1884                 if change_tracker.changed:
1885                     process_count.setdefault(copt, 0)
1886                     process_count[copt] += 1
1887                     global_process_count[copt] += 1
1888                     changed = True
1889                     node_created[copt] += change_tracker.nb_imported - nb
1890             return changed
1891         while changed and not max_use_abort:
1892             process_count = {}
1893             t0 = time.time()
1894             changed = False
1895             iter_cleanup_sub_profs = {}
1896             for copt in self.cleanup_optimizers:
1897                 iter_cleanup_sub_profs[copt] = []
1898             sub_profs = []
1899             for gopt in self.global_optimizers:
1900                 change_tracker.reset()
1901                 nb = change_tracker.nb_imported
1902                 t_opt = time.time()
1903                 sub_prof = gopt.apply(fgraph)
1904                 time_opts[gopt] += time.time() - t_opt
1905                 sub_profs.append(sub_prof)
1906                 if change_tracker.changed:
1907                     process_count.setdefault(gopt, 0)
1908                     process_count[gopt] += 1
1909                     global_process_count[gopt] += 1
1910                     changed = True
1911                     node_created[gopt] += change_tracker.nb_imported - nb
1912                     if global_process_count[gopt] &gt; max_use:
1913                         max_use_abort = True
1914                         opt_name = (getattr(gopt, "name", None) or
1915                                     getattr(gopt, "__name__", ""))
1916             global_sub_profs.append(sub_profs)
1917             global_opt_timing.append(float(time.time() - t0))
1918             changed |= apply_cleanup(iter_cleanup_sub_profs)
1919             topo_t0 = time.time()
1920             q = deque(graph.io_toposort(fgraph.inputs, start_from))
1921             io_toposort_timing.append(time.time() - topo_t0)
1922             nb_nodes.append(len(q))
1923             max_nb_nodes = max(max_nb_nodes, len(q))
1924             max_use = max_nb_nodes * self.max_use_ratio
1925             def importer(node):
1926                 if node is not current_node:
1927                     q.append(node)
1928             chin = None
1929             if self.tracks_on_change_inputs:
1930                 def chin(node, i, r, new_r, reason):
1931                     if node is not current_node and not isinstance(node, str):
1932                         q.append(node)
1933             u = self.attach_updater(fgraph, importer, None,
1934                                     chin=chin,
1935                                     name=getattr(self, 'name', None))
1936             try:
1937                 while q:
1938                     node = q.pop()
1939                     if node not in fgraph.apply_nodes:
1940                         continue
1941                     for lopt in (self.local_optimizers_all +
1942                                  self.local_optimizers_map.get(type(node.op), []) +
1943                                  self.local_optimizers_map.get(node<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.op, [])):
1944                         nb = change_tracker.nb_imported
1945                         t_opt = time.time()
1946                         lopt_change = self.process_node(fgraph, node, lopt)
1947                         time_opts[lopt] += time.time(</b></font>) - t_opt
1948                         if not lopt_change:
1949                             continue
1950                         process_count.setdefault(lopt, 0)
1951                         process_count[lopt] += 1
1952                         global_process_count[lopt] += 1
1953                         changed = True
1954                         node_created[lopt] += change_tracker.nb_imported - nb
1955                         changed |= apply_cleanup(iter_cleanup_sub_profs)
1956                         if global_process_count[lopt] &gt; max_use:
1957                             max_use_abort = True
1958                             opt_name = (getattr(lopt, "name", None) or
1959                                         getattr(lopt, "__name__", ""))
1960                         if node not in fgraph.apply_nodes:
1961                             break
1962             finally:
1963                 self.detach_updater(fgraph, u)
1964             sub_profs = []
1965             t_before_final_opt = time.time()
1966             for gopt in self.final_optimizers:
1967                 change_tracker.reset()
1968                 nb = change_tracker.nb_imported
1969                 t_opt = time.time()
1970                 sub_prof = gopt.apply(fgraph)
1971                 time_opts[gopt] += time.time() - t_opt
1972                 sub_profs.append(sub_prof)
1973                 if change_tracker.changed:
1974                     process_count.setdefault(gopt, 0)
1975                     process_count[gopt] += 1
1976                     global_process_count[gopt] += 1
1977                     changed = True
1978                     node_created[gopt] += change_tracker.nb_imported - nb
1979                     if global_process_count[gopt] &gt; max_use:
1980                         max_use_abort = True
1981                         opt_name = (getattr(gopt, "name", None) or
1982                                     getattr(gopt, "__name__", ""))
1983             final_sub_profs.append(sub_profs)
1984             global_opt_timing[-1] += time.time() - t_before_final_opt
1985             changed |= apply_cleanup(iter_cleanup_sub_profs)
1986             c_sub_profs = []
1987             for copt, sub_profs in iteritems(iter_cleanup_sub_profs):
1988                 sub_prof = sub_profs[0]
1989                 for s_p in sub_profs[1:]:
1990                     sub_prof = copt.merge_profile(sub_prof, s_p)
1991                 c_sub_profs.append(sub_prof)
1992             cleanup_sub_profs.append(c_sub_profs)
1993             loop_process_count.append(process_count)
1994             loop_timing.append(float(time.time() - t0))
1995         end_nb_nodes = len(fgraph.apply_nodes)
1996         if max_use_abort:
1997             msg = ("EquilibriumOptimizer max'ed out by '%s'" % opt_name +
1998                    ". You can safely raise the current threshold of " +
1999                    "%f with the theano flag 'optdb.max_use_ratio'." %
2000                    config.optdb.max_use_ratio)
2001             if theano.config.on_opt_error == 'raise':
2002                 raise AssertionError(msg)
2003             else:
2004                 _logger.error(msg)
2005         fgraph.remove_feature(change_tracker)
2006         assert len(loop_process_count) == len(loop_timing)
2007         assert len(loop_process_count) == len(global_opt_timing)
2008         assert len(loop_process_count) == len(nb_nodes)
2009         assert len(loop_process_count) == len(io_toposort_timing)
2010         assert len(loop_process_count) == len(global_sub_profs)
2011         assert len(loop_process_count) == len(final_sub_profs)
2012         assert len(loop_process_count) == len(cleanup_sub_profs)
2013         return (self, loop_timing, loop_process_count,
2014                 (start_nb_nodes, end_nb_nodes, max_nb_nodes),
2015                 global_opt_timing, nb_nodes, time_opts, io_toposort_timing,
2016                 node_created, global_sub_profs, final_sub_profs,
2017                 cleanup_sub_profs)
2018     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
2019         name = getattr(self, 'name', None)
2020         print("%s%s %s id=%i" % (
2021             (' ' * level), self.__class__.__name__, name, id(self)), file=stream)
2022         if depth != 0:
2023             for lopt in self.get_local_optimizers():
2024                 lopt.print_summary(stream, level=(level + 2),
2025                                    depth=(depth - 1))
2026     @staticmethod
2027     def print_profile(stream, prof, level=0):
2028         (opt, loop_timing, loop_process_count,
2029          (start_nb_nodes, end_nb_nodes, max_nb_nodes),
2030          global_opt_timing, nb_nodes, time_opts, io_toposort_timing,
2031          node_created, global_sub_profs, final_sub_profs,
2032          cleanup_sub_profs) = prof
2033         blanc = ('    ' * level)
2034         print(blanc, "EquilibriumOptimizer", end=' ', file=stream)
2035         print(blanc, getattr(opt, "name",
2036                              getattr(opt, "__name__", "")), file=stream)
2037         print(blanc, "  time %.3fs for %d passes" % (
2038             sum(loop_timing), len(loop_timing)), file=stream)
2039         print(blanc, "  nb nodes (start, end,  max) %d %d %d" % (
2040             start_nb_nodes, end_nb_nodes, max_nb_nodes), file=stream)
2041         print(blanc, "  time io_toposort %.3fs" % sum(
2042             io_toposort_timing), file=stream)
2043         s = sum([time_opts[o] for o in opt.get_local_optimizers()])
2044         print(blanc, "  time in local optimizers %.3fs" % s, file=stream)
2045         s = sum([time_opts[o] for o in opt.global_optimizers])
2046         print(blanc, "  time in global optimizers %.3fs" % s, file=stream)
2047         s = sum([time_opts[o] for o in opt.final_optimizers])
2048         print(blanc, "  time in final optimizers %.3fs" % s, file=stream)
2049         s = sum([time_opts[o] for o in opt.cleanup_optimizers])
2050         print(blanc, "  time in cleanup optimizers %.3fs" % s, file=stream)
2051         for i in range(len(loop_timing)):
2052             lopt = ""
2053             if loop_process_count[i]:
2054                 d = list(reversed(sorted(iteritems(loop_process_count[i]),
2055                                          key=lambda a: a[1])))
2056                 lopt = " ".join([str((str(k), v)) for k, v
2057                                  in d[:5]])
2058                 if len(d) &gt; 5:
2059                     lopt += " ..."
2060             print(blanc, ('  %2d - %.3fs %d (%.3fs in global opts, '
2061                           '%.3fs io_toposort) - %d nodes - %s' % (
2062                               i, loop_timing[i],
2063                               sum(loop_process_count[i].values()),
2064                               global_opt_timing[i],
2065                               io_toposort_timing[i], nb_nodes[i],
2066                               lopt)), file=stream)
2067         count_opt = []
2068         not_used = []
2069         not_used_time = 0
2070         process_count = {}
2071         for o in (opt.global_optimizers +
2072                   list(opt.get_local_optimizers()) +
2073                   list(opt.final_optimizers) +
2074                   list(opt.cleanup_optimizers)):
2075             process_count.setdefault(o, 0)
2076         for count in loop_process_count:
2077             for o, v in iteritems(count):
2078                 process_count[o] += v
2079         for o, count in iteritems(process_count):
2080             if count &gt; 0:
2081                 count_opt.append((time_opts[o], count,
2082                                   node_created[o], o))
2083             else:
2084                 not_used.append((time_opts[o], o))
2085                 not_used_time += time_opts[o]
2086         if count_opt:
2087             print(blanc,
2088                   '  times - times applied - nb node created - name:',
2089                   file=stream)
2090             count_opt.sort()
2091             for (t, count, n_created, o) in count_opt[::-1]:
2092                 print(blanc, '  %.3fs - %d - %d - %s' % (
2093                     t, count, n_created, o), file=stream)
2094             print(blanc, '  %.3fs - in %d optimization that were not used (display only those with a runtime &gt; 0)' % (
2095                 not_used_time, len(not_used)), file=stream)
2096             not_used.sort(key=lambda nu: (nu[0], str(nu[1])))
2097             for (t, o) in not_used[::-1]:
2098                 if t &gt; 0:
2099                     print(blanc + "  ", '  %.3fs - %s' % (t, o), file=stream)
2100             print(file=stream)
2101         gf_opts = [o for o in (opt.global_optimizers +
2102                                list(opt.final_optimizers) +
2103                                list(opt.cleanup_optimizers))
2104                    if o.print_profile.__code__ is not
2105                    Optimizer.print_profile.__code__]
2106         if not gf_opts:
2107             return
2108         print(blanc, "Global, final and clean up optimizers", file=stream)
2109         for i in range(len(loop_timing)):
2110             print(blanc, "Iter %d" % i, file=stream)
2111             for o, prof in zip(opt.global_optimizers, global_sub_profs[i]):
2112                 try:
2113                     o.print_profile(stream, prof, level + 2)
2114                 except NotImplementedError:
2115                     print(blanc, "merge not implemented for ", o)
2116             for o, prof in zip(opt.final_optimizers, final_sub_profs[i]):
2117                 try:
2118                     o.print_profile(stream, prof, level + 2)
2119                 except NotImplementedError:
2120                     print(blanc, "merge not implemented for ", o)
2121             for o, prof in zip(opt.cleanup_optimizers, cleanup_sub_profs[i]):
2122                 try:
2123                     o.print_profile(stream, prof, level + 2)
2124                 except NotImplementedError:
2125                     print(blanc, "merge not implemented for ", o)
2126     @staticmethod
2127     def merge_profile(prof1, prof2):
2128         local_optimizers = OrderedSet(prof1[0].get_local_optimizers()).union(
2129             prof2[0].get_local_optimizers())
2130         global_optimizers = OrderedSet(prof1[0].global_optimizers).union(
2131             prof2[0].global_optimizers)
2132         final_optimizers = list(OrderedSet(prof1[0].final_optimizers).union(
2133             prof2[0].final_optimizers))
2134         cleanup_optimizers = list(OrderedSet(prof1[0].cleanup_optimizers).union(
2135             prof2[0].cleanup_optimizers))
2136         new_opt = EquilibriumOptimizer(
2137             local_optimizers.union(global_optimizers),
2138             max_use_ratio=1,
2139             final_optimizers=final_optimizers,
2140             cleanup_optimizers=cleanup_optimizers)
2141         def add_append_list(l1, l2):
2142             l = copy.copy(l1)
2143             for idx, nb in enumerate(l2):
2144                 if idx &lt; len(l):
2145                     l[idx] += nb
2146                 else:
2147                     l.append(nb)
2148             return l
2149         loop_timing = add_append_list(prof1[1], prof2[1])
2150         loop_process_count = list(prof1[2])
2151         global_sub_profs = []
2152         final_sub_profs = []
2153         cleanup_sub_profs = []
2154         for i in range(min(len(loop_process_count), len(prof2[2]))):
2155             process_count = loop_process_count[i]
2156             for process, count in iteritems(prof2[2][i]):
2157                 if process in process_count:
2158                     process_count[process] += count
2159                 else:
2160                     process_count[process] = count
2161             def merge(opts, attr, idx):
2162                 tmp = []
2163                 for opt in opts:
2164                     o1 = getattr(prof1[0], attr)
2165                     o2 = getattr(prof2[0], attr)
2166                     if opt in o1 and opt in o2:
2167                         p1 = prof1[idx][i][o1.index(opt)]
2168                         p2 = prof2[idx][i][o2.index(opt)]
2169                         m = None
2170                         if hasattr(opt, 'merge_profile'):
2171                             m = opt.merge_profile(p1, p2)
2172                     elif opt in o1:
2173                         m = prof1[idx][i][o1.index(opt)]
2174                     else:
2175                         m = prof2[idx][i][o2.index(opt)]
2176                     tmp.append(m)
2177                 return tmp
2178             global_sub_profs.append(merge(global_optimizers, 'global_optimizers', 9))
2179             final_sub_profs.append(merge(final_optimizers, 'final_optimizers', 10))
2180             cleanup_sub_profs.append(merge(cleanup_optimizers, 'cleanup_optimizers', 11))
2181         loop_process_count.extend(prof1[2][len(loop_process_count):])
2182         global_sub_profs.extend(prof1[9][len(global_sub_profs):])
2183         final_sub_profs.extend(prof1[10][len(final_sub_profs):])
2184         cleanup_sub_profs.extend(prof1[11][len(cleanup_sub_profs):])
2185         global_sub_profs.extend(prof2[9][len(loop_process_count):])
2186         final_sub_profs.extend(prof2[10][len(loop_process_count):])
2187         cleanup_sub_profs.extend(prof2[11][len(loop_process_count):])
2188         max_nb_nodes = max(prof1[3], prof2[3])
2189         global_opt_timing = add_append_list(prof1[4], prof2[4])
2190         nb_nodes = add_append_list(prof1[5], prof2[5])
2191         time_opts = merge_dict(prof1[6], prof2[6])
2192         io_toposort_timing = add_append_list(prof1[7], prof2[7])
2193         assert (len(loop_timing) == len(global_opt_timing) ==
2194                 len(global_sub_profs) ==
2195                 len(io_toposort_timing) == len(nb_nodes))
2196         assert len(loop_timing) == max(len(prof1[1]), len(prof2[1]))
2197         node_created = merge_dict(prof1[8], prof2[8])
2198         return (new_opt,
2199                 loop_timing,
2200                 loop_process_count,
2201                 max_nb_nodes,
2202                 global_opt_timing,
2203                 nb_nodes,
2204                 time_opts,
2205                 io_toposort_timing,
2206                 node_created,
2207                 global_sub_profs,
2208                 final_sub_profs,
2209                 cleanup_sub_profs)
2210 def _check_chain(r, chain):
2211     """
2212     WRITEME
2213     """
2214     chain = list(reversed(chain))
2215     while chain:
2216         elem = chain.pop()
2217         if elem is None:
2218             if r.owner is not None:
2219                 return False
2220         elif r.owner is None:
2221             return False
2222         elif isinstance(elem, op.Op):
2223             if not r.owner.op == elem:
2224                 return False
2225         else:
2226             try:
2227                 if (issubclass(elem, op.Op) and
2228                         not isinstance(r.owner.op, elem)):
2229                     return False
2230             except TypeError:
2231                 return False
2232         if chain:
2233             r = r.owner.inputs[chain.pop()]
2234     return (r is not None)
2235 def check_chain(r, *chain):
2236     """
2237     WRITEME
2238     """
2239     if isinstance(r, graph.Apply):
2240         r = r.outputs[0]
2241     return _check_chain(r, reduce(list.__iadd__, ([x, 0] for x in chain)))
2242 def pre_greedy_local_optimizer(list_optimizations, out):
2243     """
2244     This function traverses the computation graph described by all
2245     ``node`` in the graph before the variable out but that are not in the
2246     fgraph. It applies each of the local_optimizations on the traversed graph.
2247     Its main use is to apply locally constant folding when generating
2248     the graph of the indices of a subtensor.
2249     We should not apply optimizations on node that are in fgraph.
2250     So we don't optimize node that have an attribute fgraph.
2251     Notes
2252     -----
2253     This doesn't do an equilibrium... So if there is optimization
2254     like local_upcast_elemwise_constant_inputs in the list, that
2255     adds additional node to the inputs of the node, it can
2256     be needed to call this function multiple times.
2257     """
2258     def local_recursive_function(list_opt, out, optimized_vars, depth):
2259         if not getattr(out, 'owner', None):
2260             return [out], optimized_vars
2261         node = out.owner
2262         if hasattr(node, 'fgraph'):
2263             return node.outputs, optimized_vars
2264         for idx, inp in enumerate(node.inputs):
2265             if inp in optimized_vars:
2266                 nw_in = optimized_vars[inp]
2267             else:
2268                 if inp.owner:
2269                     outs, optimized_vars = local_recursive_function(
2270                         list_opt,
2271                         inp,
2272                         optimized_vars,
2273                         depth + 1)
2274                     for k, v in zip(inp.owner.outputs, outs):
2275                         optimized_vars[k] = v
2276                     nw_in = outs[inp.owner.outputs.index(inp)]
2277                 else:
2278                     nw_in = inp
2279                     optimized_vars[inp] = inp
2280             node.inputs[idx] = nw_in
2281         results = node.outputs
2282         for opt in list_opt:
2283             ret = opt.transform(node)
2284             if ret is not False and ret is not None:
2285                 assert len(ret) == len(node.outputs), opt
2286                 for k, v in zip(node.outputs, ret):
2287                     optimized_vars[k] = v
2288                 results = ret
2289                 if ret[0].owner:
2290                     node = out.owner
2291                 else:
2292                     break
2293         return results, optimized_vars
2294     if out.owner:
2295         out_index = out.owner.outputs.index(out)
2296     else:
2297         out_index = 0
2298     final_outs, optimized_nodes = local_recursive_function(
2299         list_optimizations, out, {}, 0)
2300     return final_outs[out_index]
2301 def copy_stack_trace(from_var, to_var):
2302     """
2303     Copies the stack trace from one or more tensor variables to
2304     one or more tensor variables and returns the destination variables.
2305     Parameters
2306     ----------
2307     from_var
2308         Tensor variable or list of tensor variables to copy stack traces from.
2309     to_var
2310         Tensor variable or list of tensor variables to copy stack traces to.
2311     Notes
2312     -----
2313     The stacktrace is assumed to be of the form of a list of lists
2314     of tuples. Each tuple contains the filename, line number, function name
2315     and so on. Each list of tuples contains the truples belonging to a
2316     particular variable.
2317     """
2318     tr = []
2319     if type(from_var) is list:
2320         for v in from_var:
2321             tr += getattr(v.tag, 'trace', [])
2322     else:
2323         tr = getattr(from_var.tag, 'trace', [])
2324     if tr and isinstance(tr[0], tuple):
2325         tr = [tr]
2326     if type(to_var) is list:
2327         for v in to_var:
2328             v.tag.trace = getattr(v.tag, 'trace', []) + tr
2329     else:
2330         to_var.tag.trace = getattr(to_var.tag, 'trace', []) + tr
2331     return to_var
2332 @contextlib.contextmanager
2333 def inherit_stack_trace(from_var):
2334     """
2335     Contextmanager that copies the stack trace from one or more variable nodes to all
2336     variable nodes constructed in the body. new_nodes is the list of all the newly created
2337     variable nodes inside an optimization that is managed by graph.nodes_constructed().
2338     Parameters
2339     ----------
2340     from_var
2341         Variable node or a list of variable nodes to copy stack traces from.
2342     """
2343     with graph.nodes_constructed() as new_nodes:
2344         yield
2345     copy_stack_trace(from_var, new_nodes)
2346 def check_stack_trace(f_or_fgraph, ops_to_check='last', bug_print='raise'):
2347     """
2348     This function checks if the outputs of specific ops of a compiled graph
2349     have a stack.
2350     Parameters
2351     ----------
2352     f_or_fgraph: theano.compile.function_module.Function or
2353           theano.gof.fg.FunctionGraph
2354         The compiled function or the function graph to be analysed.
2355     ops_to_check: it can be of four different types:
2356           - classes or instances inheriting from theano.gof.Op
2357           - tuple/list of classes or instances inheriting from theano.gof.Op
2358           - string
2359           - function returning a boolean and taking as input an instance of
2360             theano.gof.Op.
2361         - if ops_to_check is a string, it should be either 'last' or 'all'.
2362           'last' will check only the last op of the graph while 'all' will
2363           check all the ops of the graph.
2364         - if ops_to_check is an op or a tuple/list of ops, the function will
2365           check that all the outputs of their occurrences in the graph have a
2366           stack trace.
2367         - if ops_to_check is a function, it should take as input a
2368           theano.gof.Op and return a boolean indicating if the input op should
2369           be checked or not.
2370     bug_print: string belonging to {'raise', 'warn', 'ignore'}
2371         You can specify the behaviour of the function when the specified
2372         ops_to_check are not in the graph of f_or_fgraph: it can either raise
2373         an exception, write a warning or simply ignore it.
2374     Returns
2375     -------
2376     boolean
2377         True if the outputs of the specified ops have a stack, False otherwise.
2378     """
2379     if isinstance(f_or_fgraph, theano.compile.function_module.Function):
2380         fgraph = f_or_fgraph.maker.fgraph
2381     elif isinstance(f_or_fgraph, theano.gof.fg.FunctionGraph):
2382         fgraph = f_or_fgraph
2383     else:
2384         raise ValueError('The type of f_or_fgraph is not supported')
2385     if (isinstance(ops_to_check, theano.gof.Op) or
2386             (inspect.isclass(ops_to_check) and
2387                 issubclass(ops_to_check, theano.gof.Op))):
2388         ops_to_check = (ops_to_check,)
2389     if isinstance(ops_to_check, string_types):
2390         if ops_to_check == 'last':
2391             apply_nodes_to_check = [fgraph.outputs[i].owner for i in range(
2392                 len(fgraph.outputs))]
2393         elif ops_to_check == 'all':
2394             apply_nodes_to_check = fgraph.apply_nodes
2395         else:
2396             raise ValueError('The string ops_to_check is not recognised')
2397     elif isinstance(ops_to_check, (tuple, list)):
2398         op_instances = []
2399         op_classes = []
2400         for obj in ops_to_check:
2401             if isinstance(obj, theano.gof.Op):
2402                 op_instances.append(obj)
2403             else:
2404                 op_classes.append(obj)
2405         op_classes = tuple(op_classes)
2406         apply_nodes_to_check = (
2407             [node for node in fgraph.apply_nodes if node.op in ops_to_check] +
2408             [node for node in fgraph.apply_nodes
2409              if isinstance(node.op, op_classes) or
2410              (hasattr(node.op, 'scalar_op') and
2411               isinstance(node.op.scalar_op, op_classes))])
2412     elif hasattr(ops_to_check, '__call__'):
2413         apply_nodes_to_check = [node for node in fgraph.apply_nodes
2414                                 if ops_to_check(node)]
2415     else:
2416         raise ValueError('ops_to_check does not have the right type')
2417     if not apply_nodes_to_check:
2418         msg = 'Provided op instances/classes are not in the graph or the ' \
2419               'graph is empty'
2420         if bug_print == 'warn':
2421             warnings.warn(msg)
2422         elif bug_print == 'raise':
2423             raise Exception(msg)
2424         elif bug_print == 'ignore':
2425             pass
2426         else:
2427             raise ValueError('The string bug_print is not recognised')
2428     for node in apply_nodes_to_check:
2429         for output in node.outputs:
2430             if (not hasattr(output.tag, 'trace') or not output.tag.trace):
2431                 return False
2432     return True
2433 class CheckStrackTraceFeature(object):
2434     def on_import(self, fgraph, node, reason):
2435         if theano.config.check_stack_trace != 'off' and not check_stack_trace(fgraph, 'all'):
2436             if theano.config.check_stack_trace == 'raise':
2437                     raise AssertionError(
2438                         'Empty stack trace! The optimization that inserted this variable is ' + str(reason))
2439             elif theano.config.check_stack_trace in ['log', 'warn']:
2440                 apply_nodes_to_check = fgraph.apply_nodes
2441                 for node in apply_nodes_to_check:
2442                     for output in node.outputs:
2443                         if not hasattr(output.tag, 'trace') or not output.tag.trace:
2444                             output.tag.trace = [[('', 0, 'Empty stack trace! The optimization that' +
2445                                                  'inserted this variable is ' + str(reason), '')]]
2446                 if theano.config.check_stack_trace == 'warn':
2447                         warnings.warn(
2448                             'Empty stack trace! The optimization that inserted this variable is' + str(reason))
2449 class CheckStackTraceOptimization(Optimizer):
2450     def add_requirements(self, fgraph):
2451         if not hasattr(fgraph, 'CheckStrackTraceFeature'):
2452             fgraph.attach_feature(CheckStrackTraceFeature())
2453     def apply(self, fgraph):
2454         pass
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
