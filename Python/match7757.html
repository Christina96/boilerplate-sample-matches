<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_subtensor.py &amp; opt_2.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_subtensor.py &amp; opt_2.py
      </h3>
<h1 align="center">
        1.3%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_subtensor.py (4.185022%)<th>opt_2.py (0.8235804%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(212-218)<td><a href="#" name="0">(2510-2514)</a><td align="center"><font color="#ff0000">13</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1-24)<td><a href="#" name="1">(6-21)</a><td align="center"><font color="#ff0000">13</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(109-114)<td><a href="#" name="2">(95-97)</a><td align="center"><font color="#eb0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_subtensor.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <a name="1"></a><font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>from __future__ import absolute_import, print_function, division
2 import numpy as np
3 import unittest
4 import theano
5 from theano import tensor
6 from theano.compile import DeepCopyOp
7 from theano.tensor.tests import test_subtensor, test_basic
8 from theano.tests import unittest_tools as utt
9 from ..basic_ops import HostFromGpu, GpuFromHost, GpuContiguous
10 from ..elemwise import GpuDimShuffle
11 from ..subtensor import (GpuIncSubtensor, GpuSubtensor,
12                          GpuAdvancedSubtensor1,
13                          GpuAdvancedSubtensor,
14                          GpuAdvancedBooleanSubtensor,
15                          GpuAdvancedIncSubtensor,
16                          GpuAdvancedIncSubtensor1,
17                          GpuAdvancedIncSubtensor1_dev20,
18                          GpuExtractDiag,
19                          GpuAllocDiag)
20 from ..type import gpuarray_shared_constructor
21 from</b></font> .config import mode_with_gpu, test_ctx_name
22 class G_subtensor(test_subtensor.T_subtensor):
23     def shortDescription(self):
24         return None
25     def __init__(self, name):
26         def shared(x, **kwargs):
27             return gpuarray_shared_constructor(x, target=test_ctx_name,
28                                                **kwargs)
29         test_subtensor.T_subtensor.__init__(
30             self, name,
31             shared=shared,
32             sub=GpuSubtensor,
33             inc_sub=GpuIncSubtensor,
34             adv_sub1=GpuAdvancedSubtensor1,
35             adv_incsub1=GpuAdvancedIncSubtensor1,
36             adv_sub=GpuAdvancedSubtensor,
37             adv_bool_sub=GpuAdvancedBooleanSubtensor,
38             dimshuffle=GpuDimShuffle,
39             mode=mode_with_gpu,
40             dtype='float32',
41             ignore_topo=(HostFromGpu, GpuFromHost,
42                          DeepCopyOp, GpuContiguous))
43         self.fast_compile = False
44         assert self.sub == GpuSubtensor
45 class G_subtensorF16(test_subtensor.T_subtensor):
46     def shortDescription(self):
47         return None
48     def __init__(self, name):
49         def shared(x, **kwargs):
50             return gpuarray_shared_constructor(x, target=test_ctx_name,
51                                                **kwargs)
52         test_subtensor.T_subtensor.__init__(
53             self, name,
54             shared=shared,
55             sub=GpuSubtensor,
56             inc_sub=GpuIncSubtensor,
57             adv_sub1=GpuAdvancedSubtensor1,
58             adv_incsub1=GpuAdvancedIncSubtensor1,
59             adv_sub=GpuAdvancedSubtensor,
60             adv_bool_sub=GpuAdvancedBooleanSubtensor,
61             dimshuffle=GpuDimShuffle,
62             mode=mode_with_gpu,
63             dtype='float16',  # use floatX?
64             ignore_topo=(HostFromGpu, GpuFromHost,
65                          DeepCopyOp, GpuContiguous))
66         self.fast_compile = False
67         assert self.sub == GpuSubtensor
68 def test_advinc_subtensor1():
69     for shp in [(3, 3), (3, 3, 3)]:
70         shared = gpuarray_shared_constructor
71         xval = np.arange(np.prod(shp), dtype='float32').reshape(shp) + 1
72         yval = np.empty((2,) + shp[1:], dtype='float32')
73         yval[:] = 10
74         x = shared(xval, name='x')
75         y = tensor.tensor(dtype='float32',
76                           broadcastable=(False,) * len(shp),
77                           name='y')
78         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
79         f = theano.function([y], expr, mode=mode_with_gpu)
80         assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1)
81                     for node in f.maker.fgraph.toposort()]) == 1
82         rval = f(yval)
83         rep = xval.copy()
84         np.add.at(rep, [0, 2], yval)
85         assert np.allclose(rval, rep)
86 <a name="2"></a>def test_advinc_subtensor1_dtype():
87     shp = (3, 4)
88     for dtype1, dtype2 in [<font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>('float32', 'int8'), ('float32', 'float64'),
89                            ('uint64', 'int8'), ('int64', 'uint8'),
90                            ('float16', 'int8'), ('float16', 'float64'),
91                            ('float16', 'float16')]:
92         shared = gpuarray_shared_constructor
93         xval = np.arange(</b></font>np.prod(shp), dtype=dtype1).reshape(shp) + 1
94         yval = np.empty((2,) + shp[1:], dtype=dtype2)
95         yval[:] = 10
96         x = shared(xval, name='x')
97         y = tensor.tensor(dtype=yval.dtype,
98                           broadcastable=(False,) * len(yval.shape),
99                           name='y')
100         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
101         f = theano.function([y], expr, mode=mode_with_gpu)
102         assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1_dev20)
103                     for node in f.maker.fgraph.toposort()]) == 1
104         rval = f(yval)
105         rep = xval.copy()
106         np.add.at(rep, [[0, 2]], yval)
107         assert np.allclose(rval, rep)
108 @theano.change_flags(deterministic='more')
109 def test_deterministic_flag():
110     shp = (3, 4)
111     for dtype1, dtype2 in [('float32', 'int8')]:
112         shared = gpuarray_shared_constructor
113         xval = np.arange(np.prod(shp), dtype=dtype1).reshape(shp) + 1
114         yval = np.empty((2,) + shp[1:], dtype=dtype2)
115         yval[:] = 10
116         x = shared(xval, name='x')
117         y = tensor.tensor(dtype=yval.dtype,
118                           broadcastable=(False,) * len(yval.shape),
119                           name='y')
120         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
121         f = theano.function([y], expr, mode=mode_with_gpu)
122         assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1)
123                     for node in f.maker.fgraph.toposort()]) == 1
124         rval = f(yval)
125         rep = xval.copy()
126         np.add.at(rep, [[0, 2]], yval)
127         assert np.allclose(rval, rep)
128 def test_advinc_subtensor1_vector_scalar():
129     shp = (3,)
130     for dtype1, dtype2 in [('float32', 'int8'), ('float32', 'float64'),
131                            ('float16', 'int8'), ('float16', 'float64'),
132                            ('float16', 'float16'), ('int8', 'int8'),
133                            ('int16', 'int16')]:
134         shared = gpuarray_shared_constructor
135         xval = np.arange(np.prod(shp), dtype=dtype1).reshape(shp) + 1
136         yval = np.asarray(10, dtype=dtype2)
137         x = shared(xval, name='x')
138         y = tensor.tensor(dtype=yval.dtype,
139                           broadcastable=(False,) * len(yval.shape),
140                           name='y')
141         expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
142         f = theano.function([y], expr, mode=mode_with_gpu)
143         assert sum([isinstance(node.op, (GpuAdvancedIncSubtensor1_dev20,
144                                          GpuAdvancedIncSubtensor1))
145                     for node in f.maker.fgraph.toposort()]) == 1
146         rval = f(yval)
147         rep = xval.copy()
148         rep[[0, 2]] += yval
149         assert np.allclose(rval, rep)
150 def test_incsub_f16():
151     shp = (3, 3)
152     shared = gpuarray_shared_constructor
153     xval = np.arange(np.prod(shp), dtype='float16').reshape(shp) + 1
154     yval = np.empty((2,) + shp[1:], dtype='float16')
155     yval[:] = 2
156     x = shared(xval, name='x')
157     y = tensor.tensor(dtype='float16',
158                       broadcastable=(False,) * len(shp),
159                       name='y')
160     expr = tensor.advanced_inc_subtensor1(x, y, [0, 2])
161     f = theano.function([y], expr, mode=mode_with_gpu)
162     assert sum([isinstance(node.op, GpuAdvancedIncSubtensor1)
163                 for node in f.maker.fgraph.toposort()]) == 1
164     rval = f(yval)
165     rep = xval.copy()
166     np.add.at(rep, [[0, 2]], yval)
167     assert np.allclose(rval, rep)
168     expr = tensor.inc_subtensor(x[1:], y)
169     f = theano.function([y], expr, mode=mode_with_gpu)
170     assert sum([isinstance(node.op, GpuIncSubtensor)
171                 for node in f.maker.fgraph.toposort()]) == 1
172     rval = f(yval)
173     rep = xval.copy()
174     rep[1:] += yval
175     assert np.allclose(rval, rep)
176 def test_incsub_offset():
177     x = gpuarray_shared_constructor(np.zeros(5, dtype=theano.config<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.floatX))
178     x1 = x[1:]
179     y = tensor.vector()
180     z = tensor.inc_subtensor(x1[2:], y)
181     f = theano.function(</b></font>[y], z, updates={x: z}, mode=mode_with_gpu)
182     utt.assert_allclose(
183         f([1, 2]),
184         np.array([0, 0, 1, 2], dtype=theano.config.floatX))
185 class G_advancedsubtensor(test_subtensor.TestAdvancedSubtensor):
186     def shortDescription(self):
187         return None
188     def __init__(self, name):
189         test_subtensor.TestAdvancedSubtensor.__init__(
190             self, name,
191             shared=gpuarray_shared_constructor,
192             sub=GpuAdvancedSubtensor,
193             inc_sub=GpuAdvancedIncSubtensor,
194             mode=mode_with_gpu,
195             dtype='float32',  # floatX?
196             ignore_topo=(HostFromGpu, GpuFromHost,
197                          DeepCopyOp))
198         self.fast_compile = False
199         assert self.sub == GpuAdvancedSubtensor
200 class G_advancedsubtensorF16(test_subtensor.TestAdvancedSubtensor):
201     def shortDescription(self):
202         return None
203     def __init__(self, name):
204         test_subtensor.TestAdvancedSubtensor.__init__(
205             self, name,
206             shared=gpuarray_shared_constructor,
207             sub=GpuAdvancedSubtensor,
208             mode=mode_with_gpu,
209             dtype='float16',  # floatX?
210             ignore_topo=(HostFromGpu, GpuFromHost,
211                          DeepCopyOp))
212         self.fast_compile = False
213         assert self.sub == GpuAdvancedSubtensor
214 def test_adv_subtensor():
215     shp = (2, 3, 4)
216     shared = gpuarray_shared_constructor
217     xval = np.arange(np.prod(shp), dtype=theano.config.floatX).reshape(shp)
218     idx1, idx2 = tensor.ivectors('idx1', 'idx2')
219     idxs = [idx1, None, slice(0, 2, 1), idx2, None]
220     x = shared(xval, name='x')
221     expr = x[idxs]
222     f = theano.function([idx1, idx2], expr, mode=mode_with_gpu)
223     assert sum([isinstance(node.op, GpuAdvancedSubtensor)
224                for node in f.maker.fgraph.toposort()]) == 1
225     idx1_val = [0, 1]
226     idx2_val = [0, 1]
227     rval = f(idx1_val, idx2_val)
228     rep = xval[idx1_val, None, slice(0, 2, 1), idx2_val, None]
229     assert np.allclose(rval, rep)
230 class test_gpuextractdiag(unittest.TestCase):
231     def test_extractdiag_opt(self):
232         x = tensor.matrix()
233         fn = theano.function([x], tensor.ExtractDiag()(x), mode=mode_with_gpu)
234         assert any([isinstance(node.op, GpuExtractDiag)
235                     for node in fn.maker.fgraph.toposort()])
236     def test_matrix(self):
237         x = tensor.matrix()
238         np_x = np.arange(77).reshape(7, 11).astype(theano.config.floatX)
239         fn = theano.function([x], GpuExtractDiag()(x), mode=mode_with_gpu)
240         assert np.allclose(fn(np_x), np_x.diagonal())
241         fn = theano.function([x], GpuExtractDiag(2)(x), mode=mode_with_gpu)
242         assert np.allclose(fn(np_x), np_x.diagonal(2))
243         fn = theano.function([x], GpuExtractDiag(-3)(x), mode=mode_with_gpu)
244         assert np.allclose(fn(np_x), np_x.diagonal(-3))
245     def test_tensor(self):
246         x = tensor.tensor4()
247         np_x = np.arange(30107).reshape(7, 11, 17, 23).astype(theano.config.floatX)
248         for offset, axis1, axis2 in [
249                 (1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0),
250                 (-3, 1, 0), (-2, 2, 0), (3, 3, 0), (-1, 3, 2),
251                 (2, 2, 3), (-1, 2, 1), (1, 3, 1), (-1, 1, 3)]:
252             assert np.allclose(
253                 GpuExtractDiag(offset, axis1, axis2)(x).eval({x: np_x}),
254                 np_x.diagonal(offset, axis1, axis2))
255     def test_tensor_float16(self):
256         x = tensor.tensor4()
257         np_x = np.arange(30107).reshape(7, 11, 17, 23).astype('float16')
258         for offset, axis1, axis2 in [
259                 (1, 0, 1), (-1, 0, 1), (0, 1, 0), (-2, 1, 0),
260                 (-3, 1, 0), (-2, 2, 0), (3, 3, 0), (-1, 3, 2),
261                 (2, 2, 3), (-1, 2, 1), (1, 3, 1), (-1, 1, 3)]:
262             assert np.allclose(
263                 GpuExtractDiag(offset, axis1, axis2)(x).eval({x: np_x}),
264                 np_x.diagonal(offset, axis1, axis2))
265 class TestGpuAllocDiag(test_basic.TestAllocDiag):
266     def __init__(self, name):
267         test_basic.TestAllocDiag.__init__(
268             self, name,
269             alloc_diag=GpuAllocDiag,
270             mode=mode_with_gpu
271         )
272 class test_gpuallocdiag(unittest.TestCase):
273     def test_allocdiag_opt(self):
274         x = tensor.vector()
275         fn = theano.function([x], tensor.AllocDiag()(x), mode=mode_with_gpu)
276         assert any([isinstance(node.op, GpuAllocDiag)
277                     for node in fn.maker.fgraph.toposort()])
278     def test_matrix(self):
279         x = tensor.vector()
280         np_x = np.arange(7).astype(theano.config.floatX)
281         fn = theano.function([x], GpuAllocDiag()(x), mode=mode_with_gpu)
282         assert np.allclose(fn(np_x), np.diag(np_x))
283         fn = theano.function([x], GpuAllocDiag(2)(x), mode=mode_with_gpu)
284         assert np.allclose(fn(np_x), np.diag(np_x, 2))
285         fn = theano.function([x], GpuAllocDiag(-3)(x), mode=mode_with_gpu)
286         assert np.allclose(fn(np_x), np.diag(np_x, -3))
287     def test_grad(self):
288         x = tensor.vector()
289         np_x = np.random.randn(7).astype(theano.config.floatX)
290         mtx_x = GpuAllocDiag()(x)
291         sum_mtx_x = tensor.sum(mtx_x)
292         grad_x = tensor.grad(sum_mtx_x, x)
293         grad_mtx_x = tensor.grad(sum_mtx_x, mtx_x)
294         fn_grad_x = theano.function([x], grad_x, mode=mode_with_gpu)
295         fn_grad_mtx_x = theano.function([x], grad_mtx_x, mode=mode_with_gpu)
296         computed_grad_x = fn_grad_x(np_x)
297         computed_grad_mtx_x = fn_grad_mtx_x(np_x)
298         true_grad_x = np.diagonal(computed_grad_mtx_x, 0)
299         assert np.allclose(computed_grad_x, true_grad_x)
300         mtx_x = GpuAllocDiag(2)(x)
301         sum_mtx_x = tensor.sum(mtx_x)
302         grad_x = tensor.grad(sum_mtx_x, x)
303         grad_mtx_x = tensor.grad(sum_mtx_x, mtx_x)
304         fn_grad_x = theano.function([x], grad_x, mode=mode_with_gpu)
305         fn_grad_mtx_x = theano.function([x], grad_mtx_x, mode=mode_with_gpu)
306         computed_grad_x = fn_grad_x(np_x)
307         computed_grad_mtx_x = fn_grad_mtx_x(np_x)
308         true_grad_x = np.diagonal(computed_grad_mtx_x, 2)
309         assert np.allclose(computed_grad_x, true_grad_x)
310         mtx_x = GpuAllocDiag(-3)(x)
311         sum_mtx_x = tensor.sum(mtx_x)
312         grad_x = tensor.grad(sum_mtx_x, x)
313         grad_mtx_x = tensor.grad(sum_mtx_x, mtx_x)
314         fn_grad_x = theano.function([x], grad_x, mode=mode_with_gpu)
315         fn_grad_mtx_x = theano.function([x], grad_mtx_x, mode=mode_with_gpu)
316         computed_grad_x = fn_grad_x(np_x)
317         computed_grad_mtx_x = fn_grad_mtx_x(np_x)
318         true_grad_x = np.diagonal(computed_grad_mtx_x, -3)
319         assert np.allclose(computed_grad_x, true_grad_x)
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>opt_2.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>from __future__ import absolute_import, print_function, division
2 from collections import deque, defaultdict, OrderedDict
3 import contextlib
4 import copy
5 import inspect
6 import logging
7 import pdb
8 import sys
9 import time
10 import warnings
11 import traceback
12 import numpy as np
13 import</b></font> theano
14 from theano import config
15 from theano.compat import izip
16 from six import string_types, iteritems, itervalues, integer_types
17 from six.moves import reduce
18 from theano.gof import graph, op, utils, unify, toolbox
19 from theano.gof.fg import InconsistencyError
20 from theano.misc.ordered_set import OrderedSet
21 from . import destroyhandler as dh
22 _logger = logging.getLogger('theano.gof.opt')
23 _optimizer_idx = [0]
24 def _list_of_nodes(fgraph):
25     return list(graph.io_toposort(fgraph.inputs, fgraph.outputs))
26 class LocalMetaOptimizerSkipAssertionError(AssertionError):
27     pass
28 class Optimizer(object):
29     def __hash__(self):
30         if not hasattr(self, '_optimizer_idx'):
31             self._optimizer_idx = _optimizer_idx[0]
32             _optimizer_idx[0] += 1
33         return self._optimizer_idx
34     def __eq__(self, other):
35         return id(self) == id(other)
36     def __ne__(self, other):
37         return id(self) != id(other)
38     def apply(self, fgraph):
39         pass
40     def optimize(self, fgraph, *args, **kwargs):
41             orig = theano<font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.tensor.basic.constant.enable
42             theano.tensor.basic.constant.enable = False
43             ret = self.apply(</b></font>fgraph, *args, **kwargs)
44         finally:
45             theano.tensor.basic.constant.enable = orig
46         return ret
47     def __call__(self, fgraph):
48         return self.optimize(fgraph)
49     def add_requirements(self, fgraph):
50         pass
51     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
52         name = getattr(self, 'name', None)
53         print("%s%s %s id=%i" % (
54             (' ' * level), self.__class__.__name__, name, id(self)), file=stream)
55     @staticmethod
56     def print_profile(stream, prof, level=0):
57         if prof is not None:
58             raise NotImplementedError(
59                 "The function print_profile must be overrided if the"
60                 " optimizer return profiling information.")
61 class FromFunctionOptimizer(Optimizer):
62     def __init__(self, fn, requirements=()):
63         self.apply = fn
64         self.requirements = requirements
65     def add_requirements(self, fgraph):
66         for req in self.requirements:
67             req(fgraph)
68     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
69         print("%s%s id=%i" % (
70             ' ' * level,
71             str(self.apply),
72             id(self)), file=stream)
73     def __call__(self, *args, **kwargs):
74         return self.fn(*args, **kwargs)
75     def __str__(self):
76         return self.__name__
77 def optimizer(f):
78     rval = FromFunctionOptimizer(f)
79     rval.__name__ = f.__name__
80     return rval
81 def inplace_optimizer(f):
82     dh_handler = dh.DestroyHandler
83     requirements = (lambda fgraph:
84                     fgraph.attach_feature(dh_handler()),)
85     rval = FromFunctionOptimizer(f, requirements)
86     rval.__name__ = f.__name__
87     return rval
88 class SeqOptimizer(Optimizer, list):
89     @staticmethod
90     def warn(exc, self, optimizer):
91         _logger.error("SeqOptimizer apply %s" % str(optimizer))
92         _logger.error("Traceback:")
93         _logger.error(traceback.format_exc())
94         if config.on_opt_error == 'raise':
95             raise exc
96         elif config.on_opt_error == 'pdb':
97             pdb.post_mortem(sys.exc_info()[2])
98     def __init__(self, *opts, **kw):
99         if len(opts) == 1 and isinstance(opts[0], (list, tuple)):
100             opts = opts[0]
101         self[:] = opts
102         self.failure_callback = kw.pop('failure_callback', None)
103         assert len(kw) == 0
104     def apply(self, fgraph):
105         l = []
106         if fgraph.profile:
107             validate_before = fgraph.profile.validate_time
108             sub_validate_time = [validate_before]
109             callbacks_before = fgraph.execute_callbacks_times.copy()
110         else:
111             sub_validate_time = []
112             callbacks_before = []
113         callback_before = fgraph.execute_callbacks_time
114         nb_node_before = len(fgraph.apply_nodes)
115         sub_profs = []
116         nb_nodes = []
117         self.pre_profile = (
118             self, l, -1, -1, nb_node_before,
119             -1, sub_profs, sub_validate_time,
120             nb_nodes, {})
121         try:
122             for optimizer in self:
123                 try:
124                     nb_nodes_before = len(fgraph.apply_nodes)
125                     t0 = time.time()
126                     sub_prof = optimizer.optimize(fgraph)
127                     l.append(float(time.time() - t0))
128                     sub_profs.append(sub_prof)
129                     nb_nodes.append((nb_nodes_before,
130                                      len(fgraph.apply_nodes)))
131                     if fgraph.profile:
132                         sub_validate_time.append(fgraph.profile.validate_time)
133                 except AssertionError:
134                     raise
135                 except Exception as e:
136                     if self.failure_callback:
137                         self.failure_callback(e, self, optimizer)
138                         continue
139                     else:
140                         raise
141         finally:
142             if fgraph.profile:
143                 validate_time = fgraph.profile.validate_time - validate_before
144                 callbacks_time = {}
145                 for k, v in iteritems(fgraph.execute_callbacks_times):
146                     if k in callbacks_before:
147                         t = v - callbacks_before[k]
148                         if t &gt; 0:
149                             callbacks_time[k] = t
150                     else:
151                         callbacks_time[k] = v
152             else:
153                 validate_time = None
154                 callbacks_time = {}
155             callback_time = fgraph.execute_callbacks_time - callback_before
156             self.pre_profile = (
157                 self, l, validate_time, callback_time, nb_node_before,
158                 len(fgraph.apply_nodes), sub_profs, sub_validate_time,
159                 nb_nodes, callbacks_time)
160         return self.pre_profile
161     def __str__(self):
162         return "SeqOpt(%s)" % list.__str__(self)
163     def __repr__(self):
164         return list.__repr__(self)
165     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
166         name = getattr(self, 'name', None)
167         print("%s%s %s id=%i" % (
168             (' ' * level), self.__class__.__name__, name, id(self)), file=stream)
169         if depth != 0:
170             depth -= 1
171             for opt in self:
172                 opt.print_summary(stream, level=(level + 2), depth=depth)
173     @staticmethod
174     def print_profile(stream, prof, level=0):
175         (opts, prof, validate_time, callback_time,
176          nb_node_before, nb_node_after, sub_profs, sub_validate_time,
177          nb_nodes, callbacks_time) = prof
178         blanc = ('    ' * level)
179         print(blanc, "SeqOptimizer", end=' ', file=stream)
180         if hasattr(opts, "name"):
181             print(blanc, opts.name, end=' ', file=stream)
182         elif hasattr(opts, "__name__"):
183             print(blanc, opts.__name__, end=' ', file=stream)
184         print((" time %.3fs for %d/%d nodes"
185                " before/after optimization" % (
186                    sum(prof), nb_node_before, nb_node_after)), file=stream)
187         print(blanc, "  %.3fs for callback" % (callback_time), file=stream)
188         print(blanc, "      %.3fs for fgraph.validate()" % (validate_time),
189               file=stream)
190         if callback_time &gt; 1:
191             print(blanc, "  callbacks_time", file=stream)
192             for i in sorted(iteritems(callbacks_time), key=lambda a: -a[1]):
193                 if i[1] &gt; 0:
194                     print(blanc, "      ", i[0], ',', i[1], file=stream)
195         if level == 0:
196             print(blanc,
197                   "  time      - (name, class, index, nodes before, nodes after) - validate time",
198                   file=stream)
199         ll = []
200         for (opt, nb_n) in zip(opts, nb_nodes):
201             if hasattr(opt, "__name__"):
202                 name = opt.__name__
203             else:
204                 name = opt.name
205             idx = opts.index(opt)
206             ll.append((name, opt.__class__.__name__,
207                        idx) + nb_n)
208         lll = sorted(zip(prof, ll), key=lambda a: a[0])
209         for (t, opt) in lll[::-1]:
210             i = opt[2]
211             if sub_validate_time:
212                 val_time = sub_validate_time[i + 1] - sub_validate_time[i]
213                 print(blanc, '  %.6fs - %s - %.3fs' % (
214                     t, opt, val_time), file=stream)
215             else:
216                 print(blanc, '  %.6fs - %s' % (t, opt), file=stream)
217             if sub_profs[i]:
218                 opts[i].print_profile(stream, sub_profs[i],
219                                       level=level + 1)
220         print(file=stream)
221     @staticmethod
222     def merge_profile(prof1, prof2):
223         new_t = []  # the time for the optimization
224         new_l = []  # the optimization
225         new_sub_profile = []
226         for l in set(prof1[0]).intersection(set(prof2[0])):
227             idx1 = prof1[0].index(l)
228             idx2 = prof2[0].index(l)
229             new_t.append(prof1[1][idx1] +
230                          prof2[1][idx2])
231             new_l.append(l)
232             if hasattr(l, 'merge_profile'):
233                 assert len(prof1[6][idx1]) == len(prof2[6][idx2])
234                 new_sub_profile.append(l.merge_profile(prof1[6][idx1],
235                                                        prof2[6][idx2]))
236             else:
237                 new_sub_profile.append(None)
238         from six import StringIO
239         for l in set(prof1[0]).symmetric_difference(set(prof2[0])):
240             new_l_names = [o.name for o in new_l]
241             if l.name in new_l_names:
242                 idx = new_l_names.index(l.name)
243                 io1 = StringIO()
244                 io2 = StringIO()
245                 l.print_summary(io1)
246                 new_l[idx].print_summary(io2)
247                 if io1.read() == io2.read():
248                     if l in prof1[0]:
249                         p = prof1
250                     else:
251                         p = prof2
252                     new_t[idx] += p[1][p[0].index(l)]
253                     if hasattr(l, 'merge_profile'):
254                         assert len(p[6][p[0].index(l)]) == \
255                             len(new_sub_profile[idx])
256                         new_sub_profile[idx] = l.merge_profile(
257                             new_sub_profile[idx], p[6][p[0].index(l)])
258                     else:
259                         new_sub_profile[idx] = None
260                 continue
261             if l in prof1[0]:
262                 p = prof1
263             else:
264                 p = prof2
265             new_t.append(p[1][p[0].index(l)])
266             idx = p[0].index(l)
267             new_l.append(l)
268             new_sub_profile.append(p[6][idx])
269         new_opt = SeqOptimizer(*new_l)
270         new_nb_nodes = []
271         for p1, p2 in zip(prof1[8], prof2[8]):
272             new_nb_nodes.append((p1[0] + p2[0], p1[1] + p2[1]))
273         new_nb_nodes.extend(prof1[8][len(new_nb_nodes):])
274         new_nb_nodes.extend(prof2[8][len(new_nb_nodes):])
275         new_callbacks_times = merge_dict(prof1[9], prof2[9])
276         assert set([l.name for l in prof1[0]]).issubset(
277             set([l.name for l in new_l]))
278         assert set([l.name for l in prof2[0]]).issubset(
279             set([l.name for l in new_l]))
280         assert len(new_t) == len(new_opt) == len(new_sub_profile)
281         return (new_opt, new_t, prof1[2] + prof2[2],
282                 prof1[3] + prof2[3],
283                 -1, -1, new_sub_profile, [],
284                 new_nb_nodes,
285                 new_callbacks_times)
286 class _metadict:
287     def __init__(self):
288         self.d = {}
289         self.l = []
290     def __getitem__(self, item):
291         return self.get(item, None)
292     def __setitem__(self, item, value):
293         try:
294             self.d[item] = value
295         except Exception:
296             for i, (key, val) in enumerate(self.l):
297                 if key == item:
298                     self.l[i] = (item, value)
299                     return
300             self.l.append((item, value))
301     def __delitem__(self, item):
302         try:
303             if item in self.d:
304                 del self.d[item]
305                 return
306         except TypeError as e:
307             assert "unhashable type" in str(e)
308         for i, (key, val) in enumerate(self.l):
309             if key == item:
310                 del self.l[i]
311                 return
312             raise KeyError(item)
313     def discard(self, item):
314         try:
315             if item in self.d:
316                 del self.d[item]
317                 return
318         except TypeError as e:
319             assert "unhashable type" in str(e)
320         for i, (key, val) in enumerate(self.l):
321             if key == item:
322                 del self.l[i]
323                 return
324     def get(self, item, default):
325         try:
326             return self.d[item]
327         except Exception:
328             for item2, value in self.l:
329                 try:
330                     if item == item2:
331                         return value
332                     if item.equals(item2):
333                         return value
334                 except Exception:
335                     if item is item2:
336                         return value
337             return default
338     def clear(self):
339         self.d = {}
340         self.l = []
341     def __str__(self):
342         return "(%s, %s)" % (self.d, self.l)
343 class MergeFeature(object):
344     def on_attach(self, fgraph):
345         assert not hasattr(fgraph, 'merge_feature')
346         fgraph.merge_feature = self
347         self.seen_constants = set()
348         self.const_sig = _metadict()
349         self.const_sig_inv = _metadict()
350         self.nodes_seen = set()
351         self.noinput_nodes = OrderedSet()
352         self.scheduled = []
353         self.blacklist = []
354         for node in fgraph.toposort():
355             self.on_import(fgraph, node, "on_attach")
356     def on_change_input(self, fgraph, node, i, r, new_r, reason):
357         if node in self.nodes_seen:
358             self.nodes_seen.discard(node)
359             self.process_node(fgraph, node)
360         if not isinstance(node, string_types):
361             assert node.inputs
362         if isinstance(new_r, graph.Constant):
363             self.process_constant(fgraph, new_r)
364     def on_import(self, fgraph, node, reason):
365         for c in node.inputs:
366             if isinstance(c, graph.Constant):
367                 self.process_constant(fgraph, c)
368         self.process_node(fgraph, node)
369     def on_prune(self, fgraph, node, reason):
370         self.nodes_seen.discard(node)
371         if not node.inputs:
372             self.noinput_nodes.discard(node)
373         for c in node.inputs:
374             if isinstance(c, graph.Constant) and (len(c.clients) &lt;= 1):
375                 sig = self.const_sig[c]
376                 self.const_sig.discard(c)
377                 self.const_sig_inv.discard(sig)
378                 self.seen_constants.discard(id(c))
379     def process_constant(self, fgraph, c):
380         if id(c) in self.seen_constants:
381             return
382         sig = c.merge_signature()
383         other_c = self.const_sig_inv.get(sig, None)
384         if other_c is not None:
385             if c.name:
386                 other_c.name = c.name
387             self.scheduled.append([[(c, other_c, 'merge')]])
388         else:
389             self.const_sig[c] = sig
390             self.const_sig_inv[sig] = c
391             self.seen_constants.add(id(c))
392     def process_node(self, fgraph, node):
393         if node in self.nodes_seen:
394             return
395         node_has_assert = False
396         if node.inputs:
397             if len(node.inputs[0].clients) &lt; len(node.inputs[-1].clients):
398                 clients = node.inputs[0].clients
399             else:
400                 clients = node.inputs[-1].clients
401             assert len(clients) &gt; 0
402             merge_candidates = [c for c, i in clients if c in self.nodes_seen]
403             for i in []:  # node.inputs:
404                 if i.owner and isinstance(i.owner.op,
405                                           theano.tensor.opt.Assert):
406                     node_has_assert = True
407                     assert_clients = [c for (c, _) in i.owner.inputs[0].clients
408                                       if c in self.nodes_seen]
409                     for idx in range(len(assert_clients)):
410                         client = assert_clients[idx]
411                         if isinstance(i.owner.op, theano.tensor.opt.Assert):
412                             for c in client.outputs[0].clients:
413                                 if c[0] in self.nodes_seen:
414                                     assert_clients.append(c[0])
415                     merge_candidates.extend(assert_clients)
416         else:
417             merge_candidates = self.noinput_nodes
418         replacement_candidates = []
419         for candidate in merge_candidates:
420             if candidate is node:
421                 continue
422             if len(node.inputs) != len(candidate.inputs):
423                 continue
424             cand_has_assert = False
425             cand_inputs_assert_removed = []
426             for i in []:  # candidate.inputs:
427                 if i.owner and isinstance(i.owner.op,
428                                           theano.tensor.opt.Assert):
429                     cand_has_assert = True
430                     cand_inputs_assert_removed.append(i.owner.inputs[0])
431                 else:
432                     cand_inputs_assert_removed.append(i)
433             cand_inputs_assert_removed = candidate.inputs
434             if node_has_assert:
435                 node_inputs_assert_removed = []
436                 for i in node.inputs:
437                     if i.owner and isinstance(i.owner.op,
438                                               theano.tensor.opt.Assert):
439                         node_inputs_assert_removed.append(i.owner.inputs[0])
440                     else:
441                         node_inputs_assert_removed.append(i)
442             else:
443                 node_inputs_assert_removed = node.inputs
444             inputs_match = all(node_in is cand_in
445                                for node_in, cand_in
446                                in zip(node_inputs_assert_removed,
447                                       cand_inputs_assert_removed))
448             if inputs_match and node.op == candidate.op:
449                 if (node, candidate) in self.blacklist:
450                     continue
451                 if not (node_has_assert or cand_has_assert):
452                     pairs = list(zip(node.outputs,
453                                      candidate.outputs,
454                                      ['merge'] * len(node.outputs)))
455                 elif node_has_assert and not cand_has_assert:
456                     pairs = list(zip(candidate.outputs,
457                                      node.outputs,
458                                      ['merge'] * len(node.outputs)))
459                 else:
460                     new_inputs = self.get_merged_assert_input(node, candidate)
461                     new_node = node.op(*new_inputs)
462                     pairs = list(zip(node.outputs,
463                                      new_node.owner.outputs,
464                                      ['new_node'] * len(node.outputs))) +\
465                         list(zip(candidate.outputs,
466                                  new_node.owner.outputs,
467                                  ['new_node'] * len(node.outputs)))
468                 for pair in pairs:
469                     node_output, cand_output = pair[:2]
470                     if node_output.name:
471                         cand_output.name = node_output.name
472                 replacement_candidates.append(pairs)
473         if replacement_candidates:
474             self.scheduled.append(replacement_candidates)
475         else:
476             self.nodes_seen.add(node)
477             if not node.inputs:
478                 self.noinput_nodes.add(node)
479     def get_merged_assert_input(self, node, candidate):
480         new_inputs = []
481         for node_i, cand_i in zip(node.inputs, candidate.inputs):
482             if (node_i.owner and
483                     isinstance(node_i.owner.op,
484                                theano.tensor.opt.Assert)):
485                 if (cand_i.owner and
486                         isinstance(cand_i.owner.op,
487                                    theano.tensor.opt.Assert)):
488                     node_cond = node_i.owner.inputs[1:]
489                     cand_cond = cand_i.owner.inputs[1:]
490                     new_cond = list(set(node_cond + cand_cond))
491                     new_inputs.append(
492                         theano.tensor.opt.assert_op(
493                             node_i.owner.inputs[0],
494                             *new_cond))
495                 else:
496                     new_inputs.append(node_i)
497             else:
498                 new_inputs.append(cand_i)
499         return new_inputs
500 class MergeOptimizer(Optimizer):
501     def add_requirements(self, fgraph):
502         if not hasattr(fgraph, 'merge_feature'):
503             fgraph.attach_feature(MergeFeature())
504     def apply(self, fgraph):
505         sched = fgraph.merge_feature.scheduled
506         nb_fail = 0
507         t0 = time.time()
508         if fgraph.profile:
509             validate_before = fgraph.profile.validate_time
510             callback_before = fgraph.execute_callbacks_time
511             callbacks_before = fgraph.execute_callbacks_times.copy()
512         nb_merged = 0
513         nb_constant = 0
514         while sched:
515             pairs_list = sched.pop()
516             success = True
517             for pairs_ in pairs_list:
518                 var, candidate, merge_mode = pairs_[0]
519                 if merge_mode == "new_node" and hasattr(var, 'fgraph'):
520                     pass
521                 elif (not hasattr(var, 'fgraph') or
522                       not hasattr(candidate, 'fgraph')):
523                     continue
524                 pairs = [pair[:2] for pair in pairs_]
525                 if var.owner and candidate.owner:
526                     node = var.owner
527                     candidate = candidate.owner
528                     cand_inputs_assert_removed = []
529                     for i in candidate.inputs:
530                         if i.owner and isinstance(i.owner.op,
531                                                   theano.tensor.opt.Assert):
532                             cand_inputs_assert_removed.append(
533                                 i.owner.inputs[0])
534                         else:
535                             cand_inputs_assert_removed.append(i)
536                     node_inputs_assert_removed = []
537                     for i in node.inputs:
538                         if i.owner and isinstance(i.owner.op,
539                                                   theano.tensor.opt.Assert):
540                             node_inputs_assert_removed.append(
541                                 i.owner.inputs[0])
542                         else:
543                             node_inputs_assert_removed.append(i)
544                     if merge_mode == "new_node":
545                         inputs_match = True
546                     else:
547                         inputs_match = all(node_in is cand_in
548                                            for node_in, cand_in in
549                                            zip(node_inputs_assert_removed,
550                                                cand_inputs_assert_removed))
551                     if not inputs_match:
552                         continue
553                     if hasattr(pairs[0][0].fgraph, 'destroy_handler'):
554                         clients = pairs[0][0].clients + pairs[0][1].clients
555                         if sum([i in utils.flatten(c.op.destroy_map.values())
556                                 for c, i in clients
557                                 if c != 'output' and
558                                 hasattr(c.op, 'destroy_map')]) &gt; 1:
559                             continue
560                 if len(pairs) == 1 and pairs[0][0].type != pairs[0][1].type:
561                     res = pairs[0][0].type.convert_variable(pairs[0][1])
562                     if not res:
563                         pairs = [(pairs[0][1], pairs[0][0])]
564                 try:
565                     if all([isinstance(old, graph.Constant) for old, new in pairs]):
566                         fgraph.replace_all(pairs, 'MergeOptimizer')
567                     else:
568                         fgraph.replace_all_validate(pairs, 'MergeOptimizer')
569                 except InconsistencyError:
570                     success = False
571                     nb_fail += 1
572                     fgraph.merge_feature.blacklist.append(
573                         (pairs[0][0].owner, pairs[0][1].owner))
574                 if success:
575                     nb_merged += len(pairs)
576                     if isinstance(pairs[0][0], graph.Constant):
577                         nb_constant += 1
578                     break
579         if fgraph.profile:
580             validate_time = fgraph.profile.validate_time - validate_before
581             callback_time = fgraph.execute_callbacks_time - callback_before
582             callbacks_time = {}
583             for k, v in iteritems(fgraph.execute_callbacks_times):
584                 if k in callbacks_before:
585                     t = v - callbacks_before[k]
586                     if t &gt; 0:
587                         callbacks_time[k] = t
588                 else:
589                     callbacks_time[k] = v
590         else:
591             validate_time = None
592             callback_time = None
593             callbacks_time = {}
594         fgraph.merge_feature.blacklist = []
595         return (nb_fail, time.time() - t0, validate_time,
596                 callback_time, callbacks_time, nb_merged, nb_constant)
597     def __str__(self):
598         return self.__class__.__name__
599     @staticmethod
600     def print_profile(stream, prof, level=0):
601         (nb_fail, replace_time, validate_time,
602          callback_time, callbacks_time, nb_merged, nb_constant) = prof
603         blanc = ('    ' * level)
604         print(blanc, "MergeOptimizer", file=stream)
605         print(blanc, "  nb fail=%5d merged=%5d constant=%5d" % (
606               nb_fail, nb_merged, nb_constant), file=stream)
607         print(blanc, "  time replace=%2.2f validate=%2.2f callback=%2.2f" % (
608               replace_time, validate_time, callback_time), file=stream)
609         if callback_time &gt; 1:
610             print(blanc, "  callbacks_time", file=stream)
611             for i in sorted(iteritems(callbacks_time), key=lambda a: a[1]):
612                 if i[1] &gt; 0:
613                     print(blanc, "      ", i[0], ',', i[1], file=stream)
614     @staticmethod
615     def merge_profile(prof1, prof2):
616         def merge_none_number(v1, v2):
617             if v1 is None:
618                 return v2
619             if v2 is None:
620                 return v1
621             return v1 + v2
622         nb_fail = prof1[0] + prof2[0]
623         replace_time = prof1[1] + prof2[1]
624         validate_time = merge_none_number(prof1[2], prof2[2])
625         callback_time = merge_none_number(prof1[3], prof2[3])
626         callbacks_time = merge_dict(prof1[4], prof2[4])
627         nb_merged = prof1[5] + prof2[5]
628         nb_constant = prof1[6] + prof2[6]
629         return (nb_fail, replace_time, validate_time,
630                 callback_time, callbacks_time, nb_merged, nb_constant)
631 def is_same_graph_with_merge(var1, var2, givens=None):
632     if givens is None:
633         givens = {}
634     copied = copy.deepcopy([var1, var2, givens])
635     vars = copied[0:2]
636     givens = copied[2]
637     inputs = theano.gof.graph.inputs(vars)
638     fgraph = theano.gof.fg.FunctionGraph(inputs, vars, clone=False)
639     for to_replace, replace_by in iteritems(givens):
640         fgraph.replace(to_replace, replace_by)
641     MergeOptimizer().optimize(fgraph)
642     vars_replaced = [givens.get(v, v) for v in vars]
643     o1, o2 = [v.owner for v in vars_replaced]
644     if o1 is None and o2 is None:
645         return vars_replaced[0] == vars_replaced[1]
646     else:
647         return o1 is o2
648 def pre_constant_merge(vars):
649     seen_var = set()
650     const_sig_inv = {}
651     if isinstance(vars, graph.Variable):
652         vars = [vars]
653     def recursive_merge(var):
654         if var in seen_var:
655             return var
656         if not hasattr(var, 'owner'):
657             return var
658         if var.owner and hasattr(var.owner, "fgraph"):
659             return var
660         seen_var.add(var)
661         if isinstance(var, graph.Constant):
662             sig = var.signature()
663             try:
664                 if sig in const_sig_inv:
665                     return const_sig_inv[sig]
666                 const_sig_inv[sig] = var
667             except TypeError:  # unhashable type
668                 warnings.warn(
669                     "We work around a problem, the following variable"
670                     " signature isn't hashable. Please, report this to"
671                     " theano-dev so that the better fix is done. %s" % var)
672                 pass
673             return var
674         if var.owner:
675             for idx, inp in enumerate(var.owner.inputs):
676                 var.owner.inputs[idx] = recursive_merge(inp)
677         return var
678     return list(map(recursive_merge, vars))
679 class LocalOptimizer(object):
680     def __hash__(self):
681         if not hasattr(self, '_optimizer_idx'):
682             self._optimizer_idx = _optimizer_idx[0]
683             _optimizer_idx[0] += 1
684         return self._optimizer_idx
685     def tracks(self):
686         return None
687     def transform(self, node):
688         raise utils.MethodNotDefined("transform",
689                                      type(self), self.__class__.__name__)
690     def add_requirements(self, fgraph):
691         pass
692     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
693         print("%s%s id=%i" % (
694             (' ' * level), self.__class__.__name__, id(self)), file=stream)
695 class LocalMetaOptimizer(LocalOptimizer):
696     def __init__(self):
697         self.verbose = config.metaopt.verbose
698         self.track_dict = defaultdict(lambda: [])
699         self.tag_dict = defaultdict(lambda: [])
700         self._tracks = []
701         self.optimizers = []
702     def register(self, optimizer, tag_list):
703         self.optimizers.append(optimizer)
704         for c in optimizer.tracks():
705             self.track_dict[c].append(optimizer)
706             self._tracks.append(c)
707         for tag in tag_list:
708             self.tag_dict[tag].append(optimizer)
709     def tracks(self):
710         return self._tracks
711     def transform(self, node):
712         if self._tracks is not None:
713             if not isinstance(node.op, tuple(self._tracks)):
714                 return
715         givens = {}
716         missing = set()
717         for input in node.inputs:
718             if isinstance(input, theano.compile.SharedVariable):
719                 pass
720             elif hasattr(input.tag, 'test_value'):
721                 givens[input] = theano.shared(
722                     input.type.filter(input.tag.test_value),
723                     input.name,
724                     broadcastable=input.broadcastable,
725                     borrow=True)
726             else:
727                 missing.add(input)
728         if missing:
729             givens.update(self.provide_inputs(node, missing))
730             missing.difference_update(givens.keys())
731         if missing:
732             if self.verbose &gt; 0:
733                 print(("%s cannot meta-optimize %s, "
734                        "%d of %d input shapes unknown" %
735                        (self.__class__.__name__, node, len(missing), node.nin)))
736             return
737         if self.verbose &gt; 1:
738             print(("%s meta-optimizing %s (%d choices):" %
739                    (self.__class__.__name__, node, len(self.get_opts(node)))))
740         timings = []
741         for opt in self.get_opts(node):
742             outputs = opt.transform(node)
743             if outputs:
744                 try:
745                     fn = theano.function([], outputs, givens=givens,
746                                          on_unused_input='ignore')
747                     fn.trust_input = True
748                     timing = min(self.time_call(fn) for _ in range(2))
749                 except LocalMetaOptimizerSkipAssertionError:
750                     continue
751                 except Exception as e:
752                     if self.verbose &gt; 0:
753                         print("* %s: exception" % opt, e)
754                     continue
755                 else:
756                     if self.verbose &gt; 1:
757                         print("* %s: %.5g sec" % (opt, timing))
758                     timings.append((timing, outputs, opt))
759             else:
760                 if self.verbose &gt; 0:
761                     print("* %s: not applicable" % opt)
762         if timings:
763             timings.sort()
764             if self.verbose &gt; 1:
765                 print("= %s" % timings[0][2])
766             return timings[0][1]
767         return
768     def provide_inputs(self, node, inputs):
769         raise NotImplementedError()
770     def get_opts(self, node):
771         return self.track_dict[type(node.op)]
772     def time_call(self, fn):
773         start = time.time()
774         fn()
775         return time.time() - start
776 class FromFunctionLocalOptimizer(LocalOptimizer):
777     def __init__(self, fn, tracks=None, requirements=()):
778         self.transform = fn
779         self._tracks = tracks
780         self.requirements = requirements
781     def add_requirements(self, fgraph):
782         for req in self.requirements:
783             req(fgraph)
784     def tracks(self):
785         return self._tracks
786     def __str__(self):
787         return getattr(self, '__name__',
788                        '&lt;FromFunctionLocalOptimizer instance&gt;')
789     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
790         print("%s%s id=%i" % (
791             ' ' * level,
792             str(self.transform),
793             id(self)), file=stream)
794 def local_optimizer(tracks, inplace=False, requirements=()):
795     def decorator(f):
796         if tracks is not None:
797             if len(tracks) == 0:
798                 raise ValueError("Use None instead of an empty list to apply to all nodes.", f.__module__, f.__name__)
799             for t in tracks:
800                 if not (isinstance(t, op.Op) or issubclass(t, op.PureOp)):
801                     raise ValueError("Tracks are op classes or instances", f.__module__, f.__name__)
802         req = requirements
803         if inplace:
804             dh_handler = dh.DestroyHandler
805             req = tuple(requirements) + (
806                 lambda fgraph:
807                 fgraph.attach_feature(dh_handler()),)
808         rval = FromFunctionLocalOptimizer(f, tracks, req)
809         rval.__name__ = f.__name__
810         return rval
811     return decorator
812 class LocalOptGroup(LocalOptimizer):
813     def __init__(self, *optimizers, **kwargs):
814         if len(optimizers) == 1 and isinstance(optimizers[0], list):
815             optimizers = tuple(optimizers[0])
816         self.opts = optimizers
817         assert isinstance(self.opts, tuple)
818         self.reentrant = any(getattr(opt, 'reentrant', True)
819                              for opt in optimizers)
820         self.retains_inputs = all(getattr(opt, 'retains_inputs', False)
821                                   for opt in optimizers)
822         self.apply_all_opts = kwargs.pop('apply_all_opts', False)
823         self.profile = kwargs.pop('profile', False)
824         self.track_map = defaultdict(lambda: [])
825         assert len(kwargs) == 0
826         if self.profile:
827             self.time_opts = {}
828             self.process_count = {}
829             self.applied_true = {}
830             self.node_created = {}
831         for o in self.opts:
832             if self.profile:
833                 self.time_opts.setdefault(o, 0)
834                 self.process_count.setdefault(o, 0)
835                 self.applied_true.setdefault(o, 0)
836                 self.node_created.setdefault(o, 0)
837             tracks = o.tracks()
838             if tracks is None:
839                 self.track_map[None].append(o)
840             else:
841                 for c in tracks:
842                     self.track_map[c].append(o)
843     def __str__(self):
844         return getattr(self, '__name__',
845                        ('LocalOptGroup(%s)' %
846                         ','.join([str(o) for o in self.opts])))
847     def tracks(self):
848         t = []
849         for l in self.opts:
850             tt = l.tracks()
851             if tt:
852                 t.extend(tt)
853         return t
854     def transform(self, node):
855         if len(self.opts) == 0:
856             return
857         fgraph = node.fgraph
858         repl = None
859         while True:
860             opts = self.track_map[type(node.op)] + self.track_map[node.op] + self.track_map[None]
861             new_repl = None
862             for opt in opts:
863                 opt_start = time.time()
864                 new_repl = opt.transform(node)
865                 opt_finish = time.time()
866                 if self.profile:
867                     self.time_opts[opt] += opt_start - opt_finish
868                     self.process_count[opt] += 1
869                 if not new_repl:
870                     continue
871                 if isinstance(new_repl, (tuple, list)):
872                     new_vars = new_repl
873                 else:  # It must be a dict
874                     new_vars = list(new_repl.values())
875                 if self.profile:
876                     self.node_created[opt] += len(graph.ops(fgraph.variables, new_vars))
877                     self.applied_true[opt] += 1
878                 break  # break from the for loop over optimization.
879             if not new_repl:  # No optimization applied in the last iteration
880                 return repl
881             if not self.apply_all_opts:
882                 return new_repl
883             if not new_vars[0].owner:
884                 return new_repl
885             if len(new_repl) &gt; 1:
886                 s = set([v.owner for v in new_repl])
887                 assert len(s) == 1
888             repl = new_repl
889             node = new_vars[0].owner
890     @staticmethod
891     def print_profile(stream, prof, level=0):
892         (time_opts, process_count, applied_true, node_created, profile) = prof
893         if not profile:
894             return
895         blanc = ('    ' * int(level))
896         print(blanc, "LocalOptGroup", file=stream)
897         print(blanc, "---------------------", file=stream)
898         count_opt = []
899         not_used = []
900         not_used_time = 0
901         for o, count in iteritems(process_count):
902             if count &gt; 0:
903                 count_opt.append((time_opts[o], applied_true[o], count, o, node_created[o]))
904             else:
905                 not_used.append((time_opts[o], o))
906                 not_used_time += time_opts[o]
907         if count_opt:
908             print(blanc,
909                   '  time taken - times applied - times tried - name - node_created:',
910                   file=stream)
911             count_opt.sort()
912             for (t, a_t, count, o, n_c) in count_opt[::-1]:
913                 print(blanc, '  %.3fs - %d - %d - %s - %d' % (
914                       t, a_t, count, o, n_c), file=stream)
915             print(blanc, '  %.3fs - in %d optimization that were not used (display those with runtime greater than 0)' % (
916                 not_used_time, len(not_used)), file=stream)
917             not_used.sort(key=lambda nu: (nu[0], str(nu[1])))
918             for (t, o) in not_used[::-1]:
919                 if t &gt; 0:
920                     print(blanc + "  ", '  %.3fs - %s' % (t, o), file=stream)
921         else:
922             print(blanc, " The Optimizer wasn't successful ", file=stream)
923         print(file=stream)
924     def merge_profile(prof1, prof2):
925         raise NotImplementedError
926     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
927         print("%s%s id=%i" % (
928             (' ' * level), self.__class__.__name__, id(self)), file=stream)
929         if depth != 0:
930             depth -= 1
931             for lopt in self.opts:
932                 lopt.print_summary(stream, level=(level + 2), depth=depth)
933     def add_requirements(self, fgraph):
934         for opt in self.opts:
935             opt.add_requirements(fgraph)
936 class GraphToGPULocalOptGroup(LocalOptGroup):
937     def __init__(self, *optimizers, **kwargs):
938         super(GraphToGPULocalOptGroup, self).__init__(*optimizers, **kwargs)
939         assert self.apply_all_opts is False
940     def transform(self, op, context_name, inputs, outputs):
941         if len(self.opts) == 0:
942             return
943         fgraph = outputs[0].fgraph
944         opts = self.track_map[type(op)] + self.track_map[op] + self.track_map[None]
945         for opt in opts:
946             opt_start = time.time()
947             new_repl = opt.transform(op, context_name, inputs, outputs)
948             opt_finish = time.time()
949             if self.profile:
950                 self.time_opts[opt] += opt_start - opt_finish
951                 self.process_count[opt] += 1
952             if not new_repl:
953                 continue
954             if self.profile:
955                 self.node_created[opt] += len(graph.ops(fgraph.variables, new_repl))
956                 self.applied_true[opt] += 1
957             return new_repl
958 class OpSub(LocalOptimizer):
959     reentrant = False
960     retains_inputs = True
961     def __init__(self, op1, op2, transfer_tags=True):
962         self.op1 = op1
963         self.op2 = op2
964         self.transfer_tags = transfer_tags
965     def op_key(self):
966         return self.op1
967     def tracks(self):
968         return [self.op1]
969     def transform(self, node):
970         if node.op != self.op1:
971             return False
972         repl = self.op2.make_node(*node.inputs)
973         if self.transfer_tags:
974             repl.tag = copy.copy(node.tag)
975             for output, new_output in zip(node.outputs, repl.outputs):
976                 new_output.tag = copy.copy(output.tag)
977         return repl.outputs
978     def __str__(self):
979         return "%s -&gt; %s" % (self.op1, self.op2)
980 class OpRemove(LocalOptimizer):
981     reentrant = False      # no nodes are added at all
982     def __init__(self, op):
983         self.op = op
984     def op_key(self):
985         return self.op
986     def tracks(self):
987         return [self.op]
988     def transform(self, node):
989         if node.op != self.op:
990             return False
991         return node.inputs
992     def __str__(self):
993         return "%s(x) -&gt; x" % (self.op)
994     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
995         print("%s%s(%s) id=%i" % (
996             ' ' * level,
997             self.__class__.__name__,
998             str(self.op),
999             id(self)), file=stream)
1000 class PatternSub(LocalOptimizer):
1001     def __init__(self, in_pattern, out_pattern,
1002                  allow_multiple_clients=False,
1003                  skip_identities_fn=None, name=None, pdb=False,
1004                  tracks=(), get_nodes=None,
1005                  values_eq_approx=None):
1006         self.in_pattern = in_pattern
1007         self.out_pattern = out_pattern
1008         self.values_eq_approx = values_eq_approx
1009         if isinstance(in_pattern, (list, tuple)):
1010             self.op = self.in_pattern[0]
1011         elif isinstance(in_pattern, dict):
1012             self.op = self.in_pattern['pattern'][0]
1013         else:
1014             raise TypeError("The pattern to search for must start with "
1015                             "a specific Op instance.")
1016         self.__doc__ = (self.__class__.__doc__ +
1017                         "\n\nThis instance does: " +
1018                         str(self) + "\n")
1019         self.allow_multiple_clients = allow_multiple_clients
1020         self.skip_identities_fn = skip_identities_fn
1021         if name:
1022             self.__name__ = name
1023         self.pdb = pdb
1024         self._tracks = tracks
1025         self.get_nodes = get_nodes
1026         if tracks != ():
1027             assert get_nodes
1028     def op_key(self):
1029         return self.op
1030     def tracks(self):
1031         if self._tracks != ():
1032             return self._tracks
1033         return [self.op]
1034     def transform(self, node, get_nodes=True):
1035         if get_nodes and self.get_nodes is not None:
1036             for real_node in self.get_nodes(node):
1037                 if real_node == "output":
1038                     continue
1039                 ret = self.transform(real_node, get_nodes=False)
1040                 if ret is not False and ret is not None:
1041                     assert len(real_node.outputs) == len(ret)
1042                     if self.values_eq_approx:
1043                         ret.tag.values_eq_approx = self.values_eq_approx
1044                     return dict(izip(real_node.outputs, ret))
1045         if node.op != self.op:
1046             return False
1047         def match(pattern, expr, u, allow_multiple_clients=False, pdb=False):
1048             def retry_with_equiv():
1049                 if not self.skip_identities_fn:
1050                     return False
1051                 expr_equiv = self.skip_identities_fn(expr)
1052                 if expr_equiv is None:
1053                     return False
1054                 return match(pattern, expr_equiv, u,
1055                              allow_multiple_clients=allow_multiple_clients)
1056             if isinstance(pattern, (list, tuple)):
1057                 if expr.owner is None:
1058                     return False
1059                 if (not (expr.owner.op == pattern[0]) or
1060                         (not allow_multiple_clients and len(expr.clients) &gt; 1)):
1061                     return retry_with_equiv()
1062                 if len(pattern) - 1 != len(expr.owner.inputs):
1063                     return retry_with_equiv()
1064                 for p, v in zip(pattern[1:], expr.owner.inputs):
1065                     u = match(p, v, u, self.allow_multiple_clients)
1066                     if not u:
1067                         return False
1068             elif isinstance(pattern, dict):
1069                 try:
1070                     real_pattern = pattern['pattern']
1071                 except KeyError:
1072                     raise KeyError(
1073                         "Malformed pattern: %s (expected key 'pattern')"
1074                         % pattern)
1075                 constraint = pattern.get('constraint', lambda expr: True)
1076                 if constraint(expr):
1077                     return match(real_pattern, expr, u,
1078                                  pattern.get('allow_multiple_clients',
1079                                              allow_multiple_clients))
1080                 else:
1081                     return retry_with_equiv()
1082             elif isinstance(pattern, string_types):
1083                 v = unify.Var(pattern)
1084                 if u[v] is not v and u[v] is not expr:
1085                     return retry_with_equiv()
1086                 else:
1087                     u = u.merge(expr, v)
1088             elif (isinstance(pattern, (integer_types, float)) and
1089                     isinstance(expr, graph.Constant)):
1090                 if np.all(theano.tensor.constant(pattern).value == expr.value):
1091                     return u
1092                 else:
1093                     return retry_with_equiv()
1094             elif (isinstance(pattern, graph.Constant) and
1095                     isinstance(expr, graph.Constant) and
1096                     pattern.equals(expr)):
1097                 return u
1098             else:
1099                 return retry_with_equiv()
1100             if pdb:
1101                 import pdb
1102                 pdb.set_trace()
1103             return u
1104         u = match(self.in_pattern, node.out, unify.Unification(), True,
1105                   self.pdb)
1106         if u:
1107             def build(pattern, u):
1108                 if isinstance(pattern, (list, tuple)):
1109                     args = [build(p, u) for p in pattern[1:]]
1110                     return pattern[0](*args)
1111                 elif isinstance(pattern, string_types):
1112                     return u[unify.Var(pattern)]
1113                 elif isinstance(pattern, (integer_types, float)):
1114                     return pattern
1115                 else:
1116                     return pattern.clone()
1117             p = self.out_pattern
1118             ret = build(p, u)
1119             if self.values_eq_approx:
1120                 ret.tag.values_eq_approx = self.values_eq_approx
1121             return [ret]
1122         else:
1123             return False
1124     def __str__(self):
1125         if getattr(self, '__name__', None):
1126             return self.__name__
1127         def pattern_to_str(pattern):
1128             if isinstance(pattern, (list, tuple)):
1129                 return "%s(%s)" % (
1130                     str(pattern[0]),
1131                     ", ".join([pattern_to_str(p) for p in pattern[1:]]))
1132             elif isinstance(pattern, dict):
1133                 return "%s subject to %s" % (
1134                     pattern_to_str(pattern['pattern']),
1135                     str(pattern.get('constraint', 'no conditions')))
1136             else:
1137                 return str(pattern)
1138         return "%s -&gt; %s" % (
1139             pattern_to_str(self.in_pattern),
1140             pattern_to_str(self.out_pattern))
1141     def __repr__(self):
1142         return str(self)
1143     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1144         name = getattr(self, '__name__', getattr(self, 'name', None))
1145         print("%s%s %s(%s, %s) id=%i" % (
1146             ' ' * level,
1147             self.__class__.__name__,
1148             name,
1149             str(self.in_pattern),
1150             str(self.out_pattern),
1151             id(self)), file=stream)
1152 class Updater:
1153     def __init__(self, importer, pruner, chin, name=None):
1154         self.importer = importer
1155         self.pruner = pruner
1156         self.chin = chin
1157         self.name = name
1158     def __str__(self):
1159         return "Updater{%s}" % str(self.name)
1160     def on_import(self, fgraph, node, reason):
1161         if self.importer:
1162             self.importer(node)
1163     def on_prune(self, fgraph, node, reason):
1164         if self.pruner:
1165             self.pruner(node)
1166     def on_change_input(self, fgraph, node, i, r, new_r, reason):
1167         if self.chin:
1168             self.chin(node, i, r, new_r, reason)
1169     def on_detach(self, fgraph):
1170         self.importer = None
1171         self.pruner = None
1172         self.chin = None
1173 class NavigatorOptimizer(Optimizer):
1174     @staticmethod
1175     def warn(exc, nav, repl_pairs, local_opt, node):
1176         if config.on_opt_error != 'ignore':
1177             _logger.error("Optimization failure due to: %s" % str(local_opt))
1178             _logger.error("node: %s" % str(node))
1179             _logger.error("TRACEBACK:")
1180             _logger.error(traceback.format_exc())
1181         if config.on_opt_error == 'pdb':
1182             pdb.post_mortem(sys.exc_info()[2])
1183         elif isinstance(exc, AssertionError) or config.on_opt_error == 'raise':
1184             raise exc
1185     @staticmethod
1186     def warn_inplace(exc, nav, repl_pairs, local_opt, node):
1187         if isinstance(exc, InconsistencyError):
1188             return
1189         return NavigatorOptimizer.warn(exc, nav, repl_pairs, local_opt, node)
1190     @staticmethod
1191     def warn_ignore(exc, nav, repl_pairs, local_opt, node):
1192         pass
1193     def __init__(self, local_opt, ignore_newtrees='auto',
1194                  failure_callback=None):
1195         self.local_opt = local_opt
1196         if ignore_newtrees == 'auto':
1197             self.ignore_newtrees = not getattr(local_opt, 'reentrant', True)
1198         else:
1199             self.ignore_newtrees = ignore_newtrees
1200         self.failure_callback = failure_callback
1201     def attach_updater(self, fgraph, importer, pruner, chin=None, name=None):
1202         if self.ignore_newtrees:
1203             importer = None
1204         if importer is None and pruner is None:
1205             return None
1206         u = Updater(importer, pruner, chin, name=name)
1207         fgraph.attach_feature(u)
1208         return u
1209     def detach_updater(self, fgraph, u):
1210         if u is not None:
1211             fgraph.remove_feature(u)
1212     def process_node(self, fgraph, node, lopt=None):
1213         lopt = lopt or self.local_opt
1214         try:
1215             replacements = lopt.transform(node)
1216         except Exception as e:
1217             if self.failure_callback is not None:
1218                 self.failure_callback(e, self,
1219                                       [(x, None) for x in node.outputs],
1220                                       lopt, node)
1221                 return False
1222             else:
1223                 raise
1224         if replacements is False or replacements is None:
1225             return False
1226         old_vars = node.outputs
1227         remove = []
1228         if isinstance(replacements, dict):
1229             if "remove" in replacements:
1230                 remove = replacements.pop("remove")
1231             old_vars = list(replacements.keys())
1232             replacements = list(replacements.values())
1233         elif not isinstance(replacements, (tuple, list)):
1234             raise TypeError('Optimizer %s gave wrong type of replacement. '
1235                             'Expected list or tuple. Got %s' % (
1236                                 lopt, replacements))
1237         if len(old_vars) != len(replacements):
1238             raise ValueError('Optimizer %s gave wrong number of replacements'
1239                              % lopt)
1240         for r, rnew in zip(old_vars, replacements):
1241             if rnew is None and len(r.clients) &gt; 0:
1242                 raise ValueError("A local optimizer tried to remove a Variable that is used")
1243         repl_pairs = [(r, rnew) for r, rnew in zip(old_vars, replacements)
1244                       if rnew is not r and rnew is not None]
1245         if len(repl_pairs) == 0:
1246             return False
1247         try:
1248             fgraph.replace_all_validate_remove(repl_pairs,
1249                                                reason=lopt,
1250                                                remove=remove)
1251             return True
1252         except Exception as e:
1253             if self.failure_callback is not None:
1254                 self.failure_callback(e, self, repl_pairs, lopt, node)
1255                 return False
1256             else:
1257                 raise
1258     def add_requirements(self, fgraph):
1259         super(NavigatorOptimizer, self).add_requirements(fgraph)
1260         if self.local_opt:
1261             self.local_opt.add_requirements(fgraph)
1262     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1263         print("%s%s (%i)" % (
1264             (' ' * level), self.__class__.__name__, id(self)), file=stream)
1265         if depth != 0:
1266             self.local_opt.print_summary(stream, level=(level + 2),
1267                                          depth=(depth - 1))
1268 class TopoOptimizer(NavigatorOptimizer):
1269     def __init__(self, local_opt, order='in_to_out', ignore_newtrees=False,
1270                  failure_callback=None):
1271         if order not in ['out_to_in', 'in_to_out']:
1272             raise ValueError("order must be 'out_to_in' or 'in_to_out'")
1273         self.order = order
1274         NavigatorOptimizer.__init__(self, local_opt, ignore_newtrees,
1275                                     failure_callback)
1276     def apply(self, fgraph, start_from=None):
1277         if start_from is None:
1278             start_from = fgraph.outputs
1279         callback_before = fgraph.execute_callbacks_time
1280         nb_nodes_start = len(fgraph.apply_nodes)
1281         t0 = time.time()
1282         q = deque(graph.io_toposort(fgraph.inputs, start_from))
1283         io_t = time.time() - t0
1284         def importer(node):
1285             if node is not current_node:
1286                 q.append(node)
1287         u = self.attach_updater(fgraph, importer, None,
1288                                 name=getattr(self, 'name', None))
1289         nb = 0
1290         try:
1291             t0 = time.time()
1292             while q:
1293                 if self.order == 'out_to_in':
1294                     node = q.pop()
1295                 else:
1296                     node = q.popleft()
1297                 if node not in fgraph.apply_nodes:
1298                     continue
1299                 current_node = node
1300                 nb += self.process_node(fgraph, node)
1301             loop_t = time.time() - t0
1302         finally:
1303             self.detach_updater(fgraph, u)
1304         callback_time = fgraph.execute_callbacks_time - callback_before
1305         nb_nodes_end = len(fgraph.apply_nodes)
1306         return (self, nb, nb_nodes_start, nb_nodes_end,
1307                 io_t, loop_t, callback_time, self.local_opt)
1308     @staticmethod
1309     def print_profile(stream, prof, level=0):
1310         blanc = ('    ' * level)
1311         if prof is None:  # Happen as merge_profile() isn't implemented
1312             print(blanc, "TopoOptimizer merge_profile not implemented",
1313                   file=stream)
1314             return
1315         (opt, nb, nb_nodes_start, nb_nodes_end,
1316          io_t, loop_t, callback_time, lopt) = prof
1317         print(blanc, "TopoOptimizer ",
1318               getattr(opt, "name", getattr(opt, "__name__", "")), file=stream)
1319         print(blanc, "  nb_node (start, end, changed)", (
1320             nb_nodes_start, nb_nodes_end, nb), file=stream)
1321         print(blanc, "  init io_toposort", io_t, file=stream)
1322         print(blanc, "  loop time", loop_t, file=stream)
1323         print(blanc, "  callback_time", callback_time, file=stream)
1324         if isinstance(lopt, LocalOptGroup):
1325             if lopt.profile:
1326                 lopt.print_profile(stream, (lopt.time_opts,
1327                                             lopt.process_count,
1328                                             lopt.applied_true,
1329                                             lopt.node_created,
1330                                             lopt.profile),
1331                                    level=level + 1)
1332     def __str__(self):
1333         return getattr(self, '__name__',
1334                        '&lt;TopoOptimizer instance&gt;')
1335 def out2in(*local_opts, **kwargs):
1336     name = (kwargs and kwargs.pop('name', None))
1337     if len(local_opts) &gt; 1:
1338         local_opts = LocalOptGroup(*local_opts)
1339     else:
1340         local_opts, = local_opts
1341         if not name:
1342             name = local_opts.__name__
1343     ret = TopoOptimizer(local_opts,
1344                         order='out_to_in',
1345                         failure_callback=TopoOptimizer.warn_inplace,
1346                         **kwargs)
1347     if name:
1348         ret.__name__ = name
1349     return ret
1350 def in2out(*local_opts, **kwargs):
1351     name = (kwargs and kwargs.pop('name', None))
1352     if len(local_opts) &gt; 1:
1353         local_opts = LocalOptGroup(*local_opts)
1354     else:
1355         local_opts, = local_opts
1356         if not name:
1357             name = local_opts.__name__
1358     ret = TopoOptimizer(local_opts,
1359                         order='in_to_out',
1360                         failure_callback=TopoOptimizer.warn_inplace,
1361                         **kwargs)
1362     if name:
1363         ret.__name__ = name
1364     return ret
1365 class OpKeyOptimizer(NavigatorOptimizer):
1366     def __init__(self, local_opt, ignore_newtrees=False,
1367                  failure_callback=None):
1368         if not hasattr(local_opt, 'op_key'):
1369             raise TypeError("LocalOptimizer for OpKeyOptimizer must have "
1370                             "an 'op_key' method.")
1371         NavigatorOptimizer.__init__(self, local_opt, ignore_newtrees,
1372                                     failure_callback)
1373     def apply(self, fgraph):
1374         op = self.local_opt.op_key()
1375         if isinstance(op, (list, tuple)):
1376             q = reduce(list.__iadd__, map(fgraph.get_nodes, op))
1377         else:
1378             q = list(fgraph.get_nodes(op))
1379         def importer(node):
1380             if node is not current_node:
1381                 if node.op == op:
1382                     q.append(node)
1383         u = self.attach_updater(fgraph, importer, None,
1384                                 name=getattr(self, 'name', None))
1385         try:
1386             while q:
1387                 node = q.pop()
1388                 if node not in fgraph.apply_nodes:
1389                     continue
1390                 current_node = node
1391                 self.process_node(fgraph, node)
1392         finally:
1393             self.detach_updater(fgraph, u)
1394     def add_requirements(self, fgraph):
1395         super(OpKeyOptimizer, self).add_requirements(fgraph)
1396         fgraph.attach_feature(toolbox.NodeFinder())
1397 class ChangeTracker:
1398     def __init__(self):
1399         self.changed = False
1400         self.nb_imported = 0
1401     def on_import(self, fgraph, node, reason):
1402         self.nb_imported += 1
1403         self.changed = True
1404     def on_change_input(self, fgraph, node, i, r, new_r, reason):
1405         self.changed = True
1406     def reset(self):
1407         self.changed = False
1408     def on_attach(self, fgraph):
1409         fgraph.change_tracker = self
1410     def on_detach(self, fgraph):
1411         del fgraph.change_tracker
1412 def merge_dict(d1, d2):
1413     d = d1.copy()
1414     for k, v in iteritems(d2):
1415         if k in d:
1416             d[k] += v
1417         else:
1418             d[k] = v
1419     return d
1420 class EquilibriumOptimizer(NavigatorOptimizer):
1421     def __init__(self,
1422                  optimizers,
1423                  failure_callback=None,
1424                  ignore_newtrees=True,
1425                  tracks_on_change_inputs=False,
1426                  max_use_ratio=None,
1427                  final_optimizers=None,
1428                  cleanup_optimizers=None):
1429         super(EquilibriumOptimizer, self).__init__(
1430             None,
1431             ignore_newtrees=ignore_newtrees,
1432             failure_callback=failure_callback)
1433         self.local_optimizers_map = OrderedDict()
1434         self.local_optimizers_all = []
1435         self.global_optimizers = []
1436         self.final_optimizers = []
1437         self.cleanup_optimizers = []
1438         self.tracks_on_change_inputs = tracks_on_change_inputs
1439         for opt in optimizers:
1440             if isinstance(opt, LocalOptimizer):
1441                 if opt.tracks() is None:
1442                     self.local_optimizers_all.append(opt)
1443                 else:
1444                     for c in opt.tracks():
1445                         self.local_optimizers_map.setdefault(c, []).append(opt)
1446             else:
1447                 self.global_optimizers.append(opt)
1448         if final_optimizers:
1449             self.final_optimizers = final_optimizers
1450         if cleanup_optimizers:
1451             self.cleanup_optimizers = cleanup_optimizers
1452         self.max_use_ratio = max_use_ratio
1453         assert self.max_use_ratio is not None, (
1454             'max_use_ratio has to be a number')
1455     def get_local_optimizers(self):
1456         for opt in self.local_optimizers_all:
1457             yield opt
1458         s = set()
1459         for lopt in itervalues(self.local_optimizers_map):
1460             for opt in lopt:
1461                 if opt not in s:
1462                     yield opt
1463                     s.add(opt)
1464     def add_requirements(self, fgraph):
1465         super(EquilibriumOptimizer, self).add_requirements(fgraph)
1466         for opt in self.get_local_optimizers():
1467             opt.add_requirements(fgraph)
1468         for opt in self.global_optimizers:
1469             opt.add_requirements(fgraph)
1470         for opt in self.final_optimizers:
1471             opt.add_requirements(fgraph)
1472         for opt in self.cleanup_optimizers:
1473             opt.add_requirements(fgraph)
1474     def apply(self, fgraph, start_from=None):
1475         change_tracker = ChangeTracker()
1476         fgraph.attach_feature(change_tracker)
1477         if start_from is None:
1478             start_from = fgraph.outputs
1479         else:
1480             for node in start_from:
1481                 assert node in fgraph.outputs
1482         changed = True
1483         max_use_abort = False
1484         opt_name = None
1485         global_process_count = {}
1486         start_nb_nodes = len(fgraph.apply_nodes)
1487         max_nb_nodes = len(fgraph.apply_nodes)
1488         max_use = max_nb_nodes * self.max_use_ratio
1489         loop_timing = []
1490         loop_process_count = []
1491         global_opt_timing = []
1492         time_opts = {}
1493         io_toposort_timing = []
1494         nb_nodes = []
1495         node_created = {}
1496         global_sub_profs = []
1497         final_sub_profs = []
1498         cleanup_sub_profs = []
1499         for opt in (self.global_optimizers +
1500                     list(self.get_local_optimizers()) +
1501                     self.final_optimizers +
1502                     self.cleanup_optimizers):
1503             global_process_count.setdefault(opt, 0)
1504             time_opts.setdefault(opt, 0)
1505             node_created.setdefault(opt, 0)
1506         def apply_cleanup(profs_dict):
1507             changed = False
1508             for copt in self.cleanup_optimizers:
1509                 change_tracker.reset()
1510                 nb = change_tracker.nb_imported
1511                 t_opt = time.time()
1512                 sub_prof = copt.apply(fgraph)
1513                 time_opts[copt] += time.time() - t_opt
1514                 profs_dict[copt].append(sub_prof)
1515                 if change_tracker.changed:
1516                     process_count.setdefault(copt, 0)
1517                     process_count[copt] += 1
1518                     global_process_count[copt] += 1
1519                     changed = True
1520                     node_created[copt] += change_tracker.nb_imported - nb
1521             return changed
1522         while changed and not max_use_abort:
1523             process_count = {}
1524             t0 = time.time()
1525             changed = False
1526             iter_cleanup_sub_profs = {}
1527             for copt in self.cleanup_optimizers:
1528                 iter_cleanup_sub_profs[copt] = []
1529             sub_profs = []
1530             for gopt in self.global_optimizers:
1531                 change_tracker.reset()
1532                 nb = change_tracker.nb_imported
1533                 t_opt = time.time()
1534                 sub_prof = gopt.apply(fgraph)
1535                 time_opts[gopt] += time.time() - t_opt
1536                 sub_profs.append(sub_prof)
1537                 if change_tracker.changed:
1538                     process_count.setdefault(gopt, 0)
1539                     process_count[gopt] += 1
1540                     global_process_count[gopt] += 1
1541                     changed = True
1542                     node_created[gopt] += change_tracker.nb_imported - nb
1543                     if global_process_count[gopt] &gt; max_use:
1544                         max_use_abort = True
1545                         opt_name = (getattr(gopt, "name", None) or
1546                                     getattr(gopt, "__name__", ""))
1547             global_sub_profs.append(sub_profs)
1548             global_opt_timing.append(float(time.time() - t0))
1549             changed |= apply_cleanup(iter_cleanup_sub_profs)
1550             topo_t0 = time.time()
1551             q = deque(graph.io_toposort(fgraph.inputs, start_from))
1552             io_toposort_timing.append(time.time() - topo_t0)
1553             nb_nodes.append(len(q))
1554             max_nb_nodes = max(max_nb_nodes, len(q))
1555             max_use = max_nb_nodes * self.max_use_ratio
1556             def importer(node):
1557                 if node is not current_node:
1558                     q.append(node)
1559             chin = None
1560             if self.tracks_on_change_inputs:
1561                 def chin(node, i, r, new_r, reason):
1562                     if node is not current_node and not isinstance(node, str):
1563                         q.append(node)
1564             u = self.attach_updater(fgraph, importer, None,
1565                                     chin=chin,
1566                                     name=getattr(self, 'name', None))
1567             try:
1568                 while q:
1569                     node = q.pop()
1570                     if node not in fgraph.apply_nodes:
1571                         continue
1572 <a name="0"></a>                    current_node = node
1573                     for lopt in (self.local_optimizers_all +
1574                                  self.local_optimizers_map.get(type(node.op), []) +
1575                                  self.local_optimizers_map.get(node<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>.op, [])):
1576                         nb = change_tracker.nb_imported
1577                         t_opt = time.time()
1578                         lopt_change = self.process_node(fgraph, node, lopt)
1579                         time_opts[lopt] += time.time(</b></font>) - t_opt
1580                         if not lopt_change:
1581                             continue
1582                         process_count.setdefault(lopt, 0)
1583                         process_count[lopt] += 1
1584                         global_process_count[lopt] += 1
1585                         changed = True
1586                         node_created[lopt] += change_tracker.nb_imported - nb
1587                         changed |= apply_cleanup(iter_cleanup_sub_profs)
1588                         if global_process_count[lopt] &gt; max_use:
1589                             max_use_abort = True
1590                             opt_name = (getattr(lopt, "name", None) or
1591                                         getattr(lopt, "__name__", ""))
1592                         if node not in fgraph.apply_nodes:
1593                             break
1594             finally:
1595                 self.detach_updater(fgraph, u)
1596             sub_profs = []
1597             t_before_final_opt = time.time()
1598             for gopt in self.final_optimizers:
1599                 change_tracker.reset()
1600                 nb = change_tracker.nb_imported
1601                 t_opt = time.time()
1602                 sub_prof = gopt.apply(fgraph)
1603                 time_opts[gopt] += time.time() - t_opt
1604                 sub_profs.append(sub_prof)
1605                 if change_tracker.changed:
1606                     process_count.setdefault(gopt, 0)
1607                     process_count[gopt] += 1
1608                     global_process_count[gopt] += 1
1609                     changed = True
1610                     node_created[gopt] += change_tracker.nb_imported - nb
1611                     if global_process_count[gopt] &gt; max_use:
1612                         max_use_abort = True
1613                         opt_name = (getattr(gopt, "name", None) or
1614                                     getattr(gopt, "__name__", ""))
1615             final_sub_profs.append(sub_profs)
1616             global_opt_timing[-1] += time.time() - t_before_final_opt
1617             changed |= apply_cleanup(iter_cleanup_sub_profs)
1618             c_sub_profs = []
1619             for copt, sub_profs in iteritems(iter_cleanup_sub_profs):
1620                 sub_prof = sub_profs[0]
1621                 for s_p in sub_profs[1:]:
1622                     sub_prof = copt.merge_profile(sub_prof, s_p)
1623                 c_sub_profs.append(sub_prof)
1624             cleanup_sub_profs.append(c_sub_profs)
1625             loop_process_count.append(process_count)
1626             loop_timing.append(float(time.time() - t0))
1627         end_nb_nodes = len(fgraph.apply_nodes)
1628         if max_use_abort:
1629             msg = ("EquilibriumOptimizer max'ed out by '%s'" % opt_name +
1630                    ". You can safely raise the current threshold of " +
1631                    "%f with the theano flag 'optdb.max_use_ratio'." %
1632                    config.optdb.max_use_ratio)
1633             if theano.config.on_opt_error == 'raise':
1634                 raise AssertionError(msg)
1635             else:
1636                 _logger.error(msg)
1637         fgraph.remove_feature(change_tracker)
1638         assert len(loop_process_count) == len(loop_timing)
1639         assert len(loop_process_count) == len(global_opt_timing)
1640         assert len(loop_process_count) == len(nb_nodes)
1641         assert len(loop_process_count) == len(io_toposort_timing)
1642         assert len(loop_process_count) == len(global_sub_profs)
1643         assert len(loop_process_count) == len(final_sub_profs)
1644         assert len(loop_process_count) == len(cleanup_sub_profs)
1645         return (self, loop_timing, loop_process_count,
1646                 (start_nb_nodes, end_nb_nodes, max_nb_nodes),
1647                 global_opt_timing, nb_nodes, time_opts, io_toposort_timing,
1648                 node_created, global_sub_profs, final_sub_profs,
1649                 cleanup_sub_profs)
1650     def print_summary(self, stream=sys.stdout, level=0, depth=-1):
1651         name = getattr(self, 'name', None)
1652         print("%s%s %s id=%i" % (
1653             (' ' * level), self.__class__.__name__, name, id(self)), file=stream)
1654         if depth != 0:
1655             for lopt in self.get_local_optimizers():
1656                 lopt.print_summary(stream, level=(level + 2),
1657                                    depth=(depth - 1))
1658     @staticmethod
1659     def print_profile(stream, prof, level=0):
1660         (opt, loop_timing, loop_process_count,
1661          (start_nb_nodes, end_nb_nodes, max_nb_nodes),
1662          global_opt_timing, nb_nodes, time_opts, io_toposort_timing,
1663          node_created, global_sub_profs, final_sub_profs,
1664          cleanup_sub_profs) = prof
1665         blanc = ('    ' * level)
1666         print(blanc, "EquilibriumOptimizer", end=' ', file=stream)
1667         print(blanc, getattr(opt, "name",
1668                              getattr(opt, "__name__", "")), file=stream)
1669         print(blanc, "  time %.3fs for %d passes" % (
1670             sum(loop_timing), len(loop_timing)), file=stream)
1671         print(blanc, "  nb nodes (start, end,  max) %d %d %d" % (
1672             start_nb_nodes, end_nb_nodes, max_nb_nodes), file=stream)
1673         print(blanc, "  time io_toposort %.3fs" % sum(
1674             io_toposort_timing), file=stream)
1675         s = sum([time_opts[o] for o in opt.get_local_optimizers()])
1676         print(blanc, "  time in local optimizers %.3fs" % s, file=stream)
1677         s = sum([time_opts[o] for o in opt.global_optimizers])
1678         print(blanc, "  time in global optimizers %.3fs" % s, file=stream)
1679         s = sum([time_opts[o] for o in opt.final_optimizers])
1680         print(blanc, "  time in final optimizers %.3fs" % s, file=stream)
1681         s = sum([time_opts[o] for o in opt.cleanup_optimizers])
1682         print(blanc, "  time in cleanup optimizers %.3fs" % s, file=stream)
1683         for i in range(len(loop_timing)):
1684             lopt = ""
1685             if loop_process_count[i]:
1686                 d = list(reversed(sorted(iteritems(loop_process_count[i]),
1687                                          key=lambda a: a[1])))
1688                 lopt = " ".join([str((str(k), v)) for k, v
1689                                  in d[:5]])
1690                 if len(d) &gt; 5:
1691                     lopt += " ..."
1692             print(blanc, ('  %2d - %.3fs %d (%.3fs in global opts, '
1693                           '%.3fs io_toposort) - %d nodes - %s' % (
1694                               i, loop_timing[i],
1695                               sum(loop_process_count[i].values()),
1696                               global_opt_timing[i],
1697                               io_toposort_timing[i], nb_nodes[i],
1698                               lopt)), file=stream)
1699         count_opt = []
1700         not_used = []
1701         not_used_time = 0
1702         process_count = {}
1703         for o in (opt.global_optimizers +
1704                   list(opt.get_local_optimizers()) +
1705                   list(opt.final_optimizers) +
1706                   list(opt.cleanup_optimizers)):
1707             process_count.setdefault(o, 0)
1708         for count in loop_process_count:
1709             for o, v in iteritems(count):
1710                 process_count[o] += v
1711         for o, count in iteritems(process_count):
1712             if count &gt; 0:
1713                 count_opt.append((time_opts[o], count,
1714                                   node_created[o], o))
1715             else:
1716                 not_used.append((time_opts[o], o))
1717                 not_used_time += time_opts[o]
1718         if count_opt:
1719             print(blanc,
1720                   '  times - times applied - nb node created - name:',
1721                   file=stream)
1722             count_opt.sort()
1723             for (t, count, n_created, o) in count_opt[::-1]:
1724                 print(blanc, '  %.3fs - %d - %d - %s' % (
1725                     t, count, n_created, o), file=stream)
1726             print(blanc, '  %.3fs - in %d optimization that were not used (display only those with a runtime &gt; 0)' % (
1727                 not_used_time, len(not_used)), file=stream)
1728             not_used.sort(key=lambda nu: (nu[0], str(nu[1])))
1729             for (t, o) in not_used[::-1]:
1730                 if t &gt; 0:
1731                     print(blanc + "  ", '  %.3fs - %s' % (t, o), file=stream)
1732             print(file=stream)
1733         gf_opts = [o for o in (opt.global_optimizers +
1734                                list(opt.final_optimizers) +
1735                                list(opt.cleanup_optimizers))
1736                    if o.print_profile.__code__ is not
1737                    Optimizer.print_profile.__code__]
1738         if not gf_opts:
1739             return
1740         print(blanc, "Global, final and clean up optimizers", file=stream)
1741         for i in range(len(loop_timing)):
1742             print(blanc, "Iter %d" % i, file=stream)
1743             for o, prof in zip(opt.global_optimizers, global_sub_profs[i]):
1744                 try:
1745                     o.print_profile(stream, prof, level + 2)
1746                 except NotImplementedError:
1747                     print(blanc, "merge not implemented for ", o)
1748             for o, prof in zip(opt.final_optimizers, final_sub_profs[i]):
1749                 try:
1750                     o.print_profile(stream, prof, level + 2)
1751                 except NotImplementedError:
1752                     print(blanc, "merge not implemented for ", o)
1753             for o, prof in zip(opt.cleanup_optimizers, cleanup_sub_profs[i]):
1754                 try:
1755                     o.print_profile(stream, prof, level + 2)
1756                 except NotImplementedError:
1757                     print(blanc, "merge not implemented for ", o)
1758     @staticmethod
1759     def merge_profile(prof1, prof2):
1760         local_optimizers = OrderedSet(prof1[0].get_local_optimizers()).union(
1761             prof2[0].get_local_optimizers())
1762         global_optimizers = OrderedSet(prof1[0].global_optimizers).union(
1763             prof2[0].global_optimizers)
1764         final_optimizers = list(OrderedSet(prof1[0].final_optimizers).union(
1765             prof2[0].final_optimizers))
1766         cleanup_optimizers = list(OrderedSet(prof1[0].cleanup_optimizers).union(
1767             prof2[0].cleanup_optimizers))
1768         new_opt = EquilibriumOptimizer(
1769             local_optimizers.union(global_optimizers),
1770             max_use_ratio=1,
1771             final_optimizers=final_optimizers,
1772             cleanup_optimizers=cleanup_optimizers)
1773         def add_append_list(l1, l2):
1774             l = copy.copy(l1)
1775             for idx, nb in enumerate(l2):
1776                 if idx &lt; len(l):
1777                     l[idx] += nb
1778                 else:
1779                     l.append(nb)
1780             return l
1781         loop_timing = add_append_list(prof1[1], prof2[1])
1782         loop_process_count = list(prof1[2])
1783         global_sub_profs = []
1784         final_sub_profs = []
1785         cleanup_sub_profs = []
1786         for i in range(min(len(loop_process_count), len(prof2[2]))):
1787             process_count = loop_process_count[i]
1788             for process, count in iteritems(prof2[2][i]):
1789                 if process in process_count:
1790                     process_count[process] += count
1791                 else:
1792                     process_count[process] = count
1793             def merge(opts, attr, idx):
1794                 tmp = []
1795                 for opt in opts:
1796                     o1 = getattr(prof1[0], attr)
1797                     o2 = getattr(prof2[0], attr)
1798                     if opt in o1 and opt in o2:
1799                         p1 = prof1[idx][i][o1.index(opt)]
1800                         p2 = prof2[idx][i][o2.index(opt)]
1801                         m = None
1802                         if hasattr(opt, 'merge_profile'):
1803                             m = opt.merge_profile(p1, p2)
1804                     elif opt in o1:
1805                         m = prof1[idx][i][o1.index(opt)]
1806                     else:
1807                         m = prof2[idx][i][o2.index(opt)]
1808                     tmp.append(m)
1809                 return tmp
1810             global_sub_profs.append(merge(global_optimizers, 'global_optimizers', 9))
1811             final_sub_profs.append(merge(final_optimizers, 'final_optimizers', 10))
1812             cleanup_sub_profs.append(merge(cleanup_optimizers, 'cleanup_optimizers', 11))
1813         loop_process_count.extend(prof1[2][len(loop_process_count):])
1814         global_sub_profs.extend(prof1[9][len(global_sub_profs):])
1815         final_sub_profs.extend(prof1[10][len(final_sub_profs):])
1816         cleanup_sub_profs.extend(prof1[11][len(cleanup_sub_profs):])
1817         global_sub_profs.extend(prof2[9][len(loop_process_count):])
1818         final_sub_profs.extend(prof2[10][len(loop_process_count):])
1819         cleanup_sub_profs.extend(prof2[11][len(loop_process_count):])
1820         max_nb_nodes = max(prof1[3], prof2[3])
1821         global_opt_timing = add_append_list(prof1[4], prof2[4])
1822         nb_nodes = add_append_list(prof1[5], prof2[5])
1823         time_opts = merge_dict(prof1[6], prof2[6])
1824         io_toposort_timing = add_append_list(prof1[7], prof2[7])
1825         assert (len(loop_timing) == len(global_opt_timing) ==
1826                 len(global_sub_profs) ==
1827                 len(io_toposort_timing) == len(nb_nodes))
1828         assert len(loop_timing) == max(len(prof1[1]), len(prof2[1]))
1829         node_created = merge_dict(prof1[8], prof2[8])
1830         return (new_opt,
1831                 loop_timing,
1832                 loop_process_count,
1833                 max_nb_nodes,
1834                 global_opt_timing,
1835                 nb_nodes,
1836                 time_opts,
1837                 io_toposort_timing,
1838                 node_created,
1839                 global_sub_profs,
1840                 final_sub_profs,
1841                 cleanup_sub_profs)
1842 def _check_chain(r, chain):
1843     chain = list(reversed(chain))
1844     while chain:
1845         elem = chain.pop()
1846         if elem is None:
1847             if r.owner is not None:
1848                 return False
1849         elif r.owner is None:
1850             return False
1851         elif isinstance(elem, op.Op):
1852             if not r.owner.op == elem:
1853                 return False
1854         else:
1855             try:
1856                 if (issubclass(elem, op.Op) and
1857                         not isinstance(r.owner.op, elem)):
1858                     return False
1859             except TypeError:
1860                 return False
1861         if chain:
1862             r = r.owner.inputs[chain.pop()]
1863     return (r is not None)
1864 def check_chain(r, *chain):
1865     if isinstance(r, graph.Apply):
1866         r = r.outputs[0]
1867     return _check_chain(r, reduce(list.__iadd__, ([x, 0] for x in chain)))
1868 def pre_greedy_local_optimizer(list_optimizations, out):
1869     def local_recursive_function(list_opt, out, optimized_vars, depth):
1870         if not getattr(out, 'owner', None):
1871             return [out], optimized_vars
1872         node = out.owner
1873         if hasattr(node, 'fgraph'):
1874             return node.outputs, optimized_vars
1875         for idx, inp in enumerate(node.inputs):
1876             if inp in optimized_vars:
1877                 nw_in = optimized_vars[inp]
1878             else:
1879                 if inp.owner:
1880                     outs, optimized_vars = local_recursive_function(
1881                         list_opt,
1882                         inp,
1883                         optimized_vars,
1884                         depth + 1)
1885                     for k, v in zip(inp.owner.outputs, outs):
1886                         optimized_vars[k] = v
1887                     nw_in = outs[inp.owner.outputs.index(inp)]
1888                 else:
1889                     nw_in = inp
1890                     optimized_vars[inp] = inp
1891             node.inputs[idx] = nw_in
1892         results = node.outputs
1893         for opt in list_opt:
1894             ret = opt.transform(node)
1895             if ret is not False and ret is not None:
1896                 assert len(ret) == len(node.outputs), opt
1897                 for k, v in zip(node.outputs, ret):
1898                     optimized_vars[k] = v
1899                 results = ret
1900                 if ret[0].owner:
1901                     node = out.owner
1902                 else:
1903                     break
1904         return results, optimized_vars
1905     if out.owner:
1906         out_index = out.owner.outputs.index(out)
1907     else:
1908         out_index = 0
1909     final_outs, optimized_nodes = local_recursive_function(
1910         list_optimizations, out, {}, 0)
1911     return final_outs[out_index]
1912 def copy_stack_trace(from_var, to_var):
1913     tr = []
1914     if type(from_var) is list:
1915         for v in from_var:
1916             tr += getattr(v.tag, 'trace', [])
1917     else:
1918         tr = getattr(from_var.tag, 'trace', [])
1919     if tr and isinstance(tr[0], tuple):
1920         tr = [tr]
1921     if type(to_var) is list:
1922         for v in to_var:
1923             v.tag.trace = getattr(v.tag, 'trace', []) + tr
1924     else:
1925         to_var.tag.trace = getattr(to_var.tag, 'trace', []) + tr
1926     return to_var
1927 @contextlib.contextmanager
1928 def inherit_stack_trace(from_var):
1929     with graph.nodes_constructed() as new_nodes:
1930         yield
1931     copy_stack_trace(from_var, new_nodes)
1932 def check_stack_trace(f_or_fgraph, ops_to_check='last', bug_print='raise'):
1933     if isinstance(f_or_fgraph, theano.compile.function_module.Function):
1934         fgraph = f_or_fgraph.maker.fgraph
1935     elif isinstance(f_or_fgraph, theano.gof.fg.FunctionGraph):
1936         fgraph = f_or_fgraph
1937     else:
1938         raise ValueError('The type of f_or_fgraph is not supported')
1939     if (isinstance(ops_to_check, theano.gof.Op) or
1940             (inspect.isclass(ops_to_check) and
1941                 issubclass(ops_to_check, theano.gof.Op))):
1942         ops_to_check = (ops_to_check,)
1943     if isinstance(ops_to_check, string_types):
1944         if ops_to_check == 'last':
1945             apply_nodes_to_check = [fgraph.outputs[i].owner for i in range(
1946                 len(fgraph.outputs))]
1947         elif ops_to_check == 'all':
1948             apply_nodes_to_check = fgraph.apply_nodes
1949         else:
1950             raise ValueError('The string ops_to_check is not recognised')
1951     elif isinstance(ops_to_check, (tuple, list)):
1952         op_instances = []
1953         op_classes = []
1954         for obj in ops_to_check:
1955             if isinstance(obj, theano.gof.Op):
1956                 op_instances.append(obj)
1957             else:
1958                 op_classes.append(obj)
1959         op_classes = tuple(op_classes)
1960         apply_nodes_to_check = (
1961             [node for node in fgraph.apply_nodes if node.op in ops_to_check] +
1962             [node for node in fgraph.apply_nodes
1963              if isinstance(node.op, op_classes) or
1964              (hasattr(node.op, 'scalar_op') and
1965               isinstance(node.op.scalar_op, op_classes))])
1966     elif hasattr(ops_to_check, '__call__'):
1967         apply_nodes_to_check = [node for node in fgraph.apply_nodes
1968                                 if ops_to_check(node)]
1969     else:
1970         raise ValueError('ops_to_check does not have the right type')
1971     if not apply_nodes_to_check:
1972         msg = 'Provided op instances/classes are not in the graph or the ' \
1973               'graph is empty'
1974         if bug_print == 'warn':
1975             warnings.warn(msg)
1976         elif bug_print == 'raise':
1977             raise Exception(msg)
1978         elif bug_print == 'ignore':
1979             pass
1980         else:
1981             raise ValueError('The string bug_print is not recognised')
1982     for node in apply_nodes_to_check:
1983         for output in node.outputs:
1984             if (not hasattr(output.tag, 'trace') or not output.tag.trace):
1985                 return False
1986     return True
1987 class CheckStrackTraceFeature(object):
1988     def on_import(self, fgraph, node, reason):
1989         if theano.config.check_stack_trace != 'off' and not check_stack_trace(fgraph, 'all'):
1990             if theano.config.check_stack_trace == 'raise':
1991                     raise AssertionError(
1992                         'Empty stack trace! The optimization that inserted this variable is ' + str(reason))
1993             elif theano.config.check_stack_trace in ['log', 'warn']:
1994                 apply_nodes_to_check = fgraph.apply_nodes
1995                 for node in apply_nodes_to_check:
1996                     for output in node.outputs:
1997                         if not hasattr(output.tag, 'trace') or not output.tag.trace:
1998                             output.tag.trace = [[('', 0, 'Empty stack trace! The optimization that' +
1999                                                  'inserted this variable is ' + str(reason), '')]]
2000                 if theano.config.check_stack_trace == 'warn':
2001                         warnings.warn(
2002                             'Empty stack trace! The optimization that inserted this variable is' + str(reason))
2003 class CheckStackTraceOptimization(Optimizer):
2004     def add_requirements(self, fgraph):
2005         if not hasattr(fgraph, 'CheckStrackTraceFeature'):
2006             fgraph.attach_feature(CheckStrackTraceFeature())
2007     def apply(self, fgraph):
2008         pass
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
