<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for template_1.py &amp; vsphere.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for template_1.py &amp; vsphere.py
      </h3>
<h1 align="center">
        0.4%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>template_1.py (9.540636%)<th>vsphere.py (0.25378326%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(115-120)<td><a href="#" name="0">(4195-4208)</a><td align="center"><font color="#ff0000">14</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(4-16)<td><a href="#" name="1">(195-225)</a><td align="center"><font color="#ec0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>template_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 <a name="1"></a>"""
2 Manage basic template commands
3 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import codecs
4 import io
5 import logging
6 import os
7 import time
8 import salt.utils.data
9 import salt.utils.files
10 import salt.utils.sanitizers
11 import salt.utils.stringio
12 import salt.utils.versions
13 log = logging.getLogger(</b></font>__name__)
14 SLS_ENCODING = "utf-8"  # this one has no BOM.
15 SLS_ENCODER = codecs.getencoder(SLS_ENCODING)
16 def compile_template(
17     template,
18     renderers,
19     default,
20     blacklist,
21     whitelist,
22     saltenv="base",
23     sls="",
24     input_data="",
25     context=None,
26     **kwargs
27 ):
28     ret = {}
29     log.debug("compile template: %s", template)
30     if "env" in kwargs:
31         kwargs.pop("env")
32     if template != ":string:":
33         if not isinstance(template, str):
34             log.error("Template was specified incorrectly: %s", template)
35             return ret
36         if not os.path.isfile(template):
37             log.error("Template does not exist: %s", template)
38             return ret
39         if salt.utils.files.is_empty(template):
40             log.debug("Template is an empty file: %s", template)
41             return ret
42         with codecs.open(template, encoding=SLS_ENCODING) as ifile:
43             input_data = ifile.read()
44             if not input_data.strip():
45                 log.error("Template is nothing but whitespace: %s", template)
46                 return ret
47     render_pipe = template_shebang(
48         template, renderers, default, blacklist, whitelist, input_data
49     )
50     windows_newline = "\r\n" in input_data
51     input_data = io.StringIO(input_data)
52     for render, argline in render_pipe:
53         if salt.utils.stringio.is_readable(input_data):
54             input_data.seek(0)  # pylint: disable=no-member
55         render_kwargs = dict(renderers=renderers, tmplpath=template)
56         if context:
57             render_kwargs["context"] = context
58         render_kwargs.update(kwargs)
59         if argline:
60             render_kwargs["argline"] = argline
61         start = time.time()
62         ret = render(input_data, saltenv, sls, **render_kwargs)
63         log.profile(
64             "Time (in seconds) to render '%s' using '%s' renderer: %s",
65             template,
66             render.__module__.split(".")[-1],
67             time.time() - start,
68         )
69         if ret is None:
70             time.sleep(0.01)
71             ret = render(input_data, saltenv, sls, **render_kwargs)
72         input_data = ret
73         if log.isEnabledFor(logging.GARBAGE):  # pylint: disable=no-member
74             if salt.utils<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.stringio.is_readable(ret):
75                 log.debug(
76                     "Rendered data from file: %s:\n%s",
77                     template,
78                     salt.utils.sanitizers.mask_args_value(
79                         salt.utils.data.decode(ret.</b></font>read()), kwargs.get("mask_value")
80                     ),
81                 )  # pylint: disable=no-member
82                 ret.seek(0)  # pylint: disable=no-member
83     if windows_newline:
84         if salt.utils.stringio.is_readable(ret):
85             is_stringio = True
86             contents = ret.read()
87         else:
88             is_stringio = False
89             contents = ret
90         if isinstance(contents, str):
91             if "\r\n" not in contents:
92                 contents = contents.replace("\n", "\r\n")
93                 ret = io.StringIO(contents) if is_stringio else contents
94             else:
95                 if is_stringio:
96                     ret.seek(0)
97     return ret
98 def compile_template_str(template, renderers, default, blacklist, whitelist):
99     fn_ = salt.utils.files.mkstemp()
100     with salt.utils.files.fopen(fn_, "wb") as ofile:
101         ofile.write(SLS_ENCODER(template)[0])
102     return compile_template(fn_, renderers, default, blacklist, whitelist)
103 def template_shebang(template, renderers, default, blacklist, whitelist, input_data):
104     line = ""
105     if template == ":string:":
106         line = input_data.split()[0]
107     else:
108         with salt.utils.files.fopen(template, "r") as ifile:
109             line = salt.utils.stringutils.to_unicode(ifile.readline())
110     if line.startswith("#!") and not line.startswith("#!/"):
111         return check_render_pipe_str(line.strip()[2:], renderers, blacklist, whitelist)
112     else:
113         return check_render_pipe_str(default, renderers, blacklist, whitelist)
114 OLD_STYLE_RENDERERS = {}
115 for comb in (
116     "yaml_jinja",
117     "yaml_mako",
118     "yaml_wempy",
119     "json_jinja",
120     "json_mako",
121     "json_wempy",
122     "yamlex_jinja",
123     "yamlexyamlex_mako",
124     "yamlexyamlex_wempy",
125 ):
126     fmt, tmpl = comb.split("_")
127     OLD_STYLE_RENDERERS[comb] = "{}|{}".format(tmpl, fmt)
128 def check_render_pipe_str(pipestr, renderers, blacklist, whitelist):
129     if pipestr is None:
130         return []
131     parts = [r.strip() for r in pipestr.split("|")]
132     results = []
133     try:
134         if parts[0] == pipestr and pipestr in OLD_STYLE_RENDERERS:
135             parts = OLD_STYLE_RENDERERS[pipestr].split("|")
136         for part in parts:
137             name, argline = (part + " ").split(" ", 1)
138             if whitelist and name not in whitelist or blacklist and name in blacklist:
139                 log.warning(
140                     'The renderer "%s" is disallowed by configuration and '
141                     "will be skipped.",
142                     name,
143                 )
144                 continue
145             results.append((renderers[name], argline.strip()))
146         return results
147     except KeyError:
148         log.error('The renderer "%s" is not available', pipestr)
149         return []
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>vsphere.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 import datetime
2 import logging
3 import sys
4 from functools import wraps
5 import salt.utils.args
6 import salt.utils.dictupdate as dictupdate
7 <a name="1"></a>import salt.utils.http
8 import salt.utils.path
9 import salt.utils.pbm
10 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import salt.utils.vmware
11 import salt.utils.vsan
12 from salt.config.schemas.esxcluster import (
13     ESXClusterConfigSchema,
14     ESXClusterEntitySchema,
15 )
16 from salt.config.schemas.esxi import (
17     DiskGroupsDiskIdSchema,
18     SimpleHostCacheSchema,
19     VmfsDatastoreSchema,
20 )
21 from salt.config.schemas.esxvm import (
22     ESXVirtualMachineDeleteSchema,
23     ESXVirtualMachineUnregisterSchema,
24 )
25 from salt.config.schemas.vcenter import VCenterEntitySchema
26 from salt.exceptions import (
27     ArgumentValueError,
28     CommandExecutionError,
29     InvalidConfigError,
30     InvalidEntityError,
31     VMwareApiError,
32     VMwareObjectExistsError,
33     VMwareObjectRetrievalError,
34     VMwareSaltError,
35 )
36 from salt.utils.decorators import depends, ignores_kwargs
37 from salt.utils.dictdiffer import recursive_diff
38 from salt.utils.listdiffer import list_diff
39 log = logging.getLogger(</b></font>__name__)
40 try:
41     import jsonschema
42     HAS_JSONSCHEMA = True
43 except ImportError:
44     HAS_JSONSCHEMA = False
45 try:
46     from pyVmomi import (
47         vim,
48         vmodl,
49         pbm,
50         VmomiSupport,
51     )
52     if (
53         "vim25/6.0" in VmomiSupport.versionMap
54         and sys.version_info &gt; (2, 7)
55         and sys.version_info &lt; (2, 7, 9)
56     ):
57         log.debug(
58             "pyVmomi not loaded: Incompatible versions of Python. See Issue #29537."
59         )
60         raise ImportError()
61     HAS_PYVMOMI = True
62 except ImportError:
63     HAS_PYVMOMI = False
64 try:
65     from com.vmware.cis.tagging_client import Category, CategoryModel
66     from com.vmware.cis.tagging_client import Tag, TagModel, TagAssociation
67     from com.vmware.vcenter_client import Cluster
68     from com.vmware.vapi.std_client import DynamicID
69     from com.vmware.vapi.std.errors_client import (
70         AlreadyExists,
71         InvalidArgument,
72         NotFound,
73         Unauthenticated,
74         Unauthorized,
75     )
76     vsphere_errors = (
77         AlreadyExists,
78         InvalidArgument,
79         NotFound,
80         Unauthenticated,
81         Unauthorized,
82     )
83     HAS_VSPHERE_SDK = True
84 except ImportError:
85     HAS_VSPHERE_SDK = False
86 esx_cli = salt.utils.path.which("esxcli")
87 if esx_cli:
88     HAS_ESX_CLI = True
89 else:
90     HAS_ESX_CLI = False
91 __virtualname__ = "vsphere"
92 __proxyenabled__ = ["esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm"]
93 def __virtual__():
94     return __virtualname__
95 def get_proxy_type():
96     if __pillar__.get("proxy", {}).get("proxytype"):
97         return __pillar__["proxy"]["proxytype"]
98     if __opts__.get("proxy", {}).get("proxytype"):
99         return __opts__["proxy"]["proxytype"]
100     return "&lt;undefined&gt;"
101 def _get_proxy_connection_details():
102     proxytype = get_proxy_type()
103     if proxytype == "esxi":
104         details = __salt__["esxi.get_details"]()
105     elif proxytype == "esxcluster":
106         details = __salt__["esxcluster.get_details"]()
107     elif proxytype == "esxdatacenter":
108         details = __salt__["esxdatacenter.get_details"]()
109     elif proxytype == "vcenter":
110         details = __salt__["vcenter.get_details"]()
111     elif proxytype == "esxvm":
112         details = __salt__["esxvm.get_details"]()
113     else:
114         raise CommandExecutionError("'{}' proxy is not supported".format(proxytype))
115     proxy_details = [
116         details.get("vcenter") if "vcenter" in details else details.get("host"),
117         details.get("username"),
118         details.get("password"),
119         details.get("protocol"),
120         details.get("port"),
121         details.get("mechanism"),
122         details.get("principal"),
123         details.get("domain"),
124     ]
125     if "verify_ssl" in details:
126         proxy_details.append(details.get("verify_ssl"))
127     return tuple(proxy_details)
128 def _supports_proxies(*proxy_types):
129     def _supports_proxies_(fn):
130         @wraps(fn)
131         def __supports_proxies_(*args, **kwargs):
132             proxy_type = get_proxy_type()
133             if proxy_type not in proxy_types:
134                 raise CommandExecutionError(
135                     "'{}' proxy is not supported by function {}".format(
136                         proxy_type, fn.__name__
137                     )
138                 )
139             return fn(*args, **salt.utils.args.clean_kwargs(**kwargs))
140         return __supports_proxies_
141     return _supports_proxies_
142 def _gets_service_instance_via_proxy(fn):
143     fn_name = fn.__name__
144     (
145         arg_names,
146         args_name,
147         kwargs_name,
148         default_values,
149     ) = salt.utils.args.get_function_argspec(fn)
150     default_values = default_values if default_values is not None else []
151     @wraps(fn)
152     def _gets_service_instance_via_proxy_(*args, **kwargs):
153         if "service_instance" not in arg_names and not kwargs_name:
154             raise CommandExecutionError(
155                 "Function {} must have either a 'service_instance', or a "
156                 "'**kwargs' type parameter".format(fn_name)
157             )
158         connection_details = _get_proxy_connection_details()
159         local_service_instance = None
160         if "service_instance" in arg_names:
161             idx = arg_names.index("service_instance")
162             if idx &gt;= len(arg_names) - len(default_values):
163                 if len(args) &gt; idx:
164                     if not args[idx]:
165                         local_service_instance = salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
166                             *connection_details
167                         )
168                         args = list(args)
169                         args[idx] = local_service_instance
170                 else:
171                     if not kwargs.get("service_instance"):
172                         local_service_instance = salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
173                             *connection_details
174                         )
175                         kwargs["service_instance"] = local_service_instance
176         else:
177             if not kwargs.get("service_instance"):
178                 local_service_instance = salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
179                     *connection_details
180                 )
181                 kwargs["service_instance"] = local_service_instance
182         try:
183             ret = fn(*args, **salt.utils.args.clean_kwargs(**kwargs))
184             if local_service_instance:
185                 salt.utils.vmware.disconnect(local_service_instance)
186             return ret
187         except Exception as e:  # pylint: disable=broad-except
188             if local_service_instance:
189                 salt.utils.vmware.disconnect(local_service_instance)
190             raise
191     return _gets_service_instance_via_proxy_
192 @depends(HAS_PYVMOMI)
193 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm")
194 def get_service_instance_via_proxy(service_instance=None):
195     connection_details = _get_proxy_connection_details()
196     return salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
197         *connection_details
198     )
199 @depends(HAS_PYVMOMI)
200 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm")
201 def disconnect(service_instance):
202     salt.utils.vmware.disconnect(service_instance)
203     return True
204 @depends(HAS_ESX_CLI)
205 def esxcli_cmd(
206     cmd_str,
207     host=None,
208     username=None,
209     password=None,
210     protocol=None,
211     port=None,
212     esxi_hosts=None,
213     credstore=None,
214 ):
215     ret = {}
216     if esxi_hosts:
217         if not isinstance(esxi_hosts, list):
218             raise CommandExecutionError("'esxi_hosts' must be a list.")
219         for esxi_host in esxi_hosts:
220             response = salt.utils.vmware.esxcli(
221                 host,
222                 username,
223                 password,
224                 cmd_str,
225                 protocol=protocol,
226                 port=port,
227                 esxi_host=esxi_host,
228                 credstore=credstore,
229             )
230             if response["retcode"] != 0:
231                 ret.update({esxi_host: {"Error": response.get("stdout")}})
232             else:
233                 ret.update({esxi_host: response})
234     else:
235         response = salt.utils.vmware.esxcli(
236             host,
237             username,
238             password,
239             cmd_str,
240             protocol=protocol,
241             port=port,
242             credstore=credstore,
243         )
244         if response["retcode"] != 0:
245             ret.update({host: {"Error": response.get("stdout")}})
246         else:
247             ret.update({host: response})
248     return ret
249 @depends(HAS_ESX_CLI)
250 def get_coredump_network_config(
251     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
252 ):
253     cmd = "system coredump network get"
254     ret = {}
255     if esxi_hosts:
256         if not isinstance(esxi_hosts, list):
257             raise CommandExecutionError("'esxi_hosts' must be a list.")
258         for esxi_host in esxi_hosts:
259             response = salt.utils.vmware.esxcli(
260                 host,
261                 username,
262                 password,
263                 cmd,
264                 protocol=protocol,
265                 port=port,
266                 esxi_host=esxi_host,
267                 credstore=credstore,
268             )
269             if response["retcode"] != 0:
270                 ret.update({esxi_host: {"Error": response.get("stdout")}})
271             else:
272                 ret.update(
273                     {esxi_host: {"Coredump Config": _format_coredump_stdout(response)}}
274                 )
275     else:
276         response = salt.utils.vmware.esxcli(
277             host,
278             username,
279             password,
280             cmd,
281             protocol=protocol,
282             port=port,
283             credstore=credstore,
284         )
285         if response["retcode"] != 0:
286             ret.update({host: {"Error": response.get("stdout")}})
287         else:
288             stdout = _format_coredump_stdout(response)
289             ret.update({host: {"Coredump Config": stdout}})
290     return ret
291 @depends(HAS_ESX_CLI)
292 def coredump_network_enable(
293     host,
294     username,
295     password,
296     enabled,
297     protocol=None,
298     port=None,
299     esxi_hosts=None,
300     credstore=None,
301 ):
302     if enabled:
303         enable_it = 1
304     else:
305         enable_it = 0
306     cmd = "system coredump network set -e {}".format(enable_it)
307     ret = {}
308     if esxi_hosts:
309         if not isinstance(esxi_hosts, list):
310             raise CommandExecutionError("'esxi_hosts' must be a list.")
311         for esxi_host in esxi_hosts:
312             response = salt.utils.vmware.esxcli(
313                 host,
314                 username,
315                 password,
316                 cmd,
317                 protocol=protocol,
318                 port=port,
319                 esxi_host=esxi_host,
320                 credstore=credstore,
321             )
322             if response["retcode"] != 0:
323                 ret.update({esxi_host: {"Error": response.get("stdout")}})
324             else:
325                 ret.update({esxi_host: {"Coredump Enabled": enabled}})
326     else:
327         response = salt.utils.vmware.esxcli(
328             host,
329             username,
330             password,
331             cmd,
332             protocol=protocol,
333             port=port,
334             credstore=credstore,
335         )
336         if response["retcode"] != 0:
337             ret.update({host: {"Error": response.get("stdout")}})
338         else:
339             ret.update({host: {"Coredump Enabled": enabled}})
340     return ret
341 @depends(HAS_ESX_CLI)
342 def set_coredump_network_config(
343     host,
344     username,
345     password,
346     dump_ip,
347     protocol=None,
348     port=None,
349     host_vnic="vmk0",
350     dump_port=6500,
351     esxi_hosts=None,
352     credstore=None,
353 ):
354     cmd = "system coredump network set -v {} -i {} -o {}".format(
355         host_vnic, dump_ip, dump_port
356     )
357     ret = {}
358     if esxi_hosts:
359         if not isinstance(esxi_hosts, list):
360             raise CommandExecutionError("'esxi_hosts' must be a list.")
361         for esxi_host in esxi_hosts:
362             response = salt.utils.vmware.esxcli(
363                 host,
364                 username,
365                 password,
366                 cmd,
367                 protocol=protocol,
368                 port=port,
369                 esxi_host=esxi_host,
370                 credstore=credstore,
371             )
372             if response["retcode"] != 0:
373                 response["success"] = False
374             else:
375                 response["success"] = True
376             ret.update({esxi_host: response})
377     else:
378         response = salt.utils.vmware.esxcli(
379             host,
380             username,
381             password,
382             cmd,
383             protocol=protocol,
384             port=port,
385             credstore=credstore,
386         )
387         if response["retcode"] != 0:
388             response["success"] = False
389         else:
390             response["success"] = True
391         ret.update({host: response})
392     return ret
393 @depends(HAS_ESX_CLI)
394 def get_firewall_status(
395     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
396 ):
397     cmd = "network firewall ruleset list"
398     ret = {}
399     if esxi_hosts:
400         if not isinstance(esxi_hosts, list):
401             raise CommandExecutionError("'esxi_hosts' must be a list.")
402         for esxi_host in esxi_hosts:
403             response = salt.utils.vmware.esxcli(
404                 host,
405                 username,
406                 password,
407                 cmd,
408                 protocol=protocol,
409                 port=port,
410                 esxi_host=esxi_host,
411                 credstore=credstore,
412             )
413             if response["retcode"] != 0:
414                 ret.update(
415                     {
416                         esxi_host: {
417                             "Error": response["stdout"],
418                             "success": False,
419                             "rulesets": None,
420                         }
421                     }
422                 )
423             else:
424                 ret.update({esxi_host: _format_firewall_stdout(response)})
425     else:
426         response = salt.utils.vmware.esxcli(
427             host,
428             username,
429             password,
430             cmd,
431             protocol=protocol,
432             port=port,
433             credstore=credstore,
434         )
435         if response["retcode"] != 0:
436             ret.update(
437                 {
438                     host: {
439                         "Error": response["stdout"],
440                         "success": False,
441                         "rulesets": None,
442                     }
443                 }
444             )
445         else:
446             ret.update({host: _format_firewall_stdout(response)})
447     return ret
448 @depends(HAS_ESX_CLI)
449 def enable_firewall_ruleset(
450     host,
451     username,
452     password,
453     ruleset_enable,
454     ruleset_name,
455     protocol=None,
456     port=None,
457     esxi_hosts=None,
458     credstore=None,
459 ):
460     cmd = "network firewall ruleset set --enabled {} --ruleset-id={}".format(
461         ruleset_enable, ruleset_name
462     )
463     ret = {}
464     if esxi_hosts:
465         if not isinstance(esxi_hosts, list):
466             raise CommandExecutionError("'esxi_hosts' must be a list.")
467         for esxi_host in esxi_hosts:
468             response = salt.utils.vmware.esxcli(
469                 host,
470                 username,
471                 password,
472                 cmd,
473                 protocol=protocol,
474                 port=port,
475                 esxi_host=esxi_host,
476                 credstore=credstore,
477             )
478             ret.update({esxi_host: response})
479     else:
480         response = salt.utils.vmware.esxcli(
481             host,
482             username,
483             password,
484             cmd,
485             protocol=protocol,
486             port=port,
487             credstore=credstore,
488         )
489         ret.update({host: response})
490     return ret
491 @depends(HAS_ESX_CLI)
492 def syslog_service_reload(
493     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
494 ):
495     cmd = "system syslog reload"
496     ret = {}
497     if esxi_hosts:
498         if not isinstance(esxi_hosts, list):
499             raise CommandExecutionError("'esxi_hosts' must be a list.")
500         for esxi_host in esxi_hosts:
501             response = salt.utils.vmware.esxcli(
502                 host,
503                 username,
504                 password,
505                 cmd,
506                 protocol=protocol,
507                 port=port,
508                 esxi_host=esxi_host,
509                 credstore=credstore,
510             )
511             ret.update({esxi_host: response})
512     else:
513         response = salt.utils.vmware.esxcli(
514             host,
515             username,
516             password,
517             cmd,
518             protocol=protocol,
519             port=port,
520             credstore=credstore,
521         )
522         ret.update({host: response})
523     return ret
524 @depends(HAS_ESX_CLI)
525 def set_syslog_config(
526     host,
527     username,
528     password,
529     syslog_config,
530     config_value,
531     protocol=None,
532     port=None,
533     firewall=True,
534     reset_service=True,
535     esxi_hosts=None,
536     credstore=None,
537 ):
538     ret = {}
539     if firewall and syslog_config == "loghost":
540         if esxi_hosts:
541             if not isinstance(esxi_hosts, list):
542                 raise CommandExecutionError("'esxi_hosts' must be a list.")
543             for esxi_host in esxi_hosts:
544                 response = enable_firewall_ruleset(
545                     host,
546                     username,
547                     password,
548                     ruleset_enable=True,
549                     ruleset_name="syslog",
550                     protocol=protocol,
551                     port=port,
552                     esxi_hosts=[esxi_host],
553                     credstore=credstore,
554                 ).get(esxi_host)
555                 if response["retcode"] != 0:
556                     ret.update(
557                         {
558                             esxi_host: {
559                                 "enable_firewall": {
560                                     "message": response["stdout"],
561                                     "success": False,
562                                 }
563                             }
564                         }
565                     )
566                 else:
567                     ret.update({esxi_host: {"enable_firewall": {"success": True}}})
568         else:
569             response = enable_firewall_ruleset(
570                 host,
571                 username,
572                 password,
573                 ruleset_enable=True,
574                 ruleset_name="syslog",
575                 protocol=protocol,
576                 port=port,
577                 credstore=credstore,
578             ).get(host)
579             if response["retcode"] != 0:
580                 ret.update(
581                     {
582                         host: {
583                             "enable_firewall": {
584                                 "message": response["stdout"],
585                                 "success": False,
586                             }
587                         }
588                     }
589                 )
590             else:
591                 ret.update({host: {"enable_firewall": {"success": True}}})
592     if esxi_hosts:
593         if not isinstance(esxi_hosts, list):
594             raise CommandExecutionError("'esxi_hosts' must be a list.")
595         for esxi_host in esxi_hosts:
596             response = _set_syslog_config_helper(
597                 host,
598                 username,
599                 password,
600                 syslog_config,
601                 config_value,
602                 protocol=protocol,
603                 port=port,
604                 reset_service=reset_service,
605                 esxi_host=esxi_host,
606                 credstore=credstore,
607             )
608             if ret.get(esxi_host) is None:
609                 ret.update({esxi_host: {}})
610             ret[esxi_host].update(response)
611     else:
612         response = _set_syslog_config_helper(
613             host,
614             username,
615             password,
616             syslog_config,
617             config_value,
618             protocol=protocol,
619             port=port,
620             reset_service=reset_service,
621             credstore=credstore,
622         )
623         if ret.get(host) is None:
624             ret.update({host: {}})
625         ret[host].update(response)
626     return ret
627 @depends(HAS_ESX_CLI)
628 def get_syslog_config(
629     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
630 ):
631     cmd = "system syslog config get"
632     ret = {}
633     if esxi_hosts:
634         if not isinstance(esxi_hosts, list):
635             raise CommandExecutionError("'esxi_hosts' must be a list.")
636         for esxi_host in esxi_hosts:
637             response = salt.utils.vmware.esxcli(
638                 host,
639                 username,
640                 password,
641                 cmd,
642                 protocol=protocol,
643                 port=port,
644                 esxi_host=esxi_host,
645                 credstore=credstore,
646             )
647             ret.update({esxi_host: _format_syslog_config(response)})
648     else:
649         response = salt.utils.vmware.esxcli(
650             host,
651             username,
652             password,
653             cmd,
654             protocol=protocol,
655             port=port,
656             credstore=credstore,
657         )
658         ret.update({host: _format_syslog_config(response)})
659     return ret
660 @depends(HAS_ESX_CLI)
661 def reset_syslog_config(
662     host,
663     username,
664     password,
665     protocol=None,
666     port=None,
667     syslog_config=None,
668     esxi_hosts=None,
669     credstore=None,
670 ):
671     if not syslog_config:
672         raise CommandExecutionError(
673             "The 'reset_syslog_config' function requires a 'syslog_config' setting."
674         )
675     valid_resets = [
676         "logdir",
677         "loghost",
678         "default-rotate",
679         "default-size",
680         "default-timeout",
681         "logdir-unique",
682     ]
683     cmd = "system syslog config set --reset="
684     if "," in syslog_config:
685         resets = [ind_reset.strip() for ind_reset in syslog_config.split(",")]
686     elif syslog_config == "all":
687         resets = valid_resets
688     else:
689         resets = [syslog_config]
690     ret = {}
691     if esxi_hosts:
692         if not isinstance(esxi_hosts, list):
693             raise CommandExecutionError("'esxi_hosts' must be a list.")
694         for esxi_host in esxi_hosts:
695             response_dict = _reset_syslog_config_params(
696                 host,
697                 username,
698                 password,
699                 cmd,
700                 resets,
701                 valid_resets,
702                 protocol=protocol,
703                 port=port,
704                 esxi_host=esxi_host,
705                 credstore=credstore,
706             )
707             ret.update({esxi_host: response_dict})
708     else:
709         response_dict = _reset_syslog_config_params(
710             host,
711             username,
712             password,
713             cmd,
714             resets,
715             valid_resets,
716             protocol=protocol,
717             port=port,
718             credstore=credstore,
719         )
720         ret.update({host: response_dict})
721     return ret
722 @ignores_kwargs("credstore")
723 def upload_ssh_key(
724     host,
725     username,
726     password,
727     ssh_key=None,
728     ssh_key_file=None,
729     protocol=None,
730     port=None,
731     certificate_verify=None,
732 ):
733     if protocol is None:
734         protocol = "https"
735     if port is None:
736         port = 443
737     if certificate_verify is None:
738         certificate_verify = True
739     url = "{}://{}:{}/host/ssh_root_authorized_keys".format(protocol, host, port)
740     ret = {}
741     result = None
742     try:
743         if ssh_key:
744             result = salt.utils.http.query(
745                 url,
746                 status=True,
747                 text=True,
748                 method="PUT",
749                 username=username,
750                 password=password,
751                 data=ssh_key,
752                 verify_ssl=certificate_verify,
753             )
754         elif ssh_key_file:
755             result = salt.utils.http.query(
756                 url,
757                 status=True,
758                 text=True,
759                 method="PUT",
760                 username=username,
761                 password=password,
762                 data_file=ssh_key_file,
763                 data_render=False,
764                 verify_ssl=certificate_verify,
765             )
766         if result.get("status") == 200:
767             ret["status"] = True
768         else:
769             ret["status"] = False
770             ret["Error"] = result["error"]
771     except Exception as msg:  # pylint: disable=broad-except
772         ret["status"] = False
773         ret["Error"] = msg
774     return ret
775 @ignores_kwargs("credstore")
776 def get_ssh_key(
777     host, username, password, protocol=None, port=None, certificate_verify=None
778 ):
779     if protocol is None:
780         protocol = "https"
781     if port is None:
782         port = 443
783     if certificate_verify is None:
784         certificate_verify = True
785     url = "{}://{}:{}/host/ssh_root_authorized_keys".format(protocol, host, port)
786     ret = {}
787     try:
788         result = salt.utils.http.query(
789             url,
790             status=True,
791             text=True,
792             method="GET",
793             username=username,
794             password=password,
795             verify_ssl=certificate_verify,
796         )
797         if result.get("status") == 200:
798             ret["status"] = True
799             ret["key"] = result["text"]
800         else:
801             ret["status"] = False
802             ret["Error"] = result["error"]
803     except Exception as msg:  # pylint: disable=broad-except
804         ret["status"] = False
805         ret["Error"] = msg
806     return ret
807 @depends(HAS_PYVMOMI)
808 @ignores_kwargs("credstore")
809 def get_host_datetime(
810     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
811 ):
812     service_instance = salt.utils.vmware.get_service_instance(
813         host=host,
814         username=username,
815         password=password,
816         protocol=protocol,
817         port=port,
818         verify_ssl=verify_ssl,
819     )
820     host_names = _check_hosts(service_instance, host, host_names)
821     ret = {}
822     for host_name in host_names:
823         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
824         date_time_manager = _get_date_time_mgr(host_ref)
825         date_time = date_time_manager.QueryDateTime()
826         ret.update({host_name: date_time})
827     return ret
828 @depends(HAS_PYVMOMI)
829 @ignores_kwargs("credstore")
830 def get_ntp_config(
831     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
832 ):
833     service_instance = salt.utils.vmware.get_service_instance(
834         host=host,
835         username=username,
836         password=password,
837         protocol=protocol,
838         port=port,
839         verify_ssl=verify_ssl,
840     )
841     host_names = _check_hosts(service_instance, host, host_names)
842     ret = {}
843     for host_name in host_names:
844         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
845         ntp_config = host_ref.configManager.dateTimeSystem.dateTimeInfo.ntpConfig.server
846         ret.update({host_name: ntp_config})
847     return ret
848 @depends(HAS_PYVMOMI)
849 @ignores_kwargs("credstore")
850 def get_service_policy(
851     host,
852     username,
853     password,
854     service_name,
855     protocol=None,
856     port=None,
857     host_names=None,
858     verify_ssl=True,
859 ):
860     service_instance = salt.utils.vmware.get_service_instance(
861         host=host,
862         username=username,
863         password=password,
864         protocol=protocol,
865         port=port,
866         verify_ssl=verify_ssl,
867     )
868     valid_services = [
869         "DCUI",
870         "TSM",
871         "SSH",
872         "ssh",
873         "lbtd",
874         "lsassd",
875         "lwiod",
876         "netlogond",
877         "ntpd",
878         "sfcbd-watchdog",
879         "snmpd",
880         "vprobed",
881         "vpxa",
882         "xorg",
883     ]
884     host_names = _check_hosts(service_instance, host, host_names)
885     ret = {}
886     for host_name in host_names:
887         if service_name not in valid_services:
888             ret.update(
889                 {
890                     host_name: {
891                         "Error": "{} is not a valid service name.".format(service_name)
892                     }
893                 }
894             )
895             return ret
896         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
897         services = host_ref.configManager.serviceSystem.serviceInfo.service
898         if service_name == "SSH" or service_name == "ssh":
899             temp_service_name = "TSM-SSH"
900         else:
901             temp_service_name = service_name
902         for service in services:
903             if service.key == temp_service_name:
904                 ret.update({host_name: {service_name: service.policy}})
905                 break
906             else:
907                 msg = "Could not find service '{}' for host '{}'.".format(
908                     service_name, host_name
909                 )
910                 ret.update({host_name: {"Error": msg}})
911         if ret.get(host_name) is None:
912             msg = "'vsphere.get_service_policy' failed for host {}.".format(host_name)
913             log.debug(msg)
914             ret.update({host_name: {"Error": msg}})
915     return ret
916 @depends(HAS_PYVMOMI)
917 @ignores_kwargs("credstore")
918 def get_service_running(
919     host,
920     username,
921     password,
922     service_name,
923     protocol=None,
924     port=None,
925     host_names=None,
926     verify_ssl=True,
927 ):
928     service_instance = salt.utils.vmware.get_service_instance(
929         host=host,
930         username=username,
931         password=password,
932         protocol=protocol,
933         port=port,
934         verify_ssl=verify_ssl,
935     )
936     valid_services = [
937         "DCUI",
938         "TSM",
939         "SSH",
940         "ssh",
941         "lbtd",
942         "lsassd",
943         "lwiod",
944         "netlogond",
945         "ntpd",
946         "sfcbd-watchdog",
947         "snmpd",
948         "vprobed",
949         "vpxa",
950         "xorg",
951     ]
952     host_names = _check_hosts(service_instance, host, host_names)
953     ret = {}
954     for host_name in host_names:
955         if service_name not in valid_services:
956             ret.update(
957                 {
958                     host_name: {
959                         "Error": "{} is not a valid service name.".format(service_name)
960                     }
961                 }
962             )
963             return ret
964         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
965         services = host_ref.configManager.serviceSystem.serviceInfo.service
966         if service_name == "SSH" or service_name == "ssh":
967             temp_service_name = "TSM-SSH"
968         else:
969             temp_service_name = service_name
970         for service in services:
971             if service.key == temp_service_name:
972                 ret.update({host_name: {service_name: service.running}})
973                 break
974             else:
975                 msg = "Could not find service '{}' for host '{}'.".format(
976                     service_name, host_name
977                 )
978                 ret.update({host_name: {"Error": msg}})
979         if ret.get(host_name) is None:
980             msg = "'vsphere.get_service_running' failed for host {}.".format(host_name)
981             log.debug(msg)
982             ret.update({host_name: {"Error": msg}})
983     return ret
984 @depends(HAS_PYVMOMI)
985 @ignores_kwargs("credstore")
986 def get_vmotion_enabled(
987     host,
988     username,
989     password,
990     protocol=None,
991     port=None,
992     host_names=None,
993     verify_ssl=True,
994 ):
995     service_instance = salt.utils.vmware.get_service_instance(
996         host=host,
997         username=username,
998         password=password,
999         protocol=protocol,
1000         port=port,
1001         verify_ssl=verify_ssl,
1002     )
1003     host_names = _check_hosts(service_instance, host, host_names)
1004     ret = {}
1005     for host_name in host_names:
1006         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1007         vmotion_vnic = host_ref.configManager.vmotionSystem.netConfig.selectedVnic
1008         if vmotion_vnic:
1009             ret.update({host_name: {"VMotion Enabled": True}})
1010         else:
1011             ret.update({host_name: {"VMotion Enabled": False}})
1012     return ret
1013 @depends(HAS_PYVMOMI)
1014 @ignores_kwargs("credstore")
1015 def get_vsan_enabled(
1016     host,
1017     username,
1018     password,
1019     protocol=None,
1020     port=None,
1021     host_names=None,
1022     verify_ssl=True,
1023 ):
1024     service_instance = salt.utils.vmware.get_service_instance(
1025         host=host,
1026         username=username,
1027         password=password,
1028         protocol=protocol,
1029         port=port,
1030         verify_ssl=verify_ssl,
1031     )
1032     host_names = _check_hosts(service_instance, host, host_names)
1033     ret = {}
1034     for host_name in host_names:
1035         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1036         vsan_config = host_ref.config.vsanHostConfig
1037         if vsan_config is None:
1038             msg = "VSAN System Config Manager is unset for host '{}'.".format(host_name)
1039             log.debug(msg)
1040             ret.update({host_name: {"Error": msg}})
1041         else:
1042             ret.update({host_name: {"VSAN Enabled": vsan_config.enabled}})
1043     return ret
1044 @depends(HAS_PYVMOMI)
1045 @ignores_kwargs("credstore")
1046 def get_vsan_eligible_disks(
1047     host,
1048     username,
1049     password,
1050     protocol=None,
1051     port=None,
1052     host_names=None,
1053     verify_ssl=True,
1054 ):
1055     service_instance = salt.utils.vmware.get_service_instance(
1056         host=host,
1057         username=username,
1058         password=password,
1059         protocol=protocol,
1060         port=port,
1061         verify_ssl=verify_ssl,
1062     )
1063     host_names = _check_hosts(service_instance, host, host_names)
1064     response = _get_vsan_eligible_disks(service_instance, host, host_names)
1065     ret = {}
1066     for host_name, value in response.items():
1067         error = value.get("Error")
1068         if error:
1069             ret.update({host_name: {"Error": error}})
1070             continue
1071         disks = value.get("Eligible")
1072         if disks and isinstance(disks, list):
1073             disk_names = []
1074             for disk in disks:
1075                 disk_names.append(disk.canonicalName)
1076             ret.update({host_name: {"Eligible": disk_names}})
1077         else:
1078             ret.update({host_name: {"Eligible": disks}})
1079     return ret
1080 @depends(HAS_PYVMOMI)
1081 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm")
1082 @_gets_service_instance_via_proxy
1083 def test_vcenter_connection(service_instance=None):
1084     try:
1085         if salt.utils.vmware.is_connection_to_a_vcenter(service_instance):
1086             return True
1087     except VMwareSaltError:
1088         return False
1089     return False
1090 @depends(HAS_PYVMOMI)
1091 @ignores_kwargs("credstore")
1092 def system_info(
1093     host,
1094     username,
1095     password,
1096     protocol=None,
1097     port=None,
1098     verify_ssl=True,
1099 ):
1100     service_instance = salt.utils.vmware.get_service_instance(
1101         host=host,
1102         username=username,
1103         password=password,
1104         protocol=protocol,
1105         port=port,
1106         verify_ssl=verify_ssl,
1107     )
1108     ret = salt.utils.vmware.get_inventory(service_instance).about.__dict__
1109     if "apiType" in ret:
1110         if ret["apiType"] == "HostAgent":
1111             ret = dictupdate.update(
1112                 ret, salt.utils.vmware.get_hardware_grains(service_instance)
1113             )
1114     return ret
1115 @depends(HAS_PYVMOMI)
1116 @ignores_kwargs("credstore")
1117 def list_datacenters(
1118     host, username, password, protocol=None, port=None, verify_ssl=True
1119 ):
1120     service_instance = salt.utils.vmware.get_service_instance(
1121         host=host,
1122         username=username,
1123         password=password,
1124         protocol=protocol,
1125         port=port,
1126         verify_ssl=verify_ssl,
1127     )
1128     return salt.utils.vmware.list_datacenters(service_instance)
1129 @depends(HAS_PYVMOMI)
1130 @ignores_kwargs("credstore")
1131 def list_clusters(host, username, password, protocol=None, port=None, verify_ssl=True):
1132     service_instance = salt.utils.vmware.get_service_instance(
1133         host=host,
1134         username=username,
1135         password=password,
1136         protocol=protocol,
1137         port=port,
1138         verify_ssl=verify_ssl,
1139     )
1140     return salt.utils.vmware.list_clusters(service_instance)
1141 @depends(HAS_PYVMOMI)
1142 @ignores_kwargs("credstore")
1143 def list_datastore_clusters(
1144     host, username, password, protocol=None, port=None, verify_ssl=True
1145 ):
1146     service_instance = salt.utils.vmware.get_service_instance(
1147         host=host,
1148         username=username,
1149         password=password,
1150         protocol=protocol,
1151         port=port,
1152         verify_ssl=verify_ssl,
1153     )
1154     return salt.utils.vmware.list_datastore_clusters(service_instance)
1155 @depends(HAS_PYVMOMI)
1156 @ignores_kwargs("credstore")
1157 def list_datastores(
1158     host, username, password, protocol=None, port=None, verify_ssl=True
1159 ):
1160     service_instance = salt.utils.vmware.get_service_instance(
1161         host=host,
1162         username=username,
1163         password=password,
1164         protocol=protocol,
1165         port=port,
1166         verify_ssl=verify_ssl,
1167     )
1168     return salt.utils.vmware.list_datastores(service_instance)
1169 @depends(HAS_PYVMOMI)
1170 @ignores_kwargs("credstore")
1171 def list_hosts(host, username, password, protocol=None, port=None, verify_ssl=True):
1172     service_instance = salt.utils.vmware.get_service_instance(
1173         host=host,
1174         username=username,
1175         password=password,
1176         protocol=protocol,
1177         port=port,
1178         verify_ssl=verify_ssl,
1179     )
1180     return salt.utils.vmware.list_hosts(service_instance)
1181 @depends(HAS_PYVMOMI)
1182 @ignores_kwargs("credstore")
1183 def list_resourcepools(
1184     host, username, password, protocol=None, port=None, verify_ssl=True
1185 ):
1186     service_instance = salt.utils.vmware.get_service_instance(
1187         host=host,
1188         username=username,
1189         password=password,
1190         protocol=protocol,
1191         port=port,
1192         verify_ssl=verify_ssl,
1193     )
1194     return salt.utils.vmware.list_resourcepools(service_instance)
1195 @depends(HAS_PYVMOMI)
1196 @ignores_kwargs("credstore")
1197 def list_networks(host, username, password, protocol=None, port=None, verify_ssl=True):
1198     service_instance = salt.utils.vmware.get_service_instance(
1199         host=host,
1200         username=username,
1201         password=password,
1202         protocol=protocol,
1203         port=port,
1204         verify_ssl=verify_ssl,
1205     )
1206     return salt.utils.vmware.list_networks(service_instance)
1207 @depends(HAS_PYVMOMI)
1208 @ignores_kwargs("credstore")
1209 def list_vms(host, username, password, protocol=None, port=None, verify_ssl=True):
1210     service_instance = salt.utils.vmware.get_service_instance(
1211         host=host,
1212         username=username,
1213         password=password,
1214         protocol=protocol,
1215         port=port,
1216         verify_ssl=verify_ssl,
1217     )
1218     return salt.utils.vmware.list_vms(service_instance)
1219 @depends(HAS_PYVMOMI)
1220 @ignores_kwargs("credstore")
1221 def list_folders(host, username, password, protocol=None, port=None, verify_ssl=True):
1222     service_instance = salt.utils.vmware.get_service_instance(
1223         host=host,
1224         username=username,
1225         password=password,
1226         protocol=protocol,
1227         port=port,
1228         verify_ssl=verify_ssl,
1229     )
1230     return salt.utils.vmware.list_folders(service_instance)
1231 @depends(HAS_PYVMOMI)
1232 @ignores_kwargs("credstore")
1233 def list_dvs(host, username, password, protocol=None, port=None, verify_ssl=True):
1234     service_instance = salt.utils.vmware.get_service_instance(
1235         host=host,
1236         username=username,
1237         password=password,
1238         protocol=protocol,
1239         port=port,
1240         verify_ssl=verify_ssl,
1241     )
1242     return salt.utils.vmware.list_dvs(service_instance)
1243 @depends(HAS_PYVMOMI)
1244 @ignores_kwargs("credstore")
1245 def list_vapps(host, username, password, protocol=None, port=None, verify_ssl=True):
1246     service_instance = salt.utils.vmware.get_service_instance(
1247         host=host,
1248         username=username,
1249         password=password,
1250         protocol=protocol,
1251         port=port,
1252         verify_ssl=verify_ssl,
1253     )
1254     return salt.utils.vmware.list_vapps(service_instance)
1255 @depends(HAS_PYVMOMI)
1256 @ignores_kwargs("credstore")
1257 def list_ssds(
1258     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1259 ):
1260     service_instance = salt.utils.vmware.get_service_instance(
1261         host=host,
1262         username=username,
1263         password=password,
1264         protocol=protocol,
1265         port=port,
1266         verify_ssl=verify_ssl,
1267     )
1268     host_names = _check_hosts(service_instance, host, host_names)
1269     ret = {}
1270     names = []
1271     for host_name in host_names:
1272         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1273         disks = _get_host_ssds(host_ref)
1274         for disk in disks:
1275             names.append(disk.canonicalName)
1276         ret.update({host_name: names})
1277     return ret
1278 @depends(HAS_PYVMOMI)
1279 @ignores_kwargs("credstore")
1280 def list_non_ssds(
1281     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1282 ):
1283     service_instance = salt.utils.vmware.get_service_instance(
1284         host=host,
1285         username=username,
1286         password=password,
1287         protocol=protocol,
1288         port=port,
1289         verify_ssl=verify_ssl,
1290     )
1291     host_names = _check_hosts(service_instance, host, host_names)
1292     ret = {}
1293     names = []
1294     for host_name in host_names:
1295         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1296         disks = _get_host_non_ssds(host_ref)
1297         for disk in disks:
1298             names.append(disk.canonicalName)
1299         ret.update({host_name: names})
1300     return ret
1301 @depends(HAS_PYVMOMI)
1302 @ignores_kwargs("credstore")
1303 def set_ntp_config(
1304     host,
1305     username,
1306     password,
1307     ntp_servers,
1308     protocol=None,
1309     port=None,
1310     host_names=None,
1311     verify_ssl=True,
1312 ):
1313     service_instance = salt.utils.vmware.get_service_instance(
1314         host=host,
1315         username=username,
1316         password=password,
1317         protocol=protocol,
1318         port=port,
1319         verify_ssl=verify_ssl,
1320     )
1321     if not isinstance(ntp_servers, list):
1322         raise CommandExecutionError("'ntp_servers' must be a list.")
1323     ntp_config = vim.HostNtpConfig(server=ntp_servers)
1324     date_config = vim.HostDateTimeConfig(ntpConfig=ntp_config)
1325     host_names = _check_hosts(service_instance, host, host_names)
1326     ret = {}
1327     for host_name in host_names:
1328         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1329         date_time_manager = _get_date_time_mgr(host_ref)
1330         log.debug("Configuring NTP Servers '%s' for host '%s'.", ntp_servers, host_name)
1331         try:
1332             date_time_manager.UpdateDateTimeConfig(config=date_config)
1333         except vim.fault.HostConfigFault as err:
1334             msg = "vsphere.ntp_configure_servers failed: {}".format(err)
1335             log.debug(msg)
1336             ret.update({host_name: {"Error": msg}})
1337             continue
1338         ret.update({host_name: {"NTP Servers": ntp_config}})
1339     return ret
1340 @depends(HAS_PYVMOMI)
1341 @ignores_kwargs("credstore")
1342 def service_start(
1343     host,
1344     username,
1345     password,
1346     service_name,
1347     protocol=None,
1348     port=None,
1349     host_names=None,
1350     verify_ssl=True,
1351 ):
1352     service_instance = salt.utils.vmware.get_service_instance(
1353         host=host,
1354         username=username,
1355         password=password,
1356         protocol=protocol,
1357         port=port,
1358         verify_ssl=verify_ssl,
1359     )
1360     host_names = _check_hosts(service_instance, host, host_names)
1361     valid_services = [
1362         "DCUI",
1363         "TSM",
1364         "SSH",
1365         "ssh",
1366         "lbtd",
1367         "lsassd",
1368         "lwiod",
1369         "netlogond",
1370         "ntpd",
1371         "sfcbd-watchdog",
1372         "snmpd",
1373         "vprobed",
1374         "vpxa",
1375         "xorg",
1376     ]
1377     ret = {}
1378     if service_name == "SSH" or service_name == "ssh":
1379         temp_service_name = "TSM-SSH"
1380     else:
1381         temp_service_name = service_name
1382     for host_name in host_names:
1383         if service_name not in valid_services:
1384             ret.update(
1385                 {
1386                     host_name: {
1387                         "Error": "{} is not a valid service name.".format(service_name)
1388                     }
1389                 }
1390             )
1391             return ret
1392         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1393         service_manager = _get_service_manager(host_ref)
1394         log.debug("Starting the '%s' service on %s.", service_name, host_name)
1395         try:
1396             service_manager.StartService(id=temp_service_name)
1397         except vim.fault.HostConfigFault as err:
1398             msg = "'vsphere.service_start' failed for host {}: {}".format(
1399                 host_name, err
1400             )
1401             log.debug(msg)
1402             ret.update({host_name: {"Error": msg}})
1403             continue
1404         except vim.fault.RestrictedVersion as err:
1405             log.debug(err)
1406             ret.update({host_name: {"Error": err}})
1407             continue
1408         ret.update({host_name: {"Service Started": True}})
1409     return ret
1410 @depends(HAS_PYVMOMI)
1411 @ignores_kwargs("credstore")
1412 def service_stop(
1413     host,
1414     username,
1415     password,
1416     service_name,
1417     protocol=None,
1418     port=None,
1419     host_names=None,
1420     verify_ssl=True,
1421 ):
1422     service_instance = salt.utils.vmware.get_service_instance(
1423         host=host,
1424         username=username,
1425         password=password,
1426         protocol=protocol,
1427         port=port,
1428         verify_ssl=verify_ssl,
1429     )
1430     host_names = _check_hosts(service_instance, host, host_names)
1431     valid_services = [
1432         "DCUI",
1433         "TSM",
1434         "SSH",
1435         "ssh",
1436         "lbtd",
1437         "lsassd",
1438         "lwiod",
1439         "netlogond",
1440         "ntpd",
1441         "sfcbd-watchdog",
1442         "snmpd",
1443         "vprobed",
1444         "vpxa",
1445         "xorg",
1446     ]
1447     ret = {}
1448     if service_name == "SSH" or service_name == "ssh":
1449         temp_service_name = "TSM-SSH"
1450     else:
1451         temp_service_name = service_name
1452     for host_name in host_names:
1453         if service_name not in valid_services:
1454             ret.update(
1455                 {
1456                     host_name: {
1457                         "Error": "{} is not a valid service name.".format(service_name)
1458                     }
1459                 }
1460             )
1461             return ret
1462         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1463         service_manager = _get_service_manager(host_ref)
1464         log.debug("Stopping the '%s' service on %s.", service_name, host_name)
1465         try:
1466             service_manager.StopService(id=temp_service_name)
1467         except vim.fault.HostConfigFault as err:
1468             msg = "'vsphere.service_stop' failed for host {}: {}".format(host_name, err)
1469             log.debug(msg)
1470             ret.update({host_name: {"Error": msg}})
1471             continue
1472         except vim.fault.RestrictedVersion as err:
1473             log.debug(err)
1474             ret.update({host_name: {"Error": err}})
1475             continue
1476         ret.update({host_name: {"Service Stopped": True}})
1477     return ret
1478 @depends(HAS_PYVMOMI)
1479 @ignores_kwargs("credstore")
1480 def service_restart(
1481     host,
1482     username,
1483     password,
1484     service_name,
1485     protocol=None,
1486     port=None,
1487     host_names=None,
1488     verify_ssl=True,
1489 ):
1490     service_instance = salt.utils.vmware.get_service_instance(
1491         host=host,
1492         username=username,
1493         password=password,
1494         protocol=protocol,
1495         port=port,
1496         verify_ssl=verify_ssl,
1497     )
1498     host_names = _check_hosts(service_instance, host, host_names)
1499     valid_services = [
1500         "DCUI",
1501         "TSM",
1502         "SSH",
1503         "ssh",
1504         "lbtd",
1505         "lsassd",
1506         "lwiod",
1507         "netlogond",
1508         "ntpd",
1509         "sfcbd-watchdog",
1510         "snmpd",
1511         "vprobed",
1512         "vpxa",
1513         "xorg",
1514     ]
1515     ret = {}
1516     if service_name == "SSH" or service_name == "ssh":
1517         temp_service_name = "TSM-SSH"
1518     else:
1519         temp_service_name = service_name
1520     for host_name in host_names:
1521         if service_name not in valid_services:
1522             ret.update(
1523                 {
1524                     host_name: {
1525                         "Error": "{} is not a valid service name.".format(service_name)
1526                     }
1527                 }
1528             )
1529             return ret
1530         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1531         service_manager = _get_service_manager(host_ref)
1532         log.debug("Restarting the '%s' service on %s.", service_name, host_name)
1533         try:
1534             service_manager.RestartService(id=temp_service_name)
1535         except vim.fault.HostConfigFault as err:
1536             msg = "'vsphere.service_restart' failed for host {}: {}".format(
1537                 host_name, err
1538             )
1539             log.debug(msg)
1540             ret.update({host_name: {"Error": msg}})
1541             continue
1542         except vim.fault.RestrictedVersion as err:
1543             log.debug(err)
1544             ret.update({host_name: {"Error": err}})
1545             continue
1546         ret.update({host_name: {"Service Restarted": True}})
1547     return ret
1548 @depends(HAS_PYVMOMI)
1549 @ignores_kwargs("credstore")
1550 def set_service_policy(
1551     host,
1552     username,
1553     password,
1554     service_name,
1555     service_policy,
1556     protocol=None,
1557     port=None,
1558     host_names=None,
1559     verify_ssl=True,
1560 ):
1561     service_instance = salt.utils.vmware.get_service_instance(
1562         host=host,
1563         username=username,
1564         password=password,
1565         protocol=protocol,
1566         port=port,
1567         verify_ssl=verify_ssl,
1568     )
1569     host_names = _check_hosts(service_instance, host, host_names)
1570     valid_services = [
1571         "DCUI",
1572         "TSM",
1573         "SSH",
1574         "ssh",
1575         "lbtd",
1576         "lsassd",
1577         "lwiod",
1578         "netlogond",
1579         "ntpd",
1580         "sfcbd-watchdog",
1581         "snmpd",
1582         "vprobed",
1583         "vpxa",
1584         "xorg",
1585     ]
1586     ret = {}
1587     for host_name in host_names:
1588         if service_name not in valid_services:
1589             ret.update(
1590                 {
1591                     host_name: {
1592                         "Error": "{} is not a valid service name.".format(service_name)
1593                     }
1594                 }
1595             )
1596             return ret
1597         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1598         service_manager = _get_service_manager(host_ref)
1599         services = host_ref.configManager.serviceSystem.serviceInfo.service
1600         for service in services:
1601             service_key = None
1602             if service.key == service_name:
1603                 service_key = service.key
1604             elif service_name == "ssh" or service_name == "SSH":
1605                 if service.key == "TSM-SSH":
1606                     service_key = "TSM-SSH"
1607             if service_key:
1608                 try:
1609                     service_manager.UpdateServicePolicy(
1610                         id=service_key, policy=service_policy
1611                     )
1612                 except vim.fault.NotFound:
1613                     msg = "The service name '{}' was not found.".format(service_name)
1614                     log.debug(msg)
1615                     ret.update({host_name: {"Error": msg}})
1616                     continue
1617                 except vim.fault.HostConfigFault as err:
1618                     msg = "'vsphere.set_service_policy' failed for host {}: {}".format(
1619                         host_name, err
1620                     )
1621                     log.debug(msg)
1622                     ret.update({host_name: {"Error": msg}})
1623                     continue
1624                 ret.update({host_name: True})
1625             if ret.get(host_name) is None:
1626                 msg = "Could not find service '{}' for host '{}'.".format(
1627                     service_name, host_name
1628                 )
1629                 log.debug(msg)
1630                 ret.update({host_name: {"Error": msg}})
1631     return ret
1632 @depends(HAS_PYVMOMI)
1633 @ignores_kwargs("credstore")
1634 def update_host_datetime(
1635     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1636 ):
1637     service_instance = salt.utils.vmware.get_service_instance(
1638         host=host,
1639         username=username,
1640         password=password,
1641         protocol=protocol,
1642         port=port,
1643         verify_ssl=verify_ssl,
1644     )
1645     host_names = _check_hosts(service_instance, host, host_names)
1646     ret = {}
1647     for host_name in host_names:
1648         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1649         date_time_manager = _get_date_time_mgr(host_ref)
1650         try:
1651             date_time_manager.UpdateDateTime(datetime.datetime.utcnow())
1652         except vim.fault.HostConfigFault as err:
1653             msg = "'vsphere.update_date_time' failed for host {}: {}".format(
1654                 host_name, err
1655             )
1656             log.debug(msg)
1657             ret.update({host_name: {"Error": msg}})
1658             continue
1659         ret.update({host_name: {"Datetime Updated": True}})
1660     return ret
1661 @depends(HAS_PYVMOMI)
1662 @ignores_kwargs("credstore")
1663 def update_host_password(
1664     host, username, password, new_password, protocol=None, port=None, verify_ssl=True
1665 ):
1666     service_instance = salt.utils.vmware.get_service_instance(
1667         host=host,
1668         username=username,
1669         password=password,
1670         protocol=protocol,
1671         port=port,
1672         verify_ssl=verify_ssl,
1673     )
1674     account_manager = salt.utils.vmware.get_inventory(service_instance).accountManager
1675     user_account = vim.host.LocalAccountManager.AccountSpecification()
1676     user_account.id = username
1677     user_account.password = new_password
1678     try:
1679         account_manager.UpdateUser(user_account)
1680     except vmodl.fault.SystemError as err:
1681         raise CommandExecutionError(err.msg)
1682     except vim.fault.UserNotFound:
1683         raise CommandExecutionError(
1684             "'vsphere.update_host_password' failed for host {}: "
1685             "User was not found.".format(host)
1686         )
1687     except vim.fault.AlreadyExists:
1688         pass
1689     return True
1690 @depends(HAS_PYVMOMI)
1691 @ignores_kwargs("credstore")
1692 def vmotion_disable(
1693     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1694 ):
1695     service_instance = salt.utils.vmware.get_service_instance(
1696         host=host,
1697         username=username,
1698         password=password,
1699         protocol=protocol,
1700         port=port,
1701         verify_ssl=verify_ssl,
1702     )
1703     host_names = _check_hosts(service_instance, host, host_names)
1704     ret = {}
1705     for host_name in host_names:
1706         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1707         vmotion_system = host_ref.configManager.vmotionSystem
1708         try:
1709             vmotion_system.DeselectVnic()
1710         except vim.fault.HostConfigFault as err:
1711             msg = "vsphere.vmotion_disable failed: {}".format(err)
1712             log.debug(msg)
1713             ret.update({host_name: {"Error": msg, "VMotion Disabled": False}})
1714             continue
1715         ret.update({host_name: {"VMotion Disabled": True}})
1716     return ret
1717 @depends(HAS_PYVMOMI)
1718 @ignores_kwargs("credstore")
1719 def vmotion_enable(
1720     host,
1721     username,
1722     password,
1723     protocol=None,
1724     port=None,
1725     host_names=None,
1726     device="vmk0",
1727     verify_ssl=True,
1728 ):
1729     service_instance = salt.utils.vmware.get_service_instance(
1730         host=host,
1731         username=username,
1732         password=password,
1733         protocol=protocol,
1734         port=port,
1735         verify_ssl=verify_ssl,
1736     )
1737     host_names = _check_hosts(service_instance, host, host_names)
1738     ret = {}
1739     for host_name in host_names:
1740         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1741         vmotion_system = host_ref.configManager.vmotionSystem
1742         try:
1743             vmotion_system.SelectVnic(device)
1744         except vim.fault.HostConfigFault as err:
1745             msg = "vsphere.vmotion_disable failed: {}".format(err)
1746             log.debug(msg)
1747             ret.update({host_name: {"Error": msg, "VMotion Enabled": False}})
1748             continue
1749         ret.update({host_name: {"VMotion Enabled": True}})
1750     return ret
1751 @depends(HAS_PYVMOMI)
1752 @ignores_kwargs("credstore")
1753 def vsan_add_disks(
1754     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1755 ):
1756     service_instance = salt.utils.vmware.get_service_instance(
1757         host=host,
1758         username=username,
1759         password=password,
1760         protocol=protocol,
1761         port=port,
1762         verify_ssl=verify_ssl,
1763     )
1764     host_names = _check_hosts(service_instance, host, host_names)
1765     response = _get_vsan_eligible_disks(service_instance, host, host_names)
1766     ret = {}
1767     for host_name, value in response.items():
1768         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1769         vsan_system = host_ref.configManager.vsanSystem
1770         if vsan_system is None:
1771             msg = (
1772                 "VSAN System Config Manager is unset for host '{}'. "
1773                 "VSAN configuration cannot be changed without a configured "
1774                 "VSAN System.".format(host_name)
1775             )
1776             log.debug(msg)
1777             ret.update({host_name: {"Error": msg}})
1778         else:
1779             eligible = value.get("Eligible")
1780             error = value.get("Error")
1781             if eligible and isinstance(eligible, list):
1782                 try:
1783                     task = vsan_system.AddDisks(eligible)
1784                     salt.utils.vmware.wait_for_task(
1785                         task, host_name, "Adding disks to VSAN", sleep_seconds=3
1786                     )
1787                 except vim.fault.InsufficientDisks as err:
1788                     log.debug(err.msg)
1789                     ret.update({host_name: {"Error": err.msg}})
1790                     continue
1791                 except Exception as err:  # pylint: disable=broad-except
1792                     msg = "'vsphere.vsan_add_disks' failed for host {}: {}".format(
1793                         host_name, err
1794                     )
1795                     log.debug(msg)
1796                     ret.update({host_name: {"Error": msg}})
1797                     continue
1798                 log.debug(
1799                     "Successfully added disks to the VSAN system for host '%s'.",
1800                     host_name,
1801                 )
1802 <a name="0"></a>                disk_names = []
1803                 for disk in eligible:
1804                     disk_names.append(disk.canonicalName)
1805                 ret.update({<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>host_name: {"Disks Added": disk_names}})
1806             elif eligible and isinstance(eligible, str):
1807                 ret.update({host_name: {"Disks Added": eligible}})
1808             elif error:
1809                 ret.update({host_name: {"Error": error}})
1810             else:
1811                 ret.update(
1812                     {
1813                         host_name</b></font>: {
1814                             "Disks Added": (
1815                                 "No new VSAN-eligible disks were found to add."
1816                             )
1817                         }
1818                     }
1819                 )
1820     return ret
1821 @depends(HAS_PYVMOMI)
1822 @ignores_kwargs("credstore")
1823 def vsan_disable(
1824     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1825 ):
1826     service_instance = salt.utils.vmware.get_service_instance(
1827         host=host,
1828         username=username,
1829         password=password,
1830         protocol=protocol,
1831         port=port,
1832         verify_ssl=verify_ssl,
1833     )
1834     vsan_config = vim.vsan.host.ConfigInfo()
1835     vsan_config.enabled = False
1836     host_names = _check_hosts(service_instance, host, host_names)
1837     ret = {}
1838     for host_name in host_names:
1839         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1840         vsan_system = host_ref.configManager.vsanSystem
1841         if vsan_system is None:
1842             msg = (
1843                 "VSAN System Config Manager is unset for host '{}'. "
1844                 "VSAN configuration cannot be changed without a configured "
1845                 "VSAN System.".format(host_name)
1846             )
1847             log.debug(msg)
1848             ret.update({host_name: {"Error": msg}})
1849         else:
1850             try:
1851                 task = vsan_system.UpdateVsan_Task(vsan_config)
1852                 salt.utils.vmware.wait_for_task(
1853                     task, host_name, "Disabling VSAN", sleep_seconds=3
1854                 )
1855             except vmodl.fault.SystemError as err:
1856                 log.debug(err.msg)
1857                 ret.update({host_name: {"Error": err.msg}})
1858                 continue
1859             except Exception as err:  # pylint: disable=broad-except
1860                 msg = "'vsphere.vsan_disable' failed for host {}: {}".format(
1861                     host_name, err
1862                 )
1863                 log.debug(msg)
1864                 ret.update({host_name: {"Error": msg}})
1865                 continue
1866             ret.update({host_name: {"VSAN Disabled": True}})
1867     return ret
1868 @depends(HAS_PYVMOMI)
1869 @ignores_kwargs("credstore")
1870 def vsan_enable(
1871     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1872 ):
1873     service_instance = salt.utils.vmware.get_service_instance(
1874         host=host,
1875         username=username,
1876         password=password,
1877         protocol=protocol,
1878         port=port,
1879         verify_ssl=verify_ssl,
1880     )
1881     vsan_config = vim.vsan.host.ConfigInfo()
1882     vsan_config.enabled = True
1883     host_names = _check_hosts(service_instance, host, host_names)
1884     ret = {}
1885     for host_name in host_names:
1886         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1887         vsan_system = host_ref.configManager.vsanSystem
1888         if vsan_system is None:
1889             msg = (
1890                 "VSAN System Config Manager is unset for host '{}'. "
1891                 "VSAN configuration cannot be changed without a configured "
1892                 "VSAN System.".format(host_name)
1893             )
1894             log.debug(msg)
1895             ret.update({host_name: {"Error": msg}})
1896         else:
1897             try:
1898                 task = vsan_system.UpdateVsan_Task(vsan_config)
1899                 salt.utils.vmware.wait_for_task(
1900                     task, host_name, "Enabling VSAN", sleep_seconds=3
1901                 )
1902             except vmodl.fault.SystemError as err:
1903                 log.debug(err.msg)
1904                 ret.update({host_name: {"Error": err.msg}})
1905                 continue
1906             except vim.fault.VsanFault as err:
1907                 msg = "'vsphere.vsan_enable' failed for host {}: {}".format(
1908                     host_name, err
1909                 )
1910                 log.debug(msg)
1911                 ret.update({host_name: {"Error": msg}})
1912                 continue
1913             ret.update({host_name: {"VSAN Enabled": True}})
1914     return ret
1915 def _get_dvs_config_dict(dvs_name, dvs_config):
1916     log.trace("Building the dict of the DVS '%s' config", dvs_name)
1917     conf_dict = {
1918         "name": dvs_name,
1919         "contact_email": dvs_config.contact.contact,
1920         "contact_name": dvs_config.contact.name,
1921         "description": dvs_config.description,
1922         "lacp_api_version": dvs_config.lacpApiVersion,
1923         "network_resource_control_version": dvs_config.networkResourceControlVersion,
1924         "network_resource_management_enabled": dvs_config.networkResourceManagementEnabled,
1925         "max_mtu": dvs_config.maxMtu,
1926     }
1927     if isinstance(dvs_config.uplinkPortPolicy, vim.DVSNameArrayUplinkPortPolicy):
1928         conf_dict.update({"uplink_names": dvs_config.uplinkPortPolicy.uplinkPortName})
1929     return conf_dict
1930 def _get_dvs_link_discovery_protocol(dvs_name, dvs_link_disc_protocol):
1931     log.trace("Building the dict of the DVS '%s' link discovery protocol", dvs_name)
1932     return {
1933         "operation": dvs_link_disc_protocol.operation,
1934         "protocol": dvs_link_disc_protocol.protocol,
1935     }
1936 def _get_dvs_product_info(dvs_name, dvs_product_info):
1937     log.trace("Building the dict of the DVS '%s' product info", dvs_name)
1938     return {
1939         "name": dvs_product_info.name,
1940         "vendor": dvs_product_info.vendor,
1941         "version": dvs_product_info.version,
1942     }
1943 def _get_dvs_capability(dvs_name, dvs_capability):
1944     log.trace("Building the dict of the DVS '%s' capability", dvs_name)
1945     return {
1946         "operation_supported": dvs_capability.dvsOperationSupported,
1947         "portgroup_operation_supported": dvs_capability.dvPortGroupOperationSupported,
1948         "port_operation_supported": dvs_capability.dvPortOperationSupported,
1949     }
1950 def _get_dvs_infrastructure_traffic_resources(dvs_name, dvs_infra_traffic_ress):
1951     log.trace(
1952         "Building the dicts of the DVS '%s' infrastructure traffic resources", dvs_name
1953     )
1954     res_dicts = []
1955     for res in dvs_infra_traffic_ress:
1956         res_dict = {
1957             "key": res.key,
1958             "limit": res.allocationInfo.limit,
1959             "reservation": res.allocationInfo.reservation,
1960         }
1961         if res.allocationInfo.shares:
1962             res_dict.update(
1963                 {
1964                     "num_shares": res.allocationInfo.shares.shares,
1965                     "share_level": res.allocationInfo.shares.level,
1966                 }
1967             )
1968         res_dicts.append(res_dict)
1969     return res_dicts
1970 @depends(HAS_PYVMOMI)
1971 @_supports_proxies("esxdatacenter", "esxcluster")
1972 @_gets_service_instance_via_proxy
1973 def list_dvss(datacenter=None, dvs_names=None, service_instance=None):
1974     ret_list = []
1975     proxy_type = get_proxy_type()
1976     if proxy_type == "esxdatacenter":
1977         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
1978         dc_ref = _get_proxy_target(service_instance)
1979     elif proxy_type == "esxcluster":
1980         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
1981         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
1982     for dvs in salt.utils.vmware.get_dvss(dc_ref, dvs_names, (not dvs_names)):
1983         dvs_dict = {}
1984         props = salt.utils.vmware.get_properties_of_managed_object(
1985             dvs, ["name", "config", "capability", "networkResourcePool"]
1986         )
1987         dvs_dict = _get_dvs_config_dict(props["name"], props["config"])
1988         dvs_dict.update(
1989             {
1990                 "product_info": _get_dvs_product_info(
1991                     props["name"], props["config"].productInfo
1992                 )
1993             }
1994         )
1995         if props["config"].linkDiscoveryProtocolConfig:
1996             dvs_dict.update(
1997                 {
1998                     "link_discovery_protocol": _get_dvs_link_discovery_protocol(
1999                         props["name"], props["config"].linkDiscoveryProtocolConfig
2000                     )
2001                 }
2002             )
2003         dvs_dict.update(
2004             {"capability": _get_dvs_capability(props["name"], props["capability"])}
2005         )
2006         if hasattr(props["config"], "infrastructureTrafficResourceConfig"):
2007             dvs_dict.update(
2008                 {
2009                     "infrastructure_traffic_resource_pools": _get_dvs_infrastructure_traffic_resources(
2010                         props["name"],
2011                         props["config"].infrastructureTrafficResourceConfig,
2012                     )
2013                 }
2014             )
2015         ret_list.append(dvs_dict)
2016     return ret_list
2017 def _apply_dvs_config(config_spec, config_dict):
2018     if config_dict.get("name"):
2019         config_spec.name = config_dict["name"]
2020     if config_dict.get("contact_email") or config_dict.get("contact_name"):
2021         if not config_spec.contact:
2022             config_spec.contact = vim.DVSContactInfo()
2023         config_spec.contact.contact = config_dict.get("contact_email")
2024         config_spec.contact.name = config_dict.get("contact_name")
2025     if config_dict.get("description"):
2026         config_spec.description = config_dict.get("description")
2027     if config_dict.get("max_mtu"):
2028         config_spec.maxMtu = config_dict.get("max_mtu")
2029     if config_dict.get("lacp_api_version"):
2030         config_spec.lacpApiVersion = config_dict.get("lacp_api_version")
2031     if config_dict.get("network_resource_control_version"):
2032         config_spec.networkResourceControlVersion = config_dict.get(
2033             "network_resource_control_version"
2034         )
2035     if config_dict.get("uplink_names"):
2036         if not config_spec.uplinkPortPolicy or not isinstance(
2037             config_spec.uplinkPortPolicy, vim.DVSNameArrayUplinkPortPolicy
2038         ):
2039             config_spec.uplinkPortPolicy = vim.DVSNameArrayUplinkPortPolicy()
2040         config_spec.uplinkPortPolicy.uplinkPortName = config_dict["uplink_names"]
2041 def _apply_dvs_link_discovery_protocol(disc_prot_config, disc_prot_dict):
2042     disc_prot_config.operation = disc_prot_dict["operation"]
2043     disc_prot_config.protocol = disc_prot_dict["protocol"]
2044 def _apply_dvs_product_info(product_info_spec, product_info_dict):
2045     if product_info_dict.get("name"):
2046         product_info_spec.name = product_info_dict["name"]
2047     if product_info_dict.get("vendor"):
2048         product_info_spec.vendor = product_info_dict["vendor"]
2049     if product_info_dict.get("version"):
2050         product_info_spec.version = product_info_dict["version"]
2051 def _apply_dvs_capability(capability_spec, capability_dict):
2052     if "operation_supported" in capability_dict:
2053         capability_spec.dvsOperationSupported = capability_dict["operation_supported"]
2054     if "port_operation_supported" in capability_dict:
2055         capability_spec.dvPortOperationSupported = capability_dict[
2056             "port_operation_supported"
2057         ]
2058     if "portgroup_operation_supported" in capability_dict:
2059         capability_spec.dvPortGroupOperationSupported = capability_dict[
2060             "portgroup_operation_supported"
2061         ]
2062 def _apply_dvs_infrastructure_traffic_resources(
2063     infra_traffic_resources, resource_dicts
2064 ):
2065     for res_dict in resource_dicts:
2066         filtered_traffic_resources = [
2067             r for r in infra_traffic_resources if r.key == res_dict["key"]
2068         ]
2069         if filtered_traffic_resources:
2070             traffic_res = filtered_traffic_resources[0]
2071         else:
2072             traffic_res = vim.DvsHostInfrastructureTrafficResource()
2073             traffic_res.key = res_dict["key"]
2074             traffic_res.allocationInfo = (
2075                 vim.DvsHostInfrastructureTrafficResourceAllocation()
2076             )
2077             infra_traffic_resources.append(traffic_res)
2078         if res_dict.get("limit"):
2079             traffic_res.allocationInfo.limit = res_dict["limit"]
2080         if res_dict.get("reservation"):
2081             traffic_res.allocationInfo.reservation = res_dict["reservation"]
2082         if res_dict.get("num_shares") or res_dict.get("share_level"):
2083             if not traffic_res.allocationInfo.shares:
2084                 traffic_res.allocationInfo.shares = vim.SharesInfo()
2085         if res_dict.get("share_level"):
2086             traffic_res.allocationInfo.shares.level = vim.SharesLevel(
2087                 res_dict["share_level"]
2088             )
2089         if res_dict.get("num_shares"):
2090             traffic_res.allocationInfo.shares.shares = res_dict["num_shares"]
2091 def _apply_dvs_network_resource_pools(network_resource_pools, resource_dicts):
2092     for res_dict in resource_dicts:
2093         ress = [r for r in network_resource_pools if r.key == res_dict["key"]]
2094         if ress:
2095             res = ress[0]
2096         else:
2097             res = vim.DVSNetworkResourcePoolConfigSpec()
2098             res.key = res_dict["key"]
2099             res.allocationInfo = vim.DVSNetworkResourcePoolAllocationInfo()
2100             network_resource_pools.append(res)
2101         if res_dict.get("limit"):
2102             res.allocationInfo.limit = res_dict["limit"]
2103         if res_dict.get("num_shares") and res_dict.get("share_level"):
2104             if not res.allocationInfo.shares:
2105                 res.allocationInfo.shares = vim.SharesInfo()
2106             res.allocationInfo.shares.shares = res_dict["num_shares"]
2107             res.allocationInfo.shares.level = vim.SharesLevel(res_dict["share_level"])
2108 @depends(HAS_PYVMOMI)
2109 @_supports_proxies("esxdatacenter", "esxcluster")
2110 @_gets_service_instance_via_proxy
2111 def create_dvs(dvs_dict, dvs_name, service_instance=None):
2112     log.trace("Creating dvs '%s' with dict = %s", dvs_name, dvs_dict)
2113     proxy_type = get_proxy_type()
2114     if proxy_type == "esxdatacenter":
2115         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2116         dc_ref = _get_proxy_target(service_instance)
2117     elif proxy_type == "esxcluster":
2118         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2119         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2120     dvs_dict["name"] = dvs_name
2121     dvs_create_spec = vim.DVSCreateSpec()
2122     dvs_create_spec.configSpec = vim.VMwareDVSConfigSpec()
2123     _apply_dvs_config(dvs_create_spec.configSpec, dvs_dict)
2124     if dvs_dict.get("product_info"):
2125         dvs_create_spec.productInfo = vim.DistributedVirtualSwitchProductSpec()
2126         _apply_dvs_product_info(dvs_create_spec.productInfo, dvs_dict["product_info"])
2127     if dvs_dict.get("capability"):
2128         dvs_create_spec.capability = vim.DVSCapability()
2129         _apply_dvs_capability(dvs_create_spec.capability, dvs_dict["capability"])
2130     if dvs_dict.get("link_discovery_protocol"):
2131         dvs_create_spec.configSpec.linkDiscoveryProtocolConfig = (
2132             vim.LinkDiscoveryProtocolConfig()
2133         )
2134         _apply_dvs_link_discovery_protocol(
2135             dvs_create_spec.configSpec.linkDiscoveryProtocolConfig,
2136             dvs_dict["link_discovery_protocol"],
2137         )
2138     if dvs_dict.get("infrastructure_traffic_resource_pools"):
2139         dvs_create_spec.configSpec.infrastructureTrafficResourceConfig = []
2140         _apply_dvs_infrastructure_traffic_resources(
2141             dvs_create_spec.configSpec.infrastructureTrafficResourceConfig,
2142             dvs_dict["infrastructure_traffic_resource_pools"],
2143         )
2144     log.trace("dvs_create_spec = %s", dvs_create_spec)
2145     salt.utils.vmware.create_dvs(dc_ref, dvs_name, dvs_create_spec)
2146     if "network_resource_management_enabled" in dvs_dict:
2147         dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs_name])
2148         if not dvs_refs:
2149             raise VMwareObjectRetrievalError(
2150                 "DVS '{}' wasn't found in datacenter '{}'".format(dvs_name, datacenter)
2151             )
2152         dvs_ref = dvs_refs[0]
2153         salt.utils.vmware.set_dvs_network_resource_management_enabled(
2154             dvs_ref, dvs_dict["network_resource_management_enabled"]
2155         )
2156     return True
2157 @depends(HAS_PYVMOMI)
2158 @_supports_proxies("esxdatacenter", "esxcluster")
2159 @_gets_service_instance_via_proxy
2160 def update_dvs(dvs_dict, dvs, service_instance=None):
2161     log.trace("Updating dvs '%s' with dict = %s", dvs, dvs_dict)
2162     for prop in ["product_info", "capability", "uplink_names", "name"]:
2163         if prop in dvs_dict:
2164             del dvs_dict[prop]
2165     proxy_type = get_proxy_type()
2166     if proxy_type == "esxdatacenter":
2167         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2168         dc_ref = _get_proxy_target(service_instance)
2169     elif proxy_type == "esxcluster":
2170         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2171         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2172     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
2173     if not dvs_refs:
2174         raise VMwareObjectRetrievalError(
2175             "DVS '{}' wasn't found in datacenter '{}'".format(dvs, datacenter)
2176         )
2177     dvs_ref = dvs_refs[0]
2178     dvs_props = salt.utils.vmware.get_properties_of_managed_object(
2179         dvs_ref, ["config", "capability"]
2180     )
2181     dvs_config = vim.VMwareDVSConfigSpec()
2182     skipped_properties = ["host"]
2183     for prop in dvs_config.__dict__.keys():
2184         if prop in skipped_properties:
2185             continue
2186         if hasattr(dvs_props["config"], prop):
2187             setattr(dvs_config, prop, getattr(dvs_props["config"], prop))
2188     _apply_dvs_config(dvs_config, dvs_dict)
2189     if dvs_dict.get("link_discovery_protocol"):
2190         if not dvs_config.linkDiscoveryProtocolConfig:
2191             dvs_config.linkDiscoveryProtocolConfig = vim.LinkDiscoveryProtocolConfig()
2192         _apply_dvs_link_discovery_protocol(
2193             dvs_config.linkDiscoveryProtocolConfig, dvs_dict["link_discovery_protocol"]
2194         )
2195     if dvs_dict.get("infrastructure_traffic_resource_pools"):
2196         if not dvs_config.infrastructureTrafficResourceConfig:
2197             dvs_config.infrastructureTrafficResourceConfig = []
2198         _apply_dvs_infrastructure_traffic_resources(
2199             dvs_config.infrastructureTrafficResourceConfig,
2200             dvs_dict["infrastructure_traffic_resource_pools"],
2201         )
2202     log.trace("dvs_config= %s", dvs_config)
2203     salt.utils.vmware.update_dvs(dvs_ref, dvs_config_spec=dvs_config)
2204     if "network_resource_management_enabled" in dvs_dict:
2205         salt.utils.vmware.set_dvs_network_resource_management_enabled(
2206             dvs_ref, dvs_dict["network_resource_management_enabled"]
2207         )
2208     return True
2209 def _get_dvportgroup_out_shaping(pg_name, pg_default_port_config):
2210     log.trace("Retrieving portgroup's '%s' out shaping config", pg_name)
2211     out_shaping_policy = pg_default_port_config.outShapingPolicy
2212     if not out_shaping_policy:
2213         return {}
2214     return {
2215         "average_bandwidth": out_shaping_policy.averageBandwidth.value,
2216         "burst_size": out_shaping_policy.burstSize.value,
2217         "enabled": out_shaping_policy.enabled.value,
2218         "peak_bandwidth": out_shaping_policy.peakBandwidth.value,
2219     }
2220 def _get_dvportgroup_security_policy(pg_name, pg_default_port_config):
2221     log.trace("Retrieving portgroup's '%s' security policy config", pg_name)
2222     sec_policy = pg_default_port_config.securityPolicy
2223     if not sec_policy:
2224         return {}
2225     return {
2226         "allow_promiscuous": sec_policy.allowPromiscuous.value,
2227         "forged_transmits": sec_policy.forgedTransmits.value,
2228         "mac_changes": sec_policy.macChanges.value,
2229     }
2230 def _get_dvportgroup_teaming(pg_name, pg_default_port_config):
2231     log.trace("Retrieving portgroup's '%s' teaming config", pg_name)
2232     teaming_policy = pg_default_port_config.uplinkTeamingPolicy
2233     if not teaming_policy:
2234         return {}
2235     ret_dict = {
2236         "notify_switches": teaming_policy.notifySwitches.value,
2237         "policy": teaming_policy.policy.value,
2238         "reverse_policy": teaming_policy.reversePolicy.value,
2239         "rolling_order": teaming_policy.rollingOrder.value,
2240     }
2241     if teaming_policy.failureCriteria:
2242         failure_criteria = teaming_policy.failureCriteria
2243         ret_dict.update(
2244             {
2245                 "failure_criteria": {
2246                     "check_beacon": failure_criteria.checkBeacon.value,
2247                     "check_duplex": failure_criteria.checkDuplex.value,
2248                     "check_error_percent": failure_criteria.checkErrorPercent.value,
2249                     "check_speed": failure_criteria.checkSpeed.value,
2250                     "full_duplex": failure_criteria.fullDuplex.value,
2251                     "percentage": failure_criteria.percentage.value,
2252                     "speed": failure_criteria.speed.value,
2253                 }
2254             }
2255         )
2256     if teaming_policy.uplinkPortOrder:
2257         uplink_order = teaming_policy.uplinkPortOrder
2258         ret_dict.update(
2259             {
2260                 "port_order": {
2261                     "active": uplink_order.activeUplinkPort,
2262                     "standby": uplink_order.standbyUplinkPort,
2263                 }
2264             }
2265         )
2266     return ret_dict
2267 def _get_dvportgroup_dict(pg_ref):
2268     props = salt.utils.vmware.get_properties_of_managed_object(
2269         pg_ref,
2270         [
2271             "name",
2272             "config.description",
2273             "config.numPorts",
2274             "config.type",
2275             "config.defaultPortConfig",
2276         ],
2277     )
2278     pg_dict = {
2279         "name": props["name"],
2280         "description": props.get("config.description"),
2281         "num_ports": props["config.numPorts"],
2282         "type": props["config.type"],
2283     }
2284     if props["config.defaultPortConfig"]:
2285         dpg = props["config.defaultPortConfig"]
2286         if dpg.vlan and isinstance(
2287             dpg.vlan, vim.VmwareDistributedVirtualSwitchVlanIdSpec
2288         ):
2289             pg_dict.update({"vlan_id": dpg.vlan.vlanId})
2290         pg_dict.update(
2291             {
2292                 "out_shaping": _get_dvportgroup_out_shaping(
2293                     props["name"], props["config.defaultPortConfig"]
2294                 )
2295             }
2296         )
2297         pg_dict.update(
2298             {
2299                 "security_policy": _get_dvportgroup_security_policy(
2300                     props["name"], props["config.defaultPortConfig"]
2301                 )
2302             }
2303         )
2304         pg_dict.update(
2305             {
2306                 "teaming": _get_dvportgroup_teaming(
2307                     props["name"], props["config.defaultPortConfig"]
2308                 )
2309             }
2310         )
2311     return pg_dict
2312 @depends(HAS_PYVMOMI)
2313 @_supports_proxies("esxdatacenter", "esxcluster")
2314 @_gets_service_instance_via_proxy
2315 def list_dvportgroups(dvs=None, portgroup_names=None, service_instance=None):
2316     ret_dict = []
2317     proxy_type = get_proxy_type()
2318     if proxy_type == "esxdatacenter":
2319         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2320         dc_ref = _get_proxy_target(service_instance)
2321     elif proxy_type == "esxcluster":
2322         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2323         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2324     if dvs:
2325         dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
2326         if not dvs_refs:
2327             raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
2328         dvs_ref = dvs_refs[0]
2329     get_all_portgroups = True if not portgroup_names else False
2330     for pg_ref in salt.utils.vmware.get_dvportgroups(
2331         parent_ref=dvs_ref if dvs else dc_ref,
2332         portgroup_names=portgroup_names,
2333         get_all_portgroups=get_all_portgroups,
2334     ):
2335         ret_dict.append(_get_dvportgroup_dict(pg_ref))
2336     return ret_dict
2337 @depends(HAS_PYVMOMI)
2338 @_supports_proxies("esxdatacenter", "esxcluster")
2339 @_gets_service_instance_via_proxy
2340 def list_uplink_dvportgroup(dvs, service_instance=None):
2341     proxy_type = get_proxy_type()
2342     if proxy_type == "esxdatacenter":
2343         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2344         dc_ref = _get_proxy_target(service_instance)
2345     elif proxy_type == "esxcluster":
2346         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2347         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2348     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
2349     if not dvs_refs:
2350         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
2351     uplink_pg_ref = salt.utils.vmware.get_uplink_dvportgroup(dvs_refs[0])
2352     return _get_dvportgroup_dict(uplink_pg_ref)
2353 def _apply_dvportgroup_out_shaping(pg_name, out_shaping, out_shaping_conf):
2354     log.trace("Building portgroup's '%s' out shaping policy", pg_name)
2355     if out_shaping_conf.get("average_bandwidth"):
2356         out_shaping.averageBandwidth = vim.LongPolicy()
2357         out_shaping.averageBandwidth.value = out_shaping_conf["average_bandwidth"]
2358     if out_shaping_conf.get("burst_size"):
2359         out_shaping.burstSize = vim.LongPolicy()
2360         out_shaping.burstSize.value = out_shaping_conf["burst_size"]
2361     if "enabled" in out_shaping_conf:
2362         out_shaping.enabled = vim.BoolPolicy()
2363         out_shaping.enabled.value = out_shaping_conf["enabled"]
2364     if out_shaping_conf.get("peak_bandwidth"):
2365         out_shaping.peakBandwidth = vim.LongPolicy()
2366         out_shaping.peakBandwidth.value = out_shaping_conf["peak_bandwidth"]
2367 def _apply_dvportgroup_security_policy(pg_name, sec_policy, sec_policy_conf):
2368     log.trace("Building portgroup's '%s' security policy", pg_name)
2369     if "allow_promiscuous" in sec_policy_conf:
2370         sec_policy.allowPromiscuous = vim.BoolPolicy()
2371         sec_policy.allowPromiscuous.value = sec_policy_conf["allow_promiscuous"]
2372     if "forged_transmits" in sec_policy_conf:
2373         sec_policy.forgedTransmits = vim.BoolPolicy()
2374         sec_policy.forgedTransmits.value = sec_policy_conf["forged_transmits"]
2375     if "mac_changes" in sec_policy_conf:
2376         sec_policy.macChanges = vim.BoolPolicy()
2377         sec_policy.macChanges.value = sec_policy_conf["mac_changes"]
2378 def _apply_dvportgroup_teaming(pg_name, teaming, teaming_conf):
2379     log.trace("Building portgroup's '%s' teaming", pg_name)
2380     if "notify_switches" in teaming_conf:
2381         teaming.notifySwitches = vim.BoolPolicy()
2382         teaming.notifySwitches.value = teaming_conf["notify_switches"]
2383     if "policy" in teaming_conf:
2384         teaming.policy = vim.StringPolicy()
2385         teaming.policy.value = teaming_conf["policy"]
2386     if "reverse_policy" in teaming_conf:
2387         teaming.reversePolicy = vim.BoolPolicy()
2388         teaming.reversePolicy.value = teaming_conf["reverse_policy"]
2389     if "rolling_order" in teaming_conf:
2390         teaming.rollingOrder = vim.BoolPolicy()
2391         teaming.rollingOrder.value = teaming_conf["rolling_order"]
2392     if "failure_criteria" in teaming_conf:
2393         if not teaming.failureCriteria:
2394             teaming.failureCriteria = vim.DVSFailureCriteria()
2395         failure_criteria_conf = teaming_conf["failure_criteria"]
2396         if "check_beacon" in failure_criteria_conf:
2397             teaming.failureCriteria.checkBeacon = vim.BoolPolicy()
2398             teaming.failureCriteria.checkBeacon.value = failure_criteria_conf[
2399                 "check_beacon"
2400             ]
2401         if "check_duplex" in failure_criteria_conf:
2402             teaming.failureCriteria.checkDuplex = vim.BoolPolicy()
2403             teaming.failureCriteria.checkDuplex.value = failure_criteria_conf[
2404                 "check_duplex"
2405             ]
2406         if "check_error_percent" in failure_criteria_conf:
2407             teaming.failureCriteria.checkErrorPercent = vim.BoolPolicy()
2408             teaming.failureCriteria.checkErrorPercent.value = failure_criteria_conf[
2409                 "check_error_percent"
2410             ]
2411         if "check_speed" in failure_criteria_conf:
2412             teaming.failureCriteria.checkSpeed = vim.StringPolicy()
2413             teaming.failureCriteria.checkSpeed.value = failure_criteria_conf[
2414                 "check_speed"
2415             ]
2416         if "full_duplex" in failure_criteria_conf:
2417             teaming.failureCriteria.fullDuplex = vim.BoolPolicy()
2418             teaming.failureCriteria.fullDuplex.value = failure_criteria_conf[
2419                 "full_duplex"
2420             ]
2421         if "percentage" in failure_criteria_conf:
2422             teaming.failureCriteria.percentage = vim.IntPolicy()
2423             teaming.failureCriteria.percentage.value = failure_criteria_conf[
2424                 "percentage"
2425             ]
2426         if "speed" in failure_criteria_conf:
2427             teaming.failureCriteria.speed = vim.IntPolicy()
2428             teaming.failureCriteria.speed.value = failure_criteria_conf["speed"]
2429     if "port_order" in teaming_conf:
2430         if not teaming.uplinkPortOrder:
2431             teaming.uplinkPortOrder = vim.VMwareUplinkPortOrderPolicy()
2432         if "active" in teaming_conf["port_order"]:
2433             teaming.uplinkPortOrder.activeUplinkPort = teaming_conf["port_order"][
2434                 "active"
2435             ]
2436         if "standby" in teaming_conf["port_order"]:
2437             teaming.uplinkPortOrder.standbyUplinkPort = teaming_conf["port_order"][
2438                 "standby"
2439             ]
2440 def _apply_dvportgroup_config(pg_name, pg_spec, pg_conf):
2441     log.trace("Building portgroup's '%s' spec", pg_name)
2442     if "name" in pg_conf:
2443         pg_spec.name = pg_conf["name"]
2444     if "description" in pg_conf:
2445         pg_spec.description = pg_conf["description"]
2446     if "num_ports" in pg_conf:
2447         pg_spec.numPorts = pg_conf["num_ports"]
2448     if "type" in pg_conf:
2449         pg_spec.type = pg_conf["type"]
2450     if not pg_spec.defaultPortConfig:
2451         for prop in ["vlan_id", "out_shaping", "security_policy", "teaming"]:
2452             if prop in pg_conf:
2453                 pg_spec.defaultPortConfig = vim.VMwareDVSPortSetting()
2454     if "vlan_id" in pg_conf:
2455         pg_spec.defaultPortConfig.vlan = vim.VmwareDistributedVirtualSwitchVlanIdSpec()
2456         pg_spec.defaultPortConfig.vlan.vlanId = pg_conf["vlan_id"]
2457     if "out_shaping" in pg_conf:
2458         if not pg_spec.defaultPortConfig.outShapingPolicy:
2459             pg_spec.defaultPortConfig.outShapingPolicy = vim.DVSTrafficShapingPolicy()
2460         _apply_dvportgroup_out_shaping(
2461             pg_name, pg_spec.defaultPortConfig.outShapingPolicy, pg_conf["out_shaping"]
2462         )
2463     if "security_policy" in pg_conf:
2464         if not pg_spec.defaultPortConfig.securityPolicy:
2465             pg_spec.defaultPortConfig.securityPolicy = vim.DVSSecurityPolicy()
2466         _apply_dvportgroup_security_policy(
2467             pg_name,
2468             pg_spec.defaultPortConfig.securityPolicy,
2469             pg_conf["security_policy"],
2470         )
2471     if "teaming" in pg_conf:
2472         if not pg_spec.defaultPortConfig.uplinkTeamingPolicy:
2473             pg_spec.defaultPortConfig.uplinkTeamingPolicy = (
2474                 vim.VmwareUplinkPortTeamingPolicy()
2475             )
2476         _apply_dvportgroup_teaming(
2477             pg_name, pg_spec.defaultPortConfig.uplinkTeamingPolicy, pg_conf["teaming"]
2478         )
2479 @depends(HAS_PYVMOMI)
2480 @_supports_proxies("esxdatacenter", "esxcluster")
2481 @_gets_service_instance_via_proxy
2482 def create_dvportgroup(portgroup_dict, portgroup_name, dvs, service_instance=None):
2483     log.trace(
2484         "Creating portgroup '%s' in dvs '%s' with dict = %s",
2485         portgroup_name,
2486         dvs,
2487         portgroup_dict,
2488     )
2489     proxy_type = get_proxy_type()
2490     if proxy_type == "esxdatacenter":
2491         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2492         dc_ref = _get_proxy_target(service_instance)
2493     elif proxy_type == "esxcluster":
2494         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2495         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2496     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
2497     if not dvs_refs:
2498         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
2499     portgroup_dict["name"] = portgroup_name
2500     spec = vim.DVPortgroupConfigSpec()
2501     _apply_dvportgroup_config(portgroup_name, spec, portgroup_dict)
2502     salt.utils.vmware.create_dvportgroup(dvs_refs[0], spec)
2503     return True
2504 @depends(HAS_PYVMOMI)
2505 @_supports_proxies("esxdatacenter", "esxcluster")
2506 @_gets_service_instance_via_proxy
2507 def update_dvportgroup(portgroup_dict, portgroup, dvs, service_instance=True):
2508     log.trace(
2509         "Updating portgroup '%s' in dvs '%s' with dict = %s",
2510         portgroup,
2511         dvs,
2512         portgroup_dict,
2513     )
2514     proxy_type = get_proxy_type()
2515     if proxy_type == "esxdatacenter":
2516         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2517         dc_ref = _get_proxy_target(service_instance)
2518     elif proxy_type == "esxcluster":
2519         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2520         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2521     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
2522     if not dvs_refs:
2523         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
2524     pg_refs = salt.utils.vmware.get_dvportgroups(
2525         dvs_refs[0], portgroup_names=[portgroup]
2526     )
2527     if not pg_refs:
2528         raise VMwareObjectRetrievalError(
2529             "Portgroup '{}' was not retrieved".format(portgroup)
2530         )
2531     pg_props = salt.utils.vmware.get_properties_of_managed_object(
2532         pg_refs[0], ["config"]
2533     )
2534     spec = vim.DVPortgroupConfigSpec()
2535     for prop in [
2536         "autoExpand",
2537         "configVersion",
2538         "defaultPortConfig",
2539         "description",
2540         "name",
2541         "numPorts",
2542         "policy",
2543         "portNameFormat",
2544         "scope",
2545         "type",
2546         "vendorSpecificConfig",
2547     ]:
2548         setattr(spec, prop, getattr(pg_props["config"], prop))
2549     _apply_dvportgroup_config(portgroup, spec, portgroup_dict)
2550     salt.utils.vmware.update_dvportgroup(pg_refs[0], spec)
2551     return True
2552 @depends(HAS_PYVMOMI)
2553 @_supports_proxies("esxdatacenter", "esxcluster")
2554 @_gets_service_instance_via_proxy
2555 def remove_dvportgroup(portgroup, dvs, service_instance=None):
2556     log.trace("Removing portgroup '%s' in dvs '%s'", portgroup, dvs)
2557     proxy_type = get_proxy_type()
2558     if proxy_type == "esxdatacenter":
2559         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2560         dc_ref = _get_proxy_target(service_instance)
2561     elif proxy_type == "esxcluster":
2562         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
2563         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
2564     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
2565     if not dvs_refs:
2566         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
2567     pg_refs = salt.utils.vmware.get_dvportgroups(
2568         dvs_refs[0], portgroup_names=[portgroup]
2569     )
2570     if not pg_refs:
2571         raise VMwareObjectRetrievalError(
2572             "Portgroup '{}' was not retrieved".format(portgroup)
2573         )
2574     salt.utils.vmware.remove_dvportgroup(pg_refs[0])
2575     return True
2576 def _get_policy_dict(policy):
2577     profile_dict = {
2578         "name": policy.name,
2579         "description": policy.description,
2580         "resource_type": policy.resourceType.resourceType,
2581     }
2582     subprofile_dicts = []
2583     if isinstance(policy, pbm.profile.CapabilityBasedProfile) and isinstance(
2584         policy.constraints, pbm.profile.SubProfileCapabilityConstraints
2585     ):
2586         for subprofile in policy.constraints.subProfiles:
2587             subprofile_dict = {
2588                 "name": subprofile.name,
2589                 "force_provision": subprofile.forceProvision,
2590             }
2591             cap_dicts = []
2592             for cap in subprofile.capability:
2593                 cap_dict = {"namespace": cap.id.namespace, "id": cap.id.id}
2594                 val = cap.constraint[0].propertyInstance[0].value
2595                 if isinstance(val, pbm.capability.types.Range):
2596                     val_dict = {"type": "range", "min": val.min, "max": val.max}
2597                 elif isinstance(val, pbm.capability.types.DiscreteSet):
2598                     val_dict = {"type": "set", "values": val.values}
2599                 else:
2600                     val_dict = {"type": "scalar", "value": val}
2601                 cap_dict["setting"] = val_dict
2602                 cap_dicts.append(cap_dict)
2603             subprofile_dict["capabilities"] = cap_dicts
2604             subprofile_dicts.append(subprofile_dict)
2605     profile_dict["subprofiles"] = subprofile_dicts
2606     return profile_dict
2607 @depends(HAS_PYVMOMI)
2608 @_supports_proxies("esxdatacenter", "vcenter")
2609 @_gets_service_instance_via_proxy
2610 def list_storage_policies(policy_names=None, service_instance=None):
2611     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2612     if not policy_names:
2613         policies = salt.utils.pbm.get_storage_policies(
2614             profile_manager, get_all_policies=True
2615         )
2616     else:
2617         policies = salt.utils.pbm.get_storage_policies(profile_manager, policy_names)
2618     return [_get_policy_dict(p) for p in policies]
2619 @depends(HAS_PYVMOMI)
2620 @_supports_proxies("esxdatacenter", "vcenter")
2621 @_gets_service_instance_via_proxy
2622 def list_default_vsan_policy(service_instance=None):
2623     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2624     policies = salt.utils.pbm.get_storage_policies(
2625         profile_manager, get_all_policies=True
2626     )
2627     def_policies = [
2628         p for p in policies if p.systemCreatedProfileType == "VsanDefaultProfile"
2629     ]
2630     if not def_policies:
2631         raise VMwareObjectRetrievalError("Default VSAN policy was not retrieved")
2632     return _get_policy_dict(def_policies[0])
2633 def _get_capability_definition_dict(cap_metadata):
2634     return {
2635         "namespace": cap_metadata.id.namespace,
2636         "id": cap_metadata.id.id,
2637         "mandatory": cap_metadata.mandatory,
2638         "description": cap_metadata.summary.summary,
2639         "type": cap_metadata.propertyMetadata[0].type.typeName,
2640     }
2641 @depends(HAS_PYVMOMI)
2642 @_supports_proxies("esxdatacenter", "vcenter")
2643 @_gets_service_instance_via_proxy
2644 def list_capability_definitions(service_instance=None):
2645     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2646     ret_list = [
2647         _get_capability_definition_dict(c)
2648         for c in salt.utils.pbm.get_capability_definitions(profile_manager)
2649     ]
2650     return ret_list
2651 def _apply_policy_config(policy_spec, policy_dict):
2652     log.trace("policy_dict = %s", policy_dict)
2653     if policy_dict.get("name"):
2654         policy_spec.name = policy_dict["name"]
2655     if policy_dict.get("description"):
2656         policy_spec.description = policy_dict["description"]
2657     if policy_dict.get("subprofiles"):
2658         policy_spec.constraints = pbm.profile.SubProfileCapabilityConstraints()
2659         subprofiles = []
2660         for subprofile_dict in policy_dict["subprofiles"]:
2661             subprofile_spec = pbm.profile.SubProfileCapabilityConstraints.SubProfile(
2662                 name=subprofile_dict["name"]
2663             )
2664             cap_specs = []
2665             if subprofile_dict.get("force_provision"):
2666                 subprofile_spec.forceProvision = subprofile_dict["force_provision"]
2667             for cap_dict in subprofile_dict["capabilities"]:
2668                 prop_inst_spec = pbm.capability.PropertyInstance(id=cap_dict["id"])
2669                 setting_type = cap_dict["setting"]["type"]
2670                 if setting_type == "set":
2671                     prop_inst_spec.value = pbm.capability.types.DiscreteSet()
2672                     prop_inst_spec.value.values = cap_dict["setting"]["values"]
2673                 elif setting_type == "range":
2674                     prop_inst_spec.value = pbm.capability.types.Range()
2675                     prop_inst_spec.value.max = cap_dict["setting"]["max"]
2676                     prop_inst_spec.value.min = cap_dict["setting"]["min"]
2677                 elif setting_type == "scalar":
2678                     prop_inst_spec.value = cap_dict["setting"]["value"]
2679                 cap_spec = pbm.capability.CapabilityInstance(
2680                     id=pbm.capability.CapabilityMetadata.UniqueId(
2681                         id=cap_dict["id"], namespace=cap_dict["namespace"]
2682                     ),
2683                     constraint=[
2684                         pbm.capability.ConstraintInstance(
2685                             propertyInstance=[prop_inst_spec]
2686                         )
2687                     ],
2688                 )
2689                 cap_specs.append(cap_spec)
2690             subprofile_spec.capability = cap_specs
2691             subprofiles.append(subprofile_spec)
2692         policy_spec.constraints.subProfiles = subprofiles
2693     log.trace("updated policy_spec = %s", policy_spec)
2694     return policy_spec
2695 @depends(HAS_PYVMOMI)
2696 @_supports_proxies("esxdatacenter", "vcenter")
2697 @_gets_service_instance_via_proxy
2698 def create_storage_policy(policy_name, policy_dict, service_instance=None):
2699     log.trace("create storage policy '%s', dict = %s", policy_name, policy_dict)
2700     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2701     policy_create_spec = pbm.profile.CapabilityBasedProfileCreateSpec()
2702     policy_create_spec.resourceType = pbm.profile.ResourceType(
2703         resourceType=pbm.profile.ResourceTypeEnum.STORAGE
2704     )
2705     policy_dict["name"] = policy_name
2706     log.trace("Setting policy values in policy_update_spec")
2707     _apply_policy_config(policy_create_spec, policy_dict)
2708     salt.utils.pbm.create_storage_policy(profile_manager, policy_create_spec)
2709     return {"create_storage_policy": True}
2710 @depends(HAS_PYVMOMI)
2711 @_supports_proxies("esxdatacenter", "vcenter")
2712 @_gets_service_instance_via_proxy
2713 def update_storage_policy(policy, policy_dict, service_instance=None):
2714     log.trace("updating storage policy, dict = %s", policy_dict)
2715     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2716     policies = salt.utils.pbm.get_storage_policies(profile_manager, [policy])
2717     if not policies:
2718         raise VMwareObjectRetrievalError("Policy '{}' was not found".format(policy))
2719     policy_ref = policies[0]
2720     policy_update_spec = pbm.profile.CapabilityBasedProfileUpdateSpec()
2721     log.trace("Setting policy values in policy_update_spec")
2722     for prop in ["description", "constraints"]:
2723         setattr(policy_update_spec, prop, getattr(policy_ref, prop))
2724     _apply_policy_config(policy_update_spec, policy_dict)
2725     salt.utils.pbm.update_storage_policy(
2726         profile_manager, policy_ref, policy_update_spec
2727     )
2728     return {"update_storage_policy": True}
2729 @depends(HAS_PYVMOMI)
2730 @_supports_proxies("esxcluster", "esxdatacenter", "vcenter")
2731 @_gets_service_instance_via_proxy
2732 def list_default_storage_policy_of_datastore(datastore, service_instance=None):
2733     log.trace("Listing the default storage policy of datastore '%s'", datastore)
2734     target_ref = _get_proxy_target(service_instance)
2735     ds_refs = salt.utils.vmware.get_datastores(
2736         service_instance, target_ref, datastore_names=[datastore]
2737     )
2738     if not ds_refs:
2739         raise VMwareObjectRetrievalError(
2740             "Datastore '{}' was not found".format(datastore)
2741         )
2742     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2743     policy = salt.utils.pbm.get_default_storage_policy_of_datastore(
2744         profile_manager, ds_refs[0]
2745     )
2746     return _get_policy_dict(policy)
2747 @depends(HAS_PYVMOMI)
2748 @_supports_proxies("esxcluster", "esxdatacenter", "vcenter")
2749 @_gets_service_instance_via_proxy
2750 def assign_default_storage_policy_to_datastore(
2751     policy, datastore, service_instance=None
2752 ):
2753     log.trace("Assigning policy %s to datastore %s", policy, datastore)
2754     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
2755     policies = salt.utils.pbm.get_storage_policies(profile_manager, [policy])
2756     if not policies:
2757         raise VMwareObjectRetrievalError("Policy '{}' was not found".format(policy))
2758     policy_ref = policies[0]
2759     target_ref = _get_proxy_target(service_instance)
2760     ds_refs = salt.utils.vmware.get_datastores(
2761         service_instance, target_ref, datastore_names=[datastore]
2762     )
2763     if not ds_refs:
2764         raise VMwareObjectRetrievalError(
2765             "Datastore '{}' was not found".format(datastore)
2766         )
2767     ds_ref = ds_refs[0]
2768     salt.utils.pbm.assign_default_storage_policy_to_datastore(
2769         profile_manager, policy_ref, ds_ref
2770     )
2771     return True
2772 @depends(HAS_PYVMOMI)
2773 @_supports_proxies("esxdatacenter", "esxcluster", "vcenter", "esxvm")
2774 @_gets_service_instance_via_proxy
2775 def list_datacenters_via_proxy(datacenter_names=None, service_instance=None):
2776     if not datacenter_names:
2777         dc_refs = salt.utils.vmware.get_datacenters(
2778             service_instance, get_all_datacenters=True
2779         )
2780     else:
2781         dc_refs = salt.utils.vmware.get_datacenters(service_instance, datacenter_names)
2782     return [
2783         {"name": salt.utils.vmware.get_managed_object_name(dc_ref)}
2784         for dc_ref in dc_refs
2785     ]
2786 @depends(HAS_PYVMOMI)
2787 @_supports_proxies("esxdatacenter", "vcenter")
2788 @_gets_service_instance_via_proxy
2789 def create_datacenter(datacenter_name, service_instance=None):
2790     salt.utils.vmware.create_datacenter(service_instance, datacenter_name)
2791     return {"create_datacenter": True}
2792 def _get_cluster_dict(cluster_name, cluster_ref):
2793     log.trace("Building a dictionary representation of cluster '%s'", cluster_name)
2794     props = salt.utils.vmware.get_properties_of_managed_object(
2795         cluster_ref, properties=["configurationEx"]
2796     )
2797     res = {
2798         "ha": {"enabled": props["configurationEx"].dasConfig.enabled},
2799         "drs": {"enabled": props["configurationEx"].drsConfig.enabled},
2800     }
2801     ha_conf = props["configurationEx"].dasConfig
2802     log.trace("ha_conf = %s", ha_conf)
2803     res["ha"]["admission_control_enabled"] = ha_conf.admissionControlEnabled
2804     if ha_conf.admissionControlPolicy and isinstance(
2805         ha_conf.admissionControlPolicy,
2806         vim.ClusterFailoverResourcesAdmissionControlPolicy,
2807     ):
2808         pol = ha_conf.admissionControlPolicy
2809         res["ha"]["admission_control_policy"] = {
2810             "cpu_failover_percent": pol.cpuFailoverResourcesPercent,
2811             "memory_failover_percent": pol.memoryFailoverResourcesPercent,
2812         }
2813     if ha_conf.defaultVmSettings:
2814         def_vm_set = ha_conf.defaultVmSettings
2815         res["ha"]["default_vm_settings"] = {
2816             "isolation_response": def_vm_set.isolationResponse,
2817             "restart_priority": def_vm_set.restartPriority,
2818         }
2819     res["ha"]["hb_ds_candidate_policy"] = ha_conf.hBDatastoreCandidatePolicy
2820     if ha_conf.hostMonitoring:
2821         res["ha"]["host_monitoring"] = ha_conf.hostMonitoring
2822     if ha_conf.option:
2823         res["ha"]["options"] = [
2824             {"key": o.key, "value": o.value} for o in ha_conf.option
2825         ]
2826     res["ha"]["vm_monitoring"] = ha_conf.vmMonitoring
2827     drs_conf = props["configurationEx"].drsConfig
2828     log.trace("drs_conf = %s", drs_conf)
2829     res["drs"]["vmotion_rate"] = 6 - drs_conf.vmotionRate
2830     res["drs"]["default_vm_behavior"] = drs_conf.defaultVmBehavior
2831     res["vm_swap_placement"] = props["configurationEx"].vmSwapPlacement
2832     si = salt.utils.vmware.get_service_instance_from_managed_object(cluster_ref)
2833     if salt.utils.vsan.vsan_supported(si):
2834         vcenter_info = salt.utils.vmware.get_service_info(si)
2835         if int(vcenter_info.build) &gt;= 3634794:  # 60u2
2836             vsan_conf = salt.utils.vsan.get_cluster_vsan_info(cluster_ref)
2837             log.trace("vsan_conf = %s", vsan_conf)
2838             res["vsan"] = {
2839                 "enabled": vsan_conf.enabled,
2840                 "auto_claim_storage": vsan_conf.defaultConfig.autoClaimStorage,
2841             }
2842             if vsan_conf.dataEfficiencyConfig:
2843                 data_eff = vsan_conf.dataEfficiencyConfig
2844                 res["vsan"].update(
2845                     {
2846                         "compression_enabled": data_eff.compressionEnabled or False,
2847                         "dedup_enabled": data_eff.dedupEnabled,
2848                     }
2849                 )
2850         else:  # before 60u2 (no advanced vsan info)
2851             if props["configurationEx"].vsanConfigInfo:
2852                 default_config = props["configurationEx"].vsanConfigInfo.defaultConfig
2853                 res["vsan"] = {
2854                     "enabled": props["configurationEx"].vsanConfigInfo.enabled,
2855                     "auto_claim_storage": default_config.autoClaimStorage,
2856                 }
2857     return res
2858 @depends(HAS_PYVMOMI)
2859 @_supports_proxies("esxcluster", "esxdatacenter")
2860 @_gets_service_instance_via_proxy
2861 def list_cluster(datacenter=None, cluster=None, service_instance=None):
2862     proxy_type = get_proxy_type()
2863     if proxy_type == "esxdatacenter":
2864         dc_ref = _get_proxy_target(service_instance)
2865         if not cluster:
2866             raise ArgumentValueError("'cluster' needs to be specified")
2867         cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)
2868     elif proxy_type == "esxcluster":
2869         cluster_ref = _get_proxy_target(service_instance)
2870         cluster = __salt__["esxcluster.get_details"]()["cluster"]
2871     log.trace(
2872         "Retrieving representation of cluster '%s' in a %s proxy", cluster, proxy_type
2873     )
2874     return _get_cluster_dict(cluster, cluster_ref)
2875 def _apply_cluster_dict(cluster_spec, cluster_dict, vsan_spec=None, vsan_61=True):
2876     log.trace("Applying cluster dict %s", cluster_dict)
2877     if cluster_dict.get("ha"):
2878         ha_dict = cluster_dict["ha"]
2879         if not cluster_spec.dasConfig:
2880             cluster_spec.dasConfig = vim.ClusterDasConfigInfo()
2881         das_config = cluster_spec.dasConfig
2882         if "enabled" in ha_dict:
2883             das_config.enabled = ha_dict["enabled"]
2884             if ha_dict["enabled"]:
2885                 das_config.failoverLevel = 1
2886         if "admission_control_enabled" in ha_dict:
2887             das_config.admissionControlEnabled = ha_dict["admission_control_enabled"]
2888         if "admission_control_policy" in ha_dict:
2889             adm_pol_dict = ha_dict["admission_control_policy"]
2890             if not das_config.admissionControlPolicy or not isinstance(
2891                 das_config.admissionControlPolicy,
2892                 vim.ClusterFailoverResourcesAdmissionControlPolicy,
2893             ):
2894                 das_config.admissionControlPolicy = (
2895                     vim.ClusterFailoverResourcesAdmissionControlPolicy(
2896                         cpuFailoverResourcesPercent=adm_pol_dict[
2897                             "cpu_failover_percent"
2898                         ],
2899                         memoryFailoverResourcesPercent=adm_pol_dict[
2900                             "memory_failover_percent"
2901                         ],
2902                     )
2903                 )
2904         if "default_vm_settings" in ha_dict:
2905             vm_set_dict = ha_dict["default_vm_settings"]
2906             if not das_config.defaultVmSettings:
2907                 das_config.defaultVmSettings = vim.ClusterDasVmSettings()
2908             if "isolation_response" in vm_set_dict:
2909                 das_config.defaultVmSettings.isolationResponse = vm_set_dict[
2910                     "isolation_response"
2911                 ]
2912             if "restart_priority" in vm_set_dict:
2913                 das_config.defaultVmSettings.restartPriority = vm_set_dict[
2914                     "restart_priority"
2915                 ]
2916         if "hb_ds_candidate_policy" in ha_dict:
2917             das_config.hBDatastoreCandidatePolicy = ha_dict["hb_ds_candidate_policy"]
2918         if "host_monitoring" in ha_dict:
2919             das_config.hostMonitoring = ha_dict["host_monitoring"]
2920         if "options" in ha_dict:
2921             das_config.option = []
2922             for opt_dict in ha_dict["options"]:
2923                 das_config.option.append(vim.OptionValue(key=opt_dict["key"]))
2924                 if "value" in opt_dict:
2925                     das_config.option[-1].value = opt_dict["value"]
2926         if "vm_monitoring" in ha_dict:
2927             das_config.vmMonitoring = ha_dict["vm_monitoring"]
2928         cluster_spec.dasConfig = das_config
2929     if cluster_dict.get("drs"):
2930         drs_dict = cluster_dict["drs"]
2931         drs_config = vim.ClusterDrsConfigInfo()
2932         if "enabled" in drs_dict:
2933             drs_config.enabled = drs_dict["enabled"]
2934         if "vmotion_rate" in drs_dict:
2935             drs_config.vmotionRate = 6 - drs_dict["vmotion_rate"]
2936         if "default_vm_behavior" in drs_dict:
2937             drs_config.defaultVmBehavior = vim.DrsBehavior(
2938                 drs_dict["default_vm_behavior"]
2939             )
2940         cluster_spec.drsConfig = drs_config
2941     if cluster_dict.get("vm_swap_placement"):
2942         cluster_spec.vmSwapPlacement = cluster_dict["vm_swap_placement"]
2943     if cluster_dict.get("vsan"):
2944         vsan_dict = cluster_dict["vsan"]
2945         if not vsan_61:  # VSAN is 6.2 and above
2946             if "enabled" in vsan_dict:
2947                 if not vsan_spec.vsanClusterConfig:
2948                     vsan_spec.vsanClusterConfig = vim.vsan.cluster.ConfigInfo()
2949                 vsan_spec.vsanClusterConfig.enabled = vsan_dict["enabled"]
2950             if "auto_claim_storage" in vsan_dict:
2951                 if not vsan_spec.vsanClusterConfig:
2952                     vsan_spec.vsanClusterConfig = vim.vsan.cluster.ConfigInfo()
2953                 if not vsan_spec.vsanClusterConfig.defaultConfig:
2954                     vsan_spec.vsanClusterConfig.defaultConfig = (
2955                         vim.VsanClusterConfigInfoHostDefaultInfo()
2956                     )
2957                 elif vsan_spec.vsanClusterConfig.defaultConfig.uuid:
2958                     vsan_spec.vsanClusterConfig.defaultConfig.uuid = None
2959                 vsan_spec.vsanClusterConfig.defaultConfig.autoClaimStorage = vsan_dict[
2960                     "auto_claim_storage"
2961                 ]
2962             if "compression_enabled" in vsan_dict:
2963                 if not vsan_spec.dataEfficiencyConfig:
2964                     vsan_spec.dataEfficiencyConfig = vim.vsan.DataEfficiencyConfig()
2965                 vsan_spec.dataEfficiencyConfig.compressionEnabled = vsan_dict[
2966                     "compression_enabled"
2967                 ]
2968             if "dedup_enabled" in vsan_dict:
2969                 if not vsan_spec.dataEfficiencyConfig:
2970                     vsan_spec.dataEfficiencyConfig = vim.vsan.DataEfficiencyConfig()
2971                 vsan_spec.dataEfficiencyConfig.dedupEnabled = vsan_dict["dedup_enabled"]
2972         if not cluster_spec.vsanConfig:
2973             cluster_spec.vsanConfig = vim.VsanClusterConfigInfo()
2974         vsan_config = cluster_spec.vsanConfig
2975         if "enabled" in vsan_dict:
2976             vsan_config.enabled = vsan_dict["enabled"]
2977         if "auto_claim_storage" in vsan_dict:
2978             if not vsan_config.defaultConfig:
2979                 vsan_config.defaultConfig = vim.VsanClusterConfigInfoHostDefaultInfo()
2980             elif vsan_config.defaultConfig.uuid:
2981                 vsan_config.defaultConfig.uuid = None
2982             vsan_config.defaultConfig.autoClaimStorage = vsan_dict["auto_claim_storage"]
2983     log.trace("cluster_spec = %s", cluster_spec)
2984 @depends(HAS_PYVMOMI)
2985 @depends(HAS_JSONSCHEMA)
2986 @_supports_proxies("esxcluster", "esxdatacenter")
2987 @_gets_service_instance_via_proxy
2988 def create_cluster(cluster_dict, datacenter=None, cluster=None, service_instance=None):
2989     schema = ESXClusterConfigSchema.serialize()
2990     try:
2991         jsonschema.validate(cluster_dict, schema)
2992     except jsonschema.exceptions.ValidationError as exc:
2993         raise InvalidConfigError(exc)
2994     proxy_type = get_proxy_type()
2995     if proxy_type == "esxdatacenter":
2996         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
2997         dc_ref = _get_proxy_target(service_instance)
2998         if not cluster:
2999             raise ArgumentValueError("'cluster' needs to be specified")
3000     elif proxy_type == "esxcluster":
3001         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3002         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3003         cluster = __salt__["esxcluster.get_details"]()["cluster"]
3004     if cluster_dict.get("vsan") and not salt.utils.vsan.vsan_supported(
3005         service_instance
3006     ):
3007         raise VMwareApiError("VSAN operations are not supported")
3008     si = service_instance
3009     cluster_spec = vim.ClusterConfigSpecEx()
3010     vsan_spec = None
3011     ha_config = None
3012     vsan_61 = None
3013     if cluster_dict.get("vsan"):
3014         vcenter_info = salt.utils.vmware.get_service_info(si)
3015         if (
3016             float(vcenter_info.apiVersion) &gt;= 6.0 and int(vcenter_info.build) &gt;= 3634794
3017         ):  # 60u2
3018             vsan_spec = vim.vsan.ReconfigSpec(modify=True)
3019             vsan_61 = False
3020             if cluster_dict.get("ha", {}).get("enabled"):
3021                 enable_ha = True
3022                 ha_config = cluster_dict["ha"]
3023                 del cluster_dict["ha"]
3024         else:
3025             vsan_61 = True
3026     _apply_cluster_dict(cluster_spec, cluster_dict, vsan_spec, vsan_61)
3027     salt.utils.vmware.create_cluster(dc_ref, cluster, cluster_spec)
3028     if not vsan_61:
3029         if vsan_spec:
3030             cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)
3031             salt.utils.vsan.reconfigure_cluster_vsan(cluster_ref, vsan_spec)
3032         if enable_ha:
3033             _apply_cluster_dict(cluster_spec, {"ha": ha_config})
3034             salt.utils.vmware.update_cluster(cluster_ref, cluster_spec)
3035             cluster_dict["ha"] = ha_config
3036     return {"create_cluster": True}
3037 @depends(HAS_PYVMOMI)
3038 @depends(HAS_JSONSCHEMA)
3039 @_supports_proxies("esxcluster", "esxdatacenter")
3040 @_gets_service_instance_via_proxy
3041 def update_cluster(cluster_dict, datacenter=None, cluster=None, service_instance=None):
3042     schema = ESXClusterConfigSchema.serialize()
3043     try:
3044         jsonschema.validate(cluster_dict, schema)
3045     except jsonschema.exceptions.ValidationError as exc:
3046         raise InvalidConfigError(exc)
3047     proxy_type = get_proxy_type()
3048     if proxy_type == "esxdatacenter":
3049         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
3050         dc_ref = _get_proxy_target(service_instance)
3051         if not cluster:
3052             raise ArgumentValueError("'cluster' needs to be specified")
3053     elif proxy_type == "esxcluster":
3054         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3055         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3056         cluster = __salt__["esxcluster.get_details"]()["cluster"]
3057     if cluster_dict.get("vsan") and not salt.utils.vsan.vsan_supported(
3058         service_instance
3059     ):
3060         raise VMwareApiError("VSAN operations are not supported")
3061     cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)
3062     cluster_spec = vim.ClusterConfigSpecEx()
3063     props = salt.utils.vmware.get_properties_of_managed_object(
3064         cluster_ref, properties=["configurationEx"]
3065     )
3066     for p in ["dasConfig", "drsConfig"]:
3067         setattr(cluster_spec, p, getattr(props["configurationEx"], p))
3068     if props["configurationEx"].vsanConfigInfo:
3069         cluster_spec.vsanConfig = props["configurationEx"].vsanConfigInfo
3070     vsan_spec = None
3071     vsan_61 = None
3072     if cluster_dict.get("vsan"):
3073         vcenter_info = salt.utils.vmware.get_service_info(service_instance)
3074         if (
3075             float(vcenter_info.apiVersion) &gt;= 6.0 and int(vcenter_info.build) &gt;= 3634794
3076         ):  # 60u2
3077             vsan_61 = False
3078             vsan_info = salt.utils.vsan.get_cluster_vsan_info(cluster_ref)
3079             vsan_spec = vim.vsan.ReconfigSpec(modify=True)
3080             vsan_spec.dataEfficiencyConfig = vsan_info.dataEfficiencyConfig
3081             vsan_info.dataEfficiencyConfig = None
3082         else:
3083             vsan_61 = True
3084     _apply_cluster_dict(cluster_spec, cluster_dict, vsan_spec, vsan_61)
3085     if vsan_spec:
3086         log.trace("vsan_spec = %s", vsan_spec)
3087         salt.utils.vsan.reconfigure_cluster_vsan(cluster_ref, vsan_spec)
3088         cluster_spec = vim.ClusterConfigSpecEx()
3089         props = salt.utils.vmware.get_properties_of_managed_object(
3090             cluster_ref, properties=["configurationEx"]
3091         )
3092         for p in ["dasConfig", "drsConfig"]:
3093             setattr(cluster_spec, p, getattr(props["configurationEx"], p))
3094         if props["configurationEx"].vsanConfigInfo:
3095             cluster_spec.vsanConfig = props["configurationEx"].vsanConfigInfo
3096         _apply_cluster_dict(cluster_spec, cluster_dict)
3097     salt.utils.vmware.update_cluster(cluster_ref, cluster_spec)
3098     return {"update_cluster": True}
3099 @depends(HAS_PYVMOMI)
3100 @_supports_proxies("esxi", "esxcluster", "esxdatacenter")
3101 @_gets_service_instance_via_proxy
3102 def list_datastores_via_proxy(
3103     datastore_names=None,
3104     backing_disk_ids=None,
3105     backing_disk_scsi_addresses=None,
3106     service_instance=None,
3107 ):
3108     target = _get_proxy_target(service_instance)
3109     target_name = salt.utils.vmware.get_managed_object_name(target)
3110     log.trace("target name = %s", target_name)
3111     get_all_datastores = (
3112         True
3113         if not (datastore_names or backing_disk_ids or backing_disk_scsi_addresses)
3114         else False
3115     )
3116     if backing_disk_scsi_addresses:
3117         log.debug(
3118             "Retrieving disk ids for scsi addresses '%s'", backing_disk_scsi_addresses
3119         )
3120         disk_ids = [
3121             d.canonicalName
3122             for d in salt.utils.vmware.get_disks(
3123                 target, scsi_addresses=backing_disk_scsi_addresses
3124             )
3125         ]
3126         log.debug("Found disk ids '%s'", disk_ids)
3127         backing_disk_ids = (
3128             backing_disk_ids.extend(disk_ids) if backing_disk_ids else disk_ids
3129         )
3130     datastores = salt.utils.vmware.get_datastores(
3131         service_instance, target, datastore_names, backing_disk_ids, get_all_datastores
3132     )
3133     mount_infos = []
3134     if isinstance(target, vim.HostSystem):
3135         storage_system = salt.utils.vmware.get_storage_system(
3136             service_instance, target, target_name
3137         )
3138         props = salt.utils.vmware.get_properties_of_managed_object(
3139             storage_system, ["fileSystemVolumeInfo.mountInfo"]
3140         )
3141         mount_infos = props.get("fileSystemVolumeInfo.mountInfo", [])
3142     ret_dict = []
3143     for ds in datastores:
3144         ds_dict = {
3145             "name": ds.name,
3146             "type": ds.summary.type,
3147             "free_space": ds.summary.freeSpace,
3148             "capacity": ds.summary.capacity,
3149         }
3150         backing_disk_ids = []
3151         for vol in [
3152             i.volume
3153             for i in mount_infos
3154             if i.volume.name == ds.name and isinstance(i.volume, vim.HostVmfsVolume)
3155         ]:
3156             backing_disk_ids.extend([e.diskName for e in vol.extent])
3157         if backing_disk_ids:
3158             ds_dict["backing_disk_ids"] = backing_disk_ids
3159         ret_dict.append(ds_dict)
3160     return ret_dict
3161 @depends(HAS_PYVMOMI)
3162 @depends(HAS_JSONSCHEMA)
3163 @_supports_proxies("esxi")
3164 @_gets_service_instance_via_proxy
3165 def create_vmfs_datastore(
3166     datastore_name,
3167     disk_id,
3168     vmfs_major_version,
3169     safety_checks=True,
3170     service_instance=None,
3171 ):
3172     log.debug("Validating vmfs datastore input")
3173     schema = VmfsDatastoreSchema.serialize()
3174     try:
3175         jsonschema.validate(
3176             {
3177                 "datastore": {
3178                     "name": datastore_name,
3179                     "backing_disk_id": disk_id,
3180                     "vmfs_version": vmfs_major_version,
3181                 }
3182             },
3183             schema,
3184         )
3185     except jsonschema.exceptions.ValidationError as exc:
3186         raise ArgumentValueError(exc)
3187     host_ref = _get_proxy_target(service_instance)
3188     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3189     if safety_checks:
3190         disks = salt.utils.vmware.get_disks(host_ref, disk_ids=[disk_id])
3191         if not disks:
3192             raise VMwareObjectRetrievalError(
3193                 "Disk '{}' was not found in host '{}'".format(disk_id, hostname)
3194             )
3195     ds_ref = salt.utils.vmware.create_vmfs_datastore(
3196         host_ref, datastore_name, disks[0], vmfs_major_version
3197     )
3198     return True
3199 @depends(HAS_PYVMOMI)
3200 @_supports_proxies("esxi", "esxcluster", "esxdatacenter")
3201 @_gets_service_instance_via_proxy
3202 def rename_datastore(datastore_name, new_datastore_name, service_instance=None):
3203     log.trace("Renaming datastore %s to %s", datastore_name, new_datastore_name)
3204     target = _get_proxy_target(service_instance)
3205     datastores = salt.utils.vmware.get_datastores(
3206         service_instance, target, datastore_names=[datastore_name]
3207     )
3208     if not datastores:
3209         raise VMwareObjectRetrievalError(
3210             "Datastore '{}' was not found".format(datastore_name)
3211         )
3212     ds = datastores[0]
3213     salt.utils.vmware.rename_datastore(ds, new_datastore_name)
3214     return True
3215 @depends(HAS_PYVMOMI)
3216 @_supports_proxies("esxi", "esxcluster", "esxdatacenter")
3217 @_gets_service_instance_via_proxy
3218 def remove_datastore(datastore, service_instance=None):
3219     log.trace("Removing datastore '%s'", datastore)
3220     target = _get_proxy_target(service_instance)
3221     datastores = salt.utils.vmware.get_datastores(
3222         service_instance, reference=target, datastore_names=[datastore]
3223     )
3224     if not datastores:
3225         raise VMwareObjectRetrievalError(
3226             "Datastore '{}' was not found".format(datastore)
3227         )
3228     if len(datastores) &gt; 1:
3229         raise VMwareObjectRetrievalError(
3230             "Multiple datastores '{}' were found".format(datastore)
3231         )
3232     salt.utils.vmware.remove_datastore(service_instance, datastores[0])
3233     return True
3234 @depends(HAS_PYVMOMI)
3235 @_supports_proxies("esxcluster", "esxdatacenter")
3236 @_gets_service_instance_via_proxy
3237 def list_licenses(service_instance=None):
3238     log.trace("Retrieving all licenses")
3239     licenses = salt.utils.vmware.get_licenses(service_instance)
3240     ret_dict = [
3241         {
3242             "key": l.licenseKey,
3243             "name": l.name,
3244             "description": l.labels[0].value if l.labels else None,
3245             "capacity": l.total if l.total &gt; 0 else sys.maxsize,
3246             "used": l.used if l.used else 0,
3247         }
3248         for l in licenses
3249     ]
3250     return ret_dict
3251 @depends(HAS_PYVMOMI)
3252 @_supports_proxies("esxcluster", "esxdatacenter")
3253 @_gets_service_instance_via_proxy
3254 def add_license(key, description, safety_checks=True, service_instance=None):
3255     log.trace("Adding license '%s'", key)
3256     salt.utils.vmware.add_license(service_instance, key, description)
3257     return True
3258 def _get_entity(service_instance, entity):
3259     log.trace("Retrieving entity: %s", entity)
3260     if entity["type"] == "cluster":
3261         dc_ref = salt.utils.vmware.get_datacenter(
3262             service_instance, entity["datacenter"]
3263         )
3264         return salt.utils.vmware.get_cluster(dc_ref, entity["cluster"])
3265     elif entity["type"] == "vcenter":
3266         return None
3267     raise ArgumentValueError("Unsupported entity type '{}'".format(entity["type"]))
3268 def _validate_entity(entity):
3269     if entity["type"] == "cluster":
3270         schema = ESXClusterEntitySchema.serialize()
3271     elif entity["type"] == "vcenter":
3272         schema = VCenterEntitySchema.serialize()
3273     else:
3274         raise ArgumentValueError("Unsupported entity type '{}'".format(entity["type"]))
3275     try:
3276         jsonschema.validate(entity, schema)
3277     except jsonschema.exceptions.ValidationError as exc:
3278         raise InvalidEntityError(exc)
3279 @depends(HAS_PYVMOMI)
3280 @depends(HAS_JSONSCHEMA)
3281 @_supports_proxies("esxcluster", "esxdatacenter")
3282 @_gets_service_instance_via_proxy
3283 def list_assigned_licenses(
3284     entity, entity_display_name, license_keys=None, service_instance=None
3285 ):
3286     log.trace("Listing assigned licenses of entity %s", entity)
3287     _validate_entity(entity)
3288     assigned_licenses = salt.utils.vmware.get_assigned_licenses(
3289         service_instance,
3290         entity_ref=_get_entity(service_instance, entity),
3291         entity_name=entity_display_name,
3292     )
3293     return [
3294         {
3295             "key": l.licenseKey,
3296             "name": l.name,
3297             "description": l.labels[0].value if l.labels else None,
3298             "capacity": l.total if l.total &gt; 0 else sys.maxsize,
3299         }
3300         for l in assigned_licenses
3301         if (license_keys is None) or (l.licenseKey in license_keys)
3302     ]
3303 @depends(HAS_PYVMOMI)
3304 @depends(HAS_JSONSCHEMA)
3305 @_supports_proxies("esxcluster", "esxdatacenter")
3306 @_gets_service_instance_via_proxy
3307 def assign_license(
3308     license_key,
3309     license_name,
3310     entity,
3311     entity_display_name,
3312     safety_checks=True,
3313     service_instance=None,
3314 ):
3315     log.trace("Assigning license %s to entity %s", license_key, entity)
3316     _validate_entity(entity)
3317     if safety_checks:
3318         licenses = salt.utils.vmware.get_licenses(service_instance)
3319         if not [l for l in licenses if l.licenseKey == license_key]:
3320             raise VMwareObjectRetrievalError(
3321                 "License '{}' wasn't found".format(license_name)
3322             )
3323     salt.utils.vmware.assign_license(
3324         service_instance,
3325         license_key,
3326         license_name,
3327         entity_ref=_get_entity(service_instance, entity),
3328         entity_name=entity_display_name,
3329     )
3330 @depends(HAS_PYVMOMI)
3331 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter")
3332 @_gets_service_instance_via_proxy
3333 def list_hosts_via_proxy(
3334     hostnames=None, datacenter=None, cluster=None, service_instance=None
3335 ):
3336     if cluster:
3337         if not datacenter:
3338             raise salt.exceptions.ArgumentValueError(
3339                 "Datacenter is required when cluster is specified"
3340             )
3341     get_all_hosts = False
3342     if not hostnames:
3343         get_all_hosts = True
3344     hosts = salt.utils.vmware.get_hosts(
3345         service_instance,
3346         datacenter_name=datacenter,
3347         host_names=hostnames,
3348         cluster_name=cluster,
3349         get_all_hosts=get_all_hosts,
3350     )
3351     return [salt.utils.vmware.get_managed_object_name(h) for h in hosts]
3352 @depends(HAS_PYVMOMI)
3353 @_supports_proxies("esxi")
3354 @_gets_service_instance_via_proxy
3355 def list_disks(disk_ids=None, scsi_addresses=None, service_instance=None):
3356     host_ref = _get_proxy_target(service_instance)
3357     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3358     log.trace(
3359         "Retrieving disks of host '%s'; disc ids = %s; scsi_address = %s",
3360         hostname,
3361         disk_ids,
3362         scsi_addresses,
3363     )
3364     get_all_disks = True if not (disk_ids or scsi_addresses) else False
3365     ret_list = []
3366     scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(
3367         host_ref, hostname=hostname
3368     )
3369     canonical_name_to_scsi_address = {
3370         lun.canonicalName: scsi_addr for scsi_addr, lun in scsi_address_to_lun.items()
3371     }
3372     for d in salt.utils.vmware.get_disks(
3373         host_ref, disk_ids, scsi_addresses, get_all_disks
3374     ):
3375         ret_list.append(
3376             {
3377                 "id": d.canonicalName,
3378                 "scsi_address": canonical_name_to_scsi_address[d.canonicalName],
3379             }
3380         )
3381     return ret_list
3382 @depends(HAS_PYVMOMI)
3383 @_supports_proxies("esxi")
3384 @_gets_service_instance_via_proxy
3385 def erase_disk_partitions(disk_id=None, scsi_address=None, service_instance=None):
3386     if not disk_id and not scsi_address:
3387         raise ArgumentValueError(
3388             "Either 'disk_id' or 'scsi_address' needs to be specified"
3389         )
3390     host_ref = _get_proxy_target(service_instance)
3391     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3392     if not disk_id:
3393         scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(host_ref)
3394         if scsi_address not in scsi_address_to_lun:
3395             raise VMwareObjectRetrievalError(
3396                 "Scsi lun with address '{}' was not found on host '{}'".format(
3397                     scsi_address, hostname
3398                 )
3399             )
3400         disk_id = scsi_address_to_lun[scsi_address].canonicalName
3401         log.trace(
3402             "[%s] Got disk id '%s' for scsi address '%s'",
3403             hostname,
3404             disk_id,
3405             scsi_address,
3406         )
3407     log.trace("Erasing disk partitions on disk '%s' in host '%s'", disk_id, hostname)
3408     salt.utils.vmware.erase_disk_partitions(
3409         service_instance, host_ref, disk_id, hostname=hostname
3410     )
3411     log.info("Erased disk partitions on disk '%s' on host '%s'", disk_id, hostname)
3412     return True
3413 @depends(HAS_PYVMOMI)
3414 @_supports_proxies("esxi")
3415 @_gets_service_instance_via_proxy
3416 def list_disk_partitions(disk_id=None, scsi_address=None, service_instance=None):
3417     if not disk_id and not scsi_address:
3418         raise ArgumentValueError(
3419             "Either 'disk_id' or 'scsi_address' needs to be specified"
3420         )
3421     host_ref = _get_proxy_target(service_instance)
3422     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3423     if not disk_id:
3424         scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(host_ref)
3425         if scsi_address not in scsi_address_to_lun:
3426             raise VMwareObjectRetrievalError(
3427                 "Scsi lun with address '{}' was not found on host '{}'".format(
3428                     scsi_address, hostname
3429                 )
3430             )
3431         disk_id = scsi_address_to_lun[scsi_address].canonicalName
3432         log.trace(
3433             "[%s] Got disk id '%s' for scsi address '%s'",
3434             hostname,
3435             disk_id,
3436             scsi_address,
3437         )
3438     log.trace("Listing disk partitions on disk '%s' in host '%s'", disk_id, hostname)
3439     partition_info = salt.utils.vmware.get_disk_partition_info(host_ref, disk_id)
3440     ret_list = []
3441     for part_spec in partition_info.spec.partition:
3442         part_layout = [
3443             p
3444             for p in partition_info.layout.partition
3445             if p.partition == part_spec.partition
3446         ][0]
3447         part_dict = {
3448             "hostname": hostname,
3449             "device": disk_id,
3450             "format": partition_info.spec.partitionFormat,
3451             "partition": part_spec.partition,
3452             "type": part_spec.type,
3453             "sectors": part_spec.endSector - part_spec.startSector + 1,
3454             "size_KB": (part_layout.end.block - part_layout.start.block + 1)
3455             * part_layout.start.blockSize
3456             / 1024,
3457         }
3458         ret_list.append(part_dict)
3459     return ret_list
3460 @depends(HAS_PYVMOMI)
3461 @_supports_proxies("esxi")
3462 @_gets_service_instance_via_proxy
3463 def list_diskgroups(cache_disk_ids=None, service_instance=None):
3464     host_ref = _get_proxy_target(service_instance)
3465     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3466     log.trace("Listing diskgroups in '%s'", hostname)
3467     get_all_diskgroups = True if not cache_disk_ids else False
3468     ret_list = []
3469     for dg in salt.utils.vmware.get_diskgroups(
3470         host_ref, cache_disk_ids, get_all_diskgroups
3471     ):
3472         ret_list.append(
3473             {
3474                 "cache_disk": dg.ssd.canonicalName,
3475                 "capacity_disks": [d.canonicalName for d in dg.nonSsd],
3476             }
3477         )
3478     return ret_list
3479 @depends(HAS_PYVMOMI)
3480 @depends(HAS_JSONSCHEMA)
3481 @_supports_proxies("esxi")
3482 @_gets_service_instance_via_proxy
3483 def create_diskgroup(
3484     cache_disk_id, capacity_disk_ids, safety_checks=True, service_instance=None
3485 ):
3486     log.trace("Validating diskgroup input")
3487     schema = DiskGroupsDiskIdSchema.serialize()
3488     try:
3489         jsonschema.validate(
3490             {
3491                 "diskgroups": [
3492                     {"cache_id": cache_disk_id, "capacity_ids": capacity_disk_ids}
3493                 ]
3494             },
3495             schema,
3496         )
3497     except jsonschema.exceptions.ValidationError as exc:
3498         raise ArgumentValueError(exc)
3499     host_ref = _get_proxy_target(service_instance)
3500     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3501     if safety_checks:
3502         diskgroups = salt.utils.vmware.get_diskgroups(host_ref, [cache_disk_id])
3503         if diskgroups:
3504             raise VMwareObjectExistsError(
3505                 "Diskgroup with cache disk id '{}' already exists ESXi "
3506                 "host '{}'".format(cache_disk_id, hostname)
3507             )
3508     disk_ids = capacity_disk_ids[:]
3509     disk_ids.insert(0, cache_disk_id)
3510     disks = salt.utils.vmware.get_disks(host_ref, disk_ids=disk_ids)
3511     for id in disk_ids:
3512         if not [d for d in disks if d.canonicalName == id]:
3513             raise VMwareObjectRetrievalError(
3514                 "No disk with id '{}' was found in ESXi host '{}'".format(id, hostname)
3515             )
3516     cache_disk = [d for d in disks if d.canonicalName == cache_disk_id][0]
3517     capacity_disks = [d for d in disks if d.canonicalName in capacity_disk_ids]
3518     vsan_disk_mgmt_system = salt.utils.vsan.get_vsan_disk_management_system(
3519         service_instance
3520     )
3521     dg = salt.utils.vsan.create_diskgroup(
3522         service_instance, vsan_disk_mgmt_system, host_ref, cache_disk, capacity_disks
3523     )
3524     return True
3525 @depends(HAS_PYVMOMI)
3526 @depends(HAS_JSONSCHEMA)
3527 @_supports_proxies("esxi")
3528 @_gets_service_instance_via_proxy
3529 def add_capacity_to_diskgroup(
3530     cache_disk_id, capacity_disk_ids, safety_checks=True, service_instance=None
3531 ):
3532     log.trace("Validating diskgroup input")
3533     schema = DiskGroupsDiskIdSchema.serialize()
3534     try:
3535         jsonschema.validate(
3536             {
3537                 "diskgroups": [
3538                     {"cache_id": cache_disk_id, "capacity_ids": capacity_disk_ids}
3539                 ]
3540             },
3541             schema,
3542         )
3543     except jsonschema.exceptions.ValidationError as exc:
3544         raise ArgumentValueError(exc)
3545     host_ref = _get_proxy_target(service_instance)
3546     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3547     disks = salt.utils.vmware.get_disks(host_ref, disk_ids=capacity_disk_ids)
3548     if safety_checks:
3549         for id in capacity_disk_ids:
3550             if not [d for d in disks if d.canonicalName == id]:
3551                 raise VMwareObjectRetrievalError(
3552                     "No disk with id '{}' was found in ESXi host '{}'".format(
3553                         id, hostname
3554                     )
3555                 )
3556     diskgroups = salt.utils.vmware.get_diskgroups(
3557         host_ref, cache_disk_ids=[cache_disk_id]
3558     )
3559     if not diskgroups:
3560         raise VMwareObjectRetrievalError(
3561             "No diskgroup with cache disk id '{}' was found in ESXi host '{}'".format(
3562                 cache_disk_id, hostname
3563             )
3564         )
3565     vsan_disk_mgmt_system = salt.utils.vsan.get_vsan_disk_management_system(
3566         service_instance
3567     )
3568     salt.utils.vsan.add_capacity_to_diskgroup(
3569         service_instance, vsan_disk_mgmt_system, host_ref, diskgroups[0], disks
3570     )
3571     return True
3572 @depends(HAS_PYVMOMI)
3573 @depends(HAS_JSONSCHEMA)
3574 @_supports_proxies("esxi")
3575 @_gets_service_instance_via_proxy
3576 def remove_capacity_from_diskgroup(
3577     cache_disk_id,
3578     capacity_disk_ids,
3579     data_evacuation=True,
3580     safety_checks=True,
3581     service_instance=None,
3582 ):
3583     log.trace("Validating diskgroup input")
3584     schema = DiskGroupsDiskIdSchema.serialize()
3585     try:
3586         jsonschema.validate(
3587             {
3588                 "diskgroups": [
3589                     {"cache_id": cache_disk_id, "capacity_ids": capacity_disk_ids}
3590                 ]
3591             },
3592             schema,
3593         )
3594     except jsonschema.exceptions.ValidationError as exc:
3595         raise ArgumentValueError(str(exc))
3596     host_ref = _get_proxy_target(service_instance)
3597     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3598     disks = salt.utils.vmware.get_disks(host_ref, disk_ids=capacity_disk_ids)
3599     if safety_checks:
3600         for id in capacity_disk_ids:
3601             if not [d for d in disks if d.canonicalName == id]:
3602                 raise VMwareObjectRetrievalError(
3603                     "No disk with id '{}' was found in ESXi host '{}'".format(
3604                         id, hostname
3605                     )
3606                 )
3607     diskgroups = salt.utils.vmware.get_diskgroups(
3608         host_ref, cache_disk_ids=[cache_disk_id]
3609     )
3610     if not diskgroups:
3611         raise VMwareObjectRetrievalError(
3612             "No diskgroup with cache disk id '{}' was found in ESXi host '{}'".format(
3613                 cache_disk_id, hostname
3614             )
3615         )
3616     log.trace("data_evacuation = %s", data_evacuation)
3617     salt.utils.vsan.remove_capacity_from_diskgroup(
3618         service_instance,
3619         host_ref,
3620         diskgroups[0],
3621         capacity_disks=[d for d in disks if d.canonicalName in capacity_disk_ids],
3622         data_evacuation=data_evacuation,
3623     )
3624     return True
3625 @depends(HAS_PYVMOMI)
3626 @depends(HAS_JSONSCHEMA)
3627 @_supports_proxies("esxi")
3628 @_gets_service_instance_via_proxy
3629 def remove_diskgroup(cache_disk_id, data_accessibility=True, service_instance=None):
3630     log.trace("Validating diskgroup input")
3631     host_ref = _get_proxy_target(service_instance)
3632     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3633     diskgroups = salt.utils.vmware.get_diskgroups(
3634         host_ref, cache_disk_ids=[cache_disk_id]
3635     )
3636     if not diskgroups:
3637         raise VMwareObjectRetrievalError(
3638             "No diskgroup with cache disk id '{}' was found in ESXi host '{}'".format(
3639                 cache_disk_id, hostname
3640             )
3641         )
3642     log.trace("data accessibility = %s", data_accessibility)
3643     salt.utils.vsan.remove_diskgroup(
3644         service_instance, host_ref, diskgroups[0], data_accessibility=data_accessibility
3645     )
3646     return True
3647 @depends(HAS_PYVMOMI)
3648 @_supports_proxies("esxi")
3649 @_gets_service_instance_via_proxy
3650 def get_host_cache(service_instance=None):
3651     ret_dict = {}
3652     host_ref = _get_proxy_target(service_instance)
3653     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3654     hci = salt.utils.vmware.get_host_cache(host_ref)
3655     if not hci:
3656         log.debug("Host cache not configured on host '%s'", hostname)
3657         ret_dict["enabled"] = False
3658         return ret_dict
3659     return {
3660         "enabled": True,
3661         "datastore": {"name": hci.key.name},
3662         "swap_size": "{}MiB".format(hci.swapSize),
3663     }
3664 @depends(HAS_PYVMOMI)
3665 @depends(HAS_JSONSCHEMA)
3666 @_supports_proxies("esxi")
3667 @_gets_service_instance_via_proxy
3668 def configure_host_cache(
3669     enabled, datastore=None, swap_size_MiB=None, service_instance=None
3670 ):
3671     log.debug("Validating host cache input")
3672     schema = SimpleHostCacheSchema.serialize()
3673     try:
3674         jsonschema.validate(
3675             {
3676                 "enabled": enabled,
3677                 "datastore_name": datastore,
3678                 "swap_size_MiB": swap_size_MiB,
3679             },
3680             schema,
3681         )
3682     except jsonschema.exceptions.ValidationError as exc:
3683         raise ArgumentValueError(exc)
3684     if not enabled:
3685         raise ArgumentValueError("Disabling the host cache is not supported")
3686     ret_dict = {"enabled": False}
3687     host_ref = _get_proxy_target(service_instance)
3688     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
3689     if datastore:
3690         ds_refs = salt.utils.vmware.get_datastores(
3691             service_instance, host_ref, datastore_names=[datastore]
3692         )
3693         if not ds_refs:
3694             raise VMwareObjectRetrievalError(
3695                 "Datastore '{}' was not found on host '{}'".format(datastore, hostname)
3696             )
3697         ds_ref = ds_refs[0]
3698     salt.utils.vmware.configure_host_cache(host_ref, ds_ref, swap_size_MiB)
3699     return True
3700 def _check_hosts(service_instance, host, host_names):
3701     if not host_names:
3702         host_name = _get_host_ref(service_instance, host)
3703         if host_name:
3704             host_names = [host]
3705         else:
3706             raise CommandExecutionError(
3707                 "No host reference found. If connecting to a "
3708                 "vCenter Server, a list of 'host_names' must be "
3709                 "provided."
3710             )
3711     elif not isinstance(host_names, list):
3712         raise CommandExecutionError("'host_names' must be a list.")
3713     return host_names
3714 def _format_coredump_stdout(cmd_ret):
3715     ret_dict = {}
3716     for line in cmd_ret["stdout"].splitlines():
3717         line = line.strip().lower()
3718         if line.startswith("enabled:"):
3719             enabled = line.split(":")
3720             if "true" in enabled[1]:
3721                 ret_dict["enabled"] = True
3722             else:
3723                 ret_dict["enabled"] = False
3724                 break
3725         if line.startswith("host vnic:"):
3726             host_vnic = line.split(":")
3727             ret_dict["host_vnic"] = host_vnic[1].strip()
3728         if line.startswith("network server ip:"):
3729             ip = line.split(":")
3730             ret_dict["ip"] = ip[1].strip()
3731         if line.startswith("network server port:"):
3732             ip_port = line.split(":")
3733             ret_dict["port"] = ip_port[1].strip()
3734     return ret_dict
3735 def _format_firewall_stdout(cmd_ret):
3736     ret_dict = {"success": True, "rulesets": {}}
3737     for line in cmd_ret["stdout"].splitlines():
3738         if line.startswith("Name"):
3739             continue
3740         if line.startswith("---"):
3741             continue
3742         ruleset_status = line.split()
3743         ret_dict["rulesets"][ruleset_status[0]] = bool(ruleset_status[1])
3744     return ret_dict
3745 def _format_syslog_config(cmd_ret):
3746     ret_dict = {"success": cmd_ret["retcode"] == 0}
3747     if cmd_ret["retcode"] != 0:
3748         ret_dict["message"] = cmd_ret["stdout"]
3749     else:
3750         for line in cmd_ret["stdout"].splitlines():
3751             line = line.strip()
3752             cfgvars = line.split(": ")
3753             key = cfgvars[0].strip()
3754             value = cfgvars[1].strip()
3755             ret_dict[key] = value
3756     return ret_dict
3757 def _get_date_time_mgr(host_reference):
3758     return host_reference.configManager.dateTimeSystem
3759 def _get_host_ref(service_instance, host, host_name=None):
3760     search_index = salt.utils.vmware.get_inventory(service_instance).searchIndex
3761     if host_name:
3762         host_ref = search_index.FindByDnsName(dnsName=host_name, vmSearch=False)
3763     else:
3764         host_ref = search_index.FindByDnsName(dnsName=host, vmSearch=False)
3765     if host_ref is None:
3766         host_ref = search_index.FindByIp(ip=host, vmSearch=False)
3767     return host_ref
3768 def _get_host_ssds(host_reference):
3769     return _get_host_disks(host_reference).get("SSDs")
3770 def _get_host_non_ssds(host_reference):
3771     return _get_host_disks(host_reference).get("Non-SSDs")
3772 def _get_host_disks(host_reference):
3773     storage_system = host_reference.configManager.storageSystem
3774     disks = storage_system.storageDeviceInfo.scsiLun
3775     ssds = []
3776     non_ssds = []
3777     for disk in disks:
3778         try:
3779             has_ssd_attr = disk.ssd
3780         except AttributeError:
3781             has_ssd_attr = False
3782         if has_ssd_attr:
3783             ssds.append(disk)
3784         else:
3785             non_ssds.append(disk)
3786     return {"SSDs": ssds, "Non-SSDs": non_ssds}
3787 def _get_service_manager(host_reference):
3788     return host_reference.configManager.serviceSystem
3789 def _get_vsan_eligible_disks(service_instance, host, host_names):
3790     ret = {}
3791     for host_name in host_names:
3792         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
3793         vsan_system = host_ref.configManager.vsanSystem
3794         if vsan_system is None:
3795             msg = (
3796                 "VSAN System Config Manager is unset for host '{}'. "
3797                 "VSAN configuration cannot be changed without a configured "
3798                 "VSAN System.".format(host_name)
3799             )
3800             log.debug(msg)
3801             ret.update({host_name: {"Error": msg}})
3802             continue
3803         suitable_disks = []
3804         query = vsan_system.QueryDisksForVsan()
3805         for item in query:
3806             if item.state == "eligible":
3807                 suitable_disks.append(item)
3808         if not suitable_disks:
3809             msg = "The host '{}' does not have any VSAN eligible disks.".format(
3810                 host_name
3811             )
3812             log.warning(msg)
3813             ret.update({host_name: {"Eligible": msg}})
3814             continue
3815         disks = _get_host_ssds(host_ref) + _get_host_non_ssds(host_ref)
3816         matching = []
3817         for disk in disks:
3818             for suitable_disk in suitable_disks:
3819                 if disk.canonicalName == suitable_disk.disk.canonicalName:
3820                     matching.append(disk)
3821         ret.update({host_name: {"Eligible": matching}})
3822     return ret
3823 def _reset_syslog_config_params(
3824     host,
3825     username,
3826     password,
3827     cmd,
3828     resets,
3829     valid_resets,
3830     protocol=None,
3831     port=None,
3832     esxi_host=None,
3833     credstore=None,
3834 ):
3835     ret_dict = {}
3836     all_success = True
3837     if not isinstance(resets, list):
3838         resets = [resets]
3839     for reset_param in resets:
3840         if reset_param in valid_resets:
3841             ret = salt.utils.vmware.esxcli(
3842                 host,
3843                 username,
3844                 password,
3845                 cmd + reset_param,
3846                 protocol=protocol,
3847                 port=port,
3848                 esxi_host=esxi_host,
3849                 credstore=credstore,
3850             )
3851             ret_dict[reset_param] = {}
3852             ret_dict[reset_param]["success"] = ret["retcode"] == 0
3853             if ret["retcode"] != 0:
3854                 all_success = False
3855                 ret_dict[reset_param]["message"] = ret["stdout"]
3856         else:
3857             all_success = False
3858             ret_dict[reset_param] = {}
3859             ret_dict[reset_param]["success"] = False
3860             ret_dict[reset_param]["message"] = "Invalid syslog configuration parameter"
3861     ret_dict["success"] = all_success
3862     return ret_dict
3863 def _set_syslog_config_helper(
3864     host,
3865     username,
3866     password,
3867     syslog_config,
3868     config_value,
3869     protocol=None,
3870     port=None,
3871     reset_service=None,
3872     esxi_host=None,
3873     credstore=None,
3874 ):
3875     cmd = "system syslog config set --{} {}".format(syslog_config, config_value)
3876     ret_dict = {}
3877     valid_resets = [
3878         "logdir",
3879         "loghost",
3880         "default-rotate",
3881         "default-size",
3882         "default-timeout",
3883         "logdir-unique",
3884     ]
3885     if syslog_config not in valid_resets:
3886         ret_dict.update(
3887             {
3888                 "success": False,
3889                 "message": "'{}' is not a valid config variable.".format(syslog_config),
3890             }
3891         )
3892         return ret_dict
3893     response = salt.utils.vmware.esxcli(
3894         host,
3895         username,
3896         password,
3897         cmd,
3898         protocol=protocol,
3899         port=port,
3900         esxi_host=esxi_host,
3901         credstore=credstore,
3902     )
3903     if response["retcode"] != 0:
3904         ret_dict.update(
3905             {syslog_config: {"success": False, "message": response["stdout"]}}
3906         )
3907     else:
3908         ret_dict.update({syslog_config: {"success": True}})
3909     if reset_service:
3910         if esxi_host:
3911             host_name = esxi_host
3912             esxi_host = [esxi_host]
3913         else:
3914             host_name = host
3915         response = syslog_service_reload(
3916             host,
3917             username,
3918             password,
3919             protocol=protocol,
3920             port=port,
3921             esxi_hosts=esxi_host,
3922             credstore=credstore,
3923         ).get(host_name)
3924         ret_dict.update({"syslog_restart": {"success": response["retcode"] == 0}})
3925     return ret_dict
3926 @depends(HAS_PYVMOMI)
3927 @ignores_kwargs("credstore")
3928 def add_host_to_dvs(
3929     host,
3930     username,
3931     password,
3932     vmknic_name,
3933     vmnic_name,
3934     dvs_name,
3935     target_portgroup_name,
3936     uplink_portgroup_name,
3937     protocol=None,
3938     port=None,
3939     host_names=None,
3940     verify_ssl=True,
3941 ):
3942     ret = {}
3943     ret["success"] = True
3944     ret["message"] = []
3945     service_instance = salt.utils.vmware.get_service_instance(
3946         host=host,
3947         username=username,
3948         password=password,
3949         protocol=protocol,
3950         port=port,
3951         verify_ssl=verify_ssl,
3952     )
3953     dvs = salt.utils.vmware._get_dvs(service_instance, dvs_name)
3954     if not dvs:
3955         ret["message"].append(
3956             "No Distributed Virtual Switch found with name {}".format(dvs_name)
3957         )
3958         ret["success"] = False
3959     target_portgroup = salt.utils.vmware._get_dvs_portgroup(dvs, target_portgroup_name)
3960     if not target_portgroup:
3961         ret["message"].append(
3962             "No target portgroup found with name {}".format(target_portgroup_name)
3963         )
3964         ret["success"] = False
3965     uplink_portgroup = salt.utils.vmware._get_dvs_uplink_portgroup(
3966         dvs, uplink_portgroup_name
3967     )
3968     if not uplink_portgroup:
3969         ret["message"].append(
3970             "No uplink portgroup found with name {}".format(uplink_portgroup_name)
3971         )
3972         ret["success"] = False
3973     if len(ret["message"]) &gt; 0:
3974         return ret
3975     dvs_uuid = dvs.config.uuid
3976     try:
3977         host_names = _check_hosts(service_instance, host, host_names)
3978     except CommandExecutionError as e:
3979         ret["message"] = "Error retrieving hosts: {}".format(e.msg)
3980         return ret
3981     for host_name in host_names:
3982         ret[host_name] = {}
3983         ret[host_name].update(
3984             {
3985                 "status": False,
3986                 "uplink": uplink_portgroup_name,
3987                 "portgroup": target_portgroup_name,
3988                 "vmknic": vmknic_name,
3989                 "vmnic": vmnic_name,
3990                 "dvs": dvs_name,
3991             }
3992         )
3993         host_ref = _get_host_ref(service_instance, host, host_name)
3994         if not host_ref:
3995             ret[host_name].update({"message": "Host {1} not found".format(host_name)})
3996             ret["success"] = False
3997             continue
3998         dvs_hostmember_config = vim.dvs.HostMember.ConfigInfo(host=host_ref)
3999         dvs_hostmember = vim.dvs.HostMember(config=dvs_hostmember_config)
4000         p_nics = salt.utils.vmware._get_pnics(host_ref)
4001         p_nic = [x for x in p_nics if x.device == vmnic_name]
4002         if len(p_nic) == 0:
4003             ret[host_name].update(
4004                 {"message": "Physical nic {} not found".format(vmknic_name)}
4005             )
4006             ret["success"] = False
4007             continue
4008         v_nics = salt.utils.vmware._get_vnics(host_ref)
4009         v_nic = [x for x in v_nics if x.device == vmknic_name]
4010         if len(v_nic) == 0:
4011             ret[host_name].update(
4012                 {"message": "Virtual nic {} not found".format(vmnic_name)}
4013             )
4014             ret["success"] = False
4015             continue
4016         v_nic_mgr = salt.utils.vmware._get_vnic_manager(host_ref)
4017         if not v_nic_mgr:
4018             ret[host_name].update(
4019                 {"message": "Unable to get the host's virtual nic manager."}
4020             )
4021             ret["success"] = False
4022             continue
4023         dvs_pnic_spec = vim.dvs.HostMember.PnicSpec(
4024             pnicDevice=vmnic_name, uplinkPortgroupKey=uplink_portgroup.key
4025         )
4026         pnic_backing = vim.dvs.HostMember.PnicBacking(pnicSpec=[dvs_pnic_spec])
4027         dvs_hostmember_config_spec = vim.dvs.HostMember.ConfigSpec(
4028             host=host_ref,
4029             operation="add",
4030         )
4031         dvs_config = vim.DVSConfigSpec(
4032             configVersion=dvs.config.configVersion, host=[dvs_hostmember_config_spec]
4033         )
4034         task = dvs.ReconfigureDvs_Task(spec=dvs_config)
4035         try:
4036             salt.utils.vmware.wait_for_task(
4037                 task, host_name, "Adding host to the DVS", sleep_seconds=3
4038             )
4039         except Exception as e:  # pylint: disable=broad-except
4040             if hasattr(e, "message") and hasattr(e.message, "msg"):
4041                 if not (
4042                     host_name in e.message.msg and "already exists" in e.message.msg
4043                 ):
4044                     ret["success"] = False
4045                     ret[host_name].update({"message": e.message.msg})
4046                     continue
4047             else:
4048                 raise
4049         network_system = host_ref.configManager.networkSystem
4050         source_portgroup = None
4051         for pg in host_ref.config.network.portgroup:
4052             if pg.spec.name == v_nic[0].portgroup:
4053                 source_portgroup = pg
4054                 break
4055         if not source_portgroup:
4056             ret[host_name].update({"message": "No matching portgroup on the vSwitch"})
4057             ret["success"] = False
4058             continue
4059         virtual_nic_config = vim.HostVirtualNicConfig(
4060             changeOperation="edit",
4061             device=v_nic[0].device,
4062             portgroup=source_portgroup.spec.name,
4063             spec=vim.HostVirtualNicSpec(
4064                 distributedVirtualPort=vim.DistributedVirtualSwitchPortConnection(
4065                     portgroupKey=target_portgroup.key,
4066                     switchUuid=target_portgroup.config.distributedVirtualSwitch.uuid,
4067                 )
4068             ),
4069         )
4070         current_vswitch_ports = host_ref.config.network.vswitch[0].numPorts
4071         vswitch_config = vim.HostVirtualSwitchConfig(
4072             changeOperation="edit",
4073             name="vSwitch0",
4074             spec=vim.HostVirtualSwitchSpec(numPorts=current_vswitch_ports),
4075         )
4076         proxyswitch_config = vim.HostProxySwitchConfig(
4077             changeOperation="edit",
4078             uuid=dvs_uuid,
4079             spec=vim.HostProxySwitchSpec(backing=pnic_backing),
4080         )
4081         host_network_config = vim.HostNetworkConfig(
4082             vswitch=[vswitch_config],
4083             proxySwitch=[proxyswitch_config],
4084             portgroup=[
4085                 vim.HostPortGroupConfig(
4086                     changeOperation="remove", spec=source_portgroup.spec
4087                 )
4088             ],
4089             vnic=[virtual_nic_config],
4090         )
4091         try:
4092             network_system.UpdateNetworkConfig(
4093                 changeMode="modify", config=host_network_config
4094             )
4095             ret[host_name].update({"status": True})
4096         except Exception as e:  # pylint: disable=broad-except
4097             if hasattr(e, "msg"):
4098                 ret[host_name].update(
4099                     {"message": "Failed to migrate adapters ({})".format(e.msg)}
4100                 )
4101                 continue
4102             else:
4103                 raise
4104     return ret
4105 @depends(HAS_PYVMOMI)
4106 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter")
4107 def _get_proxy_target(service_instance):
4108     proxy_type = get_proxy_type()
4109     if not salt.utils.vmware.is_connection_to_a_vcenter(service_instance):
4110         raise CommandExecutionError(
4111             "'_get_proxy_target' not supported when connected via the ESXi host"
4112         )
4113     reference = None
4114     if proxy_type == "esxcluster":
4115         (
4116             host,
4117             username,
4118             password,
4119             protocol,
4120             port,
4121             mechanism,
4122             principal,
4123             domain,
4124             datacenter,
4125             cluster,
4126         ) = _get_esxcluster_proxy_details()
4127         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4128         reference = salt.utils.vmware.get_cluster(dc_ref, cluster)
4129     elif proxy_type == "esxdatacenter":
4130         (
4131             host,
4132             username,
4133             password,
4134             protocol,
4135             port,
4136             mechanism,
4137             principal,
4138             domain,
4139             datacenter,
4140         ) = _get_esxdatacenter_proxy_details()
4141         reference = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4142     elif proxy_type == "vcenter":
4143         reference = salt.utils.vmware.get_root_folder(service_instance)
4144     elif proxy_type == "esxi":
4145         details = __proxy__["esxi.get_details"]()
4146         if "vcenter" not in details:
4147             raise InvalidEntityError(
4148                 "Proxies connected directly to ESXi hosts are not supported"
4149             )
4150         references = salt.utils.vmware.get_hosts(
4151             service_instance, host_names=details["esxi_host"]
4152         )
4153         if not references:
4154             raise VMwareObjectRetrievalError(
4155                 "ESXi host '{}' was not found".format(details["esxi_host"])
4156             )
4157         reference = references[0]
4158     log.trace("reference = %s", reference)
4159     return reference
4160 def _get_esxdatacenter_proxy_details():
4161     det = __salt__["esxdatacenter.get_details"]()
4162     return (
4163         det.get("vcenter"),
4164         det.get("username"),
4165         det.get("password"),
4166         det.get("protocol"),
4167         det.get("port"),
4168         det.get("mechanism"),
4169         det.get("principal"),
4170         det.get("domain"),
4171         det.get("datacenter"),
4172     )
4173 def _get_esxcluster_proxy_details():
4174     det = __salt__["esxcluster.get_details"]()
4175     return (
4176         det.get("vcenter"),
4177         det.get("username"),
4178         det.get("password"),
4179         det.get("protocol"),
4180         det.get("port"),
4181         det.get("mechanism"),
4182         det.get("principal"),
4183         det.get("domain"),
4184         det.get("datacenter"),
4185         det.get("cluster"),
4186     )
4187 def _get_esxi_proxy_details():
4188     det = __proxy__["esxi.get_details"]()
4189     host = det.get("host")
4190     if det.get("vcenter"):
4191         host = det["vcenter"]
4192     esxi_hosts = None
4193     if det.get("esxi_host"):
4194         esxi_hosts = [det["esxi_host"]]
4195     return (
4196         host,
4197         det.get("username"),
4198         det.get("password"),
4199         det.get("protocol"),
4200         det.get("port"),
4201         det.get("mechanism"),
4202         det.get("principal"),
4203         det.get("domain"),
4204         esxi_hosts,
4205     )
4206 @depends(HAS_PYVMOMI)
4207 @_gets_service_instance_via_proxy
4208 def get_vm(
4209     name,
4210     datacenter=None,
4211     vm_properties=None,
4212     traversal_spec=None,
4213     parent_ref=None,
4214     service_instance=None,
4215 ):
4216     virtual_machine = salt.utils.vmware.get_vm_by_property(
4217         service_instance,
4218         name,
4219         datacenter=datacenter,
4220         vm_properties=vm_properties,
4221         traversal_spec=traversal_spec,
4222         parent_ref=parent_ref,
4223     )
4224     return virtual_machine
4225 @depends(HAS_PYVMOMI)
4226 @_gets_service_instance_via_proxy
4227 def get_vm_config_file(name, datacenter, placement, datastore, service_instance=None):
4228     browser_spec = vim.host.DatastoreBrowser.SearchSpec()
4229     directory = name
4230     browser_spec.query = [vim.host.DatastoreBrowser.VmConfigQuery()]
4231     datacenter_object = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4232     if "cluster" in placement:
4233         container_object = salt.utils.vmware.get_cluster(
4234             datacenter_object, placement["cluster"]
4235         )
4236     else:
4237         container_objects = salt.utils.vmware.get_hosts(
4238             service_instance, datacenter_name=datacenter, host_names=[placement["host"]]
4239         )
4240         if not container_objects:
4241             raise salt.exceptions.VMwareObjectRetrievalError(
4242                 "ESXi host named '{}' wasn't found.".format(placement["host"])
4243             )
4244         container_object = container_objects[0]
4245     files = salt.utils.vmware.get_datastore_files(
4246         service_instance, directory, [datastore], container_object, browser_spec
4247     )
4248     if files and len(files[0].file) &gt; 1:
4249         raise salt.exceptions.VMwareMultipleObjectsError(
4250             "Multiple configuration files found in the same virtual machine folder"
4251         )
4252     elif files and files[0].file:
4253         return files[0]
4254     else:
4255         return None
4256 def _apply_hardware_version(hardware_version, config_spec, operation="add"):
4257     log.trace(
4258         "Configuring virtual machine hardware version version=%s", hardware_version
4259     )
4260     if operation == "edit":
4261         log.trace("Scheduling hardware version upgrade to %s", hardware_version)
4262         scheduled_hardware_upgrade = vim.vm.ScheduledHardwareUpgradeInfo()
4263         scheduled_hardware_upgrade.upgradePolicy = "always"
4264         scheduled_hardware_upgrade.versionKey = hardware_version
4265         config_spec.scheduledHardwareUpgradeInfo = scheduled_hardware_upgrade
4266     elif operation == "add":
4267         config_spec.version = str(hardware_version)
4268 def _apply_cpu_config(config_spec, cpu_props):
4269     log.trace("Configuring virtual machine CPU settings cpu_props=%s", cpu_props)
4270     if "count" in cpu_props:
4271         config_spec.numCPUs = int(cpu_props["count"])
4272     if "cores_per_socket" in cpu_props:
4273         config_spec.numCoresPerSocket = int(cpu_props["cores_per_socket"])
4274     if "nested" in cpu_props and cpu_props["nested"]:
4275         config_spec.nestedHVEnabled = cpu_props["nested"]  # True
4276     if "hotadd" in cpu_props and cpu_props["hotadd"]:
4277         config_spec.cpuHotAddEnabled = cpu_props["hotadd"]  # True
4278     if "hotremove" in cpu_props and cpu_props["hotremove"]:
4279         config_spec.cpuHotRemoveEnabled = cpu_props["hotremove"]  # True
4280 def _apply_memory_config(config_spec, memory):
4281     log.trace("Configuring virtual machine memory settings memory=%s", memory)
4282     if "size" in memory and "unit" in memory:
4283         try:
4284             if memory["unit"].lower() == "kb":
4285                 memory_mb = memory["size"] / 1024
4286             elif memory["unit"].lower() == "mb":
4287                 memory_mb = memory["size"]
4288             elif memory["unit"].lower() == "gb":
4289                 memory_mb = int(float(memory["size"]) * 1024)
4290         except (TypeError, ValueError):
4291             memory_mb = int(memory["size"])
4292         config_spec.memoryMB = memory_mb
4293     if "reservation_max" in memory:
4294         config_spec.memoryReservationLockedToMax = memory["reservation_max"]
4295     if "hotadd" in memory:
4296         config_spec.memoryHotAddEnabled = memory["hotadd"]
4297 @depends(HAS_PYVMOMI)
4298 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
4299 @_gets_service_instance_via_proxy
4300 def get_advanced_configs(vm_name, datacenter, service_instance=None):
4301     current_config = get_vm_config(
4302         vm_name, datacenter=datacenter, objects=True, service_instance=service_instance
4303     )
4304     return current_config["advanced_configs"]
4305 def _apply_advanced_config(config_spec, advanced_config, vm_extra_config=None):
4306     log.trace("Configuring advanced configuration parameters %s", advanced_config)
4307     if isinstance(advanced_config, str):
4308         raise salt.exceptions.ArgumentValueError(
4309             "The specified 'advanced_configs' configuration "
4310             "option cannot be parsed, please check the parameters"
4311         )
4312     for key, value in advanced_config.items():
4313         if vm_extra_config:
4314             for option in vm_extra_config:
4315                 if option.key == key and option.value == str(value):
4316                     continue
4317         else:
4318             option = vim.option.OptionValue(key=key, value=value)
4319             config_spec.extraConfig.append(option)
4320 @depends(HAS_PYVMOMI)
4321 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
4322 @_gets_service_instance_via_proxy
4323 def set_advanced_configs(vm_name, datacenter, advanced_configs, service_instance=None):
4324     current_config = get_vm_config(
4325         vm_name, datacenter=datacenter, objects=True, service_instance=service_instance
4326     )
4327     diffs = compare_vm_configs(
4328         {"name": vm_name, "advanced_configs": advanced_configs}, current_config
4329     )
4330     datacenter_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4331     vm_ref = salt.utils.vmware.get_mor_by_property(
4332         service_instance,
4333         vim.VirtualMachine,
4334         vm_name,
4335         property_name="name",
4336         container_ref=datacenter_ref,
4337     )
4338     config_spec = vim.vm.ConfigSpec()
4339     changes = diffs["advanced_configs"].diffs
4340     _apply_advanced_config(
4341         config_spec, diffs["advanced_configs"].new_values, vm_ref.config.extraConfig
4342     )
4343     if changes:
4344         salt.utils.vmware.update_vm(vm_ref, config_spec)
4345     return {"advanced_config_changes": changes}
4346 def _delete_advanced_config(config_spec, advanced_config, vm_extra_config):
4347     log.trace("Removing advanced configuration parameters %s", advanced_config)
4348     if isinstance(advanced_config, str):
4349         raise salt.exceptions.ArgumentValueError(
4350             "The specified 'advanced_configs' configuration "
4351             "option cannot be parsed, please check the parameters"
4352         )
4353     removed_configs = []
4354     for key in advanced_config:
4355         for option in vm_extra_config:
4356             if option.key == key:
4357                 option = vim.option.OptionValue(key=key, value="")
4358                 config_spec.extraConfig.append(option)
4359                 removed_configs.append(key)
4360     return removed_configs
4361 @depends(HAS_PYVMOMI)
4362 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
4363 @_gets_service_instance_via_proxy
4364 def delete_advanced_configs(
4365     vm_name, datacenter, advanced_configs, service_instance=None
4366 ):
4367     datacenter_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4368     vm_ref = salt.utils.vmware.get_mor_by_property(
4369         service_instance,
4370         vim.VirtualMachine,
4371         vm_name,
4372         property_name="name",
4373         container_ref=datacenter_ref,
4374     )
4375     config_spec = vim.vm.ConfigSpec()
4376     removed_configs = _delete_advanced_config(
4377         config_spec, advanced_configs, vm_ref.config.extraConfig
4378     )
4379     if removed_configs:
4380         salt.utils.vmware.update_vm(vm_ref, config_spec)
4381     return {"removed_configs": removed_configs}
4382 def _get_scsi_controller_key(bus_number, scsi_ctrls):
4383     keys = [
4384         ctrl.key for ctrl in scsi_ctrls if scsi_ctrls and ctrl.busNumber == bus_number
4385     ]
4386     if not keys:
4387         raise salt.exceptions.VMwareVmCreationError(
4388             "SCSI controller number {} doesn't exist".format(bus_number)
4389         )
4390     return keys[0]
4391 def _apply_hard_disk(
4392     unit_number,
4393     key,
4394     operation,
4395     disk_label=None,
4396     size=None,
4397     unit="GB",
4398     controller_key=None,
4399     thin_provision=None,
4400     eagerly_scrub=None,
4401     datastore=None,
4402     filename=None,
4403 ):
4404     log.trace(
4405         "Configuring hard disk %s size=%s, unit=%s, controller_key=%s, "
4406         "thin_provision=%s, eagerly_scrub=%s, datastore=%s, filename=%s",
4407         disk_label,
4408         size,
4409         unit,
4410         controller_key,
4411         thin_provision,
4412         eagerly_scrub,
4413         datastore,
4414         filename,
4415     )
4416     disk_spec = vim.vm.device.VirtualDeviceSpec()
4417     disk_spec.device = vim.vm.device.VirtualDisk()
4418     disk_spec.device.key = key
4419     disk_spec.device.unitNumber = unit_number
4420     disk_spec.device.deviceInfo = vim.Description()
4421     if size:
4422         convert_size = salt.utils.vmware.convert_to_kb(unit, size)
4423         disk_spec.device.capacityInKB = convert_size["size"]
4424     if disk_label:
4425         disk_spec.device.deviceInfo.label = disk_label
4426     if thin_provision is not None or eagerly_scrub is not None:
4427         disk_spec.device.backing = vim.vm.device.VirtualDisk.FlatVer2BackingInfo()
4428         disk_spec.device.backing.diskMode = "persistent"
4429     if thin_provision is not None:
4430         disk_spec.device.backing.thinProvisioned = thin_provision
4431     if eagerly_scrub is not None and eagerly_scrub != "None":
4432         disk_spec.device.backing.eagerlyScrub = eagerly_scrub
4433     if controller_key:
4434         disk_spec.device.controllerKey = controller_key
4435     if operation == "add":
4436         disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4437         disk_spec.device.backing.fileName = "[{}] {}".format(
4438             salt.utils.vmware.get_managed_object_name(datastore), filename
4439         )
4440         disk_spec.fileOperation = vim.vm.device.VirtualDeviceSpec.FileOperation.create
4441     elif operation == "edit":
4442         disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4443     return disk_spec
4444 def _create_adapter_type(network_adapter, adapter_type, network_adapter_label=""):
4445     log.trace(
4446         "Configuring virtual machine network adapter adapter_type=%s", adapter_type
4447     )
4448     if adapter_type in ["vmxnet", "vmxnet2", "vmxnet3", "e1000", "e1000e"]:
4449         edited_network_adapter = salt.utils.vmware.get_network_adapter_type(
4450             adapter_type
4451         )
4452         if isinstance(network_adapter, type(edited_network_adapter)):
4453             edited_network_adapter = network_adapter
4454         else:
4455             if network_adapter:
4456                 log.trace(
4457                     "Changing type of '%s' from '%s' to '%s'",
4458                     network_adapter.deviceInfo.label,
4459                     type(network_adapter).__name__.rsplit(".", 1)[1][7:].lower(),
4460                     adapter_type,
4461                 )
4462     else:
4463         if network_adapter:
4464             if adapter_type:
4465                 log.error(
4466                     "Cannot change type of '%s' to '%s'. Not changing type",
4467                     network_adapter.deviceInfo.label,
4468                     adapter_type,
4469                 )
4470             edited_network_adapter = network_adapter
4471         else:
4472             if not adapter_type:
4473                 log.trace(
4474                     "The type of '%s' has not been specified. "
4475                     "Creating of default type 'vmxnet3'",
4476                     network_adapter_label,
4477                 )
4478             edited_network_adapter = vim.vm.device.VirtualVmxnet3()
4479     return edited_network_adapter
4480 def _create_network_backing(network_name, switch_type, parent_ref):
4481     log.trace(
4482         "Configuring virtual machine network backing network_name=%s "
4483         "switch_type=%s parent=%s",
4484         network_name,
4485         switch_type,
4486         salt.utils.vmware.get_managed_object_name(parent_ref),
4487     )
4488     backing = {}
4489     if network_name:
4490         if switch_type == "standard":
4491             networks = salt.utils.vmware.get_networks(
4492                 parent_ref, network_names=[network_name]
4493             )
4494             if not networks:
4495                 raise salt.exceptions.VMwareObjectRetrievalError(
4496                     "The network '{}' could not be retrieved.".format(network_name)
4497                 )
4498             network_ref = networks[0]
4499             backing = vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()
4500             backing.deviceName = network_name
4501             backing.network = network_ref
4502         elif switch_type == "distributed":
4503             networks = salt.utils.vmware.get_dvportgroups(
4504                 parent_ref, portgroup_names=[network_name]
4505             )
4506             if not networks:
4507                 raise salt.exceptions.VMwareObjectRetrievalError(
4508                     "The port group '{}' could not be retrieved.".format(network_name)
4509                 )
4510             network_ref = networks[0]
4511             dvs_port_connection = vim.dvs.PortConnection(
4512                 portgroupKey=network_ref.key,
4513                 switchUuid=network_ref.config.distributedVirtualSwitch.uuid,
4514             )
4515             backing = (
4516                 vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()
4517             )
4518             backing.port = dvs_port_connection
4519     return backing
4520 def _apply_network_adapter_config(
4521     key,
4522     network_name,
4523     adapter_type,
4524     switch_type,
4525     network_adapter_label=None,
4526     operation="add",
4527     connectable=None,
4528     mac=None,
4529     parent=None,
4530 ):
4531     adapter_type.strip().lower()
4532     switch_type.strip().lower()
4533     log.trace(
4534         "Configuring virtual machine network adapter network_adapter_label=%s "
4535         "network_name=%s adapter_type=%s switch_type=%s mac=%s",
4536         network_adapter_label,
4537         network_name,
4538         adapter_type,
4539         switch_type,
4540         mac,
4541     )
4542     network_spec = vim.vm.device.VirtualDeviceSpec()
4543     network_spec.device = _create_adapter_type(
4544         network_spec.device, adapter_type, network_adapter_label=network_adapter_label
4545     )
4546     network_spec.device.deviceInfo = vim.Description()
4547     if operation == "add":
4548         network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4549     elif operation == "edit":
4550         network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4551     if switch_type and network_name:
4552         network_spec.device.backing = _create_network_backing(
4553             network_name, switch_type, parent
4554         )
4555         network_spec.device.deviceInfo.summary = network_name
4556     if key:
4557         network_spec.device.key = key
4558     if network_adapter_label:
4559         network_spec.device.deviceInfo.label = network_adapter_label
4560     if mac:
4561         network_spec.device.macAddress = mac
4562         network_spec.device.addressType = "Manual"
4563     network_spec.device.wakeOnLanEnabled = True
4564     if connectable:
4565         network_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
4566         network_spec.device.connectable.startConnected = connectable["start_connected"]
4567         network_spec.device.connectable.allowGuestControl = connectable[
4568             "allow_guest_control"
4569         ]
4570     return network_spec
4571 def _apply_scsi_controller(
4572     adapter, adapter_type, bus_sharing, key, bus_number, operation
4573 ):
4574     log.trace(
4575         "Configuring scsi controller adapter=%s adapter_type=%s "
4576         "bus_sharing=%s key=%s bus_number=%s",
4577         adapter,
4578         adapter_type,
4579         bus_sharing,
4580         key,
4581         bus_number,
4582     )
4583     scsi_spec = vim.vm.device.VirtualDeviceSpec()
4584     if adapter_type == "lsilogic":
4585         summary = "LSI Logic"
4586         scsi_spec.device = vim.vm.device.VirtualLsiLogicController()
4587     elif adapter_type == "lsilogic_sas":
4588         summary = "LSI Logic Sas"
4589         scsi_spec.device = vim.vm.device.VirtualLsiLogicSASController()
4590     elif adapter_type == "paravirtual":
4591         summary = "VMware paravirtual SCSI"
4592         scsi_spec.device = vim.vm.device.ParaVirtualSCSIController()
4593     elif adapter_type == "buslogic":
4594         summary = "Bus Logic"
4595         scsi_spec.device = vim.vm.device.VirtualBusLogicController()
4596     if operation == "add":
4597         scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4598     elif operation == "edit":
4599         scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4600     scsi_spec.device.key = key
4601     scsi_spec.device.busNumber = bus_number
4602     scsi_spec.device.deviceInfo = vim.Description()
4603     scsi_spec.device.deviceInfo.label = adapter
4604     scsi_spec.device.deviceInfo.summary = summary
4605     if bus_sharing == "virtual_sharing":
4606         scsi_spec.device.sharedBus = (
4607             vim.vm.device.VirtualSCSIController.Sharing.virtualSharing
4608         )
4609     elif bus_sharing == "physical_sharing":
4610         scsi_spec.device.sharedBus = (
4611             vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
4612         )
4613     elif bus_sharing == "no_sharing":
4614         scsi_spec.device.sharedBus = (
4615             vim.vm.device.VirtualSCSIController.Sharing.noSharing
4616         )
4617     return scsi_spec
4618 def _create_ide_controllers(ide_controllers):
4619     ide_ctrls = []
4620     keys = range(-200, -250, -1)
4621     if ide_controllers:
4622         devs = [ide["adapter"] for ide in ide_controllers]
4623         log.trace("Creating IDE controllers %s", devs)
4624         for ide, key in zip(ide_controllers, keys):
4625             ide_ctrls.append(
4626                 _apply_ide_controller_config(ide["adapter"], "add", key, abs(key + 200))
4627             )
4628     return ide_ctrls
4629 def _apply_ide_controller_config(ide_controller_label, operation, key, bus_number=0):
4630     log.trace(
4631         "Configuring IDE controller ide_controller_label=%s", ide_controller_label
4632     )
4633     ide_spec = vim.vm.device.VirtualDeviceSpec()
4634     ide_spec.device = vim.vm.device.VirtualIDEController()
4635     if operation == "add":
4636         ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4637     if operation == "edit":
4638         ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4639     ide_spec.device.key = key
4640     ide_spec.device.busNumber = bus_number
4641     if ide_controller_label:
4642         ide_spec.device.deviceInfo = vim.Description()
4643         ide_spec.device.deviceInfo.label = ide_controller_label
4644         ide_spec.device.deviceInfo.summary = ide_controller_label
4645     return ide_spec
4646 def _create_sata_controllers(sata_controllers):
4647     sata_ctrls = []
4648     keys = range(-15000, -15050, -1)
4649     if sata_controllers:
4650         devs = [sata["adapter"] for sata in sata_controllers]
4651         log.trace("Creating SATA controllers %s", devs)
4652         for sata, key in zip(sata_controllers, keys):
4653             sata_ctrls.append(
4654                 _apply_sata_controller_config(
4655                     sata["adapter"], "add", key, sata["bus_number"]
4656                 )
4657             )
4658     return sata_ctrls
4659 def _apply_sata_controller_config(sata_controller_label, operation, key, bus_number=0):
4660     log.trace(
4661         "Configuring SATA controller sata_controller_label=%s", sata_controller_label
4662     )
4663     sata_spec = vim.vm.device.VirtualDeviceSpec()
4664     sata_spec.device = vim.vm.device.VirtualAHCIController()
4665     if operation == "add":
4666         sata_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4667     elif operation == "edit":
4668         sata_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4669     sata_spec.device.key = key
4670     sata_spec.device.controllerKey = 100
4671     sata_spec.device.busNumber = bus_number
4672     if sata_controller_label:
4673         sata_spec.device.deviceInfo = vim.Description()
4674         sata_spec.device.deviceInfo.label = sata_controller_label
4675         sata_spec.device.deviceInfo.summary = sata_controller_label
4676     return sata_spec
4677 def _apply_cd_drive(
4678     drive_label,
4679     key,
4680     device_type,
4681     operation,
4682     client_device=None,
4683     datastore_iso_file=None,
4684     connectable=None,
4685     controller_key=200,
4686     parent_ref=None,
4687 ):
4688     log.trace(
4689         "Configuring CD/DVD drive drive_label=%s device_type=%s "
4690         "client_device=%s datastore_iso_file=%s",
4691         drive_label,
4692         device_type,
4693         client_device,
4694         datastore_iso_file,
4695     )
4696     drive_spec = vim.vm.device.VirtualDeviceSpec()
4697     drive_spec.device = vim.vm.device.VirtualCdrom()
4698     drive_spec.device.deviceInfo = vim.Description()
4699     if operation == "add":
4700         drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4701     elif operation == "edit":
4702         drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4703     if device_type == "datastore_iso_file":
4704         drive_spec.device.backing = vim.vm.device.VirtualCdrom.IsoBackingInfo()
4705         drive_spec.device.backing.fileName = datastore_iso_file["path"]
4706         datastore = datastore_iso_file["path"].partition("[")[-1].rpartition("]")[0]
4707         datastore_object = salt.utils.vmware.get_datastores(
4708             salt.utils.vmware.get_service_instance_from_managed_object(parent_ref),
4709             parent_ref,
4710             datastore_names=[datastore],
4711         )[0]
4712         if datastore_object:
4713             drive_spec.device.backing.datastore = datastore_object
4714         drive_spec.device.deviceInfo.summary = "{}".format(datastore_iso_file["path"])
4715     elif device_type == "client_device":
4716         if client_device["mode"] == "passthrough":
4717             drive_spec.device.backing = (
4718                 vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo()
4719             )
4720         elif client_device["mode"] == "atapi":
4721             drive_spec.device.backing = (
4722                 vim.vm.device.VirtualCdrom.RemoteAtapiBackingInfo()
4723             )
4724     drive_spec.device.key = key
4725     drive_spec.device.deviceInfo.label = drive_label
4726     drive_spec.device.controllerKey = controller_key
4727     drive_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
4728     if connectable:
4729         drive_spec.device.connectable.startConnected = connectable["start_connected"]
4730         drive_spec.device.connectable.allowGuestControl = connectable[
4731             "allow_guest_control"
4732         ]
4733     return drive_spec
4734 def _set_network_adapter_mapping(domain, gateway, ip_addr, subnet_mask, mac):
4735     adapter_mapping = vim.vm.customization.AdapterMapping()
4736     adapter_mapping.macAddress = mac
4737     adapter_mapping.adapter = vim.vm.customization.IPSettings()
4738     if domain:
4739         adapter_mapping.adapter.dnsDomain = domain
4740     if gateway:
4741         adapter_mapping.adapter.gateway = gateway
4742     if ip_addr:
4743         adapter_mapping.adapter.ip = vim.vm.customization.FixedIp(ipAddress=ip_addr)
4744         adapter_mapping.adapter.subnetMask = subnet_mask
4745     else:
4746         adapter_mapping.adapter.ip = vim.vm.customization.DhcpIpGenerator()
4747     return adapter_mapping
4748 def _apply_serial_port(serial_device_spec, key, operation="add"):
4749     log.trace(
4750         "Creating serial port adapter=%s type=%s connectable=%s yield=%s",
4751         serial_device_spec["adapter"],
4752         serial_device_spec["type"],
4753         serial_device_spec["connectable"],
4754         serial_device_spec["yield"],
4755     )
4756     device_spec = vim.vm.device.VirtualDeviceSpec()
4757     device_spec.device = vim.vm.device.VirtualSerialPort()
4758     if operation == "add":
4759         device_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
4760     elif operation == "edit":
4761         device_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
4762     connect_info = vim.vm.device.VirtualDevice.ConnectInfo()
4763     type_backing = None
4764     if serial_device_spec["type"] == "network":
4765         type_backing = vim.vm.device.VirtualSerialPort.URIBackingInfo()
4766         if "uri" not in serial_device_spec["backing"].keys():
4767             raise ValueError("vSPC proxy URI not specified in config")
4768         if "uri" not in serial_device_spec["backing"].keys():
4769             raise ValueError("vSPC Direction not specified in config")
4770         if "filename" not in serial_device_spec["backing"].keys():
4771             raise ValueError("vSPC Filename not specified in config")
4772         type_backing.proxyURI = serial_device_spec["backing"]["uri"]
4773         type_backing.direction = serial_device_spec["backing"]["direction"]
4774         type_backing.serviceURI = serial_device_spec["backing"]["filename"]
4775     if serial_device_spec["type"] == "pipe":
4776         type_backing = vim.vm.device.VirtualSerialPort.PipeBackingInfo()
4777     if serial_device_spec["type"] == "file":
4778         type_backing = vim.vm.device.VirtualSerialPort.FileBackingInfo()
4779     if serial_device_spec["type"] == "device":
4780         type_backing = vim.vm.device.VirtualSerialPort.DeviceBackingInfo()
4781     connect_info.allowGuestControl = serial_device_spec["connectable"][
4782         "allow_guest_control"
4783     ]
4784     connect_info.startConnected = serial_device_spec["connectable"]["start_connected"]
4785     device_spec.device.backing = type_backing
4786     device_spec.device.connectable = connect_info
4787     device_spec.device.unitNumber = 1
4788     device_spec.device.key = key
4789     device_spec.device.yieldOnPoll = serial_device_spec["yield"]
4790     return device_spec
4791 def _create_disks(service_instance, disks, scsi_controllers=None, parent=None):
4792     disk_specs = []
4793     keys = range(-2000, -2050, -1)
4794     if disks:
4795         devs = [disk["adapter"] for disk in disks]
4796         log.trace("Creating disks %s", devs)
4797     for disk, key in zip(disks, keys):
4798         filename, datastore, datastore_ref = None, None, None
4799         size = float(disk["size"])
4800         controller_key = 1000  # Default is the first SCSI controller
4801         if "address" in disk:  # 0:0
4802             controller_bus_number, unit_number = disk["address"].split(":")
4803             controller_bus_number = int(controller_bus_number)
4804             unit_number = int(unit_number)
4805             controller_key = _get_scsi_controller_key(
4806                 controller_bus_number, scsi_ctrls=scsi_controllers
4807             )
4808         elif "controller" in disk:
4809             for contr in scsi_controllers:
4810                 if contr["label"] == disk["controller"]:
4811                     controller_key = contr["key"]
4812                     break
4813             else:
4814                 raise salt.exceptions.VMwareObjectNotFoundError(
4815                     "The given controller does not exist: {}".format(disk["controller"])
4816                 )
4817         if "datastore" in disk:
4818             datastore_ref = salt.utils.vmware.get_datastores(
4819                 service_instance, parent, datastore_names=[disk["datastore"]]
4820             )[0]
4821             datastore = disk["datastore"]
4822         if "filename" in disk:
4823             filename = disk["filename"]
4824         if (not filename and datastore) or (filename and not datastore):
4825             raise salt.exceptions.ArgumentValueError(
4826                 "You must specify both filename and datastore attributes"
4827                 " to place your disk to a specific datastore "
4828                 "{}, {}".format(datastore, filename)
4829             )
4830         disk_spec = _apply_hard_disk(
4831             unit_number,
4832             key,
4833             disk_label=disk["adapter"],
4834             size=size,
4835             unit=disk["unit"],
4836             controller_key=controller_key,
4837             operation="add",
4838             thin_provision=disk["thin_provision"],
4839             eagerly_scrub=disk["eagerly_scrub"] if "eagerly_scrub" in disk else None,
4840             datastore=datastore_ref,
4841             filename=filename,
4842         )
4843         disk_specs.append(disk_spec)
4844         unit_number += 1
4845     return disk_specs
4846 def _create_scsi_devices(scsi_devices):
4847     keys = range(-1000, -1050, -1)
4848     scsi_specs = []
4849     if scsi_devices:
4850         devs = [scsi["adapter"] for scsi in scsi_devices]
4851         log.trace("Creating SCSI devices %s", devs)
4852         for (key, scsi_controller) in zip(keys, scsi_devices):
4853             scsi_spec = _apply_scsi_controller(
4854                 scsi_controller["adapter"],
4855                 scsi_controller["type"],
4856                 scsi_controller["bus_sharing"],
4857                 key,
4858                 scsi_controller["bus_number"],
4859                 "add",
4860             )
4861             scsi_specs.append(scsi_spec)
4862     return scsi_specs
4863 def _create_network_adapters(network_interfaces, parent=None):
4864     network_specs = []
4865     nics_settings = []
4866     keys = range(-4000, -4050, -1)
4867     if network_interfaces:
4868         devs = [inter["adapter"] for inter in network_interfaces]
4869         log.trace("Creating network interfaces %s", devs)
4870         for interface, key in zip(network_interfaces, keys):
4871             network_spec = _apply_network_adapter_config(
4872                 key,
4873                 interface["name"],
4874                 interface["adapter_type"],
4875                 interface["switch_type"],
4876                 network_adapter_label=interface["adapter"],
4877                 operation="add",
4878                 connectable=interface["connectable"]
4879                 if "connectable" in interface
4880                 else None,
4881                 mac=interface["mac"],
4882                 parent=parent,
4883             )
4884             network_specs.append(network_spec)
4885             if "mapping" in interface:
4886                 adapter_mapping = _set_network_adapter_mapping(
4887                     interface["mapping"]["domain"],
4888                     interface["mapping"]["gateway"],
4889                     interface["mapping"]["ip_addr"],
4890                     interface["mapping"]["subnet_mask"],
4891                     interface["mac"],
4892                 )
4893                 nics_settings.append(adapter_mapping)
4894     return (network_specs, nics_settings)
4895 def _create_serial_ports(serial_ports):
4896     ports = []
4897     keys = range(-9000, -9050, -1)
4898     if serial_ports:
4899         devs = [serial["adapter"] for serial in serial_ports]
4900         log.trace("Creating serial ports %s", devs)
4901         for port, key in zip(serial_ports, keys):
4902             serial_port_device = _apply_serial_port(port, key, "add")
4903             ports.append(serial_port_device)
4904     return ports
4905 def _create_cd_drives(cd_drives, controllers=None, parent_ref=None):
4906     cd_drive_specs = []
4907     keys = range(-3000, -3050, -1)
4908     if cd_drives:
4909         devs = [dvd["adapter"] for dvd in cd_drives]
4910         log.trace("Creating cd/dvd drives %s", devs)
4911         for drive, key in zip(cd_drives, keys):
4912             controller_key = 200
4913             if controllers:
4914                 controller = _get_device_by_label(controllers, drive["controller"])
4915                 controller_key = controller.key
4916             cd_drive_specs.append(
4917                 _apply_cd_drive(
4918                     drive["adapter"],
4919                     key,
4920                     drive["device_type"],
4921                     "add",
4922                     client_device=drive["client_device"]
4923                     if "client_device" in drive
4924                     else None,
4925                     datastore_iso_file=drive["datastore_iso_file"]
4926                     if "datastore_iso_file" in drive
4927                     else None,
4928                     connectable=drive["connectable"]
4929                     if "connectable" in drive
4930                     else None,
4931                     controller_key=controller_key,
4932                     parent_ref=parent_ref,
4933                 )
4934             )
4935     return cd_drive_specs
4936 def _get_device_by_key(devices, key):
4937     device_keys = [d for d in devices if d.key == key]
4938     if device_keys:
4939         return device_keys[0]
4940     else:
4941         raise salt.exceptions.VMwareObjectNotFoundError(
4942             "Virtual machine device with unique key {} does not exist".format(key)
4943         )
4944 def _get_device_by_label(devices, label):
4945     device_labels = [d for d in devices if d.deviceInfo.label == label]
4946     if device_labels:
4947         return device_labels[0]
4948     else:
4949         raise salt.exceptions.VMwareObjectNotFoundError(
4950             "Virtual machine device with label {} does not exist".format(label)
4951         )
4952 def _convert_units(devices):
4953     if devices:
4954         for device in devices:
4955             if "unit" in device and "size" in device:
4956                 device.update(
4957                     salt.utils.vmware.convert_to_kb(device["unit"], device["size"])
4958                 )
4959     else:
4960         return False
4961     return True
4962 def compare_vm_configs(new_config, current_config):
4963     diffs = {}
4964     keys = set(new_config.keys())
4965     keys.discard("name")
4966     keys.discard("datacenter")
4967     keys.discard("datastore")
4968     for property_key in ("version", "image"):
4969         if property_key in keys:
4970             single_value_diff = recursive_diff(
4971                 {property_key: current_config[property_key]},
4972                 {property_key: new_config[property_key]},
4973             )
4974             if single_value_diff.diffs:
4975                 diffs[property_key] = single_value_diff
4976             keys.discard(property_key)
4977     if "cpu" in keys:
4978         keys.remove("cpu")
4979         cpu_diff = recursive_diff(current_config["cpu"], new_config["cpu"])
4980         if cpu_diff.diffs:
4981             diffs["cpu"] = cpu_diff
4982     if "memory" in keys:
4983         keys.remove("memory")
4984         _convert_units([current_config["memory"]])
4985         _convert_units([new_config["memory"]])
4986         memory_diff = recursive_diff(current_config["memory"], new_config["memory"])
4987         if memory_diff.diffs:
4988             diffs["memory"] = memory_diff
4989     if "advanced_configs" in keys:
4990         keys.remove("advanced_configs")
4991         key = "advanced_configs"
4992         advanced_diff = recursive_diff(current_config[key], new_config[key])
4993         if advanced_diff.diffs:
4994             diffs[key] = advanced_diff
4995     if "disks" in keys:
4996         keys.remove("disks")
4997         _convert_units(current_config["disks"])
4998         _convert_units(new_config["disks"])
4999         disk_diffs = list_diff(current_config["disks"], new_config["disks"], "address")
5000         disk_diffs.remove_diff(diff_key="eagerly_scrub")
5001         disk_diffs.remove_diff(diff_key="filename")
5002         disk_diffs.remove_diff(diff_key="adapter")
5003         if disk_diffs.diffs:
5004             diffs["disks"] = disk_diffs
5005     if "interfaces" in keys:
5006         keys.remove("interfaces")
5007         interface_diffs = list_diff(
5008             current_config["interfaces"], new_config["interfaces"], "mac"
5009         )
5010         interface_diffs.remove_diff(diff_key="adapter")
5011         if interface_diffs.diffs:
5012             diffs["interfaces"] = interface_diffs
5013     for key in keys:
5014         if key not in current_config or key not in new_config:
5015             raise ValueError(
5016                 "A general device {} configuration was "
5017                 "not supplied or it was not retrieved from "
5018                 "remote configuration".format(key)
5019             )
5020         device_diffs = list_diff(current_config[key], new_config[key], "adapter")
5021         if device_diffs.diffs:
5022             diffs[key] = device_diffs
5023     return diffs
5024 @_gets_service_instance_via_proxy
5025 def get_vm_config(name, datacenter=None, objects=True, service_instance=None):
5026     properties = [
5027         "config.hardware.device",
5028         "config.hardware.numCPU",
5029         "config.hardware.numCoresPerSocket",
5030         "config.nestedHVEnabled",
5031         "config.cpuHotAddEnabled",
5032         "config.cpuHotRemoveEnabled",
5033         "config.hardware.memoryMB",
5034         "config.memoryReservationLockedToMax",
5035         "config.memoryHotAddEnabled",
5036         "config.version",
5037         "config.guestId",
5038         "config.extraConfig",
5039         "name",
5040     ]
5041     virtual_machine = salt.utils.vmware.get_vm_by_property(
5042         service_instance, name, vm_properties=properties, datacenter=datacenter
5043     )
5044     parent_ref = salt.utils.vmware.get_datacenter(
5045         service_instance=service_instance, datacenter_name=datacenter
5046     )
5047     current_config = {"name": name}
5048     current_config["cpu"] = {
5049         "count": virtual_machine["config.hardware.numCPU"],
5050         "cores_per_socket": virtual_machine["config.hardware.numCoresPerSocket"],
5051         "nested": virtual_machine["config.nestedHVEnabled"],
5052         "hotadd": virtual_machine["config.cpuHotAddEnabled"],
5053         "hotremove": virtual_machine["config.cpuHotRemoveEnabled"],
5054     }
5055     current_config["memory"] = {
5056         "size": virtual_machine["config.hardware.memoryMB"],
5057         "unit": "MB",
5058         "reservation_max": virtual_machine["config.memoryReservationLockedToMax"],
5059         "hotadd": virtual_machine["config.memoryHotAddEnabled"],
5060     }
5061     current_config["image"] = virtual_machine["config.guestId"]
5062     current_config["version"] = virtual_machine["config.version"]
5063     current_config["advanced_configs"] = {}
5064     for extra_conf in virtual_machine["config.extraConfig"]:
5065         try:
5066             current_config["advanced_configs"][extra_conf.key] = int(extra_conf.value)
5067         except ValueError:
5068             current_config["advanced_configs"][extra_conf.key] = extra_conf.value
5069     current_config["disks"] = []
5070     current_config["scsi_devices"] = []
5071     current_config["interfaces"] = []
5072     current_config["serial_ports"] = []
5073     current_config["cd_drives"] = []
5074     current_config["sata_controllers"] = []
5075     for device in virtual_machine["config.hardware.device"]:
5076         if isinstance(device, vim.vm.device.VirtualSCSIController):
5077             controller = {}
5078             controller["adapter"] = device.deviceInfo.label
5079             controller["bus_number"] = device.busNumber
5080             bus_sharing = device.sharedBus
5081             if bus_sharing == "noSharing":
5082                 controller["bus_sharing"] = "no_sharing"
5083             elif bus_sharing == "virtualSharing":
5084                 controller["bus_sharing"] = "virtual_sharing"
5085             elif bus_sharing == "physicalSharing":
5086                 controller["bus_sharing"] = "physical_sharing"
5087             if isinstance(device, vim.vm.device.ParaVirtualSCSIController):
5088                 controller["type"] = "paravirtual"
5089             elif isinstance(device, vim.vm.device.VirtualBusLogicController):
5090                 controller["type"] = "buslogic"
5091             elif isinstance(device, vim.vm.device.VirtualLsiLogicController):
5092                 controller["type"] = "lsilogic"
5093             elif isinstance(device, vim.vm.device.VirtualLsiLogicSASController):
5094                 controller["type"] = "lsilogic_sas"
5095             if objects:
5096                 controller["device"] = device.device
5097                 controller["key"] = device.key
5098                 controller["object"] = device
5099             current_config["scsi_devices"].append(controller)
5100         if isinstance(device, vim.vm.device.VirtualDisk):
5101             disk = {}
5102             disk["adapter"] = device.deviceInfo.label
5103             disk["size"] = device.capacityInKB
5104             disk["unit"] = "KB"
5105             controller = _get_device_by_key(
5106                 virtual_machine["config.hardware.device"], device.controllerKey
5107             )
5108             disk["controller"] = controller.deviceInfo.label
5109             disk["address"] = str(controller.busNumber) + ":" + str(device.unitNumber)
5110             disk["datastore"] = salt.utils.vmware.get_managed_object_name(
5111                 device.backing.datastore
5112             )
5113             disk["thin_provision"] = device.backing.thinProvisioned
5114             disk["eagerly_scrub"] = device.backing.eagerlyScrub
5115             if objects:
5116                 disk["key"] = device.key
5117                 disk["unit_number"] = device.unitNumber
5118                 disk["bus_number"] = controller.busNumber
5119                 disk["controller_key"] = device.controllerKey
5120                 disk["object"] = device
5121             current_config["disks"].append(disk)
5122         if isinstance(device, vim.vm.device.VirtualEthernetCard):
5123             interface = {}
5124             interface["adapter"] = device.deviceInfo.label
5125             interface[
5126                 "adapter_type"
5127             ] = salt.utils.vmware.get_network_adapter_object_type(device)
5128             interface["connectable"] = {
5129                 "allow_guest_control": device.connectable.allowGuestControl,
5130                 "connected": device.connectable.connected,
5131                 "start_connected": device.connectable.startConnected,
5132             }
5133             interface["mac"] = device.macAddress
5134             if isinstance(
5135                 device.backing,
5136                 vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo,
5137             ):
5138                 interface["switch_type"] = "distributed"
5139                 pg_key = device.backing.port.portgroupKey
5140                 network_ref = salt.utils.vmware.get_mor_by_property(
5141                     service_instance,
5142                     vim.DistributedVirtualPortgroup,
5143                     pg_key,
5144                     property_name="key",
5145                     container_ref=parent_ref,
5146                 )
5147             elif isinstance(
5148                 device.backing, vim.vm.device.VirtualEthernetCard.NetworkBackingInfo
5149             ):
5150                 interface["switch_type"] = "standard"
5151                 network_ref = device.backing.network
5152             interface["name"] = salt.utils.vmware.get_managed_object_name(network_ref)
5153             if objects:
5154                 interface["key"] = device.key
5155                 interface["object"] = device
5156             current_config["interfaces"].append(interface)
5157         if isinstance(device, vim.vm.device.VirtualCdrom):
5158             drive = {}
5159             drive["adapter"] = device.deviceInfo.label
5160             controller = _get_device_by_key(
5161                 virtual_machine["config.hardware.device"], device.controllerKey
5162             )
5163             drive["controller"] = controller.deviceInfo.label
5164             if isinstance(
5165                 device.backing, vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo
5166             ):
5167                 drive["device_type"] = "client_device"
5168                 drive["client_device"] = {"mode": "passthrough"}
5169             if isinstance(
5170                 device.backing, vim.vm.device.VirtualCdrom.RemoteAtapiBackingInfo
5171             ):
5172                 drive["device_type"] = "client_device"
5173                 drive["client_device"] = {"mode": "atapi"}
5174             if isinstance(device.backing, vim.vm.device.VirtualCdrom.IsoBackingInfo):
5175                 drive["device_type"] = "datastore_iso_file"
5176                 drive["datastore_iso_file"] = {"path": device.backing.fileName}
5177             drive["connectable"] = {
5178                 "allow_guest_control": device.connectable.allowGuestControl,
5179                 "connected": device.connectable.connected,
5180                 "start_connected": device.connectable.startConnected,
5181             }
5182             if objects:
5183                 drive["key"] = device.key
5184                 drive["controller_key"] = device.controllerKey
5185                 drive["object"] = device
5186             current_config["cd_drives"].append(drive)
5187         if isinstance(device, vim.vm.device.VirtualSerialPort):
5188             port = {}
5189             port["adapter"] = device.deviceInfo.label
5190             if isinstance(
5191                 device.backing, vim.vm.device.VirtualSerialPort.URIBackingInfo
5192             ):
5193                 port["type"] = "network"
5194                 port["backing"] = {
5195                     "uri": device.backing.proxyURI,
5196                     "direction": device.backing.direction,
5197                     "filename": device.backing.serviceURI,
5198                 }
5199             if isinstance(
5200                 device.backing, vim.vm.device.VirtualSerialPort.PipeBackingInfo
5201             ):
5202                 port["type"] = "pipe"
5203             if isinstance(
5204                 device.backing, vim.vm.device.VirtualSerialPort.FileBackingInfo
5205             ):
5206                 port["type"] = "file"
5207             if isinstance(
5208                 device.backing, vim.vm.device.VirtualSerialPort.DeviceBackingInfo
5209             ):
5210                 port["type"] = "device"
5211             port["yield"] = device.yieldOnPoll
5212             port["connectable"] = {
5213                 "allow_guest_control": device.connectable.allowGuestControl,
5214                 "connected": device.connectable.connected,
5215                 "start_connected": device.connectable.startConnected,
5216             }
5217             if objects:
5218                 port["key"] = device.key
5219                 port["object"] = device
5220             current_config["serial_ports"].append(port)
5221         if isinstance(device, vim.vm.device.VirtualSATAController):
5222             sata = {}
5223             sata["adapter"] = device.deviceInfo.label
5224             sata["bus_number"] = device.busNumber
5225             if objects:
5226                 sata["device"] = device.device  # keys of the connected devices
5227                 sata["key"] = device.key
5228                 sata["object"] = device
5229             current_config["sata_controllers"].append(sata)
5230     return current_config
5231 def _update_disks(disks_old_new):
5232     disk_changes = []
5233     if disks_old_new:
5234         devs = [disk["old"]["address"] for disk in disks_old_new]
5235         log.trace("Updating disks %s", devs)
5236         for item in disks_old_new:
5237             current_disk = item["old"]
5238             next_disk = item["new"]
5239             difference = recursive_diff(current_disk, next_disk)
5240             difference.ignore_unset_values = False
5241             if difference.changed():
5242                 if next_disk["size"] &lt; current_disk["size"]:
5243                     raise salt.exceptions.VMwareSaltError(
5244                         "Disk cannot be downsized size={} unit={} "
5245                         "controller_key={} "
5246                         "unit_number={}".format(
5247                             next_disk["size"],
5248                             next_disk["unit"],
5249                             current_disk["controller_key"],
5250                             current_disk["unit_number"],
5251                         )
5252                     )
5253                 log.trace(
5254                     "Virtual machine disk will be updated size=%s unit=%s "
5255                     "controller_key=%s unit_number=%s",
5256                     next_disk["size"],
5257                     next_disk["unit"],
5258                     current_disk["controller_key"],
5259                     current_disk["unit_number"],
5260                 )
5261                 device_config_spec = _apply_hard_disk(
5262                     current_disk["unit_number"],
5263                     current_disk["key"],
5264                     "edit",
5265                     size=next_disk["size"],
5266                     unit=next_disk["unit"],
5267                     controller_key=current_disk["controller_key"],
5268                 )
5269                 device_config_spec.device.backing = current_disk["object"].backing
5270                 disk_changes.append(device_config_spec)
5271     return disk_changes
5272 def _update_scsi_devices(scsis_old_new, current_disks):
5273     device_config_specs = []
5274     if scsis_old_new:
5275         devs = [scsi["old"]["adapter"] for scsi in scsis_old_new]
5276         log.trace("Updating SCSI controllers %s", devs)
5277         for item in scsis_old_new:
5278             next_scsi = item["new"]
5279             current_scsi = item["old"]
5280             difference = recursive_diff(current_scsi, next_scsi)
5281             difference.ignore_unset_values = False
5282             if difference.changed():
5283                 log.trace(
5284                     "Virtual machine scsi device will be updated key=%s "
5285                     "bus_number=%s type=%s bus_sharing=%s",
5286                     current_scsi["key"],
5287                     current_scsi["bus_number"],
5288                     next_scsi["type"],
5289                     next_scsi["bus_sharing"],
5290                 )
5291                 if next_scsi["type"] != current_scsi["type"]:
5292                     device_config_specs.append(_delete_device(current_scsi["object"]))
5293                     device_config_specs.append(
5294                         _apply_scsi_controller(
5295                             current_scsi["adapter"],
5296                             next_scsi["type"],
5297                             next_scsi["bus_sharing"],
5298                             current_scsi["key"],
5299                             current_scsi["bus_number"],
5300                             "add",
5301                         )
5302                     )
5303                     disks_to_update = []
5304                     for disk_key in current_scsi["device"]:
5305                         disk_objects = [disk["object"] for disk in current_disks]
5306                         disks_to_update.append(
5307                             _get_device_by_key(disk_objects, disk_key)
5308                         )
5309                     for current_disk in disks_to_update:
5310                         disk_spec = vim.vm.device.VirtualDeviceSpec()
5311                         disk_spec.device = current_disk
5312                         disk_spec.operation = "edit"
5313                         device_config_specs.append(disk_spec)
5314                 else:
5315                     device_config_specs.append(
5316                         _apply_scsi_controller(
5317                             current_scsi["adapter"],
5318                             current_scsi["type"],
5319                             next_scsi["bus_sharing"],
5320                             current_scsi["key"],
5321                             current_scsi["bus_number"],
5322                             "edit",
5323                         )
5324                     )
5325     return device_config_specs
5326 def _update_network_adapters(interface_old_new, parent):
5327     network_changes = []
5328     if interface_old_new:
5329         devs = [inter["old"]["mac"] for inter in interface_old_new]
5330         log.trace("Updating network interfaces %s", devs)
5331         for item in interface_old_new:
5332             current_interface = item["old"]
5333             next_interface = item["new"]
5334             difference = recursive_diff(current_interface, next_interface)
5335             difference.ignore_unset_values = False
5336             if difference.changed():
5337                 log.trace(
5338                     "Virtual machine network adapter will be updated "
5339                     "switch_type=%s name=%s adapter_type=%s mac=%s",
5340                     next_interface["switch_type"],
5341                     next_interface["name"],
5342                     current_interface["adapter_type"],
5343                     current_interface["mac"],
5344                 )
5345                 device_config_spec = _apply_network_adapter_config(
5346                     current_interface["key"],
5347                     next_interface["name"],
5348                     current_interface["adapter_type"],
5349                     next_interface["switch_type"],
5350                     operation="edit",
5351                     mac=current_interface["mac"],
5352                     parent=parent,
5353                 )
5354                 network_changes.append(device_config_spec)
5355     return network_changes
5356 def _update_serial_ports(serial_old_new):
5357     serial_changes = []
5358     if serial_old_new:
5359         devs = [serial["old"]["adapter"] for serial in serial_old_new]
5360         log.trace("Updating serial ports %s", devs)
5361         for item in serial_old_new:
5362             current_serial = item["old"]
5363             next_serial = item["new"]
5364             difference = recursive_diff(current_serial, next_serial)
5365             difference.ignore_unset_values = False
5366             if difference.changed():
5367                 serial_changes.append(
5368                     _apply_serial_port(next_serial, current_serial["key"], "edit")
5369                 )
5370         return serial_changes
5371 def _update_cd_drives(drives_old_new, controllers=None, parent=None):
5372     cd_changes = []
5373     if drives_old_new:
5374         devs = [drive["old"]["adapter"] for drive in drives_old_new]
5375         log.trace("Updating cd/dvd drives %s", devs)
5376         for item in drives_old_new:
5377             current_drive = item["old"]
5378             new_drive = item["new"]
5379             difference = recursive_diff(current_drive, new_drive)
5380             difference.ignore_unset_values = False
5381             if difference.changed():
5382                 if controllers:
5383                     controller = _get_device_by_label(
5384                         controllers, new_drive["controller"]
5385                     )
5386                     controller_key = controller.key
5387                 else:
5388                     controller_key = current_drive["controller_key"]
5389                 cd_changes.append(
5390                     _apply_cd_drive(
5391                         current_drive["adapter"],
5392                         current_drive["key"],
5393                         new_drive["device_type"],
5394                         "edit",
5395                         client_device=new_drive["client_device"]
5396                         if "client_device" in new_drive
5397                         else None,
5398                         datastore_iso_file=new_drive["datastore_iso_file"]
5399                         if "datastore_iso_file" in new_drive
5400                         else None,
5401                         connectable=new_drive["connectable"],
5402                         controller_key=controller_key,
5403                         parent_ref=parent,
5404                     )
5405                 )
5406     return cd_changes
5407 def _delete_device(device):
5408     log.trace("Deleting device with type %s", type(device))
5409     device_spec = vim.vm.device.VirtualDeviceSpec()
5410     device_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.remove
5411     device_spec.device = device
5412     return device_spec
5413 def _get_client(server, username, password, verify_ssl=None, ca_bundle=None):
5414     details = None
5415     if not (server and username and password):
5416         details = __salt__["vcenter.get_details"]()
5417         server = details["vcenter"]
5418         username = details["username"]
5419         password = details["password"]
5420     if verify_ssl is None:
5421         if details is None:
5422             details = __salt__["vcenter.get_details"]()
5423         verify_ssl = details.get("verify_ssl", True)
5424         if verify_ssl is None:
5425             verify_ssl = True
5426     if ca_bundle is None:
5427         if details is None:
5428             details = __salt__["vcenter.get_details"]()
5429         ca_bundle = details.get("ca_bundle", None)
5430     if verify_ssl is False and ca_bundle is not None:
5431         log.error("Cannot set verify_ssl to False and ca_bundle together")
5432         return False
5433     if ca_bundle:
5434         ca_bundle = salt.utils.http.get_ca_bundle({"ca_bundle": ca_bundle})
5435     client = salt.utils.vmware.get_vsphere_client(
5436         server=server,
5437         username=username,
5438         password=password,
5439         verify_ssl=verify_ssl,
5440         ca_bundle=ca_bundle,
5441     )
5442     return client
5443 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5444 @_supports_proxies("vcenter")
5445 @_gets_service_instance_via_proxy
5446 def list_tag_categories(
5447     server=None,
5448     username=None,
5449     password=None,
5450     service_instance=None,
5451     verify_ssl=None,
5452     ca_bundle=None,
5453 ):
5454     categories = None
5455     client = _get_client(
5456         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5457     )
5458     if client:
5459         categories = client.tagging.Category.list()
5460     return {"Categories": categories}
5461 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5462 @_supports_proxies("vcenter")
5463 @_gets_service_instance_via_proxy
5464 def list_tags(
5465     server=None,
5466     username=None,
5467     password=None,
5468     service_instance=None,
5469     verify_ssl=None,
5470     ca_bundle=None,
5471 ):
5472     tags = None
5473     client = _get_client(
5474         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5475     )
5476     if client:
5477         tags = client.tagging.Tag.list()
5478     return {"Tags": tags}
5479 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5480 @_supports_proxies("vcenter")
5481 @_gets_service_instance_via_proxy
5482 def attach_tag(
5483     object_id,
5484     tag_id,
5485     managed_obj="ClusterComputeResource",
5486     server=None,
5487     username=None,
5488     password=None,
5489     service_instance=None,
5490     verify_ssl=None,
5491     ca_bundle=None,
5492 ):
5493     tag_attached = None
5494     client = _get_client(
5495         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5496     )
5497     if client:
5498         dynamic_id = DynamicID(type=managed_obj, id=object_id)
5499         try:
5500             tag_attached = client.tagging.TagAssociation.attach(
5501                 tag_id=tag_id, object_id=dynamic_id
5502             )
5503         except vsphere_errors:
5504             log.warning(
5505                 "Unable to attach tag. Check user privileges and"
5506                 " object_id (must be a string)."
5507             )
5508     return {"Tag attached": tag_attached}
5509 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5510 @_supports_proxies("vcenter")
5511 @_gets_service_instance_via_proxy
5512 def list_attached_tags(
5513     object_id,
5514     managed_obj="ClusterComputeResource",
5515     server=None,
5516     username=None,
5517     password=None,
5518     service_instance=None,
5519     verify_ssl=None,
5520     ca_bundle=None,
5521 ):
5522     attached_tags = None
5523     client = _get_client(
5524         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5525     )
5526     if client:
5527         dynamic_id = DynamicID(type=managed_obj, id=object_id)
5528         try:
5529             attached_tags = client.tagging.TagAssociation.list_attached_tags(dynamic_id)
5530         except vsphere_errors:
5531             log.warning(
5532                 "Unable to list attached tags. Check user privileges"
5533                 " and object_id (must be a string)."
5534             )
5535     return {"Attached tags": attached_tags}
5536 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5537 @_supports_proxies("vcenter")
5538 @_gets_service_instance_via_proxy
5539 def create_tag_category(
5540     name,
5541     description,
5542     cardinality,
5543     server=None,
5544     username=None,
5545     password=None,
5546     service_instance=None,
5547     verify_ssl=None,
5548     ca_bundle=None,
5549 ):
5550     category_created = None
5551     client = _get_client(
5552         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5553     )
5554     if client:
5555         if cardinality == "SINGLE":
5556             cardinality = CategoryModel.Cardinality.SINGLE
5557         elif cardinality == "MULTIPLE":
5558             cardinality = CategoryModel.Cardinality.MULTIPLE
5559         else:
5560             cardinality = None
5561         create_spec = client.tagging.Category.CreateSpec()
5562         create_spec.name = name
5563         create_spec.description = description
5564         create_spec.cardinality = cardinality
5565         associable_types = set()
5566         create_spec.associable_types = associable_types
5567         try:
5568             category_created = client.tagging.Category.create(create_spec)
5569         except vsphere_errors:
5570             log.warning(
5571                 "Unable to create tag category. Check user privilege"
5572                 " and see if category exists."
5573             )
5574     return {"Category created": category_created}
5575 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5576 @_supports_proxies("vcenter")
5577 @_gets_service_instance_via_proxy
5578 def delete_tag_category(
5579     category_id,
5580     server=None,
5581     username=None,
5582     password=None,
5583     service_instance=None,
5584     verify_ssl=None,
5585     ca_bundle=None,
5586 ):
5587     category_deleted = None
5588     client = _get_client(
5589         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5590     )
5591     if client:
5592         try:
5593             category_deleted = client.tagging.Category.delete(category_id)
5594         except vsphere_errors:
5595             log.warning(
5596                 "Unable to delete tag category. Check user privilege"
5597                 " and see if category exists."
5598             )
5599     return {"Category deleted": category_deleted}
5600 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5601 @_supports_proxies("vcenter")
5602 @_gets_service_instance_via_proxy
5603 def create_tag(
5604     name,
5605     description,
5606     category_id,
5607     server=None,
5608     username=None,
5609     password=None,
5610     service_instance=None,
5611     verify_ssl=None,
5612     ca_bundle=None,
5613 ):
5614     tag_created = None
5615     client = _get_client(
5616         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5617     )
5618     if client:
5619         create_spec = client.tagging.Tag.CreateSpec()
5620         create_spec.name = name
5621         create_spec.description = description
5622         create_spec.category_id = category_id
5623         try:
5624             tag_created = client.tagging.Tag.create(create_spec)
5625         except vsphere_errors:
5626             log.warning(
5627                 "Unable to create tag. Check user privilege and see if category exists."
5628             )
5629     return {"Tag created": tag_created}
5630 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
5631 @_supports_proxies("vcenter")
5632 @_gets_service_instance_via_proxy
5633 def delete_tag(
5634     tag_id,
5635     server=None,
5636     username=None,
5637     password=None,
5638     service_instance=None,
5639     verify_ssl=None,
5640     ca_bundle=None,
5641 ):
5642     tag_deleted = None
5643     client = _get_client(
5644         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
5645     )
5646     if client:
5647         try:
5648             tag_deleted = client.tagging.Tag.delete(tag_id)
5649         except vsphere_errors:
5650             log.warning(
5651                 "Unable to delete category. Check user privileges"
5652                 " and that category exists."
5653             )
5654     return {"Tag deleted": tag_deleted}
5655 @depends(HAS_PYVMOMI)
5656 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
5657 @_gets_service_instance_via_proxy
5658 def create_vm(
5659     vm_name,
5660     cpu,
5661     memory,
5662     image,
5663     version,
5664     datacenter,
5665     datastore,
5666     placement,
5667     interfaces,
5668     disks,
5669     scsi_devices,
5670     serial_ports=None,
5671     ide_controllers=None,
5672     sata_controllers=None,
5673     cd_drives=None,
5674     advanced_configs=None,
5675     service_instance=None,
5676 ):
5677     container_object = salt.utils.vmware.get_datacenter(service_instance, datacenter)
5678     (resourcepool_object, placement_object) = salt.utils.vmware.get_placement(
5679         service_instance, datacenter, placement=placement
5680     )
5681     folder_object = salt.utils.vmware.get_folder(
5682         service_instance, datacenter, placement
5683     )
5684     config_spec = vim.vm.ConfigSpec()
5685     config_spec.name = vm_name
5686     config_spec.guestId = image
5687     config_spec.files = vim.vm.FileInfo()
5688     datastore_object = salt.utils.vmware.get_datastores(
5689         service_instance, placement_object, datastore_names=[datastore]
5690     )[0]
5691     if not datastore_object:
5692         raise salt.exceptions.ArgumentValueError(
5693             "Specified datastore: '{}' does not exist.".format(datastore)
5694         )
5695     try:
5696         ds_summary = salt.utils.vmware.get_properties_of_managed_object(
5697             datastore_object, "summary.type"
5698         )
5699         if "summary.type" in ds_summary and ds_summary["summary.type"] == "vsan":
5700             log.trace(
5701                 "The vmPathName should be the datastore "
5702                 "name if the datastore type is vsan"
5703             )
5704             config_spec.files.vmPathName = "[{}]".format(datastore)
5705         else:
5706             config_spec.files.vmPathName = "[{0}] {1}/{1}.vmx".format(
5707                 datastore, vm_name
5708             )
5709     except salt.exceptions.VMwareApiError:
5710         config_spec.files.vmPathName = "[{0}] {1}/{1}.vmx".format(datastore, vm_name)
5711     cd_controllers = []
5712     if version:
5713         _apply_hardware_version(version, config_spec, "add")
5714     if cpu:
5715         _apply_cpu_config(config_spec, cpu)
5716     if memory:
5717         _apply_memory_config(config_spec, memory)
5718     if scsi_devices:
5719         scsi_specs = _create_scsi_devices(scsi_devices)
5720         config_spec.deviceChange.extend(scsi_specs)
5721     if disks:
5722         scsi_controllers = [spec.device for spec in scsi_specs]
5723         disk_specs = _create_disks(
5724             service_instance,
5725             disks,
5726             scsi_controllers=scsi_controllers,
5727             parent=container_object,
5728         )
5729         config_spec.deviceChange.extend(disk_specs)
5730     if interfaces:
5731         (interface_specs, nic_settings) = _create_network_adapters(
5732             interfaces, parent=container_object
5733         )
5734         config_spec.deviceChange.extend(interface_specs)
5735     if serial_ports:
5736         serial_port_specs = _create_serial_ports(serial_ports)
5737         config_spec.deviceChange.extend(serial_port_specs)
5738     if ide_controllers:
5739         ide_specs = _create_ide_controllers(ide_controllers)
5740         config_spec.deviceChange.extend(ide_specs)
5741         cd_controllers.extend(ide_specs)
5742     if sata_controllers:
5743         sata_specs = _create_sata_controllers(sata_controllers)
5744         config_spec.deviceChange.extend(sata_specs)
5745         cd_controllers.extend(sata_specs)
5746     if cd_drives:
5747         cd_drive_specs = _create_cd_drives(
5748             cd_drives, controllers=cd_controllers, parent_ref=container_object
5749         )
5750         config_spec.deviceChange.extend(cd_drive_specs)
5751     if advanced_configs:
5752         _apply_advanced_config(config_spec, advanced_configs)
5753     salt.utils.vmware.create_vm(
5754         vm_name, config_spec, folder_object, resourcepool_object, placement_object
5755     )
5756     return {"create_vm": True}
5757 @depends(HAS_PYVMOMI)
5758 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
5759 @_gets_service_instance_via_proxy
5760 def update_vm(
5761     vm_name,
5762     cpu=None,
5763     memory=None,
5764     image=None,
5765     version=None,
5766     interfaces=None,
5767     disks=None,
5768     scsi_devices=None,
5769     serial_ports=None,
5770     datacenter=None,
5771     datastore=None,
5772     cd_dvd_drives=None,
5773     sata_controllers=None,
5774     advanced_configs=None,
5775     service_instance=None,
5776 ):
5777     current_config = get_vm_config(
5778         vm_name, datacenter=datacenter, objects=True, service_instance=service_instance
5779     )
5780     diffs = compare_vm_configs(
5781         {
5782             "name": vm_name,
5783             "cpu": cpu,
5784             "memory": memory,
5785             "image": image,
5786             "version": version,
5787             "interfaces": interfaces,
5788             "disks": disks,
5789             "scsi_devices": scsi_devices,
5790             "serial_ports": serial_ports,
5791             "datacenter": datacenter,
5792             "datastore": datastore,
5793             "cd_drives": cd_dvd_drives,
5794             "sata_controllers": sata_controllers,
5795             "advanced_configs": advanced_configs,
5796         },
5797         current_config,
5798     )
5799     config_spec = vim.vm.ConfigSpec()
5800     datacenter_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
5801     vm_ref = salt.utils.vmware.get_mor_by_property(
5802         service_instance,
5803         vim.VirtualMachine,
5804         vm_name,
5805         property_name="name",
5806         container_ref=datacenter_ref,
5807     )
5808     difference_keys = diffs.keys()
5809     if "cpu" in difference_keys:
5810         if diffs["cpu"].changed() != set():
5811             _apply_cpu_config(config_spec, diffs["cpu"].current_dict)
5812     if "memory" in difference_keys:
5813         if diffs["memory"].changed() != set():
5814             _apply_memory_config(config_spec, diffs["memory"].current_dict)
5815     if "advanced_configs" in difference_keys:
5816         _apply_advanced_config(
5817             config_spec, diffs["advanced_configs"].new_values, vm_ref.config.extraConfig
5818         )
5819     if "version" in difference_keys:
5820         _apply_hardware_version(version, config_spec, "edit")
5821     if "image" in difference_keys:
5822         config_spec.guestId = image
5823     new_scsi_devices = []
5824     if "scsi_devices" in difference_keys and "disks" in current_config:
5825         scsi_changes = []
5826         scsi_changes.extend(
5827             _update_scsi_devices(
5828                 diffs["scsi_devices"].intersect, current_config["disks"]
5829             )
5830         )
5831         for item in diffs["scsi_devices"].removed:
5832             scsi_changes.append(_delete_device(item["object"]))
5833         new_scsi_devices = _create_scsi_devices(diffs["scsi_devices"].added)
5834         scsi_changes.extend(new_scsi_devices)
5835         config_spec.deviceChange.extend(scsi_changes)
5836     if "disks" in difference_keys:
5837         disk_changes = []
5838         disk_changes.extend(_update_disks(diffs["disks"].intersect))
5839         for item in diffs["disks"].removed:
5840             disk_changes.append(_delete_device(item["object"]))
5841         scsi_controllers = [dev["object"] for dev in current_config["scsi_devices"]]
5842         scsi_controllers.extend(
5843             [device_spec.device for device_spec in new_scsi_devices]
5844         )
5845         disk_changes.extend(
5846             _create_disks(
5847                 service_instance,
5848                 diffs["disks"].added,
5849                 scsi_controllers=scsi_controllers,
5850                 parent=datacenter_ref,
5851             )
5852         )
5853         config_spec.deviceChange.extend(disk_changes)
5854     if "interfaces" in difference_keys:
5855         network_changes = []
5856         network_changes.extend(
5857             _update_network_adapters(diffs["interfaces"].intersect, datacenter_ref)
5858         )
5859         for item in diffs["interfaces"].removed:
5860             network_changes.append(_delete_device(item["object"]))
5861         (adapters, nics) = _create_network_adapters(
5862             diffs["interfaces"].added, datacenter_ref
5863         )
5864         network_changes.extend(adapters)
5865         config_spec.deviceChange.extend(network_changes)
5866     if "serial_ports" in difference_keys:
5867         serial_changes = []
5868         serial_changes.extend(_update_serial_ports(diffs["serial_ports"].intersect))
5869         for item in diffs["serial_ports"].removed:
5870             serial_changes.append(_delete_device(item["object"]))
5871         serial_changes.extend(_create_serial_ports(diffs["serial_ports"].added))
5872         config_spec.deviceChange.extend(serial_changes)
5873     new_controllers = []
5874     if "sata_controllers" in difference_keys:
5875         sata_specs = _create_sata_controllers(diffs["sata_controllers"].added)
5876         for item in diffs["sata_controllers"].removed:
5877             sata_specs.append(_delete_device(item["object"]))
5878         new_controllers.extend(sata_specs)
5879         config_spec.deviceChange.extend(sata_specs)
5880     if "cd_drives" in difference_keys:
5881         cd_changes = []
5882         controllers = [dev["object"] for dev in current_config["sata_controllers"]]
5883         controllers.extend([device_spec.device for device_spec in new_controllers])
5884         cd_changes.extend(
5885             _update_cd_drives(
5886                 diffs["cd_drives"].intersect,
5887                 controllers=controllers,
5888                 parent=datacenter_ref,
5889             )
5890         )
5891         for item in diffs["cd_drives"].removed:
5892             cd_changes.append(_delete_device(item["object"]))
5893         cd_changes.extend(
5894             _create_cd_drives(
5895                 diffs["cd_drives"].added,
5896                 controllers=controllers,
5897                 parent_ref=datacenter_ref,
5898             )
5899         )
5900         config_spec.deviceChange.extend(cd_changes)
5901     if difference_keys:
5902         salt.utils.vmware.update_vm(vm_ref, config_spec)
5903     changes = {}
5904     for key, properties in diffs.items():
5905         if isinstance(properties, salt.utils.listdiffer.ListDictDiffer):
5906             properties.remove_diff(diff_key="object", diff_list="intersect")
5907             properties.remove_diff(diff_key="key", diff_list="intersect")
5908             properties.remove_diff(diff_key="object", diff_list="removed")
5909             properties.remove_diff(diff_key="key", diff_list="removed")
5910         changes[key] = properties.diffs
5911     return changes
5912 @depends(HAS_PYVMOMI)
5913 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
5914 @_gets_service_instance_via_proxy
5915 def register_vm(name, datacenter, placement, vmx_path, service_instance=None):
5916     log.trace(
5917         "Registering virtual machine with properties datacenter=%s, "
5918         "placement=%s, vmx_path=%s",
5919         datacenter,
5920         placement,
5921         vmx_path,
5922     )
5923     datacenter_object = salt.utils.vmware.get_datacenter(service_instance, datacenter)
5924     if "cluster" in placement:
5925         cluster_obj = salt.utils.vmware.get_cluster(
5926             datacenter_object, placement["cluster"]
5927         )
5928         cluster_props = salt.utils.vmware.get_properties_of_managed_object(
5929             cluster_obj, properties=["resourcePool"]
5930         )
5931         if "resourcePool" in cluster_props:
5932             resourcepool = cluster_props["resourcePool"]
5933         else:
5934             raise salt.exceptions.VMwareObjectRetrievalError(
5935                 "The cluster's resource pool object could not be retrieved."
5936             )
5937         salt.utils.vmware.register_vm(datacenter_object, name, vmx_path, resourcepool)
5938     elif "host" in placement:
5939         hosts = salt.utils.vmware.get_hosts(
5940             service_instance, datacenter_name=datacenter, host_names=[placement["host"]]
5941         )
5942         if not hosts:
5943             raise salt.exceptions.VMwareObjectRetrievalError(
5944                 "ESXi host named '{}' wasn't found.".format(placement["host"])
5945             )
5946         host_obj = hosts[0]
5947         host_props = salt.utils.vmware.get_properties_of_managed_object(
5948             host_obj, properties=["parent"]
5949         )
5950         if "parent" in host_props:
5951             host_parent = host_props["parent"]
5952             parent = salt.utils.vmware.get_properties_of_managed_object(
5953                 host_parent, properties=["parent"]
5954             )
5955             if "parent" in parent:
5956                 resourcepool = parent["parent"]
5957             else:
5958                 raise salt.exceptions.VMwareObjectRetrievalError(
5959                     "The host parent's parent object could not be retrieved."
5960                 )
5961         else:
5962             raise salt.exceptions.VMwareObjectRetrievalError(
5963                 "The host's parent object could not be retrieved."
5964             )
5965         salt.utils.vmware.register_vm(
5966             datacenter_object, name, vmx_path, resourcepool, host_object=host_obj
5967         )
5968     result = {
5969         "comment": "Virtual machine registration action succeeded",
5970         "changes": {"register_vm": True},
5971     }
5972     return result
5973 @depends(HAS_PYVMOMI)
5974 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
5975 @_gets_service_instance_via_proxy
5976 def power_on_vm(name, datacenter=None, service_instance=None):
5977     log.trace("Powering on virtual machine %s", name)
5978     vm_properties = ["name", "summary.runtime.powerState"]
5979     virtual_machine = salt.utils.vmware.get_vm_by_property(
5980         service_instance, name, datacenter=datacenter, vm_properties=vm_properties
5981     )
5982     if virtual_machine["summary.runtime.powerState"] == "poweredOn":
5983         result = {
5984             "comment": "Virtual machine is already powered on",
5985             "changes": {"power_on": True},
5986         }
5987         return result
5988     salt.utils.vmware.power_cycle_vm(virtual_machine["object"], action="on")
5989     result = {
5990         "comment": "Virtual machine power on action succeeded",
5991         "changes": {"power_on": True},
5992     }
5993     return result
5994 @depends(HAS_PYVMOMI)
5995 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
5996 @_gets_service_instance_via_proxy
5997 def power_off_vm(name, datacenter=None, service_instance=None):
5998     log.trace("Powering off virtual machine %s", name)
5999     vm_properties = ["name", "summary.runtime.powerState"]
6000     virtual_machine = salt.utils.vmware.get_vm_by_property(
6001         service_instance, name, datacenter=datacenter, vm_properties=vm_properties
6002     )
6003     if virtual_machine["summary.runtime.powerState"] == "poweredOff":
6004         result = {
6005             "comment": "Virtual machine is already powered off",
6006             "changes": {"power_off": True},
6007         }
6008         return result
6009     salt.utils.vmware.power_cycle_vm(virtual_machine["object"], action="off")
6010     result = {
6011         "comment": "Virtual machine power off action succeeded",
6012         "changes": {"power_off": True},
6013     }
6014     return result
6015 def _remove_vm(name, datacenter, service_instance, placement=None, power_off=None):
6016     results = {}
6017     if placement:
6018         (resourcepool_object, placement_object) = salt.utils.vmware.get_placement(
6019             service_instance, datacenter, placement
6020         )
6021     else:
6022         placement_object = salt.utils.vmware.get_datacenter(
6023             service_instance, datacenter
6024         )
6025     if power_off:
6026         power_off_vm(name, datacenter, service_instance)
6027         results["powered_off"] = True
6028     vm_ref = salt.utils.vmware.get_mor_by_property(
6029         service_instance,
6030         vim.VirtualMachine,
6031         name,
6032         property_name="name",
6033         container_ref=placement_object,
6034     )
6035     if not vm_ref:
6036         raise salt.exceptions.VMwareObjectRetrievalError(
6037             "The virtual machine object {} in datacenter {} was not found".format(
6038                 name, datacenter
6039             )
6040         )
6041     return results, vm_ref
6042 @depends(HAS_PYVMOMI)
6043 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
6044 @_gets_service_instance_via_proxy
6045 def delete_vm(name, datacenter, placement=None, power_off=False, service_instance=None):
6046     results = {}
6047     schema = ESXVirtualMachineDeleteSchema.serialize()
6048     try:
6049         jsonschema.validate(
6050             {"name": name, "datacenter": datacenter, "placement": placement}, schema
6051         )
6052     except jsonschema.exceptions.ValidationError as exc:
6053         raise InvalidConfigError(exc)
6054     (results, vm_ref) = _remove_vm(
6055         name,
6056         datacenter,
6057         service_instance=service_instance,
6058         placement=placement,
6059         power_off=power_off,
6060     )
6061     salt.utils.vmware.delete_vm(vm_ref)
6062     results["deleted_vm"] = True
6063     return results
6064 @depends(HAS_PYVMOMI)
6065 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
6066 @_gets_service_instance_via_proxy
6067 def unregister_vm(
6068     name, datacenter, placement=None, power_off=False, service_instance=None
6069 ):
6070     results = {}
6071     schema = ESXVirtualMachineUnregisterSchema.serialize()
6072     try:
6073         jsonschema.validate(
6074             {"name": name, "datacenter": datacenter, "placement": placement}, schema
6075         )
6076     except jsonschema.exceptions.ValidationError as exc:
6077         raise InvalidConfigError(exc)
6078     (results, vm_ref) = _remove_vm(
6079         name,
6080         datacenter,
6081         service_instance=service_instance,
6082         placement=placement,
6083         power_off=power_off,
6084     )
6085     salt.utils.vmware.unregister_vm(vm_ref)
6086     results["unregistered_vm"] = True
6087     return results
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
