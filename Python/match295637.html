<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for template_1.py &amp; vsphere.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for template_1.py &amp; vsphere.py
      </h3>
<h1 align="center">
        0.4%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>template_1.py (9.540636%)<th>vsphere.py (0.25378326%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(115-120)<td><a href="#" name="0">(4195-4208)</a><td align="center"><font color="#ff0000">14</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(4-16)<td><a href="#" name="1">(195-225)</a><td align="center"><font color="#ec0000">13</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>template_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 Manage basic template commands
2 """
3 <font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>import codecs
4 import io
5 import logging
6 import os
7 import time
8 import salt.utils.data
9 import salt.utils.files
10 import salt.utils.sanitizers
11 import salt.utils.stringio
12 import salt.utils.versions
13 log = logging.getLogger(</b></font>__name__)
14 SLS_ENCODING = "utf-8"  # this one has no BOM.
15 SLS_ENCODER = codecs.getencoder(SLS_ENCODING)
16 def compile_template(
17     template,
18     renderers,
19     default,
20     blacklist,
21     whitelist,
22     saltenv="base",
23     sls="",
24     input_data="",
25     context=None,
26     **kwargs
27 ):
28     """
29     Take the path to a template and return the high data structure
30     derived from the template.
31     Helpers:
32     :param mask_value:
33         Mask value for debugging purposes (prevent sensitive information etc)
34         example: "mask_value="pass*". All "passwd", "password", "pass" will
35         be masked (as text).
36     """
37     ret = {}
38     log.debug("compile template: %s", template)
39     if "env" in kwargs:
40         kwargs.pop("env")
41     if template != ":string:":
42         if not isinstance(template, str):
43             log.error("Template was specified incorrectly: %s", template)
44             return ret
45         if not os.path.isfile(template):
46             log.error("Template does not exist: %s", template)
47             return ret
48         if salt.utils.files.is_empty(template):
49             log.debug("Template is an empty file: %s", template)
50             return ret
51         with codecs.open(template, encoding=SLS_ENCODING) as ifile:
52             input_data = ifile.read()
53             if not input_data.strip():
54                 log.error("Template is nothing but whitespace: %s", template)
55                 return ret
56     render_pipe = template_shebang(
57         template, renderers, default, blacklist, whitelist, input_data
58     )
59     windows_newline = "\r\n" in input_data
60     input_data = io.StringIO(input_data)
61     for render, argline in render_pipe:
62         if salt.utils.stringio.is_readable(input_data):
63             input_data.seek(0)  # pylint: disable=no-member
64         render_kwargs = dict(renderers=renderers, tmplpath=template)
65         if context:
66             render_kwargs["context"] = context
67         render_kwargs.update(kwargs)
68         if argline:
69             render_kwargs["argline"] = argline
70         start = time.time()
71         ret = render(input_data, saltenv, sls, **render_kwargs)
72         log.profile(
73             "Time (in seconds) to render '%s' using '%s' renderer: %s",
74             template,
75             render.__module__.split(".")[-1],
76             time.time() - start,
77         )
78         if ret is None:
79             time.sleep(0.01)
80             ret = render(input_data, saltenv, sls, **render_kwargs)
81         input_data = ret
82         if log.isEnabledFor(logging.GARBAGE):  # pylint: disable=no-member
83             if salt.utils<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>.stringio.is_readable(ret):
84                 log.debug(
85                     "Rendered data from file: %s:\n%s",
86                     template,
87                     salt.utils.sanitizers.mask_args_value(
88                         salt.utils.data.decode(ret.</b></font>read()), kwargs.get("mask_value")
89                     ),
90                 )  # pylint: disable=no-member
91                 ret.seek(0)  # pylint: disable=no-member
92     if windows_newline:
93         if salt.utils.stringio.is_readable(ret):
94             is_stringio = True
95             contents = ret.read()
96         else:
97             is_stringio = False
98             contents = ret
99         if isinstance(contents, str):
100             if "\r\n" not in contents:
101                 contents = contents.replace("\n", "\r\n")
102                 ret = io.StringIO(contents) if is_stringio else contents
103             else:
104                 if is_stringio:
105                     ret.seek(0)
106     return ret
107 def compile_template_str(template, renderers, default, blacklist, whitelist):
108     """
109     Take template as a string and return the high data structure
110     derived from the template.
111     """
112     fn_ = salt.utils.files.mkstemp()
113     with salt.utils.files.fopen(fn_, "wb") as ofile:
114         ofile.write(SLS_ENCODER(template)[0])
115     return compile_template(fn_, renderers, default, blacklist, whitelist)
116 def template_shebang(template, renderers, default, blacklist, whitelist, input_data):
117     """
118     Check the template shebang line and return the list of renderers specified
119     in the pipe.
120     Example shebang lines::
121     """
122     line = ""
123     if template == ":string:":
124         line = input_data.split()[0]
125     else:
126         with salt.utils.files.fopen(template, "r") as ifile:
127             line = salt.utils.stringutils.to_unicode(ifile.readline())
128     if line.startswith("#!") and not line.startswith("#!/"):
129         return check_render_pipe_str(line.strip()[2:], renderers, blacklist, whitelist)
130     else:
131         return check_render_pipe_str(default, renderers, blacklist, whitelist)
132 OLD_STYLE_RENDERERS = {}
133 for comb in (
134     "yaml_jinja",
135     "yaml_mako",
136     "yaml_wempy",
137     "json_jinja",
138     "json_mako",
139     "json_wempy",
140     "yamlex_jinja",
141     "yamlexyamlex_mako",
142     "yamlexyamlex_wempy",
143 ):
144     fmt, tmpl = comb.split("_")
145     OLD_STYLE_RENDERERS[comb] = "{}|{}".format(tmpl, fmt)
146 def check_render_pipe_str(pipestr, renderers, blacklist, whitelist):
147     """
148     Check that all renderers specified in the pipe string are available.
149     If so, return the list of render functions in the pipe as
150     (render_func, arg_str) tuples; otherwise return [].
151     """
152     if pipestr is None:
153         return []
154     parts = [r.strip() for r in pipestr.split("|")]
155     results = []
156     try:
157         if parts[0] == pipestr and pipestr in OLD_STYLE_RENDERERS:
158             parts = OLD_STYLE_RENDERERS[pipestr].split("|")
159         for part in parts:
160             name, argline = (part + " ").split(" ", 1)
161             if whitelist and name not in whitelist or blacklist and name in blacklist:
162                 log.warning(
163                     'The renderer "%s" is disallowed by configuration and '
164                     "will be skipped.",
165                     name,
166                 )
167                 continue
168             results.append((renderers[name], argline.strip()))
169         return results
170     except KeyError:
171         log.error('The renderer "%s" is not available', pipestr)
172         return []
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>vsphere.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 """
2 Manage VMware vCenter servers and ESXi hosts.
3 .. versionadded:: 2015.8.4
4 :codeauthor: Alexandru Bleotu &lt;alexandru.bleotu@morganstaley.com&gt;
5 Dependencies
6 ============
7 - pyVmomi Python Module
8 - ESXCLI
9 pyVmomi
10 -------
11 PyVmomi can be installed via pip:
12 .. code-block:: bash
13     pip install pyVmomi
14 .. note::
15     Version 6.0 of pyVmomi has some problems with SSL error handling on certain
16     versions of Python. If using version 6.0 of pyVmomi, Python 2.7.9,
17     or newer must be present. This is due to an upstream dependency
18     in pyVmomi 6.0 that is not supported in Python versions 2.7 to 2.7.8. If the
19     version of Python is not in the supported range, you will need to install an
20     earlier version of pyVmomi. See `Issue #29537`_ for more information.
21 .. _Issue #29537: https://github.com/saltstack/salt/issues/29537
22 Based on the note above, to install an earlier version of pyVmomi than the
23 version currently listed in PyPi, run the following:
24 .. code-block:: bash
25     pip install pyVmomi==5.5.0.2014.1.1
26 The 5.5.0.2014.1.1 is a known stable version that this original vSphere Execution
27 Module was developed against.
28 vSphere Automation SDK
29 ----------------------
30 vSphere Automation SDK can be installed via pip:
31 .. code-block:: bash
32     pip install --upgrade pip setuptools
33     pip install --upgrade git+https://github.com/vmware/vsphere-automation-sdk-python.git
34 .. note::
35     The SDK also requires OpenSSL 1.0.1+ if you want to connect to vSphere 6.5+ in order to support
36     TLS1.1 &amp; 1.2.
37     In order to use the tagging functions in this module, vSphere Automation SDK is necessary to
38     install.
39 The module is currently in version 1.0.3
40 (as of 8/26/2019)
41 ESXCLI
42 ------
43 Currently, about a third of the functions used in the vSphere Execution Module require
44 the ESXCLI package be installed on the machine running the Proxy Minion process.
45 The ESXCLI package is also referred to as the VMware vSphere CLI, or vCLI. VMware
46 provides vCLI package installation instructions for `vSphere 5.5`_ and
47 `vSphere 6.0`_.
48 .. _vSphere 5.5: http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.vcli.getstart.doc/cli_install.4.2.html
49 .. _vSphere 6.0: http://pubs.vmware.com/vsphere-60/index.jsp#com.vmware.vcli.getstart.doc/cli_install.4.2.html
50 Once all of the required dependencies are in place and the vCLI package is
51 installed, you can check to see if you can connect to your ESXi host or vCenter
52 server by running the following command:
53 .. code-block:: bash
54     esxcli -s &lt;host-location&gt; -u &lt;username&gt; -p &lt;password&gt; system syslog config get
55 If the connection was successful, ESXCLI was successfully installed on your system.
56 You should see output related to the ESXi host's syslog configuration.
57 .. note::
58     Be aware that some functionality in this execution module may depend on the
59     type of license attached to a vCenter Server or ESXi host(s).
60     For example, certain services are only available to manipulate service state
61     or policies with a VMware vSphere Enterprise or Enterprise Plus license, while
62     others are available with a Standard license. The ``ntpd`` service is restricted
63     to an Enterprise Plus license, while ``ssh`` is available via the Standard
64     license.
65     Please see the `vSphere Comparison`_ page for more information.
66 .. _vSphere Comparison: https://www.vmware.com/products/vsphere/compare
67 About
68 =====
69 This execution module was designed to be able to handle connections both to a
70 vCenter Server, as well as to an ESXi host. It utilizes the pyVmomi Python
71 library and the ESXCLI package to run remote execution functions against either
72 the defined vCenter server or the ESXi host.
73 Whether or not the function runs against a vCenter Server or an ESXi host depends
74 entirely upon the arguments passed into the function. Each function requires a
75 ``host`` location, ``username``, and ``password``. If the credentials provided
76 apply to a vCenter Server, then the function will be run against the vCenter
77 Server. For example, when listing hosts using vCenter credentials, you'll get a
78 list of hosts associated with that vCenter Server:
79 .. code-block:: bash
80     my-minion:
81     - esxi-1.example.com
82     - esxi-2.example.com
83 However, some functions should be used against ESXi hosts, not vCenter Servers.
84 Functionality such as getting a host's coredump network configuration should be
85 performed against a host and not a vCenter server. If the authentication
86 information you're using is against a vCenter server and not an ESXi host, you
87 can provide the host name that is associated with the vCenter server in the
88 command, as a list, using the ``host_names`` or ``esxi_host`` kwarg. For
89 example:
90 .. code-block:: bash
91         &lt;vcenter-password&gt; esxi_hosts='[esxi-1.example.com, esxi-2.example.com]'
92     my-minion:
93     ----------
94         esxi-1.example.com:
95             ----------
96             Coredump Config:
97                 ----------
98                 enabled:
99                     False
100         esxi-2.example.com:
101             ----------
102             Coredump Config:
103                 ----------
104                 enabled:
105                     True
106                 host_vnic:
107                     vmk0
108                 ip:
109                     coredump-location.example.com
110                 port:
111                     6500
112 You can also use these functions against an ESXi host directly by establishing a
113 connection to an ESXi host using the host's location, username, and password. If ESXi
114 connection credentials are used instead of vCenter credentials, the ``host_names`` and
115 ``esxi_hosts`` arguments are not needed.
116 .. code-block:: bash
117     local:
118     ----------
119         10.4.28.150:
120             ----------
121             Coredump Config:
122                 ----------
123                 enabled:
124                     True
125                 host_vnic:
126                     vmk0
127                 ip:
128                     coredump-location.example.com
129                 port:
130                     6500
131 """
132 import datetime
133 import logging
134 import sys
135 from functools import wraps
136 import salt.utils.args
137 import salt.utils.dictupdate as dictupdate
138 import salt.utils.path
139 import salt.utils.pbm
140 <font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>import salt.utils.vmware
141 import salt.utils.vsan
142 from salt.config.schemas.esxcluster import (
143     ESXClusterConfigSchema,
144     ESXClusterEntitySchema,
145 )
146 from salt.config.schemas.esxi import (
147     DiskGroupsDiskIdSchema,
148     SimpleHostCacheSchema,
149     VmfsDatastoreSchema,
150 )
151 from salt.config.schemas.esxvm import (
152     ESXVirtualMachineDeleteSchema,
153     ESXVirtualMachineUnregisterSchema,
154 )
155 from salt.config.schemas.vcenter import VCenterEntitySchema
156 from salt.exceptions import (
157     ArgumentValueError,
158     CommandExecutionError,
159     InvalidConfigError,
160     InvalidEntityError,
161     VMwareApiError,
162     VMwareObjectExistsError,
163     VMwareObjectRetrievalError,
164     VMwareSaltError,
165 )
166 from salt.utils.decorators import depends, ignores_kwargs
167 from salt.utils.dictdiffer import recursive_diff
168 from salt.utils.listdiffer import list_diff
169 log = logging.getLogger(</b></font>__name__)
170 try:
171     import jsonschema
172     HAS_JSONSCHEMA = True
173 except ImportError:
174     HAS_JSONSCHEMA = False
175 try:
176     from pyVmomi import (
177         vim,
178         vmodl,
179         pbm,
180         VmomiSupport,
181     )
182     if (
183         "vim25/6.0" in VmomiSupport.versionMap
184         and sys.version_info &gt; (2, 7)
185         and sys.version_info &lt; (2, 7, 9)
186     ):
187         log.debug(
188             "pyVmomi not loaded: Incompatible versions of Python. See Issue #29537."
189         )
190         raise ImportError()
191     HAS_PYVMOMI = True
192 except ImportError:
193     HAS_PYVMOMI = False
194 try:
195     from com.vmware.cis.tagging_client import Category, CategoryModel
196     from com.vmware.cis.tagging_client import Tag, TagModel, TagAssociation
197     from com.vmware.vcenter_client import Cluster
198     from com.vmware.vapi.std_client import DynamicID
199     from com.vmware.vapi.std.errors_client import (
200         AlreadyExists,
201         InvalidArgument,
202         NotFound,
203         Unauthenticated,
204         Unauthorized,
205     )
206     vsphere_errors = (
207         AlreadyExists,
208         InvalidArgument,
209         NotFound,
210         Unauthenticated,
211         Unauthorized,
212     )
213     HAS_VSPHERE_SDK = True
214 except ImportError:
215     HAS_VSPHERE_SDK = False
216 esx_cli = salt.utils.path.which("esxcli")
217 if esx_cli:
218     HAS_ESX_CLI = True
219 else:
220     HAS_ESX_CLI = False
221 __virtualname__ = "vsphere"
222 __proxyenabled__ = ["esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm"]
223 def __virtual__():
224     return __virtualname__
225 def get_proxy_type():
226     """
227     Returns the proxy type retrieved either from the pillar of from the proxy
228     minion's config.  Returns ``&lt;undefined&gt;`` otherwise.
229     CLI Example:
230     .. code-block:: bash
231         salt '*' vsphere.get_proxy_type
232     """
233     if __pillar__.get("proxy", {}).get("proxytype"):
234         return __pillar__["proxy"]["proxytype"]
235     if __opts__.get("proxy", {}).get("proxytype"):
236         return __opts__["proxy"]["proxytype"]
237     return "&lt;undefined&gt;"
238 def _get_proxy_connection_details():
239     """
240     Returns the connection details of the following proxies: esxi
241     """
242     proxytype = get_proxy_type()
243     if proxytype == "esxi":
244         details = __salt__["esxi.get_details"]()
245     elif proxytype == "esxcluster":
246         details = __salt__["esxcluster.get_details"]()
247     elif proxytype == "esxdatacenter":
248         details = __salt__["esxdatacenter.get_details"]()
249     elif proxytype == "vcenter":
250         details = __salt__["vcenter.get_details"]()
251     elif proxytype == "esxvm":
252         details = __salt__["esxvm.get_details"]()
253     else:
254         raise CommandExecutionError("'{}' proxy is not supported".format(proxytype))
255     proxy_details = [
256         details.get("vcenter") if "vcenter" in details else details.get("host"),
257         details.get("username"),
258         details.get("password"),
259         details.get("protocol"),
260         details.get("port"),
261         details.get("mechanism"),
262         details.get("principal"),
263         details.get("domain"),
264     ]
265     if "verify_ssl" in details:
266         proxy_details.append(details.get("verify_ssl"))
267     return tuple(proxy_details)
268 def _supports_proxies(*proxy_types):
269     """
270     Decorator to specify which proxy types are supported by a function
271     proxy_types:
272         Arbitrary list of strings with the supported types of proxies
273     """
274     def _supports_proxies_(fn):
275         @wraps(fn)
276         def __supports_proxies_(*args, **kwargs):
277             proxy_type = get_proxy_type()
278             if proxy_type not in proxy_types:
279                 raise CommandExecutionError(
280                     "'{}' proxy is not supported by function {}".format(
281                         proxy_type, fn.__name__
282                     )
283                 )
284             return fn(*args, **salt.utils.args.clean_kwargs(**kwargs))
285         return __supports_proxies_
286     return _supports_proxies_
287 def _gets_service_instance_via_proxy(fn):
288     """
289     Decorator that connects to a target system (vCenter or ESXi host) using the
290     proxy details and passes the connection (vim.ServiceInstance) to
291     the decorated function.
292     Supported proxies: esxi, esxcluster, esxdatacenter.
293     Notes:
294         1. The decorated function must have a ``service_instance`` parameter
295         or a ``**kwarg`` type argument (name of argument is not important);
296         2. If the ``service_instance`` parameter is already defined, the value
297         is passed through to the decorated function;
298         3. If the ``service_instance`` parameter in not defined, the
299         connection is created using the proxy details and the service instance
300         is returned.
301     """
302     fn_name = fn.__name__
303     (
304         arg_names,
305         args_name,
306         kwargs_name,
307         default_values,
308     ) = salt.utils.args.get_function_argspec(fn)
309     default_values = default_values if default_values is not None else []
310     @wraps(fn)
311     def _gets_service_instance_via_proxy_(*args, **kwargs):
312         if "service_instance" not in arg_names and not kwargs_name:
313             raise CommandExecutionError(
314                 "Function {} must have either a 'service_instance', or a "
315                 "'**kwargs' type parameter".format(fn_name)
316             )
317         connection_details = _get_proxy_connection_details()
318         local_service_instance = None
319         if "service_instance" in arg_names:
320             idx = arg_names.index("service_instance")
321             if idx &gt;= len(arg_names) - len(default_values):
322                 if len(args) &gt; idx:
323                     if not args[idx]:
324                         local_service_instance = salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
325                             *connection_details
326                         )
327                         args = list(args)
328                         args[idx] = local_service_instance
329                 else:
330                     if not kwargs.get("service_instance"):
331                         local_service_instance = salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
332                             *connection_details
333                         )
334                         kwargs["service_instance"] = local_service_instance
335         else:
336             if not kwargs.get("service_instance"):
337                 local_service_instance = salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
338                     *connection_details
339                 )
340                 kwargs["service_instance"] = local_service_instance
341         try:
342             ret = fn(*args, **salt.utils.args.clean_kwargs(**kwargs))
343             if local_service_instance:
344                 salt.utils.vmware.disconnect(local_service_instance)
345             return ret
346         except Exception as e:  # pylint: disable=broad-except
347             if local_service_instance:
348                 salt.utils.vmware.disconnect(local_service_instance)
349             raise
350     return _gets_service_instance_via_proxy_
351 @depends(HAS_PYVMOMI)
352 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm")
353 def get_service_instance_via_proxy(service_instance=None):
354     """
355     Returns a service instance to the proxied endpoint (vCenter/ESXi host).
356     service_instance
357         Service instance (vim.ServiceInstance) of the vCenter.
358         Default is None.
359     Note:
360         Should be used by state functions not invoked directly.
361     CLI Example:
362         See note above
363     """
364     connection_details = _get_proxy_connection_details()
365     return salt.utils.vmware.get_service_instance(  # pylint: disable=no-value-for-parameter
366         *connection_details
367     )
368 @depends(HAS_PYVMOMI)
369 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm")
370 def disconnect(service_instance):
371     """
372     Disconnects from a vCenter or ESXi host
373     Note:
374         Should be used by state functions, not invoked directly.
375     service_instance
376         Service instance (vim.ServiceInstance)
377     CLI Example:
378         See note above.
379     """
380     salt.utils.vmware.disconnect(service_instance)
381     return True
382 @depends(HAS_ESX_CLI)
383 def esxcli_cmd(
384     cmd_str,
385     host=None,
386     username=None,
387     password=None,
388     protocol=None,
389     port=None,
390     esxi_hosts=None,
391     credstore=None,
392 ):
393     """
394     Run an ESXCLI command directly on the host or list of hosts.
395     host
396         The location of the host.
397     username
398         The username used to login to the host, such as ``root``.
399     password
400         The password used to login to the host.
401     cmd_str
402         The ESXCLI command to run. Note: This should not include the ``-s``, ``-u``,
403         ``-p``, ``-h``, ``--protocol``, or ``--portnumber`` arguments that are
404         frequently passed when using a bare ESXCLI command from the command line.
405         Those arguments are handled by this function via the other args and kwargs.
406     protocol
407         Optionally set to alternate protocol if the host is not using the default
408         protocol. Default protocol is ``https``.
409     port
410         Optionally set to alternate port if the host is not using the default
411         port. Default port is ``443``.
412     esxi_hosts
413         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
414         on a list of one or more ESXi machines.
415     credstore
416         Optionally set to path to the credential store file.
417     CLI Example:
418     .. code-block:: bash
419         salt '*' vsphere.esxcli_cmd my.esxi.host root bad-password \
420             'system coredump network get'
421         salt '*' vsphere.esxcli_cmd my.vcenter.location root bad-password \
422             'system coredump network get' esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
423     """
424     ret = {}
425     if esxi_hosts:
426         if not isinstance(esxi_hosts, list):
427             raise CommandExecutionError("'esxi_hosts' must be a list.")
428         for esxi_host in esxi_hosts:
429             response = salt.utils.vmware.esxcli(
430                 host,
431                 username,
432                 password,
433                 cmd_str,
434                 protocol=protocol,
435                 port=port,
436                 esxi_host=esxi_host,
437                 credstore=credstore,
438             )
439             if response["retcode"] != 0:
440                 ret.update({esxi_host: {"Error": response.get("stdout")}})
441             else:
442                 ret.update({esxi_host: response})
443     else:
444         response = salt.utils.vmware.esxcli(
445             host,
446             username,
447             password,
448             cmd_str,
449             protocol=protocol,
450             port=port,
451             credstore=credstore,
452         )
453         if response["retcode"] != 0:
454             ret.update({host: {"Error": response.get("stdout")}})
455         else:
456             ret.update({host: response})
457     return ret
458 @depends(HAS_ESX_CLI)
459 def get_coredump_network_config(
460     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
461 ):
462     """
463     Retrieve information on ESXi or vCenter network dump collection and
464     format it into a dictionary.
465     host
466         The location of the host.
467     username
468         The username used to login to the host, such as ``root``.
469     password
470         The password used to login to the host.
471     protocol
472         Optionally set to alternate protocol if the host is not using the default
473         protocol. Default protocol is ``https``.
474     port
475         Optionally set to alternate port if the host is not using the default
476         port. Default port is ``443``.
477     esxi_hosts
478         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
479         on a list of one or more ESXi machines.
480     credstore
481         Optionally set to path to the credential store file.
482     :return: A dictionary with the network configuration, or, if getting
483              the network config failed, a an error message retrieved from the
484              standard cmd.run_all dictionary, per host.
485     CLI Example:
486     .. code-block:: bash
487         salt '*' vsphere.get_coredump_network_config my.esxi.host root bad-password
488         salt '*' vsphere.get_coredump_network_config my.vcenter.location root bad-password \
489             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
490     """
491     cmd = "system coredump network get"
492     ret = {}
493     if esxi_hosts:
494         if not isinstance(esxi_hosts, list):
495             raise CommandExecutionError("'esxi_hosts' must be a list.")
496         for esxi_host in esxi_hosts:
497             response = salt.utils.vmware.esxcli(
498                 host,
499                 username,
500                 password,
501                 cmd,
502                 protocol=protocol,
503                 port=port,
504                 esxi_host=esxi_host,
505                 credstore=credstore,
506             )
507             if response["retcode"] != 0:
508                 ret.update({esxi_host: {"Error": response.get("stdout")}})
509             else:
510                 ret.update(
511                     {esxi_host: {"Coredump Config": _format_coredump_stdout(response)}}
512                 )
513     else:
514         response = salt.utils.vmware.esxcli(
515             host,
516             username,
517             password,
518             cmd,
519             protocol=protocol,
520             port=port,
521             credstore=credstore,
522         )
523         if response["retcode"] != 0:
524             ret.update({host: {"Error": response.get("stdout")}})
525         else:
526             stdout = _format_coredump_stdout(response)
527             ret.update({host: {"Coredump Config": stdout}})
528     return ret
529 @depends(HAS_ESX_CLI)
530 def coredump_network_enable(
531     host,
532     username,
533     password,
534     enabled,
535     protocol=None,
536     port=None,
537     esxi_hosts=None,
538     credstore=None,
539 ):
540     """
541     Enable or disable ESXi core dump collection. Returns ``True`` if coredump is enabled
542     and returns ``False`` if core dump is not enabled. If there was an error, the error
543     will be the value printed in the ``Error`` key dictionary for the given host.
544     host
545         The location of the host.
546     username
547         The username used to login to the host, such as ``root``.
548     password
549         The password used to login to the host.
550     enabled
551         Python True or False to enable or disable coredumps.
552     protocol
553         Optionally set to alternate protocol if the host is not using the default
554         protocol. Default protocol is ``https``.
555     port
556         Optionally set to alternate port if the host is not using the default
557         port. Default port is ``443``.
558     esxi_hosts
559         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
560         on a list of one or more ESXi machines.
561     credstore
562         Optionally set to path to the credential store file.
563     CLI Example:
564     .. code-block:: bash
565         salt '*' vsphere.coredump_network_enable my.esxi.host root bad-password True
566         salt '*' vsphere.coredump_network_enable my.vcenter.location root bad-password True \
567             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
568     """
569     if enabled:
570         enable_it = 1
571     else:
572         enable_it = 0
573     cmd = "system coredump network set -e {}".format(enable_it)
574     ret = {}
575     if esxi_hosts:
576         if not isinstance(esxi_hosts, list):
577             raise CommandExecutionError("'esxi_hosts' must be a list.")
578         for esxi_host in esxi_hosts:
579             response = salt.utils.vmware.esxcli(
580                 host,
581                 username,
582                 password,
583                 cmd,
584                 protocol=protocol,
585                 port=port,
586                 esxi_host=esxi_host,
587                 credstore=credstore,
588             )
589             if response["retcode"] != 0:
590                 ret.update({esxi_host: {"Error": response.get("stdout")}})
591             else:
592                 ret.update({esxi_host: {"Coredump Enabled": enabled}})
593     else:
594         response = salt.utils.vmware.esxcli(
595             host,
596             username,
597             password,
598             cmd,
599             protocol=protocol,
600             port=port,
601             credstore=credstore,
602         )
603         if response["retcode"] != 0:
604             ret.update({host: {"Error": response.get("stdout")}})
605         else:
606             ret.update({host: {"Coredump Enabled": enabled}})
607     return ret
608 @depends(HAS_ESX_CLI)
609 def set_coredump_network_config(
610     host,
611     username,
612     password,
613     dump_ip,
614     protocol=None,
615     port=None,
616     host_vnic="vmk0",
617     dump_port=6500,
618     esxi_hosts=None,
619     credstore=None,
620 ):
621     """
622     Set the network parameters for a network coredump collection.
623     Note that ESXi requires that the dumps first be enabled (see
624     `coredump_network_enable`) before these parameters may be set.
625     host
626         The location of the host.
627     username
628         The username used to login to the host, such as ``root``.
629     password
630         The password used to login to the host.
631     dump_ip
632         IP address of host that will accept the dump.
633     protocol
634         Optionally set to alternate protocol if the host is not using the default
635         protocol. Default protocol is ``https``.
636     port
637         Optionally set to alternate port if the host is not using the default
638         port. Default port is ``443``.
639     esxi_hosts
640         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
641         on a list of one or more ESXi machines.
642     host_vnic
643         Host VNic port through which to communicate. Defaults to ``vmk0``.
644     dump_port
645         TCP port to use for the dump, defaults to ``6500``.
646     credstore
647         Optionally set to path to the credential store file.
648     :return: A standard cmd.run_all dictionary with a `success` key added, per host.
649              `success` will be True if the set succeeded, False otherwise.
650     CLI Example:
651     .. code-block:: bash
652         salt '*' vsphere.set_coredump_network_config my.esxi.host root bad-password 'dump_ip.host.com'
653         salt '*' vsphere.set_coredump_network_config my.vcenter.location root bad-password 'dump_ip.host.com' \
654             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
655     """
656     cmd = "system coredump network set -v {} -i {} -o {}".format(
657         host_vnic, dump_ip, dump_port
658     )
659     ret = {}
660     if esxi_hosts:
661         if not isinstance(esxi_hosts, list):
662             raise CommandExecutionError("'esxi_hosts' must be a list.")
663         for esxi_host in esxi_hosts:
664             response = salt.utils.vmware.esxcli(
665                 host,
666                 username,
667                 password,
668                 cmd,
669                 protocol=protocol,
670                 port=port,
671                 esxi_host=esxi_host,
672                 credstore=credstore,
673             )
674             if response["retcode"] != 0:
675                 response["success"] = False
676             else:
677                 response["success"] = True
678             ret.update({esxi_host: response})
679     else:
680         response = salt.utils.vmware.esxcli(
681             host,
682             username,
683             password,
684             cmd,
685             protocol=protocol,
686             port=port,
687             credstore=credstore,
688         )
689         if response["retcode"] != 0:
690             response["success"] = False
691         else:
692             response["success"] = True
693         ret.update({host: response})
694     return ret
695 @depends(HAS_ESX_CLI)
696 def get_firewall_status(
697     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
698 ):
699     """
700     Show status of all firewall rule sets.
701     host
702         The location of the host.
703     username
704         The username used to login to the host, such as ``root``.
705     password
706         The password used to login to the host.
707     protocol
708         Optionally set to alternate protocol if the host is not using the default
709         protocol. Default protocol is ``https``.
710     port
711         Optionally set to alternate port if the host is not using the default
712         port. Default port is ``443``.
713     esxi_hosts
714         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
715         on a list of one or more ESXi machines.
716     credstore
717         Optionally set to path to the credential store file.
718     :return: Nested dictionary with two toplevel keys ``rulesets`` and ``success``
719              ``success`` will be True or False depending on query success
720              ``rulesets`` will list the rulesets and their statuses if ``success``
721              was true, per host.
722     CLI Example:
723     .. code-block:: bash
724         salt '*' vsphere.get_firewall_status my.esxi.host root bad-password
725         salt '*' vsphere.get_firewall_status my.vcenter.location root bad-password \
726             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
727     """
728     cmd = "network firewall ruleset list"
729     ret = {}
730     if esxi_hosts:
731         if not isinstance(esxi_hosts, list):
732             raise CommandExecutionError("'esxi_hosts' must be a list.")
733         for esxi_host in esxi_hosts:
734             response = salt.utils.vmware.esxcli(
735                 host,
736                 username,
737                 password,
738                 cmd,
739                 protocol=protocol,
740                 port=port,
741                 esxi_host=esxi_host,
742                 credstore=credstore,
743             )
744             if response["retcode"] != 0:
745                 ret.update(
746                     {
747                         esxi_host: {
748                             "Error": response["stdout"],
749                             "success": False,
750                             "rulesets": None,
751                         }
752                     }
753                 )
754             else:
755                 ret.update({esxi_host: _format_firewall_stdout(response)})
756     else:
757         response = salt.utils.vmware.esxcli(
758             host,
759             username,
760             password,
761             cmd,
762             protocol=protocol,
763             port=port,
764             credstore=credstore,
765         )
766         if response["retcode"] != 0:
767             ret.update(
768                 {
769                     host: {
770                         "Error": response["stdout"],
771                         "success": False,
772                         "rulesets": None,
773                     }
774                 }
775             )
776         else:
777             ret.update({host: _format_firewall_stdout(response)})
778     return ret
779 @depends(HAS_ESX_CLI)
780 def enable_firewall_ruleset(
781     host,
782     username,
783     password,
784     ruleset_enable,
785     ruleset_name,
786     protocol=None,
787     port=None,
788     esxi_hosts=None,
789     credstore=None,
790 ):
791     """
792     Enable or disable an ESXi firewall rule set.
793     host
794         The location of the host.
795     username
796         The username used to login to the host, such as ``root``.
797     password
798         The password used to login to the host.
799     ruleset_enable
800         True to enable the ruleset, false to disable.
801     ruleset_name
802         Name of ruleset to target.
803     protocol
804         Optionally set to alternate protocol if the host is not using the default
805         protocol. Default protocol is ``https``.
806     port
807         Optionally set to alternate port if the host is not using the default
808         port. Default port is ``443``.
809     esxi_hosts
810         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
811         on a list of one or more ESXi machines.
812     credstore
813         Optionally set to path to the credential store file.
814     :return: A standard cmd.run_all dictionary, per host.
815     CLI Example:
816     .. code-block:: bash
817         salt '*' vsphere.enable_firewall_ruleset my.esxi.host root bad-password True 'syslog'
818         salt '*' vsphere.enable_firewall_ruleset my.vcenter.location root bad-password True 'syslog' \
819             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
820     """
821     cmd = "network firewall ruleset set --enabled {} --ruleset-id={}".format(
822         ruleset_enable, ruleset_name
823     )
824     ret = {}
825     if esxi_hosts:
826         if not isinstance(esxi_hosts, list):
827             raise CommandExecutionError("'esxi_hosts' must be a list.")
828         for esxi_host in esxi_hosts:
829             response = salt.utils.vmware.esxcli(
830                 host,
831                 username,
832                 password,
833                 cmd,
834                 protocol=protocol,
835                 port=port,
836                 esxi_host=esxi_host,
837                 credstore=credstore,
838             )
839             ret.update({esxi_host: response})
840     else:
841         response = salt.utils.vmware.esxcli(
842             host,
843             username,
844             password,
845             cmd,
846             protocol=protocol,
847             port=port,
848             credstore=credstore,
849         )
850         ret.update({host: response})
851     return ret
852 @depends(HAS_ESX_CLI)
853 def syslog_service_reload(
854     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
855 ):
856     """
857     Reload the syslog service so it will pick up any changes.
858     host
859         The location of the host.
860     username
861         The username used to login to the host, such as ``root``.
862     password
863         The password used to login to the host.
864     protocol
865         Optionally set to alternate protocol if the host is not using the default
866         protocol. Default protocol is ``https``.
867     port
868         Optionally set to alternate port if the host is not using the default
869         port. Default port is ``443``.
870     esxi_hosts
871         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
872         on a list of one or more ESXi machines.
873     credstore
874         Optionally set to path to the credential store file.
875     :return: A standard cmd.run_all dictionary.  This dictionary will at least
876              have a `retcode` key.  If `retcode` is 0 the command was successful.
877     CLI Example:
878     .. code-block:: bash
879         salt '*' vsphere.syslog_service_reload my.esxi.host root bad-password
880         salt '*' vsphere.syslog_service_reload my.vcenter.location root bad-password \
881             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
882     """
883     cmd = "system syslog reload"
884     ret = {}
885     if esxi_hosts:
886         if not isinstance(esxi_hosts, list):
887             raise CommandExecutionError("'esxi_hosts' must be a list.")
888         for esxi_host in esxi_hosts:
889             response = salt.utils.vmware.esxcli(
890                 host,
891                 username,
892                 password,
893                 cmd,
894                 protocol=protocol,
895                 port=port,
896                 esxi_host=esxi_host,
897                 credstore=credstore,
898             )
899             ret.update({esxi_host: response})
900     else:
901         response = salt.utils.vmware.esxcli(
902             host,
903             username,
904             password,
905             cmd,
906             protocol=protocol,
907             port=port,
908             credstore=credstore,
909         )
910         ret.update({host: response})
911     return ret
912 @depends(HAS_ESX_CLI)
913 def set_syslog_config(
914     host,
915     username,
916     password,
917     syslog_config,
918     config_value,
919     protocol=None,
920     port=None,
921     firewall=True,
922     reset_service=True,
923     esxi_hosts=None,
924     credstore=None,
925 ):
926     """
927     Set the specified syslog configuration parameter. By default, this function will
928     reset the syslog service after the configuration is set.
929     host
930         ESXi or vCenter host to connect to.
931     username
932         User to connect as, usually root.
933     password
934         Password to connect with.
935     syslog_config
936         Name of parameter to set (corresponds to the command line switch for
937         esxcli without the double dashes (--))
938         Valid syslog_config values are ``logdir``, ``loghost``, ``default-rotate`,
939         ``default-size``, ``default-timeout``, and ``logdir-unique``.
940     config_value
941         Value for the above parameter. For ``loghost``, URLs or IP addresses to
942         use for logging. Multiple log servers can be specified by listing them,
943         comma-separated, but without spaces before or after commas.
944         (reference: https://blogs.vmware.com/vsphere/2012/04/configuring-multiple-syslog-servers-for-esxi-5.html)
945     protocol
946         Optionally set to alternate protocol if the host is not using the default
947         protocol. Default protocol is ``https``.
948     port
949         Optionally set to alternate port if the host is not using the default
950         port. Default port is ``443``.
951     firewall
952         Enable the firewall rule set for syslog. Defaults to ``True``.
953     reset_service
954         After a successful parameter set, reset the service. Defaults to ``True``.
955     esxi_hosts
956         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
957         on a list of one or more ESXi machines.
958     credstore
959         Optionally set to path to the credential store file.
960     :return: Dictionary with a top-level key of 'success' which indicates
961              if all the parameters were reset, and individual keys
962              for each parameter indicating which succeeded or failed, per host.
963     CLI Example:
964     .. code-block:: bash
965         salt '*' vsphere.set_syslog_config my.esxi.host root bad-password \
966             loghost ssl://localhost:5432,tcp://10.1.0.1:1514
967         salt '*' vsphere.set_syslog_config my.vcenter.location root bad-password \
968             loghost ssl://localhost:5432,tcp://10.1.0.1:1514 \
969             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
970     """
971     ret = {}
972     if firewall and syslog_config == "loghost":
973         if esxi_hosts:
974             if not isinstance(esxi_hosts, list):
975                 raise CommandExecutionError("'esxi_hosts' must be a list.")
976             for esxi_host in esxi_hosts:
977                 response = enable_firewall_ruleset(
978                     host,
979                     username,
980                     password,
981                     ruleset_enable=True,
982                     ruleset_name="syslog",
983                     protocol=protocol,
984                     port=port,
985                     esxi_hosts=[esxi_host],
986                     credstore=credstore,
987                 ).get(esxi_host)
988                 if response["retcode"] != 0:
989                     ret.update(
990                         {
991                             esxi_host: {
992                                 "enable_firewall": {
993                                     "message": response["stdout"],
994                                     "success": False,
995                                 }
996                             }
997                         }
998                     )
999                 else:
1000                     ret.update({esxi_host: {"enable_firewall": {"success": True}}})
1001         else:
1002             response = enable_firewall_ruleset(
1003                 host,
1004                 username,
1005                 password,
1006                 ruleset_enable=True,
1007                 ruleset_name="syslog",
1008                 protocol=protocol,
1009                 port=port,
1010                 credstore=credstore,
1011             ).get(host)
1012             if response["retcode"] != 0:
1013                 ret.update(
1014                     {
1015                         host: {
1016                             "enable_firewall": {
1017                                 "message": response["stdout"],
1018                                 "success": False,
1019                             }
1020                         }
1021                     }
1022                 )
1023             else:
1024                 ret.update({host: {"enable_firewall": {"success": True}}})
1025     if esxi_hosts:
1026         if not isinstance(esxi_hosts, list):
1027             raise CommandExecutionError("'esxi_hosts' must be a list.")
1028         for esxi_host in esxi_hosts:
1029             response = _set_syslog_config_helper(
1030                 host,
1031                 username,
1032                 password,
1033                 syslog_config,
1034                 config_value,
1035                 protocol=protocol,
1036                 port=port,
1037                 reset_service=reset_service,
1038                 esxi_host=esxi_host,
1039                 credstore=credstore,
1040             )
1041             if ret.get(esxi_host) is None:
1042                 ret.update({esxi_host: {}})
1043             ret[esxi_host].update(response)
1044     else:
1045         response = _set_syslog_config_helper(
1046             host,
1047             username,
1048             password,
1049             syslog_config,
1050             config_value,
1051             protocol=protocol,
1052             port=port,
1053             reset_service=reset_service,
1054             credstore=credstore,
1055         )
1056         if ret.get(host) is None:
1057             ret.update({host: {}})
1058         ret[host].update(response)
1059     return ret
1060 @depends(HAS_ESX_CLI)
1061 def get_syslog_config(
1062     host, username, password, protocol=None, port=None, esxi_hosts=None, credstore=None
1063 ):
1064     """
1065     Retrieve the syslog configuration.
1066     host
1067         The location of the host.
1068     username
1069         The username used to login to the host, such as ``root``.
1070     password
1071         The password used to login to the host.
1072     protocol
1073         Optionally set to alternate protocol if the host is not using the default
1074         protocol. Default protocol is ``https``.
1075     port
1076         Optionally set to alternate port if the host is not using the default
1077         port. Default port is ``443``.
1078     esxi_hosts
1079         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
1080         on a list of one or more ESXi machines.
1081     credstore
1082         Optionally set to path to the credential store file.
1083     :return: Dictionary with keys and values corresponding to the
1084              syslog configuration, per host.
1085     CLI Example:
1086     .. code-block:: bash
1087         salt '*' vsphere.get_syslog_config my.esxi.host root bad-password
1088         salt '*' vsphere.get_syslog_config my.vcenter.location root bad-password \
1089             esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
1090     """
1091     cmd = "system syslog config get"
1092     ret = {}
1093     if esxi_hosts:
1094         if not isinstance(esxi_hosts, list):
1095             raise CommandExecutionError("'esxi_hosts' must be a list.")
1096         for esxi_host in esxi_hosts:
1097             response = salt.utils.vmware.esxcli(
1098                 host,
1099                 username,
1100                 password,
1101                 cmd,
1102                 protocol=protocol,
1103                 port=port,
1104                 esxi_host=esxi_host,
1105                 credstore=credstore,
1106             )
1107             ret.update({esxi_host: _format_syslog_config(response)})
1108     else:
1109         response = salt.utils.vmware.esxcli(
1110             host,
1111             username,
1112             password,
1113             cmd,
1114             protocol=protocol,
1115             port=port,
1116             credstore=credstore,
1117         )
1118         ret.update({host: _format_syslog_config(response)})
1119     return ret
1120 @depends(HAS_ESX_CLI)
1121 def reset_syslog_config(
1122     host,
1123     username,
1124     password,
1125     protocol=None,
1126     port=None,
1127     syslog_config=None,
1128     esxi_hosts=None,
1129     credstore=None,
1130 ):
1131     """
1132     Reset the syslog service to its default settings.
1133     Valid syslog_config values are ``logdir``, ``loghost``, ``logdir-unique``,
1134     ``default-rotate``, ``default-size``, ``default-timeout``,
1135     or ``all`` for all of these.
1136     host
1137         The location of the host.
1138     username
1139         The username used to login to the host, such as ``root``.
1140     password
1141         The password used to login to the host.
1142     protocol
1143         Optionally set to alternate protocol if the host is not using the default
1144         protocol. Default protocol is ``https``.
1145     port
1146         Optionally set to alternate port if the host is not using the default
1147         port. Default port is ``443``.
1148     syslog_config
1149         List of parameters to reset, provided as a comma-delimited string, or 'all' to
1150         reset all syslog configuration parameters. Required.
1151     esxi_hosts
1152         If ``host`` is a vCenter host, then use esxi_hosts to execute this function
1153         on a list of one or more ESXi machines.
1154     credstore
1155         Optionally set to path to the credential store file.
1156     :return: Dictionary with a top-level key of 'success' which indicates
1157              if all the parameters were reset, and individual keys
1158              for each parameter indicating which succeeded or failed, per host.
1159     CLI Example:
1160     ``syslog_config`` can be passed as a quoted, comma-separated string, e.g.
1161     .. code-block:: bash
1162         salt '*' vsphere.reset_syslog_config my.esxi.host root bad-password \
1163             syslog_config='logdir,loghost'
1164         salt '*' vsphere.reset_syslog_config my.vcenter.location root bad-password \
1165             syslog_config='logdir,loghost' esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
1166     """
1167     if not syslog_config:
1168         raise CommandExecutionError(
1169             "The 'reset_syslog_config' function requires a 'syslog_config' setting."
1170         )
1171     valid_resets = [
1172         "logdir",
1173         "loghost",
1174         "default-rotate",
1175         "default-size",
1176         "default-timeout",
1177         "logdir-unique",
1178     ]
1179     cmd = "system syslog config set --reset="
1180     if "," in syslog_config:
1181         resets = [ind_reset.strip() for ind_reset in syslog_config.split(",")]
1182     elif syslog_config == "all":
1183         resets = valid_resets
1184     else:
1185         resets = [syslog_config]
1186     ret = {}
1187     if esxi_hosts:
1188         if not isinstance(esxi_hosts, list):
1189             raise CommandExecutionError("'esxi_hosts' must be a list.")
1190         for esxi_host in esxi_hosts:
1191             response_dict = _reset_syslog_config_params(
1192                 host,
1193                 username,
1194                 password,
1195                 cmd,
1196                 resets,
1197                 valid_resets,
1198                 protocol=protocol,
1199                 port=port,
1200                 esxi_host=esxi_host,
1201                 credstore=credstore,
1202             )
1203             ret.update({esxi_host: response_dict})
1204     else:
1205         response_dict = _reset_syslog_config_params(
1206             host,
1207             username,
1208             password,
1209             cmd,
1210             resets,
1211             valid_resets,
1212             protocol=protocol,
1213             port=port,
1214             credstore=credstore,
1215         )
1216         ret.update({host: response_dict})
1217     return ret
1218 @ignores_kwargs("credstore")
1219 def upload_ssh_key(
1220     host,
1221     username,
1222     password,
1223     ssh_key=None,
1224     ssh_key_file=None,
1225     protocol=None,
1226     port=None,
1227     certificate_verify=None,
1228 ):
1229     """
1230     Upload an ssh key for root to an ESXi host via http PUT.
1231     This function only works for ESXi, not vCenter.
1232     Only one ssh key can be uploaded for root.  Uploading a second key will
1233     replace any existing key.
1234     :param host: The location of the ESXi Host
1235     :param username: Username to connect as
1236     :param password: Password for the ESXi web endpoint
1237     :param ssh_key: Public SSH key, will be added to authorized_keys on ESXi
1238     :param ssh_key_file: File containing the SSH key.  Use 'ssh_key' or
1239                          ssh_key_file, but not both.
1240     :param protocol: defaults to https, can be http if ssl is disabled on ESXi
1241     :param port: defaults to 443 for https
1242     :param certificate_verify: If true require that the SSL connection present
1243                                a valid certificate. Default: True
1244     :return: Dictionary with a 'status' key, True if upload is successful.
1245              If upload is unsuccessful, 'status' key will be False and
1246              an 'Error' key will have an informative message.
1247     CLI Example:
1248     .. code-block:: bash
1249         salt '*' vsphere.upload_ssh_key my.esxi.host root bad-password ssh_key_file='/etc/salt/my_keys/my_key.pub'
1250     """
1251     if protocol is None:
1252         protocol = "https"
1253     if port is None:
1254         port = 443
1255     if certificate_verify is None:
1256         certificate_verify = True
1257     url = "{}://{}:{}/host/ssh_root_authorized_keys".format(protocol, host, port)
1258     ret = {}
1259     result = None
1260     try:
1261         if ssh_key:
1262             result = salt.utils.http.query(
1263                 url,
1264                 status=True,
1265                 text=True,
1266                 method="PUT",
1267                 username=username,
1268                 password=password,
1269                 data=ssh_key,
1270                 verify_ssl=certificate_verify,
1271             )
1272         elif ssh_key_file:
1273             result = salt.utils.http.query(
1274                 url,
1275                 status=True,
1276                 text=True,
1277                 method="PUT",
1278                 username=username,
1279                 password=password,
1280                 data_file=ssh_key_file,
1281                 data_render=False,
1282                 verify_ssl=certificate_verify,
1283             )
1284         if result.get("status") == 200:
1285             ret["status"] = True
1286         else:
1287             ret["status"] = False
1288             ret["Error"] = result["error"]
1289     except Exception as msg:  # pylint: disable=broad-except
1290         ret["status"] = False
1291         ret["Error"] = msg
1292     return ret
1293 @ignores_kwargs("credstore")
1294 def get_ssh_key(
1295     host, username, password, protocol=None, port=None, certificate_verify=None
1296 ):
1297     """
1298     Retrieve the authorized_keys entry for root.
1299     This function only works for ESXi, not vCenter.
1300     :param host: The location of the ESXi Host
1301     :param username: Username to connect as
1302     :param password: Password for the ESXi web endpoint
1303     :param protocol: defaults to https, can be http if ssl is disabled on ESXi
1304     :param port: defaults to 443 for https
1305     :param certificate_verify: If true require that the SSL connection present
1306                                a valid certificate. Default: True
1307     :return: True if upload is successful
1308     CLI Example:
1309     .. code-block:: bash
1310         salt '*' vsphere.get_ssh_key my.esxi.host root bad-password certificate_verify=True
1311     """
1312     if protocol is None:
1313         protocol = "https"
1314     if port is None:
1315         port = 443
1316     if certificate_verify is None:
1317         certificate_verify = True
1318     url = "{}://{}:{}/host/ssh_root_authorized_keys".format(protocol, host, port)
1319     ret = {}
1320     try:
1321         result = salt.utils.http.query(
1322             url,
1323             status=True,
1324             text=True,
1325             method="GET",
1326             username=username,
1327             password=password,
1328             verify_ssl=certificate_verify,
1329         )
1330         if result.get("status") == 200:
1331             ret["status"] = True
1332             ret["key"] = result["text"]
1333         else:
1334             ret["status"] = False
1335             ret["Error"] = result["error"]
1336     except Exception as msg:  # pylint: disable=broad-except
1337         ret["status"] = False
1338         ret["Error"] = msg
1339     return ret
1340 @depends(HAS_PYVMOMI)
1341 @ignores_kwargs("credstore")
1342 def get_host_datetime(
1343     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1344 ):
1345     """
1346     Get the date/time information for a given host or list of host_names.
1347     host
1348         The location of the host.
1349     username
1350         The username used to login to the host, such as ``root``.
1351     password
1352         The password used to login to the host.
1353     protocol
1354         Optionally set to alternate protocol if the host is not using the default
1355         protocol. Default protocol is ``https``.
1356     port
1357         Optionally set to alternate port if the host is not using the default
1358         port. Default port is ``443``.
1359     host_names
1360         List of ESXi host names. When the host, username, and password credentials
1361         are provided for a vCenter Server, the host_names argument is required to tell
1362         vCenter the hosts for which to get date/time information.
1363         If host_names is not provided, the date/time information will be retrieved for the
1364         ``host`` location instead. This is useful for when service instance connection
1365         information is used for a single ESXi host.
1366     verify_ssl
1367         Verify the SSL certificate. Default: True
1368     CLI Example:
1369     .. code-block:: bash
1370         salt '*' vsphere.get_host_datetime my.esxi.host root bad-password
1371         salt '*' vsphere.get_host_datetime my.vcenter.location root bad-password \
1372         host_names='[esxi-1.host.com, esxi-2.host.com]'
1373     """
1374     service_instance = salt.utils.vmware.get_service_instance(
1375         host=host,
1376         username=username,
1377         password=password,
1378         protocol=protocol,
1379         port=port,
1380         verify_ssl=verify_ssl,
1381     )
1382     host_names = _check_hosts(service_instance, host, host_names)
1383     ret = {}
1384     for host_name in host_names:
1385         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1386         date_time_manager = _get_date_time_mgr(host_ref)
1387         date_time = date_time_manager.QueryDateTime()
1388         ret.update({host_name: date_time})
1389     return ret
1390 @depends(HAS_PYVMOMI)
1391 @ignores_kwargs("credstore")
1392 def get_ntp_config(
1393     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
1394 ):
1395     """
1396     Get the NTP configuration information for a given host or list of host_names.
1397     host
1398         The location of the host.
1399     username
1400         The username used to login to the host, such as ``root``.
1401     password
1402         The password used to login to the host.
1403     protocol
1404         Optionally set to alternate protocol if the host is not using the default
1405         protocol. Default protocol is ``https``.
1406     port
1407         Optionally set to alternate port if the host is not using the default
1408         port. Default port is ``443``.
1409     host_names
1410         List of ESXi host names. When the host, username, and password credentials
1411         are provided for a vCenter Server, the host_names argument is required to tell
1412         vCenter the hosts for which to get ntp configuration information.
1413         If host_names is not provided, the NTP configuration will be retrieved for the
1414         ``host`` location instead. This is useful for when service instance connection
1415         information is used for a single ESXi host.
1416     verify_ssl
1417         Verify the SSL certificate. Default: True
1418     CLI Example:
1419     .. code-block:: bash
1420         salt '*' vsphere.get_ntp_config my.esxi.host root bad-password
1421         salt '*' vsphere.get_ntp_config my.vcenter.location root bad-password \
1422         host_names='[esxi-1.host.com, esxi-2.host.com]'
1423     """
1424     service_instance = salt.utils.vmware.get_service_instance(
1425         host=host,
1426         username=username,
1427         password=password,
1428         protocol=protocol,
1429         port=port,
1430         verify_ssl=verify_ssl,
1431     )
1432     host_names = _check_hosts(service_instance, host, host_names)
1433     ret = {}
1434     for host_name in host_names:
1435         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1436         ntp_config = host_ref.configManager.dateTimeSystem.dateTimeInfo.ntpConfig.server
1437         ret.update({host_name: ntp_config})
1438     return ret
1439 @depends(HAS_PYVMOMI)
1440 @ignores_kwargs("credstore")
1441 def get_service_policy(
1442     host,
1443     username,
1444     password,
1445     service_name,
1446     protocol=None,
1447     port=None,
1448     host_names=None,
1449     verify_ssl=True,
1450 ):
1451     """
1452     Get the service name's policy for a given host or list of hosts.
1453     host
1454         The location of the host.
1455     username
1456         The username used to login to the host, such as ``root``.
1457     password
1458         The password used to login to the host.
1459     service_name
1460         The name of the service for which to retrieve the policy. Supported service names are:
1461           - DCUI
1462           - TSM
1463           - SSH
1464           - lbtd
1465           - lsassd
1466           - lwiod
1467           - netlogond
1468           - ntpd
1469           - sfcbd-watchdog
1470           - snmpd
1471           - vprobed
1472           - vpxa
1473           - xorg
1474     protocol
1475         Optionally set to alternate protocol if the host is not using the default
1476         protocol. Default protocol is ``https``.
1477     port
1478         Optionally set to alternate port if the host is not using the default
1479         port. Default port is ``443``.
1480     host_names
1481         List of ESXi host names. When the host, username, and password credentials
1482         are provided for a vCenter Server, the host_names argument is required to tell
1483         vCenter the hosts for which to get service policy information.
1484         If host_names is not provided, the service policy information will be retrieved
1485         for the ``host`` location instead. This is useful for when service instance
1486         connection information is used for a single ESXi host.
1487     verify_ssl
1488         Verify the SSL certificate. Default: True
1489     CLI Example:
1490     .. code-block:: bash
1491         salt '*' vsphere.get_service_policy my.esxi.host root bad-password 'ssh'
1492         salt '*' vsphere.get_service_policy my.vcenter.location root bad-password 'ntpd' \
1493         host_names='[esxi-1.host.com, esxi-2.host.com]'
1494     """
1495     service_instance = salt.utils.vmware.get_service_instance(
1496         host=host,
1497         username=username,
1498         password=password,
1499         protocol=protocol,
1500         port=port,
1501         verify_ssl=verify_ssl,
1502     )
1503     valid_services = [
1504         "DCUI",
1505         "TSM",
1506         "SSH",
1507         "ssh",
1508         "lbtd",
1509         "lsassd",
1510         "lwiod",
1511         "netlogond",
1512         "ntpd",
1513         "sfcbd-watchdog",
1514         "snmpd",
1515         "vprobed",
1516         "vpxa",
1517         "xorg",
1518     ]
1519     host_names = _check_hosts(service_instance, host, host_names)
1520     ret = {}
1521     for host_name in host_names:
1522         if service_name not in valid_services:
1523             ret.update(
1524                 {
1525                     host_name: {
1526                         "Error": "{} is not a valid service name.".format(service_name)
1527                     }
1528                 }
1529             )
1530             return ret
1531         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1532         services = host_ref.configManager.serviceSystem.serviceInfo.service
1533         if service_name == "SSH" or service_name == "ssh":
1534             temp_service_name = "TSM-SSH"
1535         else:
1536             temp_service_name = service_name
1537         for service in services:
1538             if service.key == temp_service_name:
1539                 ret.update({host_name: {service_name: service.policy}})
1540                 break
1541             else:
1542                 msg = "Could not find service '{}' for host '{}'.".format(
1543                     service_name, host_name
1544                 )
1545                 ret.update({host_name: {"Error": msg}})
1546         if ret.get(host_name) is None:
1547             msg = "'vsphere.get_service_policy' failed for host {}.".format(host_name)
1548             log.debug(msg)
1549             ret.update({host_name: {"Error": msg}})
1550     return ret
1551 @depends(HAS_PYVMOMI)
1552 @ignores_kwargs("credstore")
1553 def get_service_running(
1554     host,
1555     username,
1556     password,
1557     service_name,
1558     protocol=None,
1559     port=None,
1560     host_names=None,
1561     verify_ssl=True,
1562 ):
1563     """
1564     Get the service name's running state for a given host or list of hosts.
1565     host
1566         The location of the host.
1567     username
1568         The username used to login to the host, such as ``root``.
1569     password
1570         The password used to login to the host.
1571     service_name
1572         The name of the service for which to retrieve the policy. Supported service names are:
1573           - DCUI
1574           - TSM
1575           - SSH
1576           - lbtd
1577           - lsassd
1578           - lwiod
1579           - netlogond
1580           - ntpd
1581           - sfcbd-watchdog
1582           - snmpd
1583           - vprobed
1584           - vpxa
1585           - xorg
1586     protocol
1587         Optionally set to alternate protocol if the host is not using the default
1588         protocol. Default protocol is ``https``.
1589     port
1590         Optionally set to alternate port if the host is not using the default
1591         port. Default port is ``443``.
1592     host_names
1593         List of ESXi host names. When the host, username, and password credentials
1594         are provided for a vCenter Server, the host_names argument is required to tell
1595         vCenter the hosts for which to get the service's running state.
1596         If host_names is not provided, the service's running state will be retrieved
1597         for the ``host`` location instead. This is useful for when service instance
1598         connection information is used for a single ESXi host.
1599     verify_ssl
1600         Verify the SSL certificate. Default: True
1601     CLI Example:
1602     .. code-block:: bash
1603         salt '*' vsphere.get_service_running my.esxi.host root bad-password 'ssh'
1604         salt '*' vsphere.get_service_running my.vcenter.location root bad-password 'ntpd' \
1605         host_names='[esxi-1.host.com, esxi-2.host.com]'
1606     """
1607     service_instance = salt.utils.vmware.get_service_instance(
1608         host=host,
1609         username=username,
1610         password=password,
1611         protocol=protocol,
1612         port=port,
1613         verify_ssl=verify_ssl,
1614     )
1615     valid_services = [
1616         "DCUI",
1617         "TSM",
1618         "SSH",
1619         "ssh",
1620         "lbtd",
1621         "lsassd",
1622         "lwiod",
1623         "netlogond",
1624         "ntpd",
1625         "sfcbd-watchdog",
1626         "snmpd",
1627         "vprobed",
1628         "vpxa",
1629         "xorg",
1630     ]
1631     host_names = _check_hosts(service_instance, host, host_names)
1632     ret = {}
1633     for host_name in host_names:
1634         if service_name not in valid_services:
1635             ret.update(
1636                 {
1637                     host_name: {
1638                         "Error": "{} is not a valid service name.".format(service_name)
1639                     }
1640                 }
1641             )
1642             return ret
1643         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1644         services = host_ref.configManager.serviceSystem.serviceInfo.service
1645         if service_name == "SSH" or service_name == "ssh":
1646             temp_service_name = "TSM-SSH"
1647         else:
1648             temp_service_name = service_name
1649         for service in services:
1650             if service.key == temp_service_name:
1651                 ret.update({host_name: {service_name: service.running}})
1652                 break
1653             else:
1654                 msg = "Could not find service '{}' for host '{}'.".format(
1655                     service_name, host_name
1656                 )
1657                 ret.update({host_name: {"Error": msg}})
1658         if ret.get(host_name) is None:
1659             msg = "'vsphere.get_service_running' failed for host {}.".format(host_name)
1660             log.debug(msg)
1661             ret.update({host_name: {"Error": msg}})
1662     return ret
1663 @depends(HAS_PYVMOMI)
1664 @ignores_kwargs("credstore")
1665 def get_vmotion_enabled(
1666     host,
1667     username,
1668     password,
1669     protocol=None,
1670     port=None,
1671     host_names=None,
1672     verify_ssl=True,
1673 ):
1674     """
1675     Get the VMotion enabled status for a given host or a list of host_names. Returns ``True``
1676     if VMotion is enabled, ``False`` if it is not enabled.
1677     host
1678         The location of the host.
1679     username
1680         The username used to login to the host, such as ``root``.
1681     password
1682         The password used to login to the host.
1683     protocol
1684         Optionally set to alternate protocol if the host is not using the default
1685         protocol. Default protocol is ``https``.
1686     port
1687         Optionally set to alternate port if the host is not using the default
1688         port. Default port is ``443``.
1689     host_names
1690         List of ESXi host names. When the host, username, and password credentials
1691         are provided for a vCenter Server, the host_names argument is required to
1692         tell vCenter which hosts to check if VMotion is enabled.
1693         If host_names is not provided, the VMotion status will be retrieved for the
1694         ``host`` location instead. This is useful for when service instance
1695         connection information is used for a single ESXi host.
1696     verify_ssl
1697         Verify the SSL certificate. Default: True
1698     CLI Example:
1699     .. code-block:: bash
1700         salt '*' vsphere.get_vmotion_enabled my.esxi.host root bad-password
1701         salt '*' vsphere.get_vmotion_enabled my.vcenter.location root bad-password \
1702         host_names='[esxi-1.host.com, esxi-2.host.com]'
1703     """
1704     service_instance = salt.utils.vmware.get_service_instance(
1705         host=host,
1706         username=username,
1707         password=password,
1708         protocol=protocol,
1709         port=port,
1710         verify_ssl=verify_ssl,
1711     )
1712     host_names = _check_hosts(service_instance, host, host_names)
1713     ret = {}
1714     for host_name in host_names:
1715         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1716         vmotion_vnic = host_ref.configManager.vmotionSystem.netConfig.selectedVnic
1717         if vmotion_vnic:
1718             ret.update({host_name: {"VMotion Enabled": True}})
1719         else:
1720             ret.update({host_name: {"VMotion Enabled": False}})
1721     return ret
1722 @depends(HAS_PYVMOMI)
1723 @ignores_kwargs("credstore")
1724 def get_vsan_enabled(
1725     host,
1726     username,
1727     password,
1728     protocol=None,
1729     port=None,
1730     host_names=None,
1731     verify_ssl=True,
1732 ):
1733     """
1734     Get the VSAN enabled status for a given host or a list of host_names. Returns ``True``
1735     if VSAN is enabled, ``False`` if it is not enabled, and ``None`` if a VSAN Host Config
1736     is unset, per host.
1737     host
1738         The location of the host.
1739     username
1740         The username used to login to the host, such as ``root``.
1741     password
1742         The password used to login to the host.
1743     protocol
1744         Optionally set to alternate protocol if the host is not using the default
1745         protocol. Default protocol is ``https``.
1746     port
1747         Optionally set to alternate port if the host is not using the default
1748         port. Default port is ``443``.
1749     host_names
1750         List of ESXi host names. When the host, username, and password credentials
1751         are provided for a vCenter Server, the host_names argument is required to
1752         tell vCenter which hosts to check if VSAN enabled.
1753         If host_names is not provided, the VSAN status will be retrieved for the
1754         ``host`` location instead. This is useful for when service instance
1755         connection information is used for a single ESXi host.
1756     verify_ssl
1757         Verify the SSL certificate. Default: True
1758     CLI Example:
1759     .. code-block:: bash
1760         salt '*' vsphere.get_vsan_enabled my.esxi.host root bad-password
1761         salt '*' vsphere.get_vsan_enabled my.vcenter.location root bad-password \
1762         host_names='[esxi-1.host.com, esxi-2.host.com]'
1763     """
1764     service_instance = salt.utils.vmware.get_service_instance(
1765         host=host,
1766         username=username,
1767         password=password,
1768         protocol=protocol,
1769         port=port,
1770         verify_ssl=verify_ssl,
1771     )
1772     host_names = _check_hosts(service_instance, host, host_names)
1773     ret = {}
1774     for host_name in host_names:
1775         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
1776         vsan_config = host_ref.config.vsanHostConfig
1777         if vsan_config is None:
1778             msg = "VSAN System Config Manager is unset for host '{}'.".format(host_name)
1779             log.debug(msg)
1780             ret.update({host_name: {"Error": msg}})
1781         else:
1782             ret.update({host_name: {"VSAN Enabled": vsan_config.enabled}})
1783     return ret
1784 @depends(HAS_PYVMOMI)
1785 @ignores_kwargs("credstore")
1786 def get_vsan_eligible_disks(
1787     host,
1788     username,
1789     password,
1790     protocol=None,
1791     port=None,
1792     host_names=None,
1793     verify_ssl=True,
1794 ):
1795     """
1796     Returns a list of VSAN-eligible disks for a given host or list of host_names.
1797     host
1798         The location of the host.
1799     username
1800         The username used to login to the host, such as ``root``.
1801     password
1802         The password used to login to the host.
1803     protocol
1804         Optionally set to alternate protocol if the host is not using the default
1805         protocol. Default protocol is ``https``.
1806     port
1807         Optionally set to alternate port if the host is not using the default
1808         port. Default port is ``443``.
1809     host_names
1810         List of ESXi host names. When the host, username, and password credentials
1811         are provided for a vCenter Server, the host_names argument is required to
1812         tell vCenter which hosts to check if any VSAN-eligible disks are available.
1813         If host_names is not provided, the VSAN-eligible disks will be retrieved
1814         for the ``host`` location instead. This is useful for when service instance
1815         connection information is used for a single ESXi host.
1816     verify_ssl
1817         Verify the SSL certificate. Default: True
1818     CLI Example:
1819     .. code-block:: bash
1820         salt '*' vsphere.get_vsan_eligible_disks my.esxi.host root bad-password
1821         salt '*' vsphere.get_vsan_eligible_disks my.vcenter.location root bad-password \
1822         host_names='[esxi-1.host.com, esxi-2.host.com]'
1823     """
1824     service_instance = salt.utils.vmware.get_service_instance(
1825         host=host,
1826         username=username,
1827         password=password,
1828         protocol=protocol,
1829         port=port,
1830         verify_ssl=verify_ssl,
1831     )
1832     host_names = _check_hosts(service_instance, host, host_names)
1833     response = _get_vsan_eligible_disks(service_instance, host, host_names)
1834     ret = {}
1835     for host_name, value in response.items():
1836         error = value.get("Error")
1837         if error:
1838             ret.update({host_name: {"Error": error}})
1839             continue
1840         disks = value.get("Eligible")
1841         if disks and isinstance(disks, list):
1842             disk_names = []
1843             for disk in disks:
1844                 disk_names.append(disk.canonicalName)
1845             ret.update({host_name: {"Eligible": disk_names}})
1846         else:
1847             ret.update({host_name: {"Eligible": disks}})
1848     return ret
1849 @depends(HAS_PYVMOMI)
1850 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter", "esxvm")
1851 @_gets_service_instance_via_proxy
1852 def test_vcenter_connection(service_instance=None):
1853     """
1854     Checks if a connection is to a vCenter
1855     CLI Example:
1856     .. code-block:: bash
1857         salt '*' vsphere.test_vcenter_connection
1858     """
1859     try:
1860         if salt.utils.vmware.is_connection_to_a_vcenter(service_instance):
1861             return True
1862     except VMwareSaltError:
1863         return False
1864     return False
1865 @depends(HAS_PYVMOMI)
1866 @ignores_kwargs("credstore")
1867 def system_info(
1868     host,
1869     username,
1870     password,
1871     protocol=None,
1872     port=None,
1873     verify_ssl=True,
1874 ):
1875     """
1876     Return system information about a VMware environment.
1877     host
1878         The location of the host.
1879     username
1880         The username used to login to the host, such as ``root``.
1881     password
1882         The password used to login to the host.
1883     protocol
1884         Optionally set to alternate protocol if the host is not using the default
1885         protocol. Default protocol is ``https``.
1886     port
1887         Optionally set to alternate port if the host is not using the default
1888         port. Default port is ``443``.
1889     verify_ssl
1890         Verify the SSL certificate. Default: True
1891     CLI Example:
1892     .. code-block:: bash
1893         salt '*' vsphere.system_info 1.2.3.4 root bad-password
1894     """
1895     service_instance = salt.utils.vmware.get_service_instance(
1896         host=host,
1897         username=username,
1898         password=password,
1899         protocol=protocol,
1900         port=port,
1901         verify_ssl=verify_ssl,
1902     )
1903     ret = salt.utils.vmware.get_inventory(service_instance).about.__dict__
1904     if "apiType" in ret:
1905         if ret["apiType"] == "HostAgent":
1906             ret = dictupdate.update(
1907                 ret, salt.utils.vmware.get_hardware_grains(service_instance)
1908             )
1909     return ret
1910 @depends(HAS_PYVMOMI)
1911 @ignores_kwargs("credstore")
1912 def list_datacenters(
1913     host, username, password, protocol=None, port=None, verify_ssl=True
1914 ):
1915     """
1916     Returns a list of datacenters for the specified host.
1917     host
1918         The location of the host.
1919     username
1920         The username used to login to the host, such as ``root``.
1921     password
1922         The password used to login to the host.
1923     protocol
1924         Optionally set to alternate protocol if the host is not using the default
1925         protocol. Default protocol is ``https``.
1926     port
1927         Optionally set to alternate port if the host is not using the default
1928         port. Default port is ``443``.
1929     verify_ssl
1930         Verify the SSL certificate. Default: True
1931     CLI Example:
1932     .. code-block:: bash
1933         salt '*' vsphere.list_datacenters 1.2.3.4 root bad-password
1934     """
1935     service_instance = salt.utils.vmware.get_service_instance(
1936         host=host,
1937         username=username,
1938         password=password,
1939         protocol=protocol,
1940         port=port,
1941         verify_ssl=verify_ssl,
1942     )
1943     return salt.utils.vmware.list_datacenters(service_instance)
1944 @depends(HAS_PYVMOMI)
1945 @ignores_kwargs("credstore")
1946 def list_clusters(host, username, password, protocol=None, port=None, verify_ssl=True):
1947     """
1948     Returns a list of clusters for the specified host.
1949     host
1950         The location of the host.
1951     username
1952         The username used to login to the host, such as ``root``.
1953     password
1954         The password used to login to the host.
1955     protocol
1956         Optionally set to alternate protocol if the host is not using the default
1957         protocol. Default protocol is ``https``.
1958     port
1959         Optionally set to alternate port if the host is not using the default
1960         port. Default port is ``443``.
1961     verify_ssl
1962         Verify the SSL certificate. Default: True
1963     CLI Example:
1964     .. code-block:: bash
1965         salt '*' vsphere.list_clusters 1.2.3.4 root bad-password
1966     """
1967     service_instance = salt.utils.vmware.get_service_instance(
1968         host=host,
1969         username=username,
1970         password=password,
1971         protocol=protocol,
1972         port=port,
1973         verify_ssl=verify_ssl,
1974     )
1975     return salt.utils.vmware.list_clusters(service_instance)
1976 @depends(HAS_PYVMOMI)
1977 @ignores_kwargs("credstore")
1978 def list_datastore_clusters(
1979     host, username, password, protocol=None, port=None, verify_ssl=True
1980 ):
1981     """
1982     Returns a list of datastore clusters for the specified host.
1983     host
1984         The location of the host.
1985     username
1986         The username used to login to the host, such as ``root``.
1987     password
1988         The password used to login to the host.
1989     protocol
1990         Optionally set to alternate protocol if the host is not using the default
1991         protocol. Default protocol is ``https``.
1992     port
1993         Optionally set to alternate port if the host is not using the default
1994         port. Default port is ``443``.
1995     verify_ssl
1996         Verify the SSL certificate. Default: True
1997     CLI Example:
1998     .. code-block:: bash
1999         salt '*' vsphere.list_datastore_clusters 1.2.3.4 root bad-password
2000     """
2001     service_instance = salt.utils.vmware.get_service_instance(
2002         host=host,
2003         username=username,
2004         password=password,
2005         protocol=protocol,
2006         port=port,
2007         verify_ssl=verify_ssl,
2008     )
2009     return salt.utils.vmware.list_datastore_clusters(service_instance)
2010 @depends(HAS_PYVMOMI)
2011 @ignores_kwargs("credstore")
2012 def list_datastores(
2013     host, username, password, protocol=None, port=None, verify_ssl=True
2014 ):
2015     """
2016     Returns a list of datastores for the specified host.
2017     host
2018         The location of the host.
2019     username
2020         The username used to login to the host, such as ``root``.
2021     password
2022         The password used to login to the host.
2023     protocol
2024         Optionally set to alternate protocol if the host is not using the default
2025         protocol. Default protocol is ``https``.
2026     port
2027         Optionally set to alternate port if the host is not using the default
2028         port. Default port is ``443``.
2029     verify_ssl
2030         Verify the SSL certificate. Default: True
2031     CLI Example:
2032     .. code-block:: bash
2033         salt '*' vsphere.list_datastores 1.2.3.4 root bad-password
2034     """
2035     service_instance = salt.utils.vmware.get_service_instance(
2036         host=host,
2037         username=username,
2038         password=password,
2039         protocol=protocol,
2040         port=port,
2041         verify_ssl=verify_ssl,
2042     )
2043     return salt.utils.vmware.list_datastores(service_instance)
2044 @depends(HAS_PYVMOMI)
2045 @ignores_kwargs("credstore")
2046 def list_hosts(host, username, password, protocol=None, port=None, verify_ssl=True):
2047     """
2048     Returns a list of hosts for the specified VMware environment.
2049     host
2050         The location of the host.
2051     username
2052         The username used to login to the host, such as ``root``.
2053     password
2054         The password used to login to the host.
2055     protocol
2056         Optionally set to alternate protocol if the host is not using the default
2057         protocol. Default protocol is ``https``.
2058     port
2059         Optionally set to alternate port if the host is not using the default
2060         port. Default port is ``443``.
2061     verify_ssl
2062         Verify the SSL certificate. Default: True
2063     CLI Example:
2064     .. code-block:: bash
2065         salt '*' vsphere.list_hosts 1.2.3.4 root bad-password
2066     """
2067     service_instance = salt.utils.vmware.get_service_instance(
2068         host=host,
2069         username=username,
2070         password=password,
2071         protocol=protocol,
2072         port=port,
2073         verify_ssl=verify_ssl,
2074     )
2075     return salt.utils.vmware.list_hosts(service_instance)
2076 @depends(HAS_PYVMOMI)
2077 @ignores_kwargs("credstore")
2078 def list_resourcepools(
2079     host, username, password, protocol=None, port=None, verify_ssl=True
2080 ):
2081     """
2082     Returns a list of resource pools for the specified host.
2083     host
2084         The location of the host.
2085     username
2086         The username used to login to the host, such as ``root``.
2087     password
2088         The password used to login to the host.
2089     protocol
2090         Optionally set to alternate protocol if the host is not using the default
2091         protocol. Default protocol is ``https``.
2092     port
2093         Optionally set to alternate port if the host is not using the default
2094         port. Default port is ``443``.
2095     verify_ssl
2096         Verify the SSL certificate. Default: True
2097     CLI Example:
2098     .. code-block:: bash
2099         salt '*' vsphere.list_resourcepools 1.2.3.4 root bad-password
2100     """
2101     service_instance = salt.utils.vmware.get_service_instance(
2102         host=host,
2103         username=username,
2104         password=password,
2105         protocol=protocol,
2106         port=port,
2107         verify_ssl=verify_ssl,
2108     )
2109     return salt.utils.vmware.list_resourcepools(service_instance)
2110 @depends(HAS_PYVMOMI)
2111 @ignores_kwargs("credstore")
2112 def list_networks(host, username, password, protocol=None, port=None, verify_ssl=True):
2113     """
2114     Returns a list of networks for the specified host.
2115     host
2116         The location of the host.
2117     username
2118         The username used to login to the host, such as ``root``.
2119     password
2120         The password used to login to the host.
2121     protocol
2122         Optionally set to alternate protocol if the host is not using the default
2123         protocol. Default protocol is ``https``.
2124     port
2125         Optionally set to alternate port if the host is not using the default
2126         port. Default port is ``443``.
2127     verify_ssl
2128         Verify the SSL certificate. Default: True
2129     CLI Example:
2130     .. code-block:: bash
2131         salt '*' vsphere.list_networks 1.2.3.4 root bad-password
2132     """
2133     service_instance = salt.utils.vmware.get_service_instance(
2134         host=host,
2135         username=username,
2136         password=password,
2137         protocol=protocol,
2138         port=port,
2139         verify_ssl=verify_ssl,
2140     )
2141     return salt.utils.vmware.list_networks(service_instance)
2142 @depends(HAS_PYVMOMI)
2143 @ignores_kwargs("credstore")
2144 def list_vms(host, username, password, protocol=None, port=None, verify_ssl=True):
2145     """
2146     Returns a list of VMs for the specified host.
2147     host
2148         The location of the host.
2149     username
2150         The username used to login to the host, such as ``root``.
2151     password
2152         The password used to login to the host.
2153     protocol
2154         Optionally set to alternate protocol if the host is not using the default
2155         protocol. Default protocol is ``https``.
2156     port
2157         Optionally set to alternate port if the host is not using the default
2158         port. Default port is ``443``.
2159     verify_ssl
2160         Verify the SSL certificate. Default: True
2161     CLI Example:
2162     .. code-block:: bash
2163         salt '*' vsphere.list_vms 1.2.3.4 root bad-password
2164     """
2165     service_instance = salt.utils.vmware.get_service_instance(
2166         host=host,
2167         username=username,
2168         password=password,
2169         protocol=protocol,
2170         port=port,
2171         verify_ssl=verify_ssl,
2172     )
2173     return salt.utils.vmware.list_vms(service_instance)
2174 @depends(HAS_PYVMOMI)
2175 @ignores_kwargs("credstore")
2176 def list_folders(host, username, password, protocol=None, port=None, verify_ssl=True):
2177     """
2178     Returns a list of folders for the specified host.
2179     host
2180         The location of the host.
2181     username
2182         The username used to login to the host, such as ``root``.
2183     password
2184         The password used to login to the host.
2185     protocol
2186         Optionally set to alternate protocol if the host is not using the default
2187         protocol. Default protocol is ``https``.
2188     port
2189         Optionally set to alternate port if the host is not using the default
2190         port. Default port is ``443``.
2191     verify_ssl
2192         Verify the SSL certificate. Default: True
2193     CLI Example:
2194     .. code-block:: bash
2195         salt '*' vsphere.list_folders 1.2.3.4 root bad-password
2196     """
2197     service_instance = salt.utils.vmware.get_service_instance(
2198         host=host,
2199         username=username,
2200         password=password,
2201         protocol=protocol,
2202         port=port,
2203         verify_ssl=verify_ssl,
2204     )
2205     return salt.utils.vmware.list_folders(service_instance)
2206 @depends(HAS_PYVMOMI)
2207 @ignores_kwargs("credstore")
2208 def list_dvs(host, username, password, protocol=None, port=None, verify_ssl=True):
2209     """
2210     Returns a list of distributed virtual switches for the specified host.
2211     host
2212         The location of the host.
2213     username
2214         The username used to login to the host, such as ``root``.
2215     password
2216         The password used to login to the host.
2217     protocol
2218         Optionally set to alternate protocol if the host is not using the default
2219         protocol. Default protocol is ``https``.
2220     port
2221         Optionally set to alternate port if the host is not using the default
2222         port. Default port is ``443``.
2223     verify_ssl
2224         Verify the SSL certificate. Default: True
2225     CLI Example:
2226     .. code-block:: bash
2227         salt '*' vsphere.list_dvs 1.2.3.4 root bad-password
2228     """
2229     service_instance = salt.utils.vmware.get_service_instance(
2230         host=host,
2231         username=username,
2232         password=password,
2233         protocol=protocol,
2234         port=port,
2235         verify_ssl=verify_ssl,
2236     )
2237     return salt.utils.vmware.list_dvs(service_instance)
2238 @depends(HAS_PYVMOMI)
2239 @ignores_kwargs("credstore")
2240 def list_vapps(host, username, password, protocol=None, port=None, verify_ssl=True):
2241     """
2242     Returns a list of vApps for the specified host.
2243     host
2244         The location of the host.
2245     username
2246         The username used to login to the host, such as ``root``.
2247     password
2248         The password used to login to the host.
2249     protocol
2250         Optionally set to alternate protocol if the host is not using the default
2251         protocol. Default protocol is ``https``.
2252     port
2253         Optionally set to alternate port if the host is not using the default
2254         port. Default port is ``443``.
2255     verify_ssl
2256         Verify the SSL certificate. Default: True
2257     CLI Example:
2258     .. code-block:: bash
2259         salt '*' vsphere.list_vapps 1.2.3.4 root bad-password
2260     """
2261     service_instance = salt.utils.vmware.get_service_instance(
2262         host=host,
2263         username=username,
2264         password=password,
2265         protocol=protocol,
2266         port=port,
2267         verify_ssl=verify_ssl,
2268     )
2269     return salt.utils.vmware.list_vapps(service_instance)
2270 @depends(HAS_PYVMOMI)
2271 @ignores_kwargs("credstore")
2272 def list_ssds(
2273     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
2274 ):
2275     """
2276     Returns a list of SSDs for the given host or list of host_names.
2277     host
2278         The location of the host.
2279     username
2280         The username used to login to the host, such as ``root``.
2281     password
2282         The password used to login to the host.
2283     protocol
2284         Optionally set to alternate protocol if the host is not using the default
2285         protocol. Default protocol is ``https``.
2286     port
2287         Optionally set to alternate port if the host is not using the default
2288         port. Default port is ``443``.
2289     host_names
2290         List of ESXi host names. When the host, username, and password credentials
2291         are provided for a vCenter Server, the host_names argument is required to
2292         tell vCenter the hosts for which to retrieve SSDs.
2293         If host_names is not provided, SSDs will be retrieved for the
2294         ``host`` location instead. This is useful for when service instance
2295         connection information is used for a single ESXi host.
2296     verify_ssl
2297         Verify the SSL certificate. Default: True
2298     CLI Example:
2299     .. code-block:: bash
2300         salt '*' vsphere.list_ssds my.esxi.host root bad-password
2301         salt '*' vsphere.list_ssds my.vcenter.location root bad-password \
2302         host_names='[esxi-1.host.com, esxi-2.host.com]'
2303     """
2304     service_instance = salt.utils.vmware.get_service_instance(
2305         host=host,
2306         username=username,
2307         password=password,
2308         protocol=protocol,
2309         port=port,
2310         verify_ssl=verify_ssl,
2311     )
2312     host_names = _check_hosts(service_instance, host, host_names)
2313     ret = {}
2314     names = []
2315     for host_name in host_names:
2316         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2317         disks = _get_host_ssds(host_ref)
2318         for disk in disks:
2319             names.append(disk.canonicalName)
2320         ret.update({host_name: names})
2321     return ret
2322 @depends(HAS_PYVMOMI)
2323 @ignores_kwargs("credstore")
2324 def list_non_ssds(
2325     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
2326 ):
2327     """
2328     Returns a list of Non-SSD disks for the given host or list of host_names.
2329     .. note::
2330         In the pyVmomi StorageSystem, ScsiDisks may, or may not have an ``ssd`` attribute.
2331         This attribute indicates if the ScsiDisk is SSD backed. As this option is optional,
2332         if a relevant disk in the StorageSystem does not have ``ssd = true``, it will end
2333         up in the ``non_ssds`` list here.
2334     host
2335         The location of the host.
2336     username
2337         The username used to login to the host, such as ``root``.
2338     password
2339         The password used to login to the host.
2340     protocol
2341         Optionally set to alternate protocol if the host is not using the default
2342         protocol. Default protocol is ``https``.
2343     port
2344         Optionally set to alternate port if the host is not using the default
2345         port. Default port is ``443``.
2346     host_names
2347         List of ESXi host names. When the host, username, and password credentials
2348         are provided for a vCenter Server, the host_names argument is required to
2349         tell vCenter the hosts for which to retrieve Non-SSD disks.
2350         If host_names is not provided, Non-SSD disks will be retrieved for the
2351         ``host`` location instead. This is useful for when service instance
2352         connection information is used for a single ESXi host.
2353     verify_ssl
2354         Verify the SSL certificate. Default: True
2355     CLI Example:
2356     .. code-block:: bash
2357         salt '*' vsphere.list_non_ssds my.esxi.host root bad-password
2358         salt '*' vsphere.list_non_ssds my.vcenter.location root bad-password \
2359         host_names='[esxi-1.host.com, esxi-2.host.com]'
2360     """
2361     service_instance = salt.utils.vmware.get_service_instance(
2362         host=host,
2363         username=username,
2364         password=password,
2365         protocol=protocol,
2366         port=port,
2367         verify_ssl=verify_ssl,
2368     )
2369     host_names = _check_hosts(service_instance, host, host_names)
2370     ret = {}
2371     names = []
2372     for host_name in host_names:
2373         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2374         disks = _get_host_non_ssds(host_ref)
2375         for disk in disks:
2376             names.append(disk.canonicalName)
2377         ret.update({host_name: names})
2378     return ret
2379 @depends(HAS_PYVMOMI)
2380 @ignores_kwargs("credstore")
2381 def set_ntp_config(
2382     host,
2383     username,
2384     password,
2385     ntp_servers,
2386     protocol=None,
2387     port=None,
2388     host_names=None,
2389     verify_ssl=True,
2390 ):
2391     """
2392     Set NTP configuration for a given host of list of host_names.
2393     host
2394         The location of the host.
2395     username
2396         The username used to login to the host, such as ``root``.
2397     password
2398         The password used to login to the host.
2399     ntp_servers
2400         A list of servers that should be added to and configured for the specified
2401         host's NTP configuration.
2402     protocol
2403         Optionally set to alternate protocol if the host is not using the default
2404         protocol. Default protocol is ``https``.
2405     port
2406         Optionally set to alternate port if the host is not using the default
2407         port. Default port is ``443``.
2408     host_names
2409         List of ESXi host names. When the host, username, and password credentials
2410         are provided for a vCenter Server, the host_names argument is required to tell
2411         vCenter which hosts to configure ntp servers.
2412         If host_names is not provided, the NTP servers will be configured for the
2413         ``host`` location instead. This is useful for when service instance connection
2414         information is used for a single ESXi host.
2415     verify_ssl
2416         Verify the SSL certificate. Default: True
2417     CLI Example:
2418     .. code-block:: bash
2419         salt '*' vsphere.ntp_configure my.esxi.host root bad-password '[192.174.1.100, 192.174.1.200]'
2420         salt '*' vsphere.ntp_configure my.vcenter.location root bad-password '[192.174.1.100, 192.174.1.200]' \
2421         host_names='[esxi-1.host.com, esxi-2.host.com]'
2422     """
2423     service_instance = salt.utils.vmware.get_service_instance(
2424         host=host,
2425         username=username,
2426         password=password,
2427         protocol=protocol,
2428         port=port,
2429         verify_ssl=verify_ssl,
2430     )
2431     if not isinstance(ntp_servers, list):
2432         raise CommandExecutionError("'ntp_servers' must be a list.")
2433     ntp_config = vim.HostNtpConfig(server=ntp_servers)
2434     date_config = vim.HostDateTimeConfig(ntpConfig=ntp_config)
2435     host_names = _check_hosts(service_instance, host, host_names)
2436     ret = {}
2437     for host_name in host_names:
2438         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2439         date_time_manager = _get_date_time_mgr(host_ref)
2440         log.debug("Configuring NTP Servers '%s' for host '%s'.", ntp_servers, host_name)
2441         try:
2442             date_time_manager.UpdateDateTimeConfig(config=date_config)
2443         except vim.fault.HostConfigFault as err:
2444             msg = "vsphere.ntp_configure_servers failed: {}".format(err)
2445             log.debug(msg)
2446             ret.update({host_name: {"Error": msg}})
2447             continue
2448         ret.update({host_name: {"NTP Servers": ntp_config}})
2449     return ret
2450 @depends(HAS_PYVMOMI)
2451 @ignores_kwargs("credstore")
2452 def service_start(
2453     host,
2454     username,
2455     password,
2456     service_name,
2457     protocol=None,
2458     port=None,
2459     host_names=None,
2460     verify_ssl=True,
2461 ):
2462     """
2463     Start the named service for the given host or list of hosts.
2464     host
2465         The location of the host.
2466     username
2467         The username used to login to the host, such as ``root``.
2468     password
2469         The password used to login to the host.
2470     service_name
2471         The name of the service for which to set the policy. Supported service names are:
2472           - DCUI
2473           - TSM
2474           - SSH
2475           - lbtd
2476           - lsassd
2477           - lwiod
2478           - netlogond
2479           - ntpd
2480           - sfcbd-watchdog
2481           - snmpd
2482           - vprobed
2483           - vpxa
2484           - xorg
2485     protocol
2486         Optionally set to alternate protocol if the host is not using the default
2487         protocol. Default protocol is ``https``.
2488     port
2489         Optionally set to alternate port if the host is not using the default
2490         port. Default port is ``443``.
2491     host_names
2492         List of ESXi host names. When the host, username, and password credentials
2493         are provided for a vCenter Server, the host_names argument is required to tell
2494         vCenter the hosts for which to start the service.
2495         If host_names is not provided, the service will be started for the ``host``
2496         location instead. This is useful for when service instance connection information
2497         is used for a single ESXi host.
2498     verify_ssl
2499         Verify the SSL certificate. Default: True
2500     CLI Example:
2501     .. code-block:: bash
2502         salt '*' vsphere.service_start my.esxi.host root bad-password 'ntpd'
2503         salt '*' vsphere.service_start my.vcenter.location root bad-password 'ntpd' \
2504         host_names='[esxi-1.host.com, esxi-2.host.com]'
2505     """
2506     service_instance = salt.utils.vmware.get_service_instance(
2507         host=host,
2508         username=username,
2509         password=password,
2510         protocol=protocol,
2511         port=port,
2512         verify_ssl=verify_ssl,
2513     )
2514     host_names = _check_hosts(service_instance, host, host_names)
2515     valid_services = [
2516         "DCUI",
2517         "TSM",
2518         "SSH",
2519         "ssh",
2520         "lbtd",
2521         "lsassd",
2522         "lwiod",
2523         "netlogond",
2524         "ntpd",
2525         "sfcbd-watchdog",
2526         "snmpd",
2527         "vprobed",
2528         "vpxa",
2529         "xorg",
2530     ]
2531     ret = {}
2532     if service_name == "SSH" or service_name == "ssh":
2533         temp_service_name = "TSM-SSH"
2534     else:
2535         temp_service_name = service_name
2536     for host_name in host_names:
2537         if service_name not in valid_services:
2538             ret.update(
2539                 {
2540                     host_name: {
2541                         "Error": "{} is not a valid service name.".format(service_name)
2542                     }
2543                 }
2544             )
2545             return ret
2546         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2547         service_manager = _get_service_manager(host_ref)
2548         log.debug("Starting the '%s' service on %s.", service_name, host_name)
2549         try:
2550             service_manager.StartService(id=temp_service_name)
2551         except vim.fault.HostConfigFault as err:
2552             msg = "'vsphere.service_start' failed for host {}: {}".format(
2553                 host_name, err
2554             )
2555             log.debug(msg)
2556             ret.update({host_name: {"Error": msg}})
2557             continue
2558         except vim.fault.RestrictedVersion as err:
2559             log.debug(err)
2560             ret.update({host_name: {"Error": err}})
2561             continue
2562         ret.update({host_name: {"Service Started": True}})
2563     return ret
2564 @depends(HAS_PYVMOMI)
2565 @ignores_kwargs("credstore")
2566 def service_stop(
2567     host,
2568     username,
2569     password,
2570     service_name,
2571     protocol=None,
2572     port=None,
2573     host_names=None,
2574     verify_ssl=True,
2575 ):
2576     """
2577     Stop the named service for the given host or list of hosts.
2578     host
2579         The location of the host.
2580     username
2581         The username used to login to the host, such as ``root``.
2582     password
2583         The password used to login to the host.
2584     service_name
2585         The name of the service for which to set the policy. Supported service names are:
2586           - DCUI
2587           - TSM
2588           - SSH
2589           - lbtd
2590           - lsassd
2591           - lwiod
2592           - netlogond
2593           - ntpd
2594           - sfcbd-watchdog
2595           - snmpd
2596           - vprobed
2597           - vpxa
2598           - xorg
2599     protocol
2600         Optionally set to alternate protocol if the host is not using the default
2601         protocol. Default protocol is ``https``.
2602     port
2603         Optionally set to alternate port if the host is not using the default
2604         port. Default port is ``443``.
2605     host_names
2606         List of ESXi host names. When the host, username, and password credentials
2607         are provided for a vCenter Server, the host_names argument is required to tell
2608         vCenter the hosts for which to stop the service.
2609         If host_names is not provided, the service will be stopped for the ``host``
2610         location instead. This is useful for when service instance connection information
2611         is used for a single ESXi host.
2612     verify_ssl
2613         Verify the SSL certificate. Default: True
2614     CLI Example:
2615     .. code-block:: bash
2616         salt '*' vsphere.service_stop my.esxi.host root bad-password 'ssh'
2617         salt '*' vsphere.service_stop my.vcenter.location root bad-password 'ssh' \
2618         host_names='[esxi-1.host.com, esxi-2.host.com]'
2619     """
2620     service_instance = salt.utils.vmware.get_service_instance(
2621         host=host,
2622         username=username,
2623         password=password,
2624         protocol=protocol,
2625         port=port,
2626         verify_ssl=verify_ssl,
2627     )
2628     host_names = _check_hosts(service_instance, host, host_names)
2629     valid_services = [
2630         "DCUI",
2631         "TSM",
2632         "SSH",
2633         "ssh",
2634         "lbtd",
2635         "lsassd",
2636         "lwiod",
2637         "netlogond",
2638         "ntpd",
2639         "sfcbd-watchdog",
2640         "snmpd",
2641         "vprobed",
2642         "vpxa",
2643         "xorg",
2644     ]
2645     ret = {}
2646     if service_name == "SSH" or service_name == "ssh":
2647         temp_service_name = "TSM-SSH"
2648     else:
2649         temp_service_name = service_name
2650     for host_name in host_names:
2651         if service_name not in valid_services:
2652             ret.update(
2653                 {
2654                     host_name: {
2655                         "Error": "{} is not a valid service name.".format(service_name)
2656                     }
2657                 }
2658             )
2659             return ret
2660         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2661         service_manager = _get_service_manager(host_ref)
2662         log.debug("Stopping the '%s' service on %s.", service_name, host_name)
2663         try:
2664             service_manager.StopService(id=temp_service_name)
2665         except vim.fault.HostConfigFault as err:
2666             msg = "'vsphere.service_stop' failed for host {}: {}".format(host_name, err)
2667             log.debug(msg)
2668             ret.update({host_name: {"Error": msg}})
2669             continue
2670         except vim.fault.RestrictedVersion as err:
2671             log.debug(err)
2672             ret.update({host_name: {"Error": err}})
2673             continue
2674         ret.update({host_name: {"Service Stopped": True}})
2675     return ret
2676 @depends(HAS_PYVMOMI)
2677 @ignores_kwargs("credstore")
2678 def service_restart(
2679     host,
2680     username,
2681     password,
2682     service_name,
2683     protocol=None,
2684     port=None,
2685     host_names=None,
2686     verify_ssl=True,
2687 ):
2688     """
2689     Restart the named service for the given host or list of hosts.
2690     host
2691         The location of the host.
2692     username
2693         The username used to login to the host, such as ``root``.
2694     password
2695         The password used to login to the host.
2696     service_name
2697         The name of the service for which to set the policy. Supported service names are:
2698           - DCUI
2699           - TSM
2700           - SSH
2701           - lbtd
2702           - lsassd
2703           - lwiod
2704           - netlogond
2705           - ntpd
2706           - sfcbd-watchdog
2707           - snmpd
2708           - vprobed
2709           - vpxa
2710           - xorg
2711     protocol
2712         Optionally set to alternate protocol if the host is not using the default
2713         protocol. Default protocol is ``https``.
2714     port
2715         Optionally set to alternate port if the host is not using the default
2716         port. Default port is ``443``.
2717     host_names
2718         List of ESXi host names. When the host, username, and password credentials
2719         are provided for a vCenter Server, the host_names argument is required to tell
2720         vCenter the hosts for which to restart the service.
2721         If host_names is not provided, the service will be restarted for the ``host``
2722         location instead. This is useful for when service instance connection information
2723         is used for a single ESXi host.
2724     verify_ssl
2725         Verify the SSL certificate. Default: True
2726     CLI Example:
2727     .. code-block:: bash
2728         salt '*' vsphere.service_restart my.esxi.host root bad-password 'ntpd'
2729         salt '*' vsphere.service_restart my.vcenter.location root bad-password 'ntpd' \
2730         host_names='[esxi-1.host.com, esxi-2.host.com]'
2731     """
2732     service_instance = salt.utils.vmware.get_service_instance(
2733         host=host,
2734         username=username,
2735         password=password,
2736         protocol=protocol,
2737         port=port,
2738         verify_ssl=verify_ssl,
2739     )
2740     host_names = _check_hosts(service_instance, host, host_names)
2741     valid_services = [
2742         "DCUI",
2743         "TSM",
2744         "SSH",
2745         "ssh",
2746         "lbtd",
2747         "lsassd",
2748         "lwiod",
2749         "netlogond",
2750         "ntpd",
2751         "sfcbd-watchdog",
2752         "snmpd",
2753         "vprobed",
2754         "vpxa",
2755         "xorg",
2756     ]
2757     ret = {}
2758     if service_name == "SSH" or service_name == "ssh":
2759         temp_service_name = "TSM-SSH"
2760     else:
2761         temp_service_name = service_name
2762     for host_name in host_names:
2763         if service_name not in valid_services:
2764             ret.update(
2765                 {
2766                     host_name: {
2767                         "Error": "{} is not a valid service name.".format(service_name)
2768                     }
2769                 }
2770             )
2771             return ret
2772         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2773         service_manager = _get_service_manager(host_ref)
2774         log.debug("Restarting the '%s' service on %s.", service_name, host_name)
2775         try:
2776             service_manager.RestartService(id=temp_service_name)
2777         except vim.fault.HostConfigFault as err:
2778             msg = "'vsphere.service_restart' failed for host {}: {}".format(
2779                 host_name, err
2780             )
2781             log.debug(msg)
2782             ret.update({host_name: {"Error": msg}})
2783             continue
2784         except vim.fault.RestrictedVersion as err:
2785             log.debug(err)
2786             ret.update({host_name: {"Error": err}})
2787             continue
2788         ret.update({host_name: {"Service Restarted": True}})
2789     return ret
2790 @depends(HAS_PYVMOMI)
2791 @ignores_kwargs("credstore")
2792 def set_service_policy(
2793     host,
2794     username,
2795     password,
2796     service_name,
2797     service_policy,
2798     protocol=None,
2799     port=None,
2800     host_names=None,
2801     verify_ssl=True,
2802 ):
2803     """
2804     Set the service name's policy for a given host or list of hosts.
2805     host
2806         The location of the host.
2807     username
2808         The username used to login to the host, such as ``root``.
2809     password
2810         The password used to login to the host.
2811     service_name
2812         The name of the service for which to set the policy. Supported service names are:
2813           - DCUI
2814           - TSM
2815           - SSH
2816           - lbtd
2817           - lsassd
2818           - lwiod
2819           - netlogond
2820           - ntpd
2821           - sfcbd-watchdog
2822           - snmpd
2823           - vprobed
2824           - vpxa
2825           - xorg
2826     service_policy
2827         The policy to set for the service. For example, 'automatic'.
2828     protocol
2829         Optionally set to alternate protocol if the host is not using the default
2830         protocol. Default protocol is ``https``.
2831     port
2832         Optionally set to alternate port if the host is not using the default
2833         port. Default port is ``443``.
2834     host_names
2835         List of ESXi host names. When the host, username, and password credentials
2836         are provided for a vCenter Server, the host_names argument is required to tell
2837         vCenter the hosts for which to set the service policy.
2838         If host_names is not provided, the service policy information will be retrieved
2839         for the ``host`` location instead. This is useful for when service instance
2840         connection information is used for a single ESXi host.
2841     verify_ssl
2842         Verify the SSL certificate. Default: True
2843     CLI Example:
2844     .. code-block:: bash
2845         salt '*' vsphere.set_service_policy my.esxi.host root bad-password 'ntpd' 'automatic'
2846         salt '*' vsphere.set_service_policy my.vcenter.location root bad-password 'ntpd' 'automatic' \
2847         host_names='[esxi-1.host.com, esxi-2.host.com]'
2848     """
2849     service_instance = salt.utils.vmware.get_service_instance(
2850         host=host,
2851         username=username,
2852         password=password,
2853         protocol=protocol,
2854         port=port,
2855         verify_ssl=verify_ssl,
2856     )
2857     host_names = _check_hosts(service_instance, host, host_names)
2858     valid_services = [
2859         "DCUI",
2860         "TSM",
2861         "SSH",
2862         "ssh",
2863         "lbtd",
2864         "lsassd",
2865         "lwiod",
2866         "netlogond",
2867         "ntpd",
2868         "sfcbd-watchdog",
2869         "snmpd",
2870         "vprobed",
2871         "vpxa",
2872         "xorg",
2873     ]
2874     ret = {}
2875     for host_name in host_names:
2876         if service_name not in valid_services:
2877             ret.update(
2878                 {
2879                     host_name: {
2880                         "Error": "{} is not a valid service name.".format(service_name)
2881                     }
2882                 }
2883             )
2884             return ret
2885         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2886         service_manager = _get_service_manager(host_ref)
2887         services = host_ref.configManager.serviceSystem.serviceInfo.service
2888         for service in services:
2889             service_key = None
2890             if service.key == service_name:
2891                 service_key = service.key
2892             elif service_name == "ssh" or service_name == "SSH":
2893                 if service.key == "TSM-SSH":
2894                     service_key = "TSM-SSH"
2895             if service_key:
2896                 try:
2897                     service_manager.UpdateServicePolicy(
2898                         id=service_key, policy=service_policy
2899                     )
2900                 except vim.fault.NotFound:
2901                     msg = "The service name '{}' was not found.".format(service_name)
2902                     log.debug(msg)
2903                     ret.update({host_name: {"Error": msg}})
2904                     continue
2905                 except vim.fault.HostConfigFault as err:
2906                     msg = "'vsphere.set_service_policy' failed for host {}: {}".format(
2907                         host_name, err
2908                     )
2909                     log.debug(msg)
2910                     ret.update({host_name: {"Error": msg}})
2911                     continue
2912                 ret.update({host_name: True})
2913             if ret.get(host_name) is None:
2914                 msg = "Could not find service '{}' for host '{}'.".format(
2915                     service_name, host_name
2916                 )
2917                 log.debug(msg)
2918                 ret.update({host_name: {"Error": msg}})
2919     return ret
2920 @depends(HAS_PYVMOMI)
2921 @ignores_kwargs("credstore")
2922 def update_host_datetime(
2923     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
2924 ):
2925     """
2926     Update the date/time on the given host or list of host_names. This function should be
2927     used with caution since network delays and execution delays can result in time skews.
2928     host
2929         The location of the host.
2930     username
2931         The username used to login to the host, such as ``root``.
2932     password
2933         The password used to login to the host.
2934     protocol
2935         Optionally set to alternate protocol if the host is not using the default
2936         protocol. Default protocol is ``https``.
2937     port
2938         Optionally set to alternate port if the host is not using the default
2939         port. Default port is ``443``.
2940     host_names
2941         List of ESXi host names. When the host, username, and password credentials
2942         are provided for a vCenter Server, the host_names argument is required to
2943         tell vCenter which hosts should update their date/time.
2944         If host_names is not provided, the date/time will be updated for the ``host``
2945         location instead. This is useful for when service instance connection
2946         information is used for a single ESXi host.
2947     verify_ssl
2948         Verify the SSL certificate. Default: True
2949     CLI Example:
2950     .. code-block:: bash
2951         salt '*' vsphere.update_date_time my.esxi.host root bad-password
2952         salt '*' vsphere.update_date_time my.vcenter.location root bad-password \
2953         host_names='[esxi-1.host.com, esxi-2.host.com]'
2954     """
2955     service_instance = salt.utils.vmware.get_service_instance(
2956         host=host,
2957         username=username,
2958         password=password,
2959         protocol=protocol,
2960         port=port,
2961         verify_ssl=verify_ssl,
2962     )
2963     host_names = _check_hosts(service_instance, host, host_names)
2964     ret = {}
2965     for host_name in host_names:
2966         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
2967         date_time_manager = _get_date_time_mgr(host_ref)
2968         try:
2969             date_time_manager.UpdateDateTime(datetime.datetime.utcnow())
2970         except vim.fault.HostConfigFault as err:
2971             msg = "'vsphere.update_date_time' failed for host {}: {}".format(
2972                 host_name, err
2973             )
2974             log.debug(msg)
2975             ret.update({host_name: {"Error": msg}})
2976             continue
2977         ret.update({host_name: {"Datetime Updated": True}})
2978     return ret
2979 @depends(HAS_PYVMOMI)
2980 @ignores_kwargs("credstore")
2981 def update_host_password(
2982     host, username, password, new_password, protocol=None, port=None, verify_ssl=True
2983 ):
2984     """
2985     Update the password for a given host.
2986     .. note:: Currently only works with connections to ESXi hosts. Does not work with vCenter servers.
2987     host
2988         The location of the ESXi host.
2989     username
2990         The username used to login to the ESXi host, such as ``root``.
2991     password
2992         The password used to login to the ESXi host.
2993     new_password
2994         The new password that will be updated for the provided username on the ESXi host.
2995     protocol
2996         Optionally set to alternate protocol if the host is not using the default
2997         protocol. Default protocol is ``https``.
2998     port
2999         Optionally set to alternate port if the host is not using the default
3000         port. Default port is ``443``.
3001     verify_ssl
3002         Verify the SSL certificate. Default: True
3003     CLI Example:
3004     .. code-block:: bash
3005         salt '*' vsphere.update_host_password my.esxi.host root original-bad-password new-bad-password
3006     """
3007     service_instance = salt.utils.vmware.get_service_instance(
3008         host=host,
3009         username=username,
3010         password=password,
3011         protocol=protocol,
3012         port=port,
3013         verify_ssl=verify_ssl,
3014     )
3015     account_manager = salt.utils.vmware.get_inventory(service_instance).accountManager
3016     user_account = vim.host.LocalAccountManager.AccountSpecification()
3017     user_account.id = username
3018     user_account.password = new_password
3019     try:
3020         account_manager.UpdateUser(user_account)
3021     except vmodl.fault.SystemError as err:
3022         raise CommandExecutionError(err.msg)
3023     except vim.fault.UserNotFound:
3024         raise CommandExecutionError(
3025             "'vsphere.update_host_password' failed for host {}: "
3026             "User was not found.".format(host)
3027         )
3028     except vim.fault.AlreadyExists:
3029         pass
3030     return True
3031 @depends(HAS_PYVMOMI)
3032 @ignores_kwargs("credstore")
3033 def vmotion_disable(
3034     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
3035 ):
3036     """
3037     Disable vMotion for a given host or list of host_names.
3038     host
3039         The location of the host.
3040     username
3041         The username used to login to the host, such as ``root``.
3042     password
3043         The password used to login to the host.
3044     protocol
3045         Optionally set to alternate protocol if the host is not using the default
3046         protocol. Default protocol is ``https``.
3047     port
3048         Optionally set to alternate port if the host is not using the default
3049         port. Default port is ``443``.
3050     host_names
3051         List of ESXi host names. When the host, username, and password credentials
3052         are provided for a vCenter Server, the host_names argument is required to
3053         tell vCenter which hosts should disable VMotion.
3054         If host_names is not provided, VMotion will be disabled for the ``host``
3055         location instead. This is useful for when service instance connection
3056         information is used for a single ESXi host.
3057     verify_ssl
3058         Verify the SSL certificate. Default: True
3059     CLI Example:
3060     .. code-block:: bash
3061         salt '*' vsphere.vmotion_disable my.esxi.host root bad-password
3062         salt '*' vsphere.vmotion_disable my.vcenter.location root bad-password \
3063         host_names='[esxi-1.host.com, esxi-2.host.com]'
3064     """
3065     service_instance = salt.utils.vmware.get_service_instance(
3066         host=host,
3067         username=username,
3068         password=password,
3069         protocol=protocol,
3070         port=port,
3071         verify_ssl=verify_ssl,
3072     )
3073     host_names = _check_hosts(service_instance, host, host_names)
3074     ret = {}
3075     for host_name in host_names:
3076         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
3077         vmotion_system = host_ref.configManager.vmotionSystem
3078         try:
3079             vmotion_system.DeselectVnic()
3080         except vim.fault.HostConfigFault as err:
3081             msg = "vsphere.vmotion_disable failed: {}".format(err)
3082             log.debug(msg)
3083             ret.update({host_name: {"Error": msg, "VMotion Disabled": False}})
3084             continue
3085         ret.update({host_name: {"VMotion Disabled": True}})
3086     return ret
3087 @depends(HAS_PYVMOMI)
3088 @ignores_kwargs("credstore")
3089 def vmotion_enable(
3090     host,
3091     username,
3092     password,
3093     protocol=None,
3094     port=None,
3095     host_names=None,
3096     device="vmk0",
3097     verify_ssl=True,
3098 ):
3099     """
3100     Enable vMotion for a given host or list of host_names.
3101     host
3102         The location of the host.
3103     username
3104         The username used to login to the host, such as ``root``.
3105     password
3106         The password used to login to the host.
3107     protocol
3108         Optionally set to alternate protocol if the host is not using the default
3109         protocol. Default protocol is ``https``.
3110     port
3111         Optionally set to alternate port if the host is not using the default
3112         port. Default port is ``443``.
3113     host_names
3114         List of ESXi host names. When the host, username, and password credentials
3115         are provided for a vCenter Server, the host_names argument is required to
3116         tell vCenter which hosts should enable VMotion.
3117         If host_names is not provided, VMotion will be enabled for the ``host``
3118         location instead. This is useful for when service instance connection
3119         information is used for a single ESXi host.
3120     device
3121         The device that uniquely identifies the VirtualNic that will be used for
3122         VMotion for each host. Defaults to ``vmk0``.
3123     verify_ssl
3124         Verify the SSL certificate. Default: True
3125     CLI Example:
3126     .. code-block:: bash
3127         salt '*' vsphere.vmotion_enable my.esxi.host root bad-password
3128         salt '*' vsphere.vmotion_enable my.vcenter.location root bad-password \
3129         host_names='[esxi-1.host.com, esxi-2.host.com]'
3130     """
3131     service_instance = salt.utils.vmware.get_service_instance(
3132         host=host,
3133         username=username,
3134         password=password,
3135         protocol=protocol,
3136         port=port,
3137         verify_ssl=verify_ssl,
3138     )
3139     host_names = _check_hosts(service_instance, host, host_names)
3140     ret = {}
3141     for host_name in host_names:
3142         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
3143         vmotion_system = host_ref.configManager.vmotionSystem
3144         try:
3145             vmotion_system.SelectVnic(device)
3146         except vim.fault.HostConfigFault as err:
3147             msg = "vsphere.vmotion_disable failed: {}".format(err)
3148             log.debug(msg)
3149             ret.update({host_name: {"Error": msg, "VMotion Enabled": False}})
3150             continue
3151         ret.update({host_name: {"VMotion Enabled": True}})
3152     return ret
3153 @depends(HAS_PYVMOMI)
3154 @ignores_kwargs("credstore")
3155 def vsan_add_disks(
3156     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
3157 ):
3158     """
3159     Add any VSAN-eligible disks to the VSAN System for the given host or list of host_names.
3160     host
3161         The location of the host.
3162     username
3163         The username used to login to the host, such as ``root``.
3164     password
3165         The password used to login to the host.
3166     protocol
3167         Optionally set to alternate protocol if the host is not using the default
3168         protocol. Default protocol is ``https``.
3169     port
3170         Optionally set to alternate port if the host is not using the default
3171         port. Default port is ``443``.
3172     host_names
3173         List of ESXi host names. When the host, username, and password credentials
3174         are provided for a vCenter Server, the host_names argument is required to
3175         tell vCenter which hosts need to add any VSAN-eligible disks to the host's
3176         VSAN system.
3177         If host_names is not provided, VSAN-eligible disks will be added to the hosts's
3178         VSAN system for the ``host`` location instead. This is useful for when service
3179         instance connection information is used for a single ESXi host.
3180     verify_ssl
3181         Verify the SSL certificate. Default: True
3182     CLI Example:
3183     .. code-block:: bash
3184         salt '*' vsphere.vsan_add_disks my.esxi.host root bad-password
3185         salt '*' vsphere.vsan_add_disks my.vcenter.location root bad-password \
3186         host_names='[esxi-1.host.com, esxi-2.host.com]'
3187     """
3188     service_instance = salt.utils.vmware.get_service_instance(
3189         host=host,
3190         username=username,
3191         password=password,
3192         protocol=protocol,
3193         port=port,
3194         verify_ssl=verify_ssl,
3195     )
3196     host_names = _check_hosts(service_instance, host, host_names)
3197     response = _get_vsan_eligible_disks(service_instance, host, host_names)
3198     ret = {}
3199     for host_name, value in response.items():
3200         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
3201         vsan_system = host_ref.configManager.vsanSystem
3202         if vsan_system is None:
3203             msg = (
3204                 "VSAN System Config Manager is unset for host '{}'. "
3205                 "VSAN configuration cannot be changed without a configured "
3206                 "VSAN System.".format(host_name)
3207             )
3208             log.debug(msg)
3209             ret.update({host_name: {"Error": msg}})
3210         else:
3211             eligible = value.get("Eligible")
3212             error = value.get("Error")
3213             if eligible and isinstance(eligible, list):
3214                 try:
3215                     task = vsan_system.AddDisks(eligible)
3216                     salt.utils.vmware.wait_for_task(
3217                         task, host_name, "Adding disks to VSAN", sleep_seconds=3
3218                     )
3219                 except vim.fault.InsufficientDisks as err:
3220                     log.debug(err.msg)
3221                     ret.update({host_name: {"Error": err.msg}})
3222                     continue
3223                 except Exception as err:  # pylint: disable=broad-except
3224                     msg = "'vsphere.vsan_add_disks' failed for host {}: {}".format(
3225                         host_name, err
3226                     )
3227                     log.debug(msg)
3228                     ret.update({host_name: {"Error": msg}})
3229                     continue
3230                 log.debug(
3231                     "Successfully added disks to the VSAN system for host '%s'.",
3232                     host_name,
3233                 )
3234                 for disk in eligible:
3235                     disk_names.append(disk.canonicalName)
3236                 ret.update({<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>host_name: {"Disks Added": disk_names}})
3237             elif eligible and isinstance(eligible, str):
3238                 ret.update({host_name: {"Disks Added": eligible}})
3239             elif error:
3240                 ret.update({host_name: {"Error": error}})
3241             else:
3242                 ret.update(
3243                     {
3244                         host_name</b></font>: {
3245                             "Disks Added": (
3246                                 "No new VSAN-eligible disks were found to add."
3247                             )
3248                         }
3249                     }
3250                 )
3251     return ret
3252 @depends(HAS_PYVMOMI)
3253 @ignores_kwargs("credstore")
3254 def vsan_disable(
3255     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
3256 ):
3257     """
3258     Disable VSAN for a given host or list of host_names.
3259     host
3260         The location of the host.
3261     username
3262         The username used to login to the host, such as ``root``.
3263     password
3264         The password used to login to the host.
3265     protocol
3266         Optionally set to alternate protocol if the host is not using the default
3267         protocol. Default protocol is ``https``.
3268     port
3269         Optionally set to alternate port if the host is not using the default
3270         port. Default port is ``443``.
3271     host_names
3272         List of ESXi host names. When the host, username, and password credentials
3273         are provided for a vCenter Server, the host_names argument is required to
3274         tell vCenter which hosts should disable VSAN.
3275         If host_names is not provided, VSAN will be disabled for the ``host``
3276         location instead. This is useful for when service instance connection
3277         information is used for a single ESXi host.
3278     verify_ssl
3279         Verify the SSL certificate. Default: True
3280     CLI Example:
3281     .. code-block:: bash
3282         salt '*' vsphere.vsan_disable my.esxi.host root bad-password
3283         salt '*' vsphere.vsan_disable my.vcenter.location root bad-password \
3284         host_names='[esxi-1.host.com, esxi-2.host.com]'
3285     """
3286     service_instance = salt.utils.vmware.get_service_instance(
3287         host=host,
3288         username=username,
3289         password=password,
3290         protocol=protocol,
3291         port=port,
3292         verify_ssl=verify_ssl,
3293     )
3294     vsan_config = vim.vsan.host.ConfigInfo()
3295     vsan_config.enabled = False
3296     host_names = _check_hosts(service_instance, host, host_names)
3297     ret = {}
3298     for host_name in host_names:
3299         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
3300         vsan_system = host_ref.configManager.vsanSystem
3301         if vsan_system is None:
3302             msg = (
3303                 "VSAN System Config Manager is unset for host '{}'. "
3304                 "VSAN configuration cannot be changed without a configured "
3305                 "VSAN System.".format(host_name)
3306             )
3307             log.debug(msg)
3308             ret.update({host_name: {"Error": msg}})
3309         else:
3310             try:
3311                 task = vsan_system.UpdateVsan_Task(vsan_config)
3312                 salt.utils.vmware.wait_for_task(
3313                     task, host_name, "Disabling VSAN", sleep_seconds=3
3314                 )
3315             except vmodl.fault.SystemError as err:
3316                 log.debug(err.msg)
3317                 ret.update({host_name: {"Error": err.msg}})
3318                 continue
3319             except Exception as err:  # pylint: disable=broad-except
3320                 msg = "'vsphere.vsan_disable' failed for host {}: {}".format(
3321                     host_name, err
3322                 )
3323                 log.debug(msg)
3324                 ret.update({host_name: {"Error": msg}})
3325                 continue
3326             ret.update({host_name: {"VSAN Disabled": True}})
3327     return ret
3328 @depends(HAS_PYVMOMI)
3329 @ignores_kwargs("credstore")
3330 def vsan_enable(
3331     host, username, password, protocol=None, port=None, host_names=None, verify_ssl=True
3332 ):
3333     """
3334     Enable VSAN for a given host or list of host_names.
3335     host
3336         The location of the host.
3337     username
3338         The username used to login to the host, such as ``root``.
3339     password
3340         The password used to login to the host.
3341     protocol
3342         Optionally set to alternate protocol if the host is not using the default
3343         protocol. Default protocol is ``https``.
3344     port
3345         Optionally set to alternate port if the host is not using the default
3346         port. Default port is ``443``.
3347     host_names
3348         List of ESXi host names. When the host, username, and password credentials
3349         are provided for a vCenter Server, the host_names argument is required to
3350         tell vCenter which hosts should enable VSAN.
3351         If host_names is not provided, VSAN will be enabled for the ``host``
3352         location instead. This is useful for when service instance connection
3353         information is used for a single ESXi host.
3354     verify_ssl
3355         Verify the SSL certificate. Default: True
3356     CLI Example:
3357     .. code-block:: bash
3358         salt '*' vsphere.vsan_enable my.esxi.host root bad-password
3359         salt '*' vsphere.vsan_enable my.vcenter.location root bad-password \
3360         host_names='[esxi-1.host.com, esxi-2.host.com]'
3361     """
3362     service_instance = salt.utils.vmware.get_service_instance(
3363         host=host,
3364         username=username,
3365         password=password,
3366         protocol=protocol,
3367         port=port,
3368         verify_ssl=verify_ssl,
3369     )
3370     vsan_config = vim.vsan.host.ConfigInfo()
3371     vsan_config.enabled = True
3372     host_names = _check_hosts(service_instance, host, host_names)
3373     ret = {}
3374     for host_name in host_names:
3375         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
3376         vsan_system = host_ref.configManager.vsanSystem
3377         if vsan_system is None:
3378             msg = (
3379                 "VSAN System Config Manager is unset for host '{}'. "
3380                 "VSAN configuration cannot be changed without a configured "
3381                 "VSAN System.".format(host_name)
3382             )
3383             log.debug(msg)
3384             ret.update({host_name: {"Error": msg}})
3385         else:
3386             try:
3387                 task = vsan_system.UpdateVsan_Task(vsan_config)
3388                 salt.utils.vmware.wait_for_task(
3389                     task, host_name, "Enabling VSAN", sleep_seconds=3
3390                 )
3391             except vmodl.fault.SystemError as err:
3392                 log.debug(err.msg)
3393                 ret.update({host_name: {"Error": err.msg}})
3394                 continue
3395             except vim.fault.VsanFault as err:
3396                 msg = "'vsphere.vsan_enable' failed for host {}: {}".format(
3397                     host_name, err
3398                 )
3399                 log.debug(msg)
3400                 ret.update({host_name: {"Error": msg}})
3401                 continue
3402             ret.update({host_name: {"VSAN Enabled": True}})
3403     return ret
3404 def _get_dvs_config_dict(dvs_name, dvs_config):
3405     """
3406     Returns the dict representation of the DVS config
3407     dvs_name
3408         The name of the DVS
3409     dvs_config
3410         The DVS config
3411     """
3412     log.trace("Building the dict of the DVS '%s' config", dvs_name)
3413     conf_dict = {
3414         "name": dvs_name,
3415         "contact_email": dvs_config.contact.contact,
3416         "contact_name": dvs_config.contact.name,
3417         "description": dvs_config.description,
3418         "lacp_api_version": dvs_config.lacpApiVersion,
3419         "network_resource_control_version": dvs_config.networkResourceControlVersion,
3420         "network_resource_management_enabled": dvs_config.networkResourceManagementEnabled,
3421         "max_mtu": dvs_config.maxMtu,
3422     }
3423     if isinstance(dvs_config.uplinkPortPolicy, vim.DVSNameArrayUplinkPortPolicy):
3424         conf_dict.update({"uplink_names": dvs_config.uplinkPortPolicy.uplinkPortName})
3425     return conf_dict
3426 def _get_dvs_link_discovery_protocol(dvs_name, dvs_link_disc_protocol):
3427     """
3428     Returns the dict representation of the DVS link discovery protocol
3429     dvs_name
3430         The name of the DVS
3431     dvs_link_disc_protocl
3432         The DVS link discovery protocol
3433     """
3434     log.trace("Building the dict of the DVS '%s' link discovery protocol", dvs_name)
3435     return {
3436         "operation": dvs_link_disc_protocol.operation,
3437         "protocol": dvs_link_disc_protocol.protocol,
3438     }
3439 def _get_dvs_product_info(dvs_name, dvs_product_info):
3440     """
3441     Returns the dict representation of the DVS product_info
3442     dvs_name
3443         The name of the DVS
3444     dvs_product_info
3445         The DVS product info
3446     """
3447     log.trace("Building the dict of the DVS '%s' product info", dvs_name)
3448     return {
3449         "name": dvs_product_info.name,
3450         "vendor": dvs_product_info.vendor,
3451         "version": dvs_product_info.version,
3452     }
3453 def _get_dvs_capability(dvs_name, dvs_capability):
3454     """
3455     Returns the dict representation of the DVS product_info
3456     dvs_name
3457         The name of the DVS
3458     dvs_capability
3459         The DVS capability
3460     """
3461     log.trace("Building the dict of the DVS '%s' capability", dvs_name)
3462     return {
3463         "operation_supported": dvs_capability.dvsOperationSupported,
3464         "portgroup_operation_supported": dvs_capability.dvPortGroupOperationSupported,
3465         "port_operation_supported": dvs_capability.dvPortOperationSupported,
3466     }
3467 def _get_dvs_infrastructure_traffic_resources(dvs_name, dvs_infra_traffic_ress):
3468     """
3469     Returns a list of dict representations of the DVS infrastructure traffic
3470     resource
3471     dvs_name
3472         The name of the DVS
3473     dvs_infra_traffic_ress
3474         The DVS infrastructure traffic resources
3475     """
3476     log.trace(
3477         "Building the dicts of the DVS '%s' infrastructure traffic resources", dvs_name
3478     )
3479     res_dicts = []
3480     for res in dvs_infra_traffic_ress:
3481         res_dict = {
3482             "key": res.key,
3483             "limit": res.allocationInfo.limit,
3484             "reservation": res.allocationInfo.reservation,
3485         }
3486         if res.allocationInfo.shares:
3487             res_dict.update(
3488                 {
3489                     "num_shares": res.allocationInfo.shares.shares,
3490                     "share_level": res.allocationInfo.shares.level,
3491                 }
3492             )
3493         res_dicts.append(res_dict)
3494     return res_dicts
3495 @depends(HAS_PYVMOMI)
3496 @_supports_proxies("esxdatacenter", "esxcluster")
3497 @_gets_service_instance_via_proxy
3498 def list_dvss(datacenter=None, dvs_names=None, service_instance=None):
3499     """
3500     Returns a list of distributed virtual switches (DVSs).
3501     The list can be filtered by the datacenter or DVS names.
3502     datacenter
3503         The datacenter to look for DVSs in.
3504         Default value is None.
3505     dvs_names
3506         List of DVS names to look for. If None, all DVSs are returned.
3507         Default value is None.
3508     .. code-block:: bash
3509         salt '*' vsphere.list_dvss
3510         salt '*' vsphere.list_dvss dvs_names=[dvs1,dvs2]
3511     """
3512     ret_list = []
3513     proxy_type = get_proxy_type()
3514     if proxy_type == "esxdatacenter":
3515         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
3516         dc_ref = _get_proxy_target(service_instance)
3517     elif proxy_type == "esxcluster":
3518         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3519         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3520     for dvs in salt.utils.vmware.get_dvss(dc_ref, dvs_names, (not dvs_names)):
3521         dvs_dict = {}
3522         props = salt.utils.vmware.get_properties_of_managed_object(
3523             dvs, ["name", "config", "capability", "networkResourcePool"]
3524         )
3525         dvs_dict = _get_dvs_config_dict(props["name"], props["config"])
3526         dvs_dict.update(
3527             {
3528                 "product_info": _get_dvs_product_info(
3529                     props["name"], props["config"].productInfo
3530                 )
3531             }
3532         )
3533         if props["config"].linkDiscoveryProtocolConfig:
3534             dvs_dict.update(
3535                 {
3536                     "link_discovery_protocol": _get_dvs_link_discovery_protocol(
3537                         props["name"], props["config"].linkDiscoveryProtocolConfig
3538                     )
3539                 }
3540             )
3541         dvs_dict.update(
3542             {"capability": _get_dvs_capability(props["name"], props["capability"])}
3543         )
3544         if hasattr(props["config"], "infrastructureTrafficResourceConfig"):
3545             dvs_dict.update(
3546                 {
3547                     "infrastructure_traffic_resource_pools": _get_dvs_infrastructure_traffic_resources(
3548                         props["name"],
3549                         props["config"].infrastructureTrafficResourceConfig,
3550                     )
3551                 }
3552             )
3553         ret_list.append(dvs_dict)
3554     return ret_list
3555 def _apply_dvs_config(config_spec, config_dict):
3556     """
3557     Applies the values of the config dict dictionary to a config spec
3558     (vim.VMwareDVSConfigSpec)
3559     """
3560     if config_dict.get("name"):
3561         config_spec.name = config_dict["name"]
3562     if config_dict.get("contact_email") or config_dict.get("contact_name"):
3563         if not config_spec.contact:
3564             config_spec.contact = vim.DVSContactInfo()
3565         config_spec.contact.contact = config_dict.get("contact_email")
3566         config_spec.contact.name = config_dict.get("contact_name")
3567     if config_dict.get("description"):
3568         config_spec.description = config_dict.get("description")
3569     if config_dict.get("max_mtu"):
3570         config_spec.maxMtu = config_dict.get("max_mtu")
3571     if config_dict.get("lacp_api_version"):
3572         config_spec.lacpApiVersion = config_dict.get("lacp_api_version")
3573     if config_dict.get("network_resource_control_version"):
3574         config_spec.networkResourceControlVersion = config_dict.get(
3575             "network_resource_control_version"
3576         )
3577     if config_dict.get("uplink_names"):
3578         if not config_spec.uplinkPortPolicy or not isinstance(
3579             config_spec.uplinkPortPolicy, vim.DVSNameArrayUplinkPortPolicy
3580         ):
3581             config_spec.uplinkPortPolicy = vim.DVSNameArrayUplinkPortPolicy()
3582         config_spec.uplinkPortPolicy.uplinkPortName = config_dict["uplink_names"]
3583 def _apply_dvs_link_discovery_protocol(disc_prot_config, disc_prot_dict):
3584     """
3585     Applies the values of the disc_prot_dict dictionary to a link discovery
3586     protocol config object (vim.LinkDiscoveryProtocolConfig)
3587     """
3588     disc_prot_config.operation = disc_prot_dict["operation"]
3589     disc_prot_config.protocol = disc_prot_dict["protocol"]
3590 def _apply_dvs_product_info(product_info_spec, product_info_dict):
3591     """
3592     Applies the values of the product_info_dict dictionary to a product info
3593     spec (vim.DistributedVirtualSwitchProductSpec)
3594     """
3595     if product_info_dict.get("name"):
3596         product_info_spec.name = product_info_dict["name"]
3597     if product_info_dict.get("vendor"):
3598         product_info_spec.vendor = product_info_dict["vendor"]
3599     if product_info_dict.get("version"):
3600         product_info_spec.version = product_info_dict["version"]
3601 def _apply_dvs_capability(capability_spec, capability_dict):
3602     """
3603     Applies the values of the capability_dict dictionary to a DVS capability
3604     object (vim.vim.DVSCapability)
3605     """
3606     if "operation_supported" in capability_dict:
3607         capability_spec.dvsOperationSupported = capability_dict["operation_supported"]
3608     if "port_operation_supported" in capability_dict:
3609         capability_spec.dvPortOperationSupported = capability_dict[
3610             "port_operation_supported"
3611         ]
3612     if "portgroup_operation_supported" in capability_dict:
3613         capability_spec.dvPortGroupOperationSupported = capability_dict[
3614             "portgroup_operation_supported"
3615         ]
3616 def _apply_dvs_infrastructure_traffic_resources(
3617     infra_traffic_resources, resource_dicts
3618 ):
3619     """
3620     Applies the values of the resource dictionaries to infra traffic resources,
3621     creating the infra traffic resource if required
3622     (vim.DistributedVirtualSwitchProductSpec)
3623     """
3624     for res_dict in resource_dicts:
3625         filtered_traffic_resources = [
3626             r for r in infra_traffic_resources if r.key == res_dict["key"]
3627         ]
3628         if filtered_traffic_resources:
3629             traffic_res = filtered_traffic_resources[0]
3630         else:
3631             traffic_res = vim.DvsHostInfrastructureTrafficResource()
3632             traffic_res.key = res_dict["key"]
3633             traffic_res.allocationInfo = (
3634                 vim.DvsHostInfrastructureTrafficResourceAllocation()
3635             )
3636             infra_traffic_resources.append(traffic_res)
3637         if res_dict.get("limit"):
3638             traffic_res.allocationInfo.limit = res_dict["limit"]
3639         if res_dict.get("reservation"):
3640             traffic_res.allocationInfo.reservation = res_dict["reservation"]
3641         if res_dict.get("num_shares") or res_dict.get("share_level"):
3642             if not traffic_res.allocationInfo.shares:
3643                 traffic_res.allocationInfo.shares = vim.SharesInfo()
3644         if res_dict.get("share_level"):
3645             traffic_res.allocationInfo.shares.level = vim.SharesLevel(
3646                 res_dict["share_level"]
3647             )
3648         if res_dict.get("num_shares"):
3649             traffic_res.allocationInfo.shares.shares = res_dict["num_shares"]
3650 def _apply_dvs_network_resource_pools(network_resource_pools, resource_dicts):
3651     """
3652     Applies the values of the resource dictionaries to network resource pools,
3653     creating the resource pools if required
3654     (vim.DVSNetworkResourcePoolConfigSpec)
3655     """
3656     for res_dict in resource_dicts:
3657         ress = [r for r in network_resource_pools if r.key == res_dict["key"]]
3658         if ress:
3659             res = ress[0]
3660         else:
3661             res = vim.DVSNetworkResourcePoolConfigSpec()
3662             res.key = res_dict["key"]
3663             res.allocationInfo = vim.DVSNetworkResourcePoolAllocationInfo()
3664             network_resource_pools.append(res)
3665         if res_dict.get("limit"):
3666             res.allocationInfo.limit = res_dict["limit"]
3667         if res_dict.get("num_shares") and res_dict.get("share_level"):
3668             if not res.allocationInfo.shares:
3669                 res.allocationInfo.shares = vim.SharesInfo()
3670             res.allocationInfo.shares.shares = res_dict["num_shares"]
3671             res.allocationInfo.shares.level = vim.SharesLevel(res_dict["share_level"])
3672 @depends(HAS_PYVMOMI)
3673 @_supports_proxies("esxdatacenter", "esxcluster")
3674 @_gets_service_instance_via_proxy
3675 def create_dvs(dvs_dict, dvs_name, service_instance=None):
3676     """
3677     Creates a distributed virtual switch (DVS).
3678     Note: The ``dvs_name`` param will override any name set in ``dvs_dict``.
3679     dvs_dict
3680         Dict representation of the new DVS (example in salt.states.dvs)
3681     dvs_name
3682         Name of the DVS to be created.
3683     service_instance
3684         Service instance (vim.ServiceInstance) of the vCenter.
3685         Default is None.
3686     .. code-block:: bash
3687         salt '*' vsphere.create_dvs dvs dict=$dvs_dict dvs_name=dvs_name
3688     """
3689     log.trace("Creating dvs '%s' with dict = %s", dvs_name, dvs_dict)
3690     proxy_type = get_proxy_type()
3691     if proxy_type == "esxdatacenter":
3692         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
3693         dc_ref = _get_proxy_target(service_instance)
3694     elif proxy_type == "esxcluster":
3695         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3696         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3697     dvs_dict["name"] = dvs_name
3698     dvs_create_spec = vim.DVSCreateSpec()
3699     dvs_create_spec.configSpec = vim.VMwareDVSConfigSpec()
3700     _apply_dvs_config(dvs_create_spec.configSpec, dvs_dict)
3701     if dvs_dict.get("product_info"):
3702         dvs_create_spec.productInfo = vim.DistributedVirtualSwitchProductSpec()
3703         _apply_dvs_product_info(dvs_create_spec.productInfo, dvs_dict["product_info"])
3704     if dvs_dict.get("capability"):
3705         dvs_create_spec.capability = vim.DVSCapability()
3706         _apply_dvs_capability(dvs_create_spec.capability, dvs_dict["capability"])
3707     if dvs_dict.get("link_discovery_protocol"):
3708         dvs_create_spec.configSpec.linkDiscoveryProtocolConfig = (
3709             vim.LinkDiscoveryProtocolConfig()
3710         )
3711         _apply_dvs_link_discovery_protocol(
3712             dvs_create_spec.configSpec.linkDiscoveryProtocolConfig,
3713             dvs_dict["link_discovery_protocol"],
3714         )
3715     if dvs_dict.get("infrastructure_traffic_resource_pools"):
3716         dvs_create_spec.configSpec.infrastructureTrafficResourceConfig = []
3717         _apply_dvs_infrastructure_traffic_resources(
3718             dvs_create_spec.configSpec.infrastructureTrafficResourceConfig,
3719             dvs_dict["infrastructure_traffic_resource_pools"],
3720         )
3721     log.trace("dvs_create_spec = %s", dvs_create_spec)
3722     salt.utils.vmware.create_dvs(dc_ref, dvs_name, dvs_create_spec)
3723     if "network_resource_management_enabled" in dvs_dict:
3724         dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs_name])
3725         if not dvs_refs:
3726             raise VMwareObjectRetrievalError(
3727                 "DVS '{}' wasn't found in datacenter '{}'".format(dvs_name, datacenter)
3728             )
3729         dvs_ref = dvs_refs[0]
3730         salt.utils.vmware.set_dvs_network_resource_management_enabled(
3731             dvs_ref, dvs_dict["network_resource_management_enabled"]
3732         )
3733     return True
3734 @depends(HAS_PYVMOMI)
3735 @_supports_proxies("esxdatacenter", "esxcluster")
3736 @_gets_service_instance_via_proxy
3737 def update_dvs(dvs_dict, dvs, service_instance=None):
3738     """
3739     Updates a distributed virtual switch (DVS).
3740     Note: Updating the product info, capability, uplinks of a DVS is not
3741           supported so the corresponding entries in ``dvs_dict`` will be
3742           ignored.
3743     dvs_dict
3744         Dictionary with the values the DVS should be update with
3745         (example in salt.states.dvs)
3746     dvs
3747         Name of the DVS to be updated.
3748     service_instance
3749         Service instance (vim.ServiceInstance) of the vCenter.
3750         Default is None.
3751     .. code-block:: bash
3752         salt '*' vsphere.update_dvs dvs_dict=$dvs_dict dvs=dvs1
3753     """
3754     log.trace("Updating dvs '%s' with dict = %s", dvs, dvs_dict)
3755     for prop in ["product_info", "capability", "uplink_names", "name"]:
3756         if prop in dvs_dict:
3757             del dvs_dict[prop]
3758     proxy_type = get_proxy_type()
3759     if proxy_type == "esxdatacenter":
3760         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
3761         dc_ref = _get_proxy_target(service_instance)
3762     elif proxy_type == "esxcluster":
3763         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3764         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3765     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
3766     if not dvs_refs:
3767         raise VMwareObjectRetrievalError(
3768             "DVS '{}' wasn't found in datacenter '{}'".format(dvs, datacenter)
3769         )
3770     dvs_ref = dvs_refs[0]
3771     dvs_props = salt.utils.vmware.get_properties_of_managed_object(
3772         dvs_ref, ["config", "capability"]
3773     )
3774     dvs_config = vim.VMwareDVSConfigSpec()
3775     skipped_properties = ["host"]
3776     for prop in dvs_config.__dict__.keys():
3777         if prop in skipped_properties:
3778             continue
3779         if hasattr(dvs_props["config"], prop):
3780             setattr(dvs_config, prop, getattr(dvs_props["config"], prop))
3781     _apply_dvs_config(dvs_config, dvs_dict)
3782     if dvs_dict.get("link_discovery_protocol"):
3783         if not dvs_config.linkDiscoveryProtocolConfig:
3784             dvs_config.linkDiscoveryProtocolConfig = vim.LinkDiscoveryProtocolConfig()
3785         _apply_dvs_link_discovery_protocol(
3786             dvs_config.linkDiscoveryProtocolConfig, dvs_dict["link_discovery_protocol"]
3787         )
3788     if dvs_dict.get("infrastructure_traffic_resource_pools"):
3789         if not dvs_config.infrastructureTrafficResourceConfig:
3790             dvs_config.infrastructureTrafficResourceConfig = []
3791         _apply_dvs_infrastructure_traffic_resources(
3792             dvs_config.infrastructureTrafficResourceConfig,
3793             dvs_dict["infrastructure_traffic_resource_pools"],
3794         )
3795     log.trace("dvs_config= %s", dvs_config)
3796     salt.utils.vmware.update_dvs(dvs_ref, dvs_config_spec=dvs_config)
3797     if "network_resource_management_enabled" in dvs_dict:
3798         salt.utils.vmware.set_dvs_network_resource_management_enabled(
3799             dvs_ref, dvs_dict["network_resource_management_enabled"]
3800         )
3801     return True
3802 def _get_dvportgroup_out_shaping(pg_name, pg_default_port_config):
3803     """
3804     Returns the out shaping policy of a distributed virtual portgroup
3805     pg_name
3806         The name of the portgroup
3807     pg_default_port_config
3808         The dafault port config of the portgroup
3809     """
3810     log.trace("Retrieving portgroup's '%s' out shaping config", pg_name)
3811     out_shaping_policy = pg_default_port_config.outShapingPolicy
3812     if not out_shaping_policy:
3813         return {}
3814     return {
3815         "average_bandwidth": out_shaping_policy.averageBandwidth.value,
3816         "burst_size": out_shaping_policy.burstSize.value,
3817         "enabled": out_shaping_policy.enabled.value,
3818         "peak_bandwidth": out_shaping_policy.peakBandwidth.value,
3819     }
3820 def _get_dvportgroup_security_policy(pg_name, pg_default_port_config):
3821     """
3822     Returns the security policy of a distributed virtual portgroup
3823     pg_name
3824         The name of the portgroup
3825     pg_default_port_config
3826         The dafault port config of the portgroup
3827     """
3828     log.trace("Retrieving portgroup's '%s' security policy config", pg_name)
3829     sec_policy = pg_default_port_config.securityPolicy
3830     if not sec_policy:
3831         return {}
3832     return {
3833         "allow_promiscuous": sec_policy.allowPromiscuous.value,
3834         "forged_transmits": sec_policy.forgedTransmits.value,
3835         "mac_changes": sec_policy.macChanges.value,
3836     }
3837 def _get_dvportgroup_teaming(pg_name, pg_default_port_config):
3838     """
3839     Returns the teaming of a distributed virtual portgroup
3840     pg_name
3841         The name of the portgroup
3842     pg_default_port_config
3843         The dafault port config of the portgroup
3844     """
3845     log.trace("Retrieving portgroup's '%s' teaming config", pg_name)
3846     teaming_policy = pg_default_port_config.uplinkTeamingPolicy
3847     if not teaming_policy:
3848         return {}
3849     ret_dict = {
3850         "notify_switches": teaming_policy.notifySwitches.value,
3851         "policy": teaming_policy.policy.value,
3852         "reverse_policy": teaming_policy.reversePolicy.value,
3853         "rolling_order": teaming_policy.rollingOrder.value,
3854     }
3855     if teaming_policy.failureCriteria:
3856         failure_criteria = teaming_policy.failureCriteria
3857         ret_dict.update(
3858             {
3859                 "failure_criteria": {
3860                     "check_beacon": failure_criteria.checkBeacon.value,
3861                     "check_duplex": failure_criteria.checkDuplex.value,
3862                     "check_error_percent": failure_criteria.checkErrorPercent.value,
3863                     "check_speed": failure_criteria.checkSpeed.value,
3864                     "full_duplex": failure_criteria.fullDuplex.value,
3865                     "percentage": failure_criteria.percentage.value,
3866                     "speed": failure_criteria.speed.value,
3867                 }
3868             }
3869         )
3870     if teaming_policy.uplinkPortOrder:
3871         uplink_order = teaming_policy.uplinkPortOrder
3872         ret_dict.update(
3873             {
3874                 "port_order": {
3875                     "active": uplink_order.activeUplinkPort,
3876                     "standby": uplink_order.standbyUplinkPort,
3877                 }
3878             }
3879         )
3880     return ret_dict
3881 def _get_dvportgroup_dict(pg_ref):
3882     """
3883     Returns a dictionary with a distributed virtual portgroup data
3884     pg_ref
3885         Portgroup reference
3886     """
3887     props = salt.utils.vmware.get_properties_of_managed_object(
3888         pg_ref,
3889         [
3890             "name",
3891             "config.description",
3892             "config.numPorts",
3893             "config.type",
3894             "config.defaultPortConfig",
3895         ],
3896     )
3897     pg_dict = {
3898         "name": props["name"],
3899         "description": props.get("config.description"),
3900         "num_ports": props["config.numPorts"],
3901         "type": props["config.type"],
3902     }
3903     if props["config.defaultPortConfig"]:
3904         dpg = props["config.defaultPortConfig"]
3905         if dpg.vlan and isinstance(
3906             dpg.vlan, vim.VmwareDistributedVirtualSwitchVlanIdSpec
3907         ):
3908             pg_dict.update({"vlan_id": dpg.vlan.vlanId})
3909         pg_dict.update(
3910             {
3911                 "out_shaping": _get_dvportgroup_out_shaping(
3912                     props["name"], props["config.defaultPortConfig"]
3913                 )
3914             }
3915         )
3916         pg_dict.update(
3917             {
3918                 "security_policy": _get_dvportgroup_security_policy(
3919                     props["name"], props["config.defaultPortConfig"]
3920                 )
3921             }
3922         )
3923         pg_dict.update(
3924             {
3925                 "teaming": _get_dvportgroup_teaming(
3926                     props["name"], props["config.defaultPortConfig"]
3927                 )
3928             }
3929         )
3930     return pg_dict
3931 @depends(HAS_PYVMOMI)
3932 @_supports_proxies("esxdatacenter", "esxcluster")
3933 @_gets_service_instance_via_proxy
3934 def list_dvportgroups(dvs=None, portgroup_names=None, service_instance=None):
3935     """
3936     Returns a list of distributed virtual switch portgroups.
3937     The list can be filtered by the portgroup names or by the DVS.
3938     dvs
3939         Name of the DVS containing the portgroups.
3940         Default value is None.
3941     portgroup_names
3942         List of portgroup names to look for. If None, all portgroups are
3943         returned.
3944         Default value is None
3945     service_instance
3946         Service instance (vim.ServiceInstance) of the vCenter.
3947         Default is None.
3948     .. code-block:: bash
3949         salt '*' vsphere.list_dvporgroups
3950         salt '*' vsphere.list_dvportgroups dvs=dvs1
3951         salt '*' vsphere.list_dvportgroups portgroup_names=[pg1]
3952         salt '*' vsphere.list_dvportgroups dvs=dvs1 portgroup_names=[pg1]
3953     """
3954     ret_dict = []
3955     proxy_type = get_proxy_type()
3956     if proxy_type == "esxdatacenter":
3957         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
3958         dc_ref = _get_proxy_target(service_instance)
3959     elif proxy_type == "esxcluster":
3960         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3961         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3962     if dvs:
3963         dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
3964         if not dvs_refs:
3965             raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
3966         dvs_ref = dvs_refs[0]
3967     get_all_portgroups = True if not portgroup_names else False
3968     for pg_ref in salt.utils.vmware.get_dvportgroups(
3969         parent_ref=dvs_ref if dvs else dc_ref,
3970         portgroup_names=portgroup_names,
3971         get_all_portgroups=get_all_portgroups,
3972     ):
3973         ret_dict.append(_get_dvportgroup_dict(pg_ref))
3974     return ret_dict
3975 @depends(HAS_PYVMOMI)
3976 @_supports_proxies("esxdatacenter", "esxcluster")
3977 @_gets_service_instance_via_proxy
3978 def list_uplink_dvportgroup(dvs, service_instance=None):
3979     """
3980     Returns the uplink portgroup of a distributed virtual switch.
3981     dvs
3982         Name of the DVS containing the portgroup.
3983     service_instance
3984         Service instance (vim.ServiceInstance) of the vCenter.
3985         Default is None.
3986     .. code-block:: bash
3987         salt '*' vsphere.list_uplink_dvportgroup dvs=dvs_name
3988     """
3989     proxy_type = get_proxy_type()
3990     if proxy_type == "esxdatacenter":
3991         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
3992         dc_ref = _get_proxy_target(service_instance)
3993     elif proxy_type == "esxcluster":
3994         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
3995         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
3996     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
3997     if not dvs_refs:
3998         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
3999     uplink_pg_ref = salt.utils.vmware.get_uplink_dvportgroup(dvs_refs[0])
4000     return _get_dvportgroup_dict(uplink_pg_ref)
4001 def _apply_dvportgroup_out_shaping(pg_name, out_shaping, out_shaping_conf):
4002     """
4003     Applies the values in out_shaping_conf to an out_shaping object
4004     pg_name
4005         The name of the portgroup
4006     out_shaping
4007         The vim.DVSTrafficShapingPolicy to apply the config to
4008     out_shaping_conf
4009         The out shaping config
4010     """
4011     log.trace("Building portgroup's '%s' out shaping policy", pg_name)
4012     if out_shaping_conf.get("average_bandwidth"):
4013         out_shaping.averageBandwidth = vim.LongPolicy()
4014         out_shaping.averageBandwidth.value = out_shaping_conf["average_bandwidth"]
4015     if out_shaping_conf.get("burst_size"):
4016         out_shaping.burstSize = vim.LongPolicy()
4017         out_shaping.burstSize.value = out_shaping_conf["burst_size"]
4018     if "enabled" in out_shaping_conf:
4019         out_shaping.enabled = vim.BoolPolicy()
4020         out_shaping.enabled.value = out_shaping_conf["enabled"]
4021     if out_shaping_conf.get("peak_bandwidth"):
4022         out_shaping.peakBandwidth = vim.LongPolicy()
4023         out_shaping.peakBandwidth.value = out_shaping_conf["peak_bandwidth"]
4024 def _apply_dvportgroup_security_policy(pg_name, sec_policy, sec_policy_conf):
4025     """
4026     Applies the values in sec_policy_conf to a security policy object
4027     pg_name
4028         The name of the portgroup
4029     sec_policy
4030         The vim.DVSTrafficShapingPolicy to apply the config to
4031     sec_policy_conf
4032         The out shaping config
4033     """
4034     log.trace("Building portgroup's '%s' security policy", pg_name)
4035     if "allow_promiscuous" in sec_policy_conf:
4036         sec_policy.allowPromiscuous = vim.BoolPolicy()
4037         sec_policy.allowPromiscuous.value = sec_policy_conf["allow_promiscuous"]
4038     if "forged_transmits" in sec_policy_conf:
4039         sec_policy.forgedTransmits = vim.BoolPolicy()
4040         sec_policy.forgedTransmits.value = sec_policy_conf["forged_transmits"]
4041     if "mac_changes" in sec_policy_conf:
4042         sec_policy.macChanges = vim.BoolPolicy()
4043         sec_policy.macChanges.value = sec_policy_conf["mac_changes"]
4044 def _apply_dvportgroup_teaming(pg_name, teaming, teaming_conf):
4045     """
4046     Applies the values in teaming_conf to a teaming policy object
4047     pg_name
4048         The name of the portgroup
4049     teaming
4050         The vim.VmwareUplinkPortTeamingPolicy to apply the config to
4051     teaming_conf
4052         The teaming config
4053     """
4054     log.trace("Building portgroup's '%s' teaming", pg_name)
4055     if "notify_switches" in teaming_conf:
4056         teaming.notifySwitches = vim.BoolPolicy()
4057         teaming.notifySwitches.value = teaming_conf["notify_switches"]
4058     if "policy" in teaming_conf:
4059         teaming.policy = vim.StringPolicy()
4060         teaming.policy.value = teaming_conf["policy"]
4061     if "reverse_policy" in teaming_conf:
4062         teaming.reversePolicy = vim.BoolPolicy()
4063         teaming.reversePolicy.value = teaming_conf["reverse_policy"]
4064     if "rolling_order" in teaming_conf:
4065         teaming.rollingOrder = vim.BoolPolicy()
4066         teaming.rollingOrder.value = teaming_conf["rolling_order"]
4067     if "failure_criteria" in teaming_conf:
4068         if not teaming.failureCriteria:
4069             teaming.failureCriteria = vim.DVSFailureCriteria()
4070         failure_criteria_conf = teaming_conf["failure_criteria"]
4071         if "check_beacon" in failure_criteria_conf:
4072             teaming.failureCriteria.checkBeacon = vim.BoolPolicy()
4073             teaming.failureCriteria.checkBeacon.value = failure_criteria_conf[
4074                 "check_beacon"
4075             ]
4076         if "check_duplex" in failure_criteria_conf:
4077             teaming.failureCriteria.checkDuplex = vim.BoolPolicy()
4078             teaming.failureCriteria.checkDuplex.value = failure_criteria_conf[
4079                 "check_duplex"
4080             ]
4081         if "check_error_percent" in failure_criteria_conf:
4082             teaming.failureCriteria.checkErrorPercent = vim.BoolPolicy()
4083             teaming.failureCriteria.checkErrorPercent.value = failure_criteria_conf[
4084                 "check_error_percent"
4085             ]
4086         if "check_speed" in failure_criteria_conf:
4087             teaming.failureCriteria.checkSpeed = vim.StringPolicy()
4088             teaming.failureCriteria.checkSpeed.value = failure_criteria_conf[
4089                 "check_speed"
4090             ]
4091         if "full_duplex" in failure_criteria_conf:
4092             teaming.failureCriteria.fullDuplex = vim.BoolPolicy()
4093             teaming.failureCriteria.fullDuplex.value = failure_criteria_conf[
4094                 "full_duplex"
4095             ]
4096         if "percentage" in failure_criteria_conf:
4097             teaming.failureCriteria.percentage = vim.IntPolicy()
4098             teaming.failureCriteria.percentage.value = failure_criteria_conf[
4099                 "percentage"
4100             ]
4101         if "speed" in failure_criteria_conf:
4102             teaming.failureCriteria.speed = vim.IntPolicy()
4103             teaming.failureCriteria.speed.value = failure_criteria_conf["speed"]
4104     if "port_order" in teaming_conf:
4105         if not teaming.uplinkPortOrder:
4106             teaming.uplinkPortOrder = vim.VMwareUplinkPortOrderPolicy()
4107         if "active" in teaming_conf["port_order"]:
4108             teaming.uplinkPortOrder.activeUplinkPort = teaming_conf["port_order"][
4109                 "active"
4110             ]
4111         if "standby" in teaming_conf["port_order"]:
4112             teaming.uplinkPortOrder.standbyUplinkPort = teaming_conf["port_order"][
4113                 "standby"
4114             ]
4115 def _apply_dvportgroup_config(pg_name, pg_spec, pg_conf):
4116     """
4117     Applies the values in conf to a distributed portgroup spec
4118     pg_name
4119         The name of the portgroup
4120     pg_spec
4121         The vim.DVPortgroupConfigSpec to apply the config to
4122     pg_conf
4123         The portgroup config
4124     """
4125     log.trace("Building portgroup's '%s' spec", pg_name)
4126     if "name" in pg_conf:
4127         pg_spec.name = pg_conf["name"]
4128     if "description" in pg_conf:
4129         pg_spec.description = pg_conf["description"]
4130     if "num_ports" in pg_conf:
4131         pg_spec.numPorts = pg_conf["num_ports"]
4132     if "type" in pg_conf:
4133         pg_spec.type = pg_conf["type"]
4134     if not pg_spec.defaultPortConfig:
4135         for prop in ["vlan_id", "out_shaping", "security_policy", "teaming"]:
4136             if prop in pg_conf:
4137                 pg_spec.defaultPortConfig = vim.VMwareDVSPortSetting()
4138     if "vlan_id" in pg_conf:
4139         pg_spec.defaultPortConfig.vlan = vim.VmwareDistributedVirtualSwitchVlanIdSpec()
4140         pg_spec.defaultPortConfig.vlan.vlanId = pg_conf["vlan_id"]
4141     if "out_shaping" in pg_conf:
4142         if not pg_spec.defaultPortConfig.outShapingPolicy:
4143             pg_spec.defaultPortConfig.outShapingPolicy = vim.DVSTrafficShapingPolicy()
4144         _apply_dvportgroup_out_shaping(
4145             pg_name, pg_spec.defaultPortConfig.outShapingPolicy, pg_conf["out_shaping"]
4146         )
4147     if "security_policy" in pg_conf:
4148         if not pg_spec.defaultPortConfig.securityPolicy:
4149             pg_spec.defaultPortConfig.securityPolicy = vim.DVSSecurityPolicy()
4150         _apply_dvportgroup_security_policy(
4151             pg_name,
4152             pg_spec.defaultPortConfig.securityPolicy,
4153             pg_conf["security_policy"],
4154         )
4155     if "teaming" in pg_conf:
4156         if not pg_spec.defaultPortConfig.uplinkTeamingPolicy:
4157             pg_spec.defaultPortConfig.uplinkTeamingPolicy = (
4158                 vim.VmwareUplinkPortTeamingPolicy()
4159             )
4160         _apply_dvportgroup_teaming(
4161             pg_name, pg_spec.defaultPortConfig.uplinkTeamingPolicy, pg_conf["teaming"]
4162         )
4163 @depends(HAS_PYVMOMI)
4164 @_supports_proxies("esxdatacenter", "esxcluster")
4165 @_gets_service_instance_via_proxy
4166 def create_dvportgroup(portgroup_dict, portgroup_name, dvs, service_instance=None):
4167     """
4168     Creates a distributed virtual portgroup.
4169     Note: The ``portgroup_name`` param will override any name already set
4170     in ``portgroup_dict``.
4171     portgroup_dict
4172         Dictionary with the config values the portgroup should be created with
4173         (example in salt.states.dvs).
4174     portgroup_name
4175         Name of the portgroup to be created.
4176     dvs
4177         Name of the DVS that will contain the portgroup.
4178     service_instance
4179         Service instance (vim.ServiceInstance) of the vCenter.
4180         Default is None.
4181     .. code-block:: bash
4182         salt '*' vsphere.create_dvportgroup portgroup_dict=&lt;dict&gt;
4183             portgroup_name=pg1 dvs=dvs1
4184     """
4185     log.trace(
4186         "Creating portgroup '%s' in dvs '%s' with dict = %s",
4187         portgroup_name,
4188         dvs,
4189         portgroup_dict,
4190     )
4191     proxy_type = get_proxy_type()
4192     if proxy_type == "esxdatacenter":
4193         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
4194         dc_ref = _get_proxy_target(service_instance)
4195     elif proxy_type == "esxcluster":
4196         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
4197         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4198     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
4199     if not dvs_refs:
4200         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
4201     portgroup_dict["name"] = portgroup_name
4202     spec = vim.DVPortgroupConfigSpec()
4203     _apply_dvportgroup_config(portgroup_name, spec, portgroup_dict)
4204     salt.utils.vmware.create_dvportgroup(dvs_refs[0], spec)
4205     return True
4206 @depends(HAS_PYVMOMI)
4207 @_supports_proxies("esxdatacenter", "esxcluster")
4208 @_gets_service_instance_via_proxy
4209 def update_dvportgroup(portgroup_dict, portgroup, dvs, service_instance=True):
4210     """
4211     Updates a distributed virtual portgroup.
4212     portgroup_dict
4213         Dictionary with the values the portgroup should be update with
4214         (example in salt.states.dvs).
4215     portgroup
4216         Name of the portgroup to be updated.
4217     dvs
4218         Name of the DVS containing the portgroups.
4219     service_instance
4220         Service instance (vim.ServiceInstance) of the vCenter.
4221         Default is None.
4222     .. code-block:: bash
4223         salt '*' vsphere.update_dvportgroup portgroup_dict=&lt;dict&gt;
4224             portgroup=pg1
4225         salt '*' vsphere.update_dvportgroup portgroup_dict=&lt;dict&gt;
4226             portgroup=pg1 dvs=dvs1
4227     """
4228     log.trace(
4229         "Updating portgroup '%s' in dvs '%s' with dict = %s",
4230         portgroup,
4231         dvs,
4232         portgroup_dict,
4233     )
4234     proxy_type = get_proxy_type()
4235     if proxy_type == "esxdatacenter":
4236         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
4237         dc_ref = _get_proxy_target(service_instance)
4238     elif proxy_type == "esxcluster":
4239         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
4240         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4241     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
4242     if not dvs_refs:
4243         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
4244     pg_refs = salt.utils.vmware.get_dvportgroups(
4245         dvs_refs[0], portgroup_names=[portgroup]
4246     )
4247     if not pg_refs:
4248         raise VMwareObjectRetrievalError(
4249             "Portgroup '{}' was not retrieved".format(portgroup)
4250         )
4251     pg_props = salt.utils.vmware.get_properties_of_managed_object(
4252         pg_refs[0], ["config"]
4253     )
4254     spec = vim.DVPortgroupConfigSpec()
4255     for prop in [
4256         "autoExpand",
4257         "configVersion",
4258         "defaultPortConfig",
4259         "description",
4260         "name",
4261         "numPorts",
4262         "policy",
4263         "portNameFormat",
4264         "scope",
4265         "type",
4266         "vendorSpecificConfig",
4267     ]:
4268         setattr(spec, prop, getattr(pg_props["config"], prop))
4269     _apply_dvportgroup_config(portgroup, spec, portgroup_dict)
4270     salt.utils.vmware.update_dvportgroup(pg_refs[0], spec)
4271     return True
4272 @depends(HAS_PYVMOMI)
4273 @_supports_proxies("esxdatacenter", "esxcluster")
4274 @_gets_service_instance_via_proxy
4275 def remove_dvportgroup(portgroup, dvs, service_instance=None):
4276     """
4277     Removes a distributed virtual portgroup.
4278     portgroup
4279         Name of the portgroup to be removed.
4280     dvs
4281         Name of the DVS containing the portgroups.
4282     service_instance
4283         Service instance (vim.ServiceInstance) of the vCenter.
4284         Default is None.
4285     .. code-block:: bash
4286         salt '*' vsphere.remove_dvportgroup portgroup=pg1 dvs=dvs1
4287     """
4288     log.trace("Removing portgroup '%s' in dvs '%s'", portgroup, dvs)
4289     proxy_type = get_proxy_type()
4290     if proxy_type == "esxdatacenter":
4291         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
4292         dc_ref = _get_proxy_target(service_instance)
4293     elif proxy_type == "esxcluster":
4294         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
4295         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4296     dvs_refs = salt.utils.vmware.get_dvss(dc_ref, dvs_names=[dvs])
4297     if not dvs_refs:
4298         raise VMwareObjectRetrievalError("DVS '{}' was not retrieved".format(dvs))
4299     pg_refs = salt.utils.vmware.get_dvportgroups(
4300         dvs_refs[0], portgroup_names=[portgroup]
4301     )
4302     if not pg_refs:
4303         raise VMwareObjectRetrievalError(
4304             "Portgroup '{}' was not retrieved".format(portgroup)
4305         )
4306     salt.utils.vmware.remove_dvportgroup(pg_refs[0])
4307     return True
4308 def _get_policy_dict(policy):
4309     profile_dict = {
4310         "name": policy.name,
4311         "description": policy.description,
4312         "resource_type": policy.resourceType.resourceType,
4313     }
4314     subprofile_dicts = []
4315     if isinstance(policy, pbm.profile.CapabilityBasedProfile) and isinstance(
4316         policy.constraints, pbm.profile.SubProfileCapabilityConstraints
4317     ):
4318         for subprofile in policy.constraints.subProfiles:
4319             subprofile_dict = {
4320                 "name": subprofile.name,
4321                 "force_provision": subprofile.forceProvision,
4322             }
4323             cap_dicts = []
4324             for cap in subprofile.capability:
4325                 cap_dict = {"namespace": cap.id.namespace, "id": cap.id.id}
4326                 val = cap.constraint[0].propertyInstance[0].value
4327                 if isinstance(val, pbm.capability.types.Range):
4328                     val_dict = {"type": "range", "min": val.min, "max": val.max}
4329                 elif isinstance(val, pbm.capability.types.DiscreteSet):
4330                     val_dict = {"type": "set", "values": val.values}
4331                 else:
4332                     val_dict = {"type": "scalar", "value": val}
4333                 cap_dict["setting"] = val_dict
4334                 cap_dicts.append(cap_dict)
4335             subprofile_dict["capabilities"] = cap_dicts
4336             subprofile_dicts.append(subprofile_dict)
4337     profile_dict["subprofiles"] = subprofile_dicts
4338     return profile_dict
4339 @depends(HAS_PYVMOMI)
4340 @_supports_proxies("esxdatacenter", "vcenter")
4341 @_gets_service_instance_via_proxy
4342 def list_storage_policies(policy_names=None, service_instance=None):
4343     """
4344     Returns a list of storage policies.
4345     policy_names
4346         Names of policies to list. If None, all policies are listed.
4347         Default is None.
4348     service_instance
4349         Service instance (vim.ServiceInstance) of the vCenter.
4350         Default is None.
4351     .. code-block:: bash
4352         salt '*' vsphere.list_storage_policies
4353         salt '*' vsphere.list_storage_policy policy_names=[policy_name]
4354     """
4355     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4356     if not policy_names:
4357         policies = salt.utils.pbm.get_storage_policies(
4358             profile_manager, get_all_policies=True
4359         )
4360     else:
4361         policies = salt.utils.pbm.get_storage_policies(profile_manager, policy_names)
4362     return [_get_policy_dict(p) for p in policies]
4363 @depends(HAS_PYVMOMI)
4364 @_supports_proxies("esxdatacenter", "vcenter")
4365 @_gets_service_instance_via_proxy
4366 def list_default_vsan_policy(service_instance=None):
4367     """
4368     Returns the default vsan storage policy.
4369     service_instance
4370         Service instance (vim.ServiceInstance) of the vCenter.
4371         Default is None.
4372     .. code-block:: bash
4373         salt '*' vsphere.list_storage_policies
4374         salt '*' vsphere.list_storage_policy policy_names=[policy_name]
4375     """
4376     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4377     policies = salt.utils.pbm.get_storage_policies(
4378         profile_manager, get_all_policies=True
4379     )
4380     def_policies = [
4381         p for p in policies if p.systemCreatedProfileType == "VsanDefaultProfile"
4382     ]
4383     if not def_policies:
4384         raise VMwareObjectRetrievalError("Default VSAN policy was not retrieved")
4385     return _get_policy_dict(def_policies[0])
4386 def _get_capability_definition_dict(cap_metadata):
4387     return {
4388         "namespace": cap_metadata.id.namespace,
4389         "id": cap_metadata.id.id,
4390         "mandatory": cap_metadata.mandatory,
4391         "description": cap_metadata.summary.summary,
4392         "type": cap_metadata.propertyMetadata[0].type.typeName,
4393     }
4394 @depends(HAS_PYVMOMI)
4395 @_supports_proxies("esxdatacenter", "vcenter")
4396 @_gets_service_instance_via_proxy
4397 def list_capability_definitions(service_instance=None):
4398     """
4399     Returns a list of the metadata of all capabilities in the vCenter.
4400     service_instance
4401         Service instance (vim.ServiceInstance) of the vCenter.
4402         Default is None.
4403     .. code-block:: bash
4404         salt '*' vsphere.list_capabilities
4405     """
4406     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4407     ret_list = [
4408         _get_capability_definition_dict(c)
4409         for c in salt.utils.pbm.get_capability_definitions(profile_manager)
4410     ]
4411     return ret_list
4412 def _apply_policy_config(policy_spec, policy_dict):
4413     log.trace("policy_dict = %s", policy_dict)
4414     if policy_dict.get("name"):
4415         policy_spec.name = policy_dict["name"]
4416     if policy_dict.get("description"):
4417         policy_spec.description = policy_dict["description"]
4418     if policy_dict.get("subprofiles"):
4419         policy_spec.constraints = pbm.profile.SubProfileCapabilityConstraints()
4420         subprofiles = []
4421         for subprofile_dict in policy_dict["subprofiles"]:
4422             subprofile_spec = pbm.profile.SubProfileCapabilityConstraints.SubProfile(
4423                 name=subprofile_dict["name"]
4424             )
4425             cap_specs = []
4426             if subprofile_dict.get("force_provision"):
4427                 subprofile_spec.forceProvision = subprofile_dict["force_provision"]
4428             for cap_dict in subprofile_dict["capabilities"]:
4429                 prop_inst_spec = pbm.capability.PropertyInstance(id=cap_dict["id"])
4430                 setting_type = cap_dict["setting"]["type"]
4431                 if setting_type == "set":
4432                     prop_inst_spec.value = pbm.capability.types.DiscreteSet()
4433                     prop_inst_spec.value.values = cap_dict["setting"]["values"]
4434                 elif setting_type == "range":
4435                     prop_inst_spec.value = pbm.capability.types.Range()
4436                     prop_inst_spec.value.max = cap_dict["setting"]["max"]
4437                     prop_inst_spec.value.min = cap_dict["setting"]["min"]
4438                 elif setting_type == "scalar":
4439                     prop_inst_spec.value = cap_dict["setting"]["value"]
4440                 cap_spec = pbm.capability.CapabilityInstance(
4441                     id=pbm.capability.CapabilityMetadata.UniqueId(
4442                         id=cap_dict["id"], namespace=cap_dict["namespace"]
4443                     ),
4444                     constraint=[
4445                         pbm.capability.ConstraintInstance(
4446                             propertyInstance=[prop_inst_spec]
4447                         )
4448                     ],
4449                 )
4450                 cap_specs.append(cap_spec)
4451             subprofile_spec.capability = cap_specs
4452             subprofiles.append(subprofile_spec)
4453         policy_spec.constraints.subProfiles = subprofiles
4454     log.trace("updated policy_spec = %s", policy_spec)
4455     return policy_spec
4456 @depends(HAS_PYVMOMI)
4457 @_supports_proxies("esxdatacenter", "vcenter")
4458 @_gets_service_instance_via_proxy
4459 def create_storage_policy(policy_name, policy_dict, service_instance=None):
4460     """
4461     Creates a storage policy.
4462     Supported capability types: scalar, set, range.
4463     policy_name
4464         Name of the policy to create.
4465         The value of the argument will override any existing name in
4466         ``policy_dict``.
4467     policy_dict
4468         Dictionary containing the changes to apply to the policy.
4469         (example in salt.states.pbm)
4470     service_instance
4471         Service instance (vim.ServiceInstance) of the vCenter.
4472         Default is None.
4473     .. code-block:: bash
4474         salt '*' vsphere.create_storage_policy policy_name='policy name'
4475             policy_dict="$policy_dict"
4476     """
4477     log.trace("create storage policy '%s', dict = %s", policy_name, policy_dict)
4478     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4479     policy_create_spec = pbm.profile.CapabilityBasedProfileCreateSpec()
4480     policy_create_spec.resourceType = pbm.profile.ResourceType(
4481         resourceType=pbm.profile.ResourceTypeEnum.STORAGE
4482     )
4483     policy_dict["name"] = policy_name
4484     log.trace("Setting policy values in policy_update_spec")
4485     _apply_policy_config(policy_create_spec, policy_dict)
4486     salt.utils.pbm.create_storage_policy(profile_manager, policy_create_spec)
4487     return {"create_storage_policy": True}
4488 @depends(HAS_PYVMOMI)
4489 @_supports_proxies("esxdatacenter", "vcenter")
4490 @_gets_service_instance_via_proxy
4491 def update_storage_policy(policy, policy_dict, service_instance=None):
4492     """
4493     Updates a storage policy.
4494     Supported capability types: scalar, set, range.
4495     policy
4496         Name of the policy to update.
4497     policy_dict
4498         Dictionary containing the changes to apply to the policy.
4499         (example in salt.states.pbm)
4500     service_instance
4501         Service instance (vim.ServiceInstance) of the vCenter.
4502         Default is None.
4503     .. code-block:: bash
4504         salt '*' vsphere.update_storage_policy policy='policy name'
4505             policy_dict="$policy_dict"
4506     """
4507     log.trace("updating storage policy, dict = %s", policy_dict)
4508     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4509     policies = salt.utils.pbm.get_storage_policies(profile_manager, [policy])
4510     if not policies:
4511         raise VMwareObjectRetrievalError("Policy '{}' was not found".format(policy))
4512     policy_ref = policies[0]
4513     policy_update_spec = pbm.profile.CapabilityBasedProfileUpdateSpec()
4514     log.trace("Setting policy values in policy_update_spec")
4515     for prop in ["description", "constraints"]:
4516         setattr(policy_update_spec, prop, getattr(policy_ref, prop))
4517     _apply_policy_config(policy_update_spec, policy_dict)
4518     salt.utils.pbm.update_storage_policy(
4519         profile_manager, policy_ref, policy_update_spec
4520     )
4521     return {"update_storage_policy": True}
4522 @depends(HAS_PYVMOMI)
4523 @_supports_proxies("esxcluster", "esxdatacenter", "vcenter")
4524 @_gets_service_instance_via_proxy
4525 def list_default_storage_policy_of_datastore(datastore, service_instance=None):
4526     """
4527     Returns a list of datastores assign the storage policies.
4528     datastore
4529         Name of the datastore to assign.
4530         The datastore needs to be visible to the VMware entity the proxy
4531         points to.
4532     service_instance
4533         Service instance (vim.ServiceInstance) of the vCenter.
4534         Default is None.
4535     .. code-block:: bash
4536         salt '*' vsphere.list_default_storage_policy_of_datastore datastore=ds1
4537     """
4538     log.trace("Listing the default storage policy of datastore '%s'", datastore)
4539     target_ref = _get_proxy_target(service_instance)
4540     ds_refs = salt.utils.vmware.get_datastores(
4541         service_instance, target_ref, datastore_names=[datastore]
4542     )
4543     if not ds_refs:
4544         raise VMwareObjectRetrievalError(
4545             "Datastore '{}' was not found".format(datastore)
4546         )
4547     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4548     policy = salt.utils.pbm.get_default_storage_policy_of_datastore(
4549         profile_manager, ds_refs[0]
4550     )
4551     return _get_policy_dict(policy)
4552 @depends(HAS_PYVMOMI)
4553 @_supports_proxies("esxcluster", "esxdatacenter", "vcenter")
4554 @_gets_service_instance_via_proxy
4555 def assign_default_storage_policy_to_datastore(
4556     policy, datastore, service_instance=None
4557 ):
4558     """
4559     Assigns a storage policy as the default policy to a datastore.
4560     policy
4561         Name of the policy to assign.
4562     datastore
4563         Name of the datastore to assign.
4564         The datastore needs to be visible to the VMware entity the proxy
4565         points to.
4566     service_instance
4567         Service instance (vim.ServiceInstance) of the vCenter.
4568         Default is None.
4569     .. code-block:: bash
4570         salt '*' vsphere.assign_storage_policy_to_datastore
4571             policy='policy name' datastore=ds1
4572     """
4573     log.trace("Assigning policy %s to datastore %s", policy, datastore)
4574     profile_manager = salt.utils.pbm.get_profile_manager(service_instance)
4575     policies = salt.utils.pbm.get_storage_policies(profile_manager, [policy])
4576     if not policies:
4577         raise VMwareObjectRetrievalError("Policy '{}' was not found".format(policy))
4578     policy_ref = policies[0]
4579     target_ref = _get_proxy_target(service_instance)
4580     ds_refs = salt.utils.vmware.get_datastores(
4581         service_instance, target_ref, datastore_names=[datastore]
4582     )
4583     if not ds_refs:
4584         raise VMwareObjectRetrievalError(
4585             "Datastore '{}' was not found".format(datastore)
4586         )
4587     ds_ref = ds_refs[0]
4588     salt.utils.pbm.assign_default_storage_policy_to_datastore(
4589         profile_manager, policy_ref, ds_ref
4590     )
4591     return True
4592 @depends(HAS_PYVMOMI)
4593 @_supports_proxies("esxdatacenter", "esxcluster", "vcenter", "esxvm")
4594 @_gets_service_instance_via_proxy
4595 def list_datacenters_via_proxy(datacenter_names=None, service_instance=None):
4596     """
4597     Returns a list of dict representations of VMware datacenters.
4598     Connection is done via the proxy details.
4599     Supported proxies: esxdatacenter
4600     datacenter_names
4601         List of datacenter names.
4602         Default is None.
4603     service_instance
4604         Service instance (vim.ServiceInstance) of the vCenter.
4605         Default is None.
4606     .. code-block:: bash
4607         salt '*' vsphere.list_datacenters_via_proxy
4608         salt '*' vsphere.list_datacenters_via_proxy dc1
4609         salt '*' vsphere.list_datacenters_via_proxy dc1,dc2
4610         salt '*' vsphere.list_datacenters_via_proxy datacenter_names=[dc1, dc2]
4611     """
4612     if not datacenter_names:
4613         dc_refs = salt.utils.vmware.get_datacenters(
4614             service_instance, get_all_datacenters=True
4615         )
4616     else:
4617         dc_refs = salt.utils.vmware.get_datacenters(service_instance, datacenter_names)
4618     return [
4619         {"name": salt.utils.vmware.get_managed_object_name(dc_ref)}
4620         for dc_ref in dc_refs
4621     ]
4622 @depends(HAS_PYVMOMI)
4623 @_supports_proxies("esxdatacenter", "vcenter")
4624 @_gets_service_instance_via_proxy
4625 def create_datacenter(datacenter_name, service_instance=None):
4626     """
4627     Creates a datacenter.
4628     Supported proxies: esxdatacenter
4629     datacenter_name
4630         The datacenter name
4631     service_instance
4632         Service instance (vim.ServiceInstance) of the vCenter.
4633         Default is None.
4634     .. code-block:: bash
4635         salt '*' vsphere.create_datacenter dc1
4636     """
4637     salt.utils.vmware.create_datacenter(service_instance, datacenter_name)
4638     return {"create_datacenter": True}
4639 def _get_cluster_dict(cluster_name, cluster_ref):
4640     """
4641     Returns a cluster dict representation from
4642     a vim.ClusterComputeResource object.
4643     cluster_name
4644         Name of the cluster
4645     cluster_ref
4646         Reference to the cluster
4647     """
4648     log.trace("Building a dictionary representation of cluster '%s'", cluster_name)
4649     props = salt.utils.vmware.get_properties_of_managed_object(
4650         cluster_ref, properties=["configurationEx"]
4651     )
4652     res = {
4653         "ha": {"enabled": props["configurationEx"].dasConfig.enabled},
4654         "drs": {"enabled": props["configurationEx"].drsConfig.enabled},
4655     }
4656     ha_conf = props["configurationEx"].dasConfig
4657     log.trace("ha_conf = %s", ha_conf)
4658     res["ha"]["admission_control_enabled"] = ha_conf.admissionControlEnabled
4659     if ha_conf.admissionControlPolicy and isinstance(
4660         ha_conf.admissionControlPolicy,
4661         vim.ClusterFailoverResourcesAdmissionControlPolicy,
4662     ):
4663         pol = ha_conf.admissionControlPolicy
4664         res["ha"]["admission_control_policy"] = {
4665             "cpu_failover_percent": pol.cpuFailoverResourcesPercent,
4666             "memory_failover_percent": pol.memoryFailoverResourcesPercent,
4667         }
4668     if ha_conf.defaultVmSettings:
4669         def_vm_set = ha_conf.defaultVmSettings
4670         res["ha"]["default_vm_settings"] = {
4671             "isolation_response": def_vm_set.isolationResponse,
4672             "restart_priority": def_vm_set.restartPriority,
4673         }
4674     res["ha"]["hb_ds_candidate_policy"] = ha_conf.hBDatastoreCandidatePolicy
4675     if ha_conf.hostMonitoring:
4676         res["ha"]["host_monitoring"] = ha_conf.hostMonitoring
4677     if ha_conf.option:
4678         res["ha"]["options"] = [
4679             {"key": o.key, "value": o.value} for o in ha_conf.option
4680         ]
4681     res["ha"]["vm_monitoring"] = ha_conf.vmMonitoring
4682     drs_conf = props["configurationEx"].drsConfig
4683     log.trace("drs_conf = %s", drs_conf)
4684     res["drs"]["vmotion_rate"] = 6 - drs_conf.vmotionRate
4685     res["drs"]["default_vm_behavior"] = drs_conf.defaultVmBehavior
4686     res["vm_swap_placement"] = props["configurationEx"].vmSwapPlacement
4687     si = salt.utils.vmware.get_service_instance_from_managed_object(cluster_ref)
4688     if salt.utils.vsan.vsan_supported(si):
4689         vcenter_info = salt.utils.vmware.get_service_info(si)
4690         if int(vcenter_info.build) &gt;= 3634794:  # 60u2
4691             vsan_conf = salt.utils.vsan.get_cluster_vsan_info(cluster_ref)
4692             log.trace("vsan_conf = %s", vsan_conf)
4693             res["vsan"] = {
4694                 "enabled": vsan_conf.enabled,
4695                 "auto_claim_storage": vsan_conf.defaultConfig.autoClaimStorage,
4696             }
4697             if vsan_conf.dataEfficiencyConfig:
4698                 data_eff = vsan_conf.dataEfficiencyConfig
4699                 res["vsan"].update(
4700                     {
4701                         "compression_enabled": data_eff.compressionEnabled or False,
4702                         "dedup_enabled": data_eff.dedupEnabled,
4703                     }
4704                 )
4705         else:  # before 60u2 (no advanced vsan info)
4706             if props["configurationEx"].vsanConfigInfo:
4707                 default_config = props["configurationEx"].vsanConfigInfo.defaultConfig
4708                 res["vsan"] = {
4709                     "enabled": props["configurationEx"].vsanConfigInfo.enabled,
4710                     "auto_claim_storage": default_config.autoClaimStorage,
4711                 }
4712     return res
4713 @depends(HAS_PYVMOMI)
4714 @_supports_proxies("esxcluster", "esxdatacenter")
4715 @_gets_service_instance_via_proxy
4716 def list_cluster(datacenter=None, cluster=None, service_instance=None):
4717     """
4718     Returns a dict representation of an ESX cluster.
4719     datacenter
4720         Name of datacenter containing the cluster.
4721         Ignored if already contained by proxy details.
4722         Default value is None.
4723     cluster
4724         Name of cluster.
4725         Ignored if already contained by proxy details.
4726         Default value is None.
4727     service_instance
4728         Service instance (vim.ServiceInstance) of the vCenter.
4729         Default is None.
4730     .. code-block:: bash
4731         salt '*' vsphere.list_cluster datacenter=dc1 cluster=cl1
4732         salt '*' vsphere.list_cluster cluster=cl1
4733         salt '*' vsphere.list_cluster
4734     """
4735     proxy_type = get_proxy_type()
4736     if proxy_type == "esxdatacenter":
4737         dc_ref = _get_proxy_target(service_instance)
4738         if not cluster:
4739             raise ArgumentValueError("'cluster' needs to be specified")
4740         cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)
4741     elif proxy_type == "esxcluster":
4742         cluster_ref = _get_proxy_target(service_instance)
4743         cluster = __salt__["esxcluster.get_details"]()["cluster"]
4744     log.trace(
4745         "Retrieving representation of cluster '%s' in a %s proxy", cluster, proxy_type
4746     )
4747     return _get_cluster_dict(cluster, cluster_ref)
4748 def _apply_cluster_dict(cluster_spec, cluster_dict, vsan_spec=None, vsan_61=True):
4749     """
4750     Applies the values of cluster_dict dictionary to a cluster spec
4751     (vim.ClusterConfigSpecEx).
4752     All vsan values (cluster_dict['vsan']) will be applied to
4753     vsan_spec (vim.vsan.cluster.ConfigInfoEx). Can be not omitted
4754     if not required.
4755     VSAN 6.1 config needs to be applied differently than the post VSAN 6.1 way.
4756     The type of configuration desired is dictated by the flag vsan_61.
4757     """
4758     log.trace("Applying cluster dict %s", cluster_dict)
4759     if cluster_dict.get("ha"):
4760         ha_dict = cluster_dict["ha"]
4761         if not cluster_spec.dasConfig:
4762             cluster_spec.dasConfig = vim.ClusterDasConfigInfo()
4763         das_config = cluster_spec.dasConfig
4764         if "enabled" in ha_dict:
4765             das_config.enabled = ha_dict["enabled"]
4766             if ha_dict["enabled"]:
4767                 das_config.failoverLevel = 1
4768         if "admission_control_enabled" in ha_dict:
4769             das_config.admissionControlEnabled = ha_dict["admission_control_enabled"]
4770         if "admission_control_policy" in ha_dict:
4771             adm_pol_dict = ha_dict["admission_control_policy"]
4772             if not das_config.admissionControlPolicy or not isinstance(
4773                 das_config.admissionControlPolicy,
4774                 vim.ClusterFailoverResourcesAdmissionControlPolicy,
4775             ):
4776                 das_config.admissionControlPolicy = (
4777                     vim.ClusterFailoverResourcesAdmissionControlPolicy(
4778                         cpuFailoverResourcesPercent=adm_pol_dict[
4779                             "cpu_failover_percent"
4780                         ],
4781                         memoryFailoverResourcesPercent=adm_pol_dict[
4782                             "memory_failover_percent"
4783                         ],
4784                     )
4785                 )
4786         if "default_vm_settings" in ha_dict:
4787             vm_set_dict = ha_dict["default_vm_settings"]
4788             if not das_config.defaultVmSettings:
4789                 das_config.defaultVmSettings = vim.ClusterDasVmSettings()
4790             if "isolation_response" in vm_set_dict:
4791                 das_config.defaultVmSettings.isolationResponse = vm_set_dict[
4792                     "isolation_response"
4793                 ]
4794             if "restart_priority" in vm_set_dict:
4795                 das_config.defaultVmSettings.restartPriority = vm_set_dict[
4796                     "restart_priority"
4797                 ]
4798         if "hb_ds_candidate_policy" in ha_dict:
4799             das_config.hBDatastoreCandidatePolicy = ha_dict["hb_ds_candidate_policy"]
4800         if "host_monitoring" in ha_dict:
4801             das_config.hostMonitoring = ha_dict["host_monitoring"]
4802         if "options" in ha_dict:
4803             das_config.option = []
4804             for opt_dict in ha_dict["options"]:
4805                 das_config.option.append(vim.OptionValue(key=opt_dict["key"]))
4806                 if "value" in opt_dict:
4807                     das_config.option[-1].value = opt_dict["value"]
4808         if "vm_monitoring" in ha_dict:
4809             das_config.vmMonitoring = ha_dict["vm_monitoring"]
4810         cluster_spec.dasConfig = das_config
4811     if cluster_dict.get("drs"):
4812         drs_dict = cluster_dict["drs"]
4813         drs_config = vim.ClusterDrsConfigInfo()
4814         if "enabled" in drs_dict:
4815             drs_config.enabled = drs_dict["enabled"]
4816         if "vmotion_rate" in drs_dict:
4817             drs_config.vmotionRate = 6 - drs_dict["vmotion_rate"]
4818         if "default_vm_behavior" in drs_dict:
4819             drs_config.defaultVmBehavior = vim.DrsBehavior(
4820                 drs_dict["default_vm_behavior"]
4821             )
4822         cluster_spec.drsConfig = drs_config
4823     if cluster_dict.get("vm_swap_placement"):
4824         cluster_spec.vmSwapPlacement = cluster_dict["vm_swap_placement"]
4825     if cluster_dict.get("vsan"):
4826         vsan_dict = cluster_dict["vsan"]
4827         if not vsan_61:  # VSAN is 6.2 and above
4828             if "enabled" in vsan_dict:
4829                 if not vsan_spec.vsanClusterConfig:
4830                     vsan_spec.vsanClusterConfig = vim.vsan.cluster.ConfigInfo()
4831                 vsan_spec.vsanClusterConfig.enabled = vsan_dict["enabled"]
4832             if "auto_claim_storage" in vsan_dict:
4833                 if not vsan_spec.vsanClusterConfig:
4834                     vsan_spec.vsanClusterConfig = vim.vsan.cluster.ConfigInfo()
4835                 if not vsan_spec.vsanClusterConfig.defaultConfig:
4836                     vsan_spec.vsanClusterConfig.defaultConfig = (
4837                         vim.VsanClusterConfigInfoHostDefaultInfo()
4838                     )
4839                 elif vsan_spec.vsanClusterConfig.defaultConfig.uuid:
4840                     vsan_spec.vsanClusterConfig.defaultConfig.uuid = None
4841                 vsan_spec.vsanClusterConfig.defaultConfig.autoClaimStorage = vsan_dict[
4842                     "auto_claim_storage"
4843                 ]
4844             if "compression_enabled" in vsan_dict:
4845                 if not vsan_spec.dataEfficiencyConfig:
4846                     vsan_spec.dataEfficiencyConfig = vim.vsan.DataEfficiencyConfig()
4847                 vsan_spec.dataEfficiencyConfig.compressionEnabled = vsan_dict[
4848                     "compression_enabled"
4849                 ]
4850             if "dedup_enabled" in vsan_dict:
4851                 if not vsan_spec.dataEfficiencyConfig:
4852                     vsan_spec.dataEfficiencyConfig = vim.vsan.DataEfficiencyConfig()
4853                 vsan_spec.dataEfficiencyConfig.dedupEnabled = vsan_dict["dedup_enabled"]
4854         if not cluster_spec.vsanConfig:
4855             cluster_spec.vsanConfig = vim.VsanClusterConfigInfo()
4856         vsan_config = cluster_spec.vsanConfig
4857         if "enabled" in vsan_dict:
4858             vsan_config.enabled = vsan_dict["enabled"]
4859         if "auto_claim_storage" in vsan_dict:
4860             if not vsan_config.defaultConfig:
4861                 vsan_config.defaultConfig = vim.VsanClusterConfigInfoHostDefaultInfo()
4862             elif vsan_config.defaultConfig.uuid:
4863                 vsan_config.defaultConfig.uuid = None
4864             vsan_config.defaultConfig.autoClaimStorage = vsan_dict["auto_claim_storage"]
4865     log.trace("cluster_spec = %s", cluster_spec)
4866 @depends(HAS_PYVMOMI)
4867 @depends(HAS_JSONSCHEMA)
4868 @_supports_proxies("esxcluster", "esxdatacenter")
4869 @_gets_service_instance_via_proxy
4870 def create_cluster(cluster_dict, datacenter=None, cluster=None, service_instance=None):
4871     """
4872     Creates a cluster.
4873     Note: cluster_dict['name'] will be overridden by the cluster param value
4874     config_dict
4875         Dictionary with the config values of the new cluster.
4876     datacenter
4877         Name of datacenter containing the cluster.
4878         Ignored if already contained by proxy details.
4879         Default value is None.
4880     cluster
4881         Name of cluster.
4882         Ignored if already contained by proxy details.
4883         Default value is None.
4884     service_instance
4885         Service instance (vim.ServiceInstance) of the vCenter.
4886         Default is None.
4887     .. code-block:: bash
4888         salt '*' vsphere.create_cluster cluster_dict=$cluster_dict cluster=cl1
4889         salt '*' vsphere.create_cluster cluster_dict=$cluster_dict
4890     """
4891     schema = ESXClusterConfigSchema.serialize()
4892     try:
4893         jsonschema.validate(cluster_dict, schema)
4894     except jsonschema.exceptions.ValidationError as exc:
4895         raise InvalidConfigError(exc)
4896     proxy_type = get_proxy_type()
4897     if proxy_type == "esxdatacenter":
4898         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
4899         dc_ref = _get_proxy_target(service_instance)
4900         if not cluster:
4901             raise ArgumentValueError("'cluster' needs to be specified")
4902     elif proxy_type == "esxcluster":
4903         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
4904         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4905         cluster = __salt__["esxcluster.get_details"]()["cluster"]
4906     if cluster_dict.get("vsan") and not salt.utils.vsan.vsan_supported(
4907         service_instance
4908     ):
4909         raise VMwareApiError("VSAN operations are not supported")
4910     si = service_instance
4911     cluster_spec = vim.ClusterConfigSpecEx()
4912     vsan_spec = None
4913     ha_config = None
4914     vsan_61 = None
4915     if cluster_dict.get("vsan"):
4916         vcenter_info = salt.utils.vmware.get_service_info(si)
4917         if (
4918             float(vcenter_info.apiVersion) &gt;= 6.0 and int(vcenter_info.build) &gt;= 3634794
4919         ):  # 60u2
4920             vsan_spec = vim.vsan.ReconfigSpec(modify=True)
4921             vsan_61 = False
4922             if cluster_dict.get("ha", {}).get("enabled"):
4923                 enable_ha = True
4924                 ha_config = cluster_dict["ha"]
4925                 del cluster_dict["ha"]
4926         else:
4927             vsan_61 = True
4928     _apply_cluster_dict(cluster_spec, cluster_dict, vsan_spec, vsan_61)
4929     salt.utils.vmware.create_cluster(dc_ref, cluster, cluster_spec)
4930     if not vsan_61:
4931         if vsan_spec:
4932             cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)
4933             salt.utils.vsan.reconfigure_cluster_vsan(cluster_ref, vsan_spec)
4934         if enable_ha:
4935             _apply_cluster_dict(cluster_spec, {"ha": ha_config})
4936             salt.utils.vmware.update_cluster(cluster_ref, cluster_spec)
4937             cluster_dict["ha"] = ha_config
4938     return {"create_cluster": True}
4939 @depends(HAS_PYVMOMI)
4940 @depends(HAS_JSONSCHEMA)
4941 @_supports_proxies("esxcluster", "esxdatacenter")
4942 @_gets_service_instance_via_proxy
4943 def update_cluster(cluster_dict, datacenter=None, cluster=None, service_instance=None):
4944     """
4945     Updates a cluster.
4946     config_dict
4947         Dictionary with the config values of the new cluster.
4948     datacenter
4949         Name of datacenter containing the cluster.
4950         Ignored if already contained by proxy details.
4951         Default value is None.
4952     cluster
4953         Name of cluster.
4954         Ignored if already contained by proxy details.
4955         Default value is None.
4956     service_instance
4957         Service instance (vim.ServiceInstance) of the vCenter.
4958         Default is None.
4959     .. code-block:: bash
4960         salt '*' vsphere.update_cluster cluster_dict=$cluster_dict cluster=cl1
4961         salt '*' vsphere.update_cluster cluster_dict=$cluster_dict
4962     """
4963     schema = ESXClusterConfigSchema.serialize()
4964     try:
4965         jsonschema.validate(cluster_dict, schema)
4966     except jsonschema.exceptions.ValidationError as exc:
4967         raise InvalidConfigError(exc)
4968     proxy_type = get_proxy_type()
4969     if proxy_type == "esxdatacenter":
4970         datacenter = __salt__["esxdatacenter.get_details"]()["datacenter"]
4971         dc_ref = _get_proxy_target(service_instance)
4972         if not cluster:
4973             raise ArgumentValueError("'cluster' needs to be specified")
4974     elif proxy_type == "esxcluster":
4975         datacenter = __salt__["esxcluster.get_details"]()["datacenter"]
4976         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
4977         cluster = __salt__["esxcluster.get_details"]()["cluster"]
4978     if cluster_dict.get("vsan") and not salt.utils.vsan.vsan_supported(
4979         service_instance
4980     ):
4981         raise VMwareApiError("VSAN operations are not supported")
4982     cluster_ref = salt.utils.vmware.get_cluster(dc_ref, cluster)
4983     cluster_spec = vim.ClusterConfigSpecEx()
4984     props = salt.utils.vmware.get_properties_of_managed_object(
4985         cluster_ref, properties=["configurationEx"]
4986     )
4987     for p in ["dasConfig", "drsConfig"]:
4988         setattr(cluster_spec, p, getattr(props["configurationEx"], p))
4989     if props["configurationEx"].vsanConfigInfo:
4990         cluster_spec.vsanConfig = props["configurationEx"].vsanConfigInfo
4991     vsan_spec = None
4992     vsan_61 = None
4993     if cluster_dict.get("vsan"):
4994         vcenter_info = salt.utils.vmware.get_service_info(service_instance)
4995         if (
4996             float(vcenter_info.apiVersion) &gt;= 6.0 and int(vcenter_info.build) &gt;= 3634794
4997         ):  # 60u2
4998             vsan_61 = False
4999             vsan_info = salt.utils.vsan.get_cluster_vsan_info(cluster_ref)
5000             vsan_spec = vim.vsan.ReconfigSpec(modify=True)
5001             vsan_spec.dataEfficiencyConfig = vsan_info.dataEfficiencyConfig
5002             vsan_info.dataEfficiencyConfig = None
5003         else:
5004             vsan_61 = True
5005     _apply_cluster_dict(cluster_spec, cluster_dict, vsan_spec, vsan_61)
5006     if vsan_spec:
5007         log.trace("vsan_spec = %s", vsan_spec)
5008         salt.utils.vsan.reconfigure_cluster_vsan(cluster_ref, vsan_spec)
5009         cluster_spec = vim.ClusterConfigSpecEx()
5010         props = salt.utils.vmware.get_properties_of_managed_object(
5011             cluster_ref, properties=["configurationEx"]
5012         )
5013         for p in ["dasConfig", "drsConfig"]:
5014             setattr(cluster_spec, p, getattr(props["configurationEx"], p))
5015         if props["configurationEx"].vsanConfigInfo:
5016             cluster_spec.vsanConfig = props["configurationEx"].vsanConfigInfo
5017         _apply_cluster_dict(cluster_spec, cluster_dict)
5018     salt.utils.vmware.update_cluster(cluster_ref, cluster_spec)
5019     return {"update_cluster": True}
5020 @depends(HAS_PYVMOMI)
5021 @_supports_proxies("esxi", "esxcluster", "esxdatacenter")
5022 @_gets_service_instance_via_proxy
5023 def list_datastores_via_proxy(
5024     datastore_names=None,
5025     backing_disk_ids=None,
5026     backing_disk_scsi_addresses=None,
5027     service_instance=None,
5028 ):
5029     """
5030     Returns a list of dict representations of the datastores visible to the
5031     proxy object. The list of datastores can be filtered by datastore names,
5032     backing disk ids (canonical names) or backing disk scsi addresses.
5033     Supported proxy types: esxi, esxcluster, esxdatacenter
5034     datastore_names
5035         List of the names of datastores to filter on
5036     backing_disk_ids
5037         List of canonical names of the backing disks of the datastores to filer.
5038         Default is None.
5039     backing_disk_scsi_addresses
5040         List of scsi addresses of the backing disks of the datastores to filter.
5041         Default is None.
5042     service_instance
5043         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5044         Default is None.
5045     .. code-block:: bash
5046         salt '*' vsphere.list_datastores_via_proxy
5047         salt '*' vsphere.list_datastores_via_proxy datastore_names=[ds1, ds2]
5048     """
5049     target = _get_proxy_target(service_instance)
5050     target_name = salt.utils.vmware.get_managed_object_name(target)
5051     log.trace("target name = %s", target_name)
5052     get_all_datastores = (
5053         True
5054         if not (datastore_names or backing_disk_ids or backing_disk_scsi_addresses)
5055         else False
5056     )
5057     if backing_disk_scsi_addresses:
5058         log.debug(
5059             "Retrieving disk ids for scsi addresses '%s'", backing_disk_scsi_addresses
5060         )
5061         disk_ids = [
5062             d.canonicalName
5063             for d in salt.utils.vmware.get_disks(
5064                 target, scsi_addresses=backing_disk_scsi_addresses
5065             )
5066         ]
5067         log.debug("Found disk ids '%s'", disk_ids)
5068         backing_disk_ids = (
5069             backing_disk_ids.extend(disk_ids) if backing_disk_ids else disk_ids
5070         )
5071     datastores = salt.utils.vmware.get_datastores(
5072         service_instance, target, datastore_names, backing_disk_ids, get_all_datastores
5073     )
5074     mount_infos = []
5075     if isinstance(target, vim.HostSystem):
5076         storage_system = salt.utils.vmware.get_storage_system(
5077             service_instance, target, target_name
5078         )
5079         props = salt.utils.vmware.get_properties_of_managed_object(
5080             storage_system, ["fileSystemVolumeInfo.mountInfo"]
5081         )
5082         mount_infos = props.get("fileSystemVolumeInfo.mountInfo", [])
5083     ret_dict = []
5084     for ds in datastores:
5085         ds_dict = {
5086             "name": ds.name,
5087             "type": ds.summary.type,
5088             "free_space": ds.summary.freeSpace,
5089             "capacity": ds.summary.capacity,
5090         }
5091         backing_disk_ids = []
5092         for vol in [
5093             i.volume
5094             for i in mount_infos
5095             if i.volume.name == ds.name and isinstance(i.volume, vim.HostVmfsVolume)
5096         ]:
5097             backing_disk_ids.extend([e.diskName for e in vol.extent])
5098         if backing_disk_ids:
5099             ds_dict["backing_disk_ids"] = backing_disk_ids
5100         ret_dict.append(ds_dict)
5101     return ret_dict
5102 @depends(HAS_PYVMOMI)
5103 @depends(HAS_JSONSCHEMA)
5104 @_supports_proxies("esxi")
5105 @_gets_service_instance_via_proxy
5106 def create_vmfs_datastore(
5107     datastore_name,
5108     disk_id,
5109     vmfs_major_version,
5110     safety_checks=True,
5111     service_instance=None,
5112 ):
5113     """
5114     Creates a ESXi host disk group with the specified cache and capacity disks.
5115     datastore_name
5116         The name of the datastore to be created.
5117     disk_id
5118         The disk id (canonical name) on which the datastore is created.
5119     vmfs_major_version
5120         The VMFS major version.
5121     safety_checks
5122         Specify whether to perform safety check or to skip the checks and try
5123         performing the required task. Default is True.
5124     service_instance
5125         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5126         Default is None.
5127     .. code-block:: bash
5128         salt '*' vsphere.create_vmfs_datastore datastore_name=ds1 disk_id=
5129             vmfs_major_version=5
5130     """
5131     log.debug("Validating vmfs datastore input")
5132     schema = VmfsDatastoreSchema.serialize()
5133     try:
5134         jsonschema.validate(
5135             {
5136                 "datastore": {
5137                     "name": datastore_name,
5138                     "backing_disk_id": disk_id,
5139                     "vmfs_version": vmfs_major_version,
5140                 }
5141             },
5142             schema,
5143         )
5144     except jsonschema.exceptions.ValidationError as exc:
5145         raise ArgumentValueError(exc)
5146     host_ref = _get_proxy_target(service_instance)
5147     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5148     if safety_checks:
5149         disks = salt.utils.vmware.get_disks(host_ref, disk_ids=[disk_id])
5150         if not disks:
5151             raise VMwareObjectRetrievalError(
5152                 "Disk '{}' was not found in host '{}'".format(disk_id, hostname)
5153             )
5154     ds_ref = salt.utils.vmware.create_vmfs_datastore(
5155         host_ref, datastore_name, disks[0], vmfs_major_version
5156     )
5157     return True
5158 @depends(HAS_PYVMOMI)
5159 @_supports_proxies("esxi", "esxcluster", "esxdatacenter")
5160 @_gets_service_instance_via_proxy
5161 def rename_datastore(datastore_name, new_datastore_name, service_instance=None):
5162     """
5163     Renames a datastore. The datastore needs to be visible to the proxy.
5164     datastore_name
5165         Current datastore name.
5166     new_datastore_name
5167         New datastore name.
5168     service_instance
5169         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5170         Default is None.
5171     .. code-block:: bash
5172         salt '*' vsphere.rename_datastore old_name new_name
5173     """
5174     log.trace("Renaming datastore %s to %s", datastore_name, new_datastore_name)
5175     target = _get_proxy_target(service_instance)
5176     datastores = salt.utils.vmware.get_datastores(
5177         service_instance, target, datastore_names=[datastore_name]
5178     )
5179     if not datastores:
5180         raise VMwareObjectRetrievalError(
5181             "Datastore '{}' was not found".format(datastore_name)
5182         )
5183     ds = datastores[0]
5184     salt.utils.vmware.rename_datastore(ds, new_datastore_name)
5185     return True
5186 @depends(HAS_PYVMOMI)
5187 @_supports_proxies("esxi", "esxcluster", "esxdatacenter")
5188 @_gets_service_instance_via_proxy
5189 def remove_datastore(datastore, service_instance=None):
5190     """
5191     Removes a datastore. If multiple datastores an error is raised.
5192     datastore
5193         Datastore name
5194     service_instance
5195         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5196         Default is None.
5197     .. code-block:: bash
5198         salt '*' vsphere.remove_datastore ds_name
5199     """
5200     log.trace("Removing datastore '%s'", datastore)
5201     target = _get_proxy_target(service_instance)
5202     datastores = salt.utils.vmware.get_datastores(
5203         service_instance, reference=target, datastore_names=[datastore]
5204     )
5205     if not datastores:
5206         raise VMwareObjectRetrievalError(
5207             "Datastore '{}' was not found".format(datastore)
5208         )
5209     if len(datastores) &gt; 1:
5210         raise VMwareObjectRetrievalError(
5211             "Multiple datastores '{}' were found".format(datastore)
5212         )
5213     salt.utils.vmware.remove_datastore(service_instance, datastores[0])
5214     return True
5215 @depends(HAS_PYVMOMI)
5216 @_supports_proxies("esxcluster", "esxdatacenter")
5217 @_gets_service_instance_via_proxy
5218 def list_licenses(service_instance=None):
5219     """
5220     Lists all licenses on a vCenter.
5221     service_instance
5222         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5223         Default is None.
5224     .. code-block:: bash
5225         salt '*' vsphere.list_licenses
5226     """
5227     log.trace("Retrieving all licenses")
5228     licenses = salt.utils.vmware.get_licenses(service_instance)
5229     ret_dict = [
5230         {
5231             "key": l.licenseKey,
5232             "name": l.name,
5233             "description": l.labels[0].value if l.labels else None,
5234             "capacity": l.total if l.total &gt; 0 else sys.maxsize,
5235             "used": l.used if l.used else 0,
5236         }
5237         for l in licenses
5238     ]
5239     return ret_dict
5240 @depends(HAS_PYVMOMI)
5241 @_supports_proxies("esxcluster", "esxdatacenter")
5242 @_gets_service_instance_via_proxy
5243 def add_license(key, description, safety_checks=True, service_instance=None):
5244     """
5245     Adds a license to the vCenter or ESXi host
5246     key
5247         License key.
5248     description
5249         License description added in as a label.
5250     safety_checks
5251         Specify whether to perform safety check or to skip the checks and try
5252         performing the required task
5253     service_instance
5254         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5255         Default is None.
5256     .. code-block:: bash
5257         salt '*' vsphere.add_license key=&lt;license_key&gt; desc='License desc'
5258     """
5259     log.trace("Adding license '%s'", key)
5260     salt.utils.vmware.add_license(service_instance, key, description)
5261     return True
5262 def _get_entity(service_instance, entity):
5263     """
5264     Returns the entity associated with the entity dict representation
5265     Supported entities: cluster, vcenter
5266     Expected entity format:
5267     .. code-block:: python
5268         cluster:
5269             {'type': 'cluster',
5270              'datacenter': &lt;datacenter_name&gt;,
5271              'cluster': &lt;cluster_name&gt;}
5272         vcenter:
5273             {'type': 'vcenter'}
5274     service_instance
5275         Service instance (vim.ServiceInstance) of the vCenter.
5276     entity
5277         Entity dict in the format above
5278     """
5279     log.trace("Retrieving entity: %s", entity)
5280     if entity["type"] == "cluster":
5281         dc_ref = salt.utils.vmware.get_datacenter(
5282             service_instance, entity["datacenter"]
5283         )
5284         return salt.utils.vmware.get_cluster(dc_ref, entity["cluster"])
5285     elif entity["type"] == "vcenter":
5286         return None
5287     raise ArgumentValueError("Unsupported entity type '{}'".format(entity["type"]))
5288 def _validate_entity(entity):
5289     """
5290     Validates the entity dict representation
5291     entity
5292         Dictionary representation of an entity.
5293         See ``_get_entity`` docstrings for format.
5294     """
5295     if entity["type"] == "cluster":
5296         schema = ESXClusterEntitySchema.serialize()
5297     elif entity["type"] == "vcenter":
5298         schema = VCenterEntitySchema.serialize()
5299     else:
5300         raise ArgumentValueError("Unsupported entity type '{}'".format(entity["type"]))
5301     try:
5302         jsonschema.validate(entity, schema)
5303     except jsonschema.exceptions.ValidationError as exc:
5304         raise InvalidEntityError(exc)
5305 @depends(HAS_PYVMOMI)
5306 @depends(HAS_JSONSCHEMA)
5307 @_supports_proxies("esxcluster", "esxdatacenter")
5308 @_gets_service_instance_via_proxy
5309 def list_assigned_licenses(
5310     entity, entity_display_name, license_keys=None, service_instance=None
5311 ):
5312     """
5313     Lists the licenses assigned to an entity
5314     entity
5315         Dictionary representation of an entity.
5316         See ``_get_entity`` docstrings for format.
5317     entity_display_name
5318         Entity name used in logging
5319     license_keys:
5320         List of license keys to be retrieved. Default is None.
5321     service_instance
5322         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5323         Default is None.
5324     .. code-block:: bash
5325         salt '*' vsphere.list_assigned_licenses
5326             entity={type:cluster,datacenter:dc,cluster:cl}
5327             entiy_display_name=cl
5328     """
5329     log.trace("Listing assigned licenses of entity %s", entity)
5330     _validate_entity(entity)
5331     assigned_licenses = salt.utils.vmware.get_assigned_licenses(
5332         service_instance,
5333         entity_ref=_get_entity(service_instance, entity),
5334         entity_name=entity_display_name,
5335     )
5336     return [
5337         {
5338             "key": l.licenseKey,
5339             "name": l.name,
5340             "description": l.labels[0].value if l.labels else None,
5341             "capacity": l.total if l.total &gt; 0 else sys.maxsize,
5342         }
5343         for l in assigned_licenses
5344         if (license_keys is None) or (l.licenseKey in license_keys)
5345     ]
5346 @depends(HAS_PYVMOMI)
5347 @depends(HAS_JSONSCHEMA)
5348 @_supports_proxies("esxcluster", "esxdatacenter")
5349 @_gets_service_instance_via_proxy
5350 def assign_license(
5351     license_key,
5352     license_name,
5353     entity,
5354     entity_display_name,
5355     safety_checks=True,
5356     service_instance=None,
5357 ):
5358     """
5359     Assigns a license to an entity
5360     license_key
5361         Key of the license to assign
5362         See ``_get_entity`` docstrings for format.
5363     license_name
5364         Display name of license
5365     entity
5366         Dictionary representation of an entity
5367     entity_display_name
5368         Entity name used in logging
5369     safety_checks
5370         Specify whether to perform safety check or to skip the checks and try
5371         performing the required task. Default is False.
5372     service_instance
5373         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5374         Default is None.
5375     .. code-block:: bash
5376         salt '*' vsphere.assign_license license_key=00000:00000
5377             license name=test entity={type:cluster,datacenter:dc,cluster:cl}
5378     """
5379     log.trace("Assigning license %s to entity %s", license_key, entity)
5380     _validate_entity(entity)
5381     if safety_checks:
5382         licenses = salt.utils.vmware.get_licenses(service_instance)
5383         if not [l for l in licenses if l.licenseKey == license_key]:
5384             raise VMwareObjectRetrievalError(
5385                 "License '{}' wasn't found".format(license_name)
5386             )
5387     salt.utils.vmware.assign_license(
5388         service_instance,
5389         license_key,
5390         license_name,
5391         entity_ref=_get_entity(service_instance, entity),
5392         entity_name=entity_display_name,
5393     )
5394 @depends(HAS_PYVMOMI)
5395 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter")
5396 @_gets_service_instance_via_proxy
5397 def list_hosts_via_proxy(
5398     hostnames=None, datacenter=None, cluster=None, service_instance=None
5399 ):
5400     """
5401     Returns a list of hosts for the specified VMware environment. The list
5402     of hosts can be filtered by datacenter name and/or cluster name
5403     hostnames
5404         Hostnames to filter on.
5405     datacenter_name
5406         Name of datacenter. Only hosts in this datacenter will be retrieved.
5407         Default is None.
5408     cluster_name
5409         Name of cluster. Only hosts in this cluster will be retrieved. If a
5410         datacenter is not specified the first cluster with this name will be
5411         considerred. Default is None.
5412     service_instance
5413         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5414         Default is None.
5415     CLI Example:
5416     .. code-block:: bash
5417         salt '*' vsphere.list_hosts_via_proxy
5418         salt '*' vsphere.list_hosts_via_proxy hostnames=[esxi1.example.com]
5419         salt '*' vsphere.list_hosts_via_proxy datacenter=dc1 cluster=cluster1
5420     """
5421     if cluster:
5422         if not datacenter:
5423             raise salt.exceptions.ArgumentValueError(
5424                 "Datacenter is required when cluster is specified"
5425             )
5426     get_all_hosts = False
5427     if not hostnames:
5428         get_all_hosts = True
5429     hosts = salt.utils.vmware.get_hosts(
5430         service_instance,
5431         datacenter_name=datacenter,
5432         host_names=hostnames,
5433         cluster_name=cluster,
5434         get_all_hosts=get_all_hosts,
5435     )
5436     return [salt.utils.vmware.get_managed_object_name(h) for h in hosts]
5437 @depends(HAS_PYVMOMI)
5438 @_supports_proxies("esxi")
5439 @_gets_service_instance_via_proxy
5440 def list_disks(disk_ids=None, scsi_addresses=None, service_instance=None):
5441     """
5442     Returns a list of dict representations of the disks in an ESXi host.
5443     The list of disks can be filtered by disk canonical names or
5444     scsi addresses.
5445     disk_ids:
5446         List of disk canonical names to be retrieved. Default is None.
5447     scsi_addresses
5448         List of scsi addresses of disks to be retrieved. Default is None
5449     service_instance
5450         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5451         Default is None.
5452     .. code-block:: bash
5453         salt '*' vsphere.list_disks
5454         salt '*' vsphere.list_disks disk_ids='[naa.00, naa.001]'
5455         salt '*' vsphere.list_disks
5456             scsi_addresses='[vmhba0:C0:T0:L0, vmhba1:C0:T0:L0]'
5457     """
5458     host_ref = _get_proxy_target(service_instance)
5459     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5460     log.trace(
5461         "Retrieving disks of host '%s'; disc ids = %s; scsi_address = %s",
5462         hostname,
5463         disk_ids,
5464         scsi_addresses,
5465     )
5466     get_all_disks = True if not (disk_ids or scsi_addresses) else False
5467     ret_list = []
5468     scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(
5469         host_ref, hostname=hostname
5470     )
5471     canonical_name_to_scsi_address = {
5472         lun.canonicalName: scsi_addr for scsi_addr, lun in scsi_address_to_lun.items()
5473     }
5474     for d in salt.utils.vmware.get_disks(
5475         host_ref, disk_ids, scsi_addresses, get_all_disks
5476     ):
5477         ret_list.append(
5478             {
5479                 "id": d.canonicalName,
5480                 "scsi_address": canonical_name_to_scsi_address[d.canonicalName],
5481             }
5482         )
5483     return ret_list
5484 @depends(HAS_PYVMOMI)
5485 @_supports_proxies("esxi")
5486 @_gets_service_instance_via_proxy
5487 def erase_disk_partitions(disk_id=None, scsi_address=None, service_instance=None):
5488     """
5489     Erases the partitions on a disk.
5490     The disk can be specified either by the canonical name, or by the
5491     scsi_address.
5492     disk_id
5493         Canonical name of the disk.
5494         Either ``disk_id`` or ``scsi_address`` needs to be specified
5495         (``disk_id`` supersedes ``scsi_address``.
5496     scsi_address
5497         Scsi address of the disk.
5498         ``disk_id`` or ``scsi_address`` needs to be specified
5499         (``disk_id`` supersedes ``scsi_address``.
5500     service_instance
5501         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5502         Default is None.
5503     .. code-block:: bash
5504         salt '*' vsphere.erase_disk_partitions scsi_address='vmhaba0:C0:T0:L0'
5505         salt '*' vsphere.erase_disk_partitions disk_id='naa.000000000000001'
5506     """
5507     if not disk_id and not scsi_address:
5508         raise ArgumentValueError(
5509             "Either 'disk_id' or 'scsi_address' needs to be specified"
5510         )
5511     host_ref = _get_proxy_target(service_instance)
5512     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5513     if not disk_id:
5514         scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(host_ref)
5515         if scsi_address not in scsi_address_to_lun:
5516             raise VMwareObjectRetrievalError(
5517                 "Scsi lun with address '{}' was not found on host '{}'".format(
5518                     scsi_address, hostname
5519                 )
5520             )
5521         disk_id = scsi_address_to_lun[scsi_address].canonicalName
5522         log.trace(
5523             "[%s] Got disk id '%s' for scsi address '%s'",
5524             hostname,
5525             disk_id,
5526             scsi_address,
5527         )
5528     log.trace("Erasing disk partitions on disk '%s' in host '%s'", disk_id, hostname)
5529     salt.utils.vmware.erase_disk_partitions(
5530         service_instance, host_ref, disk_id, hostname=hostname
5531     )
5532     log.info("Erased disk partitions on disk '%s' on host '%s'", disk_id, hostname)
5533     return True
5534 @depends(HAS_PYVMOMI)
5535 @_supports_proxies("esxi")
5536 @_gets_service_instance_via_proxy
5537 def list_disk_partitions(disk_id=None, scsi_address=None, service_instance=None):
5538     """
5539     Lists the partitions on a disk.
5540     The disk can be specified either by the canonical name, or by the
5541     scsi_address.
5542     disk_id
5543         Canonical name of the disk.
5544         Either ``disk_id`` or ``scsi_address`` needs to be specified
5545         (``disk_id`` supersedes ``scsi_address``.
5546     scsi_address`
5547         Scsi address of the disk.
5548         ``disk_id`` or ``scsi_address`` needs to be specified
5549         (``disk_id`` supersedes ``scsi_address``.
5550     service_instance
5551         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5552         Default is None.
5553     .. code-block:: bash
5554         salt '*' vsphere.list_disk_partitions scsi_address='vmhaba0:C0:T0:L0'
5555         salt '*' vsphere.list_disk_partitions disk_id='naa.000000000000001'
5556     """
5557     if not disk_id and not scsi_address:
5558         raise ArgumentValueError(
5559             "Either 'disk_id' or 'scsi_address' needs to be specified"
5560         )
5561     host_ref = _get_proxy_target(service_instance)
5562     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5563     if not disk_id:
5564         scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(host_ref)
5565         if scsi_address not in scsi_address_to_lun:
5566             raise VMwareObjectRetrievalError(
5567                 "Scsi lun with address '{}' was not found on host '{}'".format(
5568                     scsi_address, hostname
5569                 )
5570             )
5571         disk_id = scsi_address_to_lun[scsi_address].canonicalName
5572         log.trace(
5573             "[%s] Got disk id '%s' for scsi address '%s'",
5574             hostname,
5575             disk_id,
5576             scsi_address,
5577         )
5578     log.trace("Listing disk partitions on disk '%s' in host '%s'", disk_id, hostname)
5579     partition_info = salt.utils.vmware.get_disk_partition_info(host_ref, disk_id)
5580     ret_list = []
5581     for part_spec in partition_info.spec.partition:
5582         part_layout = [
5583             p
5584             for p in partition_info.layout.partition
5585             if p.partition == part_spec.partition
5586         ][0]
5587         part_dict = {
5588             "hostname": hostname,
5589             "device": disk_id,
5590             "format": partition_info.spec.partitionFormat,
5591             "partition": part_spec.partition,
5592             "type": part_spec.type,
5593             "sectors": part_spec.endSector - part_spec.startSector + 1,
5594             "size_KB": (part_layout.end.block - part_layout.start.block + 1)
5595             * part_layout.start.blockSize
5596             / 1024,
5597         }
5598         ret_list.append(part_dict)
5599     return ret_list
5600 @depends(HAS_PYVMOMI)
5601 @_supports_proxies("esxi")
5602 @_gets_service_instance_via_proxy
5603 def list_diskgroups(cache_disk_ids=None, service_instance=None):
5604     """
5605     Returns a list of disk group dict representation on an ESXi host.
5606     The list of disk groups can be filtered by the cache disks
5607     canonical names. If no filtering is applied, all disk groups are returned.
5608     cache_disk_ids:
5609         List of cache disk canonical names of the disk groups to be retrieved.
5610         Default is None.
5611     use_proxy_details
5612         Specify whether to use the proxy minion's details instead of the
5613         arguments
5614     service_instance
5615         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5616         Default is None.
5617     .. code-block:: bash
5618         salt '*' vsphere.list_diskgroups
5619         salt '*' vsphere.list_diskgroups cache_disk_ids='[naa.000000000000001]'
5620     """
5621     host_ref = _get_proxy_target(service_instance)
5622     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5623     log.trace("Listing diskgroups in '%s'", hostname)
5624     get_all_diskgroups = True if not cache_disk_ids else False
5625     ret_list = []
5626     for dg in salt.utils.vmware.get_diskgroups(
5627         host_ref, cache_disk_ids, get_all_diskgroups
5628     ):
5629         ret_list.append(
5630             {
5631                 "cache_disk": dg.ssd.canonicalName,
5632                 "capacity_disks": [d.canonicalName for d in dg.nonSsd],
5633             }
5634         )
5635     return ret_list
5636 @depends(HAS_PYVMOMI)
5637 @depends(HAS_JSONSCHEMA)
5638 @_supports_proxies("esxi")
5639 @_gets_service_instance_via_proxy
5640 def create_diskgroup(
5641     cache_disk_id, capacity_disk_ids, safety_checks=True, service_instance=None
5642 ):
5643     """
5644     Creates disk group on an ESXi host with the specified cache and
5645     capacity disks.
5646     cache_disk_id
5647         The canonical name of the disk to be used as a cache. The disk must be
5648         ssd.
5649     capacity_disk_ids
5650         A list containing canonical names of the capacity disks. Must contain at
5651         least one id. Default is True.
5652     safety_checks
5653         Specify whether to perform safety check or to skip the checks and try
5654         performing the required task. Default value is True.
5655     service_instance
5656         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5657         Default is None.
5658     .. code-block:: bash
5659         salt '*' vsphere.create_diskgroup cache_disk_id='naa.000000000000001'
5660             capacity_disk_ids='[naa.000000000000002, naa.000000000000003]'
5661     """
5662     log.trace("Validating diskgroup input")
5663     schema = DiskGroupsDiskIdSchema.serialize()
5664     try:
5665         jsonschema.validate(
5666             {
5667                 "diskgroups": [
5668                     {"cache_id": cache_disk_id, "capacity_ids": capacity_disk_ids}
5669                 ]
5670             },
5671             schema,
5672         )
5673     except jsonschema.exceptions.ValidationError as exc:
5674         raise ArgumentValueError(exc)
5675     host_ref = _get_proxy_target(service_instance)
5676     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5677     if safety_checks:
5678         diskgroups = salt.utils.vmware.get_diskgroups(host_ref, [cache_disk_id])
5679         if diskgroups:
5680             raise VMwareObjectExistsError(
5681                 "Diskgroup with cache disk id '{}' already exists ESXi "
5682                 "host '{}'".format(cache_disk_id, hostname)
5683             )
5684     disk_ids = capacity_disk_ids[:]
5685     disk_ids.insert(0, cache_disk_id)
5686     disks = salt.utils.vmware.get_disks(host_ref, disk_ids=disk_ids)
5687     for id in disk_ids:
5688         if not [d for d in disks if d.canonicalName == id]:
5689             raise VMwareObjectRetrievalError(
5690                 "No disk with id '{}' was found in ESXi host '{}'".format(id, hostname)
5691             )
5692     cache_disk = [d for d in disks if d.canonicalName == cache_disk_id][0]
5693     capacity_disks = [d for d in disks if d.canonicalName in capacity_disk_ids]
5694     vsan_disk_mgmt_system = salt.utils.vsan.get_vsan_disk_management_system(
5695         service_instance
5696     )
5697     dg = salt.utils.vsan.create_diskgroup(
5698         service_instance, vsan_disk_mgmt_system, host_ref, cache_disk, capacity_disks
5699     )
5700     return True
5701 @depends(HAS_PYVMOMI)
5702 @depends(HAS_JSONSCHEMA)
5703 @_supports_proxies("esxi")
5704 @_gets_service_instance_via_proxy
5705 def add_capacity_to_diskgroup(
5706     cache_disk_id, capacity_disk_ids, safety_checks=True, service_instance=None
5707 ):
5708     """
5709     Adds capacity disks to the disk group with the specified cache disk.
5710     cache_disk_id
5711         The canonical name of the cache disk.
5712     capacity_disk_ids
5713         A list containing canonical names of the capacity disks to add.
5714     safety_checks
5715         Specify whether to perform safety check or to skip the checks and try
5716         performing the required task. Default value is True.
5717     service_instance
5718         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5719         Default is None.
5720     .. code-block:: bash
5721         salt '*' vsphere.add_capacity_to_diskgroup
5722             cache_disk_id='naa.000000000000001'
5723             capacity_disk_ids='[naa.000000000000002, naa.000000000000003]'
5724     """
5725     log.trace("Validating diskgroup input")
5726     schema = DiskGroupsDiskIdSchema.serialize()
5727     try:
5728         jsonschema.validate(
5729             {
5730                 "diskgroups": [
5731                     {"cache_id": cache_disk_id, "capacity_ids": capacity_disk_ids}
5732                 ]
5733             },
5734             schema,
5735         )
5736     except jsonschema.exceptions.ValidationError as exc:
5737         raise ArgumentValueError(exc)
5738     host_ref = _get_proxy_target(service_instance)
5739     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5740     disks = salt.utils.vmware.get_disks(host_ref, disk_ids=capacity_disk_ids)
5741     if safety_checks:
5742         for id in capacity_disk_ids:
5743             if not [d for d in disks if d.canonicalName == id]:
5744                 raise VMwareObjectRetrievalError(
5745                     "No disk with id '{}' was found in ESXi host '{}'".format(
5746                         id, hostname
5747                     )
5748                 )
5749     diskgroups = salt.utils.vmware.get_diskgroups(
5750         host_ref, cache_disk_ids=[cache_disk_id]
5751     )
5752     if not diskgroups:
5753         raise VMwareObjectRetrievalError(
5754             "No diskgroup with cache disk id '{}' was found in ESXi host '{}'".format(
5755                 cache_disk_id, hostname
5756             )
5757         )
5758     vsan_disk_mgmt_system = salt.utils.vsan.get_vsan_disk_management_system(
5759         service_instance
5760     )
5761     salt.utils.vsan.add_capacity_to_diskgroup(
5762         service_instance, vsan_disk_mgmt_system, host_ref, diskgroups[0], disks
5763     )
5764     return True
5765 @depends(HAS_PYVMOMI)
5766 @depends(HAS_JSONSCHEMA)
5767 @_supports_proxies("esxi")
5768 @_gets_service_instance_via_proxy
5769 def remove_capacity_from_diskgroup(
5770     cache_disk_id,
5771     capacity_disk_ids,
5772     data_evacuation=True,
5773     safety_checks=True,
5774     service_instance=None,
5775 ):
5776     """
5777     Remove capacity disks from the disk group with the specified cache disk.
5778     cache_disk_id
5779         The canonical name of the cache disk.
5780     capacity_disk_ids
5781         A list containing canonical names of the capacity disks to add.
5782     data_evacuation
5783         Specifies whether to gracefully evacuate the data on the capacity disks
5784         before removing them from the disk group. Default value is True.
5785     safety_checks
5786         Specify whether to perform safety check or to skip the checks and try
5787         performing the required task. Default value is True.
5788     service_instance
5789         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5790         Default is None.
5791     .. code-block:: bash
5792         salt '*' vsphere.remove_capacity_from_diskgroup
5793             cache_disk_id='naa.000000000000001'
5794             capacity_disk_ids='[naa.000000000000002, naa.000000000000003]'
5795     """
5796     log.trace("Validating diskgroup input")
5797     schema = DiskGroupsDiskIdSchema.serialize()
5798     try:
5799         jsonschema.validate(
5800             {
5801                 "diskgroups": [
5802                     {"cache_id": cache_disk_id, "capacity_ids": capacity_disk_ids}
5803                 ]
5804             },
5805             schema,
5806         )
5807     except jsonschema.exceptions.ValidationError as exc:
5808         raise ArgumentValueError(str(exc))
5809     host_ref = _get_proxy_target(service_instance)
5810     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5811     disks = salt.utils.vmware.get_disks(host_ref, disk_ids=capacity_disk_ids)
5812     if safety_checks:
5813         for id in capacity_disk_ids:
5814             if not [d for d in disks if d.canonicalName == id]:
5815                 raise VMwareObjectRetrievalError(
5816                     "No disk with id '{}' was found in ESXi host '{}'".format(
5817                         id, hostname
5818                     )
5819                 )
5820     diskgroups = salt.utils.vmware.get_diskgroups(
5821         host_ref, cache_disk_ids=[cache_disk_id]
5822     )
5823     if not diskgroups:
5824         raise VMwareObjectRetrievalError(
5825             "No diskgroup with cache disk id '{}' was found in ESXi host '{}'".format(
5826                 cache_disk_id, hostname
5827             )
5828         )
5829     log.trace("data_evacuation = %s", data_evacuation)
5830     salt.utils.vsan.remove_capacity_from_diskgroup(
5831         service_instance,
5832         host_ref,
5833         diskgroups[0],
5834         capacity_disks=[d for d in disks if d.canonicalName in capacity_disk_ids],
5835         data_evacuation=data_evacuation,
5836     )
5837     return True
5838 @depends(HAS_PYVMOMI)
5839 @depends(HAS_JSONSCHEMA)
5840 @_supports_proxies("esxi")
5841 @_gets_service_instance_via_proxy
5842 def remove_diskgroup(cache_disk_id, data_accessibility=True, service_instance=None):
5843     """
5844     Remove the diskgroup with the specified cache disk.
5845     cache_disk_id
5846         The canonical name of the cache disk.
5847     data_accessibility
5848         Specifies whether to ensure data accessibility. Default value is True.
5849     service_instance
5850         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5851         Default is None.
5852     .. code-block:: bash
5853         salt '*' vsphere.remove_diskgroup cache_disk_id='naa.000000000000001'
5854     """
5855     log.trace("Validating diskgroup input")
5856     host_ref = _get_proxy_target(service_instance)
5857     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5858     diskgroups = salt.utils.vmware.get_diskgroups(
5859         host_ref, cache_disk_ids=[cache_disk_id]
5860     )
5861     if not diskgroups:
5862         raise VMwareObjectRetrievalError(
5863             "No diskgroup with cache disk id '{}' was found in ESXi host '{}'".format(
5864                 cache_disk_id, hostname
5865             )
5866         )
5867     log.trace("data accessibility = %s", data_accessibility)
5868     salt.utils.vsan.remove_diskgroup(
5869         service_instance, host_ref, diskgroups[0], data_accessibility=data_accessibility
5870     )
5871     return True
5872 @depends(HAS_PYVMOMI)
5873 @_supports_proxies("esxi")
5874 @_gets_service_instance_via_proxy
5875 def get_host_cache(service_instance=None):
5876     """
5877     Returns the host cache configuration on the proxy host.
5878     service_instance
5879         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5880         Default is None.
5881     .. code-block:: bash
5882         salt '*' vsphere.get_host_cache
5883     """
5884     ret_dict = {}
5885     host_ref = _get_proxy_target(service_instance)
5886     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5887     hci = salt.utils.vmware.get_host_cache(host_ref)
5888     if not hci:
5889         log.debug("Host cache not configured on host '%s'", hostname)
5890         ret_dict["enabled"] = False
5891         return ret_dict
5892     return {
5893         "enabled": True,
5894         "datastore": {"name": hci.key.name},
5895         "swap_size": "{}MiB".format(hci.swapSize),
5896     }
5897 @depends(HAS_PYVMOMI)
5898 @depends(HAS_JSONSCHEMA)
5899 @_supports_proxies("esxi")
5900 @_gets_service_instance_via_proxy
5901 def configure_host_cache(
5902     enabled, datastore=None, swap_size_MiB=None, service_instance=None
5903 ):
5904     """
5905     Configures the host cache on the selected host.
5906     enabled
5907         Boolean flag specifying whether the host cache is enabled.
5908     datastore
5909         Name of the datastore that contains the host cache. Must be set if
5910         enabled is ``true``.
5911     swap_size_MiB
5912         Swap size in Mibibytes. Needs to be set if enabled is ``true``. Must be
5913         smaller than the datastore size.
5914     service_instance
5915         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
5916         Default is None.
5917     .. code-block:: bash
5918         salt '*' vsphere.configure_host_cache enabled=False
5919         salt '*' vsphere.configure_host_cache enabled=True datastore=ds1
5920             swap_size_MiB=1024
5921     """
5922     log.debug("Validating host cache input")
5923     schema = SimpleHostCacheSchema.serialize()
5924     try:
5925         jsonschema.validate(
5926             {
5927                 "enabled": enabled,
5928                 "datastore_name": datastore,
5929                 "swap_size_MiB": swap_size_MiB,
5930             },
5931             schema,
5932         )
5933     except jsonschema.exceptions.ValidationError as exc:
5934         raise ArgumentValueError(exc)
5935     if not enabled:
5936         raise ArgumentValueError("Disabling the host cache is not supported")
5937     ret_dict = {"enabled": False}
5938     host_ref = _get_proxy_target(service_instance)
5939     hostname = __proxy__["esxi.get_details"]()["esxi_host"]
5940     if datastore:
5941         ds_refs = salt.utils.vmware.get_datastores(
5942             service_instance, host_ref, datastore_names=[datastore]
5943         )
5944         if not ds_refs:
5945             raise VMwareObjectRetrievalError(
5946                 "Datastore '{}' was not found on host '{}'".format(datastore, hostname)
5947             )
5948         ds_ref = ds_refs[0]
5949     salt.utils.vmware.configure_host_cache(host_ref, ds_ref, swap_size_MiB)
5950     return True
5951 def _check_hosts(service_instance, host, host_names):
5952     """
5953     Helper function that checks to see if the host provided is a vCenter Server or
5954     an ESXi host. If it's an ESXi host, returns a list of a single host_name.
5955     If a host reference isn't found, we're trying to find a host object for a vCenter
5956     server. Raises a CommandExecutionError in this case, as we need host references to
5957     check against.
5958     """
5959     if not host_names:
5960         host_name = _get_host_ref(service_instance, host)
5961         if host_name:
5962             host_names = [host]
5963         else:
5964             raise CommandExecutionError(
5965                 "No host reference found. If connecting to a "
5966                 "vCenter Server, a list of 'host_names' must be "
5967                 "provided."
5968             )
5969     elif not isinstance(host_names, list):
5970         raise CommandExecutionError("'host_names' must be a list.")
5971     return host_names
5972 def _format_coredump_stdout(cmd_ret):
5973     """
5974     Helper function to format the stdout from the get_coredump_network_config function.
5975     cmd_ret
5976         The return dictionary that comes from a cmd.run_all call.
5977     """
5978     ret_dict = {}
5979     for line in cmd_ret["stdout"].splitlines():
5980         line = line.strip().lower()
5981         if line.startswith("enabled:"):
5982             enabled = line.split(":")
5983             if "true" in enabled[1]:
5984                 ret_dict["enabled"] = True
5985             else:
5986                 ret_dict["enabled"] = False
5987                 break
5988         if line.startswith("host vnic:"):
5989             host_vnic = line.split(":")
5990             ret_dict["host_vnic"] = host_vnic[1].strip()
5991         if line.startswith("network server ip:"):
5992             ip = line.split(":")
5993             ret_dict["ip"] = ip[1].strip()
5994         if line.startswith("network server port:"):
5995             ip_port = line.split(":")
5996             ret_dict["port"] = ip_port[1].strip()
5997     return ret_dict
5998 def _format_firewall_stdout(cmd_ret):
5999     """
6000     Helper function to format the stdout from the get_firewall_status function.
6001     cmd_ret
6002         The return dictionary that comes from a cmd.run_all call.
6003     """
6004     ret_dict = {"success": True, "rulesets": {}}
6005     for line in cmd_ret["stdout"].splitlines():
6006         if line.startswith("Name"):
6007             continue
6008         if line.startswith("---"):
6009             continue
6010         ruleset_status = line.split()
6011         ret_dict["rulesets"][ruleset_status[0]] = bool(ruleset_status[1])
6012     return ret_dict
6013 def _format_syslog_config(cmd_ret):
6014     """
6015     Helper function to format the stdout from the get_syslog_config function.
6016     cmd_ret
6017         The return dictionary that comes from a cmd.run_all call.
6018     """
6019     ret_dict = {"success": cmd_ret["retcode"] == 0}
6020     if cmd_ret["retcode"] != 0:
6021         ret_dict["message"] = cmd_ret["stdout"]
6022     else:
6023         for line in cmd_ret["stdout"].splitlines():
6024             line = line.strip()
6025             cfgvars = line.split(": ")
6026             key = cfgvars[0].strip()
6027             value = cfgvars[1].strip()
6028             ret_dict[key] = value
6029     return ret_dict
6030 def _get_date_time_mgr(host_reference):
6031     """
6032     Helper function that returns a dateTimeManager object
6033     """
6034     return host_reference.configManager.dateTimeSystem
6035 def _get_host_ref(service_instance, host, host_name=None):
6036     """
6037     Helper function that returns a host object either from the host location or the host_name.
6038     If host_name is provided, that is the host_object that will be returned.
6039     The function will first search for hosts by DNS Name. If no hosts are found, it will
6040     try searching by IP Address.
6041     """
6042     search_index = salt.utils.vmware.get_inventory(service_instance).searchIndex
6043     if host_name:
6044         host_ref = search_index.FindByDnsName(dnsName=host_name, vmSearch=False)
6045     else:
6046         host_ref = search_index.FindByDnsName(dnsName=host, vmSearch=False)
6047     if host_ref is None:
6048         host_ref = search_index.FindByIp(ip=host, vmSearch=False)
6049     return host_ref
6050 def _get_host_ssds(host_reference):
6051     """
6052     Helper function that returns a list of ssd objects for a given host.
6053     """
6054     return _get_host_disks(host_reference).get("SSDs")
6055 def _get_host_non_ssds(host_reference):
6056     """
6057     Helper function that returns a list of Non-SSD objects for a given host.
6058     """
6059     return _get_host_disks(host_reference).get("Non-SSDs")
6060 def _get_host_disks(host_reference):
6061     """
6062     Helper function that returns a dictionary containing a list of SSD and Non-SSD disks.
6063     """
6064     storage_system = host_reference.configManager.storageSystem
6065     disks = storage_system.storageDeviceInfo.scsiLun
6066     ssds = []
6067     non_ssds = []
6068     for disk in disks:
6069         try:
6070             has_ssd_attr = disk.ssd
6071         except AttributeError:
6072             has_ssd_attr = False
6073         if has_ssd_attr:
6074             ssds.append(disk)
6075         else:
6076             non_ssds.append(disk)
6077     return {"SSDs": ssds, "Non-SSDs": non_ssds}
6078 def _get_service_manager(host_reference):
6079     """
6080     Helper function that returns a service manager object from a given host object.
6081     """
6082     return host_reference.configManager.serviceSystem
6083 def _get_vsan_eligible_disks(service_instance, host, host_names):
6084     """
6085     Helper function that returns a dictionary of host_name keys with either a list of eligible
6086     disks that can be added to VSAN or either an 'Error' message or a message saying no
6087     eligible disks were found. Possible keys/values look like:
6088     return = {'host_1': {'Error': 'VSAN System Config Manager is unset ...'},
6089               'host_2': {'Eligible': 'The host xxx does not have any VSAN eligible disks.'},
6090               'host_3': {'Eligible': [disk1, disk2, disk3, disk4],
6091               'host_4': {'Eligible': []}}
6092     """
6093     ret = {}
6094     for host_name in host_names:
6095         host_ref = _get_host_ref(service_instance, host, host_name=host_name)
6096         vsan_system = host_ref.configManager.vsanSystem
6097         if vsan_system is None:
6098             msg = (
6099                 "VSAN System Config Manager is unset for host '{}'. "
6100                 "VSAN configuration cannot be changed without a configured "
6101                 "VSAN System.".format(host_name)
6102             )
6103             log.debug(msg)
6104             ret.update({host_name: {"Error": msg}})
6105             continue
6106         suitable_disks = []
6107         query = vsan_system.QueryDisksForVsan()
6108         for item in query:
6109             if item.state == "eligible":
6110                 suitable_disks.append(item)
6111         if not suitable_disks:
6112             msg = "The host '{}' does not have any VSAN eligible disks.".format(
6113                 host_name
6114             )
6115             log.warning(msg)
6116             ret.update({host_name: {"Eligible": msg}})
6117             continue
6118         disks = _get_host_ssds(host_ref) + _get_host_non_ssds(host_ref)
6119         matching = []
6120         for disk in disks:
6121             for suitable_disk in suitable_disks:
6122                 if disk.canonicalName == suitable_disk.disk.canonicalName:
6123                     matching.append(disk)
6124         ret.update({host_name: {"Eligible": matching}})
6125     return ret
6126 def _reset_syslog_config_params(
6127     host,
6128     username,
6129     password,
6130     cmd,
6131     resets,
6132     valid_resets,
6133     protocol=None,
6134     port=None,
6135     esxi_host=None,
6136     credstore=None,
6137 ):
6138     """
6139     Helper function for reset_syslog_config that resets the config and populates the return dictionary.
6140     """
6141     ret_dict = {}
6142     all_success = True
6143     if not isinstance(resets, list):
6144         resets = [resets]
6145     for reset_param in resets:
6146         if reset_param in valid_resets:
6147             ret = salt.utils.vmware.esxcli(
6148                 host,
6149                 username,
6150                 password,
6151                 cmd + reset_param,
6152                 protocol=protocol,
6153                 port=port,
6154                 esxi_host=esxi_host,
6155                 credstore=credstore,
6156             )
6157             ret_dict[reset_param] = {}
6158             ret_dict[reset_param]["success"] = ret["retcode"] == 0
6159             if ret["retcode"] != 0:
6160                 all_success = False
6161                 ret_dict[reset_param]["message"] = ret["stdout"]
6162         else:
6163             all_success = False
6164             ret_dict[reset_param] = {}
6165             ret_dict[reset_param]["success"] = False
6166             ret_dict[reset_param]["message"] = "Invalid syslog configuration parameter"
6167     ret_dict["success"] = all_success
6168     return ret_dict
6169 def _set_syslog_config_helper(
6170     host,
6171     username,
6172     password,
6173     syslog_config,
6174     config_value,
6175     protocol=None,
6176     port=None,
6177     reset_service=None,
6178     esxi_host=None,
6179     credstore=None,
6180 ):
6181     """
6182     Helper function for set_syslog_config that sets the config and populates the return dictionary.
6183     """
6184     cmd = "system syslog config set --{} {}".format(syslog_config, config_value)
6185     ret_dict = {}
6186     valid_resets = [
6187         "logdir",
6188         "loghost",
6189         "default-rotate",
6190         "default-size",
6191         "default-timeout",
6192         "logdir-unique",
6193     ]
6194     if syslog_config not in valid_resets:
6195         ret_dict.update(
6196             {
6197                 "success": False,
6198                 "message": "'{}' is not a valid config variable.".format(syslog_config),
6199             }
6200         )
6201         return ret_dict
6202     response = salt.utils.vmware.esxcli(
6203         host,
6204         username,
6205         password,
6206         cmd,
6207         protocol=protocol,
6208         port=port,
6209         esxi_host=esxi_host,
6210         credstore=credstore,
6211     )
6212     if response["retcode"] != 0:
6213         ret_dict.update(
6214             {syslog_config: {"success": False, "message": response["stdout"]}}
6215         )
6216     else:
6217         ret_dict.update({syslog_config: {"success": True}})
6218     if reset_service:
6219         if esxi_host:
6220             host_name = esxi_host
6221             esxi_host = [esxi_host]
6222         else:
6223             host_name = host
6224         response = syslog_service_reload(
6225             host,
6226             username,
6227             password,
6228             protocol=protocol,
6229             port=port,
6230             esxi_hosts=esxi_host,
6231             credstore=credstore,
6232         ).get(host_name)
6233         ret_dict.update({"syslog_restart": {"success": response["retcode"] == 0}})
6234     return ret_dict
6235 @depends(HAS_PYVMOMI)
6236 @ignores_kwargs("credstore")
6237 def add_host_to_dvs(
6238     host,
6239     username,
6240     password,
6241     vmknic_name,
6242     vmnic_name,
6243     dvs_name,
6244     target_portgroup_name,
6245     uplink_portgroup_name,
6246     protocol=None,
6247     port=None,
6248     host_names=None,
6249     verify_ssl=True,
6250 ):
6251     """
6252     Adds an ESXi host to a vSphere Distributed Virtual Switch and migrates
6253     the desired adapters to the DVS from the standard switch.
6254     host
6255         The location of the vCenter server.
6256     username
6257         The username used to login to the vCenter server.
6258     password
6259         The password used to login to the vCenter server.
6260     vmknic_name
6261         The name of the virtual NIC to migrate.
6262     vmnic_name
6263         The name of the physical NIC to migrate.
6264     dvs_name
6265         The name of the Distributed Virtual Switch.
6266     target_portgroup_name
6267         The name of the distributed portgroup in which to migrate the
6268         virtual NIC.
6269     uplink_portgroup_name
6270         The name of the uplink portgroup in which to migrate the
6271         physical NIC.
6272     protocol
6273         Optionally set to alternate protocol if the vCenter server or ESX/ESXi host is not
6274         using the default protocol. Default protocol is ``https``.
6275     port
6276         Optionally set to alternate port if the vCenter server or ESX/ESXi host is not
6277         using the default port. Default port is ``443``.
6278     host_names:
6279         An array of VMware host names to migrate
6280     verify_ssl
6281         Verify the SSL certificate. Default: True
6282     CLI Example:
6283     .. code-block:: bash
6284         salt some_host vsphere.add_host_to_dvs host='vsphere.corp.com'
6285             username='administrator@vsphere.corp.com' password='vsphere_password'
6286             vmknic_name='vmk0' vmnic_name='vnmic0' dvs_name='DSwitch'
6287             target_portgroup_name='DPortGroup' uplink_portgroup_name='DSwitch1-DVUplinks-181'
6288             protocol='https' port='443', host_names="['esxi1.corp.com','esxi2.corp.com','esxi3.corp.com']"
6289     Return Example:
6290     .. code-block:: yaml
6291         somehost:
6292             ----------
6293             esxi1.corp.com:
6294                 ----------
6295                 dvs:
6296                     DSwitch
6297                 portgroup:
6298                     DPortGroup
6299                 status:
6300                     True
6301                 uplink:
6302                     DSwitch-DVUplinks-181
6303                 vmknic:
6304                     vmk0
6305                 vmnic:
6306                     vmnic0
6307             esxi2.corp.com:
6308                 ----------
6309                 dvs:
6310                     DSwitch
6311                 portgroup:
6312                     DPortGroup
6313                 status:
6314                     True
6315                 uplink:
6316                     DSwitch-DVUplinks-181
6317                 vmknic:
6318                     vmk0
6319                 vmnic:
6320                     vmnic0
6321             esxi3.corp.com:
6322                 ----------
6323                 dvs:
6324                     DSwitch
6325                 portgroup:
6326                     DPortGroup
6327                 status:
6328                     True
6329                 uplink:
6330                     DSwitch-DVUplinks-181
6331                 vmknic:
6332                     vmk0
6333                 vmnic:
6334                     vmnic0
6335             message:
6336             success:
6337                 True
6338     This was very difficult to figure out.  VMware's PyVmomi documentation at
6339     https://github.com/vmware/pyvmomi/blob/master/docs/vim/DistributedVirtualSwitch.rst
6340     (which is a copy of the official documentation here:
6341     https://www.vmware.com/support/developer/converter-sdk/conv60_apireference/vim.DistributedVirtualSwitch.html)
6342     says to create the DVS, create distributed portgroups, and then add the
6343     host to the DVS specifying which physical NIC to use as the port backing.
6344     However, if the physical NIC is in use as the only link from the host
6345     to vSphere, this will fail with an unhelpful "busy" error.
6346     There is, however, a Powershell PowerCLI cmdlet called Add-VDSwitchPhysicalNetworkAdapter
6347     that does what we want.  I used Onyx (https://labs.vmware.com/flings/onyx)
6348     to sniff the SOAP stream from Powershell to our vSphere server and got
6349     this snippet out:
6350     .. code-block:: xml
6351         &lt;UpdateNetworkConfig xmlns="urn:vim25"&gt;
6352           &lt;_this type="HostNetworkSystem"&gt;networkSystem-187&lt;/_this&gt;
6353           &lt;config&gt;
6354             &lt;vswitch&gt;
6355               &lt;changeOperation&gt;edit&lt;/changeOperation&gt;
6356               &lt;name&gt;vSwitch0&lt;/name&gt;
6357               &lt;spec&gt;
6358                 &lt;numPorts&gt;7812&lt;/numPorts&gt;
6359               &lt;/spec&gt;
6360             &lt;/vswitch&gt;
6361             &lt;proxySwitch&gt;
6362                 &lt;changeOperation&gt;edit&lt;/changeOperation&gt;
6363                 &lt;uuid&gt;73 a4 05 50 b0 d2 7e b9-38 80 5d 24 65 8f da 70&lt;/uuid&gt;
6364                 &lt;spec&gt;
6365                 &lt;backing xsi:type="DistributedVirtualSwitchHostMemberPnicBacking"&gt;
6366                     &lt;pnicSpec&gt;&lt;pnicDevice&gt;vmnic0&lt;/pnicDevice&gt;&lt;/pnicSpec&gt;
6367                 &lt;/backing&gt;
6368                 &lt;/spec&gt;
6369             &lt;/proxySwitch&gt;
6370             &lt;portgroup&gt;
6371               &lt;changeOperation&gt;remove&lt;/changeOperation&gt;
6372               &lt;spec&gt;
6373                 &lt;name&gt;Management Network&lt;/name&gt;&lt;vlanId&gt;-1&lt;/vlanId&gt;&lt;vswitchName /&gt;&lt;policy /&gt;
6374               &lt;/spec&gt;
6375             &lt;/portgroup&gt;
6376             &lt;vnic&gt;
6377               &lt;changeOperation&gt;edit&lt;/changeOperation&gt;
6378               &lt;device&gt;vmk0&lt;/device&gt;
6379               &lt;portgroup /&gt;
6380               &lt;spec&gt;
6381                 &lt;distributedVirtualPort&gt;
6382                   &lt;switchUuid&gt;73 a4 05 50 b0 d2 7e b9-38 80 5d 24 65 8f da 70&lt;/switchUuid&gt;
6383                   &lt;portgroupKey&gt;dvportgroup-191&lt;/portgroupKey&gt;
6384                 &lt;/distributedVirtualPort&gt;
6385               &lt;/spec&gt;
6386             &lt;/vnic&gt;
6387           &lt;/config&gt;
6388           &lt;changeMode&gt;modify&lt;/changeMode&gt;
6389         &lt;/UpdateNetworkConfig&gt;
6390     The SOAP API maps closely to PyVmomi, so from there it was (relatively)
6391     easy to figure out what Python to write.
6392     """
6393     ret = {}
6394     ret["success"] = True
6395     ret["message"] = []
6396     service_instance = salt.utils.vmware.get_service_instance(
6397         host=host,
6398         username=username,
6399         password=password,
6400         protocol=protocol,
6401         port=port,
6402         verify_ssl=verify_ssl,
6403     )
6404     dvs = salt.utils.vmware._get_dvs(service_instance, dvs_name)
6405     if not dvs:
6406         ret["message"].append(
6407             "No Distributed Virtual Switch found with name {}".format(dvs_name)
6408         )
6409         ret["success"] = False
6410     target_portgroup = salt.utils.vmware._get_dvs_portgroup(dvs, target_portgroup_name)
6411     if not target_portgroup:
6412         ret["message"].append(
6413             "No target portgroup found with name {}".format(target_portgroup_name)
6414         )
6415         ret["success"] = False
6416     uplink_portgroup = salt.utils.vmware._get_dvs_uplink_portgroup(
6417         dvs, uplink_portgroup_name
6418     )
6419     if not uplink_portgroup:
6420         ret["message"].append(
6421             "No uplink portgroup found with name {}".format(uplink_portgroup_name)
6422         )
6423         ret["success"] = False
6424     if len(ret["message"]) &gt; 0:
6425         return ret
6426     dvs_uuid = dvs.config.uuid
6427     try:
6428         host_names = _check_hosts(service_instance, host, host_names)
6429     except CommandExecutionError as e:
6430         ret["message"] = "Error retrieving hosts: {}".format(e.msg)
6431         return ret
6432     for host_name in host_names:
6433         ret[host_name] = {}
6434         ret[host_name].update(
6435             {
6436                 "status": False,
6437                 "uplink": uplink_portgroup_name,
6438                 "portgroup": target_portgroup_name,
6439                 "vmknic": vmknic_name,
6440                 "vmnic": vmnic_name,
6441                 "dvs": dvs_name,
6442             }
6443         )
6444         host_ref = _get_host_ref(service_instance, host, host_name)
6445         if not host_ref:
6446             ret[host_name].update({"message": "Host {1} not found".format(host_name)})
6447             ret["success"] = False
6448             continue
6449         dvs_hostmember_config = vim.dvs.HostMember.ConfigInfo(host=host_ref)
6450         dvs_hostmember = vim.dvs.HostMember(config=dvs_hostmember_config)
6451         p_nics = salt.utils.vmware._get_pnics(host_ref)
6452         p_nic = [x for x in p_nics if x.device == vmnic_name]
6453         if len(p_nic) == 0:
6454             ret[host_name].update(
6455                 {"message": "Physical nic {} not found".format(vmknic_name)}
6456             )
6457             ret["success"] = False
6458             continue
6459         v_nics = salt.utils.vmware._get_vnics(host_ref)
6460         v_nic = [x for x in v_nics if x.device == vmknic_name]
6461         if len(v_nic) == 0:
6462             ret[host_name].update(
6463                 {"message": "Virtual nic {} not found".format(vmnic_name)}
6464             )
6465             ret["success"] = False
6466             continue
6467         v_nic_mgr = salt.utils.vmware._get_vnic_manager(host_ref)
6468         if not v_nic_mgr:
6469             ret[host_name].update(
6470                 {"message": "Unable to get the host's virtual nic manager."}
6471             )
6472             ret["success"] = False
6473             continue
6474         dvs_pnic_spec = vim.dvs.HostMember.PnicSpec(
6475             pnicDevice=vmnic_name, uplinkPortgroupKey=uplink_portgroup.key
6476         )
6477         pnic_backing = vim.dvs.HostMember.PnicBacking(pnicSpec=[dvs_pnic_spec])
6478         dvs_hostmember_config_spec = vim.dvs.HostMember.ConfigSpec(
6479             host=host_ref,
6480             operation="add",
6481         )
6482         dvs_config = vim.DVSConfigSpec(
6483             configVersion=dvs.config.configVersion, host=[dvs_hostmember_config_spec]
6484         )
6485         task = dvs.ReconfigureDvs_Task(spec=dvs_config)
6486         try:
6487             salt.utils.vmware.wait_for_task(
6488                 task, host_name, "Adding host to the DVS", sleep_seconds=3
6489             )
6490         except Exception as e:  # pylint: disable=broad-except
6491             if hasattr(e, "message") and hasattr(e.message, "msg"):
6492                 if not (
6493                     host_name in e.message.msg and "already exists" in e.message.msg
6494                 ):
6495                     ret["success"] = False
6496                     ret[host_name].update({"message": e.message.msg})
6497                     continue
6498             else:
6499                 raise
6500         network_system = host_ref.configManager.networkSystem
6501         source_portgroup = None
6502         for pg in host_ref.config.network.portgroup:
6503             if pg.spec.name == v_nic[0].portgroup:
6504                 source_portgroup = pg
6505                 break
6506         if not source_portgroup:
6507             ret[host_name].update({"message": "No matching portgroup on the vSwitch"})
6508             ret["success"] = False
6509             continue
6510         virtual_nic_config = vim.HostVirtualNicConfig(
6511             changeOperation="edit",
6512             device=v_nic[0].device,
6513             portgroup=source_portgroup.spec.name,
6514             spec=vim.HostVirtualNicSpec(
6515                 distributedVirtualPort=vim.DistributedVirtualSwitchPortConnection(
6516                     portgroupKey=target_portgroup.key,
6517                     switchUuid=target_portgroup.config.distributedVirtualSwitch.uuid,
6518                 )
6519             ),
6520         )
6521         current_vswitch_ports = host_ref.config.network.vswitch[0].numPorts
6522         vswitch_config = vim.HostVirtualSwitchConfig(
6523             changeOperation="edit",
6524             name="vSwitch0",
6525             spec=vim.HostVirtualSwitchSpec(numPorts=current_vswitch_ports),
6526         )
6527         proxyswitch_config = vim.HostProxySwitchConfig(
6528             changeOperation="edit",
6529             uuid=dvs_uuid,
6530             spec=vim.HostProxySwitchSpec(backing=pnic_backing),
6531         )
6532         host_network_config = vim.HostNetworkConfig(
6533             vswitch=[vswitch_config],
6534             proxySwitch=[proxyswitch_config],
6535             portgroup=[
6536                 vim.HostPortGroupConfig(
6537                     changeOperation="remove", spec=source_portgroup.spec
6538                 )
6539             ],
6540             vnic=[virtual_nic_config],
6541         )
6542         try:
6543             network_system.UpdateNetworkConfig(
6544                 changeMode="modify", config=host_network_config
6545             )
6546             ret[host_name].update({"status": True})
6547         except Exception as e:  # pylint: disable=broad-except
6548             if hasattr(e, "msg"):
6549                 ret[host_name].update(
6550                     {"message": "Failed to migrate adapters ({})".format(e.msg)}
6551                 )
6552                 continue
6553             else:
6554                 raise
6555     return ret
6556 @depends(HAS_PYVMOMI)
6557 @_supports_proxies("esxi", "esxcluster", "esxdatacenter", "vcenter")
6558 def _get_proxy_target(service_instance):
6559     """
6560     Returns the target object of a proxy.
6561     If the object doesn't exist a VMwareObjectRetrievalError is raised
6562     service_instance
6563         Service instance (vim.ServiceInstance) of the vCenter/ESXi host.
6564     """
6565     proxy_type = get_proxy_type()
6566     if not salt.utils.vmware.is_connection_to_a_vcenter(service_instance):
6567         raise CommandExecutionError(
6568             "'_get_proxy_target' not supported when connected via the ESXi host"
6569         )
6570     reference = None
6571     if proxy_type == "esxcluster":
6572         (
6573             host,
6574             username,
6575             password,
6576             protocol,
6577             port,
6578             mechanism,
6579             principal,
6580             domain,
6581             datacenter,
6582             cluster,
6583         ) = _get_esxcluster_proxy_details()
6584         dc_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
6585         reference = salt.utils.vmware.get_cluster(dc_ref, cluster)
6586     elif proxy_type == "esxdatacenter":
6587         (
6588             host,
6589             username,
6590             password,
6591             protocol,
6592             port,
6593             mechanism,
6594             principal,
6595             domain,
6596             datacenter,
6597         ) = _get_esxdatacenter_proxy_details()
6598         reference = salt.utils.vmware.get_datacenter(service_instance, datacenter)
6599     elif proxy_type == "vcenter":
6600         reference = salt.utils.vmware.get_root_folder(service_instance)
6601     elif proxy_type == "esxi":
6602         details = __proxy__["esxi.get_details"]()
6603         if "vcenter" not in details:
6604             raise InvalidEntityError(
6605                 "Proxies connected directly to ESXi hosts are not supported"
6606             )
6607         references = salt.utils.vmware.get_hosts(
6608             service_instance, host_names=details["esxi_host"]
6609         )
6610         if not references:
6611             raise VMwareObjectRetrievalError(
6612                 "ESXi host '{}' was not found".format(details["esxi_host"])
6613             )
6614         reference = references[0]
6615     log.trace("reference = %s", reference)
6616     return reference
6617 def _get_esxdatacenter_proxy_details():
6618     """
6619     Returns the running esxdatacenter's proxy details
6620     """
6621     det = __salt__["esxdatacenter.get_details"]()
6622     return (
6623         det.get("vcenter"),
6624         det.get("username"),
6625         det.get("password"),
6626         det.get("protocol"),
6627         det.get("port"),
6628         det.get("mechanism"),
6629         det.get("principal"),
6630         det.get("domain"),
6631         det.get("datacenter"),
6632     )
6633 def _get_esxcluster_proxy_details():
6634     """
6635     Returns the running esxcluster's proxy details
6636     """
6637     det = __salt__["esxcluster.get_details"]()
6638     return (
6639         det.get("vcenter"),
6640         det.get("username"),
6641         det.get("password"),
6642         det.get("protocol"),
6643         det.get("port"),
6644         det.get("mechanism"),
6645         det.get("principal"),
6646         det.get("domain"),
6647         det.get("datacenter"),
6648         det.get("cluster"),
6649     )
6650 def _get_esxi_proxy_details():
6651     """
6652     Returns the running esxi's proxy details
6653     """
6654     det = __proxy__["esxi.get_details"]()
6655     host = det.get("host")
6656     if det.get("vcenter"):
6657         host = det["vcenter"]
6658     esxi_hosts = None
6659     if det.get("esxi_host"):
6660         esxi_hosts = [det["esxi_host"]]
6661     return (
6662         host,
6663         det.get("username"),
6664         det.get("password"),
6665         det.get("protocol"),
6666         det.get("port"),
6667         det.get("mechanism"),
6668         det.get("principal"),
6669         det.get("domain"),
6670         esxi_hosts,
6671     )
6672 @depends(HAS_PYVMOMI)
6673 @_gets_service_instance_via_proxy
6674 def get_vm(
6675     name,
6676     datacenter=None,
6677     vm_properties=None,
6678     traversal_spec=None,
6679     parent_ref=None,
6680     service_instance=None,
6681 ):
6682     """
6683     Returns vm object properties.
6684     name
6685         Name of the virtual machine.
6686     datacenter
6687         Datacenter name
6688     vm_properties
6689         List of vm properties.
6690     traversal_spec
6691         Traversal Spec object(s) for searching.
6692     parent_ref
6693         Container Reference object for searching under a given object.
6694     service_instance
6695         Service instance (vim.ServiceInstance) of the vCenter.
6696         Default is None.
6697     """
6698     virtual_machine = salt.utils.vmware.get_vm_by_property(
6699         service_instance,
6700         name,
6701         datacenter=datacenter,
6702         vm_properties=vm_properties,
6703         traversal_spec=traversal_spec,
6704         parent_ref=parent_ref,
6705     )
6706     return virtual_machine
6707 @depends(HAS_PYVMOMI)
6708 @_gets_service_instance_via_proxy
6709 def get_vm_config_file(name, datacenter, placement, datastore, service_instance=None):
6710     """
6711     Queries the virtual machine config file and returns
6712     vim.host.DatastoreBrowser.SearchResults object on success None on failure
6713     name
6714         Name of the virtual machine
6715     datacenter
6716         Datacenter name
6717     datastore
6718         Datastore where the virtual machine files are stored
6719     service_instance
6720         Service instance (vim.ServiceInstance) of the vCenter.
6721         Default is None.
6722     """
6723     browser_spec = vim.host.DatastoreBrowser.SearchSpec()
6724     directory = name
6725     browser_spec.query = [vim.host.DatastoreBrowser.VmConfigQuery()]
6726     datacenter_object = salt.utils.vmware.get_datacenter(service_instance, datacenter)
6727     if "cluster" in placement:
6728         container_object = salt.utils.vmware.get_cluster(
6729             datacenter_object, placement["cluster"]
6730         )
6731     else:
6732         container_objects = salt.utils.vmware.get_hosts(
6733             service_instance, datacenter_name=datacenter, host_names=[placement["host"]]
6734         )
6735         if not container_objects:
6736             raise salt.exceptions.VMwareObjectRetrievalError(
6737                 "ESXi host named '{}' wasn't found.".format(placement["host"])
6738             )
6739         container_object = container_objects[0]
6740     files = salt.utils.vmware.get_datastore_files(
6741         service_instance, directory, [datastore], container_object, browser_spec
6742     )
6743     if files and len(files[0].file) &gt; 1:
6744         raise salt.exceptions.VMwareMultipleObjectsError(
6745             "Multiple configuration files found in the same virtual machine folder"
6746         )
6747     elif files and files[0].file:
6748         return files[0]
6749     else:
6750         return None
6751 def _apply_hardware_version(hardware_version, config_spec, operation="add"):
6752     """
6753     Specifies vm container version or schedules upgrade,
6754     returns True on change and False if nothing have been changed.
6755     hardware_version
6756         Hardware version string eg. vmx-08
6757     config_spec
6758         Configuration spec object
6759     operation
6760         Defines the operation which should be used,
6761         the possibles values: 'add' and 'edit', the default value is 'add'
6762     """
6763     log.trace(
6764         "Configuring virtual machine hardware version version=%s", hardware_version
6765     )
6766     if operation == "edit":
6767         log.trace("Scheduling hardware version upgrade to %s", hardware_version)
6768         scheduled_hardware_upgrade = vim.vm.ScheduledHardwareUpgradeInfo()
6769         scheduled_hardware_upgrade.upgradePolicy = "always"
6770         scheduled_hardware_upgrade.versionKey = hardware_version
6771         config_spec.scheduledHardwareUpgradeInfo = scheduled_hardware_upgrade
6772     elif operation == "add":
6773         config_spec.version = str(hardware_version)
6774 def _apply_cpu_config(config_spec, cpu_props):
6775     """
6776     Sets CPU core count to the given value
6777     config_spec
6778         vm.ConfigSpec object
6779     cpu_props
6780         CPU properties dict
6781     """
6782     log.trace("Configuring virtual machine CPU settings cpu_props=%s", cpu_props)
6783     if "count" in cpu_props:
6784         config_spec.numCPUs = int(cpu_props["count"])
6785     if "cores_per_socket" in cpu_props:
6786         config_spec.numCoresPerSocket = int(cpu_props["cores_per_socket"])
6787     if "nested" in cpu_props and cpu_props["nested"]:
6788         config_spec.nestedHVEnabled = cpu_props["nested"]  # True
6789     if "hotadd" in cpu_props and cpu_props["hotadd"]:
6790         config_spec.cpuHotAddEnabled = cpu_props["hotadd"]  # True
6791     if "hotremove" in cpu_props and cpu_props["hotremove"]:
6792         config_spec.cpuHotRemoveEnabled = cpu_props["hotremove"]  # True
6793 def _apply_memory_config(config_spec, memory):
6794     """
6795     Sets memory size to the given value
6796     config_spec
6797         vm.ConfigSpec object
6798     memory
6799         Memory size and unit
6800     """
6801     log.trace("Configuring virtual machine memory settings memory=%s", memory)
6802     if "size" in memory and "unit" in memory:
6803         try:
6804             if memory["unit"].lower() == "kb":
6805                 memory_mb = memory["size"] / 1024
6806             elif memory["unit"].lower() == "mb":
6807                 memory_mb = memory["size"]
6808             elif memory["unit"].lower() == "gb":
6809                 memory_mb = int(float(memory["size"]) * 1024)
6810         except (TypeError, ValueError):
6811             memory_mb = int(memory["size"])
6812         config_spec.memoryMB = memory_mb
6813     if "reservation_max" in memory:
6814         config_spec.memoryReservationLockedToMax = memory["reservation_max"]
6815     if "hotadd" in memory:
6816         config_spec.memoryHotAddEnabled = memory["hotadd"]
6817 @depends(HAS_PYVMOMI)
6818 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
6819 @_gets_service_instance_via_proxy
6820 def get_advanced_configs(vm_name, datacenter, service_instance=None):
6821     """
6822     Returns extra config parameters from a virtual machine advanced config list
6823     vm_name
6824         Virtual machine name
6825     datacenter
6826         Datacenter name where the virtual machine is available
6827     service_instance
6828         vCenter service instance for connection and configuration
6829     """
6830     current_config = get_vm_config(
6831         vm_name, datacenter=datacenter, objects=True, service_instance=service_instance
6832     )
6833     return current_config["advanced_configs"]
6834 def _apply_advanced_config(config_spec, advanced_config, vm_extra_config=None):
6835     """
6836     Sets configuration parameters for the vm
6837     config_spec
6838         vm.ConfigSpec object
6839     advanced_config
6840         config key value pairs
6841     vm_extra_config
6842         Virtual machine vm_ref.config.extraConfig object
6843     """
6844     log.trace("Configuring advanced configuration parameters %s", advanced_config)
6845     if isinstance(advanced_config, str):
6846         raise salt.exceptions.ArgumentValueError(
6847             "The specified 'advanced_configs' configuration "
6848             "option cannot be parsed, please check the parameters"
6849         )
6850     for key, value in advanced_config.items():
6851         if vm_extra_config:
6852             for option in vm_extra_config:
6853                 if option.key == key and option.value == str(value):
6854                     continue
6855         else:
6856             option = vim.option.OptionValue(key=key, value=value)
6857             config_spec.extraConfig.append(option)
6858 @depends(HAS_PYVMOMI)
6859 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
6860 @_gets_service_instance_via_proxy
6861 def set_advanced_configs(vm_name, datacenter, advanced_configs, service_instance=None):
6862     """
6863     Appends extra config parameters to a virtual machine advanced config list
6864     vm_name
6865         Virtual machine name
6866     datacenter
6867         Datacenter name where the virtual machine is available
6868     advanced_configs
6869         Dictionary with advanced parameter key value pairs
6870     service_instance
6871         vCenter service instance for connection and configuration
6872     """
6873     current_config = get_vm_config(
6874         vm_name, datacenter=datacenter, objects=True, service_instance=service_instance
6875     )
6876     diffs = compare_vm_configs(
6877         {"name": vm_name, "advanced_configs": advanced_configs}, current_config
6878     )
6879     datacenter_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
6880     vm_ref = salt.utils.vmware.get_mor_by_property(
6881         service_instance,
6882         vim.VirtualMachine,
6883         vm_name,
6884         property_name="name",
6885         container_ref=datacenter_ref,
6886     )
6887     config_spec = vim.vm.ConfigSpec()
6888     changes = diffs["advanced_configs"].diffs
6889     _apply_advanced_config(
6890         config_spec, diffs["advanced_configs"].new_values, vm_ref.config.extraConfig
6891     )
6892     if changes:
6893         salt.utils.vmware.update_vm(vm_ref, config_spec)
6894     return {"advanced_config_changes": changes}
6895 def _delete_advanced_config(config_spec, advanced_config, vm_extra_config):
6896     """
6897     Removes configuration parameters for the vm
6898     config_spec
6899         vm.ConfigSpec object
6900     advanced_config
6901         List of advanced config keys to be deleted
6902     vm_extra_config
6903         Virtual machine vm_ref.config.extraConfig object
6904     """
6905     log.trace("Removing advanced configuration parameters %s", advanced_config)
6906     if isinstance(advanced_config, str):
6907         raise salt.exceptions.ArgumentValueError(
6908             "The specified 'advanced_configs' configuration "
6909             "option cannot be parsed, please check the parameters"
6910         )
6911     removed_configs = []
6912     for key in advanced_config:
6913         for option in vm_extra_config:
6914             if option.key == key:
6915                 option = vim.option.OptionValue(key=key, value="")
6916                 config_spec.extraConfig.append(option)
6917                 removed_configs.append(key)
6918     return removed_configs
6919 @depends(HAS_PYVMOMI)
6920 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
6921 @_gets_service_instance_via_proxy
6922 def delete_advanced_configs(
6923     vm_name, datacenter, advanced_configs, service_instance=None
6924 ):
6925     """
6926     Removes extra config parameters from a virtual machine
6927     vm_name
6928         Virtual machine name
6929     datacenter
6930         Datacenter name where the virtual machine is available
6931     advanced_configs
6932         List of advanced config values to be removed
6933     service_instance
6934         vCenter service instance for connection and configuration
6935     """
6936     datacenter_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
6937     vm_ref = salt.utils.vmware.get_mor_by_property(
6938         service_instance,
6939         vim.VirtualMachine,
6940         vm_name,
6941         property_name="name",
6942         container_ref=datacenter_ref,
6943     )
6944     config_spec = vim.vm.ConfigSpec()
6945     removed_configs = _delete_advanced_config(
6946         config_spec, advanced_configs, vm_ref.config.extraConfig
6947     )
6948     if removed_configs:
6949         salt.utils.vmware.update_vm(vm_ref, config_spec)
6950     return {"removed_configs": removed_configs}
6951 def _get_scsi_controller_key(bus_number, scsi_ctrls):
6952     """
6953     Returns key number of the SCSI controller keys
6954     bus_number
6955         Controller bus number from the adapter
6956     scsi_ctrls
6957         List of SCSI Controller objects (old+newly created)
6958     """
6959     keys = [
6960         ctrl.key for ctrl in scsi_ctrls if scsi_ctrls and ctrl.busNumber == bus_number
6961     ]
6962     if not keys:
6963         raise salt.exceptions.VMwareVmCreationError(
6964             "SCSI controller number {} doesn't exist".format(bus_number)
6965         )
6966     return keys[0]
6967 def _apply_hard_disk(
6968     unit_number,
6969     key,
6970     operation,
6971     disk_label=None,
6972     size=None,
6973     unit="GB",
6974     controller_key=None,
6975     thin_provision=None,
6976     eagerly_scrub=None,
6977     datastore=None,
6978     filename=None,
6979 ):
6980     """
6981     Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit
6982     a virtual disk device
6983     unit_number
6984         Add network adapter to this address
6985     key
6986         Device key number
6987     operation
6988         Action which should be done on the device add or edit
6989     disk_label
6990         Label of the new disk, can be overridden
6991     size
6992         Size of the disk
6993     unit
6994         Unit of the size, can be GB, MB, KB
6995     controller_key
6996         Unique umber of the controller key
6997     thin_provision
6998         Boolean for thin provision
6999     eagerly_scrub
7000         Boolean for eagerly scrubbing
7001     datastore
7002         Datastore name where the disk will be located
7003     filename
7004         Full file name of the vm disk
7005     """
7006     log.trace(
7007         "Configuring hard disk %s size=%s, unit=%s, controller_key=%s, "
7008         "thin_provision=%s, eagerly_scrub=%s, datastore=%s, filename=%s",
7009         disk_label,
7010         size,
7011         unit,
7012         controller_key,
7013         thin_provision,
7014         eagerly_scrub,
7015         datastore,
7016         filename,
7017     )
7018     disk_spec = vim.vm.device.VirtualDeviceSpec()
7019     disk_spec.device = vim.vm.device.VirtualDisk()
7020     disk_spec.device.key = key
7021     disk_spec.device.unitNumber = unit_number
7022     disk_spec.device.deviceInfo = vim.Description()
7023     if size:
7024         convert_size = salt.utils.vmware.convert_to_kb(unit, size)
7025         disk_spec.device.capacityInKB = convert_size["size"]
7026     if disk_label:
7027         disk_spec.device.deviceInfo.label = disk_label
7028     if thin_provision is not None or eagerly_scrub is not None:
7029         disk_spec.device.backing = vim.vm.device.VirtualDisk.FlatVer2BackingInfo()
7030         disk_spec.device.backing.diskMode = "persistent"
7031     if thin_provision is not None:
7032         disk_spec.device.backing.thinProvisioned = thin_provision
7033     if eagerly_scrub is not None and eagerly_scrub != "None":
7034         disk_spec.device.backing.eagerlyScrub = eagerly_scrub
7035     if controller_key:
7036         disk_spec.device.controllerKey = controller_key
7037     if operation == "add":
7038         disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7039         disk_spec.device.backing.fileName = "[{}] {}".format(
7040             salt.utils.vmware.get_managed_object_name(datastore), filename
7041         )
7042         disk_spec.fileOperation = vim.vm.device.VirtualDeviceSpec.FileOperation.create
7043     elif operation == "edit":
7044         disk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7045     return disk_spec
7046 def _create_adapter_type(network_adapter, adapter_type, network_adapter_label=""):
7047     """
7048     Returns a vim.vm.device.VirtualEthernetCard object specifying a virtual
7049     ethernet card information
7050     network_adapter
7051         None or VirtualEthernet object
7052     adapter_type
7053         String, type of adapter
7054     network_adapter_label
7055         string, network adapter name
7056     """
7057     log.trace(
7058         "Configuring virtual machine network adapter adapter_type=%s", adapter_type
7059     )
7060     if adapter_type in ["vmxnet", "vmxnet2", "vmxnet3", "e1000", "e1000e"]:
7061         edited_network_adapter = salt.utils.vmware.get_network_adapter_type(
7062             adapter_type
7063         )
7064         if isinstance(network_adapter, type(edited_network_adapter)):
7065             edited_network_adapter = network_adapter
7066         else:
7067             if network_adapter:
7068                 log.trace(
7069                     "Changing type of '%s' from '%s' to '%s'",
7070                     network_adapter.deviceInfo.label,
7071                     type(network_adapter).__name__.rsplit(".", 1)[1][7:].lower(),
7072                     adapter_type,
7073                 )
7074     else:
7075         if network_adapter:
7076             if adapter_type:
7077                 log.error(
7078                     "Cannot change type of '%s' to '%s'. Not changing type",
7079                     network_adapter.deviceInfo.label,
7080                     adapter_type,
7081                 )
7082             edited_network_adapter = network_adapter
7083         else:
7084             if not adapter_type:
7085                 log.trace(
7086                     "The type of '%s' has not been specified. "
7087                     "Creating of default type 'vmxnet3'",
7088                     network_adapter_label,
7089                 )
7090             edited_network_adapter = vim.vm.device.VirtualVmxnet3()
7091     return edited_network_adapter
7092 def _create_network_backing(network_name, switch_type, parent_ref):
7093     """
7094     Returns a vim.vm.device.VirtualDevice.BackingInfo object specifying a
7095     virtual ethernet card backing information
7096     network_name
7097         string, network name
7098     switch_type
7099         string, type of switch
7100     parent_ref
7101         Parent reference to search for network
7102     """
7103     log.trace(
7104         "Configuring virtual machine network backing network_name=%s "
7105         "switch_type=%s parent=%s",
7106         network_name,
7107         switch_type,
7108         salt.utils.vmware.get_managed_object_name(parent_ref),
7109     )
7110     backing = {}
7111     if network_name:
7112         if switch_type == "standard":
7113             networks = salt.utils.vmware.get_networks(
7114                 parent_ref, network_names=[network_name]
7115             )
7116             if not networks:
7117                 raise salt.exceptions.VMwareObjectRetrievalError(
7118                     "The network '{}' could not be retrieved.".format(network_name)
7119                 )
7120             network_ref = networks[0]
7121             backing = vim.vm.device.VirtualEthernetCard.NetworkBackingInfo()
7122             backing.deviceName = network_name
7123             backing.network = network_ref
7124         elif switch_type == "distributed":
7125             networks = salt.utils.vmware.get_dvportgroups(
7126                 parent_ref, portgroup_names=[network_name]
7127             )
7128             if not networks:
7129                 raise salt.exceptions.VMwareObjectRetrievalError(
7130                     "The port group '{}' could not be retrieved.".format(network_name)
7131                 )
7132             network_ref = networks[0]
7133             dvs_port_connection = vim.dvs.PortConnection(
7134                 portgroupKey=network_ref.key,
7135                 switchUuid=network_ref.config.distributedVirtualSwitch.uuid,
7136             )
7137             backing = (
7138                 vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo()
7139             )
7140             backing.port = dvs_port_connection
7141     return backing
7142 def _apply_network_adapter_config(
7143     key,
7144     network_name,
7145     adapter_type,
7146     switch_type,
7147     network_adapter_label=None,
7148     operation="add",
7149     connectable=None,
7150     mac=None,
7151     parent=None,
7152 ):
7153     """
7154     Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit a
7155     network device
7156     network_adapter_label
7157         Network adapter label
7158     key
7159         Unique key for device creation
7160     network_name
7161         Network or port group name
7162     adapter_type
7163         Type of the adapter eg. vmxnet3
7164     switch_type
7165         Type of the switch: standard or distributed
7166     operation
7167         Type of operation: add or edit
7168     connectable
7169         Dictionary with the device connection properties
7170     mac
7171         MAC address of the network adapter
7172     parent
7173         Parent object reference
7174     """
7175     adapter_type.strip().lower()
7176     switch_type.strip().lower()
7177     log.trace(
7178         "Configuring virtual machine network adapter network_adapter_label=%s "
7179         "network_name=%s adapter_type=%s switch_type=%s mac=%s",
7180         network_adapter_label,
7181         network_name,
7182         adapter_type,
7183         switch_type,
7184         mac,
7185     )
7186     network_spec = vim.vm.device.VirtualDeviceSpec()
7187     network_spec.device = _create_adapter_type(
7188         network_spec.device, adapter_type, network_adapter_label=network_adapter_label
7189     )
7190     network_spec.device.deviceInfo = vim.Description()
7191     if operation == "add":
7192         network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7193     elif operation == "edit":
7194         network_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7195     if switch_type and network_name:
7196         network_spec.device.backing = _create_network_backing(
7197             network_name, switch_type, parent
7198         )
7199         network_spec.device.deviceInfo.summary = network_name
7200     if key:
7201         network_spec.device.key = key
7202     if network_adapter_label:
7203         network_spec.device.deviceInfo.label = network_adapter_label
7204     if mac:
7205         network_spec.device.macAddress = mac
7206         network_spec.device.addressType = "Manual"
7207     network_spec.device.wakeOnLanEnabled = True
7208     if connectable:
7209         network_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
7210         network_spec.device.connectable.startConnected = connectable["start_connected"]
7211         network_spec.device.connectable.allowGuestControl = connectable[
7212             "allow_guest_control"
7213         ]
7214     return network_spec
7215 def _apply_scsi_controller(
7216     adapter, adapter_type, bus_sharing, key, bus_number, operation
7217 ):
7218     """
7219     Returns a vim.vm.device.VirtualDeviceSpec object specifying to
7220     add/edit a SCSI controller
7221     adapter
7222         SCSI controller adapter name
7223     adapter_type
7224         SCSI controller adapter type eg. paravirtual
7225     bus_sharing
7226          SCSI controller bus sharing eg. virtual_sharing
7227     key
7228         SCSI controller unique key
7229     bus_number
7230         Device bus number property
7231     operation
7232         Describes the operation which should be done on the object,
7233         the possibles values: 'add' and 'edit', the default value is 'add'
7234     .. code-block:: bash
7235         scsi:
7236           adapter: 'SCSI controller 0'
7237           type: paravirtual or lsilogic or lsilogic_sas
7238           bus_sharing: 'no_sharing' or 'virtual_sharing' or 'physical_sharing'
7239     """
7240     log.trace(
7241         "Configuring scsi controller adapter=%s adapter_type=%s "
7242         "bus_sharing=%s key=%s bus_number=%s",
7243         adapter,
7244         adapter_type,
7245         bus_sharing,
7246         key,
7247         bus_number,
7248     )
7249     scsi_spec = vim.vm.device.VirtualDeviceSpec()
7250     if adapter_type == "lsilogic":
7251         summary = "LSI Logic"
7252         scsi_spec.device = vim.vm.device.VirtualLsiLogicController()
7253     elif adapter_type == "lsilogic_sas":
7254         summary = "LSI Logic Sas"
7255         scsi_spec.device = vim.vm.device.VirtualLsiLogicSASController()
7256     elif adapter_type == "paravirtual":
7257         summary = "VMware paravirtual SCSI"
7258         scsi_spec.device = vim.vm.device.ParaVirtualSCSIController()
7259     elif adapter_type == "buslogic":
7260         summary = "Bus Logic"
7261         scsi_spec.device = vim.vm.device.VirtualBusLogicController()
7262     if operation == "add":
7263         scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7264     elif operation == "edit":
7265         scsi_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7266     scsi_spec.device.key = key
7267     scsi_spec.device.busNumber = bus_number
7268     scsi_spec.device.deviceInfo = vim.Description()
7269     scsi_spec.device.deviceInfo.label = adapter
7270     scsi_spec.device.deviceInfo.summary = summary
7271     if bus_sharing == "virtual_sharing":
7272         scsi_spec.device.sharedBus = (
7273             vim.vm.device.VirtualSCSIController.Sharing.virtualSharing
7274         )
7275     elif bus_sharing == "physical_sharing":
7276         scsi_spec.device.sharedBus = (
7277             vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
7278         )
7279     elif bus_sharing == "no_sharing":
7280         scsi_spec.device.sharedBus = (
7281             vim.vm.device.VirtualSCSIController.Sharing.noSharing
7282         )
7283     return scsi_spec
7284 def _create_ide_controllers(ide_controllers):
7285     """
7286     Returns a list of vim.vm.device.VirtualDeviceSpec objects representing
7287     IDE controllers
7288     ide_controllers
7289         IDE properties
7290     """
7291     ide_ctrls = []
7292     keys = range(-200, -250, -1)
7293     if ide_controllers:
7294         devs = [ide["adapter"] for ide in ide_controllers]
7295         log.trace("Creating IDE controllers %s", devs)
7296         for ide, key in zip(ide_controllers, keys):
7297             ide_ctrls.append(
7298                 _apply_ide_controller_config(ide["adapter"], "add", key, abs(key + 200))
7299             )
7300     return ide_ctrls
7301 def _apply_ide_controller_config(ide_controller_label, operation, key, bus_number=0):
7302     """
7303     Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit an
7304     IDE controller
7305     ide_controller_label
7306         Controller label of the IDE adapter
7307     operation
7308         Type of operation: add or edit
7309     key
7310         Unique key of the device
7311     bus_number
7312         Device bus number property
7313     """
7314     log.trace(
7315         "Configuring IDE controller ide_controller_label=%s", ide_controller_label
7316     )
7317     ide_spec = vim.vm.device.VirtualDeviceSpec()
7318     ide_spec.device = vim.vm.device.VirtualIDEController()
7319     if operation == "add":
7320         ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7321     if operation == "edit":
7322         ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7323     ide_spec.device.key = key
7324     ide_spec.device.busNumber = bus_number
7325     if ide_controller_label:
7326         ide_spec.device.deviceInfo = vim.Description()
7327         ide_spec.device.deviceInfo.label = ide_controller_label
7328         ide_spec.device.deviceInfo.summary = ide_controller_label
7329     return ide_spec
7330 def _create_sata_controllers(sata_controllers):
7331     """
7332     Returns a list of vim.vm.device.VirtualDeviceSpec objects representing
7333     SATA controllers
7334     sata_controllers
7335         SATA properties
7336     """
7337     sata_ctrls = []
7338     keys = range(-15000, -15050, -1)
7339     if sata_controllers:
7340         devs = [sata["adapter"] for sata in sata_controllers]
7341         log.trace("Creating SATA controllers %s", devs)
7342         for sata, key in zip(sata_controllers, keys):
7343             sata_ctrls.append(
7344                 _apply_sata_controller_config(
7345                     sata["adapter"], "add", key, sata["bus_number"]
7346                 )
7347             )
7348     return sata_ctrls
7349 def _apply_sata_controller_config(sata_controller_label, operation, key, bus_number=0):
7350     """
7351     Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit a
7352     SATA controller
7353     sata_controller_label
7354         Controller label of the SATA adapter
7355     operation
7356         Type of operation: add or edit
7357     key
7358         Unique key of the device
7359     bus_number
7360         Device bus number property
7361     """
7362     log.trace(
7363         "Configuring SATA controller sata_controller_label=%s", sata_controller_label
7364     )
7365     sata_spec = vim.vm.device.VirtualDeviceSpec()
7366     sata_spec.device = vim.vm.device.VirtualAHCIController()
7367     if operation == "add":
7368         sata_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7369     elif operation == "edit":
7370         sata_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7371     sata_spec.device.key = key
7372     sata_spec.device.controllerKey = 100
7373     sata_spec.device.busNumber = bus_number
7374     if sata_controller_label:
7375         sata_spec.device.deviceInfo = vim.Description()
7376         sata_spec.device.deviceInfo.label = sata_controller_label
7377         sata_spec.device.deviceInfo.summary = sata_controller_label
7378     return sata_spec
7379 def _apply_cd_drive(
7380     drive_label,
7381     key,
7382     device_type,
7383     operation,
7384     client_device=None,
7385     datastore_iso_file=None,
7386     connectable=None,
7387     controller_key=200,
7388     parent_ref=None,
7389 ):
7390     """
7391     Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit a
7392     CD/DVD drive
7393     drive_label
7394         Leble of the CD/DVD drive
7395     key
7396         Unique key of the device
7397     device_type
7398         Type of the device: client or iso
7399     operation
7400         Type of operation: add or edit
7401     client_device
7402         Client device properties
7403     datastore_iso_file
7404         ISO properties
7405     connectable
7406         Connection info for the device
7407     controller_key
7408         Controller unique identifier to which we will attach this device
7409     parent_ref
7410         Parent object
7411     .. code-block:: bash
7412         cd:
7413             adapter: "CD/DVD drive 1"
7414             device_type: datastore_iso_file or client_device
7415             client_device:
7416               mode: atapi or passthrough
7417             datastore_iso_file:
7418               path: "[share] iso/disk.iso"
7419             connectable:
7420               start_connected: True
7421               allow_guest_control:
7422     """
7423     log.trace(
7424         "Configuring CD/DVD drive drive_label=%s device_type=%s "
7425         "client_device=%s datastore_iso_file=%s",
7426         drive_label,
7427         device_type,
7428         client_device,
7429         datastore_iso_file,
7430     )
7431     drive_spec = vim.vm.device.VirtualDeviceSpec()
7432     drive_spec.device = vim.vm.device.VirtualCdrom()
7433     drive_spec.device.deviceInfo = vim.Description()
7434     if operation == "add":
7435         drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7436     elif operation == "edit":
7437         drive_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7438     if device_type == "datastore_iso_file":
7439         drive_spec.device.backing = vim.vm.device.VirtualCdrom.IsoBackingInfo()
7440         drive_spec.device.backing.fileName = datastore_iso_file["path"]
7441         datastore = datastore_iso_file["path"].partition("[")[-1].rpartition("]")[0]
7442         datastore_object = salt.utils.vmware.get_datastores(
7443             salt.utils.vmware.get_service_instance_from_managed_object(parent_ref),
7444             parent_ref,
7445             datastore_names=[datastore],
7446         )[0]
7447         if datastore_object:
7448             drive_spec.device.backing.datastore = datastore_object
7449         drive_spec.device.deviceInfo.summary = "{}".format(datastore_iso_file["path"])
7450     elif device_type == "client_device":
7451         if client_device["mode"] == "passthrough":
7452             drive_spec.device.backing = (
7453                 vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo()
7454             )
7455         elif client_device["mode"] == "atapi":
7456             drive_spec.device.backing = (
7457                 vim.vm.device.VirtualCdrom.RemoteAtapiBackingInfo()
7458             )
7459     drive_spec.device.key = key
7460     drive_spec.device.deviceInfo.label = drive_label
7461     drive_spec.device.controllerKey = controller_key
7462     drive_spec.device.connectable = vim.vm.device.VirtualDevice.ConnectInfo()
7463     if connectable:
7464         drive_spec.device.connectable.startConnected = connectable["start_connected"]
7465         drive_spec.device.connectable.allowGuestControl = connectable[
7466             "allow_guest_control"
7467         ]
7468     return drive_spec
7469 def _set_network_adapter_mapping(domain, gateway, ip_addr, subnet_mask, mac):
7470     """
7471     Returns a vim.vm.customization.AdapterMapping object containing the IP
7472     properties of a network adapter card
7473     domain
7474         Domain of the host
7475     gateway
7476         Gateway address
7477     ip_addr
7478         IP address
7479     subnet_mask
7480         Subnet mask
7481     mac
7482         MAC address of the guest
7483     """
7484     adapter_mapping = vim.vm.customization.AdapterMapping()
7485     adapter_mapping.macAddress = mac
7486     adapter_mapping.adapter = vim.vm.customization.IPSettings()
7487     if domain:
7488         adapter_mapping.adapter.dnsDomain = domain
7489     if gateway:
7490         adapter_mapping.adapter.gateway = gateway
7491     if ip_addr:
7492         adapter_mapping.adapter.ip = vim.vm.customization.FixedIp(ipAddress=ip_addr)
7493         adapter_mapping.adapter.subnetMask = subnet_mask
7494     else:
7495         adapter_mapping.adapter.ip = vim.vm.customization.DhcpIpGenerator()
7496     return adapter_mapping
7497 def _apply_serial_port(serial_device_spec, key, operation="add"):
7498     """
7499     Returns a vim.vm.device.VirtualSerialPort representing a serial port
7500     component
7501     serial_device_spec
7502         Serial device properties
7503     key
7504         Unique key of the device
7505     operation
7506         Add or edit the given device
7507     .. code-block:: bash
7508         serial_ports:
7509             adapter: 'Serial port 1'
7510             backing:
7511               type: uri
7512               uri: 'telnet://something:port'
7513               direction: &lt;client|server&gt;
7514               filename: 'service_uri'
7515             connectable:
7516               allow_guest_control: True
7517               start_connected: True
7518             yield: False
7519     """
7520     log.trace(
7521         "Creating serial port adapter=%s type=%s connectable=%s yield=%s",
7522         serial_device_spec["adapter"],
7523         serial_device_spec["type"],
7524         serial_device_spec["connectable"],
7525         serial_device_spec["yield"],
7526     )
7527     device_spec = vim.vm.device.VirtualDeviceSpec()
7528     device_spec.device = vim.vm.device.VirtualSerialPort()
7529     if operation == "add":
7530         device_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add
7531     elif operation == "edit":
7532         device_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit
7533     connect_info = vim.vm.device.VirtualDevice.ConnectInfo()
7534     type_backing = None
7535     if serial_device_spec["type"] == "network":
7536         type_backing = vim.vm.device.VirtualSerialPort.URIBackingInfo()
7537         if "uri" not in serial_device_spec["backing"].keys():
7538             raise ValueError("vSPC proxy URI not specified in config")
7539         if "uri" not in serial_device_spec["backing"].keys():
7540             raise ValueError("vSPC Direction not specified in config")
7541         if "filename" not in serial_device_spec["backing"].keys():
7542             raise ValueError("vSPC Filename not specified in config")
7543         type_backing.proxyURI = serial_device_spec["backing"]["uri"]
7544         type_backing.direction = serial_device_spec["backing"]["direction"]
7545         type_backing.serviceURI = serial_device_spec["backing"]["filename"]
7546     if serial_device_spec["type"] == "pipe":
7547         type_backing = vim.vm.device.VirtualSerialPort.PipeBackingInfo()
7548     if serial_device_spec["type"] == "file":
7549         type_backing = vim.vm.device.VirtualSerialPort.FileBackingInfo()
7550     if serial_device_spec["type"] == "device":
7551         type_backing = vim.vm.device.VirtualSerialPort.DeviceBackingInfo()
7552     connect_info.allowGuestControl = serial_device_spec["connectable"][
7553         "allow_guest_control"
7554     ]
7555     connect_info.startConnected = serial_device_spec["connectable"]["start_connected"]
7556     device_spec.device.backing = type_backing
7557     device_spec.device.connectable = connect_info
7558     device_spec.device.unitNumber = 1
7559     device_spec.device.key = key
7560     device_spec.device.yieldOnPoll = serial_device_spec["yield"]
7561     return device_spec
7562 def _create_disks(service_instance, disks, scsi_controllers=None, parent=None):
7563     """
7564     Returns a list of disk specs representing the disks to be created for a
7565     virtual machine
7566     service_instance
7567         Service instance (vim.ServiceInstance) of the vCenter.
7568         Default is None.
7569     disks
7570         List of disks with properties
7571     scsi_controllers
7572         List of SCSI controllers
7573     parent
7574         Parent object reference
7575     .. code-block:: bash
7576         disk:
7577           adapter: 'Hard disk 1'
7578           size: 16
7579           unit: GB
7580           address: '0:0'
7581           controller: 'SCSI controller 0'
7582           thin_provision: False
7583           eagerly_scrub: False
7584           datastore: 'myshare'
7585           filename: 'vm/mydisk.vmdk'
7586     """
7587     disk_specs = []
7588     keys = range(-2000, -2050, -1)
7589     if disks:
7590         devs = [disk["adapter"] for disk in disks]
7591         log.trace("Creating disks %s", devs)
7592     for disk, key in zip(disks, keys):
7593         filename, datastore, datastore_ref = None, None, None
7594         size = float(disk["size"])
7595         controller_key = 1000  # Default is the first SCSI controller
7596         if "address" in disk:  # 0:0
7597             controller_bus_number, unit_number = disk["address"].split(":")
7598             controller_bus_number = int(controller_bus_number)
7599             unit_number = int(unit_number)
7600             controller_key = _get_scsi_controller_key(
7601                 controller_bus_number, scsi_ctrls=scsi_controllers
7602             )
7603         elif "controller" in disk:
7604             for contr in scsi_controllers:
7605                 if contr["label"] == disk["controller"]:
7606                     controller_key = contr["key"]
7607                     break
7608             else:
7609                 raise salt.exceptions.VMwareObjectNotFoundError(
7610                     "The given controller does not exist: {}".format(disk["controller"])
7611                 )
7612         if "datastore" in disk:
7613             datastore_ref = salt.utils.vmware.get_datastores(
7614                 service_instance, parent, datastore_names=[disk["datastore"]]
7615             )[0]
7616             datastore = disk["datastore"]
7617         if "filename" in disk:
7618             filename = disk["filename"]
7619         if (not filename and datastore) or (filename and not datastore):
7620             raise salt.exceptions.ArgumentValueError(
7621                 "You must specify both filename and datastore attributes"
7622                 " to place your disk to a specific datastore "
7623                 "{}, {}".format(datastore, filename)
7624             )
7625         disk_spec = _apply_hard_disk(
7626             unit_number,
7627             key,
7628             disk_label=disk["adapter"],
7629             size=size,
7630             unit=disk["unit"],
7631             controller_key=controller_key,
7632             operation="add",
7633             thin_provision=disk["thin_provision"],
7634             eagerly_scrub=disk["eagerly_scrub"] if "eagerly_scrub" in disk else None,
7635             datastore=datastore_ref,
7636             filename=filename,
7637         )
7638         disk_specs.append(disk_spec)
7639         unit_number += 1
7640     return disk_specs
7641 def _create_scsi_devices(scsi_devices):
7642     """
7643     Returns a list of vim.vm.device.VirtualDeviceSpec objects representing
7644     SCSI controllers
7645     scsi_devices:
7646         List of SCSI device properties
7647     """
7648     keys = range(-1000, -1050, -1)
7649     scsi_specs = []
7650     if scsi_devices:
7651         devs = [scsi["adapter"] for scsi in scsi_devices]
7652         log.trace("Creating SCSI devices %s", devs)
7653         for (key, scsi_controller) in zip(keys, scsi_devices):
7654             scsi_spec = _apply_scsi_controller(
7655                 scsi_controller["adapter"],
7656                 scsi_controller["type"],
7657                 scsi_controller["bus_sharing"],
7658                 key,
7659                 scsi_controller["bus_number"],
7660                 "add",
7661             )
7662             scsi_specs.append(scsi_spec)
7663     return scsi_specs
7664 def _create_network_adapters(network_interfaces, parent=None):
7665     """
7666     Returns a list of vim.vm.device.VirtualDeviceSpec objects representing
7667     the interfaces to be created for a virtual machine
7668     network_interfaces
7669         List of network interfaces and properties
7670     parent
7671         Parent object reference
7672     .. code-block:: bash
7673         interfaces:
7674           adapter: 'Network adapter 1'
7675           name: vlan100
7676           switch_type: distributed or standard
7677           adapter_type: vmxnet3 or vmxnet, vmxnet2, vmxnet3, e1000, e1000e
7678           mac: '00:11:22:33:44:55'
7679     """
7680     network_specs = []
7681     nics_settings = []
7682     keys = range(-4000, -4050, -1)
7683     if network_interfaces:
7684         devs = [inter["adapter"] for inter in network_interfaces]
7685         log.trace("Creating network interfaces %s", devs)
7686         for interface, key in zip(network_interfaces, keys):
7687             network_spec = _apply_network_adapter_config(
7688                 key,
7689                 interface["name"],
7690                 interface["adapter_type"],
7691                 interface["switch_type"],
7692                 network_adapter_label=interface["adapter"],
7693                 operation="add",
7694                 connectable=interface["connectable"]
7695                 if "connectable" in interface
7696                 else None,
7697                 mac=interface["mac"],
7698                 parent=parent,
7699             )
7700             network_specs.append(network_spec)
7701             if "mapping" in interface:
7702                 adapter_mapping = _set_network_adapter_mapping(
7703                     interface["mapping"]["domain"],
7704                     interface["mapping"]["gateway"],
7705                     interface["mapping"]["ip_addr"],
7706                     interface["mapping"]["subnet_mask"],
7707                     interface["mac"],
7708                 )
7709                 nics_settings.append(adapter_mapping)
7710     return (network_specs, nics_settings)
7711 def _create_serial_ports(serial_ports):
7712     """
7713     Returns a list of vim.vm.device.VirtualDeviceSpec objects representing the
7714     serial ports to be created for a virtual machine
7715     serial_ports
7716         Serial port properties
7717     """
7718     ports = []
7719     keys = range(-9000, -9050, -1)
7720     if serial_ports:
7721         devs = [serial["adapter"] for serial in serial_ports]
7722         log.trace("Creating serial ports %s", devs)
7723         for port, key in zip(serial_ports, keys):
7724             serial_port_device = _apply_serial_port(port, key, "add")
7725             ports.append(serial_port_device)
7726     return ports
7727 def _create_cd_drives(cd_drives, controllers=None, parent_ref=None):
7728     """
7729     Returns a list of vim.vm.device.VirtualDeviceSpec objects representing the
7730     CD/DVD drives to be created for a virtual machine
7731     cd_drives
7732         CD/DVD drive properties
7733     controllers
7734         CD/DVD drive controllers (IDE, SATA)
7735     parent_ref
7736         Parent object reference
7737     """
7738     cd_drive_specs = []
7739     keys = range(-3000, -3050, -1)
7740     if cd_drives:
7741         devs = [dvd["adapter"] for dvd in cd_drives]
7742         log.trace("Creating cd/dvd drives %s", devs)
7743         for drive, key in zip(cd_drives, keys):
7744             controller_key = 200
7745             if controllers:
7746                 controller = _get_device_by_label(controllers, drive["controller"])
7747                 controller_key = controller.key
7748             cd_drive_specs.append(
7749                 _apply_cd_drive(
7750                     drive["adapter"],
7751                     key,
7752                     drive["device_type"],
7753                     "add",
7754                     client_device=drive["client_device"]
7755                     if "client_device" in drive
7756                     else None,
7757                     datastore_iso_file=drive["datastore_iso_file"]
7758                     if "datastore_iso_file" in drive
7759                     else None,
7760                     connectable=drive["connectable"]
7761                     if "connectable" in drive
7762                     else None,
7763                     controller_key=controller_key,
7764                     parent_ref=parent_ref,
7765                 )
7766             )
7767     return cd_drive_specs
7768 def _get_device_by_key(devices, key):
7769     """
7770     Returns the device with the given key, raises error if the device is
7771     not found.
7772     devices
7773         list of vim.vm.device.VirtualDevice objects
7774     key
7775         Unique key of device
7776     """
7777     device_keys = [d for d in devices if d.key == key]
7778     if device_keys:
7779         return device_keys[0]
7780     else:
7781         raise salt.exceptions.VMwareObjectNotFoundError(
7782             "Virtual machine device with unique key {} does not exist".format(key)
7783         )
7784 def _get_device_by_label(devices, label):
7785     """
7786     Returns the device with the given label, raises error if the device is
7787     not found.
7788     devices
7789         list of vim.vm.device.VirtualDevice objects
7790     key
7791         Unique key of device
7792     """
7793     device_labels = [d for d in devices if d.deviceInfo.label == label]
7794     if device_labels:
7795         return device_labels[0]
7796     else:
7797         raise salt.exceptions.VMwareObjectNotFoundError(
7798             "Virtual machine device with label {} does not exist".format(label)
7799         )
7800 def _convert_units(devices):
7801     """
7802     Updates the size and unit dictionary values with the new unit values
7803     devices
7804         List of device data objects
7805     """
7806     if devices:
7807         for device in devices:
7808             if "unit" in device and "size" in device:
7809                 device.update(
7810                     salt.utils.vmware.convert_to_kb(device["unit"], device["size"])
7811                 )
7812     else:
7813         return False
7814     return True
7815 def compare_vm_configs(new_config, current_config):
7816     """
7817     Compares virtual machine current and new configuration, the current is the
7818     one which is deployed now, and the new is the target config. Returns the
7819     differences between the objects in a dictionary, the keys are the
7820     configuration parameter keys and the values are differences objects: either
7821     list or recursive difference
7822     new_config:
7823         New config dictionary with every available parameter
7824     current_config
7825         Currently deployed configuration
7826     """
7827     diffs = {}
7828     keys = set(new_config.keys())
7829     keys.discard("name")
7830     keys.discard("datacenter")
7831     keys.discard("datastore")
7832     for property_key in ("version", "image"):
7833         if property_key in keys:
7834             single_value_diff = recursive_diff(
7835                 {property_key: current_config[property_key]},
7836                 {property_key: new_config[property_key]},
7837             )
7838             if single_value_diff.diffs:
7839                 diffs[property_key] = single_value_diff
7840             keys.discard(property_key)
7841     if "cpu" in keys:
7842         keys.remove("cpu")
7843         cpu_diff = recursive_diff(current_config["cpu"], new_config["cpu"])
7844         if cpu_diff.diffs:
7845             diffs["cpu"] = cpu_diff
7846     if "memory" in keys:
7847         keys.remove("memory")
7848         _convert_units([current_config["memory"]])
7849         _convert_units([new_config["memory"]])
7850         memory_diff = recursive_diff(current_config["memory"], new_config["memory"])
7851         if memory_diff.diffs:
7852             diffs["memory"] = memory_diff
7853     if "advanced_configs" in keys:
7854         keys.remove("advanced_configs")
7855         key = "advanced_configs"
7856         advanced_diff = recursive_diff(current_config[key], new_config[key])
7857         if advanced_diff.diffs:
7858             diffs[key] = advanced_diff
7859     if "disks" in keys:
7860         keys.remove("disks")
7861         _convert_units(current_config["disks"])
7862         _convert_units(new_config["disks"])
7863         disk_diffs = list_diff(current_config["disks"], new_config["disks"], "address")
7864         disk_diffs.remove_diff(diff_key="eagerly_scrub")
7865         disk_diffs.remove_diff(diff_key="filename")
7866         disk_diffs.remove_diff(diff_key="adapter")
7867         if disk_diffs.diffs:
7868             diffs["disks"] = disk_diffs
7869     if "interfaces" in keys:
7870         keys.remove("interfaces")
7871         interface_diffs = list_diff(
7872             current_config["interfaces"], new_config["interfaces"], "mac"
7873         )
7874         interface_diffs.remove_diff(diff_key="adapter")
7875         if interface_diffs.diffs:
7876             diffs["interfaces"] = interface_diffs
7877     for key in keys:
7878         if key not in current_config or key not in new_config:
7879             raise ValueError(
7880                 "A general device {} configuration was "
7881                 "not supplied or it was not retrieved from "
7882                 "remote configuration".format(key)
7883             )
7884         device_diffs = list_diff(current_config[key], new_config[key], "adapter")
7885         if device_diffs.diffs:
7886             diffs[key] = device_diffs
7887     return diffs
7888 @_gets_service_instance_via_proxy
7889 def get_vm_config(name, datacenter=None, objects=True, service_instance=None):
7890     """
7891     Queries and converts the virtual machine properties to the available format
7892     from the schema. If the objects attribute is True the config objects will
7893     have extra properties, like 'object' which will include the
7894     vim.vm.device.VirtualDevice, this is necessary for deletion and update
7895     actions.
7896     name
7897         Name of the virtual machine
7898     datacenter
7899         Datacenter's name where the virtual machine is available
7900     objects
7901         Indicates whether to return the vmware object properties
7902         (eg. object, key) or just the properties which can be set
7903     service_instance
7904         vCenter service instance for connection and configuration
7905     """
7906     properties = [
7907         "config.hardware.device",
7908         "config.hardware.numCPU",
7909         "config.hardware.numCoresPerSocket",
7910         "config.nestedHVEnabled",
7911         "config.cpuHotAddEnabled",
7912         "config.cpuHotRemoveEnabled",
7913         "config.hardware.memoryMB",
7914         "config.memoryReservationLockedToMax",
7915         "config.memoryHotAddEnabled",
7916         "config.version",
7917         "config.guestId",
7918         "config.extraConfig",
7919         "name",
7920     ]
7921     virtual_machine = salt.utils.vmware.get_vm_by_property(
7922         service_instance, name, vm_properties=properties, datacenter=datacenter
7923     )
7924     parent_ref = salt.utils.vmware.get_datacenter(
7925         service_instance=service_instance, datacenter_name=datacenter
7926     )
7927     current_config = {"name": name}
7928     current_config["cpu"] = {
7929         "count": virtual_machine["config.hardware.numCPU"],
7930         "cores_per_socket": virtual_machine["config.hardware.numCoresPerSocket"],
7931         "nested": virtual_machine["config.nestedHVEnabled"],
7932         "hotadd": virtual_machine["config.cpuHotAddEnabled"],
7933         "hotremove": virtual_machine["config.cpuHotRemoveEnabled"],
7934     }
7935     current_config["memory"] = {
7936         "size": virtual_machine["config.hardware.memoryMB"],
7937         "unit": "MB",
7938         "reservation_max": virtual_machine["config.memoryReservationLockedToMax"],
7939         "hotadd": virtual_machine["config.memoryHotAddEnabled"],
7940     }
7941     current_config["image"] = virtual_machine["config.guestId"]
7942     current_config["version"] = virtual_machine["config.version"]
7943     current_config["advanced_configs"] = {}
7944     for extra_conf in virtual_machine["config.extraConfig"]:
7945         try:
7946             current_config["advanced_configs"][extra_conf.key] = int(extra_conf.value)
7947         except ValueError:
7948             current_config["advanced_configs"][extra_conf.key] = extra_conf.value
7949     current_config["disks"] = []
7950     current_config["scsi_devices"] = []
7951     current_config["interfaces"] = []
7952     current_config["serial_ports"] = []
7953     current_config["cd_drives"] = []
7954     current_config["sata_controllers"] = []
7955     for device in virtual_machine["config.hardware.device"]:
7956         if isinstance(device, vim.vm.device.VirtualSCSIController):
7957             controller = {}
7958             controller["adapter"] = device.deviceInfo.label
7959             controller["bus_number"] = device.busNumber
7960             bus_sharing = device.sharedBus
7961             if bus_sharing == "noSharing":
7962                 controller["bus_sharing"] = "no_sharing"
7963             elif bus_sharing == "virtualSharing":
7964                 controller["bus_sharing"] = "virtual_sharing"
7965             elif bus_sharing == "physicalSharing":
7966                 controller["bus_sharing"] = "physical_sharing"
7967             if isinstance(device, vim.vm.device.ParaVirtualSCSIController):
7968                 controller["type"] = "paravirtual"
7969             elif isinstance(device, vim.vm.device.VirtualBusLogicController):
7970                 controller["type"] = "buslogic"
7971             elif isinstance(device, vim.vm.device.VirtualLsiLogicController):
7972                 controller["type"] = "lsilogic"
7973             elif isinstance(device, vim.vm.device.VirtualLsiLogicSASController):
7974                 controller["type"] = "lsilogic_sas"
7975             if objects:
7976                 controller["device"] = device.device
7977                 controller["key"] = device.key
7978                 controller["object"] = device
7979             current_config["scsi_devices"].append(controller)
7980         if isinstance(device, vim.vm.device.VirtualDisk):
7981             disk = {}
7982             disk["adapter"] = device.deviceInfo.label
7983             disk["size"] = device.capacityInKB
7984             disk["unit"] = "KB"
7985             controller = _get_device_by_key(
7986                 virtual_machine["config.hardware.device"], device.controllerKey
7987             )
7988             disk["controller"] = controller.deviceInfo.label
7989             disk["address"] = str(controller.busNumber) + ":" + str(device.unitNumber)
7990             disk["datastore"] = salt.utils.vmware.get_managed_object_name(
7991                 device.backing.datastore
7992             )
7993             disk["thin_provision"] = device.backing.thinProvisioned
7994             disk["eagerly_scrub"] = device.backing.eagerlyScrub
7995             if objects:
7996                 disk["key"] = device.key
7997                 disk["unit_number"] = device.unitNumber
7998                 disk["bus_number"] = controller.busNumber
7999                 disk["controller_key"] = device.controllerKey
8000                 disk["object"] = device
8001             current_config["disks"].append(disk)
8002         if isinstance(device, vim.vm.device.VirtualEthernetCard):
8003             interface = {}
8004             interface["adapter"] = device.deviceInfo.label
8005             interface[
8006                 "adapter_type"
8007             ] = salt.utils.vmware.get_network_adapter_object_type(device)
8008             interface["connectable"] = {
8009                 "allow_guest_control": device.connectable.allowGuestControl,
8010                 "connected": device.connectable.connected,
8011                 "start_connected": device.connectable.startConnected,
8012             }
8013             interface["mac"] = device.macAddress
8014             if isinstance(
8015                 device.backing,
8016                 vim.vm.device.VirtualEthernetCard.DistributedVirtualPortBackingInfo,
8017             ):
8018                 interface["switch_type"] = "distributed"
8019                 pg_key = device.backing.port.portgroupKey
8020                 network_ref = salt.utils.vmware.get_mor_by_property(
8021                     service_instance,
8022                     vim.DistributedVirtualPortgroup,
8023                     pg_key,
8024                     property_name="key",
8025                     container_ref=parent_ref,
8026                 )
8027             elif isinstance(
8028                 device.backing, vim.vm.device.VirtualEthernetCard.NetworkBackingInfo
8029             ):
8030                 interface["switch_type"] = "standard"
8031                 network_ref = device.backing.network
8032             interface["name"] = salt.utils.vmware.get_managed_object_name(network_ref)
8033             if objects:
8034                 interface["key"] = device.key
8035                 interface["object"] = device
8036             current_config["interfaces"].append(interface)
8037         if isinstance(device, vim.vm.device.VirtualCdrom):
8038             drive = {}
8039             drive["adapter"] = device.deviceInfo.label
8040             controller = _get_device_by_key(
8041                 virtual_machine["config.hardware.device"], device.controllerKey
8042             )
8043             drive["controller"] = controller.deviceInfo.label
8044             if isinstance(
8045                 device.backing, vim.vm.device.VirtualCdrom.RemotePassthroughBackingInfo
8046             ):
8047                 drive["device_type"] = "client_device"
8048                 drive["client_device"] = {"mode": "passthrough"}
8049             if isinstance(
8050                 device.backing, vim.vm.device.VirtualCdrom.RemoteAtapiBackingInfo
8051             ):
8052                 drive["device_type"] = "client_device"
8053                 drive["client_device"] = {"mode": "atapi"}
8054             if isinstance(device.backing, vim.vm.device.VirtualCdrom.IsoBackingInfo):
8055                 drive["device_type"] = "datastore_iso_file"
8056                 drive["datastore_iso_file"] = {"path": device.backing.fileName}
8057             drive["connectable"] = {
8058                 "allow_guest_control": device.connectable.allowGuestControl,
8059                 "connected": device.connectable.connected,
8060                 "start_connected": device.connectable.startConnected,
8061             }
8062             if objects:
8063                 drive["key"] = device.key
8064                 drive["controller_key"] = device.controllerKey
8065                 drive["object"] = device
8066             current_config["cd_drives"].append(drive)
8067         if isinstance(device, vim.vm.device.VirtualSerialPort):
8068             port = {}
8069             port["adapter"] = device.deviceInfo.label
8070             if isinstance(
8071                 device.backing, vim.vm.device.VirtualSerialPort.URIBackingInfo
8072             ):
8073                 port["type"] = "network"
8074                 port["backing"] = {
8075                     "uri": device.backing.proxyURI,
8076                     "direction": device.backing.direction,
8077                     "filename": device.backing.serviceURI,
8078                 }
8079             if isinstance(
8080                 device.backing, vim.vm.device.VirtualSerialPort.PipeBackingInfo
8081             ):
8082                 port["type"] = "pipe"
8083             if isinstance(
8084                 device.backing, vim.vm.device.VirtualSerialPort.FileBackingInfo
8085             ):
8086                 port["type"] = "file"
8087             if isinstance(
8088                 device.backing, vim.vm.device.VirtualSerialPort.DeviceBackingInfo
8089             ):
8090                 port["type"] = "device"
8091             port["yield"] = device.yieldOnPoll
8092             port["connectable"] = {
8093                 "allow_guest_control": device.connectable.allowGuestControl,
8094                 "connected": device.connectable.connected,
8095                 "start_connected": device.connectable.startConnected,
8096             }
8097             if objects:
8098                 port["key"] = device.key
8099                 port["object"] = device
8100             current_config["serial_ports"].append(port)
8101         if isinstance(device, vim.vm.device.VirtualSATAController):
8102             sata = {}
8103             sata["adapter"] = device.deviceInfo.label
8104             sata["bus_number"] = device.busNumber
8105             if objects:
8106                 sata["device"] = device.device  # keys of the connected devices
8107                 sata["key"] = device.key
8108                 sata["object"] = device
8109             current_config["sata_controllers"].append(sata)
8110     return current_config
8111 def _update_disks(disks_old_new):
8112     """
8113     Changes the disk size and returns the config spec objects in a list.
8114     The controller property cannot be updated, because controller address
8115     identifies the disk by the unit and bus number properties.
8116     disks_diffs
8117         List of old and new disk properties, the properties are dictionary
8118         objects
8119     """
8120     disk_changes = []
8121     if disks_old_new:
8122         devs = [disk["old"]["address"] for disk in disks_old_new]
8123         log.trace("Updating disks %s", devs)
8124         for item in disks_old_new:
8125             current_disk = item["old"]
8126             next_disk = item["new"]
8127             difference = recursive_diff(current_disk, next_disk)
8128             difference.ignore_unset_values = False
8129             if difference.changed():
8130                 if next_disk["size"] &lt; current_disk["size"]:
8131                     raise salt.exceptions.VMwareSaltError(
8132                         "Disk cannot be downsized size={} unit={} "
8133                         "controller_key={} "
8134                         "unit_number={}".format(
8135                             next_disk["size"],
8136                             next_disk["unit"],
8137                             current_disk["controller_key"],
8138                             current_disk["unit_number"],
8139                         )
8140                     )
8141                 log.trace(
8142                     "Virtual machine disk will be updated size=%s unit=%s "
8143                     "controller_key=%s unit_number=%s",
8144                     next_disk["size"],
8145                     next_disk["unit"],
8146                     current_disk["controller_key"],
8147                     current_disk["unit_number"],
8148                 )
8149                 device_config_spec = _apply_hard_disk(
8150                     current_disk["unit_number"],
8151                     current_disk["key"],
8152                     "edit",
8153                     size=next_disk["size"],
8154                     unit=next_disk["unit"],
8155                     controller_key=current_disk["controller_key"],
8156                 )
8157                 device_config_spec.device.backing = current_disk["object"].backing
8158                 disk_changes.append(device_config_spec)
8159     return disk_changes
8160 def _update_scsi_devices(scsis_old_new, current_disks):
8161     """
8162     Returns a list of vim.vm.device.VirtualDeviceSpec specifying  the scsi
8163     properties as input the old and new configs are defined in a dictionary.
8164     scsi_diffs
8165         List of old and new scsi properties
8166     """
8167     device_config_specs = []
8168     if scsis_old_new:
8169         devs = [scsi["old"]["adapter"] for scsi in scsis_old_new]
8170         log.trace("Updating SCSI controllers %s", devs)
8171         for item in scsis_old_new:
8172             next_scsi = item["new"]
8173             current_scsi = item["old"]
8174             difference = recursive_diff(current_scsi, next_scsi)
8175             difference.ignore_unset_values = False
8176             if difference.changed():
8177                 log.trace(
8178                     "Virtual machine scsi device will be updated key=%s "
8179                     "bus_number=%s type=%s bus_sharing=%s",
8180                     current_scsi["key"],
8181                     current_scsi["bus_number"],
8182                     next_scsi["type"],
8183                     next_scsi["bus_sharing"],
8184                 )
8185                 if next_scsi["type"] != current_scsi["type"]:
8186                     device_config_specs.append(_delete_device(current_scsi["object"]))
8187                     device_config_specs.append(
8188                         _apply_scsi_controller(
8189                             current_scsi["adapter"],
8190                             next_scsi["type"],
8191                             next_scsi["bus_sharing"],
8192                             current_scsi["key"],
8193                             current_scsi["bus_number"],
8194                             "add",
8195                         )
8196                     )
8197                     disks_to_update = []
8198                     for disk_key in current_scsi["device"]:
8199                         disk_objects = [disk["object"] for disk in current_disks]
8200                         disks_to_update.append(
8201                             _get_device_by_key(disk_objects, disk_key)
8202                         )
8203                     for current_disk in disks_to_update:
8204                         disk_spec = vim.vm.device.VirtualDeviceSpec()
8205                         disk_spec.device = current_disk
8206                         disk_spec.operation = "edit"
8207                         device_config_specs.append(disk_spec)
8208                 else:
8209                     device_config_specs.append(
8210                         _apply_scsi_controller(
8211                             current_scsi["adapter"],
8212                             current_scsi["type"],
8213                             next_scsi["bus_sharing"],
8214                             current_scsi["key"],
8215                             current_scsi["bus_number"],
8216                             "edit",
8217                         )
8218                     )
8219     return device_config_specs
8220 def _update_network_adapters(interface_old_new, parent):
8221     """
8222     Returns a list of vim.vm.device.VirtualDeviceSpec specifying
8223     configuration(s) for changed network adapters, the adapter type cannot
8224     be changed, as input the old and new configs are defined in a dictionary.
8225     interface_old_new
8226         Dictionary with old and new keys which contains the current and the
8227         next config for a network device
8228     parent
8229         Parent managed object reference
8230     """
8231     network_changes = []
8232     if interface_old_new:
8233         devs = [inter["old"]["mac"] for inter in interface_old_new]
8234         log.trace("Updating network interfaces %s", devs)
8235         for item in interface_old_new:
8236             current_interface = item["old"]
8237             next_interface = item["new"]
8238             difference = recursive_diff(current_interface, next_interface)
8239             difference.ignore_unset_values = False
8240             if difference.changed():
8241                 log.trace(
8242                     "Virtual machine network adapter will be updated "
8243                     "switch_type=%s name=%s adapter_type=%s mac=%s",
8244                     next_interface["switch_type"],
8245                     next_interface["name"],
8246                     current_interface["adapter_type"],
8247                     current_interface["mac"],
8248                 )
8249                 device_config_spec = _apply_network_adapter_config(
8250                     current_interface["key"],
8251                     next_interface["name"],
8252                     current_interface["adapter_type"],
8253                     next_interface["switch_type"],
8254                     operation="edit",
8255                     mac=current_interface["mac"],
8256                     parent=parent,
8257                 )
8258                 network_changes.append(device_config_spec)
8259     return network_changes
8260 def _update_serial_ports(serial_old_new):
8261     """
8262     Returns a list of vim.vm.device.VirtualDeviceSpec specifying to edit a
8263     deployed serial port configuration to the new given config
8264     serial_old_new
8265          Dictionary with old and new keys which contains the current and the
8266           next config for a serial port device
8267     """
8268     serial_changes = []
8269     if serial_old_new:
8270         devs = [serial["old"]["adapter"] for serial in serial_old_new]
8271         log.trace("Updating serial ports %s", devs)
8272         for item in serial_old_new:
8273             current_serial = item["old"]
8274             next_serial = item["new"]
8275             difference = recursive_diff(current_serial, next_serial)
8276             difference.ignore_unset_values = False
8277             if difference.changed():
8278                 serial_changes.append(
8279                     _apply_serial_port(next_serial, current_serial["key"], "edit")
8280                 )
8281         return serial_changes
8282 def _update_cd_drives(drives_old_new, controllers=None, parent=None):
8283     """
8284     Returns a list of vim.vm.device.VirtualDeviceSpec specifying to edit a
8285     deployed cd drive configuration to the new given config
8286     drives_old_new
8287         Dictionary with old and new keys which contains the current and the
8288         next config for a cd drive
8289     controllers
8290         Controller device list
8291     parent
8292         Managed object reference of the parent object
8293     """
8294     cd_changes = []
8295     if drives_old_new:
8296         devs = [drive["old"]["adapter"] for drive in drives_old_new]
8297         log.trace("Updating cd/dvd drives %s", devs)
8298         for item in drives_old_new:
8299             current_drive = item["old"]
8300             new_drive = item["new"]
8301             difference = recursive_diff(current_drive, new_drive)
8302             difference.ignore_unset_values = False
8303             if difference.changed():
8304                 if controllers:
8305                     controller = _get_device_by_label(
8306                         controllers, new_drive["controller"]
8307                     )
8308                     controller_key = controller.key
8309                 else:
8310                     controller_key = current_drive["controller_key"]
8311                 cd_changes.append(
8312                     _apply_cd_drive(
8313                         current_drive["adapter"],
8314                         current_drive["key"],
8315                         new_drive["device_type"],
8316                         "edit",
8317                         client_device=new_drive["client_device"]
8318                         if "client_device" in new_drive
8319                         else None,
8320                         datastore_iso_file=new_drive["datastore_iso_file"]
8321                         if "datastore_iso_file" in new_drive
8322                         else None,
8323                         connectable=new_drive["connectable"],
8324                         controller_key=controller_key,
8325                         parent_ref=parent,
8326                     )
8327                 )
8328     return cd_changes
8329 def _delete_device(device):
8330     """
8331     Returns a vim.vm.device.VirtualDeviceSpec specifying to remove a virtual
8332     machine device
8333     device
8334         Device data type object
8335     """
8336     log.trace("Deleting device with type %s", type(device))
8337     device_spec = vim.vm.device.VirtualDeviceSpec()
8338     device_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.remove
8339     device_spec.device = device
8340     return device_spec
8341 def _get_client(server, username, password, verify_ssl=None, ca_bundle=None):
8342     """
8343     Establish client through proxy or with user provided credentials.
8344     :param basestring server:
8345         Target DNS or IP of vCenter center.
8346     :param basestring username:
8347         Username associated with the vCenter center.
8348     :param basestring password:
8349         Password associated with the vCenter center.
8350     :param boolean verify_ssl:
8351         Verify the SSL certificate. Default: True
8352     :param basestring ca_bundle:
8353         Path to the ca bundle to use when verifying SSL certificates.
8354     :returns:
8355         vSphere Client instance.
8356     :rtype:
8357         vSphere.Client
8358     """
8359     details = None
8360     if not (server and username and password):
8361         details = __salt__["vcenter.get_details"]()
8362         server = details["vcenter"]
8363         username = details["username"]
8364         password = details["password"]
8365     if verify_ssl is None:
8366         if details is None:
8367             details = __salt__["vcenter.get_details"]()
8368         verify_ssl = details.get("verify_ssl", True)
8369         if verify_ssl is None:
8370             verify_ssl = True
8371     if ca_bundle is None:
8372         if details is None:
8373             details = __salt__["vcenter.get_details"]()
8374         ca_bundle = details.get("ca_bundle", None)
8375     if verify_ssl is False and ca_bundle is not None:
8376         log.error("Cannot set verify_ssl to False and ca_bundle together")
8377         return False
8378     if ca_bundle:
8379         ca_bundle = salt.utils.http.get_ca_bundle({"ca_bundle": ca_bundle})
8380     client = salt.utils.vmware.get_vsphere_client(
8381         server=server,
8382         username=username,
8383         password=password,
8384         verify_ssl=verify_ssl,
8385         ca_bundle=ca_bundle,
8386     )
8387     return client
8388 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8389 @_supports_proxies("vcenter")
8390 @_gets_service_instance_via_proxy
8391 def list_tag_categories(
8392     server=None,
8393     username=None,
8394     password=None,
8395     service_instance=None,
8396     verify_ssl=None,
8397     ca_bundle=None,
8398 ):
8399     """
8400     List existing categories a user has access to.
8401     CLI Example:
8402     .. code-block:: bash
8403             salt vm_minion vsphere.list_tag_categories
8404     :param basestring server:
8405         Target DNS or IP of vCenter center.
8406     :param basestring username:
8407         Username associated with the vCenter center.
8408     :param basestring password:
8409         Password associated with the vCenter center.
8410     :param boolean verify_ssl:
8411         Verify the SSL certificate. Default: True
8412     :param basestring ca_bundle:
8413         Path to the ca bundle to use when verifying SSL certificates.
8414     :returns:
8415         Value(s) of category_id.
8416     :rtype:
8417         list of str
8418     """
8419     categories = None
8420     client = _get_client(
8421         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8422     )
8423     if client:
8424         categories = client.tagging.Category.list()
8425     return {"Categories": categories}
8426 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8427 @_supports_proxies("vcenter")
8428 @_gets_service_instance_via_proxy
8429 def list_tags(
8430     server=None,
8431     username=None,
8432     password=None,
8433     service_instance=None,
8434     verify_ssl=None,
8435     ca_bundle=None,
8436 ):
8437     """
8438     List existing tags a user has access to.
8439     CLI Example:
8440     .. code-block:: bash
8441             salt vm_minion vsphere.list_tags
8442     :param basestring server:
8443         Target DNS or IP of vCenter center.
8444     :param basestring username:
8445         Username associated with the vCenter center.
8446     :param basestring password:
8447         Password associated with the vCenter center.
8448     :param boolean verify_ssl:
8449         Verify the SSL certificate. Default: True
8450     :param basestring ca_bundle:
8451         Path to the ca bundle to use when verifying SSL certificates.
8452     :return:
8453         Value(s) of tag_id.
8454     :rtype:
8455         list of str
8456     """
8457     tags = None
8458     client = _get_client(
8459         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8460     )
8461     if client:
8462         tags = client.tagging.Tag.list()
8463     return {"Tags": tags}
8464 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8465 @_supports_proxies("vcenter")
8466 @_gets_service_instance_via_proxy
8467 def attach_tag(
8468     object_id,
8469     tag_id,
8470     managed_obj="ClusterComputeResource",
8471     server=None,
8472     username=None,
8473     password=None,
8474     service_instance=None,
8475     verify_ssl=None,
8476     ca_bundle=None,
8477 ):
8478     """
8479     Attach an existing tag to an input object.
8480     The tag needs to meet the cardinality (`CategoryModel.cardinality`) and
8481     associability (`CategoryModel.associable_types`) criteria in order to be
8482     eligible for attachment. If the tag is already attached to the object,
8483     then this method is a no-op and an error will not be thrown. To invoke
8484     this method, you need the attach tag privilege on the tag and the read
8485     privilege on the object.
8486     CLI Example:
8487     .. code-block:: bash
8488             salt vm_minion vsphere.attach_tag domain-c2283 \
8489                 urn:vmomi:InventoryServiceTag:b55ecc77-f4a5-49f8-ab52-38865467cfbe:GLOBAL
8490     :param str object_id:
8491         The identifier of the input object.
8492     :param str tag_id:
8493         The identifier of the tag object.
8494     :param str managed_obj:
8495         Classes that contain methods for creating and deleting resources
8496         typically contain a class attribute specifying the resource type
8497         for the resources being created and deleted.
8498     :param basestring server:
8499         Target DNS or IP of vCenter center.
8500     :param basestring username:
8501         Username associated with the vCenter center.
8502     :param basestring password:
8503         Password associated with the vCenter center.
8504     :param boolean verify_ssl:
8505         Verify the SSL certificate. Default: True
8506     :param basestring ca_bundle:
8507         Path to the ca bundle to use when verifying SSL certificates.
8508     :return:
8509         The list of all tag identifiers that correspond to the
8510         tags attached to the given object.
8511     :rtype:
8512         list of tags
8513     :raise: Unauthorized
8514         if you do not have the privilege to read the object.
8515     :raise: Unauthenticated
8516         if the user can not be authenticated.
8517     """
8518     tag_attached = None
8519     client = _get_client(
8520         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8521     )
8522     if client:
8523         dynamic_id = DynamicID(type=managed_obj, id=object_id)
8524         try:
8525             tag_attached = client.tagging.TagAssociation.attach(
8526                 tag_id=tag_id, object_id=dynamic_id
8527             )
8528         except vsphere_errors:
8529             log.warning(
8530                 "Unable to attach tag. Check user privileges and"
8531                 " object_id (must be a string)."
8532             )
8533     return {"Tag attached": tag_attached}
8534 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8535 @_supports_proxies("vcenter")
8536 @_gets_service_instance_via_proxy
8537 def list_attached_tags(
8538     object_id,
8539     managed_obj="ClusterComputeResource",
8540     server=None,
8541     username=None,
8542     password=None,
8543     service_instance=None,
8544     verify_ssl=None,
8545     ca_bundle=None,
8546 ):
8547     """
8548     List existing tags a user has access to.
8549     CLI Example:
8550     .. code-block:: bash
8551             salt vm_minion vsphere.list_attached_tags domain-c2283
8552     :param str object_id:
8553         The identifier of the input object.
8554     :param str managed_obj:
8555         Classes that contain methods for creating and deleting resources
8556         typically contain a class attribute specifying the resource type
8557         for the resources being created and deleted.
8558     :param basestring server:
8559         Target DNS or IP of vCenter center.
8560     :param basestring username:
8561         Username associated with the vCenter center.
8562     :param basestring password:
8563         Password associated with the vCenter center.
8564     :param boolean verify_ssl:
8565         Verify the SSL certificate. Default: True
8566     :param basestring ca_bundle:
8567         Path to the ca bundle to use when verifying SSL certificates.
8568     :return:
8569         The list of all tag identifiers that correspond to the
8570         tags attached to the given object.
8571     :rtype:
8572         list of tags
8573     :raise: Unauthorized
8574         if you do not have the privilege to read the object.
8575     :raise: Unauthenticated
8576         if the user can not be authenticated.
8577     """
8578     attached_tags = None
8579     client = _get_client(
8580         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8581     )
8582     if client:
8583         dynamic_id = DynamicID(type=managed_obj, id=object_id)
8584         try:
8585             attached_tags = client.tagging.TagAssociation.list_attached_tags(dynamic_id)
8586         except vsphere_errors:
8587             log.warning(
8588                 "Unable to list attached tags. Check user privileges"
8589                 " and object_id (must be a string)."
8590             )
8591     return {"Attached tags": attached_tags}
8592 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8593 @_supports_proxies("vcenter")
8594 @_gets_service_instance_via_proxy
8595 def create_tag_category(
8596     name,
8597     description,
8598     cardinality,
8599     server=None,
8600     username=None,
8601     password=None,
8602     service_instance=None,
8603     verify_ssl=None,
8604     ca_bundle=None,
8605 ):
8606     """
8607     Create a category with given cardinality.
8608     CLI Example:
8609     .. code-block:: bash
8610             salt vm_minion vsphere.create_tag_category
8611     :param str name:
8612         Name of tag category to create (ex. Machine, OS, Availability, etc.)
8613     :param str description:
8614         Given description of tag category.
8615     :param str cardinality:
8616         The associated cardinality (SINGLE, MULTIPLE) of the category.
8617     :param basestring server:
8618         Target DNS or IP of vCenter center.
8619     :param basestring username:
8620         Username associated with the vCenter center.
8621     :param basestring password:
8622         Password associated with the vCenter center.
8623     :param boolean verify_ssl:
8624         Verify the SSL certificate. Default: True
8625     :param basestring ca_bundle:
8626         Path to the ca bundle to use when verifying SSL certificates.
8627     :return:
8628         Identifier of the created category.
8629     :rtype:
8630         str
8631     :raise: AlreadyExists
8632         if the name` provided in the create_spec is the name of an already
8633         existing category.
8634     :raise: InvalidArgument
8635         if any of the information in the create_spec is invalid.
8636     :raise: Unauthorized
8637         if you do not have the privilege to create a category.
8638     """
8639     category_created = None
8640     client = _get_client(
8641         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8642     )
8643     if client:
8644         if cardinality == "SINGLE":
8645             cardinality = CategoryModel.Cardinality.SINGLE
8646         elif cardinality == "MULTIPLE":
8647             cardinality = CategoryModel.Cardinality.MULTIPLE
8648         else:
8649             cardinality = None
8650         create_spec = client.tagging.Category.CreateSpec()
8651         create_spec.name = name
8652         create_spec.description = description
8653         create_spec.cardinality = cardinality
8654         associable_types = set()
8655         create_spec.associable_types = associable_types
8656         try:
8657             category_created = client.tagging.Category.create(create_spec)
8658         except vsphere_errors:
8659             log.warning(
8660                 "Unable to create tag category. Check user privilege"
8661                 " and see if category exists."
8662             )
8663     return {"Category created": category_created}
8664 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8665 @_supports_proxies("vcenter")
8666 @_gets_service_instance_via_proxy
8667 def delete_tag_category(
8668     category_id,
8669     server=None,
8670     username=None,
8671     password=None,
8672     service_instance=None,
8673     verify_ssl=None,
8674     ca_bundle=None,
8675 ):
8676     """
8677     Delete a category.
8678     CLI Example:
8679     .. code-block:: bash
8680             salt vm_minion vsphere.delete_tag_category
8681     :param str category_id:
8682         The identifier of category to be deleted.
8683         The parameter must be an identifier for the resource type:
8684         ``com.vmware.cis.tagging.Category``.
8685     :param basestring server:
8686         Target DNS or IP of vCenter center.
8687     :param basestring username:
8688         Username associated with the vCenter center.
8689     :param basestring password:
8690         Password associated with the vCenter center.
8691     :param boolean verify_ssl:
8692         Verify the SSL certificate. Default: True
8693     :param basestring ca_bundle:
8694         Path to the ca bundle to use when verifying SSL certificates.
8695     :raise: NotFound
8696         if the tag for the given tag_id does not exist in the system.
8697     :raise: Unauthorized
8698         if you do not have the privilege to delete the tag.
8699     :raise: Unauthenticated
8700         if the user can not be authenticated.
8701     """
8702     category_deleted = None
8703     client = _get_client(
8704         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8705     )
8706     if client:
8707         try:
8708             category_deleted = client.tagging.Category.delete(category_id)
8709         except vsphere_errors:
8710             log.warning(
8711                 "Unable to delete tag category. Check user privilege"
8712                 " and see if category exists."
8713             )
8714     return {"Category deleted": category_deleted}
8715 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8716 @_supports_proxies("vcenter")
8717 @_gets_service_instance_via_proxy
8718 def create_tag(
8719     name,
8720     description,
8721     category_id,
8722     server=None,
8723     username=None,
8724     password=None,
8725     service_instance=None,
8726     verify_ssl=None,
8727     ca_bundle=None,
8728 ):
8729     """
8730     Create a tag under a category with given description.
8731     CLI Example:
8732     .. code-block:: bash
8733             salt vm_minion vsphere.create_tag
8734     :param basestring server:
8735         Target DNS or IP of vCenter client.
8736     :param basestring username:
8737          Username associated with the vCenter client.
8738     :param basestring password:
8739         Password associated with the vCenter client.
8740     :param str name:
8741         Name of tag category to create (ex. Machine, OS, Availability, etc.)
8742     :param str description:
8743         Given description of tag category.
8744     :param str category_id:
8745         Value of category_id representative of the category created previously.
8746     :param boolean verify_ssl:
8747         Verify the SSL certificate. Default: True
8748     :param basestring ca_bundle:
8749         Path to the ca bundle to use when verifying SSL certificates.
8750     :return:
8751         The identifier of the created tag.
8752     :rtype:
8753         str
8754     :raise: AlreadyExists
8755         if the name provided in the create_spec is the name of an already
8756         existing tag in the input category.
8757     :raise: InvalidArgument
8758         if any of the input information in the create_spec is invalid.
8759     :raise: NotFound
8760         if the category for in the given create_spec does not exist in
8761         the system.
8762     :raise: Unauthorized
8763         if you do not have the privilege to create tag.
8764     """
8765     tag_created = None
8766     client = _get_client(
8767         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8768     )
8769     if client:
8770         create_spec = client.tagging.Tag.CreateSpec()
8771         create_spec.name = name
8772         create_spec.description = description
8773         create_spec.category_id = category_id
8774         try:
8775             tag_created = client.tagging.Tag.create(create_spec)
8776         except vsphere_errors:
8777             log.warning(
8778                 "Unable to create tag. Check user privilege and see if category exists."
8779             )
8780     return {"Tag created": tag_created}
8781 @depends(HAS_PYVMOMI, HAS_VSPHERE_SDK)
8782 @_supports_proxies("vcenter")
8783 @_gets_service_instance_via_proxy
8784 def delete_tag(
8785     tag_id,
8786     server=None,
8787     username=None,
8788     password=None,
8789     service_instance=None,
8790     verify_ssl=None,
8791     ca_bundle=None,
8792 ):
8793     """
8794     Delete a tag.
8795     CLI Example:
8796     .. code-block:: bash
8797             salt vm_minion vsphere.delete_tag
8798     :param str tag_id:
8799         The identifier of tag to be deleted.
8800         The parameter must be an identifier for the resource type:
8801         ``com.vmware.cis.tagging.Tag``.
8802     :param basestring server:
8803         Target DNS or IP of vCenter center.
8804     :param basestring username:
8805         Username associated with the vCenter center.
8806     :param basestring password:
8807         Password associated with the vCenter center.
8808     :param boolean verify_ssl:
8809         Verify the SSL certificate. Default: True
8810     :param basestring ca_bundle:
8811         Path to the ca bundle to use when verifying SSL certificates.
8812     :raise: AlreadyExists
8813         if the name provided in the create_spec is the name of an already
8814         existing category.
8815     :raise: InvalidArgument
8816         if any of the information in the create_spec is invalid.
8817     :raise: Unauthorized
8818         if you do not have the privilege to create a category.
8819     """
8820     tag_deleted = None
8821     client = _get_client(
8822         server, username, password, verify_ssl=verify_ssl, ca_bundle=ca_bundle
8823     )
8824     if client:
8825         try:
8826             tag_deleted = client.tagging.Tag.delete(tag_id)
8827         except vsphere_errors:
8828             log.warning(
8829                 "Unable to delete category. Check user privileges"
8830                 " and that category exists."
8831             )
8832     return {"Tag deleted": tag_deleted}
8833 @depends(HAS_PYVMOMI)
8834 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
8835 @_gets_service_instance_via_proxy
8836 def create_vm(
8837     vm_name,
8838     cpu,
8839     memory,
8840     image,
8841     version,
8842     datacenter,
8843     datastore,
8844     placement,
8845     interfaces,
8846     disks,
8847     scsi_devices,
8848     serial_ports=None,
8849     ide_controllers=None,
8850     sata_controllers=None,
8851     cd_drives=None,
8852     advanced_configs=None,
8853     service_instance=None,
8854 ):
8855     """
8856     Creates a virtual machine container.
8857     CLI Example:
8858     .. code-block:: bash
8859         salt vm_minion vsphere.create_vm vm_name=vmname cpu='{count: 2, nested: True}' ...
8860     vm_name
8861         Name of the virtual machine
8862     cpu
8863         Properties of CPUs for freshly created machines
8864     memory
8865         Memory size for freshly created machines
8866     image
8867         Virtual machine guest OS version identifier
8868         VirtualMachineGuestOsIdentifier
8869     version
8870         Virtual machine container hardware version
8871     datacenter
8872         Datacenter where the virtual machine will be deployed (mandatory)
8873     datastore
8874         Datastore where the virtual machine files will be placed
8875     placement
8876         Resource pool or cluster or host or folder where the virtual machine
8877         will be deployed
8878     devices
8879         interfaces
8880         .. code-block:: bash
8881             interfaces:
8882               adapter: 'Network adapter 1'
8883               name: vlan100
8884               switch_type: distributed or standard
8885               adapter_type: vmxnet3 or vmxnet, vmxnet2, vmxnet3, e1000, e1000e
8886               mac: '00:11:22:33:44:55'
8887               connectable:
8888                 allow_guest_control: True
8889                 connected: True
8890                 start_connected: True
8891         disks
8892         .. code-block:: bash
8893             disks:
8894               adapter: 'Hard disk 1'
8895               size: 16
8896               unit: GB
8897               address: '0:0'
8898               controller: 'SCSI controller 0'
8899               thin_provision: False
8900               eagerly_scrub: False
8901               datastore: 'myshare'
8902               filename: 'vm/mydisk.vmdk'
8903         scsi_devices
8904         .. code-block:: bash
8905             scsi_devices:
8906               controller: 'SCSI controller 0'
8907               type: paravirtual
8908               bus_sharing: no_sharing
8909         serial_ports
8910         .. code-block:: bash
8911             serial_ports:
8912               adapter: 'Serial port 1'
8913               type: network
8914               backing:
8915                 uri: 'telnet://something:port'
8916                 direction: &lt;client|server&gt;
8917                 filename: 'service_uri'
8918               connectable:
8919                 allow_guest_control: True
8920                 connected: True
8921                 start_connected: True
8922               yield: False
8923         cd_drives
8924         .. code-block:: bash
8925             cd_drives:
8926               adapter: 'CD/DVD drive 0'
8927               controller: 'IDE 0'
8928               device_type: datastore_iso_file
8929               datastore_iso_file:
8930                 path: path_to_iso
8931               connectable:
8932                 allow_guest_control: True
8933                 connected: True
8934                 start_connected: True
8935     advanced_config
8936         Advanced config parameters to be set for the virtual machine
8937     """
8938     container_object = salt.utils.vmware.get_datacenter(service_instance, datacenter)
8939     (resourcepool_object, placement_object) = salt.utils.vmware.get_placement(
8940         service_instance, datacenter, placement=placement
8941     )
8942     folder_object = salt.utils.vmware.get_folder(
8943         service_instance, datacenter, placement
8944     )
8945     config_spec = vim.vm.ConfigSpec()
8946     config_spec.name = vm_name
8947     config_spec.guestId = image
8948     config_spec.files = vim.vm.FileInfo()
8949     datastore_object = salt.utils.vmware.get_datastores(
8950         service_instance, placement_object, datastore_names=[datastore]
8951     )[0]
8952     if not datastore_object:
8953         raise salt.exceptions.ArgumentValueError(
8954             "Specified datastore: '{}' does not exist.".format(datastore)
8955         )
8956     try:
8957         ds_summary = salt.utils.vmware.get_properties_of_managed_object(
8958             datastore_object, "summary.type"
8959         )
8960         if "summary.type" in ds_summary and ds_summary["summary.type"] == "vsan":
8961             log.trace(
8962                 "The vmPathName should be the datastore "
8963                 "name if the datastore type is vsan"
8964             )
8965             config_spec.files.vmPathName = "[{}]".format(datastore)
8966         else:
8967             config_spec.files.vmPathName = "[{0}] {1}/{1}.vmx".format(
8968                 datastore, vm_name
8969             )
8970     except salt.exceptions.VMwareApiError:
8971         config_spec.files.vmPathName = "[{0}] {1}/{1}.vmx".format(datastore, vm_name)
8972     cd_controllers = []
8973     if version:
8974         _apply_hardware_version(version, config_spec, "add")
8975     if cpu:
8976         _apply_cpu_config(config_spec, cpu)
8977     if memory:
8978         _apply_memory_config(config_spec, memory)
8979     if scsi_devices:
8980         scsi_specs = _create_scsi_devices(scsi_devices)
8981         config_spec.deviceChange.extend(scsi_specs)
8982     if disks:
8983         scsi_controllers = [spec.device for spec in scsi_specs]
8984         disk_specs = _create_disks(
8985             service_instance,
8986             disks,
8987             scsi_controllers=scsi_controllers,
8988             parent=container_object,
8989         )
8990         config_spec.deviceChange.extend(disk_specs)
8991     if interfaces:
8992         (interface_specs, nic_settings) = _create_network_adapters(
8993             interfaces, parent=container_object
8994         )
8995         config_spec.deviceChange.extend(interface_specs)
8996     if serial_ports:
8997         serial_port_specs = _create_serial_ports(serial_ports)
8998         config_spec.deviceChange.extend(serial_port_specs)
8999     if ide_controllers:
9000         ide_specs = _create_ide_controllers(ide_controllers)
9001         config_spec.deviceChange.extend(ide_specs)
9002         cd_controllers.extend(ide_specs)
9003     if sata_controllers:
9004         sata_specs = _create_sata_controllers(sata_controllers)
9005         config_spec.deviceChange.extend(sata_specs)
9006         cd_controllers.extend(sata_specs)
9007     if cd_drives:
9008         cd_drive_specs = _create_cd_drives(
9009             cd_drives, controllers=cd_controllers, parent_ref=container_object
9010         )
9011         config_spec.deviceChange.extend(cd_drive_specs)
9012     if advanced_configs:
9013         _apply_advanced_config(config_spec, advanced_configs)
9014     salt.utils.vmware.create_vm(
9015         vm_name, config_spec, folder_object, resourcepool_object, placement_object
9016     )
9017     return {"create_vm": True}
9018 @depends(HAS_PYVMOMI)
9019 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
9020 @_gets_service_instance_via_proxy
9021 def update_vm(
9022     vm_name,
9023     cpu=None,
9024     memory=None,
9025     image=None,
9026     version=None,
9027     interfaces=None,
9028     disks=None,
9029     scsi_devices=None,
9030     serial_ports=None,
9031     datacenter=None,
9032     datastore=None,
9033     cd_dvd_drives=None,
9034     sata_controllers=None,
9035     advanced_configs=None,
9036     service_instance=None,
9037 ):
9038     """
9039     Updates the configuration of the virtual machine if the config differs
9040     vm_name
9041         Virtual Machine name to be updated
9042     cpu
9043         CPU configuration options
9044     memory
9045         Memory configuration options
9046     version
9047         Virtual machine container hardware version
9048     image
9049         Virtual machine guest OS version identifier
9050         VirtualMachineGuestOsIdentifier
9051     interfaces
9052         Network interfaces configuration options
9053     disks
9054         Disks configuration options
9055     scsi_devices
9056         SCSI devices configuration options
9057     serial_ports
9058         Serial ports configuration options
9059     datacenter
9060         Datacenter where the virtual machine is available
9061     datastore
9062         Datastore where the virtual machine config files are available
9063     cd_dvd_drives
9064         CD/DVD drives configuration options
9065     advanced_config
9066         Advanced config parameters to be set for the virtual machine
9067     service_instance
9068         vCenter service instance for connection and configuration
9069     """
9070     current_config = get_vm_config(
9071         vm_name, datacenter=datacenter, objects=True, service_instance=service_instance
9072     )
9073     diffs = compare_vm_configs(
9074         {
9075             "name": vm_name,
9076             "cpu": cpu,
9077             "memory": memory,
9078             "image": image,
9079             "version": version,
9080             "interfaces": interfaces,
9081             "disks": disks,
9082             "scsi_devices": scsi_devices,
9083             "serial_ports": serial_ports,
9084             "datacenter": datacenter,
9085             "datastore": datastore,
9086             "cd_drives": cd_dvd_drives,
9087             "sata_controllers": sata_controllers,
9088             "advanced_configs": advanced_configs,
9089         },
9090         current_config,
9091     )
9092     config_spec = vim.vm.ConfigSpec()
9093     datacenter_ref = salt.utils.vmware.get_datacenter(service_instance, datacenter)
9094     vm_ref = salt.utils.vmware.get_mor_by_property(
9095         service_instance,
9096         vim.VirtualMachine,
9097         vm_name,
9098         property_name="name",
9099         container_ref=datacenter_ref,
9100     )
9101     difference_keys = diffs.keys()
9102     if "cpu" in difference_keys:
9103         if diffs["cpu"].changed() != set():
9104             _apply_cpu_config(config_spec, diffs["cpu"].current_dict)
9105     if "memory" in difference_keys:
9106         if diffs["memory"].changed() != set():
9107             _apply_memory_config(config_spec, diffs["memory"].current_dict)
9108     if "advanced_configs" in difference_keys:
9109         _apply_advanced_config(
9110             config_spec, diffs["advanced_configs"].new_values, vm_ref.config.extraConfig
9111         )
9112     if "version" in difference_keys:
9113         _apply_hardware_version(version, config_spec, "edit")
9114     if "image" in difference_keys:
9115         config_spec.guestId = image
9116     new_scsi_devices = []
9117     if "scsi_devices" in difference_keys and "disks" in current_config:
9118         scsi_changes = []
9119         scsi_changes.extend(
9120             _update_scsi_devices(
9121                 diffs["scsi_devices"].intersect, current_config["disks"]
9122             )
9123         )
9124         for item in diffs["scsi_devices"].removed:
9125             scsi_changes.append(_delete_device(item["object"]))
9126         new_scsi_devices = _create_scsi_devices(diffs["scsi_devices"].added)
9127         scsi_changes.extend(new_scsi_devices)
9128         config_spec.deviceChange.extend(scsi_changes)
9129     if "disks" in difference_keys:
9130         disk_changes = []
9131         disk_changes.extend(_update_disks(diffs["disks"].intersect))
9132         for item in diffs["disks"].removed:
9133             disk_changes.append(_delete_device(item["object"]))
9134         scsi_controllers = [dev["object"] for dev in current_config["scsi_devices"]]
9135         scsi_controllers.extend(
9136             [device_spec.device for device_spec in new_scsi_devices]
9137         )
9138         disk_changes.extend(
9139             _create_disks(
9140                 service_instance,
9141                 diffs["disks"].added,
9142                 scsi_controllers=scsi_controllers,
9143                 parent=datacenter_ref,
9144             )
9145         )
9146         config_spec.deviceChange.extend(disk_changes)
9147     if "interfaces" in difference_keys:
9148         network_changes = []
9149         network_changes.extend(
9150             _update_network_adapters(diffs["interfaces"].intersect, datacenter_ref)
9151         )
9152         for item in diffs["interfaces"].removed:
9153             network_changes.append(_delete_device(item["object"]))
9154         (adapters, nics) = _create_network_adapters(
9155             diffs["interfaces"].added, datacenter_ref
9156         )
9157         network_changes.extend(adapters)
9158         config_spec.deviceChange.extend(network_changes)
9159     if "serial_ports" in difference_keys:
9160         serial_changes = []
9161         serial_changes.extend(_update_serial_ports(diffs["serial_ports"].intersect))
9162         for item in diffs["serial_ports"].removed:
9163             serial_changes.append(_delete_device(item["object"]))
9164         serial_changes.extend(_create_serial_ports(diffs["serial_ports"].added))
9165         config_spec.deviceChange.extend(serial_changes)
9166     new_controllers = []
9167     if "sata_controllers" in difference_keys:
9168         sata_specs = _create_sata_controllers(diffs["sata_controllers"].added)
9169         for item in diffs["sata_controllers"].removed:
9170             sata_specs.append(_delete_device(item["object"]))
9171         new_controllers.extend(sata_specs)
9172         config_spec.deviceChange.extend(sata_specs)
9173     if "cd_drives" in difference_keys:
9174         cd_changes = []
9175         controllers = [dev["object"] for dev in current_config["sata_controllers"]]
9176         controllers.extend([device_spec.device for device_spec in new_controllers])
9177         cd_changes.extend(
9178             _update_cd_drives(
9179                 diffs["cd_drives"].intersect,
9180                 controllers=controllers,
9181                 parent=datacenter_ref,
9182             )
9183         )
9184         for item in diffs["cd_drives"].removed:
9185             cd_changes.append(_delete_device(item["object"]))
9186         cd_changes.extend(
9187             _create_cd_drives(
9188                 diffs["cd_drives"].added,
9189                 controllers=controllers,
9190                 parent_ref=datacenter_ref,
9191             )
9192         )
9193         config_spec.deviceChange.extend(cd_changes)
9194     if difference_keys:
9195         salt.utils.vmware.update_vm(vm_ref, config_spec)
9196     changes = {}
9197     for key, properties in diffs.items():
9198         if isinstance(properties, salt.utils.listdiffer.ListDictDiffer):
9199             properties.remove_diff(diff_key="object", diff_list="intersect")
9200             properties.remove_diff(diff_key="key", diff_list="intersect")
9201             properties.remove_diff(diff_key="object", diff_list="removed")
9202             properties.remove_diff(diff_key="key", diff_list="removed")
9203         changes[key] = properties.diffs
9204     return changes
9205 @depends(HAS_PYVMOMI)
9206 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
9207 @_gets_service_instance_via_proxy
9208 def register_vm(name, datacenter, placement, vmx_path, service_instance=None):
9209     """
9210     Registers a virtual machine to the inventory with the given vmx file.
9211     Returns comments and change list
9212     name
9213         Name of the virtual machine
9214     datacenter
9215         Datacenter of the virtual machine
9216     placement
9217         Placement dictionary of the virtual machine, host or cluster
9218     vmx_path:
9219         Full path to the vmx file, datastore name should be included
9220     service_instance
9221         Service instance (vim.ServiceInstance) of the vCenter.
9222         Default is None.
9223     """
9224     log.trace(
9225         "Registering virtual machine with properties datacenter=%s, "
9226         "placement=%s, vmx_path=%s",
9227         datacenter,
9228         placement,
9229         vmx_path,
9230     )
9231     datacenter_object = salt.utils.vmware.get_datacenter(service_instance, datacenter)
9232     if "cluster" in placement:
9233         cluster_obj = salt.utils.vmware.get_cluster(
9234             datacenter_object, placement["cluster"]
9235         )
9236         cluster_props = salt.utils.vmware.get_properties_of_managed_object(
9237             cluster_obj, properties=["resourcePool"]
9238         )
9239         if "resourcePool" in cluster_props:
9240             resourcepool = cluster_props["resourcePool"]
9241         else:
9242             raise salt.exceptions.VMwareObjectRetrievalError(
9243                 "The cluster's resource pool object could not be retrieved."
9244             )
9245         salt.utils.vmware.register_vm(datacenter_object, name, vmx_path, resourcepool)
9246     elif "host" in placement:
9247         hosts = salt.utils.vmware.get_hosts(
9248             service_instance, datacenter_name=datacenter, host_names=[placement["host"]]
9249         )
9250         if not hosts:
9251             raise salt.exceptions.VMwareObjectRetrievalError(
9252                 "ESXi host named '{}' wasn't found.".format(placement["host"])
9253             )
9254         host_obj = hosts[0]
9255         host_props = salt.utils.vmware.get_properties_of_managed_object(
9256             host_obj, properties=["parent"]
9257         )
9258         if "parent" in host_props:
9259             host_parent = host_props["parent"]
9260             parent = salt.utils.vmware.get_properties_of_managed_object(
9261                 host_parent, properties=["parent"]
9262             )
9263             if "parent" in parent:
9264                 resourcepool = parent["parent"]
9265             else:
9266                 raise salt.exceptions.VMwareObjectRetrievalError(
9267                     "The host parent's parent object could not be retrieved."
9268                 )
9269         else:
9270             raise salt.exceptions.VMwareObjectRetrievalError(
9271                 "The host's parent object could not be retrieved."
9272             )
9273         salt.utils.vmware.register_vm(
9274             datacenter_object, name, vmx_path, resourcepool, host_object=host_obj
9275         )
9276     result = {
9277         "comment": "Virtual machine registration action succeeded",
9278         "changes": {"register_vm": True},
9279     }
9280     return result
9281 @depends(HAS_PYVMOMI)
9282 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
9283 @_gets_service_instance_via_proxy
9284 def power_on_vm(name, datacenter=None, service_instance=None):
9285     """
9286     Powers on a virtual machine specified by its name.
9287     name
9288         Name of the virtual machine
9289     datacenter
9290         Datacenter of the virtual machine
9291     service_instance
9292         Service instance (vim.ServiceInstance) of the vCenter.
9293         Default is None.
9294     .. code-block:: bash
9295         salt '*' vsphere.power_on_vm name=my_vm
9296     """
9297     log.trace("Powering on virtual machine %s", name)
9298     vm_properties = ["name", "summary.runtime.powerState"]
9299     virtual_machine = salt.utils.vmware.get_vm_by_property(
9300         service_instance, name, datacenter=datacenter, vm_properties=vm_properties
9301     )
9302     if virtual_machine["summary.runtime.powerState"] == "poweredOn":
9303         result = {
9304             "comment": "Virtual machine is already powered on",
9305             "changes": {"power_on": True},
9306         }
9307         return result
9308     salt.utils.vmware.power_cycle_vm(virtual_machine["object"], action="on")
9309     result = {
9310         "comment": "Virtual machine power on action succeeded",
9311         "changes": {"power_on": True},
9312     }
9313     return result
9314 @depends(HAS_PYVMOMI)
9315 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
9316 @_gets_service_instance_via_proxy
9317 def power_off_vm(name, datacenter=None, service_instance=None):
9318     """
9319     Powers off a virtual machine specified by its name.
9320     name
9321         Name of the virtual machine
9322     datacenter
9323         Datacenter of the virtual machine
9324     service_instance
9325         Service instance (vim.ServiceInstance) of the vCenter.
9326         Default is None.
9327     .. code-block:: bash
9328         salt '*' vsphere.power_off_vm name=my_vm
9329     """
9330     log.trace("Powering off virtual machine %s", name)
9331     vm_properties = ["name", "summary.runtime.powerState"]
9332     virtual_machine = salt.utils.vmware.get_vm_by_property(
9333         service_instance, name, datacenter=datacenter, vm_properties=vm_properties
9334     )
9335     if virtual_machine["summary.runtime.powerState"] == "poweredOff":
9336         result = {
9337             "comment": "Virtual machine is already powered off",
9338             "changes": {"power_off": True},
9339         }
9340         return result
9341     salt.utils.vmware.power_cycle_vm(virtual_machine["object"], action="off")
9342     result = {
9343         "comment": "Virtual machine power off action succeeded",
9344         "changes": {"power_off": True},
9345     }
9346     return result
9347 def _remove_vm(name, datacenter, service_instance, placement=None, power_off=None):
9348     """
9349     Helper function to remove a virtual machine
9350     name
9351         Name of the virtual machine
9352     service_instance
9353         vCenter service instance for connection and configuration
9354     datacenter
9355         Datacenter of the virtual machine
9356     placement
9357         Placement information of the virtual machine
9358     """
9359     results = {}
9360     if placement:
9361         (resourcepool_object, placement_object) = salt.utils.vmware.get_placement(
9362             service_instance, datacenter, placement
9363         )
9364     else:
9365         placement_object = salt.utils.vmware.get_datacenter(
9366             service_instance, datacenter
9367         )
9368     if power_off:
9369         power_off_vm(name, datacenter, service_instance)
9370         results["powered_off"] = True
9371     vm_ref = salt.utils.vmware.get_mor_by_property(
9372         service_instance,
9373         vim.VirtualMachine,
9374         name,
9375         property_name="name",
9376         container_ref=placement_object,
9377     )
9378     if not vm_ref:
9379         raise salt.exceptions.VMwareObjectRetrievalError(
9380             "The virtual machine object {} in datacenter {} was not found".format(
9381                 name, datacenter
9382             )
9383         )
9384     return results, vm_ref
9385 @depends(HAS_PYVMOMI)
9386 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
9387 @_gets_service_instance_via_proxy
9388 def delete_vm(name, datacenter, placement=None, power_off=False, service_instance=None):
9389     """
9390     Deletes a virtual machine defined by name and placement
9391     name
9392         Name of the virtual machine
9393     datacenter
9394         Datacenter of the virtual machine
9395     placement
9396         Placement information of the virtual machine
9397     service_instance
9398         vCenter service instance for connection and configuration
9399     .. code-block:: bash
9400         salt '*' vsphere.delete_vm name=my_vm datacenter=my_datacenter
9401     """
9402     results = {}
9403     schema = ESXVirtualMachineDeleteSchema.serialize()
9404     try:
9405         jsonschema.validate(
9406             {"name": name, "datacenter": datacenter, "placement": placement}, schema
9407         )
9408     except jsonschema.exceptions.ValidationError as exc:
9409         raise InvalidConfigError(exc)
9410     (results, vm_ref) = _remove_vm(
9411         name,
9412         datacenter,
9413         service_instance=service_instance,
9414         placement=placement,
9415         power_off=power_off,
9416     )
9417     salt.utils.vmware.delete_vm(vm_ref)
9418     results["deleted_vm"] = True
9419     return results
9420 @depends(HAS_PYVMOMI)
9421 @_supports_proxies("esxvm", "esxcluster", "esxdatacenter")
9422 @_gets_service_instance_via_proxy
9423 def unregister_vm(
9424     name, datacenter, placement=None, power_off=False, service_instance=None
9425 ):
9426     """
9427     Unregisters a virtual machine defined by name and placement
9428     name
9429         Name of the virtual machine
9430     datacenter
9431         Datacenter of the virtual machine
9432     placement
9433         Placement information of the virtual machine
9434     service_instance
9435         vCenter service instance for connection and configuration
9436     .. code-block:: bash
9437         salt '*' vsphere.unregister_vm name=my_vm datacenter=my_datacenter
9438     """
9439     results = {}
9440     schema = ESXVirtualMachineUnregisterSchema.serialize()
9441     try:
9442         jsonschema.validate(
9443             {"name": name, "datacenter": datacenter, "placement": placement}, schema
9444         )
9445     except jsonschema.exceptions.ValidationError as exc:
9446         raise InvalidConfigError(exc)
9447     (results, vm_ref) = _remove_vm(
9448         name,
9449         datacenter,
9450         service_instance=service_instance,
9451         placement=placement,
9452         power_off=power_off,
9453     )
9454     salt.utils.vmware.unregister_vm(vm_ref)
9455     results["unregistered_vm"] = True
9456     return results
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
