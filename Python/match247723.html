<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for test_nxos_2.py &amp; esxi_3.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for test_nxos_2.py &amp; esxi_3.py
      </h3>
<h1 align="center">
        1.1%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>test_nxos_2.py (2.0378456%)<th>esxi_3.py (0.77691454%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(165-172)<td><a href="#" name="0">(1585-1598)</a><td align="center"><font color="#ff0000">14</font>
</td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_nxos_2.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 import salt.proxy.nxos as nxos_proxy
2 import salt.utils.nxos as nxos_utils
3 from salt.exceptions import CommandExecutionError
4 from tests.support.mixins import LoaderModuleMockMixin
5 from tests.support.mock import MagicMock, create_autospec, patch
6 from tests.support.unit import TestCase
7 from tests.unit.modules.nxos.nxos_grains import n9k_grains
8 from tests.unit.modules.nxos.nxos_show_cmd_output import n9k_show_ver_list
9 class NxosNxapiProxyTestCase(TestCase, LoaderModuleMockMixin):
10     def setup_loader_modules(self):
11         return {nxos_proxy: {"CONNECTION": "nxapi"}}
12     def test_check_virtual(self):
13         result = nxos_proxy.__virtual__()
14         self.assertIn("nxos", result)
15     def test_init(self):
16         with patch.object(nxos_proxy, "__opts__", {"proxy": {"connection": "nxapi"}}):
17             with patch("salt.proxy.nxos._init_nxapi", autospec=True) as init_nxapi:
18                 result = nxos_proxy.init()
19                 self.assertEqual(result, init_nxapi.return_value)
20     def test_init_opts_none(self):
21         with patch.object(nxos_proxy, "__opts__", {"proxy": {"connection": None}}):
22             with patch("salt.proxy.nxos._init_nxapi", autospec=True) as init_nxapi:
23                 result = nxos_proxy.init()
24                 self.assertEqual(result, init_nxapi.return_value)
25     def test_init_bad_connection_type(self):
26         with patch.object(nxos_proxy, "__opts__", {"proxy": {"connection": "unknown"}}):
27             self.assertFalse(nxos_proxy.init())
28     def test_initialized(self):
29         with patch(
30             "salt.proxy.nxos._initialized_nxapi", autospec=True
31         ) as initialized_nxapi:
32             result = nxos_proxy.initialized()
33             self.assertEqual(result, initialized_nxapi.return_value)
34     def test_ping(self):
35         with patch("salt.proxy.nxos._ping_nxapi", autospec=True) as ping_nxapi:
36             result = nxos_proxy.ping()
37             self.assertEqual(result, ping_nxapi.return_value)
38     def test_grains(self):
39         with patch(
40             "salt.proxy.nxos.sendline", autospec=True, return_value=n9k_show_ver_list
41         ):
42             result = nxos_proxy.grains()
43             self.assertEqual(result, n9k_grains)
44     def test_grains_cache_set(self):
45         with patch(
46             "salt.proxy.nxos.DEVICE_DETAILS", {"grains_cache": n9k_grains["nxos"]}
47         ):
48             with patch(
49                 "salt.proxy.nxos.sendline",
50                 autospec=True,
51                 return_value=n9k_show_ver_list,
52             ):
53                 result = nxos_proxy.grains()
54                 self.assertEqual(result, n9k_grains)
55     def test_grains_refresh(self):
56         device_details = {"grains_cache": None}
57         with patch("salt.proxy.nxos.DEVICE_DETAILS", device_details):
58             with patch("salt.proxy.nxos.grains", autospec=True) as grains:
59                 result = nxos_proxy.grains_refresh()
60                 self.assertEqual(nxos_proxy.DEVICE_DETAILS["grains_cache"], {})
61                 self.assertEqual(result, grains.return_value)
62     def test_sendline(self):
63         command = "show version"
64         with patch("salt.proxy.nxos._nxapi_request", autospec=True) as nxapi_request:
65             result = nxos_proxy.sendline(command)
66             self.assertEqual(result, nxapi_request.return_value)
67     def test_proxy_config(self):
68         commands = ["feature bgp", "router bgp 65535"]
69         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"save_config": False}):
70             with patch(
71                 "salt.proxy.nxos._nxapi_request", autospec=True
72             ) as nxapi_request:
73                 result = nxos_proxy.proxy_config(commands)
74                 self.assertEqual(result, [commands, nxapi_request.return_value])
75     def test_proxy_config_save_config(self):
76         commands = ["feature bgp", "router bgp 65535"]
77         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"save_config": None}):
78             with patch(
79                 "salt.proxy.nxos._nxapi_request", autospec=True
80             ) as nxapi_request:
81                 result = nxos_proxy.proxy_config(commands, save_config=True)
82                 self.assertEqual(result, [commands, nxapi_request.return_value])
83     def test__init_nxapi(self):
84         opts = {"proxy": {"arg1": None}}
85         nxapi_request = create_autospec(nxos_utils.nxapi_request, return_value="data")
86 <a name="0"></a>
87         with patch("salt.proxy.nxos.DEVICE_DETAILS", {}) as device_details:
88             with patch(
89                 "salt.proxy.nxos.__utils__", {<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>"nxos.nxapi_request": nxapi_request}
90             ):
91                 result = nxos_proxy._init_nxapi(opts)
92                 self.assertTrue(device_details["initialized"])
93                 self.assertTrue(device_details["up"])
94                 self.assertTrue(device_details["save_config"])
95                 self.</b></font>assertTrue(result)
96                 nxapi_request.assert_called_with("show clock", **opts["proxy"])
97     def test_bad__init_nxapi(self):
98         class NXAPIException(Exception):
99             pass
100         nxapi_request = create_autospec(
101             nxos_utils.nxapi_request, side_effect=NXAPIException
102         )
103         with patch("salt.proxy.nxos.__utils__", {"nxos.nxapi_request": nxapi_request}):
104             with patch("salt.proxy.nxos.log", autospec=True) as log:
105                 with self.assertRaises(NXAPIException):
106                     nxos_proxy._init_nxapi({"proxy": {"host": "HOST"}})
107                 log.error.assert_called()
108     def test__initialized_nxapi(self):
109         result = nxos_proxy._initialized_nxapi()
110         self.assertFalse(result)
111         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"initialized": True}):
112             result = nxos_proxy._initialized_nxapi()
113             self.assertTrue(result)
114     def test__ping_nxapi(self):
115         result = nxos_proxy._ping_nxapi()
116         self.assertFalse(result)
117         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"up": True}):
118             result = nxos_proxy._ping_nxapi()
119             self.assertTrue(result)
120     def test__shutdown_nxapi(self):
121         opts = {"id": "value"}
122         with patch("salt.proxy.nxos.log", autospec=True):
123             nxos_proxy._shutdown_nxapi()
124     def test__nxapi_request_ssh_return(self):
125         commands = "show version"
126         with patch("salt.proxy.nxos.CONNECTION", "ssh"):
127             result = nxos_proxy._nxapi_request(commands)
128             self.assertEqual("_nxapi_request is not available for ssh proxy", result)
129     def test__nxapi_request_connect(self):
130         commands = "show version"
131         nxapi_request = create_autospec(nxos_utils.nxapi_request, return_value="data")
132         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"conn_args": {"arg1": None}}):
133             with patch(
134                 "salt.proxy.nxos.__utils__", {"nxos.nxapi_request": nxapi_request}
135             ):
136                 result = nxos_proxy._nxapi_request(commands)
137                 self.assertEqual("data", result)
138                 nxapi_request.assert_called_with(commands, method="cli_conf", arg1=None)
139 class NxosSSHProxyTestCase(TestCase, LoaderModuleMockMixin):
140     def setup_loader_modules(self):
141         return {
142             nxos_proxy: {
143                 "__opts__": {
144                     "proxy": {
145                         "host": "dt-n9k5-1.cisco.com",
146                         "username": "admin",
147                         "password": "password",
148                     }
149                 },
150                 "CONNECTION": "ssh",
151             }
152         }
153     def test_init(self):
154         with patch("salt.proxy.nxos._init_ssh", autospec=True) as init_ssh:
155             result = nxos_proxy.init()
156             self.assertEqual(result, init_ssh.return_value)
157     def test_init_opts_none(self):
158         with patch("salt.proxy.nxos.__opts__", {"proxy": {"connection": None}}):
159             with patch("salt.proxy.nxos._init_ssh", autospec=True) as init_ssh:
160                 result = nxos_proxy.init()
161                 self.assertEqual(result, init_ssh.return_value)
162     def test_initialized(self):
163         with patch(
164             "salt.proxy.nxos._initialized_ssh", autospec=True
165         ) as initialized_ssh:
166             result = nxos_proxy.initialized()
167             self.assertEqual(result, initialized_ssh.return_value)
168     def test_ping(self):
169         with patch("salt.proxy.nxos._ping_ssh", autospec=True) as ping_ssh:
170             result = nxos_proxy.ping()
171             self.assertEqual(result, ping_ssh.return_value)
172     def test_grains(self):
173         with patch(
174             "salt.proxy.nxos.sendline", autospec=True, return_value=n9k_show_ver_list[0]
175         ):
176             result = nxos_proxy.grains()
177             self.assertEqual(result, n9k_grains)
178     def test_sendline(self):
179         command = "show version"
180         with patch("salt.proxy.nxos._sendline_ssh", autospec=True) as sendline_ssh:
181             result = nxos_proxy.sendline(command)
182             self.assertEqual(result, sendline_ssh.return_value)
183     def test_proxy_config(self):
184         commands = ["feature bgp", "router bgp 65535"]
185         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"save_config": False}):
186             with patch("salt.proxy.nxos._sendline_ssh", autospec=True) as sendline_ssh:
187                 result = nxos_proxy.proxy_config(commands)
188                 self.assertEqual(result, [commands, sendline_ssh.return_value])
189     def test_proxy_config_save_config(self):
190         commands = ["feature bgp", "router bgp 65535"]
191         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"save_config": None}):
192             with patch("salt.proxy.nxos._sendline_ssh", autospec=True) as sendline_ssh:
193                 result = nxos_proxy.proxy_config(commands, save_config=True)
194                 self.assertEqual(result, [commands, sendline_ssh.return_value])
195     def test_proxy_config_error(self):
196         with patch(
197             "salt.proxy.nxos._sendline_ssh",
198             autospec=True,
199             side_effect=CommandExecutionError,
200         ):
201             with self.assertRaises(CommandExecutionError):
202                 nxos_proxy.proxy_config("show version", save_config=True)
203     def test__init_ssh_device_details(self):
204         with patch("salt.proxy.nxos.SSHConnection", autospec=True) as SSHConnection:
205             SSHConnection().sendline.return_value = ["", ""]
206             with patch("salt.proxy.nxos.DEVICE_DETAILS", {}) as device_details:
207                 nxos_proxy._init_ssh(None)
208                 self.assertIn(nxos_proxy._worker_name(), device_details)
209                 self.assertTrue(device_details["initialized"])
210                 self.assertTrue(device_details["save_config"])
211             with patch.dict(nxos_proxy.__opts__["proxy"], {"save_config": False}):
212                 with patch("salt.proxy.nxos.DEVICE_DETAILS", {}) as device_details:
213                     nxos_proxy._init_ssh(None)
214                     self.assertIn(nxos_proxy._worker_name(), device_details)
215                     self.assertTrue(device_details["initialized"])
216                     self.assertFalse(device_details["save_config"])
217     def test__init_ssh_opts(self):
218         with patch("salt.proxy.nxos.DEVICE_DETAILS", {}):
219             with patch("salt.proxy.nxos.SSHConnection", autospec=True) as SSHConnection:
220                 SSHConnection().sendline.return_value = ["", ""]
221                 nxos_proxy._init_ssh(None)
222                 self.assertEqual(
223                     nxos_proxy.__opts__["proxy"]["host"],
224                     SSHConnection.call_args[1]["host"],
225                 )
226                 opts = MagicMock()
227                 nxos_proxy._init_ssh(opts)
228                 self.assertEqual(
229                     opts["proxy"]["host"], SSHConnection.call_args[1]["host"]
230                 )
231     def test__init_ssh_prompt(self):
232         with patch("salt.proxy.nxos.DEVICE_DETAILS", {}):
233             with patch("salt.proxy.nxos.SSHConnection", autospec=True) as SSHConnection:
234                 SSHConnection().sendline.return_value = ["", ""]
235                 with patch.dict(
236                     nxos_proxy.__opts__["proxy"], {"prompt_regex": "n9k.*device"}
237                 ):
238                     nxos_proxy._init_ssh(None)
239                     self.assertEqual(
240                         "n9k.*device", SSHConnection.call_args[1]["prompt"]
241                     )
242                 with patch.dict(
243                     nxos_proxy.__opts__["proxy"], {"prompt_name": "n9k-device"}
244                 ):
245                     nxos_proxy._init_ssh(None)
246                     self.assertEqual(
247                         "n9k-device.*#", SSHConnection.call_args[1]["prompt"]
248                     )
249                 nxos_proxy._init_ssh(None)
250                 self.assertEqual(".+#$", SSHConnection.call_args[1]["prompt"])
251     def test__initialized_ssh(self):
252         with patch("salt.proxy.nxos.DEVICE_DETAILS", {"initialized": True}):
253             result = nxos_proxy._initialized_ssh()
254             self.assertTrue(result)
255         with patch("salt.proxy.nxos.DEVICE_DETAILS", {}):
256             result = nxos_proxy._initialized_ssh()
257             self.assertFalse(result)
258     def test__parse_output_for_errors(self):
259         data = "% Incomplete command at '^' marker."
260         command = "show"
261         with self.assertRaises(CommandExecutionError):
262             nxos_proxy._parse_output_for_errors(
263                 data, command, error_pattern="Incomplete"
264             )
265         data = "% Incomplete command at '^' marker."
266         command = "show"
267         with self.assertRaises(CommandExecutionError):
268             nxos_proxy._parse_output_for_errors(
269                 data, command, error_pattern=["Incomplete", "marker"]
270             )
271         data = "% Invalid command at '^' marker."
272         command = "show bep"
273         with self.assertRaises(CommandExecutionError):
274             nxos_proxy._parse_output_for_errors(data, command)
275         data = "% Incomplete command at '^' marker."
276         command = "show"
277         nxos_proxy._parse_output_for_errors(data, command)
278         data = "% Incomplete command at '^' marker."
279         command = "show"
280         nxos_proxy._parse_output_for_errors(data, command, error_pattern="foo")
281     def test__init_ssh_raise_exception(self):
282         class SSHException(Exception):
283             pass
284         with patch("salt.proxy.nxos.SSHConnection", autospec=True) as SSHConnection:
285             with patch("salt.proxy.nxos.log", autospec=True) as log:
286                 with self.assertRaises(SSHException):
287                     SSHConnection.side_effect = SSHException
288                     nxos_proxy._init_ssh(None)
289                 log.error.assert_called()
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>esxi_3.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 import logging
2 import re
3 import sys
4 import salt.utils.files
5 from salt.config.schemas.esxi import DiskGroupsDiskScsiAddressSchema, HostCacheSchema
6 from salt.exceptions import (
7     ArgumentValueError,
8     CommandExecutionError,
9     InvalidConfigError,
10     VMwareApiError,
11     VMwareObjectRetrievalError,
12     VMwareSaltError,
13 )
14 from salt.utils.decorators import depends
15 try:
16     import jsonschema
17     HAS_JSONSCHEMA = True
18 except ImportError:
19     HAS_JSONSCHEMA = False
20 log = logging.getLogger(__name__)
21 try:
22     from pyVmomi import VmomiSupport
23     if (
24         "vim25/6.0" in VmomiSupport.versionMap
25         and sys.version_info &gt; (2, 7)
26         and sys.version_info &lt; (2, 7, 9)
27     ):
28         log.debug(
29             "pyVmomi not loaded: Incompatible versions of Python. See Issue #29537."
30         )
31         raise ImportError()
32     HAS_PYVMOMI = True
33 except ImportError:
34     HAS_PYVMOMI = False
35 def __virtual__():
36     if "esxi.cmd" in __salt__:
37         return True
38     return (False, "esxi module could not be loaded")
39 def coredump_configured(name, enabled, dump_ip, host_vnic="vmk0", dump_port=6500):
40     ret = {"name": name, "result": False, "changes": {}, "comment": ""}
41     esxi_cmd = "esxi.cmd"
42     enabled_msg = (
43         "ESXi requires that the core dump must be enabled "
44         "before any other parameters may be set."
45     )
46     host = __pillar__["proxy"]["host"]
47     current_config = __salt__[esxi_cmd]("get_coredump_network_config").get(host)
48     error = current_config.get("Error")
49     if error:
50         ret["comment"] = "Error: {}".format(error)
51         return ret
52     current_config = current_config.get("Coredump Config")
53     current_enabled = current_config.get("enabled")
54     if current_enabled != enabled:
55         enabled_changes = {"enabled": {"old": current_enabled, "new": enabled}}
56         if not __opts__["test"]:
57             response = __salt__[esxi_cmd](
58                 "coredump_network_enable", enabled=enabled
59             ).get(host)
60             error = response.get("Error")
61             if error:
62                 ret["comment"] = "Error: {}".format(error)
63                 return ret
64             if not enabled:
65                 ret["result"] = True
66                 ret["comment"] = enabled_msg
67                 ret["changes"].update(enabled_changes)
68                 return ret
69         ret["changes"].update(enabled_changes)
70     elif not enabled:
71         ret["result"] = True
72         ret["comment"] = enabled_msg
73         return ret
74     changes = False
75     current_ip = current_config.get("ip")
76     if current_ip != dump_ip:
77         ret["changes"].update({"dump_ip": {"old": current_ip, "new": dump_ip}})
78         changes = True
79     current_vnic = current_config.get("host_vnic")
80     if current_vnic != host_vnic:
81         ret["changes"].update({"host_vnic": {"old": current_vnic, "new": host_vnic}})
82         changes = True
83     current_port = current_config.get("port")
84     if current_port != str(dump_port):
85         ret["changes"].update(
86             {"dump_port": {"old": current_port, "new": str(dump_port)}}
87         )
88         changes = True
89     if not __opts__["test"] and changes is True:
90         response = __salt__[esxi_cmd](
91             "set_coredump_network_config",
92             dump_ip=dump_ip,
93             host_vnic=host_vnic,
94             dump_port=dump_port,
95         ).get(host)
96         if response.get("success") is False:
97             msg = response.get("stderr")
98             if not msg:
99                 msg = response.get("stdout")
100             ret["comment"] = "Error: {}".format(msg)
101             return ret
102     ret["result"] = True
103     if ret["changes"] == {}:
104         ret["comment"] = "Core Dump configuration is already in the desired state."
105         return ret
106     if __opts__["test"]:
107         ret["result"] = None
108         ret["comment"] = "Core dump configuration will change."
109     return ret
110 def password_present(name, password):
111     ret = {
112         "name": name,
113         "result": True,
114         "changes": {"old": "unknown", "new": "********"},
115         "comment": "Host password was updated.",
116     }
117     esxi_cmd = "esxi.cmd"
118     if __opts__["test"]:
119         ret["result"] = None
120         ret["comment"] = "Host password will change."
121         return ret
122     else:
123         try:
124             __salt__[esxi_cmd]("update_host_password", new_password=password)
125         except CommandExecutionError as err:
126             ret["result"] = False
127             ret["comment"] = "Error: {}".format(err)
128             return ret
129     return ret
130 def ntp_configured(
131     name,
132     service_running,
133     ntp_servers=None,
134     service_policy=None,
135     service_restart=False,
136     update_datetime=False,
137 ):
138     ret = {"name": name, "result": False, "changes": {}, "comment": ""}
139     esxi_cmd = "esxi.cmd"
140     host = __pillar__["proxy"]["host"]
141     ntpd = "ntpd"
142     ntp_config = __salt__[esxi_cmd]("get_ntp_config").get(host)
143     ntp_running = __salt__[esxi_cmd]("get_service_running", service_name=ntpd).get(host)
144     error = ntp_running.get("Error")
145     if error:
146         ret["comment"] = "Error: {}".format(error)
147         return ret
148     ntp_running = ntp_running.get(ntpd)
149     if ntp_servers and set(ntp_servers) != set(ntp_config):
150         if not __opts__["test"]:
151             response = __salt__[esxi_cmd](
152                 "set_ntp_config", ntp_servers=ntp_servers
153             ).get(host)
154             error = response.get("Error")
155             if error:
156                 ret["comment"] = "Error: {}".format(error)
157                 return ret
158         ret["changes"].update({"ntp_servers": {"old": ntp_config, "new": ntp_servers}})
159     if service_running != ntp_running:
160         if not __opts__["test"]:
161             if ntp_running is True:
162                 response = __salt__[esxi_cmd]("service_start", service_name=ntpd).get(
163                     host
164                 )
165                 error = response.get("Error")
166                 if error:
167                     ret["comment"] = "Error: {}".format(error)
168                     return ret
169             else:
170                 response = __salt__[esxi_cmd]("service_stop", service_name=ntpd).get(
171                     host
172                 )
173                 error = response.get("Error")
174                 if error:
175                     ret["comment"] = "Error: {}".format(error)
176                     return ret
177         ret["changes"].update(
178             {"service_running": {"old": ntp_running, "new": service_running}}
179         )
180     if service_policy:
181         current_service_policy = __salt__[esxi_cmd](
182             "get_service_policy", service_name=ntpd
183         ).get(host)
184         error = current_service_policy.get("Error")
185         if error:
186             ret["comment"] = "Error: {}".format(error)
187             return ret
188         current_service_policy = current_service_policy.get(ntpd)
189         if service_policy != current_service_policy:
190             if not __opts__["test"]:
191                 response = __salt__[esxi_cmd](
192                     "set_service_policy",
193                     service_name=ntpd,
194                     service_policy=service_policy,
195                 ).get(host)
196                 error = response.get("Error")
197                 if error:
198                     ret["comment"] = "Error: {}".format(error)
199                     return ret
200             ret["changes"].update(
201                 {
202                     "service_policy": {
203                         "old": current_service_policy,
204                         "new": service_policy,
205                     }
206                 }
207             )
208     if update_datetime:
209         if not __opts__["test"]:
210             response = __salt__[esxi_cmd]("update_host_datetime").get(host)
211             error = response.get("Error")
212             if error:
213                 ret["comment"] = "Error: {}".format(error)
214                 return ret
215         ret["changes"].update(
216             {"update_datetime": {"old": "", "new": "Host datetime was updated."}}
217         )
218     if service_restart:
219         if not __opts__["test"]:
220             response = __salt__[esxi_cmd]("service_restart", service_name=ntpd).get(
221                 host
222             )
223             error = response.get("Error")
224             if error:
225                 ret["comment"] = "Error: {}".format(error)
226                 return ret
227         ret["changes"].update(
228             {"service_restart": {"old": "", "new": "NTP Daemon Restarted."}}
229         )
230     ret["result"] = True
231     if ret["changes"] == {}:
232         ret["comment"] = "NTP is already in the desired state."
233         return ret
234     if __opts__["test"]:
235         ret["result"] = None
236         ret["comment"] = "NTP state will change."
237     return ret
238 def vmotion_configured(name, enabled, device="vmk0"):
239     ret = {"name": name, "result": False, "changes": {}, "comment": ""}
240     esxi_cmd = "esxi.cmd"
241     host = __pillar__["proxy"]["host"]
242     current_vmotion_enabled = __salt__[esxi_cmd]("get_vmotion_enabled").get(host)
243     current_vmotion_enabled = current_vmotion_enabled.get("VMotion Enabled")
244     if enabled != current_vmotion_enabled:
245         if not __opts__["test"]:
246             if enabled is True:
247                 response = __salt__[esxi_cmd]("vmotion_enable", device=device).get(host)
248                 error = response.get("Error")
249                 if error:
250                     ret["comment"] = "Error: {}".format(error)
251                     return ret
252             else:
253                 response = __salt__[esxi_cmd]("vmotion_disable").get(host)
254                 error = response.get("Error")
255                 if error:
256                     ret["comment"] = "Error: {}".format(error)
257                     return ret
258         ret["changes"].update(
259             {"enabled": {"old": current_vmotion_enabled, "new": enabled}}
260         )
261     ret["result"] = True
262     if ret["changes"] == {}:
263         ret["comment"] = "VMotion configuration is already in the desired state."
264         return ret
265     if __opts__["test"]:
266         ret["result"] = None
267         ret["comment"] = "VMotion configuration will change."
268     return ret
269 def vsan_configured(name, enabled, add_disks_to_vsan=False):
270     ret = {"name": name, "result": False, "changes": {}, "comment": ""}
271     esxi_cmd = "esxi.cmd"
272     host = __pillar__["proxy"]["host"]
273     current_vsan_enabled = __salt__[esxi_cmd]("get_vsan_enabled").get(host)
274     error = current_vsan_enabled.get("Error")
275     if error:
276         ret["comment"] = "Error: {}".format(error)
277         return ret
278     current_vsan_enabled = current_vsan_enabled.get("VSAN Enabled")
279     if enabled != current_vsan_enabled:
280         if not __opts__["test"]:
281             if enabled is True:
282                 response = __salt__[esxi_cmd]("vsan_enable").get(host)
283                 error = response.get("Error")
284                 if error:
285                     ret["comment"] = "Error: {}".format(error)
286                     return ret
287             else:
288                 response = __salt__[esxi_cmd]("vsan_disable").get(host)
289                 error = response.get("Error")
290                 if error:
291                     ret["comment"] = "Error: {}".format(error)
292                     return ret
293         ret["changes"].update(
294             {"enabled": {"old": current_vsan_enabled, "new": enabled}}
295         )
296     if add_disks_to_vsan:
297         current_eligible_disks = __salt__[esxi_cmd]("get_vsan_eligible_disks").get(host)
298         error = current_eligible_disks.get("Error")
299         if error:
300             ret["comment"] = "Error: {}".format(error)
301             return ret
302         disks = current_eligible_disks.get("Eligible")
303         if disks and isinstance(disks, list):
304             if not __opts__["test"]:
305                 response = __salt__[esxi_cmd]("vsan_add_disks").get(host)
306                 error = response.get("Error")
307                 if error:
308                     ret["comment"] = "Error: {}".format(error)
309                     return ret
310             ret["changes"].update({"add_disks_to_vsan": {"old": "", "new": disks}})
311     ret["result"] = True
312     if ret["changes"] == {}:
313         ret["comment"] = "VSAN configuration is already in the desired state."
314         return ret
315     if __opts__["test"]:
316         ret["result"] = None
317         ret["comment"] = "VSAN configuration will change."
318     return ret
319 def ssh_configured(
320     name,
321     service_running,
322     ssh_key=None,
323     ssh_key_file=None,
324     service_policy=None,
325     service_restart=False,
326     certificate_verify=None,
327 ):
328     if certificate_verify is None:
329         certificate_verify = True
330     ret = {"name": name, "result": False, "changes": {}, "comment": ""}
331     esxi_cmd = "esxi.cmd"
332     host = __pillar__["proxy"]["host"]
333     ssh = "ssh"
334     ssh_running = __salt__[esxi_cmd]("get_service_running", service_name=ssh).get(host)
335     error = ssh_running.get("Error")
336     if error:
337         ret["comment"] = "Error: {}".format(error)
338         return ret
339     ssh_running = ssh_running.get(ssh)
340     if service_running != ssh_running:
341         if not __opts__["test"]:
342             if service_running is True:
343                 enable = __salt__[esxi_cmd]("service_start", service_name=ssh).get(host)
344                 error = enable.get("Error")
345                 if error:
346                     ret["comment"] = "Error: {}".format(error)
347                     return ret
348             else:
349                 disable = __salt__[esxi_cmd]("service_stop", service_name=ssh).get(host)
350                 error = disable.get("Error")
351                 if error:
352                     ret["comment"] = "Error: {}".format(error)
353                     return ret
354         ret["changes"].update(
355             {"service_running": {"old": ssh_running, "new": service_running}}
356         )
357     current_ssh_key, ssh_key_changed = None, False
358     if ssh_key or ssh_key_file:
359         current_ssh_key = __salt__[esxi_cmd](
360             "get_ssh_key", certificate_verify=certificate_verify
361         )
362         error = current_ssh_key.get("Error")
363         if error:
364             ret["comment"] = "Error: {}".format(error)
365             return ret
366         current_ssh_key = current_ssh_key.get("key")
367         if current_ssh_key:
368             clean_current_key = _strip_key(current_ssh_key).split(" ")
369             if not ssh_key:
370                 ssh_key = ""
371                 with salt.utils.files.fopen(ssh_key_file, "r") as key_file:
372                     for line in key_file:
373                         if line.startswith("#"):
374                             continue
375                         ssh_key = ssh_key + line
376             clean_ssh_key = _strip_key(ssh_key).split(" ")
377             if (
378                 clean_current_key[0] != clean_ssh_key[0]
379                 or clean_current_key[1] != clean_ssh_key[1]
380             ):
381                 ssh_key_changed = True
382         else:
383             ssh_key_changed = True
384     if ssh_key_changed:
385         if not __opts__["test"]:
386             response = __salt__[esxi_cmd](
387                 "upload_ssh_key",
388                 ssh_key=ssh_key,
389                 ssh_key_file=ssh_key_file,
390                 certificate_verify=certificate_verify,
391             )
392             error = response.get("Error")
393             if error:
394                 ret["comment"] = "Error: {}".format(error)
395                 return ret
396         ret["changes"].update(
397             {
398                 "SSH Key": {
399                     "old": current_ssh_key,
400                     "new": ssh_key if ssh_key else ssh_key_file,
401                 }
402             }
403         )
404     if service_policy:
405         current_service_policy = __salt__[esxi_cmd](
406             "get_service_policy", service_name=ssh
407         ).get(host)
408         error = current_service_policy.get("Error")
409         if error:
410             ret["comment"] = "Error: {}".format(error)
411             return ret
412         current_service_policy = current_service_policy.get(ssh)
413         if service_policy != current_service_policy:
414             if not __opts__["test"]:
415                 response = __salt__[esxi_cmd](
416                     "set_service_policy",
417                     service_name=ssh,
418                     service_policy=service_policy,
419                 ).get(host)
420                 error = response.get("Error")
421                 if error:
422                     ret["comment"] = "Error: {}".format(error)
423                     return ret
424             ret["changes"].update(
425                 {
426                     "service_policy": {
427                         "old": current_service_policy,
428                         "new": service_policy,
429                     }
430                 }
431             )
432     if service_restart:
433         if not __opts__["test"]:
434             response = __salt__[esxi_cmd]("service_restart", service_name=ssh).get(host)
435             error = response.get("Error")
436             if error:
437                 ret["comment"] = "Error: {}".format(error)
438                 return ret
439         ret["changes"].update(
440             {"service_restart": {"old": "", "new": "SSH service restarted."}}
441         )
442     ret["result"] = True
443     if ret["changes"] == {}:
444         ret["comment"] = "SSH service is already in the desired state."
445         return ret
446     if __opts__["test"]:
447         ret["result"] = None
448         ret["comment"] = "SSH service state will change."
449     return ret
450 def syslog_configured(
451     name,
452     syslog_configs,
453     firewall=True,
454     reset_service=True,
455     reset_syslog_config=False,
456     reset_configs=None,
457 ):
458     ret = {"name": name, "result": False, "changes": {}, "comment": ""}
459     esxi_cmd = "esxi.cmd"
460     host = __pillar__["proxy"]["host"]
461     if reset_syslog_config:
462         if not reset_configs:
463             reset_configs = "all"
464         if not __opts__["test"]:
465             reset = __salt__[esxi_cmd](
466                 "reset_syslog_config", syslog_config=reset_configs
467             ).get(host)
468             for key, val in reset.items():
469                 if isinstance(val, bool):
470                     continue
471                 if not val.get("success"):
472                     msg = val.get("message")
473                     if not msg:
474                         msg = (
475                             "There was an error resetting a syslog config '{}'."
476                             "Please check debug logs.".format(val)
477                         )
478                     ret["comment"] = "Error: {}".format(msg)
479                     return ret
480         ret["changes"].update(
481             {"reset_syslog_config": {"old": "", "new": reset_configs}}
482         )
483     current_firewall = __salt__[esxi_cmd]("get_firewall_status").get(host)
484     error = current_firewall.get("Error")
485     if error:
486         ret["comment"] = "Error: {}".format(error)
487         return ret
488     current_firewall = current_firewall.get("rulesets").get("syslog")
489     if current_firewall != firewall:
490         if not __opts__["test"]:
491             enabled = __salt__[esxi_cmd](
492                 "enable_firewall_ruleset",
493                 ruleset_enable=firewall,
494                 ruleset_name="syslog",
495             ).get(host)
496             if enabled.get("retcode") != 0:
497                 err = enabled.get("stderr")
498                 out = enabled.get("stdout")
499                 ret["comment"] = "Error: {}".format(err if err else out)
500                 return ret
501         ret["changes"].update({"firewall": {"old": current_firewall, "new": firewall}})
502     current_syslog_config = __salt__[esxi_cmd]("get_syslog_config").get(host)
503     for key, val in syslog_configs.items():
504         try:
505             lookup_key = _lookup_syslog_config(key)
506         except KeyError:
507             ret["comment"] = "'{}' is not a valid config variable.".format(key)
508             return ret
509         current_val = current_syslog_config[lookup_key]
510         if str(current_val) != str(val):
511             if not __opts__["test"]:
512                 response = __salt__[esxi_cmd](
513                     "set_syslog_config",
514                     syslog_config=key,
515                     config_value=val,
516                     firewall=firewall,
517                     reset_service=reset_service,
518                 ).get(host)
519                 success = response.get(key).get("success")
520                 if not success:
521                     msg = response.get(key).get("message")
522                     if not msg:
523                         msg = (
524                             "There was an error setting syslog config '{}'. "
525                             "Please check debug logs.".format(key)
526                         )
527                     ret["comment"] = msg
528                     return ret
529             if not ret["changes"].get("syslog_config"):
530                 ret["changes"].update({"syslog_config": {}})
531             ret["changes"]["syslog_config"].update(
532                 {key: {"old": current_val, "new": val}}
533             )
534     ret["result"] = True
535     if ret["changes"] == {}:
536         ret["comment"] = "Syslog is already in the desired state."
537         return ret
538     if __opts__["test"]:
539         ret["result"] = None
540         ret["comment"] = "Syslog state will change."
541     return ret
542 @depends(HAS_PYVMOMI)
543 @depends(HAS_JSONSCHEMA)
544 def diskgroups_configured(name, diskgroups, erase_disks=False):
545     proxy_details = __salt__["esxi.get_details"]()
546     hostname = (
547         proxy_details["host"]
548         if not proxy_details.get("vcenter")
549         else proxy_details["esxi_host"]
550     )
551     log.info("Running state %s for host '%s'", name, hostname)
552     ret = {"name": name, "result": None, "changes": {}, "comments": None}
553     errors = False
554     changes = False
555     comments = []
556     diskgroup_changes = {}
557     si = None
558     try:
559         log.trace("Validating diskgroups_configured input")
560         schema = DiskGroupsDiskScsiAddressSchema.serialize()
561         try:
562             jsonschema.validate(
563                 {"diskgroups": diskgroups, "erase_disks": erase_disks}, schema
564             )
565         except jsonschema.exceptions.ValidationError as exc:
566             raise InvalidConfigError(exc)
567         si = __salt__["vsphere.get_service_instance_via_proxy"]()
568         host_disks = __salt__["vsphere.list_disks"](service_instance=si)
569         if not host_disks:
570             raise VMwareObjectRetrievalError(
571                 "No disks retrieved from host '{}'".format(hostname)
572             )
573         scsi_addr_to_disk_map = {d["scsi_address"]: d for d in host_disks}
574         log.trace("scsi_addr_to_disk_map = %s", scsi_addr_to_disk_map)
575         existing_diskgroups = __salt__["vsphere.list_diskgroups"](service_instance=si)
576         cache_disk_to_existing_diskgroup_map = {
577             dg["cache_disk"]: dg for dg in existing_diskgroups
578         }
579     except CommandExecutionError as err:
580         log.error("Error: %s", err)
581         if si:
582             __salt__["vsphere.disconnect"](si)
583         ret.update(
584             {"result": False if not __opts__["test"] else None, "comment": str(err)}
585         )
586         return ret
587     for idx, dg in enumerate(diskgroups):
588         if not dg["cache_scsi_addr"] in scsi_addr_to_disk_map:
589             comments.append(
590                 "No cache disk with scsi address '{}' was found.".format(
591                     dg["cache_scsi_addr"]
592                 )
593             )
594             log.error(comments[-1])
595             errors = True
596             continue
597         cache_disk_id = scsi_addr_to_disk_map[dg["cache_scsi_addr"]]["id"]
598         cache_disk_display = "{} (id:{})".format(dg["cache_scsi_addr"], cache_disk_id)
599         bad_scsi_addrs = []
600         capacity_disk_ids = []
601         capacity_disk_displays = []
602         for scsi_addr in dg["capacity_scsi_addrs"]:
603             if scsi_addr not in scsi_addr_to_disk_map:
604                 bad_scsi_addrs.append(scsi_addr)
605                 continue
606             capacity_disk_ids.append(scsi_addr_to_disk_map[scsi_addr]["id"])
607             capacity_disk_displays.append(
608                 "{} (id:{})".format(scsi_addr, capacity_disk_ids[-1])
609             )
610         if bad_scsi_addrs:
611             comments.append(
612                 "Error in diskgroup #{}: capacity disks with scsi addresses {} "
613                 "were not found.".format(
614                     idx, ", ".join(["'{}'".format(a) for a in bad_scsi_addrs])
615                 )
616             )
617             log.error(comments[-1])
618             errors = True
619             continue
620         if not cache_disk_to_existing_diskgroup_map.get(cache_disk_id):
621             log.trace("erase_disks = %s", erase_disks)
622             if erase_disks:
623                 if __opts__["test"]:
624                     comments.append(
625                         "State {} will "
626                         "erase all disks of disk group #{}; "
627                         "cache disk: '{}', "
628                         "capacity disk(s): {}."
629                         "".format(
630                             name,
631                             idx,
632                             cache_disk_display,
633                             ", ".join(
634                                 ["'{}'".format(a) for a in capacity_disk_displays]
635                             ),
636                         )
637                     )
638                 else:
639                     for disk_id in [cache_disk_id] + capacity_disk_ids:
640                         __salt__["vsphere.erase_disk_partitions"](
641                             disk_id=disk_id, service_instance=si
642                         )
643                     comments.append(
644                         "Erased disks of diskgroup #{}; "
645                         "cache disk: '{}', capacity disk(s): "
646                         "{}".format(
647                             idx,
648                             cache_disk_display,
649                             ", ".join(
650                                 ["'{}'".format(a) for a in capacity_disk_displays]
651                             ),
652                         )
653                     )
654                     log.info(comments[-1])
655             if __opts__["test"]:
656                 comments.append(
657                     "State {} will create "
658                     "the disk group #{}; cache disk: '{}', "
659                     "capacity disk(s): {}.".format(
660                         name,
661                         idx,
662                         cache_disk_display,
663                         ", ".join(["'{}'".format(a) for a in capacity_disk_displays]),
664                     )
665                 )
666                 log.info(comments[-1])
667                 changes = True
668                 continue
669             try:
670                 __salt__["vsphere.create_diskgroup"](
671                     cache_disk_id,
672                     capacity_disk_ids,
673                     safety_checks=False,
674                     service_instance=si,
675                 )
676             except VMwareSaltError as err:
677                 comments.append("Error creating disk group #{}: {}.".format(idx, err))
678                 log.error(comments[-1])
679                 errors = True
680                 continue
681             comments.append("Created disk group #'{}'.".format(idx))
682             log.info(comments[-1])
683             diskgroup_changes[str(idx)] = {
684                 "new": {"cache": cache_disk_display, "capacity": capacity_disk_displays}
685             }
686             changes = True
687             continue
688         log.debug(
689             "Disk group #%s exists. Checking capacity disks: %s.",
690             idx,
691             capacity_disk_displays,
692         )
693         existing_diskgroup = cache_disk_to_existing_diskgroup_map.get(cache_disk_id)
694         existing_capacity_disk_displays = [
695             "{} (id:{})".format(
696                 [d["scsi_address"] for d in host_disks if d["id"] == disk_id][0],
697                 disk_id,
698             )
699             for disk_id in existing_diskgroup["capacity_disks"]
700         ]
701         added_capacity_disk_ids = []
702         added_capacity_disk_displays = []
703         removed_capacity_disk_ids = []
704         removed_capacity_disk_displays = []
705         for disk_id in capacity_disk_ids:
706             if disk_id not in existing_diskgroup["capacity_disks"]:
707                 disk_scsi_addr = [
708                     d["scsi_address"] for d in host_disks if d["id"] == disk_id
709                 ][0]
710                 added_capacity_disk_ids.append(disk_id)
711                 added_capacity_disk_displays.append(
712                     "{} (id:{})".format(disk_scsi_addr, disk_id)
713                 )
714         for disk_id in existing_diskgroup["capacity_disks"]:
715             if disk_id not in capacity_disk_ids:
716                 disk_scsi_addr = [
717                     d["scsi_address"] for d in host_disks if d["id"] == disk_id
718                 ][0]
719                 removed_capacity_disk_ids.append(disk_id)
720                 removed_capacity_disk_displays.append(
721                     "{} (id:{})".format(disk_scsi_addr, disk_id)
722                 )
723         log.debug(
724             "Disk group #%s: existing capacity disk ids: %s; added "
725             "capacity disk ids: %s; removed capacity disk ids: %s",
726             idx,
727             existing_capacity_disk_displays,
728             added_capacity_disk_displays,
729             removed_capacity_disk_displays,
730         )
731         if removed_capacity_disk_ids:
732             comments.append(
733                 "Error removing capacity disk(s) {} from disk group #{}; "
734                 "operation is not supported."
735                 "".format(
736                     ", ".join(
737                         ["'{}'".format(id) for id in removed_capacity_disk_displays]
738                     ),
739                     idx,
740                 )
741             )
742             log.error(comments[-1])
743             errors = True
744             continue
745         if added_capacity_disk_ids:
746             s = ", ".join(["'{}'".format(id) for id in added_capacity_disk_displays])
747             if __opts__["test"]:
748                 comments.append(
749                     "State {} will add capacity disk(s) {} to disk group #{}.".format(
750                         name, s, idx
751                     )
752                 )
753                 log.info(comments[-1])
754                 changes = True
755                 continue
756             try:
757                 __salt__["vsphere.add_capacity_to_diskgroup"](
758                     cache_disk_id,
759                     added_capacity_disk_ids,
760                     safety_checks=False,
761                     service_instance=si,
762                 )
763             except VMwareSaltError as err:
764                 comments.append(
765                     "Error adding capacity disk(s) {} to disk group #{}: {}.".format(
766                         s, idx, err
767                     )
768                 )
769                 log.error(comments[-1])
770                 errors = True
771                 continue
772             com = "Added capacity disk(s) {} to disk group #{}".format(s, idx)
773             log.info(com)
774             comments.append(com)
775             diskgroup_changes[str(idx)] = {
776                 "new": {
777                     "cache": cache_disk_display,
778                     "capacity": capacity_disk_displays,
779                 },
780                 "old": {
781                     "cache": cache_disk_display,
782                     "capacity": existing_capacity_disk_displays,
783                 },
784             }
785             changes = True
786             continue
787         s = "Disk group #{} is correctly configured. Nothing to be done.".format(idx)
788         log.info(s)
789         comments.append(s)
790     __salt__["vsphere.disconnect"](si)
791     result = (
792         True
793         if not (changes or errors)
794         else None  # no changes/errors
795         if __opts__["test"]
796         else False  # running in test mode
797         if errors
798         else True
799     )  # found errors; defaults to True
800     ret.update(
801         {"result": result, "comment": "\n".join(comments), "changes": diskgroup_changes}
802     )
803     return ret
804 @depends(HAS_PYVMOMI)
805 @depends(HAS_JSONSCHEMA)
806 def host_cache_configured(
807     name,
808     enabled,
809     datastore,
810     swap_size="100%",
811     dedicated_backing_disk=False,
812     erase_backing_disk=False,
813 ):
814     log.trace("enabled = %s", enabled)
815     log.trace("datastore = %s", datastore)
816     log.trace("swap_size = %s", swap_size)
817     log.trace("erase_backing_disk = %s", erase_backing_disk)
818     proxy_details = __salt__["esxi.get_details"]()
819     hostname = (
820         proxy_details["host"]
821         if not proxy_details.get("vcenter")
822         else proxy_details["esxi_host"]
823     )
824     log.trace("hostname = %s", hostname)
825     log.info("Running host_cache_swap_configured for host '%s'", hostname)
826     ret = {
827         "name": hostname,
828         "comment": "Default comments",
829         "result": None,
830         "changes": {},
831     }
832     result = None if __opts__["test"] else True  # We assume success
833     needs_setting = False
834     comments = []
835     changes = {}
836     si = None
837     try:
838         log.debug("Validating host_cache_configured input")
839         schema = HostCacheSchema.serialize()
840         try:
841             jsonschema.validate(
842                 {
843                     "enabled": enabled,
844                     "datastore": datastore,
845                     "swap_size": swap_size,
846                     "erase_backing_disk": erase_backing_disk,
847                 },
848                 schema,
849             )
850         except jsonschema.exceptions.ValidationError as exc:
851             raise InvalidConfigError(exc)
852         m = re.match(r"(\d+)(%|GiB)", swap_size)
853         swap_size_value = int(m.group(1))
854         swap_type = m.group(2)
855         log.trace("swap_size_value = %s; swap_type = %s", swap_size_value, swap_type)
856         si = __salt__["vsphere.get_service_instance_via_proxy"]()
857         host_cache = __salt__["vsphere.get_host_cache"](service_instance=si)
858         if host_cache["enabled"] != enabled:
859             changes.update({"enabled": {"old": host_cache["enabled"], "new": enabled}})
860             needs_setting = True
861         existing_datastores = None
862         if host_cache.get("datastore"):
863             existing_datastores = __salt__["vsphere.list_datastores_via_proxy"](
864                 datastore_names=[datastore["name"]], service_instance=si
865             )
866         existing_disks = __salt__["vsphere.list_disks"](
867             scsi_addresses=[datastore["backing_disk_scsi_addr"]], service_instance=si
868         )
869         if not existing_disks:
870             raise VMwareObjectRetrievalError(
871                 "Disk with scsi address '{}' was not found in host '{}'".format(
872                     datastore["backing_disk_scsi_addr"], hostname
873                 )
874             )
875         backing_disk = existing_disks[0]
876         backing_disk_display = "{} (id:{})".format(
877             backing_disk["scsi_address"], backing_disk["id"]
878         )
879         log.trace("backing_disk = %s", backing_disk_display)
880         existing_datastore = None
881         if not existing_datastores:
882             if erase_backing_disk:
883                 if __opts__["test"]:
884                     comments.append(
885                         "State {} will erase the backing disk '{}' on host '{}'.".format(
886                             name, backing_disk_display, hostname
887                         )
888                     )
889                     log.info(comments[-1])
890                 else:
891                     __salt__["vsphere.erase_disk_partitions"](
892                         disk_id=backing_disk["id"], service_instance=si
893                     )
894                     comments.append(
895                         "Erased backing disk '{}' on host '{}'.".format(
896                             backing_disk_display, hostname
897                         )
898                     )
899                     log.info(comments[-1])
900             if __opts__["test"]:
901                 comments.append(
902                     "State {} will create the datastore '{}', with backing disk "
903                     "'{}', on host '{}'.".format(
904                         name, datastore["name"], backing_disk_display, hostname
905                     )
906                 )
907                 log.info(comments[-1])
908             else:
909                 if dedicated_backing_disk:
910                     partitions = __salt__["vsphere.list_disk_partitions"](
911                         disk_id=backing_disk["id"], service_instance=si
912                     )
913                     log.trace("partitions = %s", partitions)
914                     non_mbr_partitions = [p for p in partitions if p["format"] != "mbr"]
915                     if len(non_mbr_partitions) &gt; 0:
916                         raise VMwareApiError(
917                             "Backing disk '{}' has unexpected partitions".format(
918                                 backing_disk_display
919                             )
920                         )
921 <a name="0"></a>                __salt__["vsphere.create_vmfs_datastore"](
922                     datastore["name"],
923                     existing_disks[0]["id"],
924                     datastore<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>["vmfs_version"],
925                     service_instance=si,
926                 )
927                 comments.append(
928                     "Created vmfs datastore '{}', backed by "
929                     "disk '{}', on host '{}'.".format(
930                         datastore["name"], backing_disk_display, hostname
931                     )
932                 )
933                 log.info(comments[-1])
934                 changes.update(
935                     {
936                         "datastore": {
937                             "new"</b></font>: {
938                                 "name": datastore["name"],
939                                 "backing_disk": backing_disk_display,
940                             }
941                         }
942                     }
943                 )
944                 existing_datastore = __salt__["vsphere.list_datastores_via_proxy"](
945                     datastore_names=[datastore["name"]], service_instance=si
946                 )[0]
947             needs_setting = True
948         else:
949             if not existing_datastores[0].get("backing_disk_ids"):
950                 raise VMwareSaltError(
951                     "Datastore '{}' doesn't have a backing disk".format(
952                         datastore["name"]
953                     )
954                 )
955             if backing_disk["id"] not in existing_datastores[0]["backing_disk_ids"]:
956                 raise VMwareSaltError(
957                     "Datastore '{}' is not backed by the correct disk: "
958                     "expected '{}'; got {}".format(
959                         datastore["name"],
960                         backing_disk["id"],
961                         ", ".join(
962                             [
963                                 "'{}'".format(disk)
964                                 for disk in existing_datastores[0]["backing_disk_ids"]
965                             ]
966                         ),
967                     )
968                 )
969             comments.append(
970                 "Datastore '{}' already exists on host '{}' "
971                 "and is backed by disk '{}'. Nothing to be "
972                 "done.".format(datastore["name"], hostname, backing_disk_display)
973             )
974             existing_datastore = existing_datastores[0]
975             log.trace("existing_datastore = %s", existing_datastore)
976             log.info(comments[-1])
977         if existing_datastore:
978             if swap_type == "%":
979                 raw_size_MiB = (swap_size_value / 100.0) * (
980                     existing_datastore["capacity"] / 1024 / 1024
981                 )
982             else:
983                 raw_size_MiB = swap_size_value * 1024
984             log.trace("raw_size = %sMiB", raw_size_MiB)
985             swap_size_MiB = int(raw_size_MiB / 1024) * 1024
986             log.trace("adjusted swap_size = %sMiB", swap_size_MiB)
987             existing_swap_size_MiB = 0
988             m = (
989                 re.match(r"(\d+)MiB", host_cache.get("swap_size"))
990                 if host_cache.get("swap_size")
991                 else None
992             )
993             if m:
994                 existing_swap_size_MiB = int(m.group(1))
995             if not existing_swap_size_MiB == swap_size_MiB:
996                 needs_setting = True
997                 changes.update(
998                     {
999                         "swap_size": {
1000                             "old": "{}GiB".format(existing_swap_size_MiB / 1024),
1001                             "new": "{}GiB".format(swap_size_MiB / 1024),
1002                         }
1003                     }
1004                 )
1005         if needs_setting:
1006             if __opts__["test"]:
1007                 comments.append(
1008                     "State {} will configure the host cache on host '{}' to: {}.".format(
1009                         name,
1010                         hostname,
1011                         {
1012                             "enabled": enabled,
1013                             "datastore_name": datastore["name"],
1014                             "swap_size": swap_size,
1015                         },
1016                     )
1017                 )
1018             else:
1019                 if (existing_datastore["capacity"] / 1024.0 ** 2) &lt; swap_size_MiB:
1020                     raise ArgumentValueError(
1021                         "Capacity of host cache datastore '{}' ({} MiB) is "
1022                         "smaller than the required swap size ({} MiB)".format(
1023                             existing_datastore["name"],
1024                             existing_datastore["capacity"] / 1024.0 ** 2,
1025                             swap_size_MiB,
1026                         )
1027                     )
1028                 __salt__["vsphere.configure_host_cache"](
1029                     enabled,
1030                     datastore["name"],
1031                     swap_size_MiB=swap_size_MiB,
1032                     service_instance=si,
1033                 )
1034                 comments.append("Host cache configured on host '{}'.".format(hostname))
1035         else:
1036             comments.append(
1037                 "Host cache on host '{}' is already correctly "
1038                 "configured. Nothing to be done.".format(hostname)
1039             )
1040             result = True
1041         __salt__["vsphere.disconnect"](si)
1042         log.info(comments[-1])
1043         ret.update(
1044             {"comment": "\n".join(comments), "result": result, "changes": changes}
1045         )
1046         return ret
1047     except CommandExecutionError as err:
1048         log.error("Error: %s.", err)
1049         if si:
1050             __salt__["vsphere.disconnect"](si)
1051         ret.update(
1052             {
1053                 "result": False if not __opts__["test"] else None,
1054                 "comment": "{}.".format(err),
1055             }
1056         )
1057         return ret
1058 def _lookup_syslog_config(config):
1059     lookup = {
1060         "default-timeout": "Default Network Retry Timeout",
1061         "logdir": "Local Log Output",
1062         "default-size": "Local Logging Default Rotation Size",
1063         "logdir-unique": "Log To Unique Subdirectory",
1064         "default-rotate": "Local Logging Default Rotations",
1065         "loghost": "Remote Host",
1066     }
1067     return lookup.get(config)
1068 def _strip_key(key_string):
1069     key_string.strip()
1070     key_string.replace("\n", "")
1071     key_string.replace("\r\n", "")
1072     return key_string
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
