
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 17.38095238095238%, Tokens: 16</h2>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</h3>
            <pre><code>1  import torch
2  from utils import timer
3  from data import cfg
4  @torch.jit.script
5  def point_form(boxes):
6      return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin
7                       boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax
8  @torch.jit.script
9  def center_size(boxes):
10      return torch.cat(( (boxes[:, 2:] + boxes[:, :2])/2,     # cx, cy
11                          boxes[:, 2:] - boxes[:, :2]  ), 1)  # w, h
12  @torch.jit.script
13  def intersect(box_a, box_b):
14      n = box_a.size(0)
15      A = box_a.size(1)
16      B = box_b.size(1)
17      max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),
18                         box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))
19      min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),
20                         box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))
21      return torch.clamp(max_xy - min_xy, min=0).prod(3)  # inter
22  def jaccard(box_a, box_b, iscrowd:bool=False):
23      use_batch = True
24      if box_a.dim() == 2:
25          use_batch = False
26          box_a = box_a[None, ...]
27          box_b = box_b[None, ...]
28      inter = intersect(box_a, box_b)
29      area_a = ((box_a[:, :, 2]-box_a[:, :, 0]) *
30                (box_a[:, :, 3]-box_a[:, :, 1])).unsqueeze(2).expand_as(inter)  # [A,B]
31      area_b = ((box_b[:, :, 2]-box_b[:, :, 0]) *
32                (box_b[:, :, 3]-box_b[:, :, 1])).unsqueeze(1).expand_as(inter)  # [A,B]
33      union = area_a + area_b - inter
34      out = inter / area_a if iscrowd else inter / union
35      return out if use_batch else out.squeeze(0)
<span onclick='openModal()' class='match'>36  def elemwise_box_iou(box_a, box_b):
37      max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])
38      min_xy = torch.max(box_a[:, :2], box_b[:, :2])
39      inter = torch.clamp((max_xy - min_xy), min=0)
40      inter = inter[:, 0] * inter[:, 1]
</span>41      area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])
42      area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])
43      union = area_a + area_b - inter
44      union = torch.clamp(union, min=0.1)
45      return torch.clamp(inter / union, max=1)
46  def mask_iou(masks_a, masks_b, iscrowd=False):
47      masks_a = masks_a.view(masks_a.size(0), -1)
48      masks_b = masks_b.view(masks_b.size(0), -1)
49      intersection = masks_a @ masks_b.t()
50      area_a = masks_a.sum(dim=1).unsqueeze(1)
51      area_b = masks_b.sum(dim=1).unsqueeze(0)
52      return intersection / (area_a + area_b - intersection) if not iscrowd else intersection / area_a
53  def elemwise_mask_iou(masks_a, masks_b):
54      masks_a = masks_a.view(-1, masks_a.size(-1))
55      masks_b = masks_b.view(-1, masks_b.size(-1))
56      intersection = (masks_a * masks_b).sum(dim=0)
57      area_a = masks_a.sum(dim=0)
58      area_b = masks_b.sum(dim=0)
59      return torch.clamp(intersection / torch.clamp(area_a + area_b - intersection, min=0.1), max=1)
60  def change(gt, priors):
61      num_priors = priors.size(0)
62      num_gt     = gt.size(0)
63      gt_w = (gt[:, 2] - gt[:, 0])[:, None].expand(num_gt, num_priors)
64      gt_h = (gt[:, 3] - gt[:, 1])[:, None].expand(num_gt, num_priors)
65      gt_mat =     gt[:, None, :].expand(num_gt, num_priors, 4)
66      pr_mat = priors[None, :, :].expand(num_gt, num_priors, 4)
67      diff = gt_mat - pr_mat
68      diff[:, :, 0] /= gt_w
69      diff[:, :, 2] /= gt_w
70      diff[:, :, 1] /= gt_h
71      diff[:, :, 3] /= gt_h
72      return -torch.sqrt( (diff ** 2).sum(dim=2) )
73  def match(pos_thresh, neg_thresh, truths, priors, labels, crowd_boxes, loc_t, conf_t, idx_t, idx, loc_data):
74      decoded_priors = decode(loc_data, priors, cfg.use_yolo_regressors) if cfg.use_prediction_matching else point_form(priors)
75      overlaps = jaccard(truths, decoded_priors) if not cfg.use_change_matching else change(truths, decoded_priors)
76      best_truth_overlap, best_truth_idx = overlaps.max(0)
77      for _ in range(overlaps.size(0)):
78          best_prior_overlap, best_prior_idx = overlaps.max(1)
79          j = best_prior_overlap.max(0)[1]
80          i = best_prior_idx[j]
81          overlaps[:, i] = -1
82          overlaps[j, :] = -1
83          best_truth_overlap[i] = 2
84          best_truth_idx[i] = j
85      matches = truths[best_truth_idx]            # Shape: [num_priors,4]
86      conf = labels[best_truth_idx] + 1           # Shape: [num_priors]
87      conf[best_truth_overlap < pos_thresh] = -1  # label as neutral
88      conf[best_truth_overlap < neg_thresh] =  0  # label as background
89      if crowd_boxes is not None and cfg.crowd_iou_threshold < 1:
90          crowd_overlaps = jaccard(decoded_priors, crowd_boxes, iscrowd=True)
91          best_crowd_overlap, best_crowd_idx = crowd_overlaps.max(1)
92          conf[(conf <= 0) & (best_crowd_overlap > cfg.crowd_iou_threshold)] = -1
93      loc = encode(matches, priors, cfg.use_yolo_regressors)
94      loc_t[idx]  = loc    # [num_priors,4] encoded offsets to learn
95      conf_t[idx] = conf   # [num_priors] top class label for each prior
96      idx_t[idx]  = best_truth_idx # [num_priors] indices for lookup
97  @torch.jit.script
98  def encode(matched, priors, use_yolo_regressors:bool=False):
99      if use_yolo_regressors:
100          boxes = center_size(matched)
101          loc = torch.cat((
102              boxes[:, :2] - priors[:, :2],
103              torch.log(boxes[:, 2:] / priors[:, 2:])
104          ), 1)
105      else:
106          variances = [0.1, 0.2]
107          g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]
108          g_cxcy /= (variances[0] * priors[:, 2:])
109          g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]
110          g_wh = torch.log(g_wh) / variances[1]
111          loc = torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]
112      return loc
113  @torch.jit.script
114  def decode(loc, priors, use_yolo_regressors:bool=False):
115      if use_yolo_regressors:
116          boxes = torch.cat((
117              loc[:, :2] + priors[:, :2],
118              priors[:, 2:] * torch.exp(loc[:, 2:])
119          ), 1)
120          boxes = point_form(boxes)
121      else:
122          variances = [0.1, 0.2]
123          boxes = torch.cat((
124              priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],
125              priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)
126          boxes[:, :2] -= boxes[:, 2:] / 2
127          boxes[:, 2:] += boxes[:, :2]
128      return boxes
129  def log_sum_exp(x):
130      x_max = x.data.max()
131      return torch.log(torch.sum(torch.exp(x-x_max), 1)) + x_max
132  @torch.jit.script
133  def sanitize_coordinates(_x1, _x2, img_size:int, padding:int=0, cast:bool=True):
134      _x1 = _x1 * img_size
135      _x2 = _x2 * img_size
136      if cast:
137          _x1 = _x1.long()
138          _x2 = _x2.long()
139      x1 = torch.min(_x1, _x2)
140      x2 = torch.max(_x1, _x2)
141      x1 = torch.clamp(x1-padding, min=0)
142      x2 = torch.clamp(x2+padding, max=img_size)
143      return x1, x2
144  @torch.jit.script
145  def crop(masks, boxes, padding:int=1):
146      h, w, n = masks.size()
147      x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding, cast=False)
148      y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding, cast=False)
149      rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)
150      cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)
151      masks_left  = rows >= x1.view(1, 1, -1)
152      masks_right = rows <  x2.view(1, 1, -1)
153      masks_up    = cols >= y1.view(1, 1, -1)
154      masks_down  = cols <  y2.view(1, 1, -1)
155      crop_mask = masks_left * masks_right * masks_up * masks_down
156      return masks * crop_mask.float()
157  def index2d(src, idx):
158      offs = torch.arange(idx.size(0), device=idx.device)[:, None].expand_as(idx)
159      idx  = idx + offs * idx.size(1)
160      return src.view(-1)[idx.view(-1)].view(idx.size())
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-augmentations.py</h3>
            <pre><code>1  import torch
2  from torchvision import transforms
3  import cv2
4  import numpy as np
5  import types
6  from numpy import random
7  from math import sqrt
8  from data import cfg, MEANS, STD
<span onclick='openModal()' class='match'>9  def intersect(box_a, box_b):
10      max_xy = np.minimum(box_a[:, 2:], box_b[2:])
11      min_xy = np.maximum(box_a[:, :2], box_b[:2])
12      inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)
</span>13      return inter[:, 0] * inter[:, 1]
14  def jaccard_numpy(box_a, box_b):
15      inter = intersect(box_a, box_b)
16      area_a = ((box_a[:, 2]-box_a[:, 0]) *
17                (box_a[:, 3]-box_a[:, 1]))  # [A,B]
18      area_b = ((box_b[2]-box_b[0]) *
19                (box_b[3]-box_b[1]))  # [A,B]
20      union = area_a + area_b - inter
21      return inter / union  # [A,B]
22  class Compose(object):
23      def __init__(self, transforms):
24          self.transforms = transforms
25      def __call__(self, img, masks=None, boxes=None, labels=None):
26          for t in self.transforms:
27              img, masks, boxes, labels = t(img, masks, boxes, labels)
28          return img, masks, boxes, labels
29  class Lambda(object):
30      def __init__(self, lambd):
31          assert isinstance(lambd, types.LambdaType)
32          self.lambd = lambd
33      def __call__(self, img, masks=None, boxes=None, labels=None):
34          return self.lambd(img, masks, boxes, labels)
35  class ConvertFromInts(object):
36      def __call__(self, image, masks=None, boxes=None, labels=None):
37          return image.astype(np.float32), masks, boxes, labels
38  class ToAbsoluteCoords(object):
39      def __call__(self, image, masks=None, boxes=None, labels=None):
40          height, width, channels = image.shape
41          boxes[:, 0] *= width
42          boxes[:, 2] *= width
43          boxes[:, 1] *= height
44          boxes[:, 3] *= height
45          return image, masks, boxes, labels
46  class ToPercentCoords(object):
47      def __call__(self, image, masks=None, boxes=None, labels=None):
48          height, width, channels = image.shape
49          boxes[:, 0] /= width
50          boxes[:, 2] /= width
51          boxes[:, 1] /= height
52          boxes[:, 3] /= height
53          return image, masks, boxes, labels
54  class Pad(object):
55      def __init__(self, width, height, mean=MEANS, pad_gt=True):
56          self.mean = mean
57          self.width = width
58          self.height = height
59          self.pad_gt = pad_gt
60      def __call__(self, image, masks, boxes=None, labels=None):
61          im_h, im_w, depth = image.shape
62          expand_image = np.zeros(
63              (self.height, self.width, depth),
64              dtype=image.dtype)
65          expand_image[:, :, :] = self.mean
66          expand_image[:im_h, :im_w] = image
67          if self.pad_gt:
68              expand_masks = np.zeros(
69                  (masks.shape[0], self.height, self.width),
70                  dtype=masks.dtype)
71              expand_masks[:,:im_h,:im_w] = masks
72              masks = expand_masks
73          return expand_image, masks, boxes, labels
74  class Resize(object):
75      @staticmethod
76      def calc_size_preserve_ar(img_w, img_h, max_size):
77          ratio = sqrt(img_w / img_h)
78          w = max_size * ratio
79          h = max_size / ratio
80          return int(w), int(h)
81      def __init__(self, resize_gt=True):
82          self.resize_gt = resize_gt
83          self.max_size = cfg.max_size
84          self.preserve_aspect_ratio = cfg.preserve_aspect_ratio
85      def __call__(self, image, masks, boxes, labels=None):
86          img_h, img_w, _ = image.shape
87          if self.preserve_aspect_ratio:
88              width, height = Resize.calc_size_preserve_ar(img_w, img_h, self.max_size)
89          else:
90              width, height = self.max_size, self.max_size
91          image = cv2.resize(image, (width, height))
92          if self.resize_gt:
93              masks = masks.transpose((1, 2, 0))
94              masks = cv2.resize(masks, (width, height))
95              if len(masks.shape) == 2:
96                  masks = np.expand_dims(masks, 0)
97              else:
98                  masks = masks.transpose((2, 0, 1))
99              boxes[:, [0, 2]] *= (width  / img_w)
100              boxes[:, [1, 3]] *= (height / img_h)
101          w = boxes[:, 2] - boxes[:, 0]
102          h = boxes[:, 3] - boxes[:, 1]
103          keep = (w > cfg.discard_box_width) * (h > cfg.discard_box_height)
104          masks = masks[keep]
105          boxes = boxes[keep]
106          labels['labels'] = labels['labels'][keep]
107          labels['num_crowds'] = (labels['labels'] < 0).sum()
108          return image, masks, boxes, labels
109  class RandomSaturation(object):
110      def __init__(self, lower=0.5, upper=1.5):
111          self.lower = lower
112          self.upper = upper
113          assert self.upper >= self.lower, "contrast upper must be >= lower."
114          assert self.lower >= 0, "contrast lower must be non-negative."
115      def __call__(self, image, masks=None, boxes=None, labels=None):
116          if random.randint(2):
117              image[:, :, 1] *= random.uniform(self.lower, self.upper)
118          return image, masks, boxes, labels
119  class RandomHue(object):
120      def __init__(self, delta=18.0):
121          assert delta >= 0.0 and delta <= 360.0
122          self.delta = delta
123      def __call__(self, image, masks=None, boxes=None, labels=None):
124          if random.randint(2):
125              image[:, :, 0] += random.uniform(-self.delta, self.delta)
126              image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0
127              image[:, :, 0][image[:, :, 0] < 0.0] += 360.0
128          return image, masks, boxes, labels
129  class RandomLightingNoise(object):
130      def __init__(self):
131          self.perms = ((0, 1, 2), (0, 2, 1),
132                        (1, 0, 2), (1, 2, 0),
133                        (2, 0, 1), (2, 1, 0))
134      def __call__(self, image, masks=None, boxes=None, labels=None):
135          return image, masks, boxes, labels
136  class ConvertColor(object):
137      def __init__(self, current='BGR', transform='HSV'):
138          self.transform = transform
139          self.current = current
140      def __call__(self, image, masks=None, boxes=None, labels=None):
141          if self.current == 'BGR' and self.transform == 'HSV':
142              image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
143          elif self.current == 'HSV' and self.transform == 'BGR':
144              image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)
145          else:
146              raise NotImplementedError
147          return image, masks, boxes, labels
148  class RandomContrast(object):
149      def __init__(self, lower=0.5, upper=1.5):
150          self.lower = lower
151          self.upper = upper
152          assert self.upper >= self.lower, "contrast upper must be >= lower."
153          assert self.lower >= 0, "contrast lower must be non-negative."
154      def __call__(self, image, masks=None, boxes=None, labels=None):
155          if random.randint(2):
156              alpha = random.uniform(self.lower, self.upper)
157              image *= alpha
158          return image, masks, boxes, labels
159  class RandomBrightness(object):
160      def __init__(self, delta=32):
161          assert delta >= 0.0
162          assert delta <= 255.0
163          self.delta = delta
164      def __call__(self, image, masks=None, boxes=None, labels=None):
165          if random.randint(2):
166              delta = random.uniform(-self.delta, self.delta)
167              image += delta
168          return image, masks, boxes, labels
169  class ToCV2Image(object):
170      def __call__(self, tensor, masks=None, boxes=None, labels=None):
171          return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), masks, boxes, labels
172  class ToTensor(object):
173      def __call__(self, cvimage, masks=None, boxes=None, labels=None):
174          return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), masks, boxes, labels
175  class RandomSampleCrop(object):
176      def __init__(self):
177          self.sample_options = (
178              None,
179              (0.1, None),
180              (0.3, None),
181              (0.7, None),
182              (0.9, None),
183              (None, None),
184          )
185      def __call__(self, image, masks, boxes=None, labels=None):
186          height, width, _ = image.shape
187          while True:
188              mode = random.choice(self.sample_options)
189              if mode is None:
190                  return image, masks, boxes, labels
191              min_iou, max_iou = mode
192              if min_iou is None:
193                  min_iou = float('-inf')
194              if max_iou is None:
195                  max_iou = float('inf')
196              for _ in range(50):
197                  current_image = image
198                  w = random.uniform(0.3 * width, width)
199                  h = random.uniform(0.3 * height, height)
200                  if h / w < 0.5 or h / w > 2:
201                      continue
202                  left = random.uniform(width - w)
203                  top = random.uniform(height - h)
204                  rect = np.array([int(left), int(top), int(left+w), int(top+h)])
205                  overlap = jaccard_numpy(boxes, rect)
206                  if overlap.min() < min_iou and max_iou < overlap.max():
207                      continue
208                  current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],
209                                                :]
210                  centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0
211                  m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])
212                  m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])
213                  mask = m1 * m2
214                  num_crowds = labels['num_crowds']
215                  crowd_mask = np.zeros(mask.shape, dtype=np.int32)
216                  if num_crowds > 0:
217                      crowd_mask[-num_crowds:] = 1
218                  if not mask.any() or np.sum(1-crowd_mask[mask]) == 0:
219                      continue
220                  current_masks = masks[mask, :, :].copy()
221                  current_boxes = boxes[mask, :].copy()
222                  labels['labels'] = labels['labels'][mask]
223                  current_labels = labels
224                  if num_crowds > 0:
225                      labels['num_crowds'] = np.sum(crowd_mask[mask])
226                  current_boxes[:, :2] = np.maximum(current_boxes[:, :2],
227                                                    rect[:2])
228                  current_boxes[:, :2] -= rect[:2]
229                  current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],
230                                                    rect[2:])
231                  current_boxes[:, 2:] -= rect[:2]
232                  current_masks = current_masks[:, rect[1]:rect[3], rect[0]:rect[2]]
233                  return current_image, current_masks, current_boxes, current_labels
234  class Expand(object):
235      def __init__(self, mean):
236          self.mean = mean
237      def __call__(self, image, masks, boxes, labels):
238          if random.randint(2):
239              return image, masks, boxes, labels
240          height, width, depth = image.shape
241          ratio = random.uniform(1, 4)
242          left = random.uniform(0, width*ratio - width)
243          top = random.uniform(0, height*ratio - height)
244          expand_image = np.zeros(
245              (int(height*ratio), int(width*ratio), depth),
246              dtype=image.dtype)
247          expand_image[:, :, :] = self.mean
248          expand_image[int(top):int(top + height),
249                       int(left):int(left + width)] = image
250          image = expand_image
251          expand_masks = np.zeros(
252              (masks.shape[0], int(height*ratio), int(width*ratio)),
253              dtype=masks.dtype)
254          expand_masks[:,int(top):int(top + height),
255                         int(left):int(left + width)] = masks
256          masks = expand_masks
257          boxes = boxes.copy()
258          boxes[:, :2] += (int(left), int(top))
259          boxes[:, 2:] += (int(left), int(top))
260          return image, masks, boxes, labels
261  class RandomMirror(object):
262      def __call__(self, image, masks, boxes, labels):
263          _, width, _ = image.shape
264          if random.randint(2):
265              image = image[:, ::-1]
266              masks = masks[:, :, ::-1]
267              boxes = boxes.copy()
268              boxes[:, 0::2] = width - boxes[:, 2::-2]
269          return image, masks, boxes, labels
270  class RandomFlip(object):
271      def __call__(self, image, masks, boxes, labels):
272          height , _ , _ = image.shape
273          if random.randint(2):
274              image = image[::-1, :]
275              masks = masks[:, ::-1, :]
276              boxes = boxes.copy()
277              boxes[:, 1::2] = height - boxes[:, 3::-2]
278          return image, masks, boxes, labels
279  class RandomRot90(object):
280      def __call__(self, image, masks, boxes, labels):
281          old_height , old_width , _ = image.shape
282          k = random.randint(4)
283          image = np.rot90(image,k)
284          masks = np.array([np.rot90(mask,k) for mask in masks])
285          boxes = boxes.copy()
286          for _ in range(k):
287              boxes = np.array([[box[1], old_width - 1 - box[2], box[3], old_width - 1 - box[0]] for box in boxes])
288              old_width, old_height = old_height, old_width
289          return image, masks, boxes, labels
290  class SwapChannels(object):
291      def __init__(self, swaps):
292          self.swaps = swaps
293      def __call__(self, image):
294          image = image[:, :, self.swaps]
295          return image
296  class PhotometricDistort(object):
297      def __init__(self):
298          self.pd = [
299              RandomContrast(),
300              ConvertColor(transform='HSV'),
301              RandomSaturation(),
302              RandomHue(),
303              ConvertColor(current='HSV', transform='BGR'),
304              RandomContrast()
305          ]
306          self.rand_brightness = RandomBrightness()
307          self.rand_light_noise = RandomLightingNoise()
308      def __call__(self, image, masks, boxes, labels):
309          im = image.copy()
310          im, masks, boxes, labels = self.rand_brightness(im, masks, boxes, labels)
311          if random.randint(2):
312              distort = Compose(self.pd[:-1])
313          else:
314              distort = Compose(self.pd[1:])
315          im, masks, boxes, labels = distort(im, masks, boxes, labels)
316          return self.rand_light_noise(im, masks, boxes, labels)
317  class PrepareMasks(object):
318      def __init__(self, mask_size, use_gt_bboxes):
319          self.mask_size = mask_size
320          self.use_gt_bboxes = use_gt_bboxes
321      def __call__(self, image, masks, boxes, labels=None):
322          if not self.use_gt_bboxes:
323              return image, masks, boxes, labels
324          height, width, _ = image.shape
325          new_masks = np.zeros((masks.shape[0], self.mask_size ** 2))
326          for i in range(len(masks)):
327              x1, y1, x2, y2 = boxes[i, :]
328              x1 *= width
329              x2 *= width
330              y1 *= height
331              y2 *= height
332              x1, y1, x2, y2 = (int(x1), int(y1), int(x2), int(y2))
333              cropped_mask = masks[i, y1:(y2+1), x1:(x2+1)]
334              scaled_mask = cv2.resize(cropped_mask, (self.mask_size, self.mask_size))
335              new_masks[i, :] = scaled_mask.reshape(1, -1)
336          new_masks[new_masks >  0.5] = 1
337          new_masks[new_masks <= 0.5] = 0
338          return image, new_masks, boxes, labels
339  class BackboneTransform(object):
340      def __init__(self, transform, mean, std, in_channel_order):
341          self.mean = np.array(mean, dtype=np.float32)
342          self.std  = np.array(std,  dtype=np.float32)
343          self.transform = transform
344          self.channel_map = {c: idx for idx, c in enumerate(in_channel_order)}
345          self.channel_permutation = [self.channel_map[c] for c in transform.channel_order]
346      def __call__(self, img, masks=None, boxes=None, labels=None):
347          img = img.astype(np.float32)
348          if self.transform.normalize:
349              img = (img - self.mean) / self.std
350          elif self.transform.subtract_means:
351              img = (img - self.mean)
352          elif self.transform.to_float:
353              img = img / 255
354          img = img[:, :, self.channel_permutation]
355          return img.astype(np.float32), masks, boxes, labels
356  class BaseTransform(object):
357      def __init__(self, mean=MEANS, std=STD):
358          self.augment = Compose([
359              ConvertFromInts(),
360              Resize(resize_gt=False),
361              BackboneTransform(cfg.backbone.transform, mean, std, 'BGR')
362          ])
363      def __call__(self, img, masks=None, boxes=None, labels=None):
364          return self.augment(img, masks, boxes, labels)
365  import torch.nn.functional as F
366  class FastBaseTransform(torch.nn.Module):
367      def __init__(self):
368          super().__init__()
369          self.mean = torch.Tensor(MEANS).float().cuda()[None, :, None, None]
370          self.std  = torch.Tensor( STD ).float().cuda()[None, :, None, None]
371          self.transform = cfg.backbone.transform
372      def forward(self, img):
373          self.mean = self.mean.to(img.device)
374          self.std  = self.std.to(img.device)
375          if cfg.preserve_aspect_ratio:
376              _, h, w, _ = img.size()
377              img_size = Resize.calc_size_preserve_ar(w, h, cfg.max_size)
378              img_size = (img_size[1], img_size[0]) # Pytorch needs h, w
379          else:
380              img_size = (cfg.max_size, cfg.max_size)
381          img = img.permute(0, 3, 1, 2).contiguous()
382          img = F.interpolate(img, img_size, mode='bilinear', align_corners=False)
383          if self.transform.normalize:
384              img = (img - self.mean) / self.std
385          elif self.transform.subtract_means:
386              img = (img - self.mean)
387          elif self.transform.to_float:
388              img = img / 255
389          if self.transform.channel_order != 'RGB':
390              raise NotImplementedError
391          img = img[:, (2, 1, 0), :, :].contiguous()
392          return img
393  def do_nothing(img=None, masks=None, boxes=None, labels=None):
394      return img, masks, boxes, labels
395  def enable_if(condition, obj):
396      return obj if condition else do_nothing
397  class SSDAugmentation(object):
398      def __init__(self, mean=MEANS, std=STD):
399          self.augment = Compose([
400              ConvertFromInts(),
401              ToAbsoluteCoords(),
402              enable_if(cfg.augment_photometric_distort, PhotometricDistort()),
403              enable_if(cfg.augment_expand, Expand(mean)),
404              enable_if(cfg.augment_random_sample_crop, RandomSampleCrop()),
405              enable_if(cfg.augment_random_mirror, RandomMirror()),
406              enable_if(cfg.augment_random_flip, RandomFlip()),
407              enable_if(cfg.augment_random_flip, RandomRot90()),
408              Resize(),
409              enable_if(not cfg.preserve_aspect_ratio, Pad(cfg.max_size, cfg.max_size, mean)),
410              ToPercentCoords(),
411              PrepareMasks(cfg.mask_size, cfg.use_gt_bboxes),
412              BackboneTransform(cfg.backbone.transform, mean, std, 'BGR')
413          ])
414      def __call__(self, img, masks, boxes, labels):
415          return self.augment(img, masks, boxes, labels)
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-box_utils.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-augmentations.py</div>
                </div>
                <div class="column column_space"><pre><code>36  def elemwise_box_iou(box_a, box_b):
37      max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])
38      min_xy = torch.max(box_a[:, :2], box_b[:, :2])
39      inter = torch.clamp((max_xy - min_xy), min=0)
40      inter = inter[:, 0] * inter[:, 1]
</pre></code></div>
                <div class="column column_space"><pre><code>9  def intersect(box_a, box_b):
10      max_xy = np.minimum(box_a[:, 2:], box_b[2:])
11      min_xy = np.maximum(box_a[:, :2], box_b[:2])
12      inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    