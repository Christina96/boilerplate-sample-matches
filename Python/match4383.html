<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for blas.py &amp; test_pool_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for blas.py &amp; test_pool_1.py
      </h3>
<h1 align="center">
        2.3%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>blas.py (2.8068244%)<th>test_pool_1.py (1.9767442%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(1059-1062)<td><a href="#" name="0">(424-426)</a><td align="center"><font color="#ff0000">15</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1142-1145)<td><a href="#" name="1">(427-430)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(159-165)<td><a href="#" name="2">(694-700)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(49-55)<td><a href="#" name="3">(632-638)</a><td align="center"><font color="#cc0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>blas.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 from six import integer_types
3 import theano
4 from theano import Apply, Op
5 from theano.compile import optdb
6 from theano.gof import LocalOptGroup, ParamsType
7 from theano.scalar import bool as bool_t
8 from theano.tensor.basic import as_tensor_variable
9 from theano.tensor.opt import in2out
10 from .basic_ops import (GpuArrayType, CGpuKernelBase,
11                         as_gpuarray_variable, gpu_contiguous, infer_context_name, gpuarray_helper_inc_dir)
12 from .opt_util import inplace_allocempty
13 try:
14     import pygpu
15     from pygpu import blas
16 except ImportError as e:
17     pass
18 class BlasOp(Op):
19     def c_headers(self):
20         return ['&lt;blas_api.h&gt;', '&lt;numpy_compat.h&gt;', '&lt;gpuarray_helper.h&gt;']
21     def c_header_dirs(self):
22         return [pygpu.get_include(), gpuarray_helper_inc_dir()]
23     def c_init_code(self):
24         return ['import_pygpu__blas();']
25 class GpuGemv(BlasOp):
26     params_type = ParamsType(inplace=bool_t)
27     __props__ = ('inplace',)
28     def __init__(self, inplace=False):
29         self.inplace = inplace
30 <a name="3"></a>        if self.inplace:
31             self.destroy_map = {0: [0]}
32     <font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, y, alpha, A, x, beta):
33         ctx_name = infer_context_name(y, A, x)
34         A = as_gpuarray_variable(A, ctx_name)
35         x = as_gpuarray_variable(x, ctx_name)
36         y = as_gpuarray_variable(y, ctx_name)
37         alpha = as_tensor_variable(alpha)
38         beta =</b></font> as_tensor_variable(beta)
39         assert alpha.ndim == 0
40         assert beta.ndim == 0
41         assert A.ndim == 2
42         assert x.ndim == 1
43         assert y.ndim == 1
44         assert A.dtype == x.dtype == y.dtype
45         expected = A.dtype
46         assert theano.scalar.upcast(alpha.dtype,
47                                     beta.dtype, expected) == expected
48         alpha = alpha.astype(expected)
49         beta = beta.astype(expected)
50         return Apply(self, [y, alpha, A, x, beta], [y.type()])
51     def perform(self, node, inputs, out_storage, params):
52         y, alpha, A, x, beta = inputs
53         inplace = params.inplace
54         if inplace and y.strides[0] &lt; 0:
55             inplace = False
56         if A.shape[1] == 0:
57             out_storage[0][0] = pygpu.zeros(y.shape, dtype=y.dtype,
58                                             context=y.context)
59         else:
60             out_storage[0][0] = blas.gemv(alpha, A, x, beta, y,
61                                           overwrite_y=inplace)
62     def c_code(self, node, name, inp, out, sub):
63         vars = dict(out=out[0], y=inp[0], alpha=inp[1], A=inp[2], x=inp[3],
64                     beta=inp[4], fail=sub['fail'], name=name,
65                     params=sub['params'])
66         code = """
67                if (!%(params)s-&gt;inplace || %(y)s-&gt;ga.strides[0] &lt;= 0) {
68                  %(out)s = theano_try_copy(%(out)s, %(y)s);
69                  if (%(out)s == NULL) {
70                    %(fail)s
71                  }
72                } else {
73                  Py_XDECREF(%(out)s);
74                  %(out)s = %(y)s;
75                  Py_INCREF(%(out)s);
76                }
77         return code
78     def c_code_cache_version(self):
79         return (10,)
80 gpugemv_no_inplace = GpuGemv(inplace=False)
81 gpugemv_inplace = GpuGemv(inplace=True)
82 class GpuGemm(BlasOp):
83     params_type = ParamsType(inplace=bool_t)
84     __props__ = ('inplace',)
85     _f16_ok = True
86     def __init__(self, inplace=False):
87         self.inplace = inplace
88 <a name="2"></a>        if self.inplace:
89             self.destroy_map = {0: [0]}
90     <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, C, alpha, A, B, beta):
91         ctx_name = infer_context_name(C, A, B)
92         A = as_gpuarray_variable(A, ctx_name)
93         B = as_gpuarray_variable(B, ctx_name)
94         C = as_gpuarray_variable(C, ctx_name)
95         alpha = as_tensor_variable(alpha)
96         beta =</b></font> as_tensor_variable(beta)
97         if not (A.dtype == B.dtype == C.dtype):
98             raise TypeError(theano.tensor.blas.Gemm.E_mixed,
99                             (A.dtype, B.dtype, C.dtype,
100                              alpha.dtype, beta.dtype))
101         if not A.dtype.startswith('float'):
102             raise TypeError(theano.tensor.blas.Gemm.E_float, (A.dtype))
103         if A.dtype == 'float16':
104             expected = 'float32'
105         else:
106             expected = A.dtype
107         assert theano.scalar.upcast(alpha.dtype,
108                                     beta.dtype, expected) == expected
109         alpha = alpha.astype(expected)
110         beta = beta.astype(expected)
111         assert alpha.ndim == 0
112         assert beta.ndim == 0
113         assert A.ndim == 2
114         assert B.ndim == 2
115         assert C.ndim == 2
116         return Apply(self, [C, alpha, A, B, beta], [C.type()])
117     def perform(self, node, inputs, outputs, params):
118         C, alpha, A, B, beta = inputs
119         inplace = params.inplace
120         if inplace and not C.flags.forc:
121             inplace = False
122         outputs[0][0] = blas.gemm(alpha, A, B, beta, C,
123                                   overwrite_c=inplace)
124     def c_code(self, node, name, inp, out, sub):
125         vars = dict(out=out[0], C=inp[0], alpha=inp[1], A=inp[2], B=inp[3],
126                     beta=inp[4], fail=sub['fail'], name=name,
127                     params=sub['params'])
128         code = """
129                if (!%(params)s-&gt;inplace || !GpuArray_ISONESEGMENT(&amp;%(C)s-&gt;ga)) {
130                  %(out)s = theano_try_copy(%(out)s, %(C)s);
131                  if (%(out)s == NULL) {
132                    %(fail)s
133                  }
134                } else {
135                  Py_XDECREF(%(out)s);
136                  %(out)s = %(C)s;
137                  Py_INCREF(%(out)s);
138                }
139                if (pygpu_blas_rgemm(cb_no_trans, cb_no_trans,
140                                     ((dtype_%(alpha)s *)PyArray_DATA(%(alpha)s))[0],
141                                     %(A)s, %(B)s,
142                                     ((dtype_%(beta)s *)PyArray_DATA(%(beta)s))[0],
143                                     %(out)s, 0) == -1) {
144                  %(fail)s
145                }
146     Ger on the GPU.
147         return code
148     def c_code_cache_version(self):
149         return (5,)
150 gpuger_no_inplace = GpuGer(inplace=False)
151 gpuger_inplace = GpuGer(inplace=True)
152 class GpuDot22(BlasOp):
153     _f16_ok = True
154     __props__ = ()
155     def make_node(self, x, y):
156         ctx_name = infer_context_name(x, y)
157         x = as_gpuarray_variable(x, ctx_name)
158         y = as_gpuarray_variable(y, ctx_name)
159         assert x.ndim == 2
160         assert y.ndim == 2
161         assert x.dtype == y.dtype
162         otype = x.type.clone(
163             broadcastable=(x.type.broadcastable[0], y.type.broadcastable[1]))
164         return Apply(self, [x, y], [otype()])
165     def perform(self, node, inputs, outputs):
166         x, y = inputs
167         out = pygpu.empty((x.shape[0], y.shape[1]), dtype=x.dtype,
168                           context=x.context)
169         outputs[0][0] = blas.gemm(1., x, y, 0., out,
170                                   overwrite_c=True)
171     def c_code(self, node, name, inputs, outputs, sub):
172         dtype = node.inputs[0].dtype
173         typecode = pygpu.gpuarray.dtype_to_typecode(dtype)
174         vars = dict(A=inputs[0], B=inputs[1], dtype=dtype, out=outputs[0],
175                     typecode=typecode,
176                     fail=sub['fail'], name=name)
177         code = """
178         double one = 1.;
179         double zero = 0.;
180         size_t dims[] = {0, 0};
181         dims[0] = PyGpuArray_DIMS(%(A)s)[0];
182         dims[1] = PyGpuArray_DIMS(%(B)s)[1];
183         if (theano_prep_output(&amp;%(out)s, 2, dims, %(typecode)s, GA_C_ORDER,
184                                %(A)s-&gt;context)) {
185             %(fail)s
186         }
187         if (pygpu_blas_rgemm(cb_no_trans, cb_no_trans,
188                              one,
189                              %(A)s, %(B)s,
190                              zero,
191                              %(out)s, 0) == -1) {
192             %(fail)s
193         }
194         return code
195     def c_code_cache_version(self):
196         return (4,)
197 gpugemmbatch_no_inplace = GpuGemmBatch(inplace=False)
198 gpugemmbatch_inplace = GpuGemmBatch(inplace=True)
199 class BaseGpuCorrMM(CGpuKernelBase):
200     check_broadcast = False
201     __props__ = ('border_mode', 'subsample', 'filter_dilation', 'num_groups', 'unshared')
202     _f16_ok = True
203     def __init__(self, border_mode="valid", subsample=(1, 1),
204                  filter_dilation=(1, 1), num_groups=1, unshared=False):
205         if isinstance(border_mode, integer_types):
206             if border_mode &lt; 0:
207                 raise ValueError(
208                     'invalid border_mode {}, which must be a '
209                     'non-negative integer'.format(border_mode))
210             border_mode = ((border_mode, border_mode),) * 2
211         elif isinstance(border_mode, tuple):
212             if len(border_mode) != 2:
213                 raise ValueError(
214                     'invalid border_mode {} which must be a '
215                     'tuple of length 2'.format(border_mode))
216             border = ()
217             for mode in border_mode:
218                 if isinstance(mode, tuple) and len(mode) == 2 and \
219                         min(mode) &gt;= 0:
220                     border += ((int(mode[0]), int(mode[1])),)
221                 elif mode &gt;= 0:
222                     border += ((int(mode), int(mode)),)
223                 else:
224                     raise ValueError(
225                         'invalid border mode {}. The tuple can only contain '
226                         'integers or tuples of length 2'.format(border_mode))
227             border_mode = border
228         elif border_mode not in ('valid', 'full', 'half'):
229             raise ValueError(
230                 'invalid border_mode {}, which must be either '
231                 '"valid", "full", "half", an integer or a tuple '
232                 'of length 2'.format(border_mode))
233         self.border_mode = border_mode
234         if len(subsample) != 2:
235             raise ValueError("subsample must have two elements")
236         if len(filter_dilation) != 2:
237             raise ValueError("filter_dilation must have two elements")
238         self.subsample = tuple(subsample)
239         self.filter_dilation = tuple(filter_dilation)
240         if num_groups &lt; 1:
241             raise ValueError("Number of groups should be greater than 0")
242         self.num_groups = num_groups
243         CGpuKernelBase.__init__(self, ['c_code/corr_gemm.c'])
244         self.unshared = unshared
245     @property
246     def pad(self):
247         if self.border_mode != 'valid':
248             return self.border_mode
249         return ((0, 0),) * 2
250     def __str__(self):
251         return '%s{%s, %s, %s, %s, %s}' % (
252             self.__class__.__name__,
253             self.border_mode,
254             str(self.subsample),
255             str(self.filter_dilation),
256             str(self.num_groups),
257             str(self.unshared))
258     def __setstate__(self, d):
259         self.__dict__.update(d)
260         if not hasattr(self, 'num_groups'):
261             self.num_groups = 1
262     def flops(self, inp, outp):
263         inputs, filters = inp
264         outputs, = outp
265         assert inputs[1] == (filters[1] * self.num_groups)
266         flops = filters[2] * filters[3] * 2
267         flops *= outputs[2] * outputs[3]
268         flops *= inputs[1] * filters[0] * inputs[0] / self.num_groups
269         return flops
270     def c_headers(self):
271         return ["&lt;gpuarray/array.h&gt;", "&lt;gpuarray/blas.h&gt;", "gpuarray_helper.h"]
272     def c_header_dirs(self):
273         return [gpuarray_helper_inc_dir()]
274     def c_code_cache_version(self):
275         return (12,)
276     def c_code_helper(self, bottom, weights, top, direction, sub, height=None, width=None):
277         dH, dW = self.subsample
278         dilH, dilW = self.filter_dilation
279         numgroups = self.num_groups
280         unshared = int(self.unshared)
281         if self.border_mode == "half":
282             padH_l = padH_r = padW_l = padW_r = -1
283         elif self.border_mode == "full":
284             padH_l = padH_r = padW_l = padW_r = -2
285         elif isinstance(self.border_mode, tuple):
286             (padH_l, padH_r), (padW_l, padW_r) = self.border_mode
287         else:
288             assert self.border_mode == "valid"
289             padH_l = padH_r = padW_l = padW_r = 0
290         if direction == "forward":
291             direction = 0
292             out = top
293         elif direction == "backprop weights":
294             direction = 1
295             out = weights
296         elif direction == "backprop inputs":
297             direction = 2
298             out = bottom
299         else:
300             raise ValueError("direction must be one of 'forward', "
301                              "'backprop weights', 'backprop inputs'")
302         if height:
303             height = '(*(npy_int*)(PyArray_DATA(%s)))' % height
304         else:
305             if ((direction != 0) and (dH != 1)) or ((direction == 1) and (padH_l == -1 or padH_r == -1)):
306                 raise ValueError("height must be given for backprop with vertical sampling or pad='half'")
307             height = '-1'
308         if width:
309             width = '(*(npy_int*)(PyArray_DATA(%s)))' % width
310         else:
311             if ((direction != 0) and (dW != 1)) or ((direction == 1) and (padW_l == -1 or padW_r == -1)):
312                 raise ValueError("width must be given for backprop with horizontal sampling or pad='half'")
313             width = '-1'
314         sub = sub.copy()
315         sub.update(locals())
316         return """
317     // Mandatory args
318     int direction = %(direction)s;  // forward, bprop weights, bprop inputs
319     // Optional args
320     size_t dH = %(dH)s;
321     size_t dW = %(dW)s;
322     size_t dilH = %(dilH)s;
323     size_t dilW = %(dilW)s;
324     int padH_l = %(padH_l)s;
325     int padH_r = %(padH_r)s;
326     int padW_l = %(padW_l)s;
327     int padW_r = %(padW_r)s;
328     int numgroups = %(numgroups)s;
329     int unshared = %(unshared)s;
330     PyGpuArrayObject * bottom = %(bottom)s;
331     PyGpuArrayObject * weights = %(weights)s;
332     PyGpuArrayObject * top = %(top)s;
333     PyGpuArrayObject * out2 = NULL;
334     int wdim, odim;
335     wdim = unshared ? 6 : 4;
336     odim = 4; //Can be set to 6 later for unshared backprop wrt weights
337     // Obtain or infer kernel width and height
338     // (we need to know it early to be able to handle auto-padding)
339     size_t kH, kW, dil_kH, dil_kW;
340     if (direction != 1) {
341         // weight is an input variable, we can just read its shape
342         kH = PyGpuArray_DIMS(weights)[wdim-2];
343         kW = PyGpuArray_DIMS(weights)[wdim-1];
344     }
345     else {
346         if (%(height)s != -1) {
347             // kernel height is specified (perhaps vertical subsampling or half padding)
348             kH = %(height)s;
349         }
350         else if (padH_l == -2 || padH_r == -2) {
351             // vertical full padding, we can infer the kernel height
352             kH = (2 - PyGpuArray_DIMS(bottom)[2] + (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1;
353         }
354         else {
355             // explicit padding, we can infer the kernel height
356             kH = (PyGpuArray_DIMS(bottom)[2] + padH_l + padH_r - (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1 ;
357         }
358         if (%(width)s != -1) {
359             kW = %(width)s;
360         }
361         else if (padW_l == -2 || padW_r == -2) {
362             kW = (2 - PyGpuArray_DIMS(bottom)[3] + (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
363         }
364         else {
365             kW = (PyGpuArray_DIMS(bottom)[3] + padW_l + padW_r - (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
366         }
367     }
368     // Implicit dilated kernel size
369     dil_kH = (kH - 1) * dilH + 1;
370     dil_kW = (kW - 1) * dilW + 1;
371     // Auto-padding if requested
372     if (padH_l == -1 || padH_r == -1) {  // vertical half padding
373         padH_l = padH_r = dil_kH / 2;
374     }
375     else if (padH_l == -2 || padH_r == -2) {  // vertical full padding
376         padH_l = padH_r = dil_kH - 1;
377     }
378     else if (padH_l &lt; 0 || padH_r &lt; 0) {
379         PyErr_SetString(PyExc_ValueError, "BaseGpuCorrMM: padH must be &gt;= -2");
380         %(fail)s
381     }
382     if (padW_l == -1 || padW_r == -1) {  // horizontal half padding
383         padW_l = padW_r = dil_kW / 2;
384     }
385     else if (padW_l == -2 || padW_r == -2) {  // horizontal full padding
386         padW_l = padW_r = dil_kW - 1;
387     }
388     else if (padW_l &lt; 0 || padW_r &lt; 0) {
389         PyErr_SetString(PyExc_ValueError, "BaseGpuCorrMM: padW must be &gt;= -2");
390         %(fail)s
391     }
392     // Infer output shape and type
393     // The inferred shape can be negative.
394     long long out_dim[6];
395     size_t out_dim_size[6];
396     out_dim[4] = out_dim[5] = 0; //Only used for unshared backprop wrt weights
397     out_dim_size[4] = out_dim_size[5] = 0; //Same
398     int out_typecode;
399     PyGpuContextObject *out_context;
400     switch(direction) {
401     case 0:  // forward pass
402         // output is top: (batchsize, num_filters, height, width)
403         // height and width: top = (bottom + pad_l + pad_r - ((weight-1)*dil + 1)) / sample + 1
404         out_dim[0] = PyGpuArray_DIMS(bottom)[0];
405         out_dim[1] = PyGpuArray_DIMS(weights)[0];
406         out_dim[2] = (PyGpuArray_DIMS(bottom)[2] + padH_l + padH_r - ((PyGpuArray_DIMS(weights)[wdim-2]-1)*dilH + 1)) / dH + 1;
407         out_dim[3] = (PyGpuArray_DIMS(bottom)[3] + padW_l + padW_r - ((PyGpuArray_DIMS(weights)[wdim-1]-1)*dilW + 1)) / dW + 1;
408         out_typecode = bottom-&gt;ga.typecode;
409         out_context = bottom-&gt;context;
410         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
411         {
412             if (unshared) {
413                 PyErr_Format(PyExc_ValueError,
414                              "GpuCorrMM: impossible output shape\\n"
415                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
416                              "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld x %%ld\\n"
417                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
418                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
419                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
420                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
421                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
422                              PyGpuArray_DIMS(weights)[4], PyGpuArray_DIMS(weights)[5],
423                              out_dim[0], out_dim[1], out_dim[2], out_dim[3]);
424                 %(fail)s
425             }
426             else {
427                 PyErr_Format(PyExc_ValueError,
428                              "GpuCorrMM: impossible output shape\\n"
429                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
430                              "  weights shape: %%ld x %%ld x %%ld x %%ld\\n"
431                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
432                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
433                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
434                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
435                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
436                              out_dim[0], out_dim[1], out_dim[2], out_dim[3]);
437                 %(fail)s
438             }
439         }
440         break;
441     case 1:  // backprop wrt. weights
442         // output is weights: (num_filters, num_channels, height, width) or
443         // (num_filters, top_height, top_width, num_channels, height, width) -&gt; for unshared
444         // height and width: weights = (bottom + 2*pad - (top - 1) * sample - 1) / dil + 1
445         out_dim[0] = PyGpuArray_DIMS(top)[1];
446         if (unshared){
447             odim = 6;
448             out_dim[1] = PyGpuArray_DIMS(top)[2];
449             out_dim[2] = PyGpuArray_DIMS(top)[3];
450         }
451         out_dim[wdim-3] = PyGpuArray_DIMS(bottom)[1] / numgroups;
452         out_dim[wdim-2] = kH;  // already inferred further above
453         out_dim[wdim-1] = kW;  // how convenient
454         out_typecode = top-&gt;ga.typecode;
455         out_context = top-&gt;context;
456         if (unshared) {
457             if (out_dim[0] &lt; 0 || out_dim[1] &lt;= 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt; 0
458                     || out_dim[4] &lt;= 0 || out_dim[5] &lt;= 0){
459                 PyErr_Format(PyExc_ValueError,
460                              "GpuCorrMM backprop wrt. weights: impossible output shape\\n"
461                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
462                              "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld x %%ld\\n"
463                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
464                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
465                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
466                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
467                              out_dim[4], out_dim[5],
468                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
469                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
470                 %(fail)s
471             }
472         }
473         else {
474              if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
475             {
476                 PyErr_Format(PyExc_ValueError,
477                              "GpuCorrMM backprop wrt. weights: impossible output shape\\n"
478                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
479                              "  weights shape: %%ld x %%ld x %%ld x %%ld\\n"
480                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
481                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
482                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
483                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
484                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
485                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
486                 %(fail)s
487             }
488         }
489         break;
490     case 2:  // backprop wrt. inputs
491         // output is bottom: (batchsize, num_channels, height, width)
492         // height and width: bottom = (top - 1) * sample + (weights-1)*dil + 1 - 2*pad
493         out_dim[0] = PyGpuArray_DIMS(top)[0];
494         out_dim[1] = PyGpuArray_DIMS(weights)[wdim-3] * numgroups;
495         out_dim[2] = (%(height)s != -1) ? %(height)s : (PyGpuArray_DIMS(top)[2] - 1) * dH + (PyGpuArray_DIMS(weights)[wdim-2]-1)*dilH + 1 - padH_l - padH_r;
496         out_dim[3] = (%(width)s != -1) ? %(width)s : (PyGpuArray_DIMS(top)[3] - 1) * dW + (PyGpuArray_DIMS(weights)[wdim-1]-1)*dilW + 1 - padW_l - padW_r;
497         out_typecode = top-&gt;ga.typecode;
498         out_context = top-&gt;context;
499         if (unshared) {
500             if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
501             {
502                 PyErr_Format(PyExc_ValueError,
503                              "GpuCorrMM backprop wrt. inputs: impossible output shape\\n"
504                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
505                              "  weight shape: %%ld x %%ld x %%ld x %%ld x %%ld x %%ld\\n"
506                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
507                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
508                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
509                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
510                              PyGpuArray_DIMS(weights)[4], PyGpuArray_DIMS(weights)[5],
511                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
512                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
513                 %(fail)s
514             }
515         }
516         else {
517             if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
518             {
519                 PyErr_Format(PyExc_ValueError,
520                              "GpuCorrMM backprop wrt. inputs: impossible output shape\\n"
521                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
522                              "  weight shape: %%ld x %%ld x %%ld x %%ld\\n"
523                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
524                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
525                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
526                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
527                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
528                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
529                 %(fail)s
530             }
531         }
532         break;
533     default:
534         PyErr_SetString(PyExc_ValueError, "BaseGpuCorrMM: direction must be 0, 1, or 2\\n");
535         %(fail)s
536     }
537     out_dim_size[0] = (size_t)out_dim[0];
538     out_dim_size[1] = (size_t)out_dim[1];
539     out_dim_size[2] = (size_t)out_dim[2];
540     out_dim_size[3] = (size_t)out_dim[3];
541     if (odim == 6) {
542         out_dim_size[4] = (size_t)out_dim[4];
543         out_dim_size[5] = (size_t)out_dim[5];
544     }
545     // Prepare output array
546     if (theano_prep_output(&amp;%(out)s, odim, out_dim_size, out_typecode, GA_C_ORDER, out_context) != 0)
547     {
548         if (odim == 4) {
549             PyErr_Format(PyExc_RuntimeError,
550                     "BaseGpuCorrMM: Failed to allocate output of %%lld x %%lld x %%lld x %%lld",
551                     out_dim[0], out_dim[1], out_dim[2], out_dim[3]);
552         }
553         if (odim == 6) {
554             PyErr_Format(PyExc_RuntimeError,
555                     "BaseGpuCorrMM: Failed to allocate output of %%lld x %%lld x %%lld x %%lld %%lld %%lld",
556                     out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4], out_dim[5]);
557         }
558         %(fail)s
559     }
560     if (!GpuArray_IS_C_CONTIGUOUS(&amp;%(out)s-&gt;ga)) {
561         PyErr_SetString(PyExc_ValueError, "Only contiguous outputs are supported.");
562         %(fail)s
563     }
564     // Call GPU code
565     out2 = corrMM(%(bottom)s, %(weights)s, %(top)s, direction, dH, dW, dilH, dilW,
566                 padH_l, padH_r, padW_l, padW_r, numgroups, unshared);
567     if (out2==NULL){
568        %(fail)s
569     }
570     assert (out2 == %(out)s);
571     GPU correlation implementation using Matrix Multiplication.
572     Parameters
573     ----------
574     border_mode
575         The width of a border of implicit zeros to pad the
576         input with. Must be a tuple with 2 elements giving the numbers of rows
577         and columns to pad on each side, or a single integer to pad the same
578         on all sides, or a string shortcut setting the padding at runtime:
579         ``'valid'`` for ``(0, 0)`` (valid convolution, no padding), ``'full'``
580         for ``(kernel_rows - 1, kernel_columns - 1)`` (full convolution),
581         ``'half'`` for ``(kernel_rows // 2, kernel_columns // 2)`` (same
582         convolution for odd-sized kernels).
583         If it is a tuple containing 2 pairs of integers, then these specify
584         the padding to be applied on each side ((left, right), (top, bottom)).
585         Otherwise, each width is applied twice, once per side (left and right,
586         top and bottom).
587     subsample
588         The subsample operation applied to each output image.
589         Should be a tuple with 2 elements.
590         `(sv, sh)` is equivalent to `GpuCorrMM(...)(...)[:,:,::sv, ::sh]`,
591         but faster.
592         Set to `(1, 1)` to disable subsampling.
593     filter_dilation
594         The filter dilation operation applied to each input image.
595         Should be a tuple with 2 elements.
596         Set to `(1, 1)` to disable filter dilation.
597     num_groups
598         The number of distinct groups the image and kernel must be
599         divided into.
600         should be an int
601         set to 1 to disable grouped convolution
602     unshared
603         Perform unshared correlation (default: False)
604     Notes
605     -----
606     Currently, the Op requires the inputs, filters and outputs to be
607     C-contiguous. Use :func:`gpu_contiguous
608     &lt;theano.gpuarray.basic_ops.gpu_contiguous&gt;` on these arguments
609     if needed.
610     You can either enable the Theano flag `optimizer_including=conv_gemm`
611     to automatically replace all convolution operations with `GpuCorrMM`
612     or one of its gradients, or you can use it as a replacement for
613     :func:`conv2d &lt;theano.tensor.nnet.conv.conv2d&gt;`, called as
614     `GpuCorrMM(subsample=...)(image, filters)`. The latter is currently
615     faster, but note that it computes a correlation -- if you need to
616     compute a convolution, flip the filters as `filters[:,:,::-1,::-1]`.
617     Gradient wrt. filters for `GpuCorrMM`.
618     Notes
619     -----
620     You will not want to use this directly, but rely on Theano's automatic
621     differentiation or graph optimization to use it as needed.
622             broadcastable = [<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>topgrad.type.broadcastable[1], False, False,
623                              img.type.broadcastable[1], False, False]
624         else:
625             broadcastable = [topgrad.type.broadcastable[1], img.type.broadcastable[</b></font>1],
626                              False, False]
627         return Apply(self, [img, topgrad] + height_width, [GpuArrayType(dtype=img.dtype,
628                                                                         context_name=ctx_name,
629                                                                         broadcastable=broadcastable)()])
630     def c_code(self, node, nodename, inp, out_, sub):
631         bottom, top = inp[:2]
632         height, width = inp[2:] or (None, None)
633         weights, = out_
634         direction = "backprop weights"
635         return super(GpuCorrMM_gradWeights, self).c_code_helper(bottom, weights, top, direction, sub, height, width)
636     def grad(self, inp, grads):
637         bottom, top = inp[:2]
638         weights, = grads
639         weights = gpu_contiguous(weights)
640         d_bottom = GpuCorrMM_gradInputs(self.border_mode,
641                                         self.subsample,
642                                         self.filter_dilation,
643                                         self.num_groups,
644                                         self.unshared)(weights,
645                                                        top,
646                                                        bottom.shape[-2:])
647         d_top = GpuCorrMM(
648             self.border_mode, self.subsample, self.filter_dilation, self.num_groups, self.unshared)(bottom, weights)
649         d_height_width = (
650             theano.gradient.DisconnectedType()(),
651             ) * 2 if len(inp) == 4 else ()
652         return (d_bottom, d_top) + d_height_width
653     def connection_pattern(self, node):
654         if node.nin == 2:
655             return [[1], [1]]
656         else:
657             return [[1], [1], [0], [0]]  # no connection to height, width
658 class GpuCorrMM_gradInputs(BaseGpuCorrMM):
659     def __init__(self, border_mode="valid",
660                  subsample=(1, 1),
661                  filter_dilation=(1, 1),
662                  num_groups=1,
663                  unshared=False):
664         super(GpuCorrMM_gradInputs, self).__init__(border_mode, subsample,
665                                                    filter_dilation, num_groups,
666                                                    unshared)
667     def make_node(self, kern, topgrad, shape=None):
668         ctx_name = infer_context_name(kern, topgrad)
669         kern = as_gpuarray_variable(kern, ctx_name)
670         topgrad = as_gpuarray_variable(topgrad, ctx_name)
671         if self.unshared:
672             if kern.type.ndim != 6:
673                 raise TypeError('kern must be 6D tensor')
674         else:
675             if kern.type.ndim != 4:
676                 raise TypeError('kern must be 4D tensor')
677         if topgrad.type.ndim != 4:
678             raise TypeError('topgrad must be 4D tensor')
679         if shape is None:
680             if self.subsample != (1, 1):
681                 raise ValueError('shape must be given if subsample != (1, 1)')
682             height_width = []
683         else:
684             height_width = [shape[0], shape[1]]
685             assert shape[0].ndim == 0
686 <a name="1"></a>            assert shape[1].ndim == 0
687         if self.num_groups &gt; 1:
688             broadcastable = [<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>topgrad.type.broadcastable[0], False,
689                              False, False]
690         else:
691             broadcastable = [topgrad.type.broadcastable[0], kern.type.broadcastable[</b></font>-3],
692                              False, False]
693         return Apply(self, [kern, topgrad] + height_width, [GpuArrayType(dtype=topgrad.dtype,
694                                                                          context_name=ctx_name,
695                                                                          broadcastable=broadcastable)()])
696     def c_code(self, node, nodename, inp, out_, sub):
697         weights, top = inp[:2]
698         height, width = inp[2:] or (None, None)
699         bottom, = out_
700         direction = "backprop inputs"
701         return super(GpuCorrMM_gradInputs, self).c_code_helper(bottom, weights, top, direction, sub, height, width)
702     def grad(self, inp, grads):
703         weights, top = inp[:2]
704         bottom, = grads
705         bottom = gpu_contiguous(bottom)
706         d_weights = GpuCorrMM_gradWeights(self.border_mode,
707                                           self.subsample,
708                                           self.filter_dilation,
709                                           self.num_groups,
710                                           self.unshared)(bottom,
711                                                          top,
712                                                          weights.shape[-2:])
713         d_top = GpuCorrMM(self.border_mode,
714                           self.subsample,
715                           self.filter_dilation,
716                           self.num_groups,
717                           self.unshared)(bottom, weights)
718         d_height_width = (
719             theano.gradient.DisconnectedType()(),
720             ) * 2 if len(inp) == 4 else ()
721         return (d_weights, d_top) + d_height_width
722     def connection_pattern(self, node):
723         if node.nin == 2:
724             return [[1], [1]]
725         else:
726             return [[1], [1], [0], [0]]  # no connection to height, width
727 class BaseGpuCorr3dMM(CGpuKernelBase):
728     check_broadcast = False
729     __props__ = ('border_mode', 'subsample', 'filter_dilation', 'num_groups')
730     _f16_ok = True
731     def __init__(self, border_mode="valid", subsample=(1, 1, 1),
732                  filter_dilation=(1, 1, 1), num_groups=1):
733         if isinstance(border_mode, integer_types):
734             border_mode = (border_mode, border_mode, border_mode)
735         if isinstance(border_mode, tuple):
736             pad_h, pad_w, pad_d = map(int, border_mode)
737             border_mode = (pad_h, pad_w, pad_d)
738         if not ((isinstance(border_mode, tuple) and min(border_mode) &gt;= 0) or
739                 border_mode in ('valid', 'full', 'half')):
740             raise ValueError(
741                 'invalid border_mode {}, which must be either '
742                 '"valid", "full", "half", an integer or a tuple of'
743                 ' three integers'.format(border_mode))
744         self.border_mode = border_mode
745         if len(subsample) != 3:
746             raise ValueError("subsample must have three elements")
747         if len(filter_dilation) != 3:
748             raise ValueError("filter_dilation must have three elements")
749         self.subsample = tuple(subsample)
750         self.filter_dilation = tuple(filter_dilation)
751         if num_groups &lt; 1:
752             raise ValueError("Number of groups should be greater than 0")
753         self.num_groups = num_groups
754         CGpuKernelBase.__init__(self, ['c_code/corr3d_gemm.c'])
755     @property
756     def pad(self):
757         if self.border_mode != 'valid':
758             return self.border_mode
759         return (0, 0, 0)
760     def __str__(self):
761         return '%s{%s, %s, %s, %s}' % (
762             self.__class__.__name__,
763             self.border_mode,
764             str(self.subsample),
765             str(self.filter_dilation),
766             str(self.num_groups))
767     def __setstate__(self, d):
768         self.__dict__.update(d)
769         if not hasattr(self, 'num_groups'):
770             self.num_groups = 1
771     def flops(self, inp, outp):
772         inputs, filters = inp
773         outputs, = outp
774         assert inputs[1] == (filters[1] * self.num_groups)
775         flops = filters[2] * filters[3] * filters[4] * 2
776         flops *= outputs[2] * outputs[3] * outputs[4]
777         flops *= inputs[1] * filters[0] * inputs[0] / self.num_groups
778         return flops
779     def c_headers(self):
780         return ["&lt;gpuarray/array.h&gt;", "&lt;gpuarray/blas.h&gt;", "gpuarray_helper.h"]
781     def c_header_dirs(self):
782         return [gpuarray_helper_inc_dir()]
783     def c_code_cache_version(self):
784         return (8,)
785     def c_code_helper(self, bottom, weights, top, direction, sub,
786                       height=None, width=None, depth=None):
787         dH, dW, dD = self.subsample
788         dilH, dilW, dilD = self.filter_dilation
789         numgroups = self.num_groups
790         if self.border_mode == "half":
791             padH = padW = padD = -1
792         elif self.border_mode == "full":
793             padH = padW = padD = -2
794         elif isinstance(self.border_mode, tuple):
795             padH, padW, padD = self.border_mode
796         else:
797             assert self.border_mode == "valid"
798             padH = padW = padD = 0
799         if direction == "forward":
800             direction = 0
801             out = top
802         elif direction == "backprop weights":
803             direction = 1
804             out = weights
805         elif direction == "backprop inputs":
806             direction = 2
807             out = bottom
808         else:
809             raise ValueError("direction must be one of 'forward', "
810                              "'backprop weights', 'backprop inputs'")
811         if height:
812             height = '(*(npy_int*)(PyArray_DATA(%s)))' % height
813         else:
814             if ((direction != 0) and (dH != 1)) or ((direction == 1) and (padH == -1)):
815                 raise ValueError("height must be given for backprop with vertical sampling or pad='half'")
816             height = '-1'
817         if width:
818             width = '(*(npy_int*)(PyArray_DATA(%s)))' % width
819         else:
820             if ((direction != 0) and (dW != 1)) or ((direction == 1) and (padW == -1)):
821                 raise ValueError("width must be given for backprop with horizontal sampling or pad='half'")
822             width = '-1'
823         if depth:
824             depth = '(*(npy_int*)(PyArray_DATA(%s)))' % depth
825         else:
826             if ((direction != 0) and (dD != 1)) or ((direction == 1) and (padD == -1)):
827                 raise ValueError("depth must be given for backprop with horizontal sampling or pad='half'")
828             depth = '-1'
829         sub = sub.copy()
830         sub.update(locals())
831         return """
832     // Mandatory args
833     int direction = %(direction)s;  // forward, bprop weights, bprop inputs
834     // Optional args
835     size_t dH = %(dH)s;
836     size_t dW = %(dW)s;
837     size_t dD = %(dD)s;
838     size_t dilH = %(dilH)s;
839     size_t dilW = %(dilW)s;
840     size_t dilD = %(dilD)s;
841     int padH = %(padH)s;
842     int padW = %(padW)s;
843     int padD = %(padD)s;
844     int numgroups = %(numgroups)s;
845     PyGpuArrayObject * bottom = %(bottom)s;
846     PyGpuArrayObject * weights = %(weights)s;
847     PyGpuArrayObject * top = %(top)s;
848     PyGpuArrayObject * out2 = NULL;
849     // Obtain or infer kernel height, width and depth
850     // (we need to know it early to be able to handle auto-padding)
851     size_t kH, kW, kD, dil_kH, dil_kW, dil_kD;
852     if (direction != 1) {
853         // weight is an input variable, we can just read its shape
854         kH = PyGpuArray_DIMS(weights)[2];
855         kW = PyGpuArray_DIMS(weights)[3];
856         kD = PyGpuArray_DIMS(weights)[4];
857     }
858     else {
859         if (%(height)s != -1) {
860             // kernel height is specified (perhaps vertical subsampling or half padding)
861             kH = %(height)s;
862         }
863         else if (padH == -2) {
864             // vertical full padding, we can infer the kernel height
865             kH = (2 - PyGpuArray_DIMS(bottom)[2] + (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1;
866         }
867         else {
868             // explicit padding, we can infer the kernel height
869             kH = (PyGpuArray_DIMS(bottom)[2] + 2*padH - (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1 ;
870         }
871         if (%(width)s != -1) {
872             kW = %(width)s;
873         }
874         else if (padW == -2) {
875             kW = (2 - PyGpuArray_DIMS(bottom)[3] + (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
876         }
877         else {
878             kW = (PyGpuArray_DIMS(bottom)[3] + 2*padW - (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
879         }
880         if (%(depth)s != -1) {
881             kD = %(depth)s;
882         }
883         else if (padD == -2) {
884             kD = (2 - PyGpuArray_DIMS(bottom)[4] + (PyGpuArray_DIMS(top)[4] - 1) * dD - 1) / dilD + 1;
885         }
886         else {
887             kD = (PyGpuArray_DIMS(bottom)[4] + 2*padD - (PyGpuArray_DIMS(top)[4] - 1) * dD - 1) / dilD + 1;
888         }
889     }
890     // Implicit dilated kernel size
891     dil_kH = (kH - 1) * dilH + 1;
892     dil_kW = (kW - 1) * dilW + 1;
893     dil_kD = (kD - 1) * dilD + 1;
894     // Auto-padding if requested
895     if (padH == -1) {  // vertical half padding
896         padH = dil_kH / 2;
897     }
898     else if (padH == -2) {  // vertical full padding
899         padH = dil_kH - 1;
900     }
901     else if (padH &lt; 0) {
902         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: padH must be &gt;= -2");
903         %(fail)s
904     }
905     if (padW == -1) {  // horizontal half padding
906         padW = dil_kW / 2;
907     }
908     else if (padW == -2) {  // horizontal full padding
909         padW = dil_kW - 1;
910     }
911     else if (padW &lt; 0) {
912         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: padW must be &gt;= -2");
913         %(fail)s
914     }
915     if (padD == -1) {  // depth half padding
916         padD = dil_kD / 2;
917     }
918     else if (padD == -2) {  // depth full padding
919         padD = dil_kD - 1;
920     }
921     else if (padD &lt; 0) {
922         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: padD must be &gt;= -2");
923         %(fail)s
924     }
925     // Infer output shape and type
926     // The inferred shape can be negative.
927     long long out_dim[5];
928     size_t out_dim_size[5];
929     int out_typecode;
930     PyGpuContextObject *out_context;
931     switch(direction) {
932     case 0:  // forward pass
933         // output is top: (batchsize, num_filters, height, width, depth)
934         // height, width and depth: top = (bottom + 2*pad - ((weight-1)*dil + 1)) / sample + 1
935         out_dim[0] = PyGpuArray_DIMS(bottom)[0];
936         out_dim[1] = PyGpuArray_DIMS(weights)[0];
937         out_dim[2] = (PyGpuArray_DIMS(bottom)[2] + 2*padH - ((PyGpuArray_DIMS(weights)[2]-1)*dilH + 1)) / dH + 1;
938         out_dim[3] = (PyGpuArray_DIMS(bottom)[3] + 2*padW - ((PyGpuArray_DIMS(weights)[3]-1)*dilW + 1)) / dW + 1;
939         out_dim[4] = (PyGpuArray_DIMS(bottom)[4] + 2*padD - ((PyGpuArray_DIMS(weights)[4]-1)*dilD + 1)) / dD + 1;
940         out_typecode = bottom-&gt;ga.typecode;
941         out_context = bottom-&gt;context;
942         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0 || out_dim[4] &lt;= 0)
943         {
944             PyErr_Format(PyExc_ValueError,
945                          "GpuCorr3dMM: impossible output shape\\n"
946                          "  bottom shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
947                          "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
948                          "  top shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n",
949                          PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
950                          PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
951                          PyGpuArray_DIMS(bottom)[4],
952                          PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
953                          PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
954                          PyGpuArray_DIMS(weights)[4],
955                          out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4]);
956             %(fail)s
957         }
958         break;
959     case 1:  // backprop wrt. weights
960         // output is weights: (num_filters, num_channels, height, width, depth)
961         // height, width and depth: weights = (bottom + 2*pad - (top - 1) * sample - 1) / dil + 1
962         out_dim[0] = PyGpuArray_DIMS(top)[1];
963         out_dim[1] = PyGpuArray_DIMS(bottom)[1] / numgroups;
964         out_dim[2] = kH;  // already inferred further above
965         out_dim[3] = kW;  // how convenient
966         out_dim[4] = kD;
967         out_typecode = top-&gt;ga.typecode;
968         out_context = top-&gt;context;
969         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0 || out_dim[4] &lt;= 0)
970         {
971             PyErr_Format(PyExc_ValueError,
972                          "GpuCorr3dMM backprop wrt. weights: impossible output shape\\n"
973                          "  bottom shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
974                          "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
975                          "  top shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n",
976                          PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
977                          PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
978                          PyGpuArray_DIMS(bottom)[4],
979                          out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4],
980                          PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
981                          PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3],
982                          PyGpuArray_DIMS(top)[4]);
983             %(fail)s
984         }
985         break;
986     case 2:  // backprop wrt. inputs
987         // output is bottom: (batchsize, num_channels, height, width, depth)
988         // height, width and depth: bottom = (top - 1) * sample + (weights-1)*dil + 1 - 2*pad
989         out_dim[0] = PyGpuArray_DIMS(top)[0];
990         out_dim[1] = PyGpuArray_DIMS(weights)[1] * numgroups;
991         out_dim[2] = (%(height)s != -1) ? %(height)s : (PyGpuArray_DIMS(top)[2] - 1) * dH + (PyGpuArray_DIMS(weights)[2]-1)*dilH + 1 - 2*padH;
992         out_dim[3] = (%(width)s != -1) ? %(width)s : (PyGpuArray_DIMS(top)[3] - 1) * dW + (PyGpuArray_DIMS(weights)[3]-1)*dilW + 1 - 2*padW;
993         out_dim[4] = (%(depth)s != -1) ? %(depth)s : (PyGpuArray_DIMS(top)[4] - 1) * dD + (PyGpuArray_DIMS(weights)[4]-1)*dilD + 1 - 2*padD;
994         out_typecode = top-&gt;ga.typecode;
995         out_context = top-&gt;context;
996         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0 || out_dim[4] &lt;= 0)
997         {
998             PyErr_Format(PyExc_ValueError,
999                          "GpuCorr3dMM backprop wrt. inputs: impossible output shape\\n"
1000                          "  bottom shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1001                          "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1002                          "  top shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n",
1003                          out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4],
1004                          PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
1005                          PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
1006                          PyGpuArray_DIMS(weights)[4],
1007                          PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
1008                          PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3],
1009                          PyGpuArray_DIMS(top)[4]);
1010             %(fail)s
1011         }
1012         break;
1013     default:
1014         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: direction must be 0, 1, or 2\\n");
1015         %(fail)s
1016     }
1017     out_dim_size[0] = (size_t)out_dim[0];
1018     out_dim_size[1] = (size_t)out_dim[1];
1019     out_dim_size[2] = (size_t)out_dim[2];
1020     out_dim_size[3] = (size_t)out_dim[3];
1021     out_dim_size[4] = (size_t)out_dim[4];
1022     // Prepare output array
1023     if (theano_prep_output(&amp;%(out)s, 5, out_dim_size, out_typecode, GA_C_ORDER, out_context) != 0)
1024     {
1025         PyErr_Format(PyExc_RuntimeError,
1026                 "BaseGpuCorrMM: Failed to allocate output of %%lld x %%lld x %%lld x %%lld x %%lld",
1027                 out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4]);
1028         %(fail)s
1029     }
1030     if (!GpuArray_IS_C_CONTIGUOUS(&amp;%(out)s-&gt;ga)) {
1031         PyErr_SetString(PyExc_ValueError, "Only contiguous outputs are supported.");
1032         %(fail)s
1033     }
1034     // Call GPU code
1035     out2 = corr3dMM(%(bottom)s, %(weights)s, %(top)s, direction,
1036                     dH, dW, dD, dilH, dilW, dilD, padH, padW, padD, numgroups);
1037     if (out2==NULL){
1038        %(fail)s
1039     }
1040     assert (out2 == %(out)s);
1041     GPU correlation implementation using Matrix Multiplication.
1042     Parameters
1043     ----------
1044     border_mode
1045         The width of a border of implicit zeros to pad the
1046         input with. Must be a tuple with 3 elements giving the width of
1047         the padding on each side, or a single integer to pad the same
1048         on all sides, or a string shortcut setting the padding at runtime:
1049         ``'valid'`` for ``(0, 0, 0)`` (valid convolution, no padding), ``'full'``
1050         for ``(kernel_rows - 1, kernel_columns - 1, kernel_depth - 1)``
1051         (full convolution), ``'half'`` for ``(kernel_rows // 2,
1052         kernel_columns // 2, kernel_depth // 2)`` (same convolution for
1053         odd-sized kernels). Note that the three widths are each
1054         applied twice, once per side (left and right, top and bottom, front
1055         and back).
1056     subsample
1057         The subsample operation applied to each output image. Should be a tuple
1058         with 3 elements. `(sv, sh, sl)` is equivalent to
1059         `GpuCorrMM(...)(...)[:,:,::sv, ::sh, ::sl]`, but faster.
1060         Set to `(1, 1, 1)` to disable subsampling.
1061     filter_dilation
1062         The filter dilation operation applied to each input image.
1063         Should be a tuple with 3 elements.
1064         Set to `(1, 1, 1)` to disable filter dilation.
1065     num_groups
1066         The number of distinct groups the image and kernel must be
1067         divided into.
1068         should be an int
1069         set to 1 to disable grouped convolution
1070     Notes
1071     -----
1072     Currently, the Op requires the inputs, filters and outputs to be
1073     C-contiguous. Use :func:`gpu_contiguous
1074     &lt;theano.gpuarray.basic_ops.gpu_contiguous&gt;` on these arguments
1075     if needed.
1076     You can either enable the Theano flag `optimizer_including=conv_gemm`
1077     to automatically replace all convolution operations with `GpuCorr3dMM`
1078     or one of its gradients, or you can use it as a replacement for
1079     :func:`conv2d &lt;theano.tensor.nnet.conv.conv2d&gt;`, called as
1080     `GpuCorr3dMM(subsample=...)(image, filters)`. The latter is currently
1081     faster, but note that it computes a correlation -- if you need to
1082     compute a convolution, flip the filters as `filters[:,:,::-1,::-1,::-1]`.
1083     Gradient wrt. filters for `GpuCorr3dMM`.
1084     Notes
1085     -----
1086     You will not want to use this directly, but rely on Theano's automatic
1087     differentiation or graph optimization to use it as needed.
1088     Gradient wrt. inputs for `GpuCorr3dMM`.
1089     Notes
1090     -----
1091     You will not want to use this directly, but rely on Theano's automatic
1092     differentiation or graph optimization to use it as needed.
1093 <a name="1"></a>        maxpoolshps = ((<font color="#0000ff"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>5, 3), (5, 3), (5, 3), (5, 5), (3, 2), (7, 7), (9, 9))
1094         stridesizes = ((3, 2), (7, 5), (10, 6), (1, 1),
1095                        (2, 3), (10</b></font>, 10), (1, 1))
1096         imvsizs = ((16, 16), (16, 16), (16, 16), (<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>8, 5),
1097                    (8, 5), (8, 5), (8, 5))
1098         outputshps = ((4, 10, 4, 7), (4, 10, 5, 8), (4, 10, 2, 3),
1099                       (4, 10, 3, 4), (4, 10, 2, 3), (4</b></font>, 10, 2, 3),
1100                       (4, 10, 4, 1), (4, 10, 4, 1), (4, 10, 3, 2),
1101                       (4, 10, 4, 2), (4, 10, 1, 0), (4, 10, 1, 1),
1102                       (4, 10, 0, 0), (4, 10, 1, 1))
1103         images = tensor.dtensor4()
1104         for indx in np.arange(len(maxpoolshps)):
1105             imvsize = imvsizs[indx]
1106             imval = rng.rand(4, 10, imvsize[0], imvsize[1])
1107             stride = stridesizes[indx]
1108             maxpoolshp = maxpoolshps[indx]
1109             for ignore_border, mode in product([True, False],
1110                                                ['max', 'sum',
1111                                                 'average_inc_pad',
1112                                                 'average_exc_pad']):
1113                 indx_out = indx * 2
1114                 if not ignore_border:
1115                     indx_out += 1
1116                 outputshp = outputshps[indx_out]
1117                 numpy_output_val = \
1118                     self.numpy_max_pool_2d_stride(imval, maxpoolshp,
1119                                                   ignore_border, stride, mode)
1120                 assert numpy_output_val.shape == outputshp, (
1121                     "outshape is %s, calculated shape is %s"
1122                     % (outputshp, numpy_output_val.shape))
1123                 maxpool_op = \
1124                     Pool(ignore_border=ignore_border,
1125                          ndim=len(maxpoolshp), mode=mode)(
1126                         images, maxpoolshp, stride)
1127                 f = function([images], maxpool_op)
1128                 output_val = f(imval)
1129                 utt.assert_allclose(output_val, numpy_output_val)
1130     def test_DownsampleFactorMaxPaddingStride(self):
1131         ignore_border = True  # padding does not support ignore_border=False
1132         rng = np.random.RandomState(utt.fetch_seed())
1133         examples = (
1134             ((3,), (2,), (2,), (5,)),
1135             ((3,), (2,), (2,), (4, 5)),
1136             ((3,), (2,), (2,), (4, 2, 5, 5)),
1137             ((3, 3), (2, 2), (2, 2), (4, 2, 5, 5)),
1138             ((4, 4), (2, 2), (1, 2), (4, 2, 5, 5)),
1139             ((3, 4), (1, 1), (2, 1), (4, 2, 5, 6)),
1140             ((4, 3), (1, 2), (0, 0), (4, 2, 6, 5)),
1141             ((2, 2), (2, 2), (1, 1), (4, 2, 5, 5)),
1142             ((4, 3, 2), (1, 2, 2), (0, 2, 1), (4, 6, 6, 5)),
1143             ((4, 3, 2), (1, 2, 2), (0, 2, 1), (4, 2, 6, 5, 5)),
1144         )
1145         for example, mode in product(examples,
1146                                      ['max', 'sum', 'average_inc_pad',
1147                                       'average_exc_pad']):
1148             (maxpoolshp, stridesize, padsize, inputsize) = example
1149             imval = rng.rand(*inputsize) - 0.5
1150             images = theano.shared(imval)
1151             numpy_output_val = self.numpy_max_pool_nd_stride_pad(
1152                 imval, maxpoolshp, ignore_border,
1153                 stridesize, padsize, mode)
1154             maxpool_op = Pool(
1155                 ndim=len(maxpoolshp),
1156                 ignore_border=ignore_border,
1157                 mode=mode
1158                 )(images, maxpoolshp, stridesize, padsize)
1159             f = function([], maxpool_op)
1160             output_val = f()
1161             utt.assert_allclose(output_val, numpy_output_val)
1162     def test_DownsampleFactorMaxPaddingStride_grad(self):
1163         rng = np.random.RandomState(utt.fetch_seed())
1164         examples = (
1165             ((10,), (5,), (3,), (2,)),
1166             ((10,), (5,), (3,), (2, 2)),
1167             ((10,), (5,), (3,), (1, 1, 2)),
1168             ((10, 10), (5, 3), (3, 2), (1, 1, 2, 2)),
1169             ((10, 5), (3, 5), (2, 3), (1, 1, 2, 1)),
1170             ((5, 5), (3, 3), (3, 3), (1, 1, 2, 2)),
1171             ((5, 5, 5), (3, 3, 3), (3, 3, 3), (1, 1, 2, 2, 2)),
1172         )
1173         for mode in ['max', 'sum']:
1174             for example in examples:
1175                 (maxpoolshp, stridesize, padsize, inputsize) = example
1176                 imval = rng.rand(*inputsize) * 10.0
1177                 def mp(input):
1178                     return Pool(
1179                         ndim=len(maxpoolshp),
1180                         ignore_border=True,
1181                         mode=mode,
1182                         )(input, maxpoolshp, stridesize, padsize)
1183                 utt.verify_grad(mp, [imval], rng=rng)
1184     def test_DownsampleFactorMax_grad(self):
1185         rng = np.random.RandomState(utt.fetch_seed())
1186         examples = (
1187             ((2,), (3,)),
1188             ((2,), (2, 3)),
1189             ((2,), (2, 3, 3)),
1190             ((1, 1), (2, 3, 3, 4)),
1191             ((3, 2), (2, 3, 3, 4)),
1192             ((2, 3), (2, 3, 3, 4)),
1193             ((1, 1, 1), (2, 3, 3)),
1194             ((3, 2, 2), (2, 3, 3, 4)),
1195             ((2, 2, 3), (2, 3, 3, 4, 4)),
1196         )
1197         for example, ignore_border, mode in product(examples,
1198                                                     [True, False],
1199                                                     ['max',
1200                                                      'sum',
1201                                                      'average_inc_pad',
1202                                                      'average_exc_pad']):
1203             (maxpoolshp, inputsize) = example
1204             imval = rng.rand(*inputsize) * 10.0
1205             def mp(input):
1206                 return Pool(ndim=len(maxpoolshp),
1207                             ignore_border=ignore_border,
1208                             mode=mode)(input, maxpoolshp)
1209             utt.verify_grad(mp, [imval], rng=rng)
1210     pool_grad_stride_examples = (
1211         ((1,), (1,), (16,)),
1212         ((1,), (3,), (1, 16)),
1213         ((1,), (5,), (1, 2, 16)),
1214         ((2,), (1,), (16,)),
1215         ((2,), (3,), (1, 16)),
1216         ((2,), (5,), (1, 2, 16)),
1217         ((1, 1), (1, 1), (1, 2, 16, 16)),
1218         ((1, 1), (3, 3), (1, 2, 16, 16)),
1219         ((1, 1), (5, 7), (1, 2, 16, 16)),
1220         ((3, 3), (1, 1), (1, 2, 16, 16)),
1221         ((3, 3), (3, 3), (1, 2, 16, 16)),
1222         ((3, 3), (5, 7), (1, 2, 16, 16)),
1223         ((5, 3), (1, 1), (1, 2, 16, 16)),
1224         ((5, 3), (3, 3), (1, 2, 16, 16)),
1225         ((5, 3), (5, 7), (1, 2, 16, 16)),
1226         ((5, 1, 2), (1, 1, 1), (16, 3, 16)),
1227         ((5, 1, 2), (3, 1, 2), (1, 16, 3, 16)),
1228         ((5, 1, 2), (5, 1, 4), (1, 2, 16, 3, 16)),
1229         ((5, 3), (3, 2), (1, 2, 16, 16)),
1230         ((5, 3), (7, 5), (1, 2, 16, 16)),
1231         ((5, 3), (10, 6), (1, 2, 16, 16)),
1232         ((5, 5), (1, 1), (1, 2, 8, 5)),
1233         ((3, 2), (2, 3), (1, 2, 8, 5)),
1234         ((7, 7), (10, 10), (1, 2, 8, 5)),
1235         ((9, 9), (1, 1), (1, 2, 8, 5)),
1236     )
1237     @parameterized.expand(product(pool_grad_stride_examples,
1238                                   [True, False],
1239                                   ['max',
1240                                    'sum',
1241                                    'average_inc_pad',
1242                                    'average_exc_pad']),
1243                           testcase_func_name=utt.custom_name_func)
1244     def test_DownsampleFactorMax_grad_stride(self, example, ignore_border, mode):
1245         rng = np.random.RandomState(utt.fetch_seed())
1246         (maxpoolshp, stridesize, inputsize) = example
1247         imval = rng.rand(*inputsize)
1248         def mp(input):
1249             return Pool(ndim=len(maxpoolshp),
1250                         ignore_border=ignore_border,
1251                         mode=mode)(input, maxpoolshp, stridesize)
1252         utt.verify_grad(mp, [imval], rng=rng)
1253     def test_DownsampleFactorMaxGrad_grad(self):
1254         rng = np.random.RandomState(utt.fetch_seed())
1255         examples = (
1256             ((2,), (2,)),
1257             ((2,), (2, 3)),
1258             ((1, 1), (2, 3, 3, 4)),
1259             ((3, 2), (2, 3, 3, 4)),
1260             ((2, 3), (2, 3, 3, 4)),
1261             ((1, 1, 1), (2, 3, 3, 4)),
1262             ((3, 2, 2), (2, 3, 3, 4)),
1263             ((2, 3, 2), (2, 3, 3, 4)),
1264             ((2, 2, 3), (2, 3, 3, 4)),
1265             ((2, 2, 3), (2, 1, 3, 3, 4)),
1266         )
1267         for (maxpoolshp, inputsize) in examples:
1268             imval = rng.rand(*inputsize) * 10.0
1269             for ignore_border in [True, False]:
1270                 grad_shape = Pool.out_shape(
1271 <a name="3"></a>                    imval.shape, maxpoolshp, ndim=len(maxpoolshp), ignore_border=ignore_border)
1272                 grad_val = rng.rand(*grad_shape) * 10.0
1273                 <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def mp(input, grad):
1274                     out = Pool(
1275                         ndim=len(maxpoolshp),
1276                         ignore_border=ignore_border)(input, maxpoolshp)
1277                     grad_op = MaxPoolGrad(
1278                         ndim=len(maxpoolshp),
1279                         ignore_border=</b></font>ignore_border)
1280                     return grad_op(input, out, grad, maxpoolshp)
1281                 utt.verify_grad(mp, [imval, grad_val], rng=rng)
1282     def test_AveragePoolGrad_grad(self):
1283         rng = np.random.RandomState(utt.fetch_seed())
1284         examples = (
1285             ((2,), (2,)),
1286             ((2,), (2, 3)),
1287             ((1, 1), (2, 3, 3, 4)),
1288             ((3, 2), (2, 3, 3, 4)),
1289             ((2, 3), (2, 3, 3, 4)),
1290             ((3, 2, 2), (2, 3, 3, 4)),
1291             ((2, 2, 3), (2, 3, 3, 4)),
1292         )
1293         for (avgpoolshp, inputsize) in examples:
1294             imval = rng.rand(*inputsize) * 10.0
1295             for ignore_border in [True, False]:
1296                 for mode in ['sum', 'average_inc_pad', 'average_exc_pad']:
1297                     grad_shape = Pool.out_shape(
1298                         imval.shape, avgpoolshp, ndim=len(avgpoolshp),
1299                         ignore_border=ignore_border)
1300                     grad_val = rng.rand(*grad_shape) * 10.0
1301                     def mp(input, grad):
1302                         grad_op = AveragePoolGrad(
1303                             ndim=len(avgpoolshp),
1304                             ignore_border=ignore_border, mode=mode)
1305                         return grad_op(input, grad, avgpoolshp)
1306                     utt.verify_grad(mp, [imval, grad_val], rng=rng)
1307     @parameterized.expand(product(pool_grad_stride_examples,
1308                                   [True, False]),
1309                           testcase_func_name=utt.custom_name_func)
1310     def test_DownsampleFactorMaxGrad_grad_stride(self, example, ignore_border):
1311         rng = np.random.RandomState(utt.fetch_seed())
1312         (maxpoolshp, stride, inputsize) = example
1313         imval = rng.rand(*inputsize)
1314         grad_shape = Pool.out_shape(
1315             imval.shape, maxpoolshp, ndim=len(maxpoolshp),
1316             ignore_border=ignore_border, stride=stride)
1317 <a name="2"></a>        if np.prod(grad_shape) != 0:
1318             grad_val = rng.rand(*grad_shape)
1319             <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def mp(input, grad):
1320                 out = Pool(
1321                     ndim=len(maxpoolshp),
1322                     ignore_border=ignore_border)(input, maxpoolshp, stride)
1323                 grad_op = MaxPoolGrad(
1324                     ndim=len(maxpoolshp),
1325                     ignore_border=</b></font>ignore_border)
1326                 return grad_op(input, out, grad, maxpoolshp, stride)
1327                 utt.verify_grad(mp, [imval, grad_val], rng=rng)
1328     @parameterized.expand(product(pool_grad_stride_examples,
1329                                   [True, False],
1330                                   ['sum',
1331                                    'average_inc_pad',
1332                                    'average_exc_pad']),
1333                           testcase_func_name=utt.custom_name_func)
1334     def test_AveragePoolGrad_grad_stride(self, example, ignore_border, mode):
1335         rng = np.random.RandomState(utt.fetch_seed())
1336         (avgpoolshp, stride, inputsize) = example
1337         imval = rng.rand(*inputsize)
1338         grad_shape = Pool.out_shape(
1339             imval.shape, avgpoolshp,
1340             ndim=len(avgpoolshp),
1341             ignore_border=ignore_border, stride=stride)
1342         if np.prod(grad_shape) != 0:
1343             grad_val = rng.rand(*grad_shape)
1344             def mp(input, grad):
1345                 grad_op = AveragePoolGrad(
1346                     ndim=len(avgpoolshp),
1347                     ignore_border=ignore_border,
1348                     mode=mode)
1349                 return grad_op(input, grad, avgpoolshp, stride)
1350             utt.verify_grad(mp, [imval, grad_val], rng=rng)
1351     def test_DownsampleFactorMaxPaddingStride_grad_grad(self):
1352         rng = np.random.RandomState(utt.fetch_seed())
1353         examples = (
1354             ((3,), (2,), (2,), (10,)),
1355             ((3,), (2,), (2,), (2, 10,)),
1356             ((3,), (2,), (2,), (2, 1, 10,)),
1357             ((5, 3), (3, 2), (2, 2), (1, 1, 10, 10)),
1358             ((3, 5), (2, 3), (2, 1), (1, 1, 10, 5)),
1359             ((5, 3, 3), (3, 2, 2), (2, 2, 2), (1, 1, 10, 5, 5)),
1360             ((3, 3, 5), (2, 2, 3), (2, 2, 1), (1, 1, 5, 5, 10)),
1361         )
1362         for (maxpoolshp, stridesize, padsize, inputsize) in examples:
1363             imval = rng.rand(*inputsize) * 10.0
1364             grad_shape = Pool.out_shape(imval.shape,
1365                                         maxpoolshp,
1366                                         ndim=len(maxpoolshp),
1367                                         stride=stridesize,
1368                                         ignore_border=True,
1369                                         pad=padsize)
1370             grad_val = rng.rand(*grad_shape) * 10.0
1371             def mp(input, grad):
1372                 out = Pool(
1373                     ndim=len(maxpoolshp),
1374                     ignore_border=True,
1375                     )(input, maxpoolshp, stridesize, padsize)
1376                 grad_op = MaxPoolGrad(ndim=len(maxpoolshp),
1377                                       ignore_border=True)
1378                 return grad_op(input, out, grad, maxpoolshp, stridesize, padsize)
1379             utt.verify_grad(mp, [imval, grad_val], rng=rng)
1380     def test_AveragePoolPaddingStride_grad_grad(self):
1381         rng = np.random.RandomState(utt.fetch_seed())
1382         examples = (
1383             ((3,), (2,), (2,), (10,)),
1384             ((3,), (2,), (2,), (2, 10,)),
1385             ((3,), (2,), (2,), (2, 1, 10,)),
1386             ((5, 3), (3, 2), (2, 2), (1, 1, 10, 10)),
1387             ((3, 5), (2, 3), (2, 1), (1, 1, 10, 5)),
1388             ((5, 3, 2), (3, 2, 1), (2, 2, 2), (1, 1, 10, 5, 5)),
1389         )
1390         for (avgpoolshp, stridesize, padsize, inputsize) in examples:
1391             imval = rng.rand(*inputsize) * 10.0
1392             for mode in ['sum', 'average_inc_pad']:
1393                 grad_shape = Pool.out_shape(imval.shape,
1394                                             avgpoolshp,
1395                                             ndim=len(avgpoolshp),
1396                                             stride=stridesize,
1397                                             ignore_border=True,
1398                                             pad=padsize)
1399                 grad_val = rng.rand(*grad_shape) * 10.0
1400                 def mp(input, grad):
1401                     grad_op = AveragePoolGrad(ndim=len(avgpoolshp),
1402                                               ignore_border=True,
1403                                               mode=mode)
1404                     return grad_op(input, grad, avgpoolshp, stridesize, padsize)
1405                 utt.verify_grad(mp, [imval, grad_val], rng=rng)
1406     def test_DownsampleFactorMax_hessian(self):
1407         x_vec = tensor.vector('x')
1408         z = tensor.dot(x_vec.dimshuffle(0, 'x'),
1409                        x_vec.dimshuffle('x', 0))
1410         y = pool_2d(input=z, ws=(2, 2), ignore_border=True)
1411         C = tensor.exp(tensor.sum(y))
1412         grad_hess = tensor.hessian(cost=C, wrt=x_vec)
1413         fn_hess = function(inputs=[x_vec], outputs=grad_hess)
1414         assert np.allclose(fn_hess([1, 2]), [[0., 0.], [0., 982.7667]])
1415     def test_DownsampleFactorMaxGradGrad_grad(self):
1416         rng = np.random.RandomState(utt.fetch_seed())
1417         examples = (
1418             ((3,), (2,), (2,), (10,)),
1419             ((3,), (2,), (2,), (2, 10,)),
1420             ((3,), (2,), (2,), (2, 1, 10,)),
1421             ((5, 3), (3, 2), (2, 2), (1, 1, 10, 10)),
1422             ((3, 5), (2, 3), (2, 1), (1, 1, 10, 5)),
1423             ((3, 3), (3, 3), (2, 2), (1, 1, 5, 5)),
1424             ((5, 3, 3), (3, 2, 2), (2, 2, 2), (1, 1, 10, 5, 5)),
1425             ((3, 3, 5), (2, 2, 3), (2, 2, 1), (1, 1, 5, 5, 10)),
1426         )
1427         for (maxpoolshp, stridesize, padsize, inputsize) in examples:
1428             imval1 = rng.rand(*inputsize) * 10.0
1429             imval2 = rng.rand(*inputsize) * 10.0
1430             def mp(input1, input2):
1431                 op1 = Pool(ndim=len(maxpoolshp), ignore_border=True)
1432                 pooled_out = op1(input1, maxpoolshp, stridesize, padsize)
1433                 op2 = DownsampleFactorMaxGradGrad(
1434                     ndim=len(maxpoolshp),
1435                     ignore_border=True)
1436                 out = op2(input1, pooled_out, input2, maxpoolshp, stridesize, padsize)
1437                 return out
1438             utt.verify_grad(mp, [imval1, imval2], rng=rng)
1439     def test_max_pool_2d_2D(self):
1440         rng = np.random.RandomState(utt.fetch_seed())
1441         maxpoolshps = ((1, 1), (3, 2))
1442         imval = rng.rand(4, 5)
1443         images = tensor.dmatrix()
1444         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
1445                                                        [True, False],
1446                                                        ['max', 'sum',
1447                                                         'average_inc_pad',
1448                                                         'average_exc_pad']):
1449                 numpy_output_val = self.numpy_max_pool_2d(imval, maxpoolshp,
1450                                                           ignore_border,
1451                                                           mode=mode)
1452                 output = pool_2d(images, maxpoolshp, ignore_border,
1453                                  mode=mode)
1454                 output_val = function([images], output)(imval)
1455                 utt.assert_allclose(output_val, numpy_output_val)
1456                 def mp(input):
1457                     return pool_2d(input, maxpoolshp, ignore_border,
1458                                    mode=mode)
1459                 utt.verify_grad(mp, [imval], rng=rng)
1460     def test_max_pool_3d_3D(self):
1461         rng = np.random.RandomState(utt.fetch_seed())
1462         maxpoolshps = ((1, 1, 1), (3, 2, 1))
1463         imval = rng.rand(4, 5, 6)
1464         images = tensor.dtensor3()
1465         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
1466                                                        [True, False],
1467                                                        ['max', 'sum',
1468                                                         'average_inc_pad',
1469                                                         'average_exc_pad']):
1470                 numpy_output_val = self.numpy_max_pool_nd(imval, maxpoolshp,
1471                                                           ignore_border,
1472                                                           mode=mode)
1473                 output = pool_3d(images, maxpoolshp, ignore_border,
1474                                  mode=mode)
1475                 output_val = function([images], output)(imval)
1476                 utt.assert_allclose(output_val, numpy_output_val)
1477                 def mp(input):
1478                     return pool_3d(input, maxpoolshp, ignore_border,
1479                                    mode=mode)
1480                 utt.verify_grad(mp, [imval], rng=rng)
1481     def test_max_pool_3d_3D_deprecated_interface(self):
1482         rng = np.random.RandomState(utt.fetch_seed())
1483         maxpoolshps = ((1, 1, 1), (3, 2, 1))
1484         imval = rng.rand(4, 5, 6)
1485         images = tensor.dtensor3()
1486         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
1487                                                        [True, False],
1488                                                        ['max', 'sum',
1489                                                         'average_inc_pad',
1490                                                         'average_exc_pad']):
1491                 numpy_output_val = self.numpy_max_pool_nd(imval, maxpoolshp,
1492                                                           ignore_border,
1493                                                           mode=mode)
1494                 output = pool_3d(input=images,
1495                                  ds=maxpoolshp,
1496                                  ignore_border=ignore_border,
1497                                  st=maxpoolshp,
1498                                  padding=(0, 0, 0),
1499                                  mode=mode)
1500                 output_val = function([images], output)(imval)
1501                 utt.assert_allclose(output_val, numpy_output_val)
1502                 def mp(input):
1503                     return pool_3d(input, maxpoolshp, ignore_border,
1504                                    mode=mode)
1505     def test_max_pool_2d_2D_same_size(self):
1506         rng = np.random.RandomState(utt.fetch_seed())
1507         test_input_array = np.array([[[
1508             [1., 2., 3., 4.],
1509             [5., 6., 7., 8.]
1510         ]]]).astype(theano.config.floatX)
1511         test_answer_array = np.array([[[
1512             [0., 0., 0., 0.],
1513             [0., 6., 0., 8.]
1514         ]]]).astype(theano.config.floatX)
1515         input = tensor.tensor4(name='input')
1516         patch_size = (2, 2)
1517         op = max_pool_2d_same_size(input, patch_size)
1518         op_output = function([input], op)(test_input_array)
1519         utt.assert_allclose(op_output, test_answer_array)
1520         def mp(input):
1521             return max_pool_2d_same_size(input, patch_size)
1522         utt.verify_grad(mp, [test_input_array], rng=rng)
1523     def test_max_pool_2d_3D(self):
1524         rng = np.random.RandomState(utt.fetch_seed())
1525         maxpoolshps = [(1, 2)]
1526         imval = rng.rand(2, 3, 4)
1527         images = tensor.dtensor3()
1528         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
1529                                                        [True, False],
1530                                                        ['max', 'sum',
1531                                                         'average_inc_pad',
1532                                                         'average_exc_pad']):
1533                 numpy_output_val = self.numpy_max_pool_2d(imval, maxpoolshp,
1534                                                           ignore_border,
1535                                                           mode)
1536                 output = pool_2d(images, maxpoolshp, ignore_border,
1537                                  mode=mode)
1538                 output_val = function([images], output)(imval)
1539                 utt.assert_allclose(output_val, numpy_output_val)
1540     def test_max_pool_2d_6D(self):
1541         rng = np.random.RandomState(utt.fetch_seed())
1542         maxpoolshps = [(3, 2)]
1543         imval = rng.rand(2, 1, 1, 1, 3, 4)
1544         images = tensor.TensorType('float64', [False] * 6)()
1545         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
1546                                                        [True, False],
1547                                                        ['max', 'sum',
1548                                                         'average_inc_pad',
1549                                                         'average_exc_pad']):
1550                 numpy_output_val = self.numpy_max_pool_2d(imval, maxpoolshp,
1551                                                           ignore_border,
1552                                                           mode=mode)
1553                 output = pool_2d(images, maxpoolshp, ignore_border,
1554                                  mode=mode)
1555                 output_val = function([images], output)(imval)
1556                 utt.assert_allclose(output_val, numpy_output_val)
1557     def test_infer_shape(self):
1558         image = tensor.dtensor4()
1559         maxout = tensor.dtensor4()
1560         gz = tensor.dtensor4()
1561         rng = np.random.RandomState(utt.fetch_seed())
1562         maxpoolshps = ((1, 1), (2, 2), (3, 3), (2, 3), (3, 2))
1563         image_val = rng.rand(4, 6, 7, 9)
1564         out_shapes = [[[[4, 6, 7, 9], [4, 6, 7, 9]],
1565                        [[4, 6, 3, 4], [4, 6, 4, 5]],
1566                        [[4, 6, 2, 3], [4, 6, 3, 3]],
1567                        [[4, 6, 3, 3], [4, 6, 4, 3]],
1568                        [[4, 6, 2, 4], [4, 6, 3, 5]]],
1569                       [[None, None],
1570                        [[4, 6, 4, 5], None],
1571                        [[4, 6, 3, 3], None],
1572                        [[4, 6, 4, 3], None],
1573                        [[4, 6, 3, 5], None]],
1574                       [[None, None],
1575                        [None, None],
1576                        [[4, 6, 3, 4], None],
1577                        [[4, 6, 4, 4], None],
1578                        [None, None]]]
1579         for i, maxpoolshp in enumerate(maxpoolshps):
1580             for j, ignore_border in enumerate([True, False]):
1581                 for k, pad in enumerate([(0, 0), (1, 1), (1, 2)]):
1582                     if out_shapes[k][i][j] is None:
1583                         continue
1584                     self._compile_and_check([image],
1585                                             [Pool(ignore_border=ignore_border)
1586                                              (image, maxpoolshp, pad=pad)],
1587                                             [image_val], Pool)
1588                     maxout_val = rng.rand(*out_shapes[k][i][j])
1589                     gz_val = rng.rand(*out_shapes[k][i][j])
1590                     self._compile_and_check([image, maxout, gz],
1591                                             [MaxPoolGrad(
1592                                                 ignore_border=ignore_border)
1593                                              (image, maxout, gz, maxpoolshp,
1594                                               pad=pad)],
1595                                             [image_val, maxout_val, gz_val],
1596                                             MaxPoolGrad,
1597                                             warn=False)
1598         image = tensor.tensor(dtype='float64',
1599                               broadcastable=(False, False, True, True))
1600         image_val = rng.rand(4, 6, 1, 1)
1601         self._compile_and_check(
1602             [image],
1603             [Pool(ignore_border=True)(image, (2, 2), pad=(0, 0))],
1604             [image_val], Pool)
1605     def test_pooling_with_tensor_vars(self):
1606         x = tensor.ftensor4()
1607         window_size = tensor.ivector()
1608         stride = tensor.ivector()
1609         padding = tensor.ivector()
1610         data = np.random.normal(0, 1, (1, 1, 5, 5)).astype('float32')
1611         for ignore_border in [True, False]:
1612             for mode in ['max', 'sum', 'average_inc_pad', 'average_exc_pad']:
1613                 y = pool_2d(x, window_size, ignore_border, stride, padding,
1614                             mode)
1615                 dx = theano.gradient.grad(y.sum(), x)
1616                 var_fct = theano.function([x, window_size, stride, padding],
1617                                           [y, dx])
1618                 for ws in (4, 2, 5):
1619                     for st in (2, 3):
1620                         for pad in (0, 1):
1621                             if (pad &gt; st or st &gt; ws or
1622                                     (pad != 0 and not ignore_border) or
1623                                     (mode == 'average_exc_pad' and pad != 0)):
1624                                 continue
1625                             y = pool_2d(x, (ws, ws), ignore_border, (st, st),
1626                                         (pad, pad), mode)
1627                             dx = theano.gradient.grad(y.sum(), x)
1628                             fix_fct = theano.function([x], [y, dx])
1629                             var_y, var_dx = var_fct(data, (ws, ws), (st, st),
1630                                                     (pad, pad))
1631                             fix_y, fix_dx = fix_fct(data)
1632                             utt.assert_allclose(var_y, fix_y)
1633                             utt.assert_allclose(var_dx, fix_dx)
1634     def test_pooling_with_tensor_vars_deprecated_interface(self):
1635         x = tensor.ftensor4()
1636         window_size = tensor.ivector()
1637         stride = tensor.ivector()
1638         padding = tensor.ivector()
1639         data = np.random.normal(0, 1, (1, 1, 5, 5)).astype('float32')
1640         for ignore_border in [True, False]:
1641             for mode in ['max', 'sum', 'average_inc_pad', 'average_exc_pad']:
1642                 y = pool_2d(input=x,
1643                             ds=window_size,
1644                             ignore_border=ignore_border,
1645                             st=stride,
1646                             padding=padding,
1647                             mode=mode)
1648                 dx = theano.gradient.grad(y.sum(), x)
1649                 var_fct = theano.function([x, window_size, stride, padding],
1650                                           [y, dx])
1651                 ws = 5
1652                 st = 3
1653                 pad = 1
1654                 if (pad &gt; st or st &gt; ws or
1655                         (pad != 0 and not ignore_border) or
1656                         (mode == 'average_exc_pad' and pad != 0)):
1657                     continue
1658                 y = pool_2d(input=x,
1659                             ds=(ws, ws),
1660                             ignore_border=ignore_border,
1661                             st=(st, st),
1662                             padding=(pad, pad),
1663                             mode=mode)
1664                 dx = theano.gradient.grad(y.sum(), x)
1665                 fix_fct = theano.function([x], [y, dx])
1666                 var_y, var_dx = var_fct(data, (ws, ws), (st, st),
1667                                         (pad, pad))
1668                 fix_y, fix_dx = fix_fct(data)
1669                 utt.assert_allclose(var_y, fix_y)
1670                 utt.assert_allclose(var_dx, fix_dx)
1671     def test_old_pool_interface(self):
1672         if sys.version_info[0] != 3:
1673             raise SkipTest('Skip old pool interface with python 2.x')
1674         testfile_dir = os.path.dirname(os.path.realpath(__file__))
1675         fname = 'old_pool_interface.pkl'
1676         with open(os.path.join(testfile_dir, fname), 'rb') as fp:
1677             try:
1678                 old_fct = cPickle.load(fp, encoding='latin1')
1679             except ImportError:
1680                 if sys.platform == 'win32':
1681                     exc_type, exc_value, exc_trace = sys.exc_info()
1682                     reraise(SkipTest, exc_value, exc_trace)
1683                 raise
1684         x = theano.tensor.ftensor4()
1685         y = pool_2d(x, (2, 2), mode='max', ignore_border=True)
1686         z = pool_2d(x, (2, 2), mode='average_exc_pad', ignore_border=True)
1687         dy_dx = theano.gradient.grad(y.sum(), x)
1688         dz_dx = theano.gradient.grad(z.sum(), x)
1689         new_fct = theano.function([x], [y, z, dy_dx, dz_dx])
1690         rng = np.random.RandomState(utt.fetch_seed())
1691         image_val = rng.rand(4, 6, 7, 9).astype(np.float32)
1692         old_out = old_fct(image_val)
1693         new_out = new_fct(image_val)
1694         for o, n in zip(old_out, new_out):
1695             utt.assert_allclose(o, n)
1696 if __name__ == '__main__':
1697     unittest.main()
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
