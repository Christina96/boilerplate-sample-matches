<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Matches for blas.py &amp; test_pool_1.py</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style>.modal {display: none;position: fixed;z-index: 1;left: 0;top: 0;width: 100%;height: 100%;overflow: auto;background-color: rgb(0, 0, 0);background-color: rgba(0, 0, 0, 0.4);}  .modal-content {height: 250%;background-color: #fefefe;margin: 5% auto;padding: 20px;border: 1px solid #888;width: 80%;}  .close {color: #aaa;float: right;font-size: 20px;font-weight: bold;}  .close:hover, .close:focus {color: black;text-decoration: none;cursor: pointer;}  .column {float: left;width: 50%;}  .row:after {content: ;display: table;clear: both;}  #column1, #column2 {white-space: pre-wrap;}</style></head>
<body>
<div style="align-items: center; display: flex; justify-content: space-around;">
<div>
<h3 align="center">
Matches for blas.py &amp; test_pool_1.py
      </h3>
<h1 align="center">
        2.3%
      </h1>
<center>
<a href="#" target="_top">
          INDEX
        </a>
<span>-</span>
<a href="#" target="_top">
          HELP
        </a>
</center>
</div>
<div>
<table bgcolor="#d0d0d0" border="1" cellspacing="0">
<tr><th><th>blas.py (2.8068244%)<th>test_pool_1.py (1.9767442%)<th>Tokens
<tr onclick='openModal("#0000ff")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#0000ff"><font color="#0000ff">-</font><td><a href="#" name="0">(1059-1062)<td><a href="#" name="0">(424-426)</a><td align="center"><font color="#ff0000">15</font>
<tr onclick='openModal("#f63526")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#f63526"><font color="#f63526">-</font><td><a href="#" name="1">(1142-1145)<td><a href="#" name="1">(427-430)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#980517")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#980517"><font color="#980517">-</font><td><a href="#" name="2">(159-165)<td><a href="#" name="2">(694-700)</a><td align="center"><font color="#cc0000">12</font>
<tr onclick='openModal("#53858b")' onmouseleave='this.style.backgroundColor = "#D0D0D0"' onmouseover='this.style.backgroundColor = "yellow"' style="cursor:pointer"><td bgcolor="#53858b"><font color="#53858b">-</font><td><a href="#" name="3">(49-55)<td><a href="#" name="3">(632-638)</a><td align="center"><font color="#cc0000">12</font>
</td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></td></td></a></td></td></tr></th></th></th></th></tr></table>
</div>
</div>
<hr/>
<div style="display: flex;">
<div style="flex-grow: 1;">
<h3>
<center>
<span>blas.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 from six import integer_types
3 import theano
4 from theano import Apply, Op
5 from theano.compile import optdb
6 from theano.gof import LocalOptGroup, ParamsType
7 from theano.scalar import bool as bool_t
8 from theano.tensor.basic import as_tensor_variable
9 from theano.tensor.opt import in2out
10 from .basic_ops import (GpuArrayType, CGpuKernelBase,
11                         as_gpuarray_variable, gpu_contiguous, infer_context_name, gpuarray_helper_inc_dir)
12 from .opt_util import inplace_allocempty
13 try:
14     import pygpu
15     from pygpu import blas
16 except ImportError as e:
17     pass
18 class BlasOp(Op):
19     def c_headers(self):
20         return ['&lt;blas_api.h&gt;', '&lt;numpy_compat.h&gt;', '&lt;gpuarray_helper.h&gt;']
21     def c_header_dirs(self):
22         return [pygpu.get_include(), gpuarray_helper_inc_dir()]
23     def c_init_code(self):
24         return ['import_pygpu__blas();']
25 class GpuGemv(BlasOp):
26     params_type = ParamsType(inplace=bool_t)
27     __props__ = ('inplace',)
28     def __init__(self, inplace=False):
29         self.inplace = inplace
30             self.destroy_map = {0: [0]}
31     <font color="#53858b"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, y, alpha, A, x, beta):
32         ctx_name = infer_context_name(y, A, x)
33         A = as_gpuarray_variable(A, ctx_name)
34         x = as_gpuarray_variable(x, ctx_name)
35         y = as_gpuarray_variable(y, ctx_name)
36         alpha = as_tensor_variable(alpha)
37         beta =</b></font> as_tensor_variable(beta)
38         assert alpha.ndim == 0
39         assert beta.ndim == 0
40         assert A.ndim == 2
41         assert x.ndim == 1
42         assert y.ndim == 1
43         assert A.dtype == x.dtype == y.dtype
44         expected = A.dtype
45         assert theano.scalar.upcast(alpha.dtype,
46                                     beta.dtype, expected) == expected
47         alpha = alpha.astype(expected)
48         beta = beta.astype(expected)
49         return Apply(self, [y, alpha, A, x, beta], [y.type()])
50     def perform(self, node, inputs, out_storage, params):
51         y, alpha, A, x, beta = inputs
52         inplace = params.inplace
53         if inplace and y.strides[0] &lt; 0:
54             inplace = False
55         if A.shape[1] == 0:
56             out_storage[0][0] = pygpu.zeros(y.shape, dtype=y.dtype,
57                                             context=y.context)
58         else:
59             out_storage[0][0] = blas.gemv(alpha, A, x, beta, y,
60                                           overwrite_y=inplace)
61     def c_code(self, node, name, inp, out, sub):
62         vars = dict(out=out[0], y=inp[0], alpha=inp[1], A=inp[2], x=inp[3],
63                     beta=inp[4], fail=sub['fail'], name=name,
64                     params=sub['params'])
65         code = """
66                if (!%(params)s-&gt;inplace || %(y)s-&gt;ga.strides[0] &lt;= 0) {
67                  %(out)s = theano_try_copy(%(out)s, %(y)s);
68                  if (%(out)s == NULL) {
69                    %(fail)s
70                  }
71                } else {
72                  Py_XDECREF(%(out)s);
73                  %(out)s = %(y)s;
74                  Py_INCREF(%(out)s);
75                }
76         return code
77     def c_code_cache_version(self):
78         return (10,)
79 gpugemv_no_inplace = GpuGemv(inplace=False)
80 gpugemv_inplace = GpuGemv(inplace=True)
81 class GpuGemm(BlasOp):
82     params_type = ParamsType(inplace=bool_t)
83     __props__ = ('inplace',)
84     _f16_ok = True
85     def __init__(self, inplace=False):
86         self.inplace = inplace
87             self.destroy_map = {0: [0]}
88     <font color="#980517"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>def make_node(self, C, alpha, A, B, beta):
89         ctx_name = infer_context_name(C, A, B)
90         A = as_gpuarray_variable(A, ctx_name)
91         B = as_gpuarray_variable(B, ctx_name)
92         C = as_gpuarray_variable(C, ctx_name)
93         alpha = as_tensor_variable(alpha)
94         beta =</b></font> as_tensor_variable(beta)
95         if not (A.dtype == B.dtype == C.dtype):
96             raise TypeError(theano.tensor.blas.Gemm.E_mixed,
97                             (A.dtype, B.dtype, C.dtype,
98                              alpha.dtype, beta.dtype))
99         if not A.dtype.startswith('float'):
100             raise TypeError(theano.tensor.blas.Gemm.E_float, (A.dtype))
101         if A.dtype == 'float16':
102             expected = 'float32'
103         else:
104             expected = A.dtype
105         assert theano.scalar.upcast(alpha.dtype,
106                                     beta.dtype, expected) == expected
107         alpha = alpha.astype(expected)
108         beta = beta.astype(expected)
109         assert alpha.ndim == 0
110         assert beta.ndim == 0
111         assert A.ndim == 2
112         assert B.ndim == 2
113         assert C.ndim == 2
114         return Apply(self, [C, alpha, A, B, beta], [C.type()])
115     def perform(self, node, inputs, outputs, params):
116         C, alpha, A, B, beta = inputs
117         inplace = params.inplace
118         if inplace and not C.flags.forc:
119             inplace = False
120         outputs[0][0] = blas.gemm(alpha, A, B, beta, C,
121                                   overwrite_c=inplace)
122     def c_code(self, node, name, inp, out, sub):
123         vars = dict(out=out[0], C=inp[0], alpha=inp[1], A=inp[2], B=inp[3],
124                     beta=inp[4], fail=sub['fail'], name=name,
125                     params=sub['params'])
126         code = """
127                if (!%(params)s-&gt;inplace || !GpuArray_ISONESEGMENT(&amp;%(C)s-&gt;ga)) {
128                  %(out)s = theano_try_copy(%(out)s, %(C)s);
129                  if (%(out)s == NULL) {
130                    %(fail)s
131                  }
132                } else {
133                  Py_XDECREF(%(out)s);
134                  %(out)s = %(C)s;
135                  Py_INCREF(%(out)s);
136                }
137                if (pygpu_blas_rgemm(cb_no_trans, cb_no_trans,
138                                     ((dtype_%(alpha)s *)PyArray_DATA(%(alpha)s))[0],
139                                     %(A)s, %(B)s,
140                                     ((dtype_%(beta)s *)PyArray_DATA(%(beta)s))[0],
141                                     %(out)s, 0) == -1) {
142                  %(fail)s
143                }
144     Ger on the GPU.
145         return code
146     def c_code_cache_version(self):
147         return (5,)
148 gpuger_no_inplace = GpuGer(inplace=False)
149 gpuger_inplace = GpuGer(inplace=True)
150 class GpuDot22(BlasOp):
151     _f16_ok = True
152     __props__ = ()
153     def make_node(self, x, y):
154         ctx_name = infer_context_name(x, y)
155         x = as_gpuarray_variable(x, ctx_name)
156         y = as_gpuarray_variable(y, ctx_name)
157         assert x.ndim == 2
158         assert y.ndim == 2
159         assert x.dtype == y.dtype
160         otype = x.type.clone(
161             broadcastable=(x.type.broadcastable[0], y.type.broadcastable[1]))
162         return Apply(self, [x, y], [otype()])
163     def perform(self, node, inputs, outputs):
164         x, y = inputs
165         out = pygpu.empty((x.shape[0], y.shape[1]), dtype=x.dtype,
166                           context=x.context)
167         outputs[0][0] = blas.gemm(1., x, y, 0., out,
168                                   overwrite_c=True)
169     def c_code(self, node, name, inputs, outputs, sub):
170         dtype = node.inputs[0].dtype
171         typecode = pygpu.gpuarray.dtype_to_typecode(dtype)
172         vars = dict(A=inputs[0], B=inputs[1], dtype=dtype, out=outputs[0],
173                     typecode=typecode,
174                     fail=sub['fail'], name=name)
175         code = """
176         double one = 1.;
177         double zero = 0.;
178         size_t dims[] = {0, 0};
179         dims[0] = PyGpuArray_DIMS(%(A)s)[0];
180         dims[1] = PyGpuArray_DIMS(%(B)s)[1];
181         if (theano_prep_output(&amp;%(out)s, 2, dims, %(typecode)s, GA_C_ORDER,
182                                %(A)s-&gt;context)) {
183             %(fail)s
184         }
185         if (pygpu_blas_rgemm(cb_no_trans, cb_no_trans,
186                              one,
187                              %(A)s, %(B)s,
188                              zero,
189                              %(out)s, 0) == -1) {
190             %(fail)s
191         }
192         return code
193     def c_code_cache_version(self):
194         return (4,)
195 gpugemmbatch_no_inplace = GpuGemmBatch(inplace=False)
196 gpugemmbatch_inplace = GpuGemmBatch(inplace=True)
197 class BaseGpuCorrMM(CGpuKernelBase):
198     check_broadcast = False
199     __props__ = ('border_mode', 'subsample', 'filter_dilation', 'num_groups', 'unshared')
200     _f16_ok = True
201     def __init__(self, border_mode="valid", subsample=(1, 1),
202                  filter_dilation=(1, 1), num_groups=1, unshared=False):
203         if isinstance(border_mode, integer_types):
204             if border_mode &lt; 0:
205                 raise ValueError(
206                     'invalid border_mode {}, which must be a '
207                     'non-negative integer'.format(border_mode))
208             border_mode = ((border_mode, border_mode),) * 2
209         elif isinstance(border_mode, tuple):
210             if len(border_mode) != 2:
211                 raise ValueError(
212                     'invalid border_mode {} which must be a '
213                     'tuple of length 2'.format(border_mode))
214             border = ()
215             for mode in border_mode:
216                 if isinstance(mode, tuple) and len(mode) == 2 and \
217                         min(mode) &gt;= 0:
218                     border += ((int(mode[0]), int(mode[1])),)
219                 elif mode &gt;= 0:
220                     border += ((int(mode), int(mode)),)
221                 else:
222                     raise ValueError(
223                         'invalid border mode {}. The tuple can only contain '
224                         'integers or tuples of length 2'.format(border_mode))
225             border_mode = border
226         elif border_mode not in ('valid', 'full', 'half'):
227             raise ValueError(
228                 'invalid border_mode {}, which must be either '
229                 '"valid", "full", "half", an integer or a tuple '
230                 'of length 2'.format(border_mode))
231         self.border_mode = border_mode
232         if len(subsample) != 2:
233             raise ValueError("subsample must have two elements")
234         if len(filter_dilation) != 2:
235             raise ValueError("filter_dilation must have two elements")
236         self.subsample = tuple(subsample)
237         self.filter_dilation = tuple(filter_dilation)
238         if num_groups &lt; 1:
239             raise ValueError("Number of groups should be greater than 0")
240         self.num_groups = num_groups
241         CGpuKernelBase.__init__(self, ['c_code/corr_gemm.c'])
242         self.unshared = unshared
243     @property
244     def pad(self):
245         if self.border_mode != 'valid':
246             return self.border_mode
247         return ((0, 0),) * 2
248     def __str__(self):
249         return '%s{%s, %s, %s, %s, %s}' % (
250             self.__class__.__name__,
251             self.border_mode,
252             str(self.subsample),
253             str(self.filter_dilation),
254             str(self.num_groups),
255             str(self.unshared))
256     def __setstate__(self, d):
257         self.__dict__.update(d)
258         if not hasattr(self, 'num_groups'):
259             self.num_groups = 1
260     def flops(self, inp, outp):
261         inputs, filters = inp
262         outputs, = outp
263         assert inputs[1] == (filters[1] * self.num_groups)
264         flops = filters[2] * filters[3] * 2
265         flops *= outputs[2] * outputs[3]
266         flops *= inputs[1] * filters[0] * inputs[0] / self.num_groups
267         return flops
268     def c_headers(self):
269         return ["&lt;gpuarray/array.h&gt;", "&lt;gpuarray/blas.h&gt;", "gpuarray_helper.h"]
270     def c_header_dirs(self):
271         return [gpuarray_helper_inc_dir()]
272     def c_code_cache_version(self):
273         return (12,)
274     def c_code_helper(self, bottom, weights, top, direction, sub, height=None, width=None):
275         dH, dW = self.subsample
276         dilH, dilW = self.filter_dilation
277         numgroups = self.num_groups
278         unshared = int(self.unshared)
279         if self.border_mode == "half":
280             padH_l = padH_r = padW_l = padW_r = -1
281         elif self.border_mode == "full":
282             padH_l = padH_r = padW_l = padW_r = -2
283         elif isinstance(self.border_mode, tuple):
284             (padH_l, padH_r), (padW_l, padW_r) = self.border_mode
285         else:
286             assert self.border_mode == "valid"
287             padH_l = padH_r = padW_l = padW_r = 0
288         if direction == "forward":
289             direction = 0
290             out = top
291         elif direction == "backprop weights":
292             direction = 1
293             out = weights
294         elif direction == "backprop inputs":
295             direction = 2
296             out = bottom
297         else:
298             raise ValueError("direction must be one of 'forward', "
299                              "'backprop weights', 'backprop inputs'")
300         if height:
301             height = '(*(npy_int*)(PyArray_DATA(%s)))' % height
302         else:
303             if ((direction != 0) and (dH != 1)) or ((direction == 1) and (padH_l == -1 or padH_r == -1)):
304                 raise ValueError("height must be given for backprop with vertical sampling or pad='half'")
305             height = '-1'
306         if width:
307             width = '(*(npy_int*)(PyArray_DATA(%s)))' % width
308         else:
309             if ((direction != 0) and (dW != 1)) or ((direction == 1) and (padW_l == -1 or padW_r == -1)):
310                 raise ValueError("width must be given for backprop with horizontal sampling or pad='half'")
311             width = '-1'
312         sub = sub.copy()
313         sub.update(locals())
314         return """
315     // Mandatory args
316     int direction = %(direction)s;  // forward, bprop weights, bprop inputs
317     // Optional args
318     size_t dH = %(dH)s;
319     size_t dW = %(dW)s;
320     size_t dilH = %(dilH)s;
321     size_t dilW = %(dilW)s;
322     int padH_l = %(padH_l)s;
323     int padH_r = %(padH_r)s;
324     int padW_l = %(padW_l)s;
325     int padW_r = %(padW_r)s;
326     int numgroups = %(numgroups)s;
327     int unshared = %(unshared)s;
328     PyGpuArrayObject * bottom = %(bottom)s;
329     PyGpuArrayObject * weights = %(weights)s;
330     PyGpuArrayObject * top = %(top)s;
331     PyGpuArrayObject * out2 = NULL;
332     int wdim, odim;
333     wdim = unshared ? 6 : 4;
334     odim = 4; //Can be set to 6 later for unshared backprop wrt weights
335     // Obtain or infer kernel width and height
336     // (we need to know it early to be able to handle auto-padding)
337     size_t kH, kW, dil_kH, dil_kW;
338     if (direction != 1) {
339         // weight is an input variable, we can just read its shape
340         kH = PyGpuArray_DIMS(weights)[wdim-2];
341         kW = PyGpuArray_DIMS(weights)[wdim-1];
342     }
343     else {
344         if (%(height)s != -1) {
345             // kernel height is specified (perhaps vertical subsampling or half padding)
346             kH = %(height)s;
347         }
348         else if (padH_l == -2 || padH_r == -2) {
349             // vertical full padding, we can infer the kernel height
350             kH = (2 - PyGpuArray_DIMS(bottom)[2] + (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1;
351         }
352         else {
353             // explicit padding, we can infer the kernel height
354             kH = (PyGpuArray_DIMS(bottom)[2] + padH_l + padH_r - (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1 ;
355         }
356         if (%(width)s != -1) {
357             kW = %(width)s;
358         }
359         else if (padW_l == -2 || padW_r == -2) {
360             kW = (2 - PyGpuArray_DIMS(bottom)[3] + (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
361         }
362         else {
363             kW = (PyGpuArray_DIMS(bottom)[3] + padW_l + padW_r - (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
364         }
365     }
366     // Implicit dilated kernel size
367     dil_kH = (kH - 1) * dilH + 1;
368     dil_kW = (kW - 1) * dilW + 1;
369     // Auto-padding if requested
370     if (padH_l == -1 || padH_r == -1) {  // vertical half padding
371         padH_l = padH_r = dil_kH / 2;
372     }
373     else if (padH_l == -2 || padH_r == -2) {  // vertical full padding
374         padH_l = padH_r = dil_kH - 1;
375     }
376     else if (padH_l &lt; 0 || padH_r &lt; 0) {
377         PyErr_SetString(PyExc_ValueError, "BaseGpuCorrMM: padH must be &gt;= -2");
378         %(fail)s
379     }
380     if (padW_l == -1 || padW_r == -1) {  // horizontal half padding
381         padW_l = padW_r = dil_kW / 2;
382     }
383     else if (padW_l == -2 || padW_r == -2) {  // horizontal full padding
384         padW_l = padW_r = dil_kW - 1;
385     }
386     else if (padW_l &lt; 0 || padW_r &lt; 0) {
387         PyErr_SetString(PyExc_ValueError, "BaseGpuCorrMM: padW must be &gt;= -2");
388         %(fail)s
389     }
390     // Infer output shape and type
391     // The inferred shape can be negative.
392     long long out_dim[6];
393     size_t out_dim_size[6];
394     out_dim[4] = out_dim[5] = 0; //Only used for unshared backprop wrt weights
395     out_dim_size[4] = out_dim_size[5] = 0; //Same
396     int out_typecode;
397     PyGpuContextObject *out_context;
398     switch(direction) {
399     case 0:  // forward pass
400         // output is top: (batchsize, num_filters, height, width)
401         // height and width: top = (bottom + pad_l + pad_r - ((weight-1)*dil + 1)) / sample + 1
402         out_dim[0] = PyGpuArray_DIMS(bottom)[0];
403         out_dim[1] = PyGpuArray_DIMS(weights)[0];
404         out_dim[2] = (PyGpuArray_DIMS(bottom)[2] + padH_l + padH_r - ((PyGpuArray_DIMS(weights)[wdim-2]-1)*dilH + 1)) / dH + 1;
405         out_dim[3] = (PyGpuArray_DIMS(bottom)[3] + padW_l + padW_r - ((PyGpuArray_DIMS(weights)[wdim-1]-1)*dilW + 1)) / dW + 1;
406         out_typecode = bottom-&gt;ga.typecode;
407         out_context = bottom-&gt;context;
408         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
409         {
410             if (unshared) {
411                 PyErr_Format(PyExc_ValueError,
412                              "GpuCorrMM: impossible output shape\\n"
413                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
414                              "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld x %%ld\\n"
415                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
416                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
417                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
418                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
419                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
420                              PyGpuArray_DIMS(weights)[4], PyGpuArray_DIMS(weights)[5],
421                              out_dim[0], out_dim[1], out_dim[2], out_dim[3]);
422                 %(fail)s
423             }
424             else {
425                 PyErr_Format(PyExc_ValueError,
426                              "GpuCorrMM: impossible output shape\\n"
427                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
428                              "  weights shape: %%ld x %%ld x %%ld x %%ld\\n"
429                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
430                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
431                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
432                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
433                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
434                              out_dim[0], out_dim[1], out_dim[2], out_dim[3]);
435                 %(fail)s
436             }
437         }
438         break;
439     case 1:  // backprop wrt. weights
440         // output is weights: (num_filters, num_channels, height, width) or
441         // (num_filters, top_height, top_width, num_channels, height, width) -&gt; for unshared
442         // height and width: weights = (bottom + 2*pad - (top - 1) * sample - 1) / dil + 1
443         out_dim[0] = PyGpuArray_DIMS(top)[1];
444         if (unshared){
445             odim = 6;
446             out_dim[1] = PyGpuArray_DIMS(top)[2];
447             out_dim[2] = PyGpuArray_DIMS(top)[3];
448         }
449         out_dim[wdim-3] = PyGpuArray_DIMS(bottom)[1] / numgroups;
450         out_dim[wdim-2] = kH;  // already inferred further above
451         out_dim[wdim-1] = kW;  // how convenient
452         out_typecode = top-&gt;ga.typecode;
453         out_context = top-&gt;context;
454         if (unshared) {
455             if (out_dim[0] &lt; 0 || out_dim[1] &lt;= 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt; 0
456                     || out_dim[4] &lt;= 0 || out_dim[5] &lt;= 0){
457                 PyErr_Format(PyExc_ValueError,
458                              "GpuCorrMM backprop wrt. weights: impossible output shape\\n"
459                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
460                              "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld x %%ld\\n"
461                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
462                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
463                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
464                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
465                              out_dim[4], out_dim[5],
466                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
467                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
468                 %(fail)s
469             }
470         }
471         else {
472              if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
473             {
474                 PyErr_Format(PyExc_ValueError,
475                              "GpuCorrMM backprop wrt. weights: impossible output shape\\n"
476                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
477                              "  weights shape: %%ld x %%ld x %%ld x %%ld\\n"
478                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
479                              PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
480                              PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
481                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
482                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
483                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
484                 %(fail)s
485             }
486         }
487         break;
488     case 2:  // backprop wrt. inputs
489         // output is bottom: (batchsize, num_channels, height, width)
490         // height and width: bottom = (top - 1) * sample + (weights-1)*dil + 1 - 2*pad
491         out_dim[0] = PyGpuArray_DIMS(top)[0];
492         out_dim[1] = PyGpuArray_DIMS(weights)[wdim-3] * numgroups;
493         out_dim[2] = (%(height)s != -1) ? %(height)s : (PyGpuArray_DIMS(top)[2] - 1) * dH + (PyGpuArray_DIMS(weights)[wdim-2]-1)*dilH + 1 - padH_l - padH_r;
494         out_dim[3] = (%(width)s != -1) ? %(width)s : (PyGpuArray_DIMS(top)[3] - 1) * dW + (PyGpuArray_DIMS(weights)[wdim-1]-1)*dilW + 1 - padW_l - padW_r;
495         out_typecode = top-&gt;ga.typecode;
496         out_context = top-&gt;context;
497         if (unshared) {
498             if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
499             {
500                 PyErr_Format(PyExc_ValueError,
501                              "GpuCorrMM backprop wrt. inputs: impossible output shape\\n"
502                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
503                              "  weight shape: %%ld x %%ld x %%ld x %%ld x %%ld x %%ld\\n"
504                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
505                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
506                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
507                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
508                              PyGpuArray_DIMS(weights)[4], PyGpuArray_DIMS(weights)[5],
509                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
510                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
511                 %(fail)s
512             }
513         }
514         else {
515             if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0)
516             {
517                 PyErr_Format(PyExc_ValueError,
518                              "GpuCorrMM backprop wrt. inputs: impossible output shape\\n"
519                              "  bottom shape: %%ld x %%ld x %%ld x %%ld\\n"
520                              "  weight shape: %%ld x %%ld x %%ld x %%ld\\n"
521                              "  top shape: %%ld x %%ld x %%ld x %%ld\\n",
522                              out_dim[0], out_dim[1], out_dim[2], out_dim[3],
523                              PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
524                              PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
525                              PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
526                              PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3]);
527                 %(fail)s
528             }
529         }
530         break;
531     default:
532         PyErr_SetString(PyExc_ValueError, "BaseGpuCorrMM: direction must be 0, 1, or 2\\n");
533         %(fail)s
534     }
535     out_dim_size[0] = (size_t)out_dim[0];
536     out_dim_size[1] = (size_t)out_dim[1];
537     out_dim_size[2] = (size_t)out_dim[2];
538     out_dim_size[3] = (size_t)out_dim[3];
539     if (odim == 6) {
540         out_dim_size[4] = (size_t)out_dim[4];
541         out_dim_size[5] = (size_t)out_dim[5];
542     }
543     // Prepare output array
544     if (theano_prep_output(&amp;%(out)s, odim, out_dim_size, out_typecode, GA_C_ORDER, out_context) != 0)
545     {
546         if (odim == 4) {
547             PyErr_Format(PyExc_RuntimeError,
548                     "BaseGpuCorrMM: Failed to allocate output of %%lld x %%lld x %%lld x %%lld",
549                     out_dim[0], out_dim[1], out_dim[2], out_dim[3]);
550         }
551         if (odim == 6) {
552             PyErr_Format(PyExc_RuntimeError,
553                     "BaseGpuCorrMM: Failed to allocate output of %%lld x %%lld x %%lld x %%lld %%lld %%lld",
554                     out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4], out_dim[5]);
555         }
556         %(fail)s
557     }
558     if (!GpuArray_IS_C_CONTIGUOUS(&amp;%(out)s-&gt;ga)) {
559         PyErr_SetString(PyExc_ValueError, "Only contiguous outputs are supported.");
560         %(fail)s
561     }
562     // Call GPU code
563     out2 = corrMM(%(bottom)s, %(weights)s, %(top)s, direction, dH, dW, dilH, dilW,
564                 padH_l, padH_r, padW_l, padW_r, numgroups, unshared);
565     if (out2==NULL){
566        %(fail)s
567     }
568     assert (out2 == %(out)s);
569     GPU correlation implementation using Matrix Multiplication.
570     Parameters
571     ----------
572     border_mode
573         The width of a border of implicit zeros to pad the
574         input with. Must be a tuple with 2 elements giving the numbers of rows
575         and columns to pad on each side, or a single integer to pad the same
576         on all sides, or a string shortcut setting the padding at runtime:
577         ``'valid'`` for ``(0, 0)`` (valid convolution, no padding), ``'full'``
578         for ``(kernel_rows - 1, kernel_columns - 1)`` (full convolution),
579         ``'half'`` for ``(kernel_rows // 2, kernel_columns // 2)`` (same
580         convolution for odd-sized kernels).
581         If it is a tuple containing 2 pairs of integers, then these specify
582         the padding to be applied on each side ((left, right), (top, bottom)).
583         Otherwise, each width is applied twice, once per side (left and right,
584         top and bottom).
585     subsample
586         The subsample operation applied to each output image.
587         Should be a tuple with 2 elements.
588         `(sv, sh)` is equivalent to `GpuCorrMM(...)(...)[:,:,::sv, ::sh]`,
589         but faster.
590         Set to `(1, 1)` to disable subsampling.
591     filter_dilation
592         The filter dilation operation applied to each input image.
593         Should be a tuple with 2 elements.
594         Set to `(1, 1)` to disable filter dilation.
595     num_groups
596         The number of distinct groups the image and kernel must be
597         divided into.
598         should be an int
599         set to 1 to disable grouped convolution
600     unshared
601         Perform unshared correlation (default: False)
602     Notes
603     -----
604     Currently, the Op requires the inputs, filters and outputs to be
605     C-contiguous. Use :func:`gpu_contiguous
606     &lt;theano.gpuarray.basic_ops.gpu_contiguous&gt;` on these arguments
607     if needed.
608     You can either enable the Theano flag `optimizer_including=conv_gemm`
609     to automatically replace all convolution operations with `GpuCorrMM`
610     or one of its gradients, or you can use it as a replacement for
611     :func:`conv2d &lt;theano.tensor.nnet.conv.conv2d&gt;`, called as
612     `GpuCorrMM(subsample=...)(image, filters)`. The latter is currently
613     faster, but note that it computes a correlation -- if you need to
614     compute a convolution, flip the filters as `filters[:,:,::-1,::-1]`.
615     Gradient wrt. filters for `GpuCorrMM`.
616     Notes
617     -----
618     You will not want to use this directly, but rely on Theano's automatic
619     differentiation or graph optimization to use it as needed.
620     """
621             broadcastable = [<font color="#0000ff"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>topgrad.type.broadcastable[1], False, False,
622                              img.type.broadcastable[1], False, False]
623         else:
624             broadcastable = [topgrad.type.broadcastable[1], img.type.broadcastable[</b></font>1],
625                              False, False]
626         return Apply(self, [img, topgrad] + height_width, [GpuArrayType(dtype=img.dtype,
627                                                                         context_name=ctx_name,
628                                                                         broadcastable=broadcastable)()])
629     def c_code(self, node, nodename, inp, out_, sub):
630         bottom, top = inp[:2]
631         height, width = inp[2:] or (None, None)
632         weights, = out_
633         direction = "backprop weights"
634         return super(GpuCorrMM_gradWeights, self).c_code_helper(bottom, weights, top, direction, sub, height, width)
635     def grad(self, inp, grads):
636         bottom, top = inp[:2]
637         weights, = grads
638         weights = gpu_contiguous(weights)
639         d_bottom = GpuCorrMM_gradInputs(self.border_mode,
640                                         self.subsample,
641                                         self.filter_dilation,
642                                         self.num_groups,
643                                         self.unshared)(weights,
644                                                        top,
645                                                        bottom.shape[-2:])
646         d_top = GpuCorrMM(
647             self.border_mode, self.subsample, self.filter_dilation, self.num_groups, self.unshared)(bottom, weights)
648         d_height_width = (
649             theano.gradient.DisconnectedType()(),
650             ) * 2 if len(inp) == 4 else ()
651         return (d_bottom, d_top) + d_height_width
652     def connection_pattern(self, node):
653         if node.nin == 2:
654             return [[1], [1]]
655         else:
656             return [[1], [1], [0], [0]]  # no connection to height, width
657 class GpuCorrMM_gradInputs(BaseGpuCorrMM):
658     """
659     Gradient wrt. inputs for `GpuCorrMM`.
660     Notes
661     -----
662     You will not want to use this directly, but rely on Theano's automatic
663     differentiation or graph optimization to use it as needed.
664     """
665     def __init__(self, border_mode="valid",
666                  subsample=(1, 1),
667                  filter_dilation=(1, 1),
668                  num_groups=1,
669                  unshared=False):
670         super(GpuCorrMM_gradInputs, self).__init__(border_mode, subsample,
671                                                    filter_dilation, num_groups,
672                                                    unshared)
673     def make_node(self, kern, topgrad, shape=None):
674         ctx_name = infer_context_name(kern, topgrad)
675         kern = as_gpuarray_variable(kern, ctx_name)
676         topgrad = as_gpuarray_variable(topgrad, ctx_name)
677         if self.unshared:
678             if kern.type.ndim != 6:
679                 raise TypeError('kern must be 6D tensor')
680         else:
681             if kern.type.ndim != 4:
682                 raise TypeError('kern must be 4D tensor')
683         if topgrad.type.ndim != 4:
684             raise TypeError('topgrad must be 4D tensor')
685         if shape is None:
686             if self.subsample != (1, 1):
687                 raise ValueError('shape must be given if subsample != (1, 1)')
688             height_width = []
689         else:
690             height_width = [shape[0], shape[1]]
691             assert shape[0].ndim == 0
692         if self.num_groups &gt; 1:
693             broadcastable = [<font color="#f63526"><a href="#"><img align="right" alt="other" border="0" src="forward.gif"/></a><b>topgrad.type.broadcastable[0], False,
694                              False, False]
695         else:
696             broadcastable = [topgrad.type.broadcastable[0], kern.type.broadcastable[</b></font>-3],
697                              False, False]
698         return Apply(self, [kern, topgrad] + height_width, [GpuArrayType(dtype=topgrad.dtype,
699                                                                          context_name=ctx_name,
700                                                                          broadcastable=broadcastable)()])
701     def c_code(self, node, nodename, inp, out_, sub):
702         weights, top = inp[:2]
703         height, width = inp[2:] or (None, None)
704         bottom, = out_
705         direction = "backprop inputs"
706         return super(GpuCorrMM_gradInputs, self).c_code_helper(bottom, weights, top, direction, sub, height, width)
707     def grad(self, inp, grads):
708         weights, top = inp[:2]
709         bottom, = grads
710         bottom = gpu_contiguous(bottom)
711         d_weights = GpuCorrMM_gradWeights(self.border_mode,
712                                           self.subsample,
713                                           self.filter_dilation,
714                                           self.num_groups,
715                                           self.unshared)(bottom,
716                                                          top,
717                                                          weights.shape[-2:])
718         d_top = GpuCorrMM(self.border_mode,
719                           self.subsample,
720                           self.filter_dilation,
721                           self.num_groups,
722                           self.unshared)(bottom, weights)
723         d_height_width = (
724             theano.gradient.DisconnectedType()(),
725             ) * 2 if len(inp) == 4 else ()
726         return (d_weights, d_top) + d_height_width
727     def connection_pattern(self, node):
728         if node.nin == 2:
729             return [[1], [1]]
730         else:
731             return [[1], [1], [0], [0]]  # no connection to height, width
732 class BaseGpuCorr3dMM(CGpuKernelBase):
733     """
734     Base class for `GpuCorr3dMM`, `GpuCorr3dMM_gradWeights` and
735     `GpuCorr3dMM_gradInputs`. Cannot be used directly.
736     Parameters
737     ----------
738     border_mode : {'valid', 'full', 'half'}
739         Additionally, the padding size could be directly specified by an integer
740         or a pair of integers
741     subsample
742         Perform subsampling of the output (default: (1, 1, 1)).
743     filter_dilation
744         Perform subsampling of the input, also known as dilation (default: (1, 1, 1)).
745     num_groups :
746         Divides the image, kernel and output tensors into num_groups
747         separate groups. Each which carry out convolutions separately (default : 1).
748     """
749     check_broadcast = False
750     __props__ = ('border_mode', 'subsample', 'filter_dilation', 'num_groups')
751     _f16_ok = True
752     def __init__(self, border_mode="valid", subsample=(1, 1, 1),
753                  filter_dilation=(1, 1, 1), num_groups=1):
754         if isinstance(border_mode, integer_types):
755             border_mode = (border_mode, border_mode, border_mode)
756         if isinstance(border_mode, tuple):
757             pad_h, pad_w, pad_d = map(int, border_mode)
758             border_mode = (pad_h, pad_w, pad_d)
759         if not ((isinstance(border_mode, tuple) and min(border_mode) &gt;= 0) or
760                 border_mode in ('valid', 'full', 'half')):
761             raise ValueError(
762                 'invalid border_mode {}, which must be either '
763                 '"valid", "full", "half", an integer or a tuple of'
764                 ' three integers'.format(border_mode))
765         self.border_mode = border_mode
766         if len(subsample) != 3:
767             raise ValueError("subsample must have three elements")
768         if len(filter_dilation) != 3:
769             raise ValueError("filter_dilation must have three elements")
770         self.subsample = tuple(subsample)
771         self.filter_dilation = tuple(filter_dilation)
772         if num_groups &lt; 1:
773             raise ValueError("Number of groups should be greater than 0")
774         self.num_groups = num_groups
775         CGpuKernelBase.__init__(self, ['c_code/corr3d_gemm.c'])
776     @property
777     def pad(self):
778         if self.border_mode != 'valid':
779             return self.border_mode
780         return (0, 0, 0)
781     def __str__(self):
782         return '%s{%s, %s, %s, %s}' % (
783             self.__class__.__name__,
784             self.border_mode,
785             str(self.subsample),
786             str(self.filter_dilation),
787             str(self.num_groups))
788     def __setstate__(self, d):
789         self.__dict__.update(d)
790         if not hasattr(self, 'num_groups'):
791             self.num_groups = 1
792     def flops(self, inp, outp):
793         """
794         Useful with the hack in profilemode to print the MFlops.
795         """
796         inputs, filters = inp
797         outputs, = outp
798         assert inputs[1] == (filters[1] * self.num_groups)
799         flops = filters[2] * filters[3] * filters[4] * 2
800         flops *= outputs[2] * outputs[3] * outputs[4]
801         flops *= inputs[1] * filters[0] * inputs[0] / self.num_groups
802         return flops
803     def c_headers(self):
804         return ["&lt;gpuarray/array.h&gt;", "&lt;gpuarray/blas.h&gt;", "gpuarray_helper.h"]
805     def c_header_dirs(self):
806         return [gpuarray_helper_inc_dir()]
807     def c_code_cache_version(self):
808         return (8,)
809     def c_code_helper(self, bottom, weights, top, direction, sub,
810                       height=None, width=None, depth=None):
811         """
812         This generates the C code for GpuCorr3dMM (direction="forward"),
813         GpuCorr3dMM_gradWeights (direction="backprop weights"), and
814         GpuCorr3dMM_gradInputs (direction="backprop inputs").
815         Depending on the direction, one of bottom, weights, top will
816         receive the output, while the other two serve as inputs.
817         Parameters
818         ----------
819         bottom
820             Variable name of the input images in the forward pass,
821             or the gradient of the input images in backprop wrt. inputs
822         weights
823             Variable name of the filters in the forward pass,
824             or the gradient of the filters in backprop wrt. weights
825         top
826             Variable name of the output images / feature maps in the
827             forward pass, or the gradient of the outputs in the backprop passes
828         direction : {'forward', 'backprop weights', 'backprop inputs'}
829             "forward" to correlate bottom with weights and store results in top,
830             "backprop weights" to do a valid convolution of bottom with top
831             (swapping the first two dimensions) and store results in weights,
832             and "backprop inputs" to do a full convolution of top with weights
833             (swapping the first two dimensions) and store results in bottom.
834         sub
835             Dictionary of substitutions useable to help generating the C code.
836         height
837             Required if self.subsample[0] != 1, a variable giving the height of
838             the filters for direction="backprop weights" or the height of the
839             input images for direction="backprop inputs".
840             Required if self.border_mode == 'half', a variable giving the height
841             of the filters for direction="backprop weights".
842             Not required otherwise, but if a value is given this will be checked.
843         width
844             Required if self.subsample[1] != 1, a variable giving the width of
845             the filters for direction="backprop weights" or the width of the
846             input images for direction="backprop inputs".
847             Required if self.border_mode == 'half', a variable giving the width
848             of the filters for direction="backprop weights".
849             Not required otherwise, but if a value is given this will be checked.
850         depth
851             Required if self.subsample[2] != 1, a variable giving the depth of
852             the filters for direction="backprop weights" or the depth of the
853             input images for direction="backprop inputs".
854             Required if self.border_mode == 'half', a variable giving the depth
855             of the filters for direction="backprop weights".
856             Not required otherwise, but if a value is given this will be checked.
857         """
858         dH, dW, dD = self.subsample
859         dilH, dilW, dilD = self.filter_dilation
860         numgroups = self.num_groups
861         if self.border_mode == "half":
862             padH = padW = padD = -1
863         elif self.border_mode == "full":
864             padH = padW = padD = -2
865         elif isinstance(self.border_mode, tuple):
866             padH, padW, padD = self.border_mode
867         else:
868             assert self.border_mode == "valid"
869             padH = padW = padD = 0
870         if direction == "forward":
871             direction = 0
872             out = top
873         elif direction == "backprop weights":
874             direction = 1
875             out = weights
876         elif direction == "backprop inputs":
877             direction = 2
878             out = bottom
879         else:
880             raise ValueError("direction must be one of 'forward', "
881                              "'backprop weights', 'backprop inputs'")
882         if height:
883             height = '(*(npy_int*)(PyArray_DATA(%s)))' % height
884         else:
885             if ((direction != 0) and (dH != 1)) or ((direction == 1) and (padH == -1)):
886                 raise ValueError("height must be given for backprop with vertical sampling or pad='half'")
887             height = '-1'
888         if width:
889             width = '(*(npy_int*)(PyArray_DATA(%s)))' % width
890         else:
891             if ((direction != 0) and (dW != 1)) or ((direction == 1) and (padW == -1)):
892                 raise ValueError("width must be given for backprop with horizontal sampling or pad='half'")
893             width = '-1'
894         if depth:
895             depth = '(*(npy_int*)(PyArray_DATA(%s)))' % depth
896         else:
897             if ((direction != 0) and (dD != 1)) or ((direction == 1) and (padD == -1)):
898                 raise ValueError("depth must be given for backprop with horizontal sampling or pad='half'")
899             depth = '-1'
900         sub = sub.copy()
901         sub.update(locals())
902         return """
903     // Mandatory args
904     int direction = %(direction)s;  // forward, bprop weights, bprop inputs
905     // Optional args
906     size_t dH = %(dH)s;
907     size_t dW = %(dW)s;
908     size_t dD = %(dD)s;
909     size_t dilH = %(dilH)s;
910     size_t dilW = %(dilW)s;
911     size_t dilD = %(dilD)s;
912     int padH = %(padH)s;
913     int padW = %(padW)s;
914     int padD = %(padD)s;
915     int numgroups = %(numgroups)s;
916     PyGpuArrayObject * bottom = %(bottom)s;
917     PyGpuArrayObject * weights = %(weights)s;
918     PyGpuArrayObject * top = %(top)s;
919     PyGpuArrayObject * out2 = NULL;
920     // Obtain or infer kernel height, width and depth
921     // (we need to know it early to be able to handle auto-padding)
922     size_t kH, kW, kD, dil_kH, dil_kW, dil_kD;
923     if (direction != 1) {
924         // weight is an input variable, we can just read its shape
925         kH = PyGpuArray_DIMS(weights)[2];
926         kW = PyGpuArray_DIMS(weights)[3];
927         kD = PyGpuArray_DIMS(weights)[4];
928     }
929     else {
930         if (%(height)s != -1) {
931             // kernel height is specified (perhaps vertical subsampling or half padding)
932             kH = %(height)s;
933         }
934         else if (padH == -2) {
935             // vertical full padding, we can infer the kernel height
936             kH = (2 - PyGpuArray_DIMS(bottom)[2] + (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1;
937         }
938         else {
939             // explicit padding, we can infer the kernel height
940             kH = (PyGpuArray_DIMS(bottom)[2] + 2*padH - (PyGpuArray_DIMS(top)[2] - 1) * dH - 1) / dilH + 1 ;
941         }
942         if (%(width)s != -1) {
943             kW = %(width)s;
944         }
945         else if (padW == -2) {
946             kW = (2 - PyGpuArray_DIMS(bottom)[3] + (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
947         }
948         else {
949             kW = (PyGpuArray_DIMS(bottom)[3] + 2*padW - (PyGpuArray_DIMS(top)[3] - 1) * dW - 1) / dilW + 1;
950         }
951         if (%(depth)s != -1) {
952             kD = %(depth)s;
953         }
954         else if (padD == -2) {
955             kD = (2 - PyGpuArray_DIMS(bottom)[4] + (PyGpuArray_DIMS(top)[4] - 1) * dD - 1) / dilD + 1;
956         }
957         else {
958             kD = (PyGpuArray_DIMS(bottom)[4] + 2*padD - (PyGpuArray_DIMS(top)[4] - 1) * dD - 1) / dilD + 1;
959         }
960     }
961     // Implicit dilated kernel size
962     dil_kH = (kH - 1) * dilH + 1;
963     dil_kW = (kW - 1) * dilW + 1;
964     dil_kD = (kD - 1) * dilD + 1;
965     // Auto-padding if requested
966     if (padH == -1) {  // vertical half padding
967         padH = dil_kH / 2;
968     }
969     else if (padH == -2) {  // vertical full padding
970         padH = dil_kH - 1;
971     }
972     else if (padH &lt; 0) {
973         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: padH must be &gt;= -2");
974         %(fail)s
975     }
976     if (padW == -1) {  // horizontal half padding
977         padW = dil_kW / 2;
978     }
979     else if (padW == -2) {  // horizontal full padding
980         padW = dil_kW - 1;
981     }
982     else if (padW &lt; 0) {
983         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: padW must be &gt;= -2");
984         %(fail)s
985     }
986     if (padD == -1) {  // depth half padding
987         padD = dil_kD / 2;
988     }
989     else if (padD == -2) {  // depth full padding
990         padD = dil_kD - 1;
991     }
992     else if (padD &lt; 0) {
993         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: padD must be &gt;= -2");
994         %(fail)s
995     }
996     // Infer output shape and type
997     // The inferred shape can be negative.
998     long long out_dim[5];
999     size_t out_dim_size[5];
1000     int out_typecode;
1001     PyGpuContextObject *out_context;
1002     switch(direction) {
1003     case 0:  // forward pass
1004         // output is top: (batchsize, num_filters, height, width, depth)
1005         // height, width and depth: top = (bottom + 2*pad - ((weight-1)*dil + 1)) / sample + 1
1006         out_dim[0] = PyGpuArray_DIMS(bottom)[0];
1007         out_dim[1] = PyGpuArray_DIMS(weights)[0];
1008         out_dim[2] = (PyGpuArray_DIMS(bottom)[2] + 2*padH - ((PyGpuArray_DIMS(weights)[2]-1)*dilH + 1)) / dH + 1;
1009         out_dim[3] = (PyGpuArray_DIMS(bottom)[3] + 2*padW - ((PyGpuArray_DIMS(weights)[3]-1)*dilW + 1)) / dW + 1;
1010         out_dim[4] = (PyGpuArray_DIMS(bottom)[4] + 2*padD - ((PyGpuArray_DIMS(weights)[4]-1)*dilD + 1)) / dD + 1;
1011         out_typecode = bottom-&gt;ga.typecode;
1012         out_context = bottom-&gt;context;
1013         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0 || out_dim[4] &lt;= 0)
1014         {
1015             PyErr_Format(PyExc_ValueError,
1016                          "GpuCorr3dMM: impossible output shape\\n"
1017                          "  bottom shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1018                          "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1019                          "  top shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n",
1020                          PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
1021                          PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
1022                          PyGpuArray_DIMS(bottom)[4],
1023                          PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
1024                          PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
1025                          PyGpuArray_DIMS(weights)[4],
1026                          out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4]);
1027             %(fail)s
1028         }
1029         break;
1030     case 1:  // backprop wrt. weights
1031         // output is weights: (num_filters, num_channels, height, width, depth)
1032         // height, width and depth: weights = (bottom + 2*pad - (top - 1) * sample - 1) / dil + 1
1033         out_dim[0] = PyGpuArray_DIMS(top)[1];
1034         out_dim[1] = PyGpuArray_DIMS(bottom)[1] / numgroups;
1035         out_dim[2] = kH;  // already inferred further above
1036         out_dim[3] = kW;  // how convenient
1037         out_dim[4] = kD;
1038         out_typecode = top-&gt;ga.typecode;
1039         out_context = top-&gt;context;
1040         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0 || out_dim[4] &lt;= 0)
1041         {
1042             PyErr_Format(PyExc_ValueError,
1043                          "GpuCorr3dMM backprop wrt. weights: impossible output shape\\n"
1044                          "  bottom shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1045                          "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1046                          "  top shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n",
1047                          PyGpuArray_DIMS(bottom)[0], PyGpuArray_DIMS(bottom)[1],
1048                          PyGpuArray_DIMS(bottom)[2], PyGpuArray_DIMS(bottom)[3],
1049                          PyGpuArray_DIMS(bottom)[4],
1050                          out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4],
1051                          PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
1052                          PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3],
1053                          PyGpuArray_DIMS(top)[4]);
1054             %(fail)s
1055         }
1056         break;
1057     case 2:  // backprop wrt. inputs
1058         // output is bottom: (batchsize, num_channels, height, width, depth)
1059         // height, width and depth: bottom = (top - 1) * sample + (weights-1)*dil + 1 - 2*pad
1060         out_dim[0] = PyGpuArray_DIMS(top)[0];
1061         out_dim[1] = PyGpuArray_DIMS(weights)[1] * numgroups;
1062         out_dim[2] = (%(height)s != -1) ? %(height)s : (PyGpuArray_DIMS(top)[2] - 1) * dH + (PyGpuArray_DIMS(weights)[2]-1)*dilH + 1 - 2*padH;
1063         out_dim[3] = (%(width)s != -1) ? %(width)s : (PyGpuArray_DIMS(top)[3] - 1) * dW + (PyGpuArray_DIMS(weights)[3]-1)*dilW + 1 - 2*padW;
1064         out_dim[4] = (%(depth)s != -1) ? %(depth)s : (PyGpuArray_DIMS(top)[4] - 1) * dD + (PyGpuArray_DIMS(weights)[4]-1)*dilD + 1 - 2*padD;
1065         out_typecode = top-&gt;ga.typecode;
1066         out_context = top-&gt;context;
1067         if (out_dim[0] &lt; 0 || out_dim[1] &lt; 0 || out_dim[2] &lt;= 0 || out_dim[3] &lt;= 0 || out_dim[4] &lt;= 0)
1068         {
1069             PyErr_Format(PyExc_ValueError,
1070                          "GpuCorr3dMM backprop wrt. inputs: impossible output shape\\n"
1071                          "  bottom shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1072                          "  weights shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n"
1073                          "  top shape: %%ld x %%ld x %%ld x %%ld x %%ld\\n",
1074                          out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4],
1075                          PyGpuArray_DIMS(weights)[0], PyGpuArray_DIMS(weights)[1],
1076                          PyGpuArray_DIMS(weights)[2], PyGpuArray_DIMS(weights)[3],
1077                          PyGpuArray_DIMS(weights)[4],
1078                          PyGpuArray_DIMS(top)[0], PyGpuArray_DIMS(top)[1],
1079                          PyGpuArray_DIMS(top)[2], PyGpuArray_DIMS(top)[3],
1080                          PyGpuArray_DIMS(top)[4]);
1081             %(fail)s
1082         }
1083         break;
1084     default:
1085         PyErr_SetString(PyExc_ValueError, "BaseGpuCorr3dMM: direction must be 0, 1, or 2\\n");
1086         %(fail)s
1087     }
1088     out_dim_size[0] = (size_t)out_dim[0];
1089     out_dim_size[1] = (size_t)out_dim[1];
1090     out_dim_size[2] = (size_t)out_dim[2];
1091     out_dim_size[3] = (size_t)out_dim[3];
1092     out_dim_size[4] = (size_t)out_dim[4];
1093     // Prepare output array
1094     if (theano_prep_output(&amp;%(out)s, 5, out_dim_size, out_typecode, GA_C_ORDER, out_context) != 0)
1095     {
1096         PyErr_Format(PyExc_RuntimeError,
1097                 "BaseGpuCorrMM: Failed to allocate output of %%lld x %%lld x %%lld x %%lld x %%lld",
1098                 out_dim[0], out_dim[1], out_dim[2], out_dim[3], out_dim[4]);
1099         %(fail)s
1100     }
1101     if (!GpuArray_IS_C_CONTIGUOUS(&amp;%(out)s-&gt;ga)) {
1102         PyErr_SetString(PyExc_ValueError, "Only contiguous outputs are supported.");
1103         %(fail)s
1104     }
1105     // Call GPU code
1106     out2 = corr3dMM(%(bottom)s, %(weights)s, %(top)s, direction,
1107                     dH, dW, dD, dilH, dilW, dilD, padH, padW, padD, numgroups);
1108     if (out2==NULL){
1109        %(fail)s
1110     }
1111     assert (out2 == %(out)s);
1112 """ % sub
1113 class GpuCorr3dMM(BaseGpuCorr3dMM):
1114     """
1115     GPU correlation implementation using Matrix Multiplication.
1116     Parameters
1117     ----------
1118     border_mode
1119         The width of a border of implicit zeros to pad the
1120         input with. Must be a tuple with 3 elements giving the width of
1121         the padding on each side, or a single integer to pad the same
1122         on all sides, or a string shortcut setting the padding at runtime:
1123         ``'valid'`` for ``(0, 0, 0)`` (valid convolution, no padding), ``'full'``
1124         for ``(kernel_rows - 1, kernel_columns - 1, kernel_depth - 1)``
1125         (full convolution), ``'half'`` for ``(kernel_rows // 2,
1126         kernel_columns // 2, kernel_depth // 2)`` (same convolution for
1127         odd-sized kernels). Note that the three widths are each
1128         applied twice, once per side (left and right, top and bottom, front
1129         and back).
1130     subsample
1131         The subsample operation applied to each output image. Should be a tuple
1132         with 3 elements. `(sv, sh, sl)` is equivalent to
1133         `GpuCorrMM(...)(...)[:,:,::sv, ::sh, ::sl]`, but faster.
1134         Set to `(1, 1, 1)` to disable subsampling.
1135     filter_dilation
1136         The filter dilation operation applied to each input image.
1137         Should be a tuple with 3 elements.
1138         Set to `(1, 1, 1)` to disable filter dilation.
1139     num_groups
1140         The number of distinct groups the image and kernel must be
1141         divided into.
1142         should be an int
1143         set to 1 to disable grouped convolution
1144     Notes
1145     -----
1146     Currently, the Op requires the inputs, filters and outputs to be
1147     C-contiguous. Use :func:`gpu_contiguous
1148     &lt;theano.gpuarray.basic_ops.gpu_contiguous&gt;` on these arguments
1149     if needed.
1150     You can either enable the Theano flag `optimizer_including=conv_gemm`
1151     to automatically replace all convolution operations with `GpuCorr3dMM`
1152     or one of its gradients, or you can use it as a replacement for
1153     :func:`conv2d &lt;theano.tensor.nnet.conv.conv2d&gt;`, called as
1154     `GpuCorr3dMM(subsample=...)(image, filters)`. The latter is currently
1155     faster, but note that it computes a correlation -- if you need to
1156     compute a convolution, flip the filters as `filters[:,:,::-1,::-1,::-1]`.
1157     """
1158     def __init__(self, border_mode="valid",
1159                  subsample=(1, 1, 1),
1160                  filter_dilation=(1, 1, 1),
1161                  num_groups=1):
1162         super(GpuCorr3dMM, self).__init__(border_mode, subsample,
1163                                           filter_dilation, num_groups)
1164     def make_node(self, img, kern):
1165         ctx_name = infer_context_name(img, kern)
1166         img = as_gpuarray_variable(img, ctx_name)
1167         kern = as_gpuarray_variable(kern, ctx_name)
1168         if img.type.ndim != 5:
1169             raise TypeError('img must be 5D tensor')
1170         if kern.type.ndim != 5:
1171             raise TypeError('kern must be 5D tensor')
1172         broadcastable = [img.type.broadcastable[0], kern.type.broadcastable[0],
1173                          False, False, False]
1174         return Apply(self, [img, kern], [GpuArrayType(dtype=img.dtype,
1175                                                       context_name=ctx_name,
1176                                                       broadcastable=broadcastable)()])
1177     def c_code(self, node, nodename, inp, out_, sub):
1178         bottom, weights = inp
1179         top, = out_
1180         direction = "forward"
1181         return super(GpuCorr3dMM, self).c_code_helper(bottom, weights, top, direction, sub)
1182     def grad(self, inp, grads):
1183         bottom, weights = inp
1184         top, = grads
1185         top = gpu_contiguous(top)
1186         d_bottom = GpuCorr3dMM_gradInputs(self.border_mode,
1187                                           self.subsample,
1188                                           self.filter_dilation,
1189                                           self.num_groups)(
1190             weights, top, bottom.shape[-3:])
1191         d_weights = GpuCorr3dMM_gradWeights(self.border_mode,
1192                                             self.subsample,
1193                                             self.filter_dilation,
1194                                             self.num_groups)(
1195             bottom, top, weights.shape[-3:])
1196         return d_bottom, d_weights
1197 class GpuCorr3dMM_gradWeights(BaseGpuCorr3dMM):
1198     """
1199     Gradient wrt. filters for `GpuCorr3dMM`.
1200     Notes
1201     -----
1202     You will not want to use this directly, but rely on Theano's automatic
1203     differentiation or graph optimization to use it as needed.
1204     """
1205     def __init__(self, border_mode="valid",
1206                  subsample=(1, 1, 1),
1207                  filter_dilation=(1, 1, 1),
1208                  num_groups=1):
1209         super(GpuCorr3dMM_gradWeights, self).__init__(border_mode,
1210                                                       subsample,
1211                                                       filter_dilation,
1212                                                       num_groups)
1213     def make_node(self, img, topgrad, shape=None):
1214         ctx_name = infer_context_name(img, topgrad)
1215         img = as_gpuarray_variable(img, ctx_name)
1216         topgrad = as_gpuarray_variable(topgrad, ctx_name)
1217         if img.type.ndim != 5:
1218             raise TypeError('img must be 5D tensor')
1219         if topgrad.type.ndim != 5:
1220             raise TypeError('topgrad must be 5D tensor')
1221         if shape is None:
1222             if self.subsample != (1, 1, 1) or self.border_mode == "half":
1223                 raise ValueError('shape must be given if subsample != (1, 1, 1)'
1224                                  ' or border_mode == "half"')
1225             height_width_depth = []
1226         else:
1227             height_width_depth = [shape[0], shape[1], shape[2]]
1228             assert shape[0].ndim == 0
1229             assert shape[1].ndim == 0
1230             assert shape[2].ndim == 0
1231         broadcastable = [topgrad.type.broadcastable[1], img.type.broadcastable[1],
1232                          False, False, False]
1233         return Apply(self, [img, topgrad] + height_width_depth,
1234                      [GpuArrayType(dtype=img.dtype,
1235                                    context_name=ctx_name,
1236                                    broadcastable=broadcastable)()])
1237     def c_code(self, node, nodename, inp, out_, sub):
1238         bottom, top = inp[:2]
1239         height, width, depth = inp[2:] or (None, None, None)
1240         weights, = out_
1241         direction = "backprop weights"
1242         return super(GpuCorr3dMM_gradWeights, self).c_code_helper(bottom, weights, top, direction, sub, height, width, depth)
1243     def grad(self, inp, grads):
1244         bottom, top = inp[:2]
1245         weights, = grads
1246         weights = gpu_contiguous(weights)
1247         d_bottom = GpuCorr3dMM_gradInputs(self.border_mode,
1248                                           self.subsample,
1249                                           self.filter_dilation,
1250                                           self.num_groups)(weights,
1251                                                            top,
1252                                                            bottom.shape[-3:])
1253         d_top = GpuCorr3dMM(
1254             self.border_mode, self.subsample, self.filter_dilation,
1255             self.num_groups)(bottom, weights)
1256         d_height_width_depth = (theano.gradient.DisconnectedType()(),)\
1257             * 3 if len(inp) == 5 else ()
1258         return (d_bottom, d_top) + d_height_width_depth
1259     def connection_pattern(self, node):
1260         if node.nin == 2:
1261             return [[1], [1]]
1262         else:
1263             return [[1], [1], [0], [0], [0]]  # no connection to height, width, depth
1264 class GpuCorr3dMM_gradInputs(BaseGpuCorr3dMM):
1265     """
1266     Gradient wrt. inputs for `GpuCorr3dMM`.
1267     Notes
1268     -----
1269     You will not want to use this directly, but rely on Theano's automatic
1270     differentiation or graph optimization to use it as needed.
1271     """
1272     def __init__(self, border_mode="valid",
1273                  subsample=(1, 1, 1),
1274                  filter_dilation=(1, 1, 1),
1275                  num_groups=1):
1276         super(GpuCorr3dMM_gradInputs, self).__init__(border_mode, subsample,
1277                                                      filter_dilation, num_groups)
1278     def make_node(self, kern, topgrad, shape=None):
1279         ctx_name = infer_context_name(kern, topgrad)
1280         kern = as_gpuarray_variable(kern, ctx_name)
1281         topgrad = as_gpuarray_variable(topgrad, ctx_name)
1282         if kern.type.ndim != 5:
1283             raise TypeError('kern must be 5D tensor')
1284         if topgrad.type.ndim != 5:
1285             raise TypeError('topgrad must be 5D tensor')
1286         if shape is None:
1287             if self.subsample != (1, 1, 1):
1288                 raise ValueError('shape must be given if subsample != (1, 1, 1)')
1289             height_width_depth = []
1290         else:
1291             height_width_depth = [shape[0], shape[1], shape[2]]
1292             assert shape[0].ndim == 0
1293             assert shape[1].ndim == 0
1294             assert shape[2].ndim == 0
1295         if self.num_groups &gt; 1:
1296             broadcastable = [topgrad.type.broadcastable[0], False,
1297                              False, False, False]
1298         else:
1299             broadcastable = [topgrad.type.broadcastable[0], kern.type.broadcastable[-4],
1300                              False, False, False]
1301         return Apply(self, [kern, topgrad] + height_width_depth,
1302                      [GpuArrayType(dtype=topgrad.dtype,
1303                                    context_name=ctx_name,
1304                                    broadcastable=broadcastable)()])
1305     def c_code(self, node, nodename, inp, out_, sub):
1306         weights, top = inp[:2]
1307         height, width, depth = inp[2:] or (None, None, None)
1308         bottom, = out_
1309         direction = "backprop inputs"
1310         return super(GpuCorr3dMM_gradInputs, self).c_code_helper(bottom, weights, top, direction, sub, height, width, depth)
1311     def grad(self, inp, grads):
1312         weights, top = inp[:2]
1313         bottom, = grads
1314         bottom = gpu_contiguous(bottom)
1315         d_weights = GpuCorr3dMM_gradWeights(self.border_mode,
1316                                             self.subsample,
1317                                             self.filter_dilation,
1318                                             self.num_groups)(bottom,
1319                                                              top,
1320                                                              weights.shape[-3:])
1321         d_top = GpuCorr3dMM(self.border_mode,
1322                             self.subsample,
1323                             self.filter_dilation,
1324                             self.num_groups)(bottom, weights)
1325         d_height_width_depth = (theano.gradient.DisconnectedType()(),)\
1326             * 3 if len(inp) == 5 else ()
1327         return (d_weights, d_top) + d_height_width_depth
1328     def connection_pattern(self, node):
1329         if node.nin == 2:
1330             return [[1], [1]]
1331         else:
1332             return [[1], [1], [0], [0], [0]]  # no connection to height, width, depth
1333 @inplace_allocempty(GpuGemv, 0)
1334 def local_inplace_gpuagemv(node, inputs):
1335     return [gpugemv_inplace(*inputs)]
1336 @inplace_allocempty(GpuGemm, 0)
1337 def local_inplace_gpuagemm(node, inputs):
1338     return [gpugemm_inplace(*inputs)]
1339 @inplace_allocempty(GpuGer, 0)
1340 def local_inplace_gpuager(node, inputs):
1341     return [gpuger_inplace(*inputs)]
1342 @inplace_allocempty(GpuGemmBatch, 0)
1343 def local_inplace_gpuagemmbatch(node, inputs):
1344     return [gpugemmbatch_inplace(*inputs)]
1345 gpuablas_opt_inplace = in2out(LocalOptGroup(local_inplace_gpuagemv,
1346                                             local_inplace_gpuagemm,
1347                                             local_inplace_gpuager,
1348                                             local_inplace_gpuagemmbatch),
1349                               name='gpuablas_opt_inplace')
1350 optdb.register('InplaceGpuaBlasOpt',
1351                gpuablas_opt_inplace,
1352                70.0, 'fast_run', 'inplace', 'gpuarray')
</pre>
</div>
<div style="flex-grow: 1;">
<h3>
<center>
<span>test_pool_1.py</span>
<span> - </span>
<span></span>
</center>
</h3>
<hr/>
<pre>
1 from __future__ import absolute_import, print_function, division
2 from nose.plugins.skip import SkipTest
3 from parameterized import parameterized
4 from itertools import product
5 import os
6 import unittest
7 from six import reraise
8 from six.moves import cPickle
9 import six.moves.builtins as builtins
10 import sys
11 import numpy as np
12 import theano
13 import theano.tensor as tensor
14 from theano.tests import unittest_tools as utt
15 from theano.tensor.signal.pool import (Pool, pool_2d, pool_3d,
16                                        MaxPoolGrad, AveragePoolGrad,
17                                        max_pool_2d_same_size,
18                                        DownsampleFactorMaxGradGrad)
19 from theano import function
20 class TestDownsampleFactorMax(utt.InferShapeTester):
21     def test_out_shape(self):
22         assert Pool.out_shape((9, 8, 6), (2, 2)) == [9, 4, 3]
23         assert Pool.out_shape((8, 6), (2, 2)) == [4, 3]
24     @staticmethod
25     def numpy_max_pool_2d(input, ws, ignore_border=False, mode='max'):
26         if len(input.shape) &lt; 2:
27             raise NotImplementedError('input should have at least 2 dim,'
28                                       ' shape is %s'
29                                       % str(input.shape))
30         xi = 0
31         yi = 0
32         if not ignore_border:
33             if input.shape[-2] % ws[0]:
34                 xi += 1
35             if input.shape[-1] % ws[1]:
36                 yi += 1
37         out_shp = list(input.shape[:-2])
38         out_shp.append(input.shape[-2] // ws[0] + xi)
39         out_shp.append(input.shape[-1] // ws[1] + yi)
40         output_val = np.zeros(out_shp)
41         func = np.max
42         if mode == 'sum':
43             func = np.sum
44         elif mode != 'max':
45             func = np.average
46         for k in np.ndindex(*input.shape[:-2]):
47             for i in range(output_val.shape[-2]):
48                 ii = i * ws[0]
49                 for j in range(output_val.shape[-1]):
50                     jj = j * ws[1]
51                     patch = input[k][ii:ii + ws[0], jj:jj + ws[1]]
52                     output_val[k][i, j] = func(patch)
53         return output_val
54     @staticmethod
55     def numpy_max_pool_nd(input, ws, ignore_border=False, mode='max'):
56         if len(input.shape) &lt; len(ws):
57             raise NotImplementedError('input should have at least %s dim,'
58                                       ' shape is %s'
59                                       % (str(ws), str(input.shape)))
60         nd = len(ws)
61         si = [0] * nd
62         if not ignore_border:
63             for i in range(nd):
64                 if input.shape[-nd + i] % ws[i]:
65                     si[i] += 1
66         out_shp = list(input.shape[:-nd])
67         for i in range(nd):
68             out_shp.append(input.shape[-nd + i] // ws[i] + si[i])
69         output_val = np.zeros(out_shp)
70         func = np.max
71         if mode == 'sum':
72             func = np.sum
73         elif mode != 'max':
74             func = np.average
75         for l in np.ndindex(*input.shape[:-nd]):
76             for r in np.ndindex(*output_val.shape[-nd:]):
77                 patch = input[l][tuple(slice(r[i] * ws[i], (r[i] + 1) * ws[i])
78                                        for i in range(nd))]
79                 output_val[l][r] = func(patch)
80         return output_val
81     @staticmethod
82     def numpy_max_pool_2d_stride_pad(
83             x, ws, ignore_border=True, stride=None, pad=(0, 0), mode='max'):
84         assert ignore_border
85         pad_h = pad[0]
86         pad_w = pad[1]
87         h = x.shape[-2]
88         w = x.shape[-1]
89         assert ws[0] &gt; pad_h
90         assert ws[1] &gt; pad_w
91         def pad_img(x):
92             y = np.zeros(
93                 (x.shape[0], x.shape[1],
94                  x.shape[2] + pad_h * 2, x.shape[3] + pad_w * 2),
95                 dtype=x.dtype)
96             y[:, :, pad_h:(x.shape[2] + pad_h), pad_w:(x.shape[3] + pad_w)] = x
97             return y
98         img_rows = h + 2 * pad_h
99         img_cols = w + 2 * pad_w
100         out_r = (img_rows - ws[0]) // stride[0] + 1
101         out_c = (img_cols - ws[1]) // stride[1] + 1
102         out_shp = list(x.shape[:-2])
103         out_shp.append(out_r)
104         out_shp.append(out_c)
105         ws0, ws1 = ws
106         stride0, stride1 = stride
107         output_val = np.zeros(out_shp)
108         y = pad_img(x)
109         func = np.max
110         if mode == 'sum':
111             func = np.sum
112         elif mode != 'max':
113             func = np.average
114         inc_pad = mode == 'average_inc_pad'
115         for k in np.ndindex(*x.shape[:-2]):
116             for i in range(output_val.shape[-2]):
117                 ii_stride = i * stride[0]
118                 ii_end = builtins.min(ii_stride + ws[0], img_rows)
119                 if not inc_pad:
120                     ii_stride = builtins.max(ii_stride, pad_h)
121                     ii_end = builtins.min(ii_end, h + pad_h)
122                 for j in range(output_val.shape[-1]):
123                     jj_stride = j * stride[1]
124                     jj_end = builtins.min(jj_stride + ws[1], img_cols)
125                     if not inc_pad:
126                         jj_stride = builtins.max(jj_stride, pad_w)
127                         jj_end = builtins.min(jj_end, w + pad_w)
128                     patch = y[k][ii_stride:ii_end, jj_stride:jj_end]
129                     output_val[k][i, j] = func(patch)
130         return output_val
131     @staticmethod
132     def numpy_max_pool_nd_stride_pad(
133             input, ws, ignore_border=True, stride=None, pad=None, mode='max'):
134         assert ignore_border
135         nd = len(ws)
136         if pad is None:
137             pad = (0,) * nd
138         if stride is None:
139             stride = (0,) * nd
140         assert len(pad) == len(ws) == len(stride)
141         assert all(ws[i] &gt; pad[i] for i in range(nd))
142         def pad_img(x):
143             y = np.zeros(
144                 x.shape[0:-nd] +
145                 tuple(x.shape[-nd + i] + pad[i] * 2 for i in range(nd)),
146                 dtype=x.dtype)
147             block = ((slice(None),) * (len(x.shape) - nd) +
148                      tuple(slice(pad[i], x.shape[-nd + i] + pad[i])
149                            for i in range(nd)))
150             y[block] = x
151             return y
152         pad_img_shp = list(input.shape[:-nd])
153         out_shp = list(input.shape[:-nd])
154         for i in range(nd):
155             padded_size = input.shape[-nd + i] + 2 * pad[i]
156             pad_img_shp.append(padded_size)
157             out_shp.append((padded_size - ws[i]) // stride[i] + 1)
158         output_val = np.zeros(out_shp)
159         padded_input = pad_img(input)
160         func = np.max
161         if mode == 'sum':
162             func = np.sum
163         elif mode != 'max':
164             func = np.average
165         inc_pad = mode == 'average_inc_pad'
166         for l in np.ndindex(*input.shape[:-nd]):
167             for r in np.ndindex(*output_val.shape[-nd:]):
168                 region = []
169                 for i in range(nd):
170                     r_stride = r[i] * stride[i]
171                     r_end = builtins.min(r_stride + ws[i], pad_img_shp[-nd + i])
172                     if not inc_pad:
173                         r_stride = builtins.max(r_stride, pad[i])
174                         r_end = builtins.min(r_end, input.shape[-nd + i] + pad[i])
175                     region.append(slice(r_stride, r_end))
176                 patch = padded_input[l][region]
177                 output_val[l][r] = func(patch)
178         return output_val
179     @staticmethod
180     def numpy_max_pool_2d_stride(input, ws, ignore_border=False, stride=None,
181                                  mode='max'):
182         '''Helper function, implementing pool_2d in pure numpy
183            this function provides stride input to indicate the stide size
184            for the pooling regions. if not indicated, stride == ws.'''
185         if len(input.shape) &lt; 2:
186             raise NotImplementedError('input should have at least 2 dim,'
187                                       ' shape is %s'
188                                       % str(input.shape))
189         if stride is None:
190             stride = ws
191         img_rows = input.shape[-2]
192         img_cols = input.shape[-1]
193         out_r = 0
194         out_c = 0
195         if img_rows - ws[0] &gt;= 0:
196             out_r = (img_rows - ws[0]) // stride[0] + 1
197         if img_cols - ws[1] &gt;= 0:
198             out_c = (img_cols - ws[1]) // stride[1] + 1
199         if not ignore_border:
200             if out_r &gt; 0:
201                 if img_rows - ((out_r - 1) * stride[0] + ws[0]) &gt; 0:
202                     rr = img_rows - out_r * stride[0]
203                     if rr &gt; 0:
204                         out_r += 1
205             else:
206                 if img_rows &gt; 0:
207                         out_r += 1
208             if out_c &gt; 0:
209                 if img_cols - ((out_c - 1) * stride[1] + ws[1]) &gt; 0:
210                     cr = img_cols - out_c * stride[1]
211                     if cr &gt; 0:
212                         out_c += 1
213             else:
214                 if img_cols &gt; 0:
215                         out_c += 1
216         out_shp = list(input.shape[:-2])
217         out_shp.append(out_r)
218         out_shp.append(out_c)
219         func = np.max
220         if mode == 'sum':
221             func = np.sum
222         elif mode != 'max':
223             func = np.average
224         output_val = np.zeros(out_shp)
225         for k in np.ndindex(*input.shape[:-2]):
226             for i in range(output_val.shape[-2]):
227                 ii_stride = i * stride[0]
228                 ii_end = builtins.min(ii_stride + ws[0], img_rows)
229                 for j in range(output_val.shape[-1]):
230                     jj_stride = j * stride[1]
231                     jj_end = builtins.min(jj_stride + ws[1], img_cols)
232                     patch = input[k][ii_stride:ii_end, jj_stride:jj_end]
233                     output_val[k][i, j] = func(patch)
234         return output_val
235     @staticmethod
236     def numpy_max_pool_nd_stride(input, ws, ignore_border=False, stride=None,
237                                  mode='max'):
238         '''Helper function, implementing pooling in pure numpy
239            this function provides stride input to indicate the stide size
240            for the pooling regions. if not indicated, stride == ws.'''
241         nd = len(ws)
242         if stride is None:
243             stride = ws
244         assert len(stride) == len(ws)
245         out_shp = list(input.shape[:-nd])
246         for i in range(nd):
247             out = 0
248             if input.shape[-nd + i] - ws[i] &gt;= 0:
249                 out = (input.shape[-nd + i] - ws[i]) // stride[i] + 1
250             if not ignore_border:
251                 if out &gt; 0:
252                     if input.shape[-nd + i] - ((out - 1) * stride[i] + ws[i]) &gt; 0:
253                         if input.shape[-nd + i] - out * stride[i] &gt; 0:
254                             out += 1
255                 else:
256                     if input.shape[-nd + i] &gt; 0:
257                         out += 1
258             out_shp.append(out)
259         func = np.max
260         if mode == 'sum':
261             func = np.sum
262         elif mode != 'max':
263             func = np.average
264         output_val = np.zeros(out_shp)
265         for l in np.ndindex(*input.shape[:-nd]):
266             for r in np.ndindex(*output_val.shape[-nd:]):
267                 region = []
268                 for i in range(nd):
269                     r_stride = r[i] * stride[i]
270                     r_end = builtins.min(r_stride + ws[i], input.shape[-nd + i])
271                     region.append(slice(r_stride, r_end))
272                 patch = input[l][region]
273                 output_val[l][r] = func(patch)
274         return output_val
275     def test_DownsampleFactorMax(self):
276         rng = np.random.RandomState(utt.fetch_seed())
277         examples = (
278             ((2,), (16,)),
279             ((2,), (4, 16,)),
280             ((2,), (4, 2, 16,)),
281             ((1, 1), (4, 2, 16, 16)),
282             ((2, 2), (4, 2, 16, 16)),
283             ((3, 3), (4, 2, 16, 16)),
284             ((3, 2), (4, 2, 16, 16)),
285             ((3, 2, 2), (3, 2, 16, 16, 16)),
286             ((2, 2, 3, 2), (3, 2, 6, 6, 6, 5)),
287         )
288         for example, ignore_border, mode in product(examples,
289                                                     [True, False],
290                                                     ['max',
291                                                      'sum',
292                                                      'average_inc_pad',
293                                                      'average_exc_pad']):
294             (maxpoolshp, inputsize) = example
295             imval = rng.rand(*inputsize)
296             images = theano.shared(imval)
297             numpy_output_val = self.numpy_max_pool_nd(imval, maxpoolshp,
298                                                       ignore_border,
299                                                       mode=mode)
300             if len(maxpoolshp) == 2:
301                 output = pool_2d(images, maxpoolshp, ignore_border,
302                                  mode=mode)
303                 f = function([], [output, ])
304                 output_val = f()
305                 utt.assert_allclose(output_val, numpy_output_val)
306             elif len(maxpoolshp) == 3:
307                 output = pool_3d(images, maxpoolshp, ignore_border,
308                                  mode=mode)
309                 f = function([], [output, ])
310                 output_val = f()
311                 utt.assert_allclose(output_val, numpy_output_val)
312             maxpool_op = Pool(ndim=len(maxpoolshp),
313                               ignore_border=ignore_border,
314                               mode=mode)(images, maxpoolshp)
315             output_shape = Pool.out_shape(imval.shape, maxpoolshp,
316                                           ndim=len(maxpoolshp),
317                                           ignore_border=ignore_border)
318             utt.assert_allclose(np.asarray(output_shape), numpy_output_val.shape)
319             f = function([], maxpool_op)
320             output_val = f()
321             utt.assert_allclose(output_val, numpy_output_val)
322     def test_DownsampleFactorMaxStride(self):
323         rng = np.random.RandomState(utt.fetch_seed())
324         examples = (
325             ((1, 1), (1, 1), True, (4, 10, 16, 16), (4, 10, 16, 16)),
326             ((1, 1), (5, 7), True, (4, 10, 16, 16), (4, 10, 4, 3)),
327             ((1, 1), (1, 1), False, (4, 10, 16, 16), (4, 10, 16, 16)),
328             ((1, 1), (5, 7), False, (4, 10, 16, 16), (4, 10, 4, 3)),
329             ((3, 3), (1, 1), True, (4, 10, 16, 16), (4, 10, 14, 14)),
330             ((3, 3), (3, 3), True, (4, 10, 16, 16), (4, 10, 5, 5)),
331             ((3, 3), (5, 7), True, (4, 10, 16, 16), (4, 10, 3, 2)),
332             ((3, 3), (1, 1), False, (4, 10, 16, 16), (4, 10, 14, 14)),
333             ((3, 3), (3, 3), False, (4, 10, 16, 16), (4, 10, 6, 6)),
334             ((3, 3), (5, 7), False, (4, 10, 16, 16), (4, 10, 4, 3)),
335             ((5, 3), (1, 1), True, (4, 10, 16, 16), (4, 10, 12, 14)),
336             ((5, 3), (3, 3), True, (4, 10, 16, 16), (4, 10, 4, 5)),
337             ((5, 3), (5, 7), True, (4, 10, 16, 16), (4, 10, 3, 2)),
338             ((5, 3), (1, 1), False, (4, 10, 16, 16), (4, 10, 12, 14)),
339             ((5, 3), (3, 3), False, (4, 10, 16, 16), (4, 10, 5, 6)),
340             ((5, 3), (5, 7), False, (4, 10, 16, 16), (4, 10, 4, 3)),
341             ((16, 16), (1, 1), True, (4, 10, 16, 16), (4, 10, 1, 1)),
342             ((16, 16), (5, 7), True, (4, 10, 16, 16), (4, 10, 1, 1)),
343             ((16, 16), (1, 1), False, (4, 10, 16, 16), (4, 10, 1, 1)),
344             ((16, 16), (5, 7), False, (4, 10, 16, 16), (4, 10, 1, 1)),
345             ((3,), (5,), True, (16,), (3,)),
346             ((3,), (5,), True, (2, 16,), (2, 3,)),
347             ((5,), (3,), True, (2, 3, 16,), (2, 3, 4,)),
348             ((5, 1, 3), (3, 3, 3), True, (2, 16, 16, 16), (2, 4, 6, 5)),
349             ((5, 1, 3), (3, 3, 3), True, (4, 2, 16, 16, 16), (4, 2, 4, 6, 5)),
350         )
351         for example, mode in product(examples, ['max',
352                                                 'sum',
353                                                 'average_inc_pad',
354                                                 'average_exc_pad']):
355             (maxpoolshp, stride, ignore_border, inputshp, outputshp) = example
356             imval = rng.rand(*inputshp)
357             images = theano.shared(imval)
358             numpy_output_val = \
359                 self.numpy_max_pool_nd_stride(imval, maxpoolshp,
360                                               ignore_border, stride,
361                                               mode)
362             assert numpy_output_val.shape == outputshp, (
363                 "outshape is %s, calculated shape is %s"
364                 % (outputshp, numpy_output_val.shape))
365             maxpool_op = \
366                 Pool(ndim=len(maxpoolshp),
367                      ignore_border=ignore_border,
368                      mode=mode)(images, maxpoolshp, stride)
369             f = function([], maxpool_op)
370             output_val = f()
371             utt.assert_allclose(output_val, numpy_output_val)
372     def test_DownsampleFactorMaxStrideExtra(self):
373         rng = np.random.RandomState(utt.fetch_seed())
374         stridesizes = ((3, 2), (7, 5), (10, 6), (1, 1),
375                        (2, 3), (10</b></font>, 10), (1, 1))
376         imvsizs = ((16, 16), (16, 16), (16, 16), (<font color="#f63526"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>8, 5),
377                    (8, 5), (8, 5), (8, 5))
378         outputshps = ((4, 10, 4, 7), (4, 10, 5, 8), (4, 10, 2, 3),
379                       (4, 10, 3, 4), (4, 10, 2, 3), (4</b></font>, 10, 2, 3),
380                       (4, 10, 4, 1), (4, 10, 4, 1), (4, 10, 3, 2),
381                       (4, 10, 4, 2), (4, 10, 1, 0), (4, 10, 1, 1),
382                       (4, 10, 0, 0), (4, 10, 1, 1))
383         images = tensor.dtensor4()
384         for indx in np.arange(len(maxpoolshps)):
385             imvsize = imvsizs[indx]
386             imval = rng.rand(4, 10, imvsize[0], imvsize[1])
387             stride = stridesizes[indx]
388             maxpoolshp = maxpoolshps[indx]
389             for ignore_border, mode in product([True, False],
390                                                ['max', 'sum',
391                                                 'average_inc_pad',
392                                                 'average_exc_pad']):
393                 indx_out = indx * 2
394                 if not ignore_border:
395                     indx_out += 1
396                 outputshp = outputshps[indx_out]
397                 numpy_output_val = \
398                     self.numpy_max_pool_2d_stride(imval, maxpoolshp,
399                                                   ignore_border, stride, mode)
400                 assert numpy_output_val.shape == outputshp, (
401                     "outshape is %s, calculated shape is %s"
402                     % (outputshp, numpy_output_val.shape))
403                 maxpool_op = \
404                     Pool(ignore_border=ignore_border,
405                          ndim=len(maxpoolshp), mode=mode)(
406                         images, maxpoolshp, stride)
407                 f = function([images], maxpool_op)
408                 output_val = f(imval)
409                 utt.assert_allclose(output_val, numpy_output_val)
410     def test_DownsampleFactorMaxPaddingStride(self):
411         ignore_border = True  # padding does not support ignore_border=False
412         rng = np.random.RandomState(utt.fetch_seed())
413         examples = (
414             ((3,), (2,), (2,), (5,)),
415             ((3,), (2,), (2,), (4, 5)),
416             ((3,), (2,), (2,), (4, 2, 5, 5)),
417             ((3, 3), (2, 2), (2, 2), (4, 2, 5, 5)),
418             ((4, 4), (2, 2), (1, 2), (4, 2, 5, 5)),
419             ((3, 4), (1, 1), (2, 1), (4, 2, 5, 6)),
420             ((4, 3), (1, 2), (0, 0), (4, 2, 6, 5)),
421             ((2, 2), (2, 2), (1, 1), (4, 2, 5, 5)),
422             ((4, 3, 2), (1, 2, 2), (0, 2, 1), (4, 6, 6, 5)),
423             ((4, 3, 2), (1, 2, 2), (0, 2, 1), (4, 2, 6, 5, 5)),
424         )
425         for example, mode in product(examples,
426                                      ['max', 'sum', 'average_inc_pad',
427                                       'average_exc_pad']):
428             (maxpoolshp, stridesize, padsize, inputsize) = example
429             imval = rng.rand(*inputsize) - 0.5
430             images = theano.shared(imval)
431             numpy_output_val = self.numpy_max_pool_nd_stride_pad(
432                 imval, maxpoolshp, ignore_border,
433                 stridesize, padsize, mode)
434             maxpool_op = Pool(
435                 ndim=len(maxpoolshp),
436                 ignore_border=ignore_border,
437                 mode=mode
438                 )(images, maxpoolshp, stridesize, padsize)
439             f = function([], maxpool_op)
440             output_val = f()
441             utt.assert_allclose(output_val, numpy_output_val)
442     def test_DownsampleFactorMaxPaddingStride_grad(self):
443         rng = np.random.RandomState(utt.fetch_seed())
444         examples = (
445             ((10,), (5,), (3,), (2,)),
446             ((10,), (5,), (3,), (2, 2)),
447             ((10,), (5,), (3,), (1, 1, 2)),
448             ((10, 10), (5, 3), (3, 2), (1, 1, 2, 2)),
449             ((10, 5), (3, 5), (2, 3), (1, 1, 2, 1)),
450             ((5, 5), (3, 3), (3, 3), (1, 1, 2, 2)),
451             ((5, 5, 5), (3, 3, 3), (3, 3, 3), (1, 1, 2, 2, 2)),
452         )
453         for mode in ['max', 'sum']:
454             for example in examples:
455                 (maxpoolshp, stridesize, padsize, inputsize) = example
456                 imval = rng.rand(*inputsize) * 10.0
457                 def mp(input):
458                     return Pool(
459                         ndim=len(maxpoolshp),
460                         ignore_border=True,
461                         mode=mode,
462                         )(input, maxpoolshp, stridesize, padsize)
463                 utt.verify_grad(mp, [imval], rng=rng)
464     def test_DownsampleFactorMax_grad(self):
465         rng = np.random.RandomState(utt.fetch_seed())
466         examples = (
467             ((2,), (3,)),
468             ((2,), (2, 3)),
469             ((2,), (2, 3, 3)),
470             ((1, 1), (2, 3, 3, 4)),
471             ((3, 2), (2, 3, 3, 4)),
472             ((2, 3), (2, 3, 3, 4)),
473             ((1, 1, 1), (2, 3, 3)),
474             ((3, 2, 2), (2, 3, 3, 4)),
475             ((2, 2, 3), (2, 3, 3, 4, 4)),
476         )
477         for example, ignore_border, mode in product(examples,
478                                                     [True, False],
479                                                     ['max',
480                                                      'sum',
481                                                      'average_inc_pad',
482                                                      'average_exc_pad']):
483             (maxpoolshp, inputsize) = example
484             imval = rng.rand(*inputsize) * 10.0
485             def mp(input):
486                 return Pool(ndim=len(maxpoolshp),
487                             ignore_border=ignore_border,
488                             mode=mode)(input, maxpoolshp)
489             utt.verify_grad(mp, [imval], rng=rng)
490     pool_grad_stride_examples = (
491         ((1,), (1,), (16,)),
492         ((1,), (3,), (1, 16)),
493         ((1,), (5,), (1, 2, 16)),
494         ((2,), (1,), (16,)),
495         ((2,), (3,), (1, 16)),
496         ((2,), (5,), (1, 2, 16)),
497         ((1, 1), (1, 1), (1, 2, 16, 16)),
498         ((1, 1), (3, 3), (1, 2, 16, 16)),
499         ((1, 1), (5, 7), (1, 2, 16, 16)),
500         ((3, 3), (1, 1), (1, 2, 16, 16)),
501         ((3, 3), (3, 3), (1, 2, 16, 16)),
502         ((3, 3), (5, 7), (1, 2, 16, 16)),
503         ((5, 3), (1, 1), (1, 2, 16, 16)),
504         ((5, 3), (3, 3), (1, 2, 16, 16)),
505         ((5, 3), (5, 7), (1, 2, 16, 16)),
506         ((5, 1, 2), (1, 1, 1), (16, 3, 16)),
507         ((5, 1, 2), (3, 1, 2), (1, 16, 3, 16)),
508         ((5, 1, 2), (5, 1, 4), (1, 2, 16, 3, 16)),
509         ((5, 3), (3, 2), (1, 2, 16, 16)),
510         ((5, 3), (7, 5), (1, 2, 16, 16)),
511         ((5, 3), (10, 6), (1, 2, 16, 16)),
512         ((5, 5), (1, 1), (1, 2, 8, 5)),
513         ((3, 2), (2, 3), (1, 2, 8, 5)),
514         ((7, 7), (10, 10), (1, 2, 8, 5)),
515         ((9, 9), (1, 1), (1, 2, 8, 5)),
516     )
517     @parameterized.expand(product(pool_grad_stride_examples,
518                                   [True, False],
519                                   ['max',
520                                    'sum',
521                                    'average_inc_pad',
522                                    'average_exc_pad']),
523                           testcase_func_name=utt.custom_name_func)
524     def test_DownsampleFactorMax_grad_stride(self, example, ignore_border, mode):
525         rng = np.random.RandomState(utt.fetch_seed())
526         (maxpoolshp, stridesize, inputsize) = example
527         imval = rng.rand(*inputsize)
528         def mp(input):
529             return Pool(ndim=len(maxpoolshp),
530                         ignore_border=ignore_border,
531                         mode=mode)(input, maxpoolshp, stridesize)
532         utt.verify_grad(mp, [imval], rng=rng)
533     def test_DownsampleFactorMaxGrad_grad(self):
534         rng = np.random.RandomState(utt.fetch_seed())
535         examples = (
536             ((2,), (2,)),
537             ((2,), (2, 3)),
538             ((1, 1), (2, 3, 3, 4)),
539             ((3, 2), (2, 3, 3, 4)),
540             ((2, 3), (2, 3, 3, 4)),
541             ((1, 1, 1), (2, 3, 3, 4)),
542             ((3, 2, 2), (2, 3, 3, 4)),
543             ((2, 3, 2), (2, 3, 3, 4)),
544             ((2, 2, 3), (2, 3, 3, 4)),
545             ((2, 2, 3), (2, 1, 3, 3, 4)),
546         )
547         for (maxpoolshp, inputsize) in examples:
548             imval = rng.rand(*inputsize) * 10.0
549             for ignore_border in [True, False]:
550                 grad_shape = Pool.out_shape(
551                 grad_val = rng.rand(*grad_shape) * 10.0
552                 <font color="#53858b"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def mp(input, grad):
553                     out = Pool(
554                         ndim=len(maxpoolshp),
555                         ignore_border=ignore_border)(input, maxpoolshp)
556                     grad_op = MaxPoolGrad(
557                         ndim=len(maxpoolshp),
558                         ignore_border=</b></font>ignore_border)
559                     return grad_op(input, out, grad, maxpoolshp)
560                 utt.verify_grad(mp, [imval, grad_val], rng=rng)
561     def test_AveragePoolGrad_grad(self):
562         rng = np.random.RandomState(utt.fetch_seed())
563         examples = (
564             ((2,), (2,)),
565             ((2,), (2, 3)),
566             ((1, 1), (2, 3, 3, 4)),
567             ((3, 2), (2, 3, 3, 4)),
568             ((2, 3), (2, 3, 3, 4)),
569             ((3, 2, 2), (2, 3, 3, 4)),
570             ((2, 2, 3), (2, 3, 3, 4)),
571         )
572         for (avgpoolshp, inputsize) in examples:
573             imval = rng.rand(*inputsize) * 10.0
574             for ignore_border in [True, False]:
575                 for mode in ['sum', 'average_inc_pad', 'average_exc_pad']:
576                     grad_shape = Pool.out_shape(
577                         imval.shape, avgpoolshp, ndim=len(avgpoolshp),
578                         ignore_border=ignore_border)
579                     grad_val = rng.rand(*grad_shape) * 10.0
580                     def mp(input, grad):
581                         grad_op = AveragePoolGrad(
582                             ndim=len(avgpoolshp),
583                             ignore_border=ignore_border, mode=mode)
584                         return grad_op(input, grad, avgpoolshp)
585                     utt.verify_grad(mp, [imval, grad_val], rng=rng)
586     @parameterized.expand(product(pool_grad_stride_examples,
587                                   [True, False]),
588                           testcase_func_name=utt.custom_name_func)
589     def test_DownsampleFactorMaxGrad_grad_stride(self, example, ignore_border):
590         rng = np.random.RandomState(utt.fetch_seed())
591         (maxpoolshp, stride, inputsize) = example
592         imval = rng.rand(*inputsize)
593         grad_shape = Pool.out_shape(
594             imval.shape, maxpoolshp, ndim=len(maxpoolshp),
595             ignore_border=ignore_border, stride=stride)
596             grad_val = rng.rand(*grad_shape)
597             <font color="#980517"><div style="position:absolute;left:0"><a href="#"><img align="left" alt="other" border="0" src="back.gif"/></a></div><b>def mp(input, grad):
598                 out = Pool(
599                     ndim=len(maxpoolshp),
600                     ignore_border=ignore_border)(input, maxpoolshp, stride)
601                 grad_op = MaxPoolGrad(
602                     ndim=len(maxpoolshp),
603                     ignore_border=</b></font>ignore_border)
604                 return grad_op(input, out, grad, maxpoolshp, stride)
605                 utt.verify_grad(mp, [imval, grad_val], rng=rng)
606     @parameterized.expand(product(pool_grad_stride_examples,
607                                   [True, False],
608                                   ['sum',
609                                    'average_inc_pad',
610                                    'average_exc_pad']),
611                           testcase_func_name=utt.custom_name_func)
612     def test_AveragePoolGrad_grad_stride(self, example, ignore_border, mode):
613         rng = np.random.RandomState(utt.fetch_seed())
614         (avgpoolshp, stride, inputsize) = example
615         imval = rng.rand(*inputsize)
616         grad_shape = Pool.out_shape(
617             imval.shape, avgpoolshp,
618             ndim=len(avgpoolshp),
619             ignore_border=ignore_border, stride=stride)
620         if np.prod(grad_shape) != 0:
621             grad_val = rng.rand(*grad_shape)
622             def mp(input, grad):
623                 grad_op = AveragePoolGrad(
624                     ndim=len(avgpoolshp),
625                     ignore_border=ignore_border,
626                     mode=mode)
627                 return grad_op(input, grad, avgpoolshp, stride)
628             utt.verify_grad(mp, [imval, grad_val], rng=rng)
629     def test_DownsampleFactorMaxPaddingStride_grad_grad(self):
630         rng = np.random.RandomState(utt.fetch_seed())
631         examples = (
632             ((3,), (2,), (2,), (10,)),
633             ((3,), (2,), (2,), (2, 10,)),
634             ((3,), (2,), (2,), (2, 1, 10,)),
635             ((5, 3), (3, 2), (2, 2), (1, 1, 10, 10)),
636             ((3, 5), (2, 3), (2, 1), (1, 1, 10, 5)),
637             ((5, 3, 3), (3, 2, 2), (2, 2, 2), (1, 1, 10, 5, 5)),
638             ((3, 3, 5), (2, 2, 3), (2, 2, 1), (1, 1, 5, 5, 10)),
639         )
640         for (maxpoolshp, stridesize, padsize, inputsize) in examples:
641             imval = rng.rand(*inputsize) * 10.0
642             grad_shape = Pool.out_shape(imval.shape,
643                                         maxpoolshp,
644                                         ndim=len(maxpoolshp),
645                                         stride=stridesize,
646                                         ignore_border=True,
647                                         pad=padsize)
648             grad_val = rng.rand(*grad_shape) * 10.0
649             def mp(input, grad):
650                 out = Pool(
651                     ndim=len(maxpoolshp),
652                     ignore_border=True,
653                     )(input, maxpoolshp, stridesize, padsize)
654                 grad_op = MaxPoolGrad(ndim=len(maxpoolshp),
655                                       ignore_border=True)
656                 return grad_op(input, out, grad, maxpoolshp, stridesize, padsize)
657             utt.verify_grad(mp, [imval, grad_val], rng=rng)
658     def test_AveragePoolPaddingStride_grad_grad(self):
659         rng = np.random.RandomState(utt.fetch_seed())
660         examples = (
661             ((3,), (2,), (2,), (10,)),
662             ((3,), (2,), (2,), (2, 10,)),
663             ((3,), (2,), (2,), (2, 1, 10,)),
664             ((5, 3), (3, 2), (2, 2), (1, 1, 10, 10)),
665             ((3, 5), (2, 3), (2, 1), (1, 1, 10, 5)),
666             ((5, 3, 2), (3, 2, 1), (2, 2, 2), (1, 1, 10, 5, 5)),
667         )
668         for (avgpoolshp, stridesize, padsize, inputsize) in examples:
669             imval = rng.rand(*inputsize) * 10.0
670             for mode in ['sum', 'average_inc_pad']:
671                 grad_shape = Pool.out_shape(imval.shape,
672                                             avgpoolshp,
673                                             ndim=len(avgpoolshp),
674                                             stride=stridesize,
675                                             ignore_border=True,
676                                             pad=padsize)
677                 grad_val = rng.rand(*grad_shape) * 10.0
678                 def mp(input, grad):
679                     grad_op = AveragePoolGrad(ndim=len(avgpoolshp),
680                                               ignore_border=True,
681                                               mode=mode)
682                     return grad_op(input, grad, avgpoolshp, stridesize, padsize)
683                 utt.verify_grad(mp, [imval, grad_val], rng=rng)
684     def test_DownsampleFactorMax_hessian(self):
685         x_vec = tensor.vector('x')
686         z = tensor.dot(x_vec.dimshuffle(0, 'x'),
687                        x_vec.dimshuffle('x', 0))
688         y = pool_2d(input=z, ws=(2, 2), ignore_border=True)
689         C = tensor.exp(tensor.sum(y))
690         grad_hess = tensor.hessian(cost=C, wrt=x_vec)
691         fn_hess = function(inputs=[x_vec], outputs=grad_hess)
692         assert np.allclose(fn_hess([1, 2]), [[0., 0.], [0., 982.7667]])
693     def test_DownsampleFactorMaxGradGrad_grad(self):
694         rng = np.random.RandomState(utt.fetch_seed())
695         examples = (
696             ((3,), (2,), (2,), (10,)),
697             ((3,), (2,), (2,), (2, 10,)),
698             ((3,), (2,), (2,), (2, 1, 10,)),
699             ((5, 3), (3, 2), (2, 2), (1, 1, 10, 10)),
700             ((3, 5), (2, 3), (2, 1), (1, 1, 10, 5)),
701             ((3, 3), (3, 3), (2, 2), (1, 1, 5, 5)),
702             ((5, 3, 3), (3, 2, 2), (2, 2, 2), (1, 1, 10, 5, 5)),
703             ((3, 3, 5), (2, 2, 3), (2, 2, 1), (1, 1, 5, 5, 10)),
704         )
705         for (maxpoolshp, stridesize, padsize, inputsize) in examples:
706             imval1 = rng.rand(*inputsize) * 10.0
707             imval2 = rng.rand(*inputsize) * 10.0
708             def mp(input1, input2):
709                 op1 = Pool(ndim=len(maxpoolshp), ignore_border=True)
710                 pooled_out = op1(input1, maxpoolshp, stridesize, padsize)
711                 op2 = DownsampleFactorMaxGradGrad(
712                     ndim=len(maxpoolshp),
713                     ignore_border=True)
714                 out = op2(input1, pooled_out, input2, maxpoolshp, stridesize, padsize)
715                 return out
716             utt.verify_grad(mp, [imval1, imval2], rng=rng)
717     def test_max_pool_2d_2D(self):
718         rng = np.random.RandomState(utt.fetch_seed())
719         maxpoolshps = ((1, 1), (3, 2))
720         imval = rng.rand(4, 5)
721         images = tensor.dmatrix()
722         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
723                                                        [True, False],
724                                                        ['max', 'sum',
725                                                         'average_inc_pad',
726                                                         'average_exc_pad']):
727                 numpy_output_val = self.numpy_max_pool_2d(imval, maxpoolshp,
728                                                           ignore_border,
729                                                           mode=mode)
730                 output = pool_2d(images, maxpoolshp, ignore_border,
731                                  mode=mode)
732                 output_val = function([images], output)(imval)
733                 utt.assert_allclose(output_val, numpy_output_val)
734                 def mp(input):
735                     return pool_2d(input, maxpoolshp, ignore_border,
736                                    mode=mode)
737                 utt.verify_grad(mp, [imval], rng=rng)
738     def test_max_pool_3d_3D(self):
739         rng = np.random.RandomState(utt.fetch_seed())
740         maxpoolshps = ((1, 1, 1), (3, 2, 1))
741         imval = rng.rand(4, 5, 6)
742         images = tensor.dtensor3()
743         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
744                                                        [True, False],
745                                                        ['max', 'sum',
746                                                         'average_inc_pad',
747                                                         'average_exc_pad']):
748                 numpy_output_val = self.numpy_max_pool_nd(imval, maxpoolshp,
749                                                           ignore_border,
750                                                           mode=mode)
751                 output = pool_3d(images, maxpoolshp, ignore_border,
752                                  mode=mode)
753                 output_val = function([images], output)(imval)
754                 utt.assert_allclose(output_val, numpy_output_val)
755                 def mp(input):
756                     return pool_3d(input, maxpoolshp, ignore_border,
757                                    mode=mode)
758                 utt.verify_grad(mp, [imval], rng=rng)
759     def test_max_pool_3d_3D_deprecated_interface(self):
760         rng = np.random.RandomState(utt.fetch_seed())
761         maxpoolshps = ((1, 1, 1), (3, 2, 1))
762         imval = rng.rand(4, 5, 6)
763         images = tensor.dtensor3()
764         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
765                                                        [True, False],
766                                                        ['max', 'sum',
767                                                         'average_inc_pad',
768                                                         'average_exc_pad']):
769                 numpy_output_val = self.numpy_max_pool_nd(imval, maxpoolshp,
770                                                           ignore_border,
771                                                           mode=mode)
772                 output = pool_3d(input=images,
773                                  ds=maxpoolshp,
774                                  ignore_border=ignore_border,
775                                  st=maxpoolshp,
776                                  padding=(0, 0, 0),
777                                  mode=mode)
778                 output_val = function([images], output)(imval)
779                 utt.assert_allclose(output_val, numpy_output_val)
780                 def mp(input):
781                     return pool_3d(input, maxpoolshp, ignore_border,
782                                    mode=mode)
783     def test_max_pool_2d_2D_same_size(self):
784         rng = np.random.RandomState(utt.fetch_seed())
785         test_input_array = np.array([[[
786             [1., 2., 3., 4.],
787             [5., 6., 7., 8.]
788         ]]]).astype(theano.config.floatX)
789         test_answer_array = np.array([[[
790             [0., 0., 0., 0.],
791             [0., 6., 0., 8.]
792         ]]]).astype(theano.config.floatX)
793         input = tensor.tensor4(name='input')
794         patch_size = (2, 2)
795         op = max_pool_2d_same_size(input, patch_size)
796         op_output = function([input], op)(test_input_array)
797         utt.assert_allclose(op_output, test_answer_array)
798         def mp(input):
799             return max_pool_2d_same_size(input, patch_size)
800         utt.verify_grad(mp, [test_input_array], rng=rng)
801     def test_max_pool_2d_3D(self):
802         rng = np.random.RandomState(utt.fetch_seed())
803         maxpoolshps = [(1, 2)]
804         imval = rng.rand(2, 3, 4)
805         images = tensor.dtensor3()
806         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
807                                                        [True, False],
808                                                        ['max', 'sum',
809                                                         'average_inc_pad',
810                                                         'average_exc_pad']):
811                 numpy_output_val = self.numpy_max_pool_2d(imval, maxpoolshp,
812                                                           ignore_border,
813                                                           mode)
814                 output = pool_2d(images, maxpoolshp, ignore_border,
815                                  mode=mode)
816                 output_val = function([images], output)(imval)
817                 utt.assert_allclose(output_val, numpy_output_val)
818     def test_max_pool_2d_6D(self):
819         rng = np.random.RandomState(utt.fetch_seed())
820         maxpoolshps = [(3, 2)]
821         imval = rng.rand(2, 1, 1, 1, 3, 4)
822         images = tensor.TensorType('float64', [False] * 6)()
823         for maxpoolshp, ignore_border, mode in product(maxpoolshps,
824                                                        [True, False],
825                                                        ['max', 'sum',
826                                                         'average_inc_pad',
827                                                         'average_exc_pad']):
828                 numpy_output_val = self.numpy_max_pool_2d(imval, maxpoolshp,
829                                                           ignore_border,
830                                                           mode=mode)
831                 output = pool_2d(images, maxpoolshp, ignore_border,
832                                  mode=mode)
833                 output_val = function([images], output)(imval)
834                 utt.assert_allclose(output_val, numpy_output_val)
835     def test_infer_shape(self):
836         image = tensor.dtensor4()
837         maxout = tensor.dtensor4()
838         gz = tensor.dtensor4()
839         rng = np.random.RandomState(utt.fetch_seed())
840         maxpoolshps = ((1, 1), (2, 2), (3, 3), (2, 3), (3, 2))
841         image_val = rng.rand(4, 6, 7, 9)
842         out_shapes = [[[[4, 6, 7, 9], [4, 6, 7, 9]],
843                        [[4, 6, 3, 4], [4, 6, 4, 5]],
844                        [[4, 6, 2, 3], [4, 6, 3, 3]],
845                        [[4, 6, 3, 3], [4, 6, 4, 3]],
846                        [[4, 6, 2, 4], [4, 6, 3, 5]]],
847                       [[None, None],
848                        [[4, 6, 4, 5], None],
849                        [[4, 6, 3, 3], None],
850                        [[4, 6, 4, 3], None],
851                        [[4, 6, 3, 5], None]],
852                       [[None, None],
853                        [None, None],
854                        [[4, 6, 3, 4], None],
855                        [[4, 6, 4, 4], None],
856                        [None, None]]]
857         for i, maxpoolshp in enumerate(maxpoolshps):
858             for j, ignore_border in enumerate([True, False]):
859                 for k, pad in enumerate([(0, 0), (1, 1), (1, 2)]):
860                     if out_shapes[k][i][j] is None:
861                         continue
862                     self._compile_and_check([image],
863                                             [Pool(ignore_border=ignore_border)
864                                              (image, maxpoolshp, pad=pad)],
865                                             [image_val], Pool)
866                     maxout_val = rng.rand(*out_shapes[k][i][j])
867                     gz_val = rng.rand(*out_shapes[k][i][j])
868                     self._compile_and_check([image, maxout, gz],
869                                             [MaxPoolGrad(
870                                                 ignore_border=ignore_border)
871                                              (image, maxout, gz, maxpoolshp,
872                                               pad=pad)],
873                                             [image_val, maxout_val, gz_val],
874                                             MaxPoolGrad,
875                                             warn=False)
876         image = tensor.tensor(dtype='float64',
877                               broadcastable=(False, False, True, True))
878         image_val = rng.rand(4, 6, 1, 1)
879         self._compile_and_check(
880             [image],
881             [Pool(ignore_border=True)(image, (2, 2), pad=(0, 0))],
882             [image_val], Pool)
883     def test_pooling_with_tensor_vars(self):
884         x = tensor.ftensor4()
885         window_size = tensor.ivector()
886         stride = tensor.ivector()
887         padding = tensor.ivector()
888         data = np.random.normal(0, 1, (1, 1, 5, 5)).astype('float32')
889         for ignore_border in [True, False]:
890             for mode in ['max', 'sum', 'average_inc_pad', 'average_exc_pad']:
891                 y = pool_2d(x, window_size, ignore_border, stride, padding,
892                             mode)
893                 dx = theano.gradient.grad(y.sum(), x)
894                 var_fct = theano.function([x, window_size, stride, padding],
895                                           [y, dx])
896                 for ws in (4, 2, 5):
897                     for st in (2, 3):
898                         for pad in (0, 1):
899                             if (pad &gt; st or st &gt; ws or
900                                     (pad != 0 and not ignore_border) or
901                                     (mode == 'average_exc_pad' and pad != 0)):
902                                 continue
903                             y = pool_2d(x, (ws, ws), ignore_border, (st, st),
904                                         (pad, pad), mode)
905                             dx = theano.gradient.grad(y.sum(), x)
906                             fix_fct = theano.function([x], [y, dx])
907                             var_y, var_dx = var_fct(data, (ws, ws), (st, st),
908                                                     (pad, pad))
909                             fix_y, fix_dx = fix_fct(data)
910                             utt.assert_allclose(var_y, fix_y)
911                             utt.assert_allclose(var_dx, fix_dx)
912     def test_pooling_with_tensor_vars_deprecated_interface(self):
913         x = tensor.ftensor4()
914         window_size = tensor.ivector()
915         stride = tensor.ivector()
916         padding = tensor.ivector()
917         data = np.random.normal(0, 1, (1, 1, 5, 5)).astype('float32')
918         for ignore_border in [True, False]:
919             for mode in ['max', 'sum', 'average_inc_pad', 'average_exc_pad']:
920                 y = pool_2d(input=x,
921                             ds=window_size,
922                             ignore_border=ignore_border,
923                             st=stride,
924                             padding=padding,
925                             mode=mode)
926                 dx = theano.gradient.grad(y.sum(), x)
927                 var_fct = theano.function([x, window_size, stride, padding],
928                                           [y, dx])
929                 ws = 5
930                 st = 3
931                 pad = 1
932                 if (pad &gt; st or st &gt; ws or
933                         (pad != 0 and not ignore_border) or
934                         (mode == 'average_exc_pad' and pad != 0)):
935                     continue
936                 y = pool_2d(input=x,
937                             ds=(ws, ws),
938                             ignore_border=ignore_border,
939                             st=(st, st),
940                             padding=(pad, pad),
941                             mode=mode)
942                 dx = theano.gradient.grad(y.sum(), x)
943                 fix_fct = theano.function([x], [y, dx])
944                 var_y, var_dx = var_fct(data, (ws, ws), (st, st),
945                                         (pad, pad))
946                 fix_y, fix_dx = fix_fct(data)
947                 utt.assert_allclose(var_y, fix_y)
948                 utt.assert_allclose(var_dx, fix_dx)
949     def test_old_pool_interface(self):
950         if sys.version_info[0] != 3:
951             raise SkipTest('Skip old pool interface with python 2.x')
952         testfile_dir = os.path.dirname(os.path.realpath(__file__))
953         fname = 'old_pool_interface.pkl'
954         with open(os.path.join(testfile_dir, fname), 'rb') as fp:
955             try:
956                 old_fct = cPickle.load(fp, encoding='latin1')
957             except ImportError:
958                 if sys.platform == 'win32':
959                     exc_type, exc_value, exc_trace = sys.exc_info()
960                     reraise(SkipTest, exc_value, exc_trace)
961                 raise
962         x = theano.tensor.ftensor4()
963         y = pool_2d(x, (2, 2), mode='max', ignore_border=True)
964         z = pool_2d(x, (2, 2), mode='average_exc_pad', ignore_border=True)
965         dy_dx = theano.gradient.grad(y.sum(), x)
966         dz_dx = theano.gradient.grad(z.sum(), x)
967         new_fct = theano.function([x], [y, z, dy_dx, dz_dx])
968         rng = np.random.RandomState(utt.fetch_seed())
969         image_val = rng.rand(4, 6, 7, 9).astype(np.float32)
970         old_out = old_fct(image_val)
971         new_out = new_fct(image_val)
972         for o, n in zip(old_out, new_out):
973             utt.assert_allclose(o, n)
974 if __name__ == '__main__':
975     unittest.main()
</pre>
</div>
</div>
<div class="modal" id="myModal" style="display:none;"><div class="modal-content"><span class="close">x</span><p></p><div class="row"><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 1</div><div class="column" style="font-weight: bold;text-decoration: underline">Fragment from File 2</div></div><div class="row"><div class="column" id="column1">Column 1</div><div class="column" id="column2">Column 2</div></div></div></div><script>var modal=document.getElementById("myModal"),span=document.getElementsByClassName("close")[0];span.onclick=function(){modal.style.display="none"};window.onclick=function(a){a.target==modal&&(modal.style.display="none")};function openModal(a){console.log("the color is "+a);let b=getCodes(a);console.log(b);var c=document.getElementById("column1");c.innerText=b[0];var d=document.getElementById("column2");d.innerText=b[1];c.style.color=a;c.style.fontWeight="bold";d.style.fontWeight="bold";d.style.color=a;var e=document.getElementById("myModal");e.style.display="block"}function getCodes(a){for(var b=document.getElementsByTagName("font"),c=[],d=0;d<b.length;d++)b[d].attributes.color.nodeValue===a&&"-"!==b[d].innerText&&c.push(b[d].innerText);return c}</script><script>const params=window.location.search;const urlParams=new URLSearchParams(params);const searchText=urlParams.get('lines');let lines=searchText.split(',');for(let line of lines){const elements=document.getElementsByTagName('td');for(let i=0;i<elements.length;i++){if(elements[i].innerText.includes(line)){elements[i].style.background='green';break;}}}</script></body>
</html>
