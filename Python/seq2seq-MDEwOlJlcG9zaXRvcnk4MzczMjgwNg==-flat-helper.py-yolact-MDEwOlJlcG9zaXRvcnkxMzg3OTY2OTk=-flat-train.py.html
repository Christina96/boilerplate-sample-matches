
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <title>Code Files</title>
            <style>
                .column {
                    width: 47%;
                    float: left;
                    padding: 12px;
                    border: 2px solid #ffd0d0;
                }
        
                .modal {
                    display: none;
                    position: fixed;
                    z-index: 1;
                    left: 0;
                    top: 0;
                    width: 100%;
                    height: 100%;
                    overflow: auto;
                    background-color: rgb(0, 0, 0);
                    background-color: rgba(0, 0, 0, 0.4);
                }
    
                .modal-content {
                    height: 250%;
                    background-color: #fefefe;
                    margin: 5% auto;
                    padding: 20px;
                    border: 1px solid #888;
                    width: 80%;
                }
    
                .close {
                    color: #aaa;
                    float: right;
                    font-size: 20px;
                    font-weight: bold;
                    text-align: right;
                }
    
                .close:hover, .close:focus {
                    color: black;
                    text-decoration: none;
                    cursor: pointer;
                }
    
                .row {
                    float: right;
                    width: 100%;
                }
    
                .column_space  {
                    white - space: pre-wrap;
                }
                 
                pre {
                    width: 100%;
                    overflow-y: auto;
                    background: #f8fef2;
                }
                
                .match {
                    cursor:pointer; 
                    background-color:#00ffbb;
                }
        </style>
    </head>
    <body>
        <h2>Similarity: 7.138607971445568%, Tokens: 18</h2>
        <div class="column">
            <h3>seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-helper.py</h3>
            <pre><code><span onclick='openModal()' class='match'>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  import abc
5  import six
6  from tensorflow.contrib.distributions.python.ops import bernoulli
7  from tensorflow.contrib.distributions.python.ops import categorical
8  from tensorflow.python.framework import dtypes
9  from tensorflow.python.framework import ops
10  from tensorflow.python.layers import base as layers_base
11  from tensorflow.python.ops import array_ops
12  from tensorflow.python.ops import control_flow_ops
13  from tensorflow.python.ops import embedding_ops
14  from tensorflow.python.ops import math_ops
15  from tensorflow.python.ops import random_ops
16  from tensorflow.python.ops import tensor_array_ops
17  from tensorflow.python.util import nest
18  from seq2seq.contrib.seq2seq import decoder
</span>19  __all__ = [
20      "Helper",
21      "TrainingHelper",
22      "GreedyEmbeddingHelper",
23      "CustomHelper",
24      "ScheduledEmbeddingTrainingHelper",
25      "ScheduledOutputTrainingHelper",
26  ]
27  _transpose_batch_time = decoder._transpose_batch_time  # pylint: disable=protected-access
28  def _unstack_ta(inp):
29    return tensor_array_ops.TensorArray(
30        dtype=inp.dtype, size=array_ops.shape(inp)[0],
31        element_shape=inp.get_shape()[1:]).unstack(inp)
32  @six.add_metaclass(abc.ABCMeta)
33  class Helper(object):
34    @abc.abstractproperty
35    def batch_size(self):
36      raise NotImplementedError("batch_size has not been implemented")
37    @abc.abstractmethod
38    def initialize(self, name=None):
39      pass
40    @abc.abstractmethod
41    def sample(self, time, outputs, state, name=None):
42      pass
43    @abc.abstractmethod
44    def next_inputs(self, time, outputs, state, sample_ids, name=None):
45      pass
46  class CustomHelper(Helper):
47    def __init__(self, initialize_fn, sample_fn, next_inputs_fn):
48      self._initialize_fn = initialize_fn
49      self._sample_fn = sample_fn
50      self._next_inputs_fn = next_inputs_fn
51      self._batch_size = None
52    @property
53    def batch_size(self):
54      if self._batch_size is None:
55        raise ValueError("batch_size accessed before initialize was called")
56      return self._batch_size
57    def initialize(self, name=None):
58      with ops.name_scope(name, "%sInitialize" % type(self).__name__):
59        (finished, next_inputs) = self._initialize_fn()
60        if self._batch_size is None:
61          self._batch_size = array_ops.size(finished)
62      return (finished, next_inputs)
63    def sample(self, time, outputs, state, name=None):
64      with ops.name_scope(
65          name, "%sSample" % type(self).__name__, (time, outputs, state)):
66        return self._sample_fn(time=time, outputs=outputs, state=state)
67    def next_inputs(self, time, outputs, state, sample_ids, name=None):
68      with ops.name_scope(
69          name, "%sNextInputs" % type(self).__name__, (time, outputs, state)):
70        return self._next_inputs_fn(
71            time=time, outputs=outputs, state=state, sample_ids=sample_ids)
72  class TrainingHelper(Helper):
73    def __init__(self, inputs, sequence_length, time_major=False, name=None):
74      with ops.name_scope(name, "TrainingHelper", [inputs, sequence_length]):
75        inputs = ops.convert_to_tensor(inputs, name="inputs")
76        if not time_major:
77          inputs = nest.map_structure(_transpose_batch_time, inputs)
78        self._input_tas = nest.map_structure(_unstack_ta, inputs)
79        self._sequence_length = ops.convert_to_tensor(
80            sequence_length, name="sequence_length")
81        if self._sequence_length.get_shape().ndims != 1:
82          raise ValueError(
83              "Expected sequence_length to be a vector, but received shape: %s" %
84              self._sequence_length.get_shape())
85        self._zero_inputs = nest.map_structure(
86            lambda inp: array_ops.zeros_like(inp[0, :]), inputs)
87        self._batch_size = array_ops.size(sequence_length)
88    @property
89    def batch_size(self):
90      return self._batch_size
91    def initialize(self, name=None):
92      with ops.name_scope(name, "TrainingHelperInitialize"):
93        finished = math_ops.equal(0, self._sequence_length)
94        all_finished = math_ops.reduce_all(finished)
95        next_inputs = control_flow_ops.cond(
96            all_finished, lambda: self._zero_inputs,
97            lambda: nest.map_structure(lambda inp: inp.read(0), self._input_tas))
98        return (finished, next_inputs)
99    def sample(self, time, outputs, name=None, **unused_kwargs):
100      with ops.name_scope(name, "TrainingHelperSample", [time, outputs]):
101        sample_ids = math_ops.cast(
102            math_ops.argmax(outputs, axis=-1), dtypes.int32)
103        return sample_ids
104    def next_inputs(self, time, outputs, state, name=None, **unused_kwargs):
105      with ops.name_scope(name, "TrainingHelperNextInputs",
106                          [time, outputs, state]):
107        next_time = time + 1
108        finished = (next_time >= self._sequence_length)
109        all_finished = math_ops.reduce_all(finished)
110        def read_from_ta(inp):
111          return inp.read(next_time)
112        next_inputs = control_flow_ops.cond(
113            all_finished, lambda: self._zero_inputs,
114            lambda: nest.map_structure(read_from_ta, self._input_tas))
115        return (finished, next_inputs, state)
116  class ScheduledEmbeddingTrainingHelper(TrainingHelper):
117    def __init__(self, inputs, sequence_length, embedding, sampling_probability,
118                 time_major=False, seed=None, scheduling_seed=None, name=None):
119      with ops.name_scope(name, "ScheduledEmbeddingSamplingWrapper",
120                          [embedding, sampling_probability]):
121        if callable(embedding):
122          self._embedding_fn = embedding
123        else:
124          self._embedding_fn = (
125              lambda ids: embedding_ops.embedding_lookup(embedding, ids))
126        self._sampling_probability = ops.convert_to_tensor(
127            sampling_probability, name="sampling_probability")
128        if self._sampling_probability.get_shape().ndims not in (0, 1):
129          raise ValueError(
130              "sampling_probability must be either a scalar or a vector. "
131              "saw shape: %s" % (self._sampling_probability.get_shape()))
132        self._seed = seed
133        self._scheduling_seed = scheduling_seed
134        super(ScheduledEmbeddingTrainingHelper, self).__init__(
135            inputs=inputs,
136            sequence_length=sequence_length,
137            time_major=time_major,
138            name=name)
139    def initialize(self, name=None):
140      return super(ScheduledEmbeddingTrainingHelper, self).initialize(name=name)
141    def sample(self, time, outputs, state, name=None):
142      with ops.name_scope(name, "ScheduledEmbeddingTrainingHelperSample",
143                          [time, outputs, state]):
144        select_sample_noise = random_ops.random_uniform(
145            [self.batch_size], seed=self._scheduling_seed)
146        select_sample = (self._sampling_probability > select_sample_noise)
147        sample_id_sampler = categorical.Categorical(logits=outputs)
148        return array_ops.where(
149            select_sample,
150            sample_id_sampler.sample(seed=self._seed),
151            array_ops.tile([-1], [self.batch_size]))
152    def next_inputs(self, time, outputs, state, sample_ids, name=None):
153      with ops.name_scope(name, "ScheduledEmbeddingTrainingHelperSample",
154                          [time, outputs, state, sample_ids]):
155        (finished, base_next_inputs, state) = (
156            super(ScheduledEmbeddingTrainingHelper, self).next_inputs(
157                time=time,
158                outputs=outputs,
159                state=state,
160                sample_ids=sample_ids,
161                name=name))
162        def maybe_sample():
163          where_sampling = math_ops.cast(
164              array_ops.where(sample_ids > -1), dtypes.int32)
165          where_not_sampling = math_ops.cast(
166              array_ops.where(sample_ids <= -1), dtypes.int32)
167          where_sampling_flat = array_ops.reshape(where_sampling, [-1])
168          where_not_sampling_flat = array_ops.reshape(where_not_sampling, [-1])
169          sample_ids_sampling = array_ops.gather(sample_ids, where_sampling_flat)
170          inputs_not_sampling = array_ops.gather(
171              base_next_inputs, where_not_sampling_flat)
172          sampled_next_inputs = self._embedding_fn(sample_ids_sampling)
173          base_shape = array_ops.shape(base_next_inputs)
174          return (array_ops.scatter_nd(indices=where_sampling,
175                                       updates=sampled_next_inputs,
176                                       shape=base_shape)
177                  + array_ops.scatter_nd(indices=where_not_sampling,
178                                         updates=inputs_not_sampling,
179                                         shape=base_shape))
180        all_finished = math_ops.reduce_all(finished)
181        next_inputs = control_flow_ops.cond(
182            all_finished, lambda: base_next_inputs, maybe_sample)
183        return (finished, next_inputs, state)
184  class ScheduledOutputTrainingHelper(TrainingHelper):
185    def __init__(self, inputs, sequence_length, sampling_probability,
186                 time_major=False, seed=None, next_input_layer=None,
187                 auxiliary_inputs=None, name=None):
188      with ops.name_scope(name, "ScheduledOutputTrainingHelper",
189                          [inputs, auxiliary_inputs, sampling_probability]):
190        self._sampling_probability = ops.convert_to_tensor(
191            sampling_probability, name="sampling_probability")
192        if self._sampling_probability.get_shape().ndims not in (0, 1):
193          raise ValueError(
194              "sampling_probability must be either a scalar or a vector. "
195              "saw shape: %s" % (self._sampling_probability.get_shape()))
196        if auxiliary_inputs is None:
197          maybe_concatenated_inputs = inputs
198        else:
199          inputs = ops.convert_to_tensor(inputs, name="inputs")
200          auxiliary_inputs = ops.convert_to_tensor(
201              auxiliary_inputs, name="auxiliary_inputs")
202          maybe_concatenated_inputs = nest.map_structure(
203              lambda x, y: array_ops.concat((x, y), -1),
204              inputs, auxiliary_inputs)
205          if not time_major:
206            auxiliary_inputs = nest.map_structure(
207                _transpose_batch_time, auxiliary_inputs)
208        self._auxiliary_input_tas = (
209            nest.map_structure(_unstack_ta, auxiliary_inputs)
210            if auxiliary_inputs is not None else None)
211        self._seed = seed
212        if (next_input_layer is not None and not isinstance(next_input_layer,
213                                                            layers_base._Layer)):  # pylint: disable=protected-access
214          raise TypeError("next_input_layer must be a Layer, received: %s" %
215                          type(next_input_layer))
216        self._next_input_layer = next_input_layer
217        super(ScheduledOutputTrainingHelper, self).__init__(
218            inputs=maybe_concatenated_inputs,
219            sequence_length=sequence_length,
220            time_major=time_major,
221            name=name)
222    def initialize(self, name=None):
223      return super(ScheduledOutputTrainingHelper, self).initialize(name=name)
224    def sample(self, time, outputs, state, name=None):
225      with ops.name_scope(name, "ScheduledOutputTrainingHelperSample",
226                          [time, outputs, state]):
227        sampler = bernoulli.Bernoulli(probs=self._sampling_probability)
228        return math_ops.cast(
229            sampler.sample(sample_shape=self.batch_size, seed=self._seed),
230            dtypes.bool)
231    def next_inputs(self, time, outputs, state, sample_ids, name=None):
232      with ops.name_scope(name, "ScheduledOutputTrainingHelperNextInputs",
233                          [time, outputs, state, sample_ids]):
234        (finished, base_next_inputs, state) = (
235            super(ScheduledOutputTrainingHelper, self).next_inputs(
236                time=time,
237                outputs=outputs,
238                state=state,
239                sample_ids=sample_ids,
240                name=name))
241        def maybe_sample():
242          def maybe_concatenate_auxiliary_inputs(outputs_, indices=None):
243            if self._auxiliary_input_tas is None:
244              return outputs_
245            next_time = time + 1
246            auxiliary_inputs = nest.map_structure(
247                lambda ta: ta.read(next_time), self._auxiliary_input_tas)
248            if indices is not None:
249              auxiliary_inputs = array_ops.gather_nd(auxiliary_inputs, indices)
250            return nest.map_structure(
251                lambda x, y: array_ops.concat((x, y), -1),
252                outputs_, auxiliary_inputs)
253          if self._next_input_layer is None:
254            return array_ops.where(
255                sample_ids, maybe_concatenate_auxiliary_inputs(outputs),
256                base_next_inputs)
257          where_sampling = math_ops.cast(
258              array_ops.where(sample_ids), dtypes.int32)
259          where_not_sampling = math_ops.cast(
260              array_ops.where(math_ops.logical_not(sample_ids)), dtypes.int32)
261          outputs_sampling = array_ops.gather_nd(outputs, where_sampling)
262          inputs_not_sampling = array_ops.gather_nd(base_next_inputs,
263                                                    where_not_sampling)
264          sampled_next_inputs = maybe_concatenate_auxiliary_inputs(
265              self._next_input_layer(outputs_sampling), where_sampling)
266          base_shape = array_ops.shape(base_next_inputs)
267          return (array_ops.scatter_nd(indices=where_sampling,
268                                       updates=sampled_next_inputs,
269                                       shape=base_shape)
270                  + array_ops.scatter_nd(indices=where_not_sampling,
271                                         updates=inputs_not_sampling,
272                                         shape=base_shape))
273        all_finished = math_ops.reduce_all(finished)
274        next_inputs = control_flow_ops.cond(
275            all_finished, lambda: base_next_inputs, maybe_sample)
276        return (finished, next_inputs, state)
277  class GreedyEmbeddingHelper(Helper):
278    def __init__(self, embedding, start_tokens, end_token):
279      if callable(embedding):
280        self._embedding_fn = embedding
281      else:
282        self._embedding_fn = (
283            lambda ids: embedding_ops.embedding_lookup(embedding, ids))
284      self._start_tokens = ops.convert_to_tensor(
285          start_tokens, dtype=dtypes.int32, name="start_tokens")
286      self._end_token = ops.convert_to_tensor(
287          end_token, dtype=dtypes.int32, name="end_token")
288      if self._start_tokens.get_shape().ndims != 1:
289        raise ValueError("start_tokens must be a vector")
290      self._batch_size = array_ops.size(start_tokens)
291      if self._end_token.get_shape().ndims != 0:
292        raise ValueError("end_token must be a scalar")
293      self._start_inputs = self._embedding_fn(self._start_tokens)
294    @property
295    def batch_size(self):
296      return self._batch_size
297    def initialize(self, name=None):
298      finished = array_ops.tile([False], [self._batch_size])
299      return (finished, self._start_inputs)
300    def sample(self, time, outputs, state, name=None):
301      del time, state  # unused by sample_fn
302      if not isinstance(outputs, ops.Tensor):
303        raise TypeError("Expected outputs to be a single Tensor, got: %s" %
304                        type(outputs))
305      sample_ids = math_ops.cast(
306          math_ops.argmax(outputs, axis=-1), dtypes.int32)
307      return sample_ids
308    def next_inputs(self, time, outputs, state, sample_ids, name=None):
309      del time, outputs  # unused by next_inputs_fn
310      finished = math_ops.equal(sample_ids, self._end_token)
311      all_finished = math_ops.reduce_all(finished)
312      next_inputs = control_flow_ops.cond(
313          all_finished,
314          lambda: self._start_inputs,
315          lambda: self._embedding_fn(sample_ids))
316      return (finished, next_inputs, state)
</code></pre>
        </div>
        <div class="column">
            <h3>yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-train.py</h3>
            <pre><code><span onclick='openModal()' class='match'>1  from data import *
2  from utils.augmentations import SSDAugmentation, BaseTransform
3  from utils.functions import MovingAverage, SavePath
4  from utils.logger import Log
5  from utils import timer
6  from layers.modules import MultiBoxLoss
7  from yolact import Yolact
8  import os
9  import sys
10  import time
11  import math, random
12  from pathlib import Path
13  import torch
14  from torch.autograd import Variable
15  import torch.nn as nn
16  import torch.optim as optim
17  import torch.backends.cudnn as cudnn
18  import torch.nn.init as init
</span>19  import torch.utils.data as data
20  import numpy as np
21  import argparse
22  import datetime
23  import eval as eval_script
24  def str2bool(v):
25      return v.lower() in ("yes", "true", "t", "1")
26  parser = argparse.ArgumentParser(
27      description='Yolact Training Script')
28  parser.add_argument('--batch_size', default=8, type=int,
29                      help='Batch size for training')
30  parser.add_argument('--resume', default=None, type=str,
31                      help='Checkpoint state_dict file to resume training from. If this is "interrupt"'\
32                           ', the model will resume training from the interrupt file.')
33  parser.add_argument('--start_iter', default=-1, type=int,
34                      help='Resume training at this iter. If this is -1, the iteration will be'\
35                           'determined from the file name.')
36  parser.add_argument('--num_workers', default=4, type=int,
37                      help='Number of workers used in dataloading')
38  parser.add_argument('--cuda', default=True, type=str2bool,
39                      help='Use CUDA to train model')
40  parser.add_argument('--lr', '--learning_rate', default=None, type=float,
41                      help='Initial learning rate. Leave as None to read this from the config.')
42  parser.add_argument('--momentum', default=None, type=float,
43                      help='Momentum for SGD. Leave as None to read this from the config.')
44  parser.add_argument('--decay', '--weight_decay', default=None, type=float,
45                      help='Weight decay for SGD. Leave as None to read this from the config.')
46  parser.add_argument('--gamma', default=None, type=float,
47                      help='For each lr step, what to multiply the lr by. Leave as None to read this from the config.')
48  parser.add_argument('--save_folder', default='weights/',
49                      help='Directory for saving checkpoint models.')
50  parser.add_argument('--log_folder', default='logs/',
51                      help='Directory for saving logs.')
52  parser.add_argument('--config', default=None,
53                      help='The config object to use.')
54  parser.add_argument('--save_interval', default=10000, type=int,
55                      help='The number of iterations between saving the model.')
56  parser.add_argument('--validation_size', default=5000, type=int,
57                      help='The number of images to use for validation.')
58  parser.add_argument('--validation_epoch', default=2, type=int,
59                      help='Output validation information every n iterations. If -1, do no validation.')
60  parser.add_argument('--keep_latest', dest='keep_latest', action='store_true',
61                      help='Only keep the latest checkpoint instead of each one.')
62  parser.add_argument('--keep_latest_interval', default=100000, type=int,
63                      help='When --keep_latest is on, don\'t delete the latest file at these intervals. This should be a multiple of save_interval or 0.')
64  parser.add_argument('--dataset', default=None, type=str,
65                      help='If specified, override the dataset specified in the config with this one (example: coco2017_dataset).')
66  parser.add_argument('--no_log', dest='log', action='store_false',
67                      help='Don\'t log per iteration information into log_folder.')
68  parser.add_argument('--log_gpu', dest='log_gpu', action='store_true',
69                      help='Include GPU information in the logs. Nvidia-smi tends to be slow, so set this with caution.')
70  parser.add_argument('--no_interrupt', dest='interrupt', action='store_false',
71                      help='Don\'t save an interrupt when KeyboardInterrupt is caught.')
72  parser.add_argument('--batch_alloc', default=None, type=str,
73                      help='If using multiple GPUS, you can set this to be a comma separated list detailing which GPUs should get what local batch size (It should add up to your total batch size).')
74  parser.add_argument('--no_autoscale', dest='autoscale', action='store_false',
75                      help='YOLACT will automatically scale the lr and the number of iterations depending on the batch size. Set this if you want to disable that.')
76  parser.set_defaults(keep_latest=False, log=True, log_gpu=False, interrupt=True, autoscale=True)
77  args = parser.parse_args()
78  if args.config is not None:
79      set_cfg(args.config)
80  if args.dataset is not None:
81      set_dataset(args.dataset)
82  if args.autoscale and args.batch_size != 8:
83      factor = args.batch_size / 8
84      if __name__ == '__main__':
85          print('Scaling parameters by %.2f to account for a batch size of %d.' % (factor, args.batch_size))
86      cfg.lr *= factor
87      cfg.max_iter //= factor
88      cfg.lr_steps = [x // factor for x in cfg.lr_steps]
89  def replace(name):
90      if getattr(args, name) == None: setattr(args, name, getattr(cfg, name))
91  replace('lr')
92  replace('decay')
93  replace('gamma')
94  replace('momentum')
95  cur_lr = args.lr
96  if torch.cuda.device_count() == 0:
97      print('No GPUs detected. Exiting...')
98      exit(-1)
99  if args.batch_size // torch.cuda.device_count() < 6:
100      if __name__ == '__main__':
101          print('Per-GPU batch size is less than the recommended limit for batch norm. Disabling batch norm.')
102      cfg.freeze_bn = True
103  loss_types = ['B', 'C', 'M', 'P', 'D', 'E', 'S', 'I']
104  if torch.cuda.is_available():
105      if args.cuda:
106          torch.set_default_tensor_type('torch.cuda.FloatTensor')
107      if not args.cuda:
108          print("WARNING: It looks like you have a CUDA device, but aren't " +
109                "using CUDA.\nRun with --cuda for optimal training speed.")
110          torch.set_default_tensor_type('torch.FloatTensor')
111  else:
112      torch.set_default_tensor_type('torch.FloatTensor')
113  class NetLoss(nn.Module):
114      def __init__(self, net:Yolact, criterion:MultiBoxLoss):
115          super().__init__()
116          self.net = net
117          self.criterion = criterion
118      def forward(self, images, targets, masks, num_crowds):
119          preds = self.net(images)
120          losses = self.criterion(self.net, preds, targets, masks, num_crowds)
121          return losses
122  class CustomDataParallel(nn.DataParallel):
123      def scatter(self, inputs, kwargs, device_ids):
124          devices = ['cuda:' + str(x) for x in device_ids]
125          splits = prepare_data(inputs[0], devices, allocation=args.batch_alloc)
126          return [[split[device_idx] for split in splits] for device_idx in range(len(devices))], \
127              [kwargs] * len(devices)
128      def gather(self, outputs, output_device):
129          out = {}
130          for k in outputs[0]:
131              out[k] = torch.stack([output[k].to(output_device) for output in outputs])
132          return out
133  def train():
134      if not os.path.exists(args.save_folder):
135          os.mkdir(args.save_folder)
136      dataset = COCODetection(image_path=cfg.dataset.train_images,
137                              info_file=cfg.dataset.train_info,
138                              transform=SSDAugmentation(MEANS))
139      if args.validation_epoch > 0:
140          setup_eval()
141          val_dataset = COCODetection(image_path=cfg.dataset.valid_images,
142                                      info_file=cfg.dataset.valid_info,
143                                      transform=BaseTransform(MEANS))
144      yolact_net = Yolact()
145      net = yolact_net
146      net.train()
147      if args.log:
148          log = Log(cfg.name, args.log_folder, dict(args._get_kwargs()),
149              overwrite=(args.resume is None), log_gpu_stats=args.log_gpu)
150      timer.disable_all()
151      if args.resume == 'interrupt':
152          args.resume = SavePath.get_interrupt(args.save_folder)
153      elif args.resume == 'latest':
154          args.resume = SavePath.get_latest(args.save_folder, cfg.name)
155      if args.resume is not None:
156          print('Resuming training, loading {}...'.format(args.resume))
157          yolact_net.load_weights(args.resume)
158          if args.start_iter == -1:
159              args.start_iter = SavePath.from_str(args.resume).iteration
160      else:
161          print('Initializing weights...')
162          yolact_net.init_weights(backbone_path=args.save_folder + cfg.backbone.path)
163      optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,
164                            weight_decay=args.decay)
165      criterion = MultiBoxLoss(num_classes=cfg.num_classes,
166                               pos_threshold=cfg.positive_iou_threshold,
167                               neg_threshold=cfg.negative_iou_threshold,
168                               negpos_ratio=cfg.ohem_negpos_ratio)
169      if args.batch_alloc is not None:
170          args.batch_alloc = [int(x) for x in args.batch_alloc.split(',')]
171          if sum(args.batch_alloc) != args.batch_size:
172              print('Error: Batch allocation (%s) does not sum to batch size (%s).' % (args.batch_alloc, args.batch_size))
173              exit(-1)
174      net = CustomDataParallel(NetLoss(net, criterion))
175      if args.cuda:
176          net = net.cuda()
177      if not cfg.freeze_bn: yolact_net.freeze_bn() # Freeze bn so we don't kill our means
178      yolact_net(torch.zeros(1, 3, cfg.max_size, cfg.max_size).cuda())
179      if not cfg.freeze_bn: yolact_net.freeze_bn(True)
180      loc_loss = 0
181      conf_loss = 0
182      iteration = max(args.start_iter, 0)
183      last_time = time.time()
184      epoch_size = len(dataset) // args.batch_size
185      num_epochs = math.ceil(cfg.max_iter / epoch_size)
186      step_index = 0
187      data_loader = data.DataLoader(dataset, args.batch_size,
188                                    num_workers=args.num_workers,
189                                    shuffle=True, collate_fn=detection_collate,
190                                    pin_memory=True)
191      save_path = lambda epoch, iteration: SavePath(cfg.name, epoch, iteration).get_path(root=args.save_folder)
192      time_avg = MovingAverage()
193      global loss_types # Forms the print order
194      loss_avgs  = { k: MovingAverage(100) for k in loss_types }
195      print('Begin training!')
196      print()
197      try:
198          for epoch in range(num_epochs):
199              if (epoch+1)*epoch_size < iteration:
200                  continue
201              for datum in data_loader:
202                  if iteration == (epoch+1)*epoch_size:
203                      break
204                  if iteration == cfg.max_iter:
205                      break
206                  changed = False
207                  for change in cfg.delayed_settings:
208                      if iteration >= change[0]:
209                          changed = True
210                          cfg.replace(change[1])
211                          for avg in loss_avgs:
212                              avg.reset()
213                  if changed:
214                      cfg.delayed_settings = [x for x in cfg.delayed_settings if x[0] > iteration]
215                  if cfg.lr_warmup_until > 0 and iteration <= cfg.lr_warmup_until:
216                      set_lr(optimizer, (args.lr - cfg.lr_warmup_init) * (iteration / cfg.lr_warmup_until) + cfg.lr_warmup_init)
217                  while step_index < len(cfg.lr_steps) and iteration >= cfg.lr_steps[step_index]:
218                      step_index += 1
219                      set_lr(optimizer, args.lr * (args.gamma ** step_index))
220                  optimizer.zero_grad()
221                  losses = net(datum)
222                  losses = { k: (v).mean() for k,v in losses.items() } # Mean here because Dataparallel
223                  loss = sum([losses[k] for k in losses])
224                  loss.backward() # Do this to free up vram even if loss is not finite
225                  if torch.isfinite(loss).item():
226                      optimizer.step()
227                  for k in losses:
228                      loss_avgs[k].add(losses[k].item())
229                  cur_time  = time.time()
230                  elapsed   = cur_time - last_time
231                  last_time = cur_time
232                  if iteration != args.start_iter:
233                      time_avg.add(elapsed)
234                  if iteration % 10 == 0:
235                      eta_str = str(datetime.timedelta(seconds=(cfg.max_iter-iteration) * time_avg.get_avg())).split('.')[0]
236                      total = sum([loss_avgs[k].get_avg() for k in losses])
237                      loss_labels = sum([[k, loss_avgs[k].get_avg()] for k in loss_types if k in losses], [])
238                      print(('[%3d] %7d ||' + (' %s: %.3f |' * len(losses)) + ' T: %.3f || ETA: %s || timer: %.3f')
239                              % tuple([epoch, iteration] + loss_labels + [total, eta_str, elapsed]), flush=True)
240                  if args.log:
241                      precision = 5
242                      loss_info = {k: round(losses[k].item(), precision) for k in losses}
243                      loss_info['T'] = round(loss.item(), precision)
244                      if args.log_gpu:
245                          log.log_gpu_stats = (iteration % 10 == 0) # nvidia-smi is sloooow
246                      log.log('train', loss=loss_info, epoch=epoch, iter=iteration,
247                          lr=round(cur_lr, 10), elapsed=elapsed)
248                      log.log_gpu_stats = args.log_gpu
249                  iteration += 1
250                  if iteration % args.save_interval == 0 and iteration != args.start_iter:
251                      if args.keep_latest:
252                          latest = SavePath.get_latest(args.save_folder, cfg.name)
253                      print('Saving state, iter:', iteration)
254                      yolact_net.save_weights(save_path(epoch, iteration))
255                      if args.keep_latest and latest is not None:
256                          if args.keep_latest_interval <= 0 or iteration % args.keep_latest_interval != args.save_interval:
257                              print('Deleting old save...')
258                              os.remove(latest)
259              if args.validation_epoch > 0:
260                  if epoch % args.validation_epoch == 0 and epoch > 0:
261                      compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)
262          compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)
263      except KeyboardInterrupt:
264          if args.interrupt:
265              print('Stopping early. Saving network...')
266              SavePath.remove_interrupt(args.save_folder)
267              yolact_net.save_weights(save_path(epoch, repr(iteration) + '_interrupt'))
268          exit()
269      yolact_net.save_weights(save_path(epoch, iteration))
270  def set_lr(optimizer, new_lr):
271      for param_group in optimizer.param_groups:
272          param_group['lr'] = new_lr
273      global cur_lr
274      cur_lr = new_lr
275  def gradinator(x):
276      x.requires_grad = False
277      return x
278  def prepare_data(datum, devices:list=None, allocation:list=None):
279      with torch.no_grad():
280          if devices is None:
281              devices = ['cuda:0'] if args.cuda else ['cpu']
282          if allocation is None:
283              allocation = [args.batch_size // len(devices)] * (len(devices) - 1)
284              allocation.append(args.batch_size - sum(allocation)) # The rest might need more/less
285          images, (targets, masks, num_crowds) = datum
286          cur_idx = 0
287          for device, alloc in zip(devices, allocation):
288              for _ in range(alloc):
289                  images[cur_idx]  = gradinator(images[cur_idx].to(device))
290                  targets[cur_idx] = gradinator(targets[cur_idx].to(device))
291                  masks[cur_idx]   = gradinator(masks[cur_idx].to(device))
292                  cur_idx += 1
293          if cfg.preserve_aspect_ratio:
294              _, h, w = images[random.randint(0, len(images)-1)].size()
295              for idx, (image, target, mask, num_crowd) in enumerate(zip(images, targets, masks, num_crowds)):
296                  images[idx], targets[idx], masks[idx], num_crowds[idx] \
297                      = enforce_size(image, target, mask, num_crowd, w, h)
298          cur_idx = 0
299          split_images, split_targets, split_masks, split_numcrowds \
300              = [[None for alloc in allocation] for _ in range(4)]
301          for device_idx, alloc in enumerate(allocation):
302              split_images[device_idx]    = torch.stack(images[cur_idx:cur_idx+alloc], dim=0)
303              split_targets[device_idx]   = targets[cur_idx:cur_idx+alloc]
304              split_masks[device_idx]     = masks[cur_idx:cur_idx+alloc]
305              split_numcrowds[device_idx] = num_crowds[cur_idx:cur_idx+alloc]
306              cur_idx += alloc
307          return split_images, split_targets, split_masks, split_numcrowds
308  def no_inf_mean(x:torch.Tensor):
309      no_inf = [a for a in x if torch.isfinite(a)]
310      if len(no_inf) > 0:
311          return sum(no_inf) / len(no_inf)
312      else:
313          return x.mean()
314  def compute_validation_loss(net, data_loader, criterion):
315      global loss_types
316      with torch.no_grad():
317          losses = {}
318          iterations = 0
319          for datum in data_loader:
320              images, targets, masks, num_crowds = prepare_data(datum)
321              out = net(images)
322              wrapper = ScatterWrapper(targets, masks, num_crowds)
323              _losses = criterion(out, wrapper, wrapper.make_mask())
324              for k, v in _losses.items():
325                  v = v.mean().item()
326                  if k in losses:
327                      losses[k] += v
328                  else:
329                      losses[k] = v
330              iterations += 1
331              if args.validation_size <= iterations * args.batch_size:
332                  break
333          for k in losses:
334              losses[k] /= iterations
335          loss_labels = sum([[k, losses[k]] for k in loss_types if k in losses], [])
336          print(('Validation ||' + (' %s: %.3f |' * len(losses)) + ')') % tuple(loss_labels), flush=True)
337  def compute_validation_map(epoch, iteration, yolact_net, dataset, log:Log=None):
338      with torch.no_grad():
339          yolact_net.eval()
340          start = time.time()
341          print()
342          print("Computing validation mAP (this may take a while)...", flush=True)
343          val_info = eval_script.evaluate(yolact_net, dataset, train_mode=True)
344          end = time.time()
345          if log is not None:
346              log.log('val', val_info, elapsed=(end - start), epoch=epoch, iter=iteration)
347          yolact_net.train()
348  def setup_eval():
349      eval_script.parse_args(['--no_bar', '--max_images='+str(args.validation_size)])
350  if __name__ == '__main__':
351      train()
</code></pre>
        </div>
    
        <!-- The Modal -->
        <div id="myModal" class="modal">
            <div class="modal-content">
                <span class="row close">&times;</span>
                <div class='row'>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from seq2seq-MDEwOlJlcG9zaXRvcnk4MzczMjgwNg==-flat-helper.py</div>
                    <div class="column" style="font-weight: bold;text-decoration: underline">Fragment from yolact-MDEwOlJlcG9zaXRvcnkxMzg3OTY2OTk=-flat-train.py</div>
                </div>
                <div class="column column_space"><pre><code>1  from __future__ import absolute_import
2  from __future__ import division
3  from __future__ import print_function
4  import abc
5  import six
6  from tensorflow.contrib.distributions.python.ops import bernoulli
7  from tensorflow.contrib.distributions.python.ops import categorical
8  from tensorflow.python.framework import dtypes
9  from tensorflow.python.framework import ops
10  from tensorflow.python.layers import base as layers_base
11  from tensorflow.python.ops import array_ops
12  from tensorflow.python.ops import control_flow_ops
13  from tensorflow.python.ops import embedding_ops
14  from tensorflow.python.ops import math_ops
15  from tensorflow.python.ops import random_ops
16  from tensorflow.python.ops import tensor_array_ops
17  from tensorflow.python.util import nest
18  from seq2seq.contrib.seq2seq import decoder
</pre></code></div>
                <div class="column column_space"><pre><code>1  from data import *
2  from utils.augmentations import SSDAugmentation, BaseTransform
3  from utils.functions import MovingAverage, SavePath
4  from utils.logger import Log
5  from utils import timer
6  from layers.modules import MultiBoxLoss
7  from yolact import Yolact
8  import os
9  import sys
10  import time
11  import math, random
12  from pathlib import Path
13  import torch
14  from torch.autograd import Variable
15  import torch.nn as nn
16  import torch.optim as optim
17  import torch.backends.cudnn as cudnn
18  import torch.nn.init as init
</pre></code></div>
            </div>
        </div>
        <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the button that opens the modal
        var btn = document.getElementById("myBtn");
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks the button, open the modal
        function openModal(){
          modal.style.display = "block";
        }
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() {
        modal.style.display = "none";
        }
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
        if (event.target == modal) {
        modal.style.display = "none";
        } }
        
        </script>
    </body>
    </html>
    